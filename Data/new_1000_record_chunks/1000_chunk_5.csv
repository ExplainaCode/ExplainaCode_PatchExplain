record_number,buggy_code,fixed_code
4001,"private boolean testConnectPair() throws Exception {
  WebSocketServer.Options wssopt=new WebSocketServer.Options();
  wssopt.port=6668;
  wssopt.path=""String_Node_Str"";
  WebSocketServer wss=new WebSocketServer(ctx,wssopt,new WebSocketServer.ListeningCallback(){
    @Override public void onListening() throws Exception {
      Log.d(TAG,""String_Node_Str"");
      final WebSocket ws=new WebSocket(ctx,""String_Node_Str"",null,new WebSocket.Options());
      ws.onmessage(new onmessageListener(){
        @Override public void onMessage(        MessageEvent event) throws Exception {
          Log.d(TAG,""String_Node_Str"" + event.toString());
          if (event.isBinary()) {
            Log.d(TAG,""String_Node_Str"" + event.getData().toString());
          }
 else {
            Log.d(TAG,""String_Node_Str"" + (String)(event.getData()));
          }
        }
      }
);
      ws.onopen(new onopenListener(){
        @Override public void onOpen(        OpenEvent event) throws Exception {
          ws.send(""String_Node_Str"",new WebSocket.SendOptions(false,true),null);
          ctx.setInterval(new IntervalListener(){
            @Override public void onInterval() throws Exception {
              ws.send(""String_Node_Str"" + new Date(),new WebSocket.SendOptions(false,true),null);
            }
          }
,3000);
        }
      }
);
    }
  }
);
  wss.onconnection(new WebSocketServer.onconnectionListener(){
    @Override public void onConnection(    final WebSocket socket) throws Exception {
      Log.d(TAG,""String_Node_Str"" + socket);
      socket.onmessage(new WebSocket.onmessageListener(){
        @Override public void onMessage(        MessageEvent event) throws Exception {
          Log.d(TAG,""String_Node_Str"" + event.toString());
          if (event.isBinary()) {
            Log.d(TAG,""String_Node_Str"" + event.getData().toString());
          }
 else {
            Log.d(TAG,""String_Node_Str"" + (String)(event.getData()));
            socket.send(event.getData().toString() + ""String_Node_Str"",new WebSocket.SendOptions(false,true),null);
          }
        }
      }
);
      socket.send(""String_Node_Str"",new WebSocket.SendOptions(false,true),null);
    }
  }
);
  return true;
}","private boolean testConnectPair() throws Exception {
  WebSocketServer.Options wssopt=new WebSocketServer.Options();
  wssopt.port=6668;
  wssopt.path=""String_Node_Str"";
  WebSocketServer wss=new WebSocketServer(ctx,wssopt,new WebSocketServer.ListeningCallback(){
    @Override public void onListening() throws Exception {
      Log.d(TAG,""String_Node_Str"");
      final WebSocket ws=new WebSocket(ctx,""String_Node_Str"",null,new WebSocket.Options());
      ws.onmessage(new onmessageListener(){
        @Override public void onMessage(        MessageEvent event) throws Exception {
          Log.d(TAG,""String_Node_Str"" + event.toString());
          if (event.isBinary()) {
            Log.d(TAG,""String_Node_Str"" + event.getData().toString());
          }
 else {
            Log.d(TAG,""String_Node_Str"" + (String)(event.getData()));
          }
        }
      }
);
      ws.onerror(new WebSocket.onerrorListener(){
        @Override public void onError(        ErrorEvent event) throws Exception {
          Log.d(TAG,""String_Node_Str"" + event.getCode() + ""String_Node_Str""+ event.getError());
        }
      }
);
      ws.onopen(new onopenListener(){
        @Override public void onOpen(        OpenEvent event) throws Exception {
          ws.send(""String_Node_Str"",new WebSocket.SendOptions(false,false),null);
          ctx.setInterval(new IntervalListener(){
            @Override public void onInterval() throws Exception {
              ws.send(""String_Node_Str"" + new Date(),new WebSocket.SendOptions(false,true),null);
            }
          }
,3000);
        }
      }
);
    }
  }
);
  wss.onconnection(new WebSocketServer.onconnectionListener(){
    @Override public void onConnection(    final WebSocket socket) throws Exception {
      Log.d(TAG,""String_Node_Str"" + socket);
      socket.onmessage(new WebSocket.onmessageListener(){
        @Override public void onMessage(        MessageEvent event) throws Exception {
          Log.d(TAG,""String_Node_Str"" + event.toString());
          if (event.isBinary()) {
            Log.d(TAG,""String_Node_Str"" + event.getData().toString());
          }
 else {
            Log.d(TAG,""String_Node_Str"" + (String)(event.getData()));
            socket.send(event.getData().toString() + ""String_Node_Str"",new WebSocket.SendOptions(false,false),null);
          }
        }
      }
);
      socket.send(""String_Node_Str"",new WebSocket.SendOptions(false,false),null);
    }
  }
);
  return true;
}"
4002,"@Override public void onOpen(OpenEvent event) throws Exception {
  ws.send(""String_Node_Str"",new WebSocket.SendOptions(false,true),null);
  ctx.setInterval(new IntervalListener(){
    @Override public void onInterval() throws Exception {
      ws.send(""String_Node_Str"" + new Date(),new WebSocket.SendOptions(false,true),null);
    }
  }
,3000);
}","@Override public void onOpen(OpenEvent event) throws Exception {
  ws.send(""String_Node_Str"",new WebSocket.SendOptions(false,false),null);
  ctx.setInterval(new IntervalListener(){
    @Override public void onInterval() throws Exception {
      ws.send(""String_Node_Str"" + new Date(),new WebSocket.SendOptions(false,true),null);
    }
  }
,3000);
}"
4003,"@Override public void onMessage(MessageEvent event) throws Exception {
  Log.d(TAG,""String_Node_Str"" + event.toString());
  if (event.isBinary()) {
    Log.d(TAG,""String_Node_Str"" + event.getData().toString());
  }
 else {
    Log.d(TAG,""String_Node_Str"" + (String)(event.getData()));
    socket.send(event.getData().toString() + ""String_Node_Str"",new WebSocket.SendOptions(false,true),null);
  }
}","@Override public void onMessage(MessageEvent event) throws Exception {
  Log.d(TAG,""String_Node_Str"" + event.toString());
  if (event.isBinary()) {
    Log.d(TAG,""String_Node_Str"" + event.getData().toString());
  }
 else {
    Log.d(TAG,""String_Node_Str"" + (String)(event.getData()));
    socket.send(event.getData().toString() + ""String_Node_Str"",new WebSocket.SendOptions(false,false),null);
  }
}"
4004,"@Override public void onListening() throws Exception {
  Log.d(TAG,""String_Node_Str"");
  final WebSocket ws=new WebSocket(ctx,""String_Node_Str"",null,new WebSocket.Options());
  ws.onmessage(new onmessageListener(){
    @Override public void onMessage(    MessageEvent event) throws Exception {
      Log.d(TAG,""String_Node_Str"" + event.toString());
      if (event.isBinary()) {
        Log.d(TAG,""String_Node_Str"" + event.getData().toString());
      }
 else {
        Log.d(TAG,""String_Node_Str"" + (String)(event.getData()));
      }
    }
  }
);
  ws.onopen(new onopenListener(){
    @Override public void onOpen(    OpenEvent event) throws Exception {
      ws.send(""String_Node_Str"",new WebSocket.SendOptions(false,true),null);
      ctx.setInterval(new IntervalListener(){
        @Override public void onInterval() throws Exception {
          ws.send(""String_Node_Str"" + new Date(),new WebSocket.SendOptions(false,true),null);
        }
      }
,3000);
    }
  }
);
}","@Override public void onListening() throws Exception {
  Log.d(TAG,""String_Node_Str"");
  final WebSocket ws=new WebSocket(ctx,""String_Node_Str"",null,new WebSocket.Options());
  ws.onmessage(new onmessageListener(){
    @Override public void onMessage(    MessageEvent event) throws Exception {
      Log.d(TAG,""String_Node_Str"" + event.toString());
      if (event.isBinary()) {
        Log.d(TAG,""String_Node_Str"" + event.getData().toString());
      }
 else {
        Log.d(TAG,""String_Node_Str"" + (String)(event.getData()));
      }
    }
  }
);
  ws.onerror(new WebSocket.onerrorListener(){
    @Override public void onError(    ErrorEvent event) throws Exception {
      Log.d(TAG,""String_Node_Str"" + event.getCode() + ""String_Node_Str""+ event.getError());
    }
  }
);
  ws.onopen(new onopenListener(){
    @Override public void onOpen(    OpenEvent event) throws Exception {
      ws.send(""String_Node_Str"",new WebSocket.SendOptions(false,false),null);
      ctx.setInterval(new IntervalListener(){
        @Override public void onInterval() throws Exception {
          ws.send(""String_Node_Str"" + new Date(),new WebSocket.SendOptions(false,true),null);
        }
      }
,3000);
    }
  }
);
}"
4005,"@Override public void onConnection(final WebSocket socket) throws Exception {
  Log.d(TAG,""String_Node_Str"" + socket);
  socket.onmessage(new WebSocket.onmessageListener(){
    @Override public void onMessage(    MessageEvent event) throws Exception {
      Log.d(TAG,""String_Node_Str"" + event.toString());
      if (event.isBinary()) {
        Log.d(TAG,""String_Node_Str"" + event.getData().toString());
      }
 else {
        Log.d(TAG,""String_Node_Str"" + (String)(event.getData()));
        socket.send(event.getData().toString() + ""String_Node_Str"",new WebSocket.SendOptions(false,true),null);
      }
    }
  }
);
  socket.send(""String_Node_Str"",new WebSocket.SendOptions(false,true),null);
}","@Override public void onConnection(final WebSocket socket) throws Exception {
  Log.d(TAG,""String_Node_Str"" + socket);
  socket.onmessage(new WebSocket.onmessageListener(){
    @Override public void onMessage(    MessageEvent event) throws Exception {
      Log.d(TAG,""String_Node_Str"" + event.toString());
      if (event.isBinary()) {
        Log.d(TAG,""String_Node_Str"" + event.getData().toString());
      }
 else {
        Log.d(TAG,""String_Node_Str"" + (String)(event.getData()));
        socket.send(event.getData().toString() + ""String_Node_Str"",new WebSocket.SendOptions(false,false),null);
      }
    }
  }
);
  socket.send(""String_Node_Str"",new WebSocket.SendOptions(false,false),null);
}"
4006,"private void tickOnSocket(ClientRequest req,TCP.Socket socket) throws Exception {
  parserOnIncomingClient parser=new parserOnIncomingClient(context,socket);
  req.socket=socket;
  req.connection=socket;
  parser.Reinitialize(http_parser_type.HTTP_RESPONSE);
  parser.socket=socket;
  parser.incoming=null;
  req.setParser(parser);
  socket.setParser(parser);
  socket.set_httpMessage(req);
  Log.d(TAG,""String_Node_Str"" + req.connection);
  HTTP.httpSocketSetup(socket);
  if (req.maxHeadersCount > 0) {
    parser.maxHeaderPairs=req.maxHeadersCount << 1;
  }
 else {
    parser.maxHeaderPairs=2000;
  }
  this.socketErrorListener=new socketErrorListener(context,socket);
  socket.on(""String_Node_Str"",socketErrorListener);
  socket.get_readableState().setFlowing(TripleState.TRUE);
  this.socketOnData=new socketOnData(context,socket);
  socket.on(""String_Node_Str"",socketOnData);
  this.socketOnEnd=new socketOnEnd(context,socket);
  socket.on(""String_Node_Str"",socketOnEnd);
  this.socketCloseListener=new socketCloseListener(context,socket);
  socket.on(""String_Node_Str"",socketCloseListener);
  req.emit(""String_Node_Str"",socket);
  Log.d(TAG,""String_Node_Str"" + socket);
}","private void tickOnSocket(ClientRequest req,TCP.Socket socket) throws Exception {
  parserOnIncomingClient parser=new parserOnIncomingClient(context,socket);
  req.socket=socket;
  req.connection=socket;
  parser.Reinitialize(http_parser_type.HTTP_RESPONSE);
  parser.socket=socket;
  parser.incoming=null;
  req.setParser(parser);
  socket.setParser(parser);
  socket.set_httpMessage(req);
  Log.d(TAG,""String_Node_Str"" + req.connection);
  http.httpSocketSetup(socket);
  if (req.maxHeadersCount > 0) {
    parser.maxHeaderPairs=req.maxHeadersCount << 1;
  }
 else {
    parser.maxHeaderPairs=2000;
  }
  this.socketErrorListener=new socketErrorListener(context,socket);
  socket.on(""String_Node_Str"",socketErrorListener);
  socket.get_readableState().setFlowing(TripleState.TRUE);
  this.socketOnData=new socketOnData(context,socket);
  socket.on(""String_Node_Str"",socketOnData);
  this.socketOnEnd=new socketOnEnd(context,socket);
  socket.on(""String_Node_Str"",socketOnEnd);
  this.socketCloseListener=new socketCloseListener(context,socket);
  socket.on(""String_Node_Str"",socketCloseListener);
  req.emit(""String_Node_Str"",socket);
  Log.d(TAG,""String_Node_Str"" + socket);
}"
4007,"@Override public void onEvent(Object raw) throws Exception {
  ByteBuffer d=(ByteBuffer)raw;
  ClientRequest req=(ClientRequest)socket.get_httpMessage();
  parserOnIncomingClient parser=(parserOnIncomingClient)socket.getParser();
  assert(parser != null && parser.socket == socket);
  int ret=parser.Execute(d);
  if (ret < 0) {
    Log.d(TAG,""String_Node_Str"");
    IncomingParser.freeParser(parser,req);
    socket.destroy(null);
    req.emit(""String_Node_Str"",""String_Node_Str"");
    req.socket.set_hadError(true);
  }
 else   if (parser.incoming != null && parser.incoming.isUpgrade()) {
    int bytesParsed=ret;
    IncomingMessage res=parser.incoming;
    req.res=res;
    socket.removeListener(""String_Node_Str"",socketOnData);
    socket.removeListener(""String_Node_Str"",socketOnEnd);
    parser.Finish();
    ByteBuffer bodyHead=(ByteBuffer)Util.chunkSlice(d,bytesParsed);
    String eventName=req.method == ""String_Node_Str"" ? ""String_Node_Str"" : ""String_Node_Str"";
    if (req.listenerCount(eventName) > 0) {
      req.upgradeOrConnect=true;
      socket.emit(""String_Node_Str"");
      socket.removeListener(""String_Node_Str"",socketCloseListener);
      socket.removeListener(""String_Node_Str"",socketErrorListener);
      socket.get_readableState().setFlowing(TripleState.MAYBE);
      req.emit(eventName,new HTTP.response_socket_head_b(res,socket,bodyHead));
      req.emit(""String_Node_Str"");
    }
 else {
      socket.destroy(null);
    }
    IncomingParser.freeParser(parser,req);
  }
 else   if (parser.incoming != null && parser.incoming.isComplete() && parser.incoming.statusCode() != 100) {
    socket.removeListener(""String_Node_Str"",socketOnData);
    socket.removeListener(""String_Node_Str"",socketOnEnd);
    IncomingParser.freeParser(parser,req);
  }
}","@Override public void onEvent(Object raw) throws Exception {
  ByteBuffer d=(ByteBuffer)raw;
  ClientRequest req=(ClientRequest)socket.get_httpMessage();
  parserOnIncomingClient parser=(parserOnIncomingClient)socket.getParser();
  assert(parser != null && parser.socket == socket);
  int ret=parser.Execute(d);
  if (ret < 0) {
    Log.d(TAG,""String_Node_Str"");
    IncomingParser.freeParser(parser,req);
    socket.destroy(null);
    req.emit(""String_Node_Str"",""String_Node_Str"");
    req.socket.set_hadError(true);
  }
 else   if (parser.incoming != null && parser.incoming.isUpgrade()) {
    int bytesParsed=ret;
    IncomingMessage res=parser.incoming;
    req.res=res;
    socket.removeListener(""String_Node_Str"",socketOnData);
    socket.removeListener(""String_Node_Str"",socketOnEnd);
    parser.Finish();
    ByteBuffer bodyHead=(ByteBuffer)Util.chunkSlice(d,bytesParsed);
    String eventName=req.method == ""String_Node_Str"" ? ""String_Node_Str"" : ""String_Node_Str"";
    if (req.listenerCount(eventName) > 0) {
      req.upgradeOrConnect=true;
      socket.emit(""String_Node_Str"");
      socket.removeListener(""String_Node_Str"",socketCloseListener);
      socket.removeListener(""String_Node_Str"",socketErrorListener);
      socket.get_readableState().setFlowing(TripleState.MAYBE);
      req.emit(eventName,new http.response_socket_head_b(res,socket,bodyHead));
      req.emit(""String_Node_Str"");
    }
 else {
      socket.destroy(null);
    }
    IncomingParser.freeParser(parser,req);
  }
 else   if (parser.incoming != null && parser.incoming.isComplete() && parser.incoming.statusCode() != 100) {
    socket.removeListener(""String_Node_Str"",socketOnData);
    socket.removeListener(""String_Node_Str"",socketOnEnd);
    IncomingParser.freeParser(parser,req);
  }
}"
4008,"public boolean write(Object chunk,String encoding,WriteCB callback) throws Exception {
  if (Util.zeroString(this._header)) {
    this._implicitHeader();
  }
  Log.d(TAG,""String_Node_Str"");
  if (!this._hasBody) {
    Log.d(TAG,""String_Node_Str"" + ""String_Node_Str"");
    return true;
  }
  if (!Util.isString(chunk) && !Util.isBuffer(chunk)) {
    throw new Exception(""String_Node_Str"");
  }
  Log.d(TAG,""String_Node_Str"");
  if (Util.chunkLength(chunk) == 0)   return true;
  int len;
  boolean ret;
  if (this.chunkedEncoding) {
    Log.d(TAG,""String_Node_Str"");
    if (Util.isString(chunk) && encoding != ""String_Node_Str"" && encoding != ""String_Node_Str"" && encoding != ""String_Node_Str"") {
      Log.d(TAG,""String_Node_Str"");
      len=Util.stringByteLength((String)chunk,encoding);
      chunk=Integer.toString(len,16) + HTTP.CRLF + chunk+ HTTP.CRLF;
      Log.d(TAG,""String_Node_Str"" + chunk.toString());
      ret=this._send(chunk,encoding,callback);
    }
 else {
      if (Util.isString(chunk))       len=Util.stringByteLength((String)chunk,encoding);
 else       len=Util.chunkLength(chunk);
      if (this.connection != null && this.connection.corked() == 0) {
        this.connection.cork();
        final AbstractSocket conn=this.connection;
        context.nextTick(new nextTickListener(){
          @Override public void onNextTick() throws Exception {
            if (conn != null)             conn.uncork();
          }
        }
);
      }
      this._send(Integer.toString(len,16),""String_Node_Str"",null);
      this._send(ByteBuffer.wrap(""String_Node_Str"".getBytes(""String_Node_Str"")),null,null);
      this._send(chunk,encoding,null);
      ret=this._send(ByteBuffer.wrap(""String_Node_Str"".getBytes(""String_Node_Str"")),null,callback);
    }
  }
 else {
    Log.d(TAG,""String_Node_Str"");
    ret=this._send(chunk,encoding,callback);
  }
  Log.d(TAG,""String_Node_Str"" + ret);
  return ret;
}","public boolean write(Object chunk,String encoding,WriteCB callback) throws Exception {
  if (Util.zeroString(this._header)) {
    this._implicitHeader();
  }
  Log.d(TAG,""String_Node_Str"");
  if (!this._hasBody) {
    Log.d(TAG,""String_Node_Str"" + ""String_Node_Str"");
    return true;
  }
  if (!Util.isString(chunk) && !Util.isBuffer(chunk)) {
    throw new Exception(""String_Node_Str"");
  }
  Log.d(TAG,""String_Node_Str"");
  if (Util.chunkLength(chunk) == 0)   return true;
  int len;
  boolean ret;
  if (this.chunkedEncoding) {
    Log.d(TAG,""String_Node_Str"");
    if (Util.isString(chunk) && encoding != ""String_Node_Str"" && encoding != ""String_Node_Str"" && encoding != ""String_Node_Str"") {
      Log.d(TAG,""String_Node_Str"");
      len=Util.stringByteLength((String)chunk,encoding);
      chunk=Integer.toString(len,16) + http.CRLF + chunk+ http.CRLF;
      Log.d(TAG,""String_Node_Str"" + chunk.toString());
      ret=this._send(chunk,encoding,callback);
    }
 else {
      if (Util.isString(chunk))       len=Util.stringByteLength((String)chunk,encoding);
 else       len=Util.chunkLength(chunk);
      if (this.connection != null && this.connection.corked() == 0) {
        this.connection.cork();
        final AbstractSocket conn=this.connection;
        context.nextTick(new nextTickListener(){
          @Override public void onNextTick() throws Exception {
            if (conn != null)             conn.uncork();
          }
        }
);
      }
      this._send(Integer.toString(len,16),""String_Node_Str"",null);
      this._send(ByteBuffer.wrap(""String_Node_Str"".getBytes(""String_Node_Str"")),null,null);
      this._send(chunk,encoding,null);
      ret=this._send(ByteBuffer.wrap(""String_Node_Str"".getBytes(""String_Node_Str"")),null,callback);
    }
  }
 else {
    Log.d(TAG,""String_Node_Str"");
    ret=this._send(chunk,encoding,callback);
  }
  Log.d(TAG,""String_Node_Str"" + ret);
  return ret;
}"
4009,"public void addTrailers(Map<String,String> headers){
  this._trailer=""String_Node_Str"";
  for (  Entry<String,String> entry : headers.entrySet())   this._trailer+=entry.getKey() + ""String_Node_Str"" + entry.getValue()+ HTTP.CRLF;
}","public void addTrailers(Map<String,String> headers){
  this._trailer=""String_Node_Str"";
  for (  Entry<String,String> entry : headers.entrySet())   this._trailer+=entry.getKey() + ""String_Node_Str"" + entry.getValue()+ http.CRLF;
}"
4010,"protected void _storeHeader(String firstLine,Map<String,List<String>> headers) throws Exception {
  _State state=new _State(false,false,false,false,false,firstLine);
  if (headers != null && !headers.isEmpty()) {
    for (    Entry<String,List<String>> entry : headers.entrySet()) {
      String key=entry.getKey();
      for (      String value : entry.getValue())       storeHeader(state,key,value);
    }
  }
  Log.d(TAG,""String_Node_Str"");
  if (this.sendDate == true && state.sentDateHeader == false) {
    state.messageHeader+=""String_Node_Str"" + context.utcDate() + HTTP.CRLF;
  }
  Log.d(TAG,""String_Node_Str"");
  int statusCode=this.statusCode;
  if ((statusCode == 204 || statusCode == 304) && this.chunkedEncoding == true) {
    Log.d(TAG,""String_Node_Str"" + statusCode + ""String_Node_Str""+ ""String_Node_Str"");
    this.chunkedEncoding=false;
    this.shouldKeepAlive=false;
  }
  if (this._removedHeader.containsKey(""String_Node_Str"") && this._removedHeader.get(""String_Node_Str"")) {
    this._last=true;
    this.shouldKeepAlive=false;
  }
 else   if (state.sentConnectionHeader == false) {
    boolean shouldSendKeepAlive=this.shouldKeepAlive && (state.sentContentLengthHeader || this.useChunkedEncodingByDefault || this.agent != null);
    if (shouldSendKeepAlive) {
      state.messageHeader+=""String_Node_Str"";
    }
 else {
      this._last=true;
      state.messageHeader+=""String_Node_Str"";
    }
  }
  Log.d(TAG,""String_Node_Str"");
  if (state.sentContentLengthHeader == false && state.sentTransferEncodingHeader == false) {
    if (this._hasBody && !(this._removedHeader.containsKey(""String_Node_Str"") && this._removedHeader.get(""String_Node_Str""))) {
      if (this.useChunkedEncodingByDefault) {
        state.messageHeader+=""String_Node_Str"";
        this.chunkedEncoding=true;
      }
 else {
        this._last=true;
      }
    }
 else {
      this.chunkedEncoding=false;
    }
  }
  this._header=state.messageHeader + HTTP.CRLF;
  this._headerSent=false;
  Log.d(TAG,""String_Node_Str"");
  if (state.sentExpect)   this._send(""String_Node_Str"",""String_Node_Str"",null);
}","protected void _storeHeader(String firstLine,Map<String,List<String>> headers) throws Exception {
  _State state=new _State(false,false,false,false,false,firstLine);
  if (headers != null && !headers.isEmpty()) {
    for (    Entry<String,List<String>> entry : headers.entrySet()) {
      String key=entry.getKey();
      for (      String value : entry.getValue())       storeHeader(state,key,value);
    }
  }
  Log.d(TAG,""String_Node_Str"");
  if (this.sendDate == true && state.sentDateHeader == false) {
    state.messageHeader+=""String_Node_Str"" + context.utcDate() + http.CRLF;
  }
  Log.d(TAG,""String_Node_Str"");
  int statusCode=this.statusCode;
  if ((statusCode == 204 || statusCode == 304) && this.chunkedEncoding == true) {
    Log.d(TAG,""String_Node_Str"" + statusCode + ""String_Node_Str""+ ""String_Node_Str"");
    this.chunkedEncoding=false;
    this.shouldKeepAlive=false;
  }
  if (this._removedHeader.containsKey(""String_Node_Str"") && this._removedHeader.get(""String_Node_Str"")) {
    this._last=true;
    this.shouldKeepAlive=false;
  }
 else   if (state.sentConnectionHeader == false) {
    boolean shouldSendKeepAlive=this.shouldKeepAlive && (state.sentContentLengthHeader || this.useChunkedEncodingByDefault || this.agent != null);
    if (shouldSendKeepAlive) {
      state.messageHeader+=""String_Node_Str"";
    }
 else {
      this._last=true;
      state.messageHeader+=""String_Node_Str"";
    }
  }
  Log.d(TAG,""String_Node_Str"");
  if (state.sentContentLengthHeader == false && state.sentTransferEncodingHeader == false) {
    if (this._hasBody && !(this._removedHeader.containsKey(""String_Node_Str"") && this._removedHeader.get(""String_Node_Str""))) {
      if (this.useChunkedEncodingByDefault) {
        state.messageHeader+=""String_Node_Str"";
        this.chunkedEncoding=true;
      }
 else {
        this._last=true;
      }
    }
 else {
      this.chunkedEncoding=false;
    }
  }
  this._header=state.messageHeader + http.CRLF;
  this._headerSent=false;
  Log.d(TAG,""String_Node_Str"");
  if (state.sentExpect)   this._send(""String_Node_Str"",""String_Node_Str"",null);
}"
4011,"protected void storeHeader(_State state,String field,String value){
  OutgoingMessage self=this;
  value=value.replaceAll(""String_Node_Str"",""String_Node_Str"");
  state.messageHeader+=field + ""String_Node_Str"" + value+ HTTP.CRLF;
  if (Pattern.matches(connectionExpression,field)) {
    state.sentConnectionHeader=true;
    if (Pattern.matches(closeExpression,value)) {
      self._last=true;
    }
 else {
      self.shouldKeepAlive=true;
    }
  }
 else   if (Pattern.matches(transferEncodingExpression,field)) {
    state.sentTransferEncodingHeader=true;
    if (Pattern.matches(HTTP.chunkExpression,value))     self.chunkedEncoding=true;
  }
 else   if (Pattern.matches(contentLengthExpression,field)) {
    state.sentContentLengthHeader=true;
  }
 else   if (Pattern.matches(dateExpression,field)) {
    state.sentDateHeader=true;
  }
 else   if (Pattern.matches(expectExpression,field)) {
    state.sentExpect=true;
  }
}","protected void storeHeader(_State state,String field,String value){
  OutgoingMessage self=this;
  value=value.replaceAll(""String_Node_Str"",""String_Node_Str"");
  state.messageHeader+=field + ""String_Node_Str"" + value+ http.CRLF;
  if (Pattern.matches(connectionExpression,field)) {
    state.sentConnectionHeader=true;
    if (Pattern.matches(closeExpression,value)) {
      self._last=true;
    }
 else {
      self.shouldKeepAlive=true;
    }
  }
 else   if (Pattern.matches(transferEncodingExpression,field)) {
    state.sentTransferEncodingHeader=true;
    if (Pattern.matches(http.chunkExpression,value))     self.chunkedEncoding=true;
  }
 else   if (Pattern.matches(contentLengthExpression,field)) {
    state.sentContentLengthHeader=true;
  }
 else   if (Pattern.matches(dateExpression,field)) {
    state.sentDateHeader=true;
  }
 else   if (Pattern.matches(expectExpression,field)) {
    state.sentExpect=true;
  }
}"
4012,"@Override protected boolean onIncoming(final IncomingMessage req,boolean shouldKeepAlive) throws Exception {
  final IncomingParser ips=this;
  incomings.add(req);
  if (!socket.is_paused()) {
    boolean needPause=socket.get_writableState().isNeedDrain();
    if (needPause) {
      socket.set_paused(true);
      socket.pause();
    }
  }
  final ServerResponse res=new ServerResponse(context,req);
  res.setShouldKeepAlive(shouldKeepAlive);
  if (socket.get_httpMessage() != null) {
    outgoings.add(res);
    Log.d(TAG,""String_Node_Str"");
  }
 else {
    res.assignSocket(socket);
    Log.d(TAG,""String_Node_Str"");
  }
  Listener resOnFinish=new Listener(){
    @Override public void onEvent(    Object data) throws Exception {
      assert(incomings.size() == 0 || incomings.get(0) == req);
      if (incomings.size() > 0)       incomings.remove(0);
      if (!req.is_consuming() && !req.get_readableState().isResumeScheduled())       req._dump();
      res.detachSocket(socket);
      Log.d(TAG,""String_Node_Str"");
      ips.Reinitialize(ips.getType());
      if (res.is_last()) {
        Log.d(TAG,""String_Node_Str"");
        socket.destroySoon();
      }
 else {
        ServerResponse m=outgoings.remove(0);
        if (m != null) {
          m.assignSocket(socket);
        }
      }
    }
  }
;
  res.on(""String_Node_Str"",resOnFinish);
  if (req.getHeaders().containsKey(""String_Node_Str"") && !req.getHeaders().get(""String_Node_Str"").isEmpty() && (req.getHttpVersionMajor() == 1 && req.getHttpVersionMinor() == 1)&& Pattern.matches(HTTP.continueExpression,req.getHeaders().get(""String_Node_Str"").get(0))) {
    res.set_expect_continue(true);
    if (self.listenerCount(""String_Node_Str"") > 0) {
      self.emit(""String_Node_Str"",new request_response_b(req,res));
    }
 else {
      res.writeContinue(null);
      self.emit(""String_Node_Str"",new request_response_b(req,res));
    }
  }
 else {
    self.emit(""String_Node_Str"",new request_response_b(req,res));
  }
  return false;
}","@Override protected boolean onIncoming(final IncomingMessage req,boolean shouldKeepAlive) throws Exception {
  final IncomingParser ips=this;
  incomings.add(req);
  if (!socket.is_paused()) {
    boolean needPause=socket.get_writableState().isNeedDrain();
    if (needPause) {
      socket.set_paused(true);
      socket.pause();
    }
  }
  final ServerResponse res=new ServerResponse(context,req);
  res.setShouldKeepAlive(shouldKeepAlive);
  if (socket.get_httpMessage() != null) {
    outgoings.add(res);
    Log.d(TAG,""String_Node_Str"");
  }
 else {
    res.assignSocket(socket);
    Log.d(TAG,""String_Node_Str"");
  }
  Listener resOnFinish=new Listener(){
    @Override public void onEvent(    Object data) throws Exception {
      assert(incomings.size() == 0 || incomings.get(0) == req);
      if (incomings.size() > 0)       incomings.remove(0);
      if (!req.is_consuming() && !req.get_readableState().isResumeScheduled())       req._dump();
      res.detachSocket(socket);
      Log.d(TAG,""String_Node_Str"");
      ips.Reinitialize(ips.getType());
      if (res.is_last()) {
        Log.d(TAG,""String_Node_Str"");
        socket.destroySoon();
      }
 else {
        ServerResponse m=outgoings.remove(0);
        if (m != null) {
          m.assignSocket(socket);
        }
      }
    }
  }
;
  res.on(""String_Node_Str"",resOnFinish);
  if (req.getHeaders().containsKey(""String_Node_Str"") && !req.getHeaders().get(""String_Node_Str"").isEmpty() && (req.getHttpVersionMajor() == 1 && req.getHttpVersionMinor() == 1)&& Pattern.matches(http.continueExpression,req.getHeaders().get(""String_Node_Str"").get(0))) {
    res.set_expect_continue(true);
    if (self.listenerCount(""String_Node_Str"") > 0) {
      self.emit(""String_Node_Str"",new request_response_b(req,res));
    }
 else {
      res.writeContinue(null);
      self.emit(""String_Node_Str"",new request_response_b(req,res));
    }
  }
 else {
    self.emit(""String_Node_Str"",new request_response_b(req,res));
  }
  return false;
}"
4013,"/** 
 * create a WordElement with the specified baseForm, category, ID
 * @param baseForm - base form of WordElement
 * @param category - category of WordElement
 * @param id - ID of word in lexicon
 */
public WordElement(String baseForm,LexicalCategory category,String id){
  super();
  this.baseForm=baseForm;
  setCategory(category);
  this.id=id;
  this.inflVars=new HashMap<Inflection,InflectionSet>();
}","/** 
 * creates a duplicate WordElement from an existing WordElement
 * @param currentWord - An existing WordElement
 */
public WordElement(WordElement currentWord){
  super();
  this.baseForm=currentWord.getBaseForm();
  setCategory(currentWord.getCategory());
  this.id=currentWord.getId();
  this.inflVars=currentWord.getInflectionalVariants();
  this.defaultInfl=(Inflection)currentWord.getDefaultInflectionalVariant();
  setFeatures(currentWord);
}"
4014,"/** 
 * get matching keys from an index map
 * @param indexKey
 * @param category
 * @param indexMap
 * @return
 */
private List<WordElement> getWordsFromIndex(String indexKey,LexicalCategory category,Map<String,List<WordElement>> indexMap){
  List<WordElement> result=new ArrayList<WordElement>();
  if (!indexMap.containsKey(indexKey))   return result;
  if (category == LexicalCategory.ANY)   return indexMap.get(indexKey);
 else   for (  WordElement word : indexMap.get(indexKey))   if (word.getCategory() == category)   result.add(word);
  return result;
}","/** 
 * get matching keys from an index map
 * @param indexKey
 * @param category
 * @param indexMap
 * @return
 */
private List<WordElement> getWordsFromIndex(String indexKey,LexicalCategory category,Map<String,List<WordElement>> indexMap){
  List<WordElement> result=new ArrayList<WordElement>();
  if (!indexMap.containsKey(indexKey)) {
    return result;
  }
  if (category == LexicalCategory.ANY) {
    for (    WordElement word : indexMap.get(indexKey)) {
      result.add(new WordElement(word));
    }
    return result;
  }
 else {
    for (    WordElement word : indexMap.get(indexKey)) {
      if (word.getCategory() == category) {
        result.add(new WordElement(word));
      }
    }
  }
  return result;
}"
4015,"/** 
 * create a simplenlg WordElement from a Word node in a lexicon XML file
 * @param wordNode
 * @return
 * @throws XPathUtilException
 */
private WordElement convertNodeToWord(Node wordNode){
  if (!wordNode.getNodeName().equalsIgnoreCase(XML_WORD))   return null;
  WordElement word=new WordElement();
  List<Inflection> inflections=new ArrayList<Inflection>();
  NodeList nodes=wordNode.getChildNodes();
  for (int i=0; i < nodes.getLength(); i++) {
    Node featureNode=nodes.item(i);
    if (featureNode.getNodeType() == Node.ELEMENT_NODE) {
      String feature=featureNode.getNodeName().trim();
      String value=featureNode.getTextContent();
      if (value != null)       value=value.trim();
      if (feature == null) {
        System.out.println(""String_Node_Str"" + word.toString());
        break;
      }
      if (feature.equalsIgnoreCase(XML_BASE)) {
        word.setBaseForm(value);
      }
 else       if (feature.equalsIgnoreCase(XML_CATEGORY))       word.setCategory(LexicalCategory.valueOf(value.toUpperCase()));
 else       if (feature.equalsIgnoreCase(XML_ID))       word.setId(value);
 else       if (value == null || value.equals(""String_Node_Str"")) {
        Inflection infl=Inflection.getInflCode(feature);
        if (infl != null) {
          inflections.add(infl);
        }
 else {
          word.setFeature(feature,true);
        }
      }
 else       word.setFeature(feature,value);
    }
  }
  if (inflections.isEmpty()) {
    inflections.add(Inflection.REGULAR);
  }
  Inflection defaultInfl=inflections.contains(Inflection.REGULAR) ? Inflection.REGULAR : inflections.get(0);
  word.setFeature(LexicalFeature.DEFAULT_INFL,defaultInfl);
  word.setDefaultInflectionalVariant(defaultInfl);
  for (  Inflection infl : inflections) {
    word.addInflectionalVariant(infl);
  }
  return word;
}","/** 
 * create a simplenlg WordElement from a Word node in a lexicon XML file
 * @param wordNode
 * @return
 * @throws XPathUtilException
 */
private WordElement convertNodeToWord(Node wordNode){
  if (!wordNode.getNodeName().equalsIgnoreCase(XML_WORD))   return null;
  WordElement word=new WordElement();
  List<Inflection> inflections=new ArrayList<Inflection>();
  NodeList nodes=wordNode.getChildNodes();
  for (int i=0; i < nodes.getLength(); i++) {
    Node featureNode=nodes.item(i);
    if (featureNode.getNodeType() == Node.ELEMENT_NODE) {
      String feature=featureNode.getNodeName().trim();
      String value=featureNode.getTextContent();
      if (value != null)       value=value.trim();
      if (feature == null) {
        System.err.println(""String_Node_Str"" + word.toString());
        break;
      }
      if (feature.equalsIgnoreCase(XML_BASE)) {
        word.setBaseForm(value);
      }
 else       if (feature.equalsIgnoreCase(XML_CATEGORY))       word.setCategory(LexicalCategory.valueOf(value.toUpperCase()));
 else       if (feature.equalsIgnoreCase(XML_ID))       word.setId(value);
 else       if (value == null || value.equals(""String_Node_Str"")) {
        Inflection infl=Inflection.getInflCode(feature);
        if (infl != null) {
          inflections.add(infl);
        }
 else {
          word.setFeature(feature,true);
        }
      }
 else       word.setFeature(feature,value);
    }
  }
  if (inflections.isEmpty()) {
    inflections.add(Inflection.REGULAR);
  }
  Inflection defaultInfl=inflections.contains(Inflection.REGULAR) ? Inflection.REGULAR : inflections.get(0);
  word.setFeature(LexicalFeature.DEFAULT_INFL,defaultInfl);
  word.setDefaultInflectionalVariant(defaultInfl);
  for (  Inflection infl : inflections) {
    word.addInflectionalVariant(infl);
  }
  return word;
}"
4016,"@Override public List<WordElement> getWordsByID(String id){
  List<WordElement> result=new ArrayList<WordElement>();
  if (indexByID.containsKey(id))   result.add(indexByID.get(id));
  return result;
}","@Override public List<WordElement> getWordsByID(String id){
  List<WordElement> result=new ArrayList<WordElement>();
  if (indexByID.containsKey(id)) {
    result.add(new WordElement(indexByID.get(id)));
  }
  return result;
}"
4017,"@Override public List<NLGElement> realise(List<NLGElement> elements){
  return null;
}","@Override public List<NLGElement> realise(List<NLGElement> elements){
  List<NLGElement> realisedElements=new ArrayList<NLGElement>();
  if (null != elements) {
    for (    NLGElement element : elements) {
      NLGElement realisedElement=realise(element);
      realisedElements.add(realisedElement);
    }
  }
  return realisedElements;
}"
4018,"/** 
 * Realises the specifier of the noun phrase.
 * @param phrase the <code>PhraseElement</code> representing this noun phrase.
 * @param parent the parent <code>SyntaxProcessor</code> that will do the realisation of the complementiser.
 * @param realisedElement the current realisation of the noun phrase.
 */
private static void realiseSpecifier(PhraseElement phrase,SyntaxProcessor parent,ListElement realisedElement){
  NLGElement specifierElement=phrase.getFeatureAsElement(InternalFeature.SPECIFIER);
  if (specifierElement != null && !phrase.getFeatureAsBoolean(InternalFeature.RAISED).booleanValue() && !phrase.getFeatureAsBoolean(Feature.ELIDED).booleanValue()) {
    if (!specifierElement.isA(LexicalCategory.PRONOUN)) {
      specifierElement.setFeature(Feature.NUMBER,phrase.getFeature(Feature.NUMBER));
    }
    NLGElement currentElement=parent.realise(specifierElement);
    if (currentElement != null) {
      currentElement.setFeature(InternalFeature.DISCOURSE_FUNCTION,DiscourseFunction.SPECIFIER);
      realisedElement.addComponent(currentElement);
    }
  }
}","/** 
 * Realises the specifier of the noun phrase.
 * @param phrase the <code>PhraseElement</code> representing this noun phrase.
 * @param parent the parent <code>SyntaxProcessor</code> that will do the realisation of the complementiser.
 * @param realisedElement the current realisation of the noun phrase.
 */
private static void realiseSpecifier(PhraseElement phrase,SyntaxProcessor parent,ListElement realisedElement){
  NLGElement specifierElement=phrase.getFeatureAsElement(InternalFeature.SPECIFIER);
  if (specifierElement != null && !phrase.getFeatureAsBoolean(InternalFeature.RAISED).booleanValue() && !phrase.getFeatureAsBoolean(Feature.ELIDED).booleanValue()) {
    if (!specifierElement.isA(LexicalCategory.PRONOUN) && specifierElement.getCategory() != PhraseCategory.NOUN_PHRASE) {
      specifierElement.setFeature(Feature.NUMBER,phrase.getFeature(Feature.NUMBER));
    }
    NLGElement currentElement=parent.realise(specifierElement);
    if (currentElement != null) {
      currentElement.setFeature(InternalFeature.DISCOURSE_FUNCTION,DiscourseFunction.SPECIFIER);
      realisedElement.addComponent(currentElement);
    }
  }
}"
4019,"/** 
 * Write recording.
 * @param record the record
 * @param os the os
 * @throws JAXBException the jAXB exception
 * @throws IOException Signals that an I/O exception has occurred.
 * @throws TransformerException the transformer exception
 */
public static void writeRecording(RecordSet record,OutputStream os) throws JAXBException, IOException, TransformerException {
  JAXBContext jc;
  jc=JAXBContext.newInstance(simplenlg.xmlrealiser.wrapper.NLGSpec.class);
  Marshaller m=jc.createMarshaller();
  m.setProperty(""String_Node_Str"",new RecordingNamespacePrefixMapper());
  NLGSpec nlg=new NLGSpec();
  nlg.setRecording(record);
  StringWriter osTemp=new StringWriter();
  m.marshal(nlg,osTemp);
  Source xmlInput=new StreamSource(new StringReader(osTemp.toString()));
  StreamResult xmlOutput=new StreamResult(new OutputStreamWriter(os,""String_Node_Str""));
  Transformer transformer=TransformerFactory.newInstance().newTransformer();
  if (transformer != null) {
    transformer.setOutputProperty(OutputKeys.INDENT,""String_Node_Str"");
    transformer.setOutputProperty(""String_Node_Str"",""String_Node_Str"");
    transformer.transform(xmlInput,xmlOutput);
  }
}","/** 
 * Write recording.
 * @param record the record
 * @param os the os
 * @throws JAXBException the jAXB exception
 * @throws IOException Signals that an I/O exception has occurred.
 * @throws TransformerException the transformer exception
 */
public static void writeRecording(RecordSet record,OutputStream os) throws JAXBException, IOException, TransformerException {
  JAXBContext jc;
  jc=JAXBContext.newInstance(simplenlg.xmlrealiser.wrapper.NLGSpec.class);
  Marshaller m=jc.createMarshaller();
  NLGSpec nlg=new NLGSpec();
  nlg.setRecording(record);
  StringWriter osTemp=new StringWriter();
  m.marshal(nlg,osTemp);
  Source xmlInput=new StreamSource(new StringReader(osTemp.toString()));
  StreamResult xmlOutput=new StreamResult(new OutputStreamWriter(os,""String_Node_Str""));
  Transformer transformer=TransformerFactory.newInstance().newTransformer();
  if (transformer != null) {
    transformer.setOutputProperty(OutputKeys.INDENT,""String_Node_Str"");
    transformer.setOutputProperty(""String_Node_Str"",""String_Node_Str"");
    transformer.transform(xmlInput,xmlOutput);
  }
}"
4020,"/** 
 * Called when the HTML textbox is moved. It moves the edittext as well
 * @param frect the form rect
 * @param trect text textbox rect
 */
protected void replace(Rect frect,Rect trect){
  RelativeLayout.LayoutParams rparams;
  if (bpos.update(frect,trect)) {
    Utils utils=new Utils(wav);
    rparams=(RelativeLayout.LayoutParams)divw.getLayoutParams();
    rparams.topMargin=frect.top + (int)utils.pxFromDp(8);
    rparams.leftMargin=frect.left + (int)utils.pxFromDp(8);
    rparams.width=LayoutParams.MATCH_PARENT;
    divw.setLayoutParams(rparams);
  }
  if (bpos.shallShow()) {
    divw.setVisibility(View.VISIBLE);
    ew.requestFocus();
    if (hwkeyb)     imm.hideSoftInputFromWindow(ew.getWindowToken(),0);
  }
}","/** 
 * Called when the HTML textbox is moved. It moves the edittext as well
 * @param frect the form rect
 * @param trect text textbox rect
 */
protected void replace(Rect frect,Rect trect){
  RelativeLayout.LayoutParams rparams;
  if (bpos.update(frect,trect)) {
    Utils utils=new Utils(wav);
    rparams=(RelativeLayout.LayoutParams)divw.getLayoutParams();
    rparams.topMargin=frect.top + (int)utils.pxFromDp(8);
    rparams.leftMargin=frect.left + (int)utils.pxFromDp(8);
    rparams.width=LayoutParams.MATCH_PARENT;
    divw.setLayoutParams(rparams);
  }
  if (bpos.shallShow()) {
    divw.setVisibility(View.VISIBLE);
    wv.loadUrl(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"");
    if (hwkeyb)     imm.hideSoftInputFromWindow(ew.getWindowToken(),0);
  }
}"
4021,"public void run(){
  if (updateSequence(sequence))   showQuestion(type,name,rect,size);
}","@Override public void run(){
  ew.requestFocus();
}"
4022,"@Override public void onStop(){
  saveOfflineValues();
  super.onStop();
}","@Override public void onStop(){
  super.onStop();
}"
4023,"@Override public View onCreateView(LayoutInflater paramLayoutInflater,ViewGroup paramViewGroup,Bundle paramBundle){
  rootView=paramLayoutInflater.inflate(R.layout.fragment_dashboard,paramViewGroup,false);
  activity=getActivity();
  mPullToRefreshLayout=(PullToRefreshLayout)rootView.findViewById(R.id.dashboard_pull_to_refresh);
  ActionBarPullToRefresh.from(getActivity()).allChildrenArePullable().listener(this).setup(mPullToRefreshLayout);
  mAvailableHolder=(LinearLayout)rootView.findViewById(R.id.fragment_dashboard_available_holder);
  mReviewsHolder=(LinearLayout)rootView.findViewById(R.id.fragment_dashboard_reviews_holder);
  mRecentUnlocksFragmentHolder=(LinearLayout)rootView.findViewById(R.id.fragment_dashboard_recent_unlocks_holder);
  mCriticalItemsFragmentHolder=(LinearLayout)rootView.findViewById(R.id.fragment_dashboard_critical_items_holder);
  mPullToRefreshLayout.setRefreshing(true);
  Intent intent=new Intent(BroadcastIntents.SYNC());
  LocalBroadcastManager.getInstance(getActivity()).sendBroadcast(intent);
  return rootView;
}","@Override public View onCreateView(LayoutInflater paramLayoutInflater,ViewGroup paramViewGroup,Bundle paramBundle){
  rootView=paramLayoutInflater.inflate(R.layout.fragment_dashboard,paramViewGroup,false);
  activity=getActivity();
  mPullToRefreshLayout=(PullToRefreshLayout)rootView.findViewById(R.id.dashboard_pull_to_refresh);
  ActionBarPullToRefresh.from(getActivity()).allChildrenArePullable().listener(this).setup(mPullToRefreshLayout);
  mAvailableHolder=(LinearLayout)rootView.findViewById(R.id.fragment_dashboard_available_holder);
  mReviewsHolder=(LinearLayout)rootView.findViewById(R.id.fragment_dashboard_reviews_holder);
  mRecentUnlocksFragmentHolder=(LinearLayout)rootView.findViewById(R.id.fragment_dashboard_recent_unlocks_holder);
  mCriticalItemsFragmentHolder=(LinearLayout)rootView.findViewById(R.id.fragment_dashboard_critical_items_holder);
  if (!MainActivity.isFirstSyncDashboardDone) {
    mPullToRefreshLayout.setRefreshing(true);
    Intent intent=new Intent(BroadcastIntents.SYNC());
    LocalBroadcastManager.getInstance(getActivity()).sendBroadcast(intent);
    MainActivity.isFirstSyncDashboardDone=true;
  }
  return rootView;
}"
4024,"@Override public View onCreateView(LayoutInflater inflater,ViewGroup container,Bundle savedInstanceState){
  View rootView=inflater.inflate(R.layout.fragment_profile,container,false);
  context=getActivity();
  api=new WaniKaniApi(getActivity());
  dataMan=new OfflineDataManager(getActivity());
  prefMan=new PrefManager(getActivity());
  mAvatar=(ImageView)rootView.findViewById(R.id.profile_avatar);
  mUsername=(TextView)rootView.findViewById(R.id.profile_username);
  mTitle=(TextView)rootView.findViewById(R.id.profile_title);
  mLevel=(TextView)rootView.findViewById(R.id.profile_level);
  mTopicsCount=(TextView)rootView.findViewById(R.id.profile_topics_count);
  mPostsCount=(TextView)rootView.findViewById(R.id.profile_posts_count);
  mCreationDate=(TextView)rootView.findViewById(R.id.profile_creation_date);
  mAbout=(TextView)rootView.findViewById(R.id.profile_about);
  mWebsite=(TextView)rootView.findViewById(R.id.profile_website);
  mTwitter=(TextView)rootView.findViewById(R.id.profile_twitter);
  mAboutHolder=(LinearLayout)rootView.findViewById(R.id.profile_about_holder);
  mWebsiteHolder=(RelativeLayout)rootView.findViewById(R.id.profile_website_holder);
  mTwitterHolder=(RelativeLayout)rootView.findViewById(R.id.profile_twitter_holder);
  mViewFlipper=(ViewFlipper)rootView.findViewById(R.id.profile_view_flipper);
  mViewFlipper.setInAnimation(getActivity(),R.anim.abc_fade_in);
  mViewFlipper.setInAnimation(getActivity(),R.anim.abc_fade_out);
  mPullToRefreshLayout=(PullToRefreshLayout)rootView.findViewById(R.id.dashboard_pull_to_refresh);
  ActionBarPullToRefresh.from(getActivity()).allChildrenArePullable().listener(this).setup(mPullToRefreshLayout);
  if (prefMan.isProfileFirstTime()) {
    if (mViewFlipper.getDisplayedChild() == 0) {
      mViewFlipper.showNext();
    }
  }
  setOldValues();
  mPullToRefreshLayout.setRefreshing(true);
  new LoadTask().execute();
  return rootView;
}","@Override public View onCreateView(LayoutInflater inflater,ViewGroup container,Bundle savedInstanceState){
  View rootView=inflater.inflate(R.layout.fragment_profile,container,false);
  context=getActivity();
  api=new WaniKaniApi(getActivity());
  dataMan=new OfflineDataManager(getActivity());
  prefMan=new PrefManager(getActivity());
  mAvatar=(ImageView)rootView.findViewById(R.id.profile_avatar);
  mUsername=(TextView)rootView.findViewById(R.id.profile_username);
  mTitle=(TextView)rootView.findViewById(R.id.profile_title);
  mLevel=(TextView)rootView.findViewById(R.id.profile_level);
  mTopicsCount=(TextView)rootView.findViewById(R.id.profile_topics_count);
  mPostsCount=(TextView)rootView.findViewById(R.id.profile_posts_count);
  mCreationDate=(TextView)rootView.findViewById(R.id.profile_creation_date);
  mAbout=(TextView)rootView.findViewById(R.id.profile_about);
  mWebsite=(TextView)rootView.findViewById(R.id.profile_website);
  mTwitter=(TextView)rootView.findViewById(R.id.profile_twitter);
  mAboutHolder=(LinearLayout)rootView.findViewById(R.id.profile_about_holder);
  mWebsiteHolder=(RelativeLayout)rootView.findViewById(R.id.profile_website_holder);
  mTwitterHolder=(RelativeLayout)rootView.findViewById(R.id.profile_twitter_holder);
  mViewFlipper=(ViewFlipper)rootView.findViewById(R.id.profile_view_flipper);
  mViewFlipper.setInAnimation(getActivity(),R.anim.abc_fade_in);
  mViewFlipper.setInAnimation(getActivity(),R.anim.abc_fade_out);
  mPullToRefreshLayout=(PullToRefreshLayout)rootView.findViewById(R.id.dashboard_pull_to_refresh);
  ActionBarPullToRefresh.from(getActivity()).allChildrenArePullable().listener(this).setup(mPullToRefreshLayout);
  if (prefMan.isProfileFirstTime()) {
    if (mViewFlipper.getDisplayedChild() == 0) {
      mViewFlipper.showNext();
    }
  }
  setOldValues();
  if (!MainActivity.isFirstSyncProfileDone) {
    mPullToRefreshLayout.setRefreshing(true);
    new LoadTask().execute();
    MainActivity.isFirstSyncProfileDone=true;
  }
  return rootView;
}"
4025,"private void triggerBlockManagerWrite(){
  if (_numFetchToBuffer < _kafkaconfig._numFetchToBuffer) {
    return;
  }
  LOG.debug(""String_Node_Str"",_partition.partition);
  if ((_lastEnquedOffset >= _lastComittedOffset) && (_waitingToEmit.isEmpty())) {
    try {
synchronized (_receiver) {
        if (!_arrayBuffer.isEmpty() && !_receiver.isStopped()) {
          _receiver.store(_arrayBuffer.iterator());
          _arrayBuffer.clear();
        }
        commit();
        _numFetchToBuffer=1;
      }
    }
 catch (    Exception ex) {
      _emittedToOffset=_lastComittedOffset;
      _arrayBuffer.clear();
      if (ex instanceof InterruptedException) {
        throw ex;
      }
 else {
        _receiver.reportError(""String_Node_Str"" + _partition,ex);
      }
    }
  }
}","private void triggerBlockManagerWrite(){
  if (_numFetchToBuffer < _kafkaconfig._numFetchToBuffer) {
    return;
  }
  LOG.debug(""String_Node_Str"",_partition.partition);
  if ((_lastEnquedOffset >= _lastComittedOffset) && (_waitingToEmit.isEmpty())) {
    try {
synchronized (_receiver) {
        if (!_arrayBuffer.isEmpty() && !_receiver.isStopped()) {
          _receiver.store(_arrayBuffer.iterator());
          _arrayBuffer.clear();
          commit();
          _numFetchToBuffer=1;
        }
      }
    }
 catch (    Exception ex) {
      _emittedToOffset=_lastComittedOffset;
      _arrayBuffer.clear();
      if (ex instanceof InterruptedException) {
        throw ex;
      }
 else {
        _receiver.reportError(""String_Node_Str"" + _partition,ex);
      }
    }
  }
}"
4026,"public void commit(){
  LOG.debug(""String_Node_Str"",_lastComittedOffset);
  LOG.debug(""String_Node_Str"",_emittedToOffset);
  LOG.debug(""String_Node_Str"",_lastEnquedOffset);
  if (_lastEnquedOffset > _lastComittedOffset) {
    LOG.debug(""String_Node_Str"",_partition);
    Map<Object,Object> data=(Map<Object,Object>)ImmutableMap.builder().put(""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",_consumerId)).put(""String_Node_Str"",_emittedToOffset).put(""String_Node_Str"",_partition.partition).put(""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",_partition.host.host,""String_Node_Str"",_partition.host.port)).put(""String_Node_Str"",_topic).build();
    try {
      _state.writeJSON(committedPath(),data);
      LOG.debug(""String_Node_Str"" + _emittedToOffset);
      _waitingToEmit.clear();
      _lastComittedOffset=_emittedToOffset;
    }
 catch (    Exception zkEx) {
      LOG.error(""String_Node_Str"",zkEx);
    }
    LOG.debug(""String_Node_Str"",_lastComittedOffset,_partition,_consumerId);
  }
 else {
    LOG.debug(""String_Node_Str"" + ""String_Node_Str"",_lastEnquedOffset,_lastComittedOffset,_partition,_consumerId);
  }
}","public void commit(){
  LOG.debug(""String_Node_Str"",_lastComittedOffset);
  LOG.debug(""String_Node_Str"",_emittedToOffset);
  LOG.debug(""String_Node_Str"",_lastEnquedOffset);
  if (_lastEnquedOffset >= _lastComittedOffset) {
    LOG.debug(""String_Node_Str"",_partition);
    Map<Object,Object> data=(Map<Object,Object>)ImmutableMap.builder().put(""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",_consumerId)).put(""String_Node_Str"",_emittedToOffset).put(""String_Node_Str"",_partition.partition).put(""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",_partition.host.host,""String_Node_Str"",_partition.host.port)).put(""String_Node_Str"",_topic).build();
    try {
      _state.writeJSON(committedPath(),data);
      LOG.debug(""String_Node_Str"" + _emittedToOffset);
      _waitingToEmit.clear();
      _lastComittedOffset=_emittedToOffset;
    }
 catch (    Exception zkEx) {
      LOG.error(""String_Node_Str"",zkEx);
    }
    LOG.debug(""String_Node_Str"",_lastComittedOffset,_partition,_consumerId);
  }
 else {
    LOG.debug(""String_Node_Str"" + ""String_Node_Str"",_lastEnquedOffset,_lastComittedOffset,_partition,_consumerId);
  }
}"
4027,"public KafkaConsumer(KafkaConfig blurConfig,ZkState zkState,Receiver receiver){
  _kafkablurconfig=blurConfig;
  _state=zkState;
  _receiver=receiver;
}","public KafkaConsumer(KafkaConfig config,ZkState zkState,Receiver receiver){
  _kafkaconfig=config;
  _state=zkState;
  _receiver=receiver;
}"
4028,"@Override public void run(){
  try {
    while (!_receiver.isStopped()) {
      this.createStream();
    }
  }
 catch (  Throwable t) {
    this.close();
    throw new RuntimeException(t);
  }
}","@Override public void run(){
  try {
    while (!_receiver.isStopped()) {
      if ((System.currentTimeMillis() - _lastConsumeTime) > _kafkaconfig._fillFreqMs) {
        this.createStream();
        _lastConsumeTime=System.currentTimeMillis();
      }
 else {
        Thread.sleep(_kafkaconfig._fillFreqMs);
      }
    }
  }
 catch (  Throwable t) {
    this.close();
    throw new RuntimeException(t);
  }
}"
4029,"public void open(int partitionId){
  _currPartitionIndex=partitionId;
  _connections=new DynamicPartitionConnections(_kafkablurconfig,new ZkBrokerReader(_kafkablurconfig,_state));
  _coordinator=new ZkCoordinator(_connections,_kafkablurconfig,_state,partitionId,_receiver,true);
}","public void open(int partitionId){
  _currPartitionIndex=partitionId;
  _connections=new DynamicPartitionConnections(_kafkaconfig,new ZkBrokerReader(_kafkaconfig,_state));
  _coordinator=new ZkCoordinator(_connections,_kafkaconfig,_state,partitionId,_receiver,true);
}"
4030,"public void next(){
  if (_waitingToEmit.isEmpty()) {
    if (_lastFillTime == null || (System.currentTimeMillis() - _lastFillTime) > _kafkaconfig._fillFreqMs) {
      LOG.info(""String_Node_Str"" + _topic + ""String_Node_Str""+ _partition.partition+ ""String_Node_Str""+ _kafkaconfig._fillFreqMs+ ""String_Node_Str"");
      fill();
      _lastFillTime=System.currentTimeMillis();
    }
  }
  while (true) {
    MessageAndOffset msgAndOffset=_waitingToEmit.pollFirst();
    if (msgAndOffset != null) {
      Long key=msgAndOffset.offset();
      Message msg=msgAndOffset.message();
      try {
        _lastEnquedOffset=key;
        if (_lastEnquedOffset >= _lastComittedOffset) {
          if (msg.payload() != null) {
            MessageAndMetadata mmeta=new MessageAndMetadata();
            mmeta.setTopic(_topic);
            mmeta.setConsumer(_ConsumerId);
            mmeta.setOffset(_lastEnquedOffset);
            mmeta.setPartition(_partition);
            byte[] payload=new byte[msg.payload().remaining()];
            msg.payload().get(payload);
            mmeta.setPayload(payload);
            if (msg.hasKey()) {
              byte[] msgKey=new byte[msg.key().remaining()];
              msg.key().get(msgKey);
              mmeta.setKey(msgKey);
            }
            _dataBuffer.add(mmeta);
            LOG.info(""String_Node_Str"" + _topic + ""String_Node_Str""+ _partition.partition+ ""String_Node_Str""+ _lastEnquedOffset);
          }
        }
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"" + key + ""String_Node_Str""+ _partition+ ""String_Node_Str""+ _topic+ ""String_Node_Str""+ e.getMessage());
        e.printStackTrace();
      }
    }
 else {
      break;
    }
  }
  if ((_lastEnquedOffset >= _lastComittedOffset) && (_waitingToEmit.isEmpty())) {
    if (_dataBuffer.size() > 0) {
      try {
synchronized (_receiver) {
          _receiver.store(_dataBuffer.iterator());
          commit();
          _dataBuffer.clear();
        }
      }
 catch (      Exception ex) {
        _emittedToOffset=_lastComittedOffset;
        _dataBuffer.clear();
        _receiver.reportError(""String_Node_Str"" + _partition,ex);
      }
    }
  }
}","public void next(){
  if (_waitingToEmit.isEmpty()) {
    fill();
  }
  while (true) {
    MessageAndOffset msgAndOffset=_waitingToEmit.pollFirst();
    if (msgAndOffset != null) {
      Long key=msgAndOffset.offset();
      Message msg=msgAndOffset.message();
      try {
        _lastEnquedOffset=key;
        if (_lastEnquedOffset >= _lastComittedOffset) {
          if (msg.payload() != null) {
            MessageAndMetadata mmeta=new MessageAndMetadata();
            mmeta.setTopic(_topic);
            mmeta.setConsumer(_ConsumerId);
            mmeta.setOffset(_lastEnquedOffset);
            mmeta.setPartition(_partition);
            byte[] payload=new byte[msg.payload().remaining()];
            msg.payload().get(payload);
            mmeta.setPayload(payload);
            if (msg.hasKey()) {
              byte[] msgKey=new byte[msg.key().remaining()];
              msg.key().get(msgKey);
              mmeta.setKey(msgKey);
            }
            _dataBuffer.add(mmeta);
            LOG.info(""String_Node_Str"" + _topic + ""String_Node_Str""+ _partition.partition+ ""String_Node_Str""+ _lastEnquedOffset);
          }
        }
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"" + key + ""String_Node_Str""+ _partition+ ""String_Node_Str""+ _topic+ ""String_Node_Str""+ e.getMessage());
        e.printStackTrace();
      }
    }
 else {
      break;
    }
  }
  if ((_lastEnquedOffset >= _lastComittedOffset) && (_waitingToEmit.isEmpty())) {
    try {
synchronized (_receiver) {
        if (!_dataBuffer.isEmpty())         _receiver.store(_dataBuffer.iterator());
        commit();
        _dataBuffer.clear();
      }
    }
 catch (    Exception ex) {
      _emittedToOffset=_lastComittedOffset;
      _dataBuffer.clear();
      _receiver.reportError(""String_Node_Str"" + _partition,ex);
    }
  }
}"
4031,"public void writeJSON(String path,Map<Object,Object> data){
  LOG.info(""String_Node_Str"" + path + ""String_Node_Str""+ data.toString());
  writeBytes(path,JSONValue.toJSONString(data).getBytes(Charset.forName(""String_Node_Str"")));
}","public synchronized void writeJSON(String path,Map<Object,Object> data){
  LOG.info(""String_Node_Str"" + path + ""String_Node_Str""+ data.toString());
  writeBytes(path,JSONValue.toJSONString(data).getBytes(Charset.forName(""String_Node_Str"")));
}"
4032,"private void run(){
  Properties props=new Properties();
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  SparkConf _sparkConf=new SparkConf().setAppName(""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"");
  ;
  JavaStreamingContext jsc=new JavaStreamingContext(_sparkConf,new Duration(1000));
  int numberOfReceivers=3;
  JavaDStream<MessageAndMetadata> unionStreams=ReceiverLauncher.launch(jsc,props,numberOfReceivers,StorageLevel.MEMORY_ONLY());
  unionStreams.foreachRDD(new Function2<JavaRDD<MessageAndMetadata>,Time,Void>(){
    @Override public Void call(    JavaRDD<MessageAndMetadata> rdd,    Time time) throws Exception {
      rdd.collect();
      System.out.println(""String_Node_Str"" + rdd.count());
      return null;
    }
  }
);
  jsc.start();
  jsc.awaitTermination();
}","private void run(){
  Properties props=new Properties();
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  props.put(""String_Node_Str"",""String_Node_Str"");
  SparkConf _sparkConf=new SparkConf().setAppName(""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"");
  ;
  JavaStreamingContext jsc=new JavaStreamingContext(_sparkConf,new Duration(1000));
  int numberOfReceivers=1;
  JavaDStream<MessageAndMetadata> unionStreams=ReceiverLauncher.launch(jsc,props,numberOfReceivers,StorageLevel.MEMORY_ONLY());
  unionStreams.foreachRDD(new Function2<JavaRDD<MessageAndMetadata>,Time,Void>(){
    @Override public Void call(    JavaRDD<MessageAndMetadata> rdd,    Time time) throws Exception {
      rdd.collect();
      System.out.println(""String_Node_Str"" + rdd.count());
      return null;
    }
  }
);
  jsc.start();
  jsc.awaitTermination();
}"
4033,"public void start(){
  _executorService=Executors.newFixedThreadPool(_partitionSet.size());
  for (  Integer partitionId : _partitionSet) {
    KafkaConfig kafkaConfig=new KafkaConfig(_props);
    ZkState zkState=new ZkState(kafkaConfig);
    _kConsumer=new KafkaConsumer(kafkaConfig,zkState,this);
    _kConsumer.open(partitionId);
    Thread.UncaughtExceptionHandler eh=new Thread.UncaughtExceptionHandler(){
      public void uncaughtException(      Thread th,      Throwable ex){
        restart(""String_Node_Str"",ex,5000);
      }
    }
;
    _consumerThread=new Thread(_kConsumer);
    _consumerThread.setDaemon(true);
    _consumerThread.setUncaughtExceptionHandler(eh);
    _executorService.submit(_consumerThread);
  }
}","public void start(){
  _threadList.clear();
  KafkaConfig kafkaConfig=new KafkaConfig(_props);
  ZkState zkState=new ZkState(kafkaConfig);
  for (  Integer partitionId : _partitionSet) {
    _kConsumer=new KafkaConsumer(kafkaConfig,zkState,this);
    _kConsumer.open(partitionId);
    Thread.UncaughtExceptionHandler eh=new Thread.UncaughtExceptionHandler(){
      public void uncaughtException(      Thread th,      Throwable ex){
        restart(""String_Node_Str"",ex,5000);
      }
    }
;
    _consumerThread=new Thread(_kConsumer);
    _consumerThread.setDaemon(true);
    _consumerThread.setUncaughtExceptionHandler(eh);
    _threadList.add(_consumerThread);
    _consumerThread.start();
  }
}"
4034,"@Override public void onStop(){
  _executorService.shutdown();
}","@Override public void onStop(){
  for (  Thread t : _threadList) {
    if (t.isAlive())     t.interrupt();
  }
}"
4035,"public void next(){
  if (_waitingToEmit.isEmpty()) {
    if (_lastFillTime == null || (System.currentTimeMillis() - _lastFillTime) > _kafkaconfig._fillFreqMs) {
      LOG.info(""String_Node_Str"" + _topic + ""String_Node_Str""+ _partition.partition+ ""String_Node_Str""+ _kafkaconfig._fillFreqMs+ ""String_Node_Str"");
      fill();
      _lastFillTime=System.currentTimeMillis();
    }
  }
  while (true) {
    MessageAndOffset msgAndOffset=_waitingToEmit.pollFirst();
    if (msgAndOffset != null) {
      Long key=msgAndOffset.offset();
      Message msg=msgAndOffset.message();
      try {
        _lastEnquedOffset=key;
        if (_lastEnquedOffset >= _lastComittedOffset) {
          if (msg.payload() != null) {
            MessageAndMetadata mmeta=new MessageAndMetadata();
            mmeta.setTopic(_topic);
            mmeta.setConsumer(_ConsumerId);
            mmeta.setOffset(_lastEnquedOffset);
            mmeta.setPartition(_partition);
            byte[] payload=new byte[msg.payload().remaining()];
            msg.payload().get(payload);
            mmeta.setPayload(payload);
            if (msg.hasKey())             mmeta.setKey(msg.key().array());
            _dataBuffer.add(mmeta);
            LOG.info(""String_Node_Str"" + _topic + ""String_Node_Str""+ _partition.partition+ ""String_Node_Str""+ _lastEnquedOffset);
          }
        }
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"" + key + ""String_Node_Str""+ _partition+ ""String_Node_Str""+ _topic+ ""String_Node_Str""+ e.getMessage());
        e.printStackTrace();
      }
    }
 else {
      break;
    }
  }
  if ((_lastEnquedOffset > _lastComittedOffset) && (_waitingToEmit.isEmpty())) {
    if (_dataBuffer.size() > 0) {
      try {
synchronized (_receiver) {
          _receiver.store(_dataBuffer.iterator());
          commit();
          _dataBuffer.clear();
        }
      }
 catch (      Exception ex) {
        _emittedToOffset=_lastComittedOffset;
        _dataBuffer.clear();
        _receiver.reportError(""String_Node_Str"" + _partition,ex);
      }
    }
  }
}","public void next(){
  if (_waitingToEmit.isEmpty()) {
    if (_lastFillTime == null || (System.currentTimeMillis() - _lastFillTime) > _kafkaconfig._fillFreqMs) {
      LOG.info(""String_Node_Str"" + _topic + ""String_Node_Str""+ _partition.partition+ ""String_Node_Str""+ _kafkaconfig._fillFreqMs+ ""String_Node_Str"");
      fill();
      _lastFillTime=System.currentTimeMillis();
    }
  }
  while (true) {
    MessageAndOffset msgAndOffset=_waitingToEmit.pollFirst();
    if (msgAndOffset != null) {
      Long key=msgAndOffset.offset();
      Message msg=msgAndOffset.message();
      try {
        _lastEnquedOffset=key;
        if (_lastEnquedOffset >= _lastComittedOffset) {
          if (msg.payload() != null) {
            MessageAndMetadata mmeta=new MessageAndMetadata();
            mmeta.setTopic(_topic);
            mmeta.setConsumer(_ConsumerId);
            mmeta.setOffset(_lastEnquedOffset);
            mmeta.setPartition(_partition);
            byte[] payload=new byte[msg.payload().remaining()];
            msg.payload().get(payload);
            mmeta.setPayload(payload);
            if (msg.hasKey()) {
              byte[] msgKey=new byte[msg.key().remaining()];
              msg.key().get(msgKey);
              mmeta.setKey(msgKey);
            }
            _dataBuffer.add(mmeta);
            LOG.info(""String_Node_Str"" + _topic + ""String_Node_Str""+ _partition.partition+ ""String_Node_Str""+ _lastEnquedOffset);
          }
        }
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"" + key + ""String_Node_Str""+ _partition+ ""String_Node_Str""+ _topic+ ""String_Node_Str""+ e.getMessage());
        e.printStackTrace();
      }
    }
 else {
      break;
    }
  }
  if ((_lastEnquedOffset > _lastComittedOffset) && (_waitingToEmit.isEmpty())) {
    if (_dataBuffer.size() > 0) {
      try {
synchronized (_receiver) {
          _receiver.store(_dataBuffer.iterator());
          commit();
          _dataBuffer.clear();
        }
      }
 catch (      Exception ex) {
        _emittedToOffset=_lastComittedOffset;
        _dataBuffer.clear();
        _receiver.reportError(""String_Node_Str"" + _partition,ex);
      }
    }
  }
}"
4036,"/** 
 * Pings a HTTP URL. This effectively sends a HEAD request and returns <code>true</code> if the response code is in the 200-399 range.
 * @param url     The HTTP URL to be pinged.
 * @param timeout The timeout in millis for both the connection timeout and the response read timeout. Note thatthe total timeout is effectively two times the given timeout.
 * @return <code>true</code> if the given HTTP URL has returned response code 200-399 on a HEAD request within thegiven timeout, otherwise <code>false</code>.
 */
public static boolean ping(String url,int timeout){
  url=url.replaceFirst(""String_Node_Str"",""String_Node_Str"");
  try {
    HttpURLConnection connection=(HttpURLConnection)new URL(url).openConnection();
    connection.setConnectTimeout(timeout);
    connection.setReadTimeout(timeout);
    connection.setRequestMethod(""String_Node_Str"");
    int responseCode=connection.getResponseCode();
    return (200 <= responseCode && responseCode <= 399);
  }
 catch (  IOException exception) {
    return false;
  }
}","/** 
 * Pings a HTTP URL. This effectively sends a HEAD request and returns <code>true</code> if the response code is in the 200-399 range.
 * @param url     The HTTP URL to be pinged.
 * @param timeout The timeout in millis for both the connection timeout and the response read timeout. Note thatthe total timeout is effectively two times the given timeout.
 * @return <code>true</code> if the given HTTP URL has returned response code 200-399 on a HEAD request within thegiven timeout, otherwise <code>false</code>.
 */
public static boolean ping(String url,int timeout){
  url=url.replaceFirst(""String_Node_Str"",""String_Node_Str"");
  try {
    HttpURLConnection connection=(HttpURLConnection)new URL(url).openConnection();
    connection.setConnectTimeout(timeout);
    connection.setReadTimeout(timeout);
    connection.setRequestMethod(""String_Node_Str"");
    connection.setRequestProperty(""String_Node_Str"",""String_Node_Str"");
    int responseCode=connection.getResponseCode();
    return (200 <= responseCode && responseCode <= 399);
  }
 catch (  IOException exception) {
    exception.printStackTrace();
    return false;
  }
}"
4037,"public InternetConnectionChangeReceiver(Bus bus){
  this.bus=bus;
}","public InternetConnectionChangeReceiver(Bus bus){
  super(bus);
}"
4038,"@Override public void onReceive(Context context,Intent intent){
  if (intent.getAction().equals(NetworkEventsConfig.INTENT)) {
    boolean connectedToInternet=intent.getBooleanExtra(NetworkEventsConfig.INTENT_EXTRA,false);
    ConnectivityStatus connectivityStatus=(connectedToInternet) ? ConnectivityStatus.WIFI_CONNECTED_HAS_INTERNET : ConnectivityStatus.WIFI_CONNECTED_HAS_NO_INTERNET;
    bus.post(new ConnectivityChanged(connectivityStatus));
  }
}","@Override public void onReceive(Context context,Intent intent){
  if (intent.getAction().equals(NetworkEventsConfig.INTENT)) {
    boolean connectedToInternet=intent.getBooleanExtra(NetworkEventsConfig.INTENT_EXTRA,false);
    ConnectivityStatus connectivityStatus=(connectedToInternet) ? ConnectivityStatus.WIFI_CONNECTED_HAS_INTERNET : ConnectivityStatus.WIFI_CONNECTED_HAS_NO_INTERNET;
    if (statusNotChanged(connectivityStatus))     return;
    postConnectivityChanged(connectivityStatus);
  }
}"
4039,"public NetworkConnectionChangeReceiver(Bus bus){
  this.bus=bus;
}","public NetworkConnectionChangeReceiver(Bus bus){
  super(bus);
}"
4040,"@Override public void run(){
  ConnectivityStatus connectivityStatus=NetworkHelper.getConnectivityStatus(context);
  bus.post(new ConnectivityChanged(connectivityStatus));
  if (connectivityStatus == ConnectivityStatus.WIFI_CONNECTED) {
    new PingTask(context).execute();
  }
}","@Override public void run(){
  if (connectivityStatus == ConnectivityStatus.WIFI_CONNECTED) {
    new Ping(context).execute();
  }
}"
4041,"@Override public void onReceive(final Context context,Intent intent){
  onReceivePatched(context,new Runnable(){
    @Override public void run(){
      ConnectivityStatus connectivityStatus=NetworkHelper.getConnectivityStatus(context);
      bus.post(new ConnectivityChanged(connectivityStatus));
      if (connectivityStatus == ConnectivityStatus.WIFI_CONNECTED) {
        new PingTask(context).execute();
      }
    }
  }
);
}","@Override public void onReceive(final Context context,Intent intent){
  final ConnectivityStatus connectivityStatus=NetworkHelper.getConnectivityStatus(context);
  if (statusNotChanged(connectivityStatus))   return;
  postConnectivityChanged(connectivityStatus,new Runnable(){
    @Override public void run(){
      if (connectivityStatus == ConnectivityStatus.WIFI_CONNECTED) {
        new Ping(context).execute();
      }
    }
  }
);
}"
4042,"public WifiSignalStrengthChangeReceiver(Bus bus){
  this.bus=bus;
}","public WifiSignalStrengthChangeReceiver(Bus bus){
  super(bus);
}"
4043,"@Override public void onItemSelected(Object o,Row row){
}","@Override public void onItemSelected(Object item,Row row){
  if (item instanceof Video) {
    try {
      mBackgroundImageUrl=((Video)item).getBackgroundImageUrl();
      startBackgroundTimer();
    }
 catch (    Exception e) {
      e.printStackTrace();
    }
  }
 else   if (item instanceof VideoGroup) {
    try {
      mBackgroundImageUrl=((VideoGroup)item).getVideo().getBackgroundImageUrl();
      startBackgroundTimer();
    }
 catch (    Exception e) {
      e.printStackTrace();
    }
  }
}"
4044,"@Override public void onCreate(Bundle savedInstanceState){
  Log.i(TAG,""String_Node_Str"");
  super.onCreate(savedInstanceState);
  setupFragment();
}","@Override public void onCreate(Bundle savedInstanceState){
  super.onCreate(savedInstanceState);
  mBlurTransformation=new BlurTransform(getActivity());
  prepareBackgroundManager();
  setupFragment();
}"
4045,"private void setupFragment(){
  VerticalGridPresenter gridPresenter=new VerticalGridPresenter();
  Comparator<Video> videoNameComparator=new Comparator<Video>(){
    @Override public int compare(    Video o1,    Video o2){
      if (o2.getName() == null) {
        return (o1.getName() == null) ? 0 : -1;
      }
      if (o1.getName() == null) {
        return 1;
      }
      return o1.getName().toLowerCase().compareTo(o2.getName().toLowerCase());
    }
  }
;
  String selectedGenre=getActivity().getIntent().getStringExtra(Constants.GENRE);
  boolean isMovie=getActivity().getIntent().getBooleanExtra(Constants.IS_VIDEO,true);
  if (isMovie) {
    gridPresenter.setNumberOfColumns(NUM_COLUMNS);
    setGridPresenter(gridPresenter);
    mAdapter=new SortedObjectAdapter(videoNameComparator,new CardPresenter(getActivity()));
    List<Video> videos=Video.findWithQuery(Video.class,""String_Node_Str"" + ""String_Node_Str"" + selectedGenre + ""String_Node_Str"");
    setTitle(selectedGenre + ""String_Node_Str"" + getString(R.string.movies)+ ""String_Node_Str""+ videos.size()+ ""String_Node_Str"");
    for (    Video video : videos) {
      mAdapter.add(video);
    }
  }
 else {
    gridPresenter.setNumberOfColumns(NUM_COLUMNS - 2);
    setGridPresenter(gridPresenter);
    mAdapter=new SortedObjectAdapter(videoNameComparator,new TvShowsCardPresenter(getActivity()));
    List<Video> tvshows=Video.findWithQuery(Video.class,""String_Node_Str"" + ""String_Node_Str"" + selectedGenre + ""String_Node_Str"");
    setTitle(selectedGenre + ""String_Node_Str"" + getString(R.string.tv_shows)+ ""String_Node_Str""+ tvshows.size()+ ""String_Node_Str"");
    Map<String,VideoGroup> tvShowsMap=new TreeMap<String,VideoGroup>();
    for (    Video video : tvshows) {
      if (tvShowsMap.containsKey(video.getName())) {
        VideoGroup group=tvShowsMap.get(video.getName());
        if (TextUtils.isEmpty(group.getVideo().getCardImageUrl())) {
          group.getVideo().setCardImageUrl(video.getCardImageUrl());
        }
        group.increment();
      }
 else {
        VideoGroup vg=new VideoGroup(video);
        tvShowsMap.put(video.getName(),vg);
      }
    }
    if (tvShowsMap.size() > 0) {
      for (      VideoGroup videoGroup : tvShowsMap.values()) {
        mAdapter.add(videoGroup);
      }
    }
  }
  setAdapter(mAdapter);
  setOnItemClickedListener(new OnItemClickedListener(){
    @Override public void onItemClicked(    Object item,    Row row){
      if (item instanceof VideoGroup) {
        Intent intent=new Intent(getActivity(),DetailsActivity.class);
        intent.putExtra(Constants.IS_VIDEO,false);
        intent.putExtra(Constants.VIDEO_GROUP,(VideoGroup)item);
        startActivity(intent);
      }
 else       if (((Video)item).isMatched()) {
        Intent intent=new Intent(getActivity(),DetailsActivity.class);
        intent.putExtra(Constants.IS_VIDEO,true);
        intent.putExtra(Constants.VIDEO,(Video)item);
        startActivity(intent);
      }
    }
  }
);
  setOnItemSelectedListener(new OnItemSelectedListener(){
    @Override public void onItemSelected(    Object o,    Row row){
    }
  }
);
}","private void setupFragment(){
  VerticalGridPresenter gridPresenter=new VerticalGridPresenter();
  String selectedGenre=getActivity().getIntent().getStringExtra(Constants.GENRE);
  boolean isMovie=getActivity().getIntent().getBooleanExtra(Constants.IS_VIDEO,true);
  ArrayObjectAdapter adapter;
  if (isMovie) {
    gridPresenter.setNumberOfColumns(NUM_COLUMNS);
    setGridPresenter(gridPresenter);
    adapter=new ArrayObjectAdapter(new CardPresenter(getActivity()));
    String sql=""String_Node_Str"" + ""String_Node_Str"" + selectedGenre + ""String_Node_Str"";
    List<Video> videos=Video.findWithQuery(Video.class,sql);
    setTitle(selectedGenre);
    adapter.addAll(0,videos);
  }
 else {
    gridPresenter.setNumberOfColumns(NUM_COLUMNS - 2);
    setGridPresenter(gridPresenter);
    adapter=new ArrayObjectAdapter(new TvShowsCardPresenter(getActivity()));
    String sql=""String_Node_Str"" + ""String_Node_Str"" + selectedGenre + ""String_Node_Str"";
    List<Video> tvshows=Video.findWithQuery(Video.class,sql);
    setTitle(selectedGenre);
    Map<String,VideoGroup> tvShowsMap=new TreeMap<String,VideoGroup>();
    for (    Video video : tvshows) {
      if (tvShowsMap.containsKey(video.getName())) {
        VideoGroup group=tvShowsMap.get(video.getName());
        if (TextUtils.isEmpty(group.getVideo().getCardImageUrl())) {
          group.getVideo().setCardImageUrl(video.getCardImageUrl());
        }
        group.increment();
      }
 else {
        VideoGroup vg=new VideoGroup(video);
        tvShowsMap.put(video.getName(),vg);
      }
    }
    if (tvShowsMap.size() > 0) {
      adapter.addAll(0,tvShowsMap.values());
    }
  }
  setAdapter(adapter);
  setOnItemClickedListener(new OnItemClickedListener(){
    @Override public void onItemClicked(    Object item,    Row row){
      if (item instanceof VideoGroup) {
        Intent intent=new Intent(getActivity(),DetailsActivity.class);
        intent.putExtra(Constants.IS_VIDEO,false);
        intent.putExtra(Constants.VIDEO_GROUP,(VideoGroup)item);
        startActivity(intent);
      }
 else       if (((Video)item).isMatched()) {
        Intent intent=new Intent(getActivity(),DetailsActivity.class);
        intent.putExtra(Constants.IS_VIDEO,true);
        intent.putExtra(Constants.VIDEO,(Video)item);
        startActivity(intent);
      }
    }
  }
);
  setOnItemSelectedListener(new OnItemSelectedListener(){
    @Override public void onItemSelected(    Object item,    Row row){
      if (item instanceof Video) {
        try {
          mBackgroundImageUrl=((Video)item).getBackgroundImageUrl();
          startBackgroundTimer();
        }
 catch (        Exception e) {
          e.printStackTrace();
        }
      }
 else       if (item instanceof VideoGroup) {
        try {
          mBackgroundImageUrl=((VideoGroup)item).getVideo().getBackgroundImageUrl();
          startBackgroundTimer();
        }
 catch (        Exception e) {
          e.printStackTrace();
        }
      }
    }
  }
);
}"
4046,"@Override protected void onPostExecute(List<SmbFile> files){
  try {
    final int cpuCount=Runtime.getRuntime().availableProcessors();
    final int maxPoolSize=cpuCount * 2 + 1;
    final int partitionSize=files.size() / maxPoolSize;
    List<List<SmbFile>> subSets=ListUtils.partition(files,partitionSize);
    mNumOfSets=subSets.size();
    String[] sections=mPassword.split(""String_Node_Str"");
    String directory=sections[sections.length - 1];
    for (    List<SmbFile> subSet : subSets) {
      new DownloadMovieInfoTask(directory,subSet,this).executeOnExecutor(THREAD_POOL_EXECUTOR);
    }
  }
 catch (  Exception e) {
    if (mOnTaskCompletedListener != null) {
      mOnTaskCompletedListener.onTaskFailed();
    }
  }
}","@Override protected void onPostExecute(List<SmbFile> files){
  try {
    final int cpuCount=Runtime.getRuntime().availableProcessors();
    final int maxPoolSize=cpuCount * 2 + 1;
    final int partitionSize=files.size() < maxPoolSize ? files.size() : (files.size() / maxPoolSize);
    List<List<SmbFile>> subSets=ListUtils.partition(files,partitionSize);
    mNumOfSets=subSets.size();
    String[] sections=mPassword.split(""String_Node_Str"");
    String directory=sections[sections.length - 1];
    for (    List<SmbFile> subSet : subSets) {
      new DownloadMovieInfoTask(directory,subSet,this).executeOnExecutor(THREAD_POOL_EXECUTOR);
    }
  }
 catch (  Exception e) {
    if (mOnTaskCompletedListener != null) {
      mOnTaskCompletedListener.onTaskFailed();
    }
  }
}"
4047,"/** 
 * Get all logs that in buffer.
 */
public static Records getBufferedLogs(){
  checkIfPureeHasInitialized();
  return logger.getBufferedLogs();
}","/** 
 * Get all logs that in buffer.
 * @return {@link Records}.
 */
public static Records getBufferedLogs(){
  checkIfPureeHasInitialized();
  return logger.getBufferedLogs();
}"
4048,"/** 
 * Try to send log. This log is sent immediately or put into buffer (it's depending on output plugin).
 */
public static void send(final PureeLog log){
  checkIfPureeHasInitialized();
  logger.send(log);
}","/** 
 * Try to send log. <p> This log is sent immediately or put into buffer (it's depending on output plugin).
 * @param log {@link PureeLog}.
 */
public static void send(final PureeLog log){
  checkIfPureeHasInitialized();
  logger.send(log);
}"
4049,"/** 
 * Print mapping of SOURCE -> FILTER... OUTPUT.
 */
public void printMapping(){
  LogDumper.out(sourceOutputMap);
}","/** 
 * Print mapping of SOURCE -&gt; FILTER... OUTPUT.
 */
public void printMapping(){
  LogDumper.out(sourceOutputMap);
}"
4050,"/** 
 * Start building a new   {@link com.cookpad.puree.PureeConfiguration} instance.
 */
public Builder(Context context){
  this.context=context.getApplicationContext();
}","/** 
 * Start building a new   {@link com.cookpad.puree.PureeConfiguration} instance.
 * @param context {@link Context}.
 */
public Builder(Context context){
  this.context=context.getApplicationContext();
}"
4051,"/** 
 * Specify a source class of logs, which returns   {@link Source} an{@link Source#to(PureeOutput)} must be called to register an output plugin.
 */
public Source source(Class<? extends PureeLog> logClass){
  return new Source(this,logClass);
}","/** 
 * Specify a source class of logs, which returns   {@link Source} an{@link Source#to(PureeOutput)} must be called to register an output plugin.
 * @param logClass log class.
 * @return {@link Source}.
 */
public Source source(Class<? extends PureeLog> logClass){
  return new Source(this,logClass);
}"
4052,"/** 
 * Create the   {@link com.cookpad.puree.PureeConfiguration} instance.
 */
public PureeConfiguration build(){
  if (gson == null) {
    gson=new Gson();
  }
  if (storage == null) {
    storage=new PureeSQLiteStorage(context);
  }
  if (executor == null) {
    executor=newBackgroundExecutor();
  }
  return new PureeConfiguration(context,gson,sourceOutputMap,storage,executor);
}","/** 
 * Create the   {@link com.cookpad.puree.PureeConfiguration} instance.
 * @return {@link com.cookpad.puree.PureeConfiguration}.
 */
public PureeConfiguration build(){
  if (gson == null) {
    gson=new Gson();
  }
  if (storage == null) {
    storage=new PureeSQLiteStorage(context);
  }
  if (executor == null) {
    executor=newBackgroundExecutor();
  }
  return new PureeConfiguration(context,gson,sourceOutputMap,storage,executor);
}"
4053,"/** 
 * Specify the   {@link com.google.gson.Gson} to serialize logs.
 */
public Builder gson(Gson gson){
  this.gson=gson;
  return this;
}","/** 
 * Specify the   {@link com.google.gson.Gson} to serialize logs.
 * @param gson {@link Gson}.
 * @return {@link com.cookpad.puree.PureeConfiguration.Builder}.
 */
public Builder gson(Gson gson){
  this.gson=gson;
  return this;
}"
4054,"/** 
 * Serialize a   {@link PureeLog} into {@link JsonObject} with {@link Gson}
 */
@Nonnull public JsonObject serializeLog(PureeLog log){
  return (JsonObject)gson.toJsonTree(log);
}","/** 
 * Serialize a   {@link PureeLog} into {@link JsonObject} with {@link Gson}.
 * @param log {@link PureeLog}.
 * @return serialized json object.
 */
@Nonnull public JsonObject serializeLog(PureeLog log){
  return (JsonObject)gson.toJsonTree(log);
}"
4055,"/** 
 * Specify the   {@link com.cookpad.puree.PureeFilter}. 
 */
public Source filter(PureeFilter filter){
  filters.add(filter);
  return this;
}","/** 
 * Specify the   {@link com.cookpad.puree.PureeFilter}.
 * @param filter {@link PureeFilter}.
 * @return {@link Source}.
 */
public Source filter(PureeFilter filter){
  filters.add(filter);
  return this;
}"
4056,"/** 
 * Specify the   {@link com.cookpad.puree.PureeFilter}. 
 */
public Source filters(PureeFilter... filters){
  this.filters.addAll(Arrays.asList(filters));
  return this;
}","/** 
 * Specify the   {@link com.cookpad.puree.PureeFilter}.
 * @param filters {@link PureeFilter} list.
 * @return {@link Source}.
 */
public Source filters(PureeFilter... filters){
  this.filters.addAll(Arrays.asList(filters));
  return this;
}"
4057,"/** 
 * Specify the   {@link com.cookpad.puree.outputs.PureeOutput} that is responded to source. 
 */
public PureeConfiguration.Builder to(PureeOutput output){
  builder.register(logClass,output.withFilters(filters));
  return builder;
}","/** 
 * Specify the   {@link com.cookpad.puree.outputs.PureeOutput} that is responded to source.
 * @param output {@link com.cookpad.puree.outputs.PureeOutput}.
 * @return {@link com.cookpad.puree.PureeConfiguration.Builder}.
 */
public PureeConfiguration.Builder to(PureeOutput output){
  builder.register(logClass,output.withFilters(filters));
  return builder;
}"
4058,"@Test public void testPureeBufferedOutput() throws Exception {
  logger.send(new PvLog(""String_Node_Str""));
  logger.send(new PvLog(""String_Node_Str""));
  logger.send(new PvLog(""String_Node_Str""));
  logger.flush();
  Thread.sleep(1000);
  assertThat(logs.size(),is(3));
  assertThat(logs.get(0),is(""String_Node_Str""));
  assertThat(logs.get(1),is(""String_Node_Str""));
  assertThat(logs.get(2),is(""String_Node_Str""));
}","@Test public void testPureeBufferedOutput() throws Exception {
  logger=new PureeConfiguration.Builder(context).register(PvLog.class,new BufferedOutput(handler)).build().createPureeLogger();
  logger.discardBufferedLogs();
  logger.send(new PvLog(""String_Node_Str""));
  logger.send(new PvLog(""String_Node_Str""));
  logger.send(new PvLog(""String_Node_Str""));
  logger.flush();
  Thread.sleep(1000);
  assertThat(logs.size(),is(3));
  assertThat(logs.get(0),is(""String_Node_Str""));
  assertThat(logs.get(1),is(""String_Node_Str""));
  assertThat(logs.get(2),is(""String_Node_Str""));
}"
4059,"@Before public void setUp() throws Exception {
  Context context=InstrumentationRegistry.getTargetContext();
  Handler handler=new Handler(Looper.getMainLooper());
  output=new BufferedOutput(handler);
  logger=new PureeConfiguration.Builder(context).register(PvLog.class,output).build().createPureeLogger();
  logger.discardBufferedLogs();
}","@Before public void setUp() throws Exception {
  context=InstrumentationRegistry.getTargetContext();
  handler=new Handler(Looper.getMainLooper());
}"
4060,"@Override public void emit(JsonArray jsonArray,AsyncResult result){
  for (  JsonElement item : jsonArray) {
    logs.add(item.toString());
  }
  result.success();
}","@Override public void emit(JsonArray jsonArray,AsyncResult result){
  throw new AssertionFailedError(""String_Node_Str"");
}"
4061,"@After public void tearDown() throws Exception {
  logger.discardBufferedLogs();
}","@After public void tearDown() throws Exception {
  if (logger != null) {
    logger.discardBufferedLogs();
  }
}"
4062,"@Override public void receive(final JsonObject jsonLog){
  new AsyncRunnableTask(){
    @Override public void run(){
      JsonObject filteredLog=applyFilters(jsonLog);
      storage.insert(type(),filteredLog);
    }
  }
.execute();
  flushTask.tryToStart();
}","@Override public void receive(final JsonObject jsonLog){
  new AsyncRunnableTask(){
    @Override public void run(){
      JsonObject filteredLog=applyFilters(jsonLog);
      if (filteredLog != null) {
        storage.insert(type(),filteredLog);
      }
    }
  }
.execute();
  flushTask.tryToStart();
}"
4063,"protected JsonObject applyFilters(JsonObject jsonLog){
  JsonObject filteredLog=jsonLog;
  for (  PureeFilter filter : filters) {
    filteredLog=filter.apply(filteredLog);
    if (filteredLog == null) {
      return null;
    }
  }
  return filteredLog;
}","@Nullable protected JsonObject applyFilters(JsonObject jsonLog){
  JsonObject filteredLog=jsonLog;
  for (  PureeFilter filter : filters) {
    filteredLog=filter.apply(filteredLog);
    if (filteredLog == null) {
      return null;
    }
  }
  return filteredLog;
}"
4064,"protected JSONObject applyFilters(JSONObject jsonLog) throws JSONException {
  if (filters == null || filters.isEmpty()) {
    return jsonLog;
  }
  JSONObject filteredLog=new JSONObject();
  for (  PureeFilter filter : filters) {
    filteredLog=filter.apply(jsonLog);
  }
  return filteredLog;
}","protected JSONObject applyFilters(JSONObject jsonLog) throws JSONException {
  JSONObject filteredLog=jsonLog;
  for (  PureeFilter filter : filters) {
    filteredLog=filter.apply(filteredLog);
    if (filteredLog == null) {
      return null;
    }
  }
  return filteredLog;
}"
4065,"public static synchronized void initialize(PureeConfiguration conf){
  if (isInitialized) {
    Log.w(TAG,""String_Node_Str"");
  }
  gson=conf.getGson();
  storage=new PureeDbHelper(conf.getApplicationContext());
  for (  PureeOutput output : conf.getOutputs()) {
    output.initialize(storage);
    outputMap.put(output.type(),output);
  }
  isInitialized=true;
}","public static synchronized void initialize(PureeConfiguration conf){
  if (isInitialized) {
    Log.w(TAG,""String_Node_Str"");
    return;
  }
  gson=conf.getGson();
  storage=new PureeDbHelper(conf.getApplicationContext());
  for (  PureeOutput output : conf.getOutputs()) {
    output.initialize(storage);
    outputMap.put(output.type(),output);
  }
  isInitialized=true;
}"
4066,"public static synchronized void initialize(LogHouseConfiguration conf){
  if (isInitialized) {
    Log.w(TAG,""String_Node_Str"");
  }
  gson=conf.getGson();
  storage=new LogHouseDbHelper(conf.getApplicationContext());
  for (  LogHouseOutput output : conf.getOutputs()) {
    output.initialize(conf,storage);
    output.initialize(conf,storage);
    outputs.add(output);
  }
  isInitialized=true;
}","public static synchronized void initialize(LogHouseConfiguration conf){
  if (isInitialized) {
    Log.w(TAG,""String_Node_Str"");
  }
  gson=conf.getGson();
  storage=new LogHouseDbHelper(conf.getApplicationContext());
  for (  LogHouseOutput output : conf.getOutputs()) {
    output.initialize(conf,storage);
    outputs.add(output);
  }
  isInitialized=true;
}"
4067,"public void flushSync(){
  Records records=getRecordsFromStorage();
  if (records.isEmpty()) {
    return;
  }
  while (!records.isEmpty()) {
    final List<JSONObject> serializedLogs=records.getSerializedLogs();
    if (!isTest) {
      boolean isSuccess=flushChunkOfLogs(serializedLogs);
      if (isSuccess) {
        lazyTaskRunner.reset();
      }
 else {
        lazyTaskRunner.retryLater();
        return;
      }
    }
    afterFlushFilter.call(type(),serializedLogs);
    storage.delete(records);
    records=getRecordsFromStorage();
  }
}","public void flushSync(){
  Records records=getRecordsFromStorage();
  if (records.isEmpty()) {
    return;
  }
  while (!records.isEmpty()) {
    final List<JSONObject> serializedLogs=records.getSerializedLogs();
    if (!LogHouseConfiguration.isTest) {
      boolean isSuccess=flushChunkOfLogs(serializedLogs);
      if (isSuccess) {
        lazyTaskRunner.reset();
      }
 else {
        lazyTaskRunner.retryLater();
        return;
      }
    }
    afterFlushFilter.call(type(),serializedLogs);
    storage.delete(records);
    records=getRecordsFromStorage();
  }
}"
4068,"@Override public void start(JSONObject serializedLog){
  if (isTest) {
    insertSync(type(),serializedLog);
    flushSync();
  }
 else {
    new AsyncInsertTask(this,type(),serializedLog).execute();
    lazyTaskRunner.tryToStart();
  }
}","@Override public void start(JSONObject serializedLog){
  if (LogHouseConfiguration.isTest) {
    insertSync(type(),serializedLog);
    flushSync();
  }
 else {
    new AsyncInsertTask(this,type(),serializedLog).execute();
    lazyTaskRunner.tryToStart();
  }
}"
4069,"void setAfterFlushFilter(AfterFlushFilter afterFlushFilter){
  this.isTest=true;
  this.afterFlushFilter=afterFlushFilter;
}","void setAfterFlushFilter(AfterFlushFilter afterFlushFilter){
  isTest=true;
  this.afterFlushFilter=afterFlushFilter;
}"
4070,"public void initialize(LogHouseConfiguration logHouseConfiguration,LogHouseStorage storage){
  this.isTest=logHouseConfiguration.isTest();
  this.afterFlushFilter=logHouseConfiguration.getAfterFlushFilter();
  this.beforeEmitFilter=logHouseConfiguration.getBeforeEmitFilter();
  this.storage=storage;
  this.conf=configure(new Configuration());
}","public void initialize(LogHouseConfiguration logHouseConfiguration,LogHouseStorage storage){
  this.afterFlushFilter=logHouseConfiguration.getAfterFlushFilter();
  this.beforeEmitFilter=logHouseConfiguration.getBeforeEmitFilter();
  this.storage=storage;
  this.conf=configure(new Configuration());
}"
4071,"public void shouldBe(Matcher matcher){
synchronized (LOCK) {
    final CountDownLatch latch=new CountDownLatch(logs.size());
    final List<JSONObject> results=new ArrayList<>();
    final String[] compareInfoMessage={""String_Node_Str""};
    conf.setAfterFlushFilter(new AfterFlushFilter(){
      @Override public void call(      String type,      List<JSONObject> serializedLogs){
        compareInfoMessage[0]+=""String_Node_Str"" + target + ""String_Node_Str""+ type+ ""String_Node_Str"";
        if (target.equals(type)) {
          results.addAll(serializedLogs);
        }
        latch.countDown();
      }
    }
);
    initializeLogHouse(conf);
    putLogs(logs);
    try {
      latch.await(1000,TimeUnit.MILLISECONDS);
      matcher.expect(results);
    }
 catch (    AssertionFailedError e) {
      Records records=LogHouse.getBufferedLogs();
      String message=LogDumper.buildMessage(records);
      throw new AssertionFailedError(e.getMessage() + ""String_Node_Str"" + compareInfoMessage[0]+ ""String_Node_Str""+ results.size()+ ""String_Node_Str""+ message);
    }
catch (    JSONException|InterruptedException e) {
      throw new RuntimeException(e.getMessage());
    }
  }
}","public void shouldBe(Matcher matcher){
synchronized (LOCK) {
    final CountDownLatch latch=new CountDownLatch(logs.size());
    final List<JSONObject> results=new ArrayList<>();
    final String[] compareInfoMessage={""String_Node_Str""};
    conf.setAfterFlushFilter(new AfterFlushFilter(){
      @Override public void call(      String type,      List<JSONObject> serializedLogs){
        compareInfoMessage[0]+=""String_Node_Str"" + target + ""String_Node_Str""+ type+ ""String_Node_Str"";
        if (target.equals(type)) {
          results.addAll(serializedLogs);
        }
        latch.countDown();
      }
    }
);
    initializeLogHouse(conf);
    putLogs(logs);
    try {
      latch.await(1000,TimeUnit.MILLISECONDS);
      matcher.expect(results);
    }
 catch (    AssertionFailedError e) {
      throw new AssertionFailedError(e.getMessage() + ""String_Node_Str"" + compareInfoMessage[0]+ ""String_Node_Str""+ results.size());
    }
catch (    JSONException|InterruptedException e) {
      throw new RuntimeException(e.getMessage());
    }
  }
}"
4072,"public void flushSync(){
  Records records=getRecordsFromStorage();
  if (records.isEmpty()) {
    return;
  }
  while (!records.isEmpty()) {
    final List<JSONObject> serializedLogs=records.getSerializedLogs();
    if (!isTest) {
      if (!flushChunkOfLogs(serializedLogs)) {
        lazyTaskRunner.retryLater();
        return;
      }
    }
    afterFlushAction.call(type(),serializedLogs);
    storage.delete(records);
    records=getRecordsFromStorage();
  }
}","public void flushSync(){
  Records records=getRecordsFromStorage();
  if (records.isEmpty()) {
    return;
  }
  while (!records.isEmpty()) {
    final List<JSONObject> serializedLogs=records.getSerializedLogs();
    if (!isTest) {
      boolean isSuccess=flushChunkOfLogs(serializedLogs);
      if (isSuccess) {
        lazyTaskRunner.reset();
      }
 else {
        lazyTaskRunner.retryLater();
        return;
      }
    }
    afterFlushAction.call(type(),serializedLogs);
    storage.delete(records);
    records=getRecordsFromStorage();
  }
}"
4073,"public LazyTaskRunner(final LazyTask task,final int interval){
  this.interval=interval;
  this.handler=new Handler();
  this.hasAlreadySet=false;
  this.callback=new Runnable(){
    @Override public void run(){
      hasAlreadySet=false;
      retryCount=1;
      task.run();
    }
  }
;
}","public LazyTaskRunner(final LazyTask task,final int interval){
  this.interval=interval;
  this.handler=new Handler();
  this.hasAlreadySet=false;
  this.callback=new Runnable(){
    @Override public void run(){
      task.run();
    }
  }
;
}"
4074,"public synchronized void tryToStart(){
  if (hasAlreadySet && retryCount == 1) {
    return;
  }
  retryCount=1;
  startDelayed();
}","public synchronized void tryToStart(){
  if (hasAlreadySet && retryCount == 0) {
    return;
  }
  retryCount=0;
  startDelayed();
}"
4075,"private synchronized void startDelayed(){
  handler.removeCallbacks(callback);
  int buckOffTime=interval * retryCount;
  handler.postDelayed(callback,buckOffTime);
  hasAlreadySet=true;
}","private synchronized void startDelayed(){
  handler.removeCallbacks(callback);
  int buckOffTime=interval * retryCount;
  handler.postDelayed(callback,interval + buckOffTime);
  hasAlreadySet=true;
}"
4076,"@Override public void run(){
  hasAlreadySet=false;
  retryCount=1;
  task.run();
}","@Override public void run(){
  task.run();
}"
4077,"@SuppressLint(""String_Node_Str"") public static Bitmap fastblur(Context context,Bitmap sentBitmap,int radius){
  if (VERSION.SDK_INT > 16) {
    Bitmap bitmap=sentBitmap.copy(sentBitmap.getConfig(),true);
    final RenderScript rs=RenderScript.create(context);
    final Allocation input=Allocation.createFromBitmap(rs,sentBitmap,Allocation.MipmapControl.MIPMAP_NONE,Allocation.USAGE_SCRIPT);
    final Allocation output=Allocation.createTyped(rs,input.getType());
    final ScriptIntrinsicBlur script=ScriptIntrinsicBlur.create(rs,Element.U8_4(rs));
    script.setRadius(radius);
    script.setInput(input);
    script.forEach(output);
    output.copyTo(bitmap);
    return bitmap;
  }
  Bitmap bitmap=sentBitmap.copy(sentBitmap.getConfig(),true);
  if (radius < 1) {
    return (null);
  }
  int w=bitmap.getWidth();
  int h=bitmap.getHeight();
  int[] pix=new int[w * h];
  bitmap.getPixels(pix,0,w,0,0,w,h);
  int wm=w - 1;
  int hm=h - 1;
  int wh=w * h;
  int div=radius + radius + 1;
  int r[]=new int[wh];
  int g[]=new int[wh];
  int b[]=new int[wh];
  int rsum, gsum, bsum, x, y, i, p, yp, yi, yw;
  int vmin[]=new int[Math.max(w,h)];
  int divsum=(div + 1) >> 1;
  divsum*=divsum;
  int dv[]=new int[256 * divsum];
  for (i=0; i < 256 * divsum; i++) {
    dv[i]=(i / divsum);
  }
  yw=yi=0;
  int[][] stack=new int[div][3];
  int stackpointer;
  int stackstart;
  int[] sir;
  int rbs;
  int r1=radius + 1;
  int routsum, goutsum, boutsum;
  int rinsum, ginsum, binsum;
  for (y=0; y < h; y++) {
    rinsum=ginsum=binsum=routsum=goutsum=boutsum=rsum=gsum=bsum=0;
    for (i=-radius; i <= radius; i++) {
      p=pix[yi + Math.min(wm,Math.max(i,0))];
      sir=stack[i + radius];
      sir[0]=(p & 0xff0000) >> 16;
      sir[1]=(p & 0x00ff00) >> 8;
      sir[2]=(p & 0x0000ff);
      rbs=r1 - Math.abs(i);
      rsum+=sir[0] * rbs;
      gsum+=sir[1] * rbs;
      bsum+=sir[2] * rbs;
      if (i > 0) {
        rinsum+=sir[0];
        ginsum+=sir[1];
        binsum+=sir[2];
      }
 else {
        routsum+=sir[0];
        goutsum+=sir[1];
        boutsum+=sir[2];
      }
    }
    stackpointer=radius;
    for (x=0; x < w; x++) {
      r[yi]=dv[rsum];
      g[yi]=dv[gsum];
      b[yi]=dv[bsum];
      rsum-=routsum;
      gsum-=goutsum;
      bsum-=boutsum;
      stackstart=stackpointer - radius + div;
      sir=stack[stackstart % div];
      routsum-=sir[0];
      goutsum-=sir[1];
      boutsum-=sir[2];
      if (y == 0) {
        vmin[x]=Math.min(x + radius + 1,wm);
      }
      p=pix[yw + vmin[x]];
      sir[0]=(p & 0xff0000) >> 16;
      sir[1]=(p & 0x00ff00) >> 8;
      sir[2]=(p & 0x0000ff);
      rinsum+=sir[0];
      ginsum+=sir[1];
      binsum+=sir[2];
      rsum+=rinsum;
      gsum+=ginsum;
      bsum+=binsum;
      stackpointer=(stackpointer + 1) % div;
      sir=stack[(stackpointer) % div];
      routsum+=sir[0];
      goutsum+=sir[1];
      boutsum+=sir[2];
      rinsum-=sir[0];
      ginsum-=sir[1];
      binsum-=sir[2];
      yi++;
    }
    yw+=w;
  }
  for (x=0; x < w; x++) {
    rinsum=ginsum=binsum=routsum=goutsum=boutsum=rsum=gsum=bsum=0;
    yp=-radius * w;
    for (i=-radius; i <= radius; i++) {
      yi=Math.max(0,yp) + x;
      sir=stack[i + radius];
      sir[0]=r[yi];
      sir[1]=g[yi];
      sir[2]=b[yi];
      rbs=r1 - Math.abs(i);
      rsum+=r[yi] * rbs;
      gsum+=g[yi] * rbs;
      bsum+=b[yi] * rbs;
      if (i > 0) {
        rinsum+=sir[0];
        ginsum+=sir[1];
        binsum+=sir[2];
      }
 else {
        routsum+=sir[0];
        goutsum+=sir[1];
        boutsum+=sir[2];
      }
      if (i < hm) {
        yp+=w;
      }
    }
    yi=x;
    stackpointer=radius;
    for (y=0; y < h; y++) {
      pix[yi]=(0xff000000 & pix[yi]) | (dv[rsum] << 16) | (dv[gsum] << 8)| dv[bsum];
      rsum-=routsum;
      gsum-=goutsum;
      bsum-=boutsum;
      stackstart=stackpointer - radius + div;
      sir=stack[stackstart % div];
      routsum-=sir[0];
      goutsum-=sir[1];
      boutsum-=sir[2];
      if (x == 0) {
        vmin[y]=Math.min(y + r1,hm) * w;
      }
      p=x + vmin[y];
      sir[0]=r[p];
      sir[1]=g[p];
      sir[2]=b[p];
      rinsum+=sir[0];
      ginsum+=sir[1];
      binsum+=sir[2];
      rsum+=rinsum;
      gsum+=ginsum;
      bsum+=binsum;
      stackpointer=(stackpointer + 1) % div;
      sir=stack[stackpointer];
      routsum+=sir[0];
      goutsum+=sir[1];
      boutsum+=sir[2];
      rinsum-=sir[0];
      ginsum-=sir[1];
      binsum-=sir[2];
      yi+=w;
    }
  }
  bitmap.setPixels(pix,0,w,0,0,w,h);
  return (bitmap);
}","@SuppressLint(""String_Node_Str"") public static Bitmap fastblur(Context context,Bitmap sentBitmap,int radius,boolean canReuseInBitmap){
  if (VERSION.SDK_INT > 16) {
    Bitmap bitmap=sentBitmap.copy(sentBitmap.getConfig(),true);
    final RenderScript rs=RenderScript.create(context);
    final Allocation input=Allocation.createFromBitmap(rs,sentBitmap,Allocation.MipmapControl.MIPMAP_NONE,Allocation.USAGE_SCRIPT);
    final Allocation output=Allocation.createTyped(rs,input.getType());
    final ScriptIntrinsicBlur script=ScriptIntrinsicBlur.create(rs,Element.U8_4(rs));
    script.setRadius(radius);
    script.setInput(input);
    script.forEach(output);
    output.copyTo(bitmap);
    return bitmap;
  }
  Bitmap bitmap;
  if (canReuseInBitmap) {
    bitmap=sentBitmap;
  }
 else {
    bitmap=sentBitmap.copy(sentBitmap.getConfig(),true);
  }
  if (radius < 1) {
    return (null);
  }
  int w=bitmap.getWidth();
  int h=bitmap.getHeight();
  int[] pix=new int[w * h];
  bitmap.getPixels(pix,0,w,0,0,w,h);
  int wm=w - 1;
  int hm=h - 1;
  int wh=w * h;
  int div=radius + radius + 1;
  int r[]=new int[wh];
  int g[]=new int[wh];
  int b[]=new int[wh];
  int rsum, gsum, bsum, x, y, i, p, yp, yi, yw;
  int vmin[]=new int[Math.max(w,h)];
  int divsum=(div + 1) >> 1;
  divsum*=divsum;
  int dv[]=new int[256 * divsum];
  for (i=0; i < 256 * divsum; i++) {
    dv[i]=(i / divsum);
  }
  yw=yi=0;
  int[][] stack=new int[div][3];
  int stackpointer;
  int stackstart;
  int[] sir;
  int rbs;
  int r1=radius + 1;
  int routsum, goutsum, boutsum;
  int rinsum, ginsum, binsum;
  for (y=0; y < h; y++) {
    rinsum=ginsum=binsum=routsum=goutsum=boutsum=rsum=gsum=bsum=0;
    for (i=-radius; i <= radius; i++) {
      p=pix[yi + Math.min(wm,Math.max(i,0))];
      sir=stack[i + radius];
      sir[0]=(p & 0xff0000) >> 16;
      sir[1]=(p & 0x00ff00) >> 8;
      sir[2]=(p & 0x0000ff);
      rbs=r1 - Math.abs(i);
      rsum+=sir[0] * rbs;
      gsum+=sir[1] * rbs;
      bsum+=sir[2] * rbs;
      if (i > 0) {
        rinsum+=sir[0];
        ginsum+=sir[1];
        binsum+=sir[2];
      }
 else {
        routsum+=sir[0];
        goutsum+=sir[1];
        boutsum+=sir[2];
      }
    }
    stackpointer=radius;
    for (x=0; x < w; x++) {
      r[yi]=dv[rsum];
      g[yi]=dv[gsum];
      b[yi]=dv[bsum];
      rsum-=routsum;
      gsum-=goutsum;
      bsum-=boutsum;
      stackstart=stackpointer - radius + div;
      sir=stack[stackstart % div];
      routsum-=sir[0];
      goutsum-=sir[1];
      boutsum-=sir[2];
      if (y == 0) {
        vmin[x]=Math.min(x + radius + 1,wm);
      }
      p=pix[yw + vmin[x]];
      sir[0]=(p & 0xff0000) >> 16;
      sir[1]=(p & 0x00ff00) >> 8;
      sir[2]=(p & 0x0000ff);
      rinsum+=sir[0];
      ginsum+=sir[1];
      binsum+=sir[2];
      rsum+=rinsum;
      gsum+=ginsum;
      bsum+=binsum;
      stackpointer=(stackpointer + 1) % div;
      sir=stack[(stackpointer) % div];
      routsum+=sir[0];
      goutsum+=sir[1];
      boutsum+=sir[2];
      rinsum-=sir[0];
      ginsum-=sir[1];
      binsum-=sir[2];
      yi++;
    }
    yw+=w;
  }
  for (x=0; x < w; x++) {
    rinsum=ginsum=binsum=routsum=goutsum=boutsum=rsum=gsum=bsum=0;
    yp=-radius * w;
    for (i=-radius; i <= radius; i++) {
      yi=Math.max(0,yp) + x;
      sir=stack[i + radius];
      sir[0]=r[yi];
      sir[1]=g[yi];
      sir[2]=b[yi];
      rbs=r1 - Math.abs(i);
      rsum+=r[yi] * rbs;
      gsum+=g[yi] * rbs;
      bsum+=b[yi] * rbs;
      if (i > 0) {
        rinsum+=sir[0];
        ginsum+=sir[1];
        binsum+=sir[2];
      }
 else {
        routsum+=sir[0];
        goutsum+=sir[1];
        boutsum+=sir[2];
      }
      if (i < hm) {
        yp+=w;
      }
    }
    yi=x;
    stackpointer=radius;
    for (y=0; y < h; y++) {
      pix[yi]=(0xff000000 & pix[yi]) | (dv[rsum] << 16) | (dv[gsum] << 8)| dv[bsum];
      rsum-=routsum;
      gsum-=goutsum;
      bsum-=boutsum;
      stackstart=stackpointer - radius + div;
      sir=stack[stackstart % div];
      routsum-=sir[0];
      goutsum-=sir[1];
      boutsum-=sir[2];
      if (x == 0) {
        vmin[y]=Math.min(y + r1,hm) * w;
      }
      p=x + vmin[y];
      sir[0]=r[p];
      sir[1]=g[p];
      sir[2]=b[p];
      rinsum+=sir[0];
      ginsum+=sir[1];
      binsum+=sir[2];
      rsum+=rinsum;
      gsum+=ginsum;
      bsum+=binsum;
      stackpointer=(stackpointer + 1) % div;
      sir=stack[stackpointer];
      routsum+=sir[0];
      goutsum+=sir[1];
      boutsum+=sir[2];
      rinsum-=sir[0];
      ginsum-=sir[1];
      binsum-=sir[2];
      yi+=w;
    }
  }
  bitmap.setPixels(pix,0,w,0,0,w,h);
  return (bitmap);
}"
4078,"/** 
 * This function must be invoked if ""default"" (1st) constructor is called or if you want to change the blurred layout. <p/> We make a fake ImageView with width and height MATCH_PARENT. This ImageView will host the blurred snapshot/bitmap.
 * @param layout A {@link android.widget.RelativeLayout} to take snapshot of it.
 * @param radius Blur radius
 */
public void init(final View layout,int radius){
  this.mLayout=layout;
  this.radius=radius;
  mBlurredImageView=new ImageView(context);
  RelativeLayout.LayoutParams params=new RelativeLayout.LayoutParams(RelativeLayout.LayoutParams.MATCH_PARENT,RelativeLayout.LayoutParams.MATCH_PARENT);
  mBlurredImageView.setLayoutParams(params);
  mBlurredImageView.setClickable(false);
  mBlurredImageView.setVisibility(View.GONE);
  ((RelativeLayout)this.mLayout).addView(mBlurredImageView);
}","/** 
 * This function must be invoked if ""default"" (1st) constructor is called or if you want to change the blurred layout. <p/> We make a fake ImageView with width and height MATCH_PARENT. This ImageView will host the blurred snapshot/bitmap.
 * @param layout A {@link android.widget.RelativeLayout} to take snapshot of it.
 * @param radius Blur radius
 */
public void init(final View layout,int radius){
  this.mLayout=layout;
  this.mBlurRadius=radius;
  mBlurredImageView=new ImageView(context);
  RelativeLayout.LayoutParams params=new RelativeLayout.LayoutParams(ViewGroup.LayoutParams.MATCH_PARENT,ViewGroup.LayoutParams.MATCH_PARENT);
  mBlurredImageView.setLayoutParams(params);
  mBlurredImageView.setClickable(false);
  mBlurredImageView.setVisibility(View.GONE);
  mBlurredImageView.setScaleType(ImageView.ScaleType.FIT_XY);
  ((RelativeLayout)this.mLayout).addView(mBlurredImageView);
}"
4079,"private void handleRecycle(){
  Drawable drawable=mBlurredImageView.getDrawable();
  if (drawable instanceof BitmapDrawable) {
    BitmapDrawable bitmapDrawable=((BitmapDrawable)drawable);
    Bitmap bitmap=bitmapDrawable.getBitmap();
    if (bitmap != null)     bitmap.recycle();
    mBlurredImageView.setImageBitmap(null);
  }
  render=true;
}","private void handleRecycle(){
  Drawable drawable=mBlurredImageView.getDrawable();
  if (drawable instanceof BitmapDrawable) {
    BitmapDrawable bitmapDrawable=((BitmapDrawable)drawable);
    Bitmap bitmap=bitmapDrawable.getBitmap();
    if (bitmap != null)     bitmap.recycle();
    mBlurredImageView.setImageBitmap(null);
  }
  prepareToRender=true;
}"
4080,"@Override public void onDrawerClosed(View view){
  render=true;
  mBlurredImageView.setVisibility(View.GONE);
}","@Override public void onDrawerClosed(View view){
  prepareToRender=true;
  mBlurredImageView.setVisibility(View.GONE);
}"
4081,"private Bitmap scaleBitmap(Bitmap myBitmap){
  final int maxSize=250;
  int outWidth;
  int outHeight;
  int inWidth=myBitmap.getWidth();
  int inHeight=myBitmap.getHeight();
  if (inWidth > inHeight) {
    outWidth=maxSize;
    outHeight=(inHeight * maxSize) / inWidth;
  }
 else {
    outHeight=maxSize;
    outWidth=(inWidth * maxSize) / inHeight;
  }
  return Bitmap.createScaledBitmap(myBitmap,outWidth,outHeight,false);
}","private Bitmap scaleBitmap(Bitmap myBitmap){
  int width=(int)(myBitmap.getWidth() / mDownScaleFactor);
  int height=(int)(myBitmap.getHeight() / mDownScaleFactor);
  return Bitmap.createScaledBitmap(myBitmap,width,height,false);
}"
4082,"/** 
 * Snapshots the specified layout and scale it using scaleBitmap() function then we blur the scaled bitmap with the preferred blur radius. Finally, we post it to our fake   {@link android.widget.ImageView}.
 */
private void render(){
  if (render) {
    render=false;
    Bitmap bitmap=loadBitmapFromView(mLayout);
    bitmap=scaleBitmap(bitmap);
    bitmap=Blur.fastblur(context,bitmap,radius);
    mBlurredImageView.setVisibility(View.VISIBLE);
    mBlurredImageView.setImageBitmap(bitmap);
  }
}","/** 
 * Snapshots the specified layout and scale it using scaleBitmap() function then we blur the scaled bitmap with the preferred blur radius. Finally, we post it to our fake   {@link android.widget.ImageView}.
 */
private void render(){
  if (prepareToRender) {
    prepareToRender=false;
    Bitmap bitmap=loadBitmapFromView(mLayout);
    bitmap=scaleBitmap(bitmap);
    bitmap=Blur.fastblur(context,bitmap,mBlurRadius,false);
    mBlurredImageView.setVisibility(View.VISIBLE);
    mBlurredImageView.setImageBitmap(bitmap);
  }
}"
4083,"public void setRadius(int radius){
  this.radius=radius;
}","public void setRadius(int radius){
  mBlurRadius=radius < 1 ? 1 : radius;
}"
4084,"@Override protected void onActivityResult(int requestCode,int resuleCode,Intent intent){
  super.onActivityResult(requestCode,resuleCode,intent);
  if (resuleCode == Activity.RESULT_OK) {
    if (requestCode == INTENT_REQUEST_GET_IMAGES || requestCode == INTENT_REQUEST_GET_N_IMAGES) {
      Parcelable[] parcelableUris=intent.getParcelableArrayExtra(ImagePickerActivity.EXTRA_IMAGE_URIS);
      if (parcelableUris == null) {
        return;
      }
      Uri[] uris=new Uri[parcelableUris.length];
      System.arraycopy(parcelableUris,0,uris,0,parcelableUris.length);
      if (uris != null) {
        for (        Uri uri : uris) {
          Log.i(TAG,""String_Node_Str"" + uri);
          mMedia.add(uri);
        }
        showMedia();
      }
    }
  }
}","@Override protected void onActivityResult(int requestCode,int resuleCode,Intent intent){
  super.onActivityResult(requestCode,resuleCode,intent);
  if (resuleCode == Activity.RESULT_OK) {
    if (requestCode == INTENT_REQUEST_GET_IMAGES || requestCode == INTENT_REQUEST_GET_N_IMAGES) {
      Parcelable[] parcelableUris=intent.getParcelableArrayExtra(ImagePickerActivity.EXTRA_IMAGE_URIS);
      int[] parcelableOrientations=intent.getIntArrayExtra((ImagePickerActivity.EXTRA_IMAGE_ORIENTATIONS));
      if (parcelableUris == null) {
        return;
      }
      Uri[] uris=new Uri[parcelableUris.length];
      int[] orientations=new int[parcelableUris.length];
      System.arraycopy(parcelableUris,0,uris,0,parcelableUris.length);
      System.arraycopy(parcelableOrientations,0,orientations,0,parcelableOrientations.length);
      if (uris != null) {
        for (int i=0; i < orientations.length; i++) {
          mMediaImages.add(new Image(uris[i],orientations[i]));
        }
        showMedia();
      }
    }
  }
}"
4085,"private void showMedia(){
  mSelectedImagesContainer.removeAllViews();
  Iterator<Uri> iterator=mMedia.iterator();
  ImageInternalFetcher imageFetcher=new ImageInternalFetcher(this,500);
  while (iterator.hasNext()) {
    Uri uri=iterator.next();
    Log.i(TAG,""String_Node_Str"" + uri);
    if (mMedia.size() >= 1) {
      mSelectedImagesContainer.setVisibility(View.VISIBLE);
    }
    View imageHolder=LayoutInflater.from(this).inflate(R.layout.media_layout,null);
    ImageView thumbnail=(ImageView)imageHolder.findViewById(R.id.media_image);
    if (!uri.toString().contains(""String_Node_Str"")) {
      uri=Uri.fromFile(new File(uri.toString()));
    }
    imageFetcher.loadImage(uri,thumbnail);
    mSelectedImagesContainer.addView(imageHolder);
    int wdpx=(int)TypedValue.applyDimension(TypedValue.COMPLEX_UNIT_DIP,300,getResources().getDisplayMetrics());
    int htpx=(int)TypedValue.applyDimension(TypedValue.COMPLEX_UNIT_DIP,200,getResources().getDisplayMetrics());
    thumbnail.setLayoutParams(new FrameLayout.LayoutParams(wdpx,htpx));
  }
}","private void showMedia(){
  mSelectedImagesContainer.removeAllViews();
  Iterator<Image> iterator=mMediaImages.iterator();
  ImageInternalFetcher imageFetcher=new ImageInternalFetcher(this,500);
  while (iterator.hasNext()) {
    Image image=iterator.next();
    Log.i(TAG,""String_Node_Str"" + image);
    if (mMedia.size() >= 1) {
      mSelectedImagesContainer.setVisibility(View.VISIBLE);
    }
    View imageHolder=LayoutInflater.from(this).inflate(R.layout.media_layout,null);
    ImageView thumbnail=(ImageView)imageHolder.findViewById(R.id.media_image);
    if (!image.mUri.toString().contains(""String_Node_Str"")) {
      image.mUri=Uri.fromFile(new File(image.mUri.toString()));
    }
    imageFetcher.loadImage(image.mUri,thumbnail,image.mOrientation);
    mSelectedImagesContainer.addView(imageHolder);
    int wdpx=(int)TypedValue.applyDimension(TypedValue.COMPLEX_UNIT_DIP,300,getResources().getDisplayMetrics());
    int htpx=(int)TypedValue.applyDimension(TypedValue.COMPLEX_UNIT_DIP,200,getResources().getDisplayMetrics());
    thumbnail.setLayoutParams(new FrameLayout.LayoutParams(wdpx,htpx));
  }
}"
4086,"@Override public View getView(int position,View convertView,ViewGroup parent){
  ViewHolder holder;
  if (convertView == null) {
    convertView=LayoutInflater.from(getContext()).inflate(R.layout.pp__grid_item_gallery_thumbnail,null);
    holder=new ViewHolder();
    holder.mThumbnail=(ImageView)convertView.findViewById(R.id.pp__thumbnail_image);
    convertView.setTag(holder);
  }
 else {
    holder=(ViewHolder)convertView.getTag();
  }
  Image image=getItem(position);
  boolean isSelected=mActivity.containsImage(image);
  ((FrameLayout)convertView).setForeground(isSelected ? getResources().getDrawable(R.drawable.gallery_photo_selected) : null);
  if (holder.mImage == null || !holder.mImage.equals(image)) {
    mActivity.mImageFetcher.loadImage(image.mUri,holder.mThumbnail);
    holder.mImage=image;
  }
  return convertView;
}","@Override public View getView(int position,View convertView,ViewGroup parent){
  ViewHolder holder;
  if (convertView == null) {
    convertView=LayoutInflater.from(getContext()).inflate(R.layout.pp__grid_item_gallery_thumbnail,null);
    holder=new ViewHolder();
    holder.mThumbnail=(ImageView)convertView.findViewById(R.id.pp__thumbnail_image);
    convertView.setTag(holder);
  }
 else {
    holder=(ViewHolder)convertView.getTag();
  }
  Image image=getItem(position);
  boolean isSelected=mActivity.containsImage(image);
  ((FrameLayout)convertView).setForeground(isSelected ? getResources().getDrawable(R.drawable.gallery_photo_selected) : null);
  if (holder.mImage == null || !holder.mImage.equals(image)) {
    mActivity.mImageFetcher.loadImage(image.mUri,holder.mThumbnail,image.mOrientation);
    holder.mImage=image;
  }
  return convertView;
}"
4087,"@Override public void onClick(View view){
  if (view.getId() == R.id.pp__btn_done) {
    Uri[] uris=new Uri[mSelectedImages.size()];
    int i=0;
    for (    Image img : mSelectedImages) {
      uris[i++]=img.mUri;
    }
    Intent intent=new Intent();
    intent.putExtra(EXTRA_IMAGE_URIS,uris);
    setResult(Activity.RESULT_OK,intent);
  }
 else   if (view.getId() == R.id.pp__btn_cancel) {
    setResult(Activity.RESULT_CANCELED);
  }
  finish();
}","@Override public void onClick(View view){
  if (view.getId() == R.id.pp__btn_done) {
    Uri[] uris=new Uri[mSelectedImages.size()];
    int[] orientations=new int[mSelectedImages.size()];
    int i=0;
    for (    Image img : mSelectedImages) {
      uris[i]=img.mUri;
      orientations[i++]=img.mOrientation;
    }
    Intent intent=new Intent();
    intent.putExtra(EXTRA_IMAGE_URIS,uris);
    intent.putExtra(EXTRA_IMAGE_ORIENTATIONS,orientations);
    setResult(Activity.RESULT_OK,intent);
  }
 else   if (view.getId() == R.id.pp__btn_cancel) {
    setResult(Activity.RESULT_CANCELED);
  }
  finish();
}"
4088,"public boolean addImage(Image image){
  if (mSelectedImages == null) {
    mSelectedImages=new HashSet<Image>();
  }
  if (mSelectedImages.size() == mConfig.getSelectionLimit()) {
    Toast.makeText(this,getString(R.string.n_images_selected,mConfig.getSelectionLimit()),Toast.LENGTH_SHORT).show();
    return false;
  }
 else {
    if (mSelectedImages.add(image)) {
      View rootView=LayoutInflater.from(ImagePickerActivity.this).inflate(R.layout.pp__list_item_selected_thumbnail,null);
      ImageView thumbnail=(ImageView)rootView.findViewById(R.id.pp__selected_photo);
      rootView.setTag(image.mUri);
      mImageFetcher.loadImage(image.mUri,thumbnail);
      mSelectedImagesContainer.addView(rootView,0);
      int px=(int)TypedValue.applyDimension(TypedValue.COMPLEX_UNIT_DIP,60,getResources().getDisplayMetrics());
      thumbnail.setLayoutParams(new FrameLayout.LayoutParams(px,px));
      if (mSelectedImages.size() >= 1) {
        mSelectedImagesContainer.setVisibility(View.VISIBLE);
        mSelectedImageEmptyMessage.setVisibility(View.GONE);
      }
      return true;
    }
  }
  return false;
}","public boolean addImage(Image image){
  if (mSelectedImages == null) {
    mSelectedImages=new HashSet<Image>();
  }
  if (mSelectedImages.size() == mConfig.getSelectionLimit()) {
    Toast.makeText(this,getString(R.string.n_images_selected,mConfig.getSelectionLimit()),Toast.LENGTH_SHORT).show();
    return false;
  }
 else {
    if (mSelectedImages.add(image)) {
      View rootView=LayoutInflater.from(ImagePickerActivity.this).inflate(R.layout.pp__list_item_selected_thumbnail,null);
      ImageView thumbnail=(ImageView)rootView.findViewById(R.id.pp__selected_photo);
      rootView.setTag(image.mUri);
      mImageFetcher.loadImage(image.mUri,thumbnail,image.mOrientation);
      mSelectedImagesContainer.addView(rootView,0);
      int px=(int)TypedValue.applyDimension(TypedValue.COMPLEX_UNIT_DIP,60,getResources().getDisplayMetrics());
      thumbnail.setLayoutParams(new FrameLayout.LayoutParams(px,px));
      if (mSelectedImages.size() >= 1) {
        mSelectedImagesContainer.setVisibility(View.VISIBLE);
        mSelectedImageEmptyMessage.setVisibility(View.GONE);
      }
      return true;
    }
  }
  return false;
}"
4089,"/** 
 * Load an image specified by the data parameter into an ImageView (override  {@link changer.nl.polypicker.utils.ImageWorker#processBitmap(Object)} to define the processing logic). A memory anddisk cache will be used if an  {@link nl.changer.polypicker.utils.ImageCache} has been added using{@link changer.nl.polypicker.utils.ImageWorker#addImageCache(android.support.v4.app.FragmentManager,nl.changer.polypicker.utils.ImageCache.ImageCacheParams)}. If the image is found in the memory cache, it is set immediately, otherwise an   {@link nl.changer.polypicker.utils.AsyncTask}will be created to asynchronously load the bitmap.
 * @param data The URL of the image to download.
 * @param imageView The ImageView to bind the downloaded image to.
 */
public void loadImage(Object data,ImageView imageView){
  if (data == null) {
    return;
  }
  BitmapDrawable value=null;
  if (mImageCache != null) {
    value=mImageCache.getBitmapFromMemCache(String.valueOf(data));
  }
  if (value != null) {
    imageView.setImageDrawable(value);
  }
 else   if (cancelPotentialWork(data,imageView)) {
    final BitmapWorkerTask task=new BitmapWorkerTask(data,imageView);
    final AsyncDrawable asyncDrawable=new AsyncDrawable(mResources,mLoadingBitmap,task);
    imageView.setImageDrawable(asyncDrawable);
    task.executeOnExecutor(AsyncTask.DUAL_THREAD_EXECUTOR);
  }
}","/** 
 * Load an image specified by the data parameter into an ImageView (override  {@link changer.nl.polypicker.utils.ImageWorker#processBitmap(Object)} to define the processing logic). A memory anddisk cache will be used if an  {@link nl.changer.polypicker.utils.ImageCache} has been added using{@link changer.nl.polypicker.utils.ImageWorker#addImageCache(android.support.v4.app.FragmentManager,nl.changer.polypicker.utils.ImageCache.ImageCacheParams)}. If the image is found in the memory cache, it is set immediately, otherwise an   {@link nl.changer.polypicker.utils.AsyncTask}will be created to asynchronously load the bitmap.
 * @param data The URL of the image to download.
 * @param imageView The ImageView to bind the downloaded image to.
 */
public void loadImage(Object data,ImageView imageView,int orientation){
  if (data == null) {
    return;
  }
  BitmapDrawable value=null;
  if (mImageCache != null) {
    value=mImageCache.getBitmapFromMemCache(String.valueOf(data));
  }
  if (value != null) {
    imageView.setImageDrawable(value);
    imageView.setRotation(orientation);
  }
 else   if (cancelPotentialWork(data,imageView)) {
    final BitmapWorkerTask task=new BitmapWorkerTask(data,imageView);
    final AsyncDrawable asyncDrawable=new AsyncDrawable(mResources,mLoadingBitmap,task);
    imageView.setImageDrawable(asyncDrawable);
    imageView.setRotation(orientation);
    task.executeOnExecutor(AsyncTask.DUAL_THREAD_EXECUTOR);
  }
}"
4090,"@Override public void close(){
  cleaner.clean();
}","@Override public void close(){
  if (cleaner != null)   cleaner.clean();
}"
4091,"public DatenFilm(){
  setupArr();
  filmSize=new MSLong(0);
  databaseFilmNumber=FILM_COUNTER.getAndIncrement();
  DatenFilmCleanupTask task=new DatenFilmCleanupTask(databaseFilmNumber);
  cleaner=Cleaner.create(this,task);
}","public DatenFilm(){
  setupArr();
  filmSize=new MSLong(0);
  databaseFilmNumber=FILM_COUNTER.getAndIncrement();
  if (Functions.getOs() != Functions.OperatingSystemType.WIN32) {
    DatenFilmCleanupTask task=new DatenFilmCleanupTask(databaseFilmNumber);
    cleaner=Cleaner.create(this,task);
  }
}"
4092,"private void init(){
  jButtonDelHistory.setIcon(Icons.ICON_BUTTON_DEL);
  jComboBoxPset.setModel(new DefaultComboBoxModel<>(Daten.listePset.getListeSpeichern().getObjectDataCombo()));
  jCheckBoxStarten.setSelected(Boolean.parseBoolean(MVConfig.get(MVConfig.Configs.SYSTEM_DIALOG_DOWNLOAD_D_STARTEN)));
  jCheckBoxStarten.addActionListener(e -> MVConfig.add(MVConfig.Configs.SYSTEM_DIALOG_DOWNLOAD_D_STARTEN,String.valueOf(jCheckBoxStarten.isSelected())));
  jButtonZiel.setIcon(Icons.ICON_BUTTON_FILE_OPEN);
  jButtonZiel.setText(""String_Node_Str"");
  if (Daten.listePset.getListeSpeichern().isEmpty()) {
    ok=false;
    beenden();
  }
  jButtonZiel.addActionListener(new ZielBeobachter());
  jButtonOk.addActionListener(e -> {
    if (check()) {
      beenden();
    }
  }
);
  getRootPane().setDefaultButton(jButtonOk);
  new EscBeenden(this){
    @Override public void beenden_(){
      ok=false;
      beenden();
    }
  }
;
  jButtonAbbrechen.addActionListener(e -> {
    ok=false;
    beenden();
  }
);
  if (pSet != null) {
    jComboBoxPset.setSelectedItem(pSet.arr[DatenPset.PROGRAMMSET_NAME]);
  }
 else {
    pSet=Daten.listePset.getListeSpeichern().get(jComboBoxPset.getSelectedIndex());
  }
  if (Daten.listePset.getListeSpeichern().size() == 1) {
    jLabelSet.setVisible(false);
    jComboBoxPset.setVisible(false);
    jComboBoxPset.setEnabled(false);
  }
 else {
    jComboBoxPset.addActionListener(e -> setupResolutionButtons());
  }
  jTextFieldSender.setText(""String_Node_Str"" + datenFilm.arr[DatenFilm.FILM_SENDER] + ""String_Node_Str""+ datenFilm.arr[DatenFilm.FILM_TITEL]);
  jTextFieldName.getDocument().addDocumentListener(new DocumentListener(){
    @Override public void insertUpdate(    DocumentEvent e){
      tus();
    }
    @Override public void removeUpdate(    DocumentEvent e){
      tus();
    }
    @Override public void changedUpdate(    DocumentEvent e){
      tus();
    }
    private void tus(){
      if (!stopBeob) {
        nameGeaendert=true;
        if (!jTextFieldName.getText().equals(FilenameUtils.checkDateiname(jTextFieldName.getText(),false))) {
          jTextFieldName.setBackground(MVColor.DOWNLOAD_FEHLER.color);
        }
 else {
          jTextFieldName.setBackground(javax.swing.UIManager.getDefaults().getColor(""String_Node_Str""));
        }
      }
    }
  }
);
  cbPathTextComponent.setOpaque(true);
  cbPathTextComponent.getDocument().addDocumentListener(new DocumentListener(){
    @Override public void insertUpdate(    DocumentEvent e){
      tus();
    }
    @Override public void removeUpdate(    DocumentEvent e){
      tus();
    }
    @Override public void changedUpdate(    DocumentEvent e){
      tus();
    }
    private void tus(){
      if (!stopBeob) {
        nameGeaendert=true;
        String s=cbPathTextComponent.getText();
        if (!s.equals(FilenameUtils.checkDateiname(s,true))) {
          jComboBoxPfad.getEditor().getEditorComponent().setBackground(MVColor.DOWNLOAD_FEHLER.color);
        }
 else {
          jComboBoxPfad.getEditor().getEditorComponent().setBackground(Color.WHITE);
        }
        calculateAndCheckDiskSpace();
      }
    }
  }
);
  jRadioButtonAufloesungHd.addActionListener(new BeobRadio());
  jRadioButtonAufloesungKlein.addActionListener(new BeobRadio());
  jRadioButtonAufloesungHoch.addActionListener(new BeobRadio());
  jRadioButtonAufloesungHd.hide(!datenFilm.arr[DatenFilm.FILM_URL_HD].isEmpty());
  jRadioButtonAufloesungKlein.setEnabled(!datenFilm.arr[DatenFilm.FILM_URL_KLEIN].isEmpty());
  jRadioButtonAufloesungHoch.setSelected(true);
  if (jRadioButtonAufloesungHd.isEnabled()) {
    dateiGroesse_HD=datenFilm.getDateigroesse(datenFilm.getUrlFuerAufloesung(DatenFilm.AUFLOESUNG_HD));
    if (!dateiGroesse_HD.isEmpty()) {
      jRadioButtonAufloesungHd.setText(jRadioButtonAufloesungHd.getText() + ""String_Node_Str"" + dateiGroesse_HD+ ""String_Node_Str"");
    }
  }
  dateiGroesse_Hoch=datenFilm.getDateigroesse(datenFilm.arr[DatenFilm.FILM_URL]);
  if (!dateiGroesse_Hoch.isEmpty()) {
    jRadioButtonAufloesungHoch.setText(jRadioButtonAufloesungHoch.getText() + ""String_Node_Str"" + dateiGroesse_Hoch+ ""String_Node_Str"");
  }
  if (jRadioButtonAufloesungKlein.isEnabled()) {
    dateiGroesse_Klein=datenFilm.getDateigroesse(datenFilm.getUrlFuerAufloesung(DatenFilm.AUFLOESUNG_KLEIN));
    if (!dateiGroesse_Klein.isEmpty()) {
      jRadioButtonAufloesungKlein.setText(jRadioButtonAufloesungKlein.getText() + ""String_Node_Str"" + dateiGroesse_Klein+ ""String_Node_Str"");
    }
  }
  jButtonDelHistory.addActionListener(e -> {
    MVConfig.add(MVConfig.Configs.SYSTEM_DIALOG_DOWNLOAD__PFADE_ZUM_SPEICHERN,""String_Node_Str"");
    jComboBoxPfad.setModel(new DefaultComboBoxModel<>(new String[]{orgPfad}));
  }
);
  jCheckBoxPfadSpeichern.setSelected(Boolean.parseBoolean(MVConfig.get(MVConfig.Configs.SYSTEM_DIALOG_DOWNLOAD__LETZTEN_PFAD_ANZEIGEN)));
  jCheckBoxPfadSpeichern.addActionListener(e -> MVConfig.add(MVConfig.Configs.SYSTEM_DIALOG_DOWNLOAD__LETZTEN_PFAD_ANZEIGEN,Boolean.toString(jCheckBoxPfadSpeichern.isSelected())));
  setupResolutionButtons();
  calculateAndCheckDiskSpace();
  nameGeaendert=false;
}","private void init(){
  jButtonDelHistory.setIcon(Icons.ICON_BUTTON_DEL);
  jComboBoxPset.setModel(new DefaultComboBoxModel<>(Daten.listePset.getListeSpeichern().getObjectDataCombo()));
  jCheckBoxStarten.setSelected(Boolean.parseBoolean(MVConfig.get(MVConfig.Configs.SYSTEM_DIALOG_DOWNLOAD_D_STARTEN)));
  jCheckBoxStarten.addActionListener(e -> MVConfig.add(MVConfig.Configs.SYSTEM_DIALOG_DOWNLOAD_D_STARTEN,String.valueOf(jCheckBoxStarten.isSelected())));
  jButtonZiel.setIcon(Icons.ICON_BUTTON_FILE_OPEN);
  jButtonZiel.setText(""String_Node_Str"");
  if (Daten.listePset.getListeSpeichern().isEmpty()) {
    ok=false;
    beenden();
  }
  jButtonZiel.addActionListener(new ZielBeobachter());
  jButtonOk.addActionListener(e -> {
    if (check()) {
      beenden();
    }
  }
);
  getRootPane().setDefaultButton(jButtonOk);
  new EscBeenden(this){
    @Override public void beenden_(){
      ok=false;
      beenden();
    }
  }
;
  jButtonAbbrechen.addActionListener(e -> {
    ok=false;
    beenden();
  }
);
  if (pSet != null) {
    jComboBoxPset.setSelectedItem(pSet.arr[DatenPset.PROGRAMMSET_NAME]);
  }
 else {
    pSet=Daten.listePset.getListeSpeichern().get(jComboBoxPset.getSelectedIndex());
  }
  if (Daten.listePset.getListeSpeichern().size() == 1) {
    jLabelSet.setVisible(false);
    jComboBoxPset.setVisible(false);
    jComboBoxPset.setEnabled(false);
  }
 else {
    jComboBoxPset.addActionListener(e -> setupResolutionButtons());
  }
  jTextFieldSender.setText(""String_Node_Str"" + datenFilm.arr[DatenFilm.FILM_SENDER] + ""String_Node_Str""+ datenFilm.arr[DatenFilm.FILM_TITEL]);
  jTextFieldName.getDocument().addDocumentListener(new DocumentListener(){
    @Override public void insertUpdate(    DocumentEvent e){
      tus();
    }
    @Override public void removeUpdate(    DocumentEvent e){
      tus();
    }
    @Override public void changedUpdate(    DocumentEvent e){
      tus();
    }
    private void tus(){
      if (!stopBeob) {
        nameGeaendert=true;
        if (!jTextFieldName.getText().equals(FilenameUtils.checkDateiname(jTextFieldName.getText(),false))) {
          jTextFieldName.setBackground(MVColor.DOWNLOAD_FEHLER.color);
        }
 else {
          jTextFieldName.setBackground(javax.swing.UIManager.getDefaults().getColor(""String_Node_Str""));
        }
      }
    }
  }
);
  cbPathTextComponent.setOpaque(true);
  cbPathTextComponent.getDocument().addDocumentListener(new DocumentListener(){
    @Override public void insertUpdate(    DocumentEvent e){
      tus();
    }
    @Override public void removeUpdate(    DocumentEvent e){
      tus();
    }
    @Override public void changedUpdate(    DocumentEvent e){
      tus();
    }
    private void tus(){
      if (!stopBeob) {
        nameGeaendert=true;
        String s=cbPathTextComponent.getText();
        if (!s.equals(FilenameUtils.checkDateiname(s,true))) {
          jComboBoxPfad.getEditor().getEditorComponent().setBackground(MVColor.DOWNLOAD_FEHLER.color);
        }
 else {
          jComboBoxPfad.getEditor().getEditorComponent().setBackground(Color.WHITE);
        }
        calculateAndCheckDiskSpace();
      }
    }
  }
);
  jRadioButtonAufloesungHd.addActionListener(new BeobRadio());
  jRadioButtonAufloesungKlein.addActionListener(new BeobRadio());
  jRadioButtonAufloesungHoch.addActionListener(new BeobRadio());
  jRadioButtonAufloesungHd.setVisible(!datenFilm.arr[DatenFilm.FILM_URL_HD].isEmpty());
  jRadioButtonAufloesungKlein.setEnabled(!datenFilm.arr[DatenFilm.FILM_URL_KLEIN].isEmpty());
  jRadioButtonAufloesungHoch.setSelected(true);
  if (jRadioButtonAufloesungHd.isEnabled()) {
    dateiGroesse_HD=datenFilm.getDateigroesse(datenFilm.getUrlFuerAufloesung(DatenFilm.AUFLOESUNG_HD));
    if (!dateiGroesse_HD.isEmpty()) {
      jRadioButtonAufloesungHd.setText(jRadioButtonAufloesungHd.getText() + ""String_Node_Str"" + dateiGroesse_HD+ ""String_Node_Str"");
    }
  }
  dateiGroesse_Hoch=datenFilm.getDateigroesse(datenFilm.arr[DatenFilm.FILM_URL]);
  if (!dateiGroesse_Hoch.isEmpty()) {
    jRadioButtonAufloesungHoch.setText(jRadioButtonAufloesungHoch.getText() + ""String_Node_Str"" + dateiGroesse_Hoch+ ""String_Node_Str"");
  }
  if (jRadioButtonAufloesungKlein.isEnabled()) {
    dateiGroesse_Klein=datenFilm.getDateigroesse(datenFilm.getUrlFuerAufloesung(DatenFilm.AUFLOESUNG_KLEIN));
    if (!dateiGroesse_Klein.isEmpty()) {
      jRadioButtonAufloesungKlein.setText(jRadioButtonAufloesungKlein.getText() + ""String_Node_Str"" + dateiGroesse_Klein+ ""String_Node_Str"");
    }
  }
  jButtonDelHistory.addActionListener(e -> {
    MVConfig.add(MVConfig.Configs.SYSTEM_DIALOG_DOWNLOAD__PFADE_ZUM_SPEICHERN,""String_Node_Str"");
    jComboBoxPfad.setModel(new DefaultComboBoxModel<>(new String[]{orgPfad}));
  }
);
  jCheckBoxPfadSpeichern.setSelected(Boolean.parseBoolean(MVConfig.get(MVConfig.Configs.SYSTEM_DIALOG_DOWNLOAD__LETZTEN_PFAD_ANZEIGEN)));
  jCheckBoxPfadSpeichern.addActionListener(e -> MVConfig.add(MVConfig.Configs.SYSTEM_DIALOG_DOWNLOAD__LETZTEN_PFAD_ANZEIGEN,Boolean.toString(jCheckBoxPfadSpeichern.isSelected())));
  setupResolutionButtons();
  calculateAndCheckDiskSpace();
  nameGeaendert=false;
}"
4093,"/** 
 * Main filtering routine
 */
@SuppressWarnings(""String_Node_Str"") public synchronized void filterListe(){
  final ListeFilme listeFilme=Daten.listeFilme;
  final ListeFilme listeRet=Daten.listeFilmeNachBlackList;
  loadCurrentFilterSettings();
  Duration.counterStart(""String_Node_Str"");
  listeRet.clear();
  if (listeFilme != null) {
    listeRet.setMeta(listeFilme);
    forEach(entry -> {
      entry.toLower();
      entry.hasPattern();
    }
);
    listeRet.neueFilme=false;
    Stream<DatenFilm> initialStream=listeFilme.parallelStream().filter(this::checkDate);
    filterList.clear();
    if (blacklistIsActive) {
      if (!doNotShowGeoBlockedFilms)       filterList.add(this::checkGeoBlockedFilm);
      if (!doNotShowFutureFilms)       filterList.add(this::checkIfFilmIsInFuture);
      filterList.add(this::checkFilmLength);
      if (!isEmpty())       filterList.add(this::applyBlacklistFilters);
      for (      Predicate pred : filterList) {
        initialStream=initialStream.filter(pred);
      }
    }
    final List<DatenFilm> col=initialStream.collect(Collectors.toList());
    col.parallelStream().filter(DatenFilm::isNew).findFirst().ifPresent(ignored -> listeRet.neueFilme=true);
    listeRet.addAll(col);
    col.clear();
    listeRet.themenLaden();
  }
  Duration.counterStop(""String_Node_Str"");
}","/** 
 * Main filtering routine
 */
@SuppressWarnings(""String_Node_Str"") public synchronized void filterListe(){
  final ListeFilme listeFilme=Daten.listeFilme;
  final ListeFilme listeRet=Daten.listeFilmeNachBlackList;
  loadCurrentFilterSettings();
  Duration.counterStart(""String_Node_Str"");
  listeRet.clear();
  if (listeFilme != null) {
    listeRet.setMeta(listeFilme);
    forEach(entry -> {
      entry.toLower();
      entry.hasPattern();
    }
);
    listeRet.neueFilme=false;
    Stream<DatenFilm> initialStream=listeFilme.parallelStream().filter(this::checkDate);
    filterList.clear();
    if (blacklistIsActive) {
      if (doNotShowGeoBlockedFilms)       filterList.add(this::checkGeoBlockedFilm);
      if (doNotShowFutureFilms)       filterList.add(this::checkIfFilmIsInFuture);
      filterList.add(this::checkFilmLength);
      if (!isEmpty())       filterList.add(this::applyBlacklistFilters);
      for (      Predicate pred : filterList) {
        initialStream=initialStream.filter(pred);
      }
    }
    final List<DatenFilm> col=initialStream.collect(Collectors.toList());
    col.parallelStream().filter(DatenFilm::isNew).findFirst().ifPresent(ignored -> listeRet.neueFilme=true);
    listeRet.addAll(col);
    col.clear();
    listeRet.themenLaden();
  }
  Duration.counterStop(""String_Node_Str"");
}"
4094,"private void hide(MVFrame frame,PanelVorlage panelVorlage){
  panelVorlage.solo=true;
  if (frame != null) {
    frame.dispose();
    frame=null;
  }
  if (tabContain(panelVorlage)) {
    jTabbedPane.remove(panelVorlage);
  }
}","private void hide(MVFrame frame,PanelVorlage panelVorlage){
  panelVorlage.solo=true;
  if (frame != null) {
    frame.dispose();
  }
  if (tabContain(panelVorlage)) {
    jTabbedPane.remove(panelVorlage);
  }
}"
4095,"public ArrayList<String> getPfade(){
  ArrayList<String> pfade=new ArrayList<>();
  for (  DatenAbo abo : this) {
    String s=abo.arr[DatenAbo.ABO_ZIELPFAD];
    if (!pfade.contains(s)) {
      pfade.add(abo.arr[DatenAbo.ABO_ZIELPFAD]);
    }
  }
  GermanStringSorter sorter=GermanStringSorter.getInstance();
  Collections.sort(pfade,sorter);
  return pfade;
}","public ArrayList<String> getPfade(){
  ArrayList<String> pfade=new ArrayList<>();
  for (  DatenAbo abo : this) {
    String s=abo.arr[DatenAbo.ABO_ZIELPFAD];
    if (!pfade.contains(s)) {
      pfade.add(abo.arr[DatenAbo.ABO_ZIELPFAD]);
    }
  }
  GermanStringSorter sorter=GermanStringSorter.getInstance();
  pfade.sort(sorter);
  return pfade;
}"
4096,"public void setAboFuerFilm(ListeFilme listeFilme,boolean aboLoeschen){
  Duration.counterStart(""String_Node_Str"");
  if (this.isEmpty() && aboLoeschen) {
    DatenFilm datenFilm;
    Iterator<DatenFilm> iteratorFilm=listeFilme.iterator();
    while (iteratorFilm.hasNext()) {
      datenFilm=iteratorFilm.next();
      datenFilm.arr[DatenFilm.FILM_ABO_NAME]=""String_Node_Str"";
      datenFilm.abo=null;
    }
    return;
  }
  this.stream().filter((datenAbo) -> (datenAbo.isEmpty())).forEach((datenAbo) -> {
    this.remove(datenAbo);
  }
);
  this.stream().forEach(datenAbo -> {
    if (datenAbo.arr[DatenAbo.ABO_TITEL].isEmpty()) {
      datenAbo.titel=LEER;
    }
 else {
      datenAbo.titel=Filter.isPattern(datenAbo.arr[DatenAbo.ABO_TITEL]) ? new String[]{datenAbo.arr[DatenAbo.ABO_TITEL]} : datenAbo.arr[DatenAbo.ABO_TITEL].toLowerCase().split(""String_Node_Str"");
    }
    if (datenAbo.arr[DatenAbo.ABO_THEMA_TITEL].isEmpty()) {
      datenAbo.thema=LEER;
    }
 else {
      datenAbo.thema=Filter.isPattern(datenAbo.arr[DatenAbo.ABO_THEMA_TITEL]) ? new String[]{datenAbo.arr[DatenAbo.ABO_THEMA_TITEL]} : datenAbo.arr[DatenAbo.ABO_THEMA_TITEL].toLowerCase().split(""String_Node_Str"");
    }
    if (datenAbo.arr[DatenAbo.ABO_IRGENDWO].isEmpty()) {
      datenAbo.irgendwo=LEER;
    }
 else {
      datenAbo.irgendwo=Filter.isPattern(datenAbo.arr[DatenAbo.ABO_IRGENDWO]) ? new String[]{datenAbo.arr[DatenAbo.ABO_IRGENDWO]} : datenAbo.arr[DatenAbo.ABO_IRGENDWO].toLowerCase().split(""String_Node_Str"");
    }
  }
);
  listeFilme.stream().parallel().forEach(film -> {
    film.arr[DatenFilm.FILM_ABO_NAME]=""String_Node_Str"";
    film.abo=null;
    try {
      DatenAbo aa=this.stream().filter(abo -> Filter.filterAufFilmPruefen(abo.arr[DatenAbo.ABO_SENDER],abo.arr[DatenAbo.ABO_THEMA],abo.titel,abo.thema,abo.irgendwo,abo.mindestdauerMinuten,abo.min,film,false)).findFirst().get();
      if (aa != null) {
        if (!Filter.laengePruefen(aa.mindestdauerMinuten,film.dauerL,aa.min)) {
          film.arr[DatenFilm.FILM_ABO_NAME]=aa.arr[DatenAbo.ABO_NAME] + (aa.min ? ""String_Node_Str"" : ""String_Node_Str"");
          film.abo=aa;
        }
 else {
          film.arr[DatenFilm.FILM_ABO_NAME]=aa.arr[DatenAbo.ABO_NAME];
          film.abo=aa;
        }
      }
    }
 catch (    NoSuchElementException ignore) {
    }
  }
);
  this.stream().forEach(datenAbo -> {
    datenAbo.titel=LEER;
    datenAbo.thema=LEER;
    datenAbo.irgendwo=LEER;
  }
);
  Duration.counterStop(""String_Node_Str"");
}","public void setAboFuerFilm(ListeFilme listeFilme,boolean aboLoeschen){
  Duration.counterStart(""String_Node_Str"");
  if (this.isEmpty() && aboLoeschen) {
    listeFilme.parallelStream().forEach(this::deleteAboInFilm);
    return;
  }
  this.stream().filter((datenAbo) -> (datenAbo.isEmpty())).forEach(this::remove);
  forEach(this::createAbo);
  listeFilme.parallelStream().forEach(this::assignAboToFilm);
  forEach(datenAbo -> {
    datenAbo.titel=LEER;
    datenAbo.thema=LEER;
    datenAbo.irgendwo=LEER;
  }
);
  Duration.counterStop(""String_Node_Str"");
}"
4097,"public void addObjectData(TModelAbo model,String sender){
  Object[] object;
  DatenAbo datenAbo;
  model.setRowCount(0);
  Iterator<DatenAbo> iterator=this.iterator();
  object=new Object[DatenAbo.MAX_ELEM];
  while (iterator.hasNext()) {
    datenAbo=iterator.next();
    if (sender.isEmpty() || sender.equals(datenAbo.arr[DatenAbo.ABO_SENDER])) {
      for (int m=0; m < DatenAbo.MAX_ELEM; ++m) {
        if (m == DatenAbo.ABO_NR) {
          object[m]=datenAbo.nr;
        }
 else         if (m == DatenAbo.ABO_MINDESTDAUER) {
          object[m]=datenAbo.mindestdauerMinuten;
        }
 else         if (m == DatenAbo.ABO_DOWN_DATUM) {
          object[m]=getDatumForObject(datenAbo.arr[DatenAbo.ABO_DOWN_DATUM]);
        }
 else         if (m == DatenAbo.ABO_EINGESCHALTET) {
          object[m]=""String_Node_Str"";
        }
 else         if (m == DatenAbo.ABO_MIN) {
          object[m]=datenAbo.min ? ""String_Node_Str"" : ""String_Node_Str"";
        }
 else         if (m != DatenAbo.ABO_NAME && !DatenAbo.anzeigen(m)) {
          object[m]=""String_Node_Str"";
        }
 else {
          object[m]=datenAbo.arr[m];
        }
      }
      model.addRow(object);
    }
  }
}","public void addObjectData(TModelAbo model,String sender){
  model.setRowCount(0);
  Object[] object=new Object[DatenAbo.MAX_ELEM];
  for (  DatenAbo datenAbo : this) {
    if (sender.isEmpty() || sender.equals(datenAbo.arr[DatenAbo.ABO_SENDER])) {
      for (int m=0; m < DatenAbo.MAX_ELEM; ++m) {
        if (m == DatenAbo.ABO_NR) {
          object[m]=datenAbo.nr;
        }
 else         if (m == DatenAbo.ABO_MINDESTDAUER) {
          object[m]=datenAbo.mindestdauerMinuten;
        }
 else         if (m == DatenAbo.ABO_DOWN_DATUM) {
          object[m]=getDatumForObject(datenAbo.arr[DatenAbo.ABO_DOWN_DATUM]);
        }
 else         if (m == DatenAbo.ABO_EINGESCHALTET) {
          object[m]=""String_Node_Str"";
        }
 else         if (m == DatenAbo.ABO_MIN) {
          object[m]=datenAbo.min ? ""String_Node_Str"" : ""String_Node_Str"";
        }
 else         if (m != DatenAbo.ABO_NAME && !DatenAbo.anzeigen(m)) {
          object[m]=""String_Node_Str"";
        }
 else {
          object[m]=datenAbo.arr[m];
        }
      }
      model.addRow(object);
    }
  }
}"
4098,"/** 
 * Return the number of Starts, which are queued in state INIT or RUN.
 * @return number of queued Starts.
 */
public synchronized int getNumberOfStartsNotFinished(){
  Iterator<DatenDownload> it=this.iterator();
  while (it.hasNext()) {
    Start s=it.next().start;
    if (s != null) {
      if (s.status < Start.STATUS_FERTIG) {
        return this.size();
      }
    }
  }
  return 0;
}","/** 
 * Return the number of Starts, which are queued in state INIT or RUN.
 * @return number of queued Starts.
 */
public synchronized int getNumberOfStartsNotFinished(){
  for (  DatenDownload datenDownload : this) {
    Start s=datenDownload.start;
    if (s != null) {
      if (s.status < Start.STATUS_FERTIG) {
        return this.size();
      }
    }
  }
  return 0;
}"
4099,"private boolean getDown(int max){
  int count=0;
  Iterator<DatenDownload> it=this.iterator();
  while (it.hasNext()) {
    Start s=it.next().start;
    if (s != null) {
      if (s.status == Start.STATUS_RUN) {
        ++count;
        if (count >= max) {
          return false;
        }
      }
    }
  }
  return true;
}","private boolean getDown(int max){
  int count=0;
  for (  DatenDownload datenDownload : this) {
    Start s=datenDownload.start;
    if (s != null) {
      if (s.status == Start.STATUS_RUN) {
        ++count;
        if (count >= max) {
          return false;
        }
      }
    }
  }
  return true;
}"
4100,"public ResetSettingsPanel(JFrame pparent,Daten ddaten){
  initComponents();
  parent=pparent;
  daten=ddaten;
  jButtonHilfeReset.setIcon(Icons.ICON_BUTTON_HELP);
  jButtonHilfeReset.addActionListener(e -> new DialogHilfe(Daten.mediathekGui,true,new GetFile().getHilfeSuchen(GetFile.PFAD_HILFETEXT_RESET)).setVisible(true));
  jButtonResetSets.addActionListener(e -> {
    Daten.listePset.clear();
    GuiFunktionenProgramme.addSetVorlagen(parent,daten,ListePsetVorlagen.getStandarset(parent,daten,true),false,true);
    Listener.notify(Listener.EREIGNIS_LISTE_PSET,ResetSettingsPanel.class.getSimpleName());
  }
);
  jButtonResetAll.addActionListener(e -> {
    int ret=JOptionPane.showConfirmDialog(parent,""String_Node_Str"",""String_Node_Str"",JOptionPane.YES_NO_OPTION);
    if (ret == JOptionPane.OK_OPTION) {
      Daten.RESET=true;
      Daten.mediathekGui.beenden(false,false);
    }
  }
);
}","public ResetSettingsPanel(JFrame pparent,Daten ddaten){
  initComponents();
  parent=pparent;
  daten=ddaten;
  jButtonHilfeReset.setIcon(Icons.ICON_BUTTON_HELP);
  jButtonHilfeReset.addActionListener(e -> new DialogHilfe(parent,true,new GetFile().getHilfeSuchen(GetFile.PFAD_HILFETEXT_RESET)).setVisible(true));
  jButtonResetSets.addActionListener(e -> {
    Daten.listePset.clear();
    GuiFunktionenProgramme.addSetVorlagen(parent,daten,ListePsetVorlagen.getStandarset(parent,daten,true),false,true);
    Listener.notify(Listener.EREIGNIS_LISTE_PSET,ResetSettingsPanel.class.getSimpleName());
  }
);
  jButtonResetAll.addActionListener(e -> {
    int ret=JOptionPane.showConfirmDialog(parent,""String_Node_Str"",""String_Node_Str"",JOptionPane.YES_NO_OPTION);
    if (ret == JOptionPane.OK_OPTION) {
      Daten.RESET=true;
      Daten.mediathekGui.beenden(false,false);
    }
  }
);
}"
4101,"public static final Daten getInstance(){
  return instance == null ? new Daten() : instance;
}","public static final Daten getInstance(){
  return instance == null ? instance=new Daten() : instance;
}"
4102,"public PanelFilmBeschreibung(JFrame pparent,Daten dd){
  initComponents();
  parent=pparent;
  daten=dd;
  jCheckBoxBeschreibung.setIcon(GetIcon.getProgramIcon(""String_Node_Str""));
  jCheckBoxBeschreibung.addActionListener(e -> {
    Daten.mVConfig.add(MVConfig.SYSTEM_PANEL_BESCHREIBUNG_ANZEIGEN,Boolean.FALSE.toString());
    ListenerMediathekView.notify(ListenerMediathekView.EREIGNIS_PANEL_BESCHREIBUNG_ANZEIGEN,PanelFilmBeschreibung.class.getSimpleName());
  }
);
  jXHyperlinkWebsite.addMouseListener(new BeobMausUrl(jXHyperlinkWebsite));
  jCheckBoxChange.setIcon(GetIcon.getProgramIcon(""String_Node_Str""));
  jCheckBoxChange.addActionListener(e -> {
    if (aktFilm != null) {
      String akt=aktFilm.arr[DatenFilm.FILM_BESCHREIBUNG_NR];
      new DialogFilmBeschreibung(parent,daten,aktFilm).setVisible(true);
      if (!aktFilm.arr[DatenFilm.FILM_BESCHREIBUNG_NR].equals(akt)) {
        setText();
        Daten.filmlisteSpeichern();
        ListenerMediathekView.notify(ListenerMediathekView.EREIGNIS_BESCHREIBUNG,PanelFilmBeschreibung.class.getSimpleName());
      }
    }
  }
);
  ListenerMediathekView.addListener(new ListenerMediathekView(ListenerMediathekView.EREIGNIS_FONT,PanelFilmBeschreibung.class.getSimpleName()){
    @Override public void ping(){
      setText();
    }
  }
);
}","public PanelFilmBeschreibung(JFrame pparent,Daten dd){
  initComponents();
  parent=pparent;
  daten=dd;
  jCheckBoxBeschreibung.setIcon(GetIcon.getProgramIcon(""String_Node_Str""));
  jCheckBoxBeschreibung.addActionListener(e -> {
    Daten.mVConfig.add(MVConfig.SYSTEM_PANEL_BESCHREIBUNG_ANZEIGEN,Boolean.FALSE.toString());
    ListenerMediathekView.notify(ListenerMediathekView.EREIGNIS_PANEL_BESCHREIBUNG_ANZEIGEN,PanelFilmBeschreibung.class.getSimpleName());
  }
);
  try {
    jXHyperlinkWebsite.setAction(new UrlHyperlinkAction(parent,""String_Node_Str""));
  }
 catch (  URISyntaxException ignored) {
    jXHyperlinkWebsite.setText(""String_Node_Str"");
  }
  jXHyperlinkWebsite.addMouseListener(new BeobMausUrl(jXHyperlinkWebsite));
  jCheckBoxChange.setIcon(GetIcon.getProgramIcon(""String_Node_Str""));
  jCheckBoxChange.addActionListener(e -> {
    if (aktFilm != null) {
      String akt=aktFilm.arr[DatenFilm.FILM_BESCHREIBUNG_NR];
      new DialogFilmBeschreibung(parent,daten,aktFilm).setVisible(true);
      if (!aktFilm.arr[DatenFilm.FILM_BESCHREIBUNG_NR].equals(akt)) {
        setText();
        Daten.filmlisteSpeichern();
        ListenerMediathekView.notify(ListenerMediathekView.EREIGNIS_BESCHREIBUNG,PanelFilmBeschreibung.class.getSimpleName());
      }
    }
  }
);
  ListenerMediathekView.addListener(new ListenerMediathekView(ListenerMediathekView.EREIGNIS_FONT,PanelFilmBeschreibung.class.getSimpleName()){
    @Override public void ping(){
      setText();
    }
  }
);
}"
4103,"/** 
 * like this @section('sidebar')
 */
private void collectOverwrittenSection(final PsiElement psiElement,@NotNull Collection<LineMarkerInfo> collection,@NotNull String sectionName,@NotNull LazyVirtualFileTemplateResolver resolver){
  final List<GotoRelatedItem> gotoRelatedItems=new ArrayList<>();
  for (  PsiElement psiElement1 : psiElement.getContainingFile().getChildren()) {
    PsiElement extendDirective=psiElement1.getFirstChild();
    if (extendDirective != null && extendDirective.getNode().getElementType() == BladeTokenTypes.EXTENDS_DIRECTIVE) {
      PsiElement bladeParameter=extendDirective.getNextSibling();
      if (bladeParameter instanceof BladePsiDirectiveParameter) {
        String extendTemplate=BladePsiUtil.getSection(bladeParameter);
        if (extendTemplate != null) {
          for (          VirtualFile virtualFile : resolver.resolveTemplateName(psiElement.getProject(),extendTemplate)) {
            PsiFile psiFile=PsiManager.getInstance(psiElement.getProject()).findFile(virtualFile);
            if (psiFile != null) {
              visitOverwrittenTemplateFile(psiFile,gotoRelatedItems,sectionName,resolver);
            }
          }
        }
      }
    }
  }
  if (gotoRelatedItems.size() == 0) {
    return;
  }
  collection.add(getRelatedPopover(""String_Node_Str"",""String_Node_Str"",psiElement,gotoRelatedItems,PhpIcons.OVERRIDES));
}","/** 
 * Like this @section('sidebar')
 */
@NotNull private Collection<LineMarkerInfo> collectOverwrittenSection(@NotNull LeafPsiElement psiElement,@NotNull String sectionName,@NotNull LazyVirtualFileTemplateResolver resolver){
  List<GotoRelatedItem> gotoRelatedItems=new ArrayList<>();
  for (  PsiElement psiElement1 : psiElement.getContainingFile().getChildren()) {
    PsiElement extendDirective=psiElement1.getFirstChild();
    if (extendDirective != null && extendDirective.getNode().getElementType() == BladeTokenTypes.EXTENDS_DIRECTIVE) {
      PsiElement bladeParameter=extendDirective.getNextSibling();
      if (bladeParameter instanceof BladePsiDirectiveParameter) {
        String extendTemplate=BladePsiUtil.getSection(bladeParameter);
        if (extendTemplate != null) {
          for (          VirtualFile virtualFile : resolver.resolveTemplateName(psiElement.getProject(),extendTemplate)) {
            PsiFile psiFile=PsiManager.getInstance(psiElement.getProject()).findFile(virtualFile);
            if (psiFile != null) {
              visitOverwrittenTemplateFile(psiFile,gotoRelatedItems,sectionName,resolver);
            }
          }
        }
      }
    }
  }
  if (gotoRelatedItems.size() == 0) {
    return Collections.emptyList();
  }
  return Collections.singletonList(getRelatedPopover(""String_Node_Str"",""String_Node_Str"",psiElement,gotoRelatedItems,PhpIcons.OVERRIDES));
}"
4104,"private LineMarkerInfo getRelatedPopover(String singleItemTitle,String singleItemTooltipPrefix,PsiElement lineMarkerTarget,Collection<GotoRelatedItem> gotoRelatedItems,Icon icon){
  String title=singleItemTitle;
  if (gotoRelatedItems.size() == 1) {
    String customName=gotoRelatedItems.iterator().next().getCustomName();
    if (customName != null) {
      title=String.format(singleItemTooltipPrefix,customName);
    }
  }
  return new LineMarkerInfo<>(lineMarkerTarget,lineMarkerTarget.getTextRange(),icon,6,new ConstantFunction<>(title),new RelatedPopupGotoLineMarker.NavigationHandler(gotoRelatedItems),GutterIconRenderer.Alignment.RIGHT);
}","@NotNull private LineMarkerInfo getRelatedPopover(@NotNull String singleItemTitle,@NotNull String singleItemTooltipPrefix,@NotNull PsiElement lineMarkerTarget,@NotNull Collection<GotoRelatedItem> gotoRelatedItems,@NotNull Icon icon){
  String title=singleItemTitle;
  if (gotoRelatedItems.size() == 1) {
    String customName=gotoRelatedItems.iterator().next().getCustomName();
    if (customName != null) {
      title=String.format(singleItemTooltipPrefix,customName);
    }
  }
  return new LineMarkerInfo<>(lineMarkerTarget,lineMarkerTarget.getTextRange(),icon,6,new ConstantFunction<>(title),new RelatedPopupGotoLineMarker.NavigationHandler(gotoRelatedItems),GutterIconRenderer.Alignment.RIGHT);
}"
4105,"/** 
 * Support: @stack('foobar')
 */
private void collectStackImplements(@NotNull PsiElement psiElement,@NotNull Collection<LineMarkerInfo> collection,@NotNull String sectionName,@NotNull LazyVirtualFileTemplateResolver resolver){
  Collection<String> templateNames=resolver.resolveTemplateName(psiElement.getContainingFile());
  if (templateNames.size() == 0) {
    return;
  }
  final List<GotoRelatedItem> gotoRelatedItems=new ArrayList<>();
  Set<VirtualFile> virtualFiles=BladeTemplateUtil.getExtendsImplementations(psiElement.getProject(),templateNames);
  if (virtualFiles.size() == 0) {
    return;
  }
  for (  VirtualFile virtualFile : virtualFiles) {
    PsiFile psiFile=PsiManager.getInstance(psiElement.getProject()).findFile(virtualFile);
    if (psiFile != null) {
      BladeTemplateUtil.visit(psiFile,BladeTokenTypes.PUSH_DIRECTIVE,parameter -> {
        if (sectionName.equalsIgnoreCase(parameter.getContent())) {
          gotoRelatedItems.add(new RelatedPopupGotoLineMarker.PopupGotoRelatedItem(parameter.getPsiElement()).withIcon(LaravelIcons.LARAVEL,LaravelIcons.LARAVEL));
        }
      }
);
    }
  }
  if (gotoRelatedItems.size() == 0) {
    return;
  }
  collection.add(getRelatedPopover(""String_Node_Str"",""String_Node_Str"",psiElement,gotoRelatedItems,PhpIcons.IMPLEMENTED));
}","/** 
 * Support: @stack('foobar')
 */
@NotNull private Collection<LineMarkerInfo> collectStackImplements(@NotNull LeafPsiElement psiElement,@NotNull String sectionName,@NotNull LazyVirtualFileTemplateResolver resolver){
  Collection<String> templateNames=resolver.resolveTemplateName(psiElement.getContainingFile());
  if (templateNames.size() == 0) {
    return Collections.emptyList();
  }
  List<GotoRelatedItem> gotoRelatedItems=new ArrayList<>();
  Set<VirtualFile> virtualFiles=BladeTemplateUtil.getExtendsImplementations(psiElement.getProject(),templateNames);
  if (virtualFiles.size() == 0) {
    return Collections.emptyList();
  }
  for (  VirtualFile virtualFile : virtualFiles) {
    PsiFile psiFile=PsiManager.getInstance(psiElement.getProject()).findFile(virtualFile);
    if (psiFile != null) {
      BladeTemplateUtil.visit(psiFile,BladeTokenTypes.PUSH_DIRECTIVE,parameter -> {
        if (sectionName.equalsIgnoreCase(parameter.getContent())) {
          gotoRelatedItems.add(new RelatedPopupGotoLineMarker.PopupGotoRelatedItem(parameter.getPsiElement()).withIcon(LaravelIcons.LARAVEL,LaravelIcons.LARAVEL));
        }
      }
);
    }
  }
  if (gotoRelatedItems.size() == 0) {
    return Collections.emptyList();
  }
  return Collections.singletonList(getRelatedPopover(""String_Node_Str"",""String_Node_Str"",psiElement,gotoRelatedItems,PhpIcons.IMPLEMENTED));
}"
4106,"private void visitOverwrittenTemplateFile(final PsiFile psiFile,final List<GotoRelatedItem> gotoRelatedItems,@NotNull String sectionName,int depth,@NotNull LazyVirtualFileTemplateResolver resolver){
  if (depth-- <= 0) {
    return;
  }
  BladeTemplateUtil.DirectiveParameterVisitor visitor=parameter -> {
    if (sectionName.equalsIgnoreCase(parameter.getContent())) {
      gotoRelatedItems.add(new RelatedPopupGotoLineMarker.PopupGotoRelatedItem(parameter.getPsiElement()).withIcon(LaravelIcons.LARAVEL,LaravelIcons.LARAVEL));
    }
  }
;
  BladeTemplateUtil.visitSection(psiFile,visitor);
  BladeTemplateUtil.visitYield(psiFile,visitor);
  final int finalDepth=depth;
  BladeTemplateUtil.visitExtends(psiFile,parameter -> {
    for (    VirtualFile virtualFile : resolver.resolveTemplateName(psiFile.getProject(),parameter.getContent())) {
      PsiFile templatePsiFile=PsiManager.getInstance(psiFile.getProject()).findFile(virtualFile);
      if (templatePsiFile != null) {
        visitOverwrittenTemplateFile(templatePsiFile,gotoRelatedItems,sectionName,finalDepth,resolver);
      }
    }
  }
);
}","private void visitOverwrittenTemplateFile(@NotNull PsiFile psiFile,@NotNull List<GotoRelatedItem> gotoRelatedItems,@NotNull String sectionName,int depth,@NotNull LazyVirtualFileTemplateResolver resolver){
  if (depth-- <= 0) {
    return;
  }
  BladeTemplateUtil.DirectiveParameterVisitor visitor=parameter -> {
    if (sectionName.equalsIgnoreCase(parameter.getContent())) {
      gotoRelatedItems.add(new RelatedPopupGotoLineMarker.PopupGotoRelatedItem(parameter.getPsiElement()).withIcon(LaravelIcons.LARAVEL,LaravelIcons.LARAVEL));
    }
  }
;
  BladeTemplateUtil.visitSection(psiFile,visitor);
  BladeTemplateUtil.visitYield(psiFile,visitor);
  final int finalDepth=depth;
  BladeTemplateUtil.visitExtends(psiFile,parameter -> {
    for (    VirtualFile virtualFile : resolver.resolveTemplateName(psiFile.getProject(),parameter.getContent())) {
      PsiFile templatePsiFile=PsiManager.getInstance(psiFile.getProject()).findFile(virtualFile);
      if (templatePsiFile != null) {
        visitOverwrittenTemplateFile(templatePsiFile,gotoRelatedItems,sectionName,finalDepth,resolver);
      }
    }
  }
);
}"
4107,"/** 
 * Support: @push('foobar')
 */
private void collectPushOverwrites(@NotNull PsiElement psiElement,@NotNull Collection<LineMarkerInfo> collection,@NotNull String sectionName){
  final List<GotoRelatedItem> gotoRelatedItems=new ArrayList<>();
  BladeTemplateUtil.visitUpPath(psiElement.getContainingFile(),10,parameter -> {
    if (sectionName.equalsIgnoreCase(parameter.getContent())) {
      gotoRelatedItems.add(new RelatedPopupGotoLineMarker.PopupGotoRelatedItem(parameter.getPsiElement()).withIcon(LaravelIcons.LARAVEL,LaravelIcons.LARAVEL));
    }
  }
,BladeTokenTypes.STACK_DIRECTIVE);
  if (gotoRelatedItems.size() == 0) {
    return;
  }
  collection.add(getRelatedPopover(""String_Node_Str"",""String_Node_Str"",psiElement,gotoRelatedItems,PhpIcons.OVERRIDES));
}","/** 
 * Support: @push('foobar')
 */
@NotNull private Collection<LineMarkerInfo> collectPushOverwrites(@NotNull LeafPsiElement psiElement,@NotNull String sectionName){
  final List<GotoRelatedItem> gotoRelatedItems=new ArrayList<>();
  BladeTemplateUtil.visitUpPath(psiElement.getContainingFile(),10,parameter -> {
    if (sectionName.equalsIgnoreCase(parameter.getContent())) {
      gotoRelatedItems.add(new RelatedPopupGotoLineMarker.PopupGotoRelatedItem(parameter.getPsiElement()).withIcon(LaravelIcons.LARAVEL,LaravelIcons.LARAVEL));
    }
  }
,BladeTokenTypes.STACK_DIRECTIVE);
  if (gotoRelatedItems.size() == 0) {
    return Collections.emptyList();
  }
  return Collections.singletonList(getRelatedPopover(""String_Node_Str"",""String_Node_Str"",psiElement,gotoRelatedItems,PhpIcons.OVERRIDES));
}"
4108,"@Override public void collectSlowLineMarkers(@NotNull List<PsiElement> psiElements,@NotNull Collection<LineMarkerInfo> collection){
  if (psiElements.size() == 0) {
    return;
  }
  Project project=psiElements.get(0).getProject();
  if (!LaravelProjectComponent.isEnabled(project)) {
    return;
  }
  LazyVirtualFileTemplateResolver resolver=null;
  for (  PsiElement psiElement : psiElements) {
    if (psiElement instanceof PsiFile) {
      if (resolver == null)       resolver=new LazyVirtualFileTemplateResolver();
      collectTemplateFileRelatedFiles((PsiFile)psiElement,collection,resolver);
    }
 else     if (psiElement.getNode().getElementType() == BladeTokenTypes.SECTION_DIRECTIVE) {
      Pair<PsiElement,String> section=extractSectionParameter(psiElement);
      if (section != null) {
        if (resolver == null) {
          resolver=new LazyVirtualFileTemplateResolver();
        }
        collectOverwrittenSection(section.getFirst(),collection,section.getSecond(),resolver);
        collectImplementsSection(section.getFirst(),collection,section.getSecond(),resolver);
      }
    }
 else     if (psiElement.getNode().getElementType() == BladeTokenTypes.YIELD_DIRECTIVE) {
      Pair<PsiElement,String> section=extractSectionParameter(psiElement);
      if (section != null) {
        if (resolver == null) {
          resolver=new LazyVirtualFileTemplateResolver();
        }
        collectImplementsSection(section.getFirst(),collection,section.getSecond(),resolver);
      }
    }
 else     if (psiElement.getNode().getElementType() == BladeTokenTypes.STACK_DIRECTIVE) {
      Pair<PsiElement,String> section=extractSectionParameter(psiElement);
      if (section != null) {
        if (resolver == null) {
          resolver=new LazyVirtualFileTemplateResolver();
        }
        collectStackImplements(section.getFirst(),collection,section.getSecond(),resolver);
      }
    }
 else     if (psiElement.getNode().getElementType() == BladeTokenTypes.PUSH_DIRECTIVE) {
      Pair<PsiElement,String> section=extractSectionParameter(psiElement);
      if (section != null) {
        if (resolver == null) {
          resolver=new LazyVirtualFileTemplateResolver();
        }
        collectPushOverwrites(section.getFirst(),collection,section.getSecond());
      }
    }
 else     if (psiElement.getNode().getElementType() == BladeTokenTypes.SLOT_DIRECTIVE) {
      Pair<PsiElement,String> section=extractSectionParameter(psiElement);
      if (section != null) {
        if (resolver == null) {
          resolver=new LazyVirtualFileTemplateResolver();
        }
        collectSlotOverwrites(section.getFirst(),collection,section.getSecond(),resolver);
      }
    }
  }
}","@Override public void collectSlowLineMarkers(@NotNull List<PsiElement> psiElements,@NotNull Collection<LineMarkerInfo> lineMarkers){
  if (psiElements.size() == 0) {
    return;
  }
  Project project=psiElements.get(0).getProject();
  if (!LaravelProjectComponent.isEnabled(project)) {
    return;
  }
  LazyVirtualFileTemplateResolver resolver=null;
  for (  PsiElement psiElement : psiElements) {
    if (psiElement instanceof PsiFile) {
      if (resolver == null) {
        resolver=new LazyVirtualFileTemplateResolver();
      }
      lineMarkers.addAll(collectTemplateFileRelatedFiles((PsiFile)psiElement,resolver));
    }
 else     if (psiElement instanceof LeafPsiElement) {
      if (psiElement.getNode().getElementType() == BladeTokenTypes.SECTION_DIRECTIVE) {
        Pair<BladePsiDirectiveParameter,String> section=extractSectionParameter(psiElement);
        if (section != null) {
          if (resolver == null) {
            resolver=new LazyVirtualFileTemplateResolver();
          }
          lineMarkers.addAll(collectOverwrittenSection((LeafPsiElement)psiElement,section.getSecond(),resolver));
          lineMarkers.addAll(collectImplementsSection((LeafPsiElement)psiElement,section.getSecond(),resolver));
        }
      }
 else       if (psiElement.getNode().getElementType() == BladeTokenTypes.YIELD_DIRECTIVE) {
        Pair<BladePsiDirectiveParameter,String> section=extractSectionParameter(psiElement);
        if (section != null) {
          if (resolver == null) {
            resolver=new LazyVirtualFileTemplateResolver();
          }
          lineMarkers.addAll(collectImplementsSection((LeafPsiElement)psiElement,section.getSecond(),resolver));
        }
      }
 else       if (psiElement.getNode().getElementType() == BladeTokenTypes.STACK_DIRECTIVE) {
        Pair<BladePsiDirectiveParameter,String> section=extractSectionParameter(psiElement);
        if (section != null) {
          if (resolver == null) {
            resolver=new LazyVirtualFileTemplateResolver();
          }
          lineMarkers.addAll(collectStackImplements((LeafPsiElement)psiElement,section.getSecond(),resolver));
        }
      }
 else       if (psiElement.getNode().getElementType() == BladeTokenTypes.PUSH_DIRECTIVE) {
        Pair<BladePsiDirectiveParameter,String> section=extractSectionParameter(psiElement);
        if (section != null) {
          if (resolver == null) {
            resolver=new LazyVirtualFileTemplateResolver();
          }
          lineMarkers.addAll(collectPushOverwrites((LeafPsiElement)psiElement,section.getSecond()));
        }
      }
 else       if (psiElement.getNode().getElementType() == BladeTokenTypes.SLOT_DIRECTIVE) {
        Pair<BladePsiDirectiveParameter,String> section=extractSectionParameter(psiElement);
        if (section != null) {
          if (resolver == null) {
            resolver=new LazyVirtualFileTemplateResolver();
          }
          lineMarkers.addAll(collectSlotOverwrites((LeafPsiElement)psiElement,section.getFirst(),section.getSecond(),resolver));
        }
      }
    }
  }
}"
4109,"/** 
 * Support: @slot('foobar')
 */
private void collectSlotOverwrites(@NotNull PsiElement psiElement,@NotNull Collection<LineMarkerInfo> collection,@NotNull String sectionName,@NotNull LazyVirtualFileTemplateResolver resolver){
  if (!(psiElement instanceof BladePsiDirectiveParameter)) {
    return;
  }
  String component=BladePsiUtil.findComponentForSlotScope((BladePsiDirectiveParameter)psiElement);
  if (component == null) {
    return;
  }
  List<GotoRelatedItem> gotoRelatedItems=new ArrayList<>();
  for (  VirtualFile virtualFile : resolver.resolveTemplateName(psiElement.getProject(),component)) {
    PsiFile file=PsiManager.getInstance(psiElement.getProject()).findFile(virtualFile);
    if (file == null) {
      continue;
    }
    gotoRelatedItems.addAll(BladePsiUtil.collectPrintBlockVariableTargets(file,sectionName).stream().map((Function<PsiElement,GotoRelatedItem>)element -> new RelatedPopupGotoLineMarker.PopupGotoRelatedItem(element).withIcon(LaravelIcons.LARAVEL,LaravelIcons.LARAVEL)).collect(Collectors.toList()));
  }
  if (gotoRelatedItems.size() == 0) {
    return;
  }
  collection.add(getRelatedPopover(""String_Node_Str"",""String_Node_Str"",psiElement,gotoRelatedItems,PhpIcons.OVERRIDES));
}","/** 
 * Support: @slot('foobar')
 */
@NotNull private Collection<LineMarkerInfo> collectSlotOverwrites(@NotNull LeafPsiElement psiElement,@NotNull BladePsiDirectiveParameter parameter,@NotNull String sectionName,@NotNull LazyVirtualFileTemplateResolver resolver){
  String component=BladePsiUtil.findComponentForSlotScope(parameter);
  if (component == null) {
    return Collections.emptyList();
  }
  List<GotoRelatedItem> gotoRelatedItems=new ArrayList<>();
  for (  VirtualFile virtualFile : resolver.resolveTemplateName(psiElement.getProject(),component)) {
    PsiFile file=PsiManager.getInstance(psiElement.getProject()).findFile(virtualFile);
    if (file == null) {
      continue;
    }
    gotoRelatedItems.addAll(BladePsiUtil.collectPrintBlockVariableTargets(file,sectionName).stream().map((Function<PsiElement,GotoRelatedItem>)element -> new RelatedPopupGotoLineMarker.PopupGotoRelatedItem(element).withIcon(LaravelIcons.LARAVEL,LaravelIcons.LARAVEL)).collect(Collectors.toList()));
  }
  if (gotoRelatedItems.size() == 0) {
    return Collections.emptyList();
  }
  return Collections.singletonList(getRelatedPopover(""String_Node_Str"",""String_Node_Str"",psiElement,gotoRelatedItems,PhpIcons.OVERRIDES));
}"
4110,"/** 
 * Find all sub implementations of a section that are overwritten by an extends tag Possible targets are: @section('sidebar')
 */
private void collectImplementsSection(@NotNull PsiElement psiElement,@NotNull Collection<LineMarkerInfo> collection,@NotNull String sectionName,@NotNull LazyVirtualFileTemplateResolver resolver){
  Collection<String> templateNames=resolver.resolveTemplateName(psiElement.getContainingFile());
  if (templateNames.size() == 0) {
    return;
  }
  Collection<GotoRelatedItem> gotoRelatedItems=new ArrayList<>();
  Set<VirtualFile> virtualFiles=BladeTemplateUtil.getExtendsImplementations(psiElement.getProject(),templateNames);
  if (virtualFiles.size() == 0) {
    return;
  }
  for (  VirtualFile virtualFile : virtualFiles) {
    PsiFile psiFile=PsiManager.getInstance(psiElement.getProject()).findFile(virtualFile);
    if (psiFile != null) {
      BladeTemplateUtil.visitSection(psiFile,parameter -> {
        if (sectionName.equalsIgnoreCase(parameter.getContent())) {
          gotoRelatedItems.add(new RelatedPopupGotoLineMarker.PopupGotoRelatedItem(parameter.getPsiElement()).withIcon(LaravelIcons.LARAVEL,LaravelIcons.LARAVEL));
        }
      }
);
    }
  }
  if (gotoRelatedItems.size() == 0) {
    return;
  }
  collection.add(getRelatedPopover(""String_Node_Str"",""String_Node_Str"",psiElement,gotoRelatedItems,PhpIcons.IMPLEMENTED));
}","/** 
 * Find all sub implementations of a section that are overwritten by an extends tag Possible targets are: @section('sidebar')
 */
@NotNull private Collection<LineMarkerInfo> collectImplementsSection(@NotNull LeafPsiElement psiElement,@NotNull String sectionName,@NotNull LazyVirtualFileTemplateResolver resolver){
  Collection<String> templateNames=resolver.resolveTemplateName(psiElement.getContainingFile());
  if (templateNames.size() == 0) {
    return Collections.emptyList();
  }
  Collection<GotoRelatedItem> gotoRelatedItems=new ArrayList<>();
  Set<VirtualFile> virtualFiles=BladeTemplateUtil.getExtendsImplementations(psiElement.getProject(),templateNames);
  if (virtualFiles.size() == 0) {
    return Collections.emptyList();
  }
  for (  VirtualFile virtualFile : virtualFiles) {
    PsiFile psiFile=PsiManager.getInstance(psiElement.getProject()).findFile(virtualFile);
    if (psiFile != null) {
      BladeTemplateUtil.visitSection(psiFile,parameter -> {
        if (sectionName.equalsIgnoreCase(parameter.getContent())) {
          gotoRelatedItems.add(new RelatedPopupGotoLineMarker.PopupGotoRelatedItem(parameter.getPsiElement()).withIcon(LaravelIcons.LARAVEL,LaravelIcons.LARAVEL));
        }
      }
);
    }
  }
  if (gotoRelatedItems.size() == 0) {
    return Collections.emptyList();
  }
  return Collections.singletonList(getRelatedPopover(""String_Node_Str"",""String_Node_Str"",psiElement,gotoRelatedItems,PhpIcons.IMPLEMENTED));
}"
4111,"/** 
 * Extract parameter: @foobar('my_value')
 */
@Nullable private Pair<PsiElement,String> extractSectionParameter(@NotNull PsiElement psiElement){
  PsiElement nextSibling=psiElement.getNextSibling();
  if (nextSibling instanceof BladePsiDirectiveParameter) {
    String sectionName=BladePsiUtil.getSection(nextSibling);
    if (sectionName != null && StringUtils.isNotBlank(sectionName)) {
      return Pair.create(nextSibling,sectionName);
    }
  }
  return null;
}","/** 
 * Extract parameter: @foobar('my_value')
 */
@Nullable private Pair<BladePsiDirectiveParameter,String> extractSectionParameter(@NotNull PsiElement psiElement){
  PsiElement nextSibling=psiElement.getNextSibling();
  if (nextSibling instanceof BladePsiDirectiveParameter) {
    String sectionName=BladePsiUtil.getSection(nextSibling);
    if (sectionName != null && StringUtils.isNotBlank(sectionName)) {
      return Pair.create((BladePsiDirectiveParameter)nextSibling,sectionName);
    }
  }
  return null;
}"
4112,"private void collectTemplateFileRelatedFiles(@NotNull PsiFile psiFile,@NotNull Collection<LineMarkerInfo> collection,@NotNull LazyVirtualFileTemplateResolver resolver){
  Collection<String> collectedTemplates=resolver.resolveTemplateName(psiFile);
  if (collectedTemplates.size() == 0) {
    return;
  }
  Set<String> templateNames=new HashSet<>();
  for (  String templateName : collectedTemplates) {
    templateNames.add(templateName);
    templateNames.add(templateName.toLowerCase());
  }
  templateNames.addAll(new HashSet<>(templateNames).stream().map(templateName -> templateName.replace(""String_Node_Str"",""String_Node_Str"")).collect(Collectors.toList()));
  AtomicBoolean includeLineMarker=new AtomicBoolean(false);
  for (  ID<String,Void> key : Arrays.asList(BladeExtendsStubIndex.KEY,BladeSectionStubIndex.KEY,BladeIncludeStubIndex.KEY,BladeEachStubIndex.KEY)) {
    for (    String templateName : templateNames) {
      FileBasedIndex.getInstance().getFilesWithKey(key,new HashSet<>(Collections.singletonList(templateName)),virtualFile -> {
        includeLineMarker.set(true);
        return false;
      }
,GlobalSearchScope.getScopeRestrictedByFileTypes(GlobalSearchScope.allScope(psiFile.getProject()),BladeFileType.INSTANCE));
    }
    if (includeLineMarker.get()) {
      break;
    }
  }
  if (includeLineMarker.get()) {
    NavigationGutterIconBuilder<PsiElement> builder=NavigationGutterIconBuilder.create(PhpIcons.IMPLEMENTED).setTargets(new TemplateIncludeCollectionNotNullLazyValue(psiFile.getProject(),templateNames)).setTooltipText(""String_Node_Str"");
    collection.add(builder.createLineMarkerInfo(psiFile));
  }
  boolean controllerLineMarker=false;
  for (  String templateName : templateNames) {
    Collection<VirtualFile> files=FileBasedIndex.getInstance().getContainingFiles(PhpTemplateUsageStubIndex.KEY,templateName,GlobalSearchScope.allScope(psiFile.getProject()));
    if (files.size() > 0) {
      controllerLineMarker=true;
      break;
    }
  }
  if (controllerLineMarker) {
    NavigationGutterIconBuilder<PsiElement> builder=NavigationGutterIconBuilder.create(LaravelIcons.TEMPLATE_CONTROLLER_LINE_MARKER).setTargets(new ControllerRenderViewCollectionNotNullLazyValue(psiFile.getProject(),templateNames)).setTooltipText(""String_Node_Str"");
    collection.add(builder.createLineMarkerInfo(psiFile));
  }
}","@NotNull private Collection<LineMarkerInfo> collectTemplateFileRelatedFiles(@NotNull PsiFile psiFile,@NotNull LazyVirtualFileTemplateResolver resolver){
  Collection<String> collectedTemplates=resolver.resolveTemplateName(psiFile);
  if (collectedTemplates.size() == 0) {
    return Collections.emptyList();
  }
  Set<String> templateNames=new HashSet<>();
  for (  String templateName : collectedTemplates) {
    templateNames.add(templateName);
    templateNames.add(templateName.toLowerCase());
  }
  templateNames.addAll(new HashSet<>(templateNames).stream().map(templateName -> templateName.replace(""String_Node_Str"",""String_Node_Str"")).collect(Collectors.toList()));
  AtomicBoolean includeLineMarker=new AtomicBoolean(false);
  for (  ID<String,Void> key : Arrays.asList(BladeExtendsStubIndex.KEY,BladeSectionStubIndex.KEY,BladeIncludeStubIndex.KEY,BladeEachStubIndex.KEY)) {
    for (    String templateName : templateNames) {
      FileBasedIndex.getInstance().getFilesWithKey(key,new HashSet<>(Collections.singletonList(templateName)),virtualFile -> {
        includeLineMarker.set(true);
        return false;
      }
,GlobalSearchScope.getScopeRestrictedByFileTypes(GlobalSearchScope.allScope(psiFile.getProject()),BladeFileType.INSTANCE));
    }
    if (includeLineMarker.get()) {
      break;
    }
  }
  Collection<LineMarkerInfo> lineMarkers=new ArrayList<>();
  if (includeLineMarker.get()) {
    NavigationGutterIconBuilder<PsiElement> builder=NavigationGutterIconBuilder.create(PhpIcons.IMPLEMENTED).setTargets(new TemplateIncludeCollectionNotNullLazyValue(psiFile.getProject(),templateNames)).setTooltipText(""String_Node_Str"");
    lineMarkers.add(builder.createLineMarkerInfo(psiFile));
  }
  boolean controllerLineMarker=false;
  for (  String templateName : templateNames) {
    Collection<VirtualFile> files=FileBasedIndex.getInstance().getContainingFiles(PhpTemplateUsageStubIndex.KEY,templateName,GlobalSearchScope.allScope(psiFile.getProject()));
    if (files.size() > 0) {
      controllerLineMarker=true;
      break;
    }
  }
  if (controllerLineMarker) {
    NavigationGutterIconBuilder<PsiElement> builder=NavigationGutterIconBuilder.create(LaravelIcons.TEMPLATE_CONTROLLER_LINE_MARKER).setTargets(new ControllerRenderViewCollectionNotNullLazyValue(psiFile.getProject(),templateNames)).setTooltipText(""String_Node_Str"");
    lineMarkers.add(builder.createLineMarkerInfo(psiFile));
  }
  return lineMarkers;
}"
4113,"public static void visitController(@NotNull final Project project,@NotNull ControllerVisitor visitor,@Nullable String prefix){
  Collection<PhpClass> allSubclasses=new HashSet<PhpClass>(){
{
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
    }
  }
;
  String ns=prefix;
  boolean prioritised=false;
  if (prefix == null) {
    ns=getDefaultNamespace(project);
    prioritised=true;
  }
  for (  PhpClass phpClass : allSubclasses) {
    if (phpClass.isAbstract()) {
      continue;
    }
    String className=phpClass.getPresentableFQN();
    if (className == null) {
      continue;
    }
    if (className.startsWith(ns + ""String_Node_Str"")) {
      className=className.substring(ns.length() + 1);
    }
    if (StringUtils.isNotBlank(className)) {
      visitor.visit(phpClass,className,prioritised);
    }
  }
}","public static void visitController(@NotNull final Project project,@NotNull ControllerVisitor visitor,@Nullable String prefix){
  Collection<PhpClass> allSubclasses=new HashSet<PhpClass>(){
{
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
    }
  }
;
  String ns=getDefaultNamespace(project) + ""String_Node_Str"";
  String prefixedNs=ns + (prefix != null && !prefix.equals(""String_Node_Str"") ? prefix + ""String_Node_Str"" : ""String_Node_Str"");
  for (  PhpClass phpClass : allSubclasses) {
    if (phpClass.isAbstract()) {
      continue;
    }
    String className=phpClass.getPresentableFQN();
    boolean prioritised=false;
    if (prefix != null && className.startsWith(prefixedNs)) {
      className=className.substring(prefixedNs.length());
      prioritised=true;
    }
 else     if (className.startsWith(ns)) {
      className=className.substring(ns.length());
    }
    if (StringUtils.isNotBlank(className)) {
      visitor.visit(phpClass,className,prioritised);
    }
  }
}"
4114,"public static void visitControllerActions(@NotNull final Project project,@NotNull ControllerActionVisitor visitor,@Nullable String prefix){
  Collection<PhpClass> allSubclasses=new HashSet<PhpClass>(){
{
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
    }
  }
;
  String ns=prefix;
  boolean prioritised=false;
  if (prefix == null) {
    ns=getDefaultNamespace(project);
    prioritised=true;
  }
  for (  PhpClass phpClass : allSubclasses) {
    if (!phpClass.isAbstract()) {
      for (      Method method : phpClass.getMethods()) {
        String className=phpClass.getPresentableFQN();
        if (className != null) {
          String methodName=method.getName();
          if (!method.isStatic() && method.getAccess().isPublic() && !methodName.startsWith(""String_Node_Str"")) {
            PhpClass phpTrait=method.getContainingClass();
            if (phpTrait == null || !(""String_Node_Str"".equals(phpTrait.getName()) || ""String_Node_Str"".equals(phpTrait.getName()) || ""String_Node_Str"".equals(phpTrait.getName()))) {
              if (className.startsWith(ns + ""String_Node_Str"")) {
                className=className.substring(ns.length() + 1);
              }
              if (StringUtils.isNotBlank(className)) {
                visitor.visit(method,className + ""String_Node_Str"" + methodName,prioritised);
              }
            }
          }
        }
      }
    }
  }
}","public static void visitControllerActions(@NotNull final Project project,@NotNull ControllerActionVisitor visitor,@Nullable String prefix){
  Collection<PhpClass> allSubclasses=new HashSet<PhpClass>(){
{
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
    }
  }
;
  String ns=getDefaultNamespace(project) + ""String_Node_Str"";
  String prefixedNs=ns + (prefix != null && !prefix.equals(""String_Node_Str"") ? prefix + ""String_Node_Str"" : ""String_Node_Str"");
  for (  PhpClass phpClass : allSubclasses) {
    if (!phpClass.isAbstract()) {
      for (      Method method : phpClass.getMethods()) {
        String className=phpClass.getPresentableFQN();
        String methodName=method.getName();
        if (!method.isStatic() && method.getAccess().isPublic() && !methodName.startsWith(""String_Node_Str"")) {
          PhpClass phpTrait=method.getContainingClass();
          if (phpTrait == null || !(""String_Node_Str"".equals(phpTrait.getName()) || ""String_Node_Str"".equals(phpTrait.getName()) || ""String_Node_Str"".equals(phpTrait.getName()))) {
            boolean prioritised=false;
            if (prefix != null && className.startsWith(prefixedNs)) {
              className=className.substring(prefixedNs.length());
              prioritised=true;
            }
 else             if (className.startsWith(ns)) {
              className=className.substring(ns.length());
            }
            if (StringUtils.isNotBlank(className)) {
              visitor.visit(method,className + ""String_Node_Str"" + methodName,prioritised);
            }
          }
        }
      }
    }
  }
}"
4115,"public void testRouteUsesInsideArray(){
  assertCompletionContains(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
}","public void testRouteUsesInsideArray(){
  assertCompletionContains(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
}"
4116,"public void testRouteParameter(){
  assertCompletionContains(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  assertNavigationMatch(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"",PlatformPatterns.psiElement(Method.class));
  assertNavigationMatch(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"",PlatformPatterns.psiElement(Method.class));
}","public void testRouteParameter(){
  assertCompletionContains(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  assertNavigationMatch(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"",PlatformPatterns.psiElement(Method.class));
  assertNavigationMatch(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"",PlatformPatterns.psiElement(Method.class));
}"
4117,"public static void visitController(@NotNull final Project project,@NotNull ControllerVisitor visitor,@Nullable String prefix){
  Collection<PhpClass> allSubclasses=new HashSet<PhpClass>(){
{
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
    }
  }
;
  String ns=prefix;
  boolean prioritised=false;
  if (prefix == null) {
    ns=getDefaultNamespace(project);
    prioritised=true;
  }
  for (  PhpClass phpClass : allSubclasses) {
    if (phpClass.isAbstract()) {
      continue;
    }
    String className=phpClass.getPresentableFQN();
    if (className == null) {
      continue;
    }
    if (className.startsWith(ns + ""String_Node_Str"")) {
      className=className.substring(ns.length() + 1);
    }
    if (StringUtils.isNotBlank(className)) {
      visitor.visit(phpClass,className,prioritised);
    }
  }
}","public static void visitController(@NotNull final Project project,@NotNull ControllerVisitor visitor,@Nullable String prefix){
  Collection<PhpClass> allSubclasses=new HashSet<PhpClass>(){
{
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
    }
  }
;
  String ns=getDefaultNamespace(project) + ""String_Node_Str"";
  String prefixedNs=ns + (prefix != null && !prefix.equals(""String_Node_Str"") ? prefix + ""String_Node_Str"" : ""String_Node_Str"");
  for (  PhpClass phpClass : allSubclasses) {
    if (phpClass.isAbstract()) {
      continue;
    }
    String className=phpClass.getPresentableFQN();
    boolean prioritised=false;
    if (prefix != null && className.startsWith(prefixedNs)) {
      className=className.substring(prefixedNs.length());
      prioritised=true;
    }
 else     if (className.startsWith(ns)) {
      className=className.substring(ns.length());
    }
    if (StringUtils.isNotBlank(className)) {
      visitor.visit(phpClass,className,prioritised);
    }
  }
}"
4118,"public static void visitControllerActions(@NotNull final Project project,@NotNull ControllerActionVisitor visitor,@Nullable String prefix){
  Collection<PhpClass> allSubclasses=new HashSet<PhpClass>(){
{
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
    }
  }
;
  String ns=prefix;
  boolean prioritised=false;
  if (prefix == null) {
    ns=getDefaultNamespace(project);
    prioritised=true;
  }
  for (  PhpClass phpClass : allSubclasses) {
    if (!phpClass.isAbstract()) {
      for (      Method method : phpClass.getMethods()) {
        String className=phpClass.getPresentableFQN();
        if (className != null) {
          String methodName=method.getName();
          if (!method.isStatic() && method.getAccess().isPublic() && !methodName.startsWith(""String_Node_Str"")) {
            PhpClass phpTrait=method.getContainingClass();
            if (phpTrait == null || !(""String_Node_Str"".equals(phpTrait.getName()) || ""String_Node_Str"".equals(phpTrait.getName()) || ""String_Node_Str"".equals(phpTrait.getName()))) {
              if (className.startsWith(ns + ""String_Node_Str"")) {
                className=className.substring(ns.length() + 1);
              }
              if (StringUtils.isNotBlank(className)) {
                visitor.visit(method,className + ""String_Node_Str"" + methodName,prioritised);
              }
            }
          }
        }
      }
    }
  }
}","public static void visitControllerActions(@NotNull final Project project,@NotNull ControllerActionVisitor visitor,@Nullable String prefix){
  Collection<PhpClass> allSubclasses=new HashSet<PhpClass>(){
{
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
      addAll(PhpIndex.getInstance(project).getAllSubclasses(""String_Node_Str""));
    }
  }
;
  String ns=getDefaultNamespace(project) + ""String_Node_Str"";
  String prefixedNs=ns + (prefix != null && !prefix.equals(""String_Node_Str"") ? prefix + ""String_Node_Str"" : ""String_Node_Str"");
  for (  PhpClass phpClass : allSubclasses) {
    if (!phpClass.isAbstract()) {
      for (      Method method : phpClass.getMethods()) {
        String className=phpClass.getPresentableFQN();
        String methodName=method.getName();
        if (!method.isStatic() && method.getAccess().isPublic() && !methodName.startsWith(""String_Node_Str"")) {
          PhpClass phpTrait=method.getContainingClass();
          if (phpTrait == null || !(""String_Node_Str"".equals(phpTrait.getName()) || ""String_Node_Str"".equals(phpTrait.getName()) || ""String_Node_Str"".equals(phpTrait.getName()))) {
            boolean prioritised=false;
            if (prefix != null && className.startsWith(prefixedNs)) {
              className=className.substring(prefixedNs.length());
              prioritised=true;
            }
 else             if (className.startsWith(ns)) {
              className=className.substring(ns.length());
            }
            if (StringUtils.isNotBlank(className)) {
              visitor.visit(method,className + ""String_Node_Str"" + methodName,prioritised);
            }
          }
        }
      }
    }
  }
}"
4119,"public void testRouteUsesInsideArray(){
  assertCompletionContains(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
}","public void testRouteUsesInsideArray(){
  assertCompletionContains(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
}"
4120,"public void testRouteParameter(){
  assertCompletionContains(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  assertNavigationMatch(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"",PlatformPatterns.psiElement(Method.class));
  assertNavigationMatch(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"",PlatformPatterns.psiElement(Method.class));
}","public void testRouteParameter(){
  assertCompletionContains(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  assertNavigationMatch(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"",PlatformPatterns.psiElement(Method.class));
  assertNavigationMatch(PhpFileType.INSTANCE,""String_Node_Str"" + ""String_Node_Str"",PlatformPatterns.psiElement(Method.class));
}"
4121,"@Override public boolean visitFile(@NotNull VirtualFile virtualFile){
  if (virtualFile.isDirectory() || !isTemplateFile(virtualFile)) {
    return true;
  }
  String filename=VfsUtil.getRelativePath(virtualFile,templateDir,'.');
  if (filename == null) {
    return true;
  }
  if (filename.endsWith(""String_Node_Str"")) {
    filename=filename.substring(0,filename.length() - 4);
  }
  if (filename.endsWith(""String_Node_Str"")) {
    filename=filename.substring(0,filename.length() - 6);
  }
  if (filename.endsWith(""String_Node_Str"")) {
    filename=filename.substring(0,filename.length() - ""String_Node_Str"".length());
  }
  if (templatePath.getNamespace() != null) {
    visitor.visit(virtualFile,templatePath.getNamespace() + ""String_Node_Str"" + filename);
  }
 else {
    visitor.visit(virtualFile,filename);
  }
  return true;
}","@Override public boolean visitFile(@NotNull VirtualFile virtualFile){
  if (virtualFile.isDirectory() || !isTemplateFile(virtualFile)) {
    return true;
  }
  String filename=VfsUtil.getRelativePath(virtualFile,templateDir,'.');
  if (filename == null) {
    return true;
  }
  if (filename.endsWith(""String_Node_Str"")) {
    filename=filename.substring(0,filename.length() - 4);
  }
  if (filename.endsWith(""String_Node_Str"")) {
    filename=filename.substring(0,filename.length() - 6);
  }
  if (filename.endsWith(""String_Node_Str"")) {
    filename=filename.substring(0,filename.length() - ""String_Node_Str"".length());
  }
  String namespace=templatePath.getNamespace();
  if (namespace != null && StringUtils.isNotBlank(namespace)) {
    visitor.visit(virtualFile,namespace + ""String_Node_Str"" + filename);
  }
 else {
    visitor.visit(virtualFile,filename);
  }
  return true;
}"
4122,"public static void visitTemplatePath(@NotNull Project project,final @NotNull TemplatePath templatePath,@NotNull final ViewVisitor visitor){
  final VirtualFile templateDir=VfsUtil.findRelativeFile(templatePath.getPath(),project.getBaseDir());
  if (templateDir == null) {
    return;
  }
  VfsUtil.visitChildrenRecursively(templateDir,new VirtualFileVisitor(){
    @Override public boolean visitFile(    @NotNull VirtualFile virtualFile){
      if (virtualFile.isDirectory() || !isTemplateFile(virtualFile)) {
        return true;
      }
      String filename=VfsUtil.getRelativePath(virtualFile,templateDir,'.');
      if (filename == null) {
        return true;
      }
      if (filename.endsWith(""String_Node_Str"")) {
        filename=filename.substring(0,filename.length() - 4);
      }
      if (filename.endsWith(""String_Node_Str"")) {
        filename=filename.substring(0,filename.length() - 6);
      }
      if (filename.endsWith(""String_Node_Str"")) {
        filename=filename.substring(0,filename.length() - ""String_Node_Str"".length());
      }
      if (templatePath.getNamespace() != null) {
        visitor.visit(virtualFile,templatePath.getNamespace() + ""String_Node_Str"" + filename);
      }
 else {
        visitor.visit(virtualFile,filename);
      }
      return true;
    }
    private boolean isTemplateFile(    VirtualFile virtualFile){
      if (virtualFile.getFileType() == BladeFileType.INSTANCE || virtualFile.getFileType() == PhpFileType.INSTANCE) {
        return true;
      }
      String extension=virtualFile.getExtension();
      if (extension != null && (extension.equalsIgnoreCase(""String_Node_Str"") || extension.equalsIgnoreCase(""String_Node_Str""))) {
        return true;
      }
      return false;
    }
  }
);
}","public static void visitTemplatePath(@NotNull Project project,final @NotNull TemplatePath templatePath,@NotNull final ViewVisitor visitor){
  final VirtualFile templateDir=VfsUtil.findRelativeFile(templatePath.getPath(),project.getBaseDir());
  if (templateDir == null) {
    return;
  }
  VfsUtil.visitChildrenRecursively(templateDir,new VirtualFileVisitor(){
    @Override public boolean visitFile(    @NotNull VirtualFile virtualFile){
      if (virtualFile.isDirectory() || !isTemplateFile(virtualFile)) {
        return true;
      }
      String filename=VfsUtil.getRelativePath(virtualFile,templateDir,'.');
      if (filename == null) {
        return true;
      }
      if (filename.endsWith(""String_Node_Str"")) {
        filename=filename.substring(0,filename.length() - 4);
      }
      if (filename.endsWith(""String_Node_Str"")) {
        filename=filename.substring(0,filename.length() - 6);
      }
      if (filename.endsWith(""String_Node_Str"")) {
        filename=filename.substring(0,filename.length() - ""String_Node_Str"".length());
      }
      String namespace=templatePath.getNamespace();
      if (namespace != null && StringUtils.isNotBlank(namespace)) {
        visitor.visit(virtualFile,namespace + ""String_Node_Str"" + filename);
      }
 else {
        visitor.visit(virtualFile,filename);
      }
      return true;
    }
    private boolean isTemplateFile(    VirtualFile virtualFile){
      if (virtualFile.getFileType() == BladeFileType.INSTANCE || virtualFile.getFileType() == PhpFileType.INSTANCE) {
        return true;
      }
      String extension=virtualFile.getExtension();
      if (extension != null && (extension.equalsIgnoreCase(""String_Node_Str"") || extension.equalsIgnoreCase(""String_Node_Str""))) {
        return true;
      }
      return false;
    }
  }
);
}"
4123,"/** 
 * Single resolve doesnt work if we have non unique class names in project context, so try a multiResolve and use first matched method
 */
@Nullable protected static Method getMultiResolvedMethod(PsiReference psiReference){
  PsiElement resolvedReference=psiReference.resolve();
  if (resolvedReference instanceof Method) {
    return (Method)resolvedReference;
  }
  if (psiReference instanceof PsiPolyVariantReference) {
    for (    ResolveResult resolveResult : ((PsiPolyVariantReference)psiReference).multiResolve(false)) {
      PsiElement element=resolveResult.getElement();
      if (element instanceof Method) {
        return (Method)element;
      }
    }
  }
  return null;
}","/** 
 * Single resolve doesnt work if we have non unique class names in project context, so try a multiResolve and use first matched method
 */
@Nullable public static Method getMultiResolvedMethod(PsiReference psiReference){
  PsiElement resolvedReference=psiReference.resolve();
  if (resolvedReference instanceof Method) {
    return (Method)resolvedReference;
  }
  if (psiReference instanceof PsiPolyVariantReference) {
    for (    ResolveResult resolveResult : ((PsiPolyVariantReference)psiReference).multiResolve(false)) {
      PsiElement element=resolveResult.getElement();
      if (element instanceof Method) {
        return (Method)element;
      }
    }
  }
  return null;
}"
4124,"@Nullable public static MethodReferenceBag getMethodParameterReferenceBag(PsiElement psiElement,int wantIndex){
  PsiElement variableContext=psiElement.getContext();
  if (!(variableContext instanceof ParameterList)) {
    return null;
  }
  ParameterList parameterList=(ParameterList)variableContext;
  if (!(parameterList.getContext() instanceof MethodReference)) {
    return null;
  }
  MethodReference methodReference=(MethodReference)parameterList.getContext();
  PsiElement method=methodReference.resolve();
  if (!(method instanceof Method)) {
    return null;
  }
  ParameterBag currentIndex=getCurrentParameterIndex(psiElement);
  if (currentIndex == null) {
    return null;
  }
  if (wantIndex >= 0 && currentIndex.getIndex() != wantIndex) {
    return null;
  }
  return new MethodReferenceBag(parameterList,methodReference,currentIndex);
}","@Nullable public static MethodReferenceBag getMethodParameterReferenceBag(PsiElement psiElement,int wantIndex){
  PsiElement variableContext=psiElement.getContext();
  if (!(variableContext instanceof ParameterList)) {
    return null;
  }
  ParameterList parameterList=(ParameterList)variableContext;
  if (!(parameterList.getContext() instanceof MethodReference)) {
    return null;
  }
  ParameterBag currentIndex=getCurrentParameterIndex(psiElement);
  if (currentIndex == null) {
    return null;
  }
  if (wantIndex >= 0 && currentIndex.getIndex() != wantIndex) {
    return null;
  }
  return new MethodReferenceBag(parameterList,(MethodReference)parameterList.getContext(),currentIndex);
}"
4125,"@Override public void onCreate(){
  LocationListener listener=new LocationListener(){
    @Override public void onLocationChanged(    Location location){
      if (lastLocation == null) {
        lastLocation=location;
      }
      distanceInMeters+=location.distanceTo(lastLocation);
      lastLocation=location;
    }
    @Override public void onProviderDisabled(    String arg0){
    }
    @Override public void onProviderEnabled(    String arg0){
    }
    @Override public void onStatusChanged(    String arg0,    int arg1,    Bundle bundle){
    }
  }
;
  LocationManager locManager=(LocationManager)getSystemService(Context.LOCATION_SERVICE);
  locManager.requestLocationUpdates(LocationManager.GPS_PROVIDER,1000,1,listener);
}","@Override public void onCreate(){
  listener=new LocationListener(){
    @Override public void onLocationChanged(    Location location){
      if (lastLocation == null) {
        lastLocation=location;
      }
      distanceInMeters+=location.distanceTo(lastLocation);
      lastLocation=location;
    }
    @Override public void onProviderDisabled(    String arg0){
    }
    @Override public void onProviderEnabled(    String arg0){
    }
    @Override public void onStatusChanged(    String arg0,    int arg1,    Bundle bundle){
    }
  }
;
  locManager=(LocationManager)getSystemService(Context.LOCATION_SERVICE);
  locManager.requestLocationUpdates(LocationManager.GPS_PROVIDER,1000,1,listener);
}"
4126,"private void startDrag(){
  draggedItem.onDragStart();
  requestDisallowInterceptTouchEvent(true);
}","private void startDrag(){
  layoutTransition=getLayoutTransition();
  if (layoutTransition != null) {
    setLayoutTransition(null);
  }
  draggedItem.onDragStart();
  requestDisallowInterceptTouchEvent(true);
}"
4127,"/** 
 * Animates the dragged item to its final resting position.
 */
private void onDragStop(){
  draggedItem.settleAnimation=ValueAnimator.ofFloat(draggedItem.totalDragOffset,draggedItem.totalDragOffset - draggedItem.targetTopOffset).setDuration(getTranslateAnimationDuration(draggedItem.targetTopOffset));
  draggedItem.settleAnimation.addUpdateListener(new ValueAnimator.AnimatorUpdateListener(){
    @Override public void onAnimationUpdate(    ValueAnimator animation){
      if (!draggedItem.detecting)       return;
      draggedItem.setTotalOffset(((Float)animation.getAnimatedValue()).intValue());
      final int shadowAlpha=(int)((1 - animation.getAnimatedFraction()) * 255);
      if (null != dragTopShadowDrawable)       dragTopShadowDrawable.setAlpha(shadowAlpha);
      dragBottomShadowDrawable.setAlpha(shadowAlpha);
      invalidate();
    }
  }
);
  draggedItem.settleAnimation.addListener(new AnimatorListenerAdapter(){
    @Override public void onAnimationStart(    Animator animation){
      draggedItem.onDragStop();
    }
    @Override public void onAnimationEnd(    Animator animation){
      if (!draggedItem.detecting) {
        return;
      }
      draggedItem.settleAnimation=null;
      draggedItem.stopDetecting();
      if (null != dragTopShadowDrawable)       dragTopShadowDrawable.setAlpha(255);
      dragBottomShadowDrawable.setAlpha(255);
    }
  }
);
  draggedItem.settleAnimation.start();
}","/** 
 * Animates the dragged item to its final resting position.
 */
private void onDragStop(){
  draggedItem.settleAnimation=ValueAnimator.ofFloat(draggedItem.totalDragOffset,draggedItem.totalDragOffset - draggedItem.targetTopOffset).setDuration(getTranslateAnimationDuration(draggedItem.targetTopOffset));
  draggedItem.settleAnimation.addUpdateListener(new ValueAnimator.AnimatorUpdateListener(){
    @Override public void onAnimationUpdate(    ValueAnimator animation){
      if (!draggedItem.detecting)       return;
      draggedItem.setTotalOffset(((Float)animation.getAnimatedValue()).intValue());
      final int shadowAlpha=(int)((1 - animation.getAnimatedFraction()) * 255);
      if (null != dragTopShadowDrawable)       dragTopShadowDrawable.setAlpha(shadowAlpha);
      dragBottomShadowDrawable.setAlpha(shadowAlpha);
      invalidate();
    }
  }
);
  draggedItem.settleAnimation.addListener(new AnimatorListenerAdapter(){
    @Override public void onAnimationStart(    Animator animation){
      draggedItem.onDragStop();
    }
    @Override public void onAnimationEnd(    Animator animation){
      if (!draggedItem.detecting) {
        return;
      }
      draggedItem.settleAnimation=null;
      draggedItem.stopDetecting();
      if (null != dragTopShadowDrawable)       dragTopShadowDrawable.setAlpha(255);
      dragBottomShadowDrawable.setAlpha(255);
      if (layoutTransition != null && getLayoutTransition() == null) {
        setLayoutTransition(layoutTransition);
      }
    }
  }
);
  draggedItem.settleAnimation.start();
}"
4128,"/** 
 * Sets background color for this button. <p/> Xml attribute:   {@code app:floatingActionButtonColor}
 * @param color color
 */
public void setColor(int color){
  mColor=color;
}","/** 
 * Sets background color for this button. <p/> Xml attribute:   {@code app:floatingActionButtonColor}<p/> NOTE: this method sets the <code>mColorStateList</code> field to <code>null</code>
 * @param color color
 */
public void setColor(int color){
  mColor=color;
  mColorStateList=null;
}"
4129,"@Override protected void drawableStateChanged(){
  super.drawableStateChanged();
  if (mCircleDrawable != null && mColorStateList != null) {
    mCircleDrawable.setColor(mColorStateList.getColorForState(getDrawableState(),mColor));
  }
}","@Override protected void drawableStateChanged(){
  super.drawableStateChanged();
  if (mCircleDrawable != null && mColorStateList != null) {
    mCircleDrawable.setColor(mColorStateList.getColorForState(getDrawableState(),mColor));
    invalidate();
  }
}"
4130,"private void runHorizonLeftAnimation(View view,long delay){
  view.setAlpha(0);
  ObjectAnimator objectAnimator=ObjectAnimator.ofFloat(view,""String_Node_Str"",-ViewUtils.getScreenWidth(),0);
  objectAnimator.setInterpolator(new LinearInterpolator());
  objectAnimator.start();
  ObjectAnimator objectAnimatorAlpha=ObjectAnimator.ofFloat(view,""String_Node_Str"",0f,1f);
  AnimatorSet set=new AnimatorSet();
  set.setDuration(mDuration);
  set.setStartDelay(delay);
  set.playTogether(objectAnimator,objectAnimatorAlpha);
  set.start();
}","private void runHorizonLeftAnimation(View view,long delay){
  view.setAlpha(0);
  ObjectAnimator objectAnimator=ObjectAnimator.ofFloat(view,""String_Node_Str"",-ViewUtils.getScreenWidth(),0);
  objectAnimator.setInterpolator(new LinearInterpolator());
  ObjectAnimator objectAnimatorAlpha=ObjectAnimator.ofFloat(view,""String_Node_Str"",0f,1f);
  AnimatorSet set=new AnimatorSet();
  set.setDuration(mDuration);
  set.setStartDelay(delay);
  set.playTogether(objectAnimator,objectAnimatorAlpha);
  set.start();
}"
4131,"/** 
 * A CronDefinition with only 3 required fields is legal to instantiate, but the parser considers an expression with 4 fields as an error: java.lang.IllegalArgumentException: Cron expression contains 4 parts but we expect one of [6, 7]
 */
public void testThreeRequiredFieldsSupported() throws Exception {
  CronDefinition cronDefinition=CronDefinitionBuilder.defineCron().withSeconds().and().withMinutes().and().withHours().and().withDayOfMonth().supportsL().supportsW().supportsLW().supportsQuestionMark().optional().and().withMonth().optional().and().withDayOfWeek().withValidRange(1,7).withMondayDoWValue(2).supportsHash().supportsL().supportsQuestionMark().optional().and().withYear().withValidRange(2000,2099).optional().and().instance();
  CronParser cronParser=new CronParser(cronDefinition);
  cronParser.parse(""String_Node_Str"");
}","/** 
 * A CronDefinition with only 3 required fields is legal to instantiate, but the parser considers an expression with 4 fields as an error: java.lang.IllegalArgumentException: Cron expression contains 4 parts but we expect one of [6, 7]
 */
public void testThreeRequiredFieldsSupported(){
  final CronDefinition cronDefinition=CronDefinitionBuilder.defineCron().withSeconds().and().withMinutes().and().withHours().and().withDayOfMonth().supportsL().supportsW().supportsLW().supportsQuestionMark().optional().and().withMonth().optional().and().withDayOfWeek().withValidRange(1,7).withMondayDoWValue(2).supportsHash().supportsL().supportsQuestionMark().optional().and().withYear().withValidRange(2000,2099).optional().and().instance();
  final CronParser cronParser=new CronParser(cronDefinition);
  cronParser.parse(""String_Node_Str"");
}"
4132,"/** 
 * Issue #52: ""And"" doesn't work for day of the week 1,2 should be Monday and Tuesday, but instead it is treated as 1st/2nd of month.
 */
@Test public void testWeekdayAndLastExecution(){
  final String crontab=""String_Node_Str"";
  final CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  final CronParser parser=new CronParser(cronDefinition);
  final Cron cron=parser.parse(crontab);
  final ZonedDateTime date=ZonedDateTime.parse(""String_Node_Str"");
  final ExecutionTime executionTime=ExecutionTime.forCron(cron);
  final Optional<ZonedDateTime> lastExecution=executionTime.lastExecution(date);
  if (lastExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),lastExecution.get());
  }
 else {
    fail(LAST_EXECUTION_NOT_PRESENT_ERROR);
  }
}","/** 
 * Issue #52: ""And"" doesn't work for day of the week 1,2 should be Monday and Tuesday, but instead it is treated as 1st/2nd of month.
 */
@Test public void testWeekdayAndLastExecution(){
  final Cron cron=getUnixCron(""String_Node_Str"");
  final Optional<ZonedDateTime> lastExecution=getLastExecutionFor(cron,ZonedDateTime.parse(""String_Node_Str""));
  if (lastExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),lastExecution.get());
  }
 else {
    fail(LAST_EXECUTION_NOT_PRESENT_ERROR);
  }
}"
4133,"/** 
 * Issue #50: last execution does not match expected date when cron specifies day of week and last execution is in previous month.
 */
@Test public void testLastExecutionDaysOfWeekOverMonthBoundary(){
  final String crontab=""String_Node_Str"";
  final CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  final CronParser parser=new CronParser(cronDefinition);
  final Cron cron=parser.parse(crontab);
  final ZonedDateTime date=ZonedDateTime.parse(""String_Node_Str"");
  final ExecutionTime executionTime=ExecutionTime.forCron(cron);
  final Optional<ZonedDateTime> lastExecution=executionTime.lastExecution(date);
  if (lastExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),lastExecution.get());
  }
 else {
    fail(LAST_EXECUTION_NOT_PRESENT_ERROR);
  }
}","/** 
 * Issue #50: last execution does not match expected date when cron specifies day of week and last execution is in previous month.
 */
@Test public void testLastExecutionDaysOfWeekOverMonthBoundary(){
  final Cron cron=getUnixCron(""String_Node_Str"");
  final Optional<ZonedDateTime> lastExecution=getLastExecutionFor(cron,ZonedDateTime.parse(""String_Node_Str""));
  if (lastExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),lastExecution.get());
  }
 else {
    fail(LAST_EXECUTION_NOT_PRESENT_ERROR);
  }
}"
4134,"/** 
 * Isue #52: Additional test to ensure after fix that ""And"" and ""Between"" can both be used 1,2-3 should be Monday, Tuesday and Wednesday.
 */
@Test public void testWeekdayAndWithMixOfOnAndBetweenLastExecution(){
  final String crontab=""String_Node_Str"";
  final CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  final CronParser parser=new CronParser(cronDefinition);
  final Cron cron=parser.parse(crontab);
  final ZonedDateTime date=ZonedDateTime.parse(""String_Node_Str"");
  final ExecutionTime executionTime=ExecutionTime.forCron(cron);
  final Optional<ZonedDateTime> lastExecution=executionTime.lastExecution(date);
  if (lastExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),lastExecution.get());
  }
 else {
    fail(LAST_EXECUTION_NOT_PRESENT_ERROR);
  }
}","/** 
 * Isue #52: Additional test to ensure after fix that ""And"" and ""Between"" can both be used 1,2-3 should be Monday, Tuesday and Wednesday.
 */
@Test public void testWeekdayAndWithMixOfOnAndBetweenLastExecution(){
  final Cron cron=getUnixCron(""String_Node_Str"");
  final Optional<ZonedDateTime> lastExecution=getLastExecutionFor(cron,ZonedDateTime.parse(""String_Node_Str""));
  if (lastExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),lastExecution.get());
  }
 else {
    fail(LAST_EXECUTION_NOT_PRESENT_ERROR);
  }
}"
4135,"/** 
 * Issue #69: Getting next execution fails on leap-year when using day-of-week.
 */
@Test public void testCorrectNextExecutionDoWForLeapYear(){
  final CronParser parser=new CronParser(CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX));
  final String crontab=""String_Node_Str"";
  final ExecutionTime executionTime=ExecutionTime.forCron(parser.parse(crontab));
  final ZonedDateTime scanTime=ZonedDateTime.parse(""String_Node_Str"");
  final Optional<ZonedDateTime> nextExecution=executionTime.nextExecution(scanTime);
  if (nextExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),nextExecution.get());
  }
 else {
    fail(NEXT_EXECUTION_NOT_PRESENT_ERROR);
  }
}","/** 
 * Issue #69: Getting next execution fails on leap-year when using day-of-week.
 */
@Test public void testCorrectNextExecutionDoWForLeapYear(){
  final Optional<ZonedDateTime> nextExecution=getNextExecutionFor(getUnixCron(""String_Node_Str""),ZonedDateTime.parse(""String_Node_Str""));
  if (nextExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),nextExecution.get());
  }
 else {
    fail(NEXT_EXECUTION_NOT_PRESENT_ERROR);
  }
}"
4136,"/** 
 * Issue #59: Incorrect next execution time for ""month"" and ""day of week"". Considers bad DoW
 */
@Test public void testCorrectNextExecutionDoW(){
  final CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  final CronParser parser=new CronParser(cronDefinition);
  final String crontab=""String_Node_Str"";
  final Cron cron=parser.parse(crontab);
  final ExecutionTime executionTime=ExecutionTime.forCron(cron);
  final ZonedDateTime scanTime=ZonedDateTime.parse(""String_Node_Str"");
  final Optional<ZonedDateTime> nextExecution=executionTime.nextExecution(scanTime);
  if (nextExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),nextExecution.get());
  }
 else {
    fail(NEXT_EXECUTION_NOT_PRESENT_ERROR);
  }
}","/** 
 * Issue #59: Incorrect next execution time for ""month"" and ""day of week"". Considers bad DoW
 */
@Test public void testCorrectNextExecutionDoW(){
  final Cron cron=getUnixCron(""String_Node_Str"");
  final Optional<ZonedDateTime> nextExecution=getNextExecutionFor(cron,ZonedDateTime.parse(""String_Node_Str""));
  if (nextExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),nextExecution.get());
  }
 else {
    fail(NEXT_EXECUTION_NOT_PRESENT_ERROR);
  }
}"
4137,"/** 
 * Issue #45: last execution does not match expected date. Result is not in same timezone as reference date.
 */
@Test public void testMondayWeekdayLastExecution(){
  final String crontab=""String_Node_Str"";
  final CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  final CronParser parser=new CronParser(cronDefinition);
  final Cron cron=parser.parse(crontab);
  final ZonedDateTime date=ZonedDateTime.parse(""String_Node_Str"");
  final ExecutionTime executionTime=ExecutionTime.forCron(cron);
  final Optional<ZonedDateTime> lastExecution=executionTime.lastExecution(date);
  if (lastExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),lastExecution.get());
  }
 else {
    fail(LAST_EXECUTION_NOT_PRESENT_ERROR);
  }
}","/** 
 * Issue #45: last execution does not match expected date. Result is not in same timezone as reference date.
 */
@Test public void testMondayWeekdayLastExecution(){
  final Cron cron=getUnixCron(""String_Node_Str"");
  final Optional<ZonedDateTime> lastExecution=getLastExecutionFor(cron,ZonedDateTime.parse(""String_Node_Str""));
  if (lastExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),lastExecution.get());
  }
 else {
    fail(LAST_EXECUTION_NOT_PRESENT_ERROR);
  }
}"
4138,"/** 
 * Issue #59: Incorrect next execution time for ""month"" and ""day of week"". Considers Month in range 0-11 instead of 1-12
 */
@Test public void testCorrectMonthScaleForNextExecution1(){
  final CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  final CronParser parser=new CronParser(cronDefinition);
  final String crontab=""String_Node_Str"";
  final Cron cron=parser.parse(crontab);
  final ExecutionTime executionTime=ExecutionTime.forCron(cron);
  final ZonedDateTime scanTime=ZonedDateTime.parse(""String_Node_Str"");
  final Optional<ZonedDateTime> nextExecution=executionTime.nextExecution(scanTime);
  if (nextExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),nextExecution.get());
  }
 else {
    fail(NEXT_EXECUTION_NOT_PRESENT_ERROR);
  }
}","/** 
 * Issue #59: Incorrect next execution time for ""month"" and ""day of week"". Considers Month in range 0-11 instead of 1-12
 */
@Test public void testCorrectMonthScaleForNextExecution1(){
  final Cron cron=getUnixCron(""String_Node_Str"");
  final Optional<ZonedDateTime> nextExecution=getNextExecutionFor(cron,ZonedDateTime.parse(""String_Node_Str""));
  if (nextExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),nextExecution.get());
  }
 else {
    fail(NEXT_EXECUTION_NOT_PRESENT_ERROR);
  }
}"
4139,"/** 
 * Issue #59: Incorrect next execution time for ""day of month"" in ""time"" situation dom ""* / 4"" should mean 1, 5, 9, 13, 17th... of month instead of 4, 8, 12, 16th...
 */
@Test public void testCorrectMonthScaleForNextExecution2(){
  final CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  final CronParser parser=new CronParser(cronDefinition);
  final String crontab=""String_Node_Str"";
  final Cron cron=parser.parse(crontab);
  final ExecutionTime executionTime=ExecutionTime.forCron(cron);
  final ZonedDateTime scanTime=ZonedDateTime.parse(""String_Node_Str"");
  final Optional<ZonedDateTime> nextExecution=executionTime.nextExecution(scanTime);
  if (nextExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),nextExecution.get());
  }
 else {
    fail(NEXT_EXECUTION_NOT_PRESENT_ERROR);
  }
}","/** 
 * Issue #59: Incorrect next execution time for ""day of month"" in ""time"" situation dom ""* / 4"" should mean 1, 5, 9, 13, 17th... of month instead of 4, 8, 12, 16th...
 */
@Test public void testCorrectMonthScaleForNextExecution2(){
  final Cron cron=getUnixCron(""String_Node_Str"");
  final Optional<ZonedDateTime> nextExecution=getNextExecutionFor(cron,ZonedDateTime.parse(""String_Node_Str""));
  if (nextExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),nextExecution.get());
  }
 else {
    fail(NEXT_EXECUTION_NOT_PRESENT_ERROR);
  }
}"
4140,"@Test public void testFull(){
  final CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ);
  final CronParser parser=new CronParser(cronDefinition);
  final Cron cron=parser.parse(""String_Node_Str"");
  System.out.println(CronDescriptor.instance().describe(cron));
}","@Test public void testFull(){
  final Cron cron=getCron(""String_Node_Str"");
  assertEquals(""String_Node_Str"" + ""String_Node_Str"",descriptor.describe(cron));
}"
4141,"public void testCase3(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).get();
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2015,12,31,12,00),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}","@Test @Ignore public void testCase3(){
  final ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  final Optional<ZonedDateTime> lastExecution=et.lastExecution(currentDateTime);
  if (lastExecution.isPresent()) {
    final ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2015,12,31,12,00),ZoneId.systemDefault());
    Assert.assertEquals(expected,lastExecution.get());
  }
 else {
    fail(LAST_EXECUTION_NOT_PRESENT_ERROR);
  }
}"
4142,"@Test public void testCase2(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).get();
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2012,12,29,12,00),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}","@Test public void testCase2(){
  final ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  final Optional<ZonedDateTime> lastExecution=et.lastExecution(currentDateTime);
  if (lastExecution.isPresent()) {
    final ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2012,12,29,12,00),ZoneId.systemDefault());
    Assert.assertEquals(expected,lastExecution.get());
  }
 else {
    fail(LAST_EXECUTION_NOT_PRESENT_ERROR);
  }
}"
4143,"@Test public void testCase4(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).get();
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2016,10,29,12,00),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}","@Test public void testCase4(){
  final ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  final Optional<ZonedDateTime> lastExecution=et.lastExecution(currentDateTime);
  if (lastExecution.isPresent()) {
    final ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2016,10,29,12,00),ZoneId.systemDefault());
    Assert.assertEquals(expected,lastExecution.get());
  }
 else {
    fail(LAST_EXECUTION_NOT_PRESENT_ERROR);
  }
}"
4144,"@Before public void setUp() throws Exception {
  currentDateTime=ZonedDateTime.of(LocalDateTime.of(2016,12,20,12,00),ZoneId.systemDefault());
  parser=new CronParser(CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ));
}","@Before public void setUp(){
  currentDateTime=ZonedDateTime.of(LocalDateTime.of(2016,12,20,12,00),ZoneId.systemDefault());
  parser=new CronParser(CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ));
}"
4145,"@Test public void testCase1(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).get();
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2015,12,31,12,00),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}","@Test public void testCase1(){
  final ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  final Optional<ZonedDateTime> lastExecution=et.lastExecution(currentDateTime);
  if (lastExecution.isPresent()) {
    final ZonedDateTime actual=lastExecution.get();
    final ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2015,12,31,12,00),ZoneId.systemDefault());
    Assert.assertEquals(expected,actual);
  }
}"
4146,"@Test public void testMustMatchCronEvenIfNanoSecondsVaries(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(QUARTZ);
  CronParser parser=new CronParser(cronDefinition);
  Cron quartzCron=parser.parse(""String_Node_Str"");
  quartzCron.validate();
  ZonedDateTime zdt=ZonedDateTime.of(1999,07,18,10,00,00,03,ZoneId.systemDefault());
  assertTrue(""String_Node_Str"",ExecutionTime.forCron(quartzCron).isMatch(zdt));
}","@Test public void testMustMatchCronEvenIfNanoSecondsVaries(){
  final CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(QUARTZ);
  final CronParser parser=new CronParser(cronDefinition);
  final Cron quartzCron=parser.parse(""String_Node_Str"");
  quartzCron.validate();
  final ZonedDateTime zdt=ZonedDateTime.of(1999,07,18,10,00,00,03,ZoneId.systemDefault());
  assertTrue(""String_Node_Str"",ExecutionTime.forCron(quartzCron).isMatch(zdt));
}"
4147,"@Test public void testMatchExact(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(QUARTZ);
  CronParser parser=new CronParser(cronDefinition);
  Cron quartzCron=parser.parse(""String_Node_Str"");
  quartzCron.validate();
  ZonedDateTime zdt=ZonedDateTime.of(1999,07,18,10,00,00,00,ZoneId.systemDefault());
  assertTrue(""String_Node_Str"",ExecutionTime.forCron(quartzCron).isMatch(zdt));
}","@Test public void testMatchExact(){
  final CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(QUARTZ);
  final CronParser parser=new CronParser(cronDefinition);
  final Cron quartzCron=parser.parse(""String_Node_Str"");
  quartzCron.validate();
  final ZonedDateTime zdt=ZonedDateTime.of(1999,07,18,10,00,00,00,ZoneId.systemDefault());
  assertTrue(""String_Node_Str"",ExecutionTime.forCron(quartzCron).isMatch(zdt));
}"
4148,"@Test public void testCronDefinitionExecutionTimeGenerator(){
  CronDefinition cronDefinition=defineCron().withMinutes().and().withHours().and().withDayOfWeek().optional().and().instance();
  CronParser parser=new CronParser(cronDefinition);
  Cron cron=parser.parse(CRON_EXPRESSION);
  ExecutionTime executionTime=ExecutionTime.forCron(cron);
  executionTime.isMatch(ZonedDateTime.now());
}","@Test public void testCronDefinitionExecutionTimeGenerator(){
  final CronDefinition cronDefinition=defineCron().withMinutes().and().withHours().and().withDayOfWeek().optional().and().instance();
  final CronParser parser=new CronParser(cronDefinition);
  final Cron cron=parser.parse(CRON_EXPRESSION);
  final ExecutionTime executionTime=ExecutionTime.forCron(cron);
  executionTime.isMatch(ZonedDateTime.now());
}"
4149,"/** 
 * Issue #223: for dayOfWeek value == 3 && division of day, nextExecution do not return correct results
 */
@Test public void testEveryWednesdayOfEveryDayNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
  Cron myCron2=parser.parse(""String_Node_Str"");
  time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron2).nextExecution(time).get());
}","/** 
 * Issue #223: for dayOfWeek value == 3 && division of day, nextExecution do not return correct results.
 */
@Test public void testEveryWednesdayOfEveryDayNextExecution(){
  final CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  final CronParser parser=new CronParser(cronDefinition);
  final Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  final Optional<ZonedDateTime> nextExecution=ExecutionTime.forCron(myCron).nextExecution(time);
  if (nextExecution.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),nextExecution.get());
  }
 else {
    fail(""String_Node_Str"");
  }
  final Cron myCron2=parser.parse(""String_Node_Str"");
  time=ZonedDateTime.parse(""String_Node_Str"");
  final Optional<ZonedDateTime> nextExecution2=ExecutionTime.forCron(myCron2).nextExecution(time);
  if (nextExecution2.isPresent()) {
    assertEquals(ZonedDateTime.parse(""String_Node_Str""),nextExecution2.get());
  }
 else {
    fail(""String_Node_Str"");
  }
}"
4150,"public void testCase3(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).get();
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2015,12,31,12,00),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}","public void testCase3(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).isPresent() ? et.lastExecution(currentDateTime).get() : null;
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2015,12,31,12,0),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}"
4151,"@Test public void testCase2(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).get();
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2012,12,29,12,00),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}","@Test public void testCase2(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).isPresent() ? et.lastExecution(currentDateTime).get() : null;
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2012,12,29,12,0),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}"
4152,"@Test public void testCase4(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).get();
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2016,10,29,12,00),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}","@Test public void testCase4(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).isPresent() ? et.lastExecution(currentDateTime).get() : null;
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2016,10,29,12,0),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}"
4153,"@Before public void setUp() throws Exception {
  currentDateTime=ZonedDateTime.of(LocalDateTime.of(2016,12,20,12,00),ZoneId.systemDefault());
  parser=new CronParser(CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ));
}","@Before public void setUp() throws Exception {
  currentDateTime=ZonedDateTime.of(LocalDateTime.of(2016,12,20,12,0),ZoneId.systemDefault());
  parser=new CronParser(CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ));
}"
4154,"@Test public void testCase1(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).get();
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2015,12,31,12,00),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}","@Test public void testCase1(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).isPresent() ? et.lastExecution(currentDateTime).get() : null;
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2015,12,31,12,0),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}"
4155,"/** 
 * Issue #223: for dayOfWeek value == 3 && division of day, nextExecution do not return correct results
 */
@Test public void testEveryWednesdayOfEveryDayNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  Optional<ZonedDateTime> onext=ExecutionTime.forCron(myCron).nextExecution(time);
  ZonedDateTime next=onext.orElse(null);
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
  Cron myCron2=parser.parse(""String_Node_Str"");
  time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next2=ExecutionTime.forCron(myCron2).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron2).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next2);
}","/** 
 * Issue #223: for dayOfWeek value == 3 && division of day, nextExecution do not return correct results
 */
@Test public void testEveryWednesdayOfEveryDayNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  Optional<ZonedDateTime> onext=ExecutionTime.forCron(myCron).nextExecution(time);
  ZonedDateTime next=onext.orElse(null);
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
  Cron myCron2=parser.parse(""String_Node_Str"");
  time=ZonedDateTime.parse(""String_Node_Str"");
  Optional<ZonedDateTime> onext2=ExecutionTime.forCron(myCron2).nextExecution(time);
  ZonedDateTime next2=onext2.orElse(null);
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next2);
}"
4156,"private List<Instant> getInstants(ExecutionTime executionTime,ZonedDateTime startTime,ZonedDateTime endTime){
  List<Instant> instantList=new ArrayList<>();
  ZonedDateTime next=executionTime.nextExecution(startTime).isPresent() ? executionTime.nextExecution(startTime).get() : null;
  while (next != null && next.isBefore(endTime)) {
    instantList.add(next.toInstant());
    next=executionTime.nextExecution(next).isPresent() ? executionTime.nextExecution(next).get() : null;
  }
  return instantList;
}","private List<Instant> getInstants(ExecutionTime executionTime,ZonedDateTime startTime,ZonedDateTime endTime){
  List<Instant> instantList=new ArrayList<>();
  Optional<ZonedDateTime> onext=executionTime.nextExecution(startTime);
  ZonedDateTime next=onext.orElse(null);
  while (next != null && next.isBefore(endTime)) {
    instantList.add(next.toInstant());
    onext=executionTime.nextExecution(next);
    next=onext.orElse(null);
  }
  return instantList;
}"
4157,"public void testCase3(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).isPresent() ? et.lastExecution(currentDateTime).get() : null;
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2015,12,31,12,0),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}","public void testCase3(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  Optional<ZonedDateTime> olast=et.lastExecution(currentDateTime);
  ZonedDateTime last=olast.orElse(null);
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2015,12,31,12,0),ZoneId.systemDefault());
  Assert.assertEquals(expected,last);
}"
4158,"@Test public void testCase2(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).isPresent() ? et.lastExecution(currentDateTime).get() : null;
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2012,12,29,12,0),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}","@Test public void testCase2(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  Optional<ZonedDateTime> olast=et.lastExecution(currentDateTime);
  ZonedDateTime last=olast.orElse(null);
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2012,12,29,12,0),ZoneId.systemDefault());
  Assert.assertEquals(expected,last);
}"
4159,"@Test public void testCase4(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).isPresent() ? et.lastExecution(currentDateTime).get() : null;
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2016,10,29,12,0),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}","@Test public void testCase4(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  Optional<ZonedDateTime> olast=et.lastExecution(currentDateTime);
  ZonedDateTime last=olast.orElse(null);
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2016,10,29,12,0),ZoneId.systemDefault());
  Assert.assertEquals(expected,last);
}"
4160,"@Test public void testCase1(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime actual=et.lastExecution(currentDateTime).isPresent() ? et.lastExecution(currentDateTime).get() : null;
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2015,12,31,12,0),ZoneId.systemDefault());
  Assert.assertEquals(expected,actual);
}","@Test public void testCase1(){
  ExecutionTime et=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  Optional<ZonedDateTime> olast=et.lastExecution(currentDateTime);
  ZonedDateTime last=olast.orElse(null);
  ZonedDateTime expected=ZonedDateTime.of(LocalDateTime.of(2015,12,31,12,0),ZoneId.systemDefault());
  Assert.assertEquals(expected,last);
}"
4161,"/** 
 * Issue #223: for dayOfWeek value == 3 && division of day, nextExecution do not return correct results
 */
@Test public void testEveryWednesdayOfEveryDayNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next=ExecutionTime.forCron(myCron).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
  Cron myCron2=parser.parse(""String_Node_Str"");
  time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next2=ExecutionTime.forCron(myCron2).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron2).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next2);
}","/** 
 * Issue #223: for dayOfWeek value == 3 && division of day, nextExecution do not return correct results
 */
@Test public void testEveryWednesdayOfEveryDayNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  Optional<ZonedDateTime> onext=ExecutionTime.forCron(myCron).nextExecution(time);
  ZonedDateTime next=onext.orElse(null);
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
  Cron myCron2=parser.parse(""String_Node_Str"");
  time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next2=ExecutionTime.forCron(myCron2).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron2).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next2);
}"
4162,"@Test public void testEveryWeekendForthWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next=ExecutionTime.forCron(myCron).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
}","@Test public void testEveryWeekendForthWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  Optional<ZonedDateTime> onext=ExecutionTime.forCron(myCron).nextExecution(time);
  ZonedDateTime next=onext.orElse(null);
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
}"
4163,"@Test public void testEveryWeekdayFirstWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next=ExecutionTime.forCron(myCron).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
}","@Test public void testEveryWeekdayFirstWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  Optional<ZonedDateTime> onext=ExecutionTime.forCron(myCron).nextExecution(time);
  ZonedDateTime next=onext.orElse(null);
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
}"
4164,"@Test public void testFirstMondayOfTheMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next=ExecutionTime.forCron(myCron).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
}","@Test public void testFirstMondayOfTheMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  Optional<ZonedDateTime> onext=ExecutionTime.forCron(myCron).nextExecution(time);
  ZonedDateTime next=onext.orElse(null);
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
}"
4165,"@Test public void testEveryWeekdaySecondWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next=ExecutionTime.forCron(myCron).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
}","@Test public void testEveryWeekdaySecondWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  Optional<ZonedDateTime> onext=ExecutionTime.forCron(myCron).nextExecution(time);
  ZonedDateTime next=onext.orElse(null);
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
}"
4166,"@Test public void testEveryWeekendFirstWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next=ExecutionTime.forCron(myCron).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
}","@Test public void testEveryWeekendFirstWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  Optional<ZonedDateTime> onext=ExecutionTime.forCron(myCron).nextExecution(time);
  ZonedDateTime next=onext.orElse(null);
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
}"
4167,"/** 
 * Registers functions that map TimeFields to a human readable description.
 */
private void registerFunctions(){
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Always && timeFields.minutes instanceof Always) {
      if (timeFields.seconds instanceof Always) {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
      }
      if (timeFields.seconds instanceof On) {
        if (TimeDescriptionStrategy.this.isDefault((On)timeFields.seconds)) {
          return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
        }
 else {
          return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.seconds).getTime().getValue());
        }
      }
    }
    return StringUtils.EMPTY;
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Always && timeFields.minutes instanceof On && timeFields.seconds instanceof On) {
      if (TimeDescriptionStrategy.this.isDefault((On)timeFields.seconds)) {
        if (TimeDescriptionStrategy.this.isDefault((On)timeFields.minutes)) {
          return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
        }
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.minutes).getTime().getValue());
      }
 else {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.minutes).getTime().getValue(),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.seconds).getTime().getValue());
      }
    }
    return StringUtils.EMPTY;
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof On && timeFields.seconds instanceof Always) {
      return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue(),((On)minutes).getTime().getValue());
    }
    return StringUtils.EMPTY;
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof On && timeFields.seconds instanceof On) {
      if (TimeDescriptionStrategy.this.isDefault((On)timeFields.seconds)) {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue(),((On)minutes).getTime().getValue());
      }
 else {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue(),((On)minutes).getTime().getValue(),((On)seconds).getTime().getValue());
      }
    }
    return StringUtils.EMPTY;
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof Always && timeFields.seconds instanceof Always) {
      return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue());
    }
    return StringUtils.EMPTY;
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof Between) {
      if (timeFields.seconds instanceof On) {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getFrom().getValue(),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getTo().getValue());
      }
      if (timeFields.seconds instanceof Always) {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getFrom().getValue(),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getTo().getValue());
      }
    }
    return StringUtils.EMPTY;
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Always && timeFields.minutes instanceof Every && timeFields.seconds instanceof On) {
      final Every minute=(Every)timeFields.minutes;
      String desc;
      if (minute.getPeriod().getValue() == 1 && TimeDescriptionStrategy.this.isDefault((On)timeFields.seconds)) {
        desc=String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
      }
 else {
        desc=String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),minute.getPeriod().getValue(),bundle.getString(""String_Node_Str""));
      }
      if (minute.getExpression() instanceof Between) {
        return StringUtils.EMPTY;
      }
      return desc;
    }
    return StringUtils.EMPTY;
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Every && timeFields.minutes instanceof On && timeFields.seconds instanceof On) {
      if (((On)timeFields.minutes).getTime().getValue() == 0 && ((On)timeFields.seconds).getTime().getValue() == 0) {
        final Integer period=((Every)timeFields.hours).getPeriod().getValue();
        if (period == null || period == 1) {
          return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
        }
      }
      final String result=String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),((Every)hours).getPeriod().getValue(),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)minutes).getTime().getValue());
      if (TimeDescriptionStrategy.this.isDefault((On)timeFields.seconds)) {
        return result;
      }
 else {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)seconds).getTime().getValue());
      }
    }
    return StringUtils.EMPTY;
  }
);
}","/** 
 * Registers functions that map TimeFields to a human readable description.
 */
private void registerFunctions(){
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Always && timeFields.minutes instanceof Always) {
      if (timeFields.seconds instanceof Always) {
        return String.format(EVERY_MINUTE_FORMAT,bundle.getString(EVERY),bundle.getString(SECOND));
      }
      if (timeFields.seconds instanceof On) {
        if (TimeDescriptionStrategy.this.isDefault((On)timeFields.seconds)) {
          return String.format(EVERY_MINUTE_FORMAT,bundle.getString(EVERY),bundle.getString(MINUTE));
        }
 else {
          return String.format(""String_Node_Str"",bundle.getString(EVERY),bundle.getString(MINUTE),bundle.getString(""String_Node_Str""),bundle.getString(SECOND),((On)timeFields.seconds).getTime().getValue());
        }
      }
    }
    return StringUtils.EMPTY;
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Always && timeFields.minutes instanceof On && timeFields.seconds instanceof On) {
      if (TimeDescriptionStrategy.this.isDefault((On)timeFields.seconds)) {
        if (TimeDescriptionStrategy.this.isDefault((On)timeFields.minutes)) {
          return String.format(EVERY_MINUTE_FORMAT,bundle.getString(EVERY),bundle.getString(""String_Node_Str""));
        }
        return String.format(""String_Node_Str"",bundle.getString(EVERY),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(MINUTE),((On)timeFields.minutes).getTime().getValue());
      }
 else {
        return String.format(""String_Node_Str"",bundle.getString(EVERY),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(MINUTE),((On)timeFields.minutes).getTime().getValue(),bundle.getString(""String_Node_Str""),bundle.getString(SECOND),((On)timeFields.seconds).getTime().getValue());
      }
    }
    return StringUtils.EMPTY;
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof On && timeFields.seconds instanceof Always) {
      return String.format(""String_Node_Str"",bundle.getString(EVERY),bundle.getString(SECOND),bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue(),((On)minutes).getTime().getValue());
    }
    return StringUtils.EMPTY;
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof On && timeFields.seconds instanceof On) {
      if (TimeDescriptionStrategy.this.isDefault((On)timeFields.seconds)) {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue(),((On)minutes).getTime().getValue());
      }
 else {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue(),((On)minutes).getTime().getValue(),((On)seconds).getTime().getValue());
      }
    }
    return StringUtils.EMPTY;
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof Always && timeFields.seconds instanceof Always) {
      return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue());
    }
    return StringUtils.EMPTY;
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof Between) {
      if (timeFields.seconds instanceof On) {
        return String.format(""String_Node_Str"",bundle.getString(EVERY),bundle.getString(MINUTE),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getFrom().getValue(),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getTo().getValue());
      }
      if (timeFields.seconds instanceof Always) {
        return String.format(""String_Node_Str"",bundle.getString(EVERY),bundle.getString(SECOND),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getFrom().getValue(),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getTo().getValue());
      }
    }
    return StringUtils.EMPTY;
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Always && timeFields.minutes instanceof Every && timeFields.seconds instanceof On) {
      final Every minute=(Every)timeFields.minutes;
      String desc;
      if (minute.getPeriod().getValue() == 1 && TimeDescriptionStrategy.this.isDefault((On)timeFields.seconds)) {
        desc=String.format(EVERY_MINUTE_FORMAT,bundle.getString(EVERY),bundle.getString(MINUTE));
      }
 else {
        desc=String.format(""String_Node_Str"",bundle.getString(EVERY),minute.getPeriod().getValue(),bundle.getString(""String_Node_Str""));
      }
      if (minute.getExpression() instanceof Between) {
        return StringUtils.EMPTY;
      }
      return desc;
    }
    return StringUtils.EMPTY;
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Every && timeFields.minutes instanceof On && timeFields.seconds instanceof On) {
      if (((On)timeFields.minutes).getTime().getValue() == 0 && ((On)timeFields.seconds).getTime().getValue() == 0) {
        final Integer period=((Every)timeFields.hours).getPeriod().getValue();
        if (period == null || period == 1) {
          return String.format(EVERY_MINUTE_FORMAT,bundle.getString(EVERY),bundle.getString(""String_Node_Str""));
        }
      }
      final String result=String.format(""String_Node_Str"",bundle.getString(EVERY),((Every)hours).getPeriod().getValue(),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(MINUTE),((On)minutes).getTime().getValue());
      if (TimeDescriptionStrategy.this.isDefault((On)timeFields.seconds)) {
        return result;
      }
 else {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(SECOND),((On)seconds).getTime().getValue());
      }
    }
    return StringUtils.EMPTY;
  }
);
}"
4168,"@Override public String describe(){
  final TimeFields fields=new TimeFields(hours,minutes,seconds);
  for (  final Function<TimeFields,String> function : descriptions) {
    if (!""String_Node_Str"".equals(function.apply(fields))) {
      return function.apply(fields);
    }
  }
  String secondsDesc=""String_Node_Str"";
  String minutesDesc=""String_Node_Str"";
  final String hoursDesc=addTimeExpressions(describe(hours),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
  if (!(seconds instanceof On && isDefault((On)seconds))) {
    secondsDesc=addTimeExpressions(describe(seconds),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
  }
  if (!(minutes instanceof On && isDefault((On)minutes))) {
    minutesDesc=addTimeExpressions(describe(minutes),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
  }
  return String.format(""String_Node_Str"",secondsDesc,minutesDesc,hoursDesc);
}","@Override public String describe(){
  final TimeFields fields=new TimeFields(hours,minutes,seconds);
  for (  final Function<TimeFields,String> function : descriptions) {
    if (!""String_Node_Str"".equals(function.apply(fields))) {
      return function.apply(fields);
    }
  }
  String secondsDesc=""String_Node_Str"";
  String minutesDesc=""String_Node_Str"";
  final String hoursDesc=addTimeExpressions(describe(hours),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
  if (!(seconds instanceof On && isDefault((On)seconds))) {
    secondsDesc=addTimeExpressions(describe(seconds),bundle.getString(SECOND),bundle.getString(""String_Node_Str""));
  }
  if (!(minutes instanceof On && isDefault((On)minutes))) {
    minutesDesc=addTimeExpressions(describe(minutes),bundle.getString(MINUTE),bundle.getString(""String_Node_Str""));
  }
  return String.format(""String_Node_Str"",secondsDesc,minutesDesc,hoursDesc);
}"
4169,"private List<Instant> getInstants(ExecutionTime executionTime,ZonedDateTime startTime,ZonedDateTime endTime){
  List<Instant> instantList=new ArrayList<>();
  ZonedDateTime next=executionTime.nextExecution(startTime).get();
  while (next.isBefore(endTime)) {
    instantList.add(next.toInstant());
    next=executionTime.nextExecution(next).get();
  }
  return instantList;
}","private List<Instant> getInstants(ExecutionTime executionTime,ZonedDateTime startTime,ZonedDateTime endTime){
  List<Instant> instantList=new ArrayList<>();
  ZonedDateTime next=executionTime.nextExecution(startTime).isPresent() ? executionTime.nextExecution(startTime).get() : null;
  while (next != null && next.isBefore(endTime)) {
    instantList.add(next.toInstant());
    next=executionTime.nextExecution(next).isPresent() ? executionTime.nextExecution(next).get() : null;
  }
  return instantList;
}"
4170,"private static ZonedDateTime nextSchedule(String cronString,ZonedDateTime lastExecution){
  CronParser cronParser=new CronParser(CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX));
  Cron cron=cronParser.parse(cronString);
  ExecutionTime executionTime=ExecutionTime.forCron(cron);
  return executionTime.nextExecution(lastExecution).get();
}","private static ZonedDateTime nextSchedule(String cronString,ZonedDateTime lastExecution){
  CronParser cronParser=new CronParser(CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX));
  Cron cron=cronParser.parse(cronString);
  ExecutionTime executionTime=ExecutionTime.forCron(cron);
  return executionTime.nextExecution(lastExecution).isPresent() ? executionTime.nextExecution(lastExecution).get() : null;
}"
4171,"@Test public void testEveryWeekendForthWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
}","@Test public void testEveryWeekendForthWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next=ExecutionTime.forCron(myCron).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
}"
4172,"@Test public void testEveryWeekdayFirstWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
}","@Test public void testEveryWeekdayFirstWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next=ExecutionTime.forCron(myCron).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
}"
4173,"@Test public void testFirstMondayOfTheMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
}","@Test public void testFirstMondayOfTheMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next=ExecutionTime.forCron(myCron).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
}"
4174,"@Test public void testEveryWeekdaySecondWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
}","@Test public void testEveryWeekdaySecondWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next=ExecutionTime.forCron(myCron).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
}"
4175,"@Test public void testEveryWeekendFirstWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
}","@Test public void testEveryWeekendFirstWeekOfMonthNextExecution(){
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next=ExecutionTime.forCron(myCron).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
}"
4176,"public StringValidations(FieldConstraints constraints){
  this.lwPattern=buildLWPattern(constraints.getSpecialChars());
  this.stringToIntKeysPattern=buildStringToIntPattern(constraints.getStringMappingKeySet());
}","public StringValidations(final FieldConstraints constraints){
  lwPattern=buildLWPattern(constraints.getSpecialChars());
  stringToIntKeysPattern=buildStringToIntPattern(constraints.getStringMappingKeySet());
}"
4177,"@VisibleForTesting Pattern buildStringToIntPattern(Set<String> strings){
  return buildWordsPattern(strings);
}","@VisibleForTesting Pattern buildStringToIntPattern(final Set<String> strings){
  return buildWordsPattern(strings);
}"
4178,"@VisibleForTesting Pattern buildWordsPattern(Set<String> words){
  StringBuilder builder=new StringBuilder(ESCAPED_START);
  Iterator<String> iterator=words.iterator();
  if (!iterator.hasNext()) {
    builder.append(ESCAPED_END);
    return Pattern.compile(builder.toString());
  }
  String next=iterator.next();
  builder.append(next);
  while (iterator.hasNext()) {
    builder.append(""String_Node_Str"");
    builder.append(iterator.next());
  }
  builder.append(ESCAPED_END);
  return Pattern.compile(builder.toString());
}","@VisibleForTesting Pattern buildWordsPattern(final Set<String> words){
  final StringBuilder builder=new StringBuilder(ESCAPED_START);
  final Iterator<String> iterator=words.iterator();
  if (!iterator.hasNext()) {
    builder.append(ESCAPED_END);
    return Pattern.compile(builder.toString());
  }
  final String next=iterator.next();
  builder.append(next);
  while (iterator.hasNext()) {
    builder.append(""String_Node_Str"");
    builder.append(iterator.next());
  }
  builder.append(ESCAPED_END);
  return Pattern.compile(builder.toString());
}"
4179,"@VisibleForTesting public String removeValidChars(String exp){
  Matcher numsAndCharsMatcher=NUMS_AND_CHARS_PATTERN.matcher(exp.toUpperCase());
  Matcher stringToIntKeysMatcher=stringToIntKeysPattern.matcher(numsAndCharsMatcher.replaceAll(""String_Node_Str""));
  Matcher specialWordsMatcher=lwPattern.matcher(stringToIntKeysMatcher.replaceAll(""String_Node_Str""));
  return specialWordsMatcher.replaceAll(""String_Node_Str"").replaceAll(""String_Node_Str"",""String_Node_Str"").replaceAll(""String_Node_Str"",""String_Node_Str"").replaceAll(""String_Node_Str"",""String_Node_Str"");
}","@VisibleForTesting public String removeValidChars(final String exp){
  final Matcher numsAndCharsMatcher=NUMS_AND_CHARS_PATTERN.matcher(exp.toUpperCase());
  final Matcher stringToIntKeysMatcher=stringToIntKeysPattern.matcher(numsAndCharsMatcher.replaceAll(""String_Node_Str""));
  final Matcher specialWordsMatcher=lwPattern.matcher(stringToIntKeysMatcher.replaceAll(""String_Node_Str""));
  return specialWordsMatcher.replaceAll(""String_Node_Str"").replaceAll(""String_Node_Str"",""String_Node_Str"").replaceAll(""String_Node_Str"",""String_Node_Str"").replaceAll(""String_Node_Str"",""String_Node_Str"");
}"
4180,"@VisibleForTesting Pattern buildLWPattern(Set<SpecialChar> specialChars){
  Set<String> scs=new HashSet<>();
  for (  SpecialChar sc : SPECIAL_CHARS) {
    if (specialChars.contains(sc)) {
      scs.add(sc.name());
    }
  }
  return buildWordsPattern(scs);
}","@VisibleForTesting Pattern buildLWPattern(final Set<SpecialChar> specialChars){
  final Set<String> scs=new HashSet<>();
  for (  final SpecialChar sc : SPECIAL_CHARS) {
    if (specialChars.contains(sc)) {
      scs.add(sc.name());
    }
  }
  return buildWordsPattern(scs);
}"
4181,"public static CronBuilder cron(CronDefinition definition){
  return new CronBuilder(definition);
}","public static CronBuilder cron(final CronDefinition definition){
  return new CronBuilder(definition);
}"
4182,"public CronBuilder withHour(FieldExpression expression){
  return addField(HOUR,expression);
}","public CronBuilder withHour(final FieldExpression expression){
  return addField(HOUR,expression);
}"
4183,"public CronBuilder withDoW(FieldExpression expression){
  return addField(DAY_OF_WEEK,expression);
}","public CronBuilder withDoW(final FieldExpression expression){
  return addField(DAY_OF_WEEK,expression);
}"
4184,"private CronBuilder(CronDefinition definition){
  this.definition=definition;
}","private CronBuilder(final CronDefinition definition){
  this.definition=definition;
}"
4185,"@VisibleForTesting CronBuilder addField(CronFieldName name,FieldExpression expression){
  checkState(definition != null,""String_Node_Str"");
  FieldConstraints constraints=definition.getFieldDefinition(name).getConstraints();
  expression.accept(new ValidationFieldExpressionVisitor(constraints,definition.isStrictRanges()));
  fields.put(name,new CronField(name,expression,constraints));
  return this;
}","@VisibleForTesting CronBuilder addField(final CronFieldName name,final FieldExpression expression){
  checkState(definition != null,""String_Node_Str"");
  final FieldConstraints constraints=definition.getFieldDefinition(name).getConstraints();
  expression.accept(new ValidationFieldExpressionVisitor(constraints,definition.isStrictRanges()));
  fields.put(name,new CronField(name,expression,constraints));
  return this;
}"
4186,"public CronBuilder withDoM(FieldExpression expression){
  return addField(DAY_OF_MONTH,expression);
}","public CronBuilder withDoM(final FieldExpression expression){
  return addField(DAY_OF_MONTH,expression);
}"
4187,"public CronBuilder withYear(FieldExpression expression){
  return addField(YEAR,expression);
}","public CronBuilder withYear(final FieldExpression expression){
  return addField(YEAR,expression);
}"
4188,"public CronBuilder withMinute(FieldExpression expression){
  return addField(MINUTE,expression);
}","public CronBuilder withMinute(final FieldExpression expression){
  return addField(MINUTE,expression);
}"
4189,"public CronBuilder withDoY(FieldExpression expression){
  return addField(DAY_OF_YEAR,expression);
}","public CronBuilder withDoY(final FieldExpression expression){
  return addField(DAY_OF_YEAR,expression);
}"
4190,"public CronBuilder withMonth(FieldExpression expression){
  return addField(MONTH,expression);
}","public CronBuilder withMonth(final FieldExpression expression){
  return addField(MONTH,expression);
}"
4191,"public CronBuilder withSecond(FieldExpression expression){
  return addField(SECOND,expression);
}","public CronBuilder withSecond(final FieldExpression expression){
  return addField(SECOND,expression);
}"
4192,"private static void cronValidation(String[] args) throws ParseException {
  Options options=new Options();
  options.addOption(""String_Node_Str"",""String_Node_Str"",false,""String_Node_Str"");
  options.addOption(""String_Node_Str"",""String_Node_Str"",true,""String_Node_Str"");
  options.addOption(""String_Node_Str"",""String_Node_Str"",true,""String_Node_Str"");
  options.addOption(""String_Node_Str"",HELP,false,""String_Node_Str"");
  String header=""String_Node_Str"";
  String footer=""String_Node_Str"";
  CommandLineParser parser=new DefaultParser();
  CommandLine cmd=parser.parse(options,args);
  if (cmd.hasOption(HELP) || cmd.getOptions().length == 0) {
    showHelp(options,header,footer);
    return;
  }
  if (!cmd.hasOption(""String_Node_Str"")) {
    showHelp(options,header,footer);
    return;
  }
  if (cmd.hasOption('v')) {
    String format=cmd.getOptionValue(""String_Node_Str"");
    String expression=cmd.getOptionValue(""String_Node_Str"");
    CronType cronType=CronType.valueOf(format);
    CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(cronType);
    CronParser cronParser=new CronParser(cronDefinition);
    Cron quartzCron=cronParser.parse(expression);
    quartzCron.validate();
  }
}","private static void cronValidation(final String[] args) throws ParseException {
  final Options options=new Options();
  options.addOption(""String_Node_Str"",""String_Node_Str"",false,""String_Node_Str"");
  options.addOption(""String_Node_Str"",""String_Node_Str"",true,""String_Node_Str"");
  options.addOption(""String_Node_Str"",""String_Node_Str"",true,""String_Node_Str"");
  options.addOption(""String_Node_Str"",HELP,false,""String_Node_Str"");
  final String header=""String_Node_Str"";
  final String footer=""String_Node_Str"";
  final CommandLineParser parser=new DefaultParser();
  final CommandLine cmd=parser.parse(options,args);
  if (cmd.hasOption(HELP) || cmd.getOptions().length == 0) {
    showHelp(options,header,footer);
    return;
  }
  if (!cmd.hasOption(""String_Node_Str"")) {
    showHelp(options,header,footer);
    return;
  }
  if (cmd.hasOption('v')) {
    final String format=cmd.getOptionValue(""String_Node_Str"");
    final String expression=cmd.getOptionValue(""String_Node_Str"");
    final CronType cronType=CronType.valueOf(format);
    final CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(cronType);
    final CronParser cronParser=new CronParser(cronDefinition);
    final Cron quartzCron=cronParser.parse(expression);
    quartzCron.validate();
  }
}"
4193,"private static void showHelp(Options options,String header,String footer){
  HelpFormatter formatter=new HelpFormatter();
  formatter.printHelp(""String_Node_Str"",header,options,footer,true);
}","private static void showHelp(final Options options,final String header,final String footer){
  final HelpFormatter formatter=new HelpFormatter();
  formatter.printHelp(""String_Node_Str"",header,options,footer,true);
}"
4194,"public static void main(String[] args) throws Exception {
  cronValidation(args);
}","public static void main(final String[] args) throws Exception {
  cronValidation(args);
}"
4195,"/** 
 * Issue #223: for dayOfWeek value == 3 && division of day, nextExecution do not return correct results
 */
@Test public void testEveryWednesdayOfEveryDayNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
  Cron myCron2=parser.parse(""String_Node_Str"");
  time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron2).nextExecution(time).get());
}","/** 
 * Issue #223: for dayOfWeek value == 3 && division of day, nextExecution do not return correct results
 */
@Test public void testEveryWednesdayOfEveryDayNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next=ExecutionTime.forCron(myCron).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next);
  Cron myCron2=parser.parse(""String_Node_Str"");
  time=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime next2=ExecutionTime.forCron(myCron2).nextExecution(time).isPresent() ? ExecutionTime.forCron(myCron2).nextExecution(time).get() : null;
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),next2);
}"
4196,"private ExecutionTimeResult potentialPreviousClosestMatch(ZonedDateTime date) throws NoSuchValueException {
  List<Integer> year=yearsValueGenerator.generateCandidates(date.getYear(),date.getYear());
  TimeNode days=null;
  try {
    days=generateDays(cronDefinition,date);
  }
 catch (  NoDaysForMonthException e) {
    return new ExecutionTimeResult(date.minusMonths(1),false);
  }
  int highestMonth=months.getValues().get(months.getValues().size() - 1);
  int highestDay=days.getValues().get(days.getValues().size() - 1);
  int highestHour=hours.getValues().get(hours.getValues().size() - 1);
  int highestMinute=minutes.getValues().get(minutes.getValues().size() - 1);
  int highestSecond=seconds.getValues().get(seconds.getValues().size() - 1);
  NearestValue nearestValue;
  ZonedDateTime newDate;
  if (year.isEmpty()) {
    int previousYear=yearsValueGenerator.generatePreviousValue(date.getYear());
    if (highestDay > 28) {
      int highestDayOfMonth=LocalDate.of(previousYear,highestMonth,1).lengthOfMonth();
      if (highestDay > highestDayOfMonth) {
        nearestValue=days.getPreviousValue(highestDay,1);
        if (nearestValue.getShifts() > 0) {
          newDate=ZonedDateTime.of(LocalDateTime.of(previousYear,highestMonth,1,23,59,59),ZoneId.systemDefault()).minusMonths(nearestValue.getShifts()).with(lastDayOfMonth());
          return new ExecutionTimeResult(newDate,false);
        }
 else {
          highestDay=nearestValue.getValue();
        }
      }
    }
    return initDateTime(previousYear,highestMonth,highestDay,highestHour,highestMinute,highestSecond,date.getZone());
  }
  if (!months.getValues().contains(date.getMonthValue())) {
    nearestValue=months.getPreviousValue(date.getMonthValue(),0);
    int previousMonths=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),12,31,23,59,59),date.getZone()).minusYears(nearestValue.getShifts());
      return new ExecutionTimeResult(newDate,false);
    }
    return initDateTime(date.getYear(),previousMonths,highestDay,highestHour,highestMinute,highestSecond,date.getZone());
  }
  if (!days.getValues().contains(date.getDayOfMonth())) {
    nearestValue=days.getPreviousValue(date.getDayOfMonth(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),1,23,59,59),date.getZone()).minusMonths(nearestValue.getShifts()).with(lastDayOfMonth());
      return new ExecutionTimeResult(newDate,false);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),nearestValue.getValue(),highestHour,highestMinute,highestSecond,date.getZone());
  }
  if (!hours.getValues().contains(date.getHour())) {
    nearestValue=hours.getPreviousValue(date.getHour(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),23,59,59),date.getZone()).minusDays(nearestValue.getShifts());
      return new ExecutionTimeResult(newDate,false);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),nearestValue.getValue(),highestMinute,highestSecond,date.getZone());
  }
  if (!minutes.getValues().contains(date.getMinute())) {
    nearestValue=minutes.getPreviousValue(date.getMinute(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),59,59),date.getZone()).minusHours(nearestValue.getShifts());
      return new ExecutionTimeResult(newDate,false);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),nearestValue.getValue(),highestSecond,date.getZone());
  }
  if (!seconds.getValues().contains(date.getSecond())) {
    nearestValue=seconds.getPreviousValue(date.getSecond(),0);
    int previousSeconds=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),59),date.getZone()).minusMinutes(nearestValue.getShifts());
      return new ExecutionTimeResult(newDate,false);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),previousSeconds,date.getZone());
  }
  return new ExecutionTimeResult(date,true);
}","private ExecutionTimeResult potentialPreviousClosestMatch(ZonedDateTime date) throws NoSuchValueException {
  List<Integer> year=yearsValueGenerator.generateCandidates(date.getYear(),date.getYear());
  TimeNode days=null;
  try {
    days=generateDays(cronDefinition,date);
  }
 catch (  NoDaysForMonthException e) {
    return new ExecutionTimeResult(toEndOfPreviousMonth(date),false);
  }
  int highestMonth=months.getValues().get(months.getValues().size() - 1);
  int highestDay=days.getValues().get(days.getValues().size() - 1);
  int highestHour=hours.getValues().get(hours.getValues().size() - 1);
  int highestMinute=minutes.getValues().get(minutes.getValues().size() - 1);
  int highestSecond=seconds.getValues().get(seconds.getValues().size() - 1);
  NearestValue nearestValue;
  ZonedDateTime newDate;
  if (year.isEmpty()) {
    int previousYear=yearsValueGenerator.generatePreviousValue(date.getYear());
    if (highestDay > 28) {
      int highestDayOfMonth=LocalDate.of(previousYear,highestMonth,1).lengthOfMonth();
      if (highestDay > highestDayOfMonth) {
        nearestValue=days.getPreviousValue(highestDay,1);
        if (nearestValue.getShifts() > 0) {
          newDate=ZonedDateTime.of(LocalDateTime.of(previousYear,highestMonth,1,23,59,59),ZoneId.systemDefault()).minusMonths(nearestValue.getShifts()).with(lastDayOfMonth());
          return new ExecutionTimeResult(newDate,false);
        }
 else {
          highestDay=nearestValue.getValue();
        }
      }
    }
    return initDateTime(previousYear,highestMonth,highestDay,highestHour,highestMinute,highestSecond,date.getZone());
  }
  if (!months.getValues().contains(date.getMonthValue())) {
    nearestValue=months.getPreviousValue(date.getMonthValue(),0);
    int previousMonths=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),12,31,23,59,59),date.getZone()).minusYears(nearestValue.getShifts());
      return new ExecutionTimeResult(newDate,false);
    }
    return initDateTime(date.getYear(),previousMonths,highestDay,highestHour,highestMinute,highestSecond,date.getZone());
  }
  if (!days.getValues().contains(date.getDayOfMonth())) {
    nearestValue=days.getPreviousValue(date.getDayOfMonth(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),1,23,59,59),date.getZone()).minusMonths(nearestValue.getShifts()).with(lastDayOfMonth());
      return new ExecutionTimeResult(newDate,false);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),nearestValue.getValue(),highestHour,highestMinute,highestSecond,date.getZone());
  }
  if (!hours.getValues().contains(date.getHour())) {
    nearestValue=hours.getPreviousValue(date.getHour(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),23,59,59),date.getZone()).minusDays(nearestValue.getShifts());
      return new ExecutionTimeResult(newDate,false);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),nearestValue.getValue(),highestMinute,highestSecond,date.getZone());
  }
  if (!minutes.getValues().contains(date.getMinute())) {
    nearestValue=minutes.getPreviousValue(date.getMinute(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),59,59),date.getZone()).minusHours(nearestValue.getShifts());
      return new ExecutionTimeResult(newDate,false);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),nearestValue.getValue(),highestSecond,date.getZone());
  }
  if (!seconds.getValues().contains(date.getSecond())) {
    nearestValue=seconds.getPreviousValue(date.getSecond(),0);
    int previousSeconds=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),59),date.getZone()).minusMinutes(nearestValue.getShifts());
      return new ExecutionTimeResult(newDate,false);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),previousSeconds,date.getZone());
  }
  return new ExecutionTimeResult(date,true);
}"
4197,"/** 
 * Registers functions that map TimeFields to a human readable description.
 */
private void registerFunctions(){
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Always && timeFields.minutes instanceof Always) {
      if (timeFields.seconds instanceof Always) {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
      }
      if (timeFields.seconds instanceof On) {
        if (isDefault((On)timeFields.seconds)) {
          return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
        }
 else {
          return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.seconds).getTime().getValue());
        }
      }
    }
    return ""String_Node_Str"";
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Always && timeFields.minutes instanceof On && timeFields.seconds instanceof On) {
      if (isDefault((On)timeFields.seconds)) {
        if (isDefault((On)timeFields.minutes)) {
          return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
        }
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.minutes).getTime().getValue());
      }
 else {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.minutes).getTime().getValue(),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.seconds).getTime().getValue());
      }
    }
    return ""String_Node_Str"";
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof On && timeFields.seconds instanceof Always) {
      return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue(),((On)minutes).getTime().getValue());
    }
    return ""String_Node_Str"";
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof On && timeFields.seconds instanceof On) {
      if (isDefault((On)timeFields.seconds)) {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue(),((On)minutes).getTime().getValue());
      }
 else {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue(),((On)minutes).getTime().getValue(),((On)seconds).getTime().getValue());
      }
    }
    return ""String_Node_Str"";
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof Always && timeFields.seconds instanceof Always) {
      return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue());
    }
    return ""String_Node_Str"";
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof Between) {
      if (timeFields.seconds instanceof On) {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getFrom().getValue(),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getTo().getValue());
      }
      if (timeFields.seconds instanceof Always) {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getFrom().getValue(),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getTo().getValue());
      }
    }
    return ""String_Node_Str"";
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Always && timeFields.minutes instanceof Every && timeFields.seconds instanceof On) {
      Every minute=(Every)timeFields.minutes;
      String desc;
      if (minute.getPeriod().getValue() == 1 && isDefault((On)timeFields.seconds)) {
        desc=String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
      }
 else {
        desc=String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),minute.getPeriod().getValue(),bundle.getString(""String_Node_Str""));
      }
      if (minute.getExpression() instanceof Between) {
        desc=String.format(""String_Node_Str"",desc,describe((minute.getExpression())));
      }
      return desc;
    }
    return ""String_Node_Str"";
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Every && timeFields.minutes instanceof On && timeFields.seconds instanceof On) {
      if (((On)timeFields.minutes).getTime().getValue() == 0 && ((On)timeFields.seconds).getTime().getValue() == 0) {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
      }
      String result=String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),((Every)hours).getPeriod().getValue(),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)minutes).getTime().getValue());
      if (isDefault((On)timeFields.seconds)) {
        return result;
      }
 else {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)seconds).getTime().getValue());
      }
    }
    return ""String_Node_Str"";
  }
);
}","/** 
 * Registers functions that map TimeFields to a human readable description.
 */
private void registerFunctions(){
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Always && timeFields.minutes instanceof Always) {
      if (timeFields.seconds instanceof Always) {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
      }
      if (timeFields.seconds instanceof On) {
        if (isDefault((On)timeFields.seconds)) {
          return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
        }
 else {
          return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.seconds).getTime().getValue());
        }
      }
    }
    return ""String_Node_Str"";
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Always && timeFields.minutes instanceof On && timeFields.seconds instanceof On) {
      if (isDefault((On)timeFields.seconds)) {
        if (isDefault((On)timeFields.minutes)) {
          return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
        }
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.minutes).getTime().getValue());
      }
 else {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.minutes).getTime().getValue(),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.seconds).getTime().getValue());
      }
    }
    return ""String_Node_Str"";
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof On && timeFields.seconds instanceof Always) {
      return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue(),((On)minutes).getTime().getValue());
    }
    return ""String_Node_Str"";
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof On && timeFields.seconds instanceof On) {
      if (isDefault((On)timeFields.seconds)) {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue(),((On)minutes).getTime().getValue());
      }
 else {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue(),((On)minutes).getTime().getValue(),((On)seconds).getTime().getValue());
      }
    }
    return ""String_Node_Str"";
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof Always && timeFields.seconds instanceof Always) {
      return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),((On)hours).getTime().getValue());
    }
    return ""String_Node_Str"";
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof On && timeFields.minutes instanceof Between) {
      if (timeFields.seconds instanceof On) {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getFrom().getValue(),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getTo().getValue());
      }
      if (timeFields.seconds instanceof Always) {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getFrom().getValue(),bundle.getString(""String_Node_Str""),((On)timeFields.hours).getTime().getValue(),((Between)timeFields.minutes).getTo().getValue());
      }
    }
    return ""String_Node_Str"";
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Always && timeFields.minutes instanceof Every && timeFields.seconds instanceof On) {
      Every minute=(Every)timeFields.minutes;
      String desc;
      if (minute.getPeriod().getValue() == 1 && isDefault((On)timeFields.seconds)) {
        desc=String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
      }
 else {
        desc=String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),minute.getPeriod().getValue(),bundle.getString(""String_Node_Str""));
      }
      if (minute.getExpression() instanceof Between) {
        desc=String.format(""String_Node_Str"",desc,describe((minute.getExpression())));
      }
      return desc;
    }
    return ""String_Node_Str"";
  }
);
  descriptions.add(timeFields -> {
    if (timeFields.hours instanceof Every && timeFields.minutes instanceof On && timeFields.seconds instanceof On) {
      if (((On)timeFields.minutes).getTime().getValue() == 0 && ((On)timeFields.seconds).getTime().getValue() == 0) {
        Integer period=((Every)timeFields.hours).getPeriod().getValue();
        if (period == null || period == 1) {
          return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
        }
      }
      String result=String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),((Every)hours).getPeriod().getValue(),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)minutes).getTime().getValue());
      if (isDefault((On)timeFields.seconds)) {
        return result;
      }
 else {
        return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""),((On)seconds).getTime().getValue());
      }
    }
    return ""String_Node_Str"";
  }
);
}"
4198,"/** 
 * Issue #43: getting bad description for expression
 * @throws Exception
 */
public void testEveryDayEveryFourHoursFromHour2() throws Exception {
  assertExpression(""String_Node_Str"",""String_Node_Str"");
}","/** 
 * Issue #43: getting bad description for expression
 * @throws Exception
 */
@Test public void testEveryDayEveryFourHoursFromHour2() throws Exception {
  assertExpression(""String_Node_Str"",""String_Node_Str"");
}"
4199,"public void testThatEveryMinuteIsPreserved(){
  CronDefinition quartzDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ);
  parser=new CronParser(quartzDefinition);
  Cron expression=parser.parse(""String_Node_Str"");
  assertEquals(""String_Node_Str"",expression.asString());
}","@Test public void testThatEveryMinuteIsPreserved(){
  CronDefinition quartzDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ);
  parser=new CronParser(quartzDefinition);
  Cron expression=parser.parse(""String_Node_Str"");
  assertEquals(""String_Node_Str"",expression.asString());
}"
4200,"@Test public void testEveryWeekendForthWeekOfMonthNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
}","public void testEveryWeekendForthWeekOfMonthNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
}"
4201,"@Test public void testEveryWeekdayFirstWeekOfMonthNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
}","public void testEveryWeekdayFirstWeekOfMonthNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
}"
4202,"/** 
 * Issue #228: dayOfWeek just isn't honored in the cron next execution evaluation and needs to be
 */
@Test public void testFirstMondayOfTheMonthNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
}","/** 
 * Issue #228: dayOfWeek just isn't honored in the cron next execution evaluation and needs to be
 */
public void testFirstMondayOfTheMonthNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
}"
4203,"@Test public void testEveryWeekdaySecondWeekOfMonthNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
}","public void testEveryWeekdaySecondWeekOfMonthNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
}"
4204,"@Test public void testEveryWeekendFirstWeekOfMonthNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
}","public void testEveryWeekendFirstWeekOfMonthNextExecution(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron myCron=parser.parse(""String_Node_Str"");
  ZonedDateTime time=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(ZonedDateTime.parse(""String_Node_Str""),ExecutionTime.forCron(myCron).nextExecution(time).get());
}"
4205,"/** 
 * Constructor
 * @param fieldDefinitions - list with field definitions. Must not be null or empty.Throws a NullPointerException if a null values is received Throws an IllegalArgumentException if an empty list is received
 */
public CronDefinition(List<FieldDefinition> fieldDefinitions,Set<CronConstraint> cronConstraints,boolean strictRanges){
  Preconditions.checkNotNull(fieldDefinitions,""String_Node_Str"");
  Preconditions.checkNotNull(cronConstraints,""String_Node_Str"");
  Preconditions.checkNotNullNorEmpty(fieldDefinitions,""String_Node_Str"");
  Preconditions.checkArgument(!fieldDefinitions.get(0).isOptional(),""String_Node_Str"");
  this.fieldDefinitions=new HashMap<>();
  for (  FieldDefinition field : fieldDefinitions) {
    this.fieldDefinitions.put(field.getFieldName(),field);
  }
  this.cronConstraints=Collections.unmodifiableSet(cronConstraints);
  this.strictRanges=strictRanges;
}","/** 
 * Constructor
 * @param fieldDefinitions - list with field definitions. Must not be null or empty.Throws a NullPointerException if a null values is received Throws an IllegalArgumentException if an empty list is received
 */
public CronDefinition(List<FieldDefinition> fieldDefinitions,Set<CronConstraint> cronConstraints,boolean strictRanges,boolean matchDayOfWeekAndDayOfMonth){
  Preconditions.checkNotNull(fieldDefinitions,""String_Node_Str"");
  Preconditions.checkNotNull(cronConstraints,""String_Node_Str"");
  Preconditions.checkNotNullNorEmpty(fieldDefinitions,""String_Node_Str"");
  Preconditions.checkArgument(!fieldDefinitions.get(0).isOptional(),""String_Node_Str"");
  this.fieldDefinitions=new HashMap<>();
  for (  FieldDefinition field : fieldDefinitions) {
    this.fieldDefinitions.put(field.getFieldName(),field);
  }
  this.cronConstraints=Collections.unmodifiableSet(cronConstraints);
  this.strictRanges=strictRanges;
  this.matchDayOfWeekAndDayOfMonth=matchDayOfWeekAndDayOfMonth;
}"
4206,"/** 
 * Creates a new CronDefinition instance with provided field definitions
 * @return returns CronDefinition instance, never null
 */
public CronDefinition instance(){
  Set<CronConstraint> validations=new HashSet<CronConstraint>();
  validations.addAll(cronConstraints);
  return new CronDefinition(fields.values().stream().sorted((o1,o2) -> o1.getFieldName().getOrder() - o2.getFieldName().getOrder()).collect(Collectors.toList()),validations,enforceStrictRanges);
}","/** 
 * Creates a new CronDefinition instance with provided field definitions
 * @return returns CronDefinition instance, never null
 */
public CronDefinition instance(){
  Set<CronConstraint> validations=new HashSet<CronConstraint>();
  validations.addAll(cronConstraints);
  return new CronDefinition(fields.values().stream().sorted((o1,o2) -> o1.getFieldName().getOrder() - o2.getFieldName().getOrder()).collect(Collectors.toList()),validations,enforceStrictRanges,matchDayOfWeekAndDayOfMonth);
}"
4207,"/** 
 * Creates CronDefinition instance matching cron4j specification;
 * @return CronDefinition instance, never null;
 */
private static CronDefinition cron4j(){
  return CronDefinitionBuilder.defineCron().withMinutes().and().withHours().and().withDayOfMonth().supportsL().and().withMonth().and().withDayOfWeek().withValidRange(0,6).withMondayDoWValue(1).and().enforceStrictRanges().instance();
}","/** 
 * Creates CronDefinition instance matching cron4j specification;
 * @return CronDefinition instance, never null;
 */
private static CronDefinition cron4j(){
  return CronDefinitionBuilder.defineCron().withMinutes().and().withHours().and().withDayOfMonth().supportsL().and().withMonth().and().withDayOfWeek().withValidRange(0,6).withMondayDoWValue(1).and().enforceStrictRanges().matchDayOfWeekAndDayOfMonth().instance();
}"
4208,"private List<Integer> generateDayCandidatesQuestionMarkNotSupportedUsingDoWAndDoM(int year,int month,WeekDay mondayDoWValue){
  LocalDate date=LocalDate.of(year,month,1);
  int lengthOfMonth=date.lengthOfMonth();
  Set<Integer> candidates=new HashSet<>();
  if (daysOfMonthCronField.getExpression() instanceof Always && daysOfWeekCronField.getExpression() instanceof Always) {
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
 else   if (daysOfMonthCronField.getExpression() instanceof Always) {
    candidates.addAll(createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(1,lengthOfMonth));
  }
 else   if (daysOfWeekCronField.getExpression() instanceof Always) {
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
 else {
    candidates.addAll(createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(1,lengthOfMonth));
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
  List<Integer> candidatesList=new ArrayList<>(candidates);
  Collections.sort(candidatesList);
  return candidatesList;
}","private List<Integer> generateDayCandidatesQuestionMarkNotSupportedUsingDoWAndDoM(int year,int month,WeekDay mondayDoWValue){
  LocalDate date=LocalDate.of(year,month,1);
  int lengthOfMonth=date.lengthOfMonth();
  Set<Integer> candidates=new HashSet<>();
  if (daysOfMonthCronField.getExpression() instanceof Always && daysOfWeekCronField.getExpression() instanceof Always) {
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
 else   if (daysOfMonthCronField.getExpression() instanceof Always) {
    candidates.addAll(createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(1,lengthOfMonth));
  }
 else   if (daysOfWeekCronField.getExpression() instanceof Always) {
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
 else {
    List<Integer> dayOfWeekCandidates=createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(1,lengthOfMonth);
    List<Integer> dayOfMonthCandidates=createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth);
    if (cronDefinition.isMatchDayOfWeekAndDayOfMonth()) {
      Set<Integer> dayOfWeekCandidatesSet=Sets.newHashSet(dayOfWeekCandidates);
      Set<Integer> dayOfMonthCandidatesSet=Sets.newHashSet(dayOfMonthCandidates);
      candidates.addAll(Sets.intersection(dayOfMonthCandidatesSet,dayOfWeekCandidatesSet));
    }
 else {
      candidates.addAll(dayOfWeekCandidates);
      candidates.addAll(dayOfMonthCandidates);
    }
  }
  List<Integer> candidatesList=new ArrayList<>(candidates);
  Collections.sort(candidatesList);
  return candidatesList;
}"
4209,"@Before public void setUp(){
  testFieldName1=CronFieldName.SECOND;
  testFieldName2=CronFieldName.MINUTE;
  testFieldName3=CronFieldName.HOUR;
  MockitoAnnotations.initMocks(this);
  when(mockFieldDefinition1.getFieldName()).thenReturn(testFieldName1);
  when(mockFieldDefinition2.getFieldName()).thenReturn(testFieldName2);
  when(mockFieldDefinition3optional.getFieldName()).thenReturn(testFieldName3);
  when(mockFieldDefinition3optional.isOptional()).thenReturn(Boolean.TRUE);
  enforceStrictRange=false;
}","@Before public void setUp(){
  testFieldName1=CronFieldName.SECOND;
  testFieldName2=CronFieldName.MINUTE;
  testFieldName3=CronFieldName.HOUR;
  MockitoAnnotations.initMocks(this);
  when(mockFieldDefinition1.getFieldName()).thenReturn(testFieldName1);
  when(mockFieldDefinition2.getFieldName()).thenReturn(testFieldName2);
  when(mockFieldDefinition3optional.getFieldName()).thenReturn(testFieldName3);
  when(mockFieldDefinition3optional.isOptional()).thenReturn(Boolean.TRUE);
  enforceStrictRange=false;
  matchDayOfWeekAndDayOfMonth=false;
}"
4210,"@Test(expected=IllegalArgumentException.class) public void testLastFieldOptionalNotAllowedOnSingleFieldDefinition() throws Exception {
  List<FieldDefinition> fields=Lists.newArrayList();
  fields.add(mockFieldDefinition3optional);
  new CronDefinition(fields,Sets.newHashSet(),enforceStrictRange);
}","@Test(expected=IllegalArgumentException.class) public void testLastFieldOptionalNotAllowedOnSingleFieldDefinition() throws Exception {
  List<FieldDefinition> fields=Lists.newArrayList();
  fields.add(mockFieldDefinition3optional);
  new CronDefinition(fields,Sets.newHashSet(),enforceStrictRange,matchDayOfWeekAndDayOfMonth);
}"
4211,"@Test public void testGetFieldDefinitions() throws Exception {
  List<FieldDefinition> fields=Lists.newArrayList();
  fields.add(mockFieldDefinition1);
  CronDefinition cronDefinition=new CronDefinition(fields,Sets.newHashSet(),enforceStrictRange);
  assertNotNull(cronDefinition.getFieldDefinitions());
  assertEquals(1,cronDefinition.getFieldDefinitions().size());
  assertTrue(cronDefinition.getFieldDefinitions().contains(mockFieldDefinition1));
}","@Test public void testGetFieldDefinitions() throws Exception {
  List<FieldDefinition> fields=Lists.newArrayList();
  fields.add(mockFieldDefinition1);
  CronDefinition cronDefinition=new CronDefinition(fields,Sets.newHashSet(),enforceStrictRange,matchDayOfWeekAndDayOfMonth);
  assertNotNull(cronDefinition.getFieldDefinitions());
  assertEquals(1,cronDefinition.getFieldDefinitions().size());
  assertTrue(cronDefinition.getFieldDefinitions().contains(mockFieldDefinition1));
}"
4212,"@Test(expected=NullPointerException.class) public void testConstructorNullFieldsParameter() throws Exception {
  new CronDefinition(null,Sets.newHashSet(),enforceStrictRange);
}","@Test(expected=NullPointerException.class) public void testConstructorNullFieldsParameter() throws Exception {
  new CronDefinition(null,Sets.newHashSet(),enforceStrictRange,matchDayOfWeekAndDayOfMonth);
}"
4213,"@Test(expected=NullPointerException.class) public void testConstructorNullConstraintsParameter() throws Exception {
  new CronDefinition(Lists.newArrayList(),null,enforceStrictRange);
}","@Test(expected=NullPointerException.class) public void testConstructorNullConstraintsParameter() throws Exception {
  new CronDefinition(Lists.newArrayList(),null,enforceStrictRange,matchDayOfWeekAndDayOfMonth);
}"
4214,"@Test(expected=IllegalArgumentException.class) public void testConstructorEmptyFieldsParameter() throws Exception {
  new CronDefinition(new ArrayList<>(),Sets.newHashSet(),enforceStrictRange);
}","@Test(expected=IllegalArgumentException.class) public void testConstructorEmptyFieldsParameter() throws Exception {
  new CronDefinition(new ArrayList<>(),Sets.newHashSet(),enforceStrictRange,matchDayOfWeekAndDayOfMonth);
}"
4215,"@Test public void testLastFieldOptionalTrueWhenSet() throws Exception {
  List<FieldDefinition> fields=Lists.newArrayList();
  fields.add(mockFieldDefinition1);
  fields.add(mockFieldDefinition2);
  fields.add(mockFieldDefinition3optional);
  assertTrue(new CronDefinition(fields,Sets.newHashSet(),enforceStrictRange).getFieldDefinitions().stream().sorted((o1,o2) -> o1.getFieldName().getOrder() - o2.getFieldName().getOrder()).collect(Collectors.toList()).get(fields.size() - 1).isOptional());
}","@Test public void testLastFieldOptionalTrueWhenSet() throws Exception {
  List<FieldDefinition> fields=Lists.newArrayList();
  fields.add(mockFieldDefinition1);
  fields.add(mockFieldDefinition2);
  fields.add(mockFieldDefinition3optional);
  assertTrue(new CronDefinition(fields,Sets.newHashSet(),enforceStrictRange,matchDayOfWeekAndDayOfMonth).getFieldDefinitions().stream().sorted((o1,o2) -> o1.getFieldName().getOrder() - o2.getFieldName().getOrder()).collect(Collectors.toList()).get(fields.size() - 1).isOptional());
}"
4216,"@Override public String asString(){
  if (period.getValue() == 1) {
    return expression.asString() != null ? expression.asString() : ""String_Node_Str"";
  }
  return String.format(""String_Node_Str"",expression.asString(),getPeriod());
}","@Override public String asString(){
  String expressionAsString=expression.asString();
  if (""String_Node_Str"".equals(expressionAsString) && period.getValue() == 1) {
    return expressionAsString;
  }
  return String.format(""String_Node_Str"",expressionAsString,period);
}"
4217,"public void testAsString() throws Exception {
  assertEquals(""String_Node_Str"",new Every(new On(new IntegerFieldValue(0)),new IntegerFieldValue(1)).asString());
}","@Test public void testAsString() throws Exception {
  assertEquals(""String_Node_Str"",new Every(new On(new IntegerFieldValue(0)),new IntegerFieldValue(1)).asString());
}"
4218,"public void testParseOnWithHash01() throws Exception {
  int on=5;
  int hashValue=3;
  On onExpression=(On)parser.parse(String.format(""String_Node_Str"",on,hashValue));
  assertEquals(on,(int)(onExpression.getTime().getValue()));
  assertEquals(hashValue,onExpression.getNth().getValue().intValue());
  assertEquals(SpecialChar.HASH,onExpression.getSpecialChar().getValue());
}","@Test public void testParseOnWithHash01() throws Exception {
  int on=5;
  int hashValue=3;
  On onExpression=(On)parser.parse(String.format(""String_Node_Str"",on,hashValue));
  assertEquals(on,(int)(onExpression.getTime().getValue()));
  assertEquals(hashValue,onExpression.getNth().getValue().intValue());
  assertEquals(SpecialChar.HASH,onExpression.getSpecialChar().getValue());
}"
4219,"@Before public void setUp(){
  parser=new FieldParser(FieldConstraintsBuilder.instance().createConstraintsInstance());
}","@Before public void setUp(){
  parser=new FieldParser(FieldConstraintsBuilder.instance().addHashSupport().createConstraintsInstance());
}"
4220,"/** 
 * Creates CronDefinition instance matching cron4j specification;
 * @return CronDefinition instance, never null;
 */
private static CronDefinition cron4j(){
  return CronDefinitionBuilder.defineCron().withMinutes().and().withHours().and().withDayOfMonth().and().withMonth().and().withDayOfWeek().withValidRange(0,6).withMondayDoWValue(1).and().enforceStrictRanges().instance();
}","/** 
 * Creates CronDefinition instance matching cron4j specification;
 * @return CronDefinition instance, never null;
 */
private static CronDefinition cron4j(){
  return CronDefinitionBuilder.defineCron().withMinutes().and().withHours().and().withDayOfMonth().supportsL().and().withMonth().and().withDayOfWeek().withValidRange(0,6).withMondayDoWValue(1).and().enforceStrictRanges().instance();
}"
4221,"@VisibleForTesting protected On parseOnWithHash(String exp){
  SpecialCharFieldValue specialChar=new SpecialCharFieldValue(HASH);
  String[] array=exp.split(HASH_TAG);
  IntegerFieldValue nth=mapToIntegerFieldValue(array[1]);
  if (array[0].isEmpty()) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  return new On(mapToIntegerFieldValue(array[0]),specialChar,nth);
}","@VisibleForTesting protected On parseOnWithHash(String exp){
  if (!fieldConstraints.getSpecialChars().contains(HASH))   throw new IllegalArgumentException(""String_Node_Str"" + exp);
  SpecialCharFieldValue specialChar=new SpecialCharFieldValue(HASH);
  String[] array=exp.split(HASH_TAG);
  IntegerFieldValue nth=mapToIntegerFieldValue(array[1]);
  if (array[0].isEmpty()) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  return new On(mapToIntegerFieldValue(array[0]),specialChar,nth);
}"
4222,"@Test public void testParseSunday() throws Exception {
  String cronExpr=""String_Node_Str"";
  cron4jParser.parse(cronExpr);
}","public void testParseSunday() throws Exception {
  String cronExpr=""String_Node_Str"";
  cron4jParser.parse(cronExpr);
}"
4223,"@Override public String asString(){
  if (period.getValue() == 1) {
    return expression.asString() != null ? expression.asString() : ""String_Node_Str"";
  }
  return String.format(""String_Node_Str"",expression.asString(),getPeriod());
}","@Override public String asString(){
  String expressionAsString=expression.asString();
  if (""String_Node_Str"".equals(expressionAsString) && period.getValue() == 1) {
    return expressionAsString;
  }
  return String.format(""String_Node_Str"",expressionAsString,period);
}"
4224,"public void testAsString() throws Exception {
  assertEquals(""String_Node_Str"",new Every(new On(new IntegerFieldValue(0)),new IntegerFieldValue(1)).asString());
}","@Test public void testAsString() throws Exception {
  assertEquals(""String_Node_Str"",new Every(new On(new IntegerFieldValue(0)),new IntegerFieldValue(1)).asString());
}"
4225,"/** 
 * Creates CronDefinition instance matching cron4j specification;
 * @return CronDefinition instance, never null;
 */
private static CronDefinition cron4j(){
  return CronDefinitionBuilder.defineCron().withMinutes().and().withHours().and().withDayOfMonth().and().withMonth().and().withDayOfWeek().withValidRange(0,6).withMondayDoWValue(1).and().enforceStrictRanges().instance();
}","/** 
 * Creates CronDefinition instance matching cron4j specification;
 * @return CronDefinition instance, never null;
 */
private static CronDefinition cron4j(){
  return CronDefinitionBuilder.defineCron().withMinutes().and().withHours().and().withDayOfMonth().supportsL().and().withMonth().and().withDayOfWeek().withValidRange(0,6).withMondayDoWValue(1).and().enforceStrictRanges().instance();
}"
4226,"private TimeNode generateDayCandidatesUsingDoY(ZonedDateTime reference){
  final int year=reference.getYear();
  final int month=reference.getMonthValue();
  LocalDate date=LocalDate.of(year,1,1);
  int lengthOfYear=date.lengthOfYear();
  List<Integer> candidates=createDayOfYearValueGeneratorInstance(daysOfYearCronField,year).generateCandidates(1,lengthOfYear);
  Range<Integer> rangeOfMonth=Range.closedOpen(LocalDate.of(year,month,1).getDayOfYear(),LocalDate.of(year,month + 1,1).getDayOfYear());
  Stream<Integer> candidatesFilteredByMonth=candidates.stream().filter(dayOfYear -> rangeOfMonth.contains(dayOfYear));
  Stream<Integer> uniqueCandidates=candidatesFilteredByMonth.distinct();
  Stream<Integer> candidatesMappedToDayOfMonth=uniqueCandidates.map(dayOfYear -> LocalDate.ofYearDay(reference.getYear(),dayOfYear).getDayOfMonth());
  return new TimeNode(candidatesMappedToDayOfMonth.collect(Collectors.toList()));
}","private TimeNode generateDayCandidatesUsingDoY(ZonedDateTime reference){
  final int year=reference.getYear();
  final int month=reference.getMonthValue();
  LocalDate date=LocalDate.of(year,1,1);
  int lengthOfYear=date.lengthOfYear();
  List<Integer> candidates=createDayOfYearValueGeneratorInstance(daysOfYearCronField,year).generateCandidates(1,lengthOfYear);
  Range<Integer> rangeOfMonth=Range.closedOpen(LocalDate.of(year,month,1).getDayOfYear(),month == 12 ? LocalDate.of(year,12,31).getDayOfYear() + 1 : LocalDate.of(year,month + 1,1).getDayOfYear());
  Stream<Integer> candidatesFilteredByMonth=candidates.stream().filter(dayOfYear -> rangeOfMonth.contains(dayOfYear));
  Stream<Integer> uniqueCandidates=candidatesFilteredByMonth.distinct();
  Stream<Integer> candidatesMappedToDayOfMonth=uniqueCandidates.map(dayOfYear -> LocalDate.ofYearDay(reference.getYear(),dayOfYear).getDayOfMonth());
  return new TimeNode(candidatesMappedToDayOfMonth.collect(Collectors.toList()));
}"
4227,"@Test public void testForCron() throws Exception {
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR)).getClass());
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(FIRST_QUATER_BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR)).getClass());
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(WITHOUT_DAY_OF_YEAR)).getClass());
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(WITHOUT_SPECIFIC_DAY_OF_YEAR)).getClass());
}","@Test public void testForCron(){
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR)).getClass());
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(FIRST_QUATER_BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR)).getClass());
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(WITHOUT_DAY_OF_YEAR)).getClass());
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(WITHOUT_SPECIFIC_DAY_OF_YEAR)).getClass());
}"
4228,"public void testLastExecutionEveryTwoWeeksStartingWithFirstDayOfYear() throws Exception {
  ZonedDateTime now=truncateToDays(ZonedDateTime.now());
  int dayOfYear=now.getDayOfYear();
  int dayOfMostRecentPeriod=dayOfYear % 14;
  ZonedDateTime expected=dayOfMostRecentPeriod == 1 ? now : now.minusDays(dayOfMostRecentPeriod - 1);
  ExecutionTime executionTime=ExecutionTime.forCron(parser.parse(BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR));
  assertEquals(expected,executionTime.lastExecution(now).get());
}","@Test public void testLastExecutionEveryTwoWeeksStartingWithFirstDayOfYear(){
  ZonedDateTime now=truncateToDays(ZonedDateTime.now());
  int dayOfYear=now.getDayOfYear();
  int dayOfMostRecentPeriod=dayOfYear % 14;
  ZonedDateTime expected=dayOfMostRecentPeriod == 1 ? now.minusDays(14) : now.minusDays(dayOfMostRecentPeriod - 1);
  ExecutionTime executionTime=ExecutionTime.forCron(parser.parse(BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR));
  assertEquals(expected,executionTime.lastExecution(now).get());
}"
4229,"public void testNextExecutionEveryTwoWeeksStartingWithFirstDayOfYear() throws Exception {
  ZonedDateTime now=truncateToDays(ZonedDateTime.now());
  int dayOfYear=now.getDayOfYear();
  int dayOfMostRecentPeriod=dayOfYear % 14;
  ZonedDateTime expected=dayOfMostRecentPeriod == 1 ? now : now.plusDays(15 - dayOfMostRecentPeriod);
  ExecutionTime executionTime=ExecutionTime.forCron(parser.parse(BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR));
  assertEquals(expected,executionTime.nextExecution(now).get());
}","@Test public void testNextExecutionEveryTwoWeeksStartingWithFirstDayOfYear(){
  ZonedDateTime now=truncateToDays(ZonedDateTime.now());
  int dayOfYear=now.getDayOfYear();
  int dayOfMostRecentPeriod=dayOfYear % 14;
  ZonedDateTime expected=now.plusDays(15 - dayOfMostRecentPeriod);
  ExecutionTime executionTime=ExecutionTime.forCron(parser.parse(BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR));
  assertEquals(expected,executionTime.nextExecution(now).get());
}"
4230,"private Always(Always always){
  this();
}","/** 
 * Should be package private and not be instantiated elsewhere. Class should become package private too.
 * @deprecated rather use {@link FieldExpression#always()()}
 */
@Deprecated public Always(){
}"
4231,"public QuestionMark(){
}","/** 
 * Should be package private and not be instantiated elsewhere. Class should become package private too.
 * @deprecated rather use {@link FieldExpression#questionMark()}
 */
@Deprecated public QuestionMark(){
}"
4232,"/** 
 * Issue #151: L-7 in day of month should work to find the day 7 days prior to the last day of the month.
 */
public void testLSupportedInDoMRangeNextExecutionCalculation(){
  ExecutionTime executionTime=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime now=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime nextExecution=executionTime.nextExecution(now).get();
  ZonedDateTime assertDate=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(assertDate,nextExecution);
}","/** 
 * Issue #151: L-7 in day of month should work to find the day 7 days prior to the last day of the month.
 */
@Test public void testLSupportedInDoMRangeNextExecutionCalculation(){
  ExecutionTime executionTime=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime now=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime nextExecution=executionTime.nextExecution(now).get();
  ZonedDateTime assertDate=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(assertDate,nextExecution);
}"
4233,"private TimeNode generateDayCandidatesUsingDoY(ZonedDateTime reference){
  final int year=reference.getYear();
  final int month=reference.getMonthValue();
  LocalDate date=LocalDate.of(year,1,1);
  int lengthOfYear=date.lengthOfYear();
  List<Integer> candidates=createDayOfYearValueGeneratorInstance(daysOfYearCronField,year).generateCandidates(1,lengthOfYear);
  Range<Integer> rangeOfMonth=Range.closedOpen(LocalDate.of(year,month,1).getDayOfYear(),LocalDate.of(year,month + 1,1).getDayOfYear());
  Stream<Integer> candidatesFilteredByMonth=candidates.stream().filter(dayOfYear -> rangeOfMonth.contains(dayOfYear));
  Stream<Integer> uniqueCandidates=candidatesFilteredByMonth.distinct();
  Stream<Integer> candidatesMappedToDayOfMonth=uniqueCandidates.map(dayOfYear -> LocalDate.ofYearDay(reference.getYear(),dayOfYear).getDayOfMonth());
  return new TimeNode(candidatesMappedToDayOfMonth.collect(Collectors.toList()));
}","private TimeNode generateDayCandidatesUsingDoY(ZonedDateTime reference){
  final int year=reference.getYear();
  final int month=reference.getMonthValue();
  LocalDate date=LocalDate.of(year,1,1);
  int lengthOfYear=date.lengthOfYear();
  List<Integer> candidates=createDayOfYearValueGeneratorInstance(daysOfYearCronField,year).generateCandidates(1,lengthOfYear);
  Range<Integer> rangeOfMonth=Range.closedOpen(LocalDate.of(year,month,1).getDayOfYear(),month == 12 ? LocalDate.of(year,12,31).getDayOfYear() + 1 : LocalDate.of(year,month + 1,1).getDayOfYear());
  Stream<Integer> candidatesFilteredByMonth=candidates.stream().filter(dayOfYear -> rangeOfMonth.contains(dayOfYear));
  Stream<Integer> uniqueCandidates=candidatesFilteredByMonth.distinct();
  Stream<Integer> candidatesMappedToDayOfMonth=uniqueCandidates.map(dayOfYear -> LocalDate.ofYearDay(reference.getYear(),dayOfYear).getDayOfMonth());
  return new TimeNode(candidatesMappedToDayOfMonth.collect(Collectors.toList()));
}"
4234,"@Test public void testForCron() throws Exception {
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR)).getClass());
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(FIRST_QUATER_BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR)).getClass());
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(WITHOUT_DAY_OF_YEAR)).getClass());
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(WITHOUT_SPECIFIC_DAY_OF_YEAR)).getClass());
}","@Test public void testForCron(){
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR)).getClass());
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(FIRST_QUATER_BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR)).getClass());
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(WITHOUT_DAY_OF_YEAR)).getClass());
  assertEquals(ExecutionTime.class,ExecutionTime.forCron(parser.parse(WITHOUT_SPECIFIC_DAY_OF_YEAR)).getClass());
}"
4235,"public void testLastExecutionEveryTwoWeeksStartingWithFirstDayOfYear() throws Exception {
  ZonedDateTime now=truncateToDays(ZonedDateTime.now());
  int dayOfYear=now.getDayOfYear();
  int dayOfMostRecentPeriod=dayOfYear % 14;
  ZonedDateTime expected=dayOfMostRecentPeriod == 1 ? now : now.minusDays(dayOfMostRecentPeriod - 1);
  ExecutionTime executionTime=ExecutionTime.forCron(parser.parse(BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR));
  assertEquals(expected,executionTime.lastExecution(now).get());
}","@Test public void testLastExecutionEveryTwoWeeksStartingWithFirstDayOfYear(){
  ZonedDateTime now=truncateToDays(ZonedDateTime.now());
  int dayOfYear=now.getDayOfYear();
  int dayOfMostRecentPeriod=dayOfYear % 14;
  ZonedDateTime expected=dayOfMostRecentPeriod == 1 ? now.minusDays(14) : now.minusDays(dayOfMostRecentPeriod - 1);
  ExecutionTime executionTime=ExecutionTime.forCron(parser.parse(BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR));
  assertEquals(expected,executionTime.lastExecution(now).get());
}"
4236,"public void testNextExecutionEveryTwoWeeksStartingWithFirstDayOfYear() throws Exception {
  ZonedDateTime now=truncateToDays(ZonedDateTime.now());
  int dayOfYear=now.getDayOfYear();
  int dayOfMostRecentPeriod=dayOfYear % 14;
  ZonedDateTime expected=dayOfMostRecentPeriod == 1 ? now : now.plusDays(15 - dayOfMostRecentPeriod);
  ExecutionTime executionTime=ExecutionTime.forCron(parser.parse(BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR));
  assertEquals(expected,executionTime.nextExecution(now).get());
}","@Test public void testNextExecutionEveryTwoWeeksStartingWithFirstDayOfYear(){
  ZonedDateTime now=truncateToDays(ZonedDateTime.now());
  int dayOfYear=now.getDayOfYear();
  int dayOfMostRecentPeriod=dayOfYear % 14;
  ZonedDateTime expected=now.plusDays(15 - dayOfMostRecentPeriod);
  ExecutionTime executionTime=ExecutionTime.forCron(parser.parse(BI_WEEKLY_STARTING_WITH_FIRST_DAY_OF_YEAR));
  assertEquals(expected,executionTime.nextExecution(now).get());
}"
4237,"/** 
 * Should be package private and not be instantiated elsewhere. Class should become package private too.
 * @deprecated rather use {@link FieldExpression#always()()}
 */
@Deprecated public Always(){
}","/** 
 * Should be package private and not be instantiated elsewhere. Class should become package private too.
 * @deprecated rather use {@link FieldExpression#always()}
 */
@Deprecated public Always(){
}"
4238,"/** 
 * Issue #148: Cron Builder/Parser fails on Every X years
 */
@Test public void testEveryXYears(){
  CronBuilder.cron(CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ)).withDoM(FieldExpressionFactory.on(1)).withDoW(FieldExpressionFactory.questionMark()).withYear(FieldExpressionFactory.every(FieldExpressionFactory.between(1970,2099),4)).withMonth(FieldExpressionFactory.on(0)).withHour(FieldExpressionFactory.on(0)).withMinute(FieldExpressionFactory.on(0)).withSecond(FieldExpressionFactory.on(0));
}","/** 
 * Issue #148: Cron Builder/Parser fails on Every X years
 */
@Test public void testEveryXYears(){
  CronBuilder.cron(CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ)).withDoM(FieldExpressionFactory.on(1)).withDoW(FieldExpressionFactory.questionMark()).withYear(FieldExpressionFactory.every(FieldExpressionFactory.between(1970,2099),4)).withMonth(FieldExpressionFactory.on(1)).withHour(FieldExpressionFactory.on(0)).withMinute(FieldExpressionFactory.on(0)).withSecond(FieldExpressionFactory.on(0));
}"
4239,"/** 
 * Issue #148: Cron Builder/Parser fails on Every X years
 */
public void testEveryXYears(){
  CronBuilder.cron(CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ)).withDoM(FieldExpressionFactory.on(1)).withDoW(FieldExpressionFactory.questionMark()).withYear(FieldExpressionFactory.every(FieldExpressionFactory.between(1970,2099),4)).withMonth(FieldExpressionFactory.on(0)).withHour(FieldExpressionFactory.on(0)).withMinute(FieldExpressionFactory.on(0)).withSecond(FieldExpressionFactory.on(0));
}","/** 
 * Issue #148: Cron Builder/Parser fails on Every X years
 */
@Test public void testEveryXYears(){
  CronBuilder.cron(CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ)).withDoM(FieldExpressionFactory.on(1)).withDoW(FieldExpressionFactory.questionMark()).withYear(FieldExpressionFactory.every(FieldExpressionFactory.between(1970,2099),4)).withMonth(FieldExpressionFactory.on(1)).withHour(FieldExpressionFactory.on(0)).withMinute(FieldExpressionFactory.on(0)).withSecond(FieldExpressionFactory.on(0));
}"
4240,"/** 
 * Issue #151: L-7 in day of month should work to find the day 7 days prior to the last day of the month.
 */
public void testLSupportedInDoMRangeNextExecutionCalculation(){
  ExecutionTime executionTime=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime now=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime nextExecution=executionTime.nextExecution(now).get();
  ZonedDateTime assertDate=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(assertDate,nextExecution);
}","/** 
 * Issue #151: L-7 in day of month should work to find the day 7 days prior to the last day of the month.
 */
@Test public void testLSupportedInDoMRangeNextExecutionCalculation(){
  ExecutionTime executionTime=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ZonedDateTime now=ZonedDateTime.parse(""String_Node_Str"");
  ZonedDateTime nextExecution=executionTime.nextExecution(now).get();
  ZonedDateTime assertDate=ZonedDateTime.parse(""String_Node_Str"");
  assertEquals(assertDate,nextExecution);
}"
4241,"/** 
 * Issue #148: Cron Builder/Parser fails on Every X years
 */
@Test public void testEveryXYears(){
  CronBuilder.cron(CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ)).withDoM(FieldExpressionFactory.on(1)).withDoW(FieldExpressionFactory.questionMark()).withYear(FieldExpressionFactory.every(FieldExpressionFactory.between(1970,2099),4)).withMonth(FieldExpressionFactory.on(0)).withHour(FieldExpressionFactory.on(0)).withMinute(FieldExpressionFactory.on(0)).withSecond(FieldExpressionFactory.on(0));
}","/** 
 * Issue #148: Cron Builder/Parser fails on Every X years
 */
@Test public void testEveryXYears(){
  CronBuilder.cron(CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ)).withDoM(FieldExpressionFactory.on(1)).withDoW(FieldExpressionFactory.questionMark()).withYear(FieldExpressionFactory.every(FieldExpressionFactory.between(1970,2099),4)).withMonth(FieldExpressionFactory.on(1)).withHour(FieldExpressionFactory.on(0)).withMinute(FieldExpressionFactory.on(0)).withSecond(FieldExpressionFactory.on(0));
}"
4242,"/** 
 * Issue #148: Cron Builder/Parser fails on Every X years
 */
public void testEveryXYears(){
  CronBuilder.cron(CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ)).withDoM(FieldExpressionFactory.on(1)).withDoW(FieldExpressionFactory.questionMark()).withYear(FieldExpressionFactory.every(FieldExpressionFactory.between(1970,2099),4)).withMonth(FieldExpressionFactory.on(0)).withHour(FieldExpressionFactory.on(0)).withMinute(FieldExpressionFactory.on(0)).withSecond(FieldExpressionFactory.on(0));
}","/** 
 * Issue #148: Cron Builder/Parser fails on Every X years
 */
@Test public void testEveryXYears(){
  CronBuilder.cron(CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ)).withDoM(FieldExpressionFactory.on(1)).withDoW(FieldExpressionFactory.questionMark()).withYear(FieldExpressionFactory.every(FieldExpressionFactory.between(1970,2099),4)).withMonth(FieldExpressionFactory.on(1)).withHour(FieldExpressionFactory.on(0)).withMinute(FieldExpressionFactory.on(0)).withSecond(FieldExpressionFactory.on(0));
}"
4243,"/** 
 * If date is not match, will return previous closest match. If date is match, will return this date.
 * @param date - reference ZonedDateTime instance - never null;
 * @return ZonedDateTime instance, never null. Value obeys logic specified above.
 * @throws NoSuchValueException
 */
private ZonedDateTime previousClosestMatch(ZonedDateTime date) throws NoSuchValueException {
  List<Integer> year=yearsValueGenerator.generateCandidates(date.getYear(),date.getYear());
  TimeNode days=null;
  try {
    days=generateDays(cronDefinition,date);
  }
 catch (  NoDaysForMonthException e) {
    return previousClosestMatch(date.minusMonths(1));
  }
  int highestMonth=months.getValues().get(months.getValues().size() - 1);
  int highestDay=days.getValues().get(days.getValues().size() - 1);
  int highestHour=hours.getValues().get(hours.getValues().size() - 1);
  int highestMinute=minutes.getValues().get(minutes.getValues().size() - 1);
  int highestSecond=seconds.getValues().get(seconds.getValues().size() - 1);
  NearestValue nearestValue;
  ZonedDateTime newDate;
  if (year.isEmpty()) {
    int previousYear=yearsValueGenerator.generatePreviousValue(date.getYear());
    if (highestDay > 28) {
      int highestDayOfMonth=LocalDate.of(previousYear,highestMonth,1).lengthOfMonth();
      if (highestDay > highestDayOfMonth) {
        nearestValue=days.getPreviousValue(highestDay,1);
        if (nearestValue.getShifts() > 0) {
          newDate=ZonedDateTime.of(LocalDateTime.of(previousYear,highestMonth,1,23,59,59),ZoneId.systemDefault()).minusMonths(nearestValue.getShifts()).with(lastDayOfMonth());
          return previousClosestMatch(newDate);
        }
 else {
          highestDay=nearestValue.getValue();
        }
      }
    }
    return initDateTime(previousYear,highestMonth,highestDay,highestHour,highestMinute,highestSecond,date.getZone(),false);
  }
  if (!months.getValues().contains(date.getMonthValue())) {
    nearestValue=months.getPreviousValue(date.getMonthValue(),0);
    int previousMonths=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),12,31,23,59,59),date.getZone()).minusYears(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),previousMonths,highestDay,highestHour,highestMinute,highestSecond,date.getZone(),false);
  }
  if (!days.getValues().contains(date.getDayOfMonth())) {
    nearestValue=days.getPreviousValue(date.getDayOfMonth(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),1,23,59,59),date.getZone()).minusMonths(nearestValue.getShifts()).with(lastDayOfMonth());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),nearestValue.getValue(),highestHour,highestMinute,highestSecond,date.getZone(),false);
  }
  if (!hours.getValues().contains(date.getHour())) {
    nearestValue=hours.getPreviousValue(date.getHour(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),23,59,59),date.getZone()).minusDays(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),nearestValue.getValue(),highestMinute,highestSecond,date.getZone(),false);
  }
  if (!minutes.getValues().contains(date.getMinute())) {
    nearestValue=minutes.getPreviousValue(date.getMinute(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),59,59),date.getZone()).minusHours(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),nearestValue.getValue(),highestSecond,date.getZone(),false);
  }
  if (!seconds.getValues().contains(date.getSecond())) {
    nearestValue=seconds.getPreviousValue(date.getSecond(),0);
    int previousSeconds=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),59),date.getZone()).minusMinutes(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),previousSeconds,date.getZone(),false);
  }
  return date;
}","/** 
 * If date is not match, will return previous closest match. If date is match, will return this date.
 * @param date - reference ZonedDateTime instance - never null;
 * @return ZonedDateTime instance, never null. Value obeys logic specified above.
 * @throws NoSuchValueException
 */
private ZonedDateTime previousClosestMatch(ZonedDateTime date) throws NoSuchValueException {
  ExecutionTimeResult newdate=new ExecutionTimeResult(date,false);
  do {
    newdate=potentialPreviousClosestMatch(newdate.getTime());
  }
 while (!newdate.isMatch());
  return newdate.getTime();
}"
4244,"/** 
 * Provide feedback if a given date matches the cron expression.
 * @param date - ZonedDateTime instance. If null, a NullPointerException will be raised.
 * @return true if date matches cron expression requirements, false otherwise.
 */
public boolean isMatch(ZonedDateTime date){
  Optional<ZonedDateTime> last=lastExecution(date);
  if (last.isPresent()) {
    Optional<ZonedDateTime> next=nextExecution(last.get());
    if (next.isPresent()) {
      return next.get().equals(date);
    }
 else {
      boolean everythingInRange=false;
      try {
        everythingInRange=dateValuesInExpectedRanges(nextClosestMatch(date),date);
      }
 catch (      NoSuchValueException ignored) {
      }
      try {
        everythingInRange=dateValuesInExpectedRanges(previousClosestMatch(date),date);
      }
 catch (      NoSuchValueException ignored) {
      }
      return everythingInRange;
    }
  }
  return false;
}","public boolean isMatch(){
  return isMatch;
}"
4245,"private ZonedDateTime initDateTime(int years,int monthsOfYear,int dayOfMonth,int hoursOfDay,int minutesOfHour,int secondsOfMinute,ZoneId timeZone,boolean next) throws NoSuchValueException {
  ZonedDateTime date=ZonedDateTime.of(LocalDateTime.of(0,1,1,0,0,0),timeZone).plusYears(years).plusMonths(monthsOfYear - 1).plusDays(dayOfMonth - 1).plusHours(hoursOfDay).plusMinutes(minutesOfHour).plusSeconds(secondsOfMinute);
  ZonedDateTime result=ensureSameDate(date,years,monthsOfYear,dayOfMonth,hoursOfDay,minutesOfHour,secondsOfMinute);
  if (isSameDate(result,years,monthsOfYear,dayOfMonth,hoursOfDay,minutesOfHour,secondsOfMinute)) {
    return result;
  }
 else {
    if (next) {
      return nextClosestMatch(result);
    }
 else {
      return previousClosestMatch(result);
    }
  }
}","private ExecutionTimeResult initDateTime(int years,int monthsOfYear,int dayOfMonth,int hoursOfDay,int minutesOfHour,int secondsOfMinute,ZoneId timeZone) throws NoSuchValueException {
  ZonedDateTime date=ZonedDateTime.of(LocalDateTime.of(0,1,1,0,0,0),timeZone).plusYears(years).plusMonths(monthsOfYear - 1).plusDays(dayOfMonth - 1).plusHours(hoursOfDay).plusMinutes(minutesOfHour).plusSeconds(secondsOfMinute);
  ZonedDateTime result=ensureSameDate(date,years,monthsOfYear,dayOfMonth,hoursOfDay,minutesOfHour,secondsOfMinute);
  if (isSameDate(result,years,monthsOfYear,dayOfMonth,hoursOfDay,minutesOfHour,secondsOfMinute)) {
    return new ExecutionTimeResult(result,true);
  }
  return new ExecutionTimeResult(result,false);
}"
4246,"/** 
 * If date is not match, will return next closest match. If date is match, will return this date.
 * @param date - reference ZonedDateTime instance - never null;
 * @return ZonedDateTime instance, never null. Value obeys logic specified above.
 * @throws NoSuchValueException
 */
private ZonedDateTime nextClosestMatch(ZonedDateTime date) throws NoSuchValueException {
  List<Integer> year=yearsValueGenerator.generateCandidates(date.getYear(),date.getYear());
  TimeNode days=null;
  int lowestMonth=months.getValues().get(0);
  int lowestHour=hours.getValues().get(0);
  int lowestMinute=minutes.getValues().get(0);
  int lowestSecond=seconds.getValues().get(0);
  NearestValue nearestValue;
  ZonedDateTime newDate;
  if (year.isEmpty()) {
    int newYear=yearsValueGenerator.generateNextValue(date.getYear());
    try {
      days=generateDays(cronDefinition,ZonedDateTime.of(LocalDateTime.of(newYear,lowestMonth,1,0,0),date.getZone()));
    }
 catch (    NoDaysForMonthException e) {
      return nextClosestMatch(date.plusMonths(1));
    }
    return initDateTime(yearsValueGenerator.generateNextValue(date.getYear()),lowestMonth,days.getValues().get(0),lowestHour,lowestMinute,lowestSecond,date.getZone(),true);
  }
  if (!months.getValues().contains(date.getMonthValue())) {
    nearestValue=months.getNextValue(date.getMonthValue(),0);
    int nextMonths=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),1,1,0,0,0),date.getZone()).plusYears(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getMonthValue()) {
      date=date.plusYears(1);
    }
    try {
      days=generateDays(cronDefinition,ZonedDateTime.of(LocalDateTime.of(date.getYear(),nextMonths,1,0,0),date.getZone()));
    }
 catch (    NoDaysForMonthException e) {
      return nextClosestMatch(date.plusMonths(1));
    }
    return initDateTime(date.getYear(),nextMonths,days.getValues().get(0),lowestHour,lowestMinute,lowestSecond,date.getZone(),true);
  }
  try {
    days=generateDays(cronDefinition,date);
  }
 catch (  NoDaysForMonthException e) {
    return nextClosestMatch(date.plusMonths(1));
  }
  if (!days.getValues().contains(date.getDayOfMonth())) {
    nearestValue=days.getNextValue(date.getDayOfMonth(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),1,0,0,0),date.getZone()).plusMonths(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getDayOfMonth()) {
      date=date.plusMonths(1);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),nearestValue.getValue(),lowestHour,lowestMinute,lowestSecond,date.getZone(),true);
  }
  if (!hours.getValues().contains(date.getHour())) {
    nearestValue=hours.getNextValue(date.getHour(),0);
    int nextHours=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),0,0,0),date.getZone()).plusDays(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getHour()) {
      date=date.plusDays(1);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),nextHours,lowestMinute,lowestSecond,date.getZone(),true);
  }
  if (!minutes.getValues().contains(date.getMinute())) {
    nearestValue=minutes.getNextValue(date.getMinute(),0);
    int nextMinutes=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),0,0),date.getZone()).plusHours(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getMinute()) {
      date=date.plusHours(1);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),nextMinutes,lowestSecond,date.getZone(),true);
  }
  if (!seconds.getValues().contains(date.getSecond())) {
    nearestValue=seconds.getNextValue(date.getSecond(),0);
    int nextSeconds=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),0),date.getZone()).plusMinutes(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getSecond()) {
      date=date.plusMinutes(1);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),nextSeconds,date.getZone(),true);
  }
  return date;
}","/** 
 * If date is not match, will return next closest match. If date is match, will return this date.
 * @param date - reference ZonedDateTime instance - never null;
 * @return ZonedDateTime instance, never null. Value obeys logic specified above.
 * @throws NoSuchValueException
 */
private ZonedDateTime nextClosestMatch(ZonedDateTime date) throws NoSuchValueException {
  ExecutionTimeResult newdate=new ExecutionTimeResult(date,false);
  do {
    newdate=potentialNextClosestMatch(newdate.getTime());
  }
 while (!newdate.isMatch());
  return newdate.getTime();
}"
4247,"/** 
 * Provide description for day of week
 * @param fields - fields to describe;
 * @return description - String
 */
private String describeDayOfWeek(Map<CronFieldName,CronField> fields){
  String description=DescriptionStrategyFactory.daysOfWeekInstance(bundle,fields.containsKey(CronFieldName.DAY_OF_WEEK) ? fields.get(CronFieldName.DAY_OF_WEEK).getExpression() : null).describe();
  return this.addExpressions(description,bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
}","/** 
 * Provide description for day of week
 * @param fields - fields to describe;
 * @return description - String
 */
private String describeDayOfWeek(Map<CronFieldName,CronField> fields,Map<CronFieldName,FieldDefinition> definitions){
  String description=DescriptionStrategyFactory.daysOfWeekInstance(bundle,fields.containsKey(CronFieldName.DAY_OF_WEEK) ? fields.get(CronFieldName.DAY_OF_WEEK).getExpression() : null,definitions.containsKey(CronFieldName.DAY_OF_WEEK) ? definitions.get(CronFieldName.DAY_OF_WEEK) : null).describe();
  return this.addExpressions(description,bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
}"
4248,"/** 
 * Provide a description of given CronFieldParseResult list
 * @param cron - Cron instance, never nullif null, will throw NullPointerException
 * @return description - String
 */
public String describe(Cron cron){
  Preconditions.checkNotNull(cron,""String_Node_Str"");
  Map<CronFieldName,CronField> expressions=cron.retrieveFieldsAsMap();
  return new StringBuilder().append(describeHHmmss(expressions)).append(""String_Node_Str"").append(describeDayOfMonth(expressions)).append(""String_Node_Str"").append(describeMonth(expressions)).append(""String_Node_Str"").append(describeDayOfWeek(expressions)).append(""String_Node_Str"").append(describeYear(expressions)).toString().replaceAll(""String_Node_Str"",""String_Node_Str"").trim();
}","/** 
 * Provide a description of given CronFieldParseResult list
 * @param cron - Cron instance, never nullif null, will throw NullPointerException
 * @return description - String
 */
public String describe(Cron cron){
  Preconditions.checkNotNull(cron,""String_Node_Str"");
  Map<CronFieldName,CronField> expressions=cron.retrieveFieldsAsMap();
  Map<CronFieldName,FieldDefinition> fieldDefinitions=cron.getCronDefinition().retrieveFieldDefinitionsAsMap();
  return new StringBuilder().append(describeHHmmss(expressions)).append(""String_Node_Str"").append(describeDayOfMonth(expressions)).append(""String_Node_Str"").append(describeMonth(expressions)).append(""String_Node_Str"").append(describeDayOfWeek(expressions,fieldDefinitions)).append(""String_Node_Str"").append(describeYear(expressions)).toString().replaceAll(""String_Node_Str"",""String_Node_Str"").trim();
}"
4249,"/** 
 * Creates description strategy for days of week
 * @param bundle - locale
 * @param expression - CronFieldExpression
 * @return - DescriptionStrategy instance, never null
 */
public static DescriptionStrategy daysOfWeekInstance(final ResourceBundle bundle,final FieldExpression expression){
  final Function<Integer,String> nominal=integer -> DayOfWeek.of(integer).getDisplayName(TextStyle.FULL,bundle.getLocale());
  NominalDescriptionStrategy dow=new NominalDescriptionStrategy(bundle,nominal,expression);
  dow.addDescription(fieldExpression -> {
    if (fieldExpression instanceof On) {
      On on=(On)fieldExpression;
switch (on.getSpecialChar().getValue()) {
case HASH:
        return String.format(""String_Node_Str"",nominal.apply(on.getTime().getValue()),on.getNth(),bundle.getString(""String_Node_Str""));
case L:
      return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),nominal.apply(on.getTime().getValue()),bundle.getString(""String_Node_Str""));
default :
    return ""String_Node_Str"";
}
}
return ""String_Node_Str"";
}
);
return dow;
}","/** 
 * Creates description strategy for days of week
 * @param bundle - locale
 * @param expression - CronFieldExpression
 * @return - DescriptionStrategy instance, never null
 */
public static DescriptionStrategy daysOfWeekInstance(final ResourceBundle bundle,final FieldExpression expression,final FieldDefinition definition){
  final Function<Integer,String> nominal=integer -> {
    int diff=definition instanceof DayOfWeekFieldDefinition ? DayOfWeek.MONDAY.getValue() - ((DayOfWeekFieldDefinition)definition).getMondayDoWValue().getMondayDoWValue() : 0;
    return DayOfWeek.of(integer + diff < 1 ? 7 : integer + diff).getDisplayName(TextStyle.FULL,bundle.getLocale());
  }
;
  NominalDescriptionStrategy dow=new NominalDescriptionStrategy(bundle,nominal,expression);
  dow.addDescription(fieldExpression -> {
    if (fieldExpression instanceof On) {
      On on=(On)fieldExpression;
switch (on.getSpecialChar().getValue()) {
case HASH:
        return String.format(""String_Node_Str"",nominal.apply(on.getTime().getValue()),on.getNth(),bundle.getString(""String_Node_Str""));
case L:
      return String.format(""String_Node_Str"",bundle.getString(""String_Node_Str""),nominal.apply(on.getTime().getValue()),bundle.getString(""String_Node_Str""));
default :
    return ""String_Node_Str"";
}
}
return ""String_Node_Str"";
}
);
return dow;
}"
4250,"@Override protected List<Integer> generateCandidatesNotIncludingIntervalExtremes(int start,int end){
  List<Integer> values=new ArrayList<>();
  try {
    int reference=generateNextValue(start);
    while (reference < end) {
      values.add(reference);
      reference=generateNextValue(reference);
    }
  }
 catch (  NoSuchValueException ignored) {
  }
  return values;
}","@Override protected List<Integer> generateCandidatesNotIncludingIntervalExtremes(int start,int end){
  List<Integer> values=new ArrayList<>();
  try {
    if (start != offset()) {
      values.add(offset());
    }
    int reference=generateNextValue(start);
    while (reference < end) {
      values.add(reference);
      reference=generateNextValue(reference);
    }
  }
 catch (  NoSuchValueException ignored) {
  }
  return values;
}"
4251,"@VisibleForTesting int offset(){
  return cronField.getConstraints().getStartRange();
}","@VisibleForTesting int offset(){
  FieldExpression expression=((Every)cronField.getExpression()).getExpression();
  if (expression instanceof On) {
    return ((On)expression).getTime().getValue();
  }
  return cronField.getConstraints().getStartRange();
}"
4252,"@Override public boolean isMatch(int value){
  Every every=(Every)cronField.getExpression();
  int start=cronField.getConstraints().getStartRange();
  return ((value - start) % every.getPeriod().getValue()) == 0;
}","@Override public boolean isMatch(int value){
  Every every=(Every)cronField.getExpression();
  int start=offset();
  return ((value - start) % every.getPeriod().getValue()) == 0;
}"
4253,"/** 
 * Issue #27: execution time properly calculated
 */
@Test public void testMonthRangeExecutionTime(){
  ExecutionTime.forCron(parser.parse(""String_Node_Str""));
}","/** 
 * Issue #27: execution time properly calculated
 */
@Test public void testMonthRangeExecutionTime(){
  assertNotNull(ExecutionTime.forCron(parser.parse(""String_Node_Str"")));
}"
4254,"@Test public void testTimeToNextExecution() throws Exception {
  ZonedDateTime now=truncateToSeconds(ZonedDateTime.now());
  ZonedDateTime expected=truncateToSeconds(now.plusSeconds(1));
  ExecutionTime executionTime=ExecutionTime.forCron(parser.parse(EVERY_SECOND));
  assertEquals(java.time.Duration.between(now,expected),executionTime.timeToNextExecution(now));
}","@Test public void testTimeToNextExecution() throws Exception {
  ZonedDateTime now=truncateToSeconds(ZonedDateTime.now());
  ZonedDateTime expected=truncateToSeconds(now.plusSeconds(1));
  ExecutionTime executionTime=ExecutionTime.forCron(parser.parse(EVERY_SECOND));
  assertEquals(Duration.between(now,expected),executionTime.timeToNextExecution(now));
}"
4255,"@Test public void testParseIncompleteEvery() throws Exception {
  Set<FieldDefinition> set=Sets.newHashSet();
  set.add(new FieldDefinition(CronFieldName.SECOND,FieldConstraintsBuilder.instance().createConstraintsInstance()));
  when(definition.getFieldDefinitions()).thenReturn(set);
  parser=new CronParser(definition);
  expectedException.expect(IllegalArgumentException.class);
  expectedException.expectMessage(""String_Node_Str"");
  parser.parse(""String_Node_Str"");
}","@Test public void testParseIncompleteEvery() throws Exception {
  Set<FieldDefinition> set=Sets.newHashSet();
  set.add(new FieldDefinition(CronFieldName.SECOND,FieldConstraintsBuilder.instance().createConstraintsInstance()));
  when(definition.getFieldDefinitions()).thenReturn(set);
  parser=new CronParser(definition);
  expectedException.expect(IllegalArgumentException.class);
  expectedException.expectMessage(""String_Node_Str"");
  assertNotNull(parser.parse(""String_Node_Str""));
}"
4256,"/** 
 * Issue #27: day of week range string mapping is valid
 */
@Test public void testDayOfWeekRangeMappingIsValid(){
  parser.parse(""String_Node_Str"");
}","/** 
 * Issue #27: day of week range string mapping is valid
 */
@Test public void testDayOfWeekRangeMappingIsValid(){
  assertNotNull(parser.parse(""String_Node_Str""));
}"
4257,"@VisibleForTesting boolean isDefault(FieldValue<?> fieldValue){
  return fieldValue instanceof IntegerFieldValue && ((IntegerFieldValue)fieldValue).getValue() == -1;
}","@VisibleForTesting protected boolean isDefault(FieldValue<?> fieldValue){
  return fieldValue instanceof IntegerFieldValue && ((IntegerFieldValue)fieldValue).getValue() == -1;
}"
4258,"boolean isSpecialCharNotL(FieldValue<?> fieldValue){
  return fieldValue instanceof SpecialCharFieldValue && !SpecialChar.L.equals(fieldValue.getValue());
}","protected boolean isSpecialCharNotL(FieldValue<?> fieldValue){
  return fieldValue instanceof SpecialCharFieldValue && !SpecialChar.L.equals(fieldValue.getValue());
}"
4259,"/** 
 * Check if given number is greater or equal to start range and minor or equal to end range
 * @param fieldValue - to be validated
 * @throws IllegalArgumentException - if not in range
 */
@VisibleForTesting void isInRange(FieldValue<?> fieldValue){
  if (fieldValue instanceof IntegerFieldValue) {
    int value=((IntegerFieldValue)fieldValue).getValue();
    if (!constraints.isInRange(value)) {
      throw new IllegalArgumentException(String.format(OORANGE,value,constraints.getStartRange(),constraints.getEndRange()));
    }
  }
}","/** 
 * Check if given number is greater or equal to start range and minor or equal to end range
 * @param fieldValue - to be validated
 * @throws IllegalArgumentException - if not in range
 */
@VisibleForTesting protected void isInRange(FieldValue<?> fieldValue){
  if (fieldValue instanceof IntegerFieldValue) {
    int value=((IntegerFieldValue)fieldValue).getValue();
    if (!constraints.isInRange(value)) {
      throw new IllegalArgumentException(String.format(OORANGE,value,constraints.getStartRange(),constraints.getEndRange()));
    }
  }
}"
4260,"/** 
 * If date is not match, will return previous closest match. If date is match, will return this date.
 * @param date - reference ZonedDateTime instance - never null;
 * @return ZonedDateTime instance, never null. Value obeys logic specified above.
 * @throws NoSuchValueException
 */
ZonedDateTime previousClosestMatch(ZonedDateTime date) throws NoSuchValueException {
  List<Integer> year=yearsValueGenerator.generateCandidates(date.getYear(),date.getYear());
  TimeNode days=generateDays(cronDefinition,date);
  int highestMonth=months.getValues().get(months.getValues().size() - 1);
  int highestDay=days.getValues().get(days.getValues().size() - 1);
  int highestHour=hours.getValues().get(hours.getValues().size() - 1);
  int highestMinute=minutes.getValues().get(minutes.getValues().size() - 1);
  int highestSecond=seconds.getValues().get(seconds.getValues().size() - 1);
  NearestValue nearestValue;
  ZonedDateTime newDate;
  if (year.isEmpty()) {
    int previousYear=yearsValueGenerator.generatePreviousValue(date.getYear());
    if (highestDay > 28) {
      int highestDayOfMonth=LocalDate.of(previousYear,highestMonth,1).lengthOfMonth();
      if (highestDay > highestDayOfMonth) {
        nearestValue=days.getPreviousValue(highestDay,1);
        if (nearestValue.getShifts() > 0) {
          newDate=ZonedDateTime.of(LocalDateTime.of(previousYear,highestMonth,1,23,59,59),ZoneId.systemDefault()).minusMonths(nearestValue.getShifts()).with(lastDayOfMonth());
          return previousClosestMatch(newDate);
        }
 else {
          highestDay=nearestValue.getValue();
        }
      }
    }
    return initDateTime(previousYear,highestMonth,highestDay,highestHour,highestMinute,highestSecond,date.getZone());
  }
  if (!months.getValues().contains(date.getMonthValue())) {
    nearestValue=months.getPreviousValue(date.getMonthValue(),0);
    int previousMonths=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),12,31,23,59,59),date.getZone()).minusYears(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),previousMonths,highestDay,highestHour,highestMinute,highestSecond,date.getZone());
  }
  if (!days.getValues().contains(date.getDayOfMonth())) {
    nearestValue=days.getPreviousValue(date.getDayOfMonth(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),1,23,59,59),date.getZone()).minusMonths(nearestValue.getShifts()).with(lastDayOfMonth());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),nearestValue.getValue(),highestHour,highestMinute,highestSecond,date.getZone());
  }
  if (!hours.getValues().contains(date.getHour())) {
    nearestValue=hours.getPreviousValue(date.getHour(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),23,59,59),date.getZone()).minusDays(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),nearestValue.getValue(),highestMinute,highestSecond,date.getZone());
  }
  if (!minutes.getValues().contains(date.getMinute())) {
    nearestValue=minutes.getPreviousValue(date.getMinute(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),59,59),date.getZone()).minusHours(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),nearestValue.getValue(),highestSecond,date.getZone());
  }
  if (!seconds.getValues().contains(date.getSecond())) {
    nearestValue=seconds.getPreviousValue(date.getSecond(),0);
    int previousSeconds=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),59),date.getZone()).minusMinutes(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),previousSeconds,date.getZone());
  }
  return date;
}","/** 
 * If date is not match, will return previous closest match. If date is match, will return this date.
 * @param date - reference ZonedDateTime instance - never null;
 * @return ZonedDateTime instance, never null. Value obeys logic specified above.
 * @throws NoSuchValueException
 */
private ZonedDateTime previousClosestMatch(ZonedDateTime date) throws NoSuchValueException {
  List<Integer> year=yearsValueGenerator.generateCandidates(date.getYear(),date.getYear());
  TimeNode days=generateDays(cronDefinition,date);
  int highestMonth=months.getValues().get(months.getValues().size() - 1);
  int highestDay=days.getValues().get(days.getValues().size() - 1);
  int highestHour=hours.getValues().get(hours.getValues().size() - 1);
  int highestMinute=minutes.getValues().get(minutes.getValues().size() - 1);
  int highestSecond=seconds.getValues().get(seconds.getValues().size() - 1);
  NearestValue nearestValue;
  ZonedDateTime newDate;
  if (year.isEmpty()) {
    int previousYear=yearsValueGenerator.generatePreviousValue(date.getYear());
    if (highestDay > 28) {
      int highestDayOfMonth=LocalDate.of(previousYear,highestMonth,1).lengthOfMonth();
      if (highestDay > highestDayOfMonth) {
        nearestValue=days.getPreviousValue(highestDay,1);
        if (nearestValue.getShifts() > 0) {
          newDate=ZonedDateTime.of(LocalDateTime.of(previousYear,highestMonth,1,23,59,59),ZoneId.systemDefault()).minusMonths(nearestValue.getShifts()).with(lastDayOfMonth());
          return previousClosestMatch(newDate);
        }
 else {
          highestDay=nearestValue.getValue();
        }
      }
    }
    return initDateTime(previousYear,highestMonth,highestDay,highestHour,highestMinute,highestSecond,date.getZone());
  }
  if (!months.getValues().contains(date.getMonthValue())) {
    nearestValue=months.getPreviousValue(date.getMonthValue(),0);
    int previousMonths=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),12,31,23,59,59),date.getZone()).minusYears(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),previousMonths,highestDay,highestHour,highestMinute,highestSecond,date.getZone());
  }
  if (!days.getValues().contains(date.getDayOfMonth())) {
    nearestValue=days.getPreviousValue(date.getDayOfMonth(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),1,23,59,59),date.getZone()).minusMonths(nearestValue.getShifts()).with(lastDayOfMonth());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),nearestValue.getValue(),highestHour,highestMinute,highestSecond,date.getZone());
  }
  if (!hours.getValues().contains(date.getHour())) {
    nearestValue=hours.getPreviousValue(date.getHour(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),23,59,59),date.getZone()).minusDays(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),nearestValue.getValue(),highestMinute,highestSecond,date.getZone());
  }
  if (!minutes.getValues().contains(date.getMinute())) {
    nearestValue=minutes.getPreviousValue(date.getMinute(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),59,59),date.getZone()).minusHours(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),nearestValue.getValue(),highestSecond,date.getZone());
  }
  if (!seconds.getValues().contains(date.getSecond())) {
    nearestValue=seconds.getPreviousValue(date.getSecond(),0);
    int previousSeconds=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),59),date.getZone()).minusMinutes(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),previousSeconds,date.getZone());
  }
  return date;
}"
4261,"TimeNode generateDays(CronDefinition cronDefinition,ZonedDateTime date){
  boolean questionMarkSupported=cronDefinition.getFieldDefinition(DAY_OF_WEEK).getConstraints().getSpecialChars().contains(QUESTION_MARK);
  if (questionMarkSupported) {
    return new TimeNode(generateDayCandidatesQuestionMarkSupported(date.getYear(),date.getMonthValue(),((DayOfWeekFieldDefinition)cronDefinition.getFieldDefinition(DAY_OF_WEEK)).getMondayDoWValue()));
  }
 else {
    return new TimeNode(generateDayCandidatesQuestionMarkNotSupported(date.getYear(),date.getMonthValue(),((DayOfWeekFieldDefinition)cronDefinition.getFieldDefinition(DAY_OF_WEEK)).getMondayDoWValue()));
  }
}","private TimeNode generateDays(CronDefinition cronDefinition,ZonedDateTime date){
  boolean questionMarkSupported=cronDefinition.getFieldDefinition(DAY_OF_WEEK).getConstraints().getSpecialChars().contains(QUESTION_MARK);
  if (questionMarkSupported) {
    return new TimeNode(generateDayCandidatesQuestionMarkSupported(date.getYear(),date.getMonthValue(),((DayOfWeekFieldDefinition)cronDefinition.getFieldDefinition(DAY_OF_WEEK)).getMondayDoWValue()));
  }
 else {
    return new TimeNode(generateDayCandidatesQuestionMarkNotSupported(date.getYear(),date.getMonthValue(),((DayOfWeekFieldDefinition)cronDefinition.getFieldDefinition(DAY_OF_WEEK)).getMondayDoWValue()));
  }
}"
4262,"/** 
 * Creates execution time for given Cron
 * @param cron - Cron instance
 * @return ExecutionTime instance
 */
public static ExecutionTime forCron(Cron cron){
  Map<CronFieldName,CronField> fields=cron.retrieveFieldsAsMap();
  ExecutionTimeBuilder executionTimeBuilder=new ExecutionTimeBuilder(cron.getCronDefinition());
  for (  CronFieldName name : CronFieldName.values()) {
    if (fields.get(name) != null) {
switch (name) {
case SECOND:
        executionTimeBuilder.forSecondsMatching(fields.get(name));
      break;
case MINUTE:
    executionTimeBuilder.forMinutesMatching(fields.get(name));
  break;
case HOUR:
executionTimeBuilder.forHoursMatching(fields.get(name));
break;
case DAY_OF_WEEK:
executionTimeBuilder.forDaysOfWeekMatching(fields.get(name));
break;
case DAY_OF_MONTH:
executionTimeBuilder.forDaysOfMonthMatching(fields.get(name));
break;
case MONTH:
executionTimeBuilder.forMonthsMatching(fields.get(name));
break;
case YEAR:
executionTimeBuilder.forYearsMatching(fields.get(name));
break;
}
}
}
return executionTimeBuilder.build();
}","/** 
 * Creates execution time for given Cron
 * @param cron - Cron instance
 * @return ExecutionTime instance
 */
public static ExecutionTime forCron(Cron cron){
  Map<CronFieldName,CronField> fields=cron.retrieveFieldsAsMap();
  ExecutionTimeBuilder executionTimeBuilder=new ExecutionTimeBuilder(cron.getCronDefinition());
  for (  CronFieldName name : CronFieldName.values()) {
    if (fields.get(name) != null) {
switch (name) {
case SECOND:
        executionTimeBuilder.forSecondsMatching(fields.get(name));
      break;
case MINUTE:
    executionTimeBuilder.forMinutesMatching(fields.get(name));
  break;
case HOUR:
executionTimeBuilder.forHoursMatching(fields.get(name));
break;
case DAY_OF_WEEK:
executionTimeBuilder.forDaysOfWeekMatching(fields.get(name));
break;
case DAY_OF_MONTH:
executionTimeBuilder.forDaysOfMonthMatching(fields.get(name));
break;
case MONTH:
executionTimeBuilder.forMonthsMatching(fields.get(name));
break;
case YEAR:
executionTimeBuilder.forYearsMatching(fields.get(name));
break;
default :
break;
}
}
}
return executionTimeBuilder.build();
}"
4263,"/** 
 * If date is not match, will return next closest match. If date is match, will return this date.
 * @param date - reference ZonedDateTime instance - never null;
 * @return ZonedDateTime instance, never null. Value obeys logic specified above.
 * @throws NoSuchValueException
 */
ZonedDateTime nextClosestMatch(ZonedDateTime date) throws NoSuchValueException {
  List<Integer> year=yearsValueGenerator.generateCandidates(date.getYear(),date.getYear());
  TimeNode days=null;
  int lowestMonth=months.getValues().get(0);
  int lowestHour=hours.getValues().get(0);
  int lowestMinute=minutes.getValues().get(0);
  int lowestSecond=seconds.getValues().get(0);
  NearestValue nearestValue;
  ZonedDateTime newDate;
  if (year.isEmpty()) {
    int newYear=yearsValueGenerator.generateNextValue(date.getYear());
    days=generateDays(cronDefinition,ZonedDateTime.of(LocalDateTime.of(newYear,lowestMonth,1,0,0),date.getZone()));
    return initDateTime(yearsValueGenerator.generateNextValue(date.getYear()),lowestMonth,days.getValues().get(0),lowestHour,lowestMinute,lowestSecond,date.getZone());
  }
  if (!months.getValues().contains(date.getMonthValue())) {
    nearestValue=months.getNextValue(date.getMonthValue(),0);
    int nextMonths=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),1,1,0,0,0),date.getZone()).plusYears(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getMonthValue()) {
      date=date.plusYears(1);
    }
    days=generateDays(cronDefinition,ZonedDateTime.of(LocalDateTime.of(date.getYear(),nextMonths,1,0,0),date.getZone()));
    return initDateTime(date.getYear(),nextMonths,days.getValues().get(0),lowestHour,lowestMinute,lowestSecond,date.getZone());
  }
  days=generateDays(cronDefinition,date);
  if (!days.getValues().contains(date.getDayOfMonth())) {
    nearestValue=days.getNextValue(date.getDayOfMonth(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),1,0,0,0),date.getZone()).plusMonths(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getDayOfMonth()) {
      date=date.plusMonths(1);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),nearestValue.getValue(),lowestHour,lowestMinute,lowestSecond,date.getZone());
  }
  if (!hours.getValues().contains(date.getHour())) {
    nearestValue=hours.getNextValue(date.getHour(),0);
    int nextHours=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),0,0,0),date.getZone()).plusDays(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getHour()) {
      date=date.plusDays(1);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),nextHours,lowestMinute,lowestSecond,date.getZone());
  }
  if (!minutes.getValues().contains(date.getMinute())) {
    nearestValue=minutes.getNextValue(date.getMinute(),0);
    int nextMinutes=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),0,0),date.getZone()).plusHours(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getMinute()) {
      date=date.plusHours(1);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),nextMinutes,lowestSecond,date.getZone());
  }
  if (!seconds.getValues().contains(date.getSecond())) {
    nearestValue=seconds.getNextValue(date.getSecond(),0);
    int nextSeconds=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),0),date.getZone()).plusMinutes(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getSecond()) {
      date=date.plusMinutes(1);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),nextSeconds,date.getZone());
  }
  return date;
}","/** 
 * If date is not match, will return next closest match. If date is match, will return this date.
 * @param date - reference ZonedDateTime instance - never null;
 * @return ZonedDateTime instance, never null. Value obeys logic specified above.
 * @throws NoSuchValueException
 */
private ZonedDateTime nextClosestMatch(ZonedDateTime date) throws NoSuchValueException {
  List<Integer> year=yearsValueGenerator.generateCandidates(date.getYear(),date.getYear());
  TimeNode days=null;
  int lowestMonth=months.getValues().get(0);
  int lowestHour=hours.getValues().get(0);
  int lowestMinute=minutes.getValues().get(0);
  int lowestSecond=seconds.getValues().get(0);
  NearestValue nearestValue;
  ZonedDateTime newDate;
  if (year.isEmpty()) {
    int newYear=yearsValueGenerator.generateNextValue(date.getYear());
    days=generateDays(cronDefinition,ZonedDateTime.of(LocalDateTime.of(newYear,lowestMonth,1,0,0),date.getZone()));
    return initDateTime(yearsValueGenerator.generateNextValue(date.getYear()),lowestMonth,days.getValues().get(0),lowestHour,lowestMinute,lowestSecond,date.getZone());
  }
  if (!months.getValues().contains(date.getMonthValue())) {
    nearestValue=months.getNextValue(date.getMonthValue(),0);
    int nextMonths=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),1,1,0,0,0),date.getZone()).plusYears(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getMonthValue()) {
      date=date.plusYears(1);
    }
    days=generateDays(cronDefinition,ZonedDateTime.of(LocalDateTime.of(date.getYear(),nextMonths,1,0,0),date.getZone()));
    return initDateTime(date.getYear(),nextMonths,days.getValues().get(0),lowestHour,lowestMinute,lowestSecond,date.getZone());
  }
  days=generateDays(cronDefinition,date);
  if (!days.getValues().contains(date.getDayOfMonth())) {
    nearestValue=days.getNextValue(date.getDayOfMonth(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),1,0,0,0),date.getZone()).plusMonths(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getDayOfMonth()) {
      date=date.plusMonths(1);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),nearestValue.getValue(),lowestHour,lowestMinute,lowestSecond,date.getZone());
  }
  if (!hours.getValues().contains(date.getHour())) {
    nearestValue=hours.getNextValue(date.getHour(),0);
    int nextHours=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),0,0,0),date.getZone()).plusDays(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getHour()) {
      date=date.plusDays(1);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),nextHours,lowestMinute,lowestSecond,date.getZone());
  }
  if (!minutes.getValues().contains(date.getMinute())) {
    nearestValue=minutes.getNextValue(date.getMinute(),0);
    int nextMinutes=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),0,0),date.getZone()).plusHours(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getMinute()) {
      date=date.plusHours(1);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),nextMinutes,lowestSecond,date.getZone());
  }
  if (!seconds.getValues().contains(date.getSecond())) {
    nearestValue=seconds.getNextValue(date.getSecond(),0);
    int nextSeconds=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=ZonedDateTime.of(LocalDateTime.of(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),0),date.getZone()).plusMinutes(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getSecond()) {
      date=date.plusMinutes(1);
    }
    return initDateTime(date.getYear(),date.getMonthValue(),date.getDayOfMonth(),date.getHour(),date.getMinute(),nextSeconds,date.getZone());
  }
  return date;
}"
4264,"private List<Integer> generateDayCandidatesQuestionMarkSupported(int year,int month,WeekDay mondayDoWValue){
  LocalDate date=LocalDate.of(year,month,1);
  int lengthOfMonth=date.lengthOfMonth();
  Set<Integer> candidates=new HashSet<>();
  if (daysOfMonthCronField.getExpression() instanceof Always && daysOfWeekCronField.getExpression() instanceof Always) {
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
 else   if (daysOfMonthCronField.getExpression() instanceof QuestionMark) {
    candidates.addAll(createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(-1,lengthOfMonth));
  }
 else   if (daysOfWeekCronField.getExpression() instanceof QuestionMark) {
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
 else {
    candidates.addAll(createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(1,lengthOfMonth));
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
  List<Integer> candidatesList=new ArrayList<>(candidates);
  Collections.sort(candidatesList);
  return candidatesList;
}","private List<Integer> generateDayCandidatesQuestionMarkSupported(int year,int month,WeekDay mondayDoWValue){
  LocalDate date=LocalDate.of(year,month,1);
  int lengthOfMonth=date.lengthOfMonth();
  Set<Integer> candidates=new HashSet<>();
  if (daysOfMonthCronField.getExpression() instanceof Always && daysOfWeekCronField.getExpression() instanceof Always) {
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
 else   if (daysOfMonthCronField.getExpression() instanceof QuestionMark) {
    candidates.addAll(createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(0,lengthOfMonth + 1));
  }
 else   if (daysOfWeekCronField.getExpression() instanceof QuestionMark) {
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
 else {
    candidates.addAll(createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(1,lengthOfMonth));
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
  List<Integer> candidatesList=new ArrayList<>(candidates);
  Collections.sort(candidatesList);
  return candidatesList;
}"
4265,"private List<Integer> generateDayCandidatesQuestionMarkSupported(int year,int month,WeekDay mondayDoWValue){
  LocalDate date=LocalDate.of(year,month,1);
  int lengthOfMonth=date.lengthOfMonth();
  Set<Integer> candidates=new HashSet<>();
  if (daysOfMonthCronField.getExpression() instanceof Always && daysOfWeekCronField.getExpression() instanceof Always) {
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
 else   if (daysOfMonthCronField.getExpression() instanceof QuestionMark) {
    candidates.addAll(createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(-1,lengthOfMonth));
  }
 else   if (daysOfWeekCronField.getExpression() instanceof QuestionMark) {
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
 else {
    candidates.addAll(createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(1,lengthOfMonth));
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
  List<Integer> candidatesList=new ArrayList<>(candidates);
  Collections.sort(candidatesList);
  return candidatesList;
}","private List<Integer> generateDayCandidatesQuestionMarkSupported(int year,int month,WeekDay mondayDoWValue){
  LocalDate date=LocalDate.of(year,month,1);
  int lengthOfMonth=date.lengthOfMonth();
  Set<Integer> candidates=new HashSet<>();
  if (daysOfMonthCronField.getExpression() instanceof Always && daysOfWeekCronField.getExpression() instanceof Always) {
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
 else   if (daysOfMonthCronField.getExpression() instanceof QuestionMark) {
    candidates.addAll(createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(0,lengthOfMonth + 1));
  }
 else   if (daysOfWeekCronField.getExpression() instanceof QuestionMark) {
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
 else {
    candidates.addAll(createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(1,lengthOfMonth));
    candidates.addAll(createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,lengthOfMonth));
  }
  List<Integer> candidatesList=new ArrayList<>(candidates);
  Collections.sort(candidatesList);
  return candidatesList;
}"
4266,"@Test public void testDescriptionDayOfWeek(){
  assertExpression(""String_Node_Str"",""String_Node_Str"");
}","public void testDescriptionDayOfWeek(){
  assertExpression(""String_Node_Str"",""String_Node_Str"");
}"
4267,"private Every(Every every){
  this(every.getTime());
}","private Every(Every every){
  this(every.getStartValue(),every.getTime());
}"
4268,"@Override public String asString(){
  if (time.getValue() == 1) {
    return ""String_Node_Str"";
  }
  return String.format(""String_Node_Str"",getTime());
}","@Override public String asString(){
  if (time.getValue() == 1) {
    return startValue != null ? ""String_Node_Str"" : ""String_Node_Str"";
  }
  return String.format(""String_Node_Str"",this.startValue != null ? this.startValue.toString() : ""String_Node_Str"",getTime());
}"
4269,"/** 
 * Parse given expression for a single cron field
 * @param expression - String
 * @return CronFieldExpression object that with interpretation of given String parameter
 */
public FieldExpression parse(String expression){
  if (!StringUtils.containsAny(expression,specialCharsMinusStar)) {
    if (""String_Node_Str"".equals(expression)) {
      return new Always();
    }
 else {
      if (""String_Node_Str"".equals(expression)) {
        return new QuestionMark();
      }
      return parseOn(expression);
    }
  }
 else {
    String[] array=expression.split(""String_Node_Str"");
    if (array.length > 1) {
      And and=new And();
      for (      String exp : array) {
        and.and(parse(exp));
      }
      return and;
    }
 else {
      array=expression.split(""String_Node_Str"");
      if (array.length > 1) {
        return parseBetween(array);
      }
 else {
        String[] values=expression.split(""String_Node_Str"");
        if (values.length == 2) {
          String value=values[1];
          return new Every(new IntegerFieldValue(Integer.parseInt(value)));
        }
 else         if (values.length == 1) {
          throw new IllegalArgumentException(""String_Node_Str"" + expression);
        }
 else {
          throw new IllegalArgumentException(""String_Node_Str"" + expression);
        }
      }
    }
  }
}","/** 
 * Parse given expression for a single cron field
 * @param expression - String
 * @return CronFieldExpression object that with interpretation of given String parameter
 */
public FieldExpression parse(String expression){
  if (!StringUtils.containsAny(expression,specialCharsMinusStar)) {
    if (""String_Node_Str"".equals(expression)) {
      return new Always();
    }
 else {
      if (""String_Node_Str"".equals(expression)) {
        return new QuestionMark();
      }
      return parseOn(expression);
    }
  }
 else {
    String[] array=expression.split(""String_Node_Str"");
    if (array.length > 1) {
      And and=new And();
      for (      String exp : array) {
        and.and(parse(exp));
      }
      return and;
    }
 else {
      array=expression.split(""String_Node_Str"");
      if (array.length > 1) {
        return parseBetween(array);
      }
 else {
        String[] values=expression.split(""String_Node_Str"");
        if (values.length == 2) {
          String start=values[0];
          String value=values[1];
          if (""String_Node_Str"".equals(start.trim())) {
            return new Always(new IntegerFieldValue(Integer.parseInt(value)));
          }
 else {
            return new Every(new IntegerFieldValue(Integer.parseInt(start)),new IntegerFieldValue(Integer.parseInt(value)));
          }
        }
 else         if (values.length == 1) {
          throw new IllegalArgumentException(""String_Node_Str"" + expression);
        }
 else {
          throw new IllegalArgumentException(""String_Node_Str"" + expression);
        }
      }
    }
  }
}"
4270,"/** 
 * Issue #79: Next execution skipping valid date: 
 */
public void testNextExecution2014(){
  String crontab=""String_Node_Str"";
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron cron=parser.parse(crontab);
  DateTime date=DateTime.parse(""String_Node_Str"");
  ExecutionTime executionTime=ExecutionTime.forCron(cron);
  assertEquals(DateTime.parse(""String_Node_Str""),executionTime.nextExecution(date));
}","/** 
 * Issue #79: Next execution skipping valid date
 */
public void testNextExecution2014(){
  String crontab=""String_Node_Str"";
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  Cron cron=parser.parse(crontab);
  DateTime date=DateTime.parse(""String_Node_Str"");
  ExecutionTime executionTime=ExecutionTime.forCron(cron);
  assertEquals(DateTime.parse(""String_Node_Str""),executionTime.nextExecution(date));
}"
4271,"private Function<Integer,Integer> bothSameStartOfRange(final int startRange,final int endRange,final WeekDay source,final WeekDay target){
  return new Function<Integer,Integer>(){
    @Override public Integer apply(    Integer integer){
      int diff=target.getMondayDoWValue() - source.getMondayDoWValue();
      int result=integer;
      if (diff == 0) {
        return integer;
      }
      if (diff < 0) {
        result=integer + diff;
        int distanceToStartRange=startRange - result;
        if (result < startRange) {
          result=endRange + 1 - distanceToStartRange;
        }
      }
      if (diff > 0) {
        result=integer + diff;
        if (result > endRange) {
          result-=endRange;
        }
      }
      return result;
    }
  }
;
}","private Function<Integer,Integer> bothSameStartOfRange(final int startRange,final int endRange,final WeekDay source,final WeekDay target){
  return new Function<Integer,Integer>(){
    @Override public Integer apply(    Integer integer){
      int diff=target.getMondayDoWValue() - source.getMondayDoWValue();
      int result=integer + diff;
      if (result < startRange) {
        result-=(startRange - endRange + 1);
      }
      if (result > endRange) {
        result-=endRange;
      }
      return result;
    }
  }
;
}"
4272,"@Override public Integer apply(Integer integer){
  int diff=target.getMondayDoWValue() - source.getMondayDoWValue();
  int result=integer;
  if (diff == 0) {
    return integer;
  }
  if (diff < 0) {
    result=integer + diff;
    int distanceToStartRange=startRange - result;
    if (result < startRange) {
      result=endRange + 1 - distanceToStartRange;
    }
  }
  if (diff > 0) {
    result=integer + diff;
    if (result > endRange) {
      result-=endRange;
    }
  }
  return result;
}","@Override public Integer apply(Integer integer){
  int diff=target.getMondayDoWValue() - source.getMondayDoWValue();
  int result=integer + diff;
  if (result < startRange) {
    result-=(startRange - endRange + 1);
  }
  if (result > endRange) {
    result-=endRange;
  }
  return result;
}"
4273,"private int generateValue(On on,int year,int month) throws NoSuchValueException {
  int time=on.getTime().getValue();
switch (on.getSpecialChar().getValue()) {
case L:
    return new DateTime(year,month,1,1,1).dayOfMonth().getMaximumValue();
case W:
  DateTime doM=new DateTime(year,month,time,1,1);
if (doM.getDayOfWeek() == 6) {
  if (time == 1) {
    return 3;
  }
  return time - 1;
}
if (doM.getDayOfWeek() == 7) {
if ((time + 1) <= doM.dayOfMonth().getMaximumValue()) {
  return time + 1;
}
}
break;
case LW:
DateTime lastDayOfMonth=new DateTime(year,month,new DateTime(year,month,1,1,1).dayOfMonth().getMaximumValue(),1,1);
int dow=lastDayOfMonth.getDayOfWeek();
int diff=dow - 5;
if (diff > 0) {
return lastDayOfMonth.minusDays(diff).dayOfMonth().get();
}
return lastDayOfMonth.dayOfMonth().get();
}
throw new NoSuchValueException();
}","private int generateValue(On on,int year,int month) throws NoSuchValueException {
  int time=on.getTime().getValue();
switch (on.getSpecialChar().getValue()) {
case L:
    return new DateTime(year,month,1,1,1).dayOfMonth().getMaximumValue();
case W:
  DateTime doM=new DateTime(year,month,time,1,1);
if (doM.getDayOfWeek() == 6) {
  if (time == 1) {
    return 3;
  }
  return time - 1;
}
if (doM.getDayOfWeek() == 7) {
if ((time + 1) <= doM.dayOfMonth().getMaximumValue()) {
  return time + 1;
}
}
return time;
case LW:
DateTime lastDayOfMonth=new DateTime(year,month,new DateTime(year,month,1,1,1).dayOfMonth().getMaximumValue(),1,1);
int dow=lastDayOfMonth.getDayOfWeek();
int diff=dow - 5;
if (diff > 0) {
return lastDayOfMonth.minusDays(diff).dayOfMonth().get();
}
return lastDayOfMonth.dayOfMonth().get();
}
throw new NoSuchValueException();
}"
4274,"/** 
 * Issue #81: MON-SUN flags are not mapped correctly to 1-7 number representations  Fixed by adding shifting function when changing monday position. Fixed by adding shifting function when changing monday position. Fixed by adding shifting function when changing monday position.
 */
@Test public void testDayOfWeekMapping(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ);
  CronParser parser=new CronParser(cronDefinition);
  DateTime fridayMorning=new DateTime(2016,4,22,0,0,0,DateTimeZone.UTC);
  ExecutionTime numberExec=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ExecutionTime nameExec=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  assertEquals(""String_Node_Str"",numberExec.nextExecution(fridayMorning),nameExec.nextExecution(fridayMorning));
}","/** 
 * Issue #81: MON-SUN flags are not mapped correctly to 1-7 number representations  Fixed by adding shifting function when changing monday position.
 */
@Test public void testDayOfWeekMapping(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ);
  CronParser parser=new CronParser(cronDefinition);
  DateTime fridayMorning=new DateTime(2016,4,22,0,0,0,DateTimeZone.UTC);
  ExecutionTime numberExec=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  ExecutionTime nameExec=ExecutionTime.forCron(parser.parse(""String_Node_Str""));
  assertEquals(""String_Node_Str"",numberExec.nextExecution(fridayMorning),nameExec.nextExecution(fridayMorning));
}"
4275,"private DateTime initDateTime(int years,int monthsOfYear,int dayOfMonth,int hoursOfDay,int minutesOfHour,int secondsOfMinute,DateTimeZone timeZone){
  return new DateTime(0,1,1,0,0,0,timeZone).plusYears(years).plusMonths(monthsOfYear - 1).plusDays(dayOfMonth - 1).plusHours(hoursOfDay).plusMinutes(minutesOfHour).plusSeconds(secondsOfMinute);
}","private DateTime initDateTime(int years,int monthsOfYear,int dayOfMonth,int hoursOfDay,int minutesOfHour,int secondsOfMinute,DateTimeZone timeZone){
  DateTime date=new DateTime(0,1,1,0,0,0,timeZone).plusYears(years).plusMonths(monthsOfYear - 1).plusDays(dayOfMonth - 1).plusHours(hoursOfDay).plusMinutes(minutesOfHour).plusSeconds(secondsOfMinute);
  if (date.getSecondOfMinute() != secondsOfMinute) {
    date=date.plusSeconds(secondsOfMinute - date.getSecondOfMinute());
  }
  if (date.getMinuteOfHour() != minutesOfHour) {
    date=date.plusMinutes(minutesOfHour - date.getMinuteOfHour());
  }
  if (date.getHourOfDay() != hoursOfDay) {
    date=date.plusHours(hoursOfDay - date.getHourOfDay());
  }
  if (date.getDayOfMonth() != dayOfMonth) {
    date=date.plusDays(dayOfMonth - date.getDayOfMonth());
  }
  if (date.getMonthOfYear() != monthsOfYear) {
    date=date.plusMonths(monthsOfYear - date.getMonthOfYear());
  }
  if (date.getYear() != years) {
    date=date.plusYears(years - date.getYear());
  }
  return date;
}"
4276,"@Test public void testDayLightSavingsSwitch(){
  try {
    String expression=""String_Node_Str"";
    CronParser parser=new CronParser(CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ));
    Cron cron=parser.parse(expression);
    DateTime prevRun=new DateTime(new SimpleDateFormat(""String_Node_Str"").parseObject(""String_Node_Str""));
    ExecutionTime executionTime=ExecutionTime.forCron(cron);
    DateTime nextRun=executionTime.nextExecution(prevRun);
    assertEquals(""String_Node_Str"",3,nextRun.getHourOfDay());
    assertEquals(""String_Node_Str"",0,nextRun.getMinuteOfHour());
    nextRun=nextRun.plusMinutes(1);
    nextRun=executionTime.nextExecution(nextRun);
    assertEquals(""String_Node_Str"",3,nextRun.getHourOfDay());
    assertEquals(""String_Node_Str"",2,nextRun.getMinuteOfHour());
    prevRun=new DateTime(new SimpleDateFormat(""String_Node_Str"").parseObject(""String_Node_Str""));
    nextRun=executionTime.nextExecution(prevRun);
    assertEquals(""String_Node_Str"",nextRun.getHourOfDay(),0);
    assertEquals(""String_Node_Str"",nextRun.getMinuteOfHour(),2);
  }
 catch (  Exception e) {
    fail(""String_Node_Str"" + e.getMessage());
  }
}","@Test public void testDayLightSavingsSwitch(){
  try {
    String expression=""String_Node_Str"";
    CronParser parser=new CronParser(CronDefinitionBuilder.instanceDefinitionFor(CronType.QUARTZ));
    Cron cron=parser.parse(expression);
    DateTimeFormatter formatter=DateTimeFormat.forPattern(""String_Node_Str"").withZone(DateTimeZone.forID(""String_Node_Str""));
    DateTime prevRun=new DateTime(formatter.parseDateTime(""String_Node_Str""));
    ExecutionTime executionTime=ExecutionTime.forCron(cron);
    DateTime nextRun=executionTime.nextExecution(prevRun);
    assertEquals(""String_Node_Str"",3,nextRun.getHourOfDay());
    assertEquals(""String_Node_Str"",0,nextRun.getMinuteOfHour());
    nextRun=nextRun.plusMinutes(1);
    nextRun=executionTime.nextExecution(nextRun);
    assertEquals(""String_Node_Str"",3,nextRun.getHourOfDay());
    assertEquals(""String_Node_Str"",2,nextRun.getMinuteOfHour());
    prevRun=new DateTime(new SimpleDateFormat(""String_Node_Str"").parseObject(""String_Node_Str""));
    nextRun=executionTime.nextExecution(prevRun);
    assertEquals(""String_Node_Str"",nextRun.getHourOfDay(),0);
    assertEquals(""String_Node_Str"",nextRun.getMinuteOfHour(),2);
  }
 catch (  Exception e) {
    fail(""String_Node_Str"" + e.getMessage());
  }
}"
4277,"@Override public FieldValue apply(FieldValue fieldValue){
  if (fieldValue instanceof IntegerFieldValue) {
    return new IntegerFieldValue(ConstantsMapper.weekDayMapping(sourceDef.getMondayDoWValue(),targetDef.getMondayDoWValue(),((IntegerFieldValue)fieldValue).getValue()));
  }
 else {
    return fieldValue;
  }
}","@Override public CronField apply(final CronField field){
  FieldExpression expression=field.getExpression();
  FieldExpression dest=expression;
  if (expression instanceof QuestionMark) {
    if (!targetDef.getConstraints().getSpecialChars().contains(SpecialChar.QUESTION_MARK)) {
      dest=new Always();
    }
  }
  return new CronField(CronFieldName.DAY_OF_MONTH,dest,targetDef.getConstraints());
}"
4278,"@VisibleForTesting static Function<CronField,CronField> dayOfWeekMapping(final DayOfWeekFieldDefinition sourceDef,final DayOfWeekFieldDefinition targetDef){
  return new Function<CronField,CronField>(){
    @Override public CronField apply(    final CronField field){
      FieldExpression expression=field.getExpression().accept(new ValueMappingFieldExpressionVisitor(new Function<FieldValue,FieldValue>(){
        @Override public FieldValue apply(        FieldValue fieldValue){
          if (fieldValue instanceof IntegerFieldValue) {
            return new IntegerFieldValue(ConstantsMapper.weekDayMapping(sourceDef.getMondayDoWValue(),targetDef.getMondayDoWValue(),((IntegerFieldValue)fieldValue).getValue()));
          }
 else {
            return fieldValue;
          }
        }
      }
));
      return new CronField(CronFieldName.DAY_OF_WEEK,expression,targetDef.getConstraints());
    }
  }
;
}","@VisibleForTesting static Function<CronField,CronField> dayOfWeekMapping(final DayOfWeekFieldDefinition sourceDef,final DayOfWeekFieldDefinition targetDef){
  return new Function<CronField,CronField>(){
    @Override public CronField apply(    final CronField field){
      FieldExpression expression=field.getExpression();
      FieldExpression dest=null;
      dest=expression.accept(new ValueMappingFieldExpressionVisitor(new Function<FieldValue,FieldValue>(){
        @Override public FieldValue apply(        FieldValue fieldValue){
          if (fieldValue instanceof IntegerFieldValue) {
            return new IntegerFieldValue(ConstantsMapper.weekDayMapping(sourceDef.getMondayDoWValue(),targetDef.getMondayDoWValue(),((IntegerFieldValue)fieldValue).getValue()));
          }
          return fieldValue;
        }
      }
));
      if (expression instanceof QuestionMark) {
        if (!targetDef.getConstraints().getSpecialChars().contains(SpecialChar.QUESTION_MARK)) {
          dest=new Always();
        }
      }
      return new CronField(CronFieldName.DAY_OF_WEEK,dest,targetDef.getConstraints());
    }
  }
;
}"
4279,"/** 
 * Constructor
 * @param from - source CronDefinition;if null a NullPointerException will be raised
 * @param to - target CronDefinition;if null a NullPointerException will be raised
 */
public CronMapper(CronDefinition from,CronDefinition to){
  Validate.notNull(from,""String_Node_Str"");
  this.to=Validate.notNull(to,""String_Node_Str"");
  mappings=Maps.newHashMap();
  buildMappings(from,to);
}","/** 
 * Constructor
 * @param from - source CronDefinition;if null a NullPointerException will be raised
 * @param to - target CronDefinition;if null a NullPointerException will be raised
 */
public CronMapper(CronDefinition from,CronDefinition to,Function<Cron,Cron> cronRules){
  Validate.notNull(from,""String_Node_Str"");
  this.to=Validate.notNull(to,""String_Node_Str"");
  this.cronRules=Validate.notNull(cronRules,""String_Node_Str"");
  mappings=Maps.newHashMap();
  buildMappings(from,to);
}"
4280,"/** 
 * Maps given cron to target cron definition
 * @param cron - Instance to be mapped;if null a NullPointerException will be raised
 * @return new Cron instance, never null;
 */
public Cron map(Cron cron){
  Validate.notNull(cron,""String_Node_Str"");
  List<CronField> fields=Lists.newArrayList();
  for (  CronFieldName name : CronFieldName.values()) {
    if (mappings.containsKey(name)) {
      fields.add(mappings.get(name).apply(cron.retrieve(name)));
    }
  }
  return new Cron(to,fields);
}","/** 
 * Maps given cron to target cron definition
 * @param cron - Instance to be mapped;if null a NullPointerException will be raised
 * @return new Cron instance, never null;
 */
public Cron map(Cron cron){
  Validate.notNull(cron,""String_Node_Str"");
  List<CronField> fields=Lists.newArrayList();
  for (  CronFieldName name : CronFieldName.values()) {
    if (mappings.containsKey(name)) {
      fields.add(mappings.get(name).apply(cron.retrieve(name)));
    }
  }
  return cronRules.apply(new Cron(to,fields)).validate();
}"
4281,"/** 
 * Builds functions that map the fields from source CronDefinition to target
 * @param from - source CronDefinition
 * @param to - target CronDefinition
 */
private void buildMappings(CronDefinition from,CronDefinition to){
  Map<CronFieldName,FieldDefinition> sourceFieldDefinitions=Maps.newHashMap();
  Map<CronFieldName,FieldDefinition> destFieldDefinitions=Maps.newHashMap();
  for (  FieldDefinition fieldDefinition : from.getFieldDefinitions()) {
    sourceFieldDefinitions.put(fieldDefinition.getFieldName(),fieldDefinition);
  }
  for (  FieldDefinition fieldDefinition : to.getFieldDefinitions()) {
    destFieldDefinitions.put(fieldDefinition.getFieldName(),fieldDefinition);
  }
  boolean startedDestMapping=false;
  boolean startedSourceMapping=false;
  for (  CronFieldName name : CronFieldName.values()) {
    if (destFieldDefinitions.get(name) != null) {
      startedDestMapping=true;
    }
    if (sourceFieldDefinitions.get(name) != null) {
      startedSourceMapping=true;
    }
    if (startedDestMapping && destFieldDefinitions.get(name) == null) {
      break;
    }
    if (!startedSourceMapping && sourceFieldDefinitions.get(name) == null && destFieldDefinitions.get(name) != null) {
      mappings.put(name,returnOnZeroExpression(name));
    }
    if (startedSourceMapping && sourceFieldDefinitions.get(name) == null && destFieldDefinitions.get(name) != null) {
      mappings.put(name,returnAlwaysExpression(name));
    }
    if (sourceFieldDefinitions.get(name) != null && destFieldDefinitions.get(name) != null) {
      if (CronFieldName.DAY_OF_WEEK.equals(name)) {
        mappings.put(name,dayOfWeekMapping((DayOfWeekFieldDefinition)sourceFieldDefinitions.get(name),(DayOfWeekFieldDefinition)destFieldDefinitions.get(name)));
      }
 else {
        mappings.put(name,returnSameExpression());
      }
    }
  }
}","/** 
 * Builds functions that map the fields from source CronDefinition to target
 * @param from - source CronDefinition
 * @param to - target CronDefinition
 */
private void buildMappings(CronDefinition from,CronDefinition to){
  Map<CronFieldName,FieldDefinition> sourceFieldDefinitions=Maps.newHashMap();
  Map<CronFieldName,FieldDefinition> destFieldDefinitions=Maps.newHashMap();
  for (  FieldDefinition fieldDefinition : from.getFieldDefinitions()) {
    sourceFieldDefinitions.put(fieldDefinition.getFieldName(),fieldDefinition);
  }
  for (  FieldDefinition fieldDefinition : to.getFieldDefinitions()) {
    destFieldDefinitions.put(fieldDefinition.getFieldName(),fieldDefinition);
  }
  boolean startedDestMapping=false;
  boolean startedSourceMapping=false;
  for (  CronFieldName name : CronFieldName.values()) {
    if (destFieldDefinitions.get(name) != null) {
      startedDestMapping=true;
    }
    if (sourceFieldDefinitions.get(name) != null) {
      startedSourceMapping=true;
    }
    if (startedDestMapping && destFieldDefinitions.get(name) == null) {
      break;
    }
    if (!startedSourceMapping && sourceFieldDefinitions.get(name) == null && destFieldDefinitions.get(name) != null) {
      mappings.put(name,returnOnZeroExpression(name));
    }
    if (startedSourceMapping && sourceFieldDefinitions.get(name) == null && destFieldDefinitions.get(name) != null) {
      mappings.put(name,returnAlwaysExpression(name));
    }
    if (sourceFieldDefinitions.get(name) != null && destFieldDefinitions.get(name) != null) {
      if (CronFieldName.DAY_OF_WEEK.equals(name)) {
        mappings.put(name,dayOfWeekMapping((DayOfWeekFieldDefinition)sourceFieldDefinitions.get(name),(DayOfWeekFieldDefinition)destFieldDefinitions.get(name)));
      }
 else {
        if (CronFieldName.DAY_OF_MONTH.equals(name)) {
          mappings.put(name,dayOfMonthMapping(sourceFieldDefinitions.get(name),destFieldDefinitions.get(name)));
        }
 else {
          mappings.put(name,returnSameExpression());
        }
      }
    }
  }
}"
4282,"public Cron validate(){
  for (  CronConstraint constraint : getCronDefinition().getCronConstraints()) {
    if (!constraint.validate(this)) {
      throw new RuntimeException(String.format(""String_Node_Str"",asString(),constraint.getDescription()));
    }
  }
  for (  Map.Entry<CronFieldName,CronField> field : retrieveFieldsAsMap().entrySet()) {
    CronFieldName fieldName=field.getKey();
    field.getValue().getExpression().accept(new ValidationFieldExpressionVisitor(getCronDefinition().getFieldDefinition(fieldName).getConstraints()));
  }
  return this;
}","public Cron validate(){
  for (  Map.Entry<CronFieldName,CronField> field : retrieveFieldsAsMap().entrySet()) {
    CronFieldName fieldName=field.getKey();
    field.getValue().getExpression().accept(new ValidationFieldExpressionVisitor(getCronDefinition().getFieldDefinition(fieldName).getConstraints(),cronDefinition.isStrictRanges()));
  }
  for (  CronConstraint constraint : getCronDefinition().getCronConstraints()) {
    if (!constraint.validate(this)) {
      throw new IllegalArgumentException(String.format(""String_Node_Str"",asString(),constraint.getDescription()));
    }
  }
  return this;
}"
4283,"/** 
 * Constructor
 * @param fieldDefinitions - list with field definitions. Must not be null or empty.Throws a NullPointerException if a null values is received Throws an IllegalArgumentException if an empty list is received
 * @param lastFieldOptional - boolean, value stating if last field is optional
 */
public CronDefinition(List<FieldDefinition> fieldDefinitions,Set<CronConstraint> cronConstraints,boolean lastFieldOptional){
  Validate.notNull(fieldDefinitions,""String_Node_Str"");
  Validate.notNull(cronConstraints,""String_Node_Str"");
  Validate.notEmpty(fieldDefinitions,""String_Node_Str"");
  if (lastFieldOptional) {
    Validate.isTrue(fieldDefinitions.size() > 1,""String_Node_Str"");
  }
  this.fieldDefinitions=Maps.newHashMap();
  for (  FieldDefinition field : fieldDefinitions) {
    this.fieldDefinitions.put(field.getFieldName(),field);
  }
  this.cronConstraints=Collections.unmodifiableSet(cronConstraints);
  this.lastFieldOptional=lastFieldOptional;
}","/** 
 * Constructor
 * @param fieldDefinitions - list with field definitions. Must not be null or empty.Throws a NullPointerException if a null values is received Throws an IllegalArgumentException if an empty list is received
 * @param lastFieldOptional - boolean, value stating if last field is optional
 */
public CronDefinition(List<FieldDefinition> fieldDefinitions,Set<CronConstraint> cronConstraints,boolean lastFieldOptional,boolean strictRanges){
  Validate.notNull(fieldDefinitions,""String_Node_Str"");
  Validate.notNull(cronConstraints,""String_Node_Str"");
  Validate.notEmpty(fieldDefinitions,""String_Node_Str"");
  if (lastFieldOptional) {
    Validate.isTrue(fieldDefinitions.size() > 1,""String_Node_Str"");
  }
  this.fieldDefinitions=Maps.newHashMap();
  for (  FieldDefinition field : fieldDefinitions) {
    this.fieldDefinitions.put(field.getFieldName(),field);
  }
  this.cronConstraints=Collections.unmodifiableSet(cronConstraints);
  this.lastFieldOptional=lastFieldOptional;
  this.strictRanges=strictRanges;
}"
4284,"/** 
 * Creates a new CronDefinition instance with provided field definitions
 * @return returns CronDefinition instance, never null
 */
public CronDefinition instance(){
  Set<CronConstraint> validations=new HashSet<CronConstraint>();
  validations.addAll(cronConstraints);
  return new CronDefinition(Lists.newArrayList(this.fields.values()),validations,lastFieldOptional);
}","/** 
 * Creates a new CronDefinition instance with provided field definitions
 * @return returns CronDefinition instance, never null
 */
public CronDefinition instance(){
  Set<CronConstraint> validations=new HashSet<CronConstraint>();
  validations.addAll(cronConstraints);
  return new CronDefinition(Lists.newArrayList(this.fields.values()),validations,lastFieldOptional,enforceStrictRanges);
}"
4285,"/** 
 * Creates CronDefinition instance matching cron4j specification;
 * @return CronDefinition instance, never null;
 */
private static CronDefinition cron4j(){
  return CronDefinitionBuilder.defineCron().withMinutes().and().withHours().and().withDayOfMonth().and().withMonth().and().withDayOfWeek().withValidRange(0,6).withMondayDoWValue(1).and().instance();
}","/** 
 * Creates CronDefinition instance matching cron4j specification;
 * @return CronDefinition instance, never null;
 */
private static CronDefinition cron4j(){
  return CronDefinitionBuilder.defineCron().withMinutes().and().withHours().and().withDayOfMonth().and().withMonth().and().withDayOfWeek().withValidRange(0,6).withMondayDoWValue(1).and().enforceStrictRanges().instance();
}"
4286,"/** 
 * Creates CronDefinition instance matching unix crontab specification;
 * @return CronDefinition instance, never null;
 */
private static CronDefinition unixCrontab(){
  return CronDefinitionBuilder.defineCron().withMinutes().and().withHours().and().withDayOfMonth().and().withMonth().and().withDayOfWeek().withValidRange(0,7).withMondayDoWValue(1).withIntMapping(7,0).and().instance();
}","/** 
 * Creates CronDefinition instance matching unix crontab specification;
 * @return CronDefinition instance, never null;
 */
private static CronDefinition unixCrontab(){
  return CronDefinitionBuilder.defineCron().withMinutes().and().withHours().and().withDayOfMonth().and().withMonth().and().withDayOfWeek().withValidRange(0,7).withMondayDoWValue(1).withIntMapping(7,0).and().enforceStrictRanges().instance();
}"
4287,"/** 
 * Constructor. lastFieldOptional is defined false.
 */
private CronDefinitionBuilder(){
  fields=Maps.newHashMap();
  cronConstraints=Sets.newHashSet();
  lastFieldOptional=false;
}","/** 
 * Constructor. lastFieldOptional is defined false.
 */
private CronDefinitionBuilder(){
  fields=Maps.newHashMap();
  cronConstraints=Sets.newHashSet();
  lastFieldOptional=false;
  enforceStrictRanges=false;
}"
4288,"/** 
 * Performs action on QuestionMark instance
 * @param questionMark - QuestionMark instance, never null
 * @return QuestionMark instance, never null
 */
QuestionMark visit(QuestionMark questionMark);","/** 
 * Performs action on QuestionMark instance
 * @param questionMark - QuestionMark instance, never null
 * @return FieldExpression instance, never null
 */
FieldExpression visit(QuestionMark questionMark);"
4289,"@Override public int generateNextValue(int reference) throws NoSuchValueException {
  Every every=(Every)expression;
  int period=every.getTime().getValue();
  return reference + period;
}","@Override public int generateNextValue(int reference) throws NoSuchValueException {
  if (reference >= expression.getConstraints().getEndRange()) {
    throw new NoSuchValueException();
  }
  Every every=(Every)expression;
  int referenceWithoutOffset=reference - offset();
  int period=every.getTime().getValue();
  int remainder=referenceWithoutOffset % period;
  int next=reference + (period - remainder);
  if (next < expression.getConstraints().getStartRange()) {
    return expression.getConstraints().getStartRange();
  }
  if (next > expression.getConstraints().getEndRange()) {
    throw new NoSuchValueException();
  }
  return next;
}"
4290,"@Test() public void testGenerateNextValue() throws Exception {
  for (int j=1; j <= 10; j++) {
    int value=time * j - 1 - ((int)(2 * Math.random()));
    assertEquals(j * time,fieldValueGenerator.generateNextValue(value));
  }
}","@Test() public void testGenerateNextValue() throws Exception {
  for (int j=1; j <= 10; j++) {
    int value=time * j - (1 + ((int)(2 * Math.random())));
    System.out.println(String.format(""String_Node_Str"",value,j * time,fieldValueGenerator.generateNextValue(value)));
    assertEquals(j * time,fieldValueGenerator.generateNextValue(value));
  }
}"
4291,"/** 
 * Issue #59: Incorrect next execution time for ""month"" and ""day of week"" Considers Month in range 0-11 instead of 1-12
 */
@Test public void testCorrectMonthScaleForNextExecution1(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  String crontab=""String_Node_Str"";
  Cron cron=parser.parse(crontab);
  ExecutionTime executionTime=ExecutionTime.forCron(cron);
  DateTime scanTime=DateTime.parse(""String_Node_Str"");
  DateTime nextExecutionTime=executionTime.nextExecution(scanTime);
  System.out.println(String.format(""String_Node_Str"",scanTime,nextExecutionTime));
  assertNotNull(null);
}","/** 
 * Issue #59: Incorrect next execution time for ""month"" and ""day of week"" Considers Month in range 0-11 instead of 1-12
 */
@Test public void testCorrectMonthScaleForNextExecution1(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  String crontab=""String_Node_Str"";
  Cron cron=parser.parse(crontab);
  ExecutionTime executionTime=ExecutionTime.forCron(cron);
  DateTime scanTime=DateTime.parse(""String_Node_Str"");
  DateTime nextExecutionTime=executionTime.nextExecution(scanTime);
  assertEquals(DateTime.parse(""String_Node_Str""),nextExecutionTime);
}"
4292,"/** 
 * Issue #59: Incorrect next execution time for ""day of month"" in ""time"" situation dom ""* / 4"" should means 1, 5, 9, 13, 17th... of month instead of 4, 8, 12, 16th...
 */
@Test public void testCorrectMonthScaleForNextExecution2(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  String crontab=""String_Node_Str"";
  Cron cron=parser.parse(crontab);
  ExecutionTime executionTime=ExecutionTime.forCron(cron);
  DateTime scanTime=DateTime.parse(""String_Node_Str"");
  DateTime nextExecutionTime=executionTime.nextExecution(scanTime);
  assertEquals(DateTime.parse(""String_Node_Str""),nextExecutionTime);
}","/** 
 * Issue #59: Incorrect next execution time for ""day of month"" in ""time"" situation dom ""* / 4"" should mean 1, 5, 9, 13, 17th... of month instead of 4, 8, 12, 16th...
 */
@Test public void testCorrectMonthScaleForNextExecution2(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  CronParser parser=new CronParser(cronDefinition);
  String crontab=""String_Node_Str"";
  Cron cron=parser.parse(crontab);
  ExecutionTime executionTime=ExecutionTime.forCron(cron);
  DateTime scanTime=DateTime.parse(""String_Node_Str"");
  DateTime nextExecutionTime=executionTime.nextExecution(scanTime);
  assertEquals(DateTime.parse(""String_Node_Str""),nextExecutionTime);
}"
4293,"private DateTime initDateTime(int years,int monthsOfYear,int dayOfMonth,int hoursOfDay,int minutesOfHour,int secondsOfMinute){
  return new DateTime(0,1,1,0,0,0).plusYears(years).plusMonths(monthsOfYear - 1).plusDays(dayOfMonth - 1).plusHours(hoursOfDay).plusMinutes(minutesOfHour).plusSeconds(secondsOfMinute);
}","private DateTime initDateTime(int years,int monthsOfYear,int dayOfMonth,int hoursOfDay,int minutesOfHour,int secondsOfMinute,DateTimeZone timeZone){
  return new DateTime(0,1,1,0,0,0,timeZone).plusYears(years).plusMonths(monthsOfYear - 1).plusDays(dayOfMonth - 1).plusHours(hoursOfDay).plusMinutes(minutesOfHour).plusSeconds(secondsOfMinute);
}"
4294,"/** 
 * If date is not match, will return previous closest match. If date is match, will return this date.
 * @param date - reference DateTime instance - never null;
 * @return DateTime instance, never null. Value obeys logic specified above.
 * @throws NoSuchValueException
 */
DateTime previousClosestMatch(DateTime date) throws NoSuchValueException {
  List<Integer> year=yearsValueGenerator.generateCandidates(date.getYear(),date.getYear());
  TimeNode days=generateDays(cronDefinition,date);
  int highestMonth=months.getValues().get(months.getValues().size() - 1);
  int highestDay=days.getValues().get(days.getValues().size() - 1);
  int highestHour=hours.getValues().get(hours.getValues().size() - 1);
  int highestMinute=minutes.getValues().get(minutes.getValues().size() - 1);
  int highestSecond=seconds.getValues().get(seconds.getValues().size() - 1);
  NearestValue nearestValue;
  DateTime newDate;
  if (year.isEmpty()) {
    return initDateTime(yearsValueGenerator.generatePreviousValue(date.getYear()),highestMonth,highestDay,highestHour,highestMinute,highestSecond);
  }
  if (!months.getValues().contains(date.getMonthOfYear())) {
    nearestValue=months.getPreviousValue(date.getMonthOfYear(),0);
    int previousMonths=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),12,31,23,59,59).minusYears(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),previousMonths,highestDay,highestHour,highestMinute,highestSecond);
  }
  if (!days.getValues().contains(date.getDayOfMonth())) {
    nearestValue=days.getPreviousValue(date.getDayOfMonth(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),1,23,59,59).minusMonths(nearestValue.getShifts()).dayOfMonth().withMaximumValue();
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthOfYear(),nearestValue.getValue(),highestHour,highestMinute,highestSecond);
  }
  if (!hours.getValues().contains(date.getHourOfDay())) {
    nearestValue=hours.getPreviousValue(date.getHourOfDay(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),23,59,59).minusDays(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),nearestValue.getValue(),highestMinute,highestSecond);
  }
  if (!minutes.getValues().contains(date.getMinuteOfHour())) {
    nearestValue=minutes.getPreviousValue(date.getMinuteOfHour(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),59,59).minusHours(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),nearestValue.getValue(),highestSecond);
  }
  if (!seconds.getValues().contains(date.getSecondOfMinute())) {
    nearestValue=seconds.getPreviousValue(date.getSecondOfMinute(),0);
    int previousSeconds=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),date.getMinuteOfHour(),59).minusMinutes(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),date.getMinuteOfHour(),previousSeconds);
  }
  return date;
}","/** 
 * If date is not match, will return previous closest match. If date is match, will return this date.
 * @param date - reference DateTime instance - never null;
 * @return DateTime instance, never null. Value obeys logic specified above.
 * @throws NoSuchValueException
 */
DateTime previousClosestMatch(DateTime date) throws NoSuchValueException {
  List<Integer> year=yearsValueGenerator.generateCandidates(date.getYear(),date.getYear());
  TimeNode days=generateDays(cronDefinition,date);
  int highestMonth=months.getValues().get(months.getValues().size() - 1);
  int highestDay=days.getValues().get(days.getValues().size() - 1);
  int highestHour=hours.getValues().get(hours.getValues().size() - 1);
  int highestMinute=minutes.getValues().get(minutes.getValues().size() - 1);
  int highestSecond=seconds.getValues().get(seconds.getValues().size() - 1);
  NearestValue nearestValue;
  DateTime newDate;
  if (year.isEmpty()) {
    return initDateTime(yearsValueGenerator.generatePreviousValue(date.getYear()),highestMonth,highestDay,highestHour,highestMinute,highestSecond,date.getZone());
  }
  if (!months.getValues().contains(date.getMonthOfYear())) {
    nearestValue=months.getPreviousValue(date.getMonthOfYear(),0);
    int previousMonths=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),12,31,23,59,59).minusYears(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),previousMonths,highestDay,highestHour,highestMinute,highestSecond,date.getZone());
  }
  if (!days.getValues().contains(date.getDayOfMonth())) {
    nearestValue=days.getPreviousValue(date.getDayOfMonth(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),1,23,59,59).minusMonths(nearestValue.getShifts()).dayOfMonth().withMaximumValue();
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthOfYear(),nearestValue.getValue(),highestHour,highestMinute,highestSecond,date.getZone());
  }
  if (!hours.getValues().contains(date.getHourOfDay())) {
    nearestValue=hours.getPreviousValue(date.getHourOfDay(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),23,59,59).minusDays(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),nearestValue.getValue(),highestMinute,highestSecond,date.getZone());
  }
  if (!minutes.getValues().contains(date.getMinuteOfHour())) {
    nearestValue=minutes.getPreviousValue(date.getMinuteOfHour(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),59,59).minusHours(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),nearestValue.getValue(),highestSecond,date.getZone());
  }
  if (!seconds.getValues().contains(date.getSecondOfMinute())) {
    nearestValue=seconds.getPreviousValue(date.getSecondOfMinute(),0);
    int previousSeconds=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),date.getMinuteOfHour(),59).minusMinutes(nearestValue.getShifts());
      return previousClosestMatch(newDate);
    }
    return initDateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),date.getMinuteOfHour(),previousSeconds,date.getZone());
  }
  return date;
}"
4295,"/** 
 * If date is not match, will return next closest match. If date is match, will return this date.
 * @param date - reference DateTime instance - never null;
 * @return DateTime instance, never null. Value obeys logic specified above.
 * @throws NoSuchValueException
 */
DateTime nextClosestMatch(DateTime date) throws NoSuchValueException {
  List<Integer> year=yearsValueGenerator.generateCandidates(date.getYear(),date.getYear());
  TimeNode days=generateDays(cronDefinition,date);
  int lowestMonth=months.getValues().get(0);
  int lowestDay=days.getValues().get(0);
  int lowestHour=hours.getValues().get(0);
  int lowestMinute=minutes.getValues().get(0);
  int lowestSecond=seconds.getValues().get(0);
  log.debug(""String_Node_Str"");
  log.debug(""String_Node_Str"",date.toString());
  for (  Integer i : days.values) {
    log.debug(""String_Node_Str"",i);
  }
  for (  Integer i : hours.values) {
    log.debug(""String_Node_Str"",i);
  }
  NearestValue nearestValue;
  DateTime newDate;
  if (year.isEmpty()) {
    return initDateTime(yearsValueGenerator.generateNextValue(date.getYear()),lowestMonth,lowestDay,lowestHour,lowestMinute,lowestSecond);
  }
  if (!months.getValues().contains(date.getMonthOfYear())) {
    log.debug(""String_Node_Str"");
    nearestValue=months.getNextValue(date.getMonthOfYear(),0);
    int nextMonths=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),1,1,0,0,0).plusYears(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getMonthOfYear()) {
      log.debug(""String_Node_Str"" + nearestValue.getValue() + ""String_Node_Str"");
      date=date.plusYears(1);
    }
    log.debug(""String_Node_Str"");
    return initDateTime(date.getYear(),nextMonths,lowestDay,lowestHour,lowestMinute,lowestSecond);
  }
  if (!days.getValues().contains(date.getDayOfMonth())) {
    log.debug(""String_Node_Str"");
    nearestValue=days.getNextValue(date.getDayOfMonth(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),1,0,0,0).plusMonths(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getDayOfMonth()) {
      log.debug(""String_Node_Str"" + nearestValue.getValue() + ""String_Node_Str"");
      date=date.plusMonths(1);
    }
    log.debug(""String_Node_Str"");
    return initDateTime(date.getYear(),date.getMonthOfYear(),nearestValue.getValue(),lowestHour,lowestMinute,lowestSecond);
  }
  if (!hours.getValues().contains(date.getHourOfDay())) {
    log.debug(""String_Node_Str"");
    nearestValue=hours.getNextValue(date.getHourOfDay(),0);
    log.debug(""String_Node_Str"",nearestValue.getShifts());
    int nextHours=nearestValue.getValue();
    log.debug(""String_Node_Str"",nextHours);
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),0,0,0).plusDays(nearestValue.getShifts());
      log.debug(""String_Node_Str"",newDate.toString());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getHourOfDay()) {
      log.debug(""String_Node_Str"" + nearestValue.getValue() + ""String_Node_Str"");
      date=date.plusDays(1);
    }
    log.debug(""String_Node_Str"");
    return initDateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),nextHours,lowestMinute,lowestSecond);
  }
  if (!minutes.getValues().contains(date.getMinuteOfHour())) {
    log.debug(""String_Node_Str"");
    nearestValue=minutes.getNextValue(date.getMinuteOfHour(),0);
    log.debug(""String_Node_Str"",nearestValue.getShifts());
    int nextMinutes=nearestValue.getValue();
    log.debug(""String_Node_Str"",nextMinutes);
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),0,0).plusHours(nearestValue.getShifts());
      log.debug(""String_Node_Str"",newDate.toString());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getMinuteOfHour()) {
      log.debug(""String_Node_Str"" + nearestValue.getValue() + ""String_Node_Str"");
      date=date.plusHours(1);
    }
    log.debug(""String_Node_Str"");
    return initDateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),nextMinutes,lowestSecond);
  }
  if (!seconds.getValues().contains(date.getSecondOfMinute())) {
    log.debug(""String_Node_Str"");
    nearestValue=seconds.getNextValue(date.getSecondOfMinute(),0);
    log.debug(""String_Node_Str"",nearestValue.getShifts());
    int nextSeconds=nearestValue.getValue();
    log.debug(""String_Node_Str"",nextSeconds);
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),date.getMinuteOfHour(),0).plusMinutes(nearestValue.getShifts());
      log.debug(""String_Node_Str"",newDate.toString());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getSecondOfMinute()) {
      log.debug(""String_Node_Str"" + nearestValue.getValue() + ""String_Node_Str"");
      date=date.plusMinutes(1);
    }
    log.debug(""String_Node_Str"");
    return initDateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),date.getMinuteOfHour(),nextSeconds);
  }
  return date;
}","/** 
 * If date is not match, will return next closest match. If date is match, will return this date.
 * @param date - reference DateTime instance - never null;
 * @return DateTime instance, never null. Value obeys logic specified above.
 * @throws NoSuchValueException
 */
DateTime nextClosestMatch(DateTime date) throws NoSuchValueException {
  List<Integer> year=yearsValueGenerator.generateCandidates(date.getYear(),date.getYear());
  TimeNode days=generateDays(cronDefinition,date);
  int lowestMonth=months.getValues().get(0);
  int lowestDay=days.getValues().get(0);
  int lowestHour=hours.getValues().get(0);
  int lowestMinute=minutes.getValues().get(0);
  int lowestSecond=seconds.getValues().get(0);
  log.debug(""String_Node_Str"");
  log.debug(""String_Node_Str"",date.toString());
  for (  Integer i : days.values) {
    log.debug(""String_Node_Str"",i);
  }
  for (  Integer i : hours.values) {
    log.debug(""String_Node_Str"",i);
  }
  NearestValue nearestValue;
  DateTime newDate;
  if (year.isEmpty()) {
    return initDateTime(yearsValueGenerator.generateNextValue(date.getYear()),lowestMonth,lowestDay,lowestHour,lowestMinute,lowestSecond,date.getZone());
  }
  if (!months.getValues().contains(date.getMonthOfYear())) {
    log.debug(""String_Node_Str"");
    nearestValue=months.getNextValue(date.getMonthOfYear(),0);
    int nextMonths=nearestValue.getValue();
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),1,1,0,0,0,date.getZone()).plusYears(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getMonthOfYear()) {
      log.debug(""String_Node_Str"" + nearestValue.getValue() + ""String_Node_Str"");
      date=date.plusYears(1);
    }
    log.debug(""String_Node_Str"");
    return initDateTime(date.getYear(),nextMonths,lowestDay,lowestHour,lowestMinute,lowestSecond,date.getZone());
  }
  if (!days.getValues().contains(date.getDayOfMonth())) {
    log.debug(""String_Node_Str"");
    nearestValue=days.getNextValue(date.getDayOfMonth(),0);
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),1,0,0,0).plusMonths(nearestValue.getShifts());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getDayOfMonth()) {
      log.debug(""String_Node_Str"" + nearestValue.getValue() + ""String_Node_Str"");
      date=date.plusMonths(1);
    }
    log.debug(""String_Node_Str"");
    return initDateTime(date.getYear(),date.getMonthOfYear(),nearestValue.getValue(),lowestHour,lowestMinute,lowestSecond,date.getZone());
  }
  if (!hours.getValues().contains(date.getHourOfDay())) {
    log.debug(""String_Node_Str"");
    nearestValue=hours.getNextValue(date.getHourOfDay(),0);
    log.debug(""String_Node_Str"",nearestValue.getShifts());
    int nextHours=nearestValue.getValue();
    log.debug(""String_Node_Str"",nextHours);
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),0,0,0).plusDays(nearestValue.getShifts());
      log.debug(""String_Node_Str"",newDate.toString());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getHourOfDay()) {
      log.debug(""String_Node_Str"" + nearestValue.getValue() + ""String_Node_Str"");
      date=date.plusDays(1);
    }
    log.debug(""String_Node_Str"");
    return initDateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),nextHours,lowestMinute,lowestSecond,date.getZone());
  }
  if (!minutes.getValues().contains(date.getMinuteOfHour())) {
    log.debug(""String_Node_Str"");
    nearestValue=minutes.getNextValue(date.getMinuteOfHour(),0);
    log.debug(""String_Node_Str"",nearestValue.getShifts());
    int nextMinutes=nearestValue.getValue();
    log.debug(""String_Node_Str"",nextMinutes);
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),0,0).plusHours(nearestValue.getShifts());
      log.debug(""String_Node_Str"",newDate.toString());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getMinuteOfHour()) {
      log.debug(""String_Node_Str"" + nearestValue.getValue() + ""String_Node_Str"");
      date=date.plusHours(1);
    }
    log.debug(""String_Node_Str"");
    return initDateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),nextMinutes,lowestSecond,date.getZone());
  }
  if (!seconds.getValues().contains(date.getSecondOfMinute())) {
    log.debug(""String_Node_Str"");
    nearestValue=seconds.getNextValue(date.getSecondOfMinute(),0);
    log.debug(""String_Node_Str"",nearestValue.getShifts());
    int nextSeconds=nearestValue.getValue();
    log.debug(""String_Node_Str"",nextSeconds);
    if (nearestValue.getShifts() > 0) {
      newDate=new DateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),date.getMinuteOfHour(),0).plusMinutes(nearestValue.getShifts());
      log.debug(""String_Node_Str"",newDate.toString());
      return nextClosestMatch(newDate);
    }
    if (nearestValue.getValue() < date.getSecondOfMinute()) {
      log.debug(""String_Node_Str"" + nearestValue.getValue() + ""String_Node_Str"");
      date=date.plusMinutes(1);
    }
    log.debug(""String_Node_Str"");
    return initDateTime(date.getYear(),date.getMonthOfYear(),date.getDayOfMonth(),date.getHourOfDay(),date.getMinuteOfHour(),nextSeconds,date.getZone());
  }
  return date;
}"
4296,"/** 
 * Issue #38: every 2 min schedule doesn't roll over to next hour
 */
public void testEveryTwoMinRollsOverHour(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  Cron cron=new CronParser(cronDefinition).parse(""String_Node_Str"");
  ExecutionTime executionTime=ExecutionTime.forCron(cron);
  DateTime time=DateTime.parse(""String_Node_Str"");
  time=time.toDateTime(DateTime.now().getZone());
  DateTime next=executionTime.nextExecution(time);
  DateTime shouldBeInNextHour=executionTime.nextExecution(next);
  assertEquals(next.plusMinutes(2),shouldBeInNextHour);
}","/** 
 * Issue #38: every 2 min schedule doesn't roll over to next hour
 */
@Test public void testEveryTwoMinRollsOverHour(){
  CronDefinition cronDefinition=CronDefinitionBuilder.instanceDefinitionFor(CronType.UNIX);
  Cron cron=new CronParser(cronDefinition).parse(""String_Node_Str"");
  ExecutionTime executionTime=ExecutionTime.forCron(cron);
  DateTime time=DateTime.parse(""String_Node_Str"");
  time=time.toDateTime(DateTime.now().getZone());
  DateTime next=executionTime.nextExecution(time);
  DateTime shouldBeInNextHour=executionTime.nextExecution(next);
  assertEquals(next.plusMinutes(2),shouldBeInNextHour);
}"
4297,"/** 
 * We return same reference value if matches or previous one if does not match. Then we start applying shifts. This way we ensure same value is returned if no shift is requested.
 * @param reference - reference value
 * @param shiftsToApply - shifts to apply
 * @return NearestValue instance, never null. Holds information on nearest (backward) value and shifts performed.
 */
@VisibleForTesting NearestValue getNearestBackwardValue(int reference,int shiftsToApply){
  List<Integer> values=new ArrayList<Integer>(this.values);
  Collections.reverse(values);
  int index=0;
  boolean foundSmaller=false;
  if (!values.contains(reference)) {
    for (    Integer value : values) {
      if (value < reference) {
        index=values.indexOf(value);
        shiftsToApply--;
        foundSmaller=true;
        break;
      }
    }
    if (!foundSmaller) {
      shiftsToApply++;
    }
  }
 else {
    index=values.indexOf(reference);
  }
  AtomicInteger shift=new AtomicInteger(0);
  int value=values.get(index);
  for (int j=0; j < shiftsToApply; j++) {
    value=getValueFromList(values,index + 1,shift);
    index=values.indexOf(value);
  }
  return new NearestValue(value,shift.get());
}","/** 
 * We return same reference value if matches or previous one if does not match. Then we start applying shifts. This way we ensure same value is returned if no shift is requested.
 * @param reference - reference value
 * @param shiftsToApply - shifts to apply
 * @return NearestValue instance, never null. Holds information on nearest (backward) value and shifts performed.
 */
@VisibleForTesting NearestValue getNearestBackwardValue(int reference,int shiftsToApply){
  List<Integer> values=new ArrayList<Integer>(this.values);
  Collections.reverse(values);
  int index=0;
  boolean foundSmaller=false;
  AtomicInteger shift=new AtomicInteger(0);
  if (!values.contains(reference)) {
    for (    Integer value : values) {
      if (value < reference) {
        index=values.indexOf(value);
        shiftsToApply--;
        foundSmaller=true;
        break;
      }
    }
    if (!foundSmaller) {
      shift.incrementAndGet();
    }
  }
 else {
    index=values.indexOf(reference);
  }
  int value=values.get(index);
  for (int j=0; j < shiftsToApply; j++) {
    value=getValueFromList(values,index + 1,shift);
    index=values.indexOf(value);
  }
  return new NearestValue(value,shift.get());
}"
4298,"@Test public void testGetPreviousValue() throws Exception {
  assertResult(LIST_START_VALUE,0,timeNode.getPreviousValue(LIST_START_VALUE,0));
  assertResult(LIST_MEDIUM_VALUE,0,timeNode.getPreviousValue(LIST_MEDIUM_VALUE,0));
  assertResult(LIST_END_VALUE,0,timeNode.getPreviousValue(LIST_END_VALUE,0));
  assertResult(LIST_END_VALUE,1,timeNode.getPreviousValue(LIST_START_VALUE,1));
  assertResult(LIST_START_VALUE,0,timeNode.getPreviousValue(LIST_MEDIUM_VALUE,1));
  assertResult(LIST_MEDIUM_VALUE,0,timeNode.getPreviousValue(LIST_END_VALUE,1));
  assertResult(LIST_END_VALUE,1,timeNode.getPreviousValue(LIST_MEDIUM_VALUE,2));
}","@Test public void testGetPreviousValue() throws Exception {
  assertResult(LIST_START_VALUE,0,timeNode.getPreviousValue(LIST_START_VALUE,0));
  assertResult(LIST_MEDIUM_VALUE,0,timeNode.getPreviousValue(LIST_MEDIUM_VALUE,0));
  assertResult(LIST_END_VALUE,0,timeNode.getPreviousValue(LIST_END_VALUE,0));
  assertResult(LIST_END_VALUE,1,timeNode.getPreviousValue(LIST_START_VALUE,1));
  assertResult(LIST_START_VALUE,0,timeNode.getPreviousValue(LIST_MEDIUM_VALUE,1));
  assertResult(LIST_MEDIUM_VALUE,0,timeNode.getPreviousValue(LIST_END_VALUE,1));
  assertResult(LIST_END_VALUE,1,timeNode.getPreviousValue(LIST_MEDIUM_VALUE,2));
  assertResult(LIST_MEDIUM_VALUE,0,timeNode.getPreviousValue(HIGH_INTERMEDIATE_VALUE,1));
  assertResult(LIST_END_VALUE,1,timeNode.getPreviousValue(LOW_INTERMEDIATE_VALUE,0));
}"
4299,"private List<Integer> generateDayCandidatesQuestionMarkSupported(int year,int month,WeekDay mondayDoWValue){
  DateTime date=new DateTime(year,month,1,1,1);
  Set<Integer> candidates=Sets.newHashSet();
  if (daysOfMonthCronField.getExpression() instanceof Always || daysOfWeekCronField.getExpression() instanceof Always) {
    candidates.addAll(FieldValueGeneratorFactory.createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,date.dayOfMonth().getMaximumValue()));
  }
 else {
    if (daysOfMonthCronField.getExpression() instanceof QuestionMark) {
      candidates.addAll(FieldValueGeneratorFactory.createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(-1,date.dayOfMonth().getMaximumValue()));
    }
 else {
      if (daysOfWeekCronField.getExpression() instanceof QuestionMark) {
        candidates.addAll(FieldValueGeneratorFactory.createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,date.dayOfMonth().getMaximumValue()));
      }
 else {
        candidates.addAll(FieldValueGeneratorFactory.createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(1,date.dayOfMonth().getMaximumValue()));
        candidates.addAll(FieldValueGeneratorFactory.createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,date.dayOfMonth().getMaximumValue()));
      }
    }
  }
  List<Integer> candidatesList=Lists.newArrayList(candidates);
  Collections.sort(candidatesList);
  return candidatesList;
}","private List<Integer> generateDayCandidatesQuestionMarkSupported(int year,int month,WeekDay mondayDoWValue){
  DateTime date=new DateTime(year,month,1,1,1);
  Set<Integer> candidates=Sets.newHashSet();
  if (daysOfMonthCronField.getExpression() instanceof Always && daysOfWeekCronField.getExpression() instanceof Always) {
    candidates.addAll(FieldValueGeneratorFactory.createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,date.dayOfMonth().getMaximumValue()));
  }
 else {
    if (daysOfMonthCronField.getExpression() instanceof QuestionMark) {
      candidates.addAll(FieldValueGeneratorFactory.createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(-1,date.dayOfMonth().getMaximumValue()));
    }
 else {
      if (daysOfWeekCronField.getExpression() instanceof QuestionMark) {
        candidates.addAll(FieldValueGeneratorFactory.createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,date.dayOfMonth().getMaximumValue()));
      }
 else {
        candidates.addAll(FieldValueGeneratorFactory.createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(1,date.dayOfMonth().getMaximumValue()));
        candidates.addAll(FieldValueGeneratorFactory.createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,date.dayOfMonth().getMaximumValue()));
      }
    }
  }
  List<Integer> candidatesList=Lists.newArrayList(candidates);
  Collections.sort(candidatesList);
  return candidatesList;
}"
4300,"public static FieldValueGenerator createDayOfWeekValueGeneratorInstance(CronField cronField,int year,int month,WeekDay mondayDoWValue){
  FieldExpression fieldExpression=cronField.getExpression();
  if (fieldExpression instanceof On) {
    return new OnDayOfWeekValueGenerator(cronField,year,month,mondayDoWValue);
  }
  if (fieldExpression instanceof Between) {
    return new BetweenDayOfWeekValueGenerator(cronField,year,month,mondayDoWValue);
  }
  return forCronField(cronField);
}","public static FieldValueGenerator createDayOfWeekValueGeneratorInstance(CronField cronField,int year,int month,WeekDay mondayDoWValue){
  FieldExpression fieldExpression=cronField.getExpression();
  if (fieldExpression instanceof On) {
    return new OnDayOfWeekValueGenerator(cronField,year,month,mondayDoWValue);
  }
  if (fieldExpression instanceof Between) {
    return new BetweenDayOfWeekValueGenerator(cronField,year,month,mondayDoWValue);
  }
  if (fieldExpression instanceof And) {
    return new AndDayOfWeekValueGenerator(cronField,year,month,mondayDoWValue);
  }
  return forCronField(cronField);
}"
4301,"/** 
 * Parse given expression for a single cron field
 * @param expression - String
 * @return CronFieldExpression object that with interpretation of given String parameter
 */
public FieldExpression parse(String expression){
  if (!StringUtils.containsAny(expression,specialCharsMinusStar)) {
    if (""String_Node_Str"".equals(expression)) {
      return new Always(constraints);
    }
 else {
      constraints.validateAllCharsValid(expression);
      return parseOn(expression);
    }
  }
 else {
    String[] array=expression.split(""String_Node_Str"");
    if (array.length > 1) {
      And and=new And();
      for (      String exp : array) {
        and.and(parse(exp));
      }
      return and;
    }
 else {
      array=expression.split(""String_Node_Str"");
      if (array.length > 1) {
        return parseBetween(array);
      }
 else {
        String value=expression.split(""String_Node_Str"")[1];
        constraints.validateAllCharsValid(value);
        return new Every(constraints,new IntegerFieldValue(Integer.parseInt(value)));
      }
    }
  }
}","/** 
 * Parse given expression for a single cron field
 * @param expression - String
 * @return CronFieldExpression object that with interpretation of given String parameter
 */
public FieldExpression parse(String expression){
  if (!StringUtils.containsAny(expression,specialCharsMinusStar)) {
    if (""String_Node_Str"".equals(expression)) {
      return new Always(constraints);
    }
 else {
      return parseOn(expression);
    }
  }
 else {
    String[] array=expression.split(""String_Node_Str"");
    if (array.length > 1) {
      And and=new And();
      for (      String exp : array) {
        and.and(parse(exp));
      }
      return and;
    }
 else {
      array=expression.split(""String_Node_Str"");
      if (array.length > 1) {
        return parseBetween(array);
      }
 else {
        String value=expression.split(""String_Node_Str"")[1];
        constraints.validateAllCharsValid(value);
        return new Every(constraints,new IntegerFieldValue(Integer.parseInt(value)));
      }
    }
  }
}"
4302,"private Between parseBetween(String[] array){
  if (array[1].contains(""String_Node_Str"")) {
    String[] every=array[1].split(""String_Node_Str"");
    return new Between(constraints,map(constraints,array[0]),map(constraints,every[0]),mapToIntegerFieldValue(every[1]));
  }
 else {
    String from=array[0];
    String to=array[1];
    constraints.validateAllCharsValid(from);
    constraints.validateAllCharsValid(to);
    return new Between(constraints,map(constraints,from),map(constraints,to));
  }
}","private Between parseBetween(String[] array){
  if (array[1].contains(""String_Node_Str"")) {
    String[] every=array[1].split(""String_Node_Str"");
    return new Between(constraints,map(constraints,array[0]),map(constraints,every[0]),mapToIntegerFieldValue(every[1]));
  }
 else {
    String from=array[0];
    String to=array[1];
    return new Between(constraints,map(constraints,from),map(constraints,to));
  }
}"
4303,"private On parseOn(String exp){
  constraints.validateAllCharsValid(exp);
  SpecialCharFieldValue specialChar=new SpecialCharFieldValue(SpecialChar.NONE);
  IntegerFieldValue nth=new IntegerFieldValue(-1);
  IntegerFieldValue time=new IntegerFieldValue(-1);
  String expression=exp;
  if (exp.contains(""String_Node_Str"")) {
    specialChar=new SpecialCharFieldValue(SpecialChar.HASH);
    String[] array=exp.split(""String_Node_Str"");
    nth=mapToIntegerFieldValue(array[1]);
    if (array[0].isEmpty()) {
      throw new IllegalArgumentException(""String_Node_Str"");
    }
    expression=array[0];
  }
  if (exp.contains(""String_Node_Str"")) {
    specialChar=new SpecialCharFieldValue(SpecialChar.LW);
    exp=exp.replace(""String_Node_Str"",""String_Node_Str"");
    if (""String_Node_Str"".equals(exp)) {
      expression=null;
    }
 else {
      expression=exp;
    }
  }
  if (exp.contains(""String_Node_Str"")) {
    specialChar=new SpecialCharFieldValue(SpecialChar.L);
    exp=exp.replace(""String_Node_Str"",""String_Node_Str"");
    if (""String_Node_Str"".equals(exp)) {
      expression=null;
    }
 else {
      expression=exp;
    }
  }
  if (exp.contains(""String_Node_Str"")) {
    specialChar=new SpecialCharFieldValue(SpecialChar.W);
    expression=exp.replace(""String_Node_Str"",""String_Node_Str"");
  }
  constraints.validateSpecialCharAllowed(specialChar.getValue());
  if (expression != null) {
    return new On(constraints,mapToIntegerFieldValue(expression),specialChar,nth);
  }
 else {
    return new On(constraints,time,specialChar,nth);
  }
}","private On parseOn(String exp){
  constraints.validateAllCharsValid(exp);
  if (exp.contains(""String_Node_Str"")) {
    return parseOnWithHash(exp);
  }
  if (exp.contains(""String_Node_Str"")) {
    return parseOnWithLW(exp);
  }
  if (lPattern.matcher(exp).find() || exp.equalsIgnoreCase(""String_Node_Str"")) {
    return parseOnWithL(exp);
  }
  if (wPattern.matcher(exp).find()) {
    return parseOnWithW(exp);
  }
  return new On(constraints,mapToIntegerFieldValue(exp),new SpecialCharFieldValue(SpecialChar.NONE),new IntegerFieldValue(-1));
}"
4304,"/** 
 * Issue #27: single day of week string mapping is valid
 */
public void testDayOfWeekMappingIsValid(){
  for (  String dow : new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""}) {
    String expression=String.format(""String_Node_Str"",dow);
    assertTrue(String.format(""String_Node_Str"",expression),validator.isValid(expression));
  }
}","/** 
 * Issue #27: single day of week string mapping is valid
 */
@Test public void testDayOfWeekMappingIsValid(){
  for (  String dow : new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""}) {
    String expression=String.format(""String_Node_Str"",dow);
    assertTrue(String.format(""String_Node_Str"",expression),validator.isValid(expression));
  }
}"
4305,"/** 
 * Issue #27: single month string mapping is valid
 */
public void testSingleMonthMappingIsValid(){
  DateTime date=new DateTime(2015,1,1,1,1);
  for (int j=0; j < 12; j++) {
    String expression=String.format(""String_Node_Str"",date.plusMonths(j).toString(""String_Node_Str"",Locale.US).toUpperCase());
    assertTrue(String.format(""String_Node_Str"",expression),validator.isValid(expression));
  }
}","/** 
 * Issue #27: single month string mapping is valid
 */
@Test public void testSingleMonthMappingIsValid(){
  DateTime date=new DateTime(2015,1,1,1,1);
  for (int j=0; j < 12; j++) {
    String expression=String.format(""String_Node_Str"",date.plusMonths(j).toString(""String_Node_Str"",Locale.US).toUpperCase());
    assertTrue(String.format(""String_Node_Str"",expression),validator.isValid(expression));
  }
}"
4306,"/** 
 * Provide description for day of month
 * @param fields - fields to describe;
 * @return description - String
 */
private String describeDayOfMonth(Map<CronFieldName,CronField> fields){
  return String.format(DescriptionStrategyFactory.daysOfMonthInstance(bundle,fields.containsKey(CronFieldName.DAY_OF_MONTH) ? fields.get(CronFieldName.DAY_OF_MONTH).getExpression() : null).describe(),bundle.getString(""String_Node_Str""));
}","/** 
 * Provide description for day of month
 * @param fields - fields to describe;
 * @return description - String
 */
private String describeDayOfMonth(Map<CronFieldName,CronField> fields){
  String description=DescriptionStrategyFactory.daysOfMonthInstance(bundle,fields.containsKey(CronFieldName.DAY_OF_MONTH) ? fields.get(CronFieldName.DAY_OF_MONTH).getExpression() : null).describe();
  return addTimeExpressions(description,bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
}"
4307,"/** 
 * Provide description for month
 * @param fields - fields to describe;
 * @return description - String
 */
private String describeMonth(Map<CronFieldName,CronField> fields){
  return String.format(DescriptionStrategyFactory.monthsInstance(bundle,fields.containsKey(CronFieldName.MONTH) ? fields.get(CronFieldName.MONTH).getExpression() : null).describe(),bundle.getString(""String_Node_Str""));
}","/** 
 * Provide description for month
 * @param fields - fields to describe;
 * @return description - String
 */
private String describeMonth(Map<CronFieldName,CronField> fields){
  String description=DescriptionStrategyFactory.monthsInstance(bundle,fields.containsKey(CronFieldName.MONTH) ? fields.get(CronFieldName.MONTH).getExpression() : null).describe();
  return addTimeExpressions(description,bundle.getString(""String_Node_Str""),bundle.getString(""String_Node_Str""));
}"
4308,"/** 
 * Creates CronDefinition instance matching quartz specification;
 * @return CronDefinition instance, never null;
 */
private static CronDefinition quartz(){
  return CronDefinitionBuilder.defineCron().withSeconds().and().withMinutes().and().withHours().and().withDayOfMonth().supportsHash().supportsL().supportsW().and().withMonth().and().withDayOfWeek().withValidRange(1,7).withMondayDoWValue(2).supportsHash().supportsL().supportsW().and().withYear().withValidRange(1970,2099).and().lastFieldOptional().instance();
}","/** 
 * Creates CronDefinition instance matching quartz specification;
 * @return CronDefinition instance, never null;
 */
private static CronDefinition quartz(){
  return CronDefinitionBuilder.defineCron().withSeconds().and().withMinutes().and().withHours().and().withDayOfMonth().supportsHash().supportsL().supportsW().supportsLW().supportsQuestionMark().and().withMonth().and().withDayOfWeek().withValidRange(1,7).withMondayDoWValue(2).supportsHash().supportsL().supportsW().supportsQuestionMark().and().withYear().withValidRange(1970,2099).and().lastFieldOptional().instance();
}"
4309,"/** 
 * Performs action on On instance
 * @param on - On instance, never null
 * @return On instance, never null
 */
On visit(On on);","/** 
 * Performs action on QuestionMark instance
 * @param questionMark - QuestionMark instance, never null
 * @return QuestionMark instance, never null
 */
QuestionMark visit(QuestionMark questionMark);"
4310,"@Override public FieldExpression visit(FieldExpression expression){
  if (expression instanceof Always) {
    return visit((Always)expression);
  }
  if (expression instanceof And) {
    return visit((And)expression);
  }
  if (expression instanceof Between) {
    return visit((Between)expression);
  }
  if (expression instanceof Every) {
    return visit((Every)expression);
  }
  if (expression instanceof On) {
    return visit((On)expression);
  }
  return expression;
}","@Override public FieldExpression visit(FieldExpression expression){
  if (expression instanceof Always) {
    return visit((Always)expression);
  }
  if (expression instanceof And) {
    return visit((And)expression);
  }
  if (expression instanceof Between) {
    return visit((Between)expression);
  }
  if (expression instanceof Every) {
    return visit((Every)expression);
  }
  if (expression instanceof On) {
    return visit((On)expression);
  }
  if (expression instanceof QuestionMark) {
    return visit((QuestionMark)expression);
  }
  return expression;
}"
4311,"public NearestValue getPreviousValue(int reference,int shifts){
  return getNearestValues(reference,shifts,new Function<Integer,Integer>(){
    @Override public Integer apply(    Integer integer){
      return integer - 1;
    }
  }
);
}","public NearestValue getPreviousValue(int reference,int shifts){
  return getNearestBackwardValue(reference,shifts);
}"
4312,"public NearestValue getNextValue(int reference,int shifts){
  return getNearestValues(reference,shifts,new Function<Integer,Integer>(){
    @Override public Integer apply(    Integer integer){
      return integer + 1;
    }
  }
);
}","public NearestValue getNextValue(int reference,int shifts){
  return getNearestForwardValue(reference,shifts);
}"
4313,"@Override public int generatePreviousValue(int reference) throws NoSuchValueException {
  return new EveryFieldValueGenerator(expression).generatePreviousValue(reference);
}","@Override public int generatePreviousValue(int reference) throws NoSuchValueException {
  Always always=(Always)expression;
  return new EveryFieldValueGenerator(always.getEvery()).generatePreviousValue(reference);
}"
4314,"@Override public int generateNextValue(int reference) throws NoSuchValueException {
  return new EveryFieldValueGenerator(expression).generateNextValue(reference);
}","@Override public int generateNextValue(int reference) throws NoSuchValueException {
  Always always=(Always)expression;
  return new EveryFieldValueGenerator(always.getEvery()).generateNextValue(reference);
}"
4315,"@Override public int generatePreviousValue(final int reference) throws NoSuchValueException {
  List<Integer> candidates=computeCandidates(new Function<FieldValueGenerator,Integer>(){
    @Override public Integer apply(    FieldValueGenerator candidateGenerator){
      try {
        return candidateGenerator.generatePreviousValue(reference);
      }
 catch (      NoSuchValueException e) {
        return NO_VALUE;
      }
    }
  }
);
  return candidates.get(candidates.size() - 1);
}","@Override public int generatePreviousValue(final int reference) throws NoSuchValueException {
  List<Integer> candidates=computeCandidates(new Function<FieldValueGenerator,Integer>(){
    @Override public Integer apply(    FieldValueGenerator candidateGenerator){
      try {
        return candidateGenerator.generatePreviousValue(reference);
      }
 catch (      NoSuchValueException e) {
        return NO_VALUE;
      }
    }
  }
);
  if (candidates.isEmpty()) {
    throw new NoSuchValueException();
  }
 else {
    return candidates.get(candidates.size() - 1);
  }
}"
4316,"@Override public int generateNextValue(final int reference) throws NoSuchValueException {
  return computeCandidates(new Function<FieldValueGenerator,Integer>(){
    @Override public Integer apply(    FieldValueGenerator fieldValueGenerator){
      try {
        return fieldValueGenerator.generateNextValue(reference);
      }
 catch (      NoSuchValueException e) {
        return NO_VALUE;
      }
    }
  }
).get(0);
}","@Override public int generateNextValue(final int reference) throws NoSuchValueException {
  List<Integer> candidates=computeCandidates(new Function<FieldValueGenerator,Integer>(){
    @Override public Integer apply(    FieldValueGenerator fieldValueGenerator){
      try {
        return fieldValueGenerator.generateNextValue(reference);
      }
 catch (      NoSuchValueException e) {
        return NO_VALUE;
      }
    }
  }
);
  if (candidates.isEmpty()) {
    throw new NoSuchValueException();
  }
 else {
    return candidates.get(0);
  }
}"
4317,"@Override public boolean apply(Integer integer){
  return integer > 0;
}","@Override public boolean apply(Integer integer){
  return integer >= 0;
}"
4318,"private List<Integer> computeCandidates(Function<FieldValueGenerator,Integer> function){
  And and=(And)expression;
  List<Integer> candidates=Lists.newArrayList();
  for (  FieldExpression expression : and.getExpressions()) {
    candidates.add(function.apply(createCandidateGeneratorInstance(expression)));
  }
  candidates=new ArrayList<Integer>(Collections2.filter(candidates,new Predicate<Integer>(){
    @Override public boolean apply(    Integer integer){
      return integer > 0;
    }
  }
));
  Collections.sort(candidates);
  return candidates;
}","private List<Integer> computeCandidates(Function<FieldValueGenerator,Integer> function){
  And and=(And)expression;
  List<Integer> candidates=Lists.newArrayList();
  for (  FieldExpression expression : and.getExpressions()) {
    candidates.add(function.apply(createCandidateGeneratorInstance(expression)));
  }
  candidates=new ArrayList<Integer>(Collections2.filter(candidates,new Predicate<Integer>(){
    @Override public boolean apply(    Integer integer){
      return integer >= 0;
    }
  }
));
  Collections.sort(candidates);
  return candidates;
}"
4319,"@Override public int generatePreviousValue(int reference) throws NoSuchValueException {
  Between between=(Between)expression;
  int candidate=new EveryFieldValueGenerator(between.getEvery()).generatePreviousValue(reference);
  if (candidate < between.getFrom() || candidate > between.getTo()) {
    return between.getTo();
  }
  return candidate;
}","@Override public int generatePreviousValue(int reference) throws NoSuchValueException {
  Between between=(Between)expression;
  int candidate=new EveryFieldValueGenerator(between.getEvery()).generatePreviousValue(reference);
  if (candidate < between.getFrom()) {
    throw new NoSuchValueException();
  }
  return candidate;
}"
4320,"@Override public int generateNextValue(int reference) throws NoSuchValueException {
  Between between=(Between)expression;
  int candidate=new EveryFieldValueGenerator(between.getEvery()).generateNextValue(reference);
  if (candidate < between.getFrom() || candidate > between.getTo()) {
    return between.getFrom();
  }
  return candidate;
}","@Override public int generateNextValue(int reference) throws NoSuchValueException {
  Between between=(Between)expression;
  int candidate=new EveryFieldValueGenerator(between.getEvery()).generateNextValue(reference);
  if (candidate > between.getTo()) {
    throw new NoSuchValueException();
  }
  return candidate;
}"
4321,public abstract int generatePreviousValue(int reference) throws NoSuchValueException ;,"/** 
 * Generates previous valid value from reference
 * @param reference - reference value
 * @return generated value - Integer
 * @throws NoSuchValueException - if there is no previous value
 */
public abstract int generatePreviousValue(int reference) throws NoSuchValueException ;"
4322,public abstract int generateNextValue(int reference) throws NoSuchValueException ;,"/** 
 * Generates next valid value from reference
 * @param reference - reference value
 * @return generated value - Integer
 * @throws NoSuchValueException - if there is no next value
 */
public abstract int generateNextValue(int reference) throws NoSuchValueException ;"
4323,"public FieldValueGenerator(FieldExpression expression){
  this.expression=Validate.notNull(expression);
}","public FieldValueGenerator(FieldExpression expression){
  Validate.notNull(expression);
  Validate.isTrue(matchesFieldExpressionClass(expression),""String_Node_Str"");
  this.expression=expression;
}"
4324,"private int generateValue(On on,int year,int month) throws NoSuchValueException {
switch (on.getSpecialChar()) {
case L:
    return new DateTime(year,month,1,1,1).dayOfMonth().getMaximumValue();
case W:
  DateTime doM=new DateTime(year,month,on.getTime(),1,1);
if (doM.getDayOfWeek() == 6) {
  if (on.getTime() == 1) {
    return 3;
  }
  return on.getTime() - 1;
}
if (doM.getDayOfWeek() == 7) {
if ((on.getTime() + 1) <= doM.dayOfMonth().getMaximumValue()) {
  return on.getTime() + 1;
}
}
break;
}
throw new NoSuchElementException();
}","private int generateValue(On on,int year,int month) throws NoSuchValueException {
switch (on.getSpecialChar()) {
case L:
    return new DateTime(year,month,1,1,1).dayOfMonth().getMaximumValue();
case W:
  DateTime doM=new DateTime(year,month,on.getTime(),1,1);
if (doM.getDayOfWeek() == 6) {
  if (on.getTime() == 1) {
    return 3;
  }
  return on.getTime() - 1;
}
if (doM.getDayOfWeek() == 7) {
if ((on.getTime() + 1) <= doM.dayOfMonth().getMaximumValue()) {
  return on.getTime() + 1;
}
}
break;
case NONE:
return on.getTime();
}
throw new NoSuchElementException();
}"
4325,"public OnDayOfMonthValueGenerator(CronField cronField,int year,int month){
  super(cronField.getExpression());
  Validate.isTrue(CronFieldName.DAY_OF_MONTH.equals(cronField.getField()));
  this.year=year;
  this.month=month;
}","public OnDayOfMonthValueGenerator(CronField cronField,int year,int month){
  super(cronField.getExpression());
  Validate.isTrue(CronFieldName.DAY_OF_MONTH.equals(cronField.getField()),""String_Node_Str"");
  this.year=year;
  this.month=month;
}"
4326,"public OnDayOfWeekValueGenerator(CronField cronField,int year,int month){
  super(cronField.getExpression());
  Validate.isTrue(CronFieldName.DAY_OF_MONTH.equals(cronField.getField()));
  this.year=year;
  this.month=month;
}","public OnDayOfWeekValueGenerator(CronField cronField,int year,int month){
  super(cronField.getExpression());
  Validate.isTrue(CronFieldName.DAY_OF_WEEK.equals(cronField.getField()),""String_Node_Str"");
  this.year=year;
  this.month=month;
}"
4327,"private int generateValue(On on,int year,int month) throws NoSuchValueException {
switch (on.getSpecialChar()) {
case HASH:
    return generateHashValues(on,year,month);
case L:
  return generateLValues(on,year,month);
}
throw new NoSuchValueException();
}","private int generateValue(On on,int year,int month) throws NoSuchValueException {
switch (on.getSpecialChar()) {
case HASH:
    return generateHashValues(on,year,month);
case L:
  return generateLValues(on,year,month);
case NONE:
return on.getTime();
}
throw new NoSuchValueException();
}"
4328,"public static int weekDayMapping(WeekDay from,WeekDay to,int weekday){
  return from.map(to,weekday);
}","public static int weekDayMapping(WeekDay from,WeekDay to,int weekday){
  return to.map(from,weekday);
}"
4329,"/** 
 * Constructor
 * @param from - source CronDefinition;if null a NullPointerException will be raised
 * @param to - target CronDefinition;if null a NullPointerException will be raised
 */
public CronMapper(CronDefinition from,CronDefinition to){
  Validate.notNull(from,""String_Node_Str"");
  Validate.notNull(to,""String_Node_Str"");
  mappings=Maps.newHashMap();
  buildMappings(from,to);
}","/** 
 * Constructor
 * @param from - source CronDefinition;if null a NullPointerException will be raised
 * @param to - target CronDefinition;if null a NullPointerException will be raised
 */
public CronMapper(CronDefinition from,CronDefinition to){
  Validate.notNull(from,""String_Node_Str"");
  this.to=Validate.notNull(to,""String_Node_Str"");
  mappings=Maps.newHashMap();
  buildMappings(from,to);
}"
4330,"/** 
 * Maps given cron to target cron definition
 * @param cron - Instance to be mapped;if null a NullPointerException will be raised
 * @return new Cron instance, never null;
 */
public Cron map(Cron cron){
  Validate.notNull(cron,""String_Node_Str"");
  List<CronField> fields=Lists.newArrayList();
  for (  CronFieldName name : CronFieldName.values()) {
    if (mappings.containsKey(name)) {
      fields.add(mappings.get(name).apply(cron.retrieve(name)));
    }
  }
  return new Cron(fields);
}","/** 
 * Maps given cron to target cron definition
 * @param cron - Instance to be mapped;if null a NullPointerException will be raised
 * @return new Cron instance, never null;
 */
public Cron map(Cron cron){
  Validate.notNull(cron,""String_Node_Str"");
  List<CronField> fields=Lists.newArrayList();
  for (  CronFieldName name : CronFieldName.values()) {
    if (mappings.containsKey(name)) {
      fields.add(mappings.get(name).apply(cron.retrieve(name)));
    }
  }
  return new Cron(to,fields);
}"
4331,"public WeekDay(int monday,boolean firstDayIsZero){
  this.monday=monday;
  this.firstDayIsZero=firstDayIsZero;
}","public WeekDay(int mondayDoWValue,boolean firstDayIsZero){
  Validate.isTrue(mondayDoWValue >= 0,""String_Node_Str"");
  this.mondayDoWValue=mondayDoWValue;
  this.firstDayIsZero=firstDayIsZero;
}"
4332,"public int map(WeekDay weekDay,int day){
  int result=monday - weekDay.getMonday() + day;
  if (result == 0) {
    if (firstDayIsZero) {
      result=0;
    }
 else {
      result=7;
    }
  }
  return result;
}","/** 
 * Maps given WeekDay to representation hold by this instance.
 * @param weekDay - referred weekDay
 * @param dayOfWeek - day of week to be mapped
 * @return - int result
 */
public int map(WeekDay weekDay,int dayOfWeek){
  int result=mondayDoWValue - weekDay.getMondayDoWValue() + dayOfWeek;
  if (result == 0) {
    if (firstDayIsZero) {
      result=0;
    }
 else {
      result=7;
    }
  }
  return result;
}"
4333,"public Cron(List<CronField> fields){
  this.fields=Maps.newHashMap();
  Validate.notNull(fields,""String_Node_Str"");
  for (  CronField field : fields) {
    this.fields.put(field.getField(),field);
  }
}","public Cron(CronDefinition cronDefinition,List<CronField> fields){
  this.cronDefinition=Validate.notNull(cronDefinition,""String_Node_Str"");
  Validate.notNull(fields,""String_Node_Str"");
  this.fields=Maps.newHashMap();
  for (  CronField field : fields) {
    this.fields.put(field.getField(),field);
  }
}"
4334,"/** 
 * Validate if special char is allowed. If not, a RuntimeException will be raised.
 * @param specialChar - char to be validated
 */
public void validateSpecialCharAllowed(SpecialChar specialChar){
  if (!specialChars.contains(specialChar)) {
    throw new RuntimeException(String.format(""String_Node_Str"",specialChar));
  }
}","/** 
 * Validate if special char is allowed. If not, a RuntimeException will be raised.
 * @param specialChar - char to be validated
 */
public void validateSpecialCharAllowed(SpecialChar specialChar){
  if (!isSpecialCharAllowed(specialChar)) {
    throw new RuntimeException(String.format(""String_Node_Str"",specialChar));
  }
}"
4335,"/** 
 * Validate if given number is >= start range and <= end range
 * @param number - to be validated
 * @return - same number being validated if in range,throws RuntimeException if number out of range
 */
public int validateInRange(int number){
  if (number >= startRange && number <= endRange) {
    return number;
  }
  throw new RuntimeException(String.format(""String_Node_Str"",number,startRange,endRange));
}","/** 
 * Validate if given number is >= start range and <= end range
 * @param number - to be validated
 * @return - same number being validated if in range,throws RuntimeException if number out of range
 */
public int validateInRange(int number){
  if (isInRange(number)) {
    return number;
  }
  throw new RuntimeException(String.format(""String_Node_Str"",number,startRange,endRange));
}"
4336,"/** 
 * Constructor
 * @param fieldName   - CronFieldName; name of the fieldif null, a NullPointerException will be raised.
 * @param constraints - FieldConstraints, constraints;
 */
public DayOfWeekFieldDefinition(CronFieldName fieldName,FieldConstraints constraints,int mondayDoWValue){
  super(fieldName,constraints);
  constraints.validateInRange(mondayDoWValue);
  this.mondayDoWValue=mondayDoWValue;
}","/** 
 * Constructor
 * @param fieldName   - CronFieldName; name of the fieldif null, a NullPointerException will be raised.
 * @param constraints - FieldConstraints, constraints;
 */
public DayOfWeekFieldDefinition(CronFieldName fieldName,FieldConstraints constraints,WeekDay mondayDoWValue){
  super(fieldName,constraints);
  constraints.validateInRange(mondayDoWValue.getMondayDoWValue());
  this.mondayDoWValue=mondayDoWValue;
}"
4337,"public int getMondayDoWValue(){
  return mondayDoWValue;
}","public WeekDay getMondayDoWValue(){
  return mondayDoWValue;
}"
4338,"/** 
 * Registers CronField in ParserDefinitionBuilder and returns its instance
 * @return ParserDefinitionBuilder instance obtained from constructor
 */
public CronDefinitionBuilder and(){
  cronDefinitionBuilder.register(new DayOfWeekFieldDefinition(fieldName,constraints.createConstraintsInstance(),mondayDoWValue));
  return cronDefinitionBuilder;
}","/** 
 * Registers CronField in ParserDefinitionBuilder and returns its instance
 * @return ParserDefinitionBuilder instance obtained from constructor
 */
public CronDefinitionBuilder and(){
  boolean zeroInRange=constraints.createConstraintsInstance().isInRange(0);
  cronDefinitionBuilder.register(new DayOfWeekFieldDefinition(fieldName,constraints.createConstraintsInstance(),new WeekDay(mondayDoWValue,zeroInRange)));
  return cronDefinitionBuilder;
}"
4339,"@VisibleForTesting ExecutionTime(FieldValueGenerator yearsValueGenerator,CronField daysOfWeekCronField,CronField daysOfMonthCronField,TimeNode months,TimeNode hours,TimeNode minutes,TimeNode seconds){
  this.yearsValueGenerator=yearsValueGenerator;
  this.daysOfWeekCronField=daysOfWeekCronField;
  this.daysOfMonthCronField=daysOfMonthCronField;
  this.months=months;
  this.hours=hours;
  this.minutes=minutes;
  this.seconds=seconds;
}","@VisibleForTesting ExecutionTime(CronDefinition cronDefinition,FieldValueGenerator yearsValueGenerator,CronField daysOfWeekCronField,CronField daysOfMonthCronField,TimeNode months,TimeNode hours,TimeNode minutes,TimeNode seconds){
  this.yearsValueGenerator=yearsValueGenerator;
  this.daysOfWeekCronField=daysOfWeekCronField;
  this.daysOfMonthCronField=daysOfMonthCronField;
  this.months=months;
  this.hours=hours;
  this.minutes=minutes;
  this.seconds=seconds;
}"
4340,"/** 
 * Provide nearest date for last execution.
 * @param date - jodatime DateTime instance. If null, a NullPointerException will be raised.
 * @return DateTime instance, never null. Last execution time.
 */
public DateTime lastExecution(DateTime date){
  Validate.notNull(date);
  NearestValue secondsValue=seconds.getPreviousValue(date.getSecondOfMinute(),1);
  NearestValue minutesValue=minutes.getPreviousValue(date.getMinuteOfHour(),secondsValue.getShifts());
  NearestValue hoursValue=hours.getPreviousValue(date.getHourOfDay(),minutesValue.getShifts());
  NearestValue monthsValue;
  int month=1;
  int day=1;
  if (months.getValues().contains(date.getMonthOfYear())) {
    monthsValue=new NearestValue(date.getMonthOfYear(),0);
    day=date.getDayOfMonth();
  }
 else {
    monthsValue=months.getPreviousValue(date.getMonthOfYear(),0);
    month=monthsValue.getValue();
    day=new DateTime(date.getYear(),month,1,1,1).dayOfMonth().withMaximumValue().getDayOfMonth();
  }
  TimeNode days=new TimeNode(generateDayCandidates(date.getYear(),month));
  NearestValue daysValue=days.getPreviousValue(day,hoursValue.getShifts());
  monthsValue=months.getPreviousValue(month,daysValue.getShifts());
  if (daysValue.getShifts() > 0) {
    days=new TimeNode(generateDayCandidates(date.getYear(),monthsValue.getValue()));
    List<Integer> dayCandidates=days.getValues();
    daysValue=new NearestValue(dayCandidates.get(dayCandidates.size() - 1),0);
  }
  NearestValue yearsValue=new TimeNode(generateYearCandidates(date.getYear())).getPreviousValue(date.getYear(),monthsValue.getShifts());
  return new DateTime(yearsValue.getValue(),monthsValue.getValue(),daysValue.getValue(),hoursValue.getValue(),minutesValue.getValue(),secondsValue.getValue());
}","/** 
 * Provide nearest date for last execution.
 * @param date - jodatime DateTime instance. If null, a NullPointerException will be raised.
 * @return DateTime instance, never null. Last execution time.
 */
public DateTime lastExecution(DateTime date){
  Validate.notNull(date);
  NearestValue secondsValue=seconds.getPreviousValue(date.getSecondOfMinute(),1);
  NearestValue minutesValue=minutes.getPreviousValue(date.getMinuteOfHour(),secondsValue.getShifts());
  NearestValue hoursValue=hours.getPreviousValue(date.getHourOfDay(),minutesValue.getShifts());
  NearestValue monthsValue;
  int month=1;
  int day=1;
  if (months.getValues().contains(date.getMonthOfYear())) {
    monthsValue=new NearestValue(date.getMonthOfYear(),0);
    day=date.getDayOfMonth();
  }
 else {
    monthsValue=months.getPreviousValue(date.getMonthOfYear(),0);
    month=monthsValue.getValue();
    day=new DateTime(date.getYear(),month,1,1,1).dayOfMonth().withMaximumValue().getDayOfMonth();
  }
  TimeNode days=new TimeNode(generateDayCandidates(date.getYear(),month,((DayOfWeekFieldDefinition)cronDefinition.getFieldDefinition(CronFieldName.DAY_OF_WEEK)).getMondayDoWValue()));
  NearestValue daysValue=days.getPreviousValue(day,hoursValue.getShifts());
  monthsValue=months.getPreviousValue(month,daysValue.getShifts());
  if (daysValue.getShifts() > 0) {
    days=new TimeNode(generateDayCandidates(date.getYear(),monthsValue.getValue(),((DayOfWeekFieldDefinition)cronDefinition.getFieldDefinition(CronFieldName.DAY_OF_WEEK)).getMondayDoWValue()));
    List<Integer> dayCandidates=days.getValues();
    daysValue=new NearestValue(dayCandidates.get(dayCandidates.size() - 1),0);
  }
  NearestValue yearsValue=new TimeNode(generateYearCandidates(date.getYear())).getPreviousValue(date.getYear(),monthsValue.getShifts());
  return new DateTime(yearsValue.getValue(),monthsValue.getValue(),daysValue.getValue(),hoursValue.getValue(),minutesValue.getValue(),secondsValue.getValue());
}"
4341,"/** 
 * Creates execution time for given Cron
 * @param cron - Cron instance
 * @return ExecutionTime instance
 */
public static ExecutionTime forCron(Cron cron){
  Map<CronFieldName,CronField> fields=cron.retrieveFieldsAsMap();
  ExecutionTimeBuilder executionTimeBuilder=new ExecutionTimeBuilder();
  if (fields.containsKey(CronFieldName.SECOND)) {
    executionTimeBuilder.forSecondsMatching(fields.get(CronFieldName.SECOND));
  }
  executionTimeBuilder.forMinutesMatching(fields.get(CronFieldName.MINUTE)).forHoursMatching(fields.get(CronFieldName.HOUR)).forDaysOfMonthMatching(fields.get(CronFieldName.DAY_OF_MONTH)).forDaysOfWeekMatching(fields.get(CronFieldName.DAY_OF_WEEK)).forMonthsMatching(fields.get(CronFieldName.MONTH));
  if (fields.containsKey(CronFieldName.YEAR)) {
    executionTimeBuilder.forYearsMatching(fields.get(CronFieldName.YEAR));
  }
  return executionTimeBuilder.build();
}","/** 
 * Creates execution time for given Cron
 * @param cron - Cron instance
 * @return ExecutionTime instance
 */
public static ExecutionTime forCron(Cron cron){
  Map<CronFieldName,CronField> fields=cron.retrieveFieldsAsMap();
  ExecutionTimeBuilder executionTimeBuilder=new ExecutionTimeBuilder(cron.getCronDefinition());
  if (fields.containsKey(CronFieldName.SECOND)) {
    executionTimeBuilder.forSecondsMatching(fields.get(CronFieldName.SECOND));
  }
  executionTimeBuilder.forMinutesMatching(fields.get(CronFieldName.MINUTE)).forHoursMatching(fields.get(CronFieldName.HOUR)).forDaysOfMonthMatching(fields.get(CronFieldName.DAY_OF_MONTH)).forDaysOfWeekMatching(fields.get(CronFieldName.DAY_OF_WEEK)).forMonthsMatching(fields.get(CronFieldName.MONTH));
  if (fields.containsKey(CronFieldName.YEAR)) {
    executionTimeBuilder.forYearsMatching(fields.get(CronFieldName.YEAR));
  }
  return executionTimeBuilder.build();
}"
4342,"/** 
 * Provide nearest date for next execution.
 * @param date - jodatime DateTime instance. If null, a NullPointerException will be raised.
 * @return DateTime instance, never null. Next execution time.
 */
public DateTime nextExecution(DateTime date){
  Validate.notNull(date);
  NearestValue secondsValue=seconds.getNextValue(date.getSecondOfMinute(),1);
  NearestValue minutesValue=minutes.getNextValue(date.getMinuteOfHour(),secondsValue.getShifts());
  NearestValue hoursValue=hours.getNextValue(date.getHourOfDay(),minutesValue.getShifts());
  NearestValue monthsValue;
  int month=1;
  int day=1;
  if (months.getValues().contains(date.getMonthOfYear())) {
    monthsValue=new NearestValue(date.getMonthOfYear(),0);
    day=date.getDayOfMonth();
  }
 else {
    monthsValue=months.getNextValue(date.getMonthOfYear(),0);
    month=monthsValue.getValue();
    day=1;
  }
  TimeNode days=new TimeNode(generateDayCandidates(date.getYear(),month));
  NearestValue daysValue=days.getNextValue(day,hoursValue.getShifts());
  monthsValue=months.getNextValue(month,daysValue.getShifts());
  if (daysValue.getShifts() > 0) {
    days=new TimeNode(generateDayCandidates(date.getYear(),monthsValue.getValue()));
    daysValue=new NearestValue(days.getValues().get(0),0);
  }
  NearestValue yearsValue=new TimeNode(generateYearCandidates(date.getYear())).getNextValue(date.getYear(),monthsValue.getShifts());
  return new DateTime(yearsValue.getValue(),monthsValue.getValue(),daysValue.getValue(),hoursValue.getValue(),minutesValue.getValue(),secondsValue.getValue());
}","/** 
 * Provide nearest date for next execution.
 * @param date - jodatime DateTime instance. If null, a NullPointerException will be raised.
 * @return DateTime instance, never null. Next execution time.
 */
public DateTime nextExecution(DateTime date){
  Validate.notNull(date);
  NearestValue secondsValue=seconds.getNextValue(date.getSecondOfMinute(),1);
  NearestValue minutesValue=minutes.getNextValue(date.getMinuteOfHour(),secondsValue.getShifts());
  NearestValue hoursValue=hours.getNextValue(date.getHourOfDay(),minutesValue.getShifts());
  NearestValue monthsValue;
  int month=1;
  int day=1;
  if (months.getValues().contains(date.getMonthOfYear())) {
    monthsValue=new NearestValue(date.getMonthOfYear(),0);
    day=date.getDayOfMonth();
  }
 else {
    monthsValue=months.getNextValue(date.getMonthOfYear(),0);
    month=monthsValue.getValue();
    day=1;
  }
  TimeNode days=new TimeNode(generateDayCandidates(date.getYear(),month,((DayOfWeekFieldDefinition)cronDefinition.getFieldDefinition(CronFieldName.DAY_OF_WEEK)).getMondayDoWValue()));
  NearestValue daysValue=days.getNextValue(day,hoursValue.getShifts());
  monthsValue=months.getNextValue(month,daysValue.getShifts());
  if (daysValue.getShifts() > 0) {
    days=new TimeNode(generateDayCandidates(date.getYear(),monthsValue.getValue(),((DayOfWeekFieldDefinition)cronDefinition.getFieldDefinition(CronFieldName.DAY_OF_WEEK)).getMondayDoWValue()));
    daysValue=new NearestValue(days.getValues().get(0),0);
  }
  NearestValue yearsValue=new TimeNode(generateYearCandidates(date.getYear())).getNextValue(date.getYear(),monthsValue.getShifts());
  return new DateTime(yearsValue.getValue(),monthsValue.getValue(),daysValue.getValue(),hoursValue.getValue(),minutesValue.getValue(),secondsValue.getValue());
}"
4343,"private List<Integer> generateDayCandidates(int year,int month){
  DateTime date=new DateTime(year,month,1,1,1);
  List<Integer> candidates=Lists.newArrayList();
  candidates.addAll(FieldValueGeneratorFactory.forCronField(daysOfMonthCronField,year,month).generateCandidates(1,date.dayOfMonth().getMaximumValue()));
  candidates.addAll(FieldValueGeneratorFactory.forCronField(daysOfWeekCronField,year,month).generateCandidates(1,date.dayOfMonth().getMaximumValue()));
  return candidates;
}","private List<Integer> generateDayCandidates(int year,int month,WeekDay mondayDoWValue){
  DateTime date=new DateTime(year,month,1,1,1);
  List<Integer> candidates=Lists.newArrayList();
  candidates.addAll(FieldValueGeneratorFactory.createDayOfMonthValueGeneratorInstance(daysOfMonthCronField,year,month).generateCandidates(1,date.dayOfMonth().getMaximumValue()));
  candidates.addAll(FieldValueGeneratorFactory.createDayOfWeekValueGeneratorInstance(daysOfWeekCronField,year,month,mondayDoWValue).generateCandidates(1,date.dayOfMonth().getMaximumValue()));
  return candidates;
}"
4344,"ExecutionTime build(){
  return new ExecutionTime(yearsValueGenerator,daysOfWeekCronField,daysOfMonthCronField,months,hours,minutes,seconds);
}","ExecutionTime build(){
  return new ExecutionTime(cronDefinition,yearsValueGenerator,daysOfWeekCronField,daysOfMonthCronField,months,hours,minutes,seconds);
}"
4345,"ExecutionTimeBuilder(){
  seconds=new TimeNode(FieldValueGeneratorFactory.forCronField(new CronField(CronFieldName.SECOND,new On(FieldConstraintsBuilder.instance().forField(CronFieldName.SECOND).createConstraintsInstance(),""String_Node_Str""))).generateCandidates(1,60));
  yearsValueGenerator=FieldValueGeneratorFactory.forCronField(new CronField(CronFieldName.YEAR,new Always(FieldConstraintsBuilder.instance().forField(CronFieldName.YEAR).createConstraintsInstance())));
}","ExecutionTimeBuilder(CronDefinition cronDefinition){
  this.cronDefinition=cronDefinition;
  seconds=new TimeNode(FieldValueGeneratorFactory.forCronField(new CronField(CronFieldName.SECOND,new On(FieldConstraintsBuilder.instance().forField(CronFieldName.SECOND).createConstraintsInstance(),""String_Node_Str""))).generateCandidates(1,60));
  yearsValueGenerator=FieldValueGeneratorFactory.forCronField(new CronField(CronFieldName.YEAR,new Always(FieldConstraintsBuilder.instance().forField(CronFieldName.YEAR).createConstraintsInstance())));
}"
4346,"/** 
 * Parse string with cron expression
 * @param expression - cron expression, never null
 * @return Cron instance, corresponding to cron expression received
 */
public Cron parse(String expression){
  Validate.notNull(expression,""String_Node_Str"");
  if (StringUtils.isEmpty(expression)) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  expression=expression.toUpperCase();
  expression=expression.replace(""String_Node_Str"",""String_Node_Str"");
  String[] expressionParts=expression.split(""String_Node_Str"");
  if (expressions.containsKey(expressionParts.length)) {
    List<CronField> results=new ArrayList<CronField>();
    List<CronParserField> fields=expressions.get(expressionParts.length);
    for (int j=0; j < fields.size(); j++) {
      results.add(fields.get(j).parse(expressionParts[j]));
    }
    return new Cron(cronDefinition,results);
  }
 else {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",expressionParts.length,expressions.keySet()));
  }
}","/** 
 * Parse string with cron expression
 * @param expression - cron expression, never null
 * @return Cron instance, corresponding to cron expression received
 */
public Cron parse(String expression){
  Validate.notNull(expression,""String_Node_Str"");
  expression=expression.replaceAll(""String_Node_Str"",""String_Node_Str"").trim();
  if (StringUtils.isEmpty(expression)) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  expression=expression.toUpperCase();
  expression=expression.replace(""String_Node_Str"",""String_Node_Str"");
  String[] expressionParts=expression.split(""String_Node_Str"");
  if (expressions.containsKey(expressionParts.length)) {
    List<CronField> results=new ArrayList<CronField>();
    List<CronParserField> fields=expressions.get(expressionParts.length);
    for (int j=0; j < fields.size(); j++) {
      results.add(fields.get(j).parse(expressionParts[j]));
    }
    return new Cron(cronDefinition,results);
  }
 else {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",expressionParts.length,expressions.keySet()));
  }
}"
4347,"public List<Long> getOrCreateNode(Long start,Long end,GraphDatabaseService db){
  List<Long> relList=relationshipCache.getIfPresent(start);
  Node startNode=db.getNodeById(start);
  if (relList == null) {
    List<Long> nodeList=new ArrayList<>();
    for (    Node endNodes : db.traversalDescription().depthFirst().relationships(withName(relationshipType),Direction.OUTGOING).evaluator(Evaluators.fromDepth(1)).evaluator(Evaluators.toDepth(1)).traverse(startNode).nodes()) {
      nodeList.add(endNodes.getId());
    }
    relationshipCache.put(start,nodeList);
    relList=nodeList;
  }
  if (!relList.contains(end)) {
    Transaction tx=db.beginTx();
    try {
      Node endNode=db.getNodeById(end);
      startNode.createRelationshipTo(endNode,withName(relationshipType));
      tx.success();
    }
 catch (    final Exception e) {
      tx.failure();
    }
 finally {
      tx.finish();
      relList.add(end);
      relationshipCache.put(start,relList);
    }
  }
  return relList;
}","public List<Long> getOrCreateNode(Long start,Long end,GraphDatabaseService db){
  List<Long> relList=relationshipCache.getIfPresent(start);
  Node startNode=db.getNodeById(start);
  if (relList == null) {
    List<Long> nodeList=new ArrayList<>();
    for (    Node endNodes : db.traversalDescription().depthFirst().relationships(withName(relationshipType),Direction.OUTGOING).evaluator(Evaluators.fromDepth(1)).evaluator(Evaluators.toDepth(1)).traverse(startNode).nodes()) {
      nodeList.add(endNodes.getId());
    }
    startNode.setProperty(""String_Node_Str"",nodeList.size());
    relationshipCache.put(start,nodeList);
    relList=nodeList;
  }
  if (!relList.contains(end)) {
    Transaction tx=db.beginTx();
    try {
      Node endNode=db.getNodeById(end);
      startNode.createRelationshipTo(endNode,withName(relationshipType));
      startNode.setProperty(""String_Node_Str"",relList.size());
      tx.success();
    }
 catch (    final Exception e) {
      tx.failure();
    }
 finally {
      tx.finish();
      relList.add(end);
      relationshipCache.put(start,relList);
    }
  }
  return relList;
}"
4348,"@Override public boolean tryAdvance(Consumer<? super List<I>> action){
  boolean hadElements=source.tryAdvance(curElem -> {
    if (!isSameSlide(curElem)) {
      action.accept(currentSlide);
      currentSlide=new ArrayList<>();
    }
    currentSlide.add(curElem);
  }
);
  if (!hadElements && !currentSlide.isEmpty()) {
    action.accept(currentSlide);
    currentSlide=new ArrayList<>();
  }
  return hadElements;
}","@Override public boolean tryAdvance(Consumer<? super List<I>> action){
  boolean hadElements;
  do {
    hadElements=source.tryAdvance(curElem -> {
      wasSameSlide=isSameSlide(curElem);
      if (!wasSameSlide) {
        action.accept(currentSlide);
        currentSlide=new ArrayList<>();
      }
      currentSlide.add(curElem);
    }
);
  }
 while (wasSameSlide && hadElements);
  if (!hadElements && !currentSlide.isEmpty()) {
    action.accept(currentSlide);
    currentSlide=new ArrayList<>();
  }
  return hadElements;
}"
4349,"@Override public boolean equals(Object obj){
  if (this == obj) {
    return true;
  }
  if (obj == null || getClass() != obj.getClass()) {
    return false;
  }
  final Indexed other=(Indexed)obj;
  return Objects.equals(this.index,other.index) && Objects.equals(this.value,other.value);
}","@Override public boolean equals(Object obj){
  if (this == obj) {
    return true;
  }
  if (obj == null || getClass() != obj.getClass()) {
    return false;
  }
  final Indexed other=(Indexed)obj;
  return Objects.equals(index,other.index) && Objects.equals(value,other.value);
}"
4350,"static String message(Response response){
  try {
    return response.errorBody().string();
  }
 catch (  IOException e) {
    return response.message();
  }
}","static String message(Response response){
  try {
    ResponseBody responseBody=response.errorBody();
    return responseBody == null ? response.message() : responseBody.string();
  }
 catch (  IOException e) {
    return response.message();
  }
}"
4351,"private static EventResponse eventResponse(Response<List<Event>> response){
  String indexHeaderValue=response.headers().get(""String_Node_Str"");
  BigInteger index=new BigInteger(indexHeaderValue);
  return ImmutableEventResponse.of(response.body(),index);
}","private static EventResponse eventResponse(Response<List<Event>> response){
  String indexHeaderValue=response.headers().get(""String_Node_Str"");
  BigInteger index=indexHeaderValue == null ? BigInteger.ZERO : new BigInteger(indexHeaderValue);
  return ImmutableEventResponse.of(response.body(),index);
}"
4352,"@PUT(""String_Node_Str"") Call<Boolean> putValue(@Path(""String_Node_Str"") String key,@Body String data,@QueryMap Map<String,Object> query);","@PUT(""String_Node_Str"") Call<Boolean> putValue(@Path(""String_Node_Str"") String key,@Body RequestBody data,@QueryMap Map<String,Object> query);"
4353,"@JsonIgnore @org.immutables.value.Value.Lazy public Optional<String> getValueAsString(){
  if (getValue().isPresent()) {
    return Optional.of(unquote(new String(BaseEncoding.base64().decode(getValue().get()))));
  }
 else {
    return Optional.absent();
  }
}","@JsonIgnore @org.immutables.value.Value.Lazy public Optional<String> getValueAsString(){
  if (getValue().isPresent()) {
    return Optional.of(new String(BaseEncoding.base64().decode(getValue().get())));
  }
 else {
    return Optional.absent();
  }
}"
4354,"/** 
 * {@inheritDoc}
 */
@Override public Optional<String> deserialize(JsonParser p,DeserializationContext ctxt) throws IOException {
  String value=p.getValueAsString();
  if (StringUtils.isNotEmpty(value)) {
    return Optional.of(unquote(new String(BaseEncoding.base64().decode(value))));
  }
  return Optional.absent();
}","/** 
 * {@inheritDoc}
 */
@Override public Optional<String> deserialize(JsonParser p,DeserializationContext ctxt) throws IOException {
  String value=p.getValueAsString();
  if (StringUtils.isNotEmpty(value)) {
    return Optional.of(new String(BaseEncoding.base64().decode(value)));
  }
  return Optional.absent();
}"
4355,"@Override public void onFailure(Throwable throwable){
  LOGGER.error(""String_Node_Str"",backoffDelayQty,backoffDelayUnit,throwable);
  executorService.schedule(new Runnable(){
    @Override public void run(){
      runCallback();
    }
  }
,backoffDelayQty,backoffDelayUnit);
}","@Override public void onFailure(Throwable throwable){
  LOGGER.error(String.format(""String_Node_Str"",backoffDelayQty,backoffDelayUnit),throwable);
  executorService.schedule(new Runnable(){
    @Override public void run(){
      runCallback();
    }
  }
,backoffDelayQty,backoffDelayUnit);
}"
4356,"ConsulCache(Function<V,K> keyConversion,CallbackConsumer<V> callbackConsumer,final long backoffDelayQty,final TimeUnit backoffDelayUnit){
  this.keyConversion=keyConversion;
  this.callBackConsumer=callbackConsumer;
  this.responseCallback=new ConsulResponseCallback<List<V>>(){
    @Override public void onComplete(    ConsulResponse<List<V>> consulResponse){
      updateIndex(consulResponse);
      ImmutableMap<K,V> full=convertToMap(consulResponse);
      boolean changed=!full.equals(lastState.get());
      if (changed) {
        lastState.set(full);
      }
      if (initialized.compareAndSet(false,true)) {
        initLatch.countDown();
      }
      if (changed) {
        for (        Listener<K,V> l : listeners) {
          l.notify(full);
        }
      }
      runCallback();
    }
    @Override public void onFailure(    Throwable throwable){
      LOGGER.error(""String_Node_Str"",backoffDelayQty,backoffDelayUnit,throwable);
      executorService.schedule(new Runnable(){
        @Override public void run(){
          runCallback();
        }
      }
,backoffDelayQty,backoffDelayUnit);
    }
  }
;
}","ConsulCache(Function<V,K> keyConversion,CallbackConsumer<V> callbackConsumer,final long backoffDelayQty,final TimeUnit backoffDelayUnit){
  this.keyConversion=keyConversion;
  this.callBackConsumer=callbackConsumer;
  this.responseCallback=new ConsulResponseCallback<List<V>>(){
    @Override public void onComplete(    ConsulResponse<List<V>> consulResponse){
      updateIndex(consulResponse);
      ImmutableMap<K,V> full=convertToMap(consulResponse);
      boolean changed=!full.equals(lastState.get());
      if (changed) {
        lastState.set(full);
      }
      if (initialized.compareAndSet(false,true)) {
        initLatch.countDown();
      }
      if (changed) {
        for (        Listener<K,V> l : listeners) {
          l.notify(full);
        }
      }
      runCallback();
    }
    @Override public void onFailure(    Throwable throwable){
      LOGGER.error(String.format(""String_Node_Str"",backoffDelayQty,backoffDelayUnit),throwable);
      executorService.schedule(new Runnable(){
        @Override public void run(){
          runCallback();
        }
      }
,backoffDelayQty,backoffDelayUnit);
    }
  }
;
}"
4357,"@Override public void pushFile(File localFrom,String remoteTo){
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
  try {
    if (device.getSyncService() == null)     throw new RuntimeException(""String_Node_Str"");
    device.getSyncService().pushFile(localFrom.getAbsolutePath(),remoteTo,SyncService.getNullProgressMonitor());
  }
 catch (  Exception ex) {
    logger.error(""String_Node_Str"",ex);
    throw new RuntimeException(ex);
  }
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
}","@Override public void pushFile(File localFrom,String remoteTo){
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
  try {
    if (device.getSyncService() == null)     throw new AndroidScreenCastRuntimeException(""String_Node_Str"");
    device.getSyncService().pushFile(localFrom.getAbsolutePath(),remoteTo,SyncService.getNullProgressMonitor());
  }
 catch (  Exception ex) {
    logger.error(""String_Node_Str"",ex);
    throw new AndroidScreenCastRuntimeException(ex);
  }
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
}"
4358,"@Override public void pullFile(String removeFrom,File localTo){
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
  try {
    if (device.getSyncService() == null)     throw new RuntimeException(""String_Node_Str"");
    Method m=device.getSyncService().getClass().getDeclaredMethod(""String_Node_Str"",String.class,String.class,ISyncProgressMonitor.class);
    m.setAccessible(true);
    device.getSyncService();
    m.invoke(device.getSyncService(),removeFrom,localTo.getAbsolutePath(),SyncService.getNullProgressMonitor());
  }
 catch (  Exception ex) {
    logger.error(""String_Node_Str"",ex);
    throw new RuntimeException(ex);
  }
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
}","@Override public void pullFile(String removeFrom,File localTo){
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
  try {
    if (device.getSyncService() == null)     throw new RuntimeException(""String_Node_Str"");
    Method m=device.getSyncService().getClass().getDeclaredMethod(""String_Node_Str"",String.class,String.class,ISyncProgressMonitor.class);
    m.setAccessible(true);
    device.getSyncService();
    m.invoke(device.getSyncService(),removeFrom,localTo.getAbsolutePath(),SyncService.getNullProgressMonitor());
  }
 catch (  Exception ex) {
    logger.error(""String_Node_Str"",ex);
    throw new AndroidScreenCastRuntimeException(ex);
  }
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
}"
4359,"@Override public List<FileInfo> list(String path){
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
  try {
    String s=executeCommand(""String_Node_Str"" + path);
    String[] entries=s.split(""String_Node_Str"");
    Vector<FileInfo> liste=new Vector<>();
    for (    String entry : entries) {
      String[] data=entry.split(""String_Node_Str"");
      if (data.length < 4)       continue;
      String attributes=data[0];
      boolean directory=attributes.startsWith(""String_Node_Str"");
      String name=data[data.length - 1];
      FileInfo fi=new FileInfo();
      fi.attribs=attributes;
      fi.directory=directory;
      fi.name=name;
      fi.path=path;
      fi.device=this;
      liste.add(fi);
    }
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    return liste;
  }
 catch (  Exception ex) {
    logger.error(""String_Node_Str"",ex);
    throw new RuntimeException(ex);
  }
}","@Override public List<FileInfo> list(String path){
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
  try {
    String s=executeCommand(""String_Node_Str"" + path);
    String[] entries=s.split(""String_Node_Str"");
    List<FileInfo> fileInfos=new ArrayList<>();
    for (    String entry : entries) {
      String[] data=entry.split(""String_Node_Str"");
      if (data.length < 4)       continue;
      String attributes=data[0];
      boolean directory=attributes.charAt(0) == 'd';
      String name=data[data.length - 1];
      FileInfo fi=new FileInfo();
      fi.attribs=attributes;
      fi.directory=directory;
      fi.name=name;
      fi.path=path;
      fi.device=this;
      fileInfos.add(fi);
    }
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    return fileInfos;
  }
 catch (  Exception ex) {
    logger.error(""String_Node_Str"",ex);
    throw new AndroidScreenCastRuntimeException(ex);
  }
}"
4360,"@Override public String executeCommand(String cmd){
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
  ByteArrayOutputStream bos=new ByteArrayOutputStream();
  try {
    device.executeShellCommand(cmd,new OutputStreamShellOutputReceiver(bos));
    String returnString=new String(bos.toByteArray(),""String_Node_Str"");
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    return returnString;
  }
 catch (  Exception ex) {
    logger.error(""String_Node_Str"",ex);
    throw new ExecuteCommandException(cmd);
  }
}","@Override public String executeCommand(String cmd){
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
  try (ByteArrayOutputStream bos=new ByteArrayOutputStream()){
    device.executeShellCommand(cmd,new OutputStreamShellOutputReceiver(bos));
    String returnString=new String(bos.toByteArray(),""String_Node_Str"");
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    return returnString;
  }
 catch (  Exception ex) {
    logger.error(""String_Node_Str"",ex);
    throw new ExecuteCommandException(cmd);
  }
}"
4361,"public File downloadTemporary(){
  try {
    File tempFile=File.createTempFile(""String_Node_Str"",name);
    device.pullFile(path + name,tempFile);
    tempFile.deleteOnExit();
    return tempFile;
  }
 catch (  Exception ex) {
    throw new RuntimeException(ex);
  }
}","public File downloadTemporary(){
  try {
    File tempFile=File.createTempFile(""String_Node_Str"",name);
    device.pullFile(path + name,tempFile);
    tempFile.deleteOnExit();
    return tempFile;
  }
 catch (  IOException ex) {
    throw new IORuntimeException(ex);
  }
}"
4362,"public void add(Atom child){
  if (children.size() > 0) {
    children.getLast().finish();
  }
  children.add(child);
}","public void add(Atom child){
  if (children.size() > 0) {
    children.get(children.size() - 1).finish();
  }
  children.add(child);
}"
4363,"private void displayFolder(String path){
  List<FileInfo> fileInfos=cache.get(path);
  if (fileInfos == null)   fileInfos=androidDevice.list(path);
  List<FileInfo> files=new Vector<>();
  for (  FileInfo fi2 : fileInfos) {
    if (fi2.directory)     continue;
    files.add(fi2);
  }
  jListFichiers.setListData(files.toArray());
}","private void displayFolder(String path){
  List<FileInfo> fileInfos=cache.get(path);
  if (fileInfos == null)   fileInfos=androidDevice.list(path);
  List<FileInfo> files=new ArrayList<>();
  for (  FileInfo fi2 : fileInfos) {
    if (fi2.directory)     continue;
    files.add(fi2);
  }
  jListFichiers.setListData(files.toArray());
}"
4364,"int getMask(int length){
  int res=0;
  for (int i=0; i < length; i++) {
    res=(res << 1) + 1;
  }
  return res;
}","protected int getMask(int length){
  int res=0;
  for (int i=0; i < length; i++) {
    res=(res << 1) + 1;
  }
  return res;
}"
4365,"int getPixel(Object inData){
  return getPixel((byte[])inData);
}","protected int getPixel(Object inData){
  return getPixel((byte[])inData);
}"
4366,"/** 
 * Convert a raw image into a buffered image.
 * @param rawImage the image to convert.
 * @return the converted image.
 */
public static BufferedImage convertImage(RawImage rawImage){
switch (rawImage.bpp) {
case SIXTEEN_BIT_IMAGE:
    return rawImage16toARGB(rawImage);
case THIRTY_TWO_BIT_IMAGE:
  return rawImage32toARGB(rawImage);
}
throw new IllegalArgumentException(""String_Node_Str"" + rawImage.bpp);
}","/** 
 * Convert a raw image into a buffered image.
 * @param rawImage the image to convert.
 * @return the converted image.
 */
public static BufferedImage convertImage(RawImage rawImage){
switch (rawImage.bpp) {
case SIXTEEN_BIT_IMAGE:
    return rawImage16toARGB(rawImage);
case THIRTY_TWO_BIT_IMAGE:
  return rawImage32toARGB(rawImage);
default :
throw new IllegalStateException(""String_Node_Str"" + rawImage.bpp);
}
}"
4367,"public void stop(){
  LOGGER.debug(""String_Node_Str"");
  screenCaptureRunnable.stop();
  LOGGER.debug(""String_Node_Str"");
}","public void stop(){
  screenCaptureRunnable.stop();
}"
4368,"public void start(){
  LOGGER.debug(""String_Node_Str"");
  screenCaptureThread.start();
  LOGGER.debug(""String_Node_Str"");
}","public void start(){
  screenCaptureThread.start();
}"
4369,"public static int getKeyCode(KeyEvent e){
  LOGGER.debug(""String_Node_Str"" + e + ""String_Node_Str"");
  int code=InputKeyEvent.KEYCODE_UNKNOWN.getCode();
  char c=e.getKeyChar();
  int keyCode=e.getKeyCode();
  InputKeyEvent inputKeyEvent=InputKeyEvent.getByCharacterOrKeyCode(Character.toLowerCase(c),keyCode);
  if (inputKeyEvent != null) {
    code=inputKeyEvent.getCode();
  }
  LOGGER.debug(""String_Node_Str"" + e + ""String_Node_Str"");
  return code;
}","public static int getKeyCode(KeyEvent e){
  LOGGER.debug(""String_Node_Str"" + e + ""String_Node_Str"");
  int code=InputKeyEvent.KEYCODE_UNKNOWN.getCode();
  char c=e.getKeyChar();
  int keyCode=e.getKeyCode();
  InputKeyEvent inputKeyEvent=InputKeyEvent.getByCharacterOrKeyCode(Character.toLowerCase(c),keyCode);
  if (inputKeyEvent != null) {
    code=inputKeyEvent.getCode();
  }
  LOGGER.debug(String.format(""String_Node_Str"",String.valueOf(e),code));
  return code;
}"
4370,"@Override public void flush(){
  try {
    os.flush();
  }
 catch (  IOException e) {
    e.printStackTrace();
  }
}","@Override public void flush(){
  try {
    os.flush();
  }
 catch (  IOException e) {
    throw new IORuntimeException(e);
  }
}"
4371,"@Override public void addOutput(byte[] buf,int off,int len){
  try {
    os.write(buf,off,len);
  }
 catch (  IOException ex) {
    throw new RuntimeException(ex);
  }
}","@Override public void addOutput(byte[] buf,int off,int len){
  try {
    os.write(buf,off,len);
  }
 catch (  IOException e) {
    throw new IORuntimeException(e);
  }
}"
4372,"private void writeEpilog() throws IOException {
  Date modificationTime=new Date();
  int duration=0;
  for (  Sample s : videoFrames) {
    duration+=s.duration;
  }
  DataAtom leaf;
  CompositeAtom moovAtom=new CompositeAtom(""String_Node_Str"");
  leaf=new DataAtom(""String_Node_Str"");
  moovAtom.add(leaf);
  DataAtomOutputStream d=leaf.getOutputStream();
  d.writeByte(0);
  d.writeByte(0);
  d.writeByte(0);
  d.writeByte(0);
  d.writeMacTimestamp(creationTime);
  d.writeMacTimestamp(modificationTime);
  d.writeInt(timeScale);
  d.writeInt(duration);
  d.writeFixed16D16(1d);
  d.writeShort(256);
  d.write(new byte[10]);
  d.writeFixed16D16(1f);
  d.writeFixed16D16(0f);
  d.writeFixed2D30(0f);
  d.writeFixed16D16(0f);
  d.writeFixed16D16(1f);
  d.writeFixed2D30(0);
  d.writeFixed16D16(0);
  d.writeFixed16D16(0);
  d.writeFixed2D30(1f);
  d.writeInt(0);
  d.writeInt(0);
  d.writeInt(0);
  d.writeInt(0);
  d.writeInt(0);
  d.writeInt(0);
  d.writeInt(2);
  CompositeAtom trakAtom=new CompositeAtom(""String_Node_Str"");
  moovAtom.add(trakAtom);
  leaf=new DataAtom(""String_Node_Str"");
  trakAtom.add(leaf);
  d=leaf.getOutputStream();
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0xf);
  d.writeMacTimestamp(creationTime);
  d.writeMacTimestamp(modificationTime);
  d.writeInt(1);
  d.writeInt(0);
  d.writeInt(duration);
  d.writeLong(0);
  d.writeShort(0);
  d.writeShort(0);
  d.writeShort(0);
  d.writeShort(0);
  d.writeFixed16D16(1f);
  d.writeFixed16D16(0f);
  d.writeFixed2D30(0f);
  d.writeFixed16D16(0f);
  d.writeFixed16D16(1f);
  d.writeFixed2D30(0);
  d.writeFixed16D16(0);
  d.writeFixed16D16(0);
  d.writeFixed2D30(1f);
  d.writeFixed16D16(imgWidth);
  d.writeFixed16D16(imgHeight);
  CompositeAtom mdiaAtom=new CompositeAtom(""String_Node_Str"");
  trakAtom.add(mdiaAtom);
  leaf=new DataAtom(""String_Node_Str"");
  mdiaAtom.add(leaf);
  d=leaf.getOutputStream();
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0);
  d.writeMacTimestamp(creationTime);
  d.writeMacTimestamp(modificationTime);
  d.writeInt(timeScale);
  d.writeInt(duration);
  d.writeShort(0);
  d.writeShort(0);
  leaf=new DataAtom(""String_Node_Str"");
  mdiaAtom.add(leaf);
  d=leaf.getOutputStream();
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0);
  d.writeType(""String_Node_Str"");
  d.writeType(""String_Node_Str"");
  d.writeInt(0);
  d.writeInt(0);
  d.writeInt(0);
  d.write(0);
  CompositeAtom minfAtom=new CompositeAtom(""String_Node_Str"");
  mdiaAtom.add(minfAtom);
  leaf=new DataAtom(""String_Node_Str"");
  minfAtom.add(leaf);
  d=leaf.getOutputStream();
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0x1);
  d.writeShort(0x40);
  d.writeUShort(0);
  d.writeUShort(0);
  d.writeUShort(0);
  leaf=new DataAtom(""String_Node_Str"");
  minfAtom.add(leaf);
  d=leaf.getOutputStream();
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0);
  d.writeType(""String_Node_Str"");
  d.writeType(""String_Node_Str"");
  d.writeInt(0);
  d.writeInt(0);
  d.writeInt(0);
  d.write(0);
  CompositeAtom dinfAtom=new CompositeAtom(""String_Node_Str"");
  minfAtom.add(dinfAtom);
  leaf=new DataAtom(""String_Node_Str"");
  dinfAtom.add(leaf);
  d=leaf.getOutputStream();
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0);
  d.writeInt(1);
  d.writeInt(12);
  d.writeType(""String_Node_Str"");
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0x1);
  CompositeAtom stblAtom=new CompositeAtom(""String_Node_Str"");
  minfAtom.add(stblAtom);
  leaf=new DataAtom(""String_Node_Str"");
  stblAtom.add(leaf);
  d=leaf.getOutputStream();
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0);
  d.writeInt(1);
switch (videoFormat) {
case RAW:
{
      d.writeInt(86);
      d.writeType(""String_Node_Str"");
      d.write(new byte[6]);
      d.writeShort(1);
      d.writeShort(0);
      d.writeShort(0);
      d.writeType(""String_Node_Str"");
      d.writeInt(0);
      d.writeInt(512);
      d.writeUShort(imgWidth);
      d.writeUShort(imgHeight);
      d.writeFixed16D16(72.0);
      d.writeFixed16D16(72.0);
      d.writeInt(0);
      d.writeShort(1);
      d.writePString(""String_Node_Str"",32);
      d.writeShort(24);
      d.writeShort(-1);
      break;
    }
case JPG:
{
    d.writeInt(86);
    d.writeType(""String_Node_Str"");
    d.write(new byte[6]);
    d.writeShort(1);
    d.writeShort(0);
    d.writeShort(0);
    d.writeType(""String_Node_Str"");
    d.writeInt(0);
    d.writeInt(512);
    d.writeUShort(imgWidth);
    d.writeUShort(imgHeight);
    d.writeFixed16D16(72.0);
    d.writeFixed16D16(72.0);
    d.writeInt(0);
    d.writeShort(1);
    d.writePString(""String_Node_Str"",32);
    d.writeShort(24);
    d.writeShort(-1);
    break;
  }
case PNG:
{
  d.writeInt(86);
  d.writeType(""String_Node_Str"");
  d.write(new byte[6]);
  d.writeShort(1);
  d.writeShort(0);
  d.writeShort(0);
  d.writeType(""String_Node_Str"");
  d.writeInt(0);
  d.writeInt(512);
  d.writeUShort(imgWidth);
  d.writeUShort(imgHeight);
  d.writeFixed16D16(72.0);
  d.writeFixed16D16(72.0);
  d.writeInt(0);
  d.writeShort(1);
  d.writePString(""String_Node_Str"",32);
  d.writeShort(24);
  d.writeShort(-1);
  break;
}
}
leaf=new DataAtom(""String_Node_Str"");
stblAtom.add(leaf);
d=leaf.getOutputStream();
d.write(0);
d.write(0);
d.write(0);
d.write(0);
int runCount=1;
int prevDuration=videoFrames.size() == 0 ? 0 : videoFrames.get(0).duration;
for (Sample s : videoFrames) {
if (s.duration != prevDuration) {
runCount++;
prevDuration=s.duration;
}
}
d.writeInt(runCount);
int runLength=0;
prevDuration=videoFrames.size() == 0 ? 0 : videoFrames.get(0).duration;
for (Sample s : videoFrames) {
if (s.duration != prevDuration) {
if (runLength > 0) {
  d.writeInt(runLength);
  d.writeInt(prevDuration);
}
prevDuration=s.duration;
runLength=1;
}
 else {
runLength++;
}
}
if (runLength > 0) {
d.writeInt(runLength);
d.writeInt(prevDuration);
}
leaf=new DataAtom(""String_Node_Str"");
stblAtom.add(leaf);
d=leaf.getOutputStream();
d.write(0);
d.write(0);
d.write(0);
d.write(0);
d.writeInt(1);
d.writeInt(1);
d.writeInt(1);
d.writeInt(1);
leaf=new DataAtom(""String_Node_Str"");
stblAtom.add(leaf);
d=leaf.getOutputStream();
d.write(0);
d.write(0);
d.write(0);
d.write(0);
d.writeUInt(0);
d.writeUInt(videoFrames.size());
for (Sample s : videoFrames) {
d.writeUInt(s.length);
}
if (videoFrames.size() == 0 || videoFrames.getLast().offset <= 0xffffffffL) {
leaf=new DataAtom(""String_Node_Str"");
stblAtom.add(leaf);
d=leaf.getOutputStream();
d.write(0);
d.write(0);
d.write(0);
d.write(0);
d.writeUInt(videoFrames.size());
for (Sample s : videoFrames) {
d.writeUInt(s.offset);
}
}
 else {
leaf=new DataAtom(""String_Node_Str"");
stblAtom.add(leaf);
d=leaf.getOutputStream();
d.write(0);
d.write(0);
d.write(0);
d.write(0);
d.writeUInt(videoFrames.size());
for (Sample s : videoFrames) {
d.writeLong(s.offset);
}
}
moovAtom.finish();
}","private void writeEpilog() throws IOException {
  Date modificationTime=new Date();
  int duration=0;
  for (  Sample s : videoFrames) {
    duration+=s.duration;
  }
  DataAtom leaf;
  CompositeAtom moovAtom=new CompositeAtom(""String_Node_Str"");
  leaf=new DataAtom(""String_Node_Str"");
  moovAtom.add(leaf);
  DataAtomOutputStream d=leaf.getOutputStream();
  d.writeByte(0);
  d.writeByte(0);
  d.writeByte(0);
  d.writeByte(0);
  d.writeMacTimestamp(creationTime);
  d.writeMacTimestamp(modificationTime);
  d.writeInt(timeScale);
  d.writeInt(duration);
  d.writeFixed16D16(1d);
  d.writeShort(256);
  d.write(new byte[10]);
  d.writeFixed16D16(1f);
  d.writeFixed16D16(0f);
  d.writeFixed2D30(0f);
  d.writeFixed16D16(0f);
  d.writeFixed16D16(1f);
  d.writeFixed2D30(0);
  d.writeFixed16D16(0);
  d.writeFixed16D16(0);
  d.writeFixed2D30(1f);
  d.writeInt(0);
  d.writeInt(0);
  d.writeInt(0);
  d.writeInt(0);
  d.writeInt(0);
  d.writeInt(0);
  d.writeInt(2);
  CompositeAtom trakAtom=new CompositeAtom(""String_Node_Str"");
  moovAtom.add(trakAtom);
  leaf=new DataAtom(""String_Node_Str"");
  trakAtom.add(leaf);
  d=leaf.getOutputStream();
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0xf);
  d.writeMacTimestamp(creationTime);
  d.writeMacTimestamp(modificationTime);
  d.writeInt(1);
  d.writeInt(0);
  d.writeInt(duration);
  d.writeLong(0);
  d.writeShort(0);
  d.writeShort(0);
  d.writeShort(0);
  d.writeShort(0);
  d.writeFixed16D16(1f);
  d.writeFixed16D16(0f);
  d.writeFixed2D30(0f);
  d.writeFixed16D16(0f);
  d.writeFixed16D16(1f);
  d.writeFixed2D30(0);
  d.writeFixed16D16(0);
  d.writeFixed16D16(0);
  d.writeFixed2D30(1f);
  d.writeFixed16D16(imgWidth);
  d.writeFixed16D16(imgHeight);
  CompositeAtom mdiaAtom=new CompositeAtom(""String_Node_Str"");
  trakAtom.add(mdiaAtom);
  leaf=new DataAtom(""String_Node_Str"");
  mdiaAtom.add(leaf);
  d=leaf.getOutputStream();
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0);
  d.writeMacTimestamp(creationTime);
  d.writeMacTimestamp(modificationTime);
  d.writeInt(timeScale);
  d.writeInt(duration);
  d.writeShort(0);
  d.writeShort(0);
  leaf=new DataAtom(""String_Node_Str"");
  mdiaAtom.add(leaf);
  d=leaf.getOutputStream();
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0);
  d.writeType(""String_Node_Str"");
  d.writeType(""String_Node_Str"");
  d.writeInt(0);
  d.writeInt(0);
  d.writeInt(0);
  d.write(0);
  CompositeAtom minfAtom=new CompositeAtom(""String_Node_Str"");
  mdiaAtom.add(minfAtom);
  leaf=new DataAtom(""String_Node_Str"");
  minfAtom.add(leaf);
  d=leaf.getOutputStream();
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0x1);
  d.writeShort(0x40);
  d.writeUShort(0);
  d.writeUShort(0);
  d.writeUShort(0);
  leaf=new DataAtom(""String_Node_Str"");
  minfAtom.add(leaf);
  d=leaf.getOutputStream();
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0);
  d.writeType(""String_Node_Str"");
  d.writeType(""String_Node_Str"");
  d.writeInt(0);
  d.writeInt(0);
  d.writeInt(0);
  d.write(0);
  CompositeAtom dinfAtom=new CompositeAtom(""String_Node_Str"");
  minfAtom.add(dinfAtom);
  leaf=new DataAtom(""String_Node_Str"");
  dinfAtom.add(leaf);
  d=leaf.getOutputStream();
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0);
  d.writeInt(1);
  d.writeInt(12);
  d.writeType(""String_Node_Str"");
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0x1);
  CompositeAtom stblAtom=new CompositeAtom(""String_Node_Str"");
  minfAtom.add(stblAtom);
  leaf=new DataAtom(""String_Node_Str"");
  stblAtom.add(leaf);
  d=leaf.getOutputStream();
  d.write(0);
  d.write(0);
  d.write(0);
  d.write(0);
  d.writeInt(1);
switch (videoFormat) {
case RAW:
{
      d.writeInt(86);
      d.writeType(""String_Node_Str"");
      d.write(new byte[6]);
      d.writeShort(1);
      d.writeShort(0);
      d.writeShort(0);
      d.writeType(""String_Node_Str"");
      d.writeInt(0);
      d.writeInt(512);
      d.writeUShort(imgWidth);
      d.writeUShort(imgHeight);
      d.writeFixed16D16(72.0);
      d.writeFixed16D16(72.0);
      d.writeInt(0);
      d.writeShort(1);
      d.writePString(""String_Node_Str"",32);
      d.writeShort(24);
      d.writeShort(-1);
      break;
    }
case JPG:
{
    d.writeInt(86);
    d.writeType(""String_Node_Str"");
    d.write(new byte[6]);
    d.writeShort(1);
    d.writeShort(0);
    d.writeShort(0);
    d.writeType(""String_Node_Str"");
    d.writeInt(0);
    d.writeInt(512);
    d.writeUShort(imgWidth);
    d.writeUShort(imgHeight);
    d.writeFixed16D16(72.0);
    d.writeFixed16D16(72.0);
    d.writeInt(0);
    d.writeShort(1);
    d.writePString(""String_Node_Str"",32);
    d.writeShort(24);
    d.writeShort(-1);
    break;
  }
case PNG:
{
  d.writeInt(86);
  d.writeType(""String_Node_Str"");
  d.write(new byte[6]);
  d.writeShort(1);
  d.writeShort(0);
  d.writeShort(0);
  d.writeType(""String_Node_Str"");
  d.writeInt(0);
  d.writeInt(512);
  d.writeUShort(imgWidth);
  d.writeUShort(imgHeight);
  d.writeFixed16D16(72.0);
  d.writeFixed16D16(72.0);
  d.writeInt(0);
  d.writeShort(1);
  d.writePString(""String_Node_Str"",32);
  d.writeShort(24);
  d.writeShort(-1);
  break;
}
default :
throw new IllegalStateException(""String_Node_Str"" + videoFormat);
}
leaf=new DataAtom(""String_Node_Str"");
stblAtom.add(leaf);
d=leaf.getOutputStream();
d.write(0);
d.write(0);
d.write(0);
d.write(0);
int runCount=1;
int prevDuration=videoFrames.size() == 0 ? 0 : videoFrames.get(0).duration;
for (Sample s : videoFrames) {
if (s.duration != prevDuration) {
runCount++;
prevDuration=s.duration;
}
}
d.writeInt(runCount);
int runLength=0;
prevDuration=videoFrames.size() == 0 ? 0 : videoFrames.get(0).duration;
for (Sample s : videoFrames) {
if (s.duration != prevDuration) {
if (runLength > 0) {
d.writeInt(runLength);
d.writeInt(prevDuration);
}
prevDuration=s.duration;
runLength=1;
}
 else {
runLength++;
}
}
if (runLength > 0) {
d.writeInt(runLength);
d.writeInt(prevDuration);
}
leaf=new DataAtom(""String_Node_Str"");
stblAtom.add(leaf);
d=leaf.getOutputStream();
d.write(0);
d.write(0);
d.write(0);
d.write(0);
d.writeInt(1);
d.writeInt(1);
d.writeInt(1);
d.writeInt(1);
leaf=new DataAtom(""String_Node_Str"");
stblAtom.add(leaf);
d=leaf.getOutputStream();
d.write(0);
d.write(0);
d.write(0);
d.write(0);
d.writeUInt(0);
d.writeUInt(videoFrames.size());
for (Sample s : videoFrames) {
d.writeUInt(s.length);
}
if (videoFrames.size() == 0 || videoFrames.getLast().offset <= 0xffffffffL) {
leaf=new DataAtom(""String_Node_Str"");
stblAtom.add(leaf);
d=leaf.getOutputStream();
d.write(0);
d.write(0);
d.write(0);
d.write(0);
d.writeUInt(videoFrames.size());
for (Sample s : videoFrames) {
d.writeUInt(s.offset);
}
}
 else {
leaf=new DataAtom(""String_Node_Str"");
stblAtom.add(leaf);
d=leaf.getOutputStream();
d.write(0);
d.write(0);
d.write(0);
d.write(0);
d.writeUInt(videoFrames.size());
for (Sample s : videoFrames) {
d.writeLong(s.offset);
}
}
moovAtom.finish();
}"
4373,"@Override public void init(){
  try {
    if (useNativeLook())     UIManager.setLookAndFeel(UIManager.getSystemLookAndFeelClassName());
  }
 catch (  Exception ex) {
    throw new RuntimeException(ex);
  }
}","@Override public void init(){
  try {
    if (useNativeLook())     UIManager.setLookAndFeel(UIManager.getSystemLookAndFeelClassName());
  }
 catch (  Exception ex) {
    throw new AndroidScreenCastRuntimeException(ex);
  }
}"
4374,"@Override public void handleException(Thread thread,Throwable ex){
  try {
    StringWriter sw=new StringWriter();
    ex.printStackTrace(new PrintWriter(sw));
    if (sw.toString().contains(""String_Node_Str""))     return;
    ex.printStackTrace(System.err);
    if (jd != null && jd.isVisible())     return;
    jd=new JDialogError(ex);
    SwingUtilities.invokeLater(() -> jd.setVisible(true));
  }
 catch (  Exception ex2) {
  }
}","@Override public void handleException(Thread thread,Throwable ex){
  try {
    StringWriter sw=new StringWriter();
    ex.printStackTrace(new PrintWriter(sw));
    if (sw.toString().contains(""String_Node_Str""))     return;
    ex.printStackTrace(System.err);
    if (jd != null && jd.isVisible())     return;
    jd=new JDialogError(ex);
    SwingUtilities.invokeLater(() -> jd.setVisible(true));
  }
 catch (  Exception ignored) {
  }
}"
4375,"private void setErrorDetails(Throwable ex){
  errorDialogLabel.setText(ex.getClass().getSimpleName());
  if (ex.getClass() == RuntimeException.class && ex.getCause() != null)   ex=ex.getCause();
  try (StringWriter stringWriter=new StringWriter()){
    AndroidScreenCastRuntimeException realCause=getCause(ex);
    if (realCause != null) {
      errorDialogLabel.setText(realCause.getClass().getSimpleName());
      stringWriter.append(realCause.getMessage()).append('\n').append('\n');
      stringWriter.append(realCause.getAdditionalInformation());
    }
 else {
      stringWriter.append(ex.getMessage()).append('\n').append('\n');
      ex.printStackTrace(new PrintWriter(stringWriter));
    }
    errorDescription.setText(stringWriter.toString());
  }
 catch (  IOException e) {
    throw new RuntimeException(e);
  }
}","private void setErrorDetails(Throwable e){
  Throwable ex=getRealException(e);
  errorDialogLabel.setText(ex.getClass().getSimpleName());
  try (StringWriter stringWriter=new StringWriter()){
    AndroidScreenCastRuntimeException realCause=getCause(ex);
    if (realCause != null) {
      errorDialogLabel.setText(realCause.getClass().getSimpleName());
      stringWriter.append(realCause.getMessage()).append('\n').append('\n');
      stringWriter.append(realCause.getAdditionalInformation());
    }
 else {
      stringWriter.append(ex.getMessage()).append('\n').append('\n');
      ex.printStackTrace(new PrintWriter(stringWriter));
    }
    errorDescription.setText(stringWriter.toString());
  }
 catch (  IOException ioe) {
    throw new IORuntimeException(ioe);
  }
}"
4376,"private void initComponents(Throwable ex){
  errorDialogLabel=new JLabel();
  scrollPane=new JScrollPane();
  errorDescription=new JTextArea();
  errorDescription.setLineWrap(true);
  errorDescription.setWrapStyleWord(true);
  Container contentPane=getContentPane();
  contentPane.setLayout(new BorderLayout(5,5));
  errorDialogLabel.setText(ex.getClass().getSimpleName());
  errorDialogLabel.setLabelFor(errorDescription);
  errorDialogLabel.setHorizontalAlignment(SwingConstants.CENTER);
  contentPane.add(errorDialogLabel,BorderLayout.NORTH);
  setErrorDetails(ex);
  scrollPane.setViewportView(errorDescription);
  contentPane.add(scrollPane,BorderLayout.CENTER);
  pack();
  Dimension screenSize=Toolkit.getDefaultToolkit().getScreenSize();
  setSize((int)screenSize.getWidth() >> 1,(int)screenSize.getHeight() >> 1);
  setLocationRelativeTo(null);
  setAlwaysOnTop(true);
}","private void initComponents(Throwable ex){
  errorDialogLabel=new JLabel();
  JScrollPane scrollPane=new JScrollPane();
  errorDescription=new JTextArea();
  errorDescription.setLineWrap(true);
  errorDescription.setWrapStyleWord(true);
  Container contentPane=getContentPane();
  contentPane.setLayout(new BorderLayout(5,5));
  errorDialogLabel.setText(ex.getClass().getSimpleName());
  errorDialogLabel.setLabelFor(errorDescription);
  errorDialogLabel.setHorizontalAlignment(SwingConstants.CENTER);
  contentPane.add(errorDialogLabel,BorderLayout.NORTH);
  setErrorDetails(ex);
  scrollPane.setViewportView(errorDescription);
  contentPane.add(scrollPane,BorderLayout.CENTER);
  pack();
  Dimension screenSize=Toolkit.getDefaultToolkit().getScreenSize();
  setSize((int)screenSize.getWidth() >> 1,(int)screenSize.getHeight() >> 1);
  setLocationRelativeTo(null);
  setAlwaysOnTop(true);
}"
4377,"private void launchFile(FileInfo node){
  try {
    File tempFile=node.downloadTemporary();
    Desktop.getDesktop().open(tempFile);
  }
 catch (  Exception ex) {
    throw new RuntimeException(ex);
  }
}","private void launchFile(FileInfo node){
  try {
    File tempFile=node.downloadTemporary();
    Desktop.getDesktop().open(tempFile);
  }
 catch (  Exception ex) {
    throw new AndroidScreenCastRuntimeException(ex);
  }
}"
4378,"public void launch(){
  jt.setModel(new DefaultTreeModel(new FolderTreeNode(""String_Node_Str"",""String_Node_Str"")));
  jt.setRootVisible(true);
  jt.addTreeSelectionListener(treeSelectionEvent -> {
    TreePath tp=treeSelectionEvent.getPath();
    if (tp == null)     return;
    if (!(tp.getLastPathComponent() instanceof FolderTreeNode))     return;
    FolderTreeNode node=(FolderTreeNode)tp.getLastPathComponent();
    displayFolder(node.path);
  }
);
  JScrollPane jsp=new JScrollPane(jt);
  jListFichiers=new JList<>();
  jListFichiers.setListData(new Object[]{});
  jSplitPane=new JSplitPane(JSplitPane.HORIZONTAL_SPLIT,jsp,new JScrollPane(jListFichiers));
  add(jSplitPane,BorderLayout.CENTER);
  setSize(640,480);
  setLocationRelativeTo(null);
  jListFichiers.addMouseListener(new MouseAdapter(){
    @Override public void mouseClicked(    MouseEvent e){
      if (e.getClickCount() == 2) {
        int index=jListFichiers.locationToIndex(e.getPoint());
        ListModel<Object> dlm=jListFichiers.getModel();
        FileInfo item=(FileInfo)dlm.getElementAt(index);
        launchFile(item);
      }
    }
  }
);
}","public void launch(){
  jt.setModel(new DefaultTreeModel(new FolderTreeNode(""String_Node_Str"",""String_Node_Str"")));
  jt.setRootVisible(true);
  jt.addTreeSelectionListener(treeSelectionEvent -> {
    TreePath tp=treeSelectionEvent.getPath();
    if (tp == null)     return;
    if (!(tp.getLastPathComponent() instanceof FolderTreeNode))     return;
    FolderTreeNode node=(FolderTreeNode)tp.getLastPathComponent();
    displayFolder(node.path);
  }
);
  JScrollPane jsp=new JScrollPane(jt);
  jListFichiers=new JList<>();
  jListFichiers.setListData(new Object[]{});
  JSplitPane jSplitPane=new JSplitPane(JSplitPane.HORIZONTAL_SPLIT,jsp,new JScrollPane(jListFichiers));
  add(jSplitPane,BorderLayout.CENTER);
  setSize(640,480);
  setLocationRelativeTo(null);
  jListFichiers.addMouseListener(new MouseAdapter(){
    @Override public void mouseClicked(    MouseEvent e){
      if (e.getClickCount() == 2) {
        int index=jListFichiers.locationToIndex(e.getPoint());
        ListModel<Object> dlm=jListFichiers.getModel();
        FileInfo item=(FileInfo)dlm.getElementAt(index);
        launchFile(item);
      }
    }
  }
);
}"
4379,"@PreDestroy private void cleanUp(){
  AndroidDebugBridge.disconnectBridge();
  AndroidDebugBridge.terminate();
}","@PreDestroy void cleanUp(){
  AndroidDebugBridge.disconnectBridge();
  AndroidDebugBridge.terminate();
}"
4380,"@Override public void stop(){
}","@Override public void stop(){
  bridge.stop();
}"
4381,"@Override public void start(){
  LOGGER.info(""String_Node_Str"");
  try {
    waitDeviceList(bridge);
    final IDevice devices[]=bridge.getDevices();
    if (devices.length == 1) {
      device=devices[0];
      LOGGER.info(""String_Node_Str"");
    }
 else {
      final JDialogDeviceList jd=new JDialogDeviceList(devices);
      jd.setVisible(true);
      device=jd.getDevice();
      LOGGER.info(""String_Node_Str"",devices.length);
    }
    if (device == null) {
      throw new NoDeviceChosenException();
    }
  }
 catch (  final Throwable e) {
    bridge.stop();
    throw e;
  }
  LOGGER.info(""String_Node_Str"",device.getName());
}","@Override public void start(){
  LOGGER.info(""String_Node_Str"");
  waitDeviceList(bridge);
  final IDevice devices[]=bridge.getDevices();
  if (devices.length == 1) {
    device=devices[0];
    LOGGER.info(""String_Node_Str"");
  }
 else {
    final JDialogDeviceList jd=new JDialogDeviceList(devices);
    jd.setVisible(true);
    device=jd.getDevice();
    LOGGER.info(""String_Node_Str"",devices.length);
  }
  if (device == null) {
    throw new NoDeviceChosenException();
  }
  LOGGER.info(""String_Node_Str"",device.getName());
}"
4382,"GUIApplication(){
  Runtime.getRuntime().addShutdownHook(new Thread(GUIApplication.this::stop));
  Thread.setDefaultUncaughtExceptionHandler((thread,ex) -> {
    try {
      handleException(thread,ex);
    }
 catch (    final Exception ex2) {
      LOGGER.error(""String_Node_Str"",ex2);
    }
  }
);
}","GUIApplication(){
  Runtime.getRuntime().addShutdownHook(new Thread(this::stop));
  Thread.setDefaultUncaughtExceptionHandler((thread,ex) -> {
    try {
      handleException(thread,ex);
    }
 catch (    final Exception ex2) {
      LOGGER.error(""String_Node_Str"",ex2);
    }
  }
);
}"
4383,"@Singleton @Provides public static IDevice iDevice(final DeviceChooserApplication application){
  application.init();
  application.start();
  application.stop();
  IDevice device=application.getDevice();
  return device;
}","@Singleton @Provides public static IDevice iDevice(final DeviceChooserApplication application){
  try {
    application.init();
    application.start();
    IDevice device=application.getDevice();
    return device;
  }
 catch (  Throwable e) {
    application.stop();
    throw e;
  }
}"
4384,"public void initialize(){
  setDefaultCloseOperation(DISPOSE_ON_CLOSE);
  KeyboardFocusManager.getCurrentKeyboardFocusManager().addKeyEventDispatcher(KeyEventDispatcherFactory.getKeyEventDispatcher(this));
  jtb.setFocusable(false);
  jbExplorer.setFocusable(false);
  jbKbHome.setFocusable(false);
  jbKbMenu.setFocusable(false);
  jbKbBack.setFocusable(false);
  jbKbSearch.setFocusable(false);
  jbKbPhoneOn.setFocusable(false);
  jbKbPhoneOff.setFocusable(false);
  jbExecuteKeyEvent.setFocusable(false);
  jbRecord.setFocusable(false);
  jbKbHome.addActionListener(KeyboardActionListenerFactory.getInstance(InputKeyEvent.KEYCODE_HOME));
  jbKbMenu.addActionListener(KeyboardActionListenerFactory.getInstance(InputKeyEvent.KEYCODE_MENU));
  jbKbBack.addActionListener(KeyboardActionListenerFactory.getInstance(InputKeyEvent.KEYCODE_BACK));
  jbKbSearch.addActionListener(KeyboardActionListenerFactory.getInstance(InputKeyEvent.KEYCODE_SEARCH));
  jbKbPhoneOn.addActionListener(KeyboardActionListenerFactory.getInstance(InputKeyEvent.KEYCODE_CALL));
  jbKbPhoneOff.addActionListener(KeyboardActionListenerFactory.getInstance(InputKeyEvent.KEYCODE_ENDCALL));
  jbRecord.addActionListener(createRecordActionListener());
  jtbHardkeys.add(jbKbHome);
  jtbHardkeys.add(jbKbMenu);
  jtbHardkeys.add(jbKbBack);
  jtbHardkeys.add(jbKbSearch);
  jtbHardkeys.add(jbKbPhoneOn);
  jtbHardkeys.add(jbKbPhoneOff);
  setDefaultCloseOperation(DISPOSE_ON_CLOSE);
  setLayout(new BorderLayout());
  add(jtb,BorderLayout.NORTH);
  add(jtbHardkeys,BorderLayout.SOUTH);
  jsp=new JScrollPane(jp);
  add(jsp,BorderLayout.CENTER);
  jsp.setPreferredSize(new Dimension(100,100));
  pack();
  setLocationRelativeTo(null);
  setPreferredWindowSize();
  jp.addMouseMotionListener(ma);
  jp.addMouseListener(ma);
  jp.addMouseWheelListener(ma);
  jbExplorer.addActionListener(actionEvent -> {
    SwingUtilities.invokeLater(() -> {
      JFrameExplorer jf=frameExplorer;
      jf.setIconImage(getIconImage());
      jf.launch();
      jf.setVisible(true);
    }
);
  }
);
  jtb.add(jbExplorer);
  jbExecuteKeyEvent.addActionListener(actionEvent -> {
    SwingUtilities.invokeLater(dialogExecuteKeyEvent::open);
  }
);
  jtb.add(jbExecuteKeyEvent);
  jtb.add(jbRecord);
}","public void initialize(){
  setDefaultCloseOperation(DISPOSE_ON_CLOSE);
  KeyboardFocusManager.getCurrentKeyboardFocusManager().addKeyEventDispatcher(KeyEventDispatcherFactory.getKeyEventDispatcher(this));
  jtb.setFocusable(false);
  jbExplorer.setFocusable(false);
  jbKbHome.setFocusable(false);
  jbKbMenu.setFocusable(false);
  jbKbBack.setFocusable(false);
  jbKbSearch.setFocusable(false);
  jbKbPhoneOn.setFocusable(false);
  jbKbPhoneOff.setFocusable(false);
  jbExecuteKeyEvent.setFocusable(false);
  jbRecord.setFocusable(false);
  jbKbHome.addActionListener(KeyboardActionListenerFactory.getInstance(InputKeyEvent.KEYCODE_HOME));
  jbKbMenu.addActionListener(KeyboardActionListenerFactory.getInstance(InputKeyEvent.KEYCODE_MENU));
  jbKbBack.addActionListener(KeyboardActionListenerFactory.getInstance(InputKeyEvent.KEYCODE_BACK));
  jbKbSearch.addActionListener(KeyboardActionListenerFactory.getInstance(InputKeyEvent.KEYCODE_SEARCH));
  jbKbPhoneOn.addActionListener(KeyboardActionListenerFactory.getInstance(InputKeyEvent.KEYCODE_CALL));
  jbKbPhoneOff.addActionListener(KeyboardActionListenerFactory.getInstance(InputKeyEvent.KEYCODE_ENDCALL));
  jbRecord.addActionListener(createRecordActionListener());
  jtbHardkeys.add(jbKbHome);
  jtbHardkeys.add(jbKbMenu);
  jtbHardkeys.add(jbKbBack);
  jtbHardkeys.add(jbKbSearch);
  jtbHardkeys.add(jbKbPhoneOn);
  jtbHardkeys.add(jbKbPhoneOff);
  setLayout(new BorderLayout());
  add(jtb,BorderLayout.NORTH);
  add(jtbHardkeys,BorderLayout.SOUTH);
  jsp=new JScrollPane(jp);
  add(jsp,BorderLayout.CENTER);
  jsp.setPreferredSize(new Dimension(100,100));
  pack();
  setLocationRelativeTo(null);
  setPreferredWindowSize();
  jp.addMouseMotionListener(ma);
  jp.addMouseListener(ma);
  jp.addMouseWheelListener(ma);
  jbExplorer.addActionListener(actionEvent -> {
    SwingUtilities.invokeLater(() -> {
      JFrameExplorer jf=frameExplorer;
      jf.setIconImage(getIconImage());
      jf.launch();
      jf.setVisible(true);
    }
);
  }
);
  jtb.add(jbExplorer);
  jbExecuteKeyEvent.addActionListener(actionEvent -> {
    SwingUtilities.invokeLater(dialogExecuteKeyEvent::open);
  }
);
  jtb.add(jbExecuteKeyEvent);
  jtb.add(jbRecord);
}"
4385,"@Override protected void paintComponent(final Graphics g){
  if (isNotInitialized())   return;
  Graphics2D g2=(Graphics2D)g;
  g2.clearRect(0,0,getWidth(),getHeight());
  double width=Math.min(getWidth(),size.width * getHeight() / size.height);
  coef=(float)width / size.width;
  double height=width * size.height / size.width;
  origX=(getWidth() - width) / 2;
  origY=(getHeight() - height) / 2;
  g2.drawImage(image,(int)origX,(int)origY,(int)width,(int)height,this);
}","@Override protected void paintComponent(final Graphics g){
  if (isNotInitialized())   return;
  final Graphics2D g2=(Graphics2D)g;
  g2.clearRect(0,0,getWidth(),getHeight());
  final double width=Math.min(getWidth(),size.width * getHeight() / size.height);
  coef=(float)width / size.width;
  final double height=width * size.height / size.width;
  origX=(getWidth() - width) / 2;
  origY=(getHeight() - height) / 2;
  g2.drawImage(image,(int)origX,(int)origY,(int)width,(int)height,this);
}"
4386,"@Inject public JPanelScreen(){
  this.setFocusable(true);
}","@Inject public JPanelScreen(){
  setFocusable(true);
}"
4387,"public static <T>Observable<ObservableSet<T>> fromObservableSet(final ObservableSet<T> source){
  return Observable.create((Observable.OnSubscribe<ObservableSet<T>>)subscriber -> {
    SetChangeListener<T> listener=c -> subscriber.onNext(source);
    subscriber.add(JavaFxSubscriptions.unsubscribeInEventDispatchThread(() -> source.removeListener(listener)));
  }
).startWith(source).subscribeOn(JavaFxScheduler.getInstance());
}","public static <T>Observable<ObservableSet<T>> fromObservableSet(final ObservableSet<T> source){
  return Observable.create((Observable.OnSubscribe<ObservableSet<T>>)subscriber -> {
    SetChangeListener<T> listener=c -> subscriber.onNext(source);
    source.addListener(listener);
    subscriber.add(JavaFxSubscriptions.unsubscribeInEventDispatchThread(() -> source.removeListener(listener)));
  }
).startWith(source).subscribeOn(JavaFxScheduler.getInstance());
}"
4388,"@BeforeClass public static void initJFX(){
  Thread t=new Thread(""String_Node_Str""){
    public void run(){
      Application.launch(AsNonApp.class,new String[0]);
    }
  }
;
  t.setDaemon(true);
  t.start();
}","@BeforeClass public static void initJFX(){
  JFXPanel panel=new JFXPanel();
}"
4389,"@Override public void execute() throws Exception {
  if (!ElasticsearchProcessMonitor.isElasticsearchRunning()) {
    String exceptionMsg=""String_Node_Str"";
    logger.info(exceptionMsg);
    return;
  }
  if (config.reportMetricsFromMasterOnly() && EsUtils.amIMasterNode(config,httpModule)) {
    return;
  }
  HealthBean healthBean=new HealthBean();
  try {
    Client esTransportClient=ESTransportClient.instance(config).getTransportClient();
    ClusterHealthStatus clusterHealthStatus=esTransportClient.admin().cluster().prepareHealth().setTimeout(MASTER_NODE_TIMEOUT).execute().get().getStatus();
    ClusterHealthResponse clusterHealthResponse=esTransportClient.admin().cluster().prepareHealth().execute().actionGet(MASTER_NODE_TIMEOUT);
    if (clusterHealthStatus == null) {
      logger.info(""String_Node_Str"");
      resetHealthStats(healthBean);
      return;
    }
    if (clusterHealthStatus.name().equalsIgnoreCase(""String_Node_Str"")) {
      healthBean.greenorredstatus=0;
      healthBean.greenoryellowstatus=0;
    }
 else     if (clusterHealthStatus.name().equalsIgnoreCase(""String_Node_Str"")) {
      healthBean.greenoryellowstatus=1;
      healthBean.greenorredstatus=0;
    }
 else     if (clusterHealthStatus.name().equalsIgnoreCase(""String_Node_Str"")) {
      healthBean.greenorredstatus=1;
      healthBean.greenoryellowstatus=0;
    }
    if (config.isNodeMismatchWithDiscoveryEnabled())     healthBean.nodematch=(clusterHealthResponse.getNumberOfNodes() == instanceManager.getAllInstances().size()) ? 0 : 1;
 else     healthBean.nodematch=(clusterHealthResponse.getNumberOfNodes() == config.getDesiredNumberOfNodesInCluster()) ? 0 : 1;
    if (config.isEurekaHealthCheckEnabled())     healthBean.eurekanodematch=(clusterHealthResponse.getNumberOfNodes() == discoveryClient.getApplication(config.getAppName()).getInstances().size()) ? 0 : 1;
  }
 catch (  Exception e) {
    resetHealthStats(healthBean);
    logger.warn(""String_Node_Str"",e);
  }
  healthReporter.healthBean.set(healthBean);
}","@Override public void execute() throws Exception {
  if (!ElasticsearchProcessMonitor.isElasticsearchRunning()) {
    String exceptionMsg=""String_Node_Str"";
    logger.info(exceptionMsg);
    return;
  }
  if (config.reportMetricsFromMasterOnly() && !EsUtils.amIMasterNode(config,httpModule)) {
    return;
  }
  HealthBean healthBean=new HealthBean();
  try {
    Client esTransportClient=ESTransportClient.instance(config).getTransportClient();
    ClusterHealthStatus clusterHealthStatus=esTransportClient.admin().cluster().prepareHealth().setTimeout(MASTER_NODE_TIMEOUT).execute().get().getStatus();
    ClusterHealthResponse clusterHealthResponse=esTransportClient.admin().cluster().prepareHealth().execute().actionGet(MASTER_NODE_TIMEOUT);
    if (clusterHealthStatus == null) {
      logger.info(""String_Node_Str"");
      resetHealthStats(healthBean);
      return;
    }
    if (clusterHealthStatus.name().equalsIgnoreCase(""String_Node_Str"")) {
      healthBean.greenorredstatus=0;
      healthBean.greenoryellowstatus=0;
    }
 else     if (clusterHealthStatus.name().equalsIgnoreCase(""String_Node_Str"")) {
      healthBean.greenoryellowstatus=1;
      healthBean.greenorredstatus=0;
    }
 else     if (clusterHealthStatus.name().equalsIgnoreCase(""String_Node_Str"")) {
      healthBean.greenorredstatus=1;
      healthBean.greenoryellowstatus=0;
    }
    if (config.isNodeMismatchWithDiscoveryEnabled())     healthBean.nodematch=(clusterHealthResponse.getNumberOfNodes() == instanceManager.getAllInstances().size()) ? 0 : 1;
 else     healthBean.nodematch=(clusterHealthResponse.getNumberOfNodes() == config.getDesiredNumberOfNodesInCluster()) ? 0 : 1;
    if (config.isEurekaHealthCheckEnabled())     healthBean.eurekanodematch=(clusterHealthResponse.getNumberOfNodes() == discoveryClient.getApplication(config.getAppName()).getInstances().size()) ? 0 : 1;
  }
 catch (  Exception e) {
    resetHealthStats(healthBean);
    logger.warn(""String_Node_Str"",e);
  }
  healthReporter.healthBean.set(healthBean);
}"
4390,"@Override public View onCreateView(LayoutInflater inflater,ViewGroup container,Bundle savedInstanceState){
  View v=inflater.inflate(R.layout.fragment_general_annotation,container,false);
  Injector.injectInto(this,v);
  permissionGuard=new PermissionGuard(mContext,this);
  initData();
  return v;
}","@Override public View onCreateView(LayoutInflater inflater,ViewGroup container,Bundle savedInstanceState){
  View v=inflater.inflate(R.layout.fragment_general_annotation,container,false);
  Injector.injectInto(this,v);
  initData();
  return v;
}"
4391,"private void initViews(){
  initFab();
  initToolbar();
  navigationView.setNavigationItemSelectedListener(new NavigationView.OnNavigationItemSelectedListener(){
    @Override public boolean onNavigationItemSelected(    MenuItem menuItem){
      showMenu(menuItem);
      Snackbar.make(content,menuItem.getTitle() + ""String_Node_Str"",Snackbar.LENGTH_LONG).show();
      menuItem.setChecked(true);
      drawerLayout.closeDrawers();
      return true;
    }
  }
);
}","private void initViews(){
  initFab();
  initToolbar();
  navigationView.setNavigationItemSelectedListener(new NavigationView.OnNavigationItemSelectedListener(){
    @Override public boolean onNavigationItemSelected(    MenuItem menuItem){
      showMenu(menuItem);
      toolbar.setTitle(menuItem.getTitle());
      Snackbar.make(content,menuItem.getTitle() + ""String_Node_Str"",Snackbar.LENGTH_LONG).show();
      menuItem.setChecked(true);
      drawerLayout.closeDrawers();
      return true;
    }
  }
);
}"
4392,"@Override public boolean onNavigationItemSelected(MenuItem menuItem){
  showMenu(menuItem);
  Snackbar.make(content,menuItem.getTitle() + ""String_Node_Str"",Snackbar.LENGTH_LONG).show();
  menuItem.setChecked(true);
  drawerLayout.closeDrawers();
  return true;
}","@Override public boolean onNavigationItemSelected(MenuItem menuItem){
  showMenu(menuItem);
  toolbar.setTitle(menuItem.getTitle());
  Snackbar.make(content,menuItem.getTitle() + ""String_Node_Str"",Snackbar.LENGTH_LONG).show();
  menuItem.setChecked(true);
  drawerLayout.closeDrawers();
  return true;
}"
4393,"private void initData(){
  data=new ArrayList<String>();
  data.add(""String_Node_Str"");
  data.add(""String_Node_Str"");
  data.add(""String_Node_Str"");
  data.add(""String_Node_Str"");
  data.add(""String_Node_Str"");
  data.add(""String_Node_Str"");
  adapter=new AnnoAdapter(mContext,data);
  listview.setAdapter(adapter);
  listview.setOnClickListener(new View.OnClickListener(){
    @Override public void onClick(    View v){
    }
  }
);
}","private void initData(){
  data=new ArrayList<String>();
  data.add(""String_Node_Str"");
  data.add(""String_Node_Str"");
  data.add(""String_Node_Str"");
  data.add(""String_Node_Str"");
  data.add(""String_Node_Str"");
  data.add(""String_Node_Str"");
  adapter=new AnnoAdapter(mContext,data);
  listview.setAdapter(adapter);
}"
4394,"@TargetApi(Build.VERSION_CODES.ICE_CREAM_SANDWICH) private boolean putDiskCache(String key,Bitmap bitmap){
  if (bitmap == null)   return false;
  OutputStream out=null;
  String ekey=toMD5(key);
  DiskLruCache.Snapshot snapshot=null;
  try {
    snapshot=mCache.get(ekey);
    if (snapshot == null) {
      DiskLruCache.Editor editor=mCache.edit(ekey);
      if (editor == null)       return false;
      out=new BufferedOutputStream(editor.newOutputStream(0),IO_BUFFER_SIZE);
      Bitmap.CompressFormat format;
      if (key.equals(""String_Node_Str"") || key.endsWith(""String_Node_Str"")) {
        format=Bitmap.CompressFormat.PNG;
      }
 else       if (SAFUtils.isICSOrHigher() && key.equals(""String_Node_Str"")) {
        format=Bitmap.CompressFormat.WEBP;
      }
 else {
        format=Bitmap.CompressFormat.JPEG;
      }
      bitmap.compress(format,IMAGE_QUANLITY,out);
      editor.commit();
      mCache.flush();
      out.close();
    }
 else {
      snapshot.getInputStream(0).close();
    }
  }
 catch (  IOException e) {
    e.printStackTrace();
  }
 finally {
    IOUtils.closeQuietly(out);
  }
  return true;
}","@TargetApi(Build.VERSION_CODES.ICE_CREAM_SANDWICH) private boolean putDiskCache(String key,Bitmap bitmap){
  if (bitmap == null)   return false;
  OutputStream out=null;
  String ekey=toMD5(key);
  DiskLruCache.Snapshot snapshot=null;
  try {
    snapshot=mCache.get(ekey);
    if (snapshot == null) {
      DiskLruCache.Editor editor=mCache.edit(ekey);
      if (editor == null)       return false;
      out=new BufferedOutputStream(editor.newOutputStream(0),IO_BUFFER_SIZE);
      Bitmap.CompressFormat format;
      if (key.endsWith(""String_Node_Str"") || key.endsWith(""String_Node_Str"")) {
        format=Bitmap.CompressFormat.PNG;
      }
 else       if (SAFUtils.isICSOrHigher() && key.endsWith(""String_Node_Str"")) {
        format=Bitmap.CompressFormat.WEBP;
      }
 else {
        format=Bitmap.CompressFormat.JPEG;
      }
      bitmap.compress(format,IMAGE_QUANLITY,out);
      editor.commit();
      mCache.flush();
      out.close();
    }
 else {
      snapshot.getInputStream(0).close();
    }
  }
 catch (  IOException e) {
    e.printStackTrace();
  }
 finally {
    IOUtils.closeQuietly(out);
  }
  return true;
}"
4395,"private void injectFields(Finder finder){
  Field[] fields=clazz.getDeclaredFields();
  for (  Field field : fields) {
    Annotation[] annotations=field.getAnnotations();
    View view=null;
    for (    Annotation annotation : annotations) {
      if (annotation.annotationType() == InjectView.class) {
        int id=((InjectView)annotation).id();
        view=findViewByAnnotationId(id,field,finder);
        if (view == null) {
          throw new InjectException(""String_Node_Str"" + field.getName());
        }
        injectIntoField(field,view);
      }
 else       if (annotation.annotationType() == InjectViews.class) {
        String fieldTypeName=field.getType().getName();
        if (fieldTypeName.startsWith(""String_Node_Str"") || fieldTypeName.startsWith(""String_Node_Str"")) {
          int[] ids=((InjectViews)annotation).ids();
          List<View> views=new ArrayList<View>();
          for (          int id : ids) {
            view=findViewByAnnotationId(id,field,finder);
            if (view == null) {
              throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str""+ id);
            }
            views.add(view);
          }
          if (fieldTypeName.startsWith(""String_Node_Str"")) {
            View[] v=(View[])Array.newInstance(field.getType().getComponentType(),views.size());
            injectListIntoField(field,views.toArray(v));
          }
 else           if (fieldTypeName.startsWith(""String_Node_Str"")) {
            injectIntoField(field,views);
          }
        }
 else {
          throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str"");
        }
      }
 else       if (annotation.annotationType() == InjectResource.class) {
        Object ressource=findResource(field.getType(),field,(InjectResource)annotation);
        injectIntoField(field,ressource);
      }
 else       if (annotation.annotationType() == InjectSystemService.class) {
        String serviceName=((InjectSystemService)annotation).value();
        Object service=context.getSystemService(serviceName);
        injectIntoField(field,service);
      }
 else       if (annotation.annotationType() == InjectExtra.class) {
        if (extras != null) {
          Object value=extras.get(((InjectExtra)annotation).key());
          if (value == null) {
            if (field.getType().getName().equals(java.lang.Integer.class.getName()) || field.getType().getName().equals(""String_Node_Str"")) {
              value=((InjectExtra)annotation).defaultInt();
            }
 else             if (field.getType().getName().equals(java.lang.Boolean.class.getName()) || field.getType().getName().equals(""String_Node_Str"")) {
              value=((InjectExtra)annotation).defaultBoolean();
            }
 else             if (field.getType().getName().equals(java.lang.String.class.getName())) {
              value=((InjectExtra)annotation).defaultString();
            }
          }
          injectIntoField(field,value);
        }
 else {
          Object value=null;
          if (field.getType().getName().equals(java.lang.Integer.class.getName()) || field.getType().getName().equals(""String_Node_Str"")) {
            value=((InjectExtra)annotation).defaultInt();
          }
 else           if (field.getType().getName().equals(java.lang.Boolean.class.getName()) || field.getType().getName().equals(""String_Node_Str"")) {
            value=((InjectExtra)annotation).defaultBoolean();
          }
 else           if (field.getType().getName().equals(java.lang.String.class.getName())) {
            value=((InjectExtra)annotation).defaultString();
          }
          if (value != null) {
            injectIntoField(field,value);
          }
        }
      }
 else       if (annotation.annotationType() == InjectSupportFragment.class) {
        int id=((InjectSupportFragment)annotation).id();
        Fragment fragment=findSupportFragment(field,id);
        injectIntoField(field,fragment);
      }
    }
  }
}","private void injectFields(Finder finder){
  Field[] fields=clazz.getDeclaredFields();
  for (  Field field : fields) {
    Annotation[] annotations=field.getAnnotations();
    View view=null;
    for (    Annotation annotation : annotations) {
      if (annotation.annotationType() == InjectView.class) {
        int id=((InjectView)annotation).id();
        view=findViewByAnnotationId(id,field,finder);
        if (view == null) {
          throw new InjectException(""String_Node_Str"" + field.getName());
        }
        injectIntoField(field,view);
      }
 else       if (annotation.annotationType() == InjectViews.class) {
        String fieldTypeName=field.getType().getName();
        if (fieldTypeName.startsWith(""String_Node_Str"") || fieldTypeName.startsWith(""String_Node_Str"")) {
          int[] ids=((InjectViews)annotation).ids();
          List<View> views=new ArrayList<View>();
          for (          int id : ids) {
            view=findViewByAnnotationId(id,field,finder);
            if (view == null) {
              throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str""+ id);
            }
            views.add(view);
          }
          if (fieldTypeName.startsWith(""String_Node_Str"")) {
            View[] v=(View[])Array.newInstance(field.getType().getComponentType(),views.size());
            injectListIntoField(field,views.toArray(v));
          }
 else           if (fieldTypeName.startsWith(""String_Node_Str"")) {
            injectIntoField(field,views);
          }
        }
 else {
          throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str"");
        }
      }
 else       if (annotation.annotationType() == InjectResource.class) {
        Object ressource=findResource(field.getType(),field,(InjectResource)annotation);
        injectIntoField(field,ressource);
      }
 else       if (annotation.annotationType() == InjectSystemService.class) {
        String serviceName=((InjectSystemService)annotation).value();
        Object service=context.getSystemService(serviceName);
        injectIntoField(field,service);
      }
 else       if (annotation.annotationType() == InjectExtra.class) {
        String extraKey=((InjectExtra)annotation).key();
        if (StringUtils.isBlank(extraKey)) {
          extraKey=field.getName();
        }
        setInjectExtra(field,extras,extraKey,annotation);
      }
 else       if (annotation.annotationType() == InjectSupportFragment.class) {
        int id=((InjectSupportFragment)annotation).id();
        Fragment fragment=findSupportFragment(field,id);
        injectIntoField(field,fragment);
      }
    }
  }
}"
4396,"/** 
 * TODO frankswu : 对activity和fragment增加缓存
 * @param source
 * @param id
 * @return
 */
protected View findViewById(Object source,int id){
  View view=viewHandlerMap.get(source.getClass());
  if (view == null) {
    if (source instanceof Activity) {
      return ((Activity)source).findViewById(id);
    }
 else {
      return ((View)source).findViewById(id);
    }
  }
  return view;
}","/** 
 * TODO frankswu : 对activity和fragment增加缓存
 * @param source
 * @param id
 * @return
 */
protected View findViewById(Object source,int id){
  String key=source.getClass() + ""String_Node_Str"" + id;
  View view=viewHandlerMap.get(key);
  if (view == null) {
    if (source instanceof Activity) {
      view=((Activity)source).findViewById(id);
    }
 else {
      view=((View)source).findViewById(id);
    }
    viewHandlerMap.put(key,view);
  }
  return view;
}"
4397,"private boolean bindOnClickListener(final Method method,OnClick onClick,Set<View> modifiedViews,Finder finder){
  boolean invokeWithView=checkInvokeWithView(method,new Class[]{View.class});
  method.setAccessible(true);
  InjectedOnClickListener listener=new InjectedOnClickListener(target,method,invokeWithView);
  int[] ids=onClick.id();
  for (  int id : ids) {
    if (id != 0) {
      View view=findView(method,id,finder);
      view.setOnClickListener(listener);
    }
  }
  return invokeWithView;
}","private boolean bindOnClickListener(final Method method,OnClick onClick,Set<View> modifiedViews,Finder finder){
  boolean invokeWithView=checkInvokeWithView(method,new Class[]{View.class});
  method.setAccessible(true);
  InjectedOnClickListener listener=new InjectedOnClickListener(target,method,invokeWithView);
  int[] ids=onClick.id();
  for (  int id : ids) {
    if (id != 0) {
      View view=findView(method,id,finder);
      if (view != null) {
        view.setOnClickListener(listener);
      }
    }
  }
  return invokeWithView;
}"
4398,"private void injectFields(Finder finder){
  Field[] fields=clazz.getDeclaredFields();
  for (  Field field : fields) {
    Annotation[] annotations=field.getAnnotations();
    View view=null;
    for (    Annotation annotation : annotations) {
      if (annotation.annotationType() == InjectView.class) {
        int id=((InjectView)annotation).id();
        view=findViewByAnnotationId(id,field,finder);
        if (view == null) {
          throw new InjectException(""String_Node_Str"" + field.getName());
        }
        injectIntoField(field,view);
      }
 else       if (annotation.annotationType() == InjectViews.class) {
        String fieldTypeName=field.getType().getName();
        if (fieldTypeName.startsWith(""String_Node_Str"") || fieldTypeName.startsWith(""String_Node_Str"")) {
          int[] ids=((InjectViews)annotation).ids();
          List<View> views=new ArrayList<View>();
          for (          int id : ids) {
            view=findViewByAnnotationId(id,field,finder);
            if (view == null) {
              throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str""+ id);
            }
            views.add(view);
          }
          if (fieldTypeName.startsWith(""String_Node_Str"")) {
            View[] v=(View[])Array.newInstance(field.getType().getComponentType(),views.size());
            injectListIntoField(field,views.toArray(v));
          }
          if (fieldTypeName.startsWith(""String_Node_Str"")) {
            injectIntoField(field,views);
          }
        }
 else {
          throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str"");
        }
      }
 else       if (annotation.annotationType() == InjectResource.class) {
        Object ressource=findResource(field.getType(),field,(InjectResource)annotation);
        injectIntoField(field,ressource);
      }
 else       if (annotation.annotationType() == InjectSystemService.class) {
        String serviceName=((InjectSystemService)annotation).value();
        Object service=context.getSystemService(serviceName);
        injectIntoField(field,service);
      }
 else       if (annotation.annotationType() == InjectExtra.class) {
        if (extras != null) {
          Object value=extras.get(((InjectExtra)annotation).key());
          injectIntoField(field,value);
        }
      }
 else       if (annotation.annotationType() == InjectSupportFragment.class) {
        int id=((InjectSupportFragment)annotation).id();
        Fragment fragment=findSupportFragment(field,id);
        injectIntoField(field,fragment);
      }
    }
  }
}","private void injectFields(Finder finder){
  Field[] fields=clazz.getDeclaredFields();
  for (  Field field : fields) {
    Annotation[] annotations=field.getAnnotations();
    View view=null;
    for (    Annotation annotation : annotations) {
      if (annotation.annotationType() == InjectView.class) {
        int id=((InjectView)annotation).id();
        view=findViewByAnnotationId(id,field,finder);
        if (view == null) {
          throw new InjectException(""String_Node_Str"" + field.getName());
        }
        injectIntoField(field,view);
      }
 else       if (annotation.annotationType() == InjectViews.class) {
        String fieldTypeName=field.getType().getName();
        if (fieldTypeName.startsWith(""String_Node_Str"") || fieldTypeName.startsWith(""String_Node_Str"")) {
          int[] ids=((InjectViews)annotation).ids();
          List<View> views=new ArrayList<View>();
          for (          int id : ids) {
            view=findViewByAnnotationId(id,field,finder);
            if (view == null) {
              throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str""+ id);
            }
            views.add(view);
          }
          if (fieldTypeName.startsWith(""String_Node_Str"")) {
            View[] v=(View[])Array.newInstance(field.getType().getComponentType(),views.size());
            injectListIntoField(field,views.toArray(v));
          }
 else           if (fieldTypeName.startsWith(""String_Node_Str"")) {
            injectIntoField(field,views);
          }
        }
 else {
          throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str"");
        }
      }
 else       if (annotation.annotationType() == InjectResource.class) {
        Object ressource=findResource(field.getType(),field,(InjectResource)annotation);
        injectIntoField(field,ressource);
      }
 else       if (annotation.annotationType() == InjectSystemService.class) {
        String serviceName=((InjectSystemService)annotation).value();
        Object service=context.getSystemService(serviceName);
        injectIntoField(field,service);
      }
 else       if (annotation.annotationType() == InjectExtra.class) {
        if (extras != null) {
          Object value=extras.get(((InjectExtra)annotation).key());
          injectIntoField(field,value);
        }
      }
 else       if (annotation.annotationType() == InjectSupportFragment.class) {
        int id=((InjectSupportFragment)annotation).id();
        Fragment fragment=findSupportFragment(field,id);
        injectIntoField(field,fragment);
      }
    }
  }
}"
4399,"/** 
 * @param method
 * @param annotation
 * @param modifiedViews
 * @param finder
 */
private boolean bindOnItemClickListener(Method method,OnItemClick onItemClick,Set<View> modifiedViews,Finder finder){
  boolean invokeWithView=checkInvokeWithView(method,new Class[]{AdapterView.class,View.class,int.class,long.class});
  method.setAccessible(true);
  InjectedOnItemClickListener listener=new InjectedOnItemClickListener(target,method,invokeWithView);
  int[] ids=onItemClick.id();
  for (  int id : ids) {
    if (id != 0) {
      AdapterView<?> view=null;
      try {
        view=(AdapterView<?>)findView(method,id,finder);
      }
 catch (      Exception e) {
        throw new InjectException(""String_Node_Str"");
      }
      view.setOnItemClickListener(listener);
    }
  }
  return invokeWithView;
}","/** 
 * @param method
 * @param annotation
 * @param modifiedViews
 * @param finder
 */
private boolean bindOnItemClickListener(Method method,OnItemClick onItemClick,Set<View> modifiedViews,Finder finder){
  boolean invokeWithView=checkInvokeWithView(method,new Class[]{AdapterView.class,View.class,int.class,long.class});
  method.setAccessible(true);
  InjectedOnItemClickListener listener=new InjectedOnItemClickListener(target,method,invokeWithView);
  int[] ids=onItemClick.id();
  for (  int id : ids) {
    if (id != 0) {
      AdapterView<?> view=null;
      try {
        view=(AdapterView<?>)findView(method,id,finder);
        if (view != null) {
          view.setOnItemClickListener(listener);
        }
      }
 catch (      Exception e) {
        throw new InjectException(""String_Node_Str"");
      }
    }
  }
  return invokeWithView;
}"
4400,"private boolean bindOnLongClickListener(Method method,OnLongClick onLongClick,Set<View> modifiedViews,Finder finder){
  boolean invokeWithView=checkInvokeWithView(method,new Class[]{View.class});
  method.setAccessible(true);
  InjectedOnLongClickListener listener=new InjectedOnLongClickListener(target,method,invokeWithView);
  int[] ids=onLongClick.id();
  for (  int id : ids) {
    if (id != 0) {
      View view=findView(method,id,finder);
      view.setOnLongClickListener(listener);
    }
  }
  return invokeWithView;
}","private boolean bindOnLongClickListener(Method method,OnLongClick onLongClick,Set<View> modifiedViews,Finder finder){
  boolean invokeWithView=checkInvokeWithView(method,new Class[]{View.class});
  method.setAccessible(true);
  InjectedOnLongClickListener listener=new InjectedOnLongClickListener(target,method,invokeWithView);
  int[] ids=onLongClick.id();
  for (  int id : ids) {
    if (id != 0) {
      View view=findView(method,id,finder);
      if (view != null) {
        view.setOnLongClickListener(listener);
      }
    }
  }
  return invokeWithView;
}"
4401,"@Override public void onItemClick(AdapterView<?> parentView,View view,int position,long id){
  if (enabled) {
    enabled=false;
    view.post(ENABLE_AGAIN);
    handleOnListener(view,position,id);
  }
}","@Override public void onItemClick(AdapterView<?> parentView,View view,int position,long id){
  if (enabled) {
    enabled=false;
    view.post(ENABLE_AGAIN);
    handleOnListener(parentView,view,position,id);
  }
}"
4402,"private void bindMethods(Finder finder){
  Method[] methods=clazz.getDeclaredMethods();
  Set<View> modifiedViews=new HashSet<View>();
  for (  final Method method : methods) {
    Annotation[] annotations=method.getAnnotations();
    for (    Annotation annotation : annotations) {
      if (annotation.annotationType() == OnClick.class) {
        bindOnClickListener(method,(OnClick)annotation,modifiedViews,finder);
      }
      if (annotation.annotationType() == OnItemClick.class) {
        bindOnItemClickListener(method,(OnItemClick)annotation,modifiedViews,finder);
      }
    }
  }
}","private void bindMethods(Finder finder){
  Method[] methods=clazz.getDeclaredMethods();
  Set<View> modifiedViews=new HashSet<View>();
  for (  final Method method : methods) {
    Annotation[] annotations=method.getAnnotations();
    for (    Annotation annotation : annotations) {
      if (annotation.annotationType() == OnClick.class) {
        bindOnClickListener(method,(OnClick)annotation,modifiedViews,finder);
      }
 else       if (annotation.annotationType() == OnItemClick.class) {
        bindOnItemClickListener(method,(OnItemClick)annotation,modifiedViews,finder);
      }
    }
  }
}"
4403,"/** 
 * 增加OnClick事件的绑定
 * @param method
 * @param onClick
 * @param modifiedViews
 * @param finder
 * @return
 */
private boolean bindOnClickListener(final Method method,OnClick onClick,Set<View> modifiedViews,Finder finder){
  boolean invokeWithView=checkInvokeWithView(method);
  method.setAccessible(true);
  InjectedOnClickListener listener=new InjectedOnClickListener(target,method,invokeWithView);
  int[] ids=onClick.id();
  for (  int id : ids) {
    if (id != 0) {
      View view=findView(method,id,finder);
      boolean modified=modifiedViews.add(view);
      if (!modified) {
        throw new InjectException(""String_Node_Str"" + method.getName());
      }
      view.setOnClickListener(listener);
    }
  }
  return invokeWithView;
}","private boolean bindOnClickListener(final Method method,OnClick onClick,Set<View> modifiedViews,Finder finder){
  boolean invokeWithView=checkInvokeWithView(method,new Class[]{View.class});
  method.setAccessible(true);
  InjectedOnClickListener listener=new InjectedOnClickListener(target,method,invokeWithView);
  int[] ids=onClick.id();
  for (  int id : ids) {
    if (id != 0) {
      View view=findView(method,id,finder);
      boolean modified=modifiedViews.add(view);
      if (!modified) {
        throw new InjectException(""String_Node_Str"" + method.getName());
      }
      view.setOnClickListener(listener);
    }
  }
  return invokeWithView;
}"
4404,"private boolean checkInvokeWithView(Method method){
  Class<?>[] parameterTypes=method.getParameterTypes();
  if (parameterTypes.length == 0) {
    return false;
  }
 else   if (parameterTypes.length == 1) {
    if (parameterTypes[0] == View.class) {
      return true;
    }
 else {
      throw new InjectException(""String_Node_Str"" + method.getName() + ""String_Node_Str""+ parameterTypes[0]);
    }
  }
 else {
    throw new InjectException(""String_Node_Str"" + method.getName());
  }
}","private boolean checkInvokeWithView(Method method,Class[] paramterClass){
  Class<?>[] parameterTypes=method.getParameterTypes();
  int paramterNum=paramterClass.length;
  if (parameterTypes.length == 0) {
    return false;
  }
 else   if (parameterTypes.length == paramterNum) {
    if (paramterClass.length == parameterTypes.length) {
      for (int i=0; i < parameterTypes.length; i++) {
        if (parameterTypes[i] == paramterClass[i]) {
          return true;
        }
 else {
          throw new InjectException(""String_Node_Str"" + method.getName() + ""String_Node_Str""+ paramterClass[i]+ ""String_Node_Str""+ parameterTypes[i]+ ""String_Node_Str"");
        }
      }
    }
 else {
      return false;
    }
  }
 else {
    throw new InjectException(""String_Node_Str"" + method.getName() + ""String_Node_Str""+ paramterNum+ ""String_Node_Str""+ parameterTypes.length);
  }
  return false;
}"
4405,"/** 
 * frankswu add OnItemClick，增加OnItemClick事件的绑定
 * @param method
 * @param annotation
 * @param modifiedViews
 * @param finder
 */
private boolean bindOnItemClickListener(Method method,OnItemClick onItemClick,Set<View> modifiedViews,Finder finder){
  boolean invokeWithView=checkInvokeWithView(method);
  method.setAccessible(true);
  InjectedOnItemClickListener listener=new InjectedOnItemClickListener(target,method,invokeWithView);
  int[] ids=onItemClick.id();
  for (  int id : ids) {
    if (id != 0) {
      AdapterView<?> view=null;
      try {
        view=(AdapterView<?>)findView(method,id,finder);
      }
 catch (      Exception e) {
        throw new InjectException(""String_Node_Str"");
      }
      boolean modified=modifiedViews.add(view);
      if (!modified) {
        throw new InjectException(""String_Node_Str"" + method.getName());
      }
      view.setOnItemClickListener(listener);
    }
  }
  return invokeWithView;
}","/** 
 * @param method
 * @param annotation
 * @param modifiedViews
 * @param finder
 */
private boolean bindOnItemClickListener(Method method,OnItemClick onItemClick,Set<View> modifiedViews,Finder finder){
  boolean invokeWithView=checkInvokeWithView(method,new Class[]{AdapterView.class,View.class,int.class,long.class});
  method.setAccessible(true);
  InjectedOnItemClickListener listener=new InjectedOnItemClickListener(target,method,invokeWithView);
  int[] ids=onItemClick.id();
  for (  int id : ids) {
    if (id != 0) {
      AdapterView<?> view=null;
      try {
        view=(AdapterView<?>)findView(method,id,finder);
      }
 catch (      Exception e) {
        throw new InjectException(""String_Node_Str"");
      }
      boolean modified=modifiedViews.add(view);
      if (!modified) {
        throw new InjectException(""String_Node_Str"" + method.getName());
      }
      view.setOnItemClickListener(listener);
    }
  }
  return invokeWithView;
}"
4406,"private View findView(Member field,int viewId,Finder finder){
  View view=null;
switch (finder) {
case DIALOG:
    return Finder.DIALOG.findById(target,viewId);
case ACTIVITY:
  if (activity == null) {
    throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str""+ context.getClass());
  }
view=finder.findById(activity,viewId);
if (view == null) {
throw new InjectException(""String_Node_Str"" + field.getName());
}
break;
case FRAGMENT:
return Finder.FRAGMENT.findById(fragmentView,viewId);
case VIEW:
view=Finder.VIEW.findById(target,viewId);
break;
default :
break;
}
return view;
}","private View findView(Member field,int viewId,Finder finder){
  View view=null;
switch (finder) {
case DIALOG:
    view=Finder.DIALOG.findById(target,viewId);
  break;
case ACTIVITY:
if (activity == null) {
  throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str""+ context.getClass());
}
view=finder.findById(activity,viewId);
if (view == null) {
throw new InjectException(""String_Node_Str"" + field.getName());
}
break;
case FRAGMENT:
view=Finder.FRAGMENT.findById(fragmentView,viewId);
break;
case VIEW:
view=Finder.VIEW.findById(target,viewId);
break;
default :
break;
}
return view;
}"
4407,"/** 
 * 查找supprot fragment
 * @param field
 * @param fragmentId
 * @return
 */
private Fragment findSupportFragment(Field field,int fragmentId){
  if (activity == null) {
    throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str""+ context.getClass());
  }
  Fragment fragment=null;
  if (activity instanceof FragmentActivity) {
    fragment=((FragmentActivity)activity).getSupportFragmentManager().findFragmentById(fragmentId);
  }
  if (fragment == null) {
    throw new InjectException(""String_Node_Str"" + field.getName());
  }
  return fragment;
}","/** 
 * 查找fragment
 * @param field
 * @param fragmentId
 * @return
 */
private Fragment findSupportFragment(Field field,int fragmentId){
  if (activity == null) {
    throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str""+ context.getClass());
  }
  Fragment fragment=null;
  if (activity instanceof FragmentActivity) {
    fragment=((FragmentActivity)activity).getSupportFragmentManager().findFragmentById(fragmentId);
  }
  if (fragment == null) {
    throw new InjectException(""String_Node_Str"" + field.getName());
  }
  return fragment;
}"
4408,"public MySSLSocketFactory(KeyStore truststore) throws NoSuchAlgorithmException, KeyManagementException, KeyStoreException, UnrecoverableKeyException {
  super(null);
  try {
    SSLContext context=SSLContext.getInstance(""String_Node_Str"");
    TrustManager[] trustAllCerts=new TrustManager[]{new X509TrustManager(){
      public java.security.cert.X509Certificate[] getAcceptedIssuers(){
        return new java.security.cert.X509Certificate[]{};
      }
      public void checkClientTrusted(      X509Certificate[] chain,      String authType) throws CertificateException {
      }
      public void checkServerTrusted(      X509Certificate[] chain,      String authType) throws CertificateException {
      }
    }
};
    context.init(null,trustAllCerts,new SecureRandom());
    sslFactory=context.getSocketFactory();
  }
 catch (  Exception e) {
    e.printStackTrace();
  }
}","public MySSLSocketFactory(KeyStore truststore) throws NoSuchAlgorithmException, KeyManagementException, KeyStoreException, UnrecoverableKeyException {
  super(truststore);
  try {
    SSLContext context=SSLContext.getInstance(""String_Node_Str"");
    TrustManager[] trustAllCerts=new TrustManager[]{new X509TrustManager(){
      public java.security.cert.X509Certificate[] getAcceptedIssuers(){
        return new java.security.cert.X509Certificate[]{};
      }
      public void checkClientTrusted(      X509Certificate[] chain,      String authType) throws CertificateException {
      }
      public void checkServerTrusted(      X509Certificate[] chain,      String authType) throws CertificateException {
      }
    }
};
    context.init(null,trustAllCerts,new SecureRandom());
    sslFactory=context.getSocketFactory();
  }
 catch (  Exception e) {
    e.printStackTrace();
  }
}"
4409,"private void injectFields(Finder finder){
  Field[] fields=clazz.getDeclaredFields();
  for (  Field field : fields) {
    Annotation[] annotations=field.getAnnotations();
    View view=null;
    for (    Annotation annotation : annotations) {
      if (annotation.annotationType() == InjectView.class) {
        int id=((InjectView)annotation).id();
        view=findViewByAnnotationId(id,field,finder);
        if (view == null) {
          throw new InjectException(""String_Node_Str"" + field.getName());
        }
        injectIntoField(field,view);
      }
 else       if (annotation.annotationType() == InjectViews.class) {
        String fieldTypeName=field.getType().getName();
        if (""String_Node_Str"".equals(fieldTypeName) || ""String_Node_Str"".equals(fieldTypeName)) {
          int[] ids=((InjectViews)annotation).ids();
          List<View> views=new ArrayList<View>();
          for (          int id : ids) {
            view=findViewByAnnotationId(id,field,finder);
            if (view == null) {
              throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str""+ id);
            }
            views.add(view);
          }
          if (""String_Node_Str"".equals(fieldTypeName)) {
            injectIntoField(field,views.toArray());
          }
          if (""String_Node_Str"".equals(fieldTypeName)) {
            injectIntoField(field,views);
          }
        }
 else {
          throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str"");
        }
      }
 else       if (annotation.annotationType() == InjectResource.class) {
        Object ressource=findResource(field.getType(),field,(InjectResource)annotation);
        injectIntoField(field,ressource);
      }
 else       if (annotation.annotationType() == InjectSystemService.class) {
        String serviceName=((InjectSystemService)annotation).value();
        Object service=context.getSystemService(serviceName);
        injectIntoField(field,service);
      }
 else       if (annotation.annotationType() == InjectExtra.class) {
        if (extras != null) {
          Object value=extras.get(((InjectExtra)annotation).key());
          injectIntoField(field,value);
        }
      }
 else       if (annotation.annotationType() == InjectSupportFragment.class) {
        int id=((InjectSupportFragment)annotation).id();
        Fragment fragment=findSupportFragment(field,id);
        injectIntoField(field,fragment);
      }
    }
  }
}","private void injectFields(Finder finder){
  Field[] fields=clazz.getDeclaredFields();
  for (  Field field : fields) {
    Annotation[] annotations=field.getAnnotations();
    View view=null;
    for (    Annotation annotation : annotations) {
      if (annotation.annotationType() == InjectView.class) {
        int id=((InjectView)annotation).id();
        view=findViewByAnnotationId(id,field,finder);
        if (view == null) {
          throw new InjectException(""String_Node_Str"" + field.getName());
        }
        injectIntoField(field,view);
      }
 else       if (annotation.annotationType() == InjectViews.class) {
        String fieldTypeName=field.getType().getName();
        if (fieldTypeName.startsWith(""String_Node_Str"") || fieldTypeName.startsWith(""String_Node_Str"")) {
          int[] ids=((InjectViews)annotation).ids();
          List<View> views=new ArrayList<View>();
          for (          int id : ids) {
            view=findViewByAnnotationId(id,field,finder);
            if (view == null) {
              throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str""+ id);
            }
            views.add(view);
          }
          if (fieldTypeName.startsWith(""String_Node_Str"")) {
            View[] v=(View[])Array.newInstance(field.getType().getComponentType(),views.size());
            injectListIntoField(field,views.toArray(v));
          }
          if (fieldTypeName.startsWith(""String_Node_Str"")) {
            injectIntoField(field,views);
          }
        }
 else {
          throw new InjectException(""String_Node_Str"" + field.getName() + ""String_Node_Str"");
        }
      }
 else       if (annotation.annotationType() == InjectResource.class) {
        Object ressource=findResource(field.getType(),field,(InjectResource)annotation);
        injectIntoField(field,ressource);
      }
 else       if (annotation.annotationType() == InjectSystemService.class) {
        String serviceName=((InjectSystemService)annotation).value();
        Object service=context.getSystemService(serviceName);
        injectIntoField(field,service);
      }
 else       if (annotation.annotationType() == InjectExtra.class) {
        if (extras != null) {
          Object value=extras.get(((InjectExtra)annotation).key());
          injectIntoField(field,value);
        }
      }
 else       if (annotation.annotationType() == InjectSupportFragment.class) {
        int id=((InjectSupportFragment)annotation).id();
        Fragment fragment=findSupportFragment(field,id);
        injectIntoField(field,fragment);
      }
    }
  }
}"
4410,"@Override public void run(){
  final int buf_sz=AudioRecord.getMinBufferSize(SAMPLE_RATE,AudioFormat.CHANNEL_IN_MONO,AudioFormat.ENCODING_PCM_16BIT) * 4;
  final AudioRecord audioRecord=new AudioRecord(MediaRecorder.AudioSource.MIC,SAMPLE_RATE,AudioFormat.CHANNEL_IN_MONO,AudioFormat.ENCODING_PCM_16BIT,buf_sz);
  try {
    if (mIsCapturing) {
      if (DEBUG)       Log.v(TAG,""String_Node_Str"");
      final byte[] buf=new byte[buf_sz];
      int readBytes;
      audioRecord.startRecording();
      try {
        while (mIsCapturing && !mRequestStop && !mIsEOS) {
          readBytes=audioRecord.read(buf,0,buf_sz);
          if (readBytes > 0) {
            encode(buf,readBytes,getPTSUs());
            frameAvailableSoon();
          }
        }
        frameAvailableSoon();
      }
  finally {
        audioRecord.stop();
      }
    }
  }
  finally {
    audioRecord.release();
  }
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
}","@Override public void run(){
  try {
    final int buf_sz=AudioRecord.getMinBufferSize(SAMPLE_RATE,AudioFormat.CHANNEL_IN_MONO,AudioFormat.ENCODING_PCM_16BIT) * 4;
    final AudioRecord audioRecord=new AudioRecord(MediaRecorder.AudioSource.MIC,SAMPLE_RATE,AudioFormat.CHANNEL_IN_MONO,AudioFormat.ENCODING_PCM_16BIT,buf_sz);
    try {
      if (mIsCapturing) {
        if (DEBUG)         Log.v(TAG,""String_Node_Str"");
        final byte[] buf=new byte[buf_sz];
        int readBytes;
        audioRecord.startRecording();
        try {
          while (mIsCapturing && !mRequestStop && !mIsEOS) {
            readBytes=audioRecord.read(buf,0,buf_sz);
            if (readBytes > 0) {
              encode(buf,readBytes,getPTSUs());
              frameAvailableSoon();
            }
          }
          frameAvailableSoon();
        }
  finally {
          audioRecord.stop();
        }
      }
    }
  finally {
      audioRecord.release();
    }
  }
 catch (  Exception e) {
    Log.e(TAG,""String_Node_Str"",e);
  }
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
}"
4411,"/** 
 * generate output file
 * @param type Environment.DIRECTORY_MOVIES / Environment.DIRECTORY_DCIM etc.
 * @param ext .mp4(.m4a for audio) or .png
 * @return return null when this app has no writing permission to external storage.
 */
private static final File getCaptureFile(String type,String ext){
  final File dir=new File(Environment.getExternalStoragePublicDirectory(type),DIR_NAME);
  Log.d(TAG,""String_Node_Str"" + dir.toString());
  dir.mkdirs();
  if (dir.canWrite()) {
    return new File(dir,getDateTimeString() + ext);
  }
  return null;
}","/** 
 * generate output file
 * @param type Environment.DIRECTORY_MOVIES / Environment.DIRECTORY_DCIM etc.
 * @param ext .mp4(.m4a for audio) or .png
 * @return return null when this app has no writing permission to external storage.
 */
public static final File getCaptureFile(String type,String ext){
  final File dir=new File(Environment.getExternalStoragePublicDirectory(type),DIR_NAME);
  Log.d(TAG,""String_Node_Str"" + dir.toString());
  dir.mkdirs();
  if (dir.canWrite()) {
    return new File(dir,getDateTimeString() + ext);
  }
  return null;
}"
4412,"public void setEglContext(EGLContext shared_context,int tex_id){
  mRenderHandler.setEglContext(shared_context,tex_id,mSurface);
}","public void setEglContext(EGLContext shared_context,int tex_id){
  mRenderHandler.setEglContext(shared_context,tex_id,mSurface,true);
}"
4413,"private void init(EGLContext shared_context,boolean with_depth_buffer){
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
  if (mEglDisplay != EGL14.EGL_NO_DISPLAY) {
    throw new RuntimeException(""String_Node_Str"");
  }
  mEglDisplay=EGL14.eglGetDisplay(EGL14.EGL_DEFAULT_DISPLAY);
  if (mEglDisplay == EGL14.EGL_NO_DISPLAY) {
    throw new RuntimeException(""String_Node_Str"");
  }
  final int[] version=new int[2];
  if (!EGL14.eglInitialize(mEglDisplay,version,0,version,1)) {
    mEglDisplay=null;
    throw new RuntimeException(""String_Node_Str"");
  }
  shared_context=shared_context != null ? shared_context : EGL14.EGL_NO_CONTEXT;
  if (mEglContext == EGL14.EGL_NO_CONTEXT) {
    mEglConfig=getConfig(with_depth_buffer);
    if (mEglConfig == null) {
      throw new RuntimeException(""String_Node_Str"");
    }
    mEglContext=createContext(shared_context);
  }
  final int[] values=new int[1];
  EGL14.eglQueryContext(mEglDisplay,mEglContext,EGL14.EGL_CONTEXT_CLIENT_VERSION,values,0);
  if (DEBUG)   Log.d(TAG,""String_Node_Str"" + values[0]);
}","private void init(EGLContext shared_context,boolean with_depth_buffer,boolean isRecordable){
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
  if (mEglDisplay != EGL14.EGL_NO_DISPLAY) {
    throw new RuntimeException(""String_Node_Str"");
  }
  mEglDisplay=EGL14.eglGetDisplay(EGL14.EGL_DEFAULT_DISPLAY);
  if (mEglDisplay == EGL14.EGL_NO_DISPLAY) {
    throw new RuntimeException(""String_Node_Str"");
  }
  final int[] version=new int[2];
  if (!EGL14.eglInitialize(mEglDisplay,version,0,version,1)) {
    mEglDisplay=null;
    throw new RuntimeException(""String_Node_Str"");
  }
  shared_context=shared_context != null ? shared_context : EGL14.EGL_NO_CONTEXT;
  if (mEglContext == EGL14.EGL_NO_CONTEXT) {
    mEglConfig=getConfig(with_depth_buffer,isRecordable);
    if (mEglConfig == null) {
      throw new RuntimeException(""String_Node_Str"");
    }
    mEglContext=createContext(shared_context);
  }
  final int[] values=new int[1];
  EGL14.eglQueryContext(mEglDisplay,mEglContext,EGL14.EGL_CONTEXT_CLIENT_VERSION,values,0);
  if (DEBUG)   Log.d(TAG,""String_Node_Str"" + values[0]);
  makeDefault();
}"
4414,"/** 
 * change context to draw this window surface
 * @return
 */
private boolean makeCurrent(EGLSurface surface){
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
  if (mEglDisplay == null) {
    if (DEBUG)     Log.d(TAG,""String_Node_Str"");
  }
  if (surface == null || surface == EGL14.EGL_NO_SURFACE) {
    int error=EGL14.eglGetError();
    if (error == EGL14.EGL_BAD_NATIVE_WINDOW) {
      Log.e(TAG,""String_Node_Str"");
    }
    return false;
  }
  if (!EGL14.eglMakeCurrent(mEglDisplay,surface,surface,mEglContext)) {
    Log.w(""String_Node_Str"",""String_Node_Str"" + EGL14.eglGetError());
    return false;
  }
  return true;
}","/** 
 * change context to draw this window surface
 * @return
 */
private boolean makeCurrent(EGLSurface surface){
  if (mEglDisplay == null) {
    if (DEBUG)     Log.d(TAG,""String_Node_Str"");
  }
  if (surface == null || surface == EGL14.EGL_NO_SURFACE) {
    int error=EGL14.eglGetError();
    if (error == EGL14.EGL_BAD_NATIVE_WINDOW) {
      Log.e(TAG,""String_Node_Str"");
    }
    return false;
  }
  if (!EGL14.eglMakeCurrent(mEglDisplay,surface,surface,mEglContext)) {
    Log.w(TAG,""String_Node_Str"" + EGL14.eglGetError());
    return false;
  }
  return true;
}"
4415,"private int swap(EGLSurface surface){
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
  if (!EGL14.eglSwapBuffers(mEglDisplay,surface)) {
    final int err=EGL14.eglGetError();
    if (DEBUG)     Log.w(TAG,""String_Node_Str"" + err);
    return err;
  }
  return EGL14.EGL_SUCCESS;
}","private int swap(EGLSurface surface){
  if (!EGL14.eglSwapBuffers(mEglDisplay,surface)) {
    final int err=EGL14.eglGetError();
    if (DEBUG)     Log.w(TAG,""String_Node_Str"" + err);
    return err;
  }
  return EGL14.EGL_SUCCESS;
}"
4416,"private EGLSurface createWindowSurface(Object nativeWindow){
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
  final int[] surfaceAttribs={EGL14.EGL_NONE};
  EGLSurface result=null;
  try {
    result=EGL14.eglCreateWindowSurface(mEglDisplay,mEglConfig,nativeWindow,surfaceAttribs,0);
  }
 catch (  IllegalArgumentException e) {
    Log.e(TAG,""String_Node_Str"",e);
  }
  return result;
}","private EGLSurface createWindowSurface(Object nativeWindow){
  if (DEBUG)   Log.v(TAG,""String_Node_Str"" + nativeWindow);
  final int[] surfaceAttribs={EGL14.EGL_NONE};
  EGLSurface result=null;
  try {
    result=EGL14.eglCreateWindowSurface(mEglDisplay,mEglConfig,nativeWindow,surfaceAttribs,0);
  }
 catch (  IllegalArgumentException e) {
    Log.e(TAG,""String_Node_Str"",e);
  }
  return result;
}"
4417,"public EglSurface createFromSurface(Surface surface){
  if (DEBUG)   Log.i(TAG,""String_Node_Str"");
  final EglSurface eglSurface=new EglSurface(this,surface);
  return eglSurface;
}","public EglSurface createFromSurface(Object surface){
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
  final EglSurface eglSurface=new EglSurface(this,surface);
  eglSurface.makeCurrent();
  return eglSurface;
}"
4418,"private EGLContext createContext(EGLContext shared_context){
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
  final int[] attrib_list={EGL14.EGL_CONTEXT_CLIENT_VERSION,2,EGL14.EGL_NONE};
  final EGLContext context=EGL14.eglCreateContext(mEglDisplay,mEglConfig,shared_context,attrib_list,0);
  checkEglError(""String_Node_Str"");
  return context;
}","private EGLContext createContext(EGLContext shared_context){
  final int[] attrib_list={EGL14.EGL_CONTEXT_CLIENT_VERSION,2,EGL14.EGL_NONE};
  final EGLContext context=EGL14.eglCreateContext(mEglDisplay,mEglConfig,shared_context,attrib_list,0);
  checkEglError(""String_Node_Str"");
  return context;
}"
4419,"private EGLConfig getConfig(boolean with_depth_buffer){
  final int[] attribList={EGL14.EGL_RENDERABLE_TYPE,EGL14.EGL_OPENGL_ES2_BIT,EGL14.EGL_RED_SIZE,8,EGL14.EGL_GREEN_SIZE,8,EGL14.EGL_BLUE_SIZE,8,EGL14.EGL_ALPHA_SIZE,8,EGL_RECORDABLE_ANDROID,1,with_depth_buffer ? EGL14.EGL_DEPTH_SIZE : EGL14.EGL_NONE,with_depth_buffer ? 16 : 0,EGL14.EGL_NONE};
  final EGLConfig[] configs=new EGLConfig[1];
  final int[] numConfigs=new int[1];
  if (!EGL14.eglChooseConfig(mEglDisplay,attribList,0,configs,0,configs.length,numConfigs,0)) {
    Log.w(TAG,""String_Node_Str"" + ""String_Node_Str"");
    return null;
  }
  return configs[0];
}","@SuppressWarnings(""String_Node_Str"") private EGLConfig getConfig(boolean with_depth_buffer,boolean isRecordable){
  final int[] attribList={EGL14.EGL_RENDERABLE_TYPE,EGL14.EGL_OPENGL_ES2_BIT,EGL14.EGL_RED_SIZE,8,EGL14.EGL_GREEN_SIZE,8,EGL14.EGL_BLUE_SIZE,8,EGL14.EGL_ALPHA_SIZE,8,EGL14.EGL_NONE,EGL14.EGL_NONE,EGL14.EGL_NONE,EGL14.EGL_NONE,EGL14.EGL_NONE,EGL14.EGL_NONE,EGL14.EGL_NONE};
  int offset=10;
  if (false) {
    attribList[offset++]=EGL14.EGL_STENCIL_SIZE;
    attribList[offset++]=8;
  }
  if (with_depth_buffer) {
    attribList[offset++]=EGL14.EGL_DEPTH_SIZE;
    attribList[offset++]=16;
  }
  if (isRecordable && (Build.VERSION.SDK_INT >= 18)) {
    attribList[offset++]=EGL_RECORDABLE_ANDROID;
    attribList[offset++]=1;
  }
  for (int i=attribList.length - 1; i >= offset; i--) {
    attribList[i]=EGL14.EGL_NONE;
  }
  final EGLConfig[] configs=new EGLConfig[1];
  final int[] numConfigs=new int[1];
  if (!EGL14.eglChooseConfig(mEglDisplay,attribList,0,configs,0,configs.length,numConfigs,0)) {
    Log.w(TAG,""String_Node_Str"" + ""String_Node_Str"");
    return null;
  }
  return configs[0];
}"
4420,"private void destroyWindowSurface(EGLSurface surface){
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
  if (surface != EGL14.EGL_NO_SURFACE) {
    EGL14.eglMakeCurrent(mEglDisplay,EGL14.EGL_NO_SURFACE,EGL14.EGL_NO_SURFACE,EGL14.EGL_NO_CONTEXT);
    EGL14.eglDestroySurface(mEglDisplay,surface);
  }
  surface=EGL14.EGL_NO_SURFACE;
}","private void destroyWindowSurface(EGLSurface surface){
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
  if (surface != EGL14.EGL_NO_SURFACE) {
    EGL14.eglMakeCurrent(mEglDisplay,EGL14.EGL_NO_SURFACE,EGL14.EGL_NO_SURFACE,EGL14.EGL_NO_CONTEXT);
    EGL14.eglDestroySurface(mEglDisplay,surface);
  }
  surface=EGL14.EGL_NO_SURFACE;
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
}"
4421,"private void destroyContext(){
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
  if (!EGL14.eglDestroyContext(mEglDisplay,mEglContext)) {
    Log.e(""String_Node_Str"",""String_Node_Str"" + mEglDisplay + ""String_Node_Str""+ mEglContext);
    Log.e(TAG,""String_Node_Str"" + EGL14.eglGetError());
  }
  mEglContext=EGL14.EGL_NO_CONTEXT;
}","private void destroyContext(){
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
  if (!EGL14.eglDestroyContext(mEglDisplay,mEglContext)) {
    Log.e(""String_Node_Str"",""String_Node_Str"" + mEglDisplay + ""String_Node_Str""+ mEglContext);
    Log.e(TAG,""String_Node_Str"" + EGL14.eglGetError());
  }
  mEglContext=EGL14.EGL_NO_CONTEXT;
  if (mDefaultContext != EGL14.EGL_NO_CONTEXT) {
    if (!EGL14.eglDestroyContext(mEglDisplay,mDefaultContext)) {
      Log.e(""String_Node_Str"",""String_Node_Str"" + mEglDisplay + ""String_Node_Str""+ mDefaultContext);
      Log.e(TAG,""String_Node_Str"" + EGL14.eglGetError());
    }
    mDefaultContext=EGL14.EGL_NO_CONTEXT;
  }
}"
4422,"EglSurface(EGLBase egl,Surface surface){
  if (DEBUG)   Log.i(TAG,""String_Node_Str"");
  mEgl=egl;
  mEglSurface=mEgl.createWindowSurface(surface);
}","EglSurface(EGLBase egl,int width,int height){
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
  mEgl=egl;
  mEglSurface=mEgl.createOffscreenSurface(width,height);
}"
4423,"public EGLBase(EGLContext shared_context,boolean with_depth_buffer){
  if (DEBUG)   Log.i(TAG,""String_Node_Str"");
  init(shared_context,with_depth_buffer);
}","public EGLBase(EGLContext shared_context,boolean with_depth_buffer,boolean isRecordable){
  if (DEBUG)   Log.v(TAG,""String_Node_Str"");
  init(shared_context,with_depth_buffer,isRecordable);
}"
4424,"/** 
 * @see {@link Display#getRotation()}
 */
public int getScreenRotation(){
  return screenRotation;
}","/** 
 * @see Display#getRotation()
 */
public int getScreenRotation(){
  return screenRotation;
}"
4425,"@Override public ObservableList<TreeItem<JarTreeNode>> getChildren(){
  if (isFirstTimeChildren) {
    isFirstTimeChildren=false;
    System.out.println(""String_Node_Str"" + getValue());
    super.getChildren().setAll(buildChildren());
  }
  return super.getChildren();
}","@Override public ObservableList<TreeItem<JarTreeNode>> getChildren(){
  if (isFirstTimeChildren) {
    isFirstTimeChildren=false;
    Log.log(""String_Node_Str"" + getValue());
    super.getChildren().setAll(buildChildren());
  }
  return super.getChildren();
}"
4426,"public void select(FileComponent cc){
  int byteOffset=cc.getOffset();
  textArea2.positionCaret(calcBytesTextPosition(cc.getOffset()));
  textArea2.selectPositionCaret(calcBytesTextPosition(cc.getOffset() + cc.getLength()) - 1);
  textArea3.positionCaret(calcAsciiTextPosition(cc.getOffset()));
  textArea3.selectPositionCaret(calcAsciiTextPosition(cc.getOffset() + cc.getLength()));
}","public void select(FileComponent cc){
  int byteOffset=cc.getOffset();
  int rowIndex=byteOffset / BYTES_PER_ROW;
  textArea2.positionCaret(calcBytesTextPosition(cc.getOffset()));
  textArea2.selectPositionCaret(calcBytesTextPosition(cc.getOffset() + cc.getLength()) - 1);
  textArea3.positionCaret(calcAsciiTextPosition(cc.getOffset()));
  textArea3.selectPositionCaret(calcAsciiTextPosition(cc.getOffset() + cc.getLength()));
}"
4427,"private void initTextArea(){
  textArea1.setFont(FontHelper.textFont);
  textArea2.setFont(FontHelper.textFont);
  textArea3.setFont(FontHelper.textFont);
  textArea1.setPrefColumnCount(6);
  textArea2.setPrefColumnCount(45);
  textArea3.setPrefColumnCount(16);
  int rowCount=hex.rowHeaderText.length() / 7;
  textArea1.setPrefRowCount(rowCount);
  textArea2.setPrefRowCount(rowCount);
  textArea3.setPrefRowCount(rowCount);
  textArea1.setEditable(false);
  textArea2.setEditable(false);
  textArea3.setEditable(false);
  textArea1.setStyle(""String_Node_Str"");
}","private void initTextArea(){
  textArea1.setFont(FontHelper.textFont);
  textArea2.setFont(FontHelper.textFont);
  textArea3.setFont(FontHelper.textFont);
  textArea1.setPrefColumnCount(6);
  textArea2.setPrefColumnCount(46);
  textArea3.setPrefColumnCount(16);
  int rowCount=hex.rowHeaderText.length() / 9 + 1;
  textArea1.setPrefRowCount(rowCount);
  textArea2.setPrefRowCount(rowCount);
  textArea3.setPrefRowCount(rowCount);
  textArea1.setContextMenu(new AsciiPaneMenu(textArea1));
  textArea2.setContextMenu(new HexPaneMenu(textArea2));
  textArea3.setContextMenu(new AsciiPaneMenu(textArea3));
  textArea1.setEditable(false);
  textArea2.setEditable(false);
  textArea3.setEditable(false);
  textArea1.setStyle(""String_Node_Str"");
}"
4428,"private void listenTreeItemSelection(){
  tree.getSelectionModel().getSelectedItems().addListener((  ListChangeListener.Change<? extends TreeItem<FileComponent>> c) -> {
    if (c.next() && c.wasAdded()) {
      TreeItem<FileComponent> node=c.getList().get(c.getFrom());
      if (node != null && node.getParent() != null) {
        FileComponent cc=node.getValue();
        Log.log(""String_Node_Str"" + cc);
        statusLabel.setText(""String_Node_Str"" + cc.getClass().getSimpleName());
        if (cc.getLength() > 0) {
          hexPane.select(cc);
          bytesBar.select(cc);
        }
      }
    }
  }
);
}","private void listenTreeItemSelection(){
  tree.getSelectionModel().getSelectedItems().addListener((  ListChangeListener.Change<? extends TreeItem<FileComponent>> c) -> {
    if (c.next() && c.wasAdded()) {
      TreeItem<FileComponent> node=c.getList().get(c.getFrom());
      if (node != null && node.getParent() != null) {
        FileComponent cc=node.getValue();
        Log.log(""String_Node_Str"" + cc);
        statusLabel.setText(cc.toString());
        if (cc.getLength() > 0) {
          hexPane.select(cc);
          bytesBar.select(cc);
        }
      }
    }
  }
);
}"
4429,"public ParsedViewerPane(FileComponent file,HexText hex){
  tree=buildClassTree(file);
  hexPane=new HexPane(hex);
  statusLabel=new Label(""String_Node_Str"");
  bytesBar=new BytesBar(file.getLength());
  bytesBar.setMaxHeight(statusLabel.getPrefHeight());
  bytesBar.setPrefWidth(100);
  super.setCenter(buildSplitPane());
  super.setBottom(buildStatusBar());
  listenTreeItemSelection();
}","public ParsedViewerPane(FileComponent file,HexText hex){
  tree=buildClassTree(file);
  hexPane=new HexPane(hex);
  statusLabel=new Label(""String_Node_Str"");
  bytesBar=new BytesBar(file.getLength());
  bytesBar.setMaxHeight(statusLabel.getMaxHeight());
  bytesBar.setPrefWidth(200);
  super.setCenter(buildSplitPane());
  super.setBottom(buildStatusBar());
  listenTreeItemSelection();
}"
4430,"private void saveToTmp(){
  if (!list.isEmpty() && listChanged) {
    byte[] bytes=list.stream().map(RecentFile::toString).collect(Collectors.joining(""String_Node_Str"")).getBytes(StandardCharsets.UTF_8);
    Path tmp=Paths.get(System.getProperty(""String_Node_Str""),""String_Node_Str"");
    System.out.println(""String_Node_Str"" + tmp + ""String_Node_Str"");
    try {
      Files.write(tmp,bytes);
    }
 catch (    IOException e) {
      e.printStackTrace(System.err);
    }
  }
}","private void saveToTmp(){
  if (!list.isEmpty() && listChanged) {
    byte[] bytes=list.stream().map(RecentFile::toString).collect(Collectors.joining(""String_Node_Str"")).getBytes(StandardCharsets.UTF_8);
    Path tmp=Paths.get(System.getProperty(""String_Node_Str""),""String_Node_Str"");
    Log.log(""String_Node_Str"" + tmp + ""String_Node_Str"");
    try {
      Files.write(tmp,bytes);
    }
 catch (    IOException e) {
      e.printStackTrace(System.err);
    }
  }
}"
4431,"private void loadFromTmp(){
  Path tmp=Paths.get(System.getProperty(""String_Node_Str""),""String_Node_Str"");
  if (Files.exists(tmp)) {
    System.out.println(""String_Node_Str"" + tmp + ""String_Node_Str"");
    try {
      List<String> rfs=Files.readAllLines(tmp,StandardCharsets.UTF_8);
      for (      String rf : rfs) {
        if (rf.contains(""String_Node_Str"")) {
          list.addLast(new RecentFile(rf));
        }
      }
    }
 catch (    IOException e) {
      e.printStackTrace(System.err);
    }
  }
}","private void loadFromTmp(){
  Path tmp=Paths.get(System.getProperty(""String_Node_Str""),""String_Node_Str"");
  if (Files.exists(tmp)) {
    Log.log(""String_Node_Str"" + tmp + ""String_Node_Str"");
    try {
      List<String> rfs=Files.readAllLines(tmp,StandardCharsets.UTF_8);
      for (      String rf : rfs) {
        if (rf.contains(""String_Node_Str"")) {
          list.addLast(new RecentFile(rf));
        }
      }
    }
 catch (    IOException e) {
      e.printStackTrace(System.err);
    }
  }
}"
4432,"public void select(FileComponent cc){
  int byteOffset=cc.getOffset();
  int rowIndex=byteOffset / BYTES_PER_ROW;
  int rows=textArea3.getText().length() / (BYTES_PER_ROW + 1);
  textArea2.positionCaret(calcBytesTextPosition(cc.getOffset()));
  textArea2.selectPositionCaret(calcBytesTextPosition(cc.getOffset() + cc.getLength()) - 1);
  textArea3.positionCaret(calcAsciiTextPosition(cc.getOffset()));
  textArea3.selectPositionCaret(calcAsciiTextPosition(cc.getOffset() + cc.getLength()));
  double height=getHeight();
  double textHeight=textArea2.getHeight();
  double vvalue=(((double)rowIndex) / rows * textHeight / (textHeight - height) - height / 2 / textHeight);
  if (((Double)vvalue).isInfinite() || ((Double)vvalue).isNaN()) {
  }
 else   if (vvalue < 0) {
    this.setVvalue(0);
  }
 else   if (vvalue > 1) {
    this.setVvalue(1);
  }
 else {
    this.setVvalue(vvalue);
  }
}","public void select(FileComponent cc){
  int byteOffset=cc.getOffset();
  int rowIndex=byteOffset / BYTES_PER_ROW;
  int rows=textArea3.getText().length() / (BYTES_PER_ROW + 1);
  textArea2.positionCaret(calcBytesTextPosition(cc.getOffset()));
  textArea2.selectPositionCaret(calcBytesTextPosition(cc.getOffset() + cc.getLength()) - 1);
  textArea3.positionCaret(calcAsciiTextPosition(cc.getOffset()));
  textArea3.selectPositionCaret(calcAsciiTextPosition(cc.getOffset() + cc.getLength()));
  double height=getHeight();
  double textHeight=textArea2.getHeight();
  double vvalue=(((double)rowIndex) / rows * textHeight / (textHeight - height) - height / 2 / textHeight);
  if (Double.isFinite(vvalue)) {
    if (vvalue < 0) {
      this.setVvalue(0);
    }
 else     if (vvalue > 1) {
      this.setVvalue(1);
    }
 else {
      this.setVvalue(vvalue);
    }
  }
}"
4433,"private void openClassInJar(String url){
  try {
    openFile(new URL(url));
  }
 catch (  MalformedURLException e) {
    Log.log(e);
  }
}","private void openClassInJar(String url){
  try {
    openFile(new URL(url));
  }
 catch (  MalformedURLException e) {
    e.printStackTrace(System.err);
  }
}"
4434,"@Override protected String loadDesc(ConstantPool cp){
  long high=super.getUInt(""String_Node_Str"");
  long low=super.getUInt(""String_Node_Str"");
  double d=Double.longBitsToDouble(high << 32 | low);
  return String.valueOf(d);
}","@Override protected String loadDesc(ConstantPool cp){
  long high=super.getUInt(""String_Node_Str"");
  long low=super.getUInt(""String_Node_Str"") & 0xffffffffL;
  double d=Double.longBitsToDouble(high << 32 | low);
  return String.valueOf(d);
}"
4435,"@Override protected String loadDesc(ConstantPool cp){
  long high=super.getUInt(""String_Node_Str"");
  long low=super.getUInt(""String_Node_Str"");
  long l=high << 32 | low;
  return String.valueOf(l);
}","@Override protected String loadDesc(ConstantPool cp){
  long high=super.getUInt(""String_Node_Str"");
  long low=super.getUInt(""String_Node_Str"") & 0xffffffffL;
  long l=high << 32 | low;
  return String.valueOf(l);
}"
4436,"public static URL showDialog(File jar) throws Exception {
  Stage stage=new Stage();
  stage.initModality(Modality.APPLICATION_MODAL);
  URI jarUri=new URI(""String_Node_Str"",jar.toPath().toUri().toString(),null);
  try (FileSystem zipFs=FileSystems.newFileSystem(jarUri,new HashMap<>())){
    AtomicBoolean openButtonClicked=new AtomicBoolean(false);
    Path rootPath=zipFs.getPath(""String_Node_Str"");
    TreeView<Path> jarTree=createTreeView(rootPath);
    Button openButton=new Button(""String_Node_Str"");
    openButton.setOnAction(e -> {
      stage.close();
      openButtonClicked.set(true);
    }
);
    Button cancelButton=new Button(""String_Node_Str"");
    cancelButton.setOnAction(e -> stage.close());
    BorderPane rootPane=createRootPane(jarTree,openButton,cancelButton);
    Scene scene=new Scene(rootPane,500,300);
    stage.setScene(scene);
    stage.setTitle(""String_Node_Str"");
    stage.showAndWait();
    if (openButtonClicked.get()) {
      TreeItem<Path> selectedItem=jarTree.getSelectionModel().getSelectedItem();
      if (selectedItem != null) {
        Path path=selectedItem.getValue();
        if (path.toString().endsWith(""String_Node_Str"")) {
          String classUrl=String.format(""String_Node_Str"",jar.getAbsolutePath(),path.toAbsolutePath());
          classUrl=classUrl.replace('\\','/');
          System.out.println(classUrl);
          return new URL(classUrl);
        }
      }
    }
    return null;
  }
 }","public static URL showDialog(File jar) throws Exception {
  Stage stage=new Stage();
  stage.initModality(Modality.APPLICATION_MODAL);
  Button openButton=new Button(""String_Node_Str"");
  Button cancelButton=new Button(""String_Node_Str"");
  AtomicBoolean openButtonClicked=new AtomicBoolean(false);
  cancelButton.setOnAction(e -> stage.close());
  openButton.setOnAction(e -> {
    openButtonClicked.set(true);
    stage.close();
  }
);
  URI jarUri=new URI(""String_Node_Str"",jar.toPath().toUri().toString(),null);
  try (FileSystem zipFs=FileSystems.newFileSystem(jarUri,new HashMap<>())){
    TreeView<Path> jarTree=createTreeView(zipFs.getPath(""String_Node_Str""));
    BorderPane rootPane=createRootPane(jarTree,openButton,cancelButton);
    Scene scene=new Scene(rootPane,500,300);
    stage.setScene(scene);
    stage.setTitle(""String_Node_Str"");
    stage.showAndWait();
    if (openButtonClicked.get()) {
      TreeItem<Path> selectedItem=jarTree.getSelectionModel().getSelectedItem();
      if (selectedItem != null) {
        Path path=selectedItem.getValue();
        if (path.toString().endsWith(""String_Node_Str"")) {
          String classUrl=String.format(""String_Node_Str"",jar.getAbsolutePath(),path.toAbsolutePath());
          classUrl=classUrl.replace('\\','/');
          System.out.println(classUrl);
          return new URL(classUrl);
        }
      }
    }
    return null;
  }
 }"
4437,"private void readEncodedArrayList(DexReader reader){
  int[] offArr=classDefs.stream().mapToInt(classDef -> classDef.getStaticValuesOff().getValue()).filter(off -> off > 0).toArray();
}","private void readEncodedArrayList(DexReader reader){
  int[] offArr=classDefs.stream().mapToInt(classDef -> classDef.getStaticValuesOff().getValue()).filter(off -> off > 0).toArray();
  encodedArrayList=reader.readOffsetsKnownList(offArr,EncodedArrayItem::new);
}"
4438,"@Override protected void postRead(DexFile dexFile){
  int index=getValue();
  if (index > 0) {
    FieldIdItem fieldId=dexFile.getFieldIdItem(index);
    String fieldName=fieldId.getDesc();
    String className=dexFile.getTypeIdItem(fieldId.getClassIdx()).getDesc();
    setDesc(index + ""String_Node_Str"" + className+ ""String_Node_Str""+ fieldName);
  }
}","@Override protected void postRead(DexFile dexFile){
  int index=getValue();
  if (index >= 0) {
    FieldIdItem fieldId=dexFile.getFieldIdItem(index);
    String fieldName=fieldId.getDesc();
    String className=dexFile.getTypeIdItem(fieldId.getClassIdx()).getDesc();
    setDesc(index + ""String_Node_Str"" + className+ ""String_Node_Str""+ fieldName);
  }
}"
4439,"@Override protected void postRead(DexFile dexFile){
  int index=getValue();
  if (index > 0) {
    MethodIdItem methodId=dexFile.getMethodIdItem(index);
    String methodName=methodId.getDesc();
    String className=dexFile.getTypeIdItem(methodId.getClassIdx()).getDesc();
    setDesc(index + ""String_Node_Str"" + className+ ""String_Node_Str""+ methodName);
  }
}","@Override protected void postRead(DexFile dexFile){
  int index=getValue();
  if (index >= 0) {
    MethodIdItem methodId=dexFile.getMethodIdItem(index);
    String methodName=methodId.getDesc();
    String className=dexFile.getTypeIdItem(methodId.getClassIdx()).getDesc();
    setDesc(index + ""String_Node_Str"" + className+ ""String_Node_Str""+ methodName);
  }
}"
4440,"@Override protected void postRead(DexFile dexFile){
  int index=getValue();
  if (index > 0) {
    setDesc(index + ""String_Node_Str"" + dexFile.getString(index));
  }
}","@Override protected void postRead(DexFile dexFile){
  int index=getValue();
  if (index >= 0) {
    setDesc(index + ""String_Node_Str"" + dexFile.getString(index));
  }
}"
4441,"@Override protected void postRead(DexFile dexFile){
  int index=getValue();
  if (index > 0) {
    TypeIdItem typeId=dexFile.getTypeIdItem(index);
    String typeDesc=dexFile.getString(typeId.getDescriptorIdx());
    setDesc(index + ""String_Node_Str"" + typeDesc);
  }
}","@Override protected void postRead(DexFile dexFile){
  int index=getValue();
  if (index >= 0) {
    TypeIdItem typeId=dexFile.getTypeIdItem(index);
    String typeDesc=dexFile.getString(typeId.getDescriptorIdx());
    setDesc(index + ""String_Node_Str"" + typeDesc);
  }
}"
4442,"@Override protected void postRead(DexFile dexFile){
  int index=getValue();
  if (index > 0) {
    ProtoIdItem protoId=dexFile.getProtoIdItem(index);
    String protoDesc=dexFile.getString(protoId.getShortyIdx());
    setDesc(index + ""String_Node_Str"" + protoDesc);
  }
}","@Override protected void postRead(DexFile dexFile){
  int index=getValue();
  if (index >= 0) {
    ProtoIdItem protoId=dexFile.getProtoIdItem(index);
    String protoDesc=dexFile.getString(protoId.getShortyIdx());
    setDesc(index + ""String_Node_Str"" + protoDesc);
  }
}"
4443,"@Override protected void postRead(DexFile dexFile){
  int index=getValue();
  if (index > 0) {
    TypeIdItem typeId=dexFile.getTypeIdItem(index);
    String typeDesc=dexFile.getString(typeId.getDescriptorIdx());
    setDesc(index + ""String_Node_Str"" + typeDesc);
  }
}","@Override protected void postRead(DexFile dexFile){
  int index=getValue();
  if (index >= 0) {
    TypeIdItem typeId=dexFile.getTypeIdItem(index);
    String typeDesc=dexFile.getString(typeId.getDescriptorIdx());
    setDesc(index + ""String_Node_Str"" + typeDesc);
  }
}"
4444,"@Override protected void postRead(DexFile dexFile){
  setDesc(getValue() + ""String_Node_Str"" + dexFile.getString(getValue()));
}","@Override protected void postRead(DexFile dexFile){
  int index=getValue();
  if (index > 0) {
    setDesc(index + ""String_Node_Str"" + dexFile.getString(index));
  }
 else {
    setDesc(String.valueOf(index));
  }
}"
4445,"@Override protected void postRead(DexFile dexFile){
  TypeIdItem typeId=dexFile.getTypeIdItem(getValue());
  String typeDesc=dexFile.getString(typeId.getDescriptorIdx());
  setDesc(getValue() + ""String_Node_Str"" + typeDesc);
}","@Override protected void postRead(DexFile dexFile){
  int index=getValue();
  if (index > 0) {
    TypeIdItem typeId=dexFile.getTypeIdItem(index);
    String typeDesc=dexFile.getString(typeId.getDescriptorIdx());
    setDesc(index + ""String_Node_Str"" + typeDesc);
  }
 else {
    setDesc(String.valueOf(index));
  }
}"
4446,"@Override protected Object call() throws Exception {
  System.out.println(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"");
  if (Files.size(file.toPath()) > 512 * 1024) {
  }
  String fileType=getExtension(file.getName());
  FileParser parser=FileParsers.getParser(fileType);
  byte[] bytes=Files.readAllBytes(file.toPath());
  FileComponent fc=parser.parse(bytes);
  FileHex hex=new FileHex(bytes);
  System.out.println(""String_Node_Str"");
  return new Object[]{fc,hex};
}","@Override protected Object call() throws Exception {
  System.out.println(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"");
  if (Files.size(file.toPath()) > 512 * 1024) {
    throw new FileParseException(""String_Node_Str"");
  }
  String fileType=getExtension(file.getName());
  FileParser parser=FileParsers.getParser(fileType);
  byte[] bytes=Files.readAllBytes(file.toPath());
  FileComponent fc=parser.parse(bytes);
  FileHex hex=new FileHex(bytes);
  System.out.println(""String_Node_Str"");
  return new Object[]{fc,hex};
}"
4447,"private void readPadding(DexReader reader){
  if ((triesSize.getValue() > 0) && (insnsSize.getValue() % 2 == 1)) {
    padding=reader.readUShort();
  }
 else {
    padding=new UShort();
    padding.readNothing(reader);
  }
}","private void readPadding(DexReader reader){
  if ((triesSize.getValue() > 0) && (insnsSize.getValue() % 2 == 1)) {
    padding=reader.readUShort();
  }
}"
4448,"private void readPadding(DexReader reader){
  if ((reader.getPosition() % 4) != 0) {
    padding=reader.readUShort();
  }
 else {
    padding=new UShort();
    padding.readNothing(reader);
  }
}","private void readPadding(DexReader reader){
  if ((triesSize.getValue() > 0) && (insnsSize.getValue() % 2 == 1)) {
    padding=reader.readUShort();
  }
 else {
    padding=new UShort();
    padding.readNothing(reader);
  }
}"
4449,"public void setName(String name){
  this.name=name;
}","public final void setName(String name){
  this.name=name;
}"
4450,"public String getName(){
  return name;
}","public final String getName(){
  return name;
}"
4451,"public int getOffset(){
  return offset;
}","public final int getOffset(){
  return offset;
}"
4452,"public int getLength(){
  return length;
}","public final int getLength(){
  return length;
}"
4453,"public String getDesc(){
  return desc;
}","public final String getDesc(){
  return desc;
}"
4454,"public void setDesc(String desc){
  this.desc=desc;
}","public final void setDesc(String desc){
  this.desc=desc;
}"
4455,"public Instruction(Opcode opcode){
  this.opcode=opcode;
}","public Instruction(Opcode opcode){
  this.opcode=opcode;
  setName(opcode.name());
}"
4456,"private int calcTextPosition(int byteOffset){
  int rowIndex=byteOffset / bytesPerRow;
  int colIndex=byteOffset % bytesPerRow;
  return (75 * rowIndex) + 10 + (colIndex * 3);
}","private int calcTextPosition(int byteOffset){
  int rowIndex=byteOffset / bytesPerRow;
  int colIndex=byteOffset % bytesPerRow;
  return (76 * rowIndex) + 10 + (colIndex * 3);
}"
4457,"public void forEach(Consumer<ConstantInfo> consumer){
  for (int i=1; i < cpCount; i++) {
    consumer.accept(constants[i]);
  }
}","public void forEach(Consumer<ConstantInfo> consumer){
  for (  ConstantInfo c : constants) {
    if (c != null) {
      consumer.accept(c);
    }
  }
}"
4458,"@Override protected void readContent(ClassReader reader){
  for (int i=1; i < cpCount; i++) {
    constants[i]=reader.readConstantInfo();
    setConstantName(constants[i],i);
  }
}","@Override protected void readContent(ClassReader reader){
  for (int i=1; i < cpCount; i++) {
    ConstantInfo c=reader.readConstantInfo();
    setConstantName(c,i);
    constants[i]=c;
    if (c instanceof ConstantLongInfo || c instanceof ConstantDoubleInfo) {
      i++;
    }
  }
}"
4459,"@Test public void read() throws Exception {
  String classFileName=TestClass.class.getName().replace('.','/') + ""String_Node_Str"";
  Path classFilePath=Paths.get(TestClass.class.getClassLoader().getResource(classFileName).toURI());
  byte[] classBytes=Files.readAllBytes(classFilePath);
  ClassFile cf=ClassFile.parse(classBytes);
  assertEquals(0,cf.getMinorVersion().getValue());
  assertEquals(52,cf.getMajorVersion().getValue());
  assertEquals(110,cf.getConstantPoolCount().getValue());
  assertEquals(2,cf.getInterfacesCount().getValue());
  assertEquals(6,cf.getFieldsCount().getValue());
}","@Test public void read() throws Exception {
  String classFileName=TestClass.class.getName().replace('.','/') + ""String_Node_Str"";
  Path classFilePath=Paths.get(TestClass.class.getClassLoader().getResource(classFileName).toURI());
  byte[] classBytes=Files.readAllBytes(classFilePath);
  ClassFile cf=ClassFile.parse(classBytes);
  assertEquals(0,cf.getMinorVersion().getValue());
  assertEquals(52,cf.getMajorVersion().getValue());
  assertEquals(114,cf.getConstantPoolCount().getValue());
  assertEquals(2,cf.getInterfacesCount().getValue());
  assertEquals(7,cf.getFieldsCount().getValue());
}"
4460,"@Override public ShimDataResponse getData(ShimDataRequest shimDataRequest) throws ShimException {
  AccessParameters accessParameters=shimDataRequest.getAccessParameters();
  String accessToken=accessParameters.getAccessToken();
  String tokenSecret=accessParameters.getTokenSecret();
  FitbitDataType fitbitDataType;
  try {
    fitbitDataType=FitbitDataType.valueOf(shimDataRequest.getDataTypeKey().trim().toUpperCase());
  }
 catch (  NullPointerException|IllegalArgumentException e) {
    throw new ShimException(""String_Node_Str"" + shimDataRequest.getDataTypeKey() + ""String_Node_Str"");
  }
  DateTime today=dayFormatter.parseDateTime(new DateTime().toString(dayFormatter));
  DateTime startDate=shimDataRequest.getStartDate() == null ? today.minusDays(1) : shimDataRequest.getStartDate();
  DateTime endDate=shimDataRequest.getEndDate() == null ? today.plusDays(1) : shimDataRequest.getEndDate();
  DateTime currentDate=startDate;
  List<ShimDataResponse> dayResponses=new ArrayList<>();
  while (currentDate.toDate().before(endDate.toDate()) || currentDate.toDate().equals(endDate.toDate())) {
    dayResponses.add(getDaysData(currentDate,fitbitDataType,shimDataRequest.getNormalize(),accessToken,tokenSecret));
    currentDate=currentDate.plusDays(1);
  }
  return shimDataRequest.getNormalize() ? aggregateNormalized(dayResponses) : aggregateIntoList(dayResponses);
}","@Override public ShimDataResponse getData(ShimDataRequest shimDataRequest) throws ShimException {
  AccessParameters accessParameters=shimDataRequest.getAccessParameters();
  String accessToken=accessParameters.getAccessToken();
  String tokenSecret=accessParameters.getTokenSecret();
  FitbitDataType fitbitDataType;
  try {
    fitbitDataType=FitbitDataType.valueOf(shimDataRequest.getDataTypeKey().trim().toUpperCase());
  }
 catch (  NullPointerException|IllegalArgumentException e) {
    throw new ShimException(""String_Node_Str"" + shimDataRequest.getDataTypeKey() + ""String_Node_Str"");
  }
  DateTime today=dayFormatter.parseDateTime(new DateTime().toString(dayFormatter));
  DateTime startDate=shimDataRequest.getStartDate() == null ? today.minusDays(1) : shimDataRequest.getStartDate();
  DateTime endDate=shimDataRequest.getEndDate() == null ? today.plusDays(1) : shimDataRequest.getEndDate();
  DateTime currentDate=startDate;
  if (fitbitDataType.equals(FitbitDataType.WEIGHT)) {
    return getRangeData(startDate,endDate,fitbitDataType,shimDataRequest.getNormalize(),accessToken,tokenSecret);
  }
 else {
    List<ShimDataResponse> dayResponses=new ArrayList<>();
    while (currentDate.toDate().before(endDate.toDate()) || currentDate.toDate().equals(endDate.toDate())) {
      dayResponses.add(getDaysData(currentDate,fitbitDataType,shimDataRequest.getNormalize(),accessToken,tokenSecret));
      currentDate=currentDate.plusDays(1);
    }
    return shimDataRequest.getNormalize() ? aggregateNormalized(dayResponses) : aggregateIntoList(dayResponses);
  }
}"
4461,"@Test @SuppressWarnings(""String_Node_Str"") public void testActivityNormalize() throws IOException {
}","@Test @SuppressWarnings(""String_Node_Str"") public void testActivityNormalize() throws IOException, ProcessingException {
}"
4462,"public AnomalyLikelihood(boolean useMovingAvg,int windowSize,boolean isWeighted,int claLearningPeriod,int estimationSamples){
  super(useMovingAvg,windowSize);
  this.isWeighted=isWeighted;
  this.claLearningPeriod=claLearningPeriod == VALUE_NONE ? this.claLearningPeriod : claLearningPeriod;
  this.estimationSamples=estimationSamples == VALUE_NONE ? this.estimationSamples : estimationSamples;
  this.probationaryPeriod=claLearningPeriod + estimationSamples;
  this.reestimationPeriod=100;
}","public AnomalyLikelihood(boolean useMovingAvg,int windowSize,boolean isWeighted,int claLearningPeriod,int estimationSamples){
  super(useMovingAvg,windowSize);
  this.isWeighted=isWeighted;
  this.claLearningPeriod=claLearningPeriod == VALUE_NONE ? this.claLearningPeriod : claLearningPeriod;
  this.estimationSamples=estimationSamples == VALUE_NONE ? this.estimationSamples : estimationSamples;
  this.probationaryPeriod=this.claLearningPeriod + this.estimationSamples;
  this.reestimationPeriod=100;
}"
4463,"/** 
 * Given a series of anomaly scores, compute the likelihood for each score. This function should be called once on a bunch of historical anomaly scores for an initial estimate of the distribution. It should be called again every so often (say every 50 records) to update the estimate.
 * @param anomalyScores
 * @param averagingWindow
 * @param skipRecords
 * @return
 */
public AnomalyLikelihoodMetrics estimateAnomalyLikelihoods(List<Sample> anomalyScores,int averagingWindow,int skipRecords){
  if (anomalyScores.size() == 0) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  AveragedAnomalyRecordList records=anomalyScoreMovingAverage(anomalyScores,averagingWindow);
  Statistic distribution;
  if (records.averagedRecords.size() <= skipRecords) {
    distribution=nullDistribution();
  }
 else {
    TDoubleList samples=records.getMetrics();
    distribution=estimateNormal(samples.toArray(skipRecords,samples.size()),true);
    samples=records.getSamples();
    Statistic metricDistribution=estimateNormal(samples.toArray(skipRecords,samples.size()),false);
    if (metricDistribution.variance < 1.5e-5) {
      distribution=nullDistribution();
    }
  }
  int i=0;
  double[] likelihoods=new double[records.averagedRecords.size()];
  for (  Sample sample : records.averagedRecords) {
    likelihoods[i++]=normalProbability(sample.score,distribution);
  }
  double[] filteredLikelihoods=filterLikelihoods(likelihoods);
  int len=likelihoods.length;
  AnomalyParams params=new AnomalyParams(new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str""},distribution,new MovingAverage(records.historicalValues,records.total,averagingWindow),len > 0 ? Arrays.copyOfRange(likelihoods,len - Math.min(averagingWindow,len),len) : new double[0]);
  if (LOG.isDebugEnabled()) {
    LOG.debug(""String_Node_Str"",params,len,Arrays.copyOfRange(filteredLikelihoods,0,20));
  }
  return new AnomalyLikelihoodMetrics(filteredLikelihoods,records,params);
}","/** 
 * Given a series of anomaly scores, compute the likelihood for each score. This function should be called once on a bunch of historical anomaly scores for an initial estimate of the distribution. It should be called again every so often (say every 50 records) to update the estimate.
 * @param anomalyScores
 * @param averagingWindow
 * @param skipRecords
 * @return
 */
public AnomalyLikelihoodMetrics estimateAnomalyLikelihoods(List<Sample> anomalyScores,int averagingWindow,int skipRecords){
  if (anomalyScores.size() == 0) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  AveragedAnomalyRecordList records=anomalyScoreMovingAverage(anomalyScores,averagingWindow);
  Statistic distribution;
  if (records.averagedRecords.size() <= skipRecords) {
    distribution=nullDistribution();
  }
 else {
    TDoubleList samples=records.getMetrics();
    final int numRecordsToCopy=samples.size() - skipRecords;
    distribution=estimateNormal(samples.toArray(skipRecords,numRecordsToCopy),true);
    samples=records.getSamples();
    Statistic metricDistribution=estimateNormal(samples.toArray(skipRecords,numRecordsToCopy),false);
    if (metricDistribution.variance < 1.5e-5) {
      distribution=nullDistribution();
    }
  }
  int i=0;
  double[] likelihoods=new double[records.averagedRecords.size()];
  for (  Sample sample : records.averagedRecords) {
    likelihoods[i++]=normalProbability(sample.score,distribution);
  }
  double[] filteredLikelihoods=filterLikelihoods(likelihoods);
  int len=likelihoods.length;
  AnomalyParams params=new AnomalyParams(new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str""},distribution,new MovingAverage(records.historicalValues,records.total,averagingWindow),len > 0 ? Arrays.copyOfRange(likelihoods,len - Math.min(averagingWindow,len),len) : new double[0]);
  if (LOG.isDebugEnabled()) {
    LOG.debug(""String_Node_Str"",params,len,Arrays.copyOfRange(filteredLikelihoods,0,20));
  }
  return new AnomalyLikelihoodMetrics(filteredLikelihoods,records,params);
}"
4464,"/** 
 * Returns a   {@link Tuple} containing the current key in thezero'th slot, and a list objects which are members of the group specified by that key. {@inheritDoc}
 */
@SuppressWarnings(""String_Node_Str"") @Override public Tuple next(){
  Object[] objs=IntStream.range(0,numEntries + 1).mapToObj(i -> i == 0 ? minKeyVal : new ArrayList<R>()).toArray();
  Tuple retVal=new Tuple((Object[])objs);
  for (int i=0; i < numEntries; i++) {
    if (isEligibleList(i,minKeyVal)) {
      ((List<Object>)retVal.get(i + 1)).add(nextList[i].get().getKey());
      drainKey(retVal,i,minKeyVal);
      advanceList[i]=true;
    }
 else {
      advanceList[i]=false;
      ((List<Object>)retVal.get(i + 1)).add(Slot.empty());
    }
  }
  return retVal;
}","/** 
 * Returns a   {@link Tuple} containing the current key in thezero'th slot, and a list objects which are members of the group specified by that key. {@inheritDoc}
 */
@SuppressWarnings(""String_Node_Str"") @Override public Tuple next(){
  Object[] objs=IntStream.range(0,numEntries + 1).mapToObj(i -> i == 0 ? minKeyVal : new ArrayList<R>()).toArray();
  Tuple retVal=new Tuple((Object[])objs);
  for (int i=0; i < numEntries; i++) {
    if (isEligibleList(i,minKeyVal)) {
      ((List<Object>)retVal.get(i + 1)).add(nextList[i].get().getFirst());
      drainKey(retVal,i,minKeyVal);
      advanceList[i]=true;
    }
 else {
      advanceList[i]=false;
      ((List<Object>)retVal.get(i + 1)).add(Slot.empty());
    }
  }
  return retVal;
}"
4465,"/** 
 * Returns a flag indicating whether the list currently pointed to by the specified index contains a key which matches the specified ""targetKey"".
 * @param listIdx       the index pointing to the {@link GroupBy} beingprocessed.
 * @param targetKey     the specified key to match.
 * @return  true if so, false if not
 */
private boolean isEligibleList(int listIdx,Object targetKey){
  return nextList[listIdx].isPresent() && nextList[listIdx].get().getValue().equals(targetKey);
}","/** 
 * Returns a flag indicating whether the list currently pointed to by the specified index contains a key which matches the specified ""targetKey"".
 * @param listIdx       the index pointing to the {@link GroupBy} beingprocessed.
 * @param targetKey     the specified key to match.
 * @return  true if so, false if not
 */
private boolean isEligibleList(int listIdx,Object targetKey){
  return nextList[listIdx].isPresent() && nextList[listIdx].get().getSecond().equals(targetKey);
}"
4466,"/** 
 * Returns the next smallest generated key.
 * @return  the next smallest generated key.
 */
private boolean nextMinKey(){
  return Arrays.stream(nextList).filter(opt -> opt.isPresent()).map(opt -> opt.get().getValue()).min((k,k2) -> k.compareTo(k2)).map(k -> {
    minKeyVal=k;
    return k;
  }
).isPresent();
}","/** 
 * Returns the next smallest generated key.
 * @return  the next smallest generated key.
 */
private boolean nextMinKey(){
  return Arrays.stream(nextList).filter(opt -> opt.isPresent()).map(opt -> opt.get().getSecond()).min((k,k2) -> k.compareTo(k2)).map(k -> {
    minKeyVal=k;
    return k;
  }
).isPresent();
}"
4467,"/** 
 * (Re)initializes the internal   {@link Generator}(s). This method may be used to ""restart"" the internal   {@link Iterator}s and reuse this object.
 */
@SuppressWarnings(""String_Node_Str"") public void reset(){
  generatorList=new ArrayList<>();
  for (int i=0; i < entries.length; i++) {
    generatorList.add(GroupBy.of(entries[i].getKey(),entries[i].getValue()));
  }
  numEntries=generatorList.size();
  advanceList=new boolean[numEntries];
  Arrays.fill(advanceList,true);
  nextList=new Slot[numEntries];
  Arrays.fill(nextList,Slot.NONE);
}","/** 
 * (Re)initializes the internal   {@link Generator}(s). This method may be used to ""restart"" the internal   {@link Iterator}s and reuse this object.
 */
@SuppressWarnings(""String_Node_Str"") public void reset(){
  generatorList=new ArrayList<>();
  for (int i=0; i < entries.length; i++) {
    generatorList.add(GroupBy.of(entries[i].getFirst(),entries[i].getSecond()));
  }
  numEntries=generatorList.size();
  advanceList=new boolean[numEntries];
  Arrays.fill(advanceList,true);
  nextList=new Slot[numEntries];
  Arrays.fill(nextList,Slot.NONE);
}"
4468,"/** 
 * Each input grouper may generate multiple members which match the specified ""targetVal"". This method guarantees that all members  are added to the list residing at the specified Tuple index.
 * @param retVal        the Tuple being added to
 * @param listIdx       the index specifying the list within the tuple which will have members added to it
 * @param targetVal     the value to match in order to be an added member
 */
@SuppressWarnings(""String_Node_Str"") private void drainKey(Tuple retVal,int listIdx,R targetVal){
  while (generatorList.get(listIdx).hasNext()) {
    if (generatorList.get(listIdx).peek().getValue().equals(targetVal)) {
      nextList[listIdx]=Slot.of(generatorList.get(listIdx).next());
      ((List<Object>)retVal.get(listIdx + 1)).add(nextList[listIdx].get().getKey());
    }
 else {
      nextList[listIdx]=Slot.empty();
      break;
    }
  }
}","/** 
 * Each input grouper may generate multiple members which match the specified ""targetVal"". This method guarantees that all members  are added to the list residing at the specified Tuple index.
 * @param retVal        the Tuple being added to
 * @param listIdx       the index specifying the list within the tuple which will have members added to it
 * @param targetVal     the value to match in order to be an added member
 */
@SuppressWarnings(""String_Node_Str"") private void drainKey(Tuple retVal,int listIdx,R targetVal){
  while (generatorList.get(listIdx).hasNext()) {
    if (generatorList.get(listIdx).peek().getSecond().equals(targetVal)) {
      nextList[listIdx]=Slot.of(generatorList.get(listIdx).next());
      ((List<Object>)retVal.get(listIdx + 1)).add(nextList[listIdx].get().getFirst());
    }
 else {
      nextList[listIdx]=Slot.empty();
      break;
    }
  }
}"
4469,"/** 
 * Used internally to find the synapse with the smallest permanence on the given segment.
 * @param dd    Segment object to search for synapses on
 * @return  Synapse object on the segment with the minimal permanence
 */
private Synapse minPermanenceSynapse(DistalDendrite dd){
  List<Synapse> synapses=getSynapses(dd);
  Synapse min=null;
  double minPermanence=Double.MAX_VALUE;
  for (  Synapse synapse : synapses) {
    if (!synapse.destroyed() && synapse.getPermanence() < minPermanence - EPSILON) {
      min=synapse;
      minPermanence=synapse.getPermanence();
    }
  }
  return min;
}","/** 
 * Used internally to find the synapse with the smallest permanence on the given segment.
 * @param dd    Segment object to search for synapses on
 * @return  Synapse object on the segment with the minimal permanence
 */
private Synapse minPermanenceSynapse(DistalDendrite dd){
  List<Synapse> synapses=unDestroyedSynapsesForSegment(dd);
  Synapse min=null;
  double minPermanence=Double.MAX_VALUE;
  for (  Synapse synapse : synapses) {
    if (!synapse.destroyed() && synapse.getPermanence() < minPermanence - EPSILON) {
      min=synapse;
      minPermanence=synapse.getPermanence();
    }
  }
  return min;
}"
4470,"/** 
 * Creates a new synapse on a segment.
 * @param segment               the {@link DistalDendrite} segment to which a {@link Synapse} is being created
 * @param presynapticCell       the source {@link Cell}
 * @param permanence            the initial permanence
 * @return  the created {@link Synapse}
 */
public Synapse createSynapse(DistalDendrite segment,Cell presynapticCell,double permanence){
  while (numSynapses(segment) >= maxSynapsesPerSegment) {
    destroySynapse(minPermanenceSynapse(segment));
  }
  Synapse synapse=null;
  boolean found=false;
  if (segment.getNumDestroyedSynapses() > 0) {
    for (    Synapse s : getSynapses(segment)) {
      if (s.destroyed()) {
        synapse=s;
        found=true;
        break;
      }
    }
    if (!found) {
      throw new IllegalStateException(""String_Node_Str"");
    }
    synapse.setDestroyed(false);
    segment.decDestroyedSynapses();
    incrementDistalSynapses();
  }
 else {
    getSynapses(segment).add(synapse=new Synapse(this,presynapticCell,segment,null,incrementDistalSynapses(),presynapticCell.getIndex()));
  }
  getReceptorSynapses(presynapticCell,true).add(synapse);
  synapse.setPermanence(this,permanence);
  return synapse;
}","/** 
 * Creates a new synapse on a segment.
 * @param segment               the {@link DistalDendrite} segment to which a {@link Synapse} is being created
 * @param presynapticCell       the source {@link Cell}
 * @param permanence            the initial permanence
 * @return  the created {@link Synapse}
 */
public Synapse createSynapse(DistalDendrite segment,Cell presynapticCell,double permanence){
  while (numSynapses(segment) >= maxSynapsesPerSegment) {
    destroySynapse(minPermanenceSynapse(segment));
  }
  Synapse synapse=null;
  boolean found=false;
  if (segment.getNumDestroyedSynapses() > 0) {
    for (    Synapse s : getSynapses(segment)) {
      if (s.destroyed()) {
        synapse=s;
        found=true;
        break;
      }
    }
    if (!found) {
      throw new IllegalStateException(""String_Node_Str"");
    }
    synapse.setDestroyed(false);
    segment.decDestroyedSynapses();
    incrementDistalSynapses();
    synapse.setPresynapticCell(presynapticCell);
  }
 else {
    getSynapses(segment).add(synapse=new Synapse(this,presynapticCell,segment,null,incrementDistalSynapses(),presynapticCell.getIndex()));
  }
  getReceptorSynapses(presynapticCell,true).add(synapse);
  synapse.setPermanence(this,permanence);
  return synapse;
}"
4471,"/** 
 * Creates nDesiredNewSynapes synapses on the segment passed in if possible, choosing random cells from the previous winner cells that are not already on the segment. <p> <b>Notes:</b> The process of writing the last value into the index in the array that was most recently changed is to ensure the same results that we get in the c++ implementation using iter_swap with vectors. </p>
 * @param conn                      Connections instance for the tm
 * @param prevWinnerCells           Winner cells in `t-1`
 * @param segment                   Segment to grow synapses on.     
 * @param initialPermanence         Initial permanence of a new synapse.
 * @param nDesiredNewSynapses       Desired number of synapses to grow
 * @param random                    Tm object used to generate randomnumbers
 */
public void growSynapses(Connections conn,Set<Cell> prevWinnerCells,DistalDendrite segment,double initialPermanence,int nDesiredNewSynapses,Random random){
  List<Cell> candidates=new ArrayList<>(prevWinnerCells);
  Collections.sort(candidates);
  int eligibleEnd=candidates.size();
  for (  Synapse synapse : conn.unDestroyedSynapsesForSegment(segment)) {
    Cell presynapticCell=synapse.getPresynapticCell();
    int ineligible=candidates.subList(0,eligibleEnd).indexOf(presynapticCell);
    if (ineligible != -1) {
      eligibleEnd--;
      candidates.set(ineligible,candidates.get(eligibleEnd));
    }
  }
  int nActual=nDesiredNewSynapses < eligibleEnd ? nDesiredNewSynapses : eligibleEnd;
  for (int i=0; i < nActual; i++) {
    int rand=random.nextInt(eligibleEnd);
    conn.createSynapse(segment,candidates.get(rand),initialPermanence);
    candidates.set(rand,candidates.get(eligibleEnd));
    eligibleEnd--;
  }
}","/** 
 * Creates nDesiredNewSynapes synapses on the segment passed in if possible, choosing random cells from the previous winner cells that are not already on the segment. <p> <b>Notes:</b> The process of writing the last value into the index in the array that was most recently changed is to ensure the same results that we get in the c++ implementation using iter_swap with vectors. </p>
 * @param conn                      Connections instance for the tm
 * @param prevWinnerCells           Winner cells in `t-1`
 * @param segment                   Segment to grow synapses on.     
 * @param initialPermanence         Initial permanence of a new synapse.
 * @param nDesiredNewSynapses       Desired number of synapses to grow
 * @param random                    Tm object used to generate randomnumbers
 */
public void growSynapses(Connections conn,Set<Cell> prevWinnerCells,DistalDendrite segment,double initialPermanence,int nDesiredNewSynapses,Random random){
  List<Cell> candidates=new ArrayList<>(prevWinnerCells);
  Collections.sort(candidates);
  int eligibleEnd=candidates.size() - 1;
  for (  Synapse synapse : conn.unDestroyedSynapsesForSegment(segment)) {
    Cell presynapticCell=synapse.getPresynapticCell();
    int index=candidates.subList(0,eligibleEnd + 1).indexOf(presynapticCell);
    if (index != -1) {
      candidates.set(index,candidates.get(eligibleEnd));
      eligibleEnd--;
    }
  }
  int candidatesLength=eligibleEnd + 1;
  int nActual=nDesiredNewSynapses < candidatesLength ? nDesiredNewSynapses : candidatesLength;
  for (int i=0; i < nActual; i++) {
    int rand=random.nextInt(candidatesLength);
    conn.createSynapse(segment,candidates.get(rand),initialPermanence);
    candidates.set(rand,candidates.get(candidatesLength - 1));
    candidatesLength--;
  }
}"
4472,"/** 
 * Returns a   {@link Tuple} containing the current key in thezero'th slot, and a list objects which are members of the group specified by that key. {@inheritDoc}
 */
@SuppressWarnings(""String_Node_Str"") @Override public Tuple next(){
  advanceSequences();
  R minKeyVal=nextMinKey();
  Object[] objs=IntStream.range(0,numEntries + 1).mapToObj(i -> i == 0 ? minKeyVal : new ArrayList<R>()).toArray();
  Tuple retVal=new Tuple((Object[])objs);
  for (int i=0; i < numEntries; i++) {
    if (isEligibleList(i,minKeyVal)) {
      ((List<Object>)retVal.get(i + 1)).add(nextList[i].get().getKey());
      drainKey(retVal,i,minKeyVal);
      advanceList[i]=true;
    }
 else {
      advanceList[i]=false;
      ((List<Object>)retVal.get(i + 1)).add(NONE);
    }
  }
  return retVal;
}","/** 
 * Returns a   {@link Tuple} containing the current key in thezero'th slot, and a list objects which are members of the group specified by that key. {@inheritDoc}
 */
@SuppressWarnings(""String_Node_Str"") @Override public Tuple next(){
  Object[] objs=IntStream.range(0,numEntries + 1).mapToObj(i -> i == 0 ? minKeyVal : new ArrayList<R>()).toArray();
  Tuple retVal=new Tuple((Object[])objs);
  for (int i=0; i < numEntries; i++) {
    if (isEligibleList(i,minKeyVal)) {
      ((List<Object>)retVal.get(i + 1)).add(nextList[i].get().getKey());
      drainKey(retVal,i,minKeyVal);
      advanceList[i]=true;
    }
 else {
      advanceList[i]=false;
      ((List<Object>)retVal.get(i + 1)).add(NONE);
    }
  }
  return retVal;
}"
4473,"/** 
 * (Re)initializes the internal   {@link Generator}(s). This method may be used to ""restart"" the internal   {@link Iterator}s and reuse this object.
 */
@SuppressWarnings(""String_Node_Str"") public void reset(){
  generatorList=new ArrayList<>();
  for (int i=0; i < entries.length; i++) {
    generatorList.add(GroupBy.of(entries[i].getKey(),entries[i].getValue()));
  }
  numEntries=generatorList.size();
  for (int i=0; i < numEntries; i++) {
    for (    Pair<Object,R> p : generatorList.get(i)) {
      System.out.println(""String_Node_Str"" + i + ""String_Node_Str""+ p.getValue()+ ""String_Node_Str""+ p.getKey());
    }
    System.out.println(""String_Node_Str"");
  }
  generatorList=new ArrayList<>();
  for (int i=0; i < entries.length; i++) {
    generatorList.add(GroupBy.of(entries[i].getKey(),entries[i].getValue()));
  }
  advanceList=new boolean[numEntries];
  Arrays.fill(advanceList,true);
  nextList=new Optional[numEntries];
  Arrays.fill(nextList,NONE);
}","/** 
 * (Re)initializes the internal   {@link Generator}(s). This method may be used to ""restart"" the internal   {@link Iterator}s and reuse this object.
 */
@SuppressWarnings(""String_Node_Str"") public void reset(){
  generatorList=new ArrayList<>();
  for (int i=0; i < entries.length; i++) {
    generatorList.add(GroupBy.of(entries[i].getKey(),entries[i].getValue()));
  }
  numEntries=generatorList.size();
  for (int i=0; i < numEntries; i++) {
    for (    Pair<Object,R> p : generatorList.get(i)) {
      System.out.println(""String_Node_Str"" + i + ""String_Node_Str""+ p.getKey()+ ""String_Node_Str""+ p.getValue());
    }
    System.out.println(""String_Node_Str"");
  }
  generatorList=new ArrayList<>();
  for (int i=0; i < entries.length; i++) {
    generatorList.add(GroupBy.of(entries[i].getKey(),entries[i].getValue()));
  }
  advanceList=new boolean[numEntries];
  Arrays.fill(advanceList,true);
  nextList=new Optional[numEntries];
  Arrays.fill(nextList,NONE);
}"
4474,"/** 
 * Returns a flag indicating that at least one   {@link Generator} hasa matching key for the current ""smallest"" key generated.
 * @return a flag indicating that at least one {@link Generator} hasa matching key for the current ""smallest"" key generated.
 */
@Override public boolean hasNext(){
  if (generatorList == null) {
    reset();
  }
  return IntStream.range(0,numEntries).filter(i -> advanceList[i] && generatorList.get(i).hasNext()).mapToObj(i -> Optional.of(generatorList.get(i).peek())).anyMatch(i -> i.isPresent());
}","/** 
 * Returns a flag indicating that at least one   {@link Generator} hasa matching key for the current ""smallest"" key generated.
 * @return a flag indicating that at least one {@link Generator} hasa matching key for the current ""smallest"" key generated.
 */
@Override public boolean hasNext(){
  if (generatorList == null) {
    reset();
  }
  advanceSequences();
  return nextMinKey();
}"
4475,"/** 
 * Returns the next smallest generated key.
 * @return  the next smallest generated key.
 */
private R nextMinKey(){
  return Arrays.stream(nextList).filter(opt -> opt.isPresent()).map(opt -> opt.get().getValue()).min((k,k2) -> k.compareTo(k2)).get();
}","/** 
 * Returns the next smallest generated key.
 * @return  the next smallest generated key.
 */
private boolean nextMinKey(){
  return Arrays.stream(nextList).filter(opt -> opt.isPresent()).map(opt -> opt.get().getValue()).min((k,k2) -> k.compareTo(k2)).map(k -> {
    minKeyVal=k;
    return k;
  }
).isPresent();
}"
4476,"/** 
 * Process one input sample. This method is called by outer loop code outside the nupic-engine. We  use this instead of the nupic engine compute() because our inputs and  outputs aren't fixed size vectors of reals. <p>
 * @param recordNum <p>Record number of this input pattern. Record numbers normally increase sequentially by 1 each time unless there are missing records in the dataset. Knowing this information ensures that we don't get confused by missing records.
 * @param classification <p>{@link Map} of the classification information:<p>&emsp;""bucketIdx"" - index of the encoder bucket <p>&emsp;""actValue"" -  actual value doing into the encoder
 * @param patternNZ <p>List of the active indices from the output below. When the output is from the TemporalMemory, this array should be the indices of the active cells.
 * @param learn <p>If true, learn this sample.
 * @param infer <p>If true, perform inference.
 * @return {@link Classification} containing inference results. The Classificationcontains the computed probability distribution (relative likelihood for each bucketIdx starting from bucketIdx 0) for each step in  {@code steps}. Each bucket's likelihood can be accessed individually, or all the buckets' likelihoods can be obtained in the form of a double array. <pre>  {@code //Get likelihood val for bucket 0, 5 steps in future classification.getStat(5, 0); //Get all buckets' likelihoods as double[] where each //index is the likelihood for that bucket //(e.g. [0] contains likelihood for bucketIdx 0) classification.getStats(5);}</pre> The Classification also contains the average actual value for each bucket. The average values for the buckets can be accessed individually, or altogether as a double[]. <pre>  {@code //Get average actual val for bucket 0 classification.getActualValue(0); //Get average vals for all buckets as double[], where //each index is the average val for that bucket //(e.g. [0] contains average val for bucketIdx 0) classification.getActualValues();}</pre> The Classification can also be queried for the most probable bucket (the bucket with the highest associated likelihood value), as well as the average input value that corresponds to that bucket. <pre>  {@code //Get index of most probable bucket classification.getMostProbableBucketIndex(); //Get the average actual val for that bucket classification.getMostProbableValue();}</pre>
 */
@SuppressWarnings(""String_Node_Str"") public <T>Classification<T> compute(int recordNum,Map<String,Object> classification,int[] patternNZ,boolean learn,boolean infer){
  Classification<T> retVal=new Classification<T>();
  List<T> actualValues=(List<T>)this.actualValues;
  if (recordNumMinusLearnIteration == -1)   recordNumMinusLearnIteration=recordNum - learnIteration;
  learnIteration=recordNum - recordNumMinusLearnIteration;
  if (verbosity >= 1) {
    System.out.println(String.format(""String_Node_Str"",g_debugPrefix));
    System.out.printf(""String_Node_Str"",recordNum);
    System.out.printf(""String_Node_Str"",learnIteration);
    System.out.printf(""String_Node_Str"",patternNZ.length,patternNZ);
    System.out.println(""String_Node_Str"" + classification);
  }
  patternNZHistory.append(new Tuple(learnIteration,patternNZ));
  if (ArrayUtils.max(patternNZ) > maxInputIdx) {
    int newMaxInputIdx=Math.max(ArrayUtils.max(patternNZ),maxBucketIdx);
    for (    int nSteps : steps.toArray()) {
      for (int i=maxBucketIdx; i < ArrayUtils.max(patternNZ); i++) {
        weightMatrix.get(nSteps).addCol(new double[maxBucketIdx + 1]);
      }
    }
    maxInputIdx=newMaxInputIdx;
  }
  if (infer) {
    retVal=infer(patternNZ,classification);
  }
  if (learn && classification.get(""String_Node_Str"") != null) {
    int bucketIdx=(int)classification.get(""String_Node_Str"");
    Object actValue=classification.get(""String_Node_Str"");
    if (bucketIdx > maxBucketIdx) {
      for (      int nSteps : steps.toArray()) {
        for (int i=maxBucketIdx; i < bucketIdx; i++) {
          weightMatrix.get(nSteps).addRow(new double[maxBucketIdx + 1]);
        }
      }
      maxBucketIdx=bucketIdx;
    }
    while (maxBucketIdx > actualValues.size() - 1) {
      actualValues.add(null);
    }
    if (actualValues.get(bucketIdx) == null) {
      actualValues.set(bucketIdx,(T)actValue);
    }
 else {
      if (Number.class.isAssignableFrom(actValue.getClass())) {
        Double val=((1.0 - actValueAlpha) * ((Number)actualValues.get(bucketIdx)).doubleValue() + actValueAlpha * ((Number)actValue).doubleValue());
        actualValues.set(bucketIdx,(T)val);
      }
 else {
        actualValues.set(bucketIdx,(T)actValue);
      }
    }
    int iteration=0;
    int[] learnPatternNZ=null;
    for (    Tuple t : patternNZHistory) {
      iteration=(int)t.get(0);
      learnPatternNZ=(int[])t.get(1);
      Map<Integer,double[]> error=calculateError(classification);
      int nSteps=learnIteration - iteration;
      if (steps.contains(nSteps)) {
        for (int row=0; row <= maxBucketIdx; row++) {
          for (          int bit : learnPatternNZ) {
            weightMatrix.get(nSteps).add(row,bit,alpha * error.get(nSteps)[row]);
          }
        }
      }
    }
  }
  if (infer && verbosity >= 1) {
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"" + Arrays.toString((T[])retVal.getActualValues()));
    for (    int key : retVal.stepSet()) {
      if (retVal.getActualValue(key) == null)       continue;
      Object[] actual=new Object[]{(T)retVal.getActualValue(key)};
      System.out.println(String.format(""String_Node_Str"",key,pFormatArray(actual)));
      int bestBucketIdx=retVal.getMostProbableBucketIndex(key);
      System.out.println(String.format(""String_Node_Str"",bestBucketIdx,retVal.getActualValue(bestBucketIdx)));
    }
  }
  return retVal;
}","/** 
 * Process one input sample. This method is called by outer loop code outside the nupic-engine. We  use this instead of the nupic engine compute() because our inputs and  outputs aren't fixed size vectors of reals. <p>
 * @param recordNum <p>Record number of this input pattern. Record numbers normally increase sequentially by 1 each time unless there are missing records in the dataset. Knowing this information ensures that we don't get confused by missing records.
 * @param classification <p>{@link Map} of the classification information:<p>&emsp;""bucketIdx"" - index of the encoder bucket <p>&emsp;""actValue"" -  actual value doing into the encoder
 * @param patternNZ <p>List of the active indices from the output below. When the output is from the TemporalMemory, this array should be the indices of the active cells.
 * @param learn <p>If true, learn this sample.
 * @param infer <p>If true, perform inference. If false, null will be returned.
 * @return {@link Classification} containing inference results if {@code learn} param is true,otherwise, will return  {@code null}. The Classification contains the computed probability distribution (relative likelihood for each bucketIdx starting from bucketIdx 0) for each step in   {@code steps}. Each bucket's likelihood can be accessed individually, or all the buckets' likelihoods can be obtained in the form of a double array. <pre>  {@code //Get likelihood val for bucket 0, 5 steps in future classification.getStat(5, 0); //Get all buckets' likelihoods as double[] where each //index is the likelihood for that bucket //(e.g. [0] contains likelihood for bucketIdx 0) classification.getStats(5);}</pre> The Classification also contains the average actual value for each bucket. The average values for the buckets can be accessed individually, or altogether as a double[]. <pre>  {@code //Get average actual val for bucket 0 classification.getActualValue(0); //Get average vals for all buckets as double[], where //each index is the average val for that bucket //(e.g. [0] contains average val for bucketIdx 0) classification.getActualValues();}</pre> The Classification can also be queried for the most probable bucket (the bucket with the highest associated likelihood value), as well as the average input value that corresponds to that bucket. <pre>  {@code //Get index of most probable bucket classification.getMostProbableBucketIndex(); //Get the average actual val for that bucket classification.getMostProbableValue();}</pre>
 */
@SuppressWarnings(""String_Node_Str"") public <T>Classification<T> compute(int recordNum,Map<String,Object> classification,int[] patternNZ,boolean learn,boolean infer){
  Classification<T> retVal=null;
  List<T> actualValues=(List<T>)this.actualValues;
  if (recordNumMinusLearnIteration == -1)   recordNumMinusLearnIteration=recordNum - learnIteration;
  learnIteration=recordNum - recordNumMinusLearnIteration;
  if (verbosity >= 1) {
    System.out.println(String.format(""String_Node_Str"",g_debugPrefix));
    System.out.printf(""String_Node_Str"",recordNum);
    System.out.printf(""String_Node_Str"",learnIteration);
    System.out.printf(""String_Node_Str"",patternNZ.length,ArrayUtils.intArrayToString(patternNZ));
    System.out.println(""String_Node_Str"" + classification);
  }
  patternNZHistory.append(new Tuple(learnIteration,patternNZ));
  if (ArrayUtils.max(patternNZ) > maxInputIdx) {
    int newMaxInputIdx=ArrayUtils.max(patternNZ);
    for (    int nSteps : steps.toArray()) {
      for (int i=maxInputIdx; i < newMaxInputIdx; i++) {
        weightMatrix.get(nSteps).addCol(new double[maxBucketIdx + 1]);
      }
    }
    maxInputIdx=newMaxInputIdx;
  }
  if (infer) {
    retVal=infer(patternNZ,classification);
  }
  if (learn && classification.get(""String_Node_Str"") != null) {
    int bucketIdx=(int)classification.get(""String_Node_Str"");
    Object actValue=classification.get(""String_Node_Str"");
    if (bucketIdx > maxBucketIdx) {
      for (      int nSteps : steps.toArray()) {
        for (int i=maxBucketIdx; i < bucketIdx; i++) {
          weightMatrix.get(nSteps).addRow(new double[maxInputIdx + 1]);
        }
      }
      maxBucketIdx=bucketIdx;
    }
    while (maxBucketIdx > actualValues.size() - 1) {
      actualValues.add(null);
    }
    if (actualValues.get(bucketIdx) == null) {
      actualValues.set(bucketIdx,(T)actValue);
    }
 else {
      if (Number.class.isAssignableFrom(actValue.getClass())) {
        Double val=((1.0 - actValueAlpha) * ((Number)actualValues.get(bucketIdx)).doubleValue() + actValueAlpha * ((Number)actValue).doubleValue());
        actualValues.set(bucketIdx,(T)val);
      }
 else {
        actualValues.set(bucketIdx,(T)actValue);
      }
    }
    int iteration=0;
    int[] learnPatternNZ=null;
    for (    Tuple t : patternNZHistory) {
      iteration=(int)t.get(0);
      learnPatternNZ=(int[])t.get(1);
      Map<Integer,double[]> error=calculateError(classification);
      int nSteps=learnIteration - iteration;
      if (steps.contains(nSteps)) {
        for (int row=0; row <= maxBucketIdx; row++) {
          for (          int bit : learnPatternNZ) {
            weightMatrix.get(nSteps).add(row,bit,alpha * error.get(nSteps)[row]);
          }
        }
      }
    }
  }
  if (infer && verbosity >= 1) {
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"" + Arrays.toString((T[])retVal.getActualValues()));
    for (    int key : retVal.stepSet()) {
      if (retVal.getActualValue(key) == null)       continue;
      Object[] actual=new Object[]{(T)retVal.getActualValue(key)};
      System.out.println(String.format(""String_Node_Str"",key,pFormatArray(actual)));
      int bestBucketIdx=retVal.getMostProbableBucketIndex(key);
      System.out.println(String.format(""String_Node_Str"",bestBucketIdx,retVal.getActualValue(bestBucketIdx)));
    }
  }
  return retVal;
}"
4477,"/** 
 * Return the inference value from one input sample. The actual learning happens in compute().
 * @param patternNZ int[] of the active indices from the output below
 * @param classification {@link Map} of the classification information:<p>&emsp;""bucketIdx"" - index of the encoder bucket <p>&emsp;""actValue"" -  actual value doing into the encoder
 * @return {@link Classification} containing inference results. The Classificationcontains the computed probability distribution (relative likelihood for each bucketIdx starting from bucketIdx 0) for each step in  {@code steps}. The Classification also contains the average actual value for each bucket.
 */
private <T>Classification<T> infer(int[] patternNZ,Map<String,Object> classification){
  Classification<T> retVal=new Classification<T>();
  Object defaultValue=null;
  if (steps.get(0) == 0 || classification == null) {
    defaultValue=0;
  }
 else {
    defaultValue=classification.get(""String_Node_Str"");
  }
  T[] actValues=(T[])new Object[this.actualValues.size()];
  for (int i=0; i < actualValues.size(); i++) {
    actValues[i]=(T)(actValues[i] == null ? defaultValue : actualValues.get(i));
  }
  retVal.setActualValues(actValues);
  for (  int nSteps : steps.toArray()) {
    double[] predictDist=inferSingleStep(patternNZ,weightMatrix.get(nSteps));
    retVal.setStats(nSteps,predictDist);
  }
  return retVal;
}","/** 
 * Return the inference value from one input sample. The actual learning happens in compute().
 * @param patternNZ int[] of the active indices from the output below
 * @param classification {@link Map} of the classification information:<p>&emsp;""bucketIdx"" - index of the encoder bucket <p>&emsp;""actValue"" -  actual value doing into the encoder
 * @return {@link Classification} containing inference results. The Classificationcontains the computed probability distribution (relative likelihood for each bucketIdx starting from bucketIdx 0) for each step in  {@code steps}. The Classification also contains the average actual value for each bucket.
 */
private <T>Classification<T> infer(int[] patternNZ,Map<String,Object> classification){
  Classification<T> retVal=new Classification<T>();
  Object defaultValue=null;
  if (steps.get(0) == 0 || classification == null) {
    defaultValue=0;
  }
 else {
    defaultValue=classification.get(""String_Node_Str"");
  }
  T[] actValues=(T[])new Object[this.actualValues.size()];
  for (int i=0; i < actualValues.size(); i++) {
    actValues[i]=(T)(actualValues.get(i) == null ? defaultValue : actualValues.get(i));
  }
  retVal.setActualValues(actValues);
  for (  int nSteps : steps.toArray()) {
    double[] predictDist=inferSingleStep(patternNZ,weightMatrix.get(nSteps));
    retVal.setStats(nSteps,predictDist);
  }
  return retVal;
}"
4478,"/** 
 * Perform inference for a single step. Given an SDR input and a weight matrix, return a predicted distribution.
 * @param patternNZ int[] of the active indices from the output below
 * @param weightMatrix FlexCompColMatrix weight matrix
 * @return double[] of the predicted class label distribution
 */
private double[] inferSingleStep(int[] patternNZ,FlexCompRowMatrix weightMatrix){
  double[] outputActivation=new double[maxBucketIdx + 1];
  for (int row=0; row <= maxBucketIdx; row++) {
    for (    int bit : patternNZ) {
      outputActivation[row]+=weightMatrix.get(row,bit);
    }
  }
  double[] expOutputActivation=new double[outputActivation.length];
  for (int i=0; i < expOutputActivation.length; i++) {
    expOutputActivation[i]=Math.exp(outputActivation[i]);
  }
  double[] predictDist=new double[outputActivation.length];
  for (int i=0; i < predictDist.length; i++) {
    predictDist[i]=outputActivation[i] / ArrayUtils.sum(expOutputActivation);
  }
  return predictDist;
}","/** 
 * Perform inference for a single step. Given an SDR input and a weight matrix, return a predicted distribution.
 * @param patternNZ int[] of the active indices from the output below
 * @param weightMatrix FlexCompColMatrix weight matrix
 * @return double[] of the predicted class label distribution
 */
private double[] inferSingleStep(int[] patternNZ,FlexCompRowMatrix weightMatrix){
  double[] outputActivation=new double[maxBucketIdx + 1];
  for (int row=0; row <= maxBucketIdx; row++) {
    for (    int bit : patternNZ) {
      outputActivation[row]+=weightMatrix.get(row,bit);
    }
  }
  double[] expOutputActivation=new double[outputActivation.length];
  for (int i=0; i < expOutputActivation.length; i++) {
    expOutputActivation[i]=Math.exp(outputActivation[i]);
  }
  double[] predictDist=new double[outputActivation.length];
  for (int i=0; i < predictDist.length; i++) {
    predictDist[i]=expOutputActivation[i] / ArrayUtils.sum(expOutputActivation);
  }
  return predictDist;
}"
4479,"/** 
 * Called internally to invoke the   {@link SpatialPooler}
 * @param input
 * @return
 */
protected int[] spatialInput(int[] input){
  if (input == null) {
    LOGGER.info(""String_Node_Str"".concat(getName()).concat(""String_Node_Str""));
  }
 else   if (input.length < 1) {
    LOGGER.info(""String_Node_Str"".concat(getName()).concat(""String_Node_Str""));
    return input;
  }
  spatialPooler.compute(connections,input,feedForwardActiveColumns,sensor == null || sensor.getMetaInfo().isLearn(),isLearn);
  return feedForwardActiveColumns;
}","/** 
 * Called internally to invoke the   {@link SpatialPooler}
 * @param input
 * @return
 */
protected int[] spatialInput(int[] input){
  if (input == null) {
    LOGGER.info(""String_Node_Str"".concat(getName()).concat(""String_Node_Str""));
  }
 else   if (input.length < 1) {
    LOGGER.info(""String_Node_Str"".concat(getName()).concat(""String_Node_Str""));
    return input;
  }
  int[] activeColumns=new int[feedForwardActiveColumns.length];
  spatialPooler.compute(connections,input,activeColumns,sensor == null || sensor.getMetaInfo().isLearn(),isLearn);
  return feedForwardActiveColumns=activeColumns;
}"
4480,"/** 
 * Store a new item in our history. <p> This gets called for a bit whenever it is active and learning is enabled <p> Save duty cycle by normalizing it to the same iteration as the rest of the duty cycles which is lastTotalUpdate. <p> This is done to speed up computation in inference since all of the duty cycles can now be scaled by a single number. <p> The duty cycle is brought up to the current iteration only at inference and only when one of the duty cycles gets too large (to avoid overflow to larger data type) since the ratios between the duty cycles are what is important. As long as all of the duty cycles are at the same iteration their ratio is the same as it would be for any other iteration, because the update is simply a multiplication by a scalar that depends on the number of steps between the last update of the duty cycle and the current iteration.
 * @param iteration		the learning iteration number, which is only incrementedwhen learning is enabled
 * @param bucketIdx		the bucket index to store
 */
public void store(int iteration,int bucketIdx){
  if (lastTotalUpdate == -1) {
    lastTotalUpdate=iteration;
  }
  int statsLen=stats.size() - 1;
  if (bucketIdx > statsLen) {
    stats.add(new double[bucketIdx - statsLen]);
  }
  double dc=stats.get(bucketIdx);
  double denom=Math.pow((1.0 - classifier.alpha),(iteration - lastTotalUpdate));
  double dcNew=0;
  if (denom > 0)   dcNew=dc + (classifier.alpha / denom);
  if (denom == 0 || dcNew > DUTY_CYCLE_UPDATE_INTERVAL) {
    double exp=Math.pow((1.0 - classifier.alpha),(iteration - lastTotalUpdate));
    double dcT=0;
    for (int i=0; i < stats.size(); i++) {
      dcT*=exp;
      stats.set(i,dcT);
    }
    lastTotalUpdate=iteration;
    dc=stats.get(bucketIdx) + classifier.alpha;
  }
 else {
    dc=dcNew;
  }
  stats.set(bucketIdx,dc);
  if (classifier.verbosity >= 2) {
    System.out.println(String.format(""String_Node_Str"",id,bucketIdx,dc));
  }
}","/** 
 * Store a new item in our history. <p> This gets called for a bit whenever it is active and learning is enabled <p> Save duty cycle by normalizing it to the same iteration as the rest of the duty cycles which is lastTotalUpdate. <p> This is done to speed up computation in inference since all of the duty cycles can now be scaled by a single number. <p> The duty cycle is brought up to the current iteration only at inference and only when one of the duty cycles gets too large (to avoid overflow to larger data type) since the ratios between the duty cycles are what is important. As long as all of the duty cycles are at the same iteration their ratio is the same as it would be for any other iteration, because the update is simply a multiplication by a scalar that depends on the number of steps between the last update of the duty cycle and the current iteration.
 * @param iteration		the learning iteration number, which is only incrementedwhen learning is enabled
 * @param bucketIdx		the bucket index to store
 */
public void store(int iteration,int bucketIdx){
  if (lastTotalUpdate == -1) {
    lastTotalUpdate=iteration;
  }
  int statsLen=stats.size() - 1;
  if (bucketIdx > statsLen) {
    stats.add(new double[bucketIdx - statsLen]);
  }
  double dc=stats.get(bucketIdx);
  double denom=Math.pow((1.0 - classifier.alpha),(iteration - lastTotalUpdate));
  double dcNew=0;
  if (denom > 0)   dcNew=dc + (classifier.alpha / denom);
  if (denom == 0 || dcNew > DUTY_CYCLE_UPDATE_INTERVAL) {
    double exp=Math.pow((1.0 - classifier.alpha),(iteration - lastTotalUpdate));
    double dcT=0;
    for (int i=0; i < stats.size(); i++) {
      dcT=stats.get(i);
      dcT*=exp;
      stats.set(i,dcT);
    }
    lastTotalUpdate=iteration;
    dc=stats.get(bucketIdx) + classifier.alpha;
  }
 else {
    dc=dcNew;
  }
  stats.set(bucketIdx,dc);
  if (classifier.verbosity >= 2) {
    System.out.println(String.format(""String_Node_Str"",id,bucketIdx,dc));
  }
}"
4481,"/** 
 * Given an input field width and Spatial Pooler dimensionality; this method will return an array of dimension sizes whose number is equal to the number of column dimensions. The sum of the returned dimensions will be equal to the flat input field width specified. This method should be called when a disparity in dimensionality between the input field and the number of column dimensions is detected. Otherwise if the input field dimensionality is correctly specified, this method should <b>not</b> be used.
 * @param inputWidth        the flat input width of an {@link Encoder}'s output or the vector used as input to the   {@link SpatialPooler}
 * @param numColumnDims     a number specifying the number of column dimensions thatshould be returned.
 * @return
 */
public int[] inferInputDimensions(int inputWidth,int numColumnDims){
  double flatSize=inputWidth;
  double numColDims=numColumnDims;
  double sliceArrangement=Math.pow(flatSize,1 / numColDims);
  double remainder=sliceArrangement % (int)sliceArrangement;
  int[] retVal=new int[(int)numColDims];
  if (remainder > 0) {
    for (int i=0; i < numColDims - 1; i++)     retVal[i]=1;
    retVal[(int)numColDims - 1]=(int)flatSize;
  }
 else {
    for (int i=0; i < numColDims; i++)     retVal[i]=(int)sliceArrangement;
  }
  return retVal;
}","/** 
 * Given an input field width and Spatial Pooler dimensionality; this method will return an array of dimension sizes whose number is equal to the number of column dimensions. The sum of the returned dimensions will be equal to the flat input field width specified. This method should be called when a disparity in dimensionality between the input field and the number of column dimensions is detected. Otherwise if the input field dimensionality is correctly specified, this method should <b>not</b> be used.
 * @param inputWidth        the flat input width of an {@link Encoder}'s output or the vector used as input to the   {@link SpatialPooler}
 * @param numDims           a number specifying the number of dimensions thatshould be returned.
 * @return
 */
public int[] inferInputDimensions(int inputWidth,int numDims){
  double flatSize=inputWidth;
  double numColDims=numDims;
  int[] retVal=new int[(int)numColDims];
  BigDecimal log=new BigDecimal(Math.log10(flatSize));
  BigDecimal dimensions=new BigDecimal(numColDims);
  double sliceArrangement=new BigDecimal(Math.pow(10,log.divide(dimensions).doubleValue()),MathContext.DECIMAL32).doubleValue();
  double remainder=sliceArrangement % (int)sliceArrangement;
  if (remainder > 0) {
    for (int i=0; i < numColDims - 1; i++)     retVal[i]=1;
    retVal[(int)numColDims - 1]=(int)flatSize;
  }
 else {
    for (int i=0; i < numColDims; i++)     retVal[i]=(int)sliceArrangement;
  }
  return retVal;
}"
4482,"/** 
 * Temporary test to test basic sequence mechanisms
 */
@Test public void testBasicSetup_SpatialPooler_AUTO_MODE(){
  Sensor<File> sensor=Sensor.create(FileSensor::create,SensorParams.create(Keys::path,""String_Node_Str"",ResourceLocator.path(""String_Node_Str"")));
  Parameters p=NetworkTestHarness.getParameters().copy();
  p=p.union(NetworkTestHarness.getDayDemoTestEncoderParams());
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  p.setParameterByKey(KEY.AUTO_CLASSIFY,Boolean.TRUE);
  HTMSensor<File> htmSensor=(HTMSensor<File>)sensor;
  Network n=Network.create(""String_Node_Str"",p);
  Layer<int[]> l=new PALayer<>(n);
  l.add(htmSensor).add(new PASpatialPooler());
  final int[] expected0=new int[]{0,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,1,0,0,0};
  final int[] expected1=new int[]{0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0};
  l.subscribe(new Observer<Inference>(){
    int test=0;
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference spatialPoolerOutput){
      if (test == 0) {
        assertTrue(Arrays.equals(expected0,spatialPoolerOutput.getSDR()));
      }
      if (test == 1) {
        assertTrue(Arrays.equals(expected1,spatialPoolerOutput.getSDR()));
      }
      ++test;
    }
  }
);
  l.start();
  try {
    l.getLayerThread().join();
  }
 catch (  Exception e) {
    e.printStackTrace();
  }
}","/** 
 * Temporary test to test basic sequence mechanisms
 */
@Test public void testBasicSetup_SpatialPooler_AUTO_MODE(){
  Sensor<File> sensor=Sensor.create(FileSensor::create,SensorParams.create(Keys::path,""String_Node_Str"",ResourceLocator.path(""String_Node_Str"")));
  Parameters p=NetworkTestHarness.getParameters().copy();
  p=p.union(NetworkTestHarness.getDayDemoTestEncoderParams());
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  p.setParameterByKey(KEY.AUTO_CLASSIFY,Boolean.TRUE);
  HTMSensor<File> htmSensor=(HTMSensor<File>)sensor;
  Network n=Network.create(""String_Node_Str"",p);
  Layer<int[]> l=new PALayer<>(n);
  l.add(htmSensor).add(new PASpatialPooler());
  final int[] expected0=new int[]{0,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,1,0,0,0};
  final int[] expected1=new int[]{0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,1,0,0,0};
  l.subscribe(new Observer<Inference>(){
    int test=0;
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference spatialPoolerOutput){
      if (test == 0) {
        assertTrue(Arrays.equals(expected0,spatialPoolerOutput.getSDR()));
      }
      if (test == 1) {
        assertTrue(Arrays.equals(expected1,spatialPoolerOutput.getSDR()));
      }
      ++test;
    }
  }
);
  l.start();
  try {
    l.getLayerThread().join();
  }
 catch (  Exception e) {
    e.printStackTrace();
  }
}"
4483,"@Test public void testSpatialPoolerPrimerDelay(){
  Parameters p=NetworkTestHarness.getParameters().copy();
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  int[][] inputs=new int[7][8];
  inputs[0]=new int[]{1,1,0,0,0,0,0,1};
  inputs[1]=new int[]{1,1,1,0,0,0,0,0};
  inputs[2]=new int[]{0,1,1,1,0,0,0,0};
  inputs[3]=new int[]{0,0,1,1,1,0,0,0};
  inputs[4]=new int[]{0,0,0,1,1,1,0,0};
  inputs[5]=new int[]{0,0,0,0,1,1,1,0};
  inputs[6]=new int[]{0,0,0,0,0,1,1,1};
  final int[] expected0=new int[]{0,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,1,0,0,0};
  final int[] expected1=new int[]{0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0};
  Layer<int[]> l=new PALayer<>(p,null,new PASpatialPooler(),null,null,null);
  l.subscribe(new Observer<Inference>(){
    int test=0;
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference i){
      if (test == 0) {
        assertTrue(Arrays.equals(expected0,i.getSDR()));
      }
      if (test == 1) {
        assertTrue(Arrays.equals(expected1,i.getSDR()));
      }
      ++test;
    }
  }
);
  l.compute(inputs[0]);
  l.compute(inputs[1]);
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  p.setParameterByKey(KEY.SP_PRIMER_DELAY,1);
  Layer<int[]> l2=new PALayer<>(p,null,new PASpatialPooler(),null,null,null);
  l2.subscribe(new Observer<Inference>(){
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference i){
      assertTrue(Arrays.equals(expected1,i.getSDR()));
    }
  }
);
  l2.compute(inputs[0]);
  l2.compute(inputs[1]);
}","@Test public void testSpatialPoolerPrimerDelay(){
  Parameters p=NetworkTestHarness.getParameters().copy();
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  int[][] inputs=new int[7][8];
  inputs[0]=new int[]{1,1,0,0,0,0,0,1};
  inputs[1]=new int[]{1,1,1,0,0,0,0,0};
  inputs[2]=new int[]{0,1,1,1,0,0,0,0};
  inputs[3]=new int[]{0,0,1,1,1,0,0,0};
  inputs[4]=new int[]{0,0,0,1,1,1,0,0};
  inputs[5]=new int[]{0,0,0,0,1,1,1,0};
  inputs[6]=new int[]{0,0,0,0,0,1,1,1};
  final int[] expected0=new int[]{0,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,1,0,0,0};
  final int[] expected1=new int[]{0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,1,0,0,0};
  Layer<int[]> l=new PALayer<>(p,null,new PASpatialPooler(),null,null,null);
  l.subscribe(new Observer<Inference>(){
    int test=0;
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference i){
      if (test == 0) {
        assertTrue(Arrays.equals(expected0,i.getSDR()));
      }
      if (test == 1) {
        assertTrue(Arrays.equals(expected1,i.getSDR()));
      }
      ++test;
    }
  }
);
  l.compute(inputs[0]);
  l.compute(inputs[1]);
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  p.setParameterByKey(KEY.SP_PRIMER_DELAY,1);
  Layer<int[]> l2=new PALayer<>(p,null,new PASpatialPooler(),null,null,null);
  l2.subscribe(new Observer<Inference>(){
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference i){
      assertTrue(Arrays.equals(expected1,i.getSDR()));
    }
  }
);
  l2.compute(inputs[0]);
  l2.compute(inputs[1]);
}"
4484,"/** 
 * Temporary test to test basic sequence mechanisms
 */
@Test public void testBasicSetup_SpatialPooler_MANUAL_MODE(){
  Parameters p=NetworkTestHarness.getParameters().copy();
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  int[][] inputs=new int[7][8];
  inputs[0]=new int[]{1,1,0,0,0,0,0,1};
  inputs[1]=new int[]{1,1,1,0,0,0,0,0};
  inputs[2]=new int[]{0,1,1,1,0,0,0,0};
  inputs[3]=new int[]{0,0,1,1,1,0,0,0};
  inputs[4]=new int[]{0,0,0,1,1,1,0,0};
  inputs[5]=new int[]{0,0,0,0,1,1,1,0};
  inputs[6]=new int[]{0,0,0,0,0,1,1,1};
  final int[] expected0=new int[]{0,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,1,0,0,0};
  final int[] expected1=new int[]{0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0};
  Layer<int[]> l=new PALayer<>(p,null,new PASpatialPooler(),null,null,null);
  l.subscribe(new Observer<Inference>(){
    int test=0;
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference spatialPoolerOutput){
      if (test == 0) {
        assertTrue(Arrays.equals(expected0,spatialPoolerOutput.getSDR()));
      }
      if (test == 1) {
        assertTrue(Arrays.equals(expected1,spatialPoolerOutput.getSDR()));
      }
      ++test;
    }
  }
);
  l.compute(inputs[0]);
  l.compute(inputs[1]);
}","/** 
 * Temporary test to test basic sequence mechanisms
 */
@Test public void testBasicSetup_SpatialPooler_MANUAL_MODE(){
  Parameters p=NetworkTestHarness.getParameters().copy();
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  int[][] inputs=new int[7][8];
  inputs[0]=new int[]{1,1,0,0,0,0,0,1};
  inputs[1]=new int[]{1,1,1,0,0,0,0,0};
  inputs[2]=new int[]{0,1,1,1,0,0,0,0};
  inputs[3]=new int[]{0,0,1,1,1,0,0,0};
  inputs[4]=new int[]{0,0,0,1,1,1,0,0};
  inputs[5]=new int[]{0,0,0,0,1,1,1,0};
  inputs[6]=new int[]{0,0,0,0,0,1,1,1};
  final int[] expected0=new int[]{0,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,1,0,0,0};
  final int[] expected1=new int[]{0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,1,0,0,0};
  Layer<int[]> l=new PALayer<>(p,null,new PASpatialPooler(),null,null,null);
  l.subscribe(new Observer<Inference>(){
    int test=0;
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference spatialPoolerOutput){
      if (test == 0) {
        assertTrue(Arrays.equals(expected0,spatialPoolerOutput.getSDR()));
      }
      if (test == 1) {
        assertTrue(Arrays.equals(expected1,spatialPoolerOutput.getSDR()));
      }
      ++test;
    }
  }
);
  l.compute(inputs[0]);
  l.compute(inputs[1]);
}"
4485,"@Test public void testLayerWithGenericObservable(){
  Parameters p=NetworkTestHarness.getParameters().copy();
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  int[][] inputs=new int[7][8];
  inputs[0]=new int[]{1,1,0,0,0,0,0,1};
  inputs[1]=new int[]{1,1,1,0,0,0,0,0};
  inputs[2]=new int[]{0,1,1,1,0,0,0,0};
  inputs[3]=new int[]{0,0,1,1,1,0,0,0};
  inputs[4]=new int[]{0,0,0,1,1,1,0,0};
  inputs[5]=new int[]{0,0,0,0,1,1,1,0};
  inputs[6]=new int[]{0,0,0,0,0,1,1,1};
  final int[] expected0=new int[]{0,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,1,0,0,0};
  final int[] expected1=new int[]{0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0};
  Func1<ManualInput,ManualInput> addedFunc=l -> {
    return l.customObject(""String_Node_Str"" + Arrays.toString(l.getSDR()));
  }
;
  Network n=Network.create(""String_Node_Str"",p).add(Network.createRegion(""String_Node_Str"").add(Network.createPALayer(""String_Node_Str"",p).add(addedFunc).add(new PASpatialPooler())));
  @SuppressWarnings(""String_Node_Str"") Layer<int[]> l=(PALayer<int[]>)n.lookup(""String_Node_Str"").lookup(""String_Node_Str"");
  l.subscribe(new Observer<Inference>(){
    int test=0;
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference i){
      if (test == 0) {
        assertTrue(Arrays.equals(expected0,i.getSDR()));
        assertEquals(""String_Node_Str"",i.getCustomObject());
      }
      if (test == 1) {
        assertTrue(Arrays.equals(expected1,i.getSDR()));
        assertEquals(""String_Node_Str"",i.getCustomObject());
      }
      ++test;
    }
  }
);
  l.compute(inputs[0]);
  l.compute(inputs[1]);
}","@Test public void testLayerWithGenericObservable(){
  Parameters p=NetworkTestHarness.getParameters().copy();
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  int[][] inputs=new int[7][8];
  inputs[0]=new int[]{1,1,0,0,0,0,0,1};
  inputs[1]=new int[]{1,1,1,0,0,0,0,0};
  inputs[2]=new int[]{0,1,1,1,0,0,0,0};
  inputs[3]=new int[]{0,0,1,1,1,0,0,0};
  inputs[4]=new int[]{0,0,0,1,1,1,0,0};
  inputs[5]=new int[]{0,0,0,0,1,1,1,0};
  inputs[6]=new int[]{0,0,0,0,0,1,1,1};
  final int[] expected0=new int[]{0,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,1,0,0,0};
  final int[] expected1=new int[]{0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,1,0,0,0};
  Func1<ManualInput,ManualInput> addedFunc=l -> {
    return l.customObject(""String_Node_Str"" + Arrays.toString(l.getSDR()));
  }
;
  Network n=Network.create(""String_Node_Str"",p).add(Network.createRegion(""String_Node_Str"").add(Network.createPALayer(""String_Node_Str"",p).add(addedFunc).add(new PASpatialPooler())));
  @SuppressWarnings(""String_Node_Str"") Layer<int[]> l=(PALayer<int[]>)n.lookup(""String_Node_Str"").lookup(""String_Node_Str"");
  l.subscribe(new Observer<Inference>(){
    int test=0;
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference i){
      System.out.println(""String_Node_Str"" + Arrays.toString(i.getSDR()));
      if (test == 0) {
        assertTrue(Arrays.equals(expected0,i.getSDR()));
        assertEquals(""String_Node_Str"",i.getCustomObject());
      }
      if (test == 1) {
        assertTrue(Arrays.equals(expected1,i.getSDR()));
        assertEquals(""String_Node_Str"",i.getCustomObject());
      }
      ++test;
    }
  }
);
  l.compute(inputs[0]);
  l.compute(inputs[1]);
}"
4486,"/** 
 * Checks the anomaly score against a known threshold which indicates that the TM predictions have been ""warmed up"". When there have been enough occurrences within the threshold, we conclude that the algorithms have been adequately stabilized.
 * @param l                 the {@link PALayer} in question
 * @param anomalyScore      the current anomaly score
 * @return
 */
private boolean isWarmedUp(Layer<Map<String,Object>> l,double anomalyScore){
  if (anomalyScore >= 0 && anomalyScore <= 0.1) {
    ++timesWithinThreshold;
  }
 else {
    timesWithinThreshold=0;
  }
  if (timesWithinThreshold > 50) {
    return true;
  }
  return false;
}","/** 
 * Checks the anomaly score against a known threshold which indicates that the TM predictions have been ""warmed up"". When there have been enough occurrences within the threshold, we conclude that the algorithms have been adequately stabilized.
 * @param l                 the {@link PALayer} in question
 * @param anomalyScore      the current anomaly score
 * @return
 */
private boolean isWarmedUp(Layer<Map<String,Object>> l,double anomalyScore){
  if (anomalyScore <= 0.1) {
    ++timesWithinThreshold;
  }
 else {
    timesWithinThreshold=0;
  }
  if (timesWithinThreshold > 50) {
    return true;
  }
  return false;
}"
4487,"/** 
 * Constructs a new   {@code Synapse}
 * @param c             the connections state of the temporal memory
 * @param sourceCell    the {@link Cell} which will activate this {@code Synapse}; Null if this Synapse is proximal
 * @param segment       the owning dendritic segment
 * @param pool		    this {@link Pool} of which this synapse is a member
 * @param index         this {@code Synapse}'s index
 * @param inputIndex	the index of this {@link Synapse}'s input; be it a Cell or InputVector bit.
 */
public Synapse(Connections c,Cell sourceCell,Segment segment,Pool pool,int index,int inputIndex){
  this.sourceCell=sourceCell;
  this.segment=segment;
  this.pool=pool;
  this.synapseIndex=index;
  this.inputIndex=inputIndex;
  hashcode=hashCode();
  if (sourceCell != null) {
    sourceCell.addReceptorSynapse(c,this);
  }
}","/** 
 * Constructs a new   {@code Synapse}
 * @param c             the connections state of the temporal memory
 * @param sourceCell    the {@link Cell} which will activate this {@code Synapse}; Null if this Synapse is proximal
 * @param segment       the owning dendritic segment
 * @param pool		    this {@link Pool} of which this synapse is a member
 * @param index         this {@code Synapse}'s index
 * @param inputIndex	the index of this {@link Synapse}'s input; be it a Cell or InputVector bit.
 */
public Synapse(Connections c,Cell sourceCell,Segment segment,Pool pool,int index,int inputIndex){
  this.sourceCell=sourceCell;
  this.segment=segment;
  this.pool=pool;
  this.synapseIndex=index;
  this.inputIndex=inputIndex;
  if (sourceCell != null) {
    sourceCell.addReceptorSynapse(c,this);
  }
}"
4488,"/** 
 * Connects the first observable which does the transformation of input  types, to the rest of the sequence - then clears the helper map and sets it to null.
 * @param t
 */
private void completeDispatch(T t){
  Observable<ManualInput> sequence=resolveObservableSequence(t);
  sequence=mapEncoderBuckets(sequence);
  sequence=fillInSequence(sequence);
  subscribers.add(0,getDelegateObserver());
  subscription=sequence.subscribe(getDelegateSubscriber());
  observableDispatch.clear();
  observableDispatch=null;
  if (sensor == null) {
    sensor=parentNetwork == null ? null : parentNetwork.getSensor();
  }
 else   if (parentNetwork != null) {
    parentNetwork.setSensor(sensor);
  }
}","/** 
 * Connects the first observable which does the transformation of input  types, to the rest of the sequence - then clears the helper map and sets it to null.
 * @param t
 */
private void completeDispatch(T t){
  Observable<ManualInput> sequence=resolveObservableSequence(t);
  sequence=mapEncoderBuckets(sequence);
  sequence=fillInSequence(sequence);
  subscribers.add(getDelegateObserver());
  subscription=sequence.subscribe(getDelegateSubscriber());
  observableDispatch.clear();
  observableDispatch=null;
  if (sensor == null) {
    sensor=parentNetwork == null ? null : parentNetwork.getSensor();
  }
 else   if (parentNetwork != null) {
    parentNetwork.setSensor(sensor);
  }
}"
4489,"@Test public void testBasicSetupEncoder_AUTO_MODE(){
  Sensor<File> sensor=Sensor.create(FileSensor::create,SensorParams.create(Keys::path,""String_Node_Str"",ResourceLocator.path(""String_Node_Str"")));
  Parameters p=NetworkTestHarness.getParameters();
  p=p.union(NetworkTestHarness.getHotGymTestEncoderParams());
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  p.setParameterByKey(KEY.AUTO_CLASSIFY,Boolean.TRUE);
  HTMSensor<File> htmSensor=(HTMSensor<File>)sensor;
  Network n=Network.create(""String_Node_Str"",p);
  Layer<int[]> l=new Layer<>(n);
  l.add(htmSensor);
  int[][] expected=new int[][]{{0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1},{0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1},{0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0}};
  l.start();
  l.subscribe(new Observer<Inference>(){
    int seq=0;
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference output){
      assertTrue(Arrays.equals(expected[seq++],output.getSDR()));
    }
  }
);
  l.subscribe(new Observer<Inference>(){
    int seq=0;
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference output){
      assertTrue(Arrays.equals(expected[seq++],output.getSDR()));
    }
  }
);
  try {
    l.getLayerThread().join();
  }
 catch (  Exception e) {
    e.printStackTrace();
  }
}","@Test public void testBasicSetupEncoder_AUTO_MODE(){
  Sensor<File> sensor=Sensor.create(FileSensor::create,SensorParams.create(Keys::path,""String_Node_Str"",ResourceLocator.path(""String_Node_Str"")));
  Parameters p=NetworkTestHarness.getParameters();
  p=p.union(NetworkTestHarness.getHotGymTestEncoderParams());
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  p.setParameterByKey(KEY.AUTO_CLASSIFY,Boolean.TRUE);
  HTMSensor<File> htmSensor=(HTMSensor<File>)sensor;
  Network n=Network.create(""String_Node_Str"",p);
  Layer<int[]> l=new Layer<>(n);
  l.add(htmSensor);
  int[][] expected=new int[][]{{0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1},{0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1},{0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0},{1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0},{0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0}};
  l.start();
  l.subscribe(new Observer<Inference>(){
    int seq=0;
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference output){
      System.out.println(""String_Node_Str"" + seq + ""String_Node_Str""+ Arrays.toString(expected[seq]));
      System.out.println(""String_Node_Str"" + seq + ""String_Node_Str""+ Arrays.toString(output.getSDR()));
      assertTrue(Arrays.equals(expected[seq++],output.getSDR()));
    }
  }
);
  l.subscribe(new Observer<Inference>(){
    int seq2=0;
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference output){
      System.out.println(""String_Node_Str"" + seq2 + ""String_Node_Str""+ Arrays.toString(expected[seq2]));
      System.out.println(""String_Node_Str"" + seq2 + ""String_Node_Str""+ Arrays.toString(output.getSDR()));
      assertTrue(Arrays.equals(expected[seq2++],output.getSDR()));
    }
  }
);
  try {
    l.getLayerThread().join();
  }
 catch (  Exception e) {
    e.printStackTrace();
  }
}"
4490,"/** 
 * Temporary test to test basic sequence mechanisms
 */
@Test public void testBasicSetup_TemporalMemory_MANUAL_MODE(){
  Parameters p=NetworkTestHarness.getParameters();
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  final int[] input1=new int[]{0,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,1,0,0,0};
  final int[] input2=new int[]{0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0};
  final int[] input3=new int[]{0,0,1,1,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0};
  final int[] input4=new int[]{0,0,1,1,0,0,0,0,1,1,0,0,1,0,0,0,0,1,1,0};
  final int[] input5=new int[]{0,0,1,1,0,0,0,0,1,0,0,0,1,0,0,0,0,1,1,0};
  final int[] input6=new int[]{0,0,1,0,0,0,0,1,1,1,0,0,0,0,0,0,0,1,1,1};
  final int[] input7=new int[]{0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,1,1,0};
  final int[][] inputs={input1,input2,input3,input4,input5,input6,input7};
  int[] expected1={1,5,11,12,13};
  int[] expected2={2,3,11,12,13,14};
  int[] expected3={2,3,8,9,12,17,18};
  int[] expected4={1,2,3,5,7,8,11,12,16,17,18};
  int[] expected5={2,7,8,9,17,18,19};
  int[] expected6={1,7,8,9,17,18};
  int[] expected7={1,5,7,11,12,16};
  final int[][] expecteds={expected1,expected2,expected3,expected4,expected5,expected6,expected7};
  Layer<int[]> l=new Layer<>(p,null,null,new TemporalMemory(),null,null);
  int timeUntilStable=415;
  l.subscribe(new Observer<Inference>(){
    int test=0;
    int seq=0;
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference output){
      if (seq / 7 >= timeUntilStable) {
        assertTrue(Arrays.equals(expecteds[test],output.getSDR()));
      }
      if (test == 6)       test=0;
 else       test++;
      seq++;
    }
  }
);
  for (int j=0; j < timeUntilStable; j++) {
    for (int i=0; i < inputs.length; i++) {
      l.compute(inputs[i]);
    }
  }
  for (int j=0; j < 2; j++) {
    for (int i=0; i < inputs.length; i++) {
      l.compute(inputs[i]);
    }
  }
}","/** 
 * Temporary test to test basic sequence mechanisms
 */
@Test public void testBasicSetup_TemporalMemory_MANUAL_MODE(){
  Parameters p=NetworkTestHarness.getParameters();
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  final int[] input1=new int[]{0,1,0,0,0,1,0,1,0,0,0,1,1,0,0,0,1,0,0,0};
  final int[] input2=new int[]{0,1,0,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0};
  final int[] input3=new int[]{0,0,1,1,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0};
  final int[] input4=new int[]{0,0,1,1,0,0,0,0,1,1,0,0,1,0,0,0,0,1,1,0};
  final int[] input5=new int[]{0,0,1,1,0,0,0,0,1,0,0,0,1,0,0,0,0,1,1,0};
  final int[] input6=new int[]{0,0,1,0,0,0,0,1,1,1,0,0,0,0,0,0,0,1,1,1};
  final int[] input7=new int[]{0,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,1,1,0};
  final int[][] inputs={input1,input2,input3,input4,input5,input6,input7};
  int[] expected1={1,5,11,12,13};
  int[] expected2={2,3,11,12,13,14};
  int[] expected3={2,3,8,9,12,17,18};
  int[] expected4={2,3,8,12,17,18};
  int[] expected5={2,7,8,9,17,18,19};
  int[] expected6={1,7,8,9,17,18};
  int[] expected7={1,5,7,11,12,16};
  final int[][] expecteds={expected1,expected2,expected3,expected4,expected5,expected6,expected7};
  Layer<int[]> l=new Layer<>(p,null,null,new TemporalMemory(),null,null);
  int timeUntilStable=200;
  l.subscribe(new Observer<Inference>(){
    int test=0;
    int seq=0;
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      e.printStackTrace();
    }
    @Override public void onNext(    Inference output){
      if (seq / 7 >= timeUntilStable) {
        System.out.println(""String_Node_Str"" + (seq) + ""String_Node_Str""+ (test)+ ""String_Node_Str""+ Arrays.toString(output.getSDR()));
        assertTrue(Arrays.equals(expecteds[test],output.getSDR()));
      }
      if (test == 6)       test=0;
 else       test++;
      seq++;
    }
  }
);
  for (int j=0; j < timeUntilStable; j++) {
    for (int i=0; i < inputs.length; i++) {
      l.compute(inputs[i]);
    }
  }
  for (int j=0; j < 2; j++) {
    for (int i=0; i < inputs.length; i++) {
      l.compute(inputs[i]);
    }
  }
}"
4491,"@Test public void testGetAllPredictions(){
  final int PRIME_COUNT=35;
  final int NUM_CYCLES=199;
  final int INPUT_GROUP_COUNT=7;
  TOTAL=0;
  Parameters p=NetworkTestHarness.getParameters();
  p=p.union(NetworkTestHarness.getDayDemoTestEncoderParams());
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  p.setParameterByKey(KEY.SP_PRIMER_DELAY,PRIME_COUNT);
  MultiEncoder me=MultiEncoder.builder().name(""String_Node_Str"").build();
  final Layer<Map<String,Object>> l=new Layer<>(p,me,new SpatialPooler(),new TemporalMemory(),Boolean.TRUE,null);
  l.subscribe(new Observer<Inference>(){
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      System.out.println(""String_Node_Str"" + e.getMessage());
      e.printStackTrace();
    }
    @Override public void onNext(    Inference i){
      assertNotNull(i);
      TOTAL++;
    }
  }
);
  Map<String,Object> multiInput=new HashMap<>();
  for (int i=0; i < NUM_CYCLES; i++) {
    for (double j=0; j < INPUT_GROUP_COUNT; j++) {
      multiInput.put(""String_Node_Str"",j);
      l.compute(multiInput);
    }
  }
  assertEquals((NUM_CYCLES * INPUT_GROUP_COUNT) - PRIME_COUNT,TOTAL);
  double[] all=l.getAllPredictions(""String_Node_Str"",1);
  double highestVal=Double.NEGATIVE_INFINITY;
  int highestIdx=-1;
  int i=0;
  for (  double d : all) {
    if (d > highestVal) {
      highestIdx=i;
      highestVal=d;
    }
    i++;
  }
  assertEquals(highestIdx,l.getMostProbableBucketIndex(""String_Node_Str"",1));
  assertEquals(7,l.getAllPredictions(""String_Node_Str"",1).length);
  assertTrue(Arrays.equals(ArrayUtils.where(l.getActiveColumns(),ArrayUtils.WHERE_1),l.getPreviousPredictedColumns()));
}","@Test public void testGetAllPredictions(){
  final int PRIME_COUNT=35;
  final int NUM_CYCLES=99;
  final int INPUT_GROUP_COUNT=7;
  TOTAL=0;
  Parameters p=NetworkTestHarness.getParameters();
  p=p.union(NetworkTestHarness.getDayDemoTestEncoderParams());
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  p.setParameterByKey(KEY.SP_PRIMER_DELAY,PRIME_COUNT);
  MultiEncoder me=MultiEncoder.builder().name(""String_Node_Str"").build();
  final Layer<Map<String,Object>> l=new Layer<>(p,me,new SpatialPooler(),new TemporalMemory(),Boolean.TRUE,null);
  l.subscribe(new Observer<Inference>(){
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      System.out.println(""String_Node_Str"" + e.getMessage());
      e.printStackTrace();
    }
    @Override public void onNext(    Inference i){
      assertNotNull(i);
      TOTAL++;
    }
  }
);
  Map<String,Object> multiInput=new HashMap<>();
  for (int i=0; i < NUM_CYCLES; i++) {
    for (double j=0; j < INPUT_GROUP_COUNT; j++) {
      multiInput.put(""String_Node_Str"",j);
      l.compute(multiInput);
    }
    l.reset();
  }
  assertEquals((NUM_CYCLES * INPUT_GROUP_COUNT) - PRIME_COUNT,TOTAL);
  double[] all=l.getAllPredictions(""String_Node_Str"",1);
  double highestVal=Double.NEGATIVE_INFINITY;
  int highestIdx=-1;
  int i=0;
  for (  double d : all) {
    if (d > highestVal) {
      highestIdx=i;
      highestVal=d;
    }
    i++;
  }
  assertEquals(highestIdx,l.getMostProbableBucketIndex(""String_Node_Str"",1));
  assertEquals(7,l.getAllPredictions(""String_Node_Str"",1).length);
  assertTrue(Arrays.equals(ArrayUtils.where(l.getActiveColumns(),ArrayUtils.WHERE_1),l.getPreviousPredictedColumns()));
}"
4492,"/** 
 * <p> Complex test that tests the Anomaly function automatically being setup and working. To test this, we do the following: </p><p> <ol> <li>Reset the warm up state vars</li> <li>Warm up prediction inferencing - make sure predictions are trained</li> <li>Throw in an anomalous record</li> <li>Test to make sure the Layer detects the anomaly, and that it is significantly registered</li> </ol> </p>
 */
@Test public void testAnomalySetup(){
  TOTAL=0;
  resetWarmUp();
  final int PRIME_COUNT=35;
  final int NUM_CYCLES=10;
  final int INPUT_GROUP_COUNT=7;
  Parameters p=NetworkTestHarness.getParameters();
  p=p.union(NetworkTestHarness.getDayDemoTestEncoderParams());
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  p.setParameterByKey(KEY.SP_PRIMER_DELAY,PRIME_COUNT);
  MultiEncoder me=MultiEncoder.builder().name(""String_Node_Str"").build();
  Map<String,Object> params=new HashMap<>();
  params.put(KEY_MODE,Mode.PURE);
  params.put(KEY_WINDOW_SIZE,3);
  params.put(KEY_USE_MOVING_AVG,true);
  Anomaly anomalyComputer=Anomaly.create(params);
  final Layer<Map<String,Object>> l=new Layer<>(p,me,new SpatialPooler(),new TemporalMemory(),Boolean.TRUE,anomalyComputer);
  l.subscribe(new Observer<Inference>(){
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      System.out.println(""String_Node_Str"" + e.getMessage());
      e.printStackTrace();
    }
    @Override public void onNext(    Inference i){
      TOTAL++;
      assertNotNull(i);
      if (testingAnomaly) {
        if (i.getAnomalyScore() > highestAnomaly)         highestAnomaly=i.getAnomalyScore();
      }
    }
  }
);
  Map<String,Object> multiInput=new HashMap<>();
  boolean isWarmedUp=false;
  while (l.getInference() == null || !isWarmedUp) {
    for (double j=0; j < INPUT_GROUP_COUNT; j++) {
      multiInput.put(""String_Node_Str"",j);
      l.compute(multiInput);
      if (l.getInference() != null && isWarmedUp(l,l.getInference().getAnomalyScore())) {
        isWarmedUp=true;
      }
    }
  }
  boolean exit=false;
  for (int i=0; !exit && i < NUM_CYCLES; i++) {
    for (double j=0; j < INPUT_GROUP_COUNT; j++) {
      if (i == 2) {
        testingAnomaly=true;
        multiInput.put(""String_Node_Str"",j+=0.5);
        l.compute(multiInput);
        exit=true;
      }
 else {
        multiInput.put(""String_Node_Str"",j);
        l.compute(multiInput);
      }
    }
  }
  assertTrue(highestAnomaly > 0.2);
}","/** 
 * <p> Complex test that tests the Anomaly function automatically being setup and working. To test this, we do the following: </p><p> <ol> <li>Reset the warm up state vars</li> <li>Warm up prediction inferencing - make sure predictions are trained</li> <li>Throw in an anomalous record</li> <li>Test to make sure the Layer detects the anomaly, and that it is significantly registered</li> </ol> </p>
 */
@Test public void testAnomalySetup(){
  TOTAL=0;
  resetWarmUp();
  final int PRIME_COUNT=35;
  final int NUM_CYCLES=10;
  final int INPUT_GROUP_COUNT=7;
  Parameters p=NetworkTestHarness.getParameters();
  p=p.union(NetworkTestHarness.getDayDemoTestEncoderParams());
  p.setParameterByKey(KEY.RANDOM,new MersenneTwister(42));
  p.setParameterByKey(KEY.SP_PRIMER_DELAY,PRIME_COUNT);
  MultiEncoder me=MultiEncoder.builder().name(""String_Node_Str"").build();
  Map<String,Object> params=new HashMap<>();
  params.put(KEY_MODE,Mode.PURE);
  params.put(KEY_WINDOW_SIZE,3);
  params.put(KEY_USE_MOVING_AVG,true);
  Anomaly anomalyComputer=Anomaly.create(params);
  final Layer<Map<String,Object>> l=new Layer<>(p,me,new SpatialPooler(),new TemporalMemory(),Boolean.TRUE,anomalyComputer);
  l.subscribe(new Observer<Inference>(){
    @Override public void onCompleted(){
    }
    @Override public void onError(    Throwable e){
      System.out.println(""String_Node_Str"" + e.getMessage());
      e.printStackTrace();
    }
    @Override public void onNext(    Inference i){
      TOTAL++;
      assertNotNull(i);
      if (testingAnomaly) {
        if (i.getAnomalyScore() > highestAnomaly)         highestAnomaly=i.getAnomalyScore();
      }
    }
  }
);
  Map<String,Object> multiInput=new HashMap<>();
  boolean isWarmedUp=false;
  while (l.getInference() == null || !isWarmedUp) {
    for (double j=0; j < INPUT_GROUP_COUNT; j++) {
      multiInput.put(""String_Node_Str"",j);
      l.compute(multiInput);
      if (l.getInference() != null && isWarmedUp(l,l.getInference().getAnomalyScore())) {
        isWarmedUp=true;
      }
    }
  }
  boolean exit=false;
  for (int i=0; !exit && i < NUM_CYCLES; i++) {
    for (double j=0; j < INPUT_GROUP_COUNT; j++) {
      if (i == 2) {
        testingAnomaly=true;
        multiInput.put(""String_Node_Str"",j+=0.5);
        l.compute(multiInput);
        exit=true;
      }
 else {
        multiInput.put(""String_Node_Str"",j);
        l.compute(multiInput);
      }
    }
  }
  System.out.println(""String_Node_Str"" + highestAnomaly);
  assertTrue(highestAnomaly > 0.2);
}"
4493,"/** 
 * Checks the anomaly score against a known threshold which indicates that the TM predictions have been ""warmed up"". When there have been enough occurrences within the threshold, we conclude that the algorithms have been adequately  stabilized.
 * @param l                 the {@link Layer} in question
 * @param anomalyScore      the current anomaly score
 * @return
 */
private boolean isWarmedUp(Layer<Map<String,Object>> l,double anomalyScore){
  if (anomalyScore >= 0 && anomalyScore <= 0.1) {
    ++timesWithinThreshold;
  }
 else {
    timesWithinThreshold=0;
  }
  if (timesWithinThreshold > 13) {
    return true;
  }
  return false;
}","/** 
 * Checks the anomaly score against a known threshold which indicates that the TM predictions have been ""warmed up"". When there have been enough occurrences within the threshold, we conclude that the algorithms have been adequately  stabilized.
 * @param l                 the {@link Layer} in question
 * @param anomalyScore      the current anomaly score
 * @return
 */
private boolean isWarmedUp(Layer<Map<String,Object>> l,double anomalyScore){
  if (anomalyScore >= 0 && anomalyScore <= 0.1) {
    ++timesWithinThreshold;
  }
 else {
    timesWithinThreshold=0;
  }
  if (timesWithinThreshold > 20) {
    return true;
  }
  return false;
}"
4494,"/** 
 * Returns the configured global inhibition flag
 * @return  the configured global inhibition flag
 * @see {@link #setGlobalInhibition(boolean)}
 */
public boolean getGlobalInhibition(){
  return globalInhibition;
}","/** 
 * Returns the configured global inhibition flag
 * @return  the configured global inhibition flag
 * @see setGlobalInhibition
 */
public boolean getGlobalInhibition(){
  return globalInhibition;
}"
4495,"/** 
 * Returns the configured local area density
 * @return  the configured local area density
 * @see {@link #setLocalAreaDensity(double)}
 */
public double getLocalAreaDensity(){
  return localAreaDensity;
}","/** 
 * Returns the configured local area density
 * @return  the configured local area density
 * @see setLocalAreaDensity
 */
public double getLocalAreaDensity(){
  return localAreaDensity;
}"
4496,"/** 
 * Returns the minPctActiveDutyCycle
 * @return  the minPctActiveDutyCycle
 * @see {@link #setMinPctActiveDutyCycle(double)}
 */
public double getMinPctActiveDutyCycles(){
  return minPctActiveDutyCycles;
}","/** 
 * Returns the minPctActiveDutyCycle see   {@link #setMinPctActiveDutyCycles(double)}
 * @return  the minPctActiveDutyCycle
 */
public double getMinPctActiveDutyCycles(){
  return minPctActiveDutyCycles;
}"
4497,"/** 
 * Returns the stimulus threshold
 * @return  the stimulus threshold
 * @see {@link #setStimulusThreshold(double)}
 */
public double getStimulusThreshold(){
  return stimulusThreshold;
}","/** 
 * Returns the stimulus threshold
 * @return  the stimulus threshold
 * @see setStimulusThreshold
 */
public double getStimulusThreshold(){
  return stimulusThreshold;
}"
4498,"/** 
 * Amount by which permanences of synapses are incremented during learning.
 * @param permanenceIncrement
 */
public double getPermanenceIncrement(){
  return this.permanenceIncrement;
}","/** 
 * Amount by which permanences of synapses are incremented during learning.
 */
public double getPermanenceIncrement(){
  return this.permanenceIncrement;
}"
4499,"/** 
 * Returns the synaptic permanence inactive decrement.
 * @return  the synaptic permanence inactive decrement.
 * @see {@link #setSynPermInactiveDec(double)}
 */
public double getSynPermInactiveDec(){
  return synPermInactiveDec;
}","/** 
 * Returns the synaptic permanence inactive decrement.
 * @return  the synaptic permanence inactive decrement.
 * @see setSynPermInactiveDec
 */
public double getSynPermInactiveDec(){
  return synPermInactiveDec;
}"
4500,"/** 
 * Sets the connected count   {@link SparseBinaryMatrix}
 * @param columnIndex
 * @param count
 */
public void setConnectedMatrix(SparseBinaryMatrix matrix){
  this.connectedCounts=matrix;
}","/** 
 * Sets the connected count   {@link SparseBinaryMatrix}
 * @param matrix
 */
public void setConnectedMatrix(SparseBinaryMatrix matrix){
  this.connectedCounts=matrix;
}"
4501,"/** 
 * Returns the configured number of active columns per inhibition area.
 * @return  the configured number of active columns perinhibition area.
 * @see {@link #setNumActiveColumnsPerInhArea(double)}
 */
public double getNumActiveColumnsPerInhArea(){
  return numActiveColumnsPerInhArea;
}","/** 
 * Returns the configured number of active columns per inhibition area.
 * @return  the configured number of active columns perinhibition area.
 * @see setNumActiveColumnsPerInhArea
 */
public double getNumActiveColumnsPerInhArea(){
  return numActiveColumnsPerInhArea;
}"
4502,"/** 
 * Returns the configured input dimensions
 * @return the configured input dimensions
 * @see {@link #setInputDimensions(int[])}
 */
public int[] getInputDimensions(){
  return inputDimensions;
}","/** 
 * Returns the configured input dimensions see   {@link #setInputDimensions(int[])}
 * @return the configured input dimensions
 */
public int[] getInputDimensions(){
  return inputDimensions;
}"
4503,"/** 
 * Returns the configured active permanence increment
 * @return the configured active permanence increment
 * @see {@link #setSynPermActiveInc(double)}
 */
public double getSynPermActiveInc(){
  return synPermActiveInc;
}","/** 
 * Returns the configured active permanence increment
 * @return the configured active permanence increment
 * @see setSynPermActiveInc
 */
public double getSynPermActiveInc(){
  return synPermActiveInc;
}"
4504,"/** 
 * The maximum overlap boost factor. Each column's overlap gets multiplied by a boost factor before it gets considered for inhibition. The actual boost factor for a column is number between 1.0 and maxBoost. A boost factor of 1.0 is used if the duty cycle is >= minOverlapDutyCycle, maxBoost is used if the duty cycle is 0, and any duty cycle in between is linearly extrapolated from these 2 end points.
 * @param maxBoost
 */
public void setMaxBoost(double maxBoost){
  this.maxBoost=maxBoost;
}","/** 
 * The maximum overlap boost factor. Each column's overlap gets multiplied by a boost factor before it gets considered for inhibition. The actual boost factor for a column is number between 1.0 and maxBoost. A boost factor of 1.0 is used if the duty cycle is &gt;= minOverlapDutyCycle, maxBoost is used if the duty cycle is 0, and any duty cycle in between is linearly extrapolated from these 2 end points.
 * @param maxBoost
 */
public void setMaxBoost(double maxBoost){
  this.maxBoost=maxBoost;
}"
4505,"/** 
 * {@see #setMinPctOverlapDutyCycles(double)}
 * @return
 */
public double getMinPctOverlapDutyCycles(){
  return minPctOverlapDutyCycles;
}","/** 
 * see   {@link #setMinPctOverlapDutyCycles(double)}
 * @return
 */
public double getMinPctOverlapDutyCycles(){
  return minPctOverlapDutyCycles;
}"
4506,"/** 
 * Sets the current   {@link Set} of winner {@link Cells}s
 * @param cells
 */
public void setWinnerCells(Set<Cell> cells){
  this.winnerCells=cells;
}","/** 
 * Sets the current   {@link Set} of winner {@link Cell}s
 * @param cells
 */
public void setWinnerCells(Set<Cell> cells){
  this.winnerCells=cells;
}"
4507,"/** 
 * Returns the max boost
 * @return  the max boost
 * @see {@link #setMaxBoost(double)}
 */
public double getMaxBoost(){
  return maxBoost;
}","/** 
 * Returns the max boost see   {@link #setMaxBoost(double)}
 * @return  the max boost
 */
public double getMaxBoost(){
  return maxBoost;
}"
4508,"/** 
 * Amount by which permanences of synapses are decremented during learning.
 * @param permanenceDecrement
 */
public double getPermanenceDecrement(){
  return this.permanenceDecrement;
}","/** 
 * Amount by which permanences of synapses are decremented during learning.
 */
public double getPermanenceDecrement(){
  return this.permanenceDecrement;
}"
4509,"/** 
 * Returns the configured potential radius
 * @return  the configured potential radius
 * @see {@link #setPotentialRadius(int)}
 */
public int getPotentialRadius(){
  return Math.min(numInputs,potentialRadius);
}","/** 
 * Returns the configured potential radius
 * @return  the configured potential radius
 * @see setPotentialRadius
 */
public int getPotentialRadius(){
  return Math.min(numInputs,potentialRadius);
}"
4510,"/** 
 * Gets the number of   {@link Cells} per {@link Column}.
 * @return cellsPerColumn
 */
public int getCellsPerColumn(){
  return this.cellsPerColumn;
}","/** 
 * Gets the number of   {@link Cell}s per   {@link Column}.
 * @return cellsPerColumn
 */
public int getCellsPerColumn(){
  return this.cellsPerColumn;
}"
4511,"/** 
 * Initial permanence of a new synapse 
 * @param   
 */
public void setInitialPermanence(double initialPermanence){
  this.initialPermanence=initialPermanence;
}","/** 
 * Initial permanence of a new synapse 
 * @param initialPermanence
 */
public void setInitialPermanence(double initialPermanence){
  this.initialPermanence=initialPermanence;
}"
4512,"/** 
 * Converts a   {@link Collection} of {@link Columns}s to a list of column indexes.
 * @param columns
 * @return
 */
public static List<Integer> asColumnIndexes(Collection<Column> columns){
  List<Integer> ints=new ArrayList<Integer>();
  for (  Column col : columns) {
    ints.add(col.getIndex());
  }
  return ints;
}","/** 
 * Converts a   {@link Collection} of {@link Column}s to a list of column indexes.
 * @param columns
 * @return
 */
public static List<Integer> asColumnIndexes(Collection<Column> columns){
  List<Integer> ints=new ArrayList<Integer>();
  for (  Column col : columns) {
    ints.add(col.getIndex());
  }
  return ints;
}"
4513,"/** 
 * Returns the configured duty cycle period
 * @return  the configured duty cycle period
 * @see {@link #setDutyCyclePeriod(double)}
 */
public int getDutyCyclePeriod(){
  return dutyCyclePeriod;
}","/** 
 * Returns the configured duty cycle period see   {@link #setDutyCyclePeriod(double)}
 * @return  the configured duty cycle period
 */
public int getDutyCyclePeriod(){
  return dutyCyclePeriod;
}"
4514,"/** 
 * Returns the configured potential pct
 * @return the configured potential pct
 * @see {@link #setPotentialPct(double)}
 */
public double getPotentialPct(){
  return potentialPct;
}","/** 
 * Returns the configured potential pct
 * @return the configured potential pct
 * @see setPotentialPct
 */
public double getPotentialPct(){
  return potentialPct;
}"
4515,"/** 
 * Returns the verbosity setting.
 * @return  the verbosity setting.
 * @see {@link #setSpVerbosity(int)}
 */
public int getSpVerbosity(){
  return spVerbosity;
}","/** 
 * Returns the verbosity setting. see   {@link #setSpVerbosity(int)}
 * @return  the verbosity setting.
 */
public int getSpVerbosity(){
  return spVerbosity;
}"
4516,"/** 
 * Returns the synapse permanence connected threshold
 * @return the synapse permanence connected threshold
 * @see {@link #setSynPermConnected(double)}
 */
public double getSynPermConnected(){
  return synPermConnected;
}","/** 
 * Returns the synapse permanence connected threshold
 * @return the synapse permanence connected threshold
 * @see setSynPermConnected
 */
public double getSynPermConnected(){
  return synPermConnected;
}"
4517,"/** 
 * The default connected threshold. Any synapse whose permanence value is above the connected threshold is a ""connected synapse"", meaning it can contribute to the cell's firing.
 * @param minPctOverlapDutyCycle
 */
public void setSynPermConnected(double synPermConnected){
  this.synPermConnected=synPermConnected;
}","/** 
 * The default connected threshold. Any synapse whose permanence value is above the connected threshold is a ""connected synapse"", meaning it can contribute to the cell's firing.
 * @param synPermConnected
 */
public void setSynPermConnected(double synPermConnected){
  this.synPermConnected=synPermConnected;
}"
4518,"/** 
 * Sets the activation threshold. <p/> If the number of active connected synapses on a segment is at least this threshold, the segment is said to be active.
 * @param activationThreshold
 */
public void setActivationThreshold(int activationThreshold){
  paramMap.put(KEY.ACTIVATION_THRESHOLD,activationThreshold);
}","/** 
 * <p> Sets the activation threshold. </p> If the number of active connected synapses on a segment is at least this threshold, the segment is said to be active.
 * @param activationThreshold
 */
public void setActivationThreshold(int activationThreshold){
  paramMap.put(KEY.ACTIVATION_THRESHOLD,activationThreshold);
}"
4519,"/** 
 * Initial permanence of a new synapse
 * @param
 */
public void setInitialPermanence(double initialPermanence){
  paramMap.put(KEY.INITIAL_PERMANENCE,initialPermanence);
}","/** 
 * Initial permanence of a new synapse
 * @param initialPermanence
 */
public void setInitialPermanence(double initialPermanence){
  paramMap.put(KEY.INITIAL_PERMANENCE,initialPermanence);
}"
4520,"/** 
 * The maximum overlap boost factor. Each column's overlap gets multiplied by a boost factor before it gets considered for inhibition. The actual boost factor for a column is number between 1.0 and maxBoost. A boost factor of 1.0 is used if the duty cycle is >= minOverlapDutyCycle, maxBoost is used if the duty cycle is 0, and any duty cycle in between is linearly extrapolated from these 2 end points.
 * @param maxBoost
 */
public void setMaxBoost(double maxBoost){
  paramMap.put(KEY.MAX_BOOST,maxBoost);
}","/** 
 * The maximum overlap boost factor. Each column's overlap gets multiplied by a boost factor before it gets considered for inhibition. The actual boost factor for a column is number between 1.0 and maxBoost. A boost factor of 1.0 is used if the duty cycle is &gt;= minOverlapDutyCycle, maxBoost is used if the duty cycle is 0, and any duty cycle in between is linearly extrapolated from these 2 end points.
 * @param maxBoost
 */
public void setMaxBoost(double maxBoost){
  paramMap.put(KEY.MAX_BOOST,maxBoost);
}"
4521,"/** 
 * @param n   Number of available bits in pattern
 * @param w   Number of on bits in pattern
 * @param num Number of available patternsConstructs a new  {@code PatternMachine}
 */
public PatternMachine(int n,int w,int seed){
  this.n=n;
  this.w=w;
  random=new MersenneTwister(new int[]{seed});
  patterns=new LinkedHashMap<Integer,LinkedHashSet<Integer>>();
  generate();
}","/** 
 * @param n   Number of available bits in pattern
 * @param w   Number of on bits in pattern
 * @param seed Random seedConstructs a new  {@code PatternMachine}
 */
public PatternMachine(int n,int w,int seed){
  this.n=n;
  this.w=w;
  random=new MersenneTwister(new int[]{seed});
  patterns=new LinkedHashMap<Integer,LinkedHashSet<Integer>>();
  generate();
}"
4522,"/** 
 * Returns the synapses on a segment that are active due to lateral input from active cells.
 * @param activeSynapsesForSegment      Set of this {@code Segment}'s active   {@code Synapse}s
 * @param permanenceThreshold           Threshold at which a {@Synapse} is considered connected.
 * @return                              Set of connected Synapses
 */
public Set<Synapse> getConnectedActiveSynapses(Map<DistalDendrite,Set<Synapse>> activeSynapsesForSegment,double permanenceThreshold){
  Set<Synapse> connectedSynapses=null;
  if (!activeSynapsesForSegment.containsKey(this)) {
    return EMPTY_SYNAPSE_SET;
  }
  for (  Synapse s : activeSynapsesForSegment.get(this)) {
    if (s.getPermanence() >= permanenceThreshold) {
      if (connectedSynapses == null) {
        connectedSynapses=new LinkedHashSet<Synapse>();
      }
      connectedSynapses.add(s);
    }
  }
  return connectedSynapses == null ? EMPTY_SYNAPSE_SET : connectedSynapses;
}","/** 
 * Returns the synapses on a segment that are active due to lateral input from active cells.
 * @param activeSynapsesForSegment      Set of this {@link Segment}'s active   {@link Synapse}s
 * @param permanenceThreshold           Threshold at which a {@link Synapse} is considered connected.
 * @return                              Set of connected Synapses
 */
public Set<Synapse> getConnectedActiveSynapses(Map<DistalDendrite,Set<Synapse>> activeSynapsesForSegment,double permanenceThreshold){
  Set<Synapse> connectedSynapses=null;
  if (!activeSynapsesForSegment.containsKey(this)) {
    return EMPTY_SYNAPSE_SET;
  }
  for (  Synapse s : activeSynapsesForSegment.get(this)) {
    if (s.getPermanence() >= permanenceThreshold) {
      if (connectedSynapses == null) {
        connectedSynapses=new LinkedHashSet<Synapse>();
      }
      connectedSynapses.add(s);
    }
  }
  return connectedSynapses == null ? EMPTY_SYNAPSE_SET : connectedSynapses;
}"
4523,"/** 
 * Sets the input vector synapse indexes which are connected (>= synPermConnected)
 * @param c
 * @param connectedIndexes
 */
public void setConnectedSynapsesForTest(Connections c,int[] connectedIndexes){
  Pool pool=createPool(c,connectedIndexes);
  c.getPotentialPools().set(index,pool);
}","/** 
 * Sets the input vector synapse indexes which are connected (&gt;= synPermConnected)
 * @param c
 * @param connectedIndexes
 */
public void setConnectedSynapsesForTest(Connections c,int[] connectedIndexes){
  Pool pool=createPool(c,connectedIndexes);
  c.getPotentialPools().set(index,pool);
}"
4524,"/** 
 * Resets the   {@link TemporalMemory} if it exists.
 * @return
 */
public void reset(){
  if (temporalMemory == null) {
    LOGGER.debug(""String_Node_Str"" + getName() + ""String_Node_Str"");
  }
 else {
    temporalMemory.reset(connections);
    resetRecordNum();
  }
}","/** 
 * Resets the   {@link TemporalMemory} if it exists.
 */
public void reset(){
  if (temporalMemory == null) {
    LOGGER.debug(""String_Node_Str"" + getName() + ""String_Node_Str"");
  }
 else {
    temporalMemory.reset(connections);
    resetRecordNum();
  }
}"
4525,"/** 
 * Used to manually input data into a   {@link Region}, the other way  being the call to   {@link Region#start()} for a Region that containsa  {@link Layer} which in turn contains a {@link Sensor} <em>-OR-</em>subscribing a receiving Region to this Region's output Observable.
 * @param input One of (int[], String[], {@link ManualInput}, or Map<String, Object>)
 */
@SuppressWarnings(""String_Node_Str"") public <T>void compute(T input){
  if (!assemblyClosed) {
    close();
  }
  this.input=input;
  ((Layer<T>)tail).compute(input);
}","/** 
 * Used to manually input data into a   {@link Region}, the other way  being the call to   {@link Region#start()} for a Region that containsa  {@link Layer} which in turn contains a {@link Sensor} <em>-OR-</em>subscribing a receiving Region to this Region's output Observable.
 * @param input One of (int[], String[], {@link ManualInput}, or Map&lt;String, Object&gt;)
 */
@SuppressWarnings(""String_Node_Str"") public <T>void compute(T input){
  if (!assemblyClosed) {
    close();
  }
  this.input=input;
  ((Layer<T>)tail).compute(input);
}"
4526,"/** 
 * Searches through the specified   {@link MultiEncoder}'s previously configured  encoders to find and return one that is of type   {@link ScalarEncoder},  {@link RandomDistributedScalarEncoder},   {@link AdaptiveScalarEncoder},  {@link LogEncoder} or {@link DeltaEncoder}.
 * @param enc   the containing {@code MultiEncoder}
 * @return
 */
private Optional<Encoder<?>> getNumberEncoder(MultiEncoder enc){
  for (  EncoderTuple t : enc.getEncoders(enc)) {
    if ((t.getEncoder() instanceof RandomDistributedScalarEncoder) || (t.getEncoder() instanceof ScalarEncoder) || (t.getEncoder() instanceof AdaptiveScalarEncoder)|| (t.getEncoder() instanceof LogEncoder)|| (t.getEncoder() instanceof DeltaEncoder)) {
      return Optional.of(t.getEncoder());
    }
  }
  return Optional.of(null);
}","/** 
 * Searches through the specified   {@link MultiEncoder}'s previously configured  encoders to find and return one that is of type   {@link ScalarEncoder},  {@link RandomDistributedScalarEncoder},   {@link AdaptiveScalarEncoder},  {@link LogEncoder} or {@link DeltaEncoder}.
 * @param enc   the containing {@code MultiEncoder}
 * @return
 */
private Optional<Encoder<?>> getNumberEncoder(MultiEncoder enc){
  for (  EncoderTuple t : enc.getEncoders(enc)) {
    if ((t.getEncoder() instanceof RandomDistributedScalarEncoder) || (t.getEncoder() instanceof ScalarEncoder) || (t.getEncoder() instanceof AdaptiveScalarEncoder)|| (t.getEncoder() instanceof LogEncoder)|| (t.getEncoder() instanceof DeltaEncoder)) {
      return Optional.of(t.getEncoder());
    }
  }
  return Optional.empty();
}"
4527,"/** 
 * Searches through the specified   {@link MultiEncoder}'s previously configured  encoders to find and return one that is of type   {@link DateEncoder}
 * @param enc   the containing {@code MultiEncoder}
 * @return
 */
private Optional<DateEncoder> getDateEncoder(MultiEncoder enc){
  for (  EncoderTuple t : enc.getEncoders(enc)) {
    if (t.getEncoder() instanceof DateEncoder) {
      return Optional.of((DateEncoder)t.getEncoder());
    }
  }
  return Optional.of(null);
}","/** 
 * Searches through the specified   {@link MultiEncoder}'s previously configured  encoders to find and return one that is of type   {@link DateEncoder}
 * @param enc   the containing {@code MultiEncoder}
 * @return
 */
private Optional<DateEncoder> getDateEncoder(MultiEncoder enc){
  for (  EncoderTuple t : enc.getEncoders(enc)) {
    if (t.getEncoder() instanceof DateEncoder) {
      return Optional.of((DateEncoder)t.getEncoder());
    }
  }
  return Optional.empty();
}"
4528,"/** 
 * Searches through the specified   {@link MultiEncoder}'s previously configured  encoders to find and return one that is of type   {@link CategoryEncoder} or{@link SDRCategoryEncoder}
 * @param enc   the containing {@code MultiEncoder}
 * @return
 */
private Optional<Encoder<?>> getCategoryEncoder(MultiEncoder enc){
  for (  EncoderTuple t : enc.getEncoders(enc)) {
    if ((t.getEncoder() instanceof CategoryEncoder) || (t.getEncoder() instanceof SDRCategoryEncoder)) {
      return Optional.of(t.getEncoder());
    }
  }
  return null;
}","/** 
 * Searches through the specified   {@link MultiEncoder}'s previously configured  encoders to find and return one that is of type   {@link CategoryEncoder} or{@link SDRCategoryEncoder}
 * @param enc   the containing {@code MultiEncoder}
 * @return
 */
private Optional<Encoder<?>> getCategoryEncoder(MultiEncoder enc){
  for (  EncoderTuple t : enc.getEncoders(enc)) {
    if ((t.getEncoder() instanceof CategoryEncoder) || (t.getEncoder() instanceof SDRCategoryEncoder)) {
      return Optional.of(t.getEncoder());
    }
  }
  return Optional.empty();
}"
4529,"/** 
 * Searches through the specified   {@link MultiEncoder}'s previously configured  encoders to find and return one that is of type   {@link CoordinateEncoder} or{@link GeospatialCoordinateEncoder}
 * @param enc   the containing {@code MultiEncoder}
 * @return
 */
private Optional<Encoder<?>> getCoordinateEncoder(MultiEncoder enc){
  for (  EncoderTuple t : enc.getEncoders(enc)) {
    if ((t.getEncoder() instanceof CoordinateEncoder) || (t.getEncoder() instanceof GeospatialCoordinateEncoder)) {
      return Optional.of(t.getEncoder());
    }
  }
  return null;
}","/** 
 * Searches through the specified   {@link MultiEncoder}'s previously configured  encoders to find and return one that is of type   {@link CoordinateEncoder} or{@link GeospatialCoordinateEncoder}
 * @param enc   the containing {@code MultiEncoder}
 * @return
 */
private Optional<Encoder<?>> getCoordinateEncoder(MultiEncoder enc){
  for (  EncoderTuple t : enc.getEncoders(enc)) {
    if ((t.getEncoder() instanceof CoordinateEncoder) || (t.getEncoder() instanceof GeospatialCoordinateEncoder)) {
      return Optional.of(t.getEncoder());
    }
  }
  return Optional.empty();
}"
4530,"/** 
 * Sets up a mapping which describes the order of occurrence of comma separated fields - mapping their ordinal position to the   {@link Encoder}which services the encoding of the field occurring in that position. This sequence of types is contained by an instance of   {@link Header} whichmakes available an array of  {@link FieldMetaType}s.
 */
private void makeIndexEncoderMap(){
  indexToEncoderMap=new TIntObjectHashMap<Encoder<?>>();
  final FieldMetaType[] fieldTypes=header.getFieldTypes().toArray(new FieldMetaType[header.getFieldTypes().size()]);
  for (int i=0; i < fieldTypes.length; i++) {
switch (fieldTypes[i]) {
case DATETIME:
      Optional<DateEncoder> de=getDateEncoder(encoder);
    if (de.isPresent()) {
      indexToEncoderMap.put(i,de.get());
    }
 else {
      throw new IllegalArgumentException(""String_Node_Str"");
    }
  break;
case BOOLEAN:
case FLOAT:
case INTEGER:
Optional<Encoder<?>> opt=getNumberEncoder(encoder);
if (opt.isPresent()) {
indexToEncoderMap.put(i,opt.get());
}
 else {
throw new IllegalArgumentException(""String_Node_Str"");
}
break;
case LIST:
case STRING:
opt=getCategoryEncoder(encoder);
if (opt.isPresent()) {
indexToEncoderMap.put(i,opt.get());
}
 else {
throw new IllegalArgumentException(""String_Node_Str"");
}
break;
case COORD:
case GEO:
opt=getCoordinateEncoder(encoder);
if (opt.isPresent()) {
indexToEncoderMap.put(i,opt.get());
}
 else {
throw new IllegalArgumentException(""String_Node_Str"");
}
break;
default :
break;
}
}
}","/** 
 * Sets up a mapping which describes the order of occurrence of comma separated fields - mapping their ordinal position to the   {@link Encoder}which services the encoding of the field occurring in that position. This sequence of types is contained by an instance of   {@link Header} whichmakes available an array of  {@link FieldMetaType}s.
 */
private void makeIndexEncoderMap(){
  indexToEncoderMap=new TIntObjectHashMap<Encoder<?>>();
  for (int i=0, size=header.getFieldNames().size(); i < size; i++) {
switch (header.getFieldTypes().get(i)) {
case DATETIME:
      Optional<DateEncoder> de=getDateEncoder(encoder);
    if (de.isPresent()) {
      indexToEncoderMap.put(i,de.get());
    }
 else {
      throw new IllegalArgumentException(""String_Node_Str"" + header.getFieldNames().get(i));
    }
  break;
case BOOLEAN:
case FLOAT:
case INTEGER:
Optional<Encoder<?>> ne=getNumberEncoder(encoder);
if (ne.isPresent()) {
indexToEncoderMap.put(i,ne.get());
}
 else {
throw new IllegalArgumentException(""String_Node_Str"" + header.getFieldNames().get(i));
}
break;
case LIST:
case STRING:
Optional<Encoder<?>> ce=getCategoryEncoder(encoder);
if (ce.isPresent()) {
indexToEncoderMap.put(i,ce.get());
}
 else {
throw new IllegalArgumentException(""String_Node_Str"" + header.getFieldNames().get(i));
}
break;
case COORD:
case GEO:
Optional<Encoder<?>> ge=getCoordinateEncoder(encoder);
if (ge.isPresent()) {
indexToEncoderMap.put(i,ge.get());
}
 else {
throw new IllegalArgumentException(""String_Node_Str"" + header.getFieldNames().get(i));
}
break;
default :
break;
}
}
}"
4531,"public static String path(String s){
  URL url=ResourceLocator.class.getResource(s);
  if (url == null) {
    url=ResourceLocator.class.getClassLoader().getResource(s);
  }
  return url.getPath();
}","public static String path(String s){
  URL url=ResourceLocator.class.getResource(s);
  if (url == null) {
    url=ResourceLocator.class.getClassLoader().getResource(s);
  }
  return new File(url.getPath()).getPath();
}"
4532,"/** 
 * Called internally to create this csv stream's header
 */
private void makeHeader(){
  List<String[]> contents=new ArrayList<>();
  if (headerStateTracker < fence) {
    LOGGER.warn(""String_Node_Str"" + fence + ""String_Node_Str"");
  }
  int i=0;
  while (i++ < fence)   contents.add(it.next());
  this.header=new BatchedCsvHeader(contents,fence);
}","/** 
 * Called internally to create this csv stream's header
 */
private void makeHeader(){
  List<String[]> contents=new ArrayList<>();
  if (headerStateTracker < fence) {
    LOGGER.warn(""String_Node_Str"" + fence + ""String_Node_Str"");
  }
  int i=0;
  while (i++ < fence) {
    String[] h=it.next();
    System.out.println(""String_Node_Str"" + i + ""String_Node_Str""+ Arrays.toString(h));
    contents.add(h);
  }
  this.header=new BatchedCsvHeader(contents,fence);
}"
4533,"@Ignore public void testProgrammaticStream(){
  PublishSubject<String> manual=PublishSubject.create();
  Object[] n={""String_Node_Str"",manual};
  SensorParams parms=SensorParams.create(Keys::obs,n);
  Sensor<ObservableSensor<String[]>> sensor=Sensor.create(ObservableSensor::create,parms);
  long count=sensor.getInputStream().count();
  assertEquals(4391,count);
}","@Test public void testProgrammaticStream(){
  final ReplaySubject<String> manual=ReplaySubject.create();
  manual.onNext(""String_Node_Str"");
  manual.onNext(""String_Node_Str"");
  manual.onNext(""String_Node_Str"");
  System.out.println(""String_Node_Str"");
  Object[] n={""String_Node_Str"",manual};
  SensorParams parms=SensorParams.create(Keys::obs,n);
  final Sensor<ObservableSensor<String>> sensor=Sensor.create(ObservableSensor::create,parms);
  (new Thread(){
    public void run(){
      sensor.getInputStream().forEach(l -> {
        System.out.println(l);
      }
);
    }
  }
).start();
  String[] entries={""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  manual.onNext(entries[0]);
  manual.onNext(entries[1]);
  manual.onNext(entries[2]);
  manual.onNext(entries[3]);
}"
4534,"/** 
 * {@inheritDoc}
 */
@SuppressWarnings({""String_Node_Str"",""String_Node_Str""}) @Override public void encodeIntoArray(Date inputData,int[] output){
  TDoubleList scalars=getScalars(inputData);
  int fieldCounter=0;
  for (  EncoderTuple t : getEncoders(this)) {
    String name=t.getName();
    Encoder encoder=t.getEncoder();
    int offset=t.getOffset();
    int[] tempArray=new int[encoder.getWidth()];
    encoder.encodeIntoArray(scalars.get(fieldCounter),tempArray);
    for (int i=0; i < tempArray.length; i++) {
      output[i + offset]=tempArray[i];
    }
    ++fieldCounter;
  }
}","/** 
 * {@inheritDoc}
 */
@SuppressWarnings({""String_Node_Str"",""String_Node_Str""}) @Override public void encodeIntoArray(Date inputData,int[] output){
  TDoubleList scalars=getScalars(inputData);
  int fieldCounter=0;
  for (  EncoderTuple t : getEncoders(this)) {
    Encoder encoder=t.getEncoder();
    int offset=t.getOffset();
    int[] tempArray=new int[encoder.getWidth()];
    encoder.encodeIntoArray(scalars.get(fieldCounter),tempArray);
    System.arraycopy(tempArray,0,output,offset,tempArray.length);
    ++fieldCounter;
  }
}"
4535,"/** 
 * Set how many bits are used to encode customDays
 */
public DateEncoder.Builder customDays(int customDays){
  return this.customDays(customDays,(ArrayList<String>)this.customDays.get(1));
}","/** 
 * Set how many bits are used to encode customDays
 */
@SuppressWarnings(""String_Node_Str"") public DateEncoder.Builder customDays(int customDays){
  return this.customDays(customDays,(ArrayList<String>)this.customDays.get(1));
}"
4536,"/** 
 * Init the   {@code DateEncoder} with parameters
 */
public void init(){
  width=0;
  setForced(true);
  encoders=new LinkedHashMap<>();
  encoders.put(new EncoderTuple(""String_Node_Str"",this,0),new ArrayList<EncoderTuple>());
  if (isValidEncoderPropertyTuple(season)) {
    seasonEncoder=ScalarEncoder.builder().w((int)season.get(0)).radius((double)season.get(1)).minVal(0).maxVal(366).periodic(true).name(""String_Node_Str"").forced(this.isForced()).build();
    addChildEncoder(seasonEncoder);
  }
  if (isValidEncoderPropertyTuple(dayOfWeek)) {
    dayOfWeekEncoder=ScalarEncoder.builder().w((int)dayOfWeek.get(0)).radius((double)dayOfWeek.get(1)).minVal(0).maxVal(7).periodic(true).name(""String_Node_Str"").forced(this.isForced()).build();
    addChildEncoder(dayOfWeekEncoder);
  }
  if (isValidEncoderPropertyTuple(weekend)) {
    weekendEncoder=ScalarEncoder.builder().w((int)weekend.get(0)).radius((double)weekend.get(1)).minVal(0).maxVal(1).periodic(false).name(""String_Node_Str"").forced(this.isForced()).build();
    addChildEncoder(weekendEncoder);
  }
  if (isValidEncoderPropertyTuple(customDays)) {
    List<String> days=(List<String>)customDays.get(1);
    StringBuilder customDayEncoderName=new StringBuilder();
    if (days.size() == 1) {
      customDayEncoderName.append(days.get(0));
    }
 else {
      for (      String day : days) {
        customDayEncoderName.append(day).append(""String_Node_Str"");
      }
    }
    customDaysEncoder=ScalarEncoder.builder().w((int)customDays.get(0)).radius(1).minVal(0).maxVal(1).periodic(false).name(customDayEncoderName.toString()).forced(this.isForced()).build();
    addEncoder(""String_Node_Str"",customDaysEncoder);
    addCustomDays(days);
  }
  if (isValidEncoderPropertyTuple(holiday)) {
    holidayEncoder=ScalarEncoder.builder().w((int)holiday.get(0)).radius((double)holiday.get(1)).minVal(0).maxVal(1).periodic(false).name(""String_Node_Str"").forced(this.isForced()).build();
    addChildEncoder(holidayEncoder);
  }
  if (isValidEncoderPropertyTuple(timeOfDay)) {
    timeOfDayEncoder=ScalarEncoder.builder().w((int)timeOfDay.get(0)).radius((double)timeOfDay.get(1)).minVal(0).maxVal(24).periodic(true).name(""String_Node_Str"").forced(this.isForced()).build();
    addChildEncoder(timeOfDayEncoder);
  }
}","/** 
 * Init the   {@code DateEncoder} with parameters
 */
@SuppressWarnings(""String_Node_Str"") public void init(){
  width=0;
  setForced(true);
  encoders=new LinkedHashMap<>();
  encoders.put(new EncoderTuple(""String_Node_Str"",this,0),new ArrayList<EncoderTuple>());
  if (isValidEncoderPropertyTuple(season)) {
    seasonEncoder=ScalarEncoder.builder().w((int)season.get(0)).radius((double)season.get(1)).minVal(0).maxVal(366).periodic(true).name(""String_Node_Str"").forced(this.isForced()).build();
    addChildEncoder(seasonEncoder);
  }
  if (isValidEncoderPropertyTuple(dayOfWeek)) {
    dayOfWeekEncoder=ScalarEncoder.builder().w((int)dayOfWeek.get(0)).radius((double)dayOfWeek.get(1)).minVal(0).maxVal(7).periodic(true).name(""String_Node_Str"").forced(this.isForced()).build();
    addChildEncoder(dayOfWeekEncoder);
  }
  if (isValidEncoderPropertyTuple(weekend)) {
    weekendEncoder=ScalarEncoder.builder().w((int)weekend.get(0)).radius((double)weekend.get(1)).minVal(0).maxVal(1).periodic(false).name(""String_Node_Str"").forced(this.isForced()).build();
    addChildEncoder(weekendEncoder);
  }
  if (isValidEncoderPropertyTuple(customDays)) {
    List<String> days=(List<String>)customDays.get(1);
    StringBuilder customDayEncoderName=new StringBuilder();
    if (days.size() == 1) {
      customDayEncoderName.append(days.get(0));
    }
 else {
      for (      String day : days) {
        customDayEncoderName.append(day).append(""String_Node_Str"");
      }
    }
    customDaysEncoder=ScalarEncoder.builder().w((int)customDays.get(0)).radius(1).minVal(0).maxVal(1).periodic(false).name(customDayEncoderName.toString()).forced(this.isForced()).build();
    addEncoder(""String_Node_Str"",customDaysEncoder);
    addCustomDays(days);
  }
  if (isValidEncoderPropertyTuple(holiday)) {
    holidayEncoder=ScalarEncoder.builder().w((int)holiday.get(0)).radius((double)holiday.get(1)).minVal(0).maxVal(1).periodic(false).name(""String_Node_Str"").forced(this.isForced()).build();
    addChildEncoder(holidayEncoder);
  }
  if (isValidEncoderPropertyTuple(timeOfDay)) {
    timeOfDayEncoder=ScalarEncoder.builder().w((int)timeOfDay.get(0)).radius((double)timeOfDay.get(1)).minVal(0).maxVal(24).periodic(true).name(""String_Node_Str"").forced(this.isForced()).build();
    addChildEncoder(timeOfDayEncoder);
  }
}"
4537,"public List<String> getEncodedValues(Date inputData){
  if (inputData == null) {
    return new ArrayList<>();
  }
  List<String> values=new ArrayList<>();
  DateTime date=new DateTime(inputData);
  double timeOfDay=date.getHourOfDay() + date.getMinuteOfHour() / 60.0;
  int dayOfWeek=date.getDayOfWeek() - 1;
  if (seasonEncoder != null) {
    double dayOfYear=date.getDayOfYear() - 1;
    values.add(String.valueOf(dayOfYear));
  }
  if (dayOfWeekEncoder != null) {
    values.add(String.valueOf(dayOfWeek));
  }
  if (weekendEncoder != null) {
    boolean isWeekend=dayOfWeek == 6 || dayOfWeek == 5 || (dayOfWeek == 4 && timeOfDay > 18);
    int weekend=isWeekend ? 1 : 0;
    values.add(String.valueOf(weekend));
  }
  if (customDaysEncoder != null) {
    boolean isCustomDays=customDaysList.contains(dayOfWeek);
    int customDay=isCustomDays ? 1 : 0;
    values.add(String.valueOf(customDay));
  }
  if (holidayEncoder != null) {
    double holidayness=0;
    for (    Tuple h : HolidaysList) {
      DateTime hdate=new DateTime(date.getYear(),(int)h.get(0),(int)h.get(1),0,0,0);
      if (date.isAfter(hdate)) {
        Duration diff=new Interval(hdate,date).toDuration();
        long days=diff.getStandardDays();
        if (days == 0) {
          holidayness=1;
          break;
        }
 else         if (days == 1) {
          holidayness=1.0 - ((diff.getStandardSeconds() - 86400.0 * days) / 86400.0);
          break;
        }
      }
 else {
        Duration diff=new Interval(date,hdate).toDuration();
        long days=diff.getStandardDays();
        if (days == 0) {
          holidayness=1.0 - ((diff.getStandardSeconds() - 86400.0 * days) / 86400.0);
        }
      }
    }
    values.add(String.valueOf(holidayness));
  }
  if (timeOfDayEncoder != null) {
    values.add(String.valueOf(timeOfDay));
  }
  return values;
}","public List<String> getEncodedValues(Date inputData){
  if (inputData == null) {
    return new ArrayList<>();
  }
  List<String> values=new ArrayList<>();
  DateTime date=new DateTime(inputData);
  double timeOfDay=date.getHourOfDay() + date.getMinuteOfHour() / 60.0;
  int dayOfWeek=date.getDayOfWeek() - 1;
  if (seasonEncoder != null) {
    double dayOfYear=date.getDayOfYear() - 1;
    values.add(String.valueOf(dayOfYear));
  }
  if (dayOfWeekEncoder != null) {
    values.add(String.valueOf(dayOfWeek));
  }
  if (weekendEncoder != null) {
    boolean isWeekend=dayOfWeek == 6 || dayOfWeek == 5 || (dayOfWeek == 4 && timeOfDay > 18);
    int weekend=isWeekend ? 1 : 0;
    values.add(String.valueOf(weekend));
  }
  if (customDaysEncoder != null) {
    boolean isCustomDays=customDaysList.contains(dayOfWeek);
    int customDay=isCustomDays ? 1 : 0;
    values.add(String.valueOf(customDay));
  }
  if (holidayEncoder != null) {
    double holidayness=0;
    for (    Tuple h : holidaysList) {
      DateTime hdate=new DateTime(date.getYear(),(int)h.get(0),(int)h.get(1),0,0,0);
      if (date.isAfter(hdate)) {
        Duration diff=new Interval(hdate,date).toDuration();
        long days=diff.getStandardDays();
        if (days == 0) {
          holidayness=1;
          break;
        }
 else         if (days == 1) {
          holidayness=1.0 - ((diff.getStandardSeconds() - 86400.0 * days) / 86400.0);
          break;
        }
      }
 else {
        Duration diff=new Interval(date,hdate).toDuration();
        long days=diff.getStandardDays();
        if (days == 0) {
          holidayness=1.0 - ((diff.getStandardSeconds() - 86400.0 * days) / 86400.0);
        }
      }
    }
    values.add(String.valueOf(holidayness));
  }
  if (timeOfDayEncoder != null) {
    values.add(String.valueOf(timeOfDay));
  }
  return values;
}"
4538,"/** 
 * look at holiday more carefully because of the smooth transition
 */
@Test public void testHoliday(){
  DateEncoder e=DateEncoder.builder().holiday(5).forced(true).build();
  int[] holiday=new int[]{0,0,0,0,0,1,1,1,1,1};
  int[] notholiday=new int[]{1,1,1,1,1,0,0,0,0,0};
  int[] holiday2=new int[]{0,0,0,1,1,1,1,1,0,0};
  Date d=new DateTime(2010,12,25,4,55).toDate();
  assertArrayEquals(holiday,e.encode(d));
  d=new DateTime(2008,12,27,4,55).toDate();
  assertArrayEquals(notholiday,e.encode(d));
  d=new DateTime(1999,12,26,8,00).toDate();
  assertArrayEquals(holiday2,e.encode(d));
  d=new DateTime(2011,12,24,16,00).toDate();
  assertArrayEquals(holiday2,e.encode(d));
}","/** 
 * look at holiday more carefully because of the smooth transition
 */
@Test public void testHoliday(){
  DateEncoder e=DateEncoder.builder().holiday(5).forced(true).build();
  int[] holiday=new int[]{0,0,0,0,0,1,1,1,1,1};
  int[] notholiday=new int[]{1,1,1,1,1,0,0,0,0,0};
  int[] holiday2=new int[]{0,0,0,1,1,1,1,1,0,0};
  Date d=new DateTime(2010,12,25,4,55).toDate();
  assertArrayEquals(holiday,e.encode(d));
  d=new DateTime(2008,12,27,4,55).toDate();
  assertArrayEquals(notholiday,e.encode(d));
  d=new DateTime(1999,12,26,8,0).toDate();
  assertArrayEquals(holiday2,e.encode(d));
  d=new DateTime(2011,12,24,16,0).toDate();
  assertArrayEquals(holiday2,e.encode(d));
}"
4539,"/** 
 * Test weekend encoder
 */
@Test public void testWeekend(){
  DateEncoder e=DateEncoder.builder().customDays(21,Arrays.asList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")).forced(true).build();
  DateEncoder mon=DateEncoder.builder().customDays(21,Arrays.asList(""String_Node_Str"")).forced(true).build();
  DateEncoder e2=DateEncoder.builder().weekend(21,1).forced(true).build();
  DateTime d=new DateTime(1988,5,29,20,00);
  assertArrayEquals(e.encode(d.toDate()),e2.encode(d.toDate()));
  for (int i=0; i < 300; i++) {
    DateTime curDate=d.plusDays(i + 1);
    assertArrayEquals(e.encode(curDate.toDate()),e2.encode(curDate.toDate()));
    Tuple decoded=mon.decode(mon.encode(curDate.toDate()),null);
    Map<String,RangeList> fieldsMap=(Map<String,RangeList>)decoded.get(0);
    List<String> fieldsOrder=(List<String>)decoded.get(1);
    assertNotNull(fieldsMap);
    assertNotNull(fieldsOrder);
    assertEquals(1,fieldsMap.size());
    RangeList range=fieldsMap.get(""String_Node_Str"");
    assertEquals(1,range.size());
    assertEquals(1,((List<MinMax>)range.get(0)).size());
    MinMax minmax=range.getRange(0);
    if (minmax.min() == 1.0) {
      assertEquals(1,curDate.getDayOfWeek());
    }
 else {
      assertNotEquals(1,curDate.getDayOfWeek());
    }
  }
}","/** 
 * Test weekend encoder
 */
@Test public void testWeekend(){
  DateEncoder e=DateEncoder.builder().customDays(21,Arrays.asList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")).forced(true).build();
  DateEncoder mon=DateEncoder.builder().customDays(21,Arrays.asList(""String_Node_Str"")).forced(true).build();
  DateEncoder e2=DateEncoder.builder().weekend(21,1).forced(true).build();
  DateTime d=new DateTime(1988,5,29,20,0);
  assertArrayEquals(e.encode(d.toDate()),e2.encode(d.toDate()));
  for (int i=0; i < 300; i++) {
    DateTime curDate=d.plusDays(i + 1);
    assertArrayEquals(e.encode(curDate.toDate()),e2.encode(curDate.toDate()));
    Tuple decoded=mon.decode(mon.encode(curDate.toDate()),null);
    Map<String,RangeList> fieldsMap=(Map<String,RangeList>)decoded.get(0);
    List<String> fieldsOrder=(List<String>)decoded.get(1);
    assertNotNull(fieldsMap);
    assertNotNull(fieldsOrder);
    assertEquals(1,fieldsMap.size());
    RangeList range=fieldsMap.get(""String_Node_Str"");
    assertEquals(1,range.size());
    assertEquals(1,((List<MinMax>)range.get(0)).size());
    MinMax minmax=range.getRange(0);
    if (minmax.min() == 1.0) {
      assertEquals(1,curDate.getDayOfWeek());
    }
 else {
      assertNotEquals(1,curDate.getDayOfWeek());
    }
  }
}"
4540,"@Override public String toString(){
  StringBuilder sb=new StringBuilder();
  for (int i=0; i < container.length; i++) {
    try {
      new Double((double)container[i]);
      sb.append(container[i]);
    }
 catch (    Exception e) {
      sb.append(""String_Node_Str"").append(container[i]).append(""String_Node_Str"");
    }
    sb.append(""String_Node_Str"");
  }
  sb.setLength(sb.length() - 1);
  return sb.toString();
}","@Override public String toString(){
  StringBuilder sb=new StringBuilder();
  for (int i=0; i < container.length; i++) {
    try {
      new Double(Double.parseDouble(container[i] + ""String_Node_Str""));
      sb.append(container[i]);
    }
 catch (    Exception e) {
      sb.append(""String_Node_Str"").append(container[i]).append(""String_Node_Str"");
    }
    sb.append(""String_Node_Str"");
  }
  sb.setLength(sb.length() - 1);
  return sb.toString();
}"
4541,"/** 
 * Test the input description generation, top-down compute, and bucket support on a periodic encoder
 */
@Test public void testDecodeAndResolution(){
  setUp();
  parameters.setName(""String_Node_Str"");
  initSE();
  double resolution=mem.getResolution();
  System.out.println(""String_Node_Str"" + resolution);
  StringBuilder out=new StringBuilder();
  for (double v=mem.getMinVal(); v < mem.getMaxVal(); v+=(resolution / 4.0d)) {
    int[] output=se.encode(mem,v);
    Decode decoded=se.decode(mem,output,""String_Node_Str"");
    System.out.println(out.append(""String_Node_Str"").append(Arrays.toString(output)).append(""String_Node_Str"").append(String.format(""String_Node_Str"",v)).append(""String_Node_Str"").append(se.decodedToStr(decoded)));
    out.setLength(0);
    Map<String,Ranges> fieldsMap=decoded.getFields();
    assertEquals(1,fieldsMap.size());
    Ranges ranges=(Ranges)new ArrayList<Ranges>(fieldsMap.values()).get(0);
    assertEquals(1,ranges.size());
    assertEquals(ranges.getRange(0).min(),ranges.getRange(0).max(),0);
    assertTrue(ranges.getRange(0).min() - v < mem.getResolution());
    EncoderResult topDown=se.topDownCompute(mem,output);
    System.out.println(""String_Node_Str"" + topDown);
    assertTrue(topDown.get(3).equals(Arrays.toString(output)));
    assertTrue(Math.abs(((double)topDown.get(1)) - v) <= mem.getResolution() / 2);
    int[] bucketIndices=se.getBucketIndices(mem,v);
    System.out.println(""String_Node_Str"" + bucketIndices[0]);
    topDown=se.getBucketInfo(mem,bucketIndices);
    assertTrue(Math.abs(((double)topDown.get(1)) - v) <= mem.getResolution() / 2);
    assertEquals(topDown.get(1),se.getBucketValues(mem).toArray()[bucketIndices[0]]);
    assertEquals(topDown.get(2),topDown.get(1));
    assertTrue(topDown.get(3).equals(Arrays.toString(output)));
  }
  setUp();
  parameters.setName(""String_Node_Str"");
  parameters.setRadius(1.5);
  parameters.setW(3);
  parameters.setMinVal(1);
  parameters.setMaxVal(8);
  parameters.setPeriodic(true);
  parameters.setForced(true);
  initSE();
  System.out.println(""String_Node_Str"" + mem.getResolution());
  int[] encoded=new int[]{1,0,0,0,0,0,0,0,0,0,0,0,1,0};
  Decode decoded=se.decode(mem,encoded,""String_Node_Str"");
  Map<String,Ranges> fieldsMap=decoded.getFields();
  assertEquals(1,fieldsMap.size());
  assertEquals(1,decoded.getRanges(""String_Node_Str"").size());
  assertEquals(decoded.getRanges(""String_Node_Str"").getRange(0).toString(),""String_Node_Str"");
  encoded=new int[]{1,1,0,0,0,0,0,0,0,0,0,0,1,0};
  decoded=se.decode(mem,encoded,""String_Node_Str"");
  fieldsMap=decoded.getFields();
  assertEquals(1,fieldsMap.size());
  assertEquals(2,decoded.getRanges(""String_Node_Str"").size());
  assertEquals(decoded.getRanges(""String_Node_Str"").getRange(0).toString(),""String_Node_Str"");
  encoded=new int[]{1,1,1,1,1,0,0,0,0,0,0,0,0,0};
  decoded=se.decode(mem,encoded,""String_Node_Str"");
  fieldsMap=decoded.getFields();
  assertEquals(1,fieldsMap.size());
  assertEquals(1,decoded.getRanges(""String_Node_Str"").size());
  assertEquals(decoded.getRanges(""String_Node_Str"").getRange(0).toString(),""String_Node_Str"");
  encoded=new int[]{1,1,1,0,0,0,0,0,1,1,1,1,0,0};
  decoded=se.decode(mem,encoded,""String_Node_Str"");
  fieldsMap=decoded.getFields();
  assertEquals(1,fieldsMap.size());
  assertEquals(2,decoded.getRanges(""String_Node_Str"").size());
  assertEquals(decoded.getRanges(""String_Node_Str"").getRange(0).toString(),""String_Node_Str"");
  assertEquals(decoded.getRanges(""String_Node_Str"").getRange(1).toString(),""String_Node_Str"");
  encoded=new int[]{0,1,0,0,0,0,0,0,1,1,1,1,0,0};
  decoded=se.decode(mem,encoded,""String_Node_Str"");
  fieldsMap=decoded.getFields();
  assertEquals(1,fieldsMap.size());
  assertEquals(2,decoded.getRanges(""String_Node_Str"").size());
  assertEquals(decoded.getRanges(""String_Node_Str"").getRange(0).toString(),""String_Node_Str"");
  assertEquals(decoded.getRanges(""String_Node_Str"").getRange(1).toString(),""String_Node_Str"");
}","/** 
 * Test the input description generation, top-down compute, and bucket support on a periodic encoder
 */
@Test public void testDecodeAndResolution(){
  setUp();
  parameters.setName(""String_Node_Str"");
  initSE();
  double resolution=mem.getResolution();
  System.out.println(""String_Node_Str"" + resolution);
  StringBuilder out=new StringBuilder();
  for (double v=mem.getMinVal(); v < mem.getMaxVal(); v+=(resolution / 4.0d)) {
    int[] output=se.encode(mem,v);
    Decode decoded=se.decode(mem,output,""String_Node_Str"");
    System.out.println(out.append(""String_Node_Str"").append(Arrays.toString(output)).append(""String_Node_Str"").append(String.format(""String_Node_Str"",v)).append(""String_Node_Str"").append(se.decodedToStr(decoded)));
    out.setLength(0);
    Map<String,Ranges> fieldsMap=decoded.getFields();
    assertEquals(1,fieldsMap.size());
    Ranges ranges=(Ranges)new ArrayList<Ranges>(fieldsMap.values()).get(0);
    assertEquals(1,ranges.size());
    assertEquals(ranges.getRange(0).min(),ranges.getRange(0).max(),0);
    assertTrue(ranges.getRange(0).min() - v < mem.getResolution());
    EncoderResult topDown=se.topDownCompute(mem,output);
    System.out.println(""String_Node_Str"" + topDown);
    assertTrue(topDown.get(3).equals(Arrays.toString(output)));
    assertTrue(Math.abs(((Double)topDown.get(1)) - v) <= mem.getResolution() / 2);
    int[] bucketIndices=se.getBucketIndices(mem,v);
    System.out.println(""String_Node_Str"" + bucketIndices[0]);
    topDown=se.getBucketInfo(mem,bucketIndices);
    assertTrue(Math.abs(((Double)topDown.get(1)) - v) <= mem.getResolution() / 2);
    assertEquals(topDown.get(1),se.getBucketValues(mem).toArray()[bucketIndices[0]]);
    assertEquals(topDown.get(2),topDown.get(1));
    assertTrue(topDown.get(3).equals(Arrays.toString(output)));
  }
  setUp();
  parameters.setName(""String_Node_Str"");
  parameters.setRadius(1.5);
  parameters.setW(3);
  parameters.setMinVal(1);
  parameters.setMaxVal(8);
  parameters.setPeriodic(true);
  parameters.setForced(true);
  initSE();
  System.out.println(""String_Node_Str"" + mem.getResolution());
  int[] encoded=new int[]{1,0,0,0,0,0,0,0,0,0,0,0,1,0};
  Decode decoded=se.decode(mem,encoded,""String_Node_Str"");
  Map<String,Ranges> fieldsMap=decoded.getFields();
  assertEquals(1,fieldsMap.size());
  assertEquals(1,decoded.getRanges(""String_Node_Str"").size());
  assertEquals(decoded.getRanges(""String_Node_Str"").getRange(0).toString(),""String_Node_Str"");
  encoded=new int[]{1,1,0,0,0,0,0,0,0,0,0,0,1,0};
  decoded=se.decode(mem,encoded,""String_Node_Str"");
  fieldsMap=decoded.getFields();
  assertEquals(1,fieldsMap.size());
  assertEquals(2,decoded.getRanges(""String_Node_Str"").size());
  assertEquals(decoded.getRanges(""String_Node_Str"").getRange(0).toString(),""String_Node_Str"");
  encoded=new int[]{1,1,1,1,1,0,0,0,0,0,0,0,0,0};
  decoded=se.decode(mem,encoded,""String_Node_Str"");
  fieldsMap=decoded.getFields();
  assertEquals(1,fieldsMap.size());
  assertEquals(1,decoded.getRanges(""String_Node_Str"").size());
  assertEquals(decoded.getRanges(""String_Node_Str"").getRange(0).toString(),""String_Node_Str"");
  encoded=new int[]{1,1,1,0,0,0,0,0,1,1,1,1,0,0};
  decoded=se.decode(mem,encoded,""String_Node_Str"");
  fieldsMap=decoded.getFields();
  assertEquals(1,fieldsMap.size());
  assertEquals(2,decoded.getRanges(""String_Node_Str"").size());
  assertEquals(decoded.getRanges(""String_Node_Str"").getRange(0).toString(),""String_Node_Str"");
  assertEquals(decoded.getRanges(""String_Node_Str"").getRange(1).toString(),""String_Node_Str"");
  encoded=new int[]{0,1,0,0,0,0,0,0,1,1,1,1,0,0};
  decoded=se.decode(mem,encoded,""String_Node_Str"");
  fieldsMap=decoded.getFields();
  assertEquals(1,fieldsMap.size());
  assertEquals(2,decoded.getRanges(""String_Node_Str"").size());
  assertEquals(decoded.getRanges(""String_Node_Str"").getRange(0).toString(),""String_Node_Str"");
  assertEquals(decoded.getRanges(""String_Node_Str"").getRange(1).toString(),""String_Node_Str"");
}"
4542,"@Test public void testProperties() throws Exception {
  txnl.execute(() -> {
    Assert.assertEquals(0,dataset.getProperties(app1).size());
    Assert.assertEquals(0,dataset.getProperties(flow1).size());
    Assert.assertEquals(0,dataset.getProperties(dataset1).size());
    Assert.assertEquals(0,dataset.getProperties(stream1).size());
    Assert.assertEquals(0,dataset.getProperties(view1).size());
    Assert.assertEquals(0,dataset.getProperties(artifact1).size());
    Assert.assertEquals(0,dataset.getProperties(fileEntity).size());
    Assert.assertEquals(0,dataset.getProperties(partitionFileEntity).size());
    Assert.assertEquals(0,dataset.getProperties(jarEntity).size());
  }
);
  txnl.execute(() -> {
    dataset.setProperty(app1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(dataset1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(stream1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(stream1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(view1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(view1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(artifact1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(artifact1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(fileEntity,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(partitionFileEntity,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(jarEntity,""String_Node_Str"",""String_Node_Str"");
  }
);
  txnl.execute(() -> {
    Map<String,String> properties=dataset.getProperties(app1);
    Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),properties);
  }
);
  txnl.execute(() -> dataset.removeProperties(app1,""String_Node_Str""));
  txnl.execute(() -> {
    Map<String,String> properties=dataset.getProperties(jarEntity);
    Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),properties);
  }
);
  txnl.execute(() -> dataset.removeProperties(jarEntity,""String_Node_Str""));
  txnl.execute(() -> {
    Assert.assertNull(dataset.getProperty(app1,""String_Node_Str""));
    MetadataEntry result=dataset.getProperty(flow1,""String_Node_Str"");
    MetadataEntry expected=new MetadataEntry(flow1,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(expected,result);
    Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),dataset.getProperties(flow1));
  }
);
  txnl.execute(() -> dataset.removeProperties(flow1,""String_Node_Str""));
  txnl.execute(() -> {
    Map<String,String> properties=dataset.getProperties(flow1);
    Assert.assertEquals(1,properties.size());
    Assert.assertEquals(""String_Node_Str"",properties.get(""String_Node_Str""));
  }
);
  txnl.execute(() -> dataset.removeProperties(flow1));
  txnl.execute(() -> {
    Assert.assertEquals(0,dataset.getProperties(flow1).size());
    Assert.assertEquals(0,dataset.getProperties(jarEntity).size());
    MetadataEntry expected=new MetadataEntry(dataset1,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(expected,dataset.getProperty(dataset1,""String_Node_Str""));
    Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),dataset.getProperties(stream1));
    Map<String,String> properties=dataset.getProperties(artifact1);
    Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),properties);
    MetadataEntry result=dataset.getProperty(artifact1,""String_Node_Str"");
    expected=new MetadataEntry(artifact1,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(expected,result);
    properties=dataset.getProperties(view1);
    Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),properties);
    result=dataset.getProperty(view1,""String_Node_Str"");
    expected=new MetadataEntry(view1,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(expected,result);
    result=dataset.getProperty(fileEntity,""String_Node_Str"");
    expected=new MetadataEntry(fileEntity,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(expected,result);
    result=dataset.getProperty(partitionFileEntity,""String_Node_Str"");
    expected=new MetadataEntry(partitionFileEntity,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(expected,result);
  }
);
  txnl.execute(() -> dataset.setProperty(stream1,""String_Node_Str"",""String_Node_Str""));
  txnl.execute(() -> Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),dataset.getProperties(stream1)));
  txnl.execute(() -> {
    dataset.removeProperties(app1);
    dataset.removeProperties(flow1);
    dataset.removeProperties(dataset1);
    dataset.removeProperties(stream1);
    dataset.removeProperties(artifact1);
    dataset.removeProperties(view1);
    dataset.removeProperties(fileEntity);
    dataset.removeProperties(partitionFileEntity);
  }
);
  txnl.execute(() -> {
    Assert.assertEquals(0,dataset.getProperties(app1).size());
    Assert.assertEquals(0,dataset.getProperties(flow1).size());
    Assert.assertEquals(0,dataset.getProperties(dataset1).size());
    Assert.assertEquals(0,dataset.getProperties(stream1).size());
    Assert.assertEquals(0,dataset.getProperties(view1).size());
    Assert.assertEquals(0,dataset.getProperties(artifact1).size());
    Assert.assertEquals(0,dataset.getProperties(fileEntity).size());
    Assert.assertEquals(0,dataset.getProperties(partitionFileEntity).size());
    Assert.assertEquals(0,dataset.getProperties(jarEntity).size());
  }
);
}","@Test public void testProperties() throws Exception {
  txnl.execute(() -> {
    Assert.assertEquals(0,dataset.getProperties(app1).size());
    Assert.assertEquals(0,dataset.getProperties(flow1).size());
    Assert.assertEquals(0,dataset.getProperties(dataset1).size());
    Assert.assertEquals(0,dataset.getProperties(stream1).size());
    Assert.assertEquals(0,dataset.getProperties(view1).size());
    Assert.assertEquals(0,dataset.getProperties(artifact1).size());
    Assert.assertEquals(0,dataset.getProperties(fileEntity).size());
    Assert.assertEquals(0,dataset.getProperties(partitionFileEntity).size());
    Assert.assertEquals(0,dataset.getProperties(jarEntity).size());
  }
);
  txnl.execute(() -> {
    dataset.setProperty(app1,""String_Node_Str"",""String_Node_Str"");
    MetadataChange metadataChange=dataset.setProperty(flow1,Collections.emptyMap());
    Assert.assertEquals(metadataChange.getExisting(),new Metadata(flow1,Collections.emptyMap(),Collections.emptySet()));
    Assert.assertEquals(metadataChange.getLatest(),new Metadata(flow1,Collections.emptyMap(),Collections.emptySet()));
    metadataChange=dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(new Metadata(flow1),metadataChange.getExisting());
    Assert.assertEquals(new Metadata(flow1,ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),Collections.emptySet()),metadataChange.getLatest());
    metadataChange=dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(new Metadata(flow1,ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),Collections.emptySet()),metadataChange.getExisting());
    Assert.assertEquals(new Metadata(flow1,ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),Collections.emptySet()),metadataChange.getLatest());
    dataset.setProperty(dataset1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(stream1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(stream1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(view1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(view1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(artifact1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(artifact1,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(fileEntity,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(partitionFileEntity,""String_Node_Str"",""String_Node_Str"");
    dataset.setProperty(jarEntity,""String_Node_Str"",""String_Node_Str"");
  }
);
  txnl.execute(() -> {
    Map<String,String> properties=dataset.getProperties(app1);
    Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),properties);
  }
);
  txnl.execute(() -> dataset.removeProperties(app1,""String_Node_Str""));
  txnl.execute(() -> {
    Map<String,String> properties=dataset.getProperties(jarEntity);
    Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),properties);
  }
);
  txnl.execute(() -> dataset.removeProperties(jarEntity,""String_Node_Str""));
  txnl.execute(() -> {
    Assert.assertNull(dataset.getProperty(app1,""String_Node_Str""));
    MetadataEntry result=dataset.getProperty(flow1,""String_Node_Str"");
    MetadataEntry expected=new MetadataEntry(flow1,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(expected,result);
    Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),dataset.getProperties(flow1));
  }
);
  txnl.execute(() -> dataset.removeProperties(flow1,""String_Node_Str""));
  txnl.execute(() -> {
    Map<String,String> properties=dataset.getProperties(flow1);
    Assert.assertEquals(1,properties.size());
    Assert.assertEquals(""String_Node_Str"",properties.get(""String_Node_Str""));
  }
);
  txnl.execute(() -> dataset.removeProperties(flow1));
  txnl.execute(() -> {
    Assert.assertEquals(0,dataset.getProperties(flow1).size());
    Assert.assertEquals(0,dataset.getProperties(jarEntity).size());
    MetadataEntry expected=new MetadataEntry(dataset1,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(expected,dataset.getProperty(dataset1,""String_Node_Str""));
    Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),dataset.getProperties(stream1));
    Map<String,String> properties=dataset.getProperties(artifact1);
    Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),properties);
    MetadataEntry result=dataset.getProperty(artifact1,""String_Node_Str"");
    expected=new MetadataEntry(artifact1,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(expected,result);
    properties=dataset.getProperties(view1);
    Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),properties);
    result=dataset.getProperty(view1,""String_Node_Str"");
    expected=new MetadataEntry(view1,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(expected,result);
    result=dataset.getProperty(fileEntity,""String_Node_Str"");
    expected=new MetadataEntry(fileEntity,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(expected,result);
    result=dataset.getProperty(partitionFileEntity,""String_Node_Str"");
    expected=new MetadataEntry(partitionFileEntity,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(expected,result);
  }
);
  txnl.execute(() -> dataset.setProperty(stream1,""String_Node_Str"",""String_Node_Str""));
  txnl.execute(() -> Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),dataset.getProperties(stream1)));
  txnl.execute(() -> {
    dataset.removeProperties(app1);
    dataset.removeProperties(flow1);
    dataset.removeProperties(dataset1);
    dataset.removeProperties(stream1);
    dataset.removeProperties(artifact1);
    dataset.removeProperties(view1);
    dataset.removeProperties(fileEntity);
    dataset.removeProperties(partitionFileEntity);
  }
);
  txnl.execute(() -> {
    Assert.assertEquals(0,dataset.getProperties(app1).size());
    Assert.assertEquals(0,dataset.getProperties(flow1).size());
    Assert.assertEquals(0,dataset.getProperties(dataset1).size());
    Assert.assertEquals(0,dataset.getProperties(stream1).size());
    Assert.assertEquals(0,dataset.getProperties(view1).size());
    Assert.assertEquals(0,dataset.getProperties(artifact1).size());
    Assert.assertEquals(0,dataset.getProperties(fileEntity).size());
    Assert.assertEquals(0,dataset.getProperties(partitionFileEntity).size());
    Assert.assertEquals(0,dataset.getProperties(jarEntity).size());
  }
);
}"
4543,"@Test public void testTags() throws InterruptedException, TransactionFailureException {
  txnl.execute(() -> {
    Assert.assertEquals(0,dataset.getTags(app1).size());
    Assert.assertEquals(0,dataset.getTags(flow1).size());
    Assert.assertEquals(0,dataset.getTags(dataset1).size());
    Assert.assertEquals(0,dataset.getTags(stream1).size());
    Assert.assertEquals(0,dataset.getTags(view1).size());
    Assert.assertEquals(0,dataset.getTags(artifact1).size());
    Assert.assertEquals(0,dataset.getTags(fileEntity).size());
    Assert.assertEquals(0,dataset.getTags(partitionFileEntity).size());
    Assert.assertEquals(0,dataset.getTags(jarEntity).size());
  }
);
  txnl.execute(() -> {
    dataset.addTags(app1,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
    dataset.addTags(flow1,""String_Node_Str"");
    dataset.addTags(dataset1,""String_Node_Str"",""String_Node_Str"");
    dataset.addTags(stream1,""String_Node_Str"");
    dataset.addTags(view1,""String_Node_Str"");
    dataset.addTags(artifact1,""String_Node_Str"");
    dataset.addTags(fileEntity,""String_Node_Str"");
    dataset.addTags(partitionFileEntity,""String_Node_Str"");
    dataset.addTags(jarEntity,""String_Node_Str"",""String_Node_Str"");
  }
);
  txnl.execute(() -> {
    Set<String> tags=dataset.getTags(app1);
    Assert.assertEquals(3,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    Assert.assertTrue(tags.contains(""String_Node_Str""));
  }
);
  txnl.execute(() -> {
    Assert.assertEquals(3,dataset.getTags(app1).size());
    dataset.addTags(app1,""String_Node_Str"");
    Set<String> tags=dataset.getTags(app1);
    Assert.assertEquals(3,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    Assert.assertTrue(tags.contains(""String_Node_Str""));
  }
);
  txnl.execute(() -> dataset.addTags(app1,""String_Node_Str""));
  txnl.execute(() -> {
    Assert.assertEquals(3,dataset.getTags(app1).size());
    Set<String> tags=dataset.getTags(flow1);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    tags=dataset.getTags(dataset1);
    Assert.assertEquals(2,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    tags=dataset.getTags(stream1);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    tags=dataset.getTags(view1);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    tags=dataset.getTags(fileEntity);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    tags=dataset.getTags(partitionFileEntity);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    tags=dataset.getTags(jarEntity);
    Assert.assertEquals(2,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    Assert.assertTrue(tags.contains(""String_Node_Str""));
  }
);
  txnl.execute(() -> dataset.removeTags(app1,""String_Node_Str"",""String_Node_Str""));
  txnl.execute(() -> {
    Set<String> tags=dataset.getTags(app1);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
  }
);
  txnl.execute(() -> dataset.removeTags(dataset1,""String_Node_Str""));
  txnl.execute(() -> dataset.removeTags(jarEntity,""String_Node_Str""));
  txnl.execute(() -> {
    Set<String> tags=dataset.getTags(dataset1);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    tags=dataset.getTags(artifact1);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    tags=dataset.getTags(jarEntity);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
  }
);
  txnl.execute(() -> {
    dataset.removeTags(app1);
    dataset.removeTags(flow1);
    dataset.removeTags(dataset1);
    dataset.removeTags(stream1);
    dataset.removeTags(view1);
    dataset.removeTags(artifact1);
    dataset.removeTags(fileEntity);
    dataset.removeTags(partitionFileEntity);
    dataset.removeTags(jarEntity);
  }
);
  txnl.execute(() -> {
    Assert.assertEquals(0,dataset.getTags(app1).size());
    Assert.assertEquals(0,dataset.getTags(flow1).size());
    Assert.assertEquals(0,dataset.getTags(dataset1).size());
    Assert.assertEquals(0,dataset.getTags(stream1).size());
    Assert.assertEquals(0,dataset.getTags(view1).size());
    Assert.assertEquals(0,dataset.getTags(artifact1).size());
    Assert.assertEquals(0,dataset.getTags(fileEntity).size());
    Assert.assertEquals(0,dataset.getTags(partitionFileEntity).size());
    Assert.assertEquals(0,dataset.getTags(jarEntity).size());
  }
);
}","@Test public void testTags() throws InterruptedException, TransactionFailureException {
  txnl.execute(() -> {
    Assert.assertEquals(0,dataset.getTags(app1).size());
    Assert.assertEquals(0,dataset.getTags(flow1).size());
    Assert.assertEquals(0,dataset.getTags(dataset1).size());
    Assert.assertEquals(0,dataset.getTags(stream1).size());
    Assert.assertEquals(0,dataset.getTags(view1).size());
    Assert.assertEquals(0,dataset.getTags(artifact1).size());
    Assert.assertEquals(0,dataset.getTags(fileEntity).size());
    Assert.assertEquals(0,dataset.getTags(partitionFileEntity).size());
    Assert.assertEquals(0,dataset.getTags(jarEntity).size());
  }
);
  txnl.execute(() -> {
    dataset.addTags(app1,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
    MetadataChange metadataChange=dataset.addTags(flow1,Collections.emptySet());
    Assert.assertEquals(metadataChange.getExisting(),new Metadata(flow1,Collections.emptyMap(),Collections.emptySet()));
    Assert.assertEquals(metadataChange.getLatest(),new Metadata(flow1,Collections.emptyMap(),Collections.emptySet()));
    metadataChange=dataset.addTags(flow1,""String_Node_Str"");
    Assert.assertEquals(new Metadata(flow1),metadataChange.getExisting());
    Assert.assertEquals(new Metadata(flow1,Collections.emptyMap(),ImmutableSet.of(""String_Node_Str"")),metadataChange.getLatest());
    metadataChange=dataset.addTags(flow1,""String_Node_Str"");
    Assert.assertEquals(new Metadata(flow1,Collections.emptyMap(),ImmutableSet.of(""String_Node_Str"")),metadataChange.getExisting());
    Assert.assertEquals(new Metadata(flow1,Collections.emptyMap(),ImmutableSet.of(""String_Node_Str"",""String_Node_Str"")),metadataChange.getLatest());
    dataset.addTags(dataset1,""String_Node_Str"",""String_Node_Str"");
    dataset.addTags(stream1,""String_Node_Str"");
    dataset.addTags(view1,""String_Node_Str"");
    dataset.addTags(artifact1,""String_Node_Str"");
    dataset.addTags(fileEntity,""String_Node_Str"");
    dataset.addTags(partitionFileEntity,""String_Node_Str"");
    dataset.addTags(jarEntity,""String_Node_Str"",""String_Node_Str"");
  }
);
  txnl.execute(() -> {
    Set<String> tags=dataset.getTags(app1);
    Assert.assertEquals(3,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    Assert.assertTrue(tags.contains(""String_Node_Str""));
  }
);
  txnl.execute(() -> {
    Assert.assertEquals(3,dataset.getTags(app1).size());
    dataset.addTags(app1,""String_Node_Str"");
    Set<String> tags=dataset.getTags(app1);
    Assert.assertEquals(3,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    Assert.assertTrue(tags.contains(""String_Node_Str""));
  }
);
  txnl.execute(() -> dataset.addTags(app1,""String_Node_Str""));
  txnl.execute(() -> {
    Assert.assertEquals(3,dataset.getTags(app1).size());
    Set<String> tags=dataset.getTags(flow1);
    Assert.assertEquals(2,tags.size());
    Assert.assertTrue(tags.containsAll(ImmutableSet.of(""String_Node_Str"",""String_Node_Str"")));
    tags=dataset.getTags(dataset1);
    Assert.assertEquals(2,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    tags=dataset.getTags(stream1);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    tags=dataset.getTags(view1);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    tags=dataset.getTags(fileEntity);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    tags=dataset.getTags(partitionFileEntity);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    tags=dataset.getTags(jarEntity);
    Assert.assertEquals(2,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    Assert.assertTrue(tags.contains(""String_Node_Str""));
  }
);
  txnl.execute(() -> dataset.removeTags(app1,""String_Node_Str"",""String_Node_Str""));
  txnl.execute(() -> {
    Set<String> tags=dataset.getTags(app1);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
  }
);
  txnl.execute(() -> dataset.removeTags(dataset1,""String_Node_Str""));
  txnl.execute(() -> dataset.removeTags(jarEntity,""String_Node_Str""));
  txnl.execute(() -> {
    Set<String> tags=dataset.getTags(dataset1);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    tags=dataset.getTags(artifact1);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
    tags=dataset.getTags(jarEntity);
    Assert.assertEquals(1,tags.size());
    Assert.assertTrue(tags.contains(""String_Node_Str""));
  }
);
  txnl.execute(() -> {
    dataset.removeTags(app1);
    dataset.removeTags(flow1);
    dataset.removeTags(dataset1);
    dataset.removeTags(stream1);
    dataset.removeTags(view1);
    dataset.removeTags(artifact1);
    dataset.removeTags(fileEntity);
    dataset.removeTags(partitionFileEntity);
    dataset.removeTags(jarEntity);
  }
);
  txnl.execute(() -> {
    Assert.assertEquals(0,dataset.getTags(app1).size());
    Assert.assertEquals(0,dataset.getTags(flow1).size());
    Assert.assertEquals(0,dataset.getTags(dataset1).size());
    Assert.assertEquals(0,dataset.getTags(stream1).size());
    Assert.assertEquals(0,dataset.getTags(view1).size());
    Assert.assertEquals(0,dataset.getTags(artifact1).size());
    Assert.assertEquals(0,dataset.getTags(fileEntity).size());
    Assert.assertEquals(0,dataset.getTags(partitionFileEntity).size());
    Assert.assertEquals(0,dataset.getTags(jarEntity).size());
  }
);
}"
4544,"/** 
 * Get the metrics context for the program, the tags are constructed with the program run id and the profile id
 */
private MetricsContext getMetricsContextForProfile(MetricsCollectionService metricsCollectionService,ProgramRunId programRunId,ProfileId profileId){
  Map<String,String> tags=ImmutableMap.<String,String>builder().put(Constants.Metrics.Tag.PROFILE_SCOPE,profileId.getScope().name()).put(Constants.Metrics.Tag.PROFILE,profileId.getScopedName()).put(Constants.Metrics.Tag.NAMESPACE,programRunId.getNamespace()).put(Constants.Metrics.Tag.PROGRAM_TYPE,programRunId.getType().getPrettyName()).put(Constants.Metrics.Tag.APP,programRunId.getApplication()).put(Constants.Metrics.Tag.PROGRAM,programRunId.getProgram()).put(Constants.Metrics.Tag.RUN_ID,programRunId.getRun()).build();
  return metricsCollectionService.getContext(tags);
}","/** 
 * Get the metrics context for the program, the tags are constructed with the program run id and the profile id
 */
private MetricsContext getMetricsContextForProfile(MetricsCollectionService metricsCollectionService,ProgramRunId programRunId,ProfileId profileId){
  Map<String,String> tags=ImmutableMap.<String,String>builder().put(Constants.Metrics.Tag.PROFILE_SCOPE,profileId.getScope().name()).put(Constants.Metrics.Tag.PROFILE,profileId.getProfile()).put(Constants.Metrics.Tag.NAMESPACE,programRunId.getNamespace()).put(Constants.Metrics.Tag.PROGRAM_TYPE,programRunId.getType().getPrettyName()).put(Constants.Metrics.Tag.APP,programRunId.getApplication()).put(Constants.Metrics.Tag.PROGRAM,programRunId.getProgram()).put(Constants.Metrics.Tag.RUN_ID,programRunId.getRun()).build();
  return metricsCollectionService.getContext(tags);
}"
4545,"@Override public void initializeCluster(ProvisionerContext context,Cluster cluster) throws Exception {
  try (SSHSession session=context.getSSHContext().createSSHSession(getMasterExternalIp(cluster))){
    LOG.debug(""String_Node_Str"");
    String output=session.executeAndWait(""String_Node_Str"");
    LOG.debug(""String_Node_Str"",output);
  }
 }","@Override public void initializeCluster(ProvisionerContext context,Cluster cluster) throws Exception {
  try (SSHSession session=createSSHSession(context,getMasterExternalIp(cluster))){
    LOG.debug(""String_Node_Str"");
    String output=session.executeAndWait(""String_Node_Str"");
    LOG.debug(""String_Node_Str"",output);
  }
 }"
4546,"@GET @Path(""String_Node_Str"") public void getUpgradeStatus(HttpRequest request,HttpResponder responder) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,GSON.toJson(appVersionUpgradeService.getUpgradeStatus()));
}","@GET @Path(""String_Node_Str"") public void getUpgradeStatus(HttpRequest request,HttpResponder responder) throws Exception {
  upgradeStatus.put(""String_Node_Str"",metadataService.isMigrationInProcess());
  responder.sendJson(HttpResponseStatus.OK,GSON.toJson(upgradeStatus));
}"
4547,"@Inject UpgradeHttpHandler(AppVersionUpgradeService appVersionUpgradeService){
  this.appVersionUpgradeService=appVersionUpgradeService;
}","@Inject UpgradeHttpHandler(MetadataService metadataService){
  this.upgradeStatus=new HashMap<>();
  this.metadataService=metadataService;
}"
4548,"private List<ProgramFieldOperationInfo> processOperations(Set<ProgramRunOperations> programRunOperations){
  List<ProgramFieldOperationInfo> result=new ArrayList<>();
  for (  ProgramRunOperations entry : programRunOperations) {
    List<ProgramInfo> programInfo=computeProgramInfo(entry.getProgramRunIds());
    List<FieldOperationInfo> fieldOperationInfo=computeFieldOperationInfo(entry.getOperations());
    result.add(new ProgramFieldOperationInfo(programInfo,fieldOperationInfo));
  }
  return result;
}","private List<ProgramFieldOperationInfo> processOperations(List<ProgramRunOperations> programRunOperations){
  List<ProgramFieldOperationInfo> result=new ArrayList<>();
  for (  ProgramRunOperations entry : programRunOperations) {
    List<ProgramInfo> programInfo=computeProgramInfo(entry.getProgramRunIds());
    List<FieldOperationInfo> fieldOperationInfo=computeFieldOperationInfo(entry.getOperations());
    result.add(new ProgramFieldOperationInfo(programInfo,fieldOperationInfo));
  }
  return result;
}"
4549,"/** 
 * Get the operation details for the specified EndPointField over a given time range depending on the direction specified. Operation details in the ""incoming"" direction consists of consists of the datasets and their fields (  {@link DatasetField}) that this field originates from, as well as the programs and operations that generated this field from those origins. In outgoing direction, it consists of the datasets and their fields (  {@link DatasetField}) that were computed from this field, along with the programs and operations that performed the computation. When direction is specified as 'both', incoming as well as outgoing operations are returned.
 * @param direction the direction in which operations need to be computed
 * @param endPointField the EndPointField for which operations to be returned
 * @param start start time (inclusive) in milliseconds
 * @param end end time (exclusive) in milliseconds
 * @return the FieldLineageDetails instance
 */
FieldLineageDetails getOperationDetails(Constants.FieldLineage.Direction direction,EndPointField endPointField,long start,long end){
  List<ProgramFieldOperationInfo> incoming=null;
  List<ProgramFieldOperationInfo> outgoing=null;
  if (direction == Constants.FieldLineage.Direction.INCOMING || direction == Constants.FieldLineage.Direction.BOTH) {
    Set<ProgramRunOperations> incomingOperations=fieldLineageReader.getIncomingOperations(endPointField,start,end);
    incoming=processOperations(incomingOperations);
  }
  if (direction == Constants.FieldLineage.Direction.OUTGOING || direction == Constants.FieldLineage.Direction.BOTH) {
    Set<ProgramRunOperations> outgoingOperations=fieldLineageReader.getOutgoingOperations(endPointField,start,end);
    outgoing=processOperations(outgoingOperations);
  }
  return new FieldLineageDetails(incoming,outgoing);
}","/** 
 * Get the operation details for the specified EndPointField over a given time range depending on the direction specified. Operation details in the ""incoming"" direction consists of consists of the datasets and their fields (  {@link DatasetField}) that this field originates from, as well as the programs and operations that generated this field from those origins. In outgoing direction, it consists of the datasets and their fields (  {@link DatasetField}) that were computed from this field, along with the programs and operations that performed the computation. When direction is specified as 'both', incoming as well as outgoing operations are returned.
 * @param direction the direction in which operations need to be computed
 * @param endPointField the EndPointField for which operations to be returned
 * @param start start time (inclusive) in milliseconds
 * @param end end time (exclusive) in milliseconds
 * @return the FieldLineageDetails instance
 */
FieldLineageDetails getOperationDetails(Constants.FieldLineage.Direction direction,EndPointField endPointField,long start,long end){
  List<ProgramFieldOperationInfo> incoming=null;
  List<ProgramFieldOperationInfo> outgoing=null;
  if (direction == Constants.FieldLineage.Direction.INCOMING || direction == Constants.FieldLineage.Direction.BOTH) {
    List<ProgramRunOperations> incomingOperations=fieldLineageReader.getIncomingOperations(endPointField,start,end);
    incoming=processOperations(incomingOperations);
  }
  if (direction == Constants.FieldLineage.Direction.OUTGOING || direction == Constants.FieldLineage.Direction.BOTH) {
    List<ProgramRunOperations> outgoingOperations=fieldLineageReader.getOutgoingOperations(endPointField,start,end);
    outgoing=processOperations(outgoingOperations);
  }
  return new FieldLineageDetails(incoming,outgoing);
}"
4550,"private Constants.FieldLineage.Direction parseDirection(String directionStr) throws BadRequestException {
  try {
    return Constants.FieldLineage.Direction.valueOf(directionStr);
  }
 catch (  IllegalArgumentException e) {
    String directionValues=Joiner.on(""String_Node_Str"").join(Constants.FieldLineage.Direction.values());
    throw new BadRequestException(String.format(""String_Node_Str"" + ""String_Node_Str"",directionValues.toLowerCase()));
  }
}","private Constants.FieldLineage.Direction parseDirection(String directionStr) throws BadRequestException {
  try {
    return Constants.FieldLineage.Direction.valueOf(directionStr.toUpperCase());
  }
 catch (  IllegalArgumentException e) {
    String directionValues=Joiner.on(""String_Node_Str"").join(Constants.FieldLineage.Direction.values());
    throw new BadRequestException(String.format(""String_Node_Str"" + ""String_Node_Str"",directionValues.toLowerCase()));
  }
}"
4551,"public RuntimeMonitor(ProgramRunId programRunId,CConfiguration cConf,RuntimeMonitorClient monitorClient,DatasetFramework datasetFramework,Transactional transactional,MessagingContext messagingContext,ScheduledExecutorService scheduledExecutorService,ProfileMetricScheduledService metricScheduledService){
  super(RetryStrategies.fromConfiguration(cConf,""String_Node_Str""));
  this.programRunId=programRunId;
  this.cConf=cConf;
  this.monitorClient=monitorClient;
  this.limit=cConf.getInt(Constants.RuntimeMonitor.BATCH_LIMIT);
  this.pollTimeMillis=cConf.getLong(Constants.RuntimeMonitor.POLL_TIME_MS);
  this.gracefulShutdownMillis=cConf.getLong(Constants.RuntimeMonitor.GRACEFUL_SHUTDOWN_MS);
  this.topicsToRequest=new HashMap<>();
  this.datasetFramework=datasetFramework;
  this.messagingContext=messagingContext;
  this.transactional=transactional;
  this.scheduledExecutorService=scheduledExecutorService;
  this.programFinishTime=-1L;
  this.lastProgramStateMessages=new LinkedList<>();
  this.requestKeyToLocalTopic=createTopicConfigs(cConf);
  this.metricScheduledService=metricScheduledService;
}","public RuntimeMonitor(ProgramRunId programRunId,CConfiguration cConf,RuntimeMonitorClient monitorClient,DatasetFramework datasetFramework,Transactional transactional,MessagingContext messagingContext,ScheduledExecutorService scheduledExecutorService,RemoteExecutionLogProcessor logProcessor,ProfileMetricScheduledService metricScheduledService){
  super(RetryStrategies.fromConfiguration(cConf,""String_Node_Str""));
  this.programRunId=programRunId;
  this.cConf=cConf;
  this.monitorClient=monitorClient;
  this.limit=cConf.getInt(Constants.RuntimeMonitor.BATCH_LIMIT);
  this.pollTimeMillis=cConf.getLong(Constants.RuntimeMonitor.POLL_TIME_MS);
  this.gracefulShutdownMillis=cConf.getLong(Constants.RuntimeMonitor.GRACEFUL_SHUTDOWN_MS);
  this.topicsToRequest=new HashMap<>();
  this.datasetFramework=datasetFramework;
  this.messagingContext=messagingContext;
  this.transactional=transactional;
  this.scheduledExecutorService=scheduledExecutorService;
  this.logProcessor=logProcessor;
  this.programFinishTime=-1L;
  this.lastProgramStateMessages=new LinkedList<>();
  this.requestKeyToLocalTopic=createTopicConfigs(cConf);
  this.metricScheduledService=metricScheduledService;
}"
4552,"/** 
 * Transactionally publish the given set of messages to the local TMS.
 * @return the latest message publish time
 */
private long publish(String topicConfig,String topic,Deque<MonitorMessage> messages,AppMetadataStore store) throws Exception {
  MessagePublisher messagePublisher=messagingContext.getMessagePublisher();
  messagePublisher.publish(NamespaceId.SYSTEM.getNamespace(),topic,messages.stream().map(MonitorMessage::getMessage).iterator());
  MonitorMessage lastMessage=messages.getLast();
  store.persistSubscriberState(topicConfig,programRunId.getRun(),lastMessage.getMessageId());
  return getMessagePublishTime(lastMessage);
}","/** 
 * Transactionally publish the given set of messages to the local TMS.
 * @return the latest message publish time
 */
private long publish(String topicConfig,String topic,Deque<MonitorMessage> messages,AppMetadataStore store) throws Exception {
  if (topic.startsWith(cConf.get(Constants.Logging.TMS_TOPIC_PREFIX))) {
    logProcessor.process(messages.stream().map(MonitorMessage::getMessage).iterator());
  }
 else {
    MessagePublisher messagePublisher=messagingContext.getMessagePublisher();
    messagePublisher.publish(NamespaceId.SYSTEM.getNamespace(),topic,messages.stream().map(MonitorMessage::getMessage).iterator());
  }
  MonitorMessage lastMessage=messages.getLast();
  store.persistSubscriberState(topicConfig,programRunId.getRun(),lastMessage.getMessageId());
  return getMessagePublishTime(lastMessage);
}"
4553,"@Override public Map<String,String> getSystemTagsAsString(){
  return Maps.transformValues(getSystemTagsMap(),SYSTEM_TAG_TO_STRING);
}","@Override public Map<String,String> getSystemTagsAsString(){
  return Maps.transformValues(getSystemTagsMap(),SystemTag::getValue);
}"
4554,"/** 
 * Create a cluster. This will return after the initial request to create the cluster is completed. At this point, the cluster is likely not yet running, but in a provisioning state.
 * @param name the name of the cluster to create
 * @return the id of the created EMR cluster
 */
public String createCluster(String name){
  AmazonEC2 ec2=AmazonEC2ClientBuilder.standard().withCredentials(emrConf.getCredentialsProvider()).withRegion(emrConf.getRegion()).build();
  ec2.importKeyPair(new ImportKeyPairRequest(name,emrConf.getPublicKey().getKey()));
  RunJobFlowRequest request=new RunJobFlowRequest().withName(name).withApplications(new Application().withName(""String_Node_Str"")).withReleaseLabel(""String_Node_Str"").withServiceRole(emrConf.getServiceRole()).withJobFlowRole(emrConf.getJobFlowRole()).withInstances(new JobFlowInstancesConfig().withEc2KeyName(name).withAdditionalMasterSecurityGroups(emrConf.getAdditionalMasterSecurityGroup()).withInstanceCount(emrConf.getInstanceCount()).withEc2SubnetId(emrConf.getEc2SubnetId()).withKeepJobFlowAliveWhenNoSteps(true).withMasterInstanceType(emrConf.getMasterInstanceType()).withSlaveInstanceType(emrConf.getWorkerInstanceType()));
  if (emrConf.getLogURI() != null) {
    request.withLogUri(emrConf.getLogURI());
  }
  LOG.info(""String_Node_Str"",name);
  return client.runJobFlow(request).getJobFlowId();
}","/** 
 * Create a cluster. This will return after the initial request to create the cluster is completed. At this point, the cluster is likely not yet running, but in a provisioning state.
 * @param name the name of the cluster to create
 * @return the id of the created EMR cluster
 */
public String createCluster(String name){
  AmazonEC2 ec2=AmazonEC2ClientBuilder.standard().withCredentials(emrConf.getCredentialsProvider()).withRegion(emrConf.getRegion()).build();
  ec2.importKeyPair(new ImportKeyPairRequest(name,emrConf.getPublicKey().getKey()));
  RunJobFlowRequest request=new RunJobFlowRequest().withName(name).withApplications(new Application().withName(""String_Node_Str"")).withConfigurations(new Configuration().withClassification(""String_Node_Str"").withProperties(Collections.singletonMap(""String_Node_Str"",""String_Node_Str""))).withReleaseLabel(""String_Node_Str"").withServiceRole(emrConf.getServiceRole()).withJobFlowRole(emrConf.getJobFlowRole()).withInstances(new JobFlowInstancesConfig().withEc2KeyName(name).withAdditionalMasterSecurityGroups(emrConf.getAdditionalMasterSecurityGroup()).withInstanceCount(emrConf.getInstanceCount()).withEc2SubnetId(emrConf.getEc2SubnetId()).withKeepJobFlowAliveWhenNoSteps(true).withMasterInstanceType(emrConf.getMasterInstanceType()).withSlaveInstanceType(emrConf.getWorkerInstanceType()));
  if (emrConf.getLogURI() != null) {
    request.withLogUri(emrConf.getLogURI());
  }
  LOG.info(""String_Node_Str"",name);
  return client.runJobFlow(request).getJobFlowId();
}"
4555,"@GET @Path(""String_Node_Str"") public void readDashboardDetail(FullHttpRequest request,HttpResponder responder,@QueryParam(""String_Node_Str"") long startTimeSecs,@QueryParam(""String_Node_Str"") int durationTimeSecs,@QueryParam(""String_Node_Str"") Set<String> namespaces) throws Exception {
  if (startTimeSecs < 0) {
    throw new BadRequestException(""String_Node_Str"");
  }
  if (durationTimeSecs < 0) {
    throw new BadRequestException(""String_Node_Str"");
  }
  if (namespaces.isEmpty()) {
    throw new BadRequestException(""String_Node_Str"");
  }
  long endTimeSecs=startTimeSecs + durationTimeSecs;
  Collection<RunRecordMeta> runRecordMetas=programHeartbeatService.scan(startTimeSecs,endTimeSecs + 1,namespaces);
  List<DashboardProgramRunRecord> result=runRecordMetas.stream().map(OperationsDashboardHttpHandler::runRecordToDashboardRecord).collect(Collectors.toList());
  Set<NamespaceId> namespaceIds=namespaces.stream().map(NamespaceId::new).collect(Collectors.toSet());
  long currentTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
  long scheduleStartTimeSeconds=startTimeSecs > currentTimeInSeconds ? startTimeSecs : currentTimeInSeconds;
  if (endTimeSecs > currentTimeInSeconds) {
    result.addAll(getAllScheduledRuns(namespaceIds,scheduleStartTimeSeconds,endTimeSecs + 1));
  }
  responder.sendJson(HttpResponseStatus.OK,GSON.toJson(result));
}","@GET @Path(""String_Node_Str"") public void readDashboardDetail(FullHttpRequest request,HttpResponder responder,@QueryParam(""String_Node_Str"") long startTimeSecs,@QueryParam(""String_Node_Str"") int durationTimeSecs,@QueryParam(""String_Node_Str"") Set<String> namespaces) throws Exception {
  if (startTimeSecs < 0) {
    throw new BadRequestException(""String_Node_Str"");
  }
  if (durationTimeSecs < 0) {
    throw new BadRequestException(""String_Node_Str"");
  }
  if (namespaces.isEmpty()) {
    throw new BadRequestException(""String_Node_Str"");
  }
  long endTimeSecs=startTimeSecs + durationTimeSecs;
  Collection<RunRecordMeta> runRecordMetas=programHeartbeatService.scan(startTimeSecs,endTimeSecs + 1,namespaces);
  List<DashboardProgramRunRecord> result=new ArrayList<>();
  for (  RunRecordMeta runRecordMeta : runRecordMetas) {
    result.add(OperationsDashboardHttpHandler.runRecordToDashboardRecord(runRecordMeta));
  }
  Set<NamespaceId> namespaceIds=namespaces.stream().map(NamespaceId::new).collect(Collectors.toSet());
  long currentTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
  long scheduleStartTimeSeconds=startTimeSecs > currentTimeInSeconds ? startTimeSecs : currentTimeInSeconds;
  if (endTimeSecs > currentTimeInSeconds) {
    result.addAll(getAllScheduledRuns(namespaceIds,scheduleStartTimeSeconds,endTimeSecs + 1));
  }
  responder.sendJson(HttpResponseStatus.OK,GSON.toJson(result));
}"
4556,"/** 
 * Converts a   {@link RunRecordMeta} to a {@link DashboardProgramRunRecord}
 */
@VisibleForTesting static DashboardProgramRunRecord runRecordToDashboardRecord(RunRecordMeta meta){
  ProgramRunId runId=meta.getProgramRunId();
  String startMethod=MANUAL;
  String scheduleInfoJson=meta.getSystemArgs().get(ProgramOptionConstants.TRIGGERING_SCHEDULE_INFO);
  if (scheduleInfoJson != null) {
    TriggeringScheduleInfo scheduleInfo=GSON.fromJson(scheduleInfoJson,TriggeringScheduleInfo.class);
    startMethod=scheduleInfo.getTriggerInfos().stream().findFirst().map(trigger -> TriggerInfo.Type.TIME.equals(trigger.getType()) ? SCHEDULED : TRIGGERED).orElse(MANUAL);
  }
  return new DashboardProgramRunRecord(runId,meta,meta.getArtifactId(),meta.getPrincipal(),startMethod);
}","/** 
 * Converts a   {@link RunRecordMeta} to a {@link DashboardProgramRunRecord}
 */
@VisibleForTesting static DashboardProgramRunRecord runRecordToDashboardRecord(RunRecordMeta meta) throws IOException {
  ProgramRunId runId=meta.getProgramRunId();
  String startMethod=MANUAL;
  String scheduleInfoJson=meta.getSystemArgs().get(ProgramOptionConstants.TRIGGERING_SCHEDULE_INFO);
  if (scheduleInfoJson != null) {
    TriggeringScheduleInfo scheduleInfo=GSON.fromJson(scheduleInfoJson,TriggeringScheduleInfo.class);
    startMethod=scheduleInfo.getTriggerInfos().stream().findFirst().map(trigger -> TriggerInfo.Type.TIME.equals(trigger.getType()) ? SCHEDULED : TRIGGERED).orElse(MANUAL);
  }
  String user=meta.getPrincipal();
  if (user != null) {
    user=new KerberosName(user).getShortName();
  }
  return new DashboardProgramRunRecord(runId,meta,meta.getArtifactId(),user,startMethod);
}"
4557,"@Override protected void handleStateSaveFailure(ProvisioningTaskInfo taskInfo,TransactionFailureException e){
  if (taskInfo.getProvisioningOp().getStatus() == ProvisioningOp.Status.REQUESTING_CREATE) {
    provisionerNotifier.deprovisioned(programRunId);
  }
 else {
    provisionerNotifier.deprovisioning(programRunId);
  }
}","@Override protected void handleStateSaveFailure(ProvisioningTaskInfo taskInfo,TransactionFailureException e){
  provisionerNotifier.deprovisioning(programRunId);
}"
4558,"private ProvisioningSubtask createPollingDeleteSubtask(){
  long taskStartTime=System.currentTimeMillis();
  return new ClusterPollSubtask(provisioner,provisionerContext,ClusterStatus.DELETING,cluster -> {
switch (cluster.getStatus()) {
case CREATING:
      return Optional.of(ProvisioningOp.Status.POLLING_CREATE);
case RUNNING:
    return Optional.of(ProvisioningOp.Status.INITIALIZING);
case NOT_EXISTS:
  if (TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis() - taskStartTime) > retryTimeLimitSecs) {
    provisionerNotifier.deprovisioned(programRunId);
    return Optional.of(ProvisioningOp.Status.FAILED);
  }
 else {
    return Optional.of(ProvisioningOp.Status.REQUESTING_CREATE);
  }
case FAILED:
return Optional.of(ProvisioningOp.Status.REQUESTING_DELETE);
case DELETING:
case ORPHANED:
provisionerNotifier.deprovisioning(programRunId);
return Optional.of(ProvisioningOp.Status.FAILED);
}
throw new IllegalStateException(String.format(""String_Node_Str"",cluster.getStatus()));
}
);
}","private ProvisioningSubtask createPollingDeleteSubtask(){
  long taskStartTime=System.currentTimeMillis();
  return new ClusterPollSubtask(provisioner,provisionerContext,ClusterStatus.DELETING,cluster -> {
switch (cluster.getStatus()) {
case CREATING:
      return Optional.of(ProvisioningOp.Status.POLLING_CREATE);
case RUNNING:
    return Optional.of(ProvisioningOp.Status.INITIALIZING);
case NOT_EXISTS:
  if (TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis() - taskStartTime) > retryTimeLimitSecs) {
    provisionerNotifier.deprovisioning(programRunId);
    return Optional.of(ProvisioningOp.Status.FAILED);
  }
 else {
    return Optional.of(ProvisioningOp.Status.REQUESTING_CREATE);
  }
case FAILED:
return Optional.of(ProvisioningOp.Status.REQUESTING_DELETE);
case DELETING:
case ORPHANED:
provisionerNotifier.deprovisioning(programRunId);
return Optional.of(ProvisioningOp.Status.FAILED);
}
throw new IllegalStateException(String.format(""String_Node_Str"",cluster.getStatus()));
}
);
}"
4559,"@Inject protected AbstractNotificationSubscriberService(String name,CConfiguration cConf,String topicName,boolean transactionalFetch,int fetchSize,long emptyFetchDelayMillis,MessagingService messagingService,DatasetFramework datasetFramework,TransactionSystemClient txClient,MetricsCollectionService metricsCollectionService){
  super(NamespaceId.SYSTEM.topic(topicName),transactionalFetch,fetchSize,emptyFetchDelayMillis,RetryStrategies.fromConfiguration(cConf,""String_Node_Str""),metricsCollectionService.getContext(ImmutableMap.of(Constants.Metrics.Tag.COMPONENT,Constants.Service.MASTER_SERVICES,Constants.Metrics.Tag.INSTANCE_ID,""String_Node_Str"",Constants.Metrics.Tag.NAMESPACE,NamespaceId.SYSTEM.getNamespace(),Constants.Metrics.Tag.TOPIC,topicName,Constants.Metrics.Tag.CONSUMER,name)));
  this.name=name;
  this.messagingContext=new MultiThreadMessagingContext(messagingService);
  this.transactional=Transactions.createTransactionalWithRetry(Transactions.createTransactional(new MultiThreadDatasetCache(new SystemDatasetInstantiator(datasetFramework),new TransactionSystemClientAdapter(txClient),NamespaceId.SYSTEM,ImmutableMap.of(),null,null,messagingContext)),org.apache.tephra.RetryStrategies.retryOnConflict(20,100));
}","@Inject protected AbstractNotificationSubscriberService(String name,CConfiguration cConf,String topicName,boolean transactionalFetch,int fetchSize,long emptyFetchDelayMillis,MessagingService messagingService,DatasetFramework datasetFramework,TransactionSystemClient txClient,MetricsCollectionService metricsCollectionService){
  super(NamespaceId.SYSTEM.topic(topicName),transactionalFetch,fetchSize,cConf.getInt(TxConstants.Manager.CFG_TX_TIMEOUT),emptyFetchDelayMillis,RetryStrategies.fromConfiguration(cConf,""String_Node_Str""),metricsCollectionService.getContext(ImmutableMap.of(Constants.Metrics.Tag.COMPONENT,Constants.Service.MASTER_SERVICES,Constants.Metrics.Tag.INSTANCE_ID,""String_Node_Str"",Constants.Metrics.Tag.NAMESPACE,NamespaceId.SYSTEM.getNamespace(),Constants.Metrics.Tag.TOPIC,topicName,Constants.Metrics.Tag.CONSUMER,name)));
  this.name=name;
  this.messagingContext=new MultiThreadMessagingContext(messagingService);
  this.transactional=Transactions.createTransactionalWithRetry(Transactions.createTransactional(new MultiThreadDatasetCache(new SystemDatasetInstantiator(datasetFramework),new TransactionSystemClientAdapter(txClient),NamespaceId.SYSTEM,ImmutableMap.of(),null,null,messagingContext)),org.apache.tephra.RetryStrategies.retryOnConflict(20,100));
}"
4560,"@Override protected void processMessages(DatasetContext datasetContext,Iterator<ImmutablePair<String,MetadataMessage>> messages){
  Map<MetadataMessage.Type,MetadataMessageProcessor> processors=new HashMap<>();
  while (messages.hasNext()) {
    MetadataMessage message=messages.next().getSecond();
    MetadataMessageProcessor processor=processors.computeIfAbsent(message.getType(),type -> {
switch (type) {
case LINEAGE:
        return new DataAccessLineageProcessor(datasetContext);
case FIELD_LINEAGE:
      return new FieldLineageProcessor(datasetContext);
case USAGE:
    return new UsageProcessor(datasetContext);
case WORKFLOW_TOKEN:
case WORKFLOW_STATE:
  return new WorkflowProcessor(datasetContext);
case METADATA_OPERATION:
return new MetadataOperationProcessor();
case DATASET_OPERATION:
return new DatasetOperationMessageProcessor(datasetFramework);
case PROFILE_ASSIGNMENT:
case PROFILE_UNASSIGNMENT:
case ENTITY_CREATION:
case ENTITY_DELETION:
return new ProfileMetadataMessageProcessor(cConf,datasetContext,datasetFramework);
default :
return null;
}
}
);
if (processor == null) {
LOG.warn(""String_Node_Str"",message.getType());
continue;
}
processor.processMessage(message);
if (processor.isTimeConsumingMessage(message)) {
return;
}
}
}","@Override protected void processMessages(DatasetContext datasetContext,Iterator<ImmutablePair<String,MetadataMessage>> messages){
  Map<MetadataMessage.Type,MetadataMessageProcessor> processors=new HashMap<>();
  while (messages.hasNext()) {
    MetadataMessage message=messages.next().getSecond();
    MetadataMessageProcessor processor=processors.computeIfAbsent(message.getType(),type -> {
switch (type) {
case LINEAGE:
        return new DataAccessLineageProcessor(datasetContext);
case FIELD_LINEAGE:
      return new FieldLineageProcessor(datasetContext);
case USAGE:
    return new UsageProcessor(datasetContext);
case WORKFLOW_TOKEN:
case WORKFLOW_STATE:
  return new WorkflowProcessor(datasetContext);
case METADATA_OPERATION:
return new MetadataOperationProcessor();
case DATASET_OPERATION:
return new DatasetOperationMessageProcessor(datasetFramework);
case PROFILE_ASSIGNMENT:
case PROFILE_UNASSIGNMENT:
case ENTITY_CREATION:
case ENTITY_DELETION:
return new ProfileMetadataMessageProcessor(cConf,datasetContext,datasetFramework);
default :
return null;
}
}
);
if (processor == null) {
LOG.warn(""String_Node_Str"",message.getType());
continue;
}
processor.processMessage(message);
}
}"
4561,"@Inject MetadataSubscriberService(CConfiguration cConf,MessagingService messagingService,DatasetFramework datasetFramework,TransactionSystemClient txClient,MetricsCollectionService metricsCollectionService,MetadataAdmin metadataAdmin){
  super(NamespaceId.SYSTEM.topic(cConf.get(Constants.Metadata.MESSAGING_TOPIC)),true,cConf.getInt(Constants.Metadata.MESSAGING_FETCH_SIZE),cConf.getLong(Constants.Metadata.MESSAGING_POLL_DELAY_MILLIS),RetryStrategies.fromConfiguration(cConf,""String_Node_Str""),metricsCollectionService.getContext(ImmutableMap.of(Constants.Metrics.Tag.COMPONENT,Constants.Service.MASTER_SERVICES,Constants.Metrics.Tag.INSTANCE_ID,""String_Node_Str"",Constants.Metrics.Tag.NAMESPACE,NamespaceId.SYSTEM.getNamespace(),Constants.Metrics.Tag.TOPIC,cConf.get(Constants.Metadata.MESSAGING_TOPIC),Constants.Metrics.Tag.CONSUMER,""String_Node_Str"")));
  this.cConf=cConf;
  this.messagingContext=new MultiThreadMessagingContext(messagingService);
  this.datasetFramework=datasetFramework;
  this.metadataAdmin=metadataAdmin;
  this.transactional=Transactions.createTransactionalWithRetry(Transactions.createTransactional(new MultiThreadDatasetCache(new SystemDatasetInstantiator(datasetFramework),new TransactionSystemClientAdapter(txClient),NamespaceId.SYSTEM,Collections.emptyMap(),null,null,messagingContext)),org.apache.tephra.RetryStrategies.retryOnConflict(20,100));
}","@Inject MetadataSubscriberService(CConfiguration cConf,MessagingService messagingService,DatasetFramework datasetFramework,TransactionSystemClient txClient,MetricsCollectionService metricsCollectionService,MetadataAdmin metadataAdmin){
  super(NamespaceId.SYSTEM.topic(cConf.get(Constants.Metadata.MESSAGING_TOPIC)),true,cConf.getInt(Constants.Metadata.MESSAGING_FETCH_SIZE),cConf.getInt(TxConstants.Manager.CFG_TX_TIMEOUT),cConf.getLong(Constants.Metadata.MESSAGING_POLL_DELAY_MILLIS),RetryStrategies.fromConfiguration(cConf,""String_Node_Str""),metricsCollectionService.getContext(ImmutableMap.of(Constants.Metrics.Tag.COMPONENT,Constants.Service.MASTER_SERVICES,Constants.Metrics.Tag.INSTANCE_ID,""String_Node_Str"",Constants.Metrics.Tag.NAMESPACE,NamespaceId.SYSTEM.getNamespace(),Constants.Metrics.Tag.TOPIC,cConf.get(Constants.Metadata.MESSAGING_TOPIC),Constants.Metrics.Tag.CONSUMER,""String_Node_Str"")));
  this.cConf=cConf;
  this.messagingContext=new MultiThreadMessagingContext(messagingService);
  this.datasetFramework=datasetFramework;
  this.metadataAdmin=metadataAdmin;
  this.transactional=Transactions.createTransactionalWithRetry(Transactions.createTransactional(new MultiThreadDatasetCache(new SystemDatasetInstantiator(datasetFramework),new TransactionSystemClientAdapter(txClient),NamespaceId.SYSTEM,Collections.emptyMap(),null,null,messagingContext)),org.apache.tephra.RetryStrategies.retryOnConflict(20,100));
}"
4562,"@Override public void prepareRun(BatchSinkContext context) throws Exception {
  Map<String,String> arguments=new HashMap<>();
  PartitionKey outputPartition=PartitionKey.builder().addStringField(""String_Node_Str"",phaseName).build();
  PartitionedFileSetArguments.setOutputPartitionKey(arguments,outputPartition);
  context.addOutput(Output.ofDataset(datasetName,arguments));
}","@Override public void prepareRun(BatchSinkContext context){
  Map<String,String> arguments=new HashMap<>();
  FileSetArguments.setOutputPath(arguments,Constants.Connector.DATA_DIR + ""String_Node_Str"" + phaseName);
  context.addOutput(Output.ofDataset(datasetName,arguments));
}"
4563,"@Override public void prepareRun(BatchSourceContext context) throws Exception {
  Map<String,String> arguments=new HashMap<>();
  PartitionedFileSet inputFileset=context.getDataset(datasetName);
  for (  PartitionDetail partitionDetail : inputFileset.getPartitions(PartitionFilter.ALWAYS_MATCH)) {
    PartitionedFileSetArguments.addInputPartition(arguments,partitionDetail);
  }
  context.setInput(Input.ofDataset(datasetName,arguments));
}","@Override public void prepareRun(BatchSourceContext context){
  Map<String,String> arguments=new HashMap<>();
  FileSetArguments.setInputPath(arguments,Constants.Connector.DATA_DIR);
  context.setInput(Input.ofDataset(datasetName,arguments));
}"
4564,"public void configure(WorkflowConfigurer workflowConfigurer){
  Partitioning partitioning=Partitioning.builder().addField(""String_Node_Str"",Partitioning.FieldType.STRING).build();
  workflowConfigurer.createLocalDataset(datasetName,PartitionedFileSet.class,PartitionedFileSetProperties.builder().setPartitioning(partitioning).setInputFormat(CombineTextInputFormat.class).setOutputFormat(TextOutputFormat.class).build());
}","public void configure(WorkflowConfigurer workflowConfigurer){
  workflowConfigurer.createLocalDataset(datasetName,FileSet.class,FileSetProperties.builder().setInputFormat(CombineTextInputFormat.class).setInputProperty(FileInputFormat.INPUT_DIR_RECURSIVE,""String_Node_Str"").setOutputFormat(TextOutputFormat.class).build());
}"
4565,"@Test public void testStartProgramWithDisabledRuntimeArgs() throws Exception {
  ProfileId profileId=new NamespaceId(TEST_NAMESPACE1).profile(""String_Node_Str"");
  Profile profile=new Profile(""String_Node_Str"",Profile.NATIVE.getLabel(),Profile.NATIVE.getDescription(),Profile.NATIVE.getScope(),Profile.NATIVE.getProvisioner());
  putProfile(profileId,profile,200);
  disableProfile(profileId,200);
  deploy(AppWithWorkflow.class,200,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  ProgramId programId=new NamespaceId(TEST_NAMESPACE1).app(APP_WITH_WORKFLOW_APP_ID).workflow(APP_WITH_WORKFLOW_WORKFLOW_NAME);
  Assert.assertEquals(STOPPED,getProgramStatus(programId));
  ImmutableMap<String,String> args=ImmutableMap.of(SystemArguments.PROFILE_NAME,ProfileId.NATIVE.getScopedName());
  startProgram(programId,Collections.singletonMap(SystemArguments.PROFILE_NAME,profileId.getScopedName()),409);
  Assert.assertEquals(STOPPED,getProgramStatus(programId));
  startProgram(programId,Collections.singletonMap(SystemArguments.PROFILE_NAME,ProfileId.NATIVE.getScopedName()),200);
  waitState(programId,STOPPED);
}","@Test public void testStartProgramWithDisabledRuntimeArgs() throws Exception {
  ProfileId profileId=new NamespaceId(TEST_NAMESPACE1).profile(""String_Node_Str"");
  Profile profile=new Profile(""String_Node_Str"",Profile.NATIVE.getLabel(),Profile.NATIVE.getDescription(),Profile.NATIVE.getScope(),Profile.NATIVE.getProvisioner());
  putProfile(profileId,profile,200);
  disableProfile(profileId,200);
  deploy(AppWithWorkflow.class,200,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  ProgramId programId=new NamespaceId(TEST_NAMESPACE1).app(APP_WITH_WORKFLOW_APP_ID).workflow(APP_WITH_WORKFLOW_WORKFLOW_NAME);
  Assert.assertEquals(STOPPED,getProgramStatus(programId));
  startProgram(programId,Collections.singletonMap(SystemArguments.PROFILE_NAME,profileId.getScopedName()),409);
  Assert.assertEquals(STOPPED,getProgramStatus(programId));
  startProgram(programId,Collections.singletonMap(SystemArguments.PROFILE_NAME,ProfileId.NATIVE.getScopedName()),200);
  waitState(programId,STOPPED);
}"
4566,"@Override public ByteBuf nextChunk() throws Exception {
  ByteBuf startBuffer=Unpooled.EMPTY_BUFFER;
  if (!hasStarted) {
    hasStarted=true;
    startBuffer=Unpooled.copiedBuffer(onWriteStart());
  }
  if (logEventIter.hasNext()) {
    ByteBuf eventsBuffer=writeLogEvents(logEventIter);
    return startBuffer.isReadable() ? Unpooled.wrappedBuffer(startBuffer,eventsBuffer) : eventsBuffer;
  }
  if (!hasFinished) {
    hasFinished=true;
    return onWriteFinish();
  }
  return Unpooled.EMPTY_BUFFER;
}","@Override public ByteBuf nextChunk() throws Exception {
  ByteBuf startBuffer=Unpooled.EMPTY_BUFFER;
  if (!hasStarted) {
    hasStarted=true;
    startBuffer=Unpooled.copiedBuffer(onWriteStart());
  }
  if (logEventIter.hasNext()) {
    ByteBuf eventsBuffer=writeLogEvents(logEventIter);
    return startBuffer.isReadable() ? Unpooled.wrappedBuffer(startBuffer,eventsBuffer) : eventsBuffer;
  }
  if (!hasFinished) {
    hasFinished=true;
    return Unpooled.wrappedBuffer(startBuffer,onWriteFinish());
  }
  return Unpooled.EMPTY_BUFFER;
}"
4567,"/** 
 * Sets up a   {@link DatasetFramework} instance for standalone usage.  NOTE: should NOT be used by applications!!!Note: includeNewDatasets boolean is required because upgrade tool has two mode: 1. Normal CDAP upgrade and 2. Upgrading co processor for tables after hbase upgrade. This parameter specifies whether new system dataset which were added in the current release needs to be added in the dataset framework or not. During Normal CDAP upgrade (1) we don't need these datasets to be added in the ds framework as they will get created during upgrade rather than when cdap starts after upgrade which is what we want. Whereas during Hbase upgrade (2) we want these new tables to be added so that the co processor of these tables can be upgraded when the user runs CDAP's Hbase Upgrade after upgrading to a newer version of Hbase.
 */
private void initializeDSFramework(CConfiguration cConf,DatasetFramework datasetFramework,boolean includeNewDatasets) throws IOException, DatasetManagementException {
  DatasetMetaTableUtil.setupDatasets(datasetFramework);
  ArtifactStore.setupDatasets(datasetFramework);
  if (includeNewDatasets) {
    DefaultOwnerStore.setupDatasets(datasetFramework);
  }
  DefaultMetadataStore.setupDatasets(datasetFramework);
  LineageDataset.setupDatasets(datasetFramework);
  DefaultStore.setupDatasets(datasetFramework);
  DefaultConfigStore.setupDatasets(datasetFramework);
  LoggingStoreTableUtil.setupDatasets(datasetFramework);
  ScheduleStoreTableUtil.setupDatasets(datasetFramework);
  DefaultMetricDatasetFactory factory=new DefaultMetricDatasetFactory(cConf,datasetFramework);
  DefaultMetricDatasetFactory.setupDatasets(factory);
  UsageDataset.setupDatasets(datasetFramework);
}","/** 
 * Sets up a   {@link DatasetFramework} instance for standalone usage.  NOTE: should NOT be used by applications!!!Note: includeNewDatasets boolean is required because upgrade tool has two mode: 1. Normal CDAP upgrade and 2. Upgrading co processor for tables after hbase upgrade. This parameter specifies whether new system dataset which were added in the current release needs to be added in the dataset framework or not. During Normal CDAP upgrade (1) we don't need these datasets to be added in the ds framework as they will get created during upgrade rather than when cdap starts after upgrade which is what we want. Whereas during Hbase upgrade (2) we want these new tables to be added so that the co processor of these tables can be upgraded when the user runs CDAP's Hbase Upgrade after upgrading to a newer version of Hbase.
 */
private void initializeDSFramework(CConfiguration cConf,DatasetFramework datasetFramework,boolean includeNewDatasets) throws IOException, DatasetManagementException {
  DatasetMetaTableUtil.setupDatasets(datasetFramework);
  ArtifactStore.setupDatasets(datasetFramework);
  if (includeNewDatasets) {
  }
  DefaultOwnerStore.setupDatasets(datasetFramework);
  DefaultMetadataStore.setupDatasets(datasetFramework);
  LineageDataset.setupDatasets(datasetFramework);
  DefaultStore.setupDatasets(datasetFramework);
  DefaultConfigStore.setupDatasets(datasetFramework);
  LoggingStoreTableUtil.setupDatasets(datasetFramework);
  ScheduleStoreTableUtil.setupDatasets(datasetFramework);
  DefaultMetricDatasetFactory factory=new DefaultMetricDatasetFactory(cConf,datasetFramework);
  DefaultMetricDatasetFactory.setupDatasets(factory);
  UsageDataset.setupDatasets(datasetFramework);
}"
4568,"public MetadataServiceModule(TwillContext twillContext){
  this.instanceId=twillContext.getInstanceId();
}","public MetadataServiceModule(){
  this.instanceId=0;
}"
4569,"@Test public void testPutAndDeleteProfiles() throws Exception {
  Profile invalidProfile=new Profile(""String_Node_Str"",""String_Node_Str"",new ProvisionerInfo(""String_Node_Str"",PROPERTY_SUMMARIES));
  putProfile(NamespaceId.DEFAULT,invalidProfile.getName(),invalidProfile,400);
  Profile expected=new Profile(""String_Node_Str"",""String_Node_Str"",new ProvisionerInfo(MockProvisioner.NAME,PROPERTY_SUMMARIES));
  putProfile(NamespaceId.DEFAULT,expected.getName(),expected,200);
  Profile actual=getProfile(NamespaceId.DEFAULT,expected.getName(),200);
  Assert.assertEquals(expected,actual);
  List<Profile> profiles=listProfiles(NamespaceId.DEFAULT,true,200);
  Set<Profile> expectedList=ImmutableSet.of(Profile.DEFAULT,expected);
  Assert.assertEquals(expectedList.size(),profiles.size());
  Assert.assertEquals(expectedList,new HashSet<>(profiles));
  putProfile(NamespaceId.DEFAULT,expected.getName(),expected,409);
  deleteProfile(NamespaceId.DEFAULT,""String_Node_Str"",404);
  deleteProfile(NamespaceId.DEFAULT,expected.getName(),200);
  Assert.assertEquals(Collections.emptyList(),listProfiles(NamespaceId.DEFAULT,false,200));
}","@Test public void testPutAndDeleteProfiles() throws Exception {
  Profile invalidProfile=new Profile(""String_Node_Str"",""String_Node_Str"",new ProvisionerInfo(""String_Node_Str"",PROPERTY_SUMMARIES));
  putProfile(NamespaceId.DEFAULT,invalidProfile.getName(),invalidProfile,400);
  Profile expected=new Profile(""String_Node_Str"",""String_Node_Str"",new ProvisionerInfo(MockProvisioner.NAME,PROPERTY_SUMMARIES));
  putProfile(NamespaceId.DEFAULT,expected.getName(),expected,200);
  Profile actual=getProfile(NamespaceId.DEFAULT,expected.getName(),200);
  Assert.assertEquals(expected,actual);
  List<Profile> profiles=listProfiles(NamespaceId.DEFAULT,true,200);
  Set<Profile> expectedList=ImmutableSet.of(Profile.DEFAULT,expected);
  Assert.assertEquals(expectedList.size(),profiles.size());
  Assert.assertEquals(expectedList,new HashSet<>(profiles));
  putProfile(NamespaceId.DEFAULT,expected.getName(),expected,409);
  deleteProfile(NamespaceId.DEFAULT,""String_Node_Str"",404);
  deleteProfile(NamespaceId.DEFAULT,expected.getName(),200);
  Assert.assertEquals(Collections.emptyList(),listProfiles(NamespaceId.DEFAULT,false,200));
  ProvisionerSpecification spec=new MockProvisioner().getSpec();
  ProvisionerDetail test=new ProvisionerDetail(spec.getName(),spec.getDescription(),new ArrayList<>());
  putProfile(NamespaceId.DEFAULT,test.getName(),test,400);
}"
4570,"private void putProfile(NamespaceId namespace,String profileName,Profile profile,int expectedCode) throws Exception {
  HttpResponse response=doPut(String.format(""String_Node_Str"",namespace.getNamespace(),profileName),GSON.toJson(profile));
  Assert.assertEquals(expectedCode,response.getStatusLine().getStatusCode());
}","private void putProfile(NamespaceId namespace,String profileName,Object profile,int expectedCode) throws Exception {
  HttpResponse response=doPut(String.format(""String_Node_Str"",namespace.getNamespace(),profileName),GSON.toJson(profile));
  Assert.assertEquals(expectedCode,response.getStatusLine().getStatusCode());
}"
4571,"@VisibleForTesting static String getClusterName(ProgramRun programRun){
  String cleanedAppName=programRun.getApplication().replaceAll(""String_Node_Str"",""String_Node_Str"").toLowerCase();
  int maxAppLength=53 - programRun.getRun().length();
  if (cleanedAppName.length() > maxAppLength) {
    cleanedAppName=cleanedAppName.substring(0,maxAppLength);
  }
  return cleanedAppName + ""String_Node_Str"" + programRun.getRun();
}","@VisibleForTesting static String getClusterName(ProgramRun programRun){
  String cleanedAppName=programRun.getApplication().replaceAll(""String_Node_Str"",""String_Node_Str"").toLowerCase();
  int maxAppLength=51 - 5 - 1- programRun.getRun().length();
  if (cleanedAppName.length() > maxAppLength) {
    cleanedAppName=cleanedAppName.substring(0,maxAppLength);
  }
  return ""String_Node_Str"" + cleanedAppName + ""String_Node_Str""+ programRun.getRun();
}"
4572,"/** 
 * Record that a cluster will be provisioned for a program run, returning a Runnable that will actually perform the cluster provisioning. This method must be run within a transaction. The task returned should be run using the   {@link #execute(ProvisioningTask)} method, and should only be executedafter the transaction that ran this method has completed.
 * @param provisionRequest the provision request
 * @param datasetContext dataset context for the transaction
 * @return runnable that will actually execute the cluster provisioning
 */
public ProvisioningTask provision(ProvisionRequest provisionRequest,DatasetContext datasetContext){
  ProgramRunId programRunId=provisionRequest.getProgramRunId();
  ProgramOptions programOptions=provisionRequest.getProgramOptions();
  Map<String,String> args=programOptions.getArguments().asMap();
  String name=SystemArguments.getProfileProvisioner(args);
  Provisioner provisioner=provisionerInfo.get().provisioners.get(name);
  if (provisioner == null) {
    LOG.error(""String_Node_Str"",programRunId,name);
    provisionerNotifier.deprovisioned(programRunId);
    return new NoOpProvisioningTask(programRunId);
  }
  SSHKeyInfo sshKeyInfo=null;
  if (!YarnProvisioner.SPEC.equals(provisioner.getSpec())) {
    try {
      sshKeyInfo=generateSSHKey(programRunId);
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",programRunId,name,e);
      provisionerNotifier.deprovisioning(programRunId);
      return new NoOpProvisioningTask(programRunId);
    }
  }
  Map<String,String> properties=SystemArguments.getProfileProperties(args);
  ProvisionerContext context=new DefaultProvisionerContext(programRunId,properties,createSSHContext(sshKeyInfo));
  ClusterOp clusterOp=new ClusterOp(ClusterOp.Type.PROVISION,ClusterOp.Status.REQUESTING_CREATE);
  ClusterInfo clusterInfo=new ClusterInfo(programRunId,provisionRequest.getProgramDescriptor(),properties,name,provisionRequest.getUser(),clusterOp,null,null);
  ProvisionerDataset provisionerDataset=ProvisionerDataset.get(datasetContext,datasetFramework);
  provisionerDataset.putClusterInfo(clusterInfo);
  return new ProvisionTask(provisionRequest,provisioner,context,provisionerNotifier,transactional,datasetFramework,sshKeyInfo);
}","/** 
 * Record that a cluster will be provisioned for a program run, returning a Runnable that will actually perform the cluster provisioning. This method must be run within a transaction. The task returned should be run using the   {@link #execute(ProvisioningTask)} method, and should only be executedafter the transaction that ran this method has completed.
 * @param provisionRequest the provision request
 * @param datasetContext dataset context for the transaction
 * @return runnable that will actually execute the cluster provisioning
 */
public ProvisioningTask provision(ProvisionRequest provisionRequest,DatasetContext datasetContext){
  ProgramRunId programRunId=provisionRequest.getProgramRunId();
  ProgramOptions programOptions=provisionRequest.getProgramOptions();
  Map<String,String> args=programOptions.getArguments().asMap();
  String name=SystemArguments.getProfileProvisioner(args);
  Provisioner provisioner=provisionerInfo.get().provisioners.get(name);
  if (provisioner == null) {
    LOG.error(""String_Node_Str"",programRunId,name);
    provisionerNotifier.deprovisioned(programRunId);
    return new NoOpProvisioningTask(programRunId);
  }
  SSHKeyInfo sshKeyInfo=null;
  if (!YarnProvisioner.SPEC.equals(provisioner.getSpec())) {
    try {
      sshKeyInfo=generateSSHKey(programRunId);
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",programRunId,name,e);
      provisionerNotifier.deprovisioning(programRunId);
      return new NoOpProvisioningTask(programRunId);
    }
  }
  Map<String,String> properties=SystemArguments.getProfileProperties(args);
  ProvisionerContext context=new DefaultProvisionerContext(programRunId,properties,createSSHContext(sshKeyInfo));
  ClusterOp clusterOp=new ClusterOp(ClusterOp.Type.PROVISION,ClusterOp.Status.REQUESTING_CREATE);
  ClusterInfo clusterInfo=new ClusterInfo(programRunId,provisionRequest.getProgramDescriptor(),properties,name,provisionRequest.getUser(),clusterOp,sshKeyInfo,null);
  ProvisionerDataset provisionerDataset=ProvisionerDataset.get(datasetContext,datasetFramework);
  provisionerDataset.putClusterInfo(clusterInfo);
  return new ProvisionTask(provisionRequest,provisioner,context,provisionerNotifier,transactional,datasetFramework,sshKeyInfo);
}"
4573,"@Override public TwillPreparer prepare(TwillApplication application){
  if (application instanceof AbstractProgramTwillApplication) {
    ProgramId programId=((AbstractProgramTwillApplication)application).getProgramId();
    return new ImpersonatedTwillPreparer(delegate.prepare(application),impersonator,programId);
  }
  return delegate.prepare(application);
}","@Override public TwillPreparer prepare(TwillApplication application){
  if (application instanceof ProgramTwillApplication) {
    ProgramId programId=((ProgramTwillApplication)application).getProgramId();
    return new ImpersonatedTwillPreparer(delegate.prepare(application),impersonator,programId);
  }
  return delegate.prepare(application);
}"
4574,"/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @return the set of fixed {@link ProgramRunId}.
 */
private Set<ProgramRunId> doFixRunRecords(){
  LOG.trace(""String_Node_Str"");
  Set<ProgramRunId> fixedPrograms=new HashSet<>();
  Predicate<RunRecordMeta> filter=createFilter(fixedPrograms);
  for (  ProgramRunStatus status : NOT_STOPPED_STATUSES) {
    long startTime=0L;
    while (true) {
      Map<ProgramRunId,RunRecordMeta> runs=store.getRuns(status,startTime,Long.MAX_VALUE,txBatchSize,filter);
      LOG.trace(""String_Node_Str"",runs.size());
      if (runs.isEmpty()) {
        break;
      }
      for (      RunRecordMeta record : runs.values()) {
        startTime=Math.max(startTime,RunIds.getTime(record.getPid(),TimeUnit.SECONDS));
        ProgramRunId programRunId=record.getProgramRunId();
        if (!fixedPrograms.contains(programRunId)) {
          String msg=String.format(""String_Node_Str"",programRunId,record.getStatus());
          programStateWriter.error(programRunId,new ProgramRunAbortedException(msg));
          fixedPrograms.add(programRunId);
          LOG.warn(msg);
        }
      }
    }
  }
  if (fixedPrograms.isEmpty()) {
    LOG.trace(""String_Node_Str"",NOT_STOPPED_STATUSES);
  }
 else {
    LOG.warn(""String_Node_Str"",fixedPrograms.size(),NOT_STOPPED_STATUSES);
  }
  return fixedPrograms;
}","/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @return the set of fixed {@link ProgramRunId}.
 */
private Set<ProgramRunId> doFixRunRecords(){
  LOG.trace(""String_Node_Str"");
  Set<ProgramRunId> fixedPrograms=new HashSet<>();
  Predicate<RunRecordMeta> filter=createFilter(fixedPrograms);
  for (  ProgramRunStatus status : NOT_STOPPED_STATUSES) {
    while (true) {
      Map<ProgramRunId,RunRecordMeta> runs=store.getRuns(status,0L,Long.MAX_VALUE,txBatchSize,filter);
      LOG.trace(""String_Node_Str"",runs.size(),status);
      if (runs.isEmpty()) {
        break;
      }
      for (      RunRecordMeta record : runs.values()) {
        ProgramRunId programRunId=record.getProgramRunId();
        String msg=String.format(""String_Node_Str"",programRunId,record.getStatus());
        programStateWriter.error(programRunId,new ProgramRunAbortedException(msg));
        fixedPrograms.add(programRunId);
        LOG.warn(msg);
      }
    }
  }
  if (fixedPrograms.isEmpty()) {
    LOG.trace(""String_Node_Str"",NOT_STOPPED_STATUSES);
  }
 else {
    LOG.warn(""String_Node_Str"",fixedPrograms.size(),NOT_STOPPED_STATUSES);
  }
  return fixedPrograms;
}"
4575,"private void verifyRunningProgramCount(final Id.Program program,final String runId,final int expected) throws Exception {
  Tasks.waitFor(expected,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return runningProgramCount(program,runId);
    }
  }
,10,TimeUnit.SECONDS);
}","private void verifyRunningProgramCount(final Id.Program program,final String runId,final int expected) throws Exception {
  Tasks.waitFor(expected,() -> runningProgramCount(program,runId),10,TimeUnit.SECONDS);
}"
4576,"private void waitForStatusCode(final String path,final Id.Program program,final int expectedStatusCode) throws Exception {
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      HttpResponse response=doPost(getVersionedAPIPath(path,Constants.Gateway.API_VERSION_3_TOKEN,program.getNamespaceId()));
      return expectedStatusCode == response.getStatusLine().getStatusCode();
    }
  }
,60,TimeUnit.SECONDS);
}","private void waitForStatusCode(final String path,final Id.Program program,final int expectedStatusCode) throws Exception {
  Tasks.waitFor(true,() -> {
    HttpResponse response=doPost(getVersionedAPIPath(path,Constants.Gateway.API_VERSION_3_TOKEN,program.getNamespaceId()));
    return expectedStatusCode == response.getStatusLine().getStatusCode();
  }
,60,TimeUnit.SECONDS);
}"
4577,"@Test @SuppressWarnings(""String_Node_Str"") public void testWorkflowToken() throws Exception {
  Assert.assertEquals(200,deploy(AppWithWorkflow.class).getStatusLine().getStatusCode());
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,AppWithWorkflow.NAME);
  final Id.Workflow workflowId=Id.Workflow.from(appId,AppWithWorkflow.SampleWorkflow.NAME);
  String outputPath=new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath();
  startProgram(workflowId,ImmutableMap.of(""String_Node_Str"",createInput(""String_Node_Str""),""String_Node_Str"",outputPath));
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return getProgramRuns(workflowId,ProgramRunStatus.COMPLETED).size();
    }
  }
,60,TimeUnit.SECONDS);
  List<RunRecord> programRuns=getProgramRuns(workflowId,ProgramRunStatus.COMPLETED);
  Assert.assertEquals(1,programRuns.size());
  RunRecord runRecord=programRuns.get(0);
  String pid=runRecord.getPid();
  WorkflowTokenDetail workflowTokenDetail=getWorkflowToken(workflowId,pid,null,null);
  List<WorkflowTokenDetail.NodeValueDetail> nodeValueDetails=workflowTokenDetail.getTokenData().get(AppWithWorkflow.DummyAction.TOKEN_KEY);
  Assert.assertEquals(2,nodeValueDetails.size());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.FIRST_ACTION,nodeValueDetails.get(0).getNode());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.SECOND_ACTION,nodeValueDetails.get(1).getNode());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(0).getValue());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(1).getValue());
  workflowTokenDetail=getWorkflowToken(workflowId,pid,WorkflowToken.Scope.USER,AppWithWorkflow.DummyAction.TOKEN_KEY);
  nodeValueDetails=workflowTokenDetail.getTokenData().get(AppWithWorkflow.DummyAction.TOKEN_KEY);
  Assert.assertEquals(2,nodeValueDetails.size());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.FIRST_ACTION,nodeValueDetails.get(0).getNode());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.SECOND_ACTION,nodeValueDetails.get(1).getNode());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(0).getValue());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(1).getValue());
  WorkflowTokenNodeDetail nodeDetail=getWorkflowToken(workflowId,pid,AppWithWorkflow.SampleWorkflow.NAME,WorkflowToken.Scope.USER,null);
  Map<String,String> tokenData=nodeDetail.getTokenDataAtNode();
  Assert.assertEquals(2,tokenData.size());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.INITIALIZE_TOKEN_VALUE,tokenData.get(AppWithWorkflow.SampleWorkflow.INITIALIZE_TOKEN_KEY));
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.DESTROY_TOKEN_SUCCESS_VALUE,tokenData.get(AppWithWorkflow.SampleWorkflow.DESTROY_TOKEN_KEY));
  WorkflowTokenNodeDetail tokenAtNode=getWorkflowToken(workflowId,pid,AppWithWorkflow.SampleWorkflow.FIRST_ACTION,null,null);
  Map<String,String> tokenDataAtNode=tokenAtNode.getTokenDataAtNode();
  Assert.assertEquals(1,tokenDataAtNode.size());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,tokenDataAtNode.get(AppWithWorkflow.DummyAction.TOKEN_KEY));
  tokenAtNode=getWorkflowToken(workflowId,pid,AppWithWorkflow.SampleWorkflow.FIRST_ACTION,WorkflowToken.Scope.USER,AppWithWorkflow.DummyAction.TOKEN_KEY);
  tokenDataAtNode=tokenAtNode.getTokenDataAtNode();
  Assert.assertEquals(1,tokenDataAtNode.size());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,tokenDataAtNode.get(AppWithWorkflow.DummyAction.TOKEN_KEY));
}","@Test @SuppressWarnings(""String_Node_Str"") public void testWorkflowToken() throws Exception {
  Assert.assertEquals(200,deploy(AppWithWorkflow.class).getStatusLine().getStatusCode());
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,AppWithWorkflow.NAME);
  final Id.Workflow workflowId=Id.Workflow.from(appId,AppWithWorkflow.SampleWorkflow.NAME);
  String outputPath=new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath();
  startProgram(workflowId,ImmutableMap.of(""String_Node_Str"",createInput(""String_Node_Str""),""String_Node_Str"",outputPath));
  Tasks.waitFor(1,() -> getProgramRuns(workflowId,ProgramRunStatus.COMPLETED).size(),60,TimeUnit.SECONDS);
  List<RunRecord> programRuns=getProgramRuns(workflowId,ProgramRunStatus.COMPLETED);
  Assert.assertEquals(1,programRuns.size());
  RunRecord runRecord=programRuns.get(0);
  String pid=runRecord.getPid();
  WorkflowTokenDetail workflowTokenDetail=getWorkflowToken(workflowId,pid,null,null);
  List<WorkflowTokenDetail.NodeValueDetail> nodeValueDetails=workflowTokenDetail.getTokenData().get(AppWithWorkflow.DummyAction.TOKEN_KEY);
  Assert.assertEquals(2,nodeValueDetails.size());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.FIRST_ACTION,nodeValueDetails.get(0).getNode());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.SECOND_ACTION,nodeValueDetails.get(1).getNode());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(0).getValue());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(1).getValue());
  workflowTokenDetail=getWorkflowToken(workflowId,pid,WorkflowToken.Scope.USER,AppWithWorkflow.DummyAction.TOKEN_KEY);
  nodeValueDetails=workflowTokenDetail.getTokenData().get(AppWithWorkflow.DummyAction.TOKEN_KEY);
  Assert.assertEquals(2,nodeValueDetails.size());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.FIRST_ACTION,nodeValueDetails.get(0).getNode());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.SECOND_ACTION,nodeValueDetails.get(1).getNode());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(0).getValue());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(1).getValue());
  WorkflowTokenNodeDetail nodeDetail=getWorkflowToken(workflowId,pid,AppWithWorkflow.SampleWorkflow.NAME,WorkflowToken.Scope.USER,null);
  Map<String,String> tokenData=nodeDetail.getTokenDataAtNode();
  Assert.assertEquals(2,tokenData.size());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.INITIALIZE_TOKEN_VALUE,tokenData.get(AppWithWorkflow.SampleWorkflow.INITIALIZE_TOKEN_KEY));
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.DESTROY_TOKEN_SUCCESS_VALUE,tokenData.get(AppWithWorkflow.SampleWorkflow.DESTROY_TOKEN_KEY));
  WorkflowTokenNodeDetail tokenAtNode=getWorkflowToken(workflowId,pid,AppWithWorkflow.SampleWorkflow.FIRST_ACTION,null,null);
  Map<String,String> tokenDataAtNode=tokenAtNode.getTokenDataAtNode();
  Assert.assertEquals(1,tokenDataAtNode.size());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,tokenDataAtNode.get(AppWithWorkflow.DummyAction.TOKEN_KEY));
  tokenAtNode=getWorkflowToken(workflowId,pid,AppWithWorkflow.SampleWorkflow.FIRST_ACTION,WorkflowToken.Scope.USER,AppWithWorkflow.DummyAction.TOKEN_KEY);
  tokenDataAtNode=tokenAtNode.getTokenDataAtNode();
  Assert.assertEquals(1,tokenDataAtNode.size());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,tokenDataAtNode.get(AppWithWorkflow.DummyAction.TOKEN_KEY));
}"
4578,"@Test public void testWorkflowTokenPut() throws Exception {
  Assert.assertEquals(200,deploy(WorkflowTokenTestPutApp.class).getStatusLine().getStatusCode());
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,WorkflowTokenTestPutApp.NAME);
  Id.Workflow workflowId=Id.Workflow.from(appId,WorkflowTokenTestPutApp.WorkflowTokenTestPut.NAME);
  Id.Program mapReduceId=Id.Program.from(appId,ProgramType.MAPREDUCE,WorkflowTokenTestPutApp.RecordCounter.NAME);
  Id.Program sparkId=Id.Program.from(appId,ProgramType.SPARK,WorkflowTokenTestPutApp.SparkTestApp.NAME);
  String outputPath=new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath();
  startProgram(workflowId,ImmutableMap.of(""String_Node_Str"",createInputForRecordVerification(""String_Node_Str""),""String_Node_Str"",outputPath));
  waitState(workflowId,ProgramStatus.RUNNING.name());
  waitState(workflowId,ProgramStatus.STOPPED.name());
  verifyProgramRuns(workflowId,ProgramRunStatus.COMPLETED);
  List<RunRecord> runs=getProgramRuns(workflowId,ProgramRunStatus.COMPLETED);
  Assert.assertEquals(1,runs.size());
  String wfRunId=runs.get(0).getPid();
  WorkflowTokenDetail tokenDetail=getWorkflowToken(workflowId,wfRunId,null,null);
  List<WorkflowTokenDetail.NodeValueDetail> details=tokenDetail.getTokenData().get(""String_Node_Str"");
  Assert.assertEquals(1,details.size());
  Assert.assertEquals(wfRunId,details.get(0).getValue());
  for (  String key : new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""}) {
    Assert.assertFalse(tokenDetail.getTokenData().containsKey(key));
  }
  List<RunRecord> sparkProgramRuns=getProgramRuns(sparkId,ProgramRunStatus.COMPLETED);
  Assert.assertEquals(1,sparkProgramRuns.size());
}","@Test public void testWorkflowTokenPut() throws Exception {
  Assert.assertEquals(200,deploy(WorkflowTokenTestPutApp.class).getStatusLine().getStatusCode());
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,WorkflowTokenTestPutApp.NAME);
  Id.Workflow workflowId=Id.Workflow.from(appId,WorkflowTokenTestPutApp.WorkflowTokenTestPut.NAME);
  Id.Program sparkId=Id.Program.from(appId,ProgramType.SPARK,WorkflowTokenTestPutApp.SparkTestApp.NAME);
  String outputPath=new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath();
  startProgram(workflowId,ImmutableMap.of(""String_Node_Str"",createInputForRecordVerification(""String_Node_Str""),""String_Node_Str"",outputPath));
  waitState(workflowId,ProgramStatus.RUNNING.name());
  waitState(workflowId,ProgramStatus.STOPPED.name());
  verifyProgramRuns(workflowId,ProgramRunStatus.COMPLETED);
  List<RunRecord> runs=getProgramRuns(workflowId,ProgramRunStatus.COMPLETED);
  Assert.assertEquals(1,runs.size());
  String wfRunId=runs.get(0).getPid();
  WorkflowTokenDetail tokenDetail=getWorkflowToken(workflowId,wfRunId,null,null);
  List<WorkflowTokenDetail.NodeValueDetail> details=tokenDetail.getTokenData().get(""String_Node_Str"");
  Assert.assertEquals(1,details.size());
  Assert.assertEquals(wfRunId,details.get(0).getValue());
  for (  String key : new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""}) {
    Assert.assertFalse(tokenDetail.getTokenData().containsKey(key));
  }
  List<RunRecord> sparkProgramRuns=getProgramRuns(sparkId,ProgramRunStatus.COMPLETED);
  Assert.assertEquals(1,sparkProgramRuns.size());
}"
4579,"private String getRunIdOfRunningProgram(final Id.Program programId) throws Exception {
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return getProgramRuns(programId,ProgramRunStatus.RUNNING).size();
    }
  }
,5,TimeUnit.SECONDS);
  List<RunRecord> historyRuns=getProgramRuns(programId,ProgramRunStatus.RUNNING);
  Assert.assertEquals(1,historyRuns.size());
  RunRecord record=historyRuns.get(0);
  return record.getPid();
}","private String getRunIdOfRunningProgram(final Id.Program programId) throws Exception {
  Tasks.waitFor(1,() -> getProgramRuns(programId,ProgramRunStatus.RUNNING).size(),5,TimeUnit.SECONDS);
  List<RunRecord> historyRuns=getProgramRuns(programId,ProgramRunStatus.RUNNING);
  Assert.assertEquals(1,historyRuns.size());
  RunRecord record=historyRuns.get(0);
  return record.getPid();
}"
4580,"private void verifyFileExists(final List<File> fileList) throws Exception {
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      for (      File file : fileList) {
        if (!file.exists()) {
          return false;
        }
      }
      return true;
    }
  }
,180,TimeUnit.SECONDS);
}","private void verifyFileExists(final List<File> fileList) throws Exception {
  Tasks.waitFor(true,() -> {
    for (    File file : fileList) {
      if (!file.exists()) {
        return false;
      }
    }
    return true;
  }
,180,TimeUnit.SECONDS);
}"
4581,"/** 
 * Updates the given   {@link ProgramOptions} and return a new instance.It copies the  {@link ProgramOptions}. Then it adds all entries returned by   {@link #getExtraProgramOptions()}followed by adding the   {@link RunId} to the system arguments.Also scope resolution will be performed on the user arguments on the application and program.
 * @param programId the program id
 * @param options The {@link ProgramOptions} in which the RunId to be included
 * @param runId   The RunId to be included
 * @return the copy of the program options with RunId included in them
 */
private ProgramOptions updateProgramOptions(ArtifactId artifactId,ProgramId programId,ProgramOptions options,RunId runId){
  ImmutableMap.Builder<String,String> builder=ImmutableMap.builder();
  builder.putAll(options.getArguments().asMap());
  builder.putAll(getExtraProgramOptions());
  builder.put(ProgramOptionConstants.RUN_ID,runId.getId());
  builder.put(ProgramOptionConstants.ARTIFACT_ID,Joiner.on(':').join(artifactId.toIdParts()));
  String clusterName=options.getArguments().getOption(Constants.CLUSTER_NAME);
  Map<String,String> userArguments=options.getUserArguments().asMap();
  if (!Strings.isNullOrEmpty(clusterName)) {
    userArguments=RuntimeArguments.extractScope(CLUSTER_SCOPE,clusterName,userArguments);
  }
  userArguments=RuntimeArguments.extractScope(APPLICATION_SCOPE,programId.getApplication(),userArguments);
  userArguments=RuntimeArguments.extractScope(programId.getType().getScope(),programId.getProgram(),userArguments);
  return new SimpleProgramOptions(options.getProgramId(),new BasicArguments(builder.build()),new BasicArguments(userArguments),options.isDebug());
}","/** 
 * Updates the given   {@link ProgramOptions} and return a new instance.It copies the  {@link ProgramOptions}. Then it adds all entries returned by   {@link #getExtraProgramOptions()}followed by adding the   {@link RunId} to the system arguments.Also scope resolution will be performed on the user arguments on the application and program.
 * @param programId the program id
 * @param options The {@link ProgramOptions} in which the RunId to be included
 * @param runId   The RunId to be included
 * @return the copy of the program options with RunId included in them
 */
private ProgramOptions updateProgramOptions(ArtifactId artifactId,ProgramId programId,ProgramOptions options,RunId runId){
  Map<String,String> systemArguments=new HashMap<>();
  systemArguments.putAll(options.getArguments().asMap());
  for (  Map.Entry<String,String> extraOption : getExtraProgramOptions().entrySet()) {
    systemArguments.putIfAbsent(extraOption.getKey(),extraOption.getValue());
  }
  systemArguments.putIfAbsent(ProgramOptionConstants.RUN_ID,runId.getId());
  systemArguments.putIfAbsent(ProgramOptionConstants.ARTIFACT_ID,Joiner.on(':').join(artifactId.toIdParts()));
  String clusterName=options.getArguments().getOption(Constants.CLUSTER_NAME);
  Map<String,String> userArguments=options.getUserArguments().asMap();
  if (!Strings.isNullOrEmpty(clusterName)) {
    userArguments=RuntimeArguments.extractScope(CLUSTER_SCOPE,clusterName,userArguments);
  }
  userArguments=RuntimeArguments.extractScope(APPLICATION_SCOPE,programId.getApplication(),userArguments);
  userArguments=RuntimeArguments.extractScope(programId.getType().getScope(),programId.getProgram(),userArguments);
  return new SimpleProgramOptions(options.getProgramId(),new BasicArguments(systemArguments),new BasicArguments(userArguments),options.isDebug());
}"
4582,"@Override public void start(ProgramRunId programRunId,ProgramOptions programOptions,@Nullable String twillRunId,ArtifactId artifactId){
}","@Override public void start(ProgramRunId programRunId,ProgramOptions programOptions,@Nullable String twillRunId,ProgramDescriptor programDescriptor){
}"
4583,"/** 
 * Updates the program run's status to be   {@link ProgramRunStatus#STARTING} at the start time given by the{@link ProgramRunId}
 * @param programRunId the id of the program run
 * @param twillRunId the run id of the twill application
 */
void start(ProgramRunId programRunId,ProgramOptions programOptions,@Nullable String twillRunId,ArtifactId artifactId);","/** 
 * Updates the program run's status to be   {@link ProgramRunStatus#STARTING} at the start time given by the{@link ProgramRunId}
 * @param programRunId the id of the program run
 * @param programOptions the program options
 * @param twillRunId the run id of the twill application
 * @param programDescriptor the program descriptor
 */
void start(ProgramRunId programRunId,ProgramOptions programOptions,@Nullable String twillRunId,ProgramDescriptor programDescriptor);"
4584,"@Override public void start(ProgramRunId programRunId,ProgramOptions programOptions,@Nullable String twillRunId,ArtifactId artifactId){
  ImmutableMap.Builder<String,String> properties=ImmutableMap.<String,String>builder().put(ProgramOptionConstants.PROGRAM_RUN_ID,GSON.toJson(programRunId)).put(ProgramOptionConstants.PROGRAM_STATUS,ProgramRunStatus.STARTING.name()).put(ProgramOptionConstants.USER_OVERRIDES,GSON.toJson(programOptions.getUserArguments().asMap())).put(ProgramOptionConstants.SYSTEM_OVERRIDES,GSON.toJson(programOptions.getArguments().asMap())).put(ProgramOptionConstants.ARTIFACT_ID,GSON.toJson(artifactId));
  if (twillRunId != null) {
    properties.put(ProgramOptionConstants.TWILL_RUN_ID,twillRunId);
  }
  publish(properties);
}","@Override public void start(ProgramRunId programRunId,ProgramOptions programOptions,@Nullable String twillRunId,ProgramDescriptor programDescriptor){
  ImmutableMap.Builder<String,String> properties=ImmutableMap.<String,String>builder().put(ProgramOptionConstants.PROGRAM_RUN_ID,GSON.toJson(programRunId)).put(ProgramOptionConstants.PROGRAM_STATUS,ProgramRunStatus.STARTING.name()).put(ProgramOptionConstants.USER_OVERRIDES,GSON.toJson(programOptions.getUserArguments().asMap())).put(ProgramOptionConstants.SYSTEM_OVERRIDES,GSON.toJson(programOptions.getArguments().asMap())).put(ProgramOptionConstants.PROGRAM_DESCRIPTOR,GSON.toJson(programDescriptor));
  if (twillRunId != null) {
    properties.put(ProgramOptionConstants.TWILL_RUN_ID,twillRunId);
  }
  publish(properties);
}"
4585,"/** 
 * Get the name of the provisioner for the profile
 * @param args arguments
 * @return name of the provisioner for the profile
 */
public static Map<String,String> getProfileProperties(Map<String,String> args){
  Map<String,String> properties=new HashMap<>();
  for (  Map.Entry<String,String> arg : args.entrySet()) {
    if (arg.getKey().startsWith(PROFILE_PROPERTIES_PREFIX)) {
      String key=arg.getKey().substring(PROFILE_PROPERTIES_PREFIX.length());
      properties.put(key,arg.getValue());
    }
  }
  return properties;
}","/** 
 * Get the properties for the profile
 * @param args arguments
 * @return properties for the profile
 */
public static Map<String,String> getProfileProperties(Map<String,String> args){
  Map<String,String> properties=new HashMap<>();
  for (  Map.Entry<String,String> arg : args.entrySet()) {
    if (arg.getKey().startsWith(PROFILE_PROPERTIES_PREFIX)) {
      String key=arg.getKey().substring(PROFILE_PROPERTIES_PREFIX.length());
      properties.put(key,arg.getValue());
    }
  }
  return properties;
}"
4586,"private void runAndWait(ProgramRunner programRunner,Program program,ProgramOptions options) throws Exception {
  Closeable closeable=createCloseable(programRunner,program);
  RunId runId=ProgramRunners.getRunId(options);
  String twillRunId=options.getArguments().getOption(ProgramOptionConstants.TWILL_RUN_ID);
  programStateWriter.start(program.getId().run(runId),options,twillRunId,program.getApplicationSpecification().getArtifactId());
  ProgramController controller;
  try {
    controller=programRunner.run(program,options);
  }
 catch (  Throwable t) {
    programStateWriter.error(program.getId().run(runId),t);
    Closeables.closeQuietly(closeable);
    throw t;
  }
  blockForCompletion(closeable,controller);
  if (controller instanceof WorkflowTokenProvider) {
    updateWorkflowToken(((WorkflowTokenProvider)controller).getWorkflowToken());
  }
 else {
    throw new IllegalStateException(""String_Node_Str"" + program.getId());
  }
}","private void runAndWait(ProgramRunner programRunner,Program program,ProgramOptions options) throws Exception {
  Closeable closeable=createCloseable(programRunner,program);
  RunId runId=ProgramRunners.getRunId(options);
  String twillRunId=options.getArguments().getOption(ProgramOptionConstants.TWILL_RUN_ID);
  ProgramDescriptor programDescriptor=new ProgramDescriptor(program.getId(),program.getApplicationSpecification());
  programStateWriter.start(program.getId().run(runId),options,twillRunId,programDescriptor);
  ProgramController controller;
  try {
    controller=programRunner.run(program,options);
  }
 catch (  Throwable t) {
    programStateWriter.error(program.getId().run(runId),t);
    Closeables.closeQuietly(closeable);
    throw t;
  }
  blockForCompletion(closeable,controller);
  if (controller instanceof WorkflowTokenProvider) {
    updateWorkflowToken(((WorkflowTokenProvider)controller).getWorkflowToken());
  }
 else {
    throw new IllegalStateException(""String_Node_Str"" + program.getId());
  }
}"
4587,"/** 
 * Configures the AppFabricService pre-start.
 */
@Override protected void startUp() throws Exception {
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(NamespaceId.SYSTEM.getNamespace(),Constants.Logging.COMPONENT_NAME,Constants.Service.APP_FABRIC_HTTP));
  Futures.allAsList(ImmutableList.of(notificationService.start(),applicationLifecycleService.start(),systemArtifactLoader.start(),programRuntimeService.start(),streamCoordinatorClient.start(),programNotificationSubscriberService.start(),runRecordCorrectorService.start(),pluginService.start(),coreSchedulerService.start())).get();
  ImmutableList.Builder<HandlerHook> builder=ImmutableList.builder();
  for (  String hook : handlerHookNames) {
    builder.add(new MetricsReporterHook(metricsCollectionService,hook));
  }
  NettyHttpService.Builder httpServiceBuilder=new CommonNettyHttpServiceBuilder(cConf,Constants.Service.APP_FABRIC_HTTP).setHost(hostname.getCanonicalHostName()).setHandlerHooks(builder.build()).setHttpHandlers(handlers).setConnectionBacklog(cConf.getInt(Constants.AppFabric.BACKLOG_CONNECTIONS,Constants.AppFabric.DEFAULT_BACKLOG)).setExecThreadPoolSize(cConf.getInt(Constants.AppFabric.EXEC_THREADS,Constants.AppFabric.DEFAULT_EXEC_THREADS)).setBossThreadPoolSize(cConf.getInt(Constants.AppFabric.BOSS_THREADS,Constants.AppFabric.DEFAULT_BOSS_THREADS)).setWorkerThreadPoolSize(cConf.getInt(Constants.AppFabric.WORKER_THREADS,Constants.AppFabric.DEFAULT_WORKER_THREADS));
  if (sslEnabled) {
    httpServiceBuilder.setPort(cConf.getInt(Constants.AppFabric.SERVER_SSL_PORT));
    String password=generateRandomPassword();
    KeyStore ks=KeyStores.generatedCertKeyStore(sConf,password);
    SSLHandlerFactory sslHandlerFactory=new SSLHandlerFactory(ks,password);
    httpServiceBuilder.enableSSL(sslHandlerFactory);
  }
 else {
    httpServiceBuilder.setPort(cConf.getInt(Constants.AppFabric.SERVER_PORT));
  }
  cancelHttpService=startHttpService(httpServiceBuilder.build());
  defaultEntityEnsurer.startAndWait();
  if (appVersionUpgradeService != null) {
    appVersionUpgradeService.startAndWait();
  }
}","/** 
 * Configures the AppFabricService pre-start.
 */
@Override protected void startUp() throws Exception {
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(NamespaceId.SYSTEM.getNamespace(),Constants.Logging.COMPONENT_NAME,Constants.Service.APP_FABRIC_HTTP));
  Futures.allAsList(ImmutableList.of(notificationService.start(),provisioningService.start(),applicationLifecycleService.start(),systemArtifactLoader.start(),programRuntimeService.start(),streamCoordinatorClient.start(),programNotificationSubscriberService.start(),runRecordCorrectorService.start(),pluginService.start(),coreSchedulerService.start())).get();
  ImmutableList.Builder<HandlerHook> builder=ImmutableList.builder();
  for (  String hook : handlerHookNames) {
    builder.add(new MetricsReporterHook(metricsCollectionService,hook));
  }
  NettyHttpService.Builder httpServiceBuilder=new CommonNettyHttpServiceBuilder(cConf,Constants.Service.APP_FABRIC_HTTP).setHost(hostname.getCanonicalHostName()).setHandlerHooks(builder.build()).setHttpHandlers(handlers).setConnectionBacklog(cConf.getInt(Constants.AppFabric.BACKLOG_CONNECTIONS,Constants.AppFabric.DEFAULT_BACKLOG)).setExecThreadPoolSize(cConf.getInt(Constants.AppFabric.EXEC_THREADS,Constants.AppFabric.DEFAULT_EXEC_THREADS)).setBossThreadPoolSize(cConf.getInt(Constants.AppFabric.BOSS_THREADS,Constants.AppFabric.DEFAULT_BOSS_THREADS)).setWorkerThreadPoolSize(cConf.getInt(Constants.AppFabric.WORKER_THREADS,Constants.AppFabric.DEFAULT_WORKER_THREADS));
  if (sslEnabled) {
    httpServiceBuilder.setPort(cConf.getInt(Constants.AppFabric.SERVER_SSL_PORT));
    String password=generateRandomPassword();
    KeyStore ks=KeyStores.generatedCertKeyStore(sConf,password);
    SSLHandlerFactory sslHandlerFactory=new SSLHandlerFactory(ks,password);
    httpServiceBuilder.enableSSL(sslHandlerFactory);
  }
 else {
    httpServiceBuilder.setPort(cConf.getInt(Constants.AppFabric.SERVER_PORT));
  }
  cancelHttpService=startHttpService(httpServiceBuilder.build());
  defaultEntityEnsurer.startAndWait();
  if (appVersionUpgradeService != null) {
    appVersionUpgradeService.startAndWait();
  }
}"
4588,"/** 
 * Construct the AppFabricServer with service factory and cConf coming from guice injection.
 */
@Inject public AppFabricServer(CConfiguration cConf,SConfiguration sConf,DiscoveryService discoveryService,NotificationService notificationService,@Named(Constants.Service.MASTER_SERVICES_BIND_ADDRESS) InetAddress hostname,@Named(Constants.AppFabric.HANDLERS_BINDING) Set<HttpHandler> handlers,@Nullable MetricsCollectionService metricsCollectionService,ProgramRuntimeService programRuntimeService,RunRecordCorrectorService runRecordCorrectorService,ApplicationLifecycleService applicationLifecycleService,ProgramNotificationSubscriberService programNotificationSubscriberService,ProgramLifecycleService programLifecycleService,StreamCoordinatorClient streamCoordinatorClient,@Named(""String_Node_Str"") Set<String> servicesNames,@Named(""String_Node_Str"") Set<String> handlerHookNames,NamespaceAdmin namespaceAdmin,SystemArtifactLoader systemArtifactLoader,PluginService pluginService,@Nullable AppVersionUpgradeService appVersionUpgradeService,RouteStore routeStore,CoreSchedulerService coreSchedulerService,ProfileStore profileStore){
  this.hostname=hostname;
  this.discoveryService=discoveryService;
  this.handlers=handlers;
  this.cConf=cConf;
  this.sConf=sConf;
  this.metricsCollectionService=metricsCollectionService;
  this.programRuntimeService=programRuntimeService;
  this.notificationService=notificationService;
  this.servicesNames=servicesNames;
  this.handlerHookNames=handlerHookNames;
  this.applicationLifecycleService=applicationLifecycleService;
  this.streamCoordinatorClient=streamCoordinatorClient;
  this.programNotificationSubscriberService=programNotificationSubscriberService;
  this.runRecordCorrectorService=runRecordCorrectorService;
  this.systemArtifactLoader=systemArtifactLoader;
  this.pluginService=pluginService;
  this.appVersionUpgradeService=appVersionUpgradeService;
  this.routeStore=routeStore;
  this.defaultEntityEnsurer=new DefaultEntityEnsurer(namespaceAdmin,profileStore);
  this.sslEnabled=cConf.getBoolean(Constants.Security.SSL.INTERNAL_ENABLED);
  this.coreSchedulerService=coreSchedulerService;
}","/** 
 * Construct the AppFabricServer with service factory and cConf coming from guice injection.
 */
@Inject public AppFabricServer(CConfiguration cConf,SConfiguration sConf,DiscoveryService discoveryService,NotificationService notificationService,@Named(Constants.Service.MASTER_SERVICES_BIND_ADDRESS) InetAddress hostname,@Named(Constants.AppFabric.HANDLERS_BINDING) Set<HttpHandler> handlers,@Nullable MetricsCollectionService metricsCollectionService,ProgramRuntimeService programRuntimeService,RunRecordCorrectorService runRecordCorrectorService,ApplicationLifecycleService applicationLifecycleService,ProgramNotificationSubscriberService programNotificationSubscriberService,ProgramLifecycleService programLifecycleService,StreamCoordinatorClient streamCoordinatorClient,@Named(""String_Node_Str"") Set<String> servicesNames,@Named(""String_Node_Str"") Set<String> handlerHookNames,NamespaceAdmin namespaceAdmin,SystemArtifactLoader systemArtifactLoader,PluginService pluginService,@Nullable AppVersionUpgradeService appVersionUpgradeService,RouteStore routeStore,CoreSchedulerService coreSchedulerService,ProfileStore profileStore,ProvisioningService provisioningService){
  this.hostname=hostname;
  this.discoveryService=discoveryService;
  this.handlers=handlers;
  this.cConf=cConf;
  this.sConf=sConf;
  this.metricsCollectionService=metricsCollectionService;
  this.programRuntimeService=programRuntimeService;
  this.notificationService=notificationService;
  this.servicesNames=servicesNames;
  this.handlerHookNames=handlerHookNames;
  this.applicationLifecycleService=applicationLifecycleService;
  this.streamCoordinatorClient=streamCoordinatorClient;
  this.programNotificationSubscriberService=programNotificationSubscriberService;
  this.runRecordCorrectorService=runRecordCorrectorService;
  this.systemArtifactLoader=systemArtifactLoader;
  this.pluginService=pluginService;
  this.appVersionUpgradeService=appVersionUpgradeService;
  this.routeStore=routeStore;
  this.defaultEntityEnsurer=new DefaultEntityEnsurer(namespaceAdmin,profileStore);
  this.sslEnabled=cConf.getBoolean(Constants.Security.SSL.INTERNAL_ENABLED);
  this.coreSchedulerService=coreSchedulerService;
  this.provisioningService=provisioningService;
}"
4589,"@Override protected void shutDown() throws Exception {
  coreSchedulerService.stopAndWait();
  routeStore.close();
  defaultEntityEnsurer.stopAndWait();
  cancelHttpService.cancel();
  programRuntimeService.stopAndWait();
  applicationLifecycleService.stopAndWait();
  systemArtifactLoader.stopAndWait();
  notificationService.stopAndWait();
  programNotificationSubscriberService.stopAndWait();
  runRecordCorrectorService.stopAndWait();
  pluginService.stopAndWait();
  if (appVersionUpgradeService != null) {
    appVersionUpgradeService.stopAndWait();
  }
}","@Override protected void shutDown() throws Exception {
  coreSchedulerService.stopAndWait();
  routeStore.close();
  defaultEntityEnsurer.stopAndWait();
  cancelHttpService.cancel();
  programRuntimeService.stopAndWait();
  applicationLifecycleService.stopAndWait();
  systemArtifactLoader.stopAndWait();
  notificationService.stopAndWait();
  programNotificationSubscriberService.stopAndWait();
  runRecordCorrectorService.stopAndWait();
  pluginService.stopAndWait();
  if (appVersionUpgradeService != null) {
    appVersionUpgradeService.stopAndWait();
  }
  provisioningService.stopAndWait();
}"
4590,"@Override public TwillPreparer prepare(TwillApplication application){
  if (application instanceof AbstractProgramTwillApplication) {
    ProgramId programId=((AbstractProgramTwillApplication)application).getProgramId();
    return new ImpersonatedTwillPreparer(delegate.prepare(application),impersonator,programId);
  }
  return delegate.prepare(application);
}","@Override public TwillPreparer prepare(TwillApplication application){
  if (application instanceof ProgramTwillApplication) {
    ProgramId programId=((ProgramTwillApplication)application).getProgramId();
    return new ImpersonatedTwillPreparer(delegate.prepare(application),impersonator,programId);
  }
  return delegate.prepare(application);
}"
4591,"public ProvisionerExtensionLoader(String extDirs){
  super(extDirs);
}","@Inject ProvisionerExtensionLoader(CConfiguration cConf){
  super(cConf.get(Constants.Provisioner.EXTENSIONS_DIR));
}"
4592,"/** 
 * Checks whether the existing run record meta of a given program run are in a state for the program run to transition into the given run status. This is required because program states are not guaranteed to be written in order. For example, starting can be written from a twill AM, while running may be written from a twill runnable. If the running state is written before the starting state, we don't want to record the state as starting once it is already running.
 * @param existing the existing run record meta of the given program run
 * @param sourceId unique id representing the source of program run status, such as the message id of the programrun status notification in TMS. The source id must increase as the recording time of the program run status increases, so that the attempt to persist program run status older than the existing program run status will be ignored
 * @param recordType the type of record corresponding to the current status
 * @return {@code true} if the program run is allowed to persist the given status, {@code false} otherwise
 */
private boolean isValid(RunRecordMeta existing,byte[] sourceId,String recordType){
  byte[] existingSourceId=existing.getSourceId();
  if (existingSourceId != null && Bytes.compareTo(sourceId,existingSourceId) <= 0) {
    LOG.debug(""String_Node_Str"" + ""String_Node_Str"",Bytes.toHexString(sourceId),Bytes.toHexString(existingSourceId),existing,recordType);
  }
  return true;
}","/** 
 * Checks whether the existing run record meta of a given program run are in a state for the program run to transition into the given run status. This is required because program states are not guaranteed to be written in order. For example, starting can be written from a twill AM, while running may be written from a twill runnable. If the running state is written before the starting state, we don't want to record the state as starting once it is already running.
 * @param existing the existing run record meta of the given program run
 * @param sourceId unique id representing the source of program run status, such as the message id of the programrun status notification in TMS. The source id must increase as the recording time of the program run status increases, so that the attempt to persist program run status older than the existing program run status will be ignored
 * @param recordType the type of record corresponding to the current status
 * @return {@code true} if the program run is allowed to persist the given status, {@code false} otherwise
 */
private boolean isValid(RunRecordMeta existing,byte[] sourceId,String recordType){
  byte[] existingSourceId=existing.getSourceId();
  if (existingSourceId != null && Bytes.compareTo(sourceId,existingSourceId) <= 0) {
    LOG.debug(""String_Node_Str"" + ""String_Node_Str"",Bytes.toHexString(sourceId),Bytes.toHexString(existingSourceId),existing,recordType);
    return false;
  }
  return true;
}"
4593,"private TriggerStatusV2 readTrigger(TriggerKey key){
  byte[][] col=new byte[1][];
  col[0]=Bytes.toBytes(key.getName());
  Row result=table.get(TRIGGER_KEY,col);
  byte[] bytes=null;
  if (!result.isEmpty()) {
    bytes=result.get(col[0]);
  }
  if (bytes != null) {
    return (TriggerStatusV2)SerializationUtils.deserialize(bytes);
  }
 else {
    return null;
  }
}","@VisibleForTesting TriggerStatusV2 readTrigger(Table table,TriggerKey key){
  byte[][] col=new byte[1][];
  col[0]=Bytes.toBytes(key.getName());
  Row result=table.get(TRIGGER_KEY,col);
  byte[] bytes=null;
  if (!result.isEmpty()) {
    bytes=result.get(col[0]);
  }
  if (bytes != null) {
    return (TriggerStatusV2)SerializationUtils.deserialize(bytes);
  }
 else {
    return null;
  }
}"
4594,"private void upgradeTriggers(Table table){
  Row result=table.get(TRIGGER_KEY);
  if (result.isEmpty()) {
    return;
  }
  for (  byte[] column : result.getColumns().values()) {
    TriggerStatusV2 triggerStatus=(TriggerStatusV2)SerializationUtils.deserialize(column);
    TriggerKey oldTriggerKey=triggerStatus.trigger.getKey();
    boolean modified=addDefaultAppVersionIfNeeded(triggerStatus);
    if (modified) {
      if (readTrigger(triggerStatus.trigger.getKey()) == null) {
        persistTrigger(table,triggerStatus.trigger,triggerStatus.state);
      }
      removeTrigger(table,oldTriggerKey);
    }
  }
}","private void upgradeTriggers(Table table){
  Row result=table.get(TRIGGER_KEY);
  if (result.isEmpty()) {
    return;
  }
  for (  byte[] column : result.getColumns().values()) {
    TriggerStatusV2 triggerStatus=(TriggerStatusV2)SerializationUtils.deserialize(column);
    TriggerKey oldTriggerKey=triggerStatus.trigger.getKey();
    boolean modified=addDefaultAppVersionIfNeeded(triggerStatus);
    if (modified) {
      if (readTrigger(table,triggerStatus.trigger.getKey()) == null) {
        persistTrigger(table,triggerStatus.trigger,triggerStatus.state);
      }
      removeTrigger(table,oldTriggerKey);
    }
  }
}"
4595,"private void readSchedulesFromPersistentStore() throws Exception {
  final List<JobDetail> jobs=Lists.newArrayList();
  final List<TriggerStatusV2> triggers=Lists.newArrayList();
  factory.createExecutor(ImmutableList.of((TransactionAware)table)).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      Row result=table.get(JOB_KEY);
      if (!result.isEmpty()) {
        for (        byte[] bytes : result.getColumns().values()) {
          JobDetail jobDetail=(JobDetail)SerializationUtils.deserialize(bytes);
          jobDetail=addDefaultAppVersionIfNeeded(jobDetail);
          LOG.debug(""String_Node_Str"",jobDetail.getKey());
          jobs.add(jobDetail);
        }
      }
 else {
        LOG.debug(""String_Node_Str"");
      }
      result=table.get(TRIGGER_KEY);
      if (!result.isEmpty()) {
        for (        byte[] bytes : result.getColumns().values()) {
          TriggerStatusV2 trigger=(TriggerStatusV2)SerializationUtils.deserialize(bytes);
          addDefaultAppVersionIfNeeded(trigger);
          if (trigger.state.equals(Trigger.TriggerState.NORMAL) || trigger.state.equals(Trigger.TriggerState.PAUSED)) {
            triggers.add(trigger);
            LOG.debug(""String_Node_Str"",trigger.trigger.getKey());
          }
 else {
            LOG.debug(""String_Node_Str"",trigger.trigger.getKey(),trigger.state);
          }
        }
      }
 else {
        LOG.debug(""String_Node_Str"");
      }
    }
  }
);
  for (  JobDetail job : jobs) {
    super.storeJob(job,true);
  }
  for (  TriggerStatusV2 trigger : triggers) {
    super.storeTrigger(trigger.trigger,true);
    if (trigger.state == Trigger.TriggerState.PAUSED) {
      super.pauseTrigger(trigger.trigger.getKey());
    }
  }
}","private void readSchedulesFromPersistentStore() throws Exception {
  final List<JobDetail> jobs=Lists.newArrayList();
  final List<TriggerStatusV2> triggers=Lists.newArrayList();
  factory.createExecutor(ImmutableList.of((TransactionAware)table)).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      Row result=table.get(JOB_KEY);
      if (!result.isEmpty()) {
        for (        byte[] bytes : result.getColumns().values()) {
          JobDetail jobDetail=(JobDetail)SerializationUtils.deserialize(bytes);
          jobDetail=addDefaultAppVersionIfNeeded(jobDetail);
          LOG.debug(""String_Node_Str"",jobDetail.getKey());
          jobs.add(jobDetail);
        }
      }
 else {
        LOG.debug(""String_Node_Str"");
      }
      result=table.get(TRIGGER_KEY);
      if (!result.isEmpty()) {
        for (        byte[] bytes : result.getColumns().values()) {
          TriggerStatusV2 trigger=(TriggerStatusV2)SerializationUtils.deserialize(bytes);
          addDefaultAppVersionIfNeeded(trigger);
          if (trigger.state.equals(Trigger.TriggerState.NORMAL) || trigger.state.equals(Trigger.TriggerState.PAUSED)) {
            triggers.add(trigger);
            LOG.debug(""String_Node_Str"",trigger.trigger.getKey());
          }
 else {
            LOG.debug(""String_Node_Str"",trigger.trigger.getKey(),trigger.state);
          }
        }
      }
 else {
        LOG.debug(""String_Node_Str"");
      }
    }
  }
);
  Set<JobKey> jobKeys=new HashSet<>();
  for (  JobDetail job : jobs) {
    super.storeJob(job,true);
    jobKeys.add(job.getKey());
  }
  Set<TriggerKey> triggersWithNoJob=new HashSet<>();
  for (  TriggerStatusV2 trigger : triggers) {
    if (!jobKeys.contains(trigger.trigger.getJobKey())) {
      triggersWithNoJob.add(trigger.trigger.getKey());
      continue;
    }
    super.storeTrigger(trigger.trigger,true);
    if (trigger.state == Trigger.TriggerState.PAUSED) {
      super.pauseTrigger(trigger.trigger.getKey());
    }
  }
  for (  TriggerKey key : triggersWithNoJob) {
    LOG.error(String.format(""String_Node_Str"" + ""String_Node_Str"",key));
    executeDelete(key);
  }
}"
4596,"private void persistChangeOfState(final TriggerKey triggerKey,final Trigger.TriggerState newTriggerState){
  try {
    Preconditions.checkNotNull(triggerKey);
    factory.createExecutor(ImmutableList.of((TransactionAware)table)).execute(new TransactionExecutor.Subroutine(){
      @Override public void apply() throws Exception {
        TriggerStatusV2 storedTriggerStatus=readTrigger(triggerKey);
        if (storedTriggerStatus != null) {
          persistTrigger(table,storedTriggerStatus.trigger,newTriggerState);
        }
 else {
          LOG.warn(""String_Node_Str"",triggerKey,ScheduleStoreTableUtil.SCHEDULE_STORE_DATASET_NAME,newTriggerState);
        }
      }
    }
);
  }
 catch (  Throwable th) {
    throw Throwables.propagate(th);
  }
}","private void persistChangeOfState(final TriggerKey triggerKey,final Trigger.TriggerState newTriggerState){
  try {
    Preconditions.checkNotNull(triggerKey);
    factory.createExecutor(ImmutableList.of((TransactionAware)table)).execute(new TransactionExecutor.Subroutine(){
      @Override public void apply() throws Exception {
        TriggerStatusV2 storedTriggerStatus=readTrigger(table,triggerKey);
        if (storedTriggerStatus != null) {
          persistTrigger(table,storedTriggerStatus.trigger,newTriggerState);
        }
 else {
          LOG.warn(""String_Node_Str"",triggerKey,ScheduleStoreTableUtil.SCHEDULE_STORE_DATASET_NAME,newTriggerState);
        }
      }
    }
);
  }
 catch (  Throwable th) {
    throw Throwables.propagate(th);
  }
}"
4597,"private void removeJob(Table table,JobKey key){
  byte[][] col=new byte[1][];
  col[0]=Bytes.toBytes(key.toString());
  table.delete(JOB_KEY,col);
}","@VisibleForTesting void removeJob(Table table,JobKey key){
  byte[][] col=new byte[1][];
  col[0]=Bytes.toBytes(key.toString());
  table.delete(JOB_KEY,col);
}"
4598,"private static void schedulerSetup(boolean enablePersistence) throws SchedulerException {
  JobStore js;
  if (enablePersistence) {
    CConfiguration conf=injector.getInstance(CConfiguration.class);
    js=new DatasetBasedTimeScheduleStore(factory,new ScheduleStoreTableUtil(dsFramework,conf),conf);
  }
 else {
    js=new RAMJobStore();
  }
  SimpleThreadPool threadPool=new SimpleThreadPool(10,Thread.NORM_PRIORITY);
  threadPool.initialize();
  DirectSchedulerFactory.getInstance().createScheduler(DUMMY_SCHEDULER_NAME,""String_Node_Str"",threadPool,js);
  scheduler=DirectSchedulerFactory.getInstance().getScheduler(DUMMY_SCHEDULER_NAME);
  scheduler.start();
}","private static void schedulerSetup(boolean enablePersistence) throws SchedulerException {
  JobStore js;
  if (enablePersistence) {
    CConfiguration conf=injector.getInstance(CConfiguration.class);
    tableUtil=new ScheduleStoreTableUtil(dsFramework,conf);
    datasetBasedTimeScheduleStore=new DatasetBasedTimeScheduleStore(factory,tableUtil,conf);
    js=datasetBasedTimeScheduleStore;
  }
 else {
    js=new RAMJobStore();
  }
  SimpleThreadPool threadPool=new SimpleThreadPool(10,Thread.NORM_PRIORITY);
  threadPool.initialize();
  DirectSchedulerFactory.getInstance().createScheduler(DUMMY_SCHEDULER_NAME,""String_Node_Str"",threadPool,js);
  scheduler=DirectSchedulerFactory.getInstance().getScheduler(DUMMY_SCHEDULER_NAME);
  scheduler.start();
}"
4599,"private void processNotification(AppMetadataStore appMetadataStore,Notification notification,byte[] messageIdBytes) throws Exception {
  Map<String,String> properties=notification.getProperties();
  String programRun=properties.get(ProgramOptionConstants.PROGRAM_RUN_ID);
  String programStatus=properties.get(ProgramOptionConstants.PROGRAM_STATUS);
  if (programRun == null || programStatus == null) {
    LOG.warn(""String_Node_Str"",notification);
    return;
  }
  ProgramRunStatus programRunStatus;
  try {
    programRunStatus=ProgramRunStatus.valueOf(programStatus);
  }
 catch (  IllegalArgumentException e) {
    LOG.warn(""String_Node_Str"",programStatus,programRun,notification);
    return;
  }
  ProgramRunId programRunId=GSON.fromJson(programRun,ProgramRunId.class);
  ProgramId programId=programRunId.getParent();
  ApplicationMeta meta=appMetadataStore.getApplication(programRunId.getNamespace(),programRunId.getApplication(),programRunId.getVersion());
  if (meta == null) {
    LOG.warn(""String_Node_Str"",programRunId,notification);
    return;
  }
  if (getProgramSpecFromApp(meta.getSpec(),programRunId) == null) {
    LOG.warn(""String_Node_Str"",programRunId,notification);
    return;
  }
  LOG.trace(""String_Node_Str"",notification);
  String runId=programRunId.getRun();
  String twillRunId=notification.getProperties().get(ProgramOptionConstants.TWILL_RUN_ID);
  long endTimeSecs=getTimeSeconds(notification.getProperties(),ProgramOptionConstants.END_TIME);
  ProgramRunStatus recordedStatus;
switch (programRunStatus) {
case STARTING:
    long startTimeSecs=getTimeSeconds(notification.getProperties(),ProgramOptionConstants.START_TIME);
  String userArgumentsString=properties.get(ProgramOptionConstants.USER_OVERRIDES);
String systemArgumentsString=properties.get(ProgramOptionConstants.SYSTEM_OVERRIDES);
if (userArgumentsString == null || systemArgumentsString == null) {
LOG.warn(""String_Node_Str"",programRunId,(userArgumentsString == null) ? ""String_Node_Str"" : ""String_Node_Str"",notification);
return;
}
if (startTimeSecs == -1) {
LOG.warn(""String_Node_Str"",programRunId,notification);
return;
}
Map<String,String> userArguments=GSON.fromJson(userArgumentsString,STRING_STRING_MAP);
Map<String,String> systemArguments=GSON.fromJson(systemArgumentsString,STRING_STRING_MAP);
recordedStatus=appMetadataStore.recordProgramStart(programId,runId,startTimeSecs,twillRunId,userArguments,systemArguments,messageIdBytes);
break;
case RUNNING:
long logicalStartTimeSecs=getTimeSeconds(notification.getProperties(),ProgramOptionConstants.LOGICAL_START_TIME);
if (logicalStartTimeSecs == -1) {
LOG.warn(""String_Node_Str"",programRunId,ProgramOptionConstants.LOGICAL_START_TIME,notification);
return;
}
recordedStatus=appMetadataStore.recordProgramRunning(programId,runId,logicalStartTimeSecs,twillRunId,messageIdBytes);
break;
case SUSPENDED:
recordedStatus=appMetadataStore.recordProgramSuspend(programId,runId,messageIdBytes);
break;
case RESUMING:
recordedStatus=appMetadataStore.recordProgramResumed(programId,runId,messageIdBytes);
break;
case COMPLETED:
case KILLED:
if (endTimeSecs == -1) {
LOG.warn(""String_Node_Str"",programRunId,notification);
return;
}
recordedStatus=appMetadataStore.recordProgramStop(programId,runId,endTimeSecs,programRunStatus,null,messageIdBytes);
break;
case FAILED:
if (endTimeSecs == -1) {
LOG.warn(""String_Node_Str"",programRunId,notification);
return;
}
BasicThrowable cause=decodeBasicThrowable(properties.get(ProgramOptionConstants.PROGRAM_ERROR));
recordedStatus=appMetadataStore.recordProgramStop(programId,runId,endTimeSecs,programRunStatus,cause,messageIdBytes);
break;
default :
LOG.error(""String_Node_Str"",programRunStatus,programRunId,notification);
return;
}
if (recordedStatus != null) {
publishRecordedStatus(programRunId,recordedStatus);
}
}","private void processNotification(AppMetadataStore appMetadataStore,Notification notification,byte[] messageIdBytes) throws Exception {
  Map<String,String> properties=notification.getProperties();
  String programRun=properties.get(ProgramOptionConstants.PROGRAM_RUN_ID);
  String programStatus=properties.get(ProgramOptionConstants.PROGRAM_STATUS);
  if (programRun == null || programStatus == null) {
    LOG.warn(""String_Node_Str"",notification);
    return;
  }
  ProgramRunStatus programRunStatus;
  try {
    programRunStatus=ProgramRunStatus.valueOf(programStatus);
  }
 catch (  IllegalArgumentException e) {
    LOG.warn(""String_Node_Str"",programStatus,programRun,notification);
    return;
  }
  ProgramRunId programRunId=GSON.fromJson(programRun,ProgramRunId.class);
  ProgramId programId=programRunId.getParent();
  LOG.trace(""String_Node_Str"",notification);
  String runId=programRunId.getRun();
  String twillRunId=notification.getProperties().get(ProgramOptionConstants.TWILL_RUN_ID);
  long endTimeSecs=getTimeSeconds(notification.getProperties(),ProgramOptionConstants.END_TIME);
  ProgramRunStatus recordedStatus;
switch (programRunStatus) {
case STARTING:
    long startTimeSecs=getTimeSeconds(notification.getProperties(),ProgramOptionConstants.START_TIME);
  String userArgumentsString=properties.get(ProgramOptionConstants.USER_OVERRIDES);
String systemArgumentsString=properties.get(ProgramOptionConstants.SYSTEM_OVERRIDES);
if (userArgumentsString == null || systemArgumentsString == null) {
LOG.warn(""String_Node_Str"",programRunId,(userArgumentsString == null) ? ""String_Node_Str"" : ""String_Node_Str"",notification);
return;
}
if (startTimeSecs == -1) {
LOG.warn(""String_Node_Str"",programRunId,notification);
return;
}
Map<String,String> userArguments=GSON.fromJson(userArgumentsString,STRING_STRING_MAP);
Map<String,String> systemArguments=GSON.fromJson(systemArgumentsString,STRING_STRING_MAP);
recordedStatus=appMetadataStore.recordProgramStart(programId,runId,startTimeSecs,twillRunId,userArguments,systemArguments,messageIdBytes);
break;
case RUNNING:
long logicalStartTimeSecs=getTimeSeconds(notification.getProperties(),ProgramOptionConstants.LOGICAL_START_TIME);
if (logicalStartTimeSecs == -1) {
LOG.warn(""String_Node_Str"",programRunId,ProgramOptionConstants.LOGICAL_START_TIME,notification);
return;
}
recordedStatus=appMetadataStore.recordProgramRunning(programId,runId,logicalStartTimeSecs,twillRunId,messageIdBytes);
break;
case SUSPENDED:
recordedStatus=appMetadataStore.recordProgramSuspend(programId,runId,messageIdBytes);
break;
case RESUMING:
recordedStatus=appMetadataStore.recordProgramResumed(programId,runId,messageIdBytes);
break;
case COMPLETED:
case KILLED:
if (endTimeSecs == -1) {
LOG.warn(""String_Node_Str"",programRunId,notification);
return;
}
recordedStatus=appMetadataStore.recordProgramStop(programId,runId,endTimeSecs,programRunStatus,null,messageIdBytes);
break;
case FAILED:
if (endTimeSecs == -1) {
LOG.warn(""String_Node_Str"",programRunId,notification);
return;
}
BasicThrowable cause=decodeBasicThrowable(properties.get(ProgramOptionConstants.PROGRAM_ERROR));
recordedStatus=appMetadataStore.recordProgramStop(programId,runId,endTimeSecs,programRunStatus,cause,messageIdBytes);
break;
default :
LOG.error(""String_Node_Str"",programRunStatus,programRunId,notification);
return;
}
if (recordedStatus != null) {
publishRecordedStatus(programRunId,recordedStatus);
}
}"
4600,"private ProgramRunStatus recordProgramStop(ProgramId programId,String pid,long stopTs,ProgramRunStatus runStatus,@Nullable BasicThrowable failureCause,MDSKey.Builder builder,byte[] sourceId){
  List<RunRecordMeta> existingRecords=getRuns(programId,pid);
  boolean isValid=validateExistingRecords(existingRecords,programId,pid,sourceId,runStatus.name().toLowerCase(),runStatus);
  if (!isValid) {
    return null;
  }
  RunRecordMeta existing=existingRecords.get(0);
  MDSKey key=getProgramKeyBuilder(STATUS_TYPE_MAP.get(existing.getStatus()),programId).add(pid).build();
  deleteAll(key);
  Map<String,String> systemArgs=existing.getSystemArgs();
  if (systemArgs != null && systemArgs.containsKey(ProgramOptionConstants.WORKFLOW_NAME)) {
    addWorkflowNodeState(programId,pid,systemArgs,runStatus,failureCause,sourceId);
  }
  key=builder.add(getInvertedTsKeyPart(existing.getStartTs())).add(pid).build();
  write(key,new RunRecordMeta(existing,stopTs,runStatus,sourceId));
  return runStatus;
}","private ProgramRunStatus recordProgramStop(ProgramId programId,String pid,long stopTs,ProgramRunStatus runStatus,@Nullable BasicThrowable failureCause,MDSKey.Builder builder,byte[] sourceId){
  RunRecordMeta existing=getRun(programId,pid);
  boolean isValid=validateExistingRecord(existing,programId,pid,sourceId,runStatus.name().toLowerCase(),runStatus);
  if (!isValid) {
    return null;
  }
  MDSKey key=getProgramKeyBuilder(STATUS_TYPE_MAP.get(existing.getStatus()),programId).add(pid).build();
  deleteAll(key);
  Map<String,String> systemArgs=existing.getSystemArgs();
  if (systemArgs != null && systemArgs.containsKey(ProgramOptionConstants.WORKFLOW_NAME)) {
    addWorkflowNodeState(programId,pid,systemArgs,runStatus,failureCause,sourceId);
  }
  key=builder.add(getInvertedTsKeyPart(existing.getStartTs())).add(pid).build();
  write(key,new RunRecordMeta(existing,stopTs,runStatus,sourceId));
  return runStatus;
}"
4601,"private ProgramRunStatus recordProgramRunning(ProgramId programId,String pid,long runTs,String twillRunId,MDSKey.Builder keyBuilder,byte[] sourceId){
  List<RunRecordMeta> existingRecords=getRuns(programId,pid);
  boolean isValid=validateExistingRecords(existingRecords,programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.RUNNING);
  if (!isValid) {
    return null;
  }
  RunRecordMeta existing=existingRecords.get(0);
  Map<String,String> systemArgs=existing.getSystemArgs();
  if (systemArgs != null && systemArgs.containsKey(ProgramOptionConstants.WORKFLOW_NAME)) {
    addWorkflowNodeState(programId,pid,systemArgs,ProgramRunStatus.RUNNING,null,sourceId);
  }
  MDSKey key=getProgramKeyBuilder(STATUS_TYPE_MAP.get(existing.getStatus()),programId).add(pid).build();
  deleteAll(key);
  key=keyBuilder.add(pid).build();
  RunRecordMeta meta=new RunRecordMeta(programId.run(pid),existing.getStartTs(),runTs,null,ProgramRunStatus.RUNNING,existing.getProperties(),systemArgs,twillRunId,sourceId);
  write(key,meta);
  return ProgramRunStatus.RUNNING;
}","private ProgramRunStatus recordProgramRunning(ProgramId programId,String pid,long runTs,String twillRunId,MDSKey.Builder keyBuilder,byte[] sourceId){
  RunRecordMeta existing=getRun(programId,pid);
  boolean isValid=validateExistingRecord(existing,programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.RUNNING);
  if (!isValid) {
    return null;
  }
  Map<String,String> systemArgs=existing.getSystemArgs();
  if (systemArgs != null && systemArgs.containsKey(ProgramOptionConstants.WORKFLOW_NAME)) {
    addWorkflowNodeState(programId,pid,systemArgs,ProgramRunStatus.RUNNING,null,sourceId);
  }
  MDSKey key=getProgramKeyBuilder(STATUS_TYPE_MAP.get(existing.getStatus()),programId).add(pid).build();
  deleteAll(key);
  key=keyBuilder.add(pid).build();
  RunRecordMeta meta=new RunRecordMeta(programId.run(pid),existing.getStartTs(),runTs,null,ProgramRunStatus.RUNNING,existing.getProperties(),systemArgs,twillRunId,sourceId);
  write(key,meta);
  return ProgramRunStatus.RUNNING;
}"
4602,"/** 
 * Logs resume of a program run and sets the run status to   {@link ProgramRunStatus#RUNNING}.
 * @param programId id of the program
 * @param pid run id
 * @param sourceId id of the source of program run status, which is proportional to the timestamp ofwhen the current program run status is reached, e.g. a message id from TMS
 * @return {@link ProgramRunStatus#RUNNING} if it is successfully persisted, {@code null} otherwise.
 */
@Nullable public ProgramRunStatus recordProgramResumed(ProgramId programId,String pid,byte[] sourceId){
  List<RunRecordMeta> existingRecords=getRuns(programId,pid);
  boolean isValid=validateExistingRecords(existingRecords,programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.RESUMING);
  if (!isValid && !existingRecords.isEmpty()) {
    return null;
  }
  RunRecordMeta existing=null;
  if (existingRecords.isEmpty()) {
    if (!upgradeComplete.get() && programId.getVersion().equals(ApplicationId.DEFAULT_VERSION)) {
      MDSKey key=getVersionLessProgramKeyBuilder(TYPE_RUN_RECORD_SUSPENDED,programId).add(pid).build();
      existing=get(key,RunRecordMeta.class);
    }
    if (existing == null) {
      LOG.error(""String_Node_Str"",programId,pid);
      return null;
    }
  }
 else {
    existing=existingRecords.get(0);
  }
  recordProgramSuspendResume(programId,pid,sourceId,existing,""String_Node_Str"");
  return ProgramRunStatus.RUNNING;
}","/** 
 * Logs resume of a program run and sets the run status to   {@link ProgramRunStatus#RUNNING}.
 * @param programId id of the program
 * @param pid run id
 * @param sourceId id of the source of program run status, which is proportional to the timestamp ofwhen the current program run status is reached, e.g. a message id from TMS
 * @return {@link ProgramRunStatus#RUNNING} if it is successfully persisted, {@code null} otherwise.
 */
@Nullable public ProgramRunStatus recordProgramResumed(ProgramId programId,String pid,byte[] sourceId){
  RunRecordMeta existing=getRun(programId,pid);
  boolean isValid=validateExistingRecord(existing,programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.RESUMING);
  if (!isValid && existing != null) {
    return null;
  }
  if (existing == null) {
    LOG.error(""String_Node_Str"",programId,pid);
    return null;
  }
  recordProgramSuspendResume(programId,pid,sourceId,existing,""String_Node_Str"");
  return ProgramRunStatus.RUNNING;
}"
4603,"/** 
 * Logs initialization of program run and persists program status to   {@link ProgramRunStatus#STARTING}.
 * @param programId id of the program
 * @param pid run id
 * @param startTs initialization timestamp in seconds
 * @param twillRunId Twill run id
 * @param runtimeArgs the runtime arguments for this program run
 * @param systemArgs the system arguments for this program run
 * @param sourceId id of the source of program run status, which is proportional to the timestamp ofwhen the current program run status is reached, e.g. a message id from TMS
 * @return {@link ProgramRunStatus#STARTING} if it is successfully persisted, {@code null} otherwise.
 */
public ProgramRunStatus recordProgramStart(ProgramId programId,String pid,long startTs,String twillRunId,Map<String,String> runtimeArgs,Map<String,String> systemArgs,byte[] sourceId){
  MDSKey.Builder keyBuilder=getProgramKeyBuilder(TYPE_RUN_RECORD_STARTING,programId);
  boolean isValid=validateExistingRecords(getRuns(programId,pid),programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.STARTING);
  if (!isValid) {
    return null;
  }
  String workflowRunId=null;
  if (systemArgs != null && systemArgs.containsKey(ProgramOptionConstants.WORKFLOW_NAME)) {
    addWorkflowNodeState(programId,pid,systemArgs,ProgramRunStatus.STARTING,null,sourceId);
    workflowRunId=systemArgs.get(ProgramOptionConstants.WORKFLOW_RUN_ID);
  }
  ImmutableMap.Builder<String,String> builder=ImmutableMap.builder();
  builder.put(""String_Node_Str"",GSON.toJson(runtimeArgs,MAP_STRING_STRING_TYPE));
  if (workflowRunId != null) {
    builder.put(""String_Node_Str"",workflowRunId);
  }
  MDSKey key=keyBuilder.add(pid).build();
  RunRecordMeta meta=new RunRecordMeta(programId.run(pid),startTs,null,null,ProgramRunStatus.STARTING,builder.build(),systemArgs,twillRunId,sourceId);
  write(key,meta);
  return ProgramRunStatus.STARTING;
}","/** 
 * Logs initialization of program run and persists program status to   {@link ProgramRunStatus#STARTING}.
 * @param programId id of the program
 * @param pid run id
 * @param startTs initialization timestamp in seconds
 * @param twillRunId Twill run id
 * @param runtimeArgs the runtime arguments for this program run
 * @param systemArgs the system arguments for this program run
 * @param sourceId id of the source of program run status, which is proportional to the timestamp ofwhen the current program run status is reached, e.g. a message id from TMS
 * @return {@link ProgramRunStatus#STARTING} if it is successfully persisted, {@code null} otherwise.
 */
public ProgramRunStatus recordProgramStart(ProgramId programId,String pid,long startTs,String twillRunId,Map<String,String> runtimeArgs,Map<String,String> systemArgs,byte[] sourceId){
  MDSKey.Builder keyBuilder=getProgramKeyBuilder(TYPE_RUN_RECORD_STARTING,programId);
  boolean isValid=validateExistingRecord(getRun(programId,pid),programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.STARTING);
  if (!isValid) {
    return null;
  }
  String workflowRunId=null;
  if (systemArgs != null && systemArgs.containsKey(ProgramOptionConstants.WORKFLOW_NAME)) {
    addWorkflowNodeState(programId,pid,systemArgs,ProgramRunStatus.STARTING,null,sourceId);
    workflowRunId=systemArgs.get(ProgramOptionConstants.WORKFLOW_RUN_ID);
  }
  ImmutableMap.Builder<String,String> builder=ImmutableMap.builder();
  builder.put(""String_Node_Str"",GSON.toJson(runtimeArgs,MAP_STRING_STRING_TYPE));
  if (workflowRunId != null) {
    builder.put(""String_Node_Str"",workflowRunId);
  }
  MDSKey key=keyBuilder.add(pid).build();
  RunRecordMeta meta=new RunRecordMeta(programId.run(pid),startTs,null,null,ProgramRunStatus.STARTING,builder.build(),systemArgs,twillRunId,sourceId);
  write(key,meta);
  return ProgramRunStatus.STARTING;
}"
4604,"/** 
 * Logs suspend of a program run and sets the run status to   {@link ProgramRunStatus#SUSPENDED}.
 * @param programId id of the program
 * @param pid run id
 * @param sourceId id of the source of program run status, which is proportional to the timestamp ofwhen the current program run status is reached, e.g. a message id from TMS
 * @return {@link ProgramRunStatus#SUSPENDED} if it is successfully persisted, {@code null} otherwise.
 */
@Nullable public ProgramRunStatus recordProgramSuspend(ProgramId programId,String pid,byte[] sourceId){
  List<RunRecordMeta> existingRecords=getRuns(programId,pid);
  boolean isValid=validateExistingRecords(existingRecords,programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.SUSPENDED);
  if (!isValid) {
    return null;
  }
  RunRecordMeta existing=existingRecords.get(0);
  recordProgramSuspendResume(programId,pid,sourceId,existing,""String_Node_Str"");
  return ProgramRunStatus.SUSPENDED;
}","/** 
 * Logs suspend of a program run and sets the run status to   {@link ProgramRunStatus#SUSPENDED}.
 * @param programId id of the program
 * @param pid run id
 * @param sourceId id of the source of program run status, which is proportional to the timestamp ofwhen the current program run status is reached, e.g. a message id from TMS
 * @return {@link ProgramRunStatus#SUSPENDED} if it is successfully persisted, {@code null} otherwise.
 */
@Nullable public ProgramRunStatus recordProgramSuspend(ProgramId programId,String pid,byte[] sourceId){
  RunRecordMeta existing=getRun(programId,pid);
  boolean isValid=validateExistingRecord(existing,programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.SUSPENDED);
  if (!isValid) {
    return null;
  }
  recordProgramSuspendResume(programId,pid,sourceId,existing,""String_Node_Str"");
  return ProgramRunStatus.SUSPENDED;
}"
4605,"@Test public void testHistoryDeletion() throws Exception {
  ApplicationSpecification spec=Specifications.from(new AllProgramsApp());
  NamespaceId namespaceId=new NamespaceId(""String_Node_Str"");
  ApplicationId appId1=namespaceId.app(spec.getName());
  store.addApplication(appId1,spec);
  spec=Specifications.from(new WordCountApp());
  ApplicationId appId2=namespaceId.app(spec.getName());
  store.addApplication(appId2,spec);
  ProgramId flowProgramId1=appId1.flow(""String_Node_Str"");
  ProgramId mapreduceProgramId1=appId1.mr(""String_Node_Str"");
  ProgramId workflowProgramId1=appId1.workflow(""String_Node_Str"");
  ProgramId flowProgramId2=appId2.flow(""String_Node_Str"");
  Assert.assertNotNull(store.getApplication(appId1));
  Assert.assertNotNull(store.getApplication(appId2));
  long now=System.currentTimeMillis();
  setStartAndRunning(flowProgramId1,""String_Node_Str"",now - 1000);
  store.setStop(flowProgramId1,""String_Node_Str"",now,ProgramController.State.COMPLETED.getRunStatus(),AppFabricTestHelper.createSourceId(++sourceId));
  setStartAndRunning(mapreduceProgramId1,""String_Node_Str"",now - 1000);
  store.setStop(mapreduceProgramId1,""String_Node_Str"",now,ProgramController.State.COMPLETED.getRunStatus(),AppFabricTestHelper.createSourceId(++sourceId));
  RunId runId=RunIds.generate(System.currentTimeMillis() - TimeUnit.SECONDS.toMillis(1000));
  setStartAndRunning(workflowProgramId1,runId.getId(),now - 1000);
  store.setStop(workflowProgramId1,runId.getId(),now,ProgramController.State.COMPLETED.getRunStatus(),AppFabricTestHelper.createSourceId(++sourceId));
  setStartAndRunning(flowProgramId2,""String_Node_Str"",now - 1000);
  store.setStop(flowProgramId2,""String_Node_Str"",now,ProgramController.State.COMPLETED.getRunStatus(),AppFabricTestHelper.createSourceId(++sourceId));
  verifyRunHistory(flowProgramId1,1);
  verifyRunHistory(mapreduceProgramId1,1);
  verifyRunHistory(workflowProgramId1,1);
  verifyRunHistory(flowProgramId2,1);
  store.removeApplication(appId1);
  Assert.assertNull(store.getApplication(appId1));
  Assert.assertNotNull(store.getApplication(appId2));
  verifyRunHistory(flowProgramId1,0);
  verifyRunHistory(mapreduceProgramId1,0);
  verifyRunHistory(workflowProgramId1,0);
  verifyRunHistory(flowProgramId2,1);
  store.removeAll(namespaceId);
  verifyRunHistory(flowProgramId2,0);
}","@Test public void testHistoryDeletion() throws Exception {
  ApplicationSpecification spec=Specifications.from(new AllProgramsApp());
  NamespaceId namespaceId=new NamespaceId(""String_Node_Str"");
  ApplicationId appId1=namespaceId.app(spec.getName());
  store.addApplication(appId1,spec);
  spec=Specifications.from(new WordCountApp());
  ApplicationId appId2=namespaceId.app(spec.getName());
  store.addApplication(appId2,spec);
  ProgramId flowProgramId1=appId1.flow(""String_Node_Str"");
  ProgramId mapreduceProgramId1=appId1.mr(""String_Node_Str"");
  ProgramId workflowProgramId1=appId1.workflow(""String_Node_Str"");
  ProgramId flowProgramId2=appId2.flow(""String_Node_Str"");
  Assert.assertNotNull(store.getApplication(appId1));
  Assert.assertNotNull(store.getApplication(appId2));
  long now=System.currentTimeMillis();
  String runId=RunIds.generate().getId();
  setStartAndRunning(flowProgramId1,runId,now - 1000);
  store.setStop(flowProgramId1,runId,now,ProgramController.State.COMPLETED.getRunStatus(),AppFabricTestHelper.createSourceId(++sourceId));
  runId=RunIds.generate().getId();
  setStartAndRunning(mapreduceProgramId1,runId,now - 1000);
  store.setStop(mapreduceProgramId1,runId,now,ProgramController.State.COMPLETED.getRunStatus(),AppFabricTestHelper.createSourceId(++sourceId));
  runId=RunIds.generate(System.currentTimeMillis() - TimeUnit.SECONDS.toMillis(1000)).getId();
  setStartAndRunning(workflowProgramId1,runId,now - 1000);
  store.setStop(workflowProgramId1,runId,now,ProgramController.State.COMPLETED.getRunStatus(),AppFabricTestHelper.createSourceId(++sourceId));
  runId=RunIds.generate().getId();
  setStartAndRunning(flowProgramId2,runId,now - 1000);
  store.setStop(flowProgramId2,runId,now,ProgramController.State.COMPLETED.getRunStatus(),AppFabricTestHelper.createSourceId(++sourceId));
  verifyRunHistory(flowProgramId1,1);
  verifyRunHistory(mapreduceProgramId1,1);
  verifyRunHistory(workflowProgramId1,1);
  verifyRunHistory(flowProgramId2,1);
  store.removeApplication(appId1);
  Assert.assertNull(store.getApplication(appId1));
  Assert.assertNotNull(store.getApplication(appId2));
  verifyRunHistory(flowProgramId1,0);
  verifyRunHistory(mapreduceProgramId1,0);
  verifyRunHistory(workflowProgramId1,0);
  verifyRunHistory(flowProgramId2,1);
  store.removeAll(namespaceId);
  verifyRunHistory(flowProgramId2,0);
}"
4606,"@Test public void testRunsLimit() throws Exception {
  ApplicationSpecification spec=Specifications.from(new AllProgramsApp());
  ApplicationId appId=new ApplicationId(""String_Node_Str"",spec.getName());
  store.addApplication(appId,spec);
  ProgramId flowProgramId=new ProgramId(""String_Node_Str"",spec.getName(),ProgramType.FLOW,""String_Node_Str"");
  Assert.assertNotNull(store.getApplication(appId));
  long now=System.currentTimeMillis();
  setStartAndRunning(flowProgramId,""String_Node_Str"",now - 3000);
  store.setStop(flowProgramId,""String_Node_Str"",now - 100,ProgramController.State.COMPLETED.getRunStatus(),AppFabricTestHelper.createSourceId(++sourceId));
  setStartAndRunning(flowProgramId,""String_Node_Str"",now - 2000);
  Map<ProgramRunId,RunRecordMeta> historymap=store.getRuns(flowProgramId,ProgramRunStatus.ALL,0,Long.MAX_VALUE,1);
  Assert.assertEquals(1,historymap.size());
}","@Test public void testRunsLimit() throws Exception {
  ApplicationSpecification spec=Specifications.from(new AllProgramsApp());
  ApplicationId appId=new ApplicationId(""String_Node_Str"",spec.getName());
  store.addApplication(appId,spec);
  ProgramId flowProgramId=new ProgramId(""String_Node_Str"",spec.getName(),ProgramType.FLOW,""String_Node_Str"");
  Assert.assertNotNull(store.getApplication(appId));
  long now=System.currentTimeMillis();
  String runId=RunIds.generate().getId();
  setStartAndRunning(flowProgramId,runId,now - 3000);
  store.setStop(flowProgramId,runId,now - 100,ProgramController.State.COMPLETED.getRunStatus(),AppFabricTestHelper.createSourceId(++sourceId));
  runId=RunIds.generate().getId();
  setStartAndRunning(flowProgramId,runId,now - 2000);
  Map<ProgramRunId,RunRecordMeta> historymap=store.getRuns(flowProgramId,ProgramRunStatus.ALL,0,Long.MAX_VALUE,1);
  Assert.assertEquals(1,historymap.size());
}"
4607,"private ProgramRunStatus recordProgramStop(ProgramId programId,String pid,long stopTs,ProgramRunStatus runStatus,@Nullable BasicThrowable failureCause,MDSKey.Builder builder,byte[] sourceId){
  List<RunRecordMeta> existingRecords=getRuns(programId,pid);
  boolean isValid=validateExistingRecords(existingRecords,programId,pid,sourceId,runStatus.name().toLowerCase(),runStatus);
  if (!isValid) {
    return null;
  }
  RunRecordMeta existing=existingRecords.get(0);
  MDSKey key=getProgramKeyBuilder(STATUS_TYPE_MAP.get(existing.getStatus()),programId).add(pid).build();
  deleteAll(key);
  Map<String,String> systemArgs=existing.getSystemArgs();
  if (systemArgs != null && systemArgs.containsKey(ProgramOptionConstants.WORKFLOW_NAME)) {
    addWorkflowNodeState(programId,pid,systemArgs,runStatus,failureCause,sourceId);
  }
  key=builder.add(getInvertedTsKeyPart(existing.getStartTs())).add(pid).build();
  write(key,new RunRecordMeta(existing,stopTs,runStatus,sourceId));
  return runStatus;
}","private ProgramRunStatus recordProgramStop(ProgramId programId,String pid,long stopTs,ProgramRunStatus runStatus,@Nullable BasicThrowable failureCause,MDSKey.Builder builder,byte[] sourceId){
  RunRecordMeta existing=getRun(programId,pid);
  boolean isValid=validateExistingRecords(existing,programId,pid,sourceId,runStatus.name().toLowerCase(),runStatus);
  if (!isValid) {
    return null;
  }
  MDSKey key=getProgramKeyBuilder(STATUS_TYPE_MAP.get(existing.getStatus()),programId).add(pid).build();
  deleteAll(key);
  Map<String,String> systemArgs=existing.getSystemArgs();
  if (systemArgs != null && systemArgs.containsKey(ProgramOptionConstants.WORKFLOW_NAME)) {
    addWorkflowNodeState(programId,pid,systemArgs,runStatus,failureCause,sourceId);
  }
  key=builder.add(getInvertedTsKeyPart(existing.getStartTs())).add(pid).build();
  write(key,new RunRecordMeta(existing,stopTs,runStatus,sourceId));
  return runStatus;
}"
4608,"private ProgramRunStatus recordProgramRunning(ProgramId programId,String pid,long runTs,String twillRunId,MDSKey.Builder keyBuilder,byte[] sourceId){
  List<RunRecordMeta> existingRecords=getRuns(programId,pid);
  boolean isValid=validateExistingRecords(existingRecords,programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.RUNNING);
  if (!isValid) {
    return null;
  }
  RunRecordMeta existing=existingRecords.get(0);
  Map<String,String> systemArgs=existing.getSystemArgs();
  if (systemArgs != null && systemArgs.containsKey(ProgramOptionConstants.WORKFLOW_NAME)) {
    addWorkflowNodeState(programId,pid,systemArgs,ProgramRunStatus.RUNNING,null,sourceId);
  }
  MDSKey key=getProgramKeyBuilder(STATUS_TYPE_MAP.get(existing.getStatus()),programId).add(pid).build();
  deleteAll(key);
  key=keyBuilder.add(pid).build();
  RunRecordMeta meta=new RunRecordMeta(programId.run(pid),existing.getStartTs(),runTs,null,ProgramRunStatus.RUNNING,existing.getProperties(),systemArgs,twillRunId,sourceId);
  write(key,meta);
  return ProgramRunStatus.RUNNING;
}","private ProgramRunStatus recordProgramRunning(ProgramId programId,String pid,long runTs,String twillRunId,MDSKey.Builder keyBuilder,byte[] sourceId){
  RunRecordMeta existing=getRun(programId,pid);
  boolean isValid=validateExistingRecords(existing,programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.RUNNING);
  if (!isValid) {
    return null;
  }
  Map<String,String> systemArgs=existing.getSystemArgs();
  if (systemArgs != null && systemArgs.containsKey(ProgramOptionConstants.WORKFLOW_NAME)) {
    addWorkflowNodeState(programId,pid,systemArgs,ProgramRunStatus.RUNNING,null,sourceId);
  }
  MDSKey key=getProgramKeyBuilder(STATUS_TYPE_MAP.get(existing.getStatus()),programId).add(pid).build();
  deleteAll(key);
  key=keyBuilder.add(pid).build();
  RunRecordMeta meta=new RunRecordMeta(programId.run(pid),existing.getStartTs(),runTs,null,ProgramRunStatus.RUNNING,existing.getProperties(),systemArgs,twillRunId,sourceId);
  write(key,meta);
  return ProgramRunStatus.RUNNING;
}"
4609,"/** 
 * Checks whether the existing run record metas of a given program run are in a state for the program run to transition into the given run status.
 * @param existingRecords the existing run record metas of the given program run
 * @param programId id of the program
 * @param pid run id
 * @param sourceId the source id of the current program status
 * @param recordType the type of record corresponding to the current status
 * @param status the status that the program run is transitioning into
 * @return {@code true} if the program run is allowed to persist the given status, {@code false} otherwise
 */
private boolean validateExistingRecords(List<RunRecordMeta> existingRecords,ProgramId programId,String pid,byte[] sourceId,String recordType,ProgramRunStatus status){
  if (existingRecords.size() > 1) {
    LOG.warn(""String_Node_Str"" + ""String_Node_Str"",existingRecords,programId,pid,recordType);
    return false;
  }
  Set<ProgramRunStatus> allowedStatuses=ALLOWED_STATUSES.get(status);
  Set<ProgramRunStatus> allowedWithLogStatuses=ALLOWED_WITH_LOG_STATUSES.get(status);
  if (allowedStatuses == null || allowedWithLogStatuses == null) {
    LOG.error(""String_Node_Str"",status,programId,pid);
    return false;
  }
  if (allowedStatuses.isEmpty() && allowedWithLogStatuses.isEmpty()) {
    if (existingRecords.isEmpty()) {
      return true;
    }
    LOG.error(""String_Node_Str"",programId,pid,existingRecords);
    return false;
  }
  if (existingRecords.isEmpty()) {
    LOG.error(""String_Node_Str"",programId,pid,recordType);
    return false;
  }
  RunRecordMeta existing=existingRecords.get(0);
  byte[] existingSourceId=existing.getSourceId();
  if (existingSourceId != null && Bytes.compareTo(sourceId,existingSourceId) <= 0) {
    LOG.debug(""String_Node_Str"" + ""String_Node_Str"",Bytes.toHexString(sourceId),Bytes.toHexString(existingSourceId),existingRecords.get(0),recordType,programId,pid);
    return false;
  }
  ProgramRunStatus existingStatus=existing.getStatus();
  if (allowedStatuses.contains(existingStatus)) {
    return true;
  }
  if (allowedWithLogStatuses.contains(existingStatus)) {
    LOG.debug(""String_Node_Str"",existing,programId,pid,recordType);
    return true;
  }
  LOG.warn(""String_Node_Str"" + ""String_Node_Str"",existing,programId,pid,existingStatus,recordType);
  return false;
}","/** 
 * Checks whether the existing run record metas of a given program run are in a state for the program run to transition into the given run status.
 * @param existing the existing run record meta of the given program run
 * @param programId id of the program
 * @param pid run id
 * @param sourceId the source id of the current program status
 * @param recordType the type of record corresponding to the current status
 * @param status the status that the program run is transitioning into
 * @return {@code true} if the program run is allowed to persist the given status, {@code false} otherwise
 */
private boolean validateExistingRecords(RunRecordMeta existing,ProgramId programId,String pid,byte[] sourceId,String recordType,ProgramRunStatus status){
  Set<ProgramRunStatus> allowedStatuses=ALLOWED_STATUSES.get(status);
  Set<ProgramRunStatus> allowedWithLogStatuses=ALLOWED_WITH_LOG_STATUSES.get(status);
  if (allowedStatuses == null || allowedWithLogStatuses == null) {
    LOG.error(""String_Node_Str"",status,programId,pid);
    return false;
  }
  if (allowedStatuses.isEmpty() && allowedWithLogStatuses.isEmpty()) {
    if (existing == null) {
      return true;
    }
    LOG.error(""String_Node_Str"",programId,pid,existing);
    return false;
  }
  if (existing == null) {
    LOG.error(""String_Node_Str"",programId,pid,recordType);
    return false;
  }
  byte[] existingSourceId=existing.getSourceId();
  if (existingSourceId != null && Bytes.compareTo(sourceId,existingSourceId) <= 0) {
    LOG.debug(""String_Node_Str"" + ""String_Node_Str"",Bytes.toHexString(sourceId),Bytes.toHexString(existingSourceId),existing,recordType,programId,pid);
    return false;
  }
  ProgramRunStatus existingStatus=existing.getStatus();
  if (allowedStatuses.contains(existingStatus)) {
    return true;
  }
  if (allowedWithLogStatuses.contains(existingStatus)) {
    LOG.debug(""String_Node_Str"",existing,programId,pid,recordType);
    return true;
  }
  LOG.warn(""String_Node_Str"" + ""String_Node_Str"",existing,programId,pid,existingStatus,recordType);
  return false;
}"
4610,"/** 
 * Logs resume of a program run and sets the run status to   {@link ProgramRunStatus#RUNNING}.
 * @param programId id of the program
 * @param pid run id
 * @param sourceId id of the source of program run status, which is proportional to the timestamp ofwhen the current program run status is reached, e.g. a message id from TMS
 * @return {@link ProgramRunStatus#RUNNING} if it is successfully persisted, {@code null} otherwise.
 */
@Nullable public ProgramRunStatus recordProgramResumed(ProgramId programId,String pid,byte[] sourceId){
  List<RunRecordMeta> existingRecords=getRuns(programId,pid);
  boolean isValid=validateExistingRecords(existingRecords,programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.RESUMING);
  if (!isValid && !existingRecords.isEmpty()) {
    return null;
  }
  RunRecordMeta existing=null;
  if (existingRecords.isEmpty()) {
    if (!upgradeComplete.get() && programId.getVersion().equals(ApplicationId.DEFAULT_VERSION)) {
      MDSKey key=getVersionLessProgramKeyBuilder(TYPE_RUN_RECORD_SUSPENDED,programId).add(pid).build();
      existing=get(key,RunRecordMeta.class);
    }
    if (existing == null) {
      LOG.error(""String_Node_Str"",programId,pid);
      return null;
    }
  }
 else {
    existing=existingRecords.get(0);
  }
  recordProgramSuspendResume(programId,pid,sourceId,existing,""String_Node_Str"");
  return ProgramRunStatus.RUNNING;
}","/** 
 * Logs resume of a program run and sets the run status to   {@link ProgramRunStatus#RUNNING}.
 * @param programId id of the program
 * @param pid run id
 * @param sourceId id of the source of program run status, which is proportional to the timestamp ofwhen the current program run status is reached, e.g. a message id from TMS
 * @return {@link ProgramRunStatus#RUNNING} if it is successfully persisted, {@code null} otherwise.
 */
@Nullable public ProgramRunStatus recordProgramResumed(ProgramId programId,String pid,byte[] sourceId){
  RunRecordMeta existing=getRun(programId,pid);
  boolean isValid=validateExistingRecords(existing,programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.RESUMING);
  if (!isValid && existing != null) {
    return null;
  }
  if (existing == null) {
    LOG.error(""String_Node_Str"",programId,pid);
    return null;
  }
  recordProgramSuspendResume(programId,pid,sourceId,existing,""String_Node_Str"");
  return ProgramRunStatus.RUNNING;
}"
4611,"/** 
 * Logs initialization of program run and persists program status to   {@link ProgramRunStatus#STARTING}.
 * @param programId id of the program
 * @param pid run id
 * @param startTs initialization timestamp in seconds
 * @param twillRunId Twill run id
 * @param runtimeArgs the runtime arguments for this program run
 * @param systemArgs the system arguments for this program run
 * @param sourceId id of the source of program run status, which is proportional to the timestamp ofwhen the current program run status is reached, e.g. a message id from TMS
 * @return {@link ProgramRunStatus#STARTING} if it is successfully persisted, {@code null} otherwise.
 */
public ProgramRunStatus recordProgramStart(ProgramId programId,String pid,long startTs,String twillRunId,Map<String,String> runtimeArgs,Map<String,String> systemArgs,byte[] sourceId){
  MDSKey.Builder keyBuilder=getProgramKeyBuilder(TYPE_RUN_RECORD_STARTING,programId);
  boolean isValid=validateExistingRecords(getRuns(programId,pid),programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.STARTING);
  if (!isValid) {
    return null;
  }
  String workflowRunId=null;
  if (systemArgs != null && systemArgs.containsKey(ProgramOptionConstants.WORKFLOW_NAME)) {
    addWorkflowNodeState(programId,pid,systemArgs,ProgramRunStatus.STARTING,null,sourceId);
    workflowRunId=systemArgs.get(ProgramOptionConstants.WORKFLOW_RUN_ID);
  }
  ImmutableMap.Builder<String,String> builder=ImmutableMap.builder();
  builder.put(""String_Node_Str"",GSON.toJson(runtimeArgs,MAP_STRING_STRING_TYPE));
  if (workflowRunId != null) {
    builder.put(""String_Node_Str"",workflowRunId);
  }
  MDSKey key=keyBuilder.add(pid).build();
  RunRecordMeta meta=new RunRecordMeta(programId.run(pid),startTs,null,null,ProgramRunStatus.STARTING,builder.build(),systemArgs,twillRunId,sourceId);
  write(key,meta);
  return ProgramRunStatus.STARTING;
}","/** 
 * Logs initialization of program run and persists program status to   {@link ProgramRunStatus#STARTING}.
 * @param programId id of the program
 * @param pid run id
 * @param startTs initialization timestamp in seconds
 * @param twillRunId Twill run id
 * @param runtimeArgs the runtime arguments for this program run
 * @param systemArgs the system arguments for this program run
 * @param sourceId id of the source of program run status, which is proportional to the timestamp ofwhen the current program run status is reached, e.g. a message id from TMS
 * @return {@link ProgramRunStatus#STARTING} if it is successfully persisted, {@code null} otherwise.
 */
public ProgramRunStatus recordProgramStart(ProgramId programId,String pid,long startTs,String twillRunId,Map<String,String> runtimeArgs,Map<String,String> systemArgs,byte[] sourceId){
  MDSKey.Builder keyBuilder=getProgramKeyBuilder(TYPE_RUN_RECORD_STARTING,programId);
  boolean isValid=validateExistingRecords(getRun(programId,pid),programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.STARTING);
  if (!isValid) {
    return null;
  }
  String workflowRunId=null;
  if (systemArgs != null && systemArgs.containsKey(ProgramOptionConstants.WORKFLOW_NAME)) {
    addWorkflowNodeState(programId,pid,systemArgs,ProgramRunStatus.STARTING,null,sourceId);
    workflowRunId=systemArgs.get(ProgramOptionConstants.WORKFLOW_RUN_ID);
  }
  ImmutableMap.Builder<String,String> builder=ImmutableMap.builder();
  builder.put(""String_Node_Str"",GSON.toJson(runtimeArgs,MAP_STRING_STRING_TYPE));
  if (workflowRunId != null) {
    builder.put(""String_Node_Str"",workflowRunId);
  }
  MDSKey key=keyBuilder.add(pid).build();
  RunRecordMeta meta=new RunRecordMeta(programId.run(pid),startTs,null,null,ProgramRunStatus.STARTING,builder.build(),systemArgs,twillRunId,sourceId);
  write(key,meta);
  return ProgramRunStatus.STARTING;
}"
4612,"/** 
 * Logs suspend of a program run and sets the run status to   {@link ProgramRunStatus#SUSPENDED}.
 * @param programId id of the program
 * @param pid run id
 * @param sourceId id of the source of program run status, which is proportional to the timestamp ofwhen the current program run status is reached, e.g. a message id from TMS
 * @return {@link ProgramRunStatus#SUSPENDED} if it is successfully persisted, {@code null} otherwise.
 */
@Nullable public ProgramRunStatus recordProgramSuspend(ProgramId programId,String pid,byte[] sourceId){
  List<RunRecordMeta> existingRecords=getRuns(programId,pid);
  boolean isValid=validateExistingRecords(existingRecords,programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.SUSPENDED);
  if (!isValid) {
    return null;
  }
  RunRecordMeta existing=existingRecords.get(0);
  recordProgramSuspendResume(programId,pid,sourceId,existing,""String_Node_Str"");
  return ProgramRunStatus.SUSPENDED;
}","/** 
 * Logs suspend of a program run and sets the run status to   {@link ProgramRunStatus#SUSPENDED}.
 * @param programId id of the program
 * @param pid run id
 * @param sourceId id of the source of program run status, which is proportional to the timestamp ofwhen the current program run status is reached, e.g. a message id from TMS
 * @return {@link ProgramRunStatus#SUSPENDED} if it is successfully persisted, {@code null} otherwise.
 */
@Nullable public ProgramRunStatus recordProgramSuspend(ProgramId programId,String pid,byte[] sourceId){
  RunRecordMeta existing=getRun(programId,pid);
  boolean isValid=validateExistingRecords(existing,programId,pid,sourceId,""String_Node_Str"",ProgramRunStatus.SUSPENDED);
  if (!isValid) {
    return null;
  }
  recordProgramSuspendResume(programId,pid,sourceId,existing,""String_Node_Str"");
  return ProgramRunStatus.SUSPENDED;
}"
4613,"@Override public DatasetSpecification configure(String instanceName,DatasetProperties properties){
  Partitioning partitioning=PartitionedFileSetProperties.getPartitioning(properties.getProperties());
  Preconditions.checkNotNull(partitioning,""String_Node_Str"");
  DatasetProperties indexedTableProperties=DatasetProperties.builder().addAll(properties.getProperties()).add(IndexedTable.INDEX_COLUMNS_CONF_KEY,INDEXED_COLS).build();
  return DatasetSpecification.builder(instanceName,getName()).properties(properties.getProperties()).datasets(filesetDef.configure(FILESET_NAME,properties),indexedTableDef.configure(PARTITION_TABLE_NAME,indexedTableProperties)).build();
}","@Override public DatasetSpecification configure(String instanceName,DatasetProperties properties){
  Partitioning partitioning=PartitionedFileSetProperties.getPartitioning(properties.getProperties());
  Preconditions.checkNotNull(partitioning,""String_Node_Str"");
  DatasetProperties indexedTableProperties=DatasetProperties.builder().addAll(properties.getProperties()).add(IndexedTable.INDEX_COLUMNS_CONF_KEY,INDEXED_COLS).build();
  Map<String,String> pfsProperties=new HashMap<>(properties.getProperties());
  String defaultBasePathStr=properties.getProperties().get(NAME_AS_BASE_PATH_DEFAULT);
  boolean useNameAsBasePathDefault=defaultBasePathStr == null || Boolean.parseBoolean(defaultBasePathStr);
  DatasetProperties.Builder fileProperties=DatasetProperties.builder().addAll(properties.getProperties());
  if (useNameAsBasePathDefault && !properties.getProperties().containsKey(FileSetProperties.BASE_PATH)) {
    fileProperties.add(FileSetProperties.BASE_PATH,instanceName);
    pfsProperties.put(NAME_AS_BASE_PATH_DEFAULT,Boolean.TRUE.toString());
  }
  return DatasetSpecification.builder(instanceName,getName()).properties(pfsProperties).datasets(filesetDef.configure(FILESET_NAME,fileProperties.build()),indexedTableDef.configure(PARTITION_TABLE_NAME,indexedTableProperties)).build();
}"
4614,"@Override public DatasetSpecification reconfigure(String instanceName,DatasetProperties properties,DatasetSpecification currentSpec) throws IncompatibleUpdateException {
  Partitioning oldPartitioning=PartitionedFileSetProperties.getPartitioning(currentSpec.getProperties());
  Partitioning newPartitioning=PartitionedFileSetProperties.getPartitioning(properties.getProperties());
  Preconditions.checkNotNull(oldPartitioning,""String_Node_Str"");
  Preconditions.checkNotNull(newPartitioning,""String_Node_Str"");
  if (!Iterators.elementsEqual(oldPartitioning.getFields().entrySet().iterator(),newPartitioning.getFields().entrySet().iterator())) {
    throw new IncompatibleUpdateException(String.format(""String_Node_Str"",oldPartitioning,newPartitioning));
  }
  DatasetProperties indexedTableProperties=DatasetProperties.builder().addAll(properties.getProperties()).add(IndexedTable.INDEX_COLUMNS_CONF_KEY,INDEXED_COLS).build();
  return DatasetSpecification.builder(instanceName,getName()).properties(properties.getProperties()).datasets(AbstractDatasetDefinition.reconfigure(filesetDef,FILESET_NAME,properties,currentSpec.getSpecification(FILESET_NAME)),AbstractDatasetDefinition.reconfigure(indexedTableDef,PARTITION_TABLE_NAME,indexedTableProperties,currentSpec.getSpecification(PARTITION_TABLE_NAME))).build();
}","@Override public DatasetSpecification reconfigure(String instanceName,DatasetProperties properties,DatasetSpecification currentSpec) throws IncompatibleUpdateException {
  Partitioning oldPartitioning=PartitionedFileSetProperties.getPartitioning(currentSpec.getProperties());
  Partitioning newPartitioning=PartitionedFileSetProperties.getPartitioning(properties.getProperties());
  Preconditions.checkNotNull(oldPartitioning,""String_Node_Str"");
  Preconditions.checkNotNull(newPartitioning,""String_Node_Str"");
  if (!Iterators.elementsEqual(oldPartitioning.getFields().entrySet().iterator(),newPartitioning.getFields().entrySet().iterator())) {
    throw new IncompatibleUpdateException(String.format(""String_Node_Str"",oldPartitioning,newPartitioning));
  }
  Map<String,String> pfsProperties=new HashMap<>(properties.getProperties());
  DatasetProperties indexedTableProperties=DatasetProperties.builder().addAll(properties.getProperties()).add(IndexedTable.INDEX_COLUMNS_CONF_KEY,INDEXED_COLS).build();
  DatasetSpecification currentFileSpec=currentSpec.getSpecification(FILESET_NAME);
  DatasetProperties.Builder newFileProperties=DatasetProperties.builder().addAll(properties.getProperties());
  String useNameAsBasePathDefault=currentSpec.getProperty(NAME_AS_BASE_PATH_DEFAULT);
  if (Boolean.parseBoolean(useNameAsBasePathDefault) && !properties.getProperties().containsKey(FileSetProperties.BASE_PATH)) {
    newFileProperties.add(FileSetProperties.BASE_PATH,instanceName);
    pfsProperties.put(NAME_AS_BASE_PATH_DEFAULT,Boolean.TRUE.toString());
  }
  return DatasetSpecification.builder(instanceName,getName()).properties(pfsProperties).datasets(AbstractDatasetDefinition.reconfigure(filesetDef,FILESET_NAME,newFileProperties.build(),currentFileSpec),AbstractDatasetDefinition.reconfigure(indexedTableDef,PARTITION_TABLE_NAME,indexedTableProperties,currentSpec.getSpecification(PARTITION_TABLE_NAME))).build();
}"
4615,"@Test public void testPFSReconfigure(){
  DatasetDefinition pfsDef=registry.get(PartitionedFileSet.class.getName());
  Assert.assertTrue(pfsDef instanceof Reconfigurable);
  DatasetProperties props=PartitionedFileSetProperties.builder().setPartitioning(Partitioning.builder().addIntField(""String_Node_Str"").addStringField(""String_Node_Str"").build()).build();
  DatasetSpecification spec=pfsDef.configure(""String_Node_Str"",props);
  DatasetProperties noIprops=PartitionedFileSetProperties.builder().setPartitioning(Partitioning.builder().addStringField(""String_Node_Str"").build()).build();
  try {
    ((Reconfigurable)pfsDef).reconfigure(""String_Node_Str"",noIprops,spec);
    Assert.fail(""String_Node_Str"");
  }
 catch (  IncompatibleUpdateException e) {
  }
  DatasetProperties longIprops=PartitionedFileSetProperties.builder().setPartitioning(Partitioning.builder().addLongField(""String_Node_Str"").addStringField(""String_Node_Str"").build()).build();
  try {
    ((Reconfigurable)pfsDef).reconfigure(""String_Node_Str"",longIprops,spec);
    Assert.fail(""String_Node_Str"");
  }
 catch (  IncompatibleUpdateException e) {
  }
  DatasetProperties revProps=PartitionedFileSetProperties.builder().setPartitioning(Partitioning.builder().addStringField(""String_Node_Str"").addIntField(""String_Node_Str"").build()).build();
  try {
    ((Reconfigurable)pfsDef).reconfigure(""String_Node_Str"",revProps,spec);
    Assert.fail(""String_Node_Str"");
  }
 catch (  IncompatibleUpdateException e) {
  }
}","@Test public void testPFSReconfigure() throws IncompatibleUpdateException {
  DatasetDefinition pfsDef=registry.get(PartitionedFileSet.class.getName());
  Assert.assertTrue(pfsDef instanceof Reconfigurable);
  DatasetProperties props=PartitionedFileSetProperties.builder().setPartitioning(Partitioning.builder().addIntField(""String_Node_Str"").addStringField(""String_Node_Str"").build()).build();
  DatasetSpecification spec=pfsDef.configure(""String_Node_Str"",props);
  DatasetProperties noIprops=PartitionedFileSetProperties.builder().setPartitioning(Partitioning.builder().addStringField(""String_Node_Str"").build()).build();
  try {
    ((Reconfigurable)pfsDef).reconfigure(""String_Node_Str"",noIprops,spec);
    Assert.fail(""String_Node_Str"");
  }
 catch (  IncompatibleUpdateException e) {
  }
  DatasetProperties longIprops=PartitionedFileSetProperties.builder().setPartitioning(Partitioning.builder().addLongField(""String_Node_Str"").addStringField(""String_Node_Str"").build()).build();
  try {
    ((Reconfigurable)pfsDef).reconfigure(""String_Node_Str"",longIprops,spec);
    Assert.fail(""String_Node_Str"");
  }
 catch (  IncompatibleUpdateException e) {
  }
  DatasetProperties revProps=PartitionedFileSetProperties.builder().setPartitioning(Partitioning.builder().addStringField(""String_Node_Str"").addIntField(""String_Node_Str"").build()).build();
  try {
    ((Reconfigurable)pfsDef).reconfigure(""String_Node_Str"",revProps,spec);
    Assert.fail(""String_Node_Str"");
  }
 catch (  IncompatibleUpdateException e) {
  }
  DatasetProperties oldProps=PartitionedFileSetProperties.builder().setPartitioning(Partitioning.builder().addStringField(""String_Node_Str"").build()).add(PartitionedFileSetDefinition.NAME_AS_BASE_PATH_DEFAULT,""String_Node_Str"").build();
  DatasetSpecification oldSpec=pfsDef.configure(""String_Node_Str"",oldProps);
  DatasetSpecification newSpec=((Reconfigurable)pfsDef).reconfigure(""String_Node_Str"",oldProps,oldSpec);
  Assert.assertNull(newSpec.getSpecification(""String_Node_Str"").getProperty(FileSetProperties.BASE_PATH));
  props=PartitionedFileSetProperties.builder().setPartitioning(Partitioning.builder().addStringField(""String_Node_Str"").build()).build();
  oldSpec=pfsDef.configure(""String_Node_Str"",props);
  newSpec=((Reconfigurable)pfsDef).reconfigure(""String_Node_Str"",props,oldSpec);
  Assert.assertEquals(""String_Node_Str"",newSpec.getSpecification(""String_Node_Str"").getProperty(FileSetProperties.BASE_PATH));
  newSpec=((Reconfigurable)pfsDef).reconfigure(""String_Node_Str"",props,oldSpec);
  Assert.assertEquals(""String_Node_Str"",newSpec.getSpecification(""String_Node_Str"").getProperty(FileSetProperties.BASE_PATH));
}"
4616,"private void checkMutualExclusive(Map<String,String> props,String key1,String key2){
  Preconditions.checkArgument(!(Boolean.valueOf(props.get(key1)) && Boolean.valueOf(props.get(key2))),""String_Node_Str"",FileSetProperties.DATA_EXTERNAL,FileSetProperties.DATA_USE_EXISTING);
}","private void checkMutualExclusive(Map<String,String> props,String key1,String key2){
  Preconditions.checkArgument(!(Boolean.valueOf(props.get(key1)) && Boolean.valueOf(props.get(key2))),""String_Node_Str"",key1,key2);
}"
4617,"/** 
 * Split this dag into multiple dags. Each subdag will contain at most a single reduce node.
 * @return list of subdags.
 */
public List<Dag> split(){
  List<Dag> dags=new ArrayList<>();
  Set<String> remainingNodes=new HashSet<>(nodes);
  Set<String> possibleNewSources=Sets.union(sources,connectors.keySet());
  Set<String> possibleNewSinks=Sets.union(sinks,connectors.keySet());
  for (  String reduceNode : reduceNodes) {
    Dag subdag=subsetAround(reduceNode,possibleNewSources,possibleNewSinks);
    remainingNodes.removeAll(subdag.getNodes());
    dags.add(subdag);
  }
  Set<String> remainingSources=new TreeSet<>(Sets.intersection(remainingNodes,possibleNewSources));
  Set<String> processedNodes=new HashSet<>();
  Map<String,Set<String>> nodesAccessibleBySources=new HashMap<>();
  for (  String remainingSource : remainingSources) {
    Dag remainingNodesDag=subsetFrom(remainingSource,possibleNewSinks);
    nodesAccessibleBySources.put(remainingSource,remainingNodesDag.getNodes());
  }
  for (  String remainingSource : remainingSources) {
    if (!processedNodes.add(remainingSource)) {
      continue;
    }
    Set<String> subdag=new HashSet<>(nodesAccessibleBySources.get(remainingSource));
    Set<String> otherSources=Sets.difference(remainingSources,processedNodes);
    boolean nodesAdded;
    do {
      nodesAdded=false;
      for (      String otherSource : otherSources) {
        Set<String> otherAccessibleNodes=nodesAccessibleBySources.get(otherSource);
        if (!Sets.intersection(subdag,otherAccessibleNodes).isEmpty()) {
          if (subdag.addAll(otherAccessibleNodes)) {
            nodesAdded=true;
          }
        }
      }
    }
 while (nodesAdded);
    dags.add(createSubDag(subdag));
    processedNodes.addAll(subdag);
  }
  return dags;
}","/** 
 * Split this dag into multiple dags. Each subdag will contain at most a single reduce node.
 * @return list of subdags.
 */
public List<Dag> split(){
  List<Dag> dags=new ArrayList<>();
  Set<String> remainingNodes=new HashSet<>(nodes);
  Set<String> possibleNewSources=Sets.union(sources,connectors.keySet());
  Set<String> possibleNewSinks=Sets.union(sinks,connectors.keySet());
  for (  String reduceNode : reduceNodes) {
    Dag subdag=subsetAround(reduceNode,possibleNewSources,possibleNewSinks);
    Set<String> subdagConnectorSinks=Sets.intersection(subdag.getSinks(),connectors.keySet());
    remainingNodes.removeAll(Sets.difference(subdag.getNodes(),subdagConnectorSinks));
    dags.add(subdag);
  }
  Set<String> remainingSources=new TreeSet<>(Sets.intersection(remainingNodes,possibleNewSources));
  Map<String,Dag> remainingDags=new HashMap<>();
  for (  String remainingSource : remainingSources) {
    remainingDags.put(remainingSource,subsetFrom(remainingSource,possibleNewSinks));
  }
  Set<String> processedSources=new HashSet<>();
  for (  String remainingSource : remainingSources) {
    if (!processedSources.add(remainingSource)) {
      continue;
    }
    Dag subdag=remainingDags.get(remainingSource);
    Set<String> subdagNodes=new HashSet<>(subdag.getNodes());
    Set<String> nonSourceNodes=Sets.difference(subdagNodes,subdag.getSources());
    Set<String> otherSources=Sets.difference(remainingSources,processedSources);
    boolean nodesAdded;
    do {
      nodesAdded=false;
      for (      String otherSource : otherSources) {
        Dag otherSubdag=remainingDags.get(otherSource);
        Set<String> otherNonSourceNodes=Sets.difference(otherSubdag.getNodes(),otherSubdag.getSources());
        if (!Sets.intersection(nonSourceNodes,otherNonSourceNodes).isEmpty()) {
          if (subdagNodes.addAll(otherSubdag.getNodes())) {
            nodesAdded=true;
          }
        }
      }
    }
 while (nodesAdded);
    Dag mergedSubdag=createSubDag(subdagNodes);
    dags.add(mergedSubdag);
    processedSources.addAll(mergedSubdag.getSources());
  }
  return dags;
}"
4618,"/** 
 * Insert connector nodes into the dag. A connector node is a boundary at which the pipeline can be split into sub dags. It is treated as a sink within one subdag and as a source in another subdag. A connector is inserted in front of a reduce node (aggregator plugin type, etc) when there is a path from some source to one or more reduce nodes or sinks. This is required because in a single mapper, we can't write to both a sink and do a reduce. We also can't have 2 reducers in a single mapreduce job. A connector is also inserted in front of any node if the inputs into the node come from multiple sources. A connector is also inserted in front of a reduce node that has another reduce node as its input. After splitting, the result will be a collection of subdags, with each subdag representing a single mapreduce job (or possibly map-only job). Or in spark, each subdag would be a series of operations from one rdd to another rdd.
 * @return the nodes that had connectors inserted in front of them
 */
public Set<String> insertConnectors(){
  Set<String> addedAlready=new HashSet<>();
  for (  String isolationNode : isolationNodes) {
    isolate(isolationNode,addedAlready);
  }
  for (  String node : getTopologicalOrder()) {
    if (!sources.contains(node) && !connectors.containsKey(node)) {
      continue;
    }
    Set<String> accessibleByNode=accessibleFrom(node,Sets.union(connectors.keySet(),reduceNodes));
    Set<String> sinksAndReduceNodes=Sets.intersection(accessibleByNode,Sets.union(connectors.keySet(),Sets.union(sinks,reduceNodes)));
    sinksAndReduceNodes=Sets.difference(sinksAndReduceNodes,ImmutableSet.of(node));
    if (sinksAndReduceNodes.size() > 1) {
      for (      String reduceNodeConnector : Sets.intersection(sinksAndReduceNodes,reduceNodes)) {
        addConnectorInFrontOf(reduceNodeConnector,addedAlready);
      }
    }
  }
  for (  String reduceNode : reduceNodes) {
    Set<String> accessibleByNode=accessibleFrom(reduceNode,Sets.union(connectors.keySet(),reduceNodes));
    Set<String> accessibleReduceNodes=Sets.intersection(accessibleByNode,reduceNodes);
    accessibleReduceNodes=Sets.difference(accessibleReduceNodes,ImmutableSet.of(reduceNode));
    for (    String accessibleReduceNode : accessibleReduceNodes) {
      addConnectorInFrontOf(accessibleReduceNode,addedAlready);
    }
  }
  Map<Set<String>,Set<ConnectorHead>> connectorsToMerge=new HashMap<>();
  Set<String> stopNodes=Sets.union(connectors.keySet(),Sets.union(isolationNodes,reduceNodes));
  for (  String connector : connectors.keySet()) {
    List<String> branch=getBranch(connector,stopNodes);
    String branchHead=branch.iterator().next();
    Set<String> branchInputs=new HashSet<>(getNodeInputs(branchHead));
    if (branchInputs.isEmpty() || !Sets.intersection(multiPortNodes,branchInputs).isEmpty()) {
      continue;
    }
    Set<ConnectorHead> connectorsWithSameInput=connectorsToMerge.get(branchInputs);
    if (connectorsWithSameInput == null) {
      connectorsWithSameInput=new HashSet<>();
      connectorsToMerge.put(branchInputs,connectorsWithSameInput);
    }
    connectorsWithSameInput.add(new ConnectorHead(connector,branchHead));
  }
  for (  Map.Entry<Set<String>,Set<ConnectorHead>> entry : connectorsToMerge.entrySet()) {
    Set<String> branchInputs=entry.getKey();
    Set<ConnectorHead> toMerge=entry.getValue();
    if (toMerge.size() < 2) {
      continue;
    }
    List<String> binputs=new ArrayList<>(branchInputs);
    Collections.sort(binputs);
    String connectorName=getConnectorName(Joiner.on(""String_Node_Str"").join(binputs).concat(""String_Node_Str""));
    nodes.add(connectorName);
    for (    String branchInput : branchInputs) {
      addConnection(branchInput,connectorName);
    }
    connectors.put(connectorName,connectorName);
    for (    ConnectorHead connectorHead : toMerge) {
      addConnection(connectorName,connectorHead.branchHead);
      for (      String branchInput : branchInputs) {
        removeConnection(branchInput,connectorHead.branchHead);
      }
      for (      String connectorInput : getNodeInputs(connectorHead.connector)) {
        for (        String connectorOutput : getNodeOutputs(connectorHead.connector)) {
          addConnection(connectorInput,connectorOutput);
        }
      }
      removeNode(connectorHead.connector);
      connectors.remove(connectorHead.connector);
    }
  }
  return addedAlready;
}","/** 
 * Insert connector nodes into the dag. A connector node is a boundary at which the pipeline can be split into sub dags. It is treated as a sink within one subdag and as a source in another subdag. A connector is inserted in front of a reduce node (aggregator plugin type, etc) when there is a path from some source to one or more reduce nodes or sinks. This is required because in a single mapper, we can't write to both a sink and do a reduce. We also can't have 2 reducers in a single mapreduce job. A connector is also inserted in front of any node if the inputs into the node come from multiple sources. A connector is also inserted in front of a reduce node that has another reduce node as its input. After splitting, the result will be a collection of subdags, with each subdag representing a single mapreduce job (or possibly map-only job). Or in spark, each subdag would be a series of operations from one rdd to another rdd.
 * @return the nodes that had connectors inserted in front of them
 */
public Set<String> insertConnectors(){
  Set<String> addedAlready=new HashSet<>();
  for (  String isolationNode : isolationNodes) {
    isolate(isolationNode,addedAlready);
  }
  for (  String node : getTopologicalOrder()) {
    if (!sources.contains(node) && !connectors.containsKey(node)) {
      continue;
    }
    Set<String> accessibleByNode=accessibleFrom(node,Sets.union(connectors.keySet(),reduceNodes));
    Set<String> sinksAndReduceNodes=Sets.intersection(accessibleByNode,Sets.union(connectors.keySet(),Sets.union(sinks,reduceNodes)));
    sinksAndReduceNodes=Sets.difference(sinksAndReduceNodes,ImmutableSet.of(node));
    if (sinksAndReduceNodes.size() > 1) {
      for (      String reduceNodeConnector : Sets.intersection(sinksAndReduceNodes,reduceNodes)) {
        addConnectorInFrontOf(reduceNodeConnector,addedAlready);
      }
    }
  }
  for (  String reduceNode : reduceNodes) {
    Set<String> accessibleByNode=accessibleFrom(reduceNode,Sets.union(connectors.keySet(),reduceNodes));
    Set<String> accessibleReduceNodes=Sets.intersection(accessibleByNode,reduceNodes);
    accessibleReduceNodes=Sets.difference(accessibleReduceNodes,ImmutableSet.of(reduceNode));
    for (    String accessibleReduceNode : accessibleReduceNodes) {
      addConnectorInFrontOf(accessibleReduceNode,addedAlready);
    }
  }
  Map<Set<String>,Set<ConnectorHead>> connectorsToMerge=new HashMap<>();
  Set<String> stopNodes=Sets.union(connectors.keySet(),Sets.union(isolationNodes,reduceNodes));
  for (  String connector : connectors.keySet()) {
    List<String> branch=getBranch(connector,stopNodes);
    String branchHead=branch.iterator().next();
    Set<String> branchInputs=new HashSet<>(getNodeInputs(branchHead));
    if (branchInputs.isEmpty() || !Sets.intersection(multiPortNodes,branchInputs).isEmpty()) {
      continue;
    }
    Set<ConnectorHead> connectorsWithSameInput=connectorsToMerge.get(branchInputs);
    if (connectorsWithSameInput == null) {
      connectorsWithSameInput=new HashSet<>();
      connectorsToMerge.put(branchInputs,connectorsWithSameInput);
    }
    connectorsWithSameInput.add(new ConnectorHead(connector,branchHead));
  }
  for (  Map.Entry<Set<String>,Set<ConnectorHead>> entry : connectorsToMerge.entrySet()) {
    Set<String> branchInputs=entry.getKey();
    Set<ConnectorHead> toMerge=entry.getValue();
    if (toMerge.size() < 2) {
      continue;
    }
    List<String> binputs=new ArrayList<>(branchInputs);
    Collections.sort(binputs);
    String connectorName=getConnectorName(Joiner.on(""String_Node_Str"").join(binputs).concat(""String_Node_Str""));
    nodes.add(connectorName);
    for (    String branchInput : branchInputs) {
      addConnection(branchInput,connectorName);
    }
    connectors.put(connectorName,connectorName);
    for (    ConnectorHead connectorHead : toMerge) {
      addConnection(connectorName,connectorHead.branchHead);
      for (      String branchInput : branchInputs) {
        removeConnection(branchInput,connectorHead.branchHead);
      }
      for (      String connectorInput : getNodeInputs(connectorHead.connector)) {
        for (        String connectorOutput : getNodeOutputs(connectorHead.connector)) {
          addConnection(connectorInput,connectorOutput);
        }
      }
      removeNode(connectorHead.connector);
      connectors.remove(connectorHead.connector);
    }
  }
  for (  String sink : sinks) {
    Set<String> sourcesAndReduceNodes=Sets.union(connectors.keySet(),Sets.union(sources,reduceNodes));
    Set<String> parents=parentsOf(sink,sourcesAndReduceNodes);
    Set<String> parentSources=Sets.intersection(sourcesAndReduceNodes,parents);
    Set<String> reduceParents=Sets.intersection(parentSources,reduceNodes);
    if (reduceParents.size() > 0 && parentSources.size() > 1) {
      addConnectorInFrontOf(sink,addedAlready);
    }
  }
  return addedAlready;
}"
4619,"@Override public void exceptionCaught(ChannelHandlerContext ctx,Throwable cause) throws Exception {
  LOG.error(""String_Node_Str"",cause.getMessage(),cause);
  HttpResponse response=new DefaultFullHttpResponse(HttpVersion.HTTP_1_1,HttpResponseStatus.UNAUTHORIZED);
  ctx.channel().writeAndFlush(response).addListener(ChannelFutureListener.CLOSE);
}","@Override public void exceptionCaught(ChannelHandlerContext ctx,Throwable cause) throws Exception {
  LOG.error(""String_Node_Str"",cause.getMessage(),cause);
  HttpResponse response=new DefaultFullHttpResponse(HttpVersion.HTTP_1_1,HttpResponseStatus.UNAUTHORIZED);
  HttpUtil.setContentLength(response,0);
  HttpUtil.setKeepAlive(response,false);
  ctx.channel().writeAndFlush(response).addListener(ChannelFutureListener.CLOSE);
}"
4620,"@Override public void channelRead(ChannelHandlerContext ctx,Object msg) throws Exception {
  if (!(msg instanceof HttpRequest)) {
    ctx.fireChannelRead(msg);
    return;
  }
  HttpRequest request=(HttpRequest)msg;
  if (isBypassed(request)) {
    ctx.fireChannelRead(msg);
    return;
  }
  TokenState tokenState=validateAccessToken(request,ctx.channel());
  if (tokenState.isValid()) {
    ctx.fireChannelRead(msg);
    return;
  }
  try {
    HttpHeaders headers=new DefaultHttpHeaders();
    JsonObject jsonObject=new JsonObject();
    if (tokenState == TokenState.MISSING) {
      headers.add(HttpHeaderNames.WWW_AUTHENTICATE,String.format(""String_Node_Str"",realm));
      LOG.debug(""String_Node_Str"");
    }
 else {
      headers.add(HttpHeaderNames.WWW_AUTHENTICATE,String.format(""String_Node_Str"" + ""String_Node_Str"",realm,tokenState.getMsg()));
      jsonObject.addProperty(""String_Node_Str"",""String_Node_Str"");
      jsonObject.addProperty(""String_Node_Str"",tokenState.getMsg());
      LOG.debug(""String_Node_Str"",tokenState);
    }
    jsonObject.add(""String_Node_Str"",getAuthenticationURLs());
    ByteBuf content=Unpooled.copiedBuffer(jsonObject.toString(),StandardCharsets.UTF_8);
    HttpResponse response=new DefaultFullHttpResponse(HttpVersion.HTTP_1_1,HttpResponseStatus.UNAUTHORIZED,content);
    HttpUtil.setContentLength(response,content.readableBytes());
    response.headers().setAll(headers);
    response.headers().set(HttpHeaderNames.CONTENT_TYPE,""String_Node_Str"");
    auditLogIfNeeded(request,response,ctx.channel());
    ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE);
  }
  finally {
    ReferenceCountUtil.release(msg);
  }
}","@Override public void channelRead(ChannelHandlerContext ctx,Object msg) throws Exception {
  if (!(msg instanceof HttpRequest)) {
    ctx.fireChannelRead(msg);
    return;
  }
  HttpRequest request=(HttpRequest)msg;
  if (isBypassed(request)) {
    ctx.fireChannelRead(msg);
    return;
  }
  TokenState tokenState=validateAccessToken(request,ctx.channel());
  if (tokenState.isValid()) {
    ctx.fireChannelRead(msg);
    return;
  }
  try {
    HttpHeaders headers=new DefaultHttpHeaders();
    JsonObject jsonObject=new JsonObject();
    if (tokenState == TokenState.MISSING) {
      headers.add(HttpHeaderNames.WWW_AUTHENTICATE,String.format(""String_Node_Str"",realm));
      LOG.debug(""String_Node_Str"");
    }
 else {
      headers.add(HttpHeaderNames.WWW_AUTHENTICATE,String.format(""String_Node_Str"" + ""String_Node_Str"",realm,tokenState.getMsg()));
      jsonObject.addProperty(""String_Node_Str"",""String_Node_Str"");
      jsonObject.addProperty(""String_Node_Str"",tokenState.getMsg());
      LOG.debug(""String_Node_Str"",tokenState);
    }
    jsonObject.add(""String_Node_Str"",getAuthenticationURLs());
    ByteBuf content=Unpooled.copiedBuffer(jsonObject.toString(),StandardCharsets.UTF_8);
    HttpResponse response=new DefaultFullHttpResponse(HttpVersion.HTTP_1_1,HttpResponseStatus.UNAUTHORIZED,content);
    HttpUtil.setContentLength(response,content.readableBytes());
    HttpUtil.setKeepAlive(response,false);
    response.headers().setAll(headers);
    response.headers().set(HttpHeaderNames.CONTENT_TYPE,""String_Node_Str"");
    auditLogIfNeeded(request,response,ctx.channel());
    ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE);
  }
  finally {
    ReferenceCountUtil.release(msg);
  }
}"
4621,"private ChannelFutureListener getFailureResponseListener(final Channel inboundChannel){
  if (failureResponseListener == null) {
    failureResponseListener=new ChannelFutureListener(){
      @Override public void operationComplete(      ChannelFuture future) throws Exception {
        if (!future.isSuccess()) {
          inboundChannel.writeAndFlush(createErrorResponse(future.cause())).addListener(ChannelFutureListener.CLOSE);
        }
      }
    }
;
  }
  return failureResponseListener;
}","private ChannelFutureListener getFailureResponseListener(final Channel inboundChannel){
  if (failureResponseListener == null) {
    failureResponseListener=new ChannelFutureListener(){
      @Override public void operationComplete(      ChannelFuture future) throws Exception {
        if (!future.isSuccess()) {
          HttpResponse response=createErrorResponse(future.cause());
          HttpUtil.setKeepAlive(response,false);
          inboundChannel.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE);
        }
      }
    }
;
  }
  return failureResponseListener;
}"
4622,"@Override public void exceptionCaught(ChannelHandlerContext ctx,Throwable cause) throws Exception {
  HttpResponse response=cause instanceof HandlerException ? ((HandlerException)cause).createFailureResponse() : createErrorResponse(cause);
  ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE);
}","@Override public void exceptionCaught(ChannelHandlerContext ctx,Throwable cause) throws Exception {
  HttpResponse response=cause instanceof HandlerException ? ((HandlerException)cause).createFailureResponse() : createErrorResponse(cause);
  HttpUtil.setKeepAlive(response,false);
  ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE);
}"
4623,"@Override public void channelRead(ChannelHandlerContext ctx,Object msg) throws Exception {
  if (!(msg instanceof HttpRequest)) {
    ctx.fireChannelRead(msg);
    return;
  }
  try {
    HttpRequest request=(HttpRequest)msg;
    if (request.uri().equals(Constants.EndPoints.STATUS)) {
      ByteBuf content=Unpooled.copiedBuffer(Constants.Monitor.STATUS_OK,StandardCharsets.UTF_8);
      HttpResponse response=new DefaultFullHttpResponse(HttpVersion.HTTP_1_1,HttpResponseStatus.OK,content);
      HttpUtil.setContentLength(response,content.readableBytes());
      response.headers().set(HttpHeaderNames.CONTENT_TYPE,""String_Node_Str"");
      ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE);
    }
 else {
      ReferenceCountUtil.retain(msg);
      ctx.fireChannelRead(msg);
    }
  }
  finally {
    ReferenceCountUtil.release(msg);
  }
}","@Override public void channelRead(ChannelHandlerContext ctx,Object msg) throws Exception {
  if (!(msg instanceof HttpRequest)) {
    ctx.fireChannelRead(msg);
    return;
  }
  try {
    HttpRequest request=(HttpRequest)msg;
    if (request.uri().equals(Constants.EndPoints.STATUS)) {
      ByteBuf content=Unpooled.copiedBuffer(Constants.Monitor.STATUS_OK,StandardCharsets.UTF_8);
      HttpResponse response=new DefaultFullHttpResponse(HttpVersion.HTTP_1_1,HttpResponseStatus.OK,content);
      HttpUtil.setContentLength(response,content.readableBytes());
      HttpUtil.setKeepAlive(response,false);
      response.headers().set(HttpHeaderNames.CONTENT_TYPE,""String_Node_Str"");
      ctx.writeAndFlush(response).addListener(ChannelFutureListener.CLOSE);
    }
 else {
      ReferenceCountUtil.retain(msg);
      ctx.fireChannelRead(msg);
    }
  }
  finally {
    ReferenceCountUtil.release(msg);
  }
}"
4624,"@Override public void channelInactive(ChannelHandlerContext ctx) throws Exception {
  if (!closeByIdle) {
    Channels.closeOnFlush(inboundChannel);
  }
  ctx.fireChannelInactive();
}","@Override public void channelInactive(ChannelHandlerContext ctx) throws Exception {
  if (requestInProgress || !keepAlive) {
    Channels.closeOnFlush(inboundChannel);
  }
  ctx.fireChannelInactive();
}"
4625,"@Override public void userEventTriggered(ChannelHandlerContext ctx,Object evt) throws Exception {
  if (!(evt instanceof IdleStateEvent)) {
    ctx.fireUserEventTriggered(evt);
    return;
  }
  if (IdleState.ALL_IDLE == ((IdleStateEvent)evt).state()) {
    if (requestInProgress) {
      LOG.trace(""String_Node_Str"");
    }
 else {
      Channel channel=ctx.channel();
      channel.close();
      closeByIdle=true;
      LOG.trace(""String_Node_Str"" + ""String_Node_Str"",channel,channel.localAddress(),channel.remoteAddress());
    }
  }
}","@Override public void userEventTriggered(ChannelHandlerContext ctx,Object evt) throws Exception {
  if (!(evt instanceof IdleStateEvent)) {
    ctx.fireUserEventTriggered(evt);
    return;
  }
  if (IdleState.ALL_IDLE == ((IdleStateEvent)evt).state()) {
    if (requestInProgress) {
      LOG.trace(""String_Node_Str"");
    }
 else {
      Channel channel=ctx.channel();
      channel.close();
      LOG.trace(""String_Node_Str"" + ""String_Node_Str"",channel,channel.localAddress(),channel.remoteAddress());
    }
  }
}"
4626,"@Override public void write(ChannelHandlerContext ctx,Object msg,ChannelPromise promise) throws Exception {
  if (!requestInProgress) {
    requestInProgress=msg instanceof HttpObject;
  }
  ctx.write(msg,promise);
}","@Override public void write(ChannelHandlerContext ctx,Object msg,ChannelPromise promise) throws Exception {
  if (msg instanceof HttpRequest) {
    requestInProgress=true;
    keepAlive=HttpUtil.isKeepAlive((HttpRequest)msg);
  }
  ctx.write(msg,promise);
}"
4627,"@Override public void channelRead(ChannelHandlerContext ctx,Object msg) throws Exception {
  inboundChannel.write(msg);
  if (msg instanceof LastHttpContent) {
    requestInProgress=false;
  }
}","@Override public void channelRead(ChannelHandlerContext ctx,Object msg) throws Exception {
  inboundChannel.write(msg);
  if (msg instanceof HttpResponse) {
    keepAlive=HttpUtil.isKeepAlive((HttpResponse)msg);
  }
  if (msg instanceof LastHttpContent) {
    requestInProgress=false;
  }
}"
4628,"@Override public void channelWritabilityChanged(ChannelHandlerContext ctx) throws Exception {
  if (requestInProgress) {
    final Channel channel=ctx.channel();
    ctx.executor().execute(new Runnable(){
      @Override public void run(){
        if (channel.isWritable()) {
          LOG.trace(""String_Node_Str"");
          inboundChannel.config().setAutoRead(true);
        }
 else {
          LOG.trace(""String_Node_Str"");
          inboundChannel.config().setAutoRead(false);
        }
      }
    }
);
  }
  ctx.fireChannelWritabilityChanged();
}","@Override public void channelWritabilityChanged(ChannelHandlerContext ctx) throws Exception {
  if (requestInProgress) {
    final Channel channel=ctx.channel();
    ctx.executor().execute(() -> {
      if (channel.isWritable()) {
        LOG.trace(""String_Node_Str"");
        inboundChannel.config().setAutoRead(true);
      }
 else {
        LOG.trace(""String_Node_Str"");
        inboundChannel.config().setAutoRead(false);
      }
    }
);
  }
  ctx.fireChannelWritabilityChanged();
}"
4629,"@POST @Path(""String_Node_Str"") public void upload(FullHttpRequest request,HttpResponder responder) throws IOException {
  ByteBuf content=request.content();
  int readableBytes;
  ChunkResponder chunkResponder=responder.sendChunkStart(HttpResponseStatus.OK);
  while ((readableBytes=content.readableBytes()) > 0) {
    int read=Math.min(readableBytes,CHUNK_SIZE);
    chunkResponder.sendChunk(content.readRetainedSlice(read));
  }
  chunkResponder.close();
}","@POST @Path(""String_Node_Str"") public void upload(FullHttpRequest request,HttpResponder responder) throws IOException {
  numRequests.incrementAndGet();
  ByteBuf content=request.content();
  int readableBytes;
  ChunkResponder chunkResponder=responder.sendChunkStart(HttpResponseStatus.OK);
  while ((readableBytes=content.readableBytes()) > 0) {
    int read=Math.min(readableBytes,CHUNK_SIZE);
    chunkResponder.sendChunk(content.readRetainedSlice(read));
  }
  chunkResponder.close();
}"
4630,"@POST @Path(""String_Node_Str"") public BodyConsumer upload2(HttpRequest request,HttpResponder responder) throws Exception {
  return new AbstractBodyConsumer(TEMP_FOLDER.newFile()){
    @Override protected void onFinish(    HttpResponder responder,    File file) throws Exception {
      responder.sendFile(file);
    }
  }
;
}","@POST @Path(""String_Node_Str"") public BodyConsumer upload2(HttpRequest request,HttpResponder responder) throws Exception {
  numRequests.incrementAndGet();
  return new AbstractBodyConsumer(TEMP_FOLDER.newFile()){
    @Override protected void onFinish(    HttpResponder responder,    File file) throws Exception {
      responder.sendFile(file);
    }
  }
;
}"
4631,"private void checkMutualExclusive(Map<String,String> props,String key1,String key2){
  Preconditions.checkArgument(!(Boolean.valueOf(props.get(key1)) && Boolean.valueOf(props.get(key2))),""String_Node_Str"",FileSetProperties.DATA_EXTERNAL,FileSetProperties.DATA_USE_EXISTING);
}","private void checkMutualExclusive(Map<String,String> props,String key1,String key2){
  Preconditions.checkArgument(!(Boolean.valueOf(props.get(key1)) && Boolean.valueOf(props.get(key2))),""String_Node_Str"",key1,key2);
}"
4632,"@Override public void channelInactive(ChannelHandlerContext ctx) throws Exception {
  if (requestInProgress) {
    Channels.closeOnFlush(inboundChannel);
  }
  ctx.fireChannelInactive();
}","@Override public void channelInactive(ChannelHandlerContext ctx) throws Exception {
  if (!closeByIdle) {
    Channels.closeOnFlush(inboundChannel);
  }
  ctx.fireChannelInactive();
}"
4633,"@Override public void userEventTriggered(ChannelHandlerContext ctx,Object evt) throws Exception {
  if (!(evt instanceof IdleStateEvent)) {
    ctx.fireUserEventTriggered(evt);
    return;
  }
  if (IdleState.ALL_IDLE == ((IdleStateEvent)evt).state()) {
    if (requestInProgress) {
      LOG.trace(""String_Node_Str"");
    }
 else {
      Channel channel=ctx.channel();
      channel.close();
      LOG.trace(""String_Node_Str"" + ""String_Node_Str"",channel,channel.localAddress(),channel.remoteAddress());
    }
  }
}","@Override public void userEventTriggered(ChannelHandlerContext ctx,Object evt) throws Exception {
  if (!(evt instanceof IdleStateEvent)) {
    ctx.fireUserEventTriggered(evt);
    return;
  }
  if (IdleState.ALL_IDLE == ((IdleStateEvent)evt).state()) {
    if (requestInProgress) {
      LOG.trace(""String_Node_Str"");
    }
 else {
      Channel channel=ctx.channel();
      channel.close();
      closeByIdle=true;
      LOG.trace(""String_Node_Str"" + ""String_Node_Str"",channel,channel.localAddress(),channel.remoteAddress());
    }
  }
}"
4634,"@Inject TokenSecureStoreRenewer(YarnConfiguration yarnConf,CConfiguration cConf,LocationFactory locationFactory,SecureStore secureStore){
  this.yarnConf=yarnConf;
  this.locationFactory=locationFactory;
  this.secureStore=secureStore;
  this.secureExplore=cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED) && UserGroupInformation.isSecurityEnabled();
}","@Inject TokenSecureStoreRenewer(YarnConfiguration yarnConf,CConfiguration cConf,LocationFactory locationFactory,SecureStore secureStore){
  this.yarnConf=yarnConf;
  this.cConf=cConf;
  this.locationFactory=locationFactory;
  this.secureStore=secureStore;
  this.secureExplore=cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED) && UserGroupInformation.isSecurityEnabled();
}"
4635,"/** 
 * Creates a   {@link Credentials} that contains delegation tokens of the current user for all services that CDAP uses.
 */
public Credentials createCredentials(){
  try {
    Credentials refreshedCredentials=new Credentials();
    if (User.isSecurityEnabled()) {
      YarnTokenUtils.obtainToken(yarnConf,refreshedCredentials);
    }
    if (User.isHBaseSecurityEnabled(yarnConf)) {
      HBaseTokenUtils.obtainToken(yarnConf,refreshedCredentials);
    }
    if (secureExplore) {
      HiveTokenUtils.obtainToken(refreshedCredentials);
      JobHistoryServerTokenUtils.obtainToken(yarnConf,refreshedCredentials);
    }
    if (secureStore instanceof DelegationTokensUpdater) {
      String renewer=UserGroupInformation.getCurrentUser().getShortUserName();
      ((DelegationTokensUpdater)secureStore).addDelegationTokens(renewer,refreshedCredentials);
    }
    YarnUtils.addDelegationTokens(yarnConf,locationFactory,refreshedCredentials);
    return refreshedCredentials;
  }
 catch (  IOException ioe) {
    throw Throwables.propagate(ioe);
  }
}","/** 
 * Creates a   {@link Credentials} that contains delegation tokens of the current user for all services that CDAP uses.
 */
public Credentials createCredentials(){
  try {
    Credentials refreshedCredentials=new Credentials();
    if (User.isSecurityEnabled()) {
      YarnTokenUtils.obtainToken(yarnConf,refreshedCredentials);
    }
    if (User.isHBaseSecurityEnabled(yarnConf)) {
      HBaseTokenUtils.obtainToken(yarnConf,refreshedCredentials);
    }
    if (secureExplore) {
      HiveTokenUtils.obtainTokens(cConf,refreshedCredentials);
      JobHistoryServerTokenUtils.obtainToken(yarnConf,refreshedCredentials);
    }
    if (secureStore instanceof DelegationTokensUpdater) {
      String renewer=UserGroupInformation.getCurrentUser().getShortUserName();
      ((DelegationTokensUpdater)secureStore).addDelegationTokens(renewer,refreshedCredentials);
    }
    YarnUtils.addDelegationTokens(yarnConf,locationFactory,refreshedCredentials);
    return refreshedCredentials;
  }
 catch (  IOException ioe) {
    throw Throwables.propagate(ioe);
  }
}"
4636,"private WorkflowManager deployPipelineWithSchedule(String pipelineName,Engine engine,String triggeringPipelineName,ArgumentMapping key1Mapping,String expectedKey1Value,PluginPropertyMapping key2Mapping,String expectedKey2Value) throws Exception {
  String tableName=""String_Node_Str"" + pipelineName + engine;
  String sourceName=""String_Node_Str"" + pipelineName + engine;
  String sinkName=""String_Node_Str"" + pipelineName + engine;
  String key1=key1Mapping.getTarget();
  String key2=key2Mapping.getTarget();
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(tableName,""String_Node_Str"",""String_Node_Str"",String.format(""String_Node_Str"",key1)))).addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(tableName,""String_Node_Str"",""String_Node_Str"",String.format(""String_Node_Str"",key2)))).addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(sourceName))).addStage(new ETLStage(""String_Node_Str"",StringValueFilterTransform.getPlugin(""String_Node_Str"",String.format(""String_Node_Str"",key1)))).addStage(new ETLStage(""String_Node_Str"",StringValueFilterTransform.getPlugin(""String_Node_Str"",String.format(""String_Node_Str"",key2)))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(sinkName))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").setEngine(engine).build();
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationId appId=NamespaceId.DEFAULT.app(pipelineName);
  ApplicationManager appManager=deployApplication(appId.toId(),appRequest);
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  StructuredRecord recordSamuel=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordKey1Value=StructuredRecord.builder(schema).set(""String_Node_Str"",expectedKey1Value).build();
  StructuredRecord recordKey2Value=StructuredRecord.builder(schema).set(""String_Node_Str"",expectedKey2Value).build();
  DataSetManager<Table> inputManager=getDataset(sourceName);
  MockSource.writeInput(inputManager,ImmutableList.of(recordSamuel,recordKey1Value,recordKey2Value));
  String defaultNamespace=NamespaceId.DEFAULT.getNamespace();
  TriggeringPropertyMapping propertyMapping=new TriggeringPropertyMapping(ImmutableList.of(key1Mapping),ImmutableList.of(key2Mapping));
  ProgramStatusTrigger completeTrigger=new ProgramStatusTrigger(new WorkflowId(defaultNamespace,triggeringPipelineName,SmartWorkflow.NAME),ImmutableSet.of(ProgramStatus.COMPLETED));
  ScheduleId scheduleId=appId.schedule(""String_Node_Str"");
  appManager.addSchedule(new ScheduleDetail(scheduleId.getNamespace(),scheduleId.getApplication(),scheduleId.getVersion(),scheduleId.getSchedule(),""String_Node_Str"",new ScheduleProgramInfo(SchedulableProgramType.WORKFLOW,SmartWorkflow.NAME),ImmutableMap.of(SmartWorkflow.TRIGGERING_PROPERTIES_MAPPING,GSON.toJson(propertyMapping)),completeTrigger,ImmutableList.<Constraint>of(),Schedulers.JOB_QUEUE_TIMEOUT_MILLIS,null));
  appManager.enableSchedule(scheduleId);
  WorkflowManager manager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  return manager;
}","private WorkflowManager deployPipelineWithSchedule(String pipelineName,Engine engine,String triggeringPipelineName,ArgumentMapping key1Mapping,String expectedKey1Value,PluginPropertyMapping key2Mapping,String expectedKey2Value) throws Exception {
  String tableName=""String_Node_Str"" + pipelineName + engine;
  String sourceName=""String_Node_Str"" + pipelineName + engine;
  String sinkName=""String_Node_Str"" + pipelineName + engine;
  String key1=key1Mapping.getTarget();
  String key2=key2Mapping.getTarget();
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(tableName,""String_Node_Str"",""String_Node_Str"",String.format(""String_Node_Str"",key1)))).addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(tableName,""String_Node_Str"",""String_Node_Str"",String.format(""String_Node_Str"",key2)))).addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(sourceName))).addStage(new ETLStage(""String_Node_Str"",StringValueFilterTransform.getPlugin(""String_Node_Str"",String.format(""String_Node_Str"",key1)))).addStage(new ETLStage(""String_Node_Str"",StringValueFilterTransform.getPlugin(""String_Node_Str"",String.format(""String_Node_Str"",key2)))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(sinkName))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").setEngine(engine).build();
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationId appId=NamespaceId.DEFAULT.app(pipelineName);
  ApplicationManager appManager=deployApplication(appId.toId(),appRequest);
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  StructuredRecord recordSamuel=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordKey1Value=StructuredRecord.builder(schema).set(""String_Node_Str"",expectedKey1Value).build();
  StructuredRecord recordKey2Value=StructuredRecord.builder(schema).set(""String_Node_Str"",expectedKey2Value).build();
  DataSetManager<Table> inputManager=getDataset(sourceName);
  MockSource.writeInput(inputManager,ImmutableList.of(recordSamuel,recordKey1Value,recordKey2Value));
  String defaultNamespace=NamespaceId.DEFAULT.getNamespace();
  TriggeringPropertyMapping propertyMapping=new TriggeringPropertyMapping(ImmutableList.of(key1Mapping),ImmutableList.of(key2Mapping));
  ProgramStatusTrigger completeTrigger=new ProgramStatusTrigger(new WorkflowId(defaultNamespace,triggeringPipelineName,SmartWorkflow.NAME),ImmutableSet.of(ProgramStatus.COMPLETED));
  ScheduleId scheduleId=appId.schedule(""String_Node_Str"");
  appManager.addSchedule(new ScheduleDetail(scheduleId.getNamespace(),scheduleId.getApplication(),scheduleId.getVersion(),scheduleId.getSchedule(),""String_Node_Str"",new ScheduleProgramInfo(SchedulableProgramType.WORKFLOW,SmartWorkflow.NAME),ImmutableMap.of(SmartWorkflow.TRIGGERING_PROPERTIES_MAPPING,GSON.toJson(propertyMapping)),completeTrigger,ImmutableList.<Constraint>of(),Schedulers.JOB_QUEUE_TIMEOUT_MILLIS,null));
  appManager.enableSchedule(scheduleId);
  return appManager.getWorkflowManager(SmartWorkflow.NAME);
}"
4637,"/** 
 * Remove a specific node from the dag. Removing a node will remove all connections into the node and all connection coming out of the node. Removing a node will also re-compute the sources and sinks of the dag.
 * @param node the node to remove
 */
private void removeNode(String node){
  for (  String outputNode : outgoingConnections.removeAll(node)) {
    incomingConnections.remove(outputNode,node);
    if (incomingConnections.get(outputNode).isEmpty()) {
      sources.add(outputNode);
    }
  }
  for (  String inputNode : incomingConnections.removeAll(node)) {
    outgoingConnections.remove(inputNode,node);
    if (outgoingConnections.get(inputNode).isEmpty()) {
      sinks.add(inputNode);
    }
  }
  sinks.remove(node);
  sources.remove(node);
}","/** 
 * Remove a specific node from the dag. Removing a node will remove all connections into the node and all connection coming out of the node. Removing a node will also re-compute the sources and sinks of the dag.
 * @param node the node to remove
 */
private void removeNode(String node){
  for (  String outputNode : outgoingConnections.removeAll(node)) {
    incomingConnections.remove(outputNode,node);
    if (incomingConnections.get(outputNode).isEmpty()) {
      sources.add(outputNode);
    }
  }
  for (  String inputNode : incomingConnections.removeAll(node)) {
    outgoingConnections.remove(inputNode,node);
    if (outgoingConnections.get(inputNode).isEmpty()) {
      sinks.add(inputNode);
    }
  }
  sinks.remove(node);
  sources.remove(node);
  nodes.remove(node);
}"
4638,"/** 
 * Split the dag based on the input control nodes.
 * @param controlNodes set of conditions and actions based on which to split the Dag
 * @return set of splitted dags
 */
public Set<Dag> splitByControlNodes(Set<String> controlNodes){
  Set<Dag> dags=new HashSet<>();
  Set<String> childStopperNodes=Sets.union(sinks,controlNodes);
  Set<String> accessibleFromSources=accessibleFrom(sources,childStopperNodes);
  if (!controlNodes.containsAll(sources)) {
    dags.add(createSubDag(accessibleFromSources));
  }
  for (  String controlNode : controlNodes) {
    Set<String> outputs=getNodeOutputs(controlNode);
    for (    String output : outputs) {
      if (controlNodes.contains(output)) {
        dags.add(createSubDag(new HashSet<>(Arrays.asList(controlNode,output))));
        continue;
      }
      Set<String> childNodes=accessibleFrom(output,childStopperNodes);
      childNodes.add(controlNode);
      dags.add(createSubDag(childNodes));
    }
  }
  return dags;
}","/** 
 * Split the dag based on the input control nodes.
 * @param controlNodes set of conditions and actions based on which to split the Dag
 * @return set of splitted dags
 */
public Set<Dag> splitByControlNodes(Set<String> controlNodes){
  Set<Dag> dags=new HashSet<>();
  Dag copy=new Dag(this);
  Set<String> controlSources=new HashSet<>(Sets.intersection(copy.sources,controlNodes));
  while (!controlSources.isEmpty()) {
    for (    String controlSource : controlSources) {
      for (      String output : copy.getNodeOutputs(controlSource)) {
        dags.add(createSubDag(ImmutableSet.of(controlSource,output)));
      }
      copy.removeNode(controlSource);
    }
    controlSources.clear();
    controlSources.addAll(Sets.intersection(copy.sources,controlNodes));
  }
  Set<String> controlSinks=new HashSet<>(Sets.intersection(copy.sinks,controlNodes));
  while (!controlSinks.isEmpty()) {
    for (    String controlSink : controlSinks) {
      for (      String input : copy.getNodeInputs(controlSink)) {
        dags.add(createSubDag(ImmutableSet.of(input,controlSink)));
      }
      copy.removeNode(controlSink);
    }
    controlSinks.clear();
    controlSinks.addAll(Sets.intersection(copy.sources,controlNodes));
  }
  if (copy.getNodes().size() < 2) {
    return dags;
  }
  Set<String> remainingControlNodes=Sets.intersection(copy.nodes,controlNodes);
  List<Set<String>> sourceSubdags=new ArrayList<>(copy.sources.size());
  for (  String source : copy.sources) {
    sourceSubdags.add(copy.accessibleFrom(source,remainingControlNodes));
  }
  boolean done=sourceSubdags.size() <= 1;
  while (!done) {
    List<Set<String>> mergedSubdags=new ArrayList<>();
    Iterator<Set<String>> subdagIter=sourceSubdags.iterator();
    mergedSubdags.add(subdagIter.next());
    done=true;
    while (subdagIter.hasNext()) {
      Set<String> subdag=subdagIter.next();
      boolean merged=false;
      for (      Set<String> mergedSubdag : mergedSubdags) {
        if (!Sets.intersection(mergedSubdag,subdag).isEmpty()) {
          mergedSubdag.addAll(subdag);
          merged=true;
          done=false;
          break;
        }
      }
      if (!merged) {
        mergedSubdags.add(subdag);
      }
    }
    sourceSubdags=mergedSubdags;
  }
  for (  Set<String> sourceSubdag : sourceSubdags) {
    dags.add(createSubDag(sourceSubdag));
  }
  for (  String controlNode : remainingControlNodes) {
    Set<String> outputs=copy.getNodeOutputs(controlNode);
    for (    String output : outputs) {
      if (remainingControlNodes.contains(output)) {
        dags.add(createSubDag(new HashSet<>(Arrays.asList(controlNode,output))));
        continue;
      }
      Set<String> childNodes=accessibleFrom(output,remainingControlNodes);
      childNodes.add(controlNode);
      dags.add(createSubDag(childNodes));
    }
  }
  return dags;
}"
4639,"@Test public void testSplitByControlNodes() throws Exception {
  Dag dag=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Set<Dag> actual=dag.splitByControlNodes(new HashSet<>(Arrays.asList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  Set<Dag> expectedDags=new HashSet<>();
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""))));
  Assert.assertEquals(expectedDags,actual);
  dag=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  actual=dag.splitByControlNodes(new HashSet<>(Arrays.asList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  expectedDags.clear();
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  Assert.assertEquals(expectedDags,actual);
  dag=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  actual=dag.splitByControlNodes(new HashSet<>(Arrays.asList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  expectedDags.clear();
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  Assert.assertEquals(expectedDags,actual);
  dag=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  actual=dag.splitByControlNodes(new HashSet<>(Arrays.asList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  expectedDags.clear();
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  Assert.assertEquals(expectedDags,actual);
}","@Test public void testSplitByControlNodes() throws Exception {
  Dag dag=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Set<Dag> actual=dag.splitByControlNodes(new HashSet<>(Arrays.asList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  Set<Dag> expectedDags=new HashSet<>();
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  Assert.assertEquals(expectedDags,actual);
  dag=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  actual=dag.splitByControlNodes(new HashSet<>(Arrays.asList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  expectedDags.clear();
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  Assert.assertEquals(expectedDags,actual);
  dag=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  actual=dag.splitByControlNodes(new HashSet<>(Arrays.asList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  expectedDags.clear();
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  Assert.assertEquals(expectedDags,actual);
  dag=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  actual=dag.splitByControlNodes(new HashSet<>(Arrays.asList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  expectedDags.clear();
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  Assert.assertEquals(expectedDags,actual);
  dag=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  actual=dag.splitByControlNodes(ImmutableSet.of(""String_Node_Str""));
  expectedDags.clear();
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""))));
  Assert.assertEquals(expectedDags,actual);
  dag=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  actual=dag.splitByControlNodes(ImmutableSet.of(""String_Node_Str""));
  expectedDags.clear();
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""))));
  Assert.assertEquals(expectedDags,actual);
  dag=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  actual=dag.splitByControlNodes(ImmutableSet.of(""String_Node_Str"",""String_Node_Str""));
  expectedDags.clear();
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  expectedDags.add(new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""))));
  Assert.assertEquals(expectedDags,actual);
}"
4640,"@Override protected UGIWithPrincipal createUGI(ImpersonationRequest impersonationRequest) throws IOException {
  ImpersonationRequest jsonRequest=new ImpersonationRequest(impersonationRequest.getEntityId(),impersonationRequest.getImpersonatedOpType(),impersonationRequest.getPrincipal());
  PrincipalCredentials principalCredentials=GSON.fromJson(executeRequest(jsonRequest).getResponseBodyAsString(),PrincipalCredentials.class);
  LOG.debug(""String_Node_Str"",principalCredentials);
  Location location=locationFactory.create(URI.create(principalCredentials.getCredentialsPath()));
  try {
    UserGroupInformation impersonatedUGI=UserGroupInformation.createRemoteUser(principalCredentials.getPrincipal());
    impersonatedUGI.addCredentials(readCredentials(location));
    return new UGIWithPrincipal(principalCredentials.getPrincipal(),impersonatedUGI);
  }
  finally {
    try {
      if (!location.delete()) {
        LOG.warn(""String_Node_Str"",location);
      }
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",location,e);
    }
  }
}","@Override protected UGIWithPrincipal createUGI(ImpersonationRequest impersonationRequest) throws IOException {
  ImpersonationRequest jsonRequest=new ImpersonationRequest(impersonationRequest.getEntityId(),impersonationRequest.getImpersonatedOpType(),impersonationRequest.getPrincipal());
  PrincipalCredentials principalCredentials=GSON.fromJson(executeRequest(jsonRequest).getResponseBodyAsString(),PrincipalCredentials.class);
  LOG.debug(""String_Node_Str"",principalCredentials);
  Location location=locationFactory.create(URI.create(principalCredentials.getCredentialsPath()));
  try {
    String user=principalCredentials.getPrincipal();
    if (impersonationRequest.getImpersonatedOpType() == ImpersonatedOpType.EXPLORE) {
      user=new KerberosName(user).getShortName();
    }
    UserGroupInformation impersonatedUGI=UserGroupInformation.createRemoteUser(user);
    impersonatedUGI.addCredentials(readCredentials(location));
    return new UGIWithPrincipal(principalCredentials.getPrincipal(),impersonatedUGI);
  }
  finally {
    try {
      if (!location.delete()) {
        LOG.warn(""String_Node_Str"",location);
      }
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",location,e);
    }
  }
}"
4641,"@Test public void testHBaseVersionToCompatMapping() throws ParseException {
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_96,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH55,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH56,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_96,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
}","@Test public void testHBaseVersionToCompatMapping() throws ParseException {
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_96,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH55,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH56,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_96,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
}"
4642,"@VisibleForTesting static Version determineVersionFromVersionString(String versionString) throws ParseException {
  if (versionString.startsWith(HBASE_94_VERSION)) {
    return Version.HBASE_94;
  }
  if (versionString.startsWith(HBASE_96_VERSION)) {
    return Version.HBASE_96;
  }
  if (versionString.startsWith(HBASE_98_VERSION)) {
    return Version.HBASE_98;
  }
  VersionNumber ver=VersionNumber.create(versionString);
  if (versionString.startsWith(HBASE_10_VERSION)) {
    return getHBase10VersionFromVersion(ver);
  }
  if (versionString.startsWith(HBASE_11_VERSION)) {
    return Version.HBASE_11;
  }
  if (versionString.startsWith(HBASE_12_VERSION)) {
    return getHBase12VersionFromVersion(ver);
  }
  if (ver.getClassifier() != null && ver.getClassifier().startsWith(CDH_CLASSIFIER)) {
    return Version.UNKNOWN_CDH;
  }
  return Version.UNKNOWN;
}","@VisibleForTesting static Version determineVersionFromVersionString(String versionString) throws ParseException {
  if (versionString.startsWith(HBASE_94_VERSION)) {
    return Version.HBASE_94;
  }
  if (versionString.startsWith(HBASE_96_VERSION)) {
    return Version.HBASE_96;
  }
  if (versionString.startsWith(HBASE_98_VERSION)) {
    return Version.HBASE_98;
  }
  VersionNumber ver=VersionNumber.create(versionString);
  if (versionString.startsWith(HBASE_10_VERSION)) {
    return getHBase10VersionFromVersion(ver);
  }
  if (versionString.startsWith(HBASE_11_VERSION)) {
    return Version.HBASE_11;
  }
  if (versionString.startsWith(HBASE_12_VERSION)) {
    return getHBase12VersionFromVersion(ver);
  }
  boolean isCDH=ver.getClassifier() != null && ver.getClassifier().startsWith(CDH_CLASSIFIER);
  if (versionString.startsWith(HBASE_13_VERSION) && !isCDH) {
    return Version.HBASE_11;
  }
  if (isCDH) {
    return Version.UNKNOWN_CDH;
  }
  return Version.UNKNOWN;
}"
4643,"@Test public void testHBaseVersionToCompatMapping() throws ParseException {
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_96,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH55,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH56,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_96,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
}","@Test public void testHBaseVersionToCompatMapping() throws ParseException {
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_96,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH55,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH56,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_96,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
}"
4644,"private static Version getHBase12VersionFromVersion(VersionNumber ver){
  if (ver.getClassifier() != null && ver.getClassifier().startsWith(CDH_CLASSIFIER)) {
    if (ver.getClassifier().startsWith(CDH57_CLASSIFIER) || ver.getClassifier().startsWith(CDH58_CLASSIFIER) || ver.getClassifier().startsWith(CDH59_CLASSIFIER)|| ver.getClassifier().startsWith(CDH510_CLASSIFIER)|| ver.getClassifier().startsWith(CDH511_CLASSIFIER)|| ver.getClassifier().startsWith(CDH512_CLASSIFIER)) {
      return Version.HBASE_12_CDH57;
    }
    return Version.UNKNOWN_CDH;
  }
 else {
    return Version.HBASE_11;
  }
}","private static Version getHBase12VersionFromVersion(VersionNumber ver){
  if (ver.getClassifier() != null && ver.getClassifier().startsWith(CDH_CLASSIFIER)) {
    if (ver.getClassifier().startsWith(CDH57_CLASSIFIER) || ver.getClassifier().startsWith(CDH58_CLASSIFIER) || ver.getClassifier().startsWith(CDH59_CLASSIFIER)|| ver.getClassifier().startsWith(CDH510_CLASSIFIER)|| ver.getClassifier().startsWith(CDH511_CLASSIFIER)|| ver.getClassifier().startsWith(CDH512_CLASSIFIER)|| ver.getClassifier().startsWith(CDH513_CLASSIFIER)) {
      return Version.HBASE_12_CDH57;
    }
    return Version.UNKNOWN_CDH;
  }
 else {
    return Version.HBASE_11;
  }
}"
4645,"protected AccessToken fetchAccessToken(String username,String password) throws IOException, TimeoutException, InterruptedException {
  Properties properties=new Properties();
  properties.setProperty(""String_Node_Str"",username);
  properties.setProperty(""String_Node_Str"",password);
  final AuthenticationClient authClient=new BasicAuthenticationClient();
  authClient.configure(properties);
  ConnectionConfig connectionConfig=getClientConfig().getConnectionConfig();
  authClient.setConnectionInfo(connectionConfig.getHostname(),connectionConfig.getPort(),false);
  checkServicesWithRetry(new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return authClient.getAccessToken() != null;
    }
  }
,""String_Node_Str"" + connectionConfig);
  return authClient.getAccessToken();
}","protected AccessToken fetchAccessToken(String username,String password) throws IOException, TimeoutException, InterruptedException {
  Properties properties=new Properties();
  properties.setProperty(""String_Node_Str"",username);
  properties.setProperty(""String_Node_Str"",password);
  properties.setProperty(""String_Node_Str"",Boolean.toString(getClientConfig().isVerifySSLCert()));
  final AuthenticationClient authClient=new BasicAuthenticationClient();
  authClient.configure(properties);
  ConnectionConfig connectionConfig=getClientConfig().getConnectionConfig();
  authClient.setConnectionInfo(connectionConfig.getHostname(),connectionConfig.getPort(),connectionConfig.isSSLEnabled());
  checkServicesWithRetry(new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return authClient.getAccessToken() != null;
    }
  }
,""String_Node_Str"" + connectionConfig);
  return authClient.getAccessToken();
}"
4646,"@Override protected void shutDown() throws Exception {
  if (refreshService != null) {
    refreshService.interrupt();
  }
  if (storage != null) {
    storage.stop();
  }
}","@Override protected void shutDown() throws Exception {
  if (refreshService != null) {
    refreshService.interrupt();
    refreshService.join(10);
  }
  if (storage != null) {
    storage.stop();
  }
}"
4647,"@Path(""String_Node_Str"") @GET public void getPruneInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String regionName){
  if (!initializePruningDebug(responder)) {
    return;
  }
  try {
    Method method=debugClazz.getMethod(""String_Node_Str"",String.class);
    method.setAccessible(true);
    Object response=method.invoke(debugObject,regionName);
    if (response == null) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
      return;
    }
    RegionPruneInfo pruneInfo=(RegionPruneInfo)response;
    responder.sendJson(HttpResponseStatus.OK,pruneInfo);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}","@Path(""String_Node_Str"") @GET public void getPruneInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String regionName){
  try {
    if (!initializePruningDebug(responder)) {
      return;
    }
    RegionPruneInfo pruneInfo=pruningDebug.getRegionPruneInfo(regionName);
    if (pruneInfo == null) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
      return;
    }
    responder.sendJson(HttpResponseStatus.OK,pruneInfo);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}"
4648,"private boolean initializePruningDebug(HttpResponder responder){
  if (!pruneEnable) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return false;
  }
synchronized (this) {
    if (debugClazz == null || debugObject == null) {
      try {
        this.debugClazz=getClass().getClassLoader().loadClass(""String_Node_Str"");
        this.debugObject=debugClazz.newInstance();
        for (        Map.Entry<String,String> entry : cConf) {
          hConf.set(entry.getKey(),entry.getValue());
        }
        Method initMethod=debugClazz.getMethod(""String_Node_Str"",Configuration.class);
        initMethod.setAccessible(true);
        initMethod.invoke(debugObject,hConf);
      }
 catch (      ClassNotFoundException|IllegalAccessException|InstantiationException ex) {
        LOG.debug(""String_Node_Str"",ex);
        this.debugClazz=null;
      }
catch (      NoSuchMethodException|InvocationTargetException ex) {
        LOG.debug(""String_Node_Str"",ex);
        this.debugClazz=null;
      }
    }
  }
  return true;
}","private boolean initializePruningDebug(HttpResponder responder){
  if (!pruneEnable) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return false;
  }
synchronized (this) {
    if (pruningDebug != null) {
      return true;
    }
    Configuration configuration=new Configuration();
    configuration.clear();
    copyConf(configuration,hConf);
    copyConf(configuration,cConf);
    try {
      @SuppressWarnings(""String_Node_Str"") Class<? extends InvalidListPruningDebug> clazz=(Class<? extends InvalidListPruningDebug>)getClass().getClassLoader().loadClass(PRUNING_TOOL_CLASS_NAME);
      this.pruningDebug=clazz.newInstance();
      pruningDebug.initialize(configuration);
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",e);
      responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + e.getMessage());
      pruningDebug=null;
      return false;
    }
    return true;
  }
}"
4649,"@Override public void destroy(HandlerContext context){
  super.destroy(context);
synchronized (this) {
    if (debugClazz != null && debugObject != null) {
      try {
        Method destroyMethod=debugClazz.getMethod(""String_Node_Str"");
        destroyMethod.setAccessible(true);
        destroyMethod.invoke(debugObject);
      }
 catch (      NoSuchMethodException|InvocationTargetException|IllegalAccessException e) {
        LOG.debug(""String_Node_Str"",e);
      }
 finally {
        debugClazz=null;
        debugObject=null;
      }
    }
  }
}","@Override public void destroy(HandlerContext context){
  super.destroy(context);
synchronized (this) {
    if (pruningDebug != null) {
      try {
        pruningDebug.destroy();
      }
 catch (      IOException e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
}"
4650,"@Path(""String_Node_Str"") @GET public void getIdleRegions(HttpRequest request,HttpResponder responder,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numRegions){
  if (!initializePruningDebug(responder)) {
    return;
  }
  try {
    Method method=debugClazz.getMethod(""String_Node_Str"",Integer.class);
    method.setAccessible(true);
    Object response=method.invoke(debugObject,numRegions);
    Queue<RegionPruneInfo> pruneInfos=(Queue<RegionPruneInfo>)response;
    responder.sendJson(HttpResponseStatus.OK,pruneInfos);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}","@Path(""String_Node_Str"") @GET public void getIdleRegions(HttpRequest request,HttpResponder responder,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numRegions,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String time){
  try {
    if (!initializePruningDebug(responder)) {
      return;
    }
    SortedSet<? extends RegionPruneInfo> pruneInfos=pruningDebug.getIdleRegions(numRegions,time);
    responder.sendJson(HttpResponseStatus.OK,pruneInfos);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}"
4651,"@Path(""String_Node_Str"") @GET public void getTimeRegions(HttpRequest request,HttpResponder responder,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long time){
  if (!initializePruningDebug(responder)) {
    return;
  }
  try {
    Method method=debugClazz.getMethod(""String_Node_Str"",Long.class);
    method.setAccessible(true);
    Object response=method.invoke(debugObject,time);
    if (response == null) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",time));
      return;
    }
    Map<Long,SortedSet<String>> timeRegionInfo=(Map<Long,SortedSet<String>>)response;
    responder.sendJson(HttpResponseStatus.OK,timeRegionInfo);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}","@Path(""String_Node_Str"") @GET public void getTimeRegions(HttpRequest request,HttpResponder responder,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String time){
  try {
    if (!initializePruningDebug(responder)) {
      return;
    }
    RegionsAtTime timeRegionInfo=pruningDebug.getRegionsOnOrBeforeTime(time);
    responder.sendJson(HttpResponseStatus.OK,timeRegionInfo);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}"
4652,"@Path(""String_Node_Str"") @GET public void getRegionsToBeCompacted(HttpRequest request,HttpResponder responder,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numRegions){
  if (!initializePruningDebug(responder)) {
    return;
  }
  try {
    Method method=debugClazz.getMethod(""String_Node_Str"",Integer.class);
    method.setAccessible(true);
    Object response=method.invoke(debugObject,numRegions);
    Set<String> regionNames=(Set<String>)response;
    responder.sendJson(HttpResponseStatus.OK,regionNames);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}","@Path(""String_Node_Str"") @GET public void getRegionsToBeCompacted(HttpRequest request,HttpResponder responder,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numRegions,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String time){
  try {
    if (!initializePruningDebug(responder)) {
      return;
    }
    Set<String> regionNames=pruningDebug.getRegionsToBeCompacted(numRegions,time);
    responder.sendJson(HttpResponseStatus.OK,regionNames);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}"
4653,"protected static CConfiguration createBasicCConf() throws IOException {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Service.MASTER_SERVICES_BIND_ADDRESS,hostname);
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder(""String_Node_Str"").getAbsolutePath());
  cConf.set(Constants.AppFabric.OUTPUT_DIR,System.getProperty(""String_Node_Str""));
  cConf.set(Constants.AppFabric.TEMP_DIR,System.getProperty(""String_Node_Str""));
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  String updateSchedules=System.getProperty(Constants.AppFabric.APP_UPDATE_SCHEDULES);
  if (updateSchedules != null) {
    cConf.set(Constants.AppFabric.APP_UPDATE_SCHEDULES,updateSchedules);
  }
  cConf.setLong(Constants.Scheduler.EVENT_POLL_DELAY_MILLIS,100L);
  cConf.setLong(Constants.AppFabric.STATUS_EVENT_POLL_DELAY_MILLIS,100L);
  return cConf;
}","protected static CConfiguration createBasicCConf() throws IOException {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Service.MASTER_SERVICES_BIND_ADDRESS,hostname);
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder(""String_Node_Str"").getAbsolutePath());
  cConf.set(Constants.AppFabric.OUTPUT_DIR,System.getProperty(""String_Node_Str""));
  cConf.set(Constants.AppFabric.TEMP_DIR,System.getProperty(""String_Node_Str""));
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  String updateSchedules=System.getProperty(Constants.AppFabric.APP_UPDATE_SCHEDULES);
  if (updateSchedules != null) {
    cConf.set(Constants.AppFabric.APP_UPDATE_SCHEDULES,updateSchedules);
  }
  cConf.setLong(Constants.Scheduler.EVENT_POLL_DELAY_MILLIS,100L);
  cConf.setLong(Constants.AppFabric.STATUS_EVENT_POLL_DELAY_MILLIS,100L);
  cConf.setBoolean(TxConstants.TransactionPruning.PRUNE_ENABLE,true);
  return cConf;
}"
4654,"/** 
 * Tests invalidating a transaction.
 * @throws Exception
 */
@Test public void testInvalidateTx() throws Exception {
  TransactionSystemClient txClient=getTxClient();
  Transaction tx1=txClient.startShort();
  HttpResponse response=doPost(""String_Node_Str"" + tx1.getWritePointer() + ""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Transaction tx2=txClient.startShort();
  txClient.commitOrThrow(tx2);
  response=doPost(""String_Node_Str"" + tx2.getWritePointer() + ""String_Node_Str"");
  Assert.assertEquals(409,response.getStatusLine().getStatusCode());
  Assert.assertEquals(400,doPost(""String_Node_Str"").getStatusLine().getStatusCode());
}","/** 
 * Tests invalidating a transaction.
 */
@Test public void testInvalidateTx() throws Exception {
  TransactionSystemClient txClient=getTxClient();
  Transaction tx1=txClient.startShort();
  HttpResponse response=doPost(""String_Node_Str"" + tx1.getWritePointer() + ""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Transaction tx2=txClient.startShort();
  txClient.commitOrThrow(tx2);
  response=doPost(""String_Node_Str"" + tx2.getWritePointer() + ""String_Node_Str"");
  Assert.assertEquals(409,response.getStatusLine().getStatusCode());
  Assert.assertEquals(400,doPost(""String_Node_Str"").getStatusLine().getStatusCode());
}"
4655,"void publish(MessagingService messagingService) throws IOException {
  if (payloads.isEmpty()) {
    return;
  }
  int failureCount=0;
  long startTime=-1L;
  boolean done=false;
  while (!done) {
    try {
      messagingService.publish(StoreRequestBuilder.of(topicId).addPayloads(payloads.iterator()).build());
      payloads.clear();
      done=true;
    }
 catch (    TopicNotFoundException|ServiceUnavailableException e) {
      if (startTime < 0) {
        startTime=System.currentTimeMillis();
      }
      long retryMillis=getRetryStrategy().nextRetry(++failureCount,startTime);
      if (retryMillis < 0) {
        throw new IOException(""String_Node_Str"",e);
      }
      LOG.debug(""String_Node_Str"",e.getMessage(),retryMillis);
      try {
        TimeUnit.MILLISECONDS.sleep(retryMillis);
      }
 catch (      InterruptedException e1) {
        Thread.currentThread().interrupt();
        done=true;
      }
    }
  }
}","void publish(MessagingService messagingService) throws IOException {
  if (payloads.isEmpty()) {
    return;
  }
  int failureCount=0;
  long startTime=-1L;
  boolean done=false;
  boolean interrupted=false;
  while (!done) {
    try {
      interrupted=Thread.interrupted();
      messagingService.publish(StoreRequestBuilder.of(topicId).addPayloads(payloads.iterator()).build());
      payloads.clear();
      done=true;
    }
 catch (    TopicNotFoundException|ServiceUnavailableException e) {
      if (startTime < 0) {
        startTime=System.currentTimeMillis();
      }
      long retryMillis=getRetryStrategy().nextRetry(++failureCount,startTime);
      if (retryMillis < 0) {
        throw new IOException(""String_Node_Str"",e);
      }
      LOG.debug(""String_Node_Str"",e.getMessage(),retryMillis);
      if (interrupted) {
        LOG.warn(""String_Node_Str"");
        done=true;
      }
 else {
        try {
          TimeUnit.MILLISECONDS.sleep(retryMillis);
        }
 catch (        InterruptedException e1) {
          Thread.currentThread().interrupt();
          done=true;
        }
      }
    }
  }
  if (interrupted) {
    Thread.currentThread().interrupt();
  }
}"
4656,"public MapReduceMetricsWriter(Job jobConf,BasicMapReduceContext context){
  this.jobConf=jobConf;
  this.mapperMetrics=context.getProgramMetrics().childContext(Constants.Metrics.Tag.MR_TASK_TYPE,MapReduceMetrics.TaskType.Mapper.getId());
  this.reducerMetrics=context.getProgramMetrics().childContext(Constants.Metrics.Tag.MR_TASK_TYPE,MapReduceMetrics.TaskType.Reducer.getId());
  this.mapTaskMetricsCollectors=CacheBuilder.newBuilder().build(new CacheLoader<String,MetricsContext>(){
    @Override public MetricsContext load(    String taskId){
      return mapperMetrics.childContext(Constants.Metrics.Tag.INSTANCE_ID,taskId);
    }
  }
);
  this.reduceTaskMetricsCollectors=CacheBuilder.newBuilder().build(new CacheLoader<String,MetricsContext>(){
    @Override public MetricsContext load(    String taskId){
      return reducerMetrics.childContext(Constants.Metrics.Tag.INSTANCE_ID,taskId);
    }
  }
);
}","public MapReduceMetricsWriter(Job jobConf,BasicMapReduceContext context){
  this.jobConf=jobConf;
  this.mapperMetrics=context.getProgramMetrics().childContext(Constants.Metrics.Tag.MR_TASK_TYPE,MapReduceMetrics.TaskType.Mapper.getId());
  this.reducerMetrics=context.getProgramMetrics().childContext(Constants.Metrics.Tag.MR_TASK_TYPE,MapReduceMetrics.TaskType.Reducer.getId());
}"
4657,"private void reportMapredStats(Counters jobCounters) throws IOException, InterruptedException {
  JobStatus jobStatus=jobConf.getStatus();
  float mapProgress=jobStatus.getMapProgress();
  int runningMappers=0;
  int runningReducers=0;
  for (  TaskReport tr : jobConf.getTaskReports(TaskType.MAP)) {
    reportMapTaskMetrics(tr);
    runningMappers+=tr.getRunningTaskAttemptIds().size();
  }
  for (  TaskReport tr : jobConf.getTaskReports(TaskType.REDUCE)) {
    reportReduceTaskMetrics(tr);
    runningReducers+=tr.getRunningTaskAttemptIds().size();
  }
  int memoryPerMapper=jobConf.getConfiguration().getInt(Job.MAP_MEMORY_MB,Job.DEFAULT_MAP_MEMORY_MB);
  int memoryPerReducer=jobConf.getConfiguration().getInt(Job.REDUCE_MEMORY_MB,Job.DEFAULT_REDUCE_MEMORY_MB);
  long mapInputRecords=getTaskCounter(jobCounters,TaskCounter.MAP_INPUT_RECORDS);
  long mapOutputRecords=getTaskCounter(jobCounters,TaskCounter.MAP_OUTPUT_RECORDS);
  long mapOutputBytes=getTaskCounter(jobCounters,TaskCounter.MAP_OUTPUT_BYTES);
  mapperMetrics.gauge(MapReduceMetrics.METRIC_COMPLETION,(long)(mapProgress * 100));
  mapperMetrics.gauge(MapReduceMetrics.METRIC_INPUT_RECORDS,mapInputRecords);
  mapperMetrics.gauge(MapReduceMetrics.METRIC_OUTPUT_RECORDS,mapOutputRecords);
  mapperMetrics.gauge(MapReduceMetrics.METRIC_BYTES,mapOutputBytes);
  mapperMetrics.gauge(MapReduceMetrics.METRIC_USED_CONTAINERS,runningMappers);
  mapperMetrics.gauge(MapReduceMetrics.METRIC_USED_MEMORY,runningMappers * memoryPerMapper);
  LOG.trace(""String_Node_Str"",(int)(mapProgress * 100),runningMappers,runningMappers * memoryPerMapper);
  float reduceProgress=jobStatus.getReduceProgress();
  long reduceInputRecords=getTaskCounter(jobCounters,TaskCounter.REDUCE_INPUT_RECORDS);
  long reduceOutputRecords=getTaskCounter(jobCounters,TaskCounter.REDUCE_OUTPUT_RECORDS);
  reducerMetrics.gauge(MapReduceMetrics.METRIC_COMPLETION,(long)(reduceProgress * 100));
  reducerMetrics.gauge(MapReduceMetrics.METRIC_INPUT_RECORDS,reduceInputRecords);
  reducerMetrics.gauge(MapReduceMetrics.METRIC_OUTPUT_RECORDS,reduceOutputRecords);
  reducerMetrics.gauge(MapReduceMetrics.METRIC_USED_CONTAINERS,runningReducers);
  reducerMetrics.gauge(MapReduceMetrics.METRIC_USED_MEMORY,runningReducers * memoryPerReducer);
  LOG.trace(""String_Node_Str"",(int)(reduceProgress * 100),runningReducers,runningReducers * memoryPerReducer);
}","private void reportMapredStats(Counters jobCounters) throws IOException, InterruptedException {
  JobStatus jobStatus=jobConf.getStatus();
  float mapProgress=jobStatus.getMapProgress();
  int runningMappers=0;
  int runningReducers=0;
  for (  TaskReport tr : jobConf.getTaskReports(TaskType.MAP)) {
    runningMappers+=tr.getRunningTaskAttemptIds().size();
  }
  for (  TaskReport tr : jobConf.getTaskReports(TaskType.REDUCE)) {
    runningReducers+=tr.getRunningTaskAttemptIds().size();
  }
  int memoryPerMapper=jobConf.getConfiguration().getInt(Job.MAP_MEMORY_MB,Job.DEFAULT_MAP_MEMORY_MB);
  int memoryPerReducer=jobConf.getConfiguration().getInt(Job.REDUCE_MEMORY_MB,Job.DEFAULT_REDUCE_MEMORY_MB);
  long mapInputRecords=getTaskCounter(jobCounters,TaskCounter.MAP_INPUT_RECORDS);
  long mapOutputRecords=getTaskCounter(jobCounters,TaskCounter.MAP_OUTPUT_RECORDS);
  long mapOutputBytes=getTaskCounter(jobCounters,TaskCounter.MAP_OUTPUT_BYTES);
  mapperMetrics.gauge(MapReduceMetrics.METRIC_COMPLETION,(long)(mapProgress * 100));
  mapperMetrics.gauge(MapReduceMetrics.METRIC_INPUT_RECORDS,mapInputRecords);
  mapperMetrics.gauge(MapReduceMetrics.METRIC_OUTPUT_RECORDS,mapOutputRecords);
  mapperMetrics.gauge(MapReduceMetrics.METRIC_BYTES,mapOutputBytes);
  mapperMetrics.gauge(MapReduceMetrics.METRIC_USED_CONTAINERS,runningMappers);
  mapperMetrics.gauge(MapReduceMetrics.METRIC_USED_MEMORY,runningMappers * memoryPerMapper);
  LOG.trace(""String_Node_Str"",(int)(mapProgress * 100),runningMappers,runningMappers * memoryPerMapper);
  float reduceProgress=jobStatus.getReduceProgress();
  long reduceInputRecords=getTaskCounter(jobCounters,TaskCounter.REDUCE_INPUT_RECORDS);
  long reduceOutputRecords=getTaskCounter(jobCounters,TaskCounter.REDUCE_OUTPUT_RECORDS);
  reducerMetrics.gauge(MapReduceMetrics.METRIC_COMPLETION,(long)(reduceProgress * 100));
  reducerMetrics.gauge(MapReduceMetrics.METRIC_INPUT_RECORDS,reduceInputRecords);
  reducerMetrics.gauge(MapReduceMetrics.METRIC_OUTPUT_RECORDS,reduceOutputRecords);
  reducerMetrics.gauge(MapReduceMetrics.METRIC_USED_CONTAINERS,runningReducers);
  reducerMetrics.gauge(MapReduceMetrics.METRIC_USED_MEMORY,runningReducers * memoryPerReducer);
  LOG.trace(""String_Node_Str"",(int)(reduceProgress * 100),runningReducers,runningReducers * memoryPerReducer);
}"
4658,"private WrappedMapper.Context createAutoFlushingContext(final Context context,final BasicMapReduceTaskContext basicMapReduceContext){
  final int flushFreq=context.getConfiguration().getInt(""String_Node_Str"",10000);
  @SuppressWarnings(""String_Node_Str"") WrappedMapper.Context flushingContext=new WrappedMapper().new Context(context){
    private int processedRecords=0;
    @Override public boolean nextKeyValue() throws IOException, InterruptedException {
      boolean result=super.nextKeyValue();
      if (++processedRecords > flushFreq) {
        try {
          LOG.trace(""String_Node_Str"");
          basicMapReduceContext.flushOperations();
        }
 catch (        Exception e) {
          LOG.error(""String_Node_Str"",e);
          throw Throwables.propagate(e);
        }
        processedRecords=0;
      }
      return result;
    }
    @Override public InputSplit getInputSplit(){
      InputSplit inputSplit=super.getInputSplit();
      if (inputSplit instanceof TaggedInputSplit) {
        inputSplit=((TaggedInputSplit)inputSplit).getInputSplit();
      }
      return inputSplit;
    }
    @Override public Class<? extends InputFormat<?,?>> getInputFormatClass() throws ClassNotFoundException {
      InputSplit inputSplit=super.getInputSplit();
      if (inputSplit instanceof MultiInputTaggedSplit) {
        return ((MultiInputTaggedSplit)inputSplit).getInputFormatClass();
      }
      return super.getInputFormatClass();
    }
  }
;
  return flushingContext;
}","private WrappedMapper.Context createAutoFlushingContext(final Context context,final BasicMapReduceTaskContext basicMapReduceContext,final MapTaskMetricsWriter metricsWriter){
  final int flushFreq=context.getConfiguration().getInt(""String_Node_Str"",10000);
  final long reportIntervalInMillis=basicMapReduceContext.getMetricsReportIntervalMillis();
  @SuppressWarnings(""String_Node_Str"") WrappedMapper.Context flushingContext=new WrappedMapper().new Context(context){
    private int processedRecords=0;
    private long nextTimeToReportMetrics=0L;
    @Override public boolean nextKeyValue() throws IOException, InterruptedException {
      boolean result=super.nextKeyValue();
      if (++processedRecords > flushFreq) {
        try {
          LOG.trace(""String_Node_Str"");
          basicMapReduceContext.flushOperations();
        }
 catch (        Exception e) {
          LOG.error(""String_Node_Str"",e);
          throw Throwables.propagate(e);
        }
        processedRecords=0;
      }
      if (System.currentTimeMillis() >= nextTimeToReportMetrics) {
        metricsWriter.reportMetrics();
        nextTimeToReportMetrics=System.currentTimeMillis() + reportIntervalInMillis;
      }
      return result;
    }
    @Override public InputSplit getInputSplit(){
      InputSplit inputSplit=super.getInputSplit();
      if (inputSplit instanceof TaggedInputSplit) {
        inputSplit=((TaggedInputSplit)inputSplit).getInputSplit();
      }
      return inputSplit;
    }
    @Override public Class<? extends InputFormat<?,?>> getInputFormatClass() throws ClassNotFoundException {
      InputSplit inputSplit=super.getInputSplit();
      if (inputSplit instanceof MultiInputTaggedSplit) {
        return ((MultiInputTaggedSplit)inputSplit).getInputFormatClass();
      }
      return super.getInputFormatClass();
    }
  }
;
  return flushingContext;
}"
4659,"@Override public boolean nextKeyValue() throws IOException, InterruptedException {
  boolean result=super.nextKeyValue();
  if (++processedRecords > flushFreq) {
    try {
      LOG.trace(""String_Node_Str"");
      basicMapReduceContext.flushOperations();
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",e);
      throw Throwables.propagate(e);
    }
    processedRecords=0;
  }
  return result;
}","@Override public boolean nextKeyValue() throws IOException, InterruptedException {
  boolean result=super.nextKeyValue();
  if (++processedRecords > flushFreq) {
    try {
      LOG.trace(""String_Node_Str"");
      basicMapReduceContext.flushOperations();
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",e);
      throw Throwables.propagate(e);
    }
    processedRecords=0;
  }
  if (System.currentTimeMillis() >= nextTimeToReportMetrics) {
    metricsWriter.reportMetrics();
    nextTimeToReportMetrics=System.currentTimeMillis() + reportIntervalInMillis;
  }
  return result;
}"
4660,"@SuppressWarnings(""String_Node_Str"") @Override public void run(Context context) throws IOException, InterruptedException {
  MapReduceClassLoader classLoader=MapReduceClassLoader.getFromConfiguration(context.getConfiguration());
  ClassLoader weakReferenceClassLoader=new WeakReferenceDelegatorClassLoader(classLoader);
  BasicMapReduceTaskContext basicMapReduceContext=classLoader.getTaskContextProvider().get(context);
  String program=basicMapReduceContext.getProgramName();
  WrappedMapper.Context flushingContext=createAutoFlushingContext(context,basicMapReduceContext);
  basicMapReduceContext.setHadoopContext(flushingContext);
  InputSplit inputSplit=context.getInputSplit();
  if (inputSplit instanceof MultiInputTaggedSplit) {
    basicMapReduceContext.setInputContext(InputContexts.create((MultiInputTaggedSplit)inputSplit));
  }
  ClassLoader programClassLoader=classLoader.getProgramClassLoader();
  Mapper delegate=createMapperInstance(programClassLoader,getWrappedMapper(context.getConfiguration()),context,program);
  try {
    Reflections.visit(delegate,delegate.getClass(),new PropertyFieldSetter(basicMapReduceContext.getSpecification().getProperties()),new MetricsFieldSetter(basicMapReduceContext.getMetrics()),new DataSetFieldSetter(basicMapReduceContext));
  }
 catch (  Throwable t) {
    Throwable rootCause=Throwables.getRootCause(t);
    USERLOG.error(""String_Node_Str"",program,rootCause.getMessage(),rootCause);
    throw new IOException(String.format(""String_Node_Str"",delegate.getClass()),t);
  }
  ClassLoader oldClassLoader;
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(weakReferenceClassLoader);
    try {
      ((ProgramLifecycle)delegate).initialize(new MapReduceLifecycleContext(basicMapReduceContext));
    }
 catch (    Exception e) {
      Throwable rootCause=Throwables.getRootCause(e);
      USERLOG.error(""String_Node_Str"" + ""String_Node_Str"",program,rootCause.getMessage(),rootCause);
      throw new IOException(String.format(""String_Node_Str"",basicMapReduceContext),e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
  oldClassLoader=ClassLoaders.setContextClassLoader(weakReferenceClassLoader);
  try {
    delegate.run(flushingContext);
  }
  finally {
    ClassLoaders.setContextClassLoader(oldClassLoader);
  }
  try {
    basicMapReduceContext.flushOperations();
  }
 catch (  Exception e) {
    throw new IOException(""String_Node_Str"" + basicMapReduceContext,e);
  }
  basicMapReduceContext.closeMultiOutputs();
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(weakReferenceClassLoader);
    try {
      ((ProgramLifecycle<? extends RuntimeContext>)delegate).destroy();
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
}","@SuppressWarnings(""String_Node_Str"") @Override public void run(Context context) throws IOException, InterruptedException {
  MapReduceClassLoader classLoader=MapReduceClassLoader.getFromConfiguration(context.getConfiguration());
  ClassLoader weakReferenceClassLoader=new WeakReferenceDelegatorClassLoader(classLoader);
  BasicMapReduceTaskContext basicMapReduceContext=classLoader.getTaskContextProvider().get(context);
  String program=basicMapReduceContext.getProgramName();
  final MapTaskMetricsWriter mapTaskMetricsWriter=new MapTaskMetricsWriter(basicMapReduceContext.getProgramMetrics(),context);
  WrappedMapper.Context flushingContext=createAutoFlushingContext(context,basicMapReduceContext,mapTaskMetricsWriter);
  basicMapReduceContext.setHadoopContext(flushingContext);
  InputSplit inputSplit=context.getInputSplit();
  if (inputSplit instanceof MultiInputTaggedSplit) {
    basicMapReduceContext.setInputContext(InputContexts.create((MultiInputTaggedSplit)inputSplit));
  }
  ClassLoader programClassLoader=classLoader.getProgramClassLoader();
  Mapper delegate=createMapperInstance(programClassLoader,getWrappedMapper(context.getConfiguration()),context,program);
  try {
    Reflections.visit(delegate,delegate.getClass(),new PropertyFieldSetter(basicMapReduceContext.getSpecification().getProperties()),new MetricsFieldSetter(basicMapReduceContext.getMetrics()),new DataSetFieldSetter(basicMapReduceContext));
  }
 catch (  Throwable t) {
    Throwable rootCause=Throwables.getRootCause(t);
    USERLOG.error(""String_Node_Str"",program,rootCause.getMessage(),rootCause);
    throw new IOException(String.format(""String_Node_Str"",delegate.getClass()),t);
  }
  ClassLoader oldClassLoader;
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(weakReferenceClassLoader);
    try {
      ((ProgramLifecycle)delegate).initialize(new MapReduceLifecycleContext(basicMapReduceContext));
    }
 catch (    Exception e) {
      Throwable rootCause=Throwables.getRootCause(e);
      USERLOG.error(""String_Node_Str"" + ""String_Node_Str"",program,rootCause.getMessage(),rootCause);
      throw new IOException(String.format(""String_Node_Str"",basicMapReduceContext),e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
  oldClassLoader=ClassLoaders.setContextClassLoader(weakReferenceClassLoader);
  try {
    delegate.run(flushingContext);
  }
  finally {
    ClassLoaders.setContextClassLoader(oldClassLoader);
  }
  try {
    basicMapReduceContext.flushOperations();
  }
 catch (  Exception e) {
    throw new IOException(""String_Node_Str"" + basicMapReduceContext,e);
  }
  basicMapReduceContext.closeMultiOutputs();
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(weakReferenceClassLoader);
    try {
      ((ProgramLifecycle<? extends RuntimeContext>)delegate).destroy();
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
  mapTaskMetricsWriter.reportMetrics();
}"
4661,"private WrappedReducer.Context createAutoFlushingContext(final Context context,final BasicMapReduceTaskContext basicMapReduceContext){
  final int flushFreq=context.getConfiguration().getInt(""String_Node_Str"",10000);
  @SuppressWarnings(""String_Node_Str"") WrappedReducer.Context flushingContext=new WrappedReducer().new Context(context){
    private int processedRecords=0;
    @Override public boolean nextKeyValue() throws IOException, InterruptedException {
      boolean result=super.nextKey();
      if (++processedRecords > flushFreq) {
        try {
          LOG.trace(""String_Node_Str"");
          basicMapReduceContext.flushOperations();
        }
 catch (        Exception e) {
          LOG.error(""String_Node_Str"",e);
          throw Throwables.propagate(e);
        }
        processedRecords=0;
      }
      return result;
    }
  }
;
  return flushingContext;
}","private WrappedReducer.Context createAutoFlushingContext(final Context context,final BasicMapReduceTaskContext basicMapReduceContext,final ReduceTaskMetricsWriter metricsWriter){
  final int flushFreq=context.getConfiguration().getInt(""String_Node_Str"",10000);
  final long reportIntervalInMillis=basicMapReduceContext.getMetricsReportIntervalMillis();
  @SuppressWarnings(""String_Node_Str"") WrappedReducer.Context flushingContext=new WrappedReducer().new Context(context){
    private int processedRecords=0;
    private long nextTimeToReportMetrics=0L;
    @Override public boolean nextKeyValue() throws IOException, InterruptedException {
      boolean result=super.nextKey();
      if (++processedRecords > flushFreq) {
        try {
          LOG.trace(""String_Node_Str"");
          basicMapReduceContext.flushOperations();
        }
 catch (        Exception e) {
          LOG.error(""String_Node_Str"",e);
          throw Throwables.propagate(e);
        }
        processedRecords=0;
      }
      if (System.currentTimeMillis() >= nextTimeToReportMetrics) {
        metricsWriter.reportMetrics();
        nextTimeToReportMetrics=System.currentTimeMillis() + reportIntervalInMillis;
      }
      return result;
    }
  }
;
  return flushingContext;
}"
4662,"@Override public boolean nextKeyValue() throws IOException, InterruptedException {
  boolean result=super.nextKey();
  if (++processedRecords > flushFreq) {
    try {
      LOG.trace(""String_Node_Str"");
      basicMapReduceContext.flushOperations();
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",e);
      throw Throwables.propagate(e);
    }
    processedRecords=0;
  }
  return result;
}","@Override public boolean nextKeyValue() throws IOException, InterruptedException {
  boolean result=super.nextKey();
  if (++processedRecords > flushFreq) {
    try {
      LOG.trace(""String_Node_Str"");
      basicMapReduceContext.flushOperations();
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",e);
      throw Throwables.propagate(e);
    }
    processedRecords=0;
  }
  if (System.currentTimeMillis() >= nextTimeToReportMetrics) {
    metricsWriter.reportMetrics();
    nextTimeToReportMetrics=System.currentTimeMillis() + reportIntervalInMillis;
  }
  return result;
}"
4663,"@SuppressWarnings(""String_Node_Str"") @Override public void run(Context context) throws IOException, InterruptedException {
  MapReduceClassLoader classLoader=MapReduceClassLoader.getFromConfiguration(context.getConfiguration());
  ClassLoader weakReferenceClassLoader=new WeakReferenceDelegatorClassLoader(classLoader);
  BasicMapReduceTaskContext basicMapReduceContext=classLoader.getTaskContextProvider().get(context);
  WrappedReducer.Context flushingContext=createAutoFlushingContext(context,basicMapReduceContext);
  basicMapReduceContext.setHadoopContext(flushingContext);
  String userReducer=context.getConfiguration().get(ATTR_REDUCER_CLASS);
  ClassLoader programClassLoader=classLoader.getProgramClassLoader();
  Reducer delegate=createReducerInstance(programClassLoader,userReducer);
  try {
    Reflections.visit(delegate,delegate.getClass(),new PropertyFieldSetter(basicMapReduceContext.getSpecification().getProperties()),new MetricsFieldSetter(basicMapReduceContext.getMetrics()),new DataSetFieldSetter(basicMapReduceContext));
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",delegate.getClass(),t);
    throw Throwables.propagate(t);
  }
  ClassLoader oldClassLoader;
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(weakReferenceClassLoader);
    try {
      ((ProgramLifecycle)delegate).initialize(new MapReduceLifecycleContext(basicMapReduceContext));
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
      throw Throwables.propagate(e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
  oldClassLoader=ClassLoaders.setContextClassLoader(weakReferenceClassLoader);
  try {
    delegate.run(flushingContext);
  }
  finally {
    ClassLoaders.setContextClassLoader(oldClassLoader);
  }
  try {
    basicMapReduceContext.flushOperations();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"" + basicMapReduceContext,e);
    throw Throwables.propagate(e);
  }
  basicMapReduceContext.closeMultiOutputs();
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(weakReferenceClassLoader);
    try {
      ((ProgramLifecycle<? extends RuntimeContext>)delegate).destroy();
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
}","@SuppressWarnings(""String_Node_Str"") @Override public void run(Context context) throws IOException, InterruptedException {
  MapReduceClassLoader classLoader=MapReduceClassLoader.getFromConfiguration(context.getConfiguration());
  ClassLoader weakReferenceClassLoader=new WeakReferenceDelegatorClassLoader(classLoader);
  BasicMapReduceTaskContext basicMapReduceContext=classLoader.getTaskContextProvider().get(context);
  long metricsReportInterval=basicMapReduceContext.getMetricsReportIntervalMillis();
  final ReduceTaskMetricsWriter reduceTaskMetricsWriter=new ReduceTaskMetricsWriter(basicMapReduceContext.getProgramMetrics(),context);
  WrappedReducer.Context flushingContext=createAutoFlushingContext(context,basicMapReduceContext,reduceTaskMetricsWriter);
  basicMapReduceContext.setHadoopContext(flushingContext);
  String userReducer=context.getConfiguration().get(ATTR_REDUCER_CLASS);
  ClassLoader programClassLoader=classLoader.getProgramClassLoader();
  Reducer delegate=createReducerInstance(programClassLoader,userReducer);
  try {
    Reflections.visit(delegate,delegate.getClass(),new PropertyFieldSetter(basicMapReduceContext.getSpecification().getProperties()),new MetricsFieldSetter(basicMapReduceContext.getMetrics()),new DataSetFieldSetter(basicMapReduceContext));
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",delegate.getClass(),t);
    throw Throwables.propagate(t);
  }
  ClassLoader oldClassLoader;
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(weakReferenceClassLoader);
    try {
      ((ProgramLifecycle)delegate).initialize(new MapReduceLifecycleContext(basicMapReduceContext));
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
      throw Throwables.propagate(e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
  oldClassLoader=ClassLoaders.setContextClassLoader(weakReferenceClassLoader);
  try {
    delegate.run(flushingContext);
  }
  finally {
    ClassLoaders.setContextClassLoader(oldClassLoader);
  }
  try {
    basicMapReduceContext.flushOperations();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"" + basicMapReduceContext,e);
    throw Throwables.propagate(e);
  }
  basicMapReduceContext.closeMultiOutputs();
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(weakReferenceClassLoader);
    try {
      ((ProgramLifecycle<? extends RuntimeContext>)delegate).destroy();
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
  reduceTaskMetricsWriter.reportMetrics();
}"
4664,"@Inject public DistributedSchedulerService(TimeScheduler timeScheduler,StreamSizeScheduler streamSizeScheduler,Store store){
  super(timeScheduler,streamSizeScheduler,store);
  this.serviceDelegate=new RetryOnStartFailureService(new Supplier<Service>(){
    @Override public Service get(){
      return new AbstractService(){
        @Override protected void doStart(){
          try {
            startSchedulers();
            notifyStarted();
          }
 catch (          ServiceUnavailableException e) {
            LOG.debug(""String_Node_Str"",e.getMessage());
            notifyFailed(e);
          }
catch (          SchedulerException e) {
            LOG.warn(""String_Node_Str"",e);
            notifyFailed(e);
          }
        }
        @Override protected void doStop(){
          try {
            stopScheduler();
            notifyStopped();
          }
 catch (          SchedulerException e) {
            notifyFailed(e);
          }
        }
      }
;
    }
  }
,RetryStrategies.exponentialDelay(200,5000,TimeUnit.MILLISECONDS));
}","@Inject public DistributedSchedulerService(TimeScheduler timeScheduler,StreamSizeScheduler streamSizeScheduler,Store store){
  super(timeScheduler,streamSizeScheduler,store);
  this.startUpLatch=new CountDownLatch(1);
  this.serviceDelegate=new RetryOnStartFailureService(new Supplier<Service>(){
    @Override public Service get(){
      return new AbstractService(){
        @Override protected void doStart(){
          try {
            startSchedulers();
            notifyStarted();
            startUpLatch.countDown();
          }
 catch (          ServiceUnavailableException e) {
            LOG.debug(""String_Node_Str"",e.getMessage());
            notifyFailed(e);
          }
catch (          SchedulerException e) {
            LOG.warn(""String_Node_Str"",e);
            notifyFailed(e);
          }
        }
        @Override protected void doStop(){
          try {
            stopScheduler();
            notifyStopped();
          }
 catch (          SchedulerException e) {
            notifyFailed(e);
          }
        }
      }
;
    }
  }
,RetryStrategies.exponentialDelay(200,5000,TimeUnit.MILLISECONDS));
}"
4665,"@Override protected void startUp() throws Exception {
  LOG.info(""String_Node_Str"");
  serviceDelegate.startAndWait();
}","@Override protected void startUp() throws Exception {
  LOG.info(""String_Node_Str"");
  serviceDelegate.startAndWait();
  startUpLatch.await();
}"
4666,"@Override public Service get(){
  return new AbstractService(){
    @Override protected void doStart(){
      try {
        startSchedulers();
        notifyStarted();
      }
 catch (      ServiceUnavailableException e) {
        LOG.debug(""String_Node_Str"",e.getMessage());
        notifyFailed(e);
      }
catch (      SchedulerException e) {
        LOG.warn(""String_Node_Str"",e);
        notifyFailed(e);
      }
    }
    @Override protected void doStop(){
      try {
        stopScheduler();
        notifyStopped();
      }
 catch (      SchedulerException e) {
        notifyFailed(e);
      }
    }
  }
;
}","@Override public Service get(){
  return new AbstractService(){
    @Override protected void doStart(){
      try {
        startSchedulers();
        notifyStarted();
        startUpLatch.countDown();
      }
 catch (      ServiceUnavailableException e) {
        LOG.debug(""String_Node_Str"",e.getMessage());
        notifyFailed(e);
      }
catch (      SchedulerException e) {
        LOG.warn(""String_Node_Str"",e);
        notifyFailed(e);
      }
    }
    @Override protected void doStop(){
      try {
        stopScheduler();
        notifyStopped();
      }
 catch (      SchedulerException e) {
        notifyFailed(e);
      }
    }
  }
;
}"
4667,"@Override protected void doStart(){
  try {
    startSchedulers();
    notifyStarted();
  }
 catch (  ServiceUnavailableException e) {
    LOG.debug(""String_Node_Str"",e.getMessage());
    notifyFailed(e);
  }
catch (  SchedulerException e) {
    LOG.warn(""String_Node_Str"",e);
    notifyFailed(e);
  }
}","@Override protected void doStart(){
  try {
    startSchedulers();
    notifyStarted();
    startUpLatch.countDown();
  }
 catch (  ServiceUnavailableException e) {
    LOG.debug(""String_Node_Str"",e.getMessage());
    notifyFailed(e);
  }
catch (  SchedulerException e) {
    LOG.warn(""String_Node_Str"",e);
    notifyFailed(e);
  }
}"
4668,"@BeforeClass public static void beforeClass() throws Exception {
  cConf=CConfiguration.create();
  hBaseTableUtil=new HBaseTableUtilFactory(cConf,new SimpleNamespaceQueryAdmin()).get();
  ddlExecutor=new HBaseDDLExecutorFactory(cConf,TEST_HBASE.getConfiguration()).get();
  ddlExecutor.createNamespaceIfNotExists(hBaseTableUtil.getHBaseNamespace(NamespaceId.SYSTEM));
  ddlExecutor.createNamespaceIfNotExists(hBaseTableUtil.getHBaseNamespace(NAMESPACE1));
  ddlExecutor.createNamespaceIfNotExists(hBaseTableUtil.getHBaseNamespace(NAMESPACE2));
  ConfigurationWriter writer=new ConfigurationWriter(TEST_HBASE.getConfiguration(),cConf);
  writer.write(ConfigurationReader.Type.DEFAULT,cConf);
}","@BeforeClass public static void beforeClass() throws Exception {
  cConf=CConfiguration.create();
  hBaseTableUtil=new HBaseTableUtilFactory(cConf,new SimpleNamespaceQueryAdmin()).get();
  ddlExecutor=new HBaseDDLExecutorFactory(cConf,TEST_HBASE.getConfiguration()).get();
  ddlExecutor.createNamespaceIfNotExists(hBaseTableUtil.getHBaseNamespace(NAMESPACE1));
  ddlExecutor.createNamespaceIfNotExists(hBaseTableUtil.getHBaseNamespace(NAMESPACE2));
}"
4669,"private static void checkParts(EntityType entityType,List<String> parts,int index,Map<EntityType,String> entityParts){
switch (entityType) {
case INSTANCE:
case NAMESPACE:
    entityParts.put(entityType,parts.get(index));
  break;
case KERBEROSPRINCIPAL:
entityParts.put(entityType,parts.get(index));
break;
case ARTIFACT:
case APPLICATION:
if (parts.size() > 2 && index == (parts.size() - 1)) {
throw new UnsupportedOperationException(""String_Node_Str"" + ""String_Node_Str"" + parts);
}
checkParts(EntityType.NAMESPACE,parts,index - 1,entityParts);
entityParts.put(entityType,parts.get(index));
break;
case DATASET:
case DATASET_MODULE:
case DATASET_TYPE:
case STREAM:
case SECUREKEY:
checkParts(EntityType.NAMESPACE,parts,index - 1,entityParts);
entityParts.put(entityType,parts.get(index));
break;
case PROGRAM:
if (parts.size() > 4 && index == (parts.size() - 1)) {
throw new UnsupportedOperationException(""String_Node_Str"" + ""String_Node_Str"" + parts);
}
if (parts.size() == 3 && index == (parts.size() - 1)) {
String program=parts.get(index);
if (!""String_Node_Str"".equals(program)) {
throw new UnsupportedOperationException(""String_Node_Str"" + ""String_Node_Str"");
}
checkParts(EntityType.APPLICATION,parts,index - 1,entityParts);
entityParts.put(entityType,parts.get(index));
}
 else {
checkParts(EntityType.APPLICATION,parts,index - 2,entityParts);
entityParts.put(entityType,parts.get(index - 1) + ""String_Node_Str"" + parts.get(index));
}
break;
default :
throw new IllegalArgumentException(String.format(""String_Node_Str"",entityType));
}
}","private static void checkParts(EntityType entityType,List<String> parts,int index,Map<EntityType,String> entityParts){
switch (entityType) {
case INSTANCE:
case NAMESPACE:
case KERBEROSPRINCIPAL:
    if (parts.size() != 1 && index == (parts.size() - 1)) {
      throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"" + parts);
    }
  entityParts.put(entityType,parts.get(index));
break;
case ARTIFACT:
case APPLICATION:
if (parts.size() > 2 && index == (parts.size() - 1)) {
throw new UnsupportedOperationException(""String_Node_Str"" + ""String_Node_Str"" + parts);
}
if (parts.size() < 2 && index == (parts.size() - 1)) {
throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"" + parts);
}
checkParts(EntityType.NAMESPACE,parts,index - 1,entityParts);
entityParts.put(entityType,parts.get(index));
break;
case DATASET:
case DATASET_MODULE:
case DATASET_TYPE:
case STREAM:
case SECUREKEY:
if (parts.size() != 2 && index == (parts.size() - 1)) {
throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"" + parts);
}
checkParts(EntityType.NAMESPACE,parts,index - 1,entityParts);
entityParts.put(entityType,parts.get(index));
break;
case PROGRAM:
if (parts.size() > 4 && index == (parts.size() - 1)) {
throw new UnsupportedOperationException(""String_Node_Str"" + ""String_Node_Str"" + parts);
}
if (parts.size() < 3 && index == (parts.size() - 1)) {
throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ parts);
}
if (parts.size() == 3 && index == (parts.size() - 1)) {
String program=parts.get(index);
if (!""String_Node_Str"".equals(program)) {
throw new UnsupportedOperationException(""String_Node_Str"" + ""String_Node_Str"");
}
checkParts(EntityType.APPLICATION,parts,index - 1,entityParts);
entityParts.put(entityType,parts.get(index));
}
 else {
checkParts(EntityType.APPLICATION,parts,index - 2,entityParts);
entityParts.put(entityType,parts.get(index - 1) + ""String_Node_Str"" + parts.get(index));
}
break;
default :
throw new IllegalArgumentException(String.format(""String_Node_Str"",entityType));
}
}"
4670,"@Test public void testArtifact(){
  Authorizable authorizable;
  ArtifactId artifactId=new ArtifactId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  authorizable=Authorizable.fromEntityId(artifactId);
  String artifactIdNoVer=artifactId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(artifactIdNoVer,authorizable.toString());
  String widcardId=artifactIdNoVer.replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
}","@Test public void testArtifact(){
  Authorizable authorizable;
  ArtifactId artifactId=new ArtifactId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  authorizable=Authorizable.fromEntityId(artifactId);
  String artifactIdNoVer=artifactId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(artifactIdNoVer,authorizable.toString());
  String widcardId=artifactIdNoVer.replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
  verifyInvalidString(""String_Node_Str"");
  verifyInvalidString(""String_Node_Str"");
}"
4671,"@Test public void testNamespace(){
  Authorizable authorizable;
  NamespaceId namespaceId=new NamespaceId(""String_Node_Str"");
  authorizable=Authorizable.fromEntityId(namespaceId);
  Assert.assertEquals(namespaceId.toString(),authorizable.toString());
  String wildcardNs=namespaceId.toString() + ""String_Node_Str"";
  authorizable=Authorizable.fromString(wildcardNs);
  Assert.assertEquals(wildcardNs,authorizable.toString());
  wildcardNs=namespaceId.toString() + ""String_Node_Str"" + ""String_Node_Str"";
  authorizable=Authorizable.fromString(wildcardNs);
  Assert.assertEquals(wildcardNs,authorizable.toString());
  String widcardId=namespaceId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
}","@Test public void testNamespace(){
  Authorizable authorizable;
  NamespaceId namespaceId=new NamespaceId(""String_Node_Str"");
  authorizable=Authorizable.fromEntityId(namespaceId);
  Assert.assertEquals(namespaceId.toString(),authorizable.toString());
  String wildcardNs=namespaceId.toString() + ""String_Node_Str"";
  authorizable=Authorizable.fromString(wildcardNs);
  Assert.assertEquals(wildcardNs,authorizable.toString());
  wildcardNs=namespaceId.toString() + ""String_Node_Str"" + ""String_Node_Str"";
  authorizable=Authorizable.fromString(wildcardNs);
  Assert.assertEquals(wildcardNs,authorizable.toString());
  String widcardId=namespaceId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
  verifyInvalidString(""String_Node_Str"");
}"
4672,"@Test public void testStream(){
  StreamId streamId=new StreamId(""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(streamId);
  Assert.assertEquals(streamId.toString(),authorizable.toString());
  String widcardId=streamId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
}","@Test public void testStream(){
  StreamId streamId=new StreamId(""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(streamId);
  Assert.assertEquals(streamId.toString(),authorizable.toString());
  String widcardId=streamId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
  verifyInvalidString(""String_Node_Str"");
  verifyInvalidString(""String_Node_Str"");
  verifyInvalidString(""String_Node_Str"");
}"
4673,"@Test public void testDataset(){
  DatasetId datasetId=new DatasetId(""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(datasetId);
  Assert.assertEquals(datasetId.toString(),authorizable.toString());
  String widcardId=datasetId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
}","@Test public void testDataset(){
  DatasetId datasetId=new DatasetId(""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(datasetId);
  Assert.assertEquals(datasetId.toString(),authorizable.toString());
  String widcardId=datasetId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
  verifyInvalidString(""String_Node_Str"");
  verifyInvalidString(""String_Node_Str"");
}"
4674,"@Test public void testProgram(){
  ProgramId programId=new ProgramId(""String_Node_Str"",""String_Node_Str"",ProgramType.MAPREDUCE,""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(programId);
  Assert.assertEquals(programId.toString().replace(ApplicationId.DEFAULT_VERSION + ""String_Node_Str"",""String_Node_Str""),authorizable.toString());
  String wildCardProgramId=programId.toString() + ""String_Node_Str"";
  try {
    Authorizable.fromString(wildCardProgramId);
    Assert.fail();
  }
 catch (  UnsupportedOperationException e) {
  }
  ApplicationId appId=new ApplicationId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  programId=appId.program(ProgramType.MAPREDUCE,""String_Node_Str"");
  authorizable=Authorizable.fromEntityId(programId);
  String programIdNoVer=programId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(programIdNoVer,authorizable.toString());
  String widcardId=programIdNoVer.replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
  String allProgs=""String_Node_Str"";
  Assert.assertEquals(allProgs,Authorizable.fromString(allProgs).toString());
}","@Test public void testProgram(){
  ProgramId programId=new ProgramId(""String_Node_Str"",""String_Node_Str"",ProgramType.MAPREDUCE,""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(programId);
  Assert.assertEquals(programId.toString().replace(ApplicationId.DEFAULT_VERSION + ""String_Node_Str"",""String_Node_Str""),authorizable.toString());
  String wildCardProgramId=programId.toString() + ""String_Node_Str"";
  verifyInvalidString(wildCardProgramId);
  ApplicationId appId=new ApplicationId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  programId=appId.program(ProgramType.MAPREDUCE,""String_Node_Str"");
  authorizable=Authorizable.fromEntityId(programId);
  String programIdNoVer=programId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(programIdNoVer,authorizable.toString());
  String widcardId=programIdNoVer.replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
  String allProgs=""String_Node_Str"";
  Assert.assertEquals(allProgs,Authorizable.fromString(allProgs).toString());
  verifyInvalidString(""String_Node_Str"");
  verifyInvalidString(""String_Node_Str"");
}"
4675,"@Test public void testApplication(){
  ApplicationId appId=new ApplicationId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(appId);
  String appIdNoVer=appId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(appIdNoVer,authorizable.toString());
  try {
    Authorizable.fromString(appId.toString());
    Assert.fail();
  }
 catch (  UnsupportedOperationException e) {
  }
  String widcardId=appIdNoVer.replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
}","@Test public void testApplication(){
  ApplicationId appId=new ApplicationId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(appId);
  String appIdNoVer=appId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(appIdNoVer,authorizable.toString());
  verifyInvalidString(appId.toString());
  String widcardId=appIdNoVer.replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
  verifyInvalidString(""String_Node_Str"");
}"
4676,"/** 
 * Returns the transction timeout based on the given arguments or, as fallback, the CConfiguration.
 * @returns the integer value of the argument system.data.tx.timeout, or if that is not given in the arguments,the value for data.tx.timeout from the CConfiguration.
 * @throws IllegalArgumentException if the transaction timeout exceeds the transaction timeout limit given in theCConfiguratuion.
 */
public static int getTransactionTimeout(Map<String,String> args,CConfiguration cConf){
  Integer timeout=getPositiveInt(args,TRANSACTION_TIMEOUT,""String_Node_Str"");
  if (timeout == null) {
    return cConf.getInt(TxConstants.Manager.CFG_TX_TIMEOUT);
  }
  int maxTimeout=cConf.getInt(TxConstants.Manager.CFG_TX_MAX_TIMEOUT);
  if (timeout > maxTimeout) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",TRANSACTION_TIMEOUT,timeout,TxConstants.Manager.CFG_TX_MAX_TIMEOUT,maxTimeout));
  }
  return timeout;
}","/** 
 * Returns the transaction timeout based on the given arguments or, as fallback, the CConfiguration.
 * @return the integer value of the argument system.data.tx.timeout, or if that is not given in the arguments,the value for data.tx.timeout from the CConfiguration.
 * @throws IllegalArgumentException if the transaction timeout exceeds the transaction timeout limit given in the{@link CConfiguration}.
 */
public static int getTransactionTimeout(Map<String,String> args,CConfiguration cConf){
  Integer timeout=getPositiveInt(args,TRANSACTION_TIMEOUT,""String_Node_Str"");
  if (timeout == null) {
    return cConf.getInt(TxConstants.Manager.CFG_TX_TIMEOUT);
  }
  int maxTimeout=cConf.getInt(TxConstants.Manager.CFG_TX_MAX_TIMEOUT);
  if (timeout > maxTimeout) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",TRANSACTION_TIMEOUT,timeout,TxConstants.Manager.CFG_TX_MAX_TIMEOUT,maxTimeout));
  }
  return timeout;
}"
4677,"/** 
 * Set the log level for the   {@link LogAppenderInitializer}.
 * @param args the arguments to use for looking up resources configurations
 * @param initializer the LogAppenderInitializer which will be used to set up the log level
 */
public static void setLogLevel(Map<String,String> args,LogAppenderInitializer initializer){
  initializer.setLogLevels(getLogLevels(args));
}","/** 
 * Set the log level for the   {@link LogAppenderInitializer}.
 * @param args the arguments to use for looking up resources configurations
 * @param initializer the LogAppenderInitializer which will be used to set up the log level
 */
public static void setLogLevel(Arguments args,LogAppenderInitializer initializer){
  initializer.setLogLevels(getLogLevels(args.asMap()));
}"
4678,"public static Map<String,String> getLogLevels(Map<String,String> args){
  Map<String,String> logLevels=new HashMap<>();
  for (  Map.Entry<String,String> entry : args.entrySet()) {
    String loggerName=entry.getKey();
    if (loggerName.length() > LOG_LEVEL.length() && loggerName.startsWith(LOG_LEVEL)) {
      logLevels.put(loggerName.substring(LOG_LEVEL.length() + 1),entry.getValue());
    }
  }
  String logLevel=args.get(LOG_LEVEL);
  if (logLevel != null) {
    logLevels.put(Logger.ROOT_LOGGER_NAME,logLevel);
  }
  return logLevels;
}","/** 
 * Extracts log level settings from the given arguments. It extracts arguments prefixed with key  {@link #LOG_LEVEL} + {@code .}, with the remaining part of the key as the logger name, with the argument value as the log level. Also, the key   {@link #LOG_LEVEL} will be used to setup the log level of the root logger.
 */
public static Map<String,Level> getLogLevels(Map<String,String> args){
  String logLevelPrefix=LOG_LEVEL + ""String_Node_Str"";
  Map<String,Level> logLevels=new HashMap<>();
  for (  Map.Entry<String,String> entry : args.entrySet()) {
    String key=entry.getKey();
    if (key.startsWith(logLevelPrefix)) {
      logLevels.put(key.substring(logLevelPrefix.length()),Level.toLevel(entry.getValue()));
    }
  }
  String logLevel=args.get(LOG_LEVEL);
  if (logLevel != null) {
    logLevels.put(Logger.ROOT_LOGGER_NAME,Level.toLevel(logLevel));
  }
  return logLevels;
}"
4679,"@Override protected TwillPreparer setLogLevels(TwillPreparer twillPreparer,Program program,ProgramOptions options){
  FlowSpecification spec=program.getApplicationSpecification().getFlows().get(program.getName());
  for (  String flowlet : spec.getFlowlets().keySet()) {
    Map<String,String> logLevels=SystemArguments.getLogLevels(RuntimeArguments.extractScope(FlowUtils.FLOWLET_SCOPE,flowlet,options.getUserArguments().asMap()));
    if (!logLevels.isEmpty()) {
      twillPreparer.setLogLevels(flowlet,transformLogLevels(logLevels));
    }
  }
  return twillPreparer;
}","@Override protected TwillPreparer setLogLevels(TwillPreparer twillPreparer,Program program,ProgramOptions options){
  Map<String,String> arguments=options.getUserArguments().asMap();
  FlowSpecification spec=program.getApplicationSpecification().getFlows().get(program.getName());
  for (  String flowlet : spec.getFlowlets().keySet()) {
    Map<String,Level> logLevels=SystemArguments.getLogLevels(RuntimeArguments.extractScope(FlowUtils.FLOWLET_SCOPE,flowlet,arguments));
    if (!logLevels.isEmpty()) {
      twillPreparer.setLogLevels(flowlet,transformLogLevels(logLevels));
    }
  }
  return twillPreparer;
}"
4680,"protected TwillPreparer setLogLevels(TwillPreparer twillPreparer,Program program,ProgramOptions options){
  Map<String,String> logLevels=SystemArguments.getLogLevels(options.getUserArguments().asMap());
  if (logLevels.isEmpty()) {
    return twillPreparer;
  }
  return twillPreparer.setLogLevels(transformLogLevels(logLevels));
}","protected TwillPreparer setLogLevels(TwillPreparer twillPreparer,Program program,ProgramOptions options){
  Map<String,Level> logLevels=SystemArguments.getLogLevels(options.getUserArguments().asMap());
  if (logLevels.isEmpty()) {
    return twillPreparer;
  }
  return twillPreparer.setLogLevels(transformLogLevels(logLevels));
}"
4681,"protected Map<String,LogEntry.Level> transformLogLevels(Map<String,String> logLevels){
  return Maps.transformValues(logLevels,new Function<String,LogEntry.Level>(){
    @Override public LogEntry.Level apply(    String input){
      return LogEntry.Level.valueOf(input.toUpperCase());
    }
  }
);
}","protected Map<String,LogEntry.Level> transformLogLevels(Map<String,Level> logLevels){
  return Maps.transformValues(logLevels,new Function<Level,LogEntry.Level>(){
    @Override public LogEntry.Level apply(    Level level){
      if (level.equals(Level.ALL)) {
        return LogEntry.Level.TRACE;
      }
      if (level.equals(Level.OFF)) {
        return LogEntry.Level.FATAL;
      }
      return LogEntry.Level.valueOf(level.toString());
    }
  }
);
}"
4682,"@Override public LogEntry.Level apply(String input){
  return LogEntry.Level.valueOf(input.toUpperCase());
}","@Override public LogEntry.Level apply(Level level){
  if (level.equals(Level.ALL)) {
    return LogEntry.Level.TRACE;
  }
  if (level.equals(Level.OFF)) {
    return LogEntry.Level.FATAL;
  }
  return LogEntry.Level.valueOf(level.toString());
}"
4683,"public void setLogLevels(Map<String,String> logPairs){
  LoggerContext loggerContext=getLoggerContext();
  if (loggerContext != null) {
    for (    Map.Entry<String,String> entry : logPairs.entrySet()) {
      String loggerName=entry.getKey();
      String logLevel=entry.getValue();
      Logger logger=loggerContext.getLogger(loggerName);
      LOG.info(""String_Node_Str"",loggerName,logger.getLevel(),logLevel);
      logger.setLevel(Level.toLevel(logLevel));
    }
  }
}","public void setLogLevels(Map<String,Level> logPairs){
  LoggerContext loggerContext=getLoggerContext();
  if (loggerContext != null) {
    for (    Map.Entry<String,Level> entry : logPairs.entrySet()) {
      String loggerName=entry.getKey();
      Level logLevel=entry.getValue();
      Logger logger=loggerContext.getLogger(loggerName);
      LOG.info(""String_Node_Str"",loggerName,logger.getLevel(),logLevel);
      logger.setLevel(logLevel);
    }
  }
}"
4684,"@BeforeClass public static void beforeClass() throws Exception {
  cConf=CConfiguration.create();
  hBaseTableUtil=new HBaseTableUtilFactory(cConf,new SimpleNamespaceQueryAdmin()).get();
  ddlExecutor=new HBaseDDLExecutorFactory(cConf,TEST_HBASE.getConfiguration()).get();
  ddlExecutor.createNamespaceIfNotExists(hBaseTableUtil.getHBaseNamespace(NAMESPACE1));
  ddlExecutor.createNamespaceIfNotExists(hBaseTableUtil.getHBaseNamespace(NAMESPACE2));
}","@BeforeClass public static void beforeClass() throws Exception {
  cConf=CConfiguration.create();
  hBaseTableUtil=new HBaseTableUtilFactory(cConf,new SimpleNamespaceQueryAdmin()).get();
  ddlExecutor=new HBaseDDLExecutorFactory(cConf,TEST_HBASE.getConfiguration()).get();
  ddlExecutor.createNamespaceIfNotExists(hBaseTableUtil.getHBaseNamespace(NamespaceId.SYSTEM));
  ddlExecutor.createNamespaceIfNotExists(hBaseTableUtil.getHBaseNamespace(NAMESPACE1));
  ddlExecutor.createNamespaceIfNotExists(hBaseTableUtil.getHBaseNamespace(NAMESPACE2));
  ConfigurationWriter writer=new ConfigurationWriter(TEST_HBASE.getConfiguration(),cConf);
  writer.write(ConfigurationReader.Type.DEFAULT,cConf);
}"
4685,"@Override protected Configuration getSnapshotConfiguration() throws IOException {
  CConfiguration cConf=configReader.read();
  Configuration txConf=HBaseConfiguration.create(getConf());
  CConfigurationUtil.copyTxProperties(cConf,txConf);
  return txConf;
}","@Override protected Configuration getSnapshotConfiguration() throws IOException {
  CConfiguration cConf=configReader.read();
  Configuration txConf=HBaseConfiguration.create(getConf());
  CConfigurationUtil.copyTxProperties(cConf,txConf);
  setId(cConf.get(Constants.INSTANCE_NAME));
  return txConf;
}"
4686,"@Override public int getTotalMemory(){
  return totalMemory;
}","@Override public long getTotalMemory(){
  return totalMemory;
}"
4687,"@Override public int getFreeMemory(){
  return totalMemory - usedMemory;
}","@Override public long getFreeMemory(){
  return totalMemory - usedMemory;
}"
4688,"@Override public int getUsedMemory(){
  return usedMemory;
}","@Override public long getUsedMemory(){
  return usedMemory;
}"
4689,"@Override public synchronized void collect() throws Exception {
  reset();
  List<NodeReport> nodeReports;
  YarnClient yarnClient=createYARNClient();
  try {
    nodeReports=yarnClient.getNodeReports();
  }
  finally {
    yarnClient.stop();
  }
  for (  NodeReport nodeReport : nodeReports) {
    NodeId nodeId=nodeReport.getNodeId();
    LOG.debug(""String_Node_Str"",nodeId);
    if (!nodeReport.getNodeState().isUnusable()) {
      Resource nodeCapability=nodeReport.getCapability();
      Resource nodeUsed=nodeReport.getUsed();
      if (nodeCapability != null) {
        LOG.debug(""String_Node_Str"",nodeId,nodeCapability.getMemory(),nodeCapability.getVirtualCores());
        totalMemory+=nodeCapability.getMemory();
        totalVCores+=nodeCapability.getVirtualCores();
      }
      if (nodeUsed != null) {
        LOG.debug(""String_Node_Str"",nodeId,nodeUsed.getMemory(),nodeUsed.getVirtualCores());
        usedMemory+=nodeUsed.getMemory();
        usedVCores+=nodeUsed.getVirtualCores();
      }
    }
  }
}","@Override public synchronized void collect() throws Exception {
  reset();
  List<NodeReport> nodeReports;
  YarnClient yarnClient=createYARNClient();
  try {
    nodeReports=yarnClient.getNodeReports();
  }
  finally {
    yarnClient.stop();
  }
  for (  NodeReport nodeReport : nodeReports) {
    NodeId nodeId=nodeReport.getNodeId();
    LOG.debug(""String_Node_Str"",nodeId);
    if (!nodeReport.getNodeState().isUnusable()) {
      Resource nodeCapability=nodeReport.getCapability();
      Resource nodeUsed=nodeReport.getUsed();
      if (nodeCapability != null) {
        LOG.debug(""String_Node_Str"",nodeId,nodeCapability.getMemory(),nodeCapability.getVirtualCores());
        totalMemory+=nodeCapability.getMemory();
        totalVCores+=nodeCapability.getVirtualCores();
      }
      if (nodeUsed != null) {
        LOG.debug(""String_Node_Str"",nodeId,nodeUsed.getMemory(),nodeUsed.getVirtualCores());
        usedMemory+=nodeUsed.getMemory();
        usedVCores+=nodeUsed.getVirtualCores();
      }
    }
  }
  usedMemory*=BYTES_PER_MB;
  totalMemory*=BYTES_PER_MB;
}"
4690,"/** 
 * Returns the total memory in YARN.
 */
int getTotalMemory();","/** 
 * Returns the total memory in YARN.
 */
long getTotalMemory();"
4691,"/** 
 * Returns the free memory in YARN.
 */
int getFreeMemory();","/** 
 * Returns the free memory in YARN.
 */
long getFreeMemory();"
4692,"/** 
 * Returns the used memory in YARN.
 */
int getUsedMemory();","/** 
 * Returns the used memory in YARN.
 */
long getUsedMemory();"
4693,"@POST @Path(""String_Node_Str"") public void getCredentials(HttpRequest request,HttpResponder responder) throws Exception {
  String requestContent=request.getContent().toString(Charsets.UTF_8);
  if (requestContent == null) {
    throw new BadRequestException(""String_Node_Str"");
  }
  ImpersonationRequest impersonationRequest=GSON.fromJson(requestContent,ImpersonationRequest.class);
  LOG.debug(""String_Node_Str"",impersonationRequest);
  UGIWithPrincipal ugiWithPrincipal=ugiProvider.getConfiguredUGI(impersonationRequest);
  Credentials credentials=ImpersonationUtils.doAs(ugiWithPrincipal.getUGI(),new Callable<Credentials>(){
    @Override public Credentials call() throws Exception {
      return tokenSecureStoreRenewer.createCredentials();
    }
  }
);
  Location credentialsDir=locationFactory.create(""String_Node_Str"");
  if (credentialsDir.isDirectory() || credentialsDir.mkdirs() || credentialsDir.isDirectory()) {
    Location credentialsFile=credentialsDir.append(""String_Node_Str"").getTempFile(""String_Node_Str"");
    try (DataOutputStream os=new DataOutputStream(new BufferedOutputStream(credentialsFile.getOutputStream(""String_Node_Str"")))){
      credentials.writeTokenStorageToStream(os);
    }
     LOG.debug(""String_Node_Str"",ugiWithPrincipal.getPrincipal(),credentialsFile);
    PrincipalCredentials principalCredentials=new PrincipalCredentials(ugiWithPrincipal.getPrincipal(),credentialsFile.toURI().toString());
    responder.sendJson(HttpResponseStatus.OK,principalCredentials);
  }
 else {
    throw new IllegalStateException(""String_Node_Str"");
  }
}","@POST @Path(""String_Node_Str"") public void getCredentials(HttpRequest request,HttpResponder responder) throws Exception {
  String requestContent=request.getContent().toString(Charsets.UTF_8);
  if (requestContent == null) {
    throw new BadRequestException(""String_Node_Str"");
  }
  ImpersonationRequest impersonationRequest=GSON.fromJson(requestContent,ImpersonationRequest.class);
  LOG.debug(""String_Node_Str"",impersonationRequest);
  UGIWithPrincipal ugiWithPrincipal;
  try {
    ugiWithPrincipal=ugiProvider.getConfiguredUGI(impersonationRequest);
  }
 catch (  IOException e) {
    throw new ServiceException(e,HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
  Credentials credentials=ImpersonationUtils.doAs(ugiWithPrincipal.getUGI(),new Callable<Credentials>(){
    @Override public Credentials call() throws Exception {
      return tokenSecureStoreRenewer.createCredentials();
    }
  }
);
  Location credentialsDir=locationFactory.create(""String_Node_Str"");
  if (credentialsDir.isDirectory() || credentialsDir.mkdirs() || credentialsDir.isDirectory()) {
    Location credentialsFile=credentialsDir.append(""String_Node_Str"").getTempFile(""String_Node_Str"");
    try (DataOutputStream os=new DataOutputStream(new BufferedOutputStream(credentialsFile.getOutputStream(""String_Node_Str"")))){
      credentials.writeTokenStorageToStream(os);
    }
     LOG.debug(""String_Node_Str"",ugiWithPrincipal.getPrincipal(),credentialsFile);
    PrincipalCredentials principalCredentials=new PrincipalCredentials(ugiWithPrincipal.getPrincipal(),credentialsFile.toURI().toString());
    responder.sendJson(HttpResponseStatus.OK,principalCredentials);
  }
 else {
    throw new IllegalStateException(""String_Node_Str"");
  }
}"
4694,"@Override public final UGIWithPrincipal getConfiguredUGI(ImpersonationRequest impersonationRequest) throws IOException {
  try {
    UGIWithPrincipal ugi=impersonationRequest.getImpersonatedOpType().equals(ImpersonatedOpType.EXPLORE) || impersonationRequest.getPrincipal() == null ? null : ugiCache.getIfPresent(new UGICacheKey(impersonationRequest));
    if (ugi != null) {
      return ugi;
    }
    boolean isCache=checkExploreAndDetermineCache(impersonationRequest);
    ImpersonationInfo info=getPrincipalForEntity(impersonationRequest);
    ImpersonationRequest newRequest=new ImpersonationRequest(impersonationRequest.getEntityId(),impersonationRequest.getImpersonatedOpType(),info.getPrincipal(),info.getKeytabURI());
    return isCache ? ugiCache.get(new UGICacheKey(newRequest)) : createUGI(newRequest);
  }
 catch (  ExecutionException e) {
    Throwable cause=Throwables.getRootCause(e);
    Throwables.propagateIfPossible(cause,IOException.class);
    throw new IOException(cause);
  }
}","@Override public final UGIWithPrincipal getConfiguredUGI(ImpersonationRequest impersonationRequest) throws IOException {
  try {
    UGIWithPrincipal ugi=impersonationRequest.getImpersonatedOpType().equals(ImpersonatedOpType.EXPLORE) || impersonationRequest.getPrincipal() == null ? null : ugiCache.getIfPresent(new UGICacheKey(impersonationRequest));
    if (ugi != null) {
      return ugi;
    }
    boolean isCache=checkExploreAndDetermineCache(impersonationRequest);
    ImpersonationInfo info=getPrincipalForEntity(impersonationRequest);
    ImpersonationRequest newRequest=new ImpersonationRequest(impersonationRequest.getEntityId(),impersonationRequest.getImpersonatedOpType(),info.getPrincipal(),info.getKeytabURI());
    return isCache ? ugiCache.get(new UGICacheKey(newRequest)) : createUGI(newRequest);
  }
 catch (  ExecutionException e) {
    Throwable cause=e.getCause();
    Throwables.propagateIfPossible(cause,IOException.class);
    throw new IOException(cause);
  }
}"
4695,"/** 
 * Resolves the   {@link UserGroupInformation} for a given user, performing any keytab localization, if necessary.
 * @return a {@link UserGroupInformation}, based upon the information configured for a particular user
 * @throws IOException if there was any IOException during localization of the keytab
 */
@Override protected UGIWithPrincipal createUGI(ImpersonationRequest impersonationRequest) throws IOException {
  String configuredPrincipalShortName=new KerberosName(impersonationRequest.getPrincipal()).getShortName();
  if (UserGroupInformation.getCurrentUser().getShortUserName().equals(configuredPrincipalShortName)) {
    return new UGIWithPrincipal(impersonationRequest.getPrincipal(),UserGroupInformation.getCurrentUser());
  }
  URI keytabURI=URI.create(impersonationRequest.getKeytabURI());
  boolean isKeytabLocal=keytabURI.getScheme() == null || ""String_Node_Str"".equals(keytabURI.getScheme());
  File localKeytabFile=isKeytabLocal ? new File(keytabURI.getPath()) : localizeKeytab(locationFactory.create(keytabURI));
  try {
    String expandedPrincipal=SecurityUtil.expandPrincipal(impersonationRequest.getPrincipal());
    LOG.debug(""String_Node_Str"",expandedPrincipal,localKeytabFile);
    if (!Files.isReadable(localKeytabFile.toPath())) {
      throw new IOException(String.format(""String_Node_Str"",localKeytabFile));
    }
    UserGroupInformation loggedInUGI=UserGroupInformation.loginUserFromKeytabAndReturnUGI(expandedPrincipal,localKeytabFile.getAbsolutePath());
    return new UGIWithPrincipal(impersonationRequest.getPrincipal(),loggedInUGI);
  }
  finally {
    if (!isKeytabLocal && !localKeytabFile.delete()) {
      LOG.warn(""String_Node_Str"",localKeytabFile);
    }
  }
}","/** 
 * Resolves the   {@link UserGroupInformation} for a given user, performing any keytab localization, if necessary.
 * @return a {@link UserGroupInformation}, based upon the information configured for a particular user
 * @throws IOException if there was any IOException during localization of the keytab
 */
@Override protected UGIWithPrincipal createUGI(ImpersonationRequest impersonationRequest) throws IOException {
  String configuredPrincipalShortName=new KerberosName(impersonationRequest.getPrincipal()).getShortName();
  if (UserGroupInformation.getCurrentUser().getShortUserName().equals(configuredPrincipalShortName)) {
    return new UGIWithPrincipal(impersonationRequest.getPrincipal(),UserGroupInformation.getCurrentUser());
  }
  URI keytabURI=URI.create(impersonationRequest.getKeytabURI());
  boolean isKeytabLocal=keytabURI.getScheme() == null || ""String_Node_Str"".equals(keytabURI.getScheme());
  File localKeytabFile=isKeytabLocal ? new File(keytabURI.getPath()) : localizeKeytab(locationFactory.create(keytabURI));
  try {
    String expandedPrincipal=SecurityUtil.expandPrincipal(impersonationRequest.getPrincipal());
    LOG.debug(""String_Node_Str"",expandedPrincipal,localKeytabFile);
    if (!Files.isReadable(localKeytabFile.toPath())) {
      throw new IOException(String.format(""String_Node_Str"",localKeytabFile));
    }
    UserGroupInformation loggedInUGI;
    try {
      loggedInUGI=UserGroupInformation.loginUserFromKeytabAndReturnUGI(expandedPrincipal,localKeytabFile.getAbsolutePath());
    }
 catch (    Exception e) {
      throw new IOException(String.format(""String_Node_Str"" + ""String_Node_Str"",expandedPrincipal,keytabURI),e);
    }
    return new UGIWithPrincipal(impersonationRequest.getPrincipal(),loggedInUGI);
  }
  finally {
    if (!isKeytabLocal && !localKeytabFile.delete()) {
      LOG.warn(""String_Node_Str"",localKeytabFile);
    }
  }
}"
4696,"@Override public void readFields(DataInput in) throws IOException {
  int schemaLen=in.readInt();
  byte[] schemaBytes=new byte[schemaLen];
  in.readFully(schemaBytes,0,schemaLen);
  String schemaStr=Bytes.toString(schemaBytes);
  Schema schema=Schema.parseJson(schemaStr);
  int recordLen=in.readInt();
  byte[] recordBytes=new byte[recordLen];
  in.readFully(recordBytes,0,recordLen);
  String recordStr=Bytes.toString(recordBytes);
  this.record=StructuredRecordStringConverter.fromJsonString(recordStr,schema);
}","@Override public void readFields(DataInput in) throws IOException {
  int schemaLen=in.readInt();
  byte[] schemaBytes=new byte[schemaLen];
  in.readFully(schemaBytes,0,schemaLen);
  Schema schema;
  if (schemaCache.containsKey(schemaBytes)) {
    schema=schemaCache.get(schemaBytes);
  }
 else {
    String schemaStr=Bytes.toString(schemaBytes);
    schema=Schema.parseJson(schemaStr);
    schemaCache.put(schemaBytes,schema);
  }
  int recordLen=in.readInt();
  byte[] recordBytes=new byte[recordLen];
  in.readFully(recordBytes,0,recordLen);
  String recordStr=Bytes.toString(recordBytes);
  this.record=StructuredRecordStringConverter.fromJsonString(recordStr,schema);
}"
4697,"@Override public void start() throws Exception {
  if (authServer != null) {
    try {
      LOG.info(""String_Node_Str"");
      SecurityUtil.enableKerberosLogin(configuration);
      co.cask.cdap.common.service.Services.startAndWait(zkClientService,configuration.getLong(Constants.Zookeeper.CLIENT_STARTUP_TIMEOUT_MILLIS),TimeUnit.MILLISECONDS,String.format(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",configuration.get(Constants.Zookeeper.QUORUM)));
      authServer.startAndWait();
    }
 catch (    Exception e) {
      Throwable rootCause=Throwables.getRootCause(e);
      if (rootCause instanceof ServiceBindException) {
        LOG.error(""String_Node_Str"",rootCause.getMessage());
      }
 else {
        LOG.error(""String_Node_Str"");
      }
    }
  }
 else {
    String warning=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"";
    LOG.warn(warning);
  }
}","@Override public void start() throws Exception {
  if (authServer != null) {
    try {
      LOG.info(""String_Node_Str"");
      SecurityUtil.enableKerberosLogin(configuration);
      co.cask.cdap.common.service.Services.startAndWait(zkClientService,configuration.getLong(Constants.Zookeeper.CLIENT_STARTUP_TIMEOUT_MILLIS),TimeUnit.MILLISECONDS,String.format(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",configuration.get(Constants.Zookeeper.QUORUM)));
      authServer.startAndWait();
    }
 catch (    Exception e) {
      Throwable rootCause=Throwables.getRootCause(e);
      if (rootCause instanceof ServiceBindException) {
        LOG.error(""String_Node_Str"",rootCause.getMessage());
      }
 else {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
 else {
    String warning=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"";
    LOG.warn(warning);
  }
}"
4698,"@Override protected void startUp() throws Exception {
  try {
    server=new Server();
    try {
      bindAddress=InetAddress.getByName(cConfiguration.get(Constants.Security.AUTH_SERVER_BIND_ADDRESS));
    }
 catch (    UnknownHostException e) {
      LOG.error(""String_Node_Str"",e);
      throw e;
    }
    QueuedThreadPool threadPool=new QueuedThreadPool();
    threadPool.setMaxThreads(maxThreads);
    server.setThreadPool(threadPool);
    initHandlers();
    ServletContextHandler context=new ServletContextHandler();
    context.setServer(server);
    context.addServlet(HttpServletDispatcher.class,""String_Node_Str"");
    context.addEventListener(new AuthenticationGuiceServletContextListener(handlers));
    context.setSecurityHandler(authenticationHandler);
    ContextHandler statusContext=new ContextHandler();
    statusContext.setContextPath(Constants.EndPoints.STATUS);
    statusContext.setServer(server);
    statusContext.setHandler(new StatusRequestHandler());
    if (cConfiguration.getBoolean(Constants.Security.SSL.EXTERNAL_ENABLED,false)) {
      SslContextFactory sslContextFactory=new SslContextFactory();
      String keyStorePath=sConfiguration.get(Constants.Security.AuthenticationServer.SSL_KEYSTORE_PATH);
      String keyStorePassword=sConfiguration.get(Constants.Security.AuthenticationServer.SSL_KEYSTORE_PASSWORD);
      String keyStoreType=sConfiguration.get(Constants.Security.AuthenticationServer.SSL_KEYSTORE_TYPE,Constants.Security.AuthenticationServer.DEFAULT_SSL_KEYSTORE_TYPE);
      String keyPassword=sConfiguration.get(Constants.Security.AuthenticationServer.SSL_KEYPASSWORD);
      Preconditions.checkArgument(keyStorePath != null,""String_Node_Str"");
      Preconditions.checkArgument(keyStorePassword != null,""String_Node_Str"");
      sslContextFactory.setKeyStorePath(keyStorePath);
      sslContextFactory.setKeyStorePassword(keyStorePassword);
      sslContextFactory.setKeyStoreType(keyStoreType);
      if (keyPassword != null && keyPassword.length() != 0) {
        sslContextFactory.setKeyManagerPassword(keyPassword);
      }
      String trustStorePath=cConfiguration.get(Constants.Security.AuthenticationServer.SSL_TRUSTSTORE_PATH);
      if (StringUtils.isNotEmpty(trustStorePath)) {
        String trustStorePassword=cConfiguration.get(Constants.Security.AuthenticationServer.SSL_TRUSTSTORE_PASSWORD);
        String trustStoreType=cConfiguration.get(Constants.Security.AuthenticationServer.SSL_TRUSTSTORE_TYPE,Constants.Security.AuthenticationServer.DEFAULT_SSL_KEYSTORE_TYPE);
        sslContextFactory.setWantClientAuth(true);
        sslContextFactory.setTrustStore(trustStorePath);
        sslContextFactory.setTrustStorePassword(trustStorePassword);
        sslContextFactory.setTrustStoreType(trustStoreType);
        sslContextFactory.setValidateCerts(true);
      }
      SslSelectChannelConnector sslConnector=new SslSelectChannelConnector(sslContextFactory);
      sslConnector.setHost(bindAddress.getCanonicalHostName());
      sslConnector.setPort(port);
      server.setConnectors(new Connector[]{sslConnector});
    }
 else {
      SelectChannelConnector connector=new SelectChannelConnector();
      connector.setHost(bindAddress.getCanonicalHostName());
      connector.setPort(port);
      server.setConnectors(new Connector[]{connector});
    }
    HandlerCollection handlers=new HandlerCollection();
    handlers.addHandler(statusContext);
    handlers.addHandler(context);
    handlers.addHandler(auditLogHandler);
    server.setHandler(handlers);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
  }
  try {
    server.start();
  }
 catch (  Exception e) {
    if ((Throwables.getRootCause(e) instanceof BindException)) {
      throw new ServiceBindException(""String_Node_Str"",bindAddress.getCanonicalHostName(),port,e);
    }
    throw e;
  }
  Connector connector=server.getConnectors()[0];
  InetSocketAddress inetSocketAddress=new InetSocketAddress(connector.getHost(),connector.getLocalPort());
  serviceCancellable=discoveryService.register(ResolvingDiscoverable.of(new Discoverable(Constants.Service.EXTERNAL_AUTHENTICATION,inetSocketAddress)));
}","@Override protected void startUp() throws Exception {
  server=new Server();
  InetAddress bindAddress=InetAddress.getByName(cConfiguration.get(Constants.Security.AUTH_SERVER_BIND_ADDRESS));
  QueuedThreadPool threadPool=new QueuedThreadPool();
  threadPool.setMaxThreads(maxThreads);
  server.setThreadPool(threadPool);
  initHandlers();
  ServletContextHandler context=new ServletContextHandler();
  context.setServer(server);
  context.addServlet(HttpServletDispatcher.class,""String_Node_Str"");
  context.addEventListener(new AuthenticationGuiceServletContextListener(handlers));
  context.setSecurityHandler(authenticationHandler);
  ContextHandler statusContext=new ContextHandler();
  statusContext.setContextPath(Constants.EndPoints.STATUS);
  statusContext.setServer(server);
  statusContext.setHandler(new StatusRequestHandler());
  if (cConfiguration.getBoolean(Constants.Security.SSL.EXTERNAL_ENABLED,false)) {
    SslContextFactory sslContextFactory=new SslContextFactory();
    String keyStorePath=sConfiguration.get(Constants.Security.AuthenticationServer.SSL_KEYSTORE_PATH);
    String keyStorePassword=sConfiguration.get(Constants.Security.AuthenticationServer.SSL_KEYSTORE_PASSWORD);
    String keyStoreType=sConfiguration.get(Constants.Security.AuthenticationServer.SSL_KEYSTORE_TYPE,Constants.Security.AuthenticationServer.DEFAULT_SSL_KEYSTORE_TYPE);
    String keyPassword=sConfiguration.get(Constants.Security.AuthenticationServer.SSL_KEYPASSWORD);
    Preconditions.checkArgument(keyStorePath != null,""String_Node_Str"");
    Preconditions.checkArgument(keyStorePassword != null,""String_Node_Str"");
    sslContextFactory.setKeyStorePath(keyStorePath);
    sslContextFactory.setKeyStorePassword(keyStorePassword);
    sslContextFactory.setKeyStoreType(keyStoreType);
    if (keyPassword != null && keyPassword.length() != 0) {
      sslContextFactory.setKeyManagerPassword(keyPassword);
    }
    String trustStorePath=cConfiguration.get(Constants.Security.AuthenticationServer.SSL_TRUSTSTORE_PATH);
    if (StringUtils.isNotEmpty(trustStorePath)) {
      String trustStorePassword=cConfiguration.get(Constants.Security.AuthenticationServer.SSL_TRUSTSTORE_PASSWORD);
      String trustStoreType=cConfiguration.get(Constants.Security.AuthenticationServer.SSL_TRUSTSTORE_TYPE,Constants.Security.AuthenticationServer.DEFAULT_SSL_KEYSTORE_TYPE);
      sslContextFactory.setWantClientAuth(true);
      sslContextFactory.setTrustStore(trustStorePath);
      sslContextFactory.setTrustStorePassword(trustStorePassword);
      sslContextFactory.setTrustStoreType(trustStoreType);
      sslContextFactory.setValidateCerts(true);
    }
    SslSelectChannelConnector sslConnector=new SslSelectChannelConnector(sslContextFactory);
    sslConnector.setHost(bindAddress.getCanonicalHostName());
    sslConnector.setPort(port);
    server.setConnectors(new Connector[]{sslConnector});
  }
 else {
    SelectChannelConnector connector=new SelectChannelConnector();
    connector.setHost(bindAddress.getCanonicalHostName());
    connector.setPort(port);
    server.setConnectors(new Connector[]{connector});
  }
  HandlerCollection handlers=new HandlerCollection();
  handlers.addHandler(statusContext);
  handlers.addHandler(context);
  handlers.addHandler(auditLogHandler);
  server.setHandler(handlers);
  try {
    server.start();
  }
 catch (  Exception e) {
    if ((Throwables.getRootCause(e) instanceof BindException)) {
      throw new ServiceBindException(""String_Node_Str"",bindAddress.getCanonicalHostName(),port,e);
    }
    throw e;
  }
  Connector connector=server.getConnectors()[0];
  InetSocketAddress inetSocketAddress=new InetSocketAddress(connector.getHost(),connector.getLocalPort());
  serviceCancellable=discoveryService.register(ResolvingDiscoverable.of(new Discoverable(Constants.Service.EXTERNAL_AUTHENTICATION,inetSocketAddress)));
}"
4699,"@Inject AbstractRunRecordCorrectorService(Store store,ProgramStateWriter programStateWriter,ProgramLifecycleService programLifecycleService,ProgramRuntimeService runtimeService){
  this.store=store;
  this.programStateWriter=programStateWriter;
  this.programLifecycleService=programLifecycleService;
  this.runtimeService=runtimeService;
}","@Inject AbstractRunRecordCorrectorService(CConfiguration cConf,Store store,ProgramStateWriter programStateWriter,ProgramLifecycleService programLifecycleService,ProgramRuntimeService runtimeService){
  this.store=store;
  this.programStateWriter=programStateWriter;
  this.programLifecycleService=programLifecycleService;
  this.runtimeService=runtimeService;
  this.startTimeoutSecs=2L * cConf.getLong(Constants.AppFabric.PROGRAM_MAX_START_SECONDS);
}"
4700,"/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @param programType The type of program the run records need to validate and update.
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 */
private void validateAndCorrectRunningRunRecords(final ProgramType programType,final Set<String> processedInvalidRunRecordIds){
  final Map<RunId,ProgramRuntimeService.RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  com.google.common.base.Predicate<RunRecordMeta> filter=new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta input){
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
;
  LOG.trace(""String_Node_Str"");
  List<RunRecordMeta> notActuallyRunning=new ArrayList<>();
  notActuallyRunning.addAll(store.getRuns(ProgramRunStatus.RUNNING,filter).values());
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  notActuallyRunning.addAll(store.getRuns(ProgramRunStatus.STARTING,filter).values());
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  final Map<String,ProgramId> runIdToProgramId=new HashMap<>();
  LOG.trace(""String_Node_Str"");
  Collection<RunRecordMeta> invalidRunRecords=Collections2.filter(notActuallyRunning,new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta input){
      String runId=input.getPid();
      ProgramId targetProgramId=programLifecycleService.retrieveProgramIdForRunRecord(programType,runId);
      if (targetProgramId != null) {
        runIdToProgramId.put(runId,targetProgramId);
        return true;
      }
 else {
        return false;
      }
    }
  }
);
  invalidRunRecords=Collections2.filter(invalidRunRecords,new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta invalidRunRecordMeta){
      boolean shouldCorrect=shouldCorrectForWorkflowChildren(invalidRunRecordMeta,processedInvalidRunRecordIds);
      if (!shouldCorrect) {
        LOG.trace(""String_Node_Str"",invalidRunRecordMeta);
        return false;
      }
      return true;
    }
  }
);
  LOG.trace(""String_Node_Str"");
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"" + ""String_Node_Str"",invalidRunRecords.size(),programType.getPrettyName());
  }
 else {
    LOG.trace(""String_Node_Str"" + ""String_Node_Str"",programType.getPrettyName());
  }
  for (  RunRecordMeta invalidRunRecordMeta : invalidRunRecords) {
    String runId=invalidRunRecordMeta.getPid();
    ProgramId targetProgramId=runIdToProgramId.get(runId);
    programStateWriter.error(targetProgramId.run(runId),new Throwable(""String_Node_Str""));
    LOG.warn(""String_Node_Str"" + ""String_Node_Str"",runId,targetProgramId);
    processedInvalidRunRecordIds.add(runId);
  }
}","/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @param programType The type of program the run records need to validate and update.
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 */
private void validateAndCorrectRunningRunRecords(final ProgramType programType,final Set<String> processedInvalidRunRecordIds){
  final Map<RunId,ProgramRuntimeService.RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  final long now=TimeUnit.SECONDS.convert(System.currentTimeMillis(),TimeUnit.MILLISECONDS);
  com.google.common.base.Predicate<RunRecordMeta> filter=new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta input){
      String runId=input.getPid();
      long timeSinceStart=now - input.getStartTs();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId)) && timeSinceStart > startTimeoutSecs;
    }
  }
;
  LOG.trace(""String_Node_Str"");
  List<RunRecordMeta> notActuallyRunning=new ArrayList<>();
  notActuallyRunning.addAll(store.getRuns(ProgramRunStatus.RUNNING,filter).values());
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  notActuallyRunning.addAll(store.getRuns(ProgramRunStatus.STARTING,filter).values());
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  final Map<String,ProgramId> runIdToProgramId=new HashMap<>();
  LOG.trace(""String_Node_Str"");
  Collection<RunRecordMeta> invalidRunRecords=Collections2.filter(notActuallyRunning,new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta input){
      String runId=input.getPid();
      ProgramId targetProgramId=programLifecycleService.retrieveProgramIdForRunRecord(programType,runId);
      if (targetProgramId != null) {
        runIdToProgramId.put(runId,targetProgramId);
        return true;
      }
 else {
        return false;
      }
    }
  }
);
  invalidRunRecords=Collections2.filter(invalidRunRecords,new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta invalidRunRecordMeta){
      boolean shouldCorrect=shouldCorrectForWorkflowChildren(invalidRunRecordMeta,processedInvalidRunRecordIds);
      if (!shouldCorrect) {
        LOG.trace(""String_Node_Str"",invalidRunRecordMeta);
        return false;
      }
      return true;
    }
  }
);
  LOG.trace(""String_Node_Str"");
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"" + ""String_Node_Str"",invalidRunRecords.size(),programType.getPrettyName());
  }
 else {
    LOG.trace(""String_Node_Str"" + ""String_Node_Str"",programType.getPrettyName());
  }
  for (  RunRecordMeta invalidRunRecordMeta : invalidRunRecords) {
    String runId=invalidRunRecordMeta.getPid();
    ProgramId targetProgramId=runIdToProgramId.get(runId);
    programStateWriter.error(targetProgramId.run(runId),new Throwable(""String_Node_Str""));
    LOG.warn(""String_Node_Str"" + ""String_Node_Str"",runId,targetProgramId);
    processedInvalidRunRecordIds.add(runId);
  }
}"
4701,"@Inject public DistributedRunRecordCorrectorService(CConfiguration cConf,Store store,ProgramStateWriter programStateWriter,ProgramLifecycleService programLifecycleService,ProgramRuntimeService runtimeService){
  super(store,programStateWriter,programLifecycleService,runtimeService);
  this.cConf=cConf;
}","@Inject public DistributedRunRecordCorrectorService(CConfiguration cConf,Store store,ProgramStateWriter programStateWriter,ProgramLifecycleService programLifecycleService,ProgramRuntimeService runtimeService){
  super(cConf,store,programStateWriter,programLifecycleService,runtimeService);
  this.cConf=cConf;
}"
4702,"@Inject public LocalRunRecordCorrectorService(Store store,ProgramStateWriter programStateWriter,ProgramLifecycleService programLifecycleService,ProgramRuntimeService runtimeService){
  super(store,programStateWriter,programLifecycleService,runtimeService);
}","@Inject public LocalRunRecordCorrectorService(CConfiguration cConf,Store store,ProgramStateWriter programStateWriter,ProgramLifecycleService programLifecycleService,ProgramRuntimeService runtimeService){
  super(cConf,store,programStateWriter,programLifecycleService,runtimeService);
}"
4703,"@Test public void testInvalidFlowRunRecord() throws Exception {
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  final Id.Program wordcountFlow1=Id.Program.from(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW,""String_Node_Str"");
  Assert.assertEquals(""String_Node_Str"",getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow1);
  waitState(wordcountFlow1,ProgramRunStatus.RUNNING.toString());
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return getProgramRuns(wordcountFlow1,ProgramRunStatus.RUNNING).size();
    }
  }
,5,TimeUnit.SECONDS);
  List<RunRecord> runRecords=getProgramRuns(wordcountFlow1,ProgramRunStatus.RUNNING);
  Assert.assertEquals(1,runRecords.size());
  final RunRecord rr=runRecords.get(0);
  Assert.assertEquals(ProgramRunStatus.RUNNING,rr.getStatus());
  RuntimeInfo runtimeInfo=runtimeService.lookup(wordcountFlow1.toEntityId(),RunIds.fromString(rr.getPid()));
  ProgramController programController=runtimeInfo.getController();
  programController.stop();
  Tasks.waitFor(ProgramRunStatus.KILLED,new Callable<ProgramRunStatus>(){
    @Override public ProgramRunStatus call() throws Exception {
      RunRecordMeta runRecord=store.getRun(wordcountFlow1.toEntityId(),rr.getPid());
      return runRecord == null ? null : runRecord.getStatus();
    }
  }
,5,TimeUnit.SECONDS,100,TimeUnit.MILLISECONDS);
  long now=System.currentTimeMillis();
  long nowSecs=TimeUnit.MILLISECONDS.toSeconds(now);
  store.setStart(wordcountFlow1.toEntityId(),rr.getPid(),nowSecs,null,ImmutableMap.<String,String>of(),ImmutableMap.<String,String>of(),ByteBuffer.allocate(0).array());
  store.setRunning(wordcountFlow1.toEntityId(),rr.getPid(),nowSecs + 1,null,ByteBuffer.allocate(1).array());
  RunRecord runRecordMeta=store.getRun(wordcountFlow1.toEntityId(),rr.getPid());
  Assert.assertNotNull(runRecordMeta);
  Assert.assertEquals(ProgramRunStatus.RUNNING,runRecordMeta.getStatus());
  runRecords=getProgramRuns(wordcountFlow1,ProgramRunStatus.FAILED);
  Assert.assertEquals(0,runRecords.size());
  new LocalRunRecordCorrectorService(store,programStateWriter,programLifecycleService,runtimeService).startUp();
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return getProgramRuns(wordcountFlow1,ProgramRunStatus.FAILED).size();
    }
  }
,30,TimeUnit.SECONDS,1,TimeUnit.SECONDS);
}","@Test public void testInvalidFlowRunRecord() throws Exception {
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  final Id.Program wordcountFlow1=Id.Program.from(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW,""String_Node_Str"");
  Assert.assertEquals(""String_Node_Str"",getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow1);
  waitState(wordcountFlow1,ProgramRunStatus.RUNNING.toString());
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return getProgramRuns(wordcountFlow1,ProgramRunStatus.RUNNING).size();
    }
  }
,5,TimeUnit.SECONDS);
  List<RunRecord> runRecords=getProgramRuns(wordcountFlow1,ProgramRunStatus.RUNNING);
  Assert.assertEquals(1,runRecords.size());
  final RunRecord rr=runRecords.get(0);
  Assert.assertEquals(ProgramRunStatus.RUNNING,rr.getStatus());
  RuntimeInfo runtimeInfo=runtimeService.lookup(wordcountFlow1.toEntityId(),RunIds.fromString(rr.getPid()));
  ProgramController programController=runtimeInfo.getController();
  programController.stop();
  Tasks.waitFor(ProgramRunStatus.KILLED,new Callable<ProgramRunStatus>(){
    @Override public ProgramRunStatus call() throws Exception {
      RunRecordMeta runRecord=store.getRun(wordcountFlow1.toEntityId(),rr.getPid());
      return runRecord == null ? null : runRecord.getStatus();
    }
  }
,5,TimeUnit.SECONDS,100,TimeUnit.MILLISECONDS);
  long now=System.currentTimeMillis() - 1000L;
  long nowSecs=TimeUnit.MILLISECONDS.toSeconds(now);
  store.setStart(wordcountFlow1.toEntityId(),rr.getPid(),nowSecs,null,ImmutableMap.<String,String>of(),ImmutableMap.<String,String>of(),ByteBuffer.allocate(0).array());
  store.setRunning(wordcountFlow1.toEntityId(),rr.getPid(),nowSecs + 1,null,ByteBuffer.allocate(1).array());
  RunRecord runRecordMeta=store.getRun(wordcountFlow1.toEntityId(),rr.getPid());
  Assert.assertNotNull(runRecordMeta);
  Assert.assertEquals(ProgramRunStatus.RUNNING,runRecordMeta.getStatus());
  runRecords=getProgramRuns(wordcountFlow1,ProgramRunStatus.FAILED);
  Assert.assertEquals(0,runRecords.size());
  CConfiguration testConf=CConfiguration.create();
  testConf.set(Constants.AppFabric.PROGRAM_MAX_START_SECONDS,""String_Node_Str"");
  new LocalRunRecordCorrectorService(testConf,store,programStateWriter,programLifecycleService,runtimeService).startUp();
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return getProgramRuns(wordcountFlow1,ProgramRunStatus.FAILED).size();
    }
  }
,30,TimeUnit.SECONDS,1,TimeUnit.SECONDS);
}"
4704,"/** 
 * Configures a stage and returns the spec for it.
 * @param stage the user provided configuration for the stage
 * @param validatedPipeline the validated pipeline config
 * @param pluginConfigurer configurer used to configure the stage
 * @return the spec for the stage
 */
private ConfiguredStage configureStage(ETLStage stage,ValidatedPipeline validatedPipeline,DefaultPipelineConfigurer pluginConfigurer){
  String stageName=stage.getName();
  ETLPlugin stagePlugin=stage.getPlugin();
  if (!Strings.isNullOrEmpty(stage.getErrorDatasetName())) {
    configurer.createDataset(stage.getErrorDatasetName(),errorDatasetClass,errorDatasetProperties);
  }
  PluginSpec pluginSpec=configurePlugin(stageName,stagePlugin,pluginConfigurer);
  DefaultStageConfigurer stageConfigurer=pluginConfigurer.getStageConfigurer();
  Map<String,StageSpec.Port> outputSchemas=new HashMap<>();
  if (pluginSpec.getType().equals(SplitterTransform.PLUGIN_TYPE)) {
    Map<String,Schema> outputPortSchemas=stageConfigurer.getOutputPortSchemas();
    for (    Map.Entry<String,String> outputEntry : validatedPipeline.getOutputPorts(stageName).entrySet()) {
      String outputStage=outputEntry.getKey();
      String outputPort=outputEntry.getValue();
      if (outputPort == null) {
        throw new IllegalArgumentException(String.format(""String_Node_Str"",stageName,outputStage));
      }
      outputSchemas.put(outputStage,new StageSpec.Port(outputPort,outputPortSchemas.get(outputPort)));
    }
  }
 else {
    Schema outputSchema=stageConfigurer.getOutputSchema();
    for (    String outputStage : validatedPipeline.getOutputs(stageName)) {
      outputSchemas.put(outputStage,new StageSpec.Port(null,outputSchema));
    }
  }
  Map<String,Schema> inputSchemas=stageConfigurer.getInputSchemas();
  StageSpec stageSpec=StageSpec.builder(stageName,pluginSpec).setErrorDatasetName(stage.getErrorDatasetName()).addInputSchemas(inputSchemas).addOutputPortSchemas(outputSchemas).setErrorSchema(stageConfigurer.getErrorSchema()).setProcessTimingEnabled(validatedPipeline.isProcessTimingEnabled()).setStageLoggingEnabled(validatedPipeline.isStageLoggingEnabled()).build();
  return new ConfiguredStage(stageSpec,pluginConfigurer.getPipelineProperties());
}","/** 
 * Configures a stage and returns the spec for it.
 * @param stage the user provided configuration for the stage
 * @param validatedPipeline the validated pipeline config
 * @param pluginConfigurer configurer used to configure the stage
 * @return the spec for the stage
 */
private ConfiguredStage configureStage(ETLStage stage,ValidatedPipeline validatedPipeline,DefaultPipelineConfigurer pluginConfigurer){
  String stageName=stage.getName();
  ETLPlugin stagePlugin=stage.getPlugin();
  if (!Strings.isNullOrEmpty(stage.getErrorDatasetName())) {
    configurer.createDataset(stage.getErrorDatasetName(),errorDatasetClass,errorDatasetProperties);
  }
  PluginSpec pluginSpec=configurePlugin(stageName,stagePlugin,pluginConfigurer);
  DefaultStageConfigurer stageConfigurer=pluginConfigurer.getStageConfigurer();
  Map<String,StageSpec.Port> outputSchemas=new HashMap<>();
  Map<String,Schema> inputSchemas=stageConfigurer.getInputSchemas();
  if (pluginSpec.getType().equals(SplitterTransform.PLUGIN_TYPE)) {
    Map<String,Schema> outputPortSchemas=stageConfigurer.getOutputPortSchemas();
    for (    Map.Entry<String,String> outputEntry : validatedPipeline.getOutputPorts(stageName).entrySet()) {
      String outputStage=outputEntry.getKey();
      String outputPort=outputEntry.getValue();
      if (outputPort == null) {
        throw new IllegalArgumentException(String.format(""String_Node_Str"",stageName,outputStage));
      }
      outputSchemas.put(outputStage,new StageSpec.Port(outputPort,outputPortSchemas.get(outputPort)));
    }
  }
 else {
    Schema outputSchema=stageConfigurer.getOutputSchema();
    if (Condition.PLUGIN_TYPE.equals(pluginSpec.getType())) {
      outputSchema=null;
      for (      Schema schema : inputSchemas.values()) {
        if (schema != null) {
          if (outputSchema != null && !outputSchema.equals(schema)) {
            throw new IllegalArgumentException(""String_Node_Str"" + stageName);
          }
          outputSchema=schema;
        }
      }
    }
    for (    String outputStage : validatedPipeline.getOutputs(stageName)) {
      outputSchemas.put(outputStage,new StageSpec.Port(null,outputSchema));
    }
  }
  StageSpec stageSpec=StageSpec.builder(stageName,pluginSpec).setErrorDatasetName(stage.getErrorDatasetName()).addInputSchemas(inputSchemas).addOutputPortSchemas(outputSchemas).setErrorSchema(stageConfigurer.getErrorSchema()).setProcessTimingEnabled(validatedPipeline.isProcessTimingEnabled()).setStageLoggingEnabled(validatedPipeline.isStageLoggingEnabled()).build();
  return new ConfiguredStage(stageSpec,pluginConfigurer.getPipelineProperties());
}"
4705,"/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @param programType The type of program the run records need to validate and update.
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 */
private void validateAndCorrectRunningRunRecords(final ProgramType programType,final Set<String> processedInvalidRunRecordIds){
  final Map<RunId,ProgramRuntimeService.RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  LOG.trace(""String_Node_Str"");
  Collection<RunRecordMeta> notActuallyRunning=store.getRuns(ProgramRunStatus.RUNNING,new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta input){
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
).values();
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  final Map<String,ProgramId> runIdToProgramId=new HashMap<>();
  LOG.trace(""String_Node_Str"");
  Collection<RunRecordMeta> invalidRunRecords=Collections2.filter(notActuallyRunning,new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta input){
      String runId=input.getPid();
      ProgramId targetProgramId=programLifecycleService.retrieveProgramIdForRunRecord(programType,runId);
      if (targetProgramId != null) {
        runIdToProgramId.put(runId,targetProgramId);
        return true;
      }
 else {
        return false;
      }
    }
  }
);
  invalidRunRecords=Collections2.filter(invalidRunRecords,new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta invalidRunRecordMeta){
      boolean shouldCorrect=shouldCorrectForWorkflowChildren(invalidRunRecordMeta,processedInvalidRunRecordIds);
      if (!shouldCorrect) {
        LOG.trace(""String_Node_Str"",invalidRunRecordMeta);
        return false;
      }
      return true;
    }
  }
);
  LOG.trace(""String_Node_Str"");
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"",invalidRunRecords.size(),programType.getPrettyName());
  }
 else {
    LOG.trace(""String_Node_Str"",programType.getPrettyName());
  }
  for (  RunRecordMeta invalidRunRecordMeta : invalidRunRecords) {
    String runId=invalidRunRecordMeta.getPid();
    ProgramId targetProgramId=runIdToProgramId.get(runId);
    programStateWriter.error(targetProgramId.run(runId),new Throwable(""String_Node_Str""));
    LOG.warn(""String_Node_Str"" + ""String_Node_Str"",runId,targetProgramId);
    processedInvalidRunRecordIds.add(runId);
  }
}","/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @param programType The type of program the run records need to validate and update.
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 */
private void validateAndCorrectRunningRunRecords(final ProgramType programType,final Set<String> processedInvalidRunRecordIds){
  final Map<RunId,ProgramRuntimeService.RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  com.google.common.base.Predicate<RunRecordMeta> filter=new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta input){
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
;
  LOG.trace(""String_Node_Str"");
  List<RunRecordMeta> notActuallyRunning=new ArrayList<>();
  notActuallyRunning.addAll(store.getRuns(ProgramRunStatus.RUNNING,filter).values());
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  notActuallyRunning.addAll(store.getRuns(ProgramRunStatus.STARTING,filter).values());
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  final Map<String,ProgramId> runIdToProgramId=new HashMap<>();
  LOG.trace(""String_Node_Str"");
  Collection<RunRecordMeta> invalidRunRecords=Collections2.filter(notActuallyRunning,new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta input){
      String runId=input.getPid();
      ProgramId targetProgramId=programLifecycleService.retrieveProgramIdForRunRecord(programType,runId);
      if (targetProgramId != null) {
        runIdToProgramId.put(runId,targetProgramId);
        return true;
      }
 else {
        return false;
      }
    }
  }
);
  invalidRunRecords=Collections2.filter(invalidRunRecords,new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta invalidRunRecordMeta){
      boolean shouldCorrect=shouldCorrectForWorkflowChildren(invalidRunRecordMeta,processedInvalidRunRecordIds);
      if (!shouldCorrect) {
        LOG.trace(""String_Node_Str"",invalidRunRecordMeta);
        return false;
      }
      return true;
    }
  }
);
  LOG.trace(""String_Node_Str"");
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"" + ""String_Node_Str"",invalidRunRecords.size(),programType.getPrettyName());
  }
 else {
    LOG.trace(""String_Node_Str"" + ""String_Node_Str"",programType.getPrettyName());
  }
  for (  RunRecordMeta invalidRunRecordMeta : invalidRunRecords) {
    String runId=invalidRunRecordMeta.getPid();
    ProgramId targetProgramId=runIdToProgramId.get(runId);
    programStateWriter.error(targetProgramId.run(runId),new Throwable(""String_Node_Str""));
    LOG.warn(""String_Node_Str"" + ""String_Node_Str"",runId,targetProgramId);
    processedInvalidRunRecordIds.add(runId);
  }
}"
4706,"@Inject AbstractRunRecordCorrectorService(Store store,ProgramStateWriter programStateWriter,ProgramLifecycleService programLifecycleService,ProgramRuntimeService runtimeService){
  this.store=store;
  this.programStateWriter=programStateWriter;
  this.programLifecycleService=programLifecycleService;
  this.runtimeService=runtimeService;
}","@Inject AbstractRunRecordCorrectorService(CConfiguration cConf,Store store,ProgramStateWriter programStateWriter,ProgramLifecycleService programLifecycleService,ProgramRuntimeService runtimeService){
  this.store=store;
  this.programStateWriter=programStateWriter;
  this.programLifecycleService=programLifecycleService;
  this.runtimeService=runtimeService;
  this.startTimeoutSecs=2L * cConf.getLong(Constants.AppFabric.PROGRAM_MAX_START_SECONDS);
}"
4707,"/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @param programType The type of program the run records need to validate and update.
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 */
private void validateAndCorrectRunningRunRecords(final ProgramType programType,final Set<String> processedInvalidRunRecordIds){
  final Map<RunId,ProgramRuntimeService.RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  com.google.common.base.Predicate<RunRecordMeta> filter=new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta input){
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
;
  LOG.trace(""String_Node_Str"");
  List<RunRecordMeta> notActuallyRunning=new ArrayList<>();
  notActuallyRunning.addAll(store.getRuns(ProgramRunStatus.RUNNING,filter).values());
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  notActuallyRunning.addAll(store.getRuns(ProgramRunStatus.STARTING,filter).values());
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  final Map<String,ProgramId> runIdToProgramId=new HashMap<>();
  LOG.trace(""String_Node_Str"");
  Collection<RunRecordMeta> invalidRunRecords=Collections2.filter(notActuallyRunning,new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta input){
      String runId=input.getPid();
      ProgramId targetProgramId=programLifecycleService.retrieveProgramIdForRunRecord(programType,runId);
      if (targetProgramId != null) {
        runIdToProgramId.put(runId,targetProgramId);
        return true;
      }
 else {
        return false;
      }
    }
  }
);
  invalidRunRecords=Collections2.filter(invalidRunRecords,new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta invalidRunRecordMeta){
      boolean shouldCorrect=shouldCorrectForWorkflowChildren(invalidRunRecordMeta,processedInvalidRunRecordIds);
      if (!shouldCorrect) {
        LOG.trace(""String_Node_Str"",invalidRunRecordMeta);
        return false;
      }
      return true;
    }
  }
);
  LOG.trace(""String_Node_Str"");
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"" + ""String_Node_Str"",invalidRunRecords.size(),programType.getPrettyName());
  }
 else {
    LOG.trace(""String_Node_Str"" + ""String_Node_Str"",programType.getPrettyName());
  }
  for (  RunRecordMeta invalidRunRecordMeta : invalidRunRecords) {
    String runId=invalidRunRecordMeta.getPid();
    ProgramId targetProgramId=runIdToProgramId.get(runId);
    programStateWriter.error(targetProgramId.run(runId),new Throwable(""String_Node_Str""));
    LOG.warn(""String_Node_Str"" + ""String_Node_Str"",runId,targetProgramId);
    processedInvalidRunRecordIds.add(runId);
  }
}","/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @param programType The type of program the run records need to validate and update.
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 */
private void validateAndCorrectRunningRunRecords(final ProgramType programType,final Set<String> processedInvalidRunRecordIds){
  final Map<RunId,ProgramRuntimeService.RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  final long now=TimeUnit.SECONDS.convert(System.currentTimeMillis(),TimeUnit.MILLISECONDS);
  com.google.common.base.Predicate<RunRecordMeta> filter=new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta input){
      String runId=input.getPid();
      long timeSinceStart=now - input.getStartTs();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId)) && timeSinceStart > startTimeoutSecs;
    }
  }
;
  LOG.trace(""String_Node_Str"");
  List<RunRecordMeta> notActuallyRunning=new ArrayList<>();
  notActuallyRunning.addAll(store.getRuns(ProgramRunStatus.RUNNING,filter).values());
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  notActuallyRunning.addAll(store.getRuns(ProgramRunStatus.STARTING,filter).values());
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  final Map<String,ProgramId> runIdToProgramId=new HashMap<>();
  LOG.trace(""String_Node_Str"");
  Collection<RunRecordMeta> invalidRunRecords=Collections2.filter(notActuallyRunning,new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta input){
      String runId=input.getPid();
      ProgramId targetProgramId=programLifecycleService.retrieveProgramIdForRunRecord(programType,runId);
      if (targetProgramId != null) {
        runIdToProgramId.put(runId,targetProgramId);
        return true;
      }
 else {
        return false;
      }
    }
  }
);
  invalidRunRecords=Collections2.filter(invalidRunRecords,new com.google.common.base.Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta invalidRunRecordMeta){
      boolean shouldCorrect=shouldCorrectForWorkflowChildren(invalidRunRecordMeta,processedInvalidRunRecordIds);
      if (!shouldCorrect) {
        LOG.trace(""String_Node_Str"",invalidRunRecordMeta);
        return false;
      }
      return true;
    }
  }
);
  LOG.trace(""String_Node_Str"");
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"" + ""String_Node_Str"",invalidRunRecords.size(),programType.getPrettyName());
  }
 else {
    LOG.trace(""String_Node_Str"" + ""String_Node_Str"",programType.getPrettyName());
  }
  for (  RunRecordMeta invalidRunRecordMeta : invalidRunRecords) {
    String runId=invalidRunRecordMeta.getPid();
    ProgramId targetProgramId=runIdToProgramId.get(runId);
    programStateWriter.error(targetProgramId.run(runId),new Throwable(""String_Node_Str""));
    LOG.warn(""String_Node_Str"" + ""String_Node_Str"",runId,targetProgramId);
    processedInvalidRunRecordIds.add(runId);
  }
}"
4708,"@Inject public DistributedRunRecordCorrectorService(CConfiguration cConf,Store store,ProgramStateWriter programStateWriter,ProgramLifecycleService programLifecycleService,ProgramRuntimeService runtimeService){
  super(store,programStateWriter,programLifecycleService,runtimeService);
  this.cConf=cConf;
}","@Inject public DistributedRunRecordCorrectorService(CConfiguration cConf,Store store,ProgramStateWriter programStateWriter,ProgramLifecycleService programLifecycleService,ProgramRuntimeService runtimeService){
  super(cConf,store,programStateWriter,programLifecycleService,runtimeService);
  this.cConf=cConf;
}"
4709,"@Inject public LocalRunRecordCorrectorService(Store store,ProgramStateWriter programStateWriter,ProgramLifecycleService programLifecycleService,ProgramRuntimeService runtimeService){
  super(store,programStateWriter,programLifecycleService,runtimeService);
}","@Inject public LocalRunRecordCorrectorService(CConfiguration cConf,Store store,ProgramStateWriter programStateWriter,ProgramLifecycleService programLifecycleService,ProgramRuntimeService runtimeService){
  super(cConf,store,programStateWriter,programLifecycleService,runtimeService);
}"
4710,"@Test public void testInvalidFlowRunRecord() throws Exception {
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  final Id.Program wordcountFlow1=Id.Program.from(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW,""String_Node_Str"");
  Assert.assertEquals(""String_Node_Str"",getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow1);
  waitState(wordcountFlow1,ProgramRunStatus.RUNNING.toString());
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return getProgramRuns(wordcountFlow1,ProgramRunStatus.RUNNING).size();
    }
  }
,5,TimeUnit.SECONDS);
  List<RunRecord> runRecords=getProgramRuns(wordcountFlow1,ProgramRunStatus.RUNNING);
  Assert.assertEquals(1,runRecords.size());
  final RunRecord rr=runRecords.get(0);
  Assert.assertEquals(ProgramRunStatus.RUNNING,rr.getStatus());
  RuntimeInfo runtimeInfo=runtimeService.lookup(wordcountFlow1.toEntityId(),RunIds.fromString(rr.getPid()));
  ProgramController programController=runtimeInfo.getController();
  programController.stop();
  Tasks.waitFor(ProgramRunStatus.KILLED,new Callable<ProgramRunStatus>(){
    @Override public ProgramRunStatus call() throws Exception {
      RunRecordMeta runRecord=store.getRun(wordcountFlow1.toEntityId(),rr.getPid());
      return runRecord == null ? null : runRecord.getStatus();
    }
  }
,5,TimeUnit.SECONDS,100,TimeUnit.MILLISECONDS);
  long now=System.currentTimeMillis();
  long nowSecs=TimeUnit.MILLISECONDS.toSeconds(now);
  store.setStart(wordcountFlow1.toEntityId(),rr.getPid(),nowSecs,null,ImmutableMap.<String,String>of(),ImmutableMap.<String,String>of(),ByteBuffer.allocate(0).array());
  store.setRunning(wordcountFlow1.toEntityId(),rr.getPid(),nowSecs + 1,null,ByteBuffer.allocate(1).array());
  RunRecord runRecordMeta=store.getRun(wordcountFlow1.toEntityId(),rr.getPid());
  Assert.assertNotNull(runRecordMeta);
  Assert.assertEquals(ProgramRunStatus.RUNNING,runRecordMeta.getStatus());
  runRecords=getProgramRuns(wordcountFlow1,ProgramRunStatus.FAILED);
  Assert.assertEquals(0,runRecords.size());
  new LocalRunRecordCorrectorService(store,programStateWriter,programLifecycleService,runtimeService).startUp();
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return getProgramRuns(wordcountFlow1,ProgramRunStatus.FAILED).size();
    }
  }
,30,TimeUnit.SECONDS,1,TimeUnit.SECONDS);
}","@Test public void testInvalidFlowRunRecord() throws Exception {
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  final Id.Program wordcountFlow1=Id.Program.from(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW,""String_Node_Str"");
  Assert.assertEquals(""String_Node_Str"",getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow1);
  waitState(wordcountFlow1,ProgramRunStatus.RUNNING.toString());
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return getProgramRuns(wordcountFlow1,ProgramRunStatus.RUNNING).size();
    }
  }
,5,TimeUnit.SECONDS);
  List<RunRecord> runRecords=getProgramRuns(wordcountFlow1,ProgramRunStatus.RUNNING);
  Assert.assertEquals(1,runRecords.size());
  final RunRecord rr=runRecords.get(0);
  Assert.assertEquals(ProgramRunStatus.RUNNING,rr.getStatus());
  RuntimeInfo runtimeInfo=runtimeService.lookup(wordcountFlow1.toEntityId(),RunIds.fromString(rr.getPid()));
  ProgramController programController=runtimeInfo.getController();
  programController.stop();
  Tasks.waitFor(ProgramRunStatus.KILLED,new Callable<ProgramRunStatus>(){
    @Override public ProgramRunStatus call() throws Exception {
      RunRecordMeta runRecord=store.getRun(wordcountFlow1.toEntityId(),rr.getPid());
      return runRecord == null ? null : runRecord.getStatus();
    }
  }
,5,TimeUnit.SECONDS,100,TimeUnit.MILLISECONDS);
  long now=System.currentTimeMillis();
  long nowSecs=TimeUnit.MILLISECONDS.toSeconds(now);
  store.setStart(wordcountFlow1.toEntityId(),rr.getPid(),nowSecs,null,ImmutableMap.<String,String>of(),ImmutableMap.<String,String>of(),ByteBuffer.allocate(0).array());
  store.setRunning(wordcountFlow1.toEntityId(),rr.getPid(),nowSecs + 1,null,ByteBuffer.allocate(1).array());
  RunRecord runRecordMeta=store.getRun(wordcountFlow1.toEntityId(),rr.getPid());
  Assert.assertNotNull(runRecordMeta);
  Assert.assertEquals(ProgramRunStatus.RUNNING,runRecordMeta.getStatus());
  runRecords=getProgramRuns(wordcountFlow1,ProgramRunStatus.FAILED);
  Assert.assertEquals(0,runRecords.size());
  new LocalRunRecordCorrectorService(CConfiguration.create(),store,programStateWriter,programLifecycleService,runtimeService).startUp();
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return getProgramRuns(wordcountFlow1,ProgramRunStatus.FAILED).size();
    }
  }
,30,TimeUnit.SECONDS,1,TimeUnit.SECONDS);
}"
4711,"private static void checkParts(EntityType entityType,List<String> parts,int index,Map<EntityType,String> entityParts){
switch (entityType) {
case INSTANCE:
case NAMESPACE:
    entityParts.put(entityType,parts.get(index));
  break;
case KERBEROSPRINCIPAL:
entityParts.put(entityType,parts.get(index));
break;
case ARTIFACT:
case APPLICATION:
if (parts.size() > 2 && index == (parts.size() - 1)) {
throw new UnsupportedOperationException(""String_Node_Str"" + ""String_Node_Str"" + parts);
}
checkParts(EntityType.NAMESPACE,parts,index - 1,entityParts);
entityParts.put(entityType,parts.get(index));
break;
case DATASET:
case DATASET_MODULE:
case DATASET_TYPE:
case STREAM:
case SECUREKEY:
checkParts(EntityType.NAMESPACE,parts,index - 1,entityParts);
entityParts.put(entityType,parts.get(index));
break;
case PROGRAM:
if (parts.size() > 4 && index == (parts.size() - 1)) {
throw new UnsupportedOperationException(""String_Node_Str"" + ""String_Node_Str"" + parts);
}
checkParts(EntityType.APPLICATION,parts,index - 2,entityParts);
entityParts.put(entityType,parts.get(index - 1) + ""String_Node_Str"" + parts.get(index));
break;
default :
throw new IllegalArgumentException(String.format(""String_Node_Str"",entityType));
}
}","private static void checkParts(EntityType entityType,List<String> parts,int index,Map<EntityType,String> entityParts){
switch (entityType) {
case INSTANCE:
case NAMESPACE:
    entityParts.put(entityType,parts.get(index));
  break;
case KERBEROSPRINCIPAL:
entityParts.put(entityType,parts.get(index));
break;
case ARTIFACT:
case APPLICATION:
if (parts.size() > 2 && index == (parts.size() - 1)) {
throw new UnsupportedOperationException(""String_Node_Str"" + ""String_Node_Str"" + parts);
}
checkParts(EntityType.NAMESPACE,parts,index - 1,entityParts);
entityParts.put(entityType,parts.get(index));
break;
case DATASET:
case DATASET_MODULE:
case DATASET_TYPE:
case STREAM:
case SECUREKEY:
checkParts(EntityType.NAMESPACE,parts,index - 1,entityParts);
entityParts.put(entityType,parts.get(index));
break;
case PROGRAM:
if (parts.size() > 4 && index == (parts.size() - 1)) {
throw new UnsupportedOperationException(""String_Node_Str"" + ""String_Node_Str"" + parts);
}
if (parts.size() == 3 && index == (parts.size() - 1)) {
String program=parts.get(index);
if (!""String_Node_Str"".equals(program)) {
throw new UnsupportedOperationException(""String_Node_Str"" + ""String_Node_Str"");
}
checkParts(EntityType.APPLICATION,parts,index - 1,entityParts);
entityParts.put(entityType,parts.get(index));
}
 else {
checkParts(EntityType.APPLICATION,parts,index - 2,entityParts);
entityParts.put(entityType,parts.get(index - 1) + ""String_Node_Str"" + parts.get(index));
}
break;
default :
throw new IllegalArgumentException(String.format(""String_Node_Str"",entityType));
}
}"
4712,"/** 
 * Constructs an   {@link Authorizable} from the given entityString. The entityString must be a representation of anentity similar to  {@link EntityId#toString()} with the exception that the string can contain wildcards (? and *).Note: <ol> <li> The only validation that this class performs on the entityString is that it checks if the string has enough valid parts for the given  {@link #entityType}. It does not check if the entity exists or not. This is required to allow pre grants. </li> <li> CDAP Authorization does not support authorization on versions of   {@link co.cask.cdap.proto.id.ApplicationId}and   {@link co.cask.cdap.proto.id.ArtifactId}. If a version is included while construction an Authorizable through   {@link #fromString(String)} an {@link IllegalArgumentException} will be thrown.</li> </ol>
 * @param entityString the {@link EntityId#toString()} of the entity which may or may not contains wildcards(? or *)
 */
public static Authorizable fromString(String entityString){
  if (entityString == null || entityString.isEmpty()) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  String[] typeAndId=entityString.split(EntityId.IDSTRING_TYPE_SEPARATOR,2);
  if (typeAndId.length != 2) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",entityString));
  }
  String typeString=typeAndId[0];
  EntityType type=EntityType.valueOf(typeString.toUpperCase());
  String idString=typeAndId[1];
  List<String> idParts;
  if (type != EntityType.KERBEROSPRINCIPAL) {
    idParts=Arrays.asList(EntityId.IDSTRING_PART_SEPARATOR_PATTERN.split(idString));
  }
 else {
    idParts=Collections.singletonList(idString);
  }
  Map<EntityType,String> entityParts=new LinkedHashMap<>();
  checkParts(type,idParts,idParts.size() - 1,entityParts);
  return new Authorizable(type,entityParts);
}","/** 
 * Constructs an   {@link Authorizable} from the given entityString. The entityString must be a representation of anentity similar to  {@link EntityId#toString()} with the exception that the string can contain wildcards (? and *).Note: <ol> <li> The only validation that this class performs on the entityString is that it checks if the string has enough valid parts for the given  {@link #entityType}. It does not check if the entity exists or not. This is required to allow pre grants. </li> <li> CDAP Authorization does not support authorization on versions of   {@link co.cask.cdap.proto.id.ApplicationId}and   {@link co.cask.cdap.proto.id.ArtifactId}. If a version is included while construction an Authorizable through   {@link #fromString(String)} an {@link IllegalArgumentException} will be thrown.</li> </ol>
 * @param entityString the {@link EntityId#toString()} of the entity which may or may not contains wildcards(? or *)
 */
public static Authorizable fromString(String entityString){
  if (entityString == null || entityString.isEmpty()) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  String[] typeAndId=entityString.split(EntityId.IDSTRING_TYPE_SEPARATOR,2);
  if (typeAndId.length != 2) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",entityString));
  }
  String typeString=typeAndId[0];
  EntityType type=EntityType.valueOf(typeString.toUpperCase());
  String idString=typeAndId[1];
  List<String> idParts=Collections.emptyList();
switch (type) {
case KERBEROSPRINCIPAL:
    idParts=Collections.singletonList(idString);
  break;
case DATASET:
case DATASET_TYPE:
case DATASET_MODULE:
int namespaceSeparatorPos=idString.indexOf(EntityId.IDSTRING_PART_SEPARATOR);
if (namespaceSeparatorPos > 0) {
idParts=new ArrayList<>();
idParts.add(idString.substring(0,namespaceSeparatorPos));
idParts.add(idString.substring(namespaceSeparatorPos + 1));
}
break;
default :
idParts=Arrays.asList(EntityId.IDSTRING_PART_SEPARATOR_PATTERN.split(idString));
}
Map<EntityType,String> entityParts=new LinkedHashMap<>();
checkParts(type,idParts,idParts.size() - 1,entityParts);
return new Authorizable(type,entityParts);
}"
4713,"@Test public void testArtifact(){
  Authorizable authorizable;
  ArtifactId artifactId=new ArtifactId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  authorizable=Authorizable.fromEntityId(artifactId);
  Assert.assertEquals(artifactId.toString().replace(""String_Node_Str"",""String_Node_Str""),authorizable.toString());
}","@Test public void testArtifact(){
  Authorizable authorizable;
  ArtifactId artifactId=new ArtifactId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  authorizable=Authorizable.fromEntityId(artifactId);
  String artifactIdNoVer=artifactId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(artifactIdNoVer,authorizable.toString());
  String widcardId=artifactIdNoVer.replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
}"
4714,"@Test public void testSecureKey(){
  SecureKeyId secureKeyId=new SecureKeyId(""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(secureKeyId);
  Assert.assertEquals(secureKeyId.toString(),authorizable.toString());
}","@Test public void testSecureKey(){
  SecureKeyId secureKeyId=new SecureKeyId(""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(secureKeyId);
  Assert.assertEquals(secureKeyId.toString(),authorizable.toString());
  String widcardId=secureKeyId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
}"
4715,"@Test public void testPrincipal(){
  KerberosPrincipalId kerberosPrincipalId=new KerberosPrincipalId(""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(kerberosPrincipalId);
  Assert.assertEquals(kerberosPrincipalId.toString(),authorizable.toString());
}","@Test public void testPrincipal(){
  KerberosPrincipalId kerberosPrincipalId=new KerberosPrincipalId(""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(kerberosPrincipalId);
  Assert.assertEquals(kerberosPrincipalId.toString(),authorizable.toString());
  Assert.assertEquals(kerberosPrincipalId.toString() + ""String_Node_Str"",Authorizable.fromString(authorizable.toString() + ""String_Node_Str"").toString());
}"
4716,"@Test public void testNamespace(){
  Authorizable authorizable;
  NamespaceId namespaceId=new NamespaceId(""String_Node_Str"");
  authorizable=Authorizable.fromEntityId(namespaceId);
  Assert.assertEquals(namespaceId.toString(),authorizable.toString());
  String wildcardNs=namespaceId.toString() + ""String_Node_Str"";
  authorizable=Authorizable.fromString(wildcardNs);
  Assert.assertEquals(wildcardNs,authorizable.toString());
  wildcardNs=namespaceId.toString() + ""String_Node_Str"" + ""String_Node_Str"";
  authorizable=Authorizable.fromString(wildcardNs);
  Assert.assertEquals(wildcardNs,authorizable.toString());
}","@Test public void testNamespace(){
  Authorizable authorizable;
  NamespaceId namespaceId=new NamespaceId(""String_Node_Str"");
  authorizable=Authorizable.fromEntityId(namespaceId);
  Assert.assertEquals(namespaceId.toString(),authorizable.toString());
  String wildcardNs=namespaceId.toString() + ""String_Node_Str"";
  authorizable=Authorizable.fromString(wildcardNs);
  Assert.assertEquals(wildcardNs,authorizable.toString());
  wildcardNs=namespaceId.toString() + ""String_Node_Str"" + ""String_Node_Str"";
  authorizable=Authorizable.fromString(wildcardNs);
  Assert.assertEquals(wildcardNs,authorizable.toString());
  String widcardId=namespaceId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
}"
4717,"@Test public void testStream(){
  StreamId streamId=new StreamId(""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(streamId);
  Assert.assertEquals(streamId.toString(),authorizable.toString());
}","@Test public void testStream(){
  StreamId streamId=new StreamId(""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(streamId);
  Assert.assertEquals(streamId.toString(),authorizable.toString());
  String widcardId=streamId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
}"
4718,"@Test public void testDataset(){
  DatasetId datasetId=new DatasetId(""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(datasetId);
  Assert.assertEquals(datasetId.toString(),authorizable.toString());
}","@Test public void testDataset(){
  DatasetId datasetId=new DatasetId(""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(datasetId);
  Assert.assertEquals(datasetId.toString(),authorizable.toString());
  String widcardId=datasetId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
}"
4719,"@Test public void testProgram(){
  ProgramId programId=new ProgramId(""String_Node_Str"",""String_Node_Str"",ProgramType.MAPREDUCE,""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(programId);
  Assert.assertEquals(programId.toString().replace(ApplicationId.DEFAULT_VERSION + ""String_Node_Str"",""String_Node_Str""),authorizable.toString());
  String wildCardProgramId=programId.toString() + ""String_Node_Str"";
  try {
    Authorizable.fromString(wildCardProgramId);
    Assert.fail();
  }
 catch (  UnsupportedOperationException e) {
  }
  ApplicationId appId=new ApplicationId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  programId=appId.program(ProgramType.MAPREDUCE,""String_Node_Str"");
  authorizable=Authorizable.fromEntityId(programId);
  Assert.assertEquals(programId.toString().replace(""String_Node_Str"",""String_Node_Str""),authorizable.toString());
}","@Test public void testProgram(){
  ProgramId programId=new ProgramId(""String_Node_Str"",""String_Node_Str"",ProgramType.MAPREDUCE,""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(programId);
  Assert.assertEquals(programId.toString().replace(ApplicationId.DEFAULT_VERSION + ""String_Node_Str"",""String_Node_Str""),authorizable.toString());
  String wildCardProgramId=programId.toString() + ""String_Node_Str"";
  try {
    Authorizable.fromString(wildCardProgramId);
    Assert.fail();
  }
 catch (  UnsupportedOperationException e) {
  }
  ApplicationId appId=new ApplicationId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  programId=appId.program(ProgramType.MAPREDUCE,""String_Node_Str"");
  authorizable=Authorizable.fromEntityId(programId);
  String programIdNoVer=programId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(programIdNoVer,authorizable.toString());
  String widcardId=programIdNoVer.replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
  String allProgs=""String_Node_Str"";
  Assert.assertEquals(allProgs,Authorizable.fromString(allProgs).toString());
}"
4720,"@Test public void testApplication(){
  ApplicationId appId=new ApplicationId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(appId);
  Assert.assertEquals(appId.toString().replace(""String_Node_Str"",""String_Node_Str""),authorizable.toString());
  try {
    Authorizable.fromString(appId.toString());
    Assert.fail();
  }
 catch (  UnsupportedOperationException e) {
  }
}","@Test public void testApplication(){
  ApplicationId appId=new ApplicationId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(appId);
  String appIdNoVer=appId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(appIdNoVer,authorizable.toString());
  try {
    Authorizable.fromString(appId.toString());
    Assert.fail();
  }
 catch (  UnsupportedOperationException e) {
  }
  String widcardId=appIdNoVer.replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
}"
4721,"@Test public void testDatasetModule(){
  DatasetModuleId datasetModuleId=new DatasetModuleId(""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(datasetModuleId);
  Assert.assertEquals(datasetModuleId.toString(),authorizable.toString());
}","@Test public void testDatasetModule(){
  DatasetModuleId datasetModuleId=new DatasetModuleId(""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(datasetModuleId);
  Assert.assertEquals(datasetModuleId.toString(),authorizable.toString());
  String widcardId=datasetModuleId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
}"
4722,"@Test public void testDatasetType(){
  DatasetTypeId datasetTypeId=new DatasetTypeId(""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(datasetTypeId);
  Assert.assertEquals(datasetTypeId.toString(),authorizable.toString());
}","@Test public void testDatasetType(){
  DatasetTypeId datasetTypeId=new DatasetTypeId(""String_Node_Str"",""String_Node_Str"");
  Authorizable authorizable=Authorizable.fromEntityId(datasetTypeId);
  Assert.assertEquals(datasetTypeId.toString(),authorizable.toString());
  String widcardId=datasetTypeId.toString().replace(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(widcardId,Authorizable.fromString(widcardId).toString());
}"
4723,"/** 
 * Returns an Output defined by an OutputFormatProvider.
 * @param outputName the name of the output
 */
public static Output of(String outputName,OutputFormatProvider outputFormatProvider){
  return new OutputFormatProviderOutput(outputName,outputFormatProvider);
}","/** 
 * Returns an Output defined by an OutputFormatProvider.
 * @param outputName the name of the output
 * @param outputFormatProvider an instance of an OutputFormatProvider. It can not be an instance ofa  {@link DatasetOutputCommitter}.
 */
public static Output of(String outputName,OutputFormatProvider outputFormatProvider){
  return new OutputFormatProviderOutput(outputName,outputFormatProvider);
}"
4724,"/** 
 * @return a map from output name to provided output for the MapReduce job
 */
Map<String,ProvidedOutput> getOutputs(){
  return new LinkedHashMap<>(outputs);
}","/** 
 * @return a map from output name to provided output for the MapReduce job
 */
List<ProvidedOutput> getOutputs(){
  return new ArrayList<>(outputs.values());
}"
4725,"@Override public void addOutput(Output output){
  if (output instanceof Output.DatasetOutput) {
    Output.DatasetOutput datasetOutput=((Output.DatasetOutput)output);
    String datasetNamespace=datasetOutput.getNamespace();
    if (datasetNamespace == null) {
      datasetNamespace=getNamespace();
    }
    String datasetName=output.getName();
    Map<String,String> arguments=((Output.DatasetOutput)output).getArguments();
    DatasetOutputFormatProvider outputFormatProvider=new DatasetOutputFormatProvider(datasetNamespace,datasetName,arguments,getDataset(datasetNamespace,datasetName,arguments,AccessType.WRITE),MapReduceBatchWritableOutputFormat.class);
    addOutput(output.getAlias(),outputFormatProvider);
  }
 else   if (output instanceof Output.OutputFormatProviderOutput) {
    addOutput(output.getAlias(),((Output.OutputFormatProviderOutput)output).getOutputFormatProvider());
  }
 else {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",output.getName(),output.getClass().getCanonicalName()));
  }
}","@Override public void addOutput(Output output){
  String alias=output.getAlias();
  if (this.outputs.containsKey(alias)) {
    throw new IllegalArgumentException(""String_Node_Str"" + alias);
  }
  ProvidedOutput providedOutput;
  if (output instanceof Output.DatasetOutput) {
    providedOutput=Outputs.transform((Output.DatasetOutput)output,this);
  }
 else   if (output instanceof Output.OutputFormatProviderOutput) {
    OutputFormatProvider outputFormatProvider=((Output.OutputFormatProviderOutput)output).getOutputFormatProvider();
    if (outputFormatProvider instanceof DatasetOutputCommitter) {
      throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"");
    }
    providedOutput=new ProvidedOutput(output,outputFormatProvider);
  }
 else {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",output.getName(),output.getClass().getCanonicalName()));
  }
  this.outputs.put(alias,providedOutput);
}"
4726,"@Override public void destroy(){
  WorkflowContext workflowContext=getContext();
  if (!workflowContext.getDataTracer(PostAction.PLUGIN_TYPE).isEnabled()) {
    PipelineRuntime pipelineRuntime=new PipelineRuntime(workflowContext,workflowMetrics);
    for (    Map.Entry<String,PostAction> endingActionEntry : postActions.entrySet()) {
      String name=endingActionEntry.getKey();
      PostAction action=endingActionEntry.getValue();
      StageSpec stageSpec=stageSpecs.get(name);
      BatchActionContext context=new WorkflowBackedActionContext(workflowContext,pipelineRuntime,stageSpec);
      try {
        action.run(context);
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",name,t);
      }
    }
  }
  for (  Map.Entry<String,AlertPublisher> alertPublisherEntry : alertPublishers.entrySet()) {
    String name=alertPublisherEntry.getKey();
    AlertPublisher alertPublisher=alertPublisherEntry.getValue();
    PartitionedFileSet alertConnector=workflowContext.getDataset(name);
    try (CloseableIterator<Alert> alerts=new AlertReader(alertConnector.getPartitions(PartitionFilter.ALWAYS_MATCH))){
      StageMetrics stageMetrics=new DefaultStageMetrics(workflowMetrics,name);
      TrackedIterator<Alert> trackedIterator=new TrackedIterator<>(alerts,stageMetrics,Constants.Metrics.RECORDS_IN);
      alertPublisher.publish(trackedIterator);
    }
 catch (    Exception e) {
      LOG.warn(""String_Node_Str"",name,e);
    }
 finally {
      try {
        alertPublisher.destroy();
      }
 catch (      Exception e) {
        LOG.warn(""String_Node_Str"",name,e);
      }
    }
  }
  ProgramStatus status=getContext().getState().getStatus();
  if (status == ProgramStatus.FAILED) {
    WRAPPERLOGGER.error(""String_Node_Str"",getContext().getApplicationSpecification().getName());
  }
 else {
    WRAPPERLOGGER.info(""String_Node_Str"",getContext().getApplicationSpecification().getName(),status == ProgramStatus.COMPLETED ? ""String_Node_Str"" : status.name().toLowerCase());
  }
}","@Override public void destroy(){
  WorkflowContext workflowContext=getContext();
  PipelineRuntime pipelineRuntime=new PipelineRuntime(workflowContext,workflowMetrics);
  if (!workflowContext.getDataTracer(PostAction.PLUGIN_TYPE).isEnabled()) {
    for (    Map.Entry<String,PostAction> endingActionEntry : postActions.entrySet()) {
      String name=endingActionEntry.getKey();
      PostAction action=endingActionEntry.getValue();
      StageSpec stageSpec=stageSpecs.get(name);
      BatchActionContext context=new WorkflowBackedActionContext(workflowContext,pipelineRuntime,stageSpec);
      try {
        action.run(context);
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",name,t);
      }
    }
  }
  for (  Map.Entry<String,AlertPublisher> alertPublisherEntry : alertPublishers.entrySet()) {
    String name=alertPublisherEntry.getKey();
    AlertPublisher alertPublisher=alertPublisherEntry.getValue();
    PartitionedFileSet alertConnector=workflowContext.getDataset(name);
    try (CloseableIterator<Alert> alerts=new AlertReader(alertConnector.getPartitions(PartitionFilter.ALWAYS_MATCH))){
      if (!alerts.hasNext()) {
        continue;
      }
      StageMetrics stageMetrics=new DefaultStageMetrics(workflowMetrics,name);
      StageSpec stageSpec=stageSpecs.get(name);
      AlertPublisherContext alertContext=new DefaultAlertPublisherContext(pipelineRuntime,stageSpec,workflowContext,workflowContext.getAdmin());
      alertPublisher.initialize(alertContext);
      TrackedIterator<Alert> trackedIterator=new TrackedIterator<>(alerts,stageMetrics,Constants.Metrics.RECORDS_IN);
      alertPublisher.publish(trackedIterator);
    }
 catch (    Exception e) {
      LOG.warn(""String_Node_Str"",name,e);
    }
 finally {
      try {
        alertPublisher.destroy();
      }
 catch (      Exception e) {
        LOG.warn(""String_Node_Str"",name,e);
      }
    }
  }
  ProgramStatus status=getContext().getState().getStatus();
  if (status == ProgramStatus.FAILED) {
    WRAPPERLOGGER.error(""String_Node_Str"",getContext().getApplicationSpecification().getName());
  }
 else {
    WRAPPERLOGGER.info(""String_Node_Str"",getContext().getApplicationSpecification().getName(),status == ProgramStatus.COMPLETED ? ""String_Node_Str"" : status.name().toLowerCase());
  }
}"
4727,"@Override public void initialize(WorkflowContext context) throws Exception {
  super.initialize(context);
  String arguments=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(context.getRuntimeArguments());
  WRAPPERLOGGER.info(""String_Node_Str"",context.getApplicationSpecification().getName(),UserGroupInformation.getCurrentUser().getShortUserName(),arguments);
  alertPublishers=new HashMap<>();
  postActions=new LinkedHashMap<>();
  spec=GSON.fromJson(context.getWorkflowSpecification().getProperty(Constants.PIPELINE_SPEC_KEY),BatchPipelineSpec.class);
  stageSpecs=new HashMap<>();
  MacroEvaluator macroEvaluator=new DefaultMacroEvaluator(context.getToken(),context.getRuntimeArguments(),context.getLogicalStartTime(),context,context.getNamespace());
  PipelineRuntime pipelineRuntime=new PipelineRuntime(context,workflowMetrics);
  PluginContext pluginContext=new PipelinePluginContext(context,workflowMetrics,spec.isStageLoggingEnabled(),spec.isProcessTimingEnabled());
  for (  ActionSpec actionSpec : spec.getEndingActions()) {
    String stageName=actionSpec.getName();
    postActions.put(stageName,(PostAction)pluginContext.newPluginInstance(stageName,macroEvaluator));
    stageSpecs.put(stageName,StageSpec.builder(stageName,actionSpec.getPluginSpec()).setStageLoggingEnabled(spec.isStageLoggingEnabled()).setProcessTimingEnabled(spec.isProcessTimingEnabled()).build());
  }
  for (  StageSpec stageSpec : spec.getStages()) {
    String stageName=stageSpec.getName();
    stageSpecs.put(stageName,stageSpec);
    if (AlertPublisher.PLUGIN_TYPE.equals(stageSpec.getPluginType())) {
      AlertPublisher alertPublisher=context.newPluginInstance(stageName,macroEvaluator);
      AlertPublisherContext alertContext=new DefaultAlertPublisherContext(pipelineRuntime,stageSpec,context,context.getAdmin());
      alertPublisher.initialize(alertContext);
      alertPublishers.put(stageName,alertPublisher);
    }
  }
  WRAPPERLOGGER.info(""String_Node_Str"",context.getApplicationSpecification().getName());
}","@Override public void initialize(WorkflowContext context) throws Exception {
  super.initialize(context);
  String arguments=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(context.getRuntimeArguments());
  WRAPPERLOGGER.info(""String_Node_Str"",context.getApplicationSpecification().getName(),UserGroupInformation.getCurrentUser().getShortUserName(),arguments);
  alertPublishers=new HashMap<>();
  postActions=new LinkedHashMap<>();
  spec=GSON.fromJson(context.getWorkflowSpecification().getProperty(Constants.PIPELINE_SPEC_KEY),BatchPipelineSpec.class);
  stageSpecs=new HashMap<>();
  MacroEvaluator macroEvaluator=new DefaultMacroEvaluator(context.getToken(),context.getRuntimeArguments(),context.getLogicalStartTime(),context,context.getNamespace());
  PluginContext pluginContext=new PipelinePluginContext(context,workflowMetrics,spec.isStageLoggingEnabled(),spec.isProcessTimingEnabled());
  for (  ActionSpec actionSpec : spec.getEndingActions()) {
    String stageName=actionSpec.getName();
    postActions.put(stageName,(PostAction)pluginContext.newPluginInstance(stageName,macroEvaluator));
    stageSpecs.put(stageName,StageSpec.builder(stageName,actionSpec.getPluginSpec()).setStageLoggingEnabled(spec.isStageLoggingEnabled()).setProcessTimingEnabled(spec.isProcessTimingEnabled()).build());
  }
  for (  StageSpec stageSpec : spec.getStages()) {
    String stageName=stageSpec.getName();
    stageSpecs.put(stageName,stageSpec);
    if (AlertPublisher.PLUGIN_TYPE.equals(stageSpec.getPluginType())) {
      AlertPublisher alertPublisher=context.newPluginInstance(stageName,macroEvaluator);
      alertPublishers.put(stageName,alertPublisher);
    }
  }
  WRAPPERLOGGER.info(""String_Node_Str"",context.getApplicationSpecification().getName());
}"
4728,"@Override public void truncate() throws IOException {
  drop();
  create(true);
}","@Override public void truncate() throws IOException {
  if (!isExternal && !useExisting) {
    for (    Location child : baseLocation.list()) {
      child.delete(true);
    }
  }
}"
4729,"/** 
 * @param truncating whether this call to create() is part of a truncate() operation. The effect is:If possessExisting is true, then the truncate() has just dropped this dataset and that deleted the base directory: we must recreate it.
 */
private void create(boolean truncating) throws IOException {
  if (isExternal) {
    validateExists(FileSetProperties.DATA_EXTERNAL);
  }
 else   if (useExisting) {
    validateExists(FileSetProperties.DATA_USE_EXISTING);
  }
 else   if (!truncating && possessExisting) {
    validateExists(FileSetProperties.DATA_POSSESS_EXISTING);
  }
 else {
    if (exists()) {
      throw new IOException(String.format(""String_Node_Str"",spec.getName(),baseLocation));
    }
    String permissions=FileSetProperties.getFilePermissions(spec.getProperties());
    String group=FileSetProperties.getFileGroup(spec.getProperties());
    if (group == null) {
      String[] groups=UserGroupInformation.getCurrentUser().getGroupNames();
      if (groups.length > 0) {
        group=groups[0];
      }
    }
    Location ancestor=baseLocation;
    Location firstDirToCreate=null;
    while (ancestor != null && !ancestor.exists()) {
      firstDirToCreate=ancestor;
      ancestor=Locations.getParent(ancestor);
    }
    if (firstDirToCreate != null) {
      if (null == permissions) {
        firstDirToCreate.mkdirs();
      }
 else {
        firstDirToCreate.mkdirs(permissions);
      }
      if (group != null) {
        try {
          firstDirToCreate.setGroup(group);
        }
 catch (        Exception e) {
          LOG.warn(""String_Node_Str"",group,firstDirToCreate.toURI().toString(),spec.getName(),e.getMessage());
        }
      }
      if (null == permissions) {
        baseLocation.mkdirs();
      }
 else {
        baseLocation.mkdirs(permissions);
      }
    }
  }
}","@Override public void create() throws IOException {
  if (isExternal) {
    validateExists(FileSetProperties.DATA_EXTERNAL);
  }
 else   if (useExisting) {
    validateExists(FileSetProperties.DATA_USE_EXISTING);
  }
 else   if (possessExisting) {
    validateExists(FileSetProperties.DATA_POSSESS_EXISTING);
  }
 else {
    if (exists()) {
      throw new IOException(String.format(""String_Node_Str"",spec.getName(),baseLocation));
    }
    String permissions=FileSetProperties.getFilePermissions(spec.getProperties());
    String group=FileSetProperties.getFileGroup(spec.getProperties());
    if (group == null) {
      String[] groups=UserGroupInformation.getCurrentUser().getGroupNames();
      if (groups.length > 0) {
        group=groups[0];
      }
    }
    Location ancestor=baseLocation;
    Location firstDirToCreate=null;
    while (ancestor != null && !ancestor.exists()) {
      firstDirToCreate=ancestor;
      ancestor=Locations.getParent(ancestor);
    }
    if (firstDirToCreate != null) {
      if (null == permissions) {
        firstDirToCreate.mkdirs();
      }
 else {
        firstDirToCreate.mkdirs(permissions);
      }
      if (group != null) {
        try {
          firstDirToCreate.setGroup(group);
        }
 catch (        Exception e) {
          LOG.warn(""String_Node_Str"",group,firstDirToCreate.toURI().toString(),spec.getName(),e.getMessage());
        }
      }
      if (null == permissions) {
        baseLocation.mkdirs();
      }
 else {
        baseLocation.mkdirs(permissions);
      }
    }
  }
}"
4730,"@Test public void testPossessDoesDelete() throws IOException, DatasetManagementException {
  String existingPath=tmpFolder.newFolder() + ""String_Node_Str"";
  File existingDir=new File(existingPath);
  existingDir.mkdirs();
  File someFile=new File(existingDir,""String_Node_Str"");
  someFile.createNewFile();
  dsFrameworkUtil.createInstance(""String_Node_Str"",testFileSetInstance5,FileSetProperties.builder().setBasePath(existingPath).setPossessExisting(true).build());
  Assert.assertTrue(someFile.exists());
  dsFrameworkUtil.getFramework().truncateInstance(testFileSetInstance5);
  Assert.assertFalse(someFile.exists());
  Assert.assertTrue(existingDir.exists());
  someFile.createNewFile();
  dsFrameworkUtil.getFramework().deleteInstance(testFileSetInstance5);
  Assert.assertFalse(someFile.exists());
  Assert.assertFalse(existingDir.exists());
}","@Test public void testPossessDoesDelete() throws IOException, DatasetManagementException {
  String existingPath=tmpFolder.newFolder() + ""String_Node_Str"";
  File existingDir=new File(existingPath);
  existingDir.mkdirs();
  File someFile=new File(existingDir,""String_Node_Str"");
  someFile.createNewFile();
  dsFrameworkUtil.createInstance(""String_Node_Str"",testFileSetInstance5,FileSetProperties.builder().setBasePath(existingPath).setPossessExisting(true).build());
  Assert.assertTrue(someFile.exists());
  FileSet fs=dsFrameworkUtil.getInstance(testFileSetInstance5);
  Location base=fs.getBaseLocation();
  String permissions=base.getPermissions();
  char groupWriteFlag=permissions.charAt(4);
  char toggledGroupWriteFlag=groupWriteFlag == 'w' ? '-' : 'w';
  String toggledPermissions=permissions.substring(0,4) + toggledGroupWriteFlag + permissions.substring(5,9);
  base.setPermissions(toggledPermissions);
  dsFrameworkUtil.getFramework().truncateInstance(testFileSetInstance5);
  Assert.assertFalse(someFile.exists());
  Assert.assertTrue(existingDir.exists());
  Assert.assertEquals(toggledPermissions,base.getPermissions());
  someFile.createNewFile();
  dsFrameworkUtil.getFramework().deleteInstance(testFileSetInstance5);
  Assert.assertFalse(someFile.exists());
  Assert.assertFalse(existingDir.exists());
}"
4731,"@Override public void destroy(){
  WorkflowContext workflowContext=getContext();
  if (!workflowContext.getDataTracer(PostAction.PLUGIN_TYPE).isEnabled()) {
    PipelineRuntime pipelineRuntime=new PipelineRuntime(workflowContext,workflowMetrics);
    for (    Map.Entry<String,PostAction> endingActionEntry : postActions.entrySet()) {
      String name=endingActionEntry.getKey();
      PostAction action=endingActionEntry.getValue();
      StageSpec stageSpec=stageSpecs.get(name);
      BatchActionContext context=new WorkflowBackedActionContext(workflowContext,pipelineRuntime,stageSpec);
      try {
        action.run(context);
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",name,t);
      }
    }
  }
  for (  Map.Entry<String,AlertPublisher> alertPublisherEntry : alertPublishers.entrySet()) {
    String name=alertPublisherEntry.getKey();
    AlertPublisher alertPublisher=alertPublisherEntry.getValue();
    PartitionedFileSet alertConnector=workflowContext.getDataset(name);
    try (CloseableIterator<Alert> alerts=new AlertReader(alertConnector.getPartitions(PartitionFilter.ALWAYS_MATCH))){
      StageMetrics stageMetrics=new DefaultStageMetrics(workflowMetrics,name);
      TrackedIterator<Alert> trackedIterator=new TrackedIterator<>(alerts,stageMetrics,Constants.Metrics.RECORDS_IN);
      alertPublisher.publish(trackedIterator);
    }
 catch (    Exception e) {
      LOG.warn(""String_Node_Str"",name,e);
    }
  }
  ProgramStatus status=getContext().getState().getStatus();
  if (status == ProgramStatus.FAILED) {
    WRAPPERLOGGER.error(""String_Node_Str"",getContext().getApplicationSpecification().getName());
  }
 else {
    WRAPPERLOGGER.info(""String_Node_Str"",getContext().getApplicationSpecification().getName(),status == ProgramStatus.COMPLETED ? ""String_Node_Str"" : status.name().toLowerCase());
  }
}","@Override public void destroy(){
  WorkflowContext workflowContext=getContext();
  if (!workflowContext.getDataTracer(PostAction.PLUGIN_TYPE).isEnabled()) {
    PipelineRuntime pipelineRuntime=new PipelineRuntime(workflowContext,workflowMetrics);
    for (    Map.Entry<String,PostAction> endingActionEntry : postActions.entrySet()) {
      String name=endingActionEntry.getKey();
      PostAction action=endingActionEntry.getValue();
      StageSpec stageSpec=stageSpecs.get(name);
      BatchActionContext context=new WorkflowBackedActionContext(workflowContext,pipelineRuntime,stageSpec);
      try {
        action.run(context);
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",name,t);
      }
    }
  }
  for (  Map.Entry<String,AlertPublisher> alertPublisherEntry : alertPublishers.entrySet()) {
    String name=alertPublisherEntry.getKey();
    AlertPublisher alertPublisher=alertPublisherEntry.getValue();
    PartitionedFileSet alertConnector=workflowContext.getDataset(name);
    try (CloseableIterator<Alert> alerts=new AlertReader(alertConnector.getPartitions(PartitionFilter.ALWAYS_MATCH))){
      StageMetrics stageMetrics=new DefaultStageMetrics(workflowMetrics,name);
      TrackedIterator<Alert> trackedIterator=new TrackedIterator<>(alerts,stageMetrics,Constants.Metrics.RECORDS_IN);
      alertPublisher.publish(trackedIterator);
    }
 catch (    Exception e) {
      LOG.warn(""String_Node_Str"",name,e);
    }
 finally {
      try {
        alertPublisher.destroy();
      }
 catch (      Exception e) {
        LOG.warn(""String_Node_Str"",name,e);
      }
    }
  }
  ProgramStatus status=getContext().getState().getStatus();
  if (status == ProgramStatus.FAILED) {
    WRAPPERLOGGER.error(""String_Node_Str"",getContext().getApplicationSpecification().getName());
  }
 else {
    WRAPPERLOGGER.info(""String_Node_Str"",getContext().getApplicationSpecification().getName(),status == ProgramStatus.COMPLETED ? ""String_Node_Str"" : status.name().toLowerCase());
  }
}"
4732,"@Override public void destroy(){
  super.destroy();
  ProgramStatus status=getContext().getState().getStatus();
  WRAPPERLOGGER.info(""String_Node_Str"",getContext().getApplicationSpecification().getName(),status == ProgramStatus.COMPLETED ? ""String_Node_Str"" : status.name().toLowerCase());
}","@TransactionPolicy(TransactionControl.EXPLICIT) @Override public void destroy(){
  super.destroy();
  ProgramStatus status=getContext().getState().getStatus();
  WRAPPERLOGGER.info(""String_Node_Str"",getContext().getApplicationSpecification().getName(),status == ProgramStatus.COMPLETED ? ""String_Node_Str"" : status.name().toLowerCase());
}"
4733,"@Override public void initialize() throws Exception {
  SparkClientContext context=getContext();
  String arguments=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(context.getRuntimeArguments());
  WRAPPERLOGGER.info(""String_Node_Str"",context.getApplicationSpecification().getName(),UserGroupInformation.getCurrentUser().getShortUserName(),arguments);
  DataStreamsPipelineSpec spec=GSON.fromJson(context.getSpecification().getProperty(Constants.PIPELINEID),DataStreamsPipelineSpec.class);
  PipelinePluginContext pluginContext=new SparkPipelinePluginContext(context,context.getMetrics(),true,true);
  int numSources=0;
  for (  StageSpec stageSpec : spec.getStages()) {
    if (StreamingSource.PLUGIN_TYPE.equals(stageSpec.getPlugin().getType())) {
      StreamingSource<Object> streamingSource=pluginContext.newPluginInstance(stageSpec.getName());
      numSources=numSources + streamingSource.getRequiredExecutors();
    }
  }
  SparkConf sparkConf=new SparkConf();
  sparkConf.set(""String_Node_Str"",""String_Node_Str"");
  for (  Map.Entry<String,String> property : spec.getProperties().entrySet()) {
    sparkConf.set(property.getKey(),property.getValue());
  }
  String extraOpts=spec.getExtraJavaOpts();
  if (extraOpts != null && !extraOpts.isEmpty()) {
    sparkConf.set(""String_Node_Str"",extraOpts);
    sparkConf.set(""String_Node_Str"",extraOpts);
  }
  sparkConf.set(""String_Node_Str"",String.valueOf(numSources + 2));
  sparkConf.set(""String_Node_Str"",String.valueOf(numSources + 2));
  sparkConf.setMaster(String.format(""String_Node_Str"",numSources + 2));
  if (spec.isUnitTest()) {
    sparkConf.setMaster(String.format(""String_Node_Str"",numSources + 1));
  }
  context.setSparkConf(sparkConf);
  if (!spec.isCheckpointsDisabled()) {
    FileSet checkpointFileSet=context.getDataset(DataStreamsApp.CHECKPOINT_FILESET);
    String pipelineName=context.getApplicationSpecification().getName();
    String checkpointDir=spec.getCheckpointDirectory();
    Location pipelineCheckpointBase=checkpointFileSet.getBaseLocation().append(pipelineName);
    Location pipelineCheckpointDir=pipelineCheckpointBase.append(checkpointDir);
    if (!ensureDirExists(pipelineCheckpointBase)) {
      throw new IOException(String.format(""String_Node_Str"",pipelineCheckpointBase));
    }
    try {
      for (      Location child : pipelineCheckpointBase.list()) {
        if (!child.equals(pipelineCheckpointDir) && !child.delete(true)) {
          LOG.warn(""String_Node_Str"",child);
        }
      }
    }
 catch (    Exception e) {
      LOG.warn(""String_Node_Str"",e);
    }
    if (!ensureDirExists(pipelineCheckpointDir)) {
      throw new IOException(String.format(""String_Node_Str"",pipelineCheckpointDir));
    }
  }
  WRAPPERLOGGER.info(""String_Node_Str"",context.getApplicationSpecification().getName());
}","@TransactionPolicy(TransactionControl.EXPLICIT) @Override public void initialize() throws Exception {
  SparkClientContext context=getContext();
  String arguments=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(context.getRuntimeArguments());
  WRAPPERLOGGER.info(""String_Node_Str"",context.getApplicationSpecification().getName(),UserGroupInformation.getCurrentUser().getShortUserName(),arguments);
  DataStreamsPipelineSpec spec=GSON.fromJson(context.getSpecification().getProperty(Constants.PIPELINEID),DataStreamsPipelineSpec.class);
  PipelinePluginContext pluginContext=new SparkPipelinePluginContext(context,context.getMetrics(),true,true);
  int numSources=0;
  for (  StageSpec stageSpec : spec.getStages()) {
    if (StreamingSource.PLUGIN_TYPE.equals(stageSpec.getPlugin().getType())) {
      StreamingSource<Object> streamingSource=pluginContext.newPluginInstance(stageSpec.getName());
      numSources=numSources + streamingSource.getRequiredExecutors();
    }
  }
  SparkConf sparkConf=new SparkConf();
  sparkConf.set(""String_Node_Str"",""String_Node_Str"");
  for (  Map.Entry<String,String> property : spec.getProperties().entrySet()) {
    sparkConf.set(property.getKey(),property.getValue());
  }
  String extraOpts=spec.getExtraJavaOpts();
  if (extraOpts != null && !extraOpts.isEmpty()) {
    sparkConf.set(""String_Node_Str"",extraOpts);
    sparkConf.set(""String_Node_Str"",extraOpts);
  }
  sparkConf.set(""String_Node_Str"",String.valueOf(numSources + 2));
  sparkConf.set(""String_Node_Str"",String.valueOf(numSources + 2));
  sparkConf.setMaster(String.format(""String_Node_Str"",numSources + 2));
  if (spec.isUnitTest()) {
    sparkConf.setMaster(String.format(""String_Node_Str"",numSources + 1));
  }
  context.setSparkConf(sparkConf);
  if (!spec.isCheckpointsDisabled()) {
    FileSet checkpointFileSet=context.getDataset(DataStreamsApp.CHECKPOINT_FILESET);
    String pipelineName=context.getApplicationSpecification().getName();
    String checkpointDir=spec.getCheckpointDirectory();
    Location pipelineCheckpointBase=checkpointFileSet.getBaseLocation().append(pipelineName);
    Location pipelineCheckpointDir=pipelineCheckpointBase.append(checkpointDir);
    if (!ensureDirExists(pipelineCheckpointBase)) {
      throw new IOException(String.format(""String_Node_Str"",pipelineCheckpointBase));
    }
    try {
      for (      Location child : pipelineCheckpointBase.list()) {
        if (!child.equals(pipelineCheckpointDir) && !child.delete(true)) {
          LOG.warn(""String_Node_Str"",child);
        }
      }
    }
 catch (    Exception e) {
      LOG.warn(""String_Node_Str"",e);
    }
    if (!ensureDirExists(pipelineCheckpointDir)) {
      throw new IOException(String.format(""String_Node_Str"",pipelineCheckpointDir));
    }
  }
  WRAPPERLOGGER.info(""String_Node_Str"",context.getApplicationSpecification().getName());
}"
4734,"/** 
 * Prepare the Batch run. Used to configure the job before starting the run.
 * @param context batch execution context
 * @throws Exception if there's an error during this method invocation
 */
public abstract void prepareRun(T context) throws Exception ;","/** 
 * Prepare the Batch run. Used to configure the job before starting the run.
 * @param context batch execution context
 * @throws Exception if there's an error during this method invocation
 */
@Override public abstract void prepareRun(T context) throws Exception ;"
4735,"/** 
 * Invoked after the Batch run finishes. Used to perform any end of the run logic.
 * @param succeeded defines the result of batch execution: true if run succeeded, false otherwise
 * @param context batch execution context
 */
public void onRunFinish(boolean succeeded,T context){
}","/** 
 * Invoked after the Batch run finishes. Used to perform any end of the run logic.
 * @param succeeded defines the result of batch execution: true if run succeeded, false otherwise
 * @param context batch execution context
 */
@Override public void onRunFinish(boolean succeeded,T context){
}"
4736,"/** 
 * Prepare the Batch run. Used to configure the job before starting the run.
 * @param context batch execution context
 * @throws Exception if there's an error during this method invocation
 */
public abstract void prepareRun(T context) throws Exception ;","/** 
 * Prepare the Batch run. Used to configure the job before starting the run.
 * @param context batch execution context
 * @throws Exception if there's an error during this method invocation
 */
@Override public abstract void prepareRun(T context) throws Exception ;"
4737,"/** 
 * Invoked after the Batch run finishes. Used to perform any end of the run logic.
 * @param succeeded defines the result of batch execution: true if run succeeded, false otherwise
 * @param context batch execution context
 */
public void onRunFinish(boolean succeeded,T context){
}","/** 
 * Invoked after the Batch run finishes. Used to perform any end of the run logic.
 * @param succeeded defines the result of batch execution: true if run succeeded, false otherwise
 * @param context batch execution context
 */
@Override public void onRunFinish(boolean succeeded,T context){
}"
4738,"@Override public void destroy(){
  WorkflowContext workflowContext=getContext();
  PipelineRuntime pipelineRuntime=new PipelineRuntime(workflowContext,workflowMetrics);
  if (workflowContext.getDataTracer(PostAction.PLUGIN_TYPE).isEnabled()) {
    return;
  }
  for (  Map.Entry<String,PostAction> endingActionEntry : postActions.entrySet()) {
    String name=endingActionEntry.getKey();
    PostAction action=endingActionEntry.getValue();
    StageSpec stageSpec=postActionSpecs.get(name);
    BatchActionContext context=new WorkflowBackedActionContext(workflowContext,pipelineRuntime,stageSpec);
    try {
      action.run(context);
    }
 catch (    Throwable t) {
      LOG.error(""String_Node_Str"",name,t);
    }
  }
}","@TransactionPolicy(TransactionControl.EXPLICIT) @Override public void destroy(){
  WorkflowContext workflowContext=getContext();
  PipelineRuntime pipelineRuntime=new PipelineRuntime(workflowContext,workflowMetrics);
  if (workflowContext.getDataTracer(PostAction.PLUGIN_TYPE).isEnabled()) {
    return;
  }
  for (  Map.Entry<String,PostAction> endingActionEntry : postActions.entrySet()) {
    String name=endingActionEntry.getKey();
    PostAction action=endingActionEntry.getValue();
    StageSpec stageSpec=postActionSpecs.get(name);
    BatchActionContext context=new WorkflowBackedActionContext(workflowContext,pipelineRuntime,stageSpec);
    try {
      action.run(context);
    }
 catch (    Throwable t) {
      LOG.error(""String_Node_Str"",name,t);
    }
  }
}"
4739,"@Override public void initialize(WorkflowContext context) throws Exception {
  super.initialize(context);
  postActions=new LinkedHashMap<>();
  BatchPipelineSpec batchPipelineSpec=GSON.fromJson(context.getWorkflowSpecification().getProperty(""String_Node_Str""),BatchPipelineSpec.class);
  MacroEvaluator macroEvaluator=new DefaultMacroEvaluator(context.getToken(),context.getRuntimeArguments(),context.getLogicalStartTime(),context,context.getNamespace());
  postActionSpecs=new HashMap<>();
  for (  ActionSpec actionSpec : batchPipelineSpec.getEndingActions()) {
    String name=actionSpec.getName();
    postActions.put(name,(PostAction)context.newPluginInstance(name,macroEvaluator));
    postActionSpecs.put(name,StageSpec.builder(name,actionSpec.getPluginSpec()).setProcessTimingEnabled(batchPipelineSpec.isProcessTimingEnabled()).setStageLoggingEnabled(batchPipelineSpec.isStageLoggingEnabled()).build());
  }
}","@TransactionPolicy(TransactionControl.EXPLICIT) @Override public void initialize(WorkflowContext context) throws Exception {
  super.initialize(context);
  postActions=new LinkedHashMap<>();
  BatchPipelineSpec batchPipelineSpec=GSON.fromJson(context.getWorkflowSpecification().getProperty(""String_Node_Str""),BatchPipelineSpec.class);
  MacroEvaluator macroEvaluator=new DefaultMacroEvaluator(context.getToken(),context.getRuntimeArguments(),context.getLogicalStartTime(),context,context.getNamespace());
  postActionSpecs=new HashMap<>();
  for (  ActionSpec actionSpec : batchPipelineSpec.getEndingActions()) {
    String name=actionSpec.getName();
    postActions.put(name,(PostAction)context.newPluginInstance(name,macroEvaluator));
    postActionSpecs.put(name,StageSpec.builder(name,actionSpec.getPluginSpec()).setProcessTimingEnabled(batchPipelineSpec.isProcessTimingEnabled()).setStageLoggingEnabled(batchPipelineSpec.isStageLoggingEnabled()).build());
  }
}"
4740,"private void verifyResponse(HttpResponseStatus expected,HttpResponseStatus actual,String errorMsg){
  if (!expected.equals(actual)) {
    throw new IllegalStateException(String.format(""String_Node_Str"",expected,actual,errorMsg));
  }
}","private void verifyResponse(HttpResponseStatus expected,HttpResponseStatus actual,String errorMsg){
  if (!expected.equals(actual)) {
    if (actual.getCode() == HttpResponseStatus.FORBIDDEN.getCode()) {
      throw new UnauthorizedException(actual.getReasonPhrase());
    }
    throw new IllegalStateException(String.format(""String_Node_Str"",expected,actual,errorMsg));
  }
}"
4741,"/** 
 * Perform the request, returning the response. If there was a ConnectException while making the request, a ServiceUnavailableException is thrown.
 * @param request the request to perform
 * @return the response
 * @throws IOException if there was an IOException while performing the request
 * @throws ServiceUnavailableException if there was a ConnectException while making the request, or if the responsewas a 503
 */
public HttpResponse execute(HttpRequest request) throws IOException {
  try {
    HttpResponse response=HttpRequests.execute(request,httpRequestConfig);
    if (response.getResponseCode() == HttpURLConnection.HTTP_UNAVAILABLE) {
      throw new ServiceUnavailableException(discoverableServiceName,response.getResponseBodyAsString());
    }
    return response;
  }
 catch (  ConnectException e) {
    throw new ServiceUnavailableException(discoverableServiceName,e);
  }
}","/** 
 * Perform the request, returning the response. If there was a ConnectException while making the request, a ServiceUnavailableException is thrown.
 * @param request the request to perform
 * @return the response
 * @throws IOException if there was an IOException while performing the request
 * @throws ServiceUnavailableException if there was a ConnectException while making the request, or if the responsewas a 503
 */
public HttpResponse execute(HttpRequest request) throws IOException {
  try {
    HttpResponse response=HttpRequests.execute(request,httpRequestConfig);
switch (response.getResponseCode()) {
case HttpURLConnection.HTTP_UNAVAILABLE:
      throw new ServiceUnavailableException(discoverableServiceName,response.getResponseBodyAsString());
case HttpURLConnection.HTTP_FORBIDDEN:
    throw new UnauthorizedException(response.getResponseBodyAsString());
default :
  return response;
}
}
 catch (ConnectException e) {
throw new ServiceUnavailableException(discoverableServiceName,e);
}
}"
4742,"/** 
 * Deletes all   {@link DatasetModuleMeta dataset modules} in the specified {@link NamespaceId namespace}.
 */
public void deleteAll(NamespaceId namespaceId) throws Exception {
  if (NamespaceId.SYSTEM.equals(namespaceId)) {
    throw new UnauthorizedException(String.format(""String_Node_Str"",namespaceId));
  }
  ensureNamespaceExists(namespaceId);
  try {
    typeManager.deleteModules(namespaceId);
  }
 catch (  DatasetModuleConflictException e) {
    throw new ConflictException(e.getMessage(),e);
  }
}","/** 
 * Deletes all   {@link DatasetModuleMeta dataset modules} in the specified {@link NamespaceId namespace}.
 */
public void deleteAll(NamespaceId namespaceId) throws Exception {
  if (NamespaceId.SYSTEM.equals(namespaceId)) {
    throw new UnsupportedOperationException(String.format(""String_Node_Str"",namespaceId));
  }
  ensureNamespaceExists(namespaceId);
  try {
    typeManager.deleteModules(namespaceId);
  }
 catch (  DatasetModuleConflictException e) {
    throw new ConflictException(e.getMessage(),e);
  }
}"
4743,"/** 
 * Deletes the specified   {@link DatasetModuleId}
 */
public void delete(DatasetModuleId datasetModuleId) throws Exception {
  NamespaceId namespaceId=datasetModuleId.getParent();
  if (NamespaceId.SYSTEM.equals(namespaceId)) {
    throw new UnauthorizedException(String.format(""String_Node_Str"",datasetModuleId.getModule(),datasetModuleId.getNamespace()));
  }
  ensureNamespaceExists(namespaceId);
  DatasetModuleMeta moduleMeta=typeManager.getModule(datasetModuleId);
  if (moduleMeta == null) {
    throw new DatasetModuleNotFoundException(datasetModuleId);
  }
  try {
    typeManager.deleteModule(datasetModuleId);
  }
 catch (  DatasetModuleConflictException e) {
    throw new DatasetModuleCannotBeDeletedException(datasetModuleId,e.getMessage());
  }
}","/** 
 * Deletes the specified   {@link DatasetModuleId}
 */
public void delete(DatasetModuleId datasetModuleId) throws Exception {
  NamespaceId namespaceId=datasetModuleId.getParent();
  if (NamespaceId.SYSTEM.equals(namespaceId)) {
    throw new UnsupportedOperationException(String.format(""String_Node_Str"",datasetModuleId.getModule(),datasetModuleId.getNamespace()));
  }
  ensureNamespaceExists(namespaceId);
  DatasetModuleMeta moduleMeta=typeManager.getModule(datasetModuleId);
  if (moduleMeta == null) {
    throw new DatasetModuleNotFoundException(datasetModuleId);
  }
  try {
    typeManager.deleteModule(datasetModuleId);
  }
 catch (  DatasetModuleConflictException e) {
    throw new DatasetModuleCannotBeDeletedException(datasetModuleId,e.getMessage());
  }
}"
4744,"/** 
 * Adds a new   {@link DatasetModule}.
 * @param datasetModuleId the {@link DatasetModuleId} for the module to be added
 * @param className the module class name specified in the HTTP header
 * @param forceUpdate if true, an update will be allowed even if there are conflicts with other modules, or ifremoval of a type would break other modules' dependencies
 * @return a {@link BodyConsumer} to upload the module jar in chunks
 * @throws NotFoundException if the namespace in which the module is being added is not found
 * @throws IOException if there are issues while performing I/O like creating temporary directories, moving/unpackingmodule jar files
 * @throws DatasetModuleConflictException if #forceUpdate is {@code false}, and there are conflicts with other modules
 */
public BodyConsumer addModule(final DatasetModuleId datasetModuleId,final String className,final boolean forceUpdate) throws Exception {
  NamespaceId namespaceId=datasetModuleId.getParent();
  if (NamespaceId.SYSTEM.equals(namespaceId)) {
    throw new UnauthorizedException(String.format(""String_Node_Str"",datasetModuleId.getModule(),datasetModuleId.getNamespace()));
  }
  ensureNamespaceExists(namespaceId);
  try {
    return createModuleConsumer(datasetModuleId,className,forceUpdate);
  }
 catch (  Exception e) {
    throw e;
  }
}","/** 
 * Adds a new   {@link DatasetModule}.
 * @param datasetModuleId the {@link DatasetModuleId} for the module to be added
 * @param className the module class name specified in the HTTP header
 * @param forceUpdate if true, an update will be allowed even if there are conflicts with other modules, or ifremoval of a type would break other modules' dependencies
 * @return a {@link BodyConsumer} to upload the module jar in chunks
 * @throws NotFoundException if the namespace in which the module is being added is not found
 * @throws IOException if there are issues while performing I/O like creating temporary directories, moving/unpackingmodule jar files
 * @throws DatasetModuleConflictException if #forceUpdate is {@code false}, and there are conflicts with other modules
 */
public BodyConsumer addModule(final DatasetModuleId datasetModuleId,final String className,final boolean forceUpdate) throws Exception {
  NamespaceId namespaceId=datasetModuleId.getParent();
  if (NamespaceId.SYSTEM.equals(namespaceId)) {
    throw new UnsupportedOperationException(String.format(""String_Node_Str"",datasetModuleId.getModule(),datasetModuleId.getNamespace()));
  }
  ensureNamespaceExists(namespaceId);
  try {
    return createModuleConsumer(datasetModuleId,className,forceUpdate);
  }
 catch (  Exception e) {
    throw e;
  }
}"
4745,"/** 
 * This test is to make sure we do not bypass the authorization check for datasets in system namespace
 */
@Test public void testDeleteSystemDatasets() throws Exception {
  UserGroupInformation remoteUser=UserGroupInformation.createRemoteUser(""String_Node_Str"");
  remoteUser.doAs(new PrivilegedAction<Void>(){
    @Override public Void run(){
      try {
        deleteDatasetInstance(NamespaceId.SYSTEM.dataset(""String_Node_Str""));
        Assert.fail();
      }
 catch (      DatasetManagementException e) {
        Assert.assertTrue(e.getMessage().contains(""String_Node_Str""));
      }
catch (      Exception e) {
        Assert.fail(""String_Node_Str"");
      }
      return null;
    }
  }
);
}","/** 
 * This test is to make sure we do not bypass the authorization check for datasets in system namespace
 */
@Test public void testDeleteSystemDatasets() throws Exception {
  UserGroupInformation remoteUser=UserGroupInformation.createRemoteUser(""String_Node_Str"");
  remoteUser.doAs(new PrivilegedAction<Void>(){
    @Override public Void run(){
      try {
        deleteDatasetInstance(NamespaceId.SYSTEM.dataset(""String_Node_Str""));
        Assert.fail();
      }
 catch (      UnauthorizedException e) {
      }
catch (      Exception e) {
        Assert.fail(""String_Node_Str"");
      }
      return null;
    }
  }
);
}"
4746,"@Override public Void run(){
  try {
    deleteDatasetInstance(NamespaceId.SYSTEM.dataset(""String_Node_Str""));
    Assert.fail();
  }
 catch (  DatasetManagementException e) {
    Assert.assertTrue(e.getMessage().contains(""String_Node_Str""));
  }
catch (  Exception e) {
    Assert.fail(""String_Node_Str"");
  }
  return null;
}","@Override public Void run(){
  try {
    deleteDatasetInstance(NamespaceId.SYSTEM.dataset(""String_Node_Str""));
    Assert.fail();
  }
 catch (  UnauthorizedException e) {
  }
catch (  Exception e) {
    Assert.fail(""String_Node_Str"");
  }
  return null;
}"
4747,"@After @Override public void afterTest() throws Exception {
  Authorizer authorizer=getAuthorizer();
  grantAndAssertSuccess(AUTH_NAMESPACE,SecurityRequestContext.toPrincipal(),EnumSet.of(Action.ADMIN));
  if (getNamespaceAdmin().exists(AUTH_NAMESPACE)) {
    getNamespaceAdmin().delete(AUTH_NAMESPACE);
    Assert.assertFalse(getNamespaceAdmin().exists(AUTH_NAMESPACE));
  }
  revokeAndAssertSuccess(AUTH_NAMESPACE);
  for (  EntityId entityId : cleanUpEntities) {
    revokeAndAssertSuccess(entityId);
  }
  Assert.assertEquals(Collections.emptySet(),authorizer.listPrivileges(ALICE));
}","@After @Override public void afterTest() throws Exception {
  Authorizer authorizer=getAuthorizer();
  SecurityRequestContext.setUserId(ALICE.getName());
  grantAndAssertSuccess(AUTH_NAMESPACE,SecurityRequestContext.toPrincipal(),EnumSet.of(Action.ADMIN));
  if (getNamespaceAdmin().exists(AUTH_NAMESPACE)) {
    getNamespaceAdmin().delete(AUTH_NAMESPACE);
    Assert.assertFalse(getNamespaceAdmin().exists(AUTH_NAMESPACE));
  }
  revokeAndAssertSuccess(AUTH_NAMESPACE);
  for (  EntityId entityId : cleanUpEntities) {
    revokeAndAssertSuccess(entityId);
  }
  Assert.assertEquals(Collections.emptySet(),authorizer.listPrivileges(ALICE));
}"
4748,"@Test public void testScheduleAuth() throws Exception {
  createAuthNamespace();
  Map<EntityId,Set<Action>> neededPrivileges=ImmutableMap.<EntityId,Set<Action>>builder().put(AUTH_NAMESPACE.app(AppWithSchedule.class.getSimpleName()),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.artifact(AppWithSchedule.class.getSimpleName(),""String_Node_Str""),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(AppWithSchedule.INPUT_NAME),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(AppWithSchedule.OUTPUT_NAME),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.datasetType(ObjectStore.class.getName()),EnumSet.of(Action.ADMIN)).build();
  setUpPrivilegeAndRegisterForDeletion(ALICE,neededPrivileges);
  ApplicationManager appManager=deployApplication(AUTH_NAMESPACE,AppWithSchedule.class);
  ProgramId workflowID=new ProgramId(AUTH_NAMESPACE.getNamespace(),AppWithSchedule.class.getSimpleName(),ProgramType.WORKFLOW,AppWithSchedule.SampleWorkflow.class.getSimpleName());
  cleanUpEntities.add(workflowID);
  final WorkflowManager workflowManager=appManager.getWorkflowManager(AppWithSchedule.SampleWorkflow.class.getSimpleName());
  ScheduleManager scheduleManager=workflowManager.getSchedule(AppWithSchedule.EVERY_HOUR_SCHEDULE);
  SecurityRequestContext.setUserId(BOB.getName());
  try {
    scheduleManager.resume();
    Assert.fail(""String_Node_Str"");
  }
 catch (  Exception e) {
    Assert.assertTrue(e.getCause() instanceof UnauthorizedException);
  }
  try {
    scheduleManager.status(HttpURLConnection.HTTP_FORBIDDEN);
    Assert.fail(""String_Node_Str"");
  }
 catch (  Exception e) {
    Assert.assertTrue(e.getCause() instanceof UnauthorizedException);
  }
  SecurityRequestContext.setUserId(ALICE.getName());
  grantAndAssertSuccess(workflowID,BOB,EnumSet.of(Action.READ));
  SecurityRequestContext.setUserId(BOB.getName());
  try {
    scheduleManager.resume();
    Assert.fail(""String_Node_Str"");
  }
 catch (  Exception e) {
    Assert.assertTrue(e.getCause() instanceof UnauthorizedException);
  }
  Assert.assertEquals(ProgramScheduleStatus.SUSPENDED.name(),scheduleManager.status(HttpURLConnection.HTTP_OK));
  SecurityRequestContext.setUserId(ALICE.getName());
  grantAndAssertSuccess(workflowID,BOB,EnumSet.of(Action.EXECUTE));
  SecurityRequestContext.setUserId(BOB.getName());
  scheduleManager.resume();
  Assert.assertEquals(ProgramScheduleStatus.SCHEDULED.name(),scheduleManager.status(HttpURLConnection.HTTP_OK));
  scheduleManager.suspend();
  Assert.assertEquals(ProgramScheduleStatus.SUSPENDED.name(),scheduleManager.status(HttpURLConnection.HTTP_OK));
  SecurityRequestContext.setUserId(ALICE.getName());
}","@Test public void testScheduleAuth() throws Exception {
  createAuthNamespace();
  Map<EntityId,Set<Action>> neededPrivileges=ImmutableMap.<EntityId,Set<Action>>builder().put(AUTH_NAMESPACE.app(AppWithSchedule.class.getSimpleName()),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.artifact(AppWithSchedule.class.getSimpleName(),""String_Node_Str""),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(AppWithSchedule.INPUT_NAME),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(AppWithSchedule.OUTPUT_NAME),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.datasetType(ObjectStore.class.getName()),EnumSet.of(Action.ADMIN)).build();
  setUpPrivilegeAndRegisterForDeletion(ALICE,neededPrivileges);
  ApplicationManager appManager=deployApplication(AUTH_NAMESPACE,AppWithSchedule.class);
  ProgramId workflowID=new ProgramId(AUTH_NAMESPACE.getNamespace(),AppWithSchedule.class.getSimpleName(),ProgramType.WORKFLOW,AppWithSchedule.SampleWorkflow.class.getSimpleName());
  cleanUpEntities.add(workflowID);
  final WorkflowManager workflowManager=appManager.getWorkflowManager(AppWithSchedule.SampleWorkflow.class.getSimpleName());
  ScheduleManager scheduleManager=workflowManager.getSchedule(AppWithSchedule.EVERY_HOUR_SCHEDULE);
  SecurityRequestContext.setUserId(BOB.getName());
  try {
    scheduleManager.resume();
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  try {
    scheduleManager.status(HttpURLConnection.HTTP_FORBIDDEN);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  SecurityRequestContext.setUserId(ALICE.getName());
  grantAndAssertSuccess(workflowID,BOB,EnumSet.of(Action.READ));
  SecurityRequestContext.setUserId(BOB.getName());
  try {
    scheduleManager.resume();
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  Assert.assertEquals(ProgramScheduleStatus.SUSPENDED.name(),scheduleManager.status(HttpURLConnection.HTTP_OK));
  SecurityRequestContext.setUserId(ALICE.getName());
  grantAndAssertSuccess(workflowID,BOB,EnumSet.of(Action.EXECUTE));
  SecurityRequestContext.setUserId(BOB.getName());
  scheduleManager.resume();
  Assert.assertEquals(ProgramScheduleStatus.SCHEDULED.name(),scheduleManager.status(HttpURLConnection.HTTP_OK));
  scheduleManager.suspend();
  Assert.assertEquals(ProgramScheduleStatus.SUSPENDED.name(),scheduleManager.status(HttpURLConnection.HTTP_OK));
  SecurityRequestContext.setUserId(ALICE.getName());
}"
4749,"@Test @Category(SlowTests.class) public void testFlowStreamAuth() throws Exception {
  createAuthNamespace();
  Authorizer authorizer=getAuthorizer();
  setUpPrivilegeToDeployStreamAuthApp();
  StreamId streamId1=AUTH_NAMESPACE.stream(StreamAuthApp.STREAM);
  StreamId streamId2=AUTH_NAMESPACE.stream(StreamAuthApp.STREAM2);
  Map<EntityId,Set<Action>> additionalPrivileges=ImmutableMap.<EntityId,Set<Action>>builder().put(streamId1,EnumSet.of(Action.READ,Action.WRITE)).put(streamId2,EnumSet.of(Action.READ,Action.WRITE)).put(AUTH_NAMESPACE.dataset(StreamAuthApp.KVTABLE),EnumSet.of(Action.READ,Action.WRITE)).put(AUTH_NAMESPACE.app(StreamAuthApp.APP).flow(StreamAuthApp.FLOW),EnumSet.of(Action.EXECUTE)).build();
  setUpPrivilegeAndRegisterForDeletion(ALICE,additionalPrivileges);
  ApplicationManager appManager=deployApplication(AUTH_NAMESPACE,StreamAuthApp.class);
  final FlowManager flowManager=appManager.getFlowManager(StreamAuthApp.FLOW);
  StreamManager streamManager=getStreamManager(streamId1);
  StreamManager streamManager2=getStreamManager(streamId2);
  streamManager.send(""String_Node_Str"");
  flowManager.start();
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      DataSetManager<KeyValueTable> kvTable=getDataset(AUTH_NAMESPACE.dataset(StreamAuthApp.KVTABLE));
      return kvTable.get().read(""String_Node_Str"") != null;
    }
  }
,5,TimeUnit.SECONDS);
  flowManager.stop();
  flowManager.waitForRun(ProgramRunStatus.KILLED,60,TimeUnit.SECONDS);
  authorizer.revoke(streamId1,ALICE,EnumSet.allOf(Action.class));
  authorizer.grant(streamId1,ALICE,EnumSet.of(Action.WRITE,Action.ADMIN));
  streamManager.send(""String_Node_Str"");
  streamManager2.send(""String_Node_Str"");
  try {
    flowManager.start();
  }
 catch (  RuntimeException e) {
    Assert.assertTrue(e.getCause() instanceof UnauthorizedException);
  }
  authorizer.grant(streamId1,ALICE,ImmutableSet.of(Action.READ));
  flowManager.start();
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      DataSetManager<KeyValueTable> kvTable=getDataset(AUTH_NAMESPACE.dataset(StreamAuthApp.KVTABLE));
      return kvTable.get().read(""String_Node_Str"") != null;
    }
  }
,5,TimeUnit.SECONDS);
  TimeUnit.MILLISECONDS.sleep(10);
  flowManager.stop();
  flowManager.waitForRuns(ProgramRunStatus.KILLED,2,5,TimeUnit.SECONDS);
  appManager.delete();
}","@Test @Category(SlowTests.class) public void testFlowStreamAuth() throws Exception {
  createAuthNamespace();
  Authorizer authorizer=getAuthorizer();
  setUpPrivilegeToDeployStreamAuthApp();
  StreamId streamId1=AUTH_NAMESPACE.stream(StreamAuthApp.STREAM);
  StreamId streamId2=AUTH_NAMESPACE.stream(StreamAuthApp.STREAM2);
  Map<EntityId,Set<Action>> additionalPrivileges=ImmutableMap.<EntityId,Set<Action>>builder().put(streamId1,EnumSet.of(Action.READ,Action.WRITE)).put(streamId2,EnumSet.of(Action.READ,Action.WRITE)).put(AUTH_NAMESPACE.dataset(StreamAuthApp.KVTABLE),EnumSet.of(Action.READ,Action.WRITE)).put(AUTH_NAMESPACE.app(StreamAuthApp.APP).flow(StreamAuthApp.FLOW),EnumSet.of(Action.EXECUTE)).build();
  setUpPrivilegeAndRegisterForDeletion(ALICE,additionalPrivileges);
  ApplicationManager appManager=deployApplication(AUTH_NAMESPACE,StreamAuthApp.class);
  final FlowManager flowManager=appManager.getFlowManager(StreamAuthApp.FLOW);
  StreamManager streamManager=getStreamManager(streamId1);
  StreamManager streamManager2=getStreamManager(streamId2);
  streamManager.send(""String_Node_Str"");
  flowManager.start();
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      DataSetManager<KeyValueTable> kvTable=getDataset(AUTH_NAMESPACE.dataset(StreamAuthApp.KVTABLE));
      return kvTable.get().read(""String_Node_Str"") != null;
    }
  }
,5,TimeUnit.SECONDS);
  flowManager.stop();
  flowManager.waitForRun(ProgramRunStatus.KILLED,60,TimeUnit.SECONDS);
  authorizer.revoke(streamId1,ALICE,EnumSet.allOf(Action.class));
  authorizer.grant(streamId1,ALICE,EnumSet.of(Action.WRITE,Action.ADMIN));
  streamManager.send(""String_Node_Str"");
  streamManager2.send(""String_Node_Str"");
  try {
    flowManager.start();
  }
 catch (  UnauthorizedException e) {
  }
  authorizer.grant(streamId1,ALICE,ImmutableSet.of(Action.READ));
  flowManager.start();
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      DataSetManager<KeyValueTable> kvTable=getDataset(AUTH_NAMESPACE.dataset(StreamAuthApp.KVTABLE));
      return kvTable.get().read(""String_Node_Str"") != null;
    }
  }
,5,TimeUnit.SECONDS);
  TimeUnit.MILLISECONDS.sleep(10);
  flowManager.stop();
  flowManager.waitForRuns(ProgramRunStatus.KILLED,2,5,TimeUnit.SECONDS);
  appManager.delete();
}"
4750,"@Test @Category(SlowTests.class) public void testApps() throws Exception {
  try {
    deployApplication(NamespaceId.DEFAULT,DummyApp.class);
    Assert.fail(""String_Node_Str"");
  }
 catch (  RuntimeException e) {
    Assert.assertTrue(e.getCause() instanceof UnauthorizedException);
  }
  createAuthNamespace();
  Authorizer authorizer=getAuthorizer();
  ApplicationId dummyAppId=AUTH_NAMESPACE.app(DummyApp.class.getSimpleName());
  Map<EntityId,Set<Action>> neededPrivileges=ImmutableMap.<EntityId,Set<Action>>builder().put(dummyAppId,EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.artifact(DummyApp.class.getSimpleName(),""String_Node_Str""),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(""String_Node_Str""),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.stream(""String_Node_Str""),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(""String_Node_Str""),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.datasetType(KeyValueTable.class.getName()),EnumSet.of(Action.ADMIN)).build();
  setUpPrivilegeAndRegisterForDeletion(ALICE,neededPrivileges);
  try {
    deployApplication(AUTH_NAMESPACE,DummyApp.class);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  grantAndAssertSuccess(AUTH_NAMESPACE.datasetType(DummyApp.CustomDummyDataset.class.getName()),ALICE,EnumSet.of(Action.ADMIN));
  cleanUpEntities.add(AUTH_NAMESPACE.datasetType(DummyApp.CustomDummyDataset.class.getName()));
  grantAndAssertSuccess(AUTH_NAMESPACE.datasetModule(DummyApp.CustomDummyDataset.class.getName()),ALICE,EnumSet.of(Action.ADMIN));
  cleanUpEntities.add(AUTH_NAMESPACE.datasetModule(DummyApp.CustomDummyDataset.class.getName()));
  ApplicationManager appManager=deployApplication(AUTH_NAMESPACE,DummyApp.class);
  Assert.assertTrue(""String_Node_Str"",authorizer.listPrivileges(BOB).isEmpty());
  appManager.update(new AppRequest(new ArtifactSummary(DummyApp.class.getSimpleName(),""String_Node_Str"")));
  SecurityRequestContext.setUserId(BOB.getName());
  try {
    appManager.update(new AppRequest(new ArtifactSummary(DummyApp.class.getSimpleName(),""String_Node_Str"")));
    Assert.fail(""String_Node_Str"");
  }
 catch (  Exception expected) {
  }
  grantAndAssertSuccess(dummyAppId,BOB,ImmutableSet.of(Action.READ,Action.WRITE));
  try {
    appManager.delete();
  }
 catch (  Exception expected) {
  }
  grantAndAssertSuccess(dummyAppId,BOB,ImmutableSet.of(Action.ADMIN));
  appManager.delete();
  Assert.assertTrue(!getAuthorizer().isVisible(Collections.singleton(dummyAppId),BOB).isEmpty());
  Assert.assertEquals(3,authorizer.listPrivileges(BOB).size());
  SecurityRequestContext.setUserId(ALICE.getName());
  deployApplication(AUTH_NAMESPACE,DummyApp.class);
  Map<EntityId,Set<Action>> anotherAppNeededPrivilege=ImmutableMap.<EntityId,Set<Action>>builder().put(AUTH_NAMESPACE.app(AllProgramsApp.NAME),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.artifact(AllProgramsApp.class.getSimpleName(),""String_Node_Str""),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(AllProgramsApp.DATASET_NAME),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(AllProgramsApp.DATASET_NAME2),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(AllProgramsApp.DATASET_NAME3),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(AllProgramsApp.DS_WITH_SCHEMA_NAME),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.stream(AllProgramsApp.STREAM_NAME),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.datasetType(ObjectMappedTable.class.getName()),EnumSet.of(Action.ADMIN)).build();
  setUpPrivilegeAndRegisterForDeletion(ALICE,anotherAppNeededPrivilege);
  deployApplication(AUTH_NAMESPACE,AllProgramsApp.class);
  SecurityRequestContext.setUserId(BOB.getName());
  try {
    deleteAllApplications(AUTH_NAMESPACE);
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  SecurityRequestContext.setUserId(ALICE.getName());
  deleteAllApplications(AUTH_NAMESPACE);
}","@Test @Category(SlowTests.class) public void testApps() throws Exception {
  try {
    deployApplication(NamespaceId.DEFAULT,DummyApp.class);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  createAuthNamespace();
  Authorizer authorizer=getAuthorizer();
  ApplicationId dummyAppId=AUTH_NAMESPACE.app(DummyApp.class.getSimpleName());
  Map<EntityId,Set<Action>> neededPrivileges=ImmutableMap.<EntityId,Set<Action>>builder().put(dummyAppId,EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.artifact(DummyApp.class.getSimpleName(),""String_Node_Str""),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(""String_Node_Str""),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.stream(""String_Node_Str""),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(""String_Node_Str""),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.datasetType(KeyValueTable.class.getName()),EnumSet.of(Action.ADMIN)).build();
  setUpPrivilegeAndRegisterForDeletion(ALICE,neededPrivileges);
  try {
    deployApplication(AUTH_NAMESPACE,DummyApp.class);
    Assert.fail();
  }
 catch (  UnauthorizedException e) {
  }
  grantAndAssertSuccess(AUTH_NAMESPACE.datasetType(DummyApp.CustomDummyDataset.class.getName()),ALICE,EnumSet.of(Action.ADMIN));
  cleanUpEntities.add(AUTH_NAMESPACE.datasetType(DummyApp.CustomDummyDataset.class.getName()));
  grantAndAssertSuccess(AUTH_NAMESPACE.datasetModule(DummyApp.CustomDummyDataset.class.getName()),ALICE,EnumSet.of(Action.ADMIN));
  cleanUpEntities.add(AUTH_NAMESPACE.datasetModule(DummyApp.CustomDummyDataset.class.getName()));
  ApplicationManager appManager=deployApplication(AUTH_NAMESPACE,DummyApp.class);
  Assert.assertTrue(""String_Node_Str"",authorizer.listPrivileges(BOB).isEmpty());
  appManager.update(new AppRequest(new ArtifactSummary(DummyApp.class.getSimpleName(),""String_Node_Str"")));
  SecurityRequestContext.setUserId(BOB.getName());
  try {
    appManager.update(new AppRequest(new ArtifactSummary(DummyApp.class.getSimpleName(),""String_Node_Str"")));
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  grantAndAssertSuccess(dummyAppId,BOB,ImmutableSet.of(Action.READ,Action.WRITE));
  try {
    appManager.delete();
  }
 catch (  UnauthorizedException expected) {
  }
  grantAndAssertSuccess(dummyAppId,BOB,ImmutableSet.of(Action.ADMIN));
  appManager.delete();
  Assert.assertTrue(!getAuthorizer().isVisible(Collections.singleton(dummyAppId),BOB).isEmpty());
  Assert.assertEquals(3,authorizer.listPrivileges(BOB).size());
  SecurityRequestContext.setUserId(ALICE.getName());
  deployApplication(AUTH_NAMESPACE,DummyApp.class);
  Map<EntityId,Set<Action>> anotherAppNeededPrivilege=ImmutableMap.<EntityId,Set<Action>>builder().put(AUTH_NAMESPACE.app(AllProgramsApp.NAME),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.artifact(AllProgramsApp.class.getSimpleName(),""String_Node_Str""),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(AllProgramsApp.DATASET_NAME),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(AllProgramsApp.DATASET_NAME2),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(AllProgramsApp.DATASET_NAME3),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.dataset(AllProgramsApp.DS_WITH_SCHEMA_NAME),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.stream(AllProgramsApp.STREAM_NAME),EnumSet.of(Action.ADMIN)).put(AUTH_NAMESPACE.datasetType(ObjectMappedTable.class.getName()),EnumSet.of(Action.ADMIN)).build();
  setUpPrivilegeAndRegisterForDeletion(ALICE,anotherAppNeededPrivilege);
  deployApplication(AUTH_NAMESPACE,AllProgramsApp.class);
  SecurityRequestContext.setUserId(BOB.getName());
  try {
    deleteAllApplications(AUTH_NAMESPACE);
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  SecurityRequestContext.setUserId(ALICE.getName());
  deleteAllApplications(AUTH_NAMESPACE);
}"
4751,"/** 
 * Constructs a context. To have plugin support, the   {@code pluginInstantiator} must not be null.
 */
protected AbstractContext(Program program,ProgramOptions programOptions,CConfiguration cConf,Set<String> datasets,DatasetFramework dsFramework,TransactionSystemClient txClient,DiscoveryServiceClient discoveryServiceClient,boolean multiThreaded,@Nullable MetricsCollectionService metricsService,Map<String,String> metricsTags,SecureStore secureStore,SecureStoreManager secureStoreManager,MessagingService messagingService,@Nullable PluginInstantiator pluginInstantiator){
  super(program.getId());
  this.program=program;
  this.programOptions=programOptions;
  this.cConf=cConf;
  this.programRunId=program.getId().run(ProgramRunners.getRunId(programOptions));
  this.discoveryServiceClient=discoveryServiceClient;
  this.owners=createOwners(program.getId());
  this.programMetrics=createProgramMetrics(programRunId,cConf.getBoolean(Constants.Metrics.EMIT_PRGOGRAM_CONTAINER_METRICS) ? metricsService : new NoOpMetricsCollectionService(),metricsTags);
  this.userMetrics=new ProgramUserMetrics(programMetrics);
  this.retryStrategy=SystemArguments.getRetryStrategy(programOptions.getUserArguments().asMap(),program.getType(),cConf);
  Map<String,String> runtimeArgs=new HashMap<>(programOptions.getUserArguments().asMap());
  this.logicalStartTime=ProgramRunners.updateLogicalStartTime(runtimeArgs);
  this.runtimeArguments=Collections.unmodifiableMap(runtimeArgs);
  Map<String,Map<String,String>> staticDatasets=new HashMap<>();
  for (  String name : datasets) {
    staticDatasets.put(name,runtimeArguments);
  }
  SystemDatasetInstantiator instantiator=new SystemDatasetInstantiator(dsFramework,program.getClassLoader(),owners);
  this.messagingService=messagingService;
  this.messagingContext=new MultiThreadMessagingContext(messagingService);
  TransactionSystemClient retryingTxClient=new RetryingShortTransactionSystemClient(txClient,retryStrategy);
  this.datasetCache=multiThreaded ? new MultiThreadDatasetCache(instantiator,retryingTxClient,program.getId().getNamespaceId(),runtimeArguments,programMetrics,staticDatasets,messagingContext) : new SingleThreadDatasetCache(instantiator,retryingTxClient,program.getId().getNamespaceId(),runtimeArguments,programMetrics,staticDatasets);
  this.pluginInstantiator=pluginInstantiator;
  this.pluginContext=new DefaultPluginContext(pluginInstantiator,program.getId(),program.getApplicationSpecification().getPlugins());
  String appPrincipalExists=programOptions.getArguments().getOption(ProgramOptionConstants.APP_PRINCIPAL_EXISTS);
  KerberosPrincipalId principalId=null;
  if (appPrincipalExists != null && Boolean.parseBoolean(appPrincipalExists)) {
    principalId=new KerberosPrincipalId(programOptions.getArguments().getOption(ProgramOptionConstants.PRINCIPAL));
  }
  this.admin=new DefaultAdmin(dsFramework,program.getId().getNamespaceId(),secureStoreManager,new BasicMessagingAdmin(messagingService,program.getId().getNamespaceId()),retryStrategy,principalId);
  this.secureStore=secureStore;
  this.defaultTxTimeout=determineTransactionTimeout(cConf);
  this.transactional=Transactions.createTransactional(getDatasetCache(),defaultTxTimeout);
  if (!multiThreaded) {
    datasetCache.addExtraTransactionAware(messagingContext);
  }
}","/** 
 * Constructs a context. To have plugin support, the   {@code pluginInstantiator} must not be null.
 */
protected AbstractContext(Program program,ProgramOptions programOptions,CConfiguration cConf,Set<String> datasets,DatasetFramework dsFramework,TransactionSystemClient txClient,DiscoveryServiceClient discoveryServiceClient,boolean multiThreaded,@Nullable MetricsCollectionService metricsService,Map<String,String> metricsTags,SecureStore secureStore,SecureStoreManager secureStoreManager,MessagingService messagingService,@Nullable PluginInstantiator pluginInstantiator){
  super(program.getId());
  this.program=program;
  this.programOptions=programOptions;
  this.cConf=cConf;
  this.programRunId=program.getId().run(ProgramRunners.getRunId(programOptions));
  this.discoveryServiceClient=discoveryServiceClient;
  this.owners=createOwners(program.getId());
  Map<String,String> runtimeArgs=new HashMap<>(programOptions.getUserArguments().asMap());
  this.logicalStartTime=ProgramRunners.updateLogicalStartTime(runtimeArgs);
  this.runtimeArguments=Collections.unmodifiableMap(runtimeArgs);
  this.programMetrics=createProgramMetrics(programRunId,getMetricsService(cConf,metricsService,runtimeArgs),metricsTags);
  this.userMetrics=new ProgramUserMetrics(programMetrics);
  this.retryStrategy=SystemArguments.getRetryStrategy(programOptions.getUserArguments().asMap(),program.getType(),cConf);
  Map<String,Map<String,String>> staticDatasets=new HashMap<>();
  for (  String name : datasets) {
    staticDatasets.put(name,runtimeArguments);
  }
  SystemDatasetInstantiator instantiator=new SystemDatasetInstantiator(dsFramework,program.getClassLoader(),owners);
  this.messagingService=messagingService;
  this.messagingContext=new MultiThreadMessagingContext(messagingService);
  TransactionSystemClient retryingTxClient=new RetryingShortTransactionSystemClient(txClient,retryStrategy);
  this.datasetCache=multiThreaded ? new MultiThreadDatasetCache(instantiator,retryingTxClient,program.getId().getNamespaceId(),runtimeArguments,programMetrics,staticDatasets,messagingContext) : new SingleThreadDatasetCache(instantiator,retryingTxClient,program.getId().getNamespaceId(),runtimeArguments,programMetrics,staticDatasets);
  this.pluginInstantiator=pluginInstantiator;
  this.pluginContext=new DefaultPluginContext(pluginInstantiator,program.getId(),program.getApplicationSpecification().getPlugins());
  String appPrincipalExists=programOptions.getArguments().getOption(ProgramOptionConstants.APP_PRINCIPAL_EXISTS);
  KerberosPrincipalId principalId=null;
  if (appPrincipalExists != null && Boolean.parseBoolean(appPrincipalExists)) {
    principalId=new KerberosPrincipalId(programOptions.getArguments().getOption(ProgramOptionConstants.PRINCIPAL));
  }
  this.admin=new DefaultAdmin(dsFramework,program.getId().getNamespaceId(),secureStoreManager,new BasicMessagingAdmin(messagingService,program.getId().getNamespaceId()),retryStrategy,principalId);
  this.secureStore=secureStore;
  this.defaultTxTimeout=determineTransactionTimeout(cConf);
  this.transactional=Transactions.createTransactional(getDatasetCache(),defaultTxTimeout);
  if (!multiThreaded) {
    datasetCache.addExtraTransactionAware(messagingContext);
  }
}"
4752,"/** 
 * Adds a set of new metadata entries for a particular partition. If the metadata key already exists, it will be overwritten.
 * @throws PartitionNotFoundException when a partition for the given key is not found
 * @throws IllegalArgumentException if the partition key does not match the partitioning of the dataset
 */
void addMetadata(PartitionKey key,Map<String,String> metadata);","/** 
 * Adds a set of new metadata entries for a particular partition. Note that existing entries cannot be updated.
 * @throws DataSetException when an attempt is made to update existing entries
 * @throws PartitionNotFoundException when a partition for the given key is not found
 * @throws IllegalArgumentException if the partition key does not match the partitioning of the dataset
 */
void addMetadata(PartitionKey key,Map<String,String> metadata);"
4753,"/** 
 * Adds a set of new metadata entries for a particular partition If the metadata key already exists, it will be overwritten.
 * @param time the partition time in milliseconds since the Epoch
 * @throws PartitionNotFoundException when a partition for the given time is not found
 */
void addMetadata(long time,Map<String,String> metadata);","/** 
 * Adds a set of new metadata entries for a particular partition Note that existing entries can not be updated.
 * @param time the partition time in milliseconds since the Epoch
 * @throws DataSetException in case an attempt is made to update existing entries.
 */
void addMetadata(long time,Map<String,String> metadata);"
4754,"private void addMetadataToPut(Map<String,String> metadata,Put put){
  for (  Map.Entry<String,String> entry : metadata.entrySet()) {
    byte[] columnKey=columnKeyFromMetadataKey(entry.getKey());
    put.add(columnKey,Bytes.toBytes(entry.getValue()));
  }
}","private void addMetadataToPut(Row existingRow,Map<String,String> metadata,Put put,boolean allowUpdates){
  if (!allowUpdates) {
    checkMetadataDoesNotExist(existingRow,metadata);
  }
  for (  Map.Entry<String,String> entry : metadata.entrySet()) {
    byte[] columnKey=columnKeyFromMetadataKey(entry.getKey());
    put.add(columnKey,Bytes.toBytes(entry.getValue()));
  }
}"
4755,"@WriteOnly @Override public void addMetadata(PartitionKey key,Map<String,String> metadata){
  final byte[] rowKey=generateRowKey(key,partitioning);
  Row row=partitionsTable.get(rowKey);
  if (row.isEmpty()) {
    throw new PartitionNotFoundException(key,getName());
  }
  Put put=new Put(rowKey);
  addMetadataToPut(metadata,put);
  partitionsTable.put(put);
}","@WriteOnly @Override public void addMetadata(PartitionKey key,Map<String,String> metadata){
  setMetadata(key,metadata,false);
}"
4756,"@Test public void testUpdateMetadata() throws Exception {
  final PartitionedFileSet dataset=dsFrameworkUtil.getInstance(pfsInstance);
  dsFrameworkUtil.newTransactionExecutor((TransactionAware)dataset).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      PartitionOutput partitionOutput=dataset.getPartitionOutput(PARTITION_KEY);
      ImmutableMap<String,String> originalEntries=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
      partitionOutput.setMetadata(originalEntries);
      partitionOutput.addPartition();
      ImmutableMap<String,String> updatedMetadata=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
      dataset.addMetadata(PARTITION_KEY,updatedMetadata);
      PartitionDetail partitionDetail=dataset.getPartition(PARTITION_KEY);
      Assert.assertNotNull(partitionDetail);
      HashMap<String,String> combinedEntries=Maps.newHashMap();
      combinedEntries.putAll(originalEntries);
      combinedEntries.putAll(updatedMetadata);
      Assert.assertEquals(combinedEntries,partitionDetail.getMetadata().asMap());
      dataset.addMetadata(PARTITION_KEY,""String_Node_Str"",""String_Node_Str"");
      partitionDetail=dataset.getPartition(PARTITION_KEY);
      Assert.assertNotNull(partitionDetail);
      Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),partitionDetail.getMetadata().asMap());
      dataset.removeMetadata(PARTITION_KEY,ImmutableSet.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
      partitionDetail=dataset.getPartition(PARTITION_KEY);
      Assert.assertNotNull(partitionDetail);
      Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),partitionDetail.getMetadata().asMap());
      try {
        PartitionKey nonexistentPartitionKey=PartitionKey.builder().addIntField(""String_Node_Str"",42).addLongField(""String_Node_Str"",17L).addStringField(""String_Node_Str"",""String_Node_Str"").build();
        dataset.addMetadata(nonexistentPartitionKey,""String_Node_Str"",""String_Node_Str"");
        Assert.fail(""String_Node_Str"");
      }
 catch (      DataSetException expected) {
      }
    }
  }
);
}","@Test public void testUpdateMetadata() throws Exception {
  final PartitionedFileSet dataset=dsFrameworkUtil.getInstance(pfsInstance);
  dsFrameworkUtil.newTransactionExecutor((TransactionAware)dataset).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      PartitionOutput partitionOutput=dataset.getPartitionOutput(PARTITION_KEY);
      ImmutableMap<String,String> originalEntries=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
      partitionOutput.setMetadata(originalEntries);
      partitionOutput.addPartition();
      ImmutableMap<String,String> updatedMetadata=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
      dataset.addMetadata(PARTITION_KEY,updatedMetadata);
      PartitionDetail partitionDetail=dataset.getPartition(PARTITION_KEY);
      Assert.assertNotNull(partitionDetail);
      HashMap<String,String> combinedEntries=Maps.newHashMap();
      combinedEntries.putAll(originalEntries);
      combinedEntries.putAll(updatedMetadata);
      Assert.assertEquals(combinedEntries,partitionDetail.getMetadata().asMap());
      dataset.setMetadata(PARTITION_KEY,Collections.singletonMap(""String_Node_Str"",""String_Node_Str""));
      partitionDetail=dataset.getPartition(PARTITION_KEY);
      Assert.assertNotNull(partitionDetail);
      Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),partitionDetail.getMetadata().asMap());
      try {
        dataset.addMetadata(PARTITION_KEY,""String_Node_Str"",""String_Node_Str"");
        Assert.fail(""String_Node_Str"");
      }
 catch (      DataSetException expected) {
      }
      dataset.removeMetadata(PARTITION_KEY,ImmutableSet.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
      partitionDetail=dataset.getPartition(PARTITION_KEY);
      Assert.assertNotNull(partitionDetail);
      Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),partitionDetail.getMetadata().asMap());
      try {
        PartitionKey nonexistentPartitionKey=PartitionKey.builder().addIntField(""String_Node_Str"",42).addLongField(""String_Node_Str"",17L).addStringField(""String_Node_Str"",""String_Node_Str"").build();
        dataset.addMetadata(nonexistentPartitionKey,""String_Node_Str"",""String_Node_Str"");
        Assert.fail(""String_Node_Str"");
      }
 catch (      DataSetException expected) {
      }
    }
  }
);
}"
4757,"@Test public void testPartitionMetadata() throws Exception {
  final TimePartitionedFileSet tpfs=dsFrameworkUtil.getInstance(TPFS_INSTANCE);
  TransactionAware txAware=(TransactionAware)tpfs;
  dsFrameworkUtil.newInMemoryTransactionExecutor(txAware).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      validateTimePartitions(tpfs,0L,MAX,Collections.<Long,String>emptyMap());
      Date date=DATE_FORMAT.parse(""String_Node_Str"");
      long time=date.getTime();
      Map<String,String> allMetadata=Maps.newHashMap();
      Map<String,String> metadata=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
      tpfs.addPartition(time,""String_Node_Str"",metadata);
      allMetadata.putAll(metadata);
      TimePartitionDetail partitionByTime=tpfs.getPartitionByTime(time);
      Assert.assertNotNull(partitionByTime);
      Assert.assertEquals(metadata,partitionByTime.getMetadata().asMap());
      tpfs.addMetadata(time,""String_Node_Str"",""String_Node_Str"");
      allMetadata.put(""String_Node_Str"",""String_Node_Str"");
      tpfs.addMetadata(time,""String_Node_Str"",""String_Node_Str"");
      allMetadata.put(""String_Node_Str"",""String_Node_Str"");
      Map<String,String> newMetadata=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
      tpfs.addMetadata(time,newMetadata);
      allMetadata.putAll(newMetadata);
      partitionByTime=tpfs.getPartitionByTime(time);
      Assert.assertNotNull(partitionByTime);
      Assert.assertEquals(allMetadata,partitionByTime.getMetadata().asMap());
      tpfs.removeMetadata(time,ImmutableSet.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
      allMetadata.remove(""String_Node_Str"");
      allMetadata.remove(""String_Node_Str"");
      partitionByTime=tpfs.getPartitionByTime(time);
      Assert.assertNotNull(partitionByTime);
      Assert.assertEquals(allMetadata,partitionByTime.getMetadata().asMap());
    }
  }
);
}","@Test public void testPartitionMetadata() throws Exception {
  final TimePartitionedFileSet tpfs=dsFrameworkUtil.getInstance(TPFS_INSTANCE);
  TransactionAware txAware=(TransactionAware)tpfs;
  dsFrameworkUtil.newInMemoryTransactionExecutor(txAware).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      validateTimePartitions(tpfs,0L,MAX,Collections.<Long,String>emptyMap());
      Date date=DATE_FORMAT.parse(""String_Node_Str"");
      long time=date.getTime();
      Map<String,String> allMetadata=Maps.newHashMap();
      Map<String,String> metadata=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
      tpfs.addPartition(time,""String_Node_Str"",metadata);
      allMetadata.putAll(metadata);
      TimePartitionDetail partitionByTime=tpfs.getPartitionByTime(time);
      Assert.assertNotNull(partitionByTime);
      Assert.assertEquals(metadata,partitionByTime.getMetadata().asMap());
      tpfs.addMetadata(time,""String_Node_Str"",""String_Node_Str"");
      allMetadata.put(""String_Node_Str"",""String_Node_Str"");
      tpfs.setMetadata(time,Collections.singletonMap(""String_Node_Str"",""String_Node_Str""));
      allMetadata.put(""String_Node_Str"",""String_Node_Str"");
      Map<String,String> newMetadata=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
      tpfs.addMetadata(time,newMetadata);
      allMetadata.putAll(newMetadata);
      try {
        tpfs.addMetadata(time,""String_Node_Str"",""String_Node_Str"");
        Assert.fail(""String_Node_Str"");
      }
 catch (      DataSetException expected) {
      }
      partitionByTime=tpfs.getPartitionByTime(time);
      Assert.assertNotNull(partitionByTime);
      Assert.assertEquals(allMetadata,partitionByTime.getMetadata().asMap());
      tpfs.removeMetadata(time,ImmutableSet.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
      allMetadata.remove(""String_Node_Str"");
      allMetadata.remove(""String_Node_Str"");
      partitionByTime=tpfs.getPartitionByTime(time);
      Assert.assertNotNull(partitionByTime);
      Assert.assertEquals(allMetadata,partitionByTime.getMetadata().asMap());
    }
  }
);
}"
4758,"@Test public void testProgramList() throws Exception {
  SecurityRequestContext.setUserId(ALICE.getName());
  ApplicationId applicationId=NamespaceId.DEFAULT.app(AllProgramsApp.NAME);
  Map<EntityId,Set<Action>> neededPrivileges=ImmutableMap.<EntityId,Set<Action>>builder().put(applicationId,EnumSet.of(Action.ADMIN)).put(NamespaceId.DEFAULT.artifact(AllProgramsApp.class.getSimpleName(),""String_Node_Str""),EnumSet.of(Action.ADMIN)).put(NamespaceId.DEFAULT.dataset(AllProgramsApp.DATASET_NAME),EnumSet.of(Action.ADMIN)).put(NamespaceId.DEFAULT.dataset(AllProgramsApp.DATASET_NAME2),EnumSet.of(Action.ADMIN)).put(NamespaceId.DEFAULT.dataset(AllProgramsApp.DATASET_NAME3),EnumSet.of(Action.ADMIN)).put(NamespaceId.DEFAULT.dataset(AllProgramsApp.DS_WITH_SCHEMA_NAME),EnumSet.of(Action.ADMIN)).put(NamespaceId.DEFAULT.stream(AllProgramsApp.STREAM_NAME),EnumSet.of(Action.ADMIN)).put(NamespaceId.DEFAULT.datasetType(KeyValueTable.class.getName()),EnumSet.of(Action.ADMIN)).put(NamespaceId.DEFAULT.datasetType(ObjectMappedTable.class.getName()),EnumSet.of(Action.ADMIN)).build();
  setUpPrivilegesAndExpectFailedDeploy(neededPrivileges);
  AppFabricTestHelper.deployApplication(Id.Namespace.DEFAULT,AllProgramsApp.class,null,cConf);
  for (  ProgramType type : ProgramType.values()) {
    if (!type.equals(ProgramType.CUSTOM_ACTION) && !type.equals(ProgramType.WEBAPP)) {
      Assert.assertTrue(programLifecycleService.list(NamespaceId.DEFAULT,type).isEmpty());
    }
  }
  authorizer.grant(applicationId.program(ProgramType.FLOW,AllProgramsApp.NoOpFlow.NAME),ALICE,Collections.singleton(Action.EXECUTE));
  authorizer.grant(applicationId.program(ProgramType.SERVICE,AllProgramsApp.NoOpService.NAME),ALICE,Collections.singleton(Action.EXECUTE));
  authorizer.grant(applicationId.program(ProgramType.WORKER,AllProgramsApp.NoOpWorker.NAME),ALICE,Collections.singleton(Action.EXECUTE));
  authorizer.grant(applicationId.program(ProgramType.SPARK,AllProgramsApp.NoOpSpark.NAME),ALICE,Collections.singleton(Action.EXECUTE));
  authorizer.grant(applicationId.program(ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR.NAME),ALICE,Collections.singleton(Action.EXECUTE));
  authorizer.grant(applicationId.program(ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR2.NAME),ALICE,Collections.singleton(Action.EXECUTE));
  authorizer.grant(applicationId.program(ProgramType.WORKFLOW,AllProgramsApp.NoOpWorkflow.NAME),ALICE,Collections.singleton(Action.EXECUTE));
  for (  ProgramType type : ProgramType.values()) {
    if (!type.equals(ProgramType.CUSTOM_ACTION) && !type.equals(ProgramType.WEBAPP)) {
      Assert.assertFalse(programLifecycleService.list(NamespaceId.DEFAULT,type).isEmpty());
      SecurityRequestContext.setUserId(""String_Node_Str"");
      Assert.assertTrue(programLifecycleService.list(NamespaceId.DEFAULT,type).isEmpty());
      SecurityRequestContext.setUserId(""String_Node_Str"");
    }
  }
}","@Test public void testProgramList() throws Exception {
  SecurityRequestContext.setUserId(ALICE.getName());
  ApplicationId applicationId=NamespaceId.DEFAULT.app(AllProgramsApp.NAME);
  Map<EntityId,Set<Action>> neededPrivileges=ImmutableMap.<EntityId,Set<Action>>builder().put(applicationId,EnumSet.of(Action.ADMIN)).put(NamespaceId.DEFAULT.artifact(AllProgramsApp.class.getSimpleName(),""String_Node_Str""),EnumSet.of(Action.ADMIN)).put(NamespaceId.DEFAULT.dataset(AllProgramsApp.DATASET_NAME),EnumSet.of(Action.ADMIN)).put(NamespaceId.DEFAULT.dataset(AllProgramsApp.DATASET_NAME2),EnumSet.of(Action.ADMIN)).put(NamespaceId.DEFAULT.dataset(AllProgramsApp.DATASET_NAME3),EnumSet.of(Action.ADMIN)).put(NamespaceId.DEFAULT.dataset(AllProgramsApp.DS_WITH_SCHEMA_NAME),EnumSet.of(Action.ADMIN)).put(NamespaceId.DEFAULT.stream(AllProgramsApp.STREAM_NAME),EnumSet.of(Action.ADMIN)).build();
  setUpPrivilegesAndExpectFailedDeploy(neededPrivileges);
  AppFabricTestHelper.deployApplication(Id.Namespace.DEFAULT,AllProgramsApp.class,null,cConf);
  for (  ProgramType type : ProgramType.values()) {
    if (!type.equals(ProgramType.CUSTOM_ACTION) && !type.equals(ProgramType.WEBAPP)) {
      Assert.assertTrue(programLifecycleService.list(NamespaceId.DEFAULT,type).isEmpty());
    }
  }
  authorizer.grant(applicationId.program(ProgramType.FLOW,AllProgramsApp.NoOpFlow.NAME),ALICE,Collections.singleton(Action.EXECUTE));
  authorizer.grant(applicationId.program(ProgramType.SERVICE,AllProgramsApp.NoOpService.NAME),ALICE,Collections.singleton(Action.EXECUTE));
  authorizer.grant(applicationId.program(ProgramType.WORKER,AllProgramsApp.NoOpWorker.NAME),ALICE,Collections.singleton(Action.EXECUTE));
  authorizer.grant(applicationId.program(ProgramType.SPARK,AllProgramsApp.NoOpSpark.NAME),ALICE,Collections.singleton(Action.EXECUTE));
  authorizer.grant(applicationId.program(ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR.NAME),ALICE,Collections.singleton(Action.EXECUTE));
  authorizer.grant(applicationId.program(ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR2.NAME),ALICE,Collections.singleton(Action.EXECUTE));
  authorizer.grant(applicationId.program(ProgramType.WORKFLOW,AllProgramsApp.NoOpWorkflow.NAME),ALICE,Collections.singleton(Action.EXECUTE));
  for (  ProgramType type : ProgramType.values()) {
    if (!type.equals(ProgramType.CUSTOM_ACTION) && !type.equals(ProgramType.WEBAPP)) {
      Assert.assertFalse(programLifecycleService.list(NamespaceId.DEFAULT,type).isEmpty());
      SecurityRequestContext.setUserId(""String_Node_Str"");
      Assert.assertTrue(programLifecycleService.list(NamespaceId.DEFAULT,type).isEmpty());
      SecurityRequestContext.setUserId(""String_Node_Str"");
    }
  }
}"
4759,"/** 
 * Drops all datasets in the given namespace. If authorization is turned on, only datasets that the current principal that has   {@link Action#ADMIN} privilege will be deleted
 * @param namespaceId namespace to operate on
 * @throws Exception if it fails to delete dataset
 * @return the set of {@link DatasetId} that get deleted
 */
void dropAll(NamespaceId namespaceId) throws Exception {
  ensureNamespaceExists(namespaceId);
  Principal principal=authenticationContext.getPrincipal();
  Map<DatasetId,DatasetSpecification> datasets=new HashMap<>();
  for (  DatasetSpecification spec : instanceManager.getAll(namespaceId)) {
    DatasetId datasetId=namespaceId.dataset(spec.getName());
    if (!DatasetsUtil.isSystemDatasetInUserNamespace(datasetId)) {
      authorizationEnforcer.enforce(datasetId,principal,Action.ADMIN);
    }
    datasets.put(datasetId,spec);
  }
  for (  DatasetId datasetId : datasets.keySet()) {
    dropDataset(datasetId,datasets.get(datasetId));
  }
}","/** 
 * Drops all datasets in the given namespace. If authorization is turned on, only datasets that the current principal that has   {@link Action#ADMIN} privilege will be deleted
 * @param namespaceId namespace to operate on
 * @throws Exception if it fails to delete dataset
 */
void dropAll(NamespaceId namespaceId) throws Exception {
  ensureNamespaceExists(namespaceId);
  Principal principal=authenticationContext.getPrincipal();
  Map<DatasetId,DatasetSpecification> datasets=new HashMap<>();
  for (  DatasetSpecification spec : instanceManager.getAll(namespaceId)) {
    DatasetId datasetId=namespaceId.dataset(spec.getName());
    if (!DatasetsUtil.isSystemDatasetInUserNamespace(datasetId)) {
      authorizationEnforcer.enforce(datasetId,principal,Action.ADMIN);
    }
    datasets.put(datasetId,spec);
  }
  for (  DatasetId datasetId : datasets.keySet()) {
    dropDataset(datasetId,datasets.get(datasetId));
  }
}"
4760,"/** 
 * Finds the   {@link DatasetTypeMeta} for the specified dataset type name.Search order - first in the specified namespace, then in the 'system' namespace from defaultModules
 * @param namespaceId {@link NamespaceId} for the specified namespace
 * @param typeName the name of the dataset type to search
 * @param byPassCheck a flag which determines whether to check privilege for the dataset type
 * @return {@link DatasetTypeMeta} for the type if found in either the specified namespace or in the system namespace,null otherwise. TODO: This may need to move to a util class eventually
 */
@Nullable private DatasetTypeMeta getTypeInfo(NamespaceId namespaceId,String typeName,boolean byPassCheck) throws Exception {
  DatasetTypeId datasetTypeId=ConversionHelpers.toDatasetTypeId(namespaceId,typeName);
  try {
    return byPassCheck ? noAuthDatasetTypeService.getType(datasetTypeId) : authorizationDatasetTypeService.getType(datasetTypeId);
  }
 catch (  DatasetTypeNotFoundException e) {
    try {
      DatasetTypeId systemDatasetTypeId=ConversionHelpers.toDatasetTypeId(NamespaceId.SYSTEM,typeName);
      return noAuthDatasetTypeService.getType(systemDatasetTypeId);
    }
 catch (    DatasetTypeNotFoundException exnWithSystemNS) {
      throw e;
    }
  }
}","/** 
 * Finds the   {@link DatasetTypeMeta} for the specified dataset type name.Search order - first in the specified namespace, then in the 'system' namespace from defaultModules
 * @param namespaceId {@link NamespaceId} for the specified namespace
 * @param typeName the name of the dataset type to search
 * @param byPassCheck a flag which determines whether to check privilege for the dataset type
 * @return {@link DatasetTypeMeta} for the type if found in either the specified namespace or in the system namespace,null otherwise. TODO: This may need to move to a util class eventually
 */
@Nullable private DatasetTypeMeta getTypeInfo(NamespaceId namespaceId,String typeName,boolean byPassCheck) throws Exception {
  DatasetTypeId datasetTypeId=ConversionHelpers.toDatasetTypeId(namespaceId,typeName);
  try {
    return byPassCheck ? noAuthDatasetTypeService.getType(datasetTypeId) : authorizationDatasetTypeService.getType(datasetTypeId);
  }
 catch (  DatasetTypeNotFoundException|UnauthorizedException e) {
    try {
      DatasetTypeId systemDatasetTypeId=ConversionHelpers.toDatasetTypeId(NamespaceId.SYSTEM,typeName);
      return noAuthDatasetTypeService.getType(systemDatasetTypeId);
    }
 catch (    DatasetTypeNotFoundException exnWithSystemNS) {
      throw e;
    }
  }
}"
4761,"@Test public void testDatasetInstances() throws Exception {
  final DatasetId dsId=NamespaceId.DEFAULT.dataset(""String_Node_Str"");
  final DatasetId dsId1=NamespaceId.DEFAULT.dataset(""String_Node_Str"");
  DatasetTypeId tableTypeId=NamespaceId.DEFAULT.datasetType(Table.class.getName());
  DatasetId dsId2=NamespaceId.DEFAULT.dataset(""String_Node_Str"");
  SecurityRequestContext.setUserId(ALICE.getName());
  assertAuthorizationFailure(new DatasetOperationExecutor(){
    @Override public void execute() throws Exception {
      dsFramework.addInstance(Table.class.getName(),dsId,DatasetProperties.EMPTY);
    }
  }
,""String_Node_Str"");
  grantAndAssertSuccess(dsId,ALICE,ImmutableSet.of(Action.ADMIN));
  grantAndAssertSuccess(tableTypeId,ALICE,EnumSet.of(Action.ADMIN));
  dsFramework.addInstance(Table.class.getName(),dsId,DatasetProperties.EMPTY);
  Assert.assertTrue(dsFramework.hasInstance(dsId));
  Assert.assertNotNull(dsFramework.getDataset(dsId,ImmutableMap.<String,String>of(),null));
  dsFramework.updateInstance(dsId,DatasetProperties.builder().add(""String_Node_Str"",""String_Node_Str"").build());
  SecurityRequestContext.setUserId(BOB.getName());
  assertAuthorizationFailure(new DatasetOperationExecutor(){
    @Override public void execute() throws Exception {
      dsFramework.getDataset(dsId,ImmutableMap.<String,String>of(),null);
    }
  }
,String.format(""String_Node_Str"",BOB,dsId));
  assertAuthorizationFailure(new DatasetOperationExecutor(){
    @Override public void execute() throws Exception {
      dsFramework.updateInstance(dsId,DatasetProperties.builder().add(""String_Node_Str"",""String_Node_Str"").build());
    }
  }
,String.format(""String_Node_Str"",BOB,Action.ADMIN,dsId));
  assertAuthorizationFailure(new DatasetOperationExecutor(){
    @Override public void execute() throws Exception {
      dsFramework.truncateInstance(dsId);
    }
  }
,String.format(""String_Node_Str"",BOB,Action.ADMIN,dsId));
  grantAndAssertSuccess(dsId,BOB,ImmutableSet.of(Action.ADMIN));
  dsFramework.updateInstance(dsId,DatasetProperties.builder().add(""String_Node_Str"",""String_Node_Str"").build());
  dsFramework.truncateInstance(dsId);
  DatasetSpecification datasetSpec=dsFramework.getDatasetSpec(dsId);
  Assert.assertNotNull(datasetSpec);
  Assert.assertEquals(""String_Node_Str"",datasetSpec.getProperty(""String_Node_Str""));
  grantAndAssertSuccess(dsId1,BOB,ImmutableSet.of(Action.ADMIN));
  grantAndAssertSuccess(dsId2,BOB,ImmutableSet.of(Action.ADMIN));
  grantAndAssertSuccess(tableTypeId,BOB,EnumSet.of(Action.ADMIN));
  dsFramework.addInstance(Table.class.getName(),dsId1,DatasetProperties.EMPTY);
  dsFramework.addInstance(Table.class.getName(),dsId2,DatasetProperties.EMPTY);
  Assert.assertEquals(ImmutableSet.of(dsId,dsId1,dsId2),summaryToDatasetIdSet(dsFramework.getInstances(NamespaceId.DEFAULT)));
  SecurityRequestContext.setUserId(ALICE.getName());
  Assert.assertEquals(ImmutableSet.of(dsId),summaryToDatasetIdSet(dsFramework.getInstances(NamespaceId.DEFAULT)));
  grantAndAssertSuccess(dsId1,ALICE,ImmutableSet.of(Action.EXECUTE));
  grantAndAssertSuccess(dsId2,ALICE,ImmutableSet.of(Action.EXECUTE));
  try {
    dsFramework.deleteAllInstances(NamespaceId.DEFAULT);
    Assert.fail();
  }
 catch (  Exception e) {
    Assert.assertTrue(e.getMessage().contains(""String_Node_Str""));
  }
  Assert.assertEquals(ImmutableSet.of(dsId1,dsId2,dsId),summaryToDatasetIdSet(dsFramework.getInstances(NamespaceId.DEFAULT)));
  assertAuthorizationFailure(new DatasetOperationExecutor(){
    @Override public void execute() throws Exception {
      dsFramework.deleteInstance(dsId1);
    }
  }
,String.format(""String_Node_Str"",dsId1));
  grantAndAssertSuccess(dsId1,ALICE,ImmutableSet.of(Action.ADMIN));
  Assert.assertEquals(ImmutableSet.of(dsId1,dsId2,dsId),summaryToDatasetIdSet(dsFramework.getInstances(NamespaceId.DEFAULT)));
  dsFramework.deleteInstance(dsId1);
  Assert.assertEquals(ImmutableSet.of(dsId2,dsId),summaryToDatasetIdSet(dsFramework.getInstances(NamespaceId.DEFAULT)));
  SecurityRequestContext.setUserId(BOB.getName());
  Assert.assertEquals(ImmutableSet.of(dsId2,dsId),summaryToDatasetIdSet(dsFramework.getInstances(NamespaceId.DEFAULT)));
  dsFramework.deleteInstance(dsId2);
  SecurityRequestContext.setUserId(ALICE.getName());
  dsFramework.deleteInstance(dsId);
  grantAndAssertSuccess(dsId2,ALICE,EnumSet.of(Action.ADMIN));
  dsFramework.addInstance(Table.class.getName(),dsId,DatasetProperties.EMPTY);
  dsFramework.addInstance(Table.class.getName(),dsId1,DatasetProperties.EMPTY);
  dsFramework.addInstance(Table.class.getName(),dsId2,DatasetProperties.EMPTY);
  Assert.assertEquals(ImmutableSet.of(dsId,dsId1,dsId2),summaryToDatasetIdSet(dsFramework.getInstances(NamespaceId.DEFAULT)));
  dsFramework.deleteAllInstances(NamespaceId.DEFAULT);
  Assert.assertTrue(dsFramework.getInstances(NamespaceId.DEFAULT).isEmpty());
}","@Test public void testDatasetInstances() throws Exception {
  final DatasetId dsId=NamespaceId.DEFAULT.dataset(""String_Node_Str"");
  final DatasetId dsId1=NamespaceId.DEFAULT.dataset(""String_Node_Str"");
  DatasetId dsId2=NamespaceId.DEFAULT.dataset(""String_Node_Str"");
  SecurityRequestContext.setUserId(ALICE.getName());
  assertAuthorizationFailure(new DatasetOperationExecutor(){
    @Override public void execute() throws Exception {
      dsFramework.addInstance(Table.class.getName(),dsId,DatasetProperties.EMPTY);
    }
  }
,""String_Node_Str"");
  grantAndAssertSuccess(dsId,ALICE,ImmutableSet.of(Action.ADMIN));
  dsFramework.addInstance(Table.class.getName(),dsId,DatasetProperties.EMPTY);
  Assert.assertTrue(dsFramework.hasInstance(dsId));
  Assert.assertNotNull(dsFramework.getDataset(dsId,ImmutableMap.<String,String>of(),null));
  dsFramework.updateInstance(dsId,DatasetProperties.builder().add(""String_Node_Str"",""String_Node_Str"").build());
  SecurityRequestContext.setUserId(BOB.getName());
  assertAuthorizationFailure(new DatasetOperationExecutor(){
    @Override public void execute() throws Exception {
      dsFramework.getDataset(dsId,ImmutableMap.<String,String>of(),null);
    }
  }
,String.format(""String_Node_Str"",BOB,dsId));
  assertAuthorizationFailure(new DatasetOperationExecutor(){
    @Override public void execute() throws Exception {
      dsFramework.updateInstance(dsId,DatasetProperties.builder().add(""String_Node_Str"",""String_Node_Str"").build());
    }
  }
,String.format(""String_Node_Str"",BOB,Action.ADMIN,dsId));
  assertAuthorizationFailure(new DatasetOperationExecutor(){
    @Override public void execute() throws Exception {
      dsFramework.truncateInstance(dsId);
    }
  }
,String.format(""String_Node_Str"",BOB,Action.ADMIN,dsId));
  grantAndAssertSuccess(dsId,BOB,ImmutableSet.of(Action.ADMIN));
  dsFramework.updateInstance(dsId,DatasetProperties.builder().add(""String_Node_Str"",""String_Node_Str"").build());
  dsFramework.truncateInstance(dsId);
  DatasetSpecification datasetSpec=dsFramework.getDatasetSpec(dsId);
  Assert.assertNotNull(datasetSpec);
  Assert.assertEquals(""String_Node_Str"",datasetSpec.getProperty(""String_Node_Str""));
  grantAndAssertSuccess(dsId1,BOB,ImmutableSet.of(Action.ADMIN));
  grantAndAssertSuccess(dsId2,BOB,ImmutableSet.of(Action.ADMIN));
  dsFramework.addInstance(Table.class.getName(),dsId1,DatasetProperties.EMPTY);
  dsFramework.addInstance(Table.class.getName(),dsId2,DatasetProperties.EMPTY);
  Assert.assertEquals(ImmutableSet.of(dsId,dsId1,dsId2),summaryToDatasetIdSet(dsFramework.getInstances(NamespaceId.DEFAULT)));
  SecurityRequestContext.setUserId(ALICE.getName());
  Assert.assertEquals(ImmutableSet.of(dsId),summaryToDatasetIdSet(dsFramework.getInstances(NamespaceId.DEFAULT)));
  grantAndAssertSuccess(dsId1,ALICE,ImmutableSet.of(Action.EXECUTE));
  grantAndAssertSuccess(dsId2,ALICE,ImmutableSet.of(Action.EXECUTE));
  try {
    dsFramework.deleteAllInstances(NamespaceId.DEFAULT);
    Assert.fail();
  }
 catch (  Exception e) {
    Assert.assertTrue(e.getMessage().contains(""String_Node_Str""));
  }
  Assert.assertEquals(ImmutableSet.of(dsId1,dsId2,dsId),summaryToDatasetIdSet(dsFramework.getInstances(NamespaceId.DEFAULT)));
  assertAuthorizationFailure(new DatasetOperationExecutor(){
    @Override public void execute() throws Exception {
      dsFramework.deleteInstance(dsId1);
    }
  }
,String.format(""String_Node_Str"",dsId1));
  grantAndAssertSuccess(dsId1,ALICE,ImmutableSet.of(Action.ADMIN));
  Assert.assertEquals(ImmutableSet.of(dsId1,dsId2,dsId),summaryToDatasetIdSet(dsFramework.getInstances(NamespaceId.DEFAULT)));
  dsFramework.deleteInstance(dsId1);
  Assert.assertEquals(ImmutableSet.of(dsId2,dsId),summaryToDatasetIdSet(dsFramework.getInstances(NamespaceId.DEFAULT)));
  SecurityRequestContext.setUserId(BOB.getName());
  Assert.assertEquals(ImmutableSet.of(dsId2,dsId),summaryToDatasetIdSet(dsFramework.getInstances(NamespaceId.DEFAULT)));
  dsFramework.deleteInstance(dsId2);
  SecurityRequestContext.setUserId(ALICE.getName());
  dsFramework.deleteInstance(dsId);
  grantAndAssertSuccess(dsId2,ALICE,EnumSet.of(Action.ADMIN));
  dsFramework.addInstance(Table.class.getName(),dsId,DatasetProperties.EMPTY);
  dsFramework.addInstance(Table.class.getName(),dsId1,DatasetProperties.EMPTY);
  dsFramework.addInstance(Table.class.getName(),dsId2,DatasetProperties.EMPTY);
  Assert.assertEquals(ImmutableSet.of(dsId,dsId1,dsId2),summaryToDatasetIdSet(dsFramework.getInstances(NamespaceId.DEFAULT)));
  dsFramework.deleteAllInstances(NamespaceId.DEFAULT);
  Assert.assertTrue(dsFramework.getInstances(NamespaceId.DEFAULT).isEmpty());
}"
4762,"@Inject TokenSecureStoreRenewer(YarnConfiguration hConf,CConfiguration cConf,LocationFactory locationFactory,SecureStore secureStore){
  this.hConf=hConf;
  this.locationFactory=locationFactory;
  this.secureStore=secureStore;
  this.secureExplore=cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED) && UserGroupInformation.isSecurityEnabled();
}","@Inject TokenSecureStoreRenewer(YarnConfiguration yarnConf,CConfiguration cConf,LocationFactory locationFactory,SecureStore secureStore){
  this.yarnConf=yarnConf;
  this.locationFactory=locationFactory;
  this.secureStore=secureStore;
  this.secureExplore=cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED) && UserGroupInformation.isSecurityEnabled();
}"
4763,"/** 
 * Creates a   {@link Credentials} that contains delegation tokens of the current user for all services that CDAP uses.
 */
public Credentials createCredentials(){
  try {
    Credentials refreshedCredentials=new Credentials();
    if (User.isSecurityEnabled()) {
      YarnTokenUtils.obtainToken(hConf,refreshedCredentials);
    }
    if (User.isHBaseSecurityEnabled(hConf)) {
      HBaseTokenUtils.obtainToken(hConf,refreshedCredentials);
    }
    if (secureExplore) {
      HiveTokenUtils.obtainToken(refreshedCredentials);
      JobHistoryServerTokenUtils.obtainToken(hConf,refreshedCredentials);
    }
    if (secureStore instanceof DelegationTokensUpdater) {
      String renewer=UserGroupInformation.getCurrentUser().getShortUserName();
      ((DelegationTokensUpdater)secureStore).addDelegationTokens(renewer,refreshedCredentials);
    }
    YarnUtils.addDelegationTokens(hConf,locationFactory,refreshedCredentials);
    return refreshedCredentials;
  }
 catch (  IOException ioe) {
    throw Throwables.propagate(ioe);
  }
}","/** 
 * Creates a   {@link Credentials} that contains delegation tokens of the current user for all services that CDAP uses.
 */
public Credentials createCredentials(){
  try {
    Credentials refreshedCredentials=new Credentials();
    if (User.isSecurityEnabled()) {
      YarnTokenUtils.obtainToken(yarnConf,refreshedCredentials);
    }
    if (User.isHBaseSecurityEnabled(yarnConf)) {
      HBaseTokenUtils.obtainToken(yarnConf,refreshedCredentials);
    }
    if (secureExplore) {
      HiveTokenUtils.obtainToken(refreshedCredentials);
      JobHistoryServerTokenUtils.obtainToken(yarnConf,refreshedCredentials);
    }
    if (secureStore instanceof DelegationTokensUpdater) {
      String renewer=UserGroupInformation.getCurrentUser().getShortUserName();
      ((DelegationTokensUpdater)secureStore).addDelegationTokens(renewer,refreshedCredentials);
    }
    YarnUtils.addDelegationTokens(yarnConf,locationFactory,refreshedCredentials);
    return refreshedCredentials;
  }
 catch (  IOException ioe) {
    throw Throwables.propagate(ioe);
  }
}"
4764,"private long calculateUpdateInterval(){
  List<Long> renewalTimes=Lists.newArrayList();
  renewalTimes.add(hConf.getLong(DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_RENEW_INTERVAL_KEY,DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT));
  renewalTimes.add(hConf.getLong(Constants.HBase.AUTH_KEY_UPDATE_INTERVAL,TimeUnit.MILLISECONDS.convert(1,TimeUnit.DAYS)));
  if (secureExplore) {
    renewalTimes.add(hConf.getLong(YarnConfiguration.DELEGATION_TOKEN_RENEW_INTERVAL_KEY,YarnConfiguration.DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT));
    Configuration hiveConf=getHiveConf();
    if (hiveConf != null) {
      renewalTimes.add(hiveConf.getLong(""String_Node_Str"",TimeUnit.MILLISECONDS.convert(1,TimeUnit.DAYS)));
    }
 else {
      renewalTimes.add(TimeUnit.MILLISECONDS.convert(1,TimeUnit.DAYS));
    }
    renewalTimes.add(hConf.getLong(MRConfig.DELEGATION_TOKEN_RENEW_INTERVAL_KEY,MRConfig.DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT));
  }
  Long minimumInterval=Collections.min(renewalTimes);
  long delay=minimumInterval - TimeUnit.HOURS.toMillis(1);
  if (delay <= 0) {
    delay=(minimumInterval <= 2) ? 1 : minimumInterval / 2;
  }
  LOG.info(""String_Node_Str"",delay);
  return delay;
}","private long calculateUpdateInterval(){
  List<Long> renewalTimes=Lists.newArrayList();
  renewalTimes.add(yarnConf.getLong(DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_RENEW_INTERVAL_KEY,DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT));
  renewalTimes.add(yarnConf.getLong(Constants.HBase.AUTH_KEY_UPDATE_INTERVAL,TimeUnit.MILLISECONDS.convert(1,TimeUnit.DAYS)));
  if (yarnConf.getBoolean(Constants.Explore.TIMELINE_SERVICE_ENABLED,false)) {
    renewalTimes.add(yarnConf.getLong(Constants.Explore.TIMELINE_DELEGATION_KEY_UPDATE_INTERVAL,TimeUnit.MILLISECONDS.convert(1,TimeUnit.DAYS)));
  }
  if (secureExplore) {
    renewalTimes.add(yarnConf.getLong(YarnConfiguration.DELEGATION_TOKEN_RENEW_INTERVAL_KEY,YarnConfiguration.DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT));
    Configuration hiveConf=getHiveConf();
    if (hiveConf != null) {
      renewalTimes.add(hiveConf.getLong(""String_Node_Str"",TimeUnit.MILLISECONDS.convert(1,TimeUnit.DAYS)));
    }
 else {
      renewalTimes.add(TimeUnit.MILLISECONDS.convert(1,TimeUnit.DAYS));
    }
    renewalTimes.add(yarnConf.getLong(MRConfig.DELEGATION_TOKEN_RENEW_INTERVAL_KEY,MRConfig.DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT));
  }
  Long minimumInterval=Collections.min(renewalTimes);
  long delay=minimumInterval - TimeUnit.HOURS.toMillis(1);
  if (delay <= 0) {
    delay=(minimumInterval <= 2) ? 1 : minimumInterval / 2;
  }
  LOG.info(""String_Node_Str"",delay);
  return delay;
}"
4765,"/** 
 * Gets a Yarn delegation token and stores it in the given Credentials.
 * @return the same Credentials instance as the one given in parameter.
 */
public static Credentials obtainToken(YarnConfiguration configuration,Credentials credentials){
  if (!UserGroupInformation.isSecurityEnabled()) {
    return credentials;
  }
  try {
    YarnClient yarnClient=YarnClient.createYarnClient();
    yarnClient.init(configuration);
    yarnClient.start();
    try {
      Text renewer=new Text(UserGroupInformation.getCurrentUser().getShortUserName());
      org.apache.hadoop.yarn.api.records.Token rmDelegationToken=yarnClient.getRMDelegationToken(renewer);
      List<String> services=new ArrayList<>();
      if (HAUtil.isHAEnabled(configuration)) {
        YarnConfiguration yarnConf=new YarnConfiguration(configuration);
        for (        String rmId : HAUtil.getRMHAIds(configuration)) {
          yarnConf.set(YarnConfiguration.RM_HA_ID,rmId);
          InetSocketAddress address=yarnConf.getSocketAddr(YarnConfiguration.RM_ADDRESS,YarnConfiguration.DEFAULT_RM_ADDRESS,YarnConfiguration.DEFAULT_RM_PORT);
          services.add(SecurityUtil.buildTokenService(address).toString());
        }
      }
 else {
        services.add(SecurityUtil.buildTokenService(YarnUtils.getRMAddress(configuration)).toString());
      }
      Token<TokenIdentifier> token=ConverterUtils.convertFromYarn(rmDelegationToken,null);
      token.setService(new Text(Joiner.on(',').join(services)));
      credentials.addToken(new Text(token.getService()),token);
      LOG.debug(""String_Node_Str"",token);
    }
  finally {
      yarnClient.stop();
    }
    return credentials;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","/** 
 * Gets a Yarn delegation token and stores it in the given Credentials. Also gets Yarn App Timeline Server, if it is enabled.
 * @return the same Credentials instance as the one given in parameter.
 */
public static Credentials obtainToken(YarnConfiguration configuration,Credentials credentials){
  if (!UserGroupInformation.isSecurityEnabled()) {
    return credentials;
  }
  try {
    YarnClient yarnClient=YarnClient.createYarnClient();
    yarnClient.init(configuration);
    yarnClient.start();
    try {
      if (configuration.getBoolean(Constants.Explore.TIMELINE_SERVICE_ENABLED,false)) {
        Method method=yarnClient.getClass().getDeclaredMethod(""String_Node_Str"");
        method.setAccessible(true);
        Token<? extends TokenIdentifier> atsToken=(Token<? extends TokenIdentifier>)method.invoke(yarnClient);
        if (atsToken != null) {
          credentials.addToken(atsToken.getService(),atsToken);
          LOG.info(""String_Node_Str"",atsToken);
        }
      }
      Text renewer=new Text(UserGroupInformation.getCurrentUser().getShortUserName());
      org.apache.hadoop.yarn.api.records.Token rmDelegationToken=yarnClient.getRMDelegationToken(renewer);
      List<String> services=new ArrayList<>();
      if (HAUtil.isHAEnabled(configuration)) {
        YarnConfiguration yarnConf=new YarnConfiguration(configuration);
        for (        String rmId : HAUtil.getRMHAIds(configuration)) {
          yarnConf.set(YarnConfiguration.RM_HA_ID,rmId);
          InetSocketAddress address=yarnConf.getSocketAddr(YarnConfiguration.RM_ADDRESS,YarnConfiguration.DEFAULT_RM_ADDRESS,YarnConfiguration.DEFAULT_RM_PORT);
          services.add(SecurityUtil.buildTokenService(address).toString());
        }
      }
 else {
        services.add(SecurityUtil.buildTokenService(YarnUtils.getRMAddress(configuration)).toString());
      }
      Token<TokenIdentifier> token=ConverterUtils.convertFromYarn(rmDelegationToken,null);
      token.setService(new Text(Joiner.on(',').join(services)));
      credentials.addToken(new Text(token.getService()),token);
      LOG.debug(""String_Node_Str"",token);
    }
  finally {
      yarnClient.stop();
    }
    return credentials;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
4766,"@Override public void prepareForDeferredProcessing(){
  loggingEvent.prepareForDeferredProcessing();
}","@Override public void prepareForDeferredProcessing(){
  loggingEvent.prepareForDeferredProcessing();
  Object[] args=loggingEvent.getArgumentArray();
  if (args != null && argumentArray == null) {
    argumentArray=new Object[args.length];
    for (int i=0; i < args.length; i++) {
      argumentArray[i]=args[i].toString();
    }
  }
}"
4767,"@Override public Object[] getArgumentArray(){
  return loggingEvent.getArgumentArray();
}","@Override public Object[] getArgumentArray(){
  return argumentArray == null ? loggingEvent.getArgumentArray() : argumentArray;
}"
4768,"/** 
 * Encodes a   {@link ILoggingEvent} to byte array.
 */
public byte[] toBytes(ILoggingEvent event){
  event.prepareForDeferredProcessing();
  ByteArrayOutputStream out=new ByteArrayOutputStream();
  BinaryEncoder encoder=EncoderFactory.get().directBinaryEncoder(out,null);
  GenericDatumWriter<GenericRecord> writer=new GenericDatumWriter<>(getAvroSchema());
  try {
    writer.write(toGenericRecord(event),encoder);
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
  return out.toByteArray();
}","/** 
 * Encodes a   {@link ILoggingEvent} to byte array.
 */
public byte[] toBytes(ILoggingEvent event){
  ByteArrayOutputStream out=new ByteArrayOutputStream();
  BinaryEncoder encoder=EncoderFactory.get().directBinaryEncoder(out,null);
  GenericDatumWriter<GenericRecord> writer=new GenericDatumWriter<>(getAvroSchema());
  try {
    writer.write(toGenericRecord(event),encoder);
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
  return out.toByteArray();
}"
4769,"@BeforeClass public static void setup() throws Exception {
  SConfiguration sConf=SConfiguration.create();
  sConf.set(Constants.Security.Store.FILE_PASSWORD,""String_Node_Str"");
  CConfiguration cConf=createCConf();
  final Injector injector=AppFabricTestHelper.getInjector(cConf,sConf,new AbstractModule(){
    @Override protected void configure(){
    }
  }
);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  waitForService(Constants.Service.DATASET_MANAGER);
  secureStore=injector.getInstance(SecureStore.class);
  secureStoreManager=injector.getInstance(SecureStoreManager.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  String user=SecurityUtil.getMasterPrincipal(cConf);
  authorizer.grant(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
  injector.getInstance(NamespaceAdmin.class).create(NamespaceMeta.DEFAULT);
  authorizer.revoke(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
}","@BeforeClass public static void setup() throws Exception {
  SConfiguration sConf=SConfiguration.create();
  sConf.set(Constants.Security.Store.FILE_PASSWORD,""String_Node_Str"");
  CConfiguration cConf=createCConf();
  final Injector injector=AppFabricTestHelper.getInjector(cConf,sConf,new AbstractModule(){
    @Override protected void configure(){
    }
  }
);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  waitForService(Constants.Service.DATASET_MANAGER);
  secureStore=injector.getInstance(SecureStore.class);
  secureStoreManager=injector.getInstance(SecureStoreManager.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  String user=SecurityUtil.getMasterPrincipal(cConf);
  authorizer.grant(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
  NamespaceAdmin namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  if (!namespaceAdmin.exists(NamespaceId.DEFAULT)) {
    namespaceAdmin.create(NamespaceMeta.DEFAULT);
  }
  authorizer.revoke(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
}"
4770,"@BeforeClass public static void setup() throws Exception {
  cConf=createCConf();
  Injector injector=AppFabricTestHelper.getInjector(cConf);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  programLifecycleService=injector.getInstance(ProgramLifecycleService.class);
  String user=SecurityUtil.getMasterPrincipal(cConf);
  authorizer.grant(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
  injector.getInstance(NamespaceAdmin.class).create(NamespaceMeta.DEFAULT);
  authorizer.revoke(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
}","@BeforeClass public static void setup() throws Exception {
  cConf=createCConf();
  Injector injector=AppFabricTestHelper.getInjector(cConf);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  programLifecycleService=injector.getInstance(ProgramLifecycleService.class);
  String user=SecurityUtil.getMasterPrincipal(cConf);
  authorizer.grant(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
  NamespaceAdmin namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  if (!namespaceAdmin.exists(NamespaceId.DEFAULT)) {
    namespaceAdmin.create(NamespaceMeta.DEFAULT);
  }
  authorizer.revoke(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
}"
4771,"@BeforeClass public static void setup() throws Exception {
  cConf=createCConf();
  final Injector injector=AppFabricTestHelper.getInjector(cConf);
  metadataAdmin=injector.getInstance(MetadataAdmin.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  String user=SecurityUtil.getMasterPrincipal(cConf);
  authorizer.grant(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
  injector.getInstance(NamespaceAdmin.class).create(NamespaceMeta.DEFAULT);
  authorizer.revoke(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
}","@BeforeClass public static void setup() throws Exception {
  cConf=createCConf();
  final Injector injector=AppFabricTestHelper.getInjector(cConf);
  metadataAdmin=injector.getInstance(MetadataAdmin.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  String user=SecurityUtil.getMasterPrincipal(cConf);
  authorizer.grant(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
  NamespaceAdmin namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  if (!namespaceAdmin.exists(NamespaceId.DEFAULT)) {
    namespaceAdmin.create(NamespaceMeta.DEFAULT);
  }
  authorizer.revoke(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
}"
4772,"@Inject public TransactionHttpHandler(CConfiguration cConf,TransactionSystemClient txClient){
  this.cConf=cConf;
  this.txClient=new TransactionSystemClientAdapter(txClient);
  this.pruneEnable=cConf.getBoolean(TxConstants.TransactionPruning.PRUNE_ENABLE,TxConstants.TransactionPruning.DEFAULT_PRUNE_ENABLE);
}","@Inject public TransactionHttpHandler(Configuration hConf,CConfiguration cConf,TransactionSystemClient txClient){
  this.hConf=hConf;
  this.cConf=cConf;
  this.txClient=new TransactionSystemClientAdapter(txClient);
  this.pruneEnable=cConf.getBoolean(TxConstants.TransactionPruning.PRUNE_ENABLE,TxConstants.TransactionPruning.DEFAULT_PRUNE_ENABLE);
}"
4773,"private boolean initializePruningDebug(HttpResponder responder){
  if (!pruneEnable) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return false;
  }
synchronized (this) {
    if (debugClazz == null || debugObject == null) {
      try {
        this.debugClazz=getClass().getClassLoader().loadClass(""String_Node_Str"");
        this.debugObject=debugClazz.newInstance();
        Configuration hConf=new Configuration();
        for (        Map.Entry<String,String> entry : cConf) {
          hConf.set(entry.getKey(),entry.getValue());
        }
        Method initMethod=debugClazz.getMethod(""String_Node_Str"",Configuration.class);
        initMethod.setAccessible(true);
        initMethod.invoke(debugObject,hConf);
      }
 catch (      ClassNotFoundException|IllegalAccessException|InstantiationException ex) {
        LOG.debug(""String_Node_Str"",ex);
        this.debugClazz=null;
      }
catch (      NoSuchMethodException|InvocationTargetException ex) {
        LOG.debug(""String_Node_Str"",ex);
        this.debugClazz=null;
      }
    }
  }
  return true;
}","private boolean initializePruningDebug(HttpResponder responder){
  if (!pruneEnable) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return false;
  }
synchronized (this) {
    if (debugClazz == null || debugObject == null) {
      try {
        this.debugClazz=getClass().getClassLoader().loadClass(""String_Node_Str"");
        this.debugObject=debugClazz.newInstance();
        for (        Map.Entry<String,String> entry : cConf) {
          hConf.set(entry.getKey(),entry.getValue());
        }
        Method initMethod=debugClazz.getMethod(""String_Node_Str"",Configuration.class);
        initMethod.setAccessible(true);
        initMethod.invoke(debugObject,hConf);
      }
 catch (      ClassNotFoundException|IllegalAccessException|InstantiationException ex) {
        LOG.debug(""String_Node_Str"",ex);
        this.debugClazz=null;
      }
catch (      NoSuchMethodException|InvocationTargetException ex) {
        LOG.debug(""String_Node_Str"",ex);
        this.debugClazz=null;
      }
    }
  }
  return true;
}"
4774,"@Path(""String_Node_Str"") @GET public void getRegionsToBeCompacted(HttpRequest request,HttpResponder responder){
  if (!initializePruningDebug(responder)) {
    return;
  }
  try {
    Method method=debugClazz.getMethod(""String_Node_Str"");
    method.setAccessible(true);
    Object response=method.invoke(debugObject);
    Set<String> regionNames=(Set<String>)response;
    responder.sendJson(HttpResponseStatus.OK,regionNames);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}","@Path(""String_Node_Str"") @GET public void getRegionsToBeCompacted(HttpRequest request,HttpResponder responder,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numRegions){
  if (!initializePruningDebug(responder)) {
    return;
  }
  try {
    Method method=debugClazz.getMethod(""String_Node_Str"",Integer.class);
    method.setAccessible(true);
    Object response=method.invoke(debugObject,numRegions);
    Set<String> regionNames=(Set<String>)response;
    responder.sendJson(HttpResponseStatus.OK,regionNames);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}"
4775,"public static synchronized Injector getInjector(CConfiguration conf,@Nullable SConfiguration sConf,Module overrides){
  if (injector == null) {
    configuration=conf;
    configuration.set(Constants.CFG_LOCAL_DATA_DIR,TEMP_FOLDER.newFolder(""String_Node_Str"").getAbsolutePath());
    configuration.set(Constants.AppFabric.REST_PORT,Integer.toString(Networks.getRandomPort()));
    configuration.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
    injector=Guice.createInjector(Modules.override(new AppFabricTestModule(configuration,sConf)).with(overrides));
    if (configuration.getBoolean(Constants.Security.ENABLED) && configuration.getBoolean(Constants.Security.Authorization.ENABLED)) {
      injector.getInstance(AuthorizationBootstrapper.class).run();
    }
    MessagingService messagingService=injector.getInstance(MessagingService.class);
    if (messagingService instanceof Service) {
      ((Service)messagingService).startAndWait();
    }
    injector.getInstance(TransactionManager.class).startAndWait();
    injector.getInstance(DatasetOpExecutor.class).startAndWait();
    injector.getInstance(DatasetService.class).startAndWait();
    injector.getInstance(StreamCoordinatorClient.class).startAndWait();
    injector.getInstance(NotificationService.class).startAndWait();
    injector.getInstance(MetricsCollectionService.class).startAndWait();
    Scheduler programScheduler=injector.getInstance(Scheduler.class);
    if (programScheduler instanceof Service) {
      ((Service)programScheduler).startAndWait();
    }
  }
  return injector;
}","public static synchronized Injector getInjector(CConfiguration conf,@Nullable SConfiguration sConf,Module overrides){
  if (injector == null) {
    configuration=conf;
    configuration.set(Constants.CFG_LOCAL_DATA_DIR,TEMP_FOLDER.newFolder(""String_Node_Str"").getAbsolutePath());
    configuration.set(Constants.AppFabric.REST_PORT,Integer.toString(Networks.getRandomPort()));
    configuration.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
    injector=Guice.createInjector(Modules.override(new AppFabricTestModule(configuration,sConf)).with(overrides));
    if (configuration.getBoolean(Constants.Security.ENABLED) && configuration.getBoolean(Constants.Security.Authorization.ENABLED)) {
    }
    MessagingService messagingService=injector.getInstance(MessagingService.class);
    if (messagingService instanceof Service) {
      ((Service)messagingService).startAndWait();
    }
    injector.getInstance(TransactionManager.class).startAndWait();
    injector.getInstance(DatasetOpExecutor.class).startAndWait();
    injector.getInstance(DatasetService.class).startAndWait();
    injector.getInstance(StreamCoordinatorClient.class).startAndWait();
    injector.getInstance(NotificationService.class).startAndWait();
    injector.getInstance(MetricsCollectionService.class).startAndWait();
    Scheduler programScheduler=injector.getInstance(Scheduler.class);
    if (programScheduler instanceof Service) {
      ((Service)programScheduler).startAndWait();
    }
  }
  return injector;
}"
4776,"@BeforeClass public static void setup() throws Exception {
  SConfiguration sConf=SConfiguration.create();
  sConf.set(Constants.Security.Store.FILE_PASSWORD,""String_Node_Str"");
  final Injector injector=AppFabricTestHelper.getInjector(createCConf(),sConf,new AbstractModule(){
    @Override protected void configure(){
    }
  }
);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  waitForService(Constants.Service.DATASET_MANAGER);
  secureStore=injector.getInstance(SecureStore.class);
  secureStoreManager=injector.getInstance(SecureStoreManager.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  authorizer.grant(NamespaceId.DEFAULT,ALICE,Collections.singleton(Action.READ));
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return injector.getInstance(NamespaceAdmin.class).exists(NamespaceId.DEFAULT);
    }
  }
,5,TimeUnit.SECONDS);
  authorizer.revoke(NamespaceId.DEFAULT,ALICE,Collections.singleton(Action.READ));
}","@BeforeClass public static void setup() throws Exception {
  SConfiguration sConf=SConfiguration.create();
  sConf.set(Constants.Security.Store.FILE_PASSWORD,""String_Node_Str"");
  CConfiguration cConf=createCConf();
  final Injector injector=AppFabricTestHelper.getInjector(cConf,sConf,new AbstractModule(){
    @Override protected void configure(){
    }
  }
);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  waitForService(Constants.Service.DATASET_MANAGER);
  secureStore=injector.getInstance(SecureStore.class);
  secureStoreManager=injector.getInstance(SecureStoreManager.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  String user=SecurityUtil.getMasterPrincipal(cConf);
  authorizer.grant(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
  injector.getInstance(NamespaceAdmin.class).create(NamespaceMeta.DEFAULT);
  authorizer.revoke(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
}"
4777,"@BeforeClass public static void setup() throws Exception {
  cConf=createCConf();
  Injector injector=AppFabricTestHelper.getInjector(cConf);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  programLifecycleService=injector.getInstance(ProgramLifecycleService.class);
}","@BeforeClass public static void setup() throws Exception {
  cConf=createCConf();
  Injector injector=AppFabricTestHelper.getInjector(cConf);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  programLifecycleService=injector.getInstance(ProgramLifecycleService.class);
  String user=SecurityUtil.getMasterPrincipal(cConf);
  authorizer.grant(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
  injector.getInstance(NamespaceAdmin.class).create(NamespaceMeta.DEFAULT);
  authorizer.revoke(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
}"
4778,"@BeforeClass public static void setup() throws Exception {
  SConfiguration sConf=SConfiguration.create();
  sConf.set(Constants.Security.Store.FILE_PASSWORD,""String_Node_Str"");
  CConfiguration cConf=createCConf();
  final Injector injector=AppFabricTestHelper.getInjector(cConf,sConf,new AbstractModule(){
    @Override protected void configure(){
    }
  }
);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  waitForService(Constants.Service.DATASET_MANAGER);
  secureStore=injector.getInstance(SecureStore.class);
  secureStoreManager=injector.getInstance(SecureStoreManager.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  String user=SecurityUtil.getMasterPrincipal(cConf);
  authorizer.grant(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
  NamespaceAdmin namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  if (!namespaceAdmin.exists(NamespaceId.DEFAULT)) {
    namespaceAdmin.create(NamespaceMeta.DEFAULT);
  }
  authorizer.revoke(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
}","@BeforeClass public static void setup() throws Exception {
  SConfiguration sConf=SConfiguration.create();
  sConf.set(Constants.Security.Store.FILE_PASSWORD,""String_Node_Str"");
  CConfiguration cConf=createCConf();
  final Injector injector=AppFabricTestHelper.getInjector(cConf,sConf,new AbstractModule(){
    @Override protected void configure(){
    }
  }
);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  waitForService(Constants.Service.DATASET_MANAGER);
  secureStore=injector.getInstance(SecureStore.class);
  secureStoreManager=injector.getInstance(SecureStoreManager.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  String user=SecurityUtil.getMasterPrincipal(cConf);
  authorizer.grant(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return injector.getInstance(NamespaceAdmin.class).exists(NamespaceId.DEFAULT);
    }
  }
,5,TimeUnit.SECONDS);
  authorizer.revoke(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
}"
4779,"@BeforeClass public static void setup() throws Exception {
  cConf=createCConf();
  final Injector injector=AppFabricTestHelper.getInjector(cConf);
  metadataAdmin=injector.getInstance(MetadataAdmin.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  String user=SecurityUtil.getMasterPrincipal(cConf);
  authorizer.grant(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
  NamespaceAdmin namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  if (!namespaceAdmin.exists(NamespaceId.DEFAULT)) {
    namespaceAdmin.create(NamespaceMeta.DEFAULT);
  }
  authorizer.revoke(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
}","@BeforeClass public static void setup() throws Exception {
  cConf=createCConf();
  final Injector injector=AppFabricTestHelper.getInjector(cConf);
  metadataAdmin=injector.getInstance(MetadataAdmin.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  String user=SecurityUtil.getMasterPrincipal(cConf);
  authorizer.grant(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return injector.getInstance(NamespaceAdmin.class).exists(NamespaceId.DEFAULT);
    }
  }
,5,TimeUnit.SECONDS);
  authorizer.revoke(NamespaceId.DEFAULT,new Principal(user,Principal.PrincipalType.USER),Collections.singleton(Action.ADMIN));
}"
4780,"/** 
 * Call a Callable.
 * @param callable the callable to call
 * @param < T > the return type
 * @return the result of the callable
 * @throws Exception if there was any exception encountered while calling the callable
 */
public <T>T call(Callable<T> callable) throws Exception {
  return call(callable,CallArgs.NONE);
}","/** 
 * Call a Callable.
 * @param callable the callable to call
 * @param < T > the return type
 * @return the result of the callable
 * @throws Exception if there was any exception encountered while calling the callable
 */
public abstract <T>T call(Callable<T> callable) throws Exception ;"
4781,"/** 
 * Call a Callable that does not throw checked exceptions with the specified arguments. It is up to you to ensure that it does not throw checked exceptions. Otherwise, any checked exceptions will be wrapped in a RuntimeException and propagated.
 * @param callable the callable to call
 * @param args arguments that may override default behavior
 * @param < T > the return type
 * @return the result of the callable
 */
public <T>T callUnchecked(Callable<T> callable,CallArgs args){
  try {
    return call(callable,args);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","/** 
 * Call a Callable that does not throw checked exceptions. It is up to you to ensure that it does not throw checked exceptions. Otherwise, any checked exceptions will be wrapped in a RuntimeException and propagated.
 * @param callable the callable to call
 * @param < T > the return type
 * @return the result of the callable
 */
public <T>T callUnchecked(Callable<T> callable){
  try {
    return call(callable);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
4782,"@Override public <T>T call(Callable<T> callable,CallArgs args) throws Exception {
  String stage=MDC.get(Constants.MDC_STAGE_KEY);
  if (stage == null) {
    return delegate.call(callable,args);
  }
  MDC.remove(Constants.MDC_STAGE_KEY);
  try {
    return delegate.call(callable,args);
  }
  finally {
    MDC.put(Constants.MDC_STAGE_KEY,stage);
  }
}","@Override public <T>T call(Callable<T> callable) throws Exception {
  String stage=MDC.get(Constants.MDC_STAGE_KEY);
  if (stage == null) {
    return delegate.call(callable);
  }
  MDC.remove(Constants.MDC_STAGE_KEY);
  try {
    return delegate.call(callable);
  }
  finally {
    MDC.put(Constants.MDC_STAGE_KEY,stage);
  }
}"
4783,"public Caller getCaller(String pluginId){
  Caller caller=Caller.DEFAULT;
  if (processTimingEnabled) {
    caller=TimingCaller.wrap(caller,new DefaultStageMetrics(metrics,pluginId));
  }
  if (stageLoggingEnabled) {
    caller=StageLoggingCaller.wrap(caller,pluginId);
  }
  return caller;
}","public Caller getCaller(String pluginId){
  Caller caller=Caller.DEFAULT;
  if (stageLoggingEnabled) {
    caller=StageLoggingCaller.wrap(caller,pluginId);
  }
  return caller;
}"
4784,"private Object wrapPlugin(String pluginId,Object plugin){
  Caller caller=getCaller(pluginId);
  if (plugin instanceof Action) {
    return new WrappedAction((Action)plugin,caller);
  }
 else   if (plugin instanceof BatchSource) {
    return new WrappedBatchSource<>((BatchSource)plugin,caller);
  }
 else   if (plugin instanceof BatchSink) {
    return new WrappedBatchSink<>((BatchSink)plugin,caller);
  }
 else   if (plugin instanceof ErrorTransform) {
    return new WrappedErrorTransform<>((ErrorTransform)plugin,caller);
  }
 else   if (plugin instanceof Transform) {
    return new WrappedTransform<>((Transform)plugin,caller);
  }
 else   if (plugin instanceof BatchAggregator) {
    return new WrappedBatchAggregator<>((BatchAggregator)plugin,caller);
  }
 else   if (plugin instanceof BatchJoiner) {
    return new WrappedBatchJoiner<>((BatchJoiner)plugin,caller);
  }
 else   if (plugin instanceof PostAction) {
    return new WrappedPostAction((PostAction)plugin,caller);
  }
  return wrapUnknownPlugin(pluginId,plugin,caller);
}","private Object wrapPlugin(String pluginId,Object plugin){
  Caller caller=getCaller(pluginId);
  StageMetrics stageMetrics=new DefaultStageMetrics(metrics,pluginId);
  OperationTimer operationTimer=processTimingEnabled ? new MetricsOperationTimer(stageMetrics) : NoOpOperationTimer.INSTANCE;
  if (plugin instanceof Action) {
    return new WrappedAction((Action)plugin,caller);
  }
 else   if (plugin instanceof BatchSource) {
    return new WrappedBatchSource<>((BatchSource)plugin,caller,operationTimer);
  }
 else   if (plugin instanceof BatchSink) {
    return new WrappedBatchSink<>((BatchSink)plugin,caller,operationTimer);
  }
 else   if (plugin instanceof ErrorTransform) {
    return new WrappedErrorTransform<>((ErrorTransform)plugin,caller,operationTimer);
  }
 else   if (plugin instanceof Transform) {
    return new WrappedTransform<>((Transform)plugin,caller,operationTimer);
  }
 else   if (plugin instanceof BatchAggregator) {
    return new WrappedBatchAggregator<>((BatchAggregator)plugin,caller,operationTimer);
  }
 else   if (plugin instanceof BatchJoiner) {
    return new WrappedBatchJoiner<>((BatchJoiner)plugin,caller,operationTimer);
  }
 else   if (plugin instanceof PostAction) {
    return new WrappedPostAction((PostAction)plugin,caller);
  }
 else   if (plugin instanceof SplitterTransform) {
    return new WrappedSplitterTransform<>((SplitterTransform)plugin,caller,operationTimer);
  }
  return wrapUnknownPlugin(pluginId,plugin,caller);
}"
4785,"@Override public <T>T call(Callable<T> callable,CallArgs args) throws Exception {
  MDC.put(Constants.MDC_STAGE_KEY,stageName);
  try {
    return delegate.call(callable,args);
  }
  finally {
    MDC.remove(Constants.MDC_STAGE_KEY);
  }
}","@Override public <T>T call(Callable<T> callable) throws Exception {
  MDC.put(Constants.MDC_STAGE_KEY,stageName);
  try {
    return delegate.call(callable);
  }
  finally {
    MDC.remove(Constants.MDC_STAGE_KEY);
  }
}"
4786,"private ScanBuilder configureRangeScan(ScanBuilder scan,@Nullable byte[] startRow,@Nullable byte[] stopRow,@Nullable FuzzyRowFilter filter){
  scan.setCaching(1000);
  if (startRow != null) {
    scan.setStartRow(startRow);
  }
  if (stopRow != null) {
    scan.setStopRow(stopRow);
  }
  scan.addFamily(columnFamily);
  if (filter != null) {
    List<Pair<byte[],byte[]>> fuzzyPairs=Lists.newArrayListWithExpectedSize(filter.getFuzzyKeysData().size());
    for (    ImmutablePair<byte[],byte[]> pair : filter.getFuzzyKeysData()) {
      if (rowKeyDistributor != null) {
        fuzzyPairs.addAll(rowKeyDistributor.getDistributedFilterPairs(pair));
      }
 else {
        fuzzyPairs.add(Pair.newPair(pair.getFirst(),pair.getSecond()));
      }
    }
    scan.setFilter(new org.apache.hadoop.hbase.filter.FuzzyRowFilter(fuzzyPairs));
  }
  return scan;
}","private ScanBuilder configureRangeScan(ScanBuilder scan,@Nullable byte[] startRow,@Nullable byte[] stopRow,@Nullable FuzzyRowFilter filter){
  scan.setCaching(1000);
  if (startRow != null) {
    scan.setStartRow(startRow);
  }
  if (stopRow != null) {
    scan.setStopRow(stopRow);
  }
  scan.addFamily(columnFamily);
  if (filter != null) {
    List<Pair<byte[],byte[]>> fuzzyPairs=Lists.newArrayListWithExpectedSize(filter.getFuzzyKeysData().size());
    for (    ImmutablePair<byte[],byte[]> pair : filter.getFuzzyKeysData()) {
      if (rowKeyDistributor != null) {
        fuzzyPairs.addAll(rowKeyDistributor.getDistributedFilterPairs(pair));
      }
 else {
        fuzzyPairs.add(Pair.newPair(Arrays.copyOf(pair.getFirst(),pair.getFirst().length),Arrays.copyOf(pair.getSecond(),pair.getSecond().length)));
      }
    }
    scan.setFilter(new org.apache.hadoop.hbase.filter.FuzzyRowFilter(fuzzyPairs));
  }
  return scan;
}"
4787,"protected MetricsTable getOrCreateResolutionMetricsTable(String v3TableName,TableProperties.Builder props,int resolution){
  try {
    MetricsTable v2Table=null;
    if (cConf.getBoolean(Constants.Metrics.METRICS_V2_TABLE_SCAN_ENABLED)) {
      String v2TableName=cConf.get(Constants.Metrics.METRICS_TABLE_PREFIX,Constants.Metrics.DEFAULT_METRIC_TABLE_PREFIX + ""String_Node_Str"" + resolution);
      DatasetId v2TableId=NamespaceId.SYSTEM.dataset(v2TableName);
      v2Table=dsFramework.getDataset(v2TableId,null,null);
    }
    props.add(HBaseTableAdmin.PROPERTY_SPLITS,GSON.toJson(getV3MetricsTableSplits(Constants.Metrics.METRICS_HBASE_SPLITS)));
    props.add(Constants.Metrics.METRICS_HBASE_MAX_SCAN_THREADS,cConf.get(Constants.Metrics.METRICS_HBASE_MAX_SCAN_THREADS));
    DatasetId v3TableId=NamespaceId.SYSTEM.dataset(v3TableName);
    MetricsTable v3Table=DatasetsUtil.getOrCreateDataset(dsFramework,v3TableId,MetricsTable.class.getName(),props.build(),null);
    if (v2Table != null) {
      return new CombinedHBaseMetricsTable(v2Table,v3Table);
    }
    return v3Table;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","protected MetricsTable getOrCreateResolutionMetricsTable(String v3TableName,TableProperties.Builder props,int resolution){
  try {
    MetricsTable v2Table=null;
    if (cConf.getBoolean(Constants.Metrics.METRICS_V2_TABLE_SCAN_ENABLED)) {
      String v2TableName=cConf.get(Constants.Metrics.METRICS_TABLE_PREFIX,Constants.Metrics.DEFAULT_METRIC_TABLE_PREFIX + ""String_Node_Str"" + resolution);
      DatasetId v2TableId=NamespaceId.SYSTEM.dataset(v2TableName);
      v2Table=dsFramework.getDataset(v2TableId,ImmutableMap.<String,String>of(),null);
    }
    props.add(HBaseTableAdmin.PROPERTY_SPLITS,GSON.toJson(getV3MetricsTableSplits(Constants.Metrics.METRICS_HBASE_SPLITS)));
    props.add(Constants.Metrics.METRICS_HBASE_MAX_SCAN_THREADS,cConf.get(Constants.Metrics.METRICS_HBASE_MAX_SCAN_THREADS));
    DatasetId v3TableId=NamespaceId.SYSTEM.dataset(v3TableName);
    MetricsTable v3Table=DatasetsUtil.getOrCreateDataset(dsFramework,v3TableId,MetricsTable.class.getName(),props.build(),null);
    if (v2Table != null) {
      return new CombinedHBaseMetricsTable(v2Table,v3Table);
    }
    return v3Table;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
4788,"@Test public void testWorkflowForkApp() throws Exception {
  File directory=tmpFolder.newFolder();
  Map<String,String> runtimeArgs=new HashMap<>();
  File firstFile=new File(directory,""String_Node_Str"");
  File firstDoneFile=new File(directory,""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",firstFile.getAbsolutePath());
  runtimeArgs.put(""String_Node_Str"",firstDoneFile.getAbsolutePath());
  File branch1File=new File(directory,""String_Node_Str"");
  File branch1DoneFile=new File(directory,""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",branch1File.getAbsolutePath());
  runtimeArgs.put(""String_Node_Str"",branch1DoneFile.getAbsolutePath());
  File branch2File=new File(directory,""String_Node_Str"");
  File branch2DoneFile=new File(directory,""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",branch2File.getAbsolutePath());
  runtimeArgs.put(""String_Node_Str"",branch2DoneFile.getAbsolutePath());
  HttpResponse response=deploy(WorkflowAppWithFork.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,WorkflowAppWithFork.class.getSimpleName(),ProgramType.WORKFLOW,WorkflowAppWithFork.WorkflowWithFork.class.getSimpleName());
  setAndTestRuntimeArgs(programId,runtimeArgs);
  startProgram(programId);
  waitState(programId,ProgramStatus.RUNNING.name());
  String runId=getRunIdOfRunningProgram(programId);
  verifyFileExists(Lists.newArrayList(firstFile));
  verifyRunningProgramCount(programId,runId,1);
  stopProgram(programId);
  verifyProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(firstFile.delete());
  startProgram(programId);
  waitState(programId,ProgramStatus.RUNNING.name());
  String newRunId=getRunIdOfRunningProgram(programId);
  Assert.assertTrue(String.format(""String_Node_Str"" + ""String_Node_Str"",runId,newRunId),!runId.equals(newRunId));
  runId=newRunId;
  verifyFileExists(Lists.newArrayList(firstFile));
  verifyRunningProgramCount(programId,runId,1);
  Assert.assertTrue(firstDoneFile.createNewFile());
  verifyFileExists(Lists.newArrayList(branch1File,branch2File));
  verifyRunningProgramCount(programId,runId,2);
  stopProgram(programId,200);
  waitState(programId,ProgramStatus.STOPPED.name());
  response=getWorkflowCurrentStatus(programId,runId);
  Assert.assertEquals(404,response.getStatusLine().getStatusCode());
  verifyProgramRuns(programId,""String_Node_Str"",1);
  Assert.assertTrue(firstFile.delete());
  Assert.assertTrue(firstDoneFile.delete());
  Assert.assertTrue(branch1File.delete());
  Assert.assertTrue(branch2File.delete());
  startProgram(programId);
  waitState(programId,ProgramStatus.RUNNING.name());
  runId=getRunIdOfRunningProgram(programId);
  verifyFileExists(Lists.newArrayList(firstFile));
  verifyRunningProgramCount(programId,runId,1);
  Assert.assertTrue(firstDoneFile.createNewFile());
  verifyFileExists(Lists.newArrayList(branch1File,branch2File));
  verifyRunningProgramCount(programId,runId,2);
  Assert.assertTrue(branch1DoneFile.createNewFile());
  Assert.assertTrue(branch2DoneFile.createNewFile());
  verifyProgramRuns(programId,""String_Node_Str"");
}","@Test public void testWorkflowForkApp() throws Exception {
  File directory=tmpFolder.newFolder();
  Map<String,String> runtimeArgs=new HashMap<>();
  File firstFile=new File(directory,""String_Node_Str"");
  File firstDoneFile=new File(directory,""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",firstFile.getAbsolutePath());
  runtimeArgs.put(""String_Node_Str"",firstDoneFile.getAbsolutePath());
  File branch1File=new File(directory,""String_Node_Str"");
  File branch1DoneFile=new File(directory,""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",branch1File.getAbsolutePath());
  runtimeArgs.put(""String_Node_Str"",branch1DoneFile.getAbsolutePath());
  File branch2File=new File(directory,""String_Node_Str"");
  File branch2DoneFile=new File(directory,""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",branch2File.getAbsolutePath());
  runtimeArgs.put(""String_Node_Str"",branch2DoneFile.getAbsolutePath());
  HttpResponse response=deploy(WorkflowAppWithFork.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,WorkflowAppWithFork.class.getSimpleName(),ProgramType.WORKFLOW,WorkflowAppWithFork.WorkflowWithFork.class.getSimpleName());
  setAndTestRuntimeArgs(programId,runtimeArgs);
  startProgram(programId);
  waitState(programId,ProgramStatus.RUNNING.name());
  String runId=getRunIdOfRunningProgram(programId);
  verifyFileExists(Lists.newArrayList(firstFile));
  verifyRunningProgramCount(programId,runId,1);
  stopProgram(programId);
  verifyProgramRuns(programId,ProgramRunStatus.KILLED);
  Assert.assertTrue(firstFile.delete());
  startProgram(programId);
  waitState(programId,ProgramStatus.RUNNING.name());
  String newRunId=getRunIdOfRunningProgram(programId);
  Assert.assertTrue(String.format(""String_Node_Str"" + ""String_Node_Str"",runId,newRunId),!runId.equals(newRunId));
  runId=newRunId;
  verifyFileExists(Lists.newArrayList(firstFile));
  verifyRunningProgramCount(programId,runId,1);
  Assert.assertTrue(firstDoneFile.createNewFile());
  verifyFileExists(Lists.newArrayList(branch1File,branch2File));
  verifyRunningProgramCount(programId,runId,2);
  stopProgram(programId,200);
  waitState(programId,ProgramStatus.STOPPED.name());
  response=getWorkflowCurrentStatus(programId,runId);
  Assert.assertEquals(404,response.getStatusLine().getStatusCode());
  verifyProgramRuns(programId,ProgramRunStatus.KILLED,1);
  Assert.assertTrue(firstFile.delete());
  Assert.assertTrue(firstDoneFile.delete());
  Assert.assertTrue(branch1File.delete());
  Assert.assertTrue(branch2File.delete());
  startProgram(programId);
  waitState(programId,ProgramStatus.RUNNING.name());
  runId=getRunIdOfRunningProgram(programId);
  verifyFileExists(Lists.newArrayList(firstFile));
  verifyRunningProgramCount(programId,runId,1);
  Assert.assertTrue(firstDoneFile.createNewFile());
  verifyFileExists(Lists.newArrayList(branch1File,branch2File));
  verifyRunningProgramCount(programId,runId,2);
  Assert.assertTrue(branch1DoneFile.createNewFile());
  Assert.assertTrue(branch2DoneFile.createNewFile());
  verifyProgramRuns(programId,ProgramRunStatus.COMPLETED);
}"
4789,"@Ignore @Test public void testWorkflowSchedules() throws Exception {
  String appName=AppWithSchedule.NAME;
  String workflowName=AppWithSchedule.WORKFLOW_NAME;
  String sampleSchedule=AppWithSchedule.SCHEDULE;
  HttpResponse response=deploy(AppWithSchedule.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,appName,ProgramType.WORKFLOW,workflowName);
  Map<String,String> runtimeArguments=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  setAndTestRuntimeArgs(programId,runtimeArguments);
  List<ScheduleDetail> schedules=getSchedules(TEST_NAMESPACE2,appName,workflowName);
  Assert.assertEquals(1,schedules.size());
  String scheduleName=schedules.get(0).getName();
  Assert.assertFalse(scheduleName.isEmpty());
  List<ScheduleSpecification> specs=getScheduleSpecs(TEST_NAMESPACE2,appName,workflowName);
  Assert.assertEquals(1,specs.size());
  String specName=specs.get(0).getSchedule().getName();
  Assert.assertEquals(scheduleName,specName);
  long current=System.currentTimeMillis();
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,sampleSchedule));
  assertSchedule(programId,scheduleName,true,30,TimeUnit.SECONDS);
  List<ScheduledRuntime> runtimes=getScheduledRunTime(programId,true);
  String id=runtimes.get(0).getId();
  Assert.assertTrue(String.format(""String_Node_Str"",id,scheduleName),id.contains(scheduleName));
  Long nextRunTime=runtimes.get(0).getTime();
  Assert.assertTrue(String.format(""String_Node_Str"",nextRunTime,current),nextRunTime > current);
  verifyProgramRuns(programId,""String_Node_Str"");
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName));
  assertSchedule(programId,scheduleName,false,30,TimeUnit.SECONDS);
  List<ScheduledRuntime> previousRuntimes=getScheduledRunTime(programId,false);
  int numRuns=previousRuntimes.size();
  Assert.assertTrue(String.format(""String_Node_Str"" + ""String_Node_Str"",numRuns),numRuns >= 1);
  verifyNoRunWithStatus(programId,""String_Node_Str"");
  int workflowRuns=getProgramRuns(programId,""String_Node_Str"").size();
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,scheduleName));
  assertSchedule(programId,scheduleName,true,30,TimeUnit.SECONDS);
  verifyProgramRuns(programId,""String_Node_Str"",workflowRuns);
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName));
  assertSchedule(programId,scheduleName,false,30,TimeUnit.SECONDS);
  try {
    assertSchedule(programId,""String_Node_Str"",true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  try {
    assertSchedule(Id.Program.from(TEST_NAMESPACE1,appName,ProgramType.WORKFLOW,workflowName),scheduleName,true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  Assert.assertEquals(404,suspendSchedule(TEST_NAMESPACE1,appName,scheduleName));
  Assert.assertEquals(404,resumeSchedule(TEST_NAMESPACE1,appName,scheduleName));
  verifyNoRunWithStatus(programId,""String_Node_Str"");
  deleteApp(Id.Application.from(TEST_NAMESPACE2,AppWithSchedule.class.getSimpleName()),200);
}","@Ignore @Test public void testWorkflowSchedules() throws Exception {
  String appName=AppWithSchedule.NAME;
  String workflowName=AppWithSchedule.WORKFLOW_NAME;
  String sampleSchedule=AppWithSchedule.SCHEDULE;
  HttpResponse response=deploy(AppWithSchedule.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,appName,ProgramType.WORKFLOW,workflowName);
  Map<String,String> runtimeArguments=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  setAndTestRuntimeArgs(programId,runtimeArguments);
  List<ScheduleDetail> schedules=getSchedules(TEST_NAMESPACE2,appName,workflowName);
  Assert.assertEquals(1,schedules.size());
  String scheduleName=schedules.get(0).getName();
  Assert.assertFalse(scheduleName.isEmpty());
  List<ScheduleSpecification> specs=getScheduleSpecs(TEST_NAMESPACE2,appName,workflowName);
  Assert.assertEquals(1,specs.size());
  String specName=specs.get(0).getSchedule().getName();
  Assert.assertEquals(scheduleName,specName);
  long current=System.currentTimeMillis();
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,sampleSchedule));
  assertSchedule(programId,scheduleName,true,30,TimeUnit.SECONDS);
  List<ScheduledRuntime> runtimes=getScheduledRunTime(programId,true);
  String id=runtimes.get(0).getId();
  Assert.assertTrue(String.format(""String_Node_Str"",id,scheduleName),id.contains(scheduleName));
  Long nextRunTime=runtimes.get(0).getTime();
  Assert.assertTrue(String.format(""String_Node_Str"",nextRunTime,current),nextRunTime > current);
  verifyProgramRuns(programId,ProgramRunStatus.COMPLETED);
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName));
  assertSchedule(programId,scheduleName,false,30,TimeUnit.SECONDS);
  List<ScheduledRuntime> previousRuntimes=getScheduledRunTime(programId,false);
  int numRuns=previousRuntimes.size();
  Assert.assertTrue(String.format(""String_Node_Str"" + ""String_Node_Str"",numRuns),numRuns >= 1);
  verifyNoRunWithStatus(programId,ProgramRunStatus.RUNNING);
  int workflowRuns=getProgramRuns(programId,ProgramRunStatus.COMPLETED).size();
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,scheduleName));
  assertSchedule(programId,scheduleName,true,30,TimeUnit.SECONDS);
  verifyProgramRuns(programId,ProgramRunStatus.COMPLETED,workflowRuns);
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName));
  assertSchedule(programId,scheduleName,false,30,TimeUnit.SECONDS);
  try {
    assertSchedule(programId,""String_Node_Str"",true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  try {
    assertSchedule(Id.Program.from(TEST_NAMESPACE1,appName,ProgramType.WORKFLOW,workflowName),scheduleName,true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  Assert.assertEquals(404,suspendSchedule(TEST_NAMESPACE1,appName,scheduleName));
  Assert.assertEquals(404,resumeSchedule(TEST_NAMESPACE1,appName,scheduleName));
  verifyNoRunWithStatus(programId,ProgramRunStatus.RUNNING);
  deleteApp(Id.Application.from(TEST_NAMESPACE2,AppWithSchedule.class.getSimpleName()),200);
}"
4790,"@Category(XSlowTests.class) @Test public void testMultipleWorkflowInstances() throws Exception {
  String appWithConcurrentWorkflow=ConcurrentWorkflowApp.class.getSimpleName();
  File tempDir=tmpFolder.newFolder(appWithConcurrentWorkflow);
  File run1File=new File(tempDir,""String_Node_Str"");
  File run2File=new File(tempDir,""String_Node_Str"");
  File run1DoneFile=new File(tempDir,""String_Node_Str"");
  File run2DoneFile=new File(tempDir,""String_Node_Str"");
  String defaultNamespace=Id.Namespace.DEFAULT.getId();
  HttpResponse response=deploy(ConcurrentWorkflowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,defaultNamespace);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(Id.Namespace.DEFAULT,appWithConcurrentWorkflow,ProgramType.WORKFLOW,ConcurrentWorkflowApp.ConcurrentWorkflow.class.getSimpleName());
  startProgram(programId,ImmutableMap.of(ConcurrentWorkflowApp.FILE_TO_CREATE_ARG,run1File.getAbsolutePath(),ConcurrentWorkflowApp.DONE_FILE_ARG,run1DoneFile.getAbsolutePath()),200);
  startProgram(programId,ImmutableMap.of(ConcurrentWorkflowApp.FILE_TO_CREATE_ARG,run2File.getAbsolutePath(),ConcurrentWorkflowApp.DONE_FILE_ARG,run2DoneFile.getAbsolutePath()),200);
  while (!(run1File.exists() && run2File.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyMultipleConcurrentRuns(programId);
  Assert.assertTrue(run1DoneFile.createNewFile());
  Assert.assertTrue(run2DoneFile.createNewFile());
  verifyProgramRuns(programId,""String_Node_Str"",1);
  deleteApp(programId.getApplication(),200,60,TimeUnit.SECONDS);
}","@Category(XSlowTests.class) @Test public void testMultipleWorkflowInstances() throws Exception {
  String appWithConcurrentWorkflow=ConcurrentWorkflowApp.class.getSimpleName();
  File tempDir=tmpFolder.newFolder(appWithConcurrentWorkflow);
  File run1File=new File(tempDir,""String_Node_Str"");
  File run2File=new File(tempDir,""String_Node_Str"");
  File run1DoneFile=new File(tempDir,""String_Node_Str"");
  File run2DoneFile=new File(tempDir,""String_Node_Str"");
  String defaultNamespace=Id.Namespace.DEFAULT.getId();
  HttpResponse response=deploy(ConcurrentWorkflowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,defaultNamespace);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(Id.Namespace.DEFAULT,appWithConcurrentWorkflow,ProgramType.WORKFLOW,ConcurrentWorkflowApp.ConcurrentWorkflow.class.getSimpleName());
  startProgram(programId,ImmutableMap.of(ConcurrentWorkflowApp.FILE_TO_CREATE_ARG,run1File.getAbsolutePath(),ConcurrentWorkflowApp.DONE_FILE_ARG,run1DoneFile.getAbsolutePath()),200);
  startProgram(programId,ImmutableMap.of(ConcurrentWorkflowApp.FILE_TO_CREATE_ARG,run2File.getAbsolutePath(),ConcurrentWorkflowApp.DONE_FILE_ARG,run2DoneFile.getAbsolutePath()),200);
  while (!(run1File.exists() && run2File.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyMultipleConcurrentRuns(programId);
  Assert.assertTrue(run1DoneFile.createNewFile());
  Assert.assertTrue(run2DoneFile.createNewFile());
  verifyProgramRuns(programId,ProgramRunStatus.COMPLETED,1);
  deleteApp(programId.getApplication(),200,60,TimeUnit.SECONDS);
}"
4791,"@Category(XSlowTests.class) @Test public void testWorkflowCondition() throws Exception {
  String conditionalWorkflowApp=""String_Node_Str"";
  String conditionalWorkflow=""String_Node_Str"";
  HttpResponse response=deploy(ConditionalWorkflowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,conditionalWorkflowApp,ProgramType.WORKFLOW,conditionalWorkflow);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  File ifForkOneActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File ifForkOneActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",ifForkOneActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",ifForkOneActionDoneFile.getAbsolutePath());
  File ifForkAnotherActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File ifForkAnotherActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",ifForkAnotherActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",ifForkAnotherActionDoneFile.getAbsolutePath());
  File elseForkOneActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File elseForkOneActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",elseForkOneActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",elseForkOneActionDoneFile.getAbsolutePath());
  File elseForkAnotherActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File elseForkAnotherActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",elseForkAnotherActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",elseForkAnotherActionDoneFile.getAbsolutePath());
  File elseForkThirdActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File elseForkThirdActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",elseForkThirdActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",elseForkThirdActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",createConditionInput(""String_Node_Str"",2,12));
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  setAndTestRuntimeArgs(programId,runtimeArguments);
  startProgram(programId);
  while (!(elseForkOneActionFile.exists() && elseForkAnotherActionFile.exists() && elseForkThirdActionFile.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  String runId=getRunIdOfRunningProgram(programId);
  verifyRunningProgramCount(programId,runId,3);
  Assert.assertTrue(elseForkOneActionDoneFile.createNewFile());
  Assert.assertTrue(elseForkAnotherActionDoneFile.createNewFile());
  Assert.assertTrue(elseForkThirdActionDoneFile.createNewFile());
  verifyProgramRuns(programId,""String_Node_Str"");
  List<RunRecord> workflowHistoryRuns=getProgramRuns(programId,""String_Node_Str"");
  Id.Program recordVerifierProgramId=Id.Program.from(TEST_NAMESPACE2,conditionalWorkflowApp,ProgramType.MAPREDUCE,""String_Node_Str"");
  List<RunRecord> recordVerifierRuns=getProgramRuns(recordVerifierProgramId,""String_Node_Str"");
  Id.Program wordCountProgramId=Id.Program.from(TEST_NAMESPACE2,conditionalWorkflowApp,ProgramType.MAPREDUCE,""String_Node_Str"");
  List<RunRecord> wordCountRuns=getProgramRuns(wordCountProgramId,""String_Node_Str"");
  Assert.assertEquals(1,workflowHistoryRuns.size());
  Assert.assertEquals(1,recordVerifierRuns.size());
  Assert.assertEquals(0,wordCountRuns.size());
  runtimeArguments.put(""String_Node_Str"",createConditionInput(""String_Node_Str"",10,2));
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  setAndTestRuntimeArgs(programId,runtimeArguments);
  startProgram(programId);
  while (!(ifForkOneActionFile.exists() && ifForkAnotherActionFile.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  runId=getRunIdOfRunningProgram(programId);
  verifyRunningProgramCount(programId,runId,2);
  Assert.assertTrue(ifForkOneActionDoneFile.createNewFile());
  Assert.assertTrue(ifForkAnotherActionDoneFile.createNewFile());
  verifyProgramRuns(programId,""String_Node_Str"",1);
  workflowHistoryRuns=getProgramRuns(programId,""String_Node_Str"");
  recordVerifierRuns=getProgramRuns(recordVerifierProgramId,""String_Node_Str"");
  wordCountRuns=getProgramRuns(wordCountProgramId,""String_Node_Str"");
  Assert.assertEquals(2,workflowHistoryRuns.size());
  Assert.assertEquals(2,recordVerifierRuns.size());
  Assert.assertEquals(1,wordCountRuns.size());
}","@Category(XSlowTests.class) @Test public void testWorkflowCondition() throws Exception {
  String conditionalWorkflowApp=""String_Node_Str"";
  String conditionalWorkflow=""String_Node_Str"";
  HttpResponse response=deploy(ConditionalWorkflowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,conditionalWorkflowApp,ProgramType.WORKFLOW,conditionalWorkflow);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  File ifForkOneActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File ifForkOneActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",ifForkOneActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",ifForkOneActionDoneFile.getAbsolutePath());
  File ifForkAnotherActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File ifForkAnotherActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",ifForkAnotherActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",ifForkAnotherActionDoneFile.getAbsolutePath());
  File elseForkOneActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File elseForkOneActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",elseForkOneActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",elseForkOneActionDoneFile.getAbsolutePath());
  File elseForkAnotherActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File elseForkAnotherActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",elseForkAnotherActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",elseForkAnotherActionDoneFile.getAbsolutePath());
  File elseForkThirdActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File elseForkThirdActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",elseForkThirdActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",elseForkThirdActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",createConditionInput(""String_Node_Str"",2,12));
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  setAndTestRuntimeArgs(programId,runtimeArguments);
  startProgram(programId);
  while (!(elseForkOneActionFile.exists() && elseForkAnotherActionFile.exists() && elseForkThirdActionFile.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  String runId=getRunIdOfRunningProgram(programId);
  verifyRunningProgramCount(programId,runId,3);
  Assert.assertTrue(elseForkOneActionDoneFile.createNewFile());
  Assert.assertTrue(elseForkAnotherActionDoneFile.createNewFile());
  Assert.assertTrue(elseForkThirdActionDoneFile.createNewFile());
  verifyProgramRuns(programId,ProgramRunStatus.COMPLETED);
  List<RunRecord> workflowHistoryRuns=getProgramRuns(programId,ProgramRunStatus.COMPLETED);
  Id.Program recordVerifierProgramId=Id.Program.from(TEST_NAMESPACE2,conditionalWorkflowApp,ProgramType.MAPREDUCE,""String_Node_Str"");
  List<RunRecord> recordVerifierRuns=getProgramRuns(recordVerifierProgramId,ProgramRunStatus.COMPLETED);
  Id.Program wordCountProgramId=Id.Program.from(TEST_NAMESPACE2,conditionalWorkflowApp,ProgramType.MAPREDUCE,""String_Node_Str"");
  List<RunRecord> wordCountRuns=getProgramRuns(wordCountProgramId,ProgramRunStatus.COMPLETED);
  Assert.assertEquals(1,workflowHistoryRuns.size());
  Assert.assertEquals(1,recordVerifierRuns.size());
  Assert.assertEquals(0,wordCountRuns.size());
  runtimeArguments.put(""String_Node_Str"",createConditionInput(""String_Node_Str"",10,2));
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  setAndTestRuntimeArgs(programId,runtimeArguments);
  startProgram(programId);
  while (!(ifForkOneActionFile.exists() && ifForkAnotherActionFile.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  runId=getRunIdOfRunningProgram(programId);
  verifyRunningProgramCount(programId,runId,2);
  Assert.assertTrue(ifForkOneActionDoneFile.createNewFile());
  Assert.assertTrue(ifForkAnotherActionDoneFile.createNewFile());
  verifyProgramRuns(programId,ProgramRunStatus.COMPLETED,1);
  workflowHistoryRuns=getProgramRuns(programId,ProgramRunStatus.COMPLETED);
  recordVerifierRuns=getProgramRuns(recordVerifierProgramId,ProgramRunStatus.COMPLETED);
  wordCountRuns=getProgramRuns(wordCountProgramId,ProgramRunStatus.COMPLETED);
  Assert.assertEquals(2,workflowHistoryRuns.size());
  Assert.assertEquals(2,recordVerifierRuns.size());
  Assert.assertEquals(1,wordCountRuns.size());
}"
4792,"@Category(XSlowTests.class) @Test public void testKillSuspendedWorkflow() throws Exception {
  HttpResponse response=deploy(SleepingWorkflowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  WorkflowId workflow=new WorkflowId(TEST_NAMESPACE2,""String_Node_Str"",""String_Node_Str"");
  startProgram(workflow,200);
  waitState(workflow,ProgramStatus.RUNNING.name());
  String runId=getRunIdOfRunningProgram(workflow.toId());
  suspendWorkflow(workflow.toId(),runId,200);
  waitState(workflow,ProgramStatus.STOPPED.name());
  stopProgram(workflow.toId(),runId,200);
  waitState(workflow,ProgramStatus.STOPPED.name());
  verifyProgramRuns(workflow.toId(),ProgramRunStatus.KILLED.name(),0);
}","@Category(XSlowTests.class) @Test public void testKillSuspendedWorkflow() throws Exception {
  HttpResponse response=deploy(SleepingWorkflowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  WorkflowId workflow=new WorkflowId(TEST_NAMESPACE2,""String_Node_Str"",""String_Node_Str"");
  startProgram(workflow,200);
  waitState(workflow,ProgramStatus.RUNNING.name());
  String runId=getRunIdOfRunningProgram(workflow.toId());
  suspendWorkflow(workflow.toId(),runId,200);
  waitState(workflow,ProgramStatus.STOPPED.name());
  stopProgram(workflow.toId(),runId,200);
  waitState(workflow,ProgramStatus.STOPPED.name());
  verifyProgramRuns(workflow.toId(),ProgramRunStatus.KILLED,0);
}"
4793,"@Test @SuppressWarnings(""String_Node_Str"") public void testWorkflowToken() throws Exception {
  Assert.assertEquals(200,deploy(AppWithWorkflow.class).getStatusLine().getStatusCode());
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,AppWithWorkflow.NAME);
  final Id.Workflow workflowId=Id.Workflow.from(appId,AppWithWorkflow.SampleWorkflow.NAME);
  String outputPath=new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath();
  startProgram(workflowId,ImmutableMap.of(""String_Node_Str"",createInput(""String_Node_Str""),""String_Node_Str"",outputPath));
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return getProgramRuns(workflowId,ProgramRunStatus.COMPLETED.name()).size();
    }
  }
,60,TimeUnit.SECONDS);
  List<RunRecord> programRuns=getProgramRuns(workflowId,ProgramRunStatus.COMPLETED.name());
  Assert.assertEquals(1,programRuns.size());
  RunRecord runRecord=programRuns.get(0);
  String pid=runRecord.getPid();
  WorkflowTokenDetail workflowTokenDetail=getWorkflowToken(workflowId,pid,null,null);
  List<WorkflowTokenDetail.NodeValueDetail> nodeValueDetails=workflowTokenDetail.getTokenData().get(AppWithWorkflow.DummyAction.TOKEN_KEY);
  Assert.assertEquals(2,nodeValueDetails.size());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.FIRST_ACTION,nodeValueDetails.get(0).getNode());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.SECOND_ACTION,nodeValueDetails.get(1).getNode());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(0).getValue());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(1).getValue());
  workflowTokenDetail=getWorkflowToken(workflowId,pid,WorkflowToken.Scope.USER,AppWithWorkflow.DummyAction.TOKEN_KEY);
  nodeValueDetails=workflowTokenDetail.getTokenData().get(AppWithWorkflow.DummyAction.TOKEN_KEY);
  Assert.assertEquals(2,nodeValueDetails.size());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.FIRST_ACTION,nodeValueDetails.get(0).getNode());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.SECOND_ACTION,nodeValueDetails.get(1).getNode());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(0).getValue());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(1).getValue());
  WorkflowTokenNodeDetail nodeDetail=getWorkflowToken(workflowId,pid,AppWithWorkflow.SampleWorkflow.NAME,WorkflowToken.Scope.USER,null);
  Map<String,String> tokenData=nodeDetail.getTokenDataAtNode();
  Assert.assertEquals(2,tokenData.size());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.INITIALIZE_TOKEN_VALUE,tokenData.get(AppWithWorkflow.SampleWorkflow.INITIALIZE_TOKEN_KEY));
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.DESTROY_TOKEN_SUCCESS_VALUE,tokenData.get(AppWithWorkflow.SampleWorkflow.DESTROY_TOKEN_KEY));
  WorkflowTokenNodeDetail tokenAtNode=getWorkflowToken(workflowId,pid,AppWithWorkflow.SampleWorkflow.FIRST_ACTION,null,null);
  Map<String,String> tokenDataAtNode=tokenAtNode.getTokenDataAtNode();
  Assert.assertEquals(1,tokenDataAtNode.size());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,tokenDataAtNode.get(AppWithWorkflow.DummyAction.TOKEN_KEY));
  tokenAtNode=getWorkflowToken(workflowId,pid,AppWithWorkflow.SampleWorkflow.FIRST_ACTION,WorkflowToken.Scope.USER,AppWithWorkflow.DummyAction.TOKEN_KEY);
  tokenDataAtNode=tokenAtNode.getTokenDataAtNode();
  Assert.assertEquals(1,tokenDataAtNode.size());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,tokenDataAtNode.get(AppWithWorkflow.DummyAction.TOKEN_KEY));
}","@Test @SuppressWarnings(""String_Node_Str"") public void testWorkflowToken() throws Exception {
  Assert.assertEquals(200,deploy(AppWithWorkflow.class).getStatusLine().getStatusCode());
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,AppWithWorkflow.NAME);
  final Id.Workflow workflowId=Id.Workflow.from(appId,AppWithWorkflow.SampleWorkflow.NAME);
  String outputPath=new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath();
  startProgram(workflowId,ImmutableMap.of(""String_Node_Str"",createInput(""String_Node_Str""),""String_Node_Str"",outputPath));
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return getProgramRuns(workflowId,ProgramRunStatus.COMPLETED).size();
    }
  }
,60,TimeUnit.SECONDS);
  List<RunRecord> programRuns=getProgramRuns(workflowId,ProgramRunStatus.COMPLETED);
  Assert.assertEquals(1,programRuns.size());
  RunRecord runRecord=programRuns.get(0);
  String pid=runRecord.getPid();
  WorkflowTokenDetail workflowTokenDetail=getWorkflowToken(workflowId,pid,null,null);
  List<WorkflowTokenDetail.NodeValueDetail> nodeValueDetails=workflowTokenDetail.getTokenData().get(AppWithWorkflow.DummyAction.TOKEN_KEY);
  Assert.assertEquals(2,nodeValueDetails.size());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.FIRST_ACTION,nodeValueDetails.get(0).getNode());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.SECOND_ACTION,nodeValueDetails.get(1).getNode());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(0).getValue());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(1).getValue());
  workflowTokenDetail=getWorkflowToken(workflowId,pid,WorkflowToken.Scope.USER,AppWithWorkflow.DummyAction.TOKEN_KEY);
  nodeValueDetails=workflowTokenDetail.getTokenData().get(AppWithWorkflow.DummyAction.TOKEN_KEY);
  Assert.assertEquals(2,nodeValueDetails.size());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.FIRST_ACTION,nodeValueDetails.get(0).getNode());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.SECOND_ACTION,nodeValueDetails.get(1).getNode());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(0).getValue());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,nodeValueDetails.get(1).getValue());
  WorkflowTokenNodeDetail nodeDetail=getWorkflowToken(workflowId,pid,AppWithWorkflow.SampleWorkflow.NAME,WorkflowToken.Scope.USER,null);
  Map<String,String> tokenData=nodeDetail.getTokenDataAtNode();
  Assert.assertEquals(2,tokenData.size());
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.INITIALIZE_TOKEN_VALUE,tokenData.get(AppWithWorkflow.SampleWorkflow.INITIALIZE_TOKEN_KEY));
  Assert.assertEquals(AppWithWorkflow.SampleWorkflow.DESTROY_TOKEN_SUCCESS_VALUE,tokenData.get(AppWithWorkflow.SampleWorkflow.DESTROY_TOKEN_KEY));
  WorkflowTokenNodeDetail tokenAtNode=getWorkflowToken(workflowId,pid,AppWithWorkflow.SampleWorkflow.FIRST_ACTION,null,null);
  Map<String,String> tokenDataAtNode=tokenAtNode.getTokenDataAtNode();
  Assert.assertEquals(1,tokenDataAtNode.size());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,tokenDataAtNode.get(AppWithWorkflow.DummyAction.TOKEN_KEY));
  tokenAtNode=getWorkflowToken(workflowId,pid,AppWithWorkflow.SampleWorkflow.FIRST_ACTION,WorkflowToken.Scope.USER,AppWithWorkflow.DummyAction.TOKEN_KEY);
  tokenDataAtNode=tokenAtNode.getTokenDataAtNode();
  Assert.assertEquals(1,tokenDataAtNode.size());
  Assert.assertEquals(AppWithWorkflow.DummyAction.TOKEN_VALUE,tokenDataAtNode.get(AppWithWorkflow.DummyAction.TOKEN_KEY));
}"
4794,"@Test public void testWorkflowRuns() throws Exception {
  String appName=""String_Node_Str"";
  String workflowName=""String_Node_Str"";
  HttpResponse response=deploy(WorkflowAppWithErrorRuns.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,appName,ProgramType.WORKFLOW,workflowName);
  File instance1File=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File instance2File=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File doneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  Map<String,String> propertyMap=ImmutableMap.of(""String_Node_Str"",instance1File.getAbsolutePath(),""String_Node_Str"",doneFile.getAbsolutePath());
  startProgram(programId,propertyMap);
  propertyMap=ImmutableMap.of(""String_Node_Str"",instance2File.getAbsolutePath(),""String_Node_Str"",doneFile.getAbsolutePath());
  startProgram(programId,propertyMap);
  while (!(instance1File.exists() && instance2File.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  List<RunRecord> historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertEquals(2,historyRuns.size());
  String runId=historyRuns.get(0).getPid();
  stopProgram(programId,runId,200);
  runId=historyRuns.get(1).getPid();
  stopProgram(programId,runId,200);
  verifyProgramRuns(programId,""String_Node_Str"",1);
  File instanceFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  propertyMap=ImmutableMap.of(""String_Node_Str"",instanceFile.getAbsolutePath(),""String_Node_Str"",doneFile.getAbsolutePath());
  startProgram(programId,propertyMap);
  while (!instanceFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertEquals(1,historyRuns.size());
  Assert.assertTrue(doneFile.createNewFile());
  verifyProgramRuns(programId,""String_Node_Str"");
  propertyMap=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  startProgram(programId,propertyMap);
  verifyProgramRuns(programId,""String_Node_Str"");
}","@Test public void testWorkflowRuns() throws Exception {
  String appName=""String_Node_Str"";
  String workflowName=""String_Node_Str"";
  HttpResponse response=deploy(WorkflowAppWithErrorRuns.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,appName,ProgramType.WORKFLOW,workflowName);
  File instance1File=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File instance2File=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File doneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  Map<String,String> propertyMap=ImmutableMap.of(""String_Node_Str"",instance1File.getAbsolutePath(),""String_Node_Str"",doneFile.getAbsolutePath());
  startProgram(programId,propertyMap);
  propertyMap=ImmutableMap.of(""String_Node_Str"",instance2File.getAbsolutePath(),""String_Node_Str"",doneFile.getAbsolutePath());
  startProgram(programId,propertyMap);
  while (!(instance1File.exists() && instance2File.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyProgramRuns(programId,ProgramRunStatus.RUNNING,1);
  List<RunRecord> historyRuns=getProgramRuns(programId,ProgramRunStatus.ALL);
  String runId=historyRuns.get(0).getPid();
  stopProgram(programId,runId,200);
  runId=historyRuns.get(1).getPid();
  stopProgram(programId,runId,200);
  verifyProgramRuns(programId,ProgramRunStatus.KILLED,1);
  File instanceFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  propertyMap=ImmutableMap.of(""String_Node_Str"",instanceFile.getAbsolutePath(),""String_Node_Str"",doneFile.getAbsolutePath());
  startProgram(programId,propertyMap);
  verifyProgramRuns(programId,ProgramRunStatus.RUNNING);
  while (!instanceFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  historyRuns=getProgramRuns(programId,ProgramRunStatus.RUNNING);
  Assert.assertEquals(1,historyRuns.size());
  Assert.assertTrue(doneFile.createNewFile());
  verifyProgramRuns(programId,ProgramRunStatus.COMPLETED);
  propertyMap=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  startProgram(programId,propertyMap);
  verifyProgramRuns(programId,ProgramRunStatus.FAILED);
}"
4795,"@Test public void testWorkflowTokenPut() throws Exception {
  Assert.assertEquals(200,deploy(WorkflowTokenTestPutApp.class).getStatusLine().getStatusCode());
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,WorkflowTokenTestPutApp.NAME);
  Id.Workflow workflowId=Id.Workflow.from(appId,WorkflowTokenTestPutApp.WorkflowTokenTestPut.NAME);
  Id.Program mapReduceId=Id.Program.from(appId,ProgramType.MAPREDUCE,WorkflowTokenTestPutApp.RecordCounter.NAME);
  Id.Program sparkId=Id.Program.from(appId,ProgramType.SPARK,WorkflowTokenTestPutApp.SparkTestApp.NAME);
  String outputPath=new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath();
  startProgram(workflowId,ImmutableMap.of(""String_Node_Str"",createInputForRecordVerification(""String_Node_Str""),""String_Node_Str"",outputPath));
  waitState(workflowId,ProgramStatus.RUNNING.name());
  waitState(workflowId,ProgramStatus.STOPPED.name());
  verifyProgramRuns(workflowId,""String_Node_Str"");
  List<RunRecord> runs=getProgramRuns(workflowId,""String_Node_Str"");
  Assert.assertEquals(1,runs.size());
  String wfRunId=runs.get(0).getPid();
  WorkflowTokenDetail tokenDetail=getWorkflowToken(workflowId,wfRunId,null,null);
  List<WorkflowTokenDetail.NodeValueDetail> details=tokenDetail.getTokenData().get(""String_Node_Str"");
  Assert.assertEquals(1,details.size());
  Assert.assertEquals(wfRunId,details.get(0).getValue());
  for (  String key : new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""}) {
    Assert.assertFalse(tokenDetail.getTokenData().containsKey(key));
  }
  List<RunRecord> sparkProgramRuns=getProgramRuns(sparkId,ProgramRunStatus.COMPLETED.name());
  Assert.assertEquals(1,sparkProgramRuns.size());
}","@Test public void testWorkflowTokenPut() throws Exception {
  Assert.assertEquals(200,deploy(WorkflowTokenTestPutApp.class).getStatusLine().getStatusCode());
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,WorkflowTokenTestPutApp.NAME);
  Id.Workflow workflowId=Id.Workflow.from(appId,WorkflowTokenTestPutApp.WorkflowTokenTestPut.NAME);
  Id.Program mapReduceId=Id.Program.from(appId,ProgramType.MAPREDUCE,WorkflowTokenTestPutApp.RecordCounter.NAME);
  Id.Program sparkId=Id.Program.from(appId,ProgramType.SPARK,WorkflowTokenTestPutApp.SparkTestApp.NAME);
  String outputPath=new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath();
  startProgram(workflowId,ImmutableMap.of(""String_Node_Str"",createInputForRecordVerification(""String_Node_Str""),""String_Node_Str"",outputPath));
  waitState(workflowId,ProgramStatus.RUNNING.name());
  waitState(workflowId,ProgramStatus.STOPPED.name());
  verifyProgramRuns(workflowId,ProgramRunStatus.COMPLETED);
  List<RunRecord> runs=getProgramRuns(workflowId,ProgramRunStatus.COMPLETED);
  Assert.assertEquals(1,runs.size());
  String wfRunId=runs.get(0).getPid();
  WorkflowTokenDetail tokenDetail=getWorkflowToken(workflowId,wfRunId,null,null);
  List<WorkflowTokenDetail.NodeValueDetail> details=tokenDetail.getTokenData().get(""String_Node_Str"");
  Assert.assertEquals(1,details.size());
  Assert.assertEquals(wfRunId,details.get(0).getValue());
  for (  String key : new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""}) {
    Assert.assertFalse(tokenDetail.getTokenData().containsKey(key));
  }
  List<RunRecord> sparkProgramRuns=getProgramRuns(sparkId,ProgramRunStatus.COMPLETED);
  Assert.assertEquals(1,sparkProgramRuns.size());
}"
4796,"@Category(XSlowTests.class) @Test public void testWorkflowScopedArguments() throws Exception {
  String workflowRunIdProperty=""String_Node_Str"";
  HttpResponse response=deploy(WorkflowAppWithScopedParameters.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  ProgramId programId=Ids.namespace(TEST_NAMESPACE2).app(WorkflowAppWithScopedParameters.APP_NAME).workflow(WorkflowAppWithScopedParameters.ONE_WORKFLOW);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  setAndTestRuntimeArgs(programId.toId(),runtimeArguments);
  startProgram(programId.toId());
  waitState(programId.toId(),ProgramStatus.RUNNING.name());
  verifyProgramRuns(programId.toId(),""String_Node_Str"");
  List<RunRecord> workflowHistoryRuns=getProgramRuns(programId.toId(),""String_Node_Str"");
  String workflowRunId=workflowHistoryRuns.get(0).getPid();
  Id.Program mr1ProgramId=Id.Program.from(TEST_NAMESPACE2,WorkflowAppWithScopedParameters.APP_NAME,ProgramType.MAPREDUCE,WorkflowAppWithScopedParameters.ONE_MR);
  waitState(mr1ProgramId,ProgramStatus.RUNNING.name());
  List<RunRecord> oneMRHistoryRuns=getProgramRuns(mr1ProgramId,""String_Node_Str"");
  String expectedMessage=String.format(""String_Node_Str"" + ""String_Node_Str"",new Id.Run(mr1ProgramId,oneMRHistoryRuns.get(0).getPid()),workflowRunId);
  stopProgram(mr1ProgramId,oneMRHistoryRuns.get(0).getPid(),400,expectedMessage);
  verifyProgramRuns(programId.toId(),""String_Node_Str"");
  workflowHistoryRuns=getProgramRuns(programId.toId(),""String_Node_Str"");
  oneMRHistoryRuns=getProgramRuns(mr1ProgramId,""String_Node_Str"");
  Id.Program mr2ProgramId=Id.Program.from(TEST_NAMESPACE2,WorkflowAppWithScopedParameters.APP_NAME,ProgramType.MAPREDUCE,WorkflowAppWithScopedParameters.ANOTHER_MR);
  List<RunRecord> anotherMRHistoryRuns=getProgramRuns(mr2ProgramId,""String_Node_Str"");
  Id.Program spark1ProgramId=Id.Program.from(TEST_NAMESPACE2,WorkflowAppWithScopedParameters.APP_NAME,ProgramType.SPARK,WorkflowAppWithScopedParameters.ONE_SPARK);
  List<RunRecord> oneSparkHistoryRuns=getProgramRuns(spark1ProgramId,""String_Node_Str"");
  Id.Program spark2ProgramId=Id.Program.from(TEST_NAMESPACE2,WorkflowAppWithScopedParameters.APP_NAME,ProgramType.SPARK,WorkflowAppWithScopedParameters.ANOTHER_SPARK);
  List<RunRecord> anotherSparkHistoryRuns=getProgramRuns(spark2ProgramId,""String_Node_Str"");
  Assert.assertEquals(1,workflowHistoryRuns.size());
  Assert.assertEquals(1,oneMRHistoryRuns.size());
  Assert.assertEquals(1,anotherMRHistoryRuns.size());
  Assert.assertEquals(1,oneSparkHistoryRuns.size());
  Assert.assertEquals(1,anotherSparkHistoryRuns.size());
  Map<String,String> workflowRunRecordProperties=workflowHistoryRuns.get(0).getProperties();
  Map<String,String> oneMRRunRecordProperties=oneMRHistoryRuns.get(0).getProperties();
  Map<String,String> anotherMRRunRecordProperties=anotherMRHistoryRuns.get(0).getProperties();
  Map<String,String> oneSparkRunRecordProperties=oneSparkHistoryRuns.get(0).getProperties();
  Map<String,String> anotherSparkRunRecordProperties=anotherSparkHistoryRuns.get(0).getProperties();
  Assert.assertNotNull(oneMRRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),oneMRRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertNotNull(anotherMRRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),anotherMRRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertNotNull(oneSparkRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),oneSparkRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertNotNull(anotherSparkRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),anotherSparkRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertEquals(workflowRunRecordProperties.get(WorkflowAppWithScopedParameters.ONE_MR),oneMRHistoryRuns.get(0).getPid());
  Assert.assertEquals(workflowRunRecordProperties.get(WorkflowAppWithScopedParameters.ONE_SPARK),oneSparkHistoryRuns.get(0).getPid());
  Assert.assertEquals(workflowRunRecordProperties.get(WorkflowAppWithScopedParameters.ANOTHER_MR),anotherMRHistoryRuns.get(0).getPid());
  Assert.assertEquals(workflowRunRecordProperties.get(WorkflowAppWithScopedParameters.ANOTHER_SPARK),anotherSparkHistoryRuns.get(0).getPid());
  Map<String,WorkflowNodeStateDetail> nodeStates=getWorkflowNodeStates(programId,workflowHistoryRuns.get(0).getPid());
  Assert.assertNotNull(nodeStates);
  Assert.assertEquals(5,nodeStates.size());
  WorkflowNodeStateDetail mrNodeState=nodeStates.get(WorkflowAppWithScopedParameters.ONE_MR);
  Assert.assertNotNull(mrNodeState);
  Assert.assertEquals(WorkflowAppWithScopedParameters.ONE_MR,mrNodeState.getNodeId());
  Assert.assertEquals(oneMRHistoryRuns.get(0).getPid(),mrNodeState.getRunId());
  mrNodeState=nodeStates.get(WorkflowAppWithScopedParameters.ANOTHER_MR);
  Assert.assertNotNull(mrNodeState);
  Assert.assertEquals(WorkflowAppWithScopedParameters.ANOTHER_MR,mrNodeState.getNodeId());
  Assert.assertEquals(anotherMRHistoryRuns.get(0).getPid(),mrNodeState.getRunId());
  WorkflowNodeStateDetail sparkNodeState=nodeStates.get(WorkflowAppWithScopedParameters.ONE_SPARK);
  Assert.assertNotNull(sparkNodeState);
  Assert.assertEquals(WorkflowAppWithScopedParameters.ONE_SPARK,sparkNodeState.getNodeId());
  Assert.assertEquals(oneSparkHistoryRuns.get(0).getPid(),sparkNodeState.getRunId());
  sparkNodeState=nodeStates.get(WorkflowAppWithScopedParameters.ANOTHER_SPARK);
  Assert.assertNotNull(sparkNodeState);
  Assert.assertEquals(WorkflowAppWithScopedParameters.ANOTHER_SPARK,sparkNodeState.getNodeId());
  Assert.assertEquals(anotherSparkHistoryRuns.get(0).getPid(),sparkNodeState.getRunId());
  WorkflowNodeStateDetail oneActionNodeState=nodeStates.get(WorkflowAppWithScopedParameters.ONE_ACTION);
  Assert.assertNotNull(oneActionNodeState);
  Assert.assertEquals(WorkflowAppWithScopedParameters.ONE_ACTION,oneActionNodeState.getNodeId());
}","@Category(XSlowTests.class) @Test public void testWorkflowScopedArguments() throws Exception {
  String workflowRunIdProperty=""String_Node_Str"";
  HttpResponse response=deploy(WorkflowAppWithScopedParameters.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  ProgramId programId=Ids.namespace(TEST_NAMESPACE2).app(WorkflowAppWithScopedParameters.APP_NAME).workflow(WorkflowAppWithScopedParameters.ONE_WORKFLOW);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  setAndTestRuntimeArgs(programId.toId(),runtimeArguments);
  startProgram(programId,200);
  verifyProgramRuns(programId,ProgramRunStatus.RUNNING);
  List<RunRecord> workflowHistoryRuns=getProgramRuns(programId,ProgramRunStatus.ALL);
  String workflowRunId=workflowHistoryRuns.get(0).getPid();
  ProgramId mr1ProgramId=Ids.namespace(TEST_NAMESPACE2).app(WorkflowAppWithScopedParameters.APP_NAME).mr(WorkflowAppWithScopedParameters.ONE_MR);
  verifyProgramRuns(mr1ProgramId,ProgramRunStatus.RUNNING);
  List<RunRecord> oneMRHistoryRuns=getProgramRuns(mr1ProgramId,ProgramRunStatus.ALL);
  String expectedMessage=String.format(""String_Node_Str"" + ""String_Node_Str"",mr1ProgramId.run(oneMRHistoryRuns.get(0).getPid()),workflowRunId);
  stopProgram(mr1ProgramId.toId(),oneMRHistoryRuns.get(0).getPid(),400,expectedMessage);
  verifyProgramRuns(programId.toId(),ProgramRunStatus.COMPLETED);
  workflowHistoryRuns=getProgramRuns(programId.toId(),ProgramRunStatus.COMPLETED);
  oneMRHistoryRuns=getProgramRuns(mr1ProgramId,ProgramRunStatus.COMPLETED);
  Id.Program mr2ProgramId=Id.Program.from(TEST_NAMESPACE2,WorkflowAppWithScopedParameters.APP_NAME,ProgramType.MAPREDUCE,WorkflowAppWithScopedParameters.ANOTHER_MR);
  List<RunRecord> anotherMRHistoryRuns=getProgramRuns(mr2ProgramId,ProgramRunStatus.COMPLETED);
  Id.Program spark1ProgramId=Id.Program.from(TEST_NAMESPACE2,WorkflowAppWithScopedParameters.APP_NAME,ProgramType.SPARK,WorkflowAppWithScopedParameters.ONE_SPARK);
  List<RunRecord> oneSparkHistoryRuns=getProgramRuns(spark1ProgramId,ProgramRunStatus.COMPLETED);
  Id.Program spark2ProgramId=Id.Program.from(TEST_NAMESPACE2,WorkflowAppWithScopedParameters.APP_NAME,ProgramType.SPARK,WorkflowAppWithScopedParameters.ANOTHER_SPARK);
  List<RunRecord> anotherSparkHistoryRuns=getProgramRuns(spark2ProgramId,ProgramRunStatus.COMPLETED);
  Assert.assertEquals(1,workflowHistoryRuns.size());
  Assert.assertEquals(1,oneMRHistoryRuns.size());
  Assert.assertEquals(1,anotherMRHistoryRuns.size());
  Assert.assertEquals(1,oneSparkHistoryRuns.size());
  Assert.assertEquals(1,anotherSparkHistoryRuns.size());
  Map<String,String> workflowRunRecordProperties=workflowHistoryRuns.get(0).getProperties();
  Map<String,String> oneMRRunRecordProperties=oneMRHistoryRuns.get(0).getProperties();
  Map<String,String> anotherMRRunRecordProperties=anotherMRHistoryRuns.get(0).getProperties();
  Map<String,String> oneSparkRunRecordProperties=oneSparkHistoryRuns.get(0).getProperties();
  Map<String,String> anotherSparkRunRecordProperties=anotherSparkHistoryRuns.get(0).getProperties();
  Assert.assertNotNull(oneMRRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),oneMRRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertNotNull(anotherMRRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),anotherMRRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertNotNull(oneSparkRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),oneSparkRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertNotNull(anotherSparkRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),anotherSparkRunRecordProperties.get(workflowRunIdProperty));
  Assert.assertEquals(workflowRunRecordProperties.get(WorkflowAppWithScopedParameters.ONE_MR),oneMRHistoryRuns.get(0).getPid());
  Assert.assertEquals(workflowRunRecordProperties.get(WorkflowAppWithScopedParameters.ONE_SPARK),oneSparkHistoryRuns.get(0).getPid());
  Assert.assertEquals(workflowRunRecordProperties.get(WorkflowAppWithScopedParameters.ANOTHER_MR),anotherMRHistoryRuns.get(0).getPid());
  Assert.assertEquals(workflowRunRecordProperties.get(WorkflowAppWithScopedParameters.ANOTHER_SPARK),anotherSparkHistoryRuns.get(0).getPid());
  Map<String,WorkflowNodeStateDetail> nodeStates=getWorkflowNodeStates(programId,workflowHistoryRuns.get(0).getPid());
  Assert.assertNotNull(nodeStates);
  Assert.assertEquals(5,nodeStates.size());
  WorkflowNodeStateDetail mrNodeState=nodeStates.get(WorkflowAppWithScopedParameters.ONE_MR);
  Assert.assertNotNull(mrNodeState);
  Assert.assertEquals(WorkflowAppWithScopedParameters.ONE_MR,mrNodeState.getNodeId());
  Assert.assertEquals(oneMRHistoryRuns.get(0).getPid(),mrNodeState.getRunId());
  mrNodeState=nodeStates.get(WorkflowAppWithScopedParameters.ANOTHER_MR);
  Assert.assertNotNull(mrNodeState);
  Assert.assertEquals(WorkflowAppWithScopedParameters.ANOTHER_MR,mrNodeState.getNodeId());
  Assert.assertEquals(anotherMRHistoryRuns.get(0).getPid(),mrNodeState.getRunId());
  WorkflowNodeStateDetail sparkNodeState=nodeStates.get(WorkflowAppWithScopedParameters.ONE_SPARK);
  Assert.assertNotNull(sparkNodeState);
  Assert.assertEquals(WorkflowAppWithScopedParameters.ONE_SPARK,sparkNodeState.getNodeId());
  Assert.assertEquals(oneSparkHistoryRuns.get(0).getPid(),sparkNodeState.getRunId());
  sparkNodeState=nodeStates.get(WorkflowAppWithScopedParameters.ANOTHER_SPARK);
  Assert.assertNotNull(sparkNodeState);
  Assert.assertEquals(WorkflowAppWithScopedParameters.ANOTHER_SPARK,sparkNodeState.getNodeId());
  Assert.assertEquals(anotherSparkHistoryRuns.get(0).getPid(),sparkNodeState.getRunId());
  WorkflowNodeStateDetail oneActionNodeState=nodeStates.get(WorkflowAppWithScopedParameters.ONE_ACTION);
  Assert.assertNotNull(oneActionNodeState);
  Assert.assertEquals(WorkflowAppWithScopedParameters.ONE_ACTION,oneActionNodeState.getNodeId());
}"
4797,"private String getRunIdOfRunningProgram(final Id.Program programId) throws Exception {
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return getProgramRuns(programId,""String_Node_Str"").size();
    }
  }
,5,TimeUnit.SECONDS);
  List<RunRecord> historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertEquals(1,historyRuns.size());
  RunRecord record=historyRuns.get(0);
  return record.getPid();
}","private String getRunIdOfRunningProgram(final Id.Program programId) throws Exception {
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return getProgramRuns(programId,ProgramRunStatus.RUNNING).size();
    }
  }
,5,TimeUnit.SECONDS);
  List<RunRecord> historyRuns=getProgramRuns(programId,ProgramRunStatus.RUNNING);
  Assert.assertEquals(1,historyRuns.size());
  RunRecord record=historyRuns.get(0);
  return record.getPid();
}"
4798,"@Test public void testStreamSizeSchedules() throws Exception {
  String appName=""String_Node_Str"";
  String sampleSchedule1=""String_Node_Str"";
  String sampleSchedule2=""String_Node_Str"";
  String workflowName=""String_Node_Str"";
  String streamName=""String_Node_Str"";
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,appName,ProgramType.WORKFLOW,workflowName);
  StringBuilder longStringBuilder=new StringBuilder();
  for (int i=0; i < 10000; i++) {
    longStringBuilder.append(""String_Node_Str"");
  }
  String longString=longStringBuilder.toString();
  HttpResponse response=deploy(AppWithStreamSizeSchedule.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,sampleSchedule1));
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,sampleSchedule2));
  List<ScheduleDetail> schedules=getSchedules(TEST_NAMESPACE2,appName,workflowName);
  Assert.assertEquals(2,schedules.size());
  String scheduleName1=schedules.get(0).getName();
  String scheduleName2=schedules.get(1).getName();
  Assert.assertNotNull(scheduleName1);
  Assert.assertFalse(scheduleName1.isEmpty());
  response=doPut(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName),""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=doGet(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName));
  String json=EntityUtils.toString(response.getEntity());
  StreamProperties properties=new Gson().fromJson(json,StreamProperties.class);
  Assert.assertEquals(1,properties.getNotificationThresholdMB().intValue());
  for (int i=0; i < 12; ++i) {
    response=doPost(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName),longString);
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  }
  verifyProgramRuns(programId,""String_Node_Str"");
  assertSchedule(programId,scheduleName1,true,30,TimeUnit.SECONDS);
  assertSchedule(programId,scheduleName2,true,30,TimeUnit.SECONDS);
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName1));
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName2));
  assertSchedule(programId,scheduleName1,false,30,TimeUnit.SECONDS);
  assertSchedule(programId,scheduleName2,false,30,TimeUnit.SECONDS);
  int workflowRuns=getProgramRuns(programId,""String_Node_Str"").size();
  Assert.assertEquals(1,workflowRuns);
  for (int i=0; i < 12; ++i) {
    response=doPost(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName),longString);
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  }
  TimeUnit.SECONDS.sleep(5);
  int workflowRunsAfterSuspend=getProgramRuns(programId,""String_Node_Str"").size();
  Assert.assertEquals(workflowRuns,workflowRunsAfterSuspend);
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,scheduleName1));
  assertSchedule(programId,scheduleName1,true,30,TimeUnit.SECONDS);
  assertRunHistory(programId,""String_Node_Str"",1 + workflowRunsAfterSuspend,60,TimeUnit.SECONDS);
  try {
    assertSchedule(programId,""String_Node_Str"",true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName1));
  assertSchedule(programId,scheduleName1,false,30,TimeUnit.SECONDS);
  try {
    assertSchedule(Id.Program.from(TEST_NAMESPACE1,appName,ProgramType.WORKFLOW,workflowName),scheduleName1,true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  Assert.assertEquals(404,suspendSchedule(TEST_NAMESPACE1,appName,scheduleName1));
  Assert.assertEquals(404,resumeSchedule(TEST_NAMESPACE1,appName,scheduleName1));
  TimeUnit.SECONDS.sleep(2);
}","@Test public void testStreamSizeSchedules() throws Exception {
  String appName=""String_Node_Str"";
  String sampleSchedule1=""String_Node_Str"";
  String sampleSchedule2=""String_Node_Str"";
  String workflowName=""String_Node_Str"";
  String streamName=""String_Node_Str"";
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,appName,ProgramType.WORKFLOW,workflowName);
  StringBuilder longStringBuilder=new StringBuilder();
  for (int i=0; i < 10000; i++) {
    longStringBuilder.append(""String_Node_Str"");
  }
  String longString=longStringBuilder.toString();
  HttpResponse response=deploy(AppWithStreamSizeSchedule.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,sampleSchedule1));
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,sampleSchedule2));
  List<ScheduleDetail> schedules=getSchedules(TEST_NAMESPACE2,appName,workflowName);
  Assert.assertEquals(2,schedules.size());
  String scheduleName1=schedules.get(0).getName();
  String scheduleName2=schedules.get(1).getName();
  Assert.assertNotNull(scheduleName1);
  Assert.assertFalse(scheduleName1.isEmpty());
  response=doPut(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName),""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=doGet(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName));
  String json=EntityUtils.toString(response.getEntity());
  StreamProperties properties=new Gson().fromJson(json,StreamProperties.class);
  Assert.assertEquals(1,properties.getNotificationThresholdMB().intValue());
  for (int i=0; i < 12; ++i) {
    response=doPost(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName),longString);
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  }
  verifyProgramRuns(programId,ProgramRunStatus.COMPLETED);
  assertSchedule(programId,scheduleName1,true,30,TimeUnit.SECONDS);
  assertSchedule(programId,scheduleName2,true,30,TimeUnit.SECONDS);
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName1));
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName2));
  assertSchedule(programId,scheduleName1,false,30,TimeUnit.SECONDS);
  assertSchedule(programId,scheduleName2,false,30,TimeUnit.SECONDS);
  int workflowRuns=getProgramRuns(programId,ProgramRunStatus.COMPLETED).size();
  Assert.assertEquals(1,workflowRuns);
  for (int i=0; i < 12; ++i) {
    response=doPost(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName),longString);
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  }
  TimeUnit.SECONDS.sleep(5);
  int workflowRunsAfterSuspend=getProgramRuns(programId,ProgramRunStatus.COMPLETED).size();
  Assert.assertEquals(workflowRuns,workflowRunsAfterSuspend);
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,scheduleName1));
  assertSchedule(programId,scheduleName1,true,30,TimeUnit.SECONDS);
  assertRunHistory(programId,ProgramRunStatus.COMPLETED,1 + workflowRunsAfterSuspend,60,TimeUnit.SECONDS);
  try {
    assertSchedule(programId,""String_Node_Str"",true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName1));
  assertSchedule(programId,scheduleName1,false,30,TimeUnit.SECONDS);
  try {
    assertSchedule(Id.Program.from(TEST_NAMESPACE1,appName,ProgramType.WORKFLOW,workflowName),scheduleName1,true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  Assert.assertEquals(404,suspendSchedule(TEST_NAMESPACE1,appName,scheduleName1));
  Assert.assertEquals(404,resumeSchedule(TEST_NAMESPACE1,appName,scheduleName1));
  TimeUnit.SECONDS.sleep(2);
}"
4799,"@Ignore @Test public void testWorkflowForkFailure() throws Exception {
  Assert.assertEquals(200,deploy(WorkflowFailureInForkApp.class).getStatusLine().getStatusCode());
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,WorkflowFailureInForkApp.NAME);
  Id.Workflow workflowId=Id.Workflow.from(appId,WorkflowFailureInForkApp.WorkflowWithFailureInFork.NAME);
  Id.Program firstMRId=Id.Program.from(appId,ProgramType.MAPREDUCE,WorkflowFailureInForkApp.FIRST_MAPREDUCE_NAME);
  Id.Program secondMRId=Id.Program.from(appId,ProgramType.MAPREDUCE,WorkflowFailureInForkApp.SECOND_MAPREDUCE_NAME);
  String outputPath=new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath();
  File fileToSync=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File fileToWait=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  startProgram(workflowId,ImmutableMap.of(""String_Node_Str"",createInput(""String_Node_Str""),""String_Node_Str"",outputPath,""String_Node_Str"",fileToSync.getAbsolutePath(),""String_Node_Str"",fileToWait.getAbsolutePath(),""String_Node_Str"" + WorkflowFailureInForkApp.SECOND_MAPREDUCE_NAME + ""String_Node_Str"",""String_Node_Str""));
  waitState(workflowId,ProgramStatus.RUNNING.name());
  waitState(workflowId,ProgramStatus.STOPPED.name());
  verifyProgramRuns(workflowId,""String_Node_Str"");
  List<RunRecord> mapReduceProgramRuns=getProgramRuns(firstMRId,ProgramRunStatus.KILLED.name());
  Assert.assertEquals(1,mapReduceProgramRuns.size());
  mapReduceProgramRuns=getProgramRuns(secondMRId,ProgramRunStatus.FAILED.name());
  Assert.assertEquals(1,mapReduceProgramRuns.size());
}","@Ignore @Test public void testWorkflowForkFailure() throws Exception {
  Assert.assertEquals(200,deploy(WorkflowFailureInForkApp.class).getStatusLine().getStatusCode());
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,WorkflowFailureInForkApp.NAME);
  Id.Workflow workflowId=Id.Workflow.from(appId,WorkflowFailureInForkApp.WorkflowWithFailureInFork.NAME);
  Id.Program firstMRId=Id.Program.from(appId,ProgramType.MAPREDUCE,WorkflowFailureInForkApp.FIRST_MAPREDUCE_NAME);
  Id.Program secondMRId=Id.Program.from(appId,ProgramType.MAPREDUCE,WorkflowFailureInForkApp.SECOND_MAPREDUCE_NAME);
  String outputPath=new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath();
  File fileToSync=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File fileToWait=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  startProgram(workflowId,ImmutableMap.of(""String_Node_Str"",createInput(""String_Node_Str""),""String_Node_Str"",outputPath,""String_Node_Str"",fileToSync.getAbsolutePath(),""String_Node_Str"",fileToWait.getAbsolutePath(),""String_Node_Str"" + WorkflowFailureInForkApp.SECOND_MAPREDUCE_NAME + ""String_Node_Str"",""String_Node_Str""));
  waitState(workflowId,ProgramStatus.RUNNING.name());
  waitState(workflowId,ProgramStatus.STOPPED.name());
  verifyProgramRuns(workflowId,ProgramRunStatus.FAILED);
  List<RunRecord> mapReduceProgramRuns=getProgramRuns(firstMRId,ProgramRunStatus.KILLED);
  Assert.assertEquals(1,mapReduceProgramRuns.size());
  mapReduceProgramRuns=getProgramRuns(secondMRId,ProgramRunStatus.FAILED);
  Assert.assertEquals(1,mapReduceProgramRuns.size());
}"
4800,"@Override public Integer call() throws Exception {
  return getProgramRuns(workflowId,ProgramRunStatus.COMPLETED.name()).size();
}","@Override public Integer call() throws Exception {
  return getProgramRuns(workflowId,ProgramRunStatus.COMPLETED).size();
}"
4801,"private void verifyMultipleConcurrentRuns(Id.Program workflowId) throws Exception {
  verifyProgramRuns(workflowId,ProgramRunStatus.RUNNING.name(),1);
  List<RunRecord> historyRuns=getProgramRuns(workflowId,""String_Node_Str"");
  Assert.assertEquals(2,historyRuns.size());
  HttpResponse response=getWorkflowCurrentStatus(workflowId,historyRuns.get(0).getPid());
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  String json=EntityUtils.toString(response.getEntity());
  List<WorkflowActionNode> nodes=GSON.fromJson(json,LIST_WORKFLOWACTIONNODE_TYPE);
  Assert.assertEquals(1,nodes.size());
  Assert.assertEquals(ConcurrentWorkflowApp.SimpleAction.class.getSimpleName(),nodes.get(0).getProgram().getProgramName());
  response=getWorkflowCurrentStatus(workflowId,historyRuns.get(1).getPid());
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  json=EntityUtils.toString(response.getEntity());
  nodes=GSON.fromJson(json,LIST_WORKFLOWACTIONNODE_TYPE);
  Assert.assertEquals(1,nodes.size());
  Assert.assertEquals(ConcurrentWorkflowApp.SimpleAction.class.getSimpleName(),nodes.get(0).getProgram().getProgramName());
}","private void verifyMultipleConcurrentRuns(Id.Program workflowId) throws Exception {
  verifyProgramRuns(workflowId,ProgramRunStatus.RUNNING,1);
  List<RunRecord> historyRuns=getProgramRuns(workflowId,ProgramRunStatus.RUNNING);
  Assert.assertEquals(2,historyRuns.size());
  HttpResponse response=getWorkflowCurrentStatus(workflowId,historyRuns.get(0).getPid());
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  String json=EntityUtils.toString(response.getEntity());
  List<WorkflowActionNode> nodes=GSON.fromJson(json,LIST_WORKFLOWACTIONNODE_TYPE);
  Assert.assertEquals(1,nodes.size());
  Assert.assertEquals(ConcurrentWorkflowApp.SimpleAction.class.getSimpleName(),nodes.get(0).getProgram().getProgramName());
  response=getWorkflowCurrentStatus(workflowId,historyRuns.get(1).getPid());
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  json=EntityUtils.toString(response.getEntity());
  nodes=GSON.fromJson(json,LIST_WORKFLOWACTIONNODE_TYPE);
  Assert.assertEquals(1,nodes.size());
  Assert.assertEquals(ConcurrentWorkflowApp.SimpleAction.class.getSimpleName(),nodes.get(0).getProgram().getProgramName());
}"
4802,"@Test public void testWorkflowPauseResume() throws Exception {
  String pauseResumeWorkflowApp=""String_Node_Str"";
  String pauseResumeWorkflow=""String_Node_Str"";
  File firstSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File firstSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File forkedSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File forkedSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File anotherForkedSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File anotherForkedSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File lastSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File lastSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  HttpResponse response=deploy(PauseResumeWorklowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,pauseResumeWorkflowApp,ProgramType.WORKFLOW,pauseResumeWorkflow);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  runtimeArguments.put(""String_Node_Str"",firstSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",firstSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",forkedSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",forkedSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",anotherForkedSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",anotherForkedSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",lastSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",lastSimpleActionDoneFile.getAbsolutePath());
  setAndTestRuntimeArgs(programId,runtimeArguments);
  startProgram(programId,200);
  waitState(programId,ProgramStatus.RUNNING.name());
  String runId=getRunIdOfRunningProgram(programId);
  while (!firstSimpleActionFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,1);
  suspendWorkflow(programId,runId,200);
  waitState(programId,ProgramStatus.STOPPED.name());
  verifyProgramRuns(programId,""String_Node_Str"");
  suspendWorkflow(programId,runId,409);
  Assert.assertTrue(firstSimpleActionDoneFile.createNewFile());
  verifyRunningProgramCount(programId,runId,0);
  verifyProgramRuns(programId,""String_Node_Str"");
  resumeWorkflow(programId,runId,200);
  waitState(programId,ProgramStatus.RUNNING.name());
  verifyProgramRuns(programId,""String_Node_Str"");
  resumeWorkflow(programId,runId,409);
  while (!(forkedSimpleActionFile.exists() && anotherForkedSimpleActionFile.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,2);
  suspendWorkflow(programId,runId,200);
  waitState(programId,ProgramStatus.STOPPED.name());
  verifyProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(forkedSimpleActionDoneFile.createNewFile());
  Assert.assertTrue(anotherForkedSimpleActionDoneFile.createNewFile());
  verifyRunningProgramCount(programId,runId,0);
  verifyProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(!lastSimpleActionFile.exists());
  resumeWorkflow(programId,runId,200);
  waitState(programId,ProgramStatus.RUNNING.name());
  while (!lastSimpleActionFile.exists()) {
    TimeUnit.SECONDS.sleep(1);
  }
  verifyRunningProgramCount(programId,runId,1);
  Assert.assertTrue(lastSimpleActionDoneFile.createNewFile());
  verifyProgramRuns(programId,""String_Node_Str"");
  waitState(programId,ProgramStatus.STOPPED.name());
  suspendWorkflow(programId,runId,404);
  resumeWorkflow(programId,runId,404);
}","@Test public void testWorkflowPauseResume() throws Exception {
  String pauseResumeWorkflowApp=""String_Node_Str"";
  String pauseResumeWorkflow=""String_Node_Str"";
  File firstSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File firstSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File forkedSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File forkedSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File anotherForkedSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File anotherForkedSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File lastSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File lastSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  HttpResponse response=deploy(PauseResumeWorklowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,pauseResumeWorkflowApp,ProgramType.WORKFLOW,pauseResumeWorkflow);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  runtimeArguments.put(""String_Node_Str"",firstSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",firstSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",forkedSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",forkedSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",anotherForkedSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",anotherForkedSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",lastSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",lastSimpleActionDoneFile.getAbsolutePath());
  setAndTestRuntimeArgs(programId,runtimeArguments);
  startProgram(programId,200);
  waitState(programId,ProgramStatus.RUNNING.name());
  String runId=getRunIdOfRunningProgram(programId);
  while (!firstSimpleActionFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,1);
  suspendWorkflow(programId,runId,200);
  waitState(programId,ProgramStatus.STOPPED.name());
  verifyProgramRuns(programId,ProgramRunStatus.SUSPENDED);
  suspendWorkflow(programId,runId,409);
  Assert.assertTrue(firstSimpleActionDoneFile.createNewFile());
  verifyRunningProgramCount(programId,runId,0);
  verifyProgramRuns(programId,ProgramRunStatus.SUSPENDED);
  resumeWorkflow(programId,runId,200);
  waitState(programId,ProgramStatus.RUNNING.name());
  resumeWorkflow(programId,runId,409);
  while (!(forkedSimpleActionFile.exists() && anotherForkedSimpleActionFile.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,2);
  suspendWorkflow(programId,runId,200);
  waitState(programId,ProgramStatus.STOPPED.name());
  verifyProgramRuns(programId,ProgramRunStatus.SUSPENDED);
  Assert.assertTrue(forkedSimpleActionDoneFile.createNewFile());
  Assert.assertTrue(anotherForkedSimpleActionDoneFile.createNewFile());
  verifyRunningProgramCount(programId,runId,0);
  verifyProgramRuns(programId,ProgramRunStatus.SUSPENDED);
  Assert.assertTrue(!lastSimpleActionFile.exists());
  resumeWorkflow(programId,runId,200);
  waitState(programId,ProgramStatus.RUNNING.name());
  while (!lastSimpleActionFile.exists()) {
    TimeUnit.SECONDS.sleep(1);
  }
  verifyRunningProgramCount(programId,runId,1);
  Assert.assertTrue(lastSimpleActionDoneFile.createNewFile());
  verifyProgramRuns(programId,ProgramRunStatus.COMPLETED);
  suspendWorkflow(programId,runId,404);
  resumeWorkflow(programId,runId,404);
}"
4803,"private Module getCombinedModules(final ProgramId programId,String txClientId){
  return Modules.combine(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MetricsClientRuntimeModule().getDistributedModules(),new MessagingClientModule(),new LocationRuntimeModule().getDistributedModules(),new LoggingModules().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new DataFabricModules(txClientId).getDistributedModules(),new DataSetsModules().getDistributedModules(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new AuditModule().getDistributedModules(),new NamespaceClientRuntimeModule().getDistributedModules(),new AuthorizationEnforcementModule().getDistributedModules(),new SecureStoreModules().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(QueueReaderFactory.class).in(Scopes.SINGLETON);
      install(new DataFabricFacadeModule());
      bind(ProgramStateWriter.class).to(DirectStoreProgramStateWriter.class);
      bind(RuntimeStore.class).to(RemoteRuntimeStore.class);
      install(createStreamFactoryModule());
      bind(UGIProvider.class).to(CurrentUGIProvider.class).in(Scopes.SINGLETON);
      bind(PrivilegesManager.class).to(RemotePrivilegesManager.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
      bind(ProgramId.class).toInstance(programId);
      bind(ExploreClient.class).to(ProgramDiscoveryExploreClient.class).in(Scopes.SINGLETON);
    }
  }
);
}","private Module getCombinedModules(final ProgramId programId,String txClientId){
  return Modules.combine(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MetricsClientRuntimeModule().getDistributedModules(),new MessagingClientModule(),new LocationRuntimeModule().getDistributedModules(),new LoggingModules().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new DataFabricModules(txClientId).getDistributedModules(),new DataSetsModules().getDistributedModules(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new AuditModule().getDistributedModules(),new NamespaceClientRuntimeModule().getDistributedModules(),new AuthorizationEnforcementModule().getDistributedModules(),new SecureStoreModules().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(QueueReaderFactory.class).in(Scopes.SINGLETON);
      install(new DataFabricFacadeModule());
      bind(ProgramStateWriter.class).to(MessagingProgramStateWriter.class);
      bind(RuntimeStore.class).to(RemoteRuntimeStore.class);
      install(createStreamFactoryModule());
      bind(UGIProvider.class).to(CurrentUGIProvider.class).in(Scopes.SINGLETON);
      bind(PrivilegesManager.class).to(RemotePrivilegesManager.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
      bind(ProgramId.class).toInstance(programId);
      bind(ExploreClient.class).to(ProgramDiscoveryExploreClient.class).in(Scopes.SINGLETON);
    }
  }
);
}"
4804,"@Override protected void configure(){
  bind(ProgramStateWriter.class).to(DirectStoreProgramStateWriter.class);
  MapBinder<ProgramType,ProgramRunner> defaultProgramRunnerBinder=MapBinder.newMapBinder(binder(),ProgramType.class,ProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.FLOW).to(DistributedFlowProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.MAPREDUCE).to(DistributedMapReduceProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.WORKFLOW).to(DistributedWorkflowProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.WEBAPP).to(DistributedWebappProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.SERVICE).to(DistributedServiceProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.WORKER).to(DistributedWorkerProgramRunner.class);
  bind(ProgramRuntimeProvider.Mode.class).toInstance(ProgramRuntimeProvider.Mode.DISTRIBUTED);
  bind(ProgramRunnerFactory.class).to(DefaultProgramRunnerFactory.class).in(Scopes.SINGLETON);
  expose(ProgramRunnerFactory.class);
  bind(ProgramRuntimeService.class).to(DistributedProgramRuntimeService.class).in(Scopes.SINGLETON);
  expose(ProgramRuntimeService.class);
}","@Override protected void configure(){
  bind(ProgramStateWriter.class).to(MessagingProgramStateWriter.class);
  MapBinder<ProgramType,ProgramRunner> defaultProgramRunnerBinder=MapBinder.newMapBinder(binder(),ProgramType.class,ProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.FLOW).to(DistributedFlowProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.MAPREDUCE).to(DistributedMapReduceProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.WORKFLOW).to(DistributedWorkflowProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.WEBAPP).to(DistributedWebappProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.SERVICE).to(DistributedServiceProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.WORKER).to(DistributedWorkerProgramRunner.class);
  bind(ProgramRuntimeProvider.Mode.class).toInstance(ProgramRuntimeProvider.Mode.DISTRIBUTED);
  bind(ProgramRunnerFactory.class).to(DefaultProgramRunnerFactory.class).in(Scopes.SINGLETON);
  expose(ProgramRunnerFactory.class);
  bind(ProgramRuntimeService.class).to(DistributedProgramRuntimeService.class).in(Scopes.SINGLETON);
  expose(ProgramRuntimeService.class);
}"
4805,"/** 
 * Configures a   {@link com.google.inject.Binder} via the exposed methods.
 */
@Override protected void configure(){
  bind(ServiceAnnouncer.class).to(DiscoveryServiceAnnouncer.class);
  bind(QueueReaderFactory.class).in(Scopes.SINGLETON);
  bind(ProgramStateWriter.class).to(DirectStoreProgramStateWriter.class);
  MapBinder<ProgramType,ProgramRunner> runnerFactoryBinder=MapBinder.newMapBinder(binder(),ProgramType.class,ProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramType.FLOW).to(InMemoryFlowProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramType.MAPREDUCE).to(MapReduceProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramType.WORKFLOW).to(WorkflowProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramType.WEBAPP).to(WebappProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramType.WORKER).to(InMemoryWorkerRunner.class);
  runnerFactoryBinder.addBinding(ProgramType.SERVICE).to(InMemoryServiceProgramRunner.class);
  bind(FlowletProgramRunner.class);
  bind(ServiceProgramRunner.class);
  bind(WorkerProgramRunner.class);
  bind(ProgramRuntimeProvider.Mode.class).toInstance(ProgramRuntimeProvider.Mode.LOCAL);
  bind(ProgramRunnerFactory.class).to(DefaultProgramRunnerFactory.class).in(Scopes.SINGLETON);
  expose(ProgramRunnerFactory.class);
  bind(ProgramRuntimeService.class).to(InMemoryProgramRuntimeService.class).in(Scopes.SINGLETON);
  expose(ProgramRuntimeService.class);
  install(new DataFabricFacadeModule());
  install(new FactoryModuleBuilder().implement(JarHttpHandler.class,IntactJarHttpHandler.class).build(WebappHttpHandlerFactory.class));
  install(new FactoryModuleBuilder().implement(StreamWriter.class,streamWriterClass).build(StreamWriterFactory.class));
}","/** 
 * Configures a   {@link com.google.inject.Binder} via the exposed methods.
 */
@Override protected void configure(){
  bind(ServiceAnnouncer.class).to(DiscoveryServiceAnnouncer.class);
  bind(QueueReaderFactory.class).in(Scopes.SINGLETON);
  bind(ProgramStateWriter.class).to(MessagingProgramStateWriter.class);
  MapBinder<ProgramType,ProgramRunner> runnerFactoryBinder=MapBinder.newMapBinder(binder(),ProgramType.class,ProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramType.FLOW).to(InMemoryFlowProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramType.MAPREDUCE).to(MapReduceProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramType.WORKFLOW).to(WorkflowProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramType.WEBAPP).to(WebappProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramType.WORKER).to(InMemoryWorkerRunner.class);
  runnerFactoryBinder.addBinding(ProgramType.SERVICE).to(InMemoryServiceProgramRunner.class);
  bind(FlowletProgramRunner.class);
  bind(ServiceProgramRunner.class);
  bind(WorkerProgramRunner.class);
  bind(ProgramRuntimeProvider.Mode.class).toInstance(ProgramRuntimeProvider.Mode.LOCAL);
  bind(ProgramRunnerFactory.class).to(DefaultProgramRunnerFactory.class).in(Scopes.SINGLETON);
  expose(ProgramRunnerFactory.class);
  bind(ProgramRuntimeService.class).to(InMemoryProgramRuntimeService.class).in(Scopes.SINGLETON);
  expose(ProgramRuntimeService.class);
  install(new DataFabricFacadeModule());
  install(new FactoryModuleBuilder().implement(JarHttpHandler.class,IntactJarHttpHandler.class).build(WebappHttpHandlerFactory.class));
  install(new FactoryModuleBuilder().implement(StreamWriter.class,streamWriterClass).build(StreamWriterFactory.class));
}"
4806,"void State(ProgramRunStatus runStatus){
  this.runStatus=runStatus;
  this.programStatus=ProgramRunStatus.STARTING == runStatus || ProgramRunStatus.RUNNING == runStatus ? ProgramStatus.RUNNING : ProgramStatus.STOPPED;
}","void State(ProgramRunStatus runStatus){
  this.runStatus=runStatus;
}"
4807,"@Override public void initialize(EventHandlerContext context){
  super.initialize(context);
  this.twillRunId=context.getRunId();
  this.programRunId=GSON.fromJson(context.getSpecification().getConfigs().get(""String_Node_Str""),ProgramRunId.class);
  File cConfFile=new File(""String_Node_Str"" + CDAP_CONF_FILE_NAME);
  File hConfFile=new File(""String_Node_Str"" + HADOOP_CONF_FILE_NAME);
  if (cConfFile.exists() && hConfFile.exists()) {
    CConfiguration cConf=CConfiguration.create();
    cConf.clear();
    Configuration hConf=new Configuration();
    hConf.clear();
    try {
      cConf.addResource(cConfFile.toURI().toURL());
      hConf.addResource(hConfFile.toURI().toURL());
      Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new KafkaClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new AbstractModule(){
        @Override protected void configure(){
          bind(RuntimeStore.class).to(RemoteRuntimeStore.class);
          bind(ProgramStateWriter.class).to(DirectStoreProgramStateWriter.class);
        }
      }
);
      zkClientService=injector.getInstance(ZKClientService.class);
      startServices();
      this.programStateWriter=injector.getInstance(ProgramStateWriter.class);
    }
 catch (    Exception e) {
      throw Throwables.propagate(e);
    }
  }
 else {
    LOG.warn(""String_Node_Str"",CDAP_CONF_FILE_NAME,HADOOP_CONF_FILE_NAME);
    this.programStateWriter=new NoOpProgramStateWriter();
  }
}","@Override public void initialize(EventHandlerContext context){
  super.initialize(context);
  this.twillRunId=context.getRunId();
  this.programRunId=GSON.fromJson(context.getSpecification().getConfigs().get(""String_Node_Str""),ProgramRunId.class);
  File cConfFile=new File(""String_Node_Str"" + CDAP_CONF_FILE_NAME);
  File hConfFile=new File(""String_Node_Str"" + HADOOP_CONF_FILE_NAME);
  if (cConfFile.exists() && hConfFile.exists()) {
    CConfiguration cConf=CConfiguration.create();
    cConf.clear();
    Configuration hConf=new Configuration();
    hConf.clear();
    try {
      cConf.addResource(cConfFile.toURI().toURL());
      hConf.addResource(hConfFile.toURI().toURL());
      Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new KafkaClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new MessagingClientModule(),new AbstractModule(){
        @Override protected void configure(){
          bind(ProgramStateWriter.class).to(MessagingProgramStateWriter.class);
        }
      }
);
      zkClientService=injector.getInstance(ZKClientService.class);
      startServices();
      this.programStateWriter=injector.getInstance(ProgramStateWriter.class);
    }
 catch (    Exception e) {
      throw Throwables.propagate(e);
    }
  }
 else {
    LOG.warn(""String_Node_Str"",CDAP_CONF_FILE_NAME,HADOOP_CONF_FILE_NAME);
    this.programStateWriter=new NoOpProgramStateWriter();
  }
}"
4808,"@Override protected void configure(){
  bind(RuntimeStore.class).to(RemoteRuntimeStore.class);
  bind(ProgramStateWriter.class).to(DirectStoreProgramStateWriter.class);
}","@Override protected void configure(){
  bind(ProgramStateWriter.class).to(MessagingProgramStateWriter.class);
}"
4809,"/** 
 * Split this dag into multiple dags. Each subdag will contain at most a single reduce node.
 * @return list of subdags.
 */
public List<Dag> split(){
  List<Dag> dags=new ArrayList<>();
  Set<String> remainingNodes=new HashSet<>();
  remainingNodes.addAll(nodes);
  Set<String> possibleNewSources=Sets.union(sources,connectors.keySet());
  Set<String> possibleNewSinks=Sets.union(sinks,connectors.keySet());
  for (  String reduceNode : reduceNodes) {
    Dag subdag=subsetAround(reduceNode,possibleNewSources,possibleNewSinks);
    remainingNodes.removeAll(subdag.getNodes());
    dags.add(subdag);
  }
  Set<String> remainingSources=Sets.intersection(remainingNodes,possibleNewSources);
  Set<String> processedNodes=new HashSet<>();
  if (!remainingSources.isEmpty()) {
    Map<String,Set<String>> nodesAccessibleBySources=new HashMap<>();
    for (    String remainingSource : remainingSources) {
      Dag remainingNodesDag=subsetFrom(remainingSource,possibleNewSinks);
      nodesAccessibleBySources.put(remainingSource,remainingNodesDag.getNodes());
    }
    for (    String remainingSource : remainingSources) {
      if (processedNodes.contains(remainingSource)) {
        continue;
      }
      Set<String> remainingAccessibleNodes=nodesAccessibleBySources.get(remainingSource);
      Set<String> islandNodes=new HashSet<>();
      islandNodes.addAll(remainingAccessibleNodes);
      for (      String otherSource : remainingSources) {
        if (remainingSource.equals(otherSource)) {
          continue;
        }
        Set<String> otherAccessibleNodes=nodesAccessibleBySources.get(otherSource);
        if (!Sets.intersection(remainingAccessibleNodes,otherAccessibleNodes).isEmpty()) {
          islandNodes.addAll(otherAccessibleNodes);
        }
      }
      dags.add(createSubDag(islandNodes));
      processedNodes.addAll(islandNodes);
    }
  }
  return dags;
}","/** 
 * Split this dag into multiple dags. Each subdag will contain at most a single reduce node.
 * @return list of subdags.
 */
public List<Dag> split(){
  List<Dag> dags=new ArrayList<>();
  Set<String> remainingNodes=new HashSet<>(nodes);
  Set<String> possibleNewSources=Sets.union(sources,connectors.keySet());
  Set<String> possibleNewSinks=Sets.union(sinks,connectors.keySet());
  for (  String reduceNode : reduceNodes) {
    Dag subdag=subsetAround(reduceNode,possibleNewSources,possibleNewSinks);
    remainingNodes.removeAll(subdag.getNodes());
    dags.add(subdag);
  }
  Set<String> remainingSources=new TreeSet<>(Sets.intersection(remainingNodes,possibleNewSources));
  Set<String> processedNodes=new HashSet<>();
  Map<String,Set<String>> nodesAccessibleBySources=new HashMap<>();
  for (  String remainingSource : remainingSources) {
    Dag remainingNodesDag=subsetFrom(remainingSource,possibleNewSinks);
    nodesAccessibleBySources.put(remainingSource,remainingNodesDag.getNodes());
  }
  for (  String remainingSource : remainingSources) {
    if (!processedNodes.add(remainingSource)) {
      continue;
    }
    Set<String> subdag=new HashSet<>(nodesAccessibleBySources.get(remainingSource));
    Set<String> otherSources=Sets.difference(remainingSources,processedNodes);
    boolean nodesAdded;
    do {
      nodesAdded=false;
      for (      String otherSource : otherSources) {
        Set<String> otherAccessibleNodes=nodesAccessibleBySources.get(otherSource);
        if (!Sets.intersection(subdag,otherAccessibleNodes).isEmpty()) {
          if (subdag.addAll(otherAccessibleNodes)) {
            nodesAdded=true;
          }
        }
      }
    }
 while (nodesAdded);
    dags.add(createSubDag(subdag));
    processedNodes.addAll(subdag);
  }
  return dags;
}"
4810,"@Override public void store(StageSpec stageSpec,SparkSink<T> sink) throws Exception {
  Compat.foreachRDD(stream,new StreamingSparkSinkFunction<T>(sec,stageSpec));
}","@Override public void store(StageSpec stageSpec,SparkSink<T> sink) throws Exception {
  Compat.foreachRDD(stream.cache(),new StreamingSparkSinkFunction<T>(sec,stageSpec));
}"
4811,"@Override public Void call(JavaRDD<T> data,Time batchTime) throws Exception {
  final long logicalStartTime=batchTime.milliseconds();
  MacroEvaluator evaluator=new DefaultMacroEvaluator(sec.getWorkflowToken(),sec.getRuntimeArguments(),logicalStartTime,sec.getSecureStore(),sec.getNamespace());
  PluginContext pluginContext=new SparkPipelinePluginContext(sec.getPluginContext(),sec.getMetrics(),stageSpec.isStageLoggingEnabled(),stageSpec.isProcessTimingEnabled());
  final SparkBatchSinkFactory sinkFactory=new SparkBatchSinkFactory();
  final String stageName=stageSpec.getName();
  final BatchSink<Object,Object,Object> batchSink=pluginContext.newPluginInstance(stageName,evaluator);
  final PipelineRuntime pipelineRuntime=new SparkPipelineRuntime(sec,logicalStartTime);
  boolean isPrepared=false;
  boolean isDone=false;
  try {
    sec.execute(new TxRunnable(){
      @Override public void run(      DatasetContext datasetContext) throws Exception {
        SparkBatchSinkContext sinkContext=new SparkBatchSinkContext(sinkFactory,sec,datasetContext,pipelineRuntime,stageSpec);
        batchSink.prepareRun(sinkContext);
      }
    }
);
    isPrepared=true;
    PluginFunctionContext pluginFunctionContext=new PluginFunctionContext(stageSpec,sec,pipelineRuntime.getArguments().asMap(),batchTime.milliseconds());
    PairFlatMapFunc<T,Object,Object> sinkFunction=new BatchSinkFunction<T,Object,Object>(pluginFunctionContext);
    sinkFactory.writeFromRDD(data.flatMapToPair(Compat.convert(sinkFunction)),sec,stageName,Object.class,Object.class);
    isDone=true;
    sec.execute(new TxRunnable(){
      @Override public void run(      DatasetContext datasetContext) throws Exception {
        SparkBatchSinkContext sinkContext=new SparkBatchSinkContext(sinkFactory,sec,datasetContext,pipelineRuntime,stageSpec);
        batchSink.onRunFinish(true,sinkContext);
      }
    }
);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",stageName,logicalStartTime,e);
  }
 finally {
    if (isPrepared && !isDone) {
      sec.execute(new TxRunnable(){
        @Override public void run(        DatasetContext datasetContext) throws Exception {
          SparkBatchSinkContext sinkContext=new SparkBatchSinkContext(sinkFactory,sec,datasetContext,pipelineRuntime,stageSpec);
          batchSink.onRunFinish(false,sinkContext);
        }
      }
);
    }
  }
  return null;
}","@Override public Void call(JavaRDD<T> data,Time batchTime) throws Exception {
  if (data.isEmpty()) {
    return null;
  }
  final long logicalStartTime=batchTime.milliseconds();
  MacroEvaluator evaluator=new DefaultMacroEvaluator(sec.getWorkflowToken(),sec.getRuntimeArguments(),logicalStartTime,sec.getSecureStore(),sec.getNamespace());
  PluginContext pluginContext=new SparkPipelinePluginContext(sec.getPluginContext(),sec.getMetrics(),stageSpec.isStageLoggingEnabled(),stageSpec.isProcessTimingEnabled());
  final SparkBatchSinkFactory sinkFactory=new SparkBatchSinkFactory();
  final String stageName=stageSpec.getName();
  final BatchSink<Object,Object,Object> batchSink=pluginContext.newPluginInstance(stageName,evaluator);
  final PipelineRuntime pipelineRuntime=new SparkPipelineRuntime(sec,logicalStartTime);
  boolean isPrepared=false;
  boolean isDone=false;
  try {
    sec.execute(new TxRunnable(){
      @Override public void run(      DatasetContext datasetContext) throws Exception {
        SparkBatchSinkContext sinkContext=new SparkBatchSinkContext(sinkFactory,sec,datasetContext,pipelineRuntime,stageSpec);
        batchSink.prepareRun(sinkContext);
      }
    }
);
    isPrepared=true;
    PluginFunctionContext pluginFunctionContext=new PluginFunctionContext(stageSpec,sec,pipelineRuntime.getArguments().asMap(),batchTime.milliseconds());
    PairFlatMapFunc<T,Object,Object> sinkFunction=new BatchSinkFunction<T,Object,Object>(pluginFunctionContext);
    sinkFactory.writeFromRDD(data.flatMapToPair(Compat.convert(sinkFunction)),sec,stageName,Object.class,Object.class);
    isDone=true;
    sec.execute(new TxRunnable(){
      @Override public void run(      DatasetContext datasetContext) throws Exception {
        SparkBatchSinkContext sinkContext=new SparkBatchSinkContext(sinkFactory,sec,datasetContext,pipelineRuntime,stageSpec);
        batchSink.onRunFinish(true,sinkContext);
      }
    }
);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",stageName,logicalStartTime,e);
  }
 finally {
    if (isPrepared && !isDone) {
      sec.execute(new TxRunnable(){
        @Override public void run(        DatasetContext datasetContext) throws Exception {
          SparkBatchSinkContext sinkContext=new SparkBatchSinkContext(sinkFactory,sec,datasetContext,pipelineRuntime,stageSpec);
          batchSink.onRunFinish(false,sinkContext);
        }
      }
);
    }
  }
  return null;
}"
4812,"@Override public Void call(JavaRDD<T> data,Time batchTime) throws Exception {
  final long logicalStartTime=batchTime.milliseconds();
  MacroEvaluator evaluator=new DefaultMacroEvaluator(sec.getWorkflowToken(),sec.getRuntimeArguments(),logicalStartTime,sec.getSecureStore(),sec.getNamespace());
  final PluginContext pluginContext=new SparkPipelinePluginContext(sec.getPluginContext(),sec.getMetrics(),stageSpec.isStageLoggingEnabled(),stageSpec.isProcessTimingEnabled());
  final PipelineRuntime pipelineRuntime=new SparkPipelineRuntime(sec,batchTime.milliseconds());
  final String stageName=stageSpec.getName();
  final SparkSink<T> sparkSink=pluginContext.newPluginInstance(stageName,evaluator);
  boolean isPrepared=false;
  boolean isDone=false;
  try {
    sec.execute(new TxRunnable(){
      @Override public void run(      DatasetContext datasetContext) throws Exception {
        SparkPluginContext context=new BasicSparkPluginContext(null,pipelineRuntime,stageSpec,datasetContext,sec.getAdmin());
        sparkSink.prepareRun(context);
      }
    }
);
    isPrepared=true;
    final SparkExecutionPluginContext sparkExecutionPluginContext=new SparkStreamingExecutionContext(sec,JavaSparkContext.fromSparkContext(data.rdd().context()),logicalStartTime,stageSpec);
    final JavaRDD<T> countedRDD=data.map(new CountingFunction<T>(stageName,sec.getMetrics(),""String_Node_Str"",null)).cache();
    sec.execute(new TxRunnable(){
      @Override public void run(      DatasetContext context) throws Exception {
        sparkSink.run(sparkExecutionPluginContext,countedRDD);
      }
    }
);
    isDone=true;
    sec.execute(new TxRunnable(){
      @Override public void run(      DatasetContext datasetContext) throws Exception {
        SparkPluginContext context=new BasicSparkPluginContext(null,pipelineRuntime,stageSpec,datasetContext,sec.getAdmin());
        sparkSink.onRunFinish(true,context);
      }
    }
);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",stageName,logicalStartTime,e);
  }
 finally {
    if (isPrepared && !isDone) {
      sec.execute(new TxRunnable(){
        @Override public void run(        DatasetContext datasetContext) throws Exception {
          SparkPluginContext context=new BasicSparkPluginContext(null,pipelineRuntime,stageSpec,datasetContext,sec.getAdmin());
          sparkSink.onRunFinish(false,context);
        }
      }
);
    }
  }
  return null;
}","@Override public Void call(JavaRDD<T> data,Time batchTime) throws Exception {
  if (data.isEmpty()) {
    return null;
  }
  final long logicalStartTime=batchTime.milliseconds();
  MacroEvaluator evaluator=new DefaultMacroEvaluator(sec.getWorkflowToken(),sec.getRuntimeArguments(),logicalStartTime,sec.getSecureStore(),sec.getNamespace());
  final PluginContext pluginContext=new SparkPipelinePluginContext(sec.getPluginContext(),sec.getMetrics(),stageSpec.isStageLoggingEnabled(),stageSpec.isProcessTimingEnabled());
  final PipelineRuntime pipelineRuntime=new SparkPipelineRuntime(sec,batchTime.milliseconds());
  final String stageName=stageSpec.getName();
  final SparkSink<T> sparkSink=pluginContext.newPluginInstance(stageName,evaluator);
  boolean isPrepared=false;
  boolean isDone=false;
  try {
    sec.execute(new TxRunnable(){
      @Override public void run(      DatasetContext datasetContext) throws Exception {
        SparkPluginContext context=new BasicSparkPluginContext(null,pipelineRuntime,stageSpec,datasetContext,sec.getAdmin());
        sparkSink.prepareRun(context);
      }
    }
);
    isPrepared=true;
    final SparkExecutionPluginContext sparkExecutionPluginContext=new SparkStreamingExecutionContext(sec,JavaSparkContext.fromSparkContext(data.rdd().context()),logicalStartTime,stageSpec);
    final JavaRDD<T> countedRDD=data.map(new CountingFunction<T>(stageName,sec.getMetrics(),""String_Node_Str"",null)).cache();
    sec.execute(new TxRunnable(){
      @Override public void run(      DatasetContext context) throws Exception {
        sparkSink.run(sparkExecutionPluginContext,countedRDD);
      }
    }
);
    isDone=true;
    sec.execute(new TxRunnable(){
      @Override public void run(      DatasetContext datasetContext) throws Exception {
        SparkPluginContext context=new BasicSparkPluginContext(null,pipelineRuntime,stageSpec,datasetContext,sec.getAdmin());
        sparkSink.onRunFinish(true,context);
      }
    }
);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",stageName,logicalStartTime,e);
  }
 finally {
    if (isPrepared && !isDone) {
      sec.execute(new TxRunnable(){
        @Override public void run(        DatasetContext datasetContext) throws Exception {
          SparkPluginContext context=new BasicSparkPluginContext(null,pipelineRuntime,stageSpec,datasetContext,sec.getAdmin());
          sparkSink.onRunFinish(false,context);
        }
      }
);
    }
  }
  return null;
}"
4813,"private void flattenFrom(String node){
  Set<String> outputs=outgoingConnections.get(node);
  if (outputs.isEmpty()) {
    return;
  }
  if (outputs.size() == 1) {
    flattenFrom(outputs.iterator().next());
    return;
  }
  Multimap<String,String> branchEndpointOutputs=HashMultimap.create();
  Set<String> branchEndpoints=new HashSet<>();
  for (  String output : outputs) {
    String branchEndpoint=findBranchEnd(output);
    branchEndpoints.add(branchEndpoint);
    branchEndpointOutputs.putAll(branchEndpoint,outgoingConnections.get(branchEndpoint));
  }
  Set<String> endpointOutputs=new HashSet<>(branchEndpointOutputs.values());
  if (endpointOutputs.size() == 1) {
    flattenFrom(endpointOutputs.iterator().next());
    return;
  }
  String newJoinNode=generateJoinNodeName(branchEndpoints);
  addNode(newJoinNode,branchEndpoints,endpointOutputs);
  for (  Map.Entry<String,String> endpointEntry : branchEndpointOutputs.entries()) {
    removeConnection(endpointEntry.getKey(),endpointEntry.getValue());
  }
  trim();
  flattenFrom(newJoinNode);
}","private void flattenFrom(String node){
  Set<String> outputs=outgoingConnections.get(node);
  if (outputs.isEmpty()) {
    return;
  }
  if (outputs.size() == 1) {
    flattenFrom(outputs.iterator().next());
    return;
  }
  Map<String,Set<String>> branchEndpointOutputs=new HashMap<>();
  Set<String> branchEndpoints=new HashSet<>();
  for (  String output : outputs) {
    String branchEndpoint=findBranchEnd(output);
    branchEndpoints.add(branchEndpoint);
    branchEndpointOutputs.put(branchEndpoint,new HashSet<>(outgoingConnections.get(branchEndpoint)));
  }
  Set<String> endpointOutputs=new HashSet<>();
  boolean endpointsContainSink=false;
  for (  Set<String> branchEndpointOutput : branchEndpointOutputs.values()) {
    endpointOutputs.addAll(branchEndpointOutput);
    if (branchEndpointOutput.isEmpty()) {
      endpointsContainSink=true;
    }
  }
  if (endpointOutputs.size() == 1 && !endpointsContainSink) {
    flattenFrom(endpointOutputs.iterator().next());
    return;
  }
  String newJoinNode=generateJoinNodeName(branchEndpoints);
  addNode(newJoinNode,branchEndpoints,endpointOutputs);
  for (  Map.Entry<String,Set<String>> endpointEntry : branchEndpointOutputs.entrySet()) {
    String branchEndpoint=endpointEntry.getKey();
    for (    String branchEndpointOutput : endpointEntry.getValue()) {
      removeConnection(branchEndpoint,branchEndpointOutput);
    }
  }
  trim();
  flattenFrom(newJoinNode);
}"
4814,"@Test public void testNoOpTrim(){
  ControlDag cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(0,cdag.trim());
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(0,cdag.trim());
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(0,cdag.trim());
}","@Test public void testNoOpTrim(){
  ControlDag cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(0,cdag.trim());
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(0,cdag.trim());
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(0,cdag.trim());
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(0,cdag.trim());
}"
4815,"@Test public void testFlatten(){
  ControlDag cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  cdag.flatten();
  ControlDag expected=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(expected,cdag);
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  cdag.flatten();
  expected=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(expected,cdag);
}","@Test public void testFlatten(){
  ControlDag cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  cdag.flatten();
  ControlDag expected=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(expected,cdag);
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  cdag.flatten();
  expected=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(expected,cdag);
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  cdag.flatten();
  expected=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(expected,cdag);
}"
4816,"/** 
 * Split this dag into multiple dags. Each subdag will contain at most a single reduce node.
 * @return list of subdags.
 */
public List<Dag> split(){
  List<Dag> dags=new ArrayList<>();
  Set<String> remainingNodes=new HashSet<>();
  remainingNodes.addAll(nodes);
  Set<String> possibleNewSources=Sets.union(sources,connectors.keySet());
  Set<String> possibleNewSinks=Sets.union(sinks,connectors.keySet());
  for (  String reduceNode : reduceNodes) {
    Dag subdag=subsetAround(reduceNode,possibleNewSources,possibleNewSinks);
    remainingNodes.removeAll(subdag.getNodes());
    dags.add(subdag);
  }
  Set<String> remainingSources=Sets.intersection(remainingNodes,possibleNewSources);
  Set<String> processedNodes=new HashSet<>();
  if (!remainingSources.isEmpty()) {
    Map<String,Set<String>> nodesAccessibleBySources=new HashMap<>();
    for (    String remainingSource : remainingSources) {
      Dag remainingNodesDag=subsetFrom(remainingSource,possibleNewSinks);
      nodesAccessibleBySources.put(remainingSource,remainingNodesDag.getNodes());
    }
    for (    String remainingSource : remainingSources) {
      if (processedNodes.contains(remainingSource)) {
        continue;
      }
      Set<String> remainingAccessibleNodes=nodesAccessibleBySources.get(remainingSource);
      Set<String> islandNodes=new HashSet<>();
      islandNodes.addAll(remainingAccessibleNodes);
      for (      String otherSource : remainingSources) {
        if (remainingSource.equals(otherSource)) {
          continue;
        }
        Set<String> otherAccessibleNodes=nodesAccessibleBySources.get(otherSource);
        if (!Sets.intersection(remainingAccessibleNodes,otherAccessibleNodes).isEmpty()) {
          islandNodes.addAll(otherAccessibleNodes);
        }
      }
      dags.add(createSubDag(islandNodes));
      processedNodes.addAll(islandNodes);
    }
  }
  return dags;
}","/** 
 * Split this dag into multiple dags. Each subdag will contain at most a single reduce node.
 * @return list of subdags.
 */
public List<Dag> split(){
  List<Dag> dags=new ArrayList<>();
  Set<String> remainingNodes=new HashSet<>(nodes);
  Set<String> possibleNewSources=Sets.union(sources,connectors.keySet());
  Set<String> possibleNewSinks=Sets.union(sinks,connectors.keySet());
  for (  String reduceNode : reduceNodes) {
    Dag subdag=subsetAround(reduceNode,possibleNewSources,possibleNewSinks);
    remainingNodes.removeAll(subdag.getNodes());
    dags.add(subdag);
  }
  Set<String> remainingSources=new TreeSet<>(Sets.intersection(remainingNodes,possibleNewSources));
  Set<String> processedNodes=new HashSet<>();
  Map<String,Set<String>> nodesAccessibleBySources=new HashMap<>();
  for (  String remainingSource : remainingSources) {
    Dag remainingNodesDag=subsetFrom(remainingSource,possibleNewSinks);
    nodesAccessibleBySources.put(remainingSource,remainingNodesDag.getNodes());
  }
  for (  String remainingSource : remainingSources) {
    if (!processedNodes.add(remainingSource)) {
      continue;
    }
    Set<String> subdag=new HashSet<>(nodesAccessibleBySources.get(remainingSource));
    Set<String> otherSources=Sets.difference(remainingSources,processedNodes);
    boolean nodesAdded;
    do {
      nodesAdded=false;
      for (      String otherSource : otherSources) {
        Set<String> otherAccessibleNodes=nodesAccessibleBySources.get(otherSource);
        if (!Sets.intersection(subdag,otherAccessibleNodes).isEmpty()) {
          if (subdag.addAll(otherAccessibleNodes)) {
            nodesAdded=true;
          }
        }
      }
    }
 while (nodesAdded);
    dags.add(createSubDag(subdag));
    processedNodes.addAll(subdag);
  }
  return dags;
}"
4817,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] programIdParts=arguments.get(elementType.getArgumentName().toString()).split(""String_Node_Str"");
  String appId=programIdParts[0];
  long currentTime=System.currentTimeMillis();
  long startTime=getTimestamp(arguments.getOptional(ArgumentName.START_TIME.toString(),""String_Node_Str""),currentTime);
  long endTime=getTimestamp(arguments.getOptional(ArgumentName.END_TIME.toString(),""String_Node_Str""),currentTime);
  int limit=arguments.getIntOptional(ArgumentName.LIMIT.toString(),Integer.MAX_VALUE);
  List<RunRecord> records;
  if (elementType.getProgramType() != null) {
    if (programIdParts.length < 2) {
      throw new CommandInputError(this);
    }
    String programName=programIdParts[1];
    ProgramId programId=cliConfig.getCurrentNamespace().app(appId).program(elementType.getProgramType(),programName);
    if (arguments.hasArgument(ArgumentName.RUN_STATUS.toString())) {
      records=programClient.getProgramRuns(programId,arguments.get(ArgumentName.RUN_STATUS.toString()),startTime,endTime,limit);
    }
 else {
      records=programClient.getAllProgramRuns(programId,startTime,endTime,limit);
    }
  }
 else {
    throw new IllegalArgumentException(""String_Node_Str"" + elementType);
  }
  Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(records,new RowMaker<RunRecord>(){
    @Override public List<?> makeRow(    RunRecord object){
      return Lists.newArrayList(object.getPid(),object.getStatus(),object.getRunTs(),object.getStatus().name().equals(""String_Node_Str"") ? ""String_Node_Str"" : object.getStopTs());
    }
  }
).build();
  cliConfig.getTableRenderer().render(cliConfig,output,table);
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] programIdParts=arguments.get(elementType.getArgumentName().toString()).split(""String_Node_Str"");
  String appId=programIdParts[0];
  long currentTime=System.currentTimeMillis();
  long startTime=getTimestamp(arguments.getOptional(ArgumentName.START_TIME.toString(),""String_Node_Str""),currentTime);
  long endTime=getTimestamp(arguments.getOptional(ArgumentName.END_TIME.toString(),""String_Node_Str""),currentTime);
  int limit=arguments.getIntOptional(ArgumentName.LIMIT.toString(),Integer.MAX_VALUE);
  List<RunRecord> records;
  if (elementType.getProgramType() != null) {
    if (programIdParts.length < 2) {
      throw new CommandInputError(this);
    }
    String programName=programIdParts[1];
    ProgramId programId=cliConfig.getCurrentNamespace().app(appId).program(elementType.getProgramType(),programName);
    if (arguments.hasArgument(ArgumentName.RUN_STATUS.toString())) {
      records=programClient.getProgramRuns(programId,arguments.get(ArgumentName.RUN_STATUS.toString()),startTime,endTime,limit);
    }
 else {
      records=programClient.getAllProgramRuns(programId,startTime,endTime,limit);
    }
  }
 else {
    throw new IllegalArgumentException(""String_Node_Str"" + elementType);
  }
  Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(records,new RowMaker<RunRecord>(){
    @Override public List<?> makeRow(    RunRecord object){
      return Lists.newArrayList(object.getPid(),object.getStatus(),object.getStartTs(),object.getRunTs() == null ? ""String_Node_Str"" : object.getRunTs(),object.getStopTs() == null ? ""String_Node_Str"" : object.getStopTs());
    }
  }
).build();
  cliConfig.getTableRenderer().render(cliConfig,output,table);
}"
4818,"@Override public List<?> makeRow(RunRecord object){
  return Lists.newArrayList(object.getPid(),object.getStatus(),object.getRunTs(),object.getStatus().name().equals(""String_Node_Str"") ? ""String_Node_Str"" : object.getStopTs());
}","@Override public List<?> makeRow(RunRecord object){
  return Lists.newArrayList(object.getPid(),object.getStatus(),object.getStartTs(),object.getRunTs() == null ? ""String_Node_Str"" : object.getRunTs(),object.getStopTs() == null ? ""String_Node_Str"" : object.getStopTs());
}"
4819,"private void flattenFrom(String node){
  Set<String> outputs=outgoingConnections.get(node);
  if (outputs.isEmpty()) {
    return;
  }
  if (outputs.size() == 1) {
    flattenFrom(outputs.iterator().next());
    return;
  }
  Multimap<String,String> branchEndpointOutputs=HashMultimap.create();
  Set<String> branchEndpoints=new HashSet<>();
  for (  String output : outputs) {
    String branchEndpoint=findBranchEnd(output);
    branchEndpoints.add(branchEndpoint);
    branchEndpointOutputs.putAll(branchEndpoint,outgoingConnections.get(branchEndpoint));
  }
  Set<String> endpointOutputs=new HashSet<>(branchEndpointOutputs.values());
  if (endpointOutputs.size() == 1) {
    flattenFrom(endpointOutputs.iterator().next());
    return;
  }
  String newJoinNode=generateJoinNodeName(branchEndpoints);
  addNode(newJoinNode,branchEndpoints,endpointOutputs);
  for (  Map.Entry<String,String> endpointEntry : branchEndpointOutputs.entries()) {
    removeConnection(endpointEntry.getKey(),endpointEntry.getValue());
  }
  trim();
  flattenFrom(newJoinNode);
}","private void flattenFrom(String node){
  Set<String> outputs=outgoingConnections.get(node);
  if (outputs.isEmpty()) {
    return;
  }
  if (outputs.size() == 1) {
    flattenFrom(outputs.iterator().next());
    return;
  }
  Map<String,Set<String>> branchEndpointOutputs=new HashMap<>();
  Set<String> branchEndpoints=new HashSet<>();
  for (  String output : outputs) {
    String branchEndpoint=findBranchEnd(output);
    branchEndpoints.add(branchEndpoint);
    branchEndpointOutputs.put(branchEndpoint,new HashSet<>(outgoingConnections.get(branchEndpoint)));
  }
  Set<String> endpointOutputs=new HashSet<>();
  boolean endpointsContainSink=false;
  for (  Set<String> branchEndpointOutput : branchEndpointOutputs.values()) {
    endpointOutputs.addAll(branchEndpointOutput);
    if (branchEndpointOutput.isEmpty()) {
      endpointsContainSink=true;
    }
  }
  if (endpointOutputs.size() == 1 && !endpointsContainSink) {
    flattenFrom(endpointOutputs.iterator().next());
    return;
  }
  String newJoinNode=generateJoinNodeName(branchEndpoints);
  addNode(newJoinNode,branchEndpoints,endpointOutputs);
  for (  Map.Entry<String,Set<String>> endpointEntry : branchEndpointOutputs.entrySet()) {
    String branchEndpoint=endpointEntry.getKey();
    for (    String branchEndpointOutput : endpointEntry.getValue()) {
      removeConnection(branchEndpoint,branchEndpointOutput);
    }
  }
  trim();
  flattenFrom(newJoinNode);
}"
4820,"@Test public void testNoOpTrim(){
  ControlDag cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(0,cdag.trim());
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(0,cdag.trim());
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(0,cdag.trim());
}","@Test public void testNoOpTrim(){
  ControlDag cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(0,cdag.trim());
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(0,cdag.trim());
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(0,cdag.trim());
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(0,cdag.trim());
}"
4821,"@Test public void testFlatten(){
  ControlDag cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  cdag.flatten();
  ControlDag expected=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(expected,cdag);
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  cdag.flatten();
  expected=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(expected,cdag);
}","@Test public void testFlatten(){
  ControlDag cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  cdag.flatten();
  ControlDag expected=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(expected,cdag);
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  cdag.flatten();
  expected=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(expected,cdag);
  cdag=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  cdag.flatten();
  expected=new ControlDag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(expected,cdag);
}"
4822,"public DataSetException(String message,Throwable cause){
  super(message,cause);
}","public DataSetException(Throwable cause){
  super(cause);
}"
4823,"/** 
 * Return a partition output for a specific partition key, in preparation for creating a new partition. Obtain the location to write from the PartitionOutput, then call the   {@link PartitionOutput#addPartition}to add the partition to this dataset.
 * @throws IllegalArgumentException if the partition key does not match the partitioning of the dataset
 */
PartitionOutput getPartitionOutput(PartitionKey key);","/** 
 * Return a partition output for a specific partition key, in preparation for creating a new partition. Obtain the location to write from the PartitionOutput, then call the   {@link PartitionOutput#addPartition}to add the partition to this dataset.
 * @throws PartitionAlreadyExistsException if the partition already exists
 * @throws IllegalArgumentException if the partition key does not match the partitioning of the dataset
 */
PartitionOutput getPartitionOutput(PartitionKey key);"
4824,"/** 
 * Add a partition for a given partition key, stored at a given path (relative to the file set's base path), with the given metadata.
 * @throws DataSetException if a partition for the same key already exists
 * @throws IllegalArgumentException if the partition key does not match the partitioning of the dataset
 */
void addPartition(PartitionKey key,String path,Map<String,String> metadata);","/** 
 * Add a partition for a given partition key, stored at a given path (relative to the file set's base path), with the given metadata.
 * @throws PartitionAlreadyExistsException if a partition for the same key already exists
 * @throws IllegalArgumentException if the partition key does not match the partitioning of the dataset
 */
void addPartition(PartitionKey key,String path,Map<String,String> metadata);"
4825,"/** 
 * Return a partition output for a specific time, rounded to the minute, in preparation for creating a new partition. Obtain the location to write from the PartitionOutput, then call the   {@link PartitionOutput#addPartition}to add the partition to this dataset.
 * @param time the partition time in milliseconds since the Epoch
 */
TimePartitionOutput getPartitionOutput(long time);","/** 
 * Return a partition output for a specific time, rounded to the minute, in preparation for creating a new partition. Obtain the location to write from the PartitionOutput, then call the   {@link PartitionOutput#addPartition}to add the partition to this dataset.
 * @param time the partition time in milliseconds since the Epoch
 * @throws PartitionAlreadyExistsException if the partition for the given time already exists
 */
TimePartitionOutput getPartitionOutput(long time);"
4826,"/** 
 * Add a partition for a given time, stored at a given path (relative to the file set's base path), with given metadata.
 * @param time the partition time in milliseconds since the Epoch
 */
void addPartition(long time,String path,Map<String,String> metadata);","/** 
 * Add a partition for a given time, stored at a given path (relative to the file set's base path), with given metadata.
 * @param time the partition time in milliseconds since the Epoch
 * @throws PartitionAlreadyExistsException if the partition for the given time already exists
 */
void addPartition(long time,String path,Map<String,String> metadata);"
4827,"/** 
 * Adds a set of new metadata entries for a particular partition Note that existing entries can not be updated.
 * @param time the partition time in milliseconds since the Epoch
 * @throws DataSetException in case an attempt is made to update existing entries.
 */
void addMetadata(long time,Map<String,String> metadata);","/** 
 * Adds a set of new metadata entries for a particular partition Note that existing entries can not be updated.
 * @param time the partition time in milliseconds since the Epoch
 * @throws DataSetException in case an attempt is made to update existing entries.
 * @throws PartitionNotFoundException when a partition for the given time is not found
 */
void addMetadata(long time,Map<String,String> metadata);"
4828,"@ReadOnly @Override public PartitionOutput getPartitionOutput(PartitionKey key){
  if (isExternal) {
    throw new UnsupportedOperationException(""String_Node_Str"" + spec.getName() + ""String_Node_Str"");
  }
  return new BasicPartitionOutput(this,getOutputPath(key),key);
}","@ReadOnly @Override public PartitionOutput getPartitionOutput(PartitionKey key){
  if (isExternal) {
    throw new UnsupportedOperationException(""String_Node_Str"" + spec.getName() + ""String_Node_Str"");
  }
  assertNotExists(key,true);
  return new BasicPartitionOutput(this,getOutputPath(key),key);
}"
4829,"@Override public Map<String,String> getOutputFormatConfiguration(){
  if (isExternal) {
    throw new UnsupportedOperationException(""String_Node_Str"" + spec.getName() + ""String_Node_Str"");
  }
  Map<String,String> outputArgs=new HashMap<>(files.getOutputFormatConfiguration());
  PartitionKey outputKey=PartitionedFileSetArguments.getOutputPartitionKey(runtimeArguments,getPartitioning());
  if (outputKey == null) {
    String dynamicPartitionerClassName=PartitionedFileSetArguments.getDynamicPartitioner(runtimeArguments);
    if (dynamicPartitionerClassName == null) {
      throw new DataSetException(""String_Node_Str"");
    }
    Map<String,String> outputMetadata=PartitionedFileSetArguments.getOutputPartitionMetadata(runtimeArguments);
    PartitionedFileSetArguments.setOutputPartitionMetadata(outputArgs,outputMetadata);
    PartitionedFileSetArguments.setDynamicPartitioner(outputArgs,dynamicPartitionerClassName);
    PartitionedFileSetArguments.setDynamicPartitionerConcurrency(outputArgs,PartitionedFileSetArguments.isDynamicPartitionerConcurrencyAllowed(runtimeArguments));
    outputArgs.put(Constants.Dataset.Partitioned.HCONF_ATTR_OUTPUT_FORMAT_CLASS_NAME,files.getOutputFormatClassName());
    outputArgs.put(Constants.Dataset.Partitioned.HCONF_ATTR_OUTPUT_DATASET,getName());
  }
  return ImmutableMap.copyOf(outputArgs);
}","@Override public Map<String,String> getOutputFormatConfiguration(){
  if (isExternal) {
    throw new UnsupportedOperationException(""String_Node_Str"" + spec.getName() + ""String_Node_Str"");
  }
  Map<String,String> outputArgs=new HashMap<>(files.getOutputFormatConfiguration());
  PartitionKey outputKey=PartitionedFileSetArguments.getOutputPartitionKey(runtimeArguments,getPartitioning());
  if (outputKey == null) {
    String dynamicPartitionerClassName=PartitionedFileSetArguments.getDynamicPartitioner(runtimeArguments);
    if (dynamicPartitionerClassName == null) {
      throw new DataSetException(""String_Node_Str"");
    }
    Map<String,String> outputMetadata=PartitionedFileSetArguments.getOutputPartitionMetadata(runtimeArguments);
    PartitionedFileSetArguments.setOutputPartitionMetadata(outputArgs,outputMetadata);
    PartitionedFileSetArguments.setDynamicPartitioner(outputArgs,dynamicPartitionerClassName);
    PartitionedFileSetArguments.setDynamicPartitionerConcurrency(outputArgs,PartitionedFileSetArguments.isDynamicPartitionerConcurrencyAllowed(runtimeArguments));
    outputArgs.put(Constants.Dataset.Partitioned.HCONF_ATTR_OUTPUT_FORMAT_CLASS_NAME,files.getOutputFormatClassName());
    outputArgs.put(Constants.Dataset.Partitioned.HCONF_ATTR_OUTPUT_DATASET,getName());
  }
 else {
    assertNotExists(outputKey,true);
  }
  return ImmutableMap.copyOf(outputArgs);
}"
4830,"@Override public TimePartitionOutput getPartitionOutput(long time){
  if (isExternal) {
    throw new UnsupportedOperationException(""String_Node_Str"" + spec.getName() + ""String_Node_Str"");
  }
  PartitionKey key=partitionKeyForTime(time);
  return new BasicTimePartitionOutput(this,getOutputPath(key),key);
}","@Override public TimePartitionOutput getPartitionOutput(long time){
  if (isExternal) {
    throw new UnsupportedOperationException(""String_Node_Str"" + spec.getName() + ""String_Node_Str"");
  }
  PartitionKey key=partitionKeyForTime(time);
  assertNotExists(key,true);
  return new BasicTimePartitionOutput(this,getOutputPath(key),key);
}"
4831,"/** 
 * Relays job-level and task-level information about a particular MapReduce program run.
 */
@GET @Path(""String_Node_Str"") public void getMapReduceInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String mapreduceId,@PathParam(""String_Node_Str"") String runId) throws IOException, NotFoundException {
  ProgramId programId=new ProgramId(namespaceId,appId,ProgramType.MAPREDUCE,mapreduceId);
  ProgramRunId run=programId.run(runId);
  ApplicationSpecification appSpec=store.getApplication(programId.getParent());
  if (appSpec == null) {
    throw new NotFoundException(programId.getApplication());
  }
  if (!appSpec.getMapReduce().containsKey(mapreduceId)) {
    throw new NotFoundException(programId);
  }
  RunRecordMeta runRecordMeta=store.getRun(programId,runId);
  if (runRecordMeta == null) {
    throw new NotFoundException(run);
  }
  MRJobInfo mrJobInfo=mrJobInfoFetcher.getMRJobInfo(run.toId());
  mrJobInfo.setState(runRecordMeta.getStatus().name());
  mrJobInfo.setStartTime(TimeUnit.SECONDS.toMillis(runRecordMeta.getRunTs()));
  Long stopTs=runRecordMeta.getStopTs();
  if (stopTs != null) {
    mrJobInfo.setStopTime(TimeUnit.SECONDS.toMillis(stopTs));
  }
  Gson gson=new GsonBuilder().serializeSpecialFloatingPointValues().create();
  responder.sendJson(HttpResponseStatus.OK,mrJobInfo,mrJobInfo.getClass(),gson);
}","/** 
 * Relays job-level and task-level information about a particular MapReduce program run.
 */
@GET @Path(""String_Node_Str"") public void getMapReduceInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String mapreduceId,@PathParam(""String_Node_Str"") String runId) throws IOException, NotFoundException {
  ProgramId programId=new ProgramId(namespaceId,appId,ProgramType.MAPREDUCE,mapreduceId);
  ProgramRunId run=programId.run(runId);
  ApplicationSpecification appSpec=store.getApplication(programId.getParent());
  if (appSpec == null) {
    throw new NotFoundException(programId.getApplication());
  }
  if (!appSpec.getMapReduce().containsKey(mapreduceId)) {
    throw new NotFoundException(programId);
  }
  RunRecordMeta runRecordMeta=store.getRun(programId,runId);
  if (runRecordMeta == null) {
    throw new NotFoundException(run);
  }
  MRJobInfo mrJobInfo=mrJobInfoFetcher.getMRJobInfo(run.toId());
  mrJobInfo.setState(runRecordMeta.getStatus().name());
  mrJobInfo.setStartTime(TimeUnit.SECONDS.toMillis(runRecordMeta.getStartTs()));
  Long stopTs=runRecordMeta.getStopTs();
  if (stopTs != null) {
    mrJobInfo.setStopTime(TimeUnit.SECONDS.toMillis(stopTs));
  }
  Gson gson=new GsonBuilder().serializeSpecialFloatingPointValues().create();
  responder.sendJson(HttpResponseStatus.OK,mrJobInfo,mrJobInfo.getClass(),gson);
}"
4832,"public void setStartTime(@Nullable Long startTime){
  this.startTime=startTime;
}","public void setStartTime(Long startTime){
  this.startTime=startTime;
}"
4833,"@Nullable public Long getStartTime(){
  return startTime;
}","public Long getStartTime(){
  return startTime;
}"
4834,"private void addProgram(String phaseName,WorkflowProgramAdder programAdder){
  PipelinePhase phase=plan.getPhase(phaseName);
  if (phase == null) {
    return;
  }
  String programName=""String_Node_Str"" + phaseNum;
  phaseNum++;
  for (  StageInfo connectorInfo : phase.getStagesOfType(Constants.CONNECTOR_TYPE)) {
    String connectorName=connectorInfo.getName();
    String datasetName=connectorDatasets.get(connectorName);
    if (datasetName == null) {
      datasetName=""String_Node_Str"" + connectorNum++;
      connectorDatasets.put(connectorName,datasetName);
      ConnectorSource connectorSource=new ConnectorSource(datasetName,null);
      connectorSource.configure(getConfigurer());
    }
  }
  Map<String,String> phaseConnectorDatasets=new HashMap<>();
  for (  StageInfo connectorStage : phase.getStagesOfType(Constants.CONNECTOR_TYPE)) {
    phaseConnectorDatasets.put(connectorStage.getName(),connectorDatasets.get(connectorStage.getName()));
  }
  BatchPhaseSpec batchPhaseSpec=new BatchPhaseSpec(programName,phase,spec.getResources(),spec.getDriverResources(),spec.getClientResources(),spec.isStageLoggingEnabled(),spec.isProcessTimingEnabled(),phaseConnectorDatasets,spec.getNumOfRecordsPreview(),spec.getProperties());
  Set<String> pluginTypes=batchPhaseSpec.getPhase().getPluginTypes();
  if (pluginTypes.contains(Action.PLUGIN_TYPE)) {
    programAdder.addAction(new PipelineAction(batchPhaseSpec));
  }
 else   if (pluginTypes.contains(Constants.SPARK_PROGRAM_PLUGIN_TYPE)) {
    String stageName=phase.getStagesOfType(Constants.SPARK_PROGRAM_PLUGIN_TYPE).iterator().next().getName();
    StageSpec stageSpec=stageSpecs.get(stageName);
    applicationConfigurer.addSpark(new ExternalSparkProgram(batchPhaseSpec,stageSpec));
    programAdder.addSpark(programName);
  }
 else   if (useSpark) {
    applicationConfigurer.addSpark(new ETLSpark(batchPhaseSpec));
    programAdder.addSpark(programName);
  }
 else {
    applicationConfigurer.addMapReduce(new ETLMapReduce(batchPhaseSpec));
    programAdder.addMapReduce(programName);
  }
}","private void addProgram(String phaseName,WorkflowProgramAdder programAdder){
  PipelinePhase phase=plan.getPhase(phaseName);
  if (phase == null) {
    return;
  }
  String programName=""String_Node_Str"" + phaseNum;
  phaseNum++;
  for (  StageInfo connectorInfo : phase.getStagesOfType(Constants.CONNECTOR_TYPE)) {
    String connectorName=connectorInfo.getName();
    String datasetName=connectorDatasets.get(connectorName);
    if (datasetName == null) {
      datasetName=""String_Node_Str"" + connectorNum++;
      connectorDatasets.put(connectorName,datasetName);
      ConnectorSource connectorSource=new ConnectorSource(datasetName,null);
      connectorSource.configure(getConfigurer());
    }
  }
  Map<String,String> phaseConnectorDatasets=new HashMap<>();
  for (  StageInfo connectorStage : phase.getStagesOfType(Constants.CONNECTOR_TYPE)) {
    phaseConnectorDatasets.put(connectorStage.getName(),connectorDatasets.get(connectorStage.getName()));
  }
  BatchPhaseSpec batchPhaseSpec=new BatchPhaseSpec(programName,phase,spec.getResources(),spec.getDriverResources(),spec.getClientResources(),spec.isStageLoggingEnabled(),spec.isProcessTimingEnabled(),phaseConnectorDatasets,spec.getNumOfRecordsPreview(),spec.getProperties());
  Set<String> pluginTypes=batchPhaseSpec.getPhase().getPluginTypes();
  if (pluginTypes.contains(Action.PLUGIN_TYPE)) {
    programAdder.addAction(new PipelineAction(batchPhaseSpec));
  }
 else   if (pluginTypes.contains(Constants.SPARK_PROGRAM_PLUGIN_TYPE)) {
    String stageName=phase.getStagesOfType(Constants.SPARK_PROGRAM_PLUGIN_TYPE).iterator().next().getName();
    StageSpec stageSpec=stageSpecs.get(stageName);
    applicationConfigurer.addSpark(new ExternalSparkProgram(batchPhaseSpec,stageSpec));
    programAdder.addSpark(programName);
  }
 else   if (useSpark) {
    applicationConfigurer.addSpark(new ETLSpark(batchPhaseSpec));
    programAdder.addSpark(programName);
  }
 else {
    applicationConfigurer.addMapReduce(new ETLMapReduce(batchPhaseSpec,new HashSet<>(connectorDatasets.values())));
    programAdder.addMapReduce(programName);
  }
}"
4835,"public ETLMapReduce(BatchPhaseSpec phaseSpec){
  this.phaseSpec=phaseSpec;
}","public ETLMapReduce(BatchPhaseSpec phaseSpec,Set<String> connectorDatasets){
  this.phaseSpec=phaseSpec;
  this.connectorDatasets=connectorDatasets;
}"
4836,"@Override public void configure(){
  setName(phaseSpec.getPhaseName());
  setDescription(""String_Node_Str"" + phaseSpec.getDescription());
  setMapperResources(phaseSpec.getResources());
  setReducerResources(phaseSpec.getResources());
  setDriverResources(phaseSpec.getDriverResources());
  Set<String> sources=phaseSpec.getPhase().getSources();
  if (sources.isEmpty()) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",phaseSpec.getPhaseName()));
  }
  if (phaseSpec.getPhase().getSinks().isEmpty()) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",phaseSpec.getPhaseName()));
  }
  Set<StageInfo> reducers=phaseSpec.getPhase().getStagesOfType(BatchAggregator.PLUGIN_TYPE,BatchJoiner.PLUGIN_TYPE);
  if (reducers.size() > 1) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",phaseSpec.getPhaseName(),Joiner.on(',').join(reducers)));
  }
 else   if (!reducers.isEmpty()) {
    String reducerName=reducers.iterator().next().getName();
    PipelinePhase mapperPipeline=phaseSpec.getPhase().subsetTo(ImmutableSet.of(reducerName));
    for (    StageInfo stageInfo : mapperPipeline) {
      if (stageInfo.getErrorDatasetName() != null) {
        throw new IllegalArgumentException(String.format(""String_Node_Str"",stageInfo.getName(),reducerName));
      }
    }
  }
  Map<String,String> properties=new HashMap<>();
  properties.put(Constants.PIPELINEID,GSON.toJson(phaseSpec));
  setProperties(properties);
}","@Override public void configure(){
  setName(phaseSpec.getPhaseName());
  setDescription(""String_Node_Str"" + phaseSpec.getDescription());
  setMapperResources(phaseSpec.getResources());
  setReducerResources(phaseSpec.getResources());
  setDriverResources(phaseSpec.getDriverResources());
  Set<String> sources=phaseSpec.getPhase().getSources();
  if (sources.isEmpty()) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",phaseSpec.getPhaseName()));
  }
  if (phaseSpec.getPhase().getSinks().isEmpty()) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",phaseSpec.getPhaseName()));
  }
  Set<StageInfo> reducers=phaseSpec.getPhase().getStagesOfType(BatchAggregator.PLUGIN_TYPE,BatchJoiner.PLUGIN_TYPE);
  if (reducers.size() > 1) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",phaseSpec.getPhaseName(),Joiner.on(',').join(reducers)));
  }
 else   if (!reducers.isEmpty()) {
    String reducerName=reducers.iterator().next().getName();
    PipelinePhase mapperPipeline=phaseSpec.getPhase().subsetTo(ImmutableSet.of(reducerName));
    for (    StageInfo stageInfo : mapperPipeline) {
      if (stageInfo.getErrorDatasetName() != null) {
        throw new IllegalArgumentException(String.format(""String_Node_Str"",stageInfo.getName(),reducerName));
      }
    }
  }
  Map<String,String> properties=new HashMap<>();
  properties.put(Constants.PIPELINEID,GSON.toJson(phaseSpec));
  properties.put(Constants.CONNECTOR_DATASETS,GSON.toJson(connectorDatasets));
  setProperties(properties);
}"
4837,"public MapReduceBatchContext(MapReduceContext context,Metrics metrics,StageInfo stageInfo){
  super(context,metrics,new DatasetContextLookupProvider(context),context.getLogicalStartTime(),context.getAdmin(),stageInfo,new BasicArguments(context));
  this.mrContext=context;
  this.caller=NoStageLoggingCaller.wrap(Caller.DEFAULT);
  this.outputNames=new HashSet<>();
  this.inputNames=new HashSet<>();
  this.isPreviewEnabled=context.getDataTracer(stageInfo.getName()).isEnabled();
}","public MapReduceBatchContext(MapReduceContext context,Metrics metrics,StageInfo stageInfo){
  this(context,metrics,stageInfo,new HashSet<String>());
}"
4838,"/** 
 * Get the output, if preview is enabled, return the output with a   {@link NullOutputFormatProvider}.
 */
private Output getOutput(Output output){
  if (isPreviewEnabled) {
    return Output.of(output.getName(),new NullOutputFormatProvider());
  }
  return output;
}","/** 
 * Get the output, if preview is enabled, return the output with a   {@link NullOutputFormatProvider}.
 */
private Output getOutput(Output output){
  if (isPreviewEnabled && !connectorDatasets.contains(output.getName())) {
    return Output.of(output.getName(),new NullOutputFormatProvider());
  }
  return output;
}"
4839,"public ETLMapReduce(BatchPhaseSpec phaseSpec){
  this.phaseSpec=phaseSpec;
}","public ETLMapReduce(BatchPhaseSpec phaseSpec,Set<String> connectorDatasets){
  this.phaseSpec=phaseSpec;
  this.connectorDatasets=connectorDatasets;
}"
4840,"@Override public void configure(){
  setName(phaseSpec.getPhaseName());
  setDescription(""String_Node_Str"" + phaseSpec.getDescription());
  setMapperResources(phaseSpec.getResources());
  setReducerResources(phaseSpec.getResources());
  setDriverResources(phaseSpec.getDriverResources());
  Set<String> sources=phaseSpec.getPhase().getSources();
  if (sources.isEmpty()) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",phaseSpec.getPhaseName()));
  }
  if (phaseSpec.getPhase().getSinks().isEmpty()) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",phaseSpec.getPhaseName()));
  }
  Set<StageSpec> reducers=phaseSpec.getPhase().getStagesOfType(BatchAggregator.PLUGIN_TYPE,BatchJoiner.PLUGIN_TYPE);
  if (reducers.size() > 1) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",phaseSpec.getPhaseName(),Joiner.on(',').join(reducers)));
  }
 else   if (!reducers.isEmpty()) {
    String reducerName=reducers.iterator().next().getName();
    PipelinePhase mapperPipeline=phaseSpec.getPhase().subsetTo(ImmutableSet.of(reducerName));
    for (    StageSpec stageInfo : mapperPipeline) {
      if (stageInfo.getErrorDatasetName() != null) {
        throw new IllegalArgumentException(String.format(""String_Node_Str"",stageInfo.getName(),reducerName));
      }
    }
  }
  Map<String,String> properties=new HashMap<>();
  properties.put(Constants.PIPELINEID,GSON.toJson(phaseSpec));
  setProperties(properties);
}","@Override public void configure(){
  setName(phaseSpec.getPhaseName());
  setDescription(""String_Node_Str"" + phaseSpec.getDescription());
  setMapperResources(phaseSpec.getResources());
  setReducerResources(phaseSpec.getResources());
  setDriverResources(phaseSpec.getDriverResources());
  Set<String> sources=phaseSpec.getPhase().getSources();
  if (sources.isEmpty()) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",phaseSpec.getPhaseName()));
  }
  if (phaseSpec.getPhase().getSinks().isEmpty()) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",phaseSpec.getPhaseName()));
  }
  Set<StageSpec> reducers=phaseSpec.getPhase().getStagesOfType(BatchAggregator.PLUGIN_TYPE,BatchJoiner.PLUGIN_TYPE);
  if (reducers.size() > 1) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",phaseSpec.getPhaseName(),Joiner.on(',').join(reducers)));
  }
 else   if (!reducers.isEmpty()) {
    String reducerName=reducers.iterator().next().getName();
    PipelinePhase mapperPipeline=phaseSpec.getPhase().subsetTo(ImmutableSet.of(reducerName));
    for (    StageSpec stageInfo : mapperPipeline) {
      if (stageInfo.getErrorDatasetName() != null) {
        throw new IllegalArgumentException(String.format(""String_Node_Str"",stageInfo.getName(),reducerName));
      }
    }
  }
  Map<String,String> properties=new HashMap<>();
  properties.put(Constants.PIPELINEID,GSON.toJson(phaseSpec));
  properties.put(Constants.CONNECTOR_DATASETS,GSON.toJson(connectorDatasets));
  setProperties(properties);
}"
4841,"public MapReduceBatchContext(MapReduceContext context,Metrics metrics,StageSpec stageSpec){
  super(context,metrics,new DatasetContextLookupProvider(context),context.getLogicalStartTime(),context.getAdmin(),stageSpec,new BasicArguments(context));
  this.mrContext=context;
  this.caller=NoStageLoggingCaller.wrap(Caller.DEFAULT);
  this.outputNames=new HashSet<>();
  this.inputNames=new HashSet<>();
  this.isPreviewEnabled=context.getDataTracer(stageSpec.getName()).isEnabled();
}","public MapReduceBatchContext(MapReduceContext context,Metrics metrics,StageSpec stageSpec){
  this(context,metrics,stageSpec,new HashSet<String>());
}"
4842,"/** 
 * Get the output, if preview is enabled, return the output with a   {@link NullOutputFormatProvider}.
 */
private Output getOutput(Output output){
  if (isPreviewEnabled) {
    return Output.of(output.getName(),new NullOutputFormatProvider());
  }
  return output;
}","/** 
 * Get the output, if preview is enabled, return the output with a   {@link NullOutputFormatProvider}.
 */
private Output getOutput(Output output){
  if (isPreviewEnabled && !connectorDatasets.contains(output.getName())) {
    return Output.of(output.getName(),new NullOutputFormatProvider());
  }
  return output;
}"
4843,"/** 
 * Returns the   {@link PartitionKey} of the input configured for this task.
 */
PartitionKey getInputPartitionKey();","/** 
 * Returns the   {@link PartitionKey} of the input configured for this task.In case of CombineFileInputFormat, this will be the PartitionKey currently being processed by the task.
 */
PartitionKey getInputPartitionKey();"
4844,"@Override public void configure(){
  setName(""String_Node_Str"");
  setDescription(""String_Node_Str"");
  createDataset(INPUT,""String_Node_Str"");
  createDataset(OUTPUT,""String_Node_Str"");
  createDataset(PARTITIONED,""String_Node_Str"",PartitionedFileSetProperties.builder().setPartitioning(Partitioning.builder().addStringField(""String_Node_Str"").addLongField(""String_Node_Str"").build()).setBasePath(""String_Node_Str"").setInputFormat(TextInputFormat.class).setOutputFormat(TextOutputFormat.class).setOutputProperty(TextOutputFormat.SEPERATOR,SEPARATOR).build());
  addMapReduce(new PartitionWriter());
  addMapReduce(new PartitionReader());
}","@Override public void configure(){
  setName(""String_Node_Str"");
  setDescription(""String_Node_Str"");
  createDataset(INPUT,""String_Node_Str"");
  createDataset(OUTPUT,""String_Node_Str"");
  Class<? extends InputFormat> inputFormatClass=getConfig().isUseCombineFileInputFormat() ? CombineTextInputFormat.class : TextInputFormat.class;
  createDataset(PARTITIONED,""String_Node_Str"",PartitionedFileSetProperties.builder().setPartitioning(Partitioning.builder().addStringField(""String_Node_Str"").addLongField(""String_Node_Str"").build()).setBasePath(""String_Node_Str"").setInputFormat(inputFormatClass).setOutputFormat(TextOutputFormat.class).setOutputProperty(TextOutputFormat.SEPERATOR,SEPARATOR).build());
  addMapReduce(new PartitionWriter());
  addMapReduce(new PartitionReader());
}"
4845,"@Override public void initialize(MapReduceTaskContext<byte[],Put> context) throws Exception {
  InputContext inputContext=context.getInputContext();
  Preconditions.checkArgument(PARTITIONED.equals(inputContext.getInputName()));
  Preconditions.checkArgument(inputContext instanceof PartitionedFileSetInputContext);
  PartitionedFileSetInputContext pfsInputcontext=(PartitionedFileSetInputContext)inputContext;
  Preconditions.checkNotNull(pfsInputcontext.getInputPartitionKey());
  Map<String,String> dsArguments=RuntimeArguments.extractScope(Scope.DATASET,PARTITIONED,context.getRuntimeArguments());
  PartitionFilter inputPartitionFilter=PartitionedFileSetArguments.getInputPartitionFilter(dsArguments);
  Preconditions.checkNotNull(inputPartitionFilter);
  Preconditions.checkArgument(inputPartitionFilter.match(pfsInputcontext.getInputPartitionKey()));
}","@Override public void initialize(MapReduceTaskContext<byte[],Put> context) throws Exception {
  InputContext inputContext=context.getInputContext();
  Preconditions.checkArgument(PARTITIONED.equals(inputContext.getInputName()));
  Preconditions.checkArgument(inputContext instanceof PartitionedFileSetInputContext);
  this.pfsInputcontext=(PartitionedFileSetInputContext)inputContext;
  Preconditions.checkNotNull(pfsInputcontext.getInputPartitionKey());
  Preconditions.checkArgument(pfsInputcontext.getInputPartitionKeys().contains(pfsInputcontext.getInputPartitionKey()));
  Map<String,String> dsArguments=RuntimeArguments.extractScope(Scope.DATASET,PARTITIONED,context.getRuntimeArguments());
  PartitionFilter inputPartitionFilter=PartitionedFileSetArguments.getInputPartitionFilter(dsArguments);
  Preconditions.checkNotNull(inputPartitionFilter);
  Preconditions.checkArgument(inputPartitionFilter.match(pfsInputcontext.getInputPartitionKey()));
}"
4846,"@Override public void map(LongWritable pos,Text text,Context context) throws IOException, InterruptedException {
  String line=text.toString();
  String[] fields=line.split(SEPARATOR);
  context.write(rowToWrite,new Put(rowToWrite,Bytes.toBytes(fields[0]),Bytes.toBytes(fields[1])));
}","@Override public void map(LongWritable pos,Text text,Context context) throws IOException, InterruptedException {
  String line=text.toString();
  String[] fields=line.split(SEPARATOR);
  context.write(rowToWrite,new Put(rowToWrite,Bytes.toBytes(fields[0]),Bytes.toBytes(fields[1])));
  context.write(rowToWrite,new Put(rowToWrite,Bytes.toBytes(fields[0] + ""String_Node_Str""),Bytes.toBytes(pfsInputcontext.getInputPartitionKey().toString())));
}"
4847,"@Test public void testPartitionedFileSetWithMR() throws Exception {
  final ApplicationWithPrograms app=deployApp(AppWithPartitionedFileSet.class);
  final Table table=datasetCache.getDataset(AppWithPartitionedFileSet.INPUT);
  Transactions.createTransactionExecutor(txExecutorFactory,(TransactionAware)table).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply(){
      table.put(Bytes.toBytes(""String_Node_Str""),AppWithPartitionedFileSet.ONLY_COLUMN,Bytes.toBytes(""String_Node_Str""));
    }
  }
);
  final PartitionKey keyX=PartitionKey.builder().addStringField(""String_Node_Str"",""String_Node_Str"").addLongField(""String_Node_Str"",150000L).build();
  Map<String,String> runtimeArguments=Maps.newHashMap();
  Map<String,String> outputArgs=Maps.newHashMap();
  PartitionedFileSetArguments.setOutputPartitionKey(outputArgs,keyX);
  runtimeArguments.putAll(RuntimeArguments.addScope(Scope.DATASET,PARTITIONED,outputArgs));
  Assert.assertTrue(runProgram(app,AppWithPartitionedFileSet.PartitionWriter.class,new BasicArguments(runtimeArguments)));
  final PartitionedFileSet dataset=datasetCache.getDataset(PARTITIONED);
  Transactions.createTransactionExecutor(txExecutorFactory,(TransactionAware)dataset).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply(){
      Partition partition=dataset.getPartition(keyX);
      Assert.assertNotNull(partition);
      String path=partition.getRelativePath();
      Assert.assertTrue(path.contains(""String_Node_Str""));
      Assert.assertTrue(path.contains(""String_Node_Str""));
    }
  }
);
  Transactions.createTransactionExecutor(txExecutorFactory,(TransactionAware)table).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply(){
      table.delete(Bytes.toBytes(""String_Node_Str""));
      table.put(Bytes.toBytes(""String_Node_Str""),AppWithPartitionedFileSet.ONLY_COLUMN,Bytes.toBytes(""String_Node_Str""));
    }
  }
);
  final PartitionKey keyY=PartitionKey.builder().addStringField(""String_Node_Str"",""String_Node_Str"").addLongField(""String_Node_Str"",200000L).build();
  PartitionedFileSetArguments.setOutputPartitionKey(outputArgs,keyY);
  runtimeArguments.putAll(RuntimeArguments.addScope(Scope.DATASET,PARTITIONED,outputArgs));
  Assert.assertTrue(runProgram(app,AppWithPartitionedFileSet.PartitionWriter.class,new BasicArguments(runtimeArguments)));
  Transactions.createTransactionExecutor(txExecutorFactory,(TransactionAware)dataset).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply(){
      Partition partition=dataset.getPartition(keyY);
      Assert.assertNotNull(partition);
      String path=partition.getRelativePath();
      Assert.assertNotNull(path);
      Assert.assertTrue(path.contains(""String_Node_Str""));
      Assert.assertTrue(path.contains(""String_Node_Str""));
    }
  }
);
  PartitionFilter filterXY=PartitionFilter.builder().addRangeCondition(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build();
  runtimeArguments=Maps.newHashMap();
  Map<String,String> inputArgs=Maps.newHashMap();
  PartitionedFileSetArguments.setInputPartitionFilter(inputArgs,filterXY);
  runtimeArguments.putAll(RuntimeArguments.addScope(Scope.DATASET,PARTITIONED,inputArgs));
  runtimeArguments.put(AppWithPartitionedFileSet.ROW_TO_WRITE,""String_Node_Str"");
  Assert.assertTrue(runProgram(app,AppWithPartitionedFileSet.PartitionReader.class,new BasicArguments(runtimeArguments)));
  final Table output=datasetCache.getDataset(AppWithPartitionedFileSet.OUTPUT);
  Transactions.createTransactionExecutor(txExecutorFactory,(TransactionAware)output).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply(){
      Row row=output.get(Bytes.toBytes(""String_Node_Str""));
      Assert.assertEquals(""String_Node_Str"",row.getString(""String_Node_Str""));
      Assert.assertEquals(""String_Node_Str"",row.getString(""String_Node_Str""));
    }
  }
);
  PartitionFilter filterX=PartitionFilter.builder().addValueCondition(""String_Node_Str"",""String_Node_Str"").addRangeCondition(""String_Node_Str"",null,160000L).build();
  inputArgs.clear();
  PartitionedFileSetArguments.setInputPartitionFilter(inputArgs,filterX);
  runtimeArguments.putAll(RuntimeArguments.addScope(Scope.DATASET,PARTITIONED,inputArgs));
  runtimeArguments.put(AppWithPartitionedFileSet.ROW_TO_WRITE,""String_Node_Str"");
  Assert.assertTrue(runProgram(app,AppWithPartitionedFileSet.PartitionReader.class,new BasicArguments(runtimeArguments)));
  Transactions.createTransactionExecutor(txExecutorFactory,(TransactionAware)output).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply(){
      Row row=output.get(Bytes.toBytes(""String_Node_Str""));
      Assert.assertEquals(""String_Node_Str"",row.getString(""String_Node_Str""));
      Assert.assertNull(row.get(""String_Node_Str""));
    }
  }
);
  PartitionFilter filterMT=PartitionFilter.builder().addValueCondition(""String_Node_Str"",""String_Node_Str"").build();
  inputArgs.clear();
  PartitionedFileSetArguments.setInputPartitionFilter(inputArgs,filterMT);
  runtimeArguments.putAll(RuntimeArguments.addScope(Scope.DATASET,PARTITIONED,inputArgs));
  runtimeArguments.put(AppWithPartitionedFileSet.ROW_TO_WRITE,""String_Node_Str"");
  Assert.assertTrue(runProgram(app,AppWithPartitionedFileSet.PartitionReader.class,new BasicArguments(runtimeArguments)));
  Transactions.createTransactionExecutor(txExecutorFactory,(TransactionAware)output).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply(){
      Row row=output.get(Bytes.toBytes(""String_Node_Str""));
      Assert.assertTrue(row.isEmpty());
    }
  }
);
}","private void testPartitionedFileSetWithMR(boolean useCombineFileInputFormat) throws Exception {
  ApplicationWithPrograms app=deployApp(AppWithPartitionedFileSet.class,new AppWithPartitionedFileSet.AppConfig(useCombineFileInputFormat));
  final Table table=datasetCache.getDataset(AppWithPartitionedFileSet.INPUT);
  Transactions.createTransactionExecutor(txExecutorFactory,(TransactionAware)table).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply(){
      table.put(Bytes.toBytes(""String_Node_Str""),AppWithPartitionedFileSet.ONLY_COLUMN,Bytes.toBytes(""String_Node_Str""));
    }
  }
);
  final PartitionKey keyX=PartitionKey.builder().addStringField(""String_Node_Str"",""String_Node_Str"").addLongField(""String_Node_Str"",150000L).build();
  Map<String,String> runtimeArguments=Maps.newHashMap();
  Map<String,String> outputArgs=Maps.newHashMap();
  PartitionedFileSetArguments.setOutputPartitionKey(outputArgs,keyX);
  runtimeArguments.putAll(RuntimeArguments.addScope(Scope.DATASET,PARTITIONED,outputArgs));
  Assert.assertTrue(runProgram(app,AppWithPartitionedFileSet.PartitionWriter.class,new BasicArguments(runtimeArguments)));
  final PartitionedFileSet dataset=datasetCache.getDataset(PARTITIONED);
  Transactions.createTransactionExecutor(txExecutorFactory,(TransactionAware)dataset).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply(){
      Partition partition=dataset.getPartition(keyX);
      Assert.assertNotNull(partition);
      String path=partition.getRelativePath();
      Assert.assertTrue(path.contains(""String_Node_Str""));
      Assert.assertTrue(path.contains(""String_Node_Str""));
    }
  }
);
  Transactions.createTransactionExecutor(txExecutorFactory,(TransactionAware)table).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply(){
      table.delete(Bytes.toBytes(""String_Node_Str""));
      table.put(Bytes.toBytes(""String_Node_Str""),AppWithPartitionedFileSet.ONLY_COLUMN,Bytes.toBytes(""String_Node_Str""));
    }
  }
);
  final PartitionKey keyY=PartitionKey.builder().addStringField(""String_Node_Str"",""String_Node_Str"").addLongField(""String_Node_Str"",200000L).build();
  PartitionedFileSetArguments.setOutputPartitionKey(outputArgs,keyY);
  runtimeArguments.putAll(RuntimeArguments.addScope(Scope.DATASET,PARTITIONED,outputArgs));
  Assert.assertTrue(runProgram(app,AppWithPartitionedFileSet.PartitionWriter.class,new BasicArguments(runtimeArguments)));
  Transactions.createTransactionExecutor(txExecutorFactory,(TransactionAware)dataset).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply(){
      Partition partition=dataset.getPartition(keyY);
      Assert.assertNotNull(partition);
      String path=partition.getRelativePath();
      Assert.assertNotNull(path);
      Assert.assertTrue(path.contains(""String_Node_Str""));
      Assert.assertTrue(path.contains(""String_Node_Str""));
    }
  }
);
  PartitionFilter filterXY=PartitionFilter.builder().addRangeCondition(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build();
  runtimeArguments=Maps.newHashMap();
  Map<String,String> inputArgs=Maps.newHashMap();
  PartitionedFileSetArguments.setInputPartitionFilter(inputArgs,filterXY);
  runtimeArguments.putAll(RuntimeArguments.addScope(Scope.DATASET,PARTITIONED,inputArgs));
  runtimeArguments.put(AppWithPartitionedFileSet.ROW_TO_WRITE,""String_Node_Str"");
  Assert.assertTrue(runProgram(app,AppWithPartitionedFileSet.PartitionReader.class,new BasicArguments(runtimeArguments)));
  final Table output=datasetCache.getDataset(AppWithPartitionedFileSet.OUTPUT);
  Transactions.createTransactionExecutor(txExecutorFactory,(TransactionAware)output).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply(){
      Row row=output.get(Bytes.toBytes(""String_Node_Str""));
      Assert.assertEquals(""String_Node_Str"",row.getString(""String_Node_Str""));
      Assert.assertEquals(""String_Node_Str"",row.getString(""String_Node_Str""));
      Assert.assertEquals(""String_Node_Str"",row.getString(""String_Node_Str""));
      Assert.assertEquals(""String_Node_Str"",row.getString(""String_Node_Str""));
    }
  }
);
  PartitionFilter filterX=PartitionFilter.builder().addValueCondition(""String_Node_Str"",""String_Node_Str"").addRangeCondition(""String_Node_Str"",null,160000L).build();
  inputArgs.clear();
  PartitionedFileSetArguments.setInputPartitionFilter(inputArgs,filterX);
  runtimeArguments.putAll(RuntimeArguments.addScope(Scope.DATASET,PARTITIONED,inputArgs));
  runtimeArguments.put(AppWithPartitionedFileSet.ROW_TO_WRITE,""String_Node_Str"");
  Assert.assertTrue(runProgram(app,AppWithPartitionedFileSet.PartitionReader.class,new BasicArguments(runtimeArguments)));
  Transactions.createTransactionExecutor(txExecutorFactory,(TransactionAware)output).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply(){
      Row row=output.get(Bytes.toBytes(""String_Node_Str""));
      Assert.assertEquals(""String_Node_Str"",row.getString(""String_Node_Str""));
      Assert.assertEquals(""String_Node_Str"",row.getString(""String_Node_Str""));
      Assert.assertNull(row.get(""String_Node_Str""));
      Assert.assertNull(row.get(""String_Node_Str""));
    }
  }
);
  PartitionFilter filterMT=PartitionFilter.builder().addValueCondition(""String_Node_Str"",""String_Node_Str"").build();
  inputArgs.clear();
  PartitionedFileSetArguments.setInputPartitionFilter(inputArgs,filterMT);
  runtimeArguments.putAll(RuntimeArguments.addScope(Scope.DATASET,PARTITIONED,inputArgs));
  runtimeArguments.put(AppWithPartitionedFileSet.ROW_TO_WRITE,""String_Node_Str"");
  Assert.assertTrue(runProgram(app,AppWithPartitionedFileSet.PartitionReader.class,new BasicArguments(runtimeArguments)));
  Transactions.createTransactionExecutor(txExecutorFactory,(TransactionAware)output).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply(){
      Row row=output.get(Bytes.toBytes(""String_Node_Str""));
      Assert.assertTrue(row.isEmpty());
    }
  }
);
}"
4848,"@Override protected void configure(){
  super.configure();
  addStream(new Stream(streamName));
}","@Override protected void configure(){
  setName(WORKFLOW_NAME);
}"
4849,"@Override protected void configure(){
  super.configure();
  addStream(new Stream(streamName));
}","@Override protected void configure(){
  setName(WORKFLOW_NAME);
}"
4850,"@Test public void testAppWithConfig() throws Exception {
  LocationFactory locationFactory=new LocalLocationFactory(TMP_FOLDER.newFolder());
  Location appJar=AppJarHelper.createDeploymentJar(locationFactory,ConfigTestApp.class);
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,ConfigTestApp.class.getSimpleName(),""String_Node_Str"");
  CConfiguration cConf=CConfiguration.create();
  ArtifactRepository artifactRepo=new ArtifactRepository(conf,null,null,authorizer,new DummyProgramRunnerFactory(),new DefaultImpersonator(cConf,null),authEnforcer,authenticationContext);
  ConfigTestApp.ConfigClass config=new ConfigTestApp.ConfigClass(""String_Node_Str"",""String_Node_Str"");
  try (CloseableClassLoader artifactClassLoader=artifactRepo.createArtifactClassLoader(appJar,new EntityImpersonator(artifactId.getNamespace().toEntityId(),new DefaultImpersonator(cConf,null)))){
    Configurator configuratorWithConfig=new InMemoryConfigurator(conf,Id.Namespace.DEFAULT,artifactId,ConfigTestApp.class.getName(),artifactRepo,artifactClassLoader,null,null,new Gson().toJson(config));
    ListenableFuture<ConfigResponse> result=configuratorWithConfig.config();
    ConfigResponse response=result.get(10,TimeUnit.SECONDS);
    Assert.assertNotNull(response);
    ApplicationSpecificationAdapter adapter=ApplicationSpecificationAdapter.create(new ReflectionSchemaGenerator());
    ApplicationSpecification specification=adapter.fromJson(response.get());
    Assert.assertNotNull(specification);
    Assert.assertTrue(specification.getStreams().size() == 1);
    Assert.assertTrue(specification.getStreams().containsKey(""String_Node_Str""));
    Assert.assertTrue(specification.getDatasets().size() == 1);
    Assert.assertTrue(specification.getDatasets().containsKey(""String_Node_Str""));
    Configurator configuratorWithoutConfig=new InMemoryConfigurator(conf,Id.Namespace.DEFAULT,artifactId,ConfigTestApp.class.getName(),artifactRepo,artifactClassLoader,null,null,null);
    result=configuratorWithoutConfig.config();
    response=result.get(10,TimeUnit.SECONDS);
    Assert.assertNotNull(response);
    specification=adapter.fromJson(response.get());
    Assert.assertNotNull(specification);
    Assert.assertTrue(specification.getStreams().size() == 1);
    Assert.assertTrue(specification.getStreams().containsKey(ConfigTestApp.DEFAULT_STREAM));
    Assert.assertTrue(specification.getDatasets().size() == 1);
    Assert.assertTrue(specification.getDatasets().containsKey(ConfigTestApp.DEFAULT_TABLE));
  }
 }","@Test public void testAppWithConfig() throws Exception {
  LocationFactory locationFactory=new LocalLocationFactory(TMP_FOLDER.newFolder());
  Location appJar=AppJarHelper.createDeploymentJar(locationFactory,ConfigTestApp.class);
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,ConfigTestApp.class.getSimpleName(),""String_Node_Str"");
  CConfiguration cConf=CConfiguration.create();
  ArtifactRepository artifactRepo=new ArtifactRepository(conf,null,null,authorizer,new DummyProgramRunnerFactory(),new DefaultImpersonator(cConf,null),authEnforcer,authenticationContext);
  ConfigTestApp.ConfigClass config=new ConfigTestApp.ConfigClass(""String_Node_Str"",""String_Node_Str"");
  try (CloseableClassLoader artifactClassLoader=artifactRepo.createArtifactClassLoader(appJar,new EntityImpersonator(artifactId.getNamespace().toEntityId(),new DefaultImpersonator(cConf,null)))){
    Configurator configuratorWithConfig=new InMemoryConfigurator(conf,Id.Namespace.DEFAULT,artifactId,ConfigTestApp.class.getName(),artifactRepo,artifactClassLoader,null,null,new Gson().toJson(config));
    ListenableFuture<ConfigResponse> result=configuratorWithConfig.config();
    ConfigResponse response=result.get(10,TimeUnit.SECONDS);
    Assert.assertNotNull(response);
    ApplicationSpecificationAdapter adapter=ApplicationSpecificationAdapter.create(new ReflectionSchemaGenerator());
    ApplicationSpecification specification=adapter.fromJson(response.get());
    Assert.assertNotNull(specification);
    Assert.assertTrue(specification.getStreams().size() == 1);
    Assert.assertTrue(specification.getStreams().containsKey(""String_Node_Str""));
    Assert.assertTrue(specification.getDatasets().size() == 1);
    Assert.assertTrue(specification.getDatasets().containsKey(""String_Node_Str""));
    Configurator configuratorWithoutConfig=new InMemoryConfigurator(conf,Id.Namespace.DEFAULT,artifactId,ConfigTestApp.class.getName(),artifactRepo,artifactClassLoader,null,null,null);
    result=configuratorWithoutConfig.config();
    response=result.get(10,TimeUnit.SECONDS);
    Assert.assertNotNull(response);
    specification=adapter.fromJson(response.get());
    Assert.assertNotNull(specification);
    Assert.assertTrue(specification.getStreams().size() == 1);
    Assert.assertTrue(specification.getStreams().containsKey(ConfigTestApp.DEFAULT_STREAM));
    Assert.assertTrue(specification.getDatasets().size() == 1);
    Assert.assertTrue(specification.getDatasets().containsKey(ConfigTestApp.DEFAULT_TABLE));
    Assert.assertNotNull(specification.getProgramSchedules().get(ConfigTestApp.SCHEDULE_NAME));
    ProgramStatusTrigger trigger=(ProgramStatusTrigger)specification.getProgramSchedules().get(ConfigTestApp.SCHEDULE_NAME).getTrigger();
    Assert.assertEquals(trigger.getProgramId().getProgram(),ConfigTestApp.WORKFLOW_NAME);
  }
 }"
4851,"@Override public void run(){
  try {
    ImpersonationUtils.doAs(opInfo.getUGI(),new Callable<Void>(){
      @Override public Void call() throws Exception {
        try {
          QueryStatus status=exploreService.fetchStatus(opInfo);
          if (status.getStatus() != QueryStatus.OpStatus.FINISHED && status.getStatus() != QueryStatus.OpStatus.CLOSED && status.getStatus() != QueryStatus.OpStatus.CANCELED && status.getStatus() != QueryStatus.OpStatus.ERROR) {
            LOG.info(""String_Node_Str"",handle.getHandle(),status.getStatus());
            exploreService.cancelInternal(handle);
          }
        }
 catch (        Throwable e) {
          LOG.error(""String_Node_Str"",handle.getHandle(),e);
        }
 finally {
          LOG.debug(""String_Node_Str"",handle);
          try {
            exploreService.closeInternal(handle,opInfo);
          }
 catch (          Throwable e) {
            LOG.error(""String_Node_Str"",handle,e);
          }
        }
        return null;
      }
    }
);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",handle);
  }
 finally {
    try {
      if (!UserGroupInformation.getLoginUser().equals(opInfo.getUGI())) {
        FileSystem.closeAllForUGI(opInfo.getUGI());
      }
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",opInfo.getUGI(),e);
    }
  }
}","@Override public void run(){
  try {
    ImpersonationUtils.doAs(opInfo.getUGI(),new Callable<Void>(){
      @Override public Void call() throws Exception {
        try {
          QueryStatus status=exploreService.fetchStatus(opInfo);
          if (status.getStatus() != QueryStatus.OpStatus.FINISHED && status.getStatus() != QueryStatus.OpStatus.CLOSED && status.getStatus() != QueryStatus.OpStatus.CANCELED && status.getStatus() != QueryStatus.OpStatus.ERROR) {
            LOG.info(""String_Node_Str"",handle.getHandle(),status.getStatus());
            exploreService.cancelInternal(handle);
          }
        }
 catch (        Throwable e) {
          LOG.error(""String_Node_Str"",handle.getHandle(),e);
        }
 finally {
          LOG.debug(""String_Node_Str"",handle);
          try {
            exploreService.closeInternal(handle,opInfo);
          }
 catch (          Throwable e) {
            LOG.error(""String_Node_Str"",handle,e);
          }
        }
        return null;
      }
    }
);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",handle);
  }
}"
4852,"@Test public void testHBaseVersions() throws ParseException {
  String version=""String_Node_Str"";
  HBaseVersion.VersionNumber versionNumber=HBaseVersion.VersionNumber.create(version);
  Assert.assertEquals(new Integer(1),versionNumber.getMajor());
  Assert.assertEquals(new Integer(2),versionNumber.getMinor());
  Assert.assertEquals(new Integer(0),versionNumber.getPatch());
  Assert.assertNull(versionNumber.getLast());
  Assert.assertEquals(""String_Node_Str"",versionNumber.getClassifier());
  Assert.assertTrue(versionNumber.isSnapshot());
  version=""String_Node_Str"";
  versionNumber=HBaseVersion.VersionNumber.create(version);
  Assert.assertEquals(new Integer(1),versionNumber.getMajor());
  Assert.assertEquals(new Integer(2),versionNumber.getMinor());
  Assert.assertEquals(new Integer(0),versionNumber.getPatch());
  Assert.assertNull(versionNumber.getLast());
  Assert.assertEquals(""String_Node_Str"",versionNumber.getClassifier());
  Assert.assertFalse(versionNumber.isSnapshot());
  version=""String_Node_Str"";
  versionNumber=HBaseVersion.VersionNumber.create(version);
  Assert.assertEquals(new Integer(1),versionNumber.getMajor());
  Assert.assertEquals(new Integer(1),versionNumber.getMinor());
  Assert.assertEquals(new Integer(1),versionNumber.getPatch());
  Assert.assertNull(versionNumber.getLast());
  Assert.assertNull(versionNumber.getClassifier());
  Assert.assertFalse(versionNumber.isSnapshot());
}","@Test public void testHBaseVersions() throws ParseException {
  String version=""String_Node_Str"";
  HBaseVersion.VersionNumber versionNumber=HBaseVersion.VersionNumber.create(version);
  Assert.assertEquals(new Integer(1),versionNumber.getMajor());
  Assert.assertEquals(new Integer(2),versionNumber.getMinor());
  Assert.assertEquals(new Integer(0),versionNumber.getPatch());
  Assert.assertNull(versionNumber.getLast());
  Assert.assertEquals(""String_Node_Str"",versionNumber.getClassifier());
  Assert.assertTrue(versionNumber.isSnapshot());
  version=""String_Node_Str"";
  versionNumber=HBaseVersion.VersionNumber.create(version);
  Assert.assertEquals(new Integer(1),versionNumber.getMajor());
  Assert.assertEquals(new Integer(1),versionNumber.getMinor());
  Assert.assertEquals(new Integer(2),versionNumber.getPatch());
  Assert.assertNull(versionNumber.getLast());
  Assert.assertNull(versionNumber.getClassifier());
  Assert.assertFalse(versionNumber.isSnapshot());
  version=""String_Node_Str"";
  versionNumber=HBaseVersion.VersionNumber.create(version);
  Assert.assertEquals(new Integer(1),versionNumber.getMajor());
  Assert.assertEquals(new Integer(2),versionNumber.getMinor());
  Assert.assertEquals(new Integer(0),versionNumber.getPatch());
  Assert.assertNull(versionNumber.getLast());
  Assert.assertEquals(""String_Node_Str"",versionNumber.getClassifier());
  Assert.assertFalse(versionNumber.isSnapshot());
  version=""String_Node_Str"";
  versionNumber=HBaseVersion.VersionNumber.create(version);
  Assert.assertEquals(new Integer(1),versionNumber.getMajor());
  Assert.assertEquals(new Integer(1),versionNumber.getMinor());
  Assert.assertEquals(new Integer(1),versionNumber.getPatch());
  Assert.assertNull(versionNumber.getLast());
  Assert.assertNull(versionNumber.getClassifier());
  Assert.assertFalse(versionNumber.isSnapshot());
}"
4853,"@Test public void testHBaseVersionToCompatMapping() throws ParseException {
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_96,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH55,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH56,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
}","@Test public void testHBaseVersionToCompatMapping() throws ParseException {
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_96,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH55,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH56,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_96,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.UNKNOWN_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
}"
4854,"public static VersionNumber create(String versionString) throws ParseException {
  Matcher matcher=PATTERN.matcher(versionString);
  Matcher ibmMatcher=IBM_PATTERN.matcher(versionString);
  if (matcher.matches()) {
    String majorString=matcher.group(1);
    String minorString=matcher.group(3);
    String patchString=matcher.group(5);
    String last=matcher.group(7);
    String classifier=matcher.group(9);
    String snapshotString=matcher.group(10);
    return new VersionNumber(new Integer(majorString),minorString != null ? new Integer(minorString) : null,patchString != null ? new Integer(patchString) : null,last != null ? new Integer(last) : null,classifier,""String_Node_Str"".equals(snapshotString));
  }
 else   if (ibmMatcher.matches()) {
    String majorString=ibmMatcher.group(1);
    String minorString=ibmMatcher.group(3);
    String patchString=ibmMatcher.group(5);
    String last=ibmMatcher.group(7);
    String classifier=ibmMatcher.group(9);
    return new VersionNumber(new Integer(majorString),minorString != null ? new Integer(minorString) : null,patchString != null ? new Integer(patchString) : null,last != null ? new Integer(last) : null,classifier,false);
  }
  throw new ParseException(""String_Node_Str"",0);
}","public static VersionNumber create(String versionString) throws ParseException {
  Matcher matcher=PATTERN.matcher(versionString);
  Matcher hdpMatcher=HDP_PATTERN.matcher(versionString);
  Matcher ibmMatcher=IBM_PATTERN.matcher(versionString);
  if (matcher.matches()) {
    String majorString=matcher.group(1);
    String minorString=matcher.group(3);
    String patchString=matcher.group(5);
    String last=matcher.group(7);
    String classifier=matcher.group(9);
    String snapshotString=matcher.group(10);
    return new VersionNumber(new Integer(majorString),minorString != null ? new Integer(minorString) : null,patchString != null ? new Integer(patchString) : null,last != null ? new Integer(last) : null,classifier,""String_Node_Str"".equals(snapshotString));
  }
 else   if (hdpMatcher.matches()) {
    String majorString=hdpMatcher.group(1);
    String minorString=hdpMatcher.group(3);
    String patchString=hdpMatcher.group(5);
    return new VersionNumber(new Integer(majorString),minorString != null ? new Integer(minorString) : null,patchString != null ? new Integer(patchString) : null,null,null,false);
  }
 else   if (ibmMatcher.matches()) {
    String majorString=ibmMatcher.group(1);
    String minorString=ibmMatcher.group(3);
    String patchString=ibmMatcher.group(5);
    String last=ibmMatcher.group(7);
    String classifier=ibmMatcher.group(9);
    return new VersionNumber(new Integer(majorString),minorString != null ? new Integer(minorString) : null,patchString != null ? new Integer(patchString) : null,last != null ? new Integer(last) : null,classifier,false);
  }
  throw new ParseException(""String_Node_Str"",0);
}"
4855,"@Override public Service get(){
  return new AbstractService(){
    @Override protected void doStart(){
      try {
        namespaceAdmin.create(NamespaceMeta.DEFAULT);
        LOG.info(""String_Node_Str"",NamespaceMeta.DEFAULT);
        notifyStarted();
      }
 catch (      AlreadyExistsException e) {
        LOG.info(""String_Node_Str"");
        notifyStarted();
      }
catch (      Exception e) {
        notifyFailed(e);
      }
    }
    @Override protected void doStop(){
      notifyStopped();
    }
  }
;
}","@Override public Service get(){
  return new AbstractService(){
    @Override protected void doStart(){
      try {
        namespaceAdmin.create(NamespaceMeta.DEFAULT);
        LOG.info(""String_Node_Str"",NamespaceMeta.DEFAULT);
        notifyStarted();
      }
 catch (      FileAlreadyExistsException e) {
        LOG.warn(""String_Node_Str"",NamespaceMeta.DEFAULT,e);
        notifyStarted();
      }
catch (      NamespaceAlreadyExistsException e) {
        LOG.info(""String_Node_Str"");
        notifyStarted();
      }
catch (      Exception e) {
        notifyFailed(e);
      }
    }
    @Override protected void doStop(){
      notifyStopped();
    }
  }
;
}"
4856,"@Inject public DefaultNamespaceEnsurer(final NamespaceAdmin namespaceAdmin){
  this.serviceDelegate=new RetryOnStartFailureService(new Supplier<Service>(){
    @Override public Service get(){
      return new AbstractService(){
        @Override protected void doStart(){
          try {
            namespaceAdmin.create(NamespaceMeta.DEFAULT);
            LOG.info(""String_Node_Str"",NamespaceMeta.DEFAULT);
            notifyStarted();
          }
 catch (          AlreadyExistsException e) {
            LOG.info(""String_Node_Str"");
            notifyStarted();
          }
catch (          Exception e) {
            notifyFailed(e);
          }
        }
        @Override protected void doStop(){
          notifyStopped();
        }
      }
;
    }
  }
,RetryStrategies.exponentialDelay(200,5000,TimeUnit.MILLISECONDS));
}","@Inject public DefaultNamespaceEnsurer(final NamespaceAdmin namespaceAdmin){
  this.serviceDelegate=new RetryOnStartFailureService(new Supplier<Service>(){
    @Override public Service get(){
      return new AbstractService(){
        @Override protected void doStart(){
          try {
            namespaceAdmin.create(NamespaceMeta.DEFAULT);
            LOG.info(""String_Node_Str"",NamespaceMeta.DEFAULT);
            notifyStarted();
          }
 catch (          FileAlreadyExistsException e) {
            LOG.warn(""String_Node_Str"",NamespaceMeta.DEFAULT,e);
            notifyStarted();
          }
catch (          NamespaceAlreadyExistsException e) {
            LOG.info(""String_Node_Str"");
            notifyStarted();
          }
catch (          Exception e) {
            notifyFailed(e);
          }
        }
        @Override protected void doStop(){
          notifyStopped();
        }
      }
;
    }
  }
,RetryStrategies.exponentialDelay(200,5000,TimeUnit.MILLISECONDS));
}"
4857,"@Override public String get(){
  InputStream errorStream=urlConn.getErrorStream();
  try {
    return errorStream == null ? ""String_Node_Str"" : new String(ByteStreams.toByteArray(errorStream),StandardCharsets.UTF_8);
  }
 catch (  IOException e) {
    return ""String_Node_Str"";
  }
}","@Override public String get(){
  try (InputStream errorStream=urlConn.getErrorStream()){
    return errorStream == null ? ""String_Node_Str"" : new String(ByteStreams.toByteArray(errorStream),StandardCharsets.UTF_8);
  }
 catch (  IOException e) {
    return ""String_Node_Str"";
  }
 finally {
    urlConn.disconnect();
  }
}"
4858,"@Override public CloseableIterator<RawMessage> fetch() throws IOException, TopicNotFoundException {
  GenericRecord record=new GenericData.Record(Schemas.V1.ConsumeRequest.SCHEMA);
  if (getStartOffset() != null) {
    record.put(""String_Node_Str"",ByteBuffer.wrap(getStartOffset()));
  }
  if (getStartTime() != null) {
    record.put(""String_Node_Str"",getStartTime());
  }
  record.put(""String_Node_Str"",isIncludeStart());
  record.put(""String_Node_Str"",getLimit());
  if (getTransaction() != null) {
    record.put(""String_Node_Str"",ByteBuffer.wrap(TRANSACTION_CODEC.encode(getTransaction())));
  }
  URL url=remoteClient.resolve(createTopicPath(topicId) + ""String_Node_Str"");
  final HttpURLConnection urlConn=(HttpURLConnection)url.openConnection();
  urlConn.setConnectTimeout(HTTP_REQUEST_CONFIG.getConnectTimeout());
  urlConn.setReadTimeout(HTTP_REQUEST_CONFIG.getReadTimeout());
  urlConn.setRequestMethod(""String_Node_Str"");
  urlConn.setRequestProperty(HttpHeaders.CONTENT_TYPE,""String_Node_Str"");
  urlConn.setDoInput(true);
  urlConn.setDoOutput(true);
  Encoder encoder=EncoderFactory.get().directBinaryEncoder(urlConn.getOutputStream(),null);
  DatumWriter<GenericRecord> datumWriter=new GenericDatumWriter<>(Schemas.V1.ConsumeRequest.SCHEMA);
  datumWriter.write(record,encoder);
  int responseCode=urlConn.getResponseCode();
  if (responseCode == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new TopicNotFoundException(topicId.getNamespace(),topicId.getTopic());
  }
  handleError(responseCode,new Supplier<String>(){
    @Override public String get(){
      InputStream errorStream=urlConn.getErrorStream();
      try {
        return errorStream == null ? ""String_Node_Str"" : new String(ByteStreams.toByteArray(errorStream),StandardCharsets.UTF_8);
      }
 catch (      IOException e) {
        return ""String_Node_Str"";
      }
    }
  }
,""String_Node_Str"" + topicId);
  verifyContentType(urlConn.getHeaderFields(),""String_Node_Str"");
  final Decoder decoder=DecoderFactory.get().binaryDecoder(urlConn.getInputStream(),null);
  final long initialItemCount=decoder.readArrayStart();
  return new AbstractCloseableIterator<RawMessage>(){
    private long itemCount=initialItemCount;
    @Override protected RawMessage computeNext(){
      if (initialItemCount == 0) {
        return endOfData();
      }
      try {
        if (itemCount == 0) {
          itemCount=decoder.arrayNext();
          if (itemCount == 0) {
            return endOfData();
          }
        }
        itemCount--;
        messageRecord=messageReader.read(messageRecord,decoder);
        return new RawMessage(Bytes.toBytes((ByteBuffer)messageRecord.get(""String_Node_Str"")),Bytes.toBytes((ByteBuffer)messageRecord.get(""String_Node_Str"")));
      }
 catch (      IOException e) {
        throw Throwables.propagate(e);
      }
    }
    @Override public void close(){
      urlConn.disconnect();
    }
  }
;
}","@Override public CloseableIterator<RawMessage> fetch() throws IOException, TopicNotFoundException {
  GenericRecord record=new GenericData.Record(Schemas.V1.ConsumeRequest.SCHEMA);
  if (getStartOffset() != null) {
    record.put(""String_Node_Str"",ByteBuffer.wrap(getStartOffset()));
  }
  if (getStartTime() != null) {
    record.put(""String_Node_Str"",getStartTime());
  }
  record.put(""String_Node_Str"",isIncludeStart());
  record.put(""String_Node_Str"",getLimit());
  if (getTransaction() != null) {
    record.put(""String_Node_Str"",ByteBuffer.wrap(TRANSACTION_CODEC.encode(getTransaction())));
  }
  URL url=remoteClient.resolve(createTopicPath(topicId) + ""String_Node_Str"");
  final HttpURLConnection urlConn=(HttpURLConnection)url.openConnection();
  urlConn.setConnectTimeout(HTTP_REQUEST_CONFIG.getConnectTimeout());
  urlConn.setReadTimeout(HTTP_REQUEST_CONFIG.getReadTimeout());
  urlConn.setRequestMethod(""String_Node_Str"");
  urlConn.setRequestProperty(HttpHeaders.CONTENT_TYPE,""String_Node_Str"");
  urlConn.setDoInput(true);
  urlConn.setDoOutput(true);
  Encoder encoder=EncoderFactory.get().directBinaryEncoder(urlConn.getOutputStream(),null);
  DatumWriter<GenericRecord> datumWriter=new GenericDatumWriter<>(Schemas.V1.ConsumeRequest.SCHEMA);
  datumWriter.write(record,encoder);
  int responseCode=urlConn.getResponseCode();
  if (responseCode == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new TopicNotFoundException(topicId.getNamespace(),topicId.getTopic());
  }
  handleError(responseCode,new Supplier<String>(){
    @Override public String get(){
      try (InputStream errorStream=urlConn.getErrorStream()){
        return errorStream == null ? ""String_Node_Str"" : new String(ByteStreams.toByteArray(errorStream),StandardCharsets.UTF_8);
      }
 catch (      IOException e) {
        return ""String_Node_Str"";
      }
 finally {
        urlConn.disconnect();
      }
    }
  }
,""String_Node_Str"" + topicId);
  verifyContentType(urlConn.getHeaderFields(),""String_Node_Str"");
  final InputStream inputStream=urlConn.getInputStream();
  final Decoder decoder=DecoderFactory.get().binaryDecoder(inputStream,null);
  final long initialItemCount=decoder.readArrayStart();
  return new AbstractCloseableIterator<RawMessage>(){
    private long itemCount=initialItemCount;
    @Override protected RawMessage computeNext(){
      if (initialItemCount == 0) {
        return endOfData();
      }
      try {
        if (itemCount == 0) {
          itemCount=decoder.arrayNext();
          if (itemCount == 0) {
            return endOfData();
          }
        }
        itemCount--;
        messageRecord=messageReader.read(messageRecord,decoder);
        return new RawMessage(Bytes.toBytes((ByteBuffer)messageRecord.get(""String_Node_Str"")),Bytes.toBytes((ByteBuffer)messageRecord.get(""String_Node_Str"")));
      }
 catch (      IOException e) {
        throw Throwables.propagate(e);
      }
    }
    @Override public void close(){
      Closeables.closeQuietly(inputStream);
      urlConn.disconnect();
    }
  }
;
}"
4859,"@Override public void close(){
  urlConn.disconnect();
}","@Override public void close(){
  Closeables.closeQuietly(inputStream);
  urlConn.disconnect();
}"
4860,"/** 
 * Creates a jar that contains everything that are needed for running the MapReduce program by Hadoop.
 * @return a new {@link File} containing the job jar
 */
private File buildJobJar(Job job,File tempDir) throws IOException, URISyntaxException {
  File jobJar=new File(tempDir,""String_Node_Str"");
  LOG.debug(""String_Node_Str"",jobJar);
  if (MapReduceTaskContextProvider.isLocal(job.getConfiguration())) {
    JarOutputStream output=new JarOutputStream(new FileOutputStream(jobJar));
    output.close();
    return jobJar;
  }
  final HadoopClassExcluder hadoopClassExcluder=new HadoopClassExcluder();
  ApplicationBundler appBundler=new ApplicationBundler(new ClassAcceptor(){
    @Override public boolean accept(    String className,    URL classUrl,    URL classPathUrl){
      if (className.startsWith(""String_Node_Str"") || classPathUrl.toString().contains(""String_Node_Str"")) {
        return false;
      }
      return hadoopClassExcluder.accept(className,classUrl,classPathUrl);
    }
  }
);
  Set<Class<?>> classes=Sets.newHashSet();
  classes.add(MapReduce.class);
  classes.add(MapperWrapper.class);
  classes.add(ReducerWrapper.class);
  classes.add(SLF4JBridgeHandler.class);
  if (cConf.getBoolean(Constants.AppFabric.MAPREDUCE_INCLUDE_CUSTOM_CLASSES)) {
    try {
      Class<? extends InputFormat<?,?>> inputFormatClass=job.getInputFormatClass();
      classes.add(inputFormatClass);
      if (MapReduceStreamInputFormat.class.isAssignableFrom(inputFormatClass)) {
        Class<? extends StreamEventDecoder> decoderType=MapReduceStreamInputFormat.getDecoderClass(job.getConfiguration());
        if (decoderType != null) {
          classes.add(decoderType);
        }
      }
    }
 catch (    Throwable t) {
      LOG.debug(""String_Node_Str"",t.getMessage(),t);
    }
    try {
      Class<? extends OutputFormat<?,?>> outputFormatClass=job.getOutputFormatClass();
      classes.add(outputFormatClass);
    }
 catch (    Throwable t) {
      LOG.debug(""String_Node_Str"",t.getMessage(),t);
    }
  }
  if (SecureStoreUtils.isKMSBacked(cConf) && SecureStoreUtils.isKMSCapable()) {
    classes.add(SecureStoreUtils.getKMSSecureStore());
  }
  Class<? extends HBaseDDLExecutor> ddlExecutorClass=new HBaseDDLExecutorFactory(cConf,hConf).get().getClass();
  try {
    Class<?> hbaseTableUtilClass=HBaseTableUtilFactory.getHBaseTableUtilClass(cConf);
    classes.add(hbaseTableUtilClass);
    classes.add(ddlExecutorClass);
  }
 catch (  ProvisionException e) {
    LOG.warn(""String_Node_Str"");
  }
  ClassLoader oldCLassLoader=ClassLoaders.setContextClassLoader(new CombineClassLoader(getClass().getClassLoader(),Collections.singleton(ddlExecutorClass.getClassLoader())));
  try {
    appBundler.createBundle(Locations.toLocation(jobJar),classes);
  }
  finally {
    ClassLoaders.setContextClassLoader(oldCLassLoader);
  }
  LOG.debug(""String_Node_Str"",jobJar.toURI());
  return jobJar;
}","/** 
 * Creates a jar that contains everything that are needed for running the MapReduce program by Hadoop.
 * @return a new {@link File} containing the job jar
 */
private File buildJobJar(Job job,File tempDir) throws IOException, URISyntaxException {
  File jobJar=new File(tempDir,""String_Node_Str"");
  LOG.debug(""String_Node_Str"",jobJar);
  if (MapReduceTaskContextProvider.isLocal(job.getConfiguration())) {
    JarOutputStream output=new JarOutputStream(new FileOutputStream(jobJar));
    output.close();
    return jobJar;
  }
  final HadoopClassExcluder hadoopClassExcluder=new HadoopClassExcluder();
  ApplicationBundler appBundler=new ApplicationBundler(new ClassAcceptor(){
    @Override public boolean accept(    String className,    URL classUrl,    URL classPathUrl){
      if (className.startsWith(""String_Node_Str"") || classPathUrl.toString().contains(""String_Node_Str"")) {
        return false;
      }
      return hadoopClassExcluder.accept(className,classUrl,classPathUrl);
    }
  }
);
  Set<Class<?>> classes=Sets.newHashSet();
  classes.add(MapReduce.class);
  classes.add(MapperWrapper.class);
  classes.add(ReducerWrapper.class);
  classes.add(SLF4JBridgeHandler.class);
  if (cConf.getBoolean(Constants.AppFabric.MAPREDUCE_INCLUDE_CUSTOM_CLASSES)) {
    try {
      Class<? extends InputFormat<?,?>> inputFormatClass=job.getInputFormatClass();
      classes.add(inputFormatClass);
      if (MapReduceStreamInputFormat.class.isAssignableFrom(inputFormatClass)) {
        Class<? extends StreamEventDecoder> decoderType=MapReduceStreamInputFormat.getDecoderClass(job.getConfiguration());
        if (decoderType != null) {
          classes.add(decoderType);
        }
      }
    }
 catch (    Throwable t) {
      LOG.debug(""String_Node_Str"",t.getMessage(),t);
    }
    try {
      Class<? extends OutputFormat<?,?>> outputFormatClass=job.getOutputFormatClass();
      classes.add(outputFormatClass);
    }
 catch (    Throwable t) {
      LOG.debug(""String_Node_Str"",t.getMessage(),t);
    }
  }
  if (SecureStoreUtils.isKMSBacked(cConf) && SecureStoreUtils.isKMSCapable()) {
    classes.add(SecureStoreUtils.getKMSSecureStore());
  }
  try {
    Class<?> hbaseTableUtilClass=HBaseTableUtilFactory.getHBaseTableUtilClass(cConf);
    classes.add(hbaseTableUtilClass);
  }
 catch (  ProvisionException e) {
    LOG.warn(""String_Node_Str"");
  }
  ClassLoader oldCLassLoader=ClassLoaders.setContextClassLoader(getClass().getClassLoader());
  try {
    appBundler.createBundle(Locations.toLocation(jobJar),classes);
  }
  finally {
    ClassLoaders.setContextClassLoader(oldCLassLoader);
  }
  LOG.debug(""String_Node_Str"",jobJar.toURI());
  return jobJar;
}"
4861,"@Override protected void startUp() throws Exception {
  File tempDir=createTempDirectory();
  cleanupTask=createCleanupTask(tempDir);
  try {
    Job job=createJob(new File(tempDir,""String_Node_Str""));
    Configuration mapredConf=job.getConfiguration();
    classLoader=new MapReduceClassLoader(injector,cConf,mapredConf,context.getProgram().getClassLoader(),context.getApplicationSpecification().getPlugins(),context.getPluginInstantiator());
    cleanupTask=createCleanupTask(cleanupTask,classLoader);
    mapredConf.setClassLoader(new WeakReferenceDelegatorClassLoader(classLoader));
    ClassLoaders.setContextClassLoader(mapredConf.getClassLoader());
    context.setJob(job);
    beforeSubmit(job);
    Map<String,String> localizedUserResources=localizeUserResources(job,tempDir);
    String jobName=job.getJobName();
    if (!jobName.isEmpty()) {
      LOG.warn(""String_Node_Str"",jobName);
    }
    job.setJobName(getJobName(context));
    Location tempLocation=createTempLocationDirectory();
    cleanupTask=createCleanupTask(cleanupTask,tempLocation);
    if (!MapReduceTaskContextProvider.isLocal(mapredConf)) {
      Location pluginArchive=createPluginArchive(tempLocation);
      if (pluginArchive != null) {
        job.addCacheArchive(pluginArchive.toURI());
        mapredConf.set(Constants.Plugin.ARCHIVE,pluginArchive.getName());
      }
    }
    TaskType.MAP.setResources(mapredConf,context.getMapperResources());
    TaskType.REDUCE.setResources(mapredConf,context.getReducerResources());
    MapperWrapper.wrap(job);
    ReducerWrapper.wrap(job);
    PartitionerWrapper.wrap(job);
    RawComparatorWrapper.CombinerGroupComparatorWrapper.wrap(job);
    RawComparatorWrapper.GroupComparatorWrapper.wrap(job);
    RawComparatorWrapper.KeyComparatorWrapper.wrap(job);
    File jobJar=buildJobJar(job,tempDir);
    job.setJar(jobJar.toURI().toString());
    Location programJar=programJarLocation;
    if (!MapReduceTaskContextProvider.isLocal(mapredConf)) {
      programJar=copyProgramJar(tempLocation);
      job.addCacheFile(programJar.toURI());
      Location launcherJar=createLauncherJar(tempLocation);
      job.addCacheFile(launcherJar.toURI());
      List<String> classpath=new ArrayList<>();
      classpath.add(launcherJar.getName());
      Location logbackLocation=ProgramRunners.createLogbackJar(tempLocation.append(""String_Node_Str""));
      if (logbackLocation != null) {
        job.addCacheFile(logbackLocation.toURI());
        classpath.add(logbackLocation.getName());
        mapredConf.set(""String_Node_Str"",""String_Node_Str"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR);
        mapredConf.set(""String_Node_Str"",""String_Node_Str"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR);
        mapredConf.set(""String_Node_Str"",""String_Node_Str"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR);
      }
      List<String> jarFiles=new ArrayList<>();
      try (JarFile jobJarFile=new JarFile(jobJar)){
        Enumeration<JarEntry> entries=jobJarFile.entries();
        while (entries.hasMoreElements()) {
          JarEntry entry=entries.nextElement();
          if (entry.getName().startsWith(""String_Node_Str"") && entry.getName().endsWith(""String_Node_Str"")) {
            jarFiles.add(""String_Node_Str"" + entry.getName());
          }
        }
      }
       Collections.sort(jarFiles);
      classpath.addAll(jarFiles);
      classpath.add(""String_Node_Str"");
      for (      URI jarURI : CConfigurationUtil.getExtraJars(cConf)) {
        if (""String_Node_Str"".equals(jarURI.getScheme())) {
          Location extraJarLocation=copyFileToLocation(new File(jarURI.getPath()),tempLocation);
          job.addCacheFile(extraJarLocation.toURI());
        }
 else {
          job.addCacheFile(jarURI);
        }
        classpath.add(LocalizationUtils.getLocalizedName(jarURI));
      }
      MapReduceContainerHelper.addMapReduceClassPath(mapredConf,classpath);
      mapredConf.set(MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(classpath));
      mapredConf.set(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(classpath));
    }
    MapReduceContextConfig contextConfig=new MapReduceContextConfig(mapredConf);
    Transaction tx=txClient.startLong();
    try {
      CConfiguration cConfCopy=cConf;
      contextConfig.set(context,cConfCopy,tx,programJar.toURI(),localizedUserResources);
      job.submit();
      LOG.debug(""String_Node_Str"",context);
      this.job=job;
      this.transaction=tx;
    }
 catch (    Throwable t) {
      Transactions.invalidateQuietly(txClient,tx);
      throw t;
    }
  }
 catch (  LinkageError e) {
    throw new Exception(e.getMessage(),e);
  }
catch (  Throwable t) {
    cleanupTask.run();
    if (t instanceof TransactionFailureException) {
      throw Transactions.propagate((TransactionFailureException)t,Exception.class);
    }
    throw t;
  }
}","@Override protected void startUp() throws Exception {
  File tempDir=createTempDirectory();
  cleanupTask=createCleanupTask(tempDir);
  try {
    Job job=createJob(new File(tempDir,""String_Node_Str""));
    Configuration mapredConf=job.getConfiguration();
    classLoader=new MapReduceClassLoader(injector,cConf,mapredConf,context.getProgram().getClassLoader(),context.getApplicationSpecification().getPlugins(),context.getPluginInstantiator());
    cleanupTask=createCleanupTask(cleanupTask,classLoader);
    mapredConf.setClassLoader(new WeakReferenceDelegatorClassLoader(classLoader));
    ClassLoaders.setContextClassLoader(mapredConf.getClassLoader());
    context.setJob(job);
    beforeSubmit(job);
    Map<String,String> localizedUserResources=localizeUserResources(job,tempDir);
    String jobName=job.getJobName();
    if (!jobName.isEmpty()) {
      LOG.warn(""String_Node_Str"",jobName);
    }
    job.setJobName(getJobName(context));
    Location tempLocation=createTempLocationDirectory();
    cleanupTask=createCleanupTask(cleanupTask,tempLocation);
    if (!MapReduceTaskContextProvider.isLocal(mapredConf)) {
      Location pluginArchive=createPluginArchive(tempLocation);
      if (pluginArchive != null) {
        job.addCacheArchive(pluginArchive.toURI());
        mapredConf.set(Constants.Plugin.ARCHIVE,pluginArchive.getName());
      }
    }
    TaskType.MAP.setResources(mapredConf,context.getMapperResources());
    TaskType.REDUCE.setResources(mapredConf,context.getReducerResources());
    MapperWrapper.wrap(job);
    ReducerWrapper.wrap(job);
    PartitionerWrapper.wrap(job);
    RawComparatorWrapper.CombinerGroupComparatorWrapper.wrap(job);
    RawComparatorWrapper.GroupComparatorWrapper.wrap(job);
    RawComparatorWrapper.KeyComparatorWrapper.wrap(job);
    File jobJar=buildJobJar(job,tempDir);
    job.setJar(jobJar.toURI().toString());
    Location programJar=programJarLocation;
    String hbaseDDLExecutorDirectory=null;
    if (!MapReduceTaskContextProvider.isLocal(mapredConf)) {
      programJar=copyProgramJar(tempLocation);
      job.addCacheFile(programJar.toURI());
      Location launcherJar=createLauncherJar(tempLocation);
      job.addCacheFile(launcherJar.toURI());
      List<String> classpath=new ArrayList<>();
      classpath.add(launcherJar.getName());
      Location logbackLocation=ProgramRunners.createLogbackJar(tempLocation.append(""String_Node_Str""));
      if (logbackLocation != null) {
        job.addCacheFile(logbackLocation.toURI());
        classpath.add(logbackLocation.getName());
        mapredConf.set(""String_Node_Str"",""String_Node_Str"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR);
        mapredConf.set(""String_Node_Str"",""String_Node_Str"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR);
        mapredConf.set(""String_Node_Str"",""String_Node_Str"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR);
      }
      List<String> jarFiles=new ArrayList<>();
      try (JarFile jobJarFile=new JarFile(jobJar)){
        Enumeration<JarEntry> entries=jobJarFile.entries();
        while (entries.hasMoreElements()) {
          JarEntry entry=entries.nextElement();
          if (entry.getName().startsWith(""String_Node_Str"") && entry.getName().endsWith(""String_Node_Str"")) {
            jarFiles.add(""String_Node_Str"" + entry.getName());
          }
        }
      }
       Collections.sort(jarFiles);
      classpath.addAll(jarFiles);
      classpath.add(""String_Node_Str"");
      for (      URI jarURI : CConfigurationUtil.getExtraJars(cConf)) {
        if (""String_Node_Str"".equals(jarURI.getScheme())) {
          Location extraJarLocation=copyFileToLocation(new File(jarURI.getPath()),tempLocation);
          job.addCacheFile(extraJarLocation.toURI());
        }
 else {
          job.addCacheFile(jarURI);
        }
        classpath.add(LocalizationUtils.getLocalizedName(jarURI));
      }
      hbaseDDLExecutorDirectory=getLocalizedHBaseDDLExecutorDir(tempDir,cConf,job,tempLocation);
      MapReduceContainerHelper.addMapReduceClassPath(mapredConf,classpath);
      mapredConf.set(MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(classpath));
      mapredConf.set(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(classpath));
    }
    MapReduceContextConfig contextConfig=new MapReduceContextConfig(mapredConf);
    Transaction tx=txClient.startLong();
    try {
      CConfiguration cConfCopy=CConfiguration.copy(cConf);
      if (hbaseDDLExecutorDirectory != null) {
        cConfCopy.set(Constants.HBaseDDLExecutor.EXTENSIONS_DIR,hbaseDDLExecutorDirectory);
      }
      contextConfig.set(context,cConfCopy,tx,programJar.toURI(),localizedUserResources);
      job.submit();
      LOG.debug(""String_Node_Str"",context);
      this.job=job;
      this.transaction=tx;
    }
 catch (    Throwable t) {
      Transactions.invalidateQuietly(txClient,tx);
      throw t;
    }
  }
 catch (  LinkageError e) {
    throw new Exception(e.getMessage(),e);
  }
catch (  Throwable t) {
    cleanupTask.run();
    if (t instanceof TransactionFailureException) {
      throw Transactions.propagate((TransactionFailureException)t,Exception.class);
    }
    throw t;
  }
}"
4862,"/** 
 * Starts the   {@link TwillApplication} for the master services.
 * @return The {@link TwillController} for the application.
 */
private TwillController startTwillApplication(TwillRunnerService twillRunner,ServiceStore serviceStore,TokenSecureStoreRenewer secureStoreRenewer){
  try {
    Path tempPath=Files.createDirectories(Paths.get(cConf.get(Constants.CFG_LOCAL_DATA_DIR),cConf.get(Constants.AppFabric.TEMP_DIR)).toAbsolutePath());
    final Path runDir=Files.createTempDirectory(tempPath,""String_Node_Str"");
    try {
      Path logbackFile=saveLogbackConf(runDir.resolve(""String_Node_Str""));
      MasterTwillApplication masterTwillApp=new MasterTwillApplication(cConf,getServiceInstances(serviceStore,cConf));
      List<String> extraClassPath=masterTwillApp.prepareLocalizeResource(runDir,hConf);
      TwillPreparer preparer=twillRunner.prepare(masterTwillApp);
      if (!cConf.getBoolean(Constants.COLLECT_CONTAINER_LOGS)) {
        preparer.withConfiguration(Collections.singletonMap(Configs.Keys.LOG_COLLECTION_ENABLED,""String_Node_Str""));
      }
      if (Files.exists(logbackFile)) {
        preparer.withResources(logbackFile.toUri()).withEnv(Collections.singletonMap(""String_Node_Str"",ApplicationConstants.LOG_DIR_EXPANSION_VAR));
      }
      String queueName=cConf.get(Constants.Service.SCHEDULER_QUEUE);
      if (queueName != null) {
        LOG.info(""String_Node_Str"",queueName);
        preparer.setSchedulerQueue(queueName);
      }
      preparer.withDependencies(injector.getInstance(HBaseTableUtil.class).getClass());
      Class<? extends HBaseDDLExecutor> ddlExecutorClass=new HBaseDDLExecutorFactory(cConf,hConf).get().getClass();
      preparer.withDependencies(ddlExecutorClass);
      if (User.isHBaseSecurityEnabled(hConf) || UserGroupInformation.isSecurityEnabled()) {
        preparer.addSecureStore(YarnSecureStore.create(secureStoreRenewer.createCredentials()));
      }
      List<String> yarnAppClassPath=Arrays.asList(hConf.getTrimmedStrings(YarnConfiguration.YARN_APPLICATION_CLASSPATH,YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
      preparer.withApplicationClassPaths(yarnAppClassPath).withBundlerClassAcceptor(new HadoopClassExcluder());
      boolean yarnFirst=cConf.getBoolean(Constants.Explore.CONTAINER_YARN_APP_CLASSPATH_FIRST);
      if (yarnFirst) {
        preparer=preparer.withClassPaths(Iterables.concat(yarnAppClassPath,extraClassPath));
      }
 else {
        preparer=preparer.withClassPaths(extraClassPath);
      }
      if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED)) {
        prepareExploreContainer(preparer);
      }
      preparer.setClassLoader(MainClassLoader.class.getName());
      ClassLoader oldClassLoader=ClassLoaders.setContextClassLoader(new CombineClassLoader(Objects.firstNonNull(Thread.currentThread().getContextClassLoader(),getClass().getClassLoader()),Collections.singleton(ddlExecutorClass.getClassLoader())));
      TwillController controller;
      try {
        controller=preparer.start(cConf.getLong(Constants.AppFabric.PROGRAM_MAX_START_SECONDS),TimeUnit.SECONDS);
      }
  finally {
        ClassLoaders.setContextClassLoader(oldClassLoader);
      }
      Runnable cleanup=new Runnable(){
        @Override public void run(){
          try {
            File dir=runDir.toFile();
            if (dir.isDirectory()) {
              DirUtils.deleteDirectoryContents(dir);
            }
          }
 catch (          IOException e) {
            LOG.warn(""String_Node_Str"",runDir,e);
          }
        }
      }
;
      controller.onRunning(cleanup,Threads.SAME_THREAD_EXECUTOR);
      controller.onTerminated(cleanup,Threads.SAME_THREAD_EXECUTOR);
      return controller;
    }
 catch (    Exception e) {
      try {
        DirUtils.deleteDirectoryContents(runDir.toFile());
      }
 catch (      IOException ex) {
        LOG.warn(""String_Node_Str"",runDir,ex);
        e.addSuppressed(ex);
      }
      throw e;
    }
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}","/** 
 * Starts the   {@link TwillApplication} for the master services.
 * @return The {@link TwillController} for the application.
 */
private TwillController startTwillApplication(TwillRunnerService twillRunner,ServiceStore serviceStore,TokenSecureStoreRenewer secureStoreRenewer){
  try {
    Path tempPath=Files.createDirectories(Paths.get(cConf.get(Constants.CFG_LOCAL_DATA_DIR),cConf.get(Constants.AppFabric.TEMP_DIR)).toAbsolutePath());
    final Path runDir=Files.createTempDirectory(tempPath,""String_Node_Str"");
    try {
      Path logbackFile=saveLogbackConf(runDir.resolve(""String_Node_Str""));
      MasterTwillApplication masterTwillApp=new MasterTwillApplication(cConf,getServiceInstances(serviceStore,cConf));
      List<String> extraClassPath=masterTwillApp.prepareLocalizeResource(runDir,hConf);
      TwillPreparer preparer=twillRunner.prepare(masterTwillApp);
      if (!cConf.getBoolean(Constants.COLLECT_CONTAINER_LOGS)) {
        preparer.withConfiguration(Collections.singletonMap(Configs.Keys.LOG_COLLECTION_ENABLED,""String_Node_Str""));
      }
      if (Files.exists(logbackFile)) {
        preparer.withResources(logbackFile.toUri()).withEnv(Collections.singletonMap(""String_Node_Str"",ApplicationConstants.LOG_DIR_EXPANSION_VAR));
      }
      String queueName=cConf.get(Constants.Service.SCHEDULER_QUEUE);
      if (queueName != null) {
        LOG.info(""String_Node_Str"",queueName);
        preparer.setSchedulerQueue(queueName);
      }
      preparer.withDependencies(injector.getInstance(HBaseTableUtil.class).getClass());
      if (User.isHBaseSecurityEnabled(hConf) || UserGroupInformation.isSecurityEnabled()) {
        preparer.addSecureStore(YarnSecureStore.create(secureStoreRenewer.createCredentials()));
      }
      List<String> yarnAppClassPath=Arrays.asList(hConf.getTrimmedStrings(YarnConfiguration.YARN_APPLICATION_CLASSPATH,YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
      preparer.withApplicationClassPaths(yarnAppClassPath).withBundlerClassAcceptor(new HadoopClassExcluder());
      boolean yarnFirst=cConf.getBoolean(Constants.Explore.CONTAINER_YARN_APP_CLASSPATH_FIRST);
      if (yarnFirst) {
        preparer=preparer.withClassPaths(Iterables.concat(yarnAppClassPath,extraClassPath));
      }
 else {
        preparer=preparer.withClassPaths(extraClassPath);
      }
      if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED)) {
        prepareExploreContainer(preparer);
      }
      preparer.setClassLoader(MainClassLoader.class.getName());
      TwillController controller=preparer.start(cConf.getLong(Constants.AppFabric.PROGRAM_MAX_START_SECONDS),TimeUnit.SECONDS);
      Runnable cleanup=new Runnable(){
        @Override public void run(){
          try {
            File dir=runDir.toFile();
            if (dir.isDirectory()) {
              DirUtils.deleteDirectoryContents(dir);
            }
          }
 catch (          IOException e) {
            LOG.warn(""String_Node_Str"",runDir,e);
          }
        }
      }
;
      controller.onRunning(cleanup,Threads.SAME_THREAD_EXECUTOR);
      controller.onTerminated(cleanup,Threads.SAME_THREAD_EXECUTOR);
      return controller;
    }
 catch (    Exception e) {
      try {
        DirUtils.deleteDirectoryContents(runDir.toFile());
      }
 catch (      IOException ex) {
        LOG.warn(""String_Node_Str"",runDir,ex);
        e.addSuppressed(ex);
      }
      throw e;
    }
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}"
4863,"/** 
 * Prepares the resources that need to be localized to service containers.
 * @param tempDir a temporary directory for creating files to be localized
 * @param hConf the hadoop configuration
 * @return a list of extra classpath that need to be added to each container.
 * @throws IOException if failed to prepare localize resources
 */
List<String> prepareLocalizeResource(Path tempDir,Configuration hConf) throws IOException {
  CConfiguration containerCConf=CConfiguration.copy(cConf);
  containerCConf.set(Constants.CFG_LOCAL_DATA_DIR,""String_Node_Str"");
  List<String> extraClassPath=new ArrayList<>();
  prepareLogSaverResources(tempDir,containerCConf,runnableLocalizeResources.get(Constants.Service.LOGSAVER),extraClassPath);
  if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED)) {
    prepareExploreResources(tempDir,hConf,runnableLocalizeResources.get(Constants.Service.EXPLORE_HTTP_USER_SERVICE),extraClassPath);
  }
  Path cConfPath=saveCConf(containerCConf,Files.createTempFile(tempDir,""String_Node_Str"",""String_Node_Str""));
  Path hConfPath=saveHConf(hConf,Files.createTempFile(tempDir,""String_Node_Str"",""String_Node_Str""));
  for (  String service : ALL_SERVICES) {
    Map<String,LocalizeResource> localizeResources=runnableLocalizeResources.get(service);
    localizeResources.put(CCONF_NAME,new LocalizeResource(cConfPath.toFile(),false));
    localizeResources.put(HCONF_NAME,new LocalizeResource(hConfPath.toFile(),false));
  }
  return extraClassPath;
}","/** 
 * Prepares the resources that need to be localized to service containers.
 * @param tempDir a temporary directory for creating files to be localized
 * @param hConf the hadoop configuration
 * @return a list of extra classpath that need to be added to each container.
 * @throws IOException if failed to prepare localize resources
 */
List<String> prepareLocalizeResource(Path tempDir,Configuration hConf) throws IOException {
  CConfiguration containerCConf=CConfiguration.copy(cConf);
  containerCConf.set(Constants.CFG_LOCAL_DATA_DIR,""String_Node_Str"");
  List<String> extraClassPath=new ArrayList<>();
  prepareLogSaverResources(tempDir,containerCConf,runnableLocalizeResources.get(Constants.Service.LOGSAVER),extraClassPath);
  prepareHBaseDDLExecutorResources(tempDir,containerCConf,runnableLocalizeResources.get(Constants.Service.DATASET_EXECUTOR));
  if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED)) {
    prepareExploreResources(tempDir,hConf,runnableLocalizeResources.get(Constants.Service.EXPLORE_HTTP_USER_SERVICE),extraClassPath);
  }
  Path cConfPath=saveCConf(containerCConf,Files.createTempFile(tempDir,""String_Node_Str"",""String_Node_Str""));
  Path hConfPath=saveHConf(hConf,Files.createTempFile(tempDir,""String_Node_Str"",""String_Node_Str""));
  for (  String service : ALL_SERVICES) {
    Map<String,LocalizeResource> localizeResources=runnableLocalizeResources.get(service);
    localizeResources.put(CCONF_NAME,new LocalizeResource(cConfPath.toFile(),false));
    localizeResources.put(HCONF_NAME,new LocalizeResource(hConfPath.toFile(),false));
  }
  return extraClassPath;
}"
4864,"@Override protected void startUp() throws Exception {
  Reflections.visit(spark,spark.getClass(),new PropertyFieldSetter(runtimeContext.getSparkSpecification().getProperties()),new DataSetFieldSetter(runtimeContext.getDatasetCache()),new MetricsFieldSetter(runtimeContext));
  File tempDir=DirUtils.createTempDir(new File(cConf.get(Constants.CFG_LOCAL_DATA_DIR),cConf.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile());
  tempDir.mkdirs();
  this.cleanupTask=createCleanupTask(tempDir,System.getProperties());
  try {
    initialize();
    SparkRuntimeContextConfig contextConfig=new SparkRuntimeContextConfig(runtimeContext.getConfiguration());
    final File jobJar=generateJobJar(tempDir,contextConfig.isLocal(),cConf);
    final List<LocalizeResource> localizeResources=new ArrayList<>();
    String metricsConfPath;
    String classpath=""String_Node_Str"";
    Properties sparkDefaultConf=SparkPackageUtils.getSparkDefaultConf();
    for (    String key : sparkDefaultConf.stringPropertyNames()) {
      SparkRuntimeEnv.setProperty(key,sparkDefaultConf.getProperty(key));
    }
    if (contextConfig.isLocal()) {
      copyUserResources(context.getLocalizeResources(),tempDir);
      File metricsConf=SparkMetricsSink.writeConfig(new File(tempDir,CDAP_METRICS_PROPERTIES));
      metricsConfPath=metricsConf.getAbsolutePath();
    }
 else {
      distributedUserResources(context.getLocalizeResources(),localizeResources);
      File programJar=Locations.linkOrCopy(runtimeContext.getProgram().getJarLocation(),new File(tempDir,SparkRuntimeContextProvider.PROGRAM_JAR_NAME));
      File expandedProgramJar=Locations.linkOrCopy(runtimeContext.getProgram().getJarLocation(),new File(tempDir,SparkRuntimeContextProvider.PROGRAM_JAR_EXPANDED_NAME));
      localizeResources.add(new LocalizeResource(programJar));
      localizeResources.add(new LocalizeResource(expandedProgramJar,true));
      if (pluginArchive != null) {
        localizeResources.add(new LocalizeResource(pluginArchive,true));
      }
      localizeResources.add(new LocalizeResource(createLauncherJar(tempDir)));
      File metricsConf=SparkMetricsSink.writeConfig(new File(CDAP_METRICS_PROPERTIES));
      metricsConfPath=metricsConf.getName();
      localizeResources.add(new LocalizeResource(metricsConf));
      localizeResources.add(new LocalizeResource(saveCConf(cConf,tempDir)));
      Configuration hConf=contextConfig.set(runtimeContext,pluginArchive).getConfiguration();
      localizeResources.add(new LocalizeResource(saveHConf(hConf,tempDir)));
      Joiner joiner=Joiner.on(File.pathSeparator).skipNulls();
      File sparkJar=new File(tempDir,CDAP_SPARK_JAR);
      classpath=joiner.join(Iterables.transform(buildDependencyJar(sparkJar),new Function<String,String>(){
        @Override public String apply(        String name){
          return Paths.get(""String_Node_Str"",CDAP_SPARK_JAR,name).toString();
        }
      }
));
      localizeResources.add(new LocalizeResource(sparkJar,true));
      File logbackJar=ProgramRunners.createLogbackJar(new File(tempDir,""String_Node_Str""));
      if (logbackJar != null) {
        localizeResources.add(new LocalizeResource(logbackJar));
        classpath=joiner.join(Paths.get(""String_Node_Str"",logbackJar.getName()),classpath);
      }
      List<String> extraJars=new ArrayList<>();
      for (      URI jarURI : CConfigurationUtil.getExtraJars(cConf)) {
        extraJars.add(Paths.get(""String_Node_Str"",LocalizationUtils.getLocalizedName(jarURI)).toString());
        localizeResources.add(new LocalizeResource(jarURI,false));
      }
      classpath=joiner.join(classpath,joiner.join(extraJars));
    }
    final Map<String,String> configs=createSubmitConfigs(tempDir,metricsConfPath,classpath,context.getLocalizeResources(),contextConfig.isLocal());
    submitSpark=new Callable<ListenableFuture<RunId>>(){
      @Override public ListenableFuture<RunId> call() throws Exception {
        if (!isRunning()) {
          return immediateCancelledFuture();
        }
        return sparkSubmitter.submit(runtimeContext,configs,localizeResources,jobJar,runtimeContext.getRunId());
      }
    }
;
  }
 catch (  LinkageError e) {
    throw new Exception(e.getMessage(),e);
  }
catch (  Throwable t) {
    cleanupTask.run();
    throw t;
  }
}","@Override protected void startUp() throws Exception {
  Reflections.visit(spark,spark.getClass(),new PropertyFieldSetter(runtimeContext.getSparkSpecification().getProperties()),new DataSetFieldSetter(runtimeContext.getDatasetCache()),new MetricsFieldSetter(runtimeContext));
  CConfiguration cConfCopy=CConfiguration.copy(cConf);
  File tempDir=DirUtils.createTempDir(new File(cConfCopy.get(Constants.CFG_LOCAL_DATA_DIR),cConfCopy.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile());
  tempDir.mkdirs();
  this.cleanupTask=createCleanupTask(tempDir,System.getProperties());
  try {
    initialize();
    SparkRuntimeContextConfig contextConfig=new SparkRuntimeContextConfig(runtimeContext.getConfiguration());
    final File jobJar=generateJobJar(tempDir,contextConfig.isLocal(),cConfCopy);
    final List<LocalizeResource> localizeResources=new ArrayList<>();
    String metricsConfPath;
    String classpath=""String_Node_Str"";
    Properties sparkDefaultConf=SparkPackageUtils.getSparkDefaultConf();
    for (    String key : sparkDefaultConf.stringPropertyNames()) {
      SparkRuntimeEnv.setProperty(key,sparkDefaultConf.getProperty(key));
    }
    if (contextConfig.isLocal()) {
      copyUserResources(context.getLocalizeResources(),tempDir);
      File metricsConf=SparkMetricsSink.writeConfig(new File(tempDir,CDAP_METRICS_PROPERTIES));
      metricsConfPath=metricsConf.getAbsolutePath();
    }
 else {
      distributedUserResources(context.getLocalizeResources(),localizeResources);
      File programJar=Locations.linkOrCopy(runtimeContext.getProgram().getJarLocation(),new File(tempDir,SparkRuntimeContextProvider.PROGRAM_JAR_NAME));
      File expandedProgramJar=Locations.linkOrCopy(runtimeContext.getProgram().getJarLocation(),new File(tempDir,SparkRuntimeContextProvider.PROGRAM_JAR_EXPANDED_NAME));
      localizeResources.add(new LocalizeResource(programJar));
      localizeResources.add(new LocalizeResource(expandedProgramJar,true));
      if (pluginArchive != null) {
        localizeResources.add(new LocalizeResource(pluginArchive,true));
      }
      localizeResources.add(new LocalizeResource(createLauncherJar(tempDir)));
      File metricsConf=SparkMetricsSink.writeConfig(new File(CDAP_METRICS_PROPERTIES));
      metricsConfPath=metricsConf.getName();
      localizeResources.add(new LocalizeResource(metricsConf));
      prepareHBaseDDLExecutorResources(tempDir,cConfCopy,localizeResources);
      localizeResources.add(new LocalizeResource(saveCConf(cConfCopy,tempDir)));
      Configuration hConf=contextConfig.set(runtimeContext,pluginArchive).getConfiguration();
      localizeResources.add(new LocalizeResource(saveHConf(hConf,tempDir)));
      Joiner joiner=Joiner.on(File.pathSeparator).skipNulls();
      File sparkJar=new File(tempDir,CDAP_SPARK_JAR);
      classpath=joiner.join(Iterables.transform(buildDependencyJar(sparkJar),new Function<String,String>(){
        @Override public String apply(        String name){
          return Paths.get(""String_Node_Str"",CDAP_SPARK_JAR,name).toString();
        }
      }
));
      localizeResources.add(new LocalizeResource(sparkJar,true));
      File logbackJar=ProgramRunners.createLogbackJar(new File(tempDir,""String_Node_Str""));
      if (logbackJar != null) {
        localizeResources.add(new LocalizeResource(logbackJar));
        classpath=joiner.join(Paths.get(""String_Node_Str"",logbackJar.getName()),classpath);
      }
      List<String> extraJars=new ArrayList<>();
      for (      URI jarURI : CConfigurationUtil.getExtraJars(cConfCopy)) {
        extraJars.add(Paths.get(""String_Node_Str"",LocalizationUtils.getLocalizedName(jarURI)).toString());
        localizeResources.add(new LocalizeResource(jarURI,false));
      }
      classpath=joiner.join(classpath,joiner.join(extraJars));
    }
    final Map<String,String> configs=createSubmitConfigs(tempDir,metricsConfPath,classpath,context.getLocalizeResources(),contextConfig.isLocal());
    submitSpark=new Callable<ListenableFuture<RunId>>(){
      @Override public ListenableFuture<RunId> call() throws Exception {
        if (!isRunning()) {
          return immediateCancelledFuture();
        }
        return sparkSubmitter.submit(runtimeContext,configs,localizeResources,jobJar,runtimeContext.getRunId());
      }
    }
;
  }
 catch (  LinkageError e) {
    throw new Exception(e.getMessage(),e);
  }
catch (  Throwable t) {
    cleanupTask.run();
    throw t;
  }
}"
4865,"@Override public void run(){
  try {
    ImpersonationUtils.doAs(opInfo.getUGI(),new Callable<Void>(){
      @Override public Void call() throws Exception {
        try {
          QueryStatus status=exploreService.fetchStatus(opInfo);
          if (status.getStatus() != QueryStatus.OpStatus.FINISHED && status.getStatus() != QueryStatus.OpStatus.CLOSED && status.getStatus() != QueryStatus.OpStatus.CANCELED && status.getStatus() != QueryStatus.OpStatus.ERROR) {
            LOG.info(""String_Node_Str"",handle.getHandle(),status.getStatus());
            exploreService.cancelInternal(handle);
          }
        }
 catch (        Throwable e) {
          LOG.error(""String_Node_Str"",handle.getHandle(),e);
        }
 finally {
          LOG.debug(""String_Node_Str"",handle);
          try {
            exploreService.closeInternal(handle,opInfo);
          }
 catch (          Throwable e) {
            LOG.error(""String_Node_Str"",handle,e);
          }
        }
        return null;
      }
    }
);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",handle);
  }
}","@Override public void run(){
  try {
    ImpersonationUtils.doAs(opInfo.getUGI(),new Callable<Void>(){
      @Override public Void call() throws Exception {
        try {
          QueryStatus status=exploreService.fetchStatus(opInfo);
          if (status.getStatus() != QueryStatus.OpStatus.FINISHED && status.getStatus() != QueryStatus.OpStatus.CLOSED && status.getStatus() != QueryStatus.OpStatus.CANCELED && status.getStatus() != QueryStatus.OpStatus.ERROR) {
            LOG.info(""String_Node_Str"",handle.getHandle(),status.getStatus());
            exploreService.cancelInternal(handle);
          }
        }
 catch (        Throwable e) {
          LOG.error(""String_Node_Str"",handle.getHandle(),e);
        }
 finally {
          LOG.debug(""String_Node_Str"",handle);
          try {
            exploreService.closeInternal(handle,opInfo);
          }
 catch (          Throwable e) {
            LOG.error(""String_Node_Str"",handle,e);
          }
        }
        return null;
      }
    }
);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",handle);
  }
 finally {
    try {
      if (!UserGroupInformation.getLoginUser().equals(opInfo.getUGI())) {
        FileSystem.closeAllForUGI(opInfo.getUGI());
      }
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",opInfo.getUGI(),e);
    }
  }
}"
4866,"/** 
 * Collects stats from all   {@link OperationalStats}.
 */
private void collectOperationalStats() throws InterruptedException {
  LOG.trace(""String_Node_Str"");
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    if (!isRunning()) {
      return;
    }
    OperationalStats stats=entry.getValue();
    LOG.trace(""String_Node_Str"",stats.getServiceName(),stats.getStatType());
    try {
      stats.collect();
    }
 catch (    Throwable t) {
      Throwables.propagateIfInstanceOf(t,InterruptedException.class);
      Throwable rootCause=Throwables.getRootCause(t);
      if (rootCause instanceof ServiceUnavailableException || rootCause instanceof TException) {
        return;
      }
      if (rootCause instanceof InterruptedException) {
        throw (InterruptedException)rootCause;
      }
      LOG.warn(""String_Node_Str"",stats.getServiceName(),stats.getStatType(),rootCause.getMessage());
    }
  }
}","/** 
 * Collects stats from all   {@link OperationalStats}.
 */
private void collectOperationalStats() throws InterruptedException {
  LOG.trace(""String_Node_Str"");
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    if (!isRunning()) {
      return;
    }
    OperationalStats stats=entry.getValue();
    LOG.trace(""String_Node_Str"",stats.getServiceName(),stats.getStatType());
    try {
      stats.collect();
    }
 catch (    Throwable t) {
      Throwables.propagateIfInstanceOf(t,InterruptedException.class);
      Throwable rootCause=Throwables.getRootCause(t);
      if (rootCause instanceof ServiceUnavailableException || rootCause instanceof TException) {
        return;
      }
      if (rootCause instanceof InterruptedException) {
        throw (InterruptedException)rootCause;
      }
      READ_FAILURE_LOG.warn(""String_Node_Str"",stats.getServiceName(),stats.getStatType(),rootCause.getMessage());
    }
  }
}"
4867,"@Override protected Entry computeNext(){
  if (closed || (maxLimit <= 0)) {
    return endOfData();
  }
  while (scanner.hasNext()) {
    RawMessageTableEntry tableEntry=scanner.next();
    if (skipStartRow != null) {
      byte[] row=skipStartRow;
      skipStartRow=null;
      if (Bytes.equals(row,tableEntry.getKey())) {
        continue;
      }
    }
    MessageFilter.Result status=accept(tableEntry.getTxPtr());
    if (status == MessageFilter.Result.ACCEPT) {
      maxLimit--;
      return new ImmutableMessageTableEntry(tableEntry.getKey(),tableEntry.getPayload(),tableEntry.getTxPtr());
    }
    if (status == MessageFilter.Result.HOLD) {
      break;
    }
  }
  return endOfData();
}","@Override protected RawMessageTableEntry computeNext(){
  if (!entries.hasNext()) {
    return endOfData();
  }
  Entry entry=entries.next();
  if (topicId == null || (!topicId.equals(entry.getTopicId())) || (generation != entry.getGeneration())) {
    topicId=entry.getTopicId();
    generation=entry.getGeneration();
    topic=MessagingUtils.toDataKeyPrefix(topicId,entry.getGeneration());
    rowKey=new byte[topic.length + Bytes.SIZEOF_LONG + Bytes.SIZEOF_SHORT];
  }
  Bytes.putBytes(rowKey,0,topic,0,topic.length);
  Bytes.putLong(rowKey,topic.length,entry.getPublishTimestamp());
  Bytes.putShort(rowKey,topic.length + Bytes.SIZEOF_LONG,entry.getSequenceId());
  byte[] txPtr=null;
  if (entry.isTransactional()) {
    txPtr=Bytes.toBytes(entry.getTransactionWritePointer());
  }
  return tableEntry.set(rowKey,txPtr,entry.getPayload());
}"
4868,"@Override public void store(Iterator<? extends Entry> entries) throws IOException {
  persist(storeIterator.reset(entries));
}","@Override public void store(Iterator<? extends Entry> entries) throws IOException {
  persist(new StoreIterator(entries));
}"
4869,"@Override protected Entry computeNext(){
  if (closed || (!scanner.hasNext())) {
    return endOfData();
  }
  RawPayloadTableEntry entry=scanner.next();
  if (skipFirstRow) {
    skipFirstRow=false;
    if (!scanner.hasNext()) {
      return endOfData();
    }
    entry=scanner.next();
  }
  return new ImmutablePayloadTableEntry(entry.getKey(),entry.getValue());
}","@Override protected RawPayloadTableEntry computeNext(){
  if (!entries.hasNext()) {
    return endOfData();
  }
  Entry entry=entries.next();
  if (topicId == null || (!topicId.equals(entry.getTopicId())) || (generation != entry.getGeneration())) {
    topicId=entry.getTopicId();
    generation=entry.getGeneration();
    topic=MessagingUtils.toDataKeyPrefix(topicId,entry.getGeneration());
    rowKey=new byte[topic.length + (2 * Bytes.SIZEOF_LONG) + Bytes.SIZEOF_SHORT];
  }
  Bytes.putBytes(rowKey,0,topic,0,topic.length);
  Bytes.putLong(rowKey,topic.length,entry.getTransactionWritePointer());
  Bytes.putLong(rowKey,topic.length + Bytes.SIZEOF_LONG,entry.getPayloadWriteTimestamp());
  Bytes.putShort(rowKey,topic.length + (2 * Bytes.SIZEOF_LONG),entry.getPayloadSequenceId());
  return tableEntry.set(rowKey,entry.getPayload());
}"
4870,"@Override public void store(Iterator<? extends Entry> entries) throws IOException {
  persist(storeIterator.reset(entries));
}","@Override public void store(Iterator<? extends Entry> entries) throws IOException {
  persist(new StoreIterator(entries));
}"
4871,"@Override protected void run() throws Exception {
  try {
    Tasks.waitFor(true,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        if (stopping) {
          return true;
        }
        List<ArtifactSummary> artifacts=null;
        try {
          artifacts=artifactRepository.getArtifactSummaries(NamespaceId.SYSTEM,artifactName,Integer.MAX_VALUE,ArtifactSortOrder.DESC);
        }
 catch (        ArtifactNotFoundException ex) {
        }
        return artifacts != null && !artifacts.isEmpty();
      }
    }
,5,TimeUnit.MINUTES,2,TimeUnit.SECONDS,String.format(""String_Node_Str"",artifactName));
    List<ArtifactSummary> artifacts=new ArrayList<>(artifactRepository.getArtifactSummaries(NamespaceId.SYSTEM,artifactName,Integer.MAX_VALUE,ArtifactSortOrder.DESC));
    if (stopping) {
      LOG.debug(""String_Node_Str"",appId.getApplication());
    }
    createAppAndStartProgram(artifacts.remove(0));
  }
 catch (  Exception ex) {
    LOG.warn(""String_Node_Str"",appId,ex);
  }
}","@Override protected void run() throws Exception {
  try {
    Tasks.waitFor(true,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        if (stopping) {
          return true;
        }
        List<ArtifactSummary> artifacts=null;
        try {
          artifacts=artifactRepository.getArtifactSummaries(NamespaceId.SYSTEM,artifactName,Integer.MAX_VALUE,ArtifactSortOrder.DESC);
        }
 catch (        ArtifactNotFoundException ex) {
        }
        return artifacts != null && !artifacts.isEmpty();
      }
    }
,5,TimeUnit.MINUTES,2,TimeUnit.SECONDS,String.format(""String_Node_Str"",artifactName));
    List<ArtifactSummary> artifacts=new ArrayList<>(artifactRepository.getArtifactSummaries(NamespaceId.SYSTEM,artifactName,Integer.MAX_VALUE,ArtifactSortOrder.DESC));
    List<ArtifactSummary> userArtifacts=new ArrayList<>();
    try {
      userArtifacts.addAll(artifactRepository.getArtifactSummaries(appId.getNamespaceId(),artifactName,Integer.MAX_VALUE,ArtifactSortOrder.DESC));
    }
 catch (    ArtifactNotFoundException ex) {
    }
    artifacts.addAll(userArtifacts);
    ArtifactSummary maxSummary=artifacts.get(0);
    for (    ArtifactSummary currentSummary : artifacts) {
      ArtifactVersion currentVersion=new ArtifactVersion(currentSummary.getVersion());
      ArtifactVersion maxVersion=new ArtifactVersion(maxSummary.getVersion());
      if (currentVersion.compareTo(maxVersion) > 0) {
        maxSummary=currentSummary;
      }
    }
    if (stopping) {
      LOG.debug(""String_Node_Str"",appId.getApplication());
    }
    createAppAndStartProgram(maxSummary);
  }
 catch (  Exception ex) {
    LOG.warn(""String_Node_Str"",appId,ex);
  }
}"
4872,"private void createAppAndStartProgram(ArtifactSummary artifactSummary) throws Exception {
  LOG.info(""String_Node_Str"",appId.getApplication(),appConfig);
  applicationLifecycleService.deployApp(appId.getParent(),appId.getApplication(),appId.getVersion(),NamespaceId.SYSTEM.artifact(artifactSummary.getName(),artifactSummary.getVersion()).toId(),appConfig,new DefaultProgramTerminator());
  for (  Map.Entry<ProgramId,Map<String,String>> programEntry : programIdMap.entrySet()) {
    try {
      programLifecycleService.start(programEntry.getKey(),programEntry.getValue(),false);
    }
 catch (    IOException ex) {
      LOG.debug(""String_Node_Str"",programEntry.getKey(),ex.getMessage());
    }
  }
}","private void createAppAndStartProgram(ArtifactSummary artifactSummary) throws Exception {
  LOG.info(""String_Node_Str"",appId.getApplication(),appConfig);
  ArtifactId artifactId=artifactSummary.getScope().equals(ArtifactScope.SYSTEM) ? NamespaceId.SYSTEM.artifact(artifactSummary.getName(),artifactSummary.getVersion()) : appId.getNamespaceId().artifact(artifactSummary.getName(),artifactSummary.getVersion());
  applicationLifecycleService.deployApp(appId.getParent(),appId.getApplication(),appId.getVersion(),artifactId.toId(),appConfig,new DefaultProgramTerminator());
  for (  Map.Entry<ProgramId,Map<String,String>> programEntry : programIdMap.entrySet()) {
    try {
      programLifecycleService.start(programEntry.getKey(),programEntry.getValue(),false);
    }
 catch (    IOException ex) {
      LOG.debug(""String_Node_Str"",programEntry.getKey(),ex.getMessage());
    }
  }
}"
4873,"private boolean fetchAndProcessNotifications(DatasetContext context,MessageFetcher fetcher) throws Exception {
  boolean emptyFetch=true;
  try (CloseableIterator<Message> iterator=fetcher.fetch(NamespaceId.SYSTEM.getNamespace(),topic,100,messageId)){
    LOG.trace(""String_Node_Str"",messageId);
    while (iterator.hasNext() && !stopping) {
      emptyFetch=false;
      Message message=iterator.next();
      Notification notification;
      try {
        notification=GSON.fromJson(new String(message.getPayload(),StandardCharsets.UTF_8),Notification.class);
      }
 catch (      JsonSyntaxException e) {
        LOG.warn(""String_Node_Str"",message.getId(),e);
        messageId=message.getId();
        continue;
      }
      updateJobQueue(context,notification);
      messageId=message.getId();
    }
    if (!emptyFetch) {
      jobQueue.persistSubscriberState(topic,messageId);
    }
  }
 catch (  ServiceUnavailableException|TopicNotFoundException e) {
    SAMPLING_LOG.warn(""String_Node_Str"",e);
    failureCount++;
  }
  return emptyFetch;
}","@Nullable private String fetchAndProcessNotifications(DatasetContext context,MessageFetcher fetcher) throws Exception {
  String lastFetchedMessageId=null;
  try (CloseableIterator<Message> iterator=fetcher.fetch(NamespaceId.SYSTEM.getNamespace(),topic,100,messageId)){
    LOG.trace(""String_Node_Str"",messageId);
    while (iterator.hasNext() && !stopping) {
      Message message=iterator.next();
      Notification notification;
      try {
        notification=GSON.fromJson(new String(message.getPayload(),StandardCharsets.UTF_8),Notification.class);
      }
 catch (      JsonSyntaxException e) {
        LOG.warn(""String_Node_Str"",message.getId(),e);
        lastFetchedMessageId=message.getId();
        continue;
      }
      updateJobQueue(context,notification);
      lastFetchedMessageId=message.getId();
    }
    if (lastFetchedMessageId != null) {
      jobQueue.persistSubscriberState(topic,lastFetchedMessageId);
    }
    return lastFetchedMessageId;
  }
 }"
4874,"/** 
 * Fetch new notifications and update job queue
 * @return sleep time in milliseconds before next fetch
 */
private long processNotifications(){
  boolean emptyFetch=false;
  try {
    final MessageFetcher fetcher=messagingContext.getMessageFetcher();
    emptyFetch=Transactions.execute(transactional,new TxCallable<Boolean>(){
      @Override public Boolean call(      DatasetContext context) throws Exception {
        return fetchAndProcessNotifications(context,fetcher);
      }
    }
);
    failureCount=0;
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
    failureCount++;
  }
  if (failureCount > 0) {
    return scheduleStrategy.nextRetry(failureCount,0);
  }
  return emptyFetch ? 2000L : 0L;
}","/** 
 * Fetch new notifications and update job queue
 * @return sleep time in milliseconds before next fetch
 */
private long processNotifications(){
  try {
    final MessageFetcher fetcher=messagingContext.getMessageFetcher();
    String lastFetchedMessageId=Transactionals.execute(transactional,new TxCallable<String>(){
      @Override public String call(      DatasetContext context) throws Exception {
        return fetchAndProcessNotifications(context,fetcher);
      }
    }
,ServiceUnavailableException.class,TopicNotFoundException.class);
    failureCount=0;
    if (lastFetchedMessageId != null) {
      messageId=lastFetchedMessageId;
      return cConf.getLong(Constants.Scheduler.EVENT_POLL_DELAY_MILLIS);
    }
    return 0L;
  }
 catch (  ServiceUnavailableException e) {
    SAMPLING_LOG.warn(""String_Node_Str"",e.getServiceName(),e);
  }
catch (  TopicNotFoundException e) {
    SAMPLING_LOG.warn(""String_Node_Str"",e);
  }
catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
  }
  return scheduleStrategy.nextRetry(++failureCount,0);
}"
4875,"@Override public Boolean call(DatasetContext context) throws Exception {
  return fetchAndProcessNotifications(context,fetcher);
}","@Override public String call(DatasetContext context) throws Exception {
  return fetchAndProcessNotifications(context,fetcher);
}"
4876,"@Override protected void startUp(){
  LOG.info(""String_Node_Str"");
  taskExecutorService=MoreExecutors.listeningDecorator(Executors.newCachedThreadPool(new ThreadFactoryBuilder().setNameFormat(""String_Node_Str"").build()));
  taskExecutorService.submit(new SchedulerEventNotificationSubscriberThread(cConf.get(Constants.Scheduler.TIME_EVENT_TOPIC)));
  taskExecutorService.submit(new SchedulerEventNotificationSubscriberThread(cConf.get(Constants.Scheduler.STREAM_SIZE_EVENT_TOPIC)));
  taskExecutorService.submit(new DataEventNotificationSubscriberThread());
}","@Override protected void startUp(){
  LOG.info(""String_Node_Str"");
  taskExecutorService=Executors.newCachedThreadPool(new ThreadFactoryBuilder().setNameFormat(""String_Node_Str"").build());
  taskExecutorService.submit(new SchedulerEventNotificationSubscriberThread(cConf.get(Constants.Scheduler.TIME_EVENT_TOPIC)));
  taskExecutorService.submit(new SchedulerEventNotificationSubscriberThread(cConf.get(Constants.Scheduler.STREAM_SIZE_EVENT_TOPIC)));
  taskExecutorService.submit(new DataEventNotificationSubscriberThread());
}"
4877,"private String loadMessageId(){
  try {
    return Transactions.execute(transactional,new TxCallable<String>(){
      @Override public String call(      DatasetContext context) throws Exception {
        return jobQueue.retrieveSubscriberState(topic);
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    throw Throwables.propagate(e);
  }
}","private String loadMessageId(){
  return Transactionals.execute(transactional,new TxCallable<String>(){
    @Override public String call(    DatasetContext context) throws Exception {
      return jobQueue.retrieveSubscriberState(topic);
    }
  }
);
}"
4878,"protected static CConfiguration createBasicCConf() throws IOException {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Service.MASTER_SERVICES_BIND_ADDRESS,hostname);
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder(""String_Node_Str"").getAbsolutePath());
  cConf.set(Constants.AppFabric.OUTPUT_DIR,System.getProperty(""String_Node_Str""));
  cConf.set(Constants.AppFabric.TEMP_DIR,System.getProperty(""String_Node_Str""));
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  String updateSchedules=System.getProperty(Constants.AppFabric.APP_UPDATE_SCHEDULES);
  if (updateSchedules != null) {
    cConf.set(Constants.AppFabric.APP_UPDATE_SCHEDULES,updateSchedules);
  }
  return cConf;
}","protected static CConfiguration createBasicCConf() throws IOException {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Service.MASTER_SERVICES_BIND_ADDRESS,hostname);
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder(""String_Node_Str"").getAbsolutePath());
  cConf.set(Constants.AppFabric.OUTPUT_DIR,System.getProperty(""String_Node_Str""));
  cConf.set(Constants.AppFabric.TEMP_DIR,System.getProperty(""String_Node_Str""));
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  String updateSchedules=System.getProperty(Constants.AppFabric.APP_UPDATE_SCHEDULES);
  if (updateSchedules != null) {
    cConf.set(Constants.AppFabric.APP_UPDATE_SCHEDULES,updateSchedules);
  }
  cConf.setLong(Constants.Scheduler.EVENT_POLL_DELAY_MILLIS,100L);
  return cConf;
}"
4879,"@Override protected Entry computeNext(){
  if (closed || (maxLimit <= 0)) {
    return endOfData();
  }
  while (scanner.hasNext()) {
    RawMessageTableEntry tableEntry=scanner.next();
    if (skipStartRow != null) {
      byte[] row=skipStartRow;
      skipStartRow=null;
      if (Bytes.equals(row,tableEntry.getKey())) {
        continue;
      }
    }
    MessageFilter.Result status=accept(tableEntry.getTxPtr());
    if (status == MessageFilter.Result.ACCEPT) {
      maxLimit--;
      return new ImmutableMessageTableEntry(tableEntry.getKey(),tableEntry.getPayload(),tableEntry.getTxPtr());
    }
    if (status == MessageFilter.Result.HOLD) {
      break;
    }
  }
  return endOfData();
}","@Override protected RawMessageTableEntry computeNext(){
  if (!entries.hasNext()) {
    return endOfData();
  }
  Entry entry=entries.next();
  if (topicId == null || (!topicId.equals(entry.getTopicId())) || (generation != entry.getGeneration())) {
    topicId=entry.getTopicId();
    generation=entry.getGeneration();
    topic=MessagingUtils.toDataKeyPrefix(topicId,entry.getGeneration());
    rowKey=new byte[topic.length + Bytes.SIZEOF_LONG + Bytes.SIZEOF_SHORT];
  }
  Bytes.putBytes(rowKey,0,topic,0,topic.length);
  Bytes.putLong(rowKey,topic.length,entry.getPublishTimestamp());
  Bytes.putShort(rowKey,topic.length + Bytes.SIZEOF_LONG,entry.getSequenceId());
  byte[] txPtr=null;
  if (entry.isTransactional()) {
    txPtr=Bytes.toBytes(entry.getTransactionWritePointer());
  }
  return tableEntry.set(rowKey,txPtr,entry.getPayload());
}"
4880,"@Override public void store(Iterator<? extends Entry> entries) throws IOException {
  persist(storeIterator.reset(entries));
}","@Override public void store(Iterator<? extends Entry> entries) throws IOException {
  persist(new StoreIterator(entries));
}"
4881,"@Override protected Entry computeNext(){
  if (closed || (!scanner.hasNext())) {
    return endOfData();
  }
  RawPayloadTableEntry entry=scanner.next();
  if (skipFirstRow) {
    skipFirstRow=false;
    if (!scanner.hasNext()) {
      return endOfData();
    }
    entry=scanner.next();
  }
  return new ImmutablePayloadTableEntry(entry.getKey(),entry.getValue());
}","@Override protected RawPayloadTableEntry computeNext(){
  if (!entries.hasNext()) {
    return endOfData();
  }
  Entry entry=entries.next();
  if (topicId == null || (!topicId.equals(entry.getTopicId())) || (generation != entry.getGeneration())) {
    topicId=entry.getTopicId();
    generation=entry.getGeneration();
    topic=MessagingUtils.toDataKeyPrefix(topicId,entry.getGeneration());
    rowKey=new byte[topic.length + (2 * Bytes.SIZEOF_LONG) + Bytes.SIZEOF_SHORT];
  }
  Bytes.putBytes(rowKey,0,topic,0,topic.length);
  Bytes.putLong(rowKey,topic.length,entry.getTransactionWritePointer());
  Bytes.putLong(rowKey,topic.length + Bytes.SIZEOF_LONG,entry.getPayloadWriteTimestamp());
  Bytes.putShort(rowKey,topic.length + (2 * Bytes.SIZEOF_LONG),entry.getPayloadSequenceId());
  return tableEntry.set(rowKey,entry.getPayload());
}"
4882,"@Override public void store(Iterator<? extends Entry> entries) throws IOException {
  persist(storeIterator.reset(entries));
}","@Override public void store(Iterator<? extends Entry> entries) throws IOException {
  persist(new StoreIterator(entries));
}"
4883,"private boolean fetchAndProcessNotifications(DatasetContext context,MessageFetcher fetcher) throws Exception {
  boolean emptyFetch=true;
  try (CloseableIterator<Message> iterator=fetcher.fetch(NamespaceId.SYSTEM.getNamespace(),topic,100,messageId)){
    LOG.trace(""String_Node_Str"",messageId);
    while (iterator.hasNext() && !stopping) {
      emptyFetch=false;
      Message message=iterator.next();
      Notification notification;
      try {
        notification=GSON.fromJson(new String(message.getPayload(),StandardCharsets.UTF_8),Notification.class);
      }
 catch (      JsonSyntaxException e) {
        LOG.warn(""String_Node_Str"",message.getId(),e);
        messageId=message.getId();
        continue;
      }
      updateJobQueue(context,notification);
      messageId=message.getId();
    }
    if (!emptyFetch) {
      jobQueue.persistSubscriberState(topic,messageId);
    }
  }
 catch (  ServiceUnavailableException|TopicNotFoundException e) {
    SAMPLING_LOG.warn(""String_Node_Str"",e);
    failureCount++;
  }
  return emptyFetch;
}","@Nullable private String fetchAndProcessNotifications(DatasetContext context,MessageFetcher fetcher) throws Exception {
  String lastFetchedMessageId=null;
  try (CloseableIterator<Message> iterator=fetcher.fetch(NamespaceId.SYSTEM.getNamespace(),topic,100,messageId)){
    LOG.trace(""String_Node_Str"",messageId);
    while (iterator.hasNext() && !stopping) {
      Message message=iterator.next();
      Notification notification;
      try {
        notification=GSON.fromJson(new String(message.getPayload(),StandardCharsets.UTF_8),Notification.class);
      }
 catch (      JsonSyntaxException e) {
        LOG.warn(""String_Node_Str"",message.getId(),e);
        lastFetchedMessageId=message.getId();
        continue;
      }
      updateJobQueue(context,notification);
      lastFetchedMessageId=message.getId();
    }
    if (lastFetchedMessageId != null) {
      jobQueue.persistSubscriberState(topic,lastFetchedMessageId);
    }
    return lastFetchedMessageId;
  }
 }"
4884,"/** 
 * Fetch new notifications and update job queue
 * @return sleep time in milliseconds before next fetch
 */
private long processNotifications(){
  boolean emptyFetch=false;
  try {
    final MessageFetcher fetcher=messagingContext.getMessageFetcher();
    emptyFetch=Transactions.execute(transactional,new TxCallable<Boolean>(){
      @Override public Boolean call(      DatasetContext context) throws Exception {
        return fetchAndProcessNotifications(context,fetcher);
      }
    }
);
    failureCount=0;
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
    failureCount++;
  }
  if (failureCount > 0) {
    return scheduleStrategy.nextRetry(failureCount,0);
  }
  return emptyFetch ? 2000L : 0L;
}","/** 
 * Fetch new notifications and update job queue
 * @return sleep time in milliseconds before next fetch
 */
private long processNotifications(){
  try {
    final MessageFetcher fetcher=messagingContext.getMessageFetcher();
    String lastFetchedMessageId=Transactionals.execute(transactional,new TxCallable<String>(){
      @Override public String call(      DatasetContext context) throws Exception {
        return fetchAndProcessNotifications(context,fetcher);
      }
    }
,ServiceUnavailableException.class,TopicNotFoundException.class);
    failureCount=0;
    if (lastFetchedMessageId != null) {
      messageId=lastFetchedMessageId;
      return cConf.getLong(Constants.Scheduler.EVENT_POLL_DELAY_MILLIS);
    }
    return 0L;
  }
 catch (  ServiceUnavailableException e) {
    SAMPLING_LOG.warn(""String_Node_Str"",e.getServiceName(),e);
  }
catch (  TopicNotFoundException e) {
    SAMPLING_LOG.warn(""String_Node_Str"",e);
  }
catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
  }
  return scheduleStrategy.nextRetry(++failureCount,0);
}"
4885,"@Override public Boolean call(DatasetContext context) throws Exception {
  return fetchAndProcessNotifications(context,fetcher);
}","@Override public String call(DatasetContext context) throws Exception {
  return fetchAndProcessNotifications(context,fetcher);
}"
4886,"@Override protected void startUp(){
  LOG.info(""String_Node_Str"");
  taskExecutorService=MoreExecutors.listeningDecorator(Executors.newCachedThreadPool(new ThreadFactoryBuilder().setNameFormat(""String_Node_Str"").build()));
  taskExecutorService.submit(new SchedulerEventNotificationSubscriberThread(cConf.get(Constants.Scheduler.TIME_EVENT_TOPIC)));
  taskExecutorService.submit(new SchedulerEventNotificationSubscriberThread(cConf.get(Constants.Scheduler.STREAM_SIZE_EVENT_TOPIC)));
  taskExecutorService.submit(new DataEventNotificationSubscriberThread());
}","@Override protected void startUp(){
  LOG.info(""String_Node_Str"");
  taskExecutorService=Executors.newCachedThreadPool(new ThreadFactoryBuilder().setNameFormat(""String_Node_Str"").build());
  taskExecutorService.submit(new SchedulerEventNotificationSubscriberThread(cConf.get(Constants.Scheduler.TIME_EVENT_TOPIC)));
  taskExecutorService.submit(new SchedulerEventNotificationSubscriberThread(cConf.get(Constants.Scheduler.STREAM_SIZE_EVENT_TOPIC)));
  taskExecutorService.submit(new DataEventNotificationSubscriberThread());
}"
4887,"private String loadMessageId(){
  try {
    return Transactions.execute(transactional,new TxCallable<String>(){
      @Override public String call(      DatasetContext context) throws Exception {
        return jobQueue.retrieveSubscriberState(topic);
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    throw Throwables.propagate(e);
  }
}","private String loadMessageId(){
  return Transactionals.execute(transactional,new TxCallable<String>(){
    @Override public String call(    DatasetContext context) throws Exception {
      return jobQueue.retrieveSubscriberState(topic);
    }
  }
);
}"
4888,"protected static CConfiguration createBasicCConf() throws IOException {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Service.MASTER_SERVICES_BIND_ADDRESS,hostname);
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder(""String_Node_Str"").getAbsolutePath());
  cConf.set(Constants.AppFabric.OUTPUT_DIR,System.getProperty(""String_Node_Str""));
  cConf.set(Constants.AppFabric.TEMP_DIR,System.getProperty(""String_Node_Str""));
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  String updateSchedules=System.getProperty(Constants.AppFabric.APP_UPDATE_SCHEDULES);
  if (updateSchedules != null) {
    cConf.set(Constants.AppFabric.APP_UPDATE_SCHEDULES,updateSchedules);
  }
  return cConf;
}","protected static CConfiguration createBasicCConf() throws IOException {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Service.MASTER_SERVICES_BIND_ADDRESS,hostname);
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder(""String_Node_Str"").getAbsolutePath());
  cConf.set(Constants.AppFabric.OUTPUT_DIR,System.getProperty(""String_Node_Str""));
  cConf.set(Constants.AppFabric.TEMP_DIR,System.getProperty(""String_Node_Str""));
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  String updateSchedules=System.getProperty(Constants.AppFabric.APP_UPDATE_SCHEDULES);
  if (updateSchedules != null) {
    cConf.set(Constants.AppFabric.APP_UPDATE_SCHEDULES,updateSchedules);
  }
  cConf.setLong(Constants.Scheduler.EVENT_POLL_DELAY_MILLIS,100L);
  return cConf;
}"
4889,"@Override public void upgrade() throws Exception {
  int numThreads=cConf.getInt(Constants.Upgrade.UPGRADE_THREAD_POOL_SIZE);
  ExecutorService executor=Executors.newFixedThreadPool(numThreads,new ThreadFactoryBuilder().setNameFormat(""String_Node_Str"").setDaemon(true).build());
  try {
    upgradeSystemDatasets(executor);
    upgradeUserTables(executor);
  }
  finally {
    executor.shutdownNow();
  }
}","@Override public void upgrade() throws Exception {
  int numThreads=cConf.getInt(Constants.Upgrade.UPGRADE_THREAD_POOL_SIZE);
  ExecutorService executor=Executors.newFixedThreadPool(numThreads,new ThreadFactoryBuilder().setThreadFactory(Executors.privilegedThreadFactory()).setNameFormat(""String_Node_Str"").setDaemon(true).build());
  try {
    upgradeSystemDatasets(executor);
    upgradeUserTables(executor);
  }
  finally {
    executor.shutdownNow();
  }
}"
4890,"@Override public T call() throws Exception {
  return callable.call();
}","@Override public T call() throws Exception {
  return impersonator.doAs(namespaceMeta.getNamespaceId(),callable);
}"
4891,"/** 
 * Creates a   {@link Callable} which will impersonate the specified namespace before executing the specified callable.This is useful in case the impersonation needs to be delayed or will be performed later.
 */
public static <T>Callable<T> createImpersonatingCallable(final Impersonator impersonator,final NamespaceMeta namespaceMeta,final Callable<T> callable){
  return new Callable<T>(){
    @Override public T call() throws Exception {
      return impersonator.doAs(namespaceMeta.getNamespaceId(),new Callable<T>(){
        @Override public T call() throws Exception {
          return callable.call();
        }
      }
);
    }
  }
;
}","/** 
 * Creates a   {@link Callable} which will impersonate the specified namespace before executing the specified callable.This is useful in case the impersonation needs to be delayed or will be performed later.
 */
public static <T>Callable<T> createImpersonatingCallable(final Impersonator impersonator,final NamespaceMeta namespaceMeta,final Callable<T> callable){
  return new Callable<T>(){
    @Override public T call() throws Exception {
      return impersonator.doAs(namespaceMeta.getNamespaceId(),callable);
    }
  }
;
}"
4892,"@Override protected void configure(){
  setName(NAME);
  setDescription(DESCRIPTION);
  Map<String,String> properties=new HashMap<>();
  properties.put(Constants.PIPELINE_SPEC_KEY,GSON.toJson(spec));
  setProperties(properties);
  stageSpecs=new HashMap<>();
  useSpark=engine == Engine.SPARK;
  if (!useSpark) {
    for (    StageSpec stageSpec : spec.getStages()) {
      stageSpecs.put(stageSpec.getName(),stageSpec);
      String pluginType=stageSpec.getPlugin().getType();
      if (SparkCompute.PLUGIN_TYPE.equals(pluginType) || SparkSink.PLUGIN_TYPE.equals(pluginType)) {
        useSpark=true;
        break;
      }
    }
  }
  PipelinePlanner planner;
  Set<String> actionTypes=ImmutableSet.of(Action.PLUGIN_TYPE,Constants.SPARK_PROGRAM_PLUGIN_TYPE);
  if (useSpark) {
    planner=new PipelinePlanner(supportedPluginTypes,ImmutableSet.<String>of(),ImmutableSet.<String>of(),actionTypes);
  }
 else {
    planner=new PipelinePlanner(supportedPluginTypes,ImmutableSet.of(BatchAggregator.PLUGIN_TYPE,BatchJoiner.PLUGIN_TYPE),ImmutableSet.of(SparkCompute.PLUGIN_TYPE,SparkSink.PLUGIN_TYPE),actionTypes);
  }
  plan=planner.plan(spec);
  if (plan.getPhases().size() == 1) {
    addProgram(plan.getPhases().keySet().iterator().next(),new TrunkProgramAdder(getConfigurer()));
    return;
  }
  if (plan.getPhaseConnections().isEmpty()) {
    WorkflowProgramAdder programAdder;
    WorkflowForkConfigurer forkConfigurer=getConfigurer().fork();
    programAdder=new BranchProgramAdder(forkConfigurer);
    for (    String phaseName : plan.getPhases().keySet()) {
      addProgram(phaseName,programAdder);
    }
    if (forkConfigurer != null) {
      forkConfigurer.join();
    }
    return;
  }
  dag=new ControlDag(plan.getPhaseConnections());
  dag.flatten();
  String start=dag.getSources().iterator().next();
  addPrograms(start,getConfigurer());
}","@Override protected void configure(){
  setName(NAME);
  setDescription(DESCRIPTION);
  Map<String,String> properties=new HashMap<>();
  properties.put(Constants.PIPELINE_SPEC_KEY,GSON.toJson(spec));
  setProperties(properties);
  stageSpecs=new HashMap<>();
  useSpark=engine == Engine.SPARK;
  for (  StageSpec stageSpec : spec.getStages()) {
    stageSpecs.put(stageSpec.getName(),stageSpec);
    String pluginType=stageSpec.getPlugin().getType();
    if (SparkCompute.PLUGIN_TYPE.equals(pluginType) || SparkSink.PLUGIN_TYPE.equals(pluginType)) {
      useSpark=true;
    }
  }
  PipelinePlanner planner;
  Set<String> actionTypes=ImmutableSet.of(Action.PLUGIN_TYPE,Constants.SPARK_PROGRAM_PLUGIN_TYPE);
  if (useSpark) {
    planner=new PipelinePlanner(supportedPluginTypes,ImmutableSet.<String>of(),ImmutableSet.<String>of(),actionTypes);
  }
 else {
    planner=new PipelinePlanner(supportedPluginTypes,ImmutableSet.of(BatchAggregator.PLUGIN_TYPE,BatchJoiner.PLUGIN_TYPE),ImmutableSet.of(SparkCompute.PLUGIN_TYPE,SparkSink.PLUGIN_TYPE),actionTypes);
  }
  plan=planner.plan(spec);
  if (plan.getPhases().size() == 1) {
    addProgram(plan.getPhases().keySet().iterator().next(),new TrunkProgramAdder(getConfigurer()));
    return;
  }
  if (plan.getPhaseConnections().isEmpty()) {
    WorkflowProgramAdder programAdder;
    WorkflowForkConfigurer forkConfigurer=getConfigurer().fork();
    programAdder=new BranchProgramAdder(forkConfigurer);
    for (    String phaseName : plan.getPhases().keySet()) {
      addProgram(phaseName,programAdder);
    }
    if (forkConfigurer != null) {
      forkConfigurer.join();
    }
    return;
  }
  dag=new ControlDag(plan.getPhaseConnections());
  dag.flatten();
  String start=dag.getSources().iterator().next();
  addPrograms(start,getConfigurer());
}"
4893,"private Map<TableId,Future<?>> upgradeQueues(final NamespaceMeta namespaceMeta,ExecutorService executor) throws Exception {
  try (HBaseAdmin admin=new HBaseAdmin(hConf)){
    String hbaseNamespace=tableUtil.getHBaseNamespace(namespaceMeta);
    List<TableId> tableIds=tableUtil.listTablesInNamespace(admin,hbaseNamespace);
    List<TableId> stateStoreTableIds=Lists.newArrayList();
    Map<TableId,Future<?>> futures=new HashMap<>();
    for (    final TableId tableId : tableIds) {
      if (isDataTable(tableId)) {
        Runnable runnable=new Runnable(){
          public void run(){
            try {
              LOG.info(""String_Node_Str"",tableId);
              Properties properties=new Properties();
              HTableDescriptor desc=tableUtil.getHTableDescriptor(admin,tableId);
              if (desc.getValue(HBaseQueueAdmin.PROPERTY_PREFIX_BYTES) == null) {
                properties.setProperty(HBaseQueueAdmin.PROPERTY_PREFIX_BYTES,Integer.toString(SaltedHBaseQueueStrategy.SALT_BYTES));
              }
              upgrade(tableId,properties);
              LOG.info(""String_Node_Str"",tableId);
            }
 catch (            Exception e) {
              throw new RuntimeException(e);
            }
          }
        }
;
        Future<?> future=executor.submit(runnable);
        futures.put(tableId,future);
      }
 else       if (isStateStoreTable(tableId)) {
        stateStoreTableIds.add(tableId);
      }
    }
    for (    final TableId tableId : stateStoreTableIds) {
      Runnable runnable=new Runnable(){
        public void run(){
          try {
            LOG.info(""String_Node_Str"",tableId);
            DatasetId stateStoreId=createStateStoreDataset(namespaceMeta.getName());
            DatasetAdmin datasetAdmin=datasetFramework.getAdmin(stateStoreId,null);
            if (datasetAdmin == null) {
              LOG.error(""String_Node_Str"",stateStoreId);
              return;
            }
            datasetAdmin.upgrade();
            LOG.info(""String_Node_Str"",tableId);
          }
 catch (          Exception e) {
            new RuntimeException(e);
          }
        }
      }
;
      Future<?> future=executor.submit(runnable);
      futures.put(tableId,future);
    }
    return futures;
  }
 }","private Map<TableId,Future<?>> upgradeQueues(final NamespaceMeta namespaceMeta,ExecutorService executor,final HBaseAdmin admin) throws Exception {
  String hbaseNamespace=tableUtil.getHBaseNamespace(namespaceMeta);
  List<TableId> tableIds=tableUtil.listTablesInNamespace(admin,hbaseNamespace);
  List<TableId> stateStoreTableIds=Lists.newArrayList();
  Map<TableId,Future<?>> futures=new HashMap<>();
  for (  final TableId tableId : tableIds) {
    if (isDataTable(tableId)) {
      Callable<Void> callable=new Callable<Void>(){
        public Void call() throws Exception {
          LOG.info(""String_Node_Str"",tableId);
          Properties properties=new Properties();
          HTableDescriptor desc=tableUtil.getHTableDescriptor(admin,tableId);
          if (desc.getValue(HBaseQueueAdmin.PROPERTY_PREFIX_BYTES) == null) {
            properties.setProperty(HBaseQueueAdmin.PROPERTY_PREFIX_BYTES,Integer.toString(SaltedHBaseQueueStrategy.SALT_BYTES));
          }
          upgrade(tableId,properties);
          LOG.info(""String_Node_Str"",tableId);
          return null;
        }
      }
;
      Future<?> future=executor.submit(ImpersonationUtils.createImpersonatingCallable(impersonator,namespaceMeta,callable));
      futures.put(tableId,future);
    }
 else     if (isStateStoreTable(tableId)) {
      stateStoreTableIds.add(tableId);
    }
  }
  for (  final TableId tableId : stateStoreTableIds) {
    Callable<Void> callable=new Callable<Void>(){
      public Void call() throws Exception {
        LOG.info(""String_Node_Str"",tableId);
        DatasetId stateStoreId=createStateStoreDataset(namespaceMeta.getName());
        DatasetAdmin datasetAdmin=datasetFramework.getAdmin(stateStoreId,null);
        if (datasetAdmin == null) {
          LOG.error(""String_Node_Str"",stateStoreId);
          return null;
        }
        datasetAdmin.upgrade();
        LOG.info(""String_Node_Str"",tableId);
        return null;
      }
    }
;
    Future<?> future=executor.submit(ImpersonationUtils.createImpersonatingCallable(impersonator,namespaceMeta,callable));
    futures.put(tableId,future);
  }
  return futures;
}"
4894,"@Override public Void call() throws Exception {
  Map<TableId,Future<?>> futures=upgradeQueues(namespaceMeta,executor);
  allFutures.putAll(futures);
  return null;
}","public Void call() throws Exception {
  LOG.info(""String_Node_Str"",tableId);
  DatasetId stateStoreId=createStateStoreDataset(namespaceMeta.getName());
  DatasetAdmin datasetAdmin=datasetFramework.getAdmin(stateStoreId,null);
  if (datasetAdmin == null) {
    LOG.error(""String_Node_Str"",stateStoreId);
    return null;
  }
  datasetAdmin.upgrade();
  LOG.info(""String_Node_Str"",tableId);
  return null;
}"
4895,"public void run(){
  try {
    impersonator.doAs(namespaceMeta.getNamespaceId(),new Callable<Void>(){
      @Override public Void call() throws Exception {
        if (isCDAPUserTable(desc)) {
          upgradeUserTable(desc);
        }
 else         if (isStreamOrQueueTable(desc.getNameAsString())) {
          updateTableDesc(desc,ddlExecutor);
        }
        return null;
      }
    }
);
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
}","public void run(){
  try {
    LOG.info(""String_Node_Str"",spec.getName(),spec.toString());
    DatasetAdmin admin=dsFramework.getAdmin(datasetId,null);
    admin.upgrade();
    LOG.info(""String_Node_Str"",spec.getName());
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
}"
4896,"@Override public Void call() throws Exception {
  if (isCDAPUserTable(desc)) {
    upgradeUserTable(desc);
  }
 else   if (isStreamOrQueueTable(desc.getNameAsString())) {
    updateTableDesc(desc,ddlExecutor);
  }
  return null;
}","public Void call() throws Exception {
  if (isCDAPUserTable(desc)) {
    upgradeUserTable(desc);
  }
 else   if (isStreamOrQueueTable(desc.getNameAsString())) {
    updateTableDesc(desc,ddlExecutor);
  }
  return null;
}"
4897,"private Map<String,Future<?>> upgradeUserTables(final NamespaceMeta namespaceMeta,ExecutorService executor) throws Exception {
  Map<String,Future<?>> futures=new HashMap<>();
  String hBaseNamespace=hBaseTableUtil.getHBaseNamespace(namespaceMeta);
  try (HBaseDDLExecutor ddlExecutor=ddlExecutorFactory.get();HBaseAdmin hAdmin=new HBaseAdmin(hConf)){
    for (    final HTableDescriptor desc : hAdmin.listTableDescriptorsByNamespace(HTableNameConverter.encodeHBaseEntity(hBaseNamespace))) {
      Runnable runnable=new Runnable(){
        public void run(){
          try {
            impersonator.doAs(namespaceMeta.getNamespaceId(),new Callable<Void>(){
              @Override public Void call() throws Exception {
                if (isCDAPUserTable(desc)) {
                  upgradeUserTable(desc);
                }
 else                 if (isStreamOrQueueTable(desc.getNameAsString())) {
                  updateTableDesc(desc,ddlExecutor);
                }
                return null;
              }
            }
);
          }
 catch (          Exception e) {
            throw new RuntimeException(e);
          }
        }
      }
;
      Future<?> future=executor.submit(runnable);
      futures.put(desc.getNameAsString(),future);
    }
  }
   return futures;
}","private Map<String,Future<?>> upgradeUserTables(final NamespaceMeta namespaceMeta,final ExecutorService executor,final HBaseDDLExecutor ddlExecutor) throws Exception {
  Map<String,Future<?>> futures=new HashMap<>();
  String hBaseNamespace=hBaseTableUtil.getHBaseNamespace(namespaceMeta);
  try (HBaseAdmin hAdmin=new HBaseAdmin(hConf)){
    for (    final HTableDescriptor desc : hAdmin.listTableDescriptorsByNamespace(HTableNameConverter.encodeHBaseEntity(hBaseNamespace))) {
      Callable<Void> callable=new Callable<Void>(){
        public Void call() throws Exception {
          if (isCDAPUserTable(desc)) {
            upgradeUserTable(desc);
          }
 else           if (isStreamOrQueueTable(desc.getNameAsString())) {
            updateTableDesc(desc,ddlExecutor);
          }
          return null;
        }
      }
;
      Future<?> future=executor.submit(ImpersonationUtils.createImpersonatingCallable(impersonator,namespaceMeta,callable));
      futures.put(desc.getNameAsString(),future);
    }
  }
   return futures;
}"
4898,"@Override protected void configure(){
  setName(NAME);
  setDescription(DESCRIPTION);
  Map<String,String> properties=new HashMap<>();
  properties.put(Constants.PIPELINE_SPEC_KEY,GSON.toJson(spec));
  setProperties(properties);
  stageSpecs=new HashMap<>();
  useSpark=engine == Engine.SPARK;
  if (!useSpark) {
    for (    StageSpec stageSpec : spec.getStages()) {
      stageSpecs.put(stageSpec.getName(),stageSpec);
      String pluginType=stageSpec.getPlugin().getType();
      if (SparkCompute.PLUGIN_TYPE.equals(pluginType) || SparkSink.PLUGIN_TYPE.equals(pluginType)) {
        useSpark=true;
        break;
      }
    }
  }
  PipelinePlanner planner;
  Set<String> actionTypes=ImmutableSet.of(Action.PLUGIN_TYPE,Constants.SPARK_PROGRAM_PLUGIN_TYPE);
  if (useSpark) {
    planner=new PipelinePlanner(supportedPluginTypes,ImmutableSet.<String>of(),ImmutableSet.<String>of(),actionTypes);
  }
 else {
    planner=new PipelinePlanner(supportedPluginTypes,ImmutableSet.of(BatchAggregator.PLUGIN_TYPE,BatchJoiner.PLUGIN_TYPE),ImmutableSet.of(SparkCompute.PLUGIN_TYPE,SparkSink.PLUGIN_TYPE),actionTypes);
  }
  plan=planner.plan(spec);
  if (plan.getPhases().size() == 1) {
    addProgram(plan.getPhases().keySet().iterator().next(),new TrunkProgramAdder(getConfigurer()));
    return;
  }
  if (plan.getPhaseConnections().isEmpty()) {
    WorkflowProgramAdder programAdder;
    WorkflowForkConfigurer forkConfigurer=getConfigurer().fork();
    programAdder=new BranchProgramAdder(forkConfigurer);
    for (    String phaseName : plan.getPhases().keySet()) {
      addProgram(phaseName,programAdder);
    }
    if (forkConfigurer != null) {
      forkConfigurer.join();
    }
    return;
  }
  dag=new ControlDag(plan.getPhaseConnections());
  dag.flatten();
  String start=dag.getSources().iterator().next();
  addPrograms(start,getConfigurer());
}","@Override protected void configure(){
  setName(NAME);
  setDescription(DESCRIPTION);
  Map<String,String> properties=new HashMap<>();
  properties.put(Constants.PIPELINE_SPEC_KEY,GSON.toJson(spec));
  setProperties(properties);
  stageSpecs=new HashMap<>();
  useSpark=engine == Engine.SPARK;
  for (  StageSpec stageSpec : spec.getStages()) {
    stageSpecs.put(stageSpec.getName(),stageSpec);
    String pluginType=stageSpec.getPlugin().getType();
    if (SparkCompute.PLUGIN_TYPE.equals(pluginType) || SparkSink.PLUGIN_TYPE.equals(pluginType)) {
      useSpark=true;
    }
  }
  PipelinePlanner planner;
  Set<String> actionTypes=ImmutableSet.of(Action.PLUGIN_TYPE,Constants.SPARK_PROGRAM_PLUGIN_TYPE);
  if (useSpark) {
    planner=new PipelinePlanner(supportedPluginTypes,ImmutableSet.<String>of(),ImmutableSet.<String>of(),actionTypes);
  }
 else {
    planner=new PipelinePlanner(supportedPluginTypes,ImmutableSet.of(BatchAggregator.PLUGIN_TYPE,BatchJoiner.PLUGIN_TYPE),ImmutableSet.of(SparkCompute.PLUGIN_TYPE,SparkSink.PLUGIN_TYPE),actionTypes);
  }
  plan=planner.plan(spec);
  if (plan.getPhases().size() == 1) {
    addProgram(plan.getPhases().keySet().iterator().next(),new TrunkProgramAdder(getConfigurer()));
    return;
  }
  if (plan.getPhaseConnections().isEmpty()) {
    WorkflowProgramAdder programAdder;
    WorkflowForkConfigurer forkConfigurer=getConfigurer().fork();
    programAdder=new BranchProgramAdder(forkConfigurer);
    for (    String phaseName : plan.getPhases().keySet()) {
      addProgram(phaseName,programAdder);
    }
    if (forkConfigurer != null) {
      forkConfigurer.join();
    }
    return;
  }
  dag=new ControlDag(plan.getPhaseConnections());
  dag.flatten();
  String start=dag.getSources().iterator().next();
  addPrograms(start,getConfigurer());
}"
4899,"/** 
 * Delete the specified namespace if it exists.
 * @param name the namespace to delete
 * @throws IOException if a remote or network exception occurs
 * @throws IllegalStateException if there are tables in the namespace
 */
void deleteNamespaceIfExists(String name) throws IOException ;","/** 
 * Delete the specified namespace if it exists. This method is called during namespace deletion process.
 * @param name the namespace to delete
 * @throws IOException if a remote or network exception occurs
 * @throws IllegalStateException if there are tables in the namespace
 */
void deleteNamespaceIfExists(String name) throws IOException ;"
4900,"/** 
 * Create the specified table if it does not exist.
 * @param descriptor the descriptor for the table to create
 * @param splitKeys the initial split keys for the table
 * @throws IOException if a remote or network exception occurs
 */
void createTableIfNotExists(TableDescriptor descriptor,@Nullable byte[][] splitKeys) throws IOException ;","/** 
 * Create the specified table if it does not exist. This method is called during the creation of an HBase backed dataset (either system or user).
 * @param descriptor the descriptor for the table to create
 * @param splitKeys the initial split keys for the table
 * @throws IOException if a remote or network exception occurs
 */
void createTableIfNotExists(TableDescriptor descriptor,@Nullable byte[][] splitKeys) throws IOException ;"
4901,"/** 
 * Disable the specified table if it is enabled.
 * @param namespace the namespace of the table to disable
 * @param name the name of the table to disable
 * @throws IOException if a remote or network exception occurs
 */
void disableTableIfEnabled(String namespace,String name) throws IOException ;","/** 
 * Disable the specified table if it is enabled. This is called when an HBase backed dataset has its properties modified. In order to modify the HBase table, CDAP first disables it with this method, then calls   {@code modifyTable}, then calls  {@code enableTableIfDisabled}.
 * @param namespace the namespace of the table to disable
 * @param name the name of the table to disable
 * @throws IOException if a remote or network exception occurs
 */
void disableTableIfEnabled(String namespace,String name) throws IOException ;"
4902,"/** 
 * Modify the specified table. The table must be disabled.
 * @param namespace the namespace of the table to modify
 * @param name the name of the table to modify
 * @param descriptor the descriptor for the table
 * @throws IOException if a remote or network exception occurs
 * @throws IllegalStateException if the specified table is not disabled
 */
void modifyTable(String namespace,String name,TableDescriptor descriptor) throws IOException ;","/** 
 * Modify the specified table. This is called when an HBase backed dataset has its properties modified. In order to modify the HBase table, CDAP first calls  {@code disableTableIfEnabled}, then calls this method, then calls   {@code enableTableIfDisabled}.
 * @param namespace the namespace of the table to modify
 * @param name the name of the table to modify
 * @param descriptor the descriptor for the table
 * @throws IOException if a remote or network exception occurs
 * @throws IllegalStateException if the specified table is not disabled
 */
void modifyTable(String namespace,String name,TableDescriptor descriptor) throws IOException ;"
4903,"/** 
 * Enable the specified table if it is disabled.
 * @param namespace the namespace of the table to enable
 * @param name the name of the table to enable
 * @throws IOException if a remote or network exception occurs
 */
void enableTableIfDisabled(String namespace,String name) throws IOException ;","/** 
 * Enable the specified table if it is disabled. This is called when an HBase backed dataset has its properties modified. In order to modify the HBase table, CDAP first calls   {@code disableTableIfEnabled}, then calls   {@code modifyTable}, then enables the table with this method.
 * @param namespace the namespace of the table to enable
 * @param name the name of the table to enable
 * @throws IOException if a remote or network exception occurs
 */
void enableTableIfDisabled(String namespace,String name) throws IOException ;"
4904,"/** 
 * Create the specified namespace if it does not exist.
 * @param name the namespace to create
 * @return whether the namespace was created
 * @throws IOException if a remote or network exception occurs
 */
boolean createNamespaceIfNotExists(String name) throws IOException ;","/** 
 * Create the specified namespace if it does not exist. This method gets called when CDAP attempts to create a new namespace.
 * @param name the namespace to create
 * @return whether the namespace was created
 * @throws IOException if a remote or network exception occurs
 */
boolean createNamespaceIfNotExists(String name) throws IOException ;"
4905,"/** 
 * Truncate the specified table. The table must be disabled.
 * @param namespace the namespace of the table to truncate
 * @param name the name of the table to truncate
 * @throws IOException if a remote or network exception occurs
 * @throws IllegalStateException if the specified table is not disabled
 */
void truncateTable(String namespace,String name) throws IOException ;","/** 
 * Truncate the specified table. The table must be disabled first to truncate it, after which it must be enabled again.
 * @param namespace the namespace of the table to truncate
 * @param name the name of the table to truncate
 * @throws IOException if a remote or network exception occurs
 * @throws IllegalStateException if the specified table is not disabled
 */
void truncateTable(String namespace,String name) throws IOException ;"
4906,"/** 
 * Initialize the   {@link HBaseDDLExecutor}.
 * @param context the context for the executor
 */
void initialize(HBaseDDLExecutorContext context);","/** 
 * Initialize the   {@link HBaseDDLExecutor}. This method is called once when the executor is created, before any other methods are called.
 * @param context the context for the executor
 */
void initialize(HBaseDDLExecutorContext context);"
4907,"@Override public void run(JavaSparkExecutionContext sec) throws Exception {
  String stageName=sec.getSpecification().getProperty(ExternalSparkProgram.STAGE_NAME);
  BatchPhaseSpec batchPhaseSpec=GSON.fromJson(sec.getSpecification().getProperty(Constants.PIPELINEID),BatchPhaseSpec.class);
  PipelinePluginContext pluginContext=new SparkPipelinePluginContext(sec.getPluginContext(),sec.getMetrics(),batchPhaseSpec.isStageLoggingEnabled(),batchPhaseSpec.isProcessTimingEnabled());
  Class<?> mainClass=pluginContext.loadPluginClass(stageName);
  if (mainClass.isAssignableFrom(JavaSparkMain.class)) {
    MacroEvaluator macroEvaluator=new DefaultMacroEvaluator(sec.getWorkflowToken(),sec.getRuntimeArguments(),sec.getLogicalStartTime(),sec.getSecureStore(),sec.getNamespace());
    JavaSparkMain javaSparkMain=pluginContext.newPluginInstance(stageName,macroEvaluator);
    javaSparkMain.run(sec);
  }
 else {
    String programArgs=getProgramArgs(sec,stageName);
    String[] args=programArgs == null ? RuntimeArguments.toPosixArray(sec.getRuntimeArguments()) : programArgs.split(""String_Node_Str"");
    final Method mainMethod=mainClass.getMethod(""String_Node_Str"",String[].class);
    final Object[] methodArgs=new Object[1];
    methodArgs[0]=args;
    Caller caller=pluginContext.getCaller(stageName);
    caller.call(new Callable<Void>(){
      @Override public Void call() throws Exception {
        mainMethod.invoke(null,methodArgs);
        return null;
      }
    }
);
  }
}","@Override public void run(JavaSparkExecutionContext sec) throws Exception {
  String stageName=sec.getSpecification().getProperty(ExternalSparkProgram.STAGE_NAME);
  BatchPhaseSpec batchPhaseSpec=GSON.fromJson(sec.getSpecification().getProperty(Constants.PIPELINEID),BatchPhaseSpec.class);
  PipelinePluginContext pluginContext=new SparkPipelinePluginContext(sec.getPluginContext(),sec.getMetrics(),batchPhaseSpec.isStageLoggingEnabled(),batchPhaseSpec.isProcessTimingEnabled());
  Class<?> mainClass=pluginContext.loadPluginClass(stageName);
  if (JavaSparkMain.class.isAssignableFrom(mainClass)) {
    MacroEvaluator macroEvaluator=new DefaultMacroEvaluator(sec.getWorkflowToken(),sec.getRuntimeArguments(),sec.getLogicalStartTime(),sec.getSecureStore(),sec.getNamespace());
    JavaSparkMain javaSparkMain=pluginContext.newPluginInstance(stageName,macroEvaluator);
    javaSparkMain.run(sec);
  }
 else {
    String programArgs=getProgramArgs(sec,stageName);
    String[] args=programArgs == null ? RuntimeArguments.toPosixArray(sec.getRuntimeArguments()) : programArgs.split(""String_Node_Str"");
    final Method mainMethod=mainClass.getMethod(""String_Node_Str"",String[].class);
    final Object[] methodArgs=new Object[1];
    methodArgs[0]=args;
    Caller caller=pluginContext.getCaller(stageName);
    caller.call(new Callable<Void>(){
      @Override public Void call() throws Exception {
        mainMethod.invoke(null,methodArgs);
        return null;
      }
    }
);
  }
}"
4908,"public void run(){
  try {
    if (isCDAPUserTable(desc)) {
      upgradeUserTable(desc);
    }
 else     if (isStreamOrQueueTable(desc.getNameAsString())) {
      updateTableDesc(desc,ddlExecutor);
    }
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
}","public void run(){
  try {
    impersonator.doAs(namespaceMeta.getNamespaceId(),new Callable<Void>(){
      @Override public Void call() throws Exception {
        if (isCDAPUserTable(desc)) {
          upgradeUserTable(desc);
        }
 else         if (isStreamOrQueueTable(desc.getNameAsString())) {
          updateTableDesc(desc,ddlExecutor);
        }
        return null;
      }
    }
);
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
}"
4909,"@Override public Void call() throws Exception {
  Map<String,Future<?>> futures=upgradeUserTables(namespaceMeta,executor);
  allFutures.putAll(futures);
  return null;
}","@Override public Void call() throws Exception {
  if (isCDAPUserTable(desc)) {
    upgradeUserTable(desc);
  }
 else   if (isStreamOrQueueTable(desc.getNameAsString())) {
    updateTableDesc(desc,ddlExecutor);
  }
  return null;
}"
4910,"private Map<String,Future<?>> upgradeUserTables(NamespaceMeta namespaceMeta,ExecutorService executor) throws Exception {
  Map<String,Future<?>> futures=new HashMap<>();
  String hBaseNamespace=hBaseTableUtil.getHBaseNamespace(namespaceMeta);
  try (HBaseDDLExecutor ddlExecutor=ddlExecutorFactory.get();HBaseAdmin hAdmin=new HBaseAdmin(hConf)){
    for (    final HTableDescriptor desc : hAdmin.listTableDescriptorsByNamespace(HTableNameConverter.encodeHBaseEntity(hBaseNamespace))) {
      Runnable runnable=new Runnable(){
        public void run(){
          try {
            if (isCDAPUserTable(desc)) {
              upgradeUserTable(desc);
            }
 else             if (isStreamOrQueueTable(desc.getNameAsString())) {
              updateTableDesc(desc,ddlExecutor);
            }
          }
 catch (          Exception e) {
            throw new RuntimeException(e);
          }
        }
      }
;
      Future<?> future=executor.submit(runnable);
      futures.put(desc.getNameAsString(),future);
    }
  }
   return futures;
}","private Map<String,Future<?>> upgradeUserTables(final NamespaceMeta namespaceMeta,ExecutorService executor) throws Exception {
  Map<String,Future<?>> futures=new HashMap<>();
  String hBaseNamespace=hBaseTableUtil.getHBaseNamespace(namespaceMeta);
  try (HBaseDDLExecutor ddlExecutor=ddlExecutorFactory.get();HBaseAdmin hAdmin=new HBaseAdmin(hConf)){
    for (    final HTableDescriptor desc : hAdmin.listTableDescriptorsByNamespace(HTableNameConverter.encodeHBaseEntity(hBaseNamespace))) {
      Runnable runnable=new Runnable(){
        public void run(){
          try {
            impersonator.doAs(namespaceMeta.getNamespaceId(),new Callable<Void>(){
              @Override public Void call() throws Exception {
                if (isCDAPUserTable(desc)) {
                  upgradeUserTable(desc);
                }
 else                 if (isStreamOrQueueTable(desc.getNameAsString())) {
                  updateTableDesc(desc,ddlExecutor);
                }
                return null;
              }
            }
);
          }
 catch (          Exception e) {
            throw new RuntimeException(e);
          }
        }
      }
;
      Future<?> future=executor.submit(runnable);
      futures.put(desc.getNameAsString(),future);
    }
  }
   return futures;
}"
4911,"public CloseableIterator<Job> fullScan(){
  return createCloseableIterator(table.scan(null,null));
}","public CloseableIterator<Job> fullScan(){
  return createCloseableIterator(table.scan(JOB_ROW_PREFIX,Bytes.stopKeyForPrefix(JOB_ROW_PREFIX)));
}"
4912,"@Test public void testGetAllJobs() throws Exception {
  txExecutor.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      Assert.assertEquals(0,getAllJobs(jobQueue).size());
      jobQueue.persistSubscriberState(""String_Node_Str"",""String_Node_Str"");
      jobQueue.put(SCHED1_JOB);
      Assert.assertEquals(ImmutableSet.of(SCHED1_JOB),getAllJobs(jobQueue));
    }
  }
);
}","@Test public void testGetAllJobs() throws Exception {
  txExecutor.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      Assert.assertEquals(0,getAllJobs(jobQueue).size());
      jobQueue.persistSubscriberState(""String_Node_Str"",""String_Node_Str"");
      jobQueue.put(SCHED1_JOB);
      Assert.assertEquals(ImmutableSet.of(SCHED1_JOB),getAllJobs(jobQueue));
      Assert.assertEquals(ImmutableSet.of(SCHED1_JOB),toSet(jobQueue.fullScan()));
    }
  }
);
}"
4913,"@Override public void run(JavaSparkExecutionContext sec) throws Exception {
  String stageName=sec.getSpecification().getProperty(ExternalSparkProgram.STAGE_NAME);
  BatchPhaseSpec batchPhaseSpec=GSON.fromJson(sec.getSpecification().getProperty(Constants.PIPELINEID),BatchPhaseSpec.class);
  PipelinePluginContext pluginContext=new SparkPipelinePluginContext(sec.getPluginContext(),sec.getMetrics(),batchPhaseSpec.isStageLoggingEnabled(),batchPhaseSpec.isProcessTimingEnabled());
  Class<?> mainClass=pluginContext.loadPluginClass(stageName);
  if (mainClass.isAssignableFrom(JavaSparkMain.class)) {
    MacroEvaluator macroEvaluator=new DefaultMacroEvaluator(sec.getWorkflowToken(),sec.getRuntimeArguments(),sec.getLogicalStartTime(),sec.getSecureStore(),sec.getNamespace());
    JavaSparkMain javaSparkMain=pluginContext.newPluginInstance(stageName,macroEvaluator);
    javaSparkMain.run(sec);
  }
 else {
    String programArgs=getProgramArgs(sec,stageName);
    String[] args=programArgs == null ? RuntimeArguments.toPosixArray(sec.getRuntimeArguments()) : programArgs.split(""String_Node_Str"");
    final Method mainMethod=mainClass.getMethod(""String_Node_Str"",String[].class);
    final Object[] methodArgs=new Object[1];
    methodArgs[0]=args;
    Caller caller=pluginContext.getCaller(stageName);
    caller.call(new Callable<Void>(){
      @Override public Void call() throws Exception {
        mainMethod.invoke(null,methodArgs);
        return null;
      }
    }
);
  }
}","@Override public void run(JavaSparkExecutionContext sec) throws Exception {
  String stageName=sec.getSpecification().getProperty(ExternalSparkProgram.STAGE_NAME);
  BatchPhaseSpec batchPhaseSpec=GSON.fromJson(sec.getSpecification().getProperty(Constants.PIPELINEID),BatchPhaseSpec.class);
  PipelinePluginContext pluginContext=new SparkPipelinePluginContext(sec.getPluginContext(),sec.getMetrics(),batchPhaseSpec.isStageLoggingEnabled(),batchPhaseSpec.isProcessTimingEnabled());
  Class<?> mainClass=pluginContext.loadPluginClass(stageName);
  if (JavaSparkMain.class.isAssignableFrom(mainClass)) {
    MacroEvaluator macroEvaluator=new DefaultMacroEvaluator(sec.getWorkflowToken(),sec.getRuntimeArguments(),sec.getLogicalStartTime(),sec.getSecureStore(),sec.getNamespace());
    JavaSparkMain javaSparkMain=pluginContext.newPluginInstance(stageName,macroEvaluator);
    javaSparkMain.run(sec);
  }
 else {
    String programArgs=getProgramArgs(sec,stageName);
    String[] args=programArgs == null ? RuntimeArguments.toPosixArray(sec.getRuntimeArguments()) : programArgs.split(""String_Node_Str"");
    final Method mainMethod=mainClass.getMethod(""String_Node_Str"",String[].class);
    final Object[] methodArgs=new Object[1];
    methodArgs[0]=args;
    Caller caller=pluginContext.getCaller(stageName);
    caller.call(new Callable<Void>(){
      @Override public Void call() throws Exception {
        mainMethod.invoke(null,methodArgs);
        return null;
      }
    }
);
  }
}"
4914,"public static boolean isValidName(String name){
  return isValidId(name);
}","public static boolean isValidName(String name){
  return isValidArtifactId(name);
}"
4915,"public Artifact(Namespace namespace,String name,ArtifactVersion version){
  if (namespace == null) {
    throw new NullPointerException(""String_Node_Str"");
  }
  if (name == null) {
    throw new NullPointerException(""String_Node_Str"");
  }
  if (!isValidId(name)) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (version == null) {
    throw new NullPointerException(""String_Node_Str"");
  }
  if (version.getVersion() == null) {
    throw new NullPointerException(""String_Node_Str"");
  }
  this.namespace=namespace;
  this.name=name;
  this.version=version;
}","public Artifact(Namespace namespace,String name,ArtifactVersion version){
  if (namespace == null) {
    throw new NullPointerException(""String_Node_Str"");
  }
  if (name == null) {
    throw new NullPointerException(""String_Node_Str"");
  }
  if (!isValidArtifactId(name)) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (version == null) {
    throw new NullPointerException(""String_Node_Str"");
  }
  if (version.getVersion() == null) {
    throw new NullPointerException(""String_Node_Str"");
  }
  this.namespace=namespace;
  this.name=name;
  this.version=version;
}"
4916,"@Override public void initialize() throws Exception {
  SparkClientContext context=getContext();
  String arguments=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(context.getRuntimeArguments());
  WRAPPERLOGGER.info(""String_Node_Str"",context.getApplicationSpecification().getName(),UserGroupInformation.getCurrentUser().getShortUserName(),arguments);
  DataStreamsPipelineSpec spec=GSON.fromJson(context.getSpecification().getProperty(Constants.PIPELINEID),DataStreamsPipelineSpec.class);
  int numSources=0;
  for (  StageSpec stageSpec : spec.getStages()) {
    if (StreamingSource.PLUGIN_TYPE.equals(stageSpec.getPlugin().getType())) {
      numSources++;
    }
  }
  SparkConf sparkConf=new SparkConf();
  sparkConf.set(""String_Node_Str"",""String_Node_Str"");
  for (  Map.Entry<String,String> property : spec.getProperties().entrySet()) {
    sparkConf.set(property.getKey(),property.getValue());
  }
  String extraOpts=spec.getExtraJavaOpts();
  if (extraOpts != null && !extraOpts.isEmpty()) {
    sparkConf.set(""String_Node_Str"",extraOpts);
    sparkConf.set(""String_Node_Str"",extraOpts);
  }
  sparkConf.set(""String_Node_Str"",String.valueOf(numSources + 2));
  if (spec.isUnitTest()) {
    sparkConf.setMaster(String.format(""String_Node_Str"",numSources + 1));
  }
  context.setSparkConf(sparkConf);
  if (!spec.isCheckpointsDisabled()) {
    FileSet checkpointFileSet=context.getDataset(DataStreamsApp.CHECKPOINT_FILESET);
    String pipelineName=context.getApplicationSpecification().getName();
    String checkpointDir=spec.getCheckpointDirectory();
    Location pipelineCheckpointBase=checkpointFileSet.getBaseLocation().append(pipelineName);
    Location pipelineCheckpointDir=pipelineCheckpointBase.append(checkpointDir);
    if (!ensureDirExists(pipelineCheckpointBase)) {
      throw new IOException(String.format(""String_Node_Str"",pipelineCheckpointBase));
    }
    try {
      for (      Location child : pipelineCheckpointBase.list()) {
        if (!child.equals(pipelineCheckpointDir) && !child.delete(true)) {
          LOG.warn(""String_Node_Str"",child);
        }
      }
    }
 catch (    Exception e) {
      LOG.warn(""String_Node_Str"",e);
    }
    if (!ensureDirExists(pipelineCheckpointDir)) {
      throw new IOException(String.format(""String_Node_Str"",pipelineCheckpointDir));
    }
  }
  WRAPPERLOGGER.info(""String_Node_Str"",context.getApplicationSpecification().getName());
}","@Override public void initialize() throws Exception {
  SparkClientContext context=getContext();
  String arguments=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(context.getRuntimeArguments());
  WRAPPERLOGGER.info(""String_Node_Str"",context.getApplicationSpecification().getName(),UserGroupInformation.getCurrentUser().getShortUserName(),arguments);
  DataStreamsPipelineSpec spec=GSON.fromJson(context.getSpecification().getProperty(Constants.PIPELINEID),DataStreamsPipelineSpec.class);
  PipelinePluginContext pluginContext=new SparkPipelinePluginContext(context,context.getMetrics(),true,true);
  int numSources=0;
  for (  StageSpec stageSpec : spec.getStages()) {
    if (StreamingSource.PLUGIN_TYPE.equals(stageSpec.getPlugin().getType())) {
      StreamingSource<Object> streamingSource=pluginContext.newPluginInstance(stageSpec.getName());
      numSources=numSources + streamingSource.getRequiredExecutors();
    }
  }
  SparkConf sparkConf=new SparkConf();
  sparkConf.set(""String_Node_Str"",""String_Node_Str"");
  for (  Map.Entry<String,String> property : spec.getProperties().entrySet()) {
    sparkConf.set(property.getKey(),property.getValue());
  }
  String extraOpts=spec.getExtraJavaOpts();
  if (extraOpts != null && !extraOpts.isEmpty()) {
    sparkConf.set(""String_Node_Str"",extraOpts);
    sparkConf.set(""String_Node_Str"",extraOpts);
  }
  sparkConf.set(""String_Node_Str"",String.valueOf(numSources + 2));
  sparkConf.set(""String_Node_Str"",String.valueOf(numSources + 2));
  sparkConf.setMaster(String.format(""String_Node_Str"",numSources + 2));
  if (spec.isUnitTest()) {
    sparkConf.setMaster(String.format(""String_Node_Str"",numSources + 1));
  }
  context.setSparkConf(sparkConf);
  if (!spec.isCheckpointsDisabled()) {
    FileSet checkpointFileSet=context.getDataset(DataStreamsApp.CHECKPOINT_FILESET);
    String pipelineName=context.getApplicationSpecification().getName();
    String checkpointDir=spec.getCheckpointDirectory();
    Location pipelineCheckpointBase=checkpointFileSet.getBaseLocation().append(pipelineName);
    Location pipelineCheckpointDir=pipelineCheckpointBase.append(checkpointDir);
    if (!ensureDirExists(pipelineCheckpointBase)) {
      throw new IOException(String.format(""String_Node_Str"",pipelineCheckpointBase));
    }
    try {
      for (      Location child : pipelineCheckpointBase.list()) {
        if (!child.equals(pipelineCheckpointDir) && !child.delete(true)) {
          LOG.warn(""String_Node_Str"",child);
        }
      }
    }
 catch (    Exception e) {
      LOG.warn(""String_Node_Str"",e);
    }
    if (!ensureDirExists(pipelineCheckpointDir)) {
      throw new IOException(String.format(""String_Node_Str"",pipelineCheckpointDir));
    }
  }
  WRAPPERLOGGER.info(""String_Node_Str"",context.getApplicationSpecification().getName());
}"
4917,"@Override public HBaseDDLExecutor get(){
  Map<String,HBaseDDLExecutor> extensions=hBaseDDLExecutorLoader.getAll();
  HBaseDDLExecutor executor;
  if (!extensions.isEmpty()) {
    executor=extensions.values().iterator().next();
  }
 else {
    executor=super.get();
  }
  executor.initialize(context);
  return executor;
}","@Override public HBaseDDLExecutor get(){
  Map<String,HBaseDDLExecutor> extensions=hBaseDDLExecutorLoader.getAll();
  HBaseDDLExecutor executor;
  if (!extensions.isEmpty()) {
    executor=extensions.values().iterator().next();
  }
 else {
    if (extensionDir != null) {
      throw new RuntimeException(String.format(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",extensionDir));
    }
    executor=super.get();
  }
  executor.initialize(context);
  return executor;
}"
4918,"public HBaseDDLExecutorFactory(CConfiguration cConf,Configuration hConf){
  this.hBaseDDLExecutorLoader=new HBaseDDLExecutorLoader(cConf.get(Constants.HBaseDDLExecutor.EXTENSIONS_DIR,""String_Node_Str""));
  this.context=new BasicHBaseDDLExecutorContext(cConf,hConf);
}","public HBaseDDLExecutorFactory(CConfiguration cConf,Configuration hConf){
  this.extensionDir=cConf.get(Constants.HBaseDDLExecutor.EXTENSIONS_DIR);
  this.hBaseDDLExecutorLoader=new HBaseDDLExecutorLoader(extensionDir == null ? ""String_Node_Str"" : extensionDir);
  this.context=new BasicHBaseDDLExecutorContext(cConf,hConf);
}"
4919,"@Override public void run(){
  LOG.info(""String_Node_Str"");
  Set<String> problemKeys=new HashSet<>();
  checkServiceResources(problemKeys);
  checkBindAddresses();
  checkPotentialPortConflicts(problemKeys);
  checkKafkaTopic(problemKeys);
  checkMessagingTopics(problemKeys);
  checkLogPartitionKey(problemKeys);
  checkPruningAndReplication(problemKeys);
  if (!problemKeys.isEmpty()) {
    throw new RuntimeException(""String_Node_Str"" + Joiner.on(',').join(problemKeys));
  }
  LOG.info(""String_Node_Str"");
}","@Override public void run(){
  LOG.info(""String_Node_Str"");
  Set<String> problemKeys=new HashSet<>();
  checkServiceResources(problemKeys);
  checkBindAddresses();
  checkPotentialPortConflicts(problemKeys);
  checkKafkaTopic(problemKeys);
  checkMessagingTopics(problemKeys);
  checkLogPartitionKey(problemKeys);
  checkPruningAndReplication(problemKeys);
  checkHBaseDDLExtension(problemKeys);
  if (!problemKeys.isEmpty()) {
    throw new RuntimeException(""String_Node_Str"" + Joiner.on(',').join(problemKeys));
  }
  LOG.info(""String_Node_Str"");
}"
4920,"@Inject private ConfigurationCheck(CConfiguration cConf){
  super(cConf);
}","@Inject private ConfigurationCheck(CConfiguration cConf,Configuration hConf){
  super(cConf);
  this.hConf=hConf;
}"
4921,"private ConstraintResult notSatisfied(){
  if (!waitUntilMet) {
    return ConstraintResult.NEVER_SATISFIED;
  }
  return new ConstraintResult(ConstraintResult.SatisfiedState.NOT_SATISFIED,TimeUnit.SECONDS.toMillis(10));
}","private ConstraintResult notSatisfied(ConstraintContext context){
  if (!waitUntilMet) {
    return ConstraintResult.NEVER_SATISFIED;
  }
  return new ConstraintResult(ConstraintResult.SatisfiedState.NOT_SATISFIED,context.getCheckTime() + TimeUnit.SECONDS.toMillis(10));
}"
4922,"@Override public ConstraintResult check(ProgramSchedule schedule,ConstraintContext context){
  int numRunning=context.getProgramRuns(schedule.getProgramId(),ProgramRunStatus.RUNNING,maxConcurrency).size();
  if (numRunning >= maxConcurrency) {
    LOG.debug(""String_Node_Str"",schedule.getProgramId(),schedule.getName(),maxConcurrency);
    return notSatisfied();
  }
  int numSuspended=context.getProgramRuns(schedule.getProgramId(),ProgramRunStatus.SUSPENDED,maxConcurrency).size();
  if (numRunning + numSuspended >= maxConcurrency) {
    LOG.debug(""String_Node_Str"" + ""String_Node_Str"",schedule.getProgramId(),schedule.getName(),numRunning,numSuspended);
    return notSatisfied();
  }
  return ConstraintResult.SATISFIED;
}","@Override public ConstraintResult check(ProgramSchedule schedule,ConstraintContext context){
  int numRunning=context.getProgramRuns(schedule.getProgramId(),ProgramRunStatus.RUNNING,maxConcurrency).size();
  if (numRunning >= maxConcurrency) {
    LOG.debug(""String_Node_Str"",schedule.getProgramId(),schedule.getName(),maxConcurrency);
    return notSatisfied(context);
  }
  int numSuspended=context.getProgramRuns(schedule.getProgramId(),ProgramRunStatus.SUSPENDED,maxConcurrency).size();
  if (numRunning + numSuspended >= maxConcurrency) {
    LOG.debug(""String_Node_Str"" + ""String_Node_Str"",schedule.getProgramId(),schedule.getName(),numRunning,numSuspended);
    return notSatisfied(context);
  }
  return ConstraintResult.SATISFIED;
}"
4923,"ConstraintResult(SatisfiedState satisfiedState,Long millisBeforeNextRetry){
  if (satisfiedState == SatisfiedState.NOT_SATISFIED) {
    Preconditions.checkNotNull(millisBeforeNextRetry);
  }
  this.satisfiedState=satisfiedState;
  this.millisBeforeNextRetry=millisBeforeNextRetry;
}","ConstraintResult(SatisfiedState satisfiedState,Long nextCheckTime){
  if (satisfiedState == SatisfiedState.NOT_SATISFIED) {
    Preconditions.checkNotNull(nextCheckTime);
  }
  this.satisfiedState=satisfiedState;
  this.nextCheckTime=nextCheckTime;
}"
4924,"@Override public ConstraintResult check(ProgramSchedule schedule,ConstraintContext context){
  long elapsedTime=context.getCheckTimeMillis() - context.getJob().getCreationTime();
  if (elapsedTime >= millisAfterTrigger) {
    return ConstraintResult.SATISFIED;
  }
  return new ConstraintResult(ConstraintResult.SatisfiedState.NOT_SATISFIED,millisAfterTrigger - elapsedTime);
}","@Override public ConstraintResult check(ProgramSchedule schedule,ConstraintContext context){
  long elapsedTime=context.getCheckTimeMillis() - context.getJob().getCreationTime();
  if (elapsedTime >= millisAfterTrigger) {
    return ConstraintResult.SATISFIED;
  }
  return new ConstraintResult(ConstraintResult.SatisfiedState.NOT_SATISFIED,context.getJob().getCreationTime() + millisAfterTrigger);
}"
4925,"@Override public ConstraintResult check(ProgramSchedule schedule,ConstraintContext context){
  long startTime=TimeUnit.MILLISECONDS.toSeconds(context.getCheckTimeMillis() - millisSinceLastRun);
  Iterable<RunRecordMeta> runRecords=context.getProgramRuns(schedule.getProgramId(),ProgramRunStatus.ALL,startTime - TimeUnit.DAYS.toSeconds(1),Long.MAX_VALUE,100).values();
  if (Iterables.isEmpty(filter(runRecords,startTime))) {
    return ConstraintResult.SATISFIED;
  }
  if (!waitUntilMet) {
    return ConstraintResult.NEVER_SATISFIED;
  }
  return new ConstraintResult(ConstraintResult.SatisfiedState.NOT_SATISFIED,TimeUnit.SECONDS.toMillis(10));
}","@Override public ConstraintResult check(ProgramSchedule schedule,ConstraintContext context){
  long startTime=TimeUnit.MILLISECONDS.toSeconds(context.getCheckTimeMillis() - millisSinceLastRun);
  Iterable<RunRecordMeta> runRecords=context.getProgramRuns(schedule.getProgramId(),ProgramRunStatus.ALL,startTime - TimeUnit.DAYS.toSeconds(1),Long.MAX_VALUE,100).values();
  if (Iterables.isEmpty(filter(runRecords,startTime))) {
    return ConstraintResult.SATISFIED;
  }
  if (!waitUntilMet) {
    return ConstraintResult.NEVER_SATISFIED;
  }
  return new ConstraintResult(ConstraintResult.SatisfiedState.NOT_SATISFIED,context.getCheckTime() + TimeUnit.SECONDS.toMillis(10));
}"
4926,"@Override public ConstraintResult check(ProgramSchedule schedule,ConstraintContext context){
  initialize();
  calendar.setTimeInMillis(context.getCheckTimeMillis());
  int hourOfDay=calendar.get(Calendar.HOUR_OF_DAY);
  int minute=calendar.get(Calendar.MINUTE);
  boolean pastOrEqualStartRange=hourOfDay > startHour || (hourOfDay == startHour && minute >= startMinute);
  boolean pastOrEqualEndRange=hourOfDay > endHour || (hourOfDay == endHour && minute >= endMinute);
  if (isStartTimeSmaller) {
    boolean satisfied=pastOrEqualStartRange && !pastOrEqualEndRange;
    if (satisfied) {
      return ConstraintResult.SATISFIED;
    }
  }
 else {
    boolean satisfied=pastOrEqualStartRange || !pastOrEqualEndRange;
    if (satisfied) {
      return ConstraintResult.SATISFIED;
    }
  }
  if (!waitUntilMet) {
    return ConstraintResult.NEVER_SATISFIED;
  }
  if (pastOrEqualEndRange && isStartTimeSmaller) {
    calendar.add(Calendar.DAY_OF_YEAR,1);
  }
  calendar.set(Calendar.HOUR_OF_DAY,startHour);
  calendar.set(Calendar.MINUTE,startMinute);
  calendar.set(Calendar.SECOND,0);
  calendar.set(Calendar.MILLISECOND,0);
  return new ConstraintResult(ConstraintResult.SatisfiedState.NOT_SATISFIED,calendar.getTimeInMillis() - context.getCheckTimeMillis());
}","@Override public ConstraintResult check(ProgramSchedule schedule,ConstraintContext context){
  initialize();
  calendar.setTimeInMillis(context.getCheckTimeMillis());
  int hourOfDay=calendar.get(Calendar.HOUR_OF_DAY);
  int minute=calendar.get(Calendar.MINUTE);
  boolean pastOrEqualStartRange=hourOfDay > startHour || (hourOfDay == startHour && minute >= startMinute);
  boolean pastOrEqualEndRange=hourOfDay > endHour || (hourOfDay == endHour && minute >= endMinute);
  if (isStartTimeSmaller) {
    boolean satisfied=pastOrEqualStartRange && !pastOrEqualEndRange;
    if (satisfied) {
      return ConstraintResult.SATISFIED;
    }
  }
 else {
    boolean satisfied=pastOrEqualStartRange || !pastOrEqualEndRange;
    if (satisfied) {
      return ConstraintResult.SATISFIED;
    }
  }
  if (!waitUntilMet) {
    return ConstraintResult.NEVER_SATISFIED;
  }
  if (pastOrEqualEndRange && isStartTimeSmaller) {
    calendar.add(Calendar.DAY_OF_YEAR,1);
  }
  calendar.set(Calendar.HOUR_OF_DAY,startHour);
  calendar.set(Calendar.MINUTE,startMinute);
  calendar.set(Calendar.SECOND,0);
  calendar.set(Calendar.MILLISECOND,0);
  return new ConstraintResult(ConstraintResult.SatisfiedState.NOT_SATISFIED,calendar.getTimeInMillis());
}"
4927,"@Override public Boolean call(DatasetContext context) throws Exception {
  return runReadyJobs(context);
}","@Override public Void call(DatasetContext context) throws Exception {
  if (runReadyJob(job)) {
    readyJobsIter.remove();
  }
  return null;
}"
4928,"private boolean runReadyJobs(DatasetContext context) throws Exception {
  Iterator<Job> readyJobsIter=readyJobs.iterator();
  while (readyJobsIter.hasNext() && !stopping) {
    Job job=readyJobsIter.next();
    Job storedJob=jobQueue.getJob(job.getJobKey());
    if (storedJob == null) {
      readyJobsIter.remove();
      continue;
    }
    if (job.isToBeDeleted()) {
      readyJobsIter.remove();
      jobQueue.deleteJob(job);
      return true;
    }
    if (storedJob.getState() == Job.State.PENDING_LAUNCH) {
      try {
        taskRunner.launch(job);
      }
 catch (      Exception e) {
        LOG.warn(""String_Node_Str"",job.getSchedule().getProgramId(),job.getSchedule().getName(),e);
        continue;
      }
      readyJobsIter.remove();
      jobQueue.deleteJob(job);
      return true;
    }
  }
  return false;
}","private void runReadyJobs(){
  final Iterator<Job> readyJobsIter=readyJobs.iterator();
  while (readyJobsIter.hasNext() && !stopping) {
    final Job job=readyJobsIter.next();
    try {
      Transactions.execute(transactional,new TxCallable<Void>(){
        @Override public Void call(        DatasetContext context) throws Exception {
          if (runReadyJob(job)) {
            readyJobsIter.remove();
          }
          return null;
        }
      }
);
    }
 catch (    TransactionFailureException e) {
      LOG.warn(""String_Node_Str"",job.getSchedule().getProgramId(),job.getSchedule().getName(),e);
    }
  }
}"
4929,"/** 
 * Check jobs in job queue for constraint satisfaction.
 * @return sleep time in milliseconds before next fetch
 */
private long checkJobQueue(){
  boolean emptyFetch=false;
  try {
    emptyFetch=Transactions.execute(transactional,new TxCallable<Boolean>(){
      @Override public Boolean call(      DatasetContext context) throws Exception {
        return checkJobConstraints();
      }
    }
);
    Transactions.execute(transactional,new TxCallable<Boolean>(){
      @Override public Boolean call(      DatasetContext context) throws Exception {
        return runReadyJobs(context);
      }
    }
);
    failureCount=0;
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
    failureCount++;
  }
  if (failureCount > 0) {
    return scheduleStrategy.nextRetry(failureCount,0);
  }
  return emptyFetch ? 2000L : 0L;
}","/** 
 * Check jobs in job queue for constraint satisfaction.
 * @return sleep time in milliseconds before next fetch
 */
private long checkJobQueue(){
  boolean emptyFetch=false;
  try {
    emptyFetch=Transactions.execute(transactional,new TxCallable<Boolean>(){
      @Override public Boolean call(      DatasetContext context) throws Exception {
        return checkJobConstraints();
      }
    }
);
    runReadyJobs();
    failureCount=0;
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
    failureCount++;
  }
  if (failureCount > 0) {
    return scheduleStrategy.nextRetry(failureCount,0);
  }
  return emptyFetch && readyJobs.isEmpty() ? 2000L : 0L;
}"
4930,"@Inject CoreSchedulerService(TransactionSystemClient txClient,final DatasetFramework datasetFramework,final SchedulerService schedulerService,final NotificationSubscriberService notificationSubscriberService,final ConstraintCheckerService constraintCheckerService,final NamespaceQueryAdmin namespaceQueryAdmin,final Store store){
  this.datasetFramework=datasetFramework;
  DynamicDatasetCache datasetCache=new MultiThreadDatasetCache(new SystemDatasetInstantiator(datasetFramework),txClient,Schedulers.STORE_DATASET_ID.getParent(),Collections.<String,String>emptyMap(),null,null);
  this.transactional=Transactions.createTransactionalWithRetry(Transactions.createTransactional(datasetCache),RetryStrategies.retryOnConflict(10,100L));
  this.scheduler=schedulerService;
  this.internalService=new RetryOnStartFailureService(new Supplier<Service>(){
    @Override public Service get(){
      return new AbstractIdleService(){
        @Override protected void startUp() throws Exception {
          if (!datasetFramework.hasInstance(Schedulers.STORE_DATASET_ID)) {
            datasetFramework.addInstance(Schedulers.STORE_TYPE_NAME,Schedulers.STORE_DATASET_ID,DatasetProperties.EMPTY);
          }
          schedulerService.startAndWait();
          migrateSchedules(namespaceQueryAdmin,store);
          constraintCheckerService.startAndWait();
          notificationSubscriberService.startAndWait();
        }
        @Override protected void shutDown() throws Exception {
          notificationSubscriberService.stopAndWait();
          constraintCheckerService.stopAndWait();
          schedulerService.stopAndWait();
        }
      }
;
    }
  }
,co.cask.cdap.common.service.RetryStrategies.exponentialDelay(200,5000,TimeUnit.MILLISECONDS));
}","@Inject CoreSchedulerService(TransactionSystemClient txClient,final DatasetFramework datasetFramework,final SchedulerService schedulerService,final NotificationSubscriberService notificationSubscriberService,final ConstraintCheckerService constraintCheckerService,final NamespaceQueryAdmin namespaceQueryAdmin,final Store store){
  this.datasetFramework=datasetFramework;
  final DynamicDatasetCache datasetCache=new MultiThreadDatasetCache(new SystemDatasetInstantiator(datasetFramework),txClient,Schedulers.STORE_DATASET_ID.getParent(),Collections.<String,String>emptyMap(),null,null);
  this.transactional=Transactions.createTransactionalWithRetry(Transactions.createTransactional(datasetCache),RetryStrategies.retryOnConflict(10,100L));
  this.scheduler=schedulerService;
  this.internalService=new RetryOnStartFailureService(new Supplier<Service>(){
    @Override public Service get(){
      return new AbstractIdleService(){
        @Override protected void startUp() throws Exception {
          if (!datasetFramework.hasInstance(Schedulers.STORE_DATASET_ID)) {
            datasetFramework.addInstance(Schedulers.STORE_TYPE_NAME,Schedulers.STORE_DATASET_ID,DatasetProperties.EMPTY);
          }
          schedulerService.startAndWait();
          migrateSchedules(namespaceQueryAdmin,store);
          cleanupJobs();
          constraintCheckerService.startAndWait();
          notificationSubscriberService.startAndWait();
        }
        @Override protected void shutDown() throws Exception {
          notificationSubscriberService.stopAndWait();
          constraintCheckerService.stopAndWait();
          schedulerService.stopAndWait();
        }
      }
;
    }
  }
,co.cask.cdap.common.service.RetryStrategies.exponentialDelay(200,5000,TimeUnit.MILLISECONDS));
}"
4931,"@Override public Service get(){
  return new AbstractIdleService(){
    @Override protected void startUp() throws Exception {
      if (!datasetFramework.hasInstance(Schedulers.STORE_DATASET_ID)) {
        datasetFramework.addInstance(Schedulers.STORE_TYPE_NAME,Schedulers.STORE_DATASET_ID,DatasetProperties.EMPTY);
      }
      schedulerService.startAndWait();
      migrateSchedules(namespaceQueryAdmin,store);
      constraintCheckerService.startAndWait();
      notificationSubscriberService.startAndWait();
    }
    @Override protected void shutDown() throws Exception {
      notificationSubscriberService.stopAndWait();
      constraintCheckerService.stopAndWait();
      schedulerService.stopAndWait();
    }
  }
;
}","@Override public Service get(){
  return new AbstractIdleService(){
    @Override protected void startUp() throws Exception {
      if (!datasetFramework.hasInstance(Schedulers.STORE_DATASET_ID)) {
        datasetFramework.addInstance(Schedulers.STORE_TYPE_NAME,Schedulers.STORE_DATASET_ID,DatasetProperties.EMPTY);
      }
      schedulerService.startAndWait();
      migrateSchedules(namespaceQueryAdmin,store);
      cleanupJobs();
      constraintCheckerService.startAndWait();
      notificationSubscriberService.startAndWait();
    }
    @Override protected void shutDown() throws Exception {
      notificationSubscriberService.stopAndWait();
      constraintCheckerService.stopAndWait();
      schedulerService.stopAndWait();
    }
  }
;
}"
4932,"public LaunchConfig addRunnable(String name,TwillRunnable runnable,Resources resources,int instances){
  runnables.put(name,new RunnableResource(runnable,createResourceSpec(resources,instances)));
  return this;
}","public LaunchConfig addRunnable(String name,TwillRunnable runnable,Resources resources,int instances,@Nullable Integer maxRetries){
  runnables.put(name,new RunnableResource(runnable,createResourceSpec(resources,instances),maxRetries));
  return this;
}"
4933,"@Override public ProgramController call() throws Exception {
  ProgramTwillApplication twillApplication=new ProgramTwillApplication(program.getId(),launchConfig.getRunnables(),launchConfig.getLaunchOrder(),localizeResources,createEventHandler(cConf));
  TwillPreparer twillPreparer=twillRunner.prepare(twillApplication);
  if (options.isDebug()) {
    twillPreparer.enableDebugging();
  }
  logProgramStart(program,options);
  String serializedOptions=GSON.toJson(options,ProgramOptions.class);
  LOG.info(""String_Node_Str"",program.getId(),options.isDebug(),serializedOptions,logbackURI);
  String schedulerQueueName=options.getArguments().getOption(Constants.AppFabric.APP_SCHEDULER_QUEUE);
  if (schedulerQueueName != null && !schedulerQueueName.isEmpty()) {
    LOG.info(""String_Node_Str"",program.getId(),schedulerQueueName);
    twillPreparer.setSchedulerQueue(schedulerQueueName);
  }
  if (logbackURI != null) {
    twillPreparer.addJVMOptions(""String_Node_Str"" + LOGBACK_FILE_NAME);
  }
  setLogLevels(twillPreparer,program,options);
  String logLevelConf=cConf.get(Constants.COLLECT_APP_CONTAINER_LOG_LEVEL).toUpperCase();
  if (""String_Node_Str"".equals(logLevelConf)) {
    twillPreparer.withConfiguration(Collections.singletonMap(Configs.Keys.LOG_COLLECTION_ENABLED,""String_Node_Str""));
  }
 else {
    LogEntry.Level logLevel=LogEntry.Level.ERROR;
    if (""String_Node_Str"".equals(logLevelConf)) {
      logLevel=LogEntry.Level.TRACE;
    }
 else {
      try {
        logLevel=LogEntry.Level.valueOf(logLevelConf.toUpperCase());
      }
 catch (      Exception e) {
        LOG.warn(""String_Node_Str"",logLevelConf);
      }
    }
    twillPreparer.addLogHandler(new LoggerLogHandler(LOG,logLevel));
  }
  if (User.isHBaseSecurityEnabled(hConf) || UserGroupInformation.isSecurityEnabled()) {
    twillPreparer.addSecureStore(YarnSecureStore.create(secureStoreRenewer.createCredentials()));
  }
  twillPreparer.withEnv(Collections.singletonMap(""String_Node_Str"",ApplicationConstants.LOG_DIR_EXPANSION_VAR));
  Set<Class<?>> extraDependencies=new HashSet<>(launchConfig.getExtraDependencies());
  extraDependencies.add(HBaseTableUtilFactory.getHBaseTableUtilClass());
  extraDependencies.add(new HBaseDDLExecutorFactory(cConf,hConf).get().getClass());
  if (SecureStoreUtils.isKMSBacked(cConf) && SecureStoreUtils.isKMSCapable()) {
    extraDependencies.add(SecureStoreUtils.getKMSSecureStore());
  }
  twillPreparer.withDependencies(extraDependencies);
  twillPreparer.withClassPaths(additionalClassPaths);
  twillPreparer.withClassPaths(launchConfig.getExtraClasspath());
  twillPreparer.withEnv(launchConfig.getExtraEnv());
  List<String> yarnAppClassPath=Arrays.asList(hConf.getTrimmedStrings(YarnConfiguration.YARN_APPLICATION_CLASSPATH,YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
  twillPreparer.withApplicationClassPaths(yarnAppClassPath).withClassPaths(yarnAppClassPath).withBundlerClassAcceptor(launchConfig.getClassAcceptor()).withApplicationArguments(""String_Node_Str"" + RunnableOptions.JAR,programJarName,""String_Node_Str"" + RunnableOptions.EXPANDED_JAR,expandedProgramJarName,""String_Node_Str"" + RunnableOptions.HADOOP_CONF_FILE,HADOOP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.CDAP_CONF_FILE,CDAP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.APP_SPEC_FILE,APP_SPEC_FILE_NAME,""String_Node_Str"" + RunnableOptions.PROGRAM_OPTIONS,serializedOptions,""String_Node_Str"" + RunnableOptions.PROGRAM_ID,GSON.toJson(program.getId())).setClassLoader(MainClassLoader.class.getName());
  beforeLaunch(program,options);
  TwillController twillController;
  ClassLoader oldClassLoader=ClassLoaders.setContextClassLoader(new CombineClassLoader(DistributedProgramRunner.this.getClass().getClassLoader(),Iterables.transform(extraDependencies,new Function<Class<?>,ClassLoader>(){
    @Override public ClassLoader apply(    Class<?> input){
      return input.getClassLoader();
    }
  }
)));
  try {
    twillController=twillPreparer.start(cConf.getLong(Constants.AppFabric.PROGRAM_MAX_START_SECONDS),TimeUnit.SECONDS);
  }
  finally {
    ClassLoaders.setContextClassLoader(oldClassLoader);
  }
  return createProgramController(addCleanupListener(twillController,program,tempDir),new ProgramDescriptor(program.getId(),program.getApplicationSpecification()),ProgramRunners.getRunId(options));
}","@Override public ProgramController call() throws Exception {
  ProgramTwillApplication twillApplication=new ProgramTwillApplication(program.getId(),launchConfig.getRunnables(),launchConfig.getLaunchOrder(),localizeResources,createEventHandler(cConf));
  TwillPreparer twillPreparer=twillRunner.prepare(twillApplication);
  for (  Map.Entry<String,RunnableResource> entry : launchConfig.getRunnables().entrySet()) {
    String runnable=entry.getKey();
    RunnableResource runnableResource=entry.getValue();
    if (runnableResource.getMaxRetries() != null) {
      twillPreparer.withMaxRetries(runnable,runnableResource.getMaxRetries());
    }
  }
  if (options.isDebug()) {
    twillPreparer.enableDebugging();
  }
  logProgramStart(program,options);
  String serializedOptions=GSON.toJson(options,ProgramOptions.class);
  LOG.info(""String_Node_Str"",program.getId(),options.isDebug(),serializedOptions,logbackURI);
  String schedulerQueueName=options.getArguments().getOption(Constants.AppFabric.APP_SCHEDULER_QUEUE);
  if (schedulerQueueName != null && !schedulerQueueName.isEmpty()) {
    LOG.info(""String_Node_Str"",program.getId(),schedulerQueueName);
    twillPreparer.setSchedulerQueue(schedulerQueueName);
  }
  if (logbackURI != null) {
    twillPreparer.addJVMOptions(""String_Node_Str"" + LOGBACK_FILE_NAME);
  }
  setLogLevels(twillPreparer,program,options);
  String logLevelConf=cConf.get(Constants.COLLECT_APP_CONTAINER_LOG_LEVEL).toUpperCase();
  if (""String_Node_Str"".equals(logLevelConf)) {
    twillPreparer.withConfiguration(Collections.singletonMap(Configs.Keys.LOG_COLLECTION_ENABLED,""String_Node_Str""));
  }
 else {
    LogEntry.Level logLevel=LogEntry.Level.ERROR;
    if (""String_Node_Str"".equals(logLevelConf)) {
      logLevel=LogEntry.Level.TRACE;
    }
 else {
      try {
        logLevel=LogEntry.Level.valueOf(logLevelConf.toUpperCase());
      }
 catch (      Exception e) {
        LOG.warn(""String_Node_Str"",logLevelConf);
      }
    }
    twillPreparer.addLogHandler(new LoggerLogHandler(LOG,logLevel));
  }
  if (User.isHBaseSecurityEnabled(hConf) || UserGroupInformation.isSecurityEnabled()) {
    twillPreparer.addSecureStore(YarnSecureStore.create(secureStoreRenewer.createCredentials()));
  }
  twillPreparer.withEnv(Collections.singletonMap(""String_Node_Str"",ApplicationConstants.LOG_DIR_EXPANSION_VAR));
  Set<Class<?>> extraDependencies=new HashSet<>(launchConfig.getExtraDependencies());
  extraDependencies.add(HBaseTableUtilFactory.getHBaseTableUtilClass());
  extraDependencies.add(new HBaseDDLExecutorFactory(cConf,hConf).get().getClass());
  if (SecureStoreUtils.isKMSBacked(cConf) && SecureStoreUtils.isKMSCapable()) {
    extraDependencies.add(SecureStoreUtils.getKMSSecureStore());
  }
  twillPreparer.withDependencies(extraDependencies);
  twillPreparer.withClassPaths(additionalClassPaths);
  twillPreparer.withClassPaths(launchConfig.getExtraClasspath());
  twillPreparer.withEnv(launchConfig.getExtraEnv());
  List<String> yarnAppClassPath=Arrays.asList(hConf.getTrimmedStrings(YarnConfiguration.YARN_APPLICATION_CLASSPATH,YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
  twillPreparer.withApplicationClassPaths(yarnAppClassPath).withClassPaths(yarnAppClassPath).withBundlerClassAcceptor(launchConfig.getClassAcceptor()).withApplicationArguments(""String_Node_Str"" + RunnableOptions.JAR,programJarName,""String_Node_Str"" + RunnableOptions.EXPANDED_JAR,expandedProgramJarName,""String_Node_Str"" + RunnableOptions.HADOOP_CONF_FILE,HADOOP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.CDAP_CONF_FILE,CDAP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.APP_SPEC_FILE,APP_SPEC_FILE_NAME,""String_Node_Str"" + RunnableOptions.PROGRAM_OPTIONS,serializedOptions,""String_Node_Str"" + RunnableOptions.PROGRAM_ID,GSON.toJson(program.getId())).setClassLoader(MainClassLoader.class.getName());
  beforeLaunch(program,options);
  TwillController twillController;
  ClassLoader oldClassLoader=ClassLoaders.setContextClassLoader(new CombineClassLoader(DistributedProgramRunner.this.getClass().getClassLoader(),Iterables.transform(extraDependencies,new Function<Class<?>,ClassLoader>(){
    @Override public ClassLoader apply(    Class<?> input){
      return input.getClassLoader();
    }
  }
)));
  try {
    twillController=twillPreparer.start(cConf.getLong(Constants.AppFabric.PROGRAM_MAX_START_SECONDS),TimeUnit.SECONDS);
  }
  finally {
    ClassLoaders.setContextClassLoader(oldClassLoader);
  }
  return createProgramController(addCleanupListener(twillController,program,tempDir),new ProgramDescriptor(program.getId(),program.getApplicationSpecification()),ProgramRunners.getRunId(options));
}"
4934,"@Override protected void setupLaunchConfig(LaunchConfig launchConfig,Program program,ProgramOptions options,CConfiguration cConf,Configuration hConf,File tempDir) throws IOException {
  WorkflowSpecification spec=program.getApplicationSpecification().getWorkflows().get(program.getName());
  List<ClassAcceptor> acceptors=new ArrayList<>();
  Set<SchedulableProgramType> runnerTypes=EnumSet.of(SchedulableProgramType.MAPREDUCE,SchedulableProgramType.SPARK);
  for (  WorkflowActionNode node : Iterables.filter(spec.getNodeIdMap().values(),WorkflowActionNode.class)) {
    ScheduleProgramInfo programInfo=node.getProgram();
    if (!runnerTypes.remove(programInfo.getProgramType())) {
      continue;
    }
    ProgramType programType=ProgramType.valueOfSchedulableType(programInfo.getProgramType());
    ProgramRunner runner=programRunnerFactory.create(programType);
    try {
      if (runner instanceof DistributedProgramRunner) {
        ProgramId programId=program.getId().getParent().program(programType,programInfo.getProgramName());
        ((DistributedProgramRunner)runner).setupLaunchConfig(launchConfig,Programs.create(cConf,program,programId,runner),options,cConf,hConf,tempDir);
        acceptors.add(launchConfig.getClassAcceptor());
      }
    }
  finally {
      if (runner instanceof Closeable) {
        Closeables.closeQuietly((Closeable)runner);
      }
    }
  }
  launchConfig.setClassAcceptor(new AndClassAcceptor(acceptors));
  launchConfig.clearRunnables();
  Resources resources=findDriverResources(program.getApplicationSpecification().getSpark(),program.getApplicationSpecification().getMapReduce(),spec);
  resources=SystemArguments.getResources(options.getUserArguments(),resources);
  launchConfig.addRunnable(spec.getName(),new WorkflowTwillRunnable(spec.getName()),resources,1);
}","@Override protected void setupLaunchConfig(LaunchConfig launchConfig,Program program,ProgramOptions options,CConfiguration cConf,Configuration hConf,File tempDir) throws IOException {
  WorkflowSpecification spec=program.getApplicationSpecification().getWorkflows().get(program.getName());
  List<ClassAcceptor> acceptors=new ArrayList<>();
  Set<SchedulableProgramType> runnerTypes=EnumSet.of(SchedulableProgramType.MAPREDUCE,SchedulableProgramType.SPARK);
  for (  WorkflowActionNode node : Iterables.filter(spec.getNodeIdMap().values(),WorkflowActionNode.class)) {
    ScheduleProgramInfo programInfo=node.getProgram();
    if (!runnerTypes.remove(programInfo.getProgramType())) {
      continue;
    }
    ProgramType programType=ProgramType.valueOfSchedulableType(programInfo.getProgramType());
    ProgramRunner runner=programRunnerFactory.create(programType);
    try {
      if (runner instanceof DistributedProgramRunner) {
        ProgramId programId=program.getId().getParent().program(programType,programInfo.getProgramName());
        ((DistributedProgramRunner)runner).setupLaunchConfig(launchConfig,Programs.create(cConf,program,programId,runner),options,cConf,hConf,tempDir);
        acceptors.add(launchConfig.getClassAcceptor());
      }
    }
  finally {
      if (runner instanceof Closeable) {
        Closeables.closeQuietly((Closeable)runner);
      }
    }
  }
  launchConfig.setClassAcceptor(new AndClassAcceptor(acceptors));
  launchConfig.clearRunnables();
  Resources resources=findDriverResources(program.getApplicationSpecification().getSpark(),program.getApplicationSpecification().getMapReduce(),spec);
  resources=SystemArguments.getResources(options.getUserArguments(),resources);
  launchConfig.addRunnable(spec.getName(),new WorkflowTwillRunnable(spec.getName()),resources,1,0);
}"
4935,"public RunnableResource(TwillRunnable runnable,ResourceSpecification resources){
  this.runnable=runnable;
  this.resources=resources;
}","public RunnableResource(TwillRunnable runnable,ResourceSpecification resources,@Nullable Integer maxRetries){
  this.runnable=runnable;
  this.resources=resources;
  this.maxRetries=maxRetries;
}"
4936,"protected Supplier<TransactionStateCache> getTransactionStateCacheSupplier(HTableDescriptor htd,Configuration conf){
  String tablePrefix=htd.getValue(Constants.Dataset.TABLE_PREFIX);
  String sysConfigTablePrefix=HTableNameConverter.getSysConfigTablePrefix(tablePrefix);
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,conf);
}","protected CacheSupplier<TransactionStateCache> getTransactionStateCacheSupplier(HTableDescriptor htd,Configuration conf){
  String tablePrefix=htd.getValue(Constants.Dataset.TABLE_PREFIX);
  String sysConfigTablePrefix=HTableNameConverter.getSysConfigTablePrefix(tablePrefix);
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,conf);
}"
4937,"public void initFamily(byte[] familyName,Map<byte[],byte[]> familyValues){
  String familyAsString=Bytes.toString(familyName);
  byte[] transactionalConfig=familyValues.get(Bytes.toBytes(IncrementHandlerState.PROPERTY_TRANSACTIONAL));
  boolean txnl=transactionalConfig == null || !""String_Node_Str"".equals(Bytes.toString(transactionalConfig));
  LOG.info(""String_Node_Str"" + familyAsString + ""String_Node_Str""+ txnl);
  if (txnl) {
    txnlFamilies.add(familyName);
  }
  byte[] columnTTL=familyValues.get(Bytes.toBytes(TxConstants.PROPERTY_TTL));
  long ttl=0;
  if (columnTTL != null) {
    try {
      String stringTTL=Bytes.toString(columnTTL);
      ttl=Long.parseLong(stringTTL);
      LOG.info(""String_Node_Str"" + familyAsString + ""String_Node_Str""+ ttl);
    }
 catch (    NumberFormatException nfe) {
      LOG.warn(""String_Node_Str"" + familyAsString + ""String_Node_Str""+ Bytes.toStringBinary(columnTTL));
    }
  }
  ttlByFamily.put(familyName,ttl);
  if (!txnlFamilies.isEmpty() && cache == null) {
    Supplier<TransactionStateCache> cacheSupplier=getTransactionStateCacheSupplier(hTableDescriptor,conf);
    this.cache=cacheSupplier.get();
  }
}","public void initFamily(byte[] familyName,Map<byte[],byte[]> familyValues){
  String familyAsString=Bytes.toString(familyName);
  byte[] transactionalConfig=familyValues.get(Bytes.toBytes(IncrementHandlerState.PROPERTY_TRANSACTIONAL));
  boolean txnl=transactionalConfig == null || !""String_Node_Str"".equals(Bytes.toString(transactionalConfig));
  LOG.info(""String_Node_Str"" + familyAsString + ""String_Node_Str""+ txnl);
  if (txnl) {
    txnlFamilies.add(familyName);
  }
  byte[] columnTTL=familyValues.get(Bytes.toBytes(TxConstants.PROPERTY_TTL));
  long ttl=0;
  if (columnTTL != null) {
    try {
      String stringTTL=Bytes.toString(columnTTL);
      ttl=Long.parseLong(stringTTL);
      LOG.info(""String_Node_Str"" + familyAsString + ""String_Node_Str""+ ttl);
    }
 catch (    NumberFormatException nfe) {
      LOG.warn(""String_Node_Str"" + familyAsString + ""String_Node_Str""+ Bytes.toStringBinary(columnTTL));
    }
  }
  ttlByFamily.put(familyName,ttl);
  if (!txnlFamilies.isEmpty() && cache == null) {
    txStateCacheSupplier=getTransactionStateCacheSupplier(hTableDescriptor,conf);
    cache=txStateCacheSupplier.get();
  }
}"
4938,"private void startRefreshThread(){
  refreshThread=new Thread(""String_Node_Str""){
    @Override public void run(){
      while (!isInterrupted()) {
        updateConfig();
        long now=System.currentTimeMillis();
        if (now > (lastUpdated + configCacheUpdateFrequency)) {
          try {
            updateCache();
          }
 catch (          TableNotFoundException e) {
            LOG.warn(""String_Node_Str"",queueConfigTableName,e);
            break;
          }
catch (          IOException e) {
            LOG.warn(""String_Node_Str"",e);
          }
        }
        try {
          Thread.sleep(1000);
        }
 catch (        InterruptedException ie) {
          interrupt();
          break;
        }
      }
      LOG.info(""String_Node_Str"",queueConfigTableName);
      INSTANCES.remove(queueConfigTableName,this);
    }
  }
;
  refreshThread.setDaemon(true);
  refreshThread.start();
}","private void startRefreshThread(){
  refreshThread=new Thread(""String_Node_Str""){
    @Override public void run(){
      while (!isInterrupted() && !stopped) {
        updateConfig();
        long now=System.currentTimeMillis();
        if (now > (lastUpdated + configCacheUpdateFrequency)) {
          try {
            updateCache();
          }
 catch (          TableNotFoundException e) {
            LOG.warn(""String_Node_Str"",queueConfigTableName,e);
            break;
          }
catch (          IOException e) {
            LOG.warn(""String_Node_Str"",e);
          }
        }
        try {
          TimeUnit.SECONDS.sleep(1);
        }
 catch (        InterruptedException ie) {
          interrupt();
          break;
        }
      }
      LOG.info(""String_Node_Str"",queueConfigTableName);
    }
  }
;
  refreshThread.setDaemon(true);
  refreshThread.start();
}"
4939,"@Override public void run(){
  while (!isInterrupted()) {
    updateConfig();
    long now=System.currentTimeMillis();
    if (now > (lastUpdated + configCacheUpdateFrequency)) {
      try {
        updateCache();
      }
 catch (      TableNotFoundException e) {
        LOG.warn(""String_Node_Str"",queueConfigTableName,e);
        break;
      }
catch (      IOException e) {
        LOG.warn(""String_Node_Str"",e);
      }
    }
    try {
      Thread.sleep(1000);
    }
 catch (    InterruptedException ie) {
      interrupt();
      break;
    }
  }
  LOG.info(""String_Node_Str"",queueConfigTableName);
  INSTANCES.remove(queueConfigTableName,this);
}","@Override public void run(){
  while (!isInterrupted() && !stopped) {
    updateConfig();
    long now=System.currentTimeMillis();
    if (now > (lastUpdated + configCacheUpdateFrequency)) {
      try {
        updateCache();
      }
 catch (      TableNotFoundException e) {
        LOG.warn(""String_Node_Str"",queueConfigTableName,e);
        break;
      }
catch (      IOException e) {
        LOG.warn(""String_Node_Str"",e);
      }
    }
    try {
      TimeUnit.SECONDS.sleep(1);
    }
 catch (    InterruptedException ie) {
      interrupt();
      break;
    }
  }
  LOG.info(""String_Node_Str"",queueConfigTableName);
}"
4940,"@Override public int hashCode(){
  return hashCode;
}","@Override public int hashCode(){
  if (hashCode != 0) {
    return hashCode;
  }
  hashCode=Objects.hash(name,type,kerberosPrincipal);
  return hashCode;
}"
4941,"public Principal(String name,PrincipalType type,@Nullable String kerberosPrincipal){
  this.name=name;
  this.type=type;
  this.kerberosPrincipal=kerberosPrincipal;
  this.hashCode=Objects.hash(name,type,kerberosPrincipal);
}","public Principal(String name,PrincipalType type,@Nullable String kerberosPrincipal){
  this.name=name;
  this.type=type;
  this.kerberosPrincipal=kerberosPrincipal;
}"
4942,"private boolean checkJobConstraints() throws Exception {
  boolean emptyScan=true;
  try (CloseableIterator<Job> jobQueueIter=jobQueue.getJobs(partition,lastConsumed)){
    Stopwatch stopWatch=new Stopwatch().start();
    while (!stopping && stopWatch.elapsedMillis() < 1000) {
      if (!jobQueueIter.hasNext()) {
        jobQueueIter.close();
        lastConsumed=null;
        return emptyScan;
      }
      Job job=jobQueueIter.next();
      lastConsumed=job;
      emptyScan=false;
      checkAndUpdateJob(jobQueue,job);
    }
  }
   return emptyScan;
}","private boolean checkJobConstraints() throws Exception {
  boolean emptyScan=true;
  try (CloseableIterator<Job> jobQueueIter=jobQueue.getJobs(partition,lastConsumed)){
    Stopwatch stopWatch=new Stopwatch().start();
    while (!stopping && stopWatch.elapsedMillis() < 1000) {
      if (!jobQueueIter.hasNext()) {
        lastConsumed=null;
        return emptyScan;
      }
      Job job=jobQueueIter.next();
      lastConsumed=job;
      emptyScan=false;
      checkAndUpdateJob(jobQueue,job);
    }
  }
   return emptyScan;
}"
4943,"/** 
 * Collects HBase table stats //TODO: Explore the possiblitity of returning a   {@code Map<TableId, TableStats>}
 * @param admin instance of {@link HBaseAdmin} to communicate with HBase
 * @return map of table name -> table stats
 * @throws IOException
 */
public Map<TableId,TableStats> getTableStats(HBaseAdmin admin) throws IOException {
  Map<TableId,TableStats> datasetStat=Maps.newHashMap();
  ClusterStatus clusterStatus=admin.getClusterStatus();
  for (  ServerName serverName : clusterStatus.getServers()) {
    Map<byte[],RegionLoad> regionsLoad=clusterStatus.getLoad(serverName).getRegionsLoad();
    for (    RegionLoad regionLoad : regionsLoad.values()) {
      TableName tableName=HRegionInfo.getTable(regionLoad.getName());
      HTableDescriptor tableDescriptor=admin.getTableDescriptor(tableName);
      if (!isCDAPTable(tableDescriptor)) {
        continue;
      }
      TableId tableId=HTableNameConverter.from(tableDescriptor);
      TableStats stat=datasetStat.get(tableId);
      if (stat == null) {
        stat=new TableStats(regionLoad.getStorefileSizeMB(),regionLoad.getMemStoreSizeMB());
        datasetStat.put(tableId,stat);
      }
 else {
        stat.incStoreFileSizeMB(regionLoad.getStorefileSizeMB());
        stat.incMemStoreSizeMB(regionLoad.getMemStoreSizeMB());
      }
    }
  }
  return datasetStat;
}","/** 
 * Collects HBase table stats //TODO: Explore the possiblitity of returning a   {@code Map<TableId, TableStats>}
 * @param admin instance of {@link HBaseAdmin} to communicate with HBase
 * @return map of table name -> table stats
 * @throws IOException
 */
public Map<TableId,TableStats> getTableStats(HBaseAdmin admin) throws IOException {
  Map<TableId,TableStats> datasetStat=Maps.newHashMap();
  ClusterStatus clusterStatus=admin.getClusterStatus();
  for (  ServerName serverName : clusterStatus.getServers()) {
    Map<byte[],RegionLoad> regionsLoad=clusterStatus.getLoad(serverName).getRegionsLoad();
    for (    RegionLoad regionLoad : regionsLoad.values()) {
      TableName tableName=HRegionInfo.getTable(regionLoad.getName());
      HTableDescriptor tableDescriptor;
      try {
        tableDescriptor=admin.getTableDescriptor(tableName);
      }
 catch (      TableNotFoundException exception) {
        LOG.warn(""String_Node_Str"",tableName,exception.getMessage());
        continue;
      }
      if (!isCDAPTable(tableDescriptor)) {
        continue;
      }
      TableId tableId=HTableNameConverter.from(tableDescriptor);
      TableStats stat=datasetStat.get(tableId);
      if (stat == null) {
        stat=new TableStats(regionLoad.getStorefileSizeMB(),regionLoad.getMemStoreSizeMB());
        datasetStat.put(tableId,stat);
      }
 else {
        stat.incStoreFileSizeMB(regionLoad.getStorefileSizeMB());
        stat.incMemStoreSizeMB(regionLoad.getMemStoreSizeMB());
      }
    }
  }
  return datasetStat;
}"
4944,"/** 
 * Start the service.
 */
public void startUp() throws Exception {
  URLConnections.setDefaultUseCaches(false);
  cleanupTempDir();
  ConfigurationLogger.logImportantConfig(cConf);
  if (messagingService instanceof Service) {
    ((Service)messagingService).startAndWait();
  }
  injector.getInstance(MessagingHttpService.class).startAndWait();
  authorizationBootstrapper.run();
  txService.startAndWait();
  metricsCollectionService.startAndWait();
  datasetService.startAndWait();
  serviceStore.startAndWait();
  streamService.startAndWait();
  new LogPipelineLoader(cConf).validate();
  logAppenderInitializer.initialize();
  Service.State state=appFabricServer.startAndWait();
  if (state != Service.State.RUNNING) {
    throw new Exception(""String_Node_Str"");
  }
  metricsQueryService.startAndWait();
  router.startAndWait();
  if (userInterfaceService != null) {
    userInterfaceService.startAndWait();
  }
  if (securityEnabled) {
    externalAuthenticationServer.startAndWait();
  }
  if (exploreExecutorService != null) {
    exploreExecutorService.startAndWait();
  }
  metadataService.startAndWait();
  if (trackerAppCreationService != null) {
    trackerAppCreationService.startAndWait();
  }
  remoteSystemOperationsService.startAndWait();
  operationalStatsService.startAndWait();
  String protocol=sslEnabled ? ""String_Node_Str"" : ""String_Node_Str"";
  int dashboardPort=sslEnabled ? cConf.getInt(Constants.Dashboard.SSL_BIND_PORT) : cConf.getInt(Constants.Dashboard.BIND_PORT);
  System.out.println(""String_Node_Str"");
  System.out.printf(""String_Node_Str"",protocol,""String_Node_Str"",dashboardPort);
}","/** 
 * Start the service.
 */
public void startUp() throws Exception {
  URLConnections.setDefaultUseCaches(false);
  cleanupTempDir();
  ConfigurationLogger.logImportantConfig(cConf);
  if (messagingService instanceof Service) {
    ((Service)messagingService).startAndWait();
  }
  injector.getInstance(MessagingHttpService.class).startAndWait();
  authorizationBootstrapper.run();
  txService.startAndWait();
  metricsCollectionService.startAndWait();
  datasetService.startAndWait();
  serviceStore.startAndWait();
  streamService.startAndWait();
  new LogPipelineLoader(cConf).validate();
  logAppenderInitializer.initialize();
  Service.State state=appFabricServer.startAndWait();
  if (state != Service.State.RUNNING) {
    throw new Exception(""String_Node_Str"");
  }
  metricsQueryService.startAndWait();
  router.startAndWait();
  if (userInterfaceService != null) {
    userInterfaceService.startAndWait();
  }
  if (securityEnabled) {
    externalAuthenticationServer.startAndWait();
  }
  if (exploreExecutorService != null) {
    exploreExecutorService.startAndWait();
  }
  metadataService.startAndWait();
  if (trackerAppCreationService != null) {
    trackerAppCreationService.startAndWait();
  }
  wranglerAppCreationService.startAndWait();
  remoteSystemOperationsService.startAndWait();
  operationalStatsService.startAndWait();
  String protocol=sslEnabled ? ""String_Node_Str"" : ""String_Node_Str"";
  int dashboardPort=sslEnabled ? cConf.getInt(Constants.Dashboard.SSL_BIND_PORT) : cConf.getInt(Constants.Dashboard.BIND_PORT);
  System.out.println(""String_Node_Str"");
  System.out.printf(""String_Node_Str"",protocol,""String_Node_Str"",dashboardPort);
}"
4945,"private StandaloneMain(List<Module> modules,CConfiguration cConf){
  Thread.setDefaultUncaughtExceptionHandler(new UncaughtExceptionHandler());
  this.cConf=cConf;
  injector=Guice.createInjector(modules);
  if (cConf.getBoolean(Constants.Audit.ENABLED)) {
    trackerAppCreationService=injector.getInstance(TrackerAppCreationService.class);
  }
 else {
    trackerAppCreationService=null;
  }
  messagingService=injector.getInstance(MessagingService.class);
  authorizerInstantiator=injector.getInstance(AuthorizerInstantiator.class);
  authorizationBootstrapper=injector.getInstance(AuthorizationBootstrapper.class);
  txService=injector.getInstance(InMemoryTransactionService.class);
  router=injector.getInstance(NettyRouter.class);
  metricsQueryService=injector.getInstance(MetricsQueryService.class);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
  metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  datasetService=injector.getInstance(DatasetService.class);
  serviceStore=injector.getInstance(ServiceStore.class);
  streamService=injector.getInstance(StreamService.class);
  operationalStatsService=injector.getInstance(OperationalStatsService.class);
  if (cConf.getBoolean(DISABLE_UI,false)) {
    userInterfaceService=null;
  }
 else {
    userInterfaceService=injector.getInstance(UserInterfaceService.class);
  }
  sslEnabled=cConf.getBoolean(Constants.Security.SSL.EXTERNAL_ENABLED);
  securityEnabled=cConf.getBoolean(Constants.Security.ENABLED);
  if (securityEnabled) {
    externalAuthenticationServer=injector.getInstance(ExternalAuthenticationServer.class);
  }
  boolean exploreEnabled=cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED);
  if (exploreEnabled) {
    ExploreServiceUtils.checkHiveSupport(getClass().getClassLoader());
    exploreExecutorService=injector.getInstance(ExploreExecutorService.class);
  }
  exploreClient=injector.getInstance(ExploreClient.class);
  metadataService=injector.getInstance(MetadataService.class);
  remoteSystemOperationsService=injector.getInstance(RemoteSystemOperationsService.class);
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        shutDown();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
        System.err.println(""String_Node_Str"" + e.getMessage());
        e.printStackTrace(System.err);
      }
    }
  }
);
}","private StandaloneMain(List<Module> modules,CConfiguration cConf){
  Thread.setDefaultUncaughtExceptionHandler(new UncaughtExceptionHandler());
  this.cConf=cConf;
  injector=Guice.createInjector(modules);
  if (cConf.getBoolean(Constants.Audit.ENABLED)) {
    trackerAppCreationService=injector.getInstance(TrackerAppCreationService.class);
  }
 else {
    trackerAppCreationService=null;
  }
  wranglerAppCreationService=injector.getInstance(WranglerAppCreationService.class);
  messagingService=injector.getInstance(MessagingService.class);
  authorizerInstantiator=injector.getInstance(AuthorizerInstantiator.class);
  authorizationBootstrapper=injector.getInstance(AuthorizationBootstrapper.class);
  txService=injector.getInstance(InMemoryTransactionService.class);
  router=injector.getInstance(NettyRouter.class);
  metricsQueryService=injector.getInstance(MetricsQueryService.class);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
  metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  datasetService=injector.getInstance(DatasetService.class);
  serviceStore=injector.getInstance(ServiceStore.class);
  streamService=injector.getInstance(StreamService.class);
  operationalStatsService=injector.getInstance(OperationalStatsService.class);
  if (cConf.getBoolean(DISABLE_UI,false)) {
    userInterfaceService=null;
  }
 else {
    userInterfaceService=injector.getInstance(UserInterfaceService.class);
  }
  sslEnabled=cConf.getBoolean(Constants.Security.SSL.EXTERNAL_ENABLED);
  securityEnabled=cConf.getBoolean(Constants.Security.ENABLED);
  if (securityEnabled) {
    externalAuthenticationServer=injector.getInstance(ExternalAuthenticationServer.class);
  }
  boolean exploreEnabled=cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED);
  if (exploreEnabled) {
    ExploreServiceUtils.checkHiveSupport(getClass().getClassLoader());
    exploreExecutorService=injector.getInstance(ExploreExecutorService.class);
  }
  exploreClient=injector.getInstance(ExploreClient.class);
  metadataService=injector.getInstance(MetadataService.class);
  remoteSystemOperationsService=injector.getInstance(RemoteSystemOperationsService.class);
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        shutDown();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
        System.err.println(""String_Node_Str"" + e.getMessage());
        e.printStackTrace(System.err);
      }
    }
  }
);
}"
4946,"/** 
 * Shutdown the service.
 */
public void shutDown(){
  LOG.info(""String_Node_Str"");
  boolean halt=false;
  try {
    if (userInterfaceService != null) {
      userInterfaceService.stopAndWait();
    }
    if (trackerAppCreationService != null) {
      trackerAppCreationService.stopAndWait();
    }
    router.stopAndWait();
    operationalStatsService.stopAndWait();
    remoteSystemOperationsService.stopAndWait();
    streamService.stopAndWait();
    if (exploreExecutorService != null) {
      exploreExecutorService.stopAndWait();
    }
    exploreClient.close();
    metadataService.stopAndWait();
    serviceStore.stopAndWait();
    appFabricServer.stopAndWait();
    datasetService.stopAndWait();
    metricsQueryService.stopAndWait();
    txService.stopAndWait();
    if (securityEnabled) {
      externalAuthenticationServer.stopAndWait();
    }
    injector.getInstance(MessagingHttpService.class).startAndWait();
    if (messagingService instanceof Service) {
      ((Service)messagingService).stopAndWait();
    }
    logAppenderInitializer.close();
    authorizerInstantiator.close();
  }
 catch (  Throwable e) {
    halt=true;
    LOG.error(""String_Node_Str"",e);
  }
 finally {
    cleanupTempDir();
  }
  if (halt) {
    Runtime.getRuntime().halt(1);
  }
}","/** 
 * Shutdown the service.
 */
public void shutDown(){
  LOG.info(""String_Node_Str"");
  boolean halt=false;
  try {
    if (userInterfaceService != null) {
      userInterfaceService.stopAndWait();
    }
    wranglerAppCreationService.stopAndWait();
    if (trackerAppCreationService != null) {
      trackerAppCreationService.stopAndWait();
    }
    router.stopAndWait();
    operationalStatsService.stopAndWait();
    remoteSystemOperationsService.stopAndWait();
    streamService.stopAndWait();
    if (exploreExecutorService != null) {
      exploreExecutorService.stopAndWait();
    }
    exploreClient.close();
    metadataService.stopAndWait();
    serviceStore.stopAndWait();
    appFabricServer.stopAndWait();
    datasetService.stopAndWait();
    metricsQueryService.stopAndWait();
    txService.stopAndWait();
    if (securityEnabled) {
      externalAuthenticationServer.stopAndWait();
    }
    injector.getInstance(MessagingHttpService.class).startAndWait();
    if (messagingService instanceof Service) {
      ((Service)messagingService).stopAndWait();
    }
    logAppenderInitializer.close();
    authorizerInstantiator.close();
  }
 catch (  Throwable e) {
    halt=true;
    LOG.error(""String_Node_Str"",e);
  }
 finally {
    cleanupTempDir();
  }
  if (halt) {
    Runtime.getRuntime().halt(1);
  }
}"
4947,"@Inject public TrackerAppCreationService(CConfiguration cConf,ArtifactRepository artifactRepository,ApplicationLifecycleService applicationLifecycleService,ProgramLifecycleService programLifecycleService){
  this.cConf=cConf;
  this.artifactRepository=artifactRepository;
  this.applicationLifecycleService=applicationLifecycleService;
  this.programLifecycleService=programLifecycleService;
}","@Inject public TrackerAppCreationService(CConfiguration cConf,ArtifactRepository artifactRepository,ApplicationLifecycleService applicationLifecycleService,ProgramLifecycleService programLifecycleService){
  super(artifactRepository,applicationLifecycleService,programLifecycleService,""String_Node_Str"",TRACKER_APPID,PROGRAM_ID_MAP,cConf.get(TRACKER_CONFIG,""String_Node_Str""));
}"
4948,"/** 
 * Start the service.
 */
public void startUp() throws Exception {
  URLConnections.setDefaultUseCaches(false);
  cleanupTempDir();
  ConfigurationLogger.logImportantConfig(cConf);
  if (messagingService instanceof Service) {
    ((Service)messagingService).startAndWait();
  }
  injector.getInstance(MessagingHttpService.class).startAndWait();
  authorizationBootstrapper.run();
  txService.startAndWait();
  metricsCollectionService.startAndWait();
  datasetService.startAndWait();
  serviceStore.startAndWait();
  streamService.startAndWait();
  new LogPipelineLoader(cConf).validate();
  logAppenderInitializer.initialize();
  Service.State state=appFabricServer.startAndWait();
  if (state != Service.State.RUNNING) {
    throw new Exception(""String_Node_Str"");
  }
  metricsQueryService.startAndWait();
  router.startAndWait();
  if (userInterfaceService != null) {
    userInterfaceService.startAndWait();
  }
  if (securityEnabled) {
    externalAuthenticationServer.startAndWait();
  }
  if (exploreExecutorService != null) {
    exploreExecutorService.startAndWait();
  }
  metadataService.startAndWait();
  if (trackerAppCreationService != null) {
    trackerAppCreationService.startAndWait();
  }
  remoteSystemOperationsService.startAndWait();
  operationalStatsService.startAndWait();
  String protocol=sslEnabled ? ""String_Node_Str"" : ""String_Node_Str"";
  int dashboardPort=sslEnabled ? cConf.getInt(Constants.Dashboard.SSL_BIND_PORT) : cConf.getInt(Constants.Dashboard.BIND_PORT);
  System.out.println(""String_Node_Str"");
  System.out.printf(""String_Node_Str"",protocol,""String_Node_Str"",dashboardPort);
}","/** 
 * Start the service.
 */
public void startUp() throws Exception {
  URLConnections.setDefaultUseCaches(false);
  cleanupTempDir();
  ConfigurationLogger.logImportantConfig(cConf);
  if (messagingService instanceof Service) {
    ((Service)messagingService).startAndWait();
  }
  injector.getInstance(MessagingHttpService.class).startAndWait();
  authorizationBootstrapper.run();
  txService.startAndWait();
  metricsCollectionService.startAndWait();
  datasetService.startAndWait();
  serviceStore.startAndWait();
  streamService.startAndWait();
  new LogPipelineLoader(cConf).validate();
  logAppenderInitializer.initialize();
  Service.State state=appFabricServer.startAndWait();
  if (state != Service.State.RUNNING) {
    throw new Exception(""String_Node_Str"");
  }
  metricsQueryService.startAndWait();
  router.startAndWait();
  if (userInterfaceService != null) {
    userInterfaceService.startAndWait();
  }
  if (securityEnabled) {
    externalAuthenticationServer.startAndWait();
  }
  if (exploreExecutorService != null) {
    exploreExecutorService.startAndWait();
  }
  metadataService.startAndWait();
  if (trackerAppCreationService != null) {
    trackerAppCreationService.startAndWait();
  }
  wranglerAppCreationService.startAndWait();
  remoteSystemOperationsService.startAndWait();
  operationalStatsService.startAndWait();
  String protocol=sslEnabled ? ""String_Node_Str"" : ""String_Node_Str"";
  int dashboardPort=sslEnabled ? cConf.getInt(Constants.Dashboard.SSL_BIND_PORT) : cConf.getInt(Constants.Dashboard.BIND_PORT);
  System.out.println(""String_Node_Str"");
  System.out.printf(""String_Node_Str"",protocol,""String_Node_Str"",dashboardPort);
}"
4949,"private StandaloneMain(List<Module> modules,CConfiguration cConf){
  Thread.setDefaultUncaughtExceptionHandler(new UncaughtExceptionHandler());
  this.cConf=cConf;
  injector=Guice.createInjector(modules);
  if (cConf.getBoolean(Constants.Audit.ENABLED)) {
    trackerAppCreationService=injector.getInstance(TrackerAppCreationService.class);
  }
 else {
    trackerAppCreationService=null;
  }
  messagingService=injector.getInstance(MessagingService.class);
  authorizerInstantiator=injector.getInstance(AuthorizerInstantiator.class);
  authorizationBootstrapper=injector.getInstance(AuthorizationBootstrapper.class);
  txService=injector.getInstance(InMemoryTransactionService.class);
  router=injector.getInstance(NettyRouter.class);
  metricsQueryService=injector.getInstance(MetricsQueryService.class);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
  metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  datasetService=injector.getInstance(DatasetService.class);
  serviceStore=injector.getInstance(ServiceStore.class);
  streamService=injector.getInstance(StreamService.class);
  operationalStatsService=injector.getInstance(OperationalStatsService.class);
  if (cConf.getBoolean(DISABLE_UI,false)) {
    userInterfaceService=null;
  }
 else {
    userInterfaceService=injector.getInstance(UserInterfaceService.class);
  }
  sslEnabled=cConf.getBoolean(Constants.Security.SSL.EXTERNAL_ENABLED);
  securityEnabled=cConf.getBoolean(Constants.Security.ENABLED);
  if (securityEnabled) {
    externalAuthenticationServer=injector.getInstance(ExternalAuthenticationServer.class);
  }
  boolean exploreEnabled=cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED);
  if (exploreEnabled) {
    ExploreServiceUtils.checkHiveSupport(getClass().getClassLoader());
    exploreExecutorService=injector.getInstance(ExploreExecutorService.class);
  }
  exploreClient=injector.getInstance(ExploreClient.class);
  metadataService=injector.getInstance(MetadataService.class);
  remoteSystemOperationsService=injector.getInstance(RemoteSystemOperationsService.class);
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        shutDown();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
        System.err.println(""String_Node_Str"" + e.getMessage());
        e.printStackTrace(System.err);
      }
    }
  }
);
}","private StandaloneMain(List<Module> modules,CConfiguration cConf){
  Thread.setDefaultUncaughtExceptionHandler(new UncaughtExceptionHandler());
  this.cConf=cConf;
  injector=Guice.createInjector(modules);
  if (cConf.getBoolean(Constants.Audit.ENABLED)) {
    trackerAppCreationService=injector.getInstance(TrackerAppCreationService.class);
  }
 else {
    trackerAppCreationService=null;
  }
  wranglerAppCreationService=injector.getInstance(WranglerAppCreationService.class);
  messagingService=injector.getInstance(MessagingService.class);
  authorizerInstantiator=injector.getInstance(AuthorizerInstantiator.class);
  authorizationBootstrapper=injector.getInstance(AuthorizationBootstrapper.class);
  txService=injector.getInstance(InMemoryTransactionService.class);
  router=injector.getInstance(NettyRouter.class);
  metricsQueryService=injector.getInstance(MetricsQueryService.class);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
  metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  datasetService=injector.getInstance(DatasetService.class);
  serviceStore=injector.getInstance(ServiceStore.class);
  streamService=injector.getInstance(StreamService.class);
  operationalStatsService=injector.getInstance(OperationalStatsService.class);
  if (cConf.getBoolean(DISABLE_UI,false)) {
    userInterfaceService=null;
  }
 else {
    userInterfaceService=injector.getInstance(UserInterfaceService.class);
  }
  sslEnabled=cConf.getBoolean(Constants.Security.SSL.EXTERNAL_ENABLED);
  securityEnabled=cConf.getBoolean(Constants.Security.ENABLED);
  if (securityEnabled) {
    externalAuthenticationServer=injector.getInstance(ExternalAuthenticationServer.class);
  }
  boolean exploreEnabled=cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED);
  if (exploreEnabled) {
    ExploreServiceUtils.checkHiveSupport(getClass().getClassLoader());
    exploreExecutorService=injector.getInstance(ExploreExecutorService.class);
  }
  exploreClient=injector.getInstance(ExploreClient.class);
  metadataService=injector.getInstance(MetadataService.class);
  remoteSystemOperationsService=injector.getInstance(RemoteSystemOperationsService.class);
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        shutDown();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
        System.err.println(""String_Node_Str"" + e.getMessage());
        e.printStackTrace(System.err);
      }
    }
  }
);
}"
4950,"/** 
 * Shutdown the service.
 */
public void shutDown(){
  LOG.info(""String_Node_Str"");
  boolean halt=false;
  try {
    if (userInterfaceService != null) {
      userInterfaceService.stopAndWait();
    }
    if (trackerAppCreationService != null) {
      trackerAppCreationService.stopAndWait();
    }
    router.stopAndWait();
    operationalStatsService.stopAndWait();
    remoteSystemOperationsService.stopAndWait();
    streamService.stopAndWait();
    if (exploreExecutorService != null) {
      exploreExecutorService.stopAndWait();
    }
    exploreClient.close();
    metadataService.stopAndWait();
    serviceStore.stopAndWait();
    appFabricServer.stopAndWait();
    datasetService.stopAndWait();
    metricsQueryService.stopAndWait();
    txService.stopAndWait();
    if (securityEnabled) {
      externalAuthenticationServer.stopAndWait();
    }
    injector.getInstance(MessagingHttpService.class).startAndWait();
    if (messagingService instanceof Service) {
      ((Service)messagingService).stopAndWait();
    }
    logAppenderInitializer.close();
    authorizerInstantiator.close();
  }
 catch (  Throwable e) {
    halt=true;
    LOG.error(""String_Node_Str"",e);
  }
 finally {
    cleanupTempDir();
  }
  if (halt) {
    Runtime.getRuntime().halt(1);
  }
}","/** 
 * Shutdown the service.
 */
public void shutDown(){
  LOG.info(""String_Node_Str"");
  boolean halt=false;
  try {
    if (userInterfaceService != null) {
      userInterfaceService.stopAndWait();
    }
    wranglerAppCreationService.stopAndWait();
    if (trackerAppCreationService != null) {
      trackerAppCreationService.stopAndWait();
    }
    router.stopAndWait();
    operationalStatsService.stopAndWait();
    remoteSystemOperationsService.stopAndWait();
    streamService.stopAndWait();
    if (exploreExecutorService != null) {
      exploreExecutorService.stopAndWait();
    }
    exploreClient.close();
    metadataService.stopAndWait();
    serviceStore.stopAndWait();
    appFabricServer.stopAndWait();
    datasetService.stopAndWait();
    metricsQueryService.stopAndWait();
    txService.stopAndWait();
    if (securityEnabled) {
      externalAuthenticationServer.stopAndWait();
    }
    injector.getInstance(MessagingHttpService.class).startAndWait();
    if (messagingService instanceof Service) {
      ((Service)messagingService).stopAndWait();
    }
    logAppenderInitializer.close();
    authorizerInstantiator.close();
  }
 catch (  Throwable e) {
    halt=true;
    LOG.error(""String_Node_Str"",e);
  }
 finally {
    cleanupTempDir();
  }
  if (halt) {
    Runtime.getRuntime().halt(1);
  }
}"
4951,"@Inject public TrackerAppCreationService(CConfiguration cConf,ArtifactRepository artifactRepository,ApplicationLifecycleService applicationLifecycleService,ProgramLifecycleService programLifecycleService){
  this.cConf=cConf;
  this.artifactRepository=artifactRepository;
  this.applicationLifecycleService=applicationLifecycleService;
  this.programLifecycleService=programLifecycleService;
}","@Inject public TrackerAppCreationService(CConfiguration cConf,ArtifactRepository artifactRepository,ApplicationLifecycleService applicationLifecycleService,ProgramLifecycleService programLifecycleService){
  super(artifactRepository,applicationLifecycleService,programLifecycleService,""String_Node_Str"",TRACKER_APPID,PROGRAM_ID_MAP,cConf.get(TRACKER_CONFIG,""String_Node_Str""));
}"
4952,"@Ignore @Test public void testWorkflowSchedules() throws Exception {
  String appName=""String_Node_Str"";
  String workflowName=""String_Node_Str"";
  String sampleSchedule=""String_Node_Str"";
  HttpResponse response=deploy(AppWithSchedule.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,appName,ProgramType.WORKFLOW,workflowName);
  Map<String,String> runtimeArguments=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  setAndTestRuntimeArgs(programId,runtimeArguments);
  List<ScheduleDetail> schedules=getSchedules(TEST_NAMESPACE2,appName,workflowName);
  Assert.assertEquals(1,schedules.size());
  String scheduleName=schedules.get(0).getName();
  Assert.assertFalse(scheduleName.isEmpty());
  List<ScheduleSpecification> specs=getScheduleSpecs(TEST_NAMESPACE2,appName,workflowName);
  Assert.assertEquals(1,specs.size());
  String specName=specs.get(0).getSchedule().getName();
  Assert.assertEquals(scheduleName,specName);
  long current=System.currentTimeMillis();
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,sampleSchedule));
  assertSchedule(programId,scheduleName,true,30,TimeUnit.SECONDS);
  List<ScheduledRuntime> runtimes=getScheduledRunTime(programId,true);
  String id=runtimes.get(0).getId();
  Assert.assertTrue(String.format(""String_Node_Str"",id,scheduleName),id.contains(scheduleName));
  Long nextRunTime=runtimes.get(0).getTime();
  Assert.assertTrue(String.format(""String_Node_Str"",nextRunTime,current),nextRunTime > current);
  verifyProgramRuns(programId,""String_Node_Str"");
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName));
  assertSchedule(programId,scheduleName,false,30,TimeUnit.SECONDS);
  List<ScheduledRuntime> previousRuntimes=getScheduledRunTime(programId,false);
  int numRuns=previousRuntimes.size();
  Assert.assertTrue(String.format(""String_Node_Str"" + ""String_Node_Str"",numRuns),numRuns >= 1);
  verifyNoRunWithStatus(programId,""String_Node_Str"");
  int workflowRuns=getProgramRuns(programId,""String_Node_Str"").size();
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,scheduleName));
  assertSchedule(programId,scheduleName,true,30,TimeUnit.SECONDS);
  verifyProgramRuns(programId,""String_Node_Str"",workflowRuns);
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName));
  assertSchedule(programId,scheduleName,false,30,TimeUnit.SECONDS);
  try {
    assertSchedule(programId,""String_Node_Str"",true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  try {
    assertSchedule(Id.Program.from(TEST_NAMESPACE1,appName,ProgramType.WORKFLOW,workflowName),scheduleName,true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  Assert.assertEquals(404,suspendSchedule(TEST_NAMESPACE1,appName,scheduleName));
  Assert.assertEquals(404,resumeSchedule(TEST_NAMESPACE1,appName,scheduleName));
  verifyNoRunWithStatus(programId,""String_Node_Str"");
  deleteApp(Id.Application.from(TEST_NAMESPACE2,AppWithSchedule.class.getSimpleName()),200);
}","@Ignore @Test public void testWorkflowSchedules() throws Exception {
  String appName=AppWithSchedule.NAME;
  String workflowName=AppWithSchedule.WORKFLOW_NAME;
  String sampleSchedule=AppWithSchedule.SCHEDULE;
  HttpResponse response=deploy(AppWithSchedule.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,appName,ProgramType.WORKFLOW,workflowName);
  Map<String,String> runtimeArguments=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  setAndTestRuntimeArgs(programId,runtimeArguments);
  List<ScheduleDetail> schedules=getSchedules(TEST_NAMESPACE2,appName,workflowName);
  Assert.assertEquals(1,schedules.size());
  String scheduleName=schedules.get(0).getName();
  Assert.assertFalse(scheduleName.isEmpty());
  List<ScheduleSpecification> specs=getScheduleSpecs(TEST_NAMESPACE2,appName,workflowName);
  Assert.assertEquals(1,specs.size());
  String specName=specs.get(0).getSchedule().getName();
  Assert.assertEquals(scheduleName,specName);
  long current=System.currentTimeMillis();
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,sampleSchedule));
  assertSchedule(programId,scheduleName,true,30,TimeUnit.SECONDS);
  List<ScheduledRuntime> runtimes=getScheduledRunTime(programId,true);
  String id=runtimes.get(0).getId();
  Assert.assertTrue(String.format(""String_Node_Str"",id,scheduleName),id.contains(scheduleName));
  Long nextRunTime=runtimes.get(0).getTime();
  Assert.assertTrue(String.format(""String_Node_Str"",nextRunTime,current),nextRunTime > current);
  verifyProgramRuns(programId,""String_Node_Str"");
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName));
  assertSchedule(programId,scheduleName,false,30,TimeUnit.SECONDS);
  List<ScheduledRuntime> previousRuntimes=getScheduledRunTime(programId,false);
  int numRuns=previousRuntimes.size();
  Assert.assertTrue(String.format(""String_Node_Str"" + ""String_Node_Str"",numRuns),numRuns >= 1);
  verifyNoRunWithStatus(programId,""String_Node_Str"");
  int workflowRuns=getProgramRuns(programId,""String_Node_Str"").size();
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,scheduleName));
  assertSchedule(programId,scheduleName,true,30,TimeUnit.SECONDS);
  verifyProgramRuns(programId,""String_Node_Str"",workflowRuns);
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName));
  assertSchedule(programId,scheduleName,false,30,TimeUnit.SECONDS);
  try {
    assertSchedule(programId,""String_Node_Str"",true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  try {
    assertSchedule(Id.Program.from(TEST_NAMESPACE1,appName,ProgramType.WORKFLOW,workflowName),scheduleName,true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  Assert.assertEquals(404,suspendSchedule(TEST_NAMESPACE1,appName,scheduleName));
  Assert.assertEquals(404,resumeSchedule(TEST_NAMESPACE1,appName,scheduleName));
  verifyNoRunWithStatus(programId,""String_Node_Str"");
  deleteApp(Id.Application.from(TEST_NAMESPACE2,AppWithSchedule.class.getSimpleName()),200);
}"
4953,"@Test public void testStreamSizeSchedules() throws Exception {
  String appName=""String_Node_Str"";
  String sampleSchedule1=""String_Node_Str"";
  String sampleSchedule2=""String_Node_Str"";
  String workflowName=""String_Node_Str"";
  String streamName=""String_Node_Str"";
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,appName,ProgramType.WORKFLOW,workflowName);
  StringBuilder longStringBuilder=new StringBuilder();
  for (int i=0; i < 10000; i++) {
    longStringBuilder.append(""String_Node_Str"");
  }
  String longString=longStringBuilder.toString();
  HttpResponse response=deploy(AppWithStreamSizeSchedule.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,sampleSchedule1));
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,sampleSchedule2));
  List<ScheduleDetail> schedules=getSchedules(TEST_NAMESPACE2,appName,workflowName);
  Assert.assertEquals(2,schedules.size());
  String scheduleName1=schedules.get(0).getName();
  String scheduleName2=schedules.get(1).getName();
  Assert.assertNotNull(scheduleName1);
  Assert.assertFalse(scheduleName1.isEmpty());
  response=doPut(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName),""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=doGet(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName));
  String json=EntityUtils.toString(response.getEntity());
  StreamProperties properties=new Gson().fromJson(json,StreamProperties.class);
  Assert.assertEquals(1,properties.getNotificationThresholdMB().intValue());
  for (int i=0; i < 12; ++i) {
    response=doPost(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName),longString);
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  }
  verifyProgramRuns(programId,""String_Node_Str"");
  assertSchedule(programId,scheduleName1,true,30,TimeUnit.SECONDS);
  assertSchedule(programId,scheduleName2,true,30,TimeUnit.SECONDS);
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName1));
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName2));
  assertSchedule(programId,scheduleName1,false,30,TimeUnit.SECONDS);
  assertSchedule(programId,scheduleName2,false,30,TimeUnit.SECONDS);
  int workflowRuns=getProgramRuns(programId,""String_Node_Str"").size();
  Assert.assertEquals(1,workflowRuns);
  for (int i=0; i < 12; ++i) {
    response=doPost(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName),longString);
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  }
  TimeUnit.SECONDS.sleep(5);
  int workflowRunsAfterSuspend=getProgramRuns(programId,""String_Node_Str"").size();
  Assert.assertEquals(workflowRuns,workflowRunsAfterSuspend);
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,scheduleName1));
  assertRunHistory(programId,""String_Node_Str"",workflowRunsAfterSuspend,60,TimeUnit.SECONDS);
  assertSchedule(programId,scheduleName1,true,30,TimeUnit.SECONDS);
  try {
    assertSchedule(programId,""String_Node_Str"",true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName1));
  assertSchedule(programId,scheduleName1,false,30,TimeUnit.SECONDS);
  try {
    assertSchedule(Id.Program.from(TEST_NAMESPACE1,appName,ProgramType.WORKFLOW,workflowName),scheduleName1,true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  Assert.assertEquals(404,suspendSchedule(TEST_NAMESPACE1,appName,scheduleName1));
  Assert.assertEquals(404,resumeSchedule(TEST_NAMESPACE1,appName,scheduleName1));
  TimeUnit.SECONDS.sleep(2);
}","@Test public void testStreamSizeSchedules() throws Exception {
  String appName=""String_Node_Str"";
  String sampleSchedule1=""String_Node_Str"";
  String sampleSchedule2=""String_Node_Str"";
  String workflowName=""String_Node_Str"";
  String streamName=""String_Node_Str"";
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,appName,ProgramType.WORKFLOW,workflowName);
  StringBuilder longStringBuilder=new StringBuilder();
  for (int i=0; i < 10000; i++) {
    longStringBuilder.append(""String_Node_Str"");
  }
  String longString=longStringBuilder.toString();
  HttpResponse response=deploy(AppWithStreamSizeSchedule.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,sampleSchedule1));
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,sampleSchedule2));
  List<ScheduleDetail> schedules=getSchedules(TEST_NAMESPACE2,appName,workflowName);
  Assert.assertEquals(2,schedules.size());
  String scheduleName1=schedules.get(0).getName();
  String scheduleName2=schedules.get(1).getName();
  Assert.assertNotNull(scheduleName1);
  Assert.assertFalse(scheduleName1.isEmpty());
  response=doPut(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName),""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=doGet(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName));
  String json=EntityUtils.toString(response.getEntity());
  StreamProperties properties=new Gson().fromJson(json,StreamProperties.class);
  Assert.assertEquals(1,properties.getNotificationThresholdMB().intValue());
  for (int i=0; i < 12; ++i) {
    response=doPost(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName),longString);
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  }
  verifyProgramRuns(programId,""String_Node_Str"");
  assertSchedule(programId,scheduleName1,true,30,TimeUnit.SECONDS);
  assertSchedule(programId,scheduleName2,true,30,TimeUnit.SECONDS);
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName1));
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName2));
  assertSchedule(programId,scheduleName1,false,30,TimeUnit.SECONDS);
  assertSchedule(programId,scheduleName2,false,30,TimeUnit.SECONDS);
  int workflowRuns=getProgramRuns(programId,""String_Node_Str"").size();
  Assert.assertEquals(1,workflowRuns);
  for (int i=0; i < 12; ++i) {
    response=doPost(String.format(""String_Node_Str"",TEST_NAMESPACE2,streamName),longString);
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  }
  TimeUnit.SECONDS.sleep(5);
  int workflowRunsAfterSuspend=getProgramRuns(programId,""String_Node_Str"").size();
  Assert.assertEquals(workflowRuns,workflowRunsAfterSuspend);
  Assert.assertEquals(200,resumeSchedule(TEST_NAMESPACE2,appName,scheduleName1));
  assertSchedule(programId,scheduleName1,true,30,TimeUnit.SECONDS);
  assertRunHistory(programId,""String_Node_Str"",1 + workflowRunsAfterSuspend,60,TimeUnit.SECONDS);
  try {
    assertSchedule(programId,""String_Node_Str"",true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  Assert.assertEquals(200,suspendSchedule(TEST_NAMESPACE2,appName,scheduleName1));
  assertSchedule(programId,scheduleName1,false,30,TimeUnit.SECONDS);
  try {
    assertSchedule(Id.Program.from(TEST_NAMESPACE1,appName,ProgramType.WORKFLOW,workflowName),scheduleName1,true,2,TimeUnit.SECONDS);
    Assert.fail();
  }
 catch (  Exception e) {
  }
  Assert.assertEquals(404,suspendSchedule(TEST_NAMESPACE1,appName,scheduleName1));
  Assert.assertEquals(404,resumeSchedule(TEST_NAMESPACE1,appName,scheduleName1));
  TimeUnit.SECONDS.sleep(2);
}"
4954,"/** 
 * Collects HBase table stats //TODO: Explore the possiblitity of returning a   {@code Map<TableId, TableStats>}
 * @param admin instance of {@link HBaseAdmin} to communicate with HBase
 * @return map of table name -> table stats
 * @throws IOException
 */
public Map<TableId,TableStats> getTableStats(HBaseAdmin admin) throws IOException {
  Map<TableId,TableStats> datasetStat=Maps.newHashMap();
  ClusterStatus clusterStatus=admin.getClusterStatus();
  for (  ServerName serverName : clusterStatus.getServers()) {
    Map<byte[],RegionLoad> regionsLoad=clusterStatus.getLoad(serverName).getRegionsLoad();
    for (    RegionLoad regionLoad : regionsLoad.values()) {
      TableName tableName=HRegionInfo.getTable(regionLoad.getName());
      HTableDescriptor tableDescriptor=new HTableDescriptor(tableName);
      if (!isCDAPTable(tableDescriptor)) {
        continue;
      }
      TableId tableId=HTableNameConverter.from(tableDescriptor);
      TableStats stat=datasetStat.get(tableId);
      if (stat == null) {
        stat=new TableStats(regionLoad.getStorefileSizeMB(),regionLoad.getMemStoreSizeMB());
        datasetStat.put(tableId,stat);
      }
 else {
        stat.incStoreFileSizeMB(regionLoad.getStorefileSizeMB());
        stat.incMemStoreSizeMB(regionLoad.getMemStoreSizeMB());
      }
    }
  }
  return datasetStat;
}","/** 
 * Collects HBase table stats //TODO: Explore the possiblitity of returning a   {@code Map<TableId, TableStats>}
 * @param admin instance of {@link HBaseAdmin} to communicate with HBase
 * @return map of table name -> table stats
 * @throws IOException
 */
public Map<TableId,TableStats> getTableStats(HBaseAdmin admin) throws IOException {
  Map<TableId,TableStats> datasetStat=Maps.newHashMap();
  ClusterStatus clusterStatus=admin.getClusterStatus();
  for (  ServerName serverName : clusterStatus.getServers()) {
    Map<byte[],RegionLoad> regionsLoad=clusterStatus.getLoad(serverName).getRegionsLoad();
    for (    RegionLoad regionLoad : regionsLoad.values()) {
      TableName tableName=HRegionInfo.getTable(regionLoad.getName());
      HTableDescriptor tableDescriptor=admin.getTableDescriptor(tableName);
      if (!isCDAPTable(tableDescriptor)) {
        continue;
      }
      TableId tableId=HTableNameConverter.from(tableDescriptor);
      TableStats stat=datasetStat.get(tableId);
      if (stat == null) {
        stat=new TableStats(regionLoad.getStorefileSizeMB(),regionLoad.getMemStoreSizeMB());
        datasetStat.put(tableId,stat);
      }
 else {
        stat.incStoreFileSizeMB(regionLoad.getStorefileSizeMB());
        stat.incMemStoreSizeMB(regionLoad.getMemStoreSizeMB());
      }
    }
  }
  return datasetStat;
}"
4955,"protected boolean isCDAPTable(HTableDescriptor hTableDescriptor){
  String tableName=hTableDescriptor.getNameAsString();
  String value=hTableDescriptor.getValue(CDAP_VERSION);
  return tableName.startsWith(tablePrefix + ""String_Node_Str"") || tableName.startsWith(tablePrefix + ""String_Node_Str"") || !Strings.isNullOrEmpty(value);
}","protected boolean isCDAPTable(HTableDescriptor hTableDescriptor){
  return !Strings.isNullOrEmpty(hTableDescriptor.getValue(CDAP_VERSION));
}"
4956,"@Test public void testWorkflowTags() throws Exception {
  String appName=WorkflowAppWithFork.class.getSimpleName();
  ApplicationId appId=NamespaceId.DEFAULT.app(appName);
  String workflowName=WorkflowAppWithFork.WorkflowWithFork.class.getSimpleName();
  ArtifactId artifactId=NamespaceId.DEFAULT.artifact(appId.getApplication(),""String_Node_Str"");
  ApplicationWithPrograms appWithPrograms=createAppWithWorkflow(artifactId,appId,workflowName);
  WorkflowSpecification workflowSpec=appWithPrograms.getSpecification().getWorkflows().get(workflowName);
  SystemMetadataWriterStage systemMetadataWriterStage=new SystemMetadataWriterStage(metadataStore);
  StageContext stageContext=new StageContext(Object.class);
  systemMetadataWriterStage.process(stageContext);
  systemMetadataWriterStage.process(appWithPrograms);
  Set<String> workflowSystemTags=metadataStore.getTags(MetadataScope.SYSTEM,appId.workflow(workflowName));
  Sets.SetView<String> intersection=Sets.intersection(workflowSystemTags,getWorkflowForkNodes(workflowSpec));
  Assert.assertTrue(""String_Node_Str"" + ""String_Node_Str"" + intersection,intersection.isEmpty());
}","@Test public void testWorkflowTags() throws Exception {
  String appName=WorkflowAppWithFork.class.getSimpleName();
  ApplicationId appId=NamespaceId.DEFAULT.app(appName);
  String workflowName=WorkflowAppWithFork.WorkflowWithFork.class.getSimpleName();
  ArtifactId artifactId=NamespaceId.DEFAULT.artifact(appId.getApplication(),""String_Node_Str"");
  ApplicationWithPrograms appWithPrograms=createAppWithWorkflow(artifactId,appId,workflowName);
  WorkflowSpecification workflowSpec=appWithPrograms.getSpecification().getWorkflows().get(workflowName);
  SystemMetadataWriterStage systemMetadataWriterStage=new SystemMetadataWriterStage(metadataStore);
  StageContext stageContext=new StageContext(Object.class);
  systemMetadataWriterStage.process(stageContext);
  systemMetadataWriterStage.process(appWithPrograms);
  Set<String> workflowSystemTags=metadataStore.getTags(MetadataScope.SYSTEM,appId.workflow(workflowName));
  Sets.SetView<String> intersection=Sets.intersection(workflowSystemTags,getWorkflowForkNodes(workflowSpec));
  Assert.assertTrue(""String_Node_Str"" + ""String_Node_Str"" + intersection,intersection.isEmpty());
  Map<String,String> metadataProperties=metadataStore.getMetadata(MetadataScope.SYSTEM,appId).getProperties();
  Assert.assertEquals(WorkflowAppWithFork.SCHED_NAME + ""String_Node_Str"",metadataProperties.get(""String_Node_Str"" + WorkflowAppWithFork.SCHED_NAME));
}"
4957,"private void addSchedules(ImmutableMap.Builder<String,String> properties){
  for (  ScheduleSpecification scheduleSpec : appSpec.getSchedules().values()) {
    Schedule schedule=scheduleSpec.getSchedule();
    properties.put(""String_Node_Str"" + MetadataDataset.KEYVALUE_SEPARATOR + schedule.getName(),schedule.getName() + MetadataDataset.KEYVALUE_SEPARATOR + schedule.getDescription());
  }
}","private void addSchedules(ImmutableMap.Builder<String,String> properties){
  for (  ScheduleCreationSpec creationSpec : appSpec.getProgramSchedules().values()) {
    properties.put(""String_Node_Str"" + MetadataDataset.KEYVALUE_SEPARATOR + creationSpec.getName(),creationSpec.getName() + MetadataDataset.KEYVALUE_SEPARATOR + creationSpec.getDescription());
  }
}"
4958,"public Module createModule(final TwillContext context,ProgramId programId,@Nullable String principal){
  return Modules.combine(createModule(programId,principal),new AbstractModule(){
    @Override protected void configure(){
      bind(InetAddress.class).annotatedWith(Names.named(Constants.Service.MASTER_SERVICES_BIND_ADDRESS)).toInstance(context.getHost());
      bind(ServiceAnnouncer.class).toInstance(new ServiceAnnouncer(){
        @Override public Cancellable announce(        String serviceName,        int port){
          return context.announce(serviceName,port);
        }
        @Override public Cancellable announce(        String serviceName,        int port,        byte[] payload){
          return context.announce(serviceName,port,payload);
        }
      }
);
    }
  }
);
}","public Module createModule(final TwillContext context,ProgramId programId,String runId,String instanceId,@Nullable String principal){
  return Modules.combine(createModule(programId,runId,instanceId,principal),new AbstractModule(){
    @Override protected void configure(){
      bind(InetAddress.class).annotatedWith(Names.named(Constants.Service.MASTER_SERVICES_BIND_ADDRESS)).toInstance(context.getHost());
      bind(ServiceAnnouncer.class).toInstance(new ServiceAnnouncer(){
        @Override public Cancellable announce(        String serviceName,        int port){
          return context.announce(serviceName,port);
        }
        @Override public Cancellable announce(        String serviceName,        int port,        byte[] payload){
          return context.announce(serviceName,port,payload);
        }
      }
);
    }
  }
);
}"
4959,"private Module getCombinedModules(final ProgramId programId){
  return Modules.combine(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MetricsClientRuntimeModule().getDistributedModules(),new MessagingClientModule(),new LocationRuntimeModule().getDistributedModules(),new LoggingModules().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new AuditModule().getDistributedModules(),new NamespaceClientRuntimeModule().getDistributedModules(),new AuthorizationEnforcementModule().getDistributedModules(),new SecureStoreModules().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(QueueReaderFactory.class).in(Scopes.SINGLETON);
      install(new DataFabricFacadeModule());
      bind(RuntimeStore.class).to(RemoteRuntimeStore.class);
      install(createStreamFactoryModule());
      bind(UGIProvider.class).to(CurrentUGIProvider.class).in(Scopes.SINGLETON);
      bind(PrivilegesManager.class).to(RemotePrivilegesManager.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
      bind(ProgramId.class).toInstance(programId);
      bind(ExploreClient.class).to(ProgramDiscoveryExploreClient.class).in(Scopes.SINGLETON);
    }
  }
);
}","private Module getCombinedModules(final ProgramId programId,String txClientId){
  return Modules.combine(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MetricsClientRuntimeModule().getDistributedModules(),new MessagingClientModule(),new LocationRuntimeModule().getDistributedModules(),new LoggingModules().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new DataFabricModules(txClientId).getDistributedModules(),new DataSetsModules().getDistributedModules(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new AuditModule().getDistributedModules(),new NamespaceClientRuntimeModule().getDistributedModules(),new AuthorizationEnforcementModule().getDistributedModules(),new SecureStoreModules().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(QueueReaderFactory.class).in(Scopes.SINGLETON);
      install(new DataFabricFacadeModule());
      bind(RuntimeStore.class).to(RemoteRuntimeStore.class);
      install(createStreamFactoryModule());
      bind(UGIProvider.class).to(CurrentUGIProvider.class).in(Scopes.SINGLETON);
      bind(PrivilegesManager.class).to(RemotePrivilegesManager.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
      bind(ProgramId.class).toInstance(programId);
      bind(ExploreClient.class).to(ProgramDiscoveryExploreClient.class).in(Scopes.SINGLETON);
    }
  }
);
}"
4960,"private static Injector createInjector(CConfiguration cConf,Configuration hConf){
  MapReduceContextConfig mapReduceContextConfig=new MapReduceContextConfig(hConf);
  String principal=mapReduceContextConfig.getProgramOptions().getArguments().getOption(ProgramOptionConstants.PRINCIPAL);
  return Guice.createInjector(new DistributedProgramRunnableModule(cConf,hConf).createModule(mapReduceContextConfig.getProgramId(),principal));
}","private static Injector createInjector(CConfiguration cConf,Configuration hConf){
  MapReduceContextConfig mapReduceContextConfig=new MapReduceContextConfig(hConf);
  Arguments arguments=mapReduceContextConfig.getProgramOptions().getArguments();
  String principal=arguments.getOption(ProgramOptionConstants.PRINCIPAL);
  String runId=arguments.getOption(ProgramOptionConstants.RUN_ID);
  String instanceId=arguments.getOption(ProgramOptionConstants.INSTANCE_ID);
  return Guice.createInjector(new DistributedProgramRunnableModule(cConf,hConf).createModule(mapReduceContextConfig.getProgramId(),runId,instanceId,principal));
}"
4961,"protected Module createModule(TwillContext context,ProgramId programId,@Nullable String principal){
  return new DistributedProgramRunnableModule(cConf,hConf).createModule(context,programId,principal);
}","protected Module createModule(TwillContext context,ProgramId programId,String runId,String instanceId,@Nullable String principal){
  return new DistributedProgramRunnableModule(cConf,hConf).createModule(context,programId,runId,instanceId,principal);
}"
4962,"@Override public void initialize(TwillContext context){
  name=context.getSpecification().getName();
  LOG.info(""String_Node_Str"" + name);
  Thread.setDefaultUncaughtExceptionHandler(new UncaughtExceptionHandler());
  System.setSecurityManager(new RunnableSecurityManager(System.getSecurityManager()));
  SLF4JBridgeHandler.install();
  runLatch=new CountDownLatch(1);
  coreServices=new ArrayList<>();
  try {
    CommandLine cmdLine=parseArgs(context.getApplicationArguments());
    hConf=new Configuration();
    hConf.clear();
    hConf.addResource(new File(cmdLine.getOptionValue(RunnableOptions.HADOOP_CONF_FILE)).toURI().toURL());
    UserGroupInformation.setConfiguration(hConf);
    cConf=CConfiguration.create(new File(cmdLine.getOptionValue(RunnableOptions.CDAP_CONF_FILE)));
    programOpts=createProgramOptions(cmdLine,context,context.getSpecification().getConfigs());
    String principal=programOpts.getArguments().getOption(ProgramOptionConstants.PRINCIPAL);
    ProgramId programId=GSON.fromJson(cmdLine.getOptionValue(RunnableOptions.PROGRAM_ID),ProgramId.class);
    Injector injector=Guice.createInjector(createModule(context,programId,principal));
    coreServices.add(injector.getInstance(ZKClientService.class));
    coreServices.add(injector.getInstance(KafkaClientService.class));
    coreServices.add(injector.getInstance(BrokerService.class));
    coreServices.add(injector.getInstance(MetricsCollectionService.class));
    coreServices.add(injector.getInstance(StreamCoordinatorClient.class));
    logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
    logAppenderInitializer.initialize();
    programRunner=createProgramRunner(injector);
    try {
      Location programJarLocation=Locations.toLocation(new File(cmdLine.getOptionValue(RunnableOptions.JAR)));
      ApplicationSpecification appSpec=readAppSpec(new File(cmdLine.getOptionValue(RunnableOptions.APP_SPEC_FILE)));
      program=Programs.create(cConf,programRunner,new ProgramDescriptor(programId,appSpec),programJarLocation,new File(cmdLine.getOptionValue(RunnableOptions.EXPANDED_JAR)));
    }
 catch (    IOException e) {
      throw Throwables.propagate(e);
    }
    coreServices.add(new ProgramRunnableResourceReporter(program.getId(),injector.getInstance(MetricsCollectionService.class),context));
    LOG.info(""String_Node_Str"",name);
  }
 catch (  Throwable t) {
    LOG.error(t.getMessage(),t);
    throw Throwables.propagate(t);
  }
}","@Override public void initialize(TwillContext context){
  name=context.getSpecification().getName();
  LOG.info(""String_Node_Str"" + name);
  Thread.setDefaultUncaughtExceptionHandler(new UncaughtExceptionHandler());
  System.setSecurityManager(new RunnableSecurityManager(System.getSecurityManager()));
  SLF4JBridgeHandler.install();
  runLatch=new CountDownLatch(1);
  coreServices=new ArrayList<>();
  try {
    CommandLine cmdLine=parseArgs(context.getApplicationArguments());
    hConf=new Configuration();
    hConf.clear();
    hConf.addResource(new File(cmdLine.getOptionValue(RunnableOptions.HADOOP_CONF_FILE)).toURI().toURL());
    UserGroupInformation.setConfiguration(hConf);
    cConf=CConfiguration.create(new File(cmdLine.getOptionValue(RunnableOptions.CDAP_CONF_FILE)));
    programOpts=createProgramOptions(cmdLine,context,context.getSpecification().getConfigs());
    String principal=programOpts.getArguments().getOption(ProgramOptionConstants.PRINCIPAL);
    ProgramId programId=GSON.fromJson(cmdLine.getOptionValue(RunnableOptions.PROGRAM_ID),ProgramId.class);
    String instanceId=programOpts.getArguments().getOption(ProgramOptionConstants.INSTANCE_ID);
    String runId=programOpts.getArguments().getOption(ProgramOptionConstants.RUN_ID);
    Injector injector=Guice.createInjector(createModule(context,programId,runId,instanceId,principal));
    coreServices.add(injector.getInstance(ZKClientService.class));
    coreServices.add(injector.getInstance(KafkaClientService.class));
    coreServices.add(injector.getInstance(BrokerService.class));
    coreServices.add(injector.getInstance(MetricsCollectionService.class));
    coreServices.add(injector.getInstance(StreamCoordinatorClient.class));
    logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
    logAppenderInitializer.initialize();
    programRunner=createProgramRunner(injector);
    try {
      Location programJarLocation=Locations.toLocation(new File(cmdLine.getOptionValue(RunnableOptions.JAR)));
      ApplicationSpecification appSpec=readAppSpec(new File(cmdLine.getOptionValue(RunnableOptions.APP_SPEC_FILE)));
      program=Programs.create(cConf,programRunner,new ProgramDescriptor(programId,appSpec),programJarLocation,new File(cmdLine.getOptionValue(RunnableOptions.EXPANDED_JAR)));
    }
 catch (    IOException e) {
      throw Throwables.propagate(e);
    }
    coreServices.add(new ProgramRunnableResourceReporter(program.getId(),injector.getInstance(MetricsCollectionService.class),context));
    LOG.info(""String_Node_Str"",name);
  }
 catch (  Throwable t) {
    LOG.error(t.getMessage(),t);
    throw Throwables.propagate(t);
  }
}"
4963,"@Override protected Module createModule(TwillContext context,ProgramId programId,@Nullable String principal){
  return Modules.combine(super.createModule(context,programId,principal),new AbstractModule(){
    @Override protected void configure(){
      install(new FactoryModuleBuilder().implement(JarHttpHandler.class,ExplodeJarHttpHandler.class).build(WebappHttpHandlerFactory.class));
    }
  }
);
}","@Override protected Module createModule(TwillContext context,ProgramId programId,String runId,String instanceId,@Nullable String principal){
  return Modules.combine(super.createModule(context,programId,runId,instanceId,principal),new AbstractModule(){
    @Override protected void configure(){
      install(new FactoryModuleBuilder().implement(JarHttpHandler.class,ExplodeJarHttpHandler.class).build(WebappHttpHandlerFactory.class));
    }
  }
);
}"
4964,"@Override protected Module createModule(TwillContext context,ProgramId programId,@Nullable String principal){
  Module module=super.createModule(context,programId,principal);
  return Modules.combine(module,new PrivateModule(){
    @Override protected void configure(){
      MapBinder<ProgramType,ProgramRunner> runnerFactoryBinder=MapBinder.newMapBinder(binder(),ProgramType.class,ProgramRunner.class);
      runnerFactoryBinder.addBinding(ProgramType.MAPREDUCE).to(MapReduceProgramRunner.class);
      bind(ProgramRuntimeProvider.Mode.class).toInstance(ProgramRuntimeProvider.Mode.LOCAL);
      bind(ProgramRunnerFactory.class).to(DefaultProgramRunnerFactory.class).in(Scopes.SINGLETON);
      expose(ProgramRunnerFactory.class);
    }
  }
);
}","@Override protected Module createModule(TwillContext context,ProgramId programId,String runId,String instanceId,@Nullable String principal){
  Module module=super.createModule(context,programId,runId,instanceId,principal);
  return Modules.combine(module,new PrivateModule(){
    @Override protected void configure(){
      MapBinder<ProgramType,ProgramRunner> runnerFactoryBinder=MapBinder.newMapBinder(binder(),ProgramType.class,ProgramRunner.class);
      runnerFactoryBinder.addBinding(ProgramType.MAPREDUCE).to(MapReduceProgramRunner.class);
      bind(ProgramRuntimeProvider.Mode.class).toInstance(ProgramRuntimeProvider.Mode.LOCAL);
      bind(ProgramRunnerFactory.class).to(DefaultProgramRunnerFactory.class).in(Scopes.SINGLETON);
      expose(ProgramRunnerFactory.class);
    }
  }
);
}"
4965,"@Test public void createModule() throws Exception {
  DistributedProgramRunnableModule distributedProgramRunnableModule=new DistributedProgramRunnableModule(CConfiguration.create(),new Configuration());
  Guice.createInjector(distributedProgramRunnableModule.createModule(new ProgramId(""String_Node_Str"",""String_Node_Str"",ProgramType.MAPREDUCE,""String_Node_Str""),""String_Node_Str""));
  Guice.createInjector(distributedProgramRunnableModule.createModule(new TwillContext(){
    @Override public RunId getRunId(){
      return null;
    }
    @Override public RunId getApplicationRunId(){
      return null;
    }
    @Override public int getInstanceCount(){
      return 0;
    }
    @Override public InetAddress getHost(){
      return new InetSocketAddress(""String_Node_Str"",0).getAddress();
    }
    @Override public String[] getArguments(){
      return new String[0];
    }
    @Override public String[] getApplicationArguments(){
      return new String[0];
    }
    @Override public TwillRunnableSpecification getSpecification(){
      return null;
    }
    @Override public int getInstanceId(){
      return 0;
    }
    @Override public int getVirtualCores(){
      return 0;
    }
    @Override public int getMaxMemoryMB(){
      return 0;
    }
    @Override public ServiceDiscovered discover(    String name){
      return null;
    }
    @Override public Cancellable electLeader(    String name,    ElectionHandler participantHandler){
      return null;
    }
    @Override public Lock createLock(    String name){
      return null;
    }
    @Override public Cancellable announce(    String serviceName,    int port){
      return null;
    }
    @Override public Cancellable announce(    String serviceName,    int port,    byte[] payload){
      return null;
    }
  }
,new ProgramId(""String_Node_Str"",""String_Node_Str"",ProgramType.MAPREDUCE,""String_Node_Str""),""String_Node_Str""));
}","@Test public void createModule() throws Exception {
  DistributedProgramRunnableModule distributedProgramRunnableModule=new DistributedProgramRunnableModule(CConfiguration.create(),new Configuration());
  Guice.createInjector(distributedProgramRunnableModule.createModule(new ProgramId(""String_Node_Str"",""String_Node_Str"",ProgramType.MAPREDUCE,""String_Node_Str""),RunIds.generate().getId(),""String_Node_Str"",""String_Node_Str""));
  Guice.createInjector(distributedProgramRunnableModule.createModule(new TwillContext(){
    @Override public RunId getRunId(){
      return null;
    }
    @Override public RunId getApplicationRunId(){
      return null;
    }
    @Override public int getInstanceCount(){
      return 0;
    }
    @Override public InetAddress getHost(){
      return new InetSocketAddress(""String_Node_Str"",0).getAddress();
    }
    @Override public String[] getArguments(){
      return new String[0];
    }
    @Override public String[] getApplicationArguments(){
      return new String[0];
    }
    @Override public TwillRunnableSpecification getSpecification(){
      return null;
    }
    @Override public int getInstanceId(){
      return 0;
    }
    @Override public int getVirtualCores(){
      return 0;
    }
    @Override public int getMaxMemoryMB(){
      return 0;
    }
    @Override public ServiceDiscovered discover(    String name){
      return null;
    }
    @Override public Cancellable electLeader(    String name,    ElectionHandler participantHandler){
      return null;
    }
    @Override public Lock createLock(    String name){
      return null;
    }
    @Override public Cancellable announce(    String serviceName,    int port){
      return null;
    }
    @Override public Cancellable announce(    String serviceName,    int port,    byte[] payload){
      return null;
    }
  }
,new ProgramId(""String_Node_Str"",""String_Node_Str"",ProgramType.MAPREDUCE,""String_Node_Str""),RunIds.generate().getId(),""String_Node_Str"",""String_Node_Str""));
}"
4966,"@BeforeClass public static void setup() throws Exception {
  CConfiguration conf=CConfiguration.create();
  conf.set(Constants.CFG_HDFS_USER,System.getProperty(""String_Node_Str""));
  Injector injector=Guice.createInjector(new DataFabricDistributedModule(),new ConfigModule(conf,TEST_HBASE.getConfiguration()),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new TransactionMetricsModule(),new LocationRuntimeModule().getDistributedModules(),new NamespaceClientUnitTestModule().getModule(),new SystemDatasetRuntimeModule().getDistributedModules(),new DataSetsModules().getInMemoryModules(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getNoOpModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
    }
  }
);
  dsFramework=injector.getInstance(DatasetFramework.class);
  tableUtil=injector.getInstance(HBaseTableUtil.class);
  ddlExecutor=new HBaseDDLExecutorFactory(conf,TEST_HBASE.getHBaseAdmin().getConfiguration()).get();
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(NamespaceId.SYSTEM));
}","@BeforeClass public static void setup() throws Exception {
  CConfiguration conf=CConfiguration.create();
  conf.set(Constants.CFG_HDFS_USER,System.getProperty(""String_Node_Str""));
  Injector injector=Guice.createInjector(new DataFabricModules().getDistributedModules(),new ConfigModule(conf,TEST_HBASE.getConfiguration()),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new TransactionMetricsModule(),new LocationRuntimeModule().getDistributedModules(),new NamespaceClientUnitTestModule().getModule(),new SystemDatasetRuntimeModule().getDistributedModules(),new DataSetsModules().getInMemoryModules(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getNoOpModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
    }
  }
);
  dsFramework=injector.getInstance(DatasetFramework.class);
  tableUtil=injector.getInstance(HBaseTableUtil.class);
  ddlExecutor=new HBaseDDLExecutorFactory(conf,TEST_HBASE.getHBaseAdmin().getConfiguration()).get();
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(NamespaceId.SYSTEM));
}"
4967,"@BeforeClass public static void init() throws Exception {
  zkServer=InMemoryZKServer.builder().setDataDir(TMP_FOLDER.newFolder()).build();
  zkServer.startAndWait();
  Configuration hConf=TEST_HBASE.getConfiguration();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new NonCustomLocationUnitTestModule().getModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new TransactionMetricsModule(),new DataFabricDistributedModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(NamespaceQueryAdmin.class).to(SimpleNamespaceQueryAdmin.class);
    }
  }
,new DataSetsModules().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new ViewAdminModules().getInMemoryModules(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getNoOpModule(),Modules.override(new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
    }
  }
));
  zkClientService=injector.getInstance(ZKClientService.class);
  zkClientService.startAndWait();
  streamAdmin=injector.getInstance(StreamAdmin.class);
  stateStoreFactory=injector.getInstance(StreamConsumerStateStoreFactory.class);
  tableUtil=injector.getInstance(HBaseTableUtil.class);
  ddlExecutor=new HBaseDDLExecutorFactory(cConf,TEST_HBASE.getHBaseAdmin().getConfiguration()).get();
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(TEST_NAMESPACE));
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(OTHER_NAMESPACE));
  setupNamespaces(injector.getInstance(NamespacedLocationFactory.class));
}","@BeforeClass public static void init() throws Exception {
  zkServer=InMemoryZKServer.builder().setDataDir(TMP_FOLDER.newFolder()).build();
  zkServer.startAndWait();
  Configuration hConf=TEST_HBASE.getConfiguration();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new NonCustomLocationUnitTestModule().getModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new TransactionMetricsModule(),new DataFabricModules().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(NamespaceQueryAdmin.class).to(SimpleNamespaceQueryAdmin.class);
    }
  }
,new DataSetsModules().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new ViewAdminModules().getInMemoryModules(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getNoOpModule(),Modules.override(new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
    }
  }
));
  zkClientService=injector.getInstance(ZKClientService.class);
  zkClientService.startAndWait();
  streamAdmin=injector.getInstance(StreamAdmin.class);
  stateStoreFactory=injector.getInstance(StreamConsumerStateStoreFactory.class);
  tableUtil=injector.getInstance(HBaseTableUtil.class);
  ddlExecutor=new HBaseDDLExecutorFactory(cConf,TEST_HBASE.getHBaseAdmin().getConfiguration()).get();
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(TEST_NAMESPACE));
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(OTHER_NAMESPACE));
  setupNamespaces(injector.getInstance(NamespacedLocationFactory.class));
}"
4968,"@BeforeClass public static void init() throws Exception {
  InMemoryZKServer zkServer=InMemoryZKServer.builder().setDataDir(tmpFolder.newFolder()).build();
  zkServer.startAndWait();
  Configuration hConf=testHBase.getConfiguration();
  addCConfProperties(cConf);
  cConf.setInt(Constants.Stream.CONTAINER_INSTANCES,1);
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new NonCustomLocationUnitTestModule().getModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new TransactionMetricsModule(),new DataSetsModules().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new ViewAdminModules().getInMemoryModules(),new AuditModule().getInMemoryModules(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getNoOpModule(),Modules.override(new DataFabricDistributedModule(),new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(TransactionStateStorage.class).to(NoOpTransactionStateStorage.class);
      bind(TransactionSystemClient.class).to(InMemoryTxSystemClient.class).in(Singleton.class);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
      bind(NamespaceQueryAdmin.class).to(SimpleNamespaceQueryAdmin.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
    }
  }
));
  ZKClientService zkClientService=injector.getInstance(ZKClientService.class);
  zkClientService.startAndWait();
  streamAdmin=injector.getInstance(StreamAdmin.class);
  txManager=TxInMemory.getTransactionManager(injector.getInstance(TransactionSystemClient.class));
  fileWriterFactory=injector.getInstance(StreamFileWriterFactory.class);
  streamCoordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  inMemoryAuditPublisher=injector.getInstance(InMemoryAuditPublisher.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  ownerAdmin=injector.getInstance(OwnerAdmin.class);
  setupNamespaces(injector.getInstance(NamespacedLocationFactory.class));
  txManager.startAndWait();
  streamCoordinatorClient.startAndWait();
}","@BeforeClass public static void init() throws Exception {
  InMemoryZKServer zkServer=InMemoryZKServer.builder().setDataDir(tmpFolder.newFolder()).build();
  zkServer.startAndWait();
  Configuration hConf=testHBase.getConfiguration();
  addCConfProperties(cConf);
  cConf.setInt(Constants.Stream.CONTAINER_INSTANCES,1);
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new NonCustomLocationUnitTestModule().getModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new TransactionMetricsModule(),new DataSetsModules().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new ViewAdminModules().getInMemoryModules(),new AuditModule().getInMemoryModules(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getNoOpModule(),Modules.override(new DataFabricModules().getDistributedModules(),new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(TransactionStateStorage.class).to(NoOpTransactionStateStorage.class);
      bind(TransactionSystemClient.class).to(InMemoryTxSystemClient.class).in(Singleton.class);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
      bind(NamespaceQueryAdmin.class).to(SimpleNamespaceQueryAdmin.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
    }
  }
));
  ZKClientService zkClientService=injector.getInstance(ZKClientService.class);
  zkClientService.startAndWait();
  streamAdmin=injector.getInstance(StreamAdmin.class);
  txManager=TxInMemory.getTransactionManager(injector.getInstance(TransactionSystemClient.class));
  fileWriterFactory=injector.getInstance(StreamFileWriterFactory.class);
  streamCoordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  inMemoryAuditPublisher=injector.getInstance(InMemoryAuditPublisher.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  ownerAdmin=injector.getInstance(OwnerAdmin.class);
  setupNamespaces(injector.getInstance(NamespacedLocationFactory.class));
  txManager.startAndWait();
  streamCoordinatorClient.startAndWait();
}"
4969,"@BeforeClass public static void init() throws Exception {
  zkServer=InMemoryZKServer.builder().setDataDir(TMP_FOLDER.newFolder()).build();
  zkServer.startAndWait();
  Configuration hConf=TEST_HBASE.getConfiguration();
  cConf.setInt(Constants.Stream.CONTAINER_INSTANCES,1);
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new NonCustomLocationUnitTestModule().getModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new TransactionMetricsModule(),new DataSetsModules().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new ViewAdminModules().getInMemoryModules(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getNoOpModule(),Modules.override(new DataFabricDistributedModule(),new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(TransactionStateStorage.class).to(NoOpTransactionStateStorage.class);
      bind(TransactionSystemClient.class).to(InMemoryTxSystemClient.class).in(Singleton.class);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
      bind(NamespaceQueryAdmin.class).to(SimpleNamespaceQueryAdmin.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
    }
  }
));
  zkClientService=injector.getInstance(ZKClientService.class);
  zkClientService.startAndWait();
  streamAdmin=injector.getInstance(StreamAdmin.class);
  consumerFactory=injector.getInstance(StreamConsumerFactory.class);
  txClient=injector.getInstance(TransactionSystemClient.class);
  txManager=TxInMemory.getTransactionManager(txClient);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  fileWriterFactory=injector.getInstance(StreamFileWriterFactory.class);
  txManager.startAndWait();
  tableUtil=injector.getInstance(HBaseTableUtil.class);
  ddlExecutor=new HBaseDDLExecutorFactory(cConf,TEST_HBASE.getHBaseAdmin().getConfiguration()).get();
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(NamespaceId.SYSTEM));
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(TEST_NAMESPACE));
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(OTHER_NAMESPACE));
  setupNamespaces(injector.getInstance(NamespacedLocationFactory.class));
}","@BeforeClass public static void init() throws Exception {
  zkServer=InMemoryZKServer.builder().setDataDir(TMP_FOLDER.newFolder()).build();
  zkServer.startAndWait();
  Configuration hConf=TEST_HBASE.getConfiguration();
  cConf.setInt(Constants.Stream.CONTAINER_INSTANCES,1);
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new NonCustomLocationUnitTestModule().getModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new TransactionMetricsModule(),new DataSetsModules().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new ViewAdminModules().getInMemoryModules(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getNoOpModule(),Modules.override(new DataFabricModules().getDistributedModules(),new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(TransactionStateStorage.class).to(NoOpTransactionStateStorage.class);
      bind(TransactionSystemClient.class).to(InMemoryTxSystemClient.class).in(Singleton.class);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
      bind(NamespaceQueryAdmin.class).to(SimpleNamespaceQueryAdmin.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
    }
  }
));
  zkClientService=injector.getInstance(ZKClientService.class);
  zkClientService.startAndWait();
  streamAdmin=injector.getInstance(StreamAdmin.class);
  consumerFactory=injector.getInstance(StreamConsumerFactory.class);
  txClient=injector.getInstance(TransactionSystemClient.class);
  txManager=TxInMemory.getTransactionManager(txClient);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  fileWriterFactory=injector.getInstance(StreamFileWriterFactory.class);
  txManager.startAndWait();
  tableUtil=injector.getInstance(HBaseTableUtil.class);
  ddlExecutor=new HBaseDDLExecutorFactory(cConf,TEST_HBASE.getHBaseAdmin().getConfiguration()).get();
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(NamespaceId.SYSTEM));
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(TEST_NAMESPACE));
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(OTHER_NAMESPACE));
  setupNamespaces(injector.getInstance(NamespacedLocationFactory.class));
}"
4970,"public DataFabricDistributedModule(){
}","public DataFabricDistributedModule(String txClientId){
  this.txClientId=txClientId;
}"
4971,"@Override public void configure(){
  bind(ThriftClientProvider.class).toProvider(ThriftClientProviderSupplier.class);
  bind(QueueClientFactory.class).to(HBaseQueueClientFactory.class).in(Singleton.class);
  bind(QueueAdmin.class).to(HBaseQueueAdmin.class).in(Singleton.class);
  bind(HBaseTableUtil.class).toProvider(HBaseTableUtilFactory.class);
  bind(TxMetricsCollector.class).to(TransactionManagerMetricsCollector.class).in(Scopes.SINGLETON);
  bind(TransactionSystemClientService.class).to(DistributedTransactionSystemClientService.class);
  install(new TransactionModules().getDistributedModules());
  install(new TransactionExecutorModule());
}","@Override public void configure(){
  bind(ThriftClientProvider.class).toProvider(ThriftClientProviderSupplier.class);
  bind(QueueClientFactory.class).to(HBaseQueueClientFactory.class).in(Singleton.class);
  bind(QueueAdmin.class).to(HBaseQueueAdmin.class).in(Singleton.class);
  bind(HBaseTableUtil.class).toProvider(HBaseTableUtilFactory.class);
  bind(TxMetricsCollector.class).to(TransactionManagerMetricsCollector.class).in(Scopes.SINGLETON);
  bind(TransactionSystemClientService.class).to(DistributedTransactionSystemClientService.class);
  install(new TransactionModules(txClientId).getDistributedModules());
  install(new TransactionExecutorModule());
}"
4972,"@Override protected void configure(){
  bind(QueueClientFactory.class).to(InMemoryQueueClientFactory.class).in(Singleton.class);
  bind(QueueAdmin.class).to(InMemoryQueueAdmin.class).in(Singleton.class);
  bind(TxMetricsCollector.class).to(TransactionManagerMetricsCollector.class).in(Scopes.SINGLETON);
  bind(TransactionSystemClientService.class).to(DelegatingTransactionSystemClientService.class);
  install(new TransactionModules().getInMemoryModules());
  install(new TransactionExecutorModule());
}","@Override protected void configure(){
  bind(QueueClientFactory.class).to(InMemoryQueueClientFactory.class).in(Singleton.class);
  bind(QueueAdmin.class).to(InMemoryQueueAdmin.class).in(Singleton.class);
  bind(TxMetricsCollector.class).to(TransactionManagerMetricsCollector.class).in(Scopes.SINGLETON);
  bind(TransactionSystemClientService.class).to(DelegatingTransactionSystemClientService.class);
  install(new TransactionModules(txClientId).getInMemoryModules());
  install(new TransactionExecutorModule());
}"
4973,"@Override public Module getDistributedModules(){
  return new DataFabricDistributedModule();
}","@Override public Module getDistributedModules(){
  return new DataFabricDistributedModule(txClientId);
}"
4974,"@Override public Module getInMemoryModules(){
  return new DataFabricInMemoryModule();
}","@Override public Module getInMemoryModules(){
  return new DataFabricInMemoryModule(txClientId);
}"
4975,"public static void main(String[] args) throws Exception {
  if (args.length < 1) {
    System.out.println(String.format(""String_Node_Str"",StreamTailer.class.getName()));
    return;
  }
  String streamName=args[0];
  CConfiguration cConf=CConfiguration.create();
  Configuration hConf=new Configuration();
  Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new LocationRuntimeModule().getDistributedModules(),new ExploreClientModule(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new AuthorizationEnforcementModule().getDistributedModules(),new AuthenticationContextModules().getMasterModule(),new NotificationFeedClientModule());
  StreamAdmin streamAdmin=injector.getInstance(StreamAdmin.class);
  StreamId streamId=NamespaceId.DEFAULT.stream(streamName);
  StreamConfig streamConfig=streamAdmin.getConfig(streamId);
  Location streamLocation=streamConfig.getLocation();
  List<Location> eventFiles=Lists.newArrayList();
  for (  Location partition : streamLocation.list()) {
    if (!partition.isDirectory()) {
      continue;
    }
    for (    Location file : partition.list()) {
      if (StreamFileType.EVENT.isMatched(file.getName())) {
        eventFiles.add(file);
      }
    }
  }
  int generation=StreamUtils.getGeneration(streamConfig);
  MultiLiveStreamFileReader reader=new MultiLiveStreamFileReader(streamConfig,ImmutableList.copyOf(Iterables.transform(eventFiles,createOffsetConverter(generation))));
  List<StreamEvent> events=Lists.newArrayList();
  while (reader.read(events,10,100,TimeUnit.MILLISECONDS) >= 0) {
    for (    StreamEvent event : events) {
      System.out.println(event.getTimestamp() + ""String_Node_Str"" + Charsets.UTF_8.decode(event.getBody()));
    }
    events.clear();
  }
  reader.close();
}","public static void main(String[] args) throws Exception {
  if (args.length < 1) {
    System.out.println(String.format(""String_Node_Str"",StreamTailer.class.getName()));
    return;
  }
  String streamName=args[0];
  CConfiguration cConf=CConfiguration.create();
  Configuration hConf=new Configuration();
  String txClientId=StreamTailer.class.getName();
  Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new DataFabricModules(txClientId).getDistributedModules(),new DataSetsModules().getDistributedModules(),new LocationRuntimeModule().getDistributedModules(),new ExploreClientModule(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new AuthorizationEnforcementModule().getDistributedModules(),new AuthenticationContextModules().getMasterModule(),new NotificationFeedClientModule());
  StreamAdmin streamAdmin=injector.getInstance(StreamAdmin.class);
  StreamId streamId=NamespaceId.DEFAULT.stream(streamName);
  StreamConfig streamConfig=streamAdmin.getConfig(streamId);
  Location streamLocation=streamConfig.getLocation();
  List<Location> eventFiles=Lists.newArrayList();
  for (  Location partition : streamLocation.list()) {
    if (!partition.isDirectory()) {
      continue;
    }
    for (    Location file : partition.list()) {
      if (StreamFileType.EVENT.isMatched(file.getName())) {
        eventFiles.add(file);
      }
    }
  }
  int generation=StreamUtils.getGeneration(streamConfig);
  MultiLiveStreamFileReader reader=new MultiLiveStreamFileReader(streamConfig,ImmutableList.copyOf(Iterables.transform(eventFiles,createOffsetConverter(generation))));
  List<StreamEvent> events=Lists.newArrayList();
  while (reader.read(events,10,100,TimeUnit.MILLISECONDS) >= 0) {
    for (    StreamEvent event : events) {
      System.out.println(event.getTimestamp() + ""String_Node_Str"" + Charsets.UTF_8.decode(event.getBody()));
    }
    events.clear();
  }
  reader.close();
}"
4976,"@BeforeClass public static void init() throws Exception {
  hConf=TEST_HBASE.getConfiguration();
  cConf=CConfiguration.create();
  cConf.set(Constants.Zookeeper.QUORUM,TEST_HBASE.getZkConnectionString());
  cConf.set(TxConstants.Service.CFG_DATA_TX_BIND_PORT,Integer.toString(Networks.getRandomPort()));
  cConf.set(Constants.Dataset.TABLE_PREFIX,TABLE_PREFIX);
  cConf.set(Constants.CFG_HDFS_USER,System.getProperty(""String_Node_Str""));
  cConf.setLong(QueueConstants.QUEUE_CONFIG_UPDATE_FREQUENCY,10000L);
  cConf.setInt(QueueConstants.ConfigKeys.QUEUE_TABLE_PRESPLITS,4);
  cConf.setLong(TxConstants.Manager.CFG_TX_TIMEOUT,100000000L);
  cConf.setLong(TxConstants.Manager.CFG_TX_MAX_TIMEOUT,100000000L);
  injector=Guice.createInjector(new DataFabricDistributedModule(),new ConfigModule(cConf,hConf),new ZKClientModule(),new LocationRuntimeModule().getDistributedModules(),new NamespaceClientUnitTestModule().getModule(),new DiscoveryRuntimeModule().getDistributedModules(),new TransactionMetricsModule(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getMasterModule(),new DataSetsModules().getInMemoryModules(),new SystemDatasetRuntimeModule().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class).in(Scopes.SINGLETON);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
    }
  }
);
  hbaseAdmin=TEST_HBASE.getHBaseAdmin();
  ddlExecutor=new HBaseDDLExecutorFactory(cConf,hbaseAdmin.getConfiguration()).get();
  tableUtil=injector.getInstance(HBaseTableUtil.class);
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(NamespaceId.SYSTEM));
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(NAMESPACE_ID));
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(NAMESPACE_ID1));
  ConfigurationTable configTable=new ConfigurationTable(hConf);
  configTable.write(ConfigurationTable.Type.DEFAULT,cConf);
  zkClientService=injector.getInstance(ZKClientService.class);
  zkClientService.startAndWait();
  txService=injector.getInstance(TransactionService.class);
  Thread t=new Thread(){
    @Override public void run(){
      txService.start();
    }
  }
;
  t.start();
  txSystemClient=injector.getInstance(TransactionSystemClient.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
  executorFactory=injector.getInstance(TransactionExecutorFactory.class);
}","@BeforeClass public static void init() throws Exception {
  hConf=TEST_HBASE.getConfiguration();
  cConf=CConfiguration.create();
  cConf.set(Constants.Zookeeper.QUORUM,TEST_HBASE.getZkConnectionString());
  cConf.set(TxConstants.Service.CFG_DATA_TX_BIND_PORT,Integer.toString(Networks.getRandomPort()));
  cConf.set(Constants.Dataset.TABLE_PREFIX,TABLE_PREFIX);
  cConf.set(Constants.CFG_HDFS_USER,System.getProperty(""String_Node_Str""));
  cConf.setLong(QueueConstants.QUEUE_CONFIG_UPDATE_FREQUENCY,10000L);
  cConf.setInt(QueueConstants.ConfigKeys.QUEUE_TABLE_PRESPLITS,4);
  cConf.setLong(TxConstants.Manager.CFG_TX_TIMEOUT,100000000L);
  cConf.setLong(TxConstants.Manager.CFG_TX_MAX_TIMEOUT,100000000L);
  injector=Guice.createInjector(new DataFabricModules().getDistributedModules(),new ConfigModule(cConf,hConf),new ZKClientModule(),new LocationRuntimeModule().getDistributedModules(),new NamespaceClientUnitTestModule().getModule(),new DiscoveryRuntimeModule().getDistributedModules(),new TransactionMetricsModule(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getMasterModule(),new DataSetsModules().getInMemoryModules(),new SystemDatasetRuntimeModule().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class).in(Scopes.SINGLETON);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
    }
  }
);
  hbaseAdmin=TEST_HBASE.getHBaseAdmin();
  ddlExecutor=new HBaseDDLExecutorFactory(cConf,hbaseAdmin.getConfiguration()).get();
  tableUtil=injector.getInstance(HBaseTableUtil.class);
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(NamespaceId.SYSTEM));
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(NAMESPACE_ID));
  ddlExecutor.createNamespaceIfNotExists(tableUtil.getHBaseNamespace(NAMESPACE_ID1));
  ConfigurationTable configTable=new ConfigurationTable(hConf);
  configTable.write(ConfigurationTable.Type.DEFAULT,cConf);
  zkClientService=injector.getInstance(ZKClientService.class);
  zkClientService.startAndWait();
  txService=injector.getInstance(TransactionService.class);
  Thread t=new Thread(){
    @Override public void run(){
      txService.start();
    }
  }
;
  t.start();
  txSystemClient=injector.getInstance(TransactionSystemClient.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
  executorFactory=injector.getInstance(TransactionExecutorFactory.class);
}"
4977,"@VisibleForTesting static Injector createInjector(CConfiguration cConf,Configuration hConf){
  return Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new LocationRuntimeModule().getDistributedModules(),new NamespaceClientRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new ExploreClientModule(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new KafkaClientModule(),new AuditModule().getDistributedModules(),new AuthorizationEnforcementModule().getDistributedModules(),new SecureStoreModules().getDistributedModules(),new AuthenticationContextModules().getMasterModule(),new MessagingClientModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Scopes.SINGLETON);
      bind(PrivilegesManager.class).to(RemotePrivilegesManager.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
    }
  }
);
}","@VisibleForTesting static Injector createInjector(CConfiguration cConf,Configuration hConf){
  return Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new LocationRuntimeModule().getDistributedModules(),new NamespaceClientRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new DataFabricModules(""String_Node_Str"").getDistributedModules(),new DataSetsModules().getDistributedModules(),new ExploreClientModule(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new KafkaClientModule(),new AuditModule().getDistributedModules(),new AuthorizationEnforcementModule().getDistributedModules(),new SecureStoreModules().getDistributedModules(),new AuthenticationContextModules().getMasterModule(),new MessagingClientModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Scopes.SINGLETON);
      bind(PrivilegesManager.class).to(RemotePrivilegesManager.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
    }
  }
);
}"
4978,"@VisibleForTesting static Injector createInjector(CConfiguration cConf,Configuration hConf){
  return Guice.createInjector(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MessagingClientModule(),new MetricsClientRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new LocationRuntimeModule().getDistributedModules(),new NamespaceClientRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new DataSetServiceModules().getDistributedModules(),new LoggingModules().getDistributedModules(),new ExploreClientModule(),new NamespaceClientRuntimeModule().getDistributedModules(),new MetadataServiceModule(),new RemoteSystemOperationsServiceModule(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new AuditModule().getDistributedModules(),new EntityVerifierModule(),new SecureStoreModules().getDistributedModules(),new AuthorizationEnforcementModule().getDistributedModules(),new AuthenticationContextModules().getMasterModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(Store.class).to(DefaultStore.class);
      bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
      bind(PrivilegesManager.class).to(RemotePrivilegesManager.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
    }
  }
);
}","@VisibleForTesting static Injector createInjector(CConfiguration cConf,Configuration hConf,String txClientId){
  return Guice.createInjector(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MessagingClientModule(),new MetricsClientRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new LocationRuntimeModule().getDistributedModules(),new NamespaceClientRuntimeModule().getDistributedModules(),new DataFabricModules(txClientId).getDistributedModules(),new DataSetsModules().getDistributedModules(),new DataSetServiceModules().getDistributedModules(),new LoggingModules().getDistributedModules(),new ExploreClientModule(),new NamespaceClientRuntimeModule().getDistributedModules(),new MetadataServiceModule(),new RemoteSystemOperationsServiceModule(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new AuditModule().getDistributedModules(),new EntityVerifierModule(),new SecureStoreModules().getDistributedModules(),new AuthorizationEnforcementModule().getDistributedModules(),new AuthenticationContextModules().getMasterModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(Store.class).to(DefaultStore.class);
      bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
      bind(PrivilegesManager.class).to(RemotePrivilegesManager.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
    }
  }
);
}"
4979,"@Override protected Injector doInit(TwillContext context){
  CConfiguration cConf=getCConfiguration();
  Configuration hConf=getConfiguration();
  cConf.set(Constants.Dataset.Executor.ADDRESS,context.getHost().getHostName());
  injector=createInjector(cConf,hConf);
  injector.getInstance(LogAppenderInitializer.class).initialize();
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(NamespaceId.SYSTEM.getNamespace(),Constants.Logging.COMPONENT_NAME,Constants.Service.DATASET_EXECUTOR));
  return injector;
}","@Override protected Injector doInit(TwillContext context){
  CConfiguration cConf=getCConfiguration();
  Configuration hConf=getConfiguration();
  cConf.set(Constants.Dataset.Executor.ADDRESS,context.getHost().getHostName());
  String txClientId=String.format(""String_Node_Str"",Constants.Service.DATASET_EXECUTOR,context.getInstanceId());
  injector=createInjector(cConf,hConf,txClientId);
  injector.getInstance(LogAppenderInitializer.class).initialize();
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(NamespaceId.SYSTEM.getNamespace(),Constants.Logging.COMPONENT_NAME,Constants.Service.DATASET_EXECUTOR));
  return injector;
}"
4980,"@Override public int hashCode(){
  int result=name.hashCode();
  result=31 * result + description.hashCode();
  result=31 * result + type.hashCode();
  result=31 * result + (required ? 1 : 0);
  result=31 * result + (macroSupported ? 1 : 0);
  return result;
}","@Override public int hashCode(){
  return Objects.hash(name,description,type,required,macroSupported,macroEscapingEnabled);
}"
4981,"@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  PluginPropertyField that=(PluginPropertyField)o;
  return required == that.required && name.equals(that.name) && description.equals(that.description) && type.equals(that.type) && macroSupported == that.macroSupported;
}","@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  PluginPropertyField that=(PluginPropertyField)o;
  return required == that.required && name.equals(that.name) && description.equals(that.description) && type.equals(that.type) && macroSupported == that.macroSupported && macroEscapingEnabled == that.macroEscapingEnabled;
}"
4982,"public PluginPropertyField(String name,String description,String type,boolean required,boolean macroSupported){
  if (name == null) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (description == null) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (type == null) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  this.name=name;
  this.description=description;
  this.type=type;
  this.required=required;
  this.macroSupported=macroSupported;
}","public PluginPropertyField(String name,String description,String type,boolean required,boolean macroSupported){
  this(name,description,type,required,macroSupported,false);
}"
4983,"private static Plugin getPlugin(Map.Entry<ArtifactDescriptor,PluginClass> pluginEntry,PluginProperties properties,String pluginType,String pluginName,PluginInstantiator pluginInstantiator){
  CollectMacroEvaluator collectMacroEvaluator=new CollectMacroEvaluator();
  MacroParser parser=new MacroParser(collectMacroEvaluator);
  for (  PluginPropertyField field : pluginEntry.getValue().getProperties().values()) {
    Preconditions.checkArgument(!field.isRequired() || (properties.getProperties().containsKey(field.getName())),""String_Node_Str"",field.getName(),pluginType,pluginName);
    if (field.isMacroSupported()) {
      parser.parse(properties.getProperties().get(field.getName()));
    }
  }
  ArtifactId artifact=pluginEntry.getKey().getArtifactId();
  try {
    pluginInstantiator.addArtifact(pluginEntry.getKey().getLocation(),artifact);
  }
 catch (  IOException e) {
    Throwables.propagate(e);
  }
  return new Plugin(artifact,pluginEntry.getValue(),properties.setMacros(collectMacroEvaluator.getMacros()));
}","private static Plugin getPlugin(Map.Entry<ArtifactDescriptor,PluginClass> pluginEntry,PluginProperties properties,String pluginType,String pluginName,PluginInstantiator pluginInstantiator){
  CollectMacroEvaluator collectMacroEvaluator=new CollectMacroEvaluator();
  for (  PluginPropertyField field : pluginEntry.getValue().getProperties().values()) {
    Preconditions.checkArgument(!field.isRequired() || (properties.getProperties().containsKey(field.getName())),""String_Node_Str"",field.getName(),pluginType,pluginName);
    if (field.isMacroSupported()) {
      MacroParser parser=new MacroParser(collectMacroEvaluator,field.isMacroEscapingEnabled());
      parser.parse(properties.getProperties().get(field.getName()));
    }
  }
  ArtifactId artifact=pluginEntry.getKey().getArtifactId();
  try {
    pluginInstantiator.addArtifact(pluginEntry.getKey().getLocation(),artifact);
  }
 catch (  IOException e) {
    Throwables.propagate(e);
  }
  return new Plugin(artifact,pluginEntry.getValue(),properties.setMacros(collectMacroEvaluator.getMacros()));
}"
4984,"public MacroParser(MacroEvaluator macroEvaluator){
  this.macroEvaluator=macroEvaluator;
}","public MacroParser(MacroEvaluator macroEvaluator,boolean escapingEnabled){
  this.macroEvaluator=macroEvaluator;
  this.escapingEnabled=escapingEnabled;
}"
4985,"/** 
 * Strips all preceding backslash '\' characters.
 * @param str the string to replace escaped syntax in
 * @return the string with no escaped syntax
 */
private String replaceEscapedSyntax(String str){
  StringBuilder syntaxRebuilder=new StringBuilder();
  boolean includeNextConsecutiveBackslash=false;
  for (int i=0; i < str.length(); i++) {
    if (str.charAt(i) != '\\' || includeNextConsecutiveBackslash) {
      syntaxRebuilder.append(str.charAt(i));
      includeNextConsecutiveBackslash=false;
    }
 else {
      includeNextConsecutiveBackslash=true;
    }
  }
  return syntaxRebuilder.toString();
}","/** 
 * Strips all preceding backslash '\' characters.
 * @param str the string to replace escaped syntax in
 * @return the string with no escaped syntax
 */
private String replaceEscapedSyntax(String str){
  if (!escapingEnabled) {
    return str;
  }
  StringBuilder syntaxRebuilder=new StringBuilder();
  boolean includeNextConsecutiveBackslash=false;
  for (int i=0; i < str.length(); i++) {
    if (str.charAt(i) != '\\' || includeNextConsecutiveBackslash) {
      syntaxRebuilder.append(str.charAt(i));
      includeNextConsecutiveBackslash=false;
    }
 else {
      includeNextConsecutiveBackslash=true;
    }
  }
  return syntaxRebuilder.toString();
}"
4986,"/** 
 * Returns whether or not the character at a given index in a string is escaped. Escaped characters have an odd number of preceding backslashes.
 * @param index the index of the character to check for escaping
 * @param str the string in which the character is located at 'index'
 * @return if the character at the provided index is escaped
 */
private boolean isEscaped(int index,String str){
  int numPrecedingParens=0;
  for (int i=index - 1; i >= 0; i--) {
    if (str.charAt(i) == '\\') {
      numPrecedingParens++;
    }
 else {
      break;
    }
  }
  return ((numPrecedingParens % 2) == 1);
}","/** 
 * Returns whether or not the character at a given index in a string is escaped. Escaped characters have an odd number of preceding backslashes.
 * @param index the index of the character to check for escaping
 * @param str the string in which the character is located at 'index'
 * @return if the character at the provided index is escaped
 */
private boolean isEscaped(int index,String str){
  if (!escapingEnabled) {
    return false;
  }
  int numPrecedingParens=0;
  for (int i=index - 1; i >= 0; i--) {
    if (str.charAt(i) == '\\') {
      numPrecedingParens++;
    }
 else {
      break;
    }
  }
  return ((numPrecedingParens % 2) == 1);
}"
4987,"private PluginProperties substituteMacros(Plugin plugin,@Nullable MacroEvaluator macroEvaluator){
  Map<String,String> properties=new HashMap<>();
  Map<String,PluginPropertyField> pluginPropertyFieldMap=plugin.getPluginClass().getProperties();
  boolean configTime=(macroEvaluator == null);
  TrackingMacroEvaluator trackingMacroEvaluator=new TrackingMacroEvaluator();
  MacroParser macroParser=new MacroParser(configTime ? trackingMacroEvaluator : macroEvaluator);
  for (  Map.Entry<String,String> property : plugin.getProperties().getProperties().entrySet()) {
    PluginPropertyField field=pluginPropertyFieldMap.get(property.getKey());
    String propertyValue=property.getValue();
    if (field != null && field.isMacroSupported()) {
      if (configTime) {
        macroParser.parse(propertyValue);
        propertyValue=getOriginalOrDefaultValue(propertyValue,property.getKey(),field.getType(),trackingMacroEvaluator);
      }
 else {
        propertyValue=macroParser.parse(propertyValue);
      }
    }
    properties.put(property.getKey(),propertyValue);
  }
  return PluginProperties.builder().addAll(properties).build();
}","private PluginProperties substituteMacros(Plugin plugin,@Nullable MacroEvaluator macroEvaluator){
  Map<String,String> properties=new HashMap<>();
  Map<String,PluginPropertyField> pluginPropertyFieldMap=plugin.getPluginClass().getProperties();
  boolean configTime=(macroEvaluator == null);
  TrackingMacroEvaluator trackingMacroEvaluator=new TrackingMacroEvaluator();
  for (  Map.Entry<String,String> property : plugin.getProperties().getProperties().entrySet()) {
    PluginPropertyField field=pluginPropertyFieldMap.get(property.getKey());
    String propertyValue=property.getValue();
    if (field != null && field.isMacroSupported()) {
      if (configTime) {
        MacroParser macroParser=new MacroParser(trackingMacroEvaluator,field.isMacroEscapingEnabled());
        macroParser.parse(propertyValue);
        propertyValue=getOriginalOrDefaultValue(propertyValue,property.getKey(),field.getType(),trackingMacroEvaluator);
      }
 else {
        MacroParser macroParser=new MacroParser(macroEvaluator,field.isMacroEscapingEnabled());
        propertyValue=macroParser.parse(propertyValue);
      }
    }
    properties.put(property.getKey(),propertyValue);
  }
  return PluginProperties.builder().addAll(properties).build();
}"
4988,"private Set<String> getFieldsWithMacro(Plugin plugin){
  Set<String> macroFields=new HashSet<>();
  Map<String,PluginPropertyField> pluginPropertyFieldMap=plugin.getPluginClass().getProperties();
  TrackingMacroEvaluator trackingMacroEvaluator=new TrackingMacroEvaluator();
  MacroParser macroParser=new MacroParser(trackingMacroEvaluator);
  for (  Map.Entry<String,PluginPropertyField> pluginEntry : pluginPropertyFieldMap.entrySet()) {
    if (pluginEntry.getValue() != null && pluginEntry.getValue().isMacroSupported()) {
      String macroValue=plugin.getProperties().getProperties().get(pluginEntry.getKey());
      if (macroValue != null) {
        macroParser.parse(macroValue);
        if (trackingMacroEvaluator.hasMacro()) {
          macroFields.add(pluginEntry.getKey());
          trackingMacroEvaluator.reset();
        }
      }
    }
  }
  return macroFields;
}","private Set<String> getFieldsWithMacro(Plugin plugin){
  Set<String> macroFields=new HashSet<>();
  Map<String,PluginPropertyField> pluginPropertyFieldMap=plugin.getPluginClass().getProperties();
  TrackingMacroEvaluator trackingMacroEvaluator=new TrackingMacroEvaluator();
  for (  Map.Entry<String,PluginPropertyField> pluginEntry : pluginPropertyFieldMap.entrySet()) {
    PluginPropertyField pluginField=pluginEntry.getValue();
    if (pluginEntry.getValue() != null && pluginField.isMacroSupported()) {
      String macroValue=plugin.getProperties().getProperties().get(pluginEntry.getKey());
      if (macroValue != null) {
        MacroParser macroParser=new MacroParser(trackingMacroEvaluator,pluginField.isMacroEscapingEnabled());
        macroParser.parse(macroValue);
        if (trackingMacroEvaluator.hasMacro()) {
          macroFields.add(pluginEntry.getKey());
          trackingMacroEvaluator.reset();
        }
      }
    }
  }
  return macroFields;
}"
4989,"@Test public void testContainsSimpleEscapedMacro() throws InvalidMacroException {
  assertContainsMacroParsing(""String_Node_Str"",true);
}","@Test public void testContainsSimpleEscapedMacro() throws InvalidMacroException {
  assertContainsMacroParsing(""String_Node_Str"",true);
  assertSubstitution(""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,String>of(),ImmutableMap.<String,String>of());
}"
4990,"@ProcessInput public void process(Purchase purchase){
  URL serviceURL=getContext().getServiceURL(PurchaseApp.APP_NAME,CatalogLookupService.SERVICE_NAME);
  if (serviceURL != null) {
    String catalog=getCatalogId(serviceURL,purchase.getProduct());
    if (catalog != null) {
      purchase.setCatalogId(catalog);
    }
  }
  metrics.count(""String_Node_Str"" + purchase.getCustomer(),1);
  LOG.info(""String_Node_Str"",purchase.getCustomer(),purchase.getProduct(),purchase.getCatalogId());
  store.write(Bytes.toBytes(purchase.getPurchaseTime()),purchase);
}","@ProcessInput public void process(Purchase purchase){
  URL serviceURL=getContext().getServiceURL(PurchaseApp.APP_NAME,CatalogLookupService.SERVICE_NAME);
  if (serviceURL != null) {
    String catalog=getCatalogId(serviceURL,purchase.getProduct());
    if (catalog != null) {
      purchase.setCatalogId(catalog);
    }
  }
  metrics.count(""String_Node_Str"" + purchase.getCustomer(),1);
  LOG.info(""String_Node_Str"",purchase.getCustomer(),purchase.getProduct(),purchase.getCatalogId());
  store.write(Bytes.toBytes(UUID.randomUUID()),purchase);
}"
4991,"private void initialize(){
  if (calendar == null) {
    ValidationResult vr=doValidate();
    Calendar calendar=vr.getCalendar();
    calendar.setTime(vr.getStartDate());
    startHour=calendar.get(Calendar.HOUR_OF_DAY);
    startMinute=calendar.get(Calendar.MINUTE);
    calendar.setTime(vr.getEndDate());
    endHour=calendar.get(Calendar.HOUR_OF_DAY);
    endMinute=calendar.get(Calendar.MINUTE);
    this.calendar=calendar;
  }
}","private void initialize(){
  if (calendar == null) {
    ValidationResult vr=doValidate();
    Calendar calendar=vr.getCalendar();
    calendar.setTime(vr.getStartDate());
    startHour=calendar.get(Calendar.HOUR_OF_DAY);
    startMinute=calendar.get(Calendar.MINUTE);
    calendar.setTime(vr.getEndDate());
    endHour=calendar.get(Calendar.HOUR_OF_DAY);
    endMinute=calendar.get(Calendar.MINUTE);
    isStartTimeSmaller=vr.getStartDate().compareTo(vr.getEndDate()) < 0;
    this.calendar=calendar;
  }
}"
4992,"@Override public ConstraintResult check(ProgramSchedule schedule,ConstraintContext context){
  initialize();
  calendar.setTimeInMillis(context.getCheckTime());
  int hourOfDay=calendar.get(Calendar.HOUR_OF_DAY);
  int minute=calendar.get(Calendar.MINUTE);
  boolean pastOrEqualStartRange=hourOfDay > startHour || (hourOfDay == startHour && minute >= startMinute);
  boolean pastEndRange=hourOfDay > endHour || (hourOfDay == endHour && minute >= endMinute);
  boolean satisfied=pastOrEqualStartRange && !pastEndRange;
  if (satisfied) {
    return ConstraintResult.SATISFIED;
  }
  if (pastEndRange) {
    calendar.add(Calendar.DAY_OF_YEAR,1);
  }
  calendar.set(Calendar.HOUR_OF_DAY,startHour);
  calendar.set(Calendar.MINUTE,startMinute);
  calendar.set(Calendar.SECOND,0);
  calendar.set(Calendar.MILLISECOND,0);
  return new ConstraintResult(ConstraintResult.SatisfiedState.NOT_SATISFIED,calendar.getTimeInMillis() - context.getCheckTime());
}","@Override public ConstraintResult check(ProgramSchedule schedule,ConstraintContext context){
  initialize();
  calendar.setTimeInMillis(context.getCheckTime());
  int hourOfDay=calendar.get(Calendar.HOUR_OF_DAY);
  int minute=calendar.get(Calendar.MINUTE);
  boolean pastOrEqualStartRange=hourOfDay > startHour || (hourOfDay == startHour && minute >= startMinute);
  boolean pastOrEqualEndRange=hourOfDay > endHour || (hourOfDay == endHour && minute >= endMinute);
  if (isStartTimeSmaller) {
    boolean satisfied=pastOrEqualStartRange && !pastOrEqualEndRange;
    if (satisfied) {
      return ConstraintResult.SATISFIED;
    }
  }
 else {
    boolean satisfied=pastOrEqualStartRange || !pastOrEqualEndRange;
    if (satisfied) {
      return ConstraintResult.SATISFIED;
    }
  }
  if (pastOrEqualEndRange && isStartTimeSmaller) {
    calendar.add(Calendar.DAY_OF_YEAR,1);
  }
  calendar.set(Calendar.HOUR_OF_DAY,startHour);
  calendar.set(Calendar.MINUTE,startMinute);
  calendar.set(Calendar.SECOND,0);
  calendar.set(Calendar.MILLISECOND,0);
  return new ConstraintResult(ConstraintResult.SatisfiedState.NOT_SATISFIED,calendar.getTimeInMillis() - context.getCheckTime());
}"
4993,"@Test public void testInit(){
  TimeRangeConstraint timeRangeConstraint=new TimeRangeConstraint(""String_Node_Str"",""String_Node_Str"",TimeZone.getTimeZone(""String_Node_Str""));
  Assert.assertEquals(""String_Node_Str"",timeRangeConstraint.getTimeZone());
}","@Test public void testInit(){
  TimeRangeConstraint timeRangeConstraint=new TimeRangeConstraint(""String_Node_Str"",""String_Node_Str"",TimeZone.getTimeZone(""String_Node_Str""));
  Assert.assertEquals(""String_Node_Str"",timeRangeConstraint.getTimeZone());
  new TimeRangeConstraint(""String_Node_Str"",""String_Node_Str"",TimeZone.getDefault());
  new TimeRangeConstraint(""String_Node_Str"",""String_Node_Str"",TimeZone.getDefault());
  new TimeRangeConstraint(""String_Node_Str"",""String_Node_Str"",TimeZone.getDefault());
  try {
    new TimeRangeConstraint(""String_Node_Str"",""String_Node_Str"",TimeZone.getDefault());
    Assert.fail();
  }
 catch (  IllegalArgumentException e) {
  }
}"
4994,"/** 
 * Performs the validation. Can be called by subclasses to validate and initialize.
 * @return Calendar, start and end date as a ValidationResult.
 */
protected ValidationResult doValidate(){
  ProtoConstraint.validateNotNull(timeZone,""String_Node_Str"");
  TimeZone tz=TimeZone.getTimeZone(timeZone);
  Calendar calendar=Calendar.getInstance(tz);
  DateFormat formatter=new SimpleDateFormat(""String_Node_Str"");
  formatter.setTimeZone(tz);
  Date startDate, endDate;
  try {
    startDate=formatter.parse(startTime);
  }
 catch (  ParseException e) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",startTime),e);
  }
  try {
    endDate=formatter.parse(endTime);
  }
 catch (  ParseException e) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",endTime),e);
  }
  if (startDate.compareTo(endDate) >= 0) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  return new ValidationResult(calendar,startDate,endDate);
}","/** 
 * Performs the validation. Can be called by subclasses to validate and initialize.
 * @return Calendar, start and end date as a ValidationResult.
 */
protected ValidationResult doValidate(){
  ProtoConstraint.validateNotNull(timeZone,""String_Node_Str"");
  TimeZone tz=TimeZone.getTimeZone(timeZone);
  Calendar calendar=Calendar.getInstance(tz);
  DateFormat formatter=new SimpleDateFormat(""String_Node_Str"");
  formatter.setTimeZone(tz);
  Date startDate, endDate;
  try {
    startDate=formatter.parse(startTime);
  }
 catch (  ParseException e) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",startTime),e);
  }
  try {
    endDate=formatter.parse(endTime);
  }
 catch (  ParseException e) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",endTime),e);
  }
  if (startDate.compareTo(endDate) == 0) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",startTime,endTime));
  }
  return new ValidationResult(calendar,startDate,endDate);
}"
4995,"public static ArtifactVersionRange parse(String artifactVersionStr) throws InvalidArtifactRangeException {
  if (!isVersionRange(artifactVersionStr)) {
    ArtifactVersion version=new ArtifactVersion(artifactVersionStr);
    if (version.getVersion() == null) {
      throw new InvalidArtifactRangeException(String.format(""String_Node_Str"",version));
    }
    return new ArtifactVersionRange(version,true,version,true);
  }
  boolean isLowerInclusive=artifactVersionStr.charAt(0) == '[';
  int commaIndex=artifactVersionStr.indexOf(',',1);
  if (commaIndex < 0) {
    throw new InvalidArtifactRangeException(String.format(""String_Node_Str"",artifactVersionStr));
  }
  String lowerStr=artifactVersionStr.substring(1,commaIndex).trim();
  ArtifactVersion lower=new ArtifactVersion(lowerStr);
  if (lower.getVersion() == null) {
    throw new InvalidArtifactRangeException(String.format(""String_Node_Str"",artifactVersionStr,lowerStr));
  }
  int versionEndIndex=indexOf(artifactVersionStr,']',')',commaIndex + 1);
  if (versionEndIndex < 0) {
    throw new InvalidArtifactRangeException(String.format(""String_Node_Str"",artifactVersionStr));
  }
  String upperStr=artifactVersionStr.substring(commaIndex + 1,versionEndIndex).trim();
  ArtifactVersion upper=new ArtifactVersion(upperStr);
  if (upper.getVersion() == null) {
    throw new InvalidArtifactRangeException(String.format(""String_Node_Str"",artifactVersionStr,upperStr));
  }
  boolean isUpperInclusive=artifactVersionStr.charAt(versionEndIndex) == ']';
  int comp=lower.compareTo(upper);
  if (comp > 0) {
    throw new InvalidArtifactRangeException(String.format(""String_Node_Str"",artifactVersionStr,lowerStr,upperStr));
  }
 else   if (comp == 0 && isLowerInclusive && !isUpperInclusive) {
    throw new InvalidArtifactRangeException(String.format(""String_Node_Str"" + ""String_Node_Str"",artifactVersionStr,lowerStr));
  }
  return new ArtifactVersionRange(lower,isLowerInclusive,upper,isUpperInclusive);
}","public static ArtifactVersionRange parse(String artifactVersionStr) throws InvalidArtifactRangeException {
  if (!isVersionRange(artifactVersionStr)) {
    ArtifactVersion version=new ArtifactVersion(artifactVersionStr);
    if (version.getVersion() == null) {
      throw new InvalidArtifactRangeException(String.format(""String_Node_Str"",version));
    }
    return new ArtifactVersionRange(version,true,version,true);
  }
  boolean isLowerInclusive=artifactVersionStr.charAt(0) == '[';
  int commaIndex=artifactVersionStr.indexOf(',',1);
  if (commaIndex < 0) {
    throw new InvalidArtifactRangeException(String.format(""String_Node_Str"",artifactVersionStr));
  }
  String lowerStr=artifactVersionStr.substring(1,commaIndex).trim();
  ArtifactVersion lower=new ArtifactVersion(lowerStr);
  if (lower.getVersion() == null) {
    throw new InvalidArtifactRangeException(String.format(""String_Node_Str"",artifactVersionStr,lowerStr));
  }
  int versionEndIndex=indexOf(artifactVersionStr,']',')',commaIndex + 1);
  if (versionEndIndex < 0) {
    throw new InvalidArtifactRangeException(String.format(""String_Node_Str"",artifactVersionStr));
  }
  if (versionEndIndex != artifactVersionStr.length() - 1) {
    throw new InvalidArtifactRangeException(String.format(""String_Node_Str"",artifactVersionStr));
  }
  String upperStr=artifactVersionStr.substring(commaIndex + 1,versionEndIndex).trim();
  ArtifactVersion upper=new ArtifactVersion(upperStr);
  if (upper.getVersion() == null) {
    throw new InvalidArtifactRangeException(String.format(""String_Node_Str"",artifactVersionStr,upperStr));
  }
  boolean isUpperInclusive=artifactVersionStr.charAt(versionEndIndex) == ']';
  int comp=lower.compareTo(upper);
  if (comp > 0) {
    throw new InvalidArtifactRangeException(String.format(""String_Node_Str"",artifactVersionStr,lowerStr,upperStr));
  }
 else   if (comp == 0 && isLowerInclusive && !isUpperInclusive) {
    throw new InvalidArtifactRangeException(String.format(""String_Node_Str"" + ""String_Node_Str"",artifactVersionStr,lowerStr));
  }
  return new ArtifactVersionRange(lower,isLowerInclusive,upper,isUpperInclusive);
}"
4996,"@Override public void register(DatasetDefinitionRegistry registry){
  DatasetDefinition<Table,? extends DatasetAdmin> tableDef=registry.get(Table.class.getName());
  registry.add(new JobQueueDatasetDefinition(JobQueueDataset.class.getName(),tableDef));
  DatasetDefinition<IndexedTable,? extends DatasetAdmin> indexedTableDef=registry.get(IndexedTable.class.getName());
  registry.add(new ProgramScheduleStoreDefinition(ProgramScheduleStoreDataset.class.getName(),indexedTableDef));
}","@Override public void register(DatasetDefinitionRegistry registry){
  DatasetDefinition<Table,? extends DatasetAdmin> tableDef=registry.get(Table.class.getName());
  registry.add(new JobQueueDatasetDefinition(JobQueueDataset.class.getName(),tableDef));
  DatasetDefinition<IndexedTable,? extends DatasetAdmin> indexedTableDef=registry.get(IndexedTable.class.getName());
  registry.add(new ProgramScheduleStoreDefinition(Schedulers.STORE_TYPE_NAME,indexedTableDef));
}"
4997,"@Test public void checkDatasetType() throws DatasetManagementException {
  DatasetFramework dsFramework=getInjector().getInstance(DatasetFramework.class);
  Assert.assertTrue(dsFramework.hasType(NamespaceId.SYSTEM.datasetType(ProgramScheduleStoreDataset.class.getName())));
}","@Test public void checkDatasetType() throws DatasetManagementException {
  DatasetFramework dsFramework=getInjector().getInstance(DatasetFramework.class);
  Assert.assertTrue(dsFramework.hasType(NamespaceId.SYSTEM.datasetType(Schedulers.STORE_TYPE_NAME)));
}"
4998,"@VisibleForTesting @Inject public ArtifactRepository(CConfiguration cConf,ArtifactStore artifactStore,MetadataStore metadataStore,PrivilegesManager privilegesManager,ProgramRunnerFactory programRunnerFactory,Impersonator impersonator,AuthorizationEnforcer authorizationEnforcer,AuthenticationContext authenticationContext){
  this.artifactStore=artifactStore;
  this.artifactClassLoaderFactory=new ArtifactClassLoaderFactory(cConf,programRunnerFactory);
  this.artifactInspector=new ArtifactInspector(cConf,artifactClassLoaderFactory);
  this.systemArtifactDirs=new ArrayList<>();
  for (  String dir : cConf.get(Constants.AppFabric.SYSTEM_ARTIFACTS_DIR).split(""String_Node_Str"")) {
    File file=new File(dir);
    if (!file.isDirectory()) {
      LOG.warn(""String_Node_Str"",file);
      continue;
    }
    systemArtifactDirs.add(file);
  }
  this.configReader=new ArtifactConfigReader();
  this.metadataStore=metadataStore;
  this.privilegesManager=privilegesManager;
  this.impersonator=impersonator;
  this.authorizationEnforcer=authorizationEnforcer;
  this.authenticationContext=authenticationContext;
}","@VisibleForTesting @Inject public ArtifactRepository(CConfiguration cConf,ArtifactStore artifactStore,MetadataStore metadataStore,PrivilegesManager privilegesManager,ProgramRunnerFactory programRunnerFactory,Impersonator impersonator,AuthorizationEnforcer authorizationEnforcer,AuthenticationContext authenticationContext){
  this.artifactStore=artifactStore;
  this.artifactClassLoaderFactory=new ArtifactClassLoaderFactory(cConf,programRunnerFactory);
  this.artifactInspector=new ArtifactInspector(cConf,artifactClassLoaderFactory);
  this.systemArtifactDirs=new ArrayList<>();
  String systemArtifactsDir=cConf.get(Constants.AppFabric.SYSTEM_ARTIFACTS_DIR);
  if (!Strings.isNullOrEmpty(systemArtifactsDir)) {
    for (    String dir : systemArtifactsDir.split(""String_Node_Str"")) {
      File file=new File(dir);
      if (!file.isDirectory()) {
        LOG.warn(""String_Node_Str"",file);
        continue;
      }
      systemArtifactDirs.add(file);
    }
  }
  this.configReader=new ArtifactConfigReader();
  this.metadataStore=metadataStore;
  this.privilegesManager=privilegesManager;
  this.impersonator=impersonator;
  this.authorizationEnforcer=authorizationEnforcer;
  this.authenticationContext=authenticationContext;
}"
4999,"private static CConfiguration createCConf(File localDataDir) throws IOException {
  CConfiguration cConf=CConfiguration.create();
  cConf.setBoolean(Constants.Explore.EXPLORE_ENABLED,true);
  cConf.setBoolean(Constants.Explore.START_ON_DEMAND,false);
  for (  String key : System.getProperties().stringPropertyNames()) {
    if (key.startsWith(TestConfiguration.PROPERTY_PREFIX)) {
      String value=System.getProperty(key);
      cConf.set(key.substring(TestConfiguration.PROPERTY_PREFIX.length()),System.getProperty(key));
      LOG.info(""String_Node_Str"",key,value);
    }
  }
  String localhost=InetAddress.getLoopbackAddress().getHostAddress();
  cConf.set(Constants.Service.MASTER_SERVICES_BIND_ADDRESS,localhost);
  cConf.set(Constants.Transaction.Container.ADDRESS,localhost);
  cConf.set(Constants.Dataset.Executor.ADDRESS,localhost);
  cConf.set(Constants.Stream.ADDRESS,localhost);
  cConf.set(Constants.Metrics.ADDRESS,localhost);
  cConf.set(Constants.Metrics.SERVER_ADDRESS,localhost);
  cConf.set(Constants.MetricsProcessor.ADDRESS,localhost);
  cConf.set(Constants.LogSaver.ADDRESS,localhost);
  cConf.set(Constants.Security.AUTH_SERVER_BIND_ADDRESS,localhost);
  cConf.set(Constants.Explore.SERVER_ADDRESS,localhost);
  cConf.set(Constants.Metadata.SERVICE_BIND_ADDRESS,localhost);
  cConf.set(Constants.Metrics.SERVER_PORT,Integer.toString(Networks.getRandomPort()));
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,localDataDir.getAbsolutePath());
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  cConf.set(Constants.Explore.LOCAL_DATA_DIR,TMP_FOLDER.newFolder(""String_Node_Str"").getAbsolutePath());
  return cConf;
}","private static CConfiguration createCConf(File localDataDir) throws IOException {
  CConfiguration cConf=CConfiguration.create();
  cConf.setBoolean(Constants.Explore.EXPLORE_ENABLED,true);
  cConf.setBoolean(Constants.Explore.START_ON_DEMAND,false);
  cConf.set(Constants.AppFabric.SYSTEM_ARTIFACTS_DIR,""String_Node_Str"");
  for (  String key : System.getProperties().stringPropertyNames()) {
    if (key.startsWith(TestConfiguration.PROPERTY_PREFIX)) {
      String value=System.getProperty(key);
      cConf.set(key.substring(TestConfiguration.PROPERTY_PREFIX.length()),System.getProperty(key));
      LOG.info(""String_Node_Str"",key,value);
    }
  }
  String localhost=InetAddress.getLoopbackAddress().getHostAddress();
  cConf.set(Constants.Service.MASTER_SERVICES_BIND_ADDRESS,localhost);
  cConf.set(Constants.Transaction.Container.ADDRESS,localhost);
  cConf.set(Constants.Dataset.Executor.ADDRESS,localhost);
  cConf.set(Constants.Stream.ADDRESS,localhost);
  cConf.set(Constants.Metrics.ADDRESS,localhost);
  cConf.set(Constants.Metrics.SERVER_ADDRESS,localhost);
  cConf.set(Constants.MetricsProcessor.ADDRESS,localhost);
  cConf.set(Constants.LogSaver.ADDRESS,localhost);
  cConf.set(Constants.Security.AUTH_SERVER_BIND_ADDRESS,localhost);
  cConf.set(Constants.Explore.SERVER_ADDRESS,localhost);
  cConf.set(Constants.Metadata.SERVICE_BIND_ADDRESS,localhost);
  cConf.set(Constants.Metrics.SERVER_PORT,Integer.toString(Networks.getRandomPort()));
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,localDataDir.getAbsolutePath());
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  cConf.set(Constants.Explore.LOCAL_DATA_DIR,TMP_FOLDER.newFolder(""String_Node_Str"").getAbsolutePath());
  return cConf;
}"
5000,"/** 
 * Prepares the Spark 1 framework on the location
 * @param sparkConf the spark configuration
 * @param locationFactory the {@link LocationFactory} for saving the spark framework jar
 * @return A {@link SparkFramework} containing information about the spark framework in localization context.
 * @throws IOException If failed to prepare the framework.
 */
private static SparkFramework prepareSpark1Framework(Properties sparkConf,LocationFactory locationFactory) throws IOException {
  String sparkYarnJar=sparkConf.getProperty(SPARK_YARN_JAR);
  if (sparkYarnJar != null) {
    Location frameworkLocation=locationFactory.create(URI.create(sparkYarnJar));
    if (frameworkLocation.exists()) {
      return new SparkFramework(new LocalizeResource(resolveURI(frameworkLocation),false),SPARK_YARN_JAR);
    }
    LOG.warn(""String_Node_Str"",frameworkLocation,SPARK_YARN_JAR);
  }
  File sparkAssemblyJar=Iterables.getFirst(getLocalSparkLibrary(SparkCompat.SPARK1_2_10),null);
  Location frameworkDir=locationFactory.create(""String_Node_Str"");
  Location frameworkLocation=frameworkDir.append(sparkAssemblyJar.getName());
  if (!frameworkLocation.exists()) {
    frameworkDir.mkdirs(""String_Node_Str"");
    try (OutputStream os=frameworkLocation.getOutputStream(""String_Node_Str"")){
      Files.copy(sparkAssemblyJar.toPath(),os);
    }
   }
  return new SparkFramework(new LocalizeResource(resolveURI(frameworkLocation),false),SPARK_YARN_JAR);
}","/** 
 * Prepares the Spark 1 framework on the location
 * @param sparkConf the spark configuration
 * @param locationFactory the {@link LocationFactory} for saving the spark framework jar
 * @return A {@link SparkFramework} containing information about the spark framework in localization context.
 * @throws IOException If failed to prepare the framework.
 */
private static SparkFramework prepareSpark1Framework(Properties sparkConf,LocationFactory locationFactory) throws IOException {
  String sparkYarnJar=sparkConf.getProperty(SPARK_YARN_JAR);
  if (sparkYarnJar != null) {
    URI sparkYarnJarURI=URI.create(sparkYarnJar);
    if (locationFactory.getHomeLocation().toURI().getScheme().equals(sparkYarnJarURI.getScheme())) {
      Location frameworkLocation=locationFactory.create(sparkYarnJarURI);
      if (frameworkLocation.exists()) {
        return new SparkFramework(new LocalizeResource(resolveURI(frameworkLocation),false),SPARK_YARN_JAR);
      }
      LOG.warn(""String_Node_Str"",frameworkLocation,SPARK_YARN_JAR);
    }
  }
  File sparkAssemblyJar=Iterables.getFirst(getLocalSparkLibrary(SparkCompat.SPARK1_2_10),null);
  Location frameworkDir=locationFactory.create(""String_Node_Str"");
  Location frameworkLocation=frameworkDir.append(sparkAssemblyJar.getName());
  if (!frameworkLocation.exists()) {
    frameworkDir.mkdirs(""String_Node_Str"");
    try (OutputStream os=frameworkLocation.getOutputStream(""String_Node_Str"")){
      Files.copy(sparkAssemblyJar.toPath(),os);
    }
   }
  return new SparkFramework(new LocalizeResource(resolveURI(frameworkLocation),false),SPARK_YARN_JAR);
}"
