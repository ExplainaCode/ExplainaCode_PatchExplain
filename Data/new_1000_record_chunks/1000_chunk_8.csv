record_number,buggy_code,fixed_code
7001,"/** 
 * Closes the scanner and releases any resources.
 */
public void close();","/** 
 * Closes the scanner and releases any resources.
 */
void close();"
7002,"/** 
 * Returns value of the given argument key as a String[]
 * @param argsKey {@link String} which is the key for the argument
 * @return String[] containing all the arguments which is indexed by their position as they were supplied
 */
public String[] getRuntimeArguments(String argsKey);","/** 
 * Returns value of the given argument key as a String[]
 * @param argsKey {@link String} which is the key for the argument
 * @return String[] containing all the arguments which is indexed by their position as they were supplied
 */
String[] getRuntimeArguments(String argsKey);"
7003,"/** 
 * Returns a   {@link Serializable} {@link ServiceDiscoverer} for Service Discovery in Spark Program which can bepassed in Spark program's closures.
 * @return A {@link Serializable} {@link ServiceDiscoverer}
 */
public ServiceDiscoverer getServiceDiscoverer();","/** 
 * Returns a   {@link Serializable} {@link ServiceDiscoverer} for Service Discovery in Spark Program which can bepassed in Spark program's closures.
 * @return A {@link Serializable} {@link ServiceDiscoverer}
 */
ServiceDiscoverer getServiceDiscoverer();"
7004,"/** 
 * Returns a   {@link Serializable} {@link Metrics} which can be used to emit custom metrics from user's {@link Spark}program. This can also be passed in Spark program's closures and workers can emit their own metrics
 * @return {@link Serializable} {@link Metrics} for {@link Spark} programs
 */
public Metrics getMetrics();","/** 
 * Returns a   {@link Serializable} {@link Metrics} which can be used to emit custom metrics from user's {@link Spark}program. This can also be passed in Spark program's closures and workers can emit their own metrics
 * @return {@link Serializable} {@link Metrics} for {@link Spark} programs
 */
Metrics getMetrics();"
7005,"/** 
 * User Spark job which will be executed
 * @param context {@link SparkContext} for this job
 */
public void run(SparkContext context);","/** 
 * User Spark job which will be executed
 * @param context {@link SparkContext} for this job
 */
void run(SparkContext context);"
7006,"/** 
 * Get the Hadoop counters from the previous MapReduce program in the Workflow. The method returns null if the counters are not set.
 * @return the Hadoop MapReduce counters set by the previous MapReduce program
 */
@Nullable public Map<String,Map<String,Long>> getMapReduceCounters();","/** 
 * Get the Hadoop counters from the previous MapReduce program in the Workflow. The method returns null if the counters are not set.
 * @return the Hadoop MapReduce counters set by the previous MapReduce program
 */
@Nullable Map<String,Map<String,Long>> getMapReduceCounters();"
7007,"@Provides @Named(Constants.AppFabric.SERVER_ADDRESS) public final InetAddress providesHostname(CConfiguration cConf){
  return Networks.resolve(cConf.get(Constants.AppFabric.SERVER_ADDRESS),new InetSocketAddress(""String_Node_Str"",0).getAddress());
}","@Provides @Named(Constants.AppFabric.SERVER_ADDRESS) public InetAddress providesHostname(CConfiguration cConf){
  return Networks.resolve(cConf.get(Constants.AppFabric.SERVER_ADDRESS),new InetSocketAddress(""String_Node_Str"",0).getAddress());
}"
7008,"/** 
 * @param runId for which information will be returned.
 * @return a {@link MRJobInfo} containing information about a particular MapReduce program run.
 */
public MRJobInfo getMRJobInfo(Id.Run runId) throws Exception ;","/** 
 * @param runId for which information will be returned.
 * @return a {@link MRJobInfo} containing information about a particular MapReduce program run.
 */
MRJobInfo getMRJobInfo(Id.Run runId) throws Exception ;"
7009,"/** 
 * @return A version.
 */
public int get();","/** 
 * @return A version.
 */
int get();"
7010,"/** 
 * Report resource usage of a program.  Implementors will likely want to write usage to persistant storage.
 */
public void reportResources();","/** 
 * Report resource usage of a program.  Implementors will likely want to write usage to persistant storage.
 */
void reportResources();"
7011,"/** 
 * Lists all namespaces
 * @return a list of {@link NamespaceMeta} for all namespaces
 */
public List<NamespaceMeta> listNamespaces();","/** 
 * Lists all namespaces
 * @return a list of {@link NamespaceMeta} for all namespaces
 */
List<NamespaceMeta> listNamespaces();"
7012,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 * @throws co.cask.cdap.common.exception.NamespaceCannotBeCreatedException if the creation operation was unsuccessful
 */
public void createNamespace(NamespaceMeta metadata) throws NamespaceAlreadyExistsException, NamespaceCannotBeCreatedException ;","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 * @throws co.cask.cdap.common.exception.NamespaceCannotBeCreatedException if the creation operation was unsuccessful
 */
void createNamespace(NamespaceMeta metadata) throws NamespaceAlreadyExistsException, NamespaceCannotBeCreatedException ;"
7013,"/** 
 * Deletes the specified namespace
 * @param namespaceId the {@link Id.Namespace} of the specified namespace
 * @throws NamespaceNotFoundException if the specified namespace does not exist
 * @throws NamespaceCannotBeDeletedException if the deletion operation was unsuccessful
 */
public void deleteNamespace(Id.Namespace namespaceId) throws NamespaceNotFoundException, NamespaceCannotBeDeletedException ;","/** 
 * Deletes the specified namespace
 * @param namespaceId the {@link Id.Namespace} of the specified namespace
 * @throws NamespaceNotFoundException if the specified namespace does not exist
 * @throws NamespaceCannotBeDeletedException if the deletion operation was unsuccessful
 */
void deleteNamespace(Id.Namespace namespaceId) throws NamespaceNotFoundException, NamespaceCannotBeDeletedException ;"
7014,"/** 
 * Deletes all datasets in the specified namespace
 * @param namespaceId the {@link Id.Namespace} of the specified namespace
 * @throws NotFoundException if the specified namespace does not exist
 * @throws NamespaceCannotBeDeletedException if the deletion operation was unsuccessful
 */
public void deleteDatasets(Id.Namespace namespaceId) throws NotFoundException, NamespaceCannotBeDeletedException ;","/** 
 * Deletes all datasets in the specified namespace
 * @param namespaceId the {@link Id.Namespace} of the specified namespace
 * @throws NotFoundException if the specified namespace does not exist
 * @throws NamespaceCannotBeDeletedException if the deletion operation was unsuccessful
 */
void deleteDatasets(Id.Namespace namespaceId) throws NotFoundException, NamespaceCannotBeDeletedException ;"
7015,"/** 
 * Gets details of a namespace
 * @param namespaceId the {@link Id.Namespace} of the requested namespace
 * @return the {@link NamespaceMeta} of the requested namespace
 * @throws NamespaceNotFoundException if the requested namespace is not found
 */
public NamespaceMeta getNamespace(Id.Namespace namespaceId) throws NamespaceNotFoundException ;","/** 
 * Gets details of a namespace
 * @param namespaceId the {@link Id.Namespace} of the requested namespace
 * @return the {@link NamespaceMeta} of the requested namespace
 * @throws NamespaceNotFoundException if the requested namespace is not found
 */
NamespaceMeta getNamespace(Id.Namespace namespaceId) throws NamespaceNotFoundException ;"
7016,"/** 
 * Update namespace properties for a given namespace.
 * @param namespaceId  the {@link Id.Namespace} of the namespace to be updated
 * @param namespaceMeta namespacemeta to update
 * @throws NotFoundException if the specified namespace is not found
 */
public void updateProperties(Id.Namespace namespaceId,NamespaceMeta namespaceMeta) throws NotFoundException ;","/** 
 * Update namespace properties for a given namespace.
 * @param namespaceId  the {@link Id.Namespace} of the namespace to be updated
 * @param namespaceMeta namespacemeta to update
 * @throws NotFoundException if the specified namespace is not found
 */
void updateProperties(Id.Namespace namespaceId,NamespaceMeta namespaceMeta) throws NotFoundException ;"
7017,"/** 
 * Checks if the specified namespace exists
 * @param namespaceId the {@link Id.Namespace} to check for existence
 * @return true, if the specifed namespace exists, false otherwise
 */
public boolean hasNamespace(Id.Namespace namespaceId);","/** 
 * Checks if the specified namespace exists
 * @param namespaceId the {@link Id.Namespace} to check for existence
 * @return true, if the specifed namespace exists, false otherwise
 */
boolean hasNamespace(Id.Namespace namespaceId);"
7018,"private final void lazyStart(Scheduler scheduler) throws SchedulerException {
  if (scheduler instanceof TimeScheduler) {
    try {
      timeScheduler.lazyStart();
    }
 catch (    Throwable t) {
      Throwables.propagateIfInstanceOf(t,SchedulerException.class);
      throw new SchedulerException(t);
    }
  }
 else   if (scheduler instanceof StreamSizeScheduler) {
    try {
      streamSizeScheduler.lazyStart();
    }
 catch (    Throwable t) {
      Throwables.propagateIfInstanceOf(t,SchedulerException.class);
      throw new SchedulerException(t);
    }
  }
}","private void lazyStart(Scheduler scheduler) throws SchedulerException {
  if (scheduler instanceof TimeScheduler) {
    try {
      timeScheduler.lazyStart();
    }
 catch (    Throwable t) {
      Throwables.propagateIfInstanceOf(t,SchedulerException.class);
      throw new SchedulerException(t);
    }
  }
 else   if (scheduler instanceof StreamSizeScheduler) {
    try {
      streamSizeScheduler.lazyStart();
    }
 catch (    Throwable t) {
      Throwables.propagateIfInstanceOf(t,SchedulerException.class);
      throw new SchedulerException(t);
    }
  }
}"
7019,"private String getV3RoutingService(String[] uriParts,AllowedMethod requestMethod){
  if ((uriParts.length >= 2) && uriParts[1].equals(""String_Node_Str"")) {
    return null;
  }
 else   if ((uriParts.length >= 9) && ""String_Node_Str"".equals(uriParts[5]) && ""String_Node_Str"".equals(uriParts[7])) {
    return String.format(""String_Node_Str"",uriParts[2],uriParts[4],uriParts[6]);
  }
 else   if (matches(uriParts,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if ((uriParts.length >= 4) && uriParts[1].equals(""String_Node_Str"") && uriParts[3].equals(""String_Node_Str"")) {
    if (uriParts.length == 4) {
      return Constants.Service.APP_FABRIC_HTTP;
    }
 else     if (uriParts.length == 5) {
      return Constants.Service.STREAMS;
    }
 else     if ((uriParts.length == 6) && uriParts[5].equals(""String_Node_Str"") && requestMethod.equals(AllowedMethod.GET)) {
      return Constants.Service.APP_FABRIC_HTTP;
    }
 else {
      return Constants.Service.STREAMS;
    }
  }
 else   if ((uriParts.length >= 8 && uriParts[7].equals(""String_Node_Str"")) || (uriParts.length >= 10 && uriParts[9].equals(""String_Node_Str"")) || (uriParts.length >= 6 && uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 2 && uriParts[1].equals(""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 5 && uriParts[1].equals(""String_Node_Str"") && uriParts[2].equals(""String_Node_Str"") && (uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length >= 6 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if ((uriParts.length == 3) && uriParts[1].equals(""String_Node_Str"") && uriParts[2].equals(""String_Node_Str"")) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length == 7 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str""))) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if (matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if ((uriParts.length >= 4) && uriParts[3].equals(""String_Node_Str"")) {
    return Constants.Service.DATASET_MANAGER;
  }
  return Constants.Service.APP_FABRIC_HTTP;
}","private String getV3RoutingService(String[] uriParts,AllowedMethod requestMethod){
  if ((uriParts.length >= 2) && uriParts[1].equals(""String_Node_Str"")) {
    return null;
  }
 else   if ((uriParts.length >= 9) && ""String_Node_Str"".equals(uriParts[5]) && ""String_Node_Str"".equals(uriParts[7])) {
    return String.format(""String_Node_Str"",uriParts[2],uriParts[4],uriParts[6]);
  }
 else   if (matches(uriParts,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if ((matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) && requestMethod.equals(AllowedMethod.GET)) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if ((uriParts.length >= 4) && uriParts[1].equals(""String_Node_Str"") && uriParts[3].equals(""String_Node_Str"")) {
    if (uriParts.length == 4) {
      return Constants.Service.APP_FABRIC_HTTP;
    }
 else {
      return Constants.Service.STREAMS;
    }
  }
 else   if ((uriParts.length >= 8 && uriParts[7].equals(""String_Node_Str"")) || (uriParts.length >= 10 && uriParts[9].equals(""String_Node_Str"")) || (uriParts.length >= 6 && uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 2 && uriParts[1].equals(""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 5 && uriParts[1].equals(""String_Node_Str"") && uriParts[2].equals(""String_Node_Str"") && (uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length >= 6 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if ((uriParts.length == 3) && uriParts[1].equals(""String_Node_Str"") && uriParts[2].equals(""String_Node_Str"")) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length == 7 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str""))) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if ((uriParts.length >= 4) && uriParts[3].equals(""String_Node_Str"")) {
    return Constants.Service.DATASET_MANAGER;
  }
  return Constants.Service.APP_FABRIC_HTTP;
}"
7020,"@Test public void testStreamPath() throws Exception {
  String streamPath=""String_Node_Str"";
  HttpRequest httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  String result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.APP_FABRIC_HTTP,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.APP_FABRIC_HTTP,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.APP_FABRIC_HTTP,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.APP_FABRIC_HTTP,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.STREAMS,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.STREAMS,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.STREAMS,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.APP_FABRIC_HTTP,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.STREAMS,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.STREAMS,result);
}","@Test public void testStreamPath() throws Exception {
  String streamPath=""String_Node_Str"";
  HttpRequest httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  String result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.APP_FABRIC_HTTP,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.APP_FABRIC_HTTP,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.APP_FABRIC_HTTP,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.APP_FABRIC_HTTP,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.STREAMS,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.STREAMS,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.STREAMS,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.APP_FABRIC_HTTP,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.APP_FABRIC_HTTP,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.STREAMS,result);
  streamPath=""String_Node_Str"";
  httpRequest=new DefaultHttpRequest(VERSION,new HttpMethod(""String_Node_Str""),streamPath);
  result=pathLookup.getRoutingService(FALLBACKSERVICE,streamPath,httpRequest);
  Assert.assertEquals(Constants.Service.STREAMS,result);
}"
7021,"/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService}.
 * @param programType The type of programs the run records nee to validate and update.
 * @param store The data store that manages run records instances for all programs.
 * @param runtimeService The {@link ProgramRuntimeService} instance to check the actual state of the program.
 */
private void validateAndCorrectRunningRunRecords(ProgramType programType,Store store,ProgramRuntimeService runtimeService){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecord> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    @Nullable RunRecord input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.debug(""String_Node_Str"",invalidRunRecords.size());
  }
  for (  RunRecord rr : invalidRunRecords) {
    String runId=rr.getPid();
    RuntimeInfo runtimeInfo=runIdToRuntimeInfo.get(RunIds.fromString(runId));
    store.compareAndSetStatus(runtimeInfo.getProgramId(),runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
  }
}","/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService}.
 * @param programType The type of programs the run records nee to validate and update.
 * @param store The data store that manages run records instances for all programs.
 * @param runtimeService The {@link ProgramRuntimeService} instance to check the actual state of the program.
 */
private void validateAndCorrectRunningRunRecords(ProgramType programType,Store store,ProgramRuntimeService runtimeService){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecord> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    @Nullable RunRecord input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.debug(""String_Node_Str"",invalidRunRecords.size());
  }
  for (  RunRecord rr : invalidRunRecords) {
    String runId=rr.getPid();
    List<NamespaceMeta> namespaceMetas=store.listNamespaces();
    Id.Program targetProgramId=null;
    for (    NamespaceMeta nm : namespaceMetas) {
      Id.Namespace accId=Id.Namespace.from(nm.getName());
      Collection<ApplicationSpecification> appSpecs=store.getAllApplications(accId);
      for (      ApplicationSpecification appSpec : appSpecs) {
switch (programType) {
case FLOW:
          for (          String programName : appSpec.getFlows().keySet()) {
            Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
            if (programId != null) {
              targetProgramId=programId;
              break;
            }
          }
        break;
case MAPREDUCE:
      for (      String programName : appSpec.getMapReduce().keySet()) {
        Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
        if (programId != null) {
          targetProgramId=programId;
          break;
        }
      }
    break;
case SPARK:
  for (  String programName : appSpec.getSpark().keySet()) {
    Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
    if (programId != null) {
      targetProgramId=programId;
      break;
    }
  }
break;
case SERVICE:
for (String programName : appSpec.getServices().keySet()) {
Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
  targetProgramId=programId;
  break;
}
}
break;
case WORKER:
for (String programName : appSpec.getWorkers().keySet()) {
Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case WORKFLOW:
for (String programName : appSpec.getWorkflows().keySet()) {
Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
default :
throw new RuntimeException(""String_Node_Str"" + programType.name());
}
if (targetProgramId != null) {
break;
}
}
if (targetProgramId != null) {
break;
}
}
if (targetProgramId != null) {
store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
}
}
}"
7022,"@Override protected void configure(){
  bind(PipelineFactory.class).to(SynchronousPipelineFactory.class);
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<DeploymentInfo,ApplicationWithPrograms>>(){
  }
,new TypeLiteral<LocalApplicationManager<DeploymentInfo,ApplicationWithPrograms>>(){
  }
).build(new TypeLiteral<ManagerFactory<DeploymentInfo,ApplicationWithPrograms>>(){
  }
));
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<DeploymentInfo,ApplicationWithPrograms>>(){
  }
,LocalApplicationTemplateManager.class).build(Key.get(new TypeLiteral<ManagerFactory<DeploymentInfo,ApplicationWithPrograms>>(){
  }
,Names.named(""String_Node_Str""))));
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<AdapterDeploymentInfo,AdapterDefinition>>(){
  }
,LocalAdapterManager.class).build(Key.get(new TypeLiteral<ManagerFactory<AdapterDeploymentInfo,AdapterDefinition>>(){
  }
,Names.named(""String_Node_Str""))));
  bind(Store.class).to(DefaultStore.class);
  bind(AdapterService.class).in(Scopes.SINGLETON);
  bind(PluginRepository.class).in(Scopes.SINGLETON);
  bind(NamespaceAdmin.class).to(DefaultNamespaceAdmin.class).in(Scopes.SINGLETON);
  Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.AppFabric.HANDLERS_BINDING));
  CommonHandlers.add(handlerBinder);
  handlerBinder.addBinding().to(ConfigHandler.class);
  handlerBinder.addBinding().to(AppFabricDataHttpHandler.class);
  handlerBinder.addBinding().to(VersionHandler.class);
  handlerBinder.addBinding().to(MonitorHandler.class);
  handlerBinder.addBinding().to(UsageHandler.class);
  handlerBinder.addBinding().to(NamespaceHttpHandler.class);
  handlerBinder.addBinding().to(NotificationFeedHttpHandler.class);
  handlerBinder.addBinding().to(AppLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(DashboardHttpHandler.class);
  handlerBinder.addBinding().to(ProgramLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(PreferencesHttpHandler.class);
  handlerBinder.addBinding().to(ConsoleSettingsHttpHandler.class);
  handlerBinder.addBinding().to(TransactionHttpHandler.class);
  handlerBinder.addBinding().to(AdapterHttpHandler.class);
  handlerBinder.addBinding().to(ApplicationTemplateHandler.class);
  handlerBinder.addBinding().to(WorkflowHttpHandler.class);
  for (  Class<? extends HttpHandler> handlerClass : handlerClasses) {
    handlerBinder.addBinding().to(handlerClass);
  }
}","@Override protected void configure(){
  bind(PipelineFactory.class).to(SynchronousPipelineFactory.class);
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<DeploymentInfo,ApplicationWithPrograms>>(){
  }
,new TypeLiteral<LocalApplicationManager<DeploymentInfo,ApplicationWithPrograms>>(){
  }
).build(new TypeLiteral<ManagerFactory<DeploymentInfo,ApplicationWithPrograms>>(){
  }
));
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<DeploymentInfo,ApplicationWithPrograms>>(){
  }
,LocalApplicationTemplateManager.class).build(Key.get(new TypeLiteral<ManagerFactory<DeploymentInfo,ApplicationWithPrograms>>(){
  }
,Names.named(""String_Node_Str""))));
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<AdapterDeploymentInfo,AdapterDefinition>>(){
  }
,LocalAdapterManager.class).build(Key.get(new TypeLiteral<ManagerFactory<AdapterDeploymentInfo,AdapterDefinition>>(){
  }
,Names.named(""String_Node_Str""))));
  bind(Store.class).to(DefaultStore.class);
  bind(AdapterService.class).in(Scopes.SINGLETON);
  bind(ProgramLifecycleService.class).in(Scopes.SINGLETON);
  bind(PluginRepository.class).in(Scopes.SINGLETON);
  bind(NamespaceAdmin.class).to(DefaultNamespaceAdmin.class).in(Scopes.SINGLETON);
  Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.AppFabric.HANDLERS_BINDING));
  CommonHandlers.add(handlerBinder);
  handlerBinder.addBinding().to(ConfigHandler.class);
  handlerBinder.addBinding().to(AppFabricDataHttpHandler.class);
  handlerBinder.addBinding().to(VersionHandler.class);
  handlerBinder.addBinding().to(MonitorHandler.class);
  handlerBinder.addBinding().to(UsageHandler.class);
  handlerBinder.addBinding().to(NamespaceHttpHandler.class);
  handlerBinder.addBinding().to(NotificationFeedHttpHandler.class);
  handlerBinder.addBinding().to(AppLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(DashboardHttpHandler.class);
  handlerBinder.addBinding().to(ProgramLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(PreferencesHttpHandler.class);
  handlerBinder.addBinding().to(ConsoleSettingsHttpHandler.class);
  handlerBinder.addBinding().to(TransactionHttpHandler.class);
  handlerBinder.addBinding().to(AdapterHttpHandler.class);
  handlerBinder.addBinding().to(ApplicationTemplateHandler.class);
  handlerBinder.addBinding().to(WorkflowHttpHandler.class);
  for (  Class<? extends HttpHandler> handlerClass : handlerClasses) {
    handlerBinder.addBinding().to(handlerClass);
  }
}"
7023,"/** 
 * Start the given adapter. Creates a schedule for a workflow adapter and starts the worker for a worker adapter.
 * @param namespace the namespace the adapter is deployed in
 * @param adapterName the name of the adapter
 * @throws NotFoundException if the adapter could not be found
 * @throws InvalidAdapterOperationException if the adapter is already started
 * @throws SchedulerException if there was some error creating the schedule for the adapter
 * @throws IOException if there was some error starting worker adapter
 */
public synchronized void startAdapter(Id.Namespace namespace,String adapterName) throws NotFoundException, InvalidAdapterOperationException, SchedulerException, IOException {
  AdapterStatus adapterStatus=getAdapterStatus(namespace,adapterName);
  if (AdapterStatus.STARTED.equals(adapterStatus)) {
    throw new InvalidAdapterOperationException(""String_Node_Str"");
  }
  AdapterDefinition adapterSpec=getAdapter(namespace,adapterName);
  ProgramType programType=adapterSpec.getProgram().getType();
  if (programType == ProgramType.WORKFLOW) {
    startWorkflowAdapter(namespace,adapterSpec);
  }
 else   if (programType == ProgramType.WORKER) {
    startWorkerAdapter(namespace,adapterSpec);
  }
 else {
    LOG.warn(""String_Node_Str"",programType);
    throw new InvalidAdapterOperationException(""String_Node_Str"" + programType);
  }
  setAdapterStatus(namespace,adapterName,AdapterStatus.STARTED);
}","/** 
 * Start the given adapter. Creates a schedule for a workflow adapter and starts the worker for a worker adapter.
 * @param namespace the namespace the adapter is deployed in
 * @param adapterName the name of the adapter
 * @throws NotFoundException if the adapter could not be found
 * @throws InvalidAdapterOperationException if the adapter is already started
 * @throws SchedulerException if there was some error creating the schedule for the adapter
 * @throws IOException if there was some error starting worker adapter
 */
public synchronized void startAdapter(Id.Namespace namespace,String adapterName) throws NotFoundException, InvalidAdapterOperationException, SchedulerException, IOException {
  AdapterStatus adapterStatus=getAdapterStatus(namespace,adapterName);
  AdapterDefinition adapterSpec=getAdapter(namespace,adapterName);
  ProgramType programType=adapterSpec.getProgram().getType();
  if (AdapterStatus.STARTED.equals(adapterStatus)) {
    Id.Program program=getProgramId(namespace,adapterName);
    ProgramRuntimeService.RuntimeInfo runtimeInfo=lifecycleService.findRuntimeInfo(program,programType);
    if (runtimeInfo != null) {
      throw new InvalidAdapterOperationException(""String_Node_Str"");
    }
  }
  if (programType == ProgramType.WORKFLOW) {
    startWorkflowAdapter(namespace,adapterSpec);
  }
 else   if (programType == ProgramType.WORKER) {
    startWorkerAdapter(namespace,adapterSpec);
  }
 else {
    LOG.warn(""String_Node_Str"",programType);
    throw new InvalidAdapterOperationException(""String_Node_Str"" + programType);
  }
  setAdapterStatus(namespace,adapterName,AdapterStatus.STARTED);
}"
7024,"/** 
 * Configures the AppFabricService pre-start.
 */
@Override protected void startUp() throws Exception {
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(Constants.SYSTEM_NAMESPACE,Constants.Logging.COMPONENT_NAME,Constants.Service.APP_FABRIC_HTTP));
  File tmpDir=new File(configuration.get(Constants.CFG_LOCAL_DATA_DIR),configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  FileUtils.deleteDirectory(tmpDir);
  notificationService.start();
  schedulerService.start();
  applicationLifecycleService.start();
  adapterService.start();
  programRuntimeService.start();
  streamCoordinatorClient.start();
  ImmutableList.Builder<HandlerHook> builder=ImmutableList.builder();
  for (  String hook : handlerHookNames) {
    builder.add(new MetricsReporterHook(metricsCollectionService,hook));
  }
  httpService=new CommonNettyHttpServiceBuilder(configuration).setHost(hostname.getCanonicalHostName()).setHandlerHooks(builder.build()).addHttpHandlers(handlers).setConnectionBacklog(configuration.getInt(Constants.AppFabric.BACKLOG_CONNECTIONS,Constants.AppFabric.DEFAULT_BACKLOG)).setExecThreadPoolSize(configuration.getInt(Constants.AppFabric.EXEC_THREADS,Constants.AppFabric.DEFAULT_EXEC_THREADS)).setBossThreadPoolSize(configuration.getInt(Constants.AppFabric.BOSS_THREADS,Constants.AppFabric.DEFAULT_BOSS_THREADS)).setWorkerThreadPoolSize(configuration.getInt(Constants.AppFabric.WORKER_THREADS,Constants.AppFabric.DEFAULT_WORKER_THREADS)).build();
  httpService.addListener(new ServiceListenerAdapter(){
    private List<Cancellable> cancellables=Lists.newArrayList();
    @Override public void running(){
      final InetSocketAddress socketAddress=httpService.getBindAddress();
      LOG.info(""String_Node_Str"",socketAddress);
      for (      final String serviceName : servicesNames) {
        cancellables.add(discoveryService.register(ResolvingDiscoverable.of(new Discoverable(){
          @Override public String getName(){
            return serviceName;
          }
          @Override public InetSocketAddress getSocketAddress(){
            return socketAddress;
          }
        }
)));
      }
    }
    @Override public void terminated(    State from){
      LOG.info(""String_Node_Str"");
      for (      Cancellable cancellable : cancellables) {
        if (cancellable != null) {
          cancellable.cancel();
        }
      }
    }
    @Override public void failed(    State from,    Throwable failure){
      LOG.info(""String_Node_Str"",failure);
      for (      Cancellable cancellable : cancellables) {
        if (cancellable != null) {
          cancellable.cancel();
        }
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  httpService.startAndWait();
  Thread defaultNamespaceEnsurer=new Thread(new DefaultNamespaceEnsurer(namespaceAdmin,1,TimeUnit.SECONDS));
  defaultNamespaceEnsurer.start();
}","/** 
 * Configures the AppFabricService pre-start.
 */
@Override protected void startUp() throws Exception {
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(Constants.SYSTEM_NAMESPACE,Constants.Logging.COMPONENT_NAME,Constants.Service.APP_FABRIC_HTTP));
  File tmpDir=new File(configuration.get(Constants.CFG_LOCAL_DATA_DIR),configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  FileUtils.deleteDirectory(tmpDir);
  notificationService.start();
  schedulerService.start();
  applicationLifecycleService.start();
  adapterService.start();
  programRuntimeService.start();
  streamCoordinatorClient.start();
  programLifecycleService.start();
  ImmutableList.Builder<HandlerHook> builder=ImmutableList.builder();
  for (  String hook : handlerHookNames) {
    builder.add(new MetricsReporterHook(metricsCollectionService,hook));
  }
  httpService=new CommonNettyHttpServiceBuilder(configuration).setHost(hostname.getCanonicalHostName()).setHandlerHooks(builder.build()).addHttpHandlers(handlers).setConnectionBacklog(configuration.getInt(Constants.AppFabric.BACKLOG_CONNECTIONS,Constants.AppFabric.DEFAULT_BACKLOG)).setExecThreadPoolSize(configuration.getInt(Constants.AppFabric.EXEC_THREADS,Constants.AppFabric.DEFAULT_EXEC_THREADS)).setBossThreadPoolSize(configuration.getInt(Constants.AppFabric.BOSS_THREADS,Constants.AppFabric.DEFAULT_BOSS_THREADS)).setWorkerThreadPoolSize(configuration.getInt(Constants.AppFabric.WORKER_THREADS,Constants.AppFabric.DEFAULT_WORKER_THREADS)).build();
  httpService.addListener(new ServiceListenerAdapter(){
    private List<Cancellable> cancellables=Lists.newArrayList();
    @Override public void running(){
      final InetSocketAddress socketAddress=httpService.getBindAddress();
      LOG.info(""String_Node_Str"",socketAddress);
      for (      final String serviceName : servicesNames) {
        cancellables.add(discoveryService.register(ResolvingDiscoverable.of(new Discoverable(){
          @Override public String getName(){
            return serviceName;
          }
          @Override public InetSocketAddress getSocketAddress(){
            return socketAddress;
          }
        }
)));
      }
    }
    @Override public void terminated(    State from){
      LOG.info(""String_Node_Str"");
      for (      Cancellable cancellable : cancellables) {
        if (cancellable != null) {
          cancellable.cancel();
        }
      }
    }
    @Override public void failed(    State from,    Throwable failure){
      LOG.info(""String_Node_Str"",failure);
      for (      Cancellable cancellable : cancellables) {
        if (cancellable != null) {
          cancellable.cancel();
        }
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  httpService.startAndWait();
  Thread defaultNamespaceEnsurer=new Thread(new DefaultNamespaceEnsurer(namespaceAdmin,1,TimeUnit.SECONDS));
  defaultNamespaceEnsurer.start();
}"
7025,"/** 
 * Construct the AppFabricServer with service factory and configuration coming from guice injection.
 */
@Inject public AppFabricServer(CConfiguration configuration,DiscoveryService discoveryService,SchedulerService schedulerService,NotificationService notificationService,@Named(Constants.AppFabric.SERVER_ADDRESS) InetAddress hostname,@Named(Constants.AppFabric.HANDLERS_BINDING) Set<HttpHandler> handlers,@Nullable MetricsCollectionService metricsCollectionService,ProgramRuntimeService programRuntimeService,AdapterService adapterService,ApplicationLifecycleService applicationLifecycleService,StreamCoordinatorClient streamCoordinatorClient,@Named(""String_Node_Str"") Set<String> servicesNames,@Named(""String_Node_Str"") Set<String> handlerHookNames,NamespaceAdmin namespaceAdmin){
  this.hostname=hostname;
  this.discoveryService=discoveryService;
  this.schedulerService=schedulerService;
  this.handlers=handlers;
  this.configuration=configuration;
  this.metricsCollectionService=metricsCollectionService;
  this.programRuntimeService=programRuntimeService;
  this.adapterService=adapterService;
  this.notificationService=notificationService;
  this.servicesNames=servicesNames;
  this.handlerHookNames=handlerHookNames;
  this.namespaceAdmin=namespaceAdmin;
  this.applicationLifecycleService=applicationLifecycleService;
  this.streamCoordinatorClient=streamCoordinatorClient;
}","/** 
 * Construct the AppFabricServer with service factory and configuration coming from guice injection.
 */
@Inject public AppFabricServer(CConfiguration configuration,DiscoveryService discoveryService,SchedulerService schedulerService,NotificationService notificationService,@Named(Constants.AppFabric.SERVER_ADDRESS) InetAddress hostname,@Named(Constants.AppFabric.HANDLERS_BINDING) Set<HttpHandler> handlers,@Nullable MetricsCollectionService metricsCollectionService,ProgramRuntimeService programRuntimeService,AdapterService adapterService,ApplicationLifecycleService applicationLifecycleService,ProgramLifecycleService programLifecycleService,StreamCoordinatorClient streamCoordinatorClient,@Named(""String_Node_Str"") Set<String> servicesNames,@Named(""String_Node_Str"") Set<String> handlerHookNames,NamespaceAdmin namespaceAdmin){
  this.hostname=hostname;
  this.discoveryService=discoveryService;
  this.schedulerService=schedulerService;
  this.handlers=handlers;
  this.configuration=configuration;
  this.metricsCollectionService=metricsCollectionService;
  this.programRuntimeService=programRuntimeService;
  this.adapterService=adapterService;
  this.notificationService=notificationService;
  this.servicesNames=servicesNames;
  this.handlerHookNames=handlerHookNames;
  this.namespaceAdmin=namespaceAdmin;
  this.applicationLifecycleService=applicationLifecycleService;
  this.streamCoordinatorClient=streamCoordinatorClient;
  this.programLifecycleService=programLifecycleService;
}"
7026,"@Override protected void shutDown() throws Exception {
  httpService.stopAndWait();
  programRuntimeService.stopAndWait();
  schedulerService.stopAndWait();
  applicationLifecycleService.stopAndWait();
  adapterService.stopAndWait();
  notificationService.stopAndWait();
}","@Override protected void shutDown() throws Exception {
  httpService.stopAndWait();
  programRuntimeService.stopAndWait();
  schedulerService.stopAndWait();
  applicationLifecycleService.stopAndWait();
  adapterService.stopAndWait();
  notificationService.stopAndWait();
  programLifecycleService.stopAndWait();
}"
7027,"@Override protected void shutDown() throws Exception {
  LOG.info(""String_Node_Str"");
}","@Override protected void shutDown() throws Exception {
  LOG.info(""String_Node_Str"");
  scheduledExecutorService.shutdown();
  try {
    if (!scheduledExecutorService.awaitTermination(5,TimeUnit.SECONDS)) {
      scheduledExecutorService.shutdownNow();
    }
  }
 catch (  InterruptedException ie) {
    Thread.currentThread().interrupt();
  }
}"
7028,"@Override protected void startUp() throws Exception {
  LOG.info(""String_Node_Str"");
}","@Override protected void startUp() throws Exception {
  LOG.info(""String_Node_Str"");
  scheduledExecutorService.scheduleWithFixedDelay(new RunRecordsCorrectorRunnable(this,store,runtimeService),2L,600L,TimeUnit.SECONDS);
}"
7029,"@Inject public ProgramLifecycleService(Store store,ProgramRuntimeService runtimeService){
  this.store=store;
  this.runtimeService=runtimeService;
}","@Inject public ProgramLifecycleService(Store store,ProgramRuntimeService runtimeService){
  this.store=store;
  this.runtimeService=runtimeService;
  this.scheduledExecutorService=Executors.newScheduledThreadPool(1);
}"
7030,"/** 
 * Construct the Standalone AppFabricServer with service factory and configuration coming from guice injection.
 */
@Inject public StandaloneAppFabricServer(CConfiguration configuration,DiscoveryService discoveryService,SchedulerService schedulerService,NotificationService notificationService,@Named(Constants.AppFabric.SERVER_ADDRESS) InetAddress hostname,@Named(Constants.AppFabric.HANDLERS_BINDING) Set<HttpHandler> handlers,@Nullable MetricsCollectionService metricsCollectionService,ProgramRuntimeService programRuntimeService,AdapterService adapterService,ApplicationLifecycleService applicationLifecycleService,StreamCoordinatorClient streamCoordinatorClient,@Named(""String_Node_Str"") Set<String> servicesNames,@Named(""String_Node_Str"") Set<String> handlerHookNames,NamespaceAdmin namespaceAdmin,MetricStore metricStore){
  super(configuration,discoveryService,schedulerService,notificationService,hostname,handlers,metricsCollectionService,programRuntimeService,adapterService,applicationLifecycleService,streamCoordinatorClient,servicesNames,handlerHookNames,namespaceAdmin);
  this.metricStore=metricStore;
}","/** 
 * Construct the Standalone AppFabricServer with service factory and configuration coming from guice injection.
 */
@Inject public StandaloneAppFabricServer(CConfiguration configuration,DiscoveryService discoveryService,SchedulerService schedulerService,NotificationService notificationService,@Named(Constants.AppFabric.SERVER_ADDRESS) InetAddress hostname,@Named(Constants.AppFabric.HANDLERS_BINDING) Set<HttpHandler> handlers,@Nullable MetricsCollectionService metricsCollectionService,ProgramRuntimeService programRuntimeService,AdapterService adapterService,ApplicationLifecycleService applicationLifecycleService,ProgramLifecycleService programLifecycleService,StreamCoordinatorClient streamCoordinatorClient,@Named(""String_Node_Str"") Set<String> servicesNames,@Named(""String_Node_Str"") Set<String> handlerHookNames,NamespaceAdmin namespaceAdmin,MetricStore metricStore){
  super(configuration,discoveryService,schedulerService,notificationService,hostname,handlers,metricsCollectionService,programRuntimeService,adapterService,applicationLifecycleService,programLifecycleService,streamCoordinatorClient,servicesNames,handlerHookNames,namespaceAdmin);
  this.metricStore=metricStore;
}"
7031,"WorkflowDriver(Program program,ProgramOptions options,InetAddress hostname,WorkflowSpecification workflowSpec,ProgramRunnerFactory programRunnerFactory){
  this.program=program;
  this.hostname=hostname;
  this.runtimeArgs=createRuntimeArgs(options.getUserArguments());
  this.workflowSpec=workflowSpec;
  Arguments arguments=options.getArguments();
  this.logicalStartTime=arguments.hasOption(ProgramOptionConstants.LOGICAL_START_TIME) ? Long.parseLong(arguments.getOption(ProgramOptionConstants.LOGICAL_START_TIME)) : System.currentTimeMillis();
  this.workflowProgramRunnerFactory=new ProgramWorkflowRunnerFactory(workflowSpec,programRunnerFactory,program,options);
  lock=new ReentrantLock();
  condition=lock.newCondition();
  loggingContext=new WorkflowLoggingContext(program.getNamespaceId(),program.getApplicationId(),program.getName(),options.getArguments().getOption(ProgramOptionConstants.RUN_ID));
}","WorkflowDriver(Program program,ProgramOptions options,InetAddress hostname,WorkflowSpecification workflowSpec,ProgramRunnerFactory programRunnerFactory){
  this.program=program;
  this.hostname=hostname;
  this.runtimeArgs=createRuntimeArgs(options.getUserArguments());
  this.workflowSpec=workflowSpec;
  Arguments arguments=options.getArguments();
  this.logicalStartTime=arguments.hasOption(ProgramOptionConstants.LOGICAL_START_TIME) ? Long.parseLong(arguments.getOption(ProgramOptionConstants.LOGICAL_START_TIME)) : System.currentTimeMillis();
  this.workflowProgramRunnerFactory=new ProgramWorkflowRunnerFactory(workflowSpec,programRunnerFactory,program,options);
  this.lock=new ReentrantLock();
  this.condition=lock.newCondition();
  String adapterSpec=arguments.getOption(ProgramOptionConstants.ADAPTER_SPEC);
  String adapterName=null;
  if (adapterSpec != null) {
    adapterName=GSON.fromJson(adapterSpec,AdapterDefinition.class).getName();
  }
  this.loggingContext=new WorkflowLoggingContext(program.getNamespaceId(),program.getApplicationId(),program.getName(),arguments.getOption(ProgramOptionConstants.RUN_ID),adapterName);
}"
7032,"private void testPrev(String appId,String entityType,String entityId,String namespace,@Nullable String adapteId) throws Exception {
  String prevUrl=String.format(""String_Node_Str"",appId,entityType,entityId,getToOffset(25));
  prevUrl=getUrlWithAdapterId(prevUrl,adapteId,""String_Node_Str"");
  HttpResponse response=doGet(getVersionedAPIPath(prevUrl,namespace));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  String out=EntityUtils.toString(response.getEntity());
  List<LogLine> logLines=GSON.fromJson(out,LIST_LOGLINE_TYPE);
  Assert.assertEquals(10,logLines.size());
  int expected=15;
  for (  LogLine logLine : logLines) {
    Assert.assertEquals(expected,logLine.getOffset().getKafkaOffset());
    Assert.assertEquals(expected,logLine.getOffset().getTime());
    String expectedStr=entityId + ""String_Node_Str"" + expected+ ""String_Node_Str"";
    String log=logLine.getLog();
    Assert.assertEquals(expectedStr,log.substring(log.length() - expectedStr.length()));
    expected++;
  }
}","private void testPrev(String appId,String entityType,String entityId,String namespace,@Nullable String adapterId) throws Exception {
  String prevUrl=String.format(""String_Node_Str"",appId,entityType,entityId,getToOffset(25));
  prevUrl=getUrlWithAdapterId(prevUrl,adapterId,""String_Node_Str"");
  HttpResponse response=doGet(getVersionedAPIPath(prevUrl,namespace));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  String out=EntityUtils.toString(response.getEntity());
  List<LogLine> logLines=GSON.fromJson(out,LIST_LOGLINE_TYPE);
  Assert.assertEquals(10,logLines.size());
  int expected=15;
  for (  LogLine logLine : logLines) {
    Assert.assertEquals(expected,logLine.getOffset().getKafkaOffset());
    Assert.assertEquals(expected,logLine.getOffset().getTime());
    String expectedStr=entityId + ""String_Node_Str"" + expected+ ""String_Node_Str"";
    String log=logLine.getLog();
    Assert.assertEquals(expectedStr,log.substring(log.length() - expectedStr.length()));
    expected++;
  }
}"
7033,"@Test public void testMapReduceLogs() throws Exception {
  testLogs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
  testLogsFilter(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
  try {
    testLogsFilter(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
    Assert.fail();
  }
 catch (  AssertionError e) {
  }
  testLogsRunId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
}","@Test public void testMapReduceLogs() throws Exception {
  testLogs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
  testLogsFilter(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
  testLogsRunId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
  try {
    testLogsFilter(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,null);
  }
 catch (  AssertionError e) {
    return;
  }
  Assert.fail();
}"
7034,"@Test public void testAdapterLogsPrev() throws Exception {
  testPrev(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  testPrevNoMax(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  try {
    testPrevNoMax(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,""String_Node_Str"");
    Assert.fail();
  }
 catch (  AssertionError e) {
  }
  testPrevFilter(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  testPrevNoFrom(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
}","@Test public void testAdapterLogsPrev() throws Exception {
  testPrev(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  testPrevNoMax(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  testPrevFilter(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  testPrevNoFrom(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  try {
    testPrevNoMax(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,""String_Node_Str"");
  }
 catch (  AssertionError e) {
    return;
  }
  Assert.fail();
}"
7035,"@Test public void testAdapterLogsNext() throws Exception {
  testNext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  try {
    testNext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,Constants.DEFAULT_NAMESPACE,""String_Node_Str"");
    Assert.fail();
  }
 catch (  AssertionError e) {
  }
  testNextNoMax(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  testNextFilter(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  testNextNoFrom(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
}","@Test public void testAdapterLogsNext() throws Exception {
  testNext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  testNextNoMax(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  testNextFilter(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  testNextNoFrom(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  try {
    testNext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,Constants.DEFAULT_NAMESPACE,""String_Node_Str"");
  }
 catch (  AssertionError e) {
    return;
  }
  Assert.fail();
}"
7036,"@Test public void testFlowLogs() throws Exception {
  testLogs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,null);
  try {
    testLogs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
    Assert.fail();
  }
 catch (  AssertionError e) {
  }
  testLogsFilter(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,null);
  testLogsRunId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,null);
}","@Test public void testFlowLogs() throws Exception {
  testLogs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,null);
  testLogsFilter(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,null);
  testLogsRunId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,null);
  try {
    testLogs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
  }
 catch (  AssertionError e) {
    return;
  }
  Assert.fail();
}"
7037,"@Test public void testMapReducePrev() throws Exception {
  testPrev(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
  testPrevNoMax(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
  try {
    testPrevNoMax(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,null);
    Assert.fail();
  }
 catch (  AssertionError e) {
  }
  testPrevFilter(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
  testPrevNoFrom(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
  testPrevRunId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
}","@Test public void testMapReducePrev() throws Exception {
  testPrev(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
  testPrevNoMax(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
  testPrevFilter(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
  testPrevNoFrom(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
  testPrevRunId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,null);
  try {
    testPrevNoMax(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,null);
  }
 catch (  AssertionError e) {
    return;
  }
  Assert.fail();
}"
7038,"@Test public void testAdapterLogs() throws Exception {
  testLogs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  testLogs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,null);
  try {
    testLogs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,""String_Node_Str"");
    Assert.fail();
  }
 catch (  AssertionError e) {
  }
  testLogsFilter(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
}","@Test public void testAdapterLogs() throws Exception {
  testLogs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  testLogs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,null);
  testLogsFilter(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  testLogsRunId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MockLogReader.TEST_NAMESPACE,""String_Node_Str"");
  try {
    testLogs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Constants.DEFAULT_NAMESPACE,""String_Node_Str"");
  }
 catch (  AssertionError e) {
    return;
  }
  Assert.fail();
}"
7039,"public static LoggingContext getLoggingContext(String namespaceId,String applicationId,String entityId,ProgramType programType,@Nullable String runId,@Nullable String adapterName){
switch (programType) {
case FLOW:
    return new FlowletLoggingContext(namespaceId,applicationId,entityId,""String_Node_Str"",runId,null);
case WORKFLOW:
  return new WorkflowLoggingContext(namespaceId,applicationId,entityId,runId);
case MAPREDUCE:
return new MapReduceLoggingContext(namespaceId,applicationId,entityId,runId,adapterName);
case SPARK:
return new SparkLoggingContext(namespaceId,applicationId,entityId,runId);
case SERVICE:
return new UserServiceLoggingContext(namespaceId,applicationId,entityId,""String_Node_Str"",runId,null);
case WORKER:
return new WorkerLoggingContext(namespaceId,applicationId,entityId,runId,null,adapterName);
default :
throw new IllegalArgumentException(String.format(""String_Node_Str"",programType));
}
}","public static LoggingContext getLoggingContext(String namespaceId,String applicationId,String entityId,ProgramType programType,@Nullable String runId,@Nullable String adapterName){
switch (programType) {
case FLOW:
    return new FlowletLoggingContext(namespaceId,applicationId,entityId,""String_Node_Str"",runId,null);
case WORKFLOW:
  return new WorkflowLoggingContext(namespaceId,applicationId,entityId,runId,adapterName);
case MAPREDUCE:
return new MapReduceLoggingContext(namespaceId,applicationId,entityId,runId,adapterName);
case SPARK:
return new SparkLoggingContext(namespaceId,applicationId,entityId,runId);
case SERVICE:
return new UserServiceLoggingContext(namespaceId,applicationId,entityId,""String_Node_Str"",runId,null);
case WORKER:
return new WorkerLoggingContext(namespaceId,applicationId,entityId,runId,null,adapterName);
default :
throw new IllegalArgumentException(String.format(""String_Node_Str"",programType));
}
}"
7040,"/** 
 * Constructs ApplicationLoggingContext.
 * @param namespaceId   namespace id
 * @param applicationId application id
 * @param workflowId    workflow id
 * @param runId         run id of the application
 */
public WorkflowLoggingContext(String namespaceId,String applicationId,String workflowId,String runId){
  super(namespaceId,applicationId,runId);
  setSystemTag(TAG_WORKFLOW_ID,workflowId);
}","/** 
 * Constructs ApplicationLoggingContext.
 * @param namespaceId   namespace id
 * @param applicationId application id
 * @param workflowId    workflow id
 * @param runId         run id of the application
 */
public WorkflowLoggingContext(String namespaceId,String applicationId,String workflowId,String runId,@Nullable String adapterId){
  super(namespaceId,applicationId,runId);
  setSystemTag(TAG_WORKFLOW_ID,workflowId);
  if (adapterId != null) {
    setAdapterId(adapterId);
  }
}"
7041,"private String getV3RoutingService(String[] uriParts,AllowedMethod requestMethod){
  if ((uriParts.length >= 2) && uriParts[1].equals(""String_Node_Str"")) {
    return null;
  }
 else   if ((uriParts.length >= 9) && ""String_Node_Str"".equals(uriParts[5]) && ""String_Node_Str"".equals(uriParts[7])) {
    return String.format(""String_Node_Str"",uriParts[2],uriParts[4],uriParts[6]);
  }
 else   if (matches(uriParts,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if ((uriParts.length >= 4) && uriParts[1].equals(""String_Node_Str"") && uriParts[3].equals(""String_Node_Str"")) {
    if (uriParts.length == 4) {
      return Constants.Service.APP_FABRIC_HTTP;
    }
 else     if (uriParts.length == 5) {
      return Constants.Service.STREAMS;
    }
 else     if ((uriParts.length == 6) && uriParts[5].equals(""String_Node_Str"") && requestMethod.equals(AllowedMethod.GET)) {
      return Constants.Service.APP_FABRIC_HTTP;
    }
 else {
      return Constants.Service.STREAMS;
    }
  }
 else   if ((uriParts.length >= 8 && uriParts[7].equals(""String_Node_Str"")) || (uriParts.length >= 10 && uriParts[9].equals(""String_Node_Str"")) || (uriParts.length >= 6 && uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 2 && uriParts[1].equals(""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 5 && uriParts[1].equals(""String_Node_Str"") && uriParts[2].equals(""String_Node_Str"") && (uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length >= 6 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length == 7 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str""))) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if (matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if ((uriParts.length >= 4) && uriParts[3].equals(""String_Node_Str"")) {
    return Constants.Service.DATASET_MANAGER;
  }
  return Constants.Service.APP_FABRIC_HTTP;
}","private String getV3RoutingService(String[] uriParts,AllowedMethod requestMethod){
  if ((uriParts.length >= 2) && uriParts[1].equals(""String_Node_Str"")) {
    return null;
  }
 else   if ((uriParts.length >= 9) && ""String_Node_Str"".equals(uriParts[5]) && ""String_Node_Str"".equals(uriParts[7])) {
    return String.format(""String_Node_Str"",uriParts[2],uriParts[4],uriParts[6]);
  }
 else   if (matches(uriParts,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if ((uriParts.length >= 4) && uriParts[1].equals(""String_Node_Str"") && uriParts[3].equals(""String_Node_Str"")) {
    if (uriParts.length == 4) {
      return Constants.Service.APP_FABRIC_HTTP;
    }
 else     if (uriParts.length == 5) {
      return Constants.Service.STREAMS;
    }
 else     if ((uriParts.length == 6) && uriParts[5].equals(""String_Node_Str"") && requestMethod.equals(AllowedMethod.GET)) {
      return Constants.Service.APP_FABRIC_HTTP;
    }
 else {
      return Constants.Service.STREAMS;
    }
  }
 else   if ((uriParts.length >= 8 && uriParts[7].equals(""String_Node_Str"")) || (uriParts.length >= 10 && uriParts[9].equals(""String_Node_Str"")) || (uriParts.length >= 6 && uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 2 && uriParts[1].equals(""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 5 && uriParts[1].equals(""String_Node_Str"") && uriParts[2].equals(""String_Node_Str"") && (uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length >= 6 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if ((uriParts.length == 3) && uriParts[1].equals(""String_Node_Str"") && uriParts[2].equals(""String_Node_Str"")) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length == 7 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str""))) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if (matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if ((uriParts.length >= 4) && uriParts[3].equals(""String_Node_Str"")) {
    return Constants.Service.DATASET_MANAGER;
  }
  return Constants.Service.APP_FABRIC_HTTP;
}"
7042,"/** 
 * Creates an adapter.
 * @param adapterName name of the adapter to create
 * @param adapterSpec properties of the adapter to create
 * @throws ApplicationTemplateNotFoundException if the desired adapter type was not found
 * @throws BadRequestException if the provided {@link AdapterConfig} was bad
 * @throws IOException if a network error occurred
 * @throws UnauthorizedException if the request is not authorized successfully in the gateway server
 */
public void create(String adapterName,AdapterConfig adapterSpec) throws ApplicationTemplateNotFoundException, BadRequestException, IOException, UnauthorizedException {
  URL url=config.resolveNamespacedURLV3(String.format(""String_Node_Str"",adapterName));
  HttpRequest request=HttpRequest.put(url).withBody(GSON.toJson(adapterSpec)).build();
  HttpResponse response=restClient.execute(request,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND,HttpURLConnection.HTTP_BAD_REQUEST);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new ApplicationTemplateNotFoundException(Id.ApplicationTemplate.from(adapterSpec.getTemplate()));
  }
 else   if (response.getResponseCode() == HttpURLConnection.HTTP_BAD_REQUEST) {
    throw new BadRequestException(response.getResponseMessage());
  }
}","/** 
 * Creates an adapter.
 * @param adapterName name of the adapter to create
 * @param adapterSpec properties of the adapter to create
 * @throws ApplicationTemplateNotFoundException if the desired adapter type was not found
 * @throws BadRequestException if the provided {@link AdapterConfig} was bad
 * @throws IOException if a network error occurred
 * @throws UnauthorizedException if the request is not authorized successfully in the gateway server
 */
public void create(String adapterName,AdapterConfig adapterSpec) throws ApplicationTemplateNotFoundException, BadRequestException, IOException, UnauthorizedException {
  URL url=config.resolveNamespacedURLV3(String.format(""String_Node_Str"",adapterName));
  HttpRequest request=HttpRequest.put(url).withBody(GSON.toJson(adapterSpec)).build();
  HttpResponse response=restClient.execute(request,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND,HttpURLConnection.HTTP_BAD_REQUEST);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new ApplicationTemplateNotFoundException(Id.ApplicationTemplate.from(adapterSpec.getTemplate()));
  }
 else   if (response.getResponseCode() == HttpURLConnection.HTTP_BAD_REQUEST) {
    throw new BadRequestException(response.getResponseBodyAsString());
  }
}"
7043,"@Override public void exceptionCaught(ChannelHandlerContext ctx,ExceptionEvent e){
  LOG.error(""String_Node_Str"",e);
  ChannelFuture future=Channels.future(ctx.getChannel());
  future.addListener(ChannelFutureListener.CLOSE);
  HttpResponse response=new DefaultHttpResponse(HttpVersion.HTTP_1_1,HttpResponseStatus.UNAUTHORIZED);
  Channels.write(ctx,future,response);
}","@Override public void exceptionCaught(ChannelHandlerContext ctx,ExceptionEvent e){
  LOG.error(""String_Node_Str"",e.getCause());
  ChannelFuture future=Channels.future(ctx.getChannel());
  future.addListener(ChannelFutureListener.CLOSE);
  HttpResponse response=new DefaultHttpResponse(HttpVersion.HTTP_1_1,HttpResponseStatus.UNAUTHORIZED);
  Channels.write(ctx,future,response);
}"
7044,"/** 
 * Initialize the Source.
 * @param context {@link RealtimeContext}
 */
public void initialize(RealtimeContext context) throws Exception {
  super.initialize(context);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  if (config.getProperties() != null) {
    runtimeArguments.putAll(config.getProperties().getProperties());
  }
  Integer configMessagesToReceive=config.messagesToReceive;
  messagesToReceive=configMessagesToReceive.intValue();
  runtimeArguments.put(Context.INITIAL_CONTEXT_FACTORY,config.initialContextFactory);
  runtimeArguments.put(Context.PROVIDER_URL,config.providerUrl);
  final Hashtable<String,String> envVars=new Hashtable<String,String>();
  Maps.filterEntries(runtimeArguments,new Predicate<Map.Entry<String,String>>(){
    @Override public boolean apply(    @Nullable Map.Entry<String,String> input){
      if (input.getKey() != null && input.getKey().startsWith(JAVA_NAMING_PREFIX)) {
        envVars.put(input.getKey(),input.getValue());
        return true;
      }
      return false;
    }
  }
);
  initializeJMSConnection(envVars,config.destinationName,config.connectionFactoryName);
}","/** 
 * Initialize the Source.
 * @param context {@link RealtimeContext}
 */
public void initialize(RealtimeContext context) throws Exception {
  super.initialize(context);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  if (config.getProperties() != null) {
    runtimeArguments.putAll(config.getProperties().getProperties());
  }
  Integer configMessagesToReceive=config.messagesToReceive;
  messagesToReceive=configMessagesToReceive.intValue();
  final Hashtable<String,String> envVars=new Hashtable<String,String>();
  for (  Map.Entry<String,String> entry : runtimeArguments.entrySet()) {
    if (entry.getKey() != null && entry.getKey().startsWith(JAVA_NAMING_PREFIX)) {
      envVars.put(entry.getKey(),entry.getValue());
    }
  }
  envVars.put(Context.INITIAL_CONTEXT_FACTORY,config.initialContextFactory);
  envVars.put(Context.PROVIDER_URL,config.providerUrl);
  initializeJMSConnection(envVars,config.destinationName,config.connectionFactoryName);
}"
7045,"@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  pipelineConfigurer.usePluginClass(""String_Node_Str"",Context.INITIAL_CONTEXT_FACTORY,String.format(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Context.INITIAL_CONTEXT_FACTORY),PluginProperties.builder().build());
}","@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  pipelineConfigurer.usePluginClass(""String_Node_Str"",Context.INITIAL_CONTEXT_FACTORY,""String_Node_Str"",PluginProperties.builder().build());
}"
7046,"@Override public void process(final ApplicationDeployable input) throws Exception {
  ImmutableList.Builder<Program> programs=ImmutableList.builder();
  final ApplicationSpecification appSpec=input.getSpecification();
  final String applicationName=appSpec.getName();
  final ArchiveBundler bundler=new ArchiveBundler(input.getLocation());
  Id.Namespace namespaceId=input.getId().getNamespace();
  Location namespacedLocation=namespacedLocationFactory.get(namespaceId);
  final Location appFabricDir=namespacedLocation.append(configuration.get(Constants.AppFabric.OUTPUT_DIR));
  if (!appFabricDir.exists() && !appFabricDir.mkdirs() && !appFabricDir.exists()) {
    throw new IOException(String.format(""String_Node_Str"",appFabricDir.toURI().getPath()));
  }
  Iterable<ProgramSpecification> specifications=Iterables.concat(appSpec.getMapReduce().values(),appSpec.getFlows().values(),appSpec.getWorkflows().values(),appSpec.getServices().values(),appSpec.getSpark().values(),appSpec.getWorkers().values());
  Set<String> servingHostNames=WebappProgramRunner.getServingHostNames(Locations.newInputSupplier(input.getLocation()));
  if (!servingHostNames.isEmpty()) {
    specifications=Iterables.concat(specifications,ImmutableList.of(createWebappSpec(ProgramType.WEBAPP.toString().toLowerCase())));
  }
  ListeningExecutorService executorService=MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(10,Threads.createDaemonThreadFactory(""String_Node_Str"")));
  try {
    List<ListenableFuture<Location>> futures=Lists.newArrayList();
    for (    final ProgramSpecification spec : specifications) {
      ListenableFuture<Location> future=executorService.submit(new Callable<Location>(){
        @Override public Location call() throws Exception {
          ProgramType type=ProgramTypes.fromSpecification(spec);
          String name=String.format(Locale.ENGLISH,""String_Node_Str"",applicationName,type);
          Location programDir=appFabricDir.append(name);
          if (!programDir.exists()) {
            programDir.mkdirs();
          }
          Location output=programDir.append(String.format(""String_Node_Str"",spec.getName()));
          return ProgramBundle.create(input.getId(),bundler,output,spec.getName(),spec.getClassName(),type,appSpec);
        }
      }
);
      futures.add(future);
    }
    for (    Location jarLocation : Futures.allAsList(futures).get()) {
      programs.add(Programs.create(jarLocation,null));
    }
  }
  finally {
    executorService.shutdown();
  }
  moveAppArchiveUnderAppDirectory(input.getLocation(),applicationName);
  emit(new ApplicationWithPrograms(input,programs.build()));
}","@Override public void process(final ApplicationDeployable input) throws Exception {
  ImmutableList.Builder<Program> programs=ImmutableList.builder();
  final ApplicationSpecification appSpec=input.getSpecification();
  final String applicationName=appSpec.getName();
  final ArchiveBundler bundler=new ArchiveBundler(input.getLocation());
  Id.Namespace namespaceId=input.getId().getNamespace();
  Location namespacedLocation=namespacedLocationFactory.get(namespaceId);
  final Location appFabricDir=namespacedLocation.append(configuration.get(Constants.AppFabric.OUTPUT_DIR));
  if (!appFabricDir.exists() && !appFabricDir.mkdirs() && !appFabricDir.exists()) {
    throw new IOException(String.format(""String_Node_Str"",appFabricDir.toURI().getPath()));
  }
  Iterable<ProgramSpecification> specifications=Iterables.concat(appSpec.getMapReduce().values(),appSpec.getFlows().values(),appSpec.getWorkflows().values(),appSpec.getServices().values(),appSpec.getSpark().values(),appSpec.getWorkers().values());
  Set<String> servingHostNames=WebappProgramRunner.getServingHostNames(Locations.newInputSupplier(input.getLocation()));
  if (!servingHostNames.isEmpty()) {
    specifications=Iterables.concat(specifications,ImmutableList.of(createWebappSpec(ProgramType.WEBAPP.toString().toLowerCase())));
  }
  ListeningExecutorService executorService=MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(10,Threads.createDaemonThreadFactory(""String_Node_Str"")));
  try {
    List<ListenableFuture<Location>> futures=Lists.newArrayList();
    for (    final ProgramSpecification spec : specifications) {
      ListenableFuture<Location> future=executorService.submit(new Callable<Location>(){
        @Override public Location call() throws Exception {
          ProgramType type=ProgramTypes.fromSpecification(spec);
          String name=String.format(Locale.ENGLISH,""String_Node_Str"",applicationName,type);
          Location programDir=appFabricDir.append(name);
          if (!programDir.exists()) {
            programDir.mkdirs();
          }
          Location output=programDir.append(String.format(""String_Node_Str"",spec.getName()));
          return ProgramBundle.create(input.getId(),bundler,output,spec.getName(),spec.getClassName(),type,appSpec);
        }
      }
);
      futures.add(future);
    }
    for (    Location jarLocation : Futures.allAsList(futures).get()) {
      programs.add(Programs.create(jarLocation,null));
    }
  }
  finally {
    executorService.shutdown();
  }
  Location newLocation=moveAppArchiveUnderAppDirectory(input.getLocation(),applicationName);
  ApplicationDeployable updatedAppDeployable=new ApplicationDeployable(input.getId(),input.getSpecification(),input.getExistingAppSpec(),input.getApplicationDeployScope(),newLocation);
  emit(new ApplicationWithPrograms(updatedAppDeployable,programs.build()));
}"
7047,"private void moveAppArchiveUnderAppDirectory(Location origArchiveLocation,String appName) throws IOException {
  Location oldArchiveDir=Locations.getParent(origArchiveLocation);
  Preconditions.checkState(oldArchiveDir != null,""String_Node_Str"");
  String archiveParentDirName=oldArchiveDir.getName();
  Location appFabricOutputLocation=Locations.getParent(oldArchiveDir);
  Preconditions.checkState(appFabricOutputLocation != null,""String_Node_Str"");
  Location applicationArchiveDir=appFabricOutputLocation.append(appName);
  Locations.mkdirsIfNotExists(applicationArchiveDir);
  Location newArchiveLocation=applicationArchiveDir.append(archiveParentDirName);
  if (newArchiveLocation.exists()) {
    newArchiveLocation.delete(true);
  }
  if (oldArchiveDir.renameTo(newArchiveLocation) == null) {
    throw new IOException(String.format(""String_Node_Str"",oldArchiveDir.toURI(),newArchiveLocation.toURI()));
  }
}","private Location moveAppArchiveUnderAppDirectory(Location origArchiveLocation,String appName) throws IOException {
  Location oldArchiveDir=Locations.getParent(origArchiveLocation);
  Preconditions.checkState(oldArchiveDir != null,""String_Node_Str"");
  String archiveParentDirName=oldArchiveDir.getName();
  Location appFabricOutputLocation=Locations.getParent(oldArchiveDir);
  Preconditions.checkState(appFabricOutputLocation != null,""String_Node_Str"");
  Location applicationArchiveDir=appFabricOutputLocation.append(appName);
  Locations.mkdirsIfNotExists(applicationArchiveDir);
  Location newArchiveLocation=applicationArchiveDir.append(archiveParentDirName);
  if (newArchiveLocation.exists()) {
    newArchiveLocation.delete(true);
  }
  if (oldArchiveDir.renameTo(newArchiveLocation) == null) {
    throw new IOException(String.format(""String_Node_Str"",oldArchiveDir.toURI(),newArchiveLocation.toURI()));
  }
  return newArchiveLocation.append(origArchiveLocation.getName());
}"
7048,"/** 
 * Delete an application specified by appId.
 * @param appId the {@link Id.Application} of the application to be removed
 * @throws Exception
 */
public void removeApplication(final Id.Application appId) throws Exception {
  boolean appRunning=runtimeService.checkAnyRunning(new Predicate<Id.Program>(){
    @Override public boolean apply(    Id.Program programId){
      return programId.getApplication().equals(appId);
    }
  }
,ProgramType.values());
  if (appRunning) {
    throw new CannotBeDeletedException(appId);
  }
  ApplicationSpecification spec=store.getApplication(appId);
  if (spec == null) {
    throw new NotFoundException(appId);
  }
  for (  WorkflowSpecification workflowSpec : spec.getWorkflows().values()) {
    Id.Program workflowProgramId=Id.Program.from(appId,ProgramType.WORKFLOW,workflowSpec.getName());
    scheduler.deleteSchedules(workflowProgramId,SchedulableProgramType.WORKFLOW);
  }
  deleteMetrics(appId.getNamespaceId(),appId.getId());
  deletePreferences(appId);
  for (  FlowSpecification flowSpecification : spec.getFlows().values()) {
    Id.Program flowProgramId=Id.Program.from(appId,ProgramType.FLOW,flowSpecification.getName());
    Multimap<String,Long> streamGroups=HashMultimap.create();
    for (    FlowletConnection connection : flowSpecification.getConnections()) {
      if (connection.getSourceType() == FlowletConnection.Type.STREAM) {
        long groupId=FlowUtils.generateConsumerGroupId(flowProgramId,connection.getTargetName());
        streamGroups.put(connection.getSourceName(),groupId);
      }
    }
    String namespace=String.format(""String_Node_Str"",flowProgramId.getApplicationId(),flowProgramId.getId());
    for (    Map.Entry<String,Collection<Long>> entry : streamGroups.asMap().entrySet()) {
      streamConsumerFactory.dropAll(Id.Stream.from(appId.getNamespaceId(),entry.getKey()),namespace,entry.getValue());
    }
    queueAdmin.dropAllForFlow(appId.getNamespaceId(),appId.getId(),flowSpecification.getName());
  }
  deleteProgramLocations(appId);
  Location appArchive=store.getApplicationArchiveLocation(appId);
  Preconditions.checkNotNull(appArchive,""String_Node_Str"",appId.getId());
  appArchive.delete();
  store.removeApplication(appId);
  try {
    usageRegistry.unregister(appId);
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",appId,e);
  }
}","/** 
 * Delete an application specified by appId.
 * @param appId the {@link Id.Application} of the application to be removed
 * @throws Exception
 */
public void removeApplication(final Id.Application appId) throws Exception {
  boolean appRunning=runtimeService.checkAnyRunning(new Predicate<Id.Program>(){
    @Override public boolean apply(    Id.Program programId){
      return programId.getApplication().equals(appId);
    }
  }
,ProgramType.values());
  if (appRunning) {
    throw new CannotBeDeletedException(appId);
  }
  ApplicationSpecification spec=store.getApplication(appId);
  if (spec == null) {
    throw new NotFoundException(appId);
  }
  for (  WorkflowSpecification workflowSpec : spec.getWorkflows().values()) {
    Id.Program workflowProgramId=Id.Program.from(appId,ProgramType.WORKFLOW,workflowSpec.getName());
    scheduler.deleteSchedules(workflowProgramId,SchedulableProgramType.WORKFLOW);
  }
  deleteMetrics(appId.getNamespaceId(),appId.getId());
  deletePreferences(appId);
  for (  FlowSpecification flowSpecification : spec.getFlows().values()) {
    Id.Program flowProgramId=Id.Program.from(appId,ProgramType.FLOW,flowSpecification.getName());
    Multimap<String,Long> streamGroups=HashMultimap.create();
    for (    FlowletConnection connection : flowSpecification.getConnections()) {
      if (connection.getSourceType() == FlowletConnection.Type.STREAM) {
        long groupId=FlowUtils.generateConsumerGroupId(flowProgramId,connection.getTargetName());
        streamGroups.put(connection.getSourceName(),groupId);
      }
    }
    String namespace=String.format(""String_Node_Str"",flowProgramId.getApplicationId(),flowProgramId.getId());
    for (    Map.Entry<String,Collection<Long>> entry : streamGroups.asMap().entrySet()) {
      streamConsumerFactory.dropAll(Id.Stream.from(appId.getNamespaceId(),entry.getKey()),namespace,entry.getValue());
    }
    queueAdmin.dropAllForFlow(appId.getNamespaceId(),appId.getId(),flowSpecification.getName());
  }
  deleteProgramLocations(appId);
  Location appArchive=store.getApplicationArchiveLocation(appId);
  Preconditions.checkNotNull(appArchive,""String_Node_Str"",appId.getId());
  if (!appArchive.delete()) {
    LOG.debug(""String_Node_Str"");
  }
  store.removeApplication(appId);
  try {
    usageRegistry.unregister(appId);
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",appId,e);
  }
}"
7049,"/** 
 * Deletes all modules in a namespace, other than system. Presumes that the namespace has already been checked to be non-system.
 * @param namespaceId the {@link Id.Namespace} to delete modules from.
 */
public void deleteModules(final Id.Namespace namespaceId) throws DatasetModuleConflictException {
  Preconditions.checkArgument(namespaceId != null && !Constants.SYSTEM_NAMESPACE_ID.equals(namespaceId),""String_Node_Str"");
  LOG.warn(""String_Node_Str"",namespaceId);
  try {
    mdsDatasets.execute(new TxCallable<MDSDatasets,Void>(){
      @Override public Void call(      MDSDatasets datasets) throws DatasetModuleConflictException {
        Set<String> typesToDelete=Sets.newHashSet();
        for (        DatasetModuleMeta module : datasets.getTypeMDS().getModules(namespaceId)) {
          typesToDelete.addAll(module.getTypes());
        }
        Collection<DatasetSpecification> dependentInstances=datasets.getInstanceMDS().getByTypes(namespaceId,typesToDelete);
        if (dependentInstances.size() > 0) {
          String msg=String.format(""String_Node_Str"");
          throw new DatasetModuleConflictException(msg);
        }
        datasets.getTypeMDS().deleteModules(namespaceId);
        return null;
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    if (e.getCause() != null && e.getCause() instanceof DatasetModuleConflictException) {
      throw (DatasetModuleConflictException)e.getCause();
    }
    LOG.error(""String_Node_Str"",namespaceId);
    throw Throwables.propagate(e);
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}","/** 
 * Deletes all modules in a namespace, other than system. Presumes that the namespace has already been checked to be non-system.
 * @param namespaceId the {@link Id.Namespace} to delete modules from.
 */
public void deleteModules(final Id.Namespace namespaceId) throws DatasetModuleConflictException {
  Preconditions.checkArgument(namespaceId != null && !Constants.SYSTEM_NAMESPACE_ID.equals(namespaceId),""String_Node_Str"");
  LOG.warn(""String_Node_Str"",namespaceId);
  try {
    mdsDatasets.execute(new TxCallable<MDSDatasets,Void>(){
      @Override public Void call(      MDSDatasets datasets) throws DatasetModuleConflictException, IOException {
        Set<String> typesToDelete=Sets.newHashSet();
        List<Location> moduleLocations=Lists.newArrayList();
        for (        DatasetModuleMeta module : datasets.getTypeMDS().getModules(namespaceId)) {
          typesToDelete.addAll(module.getTypes());
          moduleLocations.add(locationFactory.create(module.getJarLocation()));
        }
        Collection<DatasetSpecification> dependentInstances=datasets.getInstanceMDS().getByTypes(namespaceId,typesToDelete);
        if (dependentInstances.size() > 0) {
          String msg=String.format(""String_Node_Str"");
          throw new DatasetModuleConflictException(msg);
        }
        datasets.getTypeMDS().deleteModules(namespaceId);
        for (        Location moduleLocation : moduleLocations) {
          if (!moduleLocation.delete()) {
            LOG.debug(""String_Node_Str"" + moduleLocation.toURI().getPath());
          }
        }
        return null;
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    if (e.getCause() != null && e.getCause() instanceof DatasetModuleConflictException) {
      throw (DatasetModuleConflictException)e.getCause();
    }
    LOG.error(""String_Node_Str"",namespaceId);
    throw Throwables.propagate(e);
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}"
7050,"/** 
 * Deletes specified dataset module
 * @param datasetModuleId {@link Id.DatasetModule} of the dataset module to delete
 * @return true if deleted successfully, false if module didn't exist: nothing to delete
 * @throws DatasetModuleConflictException when there are other modules depend on the specified one, in which casedeletion does NOT happen
 */
public boolean deleteModule(final Id.DatasetModule datasetModuleId) throws DatasetModuleConflictException {
  LOG.info(""String_Node_Str"",datasetModuleId);
  try {
    return mdsDatasets.execute(new TxCallable<MDSDatasets,Boolean>(){
      @Override public Boolean call(      MDSDatasets datasets) throws DatasetModuleConflictException {
        DatasetModuleMeta module=datasets.getTypeMDS().getModule(datasetModuleId);
        if (module == null) {
          return false;
        }
        if (module.getUsedByModules().size() > 0) {
          String msg=String.format(""String_Node_Str"",module);
          throw new DatasetModuleConflictException(msg);
        }
        Collection<DatasetSpecification> dependentInstances=datasets.getInstanceMDS().getByTypes(datasetModuleId.getNamespace(),ImmutableSet.copyOf(module.getTypes()));
        if (dependentInstances.size() > 0) {
          String msg=String.format(""String_Node_Str"",module);
          throw new DatasetModuleConflictException(msg);
        }
        for (        String usedModuleName : module.getUsesModules()) {
          Id.DatasetModule usedModuleId=Id.DatasetModule.from(datasetModuleId.getNamespace(),usedModuleName);
          DatasetModuleMeta usedModule=datasets.getTypeMDS().getModule(usedModuleId);
          if (usedModule == null) {
            usedModuleId=Id.DatasetModule.from(Constants.SYSTEM_NAMESPACE_ID,usedModuleName);
            usedModule=datasets.getTypeMDS().getModule(usedModuleId);
            Preconditions.checkState(usedModule != null,""String_Node_Str"",usedModuleName,datasetModuleId.getId());
          }
          usedModule.removeUsedByModule(datasetModuleId.getId());
          datasets.getTypeMDS().writeModule(usedModuleId.getNamespace(),usedModule);
        }
        datasets.getTypeMDS().deleteModule(datasetModuleId);
        return true;
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    if (e.getCause() != null && e.getCause() instanceof DatasetModuleConflictException) {
      throw (DatasetModuleConflictException)e.getCause();
    }
    throw Throwables.propagate(e);
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}","/** 
 * Deletes specified dataset module
 * @param datasetModuleId {@link Id.DatasetModule} of the dataset module to delete
 * @return true if deleted successfully, false if module didn't exist: nothing to delete
 * @throws DatasetModuleConflictException when there are other modules depend on the specified one, in which casedeletion does NOT happen
 */
public boolean deleteModule(final Id.DatasetModule datasetModuleId) throws DatasetModuleConflictException {
  LOG.info(""String_Node_Str"",datasetModuleId);
  try {
    return mdsDatasets.execute(new TxCallable<MDSDatasets,Boolean>(){
      @Override public Boolean call(      MDSDatasets datasets) throws DatasetModuleConflictException, IOException {
        DatasetModuleMeta module=datasets.getTypeMDS().getModule(datasetModuleId);
        if (module == null) {
          return false;
        }
        if (module.getUsedByModules().size() > 0) {
          String msg=String.format(""String_Node_Str"",module);
          throw new DatasetModuleConflictException(msg);
        }
        Collection<DatasetSpecification> dependentInstances=datasets.getInstanceMDS().getByTypes(datasetModuleId.getNamespace(),ImmutableSet.copyOf(module.getTypes()));
        if (dependentInstances.size() > 0) {
          String msg=String.format(""String_Node_Str"",module);
          throw new DatasetModuleConflictException(msg);
        }
        for (        String usedModuleName : module.getUsesModules()) {
          Id.DatasetModule usedModuleId=Id.DatasetModule.from(datasetModuleId.getNamespace(),usedModuleName);
          DatasetModuleMeta usedModule=datasets.getTypeMDS().getModule(usedModuleId);
          if (usedModule == null) {
            usedModuleId=Id.DatasetModule.from(Constants.SYSTEM_NAMESPACE_ID,usedModuleName);
            usedModule=datasets.getTypeMDS().getModule(usedModuleId);
            Preconditions.checkState(usedModule != null,""String_Node_Str"",usedModuleName,datasetModuleId.getId());
          }
          usedModule.removeUsedByModule(datasetModuleId.getId());
          datasets.getTypeMDS().writeModule(usedModuleId.getNamespace(),usedModule);
        }
        datasets.getTypeMDS().deleteModule(datasetModuleId);
        Location moduleJarLocation=locationFactory.create(module.getJarLocation());
        if (!moduleJarLocation.delete()) {
          LOG.debug(""String_Node_Str"");
        }
        return true;
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    if (e.getCause() != null && e.getCause() instanceof DatasetModuleConflictException) {
      throw (DatasetModuleConflictException)e.getCause();
    }
    throw Throwables.propagate(e);
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}"
7051,"@Override public void run(){
  try {
    Location configLocation=getConfigLocation(streamId);
    if (!configLocation.exists()) {
      return;
    }
    alterExploreStream(StreamUtils.getStreamIdFromLocation(streamLocation),false);
    configLocation.delete();
    streamMetaStore.removeStream(streamId);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","@Override public void run(){
  try {
    Location configLocation=getConfigLocation(streamId);
    if (!configLocation.exists()) {
      return;
    }
    alterExploreStream(StreamUtils.getStreamIdFromLocation(streamLocation),false);
    if (!configLocation.delete()) {
      LOG.debug(""String_Node_Str"" + streamLocation.toURI().getPath());
    }
    streamMetaStore.removeStream(streamId);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
7052,"private void doDrop(final Id.Stream streamId,final Location streamLocation) throws Exception {
  streamCoordinatorClient.deleteStream(streamId,new Runnable(){
    @Override public void run(){
      try {
        Location configLocation=getConfigLocation(streamId);
        if (!configLocation.exists()) {
          return;
        }
        alterExploreStream(StreamUtils.getStreamIdFromLocation(streamLocation),false);
        configLocation.delete();
        streamMetaStore.removeStream(streamId);
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
}","private void doDrop(final Id.Stream streamId,final Location streamLocation) throws Exception {
  streamCoordinatorClient.deleteStream(streamId,new Runnable(){
    @Override public void run(){
      try {
        Location configLocation=getConfigLocation(streamId);
        if (!configLocation.exists()) {
          return;
        }
        alterExploreStream(StreamUtils.getStreamIdFromLocation(streamLocation),false);
        if (!configLocation.delete()) {
          LOG.debug(""String_Node_Str"" + streamLocation.toURI().getPath());
        }
        streamMetaStore.removeStream(streamId);
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
}"
7053,"/** 
 * Deletes an adapter
 */
@DELETE @Path(""String_Node_Str"") public void deleteAdapter(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String adapterName){
  try {
    adapterService.removeAdapter(Id.Namespace.from(namespaceId),adapterName);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  CannotBeDeletedException e) {
    responder.sendString(HttpResponseStatus.FORBIDDEN,e.getMessage());
  }
catch (  NotFoundException e) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
  }
catch (  Throwable t) {
    LOG.error(""String_Node_Str"",namespaceId,adapterName,""String_Node_Str"",t);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Deletes an adapter
 */
@DELETE @Path(""String_Node_Str"") public void deleteAdapter(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String adapterName){
  try {
    adapterService.removeAdapter(Id.Namespace.from(namespaceId),adapterName);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  CannotBeDeletedException e) {
    responder.sendString(HttpResponseStatus.CONFLICT,e.getMessage());
  }
catch (  NotFoundException e) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
  }
catch (  Throwable t) {
    LOG.error(""String_Node_Str"",namespaceId,adapterName,""String_Node_Str"",t);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}"
7054,"/** 
 * DO NOT DOCUMENT THIS API
 */
@DELETE @Path(""String_Node_Str"") public void deleteDatasets(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespace){
  if (!cConf.getBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,Constants.Dangerous.DEFAULT_UNRECOVERABLE_RESET)) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
    return;
  }
  Id.Namespace namespaceId=Id.Namespace.from(namespace);
  try {
    namespaceAdmin.deleteDatasets(namespaceId);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  NotFoundException e) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",namespace));
  }
catch (  NamespaceCannotBeDeletedException e) {
    responder.sendString(HttpResponseStatus.FORBIDDEN,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}","/** 
 * DO NOT DOCUMENT THIS API
 */
@DELETE @Path(""String_Node_Str"") public void deleteDatasets(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespace){
  if (!cConf.getBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,Constants.Dangerous.DEFAULT_UNRECOVERABLE_RESET)) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
    return;
  }
  Id.Namespace namespaceId=Id.Namespace.from(namespace);
  try {
    namespaceAdmin.deleteDatasets(namespaceId);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  NotFoundException e) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",namespace));
  }
catch (  NamespaceCannotBeDeletedException e) {
    responder.sendString(HttpResponseStatus.CONFLICT,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}"
7055,"/** 
 * Remove adapter identified by the namespace and name and also deletes the template for the adapter if this was the last adapter associated with it
 * @param namespace namespace id
 * @param adapterName adapter name
 * @throws AdapterNotFoundException if the adapter to be removed is not found.
 * @throws CannotBeDeletedException if the adapter is not stopped.
 */
public synchronized void removeAdapter(Id.Namespace namespace,String adapterName) throws AdapterNotFoundException, CannotBeDeletedException {
  AdapterStatus adapterStatus=getAdapterStatus(namespace,adapterName);
  AdapterSpecification adapterSpec=getAdapter(namespace,adapterName);
  Id.Application applicationId=Id.Application.from(namespace,adapterSpec.getTemplate());
  if (adapterStatus != AdapterStatus.STOPPED) {
    throw new CannotBeDeletedException(Id.Adapter.from(namespace,adapterName));
  }
  store.removeAdapter(namespace,adapterName);
  try {
    deleteApp(applicationId);
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",adapterSpec.getTemplate(),adapterName);
  }
}","/** 
 * Remove adapter identified by the namespace and name and also deletes the template for the adapter if this was the last adapter associated with it
 * @param namespace namespace id
 * @param adapterName adapter name
 * @throws AdapterNotFoundException if the adapter to be removed is not found.
 * @throws CannotBeDeletedException if the adapter is not stopped.
 */
public synchronized void removeAdapter(Id.Namespace namespace,String adapterName) throws AdapterNotFoundException, CannotBeDeletedException {
  AdapterStatus adapterStatus=getAdapterStatus(namespace,adapterName);
  AdapterSpecification adapterSpec=getAdapter(namespace,adapterName);
  Id.Application applicationId=Id.Application.from(namespace,adapterSpec.getTemplate());
  if (adapterStatus != AdapterStatus.STOPPED) {
    throw new CannotBeDeletedException(Id.Adapter.from(namespace,adapterName),""String_Node_Str"" + ""String_Node_Str"");
  }
  store.removeAdapter(namespace,adapterName);
  try {
    deleteApp(applicationId);
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",adapterSpec.getTemplate(),adapterName);
  }
}"
7056,"private void testAdapterLifeCycle(String namespaceId,String templateId,String adapterName,AdapterConfig adapterConfig) throws Exception {
  String deleteURL=getVersionedAPIPath(""String_Node_Str"" + templateId,Constants.Gateway.API_VERSION_3_TOKEN,namespaceId);
  HttpResponse response=createAdapter(namespaceId,adapterName,adapterConfig);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=createAdapter(namespaceId,adapterName,adapterConfig);
  Assert.assertEquals(409,response.getStatusLine().getStatusCode());
  response=listAdapters(namespaceId);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  List<AdapterDetail> list=readResponse(response,ADAPTER_SPEC_LIST_TYPE);
  Assert.assertEquals(1,list.size());
  checkIsExpected(adapterConfig,list.get(0));
  response=getAdapter(namespaceId,adapterName);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  AdapterDetail receivedAdapterConfig=readResponse(response,AdapterDetail.class);
  checkIsExpected(adapterConfig,receivedAdapterConfig);
  List<JsonObject> deployedApps=getAppList(namespaceId);
  Assert.assertEquals(1,deployedApps.size());
  JsonObject deployedApp=deployedApps.get(0);
  Assert.assertEquals(templateId,deployedApp.get(""String_Node_Str"").getAsString());
  String status=getAdapterStatus(Id.Adapter.from(namespaceId,adapterName));
  Assert.assertEquals(""String_Node_Str"",status);
  response=startStopAdapter(namespaceId,adapterName,""String_Node_Str"");
  Assert.assertEquals(409,response.getStatusLine().getStatusCode());
  response=startStopAdapter(namespaceId,adapterName,""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  status=getAdapterStatus(Id.Adapter.from(namespaceId,adapterName));
  Assert.assertEquals(""String_Node_Str"",status);
  deleteApplication(1,deleteURL,403);
  response=deleteAdapter(namespaceId,adapterName);
  Assert.assertEquals(403,response.getStatusLine().getStatusCode());
  response=startStopAdapter(namespaceId,adapterName,""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  deleteApplication(1,deleteURL,403);
  response=deleteAdapter(namespaceId,adapterName);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=getAdapter(namespaceId,adapterName);
  Assert.assertEquals(404,response.getStatusLine().getStatusCode());
  deleteApplication(60,deleteURL,404);
  deployedApps=getAppList(namespaceId);
  Assert.assertTrue(deployedApps.isEmpty());
  response=createAdapter(namespaceId,adapterName,adapterConfig);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=deleteAdapter(namespaceId,adapterName);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  deleteApplication(60,deleteURL,404);
  deployedApps=getAppList(namespaceId);
  Assert.assertTrue(deployedApps.isEmpty());
}","private void testAdapterLifeCycle(String namespaceId,String templateId,String adapterName,AdapterConfig adapterConfig) throws Exception {
  String deleteURL=getVersionedAPIPath(""String_Node_Str"" + templateId,Constants.Gateway.API_VERSION_3_TOKEN,namespaceId);
  HttpResponse response=createAdapter(namespaceId,adapterName,adapterConfig);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=createAdapter(namespaceId,adapterName,adapterConfig);
  Assert.assertEquals(409,response.getStatusLine().getStatusCode());
  response=listAdapters(namespaceId);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  List<AdapterDetail> list=readResponse(response,ADAPTER_SPEC_LIST_TYPE);
  Assert.assertEquals(1,list.size());
  checkIsExpected(adapterConfig,list.get(0));
  response=getAdapter(namespaceId,adapterName);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  AdapterDetail receivedAdapterConfig=readResponse(response,AdapterDetail.class);
  checkIsExpected(adapterConfig,receivedAdapterConfig);
  List<JsonObject> deployedApps=getAppList(namespaceId);
  Assert.assertEquals(1,deployedApps.size());
  JsonObject deployedApp=deployedApps.get(0);
  Assert.assertEquals(templateId,deployedApp.get(""String_Node_Str"").getAsString());
  String status=getAdapterStatus(Id.Adapter.from(namespaceId,adapterName));
  Assert.assertEquals(""String_Node_Str"",status);
  response=startStopAdapter(namespaceId,adapterName,""String_Node_Str"");
  Assert.assertEquals(409,response.getStatusLine().getStatusCode());
  response=startStopAdapter(namespaceId,adapterName,""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  status=getAdapterStatus(Id.Adapter.from(namespaceId,adapterName));
  Assert.assertEquals(""String_Node_Str"",status);
  deleteApplication(1,deleteURL,403);
  response=deleteAdapter(namespaceId,adapterName);
  Assert.assertEquals(409,response.getStatusLine().getStatusCode());
  response=startStopAdapter(namespaceId,adapterName,""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  deleteApplication(1,deleteURL,403);
  response=deleteAdapter(namespaceId,adapterName);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=getAdapter(namespaceId,adapterName);
  Assert.assertEquals(404,response.getStatusLine().getStatusCode());
  deleteApplication(60,deleteURL,404);
  deployedApps=getAppList(namespaceId);
  Assert.assertTrue(deployedApps.isEmpty());
  response=createAdapter(namespaceId,adapterName,adapterConfig);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=deleteAdapter(namespaceId,adapterName);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  deleteApplication(60,deleteURL,404);
  deployedApps=getAppList(namespaceId);
  Assert.assertTrue(deployedApps.isEmpty());
}"
7057,"@Test public void testDeleteDatasetsOnly() throws Exception {
  CConfiguration cConf=getInjector().getInstance(CConfiguration.class);
  assertResponseCode(200,createNamespace(NAME));
  assertResponseCode(200,getNamespace(NAME));
  NamespacedLocationFactory namespacedLocationFactory=getInjector().getInstance(NamespacedLocationFactory.class);
  Location nsLocation=namespacedLocationFactory.get(Id.Namespace.from(NAME));
  Assert.assertTrue(nsLocation.exists());
  DatasetFramework dsFramework=getInjector().getInstance(DatasetFramework.class);
  deploy(AppWithServices.class,Constants.Gateway.API_VERSION_3_TOKEN,NAME);
  deploy(AppWithDataset.class,Constants.Gateway.API_VERSION_3_TOKEN,NAME);
  Id.DatasetInstance myDataset=Id.DatasetInstance.from(NAME,""String_Node_Str"");
  Assert.assertTrue(dsFramework.hasInstance(myDataset));
  Id.Program program=Id.Program.from(NAME_ID,""String_Node_Str"",ProgramType.SERVICE,""String_Node_Str"");
  startProgram(program);
  boolean resetEnabled=cConf.getBoolean(Constants.Dangerous.UNRECOVERABLE_RESET);
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,false);
  assertResponseCode(403,deleteNamespaceData(NAME));
  Assert.assertTrue(nsLocation.exists());
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,resetEnabled);
  assertResponseCode(409,deleteNamespace(NAME));
  Assert.assertTrue(nsLocation.exists());
  stopProgram(program);
  assertResponseCode(200,deleteNamespaceData(NAME));
  Assert.assertTrue(nsLocation.exists());
  Assert.assertTrue(getAppList(NAME).size() == 2);
  Assert.assertTrue(getAppDetails(NAME,""String_Node_Str"").get(""String_Node_Str"").getAsString().equals(""String_Node_Str""));
  Assert.assertTrue(getAppDetails(NAME,""String_Node_Str"").get(""String_Node_Str"").getAsString().equals(""String_Node_Str""));
  assertResponseCode(200,getNamespace(NAME));
  Assert.assertFalse(dsFramework.hasInstance(myDataset));
  assertResponseCode(200,deleteNamespace(NAME));
  assertResponseCode(404,getNamespace(NAME));
}","@Test public void testDeleteDatasetsOnly() throws Exception {
  CConfiguration cConf=getInjector().getInstance(CConfiguration.class);
  assertResponseCode(200,createNamespace(NAME));
  assertResponseCode(200,getNamespace(NAME));
  NamespacedLocationFactory namespacedLocationFactory=getInjector().getInstance(NamespacedLocationFactory.class);
  Location nsLocation=namespacedLocationFactory.get(Id.Namespace.from(NAME));
  Assert.assertTrue(nsLocation.exists());
  DatasetFramework dsFramework=getInjector().getInstance(DatasetFramework.class);
  deploy(AppWithServices.class,Constants.Gateway.API_VERSION_3_TOKEN,NAME);
  deploy(AppWithDataset.class,Constants.Gateway.API_VERSION_3_TOKEN,NAME);
  Id.DatasetInstance myDataset=Id.DatasetInstance.from(NAME,""String_Node_Str"");
  Assert.assertTrue(dsFramework.hasInstance(myDataset));
  Id.Program program=Id.Program.from(NAME_ID,""String_Node_Str"",ProgramType.SERVICE,""String_Node_Str"");
  startProgram(program);
  boolean resetEnabled=cConf.getBoolean(Constants.Dangerous.UNRECOVERABLE_RESET);
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,false);
  assertResponseCode(403,deleteNamespaceData(NAME));
  Assert.assertTrue(nsLocation.exists());
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,resetEnabled);
  assertResponseCode(409,deleteNamespaceData(NAME));
  Assert.assertTrue(nsLocation.exists());
  stopProgram(program);
  assertResponseCode(200,deleteNamespaceData(NAME));
  Assert.assertTrue(nsLocation.exists());
  Assert.assertTrue(getAppList(NAME).size() == 2);
  Assert.assertTrue(getAppDetails(NAME,""String_Node_Str"").get(""String_Node_Str"").getAsString().equals(""String_Node_Str""));
  Assert.assertTrue(getAppDetails(NAME,""String_Node_Str"").get(""String_Node_Str"").getAsString().equals(""String_Node_Str""));
  assertResponseCode(200,getNamespace(NAME));
  Assert.assertFalse(dsFramework.hasInstance(myDataset));
  assertResponseCode(200,deleteNamespace(NAME));
  assertResponseCode(404,getNamespace(NAME));
}"
7058,"private void executeAll(Iterator<WorkflowNode> iterator,ApplicationSpecification appSpec,InstantiatorFactory instantiator,ClassLoader classLoader,WorkflowToken token){
  while (iterator.hasNext() && runningThread != null) {
    try {
      blockIfSuspended();
      WorkflowNode node=iterator.next();
      executeNode(appSpec,node,instantiator,classLoader,token);
    }
 catch (    Throwable t) {
      Throwables.propagate(t);
    }
  }
}","private void executeAll(Iterator<WorkflowNode> iterator,ApplicationSpecification appSpec,InstantiatorFactory instantiator,ClassLoader classLoader,WorkflowToken token){
  while (iterator.hasNext() && runningThread != null) {
    try {
      blockIfSuspended();
      WorkflowNode node=iterator.next();
      executeNode(appSpec,node,instantiator,classLoader,token);
    }
 catch (    Throwable t) {
      Throwable rootCause=Throwables.getRootCause(t);
      if (rootCause instanceof InterruptedException) {
        LOG.error(""String_Node_Str"");
        break;
      }
      Throwables.propagate(t);
    }
  }
}"
7059,"@Override public void run(){
  LOG.info(""String_Node_Str"");
  try {
    File file=new File(filePath);
    file.createNewFile();
    File doneFile=new File(doneFilePath);
    while (!doneFile.exists()) {
      TimeUnit.SECONDS.sleep(1);
    }
  }
 catch (  Exception e) {
  }
}","@Override public void run(){
  LOG.info(""String_Node_Str"" + getContext().getSpecification().getName());
  try {
    File file=new File(getContext().getRuntimeArguments().get(getContext().getSpecification().getName() + ""String_Node_Str""));
    file.createNewFile();
    File doneFile=new File(getContext().getRuntimeArguments().get(getContext().getSpecification().getName() + ""String_Node_Str""));
    while (!doneFile.exists()) {
      TimeUnit.MILLISECONDS.sleep(50);
    }
  }
 catch (  Exception e) {
  }
}"
7060,"@Override public void configure(){
  setName(""String_Node_Str"");
  setDescription(""String_Node_Str"");
  fork().addAction(new OneAction()).also().addAction(new AnotherAction()).join();
}","@Override public void configure(){
  setName(""String_Node_Str"");
  setDescription(""String_Node_Str"");
  addAction(new SimpleAction(""String_Node_Str""));
  fork().addAction(new SimpleAction(""String_Node_Str"")).also().addAction(new SimpleAction(""String_Node_Str"")).join();
}"
7061,"@Test public void testWorkflowForkApp() throws Exception {
  String workflowAppWithFork=""String_Node_Str"";
  String workflowWithFork=""String_Node_Str"";
  File doneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File oneActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File anotherActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  HttpResponse response=deploy(WorkflowAppWithFork.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,workflowAppWithFork,ProgramType.WORKFLOW,workflowWithFork);
  Map<String,String> runtimeArguments=ImmutableMap.of(""String_Node_Str"",doneFile.getAbsolutePath(),""String_Node_Str"",oneActionFile.getAbsolutePath(),""String_Node_Str"",anotherActionFile.getAbsolutePath());
  setAndTestRuntimeArgs(programId,runtimeArguments);
  startProgram(programId,200);
  waitState(programId,""String_Node_Str"");
  List<RunRecord> historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(historyRuns.size() == 1);
  String runId=historyRuns.get(0).getPid();
  while (!(oneActionFile.exists() && anotherActionFile.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,2);
  stopProgram(programId,200);
  response=getWorkflowCurrentStatus(programId,runId);
  Assert.assertEquals(404,response.getStatusLine().getStatusCode());
  verifyProgramRuns(programId,""String_Node_Str"");
  oneActionFile.delete();
  anotherActionFile.delete();
  startProgram(programId,200);
  waitState(programId,""String_Node_Str"");
  historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(historyRuns.size() == 1);
  runId=historyRuns.get(0).getPid();
  while (!(oneActionFile.exists() && anotherActionFile.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,2);
  doneFile.createNewFile();
  verifyProgramRuns(programId,""String_Node_Str"");
}","@Test public void testWorkflowForkApp() throws Exception {
  String workflowAppWithFork=""String_Node_Str"";
  String workflowWithFork=""String_Node_Str"";
  Map<String,String> runtimeArgs=Maps.newHashMap();
  File firstSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File firstSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",firstSimpleActionFile.getAbsolutePath());
  runtimeArgs.put(""String_Node_Str"",firstSimpleActionDoneFile.getAbsolutePath());
  File oneSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File oneSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",oneSimpleActionFile.getAbsolutePath());
  runtimeArgs.put(""String_Node_Str"",oneSimpleActionDoneFile.getAbsolutePath());
  File anotherSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File anotherSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",anotherSimpleActionFile.getAbsolutePath());
  runtimeArgs.put(""String_Node_Str"",anotherSimpleActionDoneFile.getAbsolutePath());
  HttpResponse response=deploy(WorkflowAppWithFork.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,workflowAppWithFork,ProgramType.WORKFLOW,workflowWithFork);
  setAndTestRuntimeArgs(programId,runtimeArgs);
  startProgram(programId,200);
  waitState(programId,""String_Node_Str"");
  List<RunRecord> historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(historyRuns.size() == 1);
  RunRecord record=historyRuns.get(0);
  String runId=record.getPid();
  while (!firstSimpleActionFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,1);
  stopProgram(programId,200);
  verifyProgramRuns(programId,""String_Node_Str"");
  firstSimpleActionFile.delete();
  startProgram(programId,200);
  waitState(programId,""String_Node_Str"");
  historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(historyRuns.size() == 1);
  record=historyRuns.get(0);
  Assert.assertTrue(!runId.equals(record.getPid()));
  runId=record.getPid();
  while (!firstSimpleActionFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,1);
  firstSimpleActionDoneFile.createNewFile();
  while (!(oneSimpleActionFile.exists() && anotherSimpleActionFile.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,2);
  stopProgram(programId,200);
  response=getWorkflowCurrentStatus(programId,runId);
  Assert.assertEquals(404,response.getStatusLine().getStatusCode());
  verifyProgramRuns(programId,""String_Node_Str"",1);
  firstSimpleActionFile.delete();
  firstSimpleActionDoneFile.delete();
  oneSimpleActionFile.delete();
  anotherSimpleActionFile.delete();
  startProgram(programId,200);
  waitState(programId,""String_Node_Str"");
  historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(historyRuns.size() == 1);
  runId=historyRuns.get(0).getPid();
  while (!firstSimpleActionFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,1);
  firstSimpleActionDoneFile.createNewFile();
  while (!(oneSimpleActionFile.exists() && anotherSimpleActionFile.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,2);
  oneSimpleActionDoneFile.createNewFile();
  anotherSimpleActionDoneFile.createNewFile();
  verifyProgramRuns(programId,""String_Node_Str"");
}"
7062,"/** 
 * DO NOT DOCUMENT THIS API
 */
@DELETE @Path(""String_Node_Str"") public void delete(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespace){
  if (!cConf.getBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,Constants.Dangerous.DEFAULT_UNRECOVERABLE_RESET)) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
    return;
  }
  Id.Namespace namespaceId=Id.Namespace.from(namespace);
  try {
    namespaceAdmin.deleteNamespace(namespaceId);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  NotFoundException e) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",namespace));
  }
catch (  NamespaceCannotBeDeletedException e) {
    responder.sendString(HttpResponseStatus.FORBIDDEN,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}","/** 
 * DO NOT DOCUMENT THIS API
 */
@DELETE @Path(""String_Node_Str"") public void delete(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespace){
  if (!cConf.getBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,Constants.Dangerous.DEFAULT_UNRECOVERABLE_RESET)) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
    return;
  }
  Id.Namespace namespaceId=Id.Namespace.from(namespace);
  try {
    namespaceAdmin.deleteNamespace(namespaceId);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  NotFoundException e) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",namespace));
  }
catch (  NamespaceCannotBeDeletedException e) {
    responder.sendString(HttpResponseStatus.CONFLICT,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}"
7063,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
public void createNamespace(NamespaceMeta metadata) throws NamespaceCannotBeCreatedException, NamespaceAlreadyExistsException {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  Id.Namespace namespace=Id.Namespace.from(metadata.getName());
  if (hasNamespace(Id.Namespace.from(metadata.getName()))) {
    throw new NamespaceAlreadyExistsException(namespace);
  }
  try {
    dsFramework.createNamespace(Id.Namespace.from(metadata.getName()));
  }
 catch (  DatasetManagementException e) {
    throw new NamespaceCannotBeCreatedException(namespace,e);
  }
  store.createNamespace(metadata);
}","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
public synchronized void createNamespace(NamespaceMeta metadata) throws NamespaceCannotBeCreatedException, NamespaceAlreadyExistsException {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  Id.Namespace namespace=Id.Namespace.from(metadata.getName());
  if (hasNamespace(Id.Namespace.from(metadata.getName()))) {
    throw new NamespaceAlreadyExistsException(namespace);
  }
  try {
    dsFramework.createNamespace(Id.Namespace.from(metadata.getName()));
  }
 catch (  DatasetManagementException e) {
    throw new NamespaceCannotBeCreatedException(namespace,e);
  }
  store.createNamespace(metadata);
}"
7064,"/** 
 * Deletes the specified namespace
 * @param namespaceId the {@link Id.Namespace} of the specified namespace
 * @throws NamespaceCannotBeDeletedException if the specified namespace cannot be deleted
 * @throws NamespaceNotFoundException if the specified namespace does not exist
 */
public void deleteNamespace(final Id.Namespace namespaceId) throws NamespaceCannotBeDeletedException, NamespaceNotFoundException {
  if (!hasNamespace(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (areProgramsRunning(namespaceId)) {
    throw new NamespaceCannotBeDeletedException(namespaceId,""String_Node_Str"" + namespaceId);
  }
  LOG.info(""String_Node_Str"",namespaceId);
  try {
    preferencesStore.deleteProperties(namespaceId.getId());
    dashboardStore.delete(namespaceId.getId());
    dsFramework.deleteAllInstances(namespaceId);
    dsFramework.deleteAllModules(namespaceId);
    queueAdmin.dropAllInNamespace(namespaceId.getId());
    streamAdmin.dropAllInNamespace(namespaceId);
    scheduler.deleteAllSchedules(namespaceId);
    store.removeAll(namespaceId);
    deleteMetrics(namespaceId);
    if (!Constants.DEFAULT_NAMESPACE_ID.equals(namespaceId)) {
      dsFramework.deleteNamespace(namespaceId);
      store.deleteNamespace(namespaceId);
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
  LOG.info(""String_Node_Str"",namespaceId);
}","/** 
 * Deletes the specified namespace
 * @param namespaceId the {@link Id.Namespace} of the specified namespace
 * @throws NamespaceCannotBeDeletedException if the specified namespace cannot be deleted
 * @throws NamespaceNotFoundException if the specified namespace does not exist
 */
public synchronized void deleteNamespace(final Id.Namespace namespaceId) throws NamespaceCannotBeDeletedException, NamespaceNotFoundException {
  if (!hasNamespace(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (checkProgramsRunning(namespaceId)) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  if (checkAdaptersStarted(namespaceId)) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  LOG.info(""String_Node_Str"",namespaceId);
  try {
    preferencesStore.deleteProperties(namespaceId.getId());
    dashboardStore.delete(namespaceId.getId());
    dsFramework.deleteAllInstances(namespaceId);
    dsFramework.deleteAllModules(namespaceId);
    queueAdmin.dropAllInNamespace(namespaceId.getId());
    streamAdmin.dropAllInNamespace(namespaceId);
    scheduler.deleteAllSchedules(namespaceId);
    applicationLifecycleService.removeAll(namespaceId);
    adapterService.removeAdapters(namespaceId);
    store.removeAll(namespaceId);
    deleteMetrics(namespaceId);
    if (!Constants.DEFAULT_NAMESPACE_ID.equals(namespaceId)) {
      dsFramework.deleteNamespace(namespaceId);
      store.deleteNamespace(namespaceId);
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
  LOG.info(""String_Node_Str"",namespaceId);
}"
7065,"@Override public void deleteDatasets(Id.Namespace namespaceId) throws NamespaceNotFoundException, NamespaceCannotBeDeletedException {
  if (!hasNamespace(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (areProgramsRunning(namespaceId)) {
    throw new NamespaceCannotBeDeletedException(namespaceId,""String_Node_Str"" + namespaceId);
  }
  LOG.info(""String_Node_Str"",namespaceId);
  try {
    dsFramework.deleteAllInstances(namespaceId);
  }
 catch (  DatasetManagementException e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
catch (  IOException e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
  LOG.info(""String_Node_Str"",namespaceId);
}","@Override public synchronized void deleteDatasets(Id.Namespace namespaceId) throws NamespaceNotFoundException, NamespaceCannotBeDeletedException {
  if (!hasNamespace(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (checkProgramsRunning(namespaceId)) {
    throw new NamespaceCannotBeDeletedException(namespaceId,""String_Node_Str"" + namespaceId);
  }
  LOG.info(""String_Node_Str"",namespaceId);
  try {
    dsFramework.deleteAllInstances(namespaceId);
  }
 catch (  DatasetManagementException e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
catch (  IOException e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
  LOG.info(""String_Node_Str"",namespaceId);
}"
7066,"public void updateProperties(Id.Namespace namespaceId,NamespaceMeta namespaceMeta) throws NamespaceNotFoundException {
  if (store.getNamespace(namespaceId) == null) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  NamespaceMeta metadata=store.getNamespace(namespaceId);
  NamespaceMeta.Builder builder=new NamespaceMeta.Builder(metadata);
  if (namespaceMeta.getDescription() != null) {
    builder.setDescription(namespaceMeta.getDescription());
  }
  if (namespaceMeta.getName() != null) {
    builder.setName(namespaceMeta.getName());
  }
  NamespaceConfig config=namespaceMeta.getConfig();
  if (config != null && config.getSchedulerQueueName() != null && !config.getSchedulerQueueName().isEmpty()) {
    builder.setSchedulerQueueName(config.getSchedulerQueueName());
  }
  store.updateNamespace(builder.build());
}","public synchronized void updateProperties(Id.Namespace namespaceId,NamespaceMeta namespaceMeta) throws NamespaceNotFoundException {
  if (store.getNamespace(namespaceId) == null) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  NamespaceMeta metadata=store.getNamespace(namespaceId);
  NamespaceMeta.Builder builder=new NamespaceMeta.Builder(metadata);
  if (namespaceMeta.getDescription() != null) {
    builder.setDescription(namespaceMeta.getDescription());
  }
  if (namespaceMeta.getName() != null) {
    builder.setName(namespaceMeta.getName());
  }
  NamespaceConfig config=namespaceMeta.getConfig();
  if (config != null && config.getSchedulerQueueName() != null && !config.getSchedulerQueueName().isEmpty()) {
    builder.setSchedulerQueueName(config.getSchedulerQueueName());
  }
  store.updateNamespace(builder.build());
}"
7067,"@Inject public DefaultNamespaceAdmin(Store store,PreferencesStore preferencesStore,DashboardStore dashboardStore,DatasetFramework dsFramework,ProgramRuntimeService runtimeService,QueueAdmin queueAdmin,StreamAdmin streamAdmin,MetricStore metricStore,Scheduler scheduler){
  this.queueAdmin=queueAdmin;
  this.streamAdmin=streamAdmin;
  this.store=store;
  this.preferencesStore=preferencesStore;
  this.dashboardStore=dashboardStore;
  this.dsFramework=dsFramework;
  this.runtimeService=runtimeService;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
}","@Inject public DefaultNamespaceAdmin(Store store,PreferencesStore preferencesStore,DashboardStore dashboardStore,DatasetFramework dsFramework,ProgramRuntimeService runtimeService,QueueAdmin queueAdmin,StreamAdmin streamAdmin,MetricStore metricStore,Scheduler scheduler,ApplicationLifecycleService applicationLifecycleService,AdapterService adapterService){
  this.queueAdmin=queueAdmin;
  this.streamAdmin=streamAdmin;
  this.store=store;
  this.preferencesStore=preferencesStore;
  this.dashboardStore=dashboardStore;
  this.dsFramework=dsFramework;
  this.runtimeService=runtimeService;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
  this.applicationLifecycleService=applicationLifecycleService;
  this.adapterService=adapterService;
}"
7068,"@VisibleForTesting void registerTemplates(){
  try {
    Map<String,ApplicationTemplateInfo> newInfoMap=Maps.newHashMap();
    Map<File,ApplicationTemplateInfo> newFileTemplateMap=Maps.newHashMap();
    File baseDir=new File(configuration.get(Constants.AppFabric.APP_TEMPLATE_DIR));
    List<File> files=DirUtils.listFiles(baseDir,""String_Node_Str"");
    for (    File file : files) {
      try {
        ApplicationTemplateInfo info=getTemplateInfo(file);
        newInfoMap.put(info.getName(),info);
        newFileTemplateMap.put(info.getFile().getAbsoluteFile(),info);
      }
 catch (      IllegalArgumentException e) {
        LOG.error(""String_Node_Str"",file.getName(),e);
      }
    }
    appTemplateInfos.set(newInfoMap);
    fileToTemplateMap.set(newFileTemplateMap);
    pluginRepository.inspectPlugins(newInfoMap.values());
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"");
  }
}","@VisibleForTesting public void registerTemplates(){
  try {
    Map<String,ApplicationTemplateInfo> newInfoMap=Maps.newHashMap();
    Map<File,ApplicationTemplateInfo> newFileTemplateMap=Maps.newHashMap();
    File baseDir=new File(configuration.get(Constants.AppFabric.APP_TEMPLATE_DIR));
    List<File> files=DirUtils.listFiles(baseDir,""String_Node_Str"");
    for (    File file : files) {
      try {
        ApplicationTemplateInfo info=getTemplateInfo(file);
        newInfoMap.put(info.getName(),info);
        newFileTemplateMap.put(info.getFile().getAbsoluteFile(),info);
      }
 catch (      IllegalArgumentException e) {
        LOG.error(""String_Node_Str"",file.getName(),e);
      }
    }
    appTemplateInfos.set(newInfoMap);
    fileToTemplateMap.set(newFileTemplateMap);
    pluginRepository.inspectPlugins(newInfoMap.values());
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"");
  }
}"
7069,"@BeforeClass public static void setup() throws Exception {
  CConfiguration conf=getInjector().getInstance(CConfiguration.class);
  locationFactory=getInjector().getInstance(LocationFactory.class);
  adapterDir=new File(conf.get(Constants.AppFabric.APP_TEMPLATE_DIR));
  setupAdapter(DummyBatchTemplate.class);
  setupAdapter(DummyWorkerTemplate.class);
  adapterService=getInjector().getInstance(AdapterService.class);
  adapterService.registerTemplates();
}","@BeforeClass public static void setup() throws Exception {
  setupAdapter(DummyBatchTemplate.class);
  setupAdapter(DummyWorkerTemplate.class);
  adapterService=getInjector().getInstance(AdapterService.class);
  adapterService.registerTemplates();
}"
7070,"@Test public void testBatchAdapters() throws Exception {
  String adapterName=""String_Node_Str"";
  DummyBatchTemplate.Config config=new DummyBatchTemplate.Config(""String_Node_Str"",""String_Node_Str"");
  AdapterConfig adapterConfig=new AdapterConfig(""String_Node_Str"",DummyBatchTemplate.NAME,GSON.toJsonTree(config));
  adapterService.createAdapter(NAMESPACE,adapterName,adapterConfig);
  PreferencesStore preferencesStore=getInjector().getInstance(PreferencesStore.class);
  Map<String,String> prop=preferencesStore.getResolvedProperties(TEST_NAMESPACE1,adapterConfig.getTemplate());
  Assert.assertTrue(Boolean.parseBoolean(prop.get(ProgramOptionConstants.CONCURRENT_RUNS_ENABLED)));
  try {
    adapterService.createAdapter(NAMESPACE,adapterName,adapterConfig);
    Assert.fail(""String_Node_Str"");
  }
 catch (  AdapterAlreadyExistsException expected) {
  }
  AdapterDefinition actualAdapterSpec=adapterService.getAdapter(NAMESPACE,adapterName);
  Assert.assertNotNull(actualAdapterSpec);
  assertDummyConfigEquals(adapterConfig,actualAdapterSpec);
  Collection<AdapterDefinition> adapters=adapterService.getAdapters(NAMESPACE,DummyBatchTemplate.NAME);
  Assert.assertEquals(1,adapters.size());
  AdapterDefinition actual=adapters.iterator().next();
  assertDummyConfigEquals(adapterConfig,actual);
  Assert.assertEquals(AdapterStatus.STOPPED,adapterService.getAdapterStatus(NAMESPACE,adapterName));
  adapterService.startAdapter(NAMESPACE,adapterName);
  Assert.assertEquals(AdapterStatus.STARTED,adapterService.getAdapterStatus(NAMESPACE,adapterName));
  adapterService.stopAdapter(NAMESPACE,adapterName);
  Assert.assertEquals(AdapterStatus.STOPPED,adapterService.getAdapterStatus(NAMESPACE,adapterName));
  adapterService.removeAdapter(NAMESPACE,adapterName);
  try {
    adapterService.getAdapter(NAMESPACE,adapterName);
    Assert.fail(String.format(""String_Node_Str"",adapterName));
  }
 catch (  AdapterNotFoundException expected) {
  }
  adapters=adapterService.getAdapters(NAMESPACE,DummyBatchTemplate.NAME);
  Assert.assertTrue(adapters.isEmpty());
}","@Test public void testBatchAdapters() throws Exception {
  String adapterName=""String_Node_Str"";
  DummyBatchTemplate.Config config=new DummyBatchTemplate.Config(""String_Node_Str"",""String_Node_Str"");
  AdapterConfig adapterConfig=new AdapterConfig(""String_Node_Str"",DummyBatchTemplate.NAME,GSON.toJsonTree(config));
  adapterService.createAdapter(NAMESPACE,adapterName,adapterConfig);
  PreferencesStore preferencesStore=getInjector().getInstance(PreferencesStore.class);
  Map<String,String> prop=preferencesStore.getResolvedProperties(TEST_NAMESPACE1,adapterConfig.getTemplate());
  Assert.assertTrue(Boolean.parseBoolean(prop.get(ProgramOptionConstants.CONCURRENT_RUNS_ENABLED)));
  try {
    adapterService.createAdapter(NAMESPACE,adapterName,adapterConfig);
    Assert.fail(""String_Node_Str"");
  }
 catch (  AdapterAlreadyExistsException expected) {
  }
  AdapterDefinition actualAdapterSpec=adapterService.getAdapter(NAMESPACE,adapterName);
  Assert.assertNotNull(actualAdapterSpec);
  assertDummyConfigEquals(adapterConfig,actualAdapterSpec);
  Collection<AdapterDefinition> adapters=adapterService.getAdapters(NAMESPACE,DummyBatchTemplate.NAME);
  Assert.assertEquals(1,adapters.size());
  AdapterDefinition actual=adapters.iterator().next();
  assertDummyConfigEquals(adapterConfig,actual);
  Assert.assertEquals(AdapterStatus.STOPPED,adapterService.getAdapterStatus(NAMESPACE,adapterName));
  adapterService.startAdapter(NAMESPACE,adapterName);
  Assert.assertEquals(AdapterStatus.STARTED,adapterService.getAdapterStatus(NAMESPACE,adapterName));
  adapterService.stopAdapter(NAMESPACE,adapterName);
  Assert.assertEquals(AdapterStatus.STOPPED,adapterService.getAdapterStatus(NAMESPACE,adapterName));
  adapterService.removeAdapters(NAMESPACE);
  try {
    adapterService.getAdapter(NAMESPACE,adapterName);
    Assert.fail(String.format(""String_Node_Str"",adapterName));
  }
 catch (  AdapterNotFoundException expected) {
  }
  adapters=adapterService.getAdapters(NAMESPACE,DummyBatchTemplate.NAME);
  Assert.assertTrue(adapters.isEmpty());
}"
7071,"@BeforeClass public static void setup() throws Exception {
  CConfiguration conf=getInjector().getInstance(CConfiguration.class);
  locationFactory=getInjector().getInstance(LocationFactory.class);
  adapterDir=new File(conf.get(Constants.AppFabric.APP_TEMPLATE_DIR));
  setupAdapters();
  adapterService=getInjector().getInstance(AdapterService.class);
  adapterService.registerTemplates();
}","@BeforeClass public static void setup() throws Exception {
  setupAdapters();
  adapterService=getInjector().getInstance(AdapterService.class);
  adapterService.registerTemplates();
}"
7072,"private static void setupMetrics() throws Exception {
  MetricsCollector collector=collectionService.getCollector(getFlowletContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(getFlowletContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(getFlowletContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  emitTs=System.currentTimeMillis();
  TimeUnit.SECONDS.sleep(1);
  collector.increment(""String_Node_Str"",1);
  TimeUnit.MILLISECONDS.sleep(2000);
  collector.increment(""String_Node_Str"",2);
  collector=collectionService.getCollector(getFlowletContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(getMapReduceTaskContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MapReduceMetrics.TaskType.Mapper,""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(getMapReduceTaskContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MapReduceMetrics.TaskType.Reducer,""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(getFlowletContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(getFlowletContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(getAdapterContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MapReduceMetrics.TaskType.Mapper,""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",3);
  collector.increment(""String_Node_Str"",4);
  collector=collectionService.getCollector(getWorkerAdapterContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",5);
  collector.increment(""String_Node_Str"",6);
  Metrics userMetrics=new ProgramUserMetrics(collectionService.getCollector(getFlowletContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  userMetrics.count(""String_Node_Str"",1);
  userMetrics.count(""String_Node_Str"",2);
  collector=collectionService.getCollector(new HashMap<String,String>());
  collector.increment(""String_Node_Str"",10);
  collector=collectionService.getCollector(getFlowletContext(DOT_NAMESPACE,DOT_APP,DOT_FLOW,DOT_RUN,DOT_FLOWLET));
  collector.increment(""String_Node_Str"",55);
  TimeUnit.SECONDS.sleep(2);
}","private static void setupMetrics() throws Exception {
  MetricsCollector collector=collectionService.getCollector(getFlowletContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(getFlowletContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(getFlowletContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  emitTs=System.currentTimeMillis();
  TimeUnit.SECONDS.sleep(1);
  collector.increment(""String_Node_Str"",1);
  TimeUnit.MILLISECONDS.sleep(2000);
  collector.increment(""String_Node_Str"",2);
  collector=collectionService.getCollector(getFlowletContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(getMapReduceTaskContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MapReduceMetrics.TaskType.Mapper,""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(getMapReduceTaskContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MapReduceMetrics.TaskType.Reducer,""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(getFlowletContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(getFlowletContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(getAdapterContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MapReduceMetrics.TaskType.Mapper,""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",3);
  collector.increment(""String_Node_Str"",4);
  collector=collectionService.getCollector(getAdapterContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",MapReduceMetrics.TaskType.Mapper,""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",3);
  collector.increment(""String_Node_Str"",4);
  collector=collectionService.getCollector(getWorkerAdapterContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",5);
  collector.increment(""String_Node_Str"",6);
  collector=collectionService.getCollector(getWorkerAdapterContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  collector.increment(""String_Node_Str"",5);
  collector.increment(""String_Node_Str"",6);
  Metrics userMetrics=new ProgramUserMetrics(collectionService.getCollector(getFlowletContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  userMetrics.count(""String_Node_Str"",1);
  userMetrics.count(""String_Node_Str"",2);
  collector=collectionService.getCollector(new HashMap<String,String>());
  collector.increment(""String_Node_Str"",10);
  collector=collectionService.getCollector(getFlowletContext(DOT_NAMESPACE,DOT_APP,DOT_FLOW,DOT_RUN,DOT_FLOWLET));
  collector.increment(""String_Node_Str"",55);
  TimeUnit.SECONDS.sleep(2);
}"
7073,"@Test public void testSearchContext() throws Exception {
  verifySearchResultContains(""String_Node_Str"",ImmutableList.of(""String_Node_Str"" + DOT_NAMESPACE_ESCAPED,""String_Node_Str"",""String_Node_Str""));
  verifySearchResult(""String_Node_Str"",ImmutableList.of(""String_Node_Str""));
  verifySearchResult(""String_Node_Str"",ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResult(""String_Node_Str"",ImmutableList.of(""String_Node_Str""));
  verifySearchResult(""String_Node_Str"",ImmutableList.<String>of());
  verifySearchResult(""String_Node_Str"",ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResult(""String_Node_Str"",ImmutableList.<String>of());
  verifySearchResult(""String_Node_Str"",ImmutableList.<String>of());
  verifySearchResult(""String_Node_Str"" + ""String_Node_Str"",ImmutableList.of(""String_Node_Str"" + ""String_Node_Str""));
  verifySearchResult(""String_Node_Str"" + ""String_Node_Str"",ImmutableList.of(""String_Node_Str"" + ""String_Node_Str"",""String_Node_Str"" + ""String_Node_Str""));
  verifySearchResult(""String_Node_Str"" + ""String_Node_Str"",ImmutableList.of(""String_Node_Str"" + ""String_Node_Str""));
  verifySearchResult(""String_Node_Str"" + ""String_Node_Str"",ImmutableList.<String>of());
  verifySearchResultContains(""String_Node_Str"",ImmutableList.of(""String_Node_Str"",""String_Node_Str""));
  verifySearchResult(""String_Node_Str"",ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"" + DOT_FLOW_ESCAPED,""String_Node_Str"",""String_Node_Str""));
  verifySearchResult(""String_Node_Str"",ImmutableList.of(""String_Node_Str""));
  String parts[]=new String[]{""String_Node_Str"" + DOT_NAMESPACE_ESCAPED,""String_Node_Str"" + DOT_APP_ESCAPED,""String_Node_Str"" + DOT_FLOW_ESCAPED,""String_Node_Str"" + DOT_RUN_ESCAPED,""String_Node_Str"" + DOT_FLOWLET_ESCAPED};
  String context=parts[0];
  int i=1;
  do {
    String contextNext=context + ""String_Node_Str"" + parts[i];
    verifySearchResult(""String_Node_Str"" + context,ImmutableList.<String>of(contextNext));
    context=contextNext;
    i++;
  }
 while (i < parts.length);
}","@Test public void testSearchContext() throws Exception {
  verifySearchResultContains(""String_Node_Str"",ImmutableList.of(""String_Node_Str"" + DOT_NAMESPACE_ESCAPED,""String_Node_Str"",""String_Node_Str""));
  verifySearchResult(""String_Node_Str"",ImmutableList.of(""String_Node_Str""));
  verifySearchResult(""String_Node_Str"",ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResult(""String_Node_Str"",ImmutableList.of(""String_Node_Str""));
  verifySearchResult(""String_Node_Str"",ImmutableList.<String>of());
  verifySearchResult(""String_Node_Str"",ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResult(""String_Node_Str"",ImmutableList.<String>of());
  verifySearchResult(""String_Node_Str"",ImmutableList.<String>of(""String_Node_Str"",""String_Node_Str""));
  verifySearchResult(""String_Node_Str"" + ""String_Node_Str"",ImmutableList.of(""String_Node_Str"",""String_Node_Str""));
  verifySearchResult(""String_Node_Str"" + ""String_Node_Str"",ImmutableList.of(""String_Node_Str"" + ""String_Node_Str"",""String_Node_Str"" + ""String_Node_Str""));
  verifySearchResult(""String_Node_Str"" + ""String_Node_Str"",ImmutableList.of(""String_Node_Str"" + ""String_Node_Str""));
  verifySearchResult(""String_Node_Str"" + ""String_Node_Str"",ImmutableList.<String>of());
  verifySearchResultContains(""String_Node_Str"",ImmutableList.of(""String_Node_Str"",""String_Node_Str""));
  verifySearchResult(""String_Node_Str"",ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"" + DOT_FLOW_ESCAPED,""String_Node_Str"",""String_Node_Str""));
  verifySearchResult(""String_Node_Str"",ImmutableList.of(""String_Node_Str""));
  String parts[]=new String[]{""String_Node_Str"" + DOT_NAMESPACE_ESCAPED,""String_Node_Str"" + DOT_APP_ESCAPED,""String_Node_Str"" + DOT_FLOW_ESCAPED,""String_Node_Str"" + DOT_RUN_ESCAPED,""String_Node_Str"" + DOT_FLOWLET_ESCAPED};
  String context=parts[0];
  int i=1;
  do {
    String contextNext=context + ""String_Node_Str"" + parts[i];
    verifySearchResult(""String_Node_Str"" + context,ImmutableList.<String>of(contextNext));
    context=contextNext;
    i++;
  }
 while (i < parts.length);
}"
7074,"@Test public void testSearchWithTags() throws Exception {
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",DOT_NAMESPACE,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected());
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected());
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected());
  verifySearchResultWithTags(""String_Node_Str"" + ""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"" + ""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"" + ""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"" + ""String_Node_Str"",getSearchResultExpected());
  verifySearchResultWithTags(""String_Node_Str"" + ""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"" + ""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",DOT_APP,""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",DOT_FLOW,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",DOT_RUN));
  String parts[]=new String[]{""String_Node_Str"" + DOT_NAMESPACE,""String_Node_Str"" + DOT_APP,""String_Node_Str"" + DOT_FLOW,""String_Node_Str"" + DOT_RUN,""String_Node_Str"" + DOT_FLOWLET};
  String context=""String_Node_Str"" + parts[0];
  int i=1;
  do {
    String contextNext=context + ""String_Node_Str"" + parts[i];
    verifySearchResultWithTags(""String_Node_Str"" + context,getSearchResultExpected(parts[i].split(""String_Node_Str"",2)));
    context=contextNext;
    i++;
  }
 while (i < parts.length);
}","@Test public void testSearchWithTags() throws Exception {
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",DOT_NAMESPACE,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected());
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected());
  verifySearchResultWithTags(""String_Node_Str"" + ""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"" + ""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"" + ""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"" + ""String_Node_Str"",getSearchResultExpected());
  verifySearchResultWithTags(""String_Node_Str"" + ""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"" + ""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",DOT_APP,""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",DOT_FLOW,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  verifySearchResultWithTags(""String_Node_Str"",getSearchResultExpected(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",DOT_RUN));
  String parts[]=new String[]{""String_Node_Str"" + DOT_NAMESPACE,""String_Node_Str"" + DOT_APP,""String_Node_Str"" + DOT_FLOW,""String_Node_Str"" + DOT_RUN,""String_Node_Str"" + DOT_FLOWLET};
  String context=""String_Node_Str"" + parts[0];
  int i=1;
  do {
    String contextNext=context + ""String_Node_Str"" + parts[i];
    verifySearchResultWithTags(""String_Node_Str"" + context,getSearchResultExpected(parts[i].split(""String_Node_Str"",2)));
    context=contextNext;
    i++;
  }
 while (i < parts.length);
}"
7075,"@Test public void testQueryMetricsWithTags() throws Exception {
  verifyAggregateQueryResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",1);
  verifyAggregateQueryResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"",5);
  verifyAggregateQueryResult(""String_Node_Str"",6);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",5);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",6);
  verifyEmptyQueryResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"");
  verifyAggregateQueryResult(""String_Node_Str"" + getTags(DOT_NAMESPACE,DOT_APP,DOT_FLOW,DOT_FLOWLET) + ""String_Node_Str"",55);
  verifyRangeQueryResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",2,3);
  long start=(emitTs - 60 * 1000) / 1000;
  long end=(emitTs + 60 * 1000) / 1000;
  verifyRangeQueryResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end,2,3);
  verifyEmptyQueryResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end);
  List<TimeSeriesResult> groupByResult=ImmutableList.of(new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),1),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),3));
  verifyGroupByResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end,groupByResult);
  groupByResult=ImmutableList.of(new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),2),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),1),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),4));
  verifyGroupByResult(""String_Node_Str"" + ""String_Node_Str"" + start + ""String_Node_Str""+ end,groupByResult);
}","@Test public void testQueryMetricsWithTags() throws Exception {
  verifyAggregateQueryResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",1);
  verifyAggregateQueryResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"",6);
  verifyAggregateQueryResult(""String_Node_Str"",8);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",6);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",8);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",5);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",6);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",5);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",6);
  verifyAggregateQueryResult(""String_Node_Str"",10);
  verifyAggregateQueryResult(""String_Node_Str"",12);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",5);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",6);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",5);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",6);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",10);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",12);
  verifyEmptyQueryResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"");
  verifyAggregateQueryResult(""String_Node_Str"" + getTags(DOT_NAMESPACE,DOT_APP,DOT_FLOW,DOT_FLOWLET) + ""String_Node_Str"",55);
  verifyRangeQueryResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",2,3);
  long start=(emitTs - 60 * 1000) / 1000;
  long end=(emitTs + 60 * 1000) / 1000;
  verifyRangeQueryResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end,2,3);
  verifyEmptyQueryResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end);
  List<TimeSeriesResult> groupByResult=ImmutableList.of(new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),1),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),3));
  verifyGroupByResult(""String_Node_Str"" + getTags(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end,groupByResult);
  groupByResult=ImmutableList.of(new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),2),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),1),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),4));
  verifyGroupByResult(""String_Node_Str"" + ""String_Node_Str"" + start + ""String_Node_Str""+ end,groupByResult);
}"
7076,"@Test public void testQueryMetrics() throws Exception {
  verifyAggregateQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",1);
  verifyAggregateQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"",5);
  verifyAggregateQueryResult(""String_Node_Str"",6);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",5);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",6);
  verifyEmptyQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"");
  verifyAggregateQueryResult(""String_Node_Str"" + getContext(DOT_NAMESPACE_ESCAPED,DOT_APP_ESCAPED,DOT_FLOW_ESCAPED,DOT_FLOWLET_ESCAPED) + ""String_Node_Str"",55);
  verifyRangeQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",2,3);
  long start=(emitTs - 60 * 1000) / 1000;
  long end=(emitTs + 60 * 1000) / 1000;
  verifyRangeQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end,2,3);
  verifyEmptyQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end);
  List<TimeSeriesResult> groupByResult=ImmutableList.of(new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),1),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),3));
  verifyGroupByResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end,groupByResult);
  groupByResult=ImmutableList.of(new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),2),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),1),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),4));
  verifyGroupByResult(""String_Node_Str"" + ""String_Node_Str"" + start + ""String_Node_Str""+ end,groupByResult);
}","@Test public void testQueryMetrics() throws Exception {
  verifyAggregateQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",1);
  verifyAggregateQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"",6);
  verifyAggregateQueryResult(""String_Node_Str"",8);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",4);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",6);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",8);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",5);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",6);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",5);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",6);
  verifyAggregateQueryResult(""String_Node_Str"",10);
  verifyAggregateQueryResult(""String_Node_Str"",12);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",5);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",6);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",5);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",6);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",10);
  verifyAggregateQueryResult(""String_Node_Str"" + ""String_Node_Str"",12);
  verifyEmptyQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"");
  verifyAggregateQueryResult(""String_Node_Str"" + getContext(DOT_NAMESPACE_ESCAPED,DOT_APP_ESCAPED,DOT_FLOW_ESCAPED,DOT_FLOWLET_ESCAPED) + ""String_Node_Str"",55);
  verifyRangeQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",2,3);
  long start=(emitTs - 60 * 1000) / 1000;
  long end=(emitTs + 60 * 1000) / 1000;
  verifyRangeQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end,2,3);
  verifyEmptyQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end);
  List<TimeSeriesResult> groupByResult=ImmutableList.of(new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),1),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),3));
  verifyGroupByResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end,groupByResult);
  groupByResult=ImmutableList.of(new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),2),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),1),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),4));
  verifyGroupByResult(""String_Node_Str"" + ""String_Node_Str"" + start + ""String_Node_Str""+ end,groupByResult);
}"
7077,"private static Map<String,Aggregation> createAggregations(){
  Map<String,Aggregation> aggs=Maps.newHashMap();
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.DATASET),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.FLOW,Constants.Metrics.Tag.DATASET,Constants.Metrics.Tag.RUN_ID,Constants.Metrics.Tag.FLOWLET,Constants.Metrics.Tag.INSTANCE_ID,Constants.Metrics.Tag.FLOWLET_QUEUE),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.FLOW)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.FLOW,Constants.Metrics.Tag.CONSUMER,Constants.Metrics.Tag.PRODUCER,Constants.Metrics.Tag.FLOWLET_QUEUE)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.MAPREDUCE,Constants.Metrics.Tag.DATASET,Constants.Metrics.Tag.RUN_ID,Constants.Metrics.Tag.MR_TASK_TYPE,Constants.Metrics.Tag.INSTANCE_ID),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.MAPREDUCE)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.SERVICE,Constants.Metrics.Tag.DATASET,Constants.Metrics.Tag.RUN_ID,Constants.Metrics.Tag.HANDLER,Constants.Metrics.Tag.METHOD,Constants.Metrics.Tag.INSTANCE_ID),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.SERVICE)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.WORKER,Constants.Metrics.Tag.DATASET,Constants.Metrics.Tag.RUN_ID,Constants.Metrics.Tag.INSTANCE_ID),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.WORKER)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.WORKFLOW,Constants.Metrics.Tag.DATASET,Constants.Metrics.Tag.RUN_ID),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.WORKFLOW)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.SPARK,Constants.Metrics.Tag.DATASET,Constants.Metrics.Tag.RUN_ID),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.SPARK)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.ADAPTER),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.ADAPTER)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.STREAM),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.STREAM)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.DATASET),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.DATASET)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.COMPONENT,Constants.Metrics.Tag.HANDLER,Constants.Metrics.Tag.METHOD),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.COMPONENT)));
  return aggs;
}","private static Map<String,Aggregation> createAggregations(){
  Map<String,Aggregation> aggs=Maps.newHashMap();
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.DATASET),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.FLOW,Constants.Metrics.Tag.DATASET,Constants.Metrics.Tag.RUN_ID,Constants.Metrics.Tag.FLOWLET,Constants.Metrics.Tag.INSTANCE_ID,Constants.Metrics.Tag.FLOWLET_QUEUE),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.FLOW)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.FLOW,Constants.Metrics.Tag.CONSUMER,Constants.Metrics.Tag.PRODUCER,Constants.Metrics.Tag.FLOWLET_QUEUE)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.MAPREDUCE,Constants.Metrics.Tag.DATASET,Constants.Metrics.Tag.RUN_ID,Constants.Metrics.Tag.MR_TASK_TYPE,Constants.Metrics.Tag.INSTANCE_ID),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.MAPREDUCE)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.SERVICE,Constants.Metrics.Tag.DATASET,Constants.Metrics.Tag.RUN_ID,Constants.Metrics.Tag.HANDLER,Constants.Metrics.Tag.METHOD,Constants.Metrics.Tag.INSTANCE_ID),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.SERVICE)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.WORKER,Constants.Metrics.Tag.DATASET,Constants.Metrics.Tag.RUN_ID,Constants.Metrics.Tag.INSTANCE_ID),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.WORKER)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.WORKFLOW,Constants.Metrics.Tag.DATASET,Constants.Metrics.Tag.RUN_ID),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.WORKFLOW)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.SPARK,Constants.Metrics.Tag.DATASET,Constants.Metrics.Tag.RUN_ID),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.APP,Constants.Metrics.Tag.SPARK)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.ADAPTER,Constants.Metrics.Tag.RUN_ID),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.ADAPTER)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.STREAM),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.STREAM)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.DATASET),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.DATASET)));
  aggs.put(""String_Node_Str"",new DefaultAggregation(ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.COMPONENT,Constants.Metrics.Tag.HANDLER,Constants.Metrics.Tag.METHOD),ImmutableList.of(Constants.Metrics.Tag.NAMESPACE,Constants.Metrics.Tag.COMPONENT)));
  return aggs;
}"
7078,"@Override public Module getStandaloneModules(){
  return Modules.combine(new AppFabricServiceModule(StreamHandler.class,StreamFetchHandler.class),new ConfigStoreModule().getStandaloneModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(AppFabricServer.class).to(StandaloneAppFabricServer.class).in(Scopes.SINGLETON);
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      MapBinder<String,MasterServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,MasterServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(InMemoryLogSaverServiceManager.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(InMemoryTransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(InMemoryMetricsProcessorServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(InMemoryMetricsServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(AppFabricServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(InMemoryStreamServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(InMemoryDatasetExecutorServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(InMemoryExploreServiceManager.class);
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      servicesNamesBinder.addBinding().toInstance(Constants.Service.STREAMS);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      handlerHookNamesBinder.addBinding().toInstance(Constants.Stream.STREAM_HANDLER);
    }
  }
);
}","@Override public Module getStandaloneModules(){
  return Modules.combine(new AppFabricServiceModule(StreamHandler.class,StreamFetchHandler.class),new ConfigStoreModule().getStandaloneModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(AppFabricServer.class).to(StandaloneAppFabricServer.class).in(Scopes.SINGLETON);
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      bind(MRJobInfoFetcher.class).to(LocalMRJobInfoFetcher.class);
      MapBinder<String,MasterServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,MasterServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(InMemoryLogSaverServiceManager.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(InMemoryTransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(InMemoryMetricsProcessorServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(InMemoryMetricsServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(AppFabricServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(InMemoryStreamServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(InMemoryDatasetExecutorServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(InMemoryExploreServiceManager.class);
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      servicesNamesBinder.addBinding().toInstance(Constants.Service.STREAMS);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      handlerHookNamesBinder.addBinding().toInstance(Constants.Stream.STREAM_HANDLER);
    }
  }
);
}"
7079,"@Override public Module getDistributedModules(){
  return Modules.combine(new AppFabricServiceModule(),new ConfigStoreModule().getDistributedModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(DistributedSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      MapBinder<String,MasterServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,MasterServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(LogSaverStatusServiceManager.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(TransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(MetricsProcessorStatusServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(MetricsServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(AppFabricServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(StreamServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(DatasetExecutorServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(ExploreServiceManager.class);
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
    }
  }
);
}","@Override public Module getDistributedModules(){
  return Modules.combine(new AppFabricServiceModule(),new ConfigStoreModule().getDistributedModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(DistributedSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      bind(MRJobInfoFetcher.class).to(DistributedMRJobInfoFetcher.class);
      MapBinder<String,MasterServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,MasterServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(LogSaverStatusServiceManager.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(TransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(MetricsProcessorStatusServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(MetricsServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(AppFabricServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(StreamServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(DatasetExecutorServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(ExploreServiceManager.class);
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
    }
  }
);
}"
7080,"@Override public Module getInMemoryModules(){
  return Modules.combine(new AppFabricServiceModule(StreamHandler.class,StreamFetchHandler.class),new ConfigStoreModule().getInMemoryModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      MapBinder<String,MasterServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,MasterServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(InMemoryLogSaverServiceManager.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(InMemoryTransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(InMemoryMetricsProcessorServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(InMemoryMetricsServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(AppFabricServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(InMemoryStreamServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(InMemoryDatasetExecutorServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(InMemoryExploreServiceManager.class);
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      servicesNamesBinder.addBinding().toInstance(Constants.Service.STREAMS);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      handlerHookNamesBinder.addBinding().toInstance(Constants.Stream.STREAM_HANDLER);
    }
  }
);
}","@Override public Module getInMemoryModules(){
  return Modules.combine(new AppFabricServiceModule(StreamHandler.class,StreamFetchHandler.class),new ConfigStoreModule().getInMemoryModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      bind(MRJobInfoFetcher.class).to(LocalMRJobInfoFetcher.class);
      MapBinder<String,MasterServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,MasterServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(InMemoryLogSaverServiceManager.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(InMemoryTransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(InMemoryMetricsProcessorServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(InMemoryMetricsServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(AppFabricServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(InMemoryStreamServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(InMemoryDatasetExecutorServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(InMemoryExploreServiceManager.class);
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      servicesNamesBinder.addBinding().toInstance(Constants.Service.STREAMS);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      handlerHookNamesBinder.addBinding().toInstance(Constants.Stream.STREAM_HANDLER);
    }
  }
);
}"
7081,"/** 
 * Relays job-level and task-level information about a particular MapReduce program run.
 */
@GET @Path(""String_Node_Str"") public void getMapReduceInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String mapreduceId,@PathParam(""String_Node_Str"") String runId){
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.MAPREDUCE,mapreduceId);
    Id.Run run=new Id.Run(programId,runId);
    ApplicationSpecification appSpec=store.getApplication(programId.getApplication());
    if (appSpec == null) {
      throw new NotFoundException(programId.getApplication());
    }
    if (!appSpec.getMapReduce().containsKey(mapreduceId)) {
      throw new NotFoundException(programId);
    }
    RunRecord runRecord=store.getRun(programId,runId);
    if (runRecord == null) {
      throw new NotFoundException(run);
    }
    MRJobInfo mrJobInfo;
    try {
      mrJobInfo=mrJobClient.getMRJobInfo(run);
    }
 catch (    IOException ioe) {
      LOG.debug(""String_Node_Str"",run,ioe);
      mrJobInfo=mapReduceMetricsInfo.getMRJobInfo(run);
    }
catch (    NotFoundException nfe) {
      LOG.debug(""String_Node_Str"",run,nfe);
      mrJobInfo=mapReduceMetricsInfo.getMRJobInfo(run);
    }
    mrJobInfo.setState(runRecord.getStatus().name());
    mrJobInfo.setStartTime(TimeUnit.SECONDS.toMillis(runRecord.getStartTs()));
    Long stopTs=runRecord.getStopTs();
    if (stopTs != null) {
      mrJobInfo.setStopTime(TimeUnit.SECONDS.toMillis(stopTs));
    }
    responder.sendJson(HttpResponseStatus.OK,mrJobInfo);
  }
 catch (  NotFoundException e) {
    LOG.warn(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",runId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}","/** 
 * Relays job-level and task-level information about a particular MapReduce program run.
 */
@GET @Path(""String_Node_Str"") public void getMapReduceInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String mapreduceId,@PathParam(""String_Node_Str"") String runId){
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.MAPREDUCE,mapreduceId);
    Id.Run run=new Id.Run(programId,runId);
    ApplicationSpecification appSpec=store.getApplication(programId.getApplication());
    if (appSpec == null) {
      throw new NotFoundException(programId.getApplication());
    }
    if (!appSpec.getMapReduce().containsKey(mapreduceId)) {
      throw new NotFoundException(programId);
    }
    RunRecord runRecord=store.getRun(programId,runId);
    if (runRecord == null) {
      throw new NotFoundException(run);
    }
    MRJobInfo mrJobInfo=mrJobInfoFetcher.getMRJobInfo(run);
    mrJobInfo.setState(runRecord.getStatus().name());
    mrJobInfo.setStartTime(TimeUnit.SECONDS.toMillis(runRecord.getStartTs()));
    Long stopTs=runRecord.getStopTs();
    if (stopTs != null) {
      mrJobInfo.setStopTime(TimeUnit.SECONDS.toMillis(stopTs));
    }
    responder.sendJson(HttpResponseStatus.OK,mrJobInfo);
  }
 catch (  NotFoundException e) {
    LOG.warn(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",runId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}"
7082,"@Inject public ProgramLifecycleHttpHandler(Authenticator authenticator,Store store,CConfiguration cConf,ProgramRuntimeService runtimeService,ProgramLifecycleService lifecycleService,QueueAdmin queueAdmin,Scheduler scheduler,PreferencesStore preferencesStore,NamespacedLocationFactory namespacedLocationFactory,MRJobClient mrJobClient,MapReduceMetricsInfo mapReduceMetricsInfo,PropertiesResolver propertiesResolver,AdapterService adapterService,MetricStore metricStore){
  super(authenticator);
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.store=store;
  this.runtimeService=runtimeService;
  this.lifecycleService=lifecycleService;
  this.metricStore=metricStore;
  this.appFabricDir=cConf.get(Constants.AppFabric.OUTPUT_DIR);
  this.queueAdmin=queueAdmin;
  this.scheduler=scheduler;
  this.preferencesStore=preferencesStore;
  this.mrJobClient=mrJobClient;
  this.mapReduceMetricsInfo=mapReduceMetricsInfo;
  this.propertiesResolver=propertiesResolver;
  this.adapterService=adapterService;
}","@Inject public ProgramLifecycleHttpHandler(Authenticator authenticator,Store store,CConfiguration cConf,ProgramRuntimeService runtimeService,ProgramLifecycleService lifecycleService,QueueAdmin queueAdmin,Scheduler scheduler,PreferencesStore preferencesStore,NamespacedLocationFactory namespacedLocationFactory,MRJobInfoFetcher mrJobInfoFetcher,PropertiesResolver propertiesResolver,AdapterService adapterService,MetricStore metricStore){
  super(authenticator);
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.store=store;
  this.runtimeService=runtimeService;
  this.lifecycleService=lifecycleService;
  this.metricStore=metricStore;
  this.appFabricDir=cConf.get(Constants.AppFabric.OUTPUT_DIR);
  this.queueAdmin=queueAdmin;
  this.scheduler=scheduler;
  this.preferencesStore=preferencesStore;
  this.mrJobInfoFetcher=mrJobInfoFetcher;
  this.propertiesResolver=propertiesResolver;
  this.adapterService=adapterService;
}"
7083,"@Inject public WorkflowHttpHandler(Authenticator authenticator,Store store,WorkflowClient workflowClient,CConfiguration configuration,ProgramRuntimeService runtimeService,QueueAdmin queueAdmin,Scheduler scheduler,PreferencesStore preferencesStore,NamespacedLocationFactory namespacedLocationFactory,MRJobClient mrJobClient,MapReduceMetricsInfo mapReduceMetricsInfo,ProgramLifecycleService lifecycleService,PropertiesResolver resolver,AdapterService adapterService,MetricStore metricStore){
  super(authenticator,store,configuration,runtimeService,lifecycleService,queueAdmin,scheduler,preferencesStore,namespacedLocationFactory,mrJobClient,mapReduceMetricsInfo,resolver,adapterService,metricStore);
  this.workflowClient=workflowClient;
}","@Inject public WorkflowHttpHandler(Authenticator authenticator,Store store,WorkflowClient workflowClient,CConfiguration configuration,ProgramRuntimeService runtimeService,QueueAdmin queueAdmin,Scheduler scheduler,PreferencesStore preferencesStore,NamespacedLocationFactory namespacedLocationFactory,MRJobInfoFetcher mrJobInfoFetcher,ProgramLifecycleService lifecycleService,PropertiesResolver resolver,AdapterService adapterService,MetricStore metricStore){
  super(authenticator,store,configuration,runtimeService,lifecycleService,queueAdmin,scheduler,preferencesStore,namespacedLocationFactory,mrJobInfoFetcher,resolver,adapterService,metricStore);
  this.workflowClient=workflowClient;
}"
7084,"@PUT @Path(""String_Node_Str"") public void create(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  NamespaceMeta metadata;
  try {
    metadata=parseBody(request,NamespaceMeta.class);
  }
 catch (  JsonSyntaxException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
    return;
  }
  if (!isValid(namespaceId)) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  if (isReserved(namespaceId)) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,String.format(""String_Node_Str"",namespaceId,namespaceId));
    return;
  }
  NamespaceMeta.Builder builder=new NamespaceMeta.Builder().setName(namespaceId);
  if (metadata != null && metadata.getDescription() != null) {
    builder.setDescription(metadata.getDescription());
  }
  try {
    namespaceAdmin.createNamespace(builder.build());
    responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",namespaceId));
  }
 catch (  AlreadyExistsException e) {
    responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",namespaceId));
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}","@PUT @Path(""String_Node_Str"") public void create(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  Id.Namespace namespace;
  try {
    namespace=Id.Namespace.from(namespaceId);
  }
 catch (  IllegalArgumentException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  NamespaceMeta metadata;
  try {
    metadata=parseBody(request,NamespaceMeta.class);
  }
 catch (  JsonSyntaxException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
    return;
  }
  if (isReserved(namespaceId)) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,String.format(""String_Node_Str"",namespaceId,namespaceId));
    return;
  }
  NamespaceMeta.Builder builder=new NamespaceMeta.Builder().setName(namespace);
  if (metadata != null && metadata.getDescription() != null) {
    builder.setDescription(metadata.getDescription());
  }
  try {
    namespaceAdmin.createNamespace(builder.build());
    responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",namespaceId));
  }
 catch (  AlreadyExistsException e) {
    responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",namespaceId));
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}"
7085,"@Test public void testInvalidReservedId() throws Exception {
  HttpResponse response=createNamespace(METADATA_VALID,INVALID_NAME);
  assertResponseCode(400,response);
  response=createNamespace(METADATA_VALID,Constants.DEFAULT_NAMESPACE);
  assertResponseCode(400,response);
  response=createNamespace(METADATA_VALID,Constants.SYSTEM_NAMESPACE);
  assertResponseCode(400,response);
  deploy(AppWithDataset.class,Constants.Gateway.API_VERSION_3_TOKEN,Constants.DEFAULT_NAMESPACE,""String_Node_Str"");
  response=deleteNamespace(Constants.DEFAULT_NAMESPACE);
  assertResponseCode(200,response);
  response=getNamespace(Constants.DEFAULT_NAMESPACE);
  Assert.assertEquals(0,getAppList(Constants.DEFAULT_NAMESPACE).size());
  assertResponseCode(200,response);
  response=deleteNamespace(Constants.SYSTEM_NAMESPACE);
  assertResponseCode(404,response);
}","@Test public void testInvalidReservedId() throws Exception {
  HttpResponse response=createNamespace(METADATA_VALID,INVALID_NAME);
  assertResponseCode(400,response);
  response=createNamespace(METADATA_VALID,""String_Node_Str"");
  assertResponseCode(400,response);
  response=createNamespace(METADATA_VALID,Constants.DEFAULT_NAMESPACE);
  assertResponseCode(400,response);
  response=createNamespace(METADATA_VALID,Constants.SYSTEM_NAMESPACE);
  assertResponseCode(400,response);
  deploy(AppWithDataset.class,Constants.Gateway.API_VERSION_3_TOKEN,Constants.DEFAULT_NAMESPACE,""String_Node_Str"");
  response=deleteNamespace(Constants.DEFAULT_NAMESPACE);
  assertResponseCode(200,response);
  response=getNamespace(Constants.DEFAULT_NAMESPACE);
  Assert.assertEquals(0,getAppList(Constants.DEFAULT_NAMESPACE).size());
  assertResponseCode(200,response);
  response=deleteNamespace(Constants.SYSTEM_NAMESPACE);
  assertResponseCode(404,response);
}"
7086,"@Nullable private PluginInstantiator getPluginInstantiator(CConfiguration cConf){
  if (contextConfig.getAdapterSpec() == null) {
    return null;
  }
  ClassLoader classLoader=cConf.getClassLoader();
  if (!(classLoader instanceof MapReduceClassLoader)) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  return ((MapReduceClassLoader)classLoader).getPluginInstantiator();
}","@Nullable private PluginInstantiator getPluginInstantiator(CConfiguration cConf){
  if (contextConfig.getAdapterSpec() == null) {
    return null;
  }
  ClassLoader classLoader=Delegators.getDelegate(cConf.getClassLoader(),MapReduceClassLoader.class);
  if (!(classLoader instanceof MapReduceClassLoader)) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  return ((MapReduceClassLoader)classLoader).getPluginInstantiator();
}"
7087,"/** 
 * Returns the   {@link ClassLoader} for the MapReduce program. The ClassLoader for MapReduce job is alwaysan  {@link MapReduceClassLoader}, which set by   {@link MapReduceRuntimeService} in local mode and created by MRframework in distributed mode.
 */
static ClassLoader getProgramClassLoader(Configuration hConf){
  ClassLoader classLoader=hConf.getClassLoader();
  if (!(classLoader instanceof MapReduceClassLoader)) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  return ((MapReduceClassLoader)classLoader).getProgramClassLoader();
}","/** 
 * Returns the   {@link ClassLoader} for the MapReduce program. The ClassLoader for MapReduce job is alwaysan  {@link MapReduceClassLoader}, which set by   {@link MapReduceRuntimeService} in local mode and created by MRframework in distributed mode.
 */
static ClassLoader getProgramClassLoader(Configuration hConf){
  ClassLoader classLoader=Delegators.getDelegate(hConf.getClassLoader(),MapReduceClassLoader.class);
  if (!(classLoader instanceof MapReduceClassLoader)) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  return ((MapReduceClassLoader)classLoader).getProgramClassLoader();
}"
7088,"private boolean isLocal(Configuration hConf){
  String mrFramework=hConf.get(MRConfig.FRAMEWORK_NAME,MRConfig.LOCAL_FRAMEWORK_NAME);
  return ""String_Node_Str"".equals(mrFramework);
}","/** 
 * Helper method to tell if the MR is running in local mode or not. This method doesn't really belongs to this class, but currently there is no better place for it.
 */
static boolean isLocal(Configuration hConf){
  String mrFramework=hConf.get(MRConfig.FRAMEWORK_NAME,MRConfig.LOCAL_FRAMEWORK_NAME);
  return MRConfig.LOCAL_FRAMEWORK_NAME.equals(mrFramework);
}"
7089,"/** 
 * Creates a jar that contains everything that are needed for running the MapReduce program by Hadoop.
 * @return a new {@link Location} containing the job jar
 */
private Location buildJobJar(BasicMapReduceContext context) throws IOException {
  ApplicationBundler appBundler=new ApplicationBundler(ImmutableList.of(""String_Node_Str"",""String_Node_Str""),ImmutableList.of(""String_Node_Str"",""String_Node_Str""));
  Id.Program programId=context.getProgram().getId();
  Location jobJar=locationFactory.create(String.format(""String_Node_Str"",ProgramType.MAPREDUCE.name().toLowerCase(),programId.getNamespaceId(),programId.getApplicationId(),programId.getId(),context.getRunId().getId()));
  LOG.debug(""String_Node_Str"",jobJar.toURI());
  Set<Class<?>> classes=Sets.newHashSet();
  classes.add(MapReduce.class);
  classes.add(MapperWrapper.class);
  classes.add(ReducerWrapper.class);
  Job jobConf=context.getHadoopJob();
  if (cConf.getBoolean(Constants.AppFabric.MAPREDUCE_INCLUDE_CUSTOM_CLASSES)) {
    try {
      Class<? extends InputFormat<?,?>> inputFormatClass=jobConf.getInputFormatClass();
      LOG.info(""String_Node_Str"",inputFormatClass,inputFormatClass.getClassLoader());
      classes.add(inputFormatClass);
      if (StreamInputFormat.class.isAssignableFrom(inputFormatClass)) {
        Class<? extends StreamEventDecoder> decoderType=StreamInputFormat.getDecoderClass(jobConf.getConfiguration());
        if (decoderType != null) {
          classes.add(decoderType);
        }
      }
    }
 catch (    Throwable t) {
      LOG.info(""String_Node_Str"",t.getMessage(),t);
    }
    try {
      Class<? extends OutputFormat<?,?>> outputFormatClass=jobConf.getOutputFormatClass();
      LOG.info(""String_Node_Str"",outputFormatClass,outputFormatClass.getClassLoader());
      classes.add(outputFormatClass);
    }
 catch (    Throwable t) {
      LOG.info(""String_Node_Str"",t.getMessage(),t);
    }
  }
  try {
    Class<?> hbaseTableUtilClass=HBaseTableUtilFactory.getHBaseTableUtilClass();
    classes.add(hbaseTableUtilClass);
  }
 catch (  ProvisionException e) {
    LOG.warn(""String_Node_Str"");
  }
  ClassLoader oldCLassLoader=Thread.currentThread().getContextClassLoader();
  Thread.currentThread().setContextClassLoader(jobConf.getConfiguration().getClassLoader());
  appBundler.createBundle(jobJar,classes);
  Thread.currentThread().setContextClassLoader(oldCLassLoader);
  LOG.info(""String_Node_Str"",jobJar.toURI());
  return jobJar;
}","/** 
 * Creates a jar that contains everything that are needed for running the MapReduce program by Hadoop.
 * @return a new {@link Location} containing the job jar
 */
private Location buildJobJar(BasicMapReduceContext context) throws IOException {
  ApplicationBundler appBundler=new ApplicationBundler(ImmutableList.of(""String_Node_Str"",""String_Node_Str""),ImmutableList.of(""String_Node_Str"",""String_Node_Str""));
  Id.Program programId=context.getProgram().getId();
  Location jobJar=locationFactory.create(String.format(""String_Node_Str"",ProgramType.MAPREDUCE.name().toLowerCase(),programId.getNamespaceId(),programId.getApplicationId(),programId.getId(),context.getRunId().getId()));
  LOG.debug(""String_Node_Str"",jobJar.toURI());
  Set<Class<?>> classes=Sets.newHashSet();
  classes.add(MapReduce.class);
  classes.add(MapperWrapper.class);
  classes.add(ReducerWrapper.class);
  Job jobConf=context.getHadoopJob();
  if (cConf.getBoolean(Constants.AppFabric.MAPREDUCE_INCLUDE_CUSTOM_CLASSES)) {
    try {
      Class<? extends InputFormat<?,?>> inputFormatClass=jobConf.getInputFormatClass();
      LOG.info(""String_Node_Str"",inputFormatClass,inputFormatClass.getClassLoader());
      classes.add(inputFormatClass);
      if (StreamInputFormat.class.isAssignableFrom(inputFormatClass)) {
        Class<? extends StreamEventDecoder> decoderType=StreamInputFormat.getDecoderClass(jobConf.getConfiguration());
        if (decoderType != null) {
          classes.add(decoderType);
        }
      }
    }
 catch (    Throwable t) {
      LOG.info(""String_Node_Str"",t.getMessage(),t);
    }
    try {
      Class<? extends OutputFormat<?,?>> outputFormatClass=jobConf.getOutputFormatClass();
      LOG.info(""String_Node_Str"",outputFormatClass,outputFormatClass.getClassLoader());
      classes.add(outputFormatClass);
    }
 catch (    Throwable t) {
      LOG.info(""String_Node_Str"",t.getMessage(),t);
    }
  }
  try {
    Class<?> hbaseTableUtilClass=HBaseTableUtilFactory.getHBaseTableUtilClass();
    classes.add(hbaseTableUtilClass);
  }
 catch (  ProvisionException e) {
    LOG.warn(""String_Node_Str"");
  }
  ClassLoader oldCLassLoader=ClassLoaders.setContextClassLoader(jobConf.getConfiguration().getClassLoader());
  appBundler.createBundle(jobJar,classes);
  ClassLoaders.setContextClassLoader(oldCLassLoader);
  LOG.info(""String_Node_Str"",jobJar.toURI());
  return jobJar;
}"
7090,"@Override protected void shutDown() throws Exception {
  boolean success=job.isSuccessful();
  try {
    if (success) {
      LOG.info(""String_Node_Str"",context);
      if (!txClient.commit(transaction)) {
        LOG.warn(""String_Node_Str"");
        throw new TransactionFailureException(""String_Node_Str"" + context.toString());
      }
    }
 else {
      txClient.invalidate(transaction.getWritePointer());
    }
  }
  finally {
    try {
      onFinish(success);
    }
  finally {
      context.close();
      cleanupTask.run();
      job.getConfiguration().setClassLoader(null);
    }
  }
}","@Override protected void shutDown() throws Exception {
  boolean success=job.isSuccessful();
  try {
    if (success) {
      LOG.info(""String_Node_Str"",context);
      if (!txClient.commit(transaction)) {
        LOG.warn(""String_Node_Str"");
        throw new TransactionFailureException(""String_Node_Str"" + context.toString());
      }
    }
 else {
      txClient.invalidate(transaction.getWritePointer());
    }
  }
  finally {
    try {
      onFinish(success);
    }
  finally {
      context.close();
      cleanupTask.run();
    }
  }
}"
7091,"@Override protected void startUp() throws Exception {
  final Job job=Job.getInstance(new Configuration(hConf));
  job.setJobName(getJobName(context));
  Configuration mapredConf=job.getConfiguration();
  if (UserGroupInformation.isSecurityEnabled()) {
    mapredConf.unset(""String_Node_Str"");
    mapredConf.setBoolean(Job.JOB_AM_ACCESS_DISABLED,false);
    Credentials credentials=UserGroupInformation.getCurrentUser().getCredentials();
    LOG.info(""String_Node_Str"",credentials.getAllTokens());
    job.getCredentials().addAll(credentials);
  }
  ClassLoader classLoader=new MapReduceClassLoader(context.getProgram().getClassLoader());
  ClassLoaders.setContextClassLoader(classLoader);
  job.getConfiguration().setClassLoader(classLoader);
  context.setJob(job);
  runUserCodeInTx(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      beforeSubmit();
      setInputDatasetIfNeeded(job);
      setOutputDatasetIfNeeded(job);
    }
  }
,""String_Node_Str"");
  Location pluginArchive=createPluginArchive(context.getAdapterSpecification(),context.getProgram().getId());
  try {
    if (pluginArchive != null) {
      job.addCacheArchive(pluginArchive.toURI());
    }
    classLoader=new MapReduceClassLoader(context.getProgram().getClassLoader(),context.getAdapterSpecification(),context.getPluginInstantiator());
    ClassLoaders.setContextClassLoader(classLoader);
    job.getConfiguration().setClassLoader(classLoader);
    setOutputClassesIfNeeded(job);
    setMapOutputClassesIfNeeded(job);
    mapredConf.setBoolean(""String_Node_Str"",true);
    mapredConf.setBoolean(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,true);
    mapredConf.setBoolean(Job.MAPREDUCE_JOB_CLASSLOADER,true);
    String yarnAppClassPath=mapredConf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(',').join(YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
    mapredConf.set(YarnConfiguration.YARN_APPLICATION_CLASSPATH,""String_Node_Str"" + yarnAppClassPath);
    Resources mapperResources=context.getMapperResources();
    Resources reducerResources=context.getReducerResources();
    if (mapperResources != null) {
      mapredConf.setInt(Job.MAP_MEMORY_MB,mapperResources.getMemoryMB());
      mapredConf.set(Job.MAP_JAVA_OPTS,""String_Node_Str"" + (int)(mapperResources.getMemoryMB() * 0.8) + ""String_Node_Str"");
      setVirtualCores(mapredConf,mapperResources.getVirtualCores(),""String_Node_Str"");
    }
    if (reducerResources != null) {
      mapredConf.setInt(Job.REDUCE_MEMORY_MB,reducerResources.getMemoryMB());
      mapredConf.set(Job.REDUCE_JAVA_OPTS,""String_Node_Str"" + (int)(reducerResources.getMemoryMB() * 0.8) + ""String_Node_Str"");
      setVirtualCores(mapredConf,reducerResources.getVirtualCores(),""String_Node_Str"");
    }
    MapperWrapper.wrap(job);
    ReducerWrapper.wrap(job);
    Location jobJar=buildJobJar(context);
    try {
      Location programJarCopy=copyProgramJar();
      try {
        job.setJar(jobJar.toURI().toString());
        job.addCacheFile(programJarCopy.toURI());
        MapReduceContextConfig contextConfig=new MapReduceContextConfig(job.getConfiguration());
        Transaction tx=txClient.startLong();
        try {
          CConfiguration cConfCopy=cConf;
          if (pluginArchive != null) {
            cConfCopy=CConfiguration.copy(cConf);
            cConfCopy.set(Constants.AppFabric.APP_TEMPLATE_DIR,pluginArchive.getName());
          }
          contextConfig.set(context,cConfCopy,tx,programJarCopy.toURI());
          LOG.info(""String_Node_Str"",context);
          job.submit();
          this.job=job;
          this.transaction=tx;
          this.cleanupTask=createCleanupTask(jobJar,programJarCopy,pluginArchive);
        }
 catch (        Throwable t) {
          Transactions.invalidateQuietly(txClient,tx);
          throw Throwables.propagate(t);
        }
      }
 catch (      Throwable t) {
        Locations.deleteQuietly(programJarCopy);
        throw Throwables.propagate(t);
      }
    }
 catch (    Throwable t) {
      Locations.deleteQuietly(jobJar);
      throw Throwables.propagate(t);
    }
  }
 catch (  Throwable t) {
    if (pluginArchive != null) {
      Locations.deleteQuietly(pluginArchive);
    }
    LOG.error(""String_Node_Str"",context,t);
    throw Throwables.propagate(t);
  }
}","@Override protected void startUp() throws Exception {
  final Job job=Job.getInstance(new Configuration(hConf));
  job.setJobName(getJobName(context));
  Configuration mapredConf=job.getConfiguration();
  if (UserGroupInformation.isSecurityEnabled()) {
    mapredConf.unset(""String_Node_Str"");
    mapredConf.setBoolean(Job.JOB_AM_ACCESS_DISABLED,false);
    Credentials credentials=UserGroupInformation.getCurrentUser().getCredentials();
    LOG.info(""String_Node_Str"",credentials.getAllTokens());
    job.getCredentials().addAll(credentials);
  }
  classLoader=new MapReduceClassLoader(context.getProgram().getClassLoader());
  job.getConfiguration().setClassLoader(new WeakReferenceDelegatorClassLoader(classLoader));
  ClassLoaders.setContextClassLoader(job.getConfiguration().getClassLoader());
  context.setJob(job);
  runUserCodeInTx(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      beforeSubmit();
      setInputDatasetIfNeeded(job);
      setOutputDatasetIfNeeded(job);
    }
  }
,""String_Node_Str"");
  Location pluginArchive=createPluginArchive(context.getAdapterSpecification(),context.getProgram().getId());
  try {
    if (pluginArchive != null) {
      job.addCacheArchive(pluginArchive.toURI());
    }
    classLoader=new MapReduceClassLoader(context.getProgram().getClassLoader(),context.getAdapterSpecification(),context.getPluginInstantiator());
    job.getConfiguration().setClassLoader(new WeakReferenceDelegatorClassLoader(classLoader));
    ClassLoaders.setContextClassLoader(job.getConfiguration().getClassLoader());
    setOutputClassesIfNeeded(job);
    setMapOutputClassesIfNeeded(job);
    mapredConf.setBoolean(""String_Node_Str"",true);
    mapredConf.setBoolean(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,true);
    mapredConf.setBoolean(Job.MAPREDUCE_JOB_CLASSLOADER,true);
    String yarnAppClassPath=mapredConf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(',').join(YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
    mapredConf.set(YarnConfiguration.YARN_APPLICATION_CLASSPATH,""String_Node_Str"" + yarnAppClassPath);
    Resources mapperResources=context.getMapperResources();
    Resources reducerResources=context.getReducerResources();
    if (mapperResources != null) {
      mapredConf.setInt(Job.MAP_MEMORY_MB,mapperResources.getMemoryMB());
      mapredConf.set(Job.MAP_JAVA_OPTS,""String_Node_Str"" + (int)(mapperResources.getMemoryMB() * 0.8) + ""String_Node_Str"");
      setVirtualCores(mapredConf,mapperResources.getVirtualCores(),""String_Node_Str"");
    }
    if (reducerResources != null) {
      mapredConf.setInt(Job.REDUCE_MEMORY_MB,reducerResources.getMemoryMB());
      mapredConf.set(Job.REDUCE_JAVA_OPTS,""String_Node_Str"" + (int)(reducerResources.getMemoryMB() * 0.8) + ""String_Node_Str"");
      setVirtualCores(mapredConf,reducerResources.getVirtualCores(),""String_Node_Str"");
    }
    MapperWrapper.wrap(job);
    ReducerWrapper.wrap(job);
    Location jobJar=buildJobJar(context);
    try {
      Location programJarCopy=copyProgramJar();
      try {
        job.setJar(jobJar.toURI().toString());
        job.addCacheFile(programJarCopy.toURI());
        MapReduceContextConfig contextConfig=new MapReduceContextConfig(job.getConfiguration());
        Transaction tx=txClient.startLong();
        try {
          CConfiguration cConfCopy=cConf;
          if (pluginArchive != null) {
            cConfCopy=CConfiguration.copy(cConf);
            cConfCopy.set(Constants.AppFabric.APP_TEMPLATE_DIR,pluginArchive.getName());
          }
          contextConfig.set(context,cConfCopy,tx,programJarCopy.toURI());
          LOG.info(""String_Node_Str"",context);
          job.submit();
          this.job=job;
          this.transaction=tx;
          this.cleanupTask=createCleanupTask(jobJar,programJarCopy,pluginArchive);
        }
 catch (        Throwable t) {
          Transactions.invalidateQuietly(txClient,tx);
          throw Throwables.propagate(t);
        }
      }
 catch (      Throwable t) {
        Locations.deleteQuietly(programJarCopy);
        throw Throwables.propagate(t);
      }
    }
 catch (    Throwable t) {
      Locations.deleteQuietly(jobJar);
      throw Throwables.propagate(t);
    }
  }
 catch (  Throwable t) {
    if (pluginArchive != null) {
      Locations.deleteQuietly(pluginArchive);
    }
    LOG.error(""String_Node_Str"",context,t);
    throw Throwables.propagate(t);
  }
}"
7092,"public Job(JobID jobid,String jobSubmitDir) throws IOException {
  this.systemJobDir=new Path(jobSubmitDir);
  this.systemJobFile=new Path(systemJobDir,""String_Node_Str"");
  this.id=jobid;
  JobConf conf=new JobConf(systemJobFile);
  this.localFs=FileSystem.getLocal(conf);
  String user=UserGroupInformation.getCurrentUser().getShortUserName();
  this.localJobDir=localFs.makeQualified(new Path(new Path(conf.getLocalPath(jobDir),user),jobid.toString()));
  this.localJobFile=new Path(this.localJobDir,id + ""String_Node_Str"");
  DistributedCache.addFileToClassPath(new Path(conf.getJar()),conf,FileSystem.get(conf));
  ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf);
  localDistributedCacheManager=new LocalDistributedCacheManagerWithFix(id);
  localDistributedCacheManager.setup(conf);
  OutputStream out=localFs.create(localJobFile);
  try {
    conf.writeXml(out);
  }
  finally {
    out.close();
  }
  this.job=new JobConf(localJobFile);
  if (localDistributedCacheManager.hasLocalClasspaths()) {
    ClassLoader classLoader=localDistributedCacheManager.makeClassLoader(getContextClassLoader());
    setContextClassLoader(classLoader);
    this.job.setClassLoader(classLoader);
  }
  profile=new JobProfile(job.getUser(),id,systemJobFile.toString(),""String_Node_Str"",job.getJobName());
  status=new JobStatus(id,0.0f,0.0f,JobStatus.RUNNING,profile.getUser(),profile.getJobName(),profile.getJobFile(),profile.getURL().toString());
  jobs.put(id,this);
  this.start();
}","public Job(JobID jobid,String jobSubmitDir) throws IOException {
  this.systemJobDir=new Path(jobSubmitDir);
  this.systemJobFile=new Path(systemJobDir,""String_Node_Str"");
  this.id=jobid;
  JobConf conf=new JobConf(systemJobFile);
  this.localFs=FileSystem.getLocal(conf);
  String user=UserGroupInformation.getCurrentUser().getShortUserName();
  this.localJobDir=localFs.makeQualified(new Path(new Path(conf.getLocalPath(jobDir),user),jobid.toString()));
  this.localJobFile=new Path(this.localJobDir,id + ""String_Node_Str"");
  ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf);
  localDistributedCacheManager=new LocalDistributedCacheManagerWithFix(id);
  localDistributedCacheManager.setup(conf);
  OutputStream out=localFs.create(localJobFile);
  try {
    conf.writeXml(out);
  }
  finally {
    out.close();
  }
  this.job=new JobConf(localJobFile);
  if (localDistributedCacheManager.hasLocalClasspaths()) {
    ClassLoader classLoader=localDistributedCacheManager.makeClassLoader(getContextClassLoader());
    setContextClassLoader(classLoader);
    this.job.setClassLoader(classLoader);
  }
  profile=new JobProfile(job.getUser(),id,systemJobFile.toString(),""String_Node_Str"",job.getJobName());
  status=new JobStatus(id,0.0f,0.0f,JobStatus.RUNNING,profile.getUser(),profile.getJobName(),profile.getJobFile(),profile.getURL().toString());
  jobs.put(id,this);
  this.start();
}"
7093,"@Ignore @Category(XSlowTests.class) @Test public void testWorkflowAdapter() throws Exception {
  Id.ApplicationTemplate templateId=Id.ApplicationTemplate.from(WorkflowTemplate.NAME);
  addTemplatePlugins(templateId,""String_Node_Str"",FlipPlugin.class);
  deployTemplate(Constants.DEFAULT_NAMESPACE_ID,templateId,WorkflowTemplate.class);
  WorkflowTemplate.Config config=new WorkflowTemplate.Config(""String_Node_Str"");
  Id.Adapter adapterId=Id.Adapter.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"");
  AdapterConfig adapterConfig=new AdapterConfig(""String_Node_Str"",WorkflowTemplate.NAME,GSON.toJsonTree(config));
  AdapterManager manager=createAdapter(adapterId,adapterConfig);
  DataSetManager<KeyValueTable> inputManager=getDataset(Constants.DEFAULT_NAMESPACE_ID,WorkflowTemplate.INPUT);
  inputManager.get().write(Bytes.toBytes(1L),Bytes.toBytes(10L));
  inputManager.flush();
  manager.start();
  manager.waitForOneRunToFinish(4,TimeUnit.MINUTES);
  manager.stop();
  DataSetManager<KeyValueTable> outputManager=getDataset(Constants.DEFAULT_NAMESPACE_ID,WorkflowTemplate.OUTPUT);
  long outputVal=Bytes.toLong(outputManager.get().read(Bytes.toBytes(1L)));
  Assert.assertEquals(-10L,outputVal);
}","@Category(XSlowTests.class) @Test public void testWorkflowAdapter() throws Exception {
  Id.ApplicationTemplate templateId=Id.ApplicationTemplate.from(WorkflowTemplate.NAME);
  addTemplatePlugins(templateId,""String_Node_Str"",FlipPlugin.class);
  deployTemplate(Constants.DEFAULT_NAMESPACE_ID,templateId,WorkflowTemplate.class);
  WorkflowTemplate.Config config=new WorkflowTemplate.Config(""String_Node_Str"");
  Id.Adapter adapterId=Id.Adapter.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"");
  AdapterConfig adapterConfig=new AdapterConfig(""String_Node_Str"",WorkflowTemplate.NAME,GSON.toJsonTree(config));
  AdapterManager manager=createAdapter(adapterId,adapterConfig);
  DataSetManager<KeyValueTable> inputManager=getDataset(Constants.DEFAULT_NAMESPACE_ID,WorkflowTemplate.INPUT);
  inputManager.get().write(Bytes.toBytes(1L),Bytes.toBytes(10L));
  inputManager.flush();
  manager.start();
  manager.waitForOneRunToFinish(4,TimeUnit.MINUTES);
  manager.stop();
  DataSetManager<KeyValueTable> outputManager=getDataset(Constants.DEFAULT_NAMESPACE_ID,WorkflowTemplate.OUTPUT);
  long outputVal=Bytes.toLong(outputManager.get().read(Bytes.toBytes(1L)));
  Assert.assertEquals(-10L,outputVal);
}"
7094,"/** 
 * Creates a service listener to reactor on state changes on   {@link MapReduceRuntimeService}.
 */
private Service.Listener createRuntimeServiceListener(final Program program,final RunId runId,final AdapterDefinition adapterSpec,final String twillRunId,final PluginInstantiator pluginInstantiator){
  return new ServiceListenerAdapter(){
    @Override public void starting(){
      long startTimeInSeconds=RunIds.getTime(runId,TimeUnit.SECONDS);
      if (startTimeInSeconds == -1) {
        startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
      }
      String adapterName=adapterSpec == null ? null : adapterSpec.getName();
      if (workflowName == null) {
        store.setStart(program.getId(),runId.getId(),startTimeInSeconds,adapterName,twillRunId);
      }
 else {
        store.setWorkflowProgramStart(program.getId(),runId.getId(),workflowName,workflowRunId,workflowNodeId,startTimeInSeconds,adapterName,twillRunId);
      }
    }
    @Override public void terminated(    Service.State from){
      Closeables.closeQuietly(pluginInstantiator);
      if (from == Service.State.STOPPING) {
        store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.KILLED.getRunStatus());
      }
 else {
        store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.COMPLETED.getRunStatus());
      }
    }
    @Override public void failed(    Service.State from,    Throwable failure){
      Closeables.closeQuietly(pluginInstantiator);
      store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus());
    }
  }
;
}","/** 
 * Creates a service listener to reactor on state changes on   {@link MapReduceRuntimeService}.
 */
private Service.Listener createRuntimeServiceListener(final Program program,final RunId runId,final AdapterDefinition adapterSpec,final PluginInstantiator pluginInstantiator,Arguments arguments){
  final String twillRunId=arguments.getOption(ProgramOptionConstants.TWILL_RUN_ID);
  final String workflowName=arguments.getOption(ProgramOptionConstants.WORKFLOW_NAME);
  final String workflowNodeId=arguments.getOption(ProgramOptionConstants.WORKFLOW_NODE_ID);
  final String workflowRunId=arguments.getOption(ProgramOptionConstants.WORKFLOW_RUN_ID);
  return new ServiceListenerAdapter(){
    @Override public void starting(){
      long startTimeInSeconds=RunIds.getTime(runId,TimeUnit.SECONDS);
      if (startTimeInSeconds == -1) {
        startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
      }
      String adapterName=adapterSpec == null ? null : adapterSpec.getName();
      if (workflowName == null) {
        store.setStart(program.getId(),runId.getId(),startTimeInSeconds,adapterName,twillRunId);
      }
 else {
        store.setWorkflowProgramStart(program.getId(),runId.getId(),workflowName,workflowRunId,workflowNodeId,startTimeInSeconds,adapterName,twillRunId);
      }
    }
    @Override public void terminated(    Service.State from){
      Closeables.closeQuietly(pluginInstantiator);
      if (from == Service.State.STOPPING) {
        store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.KILLED.getRunStatus());
      }
 else {
        store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.COMPLETED.getRunStatus());
      }
    }
    @Override public void failed(    Service.State from,    Throwable failure){
      Closeables.closeQuietly(pluginInstantiator);
      store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus());
    }
  }
;
}"
7095,"/** 
 * @param runId for which information will be returned.
 * @return a {@link MRJobInfo} containing information about a particular MapReduce program run.
 * @throws IOException if there is failure to communicate through the JobClient.
 * @throws NotFoundException if a Job with the given runId is not found.
 */
public MRJobInfo getMRJobInfo(Id.Run runId) throws IOException, NotFoundException {
  Preconditions.checkArgument(ProgramType.MAPREDUCE.equals(runId.getProgram().getType()));
  JobClient jobClient;
  JobStatus[] jobs;
  try {
    jobClient=new JobClient(hConf);
    jobs=jobClient.getAllJobs();
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
    throw new IOException(e);
  }
  JobStatus thisJob=findJobForRunId(jobs,runId);
  Counters counters=jobClient.getJob(thisJob.getJobID()).getCounters();
  TaskReport[] mapTaskReports=jobClient.getMapTaskReports(thisJob.getJobID());
  TaskReport[] reduceTaskReports=jobClient.getReduceTaskReports(thisJob.getJobID());
  return new MRJobInfo(thisJob.getMapProgress(),thisJob.getReduceProgress(),groupToMap(counters.getGroup(TaskCounter.class.getName())),toMRTaskInfos(mapTaskReports),toMRTaskInfos(reduceTaskReports),true);
}","/** 
 * @param runId for which information will be returned.
 * @return a {@link MRJobInfo} containing information about a particular MapReduce program run.
 * @throws IOException if there is failure to communicate through the JobClient.
 * @throws NotFoundException if a Job with the given runId is not found.
 */
public MRJobInfo getMRJobInfo(Id.Run runId) throws IOException, NotFoundException {
  Preconditions.checkArgument(ProgramType.MAPREDUCE.equals(runId.getProgram().getType()));
  JobClient jobClient;
  JobStatus[] jobs;
  try {
    jobClient=new JobClient(hConf);
    jobs=jobClient.getAllJobs();
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
    throw new IOException(e);
  }
  JobStatus thisJob=findJobForRunId(jobs,runId);
  RunningJob runningJob=jobClient.getJob(thisJob.getJobID());
  Counters counters=runningJob.getCounters();
  TaskReport[] mapTaskReports=jobClient.getMapTaskReports(thisJob.getJobID());
  TaskReport[] reduceTaskReports=jobClient.getReduceTaskReports(thisJob.getJobID());
  return new MRJobInfo(runningJob.mapProgress(),runningJob.reduceProgress(),groupToMap(counters.getGroup(TaskCounter.class.getName())),toMRTaskInfos(mapTaskReports),toMRTaskInfos(reduceTaskReports),true);
}"
7096,"/** 
 * Returns the metric context.  A metric context is of the form {applicationId}.{programTypeId}.{programId}.{componentId}.  So for flows, it will look like appX.f.flowY.flowletZ. For mapreduce jobs, appX.b.mapredY.{optional m|r}.
 */
private static Map<String,String> getMetricContext(Program program,TwillContext context){
  ImmutableMap.Builder<String,String> builder=ImmutableMap.<String,String>builder().put(Constants.Metrics.Tag.RUN_ID,context.getRunId().getId()).put(Constants.Metrics.Tag.APP,program.getApplicationId());
  if (program.getType() == ProgramType.FLOW) {
    builder.put(Constants.Metrics.Tag.FLOW,program.getName());
    builder.put(Constants.Metrics.Tag.FLOWLET,context.getSpecification().getName());
  }
 else {
    builder.put(ProgramTypeMetricTag.getTagName(program.getType()),context.getSpecification().getName());
  }
  return builder.build();
}","/** 
 * Returns the metric context.  A metric context is of the form {applicationId}.{programTypeId}.{programId}.{componentId}.  So for flows, it will look like appX.f.flowY.flowletZ. For mapreduce jobs, appX.b.mapredY.{optional m|r}.
 */
private static Map<String,String> getMetricContext(Program program,TwillContext context){
  ImmutableMap.Builder<String,String> builder=ImmutableMap.<String,String>builder().put(Constants.Metrics.Tag.NAMESPACE,program.getNamespaceId()).put(Constants.Metrics.Tag.RUN_ID,context.getRunId().getId()).put(Constants.Metrics.Tag.APP,program.getApplicationId());
  if (program.getType() == ProgramType.FLOW) {
    builder.put(Constants.Metrics.Tag.FLOW,program.getName());
    builder.put(Constants.Metrics.Tag.FLOWLET,context.getSpecification().getName());
  }
 else {
    builder.put(ProgramTypeMetricTag.getTagName(program.getType()),context.getSpecification().getName());
  }
  return builder.build();
}"
7097,"protected AbstractHttpHandlerDelegator(DelegatorContext<T> context,MetricsCollector metricsCollector){
  this.context=context;
  this.metricsCollector=metricsCollector;
  if (context.getServiceContext() != null && context.getServiceContext().getSpecification() != null) {
    this.metricsCollector=metricsCollector.childCollector(Constants.Metrics.Tag.HANDLER,context.getServiceContext().getSpecification().getName());
  }
}","protected AbstractHttpHandlerDelegator(DelegatorContext<T> context,MetricsCollector metricsCollector){
  this.context=context;
  this.metricsCollector=metricsCollector;
}"
7098,"protected final DelayedHttpServiceResponder wrapResponder(HttpResponder responder){
  return new DelayedHttpServiceResponder(responder,metricsCollector);
}","protected final DelayedHttpServiceResponder wrapResponder(HttpResponder responder){
  MetricsCollector collector=this.metricsCollector;
  if (context.getServiceContext() != null && context.getServiceContext().getSpecification() != null) {
    collector=metricsCollector.childCollector(Constants.Metrics.Tag.HANDLER,context.getServiceContext().getSpecification().getName());
  }
  return new DelayedHttpServiceResponder(responder,collector);
}"
7099,"public void updateAppSpec(String namespaceId,String appId,ApplicationSpecification spec){
  spec=DefaultApplicationSpecification.from(spec);
  LOG.trace(""String_Node_Str"",appId,GSON.toJson(spec));
  MDSKey key=new MDSKey.Builder().add(TYPE_APP_META,namespaceId,appId).build();
  ApplicationMeta existing=getFirst(key,ApplicationMeta.class);
  if (existing == null) {
    String msg=String.format(""String_Node_Str"",namespaceId,appId);
    LOG.error(msg);
    throw new IllegalArgumentException(msg);
  }
  LOG.trace(""String_Node_Str"",existing);
  ApplicationMeta updated=ApplicationMeta.updateSpec(existing,spec);
  write(key,updated);
  for (  StreamSpecification stream : spec.getStreams().values()) {
    writeStream(namespaceId,stream);
  }
}","public void updateAppSpec(String namespaceId,String appId,ApplicationSpecification spec){
  spec=DefaultApplicationSpecification.from(spec);
  LOG.trace(""String_Node_Str"",appId,GSON.toJson(spec));
  MDSKey key=new MDSKey.Builder().add(TYPE_APP_META,namespaceId,appId).build();
  ApplicationMeta existing=getFirst(key,ApplicationMeta.class);
  if (existing == null) {
    String msg=String.format(""String_Node_Str"",namespaceId,appId);
    LOG.error(msg);
    throw new IllegalArgumentException(msg);
  }
  LOG.trace(""String_Node_Str"",existing);
  ApplicationMeta updated=ApplicationMeta.updateSpec(existing,spec);
  write(key,updated);
}"
7100,"@Override public void addApplication(final Id.Application id,final ApplicationSpecification spec,final Location appArchiveLocation){
  txnl.executeUnchecked(new TransactionExecutor.Function<AppMds,Void>(){
    @Override public Void apply(    AppMds mds) throws Exception {
      mds.apps.writeApplication(id.getNamespaceId(),id.getId(),spec,appArchiveLocation.toURI().toString());
      for (      StreamSpecification stream : spec.getStreams().values()) {
        mds.apps.writeStream(id.getNamespaceId(),stream);
      }
      return null;
    }
  }
);
}","@Override public void addApplication(final Id.Application id,final ApplicationSpecification spec,final Location appArchiveLocation){
  txnl.executeUnchecked(new TransactionExecutor.Function<AppMds,Void>(){
    @Override public Void apply(    AppMds mds) throws Exception {
      mds.apps.writeApplication(id.getNamespaceId(),id.getId(),spec,appArchiveLocation.toURI().toString());
      return null;
    }
  }
);
}"
7101,"@Test public void testRemoveAll() throws Exception {
  ApplicationSpecification spec=Specifications.from(new WordCountApp());
  Id.Namespace namespaceId=new Id.Namespace(""String_Node_Str"");
  Id.Application appId=new Id.Application(namespaceId,""String_Node_Str"");
  store.addApplication(appId,spec,new LocalLocationFactory().create(""String_Node_Str""));
  Assert.assertNotNull(store.getApplication(appId));
  Assert.assertEquals(1,store.getAllStreams(new Id.Namespace(""String_Node_Str"")).size());
  store.removeAll(namespaceId);
  Assert.assertNull(store.getApplication(appId));
  Assert.assertEquals(0,store.getAllStreams(new Id.Namespace(""String_Node_Str"")).size());
}","@Test public void testRemoveAll() throws Exception {
  ApplicationSpecification spec=Specifications.from(new WordCountApp());
  Id.Namespace namespaceId=new Id.Namespace(""String_Node_Str"");
  Id.Application appId=new Id.Application(namespaceId,""String_Node_Str"");
  store.addApplication(appId,spec,new LocalLocationFactory().create(""String_Node_Str""));
  Assert.assertNotNull(store.getApplication(appId));
  store.removeAll(namespaceId);
  Assert.assertNull(store.getApplication(appId));
}"
7102,"@Test public void testRemoveApplication() throws Exception {
  ApplicationSpecification spec=Specifications.from(new WordCountApp());
  Id.Namespace namespaceId=new Id.Namespace(""String_Node_Str"");
  Id.Application appId=new Id.Application(namespaceId,spec.getName());
  store.addApplication(appId,spec,new LocalLocationFactory().create(""String_Node_Str""));
  Assert.assertNotNull(store.getApplication(appId));
  Assert.assertEquals(1,store.getAllStreams(new Id.Namespace(""String_Node_Str"")).size());
  store.removeApplication(appId);
  Assert.assertNull(store.getApplication(appId));
  Assert.assertEquals(1,store.getAllStreams(new Id.Namespace(""String_Node_Str"")).size());
}","@Test public void testRemoveApplication() throws Exception {
  ApplicationSpecification spec=Specifications.from(new WordCountApp());
  Id.Namespace namespaceId=new Id.Namespace(""String_Node_Str"");
  Id.Application appId=new Id.Application(namespaceId,spec.getName());
  store.addApplication(appId,spec,new LocalLocationFactory().create(""String_Node_Str""));
  Assert.assertNotNull(store.getApplication(appId));
  store.removeApplication(appId);
  Assert.assertNull(store.getApplication(appId));
}"
7103,"@Test public void testRemoveAllApplications() throws Exception {
  ApplicationSpecification spec=Specifications.from(new WordCountApp());
  Id.Namespace namespaceId=new Id.Namespace(""String_Node_Str"");
  Id.Application appId=new Id.Application(namespaceId,spec.getName());
  store.addApplication(appId,spec,new LocalLocationFactory().create(""String_Node_Str""));
  Assert.assertNotNull(store.getApplication(appId));
  Assert.assertEquals(1,store.getAllStreams(new Id.Namespace(""String_Node_Str"")).size());
  store.removeAllApplications(namespaceId);
  Assert.assertNull(store.getApplication(appId));
  Assert.assertEquals(1,store.getAllStreams(new Id.Namespace(""String_Node_Str"")).size());
}","@Test public void testRemoveAllApplications() throws Exception {
  ApplicationSpecification spec=Specifications.from(new WordCountApp());
  Id.Namespace namespaceId=new Id.Namespace(""String_Node_Str"");
  Id.Application appId=new Id.Application(namespaceId,spec.getName());
  store.addApplication(appId,spec,new LocalLocationFactory().create(""String_Node_Str""));
  Assert.assertNotNull(store.getApplication(appId));
  store.removeAllApplications(namespaceId);
  Assert.assertNull(store.getApplication(appId));
}"
7104,"@Override public void configurePipeline(ETLStage stageConfig,PipelineConfigurer pipelineConfigurer){
  new Config(stageConfig.getProperties());
}","@Override public void configurePipeline(ETLStage stageConfig,PipelineConfigurer pipelineConfigurer){
  Config config=new Config(stageConfig.getProperties());
  pipelineConfigurer.addStream(new Stream(config.name));
}"
7105,"@PUT @Path(""String_Node_Str"") public void create(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream) throws Exception {
  try {
    Id.Stream streamId=Id.Stream.from(namespaceId,stream);
    namespaceClient.get(namespaceId);
    streamAdmin.create(streamId);
    streamMetaStore.addStream(streamId);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  IllegalArgumentException e) {
    LOG.error(""String_Node_Str"",stream,e);
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
catch (  NotFoundException e) {
    LOG.error(""String_Node_Str"",stream,e);
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",stream,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}","@PUT @Path(""String_Node_Str"") public void create(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream) throws Exception {
  try {
    Id.Stream streamId=Id.Stream.from(namespaceId,stream);
    namespaceClient.get(namespaceId);
    streamAdmin.create(streamId);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  IllegalArgumentException e) {
    LOG.error(""String_Node_Str"",stream,e);
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
catch (  NotFoundException e) {
    LOG.error(""String_Node_Str"",stream,e);
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",stream,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}"
7106,"@Inject public HBaseQueueClientFactory(CConfiguration cConf,Configuration hConf,HBaseTableUtil hBaseTableUtil,QueueAdmin queueAdmin,HBaseStreamAdmin streamAdmin,TransactionExecutorFactory txExecutorFactory){
  this.cConf=cConf;
  this.hConf=hConf;
  this.queueAdmin=(HBaseQueueAdmin)queueAdmin;
  this.streamAdmin=streamAdmin;
  this.queueUtil=new HBaseQueueUtilFactory().get();
  this.hBaseTableUtil=hBaseTableUtil;
  this.txExecutorFactory=txExecutorFactory;
}","@Inject public HBaseQueueClientFactory(CConfiguration cConf,Configuration hConf,HBaseTableUtil hBaseTableUtil,QueueAdmin queueAdmin,TransactionExecutorFactory txExecutorFactory){
  this.cConf=cConf;
  this.hConf=hConf;
  this.queueAdmin=(HBaseQueueAdmin)queueAdmin;
  this.queueUtil=new HBaseQueueUtilFactory().get();
  this.hBaseTableUtil=hBaseTableUtil;
  this.txExecutorFactory=txExecutorFactory;
}"
7107,"/** 
 * Helper method to select the queue or stream admin, and to ensure it's table exists.
 * @param queueName name of the queue to be opened.
 * @return the queue admin for that queue.
 * @throws IOException
 */
private HBaseQueueAdmin ensureTableExists(QueueName queueName) throws IOException {
  HBaseQueueAdmin admin=queueName.isStream() ? streamAdmin : queueAdmin;
  try {
    if (!admin.exists(queueName)) {
      admin.create(queueName);
    }
  }
 catch (  Exception e) {
    throw new IOException(""String_Node_Str"" + admin.getDataTableId(queueName),e);
  }
  return admin;
}","/** 
 * Helper method to select the queue or stream admin, and to ensure it's table exists.
 * @param queueName name of the queue to be opened.
 * @return the queue admin for that queue.
 * @throws IOException
 */
private HBaseQueueAdmin ensureTableExists(QueueName queueName) throws IOException {
  try {
    if (!queueAdmin.exists(queueName)) {
      queueAdmin.create(queueName);
    }
  }
 catch (  Exception e) {
    throw new IOException(""String_Node_Str"" + queueAdmin.getDataTableId(queueName),e);
  }
  return queueAdmin;
}"
7108,"@Override public void drop(Id.Stream streamId) throws Exception {
  drop(QueueName.fromStream(streamId));
}","@Override public void drop(Id.Stream streamId) throws Exception {
  drop(QueueName.fromStream(streamId));
  streamMetaStore.removeStream(streamId);
}"
7109,"@Inject public InMemoryStreamAdmin(InMemoryQueueService queueService,UsageRegistry usageRegistry){
  super(queueService);
  this.usageRegistry=usageRegistry;
}","@Inject public InMemoryStreamAdmin(InMemoryQueueService queueService,UsageRegistry usageRegistry,StreamMetaStore streamMetaStore){
  super(queueService);
  this.usageRegistry=usageRegistry;
  this.streamMetaStore=streamMetaStore;
}"
7110,"@Override public void dropAllInNamespace(Id.Namespace namespace) throws Exception {
  queueService.resetStreamsWithPrefix(QueueName.prefixForNamedspacedStream(namespace.getId()));
}","@Override public void dropAllInNamespace(Id.Namespace namespace) throws Exception {
  queueService.resetStreamsWithPrefix(QueueName.prefixForNamedspacedStream(namespace.getId()));
  for (  StreamSpecification spec : streamMetaStore.listStreams(namespace)) {
    streamMetaStore.removeStream(Id.Stream.from(namespace,spec.getName()));
  }
}"
7111,"@Override public void create(Id.Stream streamId,@Nullable Properties props) throws Exception {
  create(QueueName.fromStream(streamId),props);
}","@Override public void create(Id.Stream streamId,@Nullable Properties props) throws Exception {
  create(QueueName.fromStream(streamId),props);
  streamMetaStore.addStream(streamId);
}"
7112,"private void stopWorkflowAdapter(Id.Namespace namespace,AdapterSpecification adapterSpec) throws NotFoundException, SchedulerException, ExecutionException, InterruptedException {
  Id.Program workflowId=getProgramId(namespace,adapterSpec);
  String scheduleName=adapterSpec.getScheduleSpec().getSchedule().getName();
  scheduler.deleteSchedule(workflowId,SchedulableProgramType.WORKFLOW,scheduleName);
  store.deleteSchedule(workflowId,SchedulableProgramType.WORKFLOW,scheduleName);
  List<RunRecord> activeRuns=getRuns(namespace,adapterSpec.getName(),ProgramRunStatus.RUNNING,0,Long.MAX_VALUE,Integer.MAX_VALUE);
  for (  RunRecord record : activeRuns) {
    lifecycleService.stopProgram(RunIds.fromString(record.getPid()));
  }
}","private void stopWorkflowAdapter(Id.Namespace namespace,AdapterSpecification adapterSpec) throws NotFoundException, SchedulerException, ExecutionException, InterruptedException {
  Id.Program workflowId=getProgramId(namespace,adapterSpec);
  String scheduleName=adapterSpec.getScheduleSpec().getSchedule().getName();
  try {
    scheduler.deleteSchedule(workflowId,SchedulableProgramType.WORKFLOW,scheduleName);
    store.deleteSchedule(workflowId,SchedulableProgramType.WORKFLOW,scheduleName);
  }
 catch (  NotFoundException e) {
  }
  List<RunRecord> activeRuns=getRuns(namespace,adapterSpec.getName(),ProgramRunStatus.RUNNING,0,Long.MAX_VALUE,Integer.MAX_VALUE);
  for (  RunRecord record : activeRuns) {
    lifecycleService.stopProgram(RunIds.fromString(record.getPid()));
  }
}"
7113,"@Override public synchronized RuntimeInfo lookup(final RunId runId){
  RuntimeInfo runtimeInfo=super.lookup(runId);
  if (runtimeInfo != null) {
    return runtimeInfo;
  }
  String appName=null;
  TwillController controller=null;
  for (  TwillRunner.LiveInfo liveInfo : twillRunner.lookupLive()) {
    for (    TwillController c : liveInfo.getControllers()) {
      if (c.getRunId().equals(runId)) {
        appName=liveInfo.getApplicationName();
        controller=c;
        break;
      }
    }
    if (controller != null) {
      break;
    }
  }
  if (controller == null) {
    LOG.info(""String_Node_Str"",runId);
    return null;
  }
  Matcher matcher=APP_NAME_PATTERN.matcher(appName);
  if (!matcher.matches()) {
    LOG.warn(""String_Node_Str"",appName);
    return null;
  }
  ProgramType type=getType(matcher.group(1));
  if (type == null) {
    LOG.warn(""String_Node_Str"",appName);
    return null;
  }
  Id.Program programId=Id.Program.from(matcher.group(2),matcher.group(3),type,matcher.group(4));
  if (runtimeInfo != null) {
    runtimeInfo=createRuntimeInfo(type,programId,controller);
    updateRuntimeInfo(type,runId,runtimeInfo);
    return runtimeInfo;
  }
 else {
    LOG.warn(""String_Node_Str"",type,programId);
    return null;
  }
}","@Override public synchronized RuntimeInfo lookup(final RunId runId){
  RuntimeInfo runtimeInfo=super.lookup(runId);
  if (runtimeInfo != null) {
    return runtimeInfo;
  }
  String appName=null;
  TwillController controller=null;
  for (  TwillRunner.LiveInfo liveInfo : twillRunner.lookupLive()) {
    for (    TwillController c : liveInfo.getControllers()) {
      if (c.getRunId().equals(runId)) {
        appName=liveInfo.getApplicationName();
        controller=c;
        break;
      }
    }
    if (controller != null) {
      break;
    }
  }
  if (controller == null) {
    LOG.info(""String_Node_Str"",runId);
    return null;
  }
  Matcher matcher=APP_NAME_PATTERN.matcher(appName);
  if (!matcher.matches()) {
    LOG.warn(""String_Node_Str"",appName);
    return null;
  }
  ProgramType type=getType(matcher.group(1));
  if (type == null) {
    LOG.warn(""String_Node_Str"",appName);
    return null;
  }
  Id.Program programId=Id.Program.from(matcher.group(2),matcher.group(3),type,matcher.group(4));
  runtimeInfo=createRuntimeInfo(type,programId,controller);
  if (runtimeInfo != null) {
    updateRuntimeInfo(type,runId,runtimeInfo);
    return runtimeInfo;
  }
 else {
    LOG.warn(""String_Node_Str"",type,programId);
    return null;
  }
}"
7114,"public <T>Map<MDSKey,T> listKV(MDSKey startId,@Nullable MDSKey stopId,Type typeOfT,int limit,Predicate<T> filter){
  byte[] startKey=startId.getKey();
  byte[] stopKey=stopId == null ? Bytes.stopKeyForPrefix(startKey) : stopId.getKey();
  try {
    Map<MDSKey,T> map=Maps.newLinkedHashMap();
    Scanner scan=table.scan(startKey,stopKey);
    try {
      Row next;
      while ((limit-- > 0) && (next=scan.next()) != null) {
        byte[] columnValue=next.get(COLUMN);
        if (columnValue == null) {
          continue;
        }
        T value=deserialize(columnValue,typeOfT);
        if (filter.apply(value)) {
          MDSKey key=new MDSKey(next.getRow());
          map.put(key,value);
          --limit;
        }
      }
      return map;
    }
  finally {
      scan.close();
    }
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","public <T>Map<MDSKey,T> listKV(MDSKey startId,@Nullable MDSKey stopId,Type typeOfT,int limit,Predicate<T> filter){
  byte[] startKey=startId.getKey();
  byte[] stopKey=stopId == null ? Bytes.stopKeyForPrefix(startKey) : stopId.getKey();
  try {
    Map<MDSKey,T> map=Maps.newLinkedHashMap();
    Scanner scan=table.scan(startKey,stopKey);
    try {
      Row next;
      while ((limit > 0) && (next=scan.next()) != null) {
        byte[] columnValue=next.get(COLUMN);
        if (columnValue == null) {
          continue;
        }
        T value=deserialize(columnValue,typeOfT);
        if (filter.apply(value)) {
          MDSKey key=new MDSKey(next.getRow());
          map.put(key,value);
          --limit;
        }
      }
      return map;
    }
  finally {
      scan.close();
    }
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
7115,"@Test public void testLogProgramRunHistory() throws Exception {
  Id.Program programId=Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.FLOW,""String_Node_Str"");
  long now=System.currentTimeMillis();
  long nowSecs=TimeUnit.MILLISECONDS.toSeconds(now);
  RunId run1=RunIds.generate(now - 20000);
  store.setStart(programId,run1.getId(),runIdToSecs(run1));
  store.setStop(programId,run1.getId(),nowSecs - 10,ProgramController.State.ERROR.getRunStatus());
  RunId run2=RunIds.generate(now - 10000);
  store.setStart(programId,run2.getId(),runIdToSecs(run2));
  store.setStop(programId,run2.getId(),nowSecs - 5,ProgramController.State.COMPLETED.getRunStatus());
  RunId run3=RunIds.generate(now);
  store.setStart(programId,run3.getId(),runIdToSecs(run3));
  Id.Program programId2=Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.FLOW,""String_Node_Str"");
  RunId run4=RunIds.generate(now - 5000);
  store.setStart(programId2,run4.getId(),runIdToSecs(run4));
  store.setStop(programId2,run4.getId(),nowSecs - 4,ProgramController.State.COMPLETED.getRunStatus());
  store.setStart(Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.FLOW,""String_Node_Str""),run3.getId(),RunIds.getTime(run3,TimeUnit.MILLISECONDS));
  List<RunRecord> successHistory=store.getRuns(programId,ProgramRunStatus.COMPLETED,0,Long.MAX_VALUE,Integer.MAX_VALUE);
  List<RunRecord> failureHistory=store.getRuns(programId,ProgramRunStatus.FAILED,nowSecs - 20,nowSecs - 10,Integer.MAX_VALUE);
  Assert.assertEquals(failureHistory,store.getRuns(programId,ProgramRunStatus.FAILED,0,Long.MAX_VALUE,Integer.MAX_VALUE));
  Assert.assertEquals(1,successHistory.size());
  Assert.assertEquals(1,failureHistory.size());
  RunRecord run=successHistory.get(0);
  Assert.assertEquals(nowSecs - 10,run.getStartTs());
  Assert.assertEquals(nowSecs - 5,run.getStopTs());
  Assert.assertEquals(ProgramController.State.COMPLETED.getRunStatus(),run.getStatus());
  run=failureHistory.get(0);
  Assert.assertEquals(nowSecs - 20,run.getStartTs());
  Assert.assertEquals(nowSecs - 10,run.getStopTs());
  Assert.assertEquals(ProgramController.State.ERROR.getRunStatus(),run.getStatus());
  List<RunRecord> allHistory=store.getRuns(programId,ProgramRunStatus.ALL,nowSecs - 20,nowSecs + 1,Integer.MAX_VALUE);
  Assert.assertEquals(allHistory.toString(),3,allHistory.size());
  List<RunRecord> runningHistory=store.getRuns(programId,ProgramRunStatus.RUNNING,nowSecs,nowSecs + 1,100);
  Assert.assertEquals(1,runningHistory.size());
  Assert.assertEquals(runningHistory,store.getRuns(programId,ProgramRunStatus.RUNNING,0,Long.MAX_VALUE,100));
  RunRecord expectedRunning=runningHistory.get(0);
  Assert.assertNotNull(expectedRunning);
  RunRecord actualRunning=store.getRun(programId,expectedRunning.getPid());
  Assert.assertEquals(expectedRunning,actualRunning);
  RunRecord expectedCompleted=successHistory.get(0);
  Assert.assertNotNull(expectedCompleted);
  RunRecord actualCompleted=store.getRun(programId,expectedCompleted.getPid());
  Assert.assertEquals(expectedCompleted,actualCompleted);
  RunId run5=RunIds.fromString(UUID.randomUUID().toString());
  store.setStart(programId,run5.getId(),nowSecs - 8);
  store.setStop(programId,run5.getId(),nowSecs - 4,ProgramController.State.COMPLETED.getRunStatus());
  RunId run6=RunIds.fromString(UUID.randomUUID().toString());
  store.setStart(programId,run6.getId(),nowSecs - 2);
  RunRecord expectedRecord5=new RunRecord(run5.getId(),nowSecs - 8,nowSecs - 4,ProgramRunStatus.COMPLETED);
  RunRecord actualRecord5=store.getRun(programId,run5.getId());
  Assert.assertEquals(expectedRecord5,actualRecord5);
  RunRecord expectedRecord6=new RunRecord(run6.getId(),nowSecs - 2,null,ProgramRunStatus.RUNNING);
  RunRecord actualRecord6=store.getRun(programId,run6.getId());
  Assert.assertEquals(expectedRecord6,actualRecord6);
  Assert.assertNull(store.getRun(programId,UUID.randomUUID().toString()));
  Assert.assertTrue(store.getRuns(programId,ProgramRunStatus.COMPLETED,nowSecs - 5000,nowSecs - 2000,Integer.MAX_VALUE).isEmpty());
  Assert.assertTrue(store.getRuns(programId,ProgramRunStatus.ALL,nowSecs - 5000,nowSecs - 2000,Integer.MAX_VALUE).isEmpty());
}","@Test public void testLogProgramRunHistory() throws Exception {
  Id.Program programId=Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.FLOW,""String_Node_Str"");
  long now=System.currentTimeMillis();
  long nowSecs=TimeUnit.MILLISECONDS.toSeconds(now);
  RunId run1=RunIds.generate(now - 20000);
  store.setStart(programId,run1.getId(),runIdToSecs(run1));
  store.setStop(programId,run1.getId(),nowSecs - 10,ProgramController.State.ERROR.getRunStatus());
  RunId run2=RunIds.generate(now - 10000);
  store.setStart(programId,run2.getId(),runIdToSecs(run2));
  store.setStop(programId,run2.getId(),nowSecs - 5,ProgramController.State.COMPLETED.getRunStatus());
  RunId run3=RunIds.generate(now);
  store.setStart(programId,run3.getId(),runIdToSecs(run3));
  RunRecord runRecord=store.getRun(programId,run3.getId());
  Assert.assertNull(runRecord.getStopTs());
  Id.Program programId2=Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.FLOW,""String_Node_Str"");
  RunId run4=RunIds.generate(now - 5000);
  store.setStart(programId2,run4.getId(),runIdToSecs(run4));
  store.setStop(programId2,run4.getId(),nowSecs - 4,ProgramController.State.COMPLETED.getRunStatus());
  store.setStart(Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.FLOW,""String_Node_Str""),run3.getId(),RunIds.getTime(run3,TimeUnit.MILLISECONDS));
  List<RunRecord> successHistory=store.getRuns(programId,ProgramRunStatus.COMPLETED,0,Long.MAX_VALUE,Integer.MAX_VALUE);
  List<RunRecord> failureHistory=store.getRuns(programId,ProgramRunStatus.FAILED,nowSecs - 20,nowSecs - 10,Integer.MAX_VALUE);
  Assert.assertEquals(failureHistory,store.getRuns(programId,ProgramRunStatus.FAILED,0,Long.MAX_VALUE,Integer.MAX_VALUE));
  Assert.assertEquals(1,successHistory.size());
  Assert.assertEquals(1,failureHistory.size());
  RunRecord run=successHistory.get(0);
  Assert.assertEquals(nowSecs - 10,run.getStartTs());
  Assert.assertEquals(Long.valueOf(nowSecs - 5),run.getStopTs());
  Assert.assertEquals(ProgramController.State.COMPLETED.getRunStatus(),run.getStatus());
  run=failureHistory.get(0);
  Assert.assertEquals(nowSecs - 20,run.getStartTs());
  Assert.assertEquals(Long.valueOf(nowSecs - 10),run.getStopTs());
  Assert.assertEquals(ProgramController.State.ERROR.getRunStatus(),run.getStatus());
  List<RunRecord> allHistory=store.getRuns(programId,ProgramRunStatus.ALL,nowSecs - 20,nowSecs + 1,Integer.MAX_VALUE);
  Assert.assertEquals(allHistory.toString(),3,allHistory.size());
  List<RunRecord> runningHistory=store.getRuns(programId,ProgramRunStatus.RUNNING,nowSecs,nowSecs + 1,100);
  Assert.assertEquals(1,runningHistory.size());
  Assert.assertEquals(runningHistory,store.getRuns(programId,ProgramRunStatus.RUNNING,0,Long.MAX_VALUE,100));
  RunRecord expectedRunning=runningHistory.get(0);
  Assert.assertNotNull(expectedRunning);
  RunRecord actualRunning=store.getRun(programId,expectedRunning.getPid());
  Assert.assertEquals(expectedRunning,actualRunning);
  RunRecord expectedCompleted=successHistory.get(0);
  Assert.assertNotNull(expectedCompleted);
  RunRecord actualCompleted=store.getRun(programId,expectedCompleted.getPid());
  Assert.assertEquals(expectedCompleted,actualCompleted);
  RunId run5=RunIds.fromString(UUID.randomUUID().toString());
  store.setStart(programId,run5.getId(),nowSecs - 8);
  store.setStop(programId,run5.getId(),nowSecs - 4,ProgramController.State.COMPLETED.getRunStatus());
  RunId run6=RunIds.fromString(UUID.randomUUID().toString());
  store.setStart(programId,run6.getId(),nowSecs - 2);
  RunRecord expectedRecord5=new RunRecord(run5.getId(),nowSecs - 8,nowSecs - 4,ProgramRunStatus.COMPLETED);
  RunRecord actualRecord5=store.getRun(programId,run5.getId());
  Assert.assertEquals(expectedRecord5,actualRecord5);
  RunRecord expectedRecord6=new RunRecord(run6.getId(),nowSecs - 2,null,ProgramRunStatus.RUNNING);
  RunRecord actualRecord6=store.getRun(programId,run6.getId());
  Assert.assertEquals(expectedRecord6,actualRecord6);
  Assert.assertNull(store.getRun(programId,UUID.randomUUID().toString()));
  Assert.assertTrue(store.getRuns(programId,ProgramRunStatus.COMPLETED,nowSecs - 5000,nowSecs - 2000,Integer.MAX_VALUE).isEmpty());
  Assert.assertTrue(store.getRuns(programId,ProgramRunStatus.ALL,nowSecs - 5000,nowSecs - 2000,Integer.MAX_VALUE).isEmpty());
}"
7116,"public RunRecord(RunRecord started,long stopTs,ProgramRunStatus status){
  this(started.pid,started.startTs,stopTs,status,started.getAdapterName());
}","public RunRecord(RunRecord started,@Nullable Long stopTs,ProgramRunStatus status){
  this(started.pid,started.startTs,stopTs,status,started.getAdapterName());
}"
7117,"public long getStopTs(){
  return stopTs;
}","@Nullable public Long getStopTs(){
  return stopTs;
}"
7118,"@Override public MetricsCollector childCollector(Map<String,String> tags){
  if (tags.isEmpty()) {
    return this;
  }
  Map<String,String> allTags=Maps.newHashMap();
  allTags.putAll(this.tags);
  allTags.putAll(tags);
  return new MetricsCollectorImpl(allTags);
}","@Override public MetricsCollector childCollector(Map<String,String> tags){
  if (tags.isEmpty()) {
    return this;
  }
  Map<String,String> allTags=Maps.newHashMap();
  allTags.putAll(this.tags);
  allTags.putAll(tags);
  return collectors.getUnchecked(allTags);
}"
7119,"private MetricsCollectorImpl(final Map<String,String> tags){
  this.tags=tags;
  emitters.put(tags,new AggregatedMetricsEmitter(tags));
}","private MetricsCollectorImpl(final Map<String,String> tags){
  this.tags=tags;
}"
7120,"public V getEmitted(K key){
  return emitted.get(key);
}","public List<Entry<K,V>> getEmitted(){
  return emitted;
}"
7121,"@Override public void emit(K key,V value){
  emitted.put(key,value);
}","@Override public void emit(K key,V value){
  emitted.add(new Entry(key,value));
}"
7122,"@Test public void testTransform() throws Exception {
  byte[] rowKey=Bytes.toBytes(28);
  final Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.BOOLEAN))),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.LONG))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.FLOAT))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.DOUBLE))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.BYTES))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))));
  Map<byte[],byte[]> inputColumns=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  inputColumns.put(Bytes.toBytes(""String_Node_Str""),Bytes.toBytes(true));
  inputColumns.put(Bytes.toBytes(""String_Node_Str""),Bytes.toBytes(512L));
  inputColumns.put(Bytes.toBytes(""String_Node_Str""),Bytes.toBytes(3.14f));
  inputColumns.put(Bytes.toBytes(""String_Node_Str""),Bytes.toBytes(""String_Node_Str""));
  inputColumns.put(Bytes.toBytes(""String_Node_Str""),Bytes.toBytes(""String_Node_Str""));
  inputColumns.put(Bytes.toBytes(""String_Node_Str""),Bytes.toBytes(""String_Node_Str""));
  Row input=new Result(rowKey,inputColumns);
  Transform transform=new RowToStructuredRecordTransform();
  TransformContext transformContext=new MockTransformContext(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",schema.toString()));
  transform.initialize(transformContext);
  MockEmitter<byte[],StructuredRecord> emitter=new MockEmitter<byte[],StructuredRecord>();
  transform.transform(rowKey,input,emitter);
  StructuredRecord actual=emitter.getEmitted(rowKey);
  Assert.assertTrue((Boolean)actual.get(""String_Node_Str""));
  Assert.assertEquals(512L,actual.get(""String_Node_Str""));
  Assert.assertTrue(Math.abs(3.14f - (Float)actual.get(""String_Node_Str"")) < 0.000001);
  Assert.assertEquals(""String_Node_Str"",Bytes.toString((byte[])actual.get(""String_Node_Str"")));
  Assert.assertEquals(""String_Node_Str"",actual.get(""String_Node_Str""));
  Assert.assertNull(actual.get(""String_Node_Str""));
  Assert.assertNull(actual.get(""String_Node_Str""));
}","@Test public void testTransform() throws Exception {
  byte[] rowKey=Bytes.toBytes(28);
  final Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.BOOLEAN))),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.LONG))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.FLOAT))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.DOUBLE))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.BYTES))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))));
  Map<byte[],byte[]> inputColumns=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  inputColumns.put(Bytes.toBytes(""String_Node_Str""),Bytes.toBytes(true));
  inputColumns.put(Bytes.toBytes(""String_Node_Str""),Bytes.toBytes(512L));
  inputColumns.put(Bytes.toBytes(""String_Node_Str""),Bytes.toBytes(3.14f));
  inputColumns.put(Bytes.toBytes(""String_Node_Str""),Bytes.toBytes(""String_Node_Str""));
  inputColumns.put(Bytes.toBytes(""String_Node_Str""),Bytes.toBytes(""String_Node_Str""));
  inputColumns.put(Bytes.toBytes(""String_Node_Str""),Bytes.toBytes(""String_Node_Str""));
  Row input=new Result(rowKey,inputColumns);
  Transform transform=new RowToStructuredRecordTransform();
  TransformContext transformContext=new MockTransformContext(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",schema.toString()));
  transform.initialize(transformContext);
  MockEmitter<byte[],StructuredRecord> emitter=new MockEmitter<byte[],StructuredRecord>();
  transform.transform(rowKey,input,emitter);
  StructuredRecord actual=emitter.getEmitted().get(0).getVal();
  Assert.assertTrue((Boolean)actual.get(""String_Node_Str""));
  Assert.assertEquals(512L,actual.get(""String_Node_Str""));
  Assert.assertTrue(Math.abs(3.14f - (Float)actual.get(""String_Node_Str"")) < 0.000001);
  Assert.assertEquals(""String_Node_Str"",Bytes.toString((byte[])actual.get(""String_Node_Str"")));
  Assert.assertEquals(""String_Node_Str"",actual.get(""String_Node_Str""));
  Assert.assertNull(actual.get(""String_Node_Str""));
  Assert.assertNull(actual.get(""String_Node_Str""));
}"
7123,"/** 
 * Check if this is a simple type or a nullable simple type, which is a union of a null and one other non-null simple type, where a simple type is a boolean, int, long, float, double, bytes, or string type.
 * @return whether or not this is a nullable simple type.
 */
public boolean isSimpleOrNullableSimple(){
  return type.isSimpleType() || (type == Type.UNION && getNonNullable().getType().isSimpleType());
}","/** 
 * Check if this is a simple type or a nullable simple type, which is a union of a null and one other non-null simple type, where a simple type is a boolean, int, long, float, double, bytes, or string type.
 * @return whether or not this is a nullable simple type.
 */
public boolean isSimpleOrNullableSimple(){
  return type.isSimpleType() || isNullableSimple();
}"
7124,abstract Object get(byte[] rowKey);,T get(byte[] rowKey);
7125,"/** 
 * Returns the number of instances for all program runnables that are passed into the data. The data is an array of Json objects where each object must contain the following three elements: appId, programType, and programId (flow name, service name, or procedure name). Retrieving instances only applies to flows, procedures, and user services. For flows and procedures, another parameter, ""runnableId"", must be provided. This corresponds to the flowlet/runnable for which to retrieve the instances. This does not apply to procedures. Example input: [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""runnableId"": ""Runnable1""}, {""appId"": ""App1"", ""programType"": ""Procedure"", ""programId"": ""Proc2""}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1"", ""runnableId"": ""Flowlet1""}] The response will be an array of JsonObjects each of which will contain the three input parameters as well as 3 fields: ""provisioned"" which maps to the number of instances actually provided for the input runnable, ""requested"" which maps to the number of instances the user has requested for the input runnable, ""statusCode"" which maps to the http status code for the data in that JsonObjects. (200, 400, 404) If an error occurs in the input (i.e. in the example above, Flowlet1 does not exist), then all JsonObjects for which the parameters have a valid instances will have the provisioned and requested fields status code fields but all JsonObjects for which the parameters are not valid will have an error message and statusCode. E.g. given the above data, if there is no Flowlet1, then the response would be 200 OK with following possible data: [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""runnableId"": ""Runnable1"", ""statusCode"": 200, ""provisioned"": 2, ""requested"": 2}, {""appId"": ""App1"", ""programType"": ""Procedure"", ""programId"": ""Proc2"", ""statusCode"": 200, ""provisioned"": 1, ""requested"": 3}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1"", ""runnableId"": ""Flowlet1"", ""statusCode"": 404, ""error"": ""Runnable"": Flowlet1 not found""}]
 */
@POST @Path(""String_Node_Str"") public void getInstances(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getInstances(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}","/** 
 * Returns the number of instances for all program runnables that are passed into the data. The data is an array of Json objects where each object must contain the following three elements: appId, programType, and programId (flow name, service name, or procedure name). Retrieving instances only applies to flows, procedures, and user services. For flows and procedures, another parameter, ""runnableId"", must be provided. This corresponds to the flowlet/runnable for which to retrieve the instances. This does not apply to procedures. <p> Example input: <pre><code> [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""runnableId"": ""Runnable1""}, {""appId"": ""App1"", ""programType"": ""Mapreduce"", ""programId"": ""Mapreduce2""}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1"", ""runnableId"": ""Flowlet1""}] </code></pre> </p><p> The response will be an array of JsonObjects each of which will contain the three input parameters as well as 3 fields: </p><ul> <li>""provisioned"" which maps to the number of instances actually provided for the input runnable,</li> <li>""requested"" which maps to the number of instances the user has requested for the input runnable,</li> <li>""statusCode"" which maps to the http status code for the data in that JsonObjects (200, 400, 404).</li> </p><p> If an error occurs in the input (i.e. in the example above, Flowlet1 does not exist), then all JsonObjects for which the parameters have a valid instances will have the provisioned and requested fields status code fields but all JsonObjects for which the parameters are not valid will have an error message and statusCode. </p><p> For example, if there is no Flowlet1 in the above data, then the response could be 200 OK with the following data: </p> <pre><code> [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""runnableId"": ""Runnable1"", ""statusCode"": 200, ""provisioned"": 2, ""requested"": 2}, {""appId"": ""App1"", ""programType"": ""Mapreduce"", ""programId"": ""Mapreduce2"", ""statusCode"": 200, ""provisioned"": 1, ""requested"": 3}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1"", ""runnableId"": ""Flowlet1"", ""statusCode"": 404, ""error"": ""Runnable"": Flowlet1 not found""}]
 */
@POST @Path(""String_Node_Str"") public void getInstances(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getInstances(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}"
7126,"/** 
 * Returns the status for all programs that are passed into the data. The data is an array of Json objects where each object must contain the following three elements: appId, programType, and programId (flow name, service name, etc.). <p/> Example input: [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1""}, {""appId"": ""App1"", ""programType"": ""Procedure"", ""programId"": ""Proc2""}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1""}] <p/> The response will be an array of JsonObjects each of which will contain the three input parameters as well as 2 fields, ""status"" which maps to the status of the program and ""statusCode"" which maps to the status code for the data in that JsonObjects. If an error occurs in the input (i.e. in the example above, App2 does not exist), then all JsonObjects for which the parameters have a valid status will have the status field but all JsonObjects for which the parameters do not have a valid status will have an error message and statusCode. <p/> For example, if there is no App2 in the data above, then the response would be 200 OK with following possible data: [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""statusCode"": 200, ""status"": ""RUNNING""}, {""appId"": ""App1"", ""programType"": ""Procedure"", ""programId"": ""Proc2""}, ""statusCode"": 200, ""status"": ""STOPPED""}, {""appId"":""App2"", ""programType"":""Flow"", ""programId"":""Flow1"", ""statusCode"":404, ""error"": ""App: App2 not found""}]
 */
@POST @Path(""String_Node_Str"") public void getStatuses(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getStatuses(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}","/** 
 * Returns the status for all programs that are passed into the data. The data is an array of Json objects where each object must contain the following three elements: appId, programType, and programId (flow name, service name, etc.). <p> Example input: <pre><code> [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1""}, {""appId"": ""App1"", ""programType"": ""Mapreduce"", ""programId"": ""Mapreduce2""}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1""}] </code></pre> </p><p> The response will be an array of JsonObjects each of which will contain the three input parameters as well as 2 fields: <ul> <li>""status"" which maps to the status of the program and </li> <li>""statusCode"" which maps to the status code for the data in that JsonObjects.</li> </ul> </p><p> If an error occurs in the input (i.e. in the example above, App2 does not exist), then all JsonObjects for which the parameters have a valid status will have the status field but all JsonObjects for which the parameters do not have a valid status will have an error message and statusCode. </p><p> For example, if there is no App2 in the data above, then the response could be 200 OK with the following data: </p> <pre><code> [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""statusCode"": 200, ""status"": ""RUNNING""}, {""appId"": ""App1"", ""programType"": ""Mapreduce"", ""programId"": ""Mapreduce2"", ""statusCode"": 200, ""status"": ""STOPPED""}, {""appId"":""App2"", ""programType"":""Flow"", ""programId"":""Flow1"", ""statusCode"":404, ""error"": ""App: App2 not found""}] </code></pre>
 */
@POST @Path(""String_Node_Str"") public void getStatuses(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getStatuses(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}"
7127,"public Iterator<MetricValue> getMetaMetrics(){
  long currentTimeMs=System.currentTimeMillis();
  long currentTimeSec=TimeUnit.MILLISECONDS.toSeconds(currentTimeMs);
  MetricValue delayAvg=new MetricValue(ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE,""String_Node_Str""),""String_Node_Str"",currentTimeSec,currentTimeMs - TimeUnit.SECONDS.toMillis((long)processDelayStats.getAverage()),MetricType.GAUGE);
  MetricValue delayMin=new MetricValue(ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE,""String_Node_Str""),""String_Node_Str"",currentTimeSec,currentTimeMs - TimeUnit.SECONDS.toMillis(processDelayStats.getMin()),MetricType.GAUGE);
  MetricValue delayMax=new MetricValue(ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE,""String_Node_Str""),""String_Node_Str"",currentTimeSec,currentTimeMs - TimeUnit.SECONDS.toMillis(processDelayStats.getMax()),MetricType.GAUGE);
  MetricValue count=new MetricValue(ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE,""String_Node_Str""),""String_Node_Str"",currentTimeSec,processDelayStats.getCount(),MetricType.COUNTER);
  return ImmutableList.of(delayAvg,delayMin,delayMax,count).iterator();
}","public Iterator<MetricValue> getMetaMetrics(){
  if (processDelayStats.isEmpty()) {
    return Iterators.emptyIterator();
  }
  long currentTimeMs=System.currentTimeMillis();
  long currentTimeSec=TimeUnit.MILLISECONDS.toSeconds(currentTimeMs);
  MetricValue delayAvg=new MetricValue(ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE,""String_Node_Str""),""String_Node_Str"",currentTimeSec,currentTimeMs - TimeUnit.SECONDS.toMillis(processDelayStats.getAverage()),MetricType.GAUGE);
  MetricValue delayMin=new MetricValue(ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE,""String_Node_Str""),""String_Node_Str"",currentTimeSec,currentTimeMs - TimeUnit.SECONDS.toMillis(processDelayStats.getMin()),MetricType.GAUGE);
  MetricValue delayMax=new MetricValue(ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE,""String_Node_Str""),""String_Node_Str"",currentTimeSec,currentTimeMs - TimeUnit.SECONDS.toMillis(processDelayStats.getMax()),MetricType.GAUGE);
  MetricValue count=new MetricValue(ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE,""String_Node_Str""),""String_Node_Str"",currentTimeSec,processDelayStats.getCount(),MetricType.COUNTER);
  return ImmutableList.of(delayAvg,delayMin,delayMax,count).iterator();
}"
7128,"public float getAverage(){
  long sum=0;
  for (  Long value : values) {
    int valueCount=values.count(value);
    sum+=value * valueCount;
  }
  return sum * 1.0f / count;
}","public long getAverage(){
  if (count == 0) {
    return 0;
  }
  double average=0;
  for (  Long value : values.elementSet()) {
    int valueCount=values.count(value);
    average+=value * (valueCount * 1.0 / count);
  }
  return (long)average;
}"
7129,"@Test public void testTTL() throws Exception {
  File inputDir=tmpFolder.newFolder();
  File outputDir=tmpFolder.newFolder();
  outputDir.delete();
  final long currentTime=CURRENT_TIME;
  final long ttl=1500;
  generateEvents(inputDir,500,0,1,new GenerateEvent(){
    @Override public String generate(    int index,    long timestamp){
      return ""String_Node_Str"" + timestamp;
    }
  }
);
  generateEvents(inputDir,1000,0,1,new GenerateEvent(){
    @Override public String generate(    int index,    long timestamp){
      if (timestamp + ttl < currentTime) {
        return ""String_Node_Str"" + timestamp;
      }
 else {
        return ""String_Node_Str"" + timestamp;
      }
    }
  }
);
  generateEvents(inputDir,1000,1000,1,new GenerateEvent(){
    @Override public String generate(    int index,    long timestamp){
      return ""String_Node_Str"" + timestamp;
    }
  }
);
  runMR(inputDir,outputDir,0,Long.MAX_VALUE,2000,1500);
  Map<String,Integer> output=loadMRResult(outputDir);
  Assert.assertEquals(1501,output.size());
  Assert.assertEquals(null,output.get(""String_Node_Str""));
  Assert.assertEquals(1500,output.get(""String_Node_Str"").intValue());
  for (long i=(currentTime - ttl); i < currentTime; i++) {
    Assert.assertEquals(1,output.get(Long.toString(i)).intValue());
  }
}","@Test public void testTTL() throws Exception {
  File inputDir=tmpFolder.newFolder();
  File outputDir=tmpFolder.newFolder();
  outputDir.delete();
  final long currentTime=CURRENT_TIME;
  final long ttl=1500;
  generateEvents(inputDir,500,0,1,new GenerateEvent(){
    @Override public String generate(    int index,    long timestamp){
      return ""String_Node_Str"" + timestamp;
    }
  }
);
  generateEvents(inputDir,1000,0,1,new GenerateEvent(){
    @Override public String generate(    int index,    long timestamp){
      if (timestamp + ttl < currentTime) {
        return ""String_Node_Str"" + timestamp;
      }
 else {
        return ""String_Node_Str"" + timestamp;
      }
    }
  }
);
  generateEvents(inputDir,1000,1000,1,new GenerateEvent(){
    @Override public String generate(    int index,    long timestamp){
      return ""String_Node_Str"" + timestamp;
    }
  }
);
  runMR(inputDir,outputDir,0,Long.MAX_VALUE,2000,ttl);
  Map<String,Integer> output=loadMRResult(outputDir);
  Assert.assertEquals(ttl + 1,output.size());
  Assert.assertEquals(null,output.get(""String_Node_Str""));
  Assert.assertEquals(ttl,output.get(""String_Node_Str"").intValue());
  for (long i=(currentTime - ttl); i < currentTime; i++) {
    Assert.assertEquals(1,output.get(Long.toString(i)).intValue());
  }
}"
7130,"@Test public void testStreamRecordReader() throws Exception {
  File inputDir=tmpFolder.newFolder();
  File partition=new File(inputDir,""String_Node_Str"");
  partition.mkdirs();
  File eventFile=new File(partition,""String_Node_Str"" + StreamFileType.EVENT.getSuffix());
  File indexFile=new File(partition,""String_Node_Str"" + StreamFileType.INDEX.getSuffix());
  StreamDataFileWriter writer=new StreamDataFileWriter(Files.newOutputStreamSupplier(eventFile),Files.newOutputStreamSupplier(indexFile),100L);
  writer.append(StreamFileTestUtils.createEvent(1000,""String_Node_Str""));
  Configuration conf=new Configuration();
  TaskAttemptContext context=new TaskAttemptContextImpl(conf,new TaskAttemptID());
  StreamInputFormat.setStreamPath(conf,inputDir.toURI());
  StreamInputFormat format=new StreamInputFormat();
  List<InputSplit> splits=format.getSplits(new JobContextImpl(new JobConf(conf),new JobID()));
  Assert.assertEquals(2,splits.size());
  writer.append(StreamFileTestUtils.createEvent(1001,""String_Node_Str""));
  writer.close();
  StreamRecordReader<LongWritable,StreamEvent> recordReader=new StreamRecordReader<LongWritable,StreamEvent>(new IdentityStreamEventDecoder());
  recordReader.initialize(splits.get(1),context);
  Assert.assertTrue(recordReader.nextKeyValue());
  StreamEvent output=recordReader.getCurrentValue();
  Assert.assertEquals(1001,output.getTimestamp());
  Assert.assertEquals(""String_Node_Str"",Bytes.toString(output.getBody()));
  Assert.assertFalse(recordReader.nextKeyValue());
}","@Test public void testStreamRecordReader() throws Exception {
  File inputDir=tmpFolder.newFolder();
  File partition=new File(inputDir,""String_Node_Str"");
  partition.mkdirs();
  File eventFile=new File(partition,""String_Node_Str"" + StreamFileType.EVENT.getSuffix());
  File indexFile=new File(partition,""String_Node_Str"" + StreamFileType.INDEX.getSuffix());
  StreamDataFileWriter writer=new StreamDataFileWriter(Files.newOutputStreamSupplier(eventFile),Files.newOutputStreamSupplier(indexFile),100L);
  writer.append(StreamFileTestUtils.createEvent(1000,""String_Node_Str""));
  writer.flush();
  Configuration conf=new Configuration();
  TaskAttemptContext context=new TaskAttemptContextImpl(conf,new TaskAttemptID());
  StreamInputFormat.setStreamPath(conf,inputDir.toURI());
  StreamInputFormat format=new StreamInputFormat();
  List<InputSplit> splits=format.getSplits(new JobContextImpl(new JobConf(conf),new JobID()));
  Assert.assertEquals(2,splits.size());
  writer.append(StreamFileTestUtils.createEvent(1001,""String_Node_Str""));
  writer.close();
  StreamRecordReader<LongWritable,StreamEvent> recordReader=new StreamRecordReader<LongWritable,StreamEvent>(new IdentityStreamEventDecoder());
  recordReader.initialize(splits.get(1),context);
  Assert.assertTrue(recordReader.nextKeyValue());
  StreamEvent output=recordReader.getCurrentValue();
  Assert.assertEquals(1001,output.getTimestamp());
  Assert.assertEquals(""String_Node_Str"",Bytes.toString(output.getBody()));
  Assert.assertFalse(recordReader.nextKeyValue());
}"
7131,"private Map<String,Integer> loadMRResult(File outputDir) throws IOException {
  Map<String,Integer> output=Maps.newHashMap();
  BufferedReader reader=Files.newReader(new File(outputDir,""String_Node_Str""),Charsets.UTF_8);
  try {
    String line=reader.readLine();
    while (line != null) {
      int idx=line.indexOf('\t');
      output.put(line.substring(0,idx),Integer.parseInt(line.substring(idx + 1)));
      line=reader.readLine();
    }
  }
  finally {
    reader.close();
  }
  return output;
}","private Map<String,Integer> loadMRResult(File outputDir) throws IOException {
  Map<String,Integer> output=Maps.newTreeMap();
  BufferedReader reader=Files.newReader(new File(outputDir,""String_Node_Str""),Charsets.UTF_8);
  try {
    String line=reader.readLine();
    while (line != null) {
      int idx=line.indexOf('\t');
      output.put(line.substring(0,idx),Integer.parseInt(line.substring(idx + 1)));
      line=reader.readLine();
    }
  }
  finally {
    reader.close();
  }
  return output;
}"
7132,"/** 
 * Reads or skips a   {@link StreamEvent}.
 * @param filter to determine to accept or skip a stream event by offsetand accept or skip a stream event block by timestamp.
 * @return The next StreamEvent or {@code null} if the event is rejected by the filter or reached EOF.
 */
private PositionStreamEvent nextStreamEvent(ReadFilter filter) throws IOException {
  while (!eof && !streamEventBuffer.hasEvent()) {
    readDataBlock(filter);
  }
  if (eof) {
    return null;
  }
  PositionStreamEvent event=streamEventBuffer.nextEvent(timestamp,eventTemplate.getHeaders(),filter);
  position=streamEventBuffer.getPosition();
  return event;
}","/** 
 * Reads or skips a   {@link StreamEvent}.
 * @param filter to determine to accept or skip a stream event by offsetand accept or skip a stream event block by timestamp.
 * @return The next StreamEvent or {@code null} if the event is rejected by the filter or reached EOF.
 */
private PositionStreamEvent nextStreamEvent(ReadFilter filter) throws IOException {
  while (!eof && !(streamEventBuffer.hasEvent() && acceptTimestamp(filter,timestamp))) {
    readDataBlock(filter);
  }
  if (eof) {
    return null;
  }
  PositionStreamEvent event=streamEventBuffer.nextEvent(timestamp,eventTemplate.getHeaders(),filter);
  position=streamEventBuffer.getPosition();
  return event;
}"
7133,"/** 
 * Skips events until the given condition is true.
 */
private void skipUntil(SkipCondition condition) throws IOException {
  long positionBound=position=eventInput.getPos();
  try {
    while (!eof) {
      long timestamp=readTimestamp();
      eof=timestamp < 0;
      if (eof || condition.apply(positionBound,timestamp)) {
        break;
      }
      int len=readLength();
      position=positionBound;
      eventInput.seek(eventInput.getPos() + len);
      positionBound=eventInput.getPos();
      if (condition.apply(positionBound,timestamp)) {
        break;
      }
    }
    if (eof) {
      position=positionBound;
      return;
    }
    eventInput.seek(position);
    readDataBlock(ReadFilter.ALWAYS_ACCEPT);
    while (position < positionBound) {
      if (condition.apply(position,timestamp)) {
        break;
      }
      nextStreamEvent(ReadFilter.ALWAYS_REJECT_OFFSET);
    }
  }
 catch (  IOException e) {
    if (!(e instanceof EOFException)) {
      throw e;
    }
  }
}","/** 
 * Skips events until the given condition is true.
 */
private void skipUntil(SkipCondition condition) throws IOException {
  long positionBound=position=eventInput.getPos();
  try {
    while (!eof) {
      long timestamp=readTimestamp();
      eof=timestamp < 0;
      if (eof || condition.apply(positionBound,timestamp)) {
        break;
      }
      int len=readLength();
      position=positionBound;
      eventInput.seek(eventInput.getPos() + len);
      positionBound=eventInput.getPos();
      if (condition.apply(positionBound,timestamp)) {
        break;
      }
    }
    if (eof) {
      position=positionBound;
      return;
    }
    eventInput.seek(position);
    readDataBlock(ReadFilter.ALWAYS_ACCEPT);
    while (position < positionBound) {
      if (condition.apply(streamEventBuffer.getPosition(),timestamp)) {
        break;
      }
      nextStreamEvent(ReadFilter.ALWAYS_REJECT_OFFSET);
    }
  }
 catch (  IOException e) {
    if (!(e instanceof EOFException)) {
      throw e;
    }
  }
}"
7134,"private void readDataBlock(ReadFilter filter) throws IOException {
  position=eventInput.getPos();
  long timestamp=readTimestamp();
  if (timestamp < 0) {
    eof=true;
    return;
  }
  filter.reset();
  timestamp=eventTemplate.getTimestamp() >= 0 ? eventTemplate.getTimestamp() : timestamp;
  if (filter.acceptTimestamp(timestamp)) {
    streamEventBuffer.fillBuffer(eventInput,readLength());
    this.timestamp=timestamp;
    return;
  }
  if (eventTemplate.getTimestamp() >= 0) {
    eof=true;
    return;
  }
  long nextTimestamp=filter.getNextTimestampHint();
  if (nextTimestamp > timestamp) {
    eventInput.seek(position);
    initByTime(nextTimestamp);
    readDataBlock(filter);
    return;
  }
  int length=readLength();
  long bytesSkipped=eventInput.skip(length);
  if (bytesSkipped != length) {
    throw new EOFException(""String_Node_Str"" + length + ""String_Node_Str""+ bytesSkipped+ ""String_Node_Str"");
  }
  position=eventInput.getPos();
}","private void readDataBlock(ReadFilter filter) throws IOException {
  position=eventInput.getPos();
  long timestamp=readTimestamp();
  if (timestamp < 0) {
    eof=true;
    return;
  }
  timestamp=eventTemplate.getTimestamp() >= 0 ? eventTemplate.getTimestamp() : timestamp;
  if (acceptTimestamp(filter,timestamp)) {
    streamEventBuffer.fillBuffer(eventInput,readLength());
    this.timestamp=timestamp;
    return;
  }
  if (eventTemplate.getTimestamp() >= 0) {
    eof=true;
    return;
  }
  long nextTimestamp=filter.getNextTimestampHint();
  if (nextTimestamp > timestamp) {
    eventInput.seek(position);
    initByTime(nextTimestamp);
    return;
  }
  int length=readLength();
  long bytesSkipped=eventInput.skip(length);
  if (bytesSkipped != length) {
    throw new EOFException(""String_Node_Str"" + length + ""String_Node_Str""+ bytesSkipped+ ""String_Node_Str"");
  }
  position=eventInput.getPos();
}"
7135,"@Override public boolean nextKeyValue() throws IOException, InterruptedException {
  StreamEvent streamEvent;
  do {
    if (reader.getPosition() - inputSplit.getStart() >= inputSplit.getLength()) {
      return false;
    }
    events.clear();
    if (reader.read(events,1,0,TimeUnit.SECONDS,readFilter) <= 0) {
      return false;
    }
    streamEvent=events.get(0);
  }
 while (streamEvent.getTimestamp() < inputSplit.getStartTime());
  if (streamEvent.getTimestamp() >= inputSplit.getEndTime()) {
    return false;
  }
  currentEntry=decoder.decode(streamEvent,currentEntry);
  return true;
}","@Override public boolean nextKeyValue() throws IOException, InterruptedException {
  events.clear();
  if (reader.read(events,1,0,TimeUnit.SECONDS,readFilter) <= 0) {
    return false;
  }
  PositionStreamEvent streamEvent=events.get(0);
  if (streamEvent.getStart() - inputSplit.getStart() >= inputSplit.getLength()) {
    return false;
  }
  currentEntry=decoder.decode(streamEvent,currentEntry);
  return true;
}"
7136,"@Override public boolean next(Void key,ObjectWritable value) throws IOException {
  StreamEvent streamEvent;
  do {
    if (reader.getPosition() - inputSplit.getStart() >= inputSplit.getLength()) {
      return false;
    }
    events.clear();
    try {
      if (reader.read(events,1,0,TimeUnit.SECONDS,readFilter) <= 0) {
        return false;
      }
    }
 catch (    InterruptedException e) {
      LOG.error(""String_Node_Str"",e);
      return false;
    }
    streamEvent=events.get(0);
  }
 while (streamEvent.getTimestamp() < inputSplit.getStartTime());
  if (streamEvent.getTimestamp() >= inputSplit.getEndTime()) {
    return false;
  }
  value.set(streamEvent);
  return true;
}","@Override public boolean next(Void key,ObjectWritable value) throws IOException {
  events.clear();
  try {
    if (reader.read(events,1,0,TimeUnit.SECONDS,readFilter) <= 0) {
      return false;
    }
    PositionStreamEvent streamEvent=events.get(0);
    if (streamEvent.getStart() - inputSplit.getStart() >= inputSplit.getLength()) {
      return false;
    }
    value.set(streamEvent);
    return true;
  }
 catch (  InterruptedException e) {
    LOG.error(""String_Node_Str"",e);
    return false;
  }
}"
7137,"protected List<QueryResult> fetchNextResults(OperationHandle operationHandle,int size) throws HiveSQLException, ExploreException, HandleNotFoundException {
  startAndWait();
  try {
    if (operationHandle.hasResultSet()) {
      Object rowSet=getCliService().fetchResults(operationHandle,FetchOrientation.FETCH_NEXT,size);
      ImmutableList.Builder<QueryResult> rowsBuilder=ImmutableList.builder();
      if (rowSet instanceof Iterable) {
        for (        Object[] row : (Iterable<Object[]>)rowSet) {
          List<Object> cols=Lists.newArrayList();
          for (int i=0; i < row.length; i++) {
            cols.add(row[i]);
          }
          rowsBuilder.add(new QueryResult(cols));
        }
      }
 else {
        Class rowSetClass=Class.forName(""String_Node_Str"");
        Method toTRowSetMethod=rowSetClass.getMethod(""String_Node_Str"");
        TRowSet tRowSet=(TRowSet)toTRowSetMethod.invoke(rowSet);
        for (        TRow tRow : tRowSet.getRows()) {
          List<Object> cols=Lists.newArrayList();
          for (          TColumnValue tColumnValue : tRow.getColVals()) {
            cols.add(tColumnToObject(tColumnValue));
          }
          rowsBuilder.add(new QueryResult(cols));
        }
      }
      return rowsBuilder.build();
    }
 else {
      return Collections.emptyList();
    }
  }
 catch (  ClassNotFoundException e) {
    throw Throwables.propagate(e);
  }
catch (  NoSuchMethodException e) {
    throw Throwables.propagate(e);
  }
catch (  InvocationTargetException e) {
    throw Throwables.propagate(e);
  }
catch (  IllegalAccessException e) {
    throw Throwables.propagate(e);
  }
}","@SuppressWarnings(""String_Node_Str"") protected List<QueryResult> fetchNextResults(QueryHandle handle,int size) throws HiveSQLException, ExploreException, HandleNotFoundException {
  startAndWait();
  Lock nextLock=getOperationInfo(handle).getNextLock();
  nextLock.lock();
  try {
    LOG.trace(""String_Node_Str"",handle);
    OperationHandle operationHandle=getOperationHandle(handle);
    if (operationHandle.hasResultSet()) {
      Object rowSet=getCliService().fetchResults(operationHandle,FetchOrientation.FETCH_NEXT,size);
      ImmutableList.Builder<QueryResult> rowsBuilder=ImmutableList.builder();
      if (rowSet instanceof Iterable) {
        for (        Object[] row : (Iterable<Object[]>)rowSet) {
          List<Object> cols=Lists.newArrayList();
          for (int i=0; i < row.length; i++) {
            cols.add(row[i]);
          }
          rowsBuilder.add(new QueryResult(cols));
        }
      }
 else {
        Class rowSetClass=Class.forName(""String_Node_Str"");
        Method toTRowSetMethod=rowSetClass.getMethod(""String_Node_Str"");
        TRowSet tRowSet=(TRowSet)toTRowSetMethod.invoke(rowSet);
        for (        TRow tRow : tRowSet.getRows()) {
          List<Object> cols=Lists.newArrayList();
          for (          TColumnValue tColumnValue : tRow.getColVals()) {
            cols.add(tColumnToObject(tColumnValue));
          }
          rowsBuilder.add(new QueryResult(cols));
        }
      }
      return rowsBuilder.build();
    }
 else {
      return Collections.emptyList();
    }
  }
 catch (  ClassNotFoundException e) {
    throw Throwables.propagate(e);
  }
catch (  NoSuchMethodException e) {
    throw Throwables.propagate(e);
  }
catch (  InvocationTargetException e) {
    throw Throwables.propagate(e);
  }
catch (  IllegalAccessException e) {
    throw Throwables.propagate(e);
  }
 finally {
    nextLock.unlock();
  }
}"
7138,"@Override public List<QueryResult> previewResults(QueryHandle handle) throws ExploreException, HandleNotFoundException, SQLException {
  startAndWait();
  if (inactiveHandleCache.getIfPresent(handle) != null) {
    throw new HandleNotFoundException(""String_Node_Str"",true);
  }
  OperationInfo operationInfo=getOperationInfo(handle);
  Lock previewLock=operationInfo.getPreviewLock();
  previewLock.lock();
  try {
    File previewFile=operationInfo.getPreviewFile();
    if (previewFile != null) {
      try {
        Reader reader=Files.newReader(previewFile,Charsets.UTF_8);
        try {
          return GSON.fromJson(reader,new TypeToken<List<QueryResult>>(){
          }
.getType());
        }
  finally {
          Closeables.closeQuietly(reader);
        }
      }
 catch (      FileNotFoundException e) {
        LOG.error(""String_Node_Str"",previewFile,e);
        throw new ExploreException(e);
      }
    }
    try {
      previewFile=new File(previewsDir,handle.getHandle());
      FileWriter fileWriter=new FileWriter(previewFile);
      try {
        List<QueryResult> results=nextResults(handle,PREVIEW_COUNT);
        GSON.toJson(results,fileWriter);
        operationInfo.setPreviewFile(previewFile);
        return results;
      }
  finally {
        Closeables.closeQuietly(fileWriter);
      }
    }
 catch (    IOException e) {
      LOG.error(""String_Node_Str"",e);
      throw new ExploreException(e);
    }
  }
  finally {
    previewLock.unlock();
  }
}","@Override public List<QueryResult> previewResults(QueryHandle handle) throws ExploreException, HandleNotFoundException, SQLException {
  startAndWait();
  if (inactiveHandleCache.getIfPresent(handle) != null) {
    throw new HandleNotFoundException(""String_Node_Str"",true);
  }
  OperationInfo operationInfo=getOperationInfo(handle);
  Lock previewLock=operationInfo.getPreviewLock();
  previewLock.lock();
  try {
    File previewFile=operationInfo.getPreviewFile();
    if (previewFile != null) {
      try {
        Reader reader=Files.newReader(previewFile,Charsets.UTF_8);
        try {
          return GSON.fromJson(reader,new TypeToken<List<QueryResult>>(){
          }
.getType());
        }
  finally {
          Closeables.closeQuietly(reader);
        }
      }
 catch (      FileNotFoundException e) {
        LOG.error(""String_Node_Str"",previewFile,e);
        throw new ExploreException(e);
      }
    }
    try {
      previewFile=new File(previewsDir,handle.getHandle());
      FileWriter fileWriter=new FileWriter(previewFile);
      try {
        List<QueryResult> results=fetchNextResults(handle,PREVIEW_COUNT);
        GSON.toJson(results,fileWriter);
        operationInfo.setPreviewFile(previewFile);
        return results;
      }
  finally {
        Closeables.closeQuietly(fileWriter);
      }
    }
 catch (    IOException e) {
      LOG.error(""String_Node_Str"",e);
      throw new ExploreException(e);
    }
  }
  finally {
    previewLock.unlock();
  }
}"
7139,"@Override public List<QueryResult> nextResults(QueryHandle handle,int size) throws ExploreException, HandleNotFoundException, SQLException {
  startAndWait();
  InactiveOperationInfo inactiveOperationInfo=inactiveHandleCache.getIfPresent(handle);
  if (inactiveOperationInfo != null) {
    LOG.trace(""String_Node_Str"",handle);
    return ImmutableList.of();
  }
  Lock nextLock=getOperationInfo(handle).getNextLock();
  nextLock.lock();
  try {
    LOG.trace(""String_Node_Str"",handle);
    OperationHandle opHandle=getOperationHandle(handle);
    List<QueryResult> results=fetchNextResults(opHandle,size);
    QueryStatus status=getStatus(handle);
    if (results.isEmpty() && status.getStatus() == QueryStatus.OpStatus.FINISHED) {
      timeoutAggresively(handle,getResultSchema(handle),status);
    }
    return results;
  }
 catch (  HiveSQLException e) {
    throw getSqlException(e);
  }
 finally {
    nextLock.unlock();
  }
}","@Override public List<QueryResult> nextResults(QueryHandle handle,int size) throws ExploreException, HandleNotFoundException, SQLException {
  startAndWait();
  InactiveOperationInfo inactiveOperationInfo=inactiveHandleCache.getIfPresent(handle);
  if (inactiveOperationInfo != null) {
    LOG.trace(""String_Node_Str"",handle);
    return ImmutableList.of();
  }
  try {
    List<QueryResult> results=fetchNextResults(handle,size);
    QueryStatus status=getStatus(handle);
    if (results.isEmpty() && status.getStatus() == QueryStatus.OpStatus.FINISHED) {
      timeoutAggresively(handle,getResultSchema(handle),status);
    }
    return results;
  }
 catch (  HiveSQLException e) {
    throw getSqlException(e);
  }
}"
7140,"@Test public void previewResultsTest() throws Exception {
  Id.DatasetInstance myTable2=Id.DatasetInstance.from(NAMESPACE_ID,""String_Node_Str"");
  Id.DatasetInstance myTable3=Id.DatasetInstance.from(NAMESPACE_ID,""String_Node_Str"");
  Id.DatasetInstance myTable4=Id.DatasetInstance.from(NAMESPACE_ID,""String_Node_Str"");
  Id.DatasetInstance myTable5=Id.DatasetInstance.from(NAMESPACE_ID,""String_Node_Str"");
  Id.DatasetInstance myTable6=Id.DatasetInstance.from(NAMESPACE_ID,""String_Node_Str"");
  datasetFramework.addInstance(""String_Node_Str"",myTable2,DatasetProperties.EMPTY);
  datasetFramework.addInstance(""String_Node_Str"",myTable3,DatasetProperties.EMPTY);
  datasetFramework.addInstance(""String_Node_Str"",myTable4,DatasetProperties.EMPTY);
  datasetFramework.addInstance(""String_Node_Str"",myTable5,DatasetProperties.EMPTY);
  datasetFramework.addInstance(""String_Node_Str"",myTable6,DatasetProperties.EMPTY);
  try {
    QueryHandle handle=exploreService.execute(NAMESPACE_ID,""String_Node_Str"");
    QueryStatus status=waitForCompletionStatus(handle,200,TimeUnit.MILLISECONDS,50);
    Assert.assertEquals(QueryStatus.OpStatus.FINISHED,status.getStatus());
    List<QueryResult> firstPreview=exploreService.previewResults(handle);
    Assert.assertEquals(ImmutableList.of(new QueryResult(ImmutableList.<Object>of(MY_TABLE_NAME)),new QueryResult(ImmutableList.<Object>of(""String_Node_Str"")),new QueryResult(ImmutableList.<Object>of(""String_Node_Str"")),new QueryResult(ImmutableList.<Object>of(""String_Node_Str"")),new QueryResult(ImmutableList.<Object>of(""String_Node_Str""))),firstPreview);
    List<QueryResult> endResults=exploreService.nextResults(handle,100);
    Assert.assertEquals(ImmutableList.of(new QueryResult(ImmutableList.<Object>of(""String_Node_Str""))),endResults);
    List<QueryResult> secondPreview=exploreService.previewResults(handle);
    Assert.assertEquals(firstPreview,secondPreview);
    Assert.assertEquals(ImmutableList.of(),exploreService.nextResults(handle,100));
    try {
      exploreService.previewResults(handle);
      Assert.fail(""String_Node_Str"");
    }
 catch (    HandleNotFoundException e) {
      Assert.assertTrue(e.isInactive());
    }
  }
  finally {
    datasetFramework.deleteInstance(myTable2);
    datasetFramework.deleteInstance(myTable3);
    datasetFramework.deleteInstance(myTable4);
    datasetFramework.deleteInstance(myTable5);
    datasetFramework.deleteInstance(myTable6);
  }
}","@Test public void previewResultsTest() throws Exception {
  Id.DatasetInstance myTable2=Id.DatasetInstance.from(NAMESPACE_ID,""String_Node_Str"");
  Id.DatasetInstance myTable3=Id.DatasetInstance.from(NAMESPACE_ID,""String_Node_Str"");
  Id.DatasetInstance myTable4=Id.DatasetInstance.from(NAMESPACE_ID,""String_Node_Str"");
  Id.DatasetInstance myTable5=Id.DatasetInstance.from(NAMESPACE_ID,""String_Node_Str"");
  Id.DatasetInstance myTable6=Id.DatasetInstance.from(NAMESPACE_ID,""String_Node_Str"");
  datasetFramework.addInstance(""String_Node_Str"",myTable2,DatasetProperties.EMPTY);
  datasetFramework.addInstance(""String_Node_Str"",myTable3,DatasetProperties.EMPTY);
  datasetFramework.addInstance(""String_Node_Str"",myTable4,DatasetProperties.EMPTY);
  datasetFramework.addInstance(""String_Node_Str"",myTable5,DatasetProperties.EMPTY);
  datasetFramework.addInstance(""String_Node_Str"",myTable6,DatasetProperties.EMPTY);
  try {
    QueryHandle handle=exploreService.execute(NAMESPACE_ID,""String_Node_Str"");
    QueryStatus status=waitForCompletionStatus(handle,200,TimeUnit.MILLISECONDS,50);
    Assert.assertEquals(QueryStatus.OpStatus.FINISHED,status.getStatus());
    List<QueryResult> firstPreview=exploreService.previewResults(handle);
    Assert.assertEquals(ImmutableList.of(new QueryResult(ImmutableList.<Object>of(MY_TABLE_NAME)),new QueryResult(ImmutableList.<Object>of(getDatasetHiveName(myTable2))),new QueryResult(ImmutableList.<Object>of(getDatasetHiveName(myTable3))),new QueryResult(ImmutableList.<Object>of(getDatasetHiveName(myTable4))),new QueryResult(ImmutableList.<Object>of(getDatasetHiveName(myTable5)))),firstPreview);
    List<QueryResult> endResults=exploreService.nextResults(handle,100);
    Assert.assertEquals(ImmutableList.of(new QueryResult(ImmutableList.<Object>of(getDatasetHiveName(myTable6)))),endResults);
    List<QueryResult> secondPreview=exploreService.previewResults(handle);
    Assert.assertEquals(firstPreview,secondPreview);
    Assert.assertEquals(ImmutableList.of(),exploreService.nextResults(handle,100));
    try {
      exploreService.previewResults(handle);
      Assert.fail(""String_Node_Str"");
    }
 catch (    HandleNotFoundException e) {
      Assert.assertTrue(e.isInactive());
    }
    handle=exploreService.execute(NAMESPACE_ID,""String_Node_Str"" + getDatasetHiveName(myTable2));
    status=waitForCompletionStatus(handle,200,TimeUnit.MILLISECONDS,50);
    Assert.assertEquals(QueryStatus.OpStatus.FINISHED,status.getStatus());
    Assert.assertTrue(exploreService.previewResults(handle).isEmpty());
    Assert.assertTrue(exploreService.previewResults(handle).isEmpty());
  }
  finally {
    datasetFramework.deleteInstance(myTable2);
    datasetFramework.deleteInstance(myTable3);
    datasetFramework.deleteInstance(myTable4);
    datasetFramework.deleteInstance(myTable5);
    datasetFramework.deleteInstance(myTable6);
  }
}"
7141,"@Override void upgrade() throws Exception {
  TableId tableId=getTableId();
  if (!tableUtil.tableExists(new HBaseAdmin(conf),tableId)) {
    LOG.info(""String_Node_Str"",tableId);
    return;
  }
  HTable hTable=tableUtil.createHTable(conf,tableId);
  LOG.info(""String_Node_Str"",Bytes.toString(hTable.getTableName()));
  try {
    Scan scan=new Scan();
    scan.setTimeRange(0,HConstants.LATEST_TIMESTAMP);
    scan.addFamily(QueueEntryRow.COLUMN_FAMILY);
    scan.setMaxVersions(1);
    List<Mutation> mutations=Lists.newArrayList();
    Result result;
    ResultScanner resultScanner=hTable.getScanner(scan);
    try {
      while ((result=resultScanner.next()) != null) {
        byte[] row=result.getRow();
        String rowKeyString=Bytes.toString(row);
        byte[] newKey=processRowKey(row);
        NavigableMap<byte[],byte[]> columnsMap=result.getFamilyMap(QueueEntryRow.COLUMN_FAMILY);
        if (newKey != null) {
          Put put=new Put(newKey);
          for (          NavigableMap.Entry<byte[],byte[]> entry : columnsMap.entrySet()) {
            LOG.debug(""String_Node_Str"",Bytes.toString(entry.getKey()),Bytes.toString(entry.getValue()));
            put.add(QueueEntryRow.COLUMN_FAMILY,entry.getKey(),entry.getValue());
            mutations.add(put);
          }
          LOG.debug(""String_Node_Str"",rowKeyString);
          mutations.add(new Delete(row));
        }
        LOG.info(""String_Node_Str"",rowKeyString);
      }
    }
  finally {
      resultScanner.close();
    }
    hTable.batch(mutations);
    LOG.info(""String_Node_Str"",Bytes.toString(hTable.getTableName()));
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",tableId,e);
    throw Throwables.propagate(e);
  }
 finally {
    hTable.close();
  }
}","@Override void upgrade() throws Exception {
  TableId tableId=getTableId();
  if (!tableUtil.tableExists(new HBaseAdmin(conf),tableId)) {
    LOG.info(""String_Node_Str"",tableId);
    return;
  }
  HTable hTable=tableUtil.createHTable(conf,tableId);
  ProjectInfo.Version tableVersion=AbstractHBaseDataSetAdmin.getVersion(hTable.getTableDescriptor());
  if (ProjectInfo.getVersion().compareTo(tableVersion) <= 0) {
    LOG.info(""String_Node_Str"",tableId,tableVersion);
    return;
  }
  LOG.info(""String_Node_Str"",Bytes.toString(hTable.getTableName()));
  try {
    Scan scan=new Scan();
    scan.setTimeRange(0,HConstants.LATEST_TIMESTAMP);
    scan.addFamily(QueueEntryRow.COLUMN_FAMILY);
    scan.setMaxVersions(1);
    List<Mutation> mutations=Lists.newArrayList();
    Result result;
    ResultScanner resultScanner=hTable.getScanner(scan);
    try {
      while ((result=resultScanner.next()) != null) {
        byte[] row=result.getRow();
        String rowKeyString=Bytes.toString(row);
        byte[] newKey=processRowKey(row);
        NavigableMap<byte[],byte[]> columnsMap=result.getFamilyMap(QueueEntryRow.COLUMN_FAMILY);
        if (newKey != null) {
          Put put=new Put(newKey);
          for (          NavigableMap.Entry<byte[],byte[]> entry : columnsMap.entrySet()) {
            LOG.debug(""String_Node_Str"",Bytes.toString(entry.getKey()),Bytes.toString(entry.getValue()));
            put.add(QueueEntryRow.COLUMN_FAMILY,entry.getKey(),entry.getValue());
            mutations.add(put);
          }
          LOG.debug(""String_Node_Str"",rowKeyString);
          mutations.add(new Delete(row));
        }
        LOG.info(""String_Node_Str"",rowKeyString);
      }
    }
  finally {
      resultScanner.close();
    }
    hTable.batch(mutations);
    LOG.info(""String_Node_Str"",Bytes.toString(hTable.getTableName()));
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",tableId,e);
    throw Throwables.propagate(e);
  }
 finally {
    hTable.close();
  }
}"
7142,"@Inject private DatasetUpgrader(CConfiguration cConf,Configuration hConf,LocationFactory locationFactory,NamespacedLocationFactory namespacedLocationFactory,QueueAdmin queueAdmin,HBaseTableUtil hBaseTableUtil,DatasetFramework dsFramework,DatasetInstanceMDSUpgrader datasetInstanceMDSUpgrader,DatasetTypeMDSUpgrader datasetTypeMDSUpgrader){
  super(locationFactory,namespacedLocationFactory);
  this.cConf=cConf;
  this.hConf=hConf;
  this.locationFactory=locationFactory;
  this.queueAdmin=queueAdmin;
  this.hBaseTableUtil=hBaseTableUtil;
  this.dsFramework=dsFramework;
  this.datasetInstanceMDSUpgrader=datasetInstanceMDSUpgrader;
  this.datasetTypeMDSUpgrader=datasetTypeMDSUpgrader;
  this.userTablePrefix=Pattern.compile(String.format(""String_Node_Str"",cConf.get(Constants.Dataset.TABLE_PREFIX)));
}","@Inject private DatasetUpgrader(CConfiguration cConf,Configuration hConf,LocationFactory locationFactory,NamespacedLocationFactory namespacedLocationFactory,HBaseTableUtil hBaseTableUtil,DatasetFramework dsFramework,DatasetInstanceMDSUpgrader datasetInstanceMDSUpgrader,DatasetTypeMDSUpgrader datasetTypeMDSUpgrader){
  super(locationFactory,namespacedLocationFactory);
  this.cConf=cConf;
  this.hConf=hConf;
  this.locationFactory=locationFactory;
  this.hBaseTableUtil=hBaseTableUtil;
  this.dsFramework=dsFramework;
  this.datasetInstanceMDSUpgrader=datasetInstanceMDSUpgrader;
  this.datasetTypeMDSUpgrader=datasetTypeMDSUpgrader;
  this.userTablePrefix=Pattern.compile(String.format(""String_Node_Str"",cConf.get(Constants.Dataset.TABLE_PREFIX)));
}"
7143,"@Override public void upgrade() throws Exception {
  upgradeSystemDatasets();
  upgradeUserTables();
  queueAdmin.upgrade();
  datasetTypeMDSUpgrader.upgrade();
  datasetInstanceMDSUpgrader.upgrade();
  for (  DatasetSpecification fileSetSpec : datasetInstanceMDSUpgrader.getFileSetsSpecs()) {
    upgradeFileSet(fileSetSpec);
  }
}","@Override public void upgrade() throws Exception {
  upgradeSystemDatasets();
  upgradeUserTables();
  datasetTypeMDSUpgrader.upgrade();
  datasetInstanceMDSUpgrader.upgrade();
  for (  DatasetSpecification fileSetSpec : datasetInstanceMDSUpgrader.getFileSetsSpecs()) {
    upgradeFileSet(fileSetSpec);
  }
}"
7144,"@Inject public QueueConfigUpgrader(LocationFactory locationFactory,NamespacedLocationFactory namespacedLocationFactory,HBaseTableUtil tableUtil,Configuration conf){
  super(locationFactory,namespacedLocationFactory,tableUtil,conf);
}","@Inject public QueueConfigUpgrader(LocationFactory locationFactory,NamespacedLocationFactory namespacedLocationFactory,HBaseTableUtil tableUtil,Configuration conf,QueueAdmin queueAdmin){
  super(locationFactory,namespacedLocationFactory,tableUtil,conf);
  this.queueAdmin=queueAdmin;
}"
7145,"@Inject public StreamStateStoreUpgrader(LocationFactory locationFactory,NamespacedLocationFactory namespacedLocationFactory,HBaseTableUtil tableUtil,Configuration conf){
  super(locationFactory,namespacedLocationFactory,tableUtil,conf);
}","@Inject public StreamStateStoreUpgrader(LocationFactory locationFactory,NamespacedLocationFactory namespacedLocationFactory,HBaseTableUtil tableUtil,Configuration conf,StreamAdmin streamAdmin){
  super(locationFactory,namespacedLocationFactory,tableUtil,conf);
  this.streamAdmin=streamAdmin;
}"
7146,"private void doMain(String[] args) throws Exception {
  System.out.println(String.format(""String_Node_Str"",getClass().getSimpleName(),ProjectInfo.getVersion()));
  System.out.println();
  if (args.length < 1) {
    printHelp();
    return;
  }
  Action action=parseAction(args[0]);
  if (action == null) {
    System.out.println(String.format(""String_Node_Str"",args[0]));
    printHelp(true);
    return;
  }
  try {
switch (action) {
case UPGRADE:
      Scanner scan=new Scanner(System.in);
    System.out.println(String.format(""String_Node_Str"",action.name().toLowerCase(),action.getDescription()));
  System.out.println(""String_Node_Str"");
String response=scan.next();
if (response.equalsIgnoreCase(""String_Node_Str"") || response.equalsIgnoreCase(""String_Node_Str"")) {
System.out.println(""String_Node_Str"");
performUpgrade();
}
 else {
System.out.println(""String_Node_Str"");
}
break;
case HELP:
printHelp();
break;
}
}
 catch (Exception e) {
System.out.println(String.format(""String_Node_Str"",action,e.getMessage()));
e.printStackTrace(System.out);
throw e;
}
}","private void doMain(String[] args) throws Exception {
  System.out.println(String.format(""String_Node_Str"",getClass().getSimpleName(),ProjectInfo.getVersion()));
  System.out.println();
  if (args.length < 1) {
    printHelp();
    return;
  }
  Action action=parseAction(args[0]);
  if (action == null) {
    System.out.println(String.format(""String_Node_Str"",args[0]));
    printHelp(true);
    return;
  }
  try {
switch (action) {
case UPGRADE:
      Scanner scan=new Scanner(System.in);
    System.out.println(String.format(""String_Node_Str"",action.name().toLowerCase(),action.getDescription()));
  System.out.println(""String_Node_Str"");
String response=scan.next();
if (response.equalsIgnoreCase(""String_Node_Str"") || response.equalsIgnoreCase(""String_Node_Str"")) {
System.out.println(""String_Node_Str"");
try {
  startUp();
  performUpgrade();
}
  finally {
  stop();
}
}
 else {
System.out.println(""String_Node_Str"");
}
break;
case HELP:
printHelp();
break;
}
}
 catch (Exception e) {
System.out.println(String.format(""String_Node_Str"",action,e.getMessage()));
e.printStackTrace(System.out);
throw e;
}
}"
7147,"public static void main(String[] args) throws Exception {
  UpgradeTool upgradeTool=new UpgradeTool();
  upgradeTool.startUp();
  try {
    upgradeTool.doMain(args);
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",t);
  }
 finally {
    upgradeTool.stop();
  }
}","public static void main(String[] args){
  try {
    UpgradeTool upgradeTool=new UpgradeTool();
    upgradeTool.doMain(args);
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",t);
  }
}"
7148,"/** 
 * Get program location
 * @param factory  location factory
 * @param appFabricDir app fabric output directory path
 * @param id       program id
 * @param type     type of the program
 * @return         Location corresponding to the program id
 * @throws IOException incase of errors
 */
public static Location programLocation(LocationFactory factory,String appFabricDir,Id.Program id,ProgramType type) throws IOException {
  Location namespaceHome=factory.create(id.getNamespaceId());
  if (!namespaceHome.exists()) {
    throw new FileNotFoundException(""String_Node_Str"" + namespaceHome.toURI().getPath());
  }
  Location appFabricLocation=namespaceHome.append(appFabricDir);
  String name=String.format(Locale.ENGLISH,""String_Node_Str"",id.getApplicationId(),type.toString());
  Location applicationProgramsLocation=appFabricLocation.append(name);
  if (!applicationProgramsLocation.exists()) {
    throw new FileNotFoundException(""String_Node_Str"" + applicationProgramsLocation.toURI().getPath());
  }
  Location programLocation=applicationProgramsLocation.append(String.format(""String_Node_Str"",id.getId()));
  if (!programLocation.exists()) {
    throw new FileNotFoundException(String.format(""String_Node_Str"",id.getApplication(),id.getId(),type));
  }
  return programLocation;
}","/** 
 * Get program location
 * @param namespacedLocationFactory the namespaced location on the file system
 * @param appFabricDir app fabric output directory path
 * @param id program id
 * @param type type of the program    @return Location corresponding to the program id
 * @throws IOException incase of errors
 */
public static Location programLocation(NamespacedLocationFactory namespacedLocationFactory,String appFabricDir,Id.Program id,ProgramType type) throws IOException {
  Location namespaceHome=namespacedLocationFactory.get(id.getApplication().getNamespace());
  if (!namespaceHome.exists()) {
    throw new FileNotFoundException(""String_Node_Str"" + namespaceHome.toURI().getPath());
  }
  Location appFabricLocation=namespaceHome.append(appFabricDir);
  Location applicationProgramsLocation=appFabricLocation.append(id.getApplicationId()).append(type.toString());
  if (!applicationProgramsLocation.exists()) {
    throw new FileNotFoundException(""String_Node_Str"" + applicationProgramsLocation.toURI().getPath());
  }
  Location programLocation=applicationProgramsLocation.append(String.format(""String_Node_Str"",id.getId()));
  if (!programLocation.exists()) {
    throw new FileNotFoundException(String.format(""String_Node_Str"",id.getApplication(),id.getId(),type));
  }
  return programLocation;
}"
7149,"@Inject public AppLifecycleHttpHandler(Authenticator authenticator,CConfiguration configuration,ManagerFactory<DeploymentInfo,ApplicationWithPrograms> managerFactory,LocationFactory locationFactory,Scheduler scheduler,ProgramRuntimeService runtimeService,StoreFactory storeFactory,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,DiscoveryServiceClient discoveryServiceClient,PreferencesStore preferencesStore,AdapterService adapterService,NamespaceAdmin namespaceAdmin,MetricStore metricStore){
  super(authenticator);
  this.configuration=configuration;
  this.managerFactory=managerFactory;
  this.namespaceAdmin=namespaceAdmin;
  this.locationFactory=locationFactory;
  this.scheduler=scheduler;
  this.runtimeService=runtimeService;
  this.store=storeFactory.create();
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.discoveryServiceClient=discoveryServiceClient;
  this.preferencesStore=preferencesStore;
  this.adapterService=adapterService;
  this.metricStore=metricStore;
}","@Inject public AppLifecycleHttpHandler(Authenticator authenticator,CConfiguration configuration,ManagerFactory<DeploymentInfo,ApplicationWithPrograms> managerFactory,LocationFactory locationFactory,Scheduler scheduler,ProgramRuntimeService runtimeService,StoreFactory storeFactory,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,PreferencesStore preferencesStore,AdapterService adapterService,NamespaceAdmin namespaceAdmin,MetricStore metricStore,NamespacedLocationFactory namespacedLocationFactory){
  super(authenticator);
  this.configuration=configuration;
  this.managerFactory=managerFactory;
  this.namespaceAdmin=namespaceAdmin;
  this.locationFactory=locationFactory;
  this.scheduler=scheduler;
  this.runtimeService=runtimeService;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.store=storeFactory.create();
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.preferencesStore=preferencesStore;
  this.adapterService=adapterService;
  this.metricStore=metricStore;
}"
7150,"private BodyConsumer deployApplication(final HttpResponder responder,final String namespaceId,final String appId,final String archiveName) throws IOException {
  Id.Namespace namespace=Id.Namespace.from(namespaceId);
  if (!namespaceAdmin.hasNamespace(namespace)) {
    LOG.warn(""String_Node_Str"",namespaceId);
    responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",namespaceId));
    return null;
  }
  Location namespaceHomeLocation=locationFactory.create(namespaceId);
  if (!namespaceHomeLocation.exists()) {
    String msg=String.format(""String_Node_Str"",namespaceHomeLocation.toURI().getPath(),namespaceId);
    LOG.error(msg);
    responder.sendString(HttpResponseStatus.NOT_FOUND,msg);
    return null;
  }
  if (archiveName == null || archiveName.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,ARCHIVE_NAME_HEADER + ""String_Node_Str"",ImmutableMultimap.of(HttpHeaders.Names.CONNECTION,HttpHeaders.Values.CLOSE));
    return null;
  }
  String tempBase=String.format(""String_Node_Str"",configuration.get(Constants.CFG_LOCAL_DATA_DIR),namespaceId);
  File tempDir=new File(tempBase,configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  if (!DirUtils.mkdirs(tempDir)) {
    throw new IOException(""String_Node_Str"" + tempDir);
  }
  String appFabricDir=configuration.get(Constants.AppFabric.OUTPUT_DIR);
  final Location archive=namespaceHomeLocation.append(appFabricDir).append(Constants.ARCHIVE_DIR).append(archiveName);
  return new AbstractBodyConsumer(File.createTempFile(""String_Node_Str"",""String_Node_Str"",tempDir)){
    @Override protected void onFinish(    HttpResponder responder,    File uploadedFile){
      try {
        DeploymentInfo deploymentInfo=new DeploymentInfo(uploadedFile,archive);
        deploy(namespaceId,appId,deploymentInfo);
        responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"",e);
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
    }
  }
;
}","private BodyConsumer deployApplication(final HttpResponder responder,final String namespaceId,final String appId,final String archiveName) throws IOException {
  Id.Namespace namespace=Id.Namespace.from(namespaceId);
  if (!namespaceAdmin.hasNamespace(namespace)) {
    LOG.warn(""String_Node_Str"",namespaceId);
    responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",namespaceId));
    return null;
  }
  Location namespaceHomeLocation=namespacedLocationFactory.get(namespace);
  if (!namespaceHomeLocation.exists()) {
    String msg=String.format(""String_Node_Str"",namespaceHomeLocation.toURI().getPath(),namespaceId);
    LOG.error(msg);
    responder.sendString(HttpResponseStatus.NOT_FOUND,msg);
    return null;
  }
  if (archiveName == null || archiveName.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,ARCHIVE_NAME_HEADER + ""String_Node_Str"",ImmutableMultimap.of(HttpHeaders.Names.CONNECTION,HttpHeaders.Values.CLOSE));
    return null;
  }
  String namespacesDir=configuration.get(Constants.Namespace.NAMESPACES_DIR);
  File localDataDir=new File(configuration.get(Constants.CFG_LOCAL_DATA_DIR));
  File namespaceBase=new File(localDataDir,namespacesDir);
  File tempDir=new File(new File(namespaceBase,namespaceId),configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  if (!DirUtils.mkdirs(tempDir)) {
    throw new IOException(""String_Node_Str"" + tempDir);
  }
  String appFabricDir=configuration.get(Constants.AppFabric.OUTPUT_DIR);
  final Location archive=namespaceHomeLocation.append(appFabricDir).append(Constants.ARCHIVE_DIR).append(archiveName);
  return new AbstractBodyConsumer(File.createTempFile(""String_Node_Str"",""String_Node_Str"",tempDir)){
    @Override protected void onFinish(    HttpResponder responder,    File uploadedFile){
      try {
        DeploymentInfo deploymentInfo=new DeploymentInfo(uploadedFile,archive);
        deploy(namespaceId,appId,deploymentInfo);
        responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"",e);
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
    }
  }
;
}"
7151,"/** 
 * Delete the jar location of the program.
 * @param appId        applicationId.
 * @throws IOException if there are errors with location IO
 */
private void deleteProgramLocations(Id.Application appId) throws IOException {
  Iterable<ProgramSpecification> programSpecs=getProgramSpecs(appId);
  String appFabricDir=configuration.get(Constants.AppFabric.OUTPUT_DIR);
  for (  ProgramSpecification spec : programSpecs) {
    ProgramType type=ProgramTypes.fromSpecification(spec);
    Id.Program programId=Id.Program.from(appId,type,spec.getName());
    try {
      Location location=Programs.programLocation(locationFactory,appFabricDir,programId,type);
      location.delete();
    }
 catch (    FileNotFoundException e) {
      LOG.warn(""String_Node_Str"",programId.toString(),e);
    }
  }
  try {
    Id.Program programId=Id.Program.from(appId.getNamespaceId(),appId.getId(),ProgramType.WEBAPP,ProgramType.WEBAPP.name().toLowerCase());
    Location location=Programs.programLocation(locationFactory,appFabricDir,programId,ProgramType.WEBAPP);
    location.delete();
  }
 catch (  FileNotFoundException e) {
  }
}","/** 
 * Delete the jar location of the program.
 * @param appId        applicationId.
 * @throws IOException if there are errors with location IO
 */
private void deleteProgramLocations(Id.Application appId) throws IOException {
  Iterable<ProgramSpecification> programSpecs=getProgramSpecs(appId);
  String appFabricDir=configuration.get(Constants.AppFabric.OUTPUT_DIR);
  for (  ProgramSpecification spec : programSpecs) {
    ProgramType type=ProgramTypes.fromSpecification(spec);
    Id.Program programId=Id.Program.from(appId,type,spec.getName());
    try {
      Location location=Programs.programLocation(namespacedLocationFactory,appFabricDir,programId,type);
      location.delete();
    }
 catch (    FileNotFoundException e) {
      LOG.warn(""String_Node_Str"",programId.toString(),e);
    }
  }
  try {
    Id.Program programId=Id.Program.from(appId.getNamespaceId(),appId.getId(),ProgramType.WEBAPP,ProgramType.WEBAPP.name().toLowerCase());
    Location location=Programs.programLocation(namespacedLocationFactory,appFabricDir,programId,ProgramType.WEBAPP);
    location.delete();
  }
 catch (  FileNotFoundException e) {
  }
}"
7152,"@Inject public ProgramLifecycleHttpHandler(Authenticator authenticator,StoreFactory storeFactory,WorkflowClient workflowClient,LocationFactory locationFactory,CConfiguration configuration,ProgramRuntimeService runtimeService,DiscoveryServiceClient discoveryServiceClient,QueueAdmin queueAdmin,Scheduler scheduler,PreferencesStore preferencesStore){
  super(authenticator);
  this.store=storeFactory.create();
  this.workflowClient=workflowClient;
  this.locationFactory=locationFactory;
  this.configuration=configuration;
  this.runtimeService=runtimeService;
  this.appFabricDir=this.configuration.get(Constants.AppFabric.OUTPUT_DIR);
  this.discoveryServiceClient=discoveryServiceClient;
  this.queueAdmin=queueAdmin;
  this.scheduler=scheduler;
  this.preferencesStore=preferencesStore;
  this.schedulerQueueResolver=new SchedulerQueueResolver(configuration,store);
}","@Inject public ProgramLifecycleHttpHandler(Authenticator authenticator,StoreFactory storeFactory,WorkflowClient workflowClient,CConfiguration configuration,ProgramRuntimeService runtimeService,DiscoveryServiceClient discoveryServiceClient,QueueAdmin queueAdmin,Scheduler scheduler,PreferencesStore preferencesStore,NamespacedLocationFactory namespacedLocationFactory){
  super(authenticator);
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.store=storeFactory.create();
  this.workflowClient=workflowClient;
  this.configuration=configuration;
  this.runtimeService=runtimeService;
  this.appFabricDir=this.configuration.get(Constants.AppFabric.OUTPUT_DIR);
  this.discoveryServiceClient=discoveryServiceClient;
  this.queueAdmin=queueAdmin;
  this.scheduler=scheduler;
  this.preferencesStore=preferencesStore;
  this.schedulerQueueResolver=new SchedulerQueueResolver(configuration,store);
}"
7153,"/** 
 * 'protected' only to support v2 webapp APIs
 */
protected ProgramStatus getProgramStatus(Id.Program id,ProgramType type){
  try {
    ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(id,type);
    if (runtimeInfo == null) {
      if (type != ProgramType.WEBAPP) {
        ProgramSpecification spec=getProgramSpecification(id,type);
        if (spec == null) {
          return new ProgramStatus(id.getApplicationId(),id.getId(),HttpResponseStatus.NOT_FOUND.toString());
        }
 else {
          return new ProgramStatus(id.getApplicationId(),id.getId(),""String_Node_Str"");
        }
      }
 else {
        Location webappLoc=null;
        try {
          webappLoc=Programs.programLocation(locationFactory,appFabricDir,id,ProgramType.WEBAPP);
        }
 catch (        FileNotFoundException e) {
        }
        if (webappLoc != null && webappLoc.exists()) {
          return new ProgramStatus(id.getApplicationId(),id.getId(),""String_Node_Str"");
        }
 else {
          return new ProgramStatus(id.getApplicationId(),id.getId(),HttpResponseStatus.NOT_FOUND.toString());
        }
      }
    }
    String status=controllerStateToString(runtimeInfo.getController().getState());
    return new ProgramStatus(id.getApplicationId(),id.getId(),status);
  }
 catch (  Throwable throwable) {
    LOG.warn(throwable.getMessage(),throwable);
    throw Throwables.propagate(throwable);
  }
}","/** 
 * 'protected' only to support v2 webapp APIs
 */
protected ProgramStatus getProgramStatus(Id.Program id,ProgramType type){
  try {
    ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(id,type);
    if (runtimeInfo == null) {
      if (type != ProgramType.WEBAPP) {
        ProgramSpecification spec=getProgramSpecification(id,type);
        if (spec == null) {
          return new ProgramStatus(id.getApplicationId(),id.getId(),HttpResponseStatus.NOT_FOUND.toString());
        }
 else {
          return new ProgramStatus(id.getApplicationId(),id.getId(),""String_Node_Str"");
        }
      }
 else {
        Location webappLoc=null;
        try {
          webappLoc=Programs.programLocation(namespacedLocationFactory,appFabricDir,id,ProgramType.WEBAPP);
        }
 catch (        FileNotFoundException e) {
        }
        if (webappLoc != null && webappLoc.exists()) {
          return new ProgramStatus(id.getApplicationId(),id.getId(),""String_Node_Str"");
        }
 else {
          return new ProgramStatus(id.getApplicationId(),id.getId(),HttpResponseStatus.NOT_FOUND.toString());
        }
      }
    }
    String status=controllerStateToString(runtimeInfo.getController().getState());
    return new ProgramStatus(id.getApplicationId(),id.getId(),status);
  }
 catch (  Throwable throwable) {
    LOG.warn(throwable.getMessage(),throwable);
    throw Throwables.propagate(throwable);
  }
}"
7154,"@Inject public LocalManager(CConfiguration configuration,PipelineFactory pipelineFactory,LocationFactory locationFactory,StoreFactory storeFactory,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,DiscoveryServiceClient discoveryServiceClient,DatasetFramework datasetFramework,@Named(""String_Node_Str"") DatasetFramework inMemoryDatasetFramework,StreamAdmin streamAdmin,ExploreFacade exploreFacade,Scheduler scheduler,AdapterService adapterService,@Assisted ProgramTerminator programTerminator){
  this.configuration=configuration;
  this.pipelineFactory=pipelineFactory;
  this.locationFactory=locationFactory;
  this.discoveryServiceClient=discoveryServiceClient;
  this.store=storeFactory.create();
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.programTerminator=programTerminator;
  this.datasetFramework=datasetFramework;
  this.inMemoryDatasetFramework=inMemoryDatasetFramework;
  this.streamAdmin=streamAdmin;
  this.exploreFacade=exploreFacade;
  this.scheduler=scheduler;
  this.exploreEnabled=configuration.getBoolean(Constants.Explore.EXPLORE_ENABLED);
  this.adapterService=adapterService;
}","@Inject public LocalManager(CConfiguration configuration,PipelineFactory pipelineFactory,NamespacedLocationFactory namespacedLocationFactory,StoreFactory storeFactory,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,DiscoveryServiceClient discoveryServiceClient,DatasetFramework datasetFramework,@Named(""String_Node_Str"") DatasetFramework inMemoryDatasetFramework,StreamAdmin streamAdmin,ExploreFacade exploreFacade,Scheduler scheduler,AdapterService adapterService,@Assisted ProgramTerminator programTerminator){
  this.configuration=configuration;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.pipelineFactory=pipelineFactory;
  this.discoveryServiceClient=discoveryServiceClient;
  this.store=storeFactory.create();
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.programTerminator=programTerminator;
  this.datasetFramework=datasetFramework;
  this.inMemoryDatasetFramework=inMemoryDatasetFramework;
  this.streamAdmin=streamAdmin;
  this.exploreFacade=exploreFacade;
  this.scheduler=scheduler;
  this.exploreEnabled=configuration.getBoolean(Constants.Explore.EXPLORE_ENABLED);
  this.adapterService=adapterService;
}"
7155,"@Override public ListenableFuture<O> deploy(Id.Namespace id,@Nullable String appId,I input) throws Exception {
  Pipeline<O> pipeline=pipelineFactory.getPipeline();
  pipeline.addLast(new LocalArchiveLoaderStage(store,configuration,id,appId));
  pipeline.addLast(new VerificationStage(store,datasetFramework,adapterService));
  pipeline.addLast(new DeployDatasetModulesStage(configuration,datasetFramework,inMemoryDatasetFramework));
  pipeline.addLast(new CreateDatasetInstancesStage(configuration,datasetFramework));
  pipeline.addLast(new CreateStreamsStage(id,streamAdmin,exploreFacade,exploreEnabled));
  pipeline.addLast(new DeletedProgramHandlerStage(store,programTerminator,streamConsumerFactory,queueAdmin,discoveryServiceClient));
  pipeline.addLast(new ProgramGenerationStage(configuration,locationFactory));
  pipeline.addLast(new ApplicationRegistrationStage(store));
  pipeline.addLast(new CreateSchedulesStage(scheduler));
  pipeline.setFinally(new DeployCleanupStage());
  return pipeline.execute(input);
}","@Override public ListenableFuture<O> deploy(Id.Namespace id,@Nullable String appId,I input) throws Exception {
  Pipeline<O> pipeline=pipelineFactory.getPipeline();
  pipeline.addLast(new LocalArchiveLoaderStage(store,configuration,id,appId));
  pipeline.addLast(new VerificationStage(store,datasetFramework,adapterService));
  pipeline.addLast(new DeployDatasetModulesStage(configuration,datasetFramework,inMemoryDatasetFramework));
  pipeline.addLast(new CreateDatasetInstancesStage(configuration,datasetFramework));
  pipeline.addLast(new CreateStreamsStage(id,streamAdmin,exploreFacade,exploreEnabled));
  pipeline.addLast(new DeletedProgramHandlerStage(store,programTerminator,streamConsumerFactory,queueAdmin,discoveryServiceClient));
  pipeline.addLast(new ProgramGenerationStage(configuration,namespacedLocationFactory));
  pipeline.addLast(new ApplicationRegistrationStage(store));
  pipeline.addLast(new CreateSchedulesStage(scheduler));
  pipeline.setFinally(new DeployCleanupStage());
  return pipeline.execute(input);
}"
7156,"@Override public Location call() throws Exception {
  ProgramType type=ProgramTypes.fromSpecification(spec);
  String name=String.format(Locale.ENGLISH,""String_Node_Str"",applicationName,type);
  Location programDir=newOutputDir.append(name);
  if (!programDir.exists()) {
    programDir.mkdirs();
  }
  Location output=programDir.append(String.format(""String_Node_Str"",spec.getName()));
  return ProgramBundle.create(input.getId(),bundler,output,spec.getName(),spec.getClassName(),type,appSpec);
}","@Override public Location call() throws Exception {
  ProgramType type=ProgramTypes.fromSpecification(spec);
  String name=String.format(Locale.ENGLISH,""String_Node_Str"",applicationName,type);
  Location programDir=appFabricDir.append(name);
  if (!programDir.exists()) {
    programDir.mkdirs();
  }
  Location output=programDir.append(String.format(""String_Node_Str"",spec.getName()));
  return ProgramBundle.create(input.getId(),bundler,output,spec.getName(),spec.getClassName(),type,appSpec);
}"
7157,"@Override public void process(final ApplicationDeployable input) throws Exception {
  ImmutableList.Builder<Program> programs=ImmutableList.builder();
  final ApplicationSpecification appSpec=input.getSpecification();
  final String applicationName=appSpec.getName();
  final ArchiveBundler bundler=new ArchiveBundler(input.getLocation());
  String namespace=input.getId().getNamespaceId();
  Location namespaceDir=locationFactory.create(namespace);
  Location appFabricDir=namespaceDir.append(configuration.get(Constants.AppFabric.OUTPUT_DIR));
  final Location newOutputDir=appFabricDir;
  if (!newOutputDir.exists() && !newOutputDir.mkdirs() && !newOutputDir.exists()) {
    throw new IOException(String.format(""String_Node_Str"",newOutputDir.toURI().getPath()));
  }
  Iterable<ProgramSpecification> specifications=Iterables.concat(appSpec.getMapReduce().values(),appSpec.getFlows().values(),appSpec.getProcedures().values(),appSpec.getWorkflows().values(),appSpec.getServices().values(),appSpec.getSpark().values(),appSpec.getWorkers().values());
  Set<String> servingHostNames=WebappProgramRunner.getServingHostNames(Locations.newInputSupplier(input.getLocation()));
  if (!servingHostNames.isEmpty()) {
    specifications=Iterables.concat(specifications,ImmutableList.of(createWebappSpec(ProgramType.WEBAPP.toString().toLowerCase())));
  }
  ListeningExecutorService executorService=MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(10,Threads.createDaemonThreadFactory(""String_Node_Str"")));
  try {
    List<ListenableFuture<Location>> futures=Lists.newArrayList();
    for (    final ProgramSpecification spec : specifications) {
      ListenableFuture<Location> future=executorService.submit(new Callable<Location>(){
        @Override public Location call() throws Exception {
          ProgramType type=ProgramTypes.fromSpecification(spec);
          String name=String.format(Locale.ENGLISH,""String_Node_Str"",applicationName,type);
          Location programDir=newOutputDir.append(name);
          if (!programDir.exists()) {
            programDir.mkdirs();
          }
          Location output=programDir.append(String.format(""String_Node_Str"",spec.getName()));
          return ProgramBundle.create(input.getId(),bundler,output,spec.getName(),spec.getClassName(),type,appSpec);
        }
      }
);
      futures.add(future);
    }
    for (    Location jarLocation : Futures.allAsList(futures).get()) {
      programs.add(Programs.create(jarLocation,null));
    }
  }
  finally {
    executorService.shutdown();
  }
  moveAppArchiveUnderAppDirectory(input.getLocation(),applicationName);
  emit(new ApplicationWithPrograms(input,programs.build()));
}","@Override public void process(final ApplicationDeployable input) throws Exception {
  ImmutableList.Builder<Program> programs=ImmutableList.builder();
  final ApplicationSpecification appSpec=input.getSpecification();
  final String applicationName=appSpec.getName();
  final ArchiveBundler bundler=new ArchiveBundler(input.getLocation());
  Id.Namespace namespaceId=input.getId().getNamespace();
  Location namespacedLocation=namespacedLocationFactory.get(namespaceId);
  final Location appFabricDir=namespacedLocation.append(configuration.get(Constants.AppFabric.OUTPUT_DIR));
  if (!appFabricDir.exists() && !appFabricDir.mkdirs() && !appFabricDir.exists()) {
    throw new IOException(String.format(""String_Node_Str"",appFabricDir.toURI().getPath()));
  }
  Iterable<ProgramSpecification> specifications=Iterables.concat(appSpec.getMapReduce().values(),appSpec.getFlows().values(),appSpec.getProcedures().values(),appSpec.getWorkflows().values(),appSpec.getServices().values(),appSpec.getSpark().values(),appSpec.getWorkers().values());
  Set<String> servingHostNames=WebappProgramRunner.getServingHostNames(Locations.newInputSupplier(input.getLocation()));
  if (!servingHostNames.isEmpty()) {
    specifications=Iterables.concat(specifications,ImmutableList.of(createWebappSpec(ProgramType.WEBAPP.toString().toLowerCase())));
  }
  ListeningExecutorService executorService=MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(10,Threads.createDaemonThreadFactory(""String_Node_Str"")));
  try {
    List<ListenableFuture<Location>> futures=Lists.newArrayList();
    for (    final ProgramSpecification spec : specifications) {
      ListenableFuture<Location> future=executorService.submit(new Callable<Location>(){
        @Override public Location call() throws Exception {
          ProgramType type=ProgramTypes.fromSpecification(spec);
          String name=String.format(Locale.ENGLISH,""String_Node_Str"",applicationName,type);
          Location programDir=appFabricDir.append(name);
          if (!programDir.exists()) {
            programDir.mkdirs();
          }
          Location output=programDir.append(String.format(""String_Node_Str"",spec.getName()));
          return ProgramBundle.create(input.getId(),bundler,output,spec.getName(),spec.getClassName(),type,appSpec);
        }
      }
);
      futures.add(future);
    }
    for (    Location jarLocation : Futures.allAsList(futures).get()) {
      programs.add(Programs.create(jarLocation,null));
    }
  }
  finally {
    executorService.shutdown();
  }
  moveAppArchiveUnderAppDirectory(input.getLocation(),applicationName);
  emit(new ApplicationWithPrograms(input,programs.build()));
}"
7158,"public ProgramGenerationStage(Configuration configuration,LocationFactory locationFactory){
  super(TypeToken.of(ApplicationDeployable.class));
  this.configuration=configuration;
  this.locationFactory=locationFactory;
}","public ProgramGenerationStage(CConfiguration configuration,NamespacedLocationFactory namespacedLocationFactory){
  super(TypeToken.of(ApplicationDeployable.class));
  this.configuration=configuration;
  this.namespacedLocationFactory=namespacedLocationFactory;
}"
7159,"@Override public DatasetAdmin getAdmin(DatasetContext datasetContext,DatasetSpecification spec,ClassLoader classLoader) throws IOException {
  return new CompositeDatasetAdmin(filesetDef.getAdmin(datasetContext,spec.getSpecification(FILESET_NAME),classLoader),tableDef.getAdmin(datasetContext,spec.getSpecification(PARTITION_TABLE_NAME),classLoader));
}","@Override public DatasetAdmin getAdmin(DatasetContext datasetContext,DatasetSpecification spec,ClassLoader classLoader) throws IOException {
  return new PartitionedFileSetAdmin(datasetContext,spec,getExploreProvider(),filesetDef.getAdmin(datasetContext,spec.getSpecification(FILESET_NAME),classLoader),tableDef.getAdmin(datasetContext,spec.getSpecification(PARTITION_TABLE_NAME),classLoader));
}"
7160,"@Override public DequeueResult<byte[]> dequeue(int maxBatchSize) throws IOException {
  DequeueResult<byte[]> result=performDequeue(maxBatchSize);
  if (scanStartRow != null) {
    if (!consumingEntries.isEmpty()) {
      byte[] floorKey=consumingEntries.floorKey(scanStartRow);
      if (floorKey != null) {
        updateStartRow(floorKey);
      }
    }
 else {
      updateStartRow(scanStartRow);
    }
  }
  return result;
}","@Override public DequeueResult<byte[]> dequeue(int maxBatchSize) throws IOException {
  DequeueResult<byte[]> result=performDequeue(maxBatchSize);
  byte[] floorKey=consumingEntries.floorKey(scanStartRow);
  updateStartRow(floorKey == null ? scanStartRow : floorKey);
  return result;
}"
7161,"/** 
 * Creates a HBaseQueue2Consumer.
 * @param hTable The HTable instance to use for communicating with HBase. This consumer is responsible for closing it.
 * @param queueName Name of the queue.
 * @param consumerState The persisted state of this consumer.
 * @param stateStore The store for persisting state for this consumer.
 */
HBaseQueueConsumer(CConfiguration cConf,HTable hTable,QueueName queueName,HBaseConsumerState consumerState,HBaseConsumerStateStore stateStore,HBaseQueueStrategy queueStrategy){
  super(cConf,consumerState.getConsumerConfig(),queueName,consumerState.getStartRow());
  this.hTable=hTable;
  this.stateStore=stateStore;
  this.queueRowPrefix=QueueEntryRow.getQueueRowPrefix(queueName);
  this.queueStrategy=queueStrategy;
  this.state=consumerState;
  this.canConsume=false;
}","/** 
 * Creates a HBaseQueue2Consumer.
 * @param hTable The HTable instance to use for communicating with HBase. This consumer is responsible for closing it.
 * @param queueName Name of the queue.
 * @param consumerState The persisted state of this consumer.
 * @param stateStore The store for persisting state for this consumer.
 */
HBaseQueueConsumer(CConfiguration cConf,HTable hTable,QueueName queueName,HBaseConsumerState consumerState,HBaseConsumerStateStore stateStore,HBaseQueueStrategy queueStrategy){
  super(cConf,consumerState.getConsumerConfig(),queueName,consumerState.getStartRow());
  this.hTable=hTable;
  this.state=consumerState;
  this.stateStore=stateStore;
  this.queueRowPrefix=QueueEntryRow.getQueueRowPrefix(queueName);
  this.queueStrategy=queueStrategy;
  this.canConsume=false;
}"
7162,"@Override protected void updateStartRow(byte[] startRow){
  ConsumerConfig consumerConfig=getConfig();
  stateStore.updateState(consumerConfig.getGroupId(),consumerConfig.getInstanceId(),startRow);
}","@Override protected void updateStartRow(byte[] startRow){
  if (canConsume && !completed) {
    ConsumerConfig consumerConfig=getConfig();
    stateStore.updateState(consumerConfig.getGroupId(),consumerConfig.getInstanceId(),startRow);
  }
}"
7163,"/** 
 * Reads events from a stream
 * @param streamId ID of the stream
 * @param startTime Timestamp in milliseconds to start reading event from (inclusive)
 * @param endTime Timestamp in milliseconds for the last event to read (exclusive)
 * @param limit Maximum number of events to read
 * @param callback Callback to invoke for each stream event read. If the callback function returns {@code false}upon invocation, it will stops the reading
 * @throws IOException If fails to read from stream
 * @throws StreamNotFoundException If the given stream does not exists
 */
public void getEvents(String streamId,long startTime,long endTime,int limit,Function<? super StreamEvent,Boolean> callback) throws IOException, StreamNotFoundException, UnauthorizedException {
  URL url=config.resolveNamespacedURLV3(String.format(""String_Node_Str"",streamId,startTime,endTime,limit));
  HttpURLConnection urlConn=(HttpURLConnection)url.openConnection();
  AccessToken accessToken=config.getAccessToken();
  if (accessToken != null) {
    urlConn.setRequestProperty(HttpHeaders.AUTHORIZATION,accessToken.getTokenType() + ""String_Node_Str"" + accessToken.getValue());
  }
  if (urlConn instanceof HttpsURLConnection && !config.isVerifySSLCert()) {
    try {
      HttpRequests.disableCertCheck((HttpsURLConnection)urlConn);
    }
 catch (    Exception e) {
    }
  }
  try {
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED) {
      throw new UnauthorizedException(""String_Node_Str"");
    }
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
      throw new StreamNotFoundException(streamId);
    }
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_NO_CONTENT) {
      return;
    }
    JsonReader jsonReader=new JsonReader(new InputStreamReader(urlConn.getInputStream(),Charsets.UTF_8));
    jsonReader.beginArray();
    while (jsonReader.peek() != JsonToken.END_ARRAY) {
      Boolean result=callback.apply(GSON.<StreamEvent>fromJson(jsonReader,StreamEvent.class));
      if (result == null || !result) {
        break;
      }
    }
  }
  finally {
    urlConn.disconnect();
  }
}","/** 
 * Reads events from a stream
 * @param streamId ID of the stream
 * @param startTime Timestamp in milliseconds to start reading event from (inclusive)
 * @param endTime Timestamp in milliseconds for the last event to read (exclusive)
 * @param limit Maximum number of events to read
 * @param callback Callback to invoke for each stream event read. If the callback function returns {@code false}upon invocation, it will stops the reading
 * @throws IOException If fails to read from stream
 * @throws StreamNotFoundException If the given stream does not exists
 */
public void getEvents(String streamId,long startTime,long endTime,int limit,Function<? super StreamEvent,Boolean> callback) throws IOException, StreamNotFoundException, UnauthorizedException {
  URL url=config.resolveNamespacedURLV3(String.format(""String_Node_Str"",streamId,startTime,endTime,limit));
  HttpURLConnection urlConn=(HttpURLConnection)url.openConnection();
  AccessToken accessToken=config.getAccessToken();
  if (accessToken != null) {
    urlConn.setRequestProperty(HttpHeaders.AUTHORIZATION,accessToken.getTokenType() + ""String_Node_Str"" + accessToken.getValue());
  }
  if (urlConn instanceof HttpsURLConnection && !config.isVerifySSLCert()) {
    try {
      HttpRequests.disableCertCheck((HttpsURLConnection)urlConn);
    }
 catch (    Exception e) {
    }
  }
  try {
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED) {
      throw new UnauthorizedException(""String_Node_Str"");
    }
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
      throw new StreamNotFoundException(streamId);
    }
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_NO_CONTENT) {
      return;
    }
    InputStream inputStream=urlConn.getInputStream();
    JsonReader jsonReader=new JsonReader(new InputStreamReader(inputStream,Charsets.UTF_8));
    jsonReader.beginArray();
    while (jsonReader.peek() != JsonToken.END_ARRAY) {
      Boolean result=callback.apply(GSON.<StreamEvent>fromJson(jsonReader,StreamEvent.class));
      if (result == null || !result) {
        break;
      }
    }
    drain(inputStream);
  }
  finally {
    urlConn.disconnect();
  }
}"
7164,"@Test public void testCleanupGeneration() throws Exception {
  String streamName=""String_Node_Str"";
  Id.Stream streamId=Id.Stream.from(Constants.DEFAULT_NAMESPACE,streamName);
  StreamAdmin streamAdmin=getStreamAdmin();
  streamAdmin.create(streamId);
  StreamConfig streamConfig=streamAdmin.getConfig(streamId);
  StreamFileJanitor janitor=new StreamFileJanitor(getCConfiguration(),getStreamAdmin(),getLocationFactory());
  for (int i=0; i < 5; i++) {
    FileWriter<StreamEvent> writer=createWriter(streamId);
    writer.append(StreamFileTestUtils.createEvent(System.currentTimeMillis(),""String_Node_Str""));
    writer.close();
    janitor.clean(streamConfig,System.currentTimeMillis());
    verifyGeneration(streamConfig,i);
    streamAdmin.truncate(streamId);
  }
  int generation=StreamUtils.getGeneration(streamConfig);
  Assert.assertEquals(5,generation);
  janitor.clean(streamConfig,System.currentTimeMillis());
  for (  Location location : streamConfig.getLocation().list()) {
    if (location.isDirectory()) {
      Assert.assertEquals(generation,Integer.parseInt(location.getName()));
    }
  }
}","@Test public void testCleanupGeneration() throws Exception {
  String streamName=""String_Node_Str"";
  Id.Stream streamId=Id.Stream.from(Constants.DEFAULT_NAMESPACE,streamName);
  StreamAdmin streamAdmin=getStreamAdmin();
  streamAdmin.create(streamId);
  StreamConfig streamConfig=streamAdmin.getConfig(streamId);
  StreamFileJanitor janitor=new StreamFileJanitor(getCConfiguration(),getStreamAdmin(),getLocationFactory());
  for (int i=0; i < 5; i++) {
    FileWriter<StreamEvent> writer=createWriter(streamId);
    writer.append(StreamFileTestUtils.createEvent(System.currentTimeMillis(),""String_Node_Str""));
    writer.close();
    janitor.clean(streamConfig.getLocation(),streamConfig.getTTL(),System.currentTimeMillis());
    verifyGeneration(streamConfig,i);
    streamAdmin.truncate(streamId);
  }
  int generation=StreamUtils.getGeneration(streamConfig);
  Assert.assertEquals(5,generation);
  janitor.clean(streamConfig.getLocation(),streamConfig.getTTL(),System.currentTimeMillis());
  for (  Location location : streamConfig.getLocation().list()) {
    if (location.isDirectory()) {
      Assert.assertEquals(generation,Integer.parseInt(location.getName()));
    }
  }
}"
7165,"@Test public void testCleanupTTL() throws Exception {
  String streamName=""String_Node_Str"";
  Id.Stream streamId=Id.Stream.from(Constants.DEFAULT_NAMESPACE,streamName);
  StreamAdmin streamAdmin=getStreamAdmin();
  StreamFileJanitor janitor=new StreamFileJanitor(getCConfiguration(),getStreamAdmin(),getLocationFactory());
  Properties properties=new Properties();
  properties.setProperty(Constants.Stream.PARTITION_DURATION,""String_Node_Str"");
  properties.setProperty(Constants.Stream.TTL,""String_Node_Str"");
  streamAdmin.create(streamId,properties);
  streamAdmin.truncate(streamId);
  StreamConfig config=streamAdmin.getConfig(streamId);
  FileWriter<StreamEvent> writer=createWriter(streamId);
  for (int i=0; i < 10; i++) {
    writer.append(StreamFileTestUtils.createEvent(i * 1000,""String_Node_Str"" + i));
  }
  writer.close();
  Location generationLocation=StreamUtils.createGenerationLocation(config.getLocation(),1);
  Assert.assertEquals(5,generationLocation.list().size());
  janitor.clean(config,10000);
  Assert.assertEquals(3,generationLocation.list().size());
  janitor.clean(config,16000);
  Assert.assertTrue(generationLocation.list().isEmpty());
}","@Test public void testCleanupTTL() throws Exception {
  String streamName=""String_Node_Str"";
  Id.Stream streamId=Id.Stream.from(Constants.DEFAULT_NAMESPACE,streamName);
  StreamAdmin streamAdmin=getStreamAdmin();
  StreamFileJanitor janitor=new StreamFileJanitor(getCConfiguration(),getStreamAdmin(),getLocationFactory());
  Properties properties=new Properties();
  properties.setProperty(Constants.Stream.PARTITION_DURATION,""String_Node_Str"");
  properties.setProperty(Constants.Stream.TTL,""String_Node_Str"");
  streamAdmin.create(streamId,properties);
  streamAdmin.truncate(streamId);
  StreamConfig config=streamAdmin.getConfig(streamId);
  FileWriter<StreamEvent> writer=createWriter(streamId);
  for (int i=0; i < 10; i++) {
    writer.append(StreamFileTestUtils.createEvent(i * 1000,""String_Node_Str"" + i));
  }
  writer.close();
  Location generationLocation=StreamUtils.createGenerationLocation(config.getLocation(),1);
  Assert.assertEquals(5,generationLocation.list().size());
  janitor.clean(config.getLocation(),config.getTTL(),10000);
  Assert.assertEquals(3,generationLocation.list().size());
  janitor.clean(config.getLocation(),config.getTTL(),16000);
  Assert.assertTrue(generationLocation.list().isEmpty());
}"
7166,"@Test public void testFetchSize() throws Exception {
  final String streamName=""String_Node_Str"";
  Id.Stream streamId=Id.Stream.from(Constants.DEFAULT_NAMESPACE,streamName);
  final int nbEvents=100;
  StreamAdmin streamAdmin=new TestStreamAdmin(locationFactory,Long.MAX_VALUE,1000);
  streamAdmin.create(streamId);
  StreamConfig config=streamAdmin.getConfig(streamId);
  try {
    StreamUtils.fetchStreamFilesSize(config);
    Assert.fail(""String_Node_Str"");
  }
 catch (  IOException e) {
  }
  Location partitionLocation=StreamUtils.createPartitionLocation(config.getLocation(),0,Long.MAX_VALUE);
  Location dataLocation=StreamUtils.createStreamLocation(partitionLocation,""String_Node_Str"",0,StreamFileType.EVENT);
  Location idxLocation=StreamUtils.createStreamLocation(partitionLocation,""String_Node_Str"",0,StreamFileType.INDEX);
  StreamDataFileWriter writer=new StreamDataFileWriter(Locations.newOutputSupplier(dataLocation),Locations.newOutputSupplier(idxLocation),10000L);
  for (int i=0; i < nbEvents; i++) {
    writer.append(StreamFileTestUtils.createEvent(i,""String_Node_Str""));
  }
  writer.close();
  long size=streamAdmin.fetchStreamSize(config);
  Assert.assertTrue(size > 0);
  Assert.assertEquals(dataLocation.length(),size);
}","@Test public void testFetchSize() throws Exception {
  final String streamName=""String_Node_Str"";
  Id.Stream streamId=Id.Stream.from(Constants.DEFAULT_NAMESPACE,streamName);
  final int nbEvents=100;
  StreamAdmin streamAdmin=new TestStreamAdmin(locationFactory,Long.MAX_VALUE,1000);
  streamAdmin.create(streamId);
  StreamConfig config=streamAdmin.getConfig(streamId);
  try {
    StreamUtils.fetchStreamFilesSize(StreamUtils.createGenerationLocation(config.getLocation(),StreamUtils.getGeneration(config)));
    Assert.fail(""String_Node_Str"");
  }
 catch (  IOException e) {
  }
  Location partitionLocation=StreamUtils.createPartitionLocation(config.getLocation(),0,Long.MAX_VALUE);
  Location dataLocation=StreamUtils.createStreamLocation(partitionLocation,""String_Node_Str"",0,StreamFileType.EVENT);
  Location idxLocation=StreamUtils.createStreamLocation(partitionLocation,""String_Node_Str"",0,StreamFileType.INDEX);
  StreamDataFileWriter writer=new StreamDataFileWriter(Locations.newOutputSupplier(dataLocation),Locations.newOutputSupplier(idxLocation),10000L);
  for (int i=0; i < nbEvents; i++) {
    writer.append(StreamFileTestUtils.createEvent(i,""String_Node_Str""));
  }
  writer.close();
  long size=StreamUtils.fetchStreamFilesSize(StreamUtils.createGenerationLocation(config.getLocation(),StreamUtils.getGeneration(config)));
  Assert.assertTrue(size > 0);
  Assert.assertEquals(dataLocation.length(),size);
}"
7167,"@Test public void testDropAllInNamespace() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  Id.Stream otherStream=Id.Stream.from(OTHER_NAMESPACE,""String_Node_Str"");
  List<Id.Stream> fooStreams=Lists.newArrayList();
  for (int i=0; i < 4; i++) {
    fooStreams.add(Id.Stream.from(FOO_NAMESPACE,""String_Node_Str"" + i));
  }
  List<Id.Stream> allStreams=Lists.newArrayList();
  allStreams.addAll(fooStreams);
  allStreams.add(otherStream);
  for (  Id.Stream stream : allStreams) {
    streamAdmin.create(stream);
    writeEvent(stream);
    Assert.assertNotEquals(0,getStreamSize(stream));
  }
  streamAdmin.dropAllInNamespace(Id.Namespace.from(FOO_NAMESPACE));
  for (  Id.Stream defaultStream : fooStreams) {
    Assert.assertEquals(0,getStreamSize(defaultStream));
  }
  Assert.assertNotEquals(0,getStreamSize(otherStream));
  streamAdmin.truncate(otherStream);
  Assert.assertEquals(0,getStreamSize(otherStream));
}","@Test public void testDropAllInNamespace() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  Id.Stream otherStream=Id.Stream.from(OTHER_NAMESPACE,""String_Node_Str"");
  List<Id.Stream> fooStreams=Lists.newArrayList();
  for (int i=0; i < 4; i++) {
    fooStreams.add(Id.Stream.from(FOO_NAMESPACE,""String_Node_Str"" + i));
  }
  List<Id.Stream> allStreams=Lists.newArrayList();
  allStreams.addAll(fooStreams);
  allStreams.add(otherStream);
  for (  Id.Stream stream : allStreams) {
    streamAdmin.create(stream);
    writeEvent(stream);
    Assert.assertNotEquals(0,getStreamSize(stream));
  }
  streamAdmin.dropAllInNamespace(Id.Namespace.from(FOO_NAMESPACE));
  for (  Id.Stream defaultStream : fooStreams) {
    Assert.assertFalse(streamAdmin.exists(defaultStream));
  }
  Assert.assertNotEquals(0,getStreamSize(otherStream));
  streamAdmin.truncate(otherStream);
  Assert.assertEquals(0,getStreamSize(otherStream));
}"
7168,"private long getStreamSize(Id.Stream streamId) throws IOException {
  StreamAdmin streamAdmin=getStreamAdmin();
  StreamConfig config=streamAdmin.getConfig(streamId);
  return streamAdmin.fetchStreamSize(config);
}","private long getStreamSize(Id.Stream streamId) throws IOException {
  StreamAdmin streamAdmin=getStreamAdmin();
  StreamConfig config=streamAdmin.getConfig(streamId);
  Location generationLocation=StreamUtils.createGenerationLocation(config.getLocation(),StreamUtils.getGeneration(config));
  return StreamUtils.fetchStreamFilesSize(generationLocation);
}"
7169,"@BeforeClass public static void init() throws Exception {
  InMemoryZKServer zkServer=InMemoryZKServer.builder().setDataDir(tmpFolder.newFolder()).build();
  zkServer.startAndWait();
  testHBase=new HBaseTestFactory().get();
  testHBase.startHBase();
  Configuration hConf=testHBase.getConfiguration();
  CConfiguration cConf=CConfiguration.create();
  cConf.setInt(Constants.Stream.CONTAINER_INSTANCES,1);
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new TransactionMetricsModule(),new DataSetsModules().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),Modules.override(new DataFabricDistributedModule(),new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(TransactionStateStorage.class).to(NoOpTransactionStateStorage.class);
      bind(TransactionSystemClient.class).to(InMemoryTxSystemClient.class).in(Singleton.class);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
    }
  }
));
  ZKClientService zkClientService=injector.getInstance(ZKClientService.class);
  zkClientService.startAndWait();
  streamAdmin=injector.getInstance(StreamAdmin.class);
  txManager=injector.getInstance(TransactionManager.class);
  fileWriterFactory=injector.getInstance(StreamFileWriterFactory.class);
  setupNamespaces(injector.getInstance(LocationFactory.class));
  txManager.startAndWait();
}","@BeforeClass public static void init() throws Exception {
  InMemoryZKServer zkServer=InMemoryZKServer.builder().setDataDir(tmpFolder.newFolder()).build();
  zkServer.startAndWait();
  testHBase=new HBaseTestFactory().get();
  testHBase.startHBase();
  Configuration hConf=testHBase.getConfiguration();
  CConfiguration cConf=CConfiguration.create();
  cConf.setInt(Constants.Stream.CONTAINER_INSTANCES,1);
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new TransactionMetricsModule(),new DataSetsModules().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),Modules.override(new DataFabricDistributedModule(),new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(TransactionStateStorage.class).to(NoOpTransactionStateStorage.class);
      bind(TransactionSystemClient.class).to(InMemoryTxSystemClient.class).in(Singleton.class);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
    }
  }
));
  ZKClientService zkClientService=injector.getInstance(ZKClientService.class);
  zkClientService.startAndWait();
  streamAdmin=injector.getInstance(StreamAdmin.class);
  txManager=injector.getInstance(TransactionManager.class);
  fileWriterFactory=injector.getInstance(StreamFileWriterFactory.class);
  streamCoordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  setupNamespaces(injector.getInstance(LocationFactory.class));
  txManager.startAndWait();
  streamCoordinatorClient.startAndWait();
}"
7170,"@AfterClass public static void finish() throws Exception {
  txManager.stopAndWait();
  testHBase.stopHBase();
}","@AfterClass public static void finish() throws Exception {
  streamCoordinatorClient.stopAndWait();
  txManager.stopAndWait();
  testHBase.stopHBase();
}"
7171,"@Override public void updateProperties(Id.Stream streamId,Callable<CoordinatorStreamProperties> action) throws Exception {
  Lock lock=getLock(streamId);
  lock.lock();
  try {
    final CoordinatorStreamProperties properties=action.call();
    propertyStore.update(streamId.toId(),new SyncPropertyUpdater<CoordinatorStreamProperties>(){
      @Override protected CoordinatorStreamProperties compute(      @Nullable CoordinatorStreamProperties oldProperties){
        if (oldProperties == null) {
          return properties;
        }
        return new CoordinatorStreamProperties(firstNotNull(properties.getTTL(),oldProperties.getTTL()),firstNotNull(properties.getFormat(),oldProperties.getFormat()),firstNotNull(properties.getNotificationThresholdMB(),oldProperties.getNotificationThresholdMB()),firstNotNull(properties.getGeneration(),oldProperties.getGeneration()));
      }
    }
).get();
  }
  finally {
    lock.unlock();
  }
}","/** 
 * Updates stream properties in the property store.
 */
private ListenableFuture<CoordinatorStreamProperties> updateProperties(Id.Stream streamId,final CoordinatorStreamProperties properties){
  return propertyStore.update(streamId.toId(),new SyncPropertyUpdater<CoordinatorStreamProperties>(){
    @Override protected CoordinatorStreamProperties compute(    @Nullable CoordinatorStreamProperties oldProperties){
      if (oldProperties == null) {
        return properties;
      }
      return new CoordinatorStreamProperties(firstNotNull(properties.getTTL(),oldProperties.getTTL()),firstNotNull(properties.getFormat(),oldProperties.getFormat()),firstNotNull(properties.getNotificationThresholdMB(),oldProperties.getNotificationThresholdMB()),firstNotNull(properties.getGeneration(),oldProperties.getGeneration()));
    }
  }
);
}"
7172,"@Nullable @Override public ResourceRequirement apply(@Nullable ResourceRequirement existingRequirement){
  LOG.debug(""String_Node_Str"",streamId);
  Set<ResourceRequirement.Partition> partitions;
  if (existingRequirement != null) {
    partitions=existingRequirement.getPartitions();
  }
 else {
    partitions=ImmutableSet.of();
  }
  ResourceRequirement.Partition newPartition=new ResourceRequirement.Partition(streamId.toId(),1);
  if (partitions.contains(newPartition)) {
    return null;
  }
  ResourceRequirement.Builder builder=ResourceRequirement.builder(Constants.Service.STREAMS);
  builder.addPartition(newPartition);
  for (  ResourceRequirement.Partition partition : partitions) {
    builder.addPartition(partition);
  }
  return builder.build();
}","@Nullable @Override public ResourceRequirement apply(@Nullable ResourceRequirement existingRequirement){
  LOG.debug(""String_Node_Str"",streamId);
  if (existingRequirement == null) {
    return null;
  }
  Set<ResourceRequirement.Partition> partitions=existingRequirement.getPartitions();
  ResourceRequirement.Builder builder=ResourceRequirement.builder(Constants.Service.STREAMS);
  for (  ResourceRequirement.Partition partition : partitions) {
    if (!partition.getName().equals(streamId.toId())) {
      builder.addPartition(partition);
    }
  }
  return builder.build();
}"
7173,"/** 
 * Reads events from a stream
 * @param streamId ID of the stream
 * @param startTime Timestamp in milliseconds to start reading event from (inclusive)
 * @param endTime Timestamp in milliseconds for the last event to read (exclusive)
 * @param limit Maximum number of events to read
 * @param callback Callback to invoke for each stream event read. If the callback function returns {@code false}upon invocation, it will stops the reading
 * @throws IOException If fails to read from stream
 * @throws StreamNotFoundException If the given stream does not exists
 */
public void getEvents(String streamId,long startTime,long endTime,int limit,Function<? super StreamEvent,Boolean> callback) throws IOException, StreamNotFoundException, UnauthorizedException {
  URL url=config.resolveNamespacedURLV3(String.format(""String_Node_Str"",streamId,startTime,endTime,limit));
  HttpURLConnection urlConn=(HttpURLConnection)url.openConnection();
  AccessToken accessToken=config.getAccessToken();
  if (accessToken != null) {
    urlConn.setRequestProperty(HttpHeaders.AUTHORIZATION,accessToken.getTokenType() + ""String_Node_Str"" + accessToken.getValue());
  }
  if (urlConn instanceof HttpsURLConnection && !config.isVerifySSLCert()) {
    try {
      HttpRequests.disableCertCheck((HttpsURLConnection)urlConn);
    }
 catch (    Exception e) {
    }
  }
  try {
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED) {
      throw new UnauthorizedException(""String_Node_Str"");
    }
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
      throw new StreamNotFoundException(streamId);
    }
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_NO_CONTENT) {
      return;
    }
    JsonReader jsonReader=new JsonReader(new InputStreamReader(urlConn.getInputStream(),Charsets.UTF_8));
    jsonReader.beginArray();
    while (jsonReader.peek() != JsonToken.END_ARRAY) {
      Boolean result=callback.apply(GSON.<StreamEvent>fromJson(jsonReader,StreamEvent.class));
      if (result == null || !result) {
        break;
      }
    }
  }
  finally {
    urlConn.disconnect();
  }
}","/** 
 * Reads events from a stream
 * @param streamId ID of the stream
 * @param startTime Timestamp in milliseconds to start reading event from (inclusive)
 * @param endTime Timestamp in milliseconds for the last event to read (exclusive)
 * @param limit Maximum number of events to read
 * @param callback Callback to invoke for each stream event read. If the callback function returns {@code false}upon invocation, it will stops the reading
 * @throws IOException If fails to read from stream
 * @throws StreamNotFoundException If the given stream does not exists
 */
public void getEvents(String streamId,long startTime,long endTime,int limit,Function<? super StreamEvent,Boolean> callback) throws IOException, StreamNotFoundException, UnauthorizedException {
  URL url=config.resolveNamespacedURLV3(String.format(""String_Node_Str"",streamId,startTime,endTime,limit));
  HttpURLConnection urlConn=(HttpURLConnection)url.openConnection();
  AccessToken accessToken=config.getAccessToken();
  if (accessToken != null) {
    urlConn.setRequestProperty(HttpHeaders.AUTHORIZATION,accessToken.getTokenType() + ""String_Node_Str"" + accessToken.getValue());
  }
  if (urlConn instanceof HttpsURLConnection && !config.isVerifySSLCert()) {
    try {
      HttpRequests.disableCertCheck((HttpsURLConnection)urlConn);
    }
 catch (    Exception e) {
    }
  }
  try {
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED) {
      throw new UnauthorizedException(""String_Node_Str"");
    }
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
      throw new StreamNotFoundException(streamId);
    }
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_NO_CONTENT) {
      return;
    }
    InputStream inputStream=urlConn.getInputStream();
    JsonReader jsonReader=new JsonReader(new InputStreamReader(inputStream,Charsets.UTF_8));
    jsonReader.beginArray();
    while (jsonReader.peek() != JsonToken.END_ARRAY) {
      Boolean result=callback.apply(GSON.<StreamEvent>fromJson(jsonReader,StreamEvent.class));
      if (result == null || !result) {
        break;
      }
    }
    drain(inputStream);
  }
  finally {
    urlConn.disconnect();
  }
}"
7174,"@Override public void dropAllInNamespace(Id.Namespace namespace) throws Exception {
  List<Location> locations;
  try {
    locations=getStreamsHomeLocation(namespace).list();
  }
 catch (  FileNotFoundException e) {
    locations=ImmutableList.of();
  }
  for (  final Location streamLocation : locations) {
    doTruncate(streamLocation);
  }
  stateStoreFactory.dropAllInNamespace(namespace);
}","@Override public void dropAllInNamespace(Id.Namespace namespace) throws Exception {
  List<Location> locations;
  try {
    locations=getStreamsHomeLocation(namespace).list();
  }
 catch (  FileNotFoundException e) {
    locations=ImmutableList.of();
  }
  for (  final Location streamLocation : locations) {
    doTruncate(streamLocation);
    alterExploreStream(StreamUtils.getStreamIdFromLocation(streamLocation),false);
  }
  stateStoreFactory.dropAllInNamespace(namespace);
}"
7175,"@Inject private DatasetTypeMDSUpgrader(TransactionExecutorFactory executorFactory,DatasetFramework dsFramework,LocationFactory locationFactory,Configuration hConf,HBaseTableUtil tableUtil){
  this.executorFactory=executorFactory;
  this.dsFramework=dsFramework;
  this.locationFactory=locationFactory;
  this.hConf=hConf;
  this.tableUtil=tableUtil;
}","@Inject private DatasetTypeMDSUpgrader(TransactionExecutorFactory executorFactory,DatasetFramework dsFramework,LocationFactory locationFactory){
  this.executorFactory=executorFactory;
  this.dsFramework=dsFramework;
  this.locationFactory=locationFactory;
}"
7176,"/** 
 * Upgrades the   {@link DatasetTypeMDS} table for namespacesNote: We don't write to new TypeMDS table through  {@link DatasetTypeManager} because if the user's custom Datasetshas api/classes changes  {@link DatasetTypeManager#addModule} will fail. So, we directly move the meta typeinformation. User's custom datasets which don't use any such changed api/classes will work out of the box but the one which does will need to be re-deployed.
 * @throws TransactionFailureException
 * @throws InterruptedException
 * @throws IOException
 */
public void upgrade() throws Exception {
  DatasetTypeMDS oldMds=getOldDatasetTypeMDS();
  if (oldMds != null) {
    setupDatasetTypeMDS(oldMds);
    final MDSKey dsModulePrefix=new MDSKey(Bytes.toBytes(DatasetTypeMDS.MODULES_PREFIX));
    try {
      datasetTypeMDS.execute(new TransactionExecutor.Function<UpgradeMDSStores<DatasetTypeMDS>,Void>(){
        @Override public Void apply(        UpgradeMDSStores<DatasetTypeMDS> ctx) throws Exception {
          List<DatasetModuleMeta> mdsKeyDatasetModuleMetaMap=ctx.getOldMds().list(dsModulePrefix,DatasetModuleMeta.class);
          for (          DatasetModuleMeta datasetModuleMeta : mdsKeyDatasetModuleMetaMap) {
            if (!REMOVED_DATASET_MODULES.contains(datasetModuleMeta.getClassName())) {
              upgradeDatasetModuleMeta(datasetModuleMeta,ctx.getNewMds());
            }
          }
          return null;
        }
      }
);
    }
 catch (    Exception e) {
      throw e;
    }
    tableUtil.dropTable(new HBaseAdmin(hConf),TableId.from(oldDatasetId.getNamespaceId(),oldDatasetId.getId()));
  }
 else {
    LOG.info(""String_Node_Str"",oldDatasetId.getId());
  }
}","/** 
 * Upgrades the   {@link DatasetTypeMDS} table for namespacesNote: We don't write to new TypeMDS table through  {@link DatasetTypeManager} because if the user's custom Datasetshas api/classes changes  {@link DatasetTypeManager#addModule} will fail. So, we directly move the meta typeinformation. User's custom datasets which don't use any such changed api/classes will work out of the box but the one which does will need to be re-deployed.
 * @throws TransactionFailureException
 * @throws InterruptedException
 * @throws IOException
 */
public void upgrade() throws Exception {
  DatasetTypeMDS oldMds=getOldDatasetTypeMDS();
  if (oldMds != null) {
    setupDatasetTypeMDS(oldMds);
    final MDSKey dsModulePrefix=new MDSKey(Bytes.toBytes(DatasetTypeMDS.MODULES_PREFIX));
    try {
      datasetTypeMDS.execute(new TransactionExecutor.Function<UpgradeMDSStores<DatasetTypeMDS>,Void>(){
        @Override public Void apply(        UpgradeMDSStores<DatasetTypeMDS> ctx) throws Exception {
          List<DatasetModuleMeta> mdsKeyDatasetModuleMetaMap=ctx.getOldMds().list(dsModulePrefix,DatasetModuleMeta.class);
          for (          DatasetModuleMeta datasetModuleMeta : mdsKeyDatasetModuleMetaMap) {
            if (!REMOVED_DATASET_MODULES.contains(datasetModuleMeta.getClassName())) {
              upgradeDatasetModuleMeta(datasetModuleMeta,ctx.getNewMds());
            }
          }
          return null;
        }
      }
);
    }
 catch (    Exception e) {
      throw e;
    }
  }
 else {
    LOG.info(""String_Node_Str"",oldDatasetId.getId());
  }
}"
7177,"@Inject private MDSUpgrader(LocationFactory locationFactory,TransactionExecutorFactory executorFactory,final DatasetFramework dsFramework,CConfiguration cConf,@Named(""String_Node_Str"") final Store store){
  super(locationFactory);
  this.cConf=cConf;
  this.store=store;
  this.appMDS=Transactional.of(executorFactory,new Supplier<AppMDS>(){
    @Override public AppMDS get(){
      try {
        Table table=DatasetsUtil.getOrCreateDataset(dsFramework,Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE)),""String_Node_Str"",DatasetProperties.EMPTY,DatasetDefinition.NO_ARGUMENTS,null);
        return new AppMDS(new AppMetadataStoreDataset(table));
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"",Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE),e);
        throw Throwables.propagate(e);
      }
    }
  }
);
  appStreams=Sets.newHashSet();
}","@Inject private MDSUpgrader(LocationFactory locationFactory,TransactionExecutorFactory executorFactory,final DatasetFramework dsFramework,CConfiguration cConf,@Named(""String_Node_Str"") final Store store){
  super(locationFactory);
  this.cConf=cConf;
  this.store=store;
  final String appMetaTableName=Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE);
  this.appMDS=Transactional.of(executorFactory,new Supplier<AppMDS>(){
    @Override public AppMDS get(){
      try {
        Table table=DatasetsUtil.getOrCreateDataset(dsFramework,Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,appMetaTableName),""String_Node_Str"",DatasetProperties.EMPTY,DatasetDefinition.NO_ARGUMENTS,null);
        return new AppMDS(new AppMetadataStoreDataset(table));
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"",appMetaTableName,e);
        throw Throwables.propagate(e);
      }
    }
  }
);
  this.appStreams=Sets.newHashSet();
  this.appMetaTableId=TableId.from(Constants.DEFAULT_NAMESPACE_ID,appMetaTableName);
}"
7178,"@Override public AppMDS get(){
  try {
    Table table=DatasetsUtil.getOrCreateDataset(dsFramework,Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE)),""String_Node_Str"",DatasetProperties.EMPTY,DatasetDefinition.NO_ARGUMENTS,null);
    return new AppMDS(new AppMetadataStoreDataset(table));
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE),e);
    throw Throwables.propagate(e);
  }
}","@Override public AppMDS get(){
  try {
    Table table=DatasetsUtil.getOrCreateDataset(dsFramework,Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,appMetaTableName),""String_Node_Str"",DatasetProperties.EMPTY,DatasetDefinition.NO_ARGUMENTS,null);
    return new AppMDS(new AppMetadataStoreDataset(table));
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",appMetaTableName,e);
    throw Throwables.propagate(e);
  }
}"
7179,"/** 
 * Creates the   {@link Constants#SYSTEM_NAMESPACE} in hbase and {@link Constants#DEFAULT_NAMESPACE} namespace and alsoadds it to the store
 */
private void createNamespaces(){
  LOG.info(""String_Node_Str"",Constants.SYSTEM_NAMESPACE_ID);
  try {
    HBaseAdmin admin=new HBaseAdmin(hConf);
    hBaseTableUtil.createNamespaceIfNotExists(admin,Constants.SYSTEM_NAMESPACE_ID);
  }
 catch (  MasterNotRunningException e) {
    Throwables.propagate(e);
  }
catch (  ZooKeeperConnectionException e) {
    Throwables.propagate(e);
  }
catch (  IOException e) {
    Throwables.propagate(e);
  }
  LOG.info(""String_Node_Str"",Constants.DEFAULT_NAMESPACE);
  getStore().createNamespace(new NamespaceMeta.Builder().setName(Constants.DEFAULT_NAMESPACE).setDescription(Constants.DEFAULT_NAMESPACE).build());
}","/** 
 * Creates the   {@link Constants#SYSTEM_NAMESPACE} in hbase and {@link Constants#DEFAULT_NAMESPACE} namespace and alsoadds it to the store
 */
private void createNamespaces(){
  LOG.info(""String_Node_Str"",Constants.SYSTEM_NAMESPACE_ID);
  try {
    HBaseAdmin admin=new HBaseAdmin(hConf);
    hBaseTableUtil.createNamespaceIfNotExists(admin,Constants.SYSTEM_NAMESPACE_ID);
  }
 catch (  MasterNotRunningException e) {
    Throwables.propagate(e);
  }
catch (  ZooKeeperConnectionException e) {
    Throwables.propagate(e);
  }
catch (  IOException e) {
    Throwables.propagate(e);
  }
  LOG.info(""String_Node_Str"",Constants.DEFAULT_NAMESPACE);
  getStore().createNamespace(Constants.DEFAULT_NAMESPACE_META);
}"
7180,"public UpgradeTool() throws Exception {
  cConf=CConfiguration.create();
  hConf=HBaseConfiguration.create();
  this.injector=init();
  txService=injector.getInstance(TransactionService.class);
  zkClientService=injector.getInstance(ZKClientService.class);
  hBaseTableUtil=injector.getInstance(HBaseTableUtil.class);
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        UpgradeTool.this.stop();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
);
}","public UpgradeTool() throws Exception {
  this.cConf=CConfiguration.create();
  this.hConf=HBaseConfiguration.create();
  this.injector=init();
  this.txService=injector.getInstance(TransactionService.class);
  this.zkClientService=injector.getInstance(ZKClientService.class);
  this.hBaseTableUtil=injector.getInstance(HBaseTableUtil.class);
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        UpgradeTool.this.stop();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
);
}"
7181,"private void performUpgrade() throws Exception {
  LOG.info(""String_Node_Str"");
  DatasetUpgrader dsUpgrade=injector.getInstance(DatasetUpgrader.class);
  dsUpgrade.upgrade();
  LOG.info(""String_Node_Str"");
  MDSUpgrader mdsUpgrader=injector.getInstance(MDSUpgrader.class);
  mdsUpgrader.upgrade();
  LOG.info(""String_Node_Str"");
  ArchiveUpgrader archiveUpgrader=injector.getInstance(ArchiveUpgrader.class);
  archiveUpgrader.upgrade();
  LOG.info(""String_Node_Str"");
  getFileMetaDataManager().upgrade();
  LOG.info(""String_Node_Str"");
  StreamStateStoreUpgrader streamStateStoreUpgrader=injector.getInstance(StreamStateStoreUpgrader.class);
  streamStateStoreUpgrader.upgrade();
  LOG.info(""String_Node_Str"");
  QueueConfigUpgrader queueConfigUpgrader=injector.getInstance(QueueConfigUpgrader.class);
  queueConfigUpgrader.upgrade();
}","private void performUpgrade() throws Exception {
  LOG.info(""String_Node_Str"");
  HBaseAdmin hBaseAdmin=new HBaseAdmin(hConf);
  DatasetUpgrader dsUpgrade=injector.getInstance(DatasetUpgrader.class);
  dsUpgrade.upgrade();
  hBaseTableUtil.dropTable(hBaseAdmin,dsUpgrade.getDatasetInstanceMDSUpgrader().getOldDatasetInstanceTableId());
  hBaseTableUtil.dropTable(hBaseAdmin,dsUpgrade.getDatasetTypeMDSUpgrader().getOldDatasetTypeTableId());
  LOG.info(""String_Node_Str"");
  MDSUpgrader mdsUpgrader=injector.getInstance(MDSUpgrader.class);
  mdsUpgrader.upgrade();
  hBaseTableUtil.dropTable(hBaseAdmin,mdsUpgrader.getOldAppMetaTableId());
  LOG.info(""String_Node_Str"");
  ArchiveUpgrader archiveUpgrader=injector.getInstance(ArchiveUpgrader.class);
  archiveUpgrader.upgrade();
  LOG.info(""String_Node_Str"");
  getFileMetaDataManager().upgrade();
  hBaseTableUtil.dropTable(hBaseAdmin,getFileMetaDataManager().getOldLogMetaTableId());
  LOG.info(""String_Node_Str"");
  StreamStateStoreUpgrader streamStateStoreUpgrader=injector.getInstance(StreamStateStoreUpgrader.class);
  streamStateStoreUpgrader.upgrade();
  LOG.info(""String_Node_Str"");
  QueueConfigUpgrader queueConfigUpgrader=injector.getInstance(QueueConfigUpgrader.class);
  queueConfigUpgrader.upgrade();
}"
7182,"@Inject public FileMetaDataManager(final LogSaverTableUtil tableUtil,TransactionExecutorFactory txExecutorFactory,LocationFactory locationFactory,DatasetFramework dsFramework,CConfiguration cConf){
  this.dsFramework=dsFramework;
  this.txExecutorFactory=txExecutorFactory;
  this.mds=Transactional.of(txExecutorFactory,new Supplier<DatasetContext<Table>>(){
    @Override public DatasetContext<Table> get(){
      try {
        return DatasetContext.of(tableUtil.getMetaTable());
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
  this.locationFactory=locationFactory;
  this.logBaseDir=cConf.get(LoggingConfiguration.LOG_BASE_DIR);
}","@Inject public FileMetaDataManager(final LogSaverTableUtil tableUtil,TransactionExecutorFactory txExecutorFactory,LocationFactory locationFactory,DatasetFramework dsFramework,CConfiguration cConf){
  this.dsFramework=dsFramework;
  this.txExecutorFactory=txExecutorFactory;
  this.mds=Transactional.of(txExecutorFactory,new Supplier<DatasetContext<Table>>(){
    @Override public DatasetContext<Table> get(){
      try {
        return DatasetContext.of(tableUtil.getMetaTable());
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
  this.locationFactory=locationFactory;
  this.logBaseDir=cConf.get(LoggingConfiguration.LOG_BASE_DIR);
  this.metaTableName=tableUtil.getMetaTableName();
}"
7183,"private Stream(final Namespace namespace,final String streamName){
  Preconditions.checkNotNull(namespace,""String_Node_Str"");
  Preconditions.checkNotNull(streamName,""String_Node_Str"");
  Preconditions.checkArgument(isId(streamName),String.format(""String_Node_Str"" + ""String_Node_Str"",streamName));
  this.namespace=namespace;
  this.streamName=streamName;
}","private Stream(final Namespace namespace,final String streamName){
  Preconditions.checkNotNull(namespace,""String_Node_Str"");
  Preconditions.checkNotNull(streamName,""String_Node_Str"");
  Preconditions.checkArgument(isId(streamName),""String_Node_Str"" + ""String_Node_Str"",streamName);
  this.namespace=namespace;
  this.streamName=streamName;
}"
7184,"@Inject private DatasetInstanceMDSUpgrader(final TransactionExecutorFactory executorFactory,@Named(""String_Node_Str"") final DatasetFramework dsFramework){
  this.datasetInstanceMds=Transactional.of(executorFactory,new Supplier<UpgradeMDSStores<DatasetInstanceMDS>>(){
    @Override public UpgradeMDSStores<DatasetInstanceMDS> get(){
      String dsName=Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DatasetMetaTableUtil.INSTANCE_TABLE_NAME);
      Id.DatasetInstance datasetId=Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,dsName);
      DatasetInstanceMDS oldMds;
      DatasetInstanceMDS newMds;
      try {
        oldMds=DatasetsUtil.getOrCreateDataset(dsFramework,datasetId,DatasetInstanceMDS.class.getName(),DatasetProperties.EMPTY,DatasetDefinition.NO_ARGUMENTS,null);
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"",datasetId,e);
        throw Throwables.propagate(e);
      }
      try {
        newMds=new DatasetMetaTableUtil(dsFramework).getInstanceMetaTable();
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"");
        throw Throwables.propagate(e);
      }
      return new UpgradeMDSStores<DatasetInstanceMDS>(oldMds,newMds);
    }
  }
);
}","@Inject private DatasetInstanceMDSUpgrader(final TransactionExecutorFactory executorFactory,final DatasetFramework dsFramework){
  this.datasetInstanceMds=Transactional.of(executorFactory,new Supplier<UpgradeMDSStores<DatasetInstanceMDS>>(){
    @Override public UpgradeMDSStores<DatasetInstanceMDS> get(){
      String dsName=Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DatasetMetaTableUtil.INSTANCE_TABLE_NAME);
      Id.DatasetInstance datasetId=Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,dsName);
      DatasetInstanceMDS oldMds;
      DatasetInstanceMDS newMds;
      try {
        oldMds=DatasetsUtil.getOrCreateDataset(dsFramework,datasetId,DatasetInstanceMDS.class.getName(),DatasetProperties.EMPTY,DatasetDefinition.NO_ARGUMENTS,null);
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"",datasetId,e);
        throw Throwables.propagate(e);
      }
      try {
        newMds=new DatasetMetaTableUtil(dsFramework).getInstanceMetaTable();
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"");
        throw Throwables.propagate(e);
      }
      return new UpgradeMDSStores<DatasetInstanceMDS>(oldMds,newMds);
    }
  }
);
}"
7185,"@Inject private DatasetTypeMDSUpgrader(final TransactionExecutorFactory executorFactory,@Named(""String_Node_Str"") final DatasetFramework dsFramework,LocationFactory locationFactory,Configuration hConf,HBaseTableUtil tableUtil){
  this.executorFactory=executorFactory;
  this.dsFramework=dsFramework;
  this.locationFactory=locationFactory;
  this.hConf=hConf;
  this.tableUtil=tableUtil;
}","@Inject private DatasetTypeMDSUpgrader(TransactionExecutorFactory executorFactory,DatasetFramework dsFramework,LocationFactory locationFactory,Configuration hConf,HBaseTableUtil tableUtil){
  this.executorFactory=executorFactory;
  this.dsFramework=dsFramework;
  this.locationFactory=locationFactory;
  this.hConf=hConf;
  this.tableUtil=tableUtil;
}"
7186,"@Inject private DatasetUpgrader(CConfiguration cConf,Configuration hConf,LocationFactory locationFactory,QueueAdmin queueAdmin,HBaseTableUtil hBaseTableUtil,@Named(""String_Node_Str"") final DatasetFramework dsFramework,DatasetInstanceMDSUpgrader datasetInstanceMDSUpgrader,DatasetTypeMDSUpgrader datasetTypeMDSUpgrader){
  super(locationFactory);
  this.cConf=cConf;
  this.hConf=hConf;
  this.locationFactory=locationFactory;
  this.queueAdmin=queueAdmin;
  this.hBaseTableUtil=hBaseTableUtil;
  this.dsFramework=dsFramework;
  this.datasetInstanceMDSUpgrader=datasetInstanceMDSUpgrader;
  this.datasetTypeMDSUpgrader=datasetTypeMDSUpgrader;
  this.userTablePrefix=Pattern.compile(String.format(""String_Node_Str"",cConf.get(Constants.Dataset.TABLE_PREFIX)));
}","@Inject private DatasetUpgrader(CConfiguration cConf,Configuration hConf,LocationFactory locationFactory,QueueAdmin queueAdmin,HBaseTableUtil hBaseTableUtil,DatasetFramework dsFramework,DatasetInstanceMDSUpgrader datasetInstanceMDSUpgrader,DatasetTypeMDSUpgrader datasetTypeMDSUpgrader){
  super(locationFactory);
  this.cConf=cConf;
  this.hConf=hConf;
  this.locationFactory=locationFactory;
  this.queueAdmin=queueAdmin;
  this.hBaseTableUtil=hBaseTableUtil;
  this.dsFramework=dsFramework;
  this.datasetInstanceMDSUpgrader=datasetInstanceMDSUpgrader;
  this.datasetTypeMDSUpgrader=datasetTypeMDSUpgrader;
  this.userTablePrefix=Pattern.compile(String.format(""String_Node_Str"",cConf.get(Constants.Dataset.TABLE_PREFIX)));
}"
7187,"@Inject private MDSUpgrader(LocationFactory locationFactory,TransactionExecutorFactory executorFactory,@Named(""String_Node_Str"") final DatasetFramework dsFramework,CConfiguration cConf,@Named(""String_Node_Str"") final Store store){
  super(locationFactory);
  this.cConf=cConf;
  this.store=store;
  this.appMDS=Transactional.of(executorFactory,new Supplier<AppMDS>(){
    @Override public AppMDS get(){
      try {
        Table table=DatasetsUtil.getOrCreateDataset(dsFramework,Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE)),""String_Node_Str"",DatasetProperties.EMPTY,DatasetDefinition.NO_ARGUMENTS,null);
        return new AppMDS(new AppMetadataStoreDataset(table));
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"",Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE),e);
        throw Throwables.propagate(e);
      }
    }
  }
);
  appStreams=Sets.newHashSet();
}","@Inject private MDSUpgrader(LocationFactory locationFactory,TransactionExecutorFactory executorFactory,final DatasetFramework dsFramework,CConfiguration cConf,@Named(""String_Node_Str"") final Store store){
  super(locationFactory);
  this.cConf=cConf;
  this.store=store;
  this.appMDS=Transactional.of(executorFactory,new Supplier<AppMDS>(){
    @Override public AppMDS get(){
      try {
        Table table=DatasetsUtil.getOrCreateDataset(dsFramework,Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE)),""String_Node_Str"",DatasetProperties.EMPTY,DatasetDefinition.NO_ARGUMENTS,null);
        return new AppMDS(new AppMetadataStoreDataset(table));
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"",Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE),e);
        throw Throwables.propagate(e);
      }
    }
  }
);
  appStreams=Sets.newHashSet();
}"
7188,"/** 
 * Sets up a   {@link DatasetFramework} instance for standalone usage.  NOTE: should NOT be used by applications!!!
 */
private DatasetFramework createRegisteredDatasetFramework(CConfiguration cConf,DatasetDefinitionRegistryFactory registryFactory) throws DatasetManagementException, IOException {
  DatasetFramework datasetFramework=new InMemoryDatasetFramework(registryFactory,cConf);
  addModules(datasetFramework);
  DatasetMetaTableUtil.setupDatasets(datasetFramework);
  DefaultStore.setupDatasets(datasetFramework);
  DefaultConfigStore.setupDatasets(datasetFramework);
  LogSaverTableUtil.setupDatasets(datasetFramework);
  ScheduleStoreTableUtil.setupDatasets(datasetFramework);
  DefaultMetricDatasetFactory factory=new DefaultMetricDatasetFactory(cConf,datasetFramework);
  DefaultMetricDatasetFactory.setupDatasets(factory);
  return datasetFramework;
}","/** 
 * Sets up a   {@link DatasetFramework} instance for standalone usage.  NOTE: should NOT be used by applications!!!
 */
private DatasetFramework createRegisteredDatasetFramework(CConfiguration cConf,DatasetDefinitionRegistryFactory registryFactory,Map<String,DatasetModule> defaultModules) throws DatasetManagementException, IOException {
  DatasetFramework datasetFramework=new InMemoryDatasetFramework(registryFactory,defaultModules,cConf);
  DatasetMetaTableUtil.setupDatasets(datasetFramework);
  DefaultStore.setupDatasets(datasetFramework);
  DefaultConfigStore.setupDatasets(datasetFramework);
  LogSaverTableUtil.setupDatasets(datasetFramework);
  ScheduleStoreTableUtil.setupDatasets(datasetFramework);
  DefaultMetricDatasetFactory factory=new DefaultMetricDatasetFactory(cConf,datasetFramework);
  DefaultMetricDatasetFactory.setupDatasets(factory);
  return datasetFramework;
}"
7189,"private Injector init() throws Exception {
  return Guice.createInjector(new ConfigModule(cConf,hConf),new LocationRuntimeModule().getDistributedModules(),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new TwillModule(),new ProgramRunnerRuntimeModule().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      install(new DataFabricDistributedModule());
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).toInstance(createNoopScheduler());
      bind(DatasetFramework.class).to(RemoteDatasetFramework.class);
      bind(DatasetTypeClassLoaderFactory.class).to(DistributedDatasetTypeClassLoaderFactory.class);
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(NamespaceAdmin.class).to(DefaultNamespaceAdmin.class);
      install(new FactoryModuleBuilder().implement(Store.class,DefaultStore.class).build(StoreFactory.class));
      bind(ConfigStore.class).to(DefaultConfigStore.class);
    }
    @Provides @Singleton @Named(""String_Node_Str"") public DatasetFramework getDSFramework(    CConfiguration cConf,    DatasetDefinitionRegistryFactory registryFactory) throws IOException, DatasetManagementException {
      return createRegisteredDatasetFramework(cConf,registryFactory);
    }
    @Provides @Singleton @Named(""String_Node_Str"") public Store getStore(    @Named(""String_Node_Str"") DatasetFramework dsFramework,    CConfiguration cConf,    LocationFactory locationFactory,    TransactionExecutorFactory txExecutorFactory){
      return new DefaultStore(cConf,locationFactory,txExecutorFactory,dsFramework);
    }
    @Provides @Singleton @Named(""String_Node_Str"") public LogSaverTableUtil getLogSaverTableUtil(    @Named(""String_Node_Str"") DatasetFramework dsFramework,    CConfiguration cConf){
      return new LogSaverTableUtil(dsFramework,cConf);
    }
    @Provides @Singleton @Named(""String_Node_Str"") public FileMetaDataManager getFileMetaDataManager(    @Named(""String_Node_Str"") LogSaverTableUtil tableUtil,    @Named(""String_Node_Str"") DatasetFramework dsFramework,    TransactionExecutorFactory txExecutorFactory,    LocationFactory locationFactory){
      return new FileMetaDataManager(tableUtil,txExecutorFactory,locationFactory,dsFramework,cConf);
    }
  }
);
}","private Injector init() throws Exception {
  return Guice.createInjector(new ConfigModule(cConf,hConf),new LocationRuntimeModule().getDistributedModules(),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new TwillModule(),new ProgramRunnerRuntimeModule().getDistributedModules(),new SystemDatasetRuntimeModule().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      install(new DataFabricDistributedModule());
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).toInstance(createNoopScheduler());
      bind(DatasetTypeClassLoaderFactory.class).to(DistributedDatasetTypeClassLoaderFactory.class);
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(NamespaceAdmin.class).to(DefaultNamespaceAdmin.class);
      install(new FactoryModuleBuilder().implement(Store.class,DefaultStore.class).build(StoreFactory.class));
      bind(ConfigStore.class).to(DefaultConfigStore.class);
    }
    @Provides @Singleton public DatasetFramework getDSFramework(    CConfiguration cConf,    DatasetDefinitionRegistryFactory registryFactory,    @Named(""String_Node_Str"") Map<String,DatasetModule> defaultModules) throws IOException, DatasetManagementException {
      return createRegisteredDatasetFramework(cConf,registryFactory,defaultModules);
    }
    @Provides @Singleton @Named(""String_Node_Str"") public Store getStore(    DatasetFramework dsFramework,    CConfiguration cConf,    LocationFactory locationFactory,    TransactionExecutorFactory txExecutorFactory){
      return new DefaultStore(cConf,locationFactory,txExecutorFactory,dsFramework);
    }
    @Provides @Singleton @Named(""String_Node_Str"") public LogSaverTableUtil getLogSaverTableUtil(    DatasetFramework dsFramework,    CConfiguration cConf){
      return new LogSaverTableUtil(dsFramework,cConf);
    }
    @Provides @Singleton @Named(""String_Node_Str"") public FileMetaDataManager getFileMetaDataManager(    @Named(""String_Node_Str"") LogSaverTableUtil tableUtil,    DatasetFramework dsFramework,    TransactionExecutorFactory txExecutorFactory,    LocationFactory locationFactory){
      return new FileMetaDataManager(tableUtil,txExecutorFactory,locationFactory,dsFramework,cConf);
    }
  }
);
}"
7190,"@Provides @Singleton @Named(""String_Node_Str"") public DatasetFramework getDSFramework(CConfiguration cConf,DatasetDefinitionRegistryFactory registryFactory) throws IOException, DatasetManagementException {
  return createRegisteredDatasetFramework(cConf,registryFactory);
}","@Provides @Singleton public DatasetFramework getDSFramework(CConfiguration cConf,DatasetDefinitionRegistryFactory registryFactory,@Named(""String_Node_Str"") Map<String,DatasetModule> defaultModules) throws IOException, DatasetManagementException {
  return createRegisteredDatasetFramework(cConf,registryFactory,defaultModules);
}"
7191,"@Override protected void configure(){
  install(new DataFabricDistributedModule());
  bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Scopes.SINGLETON);
  bind(Scheduler.class).toInstance(createNoopScheduler());
  bind(DatasetFramework.class).to(RemoteDatasetFramework.class);
  bind(DatasetTypeClassLoaderFactory.class).to(DistributedDatasetTypeClassLoaderFactory.class);
  install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
  bind(NamespaceAdmin.class).to(DefaultNamespaceAdmin.class);
  install(new FactoryModuleBuilder().implement(Store.class,DefaultStore.class).build(StoreFactory.class));
  bind(ConfigStore.class).to(DefaultConfigStore.class);
}","@Override protected void configure(){
  install(new DataFabricDistributedModule());
  bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Scopes.SINGLETON);
  bind(Scheduler.class).toInstance(createNoopScheduler());
  bind(DatasetTypeClassLoaderFactory.class).to(DistributedDatasetTypeClassLoaderFactory.class);
  install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
  bind(NamespaceAdmin.class).to(DefaultNamespaceAdmin.class);
  install(new FactoryModuleBuilder().implement(Store.class,DefaultStore.class).build(StoreFactory.class));
  bind(ConfigStore.class).to(DefaultConfigStore.class);
}"
7192,"@Provides @Singleton @Named(""String_Node_Str"") public LogSaverTableUtil getLogSaverTableUtil(@Named(""String_Node_Str"") DatasetFramework dsFramework,CConfiguration cConf){
  return new LogSaverTableUtil(dsFramework,cConf);
}","@Provides @Singleton @Named(""String_Node_Str"") public LogSaverTableUtil getLogSaverTableUtil(DatasetFramework dsFramework,CConfiguration cConf){
  return new LogSaverTableUtil(dsFramework,cConf);
}"
7193,"/** 
 * Reads events from a stream
 * @param streamId ID of the stream
 * @param startTime Timestamp in milliseconds to start reading event from (inclusive)
 * @param endTime Timestamp in milliseconds for the last event to read (exclusive)
 * @param limit Maximum number of events to read
 * @param callback Callback to invoke for each stream event read. If the callback function returns {@code false}upon invocation, it will stops the reading
 * @throws IOException If fails to read from stream
 * @throws StreamNotFoundException If the given stream does not exists
 */
public void getEvents(String streamId,long startTime,long endTime,int limit,Function<? super StreamEvent,Boolean> callback) throws IOException, StreamNotFoundException, UnauthorizedException {
  URL url=config.resolveNamespacedURLV3(String.format(""String_Node_Str"",streamId,startTime,endTime,limit));
  HttpURLConnection urlConn=(HttpURLConnection)url.openConnection();
  AccessToken accessToken=config.getAccessToken();
  if (accessToken != null) {
    urlConn.setRequestProperty(HttpHeaders.AUTHORIZATION,accessToken.getTokenType() + ""String_Node_Str"" + accessToken.getValue());
  }
  if (urlConn instanceof HttpsURLConnection && !config.isVerifySSLCert()) {
    try {
      HttpRequests.disableCertCheck((HttpsURLConnection)urlConn);
    }
 catch (    Exception e) {
    }
  }
  try {
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED) {
      throw new UnauthorizedException(""String_Node_Str"");
    }
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
      throw new StreamNotFoundException(streamId);
    }
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_NO_CONTENT) {
      return;
    }
    JsonReader jsonReader=new JsonReader(new InputStreamReader(urlConn.getInputStream(),Charsets.UTF_8));
    jsonReader.beginArray();
    while (jsonReader.peek() != JsonToken.END_ARRAY) {
      Boolean result=callback.apply(GSON.<StreamEvent>fromJson(jsonReader,StreamEvent.class));
      if (result == null || !result) {
        break;
      }
    }
  }
  finally {
    urlConn.disconnect();
  }
}","/** 
 * Reads events from a stream
 * @param streamId ID of the stream
 * @param startTime Timestamp in milliseconds to start reading event from (inclusive)
 * @param endTime Timestamp in milliseconds for the last event to read (exclusive)
 * @param limit Maximum number of events to read
 * @param callback Callback to invoke for each stream event read. If the callback function returns {@code false}upon invocation, it will stops the reading
 * @throws IOException If fails to read from stream
 * @throws StreamNotFoundException If the given stream does not exists
 */
public void getEvents(String streamId,long startTime,long endTime,int limit,Function<? super StreamEvent,Boolean> callback) throws IOException, StreamNotFoundException, UnauthorizedException {
  URL url=config.resolveNamespacedURLV3(String.format(""String_Node_Str"",streamId,startTime,endTime,limit));
  HttpURLConnection urlConn=(HttpURLConnection)url.openConnection();
  AccessToken accessToken=config.getAccessToken();
  if (accessToken != null) {
    urlConn.setRequestProperty(HttpHeaders.AUTHORIZATION,accessToken.getTokenType() + ""String_Node_Str"" + accessToken.getValue());
  }
  if (urlConn instanceof HttpsURLConnection && !config.isVerifySSLCert()) {
    try {
      HttpRequests.disableCertCheck((HttpsURLConnection)urlConn);
    }
 catch (    Exception e) {
    }
  }
  try {
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_UNAUTHORIZED) {
      throw new UnauthorizedException(""String_Node_Str"");
    }
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
      throw new StreamNotFoundException(streamId);
    }
    if (urlConn.getResponseCode() == HttpURLConnection.HTTP_NO_CONTENT) {
      return;
    }
    InputStream inputStream=urlConn.getInputStream();
    JsonReader jsonReader=new JsonReader(new InputStreamReader(inputStream,Charsets.UTF_8));
    jsonReader.beginArray();
    while (jsonReader.peek() != JsonToken.END_ARRAY) {
      Boolean result=callback.apply(GSON.<StreamEvent>fromJson(jsonReader,StreamEvent.class));
      if (result == null || !result) {
        break;
      }
    }
    drain(inputStream);
  }
  finally {
    urlConn.disconnect();
  }
}"
7194,"@Override public void dropAllInNamespace(Id.Namespace namespace) throws Exception {
  List<Location> locations;
  try {
    locations=getStreamsHomeLocation(namespace).list();
  }
 catch (  FileNotFoundException e) {
    locations=ImmutableList.of();
  }
  for (  final Location streamLocation : locations) {
    doTruncate(streamLocation);
  }
  stateStoreFactory.dropAllInNamespace(namespace);
}","@Override public void dropAllInNamespace(Id.Namespace namespace) throws Exception {
  List<Location> locations;
  try {
    locations=getStreamsHomeLocation(namespace).list();
  }
 catch (  FileNotFoundException e) {
    locations=ImmutableList.of();
  }
  for (  final Location streamLocation : locations) {
    doTruncate(streamLocation);
    alterExploreStream(StreamUtils.getStreamIdFromLocation(streamLocation),false);
  }
  stateStoreFactory.dropAllInNamespace(namespace);
}"
7195,"/** 
 * Parse the arguments from the command line and execute the different modes.
 * @param args command line arguments
 * @param conf default configuration
 * @return true if the arguments were parsed successfully and comply with the expected usage
 */
private boolean parseArgsAndExecMode(String[] args,Configuration conf){
  CommandLineParser parser=new GnuParser();
  try {
    CommandLine line=parser.parse(options,args);
    if (line.hasOption(HELP_OPTION)) {
      printUsage(false);
      return true;
    }
    hostname=line.getOptionValue(HOST_OPTION);
    existingFilename=line.getOptionValue(FILENAME_OPTION);
    persistingFilename=line.hasOption(SAVE_OPTION) ? line.getOptionValue(SAVE_OPTION) : null;
    showTxids=line.hasOption(IDS_OPTION);
    txId=line.hasOption(TRANSACTION_OPTION) ? Long.valueOf(line.getOptionValue(TRANSACTION_OPTION)) : null;
    accessToken=line.hasOption(TOKEN_OPTION) ? line.getOptionValue(TOKEN_OPTION).replaceAll(""String_Node_Str"",""String_Node_Str"") : null;
    tokenFile=line.hasOption(TOKEN_FILE_OPTION) ? line.getOptionValue(TOKEN_FILE_OPTION).replaceAll(""String_Node_Str"",""String_Node_Str"") : null;
    portNumber=line.hasOption(PORT_OPTION) ? Integer.valueOf(line.getOptionValue(PORT_OPTION)) : conf.getInt(Constants.Router.ROUTER_PORT,Integer.getInteger(Constants.Router.DEFAULT_ROUTER_PORT));
    if (tokenFile != null) {
      if (accessToken != null) {
        tokenFile=null;
      }
 else {
        readTokenFile();
      }
    }
switch (this.mode) {
case VIEW:
      if (!line.hasOption(HOST_OPTION) && !line.hasOption(FILENAME_OPTION)) {
        usage(""String_Node_Str"" + ""String_Node_Str"");
        return false;
      }
    executeViewMode();
  break;
case INVALIDATE:
if (!line.hasOption(HOST_OPTION) || !line.hasOption(TRANSACTION_OPTION)) {
  usage(""String_Node_Str"");
  return false;
}
executeInvalidateMode();
break;
case RESET:
if (!line.hasOption(HOST_OPTION)) {
usage(""String_Node_Str"");
return false;
}
executeResetMode();
break;
default :
printUsage(true);
return false;
}
}
 catch (ParseException e) {
printUsage(true);
return false;
}
return true;
}","/** 
 * Parse the arguments from the command line and execute the different modes.
 * @param args command line arguments
 * @param conf default configuration
 * @return true if the arguments were parsed successfully and comply with the expected usage
 */
private boolean parseArgsAndExecMode(String[] args,Configuration conf){
  CommandLineParser parser=new GnuParser();
  try {
    CommandLine line=parser.parse(options,args);
    if (line.hasOption(HELP_OPTION)) {
      printUsage(false);
      return true;
    }
    hostname=line.getOptionValue(HOST_OPTION);
    existingFilename=line.getOptionValue(FILENAME_OPTION);
    persistingFilename=line.hasOption(SAVE_OPTION) ? line.getOptionValue(SAVE_OPTION) : null;
    showTxids=line.hasOption(IDS_OPTION);
    txId=line.hasOption(TRANSACTION_OPTION) ? Long.valueOf(line.getOptionValue(TRANSACTION_OPTION)) : null;
    accessToken=line.hasOption(TOKEN_OPTION) ? line.getOptionValue(TOKEN_OPTION).replaceAll(""String_Node_Str"",""String_Node_Str"") : null;
    tokenFile=line.hasOption(TOKEN_FILE_OPTION) ? line.getOptionValue(TOKEN_FILE_OPTION).replaceAll(""String_Node_Str"",""String_Node_Str"") : null;
    portNumber=line.hasOption(PORT_OPTION) ? Integer.valueOf(line.getOptionValue(PORT_OPTION)) : conf.getInt(Constants.Router.ROUTER_PORT,Integer.parseInt(Constants.Router.DEFAULT_ROUTER_PORT));
    if (tokenFile != null) {
      if (accessToken != null) {
        tokenFile=null;
      }
 else {
        readTokenFile();
      }
    }
switch (this.mode) {
case VIEW:
      if (!line.hasOption(HOST_OPTION) && !line.hasOption(FILENAME_OPTION)) {
        usage(""String_Node_Str"" + ""String_Node_Str"");
        return false;
      }
    executeViewMode();
  break;
case INVALIDATE:
if (!line.hasOption(HOST_OPTION) || !line.hasOption(TRANSACTION_OPTION)) {
  usage(""String_Node_Str"");
  return false;
}
executeInvalidateMode();
break;
case RESET:
if (!line.hasOption(HOST_OPTION)) {
usage(""String_Node_Str"");
return false;
}
executeResetMode();
break;
default :
printUsage(true);
return false;
}
}
 catch (ParseException e) {
printUsage(true);
return false;
}
return true;
}"
7196,"/** 
 * Parse the arguments from the command line and execute the different modes.
 * @param args command line arguments
 * @param conf default configuration
 * @return true if the arguments were parsed successfully and comply with the expected usage
 */
private boolean parseArgsAndExecMode(String[] args,Configuration conf){
  CommandLineParser parser=new GnuParser();
  try {
    CommandLine line=parser.parse(options,args);
    if (line.hasOption(HELP_OPTION)) {
      printUsage(false);
      return true;
    }
    hostname=line.getOptionValue(HOST_OPTION);
    existingFilename=line.getOptionValue(FILENAME_OPTION);
    persistingFilename=line.hasOption(SAVE_OPTION) ? line.getOptionValue(SAVE_OPTION) : null;
    showTxids=line.hasOption(IDS_OPTION);
    txId=line.hasOption(TRANSACTION_OPTION) ? Long.valueOf(line.getOptionValue(TRANSACTION_OPTION)) : null;
    accessToken=line.hasOption(TOKEN_OPTION) ? line.getOptionValue(TOKEN_OPTION).replaceAll(""String_Node_Str"",""String_Node_Str"") : null;
    tokenFile=line.hasOption(TOKEN_FILE_OPTION) ? line.getOptionValue(TOKEN_FILE_OPTION).replaceAll(""String_Node_Str"",""String_Node_Str"") : null;
    portNumber=line.hasOption(PORT_OPTION) ? Integer.valueOf(line.getOptionValue(PORT_OPTION)) : conf.getInt(Constants.Router.ROUTER_PORT,Integer.getInteger(Constants.Router.DEFAULT_ROUTER_PORT));
    if (tokenFile != null) {
      if (accessToken != null) {
        tokenFile=null;
      }
 else {
        readTokenFile();
      }
    }
switch (this.mode) {
case VIEW:
      if (!line.hasOption(HOST_OPTION) && !line.hasOption(FILENAME_OPTION)) {
        usage(""String_Node_Str"" + ""String_Node_Str"");
        return false;
      }
    executeViewMode();
  break;
case INVALIDATE:
if (!line.hasOption(HOST_OPTION) || !line.hasOption(TRANSACTION_OPTION)) {
  usage(""String_Node_Str"");
  return false;
}
executeInvalidateMode();
break;
case RESET:
if (!line.hasOption(HOST_OPTION)) {
usage(""String_Node_Str"");
return false;
}
executeResetMode();
break;
default :
printUsage(true);
return false;
}
}
 catch (ParseException e) {
printUsage(true);
return false;
}
return true;
}","/** 
 * Parse the arguments from the command line and execute the different modes.
 * @param args command line arguments
 * @param conf default configuration
 * @return true if the arguments were parsed successfully and comply with the expected usage
 */
private boolean parseArgsAndExecMode(String[] args,Configuration conf){
  CommandLineParser parser=new GnuParser();
  try {
    CommandLine line=parser.parse(options,args);
    if (line.hasOption(HELP_OPTION)) {
      printUsage(false);
      return true;
    }
    hostname=line.getOptionValue(HOST_OPTION);
    existingFilename=line.getOptionValue(FILENAME_OPTION);
    persistingFilename=line.hasOption(SAVE_OPTION) ? line.getOptionValue(SAVE_OPTION) : null;
    showTxids=line.hasOption(IDS_OPTION);
    txId=line.hasOption(TRANSACTION_OPTION) ? Long.valueOf(line.getOptionValue(TRANSACTION_OPTION)) : null;
    accessToken=line.hasOption(TOKEN_OPTION) ? line.getOptionValue(TOKEN_OPTION).replaceAll(""String_Node_Str"",""String_Node_Str"") : null;
    tokenFile=line.hasOption(TOKEN_FILE_OPTION) ? line.getOptionValue(TOKEN_FILE_OPTION).replaceAll(""String_Node_Str"",""String_Node_Str"") : null;
    portNumber=line.hasOption(PORT_OPTION) ? Integer.valueOf(line.getOptionValue(PORT_OPTION)) : conf.getInt(Constants.Router.ROUTER_PORT,Integer.parseInt(Constants.Router.DEFAULT_ROUTER_PORT));
    if (tokenFile != null) {
      if (accessToken != null) {
        tokenFile=null;
      }
 else {
        readTokenFile();
      }
    }
switch (this.mode) {
case VIEW:
      if (!line.hasOption(HOST_OPTION) && !line.hasOption(FILENAME_OPTION)) {
        usage(""String_Node_Str"" + ""String_Node_Str"");
        return false;
      }
    executeViewMode();
  break;
case INVALIDATE:
if (!line.hasOption(HOST_OPTION) || !line.hasOption(TRANSACTION_OPTION)) {
  usage(""String_Node_Str"");
  return false;
}
executeInvalidateMode();
break;
case RESET:
if (!line.hasOption(HOST_OPTION)) {
usage(""String_Node_Str"");
return false;
}
executeResetMode();
break;
default :
printUsage(true);
return false;
}
}
 catch (ParseException e) {
printUsage(true);
return false;
}
return true;
}"
7197,"@Path(""String_Node_Str"") @GET public void configHBase(@SuppressWarnings(""String_Node_Str"") HttpRequest request,HttpResponder responder,@DefaultValue(""String_Node_Str"") @QueryParam(""String_Node_Str"") String format){
  if (""String_Node_Str"".equals(format)) {
    responder.sendJson(HttpResponseStatus.OK,toMap(cConf));
  }
 else   if (""String_Node_Str"".equals(format)) {
    try {
      StringWriter stringWriter=new StringWriter();
      hConf.writeXml(stringWriter);
      responder.sendContent(HttpResponseStatus.OK,stringWriter2ChannelBuffer(stringWriter),""String_Node_Str"",null);
    }
 catch (    IOException e) {
      LOG.info(""String_Node_Str"",e);
      responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
    }
  }
 else {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"" + format + ""String_Node_Str"");
  }
}","@Path(""String_Node_Str"") @GET public void configHBase(@SuppressWarnings(""String_Node_Str"") HttpRequest request,HttpResponder responder,@DefaultValue(""String_Node_Str"") @QueryParam(""String_Node_Str"") String format){
  if (""String_Node_Str"".equals(format)) {
    responder.sendJson(HttpResponseStatus.OK,toMap(hConf));
  }
 else   if (""String_Node_Str"".equals(format)) {
    try {
      StringWriter stringWriter=new StringWriter();
      hConf.writeXml(stringWriter);
      responder.sendContent(HttpResponseStatus.OK,stringWriter2ChannelBuffer(stringWriter),""String_Node_Str"",null);
    }
 catch (    IOException e) {
      LOG.info(""String_Node_Str"",e);
      responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
    }
  }
 else {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"" + format + ""String_Node_Str"");
  }
}"
7198,"/** 
 * Writes the   {@link AppMetadataStore#TYPE_RUN_RECORD_STARTED} entry in the app meta table so that{@link AppMetadataStore#TYPE_RUN_RECORD_COMPLETED} can be written which deleted the started record.
 * @param appId the application id
 * @param programType {@link ProgramType} of this program
 * @param programId the program id of this program
 * @param pId the process id of the run
 * @param startTs the startTs
 */
private void writeTempRunRecordStart(String appId,ProgramType programType,String programId,String pId,long startTs){
  store.setStart(Id.Program.from(Id.Application.from(Constants.DEFAULT_NAMESPACE,appId),programType,programId),pId,Long.MAX_VALUE - startTs);
}","/** 
 * Writes the   {@link AppMetadataStore#TYPE_RUN_RECORD_STARTED} entry in the app meta table so that{@link AppMetadataStore#TYPE_RUN_RECORD_COMPLETED} can be written which deleted the started record.
 * @param appId the application id
 * @param programType {@link ProgramType} of this program
 * @param programId the program id of this program
 * @param pId the process id of the run
 * @param startTs the startTs
 */
private void writeTempRunRecordStart(String appId,ProgramType programType,String programId,String pId,long startTs){
  store.setStart(Id.Program.from(Id.Application.from(Constants.DEFAULT_NAMESPACE,appId),programType,programId),pId,startTs);
}"
7199,"private void upgradeUserTables() throws Exception {
  HBaseAdmin hAdmin=new HBaseAdmin(hConf);
  for (  HTableDescriptor desc : hAdmin.listTables(USER_TABLE_PREFIX)) {
    HTableNameConverter hTableNameConverter=new HTableNameConverterFactory().get();
    TableId tableId=hTableNameConverter.from(desc);
    LOG.info(""String_Node_Str"",tableId,desc);
    final boolean supportsIncrement=HBaseTableAdmin.supportsReadlessIncrements(desc);
    final boolean transactional=HBaseTableAdmin.isTransactional(desc);
    DatasetAdmin admin=new AbstractHBaseDataSetAdmin(tableId,hConf,hBaseTableUtil){
      @Override protected CoprocessorJar createCoprocessorJar() throws IOException {
        return HBaseTableAdmin.createCoprocessorJarInternal(cConf,locationFactory,hBaseTableUtil,transactional,supportsIncrement);
      }
      @Override protected boolean upgradeTable(      HTableDescriptor tableDescriptor){
        return false;
      }
      @Override public void create() throws IOException {
        throw new UnsupportedOperationException(""String_Node_Str"");
      }
    }
;
    admin.upgrade();
    LOG.info(""String_Node_Str"",tableId);
  }
}","private void upgradeUserTables() throws Exception {
  HBaseAdmin hAdmin=new HBaseAdmin(hConf);
  for (  HTableDescriptor desc : hAdmin.listTables(userTablePrefix)) {
    HTableNameConverter hTableNameConverter=new HTableNameConverterFactory().get();
    TableId tableId=hTableNameConverter.from(desc);
    LOG.info(""String_Node_Str"",tableId,desc);
    final boolean supportsIncrement=HBaseTableAdmin.supportsReadlessIncrements(desc);
    final boolean transactional=HBaseTableAdmin.isTransactional(desc);
    DatasetAdmin admin=new AbstractHBaseDataSetAdmin(tableId,hConf,hBaseTableUtil){
      @Override protected CoprocessorJar createCoprocessorJar() throws IOException {
        return HBaseTableAdmin.createCoprocessorJarInternal(cConf,locationFactory,hBaseTableUtil,transactional,supportsIncrement);
      }
      @Override protected boolean upgradeTable(      HTableDescriptor tableDescriptor){
        return false;
      }
      @Override public void create() throws IOException {
        throw new UnsupportedOperationException(""String_Node_Str"");
      }
    }
;
    admin.upgrade();
    LOG.info(""String_Node_Str"",tableId);
  }
}"
7200,"@Inject private DatasetUpgrader(CConfiguration cConf,Configuration hConf,LocationFactory locationFactory,QueueAdmin queueAdmin,HBaseTableUtil hBaseTableUtil,@Named(""String_Node_Str"") final DatasetFramework dsFramework,DatasetInstanceMDSUpgrader datasetInstanceMDSUpgrader,DatasetTypeMDSUpgrader datasetTypeMDSUpgrader){
  super(locationFactory);
  this.cConf=cConf;
  this.hConf=hConf;
  this.locationFactory=locationFactory;
  this.queueAdmin=queueAdmin;
  this.hBaseTableUtil=hBaseTableUtil;
  this.dsFramework=dsFramework;
  this.datasetInstanceMDSUpgrader=datasetInstanceMDSUpgrader;
  this.datasetTypeMDSUpgrader=datasetTypeMDSUpgrader;
}","@Inject private DatasetUpgrader(CConfiguration cConf,Configuration hConf,LocationFactory locationFactory,QueueAdmin queueAdmin,HBaseTableUtil hBaseTableUtil,@Named(""String_Node_Str"") final DatasetFramework dsFramework,DatasetInstanceMDSUpgrader datasetInstanceMDSUpgrader,DatasetTypeMDSUpgrader datasetTypeMDSUpgrader){
  super(locationFactory);
  this.cConf=cConf;
  this.hConf=hConf;
  this.locationFactory=locationFactory;
  this.queueAdmin=queueAdmin;
  this.hBaseTableUtil=hBaseTableUtil;
  this.dsFramework=dsFramework;
  this.datasetInstanceMDSUpgrader=datasetInstanceMDSUpgrader;
  this.datasetTypeMDSUpgrader=datasetTypeMDSUpgrader;
  this.userTablePrefix=Pattern.compile(String.format(""String_Node_Str"",cConf.get(Constants.Dataset.TABLE_PREFIX)));
}"
7201,"@Override public QueryHandle execute(Id.Namespace namespace,String statement) throws ExploreException, SQLException {
  startAndWait();
  try {
    Map<String,String> sessionConf=startSession(namespace);
    SessionHandle sessionHandle=openSession(sessionConf);
    try {
      String database=getHiveDatabase(namespace.getId());
      SessionState.get().setCurrentDatabase(database);
      OperationHandle operationHandle=doExecute(sessionHandle,statement);
      QueryHandle handle=saveOperationInfo(operationHandle,sessionHandle,sessionConf,statement,database);
      LOG.trace(""String_Node_Str"",statement,handle);
      return handle;
    }
 catch (    Throwable e) {
      closeSession(sessionHandle);
      throw e;
    }
  }
 catch (  HiveSQLException e) {
    throw getSqlException(e);
  }
catch (  Throwable e) {
    throw new ExploreException(e);
  }
}","@Override public QueryHandle execute(Id.Namespace namespace,String statement) throws ExploreException, SQLException {
  startAndWait();
  try {
    Map<String,String> sessionConf=startSession(namespace);
    SessionHandle sessionHandle=openSession(sessionConf);
    try {
      String database=getHiveDatabase(namespace.getId());
      setCurrentDatabase(database);
      OperationHandle operationHandle=doExecute(sessionHandle,statement);
      QueryHandle handle=saveOperationInfo(operationHandle,sessionHandle,sessionConf,statement,database);
      LOG.trace(""String_Node_Str"",statement,handle);
      return handle;
    }
 catch (    Throwable e) {
      closeSession(sessionHandle);
      throw e;
    }
  }
 catch (  HiveSQLException e) {
    throw getSqlException(e);
  }
catch (  Throwable e) {
    throw new ExploreException(e);
  }
}"
7202,"@Test public void testStream() throws Exception {
  String streamId=PREFIX + ""String_Node_Str"";
  testCommandOutputContains(cli,""String_Node_Str"" + streamId,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",streamId);
  testCommandOutputNotContains(cli,""String_Node_Str"" + streamId,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId,""String_Node_Str"");
  testCommandOutputNotContains(cli,""String_Node_Str"" + streamId,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId,""String_Node_Str"");
  File file=new File(TMP_FOLDER.newFolder(),""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str""+ file.getAbsolutePath(),""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str""+ file.getParentFile().getAbsolutePath(),""String_Node_Str"");
  BufferedWriter writer=Files.newWriter(file,Charsets.UTF_8);
  try {
    for (int i=0; i < 10; i++) {
      writer.write(""String_Node_Str"" + i);
      writer.newLine();
    }
  }
  finally {
    writer.close();
  }
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str""+ file.getAbsolutePath(),""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId,""String_Node_Str"");
}","@Test public void testStream() throws Exception {
  String streamId=PREFIX + ""String_Node_Str"";
  testCommandOutputContains(cli,""String_Node_Str"" + streamId,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",streamId);
  testCommandOutputNotContains(cli,""String_Node_Str"" + streamId,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId,""String_Node_Str"");
  testCommandOutputNotContains(cli,""String_Node_Str"" + streamId,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId,""String_Node_Str"");
  File file=new File(TMP_FOLDER.newFolder(),""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str""+ file.getAbsolutePath(),""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str""+ file.getParentFile().getAbsolutePath(),""String_Node_Str"");
  BufferedWriter writer=Files.newWriter(file,Charsets.UTF_8);
  try {
    for (int i=0; i < 10; i++) {
      writer.write(String.format(""String_Node_Str"",i,i));
      writer.newLine();
    }
  }
  finally {
    writer.close();
  }
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str""+ file.getAbsolutePath(),""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId,String.format(""String_Node_Str"",streamId));
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str"",String.format(""String_Node_Str"",streamId));
  testCommandOutputContains(cli,""String_Node_Str"",String.format(""String_Node_Str"",streamId));
  testCommandOutputContains(cli,""String_Node_Str"" + streamId,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + streamId + ""String_Node_Str"",""String_Node_Str"");
}"
7203,"private String getHiveTableName(String streamId){
  return String.format(""String_Node_Str"",Constants.DEFAULT_NAMESPACE,streamId);
}","private String getHiveTableName(String streamId){
  return String.format(""String_Node_Str"",streamId);
}"
7204,"/** 
 * Upgrades the   {@link DatasetModuleMeta} for namespaceThe system modules are written as it is and the user module meta is written with new jarLocation which is under namespace
 * @param olddatasetModuleMeta the old {@link DatasetModuleMeta}
 * @param newDatasetTypeMDS the new {@link DatasetTypeMDS} where the new moduleMeta will be written
 * @throws IOException
 */
private void upgradeDatasetModuleMeta(DatasetModuleMeta olddatasetModuleMeta,DatasetTypeMDS newDatasetTypeMDS) throws IOException {
  DatasetModuleMeta newDatasetModuleMeta;
  LOG.info(""String_Node_Str"",olddatasetModuleMeta.getName());
  if (olddatasetModuleMeta.getJarLocation() == null) {
    newDatasetModuleMeta=olddatasetModuleMeta;
  }
 else {
    Location oldJarLocation=locationFactory.create(olddatasetModuleMeta.getJarLocation());
    Location newJarLocation=updateUserDatasetModuleJarLocation(oldJarLocation,olddatasetModuleMeta.getClassName(),Constants.DEFAULT_NAMESPACE);
    newDatasetModuleMeta=new DatasetModuleMeta(olddatasetModuleMeta.getName(),olddatasetModuleMeta.getClassName(),newJarLocation.toURI(),olddatasetModuleMeta.getTypes(),olddatasetModuleMeta.getUsesModules());
    Collection<String> usedByModules=olddatasetModuleMeta.getUsedByModules();
    for (    String moduleName : usedByModules) {
      newDatasetModuleMeta.addUsedByModule(moduleName);
    }
    newDatasetModuleMeta=olddatasetModuleMeta;
    renameLocation(oldJarLocation,newJarLocation);
  }
  newDatasetTypeMDS.writeModule(Constants.DEFAULT_NAMESPACE_ID,newDatasetModuleMeta);
}","/** 
 * Upgrades the   {@link DatasetModuleMeta} for namespaceThe system modules are written as it is and the user module meta is written with new jarLocation which is under namespace
 * @param olddatasetModuleMeta the old {@link DatasetModuleMeta}
 * @param newDatasetTypeMDS the new {@link DatasetTypeMDS} where the new moduleMeta will be written
 * @throws IOException
 */
private void upgradeDatasetModuleMeta(DatasetModuleMeta olddatasetModuleMeta,DatasetTypeMDS newDatasetTypeMDS) throws IOException {
  DatasetModuleMeta newDatasetModuleMeta;
  LOG.info(""String_Node_Str"",olddatasetModuleMeta.getName());
  if (olddatasetModuleMeta.getJarLocation() == null) {
    newDatasetModuleMeta=olddatasetModuleMeta;
  }
 else {
    Location oldJarLocation=locationFactory.create(olddatasetModuleMeta.getJarLocation());
    Location newJarLocation=updateUserDatasetModuleJarLocation(oldJarLocation,olddatasetModuleMeta.getClassName(),Constants.DEFAULT_NAMESPACE);
    newDatasetModuleMeta=new DatasetModuleMeta(olddatasetModuleMeta.getName(),olddatasetModuleMeta.getClassName(),newJarLocation.toURI(),olddatasetModuleMeta.getTypes(),olddatasetModuleMeta.getUsesModules());
    Collection<String> usedByModules=olddatasetModuleMeta.getUsedByModules();
    for (    String moduleName : usedByModules) {
      newDatasetModuleMeta.addUsedByModule(moduleName);
    }
    renameLocation(oldJarLocation,newJarLocation);
  }
  newDatasetTypeMDS.writeModule(Constants.DEFAULT_NAMESPACE_ID,newDatasetModuleMeta);
}"
7205,"@Override protected void configure(){
  bind(AbstractNamespaceClient.class).to(DiscoveryNamespaceClient.class);
}","@Override protected void configure(){
  bind(Scheduler.class).annotatedWith(Assisted.class).toInstance(createNoopScheduler());
}"
7206,"@BeforeClass public static void before() throws Exception {
  lf=new LocalLocationFactory();
  temp=TMP_FOLDER.newFolder(""String_Node_Str"");
}","@BeforeClass public static void before() throws Exception {
  lf=new LocalLocationFactory();
  temp=TMP_FOLDER.newFolder(""String_Node_Str"");
  NamespaceAdmin namespaceAdmin=AppFabricTestHelper.getInjector().getInstance(NamespaceAdmin.class);
  namespaceAdmin.createNamespace(Constants.DEFAULT_NAMESPACE_META);
}"
7207,"private void verifyReservedCreate() throws AlreadyExistsException, IOException, UnauthorizedException {
  NamespaceMeta.Builder builder=new NamespaceMeta.Builder();
  builder.setName(Constants.DEFAULT_NAMESPACE_ID);
  try {
    namespaceClient.create(builder.build());
    Assert.fail(String.format(""String_Node_Str"",Constants.DEFAULT_NAMESPACE_ID));
  }
 catch (  BadRequestException e) {
  }
  builder.setName(Constants.SYSTEM_NAMESPACE_ID);
  try {
    namespaceClient.create(builder.build());
    Assert.fail(String.format(""String_Node_Str"",Constants.SYSTEM_NAMESPACE_ID));
  }
 catch (  BadRequestException e) {
  }
}","private void verifyReservedCreate() throws Exception {
  NamespaceMeta.Builder builder=new NamespaceMeta.Builder();
  builder.setName(Constants.DEFAULT_NAMESPACE_ID);
  try {
    namespaceClient.create(builder.build());
    Assert.fail(String.format(""String_Node_Str"",Constants.DEFAULT_NAMESPACE_ID));
  }
 catch (  BadRequestException e) {
  }
  builder.setName(Constants.SYSTEM_NAMESPACE_ID);
  try {
    namespaceClient.create(builder.build());
    Assert.fail(String.format(""String_Node_Str"",Constants.SYSTEM_NAMESPACE_ID));
  }
 catch (  BadRequestException e) {
  }
}"
7208,"public void create(NamespaceMeta namespaceMeta) throws AlreadyExistsException, BadRequestException, IOException, UnauthorizedException {
  URL url=resolve(String.format(""String_Node_Str"",namespaceMeta.getName()));
  HttpResponse response=execute(HttpRequest.put(url).withBody(new Gson().toJson(namespaceMeta)).build());
  String responseBody=response.getResponseBodyAsString();
  if (response.getResponseCode() == HttpURLConnection.HTTP_OK) {
    if (responseBody != null && responseBody.equals(String.format(""String_Node_Str"",namespaceMeta.getName()))) {
      throw new AlreadyExistsException(NAMESPACE_ENTITY_TYPE,namespaceMeta.getName());
    }
    return;
  }
  if (response.getResponseCode() == HttpURLConnection.HTTP_BAD_REQUEST) {
    throw new BadRequestException(""String_Node_Str"" + responseBody);
  }
  throw new IOException(""String_Node_Str"" + ""String_Node_Str"");
}","public void create(NamespaceMeta namespaceMeta) throws Exception {
  URL url=resolve(String.format(""String_Node_Str"",namespaceMeta.getName()));
  HttpResponse response=execute(HttpRequest.put(url).withBody(new Gson().toJson(namespaceMeta)).build());
  String responseBody=response.getResponseBodyAsString();
  if (response.getResponseCode() == HttpURLConnection.HTTP_OK) {
    if (responseBody != null && responseBody.equals(String.format(""String_Node_Str"",namespaceMeta.getName()))) {
      throw new AlreadyExistsException(NAMESPACE_ENTITY_TYPE,namespaceMeta.getName());
    }
    return;
  }
  if (response.getResponseCode() == HttpURLConnection.HTTP_BAD_REQUEST) {
    throw new BadRequestException(""String_Node_Str"" + responseBody);
  }
  throw new IOException(""String_Node_Str"" + ""String_Node_Str"");
}"
7209,"@BeforeClass public static void init() throws IOException {
  zkServer=InMemoryZKServer.builder().setDataDir(tmpFolder.newFolder()).build();
  zkServer.startAndWait();
  Configuration hConf=new Configuration();
  hConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR,tmpFolder.newFolder().getAbsolutePath());
  dfsCluster=new MiniDFSCluster.Builder(hConf).numDataNodes(1).build();
  final FileSystem fileSystem=dfsCluster.getFileSystem();
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new TransactionMetricsModule(),new NotificationFeedServiceRuntimeModule().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(LocationFactory.class).toInstance(new HDFSLocationFactory(fileSystem));
    }
  }
,Modules.override(new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
    }
  }
));
  zkClient=injector.getInstance(ZKClientService.class);
  zkClient.startAndWait();
  streamAdmin=injector.getInstance(StreamAdmin.class);
  coordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  coordinatorClient.startAndWait();
}","@BeforeClass public static void init() throws IOException {
  zkServer=InMemoryZKServer.builder().setDataDir(tmpFolder.newFolder()).build();
  zkServer.startAndWait();
  Configuration hConf=new Configuration();
  hConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR,tmpFolder.newFolder().getAbsolutePath());
  dfsCluster=new MiniDFSCluster.Builder(hConf).numDataNodes(1).build();
  final FileSystem fileSystem=dfsCluster.getFileSystem();
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new TransactionMetricsModule(),new NotificationFeedServiceRuntimeModule().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(LocationFactory.class).toInstance(new HDFSLocationFactory(fileSystem));
    }
  }
,Modules.override(new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
    }
  }
));
  zkClient=injector.getInstance(ZKClientService.class);
  zkClient.startAndWait();
  setupNamespaces(injector.getInstance(LocationFactory.class));
  streamAdmin=injector.getInstance(StreamAdmin.class);
  coordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  coordinatorClient.startAndWait();
}"
7210,"@BeforeClass public static void init() throws IOException {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new DiscoveryRuntimeModule().getInMemoryModules(),new DataFabricModules().getInMemoryModules(),new LocationRuntimeModule().getInMemoryModules(),new TransactionMetricsModule(),new NotificationFeedServiceRuntimeModule().getInMemoryModules(),Modules.override(new StreamAdminModules().getInMemoryModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
    }
  }
));
  streamAdmin=injector.getInstance(StreamAdmin.class);
  coordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  coordinatorClient.startAndWait();
}","@BeforeClass public static void init() throws IOException {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new DiscoveryRuntimeModule().getInMemoryModules(),new DataFabricModules().getInMemoryModules(),new LocationRuntimeModule().getInMemoryModules(),new TransactionMetricsModule(),new NotificationFeedServiceRuntimeModule().getInMemoryModules(),Modules.override(new StreamAdminModules().getInMemoryModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
    }
  }
));
  setupNamespaces(injector.getInstance(LocationFactory.class));
  streamAdmin=injector.getInstance(StreamAdmin.class);
  coordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  coordinatorClient.startAndWait();
}"
7211,"@Test public void streamPublishesHeartbeatTest() throws Exception {
  final int entries=10;
  final String streamName=""String_Node_Str"";
  final Id.Stream streamId=Id.Stream.from(Constants.DEFAULT_NAMESPACE,streamName);
  HttpURLConnection urlConn=openURL(String.format(""String_Node_Str"",hostname,port,streamName),HttpMethod.PUT);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),urlConn.getResponseCode());
  urlConn.disconnect();
  for (int i=0; i < entries; ++i) {
    urlConn=openURL(String.format(""String_Node_Str"",hostname,port,streamName),HttpMethod.POST);
    urlConn.setDoOutput(true);
    urlConn.addRequestProperty(""String_Node_Str"",Integer.toString(i));
    urlConn.getOutputStream().write(TWO_BYTES);
    Assert.assertEquals(HttpResponseStatus.OK.getCode(),urlConn.getResponseCode());
    urlConn.disconnect();
  }
  TimeUnit.SECONDS.sleep(Constants.Stream.HEARTBEAT_INTERVAL + 1);
  StreamWriterHeartbeat heartbeat=heartbeatPublisher.getHeartbeat();
  Assert.assertNotNull(heartbeat);
  Assert.assertEquals(1,heartbeat.getStreamsSizes().size());
  Long streamSize=heartbeat.getStreamsSizes().get(streamId);
  Assert.assertNotNull(streamSize);
  Assert.assertEquals(entries * TWO_BYTES.length,(long)streamSize);
}","@Test public void streamPublishesHeartbeatTest() throws Exception {
  final int entries=10;
  final String streamName=""String_Node_Str"";
  final Id.Stream streamId=Id.Stream.from(Constants.DEFAULT_NAMESPACE,streamName);
  streamAdmin.create(streamId);
  streamMetaStore.addStream(streamId);
  for (int i=0; i < entries; ++i) {
    HttpURLConnection urlConn=openURL(String.format(""String_Node_Str"",hostname,port,streamName),HttpMethod.POST);
    urlConn.setDoOutput(true);
    urlConn.addRequestProperty(""String_Node_Str"",Integer.toString(i));
    urlConn.getOutputStream().write(TWO_BYTES);
    Assert.assertEquals(HttpResponseStatus.OK.getCode(),urlConn.getResponseCode());
    urlConn.disconnect();
  }
  TimeUnit.SECONDS.sleep(Constants.Stream.HEARTBEAT_INTERVAL + 1);
  StreamWriterHeartbeat heartbeat=heartbeatPublisher.getHeartbeat();
  Assert.assertNotNull(heartbeat);
  Assert.assertEquals(1,heartbeat.getStreamsSizes().size());
  Long streamSize=heartbeat.getStreamsSizes().get(streamId);
  Assert.assertNotNull(streamSize);
  Assert.assertEquals(entries * TWO_BYTES.length,(long)streamSize);
}"
7212,"@BeforeClass public static void beforeClass() throws IOException {
  zkServer=InMemoryZKServer.builder().setDataDir(TEMP_FOLDER.newFolder()).build();
  zkServer.startAndWait();
  conf=CConfiguration.create();
  conf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  conf.setInt(Constants.Stream.CONTAINER_INSTANCE_ID,0);
  conf.set(Constants.CFG_LOCAL_DATA_DIR,TEMP_FOLDER.newFolder().getAbsolutePath());
  injector=Guice.createInjector(Modules.override(new ZKClientModule(),new DataFabricModules().getInMemoryModules(),new ConfigModule(conf,new Configuration()),new AuthModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new LocationRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new DataSetServiceModules().getInMemoryModules(),new DataSetsModules().getStandaloneModules(),new NotificationFeedServiceRuntimeModule().getInMemoryModules(),new NotificationServiceRuntimeModule().getInMemoryModules(),new MetricsClientRuntimeModule().getInMemoryModules(),new StreamServiceRuntimeModule().getDistributedModules(),new StreamAdminModules().getInMemoryModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
      bind(StreamConsumerStateStoreFactory.class).to(LevelDBStreamConsumerStateStoreFactory.class).in(Singleton.class);
      bind(StreamAdmin.class).to(FileStreamAdmin.class).in(Singleton.class);
      bind(StreamConsumerFactory.class).to(LevelDBStreamFileConsumerFactory.class).in(Singleton.class);
      bind(StreamFileWriterFactory.class).to(LocationStreamFileWriterFactory.class).in(Singleton.class);
      bind(StreamFileJanitorService.class).to(LocalStreamFileJanitorService.class).in(Scopes.SINGLETON);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class).in(Scopes.SINGLETON);
      bind(HeartbeatPublisher.class).to(MockHeartbeatPublisher.class).in(Scopes.SINGLETON);
    }
  }
));
  zkClient=injector.getInstance(ZKClientService.class);
  zkClient.startAndWait();
  streamHttpService=injector.getInstance(StreamHttpService.class);
  streamHttpService.startAndWait();
  streamService=injector.getInstance(StreamService.class);
  streamService.startAndWait();
  heartbeatPublisher=(MockHeartbeatPublisher)injector.getInstance(HeartbeatPublisher.class);
  hostname=streamHttpService.getBindAddress().getHostName();
  port=streamHttpService.getBindAddress().getPort();
}","@BeforeClass public static void beforeClass() throws IOException {
  zkServer=InMemoryZKServer.builder().setDataDir(TEMP_FOLDER.newFolder()).build();
  zkServer.startAndWait();
  CConfiguration conf=CConfiguration.create();
  conf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  conf.setInt(Constants.Stream.CONTAINER_INSTANCE_ID,0);
  conf.set(Constants.CFG_LOCAL_DATA_DIR,TEMP_FOLDER.newFolder().getAbsolutePath());
  Injector injector=Guice.createInjector(Modules.override(new ZKClientModule(),new DataFabricModules().getInMemoryModules(),new ConfigModule(conf,new Configuration()),new AuthModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new LocationRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new DataSetServiceModules().getInMemoryModules(),new DataSetsModules().getStandaloneModules(),new NotificationFeedServiceRuntimeModule().getInMemoryModules(),new NotificationServiceRuntimeModule().getInMemoryModules(),new MetricsClientRuntimeModule().getInMemoryModules(),new StreamServiceRuntimeModule().getDistributedModules(),new StreamAdminModules().getInMemoryModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
      bind(StreamConsumerStateStoreFactory.class).to(LevelDBStreamConsumerStateStoreFactory.class).in(Singleton.class);
      bind(StreamAdmin.class).to(FileStreamAdmin.class).in(Singleton.class);
      bind(StreamConsumerFactory.class).to(LevelDBStreamFileConsumerFactory.class).in(Singleton.class);
      bind(StreamFileWriterFactory.class).to(LocationStreamFileWriterFactory.class).in(Singleton.class);
      bind(StreamFileJanitorService.class).to(LocalStreamFileJanitorService.class).in(Scopes.SINGLETON);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class).in(Scopes.SINGLETON);
      bind(HeartbeatPublisher.class).to(MockHeartbeatPublisher.class).in(Scopes.SINGLETON);
    }
  }
));
  zkClient=injector.getInstance(ZKClientService.class);
  zkClient.startAndWait();
  streamHttpService=injector.getInstance(StreamHttpService.class);
  streamHttpService.startAndWait();
  streamService=injector.getInstance(StreamService.class);
  streamService.startAndWait();
  heartbeatPublisher=(MockHeartbeatPublisher)injector.getInstance(HeartbeatPublisher.class);
  hostname=streamHttpService.getBindAddress().getHostName();
  port=streamHttpService.getBindAddress().getPort();
  streamAdmin=injector.getInstance(StreamAdmin.class);
  streamHandler=injector.getInstance(StreamHandler.class);
  streamFetchHandler=injector.getInstance(StreamFetchHandler.class);
  streamMetaStore=injector.getInstance(StreamMetaStore.class);
  injector.getInstance(LocationFactory.class).create(Constants.DEFAULT_NAMESPACE).mkdirs();
}"
7213,"@Test public void testCreateExist() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  String streamName=""String_Node_Str"";
  Id.Stream streamId=Id.Stream.from(""String_Node_Str"",streamName);
  Id.Stream otherStreamId=Id.Stream.from(""String_Node_Str"",streamName);
  Assert.assertFalse(streamAdmin.exists(streamId));
  Assert.assertFalse(streamAdmin.exists(otherStreamId));
  streamAdmin.create(streamId);
  Assert.assertTrue(streamAdmin.exists(streamId));
  Assert.assertFalse(streamAdmin.exists(otherStreamId));
  streamAdmin.create(otherStreamId);
  Assert.assertTrue(streamAdmin.exists(otherStreamId));
}","@Test public void testCreateExist() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  String streamName=""String_Node_Str"";
  Id.Stream streamId=Id.Stream.from(FOO_NAMESPACE,streamName);
  Id.Stream otherStreamId=Id.Stream.from(OTHER_NAMESPACE,streamName);
  Assert.assertFalse(streamAdmin.exists(streamId));
  Assert.assertFalse(streamAdmin.exists(otherStreamId));
  streamAdmin.create(streamId);
  Assert.assertTrue(streamAdmin.exists(streamId));
  Assert.assertFalse(streamAdmin.exists(otherStreamId));
  streamAdmin.create(otherStreamId);
  Assert.assertTrue(streamAdmin.exists(otherStreamId));
}"
7214,"@Test public void testDropAllInNamespace() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  Id.Stream otherStream=Id.Stream.from(""String_Node_Str"",""String_Node_Str"");
  String fooNamespace=""String_Node_Str"";
  List<Id.Stream> fooStreams=Lists.newArrayList();
  for (int i=0; i < 4; i++) {
    fooStreams.add(Id.Stream.from(fooNamespace,""String_Node_Str"" + i));
  }
  List<Id.Stream> allStreams=Lists.newArrayList();
  allStreams.addAll(fooStreams);
  allStreams.add(otherStream);
  for (  Id.Stream stream : allStreams) {
    streamAdmin.create(stream);
    writeEvent(stream);
    Assert.assertNotEquals(0,getStreamSize(stream));
  }
  streamAdmin.dropAllInNamespace(Id.Namespace.from(fooNamespace));
  for (  Id.Stream defaultStream : fooStreams) {
    Assert.assertEquals(0,getStreamSize(defaultStream));
  }
  Assert.assertNotEquals(0,getStreamSize(otherStream));
  streamAdmin.truncate(otherStream);
  Assert.assertEquals(0,getStreamSize(otherStream));
}","@Test public void testDropAllInNamespace() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  Id.Stream otherStream=Id.Stream.from(OTHER_NAMESPACE,""String_Node_Str"");
  List<Id.Stream> fooStreams=Lists.newArrayList();
  for (int i=0; i < 4; i++) {
    fooStreams.add(Id.Stream.from(FOO_NAMESPACE,""String_Node_Str"" + i));
  }
  List<Id.Stream> allStreams=Lists.newArrayList();
  allStreams.addAll(fooStreams);
  allStreams.add(otherStream);
  for (  Id.Stream stream : allStreams) {
    streamAdmin.create(stream);
    writeEvent(stream);
    Assert.assertNotEquals(0,getStreamSize(stream));
  }
  streamAdmin.dropAllInNamespace(Id.Namespace.from(FOO_NAMESPACE));
  for (  Id.Stream defaultStream : fooStreams) {
    Assert.assertEquals(0,getStreamSize(defaultStream));
  }
  Assert.assertNotEquals(0,getStreamSize(otherStream));
  streamAdmin.truncate(otherStream);
  Assert.assertEquals(0,getStreamSize(otherStream));
}"
7215,"private DatasetSpecification migrateDatasetSpec(DatasetSpecification oldSpec){
  Id.DatasetInstance dsId=from(oldSpec.getName());
  String newDatasetName=dsId.getId();
  DatasetSpecification.Builder builder=DatasetSpecification.builder(newDatasetName,oldSpec.getType()).properties(oldSpec.getProperties());
  for (  DatasetSpecification embeddedDsSpec : oldSpec.getSpecifications().values()) {
    LOG.debug(""String_Node_Str"",embeddedDsSpec);
    DatasetSpecification migratedEmbeddedSpec=migrateDatasetSpec(embeddedDsSpec);
    LOG.debug(""String_Node_Str"",migratedEmbeddedSpec);
    builder.datasets(migratedEmbeddedSpec);
  }
  return builder.build();
}","private DatasetSpecification migrateDatasetSpec(DatasetSpecification oldSpec,String newDatasetName){
  DatasetSpecification.Builder builder=DatasetSpecification.builder(newDatasetName,oldSpec.getType()).properties(oldSpec.getProperties());
  for (  Map.Entry<String,DatasetSpecification> dsSpecEntry : oldSpec.getSpecifications().entrySet()) {
    DatasetSpecification embeddedDsSpec=dsSpecEntry.getValue();
    LOG.debug(""String_Node_Str"",embeddedDsSpec);
    DatasetSpecification migratedEmbeddedSpec=migrateDatasetSpec(embeddedDsSpec,dsSpecEntry.getKey());
    LOG.debug(""String_Node_Str"",migratedEmbeddedSpec);
    builder.datasets(migratedEmbeddedSpec);
  }
  return builder.build();
}"
7216,"/** 
 * Configures the AppFabricService pre-start.
 */
@Override protected void startUp() throws Exception {
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(Constants.SYSTEM_NAMESPACE,Constants.Logging.COMPONENT_NAME,Constants.Service.APP_FABRIC_HTTP));
  File tmpDir=new File(configuration.get(Constants.CFG_LOCAL_DATA_DIR),configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  FileUtils.deleteDirectory(tmpDir);
  notificationService.start();
  schedulerService.start();
  adapterService.start();
  programRuntimeService.start();
  ImmutableList.Builder<HandlerHook> builder=ImmutableList.builder();
  for (  String hook : handlerHookNames) {
    builder.add(new MetricsReporterHook(metricsCollectionService,hook));
  }
  httpService=new CommonNettyHttpServiceBuilder(configuration).setHost(hostname.getCanonicalHostName()).setHandlerHooks(builder.build()).addHttpHandlers(handlers).setConnectionBacklog(configuration.getInt(Constants.AppFabric.BACKLOG_CONNECTIONS,Constants.AppFabric.DEFAULT_BACKLOG)).setExecThreadPoolSize(configuration.getInt(Constants.AppFabric.EXEC_THREADS,Constants.AppFabric.DEFAULT_EXEC_THREADS)).setBossThreadPoolSize(configuration.getInt(Constants.AppFabric.BOSS_THREADS,Constants.AppFabric.DEFAULT_BOSS_THREADS)).setWorkerThreadPoolSize(configuration.getInt(Constants.AppFabric.WORKER_THREADS,Constants.AppFabric.DEFAULT_WORKER_THREADS)).build();
  httpService.addListener(new ServiceListenerAdapter(){
    private List<Cancellable> cancellables=Lists.newArrayList();
    @Override public void running(){
      final InetSocketAddress socketAddress=httpService.getBindAddress();
      LOG.info(""String_Node_Str"",socketAddress);
      for (      final String serviceName : servicesNames) {
        cancellables.add(discoveryService.register(new Discoverable(){
          @Override public String getName(){
            return serviceName;
          }
          @Override public InetSocketAddress getSocketAddress(){
            return socketAddress;
          }
        }
));
      }
    }
    @Override public void terminated(    State from){
      LOG.info(""String_Node_Str"");
      for (      Cancellable cancellable : cancellables) {
        if (cancellable != null) {
          cancellable.cancel();
        }
      }
    }
    @Override public void failed(    State from,    Throwable failure){
      LOG.info(""String_Node_Str"",failure);
      for (      Cancellable cancellable : cancellables) {
        if (cancellable != null) {
          cancellable.cancel();
        }
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  httpService.startAndWait();
  Thread defaultNamespaceEnsurer=new Thread(new DefaultNamespaceEnsurer(namespaceAdmin,1,TimeUnit.SECONDS));
  defaultNamespaceEnsurer.start();
}","/** 
 * Configures the AppFabricService pre-start.
 */
@Override protected void startUp() throws Exception {
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(Constants.SYSTEM_NAMESPACE,Constants.Logging.COMPONENT_NAME,Constants.Service.APP_FABRIC_HTTP));
  File tmpDir=new File(configuration.get(Constants.CFG_LOCAL_DATA_DIR),configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  FileUtils.deleteDirectory(tmpDir);
  notificationService.start();
  schedulerService.start();
  adapterService.start();
  programRuntimeService.start();
  streamCoordinatorClient.start();
  ImmutableList.Builder<HandlerHook> builder=ImmutableList.builder();
  for (  String hook : handlerHookNames) {
    builder.add(new MetricsReporterHook(metricsCollectionService,hook));
  }
  httpService=new CommonNettyHttpServiceBuilder(configuration).setHost(hostname.getCanonicalHostName()).setHandlerHooks(builder.build()).addHttpHandlers(handlers).setConnectionBacklog(configuration.getInt(Constants.AppFabric.BACKLOG_CONNECTIONS,Constants.AppFabric.DEFAULT_BACKLOG)).setExecThreadPoolSize(configuration.getInt(Constants.AppFabric.EXEC_THREADS,Constants.AppFabric.DEFAULT_EXEC_THREADS)).setBossThreadPoolSize(configuration.getInt(Constants.AppFabric.BOSS_THREADS,Constants.AppFabric.DEFAULT_BOSS_THREADS)).setWorkerThreadPoolSize(configuration.getInt(Constants.AppFabric.WORKER_THREADS,Constants.AppFabric.DEFAULT_WORKER_THREADS)).build();
  httpService.addListener(new ServiceListenerAdapter(){
    private List<Cancellable> cancellables=Lists.newArrayList();
    @Override public void running(){
      final InetSocketAddress socketAddress=httpService.getBindAddress();
      LOG.info(""String_Node_Str"",socketAddress);
      for (      final String serviceName : servicesNames) {
        cancellables.add(discoveryService.register(new Discoverable(){
          @Override public String getName(){
            return serviceName;
          }
          @Override public InetSocketAddress getSocketAddress(){
            return socketAddress;
          }
        }
));
      }
    }
    @Override public void terminated(    State from){
      LOG.info(""String_Node_Str"");
      for (      Cancellable cancellable : cancellables) {
        if (cancellable != null) {
          cancellable.cancel();
        }
      }
    }
    @Override public void failed(    State from,    Throwable failure){
      LOG.info(""String_Node_Str"",failure);
      for (      Cancellable cancellable : cancellables) {
        if (cancellable != null) {
          cancellable.cancel();
        }
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  httpService.startAndWait();
  Thread defaultNamespaceEnsurer=new Thread(new DefaultNamespaceEnsurer(namespaceAdmin,1,TimeUnit.SECONDS));
  defaultNamespaceEnsurer.start();
}"
7217,"/** 
 * Construct the AppFabricServer with service factory and configuration coming from guice injection.
 */
@Inject public AppFabricServer(CConfiguration configuration,DiscoveryService discoveryService,SchedulerService schedulerService,NotificationService notificationService,@Named(Constants.AppFabric.SERVER_ADDRESS) InetAddress hostname,@Named(""String_Node_Str"") Set<HttpHandler> handlers,@Nullable MetricsCollectionService metricsCollectionService,ProgramRuntimeService programRuntimeService,AdapterService adapterService,@Named(""String_Node_Str"") Set<String> servicesNames,@Named(""String_Node_Str"") Set<String> handlerHookNames,NamespaceAdmin namespaceAdmin){
  this.hostname=hostname;
  this.discoveryService=discoveryService;
  this.schedulerService=schedulerService;
  this.handlers=handlers;
  this.configuration=configuration;
  this.metricsCollectionService=metricsCollectionService;
  this.programRuntimeService=programRuntimeService;
  this.adapterService=adapterService;
  this.notificationService=notificationService;
  this.servicesNames=servicesNames;
  this.handlerHookNames=handlerHookNames;
  this.namespaceAdmin=namespaceAdmin;
}","/** 
 * Construct the AppFabricServer with service factory and configuration coming from guice injection.
 */
@Inject public AppFabricServer(CConfiguration configuration,DiscoveryService discoveryService,SchedulerService schedulerService,NotificationService notificationService,@Named(Constants.AppFabric.SERVER_ADDRESS) InetAddress hostname,@Named(""String_Node_Str"") Set<HttpHandler> handlers,@Nullable MetricsCollectionService metricsCollectionService,ProgramRuntimeService programRuntimeService,AdapterService adapterService,StreamCoordinatorClient streamCoordinatorClient,@Named(""String_Node_Str"") Set<String> servicesNames,@Named(""String_Node_Str"") Set<String> handlerHookNames,NamespaceAdmin namespaceAdmin){
  this.hostname=hostname;
  this.discoveryService=discoveryService;
  this.schedulerService=schedulerService;
  this.handlers=handlers;
  this.configuration=configuration;
  this.metricsCollectionService=metricsCollectionService;
  this.programRuntimeService=programRuntimeService;
  this.adapterService=adapterService;
  this.notificationService=notificationService;
  this.servicesNames=servicesNames;
  this.handlerHookNames=handlerHookNames;
  this.namespaceAdmin=namespaceAdmin;
  this.streamCoordinatorClient=streamCoordinatorClient;
}"
7218,"/** 
 * pathParts should look like {app-id}/{program-type}/{program-id}/{component-type}/{component-id}.
 */
static void parseSubContext(Iterator<String> pathParts,Map<String,String> tagValues) throws MetricsPathException {
  if (!pathParts.hasNext()) {
    return;
  }
  String pathProgramTypeStr=pathParts.next();
  ProgramType programType;
  try {
    programType=ProgramType.valueOf(pathProgramTypeStr.toUpperCase());
  }
 catch (  IllegalArgumentException e) {
    throw new MetricsPathException(""String_Node_Str"" + pathProgramTypeStr);
  }
  if (!pathParts.hasNext()) {
    return;
  }
  tagValues.put(programType.getTagName(),pathParts.next());
  if (!pathParts.hasNext()) {
    return;
  }
switch (programType) {
case MAPREDUCE:
    String mrTypeStr=pathParts.next();
  if (mrTypeStr.equals(RUN_ID)) {
    parseRunId(pathParts,tagValues);
    if (pathParts.hasNext()) {
      mrTypeStr=pathParts.next();
    }
 else {
      return;
    }
  }
MapReduceType mrType;
try {
mrType=MapReduceType.valueOf(mrTypeStr.toUpperCase());
}
 catch (IllegalArgumentException e) {
throw new MetricsPathException(""String_Node_Str"" + mrTypeStr + ""String_Node_Str"");
}
tagValues.put(Constants.Metrics.Tag.MR_TASK_TYPE,mrType.getId());
break;
case FLOWS:
buildFlowletContext(pathParts,tagValues);
break;
case HANDLERS:
buildComponentTypeContext(pathParts,tagValues,""String_Node_Str"",""String_Node_Str"",Constants.Metrics.Tag.METHOD);
break;
case SERVICES:
buildComponentTypeContext(pathParts,tagValues,""String_Node_Str"",""String_Node_Str"",Constants.Metrics.Tag.SERVICE_RUNNABLE);
break;
case PROCEDURES:
if (pathParts.hasNext()) {
if (pathParts.next().equals(RUN_ID)) {
parseRunId(pathParts,tagValues);
}
}
break;
case SPARK:
if (pathParts.hasNext()) {
if (pathParts.next().equals(RUN_ID)) {
parseRunId(pathParts,tagValues);
}
}
break;
}
if (pathParts.hasNext()) {
throw new MetricsPathException(""String_Node_Str"");
}
}","/** 
 * pathParts should look like {app-id}/{program-type}/{program-id}/{component-type}/{component-id}.
 */
static void parseSubContext(Iterator<String> pathParts,Map<String,String> tagValues) throws MetricsPathException {
  if (!pathParts.hasNext()) {
    return;
  }
  String pathProgramTypeStr=pathParts.next();
  ProgramType programType;
  try {
    programType=ProgramType.valueOf(pathProgramTypeStr.toUpperCase());
  }
 catch (  IllegalArgumentException e) {
    throw new MetricsPathException(""String_Node_Str"" + pathProgramTypeStr);
  }
  if (pathParts.hasNext()) {
    tagValues.put(programType.getTagName(),pathParts.next());
  }
 else {
    tagValues.put(programType.getTagName(),null);
  }
  if (!pathParts.hasNext()) {
    return;
  }
switch (programType) {
case MAPREDUCE:
    String mrTypeStr=pathParts.next();
  if (mrTypeStr.equals(RUN_ID)) {
    parseRunId(pathParts,tagValues);
    if (pathParts.hasNext()) {
      mrTypeStr=pathParts.next();
    }
 else {
      return;
    }
  }
MapReduceType mrType;
try {
mrType=MapReduceType.valueOf(mrTypeStr.toUpperCase());
}
 catch (IllegalArgumentException e) {
throw new MetricsPathException(""String_Node_Str"" + mrTypeStr + ""String_Node_Str"");
}
tagValues.put(Constants.Metrics.Tag.MR_TASK_TYPE,mrType.getId());
break;
case FLOWS:
buildFlowletContext(pathParts,tagValues);
break;
case HANDLERS:
buildComponentTypeContext(pathParts,tagValues,""String_Node_Str"",""String_Node_Str"",Constants.Metrics.Tag.METHOD);
break;
case SERVICES:
buildComponentTypeContext(pathParts,tagValues,""String_Node_Str"",""String_Node_Str"",Constants.Metrics.Tag.SERVICE_RUNNABLE);
break;
case PROCEDURES:
if (pathParts.hasNext()) {
if (pathParts.next().equals(RUN_ID)) {
parseRunId(pathParts,tagValues);
}
}
break;
case SPARK:
if (pathParts.hasNext()) {
if (pathParts.next().equals(RUN_ID)) {
parseRunId(pathParts,tagValues);
}
}
break;
}
if (pathParts.hasNext()) {
throw new MetricsPathException(""String_Node_Str"");
}
}"
7219,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws AlreadyExistsException if the specified namespace already exists
 */
public void createNamespace(NamespaceMeta metadata) throws NamespaceCannotBeCreatedException, AlreadyExistsException {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceMeta existing=store.getNamespace(Id.Namespace.from(metadata.getName()));
  if (existing != null) {
    throw new AlreadyExistsException(NAMESPACE_ELEMENT_TYPE,metadata.getName());
  }
  try {
    dsFramework.createNamespace(Id.Namespace.from(metadata.getName()));
  }
 catch (  DatasetManagementException e) {
    throw new NamespaceCannotBeCreatedException(metadata.getName(),e);
  }
  store.createNamespace(metadata);
}","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws AlreadyExistsException if the specified namespace already exists
 */
public void createNamespace(NamespaceMeta metadata) throws NamespaceCannotBeCreatedException, AlreadyExistsException {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  if (hasNamespace(Id.Namespace.from(metadata.getName()))) {
    throw new AlreadyExistsException(NAMESPACE_ELEMENT_TYPE,metadata.getName());
  }
  try {
    dsFramework.createNamespace(Id.Namespace.from(metadata.getName()));
  }
 catch (  DatasetManagementException e) {
    throw new NamespaceCannotBeCreatedException(metadata.getName(),e);
  }
  store.createNamespace(metadata);
}"
7220,"/** 
 * Checks if the specified namespace exists
 * @param namespaceId the {@link Id.Namespace} to check for existence
 * @return true, if the specifed namespace exists, false otherwise
 */
public boolean hasNamespace(Id.Namespace namespaceId){
  boolean exists=true;
  try {
    getNamespace(namespaceId);
  }
 catch (  NotFoundException e) {
    if (Constants.DEFAULT_NAMESPACE.equals(namespaceId.getId())) {
      createDefaultNamespace();
    }
 else {
      exists=false;
    }
  }
  return exists;
}","/** 
 * Checks if the specified namespace exists
 * @param namespaceId the {@link Id.Namespace} to check for existence
 * @return true, if the specifed namespace exists, false otherwise
 */
public boolean hasNamespace(Id.Namespace namespaceId){
  try {
    getNamespace(namespaceId);
  }
 catch (  NotFoundException e) {
    return false;
  }
  return true;
}"
7221,"/** 
 * Configures the AppFabricService pre-start.
 */
@Override protected void startUp() throws Exception {
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(Constants.SYSTEM_NAMESPACE,Constants.Logging.COMPONENT_NAME,Constants.Service.APP_FABRIC_HTTP));
  File tmpDir=new File(configuration.get(Constants.CFG_LOCAL_DATA_DIR),configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  FileUtils.deleteDirectory(tmpDir);
  notificationService.start();
  schedulerService.start();
  adapterService.start();
  programRuntimeService.start();
  ImmutableList.Builder<HandlerHook> builder=ImmutableList.builder();
  for (  String hook : handlerHookNames) {
    builder.add(new MetricsReporterHook(metricsCollectionService,hook));
  }
  httpService=new CommonNettyHttpServiceBuilder(configuration).setHost(hostname.getCanonicalHostName()).setHandlerHooks(builder.build()).addHttpHandlers(handlers).setConnectionBacklog(configuration.getInt(Constants.AppFabric.BACKLOG_CONNECTIONS,Constants.AppFabric.DEFAULT_BACKLOG)).setExecThreadPoolSize(configuration.getInt(Constants.AppFabric.EXEC_THREADS,Constants.AppFabric.DEFAULT_EXEC_THREADS)).setBossThreadPoolSize(configuration.getInt(Constants.AppFabric.BOSS_THREADS,Constants.AppFabric.DEFAULT_BOSS_THREADS)).setWorkerThreadPoolSize(configuration.getInt(Constants.AppFabric.WORKER_THREADS,Constants.AppFabric.DEFAULT_WORKER_THREADS)).build();
  httpService.addListener(new ServiceListenerAdapter(){
    private List<Cancellable> cancellables=Lists.newArrayList();
    @Override public void running(){
      final InetSocketAddress socketAddress=httpService.getBindAddress();
      LOG.info(""String_Node_Str"",socketAddress);
      for (      final String serviceName : servicesNames) {
        cancellables.add(discoveryService.register(new Discoverable(){
          @Override public String getName(){
            return serviceName;
          }
          @Override public InetSocketAddress getSocketAddress(){
            return socketAddress;
          }
        }
));
      }
    }
    @Override public void terminated(    State from){
      LOG.info(""String_Node_Str"");
      for (      Cancellable cancellable : cancellables) {
        if (cancellable != null) {
          cancellable.cancel();
        }
      }
    }
    @Override public void failed(    State from,    Throwable failure){
      LOG.info(""String_Node_Str"",failure);
      for (      Cancellable cancellable : cancellables) {
        if (cancellable != null) {
          cancellable.cancel();
        }
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  httpService.startAndWait();
}","/** 
 * Configures the AppFabricService pre-start.
 */
@Override protected void startUp() throws Exception {
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(Constants.SYSTEM_NAMESPACE,Constants.Logging.COMPONENT_NAME,Constants.Service.APP_FABRIC_HTTP));
  File tmpDir=new File(configuration.get(Constants.CFG_LOCAL_DATA_DIR),configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  FileUtils.deleteDirectory(tmpDir);
  notificationService.start();
  schedulerService.start();
  adapterService.start();
  programRuntimeService.start();
  ImmutableList.Builder<HandlerHook> builder=ImmutableList.builder();
  for (  String hook : handlerHookNames) {
    builder.add(new MetricsReporterHook(metricsCollectionService,hook));
  }
  httpService=new CommonNettyHttpServiceBuilder(configuration).setHost(hostname.getCanonicalHostName()).setHandlerHooks(builder.build()).addHttpHandlers(handlers).setConnectionBacklog(configuration.getInt(Constants.AppFabric.BACKLOG_CONNECTIONS,Constants.AppFabric.DEFAULT_BACKLOG)).setExecThreadPoolSize(configuration.getInt(Constants.AppFabric.EXEC_THREADS,Constants.AppFabric.DEFAULT_EXEC_THREADS)).setBossThreadPoolSize(configuration.getInt(Constants.AppFabric.BOSS_THREADS,Constants.AppFabric.DEFAULT_BOSS_THREADS)).setWorkerThreadPoolSize(configuration.getInt(Constants.AppFabric.WORKER_THREADS,Constants.AppFabric.DEFAULT_WORKER_THREADS)).build();
  httpService.addListener(new ServiceListenerAdapter(){
    private List<Cancellable> cancellables=Lists.newArrayList();
    @Override public void running(){
      final InetSocketAddress socketAddress=httpService.getBindAddress();
      LOG.info(""String_Node_Str"",socketAddress);
      for (      final String serviceName : servicesNames) {
        cancellables.add(discoveryService.register(new Discoverable(){
          @Override public String getName(){
            return serviceName;
          }
          @Override public InetSocketAddress getSocketAddress(){
            return socketAddress;
          }
        }
));
      }
    }
    @Override public void terminated(    State from){
      LOG.info(""String_Node_Str"");
      for (      Cancellable cancellable : cancellables) {
        if (cancellable != null) {
          cancellable.cancel();
        }
      }
    }
    @Override public void failed(    State from,    Throwable failure){
      LOG.info(""String_Node_Str"",failure);
      for (      Cancellable cancellable : cancellables) {
        if (cancellable != null) {
          cancellable.cancel();
        }
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  httpService.startAndWait();
  Thread defaultNamespaceEnsurer=new Thread(new DefaultNamespaceEnsurer(namespaceAdmin,1,TimeUnit.SECONDS));
  defaultNamespaceEnsurer.start();
}"
7222,"/** 
 * Construct the AppFabricServer with service factory and configuration coming from guice injection.
 */
@Inject public AppFabricServer(CConfiguration configuration,DiscoveryService discoveryService,SchedulerService schedulerService,NotificationService notificationService,@Named(Constants.AppFabric.SERVER_ADDRESS) InetAddress hostname,@Named(""String_Node_Str"") Set<HttpHandler> handlers,@Nullable MetricsCollectionService metricsCollectionService,ProgramRuntimeService programRuntimeService,AdapterService adapterService,@Named(""String_Node_Str"") Set<String> servicesNames,@Named(""String_Node_Str"") Set<String> handlerHookNames){
  this.hostname=hostname;
  this.discoveryService=discoveryService;
  this.schedulerService=schedulerService;
  this.handlers=handlers;
  this.configuration=configuration;
  this.metricsCollectionService=metricsCollectionService;
  this.programRuntimeService=programRuntimeService;
  this.adapterService=adapterService;
  this.notificationService=notificationService;
  this.servicesNames=servicesNames;
  this.handlerHookNames=handlerHookNames;
}","/** 
 * Construct the AppFabricServer with service factory and configuration coming from guice injection.
 */
@Inject public AppFabricServer(CConfiguration configuration,DiscoveryService discoveryService,SchedulerService schedulerService,NotificationService notificationService,@Named(Constants.AppFabric.SERVER_ADDRESS) InetAddress hostname,@Named(""String_Node_Str"") Set<HttpHandler> handlers,@Nullable MetricsCollectionService metricsCollectionService,ProgramRuntimeService programRuntimeService,AdapterService adapterService,@Named(""String_Node_Str"") Set<String> servicesNames,@Named(""String_Node_Str"") Set<String> handlerHookNames,NamespaceAdmin namespaceAdmin){
  this.hostname=hostname;
  this.discoveryService=discoveryService;
  this.schedulerService=schedulerService;
  this.handlers=handlers;
  this.configuration=configuration;
  this.metricsCollectionService=metricsCollectionService;
  this.programRuntimeService=programRuntimeService;
  this.adapterService=adapterService;
  this.notificationService=notificationService;
  this.servicesNames=servicesNames;
  this.handlerHookNames=handlerHookNames;
  this.namespaceAdmin=namespaceAdmin;
}"
7223,"@BeforeClass public static void set() throws Exception {
  schedulerService=AppFabricTestHelper.getInjector().getInstance(SchedulerService.class);
  notificationFeedManager=AppFabricTestHelper.getInjector().getInstance(NotificationFeedManager.class);
  store=AppFabricTestHelper.getInjector().getInstance(StoreFactory.class).create();
  locationFactory=AppFabricTestHelper.getInjector().getInstance(LocationFactory.class);
}","@BeforeClass public static void set() throws Exception {
  schedulerService=AppFabricTestHelper.getInjector().getInstance(SchedulerService.class);
  store=AppFabricTestHelper.getInjector().getInstance(StoreFactory.class).create();
  locationFactory=AppFabricTestHelper.getInjector().getInstance(LocationFactory.class);
  namespaceAdmin=AppFabricTestHelper.getInjector().getInstance(NamespaceAdmin.class);
  namespaceAdmin.createNamespace(Constants.DEFAULT_NAMESPACE_META);
}"
7224,"@AfterClass public static void finish(){
  schedulerService.stopAndWait();
}","@AfterClass public static void finish() throws NotFoundException, NamespaceCannotBeDeletedException {
  namespaceAdmin.deleteDatasets(Constants.DEFAULT_NAMESPACE_ID);
  schedulerService.stopAndWait();
}"
7225,"public void reset() throws Exception {
  MockResponder responder=new MockResponder();
  String uri=String.format(""String_Node_Str"");
  HttpRequest request=new DefaultHttpRequest(HttpVersion.HTTP_1_1,HttpMethod.PUT,uri);
  httpHandler.resetCDAP(request,responder);
  verifyResponse(HttpResponseStatus.OK,responder.getStatus(),""String_Node_Str"");
  for (  NamespaceMeta namespaceMeta : namespaceAdmin.listNamespaces()) {
    if (!Constants.DEFAULT_NAMESPACE.equals(namespaceMeta.getId()) && !Constants.SYSTEM_NAMESPACE.equals(namespaceMeta.getId())) {
      Id.Namespace namespace=Id.Namespace.from(namespaceMeta.getId());
      streamAdmin.dropAllInNamespace(namespace);
      namespaceHttpHandler.deleteDatasets(null,new MockResponder(),namespaceMeta.getId());
      namespaceHttpHandler.delete(null,new MockResponder(),namespaceMeta.getId());
    }
  }
}","public void reset() throws Exception {
  MockResponder responder=new MockResponder();
  String uri=String.format(""String_Node_Str"");
  HttpRequest request=new DefaultHttpRequest(HttpVersion.HTTP_1_1,HttpMethod.PUT,uri);
  httpHandler.resetCDAP(request,responder);
  verifyResponse(HttpResponseStatus.OK,responder.getStatus(),""String_Node_Str"");
  for (  NamespaceMeta namespaceMeta : namespaceAdmin.listNamespaces()) {
    if (!Constants.DEFAULT_NAMESPACE.equals(namespaceMeta.getName()) && !Constants.SYSTEM_NAMESPACE.equals(namespaceMeta.getName())) {
      Id.Namespace namespace=Id.Namespace.from(namespaceMeta.getName());
      streamAdmin.dropAllInNamespace(namespace);
      namespaceHttpHandler.deleteDatasets(null,new MockResponder(),namespaceMeta.getName());
      namespaceHttpHandler.delete(null,new MockResponder(),namespaceMeta.getName());
    }
  }
}"
7226,"@BeforeClass public static void set() throws Exception {
  schedulerService=AppFabricTestHelper.getInjector().getInstance(SchedulerService.class);
  notificationFeedManager=AppFabricTestHelper.getInjector().getInstance(NotificationFeedManager.class);
  store=AppFabricTestHelper.getInjector().getInstance(StoreFactory.class).create();
  locationFactory=AppFabricTestHelper.getInjector().getInstance(LocationFactory.class);
  namespaceAdmin=AppFabricTestHelper.getInjector().getInstance(NamespaceAdmin.class);
  namespaceAdmin.createNamespace(new NamespaceMeta.Builder().setId(namespace).build());
}","@BeforeClass public static void set() throws Exception {
  schedulerService=AppFabricTestHelper.getInjector().getInstance(SchedulerService.class);
  notificationFeedManager=AppFabricTestHelper.getInjector().getInstance(NotificationFeedManager.class);
  store=AppFabricTestHelper.getInjector().getInstance(StoreFactory.class).create();
  locationFactory=AppFabricTestHelper.getInjector().getInstance(LocationFactory.class);
  namespaceAdmin=AppFabricTestHelper.getInjector().getInstance(NamespaceAdmin.class);
  namespaceAdmin.createNamespace(new NamespaceMeta.Builder().setName(namespace).build());
}"
7227,"/** 
 * Creates a Namespace.
 * @param namespace the namespace to create
 * @throws Exception
 */
protected static void createNamespace(Id.Namespace namespace) throws Exception {
  getTestManager().createNamespace(new NamespaceMeta.Builder().setId(namespace).build());
}","/** 
 * Creates a Namespace.
 * @param namespace the namespace to create
 * @throws Exception
 */
protected static void createNamespace(Id.Namespace namespace) throws Exception {
  getTestManager().createNamespace(new NamespaceMeta.Builder().setName(namespace).build());
}"
7228,"@Test public void testPartitionedCounting() throws Exception {
  ApplicationManager appManager=getTestManager().deployApplication(SportResults.class);
  ServiceManager serviceManager=appManager.startService(""String_Node_Str"");
  serviceManager.waitForStatus(true);
  URL url=serviceManager.getServiceURL();
  uploadResults(url,""String_Node_Str"",2014,FANTASY_2014);
  uploadResults(url,""String_Node_Str"",2015,FANTASY_2015);
  uploadResults(url,""String_Node_Str"",2014,CRITTERS_2014);
  MapReduceManager mrManager=appManager.startMapReduce(""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
  mrManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<PartitionedFileSet> dataSetManager=getTestManager().getDataset(""String_Node_Str"");
  PartitionedFileSet totals=dataSetManager.get();
  String path=totals.getPartition(PartitionKey.builder().addStringField(""String_Node_Str"",""String_Node_Str"").build());
  Assert.assertNotNull(path);
  Location location=totals.getEmbeddedFileSet().getLocation(path);
  Assert.assertTrue(location.isDirectory());
  for (  Location file : location.list()) {
    if (file.getName().startsWith(""String_Node_Str"")) {
      location=file;
    }
  }
  BufferedReader reader=new BufferedReader(new InputStreamReader(location.getInputStream()));
  Map<String,String[]> expected=ImmutableMap.of(""String_Node_Str"",new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""},""String_Node_Str"",new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""},""String_Node_Str"",new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""});
  while (true) {
    String line=reader.readLine();
    if (line == null) {
      break;
    }
    String[] fields=line.split(""String_Node_Str"");
    Assert.assertArrayEquals(expected.get(fields[0]),fields);
  }
  Connection connection=getTestManager().getQueryClient();
  ResultSet results=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"").executeQuery();
  Assert.assertTrue(results.next());
  Assert.assertEquals(2,results.getInt(1));
  Assert.assertEquals(0,results.getInt(2));
  Assert.assertEquals(1,results.getInt(3));
  Assert.assertEquals(53,results.getInt(4));
  Assert.assertEquals(65,results.getInt(5));
  Assert.assertFalse(results.next());
}","@Test public void testPartitionedCounting() throws Exception {
  ApplicationManager appManager=deployApplication(SportResults.class);
  ServiceManager serviceManager=appManager.startService(""String_Node_Str"");
  serviceManager.waitForStatus(true);
  URL url=serviceManager.getServiceURL();
  uploadResults(url,""String_Node_Str"",2014,FANTASY_2014);
  uploadResults(url,""String_Node_Str"",2015,FANTASY_2015);
  uploadResults(url,""String_Node_Str"",2014,CRITTERS_2014);
  MapReduceManager mrManager=appManager.startMapReduce(""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
  mrManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<PartitionedFileSet> dataSetManager=getDataset(""String_Node_Str"");
  PartitionedFileSet totals=dataSetManager.get();
  String path=totals.getPartition(PartitionKey.builder().addStringField(""String_Node_Str"",""String_Node_Str"").build());
  Assert.assertNotNull(path);
  Location location=totals.getEmbeddedFileSet().getLocation(path);
  Assert.assertTrue(location.isDirectory());
  for (  Location file : location.list()) {
    if (file.getName().startsWith(""String_Node_Str"")) {
      location=file;
    }
  }
  BufferedReader reader=new BufferedReader(new InputStreamReader(location.getInputStream()));
  Map<String,String[]> expected=ImmutableMap.of(""String_Node_Str"",new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""},""String_Node_Str"",new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""},""String_Node_Str"",new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""});
  while (true) {
    String line=reader.readLine();
    if (line == null) {
      break;
    }
    String[] fields=line.split(""String_Node_Str"");
    Assert.assertArrayEquals(expected.get(fields[0]),fields);
  }
  Connection connection=getQueryClient();
  ResultSet results=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"").executeQuery();
  Assert.assertTrue(results.next());
  Assert.assertEquals(2,results.getInt(1));
  Assert.assertEquals(0,results.getInt(2));
  Assert.assertEquals(1,results.getInt(3));
  Assert.assertEquals(53,results.getInt(4));
  Assert.assertEquals(65,results.getInt(5));
  Assert.assertFalse(results.next());
}"
7229,"@Test public void testStreamConversion() throws Exception {
  ApplicationManager appManager=getTestManager().deployApplication(StreamConversionApp.class);
  WorkflowManager workflowManager=appManager.startWorkflow(""String_Node_Str"",RuntimeArguments.NO_ARGUMENTS);
  workflowManager.getSchedule(""String_Node_Str"").suspend();
  StreamWriter streamWriter=appManager.getStreamWriter(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  long startTime=System.currentTimeMillis();
  MapReduceManager mapReduceManager=appManager.startMapReduce(""String_Node_Str"",RuntimeArguments.NO_ARGUMENTS);
  mapReduceManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<TimePartitionedFileSet> fileSetManager=getTestManager().getDataset(""String_Node_Str"");
  TimePartitionedFileSet converted=fileSetManager.get();
  Map<Long,String> partitions=converted.getPartitions(startTime,System.currentTimeMillis());
  Assert.assertEquals(1,partitions.size());
  long partitionTime=partitions.keySet().iterator().next();
  Calendar calendar=Calendar.getInstance();
  calendar.setTimeInMillis(startTime);
  calendar.set(Calendar.SECOND,0);
  calendar.set(Calendar.MILLISECOND,0);
  long startMinute=calendar.getTimeInMillis();
  Assert.assertTrue(partitionTime >= startMinute);
  Assert.assertTrue(partitionTime <= System.currentTimeMillis());
  calendar.setTimeInMillis(partitionTime);
  int year=calendar.get(Calendar.YEAR);
  int month=calendar.get(Calendar.MONTH) + 1;
  int day=calendar.get(Calendar.DAY_OF_MONTH);
  int hour=calendar.get(Calendar.HOUR_OF_DAY);
  int minute=calendar.get(Calendar.MINUTE);
  Connection connection=getTestManager().getQueryClient();
  ResultSet results=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"").executeQuery();
  Assert.assertTrue(results.next());
  Assert.assertEquals(year,results.getInt(1));
  Assert.assertEquals(month,results.getInt(2));
  Assert.assertEquals(day,results.getInt(3));
  Assert.assertEquals(hour,results.getInt(4));
  Assert.assertEquals(minute,results.getInt(5));
  Assert.assertFalse(results.next());
}","@Test public void testStreamConversion() throws Exception {
  ApplicationManager appManager=deployApplication(StreamConversionApp.class);
  WorkflowManager workflowManager=appManager.startWorkflow(""String_Node_Str"",RuntimeArguments.NO_ARGUMENTS);
  workflowManager.getSchedule(""String_Node_Str"").suspend();
  StreamWriter streamWriter=appManager.getStreamWriter(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  long startTime=System.currentTimeMillis();
  MapReduceManager mapReduceManager=appManager.startMapReduce(""String_Node_Str"",RuntimeArguments.NO_ARGUMENTS);
  mapReduceManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<TimePartitionedFileSet> fileSetManager=getDataset(""String_Node_Str"");
  TimePartitionedFileSet converted=fileSetManager.get();
  Map<Long,String> partitions=converted.getPartitions(startTime,System.currentTimeMillis());
  Assert.assertEquals(1,partitions.size());
  long partitionTime=partitions.keySet().iterator().next();
  Calendar calendar=Calendar.getInstance();
  calendar.setTimeInMillis(startTime);
  calendar.set(Calendar.SECOND,0);
  calendar.set(Calendar.MILLISECOND,0);
  long startMinute=calendar.getTimeInMillis();
  Assert.assertTrue(partitionTime >= startMinute);
  Assert.assertTrue(partitionTime <= System.currentTimeMillis());
  calendar.setTimeInMillis(partitionTime);
  int year=calendar.get(Calendar.YEAR);
  int month=calendar.get(Calendar.MONTH) + 1;
  int day=calendar.get(Calendar.DAY_OF_MONTH);
  int hour=calendar.get(Calendar.HOUR_OF_DAY);
  int minute=calendar.get(Calendar.MINUTE);
  Connection connection=getQueryClient();
  ResultSet results=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"").executeQuery();
  Assert.assertTrue(results.next());
  Assert.assertEquals(year,results.getInt(1));
  Assert.assertEquals(month,results.getInt(2));
  Assert.assertEquals(day,results.getInt(3));
  Assert.assertEquals(hour,results.getInt(4));
  Assert.assertEquals(minute,results.getInt(5));
  Assert.assertFalse(results.next());
}"
7230,"/** 
 * only use in unit test since the singleton may be reused for multiple tests.
 */
public void clearTables(){
  tables.clear();
}","/** 
 * only use in unit test since the singleton may be reused for multiple tests.
 */
public void clearTables() throws IOException {
  for (  String name : ImmutableList.copyOf(tables.keySet())) {
    dropTable(name);
  }
}"
7231,"@AfterClass public static void tearDownClass() throws Exception {
  testStackIndex--;
  if (standaloneMain != null && testStackIndex == 0) {
    standaloneMain.shutDown();
    standaloneMain=null;
  }
}","@AfterClass public static void tearDownClass() throws Exception {
  testStackIndex--;
  if (standaloneMain != null && testStackIndex == 0) {
    standaloneMain.shutDown();
    standaloneMain=null;
    LevelDBTableService.getInstance().clearTables();
  }
}"
7232,"private ClientConfig getClientConfig(){
  return new ClientConfig.Builder(clientConfig).setNamespace(application.getNamespace()).build();
}","private ClientConfig getClientConfig(){
  ConnectionConfig connectionConfig=ConnectionConfig.builder(clientConfig.getConnectionConfig()).setNamespace(application.getNamespace()).build();
  return new ClientConfig.Builder(clientConfig).setConnectionConfig(connectionConfig).build();
}"
7233,"private ClientConfig getClientConfig(){
  return new ClientConfig.Builder(clientConfig).setNamespace(procedure.getNamespace()).build();
}","private ClientConfig getClientConfig(){
  ConnectionConfig connectionConfig=ConnectionConfig.builder(clientConfig.getConnectionConfig()).setNamespace(procedure.getNamespace()).build();
  return new ClientConfig.Builder(clientConfig).setConnectionConfig(connectionConfig).build();
}"
7234,"private ClientConfig getClientConfig(){
  return new ClientConfig.Builder(clientConfig).setNamespace(serviceId.getNamespace()).build();
}","private ClientConfig getClientConfig(){
  ConnectionConfig connectionConfig=ConnectionConfig.builder(clientConfig.getConnectionConfig()).setNamespace(serviceId.getNamespace()).build();
  return new ClientConfig.Builder(clientConfig).setConnectionConfig(connectionConfig).build();
}"
7235,"/** 
 * Returns a list of spark jobs associated with account & application.
 */
@GET @Path(""String_Node_Str"") public void getServicesByApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  getProgramsByApp(responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.SERVICE.getCategoryName());
}","/** 
 * Returns a list of services associated with account & application.
 */
@GET @Path(""String_Node_Str"") public void getServicesByApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  getProgramsByApp(responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.SERVICE.getCategoryName());
}"
7236,"@Test public void testUpgrade() throws Exception {
  String dummyPath=tmpFolder.newFolder().getAbsolutePath();
  createStream(""String_Node_Str"");
  Id.DatasetInstance kvID=Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"");
  datasetFramework.addInstance(KeyValueTable.class.getName(),kvID,DatasetProperties.EMPTY);
  Id.DatasetInstance filesetID=Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"");
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)));
  datasetFramework.addInstance(TimePartitionedFileSet.class.getName(),filesetID,FileSetProperties.builder().setBasePath(""String_Node_Str"").setEnableExploreOnCreate(true).setSerDe(""String_Node_Str"").setExploreInputFormat(""String_Node_Str"").setExploreOutputFormat(""String_Node_Str"").setTableProperty(""String_Node_Str"",schema.toString()).build());
  TimePartitionedFileSet tpfs=datasetFramework.getDataset(filesetID,Collections.<String,String>emptyMap(),null);
  Transaction tx1=transactionManager.startShort(100);
  TransactionAware txTpfs=(TransactionAware)tpfs;
  txTpfs.startTx(tx1);
  tpfs.addPartition(0L,""String_Node_Str"");
  tpfs.addPartition(86400000L,""String_Node_Str"");
  Map<PartitionKey,String> partitions=tpfs.getPartitions(null);
  txTpfs.commitTx();
  transactionManager.canCommit(tx1,txTpfs.getTxChanges());
  transactionManager.commit(tx1);
  txTpfs.postTxCommit();
  waitForCompletion(Lists.newArrayList(exploreTableService.disableStream(Id.Stream.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"")),exploreTableService.disableDataset(kvID,datasetFramework.getDatasetSpec(kvID)),exploreTableService.disableDataset(filesetID,datasetFramework.getDatasetSpec(filesetID))));
  String createOldStream=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"" + dummyPath + ""String_Node_Str"";
  String createOldRecordScannable=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"";
  String createOldFileset=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"" + dummyPath + ""String_Node_Str""+ ""String_Node_Str""+ schema.toString()+ ""String_Node_Str"";
  String createNonCDAP=""String_Node_Str"";
  waitForCompletion(Lists.newArrayList(((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createNonCDAP),((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createOldFileset),((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createOldRecordScannable),((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createOldStream)));
  exploreService.upgrade();
  TableInfo tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertFalse(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  Iterator<PartitionKey> partitionKeyIter=partitions.keySet().iterator();
  String expected1=stringify(partitionKeyIter.next());
  String expected2=stringify(partitionKeyIter.next());
  runCommand(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",true,Lists.newArrayList(new ColumnDesc(""String_Node_Str"",""String_Node_Str"",1,""String_Node_Str"")),Lists.newArrayList(new QueryResult(Lists.<Object>newArrayList(expected1)),new QueryResult(Lists.<Object>newArrayList(expected2))));
  Assert.assertEquals(4,exploreService.getTables(""String_Node_Str"").size());
}","@Test public void testUpgrade() throws Exception {
  String dummyPath=tmpFolder.newFolder().getAbsolutePath();
  createStream(""String_Node_Str"");
  Id.DatasetInstance kvID=Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"");
  datasetFramework.addInstance(KeyValueTable.class.getName(),kvID,DatasetProperties.EMPTY);
  Id.DatasetInstance filesetID=Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"");
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)));
  datasetFramework.addInstance(TimePartitionedFileSet.class.getName(),filesetID,FileSetProperties.builder().setBasePath(""String_Node_Str"").setEnableExploreOnCreate(true).setSerDe(""String_Node_Str"").setExploreInputFormat(""String_Node_Str"").setExploreOutputFormat(""String_Node_Str"").setTableProperty(""String_Node_Str"",schema.toString()).build());
  TimePartitionedFileSet tpfs=datasetFramework.getDataset(filesetID,Collections.<String,String>emptyMap(),null);
  Transaction tx1=transactionManager.startShort(100);
  TransactionAware txTpfs=(TransactionAware)tpfs;
  txTpfs.startTx(tx1);
  tpfs.addPartition(0L,""String_Node_Str"");
  Map<PartitionKey,String> partitions=tpfs.getPartitions(null);
  txTpfs.commitTx();
  transactionManager.canCommit(tx1,txTpfs.getTxChanges());
  transactionManager.commit(tx1);
  txTpfs.postTxCommit();
  waitForCompletion(Lists.newArrayList(exploreTableService.disableStream(Id.Stream.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"")),exploreTableService.disableDataset(kvID,datasetFramework.getDatasetSpec(kvID)),exploreTableService.disableDataset(filesetID,datasetFramework.getDatasetSpec(filesetID))));
  String createOldStream=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"" + dummyPath + ""String_Node_Str"";
  String createOldRecordScannable=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"";
  String createOldFileset=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"" + dummyPath + ""String_Node_Str""+ ""String_Node_Str""+ schema.toString()+ ""String_Node_Str"";
  String createNonCDAP=""String_Node_Str"";
  waitForCompletion(Lists.newArrayList(((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createNonCDAP),((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createOldFileset),((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createOldRecordScannable),((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createOldStream)));
  exploreService.upgrade();
  TableInfo tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertFalse(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  Iterator<PartitionKey> partitionKeyIter=partitions.keySet().iterator();
  String expected1=stringify(partitionKeyIter.next());
  String expected2=stringify(partitionKeyIter.next());
  runCommand(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",true,Lists.newArrayList(new ColumnDesc(""String_Node_Str"",""String_Node_Str"",1,""String_Node_Str"")),Lists.newArrayList(new QueryResult(Lists.<Object>newArrayList(expected1)),new QueryResult(Lists.<Object>newArrayList(expected2))));
  Assert.assertEquals(4,exploreService.getTables(""String_Node_Str"").size());
}"
7237,"@Test public void testUpgrade() throws Exception {
  String dummyPath=tmpFolder.newFolder().getAbsolutePath();
  createStream(""String_Node_Str"");
  Id.DatasetInstance kvID=Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"");
  datasetFramework.addInstance(KeyValueTable.class.getName(),kvID,DatasetProperties.EMPTY);
  Id.DatasetInstance filesetID=Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"");
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)));
  datasetFramework.addInstance(TimePartitionedFileSet.class.getName(),filesetID,FileSetProperties.builder().setBasePath(""String_Node_Str"").setEnableExploreOnCreate(true).setSerDe(""String_Node_Str"").setExploreInputFormat(""String_Node_Str"").setExploreOutputFormat(""String_Node_Str"").setTableProperty(""String_Node_Str"",schema.toString()).build());
  TimePartitionedFileSet tpfs=datasetFramework.getDataset(filesetID,Collections.<String,String>emptyMap(),null);
  Transaction tx1=transactionManager.startShort(100);
  TransactionAware txTpfs=(TransactionAware)tpfs;
  txTpfs.startTx(tx1);
  tpfs.addPartition(0L,""String_Node_Str"");
  tpfs.addPartition(86400000L,""String_Node_Str"");
  Map<PartitionKey,String> partitions=tpfs.getPartitions(null);
  txTpfs.commitTx();
  transactionManager.canCommit(tx1,txTpfs.getTxChanges());
  transactionManager.commit(tx1);
  txTpfs.postTxCommit();
  waitForCompletion(Lists.newArrayList(exploreTableService.disableStream(Id.Stream.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"")),exploreTableService.disableDataset(kvID,datasetFramework.getDatasetSpec(kvID)),exploreTableService.disableDataset(filesetID,datasetFramework.getDatasetSpec(filesetID))));
  String createOldStream=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"" + dummyPath + ""String_Node_Str"";
  String createOldRecordScannable=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"";
  String createOldFileset=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"" + dummyPath + ""String_Node_Str""+ ""String_Node_Str""+ schema.toString()+ ""String_Node_Str"";
  String createNonCDAP=""String_Node_Str"";
  waitForCompletion(Lists.newArrayList(((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createNonCDAP),((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createOldFileset),((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createOldRecordScannable),((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createOldStream)));
  exploreService.upgrade();
  TableInfo tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertFalse(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  Iterator<PartitionKey> partitionKeyIter=partitions.keySet().iterator();
  String expected1=stringify(partitionKeyIter.next());
  String expected2=stringify(partitionKeyIter.next());
  runCommand(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",true,Lists.newArrayList(new ColumnDesc(""String_Node_Str"",""String_Node_Str"",1,""String_Node_Str"")),Lists.newArrayList(new QueryResult(Lists.<Object>newArrayList(expected1)),new QueryResult(Lists.<Object>newArrayList(expected2))));
  Assert.assertEquals(4,exploreService.getTables(""String_Node_Str"").size());
}","@Test public void testUpgrade() throws Exception {
  String dummyPath=tmpFolder.newFolder().getAbsolutePath();
  createStream(""String_Node_Str"");
  Id.DatasetInstance kvID=Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"");
  datasetFramework.addInstance(KeyValueTable.class.getName(),kvID,DatasetProperties.EMPTY);
  Id.DatasetInstance filesetID=Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"");
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)));
  datasetFramework.addInstance(TimePartitionedFileSet.class.getName(),filesetID,FileSetProperties.builder().setBasePath(""String_Node_Str"").setEnableExploreOnCreate(true).setSerDe(""String_Node_Str"").setExploreInputFormat(""String_Node_Str"").setExploreOutputFormat(""String_Node_Str"").setTableProperty(""String_Node_Str"",schema.toString()).build());
  TimePartitionedFileSet tpfs=datasetFramework.getDataset(filesetID,Collections.<String,String>emptyMap(),null);
  Transaction tx1=transactionManager.startShort(100);
  TransactionAware txTpfs=(TransactionAware)tpfs;
  txTpfs.startTx(tx1);
  tpfs.addPartition(0L,""String_Node_Str"");
  Map<PartitionKey,String> partitions=tpfs.getPartitions(null);
  txTpfs.commitTx();
  transactionManager.canCommit(tx1,txTpfs.getTxChanges());
  transactionManager.commit(tx1);
  txTpfs.postTxCommit();
  waitForCompletion(Lists.newArrayList(exploreTableService.disableStream(Id.Stream.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"")),exploreTableService.disableDataset(kvID,datasetFramework.getDatasetSpec(kvID)),exploreTableService.disableDataset(filesetID,datasetFramework.getDatasetSpec(filesetID))));
  String createOldStream=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"" + dummyPath + ""String_Node_Str"";
  String createOldRecordScannable=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"";
  String createOldFileset=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"" + dummyPath + ""String_Node_Str""+ ""String_Node_Str""+ schema.toString()+ ""String_Node_Str"";
  String createNonCDAP=""String_Node_Str"";
  waitForCompletion(Lists.newArrayList(((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createNonCDAP),((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createOldFileset),((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createOldRecordScannable),((BaseHiveExploreService)exploreService).execute(""String_Node_Str"",createOldStream)));
  exploreService.upgrade();
  TableInfo tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  tableInfo=exploreService.getTableInfo(""String_Node_Str"",""String_Node_Str"");
  Assert.assertFalse(tableInfo.getParameters().containsKey(Constants.Explore.CDAP_VERSION));
  Iterator<PartitionKey> partitionKeyIter=partitions.keySet().iterator();
  String expected=stringify(partitionKeyIter.next());
  runCommand(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",true,Lists.newArrayList(new ColumnDesc(""String_Node_Str"",""String_Node_Str"",1,""String_Node_Str"")),Lists.newArrayList(new QueryResult(Lists.<Object>newArrayList(expected))));
  Assert.assertEquals(4,exploreService.getTables(""String_Node_Str"").size());
}"
7238,"@Test public void testStreamSizeSchedule() throws Exception {
  scheduleStore.persist(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_1,0L,0L,0L,0L,true);
  scheduleStore.persist(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,1000L,10L,1000L,10L,false);
  Assert.assertEquals(ImmutableList.of(new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_1,0L,0L,0L,0L,true),new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,1000L,10L,1000L,10L,false)),scheduleStore.list());
  scheduleStore.suspend(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1);
  Assert.assertEquals(ImmutableList.of(new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_1,0L,0L,0L,0L,false),new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,1000L,10L,1000L,10L,false)),scheduleStore.list());
  scheduleStore.resume(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  Assert.assertEquals(ImmutableList.of(new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_1,0L,0L,0L,0L,false),new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,1000L,10L,1000L,10L,true)),scheduleStore.list());
  scheduleStore.updateBaseRun(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2,10000L,100L);
  Assert.assertEquals(ImmutableList.of(new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_1,0L,0L,0L,0L,false),new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,10000L,100L,1000L,10L,true)),scheduleStore.list());
  scheduleStore.updateLastRun(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1,100L,10000L);
  Assert.assertEquals(ImmutableList.of(new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_1,0L,0L,100L,10000L,false),new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,10000L,100L,1000L,10L,true)),scheduleStore.list());
  scheduleStore.updateSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1,STREAM_SCHEDULE_2);
  Assert.assertEquals(ImmutableList.of(new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,0L,0L,100L,10000L,false),new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,10000L,100L,1000L,10L,true)),scheduleStore.list());
  scheduleStore.delete(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1);
  Assert.assertEquals(ImmutableList.of(new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,10000L,100L,1000L,10L,true)),scheduleStore.list());
  scheduleStore.delete(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  Assert.assertEquals(ImmutableList.<StreamSizeScheduleState>of(),scheduleStore.list());
}","@Test public void testStreamSizeSchedule() throws Exception {
  scheduleStore.persist(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_1,0L,0L,0L,0L,true);
  scheduleStore.persist(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,1000L,10L,1000L,10L,false);
  Assert.assertEquals(ImmutableList.of(new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_1,0L,0L,0L,0L,true),new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,1000L,10L,1000L,10L,false)),scheduleStore.list());
  scheduleStore.suspend(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1);
  Assert.assertEquals(ImmutableList.of(new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_1,0L,0L,0L,0L,false),new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,1000L,10L,1000L,10L,false)),scheduleStore.list());
  scheduleStore.resume(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  Assert.assertEquals(ImmutableList.of(new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_1,0L,0L,0L,0L,false),new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,1000L,10L,1000L,10L,true)),scheduleStore.list());
  scheduleStore.updateBaseRun(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2,10000L,100L);
  Assert.assertEquals(ImmutableList.of(new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_1,0L,0L,0L,0L,false),new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,10000L,100L,1000L,10L,true)),scheduleStore.list());
  scheduleStore.updateLastRun(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1,100L,10000L,null);
  Assert.assertEquals(ImmutableList.of(new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_1,0L,0L,100L,10000L,false),new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,10000L,100L,1000L,10L,true)),scheduleStore.list());
  scheduleStore.updateSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1,STREAM_SCHEDULE_2);
  Assert.assertEquals(ImmutableList.of(new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,0L,0L,100L,10000L,false),new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,10000L,100L,1000L,10L,true)),scheduleStore.list());
  scheduleStore.delete(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1);
  Assert.assertEquals(ImmutableList.of(new StreamSizeScheduleState(PROGRAM_ID,PROGRAM_TYPE,STREAM_SCHEDULE_2,10000L,100L,1000L,10L,true)),scheduleStore.list());
  scheduleStore.delete(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  Assert.assertEquals(ImmutableList.<StreamSizeScheduleState>of(),scheduleStore.list());
}"
7239,"@Inject private MDSUpgrader(LocationFactory locationFactory,TransactionExecutorFactory executorFactory,@Named(""String_Node_Str"") final DatasetFramework dsFramework,CConfiguration cConf,@Named(""String_Node_Str"") final Store store){
  super(locationFactory);
  this.cConf=cConf;
  this.store=store;
  this.appMDS=Transactional.of(executorFactory,new Supplier<AppMDS>(){
    @Override public AppMDS get(){
      try {
        Table table=DatasetsUtil.getOrCreateDataset(dsFramework,Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE)),""String_Node_Str"",DatasetProperties.EMPTY,DatasetDefinition.NO_ARGUMENTS,null);
        return new AppMDS(new MetadataStoreDataset(table));
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"",Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE),e);
        throw Throwables.propagate(e);
      }
    }
  }
);
  appStreams=Sets.newHashSet();
}","@Inject private MDSUpgrader(LocationFactory locationFactory,TransactionExecutorFactory executorFactory,@Named(""String_Node_Str"") final DatasetFramework dsFramework,CConfiguration cConf,@Named(""String_Node_Str"") final Store store){
  super(locationFactory);
  this.cConf=cConf;
  this.store=store;
  this.appMDS=Transactional.of(executorFactory,new Supplier<AppMDS>(){
    @Override public AppMDS get(){
      try {
        Table table=DatasetsUtil.getOrCreateDataset(dsFramework,Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE)),""String_Node_Str"",DatasetProperties.EMPTY,DatasetDefinition.NO_ARGUMENTS,null);
        return new AppMDS(new AppMetadataStoreDataset(table));
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"",Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE),e);
        throw Throwables.propagate(e);
      }
    }
  }
);
  appStreams=Sets.newHashSet();
}"
7240,"@Override public AppMDS get(){
  try {
    Table table=DatasetsUtil.getOrCreateDataset(dsFramework,Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE)),""String_Node_Str"",DatasetProperties.EMPTY,DatasetDefinition.NO_ARGUMENTS,null);
    return new AppMDS(new MetadataStoreDataset(table));
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE),e);
    throw Throwables.propagate(e);
  }
}","@Override public AppMDS get(){
  try {
    Table table=DatasetsUtil.getOrCreateDataset(dsFramework,Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE)),""String_Node_Str"",DatasetProperties.EMPTY,DatasetDefinition.NO_ARGUMENTS,null);
    return new AppMDS(new AppMetadataStoreDataset(table));
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",Joiner.on(""String_Node_Str"").join(Constants.SYSTEM_NAMESPACE,DefaultStore.APP_META_TABLE),e);
    throw Throwables.propagate(e);
  }
}"
7241,"/** 
 * Adds a partition to the Hive table for the given dataset.
 * @param datasetID the ID of the dataset to add a partition to
 * @param partitionKey the partition key to add
 * @param fsPath the path of the partition
 * @return the query handle for disabling the dataset
 * @throws ExploreException if there was an exception adding the partition
 * @throws SQLException if there was a problem with the add partition statement
 */
public QueryHandle addPartition(Id.DatasetInstance datasetID,PartitionKey partitionKey,String fsPath) throws ExploreException, SQLException {
  String addPartitionStatement=String.format(""String_Node_Str"",getDatasetTableName(datasetID),generateHivePartitionKey(partitionKey),fsPath);
  LOG.debug(""String_Node_Str"",partitionKey,datasetID,addPartitionStatement);
  return exploreService.execute(datasetID.getNamespace(),addPartitionStatement);
}","/** 
 * Adds a partition to the Hive table for the given dataset.
 * @param datasetID the ID of the dataset to add a partition to
 * @param partitionKey the partition key to add
 * @param fsPath the path of the partition
 * @return the query handle for adding the partition the dataset
 * @throws ExploreException if there was an exception adding the partition
 * @throws SQLException if there was a problem with the add partition statement
 */
public QueryHandle addPartition(Id.DatasetInstance datasetID,PartitionKey partitionKey,String fsPath) throws ExploreException, SQLException {
  String addPartitionStatement=String.format(""String_Node_Str"",getDatasetTableName(datasetID),generateHivePartitionKey(partitionKey),fsPath);
  LOG.debug(""String_Node_Str"",partitionKey,datasetID,addPartitionStatement);
  return exploreService.execute(datasetID.getNamespace(),addPartitionStatement);
}"
7242,"/** 
 * Enable ad-hoc exploration on the given dataset by creating a corresponding Hive table. If exploration has already been enabled on the dataset, this will be a no-op. Assumes the dataset actually exists.
 * @param datasetID the ID of the dataset to enable
 * @param spec the specification for the dataset to enable
 * @return query handle for creating the Hive table for the dataset
 * @throws IllegalArgumentException if some required dataset property like schema is not set
 * @throws UnsupportedTypeException if the schema of the dataset is not compatible with Hive
 * @throws ExploreException if there was an exception submitting the create table statement
 * @throws SQLException if there was a problem with the create table statement
 */
public QueryHandle enableDataset(Id.DatasetInstance datasetID,DatasetSpecification spec) throws IllegalArgumentException, ExploreException, SQLException, UnsupportedTypeException {
  String datasetName=datasetID.getId();
  Map<String,String> serdeProperties=ImmutableMap.of(Constants.Explore.DATASET_NAME,datasetName,Constants.Explore.DATASET_NAMESPACE,datasetID.getNamespaceId());
  String createStatement=null;
  String datasetType=spec.getType();
  if (ObjectMappedTableModule.FULL_NAME.equals(datasetType) || ObjectMappedTableModule.SHORT_NAME.equals(datasetType)) {
    String schemaStr=spec.getProperty(DatasetProperties.SCHEMA);
    if (schemaStr == null) {
      throw new IllegalArgumentException(""String_Node_Str"" + datasetID);
    }
    try {
      Schema schema=Schema.parseJson(schemaStr);
      createStatement=new CreateStatementBuilder(datasetName,getHiveTableName(datasetName)).setSchema(schema).setTableComment(""String_Node_Str"").buildWithStorageHandler(Constants.Explore.DATASET_STORAGE_HANDLER_CLASS,serdeProperties);
      return exploreService.execute(datasetID.getNamespace(),createStatement);
    }
 catch (    IOException e) {
      throw new IllegalArgumentException(""String_Node_Str"" + datasetID);
    }
  }
  Dataset dataset;
  try {
    dataset=instantiateDataset(datasetID);
    if (dataset == null) {
      return QueryHandle.NO_OP;
    }
  }
 catch (  Exception e) {
    throw new ExploreException(""String_Node_Str"" + datasetID,e);
  }
  if (dataset instanceof RecordScannable || dataset instanceof RecordWritable) {
    LOG.debug(""String_Node_Str"",datasetName);
    createStatement=new CreateStatementBuilder(datasetName,getDatasetTableName(datasetID)).setSchema(hiveSchemaFor(dataset)).setTableComment(""String_Node_Str"").buildWithStorageHandler(Constants.Explore.DATASET_STORAGE_HANDLER_CLASS,serdeProperties);
  }
 else   if (dataset instanceof FileSet || dataset instanceof PartitionedFileSet) {
    Map<String,String> properties=spec.getProperties();
    if (FileSetProperties.isExploreEnabled(properties)) {
      LOG.debug(""String_Node_Str"",datasetName);
      createStatement=generateFileSetCreateStatement(datasetID,dataset,properties);
    }
  }
  return exploreService.execute(datasetID.getNamespace(),createStatement);
}","/** 
 * Enable ad-hoc exploration on the given dataset by creating a corresponding Hive table. If exploration has already been enabled on the dataset, this will be a no-op. Assumes the dataset actually exists.
 * @param datasetID the ID of the dataset to enable
 * @param spec the specification for the dataset to enable
 * @return query handle for creating the Hive table for the dataset
 * @throws IllegalArgumentException if some required dataset property like schema is not set
 * @throws UnsupportedTypeException if the schema of the dataset is not compatible with Hive
 * @throws ExploreException if there was an exception submitting the create table statement
 * @throws SQLException if there was a problem with the create table statement
 * @throws DatasetNotFoundException if the dataset had to be instantiated, but could not be found
 */
public QueryHandle enableDataset(Id.DatasetInstance datasetID,DatasetSpecification spec) throws IllegalArgumentException, ExploreException, SQLException, UnsupportedTypeException, DatasetNotFoundException {
  String datasetName=datasetID.getId();
  Map<String,String> serdeProperties=ImmutableMap.of(Constants.Explore.DATASET_NAME,datasetName,Constants.Explore.DATASET_NAMESPACE,datasetID.getNamespaceId());
  String createStatement=null;
  String datasetType=spec.getType();
  if (ObjectMappedTableModule.FULL_NAME.equals(datasetType) || ObjectMappedTableModule.SHORT_NAME.equals(datasetType)) {
    String schemaStr=spec.getProperty(DatasetProperties.SCHEMA);
    if (schemaStr == null) {
      throw new IllegalArgumentException(""String_Node_Str"" + datasetID);
    }
    try {
      Schema schema=Schema.parseJson(schemaStr);
      createStatement=new CreateStatementBuilder(datasetName,getHiveTableName(datasetName)).setSchema(schema).setTableComment(""String_Node_Str"").buildWithStorageHandler(Constants.Explore.DATASET_STORAGE_HANDLER_CLASS,serdeProperties);
      return exploreService.execute(datasetID.getNamespace(),createStatement);
    }
 catch (    IOException e) {
      throw new IllegalArgumentException(""String_Node_Str"" + datasetID);
    }
  }
  Dataset dataset=ExploreServiceUtils.instantiateDataset(datasetFramework,datasetID);
  if (dataset instanceof RecordScannable || dataset instanceof RecordWritable) {
    LOG.debug(""String_Node_Str"",datasetName);
    createStatement=new CreateStatementBuilder(datasetName,getDatasetTableName(datasetID)).setSchema(hiveSchemaFor(dataset)).setTableComment(""String_Node_Str"").buildWithStorageHandler(Constants.Explore.DATASET_STORAGE_HANDLER_CLASS,serdeProperties);
  }
 else   if (dataset instanceof FileSet || dataset instanceof PartitionedFileSet) {
    Map<String,String> properties=spec.getProperties();
    if (FileSetProperties.isExploreEnabled(properties)) {
      LOG.debug(""String_Node_Str"",datasetName);
      createStatement=generateFileSetCreateStatement(datasetID,dataset,properties);
    }
  }
  return exploreService.execute(datasetID.getNamespace(),createStatement);
}"
7243,"/** 
 * Disable exploration on the given dataset by dropping the Hive table for the dataset.
 * @param datasetID the ID of the dataset to disable
 * @param spec the specification for the dataset to disable
 * @return the query handle for disabling the dataset
 * @throws ExploreException if there was an exception dropping the table
 * @throws SQLException if there was a problem with the drop table statement
 */
public QueryHandle disableDataset(Id.DatasetInstance datasetID,DatasetSpecification spec) throws ExploreException, SQLException {
  LOG.debug(""String_Node_Str"",datasetID);
  String tableName=getDatasetTableName(datasetID);
  try {
    exploreService.getTableInfo(datasetID.getNamespaceId(),tableName);
  }
 catch (  TableNotFoundException e) {
    return QueryHandle.NO_OP;
  }
  String deleteStatement=null;
  String datasetType=spec.getType();
  if (ObjectMappedTableModule.FULL_NAME.equals(datasetType) || ObjectMappedTableModule.SHORT_NAME.equals(datasetType)) {
    deleteStatement=generateDeleteStatement(tableName);
    LOG.debug(""String_Node_Str"",datasetID,deleteStatement);
    return exploreService.execute(datasetID.getNamespace(),deleteStatement);
  }
  Dataset dataset;
  try {
    dataset=instantiateDataset(datasetID);
    if (dataset == null) {
      return QueryHandle.NO_OP;
    }
  }
 catch (  Exception e) {
    throw new ExploreException(""String_Node_Str"" + datasetID,e);
  }
  if (dataset instanceof RecordScannable || dataset instanceof RecordWritable) {
    deleteStatement=generateDeleteStatement(tableName);
  }
 else   if (dataset instanceof FileSet || dataset instanceof PartitionedFileSet) {
    Map<String,String> properties=spec.getProperties();
    if (FileSetProperties.isExploreEnabled(properties)) {
      deleteStatement=generateDeleteStatement(tableName);
    }
  }
  if (deleteStatement != null) {
    LOG.debug(""String_Node_Str"",datasetID,deleteStatement);
    return exploreService.execute(datasetID.getNamespace(),deleteStatement);
  }
 else {
    return QueryHandle.NO_OP;
  }
}","/** 
 * Disable exploration on the given dataset by dropping the Hive table for the dataset.
 * @param datasetID the ID of the dataset to disable
 * @param spec the specification for the dataset to disable
 * @return the query handle for disabling the dataset
 * @throws ExploreException if there was an exception dropping the table
 * @throws SQLException if there was a problem with the drop table statement
 * @throws DatasetNotFoundException if the dataset had to be instantiated, but could not be found
 */
public QueryHandle disableDataset(Id.DatasetInstance datasetID,DatasetSpecification spec) throws ExploreException, SQLException, DatasetNotFoundException {
  LOG.debug(""String_Node_Str"",datasetID);
  String tableName=getDatasetTableName(datasetID);
  try {
    exploreService.getTableInfo(datasetID.getNamespaceId(),tableName);
  }
 catch (  TableNotFoundException e) {
    return QueryHandle.NO_OP;
  }
  String deleteStatement=null;
  String datasetType=spec.getType();
  if (ObjectMappedTableModule.FULL_NAME.equals(datasetType) || ObjectMappedTableModule.SHORT_NAME.equals(datasetType)) {
    deleteStatement=generateDeleteStatement(tableName);
    LOG.debug(""String_Node_Str"",datasetID,deleteStatement);
    return exploreService.execute(datasetID.getNamespace(),deleteStatement);
  }
  Dataset dataset=ExploreServiceUtils.instantiateDataset(datasetFramework,datasetID);
  if (dataset instanceof RecordScannable || dataset instanceof RecordWritable) {
    deleteStatement=generateDeleteStatement(tableName);
  }
 else   if (dataset instanceof FileSet || dataset instanceof PartitionedFileSet) {
    Map<String,String> properties=spec.getProperties();
    if (FileSetProperties.isExploreEnabled(properties)) {
      deleteStatement=generateDeleteStatement(tableName);
    }
  }
  if (deleteStatement != null) {
    LOG.debug(""String_Node_Str"",datasetID,deleteStatement);
    return exploreService.execute(datasetID.getNamespace(),deleteStatement);
  }
 else {
    return QueryHandle.NO_OP;
  }
}"
7244,"/** 
 * Drop a partition from the Hive table for the given dataset.
 * @param datasetID the ID of the dataset to drop the partition from
 * @param partitionKey the partition key to drop
 * @return the query handle for disabling the dataset
 * @throws ExploreException if there was an exception dropping the partition
 * @throws SQLException if there was a problem with the drop partition statement
 */
public QueryHandle dropPartition(Id.DatasetInstance datasetID,PartitionKey partitionKey) throws ExploreException, SQLException {
  String dropPartitionStatement=String.format(""String_Node_Str"",getDatasetTableName(datasetID),generateHivePartitionKey(partitionKey));
  LOG.debug(""String_Node_Str"",partitionKey,datasetID,dropPartitionStatement);
  return exploreService.execute(datasetID.getNamespace(),dropPartitionStatement);
}","/** 
 * Drop a partition from the Hive table for the given dataset.
 * @param datasetID the ID of the dataset to drop the partition from
 * @param partitionKey the partition key to drop
 * @return the query handle for dropping the partition from the dataset
 * @throws ExploreException if there was an exception dropping the partition
 * @throws SQLException if there was a problem with the drop partition statement
 */
public QueryHandle dropPartition(Id.DatasetInstance datasetID,PartitionKey partitionKey) throws ExploreException, SQLException {
  String dropPartitionStatement=String.format(""String_Node_Str"",getDatasetTableName(datasetID),generateHivePartitionKey(partitionKey));
  LOG.debug(""String_Node_Str"",partitionKey,datasetID,dropPartitionStatement);
  return exploreService.execute(datasetID.getNamespace(),dropPartitionStatement);
}"
7245,"private void upgradeStreamTable(TableInfo tableInfo) throws Exception {
  Map<String,String> serdeProperties=tableInfo.getSerdeParameters();
  String streamName=serdeProperties.get(Constants.Explore.STREAM_NAME);
  Id.Stream streamID=Id.Stream.from(Constants.DEFAULT_NAMESPACE_ID,streamName);
  LOG.info(""String_Node_Str"",streamID);
  QueryHandle enableHandle=exploreTableService.enableStream(streamID,streamAdmin.getConfig(streamID));
  QueryStatus status=waitForCompletion(enableHandle);
  if (status.getStatus() != QueryStatus.OpStatus.FINISHED) {
    throw new ExploreException(""String_Node_Str"" + streamID);
  }
  LOG.info(""String_Node_Str"",streamID);
  QueryHandle disableHandle=execute(""String_Node_Str"",""String_Node_Str"" + streamName);
  status=waitForCompletion(disableHandle);
  if (status.getStatus() != QueryStatus.OpStatus.FINISHED) {
    throw new ExploreException(""String_Node_Str"" + streamName);
  }
}","private void upgradeStreamTable(TableInfo tableInfo) throws Exception {
  Map<String,String> serdeProperties=tableInfo.getSerdeParameters();
  String streamName=serdeProperties.get(Constants.Explore.STREAM_NAME);
  Id.Stream streamID=Id.Stream.from(Constants.DEFAULT_NAMESPACE_ID,streamName);
  LOG.info(""String_Node_Str"",streamID);
  QueryHandle enableHandle=exploreTableService.enableStream(streamID,streamAdmin.getConfig(streamID));
  QueryStatus status=waitForCompletion(enableHandle);
  if (status.getStatus() != QueryStatus.OpStatus.FINISHED) {
    throw new ExploreException(""String_Node_Str"" + streamID);
  }
  dropTable(tableInfo.getTableName());
}"
7246,"private void upgradeRecordScannableTable(TableInfo tableInfo) throws Exception {
  Map<String,String> serdeProperties=tableInfo.getSerdeParameters();
  String datasetName=serdeProperties.get(Constants.Explore.DATASET_NAME);
  Id.DatasetInstance datasetID=Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,datasetName);
  DatasetSpecification spec=datasetFramework.getDatasetSpec(datasetID);
  LOG.info(""String_Node_Str"",datasetID);
  QueryHandle enableHandle=exploreTableService.enableDataset(datasetID,spec);
  QueryStatus status=waitForCompletion(enableHandle);
  if (status.getStatus() != QueryStatus.OpStatus.FINISHED) {
    throw new ExploreException(""String_Node_Str"" + datasetID);
  }
  String oldTable=tableInfo.getTableName();
  LOG.info(""String_Node_Str"",oldTable);
  QueryHandle disableHandle=execute(""String_Node_Str"",""String_Node_Str"" + oldTable);
  status=waitForCompletion(disableHandle);
  if (status.getStatus() != QueryStatus.OpStatus.FINISHED) {
    throw new ExploreException(""String_Node_Str"" + oldTable);
  }
}","private void upgradeRecordScannableTable(TableInfo tableInfo) throws Exception {
  Map<String,String> serdeProperties=tableInfo.getSerdeParameters();
  String datasetName=serdeProperties.get(Constants.Explore.DATASET_NAME);
  datasetName=datasetName.substring(""String_Node_Str"".length(),datasetName.length());
  Id.DatasetInstance datasetID=Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,datasetName);
  DatasetSpecification spec=datasetFramework.getDatasetSpec(datasetID);
  enableDataset(datasetID,spec);
  dropTable(tableInfo.getTableName());
}"
7247,"private void upgradeFilesetTable(TableInfo tableInfo) throws Exception {
  String oldName=tableInfo.getTableName();
  String newName=""String_Node_Str"" + oldName.substring(""String_Node_Str"".length(),oldName.length());
  QueryHandle renameHandle=execute(""String_Node_Str"",String.format(""String_Node_Str"",oldName,newName));
  QueryStatus status=waitForCompletion(renameHandle);
  if (status.getStatus() != QueryStatus.OpStatus.FINISHED) {
    throw new ExploreException(String.format(""String_Node_Str"",oldName,newName));
  }
  Map<String,String> tblProperties=tableInfo.getParameters();
  String cdapName=tblProperties.get(Constants.Explore.CDAP_NAME);
  QueryHandle propertyHandle=execute(""String_Node_Str"",String.format(""String_Node_Str"",newName,Constants.Explore.CDAP_NAME,cdapName,Constants.Explore.CDAP_VERSION,ProjectInfo.getVersion().toString()));
  status=waitForCompletion(propertyHandle);
  if (status.getStatus() != QueryStatus.OpStatus.FINISHED) {
    throw new ExploreException(String.format(""String_Node_Str"",newName));
  }
}","private void upgradeFilesetTable(TableInfo tableInfo) throws Exception {
  String dsName=tableInfo.getParameters().get(Constants.Explore.CDAP_NAME);
  dsName=dsName.substring(""String_Node_Str"".length(),dsName.length());
  Id.DatasetInstance datasetID=Id.DatasetInstance.from(Constants.DEFAULT_NAMESPACE_ID,dsName);
  DatasetSpecification spec=datasetFramework.getDatasetSpec(datasetID);
  enableDataset(datasetID,spec);
  Dataset dataset=ExploreServiceUtils.instantiateDataset(datasetFramework,datasetID);
  if (dataset instanceof TimePartitionedFileSet) {
    TimePartitionedFileSet tpfs=(TimePartitionedFileSet)dataset;
    Map<PartitionKey,String> partitions=tpfs.getPartitions(null);
    if (!partitions.isEmpty()) {
      QueryHandle handle=exploreTableService.addPartitions(datasetID,tpfs.getPartitions(null));
      QueryStatus status=waitForCompletion(handle);
      if (status.getStatus() != QueryStatus.OpStatus.FINISHED) {
        throw new ExploreException(""String_Node_Str"" + datasetID);
      }
    }
  }
  dropTable(tableInfo.getTableName());
}"
7248,"public CLIConfig(){
  this(null,System.out,new AltStyleTableRenderer());
}","public CLIConfig(){
  this(ClientConfig.builder().build(),System.out,new AltStyleTableRenderer());
}"
7249,"public CLIMain(final LaunchOptions options,final CLIConfig cliConfig) throws URISyntaxException, IOException {
  this.options=options;
  this.cliConfig=cliConfig;
  final PrintStream output=cliConfig.getOutput();
  final TableRenderer tableRenderer=cliConfig.getTableRenderer();
  cliConfig.getClientConfig().setVerifySSLCert(options.isVerifySSL());
  Injector injector=Guice.createInjector(new AbstractModule(){
    @Override protected void configure(){
      bind(CConfiguration.class).toInstance(CConfiguration.create());
      bind(PrintStream.class).toInstance(output);
      bind(CLIConfig.class).toInstance(cliConfig);
      bind(ClientConfig.class).toInstance(cliConfig.getClientConfig());
      bind(TableRenderer.class).toInstance(tableRenderer);
    }
  }
);
  InstanceURIParser instanceURIParser=injector.getInstance(InstanceURIParser.class);
  if (options.isAutoconnect()) {
    try {
      CLIConfig.ConnectionInfo connectionInfo=instanceURIParser.parse(options.getUri());
      cliConfig.tryConnect(connectionInfo,output,options.isDebug());
      cliConfig.getClientConfig().setHostname(connectionInfo.getHostname());
      cliConfig.getClientConfig().setPort(connectionInfo.getPort());
      cliConfig.getClientConfig().setSSLEnabled(connectionInfo.isSSLEnabled());
      cliConfig.getClientConfig().setNamespace(connectionInfo.getNamespace());
    }
 catch (    Exception e) {
      if (options.isDebug()) {
        e.printStackTrace(output);
      }
    }
  }
  this.commands=ImmutableList.of(injector.getInstance(DefaultCommands.class),new CommandSet<Command>(ImmutableList.<Command>of(new HelpCommand(getCommandsSupplier()),new SearchCommandsCommand(getCommandsSupplier()))));
  Map<String,Completer> completers=injector.getInstance(DefaultCompleters.class).get();
  cli=new CLI<Command>(Iterables.concat(commands),completers);
  cli.setExceptionHandler(new CLIExceptionHandler<Exception>(){
    @Override public boolean handleException(    PrintStream output,    Exception e,    int timesRetried){
      if (e instanceof SSLHandshakeException) {
        output.printf(""String_Node_Str"",VERIFY_SSL_OPTION.getLongOpt());
      }
 else       if (e instanceof InvalidCommandException) {
        InvalidCommandException ex=(InvalidCommandException)e;
        output.printf(""String_Node_Str"",ex.getInput());
      }
 else {
        output.println(""String_Node_Str"" + e.getMessage());
      }
      if (options.isDebug()) {
        e.printStackTrace(output);
      }
      return false;
    }
  }
);
  cli.addCompleterSupplier(injector.getInstance(EndpointSupplier.class));
  updateCLIPrompt(cliConfig.getClientConfig());
  cliConfig.addHostnameChangeListener(new CLIConfig.ConnectionChangeListener(){
    @Override public void onConnectionChanged(    ClientConfig clientConfig){
      updateCLIPrompt(clientConfig);
    }
  }
);
}","public CLIMain(final LaunchOptions options,final CLIConfig cliConfig) throws URISyntaxException, IOException {
  this.options=options;
  this.cliConfig=cliConfig;
  final PrintStream output=cliConfig.getOutput();
  final TableRenderer tableRenderer=cliConfig.getTableRenderer();
  cliConfig.getClientConfig().setVerifySSLCert(options.isVerifySSL());
  Injector injector=Guice.createInjector(new AbstractModule(){
    @Override protected void configure(){
      bind(LaunchOptions.class).toInstance(options);
      bind(CConfiguration.class).toInstance(CConfiguration.create());
      bind(PrintStream.class).toInstance(output);
      bind(CLIConfig.class).toInstance(cliConfig);
      bind(ClientConfig.class).toInstance(cliConfig.getClientConfig());
      bind(TableRenderer.class).toInstance(tableRenderer);
    }
  }
);
  InstanceURIParser instanceURIParser=injector.getInstance(InstanceURIParser.class);
  if (options.isAutoconnect()) {
    try {
      CLIConfig.ConnectionInfo connectionInfo=instanceURIParser.parse(options.getUri());
      cliConfig.tryConnect(connectionInfo,output,options.isDebug());
      cliConfig.getClientConfig().setHostname(connectionInfo.getHostname());
      cliConfig.getClientConfig().setPort(connectionInfo.getPort());
      cliConfig.getClientConfig().setSSLEnabled(connectionInfo.isSSLEnabled());
      cliConfig.getClientConfig().setNamespace(connectionInfo.getNamespace());
    }
 catch (    Exception e) {
      if (options.isDebug()) {
        e.printStackTrace(output);
      }
    }
  }
  this.commands=ImmutableList.of(injector.getInstance(DefaultCommands.class),new CommandSet<Command>(ImmutableList.<Command>of(new HelpCommand(getCommandsSupplier()),new SearchCommandsCommand(getCommandsSupplier()))));
  Map<String,Completer> completers=injector.getInstance(DefaultCompleters.class).get();
  cli=new CLI<Command>(Iterables.concat(commands),completers);
  cli.setExceptionHandler(new CLIExceptionHandler<Exception>(){
    @Override public boolean handleException(    PrintStream output,    Exception e,    int timesRetried){
      if (e instanceof SSLHandshakeException) {
        output.printf(""String_Node_Str"",VERIFY_SSL_OPTION.getLongOpt());
      }
 else       if (e instanceof InvalidCommandException) {
        InvalidCommandException ex=(InvalidCommandException)e;
        output.printf(""String_Node_Str"",ex.getInput());
      }
 else {
        output.println(""String_Node_Str"" + e.getMessage());
      }
      if (options.isDebug()) {
        e.printStackTrace(output);
      }
      return false;
    }
  }
);
  cli.addCompleterSupplier(injector.getInstance(EndpointSupplier.class));
  updateCLIPrompt(cliConfig.getClientConfig());
  cliConfig.addHostnameChangeListener(new CLIConfig.ConnectionChangeListener(){
    @Override public void onConnectionChanged(    ClientConfig clientConfig){
      updateCLIPrompt(clientConfig);
    }
  }
);
}"
7250,"@Override protected void configure(){
  bind(CConfiguration.class).toInstance(CConfiguration.create());
  bind(PrintStream.class).toInstance(output);
  bind(CLIConfig.class).toInstance(cliConfig);
  bind(ClientConfig.class).toInstance(cliConfig.getClientConfig());
  bind(TableRenderer.class).toInstance(tableRenderer);
}","@Override protected void configure(){
  bind(LaunchOptions.class).toInstance(options);
  bind(CConfiguration.class).toInstance(CConfiguration.create());
  bind(PrintStream.class).toInstance(output);
  bind(CLIConfig.class).toInstance(cliConfig);
  bind(ClientConfig.class).toInstance(cliConfig.getClientConfig());
  bind(TableRenderer.class).toInstance(tableRenderer);
}"
7251,"@Inject public ConnectCommand(CLIConfig cliConfig,InstanceURIParser instanceURIParser,@Named(CLIMain.NAME_DEBUG) final boolean debug){
  this.cliConfig=cliConfig;
  this.instanceURIParser=instanceURIParser;
  this.debug=debug;
}","@Inject public ConnectCommand(CLIConfig cliConfig,InstanceURIParser instanceURIParser,LaunchOptions launchOptions){
  this.cliConfig=cliConfig;
  this.instanceURIParser=instanceURIParser;
  this.debug=launchOptions.isDebug();
}"
7252,"public GenerateCLIDocsTable(final CLIConfig cliConfig) throws URISyntaxException, IOException {
  Injector injector=Guice.createInjector(new AbstractModule(){
    @Override protected void configure(){
      bind(PrintStream.class).toInstance(System.out);
      bind(String.class).annotatedWith(Names.named(CLIMain.NAME_URI)).toInstance(""String_Node_Str"");
      bind(Boolean.class).annotatedWith(Names.named(CLIMain.NAME_VERIFY_SSL)).toInstance(false);
      bind(Boolean.class).annotatedWith(Names.named(CLIMain.NAME_DEBUG)).toInstance(true);
      bind(Boolean.class).annotatedWith(Names.named(CLIMain.NAME_AUTOCONNECT)).toInstance(true);
      bind(CLIConfig.class).toInstance(cliConfig);
      bind(ClientConfig.class).toInstance(cliConfig.getClientConfig());
      bind(CConfiguration.class).toInstance(CConfiguration.create());
      bind(TableRenderer.class).to(CsvTableRenderer.class);
    }
  }
);
  this.printDocsCommand=new GenerateCLIDocsTableCommand(injector.getInstance(DefaultCommands.class));
}","public GenerateCLIDocsTable(final CLIConfig cliConfig) throws URISyntaxException, IOException {
  Injector injector=Guice.createInjector(new AbstractModule(){
    @Override protected void configure(){
      bind(CConfiguration.class).toInstance(CConfiguration.create());
      bind(LaunchOptions.class).toInstance(LaunchOptions.DEFAULT);
      bind(PrintStream.class).toInstance(System.out);
      bind(CLIConfig.class).toInstance(cliConfig);
      bind(ClientConfig.class).toInstance(cliConfig.getClientConfig());
      bind(TableRenderer.class).toInstance(new CsvTableRenderer());
    }
  }
);
  this.printDocsCommand=new GenerateCLIDocsTableCommand(injector.getInstance(DefaultCommands.class));
}"
7253,"@Override protected void configure(){
  bind(PrintStream.class).toInstance(System.out);
  bind(String.class).annotatedWith(Names.named(CLIMain.NAME_URI)).toInstance(""String_Node_Str"");
  bind(Boolean.class).annotatedWith(Names.named(CLIMain.NAME_VERIFY_SSL)).toInstance(false);
  bind(Boolean.class).annotatedWith(Names.named(CLIMain.NAME_DEBUG)).toInstance(true);
  bind(Boolean.class).annotatedWith(Names.named(CLIMain.NAME_AUTOCONNECT)).toInstance(true);
  bind(CLIConfig.class).toInstance(cliConfig);
  bind(ClientConfig.class).toInstance(cliConfig.getClientConfig());
  bind(CConfiguration.class).toInstance(CConfiguration.create());
  bind(TableRenderer.class).to(CsvTableRenderer.class);
}","@Override protected void configure(){
  bind(CConfiguration.class).toInstance(CConfiguration.create());
  bind(LaunchOptions.class).toInstance(LaunchOptions.DEFAULT);
  bind(PrintStream.class).toInstance(System.out);
  bind(CLIConfig.class).toInstance(cliConfig);
  bind(ClientConfig.class).toInstance(cliConfig.getClientConfig());
  bind(TableRenderer.class).toInstance(new CsvTableRenderer());
}"
7254,"public CLIMain(final LaunchOptions options,final CLIConfig cliConfig) throws URISyntaxException, IOException {
  this.options=options;
  this.cliConfig=cliConfig;
  final PrintStream output=cliConfig.getOutput();
  final TableRenderer tableRenderer=cliConfig.getTableRenderer();
  cliConfig.getClientConfig().setVerifySSLCert(options.isVerifySSL());
  Injector injector=Guice.createInjector(new AbstractModule(){
    @Override protected void configure(){
      bind(CConfiguration.class).toInstance(CConfiguration.create());
      bind(PrintStream.class).toInstance(output);
      bind(CLIConfig.class).toInstance(cliConfig);
      bind(ClientConfig.class).toInstance(cliConfig.getClientConfig());
      bind(TableRenderer.class).toInstance(tableRenderer);
    }
  }
);
  InstanceURIParser instanceURIParser=injector.getInstance(InstanceURIParser.class);
  if (options.isAutoconnect()) {
    try {
      CLIConfig.ConnectionInfo connectionInfo=instanceURIParser.parse(options.getUri());
      cliConfig.tryConnect(connectionInfo,output,options.isDebug());
      cliConfig.getClientConfig().setHostname(connectionInfo.getHostname());
      cliConfig.getClientConfig().setPort(connectionInfo.getPort());
      cliConfig.getClientConfig().setSSLEnabled(connectionInfo.isSSLEnabled());
      cliConfig.getClientConfig().setNamespace(connectionInfo.getNamespace());
    }
 catch (    Exception e) {
      if (options.isDebug()) {
        e.printStackTrace(output);
      }
    }
  }
  this.commands=ImmutableList.of(injector.getInstance(DefaultCommands.class),new CommandSet<Command>(ImmutableList.<Command>of(new HelpCommand(getCommandsSupplier()),new SearchCommandsCommand(getCommandsSupplier()))));
  Map<String,Completer> completers=injector.getInstance(DefaultCompleters.class).get();
  cli=new CLI<Command>(Iterables.concat(commands),completers);
  cli.setExceptionHandler(new CLIExceptionHandler<Exception>(){
    @Override public boolean handleException(    PrintStream output,    Exception e,    int timesRetried){
      if (e instanceof SSLHandshakeException) {
        output.printf(""String_Node_Str"",CLIConfig.PROP_VERIFY_SSL_CERT);
      }
 else       if (e instanceof InvalidCommandException) {
        InvalidCommandException ex=(InvalidCommandException)e;
        output.printf(""String_Node_Str"",ex.getInput());
      }
 else {
        output.println(""String_Node_Str"" + e.getMessage());
      }
      if (options.isDebug()) {
        e.printStackTrace(output);
      }
      return false;
    }
  }
);
  cli.addCompleterSupplier(injector.getInstance(EndpointSupplier.class));
  updateCLIPrompt(cliConfig.getClientConfig());
  cliConfig.addHostnameChangeListener(new CLIConfig.ConnectionChangeListener(){
    @Override public void onConnectionChanged(    ClientConfig clientConfig){
      updateCLIPrompt(clientConfig);
    }
  }
);
}","public CLIMain(final LaunchOptions options,final CLIConfig cliConfig) throws URISyntaxException, IOException {
  this.options=options;
  this.cliConfig=cliConfig;
  final PrintStream output=cliConfig.getOutput();
  final TableRenderer tableRenderer=cliConfig.getTableRenderer();
  cliConfig.getClientConfig().setVerifySSLCert(options.isVerifySSL());
  Injector injector=Guice.createInjector(new AbstractModule(){
    @Override protected void configure(){
      bind(CConfiguration.class).toInstance(CConfiguration.create());
      bind(PrintStream.class).toInstance(output);
      bind(CLIConfig.class).toInstance(cliConfig);
      bind(ClientConfig.class).toInstance(cliConfig.getClientConfig());
      bind(TableRenderer.class).toInstance(tableRenderer);
    }
  }
);
  InstanceURIParser instanceURIParser=injector.getInstance(InstanceURIParser.class);
  if (options.isAutoconnect()) {
    try {
      CLIConfig.ConnectionInfo connectionInfo=instanceURIParser.parse(options.getUri());
      cliConfig.tryConnect(connectionInfo,output,options.isDebug());
      cliConfig.getClientConfig().setHostname(connectionInfo.getHostname());
      cliConfig.getClientConfig().setPort(connectionInfo.getPort());
      cliConfig.getClientConfig().setSSLEnabled(connectionInfo.isSSLEnabled());
      cliConfig.getClientConfig().setNamespace(connectionInfo.getNamespace());
    }
 catch (    Exception e) {
      if (options.isDebug()) {
        e.printStackTrace(output);
      }
    }
  }
  this.commands=ImmutableList.of(injector.getInstance(DefaultCommands.class),new CommandSet<Command>(ImmutableList.<Command>of(new HelpCommand(getCommandsSupplier()),new SearchCommandsCommand(getCommandsSupplier()))));
  Map<String,Completer> completers=injector.getInstance(DefaultCompleters.class).get();
  cli=new CLI<Command>(Iterables.concat(commands),completers);
  cli.setExceptionHandler(new CLIExceptionHandler<Exception>(){
    @Override public boolean handleException(    PrintStream output,    Exception e,    int timesRetried){
      if (e instanceof SSLHandshakeException) {
        output.printf(""String_Node_Str"",VERIFY_SSL_OPTION.getLongOpt());
      }
 else       if (e instanceof InvalidCommandException) {
        InvalidCommandException ex=(InvalidCommandException)e;
        output.printf(""String_Node_Str"",ex.getInput());
      }
 else {
        output.println(""String_Node_Str"" + e.getMessage());
      }
      if (options.isDebug()) {
        e.printStackTrace(output);
      }
      return false;
    }
  }
);
  cli.addCompleterSupplier(injector.getInstance(EndpointSupplier.class));
  updateCLIPrompt(cliConfig.getClientConfig());
  cliConfig.addHostnameChangeListener(new CLIConfig.ConnectionChangeListener(){
    @Override public void onConnectionChanged(    ClientConfig clientConfig){
      updateCLIPrompt(clientConfig);
    }
  }
);
}"
7255,"@Override public boolean handleException(PrintStream output,Exception e,int timesRetried){
  if (e instanceof SSLHandshakeException) {
    output.printf(""String_Node_Str"",CLIConfig.PROP_VERIFY_SSL_CERT);
  }
 else   if (e instanceof InvalidCommandException) {
    InvalidCommandException ex=(InvalidCommandException)e;
    output.printf(""String_Node_Str"",ex.getInput());
  }
 else {
    output.println(""String_Node_Str"" + e.getMessage());
  }
  if (options.isDebug()) {
    e.printStackTrace(output);
  }
  return false;
}","@Override public boolean handleException(PrintStream output,Exception e,int timesRetried){
  if (e instanceof SSLHandshakeException) {
    output.printf(""String_Node_Str"",VERIFY_SSL_OPTION.getLongOpt());
  }
 else   if (e instanceof InvalidCommandException) {
    InvalidCommandException ex=(InvalidCommandException)e;
    output.printf(""String_Node_Str"",ex.getInput());
  }
 else {
    output.println(""String_Node_Str"" + e.getMessage());
  }
  if (options.isDebug()) {
    e.printStackTrace(output);
  }
  return false;
}"
7256,"@Test public void test() throws IOException {
  DefaultDatasetNamespace dsNamespace=new DefaultDatasetNamespace(CConfiguration.create());
  String name=dsNamespace.namespace(NAMESPACE_ID,""String_Node_Str"");
  DatasetDefinition<? extends NoTxKeyValueTable,? extends DatasetAdmin> def=getDefinition();
  DatasetSpecification spec=def.configure(name,DatasetProperties.EMPTY);
  ClassLoader cl=NoTxKeyValueTable.class.getClassLoader();
  DatasetContext datasetContext=new DatasetContext.Builder().setNamespaceId(namespaceId).build();
  DatasetAdmin admin=def.getAdmin(datasetContext,spec,cl);
  Assert.assertFalse(admin.exists());
  admin.create();
  Assert.assertTrue(admin.exists());
  NoTxKeyValueTable table=def.getDataset(datasetContext,spec,NO_ARGS,cl);
  Assert.assertNull(table.get(KEY1));
  table.put(KEY1,VALUE1);
  Assert.assertArrayEquals(VALUE1,table.get(KEY1));
  Assert.assertNull(table.get(KEY2));
  table.put(KEY1,VALUE2);
  Assert.assertArrayEquals(VALUE2,table.get(KEY1));
  Assert.assertNull(table.get(KEY2));
  table.put(KEY2,VALUE1);
  Assert.assertArrayEquals(VALUE2,table.get(KEY1));
  Assert.assertArrayEquals(VALUE1,table.get(KEY2));
  table.put(KEY2,null);
  Assert.assertNull(table.get(KEY2));
  Assert.assertArrayEquals(VALUE2,table.get(KEY1));
  admin.truncate();
  Assert.assertNull(table.get(KEY1));
  Assert.assertNull(table.get(KEY2));
  Assert.assertTrue(admin.exists());
  admin.drop();
  Assert.assertFalse(admin.exists());
  admin.create();
  Assert.assertTrue(admin.exists());
  Assert.assertNull(table.get(KEY1));
  Assert.assertNull(table.get(KEY2));
  table.put(KEY1,VALUE1);
  Assert.assertArrayEquals(VALUE1,table.get(KEY1));
  admin.drop();
  Assert.assertFalse(admin.exists());
  admin.create();
  Assert.assertTrue(admin.exists());
  Assert.assertNull(table.get(KEY1));
}","@Test public void test() throws IOException {
  DefaultDatasetNamespace dsNamespace=new DefaultDatasetNamespace(CConfiguration.create());
  String name=dsNamespace.namespace(NAMESPACE_ID,""String_Node_Str"");
  DatasetDefinition<? extends NoTxKeyValueTable,? extends DatasetAdmin> def=getDefinition();
  DatasetSpecification spec=def.configure(name,DatasetProperties.EMPTY);
  ClassLoader cl=NoTxKeyValueTable.class.getClassLoader();
  DatasetContext datasetContext=DatasetContext.from(namespaceId);
  DatasetAdmin admin=def.getAdmin(datasetContext,spec,cl);
  Assert.assertFalse(admin.exists());
  admin.create();
  Assert.assertTrue(admin.exists());
  NoTxKeyValueTable table=def.getDataset(datasetContext,spec,NO_ARGS,cl);
  Assert.assertNull(table.get(KEY1));
  table.put(KEY1,VALUE1);
  Assert.assertArrayEquals(VALUE1,table.get(KEY1));
  Assert.assertNull(table.get(KEY2));
  table.put(KEY1,VALUE2);
  Assert.assertArrayEquals(VALUE2,table.get(KEY1));
  Assert.assertNull(table.get(KEY2));
  table.put(KEY2,VALUE1);
  Assert.assertArrayEquals(VALUE2,table.get(KEY1));
  Assert.assertArrayEquals(VALUE1,table.get(KEY2));
  table.put(KEY2,null);
  Assert.assertNull(table.get(KEY2));
  Assert.assertArrayEquals(VALUE2,table.get(KEY1));
  admin.truncate();
  Assert.assertNull(table.get(KEY1));
  Assert.assertNull(table.get(KEY2));
  Assert.assertTrue(admin.exists());
  admin.drop();
  Assert.assertFalse(admin.exists());
  admin.create();
  Assert.assertTrue(admin.exists());
  Assert.assertNull(table.get(KEY1));
  Assert.assertNull(table.get(KEY2));
  table.put(KEY1,VALUE1);
  Assert.assertArrayEquals(VALUE1,table.get(KEY1));
  admin.drop();
  Assert.assertFalse(admin.exists());
  admin.create();
  Assert.assertTrue(admin.exists());
  Assert.assertNull(table.get(KEY1));
}"
7257,"@Test public void testDataset() throws Exception {
  String datasetName=PREFIX + ""String_Node_Str"";
  DatasetTypeClient datasetTypeClient=new DatasetTypeClient(cliConfig.getClientConfig());
  DatasetTypeMeta datasetType=datasetTypeClient.list().get(0);
  testCommandOutputContains(cli,""String_Node_Str"" + datasetType.getName() + ""String_Node_Str""+ datasetName,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",FakeDataset.class.getSimpleName());
  NamespaceClient namespaceClient=new NamespaceClient(cliConfig.getClientConfig());
  namespaceClient.create(new NamespaceMeta.Builder().setId(""String_Node_Str"").build());
  cliConfig.setCurrentNamespace(""String_Node_Str"");
  testCommandOutputNotContains(cli,""String_Node_Str"",FakeDataset.class.getSimpleName());
  testCommandOutputContains(cli,""String_Node_Str"" + datasetType.getName() + ""String_Node_Str""+ datasetName,""String_Node_Str"" + datasetType.getName() + ""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  try {
    testCommandOutputContains(cli,""String_Node_Str"" + datasetName,""String_Node_Str"");
  }
  finally {
    testCommandOutputContains(cli,""String_Node_Str"" + datasetName,""String_Node_Str"");
  }
}","@Test public void testDataset() throws Exception {
  String datasetName=PREFIX + ""String_Node_Str"";
  DatasetTypeClient datasetTypeClient=new DatasetTypeClient(cliConfig.getClientConfig());
  DatasetTypeMeta datasetType=datasetTypeClient.list().get(0);
  testCommandOutputContains(cli,""String_Node_Str"" + datasetType.getName() + ""String_Node_Str""+ datasetName,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",FakeDataset.class.getSimpleName());
  NamespaceClient namespaceClient=new NamespaceClient(cliConfig.getClientConfig());
  Id.Namespace barspace=Id.Namespace.from(""String_Node_Str"");
  namespaceClient.create(new NamespaceMeta.Builder().setId(barspace).build());
  cliConfig.setCurrentNamespace(barspace);
  testCommandOutputNotContains(cli,""String_Node_Str"",FakeDataset.class.getSimpleName());
  testCommandOutputContains(cli,""String_Node_Str"" + datasetType.getName() + ""String_Node_Str""+ datasetName,""String_Node_Str"" + datasetType.getName() + ""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  try {
    testCommandOutputContains(cli,""String_Node_Str"" + datasetName,""String_Node_Str"");
  }
  finally {
    testCommandOutputContains(cli,""String_Node_Str"" + datasetName,""String_Node_Str"");
  }
}"
7258,"@BeforeClass public static void setUpClass() throws Exception {
  if (START_LOCAL_STANDALONE) {
    File adapterDir=TMP_FOLDER.newFolder(""String_Node_Str"");
    configuration=CConfiguration.create();
    configuration.set(Constants.AppFabric.ADAPTER_DIR,adapterDir.getAbsolutePath());
    setupAdapters(adapterDir);
    StandaloneTestBase.setUpClass();
  }
  cliConfig=new CLIConfig(HOSTNAME);
  cliConfig.getClientConfig().setAllTimeouts(60000);
  programClient=new ProgramClient(cliConfig.getClientConfig());
  adapterClient=new AdapterClient(cliConfig.getClientConfig());
  CLIMain cliMain=new CLIMain(cliConfig);
  cli=cliMain.getCLI();
  testCommandOutputContains(cli,""String_Node_Str"" + PROTOCOL + ""String_Node_Str""+ HOSTNAME+ ""String_Node_Str""+ PORT,""String_Node_Str"");
  testCommandOutputNotContains(cli,""String_Node_Str"",FakeApp.NAME);
  File appJarFile=createAppJarFile(FakeApp.class);
  testCommandOutputContains(cli,""String_Node_Str"" + appJarFile.getAbsolutePath(),""String_Node_Str"");
  if (!appJarFile.delete()) {
    LOG.warn(""String_Node_Str"",appJarFile.getAbsolutePath());
  }
}","@BeforeClass public static void setUpClass() throws Exception {
  if (START_LOCAL_STANDALONE) {
    File adapterDir=TMP_FOLDER.newFolder(""String_Node_Str"");
    configuration=CConfiguration.create();
    configuration.set(Constants.AppFabric.ADAPTER_DIR,adapterDir.getAbsolutePath());
    setupAdapters(adapterDir);
    StandaloneTestBase.setUpClass();
  }
  clientConfig=new ClientConfig.Builder().setUri(CONNECTION).build();
  clientConfig.setAllTimeouts(60000);
  cliConfig=new CLIConfig(clientConfig);
  Injector injector=Guice.createInjector(new AbstractModule(){
    @Override protected void configure(){
      bind(PrintStream.class).toInstance(System.out);
      bind(String.class).annotatedWith(Names.named(CLIMain.NAME_NAMESPACE)).toInstance(""String_Node_Str"");
      bind(String.class).annotatedWith(Names.named(CLIMain.NAME_URI)).toInstance(CONNECTION.toString());
      bind(Boolean.class).annotatedWith(Names.named(CLIMain.NAME_VERIFY_SSL)).toInstance(false);
      bind(Boolean.class).annotatedWith(Names.named(CLIMain.NAME_VERBOSE)).toInstance(true);
      bind(Boolean.class).annotatedWith(Names.named(CLIMain.NAME_AUTOCONNECT)).toInstance(true);
      bind(CLIConfig.class).toInstance(cliConfig);
      bind(ClientConfig.class).toInstance(cliConfig.getClientConfig());
      bind(CConfiguration.class).toInstance(CConfiguration.create());
    }
  }
);
  programClient=new ProgramClient(cliConfig.getClientConfig());
  adapterClient=new AdapterClient(cliConfig.getClientConfig());
  CLIMain cliMain=injector.getInstance(CLIMain.class);
  cli=cliMain.getCLI();
  testCommandOutputContains(cli,""String_Node_Str"" + CONNECTION.toString(),""String_Node_Str"");
  testCommandOutputNotContains(cli,""String_Node_Str"",FakeApp.NAME);
  File appJarFile=createAppJarFile(FakeApp.class);
  testCommandOutputContains(cli,""String_Node_Str"" + appJarFile.getAbsolutePath(),""String_Node_Str"");
  if (!appJarFile.delete()) {
    LOG.warn(""String_Node_Str"",appJarFile.getAbsolutePath());
  }
}"
7259,"@Test public void testConnect() throws Exception {
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + HOSTNAME,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + PROTOCOL + ""String_Node_Str""+ HOSTNAME,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + PROTOCOL + ""String_Node_Str""+ HOSTNAME+ ""String_Node_Str""+ PORT,""String_Node_Str"");
}","@Test public void testConnect() throws Exception {
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + CONNECTION.toString(),""String_Node_Str"");
}"
7260,"protected void setPreferences(String[] programIdParts,PrintStream printStream,Map<String,String> args) throws Exception {
switch (type) {
case INSTANCE:
    if (programIdParts.length != 0) {
      throw new CommandInputError(this);
    }
  client.setInstancePreferences(args);
printSuccessMessage(printStream,type);
break;
case NAMESPACE:
if (programIdParts.length != 0) {
throw new CommandInputError(this);
}
client.setNamespacePreferences(cliConfig.getCurrentNamespace(),args);
printSuccessMessage(printStream,type);
break;
case APP:
if (programIdParts.length != 1) {
throw new CommandInputError(this);
}
client.setApplicationPreferences(cliConfig.getCurrentNamespace(),programIdParts[0],args);
printSuccessMessage(printStream,type);
break;
case FLOW:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.setProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1],args);
printSuccessMessage(printStream,type);
break;
case PROCEDURE:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.setProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1],args);
printSuccessMessage(printStream,type);
break;
case MAPREDUCE:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.setProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1],args);
printSuccessMessage(printStream,type);
break;
case WORKFLOW:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.setProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1],args);
printSuccessMessage(printStream,type);
break;
case SERVICE:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.setProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1],args);
printSuccessMessage(printStream,type);
break;
case SPARK:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.setProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1],args);
printSuccessMessage(printStream,type);
break;
default :
throw new IllegalArgumentException(""String_Node_Str"" + type.getPrettyName());
}
}","protected void setPreferences(String[] programIdParts,PrintStream printStream,Map<String,String> args) throws Exception {
switch (type) {
case INSTANCE:
    if (programIdParts.length != 0) {
      throw new CommandInputError(this);
    }
  client.setInstancePreferences(args);
printSuccessMessage(printStream,type);
break;
case NAMESPACE:
if (programIdParts.length != 0) {
throw new CommandInputError(this);
}
client.setNamespacePreferences(cliConfig.getCurrentNamespace(),args);
printSuccessMessage(printStream,type);
break;
case APP:
if (programIdParts.length != 1) {
throw new CommandInputError(this);
}
client.setApplicationPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),args);
printSuccessMessage(printStream,type);
break;
case FLOW:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.setProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1],args);
printSuccessMessage(printStream,type);
break;
case PROCEDURE:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.setProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1],args);
printSuccessMessage(printStream,type);
break;
case MAPREDUCE:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.setProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1],args);
printSuccessMessage(printStream,type);
break;
case WORKFLOW:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.setProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1],args);
printSuccessMessage(printStream,type);
break;
case SERVICE:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.setProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1],args);
printSuccessMessage(printStream,type);
break;
case SPARK:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.setProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1],args);
printSuccessMessage(printStream,type);
break;
default :
throw new IllegalArgumentException(""String_Node_Str"" + type.getPrettyName());
}
}"
7261,"@Override public void execute(Arguments arguments,PrintStream out) throws Exception {
  String namespaceId=arguments.get(ArgumentName.NAMESPACE_ID.toString());
  namespaceClient.delete(namespaceId);
  out.println(String.format(SUCCESS_MSG,namespaceId));
}","@Override public void execute(Arguments arguments,PrintStream out) throws Exception {
  Id.Namespace namespaceId=Id.Namespace.from(arguments.get(ArgumentName.NAMESPACE_ID.toString()));
  namespaceClient.delete(namespaceId);
  out.println(String.format(SUCCESS_MSG,namespaceId));
}"
7262,"@Override public void perform(Arguments arguments,PrintStream printStream) throws Exception {
  String[] programIdParts=new String[0];
  if (arguments.hasArgument(type.getArgumentName().toString())) {
    programIdParts=arguments.get(type.getArgumentName().toString()).split(""String_Node_Str"");
  }
switch (type) {
case INSTANCE:
    if (programIdParts.length != 0) {
      throw new CommandInputError(this);
    }
  client.deleteInstancePreferences();
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case NAMESPACE:
if (programIdParts.length != 0) {
throw new CommandInputError(this);
}
client.deleteNamespacePreferences(cliConfig.getCurrentNamespace());
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case APP:
if (programIdParts.length != 1) {
throw new CommandInputError(this);
}
client.deleteApplicationPreferences(cliConfig.getCurrentNamespace(),programIdParts[0]);
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case FLOW:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.deleteProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1]);
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case PROCEDURE:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.deleteProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1]);
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case MAPREDUCE:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.deleteProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1]);
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case WORKFLOW:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.deleteProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1]);
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case SERVICE:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.deleteProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1]);
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case SPARK:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.deleteProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1]);
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
default :
throw new IllegalArgumentException(""String_Node_Str"" + type.getPrettyName());
}
}","@Override public void perform(Arguments arguments,PrintStream printStream) throws Exception {
  String[] programIdParts=new String[0];
  if (arguments.hasArgument(type.getArgumentName().toString())) {
    programIdParts=arguments.get(type.getArgumentName().toString()).split(""String_Node_Str"");
  }
switch (type) {
case INSTANCE:
    if (programIdParts.length != 0) {
      throw new CommandInputError(this);
    }
  client.deleteInstancePreferences();
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case NAMESPACE:
if (programIdParts.length != 0) {
throw new CommandInputError(this);
}
client.deleteNamespacePreferences(cliConfig.getCurrentNamespace());
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case APP:
if (programIdParts.length != 1) {
throw new CommandInputError(this);
}
client.deleteApplicationPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]));
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case FLOW:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.deleteProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1]);
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case PROCEDURE:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.deleteProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1]);
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case MAPREDUCE:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.deleteProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1]);
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case WORKFLOW:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.deleteProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1]);
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case SERVICE:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.deleteProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1]);
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
case SPARK:
if (programIdParts.length != 2) {
throw new CommandInputError(this);
}
client.deleteProgramPreferences(Id.Application.from(cliConfig.getCurrentNamespace(),programIdParts[0]),type.getPluralName(),programIdParts[1]);
printStream.printf(SUCCESS + ""String_Node_Str"",type.getPrettyName());
break;
default :
throw new IllegalArgumentException(""String_Node_Str"" + type.getPrettyName());
}
}"
7263,"@Override public void execute(Arguments arguments,PrintStream output) throws Exception {
  NamespaceMeta namespaceMeta=namespaceClient.get(arguments.get(ArgumentName.NAMESPACE_ID.getName()));
  new AsciiTable<NamespaceMeta>(new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str""},Lists.newArrayList(namespaceMeta),new RowMaker<NamespaceMeta>(){
    @Override public Object[] makeRow(    NamespaceMeta object){
      return new Object[]{object.getId(),object.getName(),object.getDescription()};
    }
  }
).print(output);
}","@Override public void execute(Arguments arguments,PrintStream output) throws Exception {
  Id.Namespace namespace=Id.Namespace.from(arguments.get(ArgumentName.NAMESPACE_ID.getName()));
  NamespaceMeta namespaceMeta=namespaceClient.get(namespace);
  new AsciiTable<NamespaceMeta>(new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str""},Lists.newArrayList(namespaceMeta),new RowMaker<NamespaceMeta>(){
    @Override public Object[] makeRow(    NamespaceMeta object){
      return new Object[]{object.getId(),object.getName(),object.getDescription()};
    }
  }
).print(output);
}"
7264,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String namespace=arguments.get(ArgumentName.NAMESPACE_ID.toString());
  namespaceClient.get(namespace);
  cliConfig.setCurrentNamespace(Id.Namespace.from(namespace));
  output.printf(""String_Node_Str"",namespace);
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  Id.Namespace namespace=Id.Namespace.from(arguments.get(ArgumentName.NAMESPACE_ID.toString()));
  namespaceClient.get(namespace);
  cliConfig.setCurrentNamespace(namespace);
  output.printf(""String_Node_Str"",namespace);
}"
7265,"private void verifyDoesNotExist(String namespaceId) throws IOException, UnAuthorizedAccessTokenException, CannotBeDeletedException {
  try {
    namespaceClient.get(namespaceId);
    Assert.fail(String.format(""String_Node_Str"",namespaceId));
  }
 catch (  NotFoundException e) {
  }
  try {
    namespaceClient.delete(namespaceId);
    Assert.fail(String.format(""String_Node_Str"",namespaceId));
  }
 catch (  NotFoundException e) {
  }
}","private void verifyDoesNotExist(Id.Namespace namespaceId) throws IOException, UnAuthorizedAccessTokenException, CannotBeDeletedException {
  try {
    namespaceClient.get(namespaceId);
    Assert.fail(String.format(""String_Node_Str"",namespaceId));
  }
 catch (  NotFoundException e) {
  }
  try {
    namespaceClient.delete(namespaceId);
    Assert.fail(String.format(""String_Node_Str"",namespaceId));
  }
 catch (  NotFoundException e) {
  }
}"
7266,"@Test public void testPreferences() throws Exception {
  Id.Namespace invalidNamespace=Id.Namespace.from(""String_Node_Str"");
  namespaceClient.create(new NamespaceMeta.Builder().setId(invalidNamespace.getId()).build());
  Map<String,String> propMap=client.getInstancePreferences();
  Assert.assertEquals(ImmutableMap.<String,String>of(),propMap);
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  client.setInstancePreferences(propMap);
  Assert.assertEquals(propMap,client.getInstancePreferences());
  File jarFile=createAppJarFile(FakeApp.class);
  appClient.deploy(jarFile);
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  client.setNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,propMap);
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,true));
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,false));
  Assert.assertTrue(client.getNamespacePreferences(invalidNamespace,false).isEmpty());
  Assert.assertEquals(""String_Node_Str"",client.getNamespacePreferences(invalidNamespace,true).get(""String_Node_Str""));
  client.deleteNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID);
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,true));
  Assert.assertEquals(ImmutableMap.<String,String>of(),client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,false));
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  client.setNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,propMap);
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,true));
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,false));
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  client.setApplicationPreferences(FakeApp.ID,propMap);
  Assert.assertEquals(propMap,client.getApplicationPreferences(FakeApp.ID,true));
  Assert.assertEquals(propMap,client.getApplicationPreferences(FakeApp.ID,false));
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  client.setProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),propMap);
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),true));
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),false));
  client.deleteProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0));
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),false).isEmpty());
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),true));
  client.deleteApplicationPreferences(FakeApp.ID);
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(client.getApplicationPreferences(FakeApp.ID,false).isEmpty());
  Assert.assertEquals(propMap,client.getApplicationPreferences(FakeApp.ID,true));
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),true));
  client.deleteNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID);
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,false).isEmpty());
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,true));
  Assert.assertEquals(propMap,client.getApplicationPreferences(FakeApp.ID,true));
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),true));
  client.deleteInstancePreferences();
  propMap.clear();
  Assert.assertEquals(propMap,client.getInstancePreferences());
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,true));
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,true));
  Assert.assertEquals(propMap,client.getApplicationPreferences(FakeApp.ID,true));
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),true));
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  client.setApplicationPreferences(FakeApp.ID,propMap);
  Assert.assertEquals(propMap,client.getApplicationPreferences(FakeApp.ID,false));
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  client.setProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),propMap);
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),false));
  appClient.delete(FakeApp.ID.getId());
  appClient.deploy(jarFile);
  propMap.clear();
  Assert.assertEquals(propMap,client.getApplicationPreferences(FakeApp.ID,false));
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),false));
  appClient.delete(FakeApp.ID.getId());
  namespaceClient.delete(invalidNamespace.getId());
}","@Test public void testPreferences() throws Exception {
  Id.Namespace invalidNamespace=Id.Namespace.from(""String_Node_Str"");
  namespaceClient.create(new NamespaceMeta.Builder().setId(invalidNamespace.getId()).build());
  Map<String,String> propMap=client.getInstancePreferences();
  Assert.assertEquals(ImmutableMap.<String,String>of(),propMap);
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  client.setInstancePreferences(propMap);
  Assert.assertEquals(propMap,client.getInstancePreferences());
  File jarFile=createAppJarFile(FakeApp.class);
  appClient.deploy(jarFile);
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  client.setNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,propMap);
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,true));
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,false));
  Assert.assertTrue(client.getNamespacePreferences(invalidNamespace,false).isEmpty());
  Assert.assertEquals(""String_Node_Str"",client.getNamespacePreferences(invalidNamespace,true).get(""String_Node_Str""));
  client.deleteNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID);
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,true));
  Assert.assertEquals(ImmutableMap.<String,String>of(),client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,false));
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  client.setNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,propMap);
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,true));
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,false));
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  client.setApplicationPreferences(FakeApp.ID,propMap);
  Assert.assertEquals(propMap,client.getApplicationPreferences(FakeApp.ID,true));
  Assert.assertEquals(propMap,client.getApplicationPreferences(FakeApp.ID,false));
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  client.setProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),propMap);
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),true));
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),false));
  client.deleteProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0));
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),false).isEmpty());
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),true));
  client.deleteApplicationPreferences(FakeApp.ID);
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(client.getApplicationPreferences(FakeApp.ID,false).isEmpty());
  Assert.assertEquals(propMap,client.getApplicationPreferences(FakeApp.ID,true));
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),true));
  client.deleteNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID);
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,false).isEmpty());
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,true));
  Assert.assertEquals(propMap,client.getApplicationPreferences(FakeApp.ID,true));
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),true));
  client.deleteInstancePreferences();
  propMap.clear();
  Assert.assertEquals(propMap,client.getInstancePreferences());
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,true));
  Assert.assertEquals(propMap,client.getNamespacePreferences(Constants.DEFAULT_NAMESPACE_ID,true));
  Assert.assertEquals(propMap,client.getApplicationPreferences(FakeApp.ID,true));
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),true));
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  client.setApplicationPreferences(FakeApp.ID,propMap);
  Assert.assertEquals(propMap,client.getApplicationPreferences(FakeApp.ID,false));
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  client.setProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),propMap);
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),false));
  appClient.delete(FakeApp.ID.getId());
  appClient.deploy(jarFile);
  propMap.clear();
  Assert.assertEquals(propMap,client.getApplicationPreferences(FakeApp.ID,false));
  Assert.assertEquals(propMap,client.getProgramPreferences(FakeApp.ID,""String_Node_Str"",FakeApp.FLOWS.get(0),false));
  appClient.delete(FakeApp.ID.getId());
  namespaceClient.delete(invalidNamespace);
}"
7267,"@Test public void testDeletingNamespace() throws Exception {
  Map<String,String> propMap=Maps.newHashMap();
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  Id.Namespace myspace=Id.Namespace.from(""String_Node_Str"");
  namespaceClient.create(new NamespaceMeta.Builder().setId(myspace.getId()).build());
  client.setNamespacePreferences(myspace,propMap);
  Assert.assertEquals(propMap,client.getNamespacePreferences(myspace,false));
  Assert.assertEquals(propMap,client.getNamespacePreferences(myspace,true));
  namespaceClient.delete(""String_Node_Str"");
  namespaceClient.create(new NamespaceMeta.Builder().setId(myspace.getId()).build());
  Assert.assertTrue(client.getNamespacePreferences(myspace,false).isEmpty());
  Assert.assertTrue(client.getNamespacePreferences(myspace,true).isEmpty());
  namespaceClient.delete(""String_Node_Str"");
}","@Test public void testDeletingNamespace() throws Exception {
  Map<String,String> propMap=Maps.newHashMap();
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  Id.Namespace myspace=Id.Namespace.from(""String_Node_Str"");
  namespaceClient.create(new NamespaceMeta.Builder().setId(myspace.getId()).build());
  client.setNamespacePreferences(myspace,propMap);
  Assert.assertEquals(propMap,client.getNamespacePreferences(myspace,false));
  Assert.assertEquals(propMap,client.getNamespacePreferences(myspace,true));
  namespaceClient.delete(myspace);
  namespaceClient.create(new NamespaceMeta.Builder().setId(myspace.getId()).build());
  Assert.assertTrue(client.getNamespacePreferences(myspace,false).isEmpty());
  Assert.assertTrue(client.getNamespacePreferences(myspace,true).isEmpty());
  namespaceClient.delete(myspace);
}"
7268,"@Test public void testAll() throws Exception {
  File jarFile=createAppJarFile(FakeApp.class);
  appClient.deploy(jarFile);
  verifyProgramNames(FakeApp.PROCEDURES,procedureClient.list());
  programClient.start(FakeApp.NAME,ProgramType.PROCEDURE,FakeProcedure.NAME);
  assertProgramRunning(programClient,FakeApp.NAME,ProgramType.PROCEDURE,FakeProcedure.NAME);
  String result=procedureClient.call(FakeApp.NAME,FakeProcedure.NAME,FakeProcedure.METHOD_NAME,ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
  Assert.assertEquals(GSON.toJson(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"")),result);
  String testNamespace=clientConfig.getNamespace();
  clientConfig.setNamespace(""String_Node_Str"");
  try {
    procedureClient.call(FakeApp.NAME,FakeProcedure.NAME,FakeProcedure.METHOD_NAME,ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
    Assert.fail(""String_Node_Str"");
  }
 catch (  IllegalStateException e) {
    String expectedErrMsg=""String_Node_Str"";
    Assert.assertEquals(expectedErrMsg,e.getMessage());
  }
  clientConfig.setNamespace(testNamespace);
  String testVersion=clientConfig.getApiVersion();
  clientConfig.setApiVersion(Constants.Gateway.API_VERSION_3_TOKEN);
  try {
    procedureClient.call(FakeApp.NAME,FakeProcedure.NAME,FakeProcedure.METHOD_NAME,ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
    Assert.fail(""String_Node_Str"");
  }
 catch (  IllegalStateException e) {
    String expectedErrMsg=""String_Node_Str"";
    Assert.assertEquals(expectedErrMsg,e.getMessage());
  }
  clientConfig.setApiVersion(testVersion);
  programClient.stop(FakeApp.NAME,ProgramType.PROCEDURE,FakeProcedure.NAME);
  assertProgramStopped(programClient,FakeApp.NAME,ProgramType.PROCEDURE,FakeProcedure.NAME);
  appClient.delete(FakeApp.NAME);
  Assert.assertEquals(0,appClient.list().size());
}","@Test public void testAll() throws Exception {
  File jarFile=createAppJarFile(FakeApp.class);
  appClient.deploy(jarFile);
  verifyProgramNames(FakeApp.PROCEDURES,procedureClient.list());
  programClient.start(FakeApp.NAME,ProgramType.PROCEDURE,FakeProcedure.NAME);
  assertProgramRunning(programClient,FakeApp.NAME,ProgramType.PROCEDURE,FakeProcedure.NAME);
  String result=procedureClient.call(FakeApp.NAME,FakeProcedure.NAME,FakeProcedure.METHOD_NAME,ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
  Assert.assertEquals(GSON.toJson(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"")),result);
  Id.Namespace testNamespace=clientConfig.getNamespace();
  clientConfig.setNamespace(Id.Namespace.from(""String_Node_Str""));
  try {
    procedureClient.call(FakeApp.NAME,FakeProcedure.NAME,FakeProcedure.METHOD_NAME,ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
    Assert.fail(""String_Node_Str"");
  }
 catch (  IllegalStateException e) {
    String expectedErrMsg=""String_Node_Str"";
    Assert.assertEquals(expectedErrMsg,e.getMessage());
  }
  clientConfig.setNamespace(testNamespace);
  String testVersion=clientConfig.getApiVersion();
  clientConfig.setApiVersion(Constants.Gateway.API_VERSION_3_TOKEN);
  try {
    procedureClient.call(FakeApp.NAME,FakeProcedure.NAME,FakeProcedure.METHOD_NAME,ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
    Assert.fail(""String_Node_Str"");
  }
 catch (  IllegalStateException e) {
    String expectedErrMsg=""String_Node_Str"";
    Assert.assertEquals(expectedErrMsg,e.getMessage());
  }
  clientConfig.setApiVersion(testVersion);
  programClient.stop(FakeApp.NAME,ProgramType.PROCEDURE,FakeProcedure.NAME);
  assertProgramStopped(programClient,FakeApp.NAME,ProgramType.PROCEDURE,FakeProcedure.NAME);
  appClient.delete(FakeApp.NAME);
  Assert.assertEquals(0,appClient.list().size());
}"
7269,"@Test public void testAll() throws Exception {
  appClient.deploy(createAppJarFile(FakeApp.class));
  programClient.start(FakeApp.NAME,ProgramType.FLOW,FakeFlow.NAME);
  assertProgramRunning(programClient,FakeApp.NAME,ProgramType.FLOW,FakeFlow.NAME);
  streamClient.sendEvent(FakeApp.STREAM_NAME,""String_Node_Str"");
  streamClient.sendEvent(FakeApp.STREAM_NAME,""String_Node_Str"");
  Thread.sleep(3000);
  String namespace=getClientConfig().getNamespace();
  String instanceName=String.format(""String_Node_Str"",namespace,FakeApp.DS_NAME);
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(namespace,instanceName);
  executeBasicQuery(instanceName);
  exploreClient.disableExploreDataset(datasetInstance).get();
  try {
    queryClient.execute(""String_Node_Str"" + FakeApp.DS_NAME).get();
    Assert.fail(""String_Node_Str"");
  }
 catch (  ExecutionException e) {
  }
  exploreClient.enableExploreDataset(datasetInstance).get();
  executeBasicQuery(instanceName);
}","@Test public void testAll() throws Exception {
  appClient.deploy(createAppJarFile(FakeApp.class));
  programClient.start(FakeApp.NAME,ProgramType.FLOW,FakeFlow.NAME);
  assertProgramRunning(programClient,FakeApp.NAME,ProgramType.FLOW,FakeFlow.NAME);
  streamClient.sendEvent(FakeApp.STREAM_NAME,""String_Node_Str"");
  streamClient.sendEvent(FakeApp.STREAM_NAME,""String_Node_Str"");
  Thread.sleep(3000);
  Id.Namespace namespace=getClientConfig().getNamespace();
  String instanceName=String.format(""String_Node_Str"",namespace,FakeApp.DS_NAME);
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(namespace,instanceName);
  executeBasicQuery(instanceName);
  exploreClient.disableExploreDataset(datasetInstance).get();
  try {
    queryClient.execute(""String_Node_Str"" + FakeApp.DS_NAME).get();
    Assert.fail(""String_Node_Str"");
  }
 catch (  ExecutionException e) {
  }
  exploreClient.enableExploreDataset(datasetInstance).get();
  executeBasicQuery(instanceName);
}"
7270,"/** 
 * Retrieves details about a given namespace.
 * @param namespaceId id of the namespace for which details are requested.
 * @return
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws NotFoundException if the specified namespace is not found
 */
public NamespaceMeta get(String namespaceId) throws IOException, UnAuthorizedAccessTokenException, NotFoundException {
  HttpResponse response=restClient.execute(HttpMethod.GET,config.resolveURLV3(String.format(""String_Node_Str"",namespaceId)),config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (HttpURLConnection.HTTP_NOT_FOUND == response.getResponseCode()) {
    throw new NotFoundException(NAMESPACE_ENTITY_TYPE,namespaceId);
  }
  return ObjectResponse.fromJsonBody(response,new TypeToken<NamespaceMeta>(){
  }
).getResponseObject();
}","/** 
 * Retrieves details about a given namespace.
 * @param namespaceId id of the namespace for which details are requested.
 * @return
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws NotFoundException if the specified namespace is not found
 */
public NamespaceMeta get(Id.Namespace namespaceId) throws IOException, UnAuthorizedAccessTokenException, NotFoundException {
  HttpResponse response=restClient.execute(HttpMethod.GET,config.resolveURLV3(String.format(""String_Node_Str"",namespaceId)),config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (HttpURLConnection.HTTP_NOT_FOUND == response.getResponseCode()) {
    throw new NotFoundException(NAMESPACE_ENTITY_TYPE,namespaceId.getId());
  }
  return ObjectResponse.fromJsonBody(response,new TypeToken<NamespaceMeta>(){
  }
).getResponseObject();
}"
7271,"/** 
 * * Deletes a namespace from CDAP.
 * @param namespaceId id of the namespace to be deleted.
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws NotFoundException if the specified namespace is not found
 * @throws CannotBeDeletedException if the specified namespace is reserved and cannot be deleted
 */
public void delete(String namespaceId) throws IOException, UnAuthorizedAccessTokenException, NotFoundException, CannotBeDeletedException {
  HttpResponse response=restClient.execute(HttpMethod.DELETE,config.resolveURLV3(String.format(""String_Node_Str"",namespaceId)),config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND,HttpURLConnection.HTTP_FORBIDDEN);
  if (HttpURLConnection.HTTP_NOT_FOUND == response.getResponseCode()) {
    throw new NotFoundException(NAMESPACE_ENTITY_TYPE,namespaceId);
  }
  if (HttpURLConnection.HTTP_FORBIDDEN == response.getResponseCode()) {
    throw new CannotBeDeletedException(NAMESPACE_ENTITY_TYPE,namespaceId);
  }
}","/** 
 * * Deletes a namespace from CDAP.
 * @param namespaceId id of the namespace to be deleted.
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws NotFoundException if the specified namespace is not found
 * @throws CannotBeDeletedException if the specified namespace is reserved and cannot be deleted
 */
public void delete(Id.Namespace namespaceId) throws IOException, UnAuthorizedAccessTokenException, NotFoundException, CannotBeDeletedException {
  HttpResponse response=restClient.execute(HttpMethod.DELETE,config.resolveURLV3(String.format(""String_Node_Str"",namespaceId.getId())),config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND,HttpURLConnection.HTTP_FORBIDDEN);
  if (HttpURLConnection.HTTP_NOT_FOUND == response.getResponseCode()) {
    throw new NotFoundException(NAMESPACE_ENTITY_TYPE,namespaceId.getId());
  }
  if (HttpURLConnection.HTTP_FORBIDDEN == response.getResponseCode()) {
    throw new CannotBeDeletedException(NAMESPACE_ENTITY_TYPE,namespaceId.getId());
  }
}"
7272,"FileContentWriter(StreamConfig streamConfig,ConcurrentStreamWriter streamWriter,Location directory,Map<String,String> headers) throws IOException {
  this.streamConfig=streamConfig;
  this.streamWriter=streamWriter;
  this.streamEventData=new MutableStreamEventData();
  this.streamEvent=new MutableStreamEvent();
  directory.mkdirs();
  this.eventFile=directory.append(String.format(""String_Node_Str"",streamConfig.getStreamId()));
  this.indexFile=directory.append(String.format(""String_Node_Str"",streamConfig.getStreamId()));
  Map<String,String> properties=createStreamFileProperties(headers);
  properties.put(StreamDataFileConstants.Property.Key.UNI_TIMESTAMP,StreamDataFileConstants.Property.Value.CLOSE_TIMESTAMP);
  this.writer=new StreamDataFileWriter(Locations.newOutputSupplier(eventFile),Locations.newOutputSupplier(indexFile),streamConfig.getIndexInterval(),properties);
}","FileContentWriter(StreamConfig streamConfig,ConcurrentStreamWriter streamWriter,Location directory,Map<String,String> headers) throws IOException {
  this.streamConfig=streamConfig;
  this.streamWriter=streamWriter;
  this.streamEventData=new MutableStreamEventData();
  this.streamEvent=new MutableStreamEvent();
  directory.mkdirs();
  this.eventFile=directory.append(""String_Node_Str"");
  this.indexFile=directory.append(""String_Node_Str"");
  Map<String,String> properties=createStreamFileProperties(headers);
  properties.put(StreamDataFileConstants.Property.Key.UNI_TIMESTAMP,StreamDataFileConstants.Property.Value.CLOSE_TIMESTAMP);
  this.writer=new StreamDataFileWriter(Locations.newOutputSupplier(eventFile),Locations.newOutputSupplier(indexFile),streamConfig.getIndexInterval(),properties);
}"
7273,"@Test public void testQueryMetrics() throws Exception {
  verifyAggregateQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",1);
  verifyEmptyQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"");
  verifyRangeQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",2,3);
  long start=(emitTs - 60 * 1000) / 1000;
  long end=(emitTs + 60 * 1000) / 1000;
  verifyRangeQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end,2,3);
  verifyEmptyQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end);
  List<TimeSeriesResult> groupByResult=ImmutableList.of(new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),1),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),3));
  verifyGroupByResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ Constants.Metrics.Tag.FLOWLET+ ""String_Node_Str""+ start+ ""String_Node_Str""+ end,groupByResult);
  groupByResult=ImmutableList.of(new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),2),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),1),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),4));
  verifyGroupByResult(""String_Node_Str"" + ""String_Node_Str"" + Constants.Metrics.Tag.NAMESPACE + ""String_Node_Str""+ Constants.Metrics.Tag.FLOWLET+ ""String_Node_Str""+ start+ ""String_Node_Str""+ end,groupByResult);
}","@Test public void testQueryMetrics() throws Exception {
  verifyAggregateQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",3);
  verifyAggregateQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",1);
  verifyAggregateQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",4);
  verifyEmptyQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"");
  verifyRangeQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"",2,3);
  long start=(emitTs - 60 * 1000) / 1000;
  long end=(emitTs + 60 * 1000) / 1000;
  verifyRangeQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end,2,3);
  verifyEmptyQueryResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ start+ ""String_Node_Str""+ end);
  List<TimeSeriesResult> groupByResult=ImmutableList.of(new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),1),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),3));
  verifyGroupByResult(""String_Node_Str"" + getContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str""+ Constants.Metrics.Tag.FLOWLET+ ""String_Node_Str""+ start+ ""String_Node_Str""+ end,groupByResult);
  groupByResult=ImmutableList.of(new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),2),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),1),new TimeSeriesResult(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),4));
  verifyGroupByResult(""String_Node_Str"" + ""String_Node_Str"" + Constants.Metrics.Tag.NAMESPACE + ""String_Node_Str""+ Constants.Metrics.Tag.FLOWLET+ ""String_Node_Str""+ start+ ""String_Node_Str""+ end,groupByResult);
}"
7274,"public CubeQuery(long startTs,long endTs,int resolution,int limit,String measureName,MeasureType measureType,Map<String,String> sliceByTagValues,List<String> groupByTags,@Nullable Interpolator interpolator){
  this.startTs=startTs;
  this.endTs=endTs;
  this.resolution=resolution;
  this.limit=limit;
  this.measureName=measureName;
  this.measureType=measureType;
  this.sliceByTagValues=ImmutableMap.copyOf(sliceByTagValues);
  this.groupByTags=ImmutableList.copyOf(groupByTags);
  this.interpolator=interpolator;
}","public CubeQuery(long startTs,long endTs,int resolution,int limit,String measureName,MeasureType measureType,Map<String,String> sliceByTagValues,List<String> groupByTags,@Nullable Interpolator interpolator){
  this.startTs=startTs;
  this.endTs=endTs;
  this.resolution=resolution;
  this.limit=limit;
  this.measureName=measureName;
  this.measureType=measureType;
  this.sliceByTagValues=Maps.newHashMap(sliceByTagValues);
  this.groupByTags=ImmutableList.copyOf(groupByTags);
  this.interpolator=interpolator;
}"
7275,"@Test public void testGetId() throws Exception {
  InMemoryOrderedTableService.create(""String_Node_Str"");
  MetricsTable table=new InMemoryMetricsTable(""String_Node_Str"");
  EntityTable entityTable=new EntityTable(table);
  for (int i=1; i <= 10; i++) {
    Assert.assertEquals((long)i,entityTable.getId(""String_Node_Str"",""String_Node_Str"" + i));
  }
  for (int i=1; i <= 10; i++) {
    Assert.assertEquals((long)i,entityTable.getId(""String_Node_Str"",""String_Node_Str"" + i));
  }
  entityTable=new EntityTable(table);
  for (int i=1; i <= 10; i++) {
    Assert.assertEquals((long)i,entityTable.getId(""String_Node_Str"",""String_Node_Str"" + i));
  }
  for (int i=1; i <= 10; i++) {
    Assert.assertEquals((long)i,entityTable.getId(""String_Node_Str"",""String_Node_Str"" + i));
  }
}","@Test public void testGetId() throws Exception {
  InMemoryTableService.create(""String_Node_Str"");
  MetricsTable table=new InMemoryMetricsTable(""String_Node_Str"");
  EntityTable entityTable=new EntityTable(table);
  for (int i=1; i <= 10; i++) {
    Assert.assertEquals((long)i,entityTable.getId(""String_Node_Str"",""String_Node_Str"" + i));
  }
  for (int i=1; i <= 10; i++) {
    Assert.assertEquals((long)i,entityTable.getId(""String_Node_Str"",""String_Node_Str"" + i));
  }
  entityTable=new EntityTable(table);
  for (int i=1; i <= 10; i++) {
    Assert.assertEquals((long)i,entityTable.getId(""String_Node_Str"",""String_Node_Str"" + i));
  }
  for (int i=1; i <= 10; i++) {
    Assert.assertEquals((long)i,entityTable.getId(""String_Node_Str"",""String_Node_Str"" + i));
  }
}"
7276,"@Test public void testRecycleAfterMaxId() throws Exception {
  InMemoryOrderedTableService.create(""String_Node_Str"");
  MetricsTable table=new InMemoryMetricsTable(""String_Node_Str"");
  EntityTable entityTable=new EntityTable(table,101);
  for (long i=1; i <= 500; i++) {
    entityTable.getId(""String_Node_Str"",""String_Node_Str"" + i);
  }
  for (long i=1; i <= 100; i++) {
    Assert.assertEquals(""String_Node_Str"" + String.valueOf(400 + i),entityTable.getName(i,""String_Node_Str""));
  }
}","@Test public void testRecycleAfterMaxId() throws Exception {
  InMemoryTableService.create(""String_Node_Str"");
  MetricsTable table=new InMemoryMetricsTable(""String_Node_Str"");
  EntityTable entityTable=new EntityTable(table,101);
  for (long i=1; i <= 500; i++) {
    entityTable.getId(""String_Node_Str"",""String_Node_Str"" + i);
  }
  for (long i=1; i <= 100; i++) {
    Assert.assertEquals(""String_Node_Str"" + String.valueOf(400 + i),entityTable.getName(i,""String_Node_Str""));
  }
}"
7277,"@Test public void testGetName() throws Exception {
  InMemoryOrderedTableService.create(""String_Node_Str"");
  MetricsTable table=new InMemoryMetricsTable(""String_Node_Str"");
  EntityTable entityTable=new EntityTable(table);
  for (int i=1; i <= 10; i++) {
    Assert.assertEquals((long)i,entityTable.getId(""String_Node_Str"",""String_Node_Str"" + i));
  }
  for (int i=1; i <= 10; i++) {
    Assert.assertEquals(""String_Node_Str"" + i,entityTable.getName(i,""String_Node_Str""));
  }
}","@Test public void testGetName() throws Exception {
  InMemoryTableService.create(""String_Node_Str"");
  MetricsTable table=new InMemoryMetricsTable(""String_Node_Str"");
  EntityTable entityTable=new EntityTable(table);
  for (int i=1; i <= 10; i++) {
    Assert.assertEquals((long)i,entityTable.getId(""String_Node_Str"",""String_Node_Str"" + i));
  }
  for (int i=1; i <= 10; i++) {
    Assert.assertEquals(""String_Node_Str"" + i,entityTable.getName(i,""String_Node_Str""));
  }
}"
7278,"@Override protected Cube getCube(String name,int[] resolutions,Collection<? extends Aggregation> aggregations){
  FactTableSupplier supplier=new FactTableSupplier(){
    @Override public FactTable get(    int resolution,    int rollTime){
      InMemoryOrderedTableService.create(""String_Node_Str"");
      InMemoryOrderedTableService.create(""String_Node_Str"");
      return new FactTable(new InMemoryMetricsTable(""String_Node_Str""),new EntityTable(new InMemoryMetricsTable(""String_Node_Str"")),resolution,rollTime);
    }
  }
;
  return new DefaultCube(resolutions,supplier,aggregations);
}","@Override protected Cube getCube(String name,int[] resolutions,Collection<? extends Aggregation> aggregations){
  FactTableSupplier supplier=new FactTableSupplier(){
    @Override public FactTable get(    int resolution,    int rollTime){
      InMemoryTableService.create(""String_Node_Str"");
      InMemoryTableService.create(""String_Node_Str"");
      return new FactTable(new InMemoryMetricsTable(""String_Node_Str""),new EntityTable(new InMemoryMetricsTable(""String_Node_Str"")),resolution,rollTime);
    }
  }
;
  return new DefaultCube(resolutions,supplier,aggregations);
}"
7279,"@Override public FactTable get(int resolution,int rollTime){
  InMemoryOrderedTableService.create(""String_Node_Str"");
  InMemoryOrderedTableService.create(""String_Node_Str"");
  return new FactTable(new InMemoryMetricsTable(""String_Node_Str""),new EntityTable(new InMemoryMetricsTable(""String_Node_Str"")),resolution,rollTime);
}","@Override public FactTable get(int resolution,int rollTime){
  InMemoryTableService.create(""String_Node_Str"");
  InMemoryTableService.create(""String_Node_Str"");
  return new FactTable(new InMemoryMetricsTable(""String_Node_Str""),new EntityTable(new InMemoryMetricsTable(""String_Node_Str"")),resolution,rollTime);
}"
7280,"@Test public void testMaxResolution() throws Exception {
  InMemoryOrderedTableService.create(""String_Node_Str"");
  InMemoryOrderedTableService.create(""String_Node_Str"");
  int resolution=Integer.MAX_VALUE;
  int rollTimebaseInterval=3600;
  FactTable table=new FactTable(new InMemoryMetricsTable(""String_Node_Str""),new EntityTable(new InMemoryMetricsTable(""String_Node_Str"")),resolution,rollTimebaseInterval);
  long ts=System.currentTimeMillis() / 1000;
  int count=1000;
  for (int i=0; i < count; i++) {
    for (int k=0; k < 10; k++) {
      writeInc(table,""String_Node_Str"" + k,ts + i * 60 * 60* 24,i * k,""String_Node_Str"" + k,""String_Node_Str"" + k);
    }
  }
  for (int k=0; k < 10; k++) {
    FactScan scan=new FactScan(0,0,""String_Node_Str"" + k,tagValues(""String_Node_Str"" + k,""String_Node_Str"" + k));
    Table<String,List<TagValue>,List<TimeValue>> expected=HashBasedTable.create();
    expected.put(""String_Node_Str"" + k,tagValues(""String_Node_Str"" + k,""String_Node_Str"" + k),ImmutableList.of(new TimeValue(0,k * count * (count - 1) / 2)));
    assertScan(table,expected,scan);
  }
}","@Test public void testMaxResolution() throws Exception {
  InMemoryTableService.create(""String_Node_Str"");
  InMemoryTableService.create(""String_Node_Str"");
  int resolution=Integer.MAX_VALUE;
  int rollTimebaseInterval=3600;
  FactTable table=new FactTable(new InMemoryMetricsTable(""String_Node_Str""),new EntityTable(new InMemoryMetricsTable(""String_Node_Str"")),resolution,rollTimebaseInterval);
  long ts=System.currentTimeMillis() / 1000;
  int count=1000;
  for (int i=0; i < count; i++) {
    for (int k=0; k < 10; k++) {
      writeInc(table,""String_Node_Str"" + k,ts + i * 60 * 60* 24,i * k,""String_Node_Str"" + k,""String_Node_Str"" + k);
    }
  }
  for (int k=0; k < 10; k++) {
    FactScan scan=new FactScan(0,0,""String_Node_Str"" + k,tagValues(""String_Node_Str"" + k,""String_Node_Str"" + k));
    Table<String,List<TagValue>,List<TimeValue>> expected=HashBasedTable.create();
    expected.put(""String_Node_Str"" + k,tagValues(""String_Node_Str"" + k,""String_Node_Str"" + k),ImmutableList.of(new TimeValue(0,k * count * (count - 1) / 2)));
    assertScan(table,expected,scan);
  }
}"
7281,"@GET @Path(""String_Node_Str"") public void workflowStatus(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowName){
  programLifecycleHttpHandler.workflowStatus(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,workflowName);
}","@GET @Path(""String_Node_Str"") public void workflowStatus(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowName){
  programLifecycleHttpHandler.workflowStatus(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,workflowName);
}"
7282,"/** 
 * Returns specification of a runnable - flow.
 */
@GET @Path(""String_Node_Str"") public void flowSpecification(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId){
  programLifecycleHttpHandler.runnableSpecification(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.FLOW.getCategoryName(),flowId);
}","/** 
 * Returns specification of a runnable - flow.
 */
@GET @Path(""String_Node_Str"") public void flowSpecification(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId){
  programLifecycleHttpHandler.runnableSpecification(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.FLOW.getCategoryName(),flowId);
}"
7283,"/** 
 * Returns a list of procedure associated with account & application.
 */
@GET @Path(""String_Node_Str"") public void getMapreduceByApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  programLifecycleHttpHandler.getProgramsByApp(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.MAPREDUCE.getCategoryName());
}","/** 
 * Returns a list of mapreduce associated with account & application.
 */
@GET @Path(""String_Node_Str"") public void getMapreduceByApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  programLifecycleHttpHandler.getMapreduceByApp(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId);
}"
7284,"/** 
 * Returns status of a type specified by {flows,workflows,mapreduce,spark,procedures,services,schedules}.
 */
@GET @Path(""String_Node_Str"") public void getStatus(final HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String type,@PathParam(""String_Node_Str"") final String id){
  programLifecycleHttpHandler.getStatus(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,type,id);
}","/** 
 * Returns status of a type specified by {flows,workflows,mapreduce,spark,procedures,services,schedules}.
 */
@GET @Path(""String_Node_Str"") public void getStatus(final HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String type,@PathParam(""String_Node_Str"") final String id){
  programLifecycleHttpHandler.getStatus(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,type,id);
}"
7285,"/** 
 * Deploys an application.
 */
@POST @Path(""String_Node_Str"") public BodyConsumer deploy(HttpRequest request,HttpResponder responder,@HeaderParam(ARCHIVE_NAME_HEADER) final String archiveName){
  try {
    return appLifecycleHttpHandler.deploy(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,null,archiveName);
  }
 catch (  Exception ex) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + ex.getMessage());
    return null;
  }
}","/** 
 * Deploys an application.
 */
@POST @Path(""String_Node_Str"") public BodyConsumer deploy(HttpRequest request,HttpResponder responder,@HeaderParam(ARCHIVE_NAME_HEADER) final String archiveName){
  try {
    return appLifecycleHttpHandler.deploy(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,null,archiveName);
  }
 catch (  Exception ex) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + ex.getMessage());
    return null;
  }
}"
7286,"/** 
 * Returns program runs based on options it returns either currently running or completed or failed. Default it returns all.
 */
@GET @Path(""String_Node_Str"") public void runnableHistory(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String runnableType,@PathParam(""String_Node_Str"") String runnableId,@QueryParam(""String_Node_Str"") String status,@QueryParam(""String_Node_Str"") String startTs,@QueryParam(""String_Node_Str"") String endTs,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") final int resultLimit){
  programLifecycleHttpHandler.runnableHistory(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,runnableType,runnableId,status,startTs,endTs,resultLimit);
}","/** 
 * Returns program runs based on options it returns either currently running or completed or failed. Default it returns all.
 */
@GET @Path(""String_Node_Str"") public void runnableHistory(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String runnableType,@PathParam(""String_Node_Str"") String runnableId,@QueryParam(""String_Node_Str"") String status,@QueryParam(""String_Node_Str"") String startTs,@QueryParam(""String_Node_Str"") String endTs,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") final int resultLimit){
  programLifecycleHttpHandler.runnableHistory(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,runnableType,runnableId,status,startTs,endTs,resultLimit);
}"
7287,"/** 
 * Returns specification of workflow.
 */
@GET @Path(""String_Node_Str"") public void workflowSpecification(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String workflowId){
  programLifecycleHttpHandler.runnableSpecification(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.WORKFLOW.getCategoryName(),workflowId);
}","/** 
 * Returns specification of workflow.
 */
@GET @Path(""String_Node_Str"") public void workflowSpecification(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String workflowId){
  programLifecycleHttpHandler.runnableSpecification(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.WORKFLOW.getCategoryName(),workflowId);
}"
7288,"/** 
 * Returns a list of procedure associated with account & application.
 */
@GET @Path(""String_Node_Str"") public void getFlowsByApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  programLifecycleHttpHandler.getProgramsByApp(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.FLOW.getCategoryName());
}","/** 
 * Returns a list of flows associated with account & application.
 */
@GET @Path(""String_Node_Str"") public void getFlowsByApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  programLifecycleHttpHandler.getFlowsByApp(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId);
}"
7289,"/** 
 * Returns a list of applications associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllApps(HttpRequest request,HttpResponder responder){
  appLifecycleHttpHandler.getAllApps(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE);
}","/** 
 * Returns a list of applications associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllApps(HttpRequest request,HttpResponder responder){
  appLifecycleHttpHandler.getAllApps(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}"
7290,"/** 
 * Returns a list of spark jobs associated with account & application.
 */
@GET @Path(""String_Node_Str"") public void getSparkByApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  programLifecycleHttpHandler.getProgramsByApp(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.SPARK.getCategoryName());
}","/** 
 * Returns a list of spark jobs associated with account & application.
 */
@GET @Path(""String_Node_Str"") public void getSparkByApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  programLifecycleHttpHandler.getSparkByApp(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId);
}"
7291,"/** 
 * Changes input stream for a flowlet connection.
 */
@PUT @Path(""String_Node_Str"") public void changeFlowletStreamConnection(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId,@PathParam(""String_Node_Str"") String flowletId,@PathParam(""String_Node_Str"") String streamId) throws IOException {
  programLifecycleHttpHandler.changeFlowletStreamConnection(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,flowId,flowletId,streamId);
}","/** 
 * Changes input stream for a flowlet connection.
 */
@PUT @Path(""String_Node_Str"") public void changeFlowletStreamConnection(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId,@PathParam(""String_Node_Str"") String flowletId,@PathParam(""String_Node_Str"") String streamId) throws IOException {
  programLifecycleHttpHandler.changeFlowletStreamConnection(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,flowId,flowletId,streamId);
}"
7292,"/** 
 * Returns the schedule ids for a given workflow.
 */
@GET @Path(""String_Node_Str"") public void getWorkflowSchedules(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowId){
  programLifecycleHttpHandler.getWorkflowSchedules(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,workflowId);
}","/** 
 * Returns the schedule ids for a given workflow.
 */
@GET @Path(""String_Node_Str"") public void getWorkflowSchedules(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowId){
  programLifecycleHttpHandler.getWorkflowSchedules(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,workflowId);
}"
7293,"/** 
 * Deletes queues.
 */
@DELETE @Path(""String_Node_Str"") public void deleteFlowQueues(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId){
  programLifecycleHttpHandler.deleteFlowQueues(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,flowId);
}","/** 
 * Deletes queues.
 */
@DELETE @Path(""String_Node_Str"") public void deleteFlowQueues(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId){
  programLifecycleHttpHandler.deleteFlowQueues(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,flowId);
}"
7294,"@GET @Path(""String_Node_Str"") public void serviceSpecification(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId){
  programLifecycleHttpHandler.runnableSpecification(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.SERVICE.getCategoryName(),serviceId);
}","@GET @Path(""String_Node_Str"") public void serviceSpecification(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId){
  programLifecycleHttpHandler.runnableSpecification(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.SERVICE.getCategoryName(),serviceId);
}"
7295,"/** 
 * Gets number of instances for a worker.
 */
@GET @Path(""String_Node_Str"") public void getWorkerInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String workerId){
  programLifecycleHttpHandler.getWorkerInstances(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,workerId);
}","/** 
 * Gets number of instances for a worker.
 */
@GET @Path(""String_Node_Str"") public void getWorkerInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String workerId){
  programLifecycleHttpHandler.getWorkerInstances(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,workerId);
}"
7296,"/** 
 * Resume a schedule.
 */
@POST @Path(""String_Node_Str"") public void resumeSchedule(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String scheduleName){
  programLifecycleHttpHandler.performAction(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,""String_Node_Str"",scheduleName,""String_Node_Str"");
}","/** 
 * Resume a schedule.
 */
@POST @Path(""String_Node_Str"") public void resumeSchedule(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String scheduleName){
  programLifecycleHttpHandler.performAction(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,""String_Node_Str"",scheduleName,""String_Node_Str"");
}"
7297,"/** 
 * Returns a list of map/reduces associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllMapReduce(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getAllMapReduce(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE);
}","/** 
 * Returns a list of map/reduces associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllMapReduce(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getAllMapReduce(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}"
7298,"/** 
 * Suspend a schedule.
 */
@POST @Path(""String_Node_Str"") public void suspendSchedule(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String scheduleName){
  programLifecycleHttpHandler.performAction(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,""String_Node_Str"",scheduleName,""String_Node_Str"");
}","/** 
 * Suspend a schedule.
 */
@POST @Path(""String_Node_Str"") public void suspendSchedule(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String scheduleName){
  programLifecycleHttpHandler.performAction(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,""String_Node_Str"",scheduleName,""String_Node_Str"");
}"
7299,"/** 
 * Returns specification of spark program.
 */
@GET @Path(""String_Node_Str"") public void sparkSpecification(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String sparkId){
  programLifecycleHttpHandler.runnableSpecification(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.SPARK.getCategoryName(),sparkId);
}","/** 
 * Returns specification of spark program.
 */
@GET @Path(""String_Node_Str"") public void sparkSpecification(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String sparkId){
  programLifecycleHttpHandler.runnableSpecification(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.SPARK.getCategoryName(),sparkId);
}"
7300,"/** 
 * Save runnable runtime args.
 */
@PUT @Path(""String_Node_Str"") public void saveRunnableRuntimeArgs(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String runnableType,@PathParam(""String_Node_Str"") final String runnableId){
  programLifecycleHttpHandler.saveRunnableRuntimeArgs(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,runnableType,runnableId);
}","/** 
 * Save runnable runtime args.
 */
@PUT @Path(""String_Node_Str"") public void saveRunnableRuntimeArgs(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String runnableType,@PathParam(""String_Node_Str"") final String runnableId){
  programLifecycleHttpHandler.saveRunnableRuntimeArgs(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,runnableType,runnableId);
}"
7301,"@GET @Path(""String_Node_Str"") @SuppressWarnings(""String_Node_Str"") public void flowLiveInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId){
  programLifecycleHttpHandler.liveInfo(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.FLOW.getCategoryName(),flowId);
}","@GET @Path(""String_Node_Str"") @SuppressWarnings(""String_Node_Str"") public void flowLiveInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId){
  programLifecycleHttpHandler.liveInfo(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.FLOW.getCategoryName(),flowId);
}"
7302,"/** 
 * Starts a program.
 */
@POST @Path(""String_Node_Str"") public void startProgram(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String type,@PathParam(""String_Node_Str"") final String id){
  programLifecycleHttpHandler.performAction(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,type,id,""String_Node_Str"");
}","/** 
 * Starts a program.
 */
@POST @Path(""String_Node_Str"") public void startProgram(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String type,@PathParam(""String_Node_Str"") final String id){
  programLifecycleHttpHandler.performAction(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,type,id,""String_Node_Str"");
}"
7303,"/** 
 * Sets number of instances for a worker.
 */
@PUT @Path(""String_Node_Str"") public void setWorkerInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String workerId){
  programLifecycleHttpHandler.setWorkerInstances(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,workerId);
}","/** 
 * Sets number of instances for a worker.
 */
@PUT @Path(""String_Node_Str"") public void setWorkerInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String workerId){
  programLifecycleHttpHandler.setWorkerInstances(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,workerId);
}"
7304,"/** 
 * Returns the info associated with the application.
 */
@GET @Path(""String_Node_Str"") public void getAppInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  appLifecycleHttpHandler.getAppInfo(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId);
}","/** 
 * Returns the info associated with the application.
 */
@GET @Path(""String_Node_Str"") public void getAppInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  appLifecycleHttpHandler.getAppInfo(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId);
}"
7305,"/** 
 * Starts a program with debugging enabled.
 */
@POST @Path(""String_Node_Str"") public void debugProgram(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String type,@PathParam(""String_Node_Str"") final String id){
  programLifecycleHttpHandler.performAction(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,type,id,""String_Node_Str"");
}","/** 
 * Starts a program with debugging enabled.
 */
@POST @Path(""String_Node_Str"") public void debugProgram(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String type,@PathParam(""String_Node_Str"") final String id){
  programLifecycleHttpHandler.performAction(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,type,id,""String_Node_Str"");
}"
7306,"/** 
 * Returns a list of procedures associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllProcedures(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getAllProcedures(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE);
}","/** 
 * Returns a list of procedures associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllProcedures(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getAllProcedures(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}"
7307,"/** 
 * Stops a program.
 */
@POST @Path(""String_Node_Str"") public void stopProgram(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String type,@PathParam(""String_Node_Str"") final String id){
  programLifecycleHttpHandler.performAction(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,type,id,""String_Node_Str"");
}","/** 
 * Stops a program.
 */
@POST @Path(""String_Node_Str"") public void stopProgram(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String type,@PathParam(""String_Node_Str"") final String id){
  programLifecycleHttpHandler.performAction(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,type,id,""String_Node_Str"");
}"
7308,"/** 
 * Returns a list of worker jobs associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllWorkers(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getAllWorkers(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE);
}","/** 
 * Returns a list of worker jobs associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllWorkers(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getAllWorkers(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}"
7309,"@DELETE @Path(""String_Node_Str"") public void deleteQueues(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.deleteQueues(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE);
}","@DELETE @Path(""String_Node_Str"") public void deleteQueues(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.deleteQueues(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}"
7310,"/** 
 * Returns the number of instances for all program runnables that are passed into the data. The data is an array of Json objects where each object must contain the following three elements: appId, programType, and programId (flow name, service name, or procedure name). Retrieving instances only applies to flows, procedures, and user services. For flows and procedures, another parameter, ""runnableId"", must be provided. This corresponds to the flowlet/runnable for which to retrieve the instances. This does not apply to procedures. Example input: [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""runnableId"": ""Runnable1""}, {""appId"": ""App1"", ""programType"": ""Procedure"", ""programId"": ""Proc2""}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1"", ""runnableId"": ""Flowlet1""}] The response will be an array of JsonObjects each of which will contain the three input parameters as well as 3 fields: ""provisioned"" which maps to the number of instances actually provided for the input runnable, ""requested"" which maps to the number of instances the user has requested for the input runnable, ""statusCode"" which maps to the http status code for the data in that JsonObjects. (200, 400, 404) If an error occurs in the input (i.e. in the example above, Flowlet1 does not exist), then all JsonObjects for which the parameters have a valid instances will have the provisioned and requested fields status code fields but all JsonObjects for which the parameters are not valid will have an error message and statusCode. E.g. given the above data, if there is no Flowlet1, then the response would be 200 OK with following possible data: [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""runnableId"": ""Runnable1"", ""statusCode"": 200, ""provisioned"": 2, ""requested"": 2}, {""appId"": ""App1"", ""programType"": ""Procedure"", ""programId"": ""Proc2"", ""statusCode"": 200, ""provisioned"": 1, ""requested"": 3}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1"", ""runnableId"": ""Flowlet1"", ""statusCode"": 404, ""error"": ""Runnable"": Flowlet1 not found""}]
 * @param request
 * @param responder
 */
@POST @Path(""String_Node_Str"") public void getInstances(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getInstances(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE);
}","/** 
 * Returns the number of instances for all program runnables that are passed into the data. The data is an array of Json objects where each object must contain the following three elements: appId, programType, and programId (flow name, service name, or procedure name). Retrieving instances only applies to flows, procedures, and user services. For flows and procedures, another parameter, ""runnableId"", must be provided. This corresponds to the flowlet/runnable for which to retrieve the instances. This does not apply to procedures. Example input: [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""runnableId"": ""Runnable1""}, {""appId"": ""App1"", ""programType"": ""Procedure"", ""programId"": ""Proc2""}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1"", ""runnableId"": ""Flowlet1""}] The response will be an array of JsonObjects each of which will contain the three input parameters as well as 3 fields: ""provisioned"" which maps to the number of instances actually provided for the input runnable, ""requested"" which maps to the number of instances the user has requested for the input runnable, ""statusCode"" which maps to the http status code for the data in that JsonObjects. (200, 400, 404) If an error occurs in the input (i.e. in the example above, Flowlet1 does not exist), then all JsonObjects for which the parameters have a valid instances will have the provisioned and requested fields status code fields but all JsonObjects for which the parameters are not valid will have an error message and statusCode. E.g. given the above data, if there is no Flowlet1, then the response would be 200 OK with following possible data: [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""runnableId"": ""Runnable1"", ""statusCode"": 200, ""provisioned"": 2, ""requested"": 2}, {""appId"": ""App1"", ""programType"": ""Procedure"", ""programId"": ""Proc2"", ""statusCode"": 200, ""provisioned"": 1, ""requested"": 3}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1"", ""runnableId"": ""Flowlet1"", ""statusCode"": 404, ""error"": ""Runnable"": Flowlet1 not found""}]
 * @param request
 * @param responder
 */
@POST @Path(""String_Node_Str"") public void getInstances(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getInstances(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}"
7311,"/** 
 * Returns a list of workflows associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllWorkflows(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getAllWorkflows(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE);
}","/** 
 * Returns a list of workflows associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllWorkflows(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getAllWorkflows(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}"
7312,"/** 
 * Returns the status for all programs that are passed into the data. The data is an array of Json objects where each object must contain the following three elements: appId, programType, and programId (flow name, service name, etc.). <p/> Example input: [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1""}, {""appId"": ""App1"", ""programType"": ""Procedure"", ""programId"": ""Proc2""}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1""}] <p/> The response will be an array of JsonObjects each of which will contain the three input parameters as well as 2 fields, ""status"" which maps to the status of the program and ""statusCode"" which maps to the status code for the data in that JsonObjects. If an error occurs in the input (i.e. in the example above, App2 does not exist), then all JsonObjects for which the parameters have a valid status will have the status field but all JsonObjects for which the parameters do not have a valid status will have an error message and statusCode. <p/> For example, if there is no App2 in the data above, then the response would be 200 OK with following possible data: [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""statusCode"": 200, ""status"": ""RUNNING""}, {""appId"": ""App1"", ""programType"": ""Procedure"", ""programId"": ""Proc2""}, ""statusCode"": 200, ""status"": ""STOPPED""}, {""appId"":""App2"", ""programType"":""Flow"", ""programId"":""Flow1"", ""statusCode"":404, ""error"": ""App: App2 not found""}]
 * @param request
 * @param responder
 */
@POST @Path(""String_Node_Str"") public void getStatuses(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getStatuses(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE);
}","/** 
 * Returns the status for all programs that are passed into the data. The data is an array of Json objects where each object must contain the following three elements: appId, programType, and programId (flow name, service name, etc.). <p/> Example input: [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1""}, {""appId"": ""App1"", ""programType"": ""Procedure"", ""programId"": ""Proc2""}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1""}] <p/> The response will be an array of JsonObjects each of which will contain the three input parameters as well as 2 fields, ""status"" which maps to the status of the program and ""statusCode"" which maps to the status code for the data in that JsonObjects. If an error occurs in the input (i.e. in the example above, App2 does not exist), then all JsonObjects for which the parameters have a valid status will have the status field but all JsonObjects for which the parameters do not have a valid status will have an error message and statusCode. <p/> For example, if there is no App2 in the data above, then the response would be 200 OK with following possible data: [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""statusCode"": 200, ""status"": ""RUNNING""}, {""appId"": ""App1"", ""programType"": ""Procedure"", ""programId"": ""Proc2""}, ""statusCode"": 200, ""status"": ""STOPPED""}, {""appId"":""App2"", ""programType"":""Flow"", ""programId"":""Flow1"", ""statusCode"":404, ""error"": ""App: App2 not found""}]
 * @param request
 * @param responder
 */
@POST @Path(""String_Node_Str"") public void getStatuses(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getStatuses(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}"
7313,"/** 
 * Delete an application specified by appId.
 */
@DELETE @Path(""String_Node_Str"") public void deleteApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId){
  appLifecycleHttpHandler.deleteApp(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId);
}","/** 
 * Delete an application specified by appId.
 */
@DELETE @Path(""String_Node_Str"") public void deleteApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId){
  appLifecycleHttpHandler.deleteApp(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId);
}"
7314,"/** 
 * Increases number of instance for a flowlet within a flow.
 */
@PUT @Path(""String_Node_Str"") public void setFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId,@PathParam(""String_Node_Str"") String flowletId){
  programLifecycleHttpHandler.setFlowletInstances(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,flowId,flowletId);
}","/** 
 * Increases number of instance for a flowlet within a flow.
 */
@PUT @Path(""String_Node_Str"") public void setFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId,@PathParam(""String_Node_Str"") String flowletId){
  programLifecycleHttpHandler.setFlowletInstances(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,flowId,flowletId);
}"
7315,"/** 
 * Returns specification of mapreduce.
 */
@GET @Path(""String_Node_Str"") public void mapreduceSpecification(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String mapreduceId){
  programLifecycleHttpHandler.runnableSpecification(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.MAPREDUCE.getCategoryName(),mapreduceId);
}","/** 
 * Returns specification of mapreduce.
 */
@GET @Path(""String_Node_Str"") public void mapreduceSpecification(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String mapreduceId){
  programLifecycleHttpHandler.runnableSpecification(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.MAPREDUCE.getCategoryName(),mapreduceId);
}"
7316,"/** 
 * Deletes all applications in CDAP.
 */
@DELETE @Path(""String_Node_Str"") public void deleteAllApps(HttpRequest request,HttpResponder responder){
  appLifecycleHttpHandler.deleteAllApps(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE);
}","/** 
 * Deletes all applications in CDAP.
 */
@DELETE @Path(""String_Node_Str"") public void deleteAllApps(HttpRequest request,HttpResponder responder){
  appLifecycleHttpHandler.deleteAllApps(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}"
7317,"/** 
 * Returns a list of spark jobs associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllSpark(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getAllSpark(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE);
}","/** 
 * Returns a list of spark jobs associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllSpark(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getAllSpark(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}"
7318,"/** 
 * Get runnable runtime args.
 */
@GET @Path(""String_Node_Str"") public void getRunnableRuntimeArgs(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String runnableType,@PathParam(""String_Node_Str"") final String runnableId){
  programLifecycleHttpHandler.getRunnableRuntimeArgs(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,runnableType,runnableId);
}","/** 
 * Get runnable runtime args.
 */
@GET @Path(""String_Node_Str"") public void getRunnableRuntimeArgs(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String runnableType,@PathParam(""String_Node_Str"") final String runnableId){
  programLifecycleHttpHandler.getRunnableRuntimeArgs(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,runnableType,runnableId);
}"
7319,"/** 
 * Returns specification of procedure.
 */
@GET @Path(""String_Node_Str"") public void procedureSpecification(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String procId){
  programLifecycleHttpHandler.runnableSpecification(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.PROCEDURE.getCategoryName(),procId);
}","/** 
 * Returns specification of procedure.
 */
@GET @Path(""String_Node_Str"") public void procedureSpecification(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String procId){
  programLifecycleHttpHandler.runnableSpecification(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.PROCEDURE.getCategoryName(),procId);
}"
7320,"/** 
 * Returns a list of workers associated with account & application.
 */
@GET @Path(""String_Node_Str"") public void getWorkersByApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  programLifecycleHttpHandler.getProgramsByApp(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.WORKER.getCategoryName());
}","/** 
 * Returns a list of workers associated with account & application.
 */
@GET @Path(""String_Node_Str"") public void getWorkersByApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  programLifecycleHttpHandler.getWorkersByApp(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId);
}"
7321,"/** 
 * Returns next scheduled runtime of a workflow.
 */
@GET @Path(""String_Node_Str"") public void getScheduledRunTime(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowId){
  programLifecycleHttpHandler.getScheduledRunTime(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,workflowId);
}","/** 
 * Returns next scheduled runtime of a workflow.
 */
@GET @Path(""String_Node_Str"") public void getScheduledRunTime(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowId){
  programLifecycleHttpHandler.getScheduledRunTime(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,workflowId);
}"
7322,"/** 
 * Returns number of instances for a flowlet within a flow.
 */
@GET @Path(""String_Node_Str"") public void getFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId,@PathParam(""String_Node_Str"") String flowletId){
  programLifecycleHttpHandler.getFlowletInstances(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,flowId,flowletId);
}","/** 
 * Returns number of instances for a flowlet within a flow.
 */
@GET @Path(""String_Node_Str"") public void getFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId,@PathParam(""String_Node_Str"") String flowletId){
  programLifecycleHttpHandler.getFlowletInstances(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,flowId,flowletId);
}"
7323,"/** 
 * Returns a list of flows associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllFlows(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getAllFlows(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE);
}","/** 
 * Returns a list of flows associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllFlows(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getAllFlows(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}"
7324,"/** 
 * Returns a list of procedure associated with account & application.
 */
@GET @Path(""String_Node_Str"") public void getProceduresByApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  programLifecycleHttpHandler.getProgramsByApp(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.PROCEDURE.getCategoryName());
}","/** 
 * Returns a list of procedure associated with account & application.
 */
@GET @Path(""String_Node_Str"") public void getProceduresByApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  programLifecycleHttpHandler.getProgramsByApp(responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.PROCEDURE.getCategoryName());
}"
7325,"/** 
 * Returns a list of programs associated with an application within a namespace.
 */
@GET @Path(""String_Node_Str"") public void getProgramsByApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programCategory){
  ProgramType type=getProgramType(programCategory);
  if (type == null) {
    responder.sendString(HttpResponseStatus.METHOD_NOT_ALLOWED,String.format(""String_Node_Str"",programCategory));
    return;
  }
  programList(responder,namespaceId,type,appId,store);
}","protected void getProgramsByApp(HttpResponder responder,String namespaceId,String appId,String programCategory){
  ProgramType type=getProgramType(programCategory);
  if (type == null) {
    responder.sendString(HttpResponseStatus.METHOD_NOT_ALLOWED,String.format(""String_Node_Str"",programCategory));
    return;
  }
  programList(responder,namespaceId,type,appId,store);
}"
7326,"/** 
 * Return the list of user Services in an application.
 */
@Path(""String_Node_Str"") @GET public void getServicesByApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  programLifecycleHttpHandler.getProgramsByApp(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.SERVICE.getCategoryName());
}","/** 
 * Return the list of user Services in an application.
 */
@Path(""String_Node_Str"") @GET public void getServicesByApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId){
  programLifecycleHttpHandler.getServicesByApp(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId);
}"
7327,"/** 
 * Returns a list of Services associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllServices(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getAllServices(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE);
}","/** 
 * Returns a list of Services associated with an account.
 */
@GET @Path(""String_Node_Str"") public void getAllServices(HttpRequest request,HttpResponder responder){
  programLifecycleHttpHandler.getAllServices(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE);
}"
7328,"/** 
 * Return the number of instances for the given runnable of a service.
 */
@GET @Path(""String_Node_Str"") public void getInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId,@PathParam(""String_Node_Str"") String runnableName){
  programLifecycleHttpHandler.getServiceInstances(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,serviceId,runnableName);
}","/** 
 * Return the number of instances for the given runnable of a service.
 */
@GET @Path(""String_Node_Str"") public void getInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId,@PathParam(""String_Node_Str"") String runnableName){
  programLifecycleHttpHandler.getServiceInstances(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,serviceId,runnableName);
}"
7329,"/** 
 * Set instances.
 */
@PUT @Path(""String_Node_Str"") public void setInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId,@PathParam(""String_Node_Str"") String runnableName){
  programLifecycleHttpHandler.setServiceInstances(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,serviceId,runnableName);
}","/** 
 * Set instances.
 */
@PUT @Path(""String_Node_Str"") public void setInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId,@PathParam(""String_Node_Str"") String runnableName){
  programLifecycleHttpHandler.setServiceInstances(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,serviceId,runnableName);
}"
7330,"@GET @Path(""String_Node_Str"") public void serviceLiveInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId){
  programLifecycleHttpHandler.liveInfo(rewriteRequest(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.SERVICE.getCategoryName(),serviceId);
}","@GET @Path(""String_Node_Str"") public void serviceLiveInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId){
  programLifecycleHttpHandler.liveInfo(RESTMigrationUtils.rewriteV2RequestToV3(request),responder,Constants.DEFAULT_NAMESPACE,appId,ProgramType.SERVICE.getCategoryName(),serviceId);
}"
7331,"/** 
 * Tests for program list calls
 */
@Test public void testProgramList() throws Exception {
  testListInitialState(TEST_NAMESPACE1,ProgramType.FLOW);
  testListInitialState(TEST_NAMESPACE2,ProgramType.MAPREDUCE);
  testListInitialState(TEST_NAMESPACE1,ProgramType.WORKFLOW);
  testListInitialState(TEST_NAMESPACE2,ProgramType.SPARK);
  testListInitialState(TEST_NAMESPACE1,ProgramType.SERVICE);
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=deploy(AppWithServices.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  verifyProgramList(TEST_NAMESPACE1,null,ProgramType.FLOW.getCategoryName(),1);
  verifyProgramList(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.MAPREDUCE.getCategoryName(),1);
  verifyProgramList(TEST_NAMESPACE2,null,ProgramType.SERVICE.getCategoryName(),1);
  verifyProgramList(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW.getCategoryName(),1);
  verifyProgramList(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.MAPREDUCE.getCategoryName(),1);
  verifyProgramList(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.WORKFLOW.getCategoryName(),0);
  verifyProgramList(TEST_NAMESPACE2,APP_WITH_SERVICES_APP_ID,ProgramType.SERVICE.getCategoryName(),1);
  Assert.assertEquals(404,getProgramListResponseCode(TEST_NAMESPACE1,APP_WITH_SERVICES_APP_ID,ProgramType.SERVICE.getCategoryName()));
  Assert.assertEquals(404,getProgramListResponseCode(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW.getCategoryName()));
  Assert.assertEquals(405,getProgramListResponseCode(TEST_NAMESPACE2,APP_WITH_SERVICES_APP_ID,""String_Node_Str""));
}","/** 
 * Tests for program list calls
 */
@Test public void testProgramList() throws Exception {
  testListInitialState(TEST_NAMESPACE1,ProgramType.FLOW);
  testListInitialState(TEST_NAMESPACE2,ProgramType.MAPREDUCE);
  testListInitialState(TEST_NAMESPACE1,ProgramType.WORKFLOW);
  testListInitialState(TEST_NAMESPACE2,ProgramType.SPARK);
  testListInitialState(TEST_NAMESPACE1,ProgramType.SERVICE);
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=deploy(AppWithServices.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  verifyProgramList(TEST_NAMESPACE1,null,ProgramType.FLOW.getCategoryName(),1);
  verifyProgramList(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.MAPREDUCE.getCategoryName(),1);
  verifyProgramList(TEST_NAMESPACE2,null,ProgramType.SERVICE.getCategoryName(),1);
  verifyProgramList(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW.getCategoryName(),1);
  verifyProgramList(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.MAPREDUCE.getCategoryName(),1);
  verifyProgramList(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.WORKFLOW.getCategoryName(),0);
  verifyProgramList(TEST_NAMESPACE2,APP_WITH_SERVICES_APP_ID,ProgramType.SERVICE.getCategoryName(),1);
  Assert.assertEquals(404,getProgramListResponseCode(TEST_NAMESPACE1,APP_WITH_SERVICES_APP_ID,ProgramType.SERVICE.getCategoryName()));
  Assert.assertEquals(404,getProgramListResponseCode(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW.getCategoryName()));
  Assert.assertEquals(404,getProgramListResponseCode(TEST_NAMESPACE2,APP_WITH_SERVICES_APP_ID,""String_Node_Str""));
}"
7332,"private T readRow(Row row){
  try {
    if (row.isEmpty()) {
      return null;
    }
    return rowReader.read(row,objectSchema);
  }
 catch (  Exception e) {
    throw new DataSetException(""String_Node_Str"" + e.getMessage(),e);
  }
}","private T readRow(Row row){
  try {
    if (row.isEmpty()) {
      return null;
    }
    return getReflectionRowReader().read(row,objectSchema);
  }
 catch (  Exception e) {
    throw new DataSetException(""String_Node_Str"" + e.getMessage(),e);
  }
}"
7333,"@SuppressWarnings(""String_Node_Str"") public ObjectMappedTableDataset(String name,Table table,TypeRepresentation typeRep,Schema objectSchema,String keyName,Schema.Type keyType,@Nullable ClassLoader classLoader){
  super(name,table);
  Preconditions.checkArgument(keyType == Schema.Type.STRING || keyType == Schema.Type.BYTES,""String_Node_Str"");
  this.table=table;
  this.objectSchema=objectSchema;
  this.tableSchema=getTableSchema(objectSchema,keyName,keyType);
  this.keyName=keyName;
  this.keyType=keyType;
  typeRep.setClassLoader(classLoader);
  Type type=typeRep.toType();
  this.putWriter=new ReflectionPutWriter<T>(objectSchema);
  this.rowReader=new ReflectionRowReader<T>(objectSchema,(TypeToken<T>)TypeToken.of(type));
}","public ObjectMappedTableDataset(String name,Table table,TypeRepresentation typeRep,Schema objectSchema,String keyName,Schema.Type keyType,@Nullable ClassLoader classLoader){
  super(name,table);
  Preconditions.checkArgument(keyType == Schema.Type.STRING || keyType == Schema.Type.BYTES,""String_Node_Str"");
  this.table=table;
  this.objectSchema=objectSchema;
  this.tableSchema=getTableSchema(objectSchema,keyName,keyType);
  this.keyName=keyName;
  this.keyType=keyType;
  this.typeRepresentation=typeRep;
  this.typeRepresentation.setClassLoader(classLoader);
  this.putWriter=new ReflectionPutWriter<T>(objectSchema);
}"
7334,"@Override public void register(DatasetDefinitionRegistry registry){
  DatasetDefinition<Table,? extends DatasetAdmin> tableDef=registry.get(""String_Node_Str"");
  registry.add(new ObjectMappedTableDefinition(ObjectMappedTable.class.getName(),tableDef));
  registry.add(new ObjectMappedTableDefinition(""String_Node_Str"",tableDef));
}","@Override public void register(DatasetDefinitionRegistry registry){
  DatasetDefinition<Table,? extends DatasetAdmin> tableDef=registry.get(""String_Node_Str"");
  registry.add(new ObjectMappedTableDefinition(FULL_NAME,tableDef));
  registry.add(new ObjectMappedTableDefinition(SHORT_NAME,tableDef));
}"
7335,"@Override public void configure(){
  setName(APP_NAME);
  setDescription(""String_Node_Str"");
  addStream(new Stream(""String_Node_Str""));
  createDataset(""String_Node_Str"",KeyValueTable.class);
  createDataset(""String_Node_Str"",KeyValueTable.class);
  addFlow(new PurchaseFlow());
  addMapReduce(new PurchaseHistoryBuilder());
  addWorkflow(new PurchaseHistoryWorkflow());
  addService(new PurchaseHistoryService());
  addService(UserProfileServiceHandler.SERVICE_NAME,new UserProfileServiceHandler());
  addService(new CatalogLookupService());
  scheduleWorkflow(Schedules.createTimeSchedule(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),""String_Node_Str"");
  try {
    createDataset(""String_Node_Str"",PurchaseHistoryStore.class,PurchaseHistoryStore.properties());
    createDataset(""String_Node_Str"",ObjectMappedTable.class,ObjectMappedTableProperties.builder().setType(Purchase.class).build());
  }
 catch (  UnsupportedTypeException e) {
    throw new RuntimeException(e);
  }
}","@Override public void configure(){
  setName(APP_NAME);
  setDescription(""String_Node_Str"");
  addStream(new Stream(""String_Node_Str""));
  createDataset(""String_Node_Str"",KeyValueTable.class);
  createDataset(""String_Node_Str"",KeyValueTable.class);
  addFlow(new PurchaseFlow());
  addMapReduce(new PurchaseHistoryBuilder());
  addWorkflow(new PurchaseHistoryWorkflow());
  addService(new PurchaseHistoryService());
  addService(UserProfileServiceHandler.SERVICE_NAME,new UserProfileServiceHandler());
  addService(new CatalogLookupService());
  scheduleWorkflow(Schedules.createTimeSchedule(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),""String_Node_Str"");
  try {
    createDataset(""String_Node_Str"",PurchaseHistoryStore.class,PurchaseHistoryStore.properties());
    createDataset(""String_Node_Str"",ObjectMappedTable.class,ObjectMappedTableProperties.builder().setType(Purchase.class).setExploreKeyType(Schema.Type.STRING).build());
  }
 catch (  UnsupportedTypeException e) {
    throw new RuntimeException(e);
  }
}"
7336,"/** 
 * Creates properties for   {@link ObjectStore} dataset instance.
 * @param type type of objects to be stored in dataset
 * @return {@link DatasetProperties} for the dataset
 * @throws UnsupportedTypeException
 */
public static DatasetProperties objectStoreProperties(Type type,DatasetProperties props) throws UnsupportedTypeException {
  Schema schema=new ReflectionSchemaGenerator().generate(type);
  TypeRepresentation typeRep=new TypeRepresentation(type);
  return DatasetProperties.builder().add(""String_Node_Str"",GSON.toJson(schema)).add(""String_Node_Str"",GSON.toJson(typeRep)).addAll(props.getProperties()).build();
}","/** 
 * Creates properties for   {@link ObjectStore} dataset instance.
 * @param type type of objects to be stored in dataset
 * @return {@link DatasetProperties} for the dataset
 * @throws UnsupportedTypeException
 */
public static DatasetProperties objectStoreProperties(Type type,DatasetProperties props) throws UnsupportedTypeException {
  Schema schema=new ReflectionSchemaGenerator().generate(type);
  TypeRepresentation typeRep=new TypeRepresentation(type);
  return DatasetProperties.builder().add(""String_Node_Str"",schema.toString()).add(""String_Node_Str"",GSON.toJson(typeRep)).addAll(props.getProperties()).build();
}"
7337,"private String listDataEntitiesByApp(Store store,DatasetFramework dsFramework,Id.Program programId,Data type) throws Exception {
  Id.Namespace namespace=new Id.Namespace(programId.getNamespaceId());
  ApplicationSpecification appSpec=store.getApplication(new Id.Application(namespace,programId.getApplicationId()));
  if (type == Data.DATASET) {
    Set<String> dataSetsUsed=dataSetsUsedBy(appSpec);
    List<DatasetRecord> result=Lists.newArrayListWithExpectedSize(dataSetsUsed.size());
    for (    String dsName : dataSetsUsed) {
      String typeName=null;
      DatasetSpecification dsSpec=getDatasetSpec(dsFramework,namespace,dsName);
      if (dsSpec != null) {
        typeName=dsSpec.getType();
      }
      result.add(makeDataSetRecord(dsName,typeName));
    }
    return GSON.toJson(result);
  }
  if (type == Data.STREAM) {
    Set<String> streamsUsed=streamsUsedBy(appSpec);
    List<StreamRecord> result=Lists.newArrayListWithExpectedSize(streamsUsed.size());
    for (    String streamName : streamsUsed) {
      result.add(makeStreamRecord(streamName,null));
    }
    return GSON.toJson(result);
  }
  return ""String_Node_Str"";
}","private String listDataEntitiesByApp(Store store,DatasetFramework dsFramework,Id.Program programId,Data type) throws Exception {
  Id.Namespace namespace=new Id.Namespace(programId.getNamespaceId());
  ApplicationSpecification appSpec=store.getApplication(new Id.Application(namespace,programId.getApplicationId()));
  if (appSpec == null) {
    return ""String_Node_Str"";
  }
  if (type == Data.DATASET) {
    Set<String> dataSetsUsed=dataSetsUsedBy(appSpec);
    List<DatasetRecord> result=Lists.newArrayListWithExpectedSize(dataSetsUsed.size());
    for (    String dsName : dataSetsUsed) {
      String typeName=null;
      DatasetSpecification dsSpec=getDatasetSpec(dsFramework,namespace,dsName);
      if (dsSpec != null) {
        typeName=dsSpec.getType();
      }
      result.add(makeDataSetRecord(dsName,typeName));
    }
    return GSON.toJson(result);
  }
  if (type == Data.STREAM) {
    Set<String> streamsUsed=streamsUsedBy(appSpec);
    List<StreamRecord> result=Lists.newArrayListWithExpectedSize(streamsUsed.size());
    for (    String streamName : streamsUsed) {
      result.add(makeStreamRecord(streamName,null));
    }
    return GSON.toJson(result);
  }
  return ""String_Node_Str"";
}"
7338,"@Override public Object deserialize(Writable writable) throws SerDeException {
  try {
    ObjectWritable objectWritable=(ObjectWritable)writable;
    return deserializer.deserialize(objectWritable.get());
  }
 catch (  Throwable t) {
    LOG.info(""String_Node_Str"",t);
    throw new SerDeException(""String_Node_Str"",t);
  }
}","@Override public Object deserialize(Writable writable) throws SerDeException {
  ObjectWritable objectWritable=(ObjectWritable)writable;
  Object obj=objectWritable.get();
  try {
    return deserializer.deserialize(obj);
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",obj,t);
    throw new SerDeException(""String_Node_Str"",t);
  }
}"
7339,"/** 
 * Translate a field that fits a   {@link Schema} field into a type that Hive understands.For example, a ByteBuffer is allowed by schema but Hive only understands byte arrays, so all ByteBuffers must be changed into byte arrays. Reflection is used to examine java objects if the expected hive type is a struct.
 * @param field value of the field to deserialize.
 * @param typeInfo type of the field as expected by Hive.
 * @param schema schema of the field.
 * @return translated field.
 * @throws NoSuchFieldException if a struct field was expected but not found in the object.
 * @throws IllegalAccessException if a struct field was not accessible.
 */
private Object deserializeField(Object field,TypeInfo typeInfo,Schema schema) throws NoSuchFieldException, IllegalAccessException {
  boolean isNullable=schema.isNullable();
  if (field == null) {
    if (isNullable) {
      return null;
    }
 else {
      throw new UnexpectedFormatException(""String_Node_Str"");
    }
  }
  if (isNullable) {
    schema=schema.getNonNullable();
  }
switch (typeInfo.getCategory()) {
case PRIMITIVE:
    return deserializePrimitive(field,(PrimitiveTypeInfo)typeInfo);
case LIST:
  return deserializeList(field,(ListTypeInfo)typeInfo,schema.getComponentSchema());
case MAP:
return deserializeMap(field,(MapTypeInfo)typeInfo,schema.getMapSchema());
case STRUCT:
StructTypeInfo structTypeInfo=(StructTypeInfo)typeInfo;
ArrayList<String> innerFieldNames=structTypeInfo.getAllStructFieldNames();
ArrayList<TypeInfo> innerFieldTypes=structTypeInfo.getAllStructFieldTypeInfos();
return flattenRecord(field,innerFieldNames,innerFieldTypes,schema);
case UNION:
return field;
}
return null;
}","/** 
 * Translate a field that fits a   {@link Schema} field into a type that Hive understands.For example, a ByteBuffer is allowed by schema but Hive only understands byte arrays, so all ByteBuffers must be changed into byte arrays. Reflection is used to examine java objects if the expected hive type is a struct.
 * @param field value of the field to deserialize.
 * @param typeInfo type of the field as expected by Hive.
 * @param schema schema of the field.
 * @return translated field.
 * @throws NoSuchFieldException if a struct field was expected but not found in the object.
 * @throws IllegalAccessException if a struct field was not accessible.
 */
private Object deserializeField(Object field,TypeInfo typeInfo,Schema schema) throws NoSuchFieldException, IllegalAccessException {
  boolean isNullable=schema.isNullable();
  if (field == null) {
    if (isNullable) {
      return null;
    }
 else {
      throw new UnexpectedFormatException(""String_Node_Str"");
    }
  }
  if (isNullable) {
    schema=schema.getNonNullable();
  }
switch (typeInfo.getCategory()) {
case PRIMITIVE:
    return deserializePrimitive(field,(PrimitiveTypeInfo)typeInfo);
case LIST:
  ListTypeInfo listTypeInfo=(ListTypeInfo)typeInfo;
if (isByteArray(listTypeInfo) && !(field instanceof Collection)) {
  return deserializeByteArray(field);
}
return deserializeList(field,(ListTypeInfo)typeInfo,schema.getComponentSchema());
case MAP:
return deserializeMap(field,(MapTypeInfo)typeInfo,schema.getMapSchema());
case STRUCT:
StructTypeInfo structTypeInfo=(StructTypeInfo)typeInfo;
ArrayList<String> innerFieldNames=structTypeInfo.getAllStructFieldNames();
ArrayList<TypeInfo> innerFieldTypes=structTypeInfo.getAllStructFieldTypeInfos();
return flattenRecord(field,innerFieldNames,innerFieldTypes,schema);
case UNION:
return field;
}
return null;
}"
7340,"@Test public void testIdentityTranslations() throws Exception {
  List<String> names=Lists.newArrayList(""String_Node_Str"");
  ObjectDeserializer deserializer=new ObjectDeserializer(names,Lists.<TypeInfo>newArrayList(TypeInfoFactory.stringTypeInfo),Schema.of(Schema.Type.STRING));
  Assert.assertEquals(""String_Node_Str"",deserializer.deserialize(""String_Node_Str""));
  deserializer=new ObjectDeserializer(names,Lists.<TypeInfo>newArrayList(TypeInfoFactory.intTypeInfo),Schema.of(Schema.Type.INT));
  Assert.assertEquals(Integer.MIN_VALUE,deserializer.deserialize(Integer.MIN_VALUE));
  deserializer=new ObjectDeserializer(names,Lists.<TypeInfo>newArrayList(TypeInfoFactory.longTypeInfo),Schema.of(Schema.Type.LONG));
  Assert.assertEquals(Long.MAX_VALUE,deserializer.deserialize(Long.MAX_VALUE));
  deserializer=new ObjectDeserializer(names,Lists.<TypeInfo>newArrayList(TypeInfoFactory.booleanTypeInfo),Schema.of(Schema.Type.BOOLEAN));
  Assert.assertTrue((Boolean)deserializer.deserialize(true));
  deserializer=new ObjectDeserializer(names,Lists.<TypeInfo>newArrayList(TypeInfoFactory.floatTypeInfo),Schema.of(Schema.Type.FLOAT));
  Assert.assertEquals(3.14f,deserializer.deserialize(3.14f));
  deserializer=new ObjectDeserializer(names,Lists.<TypeInfo>newArrayList(TypeInfoFactory.doubleTypeInfo),Schema.of(Schema.Type.DOUBLE));
  Assert.assertEquals(3.14,deserializer.deserialize(3.14));
  deserializer=new ObjectDeserializer(names,Lists.<TypeInfo>newArrayList(TypeInfoFactory.binaryTypeInfo),Schema.of(Schema.Type.BYTES));
  Assert.assertArrayEquals(new byte[]{1,2,3},(byte[])deserializer.deserialize(new byte[]{1,2,3}));
}","@Test public void testIdentityTranslations() throws Exception {
  List<String> names=Lists.newArrayList(""String_Node_Str"");
  ObjectDeserializer deserializer=new ObjectDeserializer(names,Lists.<TypeInfo>newArrayList(TypeInfoFactory.stringTypeInfo),Schema.of(Schema.Type.STRING));
  Assert.assertEquals(""String_Node_Str"",deserializer.deserialize(""String_Node_Str""));
  deserializer=new ObjectDeserializer(names,Lists.<TypeInfo>newArrayList(TypeInfoFactory.intTypeInfo),Schema.of(Schema.Type.INT));
  Assert.assertEquals(Integer.MIN_VALUE,deserializer.deserialize(Integer.MIN_VALUE));
  deserializer=new ObjectDeserializer(names,Lists.<TypeInfo>newArrayList(TypeInfoFactory.longTypeInfo),Schema.of(Schema.Type.LONG));
  Assert.assertEquals(Long.MAX_VALUE,deserializer.deserialize(Long.MAX_VALUE));
  deserializer=new ObjectDeserializer(names,Lists.<TypeInfo>newArrayList(TypeInfoFactory.booleanTypeInfo),Schema.of(Schema.Type.BOOLEAN));
  Assert.assertTrue((Boolean)deserializer.deserialize(true));
  deserializer=new ObjectDeserializer(names,Lists.<TypeInfo>newArrayList(TypeInfoFactory.floatTypeInfo),Schema.of(Schema.Type.FLOAT));
  Assert.assertEquals(3.14f,deserializer.deserialize(3.14f));
  deserializer=new ObjectDeserializer(names,Lists.<TypeInfo>newArrayList(TypeInfoFactory.doubleTypeInfo),Schema.of(Schema.Type.DOUBLE));
  Assert.assertEquals(3.14,deserializer.deserialize(3.14));
  deserializer=new ObjectDeserializer(names,Lists.<TypeInfo>newArrayList(TypeInfoFactory.binaryTypeInfo),Schema.of(Schema.Type.BYTES));
  Assert.assertArrayEquals(new byte[]{1,2,3},(byte[])deserializer.deserialize(new byte[]{1,2,3}));
  deserializer=new ObjectDeserializer(names,Lists.newArrayList(TypeInfoFactory.getListTypeInfo(TypeInfoFactory.byteTypeInfo)),Schema.of(Schema.Type.BYTES));
  Assert.assertArrayEquals(new Byte[]{1,2,3},(Byte[])deserializer.deserialize(new byte[]{1,2,3}));
  Assert.assertArrayEquals(new Byte[]{1,2,3},(Byte[])deserializer.deserialize(ByteBuffer.wrap(new byte[]{1,2,3})));
}"
7341,"@Override public <T extends DatasetDefinition>T get(String datasetTypeName){
  T def;
  Id.DatasetType datasetTypeId=Id.DatasetType.from(namespaceId,datasetTypeName);
  if (registry.hasType(datasetTypeName)) {
    def=registry.get(datasetTypeName);
  }
 else {
    DatasetTypeMeta typeMeta=datasets.getTypeMDS().getType(datasetTypeId);
    if (typeMeta == null) {
      datasetTypeId=Id.DatasetType.from(Constants.SYSTEM_NAMESPACE_ID,datasetTypeName);
      typeMeta=datasets.getTypeMDS().getType(datasetTypeId);
      if (typeMeta == null) {
        throw new IllegalArgumentException(""String_Node_Str"" + datasetTypeName);
      }
    }
    try {
      def=new DatasetDefinitionLoader(locationFactory).load(typeMeta,registry);
    }
 catch (    IOException e) {
      throw Throwables.propagate(e);
    }
  }
  usedTypes.add(datasetTypeId);
  return def;
}","@Override public <T extends DatasetDefinition>T get(String datasetTypeName){
  T def;
  Id.DatasetType datasetTypeId=Id.DatasetType.from(namespaceId,datasetTypeName);
  DatasetTypeMeta typeMeta=datasets.getTypeMDS().getType(datasetTypeId);
  if (typeMeta == null) {
    datasetTypeId=Id.DatasetType.from(Constants.SYSTEM_NAMESPACE_ID,datasetTypeName);
    typeMeta=datasets.getTypeMDS().getType(datasetTypeId);
    if (typeMeta == null) {
      throw new IllegalArgumentException(""String_Node_Str"" + datasetTypeName);
    }
  }
  if (registry.hasType(datasetTypeName)) {
    def=registry.get(datasetTypeName);
  }
 else {
    try {
      def=new DatasetDefinitionLoader(locationFactory).load(typeMeta,registry);
    }
 catch (    IOException e) {
      throw Throwables.propagate(e);
    }
  }
  usedTypes.add(datasetTypeId);
  return def;
}"
7342,"private Store getStore(){
  if (store == null) {
    store=storeFactory.create();
  }
  return store;
}","private synchronized Store getStore(){
  if (store == null) {
    store=storeFactory.create();
  }
  return store;
}"
7343,"@Override public void run(){
  if (activeTasks > 0) {
    long size=pollStream();
    received(new StreamSizeNotification(System.currentTimeMillis(),size),null);
  }
}","@Override public void run(){
  if (activeTasks.get() > 0) {
    long size=pollStream();
    received(new StreamSizeNotification(System.currentTimeMillis(),size),null);
  }
}"
7344,"private StreamSubscriber(Id.Stream streamId){
  this.streamId=streamId;
  this.scheduleTasks=Maps.newConcurrentMap();
  this.activeTasks=0;
}","private StreamSubscriber(Id.Stream streamId){
  this.streamId=streamId;
  this.scheduleTasks=Maps.newConcurrentMap();
  this.lastNotificationLock=new Object();
  this.activeTasks=new AtomicInteger(0);
}"
7345,"/** 
 * Add a new scheduling task based on the data received by the stream referenced by   {@code this} object.
 */
public void createScheduleTask(Id.Program programId,SchedulableProgramType programType,StreamSizeSchedule streamSizeSchedule,boolean active,long baseRunSize,long baseRunTs,boolean persist){
  StreamSizeScheduleTask newTask=new StreamSizeScheduleTask(programId,programType,streamSizeSchedule);
synchronized (this) {
    StreamSizeScheduleTask previous=scheduleTasks.putIfAbsent(getScheduleId(programId,programType,streamSizeSchedule.getName()),newTask);
    if (previous == null) {
      if (active) {
        activeTasks++;
      }
      if (baseRunSize == -1 && baseRunTs == -1) {
        long baseTs=System.currentTimeMillis();
        long baseSize=pollStream();
        newTask.startSchedule(baseSize,baseTs,active,persist);
        lastNotification=new StreamSizeNotification(baseTs,baseSize);
        received(lastNotification,null);
      }
 else {
        newTask.startSchedule(baseRunSize,baseRunTs,active,persist);
      }
    }
  }
}","/** 
 * Add a new scheduling task based on the data received by the stream referenced by   {@code this} object.
 */
public void createScheduleTask(Id.Program programId,SchedulableProgramType programType,StreamSizeSchedule streamSizeSchedule,boolean active,long baseRunSize,long baseRunTs,boolean persist){
  StreamSizeScheduleTask newTask=new StreamSizeScheduleTask(programId,programType,streamSizeSchedule);
  StreamSizeScheduleTask previous=scheduleTasks.putIfAbsent(getScheduleId(programId,programType,streamSizeSchedule.getName()),newTask);
  if (previous != null) {
    return;
  }
  if (baseRunSize == -1 && baseRunTs == -1) {
    long baseTs=System.currentTimeMillis();
    long baseSize=pollStream();
    newTask.startSchedule(baseSize,baseTs,active,persist);
synchronized (lastNotificationLock) {
      lastNotification=new StreamSizeNotification(baseTs,baseSize);
    }
  }
 else {
    newTask.startSchedule(baseRunSize,baseRunTs,active,persist);
  }
  if (active) {
    activeTasks.incrementAndGet();
  }
  if (lastNotification != null) {
    received(lastNotification,null);
  }
}"
7346,"/** 
 * Delete a scheduling task that is based on the data received by the stream referenced by   {@code this} object.
 */
public void deleteSchedule(Id.Program programId,SchedulableProgramType programType,String scheduleName){
  StreamSizeScheduleTask scheduleTask=scheduleTasks.remove(getScheduleId(programId,programType,scheduleName));
  if (scheduleTask != null && scheduleTask.isRunning()) {
    activeTasks--;
  }
}","/** 
 * Delete a scheduling task that is based on the data received by the stream referenced by   {@code this} object.
 */
public void deleteSchedule(Id.Program programId,SchedulableProgramType programType,String scheduleName){
  StreamSizeScheduleTask scheduleTask=scheduleTasks.remove(getScheduleId(programId,programType,scheduleName));
  if (scheduleTask != null && scheduleTask.isRunning()) {
    activeTasks.decrementAndGet();
  }
}"
7347,"/** 
 * Suspend a scheduling task that is based on the data received by the stream referenced by   {@code this} object.
 */
public void suspendScheduleTask(Id.Program programId,SchedulableProgramType programType,String scheduleName){
  StreamSizeScheduleTask task=scheduleTasks.get(getScheduleId(programId,programType,scheduleName));
  if (task == null) {
    return;
  }
synchronized (this) {
    if (task.suspend()) {
      activeTasks--;
    }
  }
}","/** 
 * Suspend a scheduling task that is based on the data received by the stream referenced by   {@code this} object.
 */
public void suspendScheduleTask(Id.Program programId,SchedulableProgramType programType,String scheduleName){
  StreamSizeScheduleTask task=scheduleTasks.get(getScheduleId(programId,programType,scheduleName));
  if (task == null) {
    return;
  }
synchronized (this) {
    if (task.suspend()) {
      activeTasks.decrementAndGet();
    }
  }
}"
7348,"public void received(StreamSizeNotification notification){
  long pastRunSize;
  long pastRunTs;
synchronized (this) {
    if (notification.getSize() < baseSize) {
      baseSize=notification.getSize();
      baseTs=notification.getTimestamp();
      return;
    }
    if (notification.getSize() < baseSize + toBytes(streamSizeSchedule.getDataTriggerMB())) {
      return;
    }
    pastRunSize=baseSize;
    pastRunTs=baseTs;
    baseSize=notification.getSize();
    baseTs=notification.getTimestamp();
    LOG.debug(""String_Node_Str"",baseSize,baseTs,streamSizeSchedule);
  }
  Arguments args=new BasicArguments(ImmutableMap.of(ProgramOptionConstants.SCHEDULE_NAME,streamSizeSchedule.getName(),ProgramOptionConstants.LOGICAL_START_TIME,Long.toString(baseTs),ProgramOptionConstants.RUN_DATA_SIZE,Long.toString(baseSize),ProgramOptionConstants.PAST_RUN_LOGICAL_START_TIME,Long.toString(pastRunTs),ProgramOptionConstants.PAST_RUN_DATA_SIZE,Long.toString(pastRunSize)));
  while (true) {
    ScheduleTaskRunner taskRunner=new ScheduleTaskRunner(getStore(),programRuntimeService,preferencesStore);
    try {
      LOG.info(""String_Node_Str"",streamSizeSchedule);
      taskRunner.run(programId,ProgramType.valueOf(programType.name()),args);
      break;
    }
 catch (    TaskExecutionException e) {
      LOG.error(""String_Node_Str"",streamSizeSchedule.getName(),e);
      if (e.isRefireImmediately()) {
        LOG.info(""String_Node_Str"",streamSizeSchedule.getName());
      }
 else {
        break;
      }
    }
  }
}","public void received(StreamSizeNotification notification){
  if (!running) {
    return;
  }
  long pastRunSize;
  long pastRunTs;
synchronized (this) {
    if (notification.getSize() < baseSize) {
      baseSize=notification.getSize();
      baseTs=notification.getTimestamp();
      return;
    }
    if (notification.getSize() < baseSize + toBytes(streamSizeSchedule.getDataTriggerMB())) {
      return;
    }
    pastRunSize=baseSize;
    pastRunTs=baseTs;
    baseSize=notification.getSize();
    baseTs=notification.getTimestamp();
    LOG.debug(""String_Node_Str"",baseSize,baseTs,streamSizeSchedule);
  }
  Arguments args=new BasicArguments(ImmutableMap.of(ProgramOptionConstants.SCHEDULE_NAME,streamSizeSchedule.getName(),ProgramOptionConstants.LOGICAL_START_TIME,Long.toString(baseTs),ProgramOptionConstants.RUN_DATA_SIZE,Long.toString(baseSize),ProgramOptionConstants.PAST_RUN_LOGICAL_START_TIME,Long.toString(pastRunTs),ProgramOptionConstants.PAST_RUN_DATA_SIZE,Long.toString(pastRunSize)));
  while (true) {
    ScheduleTaskRunner taskRunner=new ScheduleTaskRunner(getStore(),programRuntimeService,preferencesStore);
    try {
      LOG.info(""String_Node_Str"",streamSizeSchedule);
      taskRunner.run(programId,ProgramType.valueOf(programType.name()),args);
      break;
    }
 catch (    TaskExecutionException e) {
      LOG.error(""String_Node_Str"",streamSizeSchedule.getName(),e);
      if (e.isRefireImmediately()) {
        LOG.info(""String_Node_Str"",streamSizeSchedule.getName());
      }
 else {
        break;
      }
    }
  }
}"
7349,"/** 
 * Resume a scheduling task that is based on the data received by the stream referenced by   {@code this} object.
 */
public void resumeScheduleTask(Id.Program programId,SchedulableProgramType programType,String scheduleName){
synchronized (this) {
    StreamSizeScheduleTask task=scheduleTasks.get(getScheduleId(programId,programType,scheduleName));
    if (task == null) {
      return;
    }
    if (task.resume() && ++activeTasks == 1) {
      if (lastNotification != null) {
        long lastNotificationTs=lastNotification.getTimestamp();
        if (lastNotificationTs + TimeUnit.SECONDS.toMillis(pollingDelay) <= System.currentTimeMillis()) {
          long streamSize=pollStream();
          lastNotification=new StreamSizeNotification(System.currentTimeMillis(),streamSize);
        }
      }
    }
    if (lastNotification != null) {
      task.received(lastNotification);
    }
  }
}","/** 
 * Resume a scheduling task that is based on the data received by the stream referenced by   {@code this} object.
 */
public void resumeScheduleTask(Id.Program programId,SchedulableProgramType programType,String scheduleName){
  StreamSizeScheduleTask task=scheduleTasks.get(getScheduleId(programId,programType,scheduleName));
  if (task == null || !task.resume()) {
    return;
  }
  if (activeTasks.incrementAndGet() == 1) {
synchronized (lastNotificationLock) {
      if (lastNotification == null || (lastNotification.getTimestamp() + TimeUnit.SECONDS.toMillis(pollingDelay) <= System.currentTimeMillis())) {
        long streamSize=pollStream();
        lastNotification=new StreamSizeNotification(System.currentTimeMillis(),streamSize);
      }
    }
  }
  task.received(lastNotification);
}"
7350,"/** 
 * @return a runnable that uses the {@link StreamAdmin} to poll the stream size, and creates a fake notificationwith that size, so that this information can be treated as if it came from a real notification.
 */
private Runnable createPollingRunnable(){
  return new Runnable(){
    @Override public void run(){
      if (activeTasks > 0) {
        long size=pollStream();
        received(new StreamSizeNotification(System.currentTimeMillis(),size),null);
      }
    }
  }
;
}","/** 
 * @return a runnable that uses the {@link StreamAdmin} to poll the stream size, and creates a fake notificationwith that size, so that this information can be treated as if it came from a real notification.
 */
private Runnable createPollingRunnable(){
  return new Runnable(){
    @Override public void run(){
      if (activeTasks.get() > 0) {
        long size=pollStream();
        received(new StreamSizeNotification(System.currentTimeMillis(),size),null);
      }
    }
  }
;
}"
7351,"public static void resetAll() throws Exception {
  metricStore.deleteBefore(System.currentTimeMillis());
}","public static void resetAll() throws Exception {
  metricStore.deleteBefore(System.currentTimeMillis() / 1000);
}"
7352,"@Nullable private MetricsRecord getMetricsRecord(MetricValue metricValue,Rule rule){
  String runId=metricValue.getTags().get(Constants.Metrics.Tag.RUN_ID);
  runId=runId == null ? ""String_Node_Str"" : runId;
  MetricsRecordBuilder builder=new MetricsRecordBuilder(runId,metricValue.getName(),metricValue.getTimestamp(),metricValue.getValue(),metricValue.getType());
  String namespace=metricValue.getTags().get(Constants.Metrics.Tag.NAMESPACE);
  namespace=namespace == null ? Constants.SYSTEM_NAMESPACE : namespace;
  int index=0;
  addToContext(builder,Constants.Metrics.Tag.NAMESPACE,namespace,index);
  for (  String tagName : rule.tagsToPutIntoContext) {
    String tagValue=metricValue.getTags().get(tagName);
    if (tagValue != null) {
      addToContext(builder,tagName,tagValue,index);
    }
 else {
      return null;
    }
  }
  for (  String tagName : rule.tagsToPutIntoTags) {
    String tagValue=metricValue.getTags().get(tagName);
    if (tagValue != null) {
      builder.addTag(tagValue);
    }
  }
  String scope=metricValue.getTags().get(Constants.Metrics.Tag.SCOPE);
  builder.prefixMetricName(scope == null ? ""String_Node_Str"" : scope + ""String_Node_Str"");
  return builder.build();
}","@Nullable private MetricsRecord getMetricsRecord(MetricValue metricValue,Rule rule){
  String runId=metricValue.getTags().get(Constants.Metrics.Tag.RUN_ID);
  runId=runId == null ? ""String_Node_Str"" : runId;
  MetricsRecordBuilder builder=new MetricsRecordBuilder(runId,metricValue.getName(),metricValue.getTimestamp(),metricValue.getValue(),metricValue.getType());
  String namespace=metricValue.getTags().get(Constants.Metrics.Tag.NAMESPACE);
  namespace=namespace == null ? Constants.SYSTEM_NAMESPACE : namespace;
  int index=0;
  addToContext(builder,Constants.Metrics.Tag.NAMESPACE,namespace,index++);
  for (  String tagName : rule.tagsToPutIntoContext) {
    String tagValue=metricValue.getTags().get(tagName);
    if (tagValue != null) {
      addToContext(builder,tagName,tagValue,index++);
    }
 else {
      return null;
    }
  }
  String instanceId=metricValue.getTags().get(Constants.Metrics.Tag.INSTANCE_ID);
  if (instanceId != null) {
    addToContext(builder,Constants.Metrics.Tag.INSTANCE_ID,instanceId,index++);
  }
  for (  String tagName : rule.tagsToPutIntoTags) {
    String tagValue=metricValue.getTags().get(tagName);
    if (tagValue != null) {
      builder.addTag(tagValue);
    }
  }
  String scope=metricValue.getTags().get(Constants.Metrics.Tag.SCOPE);
  builder.prefixMetricName(scope == null ? ""String_Node_Str"" : scope + ""String_Node_Str"");
  return builder.build();
}"
7353,"private void addToContext(MetricsRecordBuilder builder,String tagName,String tagValue,int index){
  if (Constants.Metrics.Tag.DATASET.equals(tagName) && index == 0) {
    builder.appendContext(""String_Node_Str"");
  }
 else   if (Constants.Metrics.Tag.CLUSTER_METRICS.equals(tagName) && index == 0) {
    builder.appendContext(""String_Node_Str"");
  }
 else {
    builder.appendContext(tagValue);
  }
}","private void addToContext(MetricsRecordBuilder builder,String tagName,String tagValue,int index){
  if (Constants.Metrics.Tag.DATASET.equals(tagName) && index == 1) {
    builder.appendContext(""String_Node_Str"");
  }
 else   if (Constants.Metrics.Tag.CLUSTER_METRICS.equals(tagName) && index == 1) {
    builder.appendContext(""String_Node_Str"");
  }
 else {
    builder.appendContext(tagValue);
  }
}"
7354,"@Test @Ignore public void testConcurrentIncrement() throws Exception {
  final MetricsTable table=getTable(""String_Node_Str"");
  final int rounds=500;
  Map<byte[],Long> inc1=ImmutableMap.of(X,1L,Y,2L);
  Map<byte[],Long> inc2=ImmutableMap.of(Y,1L,Z,2L);
  Collection<Thread> threads=ImmutableList.of(new IncThread(table,A,inc1,rounds),new IncThread(table,A,inc2,rounds),new IncAndGetThread(table,A,Z,5,rounds),new IncAndGetThread(table,A,Z,2,rounds));
  for (  Thread t : threads) {
    t.start();
  }
  for (  Thread t : threads) {
    t.join();
  }
  Assert.assertEquals(rounds + 10L,table.incrementAndGet(A,X,10L));
  Assert.assertEquals(3 * rounds - 20L,table.incrementAndGet(A,Y,-20L));
  Assert.assertEquals(9 * rounds,table.incrementAndGet(A,Z,0L));
}","@Test public void testConcurrentIncrement() throws Exception {
  final MetricsTable table=getTable(""String_Node_Str"");
  final int rounds=500;
  Map<byte[],Long> inc1=ImmutableMap.of(X,1L,Y,2L);
  Map<byte[],Long> inc2=ImmutableMap.of(Y,1L,Z,2L);
  Collection<? extends Thread> threads=ImmutableList.of(new IncThread(table,A,inc1,rounds),new IncThread(table,A,inc2,rounds),new IncAndGetThread(table,A,Z,5,rounds),new IncAndGetThread(table,A,Z,2,rounds));
  for (  Thread t : threads) {
    t.start();
  }
  for (  Thread t : threads) {
    t.join();
    if (t instanceof Closeable) {
      ((Closeable)t).close();
    }
  }
  Assert.assertEquals(rounds + 10L,table.incrementAndGet(A,X,10L));
  Assert.assertEquals(3 * rounds - 20L,table.incrementAndGet(A,Y,-20L));
  Assert.assertEquals(9 * rounds,table.incrementAndGet(A,Z,0L));
}"
7355,"IncAndGetThread(MetricsTable table,byte[] row,byte[] col,long delta,int rounds){
  this.table=table;
  this.row=row;
  this.col=col;
  this.delta=delta;
  this.rounds=rounds;
}","public IncAndGetThread(MetricsTable table,byte[] row,byte[] col,long delta,int rounds){
  this.table=table;
  this.row=row;
  this.col=col;
  this.delta=delta;
  this.rounds=rounds;
}"
7356,"IncThread(MetricsTable table,byte[] row,Map<byte[],Long> incrememts,int rounds){
  this.table=table;
  this.row=row;
  this.incrememts=incrememts;
  this.rounds=rounds;
}","public IncThread(MetricsTable table,byte[] row,Map<byte[],Long> incrememts,int rounds){
  this.table=table;
  this.row=row;
  this.incrememts=incrememts;
  this.rounds=rounds;
}"
7357,"@Nullable @Override public ResourceRequirement apply(@Nullable ResourceRequirement existingRequirement){
  Set<ResourceRequirement.Partition> partitions;
  if (existingRequirement != null) {
    partitions=existingRequirement.getPartitions();
  }
 else {
    partitions=ImmutableSet.of();
  }
  ResourceRequirement.Partition newPartition=new ResourceRequirement.Partition(streamName,1);
  if (partitions.contains(newPartition)) {
    return null;
  }
  ResourceRequirement.Builder builder=ResourceRequirement.builder(Constants.Service.STREAMS);
  builder.addPartition(newPartition);
  for (  ResourceRequirement.Partition partition : partitions) {
    builder.addPartition(partition);
  }
  return builder.build();
}","@Nullable @Override public ResourceRequirement apply(@Nullable ResourceRequirement existingRequirement){
  LOG.debug(""String_Node_Str"",streamName);
  Set<ResourceRequirement.Partition> partitions;
  if (existingRequirement != null) {
    partitions=existingRequirement.getPartitions();
  }
 else {
    partitions=ImmutableSet.of();
  }
  ResourceRequirement.Partition newPartition=new ResourceRequirement.Partition(streamName,1);
  if (partitions.contains(newPartition)) {
    return null;
  }
  ResourceRequirement.Builder builder=ResourceRequirement.builder(Constants.Service.STREAMS);
  builder.addPartition(newPartition);
  for (  ResourceRequirement.Partition partition : partitions) {
    builder.addPartition(partition);
  }
  return builder.build();
}"
7358,"@Override public ListenableFuture<Void> streamCreated(final String streamName){
  ListenableFuture<ResourceRequirement> future=resourceCoordinatorClient.modifyRequirement(Constants.Service.STREAMS,new ResourceModifier(){
    @Nullable @Override public ResourceRequirement apply(    @Nullable ResourceRequirement existingRequirement){
      Set<ResourceRequirement.Partition> partitions;
      if (existingRequirement != null) {
        partitions=existingRequirement.getPartitions();
      }
 else {
        partitions=ImmutableSet.of();
      }
      ResourceRequirement.Partition newPartition=new ResourceRequirement.Partition(streamName,1);
      if (partitions.contains(newPartition)) {
        return null;
      }
      ResourceRequirement.Builder builder=ResourceRequirement.builder(Constants.Service.STREAMS);
      builder.addPartition(newPartition);
      for (      ResourceRequirement.Partition partition : partitions) {
        builder.addPartition(partition);
      }
      return builder.build();
    }
  }
);
  return Futures.transform(future,Functions.<Void>constant(null));
}","@Override public ListenableFuture<Void> streamCreated(final String streamName){
  ListenableFuture<ResourceRequirement> future=resourceCoordinatorClient.modifyRequirement(Constants.Service.STREAMS,new ResourceModifier(){
    @Nullable @Override public ResourceRequirement apply(    @Nullable ResourceRequirement existingRequirement){
      LOG.debug(""String_Node_Str"",streamName);
      Set<ResourceRequirement.Partition> partitions;
      if (existingRequirement != null) {
        partitions=existingRequirement.getPartitions();
      }
 else {
        partitions=ImmutableSet.of();
      }
      ResourceRequirement.Partition newPartition=new ResourceRequirement.Partition(streamName,1);
      if (partitions.contains(newPartition)) {
        return null;
      }
      ResourceRequirement.Builder builder=ResourceRequirement.builder(Constants.Service.STREAMS);
      builder.addPartition(newPartition);
      for (      ResourceRequirement.Partition partition : partitions) {
        builder.addPartition(partition);
      }
      return builder.build();
    }
  }
);
  return Futures.transform(future,Functions.<Void>constant(null));
}"
7359,"@Override public synchronized void received(String streamName,long dataSize){
  AtomicLong value=streamSizes.get(streamName);
  if (value == null) {
    value=streamSizes.putIfAbsent(streamName,new AtomicLong(dataSize));
    if (value == null) {
      truncationSubscriptions.add(streamCoordinatorClient.addListener(streamName,new StreamPropertyListener(){
        @Override public void generationChanged(        String streamName,        int generation){
          streamSizes.put(streamName,new AtomicLong(0));
        }
      }
));
    }
  }
  if (value != null) {
    value.addAndGet(dataSize);
  }
}","@Override public synchronized void received(String streamName,long dataSize){
  AtomicLong value=streamSizes.get(streamName);
  if (value == null) {
    value=streamSizes.putIfAbsent(streamName,new AtomicLong(dataSize));
    if (value == null) {
      truncationSubscriptions.add(streamCoordinatorClient.addListener(streamName,new StreamPropertyListener(){
        @Override public void generationChanged(        String streamName,        int generation){
          streamSizes.put(streamName,new AtomicLong(0));
        }
      }
));
    }
  }
  if (value != null) {
    value.addAndGet(dataSize);
  }
  LOG.trace(""String_Node_Str"",streamName,dataSize,value.get());
}"
7360,"/** 
 * Elect one leader among the   {@link DistributedStreamService}s running in different Twill runnables.
 */
private void performLeaderElection(){
  leaderElection=new LeaderElection(zkClient,""String_Node_Str"" + STREAMS_COORDINATOR,new ElectionHandler(){
    @Override public void leader(){
      LOG.info(""String_Node_Str"");
      resourceCoordinator=new ResourceCoordinator(zkClient,discoveryServiceClient,new BalancedAssignmentStrategy());
      resourceCoordinator.startAndWait();
      resourceCoordinatorClient.modifyRequirement(Constants.Service.STREAMS,new ResourceModifier(){
        @Nullable @Override public ResourceRequirement apply(        @Nullable ResourceRequirement existingRequirement){
          try {
            ResourceRequirement.Builder builder=ResourceRequirement.builder(Constants.Service.STREAMS);
            for (            StreamSpecification spec : streamMetaStore.listStreams()) {
              LOG.debug(""String_Node_Str"",spec.getName());
              builder.addPartition(new ResourceRequirement.Partition(spec.getName(),1));
            }
            return builder.build();
          }
 catch (          Throwable e) {
            LOG.error(""String_Node_Str"",e);
            Throwables.propagate(e);
            return null;
          }
        }
      }
);
    }
    @Override public void follower(){
      LOG.info(""String_Node_Str"");
      if (resourceCoordinator != null) {
        resourceCoordinator.stopAndWait();
      }
    }
  }
);
}","/** 
 * Elect one leader among the   {@link DistributedStreamService}s running in different Twill runnables.
 */
private void performLeaderElection(){
  leaderElection=new LeaderElection(zkClient,""String_Node_Str"" + STREAMS_COORDINATOR,new ElectionHandler(){
    @Override public void leader(){
      LOG.info(""String_Node_Str"");
      resourceCoordinator=new ResourceCoordinator(zkClient,discoveryServiceClient,new BalancedAssignmentStrategy());
      resourceCoordinator.startAndWait();
      resourceCoordinatorClient.modifyRequirement(Constants.Service.STREAMS,new ResourceModifier(){
        @Nullable @Override public ResourceRequirement apply(        @Nullable ResourceRequirement existingRequirement){
          try {
            ResourceRequirement.Builder builder=ResourceRequirement.builder(Constants.Service.STREAMS);
            for (            StreamSpecification spec : streamMetaStore.listStreams()) {
              LOG.debug(""String_Node_Str"",spec.getName());
              builder.addPartition(new ResourceRequirement.Partition(spec.getName(),1));
            }
            return builder.build();
          }
 catch (          Throwable e) {
            LOG.error(""String_Node_Str"",e);
            Throwables.propagate(e);
            return null;
          }
        }
      }
);
    }
    @Override public void follower(){
      LOG.info(""String_Node_Str"");
      if (resourceCoordinator != null) {
        resourceCoordinator.stopAndWait();
      }
    }
  }
);
  leaderElection.start();
}"
7361,"protected StreamSizeAggregator(String streamName,long baseCount,int streamThresholdMB,Cancellable cancellable){
  this.streamWriterSizes=Maps.newHashMap();
  this.streamBaseCount=new AtomicLong(baseCount);
  this.streamThresholdMB=new AtomicInteger(streamThresholdMB);
  this.cancellable=cancellable;
  this.streamFeed=new NotificationFeed.Builder().setNamespace(Constants.DEFAULT_NAMESPACE).setCategory(Constants.Notification.Stream.STREAM_FEED_CATEGORY).setName(streamName).build();
}","protected StreamSizeAggregator(String streamName,long baseCount,int streamThresholdMB,Cancellable cancellable){
  this.streamWriterSizes=Maps.newHashMap();
  this.streamBaseCount=new AtomicLong(baseCount);
  this.countFromFiles=new AtomicLong(baseCount);
  this.streamThresholdMB=new AtomicInteger(streamThresholdMB);
  this.cancellable=cancellable;
  this.isInit=true;
  this.streamFeed=new NotificationFeed.Builder().setNamespace(Constants.DEFAULT_NAMESPACE).setCategory(Constants.Notification.Stream.STREAM_FEED_CATEGORY).setName(streamName).build();
}"
7362,"/** 
 * Perform aggregation on the Streams described by the   {@code streamNames}, and no other Streams. If aggregation was previously done on other Streams, those must be cancelled.
 * @param streamNames names of the streams to perform data sizes aggregation on
 */
private void aggregate(Set<String> streamNames){
  Set<String> existingAggregators=Sets.newHashSet(aggregators.keySet());
  for (  String streamName : streamNames) {
    if (existingAggregators.remove(streamName)) {
      continue;
    }
    try {
      StreamConfig config=streamAdmin.getConfig(streamName);
      long filesSize=StreamUtils.fetchStreamFilesSize(config);
      createSizeAggregator(streamName,filesSize,config.getNotificationThresholdMB());
    }
 catch (    IOException e) {
      LOG.error(""String_Node_Str"",streamName);
      Throwables.propagate(e);
    }
  }
  for (  String outdatedStream : existingAggregators) {
    StreamSizeAggregator aggregator=aggregators.remove(outdatedStream);
    if (aggregator != null) {
      aggregator.cancel();
    }
  }
}","/** 
 * Perform aggregation on the Streams described by the   {@code streamNames}, and no other Streams. If aggregation was previously done on other Streams, those must be cancelled.
 * @param streamNames names of the streams to perform data sizes aggregation on
 */
private void aggregate(Set<String> streamNames){
  Set<String> existingAggregators=Sets.newHashSet(aggregators.keySet());
  for (  String streamName : streamNames) {
    if (existingAggregators.remove(streamName)) {
      continue;
    }
    try {
      StreamConfig config=streamAdmin.getConfig(streamName);
      long filesSize=StreamUtils.fetchStreamFilesSize(config);
      LOG.debug(""String_Node_Str"",streamName,filesSize);
      createSizeAggregator(streamName,filesSize,config.getNotificationThresholdMB());
    }
 catch (    IOException e) {
      LOG.error(""String_Node_Str"",streamName);
      Throwables.propagate(e);
    }
  }
  for (  String outdatedStream : existingAggregators) {
    StreamSizeAggregator aggregator=aggregators.remove(outdatedStream);
    if (aggregator != null) {
      aggregator.cancel();
    }
  }
}"
7363,"/** 
 * Create a new aggregator for the   {@code streamName}, and add it to the existing map of   {@link Cancellable}{@code aggregators}. This method does not cancel previously existing aggregator associated to the  {@code streamName}.
 * @param streamName stream name to create a new aggregator for
 * @param baseCount stream size from which to start aggregating
 * @return the created {@link StreamSizeAggregator}
 */
private StreamSizeAggregator createSizeAggregator(String streamName,long baseCount,final int threshold){
  final Cancellable thresholdSubscription=getStreamCoordinatorClient().addListener(streamName,new StreamPropertyListener(){
    @Override public void thresholdChanged(    String streamName,    int threshold){
      StreamSizeAggregator aggregator=aggregators.get(streamName);
      if (aggregator == null) {
        LOG.warn(""String_Node_Str"",streamName);
        return;
      }
      aggregator.setStreamThresholdMB(threshold);
    }
  }
);
  final Cancellable truncationSubscription=getStreamCoordinatorClient().addListener(streamName,new StreamPropertyListener(){
    @Override public void generationChanged(    String streamName,    int generation){
      Cancellable previousAggregator=aggregators.replace(streamName,createSizeAggregator(streamName,0,threshold));
      if (previousAggregator != null) {
        previousAggregator.cancel();
      }
    }
  }
);
  StreamSizeAggregator newAggregator=new StreamSizeAggregator(streamName,baseCount,threshold,new Cancellable(){
    @Override public void cancel(){
      thresholdSubscription.cancel();
      truncationSubscription.cancel();
    }
  }
);
  aggregators.put(streamName,newAggregator);
  return newAggregator;
}","/** 
 * Create a new aggregator for the   {@code streamName}, and add it to the existing map of   {@link Cancellable}{@code aggregators}. This method does not cancel previously existing aggregator associated to the  {@code streamName}.
 * @param streamName stream name to create a new aggregator for
 * @param baseCount stream size from which to start aggregating
 * @return the created {@link StreamSizeAggregator}
 */
private StreamSizeAggregator createSizeAggregator(String streamName,long baseCount,final int threshold){
  LOG.debug(""String_Node_Str"",streamName);
  final Cancellable thresholdSubscription=getStreamCoordinatorClient().addListener(streamName,new StreamPropertyListener(){
    @Override public void thresholdChanged(    String streamName,    int threshold){
      StreamSizeAggregator aggregator=aggregators.get(streamName);
      if (aggregator == null) {
        LOG.warn(""String_Node_Str"",streamName);
        return;
      }
      aggregator.setStreamThresholdMB(threshold);
    }
  }
);
  final Cancellable truncationSubscription=getStreamCoordinatorClient().addListener(streamName,new StreamPropertyListener(){
    @Override public void generationChanged(    String streamName,    int generation){
      int currentThresold=threshold;
      StreamSizeAggregator currentAggregator=aggregators.get(streamName);
      if (currentAggregator != null) {
        currentThresold=currentAggregator.getStreamThresholdMB();
      }
      Cancellable previousAggregator=aggregators.replace(streamName,createSizeAggregator(streamName,0,currentThresold));
      if (previousAggregator != null) {
        previousAggregator.cancel();
      }
    }
  }
);
  StreamSizeAggregator newAggregator=new StreamSizeAggregator(streamName,baseCount,threshold,new Cancellable(){
    @Override public void cancel(){
      thresholdSubscription.cancel();
      truncationSubscription.cancel();
    }
  }
);
  aggregators.put(streamName,newAggregator);
  return newAggregator;
}"
7364,"/** 
 * Call all the listeners that are interested in knowing that this coordinator is the leader of a set of Streams.
 * @param streamNames set of Streams that this coordinator is the leader of
 */
private void invokeLeaderListeners(Set<String> streamNames){
  Set<StreamLeaderListener> listeners;
synchronized (this) {
    listeners=ImmutableSet.copyOf(leaderListeners);
  }
  for (  StreamLeaderListener listener : listeners) {
    listener.leaderOf(streamNames);
  }
}","/** 
 * Call all the listeners that are interested in knowing that this Stream writer is the leader of a set of Streams.
 * @param streamNames set of Streams that this coordinator is the leader of
 */
private void invokeLeaderListeners(Set<String> streamNames){
  LOG.debug(""String_Node_Str"",streamNames);
  Set<StreamLeaderListener> listeners;
synchronized (this) {
    listeners=ImmutableSet.copyOf(leaderListeners);
  }
  for (  StreamLeaderListener listener : listeners) {
    listener.leaderOf(streamNames);
  }
}"
7365,"/** 
 * Check if the current size of data is enough to trigger a notification.
 */
private void checkSendNotification(){
  long sum=0;
  for (  Long size : streamWriterSizes.values()) {
    sum+=size;
  }
  if (isInit || sum - streamBaseCount.get() > streamThresholdMB.get()) {
    try {
      publishNotification(sum);
    }
  finally {
      streamBaseCount.set(sum);
    }
  }
}","/** 
 * Check if the current size of data is enough to trigger a notification.
 */
private void checkSendNotification(){
  long sum=countFromFiles.get();
  for (  Long size : streamWriterSizes.values()) {
    sum+=size;
  }
  if (isInit || sum - streamBaseCount.get() > streamThresholdMB.get()) {
    try {
      publishNotification(sum);
    }
  finally {
      streamBaseCount.set(sum);
      countFromFiles.set(0);
    }
  }
  isInit=false;
}"
7366,"@Override protected void runOneIteration() throws Exception {
  LOG.trace(""String_Node_Str"",instanceId);
  ImmutableMap.Builder<String,Long> sizes=ImmutableMap.builder();
  for (  StreamSpecification streamSpec : streamMetaStore.listStreams()) {
    sizes.put(streamSpec.getName(),streamWriterSizeCollector.getTotalCollected(streamSpec.getName()));
  }
  heartbeatPublisher.sendHeartbeat(new StreamWriterHeartbeat(System.currentTimeMillis(),instanceId,sizes.build()));
}","@Override protected void runOneIteration() throws Exception {
  LOG.trace(""String_Node_Str"",instanceId);
  ImmutableMap.Builder<String,Long> sizes=ImmutableMap.builder();
  for (  StreamSpecification streamSpec : streamMetaStore.listStreams()) {
    sizes.put(streamSpec.getName(),streamWriterSizeCollector.getTotalCollected(streamSpec.getName()));
  }
  StreamWriterHeartbeat heartbeat=new StreamWriterHeartbeat(System.currentTimeMillis(),instanceId,sizes.build());
  LOG.trace(""String_Node_Str"",heartbeat);
  heartbeatPublisher.sendHeartbeat(heartbeat);
}"
7367,"/** 
 * Subscribe to the streams heartbeat notification feed. One heartbeat contains data for all existing streams, we filter that to only take into account the streams that this   {@link DistributedStreamService} is a leaderof.
 * @return a {@link Cancellable} to cancel the subscription
 * @throws NotificationException if the heartbeat feed does not exist
 */
private Cancellable subscribeToHeartbeatsFeed() throws NotificationException {
  final NotificationFeed heartbeatsFeed=new NotificationFeed.Builder().setNamespace(Constants.DEFAULT_NAMESPACE).setCategory(Constants.Notification.Stream.STREAM_INTERNAL_FEED_CATEGORY).setName(Constants.Notification.Stream.STREAM_HEARTBEAT_FEED_NAME).build();
  return notificationService.subscribe(heartbeatsFeed,new NotificationHandler<StreamWriterHeartbeat>(){
    @Override public Type getNotificationFeedType(){
      return StreamWriterHeartbeat.class;
    }
    @Override public void received(    StreamWriterHeartbeat heartbeat,    NotificationContext notificationContext){
      for (      Map.Entry<String,Long> entry : heartbeat.getStreamsSizes().entrySet()) {
        StreamSizeAggregator streamSizeAggregator=aggregators.get(entry.getKey());
        if (streamSizeAggregator == null) {
          continue;
        }
        streamSizeAggregator.bytesReceived(heartbeat.getInstanceId(),entry.getValue());
      }
    }
  }
);
}","/** 
 * Subscribe to the streams heartbeat notification feed. One heartbeat contains data for all existing streams, we filter that to only take into account the streams that this   {@link DistributedStreamService} is a leaderof.
 * @return a {@link Cancellable} to cancel the subscription
 * @throws NotificationException if the heartbeat feed does not exist
 */
private Cancellable subscribeToHeartbeatsFeed() throws NotificationException {
  final NotificationFeed heartbeatsFeed=new NotificationFeed.Builder().setNamespace(Constants.DEFAULT_NAMESPACE).setCategory(Constants.Notification.Stream.STREAM_INTERNAL_FEED_CATEGORY).setName(Constants.Notification.Stream.STREAM_HEARTBEAT_FEED_NAME).build();
  LOG.trace(""String_Node_Str"");
  return notificationService.subscribe(heartbeatsFeed,new NotificationHandler<StreamWriterHeartbeat>(){
    @Override public Type getNotificationFeedType(){
      return StreamWriterHeartbeat.class;
    }
    @Override public void received(    StreamWriterHeartbeat heartbeat,    NotificationContext notificationContext){
      LOG.trace(""String_Node_Str"",heartbeat);
      for (      Map.Entry<String,Long> entry : heartbeat.getStreamsSizes().entrySet()) {
        StreamSizeAggregator streamSizeAggregator=aggregators.get(entry.getKey());
        if (streamSizeAggregator == null) {
          LOG.trace(""String_Node_Str"",entry.getKey());
          continue;
        }
        streamSizeAggregator.bytesReceived(heartbeat.getInstanceId(),entry.getValue());
      }
    }
  }
);
}"
7368,"@Override public void onChange(Collection<PartitionReplica> partitionReplicas){
  Set<String> streamNames=ImmutableSet.copyOf(Iterables.transform(partitionReplicas,new Function<PartitionReplica,String>(){
    @Nullable @Override public String apply(    @Nullable PartitionReplica input){
      return input != null ? input.getName() : null;
    }
  }
));
  invokeLeaderListeners(ImmutableSet.copyOf(streamNames));
}","@Override public void onChange(Collection<PartitionReplica> partitionReplicas){
  LOG.debug(""String_Node_Str"");
  Set<String> streamNames=ImmutableSet.copyOf(Iterables.transform(partitionReplicas,new Function<PartitionReplica,String>(){
    @Nullable @Override public String apply(    @Nullable PartitionReplica input){
      return input != null ? input.getName() : null;
    }
  }
));
  invokeLeaderListeners(ImmutableSet.copyOf(streamNames));
}"
7369,"@Inject public DistributedStreamService(CConfiguration cConf,StreamAdmin streamAdmin,StreamCoordinatorClient streamCoordinatorClient,StreamFileJanitorService janitorService,ZKClient zkClient,DiscoveryServiceClient discoveryServiceClient,StreamMetaStore streamMetaStore,Supplier<Discoverable> discoverableSupplier,StreamWriterSizeCollector streamWriterSizeCollector,HeartbeatPublisher heartbeatPublisher,NotificationFeedManager feedManager,NotificationService notificationService){
  super(streamCoordinatorClient,janitorService,streamWriterSizeCollector);
  this.zkClient=zkClient;
  this.streamAdmin=streamAdmin;
  this.notificationService=notificationService;
  this.discoveryServiceClient=discoveryServiceClient;
  this.streamMetaStore=streamMetaStore;
  this.discoverableSupplier=discoverableSupplier;
  this.feedManager=feedManager;
  this.streamWriterSizeCollector=streamWriterSizeCollector;
  this.heartbeatPublisher=heartbeatPublisher;
  this.resourceCoordinatorClient=new ResourceCoordinatorClient(zkClient);
  this.leaderListeners=Sets.newHashSet();
  this.instanceId=cConf.getInt(Constants.Stream.CONTAINER_INSTANCE_ID);
  this.aggregators=Maps.newConcurrentMap();
  this.isInit=true;
}","@Inject public DistributedStreamService(CConfiguration cConf,StreamAdmin streamAdmin,StreamCoordinatorClient streamCoordinatorClient,StreamFileJanitorService janitorService,ZKClient zkClient,DiscoveryServiceClient discoveryServiceClient,StreamMetaStore streamMetaStore,Supplier<Discoverable> discoverableSupplier,StreamWriterSizeCollector streamWriterSizeCollector,HeartbeatPublisher heartbeatPublisher,NotificationFeedManager feedManager,NotificationService notificationService){
  super(streamCoordinatorClient,janitorService,streamWriterSizeCollector);
  this.zkClient=zkClient;
  this.streamAdmin=streamAdmin;
  this.notificationService=notificationService;
  this.discoveryServiceClient=discoveryServiceClient;
  this.streamMetaStore=streamMetaStore;
  this.discoverableSupplier=discoverableSupplier;
  this.feedManager=feedManager;
  this.streamWriterSizeCollector=streamWriterSizeCollector;
  this.heartbeatPublisher=heartbeatPublisher;
  this.resourceCoordinatorClient=new ResourceCoordinatorClient(zkClient);
  this.leaderListeners=Sets.newHashSet();
  this.instanceId=cConf.getInt(Constants.Stream.CONTAINER_INSTANCE_ID);
  this.aggregators=Maps.newConcurrentMap();
}"
7370,"@Override public void received(StreamWriterHeartbeat heartbeat,NotificationContext notificationContext){
  for (  Map.Entry<String,Long> entry : heartbeat.getStreamsSizes().entrySet()) {
    StreamSizeAggregator streamSizeAggregator=aggregators.get(entry.getKey());
    if (streamSizeAggregator == null) {
      continue;
    }
    streamSizeAggregator.bytesReceived(heartbeat.getInstanceId(),entry.getValue());
  }
}","@Override public void received(StreamWriterHeartbeat heartbeat,NotificationContext notificationContext){
  LOG.trace(""String_Node_Str"",heartbeat);
  for (  Map.Entry<String,Long> entry : heartbeat.getStreamsSizes().entrySet()) {
    StreamSizeAggregator streamSizeAggregator=aggregators.get(entry.getKey());
    if (streamSizeAggregator == null) {
      LOG.trace(""String_Node_Str"",entry.getKey());
      continue;
    }
    streamSizeAggregator.bytesReceived(heartbeat.getInstanceId(),entry.getValue());
  }
}"
7371,"/** 
 * Notify this aggregator that a certain number of bytes have been received from the stream writer with instance  {@code instanceId}.
 * @param instanceId id of the stream writer from which we received some bytes
 * @param nbBytes number of bytes of data received
 */
public void bytesReceived(int instanceId,long nbBytes){
  Long lastSize=streamWriterSizes.get(instanceId);
  if (lastSize == null) {
    streamWriterSizes.put(instanceId,nbBytes);
    return;
  }
  streamWriterSizes.put(instanceId,lastSize + nbBytes);
  checkSendNotification();
}","/** 
 * Notify this aggregator that a certain number of bytes have been received from the stream writer with instance  {@code instanceId}.
 * @param instanceId id of the stream writer from which we received some bytes
 * @param nbBytes number of bytes of data received
 */
public void bytesReceived(int instanceId,long nbBytes){
  LOG.trace(""String_Node_Str"",instanceId,nbBytes);
  streamWriterSizes.put(instanceId,nbBytes);
  checkSendNotification();
}"
7372,"@Override public void generationChanged(String streamName,int generation){
  Cancellable previousAggregator=aggregators.replace(streamName,createSizeAggregator(streamName,0,threshold));
  if (previousAggregator != null) {
    previousAggregator.cancel();
  }
}","@Override public void generationChanged(String streamName,int generation){
  int currentThresold=threshold;
  StreamSizeAggregator currentAggregator=aggregators.get(streamName);
  if (currentAggregator != null) {
    currentThresold=currentAggregator.getStreamThresholdMB();
  }
  Cancellable previousAggregator=aggregators.replace(streamName,createSizeAggregator(streamName,0,currentThresold));
  if (previousAggregator != null) {
    previousAggregator.cancel();
  }
}"
7373,"@Override public Module getDistributedModules(){
  return new AbstractModule(){
    @Override protected void configure(){
      bind(StreamFileJanitorService.class).to(DistributedStreamFileJanitorService.class).in(Scopes.SINGLETON);
      bind(StreamWriterSizeCollector.class).to(BasicStreamWriterSizeCollector.class).in(Scopes.SINGLETON);
      bind(StreamService.class).to(DistributedStreamService.class).in(Scopes.SINGLETON);
      Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.Stream.STREAM_HANDLER));
      handlerBinder.addBinding().to(StreamHandler.class);
      handlerBinder.addBinding().to(StreamFetchHandler.class);
      handlerBinder.addBinding().to(PingHandler.class);
      bind(HeartbeatPublisher.class).to(NoOpHeartbeatPublisher.class).in(Scopes.SINGLETON);
      bind(StreamHttpService.class).in(Scopes.SINGLETON);
      bind(Key.get(new TypeLiteral<Supplier<Discoverable>>(){
      }
)).to(StreamHttpService.class);
    }
  }
;
}","@Override public Module getDistributedModules(){
  return new AbstractModule(){
    @Override protected void configure(){
      bind(StreamFileJanitorService.class).to(DistributedStreamFileJanitorService.class).in(Scopes.SINGLETON);
      bind(StreamWriterSizeCollector.class).to(BasicStreamWriterSizeCollector.class).in(Scopes.SINGLETON);
      bind(StreamService.class).to(DistributedStreamService.class).in(Scopes.SINGLETON);
      Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.Stream.STREAM_HANDLER));
      handlerBinder.addBinding().to(StreamHandler.class);
      handlerBinder.addBinding().to(StreamFetchHandler.class);
      handlerBinder.addBinding().to(PingHandler.class);
      bind(HeartbeatPublisher.class).to(NotificationHeartbeatPublisher.class).in(Scopes.SINGLETON);
      bind(StreamHttpService.class).in(Scopes.SINGLETON);
      bind(Key.get(new TypeLiteral<Supplier<Discoverable>>(){
      }
)).to(StreamHttpService.class);
    }
  }
;
}"
7374,"@Override protected void configure(){
  bind(StreamFileJanitorService.class).to(DistributedStreamFileJanitorService.class).in(Scopes.SINGLETON);
  bind(StreamWriterSizeCollector.class).to(BasicStreamWriterSizeCollector.class).in(Scopes.SINGLETON);
  bind(StreamService.class).to(DistributedStreamService.class).in(Scopes.SINGLETON);
  Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.Stream.STREAM_HANDLER));
  handlerBinder.addBinding().to(StreamHandler.class);
  handlerBinder.addBinding().to(StreamFetchHandler.class);
  handlerBinder.addBinding().to(PingHandler.class);
  bind(HeartbeatPublisher.class).to(NoOpHeartbeatPublisher.class).in(Scopes.SINGLETON);
  bind(StreamHttpService.class).in(Scopes.SINGLETON);
  bind(Key.get(new TypeLiteral<Supplier<Discoverable>>(){
  }
)).to(StreamHttpService.class);
}","@Override protected void configure(){
  bind(StreamFileJanitorService.class).to(DistributedStreamFileJanitorService.class).in(Scopes.SINGLETON);
  bind(StreamWriterSizeCollector.class).to(BasicStreamWriterSizeCollector.class).in(Scopes.SINGLETON);
  bind(StreamService.class).to(DistributedStreamService.class).in(Scopes.SINGLETON);
  Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.Stream.STREAM_HANDLER));
  handlerBinder.addBinding().to(StreamHandler.class);
  handlerBinder.addBinding().to(StreamFetchHandler.class);
  handlerBinder.addBinding().to(PingHandler.class);
  bind(HeartbeatPublisher.class).to(NotificationHeartbeatPublisher.class).in(Scopes.SINGLETON);
  bind(StreamHttpService.class).in(Scopes.SINGLETON);
  bind(Key.get(new TypeLiteral<Supplier<Discoverable>>(){
  }
)).to(StreamHttpService.class);
}"
7375,"/** 
 * Publish one heartbeat.
 * @param heartbeat heartbeat to publish
 * @return a {@link ListenableFuture} describing the state of publishing. The {@link ListenableFuture#get} methodwill return the published heartbeat.
 */
ListenableFuture<StreamWriterHeartbeat> sendHeartbeat(StreamWriterHeartbeat heartbeat);","/** 
 * Publish one heartbeat.
 * @param heartbeat heartbeat to publish
 * @return a {@link ListenableFuture} describing the state of publishing. The {@link ListenableFuture#get} methodwill return the published heartbeat.
 * @throws IOException when the {@code heartbeat}
 */
ListenableFuture<StreamWriterHeartbeat> sendHeartbeat(StreamWriterHeartbeat heartbeat) throws IOException ;"
7376,"/** 
 * Called when a notification is received on a feed, to push it to all the handlers that subscribed to the feed.
 * @param feed {@link NotificationFeed} of the notification
 * @param notificationJson notification as a json object
 */
protected void notificationReceived(NotificationFeed feed,JsonElement notificationJson){
  Collection<NotificationCaller<?>> callers=subscribers.get(feed);
synchronized (subscribers) {
    callers=ImmutableList.copyOf(callers);
  }
  for (  NotificationCaller caller : callers) {
    Object notification=GSON.fromJson(notificationJson,caller.getNotificationFeedType());
    caller.received(notification,new BasicNotificationContext(dsFramework,transactionSystemClient));
  }
}","/** 
 * Called when a notification is received on a feed, to push it to all the handlers that subscribed to the feed.
 * @param feed {@link NotificationFeed} of the notification
 * @param notificationJson notification as a json object
 */
protected void notificationReceived(NotificationFeed feed,JsonElement notificationJson){
  LOG.trace(""String_Node_Str"",feed,notificationJson);
  Collection<NotificationCaller<?>> callers=subscribers.get(feed);
synchronized (subscribers) {
    callers=ImmutableList.copyOf(callers);
  }
  for (  NotificationCaller caller : callers) {
    Object notification=GSON.fromJson(notificationJson,caller.getNotificationFeedType());
    caller.received(notification,new BasicNotificationContext(dsFramework,transactionSystemClient));
  }
}"
7377,"private String nextToken(){
  char currChar=schema.charAt(pos);
  int endPos=pos;
  while (!(endPos == end || Character.isWhitespace(currChar) || currChar == ':' || currChar == ',' || currChar == '<' || currChar == '>')) {
    endPos++;
    currChar=schema.charAt(endPos);
  }
  String token=schema.substring(pos,endPos);
  pos=endPos;
  return token;
}","private String nextToken(){
  char currChar=schema.charAt(pos);
  int endPos=pos;
  while (!(Character.isWhitespace(currChar) || currChar == ':' || currChar == ',' || currChar == '<' || currChar == '>')) {
    endPos++;
    if (endPos == end) {
      break;
    }
    currChar=schema.charAt(endPos);
  }
  String token=schema.substring(pos,endPos);
  pos=endPos;
  return token;
}"
7378,"@Test public void testParseFlatSQL() throws IOException {
  String schemaStr=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
  Schema expected=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.BOOLEAN))),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.FLOAT)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.DOUBLE)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.BYTES)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.arrayOf(Schema.nullableOf(Schema.of(Schema.Type.STRING)))),Schema.Field.of(""String_Node_Str"",Schema.mapOf(Schema.nullableOf(Schema.of(Schema.Type.STRING)),Schema.nullableOf(Schema.of(Schema.Type.INT)))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.INT))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.DOUBLE)))))));
  Assert.assertEquals(expected,Schema.parseSQL(schemaStr));
}","@Test public void testParseFlatSQL() throws IOException {
  String schemaStr=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
  Schema expected=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.BOOLEAN))),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.FLOAT)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.DOUBLE)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.BYTES)),Schema.Field.of(""String_Node_Str"",Schema.arrayOf(Schema.nullableOf(Schema.of(Schema.Type.STRING)))),Schema.Field.of(""String_Node_Str"",Schema.mapOf(Schema.nullableOf(Schema.of(Schema.Type.STRING)),Schema.nullableOf(Schema.of(Schema.Type.INT)))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.INT))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.DOUBLE)))))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))));
  Assert.assertEquals(expected,Schema.parseSQL(schemaStr));
}"
7379,"@BeforeClass public static void beforeClass() throws IOException {
  zkServer=InMemoryZKServer.builder().setDataDir(tmpFolder.newFolder()).build();
  zkServer.startAndWait();
  conf=CConfiguration.create();
  conf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  conf.setInt(Constants.Stream.CONTAINER_INSTANCE_ID,0);
  conf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  injector=Guice.createInjector(Modules.override(new ZKClientModule(),new DataFabricModules().getInMemoryModules(),new ConfigModule(conf,new Configuration()),new AuthModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new LocationRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new StreamServiceRuntimeModule().getDistributedModules(),new StreamAdminModules().getInMemoryModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
      bind(StreamConsumerStateStoreFactory.class).to(LevelDBStreamConsumerStateStoreFactory.class).in(Singleton.class);
      bind(StreamAdmin.class).to(LevelDBStreamFileAdmin.class).in(Singleton.class);
      bind(StreamConsumerFactory.class).to(LevelDBStreamFileConsumerFactory.class).in(Singleton.class);
      bind(StreamFileWriterFactory.class).to(LocationStreamFileWriterFactory.class).in(Singleton.class);
      bind(StreamFileJanitorService.class).to(LocalStreamFileJanitorService.class).in(Scopes.SINGLETON);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class).in(Scopes.SINGLETON);
      bind(HeartbeatPublisher.class).to(MockHeartbeatPublisher.class).in(Scopes.SINGLETON);
    }
  }
));
  zkClient=injector.getInstance(ZKClientService.class);
  zkClient.startAndWait();
  streamHttpService=injector.getInstance(StreamHttpService.class);
  streamHttpService.startAndWait();
  streamService=injector.getInstance(StreamService.class);
  streamService.startAndWait();
  heartbeatPublisher=(MockHeartbeatPublisher)injector.getInstance(HeartbeatPublisher.class);
  hostname=streamHttpService.getBindAddress().getHostName();
  port=streamHttpService.getBindAddress().getPort();
}","@BeforeClass public static void beforeClass() throws IOException {
  zkServer=InMemoryZKServer.builder().setDataDir(TEMP_FOLDER.newFolder()).build();
  zkServer.startAndWait();
  conf=CConfiguration.create();
  conf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  conf.setInt(Constants.Stream.CONTAINER_INSTANCE_ID,0);
  conf.set(Constants.CFG_LOCAL_DATA_DIR,TEMP_FOLDER.newFolder().getAbsolutePath());
  injector=Guice.createInjector(Modules.override(new ZKClientModule(),new DataFabricModules().getInMemoryModules(),new ConfigModule(conf,new Configuration()),new AuthModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new LocationRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new StreamServiceRuntimeModule().getDistributedModules(),new StreamAdminModules().getInMemoryModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
      bind(StreamConsumerStateStoreFactory.class).to(LevelDBStreamConsumerStateStoreFactory.class).in(Singleton.class);
      bind(StreamAdmin.class).to(LevelDBStreamFileAdmin.class).in(Singleton.class);
      bind(StreamConsumerFactory.class).to(LevelDBStreamFileConsumerFactory.class).in(Singleton.class);
      bind(StreamFileWriterFactory.class).to(LocationStreamFileWriterFactory.class).in(Singleton.class);
      bind(StreamFileJanitorService.class).to(LocalStreamFileJanitorService.class).in(Scopes.SINGLETON);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class).in(Scopes.SINGLETON);
      bind(HeartbeatPublisher.class).to(MockHeartbeatPublisher.class).in(Scopes.SINGLETON);
    }
  }
));
  zkClient=injector.getInstance(ZKClientService.class);
  zkClient.startAndWait();
  streamHttpService=injector.getInstance(StreamHttpService.class);
  streamHttpService.startAndWait();
  streamService=injector.getInstance(StreamService.class);
  streamService.startAndWait();
  heartbeatPublisher=(MockHeartbeatPublisher)injector.getInstance(HeartbeatPublisher.class);
  hostname=streamHttpService.getBindAddress().getHostName();
  port=streamHttpService.getBindAddress().getPort();
}"
7380,"@Inject public DistributedStreamService(StreamCoordinatorClient streamCoordinatorClient,StreamFileJanitorService janitorService,StreamWriterSizeManager sizeManager,ZKClient zkClient,DiscoveryServiceClient discoveryServiceClient,StreamMetaStore streamMetaStore,Supplier<Discoverable> discoverableSupplier){
  super(streamCoordinatorClient,janitorService,sizeManager);
  this.zkClient=zkClient;
  this.discoveryServiceClient=discoveryServiceClient;
  this.streamMetaStore=streamMetaStore;
  this.discoverableSupplier=discoverableSupplier;
  this.resourceCoordinatorClient=new ResourceCoordinatorClient(zkClient);
  this.leaderListeners=Sets.newHashSet();
}","@Inject public DistributedStreamService(StreamCoordinatorClient streamCoordinatorClient,StreamFileJanitorService janitorService,StreamWriterSizeManager sizeManager,ZKClient zkClient,DiscoveryServiceClient discoveryServiceClient,StreamMetaStore streamMetaStore,Supplier<Discoverable> discoverableSupplier,NotificationFeedManager notificationFeedManager){
  super(streamCoordinatorClient,janitorService,sizeManager,notificationFeedManager);
  this.zkClient=zkClient;
  this.discoveryServiceClient=discoveryServiceClient;
  this.streamMetaStore=streamMetaStore;
  this.discoverableSupplier=discoverableSupplier;
  this.resourceCoordinatorClient=new ResourceCoordinatorClient(zkClient);
  this.leaderListeners=Sets.newHashSet();
}"
7381,"@Inject public LocalStreamService(StreamCoordinatorClient streamCoordinatorClient,StreamFileJanitorService janitorService,StreamWriterSizeManager sizeManager){
  super(streamCoordinatorClient,janitorService,sizeManager);
}","@Inject public LocalStreamService(StreamCoordinatorClient streamCoordinatorClient,StreamFileJanitorService janitorService,StreamWriterSizeManager sizeManager,NotificationFeedManager notificationFeedManager){
  super(streamCoordinatorClient,janitorService,sizeManager,notificationFeedManager);
}"
7382,"@Override public void increment(String metricName,int value){
}","@Override public void increment(String metricName,long value){
}"
7383,"@Test public void testNestedRecord() throws Exception {
  Schema innerSchema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.DOUBLE)));
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)),Schema.Field.of(""String_Node_Str"",innerSchema));
  org.apache.avro.Schema avroInnerSchema=convertSchema(innerSchema);
  org.apache.avro.Schema avroSchema=convertSchema(schema);
  GenericRecord record=new GenericRecordBuilder(avroSchema).set(""String_Node_Str"",Integer.MAX_VALUE).set(""String_Node_Str"",new GenericRecordBuilder(avroInnerSchema).set(""String_Node_Str"",5).set(""String_Node_Str"",3.14159).build()).build();
  ByteArrayOutputStream out=new ByteArrayOutputStream();
  BinaryEncoder encoder=EncoderFactory.get().binaryEncoder(out,null);
  DatumWriter<GenericRecord> writer=new GenericDatumWriter<GenericRecord>(avroSchema);
  writer.write(record,encoder);
  encoder.flush();
  out.close();
  byte[] serializedRecord=out.toByteArray();
  FormatSpecification formatSpecification=new FormatSpecification(RecordFormats.AVRO,schema,Collections.<String,String>emptyMap());
  AvroRecordFormat format=new AvroRecordFormat();
  format.initialize(formatSpecification);
  GenericRecord actual=format.read(ByteBuffer.wrap(serializedRecord));
  Assert.assertEquals(Integer.MAX_VALUE,actual.get(""String_Node_Str""));
  GenericRecord actualInner=(GenericRecord)actual.get(""String_Node_Str"");
  Assert.assertEquals(5,actualInner.get(""String_Node_Str""));
  Assert.assertEquals(3.14159,actualInner.get(""String_Node_Str""));
}","@Test public void testNestedRecord() throws Exception {
  Schema innerSchema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.DOUBLE)));
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)),Schema.Field.of(""String_Node_Str"",innerSchema));
  org.apache.avro.Schema avroInnerSchema=convertSchema(innerSchema);
  org.apache.avro.Schema avroSchema=convertSchema(schema);
  GenericRecord record=new GenericRecordBuilder(avroSchema).set(""String_Node_Str"",Integer.MAX_VALUE).set(""String_Node_Str"",new GenericRecordBuilder(avroInnerSchema).set(""String_Node_Str"",5).set(""String_Node_Str"",3.14159).build()).build();
  ByteArrayOutputStream out=new ByteArrayOutputStream();
  BinaryEncoder encoder=EncoderFactory.get().binaryEncoder(out,null);
  DatumWriter<GenericRecord> writer=new GenericDatumWriter<GenericRecord>(avroSchema);
  writer.write(record,encoder);
  encoder.flush();
  out.close();
  byte[] serializedRecord=out.toByteArray();
  FormatSpecification formatSpecification=new FormatSpecification(Formats.AVRO,schema,Collections.<String,String>emptyMap());
  RecordFormat<ByteBuffer,GenericRecord> format=RecordFormats.createInitializedFormat(formatSpecification);
  GenericRecord actual=format.read(ByteBuffer.wrap(serializedRecord));
  Assert.assertEquals(Integer.MAX_VALUE,actual.get(""String_Node_Str""));
  GenericRecord actualInner=(GenericRecord)actual.get(""String_Node_Str"");
  Assert.assertEquals(5,actualInner.get(""String_Node_Str""));
  Assert.assertEquals(3.14159,actualInner.get(""String_Node_Str""));
}"
7384,"@Test public void testFlatRecord() throws Exception {
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.BOOLEAN)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.BYTES)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.DOUBLE)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.FLOAT)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.arrayOf(Schema.of(Schema.Type.INT))),Schema.Field.of(""String_Node_Str"",Schema.mapOf(Schema.of(Schema.Type.STRING),Schema.of(Schema.Type.INT))),Schema.Field.of(""String_Node_Str"",Schema.unionOf(Schema.of(Schema.Type.STRING),Schema.of(Schema.Type.NULL))));
  FormatSpecification formatSpecification=new FormatSpecification(RecordFormats.AVRO,schema,Collections.<String,String>emptyMap());
  org.apache.avro.Schema avroSchema=convertSchema(schema);
  GenericRecord record=new GenericRecordBuilder(avroSchema).set(""String_Node_Str"",Integer.MAX_VALUE).set(""String_Node_Str"",Long.MAX_VALUE).set(""String_Node_Str"",false).set(""String_Node_Str"",ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str""))).set(""String_Node_Str"",Double.MAX_VALUE).set(""String_Node_Str"",Float.MAX_VALUE).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",Lists.newArrayList(1,2,3)).set(""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",1,""String_Node_Str"",2)).set(""String_Node_Str"",null).build();
  ByteArrayOutputStream out=new ByteArrayOutputStream();
  BinaryEncoder encoder=EncoderFactory.get().binaryEncoder(out,null);
  DatumWriter<GenericRecord> writer=new GenericDatumWriter<GenericRecord>(avroSchema);
  writer.write(record,encoder);
  encoder.flush();
  out.close();
  byte[] serializedRecord=out.toByteArray();
  AvroRecordFormat format=new AvroRecordFormat();
  format.initialize(formatSpecification);
  GenericRecord actual=format.read(ByteBuffer.wrap(serializedRecord));
  Assert.assertEquals(Integer.MAX_VALUE,actual.get(""String_Node_Str""));
  Assert.assertEquals(Long.MAX_VALUE,actual.get(""String_Node_Str""));
  Assert.assertFalse((Boolean)actual.get(""String_Node_Str""));
  Assert.assertArrayEquals(Bytes.toBytes(""String_Node_Str""),Bytes.toBytes((ByteBuffer)actual.get(""String_Node_Str"")));
  Assert.assertEquals(Double.MAX_VALUE,actual.get(""String_Node_Str""));
  Assert.assertEquals(Float.MAX_VALUE,actual.get(""String_Node_Str""));
  Assert.assertEquals(""String_Node_Str"",actual.get(""String_Node_Str"").toString());
  Assert.assertEquals(Lists.newArrayList(1,2,3),actual.get(""String_Node_Str""));
  assertMapEquals(ImmutableMap.<String,Object>of(""String_Node_Str"",1,""String_Node_Str"",2),(Map<Object,Object>)actual.get(""String_Node_Str""));
  Assert.assertNull(actual.get(""String_Node_Str""));
}","@Test public void testFlatRecord() throws Exception {
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.BOOLEAN)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.BYTES)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.DOUBLE)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.FLOAT)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.arrayOf(Schema.of(Schema.Type.INT))),Schema.Field.of(""String_Node_Str"",Schema.mapOf(Schema.of(Schema.Type.STRING),Schema.of(Schema.Type.INT))),Schema.Field.of(""String_Node_Str"",Schema.unionOf(Schema.of(Schema.Type.STRING),Schema.of(Schema.Type.NULL))));
  FormatSpecification formatSpecification=new FormatSpecification(Formats.AVRO,schema,Collections.<String,String>emptyMap());
  org.apache.avro.Schema avroSchema=convertSchema(schema);
  GenericRecord record=new GenericRecordBuilder(avroSchema).set(""String_Node_Str"",Integer.MAX_VALUE).set(""String_Node_Str"",Long.MAX_VALUE).set(""String_Node_Str"",false).set(""String_Node_Str"",ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str""))).set(""String_Node_Str"",Double.MAX_VALUE).set(""String_Node_Str"",Float.MAX_VALUE).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",Lists.newArrayList(1,2,3)).set(""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",1,""String_Node_Str"",2)).set(""String_Node_Str"",null).build();
  ByteArrayOutputStream out=new ByteArrayOutputStream();
  BinaryEncoder encoder=EncoderFactory.get().binaryEncoder(out,null);
  DatumWriter<GenericRecord> writer=new GenericDatumWriter<GenericRecord>(avroSchema);
  writer.write(record,encoder);
  encoder.flush();
  out.close();
  byte[] serializedRecord=out.toByteArray();
  RecordFormat<ByteBuffer,GenericRecord> format=RecordFormats.createInitializedFormat(formatSpecification);
  GenericRecord actual=format.read(ByteBuffer.wrap(serializedRecord));
  Assert.assertEquals(Integer.MAX_VALUE,actual.get(""String_Node_Str""));
  Assert.assertEquals(Long.MAX_VALUE,actual.get(""String_Node_Str""));
  Assert.assertFalse((Boolean)actual.get(""String_Node_Str""));
  Assert.assertArrayEquals(Bytes.toBytes(""String_Node_Str""),Bytes.toBytes((ByteBuffer)actual.get(""String_Node_Str"")));
  Assert.assertEquals(Double.MAX_VALUE,actual.get(""String_Node_Str""));
  Assert.assertEquals(Float.MAX_VALUE,actual.get(""String_Node_Str""));
  Assert.assertEquals(""String_Node_Str"",actual.get(""String_Node_Str"").toString());
  Assert.assertEquals(Lists.newArrayList(1,2,3),actual.get(""String_Node_Str""));
  assertMapEquals(ImmutableMap.<String,Object>of(""String_Node_Str"",1,""String_Node_Str"",2),(Map<Object,Object>)actual.get(""String_Node_Str""));
  Assert.assertNull(actual.get(""String_Node_Str""));
}"
7385,"@Test public void testAvroFormattedStream() throws Exception {
  createStream(""String_Node_Str"");
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.DOUBLE)));
  FormatSpecification formatSpecification=new FormatSpecification(RecordFormats.AVRO,schema,Collections.<String,String>emptyMap());
  StreamProperties properties=new StreamProperties(""String_Node_Str"",Long.MAX_VALUE,formatSpecification);
  setStreamProperties(""String_Node_Str"",properties);
  org.apache.avro.Schema avroSchema=new org.apache.avro.Schema.Parser().parse(schema.toString());
  sendStreamEvent(""String_Node_Str"",createAvroEvent(avroSchema,""String_Node_Str"",5,3.14));
  sendStreamEvent(""String_Node_Str"",createAvroEvent(avroSchema,""String_Node_Str"",10,2.34));
  sendStreamEvent(""String_Node_Str"",createAvroEvent(avroSchema,""String_Node_Str"",1,1.23));
  sendStreamEvent(""String_Node_Str"",createAvroEvent(avroSchema,""String_Node_Str"",50,45.67));
  sendStreamEvent(""String_Node_Str"",createAvroEvent(avroSchema,""String_Node_Str"",100,98.76));
  Double xPrice=5 * 3.14 + 10 * 2.34;
  Double yPrice=1.23;
  Double zPrice=50 * 45.67 + 100 * 98.76;
  ExploreExecutionResult result=exploreClient.submit(""String_Node_Str"" + ""String_Node_Str"").get();
  Assert.assertTrue(result.hasNext());
  Assert.assertEquals(Lists.newArrayList(new ColumnDesc(""String_Node_Str"",""String_Node_Str"",1,null),new ColumnDesc(""String_Node_Str"",""String_Node_Str"",2,null),new ColumnDesc(""String_Node_Str"",""String_Node_Str"",3,null)),result.getResultSchema());
  List<Object> rowColumns=result.next().getColumns();
  Assert.assertEquals(""String_Node_Str"",rowColumns.get(0).toString());
  Assert.assertEquals(150L,rowColumns.get(1));
  Assert.assertTrue(Math.abs(zPrice - (Double)rowColumns.get(2)) < 0.0000001);
  rowColumns=result.next().getColumns();
  Assert.assertEquals(""String_Node_Str"",rowColumns.get(0).toString());
  Assert.assertEquals(15L,rowColumns.get(1));
  Assert.assertTrue(Math.abs(xPrice - (Double)rowColumns.get(2)) < 0.0000001);
  rowColumns=result.next().getColumns();
  Assert.assertEquals(""String_Node_Str"",rowColumns.get(0).toString());
  Assert.assertEquals(1L,rowColumns.get(1));
  Assert.assertTrue(Math.abs(yPrice - (Double)rowColumns.get(2)) < 0.0000001);
  Assert.assertFalse(result.hasNext());
}","@Test public void testAvroFormattedStream() throws Exception {
  createStream(""String_Node_Str"");
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.DOUBLE)));
  FormatSpecification formatSpecification=new FormatSpecification(Formats.AVRO,schema,Collections.<String,String>emptyMap());
  StreamProperties properties=new StreamProperties(""String_Node_Str"",Long.MAX_VALUE,formatSpecification);
  setStreamProperties(""String_Node_Str"",properties);
  org.apache.avro.Schema avroSchema=new org.apache.avro.Schema.Parser().parse(schema.toString());
  sendStreamEvent(""String_Node_Str"",createAvroEvent(avroSchema,""String_Node_Str"",5,3.14));
  sendStreamEvent(""String_Node_Str"",createAvroEvent(avroSchema,""String_Node_Str"",10,2.34));
  sendStreamEvent(""String_Node_Str"",createAvroEvent(avroSchema,""String_Node_Str"",1,1.23));
  sendStreamEvent(""String_Node_Str"",createAvroEvent(avroSchema,""String_Node_Str"",50,45.67));
  sendStreamEvent(""String_Node_Str"",createAvroEvent(avroSchema,""String_Node_Str"",100,98.76));
  Double xPrice=5 * 3.14 + 10 * 2.34;
  Double yPrice=1.23;
  Double zPrice=50 * 45.67 + 100 * 98.76;
  ExploreExecutionResult result=exploreClient.submit(""String_Node_Str"" + ""String_Node_Str"").get();
  Assert.assertTrue(result.hasNext());
  Assert.assertEquals(Lists.newArrayList(new ColumnDesc(""String_Node_Str"",""String_Node_Str"",1,null),new ColumnDesc(""String_Node_Str"",""String_Node_Str"",2,null),new ColumnDesc(""String_Node_Str"",""String_Node_Str"",3,null)),result.getResultSchema());
  List<Object> rowColumns=result.next().getColumns();
  Assert.assertEquals(""String_Node_Str"",rowColumns.get(0).toString());
  Assert.assertEquals(150L,rowColumns.get(1));
  Assert.assertTrue(Math.abs(zPrice - (Double)rowColumns.get(2)) < 0.0000001);
  rowColumns=result.next().getColumns();
  Assert.assertEquals(""String_Node_Str"",rowColumns.get(0).toString());
  Assert.assertEquals(15L,rowColumns.get(1));
  Assert.assertTrue(Math.abs(xPrice - (Double)rowColumns.get(2)) < 0.0000001);
  rowColumns=result.next().getColumns();
  Assert.assertEquals(""String_Node_Str"",rowColumns.get(0).toString());
  Assert.assertEquals(1L,rowColumns.get(1));
  Assert.assertTrue(Math.abs(yPrice - (Double)rowColumns.get(2)) < 0.0000001);
  Assert.assertFalse(result.hasNext());
}"
7386,"private void runUserCodeInTx(TransactionExecutor.Procedure<MapReduceContext> userCode){
  TransactionContext txContext=new TransactionContext(txClient,context.getDatasetInstantiator().getTransactionAware());
  DynamicBasicMapReduceContext mapReduceContextWithTX=null;
  try {
    txContext.start();
    mapReduceContextWithTX=new DynamicBasicMapReduceContext(context,datasetFramework,txContext,cConf);
    userCode.apply(mapReduceContextWithTX);
    txContext.finish();
  }
 catch (  TransactionFailureException e) {
    abortTransaction(e,""String_Node_Str"",txContext);
  }
catch (  Throwable t) {
    abortTransaction(t,""String_Node_Str"",txContext);
  }
 finally {
    if (mapReduceContextWithTX != null) {
      mapReduceContextWithTX.close();
    }
  }
}","private void runUserCodeInTx(TransactionExecutor.Procedure<MapReduceContext> userCode,String methodName){
  TransactionContext txContext=new TransactionContext(txClient,context.getDatasetInstantiator().getTransactionAware());
  DynamicBasicMapReduceContext mapReduceContextWithTX=null;
  try {
    txContext.start();
    mapReduceContextWithTX=new DynamicBasicMapReduceContext(context,datasetFramework,txContext,cConf);
    userCode.apply(mapReduceContextWithTX);
    txContext.finish();
  }
 catch (  TransactionFailureException e) {
    abortTransaction(e,""String_Node_Str"" + methodName + ""String_Node_Str"",txContext);
  }
catch (  Throwable t) {
    abortTransaction(t,""String_Node_Str"" + methodName + ""String_Node_Str"",txContext);
  }
 finally {
    if (mapReduceContextWithTX != null) {
      mapReduceContextWithTX.close();
    }
  }
}"
7387,"/** 
 * Calls the   {@link MapReduce#onFinish(boolean,co.cask.cdap.api.mapreduce.MapReduceContext)} method.
 */
private void onFinish(final boolean succeeded) throws TransactionFailureException, InterruptedException {
  runUserCodeInTx(new TransactionExecutor.Procedure<MapReduceContext>(){
    @Override public void apply(    MapReduceContext context) throws Exception {
      mapReduce.onFinish(succeeded,context);
    }
  }
);
}","/** 
 * Calls the   {@link MapReduce#onFinish(boolean,co.cask.cdap.api.mapreduce.MapReduceContext)} method.
 */
private void onFinish(final boolean succeeded) throws TransactionFailureException, InterruptedException {
  runUserCodeInTx(new TransactionExecutor.Procedure<MapReduceContext>(){
    @Override public void apply(    MapReduceContext context) throws Exception {
      mapReduce.onFinish(succeeded,context);
    }
  }
,""String_Node_Str"");
}"
7388,"/** 
 * Calls the   {@link MapReduce#beforeSubmit(co.cask.cdap.api.mapreduce.MapReduceContext)} method.
 */
private void beforeSubmit() throws TransactionFailureException, InterruptedException {
  runUserCodeInTx(new TransactionExecutor.Procedure<MapReduceContext>(){
    @Override public void apply(    MapReduceContext context) throws Exception {
      mapReduce.beforeSubmit(context);
    }
  }
);
}","/** 
 * Calls the   {@link MapReduce#beforeSubmit(co.cask.cdap.api.mapreduce.MapReduceContext)} method.
 */
private void beforeSubmit() throws TransactionFailureException, InterruptedException {
  runUserCodeInTx(new TransactionExecutor.Procedure<MapReduceContext>(){
    @Override public void apply(    MapReduceContext context) throws Exception {
      mapReduce.beforeSubmit(context);
    }
  }
,""String_Node_Str"");
}"
7389,"/** 
 * Stops the Spark program by calling   {@link org.apache.spark.SparkContext#stop()}
 */
public static void stopSparkProgram(){
  sparkContextStopBugFixer();
  if (isScalaProgram()) {
    ((org.apache.spark.SparkContext)getSparkContext().getOriginalSparkContext()).stop();
  }
 else {
    ((org.apache.spark.api.java.JavaSparkContext)getSparkContext().getOriginalSparkContext()).stop();
  }
}","/** 
 * Stops the Spark program by calling   {@link org.apache.spark.SparkContext#stop()}
 */
public static void stopSparkProgram(){
  if (getSparkContext() != null) {
    if (isScalaProgram()) {
      ((org.apache.spark.SparkContext)getSparkContext().getOriginalSparkContext()).stop();
    }
 else {
      ((org.apache.spark.api.java.JavaSparkContext)getSparkContext().getOriginalSparkContext()).stop();
    }
  }
}"
7390,"/** 
 * @return {@link SparkContext}
 */
public static SparkContext getSparkContext(){
  return sparkContext;
}","/** 
 * @return {@link SparkContext}
 */
private static SparkContext getSparkContext(){
  return sparkContext;
}"
7391,"@Override public void leader(){
  LOG.info(""String_Node_Str"");
  resourceCoordinator=new ResourceCoordinator(zkClient,discoveryServiceClient,new BalancedAssignmentStrategy());
  resourceCoordinator.startAndWait();
  try {
    ResourceRequirement.Builder builder=ResourceRequirement.builder(Constants.Service.STREAMS);
    for (    StreamSpecification spec : streamMetaStore.listStreams()) {
      LOG.debug(""String_Node_Str"",spec.getName());
      builder.addPartition(new ResourceRequirement.Partition(spec.getName(),1));
    }
    resourceCoordinatorClient.submitRequirement(builder.build()).get();
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    Throwables.propagate(e);
  }
}","@Override public void leader(){
  LOG.info(""String_Node_Str"");
  resourceCoordinator=new ResourceCoordinator(zkClient,discoveryServiceClient,new BalancedAssignmentStrategy());
  resourceCoordinator.startAndWait();
  resourceCoordinatorClient.modifyRequirement(Constants.Service.STREAMS,new ResourceModifier(){
    @Nullable @Override public ResourceRequirement apply(    @Nullable ResourceRequirement existingRequirement){
      try {
        ResourceRequirement.Builder builder=ResourceRequirement.builder(Constants.Service.STREAMS);
        for (        StreamSpecification spec : streamMetaStore.listStreams()) {
          LOG.debug(""String_Node_Str"",spec.getName());
          builder.addPartition(new ResourceRequirement.Partition(spec.getName(),1));
        }
        return builder.build();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
        Throwables.propagate(e);
        return null;
      }
    }
  }
);
}"
7392,"@Override protected void startUp() throws Exception {
  Preconditions.checkNotNull(handlerDiscoverable,""String_Node_Str"");
  resourceCoordinatorClient.startAndWait();
  handlerSubscription=resourceCoordinatorClient.subscribe(handlerDiscoverable.getName(),new StreamsLeaderHandler());
  leaderElection=new LeaderElection(zkClient,""String_Node_Str"" + STREAMS_COORDINATOR,new ElectionHandler(){
    @Override public void leader(){
      LOG.info(""String_Node_Str"");
      resourceCoordinator=new ResourceCoordinator(zkClient,discoveryServiceClient,new BalancedAssignmentStrategy());
      resourceCoordinator.startAndWait();
      try {
        ResourceRequirement.Builder builder=ResourceRequirement.builder(Constants.Service.STREAMS);
        for (        StreamSpecification spec : streamMetaStore.listStreams()) {
          LOG.debug(""String_Node_Str"",spec.getName());
          builder.addPartition(new ResourceRequirement.Partition(spec.getName(),1));
        }
        resourceCoordinatorClient.submitRequirement(builder.build()).get();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
        Throwables.propagate(e);
      }
    }
    @Override public void follower(){
      LOG.info(""String_Node_Str"");
      if (resourceCoordinator != null) {
        resourceCoordinator.stopAndWait();
      }
    }
  }
);
}","@Override protected void startUp() throws Exception {
  Preconditions.checkNotNull(handlerDiscoverable,""String_Node_Str"");
  resourceCoordinatorClient.startAndWait();
  handlerSubscription=resourceCoordinatorClient.subscribe(handlerDiscoverable.getName(),new StreamsLeaderHandler());
  leaderElection=new LeaderElection(zkClient,""String_Node_Str"" + STREAMS_COORDINATOR,new ElectionHandler(){
    @Override public void leader(){
      LOG.info(""String_Node_Str"");
      resourceCoordinator=new ResourceCoordinator(zkClient,discoveryServiceClient,new BalancedAssignmentStrategy());
      resourceCoordinator.startAndWait();
      resourceCoordinatorClient.modifyRequirement(Constants.Service.STREAMS,new ResourceModifier(){
        @Nullable @Override public ResourceRequirement apply(        @Nullable ResourceRequirement existingRequirement){
          try {
            ResourceRequirement.Builder builder=ResourceRequirement.builder(Constants.Service.STREAMS);
            for (            StreamSpecification spec : streamMetaStore.listStreams()) {
              LOG.debug(""String_Node_Str"",spec.getName());
              builder.addPartition(new ResourceRequirement.Partition(spec.getName(),1));
            }
            return builder.build();
          }
 catch (          Throwable e) {
            LOG.error(""String_Node_Str"",e);
            Throwables.propagate(e);
            return null;
          }
        }
      }
);
    }
    @Override public void follower(){
      LOG.info(""String_Node_Str"");
      if (resourceCoordinator != null) {
        resourceCoordinator.stopAndWait();
      }
    }
  }
);
}"
7393,"/** 
 * @param resourceAcceptor Filter for accepting resources
 * @param parentClassLoader Parent classloader
 */
FilterClassLoader(Predicate<String> resourceAcceptor,ClassLoader parentClassLoader){
  super(parentClassLoader);
  this.resourceAcceptor=resourceAcceptor;
  this.bootstrapClassLoader=new URLClassLoader(new URL[0],null);
}","/** 
 * @param resourceAcceptor Filter for accepting resources
 * @param parentClassLoader Parent classloader
 */
FilterClassLoader(Predicate<String> resourceAcceptor,Predicate<String> packageAcceptor,ClassLoader parentClassLoader){
  super(parentClassLoader);
  this.resourceAcceptor=resourceAcceptor;
  this.packageAcceptor=packageAcceptor;
  this.bootstrapClassLoader=new URLClassLoader(new URL[0],null);
}"
7394,"/** 
 * Constructs an instance that load classes from the given directory for the given program type. <p/> The URLs for class loading are: <p/> <pre> [dir] [dir]/*.jar [dir]/lib/*.jar </pre>
 */
public static ProgramClassLoader create(File unpackedJarDir,ClassLoader parentClassLoader,@Nullable ProgramType programType) throws IOException {
  Predicate<String> predicate=Predicates.in(ProgramResources.getVisibleResources(programType));
  ClassLoader filteredParent=new FilterClassLoader(predicate,parentClassLoader);
  return new ProgramClassLoader(unpackedJarDir,filteredParent);
}","/** 
 * Constructs an instance that load classes from the given directory for the given program type. <p/> The URLs for class loading are: <p/> <pre> [dir] [dir]/*.jar [dir]/lib/*.jar </pre>
 */
public static ProgramClassLoader create(File unpackedJarDir,ClassLoader parentClassLoader,@Nullable ProgramType programType) throws IOException {
  Set<String> visibleResources=ProgramResources.getVisibleResources(programType);
  ImmutableSet.Builder<String> visiblePackages=ImmutableSet.builder();
  for (  String resource : visibleResources) {
    if (resource.endsWith(""String_Node_Str"")) {
      int idx=resource.lastIndexOf('/');
      if (idx > 0) {
        visiblePackages.add(resource.substring(0,idx));
      }
    }
  }
  ClassLoader filteredParent=new FilterClassLoader(Predicates.in(visibleResources),Predicates.in(visiblePackages.build()),parentClassLoader);
  return new ProgramClassLoader(unpackedJarDir,filteredParent);
}"
7395,"/** 
 * Returns a Set of resources name that are visible through the cdap-api module as well as Hadoop classes. This includes all classes+resources in cdap-api plus all classes+resources that cdap-api depends on (for example, sl4j, guava, gson, etc).
 */
private static Set<String> createBaseResources() throws IOException {
  ClassLoader classLoader=ProgramResources.class.getClassLoader();
  Set<ClassPath.ClassInfo> apiResources=getResources(getClassPath(classLoader,Application.class),CDAP_API_PACKAGES,Sets.<ClassPath.ClassInfo>newHashSet());
  Set<String> result=findClassDependencies(classLoader,Iterables.transform(apiResources,CLASS_INFO_TO_CLASS_NAME),Sets.<String>newHashSet());
  getResources(getClassPath(classLoader,Path.class),JAVAX_WS_RS_PACKAGES,CLASS_INFO_TO_RESOURCE_NAME,result);
  return ImmutableSet.copyOf(getResources(ClassPath.from(classLoader,JAR_ONLY_URI),HADOOP_PACKAGES,CLASS_INFO_TO_RESOURCE_NAME,result));
}","/** 
 * Returns a Set of resources name that are visible through the cdap-api module as well as Hadoop classes. This includes all classes+resources in cdap-api plus all classes+resources that cdap-api depends on (for example, sl4j, guava, gson, etc).
 */
private static Set<String> createBaseResources() throws IOException {
  ClassLoader classLoader=ProgramResources.class.getClassLoader();
  Set<ClassPath.ClassInfo> apiResources=getResources(getClassPath(classLoader,Application.class),CDAP_API_PACKAGES,Sets.<ClassPath.ClassInfo>newHashSet());
  Set<String> result=findClassDependencies(classLoader,Iterables.transform(apiResources,CLASS_INFO_TO_CLASS_NAME),Sets.<String>newHashSet());
  getResources(getClassPath(classLoader,Path.class),JAVAX_WS_RS_PACKAGES,CLASS_INFO_TO_RESOURCE_NAME,result);
  getResources(ClassPath.from(classLoader,JAR_ONLY_URI),HADOOP_PACKAGES,CLASS_INFO_TO_RESOURCE_NAME,result);
  return ImmutableSet.copyOf(Sets.filter(result,new Predicate<String>(){
    @Override public boolean apply(    String input){
      return !input.startsWith(HBASE_PACKAGE_PREFIX);
    }
  }
));
}"
7396,"@Override public String apply(ClassPath.ClassInfo input){
  return input.getResourceName();
}","@Override public boolean apply(String input){
  return !input.startsWith(HBASE_PACKAGE_PREFIX);
}"
7397,"@Test public void testMetricsContexts() throws Exception {
  metricsResponseCheck(""String_Node_Str"",2,ImmutableList.<String>of(""String_Node_Str"",""String_Node_Str""));
  metricsResponseCheck(""String_Node_Str"",1,ImmutableList.<String>of(""String_Node_Str""));
  metricsResponseCheck(""String_Node_Str"",3,ImmutableList.<String>of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
}","@Test public void testMetricsContexts() throws Exception {
  metricsResponseCheck(""String_Node_Str"",2,ImmutableList.<String>of(""String_Node_Str"",""String_Node_Str""));
  metricsResponseCheck(""String_Node_Str"",1,ImmutableList.<String>of(""String_Node_Str""));
  metricsResponseCheck(""String_Node_Str"",3,ImmutableList.<String>of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  metricsResponseCheck(""String_Node_Str"",2,ImmutableList.<String>of(""String_Node_Str"",""String_Node_Str""));
  metricsResponseCheck(""String_Node_Str"",0,ImmutableList.<String>of());
}"
7398,"private List<String> getUniqueContextAndMetrics(byte[] startRow,byte[] endRow,FuzzyRowFilter filter,boolean isContextQuery,String contextPrefix,int targetOffset,int length) throws OperationException {
  List<String> metricsScanResults=Lists.newArrayList();
  Row rowResult;
  int contextOffset=entityCodec.getEncodedSize(MetricsEntityType.CONTEXT);
  do {
    ScannerFields fields=new ScannerFields(startRow,endRow,null,filter);
    Scanner scanner=null;
    try {
      scanner=timeSeriesTable.scan(fields.startRow,fields.endRow,fields.columns,fields.filter);
    }
 catch (    Exception e) {
      throw new OperationException(StatusCode.INTERNAL_ERROR,e.getMessage(),e);
    }
    rowResult=scanner.next();
    if (rowResult != null) {
      byte[] rowKey=rowResult.getRow();
      String contextStr=entityCodec.decode(MetricsEntityType.CONTEXT,rowKey,0);
      if (contextPrefix != null && !contextStr.startsWith(contextPrefix)) {
        scanner.close();
        break;
      }
      if (isContextQuery) {
        metricsScanResults.add(contextStr);
      }
 else {
        metricsScanResults.add(entityCodec.decode(MetricsEntityType.METRIC,rowKey,contextOffset));
      }
      startRow=getNextRow(rowKey,targetOffset,length);
      if (startRow == null) {
        scanner.close();
        break;
      }
    }
    scanner.close();
  }
 while (rowResult != null);
  return metricsScanResults;
}","private List<String> getUniqueContextAndMetrics(byte[] startRow,byte[] endRow,FuzzyRowFilter filter,boolean isContextQuery,String contextPrefix,int targetOffset,int length) throws OperationException {
  List<String> metricsScanResults=Lists.newArrayList();
  Row rowResult;
  int contextOffset=entityCodec.getEncodedSize(MetricsEntityType.CONTEXT);
  if (isContextQuery && contextPrefix != null) {
    contextPrefix+=""String_Node_Str"";
  }
  do {
    ScannerFields fields=new ScannerFields(startRow,endRow,null,filter);
    Scanner scanner=null;
    try {
      scanner=timeSeriesTable.scan(fields.startRow,fields.endRow,fields.columns,fields.filter);
    }
 catch (    Exception e) {
      throw new OperationException(StatusCode.INTERNAL_ERROR,e.getMessage(),e);
    }
    rowResult=scanner.next();
    if (rowResult != null) {
      byte[] rowKey=rowResult.getRow();
      String contextStr=entityCodec.decode(MetricsEntityType.CONTEXT,rowKey,0);
      if (contextPrefix != null && !contextStr.startsWith(contextPrefix)) {
        scanner.close();
        break;
      }
      if (isContextQuery) {
        metricsScanResults.add(contextStr);
      }
 else {
        metricsScanResults.add(entityCodec.decode(MetricsEntityType.METRIC,rowKey,contextOffset));
      }
      startRow=getNextRow(rowKey,targetOffset,length);
      if (startRow == null) {
        scanner.close();
        break;
      }
    }
    scanner.close();
  }
 while (rowResult != null);
  return metricsScanResults;
}"
7399,"/** 
 * Returns all the unique metrics in the given context
 */
@GET @Path(""String_Node_Str"") public void listContextMetrics(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String scope,@PathParam(""String_Node_Str"") final String context) throws IOException {
  try {
    MetricsScope metricsScope=MetricsScope.valueOf(scope.toUpperCase());
    if (metricsScope == null) {
      responder.sendJson(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"" + MetricsScope.SYSTEM + ""String_Node_Str""+ MetricsScope.USER+ ""String_Node_Str"");
      return;
    }
    responder.sendJson(HttpResponseStatus.OK,getAvailableMetricNames(metricsScope,context));
  }
 catch (  OperationException e) {
    LOG.warn(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Returns all the unique metrics in the given context
 */
@GET @Path(""String_Node_Str"") public void listContextMetrics(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String scope,@PathParam(""String_Node_Str"") final String context) throws IOException {
  try {
    MetricsScope metricsScope=MetricsScope.valueOf(scope.toUpperCase());
    responder.sendJson(HttpResponseStatus.OK,getAvailableMetricNames(metricsScope,context));
  }
 catch (  IllegalArgumentException exception) {
    responder.sendJson(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"" + MetricsScope.SYSTEM + ""String_Node_Str""+ MetricsScope.USER);
  }
catch (  OperationException e) {
    LOG.warn(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}"
7400,"/** 
 * Returns all the unique elements available in the context after the given context prefix
 */
@GET @Path(""String_Node_Str"") public void listContextsByPrefix(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String scope,@PathParam(""String_Node_Str"") final String context,@QueryParam(""String_Node_Str"") String search) throws IOException {
  try {
    if (search == null) {
      responder.sendJson(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"" + ""String_Node_Str"");
      return;
    }
    MetricsScope metricsScope=MetricsScope.valueOf(scope.toUpperCase());
    if (metricsScope == null) {
      responder.sendJson(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"" + MetricsScope.SYSTEM + ""String_Node_Str""+ MetricsScope.USER+ ""String_Node_Str"");
      return;
    }
    responder.sendJson(HttpResponseStatus.OK,getNextContext(metricsScope,context));
  }
 catch (  OperationException e) {
    LOG.warn(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Returns all the unique elements available in the context after the given context prefix
 */
@GET @Path(""String_Node_Str"") public void listContextsByPrefix(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String scope,@PathParam(""String_Node_Str"") final String context,@QueryParam(""String_Node_Str"") String search) throws IOException {
  try {
    if (search == null || !search.equals(""String_Node_Str"")) {
      responder.sendJson(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    MetricsScope metricsScope=MetricsScope.valueOf(scope.toUpperCase());
    responder.sendJson(HttpResponseStatus.OK,getNextContext(metricsScope,context));
  }
 catch (  IllegalArgumentException exception) {
    responder.sendJson(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"" + MetricsScope.SYSTEM + ""String_Node_Str""+ MetricsScope.USER);
  }
catch (  OperationException e) {
    LOG.warn(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}"
7401,"private Set<String> getNextContext(MetricsScope scope,String contextPrefix) throws OperationException {
  SortedSet<String> nextLevelContexts=Sets.newTreeSet();
  TimeSeriesTable table=timeSeriesTables.get().get(scope);
  MetricsScanQuery query=new MetricsScanQueryBuilder().setContext(contextPrefix).allowEmptyMetric().build(-1,-1);
  List<String> results=table.getNextLevelContexts(query);
  for (  String nextContext : results) {
    if (contextPrefix == null) {
      int index=nextContext.indexOf(""String_Node_Str"");
      if (index == -1) {
        nextLevelContexts.add(nextContext);
      }
 else {
        nextLevelContexts.add(nextContext.substring(0,index));
      }
    }
 else {
      String context=nextContext.substring(contextPrefix.length() + 1);
      if (context.indexOf(""String_Node_Str"") != -1) {
        nextLevelContexts.add(context.substring(0,context.indexOf(""String_Node_Str"")));
      }
    }
  }
  return nextLevelContexts;
}","private String getNextContext(String context){
  int index=context.indexOf(""String_Node_Str"");
  if (index == -1) {
    return context;
  }
 else {
    return context.substring(0,context.indexOf(""String_Node_Str""));
  }
}"
7402,"/** 
 * Returns all the unique elements available in the first context
 */
@GET @Path(""String_Node_Str"") public void listFirstContexts(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String scope,@QueryParam(""String_Node_Str"") String search) throws IOException {
  try {
    if (search == null) {
      responder.sendJson(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"" + ""String_Node_Str"");
      return;
    }
    MetricsScope metricsScope=MetricsScope.valueOf(scope.toUpperCase());
    if (metricsScope == null) {
      responder.sendJson(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"" + MetricsScope.SYSTEM + ""String_Node_Str""+ MetricsScope.USER+ ""String_Node_Str"");
      return;
    }
    responder.sendJson(HttpResponseStatus.OK,getNextContext(metricsScope,null));
  }
 catch (  OperationException e) {
    LOG.warn(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Returns all the unique elements available in the first context
 */
@GET @Path(""String_Node_Str"") public void listFirstContexts(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String scope,@QueryParam(""String_Node_Str"") String search) throws IOException {
  try {
    if (search == null || !search.equals(""String_Node_Str"")) {
      responder.sendJson(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    MetricsScope metricsScope=MetricsScope.valueOf(scope.toUpperCase());
    responder.sendJson(HttpResponseStatus.OK,getNextContext(metricsScope,null));
  }
 catch (  IllegalArgumentException exception) {
    responder.sendJson(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"" + MetricsScope.SYSTEM + ""String_Node_Str""+ MetricsScope.USER);
  }
catch (  OperationException e) {
    LOG.warn(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}"
7403,"@BeforeClass public static void setup() throws InterruptedException {
  setupMetrics();
}","@BeforeClass public static void setup() throws Exception {
  setupMetrics();
}"
7404,"private static void setupMetrics() throws InterruptedException {
  MetricsCollector collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  TimeUnit.SECONDS.sleep(2);
}","private static void setupMetrics() throws Exception {
  HttpResponse response=doDelete(""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  MetricsCollector collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  collector=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",""String_Node_Str"");
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  collector.increment(""String_Node_Str"",1);
  TimeUnit.SECONDS.sleep(2);
}"
7405,"private byte[] getNextRow(byte[] rowKey,int offset,int length){
  byte[] stopKey=Bytes.stopKeyForPrefix(Arrays.copyOfRange(rowKey,offset,offset + length));
  if (stopKey == null) {
    return null;
  }
  byte[] nextRow=new byte[rowKey.length];
  copyByteArray(rowKey,nextRow,0,0,offset);
  copyByteArray(stopKey,nextRow,0,offset,stopKey.length);
  copyByteArray(rowKey,nextRow,offset + stopKey.length,offset + stopKey.length,rowKey.length - (offset + stopKey.length));
  return nextRow;
}","private byte[] getNextRow(byte[] rowKey,int offset,int length){
  byte[] stopKey=Bytes.stopKeyForPrefix(Arrays.copyOfRange(rowKey,offset,offset + length));
  if (stopKey == null) {
    return null;
  }
  byte[] nextRow=new byte[rowKey.length];
  System.arraycopy(rowKey,0,nextRow,0,offset);
  System.arraycopy(stopKey,0,nextRow,offset,stopKey.length);
  Arrays.fill(nextRow,offset + stopKey.length,rowKey.length,(byte)0);
  return nextRow;
}"
7406,"/** 
 * The rowkey consists of the following params in that order, while padding parameter is used to apply padding during individual parameter encoding.
 * @param contextPrefix Context the metric belongs to.
 * @param metricPrefix  metric string
 * @param tagPrefix metric tag.
 * @param timeBase timeBase.
 * @param runId runId if the metric belongs to a program.
 * @param padding Padding byte to apply for padding.
 * @return byte[] representing the rowkey that may have padding for each part.
 */
public byte[] paddedEncode(String contextPrefix,String metricPrefix,String tagPrefix,int timeBase,String runId,int padding){
  int idSize=entityTable.getIdSize();
  int totalDepth=getDepth(MetricsEntityType.CONTEXT) + getDepth(MetricsEntityType.METRIC) + getDepth(MetricsEntityType.TAG)+ getDepth(MetricsEntityType.RUN);
  int sizeOfTimeBase=4;
  byte[] result=new byte[idSize * totalDepth + sizeOfTimeBase];
  int offset=0;
  paddedEncode(MetricsEntityType.CONTEXT,contextPrefix,padding,result,offset);
  offset+=idSize * getDepth(MetricsEntityType.METRIC);
  paddedEncode(MetricsEntityType.METRIC,metricPrefix,padding,result,offset);
  offset+=idSize * getDepth(MetricsEntityType.METRIC);
  paddedEncode(MetricsEntityType.TAG,tagPrefix,padding,result,offset);
  offset+=idSize * getDepth(MetricsEntityType.TAG);
  System.arraycopy(Bytes.toBytes(timeBase),0,result,offset,sizeOfTimeBase);
  offset+=sizeOfTimeBase;
  paddedEncode(MetricsEntityType.RUN,runId,padding,result,offset);
  return result;
}","/** 
 * The rowkey consists of the following params in that order, while padding parameter is used to apply padding during individual parameter encoding.
 * @param contextPrefix Context the metric belongs to.
 * @param metricPrefix  metric string
 * @param tagPrefix metric tag.
 * @param timeBase timeBase.
 * @param runId runId if the metric belongs to a program.
 * @param padding Padding byte to apply for padding.
 * @return byte[] representing the rowkey that may have padding for each part.
 */
public byte[] paddedEncode(String contextPrefix,String metricPrefix,String tagPrefix,int timeBase,String runId,int padding){
  int idSize=entityTable.getIdSize();
  int totalDepth=getDepth(MetricsEntityType.CONTEXT) + getDepth(MetricsEntityType.METRIC) + getDepth(MetricsEntityType.TAG)+ getDepth(MetricsEntityType.RUN);
  int sizeOfTimeBase=4;
  byte[] result=new byte[idSize * totalDepth + sizeOfTimeBase];
  int offset=0;
  paddedEncode(MetricsEntityType.CONTEXT,contextPrefix,padding,result,offset);
  offset+=idSize * getDepth(MetricsEntityType.CONTEXT);
  paddedEncode(MetricsEntityType.METRIC,metricPrefix,padding,result,offset);
  offset+=idSize * getDepth(MetricsEntityType.METRIC);
  paddedEncode(MetricsEntityType.TAG,tagPrefix,padding,result,offset);
  offset+=idSize * getDepth(MetricsEntityType.TAG);
  System.arraycopy(Bytes.toBytes(timeBase),0,result,offset,sizeOfTimeBase);
  offset+=sizeOfTimeBase;
  paddedEncode(MetricsEntityType.RUN,runId,padding,result,offset);
  return result;
}"
7407,"private byte[] getNextRow(byte[] rowKey,int offset,int length){
  byte[] stopKey=Bytes.stopKeyForPrefix(Arrays.copyOfRange(rowKey,offset,offset + length));
  if (stopKey == null) {
    return null;
  }
  return Bytes.concat(Bytes.head(rowKey,offset),stopKey,Bytes.tail(rowKey,rowKey.length - (offset + length)));
}","private byte[] getNextRow(byte[] rowKey,int offset,int length){
  byte[] stopKey=Bytes.stopKeyForPrefix(Arrays.copyOfRange(rowKey,offset,offset + length));
  if (stopKey == null) {
    return null;
  }
  byte[] nextRow=new byte[rowKey.length];
  copyByteArray(rowKey,nextRow,0,0,offset);
  copyByteArray(stopKey,nextRow,0,offset,stopKey.length);
  copyByteArray(rowKey,nextRow,offset + stopKey.length,offset + stopKey.length,rowKey.length - (offset + stopKey.length));
  return nextRow;
}"
7408,"private ScannerFields getScannerFields(MetricsScanQuery query,boolean shouldMatchAllTags){
  int startTimeBase=getTimeBase(query.getStartTime());
  int endTimeBase=getTimeBase(query.getEndTime());
  byte[][] columns=null;
  if ((startTimeBase == endTimeBase) && (startTimeBase != -1)) {
    int startCol=(int)(query.getStartTime() - startTimeBase) / resolution;
    int endCol=(int)(query.getEndTime() - endTimeBase) / resolution;
    columns=new byte[endCol - startCol + 1][];
    for (int i=0; i < columns.length; i++) {
      columns[i]=Bytes.toBytes((short)(startCol + i));
    }
  }
  String tagPrefix=query.getTagPrefix();
  if (!shouldMatchAllTags && tagPrefix == null) {
    tagPrefix=MetricsConstants.EMPTY_TAG;
  }
  byte[] startRow=entityCodec.paddedEncode(query.getContextPrefix(),query.getMetricPrefix(),tagPrefix,startTimeBase,query.getRunId(),0);
  byte[] endRow=entityCodec.paddedEncode(query.getContextPrefix(),query.getMetricPrefix(),tagPrefix,endTimeBase + 1,query.getRunId(),0xff);
  FuzzyRowFilter filter=getFilter(query,startTimeBase,endTimeBase,shouldMatchAllTags);
  return new ScannerFields(startRow,endRow,columns,filter);
}","private ScannerFields getScannerFields(MetricsScanQuery query,boolean shouldMatchAllTags){
  int startTimeBase=getTimeBase(query.getStartTime());
  int endTimeBase=getTimeBase(query.getEndTime());
  byte[][] columns=null;
  if (startTimeBase == endTimeBase) {
    int startCol=(int)(query.getStartTime() - startTimeBase) / resolution;
    int endCol=(int)(query.getEndTime() - endTimeBase) / resolution;
    columns=new byte[endCol - startCol + 1][];
    for (int i=0; i < columns.length; i++) {
      columns[i]=Bytes.toBytes((short)(startCol + i));
    }
  }
  String tagPrefix=query.getTagPrefix();
  if (!shouldMatchAllTags && tagPrefix == null) {
    tagPrefix=MetricsConstants.EMPTY_TAG;
  }
  byte[] startRow=entityCodec.paddedEncode(query.getContextPrefix(),query.getMetricPrefix(),tagPrefix,startTimeBase,query.getRunId(),0);
  byte[] endRow=entityCodec.paddedEncode(query.getContextPrefix(),query.getMetricPrefix(),tagPrefix,endTimeBase + 1,query.getRunId(),0xff);
  FuzzyRowFilter filter=getFilter(query,startTimeBase,endTimeBase,shouldMatchAllTags);
  return new ScannerFields(startRow,endRow,columns,filter);
}"
7409,"@Override public ClassLoader create(DatasetModuleMeta moduleMeta,ClassLoader parentClassLoader) throws IOException {
  if (moduleMeta.getJarLocation() == null) {
    return parentClassLoader;
  }
  Location jarLocation=locationFactory.create(moduleMeta.getJarLocation());
  File tmpJar=File.createTempFile(jarLocation.getName(),null);
  try {
    MessageDigest messageDigest=MESSAGE_DIGEST.get();
    DigestOutputStream digestOutput=new DigestOutputStream(new FileOutputStream(tmpJar),messageDigest);
    try {
      ByteStreams.copy(Locations.newInputSupplier(jarLocation),digestOutput);
    }
  finally {
      digestOutput.close();
    }
    String dirName=String.format(""String_Node_Str"",moduleMeta.getName(),HashCodes.fromBytes(messageDigest.digest()));
    File expandDir=new File(tmpJar.getParent(),dirName);
    if (!expandDir.isDirectory()) {
      File tempDir=Files.createTempDir();
      try {
        BundleJarUtil.unpackProgramJar(Files.newInputStreamSupplier(tmpJar),tempDir);
        if (!tempDir.renameTo(expandDir) && !expandDir.isDirectory()) {
          throw new IOException(""String_Node_Str"" + tempDir + ""String_Node_Str""+ expandDir);
        }
      }
  finally {
        try {
          if (tempDir.exists()) {
            DirUtils.deleteDirectoryContents(tempDir);
          }
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"",tempDir,e);
        }
      }
    }
    return new DirectoryClassLoader(expandDir,parentClassLoader,""String_Node_Str"");
  }
  finally {
    tmpJar.delete();
  }
}","@Override public ClassLoader create(DatasetModuleMeta moduleMeta,ClassLoader parentClassLoader) throws IOException {
  if (moduleMeta.getJarLocation() == null) {
    return parentClassLoader;
  }
  Location jarLocation=locationFactory.create(moduleMeta.getJarLocation());
  File tmpJar=File.createTempFile(jarLocation.getName(),null);
  try {
    MessageDigest messageDigest=MESSAGE_DIGEST.get();
    DigestOutputStream digestOutput=new DigestOutputStream(new FileOutputStream(tmpJar),messageDigest);
    try {
      ByteStreams.copy(Locations.newInputSupplier(jarLocation),digestOutput);
    }
  finally {
      digestOutput.close();
    }
    String dirName=String.format(""String_Node_Str"",moduleMeta.getName(),Bytes.toStringBinary(messageDigest.digest()));
    File expandDir=new File(tmpJar.getParent(),dirName);
    if (!expandDir.isDirectory()) {
      File tempDir=Files.createTempDir();
      try {
        BundleJarUtil.unpackProgramJar(Files.newInputStreamSupplier(tmpJar),tempDir);
        if (!tempDir.renameTo(expandDir) && !expandDir.isDirectory()) {
          throw new IOException(""String_Node_Str"" + tempDir + ""String_Node_Str""+ expandDir);
        }
      }
  finally {
        try {
          if (tempDir.exists()) {
            DirUtils.deleteDirectoryContents(tempDir);
          }
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"",tempDir,e);
        }
      }
    }
    return new DirectoryClassLoader(expandDir,parentClassLoader,""String_Node_Str"");
  }
  finally {
    tmpJar.delete();
  }
}"
7410,"/** 
 * Updates the request URI to its v3 URI before forwarding (or even when delegating the handler method to a method in a v3 handler)
 * @param request the original {@link HttpRequest}
 */
private void rewriteRequest(HttpRequest request){
  String originalUri=request.getUri();
  request.setUri(originalUri.replaceFirst(""String_Node_Str"",""String_Node_Str"" + Constants.DEFAULT_NAMESPACE));
}","/** 
 * Updates the request URI to its v3 URI before delegating the call to the corresponding v3 handler.
 * @param request the original {@link HttpRequest}
 */
private void rewriteRequest(HttpRequest request){
  String originalUri=request.getUri();
  request.setUri(originalUri.replaceFirst(""String_Node_Str"",""String_Node_Str"" + Constants.DEFAULT_NAMESPACE));
}"
7411,"private BodyConsumer deployAppStream(final HttpRequest request,final HttpResponder responder,final String namespaceId,final String appId) throws IOException {
  validateNamespace(namespaceId,responder);
  final String archiveName=request.getHeader(ARCHIVE_NAME_HEADER);
  if (archiveName == null || archiveName.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,ARCHIVE_NAME_HEADER + ""String_Node_Str"",ImmutableMultimap.of(HttpHeaders.Names.CONNECTION,HttpHeaders.Values.CLOSE));
    return null;
  }
  File tempDir=new File(configuration.get(Constants.CFG_LOCAL_DATA_DIR),configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  if (!DirUtils.mkdirs(tempDir)) {
    throw new IOException(""String_Node_Str"" + tempDir);
  }
  final Location archiveDir=locationFactory.create(this.archiveDir).append(namespaceId);
  final Location archive=archiveDir.append(archiveName);
  final SessionInfo sessionInfo=new SessionInfo(namespaceId,appId,archiveName,archive,DeployStatus.UPLOADING);
  sessions.put(namespaceId,sessionInfo);
  return new AbstractBodyConsumer(File.createTempFile(""String_Node_Str"",""String_Node_Str"",tempDir)){
    @Override protected void onFinish(    HttpResponder responder,    File uploadedFile){
      try {
        Locations.mkdirsIfNotExists(archiveDir);
        Location tmpLocation=archive.getTempFile(""String_Node_Str"");
        try {
          LOG.debug(""String_Node_Str"",uploadedFile,tmpLocation.toURI());
          Files.copy(uploadedFile,Locations.newOutputSupplier(tmpLocation));
          if (tmpLocation.renameTo(archive) == null) {
            throw new IOException(String.format(""String_Node_Str"",tmpLocation.toURI(),archive.toURI()));
          }
        }
 catch (        IOException e) {
          tmpLocation.delete();
          throw e;
        }
        sessionInfo.setStatus(DeployStatus.VERIFYING);
        deploy(namespaceId,appId,archive);
        sessionInfo.setStatus(DeployStatus.DEPLOYED);
        responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
      }
 catch (      Exception e) {
        sessionInfo.setStatus(DeployStatus.FAILED);
        LOG.error(""String_Node_Str"",e);
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
 finally {
        saveSessionInfo(sessionInfo.setStatus(sessionInfo.getStatus()),namespaceId);
        sessions.remove(namespaceId);
      }
    }
  }
;
}","private BodyConsumer deployAppStream(final HttpRequest request,final HttpResponder responder,final String namespaceId,final String appId) throws IOException {
  if (!namespaceExists(namespaceId)) {
    LOG.warn(""String_Node_Str"",namespaceId);
    responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"" + ""String_Node_Str"",namespaceId));
    return null;
  }
  final String archiveName=request.getHeader(ARCHIVE_NAME_HEADER);
  if (archiveName == null || archiveName.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,ARCHIVE_NAME_HEADER + ""String_Node_Str"",ImmutableMultimap.of(HttpHeaders.Names.CONNECTION,HttpHeaders.Values.CLOSE));
    return null;
  }
  File tempDir=new File(configuration.get(Constants.CFG_LOCAL_DATA_DIR),configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  if (!DirUtils.mkdirs(tempDir)) {
    throw new IOException(""String_Node_Str"" + tempDir);
  }
  final Location archiveDir=locationFactory.create(this.archiveDir).append(namespaceId);
  final Location archive=archiveDir.append(archiveName);
  final SessionInfo sessionInfo=new SessionInfo(namespaceId,appId,archiveName,archive,DeployStatus.UPLOADING);
  sessions.put(namespaceId,sessionInfo);
  return new AbstractBodyConsumer(File.createTempFile(""String_Node_Str"",""String_Node_Str"",tempDir)){
    @Override protected void onFinish(    HttpResponder responder,    File uploadedFile){
      try {
        Locations.mkdirsIfNotExists(archiveDir);
        Location tmpLocation=archive.getTempFile(""String_Node_Str"");
        try {
          LOG.debug(""String_Node_Str"",uploadedFile,tmpLocation.toURI());
          Files.copy(uploadedFile,Locations.newOutputSupplier(tmpLocation));
          if (tmpLocation.renameTo(archive) == null) {
            throw new IOException(String.format(""String_Node_Str"",tmpLocation.toURI(),archive.toURI()));
          }
        }
 catch (        IOException e) {
          tmpLocation.delete();
          throw e;
        }
        sessionInfo.setStatus(DeployStatus.VERIFYING);
        deploy(namespaceId,appId,archive);
        sessionInfo.setStatus(DeployStatus.DEPLOYED);
        responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
      }
 catch (      Exception e) {
        sessionInfo.setStatus(DeployStatus.FAILED);
        LOG.error(""String_Node_Str"",e);
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
 finally {
        saveSessionInfo(sessionInfo.setStatus(sessionInfo.getStatus()),namespaceId);
        sessions.remove(namespaceId);
      }
    }
  }
;
}"
7412,"protected static String getVersionedAPIPath(String nonVersionedApiPath,@Nullable String version,@Nullable String namespace){
  StringBuilder versionedApiBuilder=new StringBuilder(""String_Node_Str"");
  if (version == null) {
    version=Constants.Gateway.API_VERSION_2_TOKEN;
    Preconditions.checkArgument(namespace == null,String.format(""String_Node_Str"" + ""String_Node_Str"",Constants.DEFAULT_NAMESPACE));
  }
  if (Constants.Gateway.API_VERSION_2_TOKEN.equals(version)) {
    versionedApiBuilder.append(version).append(""String_Node_Str"");
  }
 else   if (Constants.Gateway.API_VERSION_3_TOKEN.equals(version)) {
    Preconditions.checkArgument(namespace != null,""String_Node_Str"");
    versionedApiBuilder.append(version).append(""String_Node_Str"").append(namespace).append(""String_Node_Str"");
  }
  versionedApiBuilder.append(nonVersionedApiPath);
  return versionedApiBuilder.toString();
}","protected static String getVersionedAPIPath(String nonVersionedApiPath,@Nullable String version,@Nullable String namespace){
  StringBuilder versionedApiBuilder=new StringBuilder(""String_Node_Str"");
  if (version == null) {
    version=Constants.Gateway.API_VERSION_2_TOKEN;
  }
  if (Constants.Gateway.API_VERSION_2_TOKEN.equals(version)) {
    Preconditions.checkArgument(namespace == null,String.format(""String_Node_Str"" + ""String_Node_Str"",Constants.DEFAULT_NAMESPACE));
    versionedApiBuilder.append(version).append(""String_Node_Str"");
  }
 else   if (Constants.Gateway.API_VERSION_3_TOKEN.equals(version)) {
    Preconditions.checkArgument(namespace != null,""String_Node_Str"");
    versionedApiBuilder.append(version).append(""String_Node_Str"").append(namespace).append(""String_Node_Str"");
  }
 else {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",version));
  }
  versionedApiBuilder.append(nonVersionedApiPath);
  return versionedApiBuilder.toString();
}"
7413,"/** 
 * Tests deploying an application in a non-existing non-default namespace.
 */
@Test public void testDeployNonExistingNamespace() throws Exception {
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,""String_Node_Str"");
  Assert.assertEquals(404,response.getStatusLine().getStatusCode());
}","/** 
 * Tests deploying an application in a non-existing non-default namespace.
 */
@Test public void testDeployNonExistingNamespace() throws Exception {
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,""String_Node_Str"");
  Assert.assertEquals(404,response.getStatusLine().getStatusCode());
  Assert.assertEquals(""String_Node_Str"",readResponse(response));
}"
7414,"/** 
 * Execute a set of operations on datasets via a   {@link co.cask.cdap.api.TxRunnable} that are committed as a single transaction.
 * @param runnable The runnable to be executed in the transaction
 */
void execute(TxRunnable runnable);","/** 
 * Execute a set of operations on datasets via a   {@link TxRunnable} that are committed as a single transaction.
 * @param runnable The runnable to be executed in the transaction
 */
void execute(TxRunnable runnable);"
7415,"@Test public void testListAndGet() throws Exception {
  final String appName=""String_Node_Str"";
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Assert.assertEquals(""String_Node_Str"",getDeploymentStatus(TEST_NAMESPACE1));
  response=deploy(AppWithDataset.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2,appName);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Assert.assertNotNull(response.getEntity());
  response=doGet(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Type typeToken=new TypeToken<List<JsonObject>>(){
  }
.getType();
  List<JsonObject> apps=readResponse(response,typeToken);
  Assert.assertEquals(1,apps.size());
  response=doGet(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  apps=readResponse(response,typeToken);
  Assert.assertEquals(1,apps.size());
  response=doGet(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  typeToken=new TypeToken<JsonObject>(){
  }
.getType();
  JsonObject result=readResponse(response,typeToken);
  Assert.assertEquals(""String_Node_Str"",result.get(""String_Node_Str"").getAsString());
  Assert.assertEquals(""String_Node_Str"",result.get(""String_Node_Str"").getAsString());
  response=doGet(getVersionedAPIPath(""String_Node_Str"" + appName,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  typeToken=new TypeToken<JsonObject>(){
  }
.getType();
  result=readResponse(response,typeToken);
  Assert.assertEquals(appName,result.get(""String_Node_Str"").getAsString());
  response=doDelete(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=doDelete(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
}","@Test public void testListAndGet() throws Exception {
  final String appName=""String_Node_Str"";
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Assert.assertEquals(""String_Node_Str"",getDeploymentStatus(TEST_NAMESPACE1));
  response=deploy(AppWithDataset.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2,appName);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Assert.assertNotNull(response.getEntity());
  List<JsonObject> apps=getAppList(TEST_NAMESPACE1);
  Assert.assertEquals(1,apps.size());
  apps=getAppList(TEST_NAMESPACE2);
  Assert.assertEquals(1,apps.size());
  JsonObject result=getAppDetails(TEST_NAMESPACE1,""String_Node_Str"");
  Assert.assertEquals(""String_Node_Str"",result.get(""String_Node_Str"").getAsString());
  Assert.assertEquals(""String_Node_Str"",result.get(""String_Node_Str"").getAsString());
  result=getAppDetails(TEST_NAMESPACE2,appName);
  Assert.assertEquals(appName,result.get(""String_Node_Str"").getAsString());
  response=doDelete(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=doDelete(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
}"
7416,"@Override public boolean nextKeyValue() throws IOException, InterruptedException {
  StreamEvent streamEvent;
  do {
    if (reader.getPosition() >= inputSplit.getStart() + inputSplit.getLength()) {
      return false;
    }
    events.clear();
    if (reader.read(events,1,0,TimeUnit.SECONDS) <= 0) {
      return false;
    }
    streamEvent=events.get(0);
  }
 while (streamEvent.getTimestamp() < inputSplit.getStartTime());
  if (streamEvent.getTimestamp() >= inputSplit.getEndTime()) {
    return false;
  }
  currentEntry=decoder.decode(streamEvent,currentEntry);
  return true;
}","@Override public boolean nextKeyValue() throws IOException, InterruptedException {
  StreamEvent streamEvent;
  do {
    if (reader.getPosition() - inputSplit.getStart() >= inputSplit.getLength()) {
      return false;
    }
    events.clear();
    if (reader.read(events,1,0,TimeUnit.SECONDS) <= 0) {
      return false;
    }
    streamEvent=events.get(0);
  }
 while (streamEvent.getTimestamp() < inputSplit.getStartTime());
  if (streamEvent.getTimestamp() >= inputSplit.getEndTime()) {
    return false;
  }
  currentEntry=decoder.decode(streamEvent,currentEntry);
  return true;
}"
7417,"@Override public ProgramController run(Program program,ProgramOptions options){
  String componentName=options.getName();
  Preconditions.checkNotNull(componentName,""String_Node_Str"");
  int instanceId=Integer.parseInt(options.getArguments().getOption(ProgramOptionConstants.INSTANCE_ID,""String_Node_Str""));
  Preconditions.checkArgument(instanceId >= 0,""String_Node_Str"");
  int instanceCount=Integer.parseInt(options.getArguments().getOption(ProgramOptionConstants.INSTANCES,""String_Node_Str""));
  Preconditions.checkArgument(instanceCount > 0,""String_Node_Str"");
  String runIdOption=options.getArguments().getOption(ProgramOptionConstants.RUN_ID);
  Preconditions.checkNotNull(runIdOption,""String_Node_Str"");
  RunId runId=RunIds.fromString(runIdOption);
  ApplicationSpecification appSpec=program.getSpecification();
  Preconditions.checkNotNull(appSpec,""String_Node_Str"");
  ProgramType programType=program.getType();
  Preconditions.checkNotNull(programType,""String_Node_Str"");
  Preconditions.checkArgument(programType == ProgramType.SERVICE,""String_Node_Str"");
  ServiceSpecification spec=appSpec.getServices().get(program.getName());
  Service component;
  if (componentName.equals(program.getName())) {
    String host=options.getArguments().getOption(ProgramOptionConstants.HOST);
    Preconditions.checkArgument(host != null,""String_Node_Str"");
    component=new ServiceHttpServer(host,program,spec,runId,serviceAnnouncer,createHttpServiceContextFactory(program,runId,instanceId,options.getUserArguments()),metricsCollectionService,dataFabricFacadeFactory);
  }
 else {
    ServiceWorkerSpecification workerSpec=spec.getWorkers().get(componentName);
    Preconditions.checkArgument(workerSpec != null,""String_Node_Str"",program.getId());
    BasicServiceWorkerContext context=new BasicServiceWorkerContext(workerSpec,program,runId,instanceId,workerSpec.getInstances(),options.getArguments(),cConf,metricsCollectionService,datasetFramework,txClient,discoveryServiceClient);
    component=new ServiceWorkerDriver(program,workerSpec,context);
  }
  ProgramControllerServiceAdapter controller=new ProgramControllerServiceAdapter(component,componentName,runId);
  component.start();
  return controller;
}","@Override public ProgramController run(Program program,ProgramOptions options){
  String componentName=options.getName();
  Preconditions.checkNotNull(componentName,""String_Node_Str"");
  int instanceId=Integer.parseInt(options.getArguments().getOption(ProgramOptionConstants.INSTANCE_ID,""String_Node_Str""));
  Preconditions.checkArgument(instanceId >= 0,""String_Node_Str"");
  int instanceCount=Integer.parseInt(options.getArguments().getOption(ProgramOptionConstants.INSTANCES,""String_Node_Str""));
  Preconditions.checkArgument(instanceCount > 0,""String_Node_Str"");
  String runIdOption=options.getArguments().getOption(ProgramOptionConstants.RUN_ID);
  Preconditions.checkNotNull(runIdOption,""String_Node_Str"");
  RunId runId=RunIds.fromString(runIdOption);
  ApplicationSpecification appSpec=program.getSpecification();
  Preconditions.checkNotNull(appSpec,""String_Node_Str"");
  ProgramType programType=program.getType();
  Preconditions.checkNotNull(programType,""String_Node_Str"");
  Preconditions.checkArgument(programType == ProgramType.SERVICE,""String_Node_Str"");
  ServiceSpecification spec=appSpec.getServices().get(program.getName());
  Service component;
  if (componentName.equals(program.getName())) {
    String host=options.getArguments().getOption(ProgramOptionConstants.HOST);
    Preconditions.checkArgument(host != null,""String_Node_Str"");
    component=new ServiceHttpServer(host,program,spec,runId,serviceAnnouncer,createHttpServiceContextFactory(program,runId,instanceId,options.getUserArguments()),metricsCollectionService,dataFabricFacadeFactory);
  }
 else {
    ServiceWorkerSpecification workerSpec=spec.getWorkers().get(componentName);
    Preconditions.checkArgument(workerSpec != null,""String_Node_Str"",program.getId());
    BasicServiceWorkerContext context=new BasicServiceWorkerContext(workerSpec,program,runId,instanceId,workerSpec.getInstances(),options.getUserArguments(),cConf,metricsCollectionService,datasetFramework,txClient,discoveryServiceClient);
    component=new ServiceWorkerDriver(program,workerSpec,context);
  }
  ProgramControllerServiceAdapter controller=new ProgramControllerServiceAdapter(component,componentName,runId);
  component.start();
  return controller;
}"
7418,"@Override public void stop(){
  getContext().execute(new TxRunnable(){
    @Override public void run(    DatasetContext context) throws Exception {
      KeyValueTable table=context.getDataset(DATASET_NAME);
      table.write(DATASET_TEST_KEY_STOP,DATASET_TEST_VALUE_STOP);
    }
  }
);
  workerStopped=true;
}","@Override public void stop(){
  getContext().execute(new TxRunnable(){
    @Override public void run(    DatasetContext context) throws Exception {
      KeyValueTable table=context.getDataset(DATASET_NAME);
      table.write(DATASET_TEST_KEY_STOP,valueToWriteOnStop);
    }
  }
);
  workerStopped=true;
}"
7419,"@Override public void run(DatasetContext context) throws Exception {
  KeyValueTable table=context.getDataset(DATASET_NAME);
  if (datasetHashCode == System.identityHashCode(table)) {
    table.write(DATASET_TEST_KEY,DATASET_TEST_VALUE);
  }
}","@Override public void run(DatasetContext context) throws Exception {
  KeyValueTable table=context.getDataset(DATASET_NAME);
  if (datasetHashCode == System.identityHashCode(table)) {
    table.write(DATASET_TEST_KEY,valueToWriteOnRun);
  }
}"
7420,"@Override public void initialize(ServiceWorkerContext context) throws Exception {
  super.initialize(context);
  getContext().execute(new TxRunnable(){
    @Override public void run(    DatasetContext context) throws Exception {
      KeyValueTable table=context.getDataset(DATASET_NAME);
      datasetHashCode=System.identityHashCode(table);
    }
  }
);
}","@Override public void initialize(ServiceWorkerContext context) throws Exception {
  super.initialize(context);
  valueToWriteOnRun=context.getRuntimeArguments().get(WRITE_VALUE_RUN_KEY);
  valueToWriteOnStop=context.getRuntimeArguments().get(WRITE_VALUE_STOP_KEY);
  getContext().execute(new TxRunnable(){
    @Override public void run(    DatasetContext context) throws Exception {
      KeyValueTable table=context.getDataset(DATASET_NAME);
      datasetHashCode=System.identityHashCode(table);
    }
  }
);
}"
7421,"@Category(SlowTests.class) @Test public void testAppWithServices() throws Exception {
  ApplicationManager applicationManager=deployApplication(AppWithServices.class);
  try {
    LOG.info(""String_Node_Str"");
    ServiceManager serviceManager=applicationManager.startService(AppWithServices.SERVICE_NAME);
    serviceStatusCheck(serviceManager,true);
    LOG.info(""String_Node_Str"");
    URL url=new URL(serviceManager.getServiceURL(5,TimeUnit.SECONDS),""String_Node_Str"");
    HttpRequest request=HttpRequest.get(url).build();
    HttpResponse response=HttpRequests.execute(request);
    Assert.assertEquals(200,response.getResponseCode());
    url=new URL(serviceManager.getServiceURL(5,TimeUnit.SECONDS),""String_Node_Str"");
    request=HttpRequest.get(url).build();
    response=HttpRequests.execute(request);
    Assert.assertEquals(500,response.getResponseCode());
    url=new URL(serviceManager.getServiceURL(5,TimeUnit.SECONDS),""String_Node_Str"");
    request=HttpRequest.get(url).build();
    response=HttpRequests.execute(request);
    Assert.assertEquals(200,response.getResponseCode());
    RuntimeMetrics serviceMetrics=RuntimeStats.getServiceMetrics(AppWithServices.APP_NAME,AppWithServices.SERVICE_NAME);
    serviceMetrics.waitForinput(3,5,TimeUnit.SECONDS);
    Assert.assertEquals(3,serviceMetrics.getInput());
    Assert.assertEquals(2,serviceMetrics.getProcessed());
    Assert.assertEquals(1,serviceMetrics.getException());
    LOG.info(""String_Node_Str"");
    ServiceManager datasetWorkerServiceManager=applicationManager.startService(AppWithServices.DATASET_WORKER_SERVICE_NAME);
    serviceStatusCheck(datasetWorkerServiceManager,true);
    ProcedureManager procedureManager=applicationManager.startProcedure(""String_Node_Str"");
    ProcedureClient procedureClient=procedureManager.getClient();
    String result=procedureClient.query(""String_Node_Str"",ImmutableMap.of(AppWithServices.PROCEDURE_DATASET_KEY,AppWithServices.DATASET_TEST_KEY));
    String decodedResult=new Gson().fromJson(result,String.class);
    Assert.assertEquals(AppWithServices.DATASET_TEST_VALUE,decodedResult);
    String path=String.format(""String_Node_Str"",AppWithServices.APP_NAME,AppWithServices.DATASET_WORKER_SERVICE_NAME);
    url=new URL(serviceManager.getServiceURL(5,TimeUnit.SECONDS),path);
    request=HttpRequest.get(url).build();
    response=HttpRequests.execute(request);
    Assert.assertEquals(200,response.getResponseCode());
    datasetWorkerServiceManager.stop();
    serviceStatusCheck(datasetWorkerServiceManager,false);
    LOG.info(""String_Node_Str"");
    serviceManager.stop();
    serviceStatusCheck(serviceManager,false);
    LOG.info(""String_Node_Str"");
    result=procedureClient.query(""String_Node_Str"",ImmutableMap.of(AppWithServices.PROCEDURE_DATASET_KEY,AppWithServices.DATASET_TEST_KEY_STOP));
    decodedResult=new Gson().fromJson(result,String.class);
    Assert.assertEquals(AppWithServices.DATASET_TEST_VALUE_STOP,decodedResult);
    procedureManager.stop();
  }
  finally {
    applicationManager.stopAll();
  }
}","@Category(SlowTests.class) @Test public void testAppWithServices() throws Exception {
  ApplicationManager applicationManager=deployApplication(AppWithServices.class);
  try {
    LOG.info(""String_Node_Str"");
    ServiceManager serviceManager=applicationManager.startService(AppWithServices.SERVICE_NAME);
    serviceStatusCheck(serviceManager,true);
    LOG.info(""String_Node_Str"");
    URL url=new URL(serviceManager.getServiceURL(5,TimeUnit.SECONDS),""String_Node_Str"");
    HttpRequest request=HttpRequest.get(url).build();
    HttpResponse response=HttpRequests.execute(request);
    Assert.assertEquals(200,response.getResponseCode());
    url=new URL(serviceManager.getServiceURL(5,TimeUnit.SECONDS),""String_Node_Str"");
    request=HttpRequest.get(url).build();
    response=HttpRequests.execute(request);
    Assert.assertEquals(500,response.getResponseCode());
    url=new URL(serviceManager.getServiceURL(5,TimeUnit.SECONDS),""String_Node_Str"");
    request=HttpRequest.get(url).build();
    response=HttpRequests.execute(request);
    Assert.assertEquals(200,response.getResponseCode());
    RuntimeMetrics serviceMetrics=RuntimeStats.getServiceMetrics(AppWithServices.APP_NAME,AppWithServices.SERVICE_NAME);
    serviceMetrics.waitForinput(3,5,TimeUnit.SECONDS);
    Assert.assertEquals(3,serviceMetrics.getInput());
    Assert.assertEquals(2,serviceMetrics.getProcessed());
    Assert.assertEquals(1,serviceMetrics.getException());
    LOG.info(""String_Node_Str"");
    Map<String,String> args=ImmutableMap.of(AppWithServices.WRITE_VALUE_RUN_KEY,AppWithServices.DATASET_TEST_VALUE,AppWithServices.WRITE_VALUE_STOP_KEY,AppWithServices.DATASET_TEST_VALUE_STOP);
    ServiceManager datasetWorkerServiceManager=applicationManager.startService(AppWithServices.DATASET_WORKER_SERVICE_NAME,args);
    serviceStatusCheck(datasetWorkerServiceManager,true);
    ProcedureManager procedureManager=applicationManager.startProcedure(""String_Node_Str"");
    ProcedureClient procedureClient=procedureManager.getClient();
    String result=procedureClient.query(""String_Node_Str"",ImmutableMap.of(AppWithServices.PROCEDURE_DATASET_KEY,AppWithServices.DATASET_TEST_KEY));
    String decodedResult=new Gson().fromJson(result,String.class);
    Assert.assertEquals(AppWithServices.DATASET_TEST_VALUE,decodedResult);
    String path=String.format(""String_Node_Str"",AppWithServices.APP_NAME,AppWithServices.DATASET_WORKER_SERVICE_NAME);
    url=new URL(serviceManager.getServiceURL(5,TimeUnit.SECONDS),path);
    request=HttpRequest.get(url).build();
    response=HttpRequests.execute(request);
    Assert.assertEquals(200,response.getResponseCode());
    datasetWorkerServiceManager.stop();
    serviceStatusCheck(datasetWorkerServiceManager,false);
    LOG.info(""String_Node_Str"");
    serviceManager.stop();
    serviceStatusCheck(serviceManager,false);
    LOG.info(""String_Node_Str"");
    result=procedureClient.query(""String_Node_Str"",ImmutableMap.of(AppWithServices.PROCEDURE_DATASET_KEY,AppWithServices.DATASET_TEST_KEY_STOP));
    decodedResult=new Gson().fromJson(result,String.class);
    Assert.assertEquals(AppWithServices.DATASET_TEST_VALUE_STOP,decodedResult);
    procedureManager.stop();
  }
  finally {
    applicationManager.stopAll();
  }
}"
7422,"protected void sendMetrics(String context,int containers,int memory,int vcores,String runId){
  LOG.trace(""String_Node_Str"",context,containers,memory,vcores);
  MetricsCollector collector=collectionService.getCollector(MetricsScope.SYSTEM,context,runId);
  collector.increment(METRIC_CONTAINERS,containers);
  collector.increment(METRIC_MEMORY_USAGE,memory);
  collector.increment(METRIC_VIRTUAL_CORE_USAGE,vcores);
}","protected void sendMetrics(String context,int containers,int memory,int vcores,String runId){
  LOG.trace(""String_Node_Str"",context,containers,memory,vcores);
  MetricsCollector collector=collectionService.getCollector(MetricsScope.SYSTEM,context,runId);
  collector.gauge(METRIC_CONTAINERS,containers);
  collector.gauge(METRIC_MEMORY_USAGE,memory);
  collector.gauge(METRIC_VIRTUAL_CORE_USAGE,vcores);
}"
7423,"@Override public void reportResources(){
  for (  TwillRunner.LiveInfo info : twillRunner.lookupLive()) {
    String metricContext=getMetricContext(info);
    if (metricContext == null) {
      continue;
    }
    int containers=0;
    int memory=0;
    int vcores=0;
    for (    TwillController controller : info.getControllers()) {
      ResourceReport report=controller.getResourceReport();
      if (report == null) {
        continue;
      }
      containers++;
      memory+=report.getAppMasterResources().getMemoryMB();
      vcores+=report.getAppMasterResources().getVirtualCores();
      sendMetrics(metricContext,containers,memory,vcores,controller.getRunId().getId());
    }
  }
  reportClusterStorage();
  reportClusterMemory();
}","@Override public void reportResources(){
  for (  TwillRunner.LiveInfo info : twillRunner.lookupLive()) {
    String metricContext=getMetricContext(info);
    if (metricContext == null) {
      continue;
    }
    for (    TwillController controller : info.getControllers()) {
      ResourceReport report=controller.getResourceReport();
      if (report == null) {
        continue;
      }
      int memory=report.getAppMasterResources().getMemoryMB();
      int vcores=report.getAppMasterResources().getVirtualCores();
      sendMetrics(metricContext,1,memory,vcores,controller.getRunId().getId());
    }
  }
  reportClusterStorage();
  reportClusterMemory();
}"
7424,"private static HttpURLConnection openStreamConnection(String streamName) throws IOException {
  int port=streamHttpService.getBindAddress().getPort();
  URL url=new URL(String.format(""String_Node_Str"",port,Constants.Gateway.GATEWAY_VERSION,streamName));
  return (HttpURLConnection)url.openConnection();
}","private static HttpURLConnection openStreamConnection(String streamName) throws IOException {
  int port=streamHttpService.getBindAddress().getPort();
  URL url=new URL(String.format(""String_Node_Str"",port,Constants.Gateway.API_VERSION_2,streamName));
  return (HttpURLConnection)url.openConnection();
}"
7425,"@GET @Path(""String_Node_Str"") public void getNamespace(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespace){
  try {
    NamespaceMeta ns=store.getNamespace(Id.Namespace.from(namespace));
    if (ns == null) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",namespace));
      return;
    }
    responder.sendJson(HttpResponseStatus.OK,ns);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","@GET @Path(""String_Node_Str"") public void getNamespace(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespace){
  try {
    NamespaceMeta ns=store.getNamespace(Id.Namespace.from(namespace));
    if (ns == null) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",namespace));
      return;
    }
    responder.sendJson(HttpResponseStatus.OK,ns);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",namespace,e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}"
7426,"@PUT @Path(""String_Node_Str"") public void create(HttpRequest request,HttpResponder responder){
  NamespaceMeta metadata;
  try {
    metadata=parseBody(request,NamespaceMeta.class);
  }
 catch (  JsonSyntaxException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
catch (  IOException e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    return;
  }
  String name=metadata.getName();
  if (name == null || name.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  String displayName=metadata.getDisplayName();
  if (displayName == null || displayName.isEmpty()) {
    displayName=name;
  }
  String description=metadata.getDescription();
  if (description == null) {
    description=""String_Node_Str"";
  }
  try {
    NamespaceMeta existing=store.createNamespace(new NamespaceMeta.Builder().setName(name).setDisplayName(displayName).setDescription(description).build());
    if (existing == null) {
      responder.sendStatus(HttpResponseStatus.OK);
    }
 else {
      responder.sendString(HttpResponseStatus.CONFLICT,String.format(""String_Node_Str"",name));
    }
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","@PUT @Path(""String_Node_Str"") public void create(HttpRequest request,HttpResponder responder){
  NamespaceMeta metadata;
  try {
    metadata=parseBody(request,NamespaceMeta.class);
  }
 catch (  JsonSyntaxException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
catch (  IOException e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
    return;
  }
  String name=metadata.getName();
  if (name == null || name.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  String displayName=metadata.getDisplayName();
  if (displayName == null || displayName.isEmpty()) {
    displayName=name;
  }
  String description=metadata.getDescription();
  if (description == null) {
    description=""String_Node_Str"";
  }
  try {
    NamespaceMeta existing=store.createNamespace(new NamespaceMeta.Builder().setName(name).setDisplayName(displayName).setDescription(description).build());
    if (existing == null) {
      responder.sendStatus(HttpResponseStatus.OK);
    }
 else {
      responder.sendString(HttpResponseStatus.CONFLICT,String.format(""String_Node_Str"",name));
    }
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}"
7427,"@Override public void initialize(MapReduceContext context) throws Exception {
  userProfileServiceURL=context.getServiceURL(UserProfileService.SERVICE_NAME);
}","@Override public void initialize(MapReduceContext context) throws Exception {
  userProfileServiceURL=context.getServiceURL(UserProfileServiceHandler.SERVICE_NAME);
}"
7428,"@Test public void test() throws TimeoutException, InterruptedException, IOException {
  ApplicationManager appManager=deployApplication(PurchaseApp.class);
  FlowManager flowManager=appManager.startFlow(""String_Node_Str"");
  StreamWriter streamWriter=appManager.getStreamWriter(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  try {
    RuntimeMetrics metrics=RuntimeStats.getFlowletMetrics(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
    metrics.waitForProcessed(5,15,TimeUnit.SECONDS);
  }
  finally {
    flowManager.stop();
  }
  ServiceManager userProfileServiceManager=appManager.startService(UserProfileService.SERVICE_NAME);
  serviceStatusCheck(userProfileServiceManager,true);
  URL setUserProfileURL=new URL(userProfileServiceManager.getServiceURL(),UserProfileServiceHandler.USER_ENDPOINT);
  HttpURLConnection setUserProfileConnection=(HttpURLConnection)setUserProfileURL.openConnection();
  String userProfileJson=""String_Node_Str"";
  try {
    setUserProfileConnection.setDoOutput(true);
    setUserProfileConnection.setRequestMethod(""String_Node_Str"");
    setUserProfileConnection.getOutputStream().write(userProfileJson.getBytes(Charsets.UTF_8));
    Assert.assertEquals(HttpURLConnection.HTTP_OK,setUserProfileConnection.getResponseCode());
  }
  finally {
    setUserProfileConnection.disconnect();
  }
  URL getUserProfileURL=new URL(userProfileServiceManager.getServiceURL(),UserProfileServiceHandler.USER_ENDPOINT + ""String_Node_Str"");
  HttpURLConnection getUserProfileConnection=(HttpURLConnection)getUserProfileURL.openConnection();
  Assert.assertEquals(HttpURLConnection.HTTP_OK,getUserProfileConnection.getResponseCode());
  String customerJson;
  try {
    customerJson=new String(ByteStreams.toByteArray(getUserProfileConnection.getInputStream()),Charsets.UTF_8);
  }
  finally {
    getUserProfileConnection.disconnect();
  }
  UserProfile profileFromService=GSON.fromJson(customerJson,UserProfile.class);
  Assert.assertEquals(profileFromService.getFirstName(),""String_Node_Str"");
  Assert.assertEquals(profileFromService.getLastName(),""String_Node_Str"");
  MapReduceManager mapReduceManager=appManager.startMapReduce(""String_Node_Str"",ImmutableMap.<String,String>of());
  mapReduceManager.waitForFinish(3,TimeUnit.MINUTES);
  ServiceManager purchaseHistoryServiceManager=appManager.startService(PurchaseHistoryService.SERVICE_NAME);
  serviceStatusCheck(purchaseHistoryServiceManager,true);
  URL url=new URL(purchaseHistoryServiceManager.getServiceURL(),""String_Node_Str"");
  HttpURLConnection conn=(HttpURLConnection)url.openConnection();
  Assert.assertEquals(HttpURLConnection.HTTP_OK,conn.getResponseCode());
  String historyJson;
  try {
    historyJson=new String(ByteStreams.toByteArray(conn.getInputStream()),Charsets.UTF_8);
  }
  finally {
    conn.disconnect();
  }
  PurchaseHistory history=GSON.fromJson(historyJson,PurchaseHistory.class);
  Assert.assertEquals(""String_Node_Str"",history.getCustomer());
  Assert.assertEquals(2,history.getPurchases().size());
  UserProfile profileFromPurchaseHistory=history.getUserProfile();
  Assert.assertEquals(profileFromPurchaseHistory.getFirstName(),""String_Node_Str"");
  Assert.assertEquals(profileFromPurchaseHistory.getLastName(),""String_Node_Str"");
  appManager.stopAll();
}","@Test public void test() throws TimeoutException, InterruptedException, IOException {
  ApplicationManager appManager=deployApplication(PurchaseApp.class);
  FlowManager flowManager=appManager.startFlow(""String_Node_Str"");
  StreamWriter streamWriter=appManager.getStreamWriter(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  streamWriter.send(""String_Node_Str"");
  try {
    RuntimeMetrics metrics=RuntimeStats.getFlowletMetrics(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
    metrics.waitForProcessed(5,15,TimeUnit.SECONDS);
  }
  finally {
    flowManager.stop();
  }
  ServiceManager userProfileServiceManager=appManager.startService(UserProfileServiceHandler.SERVICE_NAME);
  serviceStatusCheck(userProfileServiceManager,true);
  URL setUserProfileURL=new URL(userProfileServiceManager.getServiceURL(),UserProfileServiceHandler.USER_ENDPOINT);
  HttpURLConnection setUserProfileConnection=(HttpURLConnection)setUserProfileURL.openConnection();
  String userProfileJson=""String_Node_Str"";
  try {
    setUserProfileConnection.setDoOutput(true);
    setUserProfileConnection.setRequestMethod(""String_Node_Str"");
    setUserProfileConnection.getOutputStream().write(userProfileJson.getBytes(Charsets.UTF_8));
    Assert.assertEquals(HttpURLConnection.HTTP_OK,setUserProfileConnection.getResponseCode());
  }
  finally {
    setUserProfileConnection.disconnect();
  }
  URL getUserProfileURL=new URL(userProfileServiceManager.getServiceURL(),UserProfileServiceHandler.USER_ENDPOINT + ""String_Node_Str"");
  HttpURLConnection getUserProfileConnection=(HttpURLConnection)getUserProfileURL.openConnection();
  Assert.assertEquals(HttpURLConnection.HTTP_OK,getUserProfileConnection.getResponseCode());
  String customerJson;
  try {
    customerJson=new String(ByteStreams.toByteArray(getUserProfileConnection.getInputStream()),Charsets.UTF_8);
  }
  finally {
    getUserProfileConnection.disconnect();
  }
  UserProfile profileFromService=GSON.fromJson(customerJson,UserProfile.class);
  Assert.assertEquals(profileFromService.getFirstName(),""String_Node_Str"");
  Assert.assertEquals(profileFromService.getLastName(),""String_Node_Str"");
  MapReduceManager mapReduceManager=appManager.startMapReduce(""String_Node_Str"",ImmutableMap.<String,String>of());
  mapReduceManager.waitForFinish(3,TimeUnit.MINUTES);
  ServiceManager purchaseHistoryServiceManager=appManager.startService(PurchaseHistoryService.SERVICE_NAME);
  serviceStatusCheck(purchaseHistoryServiceManager,true);
  URL url=new URL(purchaseHistoryServiceManager.getServiceURL(),""String_Node_Str"");
  HttpURLConnection conn=(HttpURLConnection)url.openConnection();
  Assert.assertEquals(HttpURLConnection.HTTP_OK,conn.getResponseCode());
  String historyJson;
  try {
    historyJson=new String(ByteStreams.toByteArray(conn.getInputStream()),Charsets.UTF_8);
  }
  finally {
    conn.disconnect();
  }
  PurchaseHistory history=GSON.fromJson(historyJson,PurchaseHistory.class);
  Assert.assertEquals(""String_Node_Str"",history.getCustomer());
  Assert.assertEquals(2,history.getPurchases().size());
  UserProfile profileFromPurchaseHistory=history.getUserProfile();
  Assert.assertEquals(profileFromPurchaseHistory.getFirstName(),""String_Node_Str"");
  Assert.assertEquals(profileFromPurchaseHistory.getLastName(),""String_Node_Str"");
  appManager.stopAll();
}"
7429,"/** 
 * Children call this method to signal the program is started.
 */
protected final void started(){
  if (!state.compareAndSet(State.STARTING,State.ALIVE)) {
    LOG.debug(""String_Node_Str"",programName,runId);
    return;
  }
  LOG.debug(""String_Node_Str"",programName,runId);
  executor(State.ALIVE).execute(new Runnable(){
    @Override public void run(){
      state.set(State.ALIVE);
      caller.alive();
    }
  }
);
}","/** 
 * Children call this method to signal the program is started.
 */
protected final void started(){
  if (!state.compareAndSet(State.STARTING,State.ALIVE)) {
    LOG.debug(""String_Node_Str"",state.get(),programName,runId);
    return;
  }
  LOG.debug(""String_Node_Str"",programName,runId);
  executor(State.ALIVE).execute(new Runnable(){
    @Override public void run(){
      state.set(State.ALIVE);
      caller.alive();
    }
  }
);
}"
7430,"/** 
 * Creates all   {@link ProcessSpecification} for the process methods of the flowlet class.
 * @param flowletType Type of the flowlet class represented by {@link TypeToken}.
 * @param processMethodFactory A {@link ProcessMethodFactory} for creating {@link ProcessMethod}.
 * @param processSpecFactory A {@link ProcessSpecificationFactory} for creating {@link ProcessSpecification}.
 * @param result A {@link Collection} for storing newly created {@link ProcessSpecification}.
 * @return The same {@link Collection} as the {@code result} parameter.
 */
@SuppressWarnings(""String_Node_Str"") private <T extends Collection<ProcessSpecification<?>>>T createProcessSpecification(BasicFlowletContext flowletContext,TypeToken<? extends Flowlet> flowletType,ProcessMethodFactory processMethodFactory,ProcessSpecificationFactory processSpecFactory,T result) throws NoSuchMethodException {
  Set<FlowletMethod> seenMethods=Sets.newHashSet();
  for (  TypeToken<?> type : flowletType.getTypes().classes()) {
    if (type.getRawType().equals(Object.class)) {
      break;
    }
    for (    Method method : type.getRawType().getDeclaredMethods()) {
      if (!seenMethods.add(new FlowletMethod(method,flowletType))) {
        continue;
      }
      ProcessInput processInputAnnotation=method.getAnnotation(ProcessInput.class);
      Tick tickAnnotation=method.getAnnotation(Tick.class);
      if (processInputAnnotation == null && tickAnnotation == null) {
        continue;
      }
      int maxRetries=(tickAnnotation == null) ? processInputAnnotation.maxRetries() : tickAnnotation.maxRetries();
      ProcessMethod processMethod=processMethodFactory.create(method,maxRetries);
      Set<String> inputNames;
      Schema schema;
      TypeToken<?> dataType;
      ConsumerConfig consumerConfig;
      int batchSize=1;
      if (tickAnnotation != null) {
        inputNames=ImmutableSet.of();
        consumerConfig=new ConsumerConfig(0,0,1,DequeueStrategy.FIFO,null);
        schema=Schema.of(Schema.Type.NULL);
        dataType=TypeToken.of(void.class);
      }
 else {
        inputNames=Sets.newHashSet(processInputAnnotation.value());
        if (inputNames.isEmpty()) {
          inputNames.add(FlowletDefinition.ANY_INPUT);
        }
        dataType=flowletType.resolveType(method.getGenericParameterTypes()[0]);
        consumerConfig=getConsumerConfig(flowletContext,method);
        Integer processBatchSize=getBatchSize(method);
        if (processBatchSize != null) {
          if (dataType.getRawType().equals(Iterator.class)) {
            Preconditions.checkArgument(dataType.getType() instanceof ParameterizedType,""String_Node_Str"");
            dataType=flowletType.resolveType(((ParameterizedType)dataType.getType()).getActualTypeArguments()[0]);
          }
          batchSize=processBatchSize;
        }
        try {
          schema=schemaGenerator.generate(dataType.getType());
        }
 catch (        UnsupportedTypeException e) {
          throw Throwables.propagate(e);
        }
      }
      ProcessSpecification processSpec=processSpecFactory.create(inputNames,schema,dataType,processMethod,consumerConfig,batchSize,tickAnnotation);
      if (processSpec != null) {
        result.add(processSpec);
      }
    }
  }
  Preconditions.checkArgument(!result.isEmpty(),""String_Node_Str"" + flowletType);
  return result;
}","/** 
 * Creates all   {@link ProcessSpecification} for the process methods of the flowlet class.
 * @param flowletType Type of the flowlet class represented by {@link TypeToken}.
 * @param processMethodFactory A {@link ProcessMethodFactory} for creating {@link ProcessMethod}.
 * @param processSpecFactory A {@link ProcessSpecificationFactory} for creating {@link ProcessSpecification}.
 * @param result A {@link Collection} for storing newly created {@link ProcessSpecification}.
 * @return The same {@link Collection} as the {@code result} parameter.
 */
@SuppressWarnings(""String_Node_Str"") private <T extends Collection<ProcessSpecification<?>>>T createProcessSpecification(BasicFlowletContext flowletContext,TypeToken<? extends Flowlet> flowletType,ProcessMethodFactory processMethodFactory,ProcessSpecificationFactory processSpecFactory,T result) throws NoSuchMethodException {
  Set<FlowletMethod> seenMethods=Sets.newHashSet();
  for (  TypeToken<?> type : flowletType.getTypes().classes()) {
    if (type.getRawType().equals(Object.class)) {
      break;
    }
    for (    Method method : type.getRawType().getDeclaredMethods()) {
      if (!seenMethods.add(new FlowletMethod(method,flowletType))) {
        continue;
      }
      ProcessInput processInputAnnotation=method.getAnnotation(ProcessInput.class);
      Tick tickAnnotation=method.getAnnotation(Tick.class);
      if (processInputAnnotation == null && tickAnnotation == null) {
        continue;
      }
      int maxRetries=(tickAnnotation == null) ? processInputAnnotation.maxRetries() : tickAnnotation.maxRetries();
      ProcessMethod processMethod=processMethodFactory.create(method,maxRetries);
      Set<String> inputNames;
      Schema schema;
      TypeToken<?> dataType;
      ConsumerConfig consumerConfig;
      int batchSize=1;
      if (tickAnnotation != null) {
        inputNames=ImmutableSet.of();
        consumerConfig=new ConsumerConfig(0,0,1,DequeueStrategy.FIFO,null);
        schema=Schema.of(Schema.Type.NULL);
        dataType=TypeToken.of(void.class);
      }
 else {
        inputNames=Sets.newHashSet(processInputAnnotation.value());
        if (inputNames.isEmpty()) {
          inputNames.add(FlowletDefinition.ANY_INPUT);
        }
        dataType=flowletType.resolveType(method.getGenericParameterTypes()[0]);
        consumerConfig=getConsumerConfig(flowletContext,method);
        Integer processBatchSize=getBatchSize(method);
        if (processBatchSize != null) {
          if (dataType.getRawType().equals(Iterator.class)) {
            Preconditions.checkArgument(dataType.getType() instanceof ParameterizedType,""String_Node_Str"");
            dataType=flowletType.resolveType(((ParameterizedType)dataType.getType()).getActualTypeArguments()[0]);
          }
          batchSize=processBatchSize;
        }
        try {
          schema=schemaGenerator.generate(dataType.getType());
        }
 catch (        UnsupportedTypeException e) {
          throw Throwables.propagate(e);
        }
      }
      ProcessSpecification processSpec=processSpecFactory.create(inputNames,schema,dataType,processMethod,consumerConfig,batchSize,tickAnnotation);
      if (processSpec != null) {
        result.add(processSpec);
      }
    }
  }
  Preconditions.checkArgument(!result.isEmpty(),""String_Node_Str"",flowletContext.getFlowletId(),flowletContext.getFlowId(),flowletContext.getApplicationId(),flowletType);
  return result;
}"
7431,"@Override public void initialize(Configuration conf,Properties properties) throws SerDeException {
  columnNames=Lists.newArrayList(properties.getProperty(serdeConstants.LIST_COLUMNS).split(""String_Node_Str""));
  columnTypes=TypeInfoUtils.getTypeInfosFromTypeString(properties.getProperty(serdeConstants.LIST_COLUMN_TYPES));
  int numCols=columnNames.size();
  final List<ObjectInspector> columnOIs=new ArrayList<ObjectInspector>(numCols);
  for (int i=0; i < numCols; i++) {
    columnOIs.add(TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(columnTypes.get(i)));
  }
  this.inspector=ObjectInspectorFactory.getStandardStructObjectInspector(columnNames,columnOIs);
  String streamName=properties.getProperty(Constants.Explore.STREAM_NAME);
  try {
    ContextManager.Context context=ContextManager.getContext(conf);
    StreamAdmin streamAdmin=context.getStreamAdmin();
    StreamConfig streamConfig=streamAdmin.getConfig(streamName);
    Location streamPath=StreamUtils.createGenerationLocation(streamConfig.getLocation(),StreamUtils.getGeneration(streamConfig));
    StreamInputFormatConfigurer.setTTL(conf,streamConfig.getTTL());
    StreamInputFormatConfigurer.setStreamPath(conf,streamPath.toURI());
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",e);
    throw new SerDeException(e);
  }
}","@Override public void initialize(Configuration conf,Properties properties) throws SerDeException {
  columnNames=Lists.newArrayList(properties.getProperty(serdeConstants.LIST_COLUMNS).split(""String_Node_Str""));
  columnTypes=TypeInfoUtils.getTypeInfosFromTypeString(properties.getProperty(serdeConstants.LIST_COLUMN_TYPES));
  int numCols=columnNames.size();
  final List<ObjectInspector> columnOIs=new ArrayList<ObjectInspector>(numCols);
  for (int i=0; i < numCols; i++) {
    columnOIs.add(TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(columnTypes.get(i)));
  }
  this.inspector=ObjectInspectorFactory.getStandardStructObjectInspector(columnNames,columnOIs);
}"
7432,"@Override public Class<? extends InputFormat> getInputFormatClass(){
  return StreamInputFormat.class;
}","@Override public Class<? extends InputFormat> getInputFormatClass(){
  return HiveStreamInputFormat.class;
}"
7433,"/** 
 * Creates a jar that contains everything that are needed for running the MapReduce program by Hadoop.
 * @return a new {@link Location} containing the job jar
 */
private Location buildJobJar(BasicMapReduceContext context) throws IOException {
  ApplicationBundler appBundler=new ApplicationBundler(ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),ImmutableList.of(""String_Node_Str"",""String_Node_Str""));
  Id.Program programId=context.getProgram().getId();
  Location jobJar=locationFactory.create(String.format(""String_Node_Str"",ProgramType.MAPREDUCE.name().toLowerCase(),programId.getAccountId(),programId.getApplicationId(),programId.getId(),context.getRunId().getId()));
  LOG.debug(""String_Node_Str"",jobJar.toURI());
  Set<Class<?>> classes=Sets.newHashSet();
  classes.add(MapReduce.class);
  classes.add(MapperWrapper.class);
  classes.add(ReducerWrapper.class);
  Job jobConf=context.getHadoopJob();
  try {
    Class<? extends InputFormat<?,?>> inputFormatClass=jobConf.getInputFormatClass();
    LOG.info(""String_Node_Str"",inputFormatClass,inputFormatClass.getClassLoader());
    classes.add(inputFormatClass);
    if (StreamInputFormatConfigurer.class.isAssignableFrom(inputFormatClass)) {
      Class<? extends StreamEventDecoder> decoderType=StreamInputFormatConfigurer.getDecoderClass(jobConf.getConfiguration());
      if (decoderType != null) {
        classes.add(decoderType);
      }
    }
  }
 catch (  Throwable t) {
    LOG.info(""String_Node_Str"",t.getMessage(),t);
  }
  try {
    Class<? extends OutputFormat<?,?>> outputFormatClass=jobConf.getOutputFormatClass();
    LOG.info(""String_Node_Str"",outputFormatClass,outputFormatClass.getClassLoader());
    classes.add(outputFormatClass);
  }
 catch (  Throwable t) {
    LOG.info(""String_Node_Str"",t.getMessage(),t);
  }
  try {
    Class<?> hbaseTableUtilClass=new HBaseTableUtilFactory().get().getClass();
    classes.add(hbaseTableUtilClass);
  }
 catch (  ProvisionException e) {
    LOG.warn(""String_Node_Str"");
  }
  ClassLoader oldCLassLoader=Thread.currentThread().getContextClassLoader();
  Thread.currentThread().setContextClassLoader(jobConf.getConfiguration().getClassLoader());
  appBundler.createBundle(jobJar,classes);
  Thread.currentThread().setContextClassLoader(oldCLassLoader);
  LOG.info(""String_Node_Str"",jobJar.toURI());
  return jobJar;
}","/** 
 * Creates a jar that contains everything that are needed for running the MapReduce program by Hadoop.
 * @return a new {@link Location} containing the job jar
 */
private Location buildJobJar(BasicMapReduceContext context) throws IOException {
  ApplicationBundler appBundler=new ApplicationBundler(ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),ImmutableList.of(""String_Node_Str"",""String_Node_Str""));
  Id.Program programId=context.getProgram().getId();
  Location jobJar=locationFactory.create(String.format(""String_Node_Str"",ProgramType.MAPREDUCE.name().toLowerCase(),programId.getAccountId(),programId.getApplicationId(),programId.getId(),context.getRunId().getId()));
  LOG.debug(""String_Node_Str"",jobJar.toURI());
  Set<Class<?>> classes=Sets.newHashSet();
  classes.add(MapReduce.class);
  classes.add(MapperWrapper.class);
  classes.add(ReducerWrapper.class);
  Job jobConf=context.getHadoopJob();
  try {
    Class<? extends InputFormat<?,?>> inputFormatClass=jobConf.getInputFormatClass();
    LOG.info(""String_Node_Str"",inputFormatClass,inputFormatClass.getClassLoader());
    classes.add(inputFormatClass);
    if (StreamInputFormat.class.isAssignableFrom(inputFormatClass)) {
      Class<? extends StreamEventDecoder> decoderType=StreamInputFormatConfigurer.getDecoderClass(jobConf.getConfiguration());
      if (decoderType != null) {
        classes.add(decoderType);
      }
    }
  }
 catch (  Throwable t) {
    LOG.info(""String_Node_Str"",t.getMessage(),t);
  }
  try {
    Class<? extends OutputFormat<?,?>> outputFormatClass=jobConf.getOutputFormatClass();
    LOG.info(""String_Node_Str"",outputFormatClass,outputFormatClass.getClassLoader());
    classes.add(outputFormatClass);
  }
 catch (  Throwable t) {
    LOG.info(""String_Node_Str"",t.getMessage(),t);
  }
  try {
    Class<?> hbaseTableUtilClass=new HBaseTableUtilFactory().get().getClass();
    classes.add(hbaseTableUtilClass);
  }
 catch (  ProvisionException e) {
    LOG.warn(""String_Node_Str"");
  }
  ClassLoader oldCLassLoader=Thread.currentThread().getContextClassLoader();
  Thread.currentThread().setContextClassLoader(jobConf.getConfiguration().getClassLoader());
  appBundler.createBundle(jobJar,classes);
  Thread.currentThread().setContextClassLoader(oldCLassLoader);
  LOG.info(""String_Node_Str"",jobJar.toURI());
  return jobJar;
}"
7434,"@Override public RecordReader<K,V> createRecordReader(InputSplit split,TaskAttemptContext context) throws IOException, InterruptedException {
  return new StreamRecordReader(createStreamEventDecoder(context.getConfiguration()));
}","@Override public RecordReader<K,V> createRecordReader(InputSplit split,TaskAttemptContext context) throws IOException, InterruptedException {
  return new StreamRecordReader<K,V>(createStreamEventDecoder(context.getConfiguration()));
}"
7435,"@Test public void testingMetricsWithRunIds() throws Exception {
  String runId1=""String_Node_Str"";
  String runId2=""String_Node_Str"";
  String runId3=""String_Node_Str"";
  MetricsCollector collector1=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",runId1);
  collector1.increment(""String_Node_Str"",1);
  MetricsCollector collector2=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",runId2);
  collector2.increment(""String_Node_Str"",2);
  MetricsCollector collector3=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",runId3);
  collector3.gauge(""String_Node_Str"",10);
  TimeUnit.SECONDS.sleep(2);
  String serviceRequest=""String_Node_Str"" + runId2 + ""String_Node_Str"";
  String serviceRequestInvalidId=""String_Node_Str"";
  String serviceRequestTotal=""String_Node_Str"";
  String mapreduceMetric=""String_Node_Str"" + runId3 + ""String_Node_Str"";
  testSingleMetric(serviceRequest,2);
  testSingleMetric(serviceRequestInvalidId,0);
  testSingleMetric(serviceRequestTotal,3);
  testSingleMetric(mapreduceMetric,10);
}","@Test public void testingMetricsWithRunIds() throws Exception {
  String runId1=""String_Node_Str"";
  String runId2=""String_Node_Str"";
  String runId3=""String_Node_Str"";
  MetricsCollector collector1=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",runId1);
  collector1.increment(""String_Node_Str"",1);
  MetricsCollector collector2=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",runId2);
  collector2.increment(""String_Node_Str"",2);
  MetricsCollector collector3=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",runId3);
  collector3.gauge(""String_Node_Str"",10);
  MetricsCollector collector4=collectionService.getCollector(MetricsScope.USER,""String_Node_Str"",runId3);
  collector4.gauge(""String_Node_Str"",10);
  TimeUnit.SECONDS.sleep(2);
  String serviceRequest=""String_Node_Str"" + runId2 + ""String_Node_Str"";
  String serviceRequestInvalidId=""String_Node_Str"";
  String serviceRequestTotal=""String_Node_Str"";
  String mappersMetric=""String_Node_Str"" + runId3 + ""String_Node_Str"";
  String reducersMetric=""String_Node_Str"" + runId3 + ""String_Node_Str"";
  String mapredMetric=""String_Node_Str"" + runId3 + ""String_Node_Str"";
  testSingleMetric(serviceRequest,2);
  testSingleMetric(serviceRequestInvalidId,0);
  testSingleMetric(serviceRequestTotal,3);
  testSingleMetric(mappersMetric,10);
  testSingleMetric(reducersMetric,10);
  testSingleMetric(mapredMetric,20);
}"
7436,"private <T>ProcessMethodCallback processMethodCallback(final BlockingQueue<FlowletProcessEntry<?>> processQueue,final FlowletProcessEntry<T> processEntry,final InputDatum<T> input){
  final int processedCount=processEntry.getProcessSpec().getProcessMethod().needsInput() ? input.size() : 1;
  return new ProcessMethodCallback(){
    @Override public void onSuccess(    Object object,    InputContext inputContext){
      try {
        gaugeEventProcessed(input.getQueueName());
        txCallback.onSuccess(object,inputContext);
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",flowletContext,t);
      }
 finally {
        enqueueEntry();
        inflight.decrementAndGet();
      }
    }
    @Override public void onFailure(    Object inputObject,    InputContext inputContext,    FailureReason reason,    InputAcknowledger inputAcknowledger){
      LOG.warn(""String_Node_Str"",flowletContext,reason.getMessage(),input,reason.getCause());
      FailurePolicy failurePolicy;
      try {
        flowletContext.getProgramMetrics().increment(""String_Node_Str"",1);
        failurePolicy=txCallback.onFailure(inputObject,inputContext,reason);
        if (failurePolicy == null) {
          failurePolicy=FailurePolicy.RETRY;
          LOG.info(""String_Node_Str"",failurePolicy);
        }
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",flowletContext,t);
        failurePolicy=FailurePolicy.RETRY;
      }
      if (input.getRetry() >= processEntry.getProcessSpec().getProcessMethod().getMaxRetries()) {
        LOG.info(""String_Node_Str"",input);
        failurePolicy=FailurePolicy.IGNORE;
      }
      if (failurePolicy == FailurePolicy.RETRY) {
        FlowletProcessEntry retryEntry=processEntry.isRetry() ? processEntry : FlowletProcessEntry.create(processEntry.getProcessSpec(),new ProcessSpecification<T>(new SingleItemQueueReader<T>(input),processEntry.getProcessSpec().getProcessMethod(),null));
        processQueue.offer(retryEntry);
      }
 else       if (failurePolicy == FailurePolicy.IGNORE) {
        try {
          gaugeEventProcessed(input.getQueueName());
          inputAcknowledger.ack();
        }
 catch (        Throwable t) {
          LOG.error(""String_Node_Str"",flowletContext,t);
        }
 finally {
          enqueueEntry();
          inflight.decrementAndGet();
        }
      }
    }
    private void enqueueEntry(){
      processQueue.offer(processEntry.resetRetry());
    }
    private void gaugeEventProcessed(    QueueName inputQueueName){
      if (processEntry.isTick()) {
        flowletContext.getProgramMetrics().increment(""String_Node_Str"",processedCount);
      }
 else       if (inputQueueName == null) {
        flowletContext.getProgramMetrics().increment(""String_Node_Str"",processedCount);
      }
 else {
        String tag=""String_Node_Str"" + inputQueueName.toString();
        flowletContext.getProgramMetrics().increment(""String_Node_Str"",processedCount,tag);
      }
    }
  }
;
}","private <T>ProcessMethodCallback processMethodCallback(final PriorityQueue<FlowletProcessEntry<?>> processQueue,final FlowletProcessEntry<T> processEntry,final InputDatum<T> input){
  final int processedCount=processEntry.getProcessSpec().getProcessMethod().needsInput() ? input.size() : 1;
  return new ProcessMethodCallback(){
    @Override public void onSuccess(    Object object,    InputContext inputContext){
      try {
        gaugeEventProcessed(input.getQueueName());
        txCallback.onSuccess(object,inputContext);
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",flowletContext,t);
      }
 finally {
        enqueueEntry();
      }
    }
    @Override public void onFailure(    Object inputObject,    InputContext inputContext,    FailureReason reason,    InputAcknowledger inputAcknowledger){
      LOG.warn(""String_Node_Str"",flowletContext,reason.getMessage(),input,reason.getCause());
      FailurePolicy failurePolicy;
      try {
        flowletContext.getProgramMetrics().increment(""String_Node_Str"",1);
        failurePolicy=txCallback.onFailure(inputObject,inputContext,reason);
        if (failurePolicy == null) {
          failurePolicy=FailurePolicy.RETRY;
          LOG.info(""String_Node_Str"",failurePolicy);
        }
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",flowletContext,t);
        failurePolicy=FailurePolicy.RETRY;
      }
      if (input.getRetry() >= processEntry.getProcessSpec().getProcessMethod().getMaxRetries()) {
        LOG.info(""String_Node_Str"",input);
        failurePolicy=FailurePolicy.IGNORE;
      }
      if (failurePolicy == FailurePolicy.RETRY) {
        FlowletProcessEntry retryEntry=processEntry.isRetry() ? processEntry : FlowletProcessEntry.create(processEntry.getProcessSpec(),new ProcessSpecification<T>(new SingleItemQueueReader<T>(input),processEntry.getProcessSpec().getProcessMethod(),null));
        processQueue.offer(retryEntry);
      }
 else       if (failurePolicy == FailurePolicy.IGNORE) {
        try {
          gaugeEventProcessed(input.getQueueName());
          inputAcknowledger.ack();
        }
 catch (        Throwable t) {
          LOG.error(""String_Node_Str"",flowletContext,t);
        }
 finally {
          enqueueEntry();
        }
      }
    }
    private void enqueueEntry(){
      processQueue.offer(processEntry.resetRetry());
    }
    private void gaugeEventProcessed(    QueueName inputQueueName){
      if (processEntry.isTick()) {
        flowletContext.getProgramMetrics().increment(""String_Node_Str"",processedCount);
      }
 else       if (inputQueueName == null) {
        flowletContext.getProgramMetrics().increment(""String_Node_Str"",processedCount);
      }
 else {
        String tag=""String_Node_Str"" + inputQueueName.toString();
        flowletContext.getProgramMetrics().increment(""String_Node_Str"",processedCount,tag);
      }
    }
  }
;
}"
7437,"@Override public void onFailure(Object inputObject,InputContext inputContext,FailureReason reason,InputAcknowledger inputAcknowledger){
  LOG.warn(""String_Node_Str"",flowletContext,reason.getMessage(),input,reason.getCause());
  FailurePolicy failurePolicy;
  try {
    flowletContext.getProgramMetrics().increment(""String_Node_Str"",1);
    failurePolicy=txCallback.onFailure(inputObject,inputContext,reason);
    if (failurePolicy == null) {
      failurePolicy=FailurePolicy.RETRY;
      LOG.info(""String_Node_Str"",failurePolicy);
    }
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",flowletContext,t);
    failurePolicy=FailurePolicy.RETRY;
  }
  if (input.getRetry() >= processEntry.getProcessSpec().getProcessMethod().getMaxRetries()) {
    LOG.info(""String_Node_Str"",input);
    failurePolicy=FailurePolicy.IGNORE;
  }
  if (failurePolicy == FailurePolicy.RETRY) {
    FlowletProcessEntry retryEntry=processEntry.isRetry() ? processEntry : FlowletProcessEntry.create(processEntry.getProcessSpec(),new ProcessSpecification<T>(new SingleItemQueueReader<T>(input),processEntry.getProcessSpec().getProcessMethod(),null));
    processQueue.offer(retryEntry);
  }
 else   if (failurePolicy == FailurePolicy.IGNORE) {
    try {
      gaugeEventProcessed(input.getQueueName());
      inputAcknowledger.ack();
    }
 catch (    Throwable t) {
      LOG.error(""String_Node_Str"",flowletContext,t);
    }
 finally {
      enqueueEntry();
      inflight.decrementAndGet();
    }
  }
}","@Override public void onFailure(Object inputObject,InputContext inputContext,FailureReason reason,InputAcknowledger inputAcknowledger){
  LOG.warn(""String_Node_Str"",flowletContext,reason.getMessage(),input,reason.getCause());
  FailurePolicy failurePolicy;
  try {
    flowletContext.getProgramMetrics().increment(""String_Node_Str"",1);
    failurePolicy=txCallback.onFailure(inputObject,inputContext,reason);
    if (failurePolicy == null) {
      failurePolicy=FailurePolicy.RETRY;
      LOG.info(""String_Node_Str"",failurePolicy);
    }
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",flowletContext,t);
    failurePolicy=FailurePolicy.RETRY;
  }
  if (input.getRetry() >= processEntry.getProcessSpec().getProcessMethod().getMaxRetries()) {
    LOG.info(""String_Node_Str"",input);
    failurePolicy=FailurePolicy.IGNORE;
  }
  if (failurePolicy == FailurePolicy.RETRY) {
    FlowletProcessEntry retryEntry=processEntry.isRetry() ? processEntry : FlowletProcessEntry.create(processEntry.getProcessSpec(),new ProcessSpecification<T>(new SingleItemQueueReader<T>(input),processEntry.getProcessSpec().getProcessMethod(),null));
    processQueue.offer(retryEntry);
  }
 else   if (failurePolicy == FailurePolicy.IGNORE) {
    try {
      gaugeEventProcessed(input.getQueueName());
      inputAcknowledger.ack();
    }
 catch (    Throwable t) {
      LOG.error(""String_Node_Str"",flowletContext,t);
    }
 finally {
      enqueueEntry();
    }
  }
}"
7438,"@Override protected void triggerShutdown(){
  LOG.info(""String_Node_Str"");
  runnerThread.interrupt();
}","@Override protected void triggerShutdown(){
  runThread.interrupt();
}"
7439,"FlowletProcessDriver(Flowlet flowlet,BasicFlowletContext flowletContext,Collection<ProcessSpecification> processSpecs,Callback txCallback,DataFabricFacade dataFabricFacade,Service serviceHook){
  this.flowlet=flowlet;
  this.flowletContext=flowletContext;
  this.loggingContext=flowletContext.getLoggingContext();
  this.processSpecs=processSpecs;
  this.txCallback=txCallback;
  this.dataFabricFacade=dataFabricFacade;
  this.serviceHook=serviceHook;
  this.inflight=new AtomicInteger(0);
  this.suspension=new AtomicReference<CountDownLatch>();
  this.suspendBarrier=new CyclicBarrier(2);
}","/** 
 * Copy constructor. Main purpose is to copy processQueue state. It's used for Flowlet suspend->resume.
 */
FlowletProcessDriver(FlowletProcessDriver other){
  Preconditions.checkArgument(other.state() == State.TERMINATED,""String_Node_Str"");
  this.flowletContext=other.flowletContext;
  this.dataFabricFacade=other.dataFabricFacade;
  this.txCallback=other.txCallback;
  this.loggingContext=other.loggingContext;
  this.processQueue=new PriorityQueue<FlowletProcessEntry<?>>(other.processQueue.size());
  Iterables.addAll(processQueue,other.processQueue);
}"
7440,"@Override protected void startUp() throws Exception {
  runnerThread=Thread.currentThread();
  flowletContext.getProgramMetrics().increment(""String_Node_Str"",1);
  processExecutor=Executors.newSingleThreadExecutor(Threads.createDaemonThreadFactory(getServiceName() + ""String_Node_Str""));
}","@Override protected void startUp() throws Exception {
  runThread=Thread.currentThread();
  processExecutor=Executors.newSingleThreadExecutor(Threads.createDaemonThreadFactory(getServiceName() + ""String_Node_Str""));
}"
7441,"/** 
 * Creates a   {@link Runnable} for execution of calling flowlet process methods.
 */
private Runnable createProcessRunner(final BlockingQueue<FlowletProcessEntry<?>> processQueue,final List<FlowletProcessEntry<?>> processList,final ClassLoader classLoader){
  return new Runnable(){
    @Override public void run(){
      Thread.currentThread().setContextClassLoader(classLoader);
      for (      FlowletProcessEntry<?> entry : processList) {
        if (!handleProcessEntry(entry,processQueue)) {
          processQueue.offer(entry);
        }
      }
    }
  }
;
}","/** 
 * Creates a   {@link Runnable} for execution of calling flowlet process methods.
 */
private Runnable createProcessRunner(final PriorityQueue<FlowletProcessEntry<?>> processQueue,final List<FlowletProcessEntry<?>> processList,final ClassLoader classLoader){
  return new Runnable(){
    @Override public void run(){
      Thread.currentThread().setContextClassLoader(classLoader);
      for (      FlowletProcessEntry<?> entry : processList) {
        if (!handleProcessEntry(entry,processQueue)) {
          processQueue.offer(entry);
        }
      }
    }
  }
;
}"
7442,"/** 
 * Invokes to perform dequeue and optionally invoke the user process input / tick method if dequeue gave a non empty result.
 * @param entry Contains information about the process method and queue.
 * @param processQueue The queue for queuing up all process input methods in a flowlet instance.
 * @param < T > Type of input of the process method accepted.
 * @return {@code true} if the entry is handled completely (regardless of process result), {@code false} otherwise.
 */
private <T>boolean handleProcessEntry(FlowletProcessEntry<T> entry,BlockingQueue<FlowletProcessEntry<?>> processQueue){
  if (!entry.shouldProcess()) {
    return false;
  }
  ProcessMethod<T> processMethod=entry.getProcessSpec().getProcessMethod();
  if (processMethod.needsInput()) {
    flowletContext.getProgramMetrics().increment(""String_Node_Str"",1);
  }
  TransactionContext txContext=dataFabricFacade.createTransactionManager();
  try {
    txContext.start();
    try {
      InputDatum<T> input=entry.getProcessSpec().getQueueReader().dequeue(0,TimeUnit.MILLISECONDS);
      if (!input.needProcess()) {
        entry.backOff();
        txContext.finish();
        return false;
      }
      entry.resetBackOff();
      if (!entry.isRetry()) {
        inflight.getAndIncrement();
      }
      try {
        ProcessMethod.ProcessResult<?> result=processMethod.invoke(input);
        postProcess(processMethodCallback(processQueue,entry,input),txContext,input,result);
        return true;
      }
 catch (      Throwable t) {
        if (!entry.isRetry()) {
          inflight.decrementAndGet();
        }
      }
    }
 catch (    Throwable t) {
      LOG.error(""String_Node_Str"",flowletContext,t);
      try {
        txContext.abort();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",flowletContext,e);
      }
    }
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",t);
  }
  return false;
}","/** 
 * Invokes to perform dequeue and optionally invoke the user process input / tick method if dequeue gave a non empty result.
 * @param entry Contains information about the process method and queue.
 * @param processQueue The queue for queuing up all process input methods in a flowlet instance.
 * @param < T > Type of input of the process method accepted.
 * @return {@code true} if the entry is handled completely (regardless of process result), {@code false} otherwise.
 */
private <T>boolean handleProcessEntry(FlowletProcessEntry<T> entry,PriorityQueue<FlowletProcessEntry<?>> processQueue){
  if (!entry.shouldProcess()) {
    return false;
  }
  ProcessMethod<T> processMethod=entry.getProcessSpec().getProcessMethod();
  if (processMethod.needsInput()) {
    flowletContext.getProgramMetrics().increment(""String_Node_Str"",1);
  }
  TransactionContext txContext=dataFabricFacade.createTransactionManager();
  try {
    txContext.start();
    try {
      InputDatum<T> input=entry.getProcessSpec().getQueueReader().dequeue(0,TimeUnit.MILLISECONDS);
      if (!input.needProcess()) {
        entry.backOff();
        txContext.finish();
        return false;
      }
      entry.resetBackOff();
      ProcessMethod.ProcessResult<?> result=processMethod.invoke(input);
      postProcess(processMethodCallback(processQueue,entry,input),txContext,input,result);
      return true;
    }
 catch (    Throwable t) {
      LOG.error(""String_Node_Str"",flowletContext,t);
      try {
        txContext.abort();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",flowletContext,e);
      }
    }
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",t);
  }
  return false;
}"
7443,"@Override public void onSuccess(Object object,InputContext inputContext){
  try {
    gaugeEventProcessed(input.getQueueName());
    txCallback.onSuccess(object,inputContext);
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",flowletContext,t);
  }
 finally {
    enqueueEntry();
    inflight.decrementAndGet();
  }
}","@Override public void onSuccess(Object object,InputContext inputContext){
  try {
    gaugeEventProcessed(input.getQueueName());
    txCallback.onSuccess(object,inputContext);
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",flowletContext,t);
  }
 finally {
    enqueueEntry();
  }
}"
7444,"private void listenDriveState(FlowletProcessDriver driver){
  driver.addListener(new ServiceListenerAdapter(){
    @Override public void running(){
      started();
    }
    @Override public void failed(    Service.State from,    Throwable failure){
      LOG.error(""String_Node_Str"",failure);
      error(failure);
    }
    @Override public void terminated(    Service.State from){
      if (getState() != State.STOPPING) {
        LOG.warn(""String_Node_Str"");
        for (        ConsumerSupplier consumerSupplier : consumerSuppliers) {
          Closeables.closeQuietly(consumerSupplier);
        }
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
}","private void listenDriveState(FlowletRuntimeService driver){
  driver.addListener(new ServiceListenerAdapter(){
    @Override public void running(){
      started();
    }
    @Override public void failed(    Service.State from,    Throwable failure){
      LOG.error(""String_Node_Str"",failure);
      error(failure);
    }
    @Override public void terminated(    Service.State from){
      if (getState() != State.STOPPING) {
        LOG.warn(""String_Node_Str"");
        for (        ConsumerSupplier consumerSupplier : consumerSuppliers) {
          Closeables.closeQuietly(consumerSupplier);
        }
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
}"
7445,"/** 
 * Constructs an instance. The instance must be constructed before the flowlet driver starts.
 */
FlowletProgramController(String programName,String flowletName,BasicFlowletContext flowletContext,FlowletProcessDriver driver,Collection<ConsumerSupplier<?>> consumerSuppliers){
  super(programName + ""String_Node_Str"" + flowletName,flowletContext.getRunId());
  this.flowletContext=flowletContext;
  this.driver=driver;
  this.consumerSuppliers=consumerSuppliers;
  listenDriveState(driver);
}","/** 
 * Constructs an instance. The instance must be constructed before the flowlet driver starts.
 */
FlowletProgramController(String programName,String flowletName,BasicFlowletContext flowletContext,FlowletRuntimeService driver,Collection<ConsumerSupplier<?>> consumerSuppliers){
  super(programName + ""String_Node_Str"" + flowletName,flowletContext.getRunId());
  this.flowletContext=flowletContext;
  this.driver=driver;
  this.consumerSuppliers=consumerSuppliers;
  listenDriveState(driver);
}"
7446,"/** 
 * Creates all   {@link ProcessSpecification} for the process methods of the flowlet class.
 * @param flowletType Type of the flowlet class represented by {@link TypeToken}.
 * @param processMethodFactory A {@link ProcessMethodFactory} for creating {@link ProcessMethod}.
 * @param processSpecFactory A {@link ProcessSpecificationFactory} for creating {@link ProcessSpecification}.
 * @param result A {@link Collection} for storing newly created {@link ProcessSpecification}.
 * @return The same {@link Collection} as the {@code result} parameter.
 */
@SuppressWarnings(""String_Node_Str"") private Collection<ProcessSpecification> createProcessSpecification(BasicFlowletContext flowletContext,TypeToken<? extends Flowlet> flowletType,ProcessMethodFactory processMethodFactory,ProcessSpecificationFactory processSpecFactory,Collection<ProcessSpecification> result) throws NoSuchMethodException {
  Set<FlowletMethod> seenMethods=Sets.newHashSet();
  for (  TypeToken<?> type : flowletType.getTypes().classes()) {
    if (type.getRawType().equals(Object.class)) {
      break;
    }
    for (    Method method : type.getRawType().getDeclaredMethods()) {
      if (!seenMethods.add(new FlowletMethod(method,flowletType))) {
        continue;
      }
      ProcessInput processInputAnnotation=method.getAnnotation(ProcessInput.class);
      Tick tickAnnotation=method.getAnnotation(Tick.class);
      if (processInputAnnotation == null && tickAnnotation == null) {
        continue;
      }
      int maxRetries=(tickAnnotation == null) ? processInputAnnotation.maxRetries() : tickAnnotation.maxRetries();
      ProcessMethod processMethod=processMethodFactory.create(method,maxRetries);
      Set<String> inputNames;
      Schema schema;
      TypeToken<?> dataType;
      ConsumerConfig consumerConfig;
      int batchSize=1;
      if (tickAnnotation != null) {
        inputNames=ImmutableSet.of();
        consumerConfig=new ConsumerConfig(0,0,1,DequeueStrategy.FIFO,null);
        schema=Schema.of(Schema.Type.NULL);
        dataType=TypeToken.of(void.class);
      }
 else {
        inputNames=Sets.newHashSet(processInputAnnotation.value());
        if (inputNames.isEmpty()) {
          inputNames.add(FlowletDefinition.ANY_INPUT);
        }
        dataType=flowletType.resolveType(method.getGenericParameterTypes()[0]);
        consumerConfig=getConsumerConfig(flowletContext,method);
        Integer processBatchSize=getBatchSize(method);
        if (processBatchSize != null) {
          if (dataType.getRawType().equals(Iterator.class)) {
            Preconditions.checkArgument(dataType.getType() instanceof ParameterizedType,""String_Node_Str"");
            dataType=flowletType.resolveType(((ParameterizedType)dataType.getType()).getActualTypeArguments()[0]);
          }
          batchSize=processBatchSize;
        }
        try {
          schema=schemaGenerator.generate(dataType.getType());
        }
 catch (        UnsupportedTypeException e) {
          throw Throwables.propagate(e);
        }
      }
      ProcessSpecification processSpec=processSpecFactory.create(inputNames,schema,dataType,processMethod,consumerConfig,batchSize,tickAnnotation);
      if (processSpec != null) {
        result.add(processSpec);
      }
    }
  }
  Preconditions.checkArgument(!result.isEmpty(),""String_Node_Str"" + flowletType);
  return result;
}","/** 
 * Creates all   {@link ProcessSpecification} for the process methods of the flowlet class.
 * @param flowletType Type of the flowlet class represented by {@link TypeToken}.
 * @param processMethodFactory A {@link ProcessMethodFactory} for creating {@link ProcessMethod}.
 * @param processSpecFactory A {@link ProcessSpecificationFactory} for creating {@link ProcessSpecification}.
 * @param result A {@link Collection} for storing newly created {@link ProcessSpecification}.
 * @return The same {@link Collection} as the {@code result} parameter.
 */
@SuppressWarnings(""String_Node_Str"") private <T extends Collection<ProcessSpecification<?>>>T createProcessSpecification(BasicFlowletContext flowletContext,TypeToken<? extends Flowlet> flowletType,ProcessMethodFactory processMethodFactory,ProcessSpecificationFactory processSpecFactory,T result) throws NoSuchMethodException {
  Set<FlowletMethod> seenMethods=Sets.newHashSet();
  for (  TypeToken<?> type : flowletType.getTypes().classes()) {
    if (type.getRawType().equals(Object.class)) {
      break;
    }
    for (    Method method : type.getRawType().getDeclaredMethods()) {
      if (!seenMethods.add(new FlowletMethod(method,flowletType))) {
        continue;
      }
      ProcessInput processInputAnnotation=method.getAnnotation(ProcessInput.class);
      Tick tickAnnotation=method.getAnnotation(Tick.class);
      if (processInputAnnotation == null && tickAnnotation == null) {
        continue;
      }
      int maxRetries=(tickAnnotation == null) ? processInputAnnotation.maxRetries() : tickAnnotation.maxRetries();
      ProcessMethod processMethod=processMethodFactory.create(method,maxRetries);
      Set<String> inputNames;
      Schema schema;
      TypeToken<?> dataType;
      ConsumerConfig consumerConfig;
      int batchSize=1;
      if (tickAnnotation != null) {
        inputNames=ImmutableSet.of();
        consumerConfig=new ConsumerConfig(0,0,1,DequeueStrategy.FIFO,null);
        schema=Schema.of(Schema.Type.NULL);
        dataType=TypeToken.of(void.class);
      }
 else {
        inputNames=Sets.newHashSet(processInputAnnotation.value());
        if (inputNames.isEmpty()) {
          inputNames.add(FlowletDefinition.ANY_INPUT);
        }
        dataType=flowletType.resolveType(method.getGenericParameterTypes()[0]);
        consumerConfig=getConsumerConfig(flowletContext,method);
        Integer processBatchSize=getBatchSize(method);
        if (processBatchSize != null) {
          if (dataType.getRawType().equals(Iterator.class)) {
            Preconditions.checkArgument(dataType.getType() instanceof ParameterizedType,""String_Node_Str"");
            dataType=flowletType.resolveType(((ParameterizedType)dataType.getType()).getActualTypeArguments()[0]);
          }
          batchSize=processBatchSize;
        }
        try {
          schema=schemaGenerator.generate(dataType.getType());
        }
 catch (        UnsupportedTypeException e) {
          throw Throwables.propagate(e);
        }
      }
      ProcessSpecification processSpec=processSpecFactory.create(inputNames,schema,dataType,processMethod,consumerConfig,batchSize,tickAnnotation);
      if (processSpec != null) {
        result.add(processSpec);
      }
    }
  }
  Preconditions.checkArgument(!result.isEmpty(),""String_Node_Str"" + flowletType);
  return result;
}"
7447,"@ProcessInput(maxRetries=0) public void process(String str){
  if (!""String_Node_Str"".equals(confTable.get(new Get(""String_Node_Str"",""String_Node_Str"")).getString(""String_Node_Str""))) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
}","@ProcessInput(maxRetries=0) public void process(String str){
  if (!""String_Node_Str"".equals(confTable.get(new Get(""String_Node_Str"",""String_Node_Str"")).getString(""String_Node_Str""))) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  LOG.info(""String_Node_Str"",str);
}"
7448,"@Override public void initialize(FlowletContext context) throws FlowletException {
  confTable.put(new Put(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
}","@Override public void initialize(FlowletContext context) throws Exception {
  super.initialize(context);
  confTable.put(new Put(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
}"
7449,"@Tick(delay=10,unit=TimeUnit.MINUTES) public void generate(){
  output.emit(""String_Node_Str"");
}","@Tick(delay=10,unit=TimeUnit.MINUTES) public void generate(){
  output.emit(""String_Node_Str"" + getContext().getInstanceId());
}"
7450,"@Override public synchronized ClassLoader getClassLoader(){
  if (classLoader == null) {
    expandIfNeeded();
    try {
      classLoader=ProgramClassLoader.create(expandFolder,parentClassLoader);
    }
 catch (    IOException e) {
      throw Throwables.propagate(e);
    }
  }
  return classLoader;
}","@Override public synchronized ClassLoader getClassLoader(){
  if (classLoader == null) {
    expandIfNeeded();
    try {
      classLoader=ProgramClassLoader.create(expandFolder,parentClassLoader,processorType);
    }
 catch (    IOException e) {
      throw Throwables.propagate(e);
    }
  }
  return classLoader;
}"
7451,"/** 
 * Build the instance of   {@link BasicSparkContext}.
 * @param runId            program run id
 * @param logicalStartTime The logical start time of the job.
 * @param workflowBatch    Tells whether the batch job is started by workflow.
 * @param runtimeArguments the runtime arguments
 * @param tx               transaction to use
 * @param classLoader      classloader to use
 * @param programLocation  program location
 * @return instance of {@link BasicMapReduceContext}
 */
public BasicSparkContext build(String runId,long logicalStartTime,String workflowBatch,Arguments runtimeArguments,Transaction tx,ClassLoader classLoader,URI programLocation){
  Injector injector=prepare();
  LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
  Program program;
  try {
    program=Programs.create(locationFactory.create(programLocation),classLoader);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"" + programLocation);
    throw Throwables.propagate(e);
  }
  DatasetFramework datasetFramework=injector.getInstance(DatasetFramework.class);
  CConfiguration configuration=injector.getInstance(CConfiguration.class);
  ApplicationSpecification appSpec=program.getSpecification();
  MetricsCollectionService metricsCollectionService=null;
  DiscoveryServiceClient discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  SparkSpecification sparkSpec=program.getSpecification().getSpark().get(program.getName());
  BasicSparkContext context=new BasicSparkContext(program,RunIds.fromString(runId),runtimeArguments,appSpec.getDatasets().keySet(),sparkSpec,logicalStartTime,workflowBatch,metricsCollectionService,datasetFramework,configuration,discoveryServiceClient);
  for (  TransactionAware txAware : context.getDatasetInstantiator().getTransactionAware()) {
    txAware.startTx(tx);
  }
  return context;
}","/** 
 * Build the instance of   {@link BasicSparkContext}.
 * @param runId            program run id
 * @param logicalStartTime The logical start time of the job.
 * @param workflowBatch    Tells whether the batch job is started by workflow.
 * @param runtimeArguments the runtime arguments
 * @param tx               transaction to use
 * @param classLoader      classloader to use
 * @param programLocation  program location
 * @return instance of {@link BasicMapReduceContext}
 */
public BasicSparkContext build(String runId,long logicalStartTime,String workflowBatch,Arguments runtimeArguments,Transaction tx,ClassLoader classLoader,URI programLocation){
  Injector injector=prepare();
  LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
  Program program;
  try {
    program=Programs.create(locationFactory.create(programLocation),classLoader);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",programLocation,e);
    throw Throwables.propagate(e);
  }
  DatasetFramework datasetFramework=injector.getInstance(DatasetFramework.class);
  CConfiguration configuration=injector.getInstance(CConfiguration.class);
  ApplicationSpecification appSpec=program.getSpecification();
  MetricsCollectionService metricsCollectionService=null;
  DiscoveryServiceClient discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  SparkSpecification sparkSpec=program.getSpecification().getSpark().get(program.getName());
  BasicSparkContext context=new BasicSparkContext(program,RunIds.fromString(runId),runtimeArguments,appSpec.getDatasets().keySet(),sparkSpec,logicalStartTime,workflowBatch,metricsCollectionService,datasetFramework,configuration,discoveryServiceClient);
  for (  TransactionAware txAware : context.getDatasetInstantiator().getTransactionAware()) {
    txAware.startTx(tx);
  }
  return context;
}"
7452,"/** 
 * Constructor
 * @param args the command line arguments
 * @throws RuntimeException if the user's program class is not found
 */
private SparkProgramWrapper(String[] args){
  arguments=validateArgs(args);
  try {
    userProgramClass=Class.forName(arguments[0],true,Thread.currentThread().getContextClassLoader());
  }
 catch (  ClassNotFoundException cnfe) {
    LOG.warn(""String_Node_Str"",arguments[0],cnfe);
    throw Throwables.propagate(cnfe);
  }
  setSparkContext();
}","/** 
 * Constructor
 * @param args the command line arguments
 * @throws RuntimeException if the user's program class is not found
 */
private SparkProgramWrapper(String[] args){
  arguments=validateArgs(args);
  try {
    userProgramClass=loadUserSparkClass(arguments[0]);
  }
 catch (  ClassNotFoundException e) {
    LOG.error(""String_Node_Str"",arguments[0],e);
    throw Throwables.propagate(e);
  }
  setSparkContext();
}"
7453,"/** 
 * Packages all the dependencies of the Spark job
 * @param context {@link BasicSparkContext} created for this job
 * @param conf    {@link Configuration} prepared for this job by {@link SparkContextConfig}
 * @return {@link Location} of the dependency jar
 * @throws IOException if failed to package the jar through{@link ApplicationBundler#createBundle(Location,Iterable,Iterable)}
 */
private Location buildDependencyJar(BasicSparkContext context,Configuration conf) throws IOException {
  ApplicationBundler appBundler=new ApplicationBundler(Lists.newArrayList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),Lists.newArrayList(""String_Node_Str"",""String_Node_Str""));
  Id.Program programId=context.getProgram().getId();
  Location appFabricDependenciesJarLocation=locationFactory.create(String.format(""String_Node_Str"",ProgramType.SPARK.name().toLowerCase(),programId.getAccountId(),programId.getApplicationId(),programId.getId(),context.getRunId().getId()));
  LOG.debug(""String_Node_Str"",appFabricDependenciesJarLocation.toURI());
  URI hConfLocation=writeHConf(context,conf);
  try {
    Set<Class<?>> classes=Sets.newHashSet();
    Set<URI> resources=Sets.newHashSet();
    classes.add(Spark.class);
    classes.add(SparkDatasetInputFormat.class);
    classes.add(SparkDatasetOutputFormat.class);
    classes.add(SparkProgramWrapper.class);
    classes.add(JavaSparkContext.class);
    classes.add(ScalaSparkContext.class);
    resources.add(hConfLocation);
    try {
      Class<?> hbaseTableUtilClass=new HBaseTableUtilFactory().get().getClass();
      classes.add(hbaseTableUtilClass);
    }
 catch (    ProvisionException e) {
      LOG.warn(""String_Node_Str"");
    }
    ClassLoader oldCLassLoader=Thread.currentThread().getContextClassLoader();
    Thread.currentThread().setContextClassLoader(conf.getClassLoader());
    appBundler.createBundle(appFabricDependenciesJarLocation,classes,resources);
    Thread.currentThread().setContextClassLoader(oldCLassLoader);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
 finally {
    deleteHConfDir(hConfLocation);
  }
  return updateDependencyJar(appFabricDependenciesJarLocation,context);
}","/** 
 * Packages all the dependencies of the Spark job
 * @param context {@link BasicSparkContext} created for this job
 * @param conf    {@link Configuration} prepared for this job by {@link SparkContextConfig}
 * @return {@link Location} of the dependency jar
 * @throws IOException if failed to package the jar through{@link ApplicationBundler#createBundle(Location,Iterable,Iterable)}
 */
private Location buildDependencyJar(BasicSparkContext context,Configuration conf) throws IOException {
  Id.Program programId=context.getProgram().getId();
  Location jobJarLocation=locationFactory.create(String.format(""String_Node_Str"",ProgramType.SPARK.name().toLowerCase(),programId.getAccountId(),programId.getApplicationId(),programId.getId(),context.getRunId().getId()));
  LOG.debug(""String_Node_Str"",jobJarLocation.toURI());
  JarOutputStream jarOut=new JarOutputStream(jobJarLocation.getOutputStream());
  try {
    jarOut.putNextEntry(new JarEntry(SPARK_HCONF_FILENAME));
    conf.writeXml(jarOut);
  }
  finally {
    Closeables.closeQuietly(jarOut);
  }
  return jobJarLocation;
}"
7454,"/** 
 * Copies the user submitted program jar
 * @param jobJarLocation {link Location} of the user's job
 * @param context        {@link BasicSparkContext} context of this job
 * @return {@link Location} where the program jar was copied
 * @throws IOException if failed to get the {@link Location#getInputStream()} or {@link Location#getOutputStream()}
 */
private Location copyProgramJar(Location jobJarLocation,BasicSparkContext context) throws IOException {
  Id.Program programId=context.getProgram().getId();
  Location programJarCopy=locationFactory.create(String.format(""String_Node_Str"",ProgramType.SPARK.name().toLowerCase(),programId.getAccountId(),programId.getApplicationId(),programId.getId(),context.getRunId().getId()));
  ByteStreams.copy(Locations.newInputSupplier(jobJarLocation),Locations.newOutputSupplier(programJarCopy));
  return programJarCopy;
}","/** 
 * Copies the user submitted program jar and flatten it out by expanding all jars in the ""/"" and ""/lib"" to top level.
 * @param jobJarLocation {link Location} of the user's job
 * @param context        {@link BasicSparkContext} context of this job
 * @return {@link Location} where the program jar was copied
 * @throws IOException if failed to get the {@link Location#getInputStream()} or {@link Location#getOutputStream()}
 */
private Location copyProgramJar(Location jobJarLocation,BasicSparkContext context) throws IOException {
  Id.Program programId=context.getProgram().getId();
  Location programJarCopy=locationFactory.create(String.format(""String_Node_Str"",ProgramType.SPARK.name().toLowerCase(),programId.getAccountId(),programId.getApplicationId(),programId.getId(),context.getRunId().getId()));
  Set<String> entries=Sets.newHashSet();
  JarInputStream programInput=new JarInputStream(jobJarLocation.getInputStream());
  try {
    Manifest manifest=getManifest(programInput,context.getProgram());
    JarOutputStream programOutput=new JarOutputStream(programJarCopy.getOutputStream(),manifest);
    try {
      JarEntry entry;
      while ((entry=programInput.getNextJarEntry()) != null) {
        String entryName=entry.getName();
        if (!entries.add(entryName)) {
          continue;
        }
        if (entryName.endsWith(""String_Node_Str"")) {
          int idx=entryName.indexOf('/');
          if (idx < 0 || entryName.indexOf('/',idx + 1) < 0) {
            copyJarContent(new JarInputStream(programInput),programOutput,entries);
            continue;
          }
        }
        programOutput.putNextEntry(entry);
        ByteStreams.copy(programInput,programOutput);
        programOutput.closeEntry();
      }
    }
  finally {
      Closeables.closeQuietly(programOutput);
    }
  }
  finally {
    Closeables.closeQuietly(programInput);
  }
  return programJarCopy;
}"
7455,"@Override protected void startUp() throws Exception {
  sparkHConf=new Configuration(hConf);
  final ClassLoader classLoader=new CombineClassLoader(Objects.firstNonNull(Thread.currentThread().getContextClassLoader(),ClassLoader.getSystemClassLoader()),ImmutableList.of(context.getProgram().getClassLoader()));
  sparkHConf.setClassLoader(classLoader);
  beforeSubmit();
  try {
    Location programJarCopy=copyProgramJar(programJarLocation,context);
    try {
      Transaction tx=txClient.startLong();
      try {
        SparkContextConfig.set(sparkHConf,context,cConf,tx,programJarCopy);
        Location dependencyJar=buildDependencyJar(context,SparkContextConfig.getHConf());
        try {
          sparkSubmitArgs=prepareSparkSubmitArgs(sparkSpecification,sparkHConf,programJarCopy,dependencyJar);
          LOG.info(""String_Node_Str"",context,Arrays.toString(sparkSubmitArgs));
          this.transaction=tx;
          this.cleanupTask=createCleanupTask(dependencyJar,programJarCopy);
        }
 catch (        Throwable t) {
          Locations.deleteQuietly(dependencyJar);
          throw Throwables.propagate(t);
        }
      }
 catch (      Throwable t) {
        Transactions.invalidateQuietly(txClient,tx);
        throw Throwables.propagate(t);
      }
    }
 catch (    Throwable t) {
      Locations.deleteQuietly(programJarCopy);
      throw Throwables.propagate(t);
    }
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",context,t);
    throw Throwables.propagate(t);
  }
}","@Override protected void startUp() throws Exception {
  Configuration sparkHConf=new Configuration(hConf);
  beforeSubmit();
  try {
    Location programJarCopy=copyProgramJar(programJarLocation,context);
    try {
      Transaction tx=txClient.startLong();
      try {
        SparkContextConfig.set(sparkHConf,context,cConf,tx,programJarCopy);
        Location dependencyJar=buildDependencyJar(context,SparkContextConfig.getHConf());
        try {
          sparkSubmitArgs=prepareSparkSubmitArgs(sparkSpecification,sparkHConf,programJarCopy,dependencyJar);
          LOG.info(""String_Node_Str"",context,Arrays.toString(sparkSubmitArgs));
          this.transaction=tx;
          this.cleanupTask=createCleanupTask(dependencyJar,programJarCopy);
        }
 catch (        Throwable t) {
          Locations.deleteQuietly(dependencyJar);
          throw Throwables.propagate(t);
        }
      }
 catch (      Throwable t) {
        Transactions.invalidateQuietly(txClient,tx);
        throw Throwables.propagate(t);
      }
    }
 catch (    Throwable t) {
      Locations.deleteQuietly(programJarCopy);
      throw Throwables.propagate(t);
    }
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",context,t);
    throw Throwables.propagate(t);
  }
}"
7456,"/** 
 * Check if any program that satisfy the given   {@link Predicate} is running.
 * @param predicate Get call on each running {@link Id.Program}.
 * @param types Types of program to checkreturns True if a program is running as defined by the predicate.
 */
private boolean checkAnyRunning(Predicate<Id.Program> predicate,ProgramType... types){
  for (  ProgramType type : types) {
    for (    Map.Entry<RunId,ProgramRuntimeService.RuntimeInfo> entry : runtimeService.list(type).entrySet()) {
      ProgramController.State programState=entry.getValue().getController().getState();
      if (programState != ProgramController.State.STOPPED && programState != ProgramController.State.ERROR) {
        Id.Program programId=entry.getValue().getProgramId();
        if (predicate.apply(programId)) {
          LOG.trace(""String_Node_Str"",programId.getApplicationId(),type,programId.getId(),entry.getValue().getController().getRunId());
          return true;
        }
      }
    }
  }
  return false;
}","/** 
 * Check if any program that satisfy the given   {@link Predicate} is running.
 * @param predicate Get call on each running {@link Id.Program}.
 * @param types Types of program to checkreturns True if a program is running as defined by the predicate.
 */
private boolean checkAnyRunning(Predicate<Id.Program> predicate,ProgramType... types){
  for (  ProgramType type : types) {
    for (    Map.Entry<RunId,ProgramRuntimeService.RuntimeInfo> entry : runtimeService.list(type).entrySet()) {
      ProgramController.State programState=entry.getValue().getController().getState();
      if (programState == ProgramController.State.STOPPED || programState == ProgramController.State.ERROR) {
        continue;
      }
      Id.Program programId=entry.getValue().getProgramId();
      if (predicate.apply(programId)) {
        LOG.trace(""String_Node_Str"",programId.getApplicationId(),type,programId.getId(),entry.getValue().getController().getRunId());
        return true;
      }
    }
  }
  return false;
}"
7457,"private static void parseRunIdFromQueryParam(Map<String,List<String>> queryParams,MetricsRequestBuilder builder) throws MetricsPathException {
  if (queryParams.containsKey(RUN_ID)) {
    if (queryParams.size() > 1) {
      throw new MetricsPathException(""String_Node_Str"");
    }
    builder.setRunId(queryParams.get(RUN_ID).get(0));
  }
}","private static void parseRunIdFromQueryParam(Map<String,List<String>> queryParams,MetricsRequestBuilder builder) throws MetricsPathException {
  if (queryParams.containsKey(RUN_ID)) {
    if (queryParams.get(RUN_ID).size() > 1) {
      throw new MetricsPathException(""String_Node_Str"");
    }
    builder.setRunId(queryParams.get(RUN_ID).get(0));
  }
}"
7458,"/** 
 * Waits for an application to be deleted.
 * @param appId ID of the application to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the application was not yet deleted before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForDeleted(final String appId,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(false,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(appId);
      }
    }
,timeout,timeoutUnit.toSeconds(1),timeoutUnit);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}","/** 
 * Waits for an application to be deleted.
 * @param appId ID of the application to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the application was not yet deleted before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForDeleted(final String appId,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(false,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(appId);
      }
    }
,timeout,timeoutUnit,1,TimeUnit.SECONDS);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}"
7459,"/** 
 * Waits for an application to be deployed.
 * @param appId ID of the application to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the application was not yet deployed before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForDeployed(final String appId,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(true,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(appId);
      }
    }
,timeout,timeoutUnit.toSeconds(1),timeoutUnit);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}","/** 
 * Waits for an application to be deployed.
 * @param appId ID of the application to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the application was not yet deployed before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForDeployed(final String appId,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(true,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(appId);
      }
    }
,timeout,timeoutUnit,1,TimeUnit.SECONDS);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}"
7460,"/** 
 * Waits for a dataset to be deleted.
 * @param datasetName Name of the dataset to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the dataset was not yet deleted before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForDeleted(final String datasetName,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(false,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(datasetName);
      }
    }
,timeout,timeoutUnit.toSeconds(1),timeoutUnit);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}","/** 
 * Waits for a dataset to be deleted.
 * @param datasetName Name of the dataset to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the dataset was not yet deleted before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForDeleted(final String datasetName,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(false,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(datasetName);
      }
    }
,timeout,timeoutUnit,1,TimeUnit.SECONDS);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}"
7461,"/** 
 * Waits for a dataset to exist.
 * @param datasetName Name of the dataset to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the dataset was not yet existent before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForExists(final String datasetName,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(true,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(datasetName);
      }
    }
,timeout,timeoutUnit.toSeconds(1),timeoutUnit);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}","/** 
 * Waits for a dataset to exist.
 * @param datasetName Name of the dataset to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the dataset was not yet existent before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForExists(final String datasetName,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(true,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(datasetName);
      }
    }
,timeout,timeoutUnit,1,TimeUnit.SECONDS);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}"
7462,"/** 
 * Waits for a dataset module to be deleted.
 * @param moduleName Name of the dataset module to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the dataset module was not yet deleted before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForDeleted(final String moduleName,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(false,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(moduleName);
      }
    }
,timeout,timeoutUnit.toSeconds(1),timeoutUnit);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}","/** 
 * Waits for a dataset module to be deleted.
 * @param moduleName Name of the dataset module to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the dataset module was not yet deleted before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForDeleted(final String moduleName,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(false,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(moduleName);
      }
    }
,timeout,timeoutUnit,1,TimeUnit.SECONDS);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}"
7463,"/** 
 * Waits for a dataset module to exist.
 * @param moduleName Name of the dataset module to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the dataset module was not yet existent before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForExists(final String moduleName,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(true,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(moduleName);
      }
    }
,timeout,timeoutUnit.toSeconds(1),timeoutUnit);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}","/** 
 * Waits for a dataset module to exist.
 * @param moduleName Name of the dataset module to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the dataset module was not yet existent before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForExists(final String moduleName,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(true,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(moduleName);
      }
    }
,timeout,timeoutUnit,1,TimeUnit.SECONDS);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}"
7464,"/** 
 * Waits for a dataset type to be deleted.
 * @param moduleName Name of the dataset type to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the dataset type was not yet deleted before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForDeleted(final String moduleName,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(false,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(moduleName);
      }
    }
,timeout,timeoutUnit.toSeconds(1),timeoutUnit);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}","/** 
 * Waits for a dataset type to be deleted.
 * @param moduleName Name of the dataset type to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the dataset type was not yet deleted before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForDeleted(final String moduleName,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(false,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(moduleName);
      }
    }
,timeout,timeoutUnit,1,TimeUnit.SECONDS);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}"
7465,"/** 
 * Waits for a dataset type to exist.
 * @param typeName Name of the dataset type to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the dataset type was not yet existent before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForExists(final String typeName,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(true,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(typeName);
      }
    }
,timeout,timeoutUnit.toSeconds(1),timeoutUnit);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}","/** 
 * Waits for a dataset type to exist.
 * @param typeName Name of the dataset type to check
 * @param timeout time to wait before timing out
 * @param timeoutUnit time unit of timeout
 * @throws IOException if a network error occurred
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the dataset type was not yet existent before {@code timeout} milliseconds
 * @throws InterruptedException if interrupted while waiting
 */
public void waitForExists(final String typeName,long timeout,TimeUnit timeoutUnit) throws IOException, UnAuthorizedAccessTokenException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(true,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        return exists(typeName);
      }
    }
,timeout,timeoutUnit,1,TimeUnit.SECONDS);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),IOException.class,UnAuthorizedAccessTokenException.class);
  }
}"
7466,"/** 
 * Waits for a program to have a certain status.
 * @param appId ID of the application that the program belongs to
 * @param programType type of the program
 * @param programId name of the program
 * @param status the desired status
 * @param timeout how long to wait in milliseconds until timing out
 * @throws IOException if a network error occurred
 * @throws ProgramNotFoundException if the program with the specified name could not be found
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the program did not achieve the desired program status before the timeout
 * @throws InterruptedException if interrupted while waiting for the desired program status
 */
public void waitForStatus(final String appId,final ProgramType programType,final String programId,String status,long timeout,TimeUnit timeoutUnit) throws UnAuthorizedAccessTokenException, IOException, ProgramNotFoundException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(status,new Callable<String>(){
      @Override public String call() throws Exception {
        return getStatus(appId,programType,programId);
      }
    }
,timeout,timeoutUnit.toSeconds(1),timeoutUnit);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),UnAuthorizedAccessTokenException.class);
    Throwables.propagateIfPossible(e.getCause(),ProgramNotFoundException.class);
    Throwables.propagateIfPossible(e.getCause(),IOException.class);
  }
}","/** 
 * Waits for a program to have a certain status.
 * @param appId ID of the application that the program belongs to
 * @param programType type of the program
 * @param programId name of the program
 * @param status the desired status
 * @param timeout how long to wait in milliseconds until timing out
 * @throws IOException if a network error occurred
 * @throws ProgramNotFoundException if the program with the specified name could not be found
 * @throws UnAuthorizedAccessTokenException if the request is not authorized successfully in the gateway server
 * @throws TimeoutException if the program did not achieve the desired program status before the timeout
 * @throws InterruptedException if interrupted while waiting for the desired program status
 */
public void waitForStatus(final String appId,final ProgramType programType,final String programId,String status,long timeout,TimeUnit timeoutUnit) throws UnAuthorizedAccessTokenException, IOException, ProgramNotFoundException, TimeoutException, InterruptedException {
  try {
    ProgramFlowUtil.waitFor(status,new Callable<String>(){
      @Override public String call() throws Exception {
        return getStatus(appId,programType,programId);
      }
    }
,timeout,timeoutUnit,1,TimeUnit.SECONDS);
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),UnAuthorizedAccessTokenException.class);
    Throwables.propagateIfPossible(e.getCause(),ProgramNotFoundException.class);
    Throwables.propagateIfPossible(e.getCause(),IOException.class);
  }
}"
7467,"/** 
 * Calls callable, waiting sleepDelay between each call, until it returns the desiredValue or the timeout has passed.
 * @param desiredValue the desired value to get from callable
 * @param callable the callable to check
 * @param timeout time until we timeout
 * @param sleepDelay time to wait between calls to callable
 * @param timeUnit unit of time for timeout and sleepDelay
 * @param < T > type of desiredValue
 * @throws TimeoutException if timeout has passed, but didn't get the desiredValue
 * @throws InterruptedException if something interrupted this waiting operation
 * @throws ExecutionException if there was an exception in calling the callable
 */
public static <T>void waitFor(T desiredValue,Callable<T> callable,long timeout,long sleepDelay,TimeUnit timeUnit) throws TimeoutException, InterruptedException, ExecutionException {
  long startTime=System.currentTimeMillis();
  long timeoutMs=timeUnit.toMillis(timeout);
  while (System.currentTimeMillis() - startTime < timeoutMs) {
    try {
      if (desiredValue.equals(callable.call())) {
        return;
      }
    }
 catch (    Exception e) {
      throw new ExecutionException(e);
    }
    Thread.sleep(sleepDelay);
  }
  throw new TimeoutException();
}","/** 
 * Calls callable, waiting sleepDelay between each call, until it returns the desiredValue or the timeout has passed.
 * @param desiredValue the desired value to get from callable
 * @param callable the callable to check
 * @param timeout time until we timeout
 * @param timeoutUnit unit of time for timeout
 * @param sleepDelay time to wait between calls to callable
 * @param sleepDelayUnit unit of time for sleepDelay
 * @param < T > type of desiredValue
 * @throws TimeoutException if timeout has passed, but didn't get the desiredValue
 * @throws InterruptedException if something interrupted this waiting operation
 * @throws ExecutionException if there was an exception in calling the callable
 */
public static <T>void waitFor(T desiredValue,Callable<T> callable,long timeout,TimeUnit timeoutUnit,long sleepDelay,TimeUnit sleepDelayUnit) throws TimeoutException, InterruptedException, ExecutionException {
  long sleepDelayMs=sleepDelayUnit.toMillis(sleepDelay);
  long startTime=System.currentTimeMillis();
  long timeoutMs=timeoutUnit.toMillis(timeout);
  while (System.currentTimeMillis() - startTime < timeoutMs) {
    try {
      if (desiredValue.equals(callable.call())) {
        return;
      }
    }
 catch (    Exception e) {
      throw new ExecutionException(e);
    }
    Thread.sleep(sleepDelayMs);
  }
  throw new TimeoutException();
}"
7468,"/** 
 * Check if any program that satisfy the given   {@link Predicate} is running.
 * @param predicate Get call on each running {@link Id.Program}.
 * @param types Types of program to checkreturns True if a program is running as defined by the predicate.
 */
private boolean checkAnyRunning(Predicate<Id.Program> predicate,ProgramType... types){
  for (  ProgramType type : types) {
    for (    Map.Entry<RunId,ProgramRuntimeService.RuntimeInfo> entry : runtimeService.list(type).entrySet()) {
      ProgramController.State programState=entry.getValue().getController().getState();
      if (programState != ProgramController.State.STOPPED && programState != ProgramController.State.ERROR) {
        Id.Program programId=entry.getValue().getProgramId();
        if (predicate.apply(programId)) {
          LOG.trace(""String_Node_Str"",programId.getApplicationId(),type,programId.getId(),entry.getValue().getController().getRunId());
          return true;
        }
      }
    }
  }
  return false;
}","/** 
 * Check if any program that satisfy the given   {@link Predicate} is running.
 * @param predicate Get call on each running {@link Id.Program}.
 * @param types Types of program to checkreturns True if a program is running as defined by the predicate.
 */
private boolean checkAnyRunning(Predicate<Id.Program> predicate,ProgramType... types){
  for (  ProgramType type : types) {
    for (    Map.Entry<RunId,ProgramRuntimeService.RuntimeInfo> entry : runtimeService.list(type).entrySet()) {
      ProgramController.State programState=entry.getValue().getController().getState();
      if (programState == ProgramController.State.STOPPED || programState == ProgramController.State.ERROR) {
        continue;
      }
      Id.Program programId=entry.getValue().getProgramId();
      if (predicate.apply(programId)) {
        LOG.trace(""String_Node_Str"",programId.getApplicationId(),type,programId.getId(),entry.getValue().getController().getRunId());
        return true;
      }
    }
  }
  return false;
}"
7469,"public static void main(String[] args){
  Configuration config=HBaseConfiguration.create();
  TransactionManagerDebuggerMain instance=new TransactionManagerDebuggerMain(config);
  boolean success=instance.execute(args,config);
  if (!success) {
    System.exit(1);
  }
}","public static void main(String[] args){
  CConfiguration cConf=CConfiguration.create();
  Configuration hConf=new Configuration();
  cConf.copyTxProperties(hConf);
  TransactionManagerDebuggerMain instance=new TransactionManagerDebuggerMain(hConf);
  boolean success=instance.execute(args);
  if (!success) {
    System.exit(1);
  }
}"
7470,"private boolean execute(String[] args,Configuration conf){
  if (args.length <= 0) {
    printUsage(true);
    return false;
  }
  mode=DebuggerMode.fromString(args[0]);
  if (mode == DebuggerMode.INVALID) {
    printUsage(true);
    return false;
  }
  List<String> subArgs=Arrays.asList(args).subList(1,args.length);
  return parseArgsAndExecMode(subArgs.toArray(new String[0]),conf);
}","private boolean execute(String[] args){
  if (args.length <= 0) {
    printUsage(true);
    return false;
  }
  mode=DebuggerMode.fromString(args[0]);
  if (mode == DebuggerMode.INVALID) {
    printUsage(true);
    return false;
  }
  List<String> subArgs=Arrays.asList(args).subList(1,args.length);
  return parseArgsAndExecMode(subArgs.toArray(new String[0]),hConf);
}"
7471,"private TransactionManagerDebuggerMain(Configuration configuration){
  codecProvider=new SnapshotCodecProvider(configuration);
  buildOptions();
}","private TransactionManagerDebuggerMain(Configuration configuration){
  codecProvider=new SnapshotCodecProvider(configuration);
  buildOptions();
  this.hConf=configuration;
}"
7472,"/** 
 * Increments (atomically) the specified row and columns by the specified amounts. NOTE: depending on the implementation this may work faster than calling   {@link #incrementAndGet(byte[],byte[],long)}multiple times (esp. in transaction that changes a lot of rows)
 * @param row row which values to increment
 * @param columns columns to increment
 * @param amounts amounts to increment columns by (same order as columns)
 * @return values of counters after the increments are performed, never null
 */
Map<byte[],Long> incrementAndGet(byte[] row,byte[][] columns,long[] amounts) throws Exception ;","/** 
 * Increments (atomically) the specified row and columns by the specified amounts. NOTE: depending on the implementation this may work faster than calling  {@link #incrementAndGet(byte[],byte[],long)} multiple times (esp. in transaction that changes a lot of rows).
 * @param row row which values to increment
 * @param columns columns to increment
 * @param amounts amounts to increment columns by (same order as columns)
 * @return values of counters after the increments are performed, never null
 */
Map<byte[],Long> incrementAndGet(byte[] row,byte[][] columns,long[] amounts) throws Exception ;"
7473,"@Override public List<QueryResult> previewResults(QueryHandle handle) throws ExploreException, HandleNotFoundException, SQLException {
  if (inactiveHandleCache.getIfPresent(handle) != null) {
    throw new HandleNotFoundException(""String_Node_Str"",true);
  }
  OperationInfo operationInfo=getOperationInfo(handle);
  File previewFile=operationInfo.getPreviewFile();
  if (previewFile != null) {
    try {
      Reader reader=new FileReader(previewFile);
      try {
        return GSON.fromJson(reader,new TypeToken<List<QueryResult>>(){
        }
.getType());
      }
  finally {
        Closeables.closeQuietly(reader);
      }
    }
 catch (    FileNotFoundException e) {
      LOG.error(""String_Node_Str"",previewFile,e);
      throw new ExploreException(e);
    }
  }
  FileWriter fileWriter=null;
  try {
    previewFile=new File(previewsDir,handle.getHandle());
    fileWriter=new FileWriter(previewFile);
    List<QueryResult> results=nextResults(handle,PREVIEW_COUNT);
    GSON.toJson(results,fileWriter);
    operationInfo.setPreviewFile(previewFile);
    return results;
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",e);
    throw new ExploreException(e);
  }
 finally {
    if (fileWriter != null) {
      Closeables.closeQuietly(fileWriter);
    }
  }
}","@Override public List<QueryResult> previewResults(QueryHandle handle) throws ExploreException, HandleNotFoundException, SQLException {
  if (inactiveHandleCache.getIfPresent(handle) != null) {
    throw new HandleNotFoundException(""String_Node_Str"",true);
  }
  OperationInfo operationInfo=getOperationInfo(handle);
  operationInfo.getPreviewLock().lock();
  try {
    File previewFile=operationInfo.getPreviewFile();
    if (previewFile != null) {
      try {
        Reader reader=new FileReader(previewFile);
        try {
          return GSON.fromJson(reader,new TypeToken<List<QueryResult>>(){
          }
.getType());
        }
  finally {
          Closeables.closeQuietly(reader);
        }
      }
 catch (      FileNotFoundException e) {
        LOG.error(""String_Node_Str"",previewFile,e);
        throw new ExploreException(e);
      }
    }
    FileWriter fileWriter=null;
    try {
      previewFile=new File(previewsDir,handle.getHandle());
      fileWriter=new FileWriter(previewFile);
      List<QueryResult> results=nextResults(handle,PREVIEW_COUNT);
      GSON.toJson(results,fileWriter);
      operationInfo.setPreviewFile(previewFile);
      return results;
    }
 catch (    IOException e) {
      LOG.error(""String_Node_Str"",e);
      throw new ExploreException(e);
    }
 finally {
      if (fileWriter != null) {
        Closeables.closeQuietly(fileWriter);
      }
    }
  }
  finally {
    operationInfo.getPreviewLock().unlock();
  }
}"
7474,"@Override public List<QueryResult> nextResults(QueryHandle handle,int size) throws ExploreException, HandleNotFoundException, SQLException {
  InactiveOperationInfo inactiveOperationInfo=inactiveHandleCache.getIfPresent(handle);
  if (inactiveOperationInfo != null) {
    LOG.trace(""String_Node_Str"",handle);
    return ImmutableList.of();
  }
  try {
    LOG.trace(""String_Node_Str"",handle);
    List<QueryResult> results=fetchNextResults(getOperationHandle(handle),size);
    QueryStatus status=getStatus(handle);
    if (results.isEmpty() && status.getStatus() == QueryStatus.OpStatus.FINISHED) {
      timeoutAggresively(handle,getResultSchema(handle),status);
    }
    return results;
  }
 catch (  HiveSQLException e) {
    throw getSqlException(e);
  }
}","@Override public List<QueryResult> nextResults(QueryHandle handle,int size) throws ExploreException, HandleNotFoundException, SQLException {
  InactiveOperationInfo inactiveOperationInfo=inactiveHandleCache.getIfPresent(handle);
  if (inactiveOperationInfo != null) {
    LOG.trace(""String_Node_Str"",handle);
    return ImmutableList.of();
  }
  try {
    getOperationInfo(handle).getNextLock().lock();
    try {
      LOG.trace(""String_Node_Str"",handle);
      List<QueryResult> results=fetchNextResults(getOperationHandle(handle),size);
      QueryStatus status=getStatus(handle);
      if (results.isEmpty() && status.getStatus() == QueryStatus.OpStatus.FINISHED) {
        timeoutAggresively(handle,getResultSchema(handle),status);
      }
      return results;
    }
  finally {
      getOperationInfo(handle).getNextLock().unlock();
    }
  }
 catch (  HiveSQLException e) {
    throw getSqlException(e);
  }
}"
7475,"private void submit(final Spark job,SparkSpecification sparkSpec,Location programJarLocation,final BasicSparkContext context) throws Exception {
  final Configuration conf=new Configuration(hConf);
  final ClassLoader classLoader=new CombineClassLoader(Objects.firstNonNull(Thread.currentThread().getContextClassLoader(),ClassLoader.getSystemClassLoader()),ImmutableList.of(context.getProgram().getClassLoader()));
  conf.setClassLoader(classLoader);
  beforeSubmit(job,context);
  final Location jobJarCopy=copyProgramJar(programJarLocation,context);
  LOG.info(""String_Node_Str"",jobJarCopy.toURI().getPath(),programJarLocation.toURI().toString());
  final Transaction tx=txSystemClient.startLong();
  SparkContextConfig.set(conf,context,cConf,tx,jobJarCopy);
  final Location dependencyJar=buildDependencyJar(context,SparkContextConfig.getHConf());
  LOG.info(""String_Node_Str"",dependencyJar.toURI().getPath());
  final String[] sparkSubmitArgs=prepareSparkSubmitArgs(sparkSpec,conf,jobJarCopy,dependencyJar);
  new Thread(){
    @Override public void run(){
      boolean success=false;
      try {
        LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
        LOG.info(""String_Node_Str"",context.toString());
        ClassLoader oldClassLoader=Thread.currentThread().getContextClassLoader();
        Thread.currentThread().setContextClassLoader(conf.getClassLoader());
        try {
          SparkProgramWrapper.setSparkProgramRunning(true);
          SparkSubmit.main(sparkSubmitArgs);
        }
 catch (        Exception e) {
          LOG.error(""String_Node_Str"",context.toString(),e);
        }
 finally {
          success=SparkProgramWrapper.isSparkProgramSuccessful();
          SparkProgramWrapper.setSparkProgramRunning(false);
          Thread.currentThread().setContextClassLoader(oldClassLoader);
        }
      }
 catch (      Exception e) {
        LOG.warn(""String_Node_Str"",e);
        throw Throwables.propagate(e);
      }
 finally {
        stopController(success,context,job,tx);
        try {
          dependencyJar.delete();
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"",dependencyJar.toURI());
        }
        try {
          jobJarCopy.delete();
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"",jobJarCopy.toURI());
        }
      }
    }
  }
.start();
}","private void submit(final Spark job,SparkSpecification sparkSpec,Location programJarLocation,final BasicSparkContext context) throws Exception {
  final Configuration conf=new Configuration(hConf);
  final ClassLoader classLoader=new CombineClassLoader(Objects.firstNonNull(Thread.currentThread().getContextClassLoader(),ClassLoader.getSystemClassLoader()),ImmutableList.of(context.getProgram().getClassLoader()));
  conf.setClassLoader(classLoader);
  beforeSubmit(job,context);
  final Location jobJarCopy=copyProgramJar(programJarLocation,context);
  LOG.info(""String_Node_Str"",jobJarCopy.toURI().getPath(),programJarLocation.toURI().toString());
  final Transaction tx=txSystemClient.startLong();
  SparkContextConfig.set(conf,context,cConf,tx,jobJarCopy);
  final Location dependencyJar=buildDependencyJar(context,SparkContextConfig.getHConf());
  LOG.info(""String_Node_Str"",dependencyJar.toURI().getPath());
  final String[] sparkSubmitArgs=prepareSparkSubmitArgs(sparkSpec,conf,jobJarCopy,dependencyJar);
  new Thread(){
    @Override public void run(){
      boolean success=false;
      try {
        LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
        LOG.info(""String_Node_Str"",context.toString(),Arrays.toString(sparkSubmitArgs));
        ClassLoader oldClassLoader=Thread.currentThread().getContextClassLoader();
        Thread.currentThread().setContextClassLoader(conf.getClassLoader());
        try {
          SparkProgramWrapper.setSparkProgramRunning(true);
          SparkSubmit.main(sparkSubmitArgs);
        }
 catch (        Exception e) {
          LOG.error(""String_Node_Str"",context.toString(),e);
        }
 finally {
          success=SparkProgramWrapper.isSparkProgramSuccessful();
          SparkProgramWrapper.setSparkProgramRunning(false);
          Thread.currentThread().setContextClassLoader(oldClassLoader);
        }
      }
 catch (      Exception e) {
        LOG.warn(""String_Node_Str"",e);
        throw Throwables.propagate(e);
      }
 finally {
        stopController(success,context,job,tx);
        try {
          dependencyJar.delete();
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"",dependencyJar.toURI());
        }
        try {
          jobJarCopy.delete();
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"",jobJarCopy.toURI());
        }
      }
    }
  }
.start();
}"
7476,"@Override public void run(){
  boolean success=false;
  try {
    LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
    LOG.info(""String_Node_Str"",context.toString());
    ClassLoader oldClassLoader=Thread.currentThread().getContextClassLoader();
    Thread.currentThread().setContextClassLoader(conf.getClassLoader());
    try {
      SparkProgramWrapper.setSparkProgramRunning(true);
      SparkSubmit.main(sparkSubmitArgs);
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",context.toString(),e);
    }
 finally {
      success=SparkProgramWrapper.isSparkProgramSuccessful();
      SparkProgramWrapper.setSparkProgramRunning(false);
      Thread.currentThread().setContextClassLoader(oldClassLoader);
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
 finally {
    stopController(success,context,job,tx);
    try {
      dependencyJar.delete();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",dependencyJar.toURI());
    }
    try {
      jobJarCopy.delete();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",jobJarCopy.toURI());
    }
  }
}","@Override public void run(){
  boolean success=false;
  try {
    LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
    LOG.info(""String_Node_Str"",context.toString(),Arrays.toString(sparkSubmitArgs));
    ClassLoader oldClassLoader=Thread.currentThread().getContextClassLoader();
    Thread.currentThread().setContextClassLoader(conf.getClassLoader());
    try {
      SparkProgramWrapper.setSparkProgramRunning(true);
      SparkSubmit.main(sparkSubmitArgs);
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",context.toString(),e);
    }
 finally {
      success=SparkProgramWrapper.isSparkProgramSuccessful();
      SparkProgramWrapper.setSparkProgramRunning(false);
      Thread.currentThread().setContextClassLoader(oldClassLoader);
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
 finally {
    stopController(success,context,job,tx);
    try {
      dependencyJar.delete();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",dependencyJar.toURI());
    }
    try {
      jobJarCopy.delete();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",jobJarCopy.toURI());
    }
  }
}"
7477,"/** 
 * Prepares arguments which   {@link SparkProgramWrapper} is submitted to {@link SparkSubmit} to run.
 * @param sparkSpec     {@link SparkSpecification} of this job
 * @param conf          {@link Configuration} of the job whose {@link MRConfig#FRAMEWORK_NAME} specifies the mode inwhich spark runs
 * @param jobJarCopy    {@link Location} copy of user program
 * @param dependencyJar {@link Location} jar containing the dependencies of this job
 * @return String[] of arguments with which {@link SparkProgramWrapper} will be submitted
 */
private String[] prepareSparkSubmitArgs(SparkSpecification sparkSpec,Configuration conf,Location jobJarCopy,Location dependencyJar){
  return new String[]{""String_Node_Str"",SparkProgramWrapper.class.getCanonicalName(),""String_Node_Str"",conf.get(MRConfig.FRAMEWORK_NAME),jobJarCopy.toURI().getPath(),""String_Node_Str"",dependencyJar.toURI().getPath(),sparkSpec.getMainClassName()};
}","/** 
 * Prepares arguments which   {@link SparkProgramWrapper} is submitted to {@link SparkSubmit} to run.
 * @param sparkSpec     {@link SparkSpecification} of this job
 * @param conf          {@link Configuration} of the job whose {@link MRConfig#FRAMEWORK_NAME} specifies the mode inwhich spark runs
 * @param jobJarCopy    {@link Location} copy of user program
 * @param dependencyJar {@link Location} jar containing the dependencies of this job
 * @return String[] of arguments with which {@link SparkProgramWrapper} will be submitted
 */
private String[] prepareSparkSubmitArgs(SparkSpecification sparkSpec,Configuration conf,Location jobJarCopy,Location dependencyJar){
  return new String[]{""String_Node_Str"",SparkProgramWrapper.class.getCanonicalName(),""String_Node_Str"",dependencyJar.toURI().getPath(),""String_Node_Str"",conf.get(MRConfig.FRAMEWORK_NAME),jobJarCopy.toURI().getPath(),sparkSpec.getMainClassName()};
}"
7478,"@Override public void configure(){
  bindConstant().annotatedWith(Names.named(""String_Node_Str"")).to(webAppPath);
}","@Override protected void configure(){
  bind(PassportClient.class).toProvider(new Provider<PassportClient>(){
    @Override public PassportClient get(){
      return client;
    }
  }
);
  if (webAppPath != null) {
    bindConstant().annotatedWith(Names.named(""String_Node_Str"")).to(webAppPath);
  }
}"
7479,"private static List<Module> createPersistentModules(CConfiguration configuration,Configuration hConf,final String webAppPath){
  configuration.setIfUnset(Constants.CFG_DATA_LEVELDB_DIR,Constants.DEFAULT_DATA_LEVELDB_DIR);
  String environment=configuration.get(Constants.CFG_APPFABRIC_ENVIRONMENT,Constants.DEFAULT_APPFABRIC_ENVIRONMENT);
  if (environment.equals(""String_Node_Str"")) {
    System.err.println(""String_Node_Str"" + environment);
  }
  configuration.set(Constants.CFG_DATA_INMEMORY_PERSISTENCE,Constants.InMemoryPersistenceType.LEVELDB.name());
  String passportUri=configuration.get(Constants.Gateway.CFG_PASSPORT_SERVER_URI);
  final PassportClient client=passportUri == null || passportUri.isEmpty() ? new PassportClient() : PassportClient.create(passportUri);
  return ImmutableList.of(new AbstractModule(){
    @Override protected void configure(){
      bind(PassportClient.class).toProvider(new Provider<PassportClient>(){
        @Override public PassportClient get(){
          return client;
        }
      }
);
    }
  }
,new ConfigModule(configuration,hConf),new IOModule(),new MetricsHandlerModule(),new AuthModule(),new DiscoveryRuntimeModule().getStandaloneModules(),new LocationRuntimeModule().getStandaloneModules(),new AppFabricServiceRuntimeModule().getStandaloneModules(),new ProgramRunnerRuntimeModule().getStandaloneModules(),new GatewayModule().getStandaloneModules(),new DataFabricModules().getStandaloneModules(),new DataSetsModules().getLocalModule(),new DataSetServiceModules().getLocalModule(),new MetricsClientRuntimeModule().getStandaloneModules(),new LoggingModules().getStandaloneModules(),new RouterModules().getStandaloneModules(),new SecurityModules().getStandaloneModules(),new StreamServiceRuntimeModule().getStandaloneModules(),new ExploreRuntimeModule().getStandaloneModules(),new ServiceStoreModules().getStandaloneModule(),new ExploreClientModule(),new AbstractModule(){
    @Override public void configure(){
      bindConstant().annotatedWith(Names.named(""String_Node_Str"")).to(webAppPath);
    }
  }
);
}","private static List<Module> createPersistentModules(CConfiguration configuration,Configuration hConf,final String webAppPath){
  configuration.setIfUnset(Constants.CFG_DATA_LEVELDB_DIR,Constants.DEFAULT_DATA_LEVELDB_DIR);
  String environment=configuration.get(Constants.CFG_APPFABRIC_ENVIRONMENT,Constants.DEFAULT_APPFABRIC_ENVIRONMENT);
  if (environment.equals(""String_Node_Str"")) {
    System.err.println(""String_Node_Str"" + environment);
  }
  configuration.set(Constants.CFG_DATA_INMEMORY_PERSISTENCE,Constants.InMemoryPersistenceType.LEVELDB.name());
  String passportUri=configuration.get(Constants.Gateway.CFG_PASSPORT_SERVER_URI);
  final PassportClient client=passportUri == null || passportUri.isEmpty() ? new PassportClient() : PassportClient.create(passportUri);
  return ImmutableList.of(new AbstractModule(){
    @Override protected void configure(){
      bind(PassportClient.class).toProvider(new Provider<PassportClient>(){
        @Override public PassportClient get(){
          return client;
        }
      }
);
      if (webAppPath != null) {
        bindConstant().annotatedWith(Names.named(""String_Node_Str"")).to(webAppPath);
      }
    }
  }
,new ConfigModule(configuration,hConf),new IOModule(),new MetricsHandlerModule(),new AuthModule(),new DiscoveryRuntimeModule().getStandaloneModules(),new LocationRuntimeModule().getStandaloneModules(),new AppFabricServiceRuntimeModule().getStandaloneModules(),new ProgramRunnerRuntimeModule().getStandaloneModules(),new GatewayModule().getStandaloneModules(),new DataFabricModules().getStandaloneModules(),new DataSetsModules().getLocalModule(),new DataSetServiceModules().getLocalModule(),new MetricsClientRuntimeModule().getStandaloneModules(),new LoggingModules().getStandaloneModules(),new RouterModules().getStandaloneModules(),new SecurityModules().getStandaloneModules(),new StreamServiceRuntimeModule().getStandaloneModules(),new ExploreRuntimeModule().getStandaloneModules(),new ServiceStoreModules().getStandaloneModule(),new ExploreClientModule());
}"
7480,"/** 
 * User Spark job which will be executed
 * @param args    the command line arguments
 * @param context {@link SparkContext} for this job
 */
public void run(String[] args,SparkContext context);","/** 
 * User Spark job which will be executed
 * @param context {@link SparkContext} for this job
 */
public void run(SparkContext context);"
7481,"/** 
 * User Spark job which will be executed
 * @param args    the command line arguments
 * @param context {@link SparkContext} for this job
 */
public void run(String[] args,SparkContext context);","/** 
 * User Spark job which will be executed
 * @param context {@link SparkContext} for this job
 */
public void run(SparkContext context);"
7482,"/** 
 * Extracts arguments which belongs to user's program and then invokes the run method on the user's program object with the arguments and the appropriate implementation   {@link SparkContext}
 * @param userProgramObject the user program's object
 * @throws RuntimeException if failed to invokeUserProgram main function on the user's program object
 */
private void runUserProgram(Object userProgramObject){
  String[] userprogramArgs=extractUserArgs();
  try {
    Method userProgramMain=userProgramClass.getMethod(""String_Node_Str"",String[].class,SparkContext.class);
    userProgramMain.invoke(userProgramObject,userprogramArgs,sparkContext);
  }
 catch (  NoSuchMethodException nsme) {
    LOG.warn(""String_Node_Str"",userProgramObject.getClass().getName(),nsme);
    throw Throwables.propagate(nsme);
  }
catch (  IllegalAccessException iae) {
    LOG.warn(""String_Node_Str"",userProgramObject.getClass().getName(),iae);
    throw Throwables.propagate(iae);
  }
catch (  InvocationTargetException ite) {
    LOG.warn(""String_Node_Str"",ite);
    throw Throwables.propagate(ite);
  }
}","/** 
 * Extracts arguments which belongs to user's program and then invokes the run method on the user's program object with the arguments and the appropriate implementation   {@link SparkContext}
 * @param userProgramObject the user program's object
 * @throws RuntimeException if failed to invokeUserProgram main function on the user's program object
 */
private void runUserProgram(Object userProgramObject){
  try {
    Method userProgramMain=userProgramClass.getMethod(""String_Node_Str"",SparkContext.class);
    userProgramMain.invoke(userProgramObject,sparkContext);
  }
 catch (  NoSuchMethodException nsme) {
    LOG.warn(""String_Node_Str"",userProgramObject.getClass().getName(),nsme);
    throw Throwables.propagate(nsme);
  }
catch (  IllegalAccessException iae) {
    LOG.warn(""String_Node_Str"",userProgramObject.getClass().getName(),iae);
    throw Throwables.propagate(iae);
  }
catch (  InvocationTargetException ite) {
    LOG.warn(""String_Node_Str"",ite);
    throw Throwables.propagate(ite);
  }
}"
7483,"private void submit(final Spark job,SparkSpecification sparkSpec,Location programJarLocation,final BasicSparkContext context) throws Exception {
  final Configuration conf=new Configuration(hConf);
  final ClassLoader classLoader=new CombineClassLoader(Objects.firstNonNull(Thread.currentThread().getContextClassLoader(),ClassLoader.getSystemClassLoader()),ImmutableList.of(context.getProgram().getClassLoader()));
  conf.setClassLoader(classLoader);
  beforeSubmit(job,context);
  final Location jobJarCopy=copyProgramJar(programJarLocation,context);
  LOG.info(""String_Node_Str"",jobJarCopy.toURI().getPath(),programJarLocation.toURI().toString());
  final Transaction tx=txSystemClient.startLong();
  SparkContextConfig.set(conf,context,cConf,tx,jobJarCopy);
  final Location dependencyJar=buildDependencyJar(context,SparkContextConfig.getHConf());
  LOG.info(""String_Node_Str"",dependencyJar.toURI().getPath());
  final String[] sparkSubmitArgs=prepareSparkSubmitArgs(sparkSpec,conf,jobJarCopy,dependencyJar);
  new Thread(){
    @Override public void run(){
      boolean success=false;
      try {
        LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
        LOG.info(""String_Node_Str"",context.toString());
        ClassLoader oldClassLoader=Thread.currentThread().getContextClassLoader();
        Thread.currentThread().setContextClassLoader(conf.getClassLoader());
        try {
          SparkProgramWrapper.setSparkProgramRunning(true);
          SparkSubmit.main(sparkSubmitArgs);
        }
 catch (        Exception e) {
          LOG.error(""String_Node_Str"",e);
        }
 finally {
          success=SparkProgramWrapper.isSparkProgramSuccessful();
          SparkProgramWrapper.setSparkProgramRunning(false);
          Thread.currentThread().setContextClassLoader(oldClassLoader);
        }
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
 finally {
        stopController(success,context,job,tx);
        try {
          dependencyJar.delete();
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"",dependencyJar.toURI());
        }
        try {
          jobJarCopy.delete();
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"",jobJarCopy.toURI());
        }
      }
    }
  }
.start();
}","private void submit(final Spark job,SparkSpecification sparkSpec,Location programJarLocation,final BasicSparkContext context) throws Exception {
  final Configuration conf=new Configuration(hConf);
  final ClassLoader classLoader=new CombineClassLoader(Objects.firstNonNull(Thread.currentThread().getContextClassLoader(),ClassLoader.getSystemClassLoader()),ImmutableList.of(context.getProgram().getClassLoader()));
  conf.setClassLoader(classLoader);
  beforeSubmit(job,context);
  final Location jobJarCopy=copyProgramJar(programJarLocation,context);
  LOG.info(""String_Node_Str"",jobJarCopy.toURI().getPath(),programJarLocation.toURI().toString());
  final Transaction tx=txSystemClient.startLong();
  SparkContextConfig.set(conf,context,cConf,tx,jobJarCopy);
  final Location dependencyJar=buildDependencyJar(context,SparkContextConfig.getHConf());
  LOG.info(""String_Node_Str"",dependencyJar.toURI().getPath());
  final String[] sparkSubmitArgs=prepareSparkSubmitArgs(sparkSpec,conf,jobJarCopy,dependencyJar);
  new Thread(){
    @Override public void run(){
      boolean success=false;
      try {
        LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
        LOG.info(""String_Node_Str"",context.toString());
        ClassLoader oldClassLoader=Thread.currentThread().getContextClassLoader();
        Thread.currentThread().setContextClassLoader(conf.getClassLoader());
        try {
          SparkProgramWrapper.setSparkProgramRunning(true);
          SparkSubmit.main(sparkSubmitArgs);
        }
 catch (        Exception e) {
          LOG.error(""String_Node_Str"",context.toString(),e);
        }
 finally {
          success=SparkProgramWrapper.isSparkProgramSuccessful();
          SparkProgramWrapper.setSparkProgramRunning(false);
          Thread.currentThread().setContextClassLoader(oldClassLoader);
        }
      }
 catch (      Exception e) {
        LOG.warn(""String_Node_Str"",e);
        throw Throwables.propagate(e);
      }
 finally {
        stopController(success,context,job,tx);
        try {
          dependencyJar.delete();
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"",dependencyJar.toURI());
        }
        try {
          jobJarCopy.delete();
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"",jobJarCopy.toURI());
        }
      }
    }
  }
.start();
}"
7484,"@Override public void run(){
  boolean success=false;
  try {
    LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
    LOG.info(""String_Node_Str"",context.toString());
    ClassLoader oldClassLoader=Thread.currentThread().getContextClassLoader();
    Thread.currentThread().setContextClassLoader(conf.getClassLoader());
    try {
      SparkProgramWrapper.setSparkProgramRunning(true);
      SparkSubmit.main(sparkSubmitArgs);
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",e);
    }
 finally {
      success=SparkProgramWrapper.isSparkProgramSuccessful();
      SparkProgramWrapper.setSparkProgramRunning(false);
      Thread.currentThread().setContextClassLoader(oldClassLoader);
    }
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
 finally {
    stopController(success,context,job,tx);
    try {
      dependencyJar.delete();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",dependencyJar.toURI());
    }
    try {
      jobJarCopy.delete();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",jobJarCopy.toURI());
    }
  }
}","@Override public void run(){
  boolean success=false;
  try {
    LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
    LOG.info(""String_Node_Str"",context.toString());
    ClassLoader oldClassLoader=Thread.currentThread().getContextClassLoader();
    Thread.currentThread().setContextClassLoader(conf.getClassLoader());
    try {
      SparkProgramWrapper.setSparkProgramRunning(true);
      SparkSubmit.main(sparkSubmitArgs);
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",context.toString(),e);
    }
 finally {
      success=SparkProgramWrapper.isSparkProgramSuccessful();
      SparkProgramWrapper.setSparkProgramRunning(false);
      Thread.currentThread().setContextClassLoader(oldClassLoader);
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
 finally {
    stopController(success,context,job,tx);
    try {
      dependencyJar.delete();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",dependencyJar.toURI());
    }
    try {
      jobJarCopy.delete();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",jobJarCopy.toURI());
    }
  }
}"
7485,"@Override public boolean programExists(final Id.Program id,final ProgramType type){
  return txnl.executeUnchecked(new TransactionExecutor.Function<AppMds,Boolean>(){
    @Override public Boolean apply(    AppMds mds) throws Exception {
      ApplicationSpecification appSpec=getApplicationSpec(mds,id.getApplication());
      if (appSpec == null) {
        return false;
      }
      ProgramSpecification programSpecification=null;
      try {
        if (type == ProgramType.FLOW) {
          programSpecification=getFlowSpecOrFail(id,appSpec);
        }
 else         if (type == ProgramType.PROCEDURE) {
          programSpecification=getProcedureSpecOrFail(id,appSpec);
        }
 else         if (type == ProgramType.SERVICE) {
          programSpecification=getServiceSpecOrFail(id,appSpec);
        }
 else         if (type == ProgramType.WORKFLOW) {
          programSpecification=appSpec.getWorkflows().get(id.getId());
        }
 else         if (type == ProgramType.MAPREDUCE) {
          programSpecification=appSpec.getMapReduce().get(id.getId());
        }
 else         if (type == ProgramType.WEBAPP) {
        }
 else {
          throw new IllegalArgumentException(""String_Node_Str"");
        }
      }
 catch (      NoSuchElementException e) {
        programSpecification=null;
      }
catch (      Exception e) {
        Throwables.propagate(e);
      }
      return (programSpecification != null);
    }
  }
);
}","@Override public boolean programExists(final Id.Program id,final ProgramType type){
  return txnl.executeUnchecked(new TransactionExecutor.Function<AppMds,Boolean>(){
    @Override public Boolean apply(    AppMds mds) throws Exception {
      ApplicationSpecification appSpec=getApplicationSpec(mds,id.getApplication());
      if (appSpec == null) {
        return false;
      }
      ProgramSpecification programSpecification=null;
      try {
        if (type == ProgramType.FLOW) {
          programSpecification=getFlowSpecOrFail(id,appSpec);
        }
 else         if (type == ProgramType.PROCEDURE) {
          programSpecification=getProcedureSpecOrFail(id,appSpec);
        }
 else         if (type == ProgramType.SERVICE) {
          programSpecification=getServiceSpecOrFail(id,appSpec);
        }
 else         if (type == ProgramType.WORKFLOW) {
          programSpecification=appSpec.getWorkflows().get(id.getId());
        }
 else         if (type == ProgramType.MAPREDUCE) {
          programSpecification=appSpec.getMapReduce().get(id.getId());
        }
 else         if (type == ProgramType.SPARK) {
          programSpecification=appSpec.getSpark().get(id.getId());
        }
 else         if (type == ProgramType.WEBAPP) {
        }
 else {
          throw new IllegalArgumentException(""String_Node_Str"");
        }
      }
 catch (      NoSuchElementException e) {
        programSpecification=null;
      }
catch (      Exception e) {
        Throwables.propagate(e);
      }
      return (programSpecification != null);
    }
  }
);
}"
7486,"@Override public Boolean apply(AppMds mds) throws Exception {
  ApplicationSpecification appSpec=getApplicationSpec(mds,id.getApplication());
  if (appSpec == null) {
    return false;
  }
  ProgramSpecification programSpecification=null;
  try {
    if (type == ProgramType.FLOW) {
      programSpecification=getFlowSpecOrFail(id,appSpec);
    }
 else     if (type == ProgramType.PROCEDURE) {
      programSpecification=getProcedureSpecOrFail(id,appSpec);
    }
 else     if (type == ProgramType.SERVICE) {
      programSpecification=getServiceSpecOrFail(id,appSpec);
    }
 else     if (type == ProgramType.WORKFLOW) {
      programSpecification=appSpec.getWorkflows().get(id.getId());
    }
 else     if (type == ProgramType.MAPREDUCE) {
      programSpecification=appSpec.getMapReduce().get(id.getId());
    }
 else     if (type == ProgramType.WEBAPP) {
    }
 else {
      throw new IllegalArgumentException(""String_Node_Str"");
    }
  }
 catch (  NoSuchElementException e) {
    programSpecification=null;
  }
catch (  Exception e) {
    Throwables.propagate(e);
  }
  return (programSpecification != null);
}","@Override public Boolean apply(AppMds mds) throws Exception {
  ApplicationSpecification appSpec=getApplicationSpec(mds,id.getApplication());
  if (appSpec == null) {
    return false;
  }
  ProgramSpecification programSpecification=null;
  try {
    if (type == ProgramType.FLOW) {
      programSpecification=getFlowSpecOrFail(id,appSpec);
    }
 else     if (type == ProgramType.PROCEDURE) {
      programSpecification=getProcedureSpecOrFail(id,appSpec);
    }
 else     if (type == ProgramType.SERVICE) {
      programSpecification=getServiceSpecOrFail(id,appSpec);
    }
 else     if (type == ProgramType.WORKFLOW) {
      programSpecification=appSpec.getWorkflows().get(id.getId());
    }
 else     if (type == ProgramType.MAPREDUCE) {
      programSpecification=appSpec.getMapReduce().get(id.getId());
    }
 else     if (type == ProgramType.SPARK) {
      programSpecification=appSpec.getSpark().get(id.getId());
    }
 else     if (type == ProgramType.WEBAPP) {
    }
 else {
      throw new IllegalArgumentException(""String_Node_Str"");
    }
  }
 catch (  NoSuchElementException e) {
    programSpecification=null;
  }
catch (  Exception e) {
    Throwables.propagate(e);
  }
  return (programSpecification != null);
}"
7487,"@Override public List<ProgramSpecification> getDeletedProgramSpecifications(final Id.Application id,ApplicationSpecification appSpec){
  ApplicationMeta existing=txnl.executeUnchecked(new TransactionExecutor.Function<AppMds,ApplicationMeta>(){
    @Override public ApplicationMeta apply(    AppMds mds) throws Exception {
      return mds.apps.getApplication(id.getAccountId(),id.getId());
    }
  }
);
  List<ProgramSpecification> deletedProgramSpecs=Lists.newArrayList();
  if (existing != null) {
    ApplicationSpecification existingAppSpec=existing.getSpec();
    ImmutableMap<String,ProgramSpecification> existingSpec=new ImmutableMap.Builder<String,ProgramSpecification>().putAll(existingAppSpec.getMapReduce()).putAll(existingAppSpec.getWorkflows()).putAll(existingAppSpec.getFlows()).putAll(existingAppSpec.getProcedures()).putAll(existingAppSpec.getServices()).build();
    ImmutableMap<String,ProgramSpecification> newSpec=new ImmutableMap.Builder<String,ProgramSpecification>().putAll(appSpec.getMapReduce()).putAll(appSpec.getWorkflows()).putAll(appSpec.getFlows()).putAll(appSpec.getProcedures()).putAll(appSpec.getServices()).build();
    MapDifference<String,ProgramSpecification> mapDiff=Maps.difference(existingSpec,newSpec);
    deletedProgramSpecs.addAll(mapDiff.entriesOnlyOnLeft().values());
  }
  return deletedProgramSpecs;
}","@Override public List<ProgramSpecification> getDeletedProgramSpecifications(final Id.Application id,ApplicationSpecification appSpec){
  ApplicationMeta existing=txnl.executeUnchecked(new TransactionExecutor.Function<AppMds,ApplicationMeta>(){
    @Override public ApplicationMeta apply(    AppMds mds) throws Exception {
      return mds.apps.getApplication(id.getAccountId(),id.getId());
    }
  }
);
  List<ProgramSpecification> deletedProgramSpecs=Lists.newArrayList();
  if (existing != null) {
    ApplicationSpecification existingAppSpec=existing.getSpec();
    ImmutableMap<String,ProgramSpecification> existingSpec=new ImmutableMap.Builder<String,ProgramSpecification>().putAll(existingAppSpec.getMapReduce()).putAll(existingAppSpec.getSpark()).putAll(existingAppSpec.getWorkflows()).putAll(existingAppSpec.getFlows()).putAll(existingAppSpec.getProcedures()).putAll(existingAppSpec.getServices()).build();
    ImmutableMap<String,ProgramSpecification> newSpec=new ImmutableMap.Builder<String,ProgramSpecification>().putAll(appSpec.getMapReduce()).putAll(existingAppSpec.getSpark()).putAll(appSpec.getWorkflows()).putAll(appSpec.getFlows()).putAll(appSpec.getProcedures()).putAll(appSpec.getServices()).build();
    MapDifference<String,ProgramSpecification> mapDiff=Maps.difference(existingSpec,newSpec);
    deletedProgramSpecs.addAll(mapDiff.entriesOnlyOnLeft().values());
  }
  return deletedProgramSpecs;
}"
7488,"/** 
 * Stops the Spark program by calling   {@link org.apache.spark.SparkContext#stop()}
 */
public static void stopSparkProgram(){
  if (scalaProgramFlag) {
    ((org.apache.spark.SparkContext)getSparkContext().getOriginalSparkContext()).stop();
  }
 else {
    ((org.apache.spark.api.java.JavaSparkContext)getSparkContext().getOriginalSparkContext()).stop();
  }
}","/** 
 * Stops the Spark program by calling   {@link org.apache.spark.SparkContext#stop()}
 */
public static void stopSparkProgram(){
  sparkContextStopBugFixer();
  if (isScalaProgram()) {
    ((org.apache.spark.SparkContext)getSparkContext().getOriginalSparkContext()).stop();
  }
 else {
    ((org.apache.spark.api.java.JavaSparkContext)getSparkContext().getOriginalSparkContext()).stop();
  }
}"
7489,"/** 
 * Sets the   {@link SparkContext} to {@link JavaSparkContext} or to {@link ScalaSparkContext} depending on whetherthe user class implements  {@link JavaSparkProgram} or {@link ScalaSparkProgram}
 */
public void setSparkContext(){
  if (JavaSparkProgram.class.isAssignableFrom(userProgramClass)) {
    sparkContext=new JavaSparkContext();
  }
 else   if (ScalaSparkProgram.class.isAssignableFrom(userProgramClass)) {
    sparkContext=new ScalaSparkContext();
    scalaProgramFlag=true;
  }
 else {
    String error=""String_Node_Str"";
    throw new IllegalArgumentException(error);
  }
}","/** 
 * Sets the   {@link SparkContext} to {@link JavaSparkContext} or to {@link ScalaSparkContext} depending on whetherthe user class implements  {@link JavaSparkProgram} or {@link ScalaSparkProgram}
 */
void setSparkContext(){
  if (JavaSparkProgram.class.isAssignableFrom(userProgramClass)) {
    sparkContext=new JavaSparkContext();
  }
 else   if (ScalaSparkProgram.class.isAssignableFrom(userProgramClass)) {
    sparkContext=new ScalaSparkContext();
    setScalaProgram(true);
  }
 else {
    String error=""String_Node_Str"";
    throw new IllegalArgumentException(error);
  }
}"
7490,"/** 
 * Constructor
 * @param args the command line arguments
 * @throws RuntimeException if the user's program class is not found
 */
public SparkProgramWrapper(String[] args){
  arguments=validateArgs(args);
  try {
    userProgramClass=Class.forName(arguments[0],true,Thread.currentThread().getContextClassLoader());
  }
 catch (  ClassNotFoundException cnfe) {
    LOG.warn(""String_Node_Str"",arguments[0],cnfe);
    throw Throwables.propagate(cnfe);
  }
  setSparkContext();
}","/** 
 * Constructor
 * @param args the command line arguments
 * @throws RuntimeException if the user's program class is not found
 */
private SparkProgramWrapper(String[] args){
  arguments=validateArgs(args);
  try {
    userProgramClass=Class.forName(arguments[0],true,Thread.currentThread().getContextClassLoader());
  }
 catch (  ClassNotFoundException cnfe) {
    LOG.warn(""String_Node_Str"",arguments[0],cnfe);
    throw Throwables.propagate(cnfe);
  }
  setSparkContext();
}"
7491,"/** 
 * Stops the Spark program by calling   {@link org.apache.spark.SparkContext#stop()}
 */
public static void stopSparkProgram(){
  if (scalaProgramFlag) {
    ((org.apache.spark.SparkContext)getSparkContext().getOriginalSparkContext()).stop();
  }
 else {
    ((org.apache.spark.api.java.JavaSparkContext)getSparkContext().getOriginalSparkContext()).stop();
  }
}","/** 
 * Stops the Spark program by calling   {@link org.apache.spark.SparkContext#stop()}
 */
public static void stopSparkProgram(){
  sparkContextStopBugFixer();
  if (isScalaProgram()) {
    ((org.apache.spark.SparkContext)getSparkContext().getOriginalSparkContext()).stop();
  }
 else {
    ((org.apache.spark.api.java.JavaSparkContext)getSparkContext().getOriginalSparkContext()).stop();
  }
}"
7492,"/** 
 * Sets the   {@link SparkContext} to {@link JavaSparkContext} or to {@link ScalaSparkContext} depending on whetherthe user class implements  {@link JavaSparkProgram} or {@link ScalaSparkProgram}
 */
public void setSparkContext(){
  if (JavaSparkProgram.class.isAssignableFrom(userProgramClass)) {
    sparkContext=new JavaSparkContext();
  }
 else   if (ScalaSparkProgram.class.isAssignableFrom(userProgramClass)) {
    sparkContext=new ScalaSparkContext();
    scalaProgramFlag=true;
  }
 else {
    String error=""String_Node_Str"";
    throw new IllegalArgumentException(error);
  }
}","/** 
 * Sets the   {@link SparkContext} to {@link JavaSparkContext} or to {@link ScalaSparkContext} depending on whetherthe user class implements  {@link JavaSparkProgram} or {@link ScalaSparkProgram}
 */
void setSparkContext(){
  if (JavaSparkProgram.class.isAssignableFrom(userProgramClass)) {
    sparkContext=new JavaSparkContext();
  }
 else   if (ScalaSparkProgram.class.isAssignableFrom(userProgramClass)) {
    sparkContext=new ScalaSparkContext();
    setScalaProgram(true);
  }
 else {
    String error=""String_Node_Str"";
    throw new IllegalArgumentException(error);
  }
}"
7493,"/** 
 * Constructor
 * @param args the command line arguments
 * @throws RuntimeException if the user's program class is not found
 */
public SparkProgramWrapper(String[] args){
  arguments=validateArgs(args);
  try {
    userProgramClass=Class.forName(arguments[0],true,Thread.currentThread().getContextClassLoader());
  }
 catch (  ClassNotFoundException cnfe) {
    LOG.warn(""String_Node_Str"",arguments[0],cnfe);
    throw Throwables.propagate(cnfe);
  }
  setSparkContext();
}","/** 
 * Constructor
 * @param args the command line arguments
 * @throws RuntimeException if the user's program class is not found
 */
private SparkProgramWrapper(String[] args){
  arguments=validateArgs(args);
  try {
    userProgramClass=Class.forName(arguments[0],true,Thread.currentThread().getContextClassLoader());
  }
 catch (  ClassNotFoundException cnfe) {
    LOG.warn(""String_Node_Str"",arguments[0],cnfe);
    throw Throwables.propagate(cnfe);
  }
  setSparkContext();
}"
7494,"/** 
 * Stops the Spark program by calling   {@link org.apache.spark.SparkContext#stop()}
 */
public static void stopSparkProgram(){
  if (scalaJobFlag) {
    ((org.apache.spark.SparkContext)getSparkContext().getOriginalSparkContext()).stop();
  }
 else {
    ((org.apache.spark.api.java.JavaSparkContext)getSparkContext().getOriginalSparkContext()).stop();
  }
}","/** 
 * Stops the Spark program by calling   {@link org.apache.spark.SparkContext#stop()}
 */
public static void stopSparkProgram(){
  if (scalaProgramFlag) {
    ((org.apache.spark.SparkContext)getSparkContext().getOriginalSparkContext()).stop();
  }
 else {
    ((org.apache.spark.api.java.JavaSparkContext)getSparkContext().getOriginalSparkContext()).stop();
  }
}"
7495,"/** 
 * Sets the   {@link SparkContext} to {@link JavaSparkContext} or to {@link ScalaSparkContext} depending on whetherthe user class implements  {@link JavaSparkProgram} or {@link ScalaSparkProgram}
 */
public void setSparkContext(){
  if (JavaSparkProgram.class.isAssignableFrom(userJobClass)) {
    sparkContext=new JavaSparkContext();
  }
 else   if (ScalaSparkProgram.class.isAssignableFrom(userJobClass)) {
    sparkContext=new ScalaSparkContext();
    scalaJobFlag=true;
  }
 else {
    String error=""String_Node_Str"";
    throw new IllegalArgumentException(error);
  }
}","/** 
 * Sets the   {@link SparkContext} to {@link JavaSparkContext} or to {@link ScalaSparkContext} depending on whetherthe user class implements  {@link JavaSparkProgram} or {@link ScalaSparkProgram}
 */
public void setSparkContext(){
  if (JavaSparkProgram.class.isAssignableFrom(userProgramClass)) {
    sparkContext=new JavaSparkContext();
  }
 else   if (ScalaSparkProgram.class.isAssignableFrom(userProgramClass)) {
    sparkContext=new ScalaSparkContext();
    scalaProgramFlag=true;
  }
 else {
    String error=""String_Node_Str"";
    throw new IllegalArgumentException(error);
  }
}"
7496,"/** 
 * @param sparkProgramSuccessful a boolean to which the jobSuccess status will be set to
 */
public static void setSparkProgramSuccessful(boolean sparkProgramSuccessful){
  SparkProgramWrapper.sparkProgramSuccessful=sparkProgramSuccessful;
}","/** 
 * @param sparkProgramSuccessful a boolean to which the programSuccess status will be set to
 */
public static void setSparkProgramSuccessful(boolean sparkProgramSuccessful){
  SparkProgramWrapper.sparkProgramSuccessful=sparkProgramSuccessful;
}"
7497,"public static void main(String[] args){
  new SparkProgramWrapper(args).instantiateUserJobClass();
}","public static void main(String[] args){
  new SparkProgramWrapper(args).instantiateUserProgramClass();
}"
7498,"/** 
 * Extracts arguments belonging to the user's job class
 * @return String[] of arguments with which user's job class should be called
 */
private String[] extractUserArgs(){
  String[] userJobArgs=new String[(arguments.length - JOB_WRAPPER_ARGUMENTS_SIZE)];
  System.arraycopy(arguments,JOB_WRAPPER_ARGUMENTS_SIZE,userJobArgs,0,(arguments.length - JOB_WRAPPER_ARGUMENTS_SIZE));
  return userJobArgs;
}","/** 
 * Extracts arguments belonging to the user's program class
 * @return String[] of arguments with which user's program class should be called
 */
private String[] extractUserArgs(){
  String[] userProgramArgs=new String[(arguments.length - PROGRAM_WRAPPER_ARGUMENTS_SIZE)];
  System.arraycopy(arguments,PROGRAM_WRAPPER_ARGUMENTS_SIZE,userProgramArgs,0,(arguments.length - PROGRAM_WRAPPER_ARGUMENTS_SIZE));
  return userProgramArgs;
}"
7499,"/** 
 * Constructor
 * @param args the command line arguments
 * @throws RuntimeException if the user's job class is not found
 */
public SparkProgramWrapper(String[] args){
  arguments=validateArgs(args);
  try {
    userJobClass=Class.forName(arguments[0],true,Thread.currentThread().getContextClassLoader());
  }
 catch (  ClassNotFoundException cnfe) {
    LOG.warn(""String_Node_Str"",arguments[0],cnfe);
    throw Throwables.propagate(cnfe);
  }
  setSparkContext();
}","/** 
 * Constructor
 * @param args the command line arguments
 * @throws RuntimeException if the user's program class is not found
 */
public SparkProgramWrapper(String[] args){
  arguments=validateArgs(args);
  try {
    userProgramClass=Class.forName(arguments[0],true,Thread.currentThread().getContextClassLoader());
  }
 catch (  ClassNotFoundException cnfe) {
    LOG.warn(""String_Node_Str"",arguments[0],cnfe);
    throw Throwables.propagate(cnfe);
  }
  setSparkContext();
}"
7500,"/** 
 * Validates command line arguments being passed Expects at least   {@link SparkProgramWrapper#JOB_WRAPPER_ARGUMENTS_SIZE} command line arguments to be present
 * @param arguments String[] the arguments
 * @return String[] if the command line arguments are sufficient else throws a {@link RuntimeException}
 * @throws IllegalArgumentException if the required numbers of command line arguments were not present
 */
private String[] validateArgs(String[] arguments){
  if (arguments.length < JOB_WRAPPER_ARGUMENTS_SIZE) {
    throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"");
  }
  return arguments;
}","/** 
 * Validates command line arguments being passed Expects at least   {@link SparkProgramWrapper#PROGRAM_WRAPPER_ARGUMENTS_SIZE} command line arguments to be present
 * @param arguments String[] the arguments
 * @return String[] if the command line arguments are sufficient else throws a {@link RuntimeException}
 * @throws IllegalArgumentException if the required numbers of command line arguments were not present
 */
private String[] validateArgs(String[] arguments){
  if (arguments.length < PROGRAM_WRAPPER_ARGUMENTS_SIZE) {
    throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"");
  }
  return arguments;
}"
7501,"/** 
 * Sets the   {@link SparkContext} to {@link JavaSparkContext} or to {@link ScalaSparkContext} depending on whetherthe user class implements  {@link JavaSparkJob} or {@link ScalaSparkJob}
 */
public void setSparkContext(){
  if (JavaSparkJob.class.isAssignableFrom(userJobClass)) {
    sparkContext=new JavaSparkContext();
  }
 else   if (ScalaSparkJob.class.isAssignableFrom(userJobClass)) {
    sparkContext=new ScalaSparkContext();
    scalaJobFlag=true;
  }
 else {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
}","/** 
 * Sets the   {@link SparkContext} to {@link JavaSparkContext} or to {@link ScalaSparkContext} depending on whetherthe user class implements  {@link JavaSparkProgram} or {@link ScalaSparkProgram}
 */
public void setSparkContext(){
  if (JavaSparkProgram.class.isAssignableFrom(userJobClass)) {
    sparkContext=new JavaSparkContext();
  }
 else   if (ScalaSparkProgram.class.isAssignableFrom(userJobClass)) {
    sparkContext=new ScalaSparkContext();
    scalaJobFlag=true;
  }
 else {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
}"
7502,"/** 
 * Sets the   {@link SparkContext} to {@link JavaSparkContext} or to {@link ScalaSparkContext} depending on whetherthe user class implements  {@link JavaSparkProgram} or {@link ScalaSparkProgram}
 */
public void setSparkContext(){
  if (JavaSparkProgram.class.isAssignableFrom(userJobClass)) {
    sparkContext=new JavaSparkContext();
  }
 else   if (ScalaSparkProgram.class.isAssignableFrom(userJobClass)) {
    sparkContext=new ScalaSparkContext();
    scalaJobFlag=true;
  }
 else {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
}","/** 
 * Sets the   {@link SparkContext} to {@link JavaSparkContext} or to {@link ScalaSparkContext} depending on whetherthe user class implements  {@link JavaSparkProgram} or {@link ScalaSparkProgram}
 */
public void setSparkContext(){
  if (JavaSparkProgram.class.isAssignableFrom(userJobClass)) {
    sparkContext=new JavaSparkContext();
  }
 else   if (ScalaSparkProgram.class.isAssignableFrom(userJobClass)) {
    sparkContext=new ScalaSparkContext();
    scalaJobFlag=true;
  }
 else {
    String error=""String_Node_Str"";
    throw new IllegalArgumentException(error);
  }
}"
7503,"/** 
 * Gets information about a service.
 * @param appId ID of the application that the service belongs to
 * @param serviceId ID of the service
 * @return {@link ServiceMeta} representing the service.
 * @throws IOException if a network error occurred
 */
public ServiceMeta get(String appId,String serviceId) throws IOException {
  URL url=config.resolveURL(String.format(""String_Node_Str"",appId,serviceId));
  HttpResponse response=restClient.execute(HttpMethod.GET,url);
  return ObjectResponse.fromJsonBody(response,ServiceMeta.class).getResponseObject();
}","/** 
 * Gets information about a service.
 * @param appId ID of the application that the service belongs to
 * @param serviceId ID of the service
 * @return {@link ServiceMeta} representing the service.
 * @throws IOException if a network error occurred
 */
public ServiceMeta get(String appId,String serviceId) throws IOException {
  URL url=config.resolveURL(String.format(""String_Node_Str"",appId,serviceId));
  HttpResponse response=restClient.execute(HttpMethod.GET,url,config.getAccessToken());
  return ObjectResponse.fromJsonBody(response,ServiceMeta.class).getResponseObject();
}"
7504,"/** 
 * Gets the configuration of a stream.
 * @param streamId ID of the stream
 * @throws IOException if a network error occurred
 * @throws StreamNotFoundException if the stream was not found
 */
public StreamProperties getConfig(String streamId) throws IOException, StreamNotFoundException {
  URL url=config.resolveURL(String.format(""String_Node_Str"",streamId));
  HttpResponse response=restClient.execute(HttpMethod.GET,url,HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new StreamNotFoundException(streamId);
  }
  return ObjectResponse.fromJsonBody(response,StreamProperties.class).getResponseObject();
}","/** 
 * Gets the configuration of a stream.
 * @param streamId ID of the stream
 * @throws IOException if a network error occurred
 * @throws StreamNotFoundException if the stream was not found
 */
public StreamProperties getConfig(String streamId) throws IOException, StreamNotFoundException {
  URL url=config.resolveURL(String.format(""String_Node_Str"",streamId));
  HttpResponse response=restClient.execute(HttpMethod.GET,url,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new StreamNotFoundException(streamId);
  }
  return ObjectResponse.fromJsonBody(response,StreamProperties.class).getResponseObject();
}"
7505,"/** 
 * Build the instance of   {@link BasicMapReduceContext}.
 * @param runId program run id
 * @param logicalStartTime The logical start time of the job.
 * @param workflowBatch Tells whether the batch job is started by workflow.
 * @param tx transaction to use
 * @param classLoader classloader to use
 * @param programLocation program location
 * @param inputDataSetName name of the input dataset if specified for this mapreduce job, null otherwise
 * @param inputSplits input splits if specified for this mapreduce job, null otherwise
 * @param outputDataSetName name of the output dataset if specified for this mapreduce job, null otherwise
 * @return instance of {@link BasicMapReduceContext}
 */
public BasicMapReduceContext build(MapReduceMetrics.TaskType type,String runId,long logicalStartTime,String workflowBatch,Arguments runtimeArguments,Transaction tx,ClassLoader classLoader,URI programLocation,@Nullable String inputDataSetName,@Nullable List<Split> inputSplits,@Nullable String outputDataSetName){
}","/** 
 * Build the instance of   {@link BasicMapReduceContext}.
 * @param runId program run id
 * @param logicalStartTime The logical start time of the job.
 * @param workflowBatch Tells whether the batch job is started by workflow.
 * @param tx transaction to use
 * @param classLoader classloader to use
 * @param programLocation program location
 * @param inputDataSetName name of the input dataset if specified for this mapreduce job, null otherwise
 * @param inputSplits input splits if specified for this mapreduce job, null otherwise
 * @param outputDataSetName name of the output dataset if specified for this mapreduce job, null otherwise
 * @return instance of {@link BasicMapReduceContext}
 */
public BasicMapReduceContext build(MapReduceMetrics.TaskType type,String runId,long logicalStartTime,String workflowBatch,Arguments runtimeArguments,Transaction tx,ClassLoader classLoader,URI programLocation,@Nullable String inputDataSetName,@Nullable List<Split> inputSplits,@Nullable String outputDataSetName){
  Injector injector=prepare();
  LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
  Program program;
  try {
    program=Programs.create(locationFactory.create(programLocation),classLoader);
    if (workflowBatch != null) {
      MapReduceSpecification mapReduceSpec=program.getSpecification().getMapReduce().get(workflowBatch);
      Preconditions.checkArgument(mapReduceSpec != null,""String_Node_Str"",workflowBatch);
      program=new WorkflowMapReduceProgram(program,mapReduceSpec);
    }
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"" + programLocation);
    throw Throwables.propagate(e);
  }
  DatasetFramework datasetFramework=injector.getInstance(DatasetFramework.class);
  CConfiguration configuration=injector.getInstance(CConfiguration.class);
  ApplicationSpecification programSpec=program.getSpecification();
  MetricsCollectionService metricsCollectionService=(type == null) ? null : injector.getInstance(MetricsCollectionService.class);
  ProgramServiceDiscovery serviceDiscovery=injector.getInstance(ProgramServiceDiscovery.class);
  DiscoveryServiceClient discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  MapReduceSpecification spec=program.getSpecification().getMapReduce().get(program.getName());
  BasicMapReduceContext context=new BasicMapReduceContext(program,type,RunIds.fromString(runId),runtimeArguments,programSpec.getDatasets().keySet(),spec,logicalStartTime,workflowBatch,serviceDiscovery,discoveryServiceClient,metricsCollectionService,datasetFramework,configuration);
  for (  TransactionAware txAware : context.getDatasetInstantiator().getTransactionAware()) {
    txAware.startTx(tx);
  }
  if (inputDataSetName != null && inputSplits != null) {
    context.setInput(inputDataSetName,inputSplits);
  }
  if (outputDataSetName != null) {
    context.setOutput(outputDataSetName);
  }
  return context;
}"
7506,"@Override public void close() throws IOException {
  flush();
  avroFileWriter.close();
}","@Override public void close() throws IOException {
  if (!closed.compareAndSet(false,true)) {
    return;
  }
  flush();
  avroFileWriter.close();
}"
7507,"@Override public void close() throws IOException {
  flush();
  avroFileWriter.close();
}","@Override public void close() throws IOException {
  if (!closed.compareAndSet(false,true)) {
    return;
  }
  flush();
  avroFileWriter.close();
}"
7508,"@Test public void testCleanup() throws Exception {
  DatasetFramework dsFramework=new InMemoryDatasetFramework(new InMemoryDefinitionRegistryFactory());
  dsFramework.addModule(""String_Node_Str"",new InMemoryOrderedTableModule());
  CConfiguration cConf=CConfiguration.create();
  Configuration conf=HBaseConfiguration.create();
  cConf.copyTxProperties(conf);
  TransactionManager txManager=new TransactionManager(conf);
  txManager.startAndWait();
  TransactionSystemClient txClient=new InMemoryTxSystemClient(txManager);
  FileMetaDataManager fileMetaDataManager=new FileMetaDataManager(new LogSaverTableUtil(dsFramework,cConf),txClient,locationFactory);
  Location baseDir=locationFactory.create(TEMP_FOLDER.newFolder().toURI());
  long deletionBoundary=System.currentTimeMillis() - RETENTION_DURATION_MS;
  LOG.info(""String_Node_Str"",deletionBoundary);
  LoggingContext dummyContext=new FlowletLoggingContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Location contextDir=baseDir.append(""String_Node_Str"");
  List<Location> toDelete=Lists.newArrayList();
  for (int i=0; i < 5; ++i) {
    toDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
    toDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
    toDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
    toDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
    toDelete.add(contextDir.append(""String_Node_Str""));
  }
  Assert.assertFalse(toDelete.isEmpty());
  List<Location> notDelete=Lists.newArrayList();
  for (int i=0; i < 5; ++i) {
    toDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
    notDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
    notDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
    notDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
  }
  Assert.assertFalse(notDelete.isEmpty());
  for (  Location location : toDelete) {
    fileMetaDataManager.writeMetaData(dummyContext,deletionBoundary - RANDOM.nextInt(50000) - 10000,createFile(location));
  }
  for (  Location location : notDelete) {
    fileMetaDataManager.writeMetaData(dummyContext,deletionBoundary + RANDOM.nextInt(50000) + 10000,createFile(location));
  }
  Assert.assertEquals(toDelete.size() + notDelete.size(),fileMetaDataManager.listFiles(dummyContext).size());
  LogCleanup logCleanup=new LogCleanup(fileMetaDataManager,baseDir,RETENTION_DURATION_MS);
  logCleanup.run();
  logCleanup.run();
  for (  Location location : toDelete) {
    Assert.assertFalse(""String_Node_Str"" + location.toURI() + ""String_Node_Str"",location.exists());
  }
  for (  Location location : notDelete) {
    Assert.assertTrue(""String_Node_Str"" + location.toURI() + ""String_Node_Str"",location.exists());
  }
  for (int i=0; i < 5; ++i) {
    Location delDir=contextDir.append(""String_Node_Str"" + i);
    Assert.assertFalse(""String_Node_Str"" + delDir.toURI() + ""String_Node_Str"",delDir.exists());
  }
}","@Test public void testCleanup() throws Exception {
  DatasetFramework dsFramework=new InMemoryDatasetFramework(new InMemoryDefinitionRegistryFactory());
  dsFramework.addModule(""String_Node_Str"",new InMemoryOrderedTableModule());
  CConfiguration cConf=CConfiguration.create();
  Configuration conf=HBaseConfiguration.create();
  cConf.copyTxProperties(conf);
  TransactionManager txManager=new TransactionManager(conf);
  txManager.startAndWait();
  TransactionSystemClient txClient=new InMemoryTxSystemClient(txManager);
  FileMetaDataManager fileMetaDataManager=new FileMetaDataManager(new LogSaverTableUtil(dsFramework,cConf),txClient,locationFactory);
  Location baseDir=locationFactory.create(TEMP_FOLDER.newFolder().toURI());
  long deletionBoundary=System.currentTimeMillis() - RETENTION_DURATION_MS;
  LOG.info(""String_Node_Str"",deletionBoundary);
  LoggingContext dummyContext=new FlowletLoggingContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Location contextDir=baseDir.append(""String_Node_Str"");
  List<Location> toDelete=Lists.newArrayList();
  for (int i=0; i < 5; ++i) {
    toDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
    toDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
    toDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
    toDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
    toDelete.add(contextDir.append(""String_Node_Str""));
  }
  Assert.assertFalse(toDelete.isEmpty());
  List<Location> notDelete=Lists.newArrayList();
  for (int i=0; i < 5; ++i) {
    toDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
    notDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
    notDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
    notDelete.add(contextDir.append(""String_Node_Str"" + i + ""String_Node_Str""));
  }
  Assert.assertFalse(notDelete.isEmpty());
  for (  Location location : toDelete) {
    fileMetaDataManager.writeMetaData(dummyContext,deletionBoundary - RANDOM.nextInt(50000) - 10000,createFile(location));
  }
  for (  Location location : notDelete) {
    fileMetaDataManager.writeMetaData(dummyContext,deletionBoundary + RANDOM.nextInt(50000) + 10000,createFile(location));
  }
  Assert.assertEquals(locationListsToString(toDelete,notDelete),toDelete.size() + notDelete.size(),fileMetaDataManager.listFiles(dummyContext).size());
  LogCleanup logCleanup=new LogCleanup(fileMetaDataManager,baseDir,RETENTION_DURATION_MS);
  logCleanup.run();
  logCleanup.run();
  for (  Location location : toDelete) {
    Assert.assertFalse(""String_Node_Str"" + location.toURI() + ""String_Node_Str"",location.exists());
  }
  for (  Location location : notDelete) {
    Assert.assertTrue(""String_Node_Str"" + location.toURI() + ""String_Node_Str"",location.exists());
  }
  for (int i=0; i < 5; ++i) {
    Location delDir=contextDir.append(""String_Node_Str"" + i);
    Assert.assertFalse(""String_Node_Str"" + delDir.toURI() + ""String_Node_Str"",delDir.exists());
  }
}"
7509,"private Location createFile(Location path) throws Exception {
  Location parent=Locations.getParent(path);
  parent.mkdirs();
  path.createNew();
  Assert.assertTrue(path.exists());
  return path;
}","private Location createFile(Location path) throws Exception {
  Location parent=Locations.getParent(path);
  Assert.assertNotNull(parent);
  parent.mkdirs();
  path.createNew();
  Assert.assertTrue(path.exists());
  return path;
}"
7510,"NettyRouter(CConfiguration cConf,@Named(Constants.Router.ADDRESS) InetAddress hostname,RouterServiceLookup serviceLookup,TokenValidator tokenValidator,AccessTokenTransformer accessTokenTransformer,DiscoveryServiceClient discoveryServiceClient,Configuration sslConfiguration){
  this.serverBossThreadPoolSize=cConf.getInt(Constants.Router.SERVER_BOSS_THREADS,Constants.Router.DEFAULT_SERVER_BOSS_THREADS);
  this.serverWorkerThreadPoolSize=cConf.getInt(Constants.Router.SERVER_WORKER_THREADS,Constants.Router.DEFAULT_SERVER_WORKER_THREADS);
  this.serverConnectionBacklog=cConf.getInt(Constants.Router.BACKLOG_CONNECTIONS,Constants.Router.DEFAULT_BACKLOG);
  this.clientBossThreadPoolSize=cConf.getInt(Constants.Router.CLIENT_BOSS_THREADS,Constants.Router.DEFAULT_CLIENT_BOSS_THREADS);
  this.clientWorkerThreadPoolSize=cConf.getInt(Constants.Router.CLIENT_WORKER_THREADS,Constants.Router.DEFAULT_CLIENT_WORKER_THREADS);
  this.hostname=hostname;
  this.forwards=Sets.newHashSet(cConf.getStrings(Constants.Router.FORWARD,Constants.Router.DEFAULT_FORWARD));
  Preconditions.checkState(!this.forwards.isEmpty(),""String_Node_Str"");
  LOG.info(""String_Node_Str"",this.forwards);
  this.serviceLookup=serviceLookup;
  this.securityEnabled=cConf.getBoolean(Constants.Security.CFG_SECURITY_ENABLED,false);
  this.realm=cConf.get(Constants.Security.CFG_REALM);
  this.tokenValidator=tokenValidator;
  this.accessTokenTransformer=accessTokenTransformer;
  this.discoveryServiceClient=discoveryServiceClient;
  this.configuration=cConf;
  this.sslEnabled=cConf.getBoolean(Constants.Security.ROUTER_SSL_ENABLED);
  if (isSSLEnabled()) {
    File keystore;
    try {
      keystore=new File(sslConfiguration.get(Constants.Security.ROUTER_SSL_KEYSTORE_PATH));
    }
 catch (    Exception e) {
      throw new RuntimeException(""String_Node_Str"" + sslConfiguration.get(Constants.Security.ROUTER_SSL_KEYSTORE_PATH));
    }
    this.sslHandlerFactory=new SSLHandlerFactory(keystore,sslConfiguration.get(Constants.Security.ROUTER_SSL_KEYSTORE_PASSWORD),sslConfiguration.get(Constants.Security.ROUTER_SSL_KEYPASSWORD));
  }
 else {
    this.sslHandlerFactory=null;
  }
}","@Inject public NettyRouter(CConfiguration cConf,@Named(Constants.Router.ADDRESS) InetAddress hostname,RouterServiceLookup serviceLookup,TokenValidator tokenValidator,AccessTokenTransformer accessTokenTransformer,DiscoveryServiceClient discoveryServiceClient){
  this.serverBossThreadPoolSize=cConf.getInt(Constants.Router.SERVER_BOSS_THREADS,Constants.Router.DEFAULT_SERVER_BOSS_THREADS);
  this.serverWorkerThreadPoolSize=cConf.getInt(Constants.Router.SERVER_WORKER_THREADS,Constants.Router.DEFAULT_SERVER_WORKER_THREADS);
  this.serverConnectionBacklog=cConf.getInt(Constants.Router.BACKLOG_CONNECTIONS,Constants.Router.DEFAULT_BACKLOG);
  this.clientBossThreadPoolSize=cConf.getInt(Constants.Router.CLIENT_BOSS_THREADS,Constants.Router.DEFAULT_CLIENT_BOSS_THREADS);
  this.clientWorkerThreadPoolSize=cConf.getInt(Constants.Router.CLIENT_WORKER_THREADS,Constants.Router.DEFAULT_CLIENT_WORKER_THREADS);
  this.hostname=hostname;
  this.forwards=Sets.newHashSet(cConf.getStrings(Constants.Router.FORWARD,Constants.Router.DEFAULT_FORWARD));
  Preconditions.checkState(!this.forwards.isEmpty(),""String_Node_Str"");
  LOG.info(""String_Node_Str"",this.forwards);
  this.serviceLookup=serviceLookup;
  this.securityEnabled=cConf.getBoolean(Constants.Security.CFG_SECURITY_ENABLED,false);
  this.realm=cConf.get(Constants.Security.CFG_REALM);
  this.tokenValidator=tokenValidator;
  this.accessTokenTransformer=accessTokenTransformer;
  this.discoveryServiceClient=discoveryServiceClient;
  this.configuration=cConf;
  this.sslEnabled=cConf.getBoolean(Constants.Security.ROUTER_SSL_ENABLED);
  if (isSSLEnabled()) {
    File keystore;
    try {
      keystore=new File(cConf.get(Constants.Security.ROUTER_SSL_KEYSTORE_PATH));
    }
 catch (    Exception e) {
      throw new RuntimeException(""String_Node_Str"" + cConf.get(Constants.Security.ROUTER_SSL_KEYSTORE_PATH));
    }
    this.sslHandlerFactory=new SSLHandlerFactory(keystore,cConf.get(Constants.Security.ROUTER_SSL_KEYSTORE_PASSWORD),cConf.get(Constants.Security.ROUTER_SSL_KEYPASSWORD));
  }
 else {
    this.sslHandlerFactory=null;
  }
}"
7511,"@Override protected void startUp(){
  CConfiguration cConf=CConfiguration.create();
  cConf.setBoolean(Constants.Security.SSL_ENABLED,true);
  URL certUrl=getClass().getClassLoader().getResource(""String_Node_Str"");
  Assert.assertNotNull(certUrl);
  Configuration configuration=new Configuration();
  configuration.set(Constants.Security.ROUTER_SSL_KEYPASSWORD,""String_Node_Str"");
  configuration.set(Constants.Security.ROUTER_SSL_KEYSTORE_PASSWORD,""String_Node_Str"");
  configuration.set(Constants.Security.ROUTER_SSL_KEYSTORE_TYPE,""String_Node_Str"");
  configuration.set(Constants.Security.ROUTER_SSL_KEYSTORE_PATH,certUrl.getPath());
  Injector injector=Guice.createInjector(new ConfigModule(),new IOModule(),new SecurityModules().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules());
  DiscoveryServiceClient discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  AccessTokenTransformer accessTokenTransformer=injector.getInstance(AccessTokenTransformer.class);
  cConf.set(Constants.Router.ADDRESS,hostname);
  cConf.setStrings(Constants.Router.FORWARD,forwards.toArray(new String[forwards.size()]));
  router=new NettyRouter(cConf,InetAddresses.forString(hostname),new RouterServiceLookup((DiscoveryServiceClient)discoveryService,new RouterPathLookup(new NoAuthenticator())),new SuccessTokenValidator(),accessTokenTransformer,discoveryServiceClient,configuration);
  router.startAndWait();
  for (  Map.Entry<Integer,String> entry : router.getServiceLookup().getServiceMap().entrySet()) {
    serviceMap.put(entry.getValue(),entry.getKey());
  }
}","@Override protected void startUp(){
  CConfiguration cConf=CConfiguration.create();
  cConf.setBoolean(Constants.Security.SSL_ENABLED,true);
  URL certUrl=getClass().getClassLoader().getResource(""String_Node_Str"");
  Assert.assertNotNull(certUrl);
  Injector injector=Guice.createInjector(new ConfigModule(),new IOModule(),new SecurityModules().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules());
  DiscoveryServiceClient discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  AccessTokenTransformer accessTokenTransformer=injector.getInstance(AccessTokenTransformer.class);
  cConf.set(Constants.Router.ADDRESS,hostname);
  cConf.setStrings(Constants.Router.FORWARD,forwards.toArray(new String[forwards.size()]));
  cConf.set(Constants.Security.ROUTER_SSL_KEYPASSWORD,""String_Node_Str"");
  cConf.set(Constants.Security.ROUTER_SSL_KEYSTORE_PASSWORD,""String_Node_Str"");
  cConf.set(Constants.Security.ROUTER_SSL_KEYSTORE_TYPE,""String_Node_Str"");
  cConf.set(Constants.Security.ROUTER_SSL_KEYSTORE_PATH,certUrl.getPath());
  router=new NettyRouter(cConf,InetAddresses.forString(hostname),new RouterServiceLookup((DiscoveryServiceClient)discoveryService,new RouterPathLookup(new NoAuthenticator())),new SuccessTokenValidator(),accessTokenTransformer,discoveryServiceClient);
  router.startAndWait();
  for (  Map.Entry<Integer,String> entry : router.getServiceLookup().getServiceMap().entrySet()) {
    serviceMap.put(entry.getValue(),entry.getKey());
  }
}"
7512,"private static Location createDeploymentJar(Class<?> clz) throws IOException {
  File tempFile=File.createTempFile(clz.getName(),""String_Node_Str"");
  Location tempJarLocation=new LocalLocationFactory().create(tempFile.getPath());
  ClassLoader remembered=Thread.currentThread().getContextClassLoader();
  Thread.currentThread().setContextClassLoader(clz.getClassLoader());
  try {
    ApplicationBundler bundler=new ApplicationBundler(ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
    bundler.createBundle(tempJarLocation,clz);
  }
  finally {
    Thread.currentThread().setContextClassLoader(remembered);
  }
  File outFile=File.createTempFile(clz.getName(),""String_Node_Str"");
  Location outJarLocation=new LocalLocationFactory().create(outFile.getPath());
  JarOutputStream jarOutput=new JarOutputStream(outJarLocation.getOutputStream());
  try {
    JarInputStream jarInput=new JarInputStream(tempJarLocation.getInputStream());
    try {
      JarEntry jarEntry=jarInput.getNextJarEntry();
      while (jarEntry != null) {
        boolean isDir=jarEntry.isDirectory();
        String entryName=jarEntry.getName();
        if (!entryName.equals(""String_Node_Str"")) {
          if (entryName.startsWith(""String_Node_Str"")) {
            jarEntry=new JarEntry(entryName.substring(""String_Node_Str"".length()));
          }
 else {
            jarEntry=new JarEntry(entryName);
          }
          jarOutput.putNextEntry(jarEntry);
          if (!isDir) {
            ByteStreams.copy(jarInput,jarOutput);
          }
        }
        jarEntry=jarInput.getNextJarEntry();
      }
    }
  finally {
      jarInput.close();
    }
  }
  finally {
    jarOutput.close();
  }
  return outJarLocation;
}","private static Location createDeploymentJar(Class<?> clz,Location destination) throws IOException {
  Location tempBundle=destination.getTempFile(""String_Node_Str"");
  try {
    ClassLoader remembered=Thread.currentThread().getContextClassLoader();
    Thread.currentThread().setContextClassLoader(clz.getClassLoader());
    try {
      ApplicationBundler bundler=new ApplicationBundler(ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
      bundler.createBundle(tempBundle,clz);
    }
  finally {
      Thread.currentThread().setContextClassLoader(remembered);
    }
    JarOutputStream jarOutput=new JarOutputStream(destination.getOutputStream());
    try {
      JarInputStream jarInput=new JarInputStream(tempBundle.getInputStream());
      try {
        Set<String> seen=Sets.newHashSet();
        JarEntry jarEntry=jarInput.getNextJarEntry();
        while (jarEntry != null) {
          boolean isDir=jarEntry.isDirectory();
          String entryName=jarEntry.getName();
          if (!entryName.equals(""String_Node_Str"")) {
            if (entryName.startsWith(""String_Node_Str"")) {
              jarEntry=new JarEntry(entryName.substring(""String_Node_Str"".length()));
            }
 else {
              jarEntry=new JarEntry(entryName);
            }
            if (seen.add(jarEntry.getName())) {
              jarOutput.putNextEntry(jarEntry);
              if (!isDir) {
                ByteStreams.copy(jarInput,jarOutput);
              }
            }
          }
          jarEntry=jarInput.getNextJarEntry();
        }
      }
  finally {
        jarInput.close();
      }
    }
  finally {
      jarOutput.close();
    }
    return destination;
  }
  finally {
    tempBundle.delete();
  }
}"
7513,"private void addModule(String moduleName,Class<?> typeClass) throws DatasetManagementException {
  Location tempJarPath;
  try {
    tempJarPath=createDeploymentJar(typeClass);
    try {
      client.addModule(moduleName,typeClass.getName(),tempJarPath);
    }
  finally {
      tempJarPath.delete();
    }
  }
 catch (  IOException e) {
    String msg=String.format(""String_Node_Str"",moduleName,typeClass.getName());
    LOG.error(msg,e);
    throw new DatasetManagementException(msg,e);
  }
}","private void addModule(String moduleName,Class<?> typeClass) throws DatasetManagementException {
  try {
    File tempFile=File.createTempFile(typeClass.getName(),""String_Node_Str"");
    try {
      Location tempJarPath=createDeploymentJar(typeClass,new LocalLocationFactory().create(tempFile.toURI()));
      client.addModule(moduleName,typeClass.getName(),tempJarPath);
    }
  finally {
      tempFile.delete();
    }
  }
 catch (  IOException e) {
    String msg=String.format(""String_Node_Str"",moduleName,typeClass.getName());
    LOG.error(msg,e);
    throw new DatasetManagementException(msg,e);
  }
}"
7514,"private static void setupClasspath() throws IOException {
  Set<String> bootstrapClassPaths=ExploreServiceUtils.getBoostrapClasses();
  Set<File> hBaseTableDeps=ExploreServiceUtils.traceDependencies(new HBaseTableUtilFactory().get().getClass().getCanonicalName(),bootstrapClassPaths,null);
  Set<File> orderedDependencies=new LinkedHashSet<File>();
  orderedDependencies.addAll(hBaseTableDeps);
  orderedDependencies.addAll(ExploreServiceUtils.traceDependencies(RemoteDatasetFramework.class.getCanonicalName(),bootstrapClassPaths,null));
  orderedDependencies.addAll(ExploreServiceUtils.traceDependencies(DatasetStorageHandler.class.getCanonicalName(),bootstrapClassPaths,null));
  ImmutableList.Builder<String> builder=ImmutableList.builder();
  ImmutableList.Builder<String> builder2=ImmutableList.builder();
  for (  File dep : orderedDependencies) {
    builder.add(""String_Node_Str"" + dep.getAbsolutePath());
    builder2.add(dep.getAbsolutePath());
  }
  List<String> orderedDependenciesStr=builder.build();
  System.setProperty(HiveConf.ConfVars.HIVEAUXJARS.toString(),Joiner.on(',').join(orderedDependenciesStr));
  LOG.debug(""String_Node_Str"",HiveConf.ConfVars.HIVEAUXJARS.toString(),System.getProperty(HiveConf.ConfVars.HIVEAUXJARS.toString()));
  LocalMapreduceClasspathSetter classpathSetter=new LocalMapreduceClasspathSetter(new HiveConf(),System.getProperty(""String_Node_Str""),builder2.build());
  for (  File jar : hBaseTableDeps) {
    classpathSetter.accept(jar.getAbsolutePath());
  }
  classpathSetter.setupClasspathScript();
}","private static void setupClasspath() throws IOException {
  Set<String> bootstrapClassPaths=ExploreServiceUtils.getBoostrapClasses();
  Set<File> hBaseTableDeps=ExploreServiceUtils.traceDependencies(new HBaseTableUtilFactory().get().getClass().getCanonicalName(),bootstrapClassPaths,null);
  Set<File> orderedDependencies=new LinkedHashSet<File>();
  orderedDependencies.addAll(hBaseTableDeps);
  orderedDependencies.addAll(ExploreServiceUtils.traceDependencies(RemoteDatasetFramework.class.getCanonicalName(),bootstrapClassPaths,null));
  orderedDependencies.addAll(ExploreServiceUtils.traceDependencies(DatasetStorageHandler.class.getCanonicalName(),bootstrapClassPaths,null));
  ImmutableList.Builder<String> builder=ImmutableList.builder();
  for (  File dep : orderedDependencies) {
    builder.add(""String_Node_Str"" + dep.getAbsolutePath());
  }
  List<String> orderedDependenciesStr=builder.build();
  System.setProperty(HiveConf.ConfVars.HIVEAUXJARS.toString(),Joiner.on(',').join(orderedDependenciesStr));
  LOG.debug(""String_Node_Str"",HiveConf.ConfVars.HIVEAUXJARS.toString(),System.getProperty(HiveConf.ConfVars.HIVEAUXJARS.toString()));
  LocalMapreduceClasspathSetter classpathSetter=new LocalMapreduceClasspathSetter(new HiveConf(),System.getProperty(""String_Node_Str""),orderedDependenciesStr);
  for (  File jar : hBaseTableDeps) {
    classpathSetter.accept(jar.getAbsolutePath());
  }
  classpathSetter.setupClasspathScript();
}"
7515,"@Override protected void configure(){
  try {
    setupClasspath();
    System.setProperty(HiveConf.ConfVars.LOCALSCRATCHDIR.toString(),new File(HiveConf.ConfVars.LOCALSCRATCHDIR.defaultVal).getAbsolutePath());
    LOG.info(""String_Node_Str"",HiveConf.ConfVars.LOCALSCRATCHDIR.toString(),System.getProperty(HiveConf.ConfVars.LOCALSCRATCHDIR.toString()));
    System.setProperty(""String_Node_Str"",""String_Node_Str"");
    System.setProperty(""String_Node_Str"",""String_Node_Str"");
    System.setProperty(""String_Node_Str"",""String_Node_Str"");
    System.setProperty(""String_Node_Str"",""String_Node_Str"");
    System.setProperty(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,""String_Node_Str"");
    File previewDir=Files.createTempDir();
    LOG.info(""String_Node_Str"",previewDir.getAbsolutePath());
    bind(File.class).annotatedWith(Names.named(Constants.Explore.PREVIEWS_DIR_NAME)).toInstance(previewDir);
  }
 catch (  Throwable e) {
    throw Throwables.propagate(e);
  }
}","@Override protected void configure(){
  try {
    setupClasspath();
    System.setProperty(HiveConf.ConfVars.LOCALSCRATCHDIR.toString(),new File(HiveConf.ConfVars.LOCALSCRATCHDIR.defaultVal).getAbsolutePath());
    LOG.info(""String_Node_Str"",HiveConf.ConfVars.LOCALSCRATCHDIR.toString(),System.getProperty(HiveConf.ConfVars.LOCALSCRATCHDIR.toString()));
    System.setProperty(""String_Node_Str"",""String_Node_Str"");
    System.setProperty(""String_Node_Str"",""String_Node_Str"");
    System.setProperty(""String_Node_Str"",""String_Node_Str"");
    File previewDir=Files.createTempDir();
    LOG.info(""String_Node_Str"",previewDir.getAbsolutePath());
    bind(File.class).annotatedWith(Names.named(Constants.Explore.PREVIEWS_DIR_NAME)).toInstance(previewDir);
  }
 catch (  Throwable e) {
    throw Throwables.propagate(e);
  }
}"
7516,"public void setupClasspathScript() throws IOException {
  if (hbaseProtocolJarPaths.isEmpty()) {
    LOG.info(""String_Node_Str"");
    return;
  }
  File exploreHadoopBin=new File(directory,""String_Node_Str"");
  LOG.info(""String_Node_Str"",hbaseProtocolJarPaths);
  String hadoopBin=hiveConf.get(HiveConf.ConfVars.HADOOPBIN.toString());
  StringBuilder fileBuilder=new StringBuilder();
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"").append(File.pathSeparatorChar).append(""String_Node_Str"").append(""String_Node_Str"").append(Joiner.on(':').join(hiveAuxJars)).append(Joiner.on(' ').join(hbaseProtocolJarPaths)).append(')').append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"").append(hadoopBin).append(""String_Node_Str"");
  Files.write(fileBuilder.toString(),exploreHadoopBin,Charsets.UTF_8);
  if (!exploreHadoopBin.setExecutable(true,false)) {
    throw new RuntimeException(""String_Node_Str"" + exploreHadoopBin.getAbsolutePath());
  }
  LOG.info(""String_Node_Str"",exploreHadoopBin.getAbsolutePath());
  System.setProperty(HiveConf.ConfVars.HADOOPBIN.toString(),exploreHadoopBin.getAbsolutePath());
}","public void setupClasspathScript() throws IOException {
  if (hbaseProtocolJarPaths.isEmpty()) {
    LOG.info(""String_Node_Str"");
    return;
  }
  File exploreHadoopBin=new File(directory,""String_Node_Str"");
  LOG.info(""String_Node_Str"",hbaseProtocolJarPaths);
  String hadoopBin=hiveConf.get(HiveConf.ConfVars.HADOOPBIN.toString());
  StringBuilder fileBuilder=new StringBuilder();
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"").append(File.pathSeparatorChar).append(""String_Node_Str"").append(Joiner.on(' ').join(hiveAuxJars)).append(""String_Node_Str"").append(Joiner.on(' ').join(hbaseProtocolJarPaths)).append(')').append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"");
  fileBuilder.append(""String_Node_Str"").append(hadoopBin).append(""String_Node_Str"");
  Files.write(fileBuilder.toString(),exploreHadoopBin,Charsets.UTF_8);
  if (!exploreHadoopBin.setExecutable(true,false)) {
    throw new RuntimeException(""String_Node_Str"" + exploreHadoopBin.getAbsolutePath());
  }
  LOG.info(""String_Node_Str"",exploreHadoopBin.getAbsolutePath());
  System.setProperty(HiveConf.ConfVars.HADOOPBIN.toString(),exploreHadoopBin.getAbsolutePath());
}"
7517,"protected HiveConf getHiveConf(){
  HiveConf hiveConf=new HiveConf();
  hiveConf.setBoolean(""String_Node_Str"",true);
  hiveConf.setBoolean(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,true);
  return hiveConf;
}","protected HiveConf getHiveConf(){
  return new HiveConf();
}"
7518,"@Test public void testClasspathSetupMulti() throws Exception {
  System.clearProperty(HiveConf.ConfVars.HADOOPBIN.toString());
  List<String> inputURLs=Lists.newArrayList();
  inputURLs.add(""String_Node_Str"");
  inputURLs.add(""String_Node_Str"");
  inputURLs.add(""String_Node_Str"" + ""String_Node_Str"");
  inputURLs.add(""String_Node_Str"");
  inputURLs.add(""String_Node_Str"");
  inputURLs.add(""String_Node_Str"");
  HiveConf hiveConf=new HiveConf();
  LocalMapreduceClasspathSetter classpathSetter=new LocalMapreduceClasspathSetter(hiveConf,TEMP_FOLDER.newFolder().getAbsolutePath(),ImmutableList.<String>of());
  for (  String url : inputURLs) {
    classpathSetter.accept(url);
  }
  Assert.assertEquals(ImmutableList.of(""String_Node_Str"",""String_Node_Str"" + ""String_Node_Str"",""String_Node_Str"" + ""String_Node_Str""),ImmutableList.copyOf(classpathSetter.getHbaseProtocolJarPaths()));
  classpathSetter.setupClasspathScript();
  String newHadoopBin=new HiveConf().get(HiveConf.ConfVars.HADOOPBIN.toString());
  Assert.assertEquals(generatedHadoopBinMulti,Joiner.on('\n').join(Files.readLines(new File(newHadoopBin),Charsets.UTF_8)));
  Assert.assertTrue(new File(newHadoopBin).canExecute());
  System.clearProperty(HiveConf.ConfVars.HADOOPBIN.toString());
}","@Test public void testClasspathSetupMulti() throws Exception {
  System.clearProperty(HiveConf.ConfVars.HADOOPBIN.toString());
  List<String> inputURLs=Lists.newArrayList();
  inputURLs.add(""String_Node_Str"");
  inputURLs.add(""String_Node_Str"");
  inputURLs.add(""String_Node_Str"" + ""String_Node_Str"");
  inputURLs.add(""String_Node_Str"");
  inputURLs.add(""String_Node_Str"");
  inputURLs.add(""String_Node_Str"");
  List<String> auxJarsURLs=Lists.newArrayList();
  auxJarsURLs.add(""String_Node_Str"");
  auxJarsURLs.add(""String_Node_Str"");
  HiveConf hiveConf=new HiveConf();
  LocalMapreduceClasspathSetter classpathSetter=new LocalMapreduceClasspathSetter(hiveConf,TEMP_FOLDER.newFolder().getAbsolutePath(),auxJarsURLs);
  for (  String url : inputURLs) {
    classpathSetter.accept(url);
  }
  Assert.assertEquals(ImmutableList.of(""String_Node_Str"",""String_Node_Str"" + ""String_Node_Str"",""String_Node_Str"" + ""String_Node_Str""),ImmutableList.copyOf(classpathSetter.getHbaseProtocolJarPaths()));
  classpathSetter.setupClasspathScript();
  String newHadoopBin=new HiveConf().get(HiveConf.ConfVars.HADOOPBIN.toString());
  Assert.assertEquals(generatedHadoopBinMulti,Joiner.on('\n').join(Files.readLines(new File(newHadoopBin),Charsets.UTF_8)));
  Assert.assertTrue(new File(newHadoopBin).canExecute());
  System.clearProperty(HiveConf.ConfVars.HADOOPBIN.toString());
}"
7519,"@Test public void testClasspathSetupSingle() throws Exception {
  System.clearProperty(HiveConf.ConfVars.HADOOPBIN.toString());
  List<String> inputURLs=Lists.newArrayList();
  inputURLs.add(""String_Node_Str"");
  inputURLs.add(""String_Node_Str"");
  inputURLs.add(""String_Node_Str"");
  inputURLs.add(""String_Node_Str"");
  HiveConf hiveConf=new HiveConf();
  LocalMapreduceClasspathSetter classpathSetter=new LocalMapreduceClasspathSetter(hiveConf,TEMP_FOLDER.newFolder().getAbsolutePath(),ImmutableList.<String>of());
  for (  String url : inputURLs) {
    classpathSetter.accept(url);
  }
  Assert.assertEquals(ImmutableList.of(""String_Node_Str""),ImmutableList.copyOf(classpathSetter.getHbaseProtocolJarPaths()));
  classpathSetter.setupClasspathScript();
  String newHadoopBin=new HiveConf().get(HiveConf.ConfVars.HADOOPBIN.toString());
  Assert.assertEquals(generatedHadoopBinSingle,Joiner.on('\n').join(Files.readLines(new File(newHadoopBin),Charsets.UTF_8)));
  Assert.assertTrue(new File(newHadoopBin).canExecute());
  System.clearProperty(HiveConf.ConfVars.HADOOPBIN.toString());
}","@Test public void testClasspathSetupSingle() throws Exception {
  System.clearProperty(HiveConf.ConfVars.HADOOPBIN.toString());
  List<String> inputURLs=Lists.newArrayList();
  inputURLs.add(""String_Node_Str"");
  inputURLs.add(""String_Node_Str"");
  inputURLs.add(""String_Node_Str"");
  inputURLs.add(""String_Node_Str"");
  List<String> auxJarsURLs=Lists.newArrayList();
  auxJarsURLs.add(""String_Node_Str"");
  auxJarsURLs.add(""String_Node_Str"");
  HiveConf hiveConf=new HiveConf();
  LocalMapreduceClasspathSetter classpathSetter=new LocalMapreduceClasspathSetter(hiveConf,TEMP_FOLDER.newFolder().getAbsolutePath(),auxJarsURLs);
  for (  String url : inputURLs) {
    classpathSetter.accept(url);
  }
  Assert.assertEquals(ImmutableList.of(""String_Node_Str""),ImmutableList.copyOf(classpathSetter.getHbaseProtocolJarPaths()));
  classpathSetter.setupClasspathScript();
  String newHadoopBin=new HiveConf().get(HiveConf.ConfVars.HADOOPBIN.toString());
  Assert.assertEquals(generatedHadoopBinSingle,Joiner.on('\n').join(Files.readLines(new File(newHadoopBin),Charsets.UTF_8)));
  Assert.assertTrue(new File(newHadoopBin).canExecute());
  System.clearProperty(HiveConf.ConfVars.HADOOPBIN.toString());
}"
7520,"/** 
 * Prepare the specs of the twill application for the Explore twill runnable. Add jars needed by the Explore module in the classpath of the containers, and add conf files (hive_site.xml, etc) as resources available for the Explore twill runnable.
 */
private TwillPreparer prepareExploreContainer(TwillPreparer preparer){
  if (!isExploreEnabled) {
    return preparer;
  }
  try {
    Set<File> jars=ExploreServiceUtils.traceExploreDependencies();
    for (    File jarFile : jars) {
      LOG.trace(""String_Node_Str"",jarFile.getName());
      preparer=preparer.withClassPaths(jarFile.getName());
    }
  }
 catch (  IOException e) {
    throw new RuntimeException(""String_Node_Str"",e);
  }
  String hiveConfFiles=System.getProperty(Constants.Explore.EXPLORE_CONF_FILES);
  LOG.debug(""String_Node_Str"",hiveConfFiles);
  if (hiveConfFiles == null) {
    throw new RuntimeException(""String_Node_Str"" + Constants.Explore.EXPLORE_CONF_FILES + ""String_Node_Str"");
  }
  Iterable<File> hiveConfFilesFiles=ExploreServiceUtils.getClassPathJarsFiles(hiveConfFiles);
  for (  File file : hiveConfFilesFiles) {
    if (file.getName().matches(""String_Node_Str"")) {
      preparer=preparer.withResources(file.toURI());
    }
  }
  return preparer;
}","/** 
 * Prepare the specs of the twill application for the Explore twill runnable. Add jars needed by the Explore module in the classpath of the containers, and add conf files (hive_site.xml, etc) as resources available for the Explore twill runnable.
 */
private TwillPreparer prepareExploreContainer(TwillPreparer preparer){
  if (!isExploreEnabled) {
    return preparer;
  }
  try {
    Set<File> jars=ExploreServiceUtils.traceExploreDependencies();
    for (    File jarFile : jars) {
      LOG.trace(""String_Node_Str"",jarFile.getName());
      preparer=preparer.withClassPaths(jarFile.getName());
    }
  }
 catch (  IOException e) {
    throw new RuntimeException(""String_Node_Str"",e);
  }
  String hiveConfFiles=System.getProperty(Constants.Explore.EXPLORE_CONF_FILES);
  LOG.debug(""String_Node_Str"",hiveConfFiles);
  if (hiveConfFiles == null) {
    throw new RuntimeException(""String_Node_Str"" + Constants.Explore.EXPLORE_CONF_FILES + ""String_Node_Str"");
  }
  Iterable<File> hiveConfFilesFiles=ExploreServiceUtils.getClassPathJarsFiles(hiveConfFiles);
  for (  File file : hiveConfFilesFiles) {
    if (file.getName().matches(""String_Node_Str"")) {
      preparer=preparer.withResources(ExploreServiceUtils.hijackHiveConfFile(file).toURI());
    }
  }
  return preparer;
}"
7521,"private TwillSpecification.Builder.RunnableSetter addExploreService(TwillSpecification.Builder.MoreRunnable builder){
  int instances=instanceCountMap.get(Constants.Service.EXPLORE_HTTP_USER_SERVICE);
  ResourceSpecification resourceSpec=ResourceSpecification.Builder.with().setVirtualCores(cConf.getInt(Constants.Explore.CONTAINER_VIRTUAL_CORES,1)).setMemory(cConf.getInt(Constants.Explore.CONTAINER_MEMORY_MB,512),ResourceSpecification.SizeUnit.MEGA).setInstances(instances).build();
  TwillSpecification.Builder.MoreFile twillSpecs=builder.add(new ExploreServiceTwillRunnable(Constants.Service.EXPLORE_HTTP_USER_SERVICE,""String_Node_Str"",""String_Node_Str""),resourceSpec).withLocalFiles().add(""String_Node_Str"",cConfFile.toURI()).add(""String_Node_Str"",hConfFile.toURI());
  try {
    Set<File> jars=ExploreServiceUtils.traceExploreDependencies();
    for (    File jarFile : jars) {
      twillSpecs=twillSpecs.add(jarFile.getName(),jarFile);
    }
  }
 catch (  IOException e) {
    throw new RuntimeException(""String_Node_Str"",e);
  }
  return twillSpecs.apply();
}","private TwillSpecification.Builder.RunnableSetter addExploreService(TwillSpecification.Builder.MoreRunnable builder){
  int instances=instanceCountMap.get(Constants.Service.EXPLORE_HTTP_USER_SERVICE);
  ResourceSpecification resourceSpec=ResourceSpecification.Builder.with().setVirtualCores(cConf.getInt(Constants.Explore.CONTAINER_VIRTUAL_CORES,1)).setMemory(cConf.getInt(Constants.Explore.CONTAINER_MEMORY_MB,1024),ResourceSpecification.SizeUnit.MEGA).setInstances(instances).build();
  TwillSpecification.Builder.MoreFile twillSpecs=builder.add(new ExploreServiceTwillRunnable(Constants.Service.EXPLORE_HTTP_USER_SERVICE,""String_Node_Str"",""String_Node_Str""),resourceSpec).withLocalFiles().add(""String_Node_Str"",cConfFile.toURI()).add(""String_Node_Str"",hConfFile.toURI());
  try {
    Set<File> jars=ExploreServiceUtils.traceExploreDependencies();
    for (    File jarFile : jars) {
      twillSpecs=twillSpecs.add(jarFile.getName(),jarFile);
    }
  }
 catch (  IOException e) {
    throw new RuntimeException(""String_Node_Str"",e);
  }
  return twillSpecs.apply();
}"
7522,"private static Location createDeploymentJar(Class<?> clz) throws IOException {
  File tempFile=File.createTempFile(clz.getName(),""String_Node_Str"");
  Location tempJarLocation=new LocalLocationFactory().create(tempFile.getPath());
  ClassLoader remembered=Thread.currentThread().getContextClassLoader();
  Thread.currentThread().setContextClassLoader(clz.getClassLoader());
  try {
    ApplicationBundler bundler=new ApplicationBundler(ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
    bundler.createBundle(tempJarLocation,clz);
  }
  finally {
    Thread.currentThread().setContextClassLoader(remembered);
  }
  File outFile=File.createTempFile(clz.getName(),""String_Node_Str"");
  Location outJarLocation=new LocalLocationFactory().create(outFile.getPath());
  JarOutputStream jarOutput=new JarOutputStream(outJarLocation.getOutputStream());
  try {
    JarInputStream jarInput=new JarInputStream(tempJarLocation.getInputStream());
    try {
      JarEntry jarEntry=jarInput.getNextJarEntry();
      while (jarEntry != null) {
        boolean isDir=jarEntry.isDirectory();
        String entryName=jarEntry.getName();
        if (!entryName.equals(""String_Node_Str"")) {
          if (entryName.startsWith(""String_Node_Str"")) {
            jarEntry=new JarEntry(entryName.substring(""String_Node_Str"".length()));
          }
 else {
            jarEntry=new JarEntry(entryName);
          }
          jarOutput.putNextEntry(jarEntry);
          if (!isDir) {
            ByteStreams.copy(jarInput,jarOutput);
          }
        }
        jarEntry=jarInput.getNextJarEntry();
      }
    }
  finally {
      jarInput.close();
    }
  }
  finally {
    jarOutput.close();
  }
  return outJarLocation;
}","private static Location createDeploymentJar(Class<?> clz,Location destination) throws IOException {
  Location tempBundle=destination.getTempFile(""String_Node_Str"");
  try {
    ClassLoader remembered=Thread.currentThread().getContextClassLoader();
    Thread.currentThread().setContextClassLoader(clz.getClassLoader());
    try {
      ApplicationBundler bundler=new ApplicationBundler(ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
      bundler.createBundle(tempBundle,clz);
    }
  finally {
      Thread.currentThread().setContextClassLoader(remembered);
    }
    JarOutputStream jarOutput=new JarOutputStream(destination.getOutputStream());
    try {
      JarInputStream jarInput=new JarInputStream(tempBundle.getInputStream());
      try {
        Set<String> seen=Sets.newHashSet();
        JarEntry jarEntry=jarInput.getNextJarEntry();
        while (jarEntry != null) {
          boolean isDir=jarEntry.isDirectory();
          String entryName=jarEntry.getName();
          if (!entryName.equals(""String_Node_Str"")) {
            if (entryName.startsWith(""String_Node_Str"")) {
              jarEntry=new JarEntry(entryName.substring(""String_Node_Str"".length()));
            }
 else {
              jarEntry=new JarEntry(entryName);
            }
            if (seen.add(jarEntry.getName())) {
              jarOutput.putNextEntry(jarEntry);
              if (!isDir) {
                ByteStreams.copy(jarInput,jarOutput);
              }
            }
          }
          jarEntry=jarInput.getNextJarEntry();
        }
      }
  finally {
        jarInput.close();
      }
    }
  finally {
      jarOutput.close();
    }
    return destination;
  }
  finally {
    tempBundle.delete();
  }
}"
7523,"private void addModule(String moduleName,Class<?> typeClass) throws DatasetManagementException {
  Location tempJarPath;
  try {
    tempJarPath=createDeploymentJar(typeClass);
    try {
      client.addModule(moduleName,typeClass.getName(),tempJarPath);
    }
  finally {
      tempJarPath.delete();
    }
  }
 catch (  IOException e) {
    String msg=String.format(""String_Node_Str"",moduleName,typeClass.getName());
    LOG.error(msg,e);
    throw new DatasetManagementException(msg,e);
  }
}","private void addModule(String moduleName,Class<?> typeClass) throws DatasetManagementException {
  try {
    File tempFile=File.createTempFile(typeClass.getName(),""String_Node_Str"");
    try {
      Location tempJarPath=createDeploymentJar(typeClass,new LocalLocationFactory().create(tempFile.toURI()));
      client.addModule(moduleName,typeClass.getName(),tempJarPath);
    }
  finally {
      tempFile.delete();
    }
  }
 catch (  IOException e) {
    String msg=String.format(""String_Node_Str"",moduleName,typeClass.getName());
    LOG.error(msg,e);
    throw new DatasetManagementException(msg,e);
  }
}"
7524,"private void createDatasetInstance(DatasetInstanceConfiguration creationProperties,String name,HttpResponder responder,String operation){
  DatasetTypeMeta typeMeta=implManager.getTypeInfo(creationProperties.getTypeName());
  if (typeMeta == null) {
    String message=String.format(""String_Node_Str"",operation,name,creationProperties.getTypeName());
    LOG.warn(message);
    responder.sendError(HttpResponseStatus.NOT_FOUND,message);
    return;
  }
  DatasetSpecification spec;
  try {
    spec=opExecutorClient.create(name,typeMeta,DatasetProperties.builder().addAll(creationProperties.getProperties()).build());
  }
 catch (  Exception e) {
    String msg=String.format(""String_Node_Str"",operation,name,creationProperties.getTypeName(),e.getMessage());
    LOG.error(msg,e);
    throw new RuntimeException(msg,e);
  }
  instanceManager.add(spec);
}","private boolean createDatasetInstance(DatasetInstanceConfiguration creationProperties,String name,HttpResponder responder,String operation){
  DatasetTypeMeta typeMeta=implManager.getTypeInfo(creationProperties.getTypeName());
  if (typeMeta == null) {
    String message=String.format(""String_Node_Str"",operation,name,creationProperties.getTypeName());
    LOG.warn(message);
    responder.sendError(HttpResponseStatus.NOT_FOUND,message);
    return false;
  }
  DatasetSpecification spec;
  try {
    spec=opExecutorClient.create(name,typeMeta,DatasetProperties.builder().addAll(creationProperties.getProperties()).build());
  }
 catch (  Exception e) {
    String msg=String.format(""String_Node_Str"",operation,name,creationProperties.getTypeName(),e.getMessage());
    LOG.error(msg,e);
    throw new RuntimeException(msg,e);
  }
  instanceManager.add(spec);
  return true;
}"
7525,"/** 
 * Updates an existing Dataset specification properties    {@link DatasetInstanceConfiguration}is constructed based on request and the Dataset instance is updated.
 */
@PUT @Path(""String_Node_Str"") public void update(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String name){
  Reader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()));
  DatasetInstanceConfiguration creationProperties=GSON.fromJson(reader,DatasetInstanceConfiguration.class);
  LOG.info(""String_Node_Str"",name,creationProperties.getTypeName(),creationProperties.getProperties());
  DatasetSpecification existing=instanceManager.get(name);
  if (existing == null) {
    responder.sendError(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",name));
    return;
  }
  if (!existing.getType().equals(creationProperties.getTypeName())) {
    String message=String.format(""String_Node_Str"",name,existing.getType());
    LOG.warn(message);
    responder.sendError(HttpResponseStatus.CONFLICT,message);
    return;
  }
  createDatasetInstance(creationProperties,name,responder,""String_Node_Str"");
  try {
    datasetExploreFacade.disableExplore(name);
    datasetExploreFacade.enableExplore(name);
  }
 catch (  Exception e) {
    String msg=String.format(""String_Node_Str"",name,creationProperties.getProperties(),e.getMessage());
    LOG.error(msg,e);
  }
  executeAdmin(request,responder,name,""String_Node_Str"");
}","/** 
 * Updates an existing Dataset specification properties    {@link DatasetInstanceConfiguration}is constructed based on request and the Dataset instance is updated.
 */
@PUT @Path(""String_Node_Str"") public void update(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String name){
  Reader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()));
  DatasetInstanceConfiguration creationProperties=GSON.fromJson(reader,DatasetInstanceConfiguration.class);
  LOG.info(""String_Node_Str"",name,creationProperties.getTypeName(),creationProperties.getProperties());
  DatasetSpecification existing=instanceManager.get(name);
  if (existing == null) {
    responder.sendError(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",name));
    return;
  }
  if (!existing.getType().equals(creationProperties.getTypeName())) {
    String message=String.format(""String_Node_Str"",name,existing.getType());
    LOG.warn(message);
    responder.sendError(HttpResponseStatus.CONFLICT,message);
    return;
  }
  if (!createDatasetInstance(creationProperties,name,responder,""String_Node_Str"")) {
    return;
  }
  try {
    datasetExploreFacade.disableExplore(name);
    datasetExploreFacade.enableExplore(name);
  }
 catch (  Exception e) {
    String msg=String.format(""String_Node_Str"",name,creationProperties.getProperties(),e.getMessage());
    LOG.error(msg,e);
  }
  executeAdmin(request,responder,name,""String_Node_Str"");
}"
7526,"@Override protected InetSocketAddress getExploreServiceAddress(){
  EndpointStrategy endpointStrategy=this.endpointStrategySupplier.get();
  if (endpointStrategy == null || endpointStrategy.pick() == null) {
    String message=String.format(""String_Node_Str"",Service.EXPLORE_HTTP_USER_SERVICE);
    LOG.error(message);
    throw new RuntimeException(message);
  }
  return endpointStrategy.pick().getSocketAddress();
}","@Override protected InetSocketAddress getExploreServiceAddress(){
  EndpointStrategy endpointStrategy=this.endpointStrategySupplier.get();
  if (endpointStrategy == null || endpointStrategy.pick() == null) {
    String message=String.format(""String_Node_Str"",Service.EXPLORE_HTTP_USER_SERVICE);
    LOG.debug(message);
    throw new RuntimeException(message);
  }
  return endpointStrategy.pick().getSocketAddress();
}"
7527,"/** 
 * Receives an input containing application specification and location and verifies both.
 * @param input An instance of {@link co.cask.cdap.internal.app.deploy.pipeline.ApplicationSpecLocation}
 */
@Override public void process(ApplicationSpecLocation input) throws Exception {
  ApplicationSpecification specification=input.getSpecification();
  File unpackedLocation=Files.createTempDir();
  try {
    BundleJarUtil.unpackProgramJar(input.getArchive(),unpackedLocation);
    ProgramClassLoader classLoader=ClassLoaders.newProgramClassLoader(unpackedLocation,ApiResourceListHolder.getResourceList(),this.getClass().getClassLoader());
    for (    Map.Entry<String,String> moduleEntry : specification.getDatasetModules().entrySet()) {
      @SuppressWarnings(""String_Node_Str"") Class<?> clazz=classLoader.loadClass(moduleEntry.getValue());
      String moduleName=moduleEntry.getKey();
      try {
        if (DatasetModule.class.isAssignableFrom(clazz)) {
          datasetFramework.addModule(moduleName,(DatasetModule)clazz.newInstance());
        }
 else         if (Dataset.class.isAssignableFrom(clazz) && !datasetFramework.hasType(clazz.getName())) {
          datasetFramework.addModule(moduleName,new SingleTypeModule((Class<Dataset>)clazz));
        }
 else {
          String msg=String.format(""String_Node_Str"",clazz.getName());
          throw new IllegalArgumentException(msg);
        }
      }
 catch (      ModuleConflictException e) {
        LOG.info(""String_Node_Str"" + moduleName + ""String_Node_Str"");
      }
    }
  }
 catch (  Exception e) {
    LOG.info(""String_Node_Str"",e);
  }
 finally {
    try {
      DirUtils.deleteDirectoryContents(unpackedLocation);
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",unpackedLocation,e);
    }
  }
  emit(input);
}","/** 
 * Receives an input containing application specification and location and verifies both.
 * @param input An instance of {@link co.cask.cdap.internal.app.deploy.pipeline.ApplicationSpecLocation}
 */
@Override public void process(ApplicationSpecLocation input) throws Exception {
  ApplicationSpecification specification=input.getSpecification();
  File unpackedLocation=Files.createTempDir();
  try {
    BundleJarUtil.unpackProgramJar(input.getArchive(),unpackedLocation);
    ProgramClassLoader classLoader=ClassLoaders.newProgramClassLoader(unpackedLocation,ApiResourceListHolder.getResourceList(),this.getClass().getClassLoader());
    for (    Map.Entry<String,String> moduleEntry : specification.getDatasetModules().entrySet()) {
      @SuppressWarnings(""String_Node_Str"") Class<?> clazz=classLoader.loadClass(moduleEntry.getValue());
      String moduleName=moduleEntry.getKey();
      try {
        if (DatasetModule.class.isAssignableFrom(clazz)) {
          datasetFramework.addModule(moduleName,(DatasetModule)clazz.newInstance());
        }
 else         if (Dataset.class.isAssignableFrom(clazz)) {
          if (!datasetFramework.hasType(clazz.getName())) {
            datasetFramework.addModule(moduleName,new SingleTypeModule((Class<Dataset>)clazz));
          }
        }
 else {
          String msg=String.format(""String_Node_Str"",clazz.getName());
          throw new IllegalArgumentException(msg);
        }
      }
 catch (      ModuleConflictException e) {
        LOG.info(""String_Node_Str"" + moduleName + ""String_Node_Str"");
      }
    }
  }
 catch (  Exception e) {
    LOG.info(""String_Node_Str"",e);
  }
 finally {
    try {
      DirUtils.deleteDirectoryContents(unpackedLocation);
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",unpackedLocation,e);
    }
  }
  emit(input);
}"
7528,"@BeforeClass public static void setUpClass() throws Throwable {
  if (singleNodeMain != null) {
    try {
      CConfiguration cConf=CConfiguration.create();
      cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
      singleNodeMain=SingleNodeMain.createSingleNodeMain(true,null,cConf,new Configuration());
      singleNodeMain.startUp();
    }
 catch (    Throwable e) {
      LOG.error(""String_Node_Str"",e);
      if (singleNodeMain != null) {
        singleNodeMain.shutDown();
      }
      throw e;
    }
  }
}","@BeforeClass public static void setUpClass() throws Throwable {
  testStackIndex++;
  if (singleNodeMain == null) {
    try {
      CConfiguration cConf=CConfiguration.create();
      cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
      singleNodeMain=SingleNodeMain.createSingleNodeMain(true,null,cConf,new Configuration());
      singleNodeMain.startUp();
    }
 catch (    Throwable e) {
      LOG.error(""String_Node_Str"",e);
      if (singleNodeMain != null) {
        singleNodeMain.shutDown();
      }
      throw e;
    }
  }
}"
7529,"@AfterClass public static void tearDownClass(){
  if (singleNodeMain != null) {
    singleNodeMain.shutDown();
    singleNodeMain=null;
  }
}","@AfterClass public static void tearDownClass(){
  testStackIndex--;
  if (singleNodeMain != null && testStackIndex == 0) {
    singleNodeMain.shutDown();
    singleNodeMain=null;
  }
}"
7530,"@Override public void run(){
  LOG.info(""String_Node_Str"");
  Future<Service.State> completion=Services.getCompletionFuture(service);
  service.startAndWait();
  int port=service.getBindAddress().getPort();
  Cancellable contextCancellable=context.announce(name,port);
  LOG.info(""String_Node_Str"");
  try {
    completion.get();
    contextCancellable.cancel();
  }
 catch (  InterruptedException e) {
    LOG.error(""String_Node_Str"",e);
  }
catch (  ExecutionException e) {
    LOG.error(""String_Node_Str"",e);
  }
}","@Override public void run(){
  LOG.info(""String_Node_Str"");
  Future<Service.State> completion=Services.getCompletionFuture(service);
  service.startAndWait();
  int port=service.getBindAddress().getPort();
  Cancellable contextCancellable=getContext().announce(name,port);
  LOG.info(""String_Node_Str"");
  try {
    completion.get();
    contextCancellable.cancel();
  }
 catch (  InterruptedException e) {
    LOG.error(""String_Node_Str"",e);
  }
catch (  ExecutionException e) {
    LOG.error(""String_Node_Str"",e);
  }
}"
7531,"@Override public void initialize(TwillContext context){
  LOG.info(""String_Node_Str"");
  this.context=context;
  Map<String,String> runnableArgs=new HashMap<String,String>(context.getSpecification().getConfigs());
  name=runnableArgs.get(""String_Node_Str"");
  handlers=new ArrayList<HttpServiceHandler>();
  List<String> handlerNames=GSON.fromJson(runnableArgs.get(""String_Node_Str""),HANDLER_NAMES_TYPE);
  for (  String handlerName : handlerNames) {
    try {
      HttpServiceHandler handler=(HttpServiceHandler)programClassLoader.loadClass(handlerName).newInstance();
      handlers.add(handler);
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"");
      Throwables.propagate(e);
    }
  }
  service=createNettyHttpService(context.getHost().getCanonicalHostName());
}","@Override public void initialize(TwillContext context){
  LOG.info(""String_Node_Str"");
  super.initialize(context);
  Map<String,String> runnableArgs=new HashMap<String,String>(context.getSpecification().getConfigs());
  name=runnableArgs.get(""String_Node_Str"");
  handlers=new ArrayList<HttpServiceHandler>();
  List<String> handlerNames=GSON.fromJson(runnableArgs.get(""String_Node_Str""),HANDLER_NAMES_TYPE);
  for (  String handlerName : handlerNames) {
    try {
      HttpServiceHandler handler=(HttpServiceHandler)programClassLoader.loadClass(handlerName).newInstance();
      handlers.add(handler);
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"");
      Throwables.propagate(e);
    }
  }
  service=createNettyHttpService(context.getHost().getCanonicalHostName());
}"
7532,"@POST @Path(""String_Node_Str"") public void downloadQueryResults(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String id){
  boolean responseStarted=false;
  try {
    QueryHandle handle=QueryHandle.fromId(id);
    if (handle.equals(QueryHandle.NO_OP)) {
      responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
      return;
    }
    StringBuffer sb=new StringBuffer();
    sb.append(getCSVHeaders(exploreService.getResultSchema(handle)));
    sb.append('\n');
    List<QueryResult> results;
    results=exploreService.previewResults(handle);
    if (results.isEmpty()) {
      results=exploreService.nextResults(handle,DOWNLOAD_FETCH_CHUNK_SIZE);
    }
    try {
      responder.sendChunkStart(HttpResponseStatus.OK,null);
      responseStarted=true;
      while (!results.isEmpty()) {
        for (        QueryResult result : results) {
          appendCSVRow(sb,result);
          sb.append('\n');
        }
        responder.sendChunk(ChannelBuffers.wrappedBuffer(sb.toString().getBytes(""String_Node_Str"")));
        sb=new StringBuffer();
        results=exploreService.nextResults(handle,DOWNLOAD_FETCH_CHUNK_SIZE);
      }
    }
  finally {
      responder.sendChunkEnd();
    }
  }
 catch (  IllegalArgumentException e) {
    LOG.debug(""String_Node_Str"",e);
    if (!responseStarted) {
      responder.sendError(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    }
  }
catch (  SQLException e) {
    LOG.debug(""String_Node_Str"",e);
    if (!responseStarted) {
      responder.sendError(HttpResponseStatus.BAD_REQUEST,String.format(""String_Node_Str"",e.getSQLState(),e.getMessage()));
    }
  }
catch (  HandleNotFoundException e) {
    if (!responseStarted) {
      responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    }
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    if (!responseStarted) {
      responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
    }
  }
}","@POST @Path(""String_Node_Str"") public void downloadQueryResults(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String id){
  boolean responseStarted=false;
  try {
    QueryHandle handle=QueryHandle.fromId(id);
    if (handle.equals(QueryHandle.NO_OP)) {
      responder.sendStatus(HttpResponseStatus.CONFLICT);
      return;
    }
    StringBuffer sb=new StringBuffer();
    sb.append(getCSVHeaders(exploreService.getResultSchema(handle)));
    sb.append('\n');
    List<QueryResult> results;
    results=exploreService.previewResults(handle);
    if (results.isEmpty()) {
      results=exploreService.nextResults(handle,DOWNLOAD_FETCH_CHUNK_SIZE);
    }
    try {
      responder.sendChunkStart(HttpResponseStatus.OK,null);
      responseStarted=true;
      while (!results.isEmpty()) {
        for (        QueryResult result : results) {
          appendCSVRow(sb,result);
          sb.append('\n');
        }
        responder.sendChunk(ChannelBuffers.wrappedBuffer(sb.toString().getBytes(""String_Node_Str"")));
        sb.delete(0,sb.length());
        results=exploreService.nextResults(handle,DOWNLOAD_FETCH_CHUNK_SIZE);
      }
    }
  finally {
      responder.sendChunkEnd();
    }
  }
 catch (  IllegalArgumentException e) {
    LOG.debug(""String_Node_Str"",e);
    if (!responseStarted) {
      responder.sendError(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    }
  }
catch (  SQLException e) {
    LOG.debug(""String_Node_Str"",e);
    if (!responseStarted) {
      responder.sendError(HttpResponseStatus.BAD_REQUEST,String.format(""String_Node_Str"",e.getSQLState(),e.getMessage()));
    }
  }
catch (  HandleNotFoundException e) {
    if (!responseStarted) {
      responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    }
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    if (!responseStarted) {
      responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
    }
  }
}"
7533,"/** 
 * Creates a new Dataset or updates existing Dataset specification's properties if an optional update parameter in the body is set to true,   {@link DatasetInstanceConfiguration}is constructed based on request and appropriate action is performed
 */
@PUT @Path(""String_Node_Str"") public void createOrUpdate(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String name){
  Reader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()));
  DatasetInstanceConfiguration creationProperties=GSON.fromJson(reader,DatasetInstanceConfiguration.class);
  String operation=(creationProperties.isUpdate() == true) ? ""String_Node_Str"" : ""String_Node_Str"";
  LOG.info(""String_Node_Str"",operation,name,creationProperties.getTypeName(),creationProperties.getProperties());
  DatasetSpecification existing=instanceManager.get(name);
  if (existing != null) {
    String message=null;
    if (!creationProperties.isUpdate()) {
      message=String.format(""String_Node_Str"",name,existing);
    }
 else     if (!existing.getType().equals(creationProperties.getTypeName())) {
      message=String.format(""String_Node_Str"",name,existing.getType());
    }
    if (message != null) {
      LOG.warn(message);
      responder.sendError(HttpResponseStatus.CONFLICT,message);
      return;
    }
  }
  if (existing == null && creationProperties.isUpdate()) {
    responder.sendError(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
  }
  DatasetTypeMeta typeMeta=implManager.getTypeInfo(creationProperties.getTypeName());
  if (typeMeta == null) {
    String message=String.format(""String_Node_Str"",operation,name,creationProperties.getTypeName());
    LOG.warn(message);
    responder.sendError(HttpResponseStatus.NOT_FOUND,message);
    return;
  }
  DatasetSpecification spec;
  try {
    spec=opExecutorClient.create(name,typeMeta,DatasetProperties.builder().addAll(creationProperties.getProperties()).build());
  }
 catch (  Exception e) {
    String msg=String.format(""String_Node_Str"",operation,name,creationProperties.getTypeName(),e.getMessage());
    LOG.error(msg,e);
    throw new RuntimeException(msg,e);
  }
  instanceManager.add(spec);
  try {
    if (creationProperties.isUpdate()) {
      datasetExploreFacade.disableExplore(name);
    }
    datasetExploreFacade.enableExplore(name);
  }
 catch (  Exception e) {
    String msg=String.format(""String_Node_Str"",name,creationProperties.getProperties(),e.getMessage());
    LOG.error(msg,e);
  }
  if (creationProperties.isUpdate()) {
    executeAdmin(request,responder,name,""String_Node_Str"");
    responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",name));
  }
  responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",name));
}","/** 
 * Creates a new Dataset or updates existing Dataset specification's properties if an optional update parameter in the body is set to true,   {@link DatasetInstanceConfiguration}is constructed based on request and appropriate action is performed
 */
@PUT @Path(""String_Node_Str"") public void createOrUpdate(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String name){
  Reader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()));
  DatasetInstanceConfiguration creationProperties=GSON.fromJson(reader,DatasetInstanceConfiguration.class);
  String operation=(creationProperties.isUpdate() == true) ? ""String_Node_Str"" : ""String_Node_Str"";
  LOG.info(""String_Node_Str"",operation,name,creationProperties.getTypeName(),creationProperties.getProperties());
  DatasetSpecification existing=instanceManager.get(name);
  if (existing != null) {
    String message=null;
    if (!creationProperties.isUpdate()) {
      message=String.format(""String_Node_Str"",name,existing);
    }
 else     if (!existing.getType().equals(creationProperties.getTypeName())) {
      message=String.format(""String_Node_Str"",name,existing.getType());
    }
    if (message != null) {
      LOG.warn(message);
      responder.sendError(HttpResponseStatus.CONFLICT,message);
      return;
    }
  }
  if (existing == null && creationProperties.isUpdate()) {
    responder.sendError(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",name));
  }
  DatasetTypeMeta typeMeta=implManager.getTypeInfo(creationProperties.getTypeName());
  if (typeMeta == null) {
    String message=String.format(""String_Node_Str"",operation,name,creationProperties.getTypeName());
    LOG.warn(message);
    responder.sendError(HttpResponseStatus.NOT_FOUND,message);
    return;
  }
  DatasetSpecification spec;
  try {
    spec=opExecutorClient.create(name,typeMeta,DatasetProperties.builder().addAll(creationProperties.getProperties()).build());
  }
 catch (  Exception e) {
    String msg=String.format(""String_Node_Str"",operation,name,creationProperties.getTypeName(),e.getMessage());
    LOG.error(msg,e);
    throw new RuntimeException(msg,e);
  }
  instanceManager.add(spec);
  try {
    if (creationProperties.isUpdate()) {
      datasetExploreFacade.disableExplore(name);
    }
    datasetExploreFacade.enableExplore(name);
  }
 catch (  Exception e) {
    String msg=String.format(""String_Node_Str"",name,creationProperties.getProperties(),e.getMessage());
    LOG.error(msg,e);
  }
  if (creationProperties.isUpdate()) {
    executeAdmin(request,responder,name,""String_Node_Str"");
  }
  responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",name));
}"
7534,"@Override protected boolean upgradeTable(HTableDescriptor tableDescriptor){
  HColumnDescriptor columnDescriptor=tableDescriptor.getFamily(DATA_COLUMN_FAMILY);
  boolean needUpgrade=false;
  if (columnDescriptor.getMaxVersions() < Integer.MAX_VALUE) {
    columnDescriptor.setMaxVersions(Integer.MAX_VALUE);
    needUpgrade=true;
  }
  if (tableUtil.getBloomFilter(columnDescriptor) != HBaseTableUtil.BloomType.ROW) {
    tableUtil.setBloomFilter(columnDescriptor,HBaseTableUtil.BloomType.ROW);
    needUpgrade=true;
  }
  if (spec.getProperty(TxConstants.PROPERTY_TTL) == null && columnDescriptor.getValue(TxConstants.PROPERTY_TTL) != null) {
    columnDescriptor.remove(TxConstants.PROPERTY_TTL.getBytes());
    needUpgrade=true;
  }
 else   if (!spec.getProperty(TxConstants.PROPERTY_TTL).equals(columnDescriptor.getValue(TxConstants.PROPERTY_TTL))) {
    columnDescriptor.setValue(TxConstants.PROPERTY_TTL,spec.getProperty(TxConstants.PROPERTY_TTL));
    needUpgrade=true;
  }
  return needUpgrade;
}","@Override protected boolean upgradeTable(HTableDescriptor tableDescriptor){
  HColumnDescriptor columnDescriptor=tableDescriptor.getFamily(DATA_COLUMN_FAMILY);
  boolean needUpgrade=false;
  if (columnDescriptor.getMaxVersions() < Integer.MAX_VALUE) {
    columnDescriptor.setMaxVersions(Integer.MAX_VALUE);
    needUpgrade=true;
  }
  if (tableUtil.getBloomFilter(columnDescriptor) != HBaseTableUtil.BloomType.ROW) {
    tableUtil.setBloomFilter(columnDescriptor,HBaseTableUtil.BloomType.ROW);
    needUpgrade=true;
  }
  if (spec.getProperty(TxConstants.PROPERTY_TTL) == null && columnDescriptor.getValue(TxConstants.PROPERTY_TTL) != null) {
    columnDescriptor.remove(TxConstants.PROPERTY_TTL.getBytes());
    needUpgrade=true;
  }
 else   if (spec.getProperty(TxConstants.PROPERTY_TTL) != null && !spec.getProperty(TxConstants.PROPERTY_TTL).equals(columnDescriptor.getValue(TxConstants.PROPERTY_TTL))) {
    columnDescriptor.setValue(TxConstants.PROPERTY_TTL,spec.getProperty(TxConstants.PROPERTY_TTL));
    needUpgrade=true;
  }
  return needUpgrade;
}"
7535,"public static <T extends Iterable<? extends V>,V>Transactional<T,V> of(TransactionExecutorFactory txFactory,Supplier<T> supplier){
  return new Transactional<T,V>(txFactory,supplier);
}","public static <T extends Iterable<V>,V>Transactional<T,V> of(TransactionExecutorFactory txFactory,Supplier<T> supplier){
  return new Transactional<T,V>(txFactory,supplier);
}"
7536,"/** 
 * Executes function within new transaction. See   {@link Transactional} for more details.
 * @param txFactory transaction factory to create new transaction
 * @param supplier supplier of transaction context
 * @param func function to execute
 * @param < V > type of object contained inside the transaction context
 * @param < T > type of the transaction context
 * @param < R > type of the function result
 * @return function result
 */
public static <V,T extends Iterable<? extends V>,R>R execute(TransactionExecutorFactory txFactory,Supplier<T> supplier,TransactionExecutor.Function<T,R> func) throws TransactionFailureException, IOException, InterruptedException {
  T it=supplier.get();
  Iterable<TransactionAware> txAwares=Iterables.transform(Iterables.filter(it,Predicates.instanceOf(TransactionAware.class)),new Function<V,TransactionAware>(){
    @Override public TransactionAware apply(    V input){
      return (TransactionAware)input;
    }
  }
);
  TransactionExecutor executor=txFactory.createExecutor(txAwares);
  try {
    return executor.execute(func,it);
  }
  finally {
    for (    V t : it) {
      if (t instanceof Closeable) {
        ((Closeable)t).close();
      }
    }
  }
}","/** 
 * Executes function within new transaction. See   {@link Transactional} for more details.
 * @param txFactory transaction factory to create new transaction
 * @param supplier supplier of transaction context
 * @param func function to execute
 * @param < V > type of object contained inside the transaction context
 * @param < T > type of the transaction context
 * @param < R > type of the function result
 * @return function result
 */
public static <V,T extends Iterable<V>,R>R execute(TransactionExecutorFactory txFactory,Supplier<T> supplier,TransactionExecutor.Function<T,R> func) throws TransactionFailureException, IOException, InterruptedException {
  T it=supplier.get();
  Iterable<TransactionAware> txAwares=Iterables.transform(Iterables.filter(it,Predicates.instanceOf(TransactionAware.class)),new Function<V,TransactionAware>(){
    @Override public TransactionAware apply(    V input){
      return (TransactionAware)input;
    }
  }
);
  TransactionExecutor executor=txFactory.createExecutor(txAwares);
  try {
    return executor.execute(func,it);
  }
  finally {
    for (    V t : it) {
      if (t instanceof Closeable) {
        ((Closeable)t).close();
      }
    }
  }
}"
7537,"@Inject public MDSStreamMetaStore(CConfiguration conf,final TransactionSystemClient txClient,DatasetFramework framework){
  final DatasetFramework dsFramework=new NamespacedDatasetFramework(framework,new ReactorDatasetNamespace(conf,DataSetAccessor.Namespace.SYSTEM));
  txnl=Transactional.of(new TransactionExecutorFactory(){
    @Override public TransactionExecutor createExecutor(    Iterable<TransactionAware> transactionAwares){
      return new DefaultTransactionExecutor(txClient,transactionAwares);
    }
  }
,new Supplier<StreamMds>(){
    @Override public StreamMds get(){
      try {
        Table mdsTable=DatasetsUtil.getOrCreateDataset(dsFramework,STREAM_META_TABLE,""String_Node_Str"",DatasetProperties.EMPTY,null);
        return new StreamMds(new MetadataStoreDataset(mdsTable));
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"",e);
        throw Throwables.propagate(e);
      }
    }
  }
);
}","@Inject public MDSStreamMetaStore(CConfiguration conf,final TransactionSystemClient txClient,DatasetFramework framework){
  final DatasetFramework dsFramework=new NamespacedDatasetFramework(framework,new ReactorDatasetNamespace(conf,DataSetAccessor.Namespace.SYSTEM));
  txnl=Transactional.of(new TransactionExecutorFactory(){
    @Override public TransactionExecutor createExecutor(    Iterable<TransactionAware> transactionAwares){
      return new DefaultTransactionExecutor(txClient,transactionAwares);
    }
  }
,new Supplier<StreamMds>(){
    @Override public StreamMds get(){
      try {
        Table mdsTable=DatasetsUtil.getOrCreateDataset(dsFramework,STREAM_META_TABLE,""String_Node_Str"",DatasetProperties.EMPTY,DatasetDefinition.NO_ARGUMENTS,null);
        return new StreamMds(new MetadataStoreDataset(mdsTable));
      }
 catch (      Exception e) {
        LOG.error(""String_Node_Str"",e);
        throw Throwables.propagate(e);
      }
    }
  }
);
}"
7538,"@Override public StreamMds get(){
  try {
    Table mdsTable=DatasetsUtil.getOrCreateDataset(dsFramework,STREAM_META_TABLE,""String_Node_Str"",DatasetProperties.EMPTY,null);
    return new StreamMds(new MetadataStoreDataset(mdsTable));
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}","@Override public StreamMds get(){
  try {
    Table mdsTable=DatasetsUtil.getOrCreateDataset(dsFramework,STREAM_META_TABLE,""String_Node_Str"",DatasetProperties.EMPTY,DatasetDefinition.NO_ARGUMENTS,null);
    return new StreamMds(new MetadataStoreDataset(mdsTable));
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}"
7539,"public OrderedTable getMetaTable() throws Exception {
  return DatasetsUtil.getOrCreateDataset(dsFramework,getMetaTableName(),OrderedTable.class.getName(),DatasetProperties.EMPTY,null);
}","public OrderedTable getMetaTable() throws Exception {
  return DatasetsUtil.getOrCreateDataset(dsFramework,getMetaTableName(),OrderedTable.class.getName(),DatasetProperties.EMPTY,DatasetDefinition.NO_ARGUMENTS,null);
}"
7540,"@Test public void executeTest() throws Exception {
  ExploreClient exploreClient=new MockExploreClient(ImmutableMap.of(""String_Node_Str"",(List<ColumnDesc>)Lists.newArrayList(new ColumnDesc(""String_Node_Str"",""String_Node_Str"",1,""String_Node_Str""),new ColumnDesc(""String_Node_Str"",""String_Node_Str"",2,""String_Node_Str""))),ImmutableMap.of(""String_Node_Str"",(List<Result>)Lists.<Result>newArrayList()));
  ExplorePreparedStatement statement=new ExplorePreparedStatement(null,exploreClient,""String_Node_Str"");
  statement.setInt(1,100);
  try {
    statement.execute();
    Assert.fail();
  }
 catch (  SQLException e) {
  }
  statement.setString(2,""String_Node_Str"");
  Assert.assertEquals(""String_Node_Str"",statement.updateSql());
  Assert.assertTrue(statement.execute());
  ResultSet rs=statement.getResultSet();
  Assert.assertNotNull(rs);
  Assert.assertFalse(rs.isClosed());
  Assert.assertFalse(rs.next());
  statement=new ExplorePreparedStatement(null,exploreClient,""String_Node_Str"");
  Assert.assertEquals(""String_Node_Str"",statement.updateSql());
  statement=new ExplorePreparedStatement(null,exploreClient,""String_Node_Str"");
  statement.setInt(1,100);
  Assert.assertEquals(""String_Node_Str"",statement.updateSql());
  statement=new ExplorePreparedStatement(null,exploreClient,""String_Node_Str"");
  statement.setInt(1,100);
  Assert.assertEquals(""String_Node_Str"",statement.updateSql());
  statement=new ExplorePreparedStatement(null,exploreClient,""String_Node_Str"");
  statement.setInt(1,100);
  Assert.assertEquals(""String_Node_Str"",statement.updateSql());
  statement=new ExplorePreparedStatement(null,exploreClient,""String_Node_Str"");
  statement.setInt(1,100);
  Assert.assertEquals(""String_Node_Str"",statement.updateSql());
  statement=new ExplorePreparedStatement(null,exploreClient,""String_Node_Str"");
  statement.setInt(1,100);
  Assert.assertEquals(""String_Node_Str"",statement.updateSql());
}","@Test public void executeTest() throws Exception {
  ExploreClient exploreClient=new MockExploreClient(ImmutableMap.of(""String_Node_Str"",(List<ColumnDesc>)Lists.newArrayList(new ColumnDesc(""String_Node_Str"",""String_Node_Str"",1,""String_Node_Str""),new ColumnDesc(""String_Node_Str"",""String_Node_Str"",2,""String_Node_Str""))),ImmutableMap.of(""String_Node_Str"",(List<QueryResult>)Lists.<QueryResult>newArrayList()));
  ExplorePreparedStatement statement=new ExplorePreparedStatement(null,exploreClient,""String_Node_Str"");
  statement.setInt(1,100);
  try {
    statement.execute();
    Assert.fail();
  }
 catch (  SQLException e) {
  }
  statement.setString(2,""String_Node_Str"");
  Assert.assertEquals(""String_Node_Str"",statement.updateSql());
  Assert.assertTrue(statement.execute());
  ResultSet rs=statement.getResultSet();
  Assert.assertNotNull(rs);
  Assert.assertFalse(rs.isClosed());
  Assert.assertFalse(rs.next());
  statement=new ExplorePreparedStatement(null,exploreClient,""String_Node_Str"");
  Assert.assertEquals(""String_Node_Str"",statement.updateSql());
  statement=new ExplorePreparedStatement(null,exploreClient,""String_Node_Str"");
  statement.setInt(1,100);
  Assert.assertEquals(""String_Node_Str"",statement.updateSql());
  statement=new ExplorePreparedStatement(null,exploreClient,""String_Node_Str"");
  statement.setInt(1,100);
  Assert.assertEquals(""String_Node_Str"",statement.updateSql());
  statement=new ExplorePreparedStatement(null,exploreClient,""String_Node_Str"");
  statement.setInt(1,100);
  Assert.assertEquals(""String_Node_Str"",statement.updateSql());
  statement=new ExplorePreparedStatement(null,exploreClient,""String_Node_Str"");
  statement.setInt(1,100);
  Assert.assertEquals(""String_Node_Str"",statement.updateSql());
  statement=new ExplorePreparedStatement(null,exploreClient,""String_Node_Str"");
  statement.setInt(1,100);
  Assert.assertEquals(""String_Node_Str"",statement.updateSql());
}"
7541,"private String getStatus(final Id.Program id,final Type type) throws Throwable {
  final String[] statusStr={null};
  LOG.error(type.prettyName());
  if (type == Type.MAPREDUCE) {
    String workflowName=getWorkflowName(id.getId());
    LOG.error(workflowName);
    if (workflowName != null) {
      LOG.error(""String_Node_Str"");
      workflowClient.getWorkflowStatus(id.getAccountId(),id.getApplicationId(),workflowName,new WorkflowClient.Callback(){
        @Override public void handle(        WorkflowClient.Status status){
          LOG.error(""String_Node_Str"");
          if (status.getCode().equals(WorkflowClient.Status.Code.OK)) {
            LOG.error(""String_Node_Str"");
            statusStr[0]=""String_Node_Str"";
          }
 else {
            LOG.error(""String_Node_Str"");
            try {
              statusStr[0]=getProgramStatus(id,type).getStatus();
            }
 catch (            Exception e) {
              LOG.error(""String_Node_Str"",e);
              statusStr[0]=null;
            }
          }
        }
      }
);
    }
 else {
      LOG.error(""String_Node_Str"");
      statusStr[0]=getProgramStatus(id,type).getStatus();
    }
  }
 else   if (type == null) {
    return ""String_Node_Str"";
  }
 else {
    statusStr[0]=getProgramStatus(id,type).getStatus();
  }
  if (statusStr[0] == null) {
    throw new Throwable(""String_Node_Str"");
  }
  return statusStr[0];
}","private void getStatus(final Id.Program id,final Type type,final Map<Id.Program,String> statusMap) throws Throwable {
  if (type == Type.MAPREDUCE) {
    String workflowName=getWorkflowName(id.getId());
    if (workflowName != null) {
      ApplicationSpecification appSpec=store.getApplication(id.getApplication());
      if (appSpec == null || !appSpec.getMapReduce().containsKey(id.getId())) {
        statusMap.put(id,""String_Node_Str"");
      }
      workflowClient.getWorkflowStatus(id.getAccountId(),id.getApplicationId(),workflowName,new WorkflowClient.Callback(){
        @Override public void handle(        WorkflowClient.Status status){
          if (status.getCode().equals(WorkflowClient.Status.Code.OK)) {
            statusMap.put(id,""String_Node_Str"");
          }
 else {
            try {
              statusMap.put(id,getProgramStatus(id,type).getStatus());
            }
 catch (            Exception e) {
              LOG.error(""String_Node_Str"",e);
              statusMap.put(id,null);
            }
          }
        }
      }
);
    }
 else {
      statusMap.put(id,getProgramStatus(id,type).getStatus());
    }
  }
 else   if (type == null) {
    statusMap.put(id,""String_Node_Str"");
  }
 else {
    statusMap.put(id,getProgramStatus(id,type).getStatus());
  }
}"
7542,"@Nullable protected List<JsonObject> decodeArrayArguments(HttpRequest request) throws IOException {
  ChannelBuffer content=request.getContent();
  if (!content.readable()) {
    return new ArrayList<JsonObject>();
  }
  Reader reader=new InputStreamReader(new ChannelBufferInputStream(content),Charsets.UTF_8);
  try {
    List<JsonObject> args=GSON.fromJson(reader,new TypeToken<List<JsonObject>>(){
    }
.getType());
    System.err.println(args);
    return args == null ? new ArrayList<JsonObject>() : args;
  }
 catch (  JsonSyntaxException e) {
    LOG.info(""String_Node_Str"",request.getUri(),e);
    throw e;
  }
 finally {
    reader.close();
  }
}","@Nullable protected List<JsonObject> decodeArrayArguments(HttpRequest request) throws IOException {
  ChannelBuffer content=request.getContent();
  if (!content.readable()) {
    return new ArrayList<JsonObject>();
  }
  Reader reader=new InputStreamReader(new ChannelBufferInputStream(content),Charsets.UTF_8);
  try {
    List<JsonObject> args=GSON.fromJson(reader,new TypeToken<List<JsonObject>>(){
    }
.getType());
    return args == null ? new ArrayList<JsonObject>() : args;
  }
 catch (  JsonSyntaxException e) {
    LOG.info(""String_Node_Str"",request.getUri(),e);
    throw e;
  }
 finally {
    reader.close();
  }
}"
7543,"@POST @Path(""String_Node_Str"") public void getInstances(HttpRequest request,HttpResponder responder){
  try {
    String accountId=getAuthenticatedAccountId(request);
    List<JsonObject> args=decodeArrayArguments(request);
    if (args == null || args.isEmpty()) {
      respondAndLog(responder,HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    for (int i=0; i < args.size(); ++i) {
      JsonObject requestedObj=args.get(i);
      String appId, programTypeStr, programId, runnableId;
      try {
        appId=requestedObj.getAsJsonPrimitive(APP_ID_ARG).getAsString();
        programTypeStr=requestedObj.getAsJsonPrimitive(PROGRAM_TYPE_ARG).getAsString();
        programId=requestedObj.getAsJsonPrimitive(PROGRAM_ID_ARG).getAsString();
      }
 catch (      NullPointerException e) {
        respondAndLog(responder,HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
        return;
      }
      Type programType=Type.valueOfPrettyName(programTypeStr);
      if (programType == null) {
        respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + programTypeStr + ""String_Node_Str"");
        return;
      }
      int requested, provisioned;
      ApplicationSpecification spec=store.getApplication(Id.Application.from(accountId,appId));
      if (spec == null) {
        respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + appId + ""String_Node_Str"");
        return;
      }
      if (programType == Type.PROCEDURE) {
        runnableId=programId;
        if (spec.getProcedures().containsKey(programId)) {
          requested=store.getProcedureInstances(Id.Program.from(accountId,appId,programId));
        }
 else {
          respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + programId + ""String_Node_Str"");
          return;
        }
      }
 else {
        try {
          runnableId=requestedObj.getAsJsonPrimitive(RUNNABLE_ID_ARG).getAsString();
        }
 catch (        NullPointerException e) {
          respondAndLog(responder,HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
          return;
        }
        if (programType == Type.FLOW) {
          FlowSpecification flowSpec=spec.getFlows().get(programId);
          if (flowSpec != null) {
            Map<String,FlowletDefinition> flowletSpecs=flowSpec.getFlowlets();
            if (flowletSpecs != null && flowletSpecs.containsKey(runnableId)) {
              requested=flowletSpecs.get(runnableId).getInstances();
            }
 else {
              respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + runnableId + ""String_Node_Str"");
              return;
            }
          }
 else {
            respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + programId + ""String_Node_Str"");
            return;
          }
        }
 else         if (programType == Type.SERVICE) {
          ServiceSpecification serviceSpec=spec.getServices().get(programId);
          if (serviceSpec != null) {
            Map<String,RuntimeSpecification> runtimeSpecs=serviceSpec.getRunnables();
            if (runtimeSpecs != null && runtimeSpecs.containsKey(runnableId)) {
              requested=runtimeSpecs.get(runnableId).getResourceSpecification().getInstances();
            }
 else {
              respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + runnableId + ""String_Node_Str"");
              return;
            }
          }
 else {
            respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + programId + ""String_Node_Str"");
            return;
          }
        }
 else {
          respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + programTypeStr + ""String_Node_Str"");
          return;
        }
      }
      provisioned=getRunnableCount(accountId,appId,programType,programId,runnableId);
      requestedObj.addProperty(""String_Node_Str"",requested);
      requestedObj.addProperty(""String_Node_Str"",provisioned);
      args.set(i,requestedObj);
    }
    responder.sendJson(HttpResponseStatus.OK,args);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","@POST @Path(""String_Node_Str"") public void getInstances(HttpRequest request,HttpResponder responder){
  try {
    String accountId=getAuthenticatedAccountId(request);
    List<JsonObject> args=decodeArrayArguments(request);
    if (args == null || args.isEmpty()) {
      respondAndLog(responder,HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    for (int i=0; i < args.size(); ++i) {
      JsonObject requestedObj=args.get(i);
      String appId, programTypeStr, programId, runnableId;
      try {
        appId=requestedObj.getAsJsonPrimitive(APP_ID_ARG).getAsString();
        programTypeStr=requestedObj.getAsJsonPrimitive(PROGRAM_TYPE_ARG).getAsString();
        programId=requestedObj.getAsJsonPrimitive(PROGRAM_ID_ARG).getAsString();
      }
 catch (      NullPointerException e) {
        respondAndLog(responder,HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
        return;
      }
      Type programType;
      try {
        programType=Type.valueOfPrettyName(programTypeStr);
      }
 catch (      IllegalArgumentException e) {
        respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + programTypeStr);
        return;
      }
      if (programType == null) {
        respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + programTypeStr + ""String_Node_Str"");
        return;
      }
      int requested, provisioned;
      ApplicationSpecification spec=store.getApplication(Id.Application.from(accountId,appId));
      if (spec == null) {
        respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + appId + ""String_Node_Str"");
        return;
      }
      if (programType == Type.PROCEDURE) {
        runnableId=programId;
        if (spec.getProcedures().containsKey(programId)) {
          requested=store.getProcedureInstances(Id.Program.from(accountId,appId,programId));
        }
 else {
          respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + programId + ""String_Node_Str"");
          return;
        }
      }
 else {
        if (programType != Type.FLOW && programType != Type.SERVICE) {
          respondAndLog(responder,HttpResponseStatus.BAD_REQUEST,""String_Node_Str"" + programTypeStr);
          return;
        }
        try {
          runnableId=requestedObj.getAsJsonPrimitive(RUNNABLE_ID_ARG).getAsString();
        }
 catch (        NullPointerException e) {
          respondAndLog(responder,HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
          return;
        }
        if (programType == Type.FLOW) {
          FlowSpecification flowSpec=spec.getFlows().get(programId);
          if (flowSpec != null) {
            Map<String,FlowletDefinition> flowletSpecs=flowSpec.getFlowlets();
            if (flowletSpecs != null && flowletSpecs.containsKey(runnableId)) {
              requested=flowletSpecs.get(runnableId).getInstances();
            }
 else {
              respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + runnableId + ""String_Node_Str"");
              return;
            }
          }
 else {
            respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + programId + ""String_Node_Str"");
            return;
          }
        }
 else {
          ServiceSpecification serviceSpec=spec.getServices().get(programId);
          if (serviceSpec != null) {
            Map<String,RuntimeSpecification> runtimeSpecs=serviceSpec.getRunnables();
            if (runtimeSpecs != null && runtimeSpecs.containsKey(runnableId)) {
              requested=runtimeSpecs.get(runnableId).getResourceSpecification().getInstances();
            }
 else {
              respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + runnableId + ""String_Node_Str"");
              return;
            }
          }
 else {
            respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + programId + ""String_Node_Str"");
            return;
          }
        }
      }
      provisioned=getRunnableCount(accountId,appId,programType,programId,runnableId);
      requestedObj.addProperty(""String_Node_Str"",requested);
      requestedObj.addProperty(""String_Node_Str"",provisioned);
      args.set(i,requestedObj);
    }
    responder.sendJson(HttpResponseStatus.OK,args);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}"
7544,"@POST @Path(""String_Node_Str"") public void getStatuses(HttpRequest request,HttpResponder responder){
  try {
    String accountId=getAuthenticatedAccountId(request);
    List<JsonObject> args=decodeArrayArguments(request);
    if (args == null || args.isEmpty()) {
      respondAndLog(responder,HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    for (int i=0; i < args.size(); ++i) {
      JsonObject requestedObj=args.get(i);
      String appId, programType, programId;
      try {
        appId=requestedObj.getAsJsonPrimitive(APP_ID_ARG).getAsString();
        programType=requestedObj.getAsJsonPrimitive(PROGRAM_TYPE_ARG).getAsString();
        programId=requestedObj.getAsJsonPrimitive(PROGRAM_ID_ARG).getAsString();
      }
 catch (      NullPointerException e) {
        respondAndLog(responder,HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
        return;
      }
      String status=getStatus(Id.Program.from(accountId,appId,programId),Type.valueOfPrettyName(programType));
      if (status.equals(""String_Node_Str"")) {
        responder.sendStatus(HttpResponseStatus.NOT_FOUND);
        return;
      }
 else {
        requestedObj.addProperty(""String_Node_Str"",status);
      }
      args.set(i,requestedObj);
    }
    responder.sendJson(HttpResponseStatus.OK,args);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","@POST @Path(""String_Node_Str"") public void getStatuses(HttpRequest request,HttpResponder responder){
  try {
    String accountId=getAuthenticatedAccountId(request);
    List<JsonObject> args=decodeArrayArguments(request);
    if (args == null || args.isEmpty()) {
      respondAndLog(responder,HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    final Map<Id.Program,String> statusMap=new HashMap<Id.Program,String>();
    for (int i=0; i < args.size(); ++i) {
      JsonObject requestedObj=args.get(i);
      String appId, programTypeStr, programId;
      try {
        appId=requestedObj.getAsJsonPrimitive(APP_ID_ARG).getAsString();
        programTypeStr=requestedObj.getAsJsonPrimitive(PROGRAM_TYPE_ARG).getAsString();
        programId=requestedObj.getAsJsonPrimitive(PROGRAM_ID_ARG).getAsString();
      }
 catch (      NullPointerException e) {
        respondAndLog(responder,HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
        return;
      }
      Id.Program progId=Id.Program.from(accountId,appId,programId);
      Type programType;
      try {
        programType=Type.valueOfPrettyName(programTypeStr);
      }
 catch (      IllegalArgumentException e) {
        respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + programTypeStr);
        return;
      }
      if (programType == null) {
        respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + programTypeStr + ""String_Node_Str"");
        return;
      }
      getStatus(progId,programType,statusMap);
    }
    while (statusMap.size() != args.size()) {
    }
    for (int i=0; i < args.size(); ++i) {
      JsonObject requestedObj=args.get(i);
      for (      Id.Program id : statusMap.keySet()) {
        String status=statusMap.get(id);
        if (status == null) {
          throw new Throwable(""String_Node_Str"" + id.getId());
        }
 else         if (status.equals(""String_Node_Str"")) {
          respondAndLog(responder,HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
          return;
        }
        if (id.getId().equals(requestedObj.getAsJsonPrimitive(PROGRAM_ID_ARG).getAsString()) && id.getApplicationId().equals(requestedObj.getAsJsonPrimitive(APP_ID_ARG).getAsString())) {
          requestedObj.addProperty(""String_Node_Str"",status);
        }
      }
    }
    responder.sendJson(HttpResponseStatus.OK,args);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}"
7545,"@Override protected void doInit(TwillContext context){
  this.user=DEFAULT_USER;
  CConfiguration cConf=getCConfiguration();
  Configuration hConf=getConfiguration();
  cConf.set(Constants.Dataset.Executor.ADDRESS,context.getHost().getHostName());
  injector=Guice.createInjector(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MetricsClientRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new LocationRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModule(),new DataSetServiceModules().getDistributedModule(),new LoggingModules().getDistributedModules(),new AuthModule());
  injector.getInstance(LogAppenderInitializer.class).initialize();
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(Constants.Logging.SYSTEM_NAME,Constants.Logging.COMPONENT_NAME,Constants.Service.DATASET_EXECUTOR));
}","@Override protected void doInit(TwillContext context){
  this.user=DEFAULT_USER;
  CConfiguration cConf=getCConfiguration();
  Configuration hConf=getConfiguration();
  cConf.set(Constants.Dataset.Executor.ADDRESS,context.getHost().getHostName());
  injector=Guice.createInjector(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MetricsClientRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new LocationRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModule(),new DataSetServiceModules().getDistributedModule(),new LoggingModules().getDistributedModules(),new AuthModule(),new ExploreClientModule());
  injector.getInstance(LogAppenderInitializer.class).initialize();
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(Constants.Logging.SYSTEM_NAME,Constants.Logging.COMPONENT_NAME,Constants.Service.DATASET_EXECUTOR));
}"
7546,"@BeforeClass public static void init() throws Exception {
  testAppDir=tmpFolder.newFolder();
  File appDir=new File(testAppDir,""String_Node_Str"");
  File datasetDir=new File(testAppDir,""String_Node_Str"");
  File tmpDir=new File(testAppDir,""String_Node_Str"");
  appDir.mkdirs();
  datasetDir.mkdirs();
  tmpDir.mkdirs();
  CConfiguration configuration=CConfiguration.create();
  configuration.set(Constants.AppFabric.OUTPUT_DIR,appDir.getAbsolutePath());
  configuration.set(Constants.AppFabric.TEMP_DIR,tmpDir.getAbsolutePath());
  configuration.set(Constants.Dataset.Manager.OUTPUT_DIR,datasetDir.getAbsolutePath());
  configuration.set(Constants.Dataset.Manager.ADDRESS,""String_Node_Str"");
  configuration.set(MetricsConstants.ConfigKeys.SERVER_PORT,Integer.toString(Networks.getRandomPort()));
  configuration.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder(""String_Node_Str"").getAbsolutePath());
  configuration.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  configuration.setBoolean(Constants.Explore.EXPLORE_ENABLED,true);
  configuration.set(Constants.Explore.LOCAL_DATA_DIR,tmpFolder.newFolder(""String_Node_Str"").getAbsolutePath());
  if (OSDetector.isWindows()) {
    File binDir=new File(tmpDir,""String_Node_Str"");
    binDir.mkdir();
    copyTempFile(""String_Node_Str"",tmpDir);
    copyTempFile(""String_Node_Str"",binDir);
    System.setProperty(""String_Node_Str"",tmpDir.getAbsolutePath());
    System.load(new File(tmpDir,""String_Node_Str"").getAbsolutePath());
  }
  injector=Guice.createInjector(createDataFabricModule(configuration),new DataSetsModules().getInMemoryModule(),new DataSetServiceModules().getInMemoryModule(),new ConfigModule(configuration),new IOModule(),new AuthModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new AppFabricServiceRuntimeModule().getInMemoryModules(),new ServiceStoreModules().getInMemoryModule(),new ProgramRunnerRuntimeModule().getInMemoryModules(),new StreamServiceModule(){
    @Override protected void configure(){
      super.configure();
      bind(StreamHandler.class).in(Scopes.SINGLETON);
      bind(StreamFileJanitorService.class).to(LocalStreamFileJanitorService.class).in(Scopes.SINGLETON);
      expose(StreamHandler.class);
    }
  }
,new TestMetricsClientModule(),new MetricsHandlerModule(),new LoggingModules().getInMemoryModules(),new ExploreRuntimeModule().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      install(new FactoryModuleBuilder().implement(ApplicationManager.class,DefaultApplicationManager.class).build(ApplicationManagerFactory.class));
      install(new FactoryModuleBuilder().implement(StreamWriter.class,DefaultStreamWriter.class).build(StreamWriterFactory.class));
      install(new FactoryModuleBuilder().implement(ProcedureClient.class,DefaultProcedureClient.class).build(ProcedureClientFactory.class));
    }
  }
);
  injector.getInstance(InMemoryTransactionManager.class).startAndWait();
  locationFactory=injector.getInstance(LocationFactory.class);
  metricsQueryService=injector.getInstance(MetricsQueryService.class);
  metricsQueryService.startAndWait();
  metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  metricsCollectionService.startAndWait();
  logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
  logAppenderInitializer.initialize();
  httpHandler=injector.getInstance(AppFabricHttpHandler.class);
  datasetService=injector.getInstance(DatasetService.class);
  datasetService.startAndWait();
  DatasetFramework dsFramework=injector.getInstance(DatasetFramework.class);
  datasetFramework=new NamespacedDatasetFramework(dsFramework,new ReactorDatasetNamespace(configuration,DataSetAccessor.Namespace.USER));
  schedulerService=injector.getInstance(SchedulerService.class);
  schedulerService.startAndWait();
  discoveryClient=injector.getInstance(DiscoveryServiceClient.class);
  exploreExecutorService=injector.getInstance(ExploreExecutorService.class);
  exploreExecutorService.startAndWait();
}","@BeforeClass public static void init() throws Exception {
  testAppDir=tmpFolder.newFolder();
  File appDir=new File(testAppDir,""String_Node_Str"");
  File datasetDir=new File(testAppDir,""String_Node_Str"");
  File tmpDir=new File(testAppDir,""String_Node_Str"");
  appDir.mkdirs();
  datasetDir.mkdirs();
  tmpDir.mkdirs();
  CConfiguration configuration=CConfiguration.create();
  configuration.set(Constants.AppFabric.OUTPUT_DIR,appDir.getAbsolutePath());
  configuration.set(Constants.AppFabric.TEMP_DIR,tmpDir.getAbsolutePath());
  configuration.set(Constants.Dataset.Manager.OUTPUT_DIR,datasetDir.getAbsolutePath());
  configuration.set(Constants.Dataset.Manager.ADDRESS,""String_Node_Str"");
  configuration.set(MetricsConstants.ConfigKeys.SERVER_PORT,Integer.toString(Networks.getRandomPort()));
  configuration.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder(""String_Node_Str"").getAbsolutePath());
  configuration.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  configuration.setBoolean(Constants.Explore.EXPLORE_ENABLED,true);
  configuration.set(Constants.Explore.LOCAL_DATA_DIR,tmpFolder.newFolder(""String_Node_Str"").getAbsolutePath());
  if (OSDetector.isWindows()) {
    File binDir=new File(tmpDir,""String_Node_Str"");
    binDir.mkdir();
    copyTempFile(""String_Node_Str"",tmpDir);
    copyTempFile(""String_Node_Str"",binDir);
    System.setProperty(""String_Node_Str"",tmpDir.getAbsolutePath());
    System.load(new File(tmpDir,""String_Node_Str"").getAbsolutePath());
  }
  injector=Guice.createInjector(createDataFabricModule(configuration),new DataSetsModules().getInMemoryModule(),new DataSetServiceModules().getInMemoryModule(),new ConfigModule(configuration),new IOModule(),new AuthModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new AppFabricServiceRuntimeModule().getInMemoryModules(),new ServiceStoreModules().getInMemoryModule(),new ProgramRunnerRuntimeModule().getInMemoryModules(),new StreamServiceModule(){
    @Override protected void configure(){
      super.configure();
      bind(StreamHandler.class).in(Scopes.SINGLETON);
      bind(StreamFileJanitorService.class).to(LocalStreamFileJanitorService.class).in(Scopes.SINGLETON);
      expose(StreamHandler.class);
    }
  }
,new TestMetricsClientModule(),new MetricsHandlerModule(),new LoggingModules().getInMemoryModules(),new ExploreRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new AbstractModule(){
    @Override protected void configure(){
      install(new FactoryModuleBuilder().implement(ApplicationManager.class,DefaultApplicationManager.class).build(ApplicationManagerFactory.class));
      install(new FactoryModuleBuilder().implement(StreamWriter.class,DefaultStreamWriter.class).build(StreamWriterFactory.class));
      install(new FactoryModuleBuilder().implement(ProcedureClient.class,DefaultProcedureClient.class).build(ProcedureClientFactory.class));
    }
  }
);
  injector.getInstance(InMemoryTransactionManager.class).startAndWait();
  locationFactory=injector.getInstance(LocationFactory.class);
  metricsQueryService=injector.getInstance(MetricsQueryService.class);
  metricsQueryService.startAndWait();
  metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  metricsCollectionService.startAndWait();
  logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
  logAppenderInitializer.initialize();
  httpHandler=injector.getInstance(AppFabricHttpHandler.class);
  datasetService=injector.getInstance(DatasetService.class);
  datasetService.startAndWait();
  DatasetFramework dsFramework=injector.getInstance(DatasetFramework.class);
  datasetFramework=new NamespacedDatasetFramework(dsFramework,new ReactorDatasetNamespace(configuration,DataSetAccessor.Namespace.USER));
  schedulerService=injector.getInstance(SchedulerService.class);
  schedulerService.startAndWait();
  discoveryClient=injector.getInstance(DiscoveryServiceClient.class);
  exploreExecutorService=injector.getInstance(ExploreExecutorService.class);
  exploreExecutorService.startAndWait();
}"
7547,"/** 
 * Constructor.
 * @param streamConfig Stream configuration.
 * @param consumerConfig Consumer configuration.
 * @param hTable For communicate with HBase for storing polled entry states (not consumer state). This class isresponsible for closing the HTable.
 * @param reader For reading stream events. This class is responsible for closing the reader.
 */
public HBaseStreamFileConsumer(StreamConfig streamConfig,ConsumerConfig consumerConfig,HTable hTable,FileReader<StreamEventOffset,Iterable<StreamFileOffset>> reader,StreamConsumerStateStore stateStore,StreamConsumerState beginConsumerState,@Nullable ReadFilter extraFilter,AbstractRowKeyDistributor keyDistributor){
  super(streamConfig,consumerConfig,reader,stateStore,beginConsumerState,extraFilter);
  this.hTable=hTable;
  this.keyDistributor=keyDistributor;
  this.scanExecutor=createScanExecutor(streamConfig.getName());
}","/** 
 * Constructor.
 * @param streamConfig Stream configuration.
 * @param consumerConfig Consumer configuration.
 * @param hTable For communicate with HBase for storing polled entry states (not consumer state). This class isresponsible for closing the HTable.
 * @param reader For reading stream events. This class is responsible for closing the reader.
 */
public HBaseStreamFileConsumer(CConfiguration cConf,StreamConfig streamConfig,ConsumerConfig consumerConfig,HTable hTable,FileReader<StreamEventOffset,Iterable<StreamFileOffset>> reader,StreamConsumerStateStore stateStore,StreamConsumerState beginConsumerState,@Nullable ReadFilter extraFilter,AbstractRowKeyDistributor keyDistributor){
  super(cConf,streamConfig,consumerConfig,reader,stateStore,beginConsumerState,extraFilter);
  this.hTable=hTable;
  this.keyDistributor=keyDistributor;
  this.scanExecutor=createScanExecutor(streamConfig.getName());
}"
7548,"@Override protected StreamConsumer create(String tableName,StreamConfig streamConfig,ConsumerConfig consumerConfig,StreamConsumerStateStore stateStore,StreamConsumerState beginConsumerState,FileReader<StreamEventOffset,Iterable<StreamFileOffset>> reader,@Nullable ReadFilter extraFilter) throws IOException {
  String hBaseTableName=HBaseTableUtil.getHBaseTableName(tableName);
  HTableDescriptor htd=new HTableDescriptor(hBaseTableName);
  HColumnDescriptor hcd=new HColumnDescriptor(QueueEntryRow.COLUMN_FAMILY);
  htd.addFamily(hcd);
  hcd.setMaxVersions(1);
  int splits=cConf.getInt(Constants.Stream.CONSUMER_TABLE_PRESPLITS,1);
  byte[][] splitKeys=HBaseTableUtil.getSplitKeys(splits);
  tableUtil.createTableIfNotExists(getAdmin(),Bytes.toBytes(hBaseTableName),htd,splitKeys,QueueConstants.MAX_CREATE_TABLE_WAIT,TimeUnit.MILLISECONDS);
  HTable hTable=new HTable(hConf,hBaseTableName);
  hTable.setWriteBufferSize(Constants.Stream.HBASE_WRITE_BUFFER_SIZE);
  hTable.setAutoFlush(false);
  return new HBaseStreamFileConsumer(streamConfig,consumerConfig,hTable,reader,stateStore,beginConsumerState,extraFilter,HBaseQueueAdmin.ROW_KEY_DISTRIBUTOR);
}","@Override protected StreamConsumer create(String tableName,StreamConfig streamConfig,ConsumerConfig consumerConfig,StreamConsumerStateStore stateStore,StreamConsumerState beginConsumerState,FileReader<StreamEventOffset,Iterable<StreamFileOffset>> reader,@Nullable ReadFilter extraFilter) throws IOException {
  String hBaseTableName=HBaseTableUtil.getHBaseTableName(tableName);
  HTableDescriptor htd=new HTableDescriptor(hBaseTableName);
  HColumnDescriptor hcd=new HColumnDescriptor(QueueEntryRow.COLUMN_FAMILY);
  htd.addFamily(hcd);
  hcd.setMaxVersions(1);
  int splits=cConf.getInt(Constants.Stream.CONSUMER_TABLE_PRESPLITS,1);
  byte[][] splitKeys=HBaseTableUtil.getSplitKeys(splits);
  tableUtil.createTableIfNotExists(getAdmin(),Bytes.toBytes(hBaseTableName),htd,splitKeys,QueueConstants.MAX_CREATE_TABLE_WAIT,TimeUnit.MILLISECONDS);
  HTable hTable=new HTable(hConf,hBaseTableName);
  hTable.setWriteBufferSize(Constants.Stream.HBASE_WRITE_BUFFER_SIZE);
  hTable.setAutoFlush(false);
  return new HBaseStreamFileConsumer(cConf,streamConfig,consumerConfig,hTable,reader,stateStore,beginConsumerState,extraFilter,HBaseQueueAdmin.ROW_KEY_DISTRIBUTOR);
}"
7549,"@BeforeClass public static void init() throws Exception {
  testAppDir=tmpFolder.newFolder();
  File appDir=new File(testAppDir,""String_Node_Str"");
  File datasetDir=new File(testAppDir,""String_Node_Str"");
  File tmpDir=new File(testAppDir,""String_Node_Str"");
  appDir.mkdirs();
  datasetDir.mkdirs();
  tmpDir.mkdirs();
  CConfiguration configuration=CConfiguration.create();
  configuration.set(Constants.AppFabric.OUTPUT_DIR,appDir.getAbsolutePath());
  configuration.set(Constants.AppFabric.TEMP_DIR,tmpDir.getAbsolutePath());
  configuration.set(Constants.Dataset.Manager.OUTPUT_DIR,datasetDir.getAbsolutePath());
  configuration.set(Constants.Dataset.Manager.ADDRESS,""String_Node_Str"");
  configuration.set(MetricsConstants.ConfigKeys.SERVER_PORT,Integer.toString(Networks.getRandomPort()));
  configuration.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder(""String_Node_Str"").getAbsolutePath());
  configuration.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  configuration.setBoolean(Constants.Explore.CFG_EXPLORE_ENABLED,true);
  configuration.set(Constants.Explore.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder(""String_Node_Str"").getAbsolutePath());
  if (System.getProperty(""String_Node_Str"").startsWith(""String_Node_Str"")) {
    File binDir=new File(tmpDir,""String_Node_Str"");
    binDir.mkdir();
    copyTempFile(""String_Node_Str"",tmpDir);
    copyTempFile(""String_Node_Str"",binDir);
    System.setProperty(""String_Node_Str"",tmpDir.getAbsolutePath());
    System.load(new File(tmpDir,""String_Node_Str"").getAbsolutePath());
  }
  injector=Guice.createInjector(createDataFabricModule(configuration),new DataSetServiceModules().getInMemoryModule(),new ConfigModule(configuration),new IOModule(),new AuthModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new AppFabricServiceRuntimeModule().getInMemoryModules(),new ProgramRunnerRuntimeModule().getInMemoryModules(),new StreamServiceModule(){
    @Override protected void configure(){
      super.configure();
      bind(StreamHandler.class).in(Scopes.SINGLETON);
      bind(StreamFileJanitorService.class).to(LocalStreamFileJanitorService.class).in(Scopes.SINGLETON);
      expose(StreamHandler.class);
    }
  }
,new TestMetricsClientModule(),new MetricsHandlerModule(),new LoggingModules().getInMemoryModules(),new ExploreRuntimeModule().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      install(new FactoryModuleBuilder().implement(ApplicationManager.class,DefaultApplicationManager.class).build(ApplicationManagerFactory.class));
      install(new FactoryModuleBuilder().implement(StreamWriter.class,DefaultStreamWriter.class).build(StreamWriterFactory.class));
      install(new FactoryModuleBuilder().implement(ProcedureClient.class,DefaultProcedureClient.class).build(ProcedureClientFactory.class));
    }
  }
);
  injector.getInstance(InMemoryTransactionManager.class).startAndWait();
  locationFactory=injector.getInstance(LocationFactory.class);
  metricsQueryService=injector.getInstance(MetricsQueryService.class);
  metricsQueryService.startAndWait();
  metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  metricsCollectionService.startAndWait();
  logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
  logAppenderInitializer.initialize();
  httpHandler=injector.getInstance(AppFabricHttpHandler.class);
  datasetService=injector.getInstance(DatasetService.class);
  datasetService.startAndWait();
  datasetFramework=injector.getInstance(DatasetFramework.class);
  schedulerService=injector.getInstance(SchedulerService.class);
  schedulerService.startAndWait();
  discoveryClient=injector.getInstance(DiscoveryServiceClient.class);
  exploreExecutorService=injector.getInstance(ExploreExecutorService.class);
  exploreExecutorService.startAndWait();
}","@BeforeClass public static void init() throws Exception {
  testAppDir=tmpFolder.newFolder();
  File appDir=new File(testAppDir,""String_Node_Str"");
  File datasetDir=new File(testAppDir,""String_Node_Str"");
  File tmpDir=new File(testAppDir,""String_Node_Str"");
  appDir.mkdirs();
  datasetDir.mkdirs();
  tmpDir.mkdirs();
  CConfiguration configuration=CConfiguration.create();
  configuration.set(Constants.AppFabric.OUTPUT_DIR,appDir.getAbsolutePath());
  configuration.set(Constants.AppFabric.TEMP_DIR,tmpDir.getAbsolutePath());
  configuration.set(Constants.Dataset.Manager.OUTPUT_DIR,datasetDir.getAbsolutePath());
  configuration.set(Constants.Dataset.Manager.ADDRESS,""String_Node_Str"");
  configuration.set(MetricsConstants.ConfigKeys.SERVER_PORT,Integer.toString(Networks.getRandomPort()));
  configuration.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder(""String_Node_Str"").getAbsolutePath());
  configuration.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  configuration.setBoolean(Constants.Explore.CFG_EXPLORE_ENABLED,true);
  configuration.set(Constants.Explore.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder(""String_Node_Str"").getAbsolutePath());
  if (System.getProperty(""String_Node_Str"").startsWith(""String_Node_Str"")) {
    File binDir=new File(tmpDir,""String_Node_Str"");
    binDir.mkdir();
    copyTempFile(""String_Node_Str"",tmpDir);
    copyTempFile(""String_Node_Str"",binDir);
    System.setProperty(""String_Node_Str"",tmpDir.getAbsolutePath());
    System.load(new File(tmpDir,""String_Node_Str"").getAbsolutePath());
  }
  injector=Guice.createInjector(createDataFabricModule(configuration),new DataSetServiceModules().getInMemoryModule(),new ConfigModule(configuration),new IOModule(),new AuthModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new AppFabricServiceRuntimeModule().getInMemoryModules(),new ServiceStoreModules().getInMemoryModule(),new ProgramRunnerRuntimeModule().getInMemoryModules(),new StreamServiceModule(){
    @Override protected void configure(){
      super.configure();
      bind(StreamHandler.class).in(Scopes.SINGLETON);
      bind(StreamFileJanitorService.class).to(LocalStreamFileJanitorService.class).in(Scopes.SINGLETON);
      expose(StreamHandler.class);
    }
  }
,new TestMetricsClientModule(),new MetricsHandlerModule(),new LoggingModules().getInMemoryModules(),new ExploreRuntimeModule().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      install(new FactoryModuleBuilder().implement(ApplicationManager.class,DefaultApplicationManager.class).build(ApplicationManagerFactory.class));
      install(new FactoryModuleBuilder().implement(StreamWriter.class,DefaultStreamWriter.class).build(StreamWriterFactory.class));
      install(new FactoryModuleBuilder().implement(ProcedureClient.class,DefaultProcedureClient.class).build(ProcedureClientFactory.class));
    }
  }
);
  injector.getInstance(InMemoryTransactionManager.class).startAndWait();
  locationFactory=injector.getInstance(LocationFactory.class);
  metricsQueryService=injector.getInstance(MetricsQueryService.class);
  metricsQueryService.startAndWait();
  metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  metricsCollectionService.startAndWait();
  logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
  logAppenderInitializer.initialize();
  httpHandler=injector.getInstance(AppFabricHttpHandler.class);
  datasetService=injector.getInstance(DatasetService.class);
  datasetService.startAndWait();
  datasetFramework=injector.getInstance(DatasetFramework.class);
  schedulerService=injector.getInstance(SchedulerService.class);
  schedulerService.startAndWait();
  discoveryClient=injector.getInstance(DiscoveryServiceClient.class);
  exploreExecutorService=injector.getInstance(ExploreExecutorService.class);
  exploreExecutorService.startAndWait();
}"
7550,"@Override public Module getInMemoryModules(){
  return Modules.combine(new AppFabricServiceModule(),new ServiceStoreModules().getInMemoryModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      MapBinder<String,ReactorServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,ReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(InMemoryLogSaverServiceMananger.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(InMemoryTransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(InMemoryLogSaverServiceMananger.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(InMemoryReactorServiceManager.class);
    }
  }
);
}","@Override public Module getInMemoryModules(){
  return Modules.combine(new AppFabricServiceModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      MapBinder<String,ReactorServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,ReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(InMemoryLogSaverServiceMananger.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(InMemoryTransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(InMemoryLogSaverServiceMananger.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(InMemoryReactorServiceManager.class);
    }
  }
);
}"
7551,"private Injector createPersistentModules(){
  ImmutableList<Module> singleNodeModules=ImmutableList.of(new ConfigModule(cConf),new LocalConfigModule(),new IOModule(),new AuthModule(),new LocationRuntimeModule().getSingleNodeModules(),new DiscoveryRuntimeModule().getSingleNodeModules(),new ProgramRunnerRuntimeModule().getSingleNodeModules(),new DataFabricModules().getSingleNodeModules(),new MetricsClientRuntimeModule().getMapReduceModules(taskContext),new LoggingModules().getSingleNodeModules());
  return Guice.createInjector(singleNodeModules);
}","private Injector createPersistentModules(){
  ImmutableList<Module> singleNodeModules=ImmutableList.of(new ConfigModule(cConf),new LocalConfigModule(),new IOModule(),new AuthModule(),new LocationRuntimeModule().getSingleNodeModules(),new DiscoveryRuntimeModule().getSingleNodeModules(),new ProgramRunnerRuntimeModule().getSingleNodeModules(),new DataFabricModules().getSingleNodeModules(),new MetricsClientRuntimeModule().getMapReduceModules(taskContext),new LoggingModules().getSingleNodeModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(ProgramServiceDiscovery.class).to(DistributedProgramServiceDiscovery.class).in(Scopes.SINGLETON);
    }
  }
);
  return Guice.createInjector(singleNodeModules);
}"
7552,"private Injector createInMemoryModules(){
  ImmutableList<Module> inMemoryModules=ImmutableList.of(new ConfigModule(cConf),new LocalConfigModule(),new IOModule(),new AuthModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new ProgramRunnerRuntimeModule().getInMemoryModules(),new DataFabricModules().getInMemoryModules(),new MetricsClientRuntimeModule().getNoopModules(),new LoggingModules().getInMemoryModules());
  return Guice.createInjector(inMemoryModules);
}","private Injector createInMemoryModules(){
  ImmutableList<Module> inMemoryModules=ImmutableList.of(new ConfigModule(cConf),new LocalConfigModule(),new IOModule(),new AuthModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new ProgramRunnerRuntimeModule().getInMemoryModules(),new DataFabricModules().getInMemoryModules(),new MetricsClientRuntimeModule().getNoopModules(),new LoggingModules().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(ProgramServiceDiscovery.class).to(DistributedProgramServiceDiscovery.class).in(Scopes.SINGLETON);
    }
  }
);
  return Guice.createInjector(inMemoryModules);
}"
7553,"/** 
 * Executes an HTTP request to the url provided.
 * @param requestMethod HTTP method of the request.
 * @param url URL of the request.
 * @param headers Headers of the request.
 * @param body Body of the request. If provided, bodySrc must be null.
 * @param bodySrc Body of the request as an {@link InputStream}. If provided, body must be null.
 * @return repsonse of the request
 * @throws IOException
 */
public static HttpResponse doRequest(String requestMethod,URL url,@Nullable Map<String,String> headers,@Nullable byte[] body,@Nullable InputStream bodySrc) throws IOException {
  Preconditions.checkArgument(!(body != null && bodySrc != null),""String_Node_Str"");
  HttpURLConnection conn=(HttpURLConnection)url.openConnection();
  conn.setRequestMethod(requestMethod);
  if (headers != null) {
    for (    Map.Entry<String,String> header : headers.entrySet()) {
      conn.setRequestProperty(header.getKey(),header.getValue());
    }
  }
  if (body != null || bodySrc != null) {
    conn.setDoOutput(true);
  }
  conn.connect();
  try {
    if (body != null || bodySrc != null) {
      OutputStream os=conn.getOutputStream();
      if (body != null) {
        os.write(body);
      }
 else {
        ByteStreams.copy(bodySrc,os);
      }
    }
    try {
      if (isSuccessful(conn.getResponseCode())) {
        return new HttpResponse(conn.getResponseCode(),conn.getResponseMessage(),ByteStreams.toByteArray(conn.getInputStream()));
      }
    }
 catch (    FileNotFoundException e) {
    }
    InputStream es=conn.getErrorStream();
    byte[] content=(es == null) ? new byte[0] : ByteStreams.toByteArray(es);
    return new HttpResponse(conn.getResponseCode(),conn.getResponseMessage(),content);
  }
  finally {
    conn.disconnect();
  }
}","/** 
 * Executes an HTTP request to the url provided.
 * @param requestMethod HTTP method of the request.
 * @param url URL of the request.
 * @param headers Headers of the request.
 * @param body Body of the request. If provided, bodySrc must be null.
 * @param bodySrc Body of the request as an {@link InputStream}. If provided, body must be null.
 * @return repsonse of the request
 * @throws IOException
 */
public static HttpResponse doRequest(String requestMethod,URL url,@Nullable Map<String,String> headers,@Nullable String body,@Nullable InputStream bodySrc) throws IOException {
  return doRequest(requestMethod,url,headers,body != null ? body.getBytes(Charsets.UTF_8) : null,bodySrc);
}"
7554,"/** 
 * Executes a GET request to the url provided.
 * @param url URL of the request.
 * @return response of the request
 * @throws IOException
 */
public static HttpResponse get(URL url) throws IOException {
  return doRequest(""String_Node_Str"",url,null,null,null);
}","/** 
 * Executes a GET request to the url provided.
 * @param url URL of the request.
 * @return response of the request
 * @throws IOException
 */
public static HttpResponse get(URL url) throws IOException {
  return doRequest(""String_Node_Str"",url,null,(byte[])null,null);
}"
7555,"/** 
 * Executes a DELETE request to the url provided.
 * @param url URL of the request.
 * @return response of the request
 * @throws IOException
 */
public static HttpResponse delete(URL url) throws IOException {
  return doRequest(""String_Node_Str"",url,null,null,null);
}","/** 
 * Executes a DELETE request to the url provided.
 * @param url URL of the request.
 * @return response of the request
 * @throws IOException
 */
public static HttpResponse delete(URL url) throws IOException {
  return doRequest(""String_Node_Str"",url,null,(byte[])null,null);
}"
7556,"@Override public Module getInMemoryModules(){
  return Modules.combine(new AppFabricServiceModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      MapBinder<String,ReactorServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,ReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(InMemoryLogSaverServiceMananger.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(InMemoryTransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(InMemoryLogSaverServiceMananger.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(InMemoryReactorServiceManager.class);
    }
  }
);
}","@Override public Module getInMemoryModules(){
  return Modules.combine(new AppFabricServiceModule(),new ServiceStoreModules().getInMemoryModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      MapBinder<String,ReactorServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,ReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(InMemoryLogSaverServiceMananger.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(InMemoryTransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(InMemoryLogSaverServiceMananger.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(InMemoryReactorServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(InMemoryReactorServiceManager.class);
    }
  }
);
}"
7557,"/** 
 * Configures a   {@link com.google.inject.Binder} via the exposed methods.
 */
@Override protected void configure(){
  bind(LogWriter.class).to(LocalLogWriter.class);
  expose(LogWriter.class);
  bind(ServiceAnnouncer.class).to(DiscoveryServiceAnnouncer.class);
  bind(QueueReaderFactory.class).in(Scopes.SINGLETON);
  MapBinder<ProgramRunnerFactory.Type,ProgramRunner> runnerFactoryBinder=MapBinder.newMapBinder(binder(),ProgramRunnerFactory.Type.class,ProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.FLOW).to(FlowProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.FLOWLET).to(FlowletProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.PROCEDURE).to(ProcedureProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.MAPREDUCE).to(MapReduceProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.WORKFLOW).to(WorkflowProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.WEBAPP).to(WebappProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.SERVICE).to(InMemoryServiceRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.RUNNABLE).to(InMemoryRunnableRunner.class);
  bind(ProgramRunnerFactory.class).to(InMemoryFlowProgramRunnerFactory.class).in(Scopes.SINGLETON);
  expose(ProgramRunnerFactory.class);
  bind(ProgramRuntimeService.class).to(InMemoryProgramRuntimeService.class).in(Scopes.SINGLETON);
  expose(ProgramRuntimeService.class);
  install(new DataFabricFacadeModule());
  install(new ProgramServiceDiscoveryModules().getInMemoryModules());
  install(new FactoryModuleBuilder().implement(JarHttpHandler.class,IntactJarHttpHandler.class).build(WebappHttpHandlerFactory.class));
}","/** 
 * Configures a   {@link com.google.inject.Binder} via the exposed methods.
 */
@Override protected void configure(){
  bind(LogWriter.class).to(LocalLogWriter.class);
  expose(LogWriter.class);
  bind(ServiceAnnouncer.class).to(DiscoveryServiceAnnouncer.class);
  bind(QueueReaderFactory.class).in(Scopes.SINGLETON);
  MapBinder<ProgramRunnerFactory.Type,ProgramRunner> runnerFactoryBinder=MapBinder.newMapBinder(binder(),ProgramRunnerFactory.Type.class,ProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.FLOW).to(FlowProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.FLOWLET).to(FlowletProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.PROCEDURE).to(ProcedureProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.MAPREDUCE).to(MapReduceProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.WORKFLOW).to(WorkflowProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.WEBAPP).to(WebappProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.SERVICE).to(InMemoryServiceRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.RUNNABLE).to(InMemoryRunnableRunner.class);
  bind(ProgramRunnerFactory.class).to(InMemoryFlowProgramRunnerFactory.class).in(Scopes.SINGLETON);
  expose(ProgramRunnerFactory.class);
  bind(ProgramRuntimeService.class).to(InMemoryProgramRuntimeService.class).in(Scopes.SINGLETON);
  expose(ProgramRuntimeService.class);
  install(new DataFabricFacadeModule());
  bind(ProgramServiceDiscovery.class).to(InMemoryProgramServiceDiscovery.class).in(Scopes.SINGLETON);
  expose(ProgramServiceDiscovery.class);
  install(new FactoryModuleBuilder().implement(JarHttpHandler.class,IntactJarHttpHandler.class).build(WebappHttpHandlerFactory.class));
}"
7558,"/** 
 * Receives an input containing application specification and location and verifies both.
 * @param input An instance of {@link com.continuuity.internal.app.deploy.pipeline.ApplicationSpecLocation}
 */
@Override public void process(ApplicationSpecLocation input) throws Exception {
  ApplicationSpecification specification=input.getSpecification();
  for (  Map.Entry<String,String> moduleEntry : specification.getDatasetModules().entrySet()) {
    JarClassLoader classLoader=new JarClassLoader(input.getArchive());
    @SuppressWarnings(""String_Node_Str"") Class<?> clazz=classLoader.loadClass(moduleEntry.getValue());
    String moduleName=moduleEntry.getKey();
    try {
      if (DatasetModule.class.isAssignableFrom(clazz)) {
        datasetFramework.addModule(moduleName,(DatasetModule)clazz.newInstance());
      }
 else       if (Dataset.class.isAssignableFrom(clazz)) {
        datasetFramework.addModule(moduleName,new SingleTypeModule((Class<Dataset>)clazz));
      }
 else {
        String msg=String.format(""String_Node_Str"",clazz.getName());
        throw new IllegalArgumentException(msg);
      }
    }
 catch (    ModuleConflictException e) {
      LOG.info(""String_Node_Str"" + moduleName + ""String_Node_Str"");
    }
  }
  emit(input);
}","/** 
 * Receives an input containing application specification and location and verifies both.
 * @param input An instance of {@link com.continuuity.internal.app.deploy.pipeline.ApplicationSpecLocation}
 */
@Override public void process(ApplicationSpecLocation input) throws Exception {
  ApplicationSpecification specification=input.getSpecification();
  for (  Map.Entry<String,String> moduleEntry : specification.getDatasetModules().entrySet()) {
    JarClassLoader classLoader=new JarClassLoader(input.getArchive());
    @SuppressWarnings(""String_Node_Str"") Class<?> clazz=classLoader.loadClass(moduleEntry.getValue());
    String moduleName=moduleEntry.getKey();
    try {
      if (DatasetModule.class.isAssignableFrom(clazz)) {
        datasetFramework.addModule(moduleName,(DatasetModule)clazz.newInstance());
      }
 else       if (Dataset.class.isAssignableFrom(clazz)) {
        if (!datasetFramework.hasType(clazz.getName())) {
          datasetFramework.addModule(moduleName,new SingleTypeModule((Class<Dataset>)clazz));
        }
      }
 else {
        String msg=String.format(""String_Node_Str"",clazz.getName());
        throw new IllegalArgumentException(msg);
      }
    }
 catch (    ModuleConflictException e) {
      LOG.info(""String_Node_Str"" + moduleName + ""String_Node_Str"");
    }
  }
  emit(input);
}"
7559,"private void writeToObjectStore(byte[] key,T object) throws Exception {
  objectStore.write(key,object);
}","private void writeToObjectStore(byte[] key,T object){
  objectStore.write(key,object);
}"
7560,"/** 
 * Read all the objects from objectStore via index. Returns all the objects that match the secondaryKey. Returns an empty list if no value is found. Never returns null.
 * @param secondaryKey for the lookup.
 * @return List of Objects matching the secondaryKey.
 */
public List<T> readAllByIndex(byte[] secondaryKey) throws Exception {
  ImmutableList.Builder<T> resultList=ImmutableList.builder();
  Row row=index.get(secondaryKey);
  if (!row.isEmpty()) {
    for (    byte[] column : row.getColumns().keySet()) {
      T obj=objectStore.read(column);
      resultList.add(obj);
    }
  }
  return resultList.build();
}","/** 
 * Read all the objects from objectStore via index. Returns all the objects that match the secondaryKey. Returns an empty list if no value is found. Never returns null.
 * @param secondaryKey for the lookup.
 * @return List of Objects matching the secondaryKey.
 */
public List<T> readAllByIndex(byte[] secondaryKey){
  ImmutableList.Builder<T> resultList=ImmutableList.builder();
  Row row=index.get(secondaryKey);
  if (!row.isEmpty()) {
    for (    byte[] column : row.getColumns().keySet()) {
      T obj=objectStore.read(column);
      resultList.add(obj);
    }
  }
  return resultList.build();
}"
7561,"public void write(byte[] key,T object) throws Exception {
  Row row=index.get(getPrefixedPrimaryKey(key));
  if (!row.isEmpty()) {
    Set<byte[]> columnsToDelete=row.getColumns().keySet();
    deleteSecondaryKeys(key,columnsToDelete.toArray(new byte[columnsToDelete.size()][]));
  }
  writeToObjectStore(key,object);
}","public void write(byte[] key,T object){
  Row row=index.get(getPrefixedPrimaryKey(key));
  if (!row.isEmpty()) {
    Set<byte[]> columnsToDelete=row.getColumns().keySet();
    deleteSecondaryKeys(key,columnsToDelete.toArray(new byte[columnsToDelete.size()][]));
  }
  writeToObjectStore(key,object);
}"
7562,"/** 
 * Adds   {@link ObjectStore} data set to be created at application deploy if not exists.
 * @param configurer application configurer
 * @param datasetName data set name
 * @param type type of objects to be stored in {@link ObjectStore}
 * @param props any additional data set properties
 * @throws UnsupportedTypeException
 */
public static void createObjectStore(ApplicationConfigurer configurer,String datasetName,Type type,DatasetProperties props) throws UnsupportedTypeException {
  configurer.createDataSet(datasetName,ObjectStore.class,objectStoreProperties(type,props));
}","/** 
 * Same as   {@link #createObjectStore(ApplicationConfigurer,String,Type,DatasetProperties)} but with emptyproperties.
 */
public static void createObjectStore(ApplicationConfigurer configurer,String datasetName,Type type) throws UnsupportedTypeException {
  createObjectStore(configurer,datasetName,type,DatasetProperties.EMPTY);
}"
7563,"/** 
 * Adds   {@link MultiObjectStore} data set to be created at application deploy if not exists.
 * @param configurer application configurer
 * @param datasetName data set name
 * @param type type of objects to be stored in {@link ObjectStore}
 * @param props any additional data set properties
 * @throws UnsupportedTypeException
 */
public static void createMultiObjectStore(ApplicationConfigurer configurer,String datasetName,Type type,DatasetProperties props) throws UnsupportedTypeException {
  configurer.createDataSet(datasetName,MultiObjectStore.class,objectStoreProperties(type,props));
}","/** 
 * Same as   {@link #createMultiObjectStore(ApplicationConfigurer,String,Type,DatasetProperties)} but with emptyproperties.
 */
public static void createMultiObjectStore(ApplicationConfigurer configurer,String datasetName,Type type) throws UnsupportedTypeException {
  createMultiObjectStore(configurer,datasetName,type,DatasetProperties.EMPTY);
}"
7564,"/** 
 * Adds   {@link IndexedObjectStore} data set to be created at application deploy if not exists.
 * @param configurer application configurer
 * @param datasetName data set name
 * @param type type of objects to be stored in {@link IndexedObjectStore}
 * @param props any additional data set properties
 * @throws UnsupportedTypeException
 */
public static void createIndexedObjectStore(ApplicationConfigurer configurer,String datasetName,Type type,DatasetProperties props) throws UnsupportedTypeException {
  configurer.createDataSet(datasetName,IndexedObjectStore.class,objectStoreProperties(type,props));
}","/** 
 * Same as   {@link #createIndexedObjectStore(ApplicationConfigurer,String,Type,DatasetProperties)} but with emptyproperties.
 */
public static void createIndexedObjectStore(ApplicationConfigurer configurer,String datasetName,Type type) throws UnsupportedTypeException {
  createIndexedObjectStore(configurer,datasetName,type,DatasetProperties.EMPTY);
}"
7565,"@Override public DatasetInstanceSpec configure(String instanceName,DatasetInstanceProperties properties){
  return new DatasetInstanceSpec.Builder(instanceName,getName()).properties(properties.getProperties()).datasets(tableDef.configure(""String_Node_Str"",properties.getProperties(""String_Node_Str""))).build();
}","@Override public DatasetSpecification configure(String instanceName,DatasetProperties properties){
  return DatasetSpecification.builder(instanceName,getName()).properties(properties.getProperties()).datasets(tableDef.configure(""String_Node_Str"",properties)).build();
}"
7566,"@Override public KeyStructValueTable getDataset(DatasetInstanceSpec spec) throws IOException {
  Table table=tableDef.getDataset(spec.getSpecification(""String_Node_Str""));
  return new KeyStructValueTable(spec.getName(),table);
}","@Override public KeyStructValueTable getDataset(DatasetSpecification spec) throws IOException {
  Table table=tableDef.getDataset(spec.getSpecification(""String_Node_Str""));
  return new KeyStructValueTable(spec.getName(),table);
}"
7567,"@Override public boolean equals(Object o){
  if (this == o)   return true;
  if (o == null || getClass() != o.getClass())   return false;
  Value that=(Value)o;
  return Objects.equal(this.name,that.name) && Objects.equal(this.ints,that.ints);
}","@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  Value that=(Value)o;
  return Objects.equal(this.name,that.name) && Objects.equal(this.ints,that.ints);
}"
7568,"@Override public DatasetAdmin getAdmin(DatasetInstanceSpec spec) throws IOException {
  return tableDef.getAdmin(spec.getSpecification(""String_Node_Str""));
}","@Override public DatasetAdmin getAdmin(DatasetSpecification spec) throws IOException {
  return tableDef.getAdmin(spec.getSpecification(""String_Node_Str""));
}"
7569,"static TransactionService createTxService(String zkConnectionString,int txServicePort,Configuration hConf,final File outPath){
  final CConfiguration cConf=CConfiguration.create();
  cConf.unset(Constants.CFG_HDFS_USER);
  cConf.set(Constants.Zookeeper.QUORUM,zkConnectionString);
  cConf.set(Constants.Transaction.Service.CFG_DATA_TX_BIND_PORT,Integer.toString(txServicePort));
  cConf.setBoolean(Constants.Transaction.Manager.CFG_DO_PERSIST,true);
  final DataFabricDistributedModule dfModule=new DataFabricDistributedModule();
  final Injector injector=Guice.createInjector(dfModule,new ConfigModule(cConf,hConf),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(LocationFactory.class).toInstance(new LocalLocationFactory(outPath));
    }
  }
);
  injector.getInstance(ZKClientService.class).startAndWait();
  return injector.getInstance(TransactionService.class);
}","static TransactionService createTxService(String zkConnectionString,int txServicePort,Configuration hConf,final File outPath){
  final CConfiguration cConf=CConfiguration.create();
  cConf.unset(Constants.CFG_HDFS_USER);
  cConf.set(Constants.Zookeeper.QUORUM,zkConnectionString);
  cConf.set(Constants.Transaction.Service.CFG_DATA_TX_BIND_PORT,Integer.toString(txServicePort));
  cConf.setBoolean(Constants.Transaction.Manager.CFG_DO_PERSIST,true);
  final DataFabricDistributedModule dfModule=new DataFabricDistributedModule();
  final Injector injector=Guice.createInjector(dfModule,new ConfigModule(cConf,hConf),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(LocationFactory.class).toInstance(new LocalLocationFactory(outPath));
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
    }
  }
);
  injector.getInstance(ZKClientService.class).startAndWait();
  return injector.getInstance(TransactionService.class);
}"
7570,"@Override protected void configure(){
  bind(LocationFactory.class).toInstance(new LocalLocationFactory(outPath));
}","@Override protected void configure(){
  bind(LocationFactory.class).toInstance(new LocalLocationFactory(outPath));
  bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
}"
7571,"@Test(timeout=60000) public void testHA() throws Exception {
  HBaseTestingUtility hBaseTestingUtility=new HBaseTestingUtility();
  hBaseTestingUtility.startMiniDFSCluster(1);
  Configuration hConf=hBaseTestingUtility.getConfiguration();
  hConf.setBoolean(""String_Node_Str"",true);
  InMemoryZKServer zkServer=InMemoryZKServer.builder().build();
  zkServer.startAndWait();
  try {
    CConfiguration cConf=CConfiguration.create();
    cConf.unset(Constants.CFG_HDFS_USER);
    cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
    cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
    Injector injector=Guice.createInjector(new ConfigModule(cConf),new ZKClientModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules());
    ZKClientService zkClient=injector.getInstance(ZKClientService.class);
    zkClient.startAndWait();
    final OrderedColumnarTable table=createTable(""String_Node_Str"",cConf);
    try {
      TransactionSystemClient txClient=injector.getInstance(TransactionSystemClient.class);
      TransactionExecutor txExecutor=new DefaultTransactionExecutor(txClient,ImmutableList.of((TransactionAware)table));
      TransactionService first=createTxService(zkServer.getConnectionStr(),Networks.getRandomPort(),hConf,tmpFolder.newFolder());
      first.startAndWait();
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,null,""String_Node_Str"");
      TransactionService second=createTxService(zkServer.getConnectionStr(),Networks.getRandomPort(),hConf,tmpFolder.newFolder());
      second.startAndWait();
      TimeUnit.SECONDS.sleep(1);
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,""String_Node_Str"",""String_Node_Str"");
      first.stopAndWait();
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,""String_Node_Str"",""String_Node_Str"");
      TransactionService third=createTxService(zkServer.getConnectionStr(),Networks.getRandomPort(),hConf,tmpFolder.newFolder());
      third.start();
      second.stopAndWait();
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,""String_Node_Str"",""String_Node_Str"");
      third.stop();
    }
  finally {
      dropTable(""String_Node_Str"",cConf);
      zkClient.stopAndWait();
    }
  }
  finally {
    zkServer.stop();
  }
}","@Test(timeout=60000) public void testHA() throws Exception {
  HBaseTestingUtility hBaseTestingUtility=new HBaseTestingUtility();
  hBaseTestingUtility.startMiniDFSCluster(1);
  Configuration hConf=hBaseTestingUtility.getConfiguration();
  hConf.setBoolean(""String_Node_Str"",true);
  InMemoryZKServer zkServer=InMemoryZKServer.builder().build();
  zkServer.startAndWait();
  try {
    CConfiguration cConf=CConfiguration.create();
    cConf.unset(Constants.CFG_HDFS_USER);
    cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
    cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
    Injector injector=Guice.createInjector(new ConfigModule(cConf),new ZKClientModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getDistributedModules(),new AbstractModule(){
      @Override protected void configure(){
        bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
      }
    }
,new DataFabricModules().getDistributedModules());
    ZKClientService zkClient=injector.getInstance(ZKClientService.class);
    zkClient.startAndWait();
    final OrderedColumnarTable table=createTable(""String_Node_Str"",cConf);
    try {
      TransactionSystemClient txClient=injector.getInstance(TransactionSystemClient.class);
      TransactionExecutor txExecutor=new DefaultTransactionExecutor(txClient,ImmutableList.of((TransactionAware)table));
      TransactionService first=createTxService(zkServer.getConnectionStr(),Networks.getRandomPort(),hConf,tmpFolder.newFolder());
      first.startAndWait();
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,null,""String_Node_Str"");
      TransactionService second=createTxService(zkServer.getConnectionStr(),Networks.getRandomPort(),hConf,tmpFolder.newFolder());
      second.startAndWait();
      TimeUnit.SECONDS.sleep(1);
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,""String_Node_Str"",""String_Node_Str"");
      first.stopAndWait();
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,""String_Node_Str"",""String_Node_Str"");
      TransactionService third=createTxService(zkServer.getConnectionStr(),Networks.getRandomPort(),hConf,tmpFolder.newFolder());
      third.start();
      second.stopAndWait();
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,""String_Node_Str"",""String_Node_Str"");
      third.stop();
    }
  finally {
      dropTable(""String_Node_Str"",cConf);
      zkClient.stopAndWait();
    }
  }
  finally {
    zkServer.stop();
  }
}"
7572,"@Override public void configure(){
  bind(MetaDataTable.class).to(SerializingMetaDataTable.class).in(Singleton.class);
  bind(LevelDBOcTableService.class).toInstance(LevelDBOcTableService.getInstance());
  bind(TransactionStateStorage.class).annotatedWith(Names.named(""String_Node_Str"")).to(LocalFileTransactionStateStorage.class).in(Singleton.class);
  bind(TransactionStateStorage.class).toProvider(TransactionStateStorageProvider.class).in(Singleton.class);
  bind(InMemoryTransactionManager.class).in(Singleton.class);
  bind(TransactionSystemClient.class).to(InMemoryTxSystemClient.class).in(Singleton.class);
  bind(DataSetAccessor.class).to(LocalDataSetAccessor.class).in(Singleton.class);
  bind(QueueClientFactory.class).to(LevelDBQueueClientFactory.class).in(Singleton.class);
  bind(QueueAdmin.class).to(LevelDBQueueAdmin.class).in(Singleton.class);
  bind(StreamCoordinator.class).to(InMemoryStreamCoordinator.class).in(Singleton.class);
  bind(StreamConsumerStateStoreFactory.class).to(LevelDBStreamConsumerStateStoreFactory.class).in(Singleton.class);
  bind(StreamAdmin.class).to(LevelDBStreamFileAdmin.class).in(Singleton.class);
  bind(StreamConsumerFactory.class).to(LevelDBStreamFileConsumerFactory.class).in(Singleton.class);
  bind(StreamFileWriterFactory.class).to(LocationStreamFileWriterFactory.class).in(Singleton.class);
  install(new FactoryModuleBuilder().implement(TransactionExecutor.class,DefaultTransactionExecutor.class).build(TransactionExecutorFactory.class));
}","@Override public void configure(){
  bind(MetaDataTable.class).to(SerializingMetaDataTable.class).in(Singleton.class);
  bind(LevelDBOcTableService.class).toInstance(LevelDBOcTableService.getInstance());
  bind(TransactionStateStorage.class).annotatedWith(Names.named(""String_Node_Str"")).to(LocalFileTransactionStateStorage.class).in(Singleton.class);
  bind(TransactionStateStorage.class).toProvider(TransactionStateStorageProvider.class).in(Singleton.class);
  bind(InMemoryTransactionManager.class).in(Singleton.class);
  bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
  bind(TransactionSystemClient.class).to(InMemoryTxSystemClient.class).in(Singleton.class);
  bind(DataSetAccessor.class).to(LocalDataSetAccessor.class).in(Singleton.class);
  bind(QueueClientFactory.class).to(LevelDBQueueClientFactory.class).in(Singleton.class);
  bind(QueueAdmin.class).to(LevelDBQueueAdmin.class).in(Singleton.class);
  bind(StreamCoordinator.class).to(InMemoryStreamCoordinator.class).in(Singleton.class);
  bind(StreamConsumerStateStoreFactory.class).to(LevelDBStreamConsumerStateStoreFactory.class).in(Singleton.class);
  bind(StreamAdmin.class).to(LevelDBStreamFileAdmin.class).in(Singleton.class);
  bind(StreamConsumerFactory.class).to(LevelDBStreamFileConsumerFactory.class).in(Singleton.class);
  bind(StreamFileWriterFactory.class).to(LocationStreamFileWriterFactory.class).in(Singleton.class);
  install(new FactoryModuleBuilder().implement(TransactionExecutor.class,DefaultTransactionExecutor.class).build(TransactionExecutorFactory.class));
}"
7573,"@BeforeClass public static void beforeClass() throws Exception {
  NamespacingDataSetAccessorTest.beforeClass();
  Injector injector=Guice.createInjector(new ConfigModule(conf),new DataFabricModules().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new LocationRuntimeModule().getInMemoryModules());
  dsAccessor=injector.getInstance(DataSetAccessor.class);
}","@BeforeClass public static void beforeClass() throws Exception {
  NamespacingDataSetAccessorTest.beforeClass();
  Injector injector=Guice.createInjector(new ConfigModule(conf),new DataFabricModules().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new LocationRuntimeModule().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
    }
  }
);
  dsAccessor=injector.getInstance(DataSetAccessor.class);
}"
7574,"/** 
 * Sets up the in-memory data fabric.
 */
@BeforeClass public static void setupDataFabric(){
  final Injector injector=Guice.createInjector(new DataFabricModules().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(LocationFactory.class).to(LocalLocationFactory.class);
    }
  }
);
  injector.getInstance(InMemoryTransactionManager.class).startAndWait();
  txSystemClient=injector.getInstance(TransactionSystemClient.class);
  LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
  DataSetAccessor dataSetAccessor=injector.getInstance(DataSetAccessor.class);
  fabric=new DataFabric2Impl(locationFactory,dataSetAccessor);
  datasetFramework=injector.getInstance(DatasetFramework.class);
}","/** 
 * Sets up the in-memory data fabric.
 */
@BeforeClass public static void setupDataFabric(){
  final Injector injector=Guice.createInjector(new DataFabricModules().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(LocationFactory.class).to(LocalLocationFactory.class);
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
    }
  }
);
  injector.getInstance(InMemoryTransactionManager.class).startAndWait();
  txSystemClient=injector.getInstance(TransactionSystemClient.class);
  LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
  DataSetAccessor dataSetAccessor=injector.getInstance(DataSetAccessor.class);
  fabric=new DataFabric2Impl(locationFactory,dataSetAccessor);
  datasetFramework=injector.getInstance(DatasetFramework.class);
}"
7575,"@Override protected void configure(){
  bind(LocationFactory.class).to(LocalLocationFactory.class);
}","@Override protected void configure(){
  bind(LocationFactory.class).to(LocalLocationFactory.class);
  bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
}"
7576,"@BeforeClass public static void init() throws Exception {
  injector=Guice.createInjector(new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new DataFabricModules().getInMemoryModules());
  transactionManager=injector.getInstance(InMemoryTransactionManager.class);
  transactionManager.startAndWait();
  txSystemClient=injector.getInstance(TransactionSystemClient.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
  streamAdmin=injector.getInstance(StreamAdmin.class);
  executorFactory=injector.getInstance(TransactionExecutorFactory.class);
}","@BeforeClass public static void init() throws Exception {
  injector=Guice.createInjector(new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new DataFabricModules().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
    }
  }
);
  transactionManager=injector.getInstance(InMemoryTransactionManager.class);
  transactionManager.startAndWait();
  txSystemClient=injector.getInstance(TransactionSystemClient.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
  streamAdmin=injector.getInstance(StreamAdmin.class);
  executorFactory=injector.getInstance(TransactionExecutorFactory.class);
}"
7577,"@BeforeClass public static void setupDataFabric() throws Exception {
  injector=Guice.createInjector(new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new DataFabricModules().getInMemoryModules());
}","@BeforeClass public static void setupDataFabric() throws Exception {
  injector=Guice.createInjector(new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new DataFabricModules().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
    }
  }
);
}"
7578,"/** 
 * Deletes dir if it is empty, and recursively deletes parent dirs if they are empty too. The recursion stops at non-empty parent or base directory. If dir is not child of base directory then the recursion stops at root.
 * @param dir dir to be deleted.
 */
void deleteEmptyDir(Location dir){
  dir=LocationUtils.normalize(locationFactory,dir);
  LOG.debug(""String_Node_Str"",dir.toURI());
  if (logBaseDir.toURI().equals(dir.toURI()) || !dir.toURI().getRawPath().startsWith(logBaseDir.toURI().getRawPath())) {
    LOG.debug(""String_Node_Str"",dir.toURI());
    return;
  }
  try {
    if (dir.delete()) {
      LOG.info(""String_Node_Str"",dir.toURI());
      Location parent=LocationUtils.getParent(locationFactory,dir);
      LOG.debug(""String_Node_Str"",parent);
      deleteEmptyDir(parent);
    }
 else {
      LOG.debug(""String_Node_Str"",dir.toURI());
    }
  }
 catch (  RemoteException e) {
    LOG.debug(""String_Node_Str"",dir.toURI(),e);
  }
catch (  IOException e) {
    LOG.error(""String_Node_Str"",dir.toURI(),e);
  }
}","/** 
 * Deletes dir if it is empty, and recursively deletes parent dirs if they are empty too. The recursion stops at non-empty parent or base directory. If dir is not child of base directory then the recursion stops at root.
 * @param dir dir to be deleted.
 */
void deleteEmptyDir(Location dir){
  LOG.debug(""String_Node_Str"",dir.toURI());
  if (logBaseDir.toURI().equals(dir.toURI()) || !dir.toURI().getRawPath().startsWith(logBaseDir.toURI().getRawPath())) {
    LOG.debug(""String_Node_Str"",dir.toURI());
    return;
  }
  try {
    if (dir.list().isEmpty() && dir.delete()) {
      LOG.info(""String_Node_Str"",dir.toURI());
      Location parent=LocationUtils.getParent(dir);
      LOG.debug(""String_Node_Str"",parent);
      deleteEmptyDir(parent);
    }
 else {
      LOG.debug(""String_Node_Str"",dir.toURI());
    }
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",dir.toURI(),e);
  }
}"
7579,"@Override public void run(){
  LOG.info(""String_Node_Str"");
  try {
    long tillTime=System.currentTimeMillis() - retentionDurationMs;
    final Set<Location> parentDirs=Sets.newHashSet();
    fileMetaDataManager.cleanMetaData(tillTime,new FileMetaDataManager.DeleteCallback(){
      @Override public void handle(      Location location){
        try {
          if (location.exists()) {
            LOG.info(String.format(""String_Node_Str"",location.toURI()));
            location.delete();
          }
          parentDirs.add(LocationUtils.getParent(locationFactory,location));
        }
 catch (        IOException e) {
          LOG.error(String.format(""String_Node_Str"",location.toURI()),e);
          throw Throwables.propagate(e);
        }
      }
    }
);
    for (    Location dir : parentDirs) {
      deleteEmptyDir(dir);
    }
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
  }
}","@Override public void run(){
  LOG.info(""String_Node_Str"");
  try {
    long tillTime=System.currentTimeMillis() - retentionDurationMs;
    final Set<Location> parentDirs=Sets.newHashSet();
    fileMetaDataManager.cleanMetaData(tillTime,new FileMetaDataManager.DeleteCallback(){
      @Override public void handle(      Location location){
        try {
          if (location.exists()) {
            LOG.info(String.format(""String_Node_Str"",location.toURI()));
            location.delete();
          }
          parentDirs.add(LocationUtils.getParent(location));
        }
 catch (        IOException e) {
          LOG.error(String.format(""String_Node_Str"",location.toURI()),e);
          throw Throwables.propagate(e);
        }
      }
    }
);
    for (    Location dir : parentDirs) {
      deleteEmptyDir(dir);
    }
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
  }
}"
7580,"@Override public void handle(Location location){
  try {
    if (location.exists()) {
      LOG.info(String.format(""String_Node_Str"",location.toURI()));
      location.delete();
    }
    parentDirs.add(LocationUtils.getParent(locationFactory,location));
  }
 catch (  IOException e) {
    LOG.error(String.format(""String_Node_Str"",location.toURI()),e);
    throw Throwables.propagate(e);
  }
}","@Override public void handle(Location location){
  try {
    if (location.exists()) {
      LOG.info(String.format(""String_Node_Str"",location.toURI()));
      location.delete();
    }
    parentDirs.add(LocationUtils.getParent(location));
  }
 catch (  IOException e) {
    LOG.error(String.format(""String_Node_Str"",location.toURI()),e);
    throw Throwables.propagate(e);
  }
}"
7581,"public LogCleanup(LocationFactory locationFactory,FileMetaDataManager fileMetaDataManager,Location logBaseDir,long retentionDurationMs){
  this.locationFactory=locationFactory;
  this.fileMetaDataManager=fileMetaDataManager;
  this.logBaseDir=LocationUtils.normalize(locationFactory,logBaseDir);
  this.retentionDurationMs=retentionDurationMs;
  LOG.info(""String_Node_Str"",logBaseDir.toURI());
  LOG.info(""String_Node_Str"",retentionDurationMs);
}","public LogCleanup(LocationFactory locationFactory,FileMetaDataManager fileMetaDataManager,Location logBaseDir,long retentionDurationMs){
  this.fileMetaDataManager=fileMetaDataManager;
  this.logBaseDir=LocationUtils.normalize(locationFactory,logBaseDir);
  this.retentionDurationMs=retentionDurationMs;
  LOG.info(""String_Node_Str"",logBaseDir.toURI());
  LOG.info(""String_Node_Str"",retentionDurationMs);
}"
7582,"@Test public void testGetParent() throws Exception {
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory,locationFactory.create(""String_Node_Str"")).toURI());
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory,locationFactory.create(""String_Node_Str"")).toURI());
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory,locationFactory.create(""String_Node_Str"")).toURI());
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory,locationFactory.create(""String_Node_Str"")).toURI());
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory,locationFactory.create(""String_Node_Str"")).toURI());
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory,locationFactory.create(""String_Node_Str"")).toURI());
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory,locationFactory.create(""String_Node_Str"")).toURI());
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory,locationFactory.create(""String_Node_Str"")).toURI());
}","@Test public void testGetParent() throws Exception {
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory.create(""String_Node_Str"")).toURI());
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory.create(""String_Node_Str"")).toURI());
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory.create(""String_Node_Str"")).toURI());
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory.create(""String_Node_Str"")).toURI());
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory.create(""String_Node_Str"")).toURI());
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory.create(""String_Node_Str"")).toURI());
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory.create(""String_Node_Str"")).toURI());
  Assert.assertEquals(locationFactory.create(""String_Node_Str"").toURI(),LocationUtils.getParent(locationFactory.create(""String_Node_Str"")).toURI());
}"
7583,"private Location createFile(Location path) throws Exception {
  Location parent=LocationUtils.getParent(locationFactory,path);
  parent.mkdirs();
  path.createNew();
  Assert.assertTrue(path.exists());
  return path;
}","private Location createFile(Location path) throws Exception {
  Location parent=LocationUtils.getParent(path);
  parent.mkdirs();
  path.createNew();
  Assert.assertTrue(path.exists());
  return path;
}"
7584,"/** 
 * Given a full metrics path like '/v2/metrics/reactor/apps/collect.events', strip the preceding version and metrics to return 'reactor/apps/collect.events', representing the context and metric, which can then be parsed by this parser.
 * @param path request path.
 * @return request path stripped of version and metrics.
 */
static String stripVersionAndMetricsFromPath(String path){
  int startPos=Constants.Gateway.GATEWAY_VERSION.length() + 9;
  return path.substring(startPos,path.length());
}","/** 
 * Given a full metrics path like '/v2/metrics/reactor/apps/collect.events', strip the preceding version and metrics to return 'reactor/apps/collect.events', representing the context and metric, which can then be parsed by this parser.
 * @param path request path.
 * @return request path stripped of version and metrics.
 */
static String stripVersionAndMetricsFromPath(String path){
  int startPos=Constants.Gateway.GATEWAY_VERSION.length() + 9;
  return path.substring(startPos - 1,path.length());
}"
7585,"@BeforeClass public static void init() throws IOException {
  zkServer=InMemoryZKServer.builder().setDataDir(tmpFolder.newFolder()).build();
  zkServer.startAndWait();
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  injector=Guice.createInjector(new ConfigModule(cConf),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(StreamCoordinator.class).to(DistributedStreamCoordinator.class);
    }
  }
);
  zkClient=injector.getInstance(ZKClientService.class);
  zkClient.startAndWait();
}","@BeforeClass public static void init() throws IOException {
  zkServer=InMemoryZKServer.builder().setDataDir(tmpFolder.newFolder()).build();
  zkServer.startAndWait();
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  injector=Guice.createInjector(new ConfigModule(cConf),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new LocationRuntimeModule().getDistributedModules());
  zkClient=injector.getInstance(ZKClientService.class);
  zkClient.startAndWait();
}"
7586,"@BeforeClass public static void init(){
  injector=Guice.createInjector(new ConfigModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(StreamCoordinator.class).to(InMemoryStreamCoordinator.class).in(Scopes.SINGLETON);
    }
  }
);
}","@BeforeClass public static void init(){
  injector=Guice.createInjector(new ConfigModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new DataFabricModules().getInMemoryModules(),new LocationRuntimeModule().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(StreamCoordinator.class).to(InMemoryStreamCoordinator.class).in(Scopes.SINGLETON);
    }
  }
);
}"
7587,"@Inject public InMemoryTransactionService(@Named(""String_Node_Str"") CConfiguration conf,DiscoveryService discoveryService,Provider<InMemoryTransactionManager> txManagerProvider){
  this.discoveryService=discoveryService;
  this.txManagerProvider=txManagerProvider;
  address=conf.get(Constants.Transaction.Container.ADDRESS);
  threads=conf.getInt(Constants.Transaction.Service.CFG_DATA_TX_SERVER_THREADS,Constants.Transaction.Service.DEFAULT_DATA_TX_SERVER_THREADS);
  ioThreads=conf.getInt(Constants.Transaction.Service.CFG_DATA_TX_SERVER_IO_THREADS,Constants.Transaction.Service.DEFAULT_DATA_TX_SERVER_IO_THREADS);
  maxReadBufferBytes=conf.getInt(com.continuuity.common.conf.Constants.Thrift.MAX_READ_BUFFER,com.continuuity.common.conf.Constants.Thrift.DEFAULT_MAX_READ_BUFFER);
  LOG.info(""String_Node_Str"" + ""String_Node_Str"" + address + ""String_Node_Str""+ threads+ ""String_Node_Str""+ ioThreads+ ""String_Node_Str""+ maxReadBufferBytes);
}","@Inject public InMemoryTransactionService(CConfiguration conf,DiscoveryService discoveryService,Provider<InMemoryTransactionManager> txManagerProvider){
  this.discoveryService=discoveryService;
  this.txManagerProvider=txManagerProvider;
  address=conf.get(Constants.Transaction.Container.ADDRESS);
  threads=conf.getInt(Constants.Transaction.Service.CFG_DATA_TX_SERVER_THREADS,Constants.Transaction.Service.DEFAULT_DATA_TX_SERVER_THREADS);
  ioThreads=conf.getInt(Constants.Transaction.Service.CFG_DATA_TX_SERVER_IO_THREADS,Constants.Transaction.Service.DEFAULT_DATA_TX_SERVER_IO_THREADS);
  maxReadBufferBytes=conf.getInt(com.continuuity.common.conf.Constants.Thrift.MAX_READ_BUFFER,com.continuuity.common.conf.Constants.Thrift.DEFAULT_MAX_READ_BUFFER);
  LOG.info(""String_Node_Str"" + ""String_Node_Str"" + address + ""String_Node_Str""+ threads+ ""String_Node_Str""+ ioThreads+ ""String_Node_Str""+ maxReadBufferBytes);
}"
7588,"/** 
 * Reads or skips a   {@link StreamEvent}.
 * @param filter to determine to accept or skip a stream event by offsetand accept or skip a stream event block by timestamp.
 * @return The next StreamEvent or {@code null} if the event is rejected by the filter or reached EOF.
 */
private PositionStreamEvent nextStreamEvent(ReadFilter filter) throws IOException {
  PositionStreamEvent event=null;
  boolean done=false;
  while (!done) {
    if (timestamp < 0) {
      timestamp=readTimestamp();
    }
    if (timestamp == -1L) {
      eof=true;
      break;
    }
    boolean isReadBlockLength=length < 0;
    if (isReadBlockLength) {
      length=readLength();
    }
    if (isReadBlockLength && !filter.acceptTimestamp(timestamp)) {
      long bytesSkipped=eventInput.skip(length);
      timestamp=-1L;
      length=-1;
      if (bytesSkipped == length) {
        continue;
      }
 else {
        throw new EOFException();
      }
    }
    if (length > 0) {
      long startPos=eventInput.getPos();
      try {
        if (filter.acceptOffset(startPos)) {
          event=new DefaultPositionStreamEvent(readStreamData(),timestamp,startPos);
        }
 else {
          skipStreamData();
        }
      }
 catch (      IOException e) {
        if (isReadBlockLength) {
          timestamp=-1L;
          length=-1;
        }
        throw e;
      }
      long endPos=eventInput.getPos();
      done=true;
      length-=(int)(endPos - startPos);
    }
    if (length == 0) {
      timestamp=-1L;
      length=-1;
    }
  }
  return event;
}","/** 
 * Reads or skips a   {@link StreamEvent}.
 * @param filter to determine to accept or skip a stream event by offsetand accept or skip a stream event block by timestamp.
 * @return The next StreamEvent or {@code null} if the event is rejected by the filter or reached EOF.
 */
private PositionStreamEvent nextStreamEvent(ReadFilter filter) throws IOException {
  PositionStreamEvent event=null;
  boolean done=false;
  while (!done) {
    if (timestamp < 0) {
      timestamp=readTimestamp();
    }
    if (timestamp == -1L) {
      eof=true;
      break;
    }
    boolean isReadBlockLength=length < 0;
    if (isReadBlockLength) {
      length=readLength();
    }
    if (isReadBlockLength && !filter.acceptTimestamp(timestamp)) {
      long bytesSkipped=eventInput.skip(length);
      if (bytesSkipped == length) {
        timestamp=-1L;
        length=-1;
        continue;
      }
 else {
        timestamp=-1L;
        length=-1;
        throw new EOFException();
      }
    }
    if (length > 0) {
      long startPos=eventInput.getPos();
      try {
        if (filter.acceptOffset(startPos)) {
          event=new DefaultPositionStreamEvent(readStreamData(),timestamp,startPos);
        }
 else {
          skipStreamData();
        }
      }
 catch (      IOException e) {
        if (isReadBlockLength) {
          timestamp=-1L;
          length=-1;
        }
        throw e;
      }
      long endPos=eventInput.getPos();
      done=true;
      length-=(int)(endPos - startPos);
    }
    if (length == 0) {
      timestamp=-1L;
      length=-1;
    }
  }
  return event;
}"
7589,"private StreamConfig createStreamConfig(String stream) throws IOException {
  return new StreamConfig(stream,3600000,10000,new LocalLocationFactory(tmpFolder.newFolder()).create(stream));
}","private StreamConfig createStreamConfig(String stream) throws IOException {
  return new StreamConfig(stream,3600000,10000,Long.MAX_VALUE,new LocalLocationFactory(tmpFolder.newFolder()).create(stream));
}"
7590,"private StreamConfig createStreamConfig(String stream) throws IOException {
  return new StreamConfig(stream,3600000,10000,new LocalLocationFactory(tmpFolder.newFolder()).create(stream));
}","private StreamConfig createStreamConfig(String stream) throws IOException {
  return new StreamConfig(stream,3600000,10000,Long.MAX_VALUE,new LocalLocationFactory(tmpFolder.newFolder()).create(stream));
}"
7591,"@Override public void updateConfig(String streamName,StreamConfig config) throws IOException {
  Location streamLocation=streamBaseLocation.append(streamName);
  Preconditions.checkArgument(streamLocation.isDirectory(),""String_Node_Str"",streamName);
  StreamConfig originalConfig=getConfig(streamName);
  Preconditions.checkArgument(isValidConfigUpdate(originalConfig,config),""String_Node_Str"",streamName);
  Location configLocation=streamLocation.append(CONFIG_FILE_NAME);
  Location tempLocation=configLocation.getTempFile(""String_Node_Str"");
  Writer writer=new OutputStreamWriter(tempLocation.getOutputStream(),Charsets.UTF_8);
  try {
    writer.write(GSON.toJson(config));
  }
  finally {
    Closeables.closeQuietly(writer);
  }
  Preconditions.checkState(tempLocation.renameTo(configLocation) != null,""String_Node_Str"",tempLocation,configLocation);
  Preconditions.checkState(tempLocation.delete(),""String_Node_Str"",tempLocation);
}","@Override public void updateConfig(String streamName,StreamConfig config) throws IOException {
  Location streamLocation=streamBaseLocation.append(streamName);
  Preconditions.checkArgument(streamLocation.isDirectory(),""String_Node_Str"",streamName);
  StreamConfig originalConfig=getConfig(streamName);
  Preconditions.checkArgument(isValidConfigUpdate(originalConfig,config),""String_Node_Str"",streamName);
  Location configLocation=streamLocation.append(CONFIG_FILE_NAME);
  Location tempLocation=configLocation.getTempFile(""String_Node_Str"");
  Writer writer=new OutputStreamWriter(tempLocation.getOutputStream(),Charsets.UTF_8);
  try {
    writer.write(GSON.toJson(config));
  }
  finally {
    Closeables.closeQuietly(writer);
  }
  Preconditions.checkState(tempLocation.renameTo(configLocation) != null,""String_Node_Str"",tempLocation,configLocation);
}"
7592,"@Test public void testHiveDatasetsJoin() throws Exception {
  datasetManager.addInstance(""String_Node_Str"",""String_Node_Str"",DatasetInstanceProperties.EMPTY);
  DatasetAdmin admin=datasetManager.getAdmin(""String_Node_Str"",null);
  admin.create();
  Transaction tx1=transactionManager.startShort(100);
  KeyValueTableDefinition.KeyValueTable table=datasetManager.getDataset(""String_Node_Str"",null);
  Assert.assertNotNull(table);
  table.startTx(tx1);
  table.put(""String_Node_Str"",""String_Node_Str"");
  table.put(""String_Node_Str"",""String_Node_Str"");
  table.put(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(table.commitTx());
  transactionManager.canCommit(tx1,table.getTxChanges());
  transactionManager.commit(tx1);
  table.postTxCommit();
  Transaction tx2=transactionManager.startShort(100);
  table.startTx(tx2);
  Assert.assertEquals(""String_Node_Str"",table.get(""String_Node_Str""));
  hiveClient.sendCommand(""String_Node_Str"",null,null);
  hiveClient.sendCommand(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",null,null);
  hiveClient.sendCommand(""String_Node_Str"",null,null);
  hiveClient.sendCommand(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",null,null);
  HiveClientTestUtils.assertCmdFindPattern(hiveClient,""String_Node_Str"",""String_Node_Str"");
}","@Test public void testHiveDatasetsJoin() throws Exception {
  datasetFramework.addInstance(""String_Node_Str"",""String_Node_Str"",DatasetInstanceProperties.EMPTY);
  DatasetAdmin admin=datasetFramework.getAdmin(""String_Node_Str"",null);
  admin.create();
  Transaction tx1=transactionManager.startShort(100);
  KeyValueTableDefinition.KeyValueTable table=datasetFramework.getDataset(""String_Node_Str"",null);
  Assert.assertNotNull(table);
  table.startTx(tx1);
  table.put(""String_Node_Str"",""String_Node_Str"");
  table.put(""String_Node_Str"",""String_Node_Str"");
  table.put(""String_Node_Str"",""String_Node_Str"");
  Assert.assertTrue(table.commitTx());
  transactionManager.canCommit(tx1,table.getTxChanges());
  transactionManager.commit(tx1);
  table.postTxCommit();
  Transaction tx2=transactionManager.startShort(100);
  table.startTx(tx2);
  Assert.assertEquals(""String_Node_Str"",table.get(""String_Node_Str""));
  hiveClient.sendCommand(""String_Node_Str"",null,null);
  hiveClient.sendCommand(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",null,null);
  hiveClient.sendCommand(""String_Node_Str"",null,null);
  hiveClient.sendCommand(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",null,null);
  HiveClientTestUtils.assertCmdFindPattern(hiveClient,""String_Node_Str"",""String_Node_Str"");
}"
7593,"@Test public void testHiveSchemaFor() throws Exception {
  Assert.assertEquals(""String_Node_Str"",Scannables.hiveSchemaFor(Int.class));
  Assert.assertEquals(""String_Node_Str"",Scannables.hiveSchemaFor(Longg.class));
  Assert.assertEquals(""String_Node_Str"",Scannables.hiveSchemaFor(new TypeToken<ImmutablePair<Integer,String>>(){
  }
.getType()));
  Assert.assertEquals(""String_Node_Str"" + ""String_Node_Str"",Scannables.hiveSchemaFor(Record.class));
  Assert.assertEquals(""String_Node_Str"",Scannables.hiveSchemaFor(new ObjectStore<ImmutablePair<Integer,String>>(""String_Node_Str"",new TypeToken<ImmutablePair<Integer,String>>(){
  }
.getType())));
}","@Test public void testHiveSchemaFor() throws Exception {
  Assert.assertEquals(""String_Node_Str"",Scannables.hiveSchemaFor(Int.class));
  Assert.assertEquals(""String_Node_Str"",Scannables.hiveSchemaFor(Longg.class));
  Assert.assertEquals(""String_Node_Str"",Scannables.hiveSchemaFor(new TypeToken<ImmutablePair<Integer,String>>(){
  }
.getType()));
  Assert.assertEquals(""String_Node_Str"" + ""String_Node_Str"",Scannables.hiveSchemaFor(Record.class));
}"
7594,"public static Dataset getDataSetInstance(String datasetName,Transaction tx) throws IOException {
  DatasetManager manager=HiveServer.getDatasetManager();
  try {
    Dataset dataset=manager.getDataset(datasetName,null);
    if (dataset instanceof TransactionAware) {
      ((TransactionAware)dataset).startTx(tx);
    }
    return dataset;
  }
 catch (  DatasetManagementException e) {
    throw new IOException(e);
  }
}","public static Dataset getDataSetInstance(String datasetName,Transaction tx) throws IOException {
  DatasetManager manager=RuntimeHiveServer.getDatasetManager();
  try {
    Dataset dataset=manager.getDataset(datasetName,null);
    if (dataset instanceof TransactionAware) {
      ((TransactionAware)dataset).startTx(tx);
    }
    return dataset;
  }
 catch (  DatasetManagementException e) {
    throw new IOException(e);
  }
}"
7595,"@Override public Module getSingleNodeModules(){
  System.setProperty(""String_Node_Str"",conf.get(Constants.CFG_DATA_LEVELDB_DIR));
  return getLocalModules();
}","@Override public Module getSingleNodeModules(){
  File warehouseDir=new File(new File(conf.get(Constants.CFG_LOCAL_DATA_DIR),""String_Node_Str""),""String_Node_Str"");
  File databaseDir=new File(new File(conf.get(Constants.CFG_LOCAL_DATA_DIR),""String_Node_Str""),""String_Node_Str"");
  LOG.debug(""String_Node_Str"",Constants.Hive.METASTORE_WAREHOUSE_DIR,warehouseDir.getAbsolutePath());
  LOG.debug(""String_Node_Str"",Constants.Hive.DATABASE_DIR,databaseDir.getAbsolutePath());
  System.setProperty(Constants.Hive.METASTORE_WAREHOUSE_DIR,warehouseDir.getAbsolutePath());
  System.setProperty(Constants.Hive.DATABASE_DIR,databaseDir.getAbsolutePath());
  return getLocalModules();
}"
7596,"@Override public Module getInMemoryModules(){
  String tmpDir=System.getProperty(""String_Node_Str"") + System.getProperty(""String_Node_Str"") + ""String_Node_Str""+ Long.toString(System.currentTimeMillis());
  System.setProperty(""String_Node_Str"",tmpDir);
  return getLocalModules();
}","@Override public Module getInMemoryModules(){
  String warehouseDir=System.getProperty(""String_Node_Str"") + System.getProperty(""String_Node_Str"") + ""String_Node_Str""+ System.getProperty(""String_Node_Str"")+ ""String_Node_Str""+ Long.toString(System.currentTimeMillis());
  LOG.debug(""String_Node_Str"",Constants.Hive.METASTORE_WAREHOUSE_DIR,warehouseDir);
  System.setProperty(Constants.Hive.METASTORE_WAREHOUSE_DIR,warehouseDir);
  return getLocalModules();
}"
7597,"@Override protected void startUp() throws Exception {
  int hiveServerPort=hiveConf.getInt(""String_Node_Str"",0);
  Preconditions.checkArgument(hiveServerPort != 0,""String_Node_Str"");
  LOG.info(""String_Node_Str"",hostname,hiveServerPort,2.1);
  hiveServer2=new HiveServer2();
  hiveServer2.init(hiveConf);
  hiveServer2.start();
  waitForPort(hostname.getHostName(),hiveServerPort);
  LOG.info(""String_Node_Str"");
  final InetSocketAddress socketAddress=new InetSocketAddress(hostname,hiveServerPort);
  discoveryService.register(new Discoverable(){
    @Override public String getName(){
      return Constants.Service.HIVE;
    }
    @Override public InetSocketAddress getSocketAddress(){
      return socketAddress;
    }
  }
);
  LOG.info(""String_Node_Str"");
}","@Override protected void startUp() throws Exception {
  int hiveServerPort=hiveConf.getInt(""String_Node_Str"",0);
  Preconditions.checkArgument(hiveServerPort != 0,""String_Node_Str"");
  LOG.info(""String_Node_Str"",hostname,hiveServerPort);
  hiveServer2=new HiveServer2();
  hiveServer2.init(hiveConf);
  hiveServer2.start();
  waitForPort(hostname.getHostName(),hiveServerPort);
  LOG.info(""String_Node_Str"");
  final InetSocketAddress socketAddress=new InetSocketAddress(hostname,hiveServerPort);
  discoveryService.register(new Discoverable(){
    @Override public String getName(){
      return Constants.Service.HIVE;
    }
    @Override public InetSocketAddress getSocketAddress(){
      return socketAddress;
    }
  }
);
  LOG.info(""String_Node_Str"");
}"
7598,"@Override public void doInit(TwillContext context){
  LOG.info(""String_Node_Str"",name);
  System.setProperty(""String_Node_Str"",new File(System.getProperty(""String_Node_Str"")).getAbsolutePath());
  try {
    LOG.info(""String_Node_Str"",name,context.getHost().getCanonicalHostName());
    getCConfiguration().set(Constants.Hive.Container.SERVER_ADDRESS,context.getHost().getCanonicalHostName());
    Map<String,String> configs=context.getSpecification().getConfigs();
    hiveConf=new HiveConf();
    hiveConf.clear();
    hiveConf.addResource(new File(configs.get(hiveConfName)).toURI().toURL());
    int hiveServerPort=PortDetector.findFreePort();
    LOG.info(""String_Node_Str"",hiveServerPort);
    hiveConf.setInt(""String_Node_Str"",hiveServerPort);
    Injector injector=createGuiceInjector(getCConfiguration(),getConfiguration(),hiveConf);
    zkClient=injector.getInstance(ZKClientService.class);
    hiveServer=injector.getInstance(HiveServer.class);
    LOG.info(""String_Node_Str"",name);
  }
 catch (  Throwable t) {
    LOG.error(t.getMessage(),t);
    throw Throwables.propagate(t);
  }
}","@Override public void doInit(TwillContext context){
  LOG.info(""String_Node_Str"",name);
  System.setProperty(""String_Node_Str"",new File(System.getProperty(""String_Node_Str"")).getAbsolutePath());
  try {
    LOG.info(""String_Node_Str"",name,context.getHost().getCanonicalHostName());
    getCConfiguration().set(Constants.Hive.Container.SERVER_ADDRESS,context.getHost().getCanonicalHostName());
    Map<String,String> configs=context.getSpecification().getConfigs();
    hiveConf=new HiveConf();
    hiveConf.clear();
    hiveConf.addResource(new File(configs.get(""String_Node_Str"")).toURI().toURL());
    int hiveServerPort=PortDetector.findFreePort();
    LOG.info(""String_Node_Str"",hiveServerPort);
    hiveConf.setInt(""String_Node_Str"",hiveServerPort);
    Injector injector=createGuiceInjector(getCConfiguration(),getConfiguration(),hiveConf);
    zkClient=injector.getInstance(ZKClientService.class);
    hiveServer=injector.getInstance(HiveServer.class);
    LOG.info(""String_Node_Str"",name);
  }
 catch (  Throwable t) {
    LOG.error(t.getMessage(),t);
    throw Throwables.propagate(t);
  }
}"
7599,"/** 
 * Reads or skips a   {@link StreamEvent}.
 * @param filter to determine to accept or skip a stream event.
 * @return The next StreamEvent or {@code null} if the event is rejected by the filter or reached EOF.
 */
private PositionStreamEvent nextStreamEvent(ReadFilter filter) throws IOException {
  PositionStreamEvent event=null;
  boolean done=false;
  while (!done) {
    if (timestamp < 0) {
      timestamp=readTimestamp();
    }
    if (timestamp == -1L) {
      eof=true;
      break;
    }
    if (length < 0) {
      length=readLength();
    }
    if (length > 0) {
      long startPos=eventInput.getPos();
      if (filter.acceptOffset(startPos)) {
        event=new DefaultPositionStreamEvent(readStreamData(),timestamp,startPos);
      }
 else {
        skipStreamData();
      }
      long endPos=eventInput.getPos();
      done=true;
      length-=(int)(endPos - startPos);
    }
    if (length == 0) {
      timestamp=-1L;
      length=-1;
    }
  }
  return event;
}","/** 
 * Reads or skips a   {@link StreamEvent}.
 * @param filter to determine to accept or skip a stream event.
 * @return The next StreamEvent or {@code null} if the event is rejected by the filter or reached EOF.
 */
private PositionStreamEvent nextStreamEvent(ReadFilter filter) throws IOException {
  PositionStreamEvent event=null;
  boolean done=false;
  while (!done) {
    if (timestamp < 0) {
      timestamp=readTimestamp();
    }
    if (timestamp == -1L) {
      eof=true;
      break;
    }
    boolean isReadBlockLength=length < 0;
    if (isReadBlockLength) {
      length=readLength();
    }
    if (length > 0) {
      long startPos=eventInput.getPos();
      try {
        if (filter.acceptOffset(startPos)) {
          event=new DefaultPositionStreamEvent(readStreamData(),timestamp,startPos);
        }
 else {
          skipStreamData();
        }
      }
 catch (      IOException e) {
        if (isReadBlockLength) {
          timestamp=-1L;
          length=-1;
        }
        throw e;
      }
      long endPos=eventInput.getPos();
      done=true;
      length-=(int)(endPos - startPos);
    }
    if (length == 0) {
      timestamp=-1L;
      length=-1;
    }
  }
  return event;
}"
7600,"@Test public void testBundler() throws Exception {
  LocationFactory lf=new LocalLocationFactory();
  Location out=lf.create(File.createTempFile(""String_Node_Str"",""String_Node_Str"").toURI());
  try {
    Manifest manifest=new Manifest();
    manifest.getMainAttributes().put(ManifestFields.MANIFEST_VERSION,""String_Node_Str"");
    manifest.getMainAttributes().put(ManifestFields.MAIN_CLASS,""String_Node_Str"");
    manifest.getMainAttributes().put(ManifestFields.PROCESSOR_TYPE,""String_Node_Str"");
    manifest.getMainAttributes().put(ManifestFields.SPEC_FILE,""String_Node_Str"");
    Location jarfile=lf.create(JarFinder.getJar(WebCrawlApp.class));
    ArchiveBundler bundler=new ArchiveBundler(jarfile);
    bundler.clone(out,manifest,ImmutableMap.of(""String_Node_Str"",ByteStreams.newInputStreamSupplier(""String_Node_Str"".getBytes(Charsets.UTF_8))));
    Assert.assertTrue(out.exists());
    JarFile file=new JarFile(new File(out.toURI()));
    Enumeration<JarEntry> entries=file.entries();
    Manifest newManifest=file.getManifest();
    Assert.assertTrue(newManifest.getMainAttributes().get(ManifestFields.MANIFEST_VERSION).equals(""String_Node_Str""));
    Assert.assertTrue(newManifest.getMainAttributes().get(ManifestFields.MAIN_CLASS).equals(""String_Node_Str""));
    Assert.assertTrue(newManifest.getMainAttributes().get(ManifestFields.PROCESSOR_TYPE).equals(""String_Node_Str""));
    Assert.assertTrue(newManifest.getMainAttributes().get(ManifestFields.SPEC_FILE).equals(""String_Node_Str""));
    JarResources oldJar=new JarResources(jarfile);
    boolean found_app_json=false;
    while (entries.hasMoreElements()) {
      JarEntry entry=entries.nextElement();
      if (entry.getName().contains(""String_Node_Str"")) {
        found_app_json=true;
      }
 else       if (!entry.isDirectory() && !entry.getName().equals(JarFile.MANIFEST_NAME)) {
        Assert.assertNotNull(oldJar.getResource(entry.getName()));
      }
    }
    Assert.assertTrue(found_app_json);
  }
  finally {
    out.delete();
  }
}","@Test public void testBundler() throws Exception {
  LocationFactory lf=new LocalLocationFactory();
  Location out=lf.create(File.createTempFile(""String_Node_Str"",""String_Node_Str"").toURI());
  try {
    Manifest manifest=new Manifest();
    manifest.getMainAttributes().put(ManifestFields.MANIFEST_VERSION,""String_Node_Str"");
    manifest.getMainAttributes().put(ManifestFields.MAIN_CLASS,""String_Node_Str"");
    manifest.getMainAttributes().put(ManifestFields.PROCESSOR_TYPE,""String_Node_Str"");
    manifest.getMainAttributes().put(ManifestFields.SPEC_FILE,""String_Node_Str"");
    Location jarfile=lf.create(JarFinder.getJar(WebCrawlApp.class));
    ArchiveBundler bundler=new ArchiveBundler(jarfile);
    bundler.clone(out,manifest,ImmutableMap.of(""String_Node_Str"",ByteStreams.newInputStreamSupplier(""String_Node_Str"".getBytes(Charsets.UTF_8))));
    Assert.assertTrue(out.exists());
    JarFile file=new JarFile(new File(out.toURI()));
    Enumeration<JarEntry> entries=file.entries();
    Manifest newManifest=file.getManifest();
    Assert.assertTrue(newManifest.getMainAttributes().get(ManifestFields.MANIFEST_VERSION).equals(""String_Node_Str""));
    Assert.assertTrue(newManifest.getMainAttributes().get(ManifestFields.MAIN_CLASS).equals(""String_Node_Str""));
    Assert.assertTrue(newManifest.getMainAttributes().get(ManifestFields.PROCESSOR_TYPE).equals(""String_Node_Str""));
    Assert.assertTrue(newManifest.getMainAttributes().get(ManifestFields.SPEC_FILE).equals(""String_Node_Str""));
    JarResources oldJar=new JarResources(jarfile);
    boolean foundAppJson=false;
    while (entries.hasMoreElements()) {
      JarEntry entry=entries.nextElement();
      if (entry.getName().contains(""String_Node_Str"")) {
        foundAppJson=true;
      }
 else       if (!entry.isDirectory() && !entry.getName().equals(JarFile.MANIFEST_NAME)) {
        Assert.assertNotNull(oldJar.getResource(entry.getName()));
      }
    }
    Assert.assertTrue(foundAppJson);
  }
  finally {
    out.delete();
  }
}"
7601,"@Test public void testExplodeA() throws Exception {
  URL jarUrl=getClass().getResource(""String_Node_Str"");
  Assert.assertNotNull(jarUrl);
  File dest=tempFolder.newFolder();
  int numFiles=JarExploder.explode(new File(jarUrl.toURI()),dest,new Predicate<JarEntry>(){
    @Override public boolean apply(    JarEntry input){
      return input.getName().startsWith(""String_Node_Str"");
    }
  }
);
  Assert.assertEquals(aFileContentMap.size(),numFiles);
  verifyA(dest);
}","@Test public void testExplodeA() throws Exception {
  URL jarUrl=getClass().getResource(""String_Node_Str"");
  Assert.assertNotNull(jarUrl);
  File dest=TEMP_FOLDER.newFolder();
  int numFiles=JarExploder.explode(new File(jarUrl.toURI()),dest,new Predicate<JarEntry>(){
    @Override public boolean apply(    JarEntry input){
      return input.getName().startsWith(""String_Node_Str"");
    }
  }
);
  Assert.assertEquals(aFileContentMap.size(),numFiles);
  verifyA(dest);
}"
7602,"/** 
 * Prints the usage based on declared Options in the class.
 * @param options extracted options from introspecting a class
 * @param out Stream to output the usage.
 */
private static void printUsage(Map<String,OptionSpec> options,String appName,String appVersion,PrintStream out){
  final String NON_DEFAULT_FORMAT_STRING=""String_Node_Str"";
  final String DEFAULT_FORMAT_STRING=""String_Node_Str"";
  out.print(String.format(""String_Node_Str"",appName,appVersion));
  out.println(""String_Node_Str"");
  if (!options.containsKey(""String_Node_Str"")) {
    out.printf(NON_DEFAULT_FORMAT_STRING,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  }
  for (  OptionSpec option : options.values()) {
    if (option.isHidden()) {
      continue;
    }
    String usage=option.getUsage();
    if (!usage.isEmpty()) {
      usage=""String_Node_Str"" + usage;
    }
    String def=option.getDefaultValue();
    if (""String_Node_Str"".equals(def)) {
      out.printf(NON_DEFAULT_FORMAT_STRING,option.getName(),""String_Node_Str"" + option.getTypeName() + ""String_Node_Str"",usage);
    }
 else {
      out.printf(DEFAULT_FORMAT_STRING,option.getName(),""String_Node_Str"" + option.getTypeName() + ""String_Node_Str"",usage,option.getDefaultValue());
    }
  }
}","/** 
 * Prints the usage based on declared Options in the class.
 * @param options extracted options from introspecting a class
 * @param out Stream to output the usage.
 */
private static void printUsage(Map<String,OptionSpec> options,String appName,String appVersion,PrintStream out){
  out.print(String.format(""String_Node_Str"",appName,appVersion));
  out.println(""String_Node_Str"");
  if (!options.containsKey(""String_Node_Str"")) {
    out.printf(NON_DEFAULT_FORMAT_STRING,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  }
  for (  OptionSpec option : options.values()) {
    if (option.isHidden()) {
      continue;
    }
    String usage=option.getUsage();
    if (!usage.isEmpty()) {
      usage=""String_Node_Str"" + usage;
    }
    String def=option.getDefaultValue();
    if (""String_Node_Str"".equals(def)) {
      out.printf(NON_DEFAULT_FORMAT_STRING,option.getName(),""String_Node_Str"" + option.getTypeName() + ""String_Node_Str"",usage);
    }
 else {
      out.printf(DEFAULT_FORMAT_STRING,option.getName(),""String_Node_Str"" + option.getTypeName() + ""String_Node_Str"",usage,option.getDefaultValue());
    }
  }
}"
7603,"/** 
 * Splits a comma separated value <code>String</code>, trimming leading and trailing whitespace on each value.
 * @param str a comma separated <String> with values
 * @return an array of <code>String</code> values
 */
public static String[] getTrimmedStrings(String str){
  if (null == str || ""String_Node_Str"".equals(str.trim())) {
    return emptyStringArray;
  }
  return str.trim().split(""String_Node_Str"");
}","/** 
 * Splits a comma separated value <code>String</code>, trimming leading and trailing whitespace on each value.
 * @param str a comma separated <String> with values
 * @return an array of <code>String</code> values
 */
public static String[] getTrimmedStrings(String str){
  if (null == str || ""String_Node_Str"".equals(str.trim())) {
    return EMPTY_STRING_ARRAY;
  }
  return str.trim().split(""String_Node_Str"");
}"
7604,"@Override public void readFields(DataInput in) throws IOException {
  try {
    ClassLoader classLoader=Thread.currentThread().getContextClassLoader();
    if (classLoader == null) {
      classLoader=getClass().getClassLoader();
    }
    Class<? extends Split> splitClass=(Class<Split>)classLoader.loadClass(Text.readString(in));
    dataSetSplit=new Gson().fromJson(Text.readString(in),splitClass);
  }
 catch (  ClassNotFoundException e) {
    throw Throwables.propagate(e);
  }
}","@Override public void readFields(DataInput in) throws IOException {
  try {
    ClassLoader classLoader=Thread.currentThread().getContextClassLoader();
    if (classLoader == null) {
      classLoader=getClass().getClassLoader();
    }
    Class<? extends Split> splitClass=(Class<Split>)classLoader.loadClass(Text.readString(in));
    dataSetSplit=new Gson().fromJson(Text.readString(in),splitClass);
    dummyPath=new Path(Text.readString(in));
  }
 catch (  ClassNotFoundException e) {
    throw Throwables.propagate(e);
  }
}"
7605,"public DatasetInputSplit(Split dataSetSplit){
  this.dataSetSplit=dataSetSplit;
}","public DatasetInputSplit(Split dataSetSplit,Path dummyPath){
  this.dataSetSplit=dataSetSplit;
  this.dummyPath=dummyPath;
}"
7606,"@Override public void write(DataOutput out) throws IOException {
  Text.writeString(out,dataSetSplit.getClass().getName());
  String ser=new Gson().toJson(dataSetSplit);
  Text.writeString(out,ser);
}","@Override public void write(DataOutput out) throws IOException {
  Text.writeString(out,dataSetSplit.getClass().getName());
  String ser=new Gson().toJson(dataSetSplit);
  Text.writeString(out,ser);
  Text.writeString(out,dummyPath.toUri().toString());
}"
7607,"@Override public InputSplit[] getSplits(JobConf jobConf,int numSplits) throws IOException {
  RowScannable rowScannable=getDataset(jobConf.get(DATASET_NAME));
  List<Split> dsSplits=rowScannable.getSplits();
  InputSplit[] inputSplits=new InputSplit[dsSplits.size()];
  for (int i=0; i < dsSplits.size(); i++) {
    inputSplits[i]=new DatasetInputSplit(dsSplits.get(i));
  }
  return inputSplits;
}","@Override public InputSplit[] getSplits(JobConf jobConf,int numSplits) throws IOException {
  RowScannable rowScannable=getDataset(jobConf.get(DATASET_NAME));
  Job job=new Job(jobConf);
  JobContext jobContext=ShimLoader.getHadoopShims().newJobContext(job);
  Path[] tablePaths=FileInputFormat.getInputPaths(jobContext);
  List<Split> dsSplits=rowScannable.getSplits();
  InputSplit[] inputSplits=new InputSplit[dsSplits.size()];
  for (int i=0; i < dsSplits.size(); i++) {
    inputSplits[i]=new DatasetInputSplit(dsSplits.get(i),tablePaths[0]);
  }
  return inputSplits;
}"
7608,"@SuppressWarnings(""String_Node_Str"") @Override protected final <T>T getDataSetClient(String name,Class<? extends T> type,@Nullable Properties props) throws Exception {
  if (type == InMemoryOcTableClient.class) {
    return (T)new InMemoryOcTableClient(name,ConflictDetection.NONE);
  }
 else   if (type == OrderedColumnarTable.class) {
    ConflictDetection level=null;
    int ttl=-1;
    if (props != null) {
      String levelProperty=props.getProperty(""String_Node_Str"");
      level=levelProperty == null ? null : ConflictDetection.valueOf(levelProperty);
      String ttlProperty=props.getProperty(TxConstants.PROPERTY_TTL);
      ttl=ttlProperty == null ? null : Integer.valueOf(ttlProperty);
    }
    level=level == null ? ConflictDetection.ROW : level;
    return getOcTableClient(name,level,ttl);
  }
  if (type == MetricsTable.class) {
    return getMetricsTableClient(name);
  }
  return null;
}","@SuppressWarnings(""String_Node_Str"") @Override protected final <T>T getDataSetClient(String name,Class<? extends T> type,@Nullable Properties props) throws Exception {
  if (type == InMemoryOcTableClient.class) {
    return (T)new InMemoryOcTableClient(name,ConflictDetection.NONE);
  }
 else   if (type == OrderedColumnarTable.class) {
    ConflictDetection level=null;
    int ttl=-1;
    if (props != null) {
      String levelProperty=props.getProperty(""String_Node_Str"");
      level=levelProperty == null ? null : ConflictDetection.valueOf(levelProperty);
      String ttlProperty=props.getProperty(TxConstants.PROPERTY_TTL);
      ttl=ttlProperty == null ? -1 : Integer.valueOf(ttlProperty);
    }
    level=level == null ? ConflictDetection.ROW : level;
    return getOcTableClient(name,level,ttl);
  }
  if (type == MetricsTable.class) {
    return getMetricsTableClient(name);
  }
  return null;
}"
7609,"public static void setFlowletInstances(AppFabricHttpHandler httpHandler,String applicationId,String flowId,String flowletName,int instances){
  MockResponder responder=new MockResponder();
  String uri=String.format(""String_Node_Str"",applicationId,flowId,flowletName,instances);
  HttpRequest request=new DefaultHttpRequest(HttpVersion.HTTP_1_1,HttpMethod.PUT,uri);
  request.addHeader(""String_Node_Str"",instances);
  httpHandler.setFlowletInstances(request,responder,applicationId,flowId,flowletName);
  Preconditions.checkArgument(responder.getStatus().getCode() == 200,""String_Node_Str"");
}","public static void setFlowletInstances(AppFabricHttpHandler httpHandler,String applicationId,String flowId,String flowletName,int instances){
  MockResponder responder=new MockResponder();
  String uri=String.format(""String_Node_Str"",applicationId,flowId,flowletName,instances);
  HttpRequest request=new DefaultHttpRequest(HttpVersion.HTTP_1_1,HttpMethod.PUT,uri);
  JsonObject json=new JsonObject();
  json.addProperty(""String_Node_Str"",instances);
  request.setContent(ChannelBuffers.wrappedBuffer(json.toString().getBytes()));
  httpHandler.setFlowletInstances(request,responder,applicationId,flowId,flowletName);
  Preconditions.checkArgument(responder.getStatus().getCode() == 200,""String_Node_Str"");
}"
7610,"@Test public void testChangeInstance() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  String streamName=""String_Node_Str"";
  streamAdmin.create(streamName);
  StreamConfig config=streamAdmin.getConfig(streamName);
  StreamConsumerState state=generateState(0L,0,config,0L,4);
  StreamConsumerStateStore stateStore=createStateStore(config);
  stateStore.save(state);
  streamAdmin.configureInstances(QueueName.fromStream(streamName),0L,2);
  StreamConsumerState newState=stateStore.get(0L,1);
  Assert.assertTrue(Iterables.elementsEqual(state.getState(),newState.getState()));
  StreamFileOffset fileOffset=Iterables.get(state.getState(),0);
  long oldOffset=fileOffset.getOffset();
  long newOffset=oldOffset + 100000;
  fileOffset.setOffset(newOffset);
  stateStore.save(state);
  state=stateStore.get(0L,0);
  Assert.assertEquals(newOffset,Iterables.get(state.getState(),0).getOffset());
  streamAdmin.configureInstances(QueueName.fromStream(streamName),0L,3);
  state=stateStore.get(0L,0);
  Assert.assertEquals(oldOffset,Iterables.get(state.getState(),0).getOffset());
  List<StreamConsumerState> states=Lists.newArrayList();
  stateStore.getByGroup(0L,states);
  Assert.assertEquals(3,states.size());
  Assert.assertTrue(Iterables.elementsEqual(states.get(0).getState(),states.get(1).getState()));
  Assert.assertTrue(Iterables.elementsEqual(states.get(0).getState(),states.get(2).getState()));
}","@Test public void testChangeInstance() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  String streamName=""String_Node_Str"";
  streamAdmin.create(streamName);
  StreamConfig config=streamAdmin.getConfig(streamName);
  StreamConsumerState state=generateState(0L,0,config,0L,4);
  StreamConsumerStateStore stateStore=createStateStore(config);
  stateStore.save(state);
  streamAdmin.configureInstances(QueueName.fromStream(streamName),0L,2);
  StreamConsumerState newState=stateStore.get(0L,1);
  Assert.assertTrue(Iterables.elementsEqual(state.getState(),newState.getState()));
  StreamFileOffset fileOffset=Iterables.get(state.getState(),0);
  long oldOffset=fileOffset.getOffset();
  long newOffset=oldOffset + 100000;
  fileOffset.setOffset(newOffset);
  stateStore.save(state);
  state=stateStore.get(0L,0);
  Assert.assertEquals(newOffset,Iterables.get(state.getState(),0).getOffset());
  streamAdmin.configureInstances(QueueName.fromStream(streamName),0L,3);
  state=stateStore.get(0L,0);
  Assert.assertEquals(oldOffset,Iterables.get(state.getState(),0).getOffset());
  Assert.assertEquals(4,Iterables.size(state.getState()));
  List<StreamConsumerState> states=Lists.newArrayList();
  stateStore.getByGroup(0L,states);
  Assert.assertEquals(3,states.size());
  Assert.assertTrue(Iterables.elementsEqual(states.get(0).getState(),states.get(1).getState()));
  Assert.assertTrue(Iterables.elementsEqual(states.get(0).getState(),states.get(2).getState()));
}"
7611,"private void getAppDetails(HttpRequest request,HttpResponder responder,String appid){
  if (appid != null && appid.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  try {
    String accountId=getAuthenticatedAccountId(request);
    Id.Account accId=Id.Account.from(accountId);
    List<Map<String,String>> result=Lists.newArrayList();
    List<ApplicationSpecification> specList;
    if (appid == null) {
      specList=new ArrayList<ApplicationSpecification>(store.getAllApplications(accId));
    }
 else {
      ApplicationSpecification appSpec=store.getApplication(new Id.Application(accId,appid));
      if (appSpec == null) {
        responder.sendStatus(HttpResponseStatus.NOT_FOUND);
        return;
      }
      specList=Collections.singletonList(store.getApplication(new Id.Application(accId,appid)));
    }
    for (    ApplicationSpecification appSpec : specList) {
      result.add(makeAppRecord(appSpec));
    }
    String json=new Gson().toJson(result);
    responder.sendByteArray(HttpResponseStatus.OK,json.getBytes(Charsets.UTF_8),ImmutableMultimap.of(HttpHeaders.Names.CONTENT_TYPE,""String_Node_Str""));
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","private void getAppDetails(HttpRequest request,HttpResponder responder,String appid){
  if (appid != null && appid.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  try {
    String accountId=getAuthenticatedAccountId(request);
    Id.Account accId=Id.Account.from(accountId);
    List<Map<String,String>> result=Lists.newArrayList();
    List<ApplicationSpecification> specList;
    if (appid == null) {
      specList=new ArrayList<ApplicationSpecification>(store.getAllApplications(accId));
    }
 else {
      ApplicationSpecification appSpec=store.getApplication(new Id.Application(accId,appid));
      if (appSpec == null) {
        responder.sendStatus(HttpResponseStatus.NOT_FOUND);
        return;
      }
      specList=Collections.singletonList(store.getApplication(new Id.Application(accId,appid)));
    }
    for (    ApplicationSpecification appSpec : specList) {
      result.add(makeAppRecord(appSpec));
    }
    String json;
    if (appid == null) {
      json=new Gson().toJson(result);
    }
 else {
      json=new Gson().toJson(result.get(0));
    }
    responder.sendByteArray(HttpResponseStatus.OK,json.getBytes(Charsets.UTF_8),ImmutableMultimap.of(HttpHeaders.Names.CONTENT_TYPE,""String_Node_Str""));
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}"
7612,"/** 
 * Metadata tests through appfabric apis.
 */
@Test public void testGetMetadata() throws Exception {
  try {
    HttpResponse response=AppFabricTestsSuite.doPost(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    response=deploy(WordCountApp.class);
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    response=deploy(AppWithWorkflow.class);
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    String result=EntityUtils.toString(response.getEntity());
    Assert.assertNotNull(result);
    Assert.assertTrue(result.contains(""String_Node_Str""));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    result=EntityUtils.toString(response.getEntity());
    Assert.assertNotNull(result);
    Assert.assertTrue(result.contains(""String_Node_Str""));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    result=EntityUtils.toString(response.getEntity());
    Assert.assertNotNull(result);
    Assert.assertTrue(result.contains(""String_Node_Str""));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    result=EntityUtils.toString(response.getEntity());
    Assert.assertNotNull(result);
    Assert.assertTrue(result.contains(""String_Node_Str""));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    String s=EntityUtils.toString(response.getEntity());
    List<Map<String,String>> o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(2,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    Map<String,String> app=new Gson().fromJson(s,MAP_STRING_STRING_TYPE);
    Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),app);
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"" + ""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(404,response.getStatusLine().getStatusCode());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(404,response.getStatusLine().getStatusCode());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(404,response.getStatusLine().getStatusCode());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(404,response.getStatusLine().getStatusCode());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertTrue(o.isEmpty());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertTrue(o.isEmpty());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertTrue(o.isEmpty());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertTrue(o.isEmpty());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    Map<String,String> map=new Gson().fromJson(s,MAP_STRING_STRING_TYPE);
    Assert.assertNotNull(map);
    Assert.assertEquals(""String_Node_Str"",map.get(""String_Node_Str""));
    Assert.assertEquals(""String_Node_Str"",map.get(""String_Node_Str""));
    Assert.assertNotNull(map.get(""String_Node_Str""));
    DataSetSpecification spec=new Gson().fromJson(map.get(""String_Node_Str""),DataSetSpecification.class);
    Assert.assertNotNull(spec);
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(3,o.size());
    Map<String,String> expectedDataSets=ImmutableMap.<String,String>builder().put(""String_Node_Str"",ObjectStore.class.getName()).put(""String_Node_Str"",ObjectStore.class.getName()).put(""String_Node_Str"",KeyValueTable.class.getName()).build();
    for (    Map<String,String> ds : o) {
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),ds.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),ds.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),ds.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),expectedDataSets.containsKey(ds.get(""String_Node_Str"")));
      Assert.assertEquals(""String_Node_Str"" + ds.get(""String_Node_Str""),expectedDataSets.get(ds.get(""String_Node_Str"")),ds.get(""String_Node_Str""));
    }
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    expectedDataSets=ImmutableMap.<String,String>builder().put(""String_Node_Str"",KeyValueTable.class.getName()).build();
    for (    Map<String,String> ds : o) {
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),ds.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),ds.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),ds.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),expectedDataSets.containsKey(ds.get(""String_Node_Str"")));
      Assert.assertEquals(""String_Node_Str"" + ds.get(""String_Node_Str""),expectedDataSets.get(ds.get(""String_Node_Str"")),ds.get(""String_Node_Str""));
    }
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    map=new Gson().fromJson(s,MAP_STRING_STRING_TYPE);
    Assert.assertNotNull(map);
    Assert.assertEquals(""String_Node_Str"",map.get(""String_Node_Str""));
    Assert.assertEquals(""String_Node_Str"",map.get(""String_Node_Str""));
    Assert.assertNotNull(map.get(""String_Node_Str""));
    StreamSpecification sspec=new Gson().fromJson(map.get(""String_Node_Str""),StreamSpecification.class);
    Assert.assertNotNull(sspec);
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Set<String> expectedStreams=ImmutableSet.of(""String_Node_Str"");
    for (    Map<String,String> stream : o) {
      Assert.assertTrue(""String_Node_Str"" + stream.get(""String_Node_Str""),stream.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + stream.get(""String_Node_Str""),stream.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + stream.get(""String_Node_Str""),expectedStreams.contains(stream.get(""String_Node_Str"")));
    }
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(2,o.size());
    expectedStreams=ImmutableSet.of(""String_Node_Str"");
    for (    Map<String,String> stream : o) {
      Assert.assertTrue(""String_Node_Str"" + stream.get(""String_Node_Str""),stream.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + stream.get(""String_Node_Str""),stream.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + stream.get(""String_Node_Str""),expectedStreams.contains(stream.get(""String_Node_Str"")));
    }
  }
  finally {
    Assert.assertEquals(200,AppFabricTestsSuite.doDelete(""String_Node_Str"").getStatusLine().getStatusCode());
  }
}","/** 
 * Metadata tests through appfabric apis.
 */
@Test public void testGetMetadata() throws Exception {
  try {
    HttpResponse response=AppFabricTestsSuite.doPost(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    response=deploy(WordCountApp.class);
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    response=deploy(AppWithWorkflow.class);
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    String result=EntityUtils.toString(response.getEntity());
    Assert.assertNotNull(result);
    Assert.assertTrue(result.contains(""String_Node_Str""));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    result=EntityUtils.toString(response.getEntity());
    Assert.assertNotNull(result);
    Assert.assertTrue(result.contains(""String_Node_Str""));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    result=EntityUtils.toString(response.getEntity());
    Assert.assertNotNull(result);
    Assert.assertTrue(result.contains(""String_Node_Str""));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    result=EntityUtils.toString(response.getEntity());
    Assert.assertNotNull(result);
    Assert.assertTrue(result.contains(""String_Node_Str""));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    String s=EntityUtils.toString(response.getEntity());
    List<Map<String,String>> o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(2,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    Map<String,String> app=new Gson().fromJson(s,MAP_STRING_STRING_TYPE);
    Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),app);
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"" + ""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(404,response.getStatusLine().getStatusCode());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(404,response.getStatusLine().getStatusCode());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(404,response.getStatusLine().getStatusCode());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(404,response.getStatusLine().getStatusCode());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertTrue(o.isEmpty());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertTrue(o.isEmpty());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertTrue(o.isEmpty());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertTrue(o.isEmpty());
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Assert.assertTrue(o.contains(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    Map<String,String> map=new Gson().fromJson(s,MAP_STRING_STRING_TYPE);
    Assert.assertNotNull(map);
    Assert.assertEquals(""String_Node_Str"",map.get(""String_Node_Str""));
    Assert.assertEquals(""String_Node_Str"",map.get(""String_Node_Str""));
    Assert.assertNotNull(map.get(""String_Node_Str""));
    DataSetSpecification spec=new Gson().fromJson(map.get(""String_Node_Str""),DataSetSpecification.class);
    Assert.assertNotNull(spec);
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(3,o.size());
    Map<String,String> expectedDataSets=ImmutableMap.<String,String>builder().put(""String_Node_Str"",ObjectStore.class.getName()).put(""String_Node_Str"",ObjectStore.class.getName()).put(""String_Node_Str"",KeyValueTable.class.getName()).build();
    for (    Map<String,String> ds : o) {
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),ds.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),ds.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),ds.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),expectedDataSets.containsKey(ds.get(""String_Node_Str"")));
      Assert.assertEquals(""String_Node_Str"" + ds.get(""String_Node_Str""),expectedDataSets.get(ds.get(""String_Node_Str"")),ds.get(""String_Node_Str""));
    }
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    expectedDataSets=ImmutableMap.<String,String>builder().put(""String_Node_Str"",KeyValueTable.class.getName()).build();
    for (    Map<String,String> ds : o) {
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),ds.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),ds.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),ds.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + ds.get(""String_Node_Str""),expectedDataSets.containsKey(ds.get(""String_Node_Str"")));
      Assert.assertEquals(""String_Node_Str"" + ds.get(""String_Node_Str""),expectedDataSets.get(ds.get(""String_Node_Str"")),ds.get(""String_Node_Str""));
    }
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    map=new Gson().fromJson(s,MAP_STRING_STRING_TYPE);
    Assert.assertNotNull(map);
    Assert.assertEquals(""String_Node_Str"",map.get(""String_Node_Str""));
    Assert.assertEquals(""String_Node_Str"",map.get(""String_Node_Str""));
    Assert.assertNotNull(map.get(""String_Node_Str""));
    StreamSpecification sspec=new Gson().fromJson(map.get(""String_Node_Str""),StreamSpecification.class);
    Assert.assertNotNull(sspec);
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    Set<String> expectedStreams=ImmutableSet.of(""String_Node_Str"");
    for (    Map<String,String> stream : o) {
      Assert.assertTrue(""String_Node_Str"" + stream.get(""String_Node_Str""),stream.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + stream.get(""String_Node_Str""),stream.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + stream.get(""String_Node_Str""),expectedStreams.contains(stream.get(""String_Node_Str"")));
    }
    response=AppFabricTestsSuite.doGet(""String_Node_Str"");
    Assert.assertEquals(200,response.getStatusLine().getStatusCode());
    s=EntityUtils.toString(response.getEntity());
    o=new Gson().fromJson(s,LIST_MAP_STRING_STRING_TYPE);
    Assert.assertEquals(1,o.size());
    expectedStreams=ImmutableSet.of(""String_Node_Str"");
    for (    Map<String,String> stream : o) {
      Assert.assertTrue(""String_Node_Str"" + stream.get(""String_Node_Str""),stream.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + stream.get(""String_Node_Str""),stream.containsKey(""String_Node_Str""));
      Assert.assertTrue(""String_Node_Str"" + stream.get(""String_Node_Str""),expectedStreams.contains(stream.get(""String_Node_Str"")));
    }
  }
  finally {
    Assert.assertEquals(200,AppFabricTestsSuite.doDelete(""String_Node_Str"").getStatusLine().getStatusCode());
  }
}"
7613,"/** 
 * Tests procedure instances.
 */
@Test public void testProcedureInstances() throws Exception {
  HttpResponse response=deploy(WordCountApp.class);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=AppFabricTestsSuite.doGet(""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  String s=EntityUtils.toString(response.getEntity());
  Map<String,String> result=new Gson().fromJson(s,MAP_STRING_STRING_TYPE);
  Assert.assertEquals(1,result.size());
  Assert.assertEquals(1,Integer.parseInt(result.get(""String_Node_Str"")));
  JsonObject json=new JsonObject();
  json.addProperty(""String_Node_Str"",10);
  response=AppFabricTestsSuite.doPut(""String_Node_Str"",json.toString());
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=AppFabricTestsSuite.doGet(""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  s=EntityUtils.toString(response.getEntity());
  result=new Gson().fromJson(s,MAP_STRING_STRING_TYPE);
  Assert.assertEquals(1,result.size());
  Assert.assertEquals(10,Integer.parseInt(result.get(""String_Node_Str"")));
  Assert.assertEquals(200,AppFabricTestsSuite.doDelete(""String_Node_Str"").getStatusLine().getStatusCode());
}","/** 
 * Tests procedure instances.
 */
@Test public void testProcedureInstances() throws Exception {
  Assert.assertEquals(200,AppFabricTestsSuite.doDelete(""String_Node_Str"").getStatusLine().getStatusCode());
  Assert.assertEquals(200,AppFabricTestsSuite.doPost(""String_Node_Str"").getStatusLine().getStatusCode());
  HttpResponse response=deploy(WordCountApp.class);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=AppFabricTestsSuite.doGet(""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  String s=EntityUtils.toString(response.getEntity());
  Map<String,String> result=new Gson().fromJson(s,MAP_STRING_STRING_TYPE);
  Assert.assertEquals(1,result.size());
  Assert.assertEquals(1,Integer.parseInt(result.get(""String_Node_Str"")));
  JsonObject json=new JsonObject();
  json.addProperty(""String_Node_Str"",10);
  response=AppFabricTestsSuite.doPut(""String_Node_Str"",json.toString());
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=AppFabricTestsSuite.doGet(""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  s=EntityUtils.toString(response.getEntity());
  result=new Gson().fromJson(s,MAP_STRING_STRING_TYPE);
  Assert.assertEquals(1,result.size());
  Assert.assertEquals(10,Integer.parseInt(result.get(""String_Node_Str"")));
  Assert.assertEquals(200,AppFabricTestsSuite.doDelete(""String_Node_Str"").getStatusLine().getStatusCode());
}"
7614,"public void handleError(Throwable t){
  try {
    os.close();
    sessionInfo.setStatus(DeployStatus.FAILED);
  }
 catch (  IOException e) {
    e.printStackTrace();
  }
}","@Override public void handleError(Throwable t){
  try {
    os.close();
    sessionInfo.setStatus(DeployStatus.FAILED);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,t.getCause().getMessage());
  }
 catch (  IOException e) {
    e.printStackTrace();
  }
 finally {
    save(sessionInfo.setStatus(sessionInfo.getStatus()),accountId);
    sessions.remove(accountId);
  }
}"
7615,"private BodyConsumer deployAppStream(final HttpRequest request,HttpResponder responder,final String appId) throws IOException {
  final String archiveName=request.getHeader(ARCHIVE_NAME_HEADER);
  final String accountId=getAuthenticatedAccountId(request);
  final Location uploadDir=locationFactory.create(archiveDir + ""String_Node_Str"" + accountId);
  final Location archive=uploadDir.append(archiveName);
  final OutputStream os=archive.getOutputStream();
  if (archiveName == null || archiveName.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,ARCHIVE_NAME_HEADER + ""String_Node_Str"");
  }
  final SessionInfo sessionInfo=new SessionInfo(accountId,appId,archiveName,archive,DeployStatus.UPLOADING);
  sessions.put(accountId,sessionInfo);
  return new BodyConsumer(){
    @Override public void chunk(    ChannelBuffer request,    HttpResponder responder){
      try {
        request.readBytes(os,request.readableBytes());
      }
 catch (      IOException e) {
        sessionInfo.setStatus(DeployStatus.FAILED);
        e.printStackTrace();
        responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
      }
    }
    @Override public void finished(    HttpResponder responder){
      try {
        os.close();
        sessionInfo.setStatus(DeployStatus.VERIFYING);
        deploy(accountId,appId,archive);
        sessionInfo.setStatus(DeployStatus.DEPLOYED);
        responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
      }
 catch (      Exception ex) {
        sessionInfo.setStatus(DeployStatus.FAILED);
        ex.printStackTrace();
        responder.sendString(HttpResponseStatus.BAD_REQUEST,ex.getMessage());
      }
 finally {
        save(sessionInfo.setStatus(sessionInfo.getStatus()),accountId);
        sessions.remove(accountId);
      }
    }
    public void handleError(    Throwable t){
      try {
        os.close();
        sessionInfo.setStatus(DeployStatus.FAILED);
      }
 catch (      IOException e) {
        e.printStackTrace();
      }
    }
  }
;
}","private BodyConsumer deployAppStream(final HttpRequest request,final HttpResponder responder,final String appId) throws IOException {
  final String archiveName=request.getHeader(ARCHIVE_NAME_HEADER);
  final String accountId=getAuthenticatedAccountId(request);
  final Location uploadDir=locationFactory.create(archiveDir + ""String_Node_Str"" + accountId);
  final Location archive=uploadDir.append(archiveName);
  final OutputStream os=archive.getOutputStream();
  if (archiveName == null || archiveName.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,ARCHIVE_NAME_HEADER + ""String_Node_Str"");
  }
  final SessionInfo sessionInfo=new SessionInfo(accountId,appId,archiveName,archive,DeployStatus.UPLOADING);
  sessions.put(accountId,sessionInfo);
  return new BodyConsumer(){
    @Override public void chunk(    ChannelBuffer request,    HttpResponder responder){
      try {
        request.readBytes(os,request.readableBytes());
      }
 catch (      IOException e) {
        sessionInfo.setStatus(DeployStatus.FAILED);
        e.printStackTrace();
        responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
      }
    }
    @Override public void finished(    HttpResponder responder){
      try {
        os.close();
        sessionInfo.setStatus(DeployStatus.VERIFYING);
        deploy(accountId,appId,archive);
        sessionInfo.setStatus(DeployStatus.DEPLOYED);
        responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
      }
 catch (      Exception ex) {
        sessionInfo.setStatus(DeployStatus.FAILED);
        ex.printStackTrace();
        responder.sendString(HttpResponseStatus.BAD_REQUEST,ex.getMessage());
      }
 finally {
        save(sessionInfo.setStatus(sessionInfo.getStatus()),accountId);
        sessions.remove(accountId);
      }
    }
    @Override public void handleError(    Throwable t){
      try {
        os.close();
        sessionInfo.setStatus(DeployStatus.FAILED);
        responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,t.getCause().getMessage());
      }
 catch (      IOException e) {
        e.printStackTrace();
      }
 finally {
        save(sessionInfo.setStatus(sessionInfo.getStatus()),accountId);
        sessions.remove(accountId);
      }
    }
  }
;
}"
7616,"@BeforeClass public static void setup() throws Exception {
  Injector injector=Guice.createInjector(new IOModule(),new ConfigModule(),new FileBasedSecurityModule());
  CConfiguration conf=injector.getInstance(CConfiguration.class);
  keyIdentifierCodec=injector.getInstance(KeyIdentifierCodec.class);
  keyLength=conf.getInt(Constants.Security.TOKEN_DIGEST_KEY_LENGTH,Constants.Security.DEFAULT_TOKEN_DIGEST_KEY_LENGTH);
  keyAlgo=conf.get(Constants.Security.TOKEN_DIGEST_ALGO,Constants.Security.DEFAULT_TOKEN_DIGEST_ALGO);
  keyGenerator=KeyGenerator.getInstance(keyAlgo);
  keyGenerator.init(keyLength);
}","@BeforeClass public static void setup() throws Exception {
  Injector injector=Guice.createInjector(new IOModule(),new ConfigModule(),new FileBasedSecurityModule(),new DiscoveryRuntimeModule().getInMemoryModules());
  CConfiguration conf=injector.getInstance(CConfiguration.class);
  keyIdentifierCodec=injector.getInstance(KeyIdentifierCodec.class);
  keyLength=conf.getInt(Constants.Security.TOKEN_DIGEST_KEY_LENGTH,Constants.Security.DEFAULT_TOKEN_DIGEST_KEY_LENGTH);
  keyAlgo=conf.get(Constants.Security.TOKEN_DIGEST_ALGO,Constants.Security.DEFAULT_TOKEN_DIGEST_ALGO);
  keyGenerator=KeyGenerator.getInstance(keyAlgo);
  keyGenerator.init(keyLength);
}"
7617,"/** 
 * Tests taking a snapshot of the transaction manager
 */
@Test public void testTxManagerSnapshot() throws Exception {
  Long currentTs=System.currentTimeMillis();
  HttpResponse response=AppFabricTestsSuite.doGet(""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  InputStream in=response.getEntity().getContent();
  SnapshotCodecV2 codec=new SnapshotCodecV2();
  TransactionSnapshot snapshot=codec.decodeState(in);
  Assert.assertTrue(snapshot.getTimestamp() >= currentTs);
}","/** 
 * Tests taking a snapshot of the transaction manager
 */
@Test public void testTxManagerSnapshot() throws Exception {
  Long currentTs=System.currentTimeMillis();
  HttpResponse response=AppFabricTestsSuite.doGet(""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  InputStream in=response.getEntity().getContent();
  try {
    SnapshotCodecV2 codec=new SnapshotCodecV2();
    TransactionSnapshot snapshot=codec.decodeState(in);
    Assert.assertTrue(snapshot.getTimestamp() >= currentTs);
  }
  finally {
    in.close();
  }
}"
7618,"private void doSnapshot(boolean closing) throws IOException {
  long snapshotTime=0L;
  TransactionSnapshot snapshot=null;
  TransactionLog oldLog=null;
  try {
    this.logWriteLock.lock();
    try {
synchronized (this) {
        if (!isRunning() && !closing) {
          return;
        }
        long now=System.currentTimeMillis();
        if (now == lastSnapshotTime || (currentLog != null && now == currentLog.getTimestamp())) {
          try {
            TimeUnit.MILLISECONDS.sleep(1);
          }
 catch (          InterruptedException ie) {
          }
        }
        snapshot=getCurrentState();
        snapshotTime=snapshot.getTimestamp();
        LOG.info(""String_Node_Str"",snapshotTime);
        LOG.info(""String_Node_Str"" + snapshot);
        oldLog=currentLog;
        if (!closing) {
          currentLog=persistor.createLog(snapshotTime);
        }
      }
      if (oldLog != null) {
        oldLog.close();
      }
    }
  finally {
      this.logWriteLock.unlock();
    }
    persistor.writeSnapshot(snapshot);
    lastSnapshotTime=snapshotTime;
    long oldestRetainedTimestamp=persistor.deleteOldSnapshots(snapshotRetainCount);
    persistor.deleteLogsOlderThan(oldestRetainedTimestamp);
  }
 catch (  IOException ioe) {
    abortService(""String_Node_Str"" + snapshotTime + ""String_Node_Str""+ ioe.getMessage(),ioe);
  }
}","private void doSnapshot(boolean closing) throws IOException {
  long snapshotTime=0L;
  TransactionSnapshot snapshot=null;
  TransactionLog oldLog=null;
  try {
    this.logWriteLock.lock();
    try {
      snapshot=getSnapshot();
synchronized (this) {
        if (snapshot == null && !closing) {
          return;
        }
        if (snapshot != null) {
          snapshotTime=snapshot.getTimestamp();
        }
        oldLog=currentLog;
        if (!closing) {
          currentLog=persistor.createLog(snapshot.getTimestamp());
        }
      }
      if (oldLog != null) {
        oldLog.close();
      }
    }
  finally {
      this.logWriteLock.unlock();
    }
    persistor.writeSnapshot(snapshot);
    lastSnapshotTime=snapshotTime;
    long oldestRetainedTimestamp=persistor.deleteOldSnapshots(snapshotRetainCount);
    persistor.deleteLogsOlderThan(oldestRetainedTimestamp);
  }
 catch (  IOException ioe) {
    abortService(""String_Node_Str"" + snapshotTime + ""String_Node_Str""+ ioe.getMessage(),ioe);
  }
}"
7619,"public TransactionSnapshot getSnapshot() throws IOException {
  long snapshotTime=0L;
  TransactionSnapshot snapshot=null;
  this.logWriteLock.lock();
  try {
    if (!isRunning()) {
      return null;
    }
    long now=System.currentTimeMillis();
    if (now == lastSnapshotTime || (currentLog != null && now == currentLog.getTimestamp())) {
      try {
        TimeUnit.MILLISECONDS.sleep(1);
      }
 catch (      InterruptedException ie) {
      }
    }
    snapshot=getCurrentState();
    snapshotTime=snapshot.getTimestamp();
    LOG.info(""String_Node_Str"",snapshotTime);
    LOG.info(""String_Node_Str"" + snapshot);
    return snapshot;
  }
  finally {
    this.logWriteLock.unlock();
  }
}","public synchronized TransactionSnapshot getSnapshot() throws IOException {
  TransactionSnapshot snapshot=null;
  if (!isRunning()) {
    return null;
  }
  long now=System.currentTimeMillis();
  if (now == lastSnapshotTime || (currentLog != null && now == currentLog.getTimestamp())) {
    try {
      TimeUnit.MILLISECONDS.sleep(1);
    }
 catch (    InterruptedException ie) {
    }
  }
  snapshot=getCurrentState();
  LOG.info(""String_Node_Str"",snapshot.getTimestamp());
  LOG.info(""String_Node_Str"" + snapshot);
  return snapshot;
}"
7620,"/** 
 * Configures a   {@link com.google.inject.Binder} via the exposed methods.
 */
@Override protected void configure(){
  bind(LogWriter.class).to(LocalLogWriter.class);
  expose(LogWriter.class);
  bind(ServiceAnnouncer.class).to(DiscoveryServiceAnnouncer.class);
  MapBinder<ProgramRunnerFactory.Type,ProgramRunner> runnerFactoryBinder=MapBinder.newMapBinder(binder(),ProgramRunnerFactory.Type.class,ProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.FLOW).to(FlowProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.FLOWLET).to(FlowletProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.PROCEDURE).to(ProcedureProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.MAPREDUCE).to(MapReduceProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.WORKFLOW).to(WorkflowProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.WEBAPP).to(WebappProgramRunner.class);
  bind(ProgramRunnerFactory.class).to(InMemoryFlowProgramRunnerFactory.class).in(Scopes.SINGLETON);
  expose(ProgramRunnerFactory.class);
  bind(ProgramRuntimeService.class).to(InMemoryProgramRuntimeService.class).in(Scopes.SINGLETON);
  expose(ProgramRuntimeService.class);
  install(new DataFabricFacadeModule());
  install(new FactoryModuleBuilder().implement(QueueReader.class,SingleQueue2Reader.class).build(QueueReaderFactory.class));
  install(new FactoryModuleBuilder().implement(JarHttpHandler.class,IntactJarHttpHandler.class).build(WebappHttpHandlerFactory.class));
  install(new GatewayAuthModule());
  install(new GatewayCommonHandlerModule());
  install(new AppFabricGatewayModule());
  install(new LogHandlerModule());
  install(new MetricsHandlerModule());
}","/** 
 * Configures a   {@link com.google.inject.Binder} via the exposed methods.
 */
@Override protected void configure(){
  bind(LogWriter.class).to(LocalLogWriter.class);
  expose(LogWriter.class);
  bind(ServiceAnnouncer.class).to(DiscoveryServiceAnnouncer.class);
  MapBinder<ProgramRunnerFactory.Type,ProgramRunner> runnerFactoryBinder=MapBinder.newMapBinder(binder(),ProgramRunnerFactory.Type.class,ProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.FLOW).to(FlowProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.FLOWLET).to(FlowletProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.PROCEDURE).to(ProcedureProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.MAPREDUCE).to(MapReduceProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.WORKFLOW).to(WorkflowProgramRunner.class);
  runnerFactoryBinder.addBinding(ProgramRunnerFactory.Type.WEBAPP).to(WebappProgramRunner.class);
  bind(ProgramRunnerFactory.class).to(InMemoryFlowProgramRunnerFactory.class).in(Scopes.SINGLETON);
  expose(ProgramRunnerFactory.class);
  bind(ProgramRuntimeService.class).to(InMemoryProgramRuntimeService.class).in(Scopes.SINGLETON);
  expose(ProgramRuntimeService.class);
  install(new DataFabricFacadeModule());
  install(new FactoryModuleBuilder().implement(QueueReader.class,SingleQueue2Reader.class).build(QueueReaderFactory.class));
  install(new FactoryModuleBuilder().implement(JarHttpHandler.class,IntactJarHttpHandler.class).build(WebappHttpHandlerFactory.class));
  install(new GatewayCommonHandlerModule());
  install(new AppFabricGatewayModule());
  install(new LogHandlerModule());
  install(new MetricsHandlerModule());
}"
7621,"@Override public void init(String[] args){
  CConfiguration cConf=CConfiguration.create();
  Configuration hConf=HBaseConfiguration.create(new HdfsConfiguration());
  injector=Guice.createInjector(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new TwillModule(),new LocationRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new AppFabricServiceRuntimeModule().getDistributedModules(),new ProgramRunnerRuntimeModule().getDistributedModules(),new DataFabricModules(cConf,hConf).getDistributedModules(),new MetricsClientRuntimeModule().getDistributedModules());
  zkClientService=injector.getInstance(ZKClientService.class);
  kafkaClientService=injector.getInstance(KafkaClientService.class);
}","@Override public void init(String[] args){
  CConfiguration cConf=CConfiguration.create();
  Configuration hConf=HBaseConfiguration.create(new HdfsConfiguration());
  injector=Guice.createInjector(new ConfigModule(cConf,hConf),new IOModule(),new AuthModule(),new ZKClientModule(),new KafkaClientModule(),new TwillModule(),new LocationRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new AppFabricServiceRuntimeModule().getDistributedModules(),new ProgramRunnerRuntimeModule().getDistributedModules(),new DataFabricModules(cConf,hConf).getDistributedModules(),new MetricsClientRuntimeModule().getDistributedModules());
  zkClientService=injector.getInstance(ZKClientService.class);
  kafkaClientService=injector.getInstance(KafkaClientService.class);
}"
7622,"private void runnableStatus(HttpRequest request,HttpResponder responder,ProgramId id){
  String accountId=""String_Node_Str"";
  id.setAccountId(accountId);
  try {
    AuthToken token=new AuthToken(request.getHeader(Constants.Gateway.CONTINUUITY_API_KEY));
    ProgramStatus status=getProgramStatus(token,id);
    if (status.getStatus().equals(""String_Node_Str"")) {
      responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    }
 else {
      JsonObject o=new JsonObject();
      o.addProperty(""String_Node_Str"",status.getStatus());
      responder.sendJson(HttpResponseStatus.OK,o);
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","private void runnableStatus(HttpRequest request,HttpResponder responder,ProgramId id){
  String accountId=getAuthenticatedAccountId(request);
  id.setAccountId(accountId);
  try {
    AuthToken token=new AuthToken(request.getHeader(Constants.Gateway.CONTINUUITY_API_KEY));
    ProgramStatus status=getProgramStatus(token,id);
    if (status.getStatus().equals(""String_Node_Str"")) {
      responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    }
 else {
      JsonObject o=new JsonObject();
      o.addProperty(""String_Node_Str"",status.getStatus());
      responder.sendJson(HttpResponseStatus.OK,o);
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}"
7623,"/** 
 * Returns status of a flow.
 */
@Path(""String_Node_Str"") @GET public void flowStatus(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String flowId){
  ProgramId id=new ProgramId();
  id.setApplicationId(appId);
  id.setFlowId(flowId);
  id.setType(EntityType.FLOW);
  runnableStatus(request,responder,id);
}","/** 
 * Returns status of a flow.
 */
@Path(""String_Node_Str"") @GET public void flowStatus(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String flowId){
  ProgramId id=new ProgramId();
  id.setApplicationId(appId);
  id.setFlowId(flowId);
  id.setType(EntityType.FLOW);
  LOG.info(""String_Node_Str"",appId,flowId);
  runnableStatus(request,responder,id);
}"
7624,"/** 
 * Constructs an new instance. Parameters are binded by Guice.
 */
@Inject public AppFabricHttpHandler(CConfiguration configuration,DataSetAccessor dataSetAccessor,LocationFactory locationFactory,ManagerFactory managerFactory,AuthorizationFactory authFactory,StoreFactory storeFactory,ProgramRuntimeService runtimeService,DiscoveryServiceClient discoveryServiceClient,QueueAdmin queueAdmin,StreamAdmin streamAdmin){
  this.dataSetAccessor=dataSetAccessor;
  this.locationFactory=locationFactory;
  this.configuration=configuration;
  this.managerFactory=managerFactory;
  this.authFactory=authFactory;
  this.runtimeService=runtimeService;
  this.discoveryServiceClient=discoveryServiceClient;
  this.queueAdmin=queueAdmin;
  this.streamAdmin=streamAdmin;
  this.store=storeFactory.create();
  this.appFabricDir=configuration.get(Constants.AppFabric.OUTPUT_DIR,System.getProperty(""String_Node_Str""));
  this.archiveDir=this.appFabricDir + ""String_Node_Str"";
}","/** 
 * Constructs an new instance. Parameters are binded by Guice.
 */
@Inject public AppFabricHttpHandler(Authenticator authenticator,CConfiguration configuration,DataSetAccessor dataSetAccessor,LocationFactory locationFactory,ManagerFactory managerFactory,AuthorizationFactory authFactory,StoreFactory storeFactory,ProgramRuntimeService runtimeService,DiscoveryServiceClient discoveryServiceClient,QueueAdmin queueAdmin,StreamAdmin streamAdmin){
  super(authenticator);
  this.dataSetAccessor=dataSetAccessor;
  this.locationFactory=locationFactory;
  this.configuration=configuration;
  this.managerFactory=managerFactory;
  this.authFactory=authFactory;
  this.runtimeService=runtimeService;
  this.discoveryServiceClient=discoveryServiceClient;
  this.queueAdmin=queueAdmin;
  this.streamAdmin=streamAdmin;
  this.store=storeFactory.create();
  this.appFabricDir=configuration.get(Constants.AppFabric.OUTPUT_DIR,System.getProperty(""String_Node_Str""));
  this.archiveDir=this.appFabricDir + ""String_Node_Str"";
}"
7625,"@Inject public DatasetHandler(GatewayAuthenticator authenticator,DataSetInstantiatorFromMetaData datasetInstantiator,DataSetAccessor dataSetAccessor,DiscoveryServiceClient discoveryClient){
  super(authenticator);
  this.datasetInstantiator=datasetInstantiator;
  this.dataSetAccessor=dataSetAccessor;
  this.discoveryClient=discoveryClient;
}","@Inject public DatasetHandler(Authenticator authenticator,DataSetInstantiatorFromMetaData datasetInstantiator,DataSetAccessor dataSetAccessor,DiscoveryServiceClient discoveryClient){
  super(authenticator);
  this.datasetInstantiator=datasetInstantiator;
  this.dataSetAccessor=dataSetAccessor;
  this.discoveryClient=discoveryClient;
}"
7626,"@Override protected Module createModule(TwillContext context){
  return Modules.combine(super.createModule(context),new DiscoveryRuntimeModule().getDistributedModules(),new GatewayAuthModule(),new GatewayCommonHandlerModule(),new AppFabricGatewayModule(),new LogHandlerModule(),new MetricsHandlerModule(),new AbstractModule(){
    @Override protected void configure(){
      install(new FactoryModuleBuilder().implement(JarHttpHandler.class,ExplodeJarHttpHandler.class).build(WebappHttpHandlerFactory.class));
    }
  }
);
}","@Override protected Module createModule(TwillContext context){
  return Modules.combine(super.createModule(context),new DiscoveryRuntimeModule().getDistributedModules(),new AuthModule(),new GatewayCommonHandlerModule(),new AppFabricGatewayModule(),new LogHandlerModule(),new MetricsHandlerModule(),new AbstractModule(){
    @Override protected void configure(){
      install(new FactoryModuleBuilder().implement(JarHttpHandler.class,ExplodeJarHttpHandler.class).build(WebappHttpHandlerFactory.class));
    }
  }
);
}"
7627,"private void deleteConsumerConfigurations(String app,String flow) throws IOException {
  HTable hTable=new HTable(admin.getConfiguration(),configTableName);
  try {
    byte[] prefix=Bytes.toBytes(QueueName.prefixForFlow(app,flow));
    byte[] stop=Arrays.copyOf(prefix,prefix.length);
    stop[prefix.length - 1]++;
    Scan scan=new Scan();
    scan.setStartRow(prefix);
    scan.setStopRow(stop);
    scan.setFilter(new FirstKeyOnlyFilter());
    scan.setMaxVersions(1);
    ResultScanner resultScanner=hTable.getScanner(scan);
    List<Delete> deletes=Lists.newArrayList();
    Result result;
    try {
      while ((result=resultScanner.next()) != null) {
        byte[] row=result.getRow();
        deletes.add(new Delete(row));
      }
    }
  finally {
      resultScanner.close();
    }
    hTable.delete(deletes);
  }
  finally {
    hTable.close();
  }
}","private void deleteConsumerConfigurations(String app,String flow) throws IOException {
  if (admin.tableExists(configTableName)) {
    HTable hTable=new HTable(admin.getConfiguration(),configTableName);
    try {
      byte[] prefix=Bytes.toBytes(QueueName.prefixForFlow(app,flow));
      byte[] stop=Arrays.copyOf(prefix,prefix.length);
      stop[prefix.length - 1]++;
      Scan scan=new Scan();
      scan.setStartRow(prefix);
      scan.setStopRow(stop);
      scan.setFilter(new FirstKeyOnlyFilter());
      scan.setMaxVersions(1);
      ResultScanner resultScanner=hTable.getScanner(scan);
      List<Delete> deletes=Lists.newArrayList();
      Result result;
      try {
        while ((result=resultScanner.next()) != null) {
          byte[] row=result.getRow();
          deletes.add(new Delete(row));
        }
      }
  finally {
        resultScanner.close();
      }
      hTable.delete(deletes);
    }
  finally {
      hTable.close();
    }
  }
}"
7628,"/** 
 * Configures the AppFabricService pre-start.
 */
@Override protected void startUp() throws Exception {
  executor=Executors.newFixedThreadPool(THREAD_COUNT,Threads.createDaemonThreadFactory(""String_Node_Str""));
  schedulerService.start();
  InetSocketAddress socketAddress=new InetSocketAddress(hostname,port);
  InetAddress address=socketAddress.getAddress();
  if (address.isAnyLocalAddress()) {
    address=InetAddress.getLocalHost();
  }
  final InetSocketAddress finalSocketAddress=new InetSocketAddress(address,port);
  discoveryService.register(new Discoverable(){
    @Override public String getName(){
      return Constants.Service.APP_FABRIC;
    }
    @Override public InetSocketAddress getSocketAddress(){
      return finalSocketAddress;
    }
  }
);
  InetSocketAddress httpSocketAddress=new InetSocketAddress(httpHostName,httpPort);
  InetAddress httpAddress=socketAddress.getAddress();
  if (httpAddress.isAnyLocalAddress()) {
    httpAddress=InetAddress.getLocalHost();
  }
  final InetSocketAddress finalHttpSocketAddress=new InetSocketAddress(httpAddress,httpPort);
  discoveryService.register(new Discoverable(){
    @Override public String getName(){
      return Constants.Service.APP_FABRIC_HTTP;
    }
    @Override public InetSocketAddress getSocketAddress(){
      return finalHttpSocketAddress;
    }
  }
);
  TThreadedSelectorServer.Args options=new TThreadedSelectorServer.Args(new TNonblockingServerSocket(socketAddress)).executorService(executor).processor(new AppFabricService.Processor<AppFabricService.Iface>(service)).workerThreads(THREAD_COUNT);
  options.maxReadBufferBytes=Constants.Thrift.DEFAULT_MAX_READ_BUFFER;
  server=new TThreadedSelectorServer(options);
  httpService=NettyHttpService.builder().setPort(httpPort).addHttpHandlers(handlers).build();
}","/** 
 * Configures the AppFabricService pre-start.
 */
@Override protected void startUp() throws Exception {
  executor=Executors.newFixedThreadPool(THREAD_COUNT,Threads.createDaemonThreadFactory(""String_Node_Str""));
  schedulerService.start();
  InetSocketAddress socketAddress=new InetSocketAddress(hostname,port);
  InetAddress address=socketAddress.getAddress();
  if (address.isAnyLocalAddress()) {
    address=InetAddress.getLocalHost();
  }
  final InetSocketAddress finalSocketAddress=new InetSocketAddress(address,port);
  discoveryService.register(new Discoverable(){
    @Override public String getName(){
      return Constants.Service.APP_FABRIC;
    }
    @Override public InetSocketAddress getSocketAddress(){
      return finalSocketAddress;
    }
  }
);
  InetSocketAddress httpSocketAddress=new InetSocketAddress(hostname,httpPort);
  InetAddress httpAddress=socketAddress.getAddress();
  if (httpAddress.isAnyLocalAddress()) {
    httpAddress=InetAddress.getLocalHost();
  }
  final InetSocketAddress finalHttpSocketAddress=new InetSocketAddress(httpAddress,httpPort);
  discoveryService.register(new Discoverable(){
    @Override public String getName(){
      return Constants.Service.APP_FABRIC_HTTP;
    }
    @Override public InetSocketAddress getSocketAddress(){
      return finalHttpSocketAddress;
    }
  }
);
  TThreadedSelectorServer.Args options=new TThreadedSelectorServer.Args(new TNonblockingServerSocket(socketAddress)).executorService(executor).processor(new AppFabricService.Processor<AppFabricService.Iface>(service)).workerThreads(THREAD_COUNT);
  options.maxReadBufferBytes=Constants.Thrift.DEFAULT_MAX_READ_BUFFER;
  server=new TThreadedSelectorServer(options);
  httpService=NettyHttpService.builder().setHost(hostname.getHostName()).setPort(httpPort).addHttpHandlers(handlers).build();
}"
7629,"/** 
 * Construct the AppFabricServer with service factory and configuration coming from guice injection.
 */
@Inject public AppFabricServer(AppFabricServiceFactory serviceFactory,CConfiguration configuration,DiscoveryService discoveryService,SchedulerService schedulerService,@Named(Constants.AppFabric.SERVER_ADDRESS) InetAddress hostname,@Named(""String_Node_Str"") Set<HttpHandler> handlers){
  this.hostname=hostname;
  this.discoveryService=discoveryService;
  this.schedulerService=schedulerService;
  this.service=serviceFactory.create(schedulerService);
  this.port=configuration.getInt(Constants.AppFabric.SERVER_PORT,Constants.AppFabric.DEFAULT_THRIFT_PORT);
  this.httpHostName=Constants.AppFabric.DEFAULT_SERVER_ADDRESS;
  this.httpPort=Constants.AppFabric.DEFAULT_SERVER_PORT;
  this.handlers=handlers;
}","/** 
 * Construct the AppFabricServer with service factory and configuration coming from guice injection.
 */
@Inject public AppFabricServer(AppFabricServiceFactory serviceFactory,CConfiguration configuration,DiscoveryService discoveryService,SchedulerService schedulerService,@Named(Constants.AppFabric.SERVER_ADDRESS) InetAddress hostname,@Named(""String_Node_Str"") Set<HttpHandler> handlers){
  this.hostname=hostname;
  this.discoveryService=discoveryService;
  this.schedulerService=schedulerService;
  this.service=serviceFactory.create(schedulerService);
  this.port=configuration.getInt(Constants.AppFabric.SERVER_PORT,Constants.AppFabric.DEFAULT_THRIFT_PORT);
  this.httpPort=Constants.AppFabric.DEFAULT_SERVER_PORT;
  this.handlers=handlers;
}"
7630,"@Test public void startStopServer() throws Exception {
  Service.State state=server.startAndWait();
  Assert.assertTrue(state == Service.State.RUNNING);
  TimeUnit.SECONDS.sleep(60);
  state=server.stopAndWait();
  Assert.assertTrue(state == Service.State.TERMINATED);
}","@Test public void startStopServer() throws Exception {
  Service.State state=server.startAndWait();
  Assert.assertTrue(state == Service.State.RUNNING);
  TimeUnit.SECONDS.sleep(5);
  state=server.stopAndWait();
  Assert.assertTrue(state == Service.State.TERMINATED);
}"
7631,"@Override public Module getDistributedModules(){
  return Modules.combine(getCommonModules(),new TwillModule());
}","@Override public Module getDistributedModules(){
  return Modules.combine(getCommonModules(),new TwillModule(),new DiscoveryRuntimeModule().getDistributedModules());
}"
7632,"@Override public Module getSingleNodeModules(){
  return getCommonModules();
}","@Override public Module getSingleNodeModules(){
  return Modules.combine(getCommonModules(),new DiscoveryRuntimeModule().getSingleNodeModules());
}"
7633,"@Override public Module getInMemoryModules(){
  return getCommonModules();
}","@Override public Module getInMemoryModules(){
  return Modules.combine(getCommonModules(),new DiscoveryRuntimeModule().getInMemoryModules());
}"
7634,"public static void setupMeta() throws OperationException {
  String account=Constants.DEVELOPER_ACCOUNT_ID;
  Location appArchiveLocation=locationFactory.getHomeLocation();
  Application app=new WordCount();
  ApplicationSpecification appSpec=app.configure();
  wordCountAppId=new Id.Application(new Id.Account(account),appSpec.getName());
  store.addApplication(wordCountAppId,appSpec,appArchiveLocation);
  app=new WCount();
  appSpec=app.configure();
  wCountAppId=new Id.Application(new Id.Account(account),appSpec.getName());
  store.addApplication(wCountAppId,appSpec,appArchiveLocation);
  validResources=ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  invalidResources=ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
}","public static void setupMeta() throws OperationException {
  String account=Constants.DEVELOPER_ACCOUNT_ID;
  Location appArchiveLocation=locationFactory.getHomeLocation();
  Application app=new WordCount();
  ApplicationSpecification appSpec=app.configure();
  wordCountAppId=new Id.Application(new Id.Account(account),appSpec.getName());
  store.addApplication(wordCountAppId,appSpec,appArchiveLocation);
  app=new WCount();
  appSpec=app.configure();
  wCountAppId=new Id.Application(new Id.Account(account),appSpec.getName());
  store.addApplication(wCountAppId,appSpec,appArchiveLocation);
  validResources=ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  invalidResources=ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
}"
7635,"@Test public void testGetMetric() throws Exception {
  MetricsCollector collector=collectionService.getCollector(MetricsScope.REACTOR,""String_Node_Str"",""String_Node_Str"");
  collector.gauge(""String_Node_Str"",10,""String_Node_Str"");
  collector.gauge(""String_Node_Str"",10,""String_Node_Str"");
  TimeUnit.SECONDS.sleep(2);
  for (  String resource : validResources) {
    HttpResponse response=GatewayFastTestsSuite.doGet(""String_Node_Str"" + resource);
    Reader reader=new InputStreamReader(response.getEntity().getContent(),Charsets.UTF_8);
    try {
      Assert.assertEquals(""String_Node_Str"" + resource + ""String_Node_Str"",HttpStatus.SC_OK,response.getStatusLine().getStatusCode());
      JsonObject json=new Gson().fromJson(reader,JsonObject.class);
      Assert.assertEquals(""String_Node_Str"" + resource + ""String_Node_Str"",10,json.get(""String_Node_Str"").getAsInt());
    }
  finally {
      reader.close();
    }
  }
}","@Test public void testGetMetric() throws Exception {
  MetricsCollector collector=collectionService.getCollector(MetricsScope.REACTOR,""String_Node_Str"",""String_Node_Str"");
  collector.gauge(""String_Node_Str"",10,""String_Node_Str"");
  collector.gauge(""String_Node_Str"",10,""String_Node_Str"");
  collector=collectionService.getCollector(MetricsScope.REACTOR,""String_Node_Str"",""String_Node_Str"");
  collector.gauge(""String_Node_Str"",10);
  TimeUnit.SECONDS.sleep(2);
  for (  String resource : validResources) {
    HttpResponse response=GatewayFastTestsSuite.doGet(""String_Node_Str"" + resource);
    Reader reader=new InputStreamReader(response.getEntity().getContent(),Charsets.UTF_8);
    try {
      Assert.assertEquals(""String_Node_Str"" + resource + ""String_Node_Str"",HttpStatus.SC_OK,response.getStatusLine().getStatusCode());
      JsonObject json=new Gson().fromJson(reader,JsonObject.class);
      Assert.assertEquals(""String_Node_Str"" + resource + ""String_Node_Str"",10,json.get(""String_Node_Str"").getAsInt());
    }
  finally {
      reader.close();
    }
  }
}"
7636,"static ImmutablePair<MetricsRequest,MetricsRequestContext> parseRequestAndContext(URI requestURI) throws MetricsPathException {
  MetricsRequestBuilder builder=new MetricsRequestBuilder(requestURI);
  String uriPath=requestURI.getRawPath();
  int index=uriPath.lastIndexOf(""String_Node_Str"");
  builder.setMetricPrefix(urlDecode(uriPath.substring(index + 1)));
  String strippedPath=uriPath.substring(0,index);
  MetricsRequestContext metricsRequestContext=null;
  if (strippedPath.startsWith(""String_Node_Str"")) {
    builder.setContextPrefix(CLUSTER_METRICS_CONTEXT);
    builder.setScope(MetricsScope.REACTOR);
  }
 else {
    metricsRequestContext=parseContext(strippedPath,builder);
  }
  parseQueryString(requestURI,builder);
  return new ImmutablePair<MetricsRequest,MetricsRequestContext>(builder.build(),metricsRequestContext);
}","static ImmutablePair<MetricsRequest,MetricsRequestContext> parseRequestAndContext(URI requestURI) throws MetricsPathException {
  MetricsRequestBuilder builder=new MetricsRequestBuilder(requestURI);
  String uriPath=requestURI.getRawPath();
  int index=uriPath.lastIndexOf(""String_Node_Str"");
  builder.setMetricPrefix(urlDecode(uriPath.substring(index + 1)));
  String strippedPath=uriPath.substring(0,index);
  MetricsRequestContext metricsRequestContext;
  if (strippedPath.startsWith(""String_Node_Str"")) {
    builder.setContextPrefix(CLUSTER_METRICS_CONTEXT);
    builder.setScope(MetricsScope.REACTOR);
    metricsRequestContext=new MetricsRequestContext.Builder().build();
  }
 else {
    metricsRequestContext=parseContext(strippedPath,builder);
  }
  parseQueryString(requestURI,builder);
  return new ImmutablePair<MetricsRequest,MetricsRequestContext>(builder.build(),metricsRequestContext);
}"
7637,"public static void setupMeta() throws OperationException {
  String account=Constants.DEVELOPER_ACCOUNT_ID;
  Location appArchiveLocation=locationFactory.getHomeLocation();
  Application app=new WordCount();
  ApplicationSpecification appSpec=app.configure();
  wordCountAppId=new Id.Application(new Id.Account(account),appSpec.getName());
  store.addApplication(wordCountAppId,appSpec,appArchiveLocation);
  app=new WCount();
  appSpec=app.configure();
  wCountAppId=new Id.Application(new Id.Account(account),appSpec.getName());
  store.addApplication(wCountAppId,appSpec,appArchiveLocation);
  validResources=ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  invalidResources=ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
}","public static void setupMeta() throws OperationException {
  String account=Constants.DEVELOPER_ACCOUNT_ID;
  Location appArchiveLocation=locationFactory.getHomeLocation();
  Application app=new WordCount();
  ApplicationSpecification appSpec=app.configure();
  wordCountAppId=new Id.Application(new Id.Account(account),appSpec.getName());
  store.addApplication(wordCountAppId,appSpec,appArchiveLocation);
  app=new WCount();
  appSpec=app.configure();
  wCountAppId=new Id.Application(new Id.Account(account),appSpec.getName());
  store.addApplication(wCountAppId,appSpec,appArchiveLocation);
  validResources=ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  invalidResources=ImmutableList.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
}"
7638,"@Test public void testGetMetric() throws Exception {
  MetricsCollector collector=collectionService.getCollector(MetricsScope.REACTOR,""String_Node_Str"",""String_Node_Str"");
  collector.gauge(""String_Node_Str"",10,""String_Node_Str"");
  collector.gauge(""String_Node_Str"",10,""String_Node_Str"");
  TimeUnit.SECONDS.sleep(2);
  for (  String resource : validResources) {
    HttpResponse response=GatewayFastTestsSuite.doGet(""String_Node_Str"" + resource);
    Reader reader=new InputStreamReader(response.getEntity().getContent(),Charsets.UTF_8);
    try {
      Assert.assertEquals(""String_Node_Str"" + resource + ""String_Node_Str"",HttpStatus.SC_OK,response.getStatusLine().getStatusCode());
      JsonObject json=new Gson().fromJson(reader,JsonObject.class);
      Assert.assertEquals(""String_Node_Str"" + resource + ""String_Node_Str"",10,json.get(""String_Node_Str"").getAsInt());
    }
  finally {
      reader.close();
    }
  }
}","@Test public void testGetMetric() throws Exception {
  MetricsCollector collector=collectionService.getCollector(MetricsScope.REACTOR,""String_Node_Str"",""String_Node_Str"");
  collector.gauge(""String_Node_Str"",10,""String_Node_Str"");
  collector.gauge(""String_Node_Str"",10,""String_Node_Str"");
  collector=collectionService.getCollector(MetricsScope.REACTOR,""String_Node_Str"",""String_Node_Str"");
  collector.gauge(""String_Node_Str"",10);
  TimeUnit.SECONDS.sleep(2);
  for (  String resource : validResources) {
    HttpResponse response=GatewayFastTestsSuite.doGet(""String_Node_Str"" + resource);
    Reader reader=new InputStreamReader(response.getEntity().getContent(),Charsets.UTF_8);
    try {
      Assert.assertEquals(""String_Node_Str"" + resource + ""String_Node_Str"",HttpStatus.SC_OK,response.getStatusLine().getStatusCode());
      JsonObject json=new Gson().fromJson(reader,JsonObject.class);
      Assert.assertEquals(""String_Node_Str"" + resource + ""String_Node_Str"",10,json.get(""String_Node_Str"").getAsInt());
    }
  finally {
      reader.close();
    }
  }
}"
7639,"static ImmutablePair<MetricsRequest,MetricsRequestContext> parseRequestAndContext(URI requestURI) throws MetricsPathException {
  MetricsRequestBuilder builder=new MetricsRequestBuilder(requestURI);
  String uriPath=requestURI.getRawPath();
  int index=uriPath.lastIndexOf(""String_Node_Str"");
  builder.setMetricPrefix(urlDecode(uriPath.substring(index + 1)));
  String strippedPath=uriPath.substring(0,index);
  MetricsRequestContext metricsRequestContext=null;
  if (strippedPath.startsWith(""String_Node_Str"")) {
    builder.setContextPrefix(CLUSTER_METRICS_CONTEXT);
    builder.setScope(MetricsScope.REACTOR);
  }
 else {
    metricsRequestContext=parseContext(strippedPath,builder);
  }
  parseQueryString(requestURI,builder);
  return new ImmutablePair<MetricsRequest,MetricsRequestContext>(builder.build(),metricsRequestContext);
}","static ImmutablePair<MetricsRequest,MetricsRequestContext> parseRequestAndContext(URI requestURI) throws MetricsPathException {
  MetricsRequestBuilder builder=new MetricsRequestBuilder(requestURI);
  String uriPath=requestURI.getRawPath();
  int index=uriPath.lastIndexOf(""String_Node_Str"");
  builder.setMetricPrefix(urlDecode(uriPath.substring(index + 1)));
  String strippedPath=uriPath.substring(0,index);
  MetricsRequestContext metricsRequestContext;
  if (strippedPath.startsWith(""String_Node_Str"")) {
    builder.setContextPrefix(CLUSTER_METRICS_CONTEXT);
    builder.setScope(MetricsScope.REACTOR);
    metricsRequestContext=new MetricsRequestContext.Builder().build();
  }
 else {
    metricsRequestContext=parseContext(strippedPath,builder);
  }
  parseQueryString(requestURI,builder);
  return new ImmutablePair<MetricsRequest,MetricsRequestContext>(builder.build(),metricsRequestContext);
}"
7640,"private Location createJobJarTempCopy(Location jobJarLocation) throws IOException {
  Location programJarCopy=locationFactory.create(jobJarLocation.getTempFile(""String_Node_Str"").toURI());
  InputStream src=jobJarLocation.getInputStream();
  try {
    OutputStream dest=programJarCopy.getOutputStream();
    try {
      ByteStreams.copy(src,dest);
    }
  finally {
      dest.close();
    }
  }
  finally {
    src.close();
  }
  return programJarCopy;
}","private Location createJobJarTempCopy(Location jobJarLocation) throws IOException {
  Location programJarCopy=locationFactory.create(""String_Node_Str"");
  InputStream src=jobJarLocation.getInputStream();
  try {
    OutputStream dest=programJarCopy.getOutputStream();
    try {
      ByteStreams.copy(src,dest);
    }
  finally {
      dest.close();
    }
  }
  finally {
    src.close();
  }
  return programJarCopy;
}"
7641,"private Location buildJobJar(BasicMapReduceContext context) throws IOException {
  ApplicationBundler appBundler=new ApplicationBundler(Lists.newArrayList(""String_Node_Str""),Lists.newArrayList(""String_Node_Str""));
  Id.Program programId=context.getProgram().getId();
  String programJarPath=context.getProgram().getJarLocation().toURI().toString();
  String programDir=programJarPath.substring(0,programJarPath.lastIndexOf('/'));
  Location appFabricDependenciesJarLocation=locationFactory.create(URI.create(programDir)).append(String.format(""String_Node_Str"",Type.MAPREDUCE.name().toLowerCase(),programId.getAccountId(),programId.getApplicationId(),programId.getId(),context.getRunId().getId()));
  LOG.debug(""String_Node_Str"",appFabricDependenciesJarLocation.toURI());
  appBundler.createBundle(appFabricDependenciesJarLocation,MapReduce.class,DataSetOutputFormat.class,DataSetInputFormat.class,MapperWrapper.class,ReducerWrapper.class);
  return appFabricDependenciesJarLocation;
}","private Location buildJobJar(BasicMapReduceContext context) throws IOException {
  ApplicationBundler appBundler=new ApplicationBundler(Lists.newArrayList(""String_Node_Str""),Lists.newArrayList(""String_Node_Str""));
  Id.Program programId=context.getProgram().getId();
  Location appFabricDependenciesJarLocation=locationFactory.create(String.format(""String_Node_Str"",Type.MAPREDUCE.name().toLowerCase(),programId.getAccountId(),programId.getApplicationId(),programId.getId(),context.getRunId().getId()));
  LOG.debug(""String_Node_Str"",appFabricDependenciesJarLocation.toURI());
  List<Class<?>> classes=Lists.<Class<?>>newArrayList(MapReduce.class,DataSetOutputFormat.class,DataSetInputFormat.class,MapperWrapper.class,ReducerWrapper.class);
  try {
    Class hbaseTableUtilClass=new HBaseTableUtilFactory().get().getClass();
    classes.add(hbaseTableUtilClass);
  }
 catch (  ProvisionException e) {
    LOG.warn(""String_Node_Str"");
  }
  appBundler.createBundle(appFabricDependenciesJarLocation,classes);
  return appFabricDependenciesJarLocation;
}"
7642,"protected Injector createInjector(){
  return Guice.createInjector(new ConfigModule(cConf,hConf),new LocationRuntimeModule().getDistributedModules(),new IOModule(),new MetricsClientRuntimeModule().getMapReduceModules(taskContext),new AbstractModule(){
    @Override protected void configure(){
      bind(Configuration.class).annotatedWith(Names.named(""String_Node_Str"")).to(Configuration.class);
      bind(CConfiguration.class).annotatedWith(Names.named(""String_Node_Str"")).to(CConfiguration.class);
      bind(DataSetAccessor.class).to(DistributedDataSetAccessor.class).in(Singleton.class);
      bind(LogAppender.class).to(KafkaLogAppender.class);
    }
  }
);
}","protected Injector createInjector(){
  return Guice.createInjector(new ConfigModule(cConf,hConf),new LocationRuntimeModule().getDistributedModules(),new IOModule(),new MetricsClientRuntimeModule().getMapReduceModules(taskContext),new AbstractModule(){
    @Override protected void configure(){
      bind(Configuration.class).annotatedWith(Names.named(""String_Node_Str"")).to(Configuration.class);
      bind(CConfiguration.class).annotatedWith(Names.named(""String_Node_Str"")).to(CConfiguration.class);
      bind(HBaseTableUtil.class).toProvider(HBaseTableUtilFactory.class);
      bind(DataSetAccessor.class).to(DistributedDataSetAccessor.class).in(Singleton.class);
      bind(LogAppender.class).to(KafkaLogAppender.class);
    }
  }
);
}"
7643,"@Override protected void configure(){
  bind(Configuration.class).annotatedWith(Names.named(""String_Node_Str"")).to(Configuration.class);
  bind(CConfiguration.class).annotatedWith(Names.named(""String_Node_Str"")).to(CConfiguration.class);
  bind(DataSetAccessor.class).to(DistributedDataSetAccessor.class).in(Singleton.class);
  bind(LogAppender.class).to(KafkaLogAppender.class);
}","@Override protected void configure(){
  bind(Configuration.class).annotatedWith(Names.named(""String_Node_Str"")).to(Configuration.class);
  bind(CConfiguration.class).annotatedWith(Names.named(""String_Node_Str"")).to(CConfiguration.class);
  bind(HBaseTableUtil.class).toProvider(HBaseTableUtilFactory.class);
  bind(DataSetAccessor.class).to(DistributedDataSetAccessor.class).in(Singleton.class);
  bind(LogAppender.class).to(KafkaLogAppender.class);
}"
7644,"@Test public void configTest() throws Exception {
  QueueName queueName=QueueName.fromFlowlet(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  String tableName=((HBaseQueueClientFactory)queueClientFactory).getConfigTableName(queueName);
  queueAdmin.configureGroups(queueName,ImmutableMap.of(1L,1,2L,2,3L,3));
  HTable hTable=testHBase.getHTable(Bytes.toBytes(tableName));
  try {
    byte[] rowKey=queueName.toBytes();
    Result result=hTable.get(new Get(rowKey));
    NavigableMap<byte[],byte[]> familyMap=result.getFamilyMap(QueueEntryRow.COLUMN_FAMILY);
    Assert.assertEquals(1 + 2 + 3,familyMap.size());
    Put put=new Put(rowKey);
    put.add(QueueEntryRow.COLUMN_FAMILY,HBaseQueueAdmin.getConsumerStateColumn(2L,0),Bytes.toBytes(4));
    put.add(QueueEntryRow.COLUMN_FAMILY,HBaseQueueAdmin.getConsumerStateColumn(2L,1),Bytes.toBytes(5));
    hTable.put(put);
    queueAdmin.configureInstances(queueName,2L,3);
    result=hTable.get(new Get(rowKey));
    int startRow=Bytes.toInt(result.getColumnLatest(QueueEntryRow.COLUMN_FAMILY,HBaseQueueAdmin.getConsumerStateColumn(2L,2)).getValue());
    Assert.assertEquals(4,startRow);
    put=new Put(rowKey);
    put.add(QueueEntryRow.COLUMN_FAMILY,HBaseQueueAdmin.getConsumerStateColumn(2L,0),Bytes.toBytes(7));
    hTable.put(put);
    queueAdmin.configureGroups(queueName,ImmutableMap.of(2L,1,4L,1));
    result=hTable.get(new Get(rowKey));
    startRow=Bytes.toInt(result.getColumnLatest(QueueEntryRow.COLUMN_FAMILY,HBaseQueueAdmin.getConsumerStateColumn(2L,0)).getValue());
    Assert.assertEquals(4,startRow);
    result=hTable.get(new Get(rowKey));
    familyMap=result.getFamilyMap(QueueEntryRow.COLUMN_FAMILY);
    Assert.assertEquals(2,familyMap.size());
    startRow=Bytes.toInt(result.getColumnLatest(QueueEntryRow.COLUMN_FAMILY,HBaseQueueAdmin.getConsumerStateColumn(4L,0)).getValue());
    Assert.assertEquals(4,startRow);
  }
  finally {
    hTable.close();
    queueAdmin.dropAll();
  }
}","@Test public void configTest() throws Exception {
  QueueName queueName=QueueName.fromFlowlet(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  String tableName=((HBaseQueueClientFactory)queueClientFactory).getConfigTableName(queueName);
  queueAdmin.configureGroups(queueName,ImmutableMap.of(1L,1,2L,2,3L,3));
  HTable hTable=testHBase.getHTable(Bytes.toBytes(tableName));
  try {
    byte[] rowKey=queueName.toBytes();
    Result result=hTable.get(new Get(rowKey));
    NavigableMap<byte[],byte[]> familyMap=result.getFamilyMap(QueueEntryRow.COLUMN_FAMILY);
    Assert.assertEquals(1 + 2 + 3,familyMap.size());
    Put put=new Put(rowKey);
    put.add(QueueEntryRow.COLUMN_FAMILY,HBaseQueueAdmin.getConsumerStateColumn(2L,0),Bytes.toBytes(4));
    put.add(QueueEntryRow.COLUMN_FAMILY,HBaseQueueAdmin.getConsumerStateColumn(2L,1),Bytes.toBytes(5));
    hTable.put(put);
    queueAdmin.configureInstances(queueName,2L,3);
    result=hTable.get(new Get(rowKey));
    for (int i=0; i < 3; i++) {
      int startRow=Bytes.toInt(result.getColumnLatest(QueueEntryRow.COLUMN_FAMILY,HBaseQueueAdmin.getConsumerStateColumn(2L,i)).getValue());
      Assert.assertEquals(4,startRow);
    }
    put=new Put(rowKey);
    put.add(QueueEntryRow.COLUMN_FAMILY,HBaseQueueAdmin.getConsumerStateColumn(2L,0),Bytes.toBytes(7));
    hTable.put(put);
    queueAdmin.configureGroups(queueName,ImmutableMap.of(2L,1,4L,1));
    result=hTable.get(new Get(rowKey));
    int startRow=Bytes.toInt(result.getColumnLatest(QueueEntryRow.COLUMN_FAMILY,HBaseQueueAdmin.getConsumerStateColumn(2L,0)).getValue());
    Assert.assertEquals(4,startRow);
    result=hTable.get(new Get(rowKey));
    familyMap=result.getFamilyMap(QueueEntryRow.COLUMN_FAMILY);
    Assert.assertEquals(2,familyMap.size());
    startRow=Bytes.toInt(result.getColumnLatest(QueueEntryRow.COLUMN_FAMILY,HBaseQueueAdmin.getConsumerStateColumn(4L,0)).getValue());
    Assert.assertEquals(4,startRow);
  }
  finally {
    hTable.close();
    queueAdmin.dropAll();
  }
}"
7645,"/** 
 * Build the instance of   {@link BasicMapReduceContext}.
 * @param conf runtime configuration
 * @param runId program run id
 * @param logicalStartTime The logical start time of the job.
 * @param workflowBatch Tells whether the batch job is started by workflow.
 * @param tx transaction to use
 * @param classLoader classloader to use
 * @param programLocation program location
 * @param inputDataSetName name of the input dataset if specified for this mapreduce job, null otherwise
 * @param inputSplits input splits if specified for this mapreduce job, null otherwise
 * @param outputDataSetName name of the output dataset if specified for this mapreduce job, null otherwise
 * @return instance of {@link BasicMapReduceContext}
 */
public BasicMapReduceContext build(CConfiguration conf,MapReduceMetrics.TaskType type,String runId,long logicalStartTime,String workflowBatch,Arguments runtimeArguments,Transaction tx,ClassLoader classLoader,URI programLocation,@Nullable String inputDataSetName,@Nullable List<Split> inputSplits,@Nullable String outputDataSetName){
  Injector injector=createInjector();
  LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
  Program program;
  try {
    program=loadProgram(programLocation,locationFactory);
    if (workflowBatch != null) {
      MapReduceSpecification mapReduceSpec=program.getSpecification().getMapReduce().get(workflowBatch);
      Preconditions.checkArgument(mapReduceSpec != null,""String_Node_Str"",workflowBatch);
      program=new WorkflowMapReduceProgram(program,mapReduceSpec);
    }
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"" + programLocation);
    throw Throwables.propagate(e);
  }
  DataSetAccessor dataSetAccessor=injector.getInstance(DataSetAccessor.class);
  DataFabric dataFabric=new DataFabric2Impl(locationFactory,dataSetAccessor);
  DataSetInstantiator dataSetContext=new DataSetInstantiator(dataFabric,classLoader);
  dataSetContext.setDataSets(program.getSpecification().getDataSets().values());
  MetricsCollectionService metricsCollectionService=(type == null) ? null : injector.getInstance(MetricsCollectionService.class);
  Map<String,DataSet> dataSets=DataSets.createDataSets(dataSetContext,program.getSpecification().getDataSets().keySet());
  MapReduceSpecification spec=program.getSpecification().getMapReduce().get(program.getName());
  BasicMapReduceContext context=new BasicMapReduceContext(program,type,RunIds.fromString(runId),runtimeArguments,dataSets,spec,dataSetContext.getTransactionAware(),logicalStartTime,workflowBatch,metricsCollectionService);
  if (type == MapReduceMetrics.TaskType.Mapper) {
    dataSetContext.setMetricsCollector(context.getSystemMapperMetrics());
  }
 else   if (type == MapReduceMetrics.TaskType.Reducer) {
    dataSetContext.setMetricsCollector(context.getSystemReducerMetrics());
  }
  for (  TransactionAware txAware : dataSetContext.getTransactionAware()) {
    txAware.startTx(tx);
  }
  if (inputDataSetName != null && inputSplits != null) {
    context.setInput((BatchReadable)context.getDataSet(inputDataSetName),inputSplits);
  }
  if (outputDataSetName != null) {
    context.setOutput((BatchWritable)context.getDataSet(outputDataSetName));
  }
  LogAppenderInitializer logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
  logAppenderInitializer.initialize();
  return context;
}","/** 
 * Build the instance of   {@link BasicMapReduceContext}.
 * @param conf runtime configuration
 * @param runId program run id
 * @param logicalStartTime The logical start time of the job.
 * @param workflowBatch Tells whether the batch job is started by workflow.
 * @param tx transaction to use
 * @param classLoader classloader to use
 * @param programLocation program location
 * @param inputDataSetName name of the input dataset if specified for this mapreduce job, null otherwise
 * @param inputSplits input splits if specified for this mapreduce job, null otherwise
 * @param outputDataSetName name of the output dataset if specified for this mapreduce job, null otherwise
 * @return instance of {@link BasicMapReduceContext}
 */
public BasicMapReduceContext build(CConfiguration conf,MapReduceMetrics.TaskType type,String runId,long logicalStartTime,String workflowBatch,Arguments runtimeArguments,Transaction tx,ClassLoader classLoader,URI programLocation,@Nullable String inputDataSetName,@Nullable List<Split> inputSplits,@Nullable String outputDataSetName){
  Injector injector=createInjector();
  LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
  Program program;
  try {
    program=loadProgram(programLocation,locationFactory);
    if (workflowBatch != null) {
      MapReduceSpecification mapReduceSpec=program.getSpecification().getMapReduce().get(workflowBatch);
      Preconditions.checkArgument(mapReduceSpec != null,""String_Node_Str"",workflowBatch);
      program=new WorkflowMapReduceProgram(program,mapReduceSpec);
    }
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"" + programLocation);
    throw Throwables.propagate(e);
  }
  DataSetAccessor dataSetAccessor=injector.getInstance(DataSetAccessor.class);
  DataFabric dataFabric=new DataFabric2Impl(locationFactory,dataSetAccessor);
  DataSetInstantiator dataSetContext=new DataSetInstantiator(dataFabric,classLoader);
  dataSetContext.setDataSets(program.getSpecification().getDataSets().values());
  MetricsCollectionService metricsCollectionService=(type == null) ? null : injector.getInstance(MetricsCollectionService.class);
  Map<String,DataSet> dataSets=DataSets.createDataSets(dataSetContext,program.getSpecification().getDataSets().keySet());
  MapReduceSpecification spec=program.getSpecification().getMapReduce().get(program.getName());
  BasicMapReduceContext context=new BasicMapReduceContext(program,type,RunIds.fromString(runId),runtimeArguments,dataSets,spec,dataSetContext.getTransactionAware(),logicalStartTime,workflowBatch,metricsCollectionService);
  if (type == MapReduceMetrics.TaskType.Mapper) {
    dataSetContext.setMetricsCollector(metricsCollectionService,context.getSystemMapperMetrics());
  }
 else   if (type == MapReduceMetrics.TaskType.Reducer) {
    dataSetContext.setMetricsCollector(metricsCollectionService,context.getSystemReducerMetrics());
  }
  for (  TransactionAware txAware : dataSetContext.getTransactionAware()) {
    txAware.startTx(tx);
  }
  if (inputDataSetName != null && inputSplits != null) {
    context.setInput((BatchReadable)context.getDataSet(inputDataSetName),inputSplits);
  }
  if (outputDataSetName != null) {
    context.setOutput((BatchWritable)context.getDataSet(outputDataSetName));
  }
  LogAppenderInitializer logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
  logAppenderInitializer.initialize();
  return context;
}"
7646,"@Override public ProgramController run(Program program,ProgramOptions options){
  BasicFlowletContext flowletContext=null;
  try {
    String flowletName=options.getName();
    int instanceId=Integer.parseInt(options.getArguments().getOption(ProgramOptionConstants.INSTANCE_ID,""String_Node_Str""));
    Preconditions.checkArgument(instanceId >= 0,""String_Node_Str"");
    int instanceCount=Integer.parseInt(options.getArguments().getOption(ProgramOptionConstants.INSTANCES,""String_Node_Str""));
    Preconditions.checkArgument(instanceCount > 0,""String_Node_Str"");
    String runIdOption=options.getArguments().getOption(ProgramOptionConstants.RUN_ID);
    Preconditions.checkNotNull(runIdOption,""String_Node_Str"");
    RunId runId=RunIds.fromString(runIdOption);
    boolean disableTransaction=Boolean.parseBoolean(options.getArguments().getOption(ProgramOptionConstants.DISABLE_TRANSACTION,Boolean.toString(false)));
    ApplicationSpecification appSpec=program.getSpecification();
    Preconditions.checkNotNull(appSpec,""String_Node_Str"");
    Type processorType=program.getType();
    Preconditions.checkNotNull(processorType,""String_Node_Str"");
    Preconditions.checkArgument(processorType == Type.FLOW,""String_Node_Str"");
    String processorName=program.getName();
    Preconditions.checkNotNull(processorName,""String_Node_Str"");
    FlowSpecification flowSpec=appSpec.getFlows().get(processorName);
    FlowletDefinition flowletDef=flowSpec.getFlowlets().get(flowletName);
    Preconditions.checkNotNull(flowletDef,""String_Node_Str"",flowletName);
    Class<?> clz=Class.forName(flowletDef.getFlowletSpec().getClassName(),true,program.getMainClass().getClassLoader());
    Preconditions.checkArgument(Flowlet.class.isAssignableFrom(clz),""String_Node_Str"",clz);
    Class<? extends Flowlet> flowletClass=(Class<? extends Flowlet>)clz;
    DataFabricFacade dataFabricFacade=disableTransaction ? dataFabricFacadeFactory.createNoTransaction(program) : dataFabricFacadeFactory.create(program);
    DataSetContext dataSetContext=dataFabricFacade.getDataSetContext();
    flowletContext=new BasicFlowletContext(program,flowletName,instanceId,runId,instanceCount,DataSets.createDataSets(dataSetContext,flowletDef.getDatasets()),options.getUserArguments(),flowletDef.getFlowletSpec(),metricsCollectionService);
    if (dataSetContext instanceof DataSetInstantiationBase) {
      ((DataSetInstantiationBase)dataSetContext).setMetricsCollector(flowletContext.getSystemMetrics());
    }
    Table<Node,String,Set<QueueSpecification>> queueSpecs=new SimpleQueueSpecificationGenerator(Id.Application.from(program.getAccountId(),program.getApplicationId())).create(flowSpec);
    Flowlet flowlet=new InstantiatorFactory(false).get(TypeToken.of(flowletClass)).create();
    TypeToken<? extends Flowlet> flowletType=TypeToken.of(flowletClass);
    Reflections.visit(flowlet,TypeToken.of(flowlet.getClass()),new PropertyFieldSetter(flowletDef.getFlowletSpec().getProperties()),new DataSetFieldSetter(flowletContext),new MetricsFieldSetter(flowletContext.getMetrics()),new OutputEmitterFieldSetter(outputEmitterFactory(flowletContext,flowletName,dataFabricFacade,queueSpecs)));
    ImmutableList.Builder<QueueConsumerSupplier> queueConsumerSupplierBuilder=ImmutableList.builder();
    Collection<ProcessSpecification> processSpecs=createProcessSpecification(flowletContext,flowletType,processMethodFactory(flowlet),processSpecificationFactory(dataFabricFacade,queueReaderFactory,flowletName,queueSpecs,queueConsumerSupplierBuilder,createSchemaCache(program)),Lists.<ProcessSpecification>newLinkedList());
    List<QueueConsumerSupplier> queueConsumerSuppliers=queueConsumerSupplierBuilder.build();
    FlowletProcessDriver driver=new FlowletProcessDriver(flowlet,flowletContext,processSpecs,createCallback(flowlet,flowletDef.getFlowletSpec()),dataFabricFacade);
    if (disableTransaction) {
      LOG.info(""String_Node_Str"",flowletContext);
    }
    LOG.info(""String_Node_Str"",flowletContext);
    driver.start();
    LOG.info(""String_Node_Str"",flowletContext);
    return new FlowletProgramController(program.getName(),flowletName,flowletContext,driver,queueConsumerSuppliers);
  }
 catch (  Exception e) {
    if (flowletContext != null) {
      flowletContext.close();
    }
    throw Throwables.propagate(e);
  }
}","@Override public ProgramController run(Program program,ProgramOptions options){
  BasicFlowletContext flowletContext=null;
  try {
    String flowletName=options.getName();
    int instanceId=Integer.parseInt(options.getArguments().getOption(ProgramOptionConstants.INSTANCE_ID,""String_Node_Str""));
    Preconditions.checkArgument(instanceId >= 0,""String_Node_Str"");
    int instanceCount=Integer.parseInt(options.getArguments().getOption(ProgramOptionConstants.INSTANCES,""String_Node_Str""));
    Preconditions.checkArgument(instanceCount > 0,""String_Node_Str"");
    String runIdOption=options.getArguments().getOption(ProgramOptionConstants.RUN_ID);
    Preconditions.checkNotNull(runIdOption,""String_Node_Str"");
    RunId runId=RunIds.fromString(runIdOption);
    boolean disableTransaction=Boolean.parseBoolean(options.getArguments().getOption(ProgramOptionConstants.DISABLE_TRANSACTION,Boolean.toString(false)));
    ApplicationSpecification appSpec=program.getSpecification();
    Preconditions.checkNotNull(appSpec,""String_Node_Str"");
    Type processorType=program.getType();
    Preconditions.checkNotNull(processorType,""String_Node_Str"");
    Preconditions.checkArgument(processorType == Type.FLOW,""String_Node_Str"");
    String processorName=program.getName();
    Preconditions.checkNotNull(processorName,""String_Node_Str"");
    FlowSpecification flowSpec=appSpec.getFlows().get(processorName);
    FlowletDefinition flowletDef=flowSpec.getFlowlets().get(flowletName);
    Preconditions.checkNotNull(flowletDef,""String_Node_Str"",flowletName);
    Class<?> clz=Class.forName(flowletDef.getFlowletSpec().getClassName(),true,program.getMainClass().getClassLoader());
    Preconditions.checkArgument(Flowlet.class.isAssignableFrom(clz),""String_Node_Str"",clz);
    Class<? extends Flowlet> flowletClass=(Class<? extends Flowlet>)clz;
    DataFabricFacade dataFabricFacade=disableTransaction ? dataFabricFacadeFactory.createNoTransaction(program) : dataFabricFacadeFactory.create(program);
    DataSetContext dataSetContext=dataFabricFacade.getDataSetContext();
    flowletContext=new BasicFlowletContext(program,flowletName,instanceId,runId,instanceCount,DataSets.createDataSets(dataSetContext,flowletDef.getDatasets()),options.getUserArguments(),flowletDef.getFlowletSpec(),metricsCollectionService);
    if (dataSetContext instanceof DataSetInstantiationBase) {
      ((DataSetInstantiationBase)dataSetContext).setMetricsCollector(metricsCollectionService,flowletContext.getSystemMetrics());
    }
    Table<Node,String,Set<QueueSpecification>> queueSpecs=new SimpleQueueSpecificationGenerator(Id.Application.from(program.getAccountId(),program.getApplicationId())).create(flowSpec);
    Flowlet flowlet=new InstantiatorFactory(false).get(TypeToken.of(flowletClass)).create();
    TypeToken<? extends Flowlet> flowletType=TypeToken.of(flowletClass);
    Reflections.visit(flowlet,TypeToken.of(flowlet.getClass()),new PropertyFieldSetter(flowletDef.getFlowletSpec().getProperties()),new DataSetFieldSetter(flowletContext),new MetricsFieldSetter(flowletContext.getMetrics()),new OutputEmitterFieldSetter(outputEmitterFactory(flowletContext,flowletName,dataFabricFacade,queueSpecs)));
    ImmutableList.Builder<QueueConsumerSupplier> queueConsumerSupplierBuilder=ImmutableList.builder();
    Collection<ProcessSpecification> processSpecs=createProcessSpecification(flowletContext,flowletType,processMethodFactory(flowlet),processSpecificationFactory(dataFabricFacade,queueReaderFactory,flowletName,queueSpecs,queueConsumerSupplierBuilder,createSchemaCache(program)),Lists.<ProcessSpecification>newLinkedList());
    List<QueueConsumerSupplier> queueConsumerSuppliers=queueConsumerSupplierBuilder.build();
    FlowletProcessDriver driver=new FlowletProcessDriver(flowlet,flowletContext,processSpecs,createCallback(flowlet,flowletDef.getFlowletSpec()),dataFabricFacade);
    if (disableTransaction) {
      LOG.info(""String_Node_Str"",flowletContext);
    }
    LOG.info(""String_Node_Str"",flowletContext);
    driver.start();
    LOG.info(""String_Node_Str"",flowletContext);
    return new FlowletProgramController(program.getName(),flowletName,flowletContext,driver,queueConsumerSuppliers);
  }
 catch (  Exception e) {
    if (flowletContext != null) {
      flowletContext.close();
    }
    throw Throwables.propagate(e);
  }
}"
7647,"BasicProcedureContext create(DataFabricFacade dataFabricFacade){
  DataSetContext dataSetContext=dataFabricFacade.getDataSetContext();
  Map<String,DataSet> dataSets=DataSets.createDataSets(dataSetContext,procedureSpec.getDataSets());
  BasicProcedureContext context=new BasicProcedureContext(program,runId,instanceId,instanceCount,dataSets,userArguments,procedureSpec,collectionService);
  if (dataSetContext instanceof DataSetInstantiationBase) {
    ((DataSetInstantiationBase)dataSetContext).setMetricsCollector(context.getSystemMetrics());
  }
  return context;
}","BasicProcedureContext create(DataFabricFacade dataFabricFacade){
  DataSetContext dataSetContext=dataFabricFacade.getDataSetContext();
  Map<String,DataSet> dataSets=DataSets.createDataSets(dataSetContext,procedureSpec.getDataSets());
  BasicProcedureContext context=new BasicProcedureContext(program,runId,instanceId,instanceCount,dataSets,userArguments,procedureSpec,collectionService);
  if (dataSetContext instanceof DataSetInstantiationBase) {
    ((DataSetInstantiationBase)dataSetContext).setMetricsCollector(collectionService,context.getSystemMetrics());
  }
  return context;
}"
7648,"@Override public void recordRead(int opsCount){
  if (programContextMetrics != null) {
    programContextMetrics.gauge(""String_Node_Str"",1,dataSetName);
    programContextMetrics.gauge(""String_Node_Str"",1);
  }
}","@Override public void recordRead(int opsCount){
  if (programContextMetrics != null) {
    programContextMetrics.gauge(""String_Node_Str"",1,dataSetName);
    programContextMetrics.gauge(""String_Node_Str"",1,dataSetName);
  }
  if (dataSetMetrics != null) {
    dataSetMetrics.gauge(""String_Node_Str"",1,dataSetName);
    dataSetMetrics.gauge(""String_Node_Str"",1,dataSetName);
  }
}"
7649,"public void setMetricsCollector(final MetricsCollector programContextMetrics){
  for (  Map.Entry<TransactionAware,String> txAware : this.txAwareToMetricNames.entrySet()) {
    if (txAware.getKey() instanceof DataSetClient) {
      final String dataSetName=txAware.getValue();
      DataSetClient.DataOpsMetrics dataOpsMetrics=new DataSetClient.DataOpsMetrics(){
        @Override public void recordRead(        int opsCount){
          if (programContextMetrics != null) {
            programContextMetrics.gauge(""String_Node_Str"",1,dataSetName);
            programContextMetrics.gauge(""String_Node_Str"",1);
          }
        }
        @Override public void recordWrite(        int opsCount,        int dataSize){
          if (programContextMetrics != null) {
            programContextMetrics.gauge(""String_Node_Str"",1,dataSetName);
            programContextMetrics.gauge(""String_Node_Str"",dataSize,dataSetName);
            programContextMetrics.gauge(""String_Node_Str"",1);
          }
        }
      }
;
      ((DataSetClient)txAware.getKey()).setMetricsCollector(dataOpsMetrics);
    }
  }
}","public void setMetricsCollector(final MetricsCollectionService metricsCollectionService,final MetricsCollector programContextMetrics){
  final MetricsCollector dataSetMetrics=metricsCollectionService.getCollector(MetricsScope.REACTOR,Constants.Metrics.DATASET_CONTEXT,""String_Node_Str"");
  for (  Map.Entry<TransactionAware,String> txAware : this.txAwareToMetricNames.entrySet()) {
    if (txAware.getKey() instanceof DataSetClient) {
      final String dataSetName=txAware.getValue();
      DataSetClient.DataOpsMetrics dataOpsMetrics=new DataSetClient.DataOpsMetrics(){
        @Override public void recordRead(        int opsCount){
          if (programContextMetrics != null) {
            programContextMetrics.gauge(""String_Node_Str"",1,dataSetName);
            programContextMetrics.gauge(""String_Node_Str"",1,dataSetName);
          }
          if (dataSetMetrics != null) {
            dataSetMetrics.gauge(""String_Node_Str"",1,dataSetName);
            dataSetMetrics.gauge(""String_Node_Str"",1,dataSetName);
          }
        }
        @Override public void recordWrite(        int opsCount,        int dataSize){
          if (programContextMetrics != null) {
            programContextMetrics.gauge(""String_Node_Str"",1,dataSetName);
            programContextMetrics.gauge(""String_Node_Str"",dataSize,dataSetName);
            programContextMetrics.gauge(""String_Node_Str"",1,dataSetName);
          }
          if (dataSetMetrics != null) {
            dataSetMetrics.gauge(""String_Node_Str"",1,dataSetName);
            dataSetMetrics.gauge(""String_Node_Str"",dataSize,dataSetName);
            dataSetMetrics.gauge(""String_Node_Str"",1,dataSetName);
          }
        }
      }
;
      ((DataSetClient)txAware.getKey()).setMetricsCollector(dataOpsMetrics);
    }
  }
}"
7650,"@Override public void recordWrite(int opsCount,int dataSize){
  if (programContextMetrics != null) {
    programContextMetrics.gauge(""String_Node_Str"",1,dataSetName);
    programContextMetrics.gauge(""String_Node_Str"",dataSize,dataSetName);
    programContextMetrics.gauge(""String_Node_Str"",1);
  }
}","@Override public void recordWrite(int opsCount,int dataSize){
  if (programContextMetrics != null) {
    programContextMetrics.gauge(""String_Node_Str"",1,dataSetName);
    programContextMetrics.gauge(""String_Node_Str"",dataSize,dataSetName);
    programContextMetrics.gauge(""String_Node_Str"",1,dataSetName);
  }
  if (dataSetMetrics != null) {
    dataSetMetrics.gauge(""String_Node_Str"",1,dataSetName);
    dataSetMetrics.gauge(""String_Node_Str"",dataSize,dataSetName);
    dataSetMetrics.gauge(""String_Node_Str"",1,dataSetName);
  }
}"
7651,"private AggregatesScanResult findNextResult(){
  while (currentTag != null && currentTag.hasNext()) {
    Map.Entry<byte[],byte[]> tagValue=currentTag.next();
    String tag=Bytes.toString(tagValue.getKey());
    if (!tag.startsWith(tagPrefix)) {
      continue;
    }
    if (MetricsConstants.EMPTY_TAG.equals(tag)) {
      tag=null;
    }
    return new AggregatesScanResult(context,metric,rid,tag,Bytes.toLong(tagValue.getValue()));
  }
  return null;
}","private AggregatesScanResult findNextResult(){
  while (currentTag != null && currentTag.hasNext()) {
    Map.Entry<byte[],byte[]> tagValue=currentTag.next();
    String tag=Bytes.toString(tagValue.getKey());
    if (tagPrefix != null && !tag.startsWith(tagPrefix)) {
      continue;
    }
    if (MetricsConstants.EMPTY_TAG.equals(tag)) {
      tag=null;
    }
    return new AggregatesScanResult(context,metric,rid,tag,Bytes.toLong(tagValue.getValue()));
  }
  return null;
}"
7652,"private Iterator<AggregatesScanResult> createIterator(){
  return new AbstractIterator<AggregatesScanResult>(){
    private String context;
    private String metric;
    private String rid;
    private Iterator<Map.Entry<byte[],byte[]>> currentTag=null;
    @Override protected AggregatesScanResult computeNext(){
      AggregatesScanResult result=findNextResult();
      if (result != null) {
        return result;
      }
      ImmutablePair<byte[],Map<byte[],byte[]>> rowResult;
      while ((rowResult=scanner.next()) != null) {
        rowScanned++;
        byte[] rowKey=rowResult.getFirst();
        int offset=0;
        context=entityCodec.decode(MetricsEntityType.CONTEXT,rowKey,offset);
        if (contextPrefix != null && !context.startsWith(contextPrefix)) {
          continue;
        }
        offset+=entityCodec.getEncodedSize(MetricsEntityType.CONTEXT);
        metric=entityCodec.decode(MetricsEntityType.METRIC,rowKey,offset);
        if (metricPrefix != null && !metric.startsWith(metricPrefix)) {
          continue;
        }
        offset+=entityCodec.getEncodedSize(MetricsEntityType.METRIC);
        rid=entityCodec.decode(MetricsEntityType.RUN,rowKey,offset);
        if (runId != null && !runId.equals(rid)) {
          continue;
        }
        currentTag=rowResult.getSecond().entrySet().iterator();
        result=findNextResult();
        if (result != null) {
          return result;
        }
      }
      scanner.close();
      return endOfData();
    }
    private AggregatesScanResult findNextResult(){
      while (currentTag != null && currentTag.hasNext()) {
        Map.Entry<byte[],byte[]> tagValue=currentTag.next();
        String tag=Bytes.toString(tagValue.getKey());
        if (!tag.startsWith(tagPrefix)) {
          continue;
        }
        if (MetricsConstants.EMPTY_TAG.equals(tag)) {
          tag=null;
        }
        return new AggregatesScanResult(context,metric,rid,tag,Bytes.toLong(tagValue.getValue()));
      }
      return null;
    }
  }
;
}","private Iterator<AggregatesScanResult> createIterator(){
  return new AbstractIterator<AggregatesScanResult>(){
    private String context;
    private String metric;
    private String rid;
    private Iterator<Map.Entry<byte[],byte[]>> currentTag=null;
    @Override protected AggregatesScanResult computeNext(){
      AggregatesScanResult result=findNextResult();
      if (result != null) {
        return result;
      }
      ImmutablePair<byte[],Map<byte[],byte[]>> rowResult;
      while ((rowResult=scanner.next()) != null) {
        rowScanned++;
        byte[] rowKey=rowResult.getFirst();
        int offset=0;
        context=entityCodec.decode(MetricsEntityType.CONTEXT,rowKey,offset);
        if (contextPrefix != null && !context.startsWith(contextPrefix)) {
          continue;
        }
        offset+=entityCodec.getEncodedSize(MetricsEntityType.CONTEXT);
        metric=entityCodec.decode(MetricsEntityType.METRIC,rowKey,offset);
        if (metricPrefix != null && !metric.startsWith(metricPrefix)) {
          continue;
        }
        offset+=entityCodec.getEncodedSize(MetricsEntityType.METRIC);
        rid=entityCodec.decode(MetricsEntityType.RUN,rowKey,offset);
        if (runId != null && !runId.equals(rid)) {
          continue;
        }
        currentTag=rowResult.getSecond().entrySet().iterator();
        result=findNextResult();
        if (result != null) {
          return result;
        }
      }
      scanner.close();
      return endOfData();
    }
    private AggregatesScanResult findNextResult(){
      while (currentTag != null && currentTag.hasNext()) {
        Map.Entry<byte[],byte[]> tagValue=currentTag.next();
        String tag=Bytes.toString(tagValue.getKey());
        if (tagPrefix != null && !tag.startsWith(tagPrefix)) {
          continue;
        }
        if (MetricsConstants.EMPTY_TAG.equals(tag)) {
          tag=null;
        }
        return new AggregatesScanResult(context,metric,rid,tag,Bytes.toLong(tagValue.getValue()));
      }
      return null;
    }
  }
;
}"
7653,"/** 
 * Scans the aggregate table.
 * @param contextPrefix Prefix of context to match
 * @param metricPrefix Prefix of metric to match
 * @param tagPrefix Prefix of tag to match
 * @return A {@link AggregatesScanner} for iterating scan over matching rows
 */
public AggregatesScanner scan(String contextPrefix,String metricPrefix,String runId,String tagPrefix){
  byte[] startRow=getPaddedKey(contextPrefix,metricPrefix,runId,0);
  byte[] endRow=getPaddedKey(contextPrefix,metricPrefix,runId,0xff);
  try {
    Scanner scanner=aggregatesTable.scan(startRow,endRow,null,getFilter(contextPrefix,metricPrefix,runId));
    return new AggregatesScanner(contextPrefix,metricPrefix,runId,tagPrefix == null ? MetricsConstants.EMPTY_TAG : tagPrefix,scanner,entityCodec);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","/** 
 * Scans the aggregate table.
 * @param contextPrefix Prefix of context to match
 * @param metricPrefix Prefix of metric to match
 * @param tagPrefix Prefix of tag to match
 * @return A {@link AggregatesScanner} for iterating scan over matching rows
 */
public AggregatesScanner scan(String contextPrefix,String metricPrefix,String runId,String tagPrefix){
  return scanFor(contextPrefix,metricPrefix,runId,tagPrefix == null ? MetricsConstants.EMPTY_TAG : tagPrefix);
}"
7654,"/** 
 * Updates aggregates for the given iterator of   {@link MetricsRecord}.
 * @throws OperationException When there is error updating the table.
 */
public void update(Iterator<MetricsRecord> records) throws OperationException {
  try {
    while (records.hasNext()) {
      MetricsRecord record=records.next();
      byte[] rowKey=getKey(record.getContext(),record.getName(),record.getRunId());
      Map<byte[],Long> increments=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
      increments.put(Bytes.toBytes(MetricsConstants.EMPTY_TAG),(long)record.getValue());
      for (      TagMetric tag : record.getTags()) {
        increments.put(Bytes.toBytes(tag.getTag()),(long)tag.getValue());
      }
      aggregatesTable.increment(rowKey,increments);
    }
  }
 catch (  Exception e) {
    throw new OperationException(StatusCode.INTERNAL_ERROR,e.getMessage(),e);
  }
}","/** 
 * Updates aggregates for the given iterator of   {@link MetricsRecord}.
 * @throws OperationException When there is an error updating the table.
 */
public void update(Iterator<MetricsRecord> records) throws OperationException {
  try {
    while (records.hasNext()) {
      MetricsRecord record=records.next();
      byte[] rowKey=getKey(record.getContext(),record.getName(),record.getRunId());
      Map<byte[],Long> increments=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
      increments.put(Bytes.toBytes(MetricsConstants.EMPTY_TAG),(long)record.getValue());
      for (      TagMetric tag : record.getTags()) {
        increments.put(Bytes.toBytes(tag.getTag()),(long)tag.getValue());
      }
      aggregatesTable.increment(rowKey,increments);
    }
  }
 catch (  Exception e) {
    throw new OperationException(StatusCode.INTERNAL_ERROR,e.getMessage(),e);
  }
}"
7655,"/** 
 * Generates an QueueName for the stream.
 * @param stream  connected to flow
 * @return An {@link QueueName} with schema as stream
 */
public static QueueName fromStream(String stream){
  URI uri=URI.create(Joiner.on(""String_Node_Str"").join(""String_Node_Str"",""String_Node_Str"",stream));
  return new QueueName(uri);
}","/** 
 * Generates an QueueName for the stream.
 * @param stream  connected to flow
 * @return An {@link QueueName} with schema as stream
 */
public static QueueName fromStream(String stream){
  URI uri=URI.create(Joiner.on(""String_Node_Str"").join(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",stream));
  return new QueueName(uri);
}"
7656,"public static QueueName fromFlowlet(String app,String flow,String flowlet,String output){
  URI uri=URI.create(Joiner.on(""String_Node_Str"").join(""String_Node_Str"",""String_Node_Str"",app,flow,flowlet,output));
  return new QueueName(uri);
}","public static QueueName fromFlowlet(String app,String flow,String flowlet,String output){
  URI uri=URI.create(Joiner.on(""String_Node_Str"").join(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",app,flow,flowlet,output));
  return new QueueName(uri);
}"
7657,"public static String prefixForFlow(String app,String flow){
  return Joiner.on(""String_Node_Str"").join(""String_Node_Str"",""String_Node_Str"",app,flow,""String_Node_Str"");
}","public static String prefixForFlow(String app,String flow){
  return Joiner.on(""String_Node_Str"").join(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",app,flow,""String_Node_Str"");
}"
7658,"/** 
 * Called from static method   {@code QueueName#from(URI)} and {@code QueueName#from(bytes[])}.
 * @param uri of the queue.
 */
private QueueName(URI uri){
  this.uri=uri;
  this.stringName=uri.toASCIIString();
  this.byteName=stringName.getBytes(Charsets.US_ASCII);
  Iterable<String> comps=Splitter.on('/').omitEmptyStrings().split(uri.getPath());
  components=new String[1 + Iterables.size(comps)];
  components[0]=uri.getHost();
  Iterator<String> iter=comps.iterator();
  for (int i=1; i < components.length; i++) {
    components[i]=iter.next();
  }
}","/** 
 * Called from static method   {@code QueueName#from(URI)} and {@code QueueName#from(bytes[])}.
 * @param uri of the queue.
 */
private QueueName(URI uri){
  this.uri=uri;
  this.stringName=uri.toASCIIString();
  this.byteName=stringName.getBytes(Charsets.US_ASCII);
  Iterable<String> comps=Splitter.on('/').omitEmptyStrings().split(uri.getPath());
  components=new String[Iterables.size(comps)];
  Iterator<String> iter=comps.iterator();
  for (int i=0; i < components.length; i++) {
    components[i]=iter.next();
  }
}"
7659,"@Test public void testQueueNameForStream(){
  QueueName queueName=QueueName.fromStream(""String_Node_Str"");
  Assert.assertFalse(queueName.isQueue());
  Assert.assertTrue(queueName.isStream());
  Assert.assertEquals(""String_Node_Str"",queueName.getFirstComponent());
  Assert.assertNull(queueName.getSecondComponent());
  Assert.assertNull(queueName.getThirdComponent());
  Assert.assertEquals(""String_Node_Str"",queueName.getSimpleName());
  queueName=QueueName.from(queueName.toURI());
  Assert.assertFalse(queueName.isQueue());
  Assert.assertTrue(queueName.isStream());
  Assert.assertEquals(""String_Node_Str"",queueName.getFirstComponent());
  Assert.assertNull(queueName.getSecondComponent());
  Assert.assertNull(queueName.getThirdComponent());
  Assert.assertEquals(""String_Node_Str"",queueName.getSimpleName());
  queueName=QueueName.from(queueName.toBytes());
  Assert.assertFalse(queueName.isQueue());
  Assert.assertTrue(queueName.isStream());
  Assert.assertEquals(""String_Node_Str"",queueName.getFirstComponent());
  Assert.assertNull(queueName.getSecondComponent());
  Assert.assertNull(queueName.getThirdComponent());
  Assert.assertEquals(""String_Node_Str"",queueName.getSimpleName());
}","@Test public void testQueueNameForStream(){
  QueueName queueName=QueueName.fromStream(""String_Node_Str"");
  verifyStreamName(queueName,""String_Node_Str"");
  queueName=QueueName.fromStream(""String_Node_Str"");
  verifyStreamName(queueName,""String_Node_Str"");
  queueName=QueueName.fromStream(""String_Node_Str"");
  verifyStreamName(queueName,""String_Node_Str"");
}"
7660,"@Test public void testQueueSpecificationGenWithWordCount() throws Exception {
  ApplicationSpecification appSpec=new WordCountApp().configure();
  ApplicationSpecificationAdapter adapter=ApplicationSpecificationAdapter.create(new ReflectionSchemaGenerator());
  ApplicationSpecification newSpec=adapter.fromJson(adapter.toJson(appSpec));
  QueueSpecificationGenerator generator=new SimpleQueueSpecificationGenerator(Id.Application.from(TEST_ACCOUNT_ID,newSpec.getName()));
  table=generator.create(newSpec.getFlows().values().iterator().next());
  Assert.assertEquals(get(FlowletConnection.Type.STREAM,""String_Node_Str"",""String_Node_Str"").iterator().next().getQueueName().toString(),""String_Node_Str"" + TEST_ACCOUNT_ID.getId() + ""String_Node_Str"");
  Assert.assertEquals(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str"").iterator().next().getQueueName().toString(),""String_Node_Str"");
  Assert.assertEquals(1,get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str"").size());
}","@Test public void testQueueSpecificationGenWithWordCount() throws Exception {
  ApplicationSpecification appSpec=new WordCountApp().configure();
  ApplicationSpecificationAdapter adapter=ApplicationSpecificationAdapter.create(new ReflectionSchemaGenerator());
  ApplicationSpecification newSpec=adapter.fromJson(adapter.toJson(appSpec));
  QueueSpecificationGenerator generator=new SimpleQueueSpecificationGenerator(Id.Application.from(TEST_ACCOUNT_ID,newSpec.getName()));
  table=generator.create(newSpec.getFlows().values().iterator().next());
  Assert.assertEquals(get(FlowletConnection.Type.STREAM,""String_Node_Str"",""String_Node_Str"").iterator().next().getQueueName().toString(),""String_Node_Str"");
  Assert.assertEquals(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str"").iterator().next().getQueueName().toString(),""String_Node_Str"");
  Assert.assertEquals(1,get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str"").size());
}"
7661,"@Test public void testQueueSpecificationGenWithToyApp() throws Exception {
  ApplicationSpecification appSpec=new ToyApp().configure();
  ApplicationSpecificationAdapter adapter=ApplicationSpecificationAdapter.create(new ReflectionSchemaGenerator());
  ApplicationSpecification newSpec=adapter.fromJson(adapter.toJson(appSpec));
  QueueSpecificationGenerator generator=new SimpleQueueSpecificationGenerator(Id.Application.from(TEST_ACCOUNT_ID,newSpec.getName()));
  table=generator.create(newSpec.getFlows().values().iterator().next());
  dumpConnectionQueue(table);
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.STREAM,""String_Node_Str"",""String_Node_Str""),""String_Node_Str"" + TEST_ACCOUNT_ID.getId() + ""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.STREAM,""String_Node_Str"",""String_Node_Str""),""String_Node_Str"" + TEST_ACCOUNT_ID.getId() + ""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
}","@Test public void testQueueSpecificationGenWithToyApp() throws Exception {
  ApplicationSpecification appSpec=new ToyApp().configure();
  ApplicationSpecificationAdapter adapter=ApplicationSpecificationAdapter.create(new ReflectionSchemaGenerator());
  ApplicationSpecification newSpec=adapter.fromJson(adapter.toJson(appSpec));
  QueueSpecificationGenerator generator=new SimpleQueueSpecificationGenerator(Id.Application.from(TEST_ACCOUNT_ID,newSpec.getName()));
  table=generator.create(newSpec.getFlows().values().iterator().next());
  dumpConnectionQueue(table);
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.STREAM,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.STREAM,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
  Assert.assertTrue(containsQueue(get(FlowletConnection.Type.FLOWLET,""String_Node_Str"",""String_Node_Str""),""String_Node_Str""));
}"
7662,"/** 
 * @return the second component of the URI (the flow for a queue, the stream name for a stream).
 */
public String getSecondComponent(){
  return getNthComponent(2);
}","/** 
 * @return the second component of the URI (the flow for a queue, the stream name for a stream).
 */
public String getSecondComponent(){
  return getNthComponent(1);
}"
7663,"/** 
 * @return the third component of the URI (the flowlet for a queue, null for a stream).
 */
public String getThirdComponent(){
  return getNthComponent(3);
}","/** 
 * @return the third component of the URI (the flowlet for a queue, null for a stream).
 */
public String getThirdComponent(){
  return getNthComponent(2);
}"
7664,"/** 
 * Called from static method   {@code QueueName#from(URI)} and {@code QueueName#from(bytes[])}.
 * @param uri of the queue.
 */
private QueueName(URI uri){
  this.uri=uri;
  this.stringName=uri.toASCIIString();
  this.byteName=stringName.getBytes(Charsets.US_ASCII);
  List<String> comps=Lists.asList(uri.getHost(),uri.getPath().split(""String_Node_Str""));
  this.components=comps.toArray(new String[comps.size()]);
}","/** 
 * Called from static method   {@code QueueName#from(URI)} and {@code QueueName#from(bytes[])}.
 * @param uri of the queue.
 */
private QueueName(URI uri){
  this.uri=uri;
  this.stringName=uri.toASCIIString();
  this.byteName=stringName.getBytes(Charsets.US_ASCII);
  Iterable<String> comps=Splitter.on('/').omitEmptyStrings().split(uri.getPath());
  components=new String[1 + Iterables.size(comps)];
  components[0]=uri.getHost();
  Iterator<String> iter=comps.iterator();
  for (int i=1; i < components.length; i++) {
    components[i]=iter.next();
  }
}"
7665,"/** 
 * For guice injecting configuration object to this singleton.
 */
@Inject public void setConfiguration(@Named(""String_Node_Str"") CConfiguration config) throws IOException {
  basePath=config.get(Constants.CFG_DATA_LEVELDB_DIR);
  Preconditions.checkNotNull(basePath,""String_Node_Str"");
  blockSize=config.getInt(Constants.CFG_DATA_LEVELDB_BLOCKSIZE,Constants.DEFAULT_DATA_LEVELDB_BLOCKSIZE);
  cacheSize=config.getLong(Constants.CFG_DATA_LEVELDB_CACHESIZE,Constants.DEFAULT_DATA_LEVELDB_CACHESIZE);
  writeOptions=new WriteOptions().sync(config.getBoolean(Constants.CFG_DATA_LEVELDB_FSYNC,Constants.DEFAULT_DATA_LEVELDB_FSYNC));
}","/** 
 * For guice injecting configuration object to this singleton.
 */
@Inject public void setConfiguration(@Named(""String_Node_Str"") CConfiguration config) throws IOException {
  tables.clear();
  basePath=config.get(Constants.CFG_DATA_LEVELDB_DIR);
  Preconditions.checkNotNull(basePath,""String_Node_Str"");
  blockSize=config.getInt(Constants.CFG_DATA_LEVELDB_BLOCKSIZE,Constants.DEFAULT_DATA_LEVELDB_BLOCKSIZE);
  cacheSize=config.getLong(Constants.CFG_DATA_LEVELDB_CACHESIZE,Constants.DEFAULT_DATA_LEVELDB_CACHESIZE);
  writeOptions=new WriteOptions().sync(config.getBoolean(Constants.CFG_DATA_LEVELDB_FSYNC,Constants.DEFAULT_DATA_LEVELDB_FSYNC));
}"
7666,"/** 
 * Helper method to select the queue or stream admin, and to ensure it's table exists.
 * @param queueName name of the queue to be opened.
 * @return the queue admin for that queue.
 * @throws IOException
 */
private LevelDBQueueAdmin ensureTableExists(QueueName queueName) throws IOException {
  LevelDBQueueAdmin admin=queueName.isStream() ? streamAdmin : queueAdmin;
  String queueTableName=admin.getActualTableName(queueName);
  try {
    admin.create(queueTableName);
  }
 catch (  Exception e) {
    throw new IOException(""String_Node_Str"" + queueTableName,e);
  }
  return admin;
}","/** 
 * Helper method to select the queue or stream admin, and to ensure it's table exists.
 * @param queueName name of the queue to be opened.
 * @return the queue admin for that queue.
 * @throws IOException
 */
private LevelDBQueueAdmin ensureTableExists(QueueName queueName) throws IOException {
  LevelDBQueueAdmin admin=queueName.isStream() ? streamAdmin : queueAdmin;
  try {
    admin.create(queueName);
  }
 catch (  Exception e) {
    throw new IOException(""String_Node_Str"" + admin.getActualTableName(queueName),e);
  }
  return admin;
}"
7667,"private void startCleanupThread(){
  if (cleanupInterval <= 0 && defaultTimeout <= 0) {
    return;
  }
  LOG.info(""String_Node_Str"" + cleanupInterval + ""String_Node_Str""+ defaultTimeout+ ""String_Node_Str"");
  this.cleanupThread=new DaemonThreadExecutor(""String_Node_Str""){
    @Override public void doRun(){
      cleanupTimedOutTransactions();
    }
    @Override public long getSleepMillis(){
      return cleanupInterval * 1000;
    }
  }
;
  cleanupThread.start();
}","private void startCleanupThread(){
  if (cleanupInterval <= 0 || defaultTimeout <= 0) {
    return;
  }
  LOG.info(""String_Node_Str"" + cleanupInterval + ""String_Node_Str""+ defaultTimeout+ ""String_Node_Str"");
  this.cleanupThread=new DaemonThreadExecutor(""String_Node_Str""){
    @Override public void doRun(){
      cleanupTimedOutTransactions();
    }
    @Override public long getSleepMillis(){
      return cleanupInterval * 1000;
    }
  }
;
  cleanupThread.start();
}"
7668,"/** 
 * @return the second component of the URI (the flow for a queue, the stream name for a stream).
 */
public String getSecondComponent(){
  return getNthComponent(1);
}","/** 
 * @return the second component of the URI (the flow for a queue, the stream name for a stream).
 */
public String getSecondComponent(){
  return getNthComponent(2);
}"
7669,"/** 
 * @return the third component of the URI (the flowlet for a queue, null for a stream).
 */
public String getThirdComponent(){
  return getNthComponent(2);
}","/** 
 * @return the third component of the URI (the flowlet for a queue, null for a stream).
 */
public String getThirdComponent(){
  return getNthComponent(3);
}"
7670,"@Override public void getLogPrev(final LoggingContext loggingContext,final long fromOffset,final int maxEvents,final Filter filter,final Callback callback){
  executor.submit(new Runnable(){
    @Override public void run(){
      int partition=partitioner.partition(loggingContext.getLogPartition(),numPartitions);
      callback.init();
      KafkaConsumer kafkaConsumer=new KafkaConsumer(seedBrokers,topic,partition,kafkaTailFetchTimeoutMs);
      try {
        Filter logFilter=new AndFilter(ImmutableList.of(LoggingContextHelper.createFilter(loggingContext),filter));
        long latestOffset=kafkaConsumer.fetchOffset(KafkaConsumer.Offset.LATEST);
        long earliestOffset=kafkaConsumer.fetchOffset(KafkaConsumer.Offset.EARLIEST);
        long startOffset=fromOffset - maxEvents;
        long stopOffset=fromOffset;
        int adjMaxEvents=maxEvents;
        if (fromOffset < 0) {
          startOffset=latestOffset - maxEvents;
          stopOffset=latestOffset;
        }
        if (startOffset < earliestOffset) {
          startOffset=earliestOffset;
          adjMaxEvents=(int)(fromOffset - startOffset);
        }
        if (startOffset >= stopOffset || startOffset >= latestOffset) {
          return;
        }
        int fetchCount=0;
        while (fetchCount == 0) {
          fetchCount=fetchLogEvents(kafkaConsumer,logFilter,startOffset,stopOffset,adjMaxEvents,callback);
          stopOffset=startOffset;
          if (stopOffset <= earliestOffset) {
            break;
          }
          startOffset=startOffset - adjMaxEvents;
          if (startOffset < earliestOffset) {
            startOffset=earliestOffset;
          }
        }
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
        throw Throwables.propagate(e);
      }
 finally {
        try {
          try {
            callback.close();
          }
  finally {
            kafkaConsumer.close();
          }
        }
 catch (        IOException e) {
          LOG.error(String.format(""String_Node_Str"",topic,partition),e);
        }
      }
    }
  }
);
}","@Override public void getLogPrev(final LoggingContext loggingContext,final long fromOffset,final int maxEvents,final Filter filter,final Callback callback){
  executor.submit(new Runnable(){
    @Override public void run(){
      int partition=partitioner.partition(loggingContext.getLogPartition(),numPartitions);
      callback.init();
      KafkaConsumer kafkaConsumer=new KafkaConsumer(seedBrokers,topic,partition,kafkaTailFetchTimeoutMs);
      try {
        Filter logFilter=new AndFilter(ImmutableList.of(LoggingContextHelper.createFilter(loggingContext),filter));
        long latestOffset=kafkaConsumer.fetchOffset(KafkaConsumer.Offset.LATEST);
        long earliestOffset=kafkaConsumer.fetchOffset(KafkaConsumer.Offset.EARLIEST);
        long stopOffset;
        long startOffset;
        if (fromOffset < 0) {
          stopOffset=latestOffset;
        }
 else {
          stopOffset=fromOffset;
        }
        startOffset=stopOffset - maxEvents;
        if (startOffset < earliestOffset) {
          startOffset=earliestOffset;
        }
        if (startOffset >= stopOffset || startOffset >= latestOffset) {
          return;
        }
        int fetchCount=0;
        while (fetchCount == 0) {
          fetchCount=fetchLogEvents(kafkaConsumer,logFilter,startOffset,stopOffset,maxEvents,callback);
          stopOffset=startOffset;
          if (stopOffset <= earliestOffset) {
            break;
          }
          startOffset=stopOffset - maxEvents;
          if (startOffset < earliestOffset) {
            startOffset=earliestOffset;
          }
        }
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
        throw Throwables.propagate(e);
      }
 finally {
        try {
          try {
            callback.close();
          }
  finally {
            kafkaConsumer.close();
          }
        }
 catch (        IOException e) {
          LOG.error(String.format(""String_Node_Str"",topic,partition),e);
        }
      }
    }
  }
);
}"
7671,"private <T>Verifier<T> getVerifier(Class<? extends T> clz){
  if (verifiers.containsKey(clz)) {
    return (Verifier<T>)verifiers.get(clz);
  }
  if (clz.equals(ApplicationSpecification.class)) {
    verifiers.put(clz,new ApplicationVerification());
  }
 else   if (clz.equals(StreamSpecification.class)) {
    verifiers.put(clz,new StreamVerification());
  }
 else   if (clz.equals(DataSetSpecification.class)) {
    verifiers.put(clz,new DataSetVerification());
  }
 else   if (clz.equals(FlowSpecification.class)) {
    verifiers.put(clz,new FlowVerification());
  }
 else   if (ProgramSpecification.class.isAssignableFrom(clz)) {
    verifiers.put(clz,createProgramVerifier((Class<ProgramSpecification>)clz));
  }
  return (Verifier<T>)verifiers.get(clz);
}","private <T>Verifier<T> getVerifier(Class<? extends T> clz){
  if (verifiers.containsKey(clz)) {
    return (Verifier<T>)verifiers.get(clz);
  }
  if (ApplicationSpecification.class.isAssignableFrom(clz)) {
    verifiers.put(clz,new ApplicationVerification());
  }
 else   if (StreamSpecification.class.isAssignableFrom(clz)) {
    verifiers.put(clz,new StreamVerification());
  }
 else   if (DataSetSpecification.class.isAssignableFrom(clz)) {
    verifiers.put(clz,new DataSetVerification());
  }
 else   if (FlowSpecification.class.isAssignableFrom(clz)) {
    verifiers.put(clz,new FlowVerification());
  }
 else   if (ProgramSpecification.class.isAssignableFrom(clz)) {
    verifiers.put(clz,createProgramVerifier((Class<ProgramSpecification>)clz));
  }
  return (Verifier<T>)verifiers.get(clz);
}"
7672,"@Test public void testInjection() throws IOException {
  Injector injector=Guice.createInjector(new DataFabricModules().getSingleNodeModules());
  QueueClientFactory factory=injector.getInstance(QueueClientFactory.class);
  Queue2Producer producer=factory.createProducer(QueueName.fromStream(""String_Node_Str"",""String_Node_Str""));
  Assert.assertTrue(producer instanceof LevelDBQueue2Producer);
  producer=factory.createProducer(QueueName.fromFlowlet(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  Assert.assertTrue(producer instanceof InMemoryQueue2Producer);
}","@Test public void testInjection() throws IOException {
  Injector injector=Guice.createInjector(new DataFabricModules().getSingleNodeModules(conf));
  QueueClientFactory factory=injector.getInstance(QueueClientFactory.class);
  Queue2Producer producer=factory.createProducer(QueueName.fromStream(""String_Node_Str"",""String_Node_Str""));
  Assert.assertTrue(producer instanceof LevelDBQueue2Producer);
  producer=factory.createProducer(QueueName.fromFlowlet(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  Assert.assertTrue(producer instanceof InMemoryQueue2Producer);
}"
7673,"@BeforeClass public static void init() throws Exception {
  CConfiguration conf=CConfiguration.create();
  conf.unset(Constants.CFG_DATA_LEVELDB_DIR);
  conf.setBoolean(Constants.Transaction.Manager.CFG_DO_PERSIST,false);
  Injector injector=Guice.createInjector(new DataFabricLocalModule(conf));
  transactionManager=injector.getInstance(InMemoryTransactionManager.class);
  transactionManager.startAndWait();
  txSystemClient=injector.getInstance(TransactionSystemClient.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
  streamAdmin=injector.getInstance(StreamAdmin.class);
  executorFactory=injector.getInstance(TransactionExecutorFactory.class);
}","@BeforeClass public static void init() throws Exception {
  conf=CConfiguration.create();
  conf.unset(Constants.CFG_DATA_LEVELDB_DIR);
  conf.setBoolean(Constants.Transaction.Manager.CFG_DO_PERSIST,false);
  Injector injector=Guice.createInjector(new DataFabricLocalModule(conf));
  transactionManager=injector.getInstance(InMemoryTransactionManager.class);
  transactionManager.startAndWait();
  txSystemClient=injector.getInstance(TransactionSystemClient.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
  streamAdmin=injector.getInstance(StreamAdmin.class);
  executorFactory=injector.getInstance(TransactionExecutorFactory.class);
}"
7674,"@Override public void stop(){
  LOG.info(""String_Node_Str"" + name);
  Futures.getUnchecked(Services.chainStart(logSaver,multiElection,kafkaClientService,zkClientService));
  runLatch.countDown();
}","@Override public void stop(){
  LOG.info(""String_Node_Str"" + name);
  Futures.getUnchecked(Services.chainStop(multiElection,logSaver,kafkaClientService,zkClientService));
  runLatch.countDown();
}"
7675,"@Override public void run(){
  LOG.info(""String_Node_Str"" + name);
  Futures.getUnchecked(Services.chainStart(zkClientService,kafkaClientService,logSaver));
  multiElection.startAndWait();
  LOG.info(""String_Node_Str"" + name);
  try {
    runLatch.await();
    LOG.info(""String_Node_Str"" + name);
  }
 catch (  InterruptedException e) {
    LOG.error(""String_Node_Str"");
    Thread.currentThread().interrupt();
  }
}","@Override public void run(){
  LOG.info(""String_Node_Str"" + name);
  Futures.getUnchecked(Services.chainStart(zkClientService,kafkaClientService,logSaver,multiElection));
  LOG.info(""String_Node_Str"" + name);
  try {
    runLatch.await();
    LOG.info(""String_Node_Str"" + name);
  }
 catch (  InterruptedException e) {
    LOG.error(""String_Node_Str"");
    Thread.currentThread().interrupt();
  }
}"
7676,"/** 
 * Deletes dir if it is empty, and recursively deletes parent dirs if they are empty too. The recursion stops at non-empty parent or base directory. If dir is not child of base directory then the recursion stops at root.
 * @param dir dir to be deleted.
 */
void deleteEmptyDir(Location dir){
  dir=LocationUtils.normalize(locationFactory,dir);
  LOG.debug(""String_Node_Str"",dir.toURI());
  if (logBaseDir.toURI().equals(dir.toURI()) || !dir.toURI().getRawPath().startsWith(logBaseDir.toURI().getRawPath())) {
    LOG.debug(""String_Node_Str"",dir.toURI());
    return;
  }
  try {
    if (dir.delete()) {
      LOG.info(""String_Node_Str"",dir.toURI());
      Location parent=LocationUtils.getParent(locationFactory,dir);
      LOG.debug(""String_Node_Str"",parent);
      deleteEmptyDir(parent);
    }
 else {
      LOG.debug(""String_Node_Str"",dir.toURI());
    }
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",dir.toURI(),e);
  }
}","/** 
 * Deletes dir if it is empty, and recursively deletes parent dirs if they are empty too. The recursion stops at non-empty parent or base directory. If dir is not child of base directory then the recursion stops at root.
 * @param dir dir to be deleted.
 */
void deleteEmptyDir(Location dir){
  dir=LocationUtils.normalize(locationFactory,dir);
  LOG.debug(""String_Node_Str"",dir.toURI());
  if (logBaseDir.toURI().equals(dir.toURI()) || !dir.toURI().getRawPath().startsWith(logBaseDir.toURI().getRawPath())) {
    LOG.debug(""String_Node_Str"",dir.toURI());
    return;
  }
  try {
    if (dir.delete()) {
      LOG.info(""String_Node_Str"",dir.toURI());
      Location parent=LocationUtils.getParent(locationFactory,dir);
      LOG.debug(""String_Node_Str"",parent);
      deleteEmptyDir(parent);
    }
 else {
      LOG.debug(""String_Node_Str"",dir.toURI());
    }
  }
 catch (  RemoteException e) {
    LOG.debug(""String_Node_Str"",dir.toURI(),e);
  }
catch (  IOException e) {
    LOG.error(""String_Node_Str"",dir.toURI(),e);
  }
}"
7677,"@Override public void stop(){
  LOG.info(""String_Node_Str"" + name);
  Futures.getUnchecked(Services.chainStart(logSaver,multiElection,kafkaClientService,zkClientService));
  runLatch.countDown();
}","@Override public void stop(){
  LOG.info(""String_Node_Str"" + name);
  Futures.getUnchecked(Services.chainStop(multiElection,logSaver,kafkaClientService,zkClientService));
  runLatch.countDown();
}"
7678,"@Override public void run(){
  LOG.info(""String_Node_Str"" + name);
  Futures.getUnchecked(Services.chainStart(zkClientService,kafkaClientService,logSaver));
  multiElection.startAndWait();
  LOG.info(""String_Node_Str"" + name);
  try {
    runLatch.await();
    LOG.info(""String_Node_Str"" + name);
  }
 catch (  InterruptedException e) {
    LOG.error(""String_Node_Str"");
    Thread.currentThread().interrupt();
  }
}","@Override public void run(){
  LOG.info(""String_Node_Str"" + name);
  Futures.getUnchecked(Services.chainStart(zkClientService,kafkaClientService,logSaver,multiElection));
  LOG.info(""String_Node_Str"" + name);
  try {
    runLatch.await();
    LOG.info(""String_Node_Str"" + name);
  }
 catch (  InterruptedException e) {
    LOG.error(""String_Node_Str"");
    Thread.currentThread().interrupt();
  }
}"
7679,"/** 
 * Deletes dir if it is empty, and recursively deletes parent dirs if they are empty too. The recursion stops at non-empty parent or base directory. If dir is not child of base directory then the recursion stops at root.
 * @param dir dir to be deleted.
 */
void deleteEmptyDir(Location dir){
  dir=LocationUtils.normalize(locationFactory,dir);
  LOG.debug(""String_Node_Str"",dir.toURI());
  if (logBaseDir.toURI().equals(dir.toURI()) || !dir.toURI().getRawPath().startsWith(logBaseDir.toURI().getRawPath())) {
    LOG.debug(""String_Node_Str"",dir.toURI());
    return;
  }
  try {
    if (dir.delete()) {
      LOG.info(""String_Node_Str"",dir.toURI());
      Location parent=LocationUtils.getParent(locationFactory,dir);
      LOG.debug(""String_Node_Str"",parent);
      deleteEmptyDir(parent);
    }
 else {
      LOG.debug(""String_Node_Str"",dir.toURI());
    }
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",dir.toURI(),e);
  }
}","/** 
 * Deletes dir if it is empty, and recursively deletes parent dirs if they are empty too. The recursion stops at non-empty parent or base directory. If dir is not child of base directory then the recursion stops at root.
 * @param dir dir to be deleted.
 */
void deleteEmptyDir(Location dir){
  dir=LocationUtils.normalize(locationFactory,dir);
  LOG.debug(""String_Node_Str"",dir.toURI());
  if (logBaseDir.toURI().equals(dir.toURI()) || !dir.toURI().getRawPath().startsWith(logBaseDir.toURI().getRawPath())) {
    LOG.debug(""String_Node_Str"",dir.toURI());
    return;
  }
  try {
    if (dir.delete()) {
      LOG.info(""String_Node_Str"",dir.toURI());
      Location parent=LocationUtils.getParent(locationFactory,dir);
      LOG.debug(""String_Node_Str"",parent);
      deleteEmptyDir(parent);
    }
 else {
      LOG.debug(""String_Node_Str"",dir.toURI());
    }
  }
 catch (  RemoteException e) {
    LOG.debug(""String_Node_Str"",dir.toURI(),e);
  }
catch (  IOException e) {
    LOG.error(""String_Node_Str"",dir.toURI(),e);
  }
}"
7680,"private void submit(final MapReduce job,MapReduceSpecification mapredSpec,Location jobJarLocation,final BasicMapReduceContext context,final DataSetInstantiator dataSetInstantiator) throws Exception {
  Configuration mapredConf=new Configuration(hConf);
  int mapperMemory=mapredSpec.getMapperMemoryMB();
  int reducerMemory=mapredSpec.getReducerMemoryMB();
  mapredConf.setInt(""String_Node_Str"",mapperMemory);
  mapredConf.setInt(""String_Node_Str"",reducerMemory);
  mapredConf.set(""String_Node_Str"",""String_Node_Str"" + mapperMemory + ""String_Node_Str"");
  mapredConf.set(""String_Node_Str"",""String_Node_Str"" + reducerMemory + ""String_Node_Str"");
  jobConf=Job.getInstance(mapredConf);
  context.setJob(jobConf);
  beforeSubmit(job,context,dataSetInstantiator);
  wrapMapperClassIfNeeded(jobConf);
  wrapReducerClassIfNeeded(jobConf);
  setInputDataSetIfNeeded(jobConf,context);
  setOutputDataSetIfNeeded(jobConf,context);
  final Location jobJar=buildJobJar(context);
  LOG.info(""String_Node_Str"" + jobJar.toURI().toString());
  final Location programJarCopy=createJobJarTempCopy(jobJarLocation);
  LOG.info(""String_Node_Str"" + programJarCopy.toURI().toString() + ""String_Node_Str""+ jobJarLocation.toURI().toString());
  jobConf.setJar(jobJar.toURI().toString());
  jobConf.addFileToClassPath(new Path(programJarCopy.toURI()));
  jobConf.getConfiguration().setClassLoader(context.getProgram().getClassLoader());
  MapReduceContextProvider contextProvider=new MapReduceContextProvider(jobConf);
  final Transaction tx=txSystemClient.startLong();
  contextProvider.set(context,cConf,tx,programJarCopy.getName());
  new Thread(){
    @Override public void run(){
      boolean success=false;
      try {
        LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
        try {
          LOG.info(""String_Node_Str"",context.toString());
          jobConf.submit();
          while (!jobConf.isComplete()) {
            reportStats(context);
            TimeUnit.MILLISECONDS.sleep(1000);
          }
          LOG.info(""String_Node_Str"" + jobConf.getStatus() + ""String_Node_Str""+ success+ ""String_Node_Str""+ context.toString());
          reportStats(context);
          success=jobConf.isSuccessful();
        }
 catch (        InterruptedException e) {
          throw Throwables.propagate(e);
        }
catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
 finally {
        stopController(success,context,job,tx,dataSetInstantiator);
        try {
          jobJar.delete();
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"" + jobJar.toURI());
        }
        try {
          programJarCopy.delete();
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"" + programJarCopy.toURI());
        }
      }
    }
  }
.start();
}","private void submit(final MapReduce job,MapReduceSpecification mapredSpec,Location jobJarLocation,final BasicMapReduceContext context,final DataSetInstantiator dataSetInstantiator) throws Exception {
  Configuration mapredConf=new Configuration(hConf);
  int mapperMemory=mapredSpec.getMapperMemoryMB();
  int reducerMemory=mapredSpec.getReducerMemoryMB();
  mapredConf.setInt(""String_Node_Str"",mapperMemory);
  mapredConf.setInt(""String_Node_Str"",reducerMemory);
  mapredConf.set(""String_Node_Str"",""String_Node_Str"" + mapperMemory + ""String_Node_Str"");
  mapredConf.set(""String_Node_Str"",""String_Node_Str"" + reducerMemory + ""String_Node_Str"");
  jobConf=Job.getInstance(mapredConf);
  context.setJob(jobConf);
  beforeSubmit(job,context,dataSetInstantiator);
  wrapMapperClassIfNeeded(jobConf);
  wrapReducerClassIfNeeded(jobConf);
  setInputDataSetIfNeeded(jobConf,context);
  setOutputDataSetIfNeeded(jobConf,context);
  final Location jobJar=buildJobJar(context);
  LOG.info(""String_Node_Str"" + jobJar.toURI().toString());
  final Location programJarCopy=createJobJarTempCopy(jobJarLocation);
  LOG.info(""String_Node_Str"" + programJarCopy.toURI().toString() + ""String_Node_Str""+ jobJarLocation.toURI().toString());
  jobConf.setJar(jobJar.toURI().toString());
  jobConf.addFileToClassPath(new Path(programJarCopy.toURI()));
  jobConf.getConfiguration().setClassLoader(context.getProgram().getClassLoader());
  MapReduceContextProvider contextProvider=new MapReduceContextProvider(jobConf);
  final Transaction tx=txSystemClient.startLong();
  contextProvider.set(context,cConf,tx,programJarCopy.getName());
  new Thread(){
    @Override public void run(){
      boolean success=false;
      try {
        LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
        try {
          LOG.info(""String_Node_Str"",context.toString());
          jobConf.submit();
          initializeStats();
          while (!jobConf.isComplete()) {
            reportStats(context);
            TimeUnit.MILLISECONDS.sleep(1000);
          }
          LOG.info(""String_Node_Str"" + jobConf.getStatus() + ""String_Node_Str""+ success+ ""String_Node_Str""+ context.toString());
          reportStats(context);
          TimeUnit.SECONDS.sleep(2L);
          success=jobConf.isSuccessful();
        }
 catch (        InterruptedException e) {
          throw Throwables.propagate(e);
        }
catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
 finally {
        stopController(success,context,job,tx,dataSetInstantiator);
        try {
          jobJar.delete();
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"" + jobJar.toURI());
        }
        try {
          programJarCopy.delete();
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"" + programJarCopy.toURI());
        }
      }
    }
  }
.start();
}"
7681,"@Override public void run(){
  boolean success=false;
  try {
    LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
    try {
      LOG.info(""String_Node_Str"",context.toString());
      jobConf.submit();
      while (!jobConf.isComplete()) {
        reportStats(context);
        TimeUnit.MILLISECONDS.sleep(1000);
      }
      LOG.info(""String_Node_Str"" + jobConf.getStatus() + ""String_Node_Str""+ success+ ""String_Node_Str""+ context.toString());
      reportStats(context);
      success=jobConf.isSuccessful();
    }
 catch (    InterruptedException e) {
      throw Throwables.propagate(e);
    }
catch (    Exception e) {
      throw Throwables.propagate(e);
    }
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
 finally {
    stopController(success,context,job,tx,dataSetInstantiator);
    try {
      jobJar.delete();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"" + jobJar.toURI());
    }
    try {
      programJarCopy.delete();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"" + programJarCopy.toURI());
    }
  }
}","@Override public void run(){
  boolean success=false;
  try {
    LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
    try {
      LOG.info(""String_Node_Str"",context.toString());
      jobConf.submit();
      initializeStats();
      while (!jobConf.isComplete()) {
        reportStats(context);
        TimeUnit.MILLISECONDS.sleep(1000);
      }
      LOG.info(""String_Node_Str"" + jobConf.getStatus() + ""String_Node_Str""+ success+ ""String_Node_Str""+ context.toString());
      reportStats(context);
      TimeUnit.SECONDS.sleep(2L);
      success=jobConf.isSuccessful();
    }
 catch (    InterruptedException e) {
      throw Throwables.propagate(e);
    }
catch (    Exception e) {
      throw Throwables.propagate(e);
    }
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
 finally {
    stopController(success,context,job,tx,dataSetInstantiator);
    try {
      jobJar.delete();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"" + jobJar.toURI());
    }
    try {
      programJarCopy.delete();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"" + programJarCopy.toURI());
    }
  }
}"
7682,"private void reportStats(BasicMapReduceContext context) throws IOException, InterruptedException {
  float mapProgress=jobConf.getStatus().getMapProgress();
  int runningMappers=0;
  int runningReducers=0;
  for (  TaskReport tr : jobConf.getTaskReports(TaskType.MAP)) {
    runningMappers+=tr.getRunningTaskAttemptIds().size();
  }
  for (  TaskReport tr : jobConf.getTaskReports(TaskType.REDUCE)) {
    runningReducers+=tr.getRunningTaskAttemptIds().size();
  }
  int memoryPerMapper=context.getSpecification().getMapperMemoryMB();
  int memoryPerReducer=context.getSpecification().getReducerMemoryMB();
  long mapInputRecords=getTaskCounter(jobConf,TaskCounter.MAP_INPUT_RECORDS);
  long mapOutputRecords=getTaskCounter(jobConf,TaskCounter.MAP_OUTPUT_RECORDS);
  long mapOutputBytes=getTaskCounter(jobConf,TaskCounter.MAP_OUTPUT_BYTES);
  context.getSystemMapperMetrics().gauge(""String_Node_Str"",(int)(mapProgress * 100));
  context.getSystemMapperMetrics().gauge(""String_Node_Str"",(int)mapInputRecords);
  context.getSystemMapperMetrics().gauge(""String_Node_Str"",(int)mapOutputRecords);
  context.getSystemMapperMetrics().gauge(""String_Node_Str"",(int)mapOutputBytes);
  context.getSystemMapperMetrics().gauge(""String_Node_Str"",runningMappers);
  context.getSystemMapperMetrics().gauge(""String_Node_Str"",runningMappers * memoryPerMapper);
  LOG.trace(""String_Node_Str"",(int)(mapProgress * 100),mapInputRecords,mapOutputRecords,mapOutputBytes,runningMappers,runningMappers * memoryPerMapper);
  float reduceProgress=jobConf.getStatus().getReduceProgress();
  long reduceInputRecords=getTaskCounter(jobConf,TaskCounter.REDUCE_INPUT_RECORDS);
  long reduceOutputRecords=getTaskCounter(jobConf,TaskCounter.REDUCE_OUTPUT_RECORDS);
  context.getSystemReducerMetrics().gauge(""String_Node_Str"",(int)(reduceProgress * 100));
  context.getSystemReducerMetrics().gauge(""String_Node_Str"",(int)reduceInputRecords);
  context.getSystemReducerMetrics().gauge(""String_Node_Str"",(int)reduceOutputRecords);
  context.getSystemReducerMetrics().gauge(""String_Node_Str"",runningReducers);
  context.getSystemReducerMetrics().gauge(""String_Node_Str"",runningReducers * memoryPerReducer);
  LOG.trace(""String_Node_Str"",(int)(reduceProgress * 100),reduceInputRecords,reduceOutputRecords,runningReducers,runningReducers * memoryPerReducer);
}","private void reportStats(BasicMapReduceContext context) throws IOException, InterruptedException {
  float mapProgress=jobConf.getStatus().getMapProgress();
  int runningMappers=0;
  int runningReducers=0;
  for (  TaskReport tr : jobConf.getTaskReports(TaskType.MAP)) {
    runningMappers+=tr.getRunningTaskAttemptIds().size();
  }
  for (  TaskReport tr : jobConf.getTaskReports(TaskType.REDUCE)) {
    runningReducers+=tr.getRunningTaskAttemptIds().size();
  }
  int memoryPerMapper=context.getSpecification().getMapperMemoryMB();
  int memoryPerReducer=context.getSpecification().getReducerMemoryMB();
  long mapInputRecords=getTaskCounter(jobConf,TaskCounter.MAP_INPUT_RECORDS);
  long mapOutputRecords=getTaskCounter(jobConf,TaskCounter.MAP_OUTPUT_RECORDS);
  long mapOutputBytes=getTaskCounter(jobConf,TaskCounter.MAP_OUTPUT_BYTES);
  context.getSystemMapperMetrics().gauge(METRIC_COMPLETION,(int)(mapProgress * 100));
  context.getSystemMapperMetrics().gauge(METRIC_INPUT_RECORDS,(int)mapInputRecords - previousMapStats.get(METRIC_INPUT_RECORDS));
  context.getSystemMapperMetrics().gauge(METRIC_OUTPUT_RECORDS,(int)mapOutputRecords - previousMapStats.get(METRIC_OUTPUT_RECORDS));
  context.getSystemMapperMetrics().gauge(METRIC_BYTES,(int)mapOutputBytes - previousMapStats.get(METRIC_BYTES));
  context.getSystemMapperMetrics().gauge(METRIC_USED_CONTAINERS,runningMappers);
  context.getSystemMapperMetrics().gauge(METRIC_USED_MEMORY,runningMappers * memoryPerMapper);
  previousMapStats.put(METRIC_INPUT_RECORDS,(int)mapInputRecords);
  previousMapStats.put(METRIC_OUTPUT_RECORDS,(int)mapOutputRecords);
  previousMapStats.put(METRIC_BYTES,(int)mapOutputBytes);
  LOG.trace(""String_Node_Str"",(int)(mapProgress * 100),mapInputRecords,mapOutputRecords,mapOutputBytes,runningMappers,runningMappers * memoryPerMapper);
  float reduceProgress=jobConf.getStatus().getReduceProgress();
  long reduceInputRecords=getTaskCounter(jobConf,TaskCounter.REDUCE_INPUT_RECORDS);
  long reduceOutputRecords=getTaskCounter(jobConf,TaskCounter.REDUCE_OUTPUT_RECORDS);
  context.getSystemReducerMetrics().gauge(METRIC_COMPLETION,(int)(reduceProgress * 100));
  context.getSystemReducerMetrics().gauge(METRIC_INPUT_RECORDS,(int)reduceInputRecords - previousReduceStats.get(METRIC_INPUT_RECORDS));
  context.getSystemReducerMetrics().gauge(METRIC_OUTPUT_RECORDS,(int)reduceOutputRecords - previousReduceStats.get(METRIC_OUTPUT_RECORDS));
  context.getSystemReducerMetrics().gauge(METRIC_USED_CONTAINERS,runningReducers);
  context.getSystemReducerMetrics().gauge(METRIC_USED_MEMORY,runningReducers * memoryPerReducer);
  previousReduceStats.put(METRIC_INPUT_RECORDS,(int)reduceInputRecords);
  previousReduceStats.put(METRIC_OUTPUT_RECORDS,(int)reduceOutputRecords);
  LOG.trace(""String_Node_Str"",(int)(reduceProgress * 100),reduceInputRecords,reduceOutputRecords,runningReducers,runningReducers * memoryPerReducer);
}"
7683,"@Override public void init(String[] args){
  LOG.info(""String_Node_Str"");
  try {
    cConf=CConfiguration.create();
    String zookeeper=cConf.get(Constants.CFG_ZOOKEEPER_ENSEMBLE);
    if (zookeeper == null) {
      LOG.error(""String_Node_Str"");
      System.exit(1);
    }
    zkClientService=ZKClientServices.delegate(ZKClients.reWatchOnExpire(ZKClients.retryOnFailure(ZKClientService.Builder.of(zookeeper).build(),RetryStrategies.exponentialDelay(500,2000,TimeUnit.MILLISECONDS))));
    Injector injector=createGuiceInjector();
    weaveRunnerService=injector.getInstance(WeaveRunnerService.class);
    router=injector.getInstance(NettyRouter.class);
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    LOG.error(t.getMessage(),t);
    throw Throwables.propagate(t);
  }
}","@Override public void init(String[] args){
  LOG.info(""String_Node_Str"");
  try {
    CConfiguration cConf=CConfiguration.create();
    String zookeeper=cConf.get(Constants.CFG_ZOOKEEPER_ENSEMBLE);
    if (zookeeper == null) {
      LOG.error(""String_Node_Str"");
      System.exit(1);
    }
    zkClientService=ZKClientServices.delegate(ZKClients.reWatchOnExpire(ZKClients.retryOnFailure(ZKClientService.Builder.of(zookeeper).build(),RetryStrategies.exponentialDelay(500,2000,TimeUnit.MILLISECONDS))));
    Injector injector=createGuiceInjector(cConf,zkClientService);
    weaveRunnerService=injector.getInstance(WeaveRunnerService.class);
    router=injector.getInstance(NettyRouter.class);
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    LOG.error(t.getMessage(),t);
    throw Throwables.propagate(t);
  }
}"
7684,"private Injector createGuiceInjector(){
  return Guice.createInjector(new ConfigModule(cConf),new LocationRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule(zkClientService).getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(WeaveRunnerService.class).to(YarnWeaveRunnerService.class);
    }
    @Provides @Named(Constants.Router.ADDRESS) public final InetAddress providesHostname(    CConfiguration cConf){
      return Networks.resolve(cConf.get(Constants.Router.ADDRESS),new InetSocketAddress(""String_Node_Str"",0).getAddress());
    }
    @Singleton @Provides private YarnWeaveRunnerService provideYarnWeaveRunnerService(    CConfiguration configuration,    YarnConfiguration yarnConfiguration,    LocationFactory locationFactory){
      String zkNamespace=configuration.get(Constants.CFG_WEAVE_ZK_NAMESPACE,""String_Node_Str"");
      return new YarnWeaveRunnerService(yarnConfiguration,configuration.get(Constants.Zookeeper.QUORUM) + zkNamespace,LocationFactories.namespace(locationFactory,""String_Node_Str""));
    }
    @Provides public Iterable<WeaveRunner.LiveInfo> providesWeaveLiveInfo(    WeaveRunnerService weaveRunnerService){
      return weaveRunnerService.lookupLive();
    }
  }
);
}","static Injector createGuiceInjector(CConfiguration cConf,ZKClientService zkClientService){
  return Guice.createInjector(new ConfigModule(cConf),new LocationRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule(zkClientService).getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(WeaveRunnerService.class).to(YarnWeaveRunnerService.class);
      bind(new TypeLiteral<Iterable<WeaveRunner.LiveInfo>>(){
      }
).toProvider(WeaveLiveInfoProvider.class);
    }
    @Provides @Named(Constants.Router.ADDRESS) public final InetAddress providesHostname(    CConfiguration cConf){
      return Networks.resolve(cConf.get(Constants.Router.ADDRESS),new InetSocketAddress(""String_Node_Str"",0).getAddress());
    }
    @Singleton @Provides private YarnWeaveRunnerService provideYarnWeaveRunnerService(    CConfiguration configuration,    YarnConfiguration yarnConfiguration,    LocationFactory locationFactory){
      String zkNamespace=configuration.get(Constants.CFG_WEAVE_ZK_NAMESPACE,""String_Node_Str"");
      return new YarnWeaveRunnerService(yarnConfiguration,configuration.get(Constants.Zookeeper.QUORUM) + zkNamespace,LocationFactories.namespace(locationFactory,""String_Node_Str""));
    }
  }
);
}"
7685,"@Override protected void configure(){
  bind(WeaveRunnerService.class).to(YarnWeaveRunnerService.class);
}","@Override protected void configure(){
  bind(WeaveRunnerService.class).to(YarnWeaveRunnerService.class);
  bind(new TypeLiteral<Iterable<WeaveRunner.LiveInfo>>(){
  }
).toProvider(WeaveLiveInfoProvider.class);
}"
7686,"@Inject public RouterServiceLookup(final Iterable<WeaveRunner.LiveInfo> liveApps,final DiscoveryServiceClient discoveryServiceClient){
  this.discoverableCache=CacheBuilder.newBuilder().expireAfterAccess(1,TimeUnit.HOURS).build(new CacheLoader<String,EndpointStrategy>(){
    @Override public EndpointStrategy load(    String serviceName) throws Exception {
      for (      WeaveRunner.LiveInfo liveInfo : liveApps) {
        String appName=liveInfo.getApplicationName();
        LOG.debug(""String_Node_Str"",appName);
        if (appName.endsWith(""String_Node_Str"" + serviceName)) {
          String[] splits=Iterables.toArray(Splitter.on('.').split(appName),String.class);
          if (splits.length > 3) {
            serviceName=String.format(""String_Node_Str"",splits[1],splits[2],serviceName);
          }
        }
      }
      LOG.debug(""String_Node_Str"",serviceName);
      return new RandomEndpointStrategy(discoveryServiceClient.discover(serviceName));
    }
  }
);
}","@Inject public RouterServiceLookup(final DiscoveryServiceClient discoveryServiceClient,final Provider<Iterable<WeaveRunner.LiveInfo>> liveAppsProvider){
  this.discoverableCache=CacheBuilder.newBuilder().expireAfterAccess(1,TimeUnit.HOURS).build(new CacheLoader<String,EndpointStrategy>(){
    @Override public EndpointStrategy load(    String serviceName) throws Exception {
      for (      WeaveRunner.LiveInfo liveInfo : liveAppsProvider.get()) {
        String appName=liveInfo.getApplicationName();
        LOG.debug(""String_Node_Str"",appName);
        if (appName.endsWith(""String_Node_Str"" + serviceName)) {
          String[] splits=Iterables.toArray(Splitter.on('.').split(appName),String.class);
          if (splits.length > 3) {
            serviceName=String.format(""String_Node_Str"",splits[1],splits[2],serviceName);
          }
        }
      }
      LOG.debug(""String_Node_Str"",serviceName);
      return new RandomEndpointStrategy(discoveryServiceClient.discover(serviceName));
    }
  }
);
}"
7687,"@Override public EndpointStrategy load(String serviceName) throws Exception {
  for (  WeaveRunner.LiveInfo liveInfo : liveApps) {
    String appName=liveInfo.getApplicationName();
    LOG.debug(""String_Node_Str"",appName);
    if (appName.endsWith(""String_Node_Str"" + serviceName)) {
      String[] splits=Iterables.toArray(Splitter.on('.').split(appName),String.class);
      if (splits.length > 3) {
        serviceName=String.format(""String_Node_Str"",splits[1],splits[2],serviceName);
      }
    }
  }
  LOG.debug(""String_Node_Str"",serviceName);
  return new RandomEndpointStrategy(discoveryServiceClient.discover(serviceName));
}","@Override public EndpointStrategy load(String serviceName) throws Exception {
  for (  WeaveRunner.LiveInfo liveInfo : liveAppsProvider.get()) {
    String appName=liveInfo.getApplicationName();
    LOG.debug(""String_Node_Str"",appName);
    if (appName.endsWith(""String_Node_Str"" + serviceName)) {
      String[] splits=Iterables.toArray(Splitter.on('.').split(appName),String.class);
      if (splits.length > 3) {
        serviceName=String.format(""String_Node_Str"",splits[1],splits[2],serviceName);
      }
    }
  }
  LOG.debug(""String_Node_Str"",serviceName);
  return new RandomEndpointStrategy(discoveryServiceClient.discover(serviceName));
}"
7688,"private HttpResponse get(String url) throws Exception {
  DefaultHttpClient client=new DefaultHttpClient();
  HttpGet get=new HttpGet(url);
  return client.execute(get);
}","@Override public Iterable<WeaveRunner.LiveInfo> get(){
  return ImmutableSet.of();
}"
7689,"@Override public void initialize(DataSetSpecification spec){
  super.initialize(spec);
  track(getName(),""String_Node_Str"");
}","@Override public void initialize(DataSetSpecification spec,DataSetContext context){
  super.initialize(spec,context);
  track(getName(),""String_Node_Str"");
}"
7690,"public Scanner scan(byte[] startRow,byte[] stopRow,@Nullable FuzzyRowFilter filter,@Nullable byte[][] columns,@Nullable Transaction tx) throws IOException {
  DBIterator iterator=db.iterator();
  try {
    if (startRow != null) {
      iterator.seek(createStartKey(startRow));
    }
 else {
      iterator.seekToFirst();
    }
  }
 catch (  RuntimeException e) {
    try {
      iterator.close();
    }
 catch (    IOException ioe) {
      LOG.warn(""String_Node_Str"",ioe);
    }
    throw e;
  }
  byte[] endKey=stopRow == null ? null : createEndKey(stopRow);
  return new LevelDBScanner(iterator,endKey,filter,columns,tx);
}","public Scanner scan(byte[] startRow,byte[] stopRow,@Nullable FuzzyRowFilter filter,@Nullable byte[][] columns,@Nullable Transaction tx) throws IOException {
  DBIterator iterator=getDB().iterator();
  try {
    if (startRow != null) {
      iterator.seek(createStartKey(startRow));
    }
 else {
      iterator.seekToFirst();
    }
  }
 catch (  RuntimeException e) {
    try {
      iterator.close();
    }
 catch (    IOException ioe) {
      LOG.warn(""String_Node_Str"",ioe);
    }
    throw e;
  }
  byte[] endKey=stopRow == null ? null : createEndKey(stopRow);
  return new LevelDBScanner(iterator,endKey,filter,columns,tx);
}"
7691,"/** 
 * Delete a list of rows from the table entirely, disregarding transactions.
 * @param toDelete the row keys to delete
 */
public void deleteRows(Collection<byte[]> toDelete) throws IOException {
  if (toDelete.isEmpty()) {
    return;
  }
  Iterator<byte[]> rows=toDelete.iterator();
  byte[] currentRow=rows.next();
  byte[] startKey=createStartKey(currentRow);
  DBIterator iterator=db.iterator();
  WriteBatch batch=db.createWriteBatch();
  try {
    iterator.seek(startKey);
    if (!iterator.hasNext()) {
      return;
    }
    Map.Entry<byte[],byte[]> entry=iterator.next();
    while (entry != null && currentRow != null) {
      KeyValue kv=KeyValue.fromKey(entry.getKey());
      int comp=Bytes.compareTo(kv.getRow(),currentRow);
      if (comp == 0) {
        batch.delete(entry.getKey());
        entry=iterator.hasNext() ? iterator.next() : null;
      }
 else       if (comp > 0) {
        currentRow=rows.hasNext() ? rows.next() : null;
      }
 else       if (comp < 0) {
        iterator.seek(createStartKey(currentRow));
        entry=iterator.hasNext() ? iterator.next() : null;
      }
    }
  }
  finally {
    iterator.close();
  }
  db.write(batch,writeOptions);
}","/** 
 * Delete a list of rows from the table entirely, disregarding transactions.
 * @param toDelete the row keys to delete
 */
public void deleteRows(Collection<byte[]> toDelete) throws IOException {
  if (toDelete.isEmpty()) {
    return;
  }
  Iterator<byte[]> rows=toDelete.iterator();
  byte[] currentRow=rows.next();
  byte[] startKey=createStartKey(currentRow);
  DB db=getDB();
  DBIterator iterator=db.iterator();
  WriteBatch batch=db.createWriteBatch();
  try {
    iterator.seek(startKey);
    if (!iterator.hasNext()) {
      return;
    }
    Map.Entry<byte[],byte[]> entry=iterator.next();
    while (entry != null && currentRow != null) {
      KeyValue kv=KeyValue.fromKey(entry.getKey());
      int comp=Bytes.compareTo(kv.getRow(),currentRow);
      if (comp == 0) {
        batch.delete(entry.getKey());
        entry=iterator.hasNext() ? iterator.next() : null;
      }
 else       if (comp > 0) {
        currentRow=rows.hasNext() ? rows.next() : null;
      }
 else       if (comp < 0) {
        iterator.seek(createStartKey(currentRow));
        entry=iterator.hasNext() ? iterator.next() : null;
      }
    }
  }
  finally {
    iterator.close();
  }
  db.write(batch,getWriteOptions());
}"
7692,"public void undo(NavigableMap<byte[],NavigableMap<byte[],byte[]>> persisted,long version){
  if (persisted.isEmpty()) {
    return;
  }
  WriteBatch batch=db.createWriteBatch();
  for (  Map.Entry<byte[],NavigableMap<byte[],byte[]>> row : persisted.entrySet()) {
    for (    Map.Entry<byte[],byte[]> column : row.getValue().entrySet()) {
      byte[] key=createPutKey(row.getKey(),column.getKey(),version);
      batch.delete(key);
    }
  }
  db.write(batch,writeOptions);
}","public void undo(NavigableMap<byte[],NavigableMap<byte[],byte[]>> persisted,long version) throws IOException {
  if (persisted.isEmpty()) {
    return;
  }
  DB db=getDB();
  WriteBatch batch=db.createWriteBatch();
  for (  Map.Entry<byte[],NavigableMap<byte[],byte[]>> row : persisted.entrySet()) {
    for (    Map.Entry<byte[],byte[]> column : row.getValue().entrySet()) {
      byte[] key=createPutKey(row.getKey(),column.getKey(),version);
      batch.delete(key);
    }
  }
  db.write(batch,service.getWriteOptions());
}"
7693,"public void persist(NavigableMap<byte[],NavigableMap<byte[],byte[]>> changes,long version) throws IOException {
  WriteBatch batch=db.createWriteBatch();
  for (  Map.Entry<byte[],NavigableMap<byte[],byte[]>> row : changes.entrySet()) {
    for (    Map.Entry<byte[],byte[]> column : row.getValue().entrySet()) {
      byte[] key=createPutKey(row.getKey(),column.getKey(),version);
      batch.put(key,column.getValue() == null ? DELETE_MARKER : column.getValue());
    }
  }
  db.write(batch,writeOptions);
}","public void persist(NavigableMap<byte[],NavigableMap<byte[],byte[]>> changes,long version) throws IOException {
  DB db=getDB();
  WriteBatch batch=db.createWriteBatch();
  for (  Map.Entry<byte[],NavigableMap<byte[],byte[]>> row : changes.entrySet()) {
    for (    Map.Entry<byte[],byte[]> column : row.getValue().entrySet()) {
      byte[] key=createPutKey(row.getKey(),column.getKey(),version);
      batch.put(key,column.getValue() == null ? DELETE_MARKER : column.getValue());
    }
  }
  db.write(batch,service.getWriteOptions());
}"
7694,"public LevelDBOcTableCore(String tableName,LevelDBOcTableService service) throws IOException {
  this.db=service.getTable(tableName);
  this.writeOptions=service.getWriteOptions();
}","public LevelDBOcTableCore(String tableName,LevelDBOcTableService service) throws IOException {
  this.tableName=tableName;
  this.service=service;
}"
7695,"private void deleteColumn(byte[] row,byte[] column) throws IOException {
  WriteBatch batch=db.createWriteBatch();
  DBIterator iterator=db.iterator();
  byte[] endKey=createStartKey(row,Bytes.add(column,new byte[]{0}));
  try {
    iterator.seek(createStartKey(row,column));
    while (iterator.hasNext()) {
      Map.Entry<byte[],byte[]> entry=iterator.next();
      if (KeyValue.KEY_COMPARATOR.compare(entry.getKey(),endKey) >= 0) {
        break;
      }
      batch.delete(entry.getKey());
    }
    db.write(batch);
  }
  finally {
    iterator.close();
  }
}","private void deleteColumn(byte[] row,byte[] column) throws IOException {
  DB db=getDB();
  WriteBatch batch=db.createWriteBatch();
  DBIterator iterator=db.iterator();
  byte[] endKey=createStartKey(row,Bytes.add(column,new byte[]{0}));
  try {
    iterator.seek(createStartKey(row,column));
    while (iterator.hasNext()) {
      Map.Entry<byte[],byte[]> entry=iterator.next();
      if (KeyValue.KEY_COMPARATOR.compare(entry.getKey(),endKey) >= 0) {
        break;
      }
      batch.delete(entry.getKey());
    }
    db.write(batch);
  }
  finally {
    iterator.close();
  }
}"
7696,"@Override public boolean commitTx() throws Exception {
  queue.ack(dequeuedKeys,config);
  committed=true;
  return true;
}","@Override public boolean commitTx() throws Exception {
  getQueue().ack(dequeuedKeys,config);
  committed=true;
  return true;
}"
7697,"@Override public DequeueResult dequeue(int maxBatchSize) throws IOException {
  ImmutablePair<List<InMemoryQueue.Key>,List<byte[]>> result=queue.dequeue(currentTx,config,state,maxBatchSize);
  if (result == null) {
    return DequeueResult.EMPTY_RESULT;
  }
 else {
    dequeuedKeys=result.getFirst();
    return new InMemoryDequeueResult(result);
  }
}","@Override public DequeueResult dequeue(int maxBatchSize) throws IOException {
  ImmutablePair<List<InMemoryQueue.Key>,List<byte[]>> result=getQueue().dequeue(currentTx,config,state,maxBatchSize);
  if (result == null) {
    return DequeueResult.EMPTY_RESULT;
  }
 else {
    dequeuedKeys=result.getFirst();
    return new InMemoryDequeueResult(result);
  }
}"
7698,"public InMemoryQueue2Consumer(QueueName queueName,ConsumerConfig config,int numGroups,InMemoryQueueService queueService){
  this.queueName=queueName;
  this.queue=queueService.getQueue(queueName);
  this.config=config;
  this.numGroups=numGroups;
}","public InMemoryQueue2Consumer(QueueName queueName,ConsumerConfig config,int numGroups,InMemoryQueueService queueService){
  this.queueName=queueName;
  this.queueService=queueService;
  this.config=config;
  this.numGroups=numGroups;
}"
7699,"@Override public boolean rollbackTx() throws Exception {
  if (committed || DequeueStrategy.FIFO.equals(config.getDequeueStrategy())) {
    if (dequeuedKeys != null) {
      queue.undoDequeue(dequeuedKeys,config);
    }
  }
  dequeuedKeys=null;
  return true;
}","@Override public boolean rollbackTx() throws Exception {
  if (committed || DequeueStrategy.FIFO.equals(config.getDequeueStrategy())) {
    if (dequeuedKeys != null) {
      getQueue().undoDequeue(dequeuedKeys,config);
    }
  }
  dequeuedKeys=null;
  return true;
}"
7700,"@Override public void postTxCommit(){
  queue.evict(dequeuedKeys,numGroups);
}","@Override public void postTxCommit(){
  getQueue().evict(dequeuedKeys,numGroups);
}"
7701,"public InMemoryQueue2Producer(QueueName queueName,InMemoryQueueService queueService,QueueMetrics queueMetrics){
  super(queueMetrics,queueName);
  this.queue=queueService.getQueue(queueName);
}","public InMemoryQueue2Producer(QueueName queueName,InMemoryQueueService queueService,QueueMetrics queueMetrics){
  super(queueMetrics,queueName);
  this.queueName=queueName;
  this.queueService=queueService;
}"
7702,"@Override protected void doRollback(){
  if (commitTransaction != null) {
    for (int seqId=0; seqId < lastEnqueueCount; seqId++) {
      queue.undoEnqueue(commitTransaction.getWritePointer(),seqId);
    }
  }
}","@Override protected void doRollback(){
  if (commitTransaction != null) {
    InMemoryQueue queue=getQueue();
    for (int seqId=0; seqId < lastEnqueueCount; seqId++) {
      queue.undoEnqueue(commitTransaction.getWritePointer(),seqId);
    }
  }
}"
7703,"@Override protected int persist(Iterable<QueueEntry> entries,Transaction transaction) throws Exception {
  commitTransaction=transaction;
  int seqId=0;
  int bytes=0;
  for (  QueueEntry entry : entries) {
    queue.enqueue(transaction.getWritePointer(),seqId++,entry);
    bytes+=entry.getData().length;
  }
  lastEnqueueCount=seqId;
  return bytes;
}","@Override protected int persist(Iterable<QueueEntry> entries,Transaction transaction) throws Exception {
  commitTransaction=transaction;
  int seqId=0;
  int bytes=0;
  InMemoryQueue queue=getQueue();
  for (  QueueEntry entry : entries) {
    queue.enqueue(transaction.getWritePointer(),seqId++,entry);
    bytes+=entry.getData().length;
  }
  lastEnqueueCount=seqId;
  return bytes;
}"
7704,"@Test public void testReset() throws Exception {
  QueueName queueName=QueueName.fromFlowlet(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  createEnqueueRunnable(queueName,5,1,null).run();
  Queue2Consumer consumer1=queueClientFactory.createConsumer(queueName,new ConsumerConfig(0,0,1,DequeueStrategy.FIFO,null),2);
  TransactionContext txContext=createTxContext(consumer1);
  txContext.start();
  Assert.assertEquals(0,Bytes.toInt(consumer1.dequeue().iterator().next()));
  txContext.abort();
  System.out.println(""String_Node_Str"");
  queueAdmin.dropAll();
  System.out.println(""String_Node_Str"");
  Queue2Consumer consumer2=queueClientFactory.createConsumer(queueName,new ConsumerConfig(1,0,1,DequeueStrategy.FIFO,null),2);
  txContext=createTxContext(consumer2);
  txContext.start();
  Assert.assertFalse(consumer2.dequeue().iterator().hasNext());
  txContext.finish();
}","@Test public void testReset() throws Exception {
  QueueName queueName=QueueName.fromFlowlet(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Queue2Producer producer=queueClientFactory.createProducer(queueName);
  TransactionContext txContext=createTxContext(producer);
  txContext.start();
  producer.enqueue(new QueueEntry(Bytes.toBytes(0)));
  producer.enqueue(new QueueEntry(Bytes.toBytes(1)));
  producer.enqueue(new QueueEntry(Bytes.toBytes(2)));
  producer.enqueue(new QueueEntry(Bytes.toBytes(3)));
  producer.enqueue(new QueueEntry(Bytes.toBytes(4)));
  txContext.finish();
  Queue2Consumer consumer1=queueClientFactory.createConsumer(queueName,new ConsumerConfig(0,0,1,DequeueStrategy.FIFO,null),2);
  txContext=createTxContext(consumer1);
  txContext.start();
  Assert.assertEquals(0,Bytes.toInt(consumer1.dequeue().iterator().next()));
  txContext.finish();
  System.out.println(""String_Node_Str"");
  queueAdmin.dropAll();
  System.out.println(""String_Node_Str"");
  Queue2Consumer consumer2=queueClientFactory.createConsumer(queueName,new ConsumerConfig(1,0,1,DequeueStrategy.FIFO,null),2);
  txContext=createTxContext(consumer2);
  txContext.start();
  Assert.assertTrue(consumer2.dequeue().isEmpty());
  txContext.finish();
  if (this.getClass().getSimpleName().endsWith(""String_Node_Str"")) {
    return;
  }
  txContext=createTxContext(consumer1);
  txContext.start();
  Assert.assertTrue(consumer1.dequeue().isEmpty());
  txContext.finish();
  txContext=createTxContext(producer);
  txContext.start();
  producer.enqueue(new QueueEntry(Bytes.toBytes(5)));
  txContext.finish();
  txContext=createTxContext(consumer1);
  txContext.start();
  Assert.assertEquals(5,Bytes.toInt(consumer1.dequeue().iterator().next()));
  txContext.finish();
}"
7705,"public TimeValueAggregator(Collection<? extends Iterable<TimeValue>> timeValues){
  this.timeValues=ImmutableList.copyOf(timeValues);
}","public TimeValueAggregator(Collection<? extends Iterable<TimeValue>> timeValues,Interpolator interpolator){
  this.allTimeseries=ImmutableList.copyOf(timeValues);
  this.interpolator=interpolator;
}"
7706,"@Override public Iterator<TimeValue> iterator(){
  final List<PeekingIterator<TimeValue>> iterators=Lists.newLinkedList();
  for (  Iterable<TimeValue> timeValue : timeValues) {
    iterators.add(Iterators.peekingIterator(timeValue.iterator()));
  }
  return new AbstractIterator<TimeValue>(){
    @Override protected TimeValue computeNext(){
      long timestamp=Long.MAX_VALUE;
      boolean found=false;
      Iterator<PeekingIterator<TimeValue>> timeValuesItor=iterators.iterator();
      while (timeValuesItor.hasNext()) {
        PeekingIterator<TimeValue> iterator=timeValuesItor.next();
        if (!iterator.hasNext()) {
          timeValuesItor.remove();
          continue;
        }
        long ts=iterator.peek().getTime();
        if (ts <= timestamp) {
          timestamp=ts;
          found=true;
        }
      }
      if (!found) {
        return endOfData();
      }
      int value=0;
      timeValuesItor=iterators.iterator();
      while (timeValuesItor.hasNext()) {
        PeekingIterator<TimeValue> iterator=timeValuesItor.next();
        if (iterator.peek().getTime() == timestamp) {
          value+=iterator.next().getValue();
        }
      }
      return new TimeValue(timestamp,value);
    }
  }
;
}","@Override public Iterator<TimeValue> iterator(){
  return new InterpolatedAggregatorIterator();
}"
7707,"@Override protected TimeValue computeNext(){
  long timestamp=Long.MAX_VALUE;
  boolean found=false;
  Iterator<PeekingIterator<TimeValue>> timeValuesItor=iterators.iterator();
  while (timeValuesItor.hasNext()) {
    PeekingIterator<TimeValue> iterator=timeValuesItor.next();
    if (!iterator.hasNext()) {
      timeValuesItor.remove();
      continue;
    }
    long ts=iterator.peek().getTime();
    if (ts <= timestamp) {
      timestamp=ts;
      found=true;
    }
  }
  if (!found) {
    return endOfData();
  }
  int value=0;
  timeValuesItor=iterators.iterator();
  while (timeValuesItor.hasNext()) {
    PeekingIterator<TimeValue> iterator=timeValuesItor.next();
    if (iterator.peek().getTime() == timestamp) {
      value+=iterator.next().getValue();
    }
  }
  return new TimeValue(timestamp,value);
}","@Override protected TimeValue computeNext(){
  if (currentTs == Long.MAX_VALUE) {
    return endOfData();
  }
  boolean atEnd=true;
  int currentTsValue=0;
  Iterator<BiDirectionalPeekingIterator> timeseriesIter=timeseriesList.iterator();
  while (timeseriesIter.hasNext()) {
    BiDirectionalPeekingIterator timeseries=timeseriesIter.next();
    if (!timeseries.hasNext()) {
      timeseriesIter.remove();
      continue;
    }
    atEnd=false;
    if (timeseries.peek().getTime() == currentTs) {
      currentTsValue+=timeseries.peek().getValue();
      timeseries.next();
    }
 else     if (interpolator != null && timeseries.peekBefore() != null) {
      currentTsValue+=interpolator.interpolate(timeseries.peekBefore(),timeseries.peek(),currentTs);
    }
  }
  if (atEnd) {
    return endOfData();
  }
  TimeValue output=new TimeValue(currentTs,currentTsValue);
  currentTs=(interpolator == null) ? findEarliestTimestamp() : currentTs + 1;
  return output;
}"
7708,"@Test public void testHandlerException() throws Exception {
  HttpResponse response=doGet(""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.INTERNAL_SERVER_ERROR.getCode(),response.getStatusLine().getStatusCode());
  handlerHook2.awaitPost();
  Assert.assertEquals(1,handlerHook1.getNumPreCalls());
  Assert.assertEquals(1,handlerHook1.getNumPostCalls());
  Assert.assertEquals(1,handlerHook2.getNumPreCalls());
  Assert.assertEquals(1,handlerHook2.getNumPostCalls());
}","@Test public void testHandlerException() throws Exception {
  HttpResponse response=doGet(""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.INTERNAL_SERVER_ERROR.getCode(),response.getStatusLine().getStatusCode());
  awaitPostHook();
  Assert.assertEquals(1,handlerHook1.getNumPreCalls());
  Assert.assertEquals(1,handlerHook1.getNumPostCalls());
  Assert.assertEquals(1,handlerHook2.getNumPreCalls());
  Assert.assertEquals(1,handlerHook2.getNumPostCalls());
}"
7709,"@Test public void testHandlerHookCall() throws Exception {
  HttpResponse response=doGet(""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  handlerHook2.awaitPost();
  Assert.assertEquals(1,handlerHook1.getNumPreCalls());
  Assert.assertEquals(1,handlerHook1.getNumPostCalls());
  Assert.assertEquals(1,handlerHook2.getNumPreCalls());
  Assert.assertEquals(1,handlerHook2.getNumPostCalls());
}","@Test public void testHandlerHookCall() throws Exception {
  HttpResponse response=doGet(""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  awaitPostHook();
  Assert.assertEquals(1,handlerHook1.getNumPreCalls());
  Assert.assertEquals(1,handlerHook1.getNumPostCalls());
  Assert.assertEquals(1,handlerHook2.getNumPreCalls());
  Assert.assertEquals(1,handlerHook2.getNumPostCalls());
}"
7710,"@Override public void postCall(HttpRequest request,HttpResponseStatus status,HandlerInfo handlerInfo){
  try {
    ++numPostCalls;
    String header=request.getHeader(""String_Node_Str"");
    if (header != null && header.equals(""String_Node_Str"")) {
      throw new IllegalArgumentException(""String_Node_Str"");
    }
  }
  finally {
    postLatch.countDown();
  }
}","@Override public void postCall(HttpRequest request,HttpResponseStatus status,HandlerInfo handlerInfo){
  try {
    ++numPostCalls;
    String header=request.getHeader(""String_Node_Str"");
    if (header != null && header.equals(""String_Node_Str"")) {
      throw new IllegalArgumentException(""String_Node_Str"");
    }
  }
  finally {
    try {
      postBarrier.await();
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",e);
    }
  }
}"
7711,"@Test public void testPostException() throws Exception {
  HttpResponse response=doGet(""String_Node_Str"",new Header[]{new BasicHeader(""String_Node_Str"",""String_Node_Str"")});
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  handlerHook2.awaitPost();
  Assert.assertEquals(1,handlerHook1.getNumPreCalls());
  Assert.assertEquals(1,handlerHook1.getNumPostCalls());
  Assert.assertEquals(1,handlerHook2.getNumPreCalls());
  Assert.assertEquals(1,handlerHook2.getNumPostCalls());
}","@Test public void testPostException() throws Exception {
  HttpResponse response=doGet(""String_Node_Str"",new Header[]{new BasicHeader(""String_Node_Str"",""String_Node_Str"")});
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  awaitPostHook();
  Assert.assertEquals(1,handlerHook1.getNumPreCalls());
  Assert.assertEquals(1,handlerHook1.getNumPostCalls());
  Assert.assertEquals(1,handlerHook2.getNumPreCalls());
  Assert.assertEquals(1,handlerHook2.getNumPostCalls());
}"
7712,"public void awaitPost() throws Exception {
  postLatch.await();
}","public void awaitPost() throws Exception {
  postBarrier.await();
}"
7713,"private String getMetricContext(WeaveRunner.LiveInfo info){
  Matcher matcher=APP_NAME_PATTERN.matcher(info.getApplicationName());
  if (!matcher.matches()) {
    return null;
  }
  Type type=getType(matcher.group(1));
  if (type == null) {
    return null;
  }
  Id.Program programId=Id.Program.from(matcher.group(2),matcher.group(3),matcher.group(4));
  return Joiner.on(""String_Node_Str"").join(programId.getApplicationId(),TypeId.getMetricContextId(type),programId.getApplication().getId());
}","private String getMetricContext(WeaveRunner.LiveInfo info){
  Matcher matcher=APP_NAME_PATTERN.matcher(info.getApplicationName());
  if (!matcher.matches()) {
    return null;
  }
  Type type=getType(matcher.group(1));
  if (type == null) {
    return null;
  }
  Id.Program programId=Id.Program.from(matcher.group(2),matcher.group(3),matcher.group(4));
  return Joiner.on(""String_Node_Str"").join(programId.getApplicationId(),TypeId.getMetricContextId(type),programId.getId());
}"
7714,"@PUT @Path(""String_Node_Str"") public void create(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String destination){
  GatewayMetricsHelperWrapper helper=new GatewayMetricsHelperWrapper(new MetricsHelper(this.getClass(),cMetrics,Constants.Gateway.GATEWAY_PREFIX + NAME),gatewayMetrics);
  helper.setMethod(""String_Node_Str"");
  helper.setScope(destination);
  String accountId=authenticate(request,responder,helper);
  if (accountId == null) {
    return;
  }
  if (!isId(destination)) {
    LOG.trace(""String_Node_Str"",destination);
    responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
    helper.finish(BadRequest);
    return;
  }
  Stream stream=new Stream(destination);
  stream.setName(destination);
  try {
    Stream existingStream=metadataService.getStream(accountId,stream.getId());
    if (existingStream != null) {
      metadataService.createStream(accountId,stream);
    }
  }
 catch (  Exception e) {
    LOG.trace(""String_Node_Str"",destination,e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
    return;
  }
  responder.sendStatus(HttpResponseStatus.OK);
  helper.finish(Success);
}","@PUT @Path(""String_Node_Str"") public void create(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String destination){
  GatewayMetricsHelperWrapper helper=new GatewayMetricsHelperWrapper(new MetricsHelper(this.getClass(),cMetrics,Constants.Gateway.GATEWAY_PREFIX + NAME),gatewayMetrics);
  helper.setMethod(""String_Node_Str"");
  helper.setScope(destination);
  String accountId=authenticate(request,responder,helper);
  if (accountId == null) {
    return;
  }
  if (!isId(destination)) {
    LOG.trace(""String_Node_Str"",destination);
    responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
    helper.finish(BadRequest);
    return;
  }
  Stream stream=new Stream(destination);
  stream.setName(destination);
  try {
    Stream existingStream=metadataService.getStream(accountId,stream.getId());
    if (existingStream == null) {
      metadataService.createStream(accountId,stream);
    }
  }
 catch (  Exception e) {
    LOG.trace(""String_Node_Str"",destination,e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
    return;
  }
  responder.sendStatus(HttpResponseStatus.OK);
  helper.finish(Success);
}"
7715,"@Override public void run(){
  LOG.info(""String_Node_Str"");
  Futures.getUnchecked(Services.chainStart(zkClientService,kafkaClientService,metricsCollectionService));
  LOG.info(""String_Node_Str"" + name);
  controller=injector.getInstance(getProgramClass()).run(program,programOpts);
  final SettableFuture<ProgramController.State> state=SettableFuture.create();
  controller.addListener(new AbstractListener(){
    @Override public void stopped(){
      state.set(ProgramController.State.STOPPED);
    }
    @Override public void error(    Throwable cause){
      LOG.error(""String_Node_Str"",cause);
      state.set(ProgramController.State.ERROR);
    }
  }
,MoreExecutors.sameThreadExecutor());
  LOG.info(""String_Node_Str"",Futures.getUnchecked(state));
}","@Override public void run(){
  LOG.info(""String_Node_Str"");
  Futures.getUnchecked(Services.chainStart(zkClientService,kafkaClientService,metricsCollectionService));
  LOG.info(""String_Node_Str"",name);
  controller=injector.getInstance(getProgramClass()).run(program,programOpts);
  final SettableFuture<ProgramController.State> state=SettableFuture.create();
  controller.addListener(new AbstractListener(){
    @Override public void stopped(){
      state.set(ProgramController.State.STOPPED);
    }
    @Override public void error(    Throwable cause){
      LOG.error(""String_Node_Str"",cause);
      state.set(ProgramController.State.ERROR);
    }
  }
,MoreExecutors.sameThreadExecutor());
  LOG.info(""String_Node_Str"",Futures.getUnchecked(state));
}"
7716,"@Override public void stop(){
  try {
    LOG.info(""String_Node_Str"" + name);
    controller.stop().get();
    LOG.info(""String_Node_Str"" + name);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e,e);
    throw Throwables.propagate(e);
  }
 finally {
    LOG.info(""String_Node_Str"");
    Futures.getUnchecked(Services.chainStop(metricsCollectionService,kafkaClientService,zkClientService));
  }
}","@Override public void stop(){
  try {
    LOG.info(""String_Node_Str"",name);
    controller.stop().get();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e,e);
    throw Throwables.propagate(e);
  }
}"
7717,"public ExecutorThreadPool(){
  executor=Executors.newFixedThreadPool(MAX_THREAD_POOL_SIZE);
}","public ExecutorThreadPool(){
  executor=Executors.newFixedThreadPool(MAX_THREAD_POOL_SIZE,Threads.createDaemonThreadFactory(""String_Node_Str""));
}"
7718,"private void startListen(Service service){
  service.addListener(new ServiceListenerAdapter(){
    @Override public void running(){
      InetSocketAddress endpoint=driver.getServiceEndpoint();
      cancelAnnounce=serviceAnnouncer.announce(serviceName,endpoint.getPort());
      LOG.info(""String_Node_Str"",serviceName,endpoint);
      started();
    }
    @Override public void terminated(    Service.State from){
      cancelAnnounce.cancel();
      stop();
    }
    @Override public void failed(    Service.State from,    Throwable failure){
      cancelAnnounce.cancel();
      error(failure);
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
}","private void startListen(Service service){
  service.addListener(new ServiceListenerAdapter(){
    @Override public void running(){
      InetSocketAddress endpoint=driver.getServiceEndpoint();
      cancelAnnounce=serviceAnnouncer.announce(serviceName,endpoint.getPort());
      LOG.info(""String_Node_Str"",serviceName,endpoint);
      started();
    }
    @Override public void terminated(    Service.State from){
      LOG.info(""String_Node_Str"",from,serviceName);
      cancelAnnounce.cancel();
      LOG.info(""String_Node_Str"",serviceName);
      stop();
    }
    @Override public void failed(    Service.State from,    Throwable failure){
      LOG.info(""String_Node_Str"",from,serviceName,failure);
      cancelAnnounce.cancel();
      LOG.info(""String_Node_Str"",serviceName);
      error(failure);
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
}"
7719,"@Override public void failed(Service.State from,Throwable failure){
  cancelAnnounce.cancel();
  error(failure);
}","@Override public void failed(Service.State from,Throwable failure){
  LOG.info(""String_Node_Str"",from,serviceName,failure);
  cancelAnnounce.cancel();
  LOG.info(""String_Node_Str"",serviceName);
  error(failure);
}"
7720,"@Override public void terminated(Service.State from){
  cancelAnnounce.cancel();
  stop();
}","@Override public void terminated(Service.State from){
  LOG.info(""String_Node_Str"",from,serviceName);
  cancelAnnounce.cancel();
  LOG.info(""String_Node_Str"",serviceName);
  stop();
}"
7721,"/** 
 * Configures the AppFabricService pre-start.
 */
@Override protected void startUp() throws Exception {
  executor=Executors.newFixedThreadPool(THREAD_COUNT);
  schedulerService.start();
  InetSocketAddress socketAddress=new InetSocketAddress(hostname,port);
  InetAddress address=socketAddress.getAddress();
  if (address.isAnyLocalAddress()) {
    address=InetAddress.getLocalHost();
  }
  final InetSocketAddress finalSocketAddress=new InetSocketAddress(address,port);
  discoveryService.register(new Discoverable(){
    @Override public String getName(){
      return Constants.Service.APP_FABRIC;
    }
    @Override public InetSocketAddress getSocketAddress(){
      return finalSocketAddress;
    }
  }
);
  TThreadedSelectorServer.Args options=new TThreadedSelectorServer.Args(new TNonblockingServerSocket(socketAddress)).executorService(executor).processor(new AppFabricService.Processor<AppFabricService.Iface>(service)).workerThreads(THREAD_COUNT);
  options.maxReadBufferBytes=Constants.Thrift.DEFAULT_MAX_READ_BUFFER;
  server=new TThreadedSelectorServer(options);
}","/** 
 * Configures the AppFabricService pre-start.
 */
@Override protected void startUp() throws Exception {
  executor=Executors.newFixedThreadPool(THREAD_COUNT,Threads.createDaemonThreadFactory(""String_Node_Str""));
  schedulerService.start();
  InetSocketAddress socketAddress=new InetSocketAddress(hostname,port);
  InetAddress address=socketAddress.getAddress();
  if (address.isAnyLocalAddress()) {
    address=InetAddress.getLocalHost();
  }
  final InetSocketAddress finalSocketAddress=new InetSocketAddress(address,port);
  discoveryService.register(new Discoverable(){
    @Override public String getName(){
      return Constants.Service.APP_FABRIC;
    }
    @Override public InetSocketAddress getSocketAddress(){
      return finalSocketAddress;
    }
  }
);
  TThreadedSelectorServer.Args options=new TThreadedSelectorServer.Args(new TNonblockingServerSocket(socketAddress)).executorService(executor).processor(new AppFabricService.Processor<AppFabricService.Iface>(service)).workerThreads(THREAD_COUNT);
  options.maxReadBufferBytes=Constants.Thrift.DEFAULT_MAX_READ_BUFFER;
  server=new TThreadedSelectorServer(options);
}"
7722,"@Override public ProgramController run(Program program,ProgramOptions options){
  ApplicationSpecification appSpec=program.getSpecification();
  Preconditions.checkNotNull(appSpec,""String_Node_Str"");
  Type processorType=program.getType();
  Preconditions.checkNotNull(processorType,""String_Node_Str"");
  Preconditions.checkArgument(processorType == Type.WORKFLOW,""String_Node_Str"");
  WorkflowSpecification workflowSpec=appSpec.getWorkflows().get(program.getName());
  Preconditions.checkNotNull(workflowSpec,""String_Node_Str"",program.getName());
  LOG.info(""String_Node_Str"" + program.getName() + ""String_Node_Str""+ workflowSpec.getName());
  String runtimeArgs=new Gson().toJson(options.getUserArguments());
  WeavePreparer preparer=weaveRunner.prepare(new WorkflowWeaveApplication(program,workflowSpec,hConfFile,cConfFile)).addLogHandler(new PrinterLogHandler(new PrintWriter(System.out))).withArguments(workflowSpec.getName(),String.format(""String_Node_Str"",RunnableOptions.JAR),program.getJarLocation().getName()).withArguments(workflowSpec.getName(),String.format(""String_Node_Str"",RunnableOptions.RUNTIME_ARGS),runtimeArgs);
  WeaveController controller=preparer.start();
  ProgramResourceReporter resourceReporter=new DistributedResourceReporter(program,metricsCollectionService,controller);
  return new WorkflowWeaveProgramController(program.getName(),preparer.start(),resourceReporter).startListen();
}","@Override public ProgramController run(Program program,ProgramOptions options){
  ApplicationSpecification appSpec=program.getSpecification();
  Preconditions.checkNotNull(appSpec,""String_Node_Str"");
  Type processorType=program.getType();
  Preconditions.checkNotNull(processorType,""String_Node_Str"");
  Preconditions.checkArgument(processorType == Type.WORKFLOW,""String_Node_Str"");
  WorkflowSpecification workflowSpec=appSpec.getWorkflows().get(program.getName());
  Preconditions.checkNotNull(workflowSpec,""String_Node_Str"",program.getName());
  LOG.info(""String_Node_Str"" + program.getName() + ""String_Node_Str""+ workflowSpec.getName());
  String runtimeArgs=new Gson().toJson(options.getUserArguments());
  WeavePreparer preparer=weaveRunner.prepare(new WorkflowWeaveApplication(program,workflowSpec,hConfFile,cConfFile)).addLogHandler(new PrinterLogHandler(new PrintWriter(System.out))).withArguments(workflowSpec.getName(),String.format(""String_Node_Str"",RunnableOptions.JAR),program.getJarLocation().getName()).withArguments(workflowSpec.getName(),String.format(""String_Node_Str"",RunnableOptions.RUNTIME_ARGS),runtimeArgs);
  WeaveController controller=preparer.start();
  ProgramResourceReporter resourceReporter=new DistributedResourceReporter(program,metricsCollectionService,controller);
  return new WorkflowWeaveProgramController(program.getName(),controller,resourceReporter).startListen();
}"
7723,"/** 
 * Open the table in the data fabric, to ensure it exists and is accessible.
 * @throws com.continuuity.api.data.OperationException if something goes wrong
 */
public void open() throws OperationException {
  try {
    if (!ocTableManager.exists(this.getName())) {
      ocTableManager.create(this.getName());
    }
  }
 catch (  OperationException oe) {
    throw oe;
  }
catch (  Exception e) {
    throw new OperationException(StatusCode.INTERNAL_ERROR,""String_Node_Str"",e);
  }
}","/** 
 * Open the table in the data fabric, to ensure it exists and is accessible.
 * @throws OperationException if something goes wrong
 */
public void open() throws OperationException {
  try {
    if (!ocTableManager.exists(this.getName())) {
      ocTableManager.create(this.getName());
    }
  }
 catch (  OperationException oe) {
    throw oe;
  }
catch (  Exception e) {
    throw new OperationException(StatusCode.INTERNAL_ERROR,""String_Node_Str"",e);
  }
}"
7724,"/** 
 * Increments (atomically) the specified row and columns by the specified amounts
 * @param amounts amounts to increment columns by
 * @return values of counters after the increments are performed, never null
 */
Map<byte[],Long> increment(byte[] row,byte[][] columns,long[] amounts) throws Exception ;","/** 
 * Increments (atomically) the specified row and columns by the specified amounts.
 * @param amounts amounts to increment columns by
 * @return values of counters after the increments are performed, never null
 */
Map<byte[],Long> increment(byte[] row,byte[][] columns,long[] amounts) throws Exception ;"
7725,"/** 
 * @return changes made by current transaction to be used for conflicts detection before commit
 */
Collection<byte[]> getTxChanges();","/** 
 * @return changes made by current transaction to be used for conflicts detection before commit.
 */
Collection<byte[]> getTxChanges();"
7726,"/** 
 * Called when new transaction has started
 * @param tx transaction info
 */
void startTx(Transaction tx);","/** 
 * Called when new transaction has started.
 * @param tx transaction info
 */
void startTx(Transaction tx);"
7727,"private Injector createGuiceInjector(){
  return Guice.createInjector(new MetricsClientRuntimeModule(kafkaClientService).getDistributedModules(),new GatewayModules(cConf).getDistributedModules(),new DataFabricModules(cConf,hConf).getDistributedModules(),new ConfigModule(cConf),new IOModule(),new LocationRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule(zkClientService).getDistributedModules(),new LoggingModules().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetadataService.Iface.class).to(com.continuuity.metadata.MetadataService.class);
      bind(StoreFactory.class).to(MDSStoreFactory.class);
    }
  }
);
}","static Injector createGuiceInjector(KafkaClientService kafkaClientService,ZKClientService zkClientService,CConfiguration cConf,Configuration hConf){
  return Guice.createInjector(new MetricsClientRuntimeModule(kafkaClientService).getDistributedModules(),new GatewayModules(cConf).getDistributedModules(),new DataFabricModules(cConf,hConf).getDistributedModules(),new ConfigModule(cConf),new IOModule(),new LocationRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule(zkClientService).getDistributedModules(),new LoggingModules().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetadataService.Iface.class).to(com.continuuity.metadata.MetadataService.class);
      bind(StoreFactory.class).to(MDSStoreFactory.class);
    }
  }
);
}"
7728,"@Override public void initialize(WeaveContext context){
  super.initialize(context);
  runLatch=new CountDownLatch(1);
  name=context.getSpecification().getName();
  Map<String,String> configs=context.getSpecification().getConfigs();
  LOG.info(""String_Node_Str"" + name);
  try {
    cConf=CConfiguration.create();
    cConf.clear();
    cConf.addResource(new File(configs.get(""String_Node_Str"")).toURI().toURL());
    hConf=new Configuration();
    hConf.clear();
    hConf.addResource(new File(configs.get(""String_Node_Str"")).toURI().toURL());
    LOG.info(""String_Node_Str"" + context.getHost().getCanonicalHostName());
    cConf.set(Constants.Gateway.ADDRESS,context.getHost().getCanonicalHostName());
    cConf.setInt(Constants.Gateway.PORT,0);
    LOG.info(""String_Node_Str"",cConf);
    String zookeeper=cConf.get(CFG_ZOOKEEPER_ENSEMBLE);
    if (zookeeper == null) {
      LOG.error(""String_Node_Str"");
      throw new IllegalStateException(""String_Node_Str"");
    }
    zkClientService=ZKClientServices.delegate(ZKClients.reWatchOnExpire(ZKClients.retryOnFailure(ZKClientService.Builder.of(zookeeper).build(),RetryStrategies.exponentialDelay(500,2000,TimeUnit.MILLISECONDS))));
    String kafkaZKNamespace=cConf.get(KafkaConstants.ConfigKeys.ZOOKEEPER_NAMESPACE_CONFIG);
    kafkaClientService=new ZKKafkaClientService(kafkaZKNamespace == null ? zkClientService : ZKClients.namespace(zkClientService,""String_Node_Str"" + kafkaZKNamespace));
    Injector injector=createGuiceInjector();
    metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
    gateway=injector.getInstance(Gateway.class);
    LOG.info(""String_Node_Str"" + name);
  }
 catch (  Throwable t) {
    LOG.error(t.getMessage(),t);
    throw Throwables.propagate(t);
  }
}","@Override public void initialize(WeaveContext context){
  super.initialize(context);
  runLatch=new CountDownLatch(1);
  name=context.getSpecification().getName();
  Map<String,String> configs=context.getSpecification().getConfigs();
  LOG.info(""String_Node_Str"" + name);
  try {
    CConfiguration cConf=CConfiguration.create();
    cConf.clear();
    cConf.addResource(new File(configs.get(""String_Node_Str"")).toURI().toURL());
    Configuration hConf=new Configuration();
    hConf.clear();
    hConf.addResource(new File(configs.get(""String_Node_Str"")).toURI().toURL());
    LOG.info(""String_Node_Str"" + context.getHost().getCanonicalHostName());
    cConf.set(Constants.Gateway.ADDRESS,context.getHost().getCanonicalHostName());
    cConf.setInt(Constants.Gateway.PORT,0);
    LOG.info(""String_Node_Str"",cConf);
    String zookeeper=cConf.get(CFG_ZOOKEEPER_ENSEMBLE);
    if (zookeeper == null) {
      LOG.error(""String_Node_Str"");
      throw new IllegalStateException(""String_Node_Str"");
    }
    zkClientService=ZKClientServices.delegate(ZKClients.reWatchOnExpire(ZKClients.retryOnFailure(ZKClientService.Builder.of(zookeeper).build(),RetryStrategies.exponentialDelay(500,2000,TimeUnit.MILLISECONDS))));
    String kafkaZKNamespace=cConf.get(KafkaConstants.ConfigKeys.ZOOKEEPER_NAMESPACE_CONFIG);
    kafkaClientService=new ZKKafkaClientService(kafkaZKNamespace == null ? zkClientService : ZKClients.namespace(zkClientService,""String_Node_Str"" + kafkaZKNamespace));
    Injector injector=createGuiceInjector(kafkaClientService,zkClientService,cConf,hConf);
    metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
    gateway=injector.getInstance(Gateway.class);
    LOG.info(""String_Node_Str"" + name);
  }
 catch (  Throwable t) {
    LOG.error(t.getMessage(),t);
    throw Throwables.propagate(t);
  }
}"
7729,"@Override protected void bindTableHandle(){
  bind(TransactionOracle.class).to(NoopTransactionOracle.class).in(Scopes.SINGLETON);
  bind(OVCTableHandle.class).annotatedWith(MetricsAnnotation.class).to(HBaseFilterableOVCTableHandle.class).in(Scopes.SINGLETON);
}","@Override protected void bindTableHandle(){
  bind(OVCTableHandle.class).annotatedWith(MetricsAnnotation.class).to(HBaseFilterableOVCTableHandle.class).in(Scopes.SINGLETON);
}"
7730,"@Override public void create(String name) throws Exception {
  if (admin.tableExists(HBaseTableUtil.getHBaseTableName(name))) {
    return;
  }
  HTableDescriptor tableDescriptor=new HTableDescriptor(getHBaseTableName(name));
  HColumnDescriptor columnDescriptor=new HColumnDescriptor(DATA_COLUMN_FAMILY);
  columnDescriptor.setMaxVersions(100);
  columnDescriptor.setBloomFilterType(StoreFile.BloomType.ROW);
  tableDescriptor.addFamily(columnDescriptor);
  admin.createTable(tableDescriptor);
}","@Override public void create(String name) throws Exception {
  HBaseTableUtil.createTableIfNotExists(admin,HBaseTableUtil.getHBaseTableName(name),DATA_COLUMN_FAMILY);
}"
7731,"@Override public void run(){
  DefaultTransactionExecutor txExecutor=txExecutorFactory.createExecutor(Lists.newArrayList((TransactionAware)table));
  for (int k=0; k < 100; k++) {
    for (int i=0; i < ROWS_TO_APPEND_TO.length / 2; i++) {
      final byte[] row1=ROWS_TO_APPEND_TO[i * 2];
      final byte[] row2=ROWS_TO_APPEND_TO[i * 2 + 1];
      boolean appended=false;
      while (!appended) {
        try {
          txExecutor.execute(new TransactionExecutor.Subroutine(){
            @Override public void apply() throws Exception {
              appendColumn(row1);
              appendColumn(row2);
            }
            private void appendColumn(            byte[] row) throws Exception {
              OperationResult<Map<byte[],byte[]>> columns=table.get(row,null);
              int columnsCount=columns.isEmpty() ? 0 : columns.getValue().size();
              byte[] columnToAppend=Bytes.toBytes(""String_Node_Str"" + columnsCount);
              table.put(row,new byte[][]{columnToAppend},new byte[][]{Bytes.toBytes(""String_Node_Str"" + columnsCount)});
            }
          }
);
        }
 catch (        Throwable t) {
          LOG.warn(""String_Node_Str"",t);
          appended=false;
          continue;
        }
        appended=true;
      }
    }
  }
}","@Override public void run(){
  try {
    success.set(false);
    getTableManager().create(""String_Node_Str"");
    success.set(true);
  }
 catch (  Throwable throwable) {
    success.set(false);
    throwable.printStackTrace(System.err);
  }
}"
7732,"/** 
 * Constructor for a transaction executor.
 */
@Inject public DefaultTransactionExecutor(TransactionSystemClient txClient,@Assisted TransactionAware... txAwares){
  this.txAwares=ImmutableList.copyOf(txAwares);
  this.txClient=txClient;
}","/** 
 * Constructor for a transaction executor.
 */
@Inject public DefaultTransactionExecutor(TransactionSystemClient txClient,@Assisted Iterable<TransactionAware> txAwares){
  this.txAwares=ImmutableList.copyOf(txAwares);
  this.txClient=txClient;
}"
7733,"/** 
 * Deletes an application specified by appId
 */
@POST @Path(""String_Node_Str"") public void promoteApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId){
  try {
    String postBody=null;
    try {
      postBody=IOUtils.toString(new ChannelBufferInputStream(request.getContent()));
    }
 catch (    IOException e) {
      responder.sendError(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      return;
    }
    Map<String,String> o=null;
    try {
      o=new Gson().fromJson(postBody,new TypeToken<HashMap<String,String>>(){
      }
.getType());
    }
 catch (    JsonSyntaxException e) {
      responder.sendError(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (!o.containsKey(""String_Node_Str"")) {
      responder.sendError(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    try {
      InetAddress address=InetAddress.getByName(o.get(""String_Node_Str""));
    }
 catch (    UnknownHostException e) {
      responder.sendError(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    String accountId=getAuthenticatedAccountId(request);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Services.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      try {
        if (!client.promote(token,new ArchiveId(accountId,appId,""String_Node_Str"" + System.currentTimeMillis() + ""String_Node_Str""),o.get(""String_Node_Str""))) {
          responder.sendError(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + appId);
        }
 else {
          responder.sendStatus(HttpResponseStatus.OK);
        }
      }
 catch (      AppFabricServiceException e) {
        responder.sendError(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
        return;
      }
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}","/** 
 * Deletes an application specified by appId
 */
@POST @Path(""String_Node_Str"") public void promoteApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId){
  try {
    String postBody=null;
    try {
      postBody=IOUtils.toString(new ChannelBufferInputStream(request.getContent()));
    }
 catch (    IOException e) {
      responder.sendError(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      return;
    }
    Map<String,String> o=null;
    try {
      o=new Gson().fromJson(postBody,new TypeToken<HashMap<String,String>>(){
      }
.getType());
    }
 catch (    JsonSyntaxException e) {
      responder.sendError(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (!o.containsKey(""String_Node_Str"")) {
      responder.sendError(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    try {
      InetAddress address=InetAddress.getByName(o.get(""String_Node_Str""));
    }
 catch (    UnknownHostException e) {
      responder.sendError(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    String accountId=getAuthenticatedAccountId(request);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Constants.Service.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      try {
        if (!client.promote(token,new ArchiveId(accountId,appId,""String_Node_Str"" + System.currentTimeMillis() + ""String_Node_Str""),o.get(""String_Node_Str""))) {
          responder.sendError(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + appId);
        }
 else {
          responder.sendStatus(HttpResponseStatus.OK);
        }
      }
 catch (      AppFabricServiceException e) {
        responder.sendError(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
        return;
      }
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}"
7734,"/** 
 * Deploys an application.
 */
@PUT @Path(""String_Node_Str"") public void deploy(HttpRequest request,HttpResponder responder){
  try {
    String accountId=getAuthenticatedAccountId(request);
    String archiveName=request.getHeader(ARCHIVE_NAME_HEADER);
    if (archiveName == null || archiveName.isEmpty()) {
      responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
      return;
    }
    ChannelBuffer content=request.getContent();
    if (content == null) {
      responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
      return;
    }
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Services.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      ArchiveInfo rInfo=new ArchiveInfo(accountId,""String_Node_Str"",archiveName);
      ArchiveId rIdentifier=client.init(token,rInfo);
      while (content.readableBytes() > 0) {
        int bytesToRead=Math.min(1024 * 1024,content.readableBytes());
        client.chunk(token,rIdentifier,content.readSlice(bytesToRead).toByteBuffer());
      }
      client.deploy(token,rIdentifier);
      responder.sendStatus(HttpResponseStatus.OK);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}","/** 
 * Deploys an application.
 */
@PUT @Path(""String_Node_Str"") public void deploy(HttpRequest request,HttpResponder responder){
  try {
    String accountId=getAuthenticatedAccountId(request);
    String archiveName=request.getHeader(ARCHIVE_NAME_HEADER);
    if (archiveName == null || archiveName.isEmpty()) {
      responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
      return;
    }
    ChannelBuffer content=request.getContent();
    if (content == null) {
      responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
      return;
    }
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Constants.Service.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      ArchiveInfo rInfo=new ArchiveInfo(accountId,""String_Node_Str"",archiveName);
      ArchiveId rIdentifier=client.init(token,rInfo);
      while (content.readableBytes() > 0) {
        int bytesToRead=Math.min(1024 * 1024,content.readableBytes());
        client.chunk(token,rIdentifier,content.readSlice(bytesToRead).toByteBuffer());
      }
      client.deploy(token,rIdentifier);
      responder.sendStatus(HttpResponseStatus.OK);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}"
7735,"private void getHistory(HttpRequest request,HttpResponder responder,String appId,String id){
  try {
    String accountId=getAuthenticatedAccountId(request);
    TProtocol protocol=getThriftProtocol(Services.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      List<ProgramRunRecord> records=client.getHistory(new ProgramId(accountId,appId,id));
      JsonArray history=new JsonArray();
      for (      ProgramRunRecord record : records) {
        JsonObject object=new JsonObject();
        object.addProperty(""String_Node_Str"",record.getRunId());
        object.addProperty(""String_Node_Str"",record.getStartTime());
        object.addProperty(""String_Node_Str"",record.getEndTime());
        object.addProperty(""String_Node_Str"",record.getEndStatus());
        history.add(object);
      }
      responder.sendJson(HttpResponseStatus.OK,history);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}","private void getHistory(HttpRequest request,HttpResponder responder,String appId,String id){
  try {
    String accountId=getAuthenticatedAccountId(request);
    TProtocol protocol=getThriftProtocol(Constants.Service.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      List<ProgramRunRecord> records=client.getHistory(new ProgramId(accountId,appId,id));
      JsonArray history=new JsonArray();
      for (      ProgramRunRecord record : records) {
        JsonObject object=new JsonObject();
        object.addProperty(""String_Node_Str"",record.getRunId());
        object.addProperty(""String_Node_Str"",record.getStartTime());
        object.addProperty(""String_Node_Str"",record.getEndTime());
        object.addProperty(""String_Node_Str"",record.getEndStatus());
        history.add(object);
      }
      responder.sendJson(HttpResponseStatus.OK,history);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}"
7736,"private void runnableStartStop(HttpRequest request,HttpResponder responder,ProgramId id,String action){
  try {
    String accountId=getAuthenticatedAccountId(request);
    id.setAccountId(accountId);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Services.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      if (""String_Node_Str"".equals(action)) {
        client.start(token,new ProgramDescriptor(id,null));
      }
 else       if (""String_Node_Str"".equals(action)) {
        client.stop(token,id);
      }
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}","private void runnableStartStop(HttpRequest request,HttpResponder responder,ProgramId id,String action){
  try {
    String accountId=getAuthenticatedAccountId(request);
    id.setAccountId(accountId);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Constants.Service.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      if (""String_Node_Str"".equals(action)) {
        client.start(token,new ProgramDescriptor(id,null));
      }
 else       if (""String_Node_Str"".equals(action)) {
        client.stop(token,id);
      }
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}"
7737,"private void runnableStatus(HttpRequest request,HttpResponder responder,ProgramId id){
  try {
    String accountId=getAuthenticatedAccountId(request);
    id.setAccountId(accountId);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Services.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      ProgramStatus status=client.status(token,id);
      JsonObject o=new JsonObject();
      o.addProperty(""String_Node_Str"",status.getStatus());
      responder.sendJson(HttpResponseStatus.OK,o);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}","private void runnableStatus(HttpRequest request,HttpResponder responder,ProgramId id){
  try {
    String accountId=getAuthenticatedAccountId(request);
    id.setAccountId(accountId);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Constants.Service.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      ProgramStatus status=client.status(token,id);
      JsonObject o=new JsonObject();
      o.addProperty(""String_Node_Str"",status.getStatus());
      responder.sendJson(HttpResponseStatus.OK,o);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}"
7738,"/** 
 * Deletes an application specified by appId
 */
@DELETE @Path(""String_Node_Str"") public void deleteApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId){
  try {
    String accountId=getAuthenticatedAccountId(request);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Services.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      client.removeApplication(token,new ProgramId(accountId,appId,""String_Node_Str""));
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}","/** 
 * Deletes an application specified by appId
 */
@DELETE @Path(""String_Node_Str"") public void deleteApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId){
  try {
    String accountId=getAuthenticatedAccountId(request);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Constants.Service.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      client.removeApplication(token,new ProgramId(accountId,appId,""String_Node_Str""));
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}"
7739,"/** 
 * *DO NOT DOCUMENT THIS API
 */
@DELETE @Path(""String_Node_Str"") public void resetReactor(HttpRequest request,HttpResponder responder){
  try {
    if (!conf.getBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,Constants.Dangerous.DEFAULT_UNRECOVERABLE_RESET)) {
      responder.sendStatus(HttpResponseStatus.FORBIDDEN);
      return;
    }
    String accountId=getAuthenticatedAccountId(request);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Services.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      client.reset(token,accountId);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}","/** 
 * *DO NOT DOCUMENT THIS API
 */
@DELETE @Path(""String_Node_Str"") public void resetReactor(HttpRequest request,HttpResponder responder){
  try {
    if (!conf.getBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,Constants.Dangerous.DEFAULT_UNRECOVERABLE_RESET)) {
      responder.sendStatus(HttpResponseStatus.FORBIDDEN);
      return;
    }
    String accountId=getAuthenticatedAccountId(request);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Constants.Service.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      client.reset(token,accountId);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}"
7740,"@Inject public AppFabricServiceHandler(GatewayAuthenticator authenticator,CConfiguration conf,DiscoveryServiceClient discoveryClient){
  super(authenticator);
  this.discoveryClient=discoveryClient;
  this.conf=conf;
  this.endpointStrategy=new RandomEndpointStrategy(discoveryClient.discover(Services.APP_FABRIC));
}","@Inject public AppFabricServiceHandler(GatewayAuthenticator authenticator,CConfiguration conf,DiscoveryServiceClient discoveryClient){
  super(authenticator);
  this.discoveryClient=discoveryClient;
  this.conf=conf;
  this.endpointStrategy=new RandomEndpointStrategy(discoveryClient.discover(Constants.Service.APP_FABRIC));
}"
7741,"/** 
 * Increases number of instance for a flowlet within a flow.
 */
@PUT @Path(""String_Node_Str"") public void setFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String flowId,@PathParam(""String_Node_Str"") final String flowletId,@PathParam(""String_Node_Str"") final String instanceCount){
  short instances=0;
  try {
    Short count=Short.parseShort(instanceCount);
    instances=count.shortValue();
    if (instances < 1) {
      responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
      return;
    }
  }
 catch (  NumberFormatException e) {
    responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
    return;
  }
  try {
    String accountId=getAuthenticatedAccountId(request);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Services.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      client.setInstances(token,new ProgramId(accountId,appId,flowId),flowletId,instances);
      responder.sendStatus(HttpResponseStatus.OK);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}","/** 
 * Increases number of instance for a flowlet within a flow.
 */
@PUT @Path(""String_Node_Str"") public void setFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String flowId,@PathParam(""String_Node_Str"") final String flowletId,@PathParam(""String_Node_Str"") final String instanceCount){
  short instances=0;
  try {
    Short count=Short.parseShort(instanceCount);
    instances=count.shortValue();
    if (instances < 1) {
      responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
      return;
    }
  }
 catch (  NumberFormatException e) {
    responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
    return;
  }
  try {
    String accountId=getAuthenticatedAccountId(request);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Constants.Service.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      client.setInstances(token,new ProgramId(accountId,appId,flowId),flowletId,instances);
      responder.sendStatus(HttpResponseStatus.OK);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}"
7742,"/** 
 * Deletes all applications in the reactor.
 */
@DELETE @Path(""String_Node_Str"") public void deleteAllApps(HttpRequest request,HttpResponder responder){
  try {
    String accountId=getAuthenticatedAccountId(request);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Services.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      client.removeAll(token,accountId);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}","/** 
 * Deletes all applications in the reactor.
 */
@DELETE @Path(""String_Node_Str"") public void deleteAllApps(HttpRequest request,HttpResponder responder){
  try {
    String accountId=getAuthenticatedAccountId(request);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Constants.Service.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      client.removeAll(token,accountId);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}"
7743,"private void runnableSpecification(HttpRequest request,HttpResponder responder,ProgramId id){
  try {
    String accountId=getAuthenticatedAccountId(request);
    id.setAccountId(accountId);
    TProtocol protocol=getThriftProtocol(Services.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      String specification=client.getSpecification(id);
      responder.sendByteArray(HttpResponseStatus.OK,specification.getBytes(Charsets.UTF_8),ImmutableMultimap.of(HttpHeaders.Names.CONTENT_TYPE,""String_Node_Str""));
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}","private void runnableSpecification(HttpRequest request,HttpResponder responder,ProgramId id){
  try {
    String accountId=getAuthenticatedAccountId(request);
    id.setAccountId(accountId);
    TProtocol protocol=getThriftProtocol(Constants.Service.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      String specification=client.getSpecification(id);
      responder.sendByteArray(HttpResponseStatus.OK,specification.getBytes(Charsets.UTF_8),ImmutableMultimap.of(HttpHeaders.Names.CONTENT_TYPE,""String_Node_Str""));
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}"
7744,"/** 
 * Returns number of instances for a flowlet within a flow.
 */
@GET @Path(""String_Node_Str"") public void getFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String flowId,@PathParam(""String_Node_Str"") final String flowletId){
  try {
    String accountId=getAuthenticatedAccountId(request);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Services.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      int count=client.getInstances(token,new ProgramId(accountId,appId,flowId),flowletId);
      JsonObject o=new JsonObject();
      o.addProperty(""String_Node_Str"",count);
      responder.sendJson(HttpResponseStatus.OK,o);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}","/** 
 * Returns number of instances for a flowlet within a flow.
 */
@GET @Path(""String_Node_Str"") public void getFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String appId,@PathParam(""String_Node_Str"") final String flowId,@PathParam(""String_Node_Str"") final String flowletId){
  try {
    String accountId=getAuthenticatedAccountId(request);
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Constants.Service.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      int count=client.getInstances(token,new ProgramId(accountId,appId,flowId),flowletId);
      JsonObject o=new JsonObject();
      o.addProperty(""String_Node_Str"",count);
      responder.sendJson(HttpResponseStatus.OK,o);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}"
7745,"@Override protected void before() throws Throwable {
  conf.setInt(Constants.Gateway.PORT,0);
  conf.setInt(Constants.AppFabric.SERVER_PORT,0);
  conf.set(Constants.Gateway.ADDRESS,hostname);
  conf.set(Constants.AppFabric.OUTPUT_DIR,System.getProperty(""String_Node_Str""));
  conf.set(Constants.AppFabric.TEMP_DIR,System.getProperty(""String_Node_Str""));
  conf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  injector=Guice.createInjector(new GatewayModules(conf).getInMemoryModules(),new AppFabricTestModule(conf),new AbstractModule(){
    @Override protected void configure(){
      bind(LogReader.class).to(MockLogReader.class).in(Scopes.SINGLETON);
      bind(DataSetInstantiatorFromMetaData.class).in(Scopes.SINGLETON);
    }
  }
);
  discoveryService=injector.getInstance(DiscoveryService.class);
  gateway=injector.getInstance(Gateway.class);
  mds=injector.getInstance(MetadataService.Iface.class);
  injector.getInstance(InMemoryTransactionManager.class).init();
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  gateway.startAndWait();
  port=gateway.getBindAddress().getPort();
}","@Override protected void before() throws Throwable {
  conf.setInt(Constants.Gateway.PORT,0);
  conf.set(Constants.Gateway.ADDRESS,hostname);
  conf.set(Constants.AppFabric.OUTPUT_DIR,System.getProperty(""String_Node_Str""));
  conf.set(Constants.AppFabric.TEMP_DIR,System.getProperty(""String_Node_Str""));
  conf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  injector=Guice.createInjector(new GatewayModules(conf).getInMemoryModules(),new AppFabricTestModule(conf),new AbstractModule(){
    @Override protected void configure(){
      bind(LogReader.class).to(MockLogReader.class).in(Scopes.SINGLETON);
      bind(DataSetInstantiatorFromMetaData.class).in(Scopes.SINGLETON);
    }
  }
);
  discoveryService=injector.getInstance(DiscoveryService.class);
  gateway=injector.getInstance(Gateway.class);
  mds=injector.getInstance(MetadataService.Iface.class);
  injector.getInstance(InMemoryTransactionManager.class).init();
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  gateway.startAndWait();
  port=gateway.getBindAddress().getPort();
}"
7746,"/** 
 * Deploys an application.
 */
@PUT @Path(""String_Node_Str"") public void deploy(HttpRequest request,HttpResponder responder){
  try {
    String accountId=getAuthenticatedAccountId(request);
    String archiveName=request.getHeader(ARCHIVE_NAME_HEADER);
    if (archiveName == null || archiveName.isEmpty()) {
      responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
      return;
    }
    ChannelBuffer content=request.getContent();
    if (content == null) {
      responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
      return;
    }
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Services.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      ArchiveInfo rInfo=new ArchiveInfo(accountId,""String_Node_Str"",archiveName);
      ArchiveId rIdentifier=client.init(token,rInfo);
      while (content.readableBytes() > 0) {
        int bytesToRead=Math.min(1024 * 1024,content.readableBytes());
        client.chunk(token,rIdentifier,content.readSlice(bytesToRead).toByteBuffer());
      }
      client.deploy(token,rIdentifier);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}","/** 
 * Deploys an application.
 */
@PUT @Path(""String_Node_Str"") public void deploy(HttpRequest request,HttpResponder responder){
  try {
    String accountId=getAuthenticatedAccountId(request);
    String archiveName=request.getHeader(ARCHIVE_NAME_HEADER);
    if (archiveName == null || archiveName.isEmpty()) {
      responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
      return;
    }
    ChannelBuffer content=request.getContent();
    if (content == null) {
      responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
      return;
    }
    AuthToken token=new AuthToken(request.getHeader(GatewayAuthenticator.CONTINUUITY_API_KEY));
    TProtocol protocol=getThriftProtocol(Services.APP_FABRIC,endpointStrategy);
    AppFabricService.Client client=new AppFabricService.Client(protocol);
    try {
      ArchiveInfo rInfo=new ArchiveInfo(accountId,""String_Node_Str"",archiveName);
      ArchiveId rIdentifier=client.init(token,rInfo);
      while (content.readableBytes() > 0) {
        int bytesToRead=Math.min(1024 * 1024,content.readableBytes());
        client.chunk(token,rIdentifier,content.readSlice(bytesToRead).toByteBuffer());
      }
      client.deploy(token,rIdentifier);
      responder.sendStatus(HttpResponseStatus.OK);
    }
  finally {
      if (client.getInputProtocol().getTransport().isOpen()) {
        client.getInputProtocol().getTransport().close();
      }
      if (client.getOutputProtocol().getTransport().isOpen()) {
        client.getOutputProtocol().getTransport().close();
      }
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.FORBIDDEN);
  }
catch (  Exception e) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}"
7747,"/** 
 * The root of all goodness!
 * @param args Our cmdline arguments
 */
public static void main(String[] args){
  CConfiguration configuration=CConfiguration.create();
  boolean inMemory=false;
  String webAppPath=WebCloudAppService.WEB_APP;
  String webAppPath=WebCloudAppService.WEB_APP;
  if (args.length > 0) {
    if (""String_Node_Str"".equals(args[0]) || ""String_Node_Str"".equals(args[0])) {
      usage(false);
      return;
    }
 else     if (""String_Node_Str"".equals(args[0])) {
      inMemory=true;
    }
 else     if (""String_Node_Str"".equals(args[0])) {
      configuration.setBoolean(Constants.CFG_DATA_LEVELDB_ENABLED,false);
    }
 else     if (""String_Node_Str"".equals(args[0])) {
      webAppPath=args[1];
    }
 else {
      usage(true);
    }
  }
  Configuration hConf=new Configuration();
  hConf.addResource(""String_Node_Str"");
  hConf.reloadConfiguration();
  List<Module> modules=inMemory ? createInMemoryModules(configuration,hConf) : createPersistentModules(configuration,hConf);
  SingleNodeMain main=new SingleNodeMain(modules,configuration,webAppPath);
  try {
    main.startUp(args);
  }
 catch (  Exception e) {
    System.err.println(""String_Node_Str"" + e.getMessage());
    LOG.error(""String_Node_Str"",e);
    main.shutDown();
    System.exit(-2);
  }
}","/** 
 * The root of all goodness!
 * @param args Our cmdline arguments
 */
public static void main(String[] args){
  CConfiguration configuration=CConfiguration.create();
  boolean inMemory=false;
  String webAppPath=WebCloudAppService.WEB_APP;
  if (args.length > 0) {
    if (""String_Node_Str"".equals(args[0]) || ""String_Node_Str"".equals(args[0])) {
      usage(false);
      return;
    }
 else     if (""String_Node_Str"".equals(args[0])) {
      inMemory=true;
    }
 else     if (""String_Node_Str"".equals(args[0])) {
      configuration.setBoolean(Constants.CFG_DATA_LEVELDB_ENABLED,false);
    }
 else     if (""String_Node_Str"".equals(args[0])) {
      webAppPath=args[1];
    }
 else {
      usage(true);
    }
  }
  Configuration hConf=new Configuration();
  hConf.addResource(""String_Node_Str"");
  hConf.reloadConfiguration();
  List<Module> modules=inMemory ? createInMemoryModules(configuration,hConf) : createPersistentModules(configuration,hConf);
  SingleNodeMain main=new SingleNodeMain(modules,configuration,webAppPath);
  try {
    main.startUp(args);
  }
 catch (  Exception e) {
    System.err.println(""String_Node_Str"" + e.getMessage());
    LOG.error(""String_Node_Str"",e);
    main.shutDown();
    System.exit(-2);
  }
}"
7748,"@Override public ProcedureManager startProcedure(final String procedureName,Map<String,String> arguments){
  try {
    final FlowIdentifier procedureId=new FlowIdentifier(accountId,applicationId,procedureName,0);
    procedureId.setType(EntityType.PROCEDURE);
    Preconditions.checkState(runningProcessses.putIfAbsent(procedureName,procedureId) == null,""String_Node_Str"",procedureName);
    try {
      appFabricServer.start(token,new FlowDescriptor(procedureId,arguments));
    }
 catch (    Exception e) {
      runningProcessses.remove(procedureName);
      throw Throwables.propagate(e);
    }
    return new ProcedureManager(){
      @Override public void stop(){
        try {
          if (runningProcessses.remove(procedureName,procedureId)) {
            appFabricServer.stop(token,procedureId);
          }
        }
 catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
      @Override public ProcedureClient getClient(){
        return procedureClientFactory.create(accountId,applicationId,procedureName);
      }
    }
;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","@Override public ProcedureManager startProcedure(final String procedureName,Map<String,String> arguments){
  try {
    final ProgramId procedureId=new ProgramId(accountId,applicationId,procedureName);
    procedureId.setType(EntityType.PROCEDURE);
    Preconditions.checkState(runningProcessses.putIfAbsent(procedureName,procedureId) == null,""String_Node_Str"",procedureName);
    try {
      appFabricServer.start(token,new ProgramDescriptor(procedureId,arguments));
    }
 catch (    Exception e) {
      runningProcessses.remove(procedureName);
      throw Throwables.propagate(e);
    }
    return new ProcedureManager(){
      @Override public void stop(){
        try {
          if (runningProcessses.remove(procedureName,procedureId)) {
            appFabricServer.stop(token,procedureId);
          }
        }
 catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
      @Override public ProcedureClient getClient(){
        return procedureClientFactory.create(accountId,applicationId,procedureName);
      }
    }
;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
7749,"@Override public void stopAll(){
  try {
    for (    Map.Entry<String,FlowIdentifier> entry : Iterables.consumingIterable(runningProcessses.entrySet())) {
      if (isRunning(entry.getValue())) {
        appFabricServer.stop(token,entry.getValue());
      }
    }
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
 finally {
    RuntimeStats.clearStats(applicationId);
  }
}","@Override public void stopAll(){
  try {
    for (    Map.Entry<String,ProgramId> entry : Iterables.consumingIterable(runningProcessses.entrySet())) {
      if (isRunning(entry.getValue())) {
        appFabricServer.stop(token,entry.getValue());
      }
    }
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
 finally {
    RuntimeStats.clearStats(applicationId);
  }
}"
7750,"private boolean isRunning(FlowIdentifier flowId){
  try {
    FlowStatus status=appFabricServer.status(token,flowId);
    return ""String_Node_Str"".equals(status.getStatus());
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","private boolean isRunning(ProgramId flowId){
  try {
    ProgramStatus status=appFabricServer.status(token,flowId);
    return ""String_Node_Str"".equals(status.getStatus());
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
7751,"@Override public MapReduceManager startMapReduce(final String jobName,Map<String,String> arguments){
  try {
    final FlowIdentifier jobId=new FlowIdentifier(accountId,applicationId,jobName,0);
    jobId.setType(EntityType.MAPREDUCE);
    if (!isRunning(jobId)) {
      runningProcessses.remove(jobName);
    }
    Preconditions.checkState(runningProcessses.putIfAbsent(jobName,jobId) == null,""String_Node_Str"",jobName);
    try {
      appFabricServer.start(token,new FlowDescriptor(jobId,arguments));
    }
 catch (    Exception e) {
      runningProcessses.remove(jobName);
      throw Throwables.propagate(e);
    }
    return new MapReduceManager(){
      @Override public void stop(){
        try {
          if (runningProcessses.remove(jobName,jobId)) {
            appFabricServer.stop(token,jobId);
          }
        }
 catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
      @Override public void waitForFinish(      long timeout,      TimeUnit timeoutUnit) throws TimeoutException, InterruptedException {
        while (timeout > 0 && isRunning(jobId)) {
          timeoutUnit.sleep(1);
          timeout--;
        }
        if (timeout == 0 && isRunning(jobId)) {
          throw new TimeoutException(""String_Node_Str"");
        }
      }
    }
;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","@Override public MapReduceManager startMapReduce(final String jobName,Map<String,String> arguments){
  try {
    final ProgramId jobId=new ProgramId(accountId,applicationId,jobName);
    jobId.setType(EntityType.MAPREDUCE);
    if (!isRunning(jobId)) {
      runningProcessses.remove(jobName);
    }
    Preconditions.checkState(runningProcessses.putIfAbsent(jobName,jobId) == null,""String_Node_Str"",jobName);
    try {
      appFabricServer.start(token,new ProgramDescriptor(jobId,arguments));
    }
 catch (    Exception e) {
      runningProcessses.remove(jobName);
      throw Throwables.propagate(e);
    }
    return new MapReduceManager(){
      @Override public void stop(){
        try {
          if (runningProcessses.remove(jobName,jobId)) {
            appFabricServer.stop(token,jobId);
          }
        }
 catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
      @Override public void waitForFinish(      long timeout,      TimeUnit timeoutUnit) throws TimeoutException, InterruptedException {
        while (timeout > 0 && isRunning(jobId)) {
          timeoutUnit.sleep(1);
          timeout--;
        }
        if (timeout == 0 && isRunning(jobId)) {
          throw new TimeoutException(""String_Node_Str"");
        }
      }
    }
;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
7752,"@Override public FlowManager startFlow(final String flowName,Map<String,String> arguments){
  try {
    final FlowIdentifier flowId=new FlowIdentifier(accountId,applicationId,flowName,0);
    Preconditions.checkState(runningProcessses.putIfAbsent(flowName,flowId) == null,""String_Node_Str"",flowName);
    try {
      appFabricServer.start(token,new FlowDescriptor(flowId,arguments));
    }
 catch (    Exception e) {
      runningProcessses.remove(flowName);
      throw Throwables.propagate(e);
    }
    return new FlowManager(){
      @Override public void setFlowletInstances(      String flowletName,      int instances){
        Preconditions.checkArgument(instances > 0,""String_Node_Str"");
        try {
          appFabricServer.setInstances(token,flowId,flowletName,(short)instances);
        }
 catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
      @Override public void stop(){
        try {
          if (runningProcessses.remove(flowName,flowId)) {
            appFabricServer.stop(token,flowId);
          }
        }
 catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
    }
;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","@Override public FlowManager startFlow(final String flowName,Map<String,String> arguments){
  try {
    final ProgramId flowId=new ProgramId(accountId,applicationId,flowName);
    Preconditions.checkState(runningProcessses.putIfAbsent(flowName,flowId) == null,""String_Node_Str"",flowName);
    try {
      appFabricServer.start(token,new ProgramDescriptor(flowId,arguments));
    }
 catch (    Exception e) {
      runningProcessses.remove(flowName);
      throw Throwables.propagate(e);
    }
    return new FlowManager(){
      @Override public void setFlowletInstances(      String flowletName,      int instances){
        Preconditions.checkArgument(instances > 0,""String_Node_Str"");
        try {
          appFabricServer.setInstances(token,flowId,flowletName,(short)instances);
        }
 catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
      @Override public void stop(){
        try {
          if (runningProcessses.remove(flowName,flowId)) {
            appFabricServer.stop(token,flowId);
          }
        }
 catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
    }
;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
7753,"@POST @Path(""String_Node_Str"") public void procedureCall(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String procedureName,@PathParam(""String_Node_Str"") String methodName){
  Request postRequest;
  try {
    String accountId=getAuthenticatedAccountId(request);
    String serviceName=String.format(""String_Node_Str"",accountId,appId,procedureName);
    List<Discoverable> endpoints=Lists.newArrayList(discoveryServiceClient.discover(serviceName));
    if (endpoints.isEmpty()) {
      LOG.trace(""String_Node_Str"",serviceName);
      responder.sendStatus(NOT_FOUND);
      return;
    }
    Collections.shuffle(endpoints);
    InetSocketAddress endpoint=endpoints.get(0).getSocketAddress();
    String relayUri=Joiner.on('/').appendTo(new StringBuilder(""String_Node_Str"").append(endpoint.getHostName()).append(""String_Node_Str"").append(endpoint.getPort()).append(""String_Node_Str""),""String_Node_Str"",appId,""String_Node_Str"",procedureName,methodName).toString();
    LOG.trace(""String_Node_Str"" + relayUri);
    RequestBuilder requestBuilder=new RequestBuilder(""String_Node_Str"");
    postRequest=requestBuilder.setUrl(relayUri).setBody(request.getContent().array()).build();
    final Request relayRequest=postRequest;
    asyncHttpClient.executeRequest(postRequest,new AsyncCompletionHandler<Void>(){
      @Override public Void onCompleted(      Response response) throws Exception {
        if (response.getStatusCode() == OK.getCode()) {
          String contentType=response.getContentType();
          ChannelBuffer content;
          int contentLength=Integer.parseInt(response.getHeader(CONTENT_LENGTH));
          if (contentLength > 0) {
            content=ChannelBuffers.dynamicBuffer(contentLength);
          }
 else {
            content=ChannelBuffers.dynamicBuffer();
          }
          InputStream input=response.getResponseBodyAsStream();
          ByteStreams.copy(input,new ChannelBufferOutputStream(content));
          responder.sendContent(OK,ChannelBuffers.wrappedBuffer(response.getResponseBodyAsByteBuffer()),contentType,ImmutableListMultimap.<String,String>of());
        }
 else {
          responder.sendStatus(HttpResponseStatus.valueOf(response.getStatusCode()));
        }
        return null;
      }
      @Override public void onThrowable(      Throwable t){
        LOG.trace(""String_Node_Str"",relayRequest,t);
        responder.sendStatus(INTERNAL_SERVER_ERROR);
      }
    }
);
  }
 catch (  SecurityException e) {
    responder.sendStatus(FORBIDDEN);
  }
catch (  IllegalArgumentException e) {
    responder.sendStatus(BAD_REQUEST);
  }
catch (  Throwable e) {
    responder.sendStatus(INTERNAL_SERVER_ERROR);
  }
}","@POST @Path(""String_Node_Str"") public void procedureCall(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String procedureName,@PathParam(""String_Node_Str"") String methodName){
  Request postRequest;
  try {
    String accountId=getAuthenticatedAccountId(request);
    String serviceName=String.format(""String_Node_Str"",accountId,appId,procedureName);
    List<Discoverable> endpoints=Lists.newArrayList(discoveryServiceClient.discover(serviceName));
    if (endpoints.isEmpty()) {
      LOG.trace(""String_Node_Str"",serviceName);
      responder.sendStatus(NOT_FOUND);
      return;
    }
    Collections.shuffle(endpoints);
    InetSocketAddress endpoint=endpoints.get(0).getSocketAddress();
    String relayUri=Joiner.on('/').appendTo(new StringBuilder(""String_Node_Str"").append(endpoint.getHostName()).append(""String_Node_Str"").append(endpoint.getPort()).append(""String_Node_Str""),""String_Node_Str"",appId,""String_Node_Str"",procedureName,methodName).toString();
    LOG.trace(""String_Node_Str"" + relayUri);
    RequestBuilder requestBuilder=new RequestBuilder(""String_Node_Str"");
    postRequest=requestBuilder.setUrl(relayUri).setBody(request.getContent().array()).build();
    final Request relayRequest=postRequest;
    asyncHttpClient.executeRequest(postRequest,new AsyncCompletionHandler<Void>(){
      @Override public Void onCompleted(      Response response) throws Exception {
        if (response.getStatusCode() == OK.getCode()) {
          String contentType=response.getContentType();
          ChannelBuffer content;
          int contentLength=getContentLength(response);
          if (contentLength > 0) {
            content=ChannelBuffers.dynamicBuffer(contentLength);
          }
 else {
            content=ChannelBuffers.dynamicBuffer();
          }
          InputStream input=response.getResponseBodyAsStream();
          ByteStreams.copy(input,new ChannelBufferOutputStream(content));
          responder.sendContent(OK,content,contentType,ImmutableListMultimap.<String,String>of());
        }
 else {
          responder.sendStatus(HttpResponseStatus.valueOf(response.getStatusCode()));
        }
        return null;
      }
      @Override public void onThrowable(      Throwable t){
        LOG.trace(""String_Node_Str"",relayRequest,t);
        responder.sendStatus(INTERNAL_SERVER_ERROR);
      }
    }
);
  }
 catch (  SecurityException e) {
    responder.sendStatus(FORBIDDEN);
  }
catch (  IllegalArgumentException e) {
    responder.sendStatus(BAD_REQUEST);
  }
catch (  Throwable e) {
    responder.sendStatus(INTERNAL_SERVER_ERROR);
  }
}"
7754,"@Override public Void onCompleted(Response response) throws Exception {
  if (response.getStatusCode() == OK.getCode()) {
    String contentType=response.getContentType();
    ChannelBuffer content;
    int contentLength=Integer.parseInt(response.getHeader(CONTENT_LENGTH));
    if (contentLength > 0) {
      content=ChannelBuffers.dynamicBuffer(contentLength);
    }
 else {
      content=ChannelBuffers.dynamicBuffer();
    }
    InputStream input=response.getResponseBodyAsStream();
    ByteStreams.copy(input,new ChannelBufferOutputStream(content));
    responder.sendContent(OK,ChannelBuffers.wrappedBuffer(response.getResponseBodyAsByteBuffer()),contentType,ImmutableListMultimap.<String,String>of());
  }
 else {
    responder.sendStatus(HttpResponseStatus.valueOf(response.getStatusCode()));
  }
  return null;
}","@Override public Void onCompleted(Response response) throws Exception {
  if (response.getStatusCode() == OK.getCode()) {
    String contentType=response.getContentType();
    ChannelBuffer content;
    int contentLength=getContentLength(response);
    if (contentLength > 0) {
      content=ChannelBuffers.dynamicBuffer(contentLength);
    }
 else {
      content=ChannelBuffers.dynamicBuffer();
    }
    InputStream input=response.getResponseBodyAsStream();
    ByteStreams.copy(input,new ChannelBufferOutputStream(content));
    responder.sendContent(OK,content,contentType,ImmutableListMultimap.<String,String>of());
  }
 else {
    responder.sendStatus(HttpResponseStatus.valueOf(response.getStatusCode()));
  }
  return null;
}"
7755,"@Test public void testChunkedProcedureCall() throws Exception {
  Map<String,String> content=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Type type=new TypeToken<Map<String,String>>(){
  }
.getType();
  Gson gson=new Gson();
  String contentStr=gson.toJson(content,type);
  Assert.assertNotNull(contentStr);
  Assert.assertFalse(contentStr.isEmpty());
  HttpResponse response=sendPost(""String_Node_Str"",contentStr);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  String expected=contentStr + contentStr;
  Assert.assertEquals(expected,EntityUtils.toString(response.getEntity()));
}","@Test public void testChunkedProcedureCall() throws Exception {
  Map<String,String> content=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Type type=new TypeToken<Map<String,String>>(){
  }
.getType();
  Gson gson=new Gson();
  String contentStr=gson.toJson(content,type);
  Assert.assertNotNull(contentStr);
  Assert.assertFalse(contentStr.isEmpty());
  HttpResponse response=POST(""String_Node_Str"",contentStr);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  String expected=contentStr + contentStr;
  String responseStr=EntityUtils.toString(response.getEntity());
  Assert.assertEquals(expected,responseStr);
}"
7756,"@Test public void testErrorProcedureCall() throws Exception {
  Map<String,String> content=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  HttpResponse response=sendPost(""String_Node_Str"",new Gson().toJson(content,new TypeToken<Map<String,String>>(){
  }
.getType()));
  Assert.assertEquals(HttpResponseStatus.INTERNAL_SERVER_ERROR.getCode(),response.getStatusLine().getStatusCode());
}","@Test public void testErrorProcedureCall() throws Exception {
  Map<String,String> content=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  HttpResponse response=POST(""String_Node_Str"",new Gson().toJson(content,new TypeToken<Map<String,String>>(){
  }
.getType()));
  Assert.assertEquals(HttpResponseStatus.INTERNAL_SERVER_ERROR.getCode(),response.getStatusLine().getStatusCode());
}"
7757,"@Test public void testProcedureCall() throws Exception {
  Map<String,String> content=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Type type=new TypeToken<Map<String,String>>(){
  }
.getType();
  Gson gson=new Gson();
  String contentStr=gson.toJson(content,type);
  Assert.assertNotNull(contentStr);
  Assert.assertFalse(contentStr.isEmpty());
  HttpResponse response=sendPost(""String_Node_Str"",contentStr);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  Assert.assertEquals(content,gson.fromJson(EntityUtils.toString(response.getEntity()),type));
}","@Test public void testProcedureCall() throws Exception {
  Map<String,String> content=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Type type=new TypeToken<Map<String,String>>(){
  }
.getType();
  Gson gson=new Gson();
  String contentStr=gson.toJson(content,type);
  Assert.assertNotNull(contentStr);
  Assert.assertFalse(contentStr.isEmpty());
  HttpResponse response=POST(""String_Node_Str"",contentStr);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  String responseStr=EntityUtils.toString(response.getEntity());
  Assert.assertEquals(content,gson.fromJson(responseStr,type));
}"
7758,"@POST @Path(""String_Node_Str"") public void handle(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String procedureName,@PathParam(""String_Node_Str"") String methodName){
  if (""String_Node_Str"".equals(appId) && ""String_Node_Str"".equals(procedureName) && ""String_Node_Str"".equals(methodName)) {
    responder.sendByteArray(HttpResponseStatus.OK,request.getContent().array(),ImmutableMultimap.of(CONTENT_TYPE,""String_Node_Str"",CONTENT_LENGTH,Integer.toString(request.getContent().array().length)));
  }
 else   if (""String_Node_Str"".equals(appId) && ""String_Node_Str"".equals(procedureName) && ""String_Node_Str"".equals(methodName)) {
    responder.sendChunkStart(HttpResponseStatus.OK,ImmutableMultimap.of(CONTENT_TYPE,""String_Node_Str""));
    responder.sendChunk(ChannelBuffers.wrappedBuffer(request.getContent().array()));
    responder.sendChunk(ChannelBuffers.wrappedBuffer(request.getContent().array()));
    responder.sendChunkEnd();
  }
 else   if (""String_Node_Str"".equals(appId) && ""String_Node_Str"".equals(procedureName) && ""String_Node_Str"".equals(methodName)) {
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
 else {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}","@POST @Path(""String_Node_Str"") public void handle(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String procedureName,@PathParam(""String_Node_Str"") String methodName){
  if (""String_Node_Str"".equals(appId) && ""String_Node_Str"".equals(procedureName) && ""String_Node_Str"".equals(methodName)) {
    byte[] content=request.getContent().array();
    responder.sendByteArray(HttpResponseStatus.OK,content,ImmutableMultimap.of(CONTENT_TYPE,""String_Node_Str"",CONTENT_LENGTH,Integer.toString(content.length)));
  }
 else   if (""String_Node_Str"".equals(appId) && ""String_Node_Str"".equals(procedureName) && ""String_Node_Str"".equals(methodName)) {
    responder.sendChunkStart(HttpResponseStatus.OK,ImmutableMultimap.of(CONTENT_TYPE,""String_Node_Str""));
    responder.sendChunk(ChannelBuffers.wrappedBuffer(request.getContent().array()));
    responder.sendChunk(ChannelBuffers.wrappedBuffer(request.getContent().array()));
    responder.sendChunkEnd();
  }
 else   if (""String_Node_Str"".equals(appId) && ""String_Node_Str"".equals(procedureName) && ""String_Node_Str"".equals(methodName)) {
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
 else {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
}"
7759,"@Test public void testNoProcedureCall() throws Exception {
  Map<String,String> content=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  HttpResponse response=sendPost(""String_Node_Str"",new Gson().toJson(content,new TypeToken<Map<String,String>>(){
  }
.getType()));
  Assert.assertEquals(HttpResponseStatus.NOT_FOUND.getCode(),response.getStatusLine().getStatusCode());
}","@Test public void testNoProcedureCall() throws Exception {
  Map<String,String> content=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  HttpResponse response=POST(""String_Node_Str"",new Gson().toJson(content,new TypeToken<Map<String,String>>(){
  }
.getType()));
  Assert.assertEquals(HttpResponseStatus.NOT_FOUND.getCode(),response.getStatusLine().getStatusCode());
}"
7760,"/** 
 * Sets stream URI
 * @param flowlet name of the flowlet
 * @param stream  name of the stream
 * @param uri     URI associated with the stream.
 */
@Override public void setStreamURI(String flowlet,String stream,URI uri,StreamType type){
  if (flowletStreams.containsKey(flowlet)) {
    if (!flowletStreams.get(flowlet).containsKey(stream)) {
      flowletStreams.get(flowlet).put(stream,new ImmutablePair<URI,StreamType>(uri,type));
    }
  }
 else {
    Map<String,ImmutablePair<URI,StreamType>> maps=Maps.newHashMap();
    maps.put(stream,new ImmutablePair<URI,StreamType>(uri,type));
    flowletStreams.put(flowlet,maps);
  }
}","/** 
 * Sets stream URI
 * @param flowlet name of the flowlet
 * @param stream  name of the stream
 * @param uri     URI associated with the stream.
 */
@Override public void setStreamURI(String flowlet,String stream,URI uri,StreamType type){
  if (flowletStreams.containsKey(flowlet)) {
    flowletStreams.get(flowlet).put(stream + ""String_Node_Str"" + type,new ImmutablePair<URI,StreamType>(uri,type));
  }
 else {
    Map<String,ImmutablePair<URI,StreamType>> maps=Maps.newHashMap();
    maps.put(stream + ""String_Node_Str"" + type,new ImmutablePair<URI,StreamType>(uri,type));
    flowletStreams.put(flowlet,maps);
  }
}"
7761,"@Test public void testTimeSeriesRecordsCount() throws Exception {
  final ApplicationWithPrograms app=TestHelper.deployApplicationWithManager(AppWithMapReduce.class);
  OperationExecutor opex=injector.getInstance(OperationExecutor.class);
  LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
  DataSetAccessor dataSetAccessor=injector.getInstance(DataSetAccessor.class);
  TransactionSystemClient txSystemClient=injector.getInstance(TransactionSystemClient.class);
  OperationContext opCtx=new OperationContext(DefaultId.ACCOUNT.getId(),app.getAppSpecLoc().getSpecification().getName());
  TransactionProxy proxy=new TransactionProxy();
  DataSetInstantiator dataSetInstantiator=new DataSetInstantiator(new DataFabricImpl(opex,locationFactory,dataSetAccessor,opCtx),proxy,getClass().getClassLoader());
  dataSetInstantiator.setDataSets(ImmutableList.copyOf(new AppWithMapReduce().configure().getDataSets().values()));
  TransactionAgent txAgent=new SynchronousTransactionAgent(opex,opCtx,dataSetInstantiator.getTransactionAware(),txSystemClient);
  proxy.setTransactionAgent(txAgent);
  TimeseriesTable table=(TimeseriesTable)dataSetInstantiator.getDataSet(""String_Node_Str"");
  txAgent.start();
  fillTestInputData(table);
  txAgent.finish();
  Thread.sleep(2);
  long start=System.currentTimeMillis();
  runProgram(app,AppWithMapReduce.AggregateTimeseriesByTag.class);
  long stop=System.currentTimeMillis();
  Map<String,Long> expected=Maps.newHashMap();
  expected.put(""String_Node_Str"",18L);
  expected.put(""String_Node_Str"",3L);
  expected.put(""String_Node_Str"",18L);
  table=(TimeseriesTable)dataSetInstantiator.getDataSet(""String_Node_Str"");
  txAgent.start();
  List<TimeseriesTable.Entry> agg=table.read(AggregateMetricsByTag.BY_TAGS,start,stop);
  Assert.assertEquals(expected.size(),agg.size());
  for (  TimeseriesTable.Entry entry : agg) {
    String tag=Bytes.toString(entry.getTags()[0]);
    Assert.assertEquals((long)expected.get(tag),Bytes.toLong(entry.getValue()));
  }
  txAgent.finish();
}","@Test public void testTimeSeriesRecordsCount() throws Exception {
  final ApplicationWithPrograms app=TestHelper.deployApplicationWithManager(AppWithMapReduce.class);
  injector.getInstance(InMemoryTransactionManager.class).init();
  OperationExecutor opex=injector.getInstance(OperationExecutor.class);
  LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
  DataSetAccessor dataSetAccessor=injector.getInstance(DataSetAccessor.class);
  TransactionSystemClient txSystemClient=injector.getInstance(TransactionSystemClient.class);
  OperationContext opCtx=new OperationContext(DefaultId.ACCOUNT.getId(),app.getAppSpecLoc().getSpecification().getName());
  TransactionProxy proxy=new TransactionProxy();
  DataSetInstantiator dataSetInstantiator=new DataSetInstantiator(new DataFabricImpl(opex,locationFactory,dataSetAccessor,opCtx),proxy,getClass().getClassLoader());
  dataSetInstantiator.setDataSets(ImmutableList.copyOf(new AppWithMapReduce().configure().getDataSets().values()));
  TransactionAgent txAgent=new SynchronousTransactionAgent(opex,opCtx,dataSetInstantiator.getTransactionAware(),txSystemClient);
  proxy.setTransactionAgent(txAgent);
  TimeseriesTable table=(TimeseriesTable)dataSetInstantiator.getDataSet(""String_Node_Str"");
  txAgent.start();
  fillTestInputData(table);
  txAgent.finish();
  Thread.sleep(2);
  long start=System.currentTimeMillis();
  runProgram(app,AppWithMapReduce.AggregateTimeseriesByTag.class);
  long stop=System.currentTimeMillis();
  Map<String,Long> expected=Maps.newHashMap();
  expected.put(""String_Node_Str"",18L);
  expected.put(""String_Node_Str"",3L);
  expected.put(""String_Node_Str"",18L);
  table=(TimeseriesTable)dataSetInstantiator.getDataSet(""String_Node_Str"");
  txAgent.start();
  List<TimeseriesTable.Entry> agg=table.read(AggregateMetricsByTag.BY_TAGS,start,stop);
  Assert.assertEquals(expected.size(),agg.size());
  for (  TimeseriesTable.Entry entry : agg) {
    String tag=Bytes.toString(entry.getTags()[0]);
    Assert.assertEquals((long)expected.get(tag),Bytes.toLong(entry.getValue()));
  }
  txAgent.finish();
}"
7762,"@Test public void testWordCount() throws Exception {
  final ApplicationWithPrograms app=TestHelper.deployApplicationWithManager(AppWithMapReduce.class);
  OperationExecutor opex=injector.getInstance(OperationExecutor.class);
  LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
  DataSetAccessor dataSetAccessor=injector.getInstance(DataSetAccessor.class);
  TransactionSystemClient txSystemClient=injector.getInstance(TransactionSystemClient.class);
  OperationContext opCtx=new OperationContext(DefaultId.ACCOUNT.getId(),app.getAppSpecLoc().getSpecification().getName());
  String inputPath=createInput();
  File outputDir=new File(tmpFolder.newFolder(),""String_Node_Str"");
  TransactionProxy proxy=new TransactionProxy();
  DataSetInstantiator dataSetInstantiator=new DataSetInstantiator(new DataFabricImpl(opex,locationFactory,dataSetAccessor,opCtx),proxy,getClass().getClassLoader());
  dataSetInstantiator.setDataSets(ImmutableList.copyOf(new AppWithMapReduce().configure().getDataSets().values()));
  TransactionAgent txAgent=new SynchronousTransactionAgent(opex,opCtx,dataSetInstantiator.getTransactionAware(),txSystemClient);
  proxy.setTransactionAgent(txAgent);
  KeyValueTable jobConfigTable=(KeyValueTable)dataSetInstantiator.getDataSet(""String_Node_Str"");
  txAgent.start();
  jobConfigTable.write(tb(""String_Node_Str""),tb(inputPath));
  jobConfigTable.write(tb(""String_Node_Str""),tb(outputDir.getPath()));
  txAgent.finish();
  runProgram(app,AppWithMapReduce.ClassicWordCount.class);
  File outputFile=outputDir.listFiles()[0];
  int lines=0;
  BufferedReader reader=new BufferedReader(new FileReader(outputFile));
  try {
    while (true) {
      String line=reader.readLine();
      if (line == null) {
        break;
      }
      lines++;
    }
  }
  finally {
    reader.close();
  }
  Assert.assertTrue(lines > 0);
}","@Test public void testWordCount() throws Exception {
  final ApplicationWithPrograms app=TestHelper.deployApplicationWithManager(AppWithMapReduce.class);
  injector.getInstance(InMemoryTransactionManager.class).init();
  OperationExecutor opex=injector.getInstance(OperationExecutor.class);
  LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
  DataSetAccessor dataSetAccessor=injector.getInstance(DataSetAccessor.class);
  TransactionSystemClient txSystemClient=injector.getInstance(TransactionSystemClient.class);
  OperationContext opCtx=new OperationContext(DefaultId.ACCOUNT.getId(),app.getAppSpecLoc().getSpecification().getName());
  String inputPath=createInput();
  File outputDir=new File(tmpFolder.newFolder(),""String_Node_Str"");
  TransactionProxy proxy=new TransactionProxy();
  DataSetInstantiator dataSetInstantiator=new DataSetInstantiator(new DataFabricImpl(opex,locationFactory,dataSetAccessor,opCtx),proxy,getClass().getClassLoader());
  dataSetInstantiator.setDataSets(ImmutableList.copyOf(new AppWithMapReduce().configure().getDataSets().values()));
  TransactionAgent txAgent=new SynchronousTransactionAgent(opex,opCtx,dataSetInstantiator.getTransactionAware(),txSystemClient);
  proxy.setTransactionAgent(txAgent);
  KeyValueTable jobConfigTable=(KeyValueTable)dataSetInstantiator.getDataSet(""String_Node_Str"");
  txAgent.start();
  jobConfigTable.write(tb(""String_Node_Str""),tb(inputPath));
  jobConfigTable.write(tb(""String_Node_Str""),tb(outputDir.getPath()));
  txAgent.finish();
  runProgram(app,AppWithMapReduce.ClassicWordCount.class);
  File outputFile=outputDir.listFiles()[0];
  int lines=0;
  BufferedReader reader=new BufferedReader(new FileReader(outputFile));
  try {
    while (true) {
      String line=reader.readLine();
      if (line == null) {
        break;
      }
      lines++;
    }
  }
  finally {
    reader.close();
  }
  Assert.assertTrue(lines > 0);
}"
7763,"@Override public void flush() throws OperationException {
  commitTxAwareDataSets();
}","@Override public void flush() throws OperationException {
  flushTxAwareDataSets();
}"
7764,"private void commitTxAwareDataSets() throws OperationException {
  List<byte[]> changes=Lists.newArrayList();
  for (  TransactionAware txnl : txAware) {
    changes.addAll(txnl.getTxChanges());
  }
  if (changes.size() > 0) {
    if (!txSystemClient.canCommit(currentTx,changes)) {
      throw new OperationException(StatusCode.TRANSACTION_CONFLICT,""String_Node_Str"");
    }
  }
  for (  TransactionAware txAware : this.txAware) {
    try {
      if (!txAware.commitTx()) {
        throw new OperationException(StatusCode.INVALID_TRANSACTION,String.format(""String_Node_Str"",txAware.getClass()));
      }
    }
 catch (    Exception e) {
      throw new OperationException(StatusCode.INVALID_TRANSACTION,""String_Node_Str"",e);
    }
  }
  if (!txSystemClient.commit(currentTx)) {
    throw new OperationException(StatusCode.INVALID_TRANSACTION,""String_Node_Str"");
  }
}","private void commitTxAwareDataSets() throws OperationException {
  List<byte[]> changes=Lists.newArrayList();
  for (  TransactionAware txnl : txAware) {
    changes.addAll(txnl.getTxChanges());
  }
  if (changes.size() > 0) {
    if (!txSystemClient.canCommit(currentTx,changes)) {
      throw new OperationException(StatusCode.TRANSACTION_CONFLICT,""String_Node_Str"");
    }
  }
  flushTxAwareDataSets();
  if (!txSystemClient.commit(currentTx)) {
    throw new OperationException(StatusCode.INVALID_TRANSACTION,""String_Node_Str"");
  }
}"
7765,"@Override public void delete(String tag) throws IOException {
  String path=pathForTag(tag);
  try {
    Uninterruptibles.getUninterruptibly(ZKOperations.ignoreError(zkClient.delete(path),KeeperException.NodeExistsException.class,path));
  }
 catch (  Throwable e) {
    throw new IOException(""String_Node_Str"" + tag + ""String_Node_Str""+ path+ ""String_Node_Str"",e);
  }
}","@Override public void delete(String tag) throws IOException {
  String path=pathForTag(tag);
  try {
    Uninterruptibles.getUninterruptibly(ZKOperations.ignoreError(zkClient.delete(path),KeeperException.NoNodeException.class,path));
  }
 catch (  Throwable e) {
    throw new IOException(""String_Node_Str"" + tag + ""String_Node_Str""+ path+ ""String_Node_Str"",e);
  }
}"
7766,"@Override public byte[] readBack(String tag) throws IOException {
  String path=pathForTag(tag);
  try {
    NodeData nodeData=Uninterruptibles.getUninterruptibly(ZKOperations.ignoreError(zkClient.getData(path),KeeperException.NodeExistsException.class,null));
    return nodeData != null ? nodeData.getData() : null;
  }
 catch (  Throwable t) {
    throw new IOException(""String_Node_Str"" + tag + ""String_Node_Str""+ path+ ""String_Node_Str"",t);
  }
}","@Override public byte[] readBack(String tag) throws IOException {
  String path=pathForTag(tag);
  try {
    NodeData nodeData=Uninterruptibles.getUninterruptibly(ZKOperations.ignoreError(zkClient.getData(path),KeeperException.NoNodeException.class,null));
    return nodeData != null ? nodeData.getData() : null;
  }
 catch (  Throwable t) {
    throw new IOException(""String_Node_Str"" + tag + ""String_Node_Str""+ path+ ""String_Node_Str"",t);
  }
}"
7767,"@Override public void run(){
  while (true) {
    int excludedListSize=txManager.getExcludedListSize();
    if (excludedListSize > 0) {
      if (txSystemMetrics != null) {
        txSystemMetrics.gauge(""String_Node_Str"",excludedListSize);
      }
      Log.info(""String_Node_Str"" + excludedListSize);
    }
    try {
      TimeUnit.SECONDS.sleep(10);
    }
 catch (    InterruptedException e) {
      this.interrupt();
      break;
    }
  }
}","@Override public void run(){
  while (true) {
    int excludedListSize=txManager.getExcludedListSize();
    if (txSystemMetrics != null) {
      txSystemMetrics.gauge(""String_Node_Str"",excludedListSize);
    }
    if (excludedListSize > 0) {
      Log.info(""String_Node_Str"" + excludedListSize);
    }
    try {
      TimeUnit.SECONDS.sleep(10);
    }
 catch (    InterruptedException e) {
      this.interrupt();
      break;
    }
  }
}"
7768,"private void startTxSystemMetricsReporter(){
  Thread txSystemMetricsReporter=new Thread(""String_Node_Str""){
    @Override public void run(){
      while (true) {
        int excludedListSize=txManager.getExcludedListSize();
        if (excludedListSize > 0) {
          if (txSystemMetrics != null) {
            txSystemMetrics.gauge(""String_Node_Str"",excludedListSize);
          }
          Log.info(""String_Node_Str"" + excludedListSize);
        }
        try {
          TimeUnit.SECONDS.sleep(10);
        }
 catch (        InterruptedException e) {
          this.interrupt();
          break;
        }
      }
    }
  }
;
  txSystemMetricsReporter.setDaemon(true);
  txSystemMetricsReporter.start();
}","private void startTxSystemMetricsReporter(){
  Thread txSystemMetricsReporter=new Thread(""String_Node_Str""){
    @Override public void run(){
      while (true) {
        int excludedListSize=txManager.getExcludedListSize();
        if (txSystemMetrics != null) {
          txSystemMetrics.gauge(""String_Node_Str"",excludedListSize);
        }
        if (excludedListSize > 0) {
          Log.info(""String_Node_Str"" + excludedListSize);
        }
        try {
          TimeUnit.SECONDS.sleep(10);
        }
 catch (        InterruptedException e) {
          this.interrupt();
          break;
        }
      }
    }
  }
;
  txSystemMetricsReporter.setDaemon(true);
  txSystemMetricsReporter.start();
}"
7769,"public static OperationExecutor getOpex(){
  try {
    MemoryOracle memoryOracle=new MemoryOracle();
    injectField(memoryOracle.getClass(),memoryOracle,""String_Node_Str"",new MemoryStrictlyMonotonicTimeOracle());
    return new OmidTransactionalOperationExecutor(memoryOracle,MemoryOVCTableHandle.getInstance(),new CConfiguration());
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","public static OperationExecutor getOpex(){
  try {
    MemoryOracle memoryOracle=new MemoryOracle();
    injectField(memoryOracle.getClass(),memoryOracle,""String_Node_Str"",new MemoryStrictlyMonotonicTimeOracle());
    InMemoryTransactionManager txManager=new InMemoryTransactionManager();
    return new OmidTransactionalOperationExecutor(memoryOracle,txManager,MemoryOVCTableHandle.getInstance(),new CConfiguration());
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
7770,"@Test public void testTimeSeriesRecordsCount() throws Exception {
  final ApplicationWithPrograms app=TestHelper.deployApplicationWithManager(AppWithMapReduce.class);
  OperationExecutor opex=injector.getInstance(OperationExecutor.class);
  LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
  DataSetAccessor dataSetAccessor=injector.getInstance(DataSetAccessor.class);
  TransactionSystemClient txSystemClient=injector.getInstance(TransactionSystemClient.class);
  OperationContext opCtx=new OperationContext(DefaultId.ACCOUNT.getId(),app.getAppSpecLoc().getSpecification().getName());
  TransactionProxy proxy=new TransactionProxy();
  DataSetInstantiator dataSetInstantiator=new DataSetInstantiator(new DataFabricImpl(opex,locationFactory,dataSetAccessor,opCtx),proxy,getClass().getClassLoader());
  dataSetInstantiator.setDataSets(ImmutableList.copyOf(new AppWithMapReduce().configure().getDataSets().values()));
  TransactionAgent txAgent=new SynchronousTransactionAgent(opex,opCtx,dataSetInstantiator.getTransactionAware(),txSystemClient);
  proxy.setTransactionAgent(txAgent);
  TimeseriesTable table=(TimeseriesTable)dataSetInstantiator.getDataSet(""String_Node_Str"");
  txAgent.start();
  fillTestInputData(table);
  txAgent.finish();
  Thread.sleep(2);
  long start=System.currentTimeMillis();
  runProgram(app,AppWithMapReduce.AggregateTimeseriesByTag.class);
  long stop=System.currentTimeMillis();
  Map<String,Long> expected=Maps.newHashMap();
  expected.put(""String_Node_Str"",18L);
  expected.put(""String_Node_Str"",3L);
  expected.put(""String_Node_Str"",18L);
  table=(TimeseriesTable)dataSetInstantiator.getDataSet(""String_Node_Str"");
  txAgent.start();
  List<TimeseriesTable.Entry> agg=table.read(AggregateMetricsByTag.BY_TAGS,start,stop);
  Assert.assertEquals(expected.size(),agg.size());
  for (  TimeseriesTable.Entry entry : agg) {
    String tag=Bytes.toString(entry.getTags()[0]);
    Assert.assertEquals((long)expected.get(tag),Bytes.toLong(entry.getValue()));
  }
  txAgent.finish();
}","@Test public void testTimeSeriesRecordsCount() throws Exception {
  final ApplicationWithPrograms app=TestHelper.deployApplicationWithManager(AppWithMapReduce.class);
  injector.getInstance(InMemoryTransactionManager.class).init();
  OperationExecutor opex=injector.getInstance(OperationExecutor.class);
  LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
  DataSetAccessor dataSetAccessor=injector.getInstance(DataSetAccessor.class);
  TransactionSystemClient txSystemClient=injector.getInstance(TransactionSystemClient.class);
  OperationContext opCtx=new OperationContext(DefaultId.ACCOUNT.getId(),app.getAppSpecLoc().getSpecification().getName());
  TransactionProxy proxy=new TransactionProxy();
  DataSetInstantiator dataSetInstantiator=new DataSetInstantiator(new DataFabricImpl(opex,locationFactory,dataSetAccessor,opCtx),proxy,getClass().getClassLoader());
  dataSetInstantiator.setDataSets(ImmutableList.copyOf(new AppWithMapReduce().configure().getDataSets().values()));
  TransactionAgent txAgent=new SynchronousTransactionAgent(opex,opCtx,dataSetInstantiator.getTransactionAware(),txSystemClient);
  proxy.setTransactionAgent(txAgent);
  TimeseriesTable table=(TimeseriesTable)dataSetInstantiator.getDataSet(""String_Node_Str"");
  txAgent.start();
  fillTestInputData(table);
  txAgent.finish();
  Thread.sleep(2);
  long start=System.currentTimeMillis();
  runProgram(app,AppWithMapReduce.AggregateTimeseriesByTag.class);
  long stop=System.currentTimeMillis();
  Map<String,Long> expected=Maps.newHashMap();
  expected.put(""String_Node_Str"",18L);
  expected.put(""String_Node_Str"",3L);
  expected.put(""String_Node_Str"",18L);
  table=(TimeseriesTable)dataSetInstantiator.getDataSet(""String_Node_Str"");
  txAgent.start();
  List<TimeseriesTable.Entry> agg=table.read(AggregateMetricsByTag.BY_TAGS,start,stop);
  Assert.assertEquals(expected.size(),agg.size());
  for (  TimeseriesTable.Entry entry : agg) {
    String tag=Bytes.toString(entry.getTags()[0]);
    Assert.assertEquals((long)expected.get(tag),Bytes.toLong(entry.getValue()));
  }
  txAgent.finish();
}"
7771,"@Test public void testWordCount() throws Exception {
  final ApplicationWithPrograms app=TestHelper.deployApplicationWithManager(AppWithMapReduce.class);
  OperationExecutor opex=injector.getInstance(OperationExecutor.class);
  LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
  DataSetAccessor dataSetAccessor=injector.getInstance(DataSetAccessor.class);
  TransactionSystemClient txSystemClient=injector.getInstance(TransactionSystemClient.class);
  OperationContext opCtx=new OperationContext(DefaultId.ACCOUNT.getId(),app.getAppSpecLoc().getSpecification().getName());
  String inputPath=createInput();
  File outputDir=new File(tmpFolder.newFolder(),""String_Node_Str"");
  TransactionProxy proxy=new TransactionProxy();
  DataSetInstantiator dataSetInstantiator=new DataSetInstantiator(new DataFabricImpl(opex,locationFactory,dataSetAccessor,opCtx),proxy,getClass().getClassLoader());
  dataSetInstantiator.setDataSets(ImmutableList.copyOf(new AppWithMapReduce().configure().getDataSets().values()));
  TransactionAgent txAgent=new SynchronousTransactionAgent(opex,opCtx,dataSetInstantiator.getTransactionAware(),txSystemClient);
  proxy.setTransactionAgent(txAgent);
  KeyValueTable jobConfigTable=(KeyValueTable)dataSetInstantiator.getDataSet(""String_Node_Str"");
  txAgent.start();
  jobConfigTable.write(tb(""String_Node_Str""),tb(inputPath));
  jobConfigTable.write(tb(""String_Node_Str""),tb(outputDir.getPath()));
  txAgent.finish();
  runProgram(app,AppWithMapReduce.ClassicWordCount.class);
  File outputFile=outputDir.listFiles()[0];
  int lines=0;
  BufferedReader reader=new BufferedReader(new FileReader(outputFile));
  try {
    while (true) {
      String line=reader.readLine();
      if (line == null) {
        break;
      }
      lines++;
    }
  }
  finally {
    reader.close();
  }
  Assert.assertTrue(lines > 0);
}","@Test public void testWordCount() throws Exception {
  final ApplicationWithPrograms app=TestHelper.deployApplicationWithManager(AppWithMapReduce.class);
  injector.getInstance(InMemoryTransactionManager.class).init();
  OperationExecutor opex=injector.getInstance(OperationExecutor.class);
  LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
  DataSetAccessor dataSetAccessor=injector.getInstance(DataSetAccessor.class);
  TransactionSystemClient txSystemClient=injector.getInstance(TransactionSystemClient.class);
  OperationContext opCtx=new OperationContext(DefaultId.ACCOUNT.getId(),app.getAppSpecLoc().getSpecification().getName());
  String inputPath=createInput();
  File outputDir=new File(tmpFolder.newFolder(),""String_Node_Str"");
  TransactionProxy proxy=new TransactionProxy();
  DataSetInstantiator dataSetInstantiator=new DataSetInstantiator(new DataFabricImpl(opex,locationFactory,dataSetAccessor,opCtx),proxy,getClass().getClassLoader());
  dataSetInstantiator.setDataSets(ImmutableList.copyOf(new AppWithMapReduce().configure().getDataSets().values()));
  TransactionAgent txAgent=new SynchronousTransactionAgent(opex,opCtx,dataSetInstantiator.getTransactionAware(),txSystemClient);
  proxy.setTransactionAgent(txAgent);
  KeyValueTable jobConfigTable=(KeyValueTable)dataSetInstantiator.getDataSet(""String_Node_Str"");
  txAgent.start();
  jobConfigTable.write(tb(""String_Node_Str""),tb(inputPath));
  jobConfigTable.write(tb(""String_Node_Str""),tb(outputDir.getPath()));
  txAgent.finish();
  runProgram(app,AppWithMapReduce.ClassicWordCount.class);
  File outputFile=outputDir.listFiles()[0];
  int lines=0;
  BufferedReader reader=new BufferedReader(new FileReader(outputFile));
  try {
    while (true) {
      String line=reader.readLine();
      if (line == null) {
        break;
      }
      lines++;
    }
  }
  finally {
    reader.close();
  }
  Assert.assertTrue(lines > 0);
}"
7772,"@Override public void flush() throws OperationException {
  commitTxAwareDataSets();
}","@Override public void flush() throws OperationException {
  flushTxAwareDataSets();
}"
7773,"private void commitTxAwareDataSets() throws OperationException {
  List<byte[]> changes=Lists.newArrayList();
  for (  TransactionAware txnl : txAware) {
    changes.addAll(txnl.getTxChanges());
  }
  if (changes.size() > 0) {
    if (!txSystemClient.canCommit(currentTx,changes)) {
      throw new OperationException(StatusCode.TRANSACTION_CONFLICT,""String_Node_Str"");
    }
  }
  for (  TransactionAware txAware : this.txAware) {
    try {
      if (!txAware.commitTx()) {
        throw new OperationException(StatusCode.INVALID_TRANSACTION,String.format(""String_Node_Str"",txAware.getClass()));
      }
    }
 catch (    Exception e) {
      throw new OperationException(StatusCode.INVALID_TRANSACTION,""String_Node_Str"",e);
    }
  }
  if (!txSystemClient.commit(currentTx)) {
    throw new OperationException(StatusCode.INVALID_TRANSACTION,""String_Node_Str"");
  }
}","private void commitTxAwareDataSets() throws OperationException {
  List<byte[]> changes=Lists.newArrayList();
  for (  TransactionAware txnl : txAware) {
    changes.addAll(txnl.getTxChanges());
  }
  if (changes.size() > 0) {
    if (!txSystemClient.canCommit(currentTx,changes)) {
      throw new OperationException(StatusCode.TRANSACTION_CONFLICT,""String_Node_Str"");
    }
  }
  flushTxAwareDataSets();
  if (!txSystemClient.commit(currentTx)) {
    throw new OperationException(StatusCode.INVALID_TRANSACTION,""String_Node_Str"");
  }
}"
7774,"@BeforeClass public static void init() throws Exception {
  CConfiguration conf=CConfiguration.create();
  conf.unset(Constants.CFG_DATA_LEVELDB_DIR);
  conf.setBoolean(StatePersistor.CFG_DO_PERSIST,false);
  Injector injector=Guice.createInjector(new DataFabricLocalModule(conf));
  injector.getInstance(InMemoryTransactionManager.class).init();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}","@BeforeClass public static void init() throws Exception {
  CConfiguration conf=CConfiguration.create();
  conf.unset(Constants.CFG_DATA_LEVELDB_DIR);
  conf.setBoolean(StatePersistor.CFG_DO_PERSIST,false);
  Injector injector=Guice.createInjector(new DataFabricLocalModule(conf));
  transactionManager=injector.getInstance(InMemoryTransactionManager.class);
  transactionManager.init();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}"
7775,"@BeforeClass public static void init() throws Exception {
  zkServer=InMemoryZKServer.builder().setDataDir(tmpFolder.newFolder()).build();
  zkServer.startAndWait();
  HBaseTestBase.startHBase();
  final DataFabricDistributedModule dataFabricModule=new DataFabricDistributedModule(HBaseTestBase.getConfiguration());
  final CConfiguration cConf=dataFabricModule.getConfiguration();
  cConf.set(Constants.CFG_ZOOKEEPER_ENSEMBLE,zkServer.getConnectionStr());
  cConf.set(com.continuuity.data.operation.executor.remote.Constants.CFG_DATA_OPEX_SERVER_PORT,Integer.toString(Networks.getRandomPort()));
  cConf.set(HBaseTableUtil.CFG_TABLE_PREFIX,""String_Node_Str"");
  cConf.setBoolean(StatePersistor.CFG_DO_PERSIST,false);
  final Injector injector=Guice.createInjector(dataFabricModule,new AbstractModule(){
    @Override protected void configure(){
      try {
        bind(LocationFactory.class).toInstance(new LocalLocationFactory(tmpFolder.newFolder()));
      }
 catch (      IOException e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
  injector.getInstance(InMemoryTransactionManager.class).init();
  opexService=injector.getInstance(OperationExecutorService.class);
  Thread t=new Thread(){
    @Override public void run(){
      try {
        opexService.start(new String[]{},cConf);
      }
 catch (      ServerException e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
;
  t.start();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}","@BeforeClass public static void init() throws Exception {
  HBaseTestBase.startHBase();
  final DataFabricDistributedModule dataFabricModule=new DataFabricDistributedModule(HBaseTestBase.getConfiguration());
  final CConfiguration cConf=dataFabricModule.getConfiguration();
  cConf.set(Constants.CFG_ZOOKEEPER_ENSEMBLE,HBaseTestBase.getZkConnectionString());
  cConf.set(com.continuuity.data.operation.executor.remote.Constants.CFG_DATA_OPEX_SERVER_PORT,Integer.toString(Networks.getRandomPort()));
  cConf.set(HBaseTableUtil.CFG_TABLE_PREFIX,""String_Node_Str"");
  cConf.setBoolean(StatePersistor.CFG_DO_PERSIST,false);
  final Injector injector=Guice.createInjector(dataFabricModule,new AbstractModule(){
    @Override protected void configure(){
      try {
        bind(LocationFactory.class).toInstance(new LocalLocationFactory(tmpFolder.newFolder()));
      }
 catch (      IOException e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
  transactionManager=injector.getInstance(InMemoryTransactionManager.class);
  transactionManager.init();
  opexService=injector.getInstance(OperationExecutorService.class);
  Thread t=new Thread(){
    @Override public void run(){
      try {
        opexService.start(new String[]{},cConf);
      }
 catch (      ServerException e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
;
  t.start();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}"
7776,"@AfterClass public static void finish() throws Exception {
  opexService.stop(true);
  HBaseTestBase.stopHBase();
  zkServer.stopAndWait();
}","@AfterClass public static void finish() throws Exception {
  opexService.stop(true);
  HBaseTestBase.stopHBase();
}"
7777,"@BeforeClass public static void init() throws Exception {
  injector=Guice.createInjector(new DataFabricModules().getInMemoryModules());
  injector.getInstance(InMemoryTransactionManager.class).init();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}","@BeforeClass public static void init() throws Exception {
  injector=Guice.createInjector(new DataFabricModules().getInMemoryModules());
  transactionManager=injector.getInstance(InMemoryTransactionManager.class);
  transactionManager.init();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}"
7778,"@BeforeClass public static void init() throws Exception {
  CConfiguration conf=CConfiguration.create();
  conf.unset(Constants.CFG_DATA_LEVELDB_DIR);
  Injector injector=Guice.createInjector(new DataFabricLevelDBModule(conf));
  injector.getInstance(InMemoryTransactionManager.class).init();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}","@BeforeClass public static void init() throws Exception {
  CConfiguration conf=CConfiguration.create();
  conf.unset(Constants.CFG_DATA_LEVELDB_DIR);
  Injector injector=Guice.createInjector(new DataFabricLevelDBModule(conf));
  transactionManager=injector.getInstance(InMemoryTransactionManager.class);
  transactionManager.init();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}"
7779,"@BeforeClass public static void init() throws Exception {
  CConfiguration conf=CConfiguration.create();
  conf.unset(Constants.CFG_DATA_LEVELDB_DIR);
  conf.setBoolean(StatePersistor.CFG_DO_PERSIST,false);
  Injector injector=Guice.createInjector(new DataFabricLocalModule(conf));
  injector.getInstance(InMemoryTransactionManager.class).init();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}","@BeforeClass public static void init() throws Exception {
  CConfiguration conf=CConfiguration.create();
  conf.unset(Constants.CFG_DATA_LEVELDB_DIR);
  conf.setBoolean(StatePersistor.CFG_DO_PERSIST,false);
  Injector injector=Guice.createInjector(new DataFabricLocalModule(conf));
  transactionManager=injector.getInstance(InMemoryTransactionManager.class);
  transactionManager.init();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}"
7780,"@BeforeClass public static void init() throws Exception {
  zkServer=InMemoryZKServer.builder().setDataDir(tmpFolder.newFolder()).build();
  zkServer.startAndWait();
  HBaseTestBase.startHBase();
  final DataFabricDistributedModule dataFabricModule=new DataFabricDistributedModule(HBaseTestBase.getConfiguration());
  final CConfiguration cConf=dataFabricModule.getConfiguration();
  cConf.set(Constants.CFG_ZOOKEEPER_ENSEMBLE,zkServer.getConnectionStr());
  cConf.set(com.continuuity.data.operation.executor.remote.Constants.CFG_DATA_OPEX_SERVER_PORT,Integer.toString(Networks.getRandomPort()));
  cConf.set(HBaseTableUtil.CFG_TABLE_PREFIX,""String_Node_Str"");
  cConf.setBoolean(StatePersistor.CFG_DO_PERSIST,false);
  final Injector injector=Guice.createInjector(dataFabricModule,new AbstractModule(){
    @Override protected void configure(){
      try {
        bind(LocationFactory.class).toInstance(new LocalLocationFactory(tmpFolder.newFolder()));
      }
 catch (      IOException e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
  injector.getInstance(InMemoryTransactionManager.class).init();
  opexService=injector.getInstance(OperationExecutorService.class);
  Thread t=new Thread(){
    @Override public void run(){
      try {
        opexService.start(new String[]{},cConf);
      }
 catch (      ServerException e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
;
  t.start();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}","@BeforeClass public static void init() throws Exception {
  HBaseTestBase.startHBase();
  final DataFabricDistributedModule dataFabricModule=new DataFabricDistributedModule(HBaseTestBase.getConfiguration());
  final CConfiguration cConf=dataFabricModule.getConfiguration();
  cConf.set(Constants.CFG_ZOOKEEPER_ENSEMBLE,HBaseTestBase.getZkConnectionString());
  cConf.set(com.continuuity.data.operation.executor.remote.Constants.CFG_DATA_OPEX_SERVER_PORT,Integer.toString(Networks.getRandomPort()));
  cConf.set(HBaseTableUtil.CFG_TABLE_PREFIX,""String_Node_Str"");
  cConf.setBoolean(StatePersistor.CFG_DO_PERSIST,false);
  final Injector injector=Guice.createInjector(dataFabricModule,new AbstractModule(){
    @Override protected void configure(){
      try {
        bind(LocationFactory.class).toInstance(new LocalLocationFactory(tmpFolder.newFolder()));
      }
 catch (      IOException e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
  transactionManager=injector.getInstance(InMemoryTransactionManager.class);
  transactionManager.init();
  opexService=injector.getInstance(OperationExecutorService.class);
  Thread t=new Thread(){
    @Override public void run(){
      try {
        opexService.start(new String[]{},cConf);
      }
 catch (      ServerException e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
;
  t.start();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}"
7781,"@AfterClass public static void finish() throws Exception {
  opexService.stop(true);
  HBaseTestBase.stopHBase();
  zkServer.stopAndWait();
}","@AfterClass public static void finish() throws Exception {
  opexService.stop(true);
  HBaseTestBase.stopHBase();
}"
7782,"@BeforeClass public static void init() throws Exception {
  injector=Guice.createInjector(new DataFabricModules().getInMemoryModules());
  injector.getInstance(InMemoryTransactionManager.class).init();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}","@BeforeClass public static void init() throws Exception {
  injector=Guice.createInjector(new DataFabricModules().getInMemoryModules());
  transactionManager=injector.getInstance(InMemoryTransactionManager.class);
  transactionManager.init();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}"
7783,"@BeforeClass public static void init() throws Exception {
  CConfiguration conf=CConfiguration.create();
  conf.unset(Constants.CFG_DATA_LEVELDB_DIR);
  Injector injector=Guice.createInjector(new DataFabricLevelDBModule(conf));
  injector.getInstance(InMemoryTransactionManager.class).init();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}","@BeforeClass public static void init() throws Exception {
  CConfiguration conf=CConfiguration.create();
  conf.unset(Constants.CFG_DATA_LEVELDB_DIR);
  Injector injector=Guice.createInjector(new DataFabricLevelDBModule(conf));
  transactionManager=injector.getInstance(InMemoryTransactionManager.class);
  transactionManager.init();
  opex=injector.getInstance(OperationExecutor.class);
  queueClientFactory=injector.getInstance(QueueClientFactory.class);
  queueAdmin=injector.getInstance(QueueAdmin.class);
}"
7784,"@Override public void delete(String tag) throws IOException {
  String path=pathForTag(tag);
  try {
    Uninterruptibles.getUninterruptibly(ZKOperations.ignoreError(zkClient.delete(path),KeeperException.NodeExistsException.class,path));
  }
 catch (  Throwable e) {
    throw new IOException(""String_Node_Str"" + tag + ""String_Node_Str""+ path+ ""String_Node_Str"",e);
  }
}","@Override public void delete(String tag) throws IOException {
  String path=pathForTag(tag);
  try {
    Uninterruptibles.getUninterruptibly(ZKOperations.ignoreError(zkClient.delete(path),KeeperException.NoNodeException.class,path));
  }
 catch (  Throwable e) {
    throw new IOException(""String_Node_Str"" + tag + ""String_Node_Str""+ path+ ""String_Node_Str"",e);
  }
}"
7785,"@Override public byte[] readBack(String tag) throws IOException {
  String path=pathForTag(tag);
  try {
    NodeData nodeData=Uninterruptibles.getUninterruptibly(ZKOperations.ignoreError(zkClient.getData(path),KeeperException.NodeExistsException.class,null));
    return nodeData != null ? nodeData.getData() : null;
  }
 catch (  Throwable t) {
    throw new IOException(""String_Node_Str"" + tag + ""String_Node_Str""+ path+ ""String_Node_Str"",t);
  }
}","@Override public byte[] readBack(String tag) throws IOException {
  String path=pathForTag(tag);
  try {
    NodeData nodeData=Uninterruptibles.getUninterruptibly(ZKOperations.ignoreError(zkClient.getData(path),KeeperException.NoNodeException.class,null));
    return nodeData != null ? nodeData.getData() : null;
  }
 catch (  Throwable t) {
    throw new IOException(""String_Node_Str"" + tag + ""String_Node_Str""+ path+ ""String_Node_Str"",t);
  }
}"
7786,"synchronized public void initializeTable() throws OperationException {
  try {
    this.db=factory.open(new File(generateDBPath()),generateDBOptions(true,false));
  }
 catch (  IOException e) {
    throw createOperationException(e,""String_Node_Str"");
  }
}","public synchronized void initializeTable() throws OperationException {
  try {
    this.db=factory.open(new File(generateDBPath()),generateDBOptions(true,false));
  }
 catch (  IOException e) {
    throw createOperationException(e,""String_Node_Str"");
  }
}"
7787,"synchronized public boolean openTable() throws OperationException {
  try {
    this.db=factory.open(new File(generateDBPath()),generateDBOptions(false,false));
    return true;
  }
 catch (  IOException e) {
    return false;
  }
}","public synchronized boolean openTable() throws OperationException {
  try {
    this.db=factory.open(new File(generateDBPath()),generateDBOptions(false,false));
    return true;
  }
 catch (  IOException e) {
    return false;
  }
}"
7788,"synchronized public void initializeTable() throws OperationException {
  try {
    this.db=factory.open(new File(generateDBPath()),generateDBOptions(true,false));
  }
 catch (  IOException e) {
    throw createOperationException(e,""String_Node_Str"");
  }
}","public synchronized void initializeTable() throws OperationException {
  try {
    this.db=factory.open(new File(generateDBPath()),generateDBOptions(true,false));
  }
 catch (  IOException e) {
    throw createOperationException(e,""String_Node_Str"");
  }
}"
7789,"synchronized public boolean openTable() throws OperationException {
  try {
    this.db=factory.open(new File(generateDBPath()),generateDBOptions(false,false));
    return true;
  }
 catch (  IOException e) {
    return false;
  }
}","public synchronized boolean openTable() throws OperationException {
  try {
    this.db=factory.open(new File(generateDBPath()),generateDBOptions(false,false));
    return true;
  }
 catch (  IOException e) {
    return false;
  }
}"
7790,"@Override public void run(){
  while (true) {
    int excludedListSize=txManager.getExcludedListSize();
    if (excludedListSize > 0) {
      if (txSystemMetrics != null) {
        txSystemMetrics.gauge(""String_Node_Str"",excludedListSize);
      }
      Log.info(""String_Node_Str"" + excludedListSize);
    }
    try {
      TimeUnit.SECONDS.sleep(10);
    }
 catch (    InterruptedException e) {
      this.interrupt();
      break;
    }
  }
}","@Override public void run(){
  while (true) {
    int excludedListSize=txManager.getExcludedListSize();
    if (txSystemMetrics != null) {
      txSystemMetrics.gauge(""String_Node_Str"",excludedListSize);
    }
    if (excludedListSize > 0) {
      Log.info(""String_Node_Str"" + excludedListSize);
    }
    try {
      TimeUnit.SECONDS.sleep(10);
    }
 catch (    InterruptedException e) {
      this.interrupt();
      break;
    }
  }
}"
7791,"private void startTxSystemMetricsReporter(){
  Thread txSystemMetricsReporter=new Thread(""String_Node_Str""){
    @Override public void run(){
      while (true) {
        int excludedListSize=txManager.getExcludedListSize();
        if (excludedListSize > 0) {
          if (txSystemMetrics != null) {
            txSystemMetrics.gauge(""String_Node_Str"",excludedListSize);
          }
          Log.info(""String_Node_Str"" + excludedListSize);
        }
        try {
          TimeUnit.SECONDS.sleep(10);
        }
 catch (        InterruptedException e) {
          this.interrupt();
          break;
        }
      }
    }
  }
;
  txSystemMetricsReporter.setDaemon(true);
  txSystemMetricsReporter.start();
}","private void startTxSystemMetricsReporter(){
  Thread txSystemMetricsReporter=new Thread(""String_Node_Str""){
    @Override public void run(){
      while (true) {
        int excludedListSize=txManager.getExcludedListSize();
        if (txSystemMetrics != null) {
          txSystemMetrics.gauge(""String_Node_Str"",excludedListSize);
        }
        if (excludedListSize > 0) {
          Log.info(""String_Node_Str"" + excludedListSize);
        }
        try {
          TimeUnit.SECONDS.sleep(10);
        }
 catch (        InterruptedException e) {
          this.interrupt();
          break;
        }
      }
    }
  }
;
  txSystemMetricsReporter.setDaemon(true);
  txSystemMetricsReporter.start();
}"
7792,"public static OperationExecutor getOpex(){
  try {
    MemoryOracle memoryOracle=new MemoryOracle();
    injectField(memoryOracle.getClass(),memoryOracle,""String_Node_Str"",new MemoryStrictlyMonotonicTimeOracle());
    return new OmidTransactionalOperationExecutor(memoryOracle,MemoryOVCTableHandle.getInstance(),new CConfiguration());
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","public static OperationExecutor getOpex(){
  try {
    MemoryOracle memoryOracle=new MemoryOracle();
    injectField(memoryOracle.getClass(),memoryOracle,""String_Node_Str"",new MemoryStrictlyMonotonicTimeOracle());
    InMemoryTransactionManager txManager=new InMemoryTransactionManager();
    return new OmidTransactionalOperationExecutor(memoryOracle,txManager,MemoryOVCTableHandle.getInstance(),new CConfiguration());
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
7793,"/** 
 * Call the appropriate handler for handling the httprequest. 404 if path is not found. 405 if path is found but httpMethod does not match what's configured.
 * @param request instance of {@code HttpRequest}
 * @param responder instance of {@code HttpResponder} to handle the request.
 */
public void handle(HttpRequest request,HttpResponder responder){
  Map<String,String> groupValues=Maps.newHashMap();
  List<HttpResourceModel> resourceModels=patternRouter.getDestinations(request.getUri(),groupValues);
  HttpResourceModel httpResourceModel=getMatchedResourceModel(resourceModels,request.getMethod());
  if (httpResourceModel != null) {
    httpResourceModel.handle(request,responder,groupValues);
  }
 else   if (resourceModels.size() > 0) {
    responder.sendError(HttpResponseStatus.METHOD_NOT_ALLOWED,String.format(""String_Node_Str"",request.getUri()));
  }
 else {
    responder.sendError(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",request.getUri()));
  }
}","/** 
 * Call the appropriate handler for handling the httprequest. 404 if path is not found. 405 if path is found but httpMethod does not match what's configured.
 * @param request instance of {@code HttpRequest}
 * @param responder instance of {@code HttpResponder} to handle the request.
 */
public void handle(HttpRequest request,HttpResponder responder){
  Map<String,String> groupValues=Maps.newHashMap();
  String path=URI.create(request.getUri()).getPath();
  List<HttpResourceModel> resourceModels=patternRouter.getDestinations(path,groupValues);
  HttpResourceModel httpResourceModel=getMatchedResourceModel(resourceModels,request.getMethod());
  try {
    if (httpResourceModel != null) {
      httpResourceModel.handle(request,responder,groupValues);
    }
 else     if (resourceModels.size() > 0) {
      responder.sendError(HttpResponseStatus.METHOD_NOT_ALLOWED,String.format(""String_Node_Str"",request.getUri()));
    }
 else {
      responder.sendError(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",request.getUri()));
    }
  }
 catch (  Throwable t) {
    responder.sendError(HttpResponseStatus.INTERNAL_SERVER_ERROR,String.format(""String_Node_Str"",t.getMessage()));
  }
}"
7794,"/** 
 * Handle http Request.
 * @param request  HttpRequest to be handled.
 * @param responder HttpResponder to write the response.
 * @param groupValues Values needed for the invocation.
 */
public void handle(HttpRequest request,HttpResponder responder,Map<String,String> groupValues){
  try {
    if (httpMethods.contains(request.getMethod())) {
      Object[] args=new Object[method.getParameterTypes().length];
      int index=0;
      args[index]=request;
      index++;
      args[index]=responder;
      if (method.getParameterTypes().length > 2) {
        Class<?>[] parameterTypes=method.getParameterTypes();
        for (        Map.Entry<String,String> entry : groupValues.entrySet()) {
          index++;
          args[index]=ConvertUtils.convert(entry.getValue(),parameterTypes[index]);
        }
      }
      method.invoke(handler,args);
    }
 else {
      responder.sendError(HttpResponseStatus.METHOD_NOT_ALLOWED,String.format(""String_Node_Str"",request.getUri()));
    }
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",request.getUri(),e,e);
    responder.sendError(HttpResponseStatus.INTERNAL_SERVER_ERROR,String.format(""String_Node_Str"",request.getUri()));
  }
}","/** 
 * Handle http Request.
 * @param request  HttpRequest to be handled.
 * @param responder HttpResponder to write the response.
 * @param groupValues Values needed for the invocation.
 */
public void handle(HttpRequest request,HttpResponder responder,Map<String,String> groupValues){
  try {
    if (httpMethods.contains(request.getMethod())) {
      Object[] args=new Object[method.getParameterTypes().length];
      int parameterIndex=0;
      args[parameterIndex]=request;
      parameterIndex++;
      args[parameterIndex]=responder;
      if (method.getParameterTypes().length > 2) {
        Class<?>[] parameterTypes=method.getParameterTypes();
        for (        Annotation[] annotations : method.getParameterAnnotations()) {
          for (          Annotation annotation : annotations) {
            if (annotation.annotationType().isAssignableFrom(PathParam.class)) {
              PathParam param=(PathParam)annotation;
              String value=groupValues.get(param.value());
              Preconditions.checkArgument(value != null,""String_Node_Str"",param.value());
              parameterIndex++;
              args[parameterIndex]=ConvertUtils.convert(value,parameterTypes[parameterIndex]);
            }
          }
        }
        Preconditions.checkArgument(method.getParameterTypes().length == parameterIndex + 1,""String_Node_Str"",method.getName());
      }
      method.invoke(handler,args);
    }
 else {
      responder.sendError(HttpResponseStatus.METHOD_NOT_ALLOWED,String.format(""String_Node_Str"",request.getUri()));
    }
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",request.getUri(),e,e);
    responder.sendError(HttpResponseStatus.INTERNAL_SERVER_ERROR,String.format(""String_Node_Str"",request.getUri()));
  }
}"
7795,"private <T>ProcessMethodCallback processMethodCallback(final BlockingQueue<FlowletProcessEntry<?>> processQueue,final FlowletProcessEntry<T> processEntry,final InputDatum input){
  return new ProcessMethodCallback(){
    @Override public void onSuccess(    Object object,    InputContext inputContext){
      try {
        flowletContext.getSystemMetrics().gauge(""String_Node_Str"",1);
        txCallback.onSuccess(object,inputContext);
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",flowletContext,t);
      }
 finally {
        enqueueEntry();
        inflight.decrementAndGet();
      }
    }
    @Override public void onFailure(    Object inputObject,    InputContext inputContext,    FailureReason reason,    InputAcknowledger inputAcknowledger){
      LOG.warn(""String_Node_Str"",flowletContext,reason.getMessage(),input,reason.getCause());
      FailurePolicy failurePolicy;
      try {
        flowletContext.getSystemMetrics().gauge(""String_Node_Str"",1);
        failurePolicy=txCallback.onFailure(inputObject,inputContext,reason);
        if (failurePolicy == null) {
          failurePolicy=FailurePolicy.RETRY;
          LOG.info(""String_Node_Str"",failurePolicy);
        }
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",flowletContext,t);
        failurePolicy=FailurePolicy.RETRY;
      }
      if (input.getRetry() >= processEntry.getProcessSpec().getProcessMethod().getMaxRetries()) {
        LOG.info(""String_Node_Str"",input);
        failurePolicy=FailurePolicy.IGNORE;
      }
      if (failurePolicy == FailurePolicy.RETRY) {
        FlowletProcessEntry retryEntry=processEntry.isRetry() ? processEntry : FlowletProcessEntry.create(processEntry.getProcessSpec(),new ProcessSpecification<T>(new SingleItemQueueReader(input),processEntry.getProcessSpec().getInputDecoder(),processEntry.getProcessSpec().getProcessMethod()));
        processQueue.offer(retryEntry);
      }
 else       if (failurePolicy == FailurePolicy.IGNORE) {
        try {
          flowletContext.getSystemMetrics().gauge(""String_Node_Str"",1);
          inputAcknowledger.ack();
        }
 catch (        OperationException e) {
          LOG.error(""String_Node_Str"",flowletContext,e);
        }
 finally {
          enqueueEntry();
          inflight.decrementAndGet();
        }
      }
    }
    private void enqueueEntry(){
      if (!flowletContext.isAsyncMode()) {
        processQueue.offer(processEntry.resetRetry());
      }
    }
  }
;
}","private <T>ProcessMethodCallback processMethodCallback(final BlockingQueue<FlowletProcessEntry<?>> processQueue,final FlowletProcessEntry<T> processEntry,final InputDatum input){
  return new ProcessMethodCallback(){
    @Override public void onSuccess(    Object object,    InputContext inputContext){
      try {
        flowletContext.getSystemMetrics().gauge(""String_Node_Str"",input.size());
        txCallback.onSuccess(object,inputContext);
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",flowletContext,t);
      }
 finally {
        enqueueEntry();
        inflight.decrementAndGet();
      }
    }
    @Override public void onFailure(    Object inputObject,    InputContext inputContext,    FailureReason reason,    InputAcknowledger inputAcknowledger){
      LOG.warn(""String_Node_Str"",flowletContext,reason.getMessage(),input,reason.getCause());
      FailurePolicy failurePolicy;
      try {
        flowletContext.getSystemMetrics().gauge(""String_Node_Str"",1);
        failurePolicy=txCallback.onFailure(inputObject,inputContext,reason);
        if (failurePolicy == null) {
          failurePolicy=FailurePolicy.RETRY;
          LOG.info(""String_Node_Str"",failurePolicy);
        }
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",flowletContext,t);
        failurePolicy=FailurePolicy.RETRY;
      }
      if (input.getRetry() >= processEntry.getProcessSpec().getProcessMethod().getMaxRetries()) {
        LOG.info(""String_Node_Str"",input);
        failurePolicy=FailurePolicy.IGNORE;
      }
      if (failurePolicy == FailurePolicy.RETRY) {
        FlowletProcessEntry retryEntry=processEntry.isRetry() ? processEntry : FlowletProcessEntry.create(processEntry.getProcessSpec(),new ProcessSpecification<T>(new SingleItemQueueReader(input),processEntry.getProcessSpec().getInputDecoder(),processEntry.getProcessSpec().getProcessMethod()));
        processQueue.offer(retryEntry);
      }
 else       if (failurePolicy == FailurePolicy.IGNORE) {
        try {
          flowletContext.getSystemMetrics().gauge(""String_Node_Str"",input.size());
          inputAcknowledger.ack();
        }
 catch (        OperationException e) {
          LOG.error(""String_Node_Str"",flowletContext,e);
        }
 finally {
          enqueueEntry();
          inflight.decrementAndGet();
        }
      }
    }
    private void enqueueEntry(){
      if (!flowletContext.isAsyncMode()) {
        processQueue.offer(processEntry.resetRetry());
      }
    }
  }
;
}"
7796,"@Override public void onFailure(Object inputObject,InputContext inputContext,FailureReason reason,InputAcknowledger inputAcknowledger){
  LOG.warn(""String_Node_Str"",flowletContext,reason.getMessage(),input,reason.getCause());
  FailurePolicy failurePolicy;
  try {
    flowletContext.getSystemMetrics().gauge(""String_Node_Str"",1);
    failurePolicy=txCallback.onFailure(inputObject,inputContext,reason);
    if (failurePolicy == null) {
      failurePolicy=FailurePolicy.RETRY;
      LOG.info(""String_Node_Str"",failurePolicy);
    }
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",flowletContext,t);
    failurePolicy=FailurePolicy.RETRY;
  }
  if (input.getRetry() >= processEntry.getProcessSpec().getProcessMethod().getMaxRetries()) {
    LOG.info(""String_Node_Str"",input);
    failurePolicy=FailurePolicy.IGNORE;
  }
  if (failurePolicy == FailurePolicy.RETRY) {
    FlowletProcessEntry retryEntry=processEntry.isRetry() ? processEntry : FlowletProcessEntry.create(processEntry.getProcessSpec(),new ProcessSpecification<T>(new SingleItemQueueReader(input),processEntry.getProcessSpec().getInputDecoder(),processEntry.getProcessSpec().getProcessMethod()));
    processQueue.offer(retryEntry);
  }
 else   if (failurePolicy == FailurePolicy.IGNORE) {
    try {
      flowletContext.getSystemMetrics().gauge(""String_Node_Str"",1);
      inputAcknowledger.ack();
    }
 catch (    OperationException e) {
      LOG.error(""String_Node_Str"",flowletContext,e);
    }
 finally {
      enqueueEntry();
      inflight.decrementAndGet();
    }
  }
}","@Override public void onFailure(Object inputObject,InputContext inputContext,FailureReason reason,InputAcknowledger inputAcknowledger){
  LOG.warn(""String_Node_Str"",flowletContext,reason.getMessage(),input,reason.getCause());
  FailurePolicy failurePolicy;
  try {
    flowletContext.getSystemMetrics().gauge(""String_Node_Str"",1);
    failurePolicy=txCallback.onFailure(inputObject,inputContext,reason);
    if (failurePolicy == null) {
      failurePolicy=FailurePolicy.RETRY;
      LOG.info(""String_Node_Str"",failurePolicy);
    }
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",flowletContext,t);
    failurePolicy=FailurePolicy.RETRY;
  }
  if (input.getRetry() >= processEntry.getProcessSpec().getProcessMethod().getMaxRetries()) {
    LOG.info(""String_Node_Str"",input);
    failurePolicy=FailurePolicy.IGNORE;
  }
  if (failurePolicy == FailurePolicy.RETRY) {
    FlowletProcessEntry retryEntry=processEntry.isRetry() ? processEntry : FlowletProcessEntry.create(processEntry.getProcessSpec(),new ProcessSpecification<T>(new SingleItemQueueReader(input),processEntry.getProcessSpec().getInputDecoder(),processEntry.getProcessSpec().getProcessMethod()));
    processQueue.offer(retryEntry);
  }
 else   if (failurePolicy == FailurePolicy.IGNORE) {
    try {
      flowletContext.getSystemMetrics().gauge(""String_Node_Str"",input.size());
      inputAcknowledger.ack();
    }
 catch (    OperationException e) {
      LOG.error(""String_Node_Str"",flowletContext,e);
    }
 finally {
      enqueueEntry();
      inflight.decrementAndGet();
    }
  }
}"
7797,"/** 
 * Wait for all inflight processes in the queue.
 * @param processQueue list of inflight processes
 */
@SuppressWarnings(""String_Node_Str"") private void waitForInflight(BlockingQueue<FlowletProcessEntry<?>> processQueue){
  List<FlowletProcessEntry> processList=Lists.newArrayListWithCapacity(processQueue.size());
  boolean hasRetry;
  do {
    hasRetry=false;
    processList.clear();
    processQueue.drainTo(processList);
    for (    FlowletProcessEntry<?> entry : processList) {
      if (!entry.isRetry()) {
        processQueue.offer(entry);
        continue;
      }
      hasRetry=true;
      ProcessMethod processMethod=entry.getProcessSpec().getProcessMethod();
      TransactionAgent txAgent=dataFabricFacade.createAndUpdateTransactionAgentProxy();
      try {
        txAgent.start();
        InputDatum input=entry.getProcessSpec().getQueueReader().dequeue();
        flowletContext.getSystemMetrics().gauge(""String_Node_Str"",1);
        ProcessMethod.ProcessResult result=processMethod.invoke(input,wrapInputDecoder(input,entry.getProcessSpec().getInputDecoder()));
        postProcess(transactionExecutor,processMethodCallback(processQueue,entry,input),txAgent,input,result);
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",flowletContext,t);
        try {
          txAgent.abort();
        }
 catch (        OperationException e) {
          LOG.error(""String_Node_Str"",flowletContext,e);
        }
      }
    }
  }
 while (hasRetry || inflight.get() != 0);
}","/** 
 * Wait for all inflight processes in the queue.
 * @param processQueue list of inflight processes
 */
@SuppressWarnings(""String_Node_Str"") private void waitForInflight(BlockingQueue<FlowletProcessEntry<?>> processQueue){
  List<FlowletProcessEntry> processList=Lists.newArrayListWithCapacity(processQueue.size());
  boolean hasRetry;
  do {
    hasRetry=false;
    processList.clear();
    processQueue.drainTo(processList);
    for (    FlowletProcessEntry<?> entry : processList) {
      if (!entry.isRetry()) {
        processQueue.offer(entry);
        continue;
      }
      hasRetry=true;
      ProcessMethod processMethod=entry.getProcessSpec().getProcessMethod();
      TransactionAgent txAgent=dataFabricFacade.createAndUpdateTransactionAgentProxy();
      try {
        txAgent.start();
        InputDatum input=entry.getProcessSpec().getQueueReader().dequeue();
        flowletContext.getSystemMetrics().gauge(""String_Node_Str"",input.size());
        ProcessMethod.ProcessResult result=processMethod.invoke(input,wrapInputDecoder(input,entry.getProcessSpec().getInputDecoder()));
        postProcess(transactionExecutor,processMethodCallback(processQueue,entry,input),txAgent,input,result);
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",flowletContext,t);
        try {
          txAgent.abort();
        }
 catch (        OperationException e) {
          LOG.error(""String_Node_Str"",flowletContext,e);
        }
      }
    }
  }
 while (hasRetry || inflight.get() != 0);
}"
7798,"@Override public void onSuccess(Object object,InputContext inputContext){
  try {
    flowletContext.getSystemMetrics().gauge(""String_Node_Str"",1);
    txCallback.onSuccess(object,inputContext);
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",flowletContext,t);
  }
 finally {
    enqueueEntry();
    inflight.decrementAndGet();
  }
}","@Override public void onSuccess(Object object,InputContext inputContext){
  try {
    flowletContext.getSystemMetrics().gauge(""String_Node_Str"",input.size());
    txCallback.onSuccess(object,inputContext);
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",flowletContext,t);
  }
 finally {
    enqueueEntry();
    inflight.decrementAndGet();
  }
}"
7799,"@Override public ProgramController run(Program program,ProgramOptions options){
  BasicFlowletContext flowletContext=null;
  try {
    String flowletName=options.getName();
    int instanceId=Integer.parseInt(options.getArguments().getOption(""String_Node_Str"",""String_Node_Str""));
    Preconditions.checkArgument(instanceId >= 0,""String_Node_Str"");
    int instanceCount=Integer.parseInt(options.getArguments().getOption(""String_Node_Str"",""String_Node_Str""));
    Preconditions.checkArgument(instanceCount > 0,""String_Node_Str"");
    String runIdOption=options.getArguments().getOption(""String_Node_Str"");
    Preconditions.checkNotNull(runIdOption,""String_Node_Str"");
    RunId runId=RunIds.fromString(runIdOption);
    ApplicationSpecification appSpec=program.getSpecification();
    Preconditions.checkNotNull(appSpec,""String_Node_Str"");
    Type processorType=program.getProcessorType();
    Preconditions.checkNotNull(processorType,""String_Node_Str"");
    Preconditions.checkArgument(processorType == Type.FLOW,""String_Node_Str"");
    String processorName=program.getProgramName();
    Preconditions.checkNotNull(processorName,""String_Node_Str"");
    FlowSpecification flowSpec=appSpec.getFlows().get(processorName);
    FlowletDefinition flowletDef=flowSpec.getFlowlets().get(flowletName);
    Preconditions.checkNotNull(flowletDef,""String_Node_Str"",flowletName);
    Class<?> clz=Class.forName(flowletDef.getFlowletSpec().getClassName(),true,program.getMainClass().getClassLoader());
    Preconditions.checkArgument(Flowlet.class.isAssignableFrom(clz),""String_Node_Str"",clz);
    Class<? extends Flowlet> flowletClass=(Class<? extends Flowlet>)clz;
    DataFabricFacade dataFabricFacade=dataFabricFacadeFactory.createDataFabricFacadeFactory(program);
    DataSetContext dataSetContext=dataFabricFacade.getDataSetContext();
    flowletContext=new BasicFlowletContext(program,flowletName,instanceId,runId,instanceCount,DataSets.createDataSets(dataSetContext,flowletDef.getDatasets()),options.getUserArguments(),flowletDef.getFlowletSpec(),false,metricsCollectionService);
    if (dataSetContext instanceof DataSetInstantiationBase) {
      ((DataSetInstantiationBase)dataSetContext).setMetricsCollector(metricsCollectionService,flowletContext.getSystemMetrics());
    }
    Table<Node,String,Set<QueueSpecification>> queueSpecs=new SimpleQueueSpecificationGenerator(Id.Account.from(program.getAccountId())).create(flowSpec);
    Flowlet flowlet=new InstantiatorFactory(false).get(TypeToken.of(flowletClass)).create();
    TypeToken<? extends Flowlet> flowletType=TypeToken.of(flowletClass);
    flowletContext.injectFields(flowlet);
    injectFields(flowlet,flowletType,outputEmitterFactory(flowletContext,flowletName,dataFabricFacade,queueSpecs));
    ImmutableList.Builder<QueueConsumerSupplier> queueConsumerSupplierBuilder=ImmutableList.builder();
    Collection<ProcessSpecification> processSpecs=createProcessSpecification(flowletContext,flowletType,processMethodFactory(flowlet),processSpecificationFactory(queueReaderFactory,dataFabricFacade,flowletName,queueSpecs,queueConsumerSupplierBuilder,createSchemaCache(program)),Lists.<ProcessSpecification>newLinkedList());
    List<QueueConsumerSupplier> queueConsumerSuppliers=queueConsumerSupplierBuilder.build();
    FlowletProcessDriver driver=new FlowletProcessDriver(flowlet,flowletContext,processSpecs,createCallback(flowlet,flowletDef.getFlowletSpec()),dataFabricFacade);
    LOG.info(""String_Node_Str"" + flowletContext);
    driver.start();
    LOG.info(""String_Node_Str"" + flowletContext);
    return new FlowletProgramController(program.getProgramName(),flowletName,flowletContext,driver,queueConsumerSuppliers);
  }
 catch (  Exception e) {
    if (flowletContext != null) {
      flowletContext.close();
    }
    throw Throwables.propagate(e);
  }
}","@Override public ProgramController run(Program program,ProgramOptions options){
  BasicFlowletContext flowletContext=null;
  try {
    String flowletName=options.getName();
    int instanceId=Integer.parseInt(options.getArguments().getOption(""String_Node_Str"",""String_Node_Str""));
    Preconditions.checkArgument(instanceId >= 0,""String_Node_Str"");
    int instanceCount=Integer.parseInt(options.getArguments().getOption(""String_Node_Str"",""String_Node_Str""));
    Preconditions.checkArgument(instanceCount > 0,""String_Node_Str"");
    String runIdOption=options.getArguments().getOption(""String_Node_Str"");
    Preconditions.checkNotNull(runIdOption,""String_Node_Str"");
    RunId runId=RunIds.fromString(runIdOption);
    ApplicationSpecification appSpec=program.getSpecification();
    Preconditions.checkNotNull(appSpec,""String_Node_Str"");
    Type processorType=program.getProcessorType();
    Preconditions.checkNotNull(processorType,""String_Node_Str"");
    Preconditions.checkArgument(processorType == Type.FLOW,""String_Node_Str"");
    String processorName=program.getProgramName();
    Preconditions.checkNotNull(processorName,""String_Node_Str"");
    FlowSpecification flowSpec=appSpec.getFlows().get(processorName);
    FlowletDefinition flowletDef=flowSpec.getFlowlets().get(flowletName);
    Preconditions.checkNotNull(flowletDef,""String_Node_Str"",flowletName);
    Class<?> clz=Class.forName(flowletDef.getFlowletSpec().getClassName(),true,program.getMainClass().getClassLoader());
    Preconditions.checkArgument(Flowlet.class.isAssignableFrom(clz),""String_Node_Str"",clz);
    Class<? extends Flowlet> flowletClass=(Class<? extends Flowlet>)clz;
    DataFabricFacade dataFabricFacade=dataFabricFacadeFactory.createDataFabricFacadeFactory(program);
    DataSetContext dataSetContext=dataFabricFacade.getDataSetContext();
    flowletContext=new BasicFlowletContext(program,flowletName,instanceId,runId,instanceCount,DataSets.createDataSets(dataSetContext,flowletDef.getDatasets()),options.getUserArguments(),flowletDef.getFlowletSpec(),false,metricsCollectionService);
    if (dataSetContext instanceof DataSetInstantiationBase) {
      ((DataSetInstantiationBase)dataSetContext).setMetricsCollector(metricsCollectionService,flowletContext.getSystemMetrics());
    }
    Table<Node,String,Set<QueueSpecification>> queueSpecs=new SimpleQueueSpecificationGenerator(Id.Account.from(program.getAccountId())).create(flowSpec);
    Flowlet flowlet=new InstantiatorFactory(false).get(TypeToken.of(flowletClass)).create();
    TypeToken<? extends Flowlet> flowletType=TypeToken.of(flowletClass);
    flowletContext.injectFields(flowlet);
    injectFields(flowlet,flowletType,outputEmitterFactory(flowletContext,flowletName,dataFabricFacade,queueSpecs));
    ImmutableList.Builder<QueueConsumerSupplier> queueConsumerSupplierBuilder=ImmutableList.builder();
    Collection<ProcessSpecification> processSpecs=createProcessSpecification(flowletContext,flowletType,processMethodFactory(flowlet),processSpecificationFactory(dataFabricFacade,queueReaderFactory,flowletName,queueSpecs,queueConsumerSupplierBuilder,createSchemaCache(program)),Lists.<ProcessSpecification>newLinkedList());
    List<QueueConsumerSupplier> queueConsumerSuppliers=queueConsumerSupplierBuilder.build();
    FlowletProcessDriver driver=new FlowletProcessDriver(flowlet,flowletContext,processSpecs,createCallback(flowlet,flowletDef.getFlowletSpec()),dataFabricFacade);
    LOG.info(""String_Node_Str"" + flowletContext);
    driver.start();
    LOG.info(""String_Node_Str"" + flowletContext);
    return new FlowletProgramController(program.getProgramName(),flowletName,flowletContext,driver,queueConsumerSuppliers);
  }
 catch (  Exception e) {
    if (flowletContext != null) {
      flowletContext.close();
    }
    throw Throwables.propagate(e);
  }
}"
7800,"private ProcessSpecificationFactory processSpecificationFactory(final QueueReaderFactory queueReaderFactory,final QueueClientFactory queueClientFactory,final String flowletName,final Table<Node,String,Set<QueueSpecification>> queueSpecs,final ImmutableList.Builder<QueueConsumerSupplier> queueConsumerSupplierBuilder,final SchemaCache schemaCache){
  return new ProcessSpecificationFactory(){
    @Override public <T>ProcessSpecification create(    Set<String> inputNames,    Schema schema,    TypeToken<T> dataType,    ProcessMethod method,    ConsumerConfig consumerConfig,    int batchSize){
      List<QueueReader> queueReaders=Lists.newLinkedList();
      for (      Map.Entry<Node,Set<QueueSpecification>> entry : queueSpecs.column(flowletName).entrySet()) {
        for (        QueueSpecification queueSpec : entry.getValue()) {
          final QueueName queueName=queueSpec.getQueueName();
          if (queueSpec.getInputSchema().equals(schema) && (inputNames.contains(queueName.getSimpleName()) || inputNames.contains(FlowletDefinition.ANY_INPUT))) {
            int numGroups=(entry.getKey().getType() == FlowletConnection.Type.STREAM) ? -1 : getNumGroups(Iterables.concat(queueSpecs.row(entry.getKey()).values()),queueName);
            QueueConsumerSupplier consumerSupplier=new QueueConsumerSupplier(queueClientFactory,queueName,consumerConfig,numGroups);
            queueConsumerSupplierBuilder.add(consumerSupplier);
            queueReaders.add(queueReaderFactory.create(consumerSupplier,batchSize));
          }
        }
      }
      if (!inputNames.isEmpty() && queueReaders.isEmpty()) {
        return null;
      }
      return new ProcessSpecification<T>(new RoundRobinQueueReader(queueReaders),createInputDatumDecoder(dataType,schema,schemaCache),method);
    }
  }
;
}","private ProcessSpecificationFactory processSpecificationFactory(final DataFabricFacade dataFabricFacade,final QueueReaderFactory queueReaderFactory,final String flowletName,final Table<Node,String,Set<QueueSpecification>> queueSpecs,final ImmutableList.Builder<QueueConsumerSupplier> queueConsumerSupplierBuilder,final SchemaCache schemaCache){
  return new ProcessSpecificationFactory(){
    @Override public <T>ProcessSpecification create(    Set<String> inputNames,    Schema schema,    TypeToken<T> dataType,    ProcessMethod method,    ConsumerConfig consumerConfig,    int batchSize){
      List<QueueReader> queueReaders=Lists.newLinkedList();
      for (      Map.Entry<Node,Set<QueueSpecification>> entry : queueSpecs.column(flowletName).entrySet()) {
        for (        QueueSpecification queueSpec : entry.getValue()) {
          final QueueName queueName=queueSpec.getQueueName();
          if (queueSpec.getInputSchema().equals(schema) && (inputNames.contains(queueName.getSimpleName()) || inputNames.contains(FlowletDefinition.ANY_INPUT))) {
            int numGroups=(entry.getKey().getType() == FlowletConnection.Type.STREAM) ? -1 : getNumGroups(Iterables.concat(queueSpecs.row(entry.getKey()).values()),queueName);
            QueueConsumerSupplier consumerSupplier=new QueueConsumerSupplier(dataFabricFacade,queueName,consumerConfig,numGroups);
            queueConsumerSupplierBuilder.add(consumerSupplier);
            queueReaders.add(queueReaderFactory.create(consumerSupplier,batchSize));
          }
        }
      }
      if (!inputNames.isEmpty() && queueReaders.isEmpty()) {
        return null;
      }
      return new ProcessSpecification<T>(new RoundRobinQueueReader(queueReaders),createInputDatumDecoder(dataType,schema,schemaCache),method);
    }
  }
;
}"
7801,"private Queue2Consumer createConsumer(Queue2Consumer oldConsumer){
  try {
    if (oldConsumer != null && oldConsumer instanceof Closeable) {
      ((Closeable)oldConsumer).close();
    }
  }
 catch (  IOException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  try {
    return clientFactory.createConsumer(queueName,consumerConfig,numGroups);
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}","private Queue2Consumer createConsumer(Queue2Consumer oldConsumer){
  try {
    if (oldConsumer != null && oldConsumer instanceof Closeable) {
      TransactionAgent txAgent=dataFabricFacade.createTransactionAgent();
      txAgent.start();
      try {
        ((Closeable)oldConsumer).close();
        txAgent.finish();
      }
 catch (      OperationException e) {
        LOG.warn(""String_Node_Str"");
        txAgent.abort();
      }
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
  }
  try {
    return dataFabricFacade.createConsumer(queueName,consumerConfig,numGroups);
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}"
7802,"QueueConsumerSupplier(QueueClientFactory clientFactory,QueueName queueName,ConsumerConfig consumerConfig,int numGroups){
  this.clientFactory=clientFactory;
  this.queueName=queueName;
  this.consumerConfig=consumerConfig;
  this.numGroups=numGroups;
  this.consumer=createConsumer(null);
}","QueueConsumerSupplier(DataFabricFacade dataFabricFacade,QueueName queueName,ConsumerConfig consumerConfig,int numGroups){
  this.dataFabricFacade=dataFabricFacade;
  this.queueName=queueName;
  this.consumerConfig=consumerConfig;
  this.numGroups=numGroups;
  this.consumer=createConsumer(null);
}"
7803,"@Override public boolean commitTx() throws Exception {
  if (consumingEntries.isEmpty()) {
    return true;
  }
  byte[] stateContent=encodeStateColumn(ConsumerEntryState.PROCESSED);
  updateState(consumingEntries.keySet(),stateColumnName,stateContent);
  committed=true;
  return true;
}","@Override public boolean commitTx() throws Exception {
  if (consumingEntries.isEmpty()) {
    return true;
  }
  byte[] stateContent=encodeStateColumn(ConsumerEntryState.PROCESSED);
  updateState(consumingEntries.keySet(),stateColumnName,stateContent);
  commitCount+=consumingEntries.size();
  committed=true;
  return true;
}"
7804,"@Override public boolean rollbackTx() throws Exception {
  if (consumingEntries.isEmpty()) {
    return true;
  }
  entryCache.putAll(consumingEntries);
  if (!committed) {
    return true;
  }
  if (consumerConfig.getDequeueStrategy() == DequeueStrategy.FIFO && consumerConfig.getGroupSize() > 1) {
    byte[] stateContent=encodeStateColumn(ConsumerEntryState.CLAIMED);
    updateState(consumingEntries.keySet(),stateColumnName,stateContent);
  }
 else {
    undoState(consumingEntries.keySet(),stateColumnName);
  }
  return true;
}","@Override public boolean rollbackTx() throws Exception {
  if (consumingEntries.isEmpty()) {
    return true;
  }
  entryCache.putAll(consumingEntries);
  if (!committed) {
    return true;
  }
  commitCount-=consumingEntries.size();
  if (consumerConfig.getDequeueStrategy() == DequeueStrategy.FIFO && consumerConfig.getGroupSize() > 1) {
    byte[] stateContent=encodeStateColumn(ConsumerEntryState.CLAIMED);
    updateState(consumingEntries.keySet(),stateColumnName,stateContent);
  }
 else {
    undoState(consumingEntries.keySet(),stateColumnName);
  }
  return true;
}"
7805,"@Override public void postTxCommit(){
  commitCount++;
  if (commitCount >= EVICTION_LIMIT) {
    commitCount=0;
    queueEvictor.evict(transaction);
  }
}","@Override public void postTxCommit(){
  if (commitCount >= EVICTION_LIMIT) {
    commitCount=0;
    queueEvictor.evict(transaction);
  }
}"
7806,"public static byte[] getQueueRowPrefix(QueueName queueName){
  byte[] bytes=Arrays.copyOf(queueName.toBytes(),queueName.toBytes().length);
  int i=0;
  int j=bytes.length - 1;
  while (i < j) {
    byte tmp=bytes[i];
    bytes[i]=bytes[j];
    bytes[j]=tmp;
    i++;
    j--;
  }
  return bytes;
}","/** 
 * Returns a byte array representing prefix of a queue. The prefix is formed by first two bytes of MD5 of the queue name followed by the queue name.
 */
public static byte[] getQueueRowPrefix(QueueName queueName){
  byte[] queueBytes=queueName.toBytes();
  byte[] bytes=new byte[queueBytes.length + 2];
  Hashing.md5().hashBytes(queueBytes).writeBytesTo(bytes,0,2);
  System.arraycopy(queueBytes,0,bytes,2,queueBytes.length);
  return bytes;
}"
7807,"/** 
 * Creates a jar files container coprocessors that are using by queue.
 * @param fileSystem
 * @param jarDir
 * @return The Path of the jar file on the file system.
 * @throws IOException
 */
private Path createCoProcessorJar(FileSystem fileSystem,Path jarDir) throws IOException {
  final Set<String> acceptClasses=ImmutableSet.of(HBaseQueueEvictionEndpoint.class.getName(),HBaseQueueEvictionProtocol.class.getName());
  File jarFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
  try {
    final JarOutputStream jarOutput=new JarOutputStream(new FileOutputStream(jarFile));
    try {
      Dependencies.findClassDependencies(HBaseQueueEvictionEndpoint.class.getClassLoader(),new Dependencies.ClassAcceptor(){
        @Override public boolean accept(        String className,        final URL classUrl,        URL classPathUrl){
          if (acceptClasses.contains(className)) {
            try {
              jarOutput.putNextEntry(new JarEntry(className.replace('.',File.separatorChar) + ""String_Node_Str""));
              ByteStreams.copy(new InputSupplier<InputStream>(){
                @Override public InputStream getInput() throws IOException {
                  return classUrl.openStream();
                }
              }
,jarOutput);
              return true;
            }
 catch (            IOException e) {
              throw Throwables.propagate(e);
            }
          }
          return false;
        }
      }
,HBaseQueueEvictionEndpoint.class.getName());
    }
  finally {
      jarOutput.close();
    }
    Path targetPath=new Path(jarDir,Files.hash(jarFile,Hashing.md5()).toString() + ""String_Node_Str"");
    if (fileSystem.exists(targetPath) && fileSystem.getFileStatus(targetPath).getLen() == jarFile.length()) {
      return targetPath;
    }
    if (!fileSystem.mkdirs(jarDir)) {
      System.out.println(""String_Node_Str"");
    }
    fileSystem.copyFromLocalFile(false,true,new Path(jarFile.toURI()),targetPath);
    return targetPath;
  }
  finally {
    jarFile.delete();
  }
}","/** 
 * Creates a jar files container coprocessors that are using by queue.
 * @param fileSystem
 * @param jarDir
 * @return The Path of the jar file on the file system.
 * @throws IOException
 */
private Path createCoProcessorJar(FileSystem fileSystem,Path jarDir) throws IOException {
  final Hasher hasher=Hashing.md5().newHasher();
  final byte[] buffer=new byte[COPY_BUFFER_SIZE];
  File jarFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
  try {
    final JarOutputStream jarOutput=new JarOutputStream(new FileOutputStream(jarFile));
    try {
      Dependencies.findClassDependencies(HBaseQueueEvictionEndpoint.class.getClassLoader(),new Dependencies.ClassAcceptor(){
        @Override public boolean accept(        String className,        final URL classUrl,        URL classPathUrl){
          if (className.startsWith(""String_Node_Str"")) {
            try {
              jarOutput.putNextEntry(new JarEntry(className.replace('.',File.separatorChar) + ""String_Node_Str""));
              InputStream inputStream=classUrl.openStream();
              try {
                int len=inputStream.read(buffer);
                while (len >= 0) {
                  hasher.putBytes(buffer,0,len);
                  jarOutput.write(buffer,0,len);
                  len=inputStream.read(buffer);
                }
              }
  finally {
                inputStream.close();
              }
              return true;
            }
 catch (            IOException e) {
              throw Throwables.propagate(e);
            }
          }
          return false;
        }
      }
,HBaseQueueEvictionEndpoint.class.getName());
    }
  finally {
      jarOutput.close();
    }
    Path targetPath=new Path(jarDir,""String_Node_Str"" + hasher.hash().toString() + ""String_Node_Str"");
    if (fileSystem.exists(targetPath) && fileSystem.getFileStatus(targetPath).getLen() == jarFile.length()) {
      return targetPath;
    }
    if (!fileSystem.mkdirs(jarDir)) {
      System.out.println(""String_Node_Str"");
    }
    fileSystem.copyFromLocalFile(false,true,new Path(jarFile.toURI()),targetPath);
    return targetPath;
  }
  finally {
    jarFile.delete();
  }
}"
7808,"@Override public boolean accept(String className,final URL classUrl,URL classPathUrl){
  if (acceptClasses.contains(className)) {
    try {
      jarOutput.putNextEntry(new JarEntry(className.replace('.',File.separatorChar) + ""String_Node_Str""));
      ByteStreams.copy(new InputSupplier<InputStream>(){
        @Override public InputStream getInput() throws IOException {
          return classUrl.openStream();
        }
      }
,jarOutput);
      return true;
    }
 catch (    IOException e) {
      throw Throwables.propagate(e);
    }
  }
  return false;
}","@Override public boolean accept(String className,final URL classUrl,URL classPathUrl){
  if (className.startsWith(""String_Node_Str"")) {
    try {
      jarOutput.putNextEntry(new JarEntry(className.replace('.',File.separatorChar) + ""String_Node_Str""));
      InputStream inputStream=classUrl.openStream();
      try {
        int len=inputStream.read(buffer);
        while (len >= 0) {
          hasher.putBytes(buffer,0,len);
          jarOutput.write(buffer,0,len);
          len=inputStream.read(buffer);
        }
      }
  finally {
        inputStream.close();
      }
      return true;
    }
 catch (    IOException e) {
      throw Throwables.propagate(e);
    }
  }
  return false;
}"
7809,"private boolean isCommittedProcessed(KeyValue stateColumn,long readPointer,long[] excludes){
  long writePointer=Bytes.toLong(stateColumn.getBuffer(),stateColumn.getValueOffset(),Longs.BYTES);
  if (writePointer > readPointer || Arrays.binarySearch(excludes,writePointer) >= 0) {
    return false;
  }
  byte state=stateColumn.getBuffer()[stateColumn.getValueOffset() + Longs.BYTES + Ints.BYTES];
  return state == ConsumerEntryState.PROCESSED.getState();
}","private boolean isCommittedProcessed(KeyValue stateColumn,long readPointer,long[] excludes){
  long writePointer=Bytes.toLong(stateColumn.getBuffer(),stateColumn.getValueOffset(),LONG_SIZE);
  if (writePointer > readPointer || Arrays.binarySearch(excludes,writePointer) >= 0) {
    return false;
  }
  byte state=stateColumn.getBuffer()[stateColumn.getValueOffset() + LONG_SIZE + INT_SIZE];
  return state == ConsumerEntryState.PROCESSED.getState();
}"
7810,"/** 
 * Call the appropriate handler for handling the httprequest. 404 if path is not found. 405 if path is found but httpMethod does not match what's configured.
 * @param request instance of {@code HttpRequest}
 * @param responder instance of {@code HttpResponder} to handle the request.
 */
public void handle(HttpRequest request,HttpResponder responder){
  Map<String,String> groupValues=Maps.newHashMap();
  List<HttpResourceModel> resourceModels=patternRouter.getDestinations(request.getUri(),groupValues);
  HttpResourceModel httpResourceModel=getMatchedResourceModel(resourceModels,request.getMethod());
  if (httpResourceModel != null) {
    httpResourceModel.handle(request,responder,groupValues);
  }
 else   if (resourceModels.size() > 0) {
    responder.sendError(HttpResponseStatus.METHOD_NOT_ALLOWED,String.format(""String_Node_Str"",request.getUri()));
  }
 else {
    responder.sendError(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",request.getUri()));
  }
}","/** 
 * Call the appropriate handler for handling the httprequest. 404 if path is not found. 405 if path is found but httpMethod does not match what's configured.
 * @param request instance of {@code HttpRequest}
 * @param responder instance of {@code HttpResponder} to handle the request.
 */
public void handle(HttpRequest request,HttpResponder responder){
  Map<String,String> groupValues=Maps.newHashMap();
  String path=request.getUri().split(""String_Node_Str"")[0];
  List<HttpResourceModel> resourceModels=patternRouter.getDestinations(path,groupValues);
  HttpResourceModel httpResourceModel=getMatchedResourceModel(resourceModels,request.getMethod());
  try {
    if (httpResourceModel != null) {
      httpResourceModel.handle(request,responder,groupValues);
    }
 else     if (resourceModels.size() > 0) {
      responder.sendError(HttpResponseStatus.METHOD_NOT_ALLOWED,String.format(""String_Node_Str"",request.getUri()));
    }
 else {
      responder.sendError(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",request.getUri()));
    }
  }
 catch (  Throwable t) {
    responder.sendError(HttpResponseStatus.INTERNAL_SERVER_ERROR,String.format(""String_Node_Str"",t.getMessage()));
  }
}"
7811,"/** 
 * Handle http Request.
 * @param request  HttpRequest to be handled.
 * @param responder HttpResponder to write the response.
 * @param groupValues Values needed for the invocation.
 */
public void handle(HttpRequest request,HttpResponder responder,Map<String,String> groupValues){
  try {
    if (httpMethods.contains(request.getMethod())) {
      Object[] args=new Object[method.getParameterTypes().length];
      int index=0;
      args[index]=request;
      index++;
      args[index]=responder;
      if (method.getParameterTypes().length > 2) {
        Class<?>[] parameterTypes=method.getParameterTypes();
        for (        Map.Entry<String,String> entry : groupValues.entrySet()) {
          index++;
          args[index]=ConvertUtils.convert(entry.getValue(),parameterTypes[index]);
        }
      }
      method.invoke(handler,args);
    }
 else {
      responder.sendError(HttpResponseStatus.METHOD_NOT_ALLOWED,String.format(""String_Node_Str"",request.getUri()));
    }
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",request.getUri(),e,e);
    responder.sendError(HttpResponseStatus.INTERNAL_SERVER_ERROR,String.format(""String_Node_Str"",request.getUri()));
  }
}","/** 
 * Handle http Request.
 * @param request  HttpRequest to be handled.
 * @param responder HttpResponder to write the response.
 * @param groupValues Values needed for the invocation.
 */
public void handle(HttpRequest request,HttpResponder responder,Map<String,String> groupValues){
  try {
    if (httpMethods.contains(request.getMethod())) {
      Object[] args=new Object[method.getParameterTypes().length];
      int parameterIndex=0;
      args[parameterIndex]=request;
      parameterIndex++;
      args[parameterIndex]=responder;
      if (method.getParameterTypes().length > 2) {
        Class<?>[] parameterTypes=method.getParameterTypes();
        for (        Annotation[] annotations : method.getParameterAnnotations()) {
          for (          Annotation annotation : annotations) {
            if (annotation.annotationType().isAssignableFrom(PathParam.class)) {
              PathParam param=(PathParam)annotation;
              String value=groupValues.get(param.value());
              Preconditions.checkArgument(value != null,""String_Node_Str"",param.value());
              parameterIndex++;
              args[parameterIndex]=ConvertUtils.convert(value,parameterTypes[parameterIndex]);
            }
          }
        }
        Preconditions.checkArgument(method.getParameterTypes().length == parameterIndex + 1,""String_Node_Str"",method.getName());
      }
      method.invoke(handler,args);
    }
 else {
      responder.sendError(HttpResponseStatus.METHOD_NOT_ALLOWED,String.format(""String_Node_Str"",request.getUri()));
    }
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",request.getUri(),e,e);
    responder.sendError(HttpResponseStatus.INTERNAL_SERVER_ERROR,String.format(""String_Node_Str"",request.getUri()));
  }
}"
7812,"/** 
 * Call the appropriate handler for handling the httprequest. 404 if path is not found. 405 if path is found but httpMethod does not match what's configured.
 * @param request instance of {@code HttpRequest}
 * @param responder instance of {@code HttpResponder} to handle the request.
 */
public void handle(HttpRequest request,HttpResponder responder){
  Map<String,String> groupValues=Maps.newHashMap();
  List<HttpResourceModel> resourceModels=patternRouter.getDestinations(request.getUri(),groupValues);
  HttpResourceModel httpResourceModel=getMatchedResourceModel(resourceModels,request.getMethod());
  if (httpResourceModel != null) {
    httpResourceModel.handle(request,responder,groupValues);
  }
 else   if (resourceModels.size() > 0) {
    responder.sendError(HttpResponseStatus.METHOD_NOT_ALLOWED,String.format(""String_Node_Str"",request.getUri()));
  }
 else {
    responder.sendError(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",request.getUri()));
  }
}","/** 
 * Call the appropriate handler for handling the httprequest. 404 if path is not found. 405 if path is found but httpMethod does not match what's configured.
 * @param request instance of {@code HttpRequest}
 * @param responder instance of {@code HttpResponder} to handle the request.
 */
public void handle(HttpRequest request,HttpResponder responder){
  Map<String,String> groupValues=Maps.newHashMap();
  String path;
  try {
    URI uri=new URI(request.getUri());
    path=uri.getPath();
  }
 catch (  URISyntaxException e) {
    responder.sendError(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",request.getUri()));
    return;
  }
  List<HttpResourceModel> resourceModels=patternRouter.getDestinations(path,groupValues);
  HttpResourceModel httpResourceModel=getMatchedResourceModel(resourceModels,request.getMethod());
  if (httpResourceModel != null) {
    httpResourceModel.handle(request,responder,groupValues);
  }
 else   if (resourceModels.size() > 0) {
    responder.sendError(HttpResponseStatus.METHOD_NOT_ALLOWED,String.format(""String_Node_Str"",request.getUri()));
  }
 else {
    responder.sendError(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",request.getUri()));
  }
}"
7813,"@Override public TimeSeriesTable load(Integer key) throws Exception {
  return metricsTableFactory.createTimeSeries(MetricsScope.REACTOR.name(),key);
}","@Override public TimeSeriesTable load(Integer key) throws Exception {
  return metricsTableFactory.createTimeSeries(scope.name(),key);
}"
7814,"@Inject public BatchMetricsHandler(final MetricsTableFactory metricsTableFactory){
  this.metricsTableCaches=Maps.newHashMap();
  this.aggregatesTables=Maps.newHashMap();
  for (  MetricsScope scope : MetricsScope.values()) {
    LoadingCache<Integer,TimeSeriesTable> cache=CacheBuilder.newBuilder().build(new CacheLoader<Integer,TimeSeriesTable>(){
      @Override public TimeSeriesTable load(      Integer key) throws Exception {
        return metricsTableFactory.createTimeSeries(MetricsScope.REACTOR.name(),key);
      }
    }
);
    this.metricsTableCaches.put(scope,cache);
    this.aggregatesTables.put(scope,metricsTableFactory.createAggregates(scope.name()));
  }
}","@Inject public BatchMetricsHandler(final MetricsTableFactory metricsTableFactory){
  this.metricsTableCaches=Maps.newHashMap();
  this.aggregatesTables=Maps.newHashMap();
  for (  final MetricsScope scope : MetricsScope.values()) {
    LoadingCache<Integer,TimeSeriesTable> cache=CacheBuilder.newBuilder().build(new CacheLoader<Integer,TimeSeriesTable>(){
      @Override public TimeSeriesTable load(      Integer key) throws Exception {
        return metricsTableFactory.createTimeSeries(scope.name(),key);
      }
    }
);
    this.metricsTableCaches.put(scope,cache);
    this.aggregatesTables.put(scope,metricsTableFactory.createAggregates(scope.name()));
  }
}"
7815,"private JsonArray getMetrics(HttpRequest request,String contextPrefix){
  Map<String,List<String>> queryParams=new QueryStringDecoder(request.getUri()).getParameters();
  List<String> prefixEntity=queryParams.get(""String_Node_Str"");
  String metricPrefix=(prefixEntity == null) ? ""String_Node_Str"" : prefixEntity.get(0);
  Map<String,ContextNode> metricContextsMap=Maps.newHashMap();
  AggregatesScanner scanner=this.aggregatesTable.scan(contextPrefix,metricPrefix);
  while (scanner.hasNext()) {
    AggregatesScanResult result=scanner.next();
    addContext(result.getContext(),result.getMetric(),metricContextsMap);
  }
  JsonArray output=new JsonArray();
  List<String> sortedMetrics=Lists.newArrayList(metricContextsMap.keySet());
  Collections.sort(sortedMetrics);
  for (  String metric : sortedMetrics) {
    JsonObject metricNode=new JsonObject();
    metricNode.addProperty(""String_Node_Str"",metric);
    ContextNode metricContexts=metricContextsMap.get(metric);
    JsonObject tmp=metricContexts.toJson();
    metricNode.add(""String_Node_Str"",tmp.getAsJsonArray(""String_Node_Str""));
    output.add(metricNode);
  }
  return output;
}","private JsonArray getMetrics(HttpRequest request,String contextPrefix){
  Map<String,List<String>> queryParams=new QueryStringDecoder(request.getUri()).getParameters();
  List<String> prefixEntity=queryParams.get(""String_Node_Str"");
  String metricPrefix=(prefixEntity == null) ? ""String_Node_Str"" : prefixEntity.get(0);
  Map<String,ContextNode> metricContextsMap=Maps.newHashMap();
  for (  AggregatesTable table : aggregatesTables.values()) {
    AggregatesScanner scanner=table.scan(contextPrefix,metricPrefix);
    while (scanner.hasNext()) {
      AggregatesScanResult result=scanner.next();
      addContext(result.getContext(),result.getMetric(),metricContextsMap);
    }
  }
  JsonArray output=new JsonArray();
  List<String> sortedMetrics=Lists.newArrayList(metricContextsMap.keySet());
  Collections.sort(sortedMetrics);
  for (  String metric : sortedMetrics) {
    JsonObject metricNode=new JsonObject();
    metricNode.addProperty(""String_Node_Str"",metric);
    ContextNode metricContexts=metricContextsMap.get(metric);
    JsonObject tmp=metricContexts.toJson();
    metricNode.add(""String_Node_Str"",tmp.getAsJsonArray(""String_Node_Str""));
    output.add(metricNode);
  }
  return output;
}"
7816,"@Inject public MetricsDiscoveryHandler(final MetricsTableFactory metricsTableFactory){
  this.aggregatesTable=metricsTableFactory.createAggregates(MetricsScope.USER.name());
}","@Inject public MetricsDiscoveryHandler(final MetricsTableFactory metricsTableFactory){
  this.aggregatesTables=Maps.newHashMap();
  for (  MetricsScope scope : scopesToDiscover) {
    aggregatesTables.put(scope,metricsTableFactory.createAggregates(scope.name()));
  }
}"
7817,"@Override public Queue2Consumer createConsumer(int groupSize){
  DequeueStrategy strategy=DequeueStrategy.valueOf(queueInfo.getPartitionerType().name());
  try {
    return queueClientFactory.createConsumer(new ConsumerConfig(groupId,instanceId,groupSize,strategy,queueInfo.getPartitionKey()));
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}","@Override public Queue2Consumer createConsumer(int groupSize){
  DequeueStrategy strategy=DequeueStrategy.valueOf(queueInfo.getPartitionerType().name());
  try {
    return queueClientFactory.createConsumer(queueName,new ConsumerConfig(groupId,instanceId,groupSize,strategy,queueInfo.getPartitionKey()));
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}"
7818,"public static synchronized boolean commit(Transaction tx){
  Set<byte[]> changeSet=committingChangeSets.remove(tx.getWritePointer());
  if (changeSet != null) {
    if (hasConflicts(tx,changeSet)) {
      return false;
    }
    if (committedChangeSets.containsKey(nextWritePointer)) {
      committedChangeSets.get(nextWritePointer).addAll(changeSet);
    }
 else {
      committedChangeSets.put(nextWritePointer,Sets.newHashSet(changeSet));
    }
  }
  makeVisible(tx);
  committedChangeSets.headMap(excludedList.isEmpty() ? Long.MAX_VALUE : excludedList.get(0)).clear();
  return true;
}","public static synchronized boolean commit(Transaction tx){
  Set<byte[]> changeSet=committingChangeSets.remove(tx.getWritePointer());
  if (changeSet != null) {
    if (hasConflicts(tx,changeSet)) {
      return false;
    }
    if (committedChangeSets.containsKey(nextWritePointer)) {
      committedChangeSets.get(nextWritePointer).addAll(changeSet);
    }
 else {
      TreeSet<byte[]> committedChangeSet=Sets.newTreeSet(Bytes.BYTES_COMPARATOR);
      committedChangeSet.addAll(changeSet);
      committedChangeSets.put(nextWritePointer,committedChangeSet);
    }
  }
  makeVisible(tx);
  committedChangeSets.headMap(excludedList.isEmpty() ? Long.MAX_VALUE : excludedList.get(0)).clear();
  return true;
}"
7819,"private static boolean hasConflicts(Transaction tx,Collection<byte[]> changeIds){
  if (changeIds.isEmpty()) {
    return false;
  }
  for (  Map.Entry<Long,Set<byte[]>> changeSet : committedChangeSets.entrySet()) {
    if (changeSet.getKey() > tx.getReadPointer()) {
      if (containsAny(changeSet.getValue(),changeIds)) {
        return true;
      }
    }
  }
  return false;
}","private static boolean hasConflicts(Transaction tx,Collection<byte[]> changeIds){
  if (changeIds.isEmpty()) {
    return false;
  }
  for (  Map.Entry<Long,Set<byte[]>> changeSet : committedChangeSets.entrySet()) {
    if (changeSet.getKey() > tx.getWritePointer()) {
      if (containsAny(changeSet.getValue(),changeIds)) {
        return true;
      }
    }
  }
  return false;
}"
7820,"@Override public List<TLogResult> getLogPrev(String accountId,String applicationId,String entityId,TEntityType entityType,long fromOffset,int maxEvents,String filterStr) throws MetricsServiceException, TException {
  LoggingContext loggingContext=LoggingContextHelper.getLoggingContext(accountId,applicationId,entityId,getEntityType(entityType));
  LogCallback logCallback=new LogCallback(maxEvents,logPattern);
  try {
    Filter filter=FilterParser.parse(filterStr);
    logReader.getLogPrev(loggingContext,fromOffset,maxEvents,filter,logCallback);
    logCallback.await();
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
  return logCallback.getLogResults();
}","@Override public List<TLogResult> getLogPrev(String accountId,String applicationId,String entityId,TEntityType entityType,long fromOffset,int maxEvents,String filterStr) throws MetricsServiceException, TException {
  LoggingContext loggingContext=LoggingContextHelper.getLoggingContext(accountId,applicationId,entityId,getEntityType(entityType));
  LogCallback logCallback=new LogCallback(maxEvents,logPattern);
  try {
    Filter filter=FilterParser.parse(filterStr);
    logReader.getLogPrev(loggingContext,fromOffset,maxEvents,filter,logCallback);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
  return logCallback.getLogResults();
}"
7821,"@Override public List<TLogResult> getLogNext(String accountId,String applicationId,String entityId,TEntityType entityType,long fromOffset,int maxEvents,String filterStr) throws MetricsServiceException, TException {
  LoggingContext loggingContext=LoggingContextHelper.getLoggingContext(accountId,applicationId,entityId,getEntityType(entityType));
  LogCallback logCallback=new LogCallback(maxEvents,logPattern);
  try {
    Filter filter=FilterParser.parse(filterStr);
    logReader.getLogNext(loggingContext,fromOffset,maxEvents,filter,logCallback);
    logCallback.await();
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
  return logCallback.getLogResults();
}","@Override public List<TLogResult> getLogNext(String accountId,String applicationId,String entityId,TEntityType entityType,long fromOffset,int maxEvents,String filterStr) throws MetricsServiceException, TException {
  LoggingContext loggingContext=LoggingContextHelper.getLoggingContext(accountId,applicationId,entityId,getEntityType(entityType));
  LogCallback logCallback=new LogCallback(maxEvents,logPattern);
  try {
    Filter filter=FilterParser.parse(filterStr);
    logReader.getLogNext(loggingContext,fromOffset,maxEvents,filter,logCallback);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
  return logCallback.getLogResults();
}"
7822,"public List<TLogResult> getLogResults(){
  return Collections.unmodifiableList(logResults);
}","public List<TLogResult> getLogResults(){
  try {
    doneLatch.await();
  }
 catch (  InterruptedException e) {
    Thread.currentThread().interrupt();
  }
  return Collections.unmodifiableList(logResults);
}"
7823,"private void rotate(long ts) throws IOException {
  long timeInterval=getMinuteInterval(ts);
  if ((currentTimeInterval != timeInterval && timeInterval % fileRotateIntervalMs == 0) || dataFileWriter == null) {
    close();
    create(timeInterval);
    currentTimeInterval=timeInterval;
    cleanUp();
  }
}","private void rotate(long ts) throws IOException {
  if ((currentTimeInterval != ts && ts % fileRotateIntervalMs == 0) || dataFileWriter == null) {
    close();
    create(ts);
    currentTimeInterval=ts;
    cleanUp();
  }
}"
7824,"@Override public void getLogNext(final LoggingContext loggingContext,final long fromOffset,final int maxEvents,final Filter filter,final Callback callback){
  if (fromOffset < 0) {
    getLogPrev(loggingContext,-1,maxEvents,filter,callback);
  }
  executor.submit(new Runnable(){
    @Override public void run(){
      Filter logFilter=new AndFilter(ImmutableList.of(LoggingContextHelper.createFilter(loggingContext),filter));
      long fromTimeMs=fromOffset + 1;
      SortedMap<Long,FileStatus> sortedFiles=getFiles(null);
      if (sortedFiles.isEmpty()) {
        return;
      }
      long prevInterval=-1;
      Path prevPath=null;
      List<Path> tailFiles=Lists.newArrayListWithExpectedSize(sortedFiles.size());
      for (      Map.Entry<Long,FileStatus> entry : sortedFiles.entrySet()) {
        if (entry.getKey() >= fromTimeMs && prevPath != null) {
          tailFiles.add(prevPath);
        }
        prevInterval=entry.getKey();
        prevPath=entry.getValue().getPath();
      }
      if (prevInterval != -1) {
        tailFiles.add(prevPath);
      }
      callback.init();
      final List<ILoggingEvent> loggingEvents=Lists.newLinkedList();
      AvroFileLogReader logReader=new AvroFileLogReader(hConf,schema);
      for (      Path file : tailFiles) {
        logReader.readLog(file,logFilter,fromTimeMs,Long.MAX_VALUE,maxEvents - loggingEvents.size(),callback);
        if (loggingEvents.size() >= maxEvents) {
          break;
        }
      }
      callback.close();
    }
  }
);
}","@Override public void getLogNext(final LoggingContext loggingContext,final long fromOffset,final int maxEvents,final Filter filter,final Callback callback){
  if (fromOffset < 0) {
    getLogPrev(loggingContext,-1,maxEvents,filter,callback);
    return;
  }
  executor.submit(new Runnable(){
    @Override public void run(){
      Filter logFilter=new AndFilter(ImmutableList.of(LoggingContextHelper.createFilter(loggingContext),filter));
      long fromTimeMs=fromOffset + 1;
      SortedMap<Long,FileStatus> sortedFiles=getFiles(null);
      if (sortedFiles.isEmpty()) {
        return;
      }
      long prevInterval=-1;
      Path prevPath=null;
      List<Path> tailFiles=Lists.newArrayListWithExpectedSize(sortedFiles.size());
      for (      Map.Entry<Long,FileStatus> entry : sortedFiles.entrySet()) {
        if (entry.getKey() >= fromTimeMs && prevPath != null) {
          tailFiles.add(prevPath);
        }
        prevInterval=entry.getKey();
        prevPath=entry.getValue().getPath();
      }
      if (prevInterval != -1) {
        tailFiles.add(prevPath);
      }
      callback.init();
      final List<ILoggingEvent> loggingEvents=Lists.newLinkedList();
      AvroFileLogReader logReader=new AvroFileLogReader(hConf,schema);
      for (      Path file : tailFiles) {
        logReader.readLog(file,logFilter,fromTimeMs,Long.MAX_VALUE,maxEvents - loggingEvents.size(),callback);
        if (loggingEvents.size() >= maxEvents) {
          break;
        }
      }
      callback.close();
    }
  }
);
}"
7825,"public static synchronized boolean commit(Transaction tx){
  Set<byte[]> changeSet=committingChangeSets.remove(tx.getWritePointer());
  if (changeSet != null) {
    if (hasConflicts(tx,changeSet)) {
      return false;
    }
    committedChangeSets.put(nextWritePointer,changeSet);
  }
  makeVisible(tx);
  committedChangeSets.headMap(excludedList.isEmpty() ? Long.MAX_VALUE : excludedList.get(0)).clear();
  return true;
}","public static synchronized boolean commit(Transaction tx){
  Set<byte[]> changeSet=committingChangeSets.remove(tx.getWritePointer());
  if (changeSet != null) {
    if (hasConflicts(tx,changeSet)) {
      return false;
    }
    if (committedChangeSets.containsKey(nextWritePointer)) {
      committedChangeSets.get(nextWritePointer).addAll(changeSet);
    }
 else {
      committedChangeSets.put(nextWritePointer,Sets.newHashSet(changeSet));
    }
  }
  makeVisible(tx);
  committedChangeSets.headMap(excludedList.isEmpty() ? Long.MAX_VALUE : excludedList.get(0)).clear();
  return true;
}"
7826,"private RuntimeInfo addRemover(final RuntimeInfo runtimeInfo){
  runtimeInfo.getController().addListener(new AbstractListener(){
    @Override public void stopped(){
      remove(runtimeInfo);
    }
    @Override public void error(){
      remove(runtimeInfo);
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  return runtimeInfo;
}","private RuntimeInfo addRemover(final RuntimeInfo runtimeInfo){
  runtimeInfo.getController().addListener(new AbstractListener(){
    @Override public void stopped(){
      remove(runtimeInfo);
    }
    @Override public void error(    Throwable cause){
      remove(runtimeInfo);
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  return runtimeInfo;
}"
7827,"@Override public void error(){
  remove(runtimeInfo);
}","@Override public void error(Throwable cause){
  remove(runtimeInfo);
}"
7828,void error();,void error(Throwable cause);
7829,"@Override public void error(){
}","@Override public void error(Throwable cause){
}"
7830,"@Override public void run(){
  try {
    listener.error();
  }
 catch (  Throwable t) {
    LOG.info(t.getMessage(),t);
  }
}","@Override public void run(){
  try {
    listener.error(cause);
  }
 catch (  Throwable t) {
    LOG.info(t.getMessage(),t);
  }
}"
7831,"@Override public void error(){
  executor.execute(new Runnable(){
    @Override public void run(){
      try {
        listener.error();
      }
 catch (      Throwable t) {
        LOG.info(t.getMessage(),t);
      }
    }
  }
);
}","@Override public void error(final Throwable cause){
  executor.execute(new Runnable(){
    @Override public void run(){
      try {
        listener.error(cause);
      }
 catch (      Throwable t) {
        LOG.info(t.getMessage(),t);
      }
    }
  }
);
}"
7832,"@Override public void run(){
  LOG.info(""String_Node_Str"");
  Futures.getUnchecked(Services.chainStart(zkClientService,kafkaClientService,metricsCollectionService));
  LOG.info(""String_Node_Str"" + name);
  controller=injector.getInstance(getProgramClass()).run(program,programOpts);
  final SettableFuture<ProgramController.State> state=SettableFuture.create();
  controller.addListener(new AbstractListener(){
    @Override public void stopped(){
      state.set(ProgramController.State.STOPPED);
    }
    @Override public void error(){
      state.set(ProgramController.State.ERROR);
    }
  }
,MoreExecutors.sameThreadExecutor());
  LOG.info(""String_Node_Str"",Futures.getUnchecked(state));
}","@Override public void run(){
  LOG.info(""String_Node_Str"");
  Futures.getUnchecked(Services.chainStart(zkClientService,kafkaClientService,metricsCollectionService));
  LOG.info(""String_Node_Str"" + name);
  controller=injector.getInstance(getProgramClass()).run(program,programOpts);
  final SettableFuture<ProgramController.State> state=SettableFuture.create();
  controller.addListener(new AbstractListener(){
    @Override public void stopped(){
      state.set(ProgramController.State.STOPPED);
    }
    @Override public void error(    Throwable cause){
      LOG.error(""String_Node_Str"",cause);
      state.set(ProgramController.State.ERROR);
    }
  }
,MoreExecutors.sameThreadExecutor());
  LOG.info(""String_Node_Str"",Futures.getUnchecked(state));
}"
7833,"@Override public void error(){
  state.set(ProgramController.State.ERROR);
}","@Override public void error(Throwable cause){
  LOG.error(""String_Node_Str"",cause);
  state.set(ProgramController.State.ERROR);
}"
7834,"@Override public void handleCommand(Command command) throws Exception {
  if (ProgramCommands.SUSPEND.equals(command)) {
    controller.suspend().get();
    return;
  }
  if (ProgramCommands.RESUME.equals(command)) {
    controller.resume().get();
    return;
  }
  if (""String_Node_Str"".equals(command.getCommand())) {
    int instances=Integer.parseInt(command.getOptions().get(""String_Node_Str""));
    controller.command(""String_Node_Str"",instances);
    return;
  }
  LOG.warn(""String_Node_Str"" + command);
}","@Override public void handleCommand(Command command) throws Exception {
  if (ProgramCommands.SUSPEND.equals(command)) {
    controller.suspend().get();
    return;
  }
  if (ProgramCommands.RESUME.equals(command)) {
    controller.resume().get();
    return;
  }
  if (""String_Node_Str"".equals(command.getCommand())) {
    int instances=Integer.parseInt(command.getOptions().get(""String_Node_Str""));
    controller.command(""String_Node_Str"",instances).get();
    return;
  }
  LOG.warn(""String_Node_Str"" + command);
}"
7835,"/** 
 * Invoked during shutdown of the thread.
 */
protected void triggerShutdown(){
  server.stop();
}","/** 
 * Invoked during shutdown of the thread.
 */
protected void triggerShutdown(){
  executor.shutdownNow();
  server.stop();
}"
7836,"@Override public boolean commitTx() throws Exception {
  if (consumingEntries.isEmpty()) {
    return true;
  }
  byte[] stateContent=encodeStateColumn(ConsumerEntryState.PROCESSED);
  List<Put> puts=Lists.newArrayListWithCapacity(consumingEntries.size());
  for (  byte[] rowKey : consumingEntries.keySet()) {
    Put put=new Put(rowKey);
    put.add(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.STATE_COLUMN,stateContent);
    puts.add(put);
  }
  hTable.put(puts);
  hTable.flushCommits();
  return true;
}","@Override public boolean commitTx() throws Exception {
  if (consumingEntries.isEmpty()) {
    return true;
  }
  byte[] stateContent=encodeStateColumn(ConsumerEntryState.PROCESSED);
  List<Put> puts=Lists.newArrayListWithCapacity(consumingEntries.size());
  for (  byte[] rowKey : consumingEntries.keySet()) {
    Put put=new Put(rowKey);
    put.add(HBaseQueueConstants.COLUMN_FAMILY,stateColumnName,stateContent);
    puts.add(put);
  }
  hTable.put(puts);
  hTable.flushCommits();
  return true;
}"
7837,"@Override public DequeueResult dequeue(int maxBatchSize) throws IOException {
  Preconditions.checkArgument(maxBatchSize > 0,""String_Node_Str"");
  List<Entry> dequeueEntries=Lists.newLinkedList();
  while (dequeueEntries.size() < maxBatchSize && getEntries(dequeueEntries,maxBatchSize)) {
    Iterator<Entry> iterator=dequeueEntries.iterator();
    while (iterator.hasNext()) {
      Entry entry=iterator.next();
      if (consumerConfig.getDequeueStrategy() == DequeueStrategy.FIFO) {
        if (entry.getState() == null) {
          Put put=new Put(entry.getRowKey());
          byte[] stateValue=encodeStateColumn(ConsumerEntryState.CLAIMED);
          put.add(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.STATE_COLUMN,stateValue);
          boolean claimed=hTable.checkAndPut(entry.getRowKey(),HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.STATE_COLUMN,entry.getState(),put);
          if (!claimed) {
            iterator.remove();
            continue;
          }
          entry=new Entry(entry.getRowKey(),entry.getData(),stateValue);
        }
      }
      consumingEntries.put(entry.getRowKey(),entry);
    }
  }
  if (dequeueEntries.isEmpty()) {
    return EMPTY_RESULT;
  }
  return new DequeueResultImpl(dequeueEntries);
}","@Override public DequeueResult dequeue(int maxBatchSize) throws IOException {
  Preconditions.checkArgument(maxBatchSize > 0,""String_Node_Str"");
  List<Entry> dequeueEntries=Lists.newLinkedList();
  while (dequeueEntries.size() < maxBatchSize && getEntries(dequeueEntries,maxBatchSize)) {
    Iterator<Entry> iterator=dequeueEntries.iterator();
    while (iterator.hasNext()) {
      Entry entry=iterator.next();
      if (consumerConfig.getDequeueStrategy() == DequeueStrategy.FIFO && consumerConfig.getGroupSize() > 1) {
        if (entry.getState() == null) {
          Put put=new Put(entry.getRowKey());
          byte[] stateValue=encodeStateColumn(ConsumerEntryState.CLAIMED);
          put.add(HBaseQueueConstants.COLUMN_FAMILY,stateColumnName,stateValue);
          boolean claimed=hTable.checkAndPut(entry.getRowKey(),HBaseQueueConstants.COLUMN_FAMILY,stateColumnName,entry.getState(),put);
          if (!claimed) {
            iterator.remove();
            continue;
          }
          entry=new Entry(entry.getRowKey(),entry.getData(),stateValue);
        }
      }
      consumingEntries.put(entry.getRowKey(),entry);
    }
  }
  if (dequeueEntries.isEmpty()) {
    return EMPTY_RESULT;
  }
  return new DequeueResultImpl(dequeueEntries);
}"
7838,"@Override public boolean rollbackTx() throws Exception {
  if (consumingEntries.isEmpty()) {
    return true;
  }
  entryCache.putAll(consumingEntries);
  List<Row> ops=Lists.newArrayListWithCapacity(consumingEntries.size());
  if (consumerConfig.getDequeueStrategy() == DequeueStrategy.FIFO) {
    byte[] stateContent=encodeStateColumn(ConsumerEntryState.CLAIMED);
    for (    byte[] rowKey : consumingEntries.keySet()) {
      Put put=new Put(rowKey);
      put.add(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.STATE_COLUMN,stateContent);
      ops.add(put);
    }
  }
 else {
    for (    byte[] rowKey : consumingEntries.keySet()) {
      Delete delete=new Delete(rowKey);
      delete.deleteColumn(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.STATE_COLUMN);
      ops.add(delete);
    }
  }
  hTable.batch(ops);
  hTable.flushCommits();
  return true;
}","@Override public boolean rollbackTx() throws Exception {
  if (consumingEntries.isEmpty()) {
    return true;
  }
  entryCache.putAll(consumingEntries);
  List<Row> ops=Lists.newArrayListWithCapacity(consumingEntries.size());
  if (consumerConfig.getDequeueStrategy() == DequeueStrategy.FIFO && consumerConfig.getGroupSize() > 1) {
    byte[] stateContent=encodeStateColumn(ConsumerEntryState.CLAIMED);
    for (    byte[] rowKey : consumingEntries.keySet()) {
      Put put=new Put(rowKey);
      put.add(HBaseQueueConstants.COLUMN_FAMILY,stateColumnName,stateContent);
      ops.add(put);
    }
  }
 else {
    for (    byte[] rowKey : consumingEntries.keySet()) {
      Delete delete=new Delete(rowKey);
      delete.deleteColumn(HBaseQueueConstants.COLUMN_FAMILY,stateColumnName);
      ops.add(delete);
    }
  }
  hTable.batch(ops);
  hTable.flushCommits();
  return true;
}"
7839,"HBaseQueueConsumer(ConsumerConfig consumerConfig,HTable hTable,QueueName queueName){
  this.consumerConfig=consumerConfig;
  this.hTable=hTable;
  this.queueName=queueName;
  this.entryCache=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  this.consumingEntries=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  this.startRow=queueName.toBytes();
  byte[] tableName=hTable.getTableName();
  final byte[] changeTxPrefix=ByteBuffer.allocate(tableName.length + 1).put((byte)tableName.length).put(tableName).array();
  rowKeyToChangeTx=new Function<byte[],byte[]>(){
    @Override public byte[] apply(    byte[] rowKey){
      return Bytes.add(changeTxPrefix,rowKey);
    }
  }
;
}","HBaseQueueConsumer(ConsumerConfig consumerConfig,HTable hTable,QueueName queueName){
  this.consumerConfig=consumerConfig;
  this.hTable=hTable;
  this.queueName=queueName;
  this.entryCache=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  this.consumingEntries=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  this.startRow=queueName.toBytes();
  this.stateColumnName=Bytes.add(HBaseQueueConstants.STATE_COLUMN_PREFIX,Bytes.toBytes(consumerConfig.getGroupId()));
  byte[] tableName=hTable.getTableName();
  final byte[] changeTxPrefix=ByteBuffer.allocate(tableName.length + 1).put((byte)tableName.length).put(tableName).array();
  rowKeyToChangeTx=new Function<byte[],byte[]>(){
    @Override public byte[] apply(    byte[] rowKey){
      return Bytes.add(changeTxPrefix,rowKey);
    }
  }
;
}"
7840,"private void populateRowCache() throws IOException {
  Scan scan=new Scan();
  scan.setCaching(MAX_CACHE_ROWS);
  scan.setStartRow(startRow);
  scan.setStopRow(getStopRow());
  scan.addColumn(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.DATA_COLUMN);
  scan.addColumn(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.META_COLUMN);
  scan.addColumn(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.STATE_COLUMN);
  scan.setMaxVersions();
  long readPointer=transaction.getReadPointer();
  long[] excludedList=transaction.getExcludedList();
  ResultScanner scanner=hTable.getScanner(scan);
  while (entryCache.size() < MAX_CACHE_ROWS) {
    Result[] results=scanner.next(MAX_CACHE_ROWS);
    if (results.length == 0) {
      break;
    }
    for (    Result result : results) {
      byte[] rowKey=result.getRow();
      if (entryCache.containsKey(rowKey) || consumingEntries.containsKey(rowKey)) {
        continue;
      }
      if (excludedList.length == 0) {
        startRow=rowKey;
      }
      long writePointer=Bytes.toLong(rowKey,queueName.toBytes().length,Longs.BYTES);
      if (writePointer > readPointer) {
        break;
      }
      if (Arrays.binarySearch(excludedList,writePointer) >= 0) {
        continue;
      }
      KeyValue metaColumn=result.getColumnLatest(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.META_COLUMN);
      KeyValue stateColumn=result.getColumnLatest(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.STATE_COLUMN);
      int counter=Bytes.toInt(rowKey,rowKey.length - 4,Ints.BYTES);
      if (!shouldInclude(writePointer,counter,metaColumn,stateColumn)) {
        continue;
      }
      entryCache.put(rowKey,new Entry(rowKey,result.getValue(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.DATA_COLUMN),result.getValue(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.STATE_COLUMN)));
    }
  }
  scanner.close();
}","private void populateRowCache() throws IOException {
  Scan scan=new Scan();
  scan.setCaching(MAX_CACHE_ROWS);
  scan.setStartRow(startRow);
  scan.setStopRow(getStopRow());
  scan.addColumn(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.DATA_COLUMN);
  scan.addColumn(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.META_COLUMN);
  scan.addColumn(HBaseQueueConstants.COLUMN_FAMILY,stateColumnName);
  scan.setMaxVersions();
  long readPointer=transaction.getReadPointer();
  long[] excludedList=transaction.getExcludedList();
  ResultScanner scanner=hTable.getScanner(scan);
  while (entryCache.size() < MAX_CACHE_ROWS) {
    Result[] results=scanner.next(MAX_CACHE_ROWS);
    if (results.length == 0) {
      break;
    }
    for (    Result result : results) {
      byte[] rowKey=result.getRow();
      if (entryCache.containsKey(rowKey) || consumingEntries.containsKey(rowKey)) {
        continue;
      }
      long writePointer=Bytes.toLong(rowKey,queueName.toBytes().length,Longs.BYTES);
      if (writePointer > readPointer) {
        break;
      }
      if (Arrays.binarySearch(excludedList,writePointer) >= 0) {
        continue;
      }
      KeyValue metaColumn=result.getColumnLatest(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.META_COLUMN);
      KeyValue stateColumn=result.getColumnLatest(HBaseQueueConstants.COLUMN_FAMILY,stateColumnName);
      int counter=Bytes.toInt(rowKey,rowKey.length - 4,Ints.BYTES);
      if (!shouldInclude(writePointer,counter,metaColumn,stateColumn)) {
        continue;
      }
      entryCache.put(rowKey,new Entry(rowKey,result.getValue(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.DATA_COLUMN),result.getValue(HBaseQueueConstants.COLUMN_FAMILY,stateColumnName)));
    }
  }
  scanner.close();
}"
7841,"@Override public void onFailure(Throwable t){
  LOG.warn(StackTraceUtil.toStringStackTrace(t));
  LOG.warn(t.getCause().toString());
  DeployStatus status=DeployStatus.FAILED;
  Throwable cause=t.getCause();
  if (cause instanceof ClassNotFoundException) {
    status.setMessage(String.format(UserMessages.getMessage(""String_Node_Str""),t.getMessage()));
  }
 else   if (cause instanceof IllegalArgumentException) {
    status.setMessage(String.format(UserMessages.getMessage(""String_Node_Str""),t.getMessage()));
  }
 else {
    status.setMessage(t.getMessage());
  }
  save(sessionInfo.setStatus(status));
  sessions.remove(resource.getAccountId());
}","@Override public void onFailure(Throwable t){
  LOG.warn(StackTraceUtil.toStringStackTrace(t));
  DeployStatus status=DeployStatus.FAILED;
  Throwable cause=t.getCause();
  if (cause instanceof ClassNotFoundException) {
    status.setMessage(String.format(UserMessages.getMessage(""String_Node_Str""),t.getMessage()));
  }
 else   if (cause instanceof IllegalArgumentException) {
    status.setMessage(String.format(UserMessages.getMessage(""String_Node_Str""),t.getMessage()));
  }
 else {
    status.setMessage(t.getMessage());
  }
  save(sessionInfo.setStatus(status));
  sessions.remove(resource.getAccountId());
}"
7842,"/** 
 * Finalizes the deployment of a archive. Once upload is completed, it will start the pipeline responsible for verification and registration of archive resources.
 * @param resource identifier to be finalized.
 */
@Override public void deploy(AuthToken token,final ResourceIdentifier resource) throws AppFabricServiceException {
  LOG.warn(""String_Node_Str"" + resource.toString());
  if (!sessions.containsKey(resource.getAccountId())) {
    throw new AppFabricServiceException(""String_Node_Str"");
  }
  final SessionInfo sessionInfo=sessions.get(resource.getAccountId());
  try {
    Id.Account id=Id.Account.from(resource.getAccountId());
    Location archiveLocation=sessionInfo.getArchiveLocation();
    sessionInfo.getOutputStream().close();
    sessionInfo.setStatus(DeployStatus.VERIFYING);
    Manager<Location,ApplicationWithPrograms> manager=managerFactory.create();
    ListenableFuture<ApplicationWithPrograms> future=manager.deploy(id,archiveLocation);
    Futures.addCallback(future,new FutureCallback<ApplicationWithPrograms>(){
      @Override public void onSuccess(      ApplicationWithPrograms result){
        save(sessionInfo.setStatus(DeployStatus.DEPLOYED));
        sessions.remove(resource.getAccountId());
      }
      @Override public void onFailure(      Throwable t){
        LOG.warn(StackTraceUtil.toStringStackTrace(t));
        LOG.warn(t.getCause().toString());
        DeployStatus status=DeployStatus.FAILED;
        Throwable cause=t.getCause();
        if (cause instanceof ClassNotFoundException) {
          status.setMessage(String.format(UserMessages.getMessage(""String_Node_Str""),t.getMessage()));
        }
 else         if (cause instanceof IllegalArgumentException) {
          status.setMessage(String.format(UserMessages.getMessage(""String_Node_Str""),t.getMessage()));
        }
 else {
          status.setMessage(t.getMessage());
        }
        save(sessionInfo.setStatus(status));
        sessions.remove(resource.getAccountId());
      }
    }
);
  }
 catch (  Throwable e) {
    LOG.warn(StackTraceUtil.toStringStackTrace(e));
    DeployStatus status=DeployStatus.FAILED;
    status.setMessage(e.getMessage());
    save(sessionInfo.setStatus(status));
    sessions.remove(resource.getAccountId());
    throw new AppFabricServiceException(e.getMessage());
  }
}","/** 
 * Finalizes the deployment of a archive. Once upload is completed, it will start the pipeline responsible for verification and registration of archive resources.
 * @param resource identifier to be finalized.
 */
@Override public void deploy(AuthToken token,final ResourceIdentifier resource) throws AppFabricServiceException {
  LOG.debug(""String_Node_Str"" + resource.toString());
  if (!sessions.containsKey(resource.getAccountId())) {
    throw new AppFabricServiceException(""String_Node_Str"");
  }
  final SessionInfo sessionInfo=sessions.get(resource.getAccountId());
  try {
    Id.Account id=Id.Account.from(resource.getAccountId());
    Location archiveLocation=sessionInfo.getArchiveLocation();
    sessionInfo.getOutputStream().close();
    sessionInfo.setStatus(DeployStatus.VERIFYING);
    Manager<Location,ApplicationWithPrograms> manager=managerFactory.create();
    ListenableFuture<ApplicationWithPrograms> future=manager.deploy(id,archiveLocation);
    Futures.addCallback(future,new FutureCallback<ApplicationWithPrograms>(){
      @Override public void onSuccess(      ApplicationWithPrograms result){
        save(sessionInfo.setStatus(DeployStatus.DEPLOYED));
        sessions.remove(resource.getAccountId());
      }
      @Override public void onFailure(      Throwable t){
        LOG.warn(StackTraceUtil.toStringStackTrace(t));
        DeployStatus status=DeployStatus.FAILED;
        Throwable cause=t.getCause();
        if (cause instanceof ClassNotFoundException) {
          status.setMessage(String.format(UserMessages.getMessage(""String_Node_Str""),t.getMessage()));
        }
 else         if (cause instanceof IllegalArgumentException) {
          status.setMessage(String.format(UserMessages.getMessage(""String_Node_Str""),t.getMessage()));
        }
 else {
          status.setMessage(t.getMessage());
        }
        save(sessionInfo.setStatus(status));
        sessions.remove(resource.getAccountId());
      }
    }
);
  }
 catch (  Throwable e) {
    LOG.warn(StackTraceUtil.toStringStackTrace(e));
    DeployStatus status=DeployStatus.FAILED;
    status.setMessage(e.getMessage());
    save(sessionInfo.setStatus(status));
    sessions.remove(resource.getAccountId());
    throw new AppFabricServiceException(e.getMessage());
  }
}"
7843,"@Override public DequeueResult dequeue(int maxBatchSize) throws IOException {
  Preconditions.checkArgument(maxBatchSize > 0,""String_Node_Str"");
  List<Entry> dequeueEntries=Lists.newLinkedList();
  while (dequeueEntries.size() < maxBatchSize && getEntries(dequeueEntries,maxBatchSize)) {
    Iterator<Entry> iterator=dequeueEntries.iterator();
    while (iterator.hasNext()) {
      Entry entry=iterator.next();
      if (consumerConfig.getDequeueStrategy() == DequeueStrategy.FIFO && consumerConfig.getGroupSize() > 1) {
        if (entry.getState() == null) {
          Put put=new Put(entry.getRowKey());
          byte[] stateValue=encodeStateColumn(ConsumerEntryState.CLAIMED);
          put.add(HBaseQueueConstants.COLUMN_FAMILY,stateColumnName,stateValue);
          boolean claimed=hTable.checkAndPut(entry.getRowKey(),HBaseQueueConstants.COLUMN_FAMILY,stateColumnName,entry.getState(),put);
          if (!claimed) {
            iterator.remove();
            continue;
          }
          entry=new Entry(entry.getRowKey(),entry.getData(),stateValue);
        }
      }
      consumingEntries.put(entry.getRowKey(),entry);
    }
  }
  if (dequeueEntries.isEmpty()) {
    return EMPTY_RESULT;
  }
  return new DequeueResultImpl(dequeueEntries);
}","@Override public DequeueResult dequeue(int maxBatchSize) throws IOException {
  Preconditions.checkArgument(maxBatchSize > 0,""String_Node_Str"");
  while (consumingEntries.size() < maxBatchSize && getEntries(consumingEntries,maxBatchSize)) {
    if (consumerConfig.getDequeueStrategy() == DequeueStrategy.FIFO && consumerConfig.getGroupSize() > 1) {
      Iterator<Map.Entry<byte[],Entry>> iterator=consumingEntries.entrySet().iterator();
      while (iterator.hasNext()) {
        Entry entry=iterator.next().getValue();
        if (entry.getState() == null) {
          Put put=new Put(entry.getRowKey());
          byte[] stateValue=encodeStateColumn(ConsumerEntryState.CLAIMED);
          put.add(HBaseQueueConstants.COLUMN_FAMILY,stateColumnName,stateValue);
          boolean claimed=hTable.checkAndPut(entry.getRowKey(),HBaseQueueConstants.COLUMN_FAMILY,stateColumnName,null,put);
          if (!claimed) {
            iterator.remove();
            continue;
          }
        }
      }
    }
  }
  if (consumingEntries.isEmpty()) {
    return EMPTY_RESULT;
  }
  return new DequeueResultImpl(consumingEntries.values());
}"
7844,"private void populateRowCache() throws IOException {
  Scan scan=new Scan();
  scan.setCaching(MAX_CACHE_ROWS);
  scan.setStartRow(startRow);
  scan.setStopRow(getStopRow());
  scan.addColumn(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.DATA_COLUMN);
  scan.addColumn(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.META_COLUMN);
  scan.addColumn(HBaseQueueConstants.COLUMN_FAMILY,stateColumnName);
  scan.setMaxVersions();
  long readPointer=transaction.getReadPointer();
  long[] excludedList=transaction.getExcludedList();
  ResultScanner scanner=hTable.getScanner(scan);
  while (entryCache.size() < MAX_CACHE_ROWS) {
    Result[] results=scanner.next(MAX_CACHE_ROWS);
    if (results.length == 0) {
      break;
    }
    for (    Result result : results) {
      byte[] rowKey=result.getRow();
      if (entryCache.containsKey(rowKey) || consumingEntries.containsKey(rowKey)) {
        continue;
      }
      long writePointer=Bytes.toLong(rowKey,queueName.toBytes().length,Longs.BYTES);
      if (writePointer > readPointer) {
        break;
      }
      if (Arrays.binarySearch(excludedList,writePointer) >= 0) {
        continue;
      }
      KeyValue metaColumn=result.getColumnLatest(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.META_COLUMN);
      KeyValue stateColumn=result.getColumnLatest(HBaseQueueConstants.COLUMN_FAMILY,stateColumnName);
      int counter=Bytes.toInt(rowKey,rowKey.length - 4,Ints.BYTES);
      if (!shouldInclude(writePointer,counter,metaColumn,stateColumn)) {
        continue;
      }
      entryCache.put(rowKey,new Entry(rowKey,result.getValue(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.DATA_COLUMN),result.getValue(HBaseQueueConstants.COLUMN_FAMILY,stateColumnName)));
    }
  }
  scanner.close();
}","private void populateRowCache(Set<byte[]> excludeRows) throws IOException {
  Scan scan=new Scan();
  scan.setCaching(MAX_CACHE_ROWS);
  scan.setStartRow(startRow);
  scan.setStopRow(getStopRow());
  scan.addColumn(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.DATA_COLUMN);
  scan.addColumn(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.META_COLUMN);
  scan.addColumn(HBaseQueueConstants.COLUMN_FAMILY,stateColumnName);
  scan.setMaxVersions();
  long readPointer=transaction.getReadPointer();
  long[] excludedList=transaction.getExcludedList();
  ResultScanner scanner=hTable.getScanner(scan);
  while (entryCache.size() < MAX_CACHE_ROWS) {
    Result[] results=scanner.next(MAX_CACHE_ROWS);
    if (results.length == 0) {
      break;
    }
    for (    Result result : results) {
      byte[] rowKey=result.getRow();
      if (excludeRows.contains(rowKey)) {
        continue;
      }
      long writePointer=Bytes.toLong(rowKey,queueName.toBytes().length,Longs.BYTES);
      if (writePointer > readPointer) {
        break;
      }
      if (Arrays.binarySearch(excludedList,writePointer) >= 0) {
        continue;
      }
      KeyValue metaColumn=result.getColumnLatest(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.META_COLUMN);
      KeyValue stateColumn=result.getColumnLatest(HBaseQueueConstants.COLUMN_FAMILY,stateColumnName);
      int counter=Bytes.toInt(rowKey,rowKey.length - 4,Ints.BYTES);
      if (!shouldInclude(writePointer,counter,metaColumn,stateColumn)) {
        continue;
      }
      entryCache.put(rowKey,new Entry(rowKey,result.getValue(HBaseQueueConstants.COLUMN_FAMILY,HBaseQueueConstants.DATA_COLUMN),result.getValue(HBaseQueueConstants.COLUMN_FAMILY,stateColumnName)));
    }
  }
  scanner.close();
}"
7845,"/** 
 * Try to dequeue (claim) entries up to a maximum size.
 * @param entries For claimed entries to fill in.
 * @param maxBatchSize Maximum number of entries to claim.
 * @return The entries instance.
 * @throws IOException
 */
private boolean getEntries(List<Entry> entries,int maxBatchSize) throws IOException {
  boolean hasEntry=fetchFromCache(entries,maxBatchSize);
  if (entries.size() < maxBatchSize) {
    populateRowCache();
    hasEntry=fetchFromCache(entries,maxBatchSize);
  }
  return hasEntry;
}","/** 
 * Try to dequeue (claim) entries up to a maximum size.
 * @param entries For claimed entries to fill in.
 * @param maxBatchSize Maximum number of entries to claim.
 * @return The entries instance.
 * @throws IOException
 */
private boolean getEntries(SortedMap<byte[],Entry> entries,int maxBatchSize) throws IOException {
  boolean hasEntry=fetchFromCache(entries,maxBatchSize);
  if (entries.size() < maxBatchSize) {
    populateRowCache(entries.keySet());
    hasEntry=fetchFromCache(entries,maxBatchSize) || hasEntry;
  }
  return hasEntry;
}"
7846,"private boolean fetchFromCache(List<Entry> entries,int maxBatchSize){
  if (entryCache.isEmpty()) {
    return false;
  }
  Iterator<Map.Entry<byte[],Entry>> iterator=entryCache.entrySet().iterator();
  while (entries.size() < maxBatchSize && iterator.hasNext()) {
    entries.add(iterator.next().getValue());
    iterator.remove();
  }
  return true;
}","private boolean fetchFromCache(SortedMap<byte[],Entry> entries,int maxBatchSize){
  if (entryCache.isEmpty()) {
    return false;
  }
  Iterator<Map.Entry<byte[],Entry>> iterator=entryCache.entrySet().iterator();
  while (entries.size() < maxBatchSize && iterator.hasNext()) {
    Map.Entry<byte[],Entry> entry=iterator.next();
    entries.put(entry.getKey(),entry.getValue());
    iterator.remove();
  }
  return true;
}"
7847,"/** 
 * Given a key prefix, return the smallest key that is greater than all keys starting with that prefix.
 */
private byte[] getStopRow(){
  return Bytes.add(queueName.toBytes(),Bytes.toBytes(transaction.getReadPointer() + 1L));
}","/** 
 * Gets the stop row for scan. Stop row is queueName + (readPointer + 1).
 */
private byte[] getStopRow(){
  return Bytes.add(queueName.toBytes(),Bytes.toBytes(transaction.getReadPointer() + 1L));
}"
7848,"@Test public void testSingleFifo() throws Exception {
  QueueName queueName=QueueName.fromFlowlet(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  final HBaseQueueClient queueClient=new HBaseQueueClient(HBaseTestBase.getHBaseAdmin(),""String_Node_Str"",queueName);
  final int count=5000;
  LOG.info(""String_Node_Str"",count);
  Stopwatch stopwatch=new Stopwatch();
  stopwatch.start();
  for (int i=0; i < count; i++) {
    Transaction transaction=opex.start();
    try {
      byte[] queueData=Bytes.toBytes(i);
      queueClient.startTx(transaction);
      queueClient.enqueue(new QueueEntry(queueData));
      if (opex.canCommit(transaction,queueClient.getTxChanges()) && queueClient.commitTx()) {
        if (!opex.commit(transaction)) {
          queueClient.rollbackTx();
        }
      }
    }
 catch (    Exception e) {
      opex.abort(transaction);
      throw Throwables.propagate(e);
    }
  }
  long elapsed=stopwatch.elapsedTime(TimeUnit.MILLISECONDS);
  LOG.info(""String_Node_Str"",count,elapsed);
  LOG.info(""String_Node_Str"",(double)count * 1000 / elapsed);
  final int expectedSum=(count / 2 * (count - 1));
  final AtomicInteger valueSum=new AtomicInteger();
  final int consumerSize=5;
  final CyclicBarrier startBarrier=new CyclicBarrier(consumerSize + 1);
  final CountDownLatch completeLatch=new CountDownLatch(consumerSize);
  ExecutorService executor=Executors.newFixedThreadPool(consumerSize);
  for (int i=0; i < consumerSize; i++) {
    final int instanceId=i;
    executor.submit(new Runnable(){
      @Override public void run(){
        try {
          startBarrier.await();
          QueueConsumer consumer=queueClient.createConsumer(new ConsumerConfig(0,instanceId,consumerSize,DequeueStrategy.FIFO,null));
          TransactionAware txAware=(TransactionAware)consumer;
          Stopwatch stopwatch=new Stopwatch();
          stopwatch.start();
          int dequeueCount=0;
          while (valueSum.get() != expectedSum) {
            Transaction transaction=opex.start();
            txAware.startTx(transaction);
            try {
              DequeueResult result=consumer.dequeue();
              if (opex.canCommit(transaction,queueClient.getTxChanges()) && txAware.commitTx()) {
                if (!opex.commit(transaction)) {
                  txAware.rollbackTx();
                }
              }
              if (result.isEmpty()) {
                continue;
              }
              byte[] data=result.getData().iterator().next();
              valueSum.addAndGet(Bytes.toInt(data));
              dequeueCount++;
            }
 catch (            Exception e) {
              opex.abort(transaction);
              throw Throwables.propagate(e);
            }
          }
          long elapsed=stopwatch.elapsedTime(TimeUnit.MILLISECONDS);
          LOG.info(""String_Node_Str"",dequeueCount,elapsed);
          LOG.info(""String_Node_Str"",(double)dequeueCount * 1000 / elapsed);
          completeLatch.countDown();
        }
 catch (        Exception e) {
          LOG.error(e.getMessage(),e);
        }
      }
    }
);
  }
  startBarrier.await();
  Assert.assertTrue(completeLatch.await(40,TimeUnit.SECONDS));
  TimeUnit.SECONDS.sleep(2);
  Assert.assertEquals(expectedSum,valueSum.get());
  executor.shutdownNow();
}","@Test public void testSingleFifo() throws Exception {
  QueueName queueName=QueueName.fromFlowlet(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  final HBaseQueueClient queueClient=new HBaseQueueClient(HBaseTestBase.getHBaseAdmin(),""String_Node_Str"",queueName);
  final int count=5000;
  LOG.info(""String_Node_Str"",count);
  Stopwatch stopwatch=new Stopwatch();
  stopwatch.start();
  for (int i=0; i < count; i++) {
    Transaction transaction=opex.start();
    try {
      byte[] queueData=Bytes.toBytes(i);
      queueClient.startTx(transaction);
      queueClient.enqueue(new QueueEntry(queueData));
      if (opex.canCommit(transaction,queueClient.getTxChanges()) && queueClient.commitTx()) {
        if (!opex.commit(transaction)) {
          queueClient.rollbackTx();
        }
      }
    }
 catch (    Exception e) {
      opex.abort(transaction);
      throw Throwables.propagate(e);
    }
  }
  long elapsed=stopwatch.elapsedTime(TimeUnit.MILLISECONDS);
  LOG.info(""String_Node_Str"",count,elapsed);
  LOG.info(""String_Node_Str"",(double)count * 1000 / elapsed);
  final int expectedSum=(count / 2 * (count - 1));
  final AtomicInteger valueSum=new AtomicInteger();
  final int consumerSize=3;
  final CyclicBarrier startBarrier=new CyclicBarrier(consumerSize + 1);
  final CountDownLatch completeLatch=new CountDownLatch(consumerSize);
  ExecutorService executor=Executors.newFixedThreadPool(consumerSize);
  for (int i=0; i < consumerSize; i++) {
    final int instanceId=i;
    executor.submit(new Runnable(){
      @Override public void run(){
        try {
          startBarrier.await();
          QueueConsumer consumer=queueClient.createConsumer(new ConsumerConfig(0,instanceId,consumerSize,DequeueStrategy.FIFO,null));
          TransactionAware txAware=(TransactionAware)consumer;
          Stopwatch stopwatch=new Stopwatch();
          stopwatch.start();
          int dequeueCount=0;
          while (valueSum.get() != expectedSum) {
            Transaction transaction=opex.start();
            txAware.startTx(transaction);
            try {
              DequeueResult result=consumer.dequeue();
              if (opex.canCommit(transaction,queueClient.getTxChanges()) && txAware.commitTx()) {
                if (!opex.commit(transaction)) {
                  txAware.rollbackTx();
                }
              }
              if (result.isEmpty()) {
                continue;
              }
              for (              byte[] data : result.getData()) {
                valueSum.addAndGet(Bytes.toInt(data));
                dequeueCount++;
              }
            }
 catch (            Exception e) {
              opex.abort(transaction);
              throw Throwables.propagate(e);
            }
          }
          long elapsed=stopwatch.elapsedTime(TimeUnit.MILLISECONDS);
          LOG.info(""String_Node_Str"",dequeueCount,elapsed);
          LOG.info(""String_Node_Str"",(double)dequeueCount * 1000 / elapsed);
          completeLatch.countDown();
        }
 catch (        Exception e) {
          LOG.error(e.getMessage(),e);
        }
      }
    }
);
  }
  startBarrier.await();
  Assert.assertTrue(completeLatch.await(120,TimeUnit.SECONDS));
  TimeUnit.SECONDS.sleep(2);
  Assert.assertEquals(expectedSum,valueSum.get());
  executor.shutdownNow();
}"
7849,"@Override public StreamWriter getStreamWriter(String streamName){
  QueueName queueName=QueueName.fromStream(idAccount,streamName);
  StreamWriter streamWriter=benchmarkStreamWriterFactory.create(CConfiguration.create(),queueName);
  streamWriters.add((MultiThreadedStreamWriter)streamWriter);
  return streamWriter;
}","@Override public StreamWriter getStreamWriter(String streamName){
  QueueName queueName=QueueName.fromStream(idAccount.getId(),streamName);
  StreamWriter streamWriter=benchmarkStreamWriterFactory.create(CConfiguration.create(),queueName);
  streamWriters.add((MultiThreadedStreamWriter)streamWriter);
  return streamWriter;
}"
7850,"/** 
 * Decodes the batch request
 * @return a List of String containing all requests from the batch.
 */
private List<MetricsRequest> decodeRequests(ChannelBuffer content) throws IOException {
  Reader reader=new InputStreamReader(new ChannelBufferInputStream(content),Charsets.UTF_8);
  try {
    List<URI> uris=GSON.fromJson(reader,new TypeToken<List<URI>>(){
    }
.getType());
    LOG.trace(""String_Node_Str"",uris);
    return ImmutableList.copyOf(Iterables.transform(uris,URI_TO_METRIC_REQUEST));
  }
  finally {
    reader.close();
  }
}","/** 
 * Decodes the batch request.
 * @return a List of String containing all requests from the batch.
 */
private List<MetricsRequest> decodeRequests(ChannelBuffer content) throws IOException {
  Reader reader=new InputStreamReader(new ChannelBufferInputStream(content),Charsets.UTF_8);
  try {
    List<URI> uris=GSON.fromJson(reader,new TypeToken<List<URI>>(){
    }
.getType());
    LOG.trace(""String_Node_Str"",uris);
    return ImmutableList.copyOf(Iterables.transform(uris,URI_TO_METRIC_REQUEST));
  }
  finally {
    reader.close();
  }
}"
7851,"@Override public void init(String[] args){
  int brokerId=generateBrokerId();
  LOG.info(String.format(""String_Node_Str"",brokerId));
  CConfiguration cConf=CConfiguration.create();
  zkConnectStr=cConf.get(Constants.CFG_ZOOKEEPER_ENSEMBLE);
  zkNamespace=cConf.get(KafkaConstants.ConfigKeys.ZOOKEEPER_NAMESPACE_CONFIG);
  int port=cConf.getInt(KafkaConstants.ConfigKeys.PORT_CONFIG,-1);
  String hostname=cConf.get(KafkaConstants.ConfigKeys.HOSTNAME_CONFIG);
  if (hostname != null) {
    InetSocketAddress socketAddress=new InetSocketAddress(hostname,0);
    InetAddress address=socketAddress.getAddress();
    if (address.isAnyLocalAddress()) {
      try {
        hostname=InetAddress.getLocalHost().getCanonicalHostName();
      }
 catch (      UnknownHostException e) {
        throw Throwables.propagate(e);
      }
    }
  }
  int numPartitions=cConf.getInt(KafkaConstants.ConfigKeys.NUM_PARTITIONS_CONFIG,KafkaConstants.DEFAULT_NUM_PARTITIONS);
  String logDir=cConf.get(KafkaConstants.ConfigKeys.LOG_DIR_CONFIG);
  int replicationFactor=cConf.getInt(KafkaConstants.ConfigKeys.REPLICATION_FACTOR,KafkaConstants.DEFAULT_REPLICATION_FACTOR);
  LOG.info(""String_Node_Str"",replicationFactor);
  if (zkNamespace != null) {
    ZKClientService client=ZKClientService.Builder.of(zkConnectStr).build();
    try {
      client.startAndWait();
      String path=""String_Node_Str"" + zkNamespace;
      LOG.info(String.format(""String_Node_Str"",path));
      ZKOperations.ignoreError(client.create(path,null,CreateMode.PERSISTENT),KeeperException.NodeExistsException.class,path).get();
      client.stopAndWait();
      zkConnectStr=String.format(""String_Node_Str"",zkConnectStr,zkNamespace);
    }
 catch (    Exception e) {
      throw Throwables.propagate(e);
    }
 finally {
      client.stopAndWait();
    }
  }
  kafkaProperties=generateKafkaConfig(brokerId,zkConnectStr,hostname,port,numPartitions,replicationFactor,logDir);
}","@Override public void init(String[] args){
  int brokerId=generateBrokerId();
  LOG.info(String.format(""String_Node_Str"",brokerId));
  CConfiguration cConf=CConfiguration.create();
  String zkConnectStr=cConf.get(Constants.CFG_ZOOKEEPER_ENSEMBLE);
  String zkNamespace=cConf.get(KafkaConstants.ConfigKeys.ZOOKEEPER_NAMESPACE_CONFIG);
  int port=cConf.getInt(KafkaConstants.ConfigKeys.PORT_CONFIG,-1);
  String hostname=cConf.get(KafkaConstants.ConfigKeys.HOSTNAME_CONFIG);
  if (hostname != null) {
    InetSocketAddress socketAddress=new InetSocketAddress(hostname,0);
    InetAddress address=socketAddress.getAddress();
    if (address.isAnyLocalAddress()) {
      try {
        hostname=InetAddress.getLocalHost().getCanonicalHostName();
      }
 catch (      UnknownHostException e) {
        throw Throwables.propagate(e);
      }
    }
  }
  int numPartitions=cConf.getInt(KafkaConstants.ConfigKeys.NUM_PARTITIONS_CONFIG,KafkaConstants.DEFAULT_NUM_PARTITIONS);
  String logDir=cConf.get(KafkaConstants.ConfigKeys.LOG_DIR_CONFIG);
  int replicationFactor=cConf.getInt(KafkaConstants.ConfigKeys.REPLICATION_FACTOR,KafkaConstants.DEFAULT_REPLICATION_FACTOR);
  LOG.info(""String_Node_Str"",replicationFactor);
  if (zkNamespace != null) {
    ZKClientService client=ZKClientService.Builder.of(zkConnectStr).build();
    try {
      client.startAndWait();
      String path=""String_Node_Str"" + zkNamespace;
      LOG.info(String.format(""String_Node_Str"",path));
      ZKOperations.ignoreError(client.create(path,null,CreateMode.PERSISTENT),KeeperException.NodeExistsException.class,path).get();
      client.stopAndWait();
      zkConnectStr=String.format(""String_Node_Str"",zkConnectStr,zkNamespace);
    }
 catch (    Exception e) {
      throw Throwables.propagate(e);
    }
 finally {
      client.stopAndWait();
    }
  }
  kafkaProperties=generateKafkaConfig(brokerId,zkConnectStr,hostname,port,numPartitions,replicationFactor,logDir);
}"
7852,"@Override public void stop(){
  LOG.info(""String_Node_Str"");
  if (kafkaServer != null) {
    kafkaServer.stopAndWait();
  }
}","@Override public void stop(){
  LOG.info(""String_Node_Str"");
  if (kafkaServer != null && kafkaServer.isRunning()) {
    kafkaServer.stopAndWait();
  }
}"
7853,"@Override public void start(){
  LOG.info(""String_Node_Str"");
  kafkaServer=new EmbeddedKafkaServer(KafkaServerMain.class.getClassLoader(),kafkaProperties);
  kafkaServer.startAndWait();
  LOG.info(""String_Node_Str"");
}","@Override public void start(){
  LOG.info(""String_Node_Str"");
  kafkaServer=new EmbeddedKafkaServer(KafkaServerMain.class.getClassLoader(),kafkaProperties);
  Service.State state=kafkaServer.startAndWait();
  if (state != Service.State.RUNNING) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  LOG.info(""String_Node_Str"");
}"
7854,"@Override protected OrderedVersionedColumnarTable createNewTable(byte[] tableName) throws OperationException {
  LevelDBOVCTable table=new LevelDBOVCTable(basePath,Bytes.toString(tableName),blockSize,cacheSize);
  table.initializeTable();
  return table;
}","@Override protected synchronized OrderedVersionedColumnarTable createNewTable(byte[] tableName) throws OperationException {
  LevelDBOVCTable table=new LevelDBOVCTable(basePath,Bytes.toString(tableName),blockSize,cacheSize);
  if (table.openTable()) {
    return table;
  }
  table.initializeTable();
  return table;
}"
7855,"@Override protected OrderedVersionedColumnarTable createNewTable(byte[] tableName) throws OperationException {
  LevelDBOVCTable table=new LevelDBOVCTable(basePath,Bytes.toString(tableName),blockSize,cacheSize);
  table.initializeTable();
  return table;
}","@Override protected synchronized OrderedVersionedColumnarTable createNewTable(byte[] tableName) throws OperationException {
  LevelDBOVCTable table=new LevelDBOVCTable(basePath,Bytes.toString(tableName),blockSize,cacheSize);
  if (table.openTable()) {
    return table;
  }
  table.initializeTable();
  return table;
}"
7856,"private Counter getCounter(String counterPath){
  String json=""String_Node_Str"" + counterPath + ""String_Node_Str"";
  LOG.info(""String_Node_Str"" + json);
  String response=sendJSonPostRequest(url,json,null);
  List<MetricsResponse> responseData=GSON.fromJson(response,new TypeToken<List<MetricsResponse>>(){
  }
.getType());
  if (responseData == null || responseData.size() == 0) {
    throw new RuntimeException(String.format(""String_Node_Str"",counterPath));
  }
  if (responseData.size() > 1) {
    throw new RuntimeException(String.format(""String_Node_Str"",counterPath));
  }
  return new Counter(responseData.get(0).getPath(),responseData.get(0).getResult().getData());
}","private Counter getCounter(String counterPath){
  String json=""String_Node_Str"" + counterPath + ""String_Node_Str"";
  if (LOG.isDebugEnabled()) {
    LOG.debug(""String_Node_Str"",json);
  }
  String response=sendJSonPostRequest(url,json,null);
  List<MetricsResponse> responseData=GSON.fromJson(response,new TypeToken<List<MetricsResponse>>(){
  }
.getType());
  if (responseData == null || responseData.size() == 0) {
    throw new RuntimeException(String.format(""String_Node_Str"",counterPath));
  }
  if (responseData.size() > 1) {
    throw new RuntimeException(String.format(""String_Node_Str"",counterPath));
  }
  return new Counter(responseData.get(0).getPath(),responseData.get(0).getResult().getData());
}"
7857,"private ConsumerThread(TopicPartition topicPart,long startOffset,MessageCallback callback){
  this.topicPart=topicPart;
  this.startOffset=startOffset;
  this.callback=callback;
  this.running=true;
  this.fetchedMessage=new BasicFetchedMessage(topicPart);
}","private ConsumerThread(TopicPartition topicPart,long startOffset,MessageCallback callback){
  super(String.format(""String_Node_Str"",topicPart.getTopic(),topicPart.getPartition()));
  this.topicPart=topicPart;
  this.startOffset=startOffset;
  this.callback=callback;
  this.running=true;
  this.fetchedMessage=new BasicFetchedMessage(topicPart);
}"
7858,"/** 
 * Makes a call to kafka to fetch messages.
 */
private FetchResponse fetchMessages(SimpleConsumer consumer,long offset){
  FetchRequest request=new FetchRequestBuilder().clientId(consumer.clientId()).addFetch(topicPart.getTopic(),topicPart.getPartition(),offset,FETCH_SIZE).build();
  return consumer.fetch(request);
}","/** 
 * Makes a call to kafka to fetch messages.
 */
private FetchResponse fetchMessages(SimpleConsumer consumer,long offset){
  FetchRequest request=new FetchRequestBuilder().clientId(consumer.clientId()).addFetch(topicPart.getTopic(),topicPart.getPartition(),offset,FETCH_SIZE).maxWait(1000).build();
  return consumer.fetch(request);
}"
7859,"public void terminate(){
  running=false;
  interrupt();
}","public void terminate(){
  LOG.info(""String_Node_Str"",getName());
  running=false;
  interrupt();
}"
7860,"private void subscribe(){
  LOG.info(""String_Node_Str"");
  KafkaConsumer.Preparer preparer=kafkaClient.getConsumer().prepare();
  for (  MetricsScope scope : MetricsScope.values()) {
    String topic=topicPrefix + ""String_Node_Str"" + scope.name();
    for (int i=0; i < partitionSize; i++) {
      long offset=getOffset(topic,i);
      if (offset >= 0) {
        preparer.add(topic,i,offset);
      }
 else {
        preparer.addFromBeginning(topic,i);
      }
    }
    unsubscribes.add(preparer.consume(callbackFactory.create(metaTable,scope)));
    LOG.info(""String_Node_Str"",topic,partitionSize);
  }
  LOG.info(""String_Node_Str"");
}","private void subscribe(){
  LOG.info(""String_Node_Str"");
  for (  MetricsScope scope : MetricsScope.values()) {
    KafkaConsumer.Preparer preparer=kafkaClient.getConsumer().prepare();
    String topic=topicPrefix + ""String_Node_Str"" + scope.name();
    for (int i=0; i < partitionSize; i++) {
      long offset=getOffset(topic,i);
      if (offset >= 0) {
        preparer.add(topic,i,offset);
      }
 else {
        preparer.addFromBeginning(topic,i);
      }
    }
    unsubscribes.add(preparer.consume(callbackFactory.create(metaTable,scope)));
    LOG.info(""String_Node_Str"",topic,partitionSize);
  }
  LOG.info(""String_Node_Str"");
}"
7861,"@Override public void onReceived(Iterator<FetchedMessage> messages){
  final ByteBufferInputStream is=new ByteBufferInputStream(null);
  List<MetricsRecord> records=ImmutableList.copyOf(Iterators.filter(Iterators.transform(messages,new Function<FetchedMessage,MetricsRecord>(){
    @Override public MetricsRecord apply(    FetchedMessage input){
      try {
        return recordReader.read(new BinaryDecoder(is.reset(input.getPayload())),recordSchema);
      }
 catch (      IOException e) {
        LOG.warn(""String_Node_Str"",e.getMessage(),e);
        return null;
      }
    }
  }
),Predicates.notNull()));
  for (  MetricsProcessor processor : processors) {
    processor.process(scope,records.iterator());
  }
}","@Override public void onReceived(Iterator<FetchedMessage> messages){
  final ByteBufferInputStream is=new ByteBufferInputStream(null);
  List<MetricsRecord> records=ImmutableList.copyOf(Iterators.filter(Iterators.transform(messages,new Function<FetchedMessage,MetricsRecord>(){
    @Override public MetricsRecord apply(    FetchedMessage input){
      try {
        return recordReader.read(new BinaryDecoder(is.reset(input.getPayload())),recordSchema);
      }
 catch (      IOException e) {
        LOG.warn(""String_Node_Str"",e.getMessage(),e);
        return null;
      }
    }
  }
),Predicates.notNull()));
  if (records.isEmpty()) {
    LOG.info(""String_Node_Str"");
    return;
  }
  for (  MetricsProcessor processor : processors) {
    processor.process(scope,records.iterator());
  }
  recordProcessed+=records.size();
  if (recordProcessed % 1000 == 0) {
    LOG.info(""String_Node_Str"",scope,recordProcessed);
    LOG.info(""String_Node_Str"",records.get(records.size() - 1).getTimestamp());
  }
}"
7862,"private ConsumerThread(TopicPartition topicPart,long startOffset,MessageCallback callback){
  this.topicPart=topicPart;
  this.startOffset=startOffset;
  this.callback=callback;
  this.running=true;
  this.fetchedMessage=new BasicFetchedMessage(topicPart);
}","private ConsumerThread(TopicPartition topicPart,long startOffset,MessageCallback callback){
  super(String.format(""String_Node_Str"",topicPart.getTopic(),topicPart.getPartition()));
  this.topicPart=topicPart;
  this.startOffset=startOffset;
  this.callback=callback;
  this.running=true;
  this.fetchedMessage=new BasicFetchedMessage(topicPart);
}"
7863,"/** 
 * Makes a call to kafka to fetch messages.
 */
private FetchResponse fetchMessages(SimpleConsumer consumer,long offset){
  FetchRequest request=new FetchRequestBuilder().clientId(consumer.clientId()).addFetch(topicPart.getTopic(),topicPart.getPartition(),offset,FETCH_SIZE).build();
  return consumer.fetch(request);
}","/** 
 * Makes a call to kafka to fetch messages.
 */
private FetchResponse fetchMessages(SimpleConsumer consumer,long offset){
  FetchRequest request=new FetchRequestBuilder().clientId(consumer.clientId()).addFetch(topicPart.getTopic(),topicPart.getPartition(),offset,FETCH_SIZE).maxWait(1000).build();
  return consumer.fetch(request);
}"
7864,"public void terminate(){
  running=false;
  interrupt();
}","public void terminate(){
  LOG.info(""String_Node_Str"",getName());
  running=false;
  interrupt();
}"
7865,"private void subscribe(){
  LOG.info(""String_Node_Str"");
  KafkaConsumer.Preparer preparer=kafkaClient.getConsumer().prepare();
  for (  MetricsScope scope : MetricsScope.values()) {
    String topic=topicPrefix + ""String_Node_Str"" + scope.name();
    for (int i=0; i < partitionSize; i++) {
      long offset=getOffset(topic,i);
      if (offset >= 0) {
        preparer.add(topic,i,offset);
      }
 else {
        preparer.addFromBeginning(topic,i);
      }
    }
    unsubscribes.add(preparer.consume(callbackFactory.create(metaTable,scope)));
    LOG.info(""String_Node_Str"",topic,partitionSize);
  }
  LOG.info(""String_Node_Str"");
}","private void subscribe(){
  LOG.info(""String_Node_Str"");
  for (  MetricsScope scope : MetricsScope.values()) {
    KafkaConsumer.Preparer preparer=kafkaClient.getConsumer().prepare();
    String topic=topicPrefix + ""String_Node_Str"" + scope.name();
    for (int i=0; i < partitionSize; i++) {
      long offset=getOffset(topic,i);
      if (offset >= 0) {
        preparer.add(topic,i,offset);
      }
 else {
        preparer.addFromBeginning(topic,i);
      }
    }
    unsubscribes.add(preparer.consume(callbackFactory.create(metaTable,scope)));
    LOG.info(""String_Node_Str"",topic,partitionSize);
  }
  LOG.info(""String_Node_Str"");
}"
7866,"@Override public void onReceived(Iterator<FetchedMessage> messages){
  final ByteBufferInputStream is=new ByteBufferInputStream(null);
  List<MetricsRecord> records=ImmutableList.copyOf(Iterators.filter(Iterators.transform(messages,new Function<FetchedMessage,MetricsRecord>(){
    @Override public MetricsRecord apply(    FetchedMessage input){
      try {
        return recordReader.read(new BinaryDecoder(is.reset(input.getPayload())),recordSchema);
      }
 catch (      IOException e) {
        LOG.warn(""String_Node_Str"",e.getMessage(),e);
        return null;
      }
    }
  }
),Predicates.notNull()));
  for (  MetricsProcessor processor : processors) {
    processor.process(scope,records.iterator());
  }
}","@Override public void onReceived(Iterator<FetchedMessage> messages){
  final ByteBufferInputStream is=new ByteBufferInputStream(null);
  List<MetricsRecord> records=ImmutableList.copyOf(Iterators.filter(Iterators.transform(messages,new Function<FetchedMessage,MetricsRecord>(){
    @Override public MetricsRecord apply(    FetchedMessage input){
      try {
        return recordReader.read(new BinaryDecoder(is.reset(input.getPayload())),recordSchema);
      }
 catch (      IOException e) {
        LOG.warn(""String_Node_Str"",e.getMessage(),e);
        return null;
      }
    }
  }
),Predicates.notNull()));
  if (records.isEmpty()) {
    LOG.info(""String_Node_Str"");
    return;
  }
  for (  MetricsProcessor processor : processors) {
    processor.process(scope,records.iterator());
  }
  recordProcessed+=records.size();
  if (recordProcessed % 1000 == 0) {
    LOG.info(""String_Node_Str"",scope,recordProcessed);
    LOG.info(""String_Node_Str"",records.get(records.size() - 1).getTimestamp());
  }
}"
7867,"@Override public void cancel(){
  if (!cancelled.compareAndSet(false,true)) {
    return;
  }
  consumerCancels.remove(this);
  for (  ConsumerThread consumerThread : pollers) {
    consumerThread.terminate();
  }
  for (  ConsumerThread consumerThread : pollers) {
    try {
      consumerThread.join();
    }
 catch (    InterruptedException e) {
      LOG.warn(""String_Node_Str"",e);
    }
  }
  executor.shutdown();
}","@Override public void cancel(){
  if (!cancelled.compareAndSet(false,true)) {
    return;
  }
  consumerCancels.remove(this);
  LOG.info(""String_Node_Str"");
  for (  ConsumerThread consumerThread : pollers) {
    consumerThread.terminate();
  }
  LOG.info(""String_Node_Str"");
  for (  ConsumerThread consumerThread : pollers) {
    try {
      consumerThread.join();
    }
 catch (    InterruptedException e) {
      LOG.warn(""String_Node_Str"",e);
    }
  }
  LOG.info(""String_Node_Str"");
  executor.shutdown();
}"
7868,"@Override public Cancellable consume(MessageCallback callback){
  final ExecutorService executor=Executors.newSingleThreadExecutor(threadFactory);
  final List<ConsumerThread> pollers=Lists.newArrayList();
  final AtomicBoolean cancelled=new AtomicBoolean();
  Cancellable cancellable=new Cancellable(){
    @Override public void cancel(){
      if (!cancelled.compareAndSet(false,true)) {
        return;
      }
      consumerCancels.remove(this);
      for (      ConsumerThread consumerThread : pollers) {
        consumerThread.terminate();
      }
      for (      ConsumerThread consumerThread : pollers) {
        try {
          consumerThread.join();
        }
 catch (        InterruptedException e) {
          LOG.warn(""String_Node_Str"",e);
        }
      }
      executor.shutdown();
    }
  }
;
  MessageCallback messageCallback=wrapCallback(callback,executor,cancellable);
  for (  Map.Entry<TopicPartition,Long> entry : requests.entrySet()) {
    ConsumerThread consumerThread=new ConsumerThread(entry.getKey(),entry.getValue(),messageCallback);
    consumerThread.setDaemon(true);
    consumerThread.start();
    pollers.add(consumerThread);
  }
  consumerCancels.add(cancellable);
  return cancellable;
}","@Override public Cancellable consume(MessageCallback callback){
  final ExecutorService executor=Executors.newSingleThreadExecutor(threadFactory);
  final List<ConsumerThread> pollers=Lists.newArrayList();
  final AtomicBoolean cancelled=new AtomicBoolean();
  Cancellable cancellable=new Cancellable(){
    @Override public void cancel(){
      if (!cancelled.compareAndSet(false,true)) {
        return;
      }
      consumerCancels.remove(this);
      LOG.info(""String_Node_Str"");
      for (      ConsumerThread consumerThread : pollers) {
        consumerThread.terminate();
      }
      LOG.info(""String_Node_Str"");
      for (      ConsumerThread consumerThread : pollers) {
        try {
          consumerThread.join();
        }
 catch (        InterruptedException e) {
          LOG.warn(""String_Node_Str"",e);
        }
      }
      LOG.info(""String_Node_Str"");
      executor.shutdown();
    }
  }
;
  MessageCallback messageCallback=wrapCallback(callback,executor,cancellable);
  for (  Map.Entry<TopicPartition,Long> entry : requests.entrySet()) {
    ConsumerThread consumerThread=new ConsumerThread(entry.getKey(),entry.getValue(),messageCallback);
    consumerThread.setDaemon(true);
    consumerThread.start();
    pollers.add(consumerThread);
  }
  consumerCancels.add(cancellable);
  return cancellable;
}"
7869,"@Override public void run(){
  final AtomicLong offset=new AtomicLong(startOffset);
  Map.Entry<BrokerInfo,SimpleConsumer> consumerEntry=null;
  Throwable errorCause=null;
  while (running) {
    if (consumerEntry == null && (consumerEntry=getConsumerEntry()) == null) {
      LOG.error(""String_Node_Str"",topicPart,FAILURE_RETRY_INTERVAL);
      Uninterruptibles.sleepUninterruptibly(FAILURE_RETRY_INTERVAL,TimeUnit.MILLISECONDS);
      continue;
    }
    SimpleConsumer consumer=consumerEntry.getValue();
    FetchResponse response=fetchMessages(consumer,offset.get());
    if (response.hasError()) {
      short errorCode=response.errorCode(topicPart.getTopic(),topicPart.getPartition());
      LOG.info(""String_Node_Str"",topicPart,errorCode);
      if (errorCode == ErrorMapping.OffsetOutOfRangeCode()) {
        errorCause=new OffsetOutOfRangeException(""String_Node_Str"" + offset.get());
        break;
      }
      consumers.refresh(consumerEntry.getKey());
      consumerEntry=null;
      continue;
    }
    ByteBufferMessageSet messages=response.messageSet(topicPart.getTopic(),topicPart.getPartition());
    if (sleepIfEmpty(messages)) {
      continue;
    }
    invokeCallback(messages,offset);
  }
  try {
    callback.finished(running,errorCause);
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",running,t);
  }
}","@Override public void run(){
  final AtomicLong offset=new AtomicLong(startOffset);
  Map.Entry<BrokerInfo,SimpleConsumer> consumerEntry=null;
  Throwable errorCause=null;
  while (running) {
    if (consumerEntry == null && (consumerEntry=getConsumerEntry()) == null) {
      try {
        TimeUnit.MICROSECONDS.sleep(CONSUMER_FAILER_RETRY_INTERVAL);
      }
 catch (      InterruptedException e) {
        LOG.debug(""String_Node_Str"",e);
      }
      continue;
    }
    SimpleConsumer consumer=consumerEntry.getValue();
    FetchResponse response=fetchMessages(consumer,offset.get());
    if (response.hasError()) {
      short errorCode=response.errorCode(topicPart.getTopic(),topicPart.getPartition());
      LOG.info(""String_Node_Str"",topicPart,errorCode);
      if (errorCode == ErrorMapping.OffsetOutOfRangeCode()) {
        errorCause=new OffsetOutOfRangeException(""String_Node_Str"" + offset.get());
        break;
      }
      consumers.refresh(consumerEntry.getKey());
      consumerEntry=null;
      continue;
    }
    ByteBufferMessageSet messages=response.messageSet(topicPart.getTopic(),topicPart.getPartition());
    if (sleepIfEmpty(messages)) {
      continue;
    }
    invokeCallback(messages,offset);
  }
  try {
    callback.finished(running,errorCause);
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",running,t);
  }
}"
7870,"private void subscribe(){
  KafkaConsumer.Preparer preparer=kafkaClient.getConsumer().prepare();
  for (  MetricsScope scope : MetricsScope.values()) {
    String topic=topicPrefix + ""String_Node_Str"" + scope.name();
    for (int i=0; i < partitionSize; i++) {
      long offset=getOffset(topic,i);
      if (offset >= 0) {
        preparer.add(topic,i,offset);
      }
 else {
        preparer.addFromBeginning(topic,i);
      }
    }
    unsubscribes.add(preparer.consume(callbackFactory.create(metaTable,scope)));
    LOG.info(""String_Node_Str"",topic);
  }
}","private void subscribe(){
  LOG.info(""String_Node_Str"");
  KafkaConsumer.Preparer preparer=kafkaClient.getConsumer().prepare();
  for (  MetricsScope scope : MetricsScope.values()) {
    String topic=topicPrefix + ""String_Node_Str"" + scope.name();
    for (int i=0; i < partitionSize; i++) {
      long offset=getOffset(topic,i);
      if (offset >= 0) {
        preparer.add(topic,i,offset);
      }
 else {
        preparer.addFromBeginning(topic,i);
      }
    }
    unsubscribes.add(preparer.consume(callbackFactory.create(metaTable,scope)));
    LOG.info(""String_Node_Str"",topic,partitionSize);
  }
  LOG.info(""String_Node_Str"");
}"
7871,"private long getOffset(String topic,int partition){
  try {
    return metaTable.get(new TopicPartition(topic,partition));
  }
 catch (  OperationException e) {
    LOG.error(""String_Node_Str"",e.getMessage(),e);
  }
  return -1;
}","private long getOffset(String topic,int partition){
  LOG.info(""String_Node_Str"",topic,partition);
  try {
    long offset=metaTable.get(new TopicPartition(topic,partition));
    LOG.info(""String_Node_Str"",topic,partition,offset);
    return offset;
  }
 catch (  OperationException e) {
    LOG.error(""String_Node_Str"",e.getMessage(),e);
  }
  return -1L;
}"
7872,"private void checkPoint(boolean force) throws IOException, OperationException {
  long currentTs=System.currentTimeMillis();
  if (!force && currentTs - lastCheckpointTime < checkpointIntervalMs) {
    return;
  }
  long checkpointOffset=Long.MAX_VALUE;
  Set<String> files=Sets.newHashSetWithExpectedSize(fileMap.size());
  for (Iterator<Map.Entry<String,AvroFile>> it=fileMap.entrySet().iterator(); it.hasNext(); ) {
    AvroFile avroFile=it.next().getValue();
    avroFile.flush();
    if (currentTs - avroFile.getLastModifiedTs() > inactiveIntervalMs) {
      avroFile.close();
      it.remove();
    }
    files.add(avroFile.getPath().toUri().toString());
    if (checkpointOffset > avroFile.getMaxOffsetSeen()) {
      checkpointOffset=avroFile.getMaxOffsetSeen();
    }
  }
  if (checkpointOffset != Long.MAX_VALUE) {
    checkpointManager.saveCheckpoint(new CheckpointInfo(checkpointOffset,files));
  }
  lastCheckpointTime=currentTs;
}","private void checkPoint(boolean force) throws IOException, OperationException {
  long currentTs=System.currentTimeMillis();
  if (!force && currentTs - lastCheckpointTime < checkpointIntervalMs) {
    return;
  }
  long checkpointOffset=Long.MAX_VALUE;
  Set<String> files=Sets.newHashSetWithExpectedSize(fileMap.size());
  for (Iterator<Map.Entry<String,AvroFile>> it=fileMap.entrySet().iterator(); it.hasNext(); ) {
    AvroFile avroFile=it.next().getValue();
    avroFile.flush();
    if (currentTs - avroFile.getLastModifiedTs() > inactiveIntervalMs) {
      avroFile.close();
      it.remove();
    }
    files.add(avroFile.getPath().toUri().toString());
    if (checkpointOffset > avroFile.getMaxOffsetSeen()) {
      checkpointOffset=avroFile.getMaxOffsetSeen();
    }
  }
  if (checkpointOffset != Long.MAX_VALUE) {
    LOG.info(String.format(""String_Node_Str"",checkpointOffset,files.size()));
    checkpointManager.saveCheckpoint(new CheckpointInfo(checkpointOffset,files));
  }
  lastCheckpointTime=currentTs;
}"
7873,"@Override public void close() throws IOException {
  if (dataFileWriter != null) {
    dataFileWriter.close();
  }
  if (outputStream != null) {
    outputStream.close();
  }
}","@Override public void close() throws IOException {
  try {
    if (dataFileWriter != null) {
      dataFileWriter.close();
    }
  }
  finally {
    if (outputStream != null) {
      outputStream.close();
    }
  }
}"
7874,"private void rotateFile(AvroFile avroFile,LoggingContext loggingContext,long timestamp) throws IOException, OperationException {
  if (avroFile.getPos() > maxFileSize) {
    avroFile.close();
    createAvroFile(loggingContext,timestamp);
    checkPoint(true);
  }
}","private void rotateFile(AvroFile avroFile,LoggingContext loggingContext,long timestamp) throws IOException, OperationException {
  if (avroFile.getPos() > maxFileSize) {
    LOG.info(String.format(""String_Node_Str"",avroFile.getPath()));
    avroFile.close();
    createAvroFile(loggingContext,timestamp);
    checkPoint(true);
  }
}"
7875,"@Override public void run(){
  waitForRun();
  LOG.info(String.format(""String_Node_Str"",topic,partition));
  AvroFileWriter avroFileWriter=new AvroFileWriter(checkpointManager,fileMetaDataManager,fileSystem,logBaseDir,serializer.getAvroSchema(),maxLogFileSizeBytes,syncIntervalBytes,checkpointIntervalMs,inactiveIntervalMs);
  List<List<KafkaLogEvent>> writeLists=Lists.newArrayList();
  try {
    while (isRunning()) {
      int messages=0;
      try {
        if (writeLists.isEmpty()) {
          long processKey=(System.currentTimeMillis() - eventProcessingDelayMs) / eventProcessingDelayMs;
synchronized (messageTable) {
            for (Iterator<Table.Cell<Long,String,List<KafkaLogEvent>>> it=messageTable.cellSet().iterator(); it.hasNext(); ) {
              Table.Cell<Long,String,List<KafkaLogEvent>> cell=it.next();
              if (cell.getRowKey() >= processKey) {
                continue;
              }
              writeLists.add(cell.getValue());
              it.remove();
              messages+=cell.getValue().size();
            }
          }
        }
        if (writeLists.isEmpty()) {
          LOG.info(String.format(""String_Node_Str"",topic,partition,kafkaEmptySleepMs));
          TimeUnit.MILLISECONDS.sleep(kafkaEmptySleepMs);
        }
        LOG.info(String.format(""String_Node_Str"",messages,topic,partition));
        for (Iterator<List<KafkaLogEvent>> it=writeLists.iterator(); it.hasNext(); ) {
          avroFileWriter.append(it.next());
          it.remove();
        }
      }
 catch (      Throwable e) {
        LOG.error(String.format(""String_Node_Str"",topic,partition,kafkaErrorSleepMs),e);
        try {
          TimeUnit.MILLISECONDS.sleep(kafkaErrorSleepMs);
        }
 catch (        InterruptedException e1) {
          LOG.error(String.format(""String_Node_Str"",topic,partition),e1);
          Thread.currentThread().interrupt();
        }
      }
    }
    LOG.info(String.format(""String_Node_Str"",topic,partition));
  }
  finally {
    try {
      avroFileWriter.close();
      fileSystem.close();
    }
 catch (    IOException e) {
      LOG.error(String.format(""String_Node_Str"",topic,partition),e);
    }
  }
}","@Override public void run(){
  waitForRun();
  LOG.info(String.format(""String_Node_Str"",topic,partition));
  AvroFileWriter avroFileWriter=new AvroFileWriter(checkpointManager,fileMetaDataManager,fileSystem,logBaseDir,serializer.getAvroSchema(),maxLogFileSizeBytes,syncIntervalBytes,checkpointIntervalMs,inactiveIntervalMs);
  List<List<KafkaLogEvent>> writeLists=Lists.newArrayList();
  try {
    while (isRunning()) {
      int messages=0;
      try {
        if (writeLists.isEmpty()) {
          long processKey=(System.currentTimeMillis() - eventProcessingDelayMs) / eventProcessingDelayMs;
synchronized (messageTable) {
            for (Iterator<Table.Cell<Long,String,List<KafkaLogEvent>>> it=messageTable.cellSet().iterator(); it.hasNext(); ) {
              Table.Cell<Long,String,List<KafkaLogEvent>> cell=it.next();
              if (cell.getRowKey() >= processKey) {
                continue;
              }
              writeLists.add(cell.getValue());
              it.remove();
              messages+=cell.getValue().size();
            }
          }
        }
        if (writeLists.isEmpty()) {
          LOG.info(String.format(""String_Node_Str"",topic,partition,kafkaEmptySleepMs));
          TimeUnit.MILLISECONDS.sleep(kafkaEmptySleepMs);
        }
        LOG.info(String.format(""String_Node_Str"",messages,topic,partition));
        for (Iterator<List<KafkaLogEvent>> it=writeLists.iterator(); it.hasNext(); ) {
          avroFileWriter.append(it.next());
          it.remove();
        }
      }
 catch (      Throwable e) {
        LOG.error(String.format(""String_Node_Str"",topic,partition,kafkaErrorSleepMs),e);
        try {
          TimeUnit.MILLISECONDS.sleep(kafkaErrorSleepMs);
        }
 catch (        InterruptedException e1) {
          LOG.error(String.format(""String_Node_Str"",topic,partition),e1);
          Thread.currentThread().interrupt();
        }
      }
    }
    LOG.info(String.format(""String_Node_Str"",topic,partition));
  }
  finally {
    try {
      try {
        avroFileWriter.close();
      }
  finally {
        fileSystem.close();
      }
    }
 catch (    IOException e) {
      LOG.error(String.format(""String_Node_Str"",topic,partition),e);
    }
  }
}"
7876,"@Override public void stop(){
  weaveController.stopAndWait();
}","@Override public void stop(){
  if (weaveController != null) {
    weaveController.stopAndWait();
  }
}"
7877,"@Override public void destroy(){
  weaveRunnerService.stopAndWait();
}","@Override public void destroy(){
  if (weaveRunnerService != null) {
    weaveRunnerService.stopAndWait();
  }
}"
7878,"/** 
 * Returns a list of log files for a logging context.
 * @param loggingContext logging context.
 * @return Sorted map containing key as start time, and value as log file.
 * @throws OperationException
 */
public SortedMap<Long,Path> listFiles(LoggingContext loggingContext) throws OperationException {
  OperationResult<Map<byte[],byte[]>> cols=opex.execute(operationContext,new ReadColumnRange(Bytes.add(ROW_KEY_PREFIX,Bytes.toBytes(loggingContext.getLogPartition())),Bytes.toBytes(0)));
  if (cols.isEmpty() || cols.getValue() != null) {
    return ImmutableSortedMap.of();
  }
  SortedMap<Long,Path> files=Maps.newTreeMap();
  for (  Map.Entry<byte[],byte[]> entry : cols.getValue().entrySet()) {
    files.put(Bytes.toLong(entry.getKey()),new Path(Bytes.toString(entry.getValue())));
  }
  return files;
}","/** 
 * Returns a list of log files for a logging context.
 * @param loggingContext logging context.
 * @return Sorted map containing key as start time, and value as log file.
 * @throws OperationException
 */
public SortedMap<Long,Path> listFiles(LoggingContext loggingContext) throws OperationException {
  OperationResult<Map<byte[],byte[]>> cols=opex.execute(operationContext,new ReadColumnRange(table,Bytes.add(ROW_KEY_PREFIX,Bytes.toBytes(loggingContext.getLogPartition())),null,null));
  if (cols.isEmpty() || cols.getValue() == null) {
    return ImmutableSortedMap.of();
  }
  SortedMap<Long,Path> files=Maps.newTreeMap();
  for (  Map.Entry<byte[],byte[]> entry : cols.getValue().entrySet()) {
    files.put(Bytes.toLong(entry.getKey()),new Path(Bytes.toString(entry.getValue())));
  }
  return files;
}"
7879,"public synchronized static int getTracker(String table,String op){
  String key=table + ""String_Node_Str"" + op;
  String value=System.getProperty(key,""String_Node_Str"");
  return Integer.valueOf(value);
}","public static synchronized int getTracker(String table,String op){
  String key=table + ""String_Node_Str"" + op;
  String value=System.getProperty(key,""String_Node_Str"");
  return Integer.valueOf(value);
}"
7880,"private synchronized static void track(String table,String op){
  String key=table + ""String_Node_Str"" + op;
  String value=System.getProperty(key,""String_Node_Str"");
  String newValue=Integer.toString(Integer.valueOf(value) + 1);
  System.setProperty(key,newValue);
}","private static synchronized void track(String table,String op){
  String key=table + ""String_Node_Str"" + op;
  String value=System.getProperty(key,""String_Node_Str"");
  String newValue=Integer.toString(Integer.valueOf(value) + 1);
  System.setProperty(key,newValue);
}"
7881,"public synchronized static void resetTracker(){
  for (  String table : Arrays.asList(""String_Node_Str"",""String_Node_Str"")) {
    for (    String op : Arrays.asList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")) {
      String key=table + ""String_Node_Str"" + op;
      System.clearProperty(key);
    }
  }
}","public static synchronized void resetTracker(){
  for (  String table : Arrays.asList(""String_Node_Str"",""String_Node_Str"")) {
    for (    String op : Arrays.asList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")) {
      String key=table + ""String_Node_Str"" + op;
      System.clearProperty(key);
    }
  }
}"
7882,"@Override public Flow makeFromEntry(MetaDataEntry entry){
  Flow fl=new Flow(entry.getId(),entry.getApplication());
  fl.setName(entry.getTextField(FieldTypes.Flow.NAME));
  fl.setStreams(StringToList(entry.getTextField(FieldTypes.Flow.STREAMS)));
  fl.setDatasets(StringToList(entry.getTextField(FieldTypes.Flow.DATASETS)));
  return fl;
}","@Override public Flow makeFromEntry(MetaDataEntry entry){
  Flow fl=new Flow(entry.getId(),entry.getApplication());
  fl.setName(entry.getTextField(FieldTypes.Flow.NAME));
  fl.setStreams(stringToList(entry.getTextField(FieldTypes.Flow.STREAMS)));
  fl.setDatasets(stringToList(entry.getTextField(FieldTypes.Flow.DATASETS)));
  return fl;
}"
7883,"@Override public MetaDataEntry makeEntry(Account account,Flow flow){
  MetaDataEntry entry=new MetaDataEntry(account.getId(),flow.getApplication(),FieldTypes.Flow.ID,flow.getId());
  entry.addField(FieldTypes.Flow.NAME,flow.getName());
  entry.addField(FieldTypes.Flow.STREAMS,ListToString(flow.getStreams()));
  entry.addField(FieldTypes.Flow.DATASETS,ListToString(flow.getDatasets()));
  return entry;
}","@Override public MetaDataEntry makeEntry(Account account,Flow flow){
  MetaDataEntry entry=new MetaDataEntry(account.getId(),flow.getApplication(),FieldTypes.Flow.ID,flow.getId());
  entry.addField(FieldTypes.Flow.NAME,flow.getName());
  entry.addField(FieldTypes.Flow.STREAMS,listToString(flow.getStreams()));
  entry.addField(FieldTypes.Flow.DATASETS,listToString(flow.getDatasets()));
  return entry;
}"
7884,"@Override public List<Mapreduce> getMapreducesByDataset(String account,String dataset) throws MetadataServiceException, TException {
  validateAccount(account);
  List<Mapreduce> mapreduces=getMapreduces(new Account(account));
  List<Mapreduce> queriesForDS=Lists.newLinkedList();
  for (  Mapreduce mapreduce : mapreduces) {
    if (mapreduce.getDatasets().contains(dataset))     queriesForDS.add(mapreduce);
  }
  return queriesForDS;
}","@Override public List<Mapreduce> getMapreducesByDataset(String account,String dataset) throws MetadataServiceException, TException {
  validateAccount(account);
  List<Mapreduce> mapreduces=getMapreduces(new Account(account));
  List<Mapreduce> queriesForDS=Lists.newLinkedList();
  for (  Mapreduce mapreduce : mapreduces) {
    if (mapreduce.getDatasets().contains(dataset)) {
      queriesForDS.add(mapreduce);
    }
  }
  return queriesForDS;
}"
7885,"@Override public List<Dataset> getDatasetsByApplication(String account,String app) throws MetadataServiceException, TException {
  validateAccount(account);
  Map<String,Dataset> foundDatasets=Maps.newHashMap();
  List<Flow> flows=getFlowsByApplication(account,app);
  for (  Flow flow : flows) {
    List<String> flowDatasets=flow.getDatasets();
    if (flowDatasets == null || flowDatasets.isEmpty()) {
      continue;
    }
    for (    String datasetName : flowDatasets) {
      if (foundDatasets.containsKey(datasetName)) {
        continue;
      }
      Dataset dataset=getDataset(new Account(account),new Dataset(datasetName));
      if (dataset.isExists()) {
        foundDatasets.put(datasetName,dataset);
      }
    }
  }
  List<Query> queries=getQueriesByApplication(account,app);
  for (  Query query : queries) {
    List<String> queryDatasets=query.getDatasets();
    if (queryDatasets == null || queryDatasets.isEmpty()) {
      continue;
    }
    for (    String datasetName : queryDatasets) {
      if (foundDatasets.containsKey(datasetName)) {
        continue;
      }
      Dataset dataset=getDataset(new Account(account),new Dataset(datasetName));
      if (dataset.isExists()) {
        foundDatasets.put(datasetName,dataset);
      }
    }
  }
  List<Mapreduce> mapreduces=getMapreducesByApplication(account,app);
  for (  Mapreduce mapreduce : mapreduces) {
    List<String> mapreduceDatasets=mapreduce.getDatasets();
    if (mapreduceDatasets == null || mapreduceDatasets.isEmpty())     continue;
    for (    String datasetName : mapreduceDatasets) {
      if (foundDatasets.containsKey(datasetName))       continue;
      Dataset dataset=getDataset(new Account(account),new Dataset(datasetName));
      if (dataset.isExists()) {
        foundDatasets.put(datasetName,dataset);
      }
    }
  }
  List<Dataset> datasets=Lists.newArrayList();
  for (  Dataset dataset : foundDatasets.values()) {
    datasets.add(dataset);
  }
  return datasets;
}","@Override public List<Dataset> getDatasetsByApplication(String account,String app) throws MetadataServiceException, TException {
  validateAccount(account);
  Map<String,Dataset> foundDatasets=Maps.newHashMap();
  List<Flow> flows=getFlowsByApplication(account,app);
  for (  Flow flow : flows) {
    List<String> flowDatasets=flow.getDatasets();
    if (flowDatasets == null || flowDatasets.isEmpty()) {
      continue;
    }
    for (    String datasetName : flowDatasets) {
      if (foundDatasets.containsKey(datasetName)) {
        continue;
      }
      Dataset dataset=getDataset(new Account(account),new Dataset(datasetName));
      if (dataset.isExists()) {
        foundDatasets.put(datasetName,dataset);
      }
    }
  }
  List<Query> queries=getQueriesByApplication(account,app);
  for (  Query query : queries) {
    List<String> queryDatasets=query.getDatasets();
    if (queryDatasets == null || queryDatasets.isEmpty()) {
      continue;
    }
    for (    String datasetName : queryDatasets) {
      if (foundDatasets.containsKey(datasetName)) {
        continue;
      }
      Dataset dataset=getDataset(new Account(account),new Dataset(datasetName));
      if (dataset.isExists()) {
        foundDatasets.put(datasetName,dataset);
      }
    }
  }
  List<Mapreduce> mapreduces=getMapreducesByApplication(account,app);
  for (  Mapreduce mapreduce : mapreduces) {
    List<String> mapreduceDatasets=mapreduce.getDatasets();
    if (mapreduceDatasets == null || mapreduceDatasets.isEmpty()) {
      continue;
    }
    for (    String datasetName : mapreduceDatasets) {
      if (foundDatasets.containsKey(datasetName)) {
        continue;
      }
      Dataset dataset=getDataset(new Account(account),new Dataset(datasetName));
      if (dataset.isExists()) {
        foundDatasets.put(datasetName,dataset);
      }
    }
  }
  List<Dataset> datasets=Lists.newArrayList();
  for (  Dataset dataset : foundDatasets.values()) {
    datasets.add(dataset);
  }
  return datasets;
}"
7886,"private static File createDeploymentJar(Class<?> clz,ApplicationSpecification appSpec){
  File testAppDir;
  File tmpDir;
  testAppDir=Files.createTempDir();
  File outputDir=new File(testAppDir,""String_Node_Str"");
  tmpDir=new File(testAppDir,""String_Node_Str"");
  outputDir.mkdirs();
  tmpDir.mkdirs();
  Manifest manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.MANIFEST_VERSION,""String_Node_Str"");
  manifest.getMainAttributes().put(ManifestFields.MAIN_CLASS,clz.getName());
  ClassLoader loader=clz.getClassLoader();
  Preconditions.checkArgument(loader != null,""String_Node_Str"" + clz);
  String classFile=clz.getName().replace('.','/') + ""String_Node_Str"";
  try {
    for (Enumeration<URL> itr=loader.getResources(classFile); itr.hasMoreElements(); ) {
      URI uri=itr.nextElement().toURI();
      if (uri.getScheme().equals(""String_Node_Str"")) {
        File baseDir=new File(uri).getParentFile();
        Package appPackage=clz.getPackage();
        String packagePath=appPackage == null ? ""String_Node_Str"" : appPackage.getName().replace('.','/');
        String basePath=baseDir.getAbsolutePath();
        File relativeBase=new File(basePath.substring(0,basePath.length() - packagePath.length()));
        File jarFile=File.createTempFile(String.format(""String_Node_Str"",clz.getSimpleName(),System.currentTimeMillis()),""String_Node_Str"",tmpDir);
        return jarDir(baseDir,relativeBase,manifest,jarFile,appSpec);
      }
 else       if (uri.getScheme().equals(""String_Node_Str"")) {
        return new File(uri.getPath());
      }
    }
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
  return null;
}","private static File createDeploymentJar(Class<?> clz,ApplicationSpecification appSpec){
  File testAppDir;
  File tmpDir;
  testAppDir=Files.createTempDir();
  File outputDir=new File(testAppDir,""String_Node_Str"");
  tmpDir=new File(testAppDir,""String_Node_Str"");
  outputDir.mkdirs();
  tmpDir.mkdirs();
  Manifest manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.MANIFEST_VERSION,""String_Node_Str"");
  manifest.getMainAttributes().put(ManifestFields.MAIN_CLASS,clz.getName());
  ClassLoader loader=clz.getClassLoader();
  Preconditions.checkArgument(loader != null,""String_Node_Str"" + clz);
  String classFile=clz.getName().replace('.','/') + ""String_Node_Str"";
  try {
    for (Enumeration<URL> itr=loader.getResources(classFile); itr.hasMoreElements(); ) {
      URI uri=itr.nextElement().toURI();
      if (uri.getScheme().equals(""String_Node_Str"")) {
        File baseDir=new File(uri).getParentFile();
        Package appPackage=clz.getPackage();
        String packagePath=appPackage == null ? ""String_Node_Str"" : appPackage.getName().replace('.','/');
        String basePath=baseDir.getAbsolutePath();
        File relativeBase=new File(basePath.substring(0,basePath.length() - packagePath.length()));
        File jarFile=File.createTempFile(String.format(""String_Node_Str"",clz.getSimpleName(),System.currentTimeMillis()),""String_Node_Str"",tmpDir);
        return jarDir(baseDir,relativeBase,manifest,jarFile,appSpec);
      }
 else       if (uri.getScheme().equals(""String_Node_Str"")) {
        String rawSchemeSpecificPart=uri.getRawSchemeSpecificPart();
        if (rawSchemeSpecificPart.startsWith(""String_Node_Str"") && rawSchemeSpecificPart.contains(""String_Node_Str"")) {
          String[] parts=rawSchemeSpecificPart.substring(""String_Node_Str"".length()).split(""String_Node_Str"");
          return new File(parts[0]);
        }
 else {
          return new File(uri.getPath());
        }
      }
    }
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
  return null;
}"
7887,"private static AppFabricService.Client getAppFabricClient() throws TTransportException {
  CConfiguration config=CConfiguration.create();
  return new AppFabricService.Client(getThriftProtocol(config.get(Constants.CFG_APP_FABRIC_SERVER_ADDRESS,Constants.DEFAULT_APP_FABRIC_SERVER_ADDRESS),config.getInt(Constants.CFG_APP_FABRIC_SERVER_PORT,Constants.DEFAULT_APP_FABRIC_SERVER_PORT)));
}","private static AppFabricService.Client getAppFabricClient() throws TTransportException {
  CConfiguration config=CConfiguration.create();
  String host=config.get(Constants.CFG_APP_FABRIC_SERVER_ADDRESS,Constants.DEFAULT_APP_FABRIC_SERVER_ADDRESS);
  int port=config.getInt(Constants.CFG_APP_FABRIC_SERVER_PORT,Constants.DEFAULT_APP_FABRIC_SERVER_PORT);
  return new AppFabricService.Client(getThriftProtocol(config.get(Constants.CFG_APP_FABRIC_SERVER_ADDRESS,Constants.DEFAULT_APP_FABRIC_SERVER_ADDRESS),config.getInt(Constants.CFG_APP_FABRIC_SERVER_PORT,Constants.DEFAULT_APP_FABRIC_SERVER_PORT)));
}"
7888,"private void beforeClass() throws ClassNotFoundException {
  init(config);
  Context runManager=Context.getInstance(this);
  Class<? extends Application>[] apps=getApplications(testClass);
  if (apps != null && apps.length != 0) {
    for (    Class<? extends Application> each : apps) {
      appManager=deployApplication(each);
      runManager.addApplicationManager(each.getSimpleName(),appManager);
    }
  }
  if (""String_Node_Str"".equalsIgnoreCase(config.get(""String_Node_Str""))) {
    String metrics=config.get(""String_Node_Str"");
    if (StringUtils.isNotEmpty(metrics)) {
      List<String> metricList=ImmutableList.copyOf(metrics.replace(""String_Node_Str"",""String_Node_Str"").split(""String_Node_Str""));
      String tags=""String_Node_Str"";
      int interval=10;
      if (StringUtils.isNotEmpty(config.get(""String_Node_Str""))) {
        interval=Integer.valueOf(config.get(""String_Node_Str""));
      }
      Context.report(metricList,tags,interval);
    }
  }
}","private void beforeClass() throws ClassNotFoundException {
  init(config);
  Context runManager=Context.getInstance(this);
  Class<? extends Application>[] apps=getApplications(testClass);
  if (apps != null && apps.length != 0) {
    clearAppFabric();
    for (    Class<? extends Application> each : apps) {
      appManager=deployApplication(each);
      runManager.addApplicationManager(each.getSimpleName(),appManager);
    }
  }
  if (""String_Node_Str"".equalsIgnoreCase(config.get(""String_Node_Str""))) {
    String metrics=config.get(""String_Node_Str"");
    if (StringUtils.isNotEmpty(metrics)) {
      List<String> metricList=ImmutableList.copyOf(metrics.replace(""String_Node_Str"",""String_Node_Str"").split(""String_Node_Str""));
      String tags=""String_Node_Str"";
      int interval=10;
      if (StringUtils.isNotEmpty(config.get(""String_Node_Str""))) {
        interval=Integer.valueOf(config.get(""String_Node_Str""));
      }
      Context.report(metricList,tags,interval);
    }
  }
}"
7889,"private void init(CConfiguration configuration){
  LOG.debug(""String_Node_Str"");
  File testAppDir=Files.createTempDir();
  File outputDir=new File(testAppDir,""String_Node_Str"");
  tmpDir=new File(testAppDir,""String_Node_Str"");
  outputDir.mkdirs();
  tmpDir.mkdirs();
  configuration.set(""String_Node_Str"",outputDir.getAbsolutePath());
  configuration.set(""String_Node_Str"",tmpDir.getAbsolutePath());
  try {
    LOG.debug(""String_Node_Str"");
    appFabricServer=getAppFabricClient();
  }
 catch (  TTransportException e) {
    LOG.error(""String_Node_Str"");
    Throwables.propagate(e);
  }
  Module dataFabricModule;
  if (configuration.get(""String_Node_Str"") != null && configuration.get(""String_Node_Str"").equals(""String_Node_Str"")) {
    dataFabricModule=new DataFabricModules().getDistributedModules();
  }
 else {
    dataFabricModule=new DataFabricModules().getSingleNodeModules();
  }
}","private void init(CConfiguration configuration){
  LOG.debug(""String_Node_Str"");
  File testAppDir=Files.createTempDir();
  File outputDir=new File(testAppDir,""String_Node_Str"");
  tmpDir=new File(testAppDir,""String_Node_Str"");
  outputDir.mkdirs();
  tmpDir.mkdirs();
  configuration.set(""String_Node_Str"",outputDir.getAbsolutePath());
  configuration.set(""String_Node_Str"",tmpDir.getAbsolutePath());
  try {
    LOG.debug(""String_Node_Str"");
    appFabricServer=getAppFabricClient();
  }
 catch (  TTransportException e) {
    LOG.error(""String_Node_Str"");
    Throwables.propagate(e);
  }
  Module dataFabricModule;
  if (configuration.get(""String_Node_Str"") != null && configuration.get(""String_Node_Str"").equals(""String_Node_Str"")) {
    dataFabricModule=new DataFabricModules().getDistributedModules();
  }
 else {
    dataFabricModule=new DataFabricModules().getSingleNodeModules();
  }
  injector=Guice.createInjector(dataFabricModule,new ConfigModule(configuration),new IOModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new ProgramRunnerRuntimeModule().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      install(new FactoryModuleBuilder().implement(ApplicationManager.class,DefaultBenchmarkManager.class).build(BenchmarkManagerFactory.class));
      install(new FactoryModuleBuilder().implement(StreamWriter.class,MultiThreadedStreamWriter.class).build(BenchmarkStreamWriterFactory.class));
      install(new FactoryModuleBuilder().implement(ProcedureClient.class,DefaultProcedureClient.class).build(ProcedureClientFactory.class));
    }
  }
,new Module(){
    @Override public void configure(    Binder binder){
      binder.bind(new TypeLiteral<PipelineFactory<?>>(){
      }
).to(new TypeLiteral<SynchronousPipelineFactory<?>>(){
      }
);
      binder.bind(ManagerFactory.class).to(SyncManagerFactory.class);
      binder.bind(AuthorizationFactory.class).to(PassportAuthorizationFactory.class);
      binder.bind(MetadataService.Iface.class).to(com.continuuity.metadata.MetadataService.class);
      binder.bind(AppFabricService.Iface.class).toInstance(appFabricServer);
      binder.bind(MetaDataStore.class).to(SerializingMetaDataStore.class);
      binder.bind(StoreFactory.class).to(MDSStoreFactory.class);
    }
  }
);
  locationFactory=injector.getInstance(LocationFactory.class);
}"
7890,"public ApplicationManager deployApplication(Class<? extends Application> applicationClz){
  Preconditions.checkNotNull(applicationClz,""String_Node_Str"");
  Application application;
  try {
    application=applicationClz.newInstance();
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
  ApplicationSpecification appSpec=application.configure();
  final String applicationId=appSpec.getName();
  File jarFile=createDeploymentJar(applicationClz,appSpec);
  LOG.debug(""String_Node_Str"",jarFile.getAbsolutePath());
  Location deployedJar=locationFactory.create(jarFile.getAbsolutePath());
  try {
    final AuthToken token=new AuthToken(""String_Node_Str"");
    ResourceIdentifier id=appFabricServer.init(token,new ResourceInfo(accountId,""String_Node_Str"",applicationId,0,System.currentTimeMillis()));
    BufferFileInputStream is=new BufferFileInputStream(deployedJar.getInputStream(),100 * 1024);
    try {
      byte[] chunk=is.read();
      while (chunk.length > 0) {
        appFabricServer.chunk(token,id,ByteBuffer.wrap(chunk));
        chunk=is.read();
        DeploymentStatus status=appFabricServer.dstatus(token,id);
        Preconditions.checkState(status.getOverall() == 2,""String_Node_Str"");
      }
    }
  finally {
      is.close();
    }
    appFabricServer.deploy(token,id);
    int status=appFabricServer.dstatus(token,id).getOverall();
    while (status == 3) {
      status=appFabricServer.dstatus(token,id).getOverall();
      TimeUnit.MILLISECONDS.sleep(100);
    }
    Preconditions.checkState(status == 5,""String_Node_Str"");
    ApplicationManager appManager=(ApplicationManager)injector.getInstance(BenchmarkManagerFactory.class).create(token,accountId,applicationId,appFabricServer,deployedJar,appSpec);
    Preconditions.checkNotNull(appManager,""String_Node_Str"");
    LOG.debug(""String_Node_Str"",jarFile.getAbsolutePath());
    return appManager;
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",jarFile.getAbsolutePath());
    throw Throwables.propagate(e);
  }
}","public ApplicationManager deployApplication(Class<? extends Application> applicationClz){
  Preconditions.checkNotNull(applicationClz,""String_Node_Str"");
  try {
    ApplicationSpecification appSpec=applicationClz.newInstance().configure();
    Location deployedJar=TestHelper.deployApplication(appFabricServer,locationFactory,new Id.Account(accountId),TestHelper.DUMMY_AUTH_TOKEN,""String_Node_Str"",appSpec.getName(),applicationClz);
    return injector.getInstance(BenchmarkManagerFactory.class).create(TestHelper.DUMMY_AUTH_TOKEN,accountId,appSpec.getName(),appFabricServer,deployedJar,appSpec);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
7891,"@Override public synchronized RuntimeInfo lookup(final RunId runId){
  RuntimeInfo runtimeInfo=super.lookup(runId);
  if (runtimeInfo != null) {
    return runtimeInfo;
  }
  Optional<WeaveRunner.LiveInfo> result=Iterables.tryFind(weaveRunner.lookupLive(),new Predicate<WeaveRunner.LiveInfo>(){
    @Override public boolean apply(    WeaveRunner.LiveInfo input){
      return Iterables.indexOf(input.getRunIds(),Predicates.equalTo(runId)) != -1;
    }
  }
);
  if (!result.isPresent()) {
    LOG.info(""String_Node_Str"",runId);
    return null;
  }
  WeaveRunner.LiveInfo liveInfo=result.get();
  String appName=liveInfo.getApplicationName();
  Matcher matcher=APP_NAME_PATTERN.matcher(appName);
  if (!matcher.matches()) {
    LOG.warn(""String_Node_Str"",appName);
    return null;
  }
  Type type=getType(matcher.group(1));
  if (type == null) {
    LOG.warn(""String_Node_Str"",appName);
    return null;
  }
  Id.Program programId=Id.Program.from(matcher.group(2),matcher.group(3),matcher.group(4));
  WeaveController weaveController=weaveRunner.lookup(appName,runId);
  if (weaveController == null) {
    LOG.info(""String_Node_Str"",runId);
    return null;
  }
  return createRuntimeInfo(type,programId,weaveController);
}","@Override public synchronized RuntimeInfo lookup(final RunId runId){
  RuntimeInfo runtimeInfo=super.lookup(runId);
  if (runtimeInfo != null) {
    return runtimeInfo;
  }
  Optional<WeaveRunner.LiveInfo> result=Iterables.tryFind(weaveRunner.lookupLive(),new Predicate<WeaveRunner.LiveInfo>(){
    @Override public boolean apply(    WeaveRunner.LiveInfo input){
      return Iterables.indexOf(input.getRunIds(),Predicates.equalTo(runId)) != -1;
    }
  }
);
  if (!result.isPresent()) {
    LOG.info(""String_Node_Str"",runId);
    return null;
  }
  WeaveRunner.LiveInfo liveInfo=result.get();
  String appName=liveInfo.getApplicationName();
  Matcher matcher=APP_NAME_PATTERN.matcher(appName);
  if (!matcher.matches()) {
    LOG.warn(""String_Node_Str"",appName);
    return null;
  }
  Type type=getType(matcher.group(1));
  if (type == null) {
    LOG.warn(""String_Node_Str"",appName);
    return null;
  }
  Id.Program programId=Id.Program.from(matcher.group(2),matcher.group(3),matcher.group(4));
  WeaveController weaveController=weaveRunner.lookup(appName,runId);
  if (weaveController == null) {
    LOG.info(""String_Node_Str"",runId);
    return null;
  }
  runtimeInfo=createRuntimeInfo(type,programId,weaveController);
  updateRuntimeInfo(type,runId,runtimeInfo);
  return runtimeInfo;
}"
7892,"@Override public synchronized Map<RunId,RuntimeInfo> list(Type type){
  Map<RunId,RuntimeInfo> result=Maps.newHashMap();
  result.putAll(super.list(type));
  for (  WeaveRunner.LiveInfo liveInfo : weaveRunner.lookupLive()) {
    String appName=liveInfo.getApplicationName();
    Matcher matcher=APP_NAME_PATTERN.matcher(appName);
    if (!matcher.matches()) {
      continue;
    }
    Type appType=getType(matcher.group(1));
    if (appType != type) {
      continue;
    }
    for (    RunId runId : liveInfo.getRunIds()) {
      if (result.containsKey(runId)) {
        continue;
      }
      Id.Program programId=Id.Program.from(matcher.group(2),matcher.group(3),matcher.group(4));
      WeaveController weaveController=weaveRunner.lookup(appName,runId);
      if (weaveController != null) {
        result.put(runId,createRuntimeInfo(type,programId,weaveController));
      }
    }
  }
  return ImmutableMap.copyOf(result);
}","@Override public synchronized Map<RunId,RuntimeInfo> list(Type type){
  Map<RunId,RuntimeInfo> result=Maps.newHashMap();
  result.putAll(super.list(type));
  for (  WeaveRunner.LiveInfo liveInfo : weaveRunner.lookupLive()) {
    String appName=liveInfo.getApplicationName();
    Matcher matcher=APP_NAME_PATTERN.matcher(appName);
    if (!matcher.matches()) {
      continue;
    }
    Type appType=getType(matcher.group(1));
    if (appType != type) {
      continue;
    }
    for (    RunId runId : liveInfo.getRunIds()) {
      if (result.containsKey(runId)) {
        continue;
      }
      Id.Program programId=Id.Program.from(matcher.group(2),matcher.group(3),matcher.group(4));
      WeaveController weaveController=weaveRunner.lookup(appName,runId);
      if (weaveController != null) {
        RuntimeInfo runtimeInfo=createRuntimeInfo(type,programId,weaveController);
        result.put(runId,runtimeInfo);
        updateRuntimeInfo(type,runId,runtimeInfo);
      }
    }
  }
  return ImmutableMap.copyOf(result);
}"
7893,"@Override public ImmutablePair<byte[],Map<byte[],byte[]>> next(){
  if (scanner == null) {
    return null;
  }
  try {
    Result result=scanner.next();
    if (result == null) {
      return null;
    }
    Map<byte[],byte[]> colValue=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
    byte[] rowKey=null;
    byte[] last=null;
    for (    KeyValue kv : result.raw()) {
      if (readPointer != null && !readPointer.isVisible(kv.getTimestamp())) {
        continue;
      }
      rowKey=kv.getKey();
      byte[] column=kv.getQualifier();
      if (Bytes.equals(column,last)) {
        continue;
      }
      byte[] value=kv.getValue();
      last=column;
      colValue.put(column,value);
    }
    if (rowKey == null) {
      return null;
    }
 else {
      return new ImmutablePair<byte[],Map<byte[],byte[]>>(rowKey,colValue);
    }
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}","@Override public ImmutablePair<byte[],Map<byte[],byte[]>> next(){
  if (scanner == null) {
    return null;
  }
  try {
    Result result=scanner.next();
    if (result == null) {
      return null;
    }
    Map<byte[],byte[]> colValue=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
    byte[] rowKey=null;
    byte[] last=null;
    for (    KeyValue kv : result.raw()) {
      if (readPointer != null && !readPointer.isVisible(kv.getTimestamp())) {
        continue;
      }
      rowKey=kv.getRow();
      byte[] column=kv.getQualifier();
      if (Bytes.equals(column,last)) {
        continue;
      }
      byte[] value=kv.getValue();
      last=column;
      colValue.put(column,value);
    }
    if (rowKey == null) {
      return null;
    }
 else {
      return new ImmutablePair<byte[],Map<byte[],byte[]>>(rowKey,colValue);
    }
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}"
7894,"@Test(timeout=20000) public void testBatchReads() throws OperationException, InterruptedException {
  final String table=""String_Node_Str"";
  SortedSet<Long> keysWritten=Sets.newTreeSet();
  List<WriteOperation> ops=Lists.newArrayListWithCapacity(500);
  Random rand=new Random(451);
  for (int i=0; i < 500; i++) {
    long keyLong=rand.nextLong();
    byte[] key=org.apache.hadoop.hbase.util.Bytes.toBytes(keyLong);
    ops.add(new Write(table,key,new byte[][]{c,key},new byte[][]{key,v}));
    keysWritten.add(keyLong);
  }
  local.commit(context,ops);
  OperationResult<List<KeyRange>> result=remote.execute(context,new GetSplits(table));
  Assert.assertFalse(result.isEmpty());
  List<KeyRange> splits=result.getValue();
  SortedSet<Long> keysToVerify=Sets.newTreeSet(keysWritten);
  verifySplits(table,splits,keysToVerify);
  long start=0x10000000L, stop=0x40000000L;
  result=remote.execute(context,new GetSplits(table,5,org.apache.hadoop.hbase.util.Bytes.toBytes(start),org.apache.hadoop.hbase.util.Bytes.toBytes(stop)));
  Assert.assertFalse(result.isEmpty());
  splits=result.getValue();
  Assert.assertTrue(splits.size() <= 5);
  keysToVerify=Sets.newTreeSet(keysWritten.subSet(start,stop));
  verifySplits(table,splits,keysToVerify);
}","@Test(timeout=30000) public void testBatchReads() throws OperationException, InterruptedException {
  final String table=""String_Node_Str"";
  SortedSet<Long> keysWritten=Sets.newTreeSet();
  List<WriteOperation> ops=Lists.newArrayListWithCapacity(500);
  Random rand=new Random(451);
  for (int i=0; i < 500; i++) {
    long keyLong=rand.nextLong();
    byte[] key=Bytes.toBytes(keyLong);
    ops.add(new Write(table,key,new byte[][]{c,key},new byte[][]{key,v}));
    keysWritten.add(keyLong);
  }
  local.commit(context,ops);
  OperationResult<List<KeyRange>> result=remote.execute(context,new GetSplits(table));
  Assert.assertFalse(result.isEmpty());
  List<KeyRange> splits=result.getValue();
  SortedSet<Long> keysToVerify=Sets.newTreeSet(keysWritten);
  verifySplits(table,splits,keysToVerify);
  long start=0x10000000L, stop=0x40000000L;
  result=remote.execute(context,new GetSplits(table,5,Bytes.toBytes(start),Bytes.toBytes(stop)));
  Assert.assertFalse(result.isEmpty());
  splits=result.getValue();
  Assert.assertTrue(splits.size() <= 5);
  keysToVerify=Sets.newTreeSet(keysWritten.subSet(start,stop));
  verifySplits(table,splits,keysToVerify);
}"
7895,"@Override public MapReduceManager startMapReduce(final String jobName,Map<String,String> arguments){
  try {
    final FlowIdentifier jobId=new FlowIdentifier(accountId,applicationId,jobName,0);
    jobId.setType(EntityType.QUERY);
    if (!isRunning(jobId)) {
      runningProcessses.remove(jobName);
    }
    Preconditions.checkState(runningProcessses.putIfAbsent(jobName,jobId) == null,""String_Node_Str"",jobName);
    try {
      appFabricServer.start(token,new FlowDescriptor(jobId,arguments));
    }
 catch (    Exception e) {
      runningProcessses.remove(jobName);
      throw Throwables.propagate(e);
    }
    return new MapReduceManager(){
      @Override public void stop(){
        try {
          if (runningProcessses.remove(jobName,jobId)) {
            appFabricServer.stop(token,jobId);
          }
        }
 catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
      @Override public void waitForFinish(      long timeout,      TimeUnit timeoutUnit) throws TimeoutException, InterruptedException {
        while (timeout > 0 && isRunning(jobId)) {
          timeoutUnit.sleep(1);
          timeout--;
        }
        if (timeout == 0 && isRunning(jobId)) {
          throw new TimeoutException(""String_Node_Str"");
        }
      }
    }
;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","@Override public MapReduceManager startMapReduce(final String jobName,Map<String,String> arguments){
  try {
    final FlowIdentifier jobId=new FlowIdentifier(accountId,applicationId,jobName,0);
    jobId.setType(EntityType.MAPREDUCE);
    if (!isRunning(jobId)) {
      runningProcessses.remove(jobName);
    }
    Preconditions.checkState(runningProcessses.putIfAbsent(jobName,jobId) == null,""String_Node_Str"",jobName);
    try {
      appFabricServer.start(token,new FlowDescriptor(jobId,arguments));
    }
 catch (    Exception e) {
      runningProcessses.remove(jobName);
      throw Throwables.propagate(e);
    }
    return new MapReduceManager(){
      @Override public void stop(){
        try {
          if (runningProcessses.remove(jobName,jobId)) {
            appFabricServer.stop(token,jobId);
          }
        }
 catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
      @Override public void waitForFinish(      long timeout,      TimeUnit timeoutUnit) throws TimeoutException, InterruptedException {
        while (timeout > 0 && isRunning(jobId)) {
          timeoutUnit.sleep(1);
          timeout--;
        }
        if (timeout == 0 && isRunning(jobId)) {
          throw new TimeoutException(""String_Node_Str"");
        }
      }
    }
;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
7896,"public static Location deployApplication(AppFabricService.Iface appFabricServer,LocationFactory locationFactory,final String account,final AuthToken token,final String applicationId,final String fileName,Class<? extends Application> applicationClz) throws Exception {
  Preconditions.checkNotNull(applicationClz,""String_Node_Str"");
  Application application=applicationClz.newInstance();
  ApplicationSpecification appSpec=application.configure();
  Location deployedJar=locationFactory.create(createDeploymentJar(applicationClz,appSpec).getAbsolutePath());
  try {
    ResourceIdentifier id=appFabricServer.init(token,new ResourceInfo(account,applicationId,fileName,0,System.currentTimeMillis()));
    BufferFileInputStream is=new BufferFileInputStream(deployedJar.getInputStream(),100 * 1024);
    try {
      byte[] chunk=is.read();
      while (chunk.length > 0) {
        appFabricServer.chunk(token,id,ByteBuffer.wrap(chunk));
        chunk=is.read();
        DeploymentStatus status=appFabricServer.dstatus(token,id);
        Preconditions.checkState(status.getOverall() == 2,""String_Node_Str"");
      }
    }
  finally {
      is.close();
    }
    appFabricServer.deploy(token,id);
    int status=appFabricServer.dstatus(token,id).getOverall();
    while (status == 3) {
      status=appFabricServer.dstatus(token,id).getOverall();
      TimeUnit.MILLISECONDS.sleep(100);
    }
    Preconditions.checkState(status == 5,""String_Node_Str"");
  }
  finally {
    deployedJar.delete(true);
  }
  return deployedJar;
}","private static Location deployApplication(AppFabricService.Iface appFabricServer,LocationFactory locationFactory,final String account,final AuthToken token,final String applicationId,final String fileName,Class<? extends Application> applicationClz) throws Exception {
  Preconditions.checkNotNull(applicationClz,""String_Node_Str"");
  Application application=applicationClz.newInstance();
  ApplicationSpecification appSpec=application.configure();
  Location deployedJar=locationFactory.create(createDeploymentJar(applicationClz,appSpec).getAbsolutePath());
  ResourceIdentifier id=appFabricServer.init(token,new ResourceInfo(account,applicationId,fileName,0,System.currentTimeMillis()));
  BufferFileInputStream is=new BufferFileInputStream(deployedJar.getInputStream(),100 * 1024);
  try {
    byte[] chunk=is.read();
    while (chunk.length > 0) {
      appFabricServer.chunk(token,id,ByteBuffer.wrap(chunk));
      chunk=is.read();
      DeploymentStatus status=appFabricServer.dstatus(token,id);
      Preconditions.checkState(status.getOverall() == 2,""String_Node_Str"");
    }
  }
  finally {
    is.close();
  }
  appFabricServer.deploy(token,id);
  int status=appFabricServer.dstatus(token,id).getOverall();
  while (status == 3) {
    status=appFabricServer.dstatus(token,id).getOverall();
    TimeUnit.MILLISECONDS.sleep(100);
  }
  Preconditions.checkState(status == 5,""String_Node_Str"");
  return deployedJar;
}"
7897,"@Override public FlowManager startFlow(final String flowName,Map<String,String> arguments){
  try {
    final FlowIdentifier flowId=new FlowIdentifier(accountId,applicationId,flowName,0);
    Preconditions.checkState(runningProcessses.putIfAbsent(flowName,flowId) == null,""String_Node_Str"",flowName);
    try {
    }
 catch (    Exception e) {
      runningProcessses.remove(flowName);
      throw Throwables.propagate(e);
    }
    return new FlowManager(){
      @Override public void setFlowletInstances(      String flowletName,      int instances){
        Preconditions.checkArgument(instances > 0,""String_Node_Str"");
        try {
          appFabricServer.setInstances(token,flowId,flowletName,(short)instances);
        }
 catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
      @Override public void stop(){
        try {
          if (runningProcessses.remove(flowName,flowId)) {
            appFabricServer.stop(token,flowId);
          }
        }
 catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
    }
;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","@Override public FlowManager startFlow(final String flowName,Map<String,String> arguments){
  try {
    final FlowIdentifier flowId=new FlowIdentifier(accountId,applicationId,flowName,0);
    Preconditions.checkState(runningProcessses.putIfAbsent(flowName,flowId) == null,""String_Node_Str"",flowName);
    try {
      appFabricServer.start(token,new FlowDescriptor(flowId,arguments));
    }
 catch (    Exception e) {
      runningProcessses.remove(flowName);
      throw Throwables.propagate(e);
    }
    return new FlowManager(){
      @Override public void setFlowletInstances(      String flowletName,      int instances){
        Preconditions.checkArgument(instances > 0,""String_Node_Str"");
        try {
          appFabricServer.setInstances(token,flowId,flowletName,(short)instances);
        }
 catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
      @Override public void stop(){
        try {
          if (runningProcessses.remove(flowName,flowId)) {
            appFabricServer.stop(token,flowId);
          }
        }
 catch (        Exception e) {
          throw Throwables.propagate(e);
        }
      }
    }
;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
7898,"private void handleFlowOperation(MessageEvent message,HttpRequest request,GatewayMetricsHelperWrapper metricsHelper,AppFabricService.Client client,AuthToken token,FlowIdentifier flowIdent,Map<String,List<String>> parameters) throws TException, AppFabricServiceException {
  List<String> types=parameters.get(""String_Node_Str"");
  if (types == null || types.size() == 0) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
 else   if (types.size() > 1) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
 else   if (types.size() == 1 && !SUPPORTED_FLOW_TYPES.contains(types.get(0))) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
  String flowType=types.get(0);
  if (""String_Node_Str"".equals(flowType)) {
    flowIdent.setType(EntityType.FLOW);
  }
 else   if (""String_Node_Str"".equals(flowType)) {
    flowIdent.setType(EntityType.QUERY);
  }
 else   if (""String_Node_Str"".equals(flowType)) {
    flowIdent.setType(EntityType.MAPREDUCE);
  }
  List<String> operations=parameters.get(""String_Node_Str"");
  if (operations == null || operations.size() == 0) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
 else   if (operations.size() > 1) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
 else   if (operations.size() == 1 && !SUPPORTED_FLOW_OPERATIONS.contains(operations.get(0))) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
  String operation=operations.get(0);
  if ((""String_Node_Str"".equals(operation) || ""String_Node_Str"".equals(operation)) && request.getMethod() != HttpMethod.POST) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
  if (""String_Node_Str"".equals(operation) && request.getMethod() != HttpMethod.GET) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
  if (""String_Node_Str"".equals(operation)) {
    client.start(token,new FlowDescriptor(flowIdent,new ArrayList<String>()));
    if (FLOW_STATUS_RUNNING.equals(client.status(token,flowIdent).getStatus())) {
      respondSuccess(message.getChannel(),request);
    }
 else {
      respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    }
  }
 else   if (""String_Node_Str"".equals(operation)) {
    client.stop(token,flowIdent);
    if (FLOW_STATUS_STOPPED.equals(client.status(token,flowIdent).getStatus())) {
      respondSuccess(message.getChannel(),request);
    }
 else {
      respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    }
  }
 else   if (""String_Node_Str"".equals(operation)) {
    FlowStatus flowStatus=client.status(token,flowIdent);
    if (flowStatus != null) {
      byte[] response=Bytes.toBytes(""String_Node_Str"" + flowStatus.getStatus() + ""String_Node_Str"");
      respondSuccess(message.getChannel(),request,response);
    }
 else {
      respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    }
  }
}","private void handleFlowOperation(MessageEvent message,HttpRequest request,GatewayMetricsHelperWrapper metricsHelper,AppFabricService.Client client,AuthToken token,FlowIdentifier flowIdent,Map<String,List<String>> parameters) throws TException, AppFabricServiceException {
  List<String> types=parameters.get(""String_Node_Str"");
  if (types == null || types.size() == 0) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
 else   if (types.size() > 1) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
 else   if (types.size() == 1 && !SUPPORTED_FLOW_TYPES.contains(types.get(0))) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
  String flowType=types.get(0);
  if (""String_Node_Str"".equals(flowType)) {
    flowIdent.setType(EntityType.FLOW);
  }
 else   if (""String_Node_Str"".equals(flowType)) {
    flowIdent.setType(EntityType.QUERY);
  }
 else   if (""String_Node_Str"".equals(flowType)) {
    flowIdent.setType(EntityType.MAPREDUCE);
  }
  List<String> operations=parameters.get(""String_Node_Str"");
  if (operations == null || operations.size() == 0) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
 else   if (operations.size() > 1) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
 else   if (operations.size() == 1 && !SUPPORTED_FLOW_OPERATIONS.contains(operations.get(0))) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
  String operation=operations.get(0);
  if ((""String_Node_Str"".equals(operation) || ""String_Node_Str"".equals(operation)) && request.getMethod() != HttpMethod.POST) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
  if (""String_Node_Str"".equals(operation) && request.getMethod() != HttpMethod.GET) {
    respondBadRequest(message,request,metricsHelper,""String_Node_Str"");
    return;
  }
  if (""String_Node_Str"".equals(operation)) {
    client.start(token,new FlowDescriptor(flowIdent,ImmutableMap.<String,String>of()));
    if (FLOW_STATUS_RUNNING.equals(client.status(token,flowIdent).getStatus())) {
      respondSuccess(message.getChannel(),request);
    }
 else {
      respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    }
  }
 else   if (""String_Node_Str"".equals(operation)) {
    client.stop(token,flowIdent);
    if (FLOW_STATUS_STOPPED.equals(client.status(token,flowIdent).getStatus())) {
      respondSuccess(message.getChannel(),request);
    }
 else {
      respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    }
  }
 else   if (""String_Node_Str"".equals(operation)) {
    FlowStatus flowStatus=client.status(token,flowIdent);
    if (flowStatus != null) {
      byte[] response=Bytes.toBytes(""String_Node_Str"" + flowStatus.getStatus() + ""String_Node_Str"");
      respondSuccess(message.getChannel(),request,response);
    }
 else {
      respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    }
  }
}"
7899,"private void handleTableOperation(MessageEvent message,HttpRequest request,MetricsHelper helper,LinkedList<String> pathComponents,Map<String,List<String>> parameters,OperationContext opContext){
  if (pathComponents.isEmpty()) {
    respondBadRequest(message,request,helper,""String_Node_Str"");
    return;
  }
  String tableName=pathComponents.removeFirst();
  String row=pathComponents.isEmpty() ? null : pathComponents.removeFirst();
  if (!pathComponents.isEmpty()) {
    respondBadRequest(message,request,helper,""String_Node_Str"");
    return;
  }
  TableOp operation=null;
  List<String> operations=parameters.get(""String_Node_Str"");
  if (operations != null) {
    if (operations.size() > 1) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
 else     if (operations.size() == 1) {
      String op=operations.get(0);
      if (""String_Node_Str"".equals(op)) {
        operation=TableOp.List;
      }
 else       if (""String_Node_Str"".equals(op)) {
        operation=TableOp.Increment;
      }
 else {
        respondBadRequest(message,request,helper,""String_Node_Str"");
        return;
      }
    }
  }
  List<String> columns=null;
  List<String> columnParams=parameters.get(""String_Node_Str"");
  if (columnParams != null && columnParams.size() > 0) {
    columns=Lists.newLinkedList();
    for (    String param : columnParams) {
      Collections.addAll(columns,param.split(""String_Node_Str""));
    }
  }
  List<String> startParams=parameters.get(""String_Node_Str"");
  if (startParams != null && startParams.size() > 1) {
    respondBadRequest(message,request,helper,""String_Node_Str"");
    return;
  }
  String start=(startParams == null || startParams.isEmpty()) ? null : startParams.get(0);
  List<String> stopParams=parameters.get(""String_Node_Str"");
  if (stopParams != null && stopParams.size() > 1) {
    respondBadRequest(message,request,helper,""String_Node_Str"");
    return;
  }
  String stop=(stopParams == null || stopParams.isEmpty()) ? null : stopParams.get(0);
  List<String> limitParams=parameters.get(""String_Node_Str"");
  if (limitParams != null && limitParams.size() > 1) {
    respondBadRequest(message,request,helper,""String_Node_Str"");
    return;
  }
  Integer limit;
  try {
    limit=(limitParams == null || limitParams.isEmpty()) ? null : Integer.parseInt(limitParams.get(0));
  }
 catch (  NumberFormatException e) {
    respondBadRequest(message,request,helper,""String_Node_Str"");
    return;
  }
  String encoding=null;
  List<String> encodingParams=parameters.get(""String_Node_Str"");
  if (encodingParams != null) {
    if (encodingParams.size() > 1) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (!encodingParams.isEmpty()) {
      encoding=encodingParams.get(0);
      if (!Util.supportedEncoding(encoding)) {
        respondBadRequest(message,request,helper,""String_Node_Str"");
        return;
      }
    }
  }
  boolean counter=false;
  List<String> counterParams=parameters.get(""String_Node_Str"");
  if (counterParams != null) {
    if (counterParams.size() > 1) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (!counterParams.isEmpty()) {
      String param=counterParams.get(0);
      counter=""String_Node_Str"".equals(param) || ""String_Node_Str"".equals(param);
    }
  }
  HttpMethod method=request.getMethod();
  if (HttpMethod.GET.equals(method)) {
    if (operation == TableOp.List) {
      if (row != null) {
        respondBadRequest(message,request,helper,""String_Node_Str"");
        return;
      }
 else {
        respondBadRequest(message,request,helper,""String_Node_Str"",HttpResponseStatus.NOT_IMPLEMENTED);
        return;
      }
    }
    if (operation != null) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (row == null) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (columns != null && !columns.isEmpty() && (start != null || stop != null)) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    operation=TableOp.Read;
  }
 else   if (HttpMethod.DELETE.equals(method)) {
    if (operation != null) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (row == null) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (columns == null || columns.isEmpty()) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    operation=TableOp.Delete;
  }
 else   if (HttpMethod.PUT.equals(method)) {
    if (operation != null) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (row == null) {
      operation=TableOp.Create;
    }
 else {
      operation=TableOp.Write;
    }
  }
 else   if (HttpMethod.POST.equals(method)) {
    if (operation == null) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (operation != TableOp.Increment) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (row == null) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    operation=TableOp.Increment;
  }
  Type stringMapType=new TypeToken<Map<String,String>>(){
  }
.getType();
  Map<String,String> valueMap=null;
  try {
    if (operation == TableOp.Increment || operation == TableOp.Write) {
      InputStreamReader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()),Charsets.UTF_8);
      if (operation == TableOp.Write) {
        valueMap=new Gson().fromJson(reader,stringMapType);
      }
 else {
        valueMap=new Gson().fromJson(reader,stringMapType);
      }
    }
  }
 catch (  Exception e) {
    respondBadRequest(message,request,helper,""String_Node_Str"" + e.getMessage());
    return;
  }
  if (operation.equals(TableOp.Create)) {
    DataSetSpecification spec=new Table(tableName).configure();
    Dataset ds=new Dataset(spec.getName());
    ds.setName(spec.getName());
    ds.setType(spec.getType());
    ds.setSpecification(new Gson().toJson(spec));
    try {
      this.accessor.getMetadataService().assertDataset(new Account(opContext.getAccount()),ds);
    }
 catch (    MetadataServiceException e) {
      respondBadRequest(message,request,helper,""String_Node_Str"",HttpResponseStatus.CONFLICT);
      return;
    }
catch (    TException e) {
      helper.finish(Error);
      LOG.error(""String_Node_Str"" + e.getMessage(),e);
      respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR);
      return;
    }
    respondSuccess(message.getChannel(),request);
    helper.finish(Success);
    return;
  }
  Table table;
  try {
    table=this.accessor.getInstantiator().getDataSet(tableName,opContext);
  }
 catch (  Exception e) {
    if (LOG.isTraceEnabled()) {
      LOG.trace(""String_Node_Str"" + tableName + ""String_Node_Str""+ e.getMessage()+ ""String_Node_Str""+ request.getUri()+ ""String_Node_Str"");
    }
    helper.finish(BadRequest);
    respondError(message.getChannel(),HttpResponseStatus.NOT_FOUND);
    return;
  }
  byte[] rowKey;
  try {
    rowKey=row == null ? null : Util.decodeBinary(row,encoding);
  }
 catch (  Exception e) {
    respondBadRequest(message,request,helper,""String_Node_Str"",e);
    return;
  }
  if (operation.equals(TableOp.List)) {
  }
 else   if (operation.equals(TableOp.Read)) {
    Read read;
    try {
      if (columns == null || columns.isEmpty()) {
        byte[] startCol=start == null ? null : Util.decodeBinary(start,encoding);
        byte[] stopCol=stop == null ? null : Util.decodeBinary(stop,encoding);
        read=new Read(rowKey,startCol,stopCol,limit == null ? -1 : limit);
      }
 else {
        byte[][] cols=new byte[columns.size()][];
        int i=0;
        for (        String column : columns) {
          cols[i++]=Util.decodeBinary(column,encoding);
        }
        read=new Read(rowKey,cols);
      }
    }
 catch (    Exception e) {
      respondBadRequest(message,request,helper,""String_Node_Str"",e);
      return;
    }
    OperationResult<Map<byte[],byte[]>> result;
    try {
      result=table.read(read);
    }
 catch (    OperationException e) {
      helper.finish(Error);
      LOG.error(""String_Node_Str"" + e.getMessage(),e);
      respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR);
      return;
    }
    if (result.isEmpty() || result.getValue().isEmpty()) {
      helper.finish(NoData);
      respondError(message.getChannel(),HttpResponseStatus.NOT_FOUND);
    }
 else {
      Map<String,String> map=Maps.newTreeMap();
      for (      Map.Entry<byte[],byte[]> entry : result.getValue().entrySet()) {
        map.put(Util.encodeBinary(entry.getKey(),encoding),Util.encodeBinary(entry.getValue(),encoding,counter));
      }
      byte[] response=new Gson().toJson(map).getBytes(Charsets.UTF_8);
      respondSuccess(message.getChannel(),request,response);
      helper.finish(Success);
    }
  }
 else   if (operation.equals(TableOp.Delete)) {
    Delete delete;
    try {
      byte[][] cols=new byte[columns.size()][];
      int i=0;
      for (      String column : columns) {
        cols[i++]=Util.decodeBinary(column,encoding);
      }
      delete=new Delete(rowKey,cols);
    }
 catch (    Exception e) {
      respondBadRequest(message,request,helper,""String_Node_Str"",e);
      return;
    }
    try {
      table.write(delete);
    }
 catch (    OperationException e) {
      helper.finish(Error);
      LOG.error(""String_Node_Str"" + e.getMessage(),e);
      respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR);
      return;
    }
    helper.finish(Success);
    respondSuccess(message.getChannel(),request);
  }
 else   if (operation.equals(TableOp.Write)) {
    Write write;
    if (valueMap == null || valueMap.isEmpty()) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    try {
      byte[][] cols=new byte[valueMap.size()][];
      byte[][] vals=new byte[valueMap.size()][];
      int i=0;
      for (      Map.Entry<String,String> entry : valueMap.entrySet()) {
        cols[i]=Util.decodeBinary(entry.getKey(),encoding);
        vals[i]=Util.decodeBinary(entry.getValue(),encoding,counter);
        i++;
      }
      write=new Write(rowKey,cols,vals);
    }
 catch (    Exception e) {
      respondBadRequest(message,request,helper,""String_Node_Str"",e);
      return;
    }
    try {
      table.write(write);
    }
 catch (    OperationException e) {
      helper.finish(Error);
      LOG.error(""String_Node_Str"" + e.getMessage(),e);
      respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR);
      return;
    }
    helper.finish(Success);
    respondSuccess(message.getChannel(),request);
  }
 else   if (operation.equals(TableOp.Increment)) {
    Increment increment;
    if (valueMap == null || valueMap.isEmpty()) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    try {
      byte[][] cols=new byte[valueMap.size()][];
      long[] vals=new long[valueMap.size()];
      int i=0;
      for (      Map.Entry<String,String> entry : valueMap.entrySet()) {
        cols[i]=Util.decodeBinary(entry.getKey(),encoding);
        vals[i]=Long.parseLong(entry.getValue());
        i++;
      }
      increment=new Increment(rowKey,cols,vals);
    }
 catch (    Exception e) {
      respondBadRequest(message,request,helper,""String_Node_Str"",e);
      return;
    }
    Map<byte[],Long> results;
    try {
      results=table.incrementAndGet(increment);
    }
 catch (    OperationException e) {
      if (StatusCode.ILLEGAL_INCREMENT == e.getStatus()) {
        respondBadRequest(message,request,helper,""String_Node_Str"");
      }
 else {
        helper.finish(Error);
        LOG.error(""String_Node_Str"" + e.getMessage(),e);
        respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR);
      }
      return;
    }
    Map<String,Long> map=Maps.newTreeMap();
    for (    Map.Entry<byte[],Long> entry : results.entrySet()) {
      map.put(Util.encodeBinary(entry.getKey(),encoding),entry.getValue());
    }
    byte[] response=new Gson().toJson(map).getBytes(Charsets.UTF_8);
    respondSuccess(message.getChannel(),request,response);
    helper.finish(Success);
  }
}","private void handleTableOperation(MessageEvent message,HttpRequest request,MetricsHelper helper,LinkedList<String> pathComponents,Map<String,List<String>> parameters,OperationContext opContext){
  if (pathComponents.isEmpty()) {
    respondBadRequest(message,request,helper,""String_Node_Str"");
    return;
  }
  String tableName=pathComponents.removeFirst();
  String row=pathComponents.isEmpty() ? null : pathComponents.removeFirst();
  if (!pathComponents.isEmpty()) {
    respondBadRequest(message,request,helper,""String_Node_Str"");
    return;
  }
  TableOp operation=null;
  List<String> operations=parameters.get(""String_Node_Str"");
  if (operations != null) {
    if (operations.size() > 1) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
 else     if (operations.size() == 1) {
      String op=operations.get(0);
      if (""String_Node_Str"".equals(op)) {
        operation=TableOp.List;
      }
 else       if (""String_Node_Str"".equals(op)) {
        operation=TableOp.Increment;
      }
 else {
        respondBadRequest(message,request,helper,""String_Node_Str"");
        return;
      }
    }
  }
  List<String> columns=null;
  List<String> columnParams=parameters.get(""String_Node_Str"");
  if (columnParams != null && columnParams.size() > 0) {
    columns=Lists.newLinkedList();
    for (    String param : columnParams) {
      Collections.addAll(columns,param.split(""String_Node_Str""));
    }
  }
  List<String> startParams=parameters.get(""String_Node_Str"");
  if (startParams != null && startParams.size() > 1) {
    respondBadRequest(message,request,helper,""String_Node_Str"");
    return;
  }
  String start=(startParams == null || startParams.isEmpty()) ? null : startParams.get(0);
  List<String> stopParams=parameters.get(""String_Node_Str"");
  if (stopParams != null && stopParams.size() > 1) {
    respondBadRequest(message,request,helper,""String_Node_Str"");
    return;
  }
  String stop=(stopParams == null || stopParams.isEmpty()) ? null : stopParams.get(0);
  List<String> limitParams=parameters.get(""String_Node_Str"");
  if (limitParams != null && limitParams.size() > 1) {
    respondBadRequest(message,request,helper,""String_Node_Str"");
    return;
  }
  Integer limit;
  try {
    limit=(limitParams == null || limitParams.isEmpty()) ? null : Integer.parseInt(limitParams.get(0));
  }
 catch (  NumberFormatException e) {
    respondBadRequest(message,request,helper,""String_Node_Str"");
    return;
  }
  String encoding=null;
  List<String> encodingParams=parameters.get(""String_Node_Str"");
  if (encodingParams != null) {
    if (encodingParams.size() > 1) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (!encodingParams.isEmpty()) {
      encoding=encodingParams.get(0);
      if (!Util.supportedEncoding(encoding)) {
        respondBadRequest(message,request,helper,""String_Node_Str"");
        return;
      }
    }
  }
  boolean counter=false;
  List<String> counterParams=parameters.get(""String_Node_Str"");
  if (counterParams != null) {
    if (counterParams.size() > 1) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (!counterParams.isEmpty()) {
      String param=counterParams.get(0);
      counter=""String_Node_Str"".equals(param) || ""String_Node_Str"".equals(param);
    }
  }
  HttpMethod method=request.getMethod();
  if (HttpMethod.GET.equals(method)) {
    if (operation == TableOp.List) {
      if (row != null) {
        respondBadRequest(message,request,helper,""String_Node_Str"");
        return;
      }
 else {
        respondBadRequest(message,request,helper,""String_Node_Str"",HttpResponseStatus.NOT_IMPLEMENTED);
        return;
      }
    }
    if (operation != null) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (row == null) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (columns != null && !columns.isEmpty() && (start != null || stop != null)) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    operation=TableOp.Read;
  }
 else   if (HttpMethod.DELETE.equals(method)) {
    if (operation != null) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (row == null) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (columns == null || columns.isEmpty()) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    operation=TableOp.Delete;
  }
 else   if (HttpMethod.PUT.equals(method)) {
    if (operation != null) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (row == null) {
      operation=TableOp.Create;
    }
 else {
      operation=TableOp.Write;
    }
  }
 else   if (HttpMethod.POST.equals(method)) {
    if (operation == null) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (operation != TableOp.Increment) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    if (row == null) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    operation=TableOp.Increment;
  }
  Type stringMapType=new TypeToken<Map<String,String>>(){
  }
.getType();
  Map<String,String> valueMap=null;
  try {
    if (operation == TableOp.Increment || operation == TableOp.Write) {
      InputStreamReader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()),Charsets.UTF_8);
      if (operation == TableOp.Write) {
        valueMap=new Gson().fromJson(reader,stringMapType);
      }
 else {
        valueMap=new Gson().fromJson(reader,stringMapType);
      }
    }
  }
 catch (  Exception e) {
    respondBadRequest(message,request,helper,""String_Node_Str"" + e.getMessage());
    return;
  }
  if (operation.equals(TableOp.Create)) {
    DataSetSpecification spec=new Table(tableName).configure();
    Dataset ds=new Dataset(spec.getName());
    ds.setName(spec.getName());
    ds.setType(spec.getType());
    ds.setSpecification(new Gson().toJson(spec));
    try {
      this.accessor.getMetadataService().assertDataset(new Account(opContext.getAccount()),ds);
    }
 catch (    MetadataServiceException e) {
      respondBadRequest(message,request,helper,""String_Node_Str"",HttpResponseStatus.CONFLICT);
      return;
    }
catch (    TException e) {
      helper.finish(Error);
      LOG.error(""String_Node_Str"" + e.getMessage(),e);
      respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR);
      return;
    }
    respondSuccess(message.getChannel(),request);
    helper.finish(Success);
    return;
  }
  Table table;
  try {
    table=this.accessor.getInstantiator().getDataSet(tableName,opContext);
  }
 catch (  Exception e) {
    if (LOG.isTraceEnabled()) {
      LOG.trace(""String_Node_Str"" + tableName + ""String_Node_Str""+ e.getMessage()+ ""String_Node_Str""+ request.getUri()+ ""String_Node_Str"");
    }
    helper.finish(BadRequest);
    respondError(message.getChannel(),HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
    return;
  }
  byte[] rowKey;
  try {
    rowKey=row == null ? null : Util.decodeBinary(row,encoding);
  }
 catch (  Exception e) {
    respondBadRequest(message,request,helper,""String_Node_Str"",e);
    return;
  }
  if (operation.equals(TableOp.List)) {
  }
 else   if (operation.equals(TableOp.Read)) {
    Read read;
    try {
      if (columns == null || columns.isEmpty()) {
        byte[] startCol=start == null ? null : Util.decodeBinary(start,encoding);
        byte[] stopCol=stop == null ? null : Util.decodeBinary(stop,encoding);
        read=new Read(rowKey,startCol,stopCol,limit == null ? -1 : limit);
      }
 else {
        byte[][] cols=new byte[columns.size()][];
        int i=0;
        for (        String column : columns) {
          cols[i++]=Util.decodeBinary(column,encoding);
        }
        read=new Read(rowKey,cols);
      }
    }
 catch (    Exception e) {
      respondBadRequest(message,request,helper,""String_Node_Str"",e);
      return;
    }
    OperationResult<Map<byte[],byte[]>> result;
    try {
      result=table.read(read);
    }
 catch (    OperationException e) {
      helper.finish(Error);
      LOG.error(""String_Node_Str"" + e.getMessage(),e);
      respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR);
      return;
    }
    if (result.isEmpty() || result.getValue().isEmpty()) {
      helper.finish(NoData);
      respondError(message.getChannel(),HttpResponseStatus.NOT_FOUND);
    }
 else {
      Map<String,String> map=Maps.newTreeMap();
      for (      Map.Entry<byte[],byte[]> entry : result.getValue().entrySet()) {
        map.put(Util.encodeBinary(entry.getKey(),encoding),Util.encodeBinary(entry.getValue(),encoding,counter));
      }
      byte[] response=new Gson().toJson(map).getBytes(Charsets.UTF_8);
      respondSuccess(message.getChannel(),request,response);
      helper.finish(Success);
    }
  }
 else   if (operation.equals(TableOp.Delete)) {
    Delete delete;
    try {
      byte[][] cols=new byte[columns.size()][];
      int i=0;
      for (      String column : columns) {
        cols[i++]=Util.decodeBinary(column,encoding);
      }
      delete=new Delete(rowKey,cols);
    }
 catch (    Exception e) {
      respondBadRequest(message,request,helper,""String_Node_Str"",e);
      return;
    }
    try {
      table.write(delete);
    }
 catch (    OperationException e) {
      helper.finish(Error);
      LOG.error(""String_Node_Str"" + e.getMessage(),e);
      respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR);
      return;
    }
    helper.finish(Success);
    respondSuccess(message.getChannel(),request);
  }
 else   if (operation.equals(TableOp.Write)) {
    Write write;
    if (valueMap == null || valueMap.isEmpty()) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    try {
      byte[][] cols=new byte[valueMap.size()][];
      byte[][] vals=new byte[valueMap.size()][];
      int i=0;
      for (      Map.Entry<String,String> entry : valueMap.entrySet()) {
        cols[i]=Util.decodeBinary(entry.getKey(),encoding);
        vals[i]=Util.decodeBinary(entry.getValue(),encoding,counter);
        i++;
      }
      write=new Write(rowKey,cols,vals);
    }
 catch (    Exception e) {
      respondBadRequest(message,request,helper,""String_Node_Str"",e);
      return;
    }
    try {
      table.write(write);
    }
 catch (    OperationException e) {
      helper.finish(Error);
      LOG.error(""String_Node_Str"" + e.getMessage(),e);
      respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR);
      return;
    }
    helper.finish(Success);
    respondSuccess(message.getChannel(),request);
  }
 else   if (operation.equals(TableOp.Increment)) {
    Increment increment;
    if (valueMap == null || valueMap.isEmpty()) {
      respondBadRequest(message,request,helper,""String_Node_Str"");
      return;
    }
    try {
      byte[][] cols=new byte[valueMap.size()][];
      long[] vals=new long[valueMap.size()];
      int i=0;
      for (      Map.Entry<String,String> entry : valueMap.entrySet()) {
        cols[i]=Util.decodeBinary(entry.getKey(),encoding);
        vals[i]=Long.parseLong(entry.getValue());
        i++;
      }
      increment=new Increment(rowKey,cols,vals);
    }
 catch (    Exception e) {
      respondBadRequest(message,request,helper,""String_Node_Str"",e);
      return;
    }
    Map<byte[],Long> results;
    try {
      results=table.incrementAndGet(increment);
    }
 catch (    OperationException e) {
      if (StatusCode.ILLEGAL_INCREMENT == e.getStatus()) {
        respondBadRequest(message,request,helper,""String_Node_Str"");
      }
 else {
        helper.finish(Error);
        LOG.error(""String_Node_Str"" + e.getMessage(),e);
        respondError(message.getChannel(),HttpResponseStatus.INTERNAL_SERVER_ERROR);
      }
      return;
    }
    Map<String,Long> map=Maps.newTreeMap();
    for (    Map.Entry<byte[],Long> entry : results.entrySet()) {
      map.put(Util.encodeBinary(entry.getKey(),encoding),entry.getValue());
    }
    byte[] response=new Gson().toJson(map).getBytes(Charsets.UTF_8);
    respondSuccess(message.getChannel(),request,response);
    helper.finish(Success);
  }
}"
7900,"/** 
 * Check whether the Http return code is positive. If not, print the error message and return false. Otherwise, if verbose is on, print the response status line.
 * @param response the HTTP response
 * @return whether the response indicates success
 */
boolean checkHttpStatus(HttpResponse response){
  try {
    if (response.getStatusLine().getStatusCode() != HttpStatus.SC_OK) {
      String reason=response.getEntity().getContent() == null ? null : IOUtils.toString(response.getEntity().getContent());
      if (verbose) {
        System.out.println(response.getStatusLine());
        if (reason != null) {
          System.out.println(reason);
        }
      }
 else {
        if (reason != null) {
          System.err.println(response.getStatusLine().getReasonPhrase() + ""String_Node_Str"" + reason);
        }
 else {
          System.err.println(response.getStatusLine().getReasonPhrase());
        }
      }
      return false;
    }
    if (verbose) {
      System.out.println(response.getStatusLine());
    }
    return true;
  }
 catch (  IOException e) {
    System.err.println(""String_Node_Str"" + e.getMessage());
    return false;
  }
}","/** 
 * Check whether the Http return code is positive. If not, print the error message and return false. Otherwise, if verbose is on, print the response status line.
 * @param response the HTTP response
 * @return whether the response indicates success
 */
boolean checkHttpStatus(HttpResponse response){
  try {
    if (response.getStatusLine().getStatusCode() != HttpStatus.SC_OK) {
      String reason=response.getEntity().getContent() == null ? null : IOUtils.toString(response.getEntity().getContent());
      if (verbose) {
        System.out.println(response.getStatusLine());
        if (reason != null && !reason.isEmpty()) {
          System.out.println(reason);
        }
      }
 else {
        if (reason != null && !reason.isEmpty()) {
          System.err.println(response.getStatusLine().getReasonPhrase() + ""String_Node_Str"" + reason);
        }
 else {
          System.err.println(response.getStatusLine().getReasonPhrase());
        }
      }
      return false;
    }
    if (verbose) {
      System.out.println(response.getStatusLine());
    }
    return true;
  }
 catch (  IOException e) {
    System.err.println(""String_Node_Str"" + e.getMessage());
    return false;
  }
}"
7901,"private void init(CConfiguration configuration){
  LOG.debug(""String_Node_Str"");
  File testAppDir=Files.createTempDir();
  File outputDir=new File(testAppDir,""String_Node_Str"");
  tmpDir=new File(testAppDir,""String_Node_Str"");
  outputDir.mkdirs();
  tmpDir.mkdirs();
  configuration.set(""String_Node_Str"",outputDir.getAbsolutePath());
  configuration.set(""String_Node_Str"",tmpDir.getAbsolutePath());
  try {
    LOG.debug(""String_Node_Str"");
    appFabricService=getAppFabricClient();
  }
 catch (  TTransportException e) {
    LOG.error(""String_Node_Str"");
    Throwables.propagate(e);
  }
  Module dataFabricModule;
  if (configuration.get(""String_Node_Str"").equals(""String_Node_Str"")) {
    dataFabricModule=new DataFabricModules().getDistributedModules();
  }
 else {
    dataFabricModule=new DataFabricModules().getSingleNodeModules();
  }
  injector=Guice.createInjector(dataFabricModule,new AngryMamaModule(configuration),new AbstractModule(){
    @Override protected void configure(){
      install(new FactoryModuleBuilder().implement(ApplicationManager.class,DefaultBenchmarkManager.class).build(BenchmarkManagerFactory.class));
      install(new FactoryModuleBuilder().implement(StreamWriter.class,MultiThreadedStreamWriter.class).build(BenchmarkStreamWriterFactory.class));
      install(new FactoryModuleBuilder().implement(ProcedureClient.class,DefaultProcedureClient.class).build(ProcedureClientFactory.class));
    }
  }
,new Module(){
    @Override public void configure(    Binder binder){
      binder.bind(AppFabricService.Iface.class).toInstance(appFabricService);
    }
  }
);
  DiscoveryServiceClient discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  discoveryServiceClient.startAndWait();
  injector.getInstance(DiscoveryService.class).startAndWait();
  locationFactory=injector.getInstance(LocationFactory.class);
}","private void init(CConfiguration configuration){
  LOG.debug(""String_Node_Str"");
  File testAppDir=Files.createTempDir();
  File outputDir=new File(testAppDir,""String_Node_Str"");
  tmpDir=new File(testAppDir,""String_Node_Str"");
  outputDir.mkdirs();
  tmpDir.mkdirs();
  configuration.set(""String_Node_Str"",outputDir.getAbsolutePath());
  configuration.set(""String_Node_Str"",tmpDir.getAbsolutePath());
  try {
    LOG.debug(""String_Node_Str"");
    appFabricService=getAppFabricClient();
  }
 catch (  TTransportException e) {
    LOG.error(""String_Node_Str"");
    Throwables.propagate(e);
  }
  Module dataFabricModule;
  if (configuration.get(""String_Node_Str"") != null && configuration.get(""String_Node_Str"").equals(""String_Node_Str"")) {
    dataFabricModule=new DataFabricModules().getDistributedModules();
  }
 else {
    dataFabricModule=new DataFabricModules().getSingleNodeModules();
  }
  injector=Guice.createInjector(dataFabricModule,new AngryMamaModule(configuration),new AbstractModule(){
    @Override protected void configure(){
      install(new FactoryModuleBuilder().implement(ApplicationManager.class,DefaultBenchmarkManager.class).build(BenchmarkManagerFactory.class));
      install(new FactoryModuleBuilder().implement(StreamWriter.class,GatewayStreamWriter.class).build(BenchmarkStreamWriterFactory.class));
      install(new FactoryModuleBuilder().implement(ProcedureClient.class,DefaultProcedureClient.class).build(ProcedureClientFactory.class));
    }
  }
,new Module(){
    @Override public void configure(    Binder binder){
      binder.bind(AppFabricService.Iface.class).toInstance(appFabricService);
    }
  }
);
  DiscoveryServiceClient discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  discoveryServiceClient.startAndWait();
  injector.getInstance(DiscoveryService.class).startAndWait();
  locationFactory=injector.getInstance(LocationFactory.class);
}"
7902,"/** 
 * Configures the metrics collection server and registers the address and port the server is running on.
 * @param args command line arguments
 * @param conf configuration object.
 * @return instance of server info to be used for service registration.
 */
@Override protected RegisteredServerInfo configure(String[] args,CConfiguration conf){
  try {
    if (handler == null) {
      handler=new MetricsCollectionServerIoHandler(conf);
    }
    int serverPort=conf.getInt(Constants.CFG_METRICS_COLLECTOR_SERVER_PORT,Constants.DEFAULT_METRICS_COLLECTOR_SERVER_PORT);
    InetAddress serverAddress=getServerInetAddress(conf.get(Constants.CFG_METRICS_COLLECTOR_SERVER_ADDRESS,Constants.DEFAULT_METRICS_COLLECTOR_SERVER_ADDRESS));
    MBeanServer mBeanServer=handler.getmBeanServer();
    acceptor=new NioSocketAcceptor();
    acceptor.setReuseAddress(true);
    acceptor.getSessionConfig().setKeepAlive(true);
    acceptor.getSessionConfig().setTcpNoDelay(true);
    IoServiceMBean acceptorMBean=new IoServiceMBean(acceptor);
    if (mBeanServer != null) {
      ObjectName acceptorName=new ObjectName(acceptor.getClass().getPackage().getName() + ""String_Node_Str"" + acceptor.getClass().getSimpleName());
      mBeanServer.registerMBean(acceptorMBean,acceptorName);
    }
    ProtocolCodecFilter protocolFilter=new ProtocolCodecFilter(new MetricCodecFactory(false));
    IoFilterMBean protocolFilterMBean=new IoFilterMBean(protocolFilter);
    if (mBeanServer != null) {
      ObjectName protocolFilterName=new ObjectName(protocolFilter.getClass().getPackage().getName() + ""String_Node_Str"" + protocolFilter.getClass().getSimpleName());
      mBeanServer.registerMBean(protocolFilterMBean,protocolFilterName);
    }
    acceptor.getFilterChain().addLast(""String_Node_Str"",protocolFilter);
    DefaultIoFilterChainBuilder filterChainBuilder=acceptor.getFilterChain();
    filterChainBuilder.addLast(""String_Node_Str"",new ExecutorFilter(new ThreadPoolExecutor(IDLE_THREAD_POOL_SIZE,THREAD_POOL_SIZE,5 * 60,TimeUnit.SECONDS,new ArrayBlockingQueue<Runnable>(WORKER_QUEUE_LENGTH))));
    acceptor.setHandler(handler);
    acceptor.bind(new InetSocketAddress(serverAddress,serverPort));
    setServerName(Constants.SERVICE_METRICS_COLLECTION_SERVER);
    Runtime.getRuntime().addShutdownHook(new Thread(new Runnable(){
      @Override public void run(){
        try {
          Log.info(""String_Node_Str"");
          handler.close();
        }
 catch (        IOException e) {
          Log.error(""String_Node_Str"",e.getMessage());
        }
      }
    }
));
    RegisteredServerInfo registrationInfo=new RegisteredServerInfo(serverAddress.getHostAddress(),serverPort);
    return registrationInfo;
  }
 catch (  Exception e) {
    Log.debug(StackTraceUtil.toStringStackTrace(e));
    Log.error(""String_Node_Str"",e.getMessage());
  }
  return null;
}","/** 
 * Configures the metrics collection server and registers the address and port the server is running on.
 * @param args command line arguments
 * @param conf configuration object.
 * @return instance of server info to be used for service registration.
 */
@Override protected RegisteredServerInfo configure(String[] args,CConfiguration conf){
  try {
    if (handler == null) {
      handler=new MetricsCollectionServerIoHandler(conf);
    }
    int serverPort=conf.getInt(Constants.CFG_METRICS_COLLECTOR_SERVER_PORT,Constants.DEFAULT_METRICS_COLLECTOR_SERVER_PORT);
    InetAddress serverAddress=getServerInetAddress(conf.get(Constants.CFG_METRICS_COLLECTOR_SERVER_ADDRESS,Constants.DEFAULT_METRICS_COLLECTOR_SERVER_ADDRESS));
    MBeanServer mBeanServer=handler.getmBeanServer();
    acceptor=new NioSocketAcceptor();
    acceptor.setReuseAddress(true);
    acceptor.getSessionConfig().setKeepAlive(true);
    acceptor.getSessionConfig().setTcpNoDelay(true);
    IoServiceMBean acceptorMBean=new IoServiceMBean(acceptor);
    if (mBeanServer != null) {
      ObjectName acceptorName=new ObjectName(acceptor.getClass().getPackage().getName() + ""String_Node_Str"" + acceptor.getClass().getSimpleName());
      mBeanServer.registerMBean(acceptorMBean,acceptorName);
    }
    ProtocolCodecFilter protocolFilter=new ProtocolCodecFilter(new MetricCodecFactory(false));
    IoFilterMBean protocolFilterMBean=new IoFilterMBean(protocolFilter);
    if (mBeanServer != null) {
      ObjectName protocolFilterName=new ObjectName(protocolFilter.getClass().getPackage().getName() + ""String_Node_Str"" + protocolFilter.getClass().getSimpleName());
      mBeanServer.registerMBean(protocolFilterMBean,protocolFilterName);
    }
    acceptor.getFilterChain().addLast(""String_Node_Str"",protocolFilter);
    DefaultIoFilterChainBuilder filterChainBuilder=acceptor.getFilterChain();
    filterChainBuilder.addLast(""String_Node_Str"",new ExecutorFilter(new ThreadPoolExecutor(IDLE_THREAD_POOL_SIZE,THREAD_POOL_SIZE,5 * 60,TimeUnit.SECONDS,new ArrayBlockingQueue<Runnable>(WORKER_QUEUE_LENGTH))));
    acceptor.setHandler(handler);
    acceptor.bind(new InetSocketAddress(serverAddress,serverPort));
    setServerName(Constants.SERVICE_METRICS_COLLECTION_SERVER);
    Runtime.getRuntime().addShutdownHook(new Thread(new Runnable(){
      @Override public void run(){
        try {
          Log.info(""String_Node_Str"");
          handler.close();
        }
 catch (        IOException e) {
          Log.error(""String_Node_Str"",e.getMessage());
        }
      }
    }
));
    RegisteredServerInfo registrationInfo=new RegisteredServerInfo(serverAddress.getHostAddress(),serverPort);
    return registrationInfo;
  }
 catch (  Exception e) {
    Log.error(""String_Node_Str"",e.getMessage(),e);
  }
  return null;
}"
7903,"Manager<?,?> create();","<U,V>Manager<U,V> create();"
7904,"/** 
 * Creates new application if it doesn't exist. Updates existing one otherwise.
 * @param id            application id
 * @param specification application specification to store
 * @param appArchiveLocation location of the deployed app archive
 * @throws OperationException
 */
void addApplication(Id.Application id,ApplicationSpecification specification,com.continuuity.weave.filesystem.Location appArchiveLocation) throws OperationException ;","/** 
 * Creates new application if it doesn't exist. Updates existing one otherwise.
 * @param id            application id
 * @param specification application specification to store
 * @param appArchiveLocation location of the deployed app archive
 * @throws OperationException
 */
void addApplication(Id.Application id,ApplicationSpecification specification,Location appArchiveLocation) throws OperationException ;"
7905,"/** 
 * Creates a JarResources using a   {@link Location}. It extracts all resources from a Jar into a internal map, keyed by resource names.
 * @param jar location of JAR file.
 * @throws IOException
 */
public JarResources(com.continuuity.weave.filesystem.Location jar) throws IOException {
  manifest=init(jar);
}","/** 
 * Creates a JarResources using a   {@link Location}. It extracts all resources from a Jar into a internal map, keyed by resource names.
 * @param jar location of JAR file.
 * @throws IOException
 */
public JarResources(Location jar) throws IOException {
  manifest=init(jar);
}"
7906,"/** 
 * Executes the <code>Application.configure</code> within the same JVM. <p> This method could be dangerous and should be used only in singlenode. </p>
 * @return A instance of {@link ListenableFuture}.
 */
@Override public ListenableFuture<ConfigResponse> config(){
  StringWriter writer=null;
  SettableFuture result=SettableFuture.create();
  try {
    Application app=null;
    if (archive != null && application == null) {
      Object mainClass=new Archive(id,archive).getMainClass().newInstance();
      app=(Application)mainClass;
    }
 else     if (application != null && archive == null) {
      app=application;
    }
 else {
      throw new IllegalStateException(""String_Node_Str"");
    }
    ApplicationSpecification specification=app.configure();
    writer=new StringWriter();
    ApplicationSpecificationAdapter.create(new ReflectionSchemaGenerator()).toJson(specification,writer);
    result.set(new DefaultConfigResponse(0,newStringStream(writer.toString())));
  }
 catch (  Exception e) {
    LOG.debug(StackTraceUtil.toStringStackTrace(e));
    return Futures.immediateFailedFuture(e);
  }
catch (  Throwable throwable) {
    LOG.debug(StackTraceUtil.toStringStackTrace(throwable));
    return Futures.immediateFailedFuture(throwable);
  }
 finally {
    if (writer != null) {
      try {
        writer.close();
      }
 catch (      IOException e) {
        LOG.debug(StackTraceUtil.toStringStackTrace(e));
        return Futures.immediateFailedFuture(e);
      }
    }
  }
  return result;
}","/** 
 * Executes the <code>Application.configure</code> within the same JVM. <p> This method could be dangerous and should be used only in singlenode. </p>
 * @return A instance of {@link ListenableFuture}.
 */
@Override public ListenableFuture<ConfigResponse> config(){
  StringWriter writer=null;
  SettableFuture result=SettableFuture.create();
  try {
    Application app;
    if (archive != null && application == null) {
      Object mainClass=new Archive(id,archive).getMainClass().newInstance();
      app=(Application)mainClass;
    }
 else     if (application != null && archive == null) {
      app=application;
    }
 else {
      throw new IllegalStateException(""String_Node_Str"");
    }
    ApplicationSpecification specification=app.configure();
    writer=new StringWriter();
    ApplicationSpecificationAdapter.create(new ReflectionSchemaGenerator()).toJson(specification,writer);
    result.set(new DefaultConfigResponse(0,newStringStream(writer.toString())));
  }
 catch (  Throwable t) {
    LOG.error(t.getMessage(),t);
    return Futures.immediateFailedFuture(t);
  }
 finally {
    if (writer != null) {
      try {
        writer.close();
      }
 catch (      IOException e) {
        LOG.debug(StackTraceUtil.toStringStackTrace(e));
        return Futures.immediateFailedFuture(e);
      }
    }
  }
  return result;
}"
7907,"/** 
 * Constructs the object with identifier and resource info provided.
 * @param info about the resource being uploaded.
 */
public SessionInfo(ResourceIdentifier identifier,ResourceInfo info,com.continuuity.weave.filesystem.Location archive,DeployStatus status){
  this.identifier=identifier;
  this.regtime=System.currentTimeMillis() / 1000;
  this.filename=info.getFilename();
  this.size=info.getSize();
  this.archive=archive;
  this.status=status;
}","/** 
 * Constructs the object with identifier and resource info provided.
 * @param info about the resource being uploaded.
 */
public SessionInfo(ResourceIdentifier identifier,ResourceInfo info,Location archive,DeployStatus status){
  this.identifier=identifier;
  this.regtime=System.currentTimeMillis() / 1000;
  this.filename=info.getFilename();
  this.size=info.getSize();
  this.archive=archive;
  this.status=status;
}"
7908,"/** 
 * Returns location of the resource.
 * @return Location to the resource.
 */
public com.continuuity.weave.filesystem.Location getArchiveLocation(){
  return archive;
}","/** 
 * Returns location of the resource.
 * @return Location to the resource.
 */
public Location getArchiveLocation(){
  return archive;
}"
7909,"/** 
 * Creates a   {@link com.continuuity.internal.app.deploy.InMemoryConfigurator} to run throughthe process of generation of  {@link ApplicationSpecification}
 * @param archive Location of archive.
 */
@Override public void process(com.continuuity.weave.filesystem.Location archive) throws Exception {
  InMemoryConfigurator inMemoryConfigurator=new InMemoryConfigurator(id,archive);
  ListenableFuture<ConfigResponse> result=inMemoryConfigurator.config();
  ConfigResponse response=result.get(120,TimeUnit.SECONDS);
  ApplicationSpecification specification=adapter.fromJson(response.get());
  Id.Application appId=Id.Application.from(id,specification.getName());
  emit(new ApplicationSpecLocation(appId,specification,archive));
}","/** 
 * Creates a   {@link com.continuuity.internal.app.deploy.InMemoryConfigurator} to run throughthe process of generation of  {@link ApplicationSpecification}
 * @param archive Location of archive.
 */
@Override public void process(Location archive) throws Exception {
  InMemoryConfigurator inMemoryConfigurator=new InMemoryConfigurator(id,archive);
  ListenableFuture<ConfigResponse> result=inMemoryConfigurator.config();
  ConfigResponse response=result.get(120,TimeUnit.SECONDS);
  ApplicationSpecification specification=adapter.fromJson(response.get());
  Id.Application appId=Id.Application.from(id,specification.getName());
  emit(new ApplicationSpecLocation(appId,specification,archive));
}"
7910,"@Override public void process(final ApplicationSpecLocation o) throws Exception {
  ImmutableList.Builder<Program> programs=ImmutableList.builder();
  ApplicationSpecification appSpec=o.getSpecification();
  String applicationName=appSpec.getName();
  ArchiveBundler bundler=new ArchiveBundler(o.getArchive());
  com.continuuity.weave.filesystem.Location outputDir=locationFactory.create(configuration.get(Constants.CFG_APP_FABRIC_OUTPUT_DIR,System.getProperty(""String_Node_Str"")));
  com.continuuity.weave.filesystem.Location newOutputDir=outputDir.append(o.getApplicationId().getAccountId());
  if (!newOutputDir.exists() && !newOutputDir.mkdirs() && !newOutputDir.exists()) {
    throw new IOException(""String_Node_Str"");
  }
  for (  FlowSpecification flow : appSpec.getFlows().values()) {
    String name=String.format(Locale.ENGLISH,""String_Node_Str"",Type.FLOW.toString(),applicationName);
    com.continuuity.weave.filesystem.Location flowAppDir=newOutputDir.append(name);
    if (!flowAppDir.exists()) {
      flowAppDir.mkdirs();
    }
    com.continuuity.weave.filesystem.Location output=flowAppDir.append(String.format(""String_Node_Str"",flow.getName()));
    Location loc=ProgramBundle.create(o.getApplicationId(),bundler,output,flow.getName(),flow.getClassName(),Type.FLOW,appSpec);
    programs.add(new Program(loc));
  }
  for (  ProcedureSpecification procedure : appSpec.getProcedures().values()) {
    String name=String.format(Locale.ENGLISH,""String_Node_Str"",Type.PROCEDURE.toString(),applicationName);
    com.continuuity.weave.filesystem.Location procedureAppDir=newOutputDir.append(name);
    if (!procedureAppDir.exists()) {
      procedureAppDir.mkdirs();
    }
    com.continuuity.weave.filesystem.Location output=procedureAppDir.append(String.format(""String_Node_Str"",procedure.getName()));
    Location loc=ProgramBundle.create(o.getApplicationId(),bundler,output,procedure.getName(),procedure.getClassName(),Type.PROCEDURE,appSpec);
    programs.add(new Program(loc));
  }
  for (  MapReduceSpecification job : appSpec.getMapReduces().values()) {
    String name=String.format(Locale.ENGLISH,""String_Node_Str"",Type.MAPREDUCE.toString(),applicationName);
    com.continuuity.weave.filesystem.Location jobAppDir=newOutputDir.append(name);
    if (!jobAppDir.exists()) {
      jobAppDir.mkdirs();
    }
    com.continuuity.weave.filesystem.Location output=jobAppDir.append(String.format(""String_Node_Str"",job.getName()));
    Location loc=ProgramBundle.create(o.getApplicationId(),bundler,output,job.getName(),job.getClassName(),Type.MAPREDUCE,appSpec);
    programs.add(new Program(loc));
  }
  emit(new ApplicationWithPrograms(o,programs.build()));
}","@Override public void process(final ApplicationSpecLocation o) throws Exception {
  ImmutableList.Builder<Program> programs=ImmutableList.builder();
  ApplicationSpecification appSpec=o.getSpecification();
  String applicationName=appSpec.getName();
  ArchiveBundler bundler=new ArchiveBundler(o.getArchive());
  Location outputDir=locationFactory.create(configuration.get(Constants.CFG_APP_FABRIC_OUTPUT_DIR,System.getProperty(""String_Node_Str"")));
  Location newOutputDir=outputDir.append(o.getApplicationId().getAccountId());
  if (!newOutputDir.exists() && !newOutputDir.mkdirs() && !newOutputDir.exists()) {
    throw new IOException(""String_Node_Str"");
  }
  for (  FlowSpecification flow : appSpec.getFlows().values()) {
    String name=String.format(Locale.ENGLISH,""String_Node_Str"",Type.FLOW.toString(),applicationName);
    Location flowAppDir=newOutputDir.append(name);
    if (!flowAppDir.exists()) {
      flowAppDir.mkdirs();
    }
    Location output=flowAppDir.append(String.format(""String_Node_Str"",flow.getName()));
    Location loc=ProgramBundle.create(o.getApplicationId(),bundler,output,flow.getName(),flow.getClassName(),Type.FLOW,appSpec);
    programs.add(new Program(loc));
  }
  for (  ProcedureSpecification procedure : appSpec.getProcedures().values()) {
    String name=String.format(Locale.ENGLISH,""String_Node_Str"",Type.PROCEDURE.toString(),applicationName);
    Location procedureAppDir=newOutputDir.append(name);
    if (!procedureAppDir.exists()) {
      procedureAppDir.mkdirs();
    }
    Location output=procedureAppDir.append(String.format(""String_Node_Str"",procedure.getName()));
    Location loc=ProgramBundle.create(o.getApplicationId(),bundler,output,procedure.getName(),procedure.getClassName(),Type.PROCEDURE,appSpec);
    programs.add(new Program(loc));
  }
  for (  MapReduceSpecification job : appSpec.getMapReduces().values()) {
    String name=String.format(Locale.ENGLISH,""String_Node_Str"",Type.MAPREDUCE.toString(),applicationName);
    Location jobAppDir=newOutputDir.append(name);
    if (!jobAppDir.exists()) {
      jobAppDir.mkdirs();
    }
    Location output=jobAppDir.append(String.format(""String_Node_Str"",job.getName()));
    Location loc=ProgramBundle.create(o.getApplicationId(),bundler,output,job.getName(),job.getClassName(),Type.MAPREDUCE,appSpec);
    programs.add(new Program(loc));
  }
  emit(new ApplicationWithPrograms(o,programs.build()));
}"
7911,"public ProgramGenerationStage(Configuration configuration,com.continuuity.weave.filesystem.LocationFactory locationFactory){
  super(TypeToken.of(ApplicationSpecLocation.class));
  this.configuration=configuration;
  this.locationFactory=locationFactory;
}","public ProgramGenerationStage(Configuration configuration,LocationFactory locationFactory){
  super(TypeToken.of(ApplicationSpecLocation.class));
  this.configuration=configuration;
  this.locationFactory=locationFactory;
}"
7912,"@Override public Cancellable submit(final MapReduce job,com.continuuity.weave.filesystem.Location jobJarLocation,final BasicMapReduceContext context,final JobFinishCallback callback) throws Exception {
  final Job jobConf=Job.getInstance(conf);
  context.setJob(jobConf);
  job.beforeSubmit(context);
  DataSet inputDataset=setInputDataSetIfNeeded(jobConf,context);
  DataSet outputDataset=setOutputDataSetIfNeeded(jobConf,context);
  boolean useDataSetAsInputOrOutput=inputDataset != null || outputDataset != null;
  if (useDataSetAsInputOrOutput) {
    MapReduceContextAccessor.setRunId(jobConf.getConfiguration(),context.getRunId().getId());
    MapReduceContextAccessor.put(context.getRunId().getId(),context);
  }
  jobConf.addArchiveToClassPath(new Path(jobJarLocation.toURI().getPath()));
  new Thread(){
    @Override public void run(){
      try {
        LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
        boolean success;
        try {
          LOG.info(""String_Node_Str"",context.toString());
          success=jobConf.waitForCompletion(true);
        }
 catch (        InterruptedException e) {
          throw Throwables.propagate(e);
        }
catch (        Exception e) {
          throw Throwables.propagate(e);
        }
        job.onFinish(success,context);
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
 finally {
        callback.onFinished(true);
      }
    }
  }
.start();
  return new Cancellable(){
    @Override public void cancel(){
      try {
        jobConf.killJob();
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
    }
  }
;
}","@Override public Cancellable submit(final MapReduce job,Location jobJarLocation,final BasicMapReduceContext context,final JobFinishCallback callback) throws Exception {
  final Job jobConf=Job.getInstance(conf);
  context.setJob(jobConf);
  job.beforeSubmit(context);
  DataSet inputDataset=setInputDataSetIfNeeded(jobConf,context);
  DataSet outputDataset=setOutputDataSetIfNeeded(jobConf,context);
  boolean useDataSetAsInputOrOutput=inputDataset != null || outputDataset != null;
  if (useDataSetAsInputOrOutput) {
    MapReduceContextAccessor.setRunId(jobConf.getConfiguration(),context.getRunId().getId());
    MapReduceContextAccessor.put(context.getRunId().getId(),context);
  }
  jobConf.addArchiveToClassPath(new Path(jobJarLocation.toURI().getPath()));
  new Thread(){
    @Override public void run(){
      try {
        LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
        boolean success;
        try {
          LOG.info(""String_Node_Str"",context.toString());
          success=jobConf.waitForCompletion(true);
        }
 catch (        InterruptedException e) {
          throw Throwables.propagate(e);
        }
catch (        Exception e) {
          throw Throwables.propagate(e);
        }
        job.onFinish(success,context);
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
 finally {
        callback.onFinished(true);
      }
    }
  }
.start();
  return new Cancellable(){
    @Override public void cancel(){
      try {
        jobConf.killJob();
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
    }
  }
;
}"
7913,"private void logProcessed(){
  String[] flowlets={""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  for (  String flowlet : flowlets)   LOG.info(""String_Node_Str"" + flowlet + ""String_Node_Str""+ BenchmarkRuntimeStats.getFlowletMetrics(appName,flowName,flowlet).getProcessed());
}","private void logProcessed(){
  String[] flowlets={""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  for (  String flowlet : flowlets) {
    LOG.info(""String_Node_Str"" + flowlet + ""String_Node_Str""+ BenchmarkRuntimeStats.getFlowletMetrics(appName,flowName,flowlet).getProcessed());
  }
}"
7914,"public void testApp(){
  ApplicationManager bam=deployApplication(appClass);
  LOG.info(""String_Node_Str"");
  MensaMetricsReporter mmr=new MensaMetricsReporter(""String_Node_Str"",4242,ImmutableList.of(""String_Node_Str""),""String_Node_Str"",10);
  try {
    LOG.info(""String_Node_Str"");
    FlowManager flowMgr=bam.startFlow(flowName);
    LOG.info(""String_Node_Str"");
    StreamWriter kvStream=bam.getStreamWriter(""String_Node_Str"");
    for (int i=0; i < 40000; i++) {
      kvStream.send(""String_Node_Str"" + i + ""String_Node_Str""+ ""String_Node_Str""+ i);
    }
    Map<String,Double> allCounters=BenchmarkRuntimeStats.getCounters(appName,flowName);
    BenchmarkRuntimeMetrics sourceMetrics=BenchmarkRuntimeStats.getFlowletMetrics(appName,flowName,""String_Node_Str"");
    logProcessed();
    try {
      sourceMetrics.waitForProcessed(10000,30,TimeUnit.SECONDS);
    }
 catch (    TimeoutException te) {
      LOG.warn(""String_Node_Str"");
    }
    logProcessed();
    Counter meanReadRate=BenchmarkRuntimeStats.getCounter(appName,flowName,""String_Node_Str"",""String_Node_Str"");
    Map<String,Double> sourceCounters=BenchmarkRuntimeStats.getCounters(appName,flowName,""String_Node_Str"");
    Map<String,Double> writerCounters=BenchmarkRuntimeStats.getCounters(appName,flowName,""String_Node_Str"");
    Map<String,Double> readerCounters=BenchmarkRuntimeStats.getCounters(appName,flowName,""String_Node_Str"");
    BenchmarkRuntimeMetrics readerMetrics=BenchmarkRuntimeStats.getFlowletMetrics(appName,flowName,""String_Node_Str"");
    logProcessed();
    readerMetrics.waitForProcessed(10000,30,TimeUnit.SECONDS);
    logProcessed();
    Counter datasetStorageWordCountsCount=BenchmarkRuntimeStats.getCounter(""String_Node_Str"");
    mmr.reportNow(""String_Node_Str"",1000.0);
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
 finally {
    LOG.info(""String_Node_Str"");
    mmr.shutdown();
  }
}","public void testApp(){
  LOG.info(""String_Node_Str"");
  clearAppFabric();
  LOG.info(""String_Node_Str"",appName);
  ApplicationManager bam=deployApplication(appClass);
  LOG.info(""String_Node_Str"");
  MensaMetricsReporter mmr=new MensaMetricsReporter(""String_Node_Str"",4242,ImmutableList.of(""String_Node_Str""),""String_Node_Str"",10);
  try {
    LOG.info(""String_Node_Str"",flowName);
    FlowManager flowMgr=bam.startFlow(flowName);
    LOG.info(""String_Node_Str"");
    StreamWriter kvStream=bam.getStreamWriter(""String_Node_Str"");
    for (int i=0; i < 40000; i++) {
      kvStream.send(""String_Node_Str"" + i + ""String_Node_Str""+ ""String_Node_Str""+ i);
    }
    Map<String,Double> allCounters=BenchmarkRuntimeStats.getCounters(appName,flowName);
    BenchmarkRuntimeMetrics sourceMetrics=BenchmarkRuntimeStats.getFlowletMetrics(appName,flowName,""String_Node_Str"");
    logProcessed();
    try {
      sourceMetrics.waitForProcessed(10000,30,TimeUnit.SECONDS);
    }
 catch (    TimeoutException te) {
      LOG.warn(""String_Node_Str"");
    }
    logProcessed();
    BenchmarkRuntimeMetrics readerMetrics=BenchmarkRuntimeStats.getFlowletMetrics(appName,flowName,""String_Node_Str"");
    logProcessed();
    readerMetrics.waitForProcessed(10000,30,TimeUnit.SECONDS);
    logProcessed();
    mmr.reportNow(""String_Node_Str"",1000.0);
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
 finally {
    LOG.info(""String_Node_Str"");
    mmr.shutdown();
    LOG.info(""String_Node_Str"");
    bam.stopAll();
    LOG.info(""String_Node_Str"");
    clearAppFabric();
  }
}"
7915,"private List<Split> getInputSelection(){
  String splitClassName=jobContext.getConfiguration().get(HCONF_ATTR_INPUT_SPLIT_CLASS);
  String splitsJson=jobContext.getConfiguration().get(HCONF_ATTR_INPUT_SPLITS);
  try {
    @SuppressWarnings(""String_Node_Str"") Class<? extends Split> splitClass=(Class<? extends Split>)Class.forName(splitClassName);
    return new Gson().fromJson(splitsJson,new ListSplitType(splitClass));
  }
 catch (  ClassNotFoundException e) {
    throw Throwables.propagate(e);
  }
}","private List<Split> getInputSelection(){
  String splitClassName=jobContext.getConfiguration().get(HCONF_ATTR_INPUT_SPLIT_CLASS);
  String splitsJson=jobContext.getConfiguration().get(HCONF_ATTR_INPUT_SPLITS);
  try {
    @SuppressWarnings(""String_Node_Str"") Class<? extends Split> splitClass=(Class<? extends Split>)jobContext.getConfiguration().getClassLoader().loadClass(splitClassName);
    return new Gson().fromJson(splitsJson,new ListSplitType(splitClass));
  }
 catch (  ClassNotFoundException e) {
    throw Throwables.propagate(e);
  }
}"
7916,"private void submit(final MapReduce job,Location jobJarLocation,final BasicMapReduceContext context) throws Exception {
  jobConf=Job.getInstance(hConf);
  context.setJob(jobConf);
  job.beforeSubmit(context);
  wrapMapperClassIfNeeded(jobConf);
  wrapReducerClassIfNeeded(jobConf);
  DataSet inputDataset=setInputDataSetIfNeeded(jobConf,context);
  DataSet outputDataset=setOutputDataSetIfNeeded(jobConf,context);
  boolean useDataSetAsInputOrOutput=inputDataset != null || outputDataset != null;
  if (useDataSetAsInputOrOutput) {
    MapReduceContextProvider contextProvider=new MapReduceContextProvider(jobConf);
    contextProvider.set(context,cConf);
  }
  addContinuuityJarsToClasspath(jobConf);
  jobConf.setJar(jobJarLocation.toURI().getPath());
  new Thread(){
    @Override public void run(){
      boolean success=false;
      try {
        LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
        try {
          LOG.info(""String_Node_Str"",context.toString());
          success=jobConf.waitForCompletion(true);
        }
 catch (        InterruptedException e) {
          throw Throwables.propagate(e);
        }
catch (        Exception e) {
          throw Throwables.propagate(e);
        }
        job.onFinish(success,context);
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
 finally {
        stopController(success);
      }
    }
  }
.start();
}","private void submit(final MapReduce job,Location jobJarLocation,final BasicMapReduceContext context) throws Exception {
  jobConf=Job.getInstance(hConf);
  context.setJob(jobConf);
  job.beforeSubmit(context);
  wrapMapperClassIfNeeded(jobConf);
  wrapReducerClassIfNeeded(jobConf);
  DataSet inputDataset=setInputDataSetIfNeeded(jobConf,context);
  DataSet outputDataset=setOutputDataSetIfNeeded(jobConf,context);
  boolean useDataSetAsInputOrOutput=inputDataset != null || outputDataset != null;
  if (useDataSetAsInputOrOutput) {
    MapReduceContextProvider contextProvider=new MapReduceContextProvider(jobConf);
    contextProvider.set(context,cConf);
  }
  addContinuuityJarsToClasspath(jobConf);
  jobConf.setJar(jobJarLocation.toURI().getPath());
  jobConf.getConfiguration().setClassLoader(context.getProgram().getClassLoader());
  new Thread(){
    @Override public void run(){
      boolean success=false;
      try {
        LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
        try {
          LOG.info(""String_Node_Str"",context.toString());
          success=jobConf.waitForCompletion(true);
        }
 catch (        InterruptedException e) {
          throw Throwables.propagate(e);
        }
catch (        Exception e) {
          throw Throwables.propagate(e);
        }
        job.onFinish(success,context);
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
 finally {
        stopController(success);
      }
    }
  }
.start();
}"
7917,"@Override public void readFields(final DataInput in) throws IOException {
  try {
    Class<? extends Split> splitClass=(Class<Split>)Class.forName(Text.readString(in));
    split=new Gson().fromJson(Text.readString(in),splitClass);
  }
 catch (  ClassNotFoundException e) {
    throw Throwables.propagate(e);
  }
}","@Override public void readFields(final DataInput in) throws IOException {
  try {
    ClassLoader classLoader=Thread.currentThread().getContextClassLoader();
    if (classLoader == null) {
      classLoader=getClass().getClassLoader();
    }
    Class<? extends Split> splitClass=(Class<Split>)classLoader.loadClass(Text.readString(in));
    split=new Gson().fromJson(Text.readString(in),splitClass);
  }
 catch (  ClassNotFoundException e) {
    throw Throwables.propagate(e);
  }
}"
7918,"@Override public void run(Context context) throws IOException, InterruptedException {
  String userMapper=context.getConfiguration().get(ATTR_MAPPER_CLASS);
  Mapper delegate=createMapperInstance(userMapper);
  MapReduceContextProvider mrContextProvider=new MapReduceContextProvider(context);
  BasicMapReduceContext basicMapReduceContext=mrContextProvider.get();
  basicMapReduceContext.injectFields(delegate);
  LoggingContextAccessor.setLoggingContext(basicMapReduceContext.getLoggingContext());
  delegate.run(context);
}","@Override public void run(Context context) throws IOException, InterruptedException {
  String userMapper=context.getConfiguration().get(ATTR_MAPPER_CLASS);
  Mapper delegate=createMapperInstance(context,userMapper);
  MapReduceContextProvider mrContextProvider=new MapReduceContextProvider(context);
  BasicMapReduceContext basicMapReduceContext=mrContextProvider.get();
  basicMapReduceContext.injectFields(delegate);
  LoggingContextAccessor.setLoggingContext(basicMapReduceContext.getLoggingContext());
  delegate.run(context);
}"
7919,"private Mapper createMapperInstance(String userMapper){
  try {
    return (Mapper)Class.forName(userMapper).newInstance();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"" + userMapper);
    throw Throwables.propagate(e);
  }
}","private Mapper createMapperInstance(Context context,String userMapper){
  try {
    return (Mapper)context.getConfiguration().getClassLoader().loadClass(userMapper).newInstance();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"" + userMapper);
    throw Throwables.propagate(e);
  }
}"
7920,"@Override public void run(Context context) throws IOException, InterruptedException {
  String userReducer=context.getConfiguration().get(ATTR_REDUCER_CLASS);
  Reducer delegate=createReducerInstance(userReducer);
  MapReduceContextProvider mrContextProvider=new MapReduceContextProvider(context);
  BasicMapReduceContext basicMapReduceContext=mrContextProvider.get();
  basicMapReduceContext.injectFields(delegate);
  LoggingContextAccessor.setLoggingContext(basicMapReduceContext.getLoggingContext());
  delegate.run(context);
}","@Override public void run(Context context) throws IOException, InterruptedException {
  String userReducer=context.getConfiguration().get(ATTR_REDUCER_CLASS);
  Reducer delegate=createReducerInstance(context,userReducer);
  MapReduceContextProvider mrContextProvider=new MapReduceContextProvider(context);
  BasicMapReduceContext basicMapReduceContext=mrContextProvider.get();
  basicMapReduceContext.injectFields(delegate);
  LoggingContextAccessor.setLoggingContext(basicMapReduceContext.getLoggingContext());
  delegate.run(context);
}"
7921,"private Reducer createReducerInstance(String userReducer){
  try {
    return (Reducer)Class.forName(userReducer).newInstance();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"" + userReducer);
    throw Throwables.propagate(e);
  }
}","private Reducer createReducerInstance(Context context,String userReducer){
  try {
    return (Reducer)context.getConfiguration().getClassLoader().loadClass(userReducer).newInstance();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"" + userReducer);
    throw Throwables.propagate(e);
  }
}"
7922,"public Job(JobID jobid,String jobSubmitDir) throws IOException {
  this.systemJobDir=new Path(jobSubmitDir);
  this.systemJobFile=new Path(systemJobDir,""String_Node_Str"");
  this.id=jobid;
  JobConf conf=new JobConf(systemJobFile);
  this.localFs=FileSystem.getLocal(conf);
  this.localJobDir=localFs.makeQualified(conf.getLocalPath(jobDir));
  this.localJobFile=new Path(this.localJobDir,id + ""String_Node_Str"");
  localDistributedCacheManager=new LocalDistributedCacheManager();
  localDistributedCacheManager.setup(conf);
  OutputStream out=localFs.create(localJobFile);
  try {
    conf.writeXml(out);
  }
  finally {
    out.close();
  }
  this.job=new JobConf(localJobFile);
  if (localDistributedCacheManager.hasLocalClasspaths()) {
    ClassLoader classLoader=localDistributedCacheManager.makeClassLoader(getContextClassLoader());
    setContextClassLoader(classLoader);
    this.job.setClassLoader(classLoader);
  }
  profile=new JobProfile(job.getUser(),id,systemJobFile.toString(),""String_Node_Str"",job.getJobName());
  status=new JobStatus(id,0.0f,0.0f,JobStatus.RUNNING,profile.getUser(),profile.getJobName(),profile.getJobFile(),profile.getURL().toString());
  jobs.put(id,this);
  this.start();
}","public Job(JobID jobid,String jobSubmitDir) throws IOException {
  this.systemJobDir=new Path(jobSubmitDir);
  this.systemJobFile=new Path(systemJobDir,""String_Node_Str"");
  this.id=jobid;
  JobConf conf=new JobConf(systemJobFile);
  this.localFs=FileSystem.getLocal(conf);
  this.localJobDir=localFs.makeQualified(conf.getLocalPath(jobDir));
  this.localJobFile=new Path(this.localJobDir,id + ""String_Node_Str"");
  DistributedCache.addFileToClassPath(new Path(conf.getJar()),conf,FileSystem.get(conf));
  ClientDistributedCacheManager.determineTimestampsAndCacheVisibilities(conf);
  localDistributedCacheManager=new LocalDistributedCacheManager();
  localDistributedCacheManager.setup(conf);
  OutputStream out=localFs.create(localJobFile);
  try {
    conf.writeXml(out);
  }
  finally {
    out.close();
  }
  this.job=new JobConf(localJobFile);
  if (localDistributedCacheManager.hasLocalClasspaths()) {
    ClassLoader classLoader=localDistributedCacheManager.makeClassLoader(getContextClassLoader());
    setContextClassLoader(classLoader);
    this.job.setClassLoader(classLoader);
  }
  profile=new JobProfile(job.getUser(),id,systemJobFile.toString(),""String_Node_Str"",job.getJobName());
  status=new JobStatus(id,0.0f,0.0f,JobStatus.RUNNING,profile.getUser(),profile.getJobName(),profile.getJobFile(),profile.getURL().toString());
  jobs.put(id,this);
  this.start();
}"
7923,"@Override public Scanner scan(ReadPointer readPointer){
  return new MemoryScanner(this.map.entrySet().iterator(),readPointer);
}","@Override public Scanner scan(byte[] startRow,byte[] stopRow,byte[][] columns,ReadPointer readPointer){
  ConcurrentNavigableMap<RowLockTable.Row,NavigableMap<Column,NavigableMap<Version,Value>>> submap;
  if (startRow != null) {
    if (stopRow != null) {
      submap=this.map.subMap(new RowLockTable.Row(startRow),new RowLockTable.Row(stopRow));
    }
 else {
      submap=this.map.tailMap(new RowLockTable.Row(startRow));
    }
  }
 else {
    if (stopRow != null) {
      submap=this.map.headMap(new RowLockTable.Row(stopRow));
    }
 else {
      submap=this.map;
    }
  }
  return new MemoryScanner(submap.entrySet().iterator(),columns,readPointer);
}"
7924,"@BeforeClass public static void configure() throws Exception {
  DataSet kv=new Table(""String_Node_Str"");
  DataSet t1=new Table(""String_Node_Str"");
  DataSet t2=new Table(""String_Node_Str"");
  DataSet t3=new Table(""String_Node_Str"");
  DataSet t4=new Table(""String_Node_Str"");
  setupInstantiator(Lists.newArrayList(kv,t1,t2,t3,t4));
  table=instantiator.getDataSet(""String_Node_Str"");
}","@BeforeClass public static void configure() throws Exception {
  DataSet kv=new Table(""String_Node_Str"");
  DataSet t1=new Table(""String_Node_Str"");
  DataSet t2=new Table(""String_Node_Str"");
  DataSet t3=new Table(""String_Node_Str"");
  DataSet t4=new Table(""String_Node_Str"");
  DataSet tBatch=new Table(""String_Node_Str"");
  setupInstantiator(Lists.newArrayList(kv,t1,t2,t3,t4,tBatch));
  table=instantiator.getDataSet(""String_Node_Str"");
}"
7925,"/** 
 * Sets up the in-memory operation executor and the data fabric
 */
@BeforeClass public static void setupDataFabric(){
  final Injector injector=Guice.createInjector(new DataFabricLocalModule(""String_Node_Str"",null));
  opex=injector.getInstance(OperationExecutor.class);
  fabric=new DataFabricImpl(opex,OperationUtil.DEFAULT);
}","/** 
 * Sets up the in-memory operation executor and the data fabric
 */
@BeforeClass public static void setupDataFabric(){
  final Injector injector=Guice.createInjector(new DataFabricModules().getInMemoryModules());
  opex=injector.getInstance(OperationExecutor.class);
  fabric=new DataFabricImpl(opex,OperationUtil.DEFAULT);
}"
7926,"/** 
 * Add 1 to the number of failed operations
 */
protected void failedOne(){
  failed.incrementAndGet();
}","/** 
 * Add 1 to the number of failed operations.
 */
protected void failedOne(){
  failed.incrementAndGet();
}"
7927,"/** 
 * Add a delta to the number of succeeded operations
 * @param count how many operations succeeded
 */
protected void succeededSome(int count){
  succeeded.addAndGet(count);
}","/** 
 * Add a delta to the number of succeeded operations.
 * @param count how many operations succeeded
 */
protected void succeededSome(int count){
  succeeded.addAndGet(count);
}"
7928,"/** 
 * Add 1 to the number of succeeded operations
 */
protected void succeededOne(){
  succeeded.incrementAndGet();
}","/** 
 * Add 1 to the number of succeeded operations.
 */
protected void succeededOne(){
  succeeded.incrementAndGet();
}"
7929,"/** 
 * Add a delta to the number of failed operations
 * @param count how many operations failed
 */
protected void failedSome(int count){
  failed.addAndGet(count);
}","/** 
 * Add a delta to the number of failed operations.
 * @param count how many operations failed
 */
protected void failedSome(int count){
  failed.addAndGet(count);
}"
7930,"/** 
 * Start a client-side transaction
 * @return the new transaction
 */
public Transaction startTransaction(OperationContext context) throws OperationException ;","/** 
 * Start a client-side transaction.
 * @return the new transaction
 */
public Transaction startTransaction(OperationContext context) throws OperationException ;"
7931,"/** 
 * Abort an existing transaction
 * @param context the operation context
 * @param transaction the transaction to be committed
 * @throws OperationException if the abort fails for any reason
 */
public void abort(OperationContext context,Transaction transaction) throws OperationException ;","/** 
 * Abort an existing transaction.
 * @param context the operation context
 * @param transaction the transaction to be committed
 * @throws OperationException if the abort fails for any reason
 */
public void abort(OperationContext context,Transaction transaction) throws OperationException ;"
7932,"/** 
 * Set the limit for the number of deferred operations May not be called after the transaction has started
 * @param countLimit the new limit
 */
public void setCountLimit(int countLimit){
  if (this.state != State.New) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  this.countLimit=countLimit;
}","/** 
 * Set the limit for the number of deferred operations. May not be called after the transaction has started.
 * @param countLimit the new limit
 */
public void setCountLimit(int countLimit){
  if (this.state != State.New) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  this.countLimit=countLimit;
}"
7933,"/** 
 * return the number of operations that failed in this transaction
 * @return the number of operations
 */
public int getFailedCount();","/** 
 * return the number of operations that failed in this transaction.
 * @return the number of operations
 */
public int getFailedCount();"
7934,"/** 
 * return the number of operations performed successfully in this transaction
 * @return the number of operations
 */
public int getSucceededCount();","/** 
 * return the number of operations performed successfully in this transaction.
 * @return the number of operations
 */
public int getSucceededCount();"
7935,"/** 
 * Returns the write version
 * @return write version
 */
long getWriteVersion();","/** 
 * Returns the write version.
 * @return write version
 */
long getWriteVersion();"
7936,"/** 
 * Get an instance of the named data set
 * @param name the name of the data set
 * @param < T > the type of the data set
 * @return a new instance of the named data set
 * @throws DataSetInstantiationException if for any reason, the data setcannot be instantiated, for instance, its class cannot be loaded, its class is missing the runtime constructor (@see Dataset#DataSet(DataSetSpecification)), or the constructor throws an exception. Also if a data set cannot be opened, for instance, if we fail to access one of the underlying Tables in the data fabric.
 */
public <T extends DataSet>T getDataSet(String name) throws DataSetInstantiationException ;","/** 
 * Get an instance of the named data set.
 * @param name the name of the data set
 * @param < T > the type of the data set
 * @return a new instance of the named data set
 * @throws DataSetInstantiationException if for any reason, the data setcannot be instantiated, for instance, its class cannot be loaded, its class is missing the runtime constructor (@see Dataset#DataSet(DataSetSpecification)), or the constructor throws an exception. Also if a data set cannot be opened, for instance, if we fail to access one of the underlying Tables in the data fabric.
 */
public <T extends DataSet>T getDataSet(String name) throws DataSetInstantiationException ;"
7937,"/** 
 * whether this is a read-only instantiation
 */
public boolean isReadOnly(){
  return readOnly;
}","/** 
 * Whether this is a read-only instantiation.
 */
public boolean isReadOnly(){
  return readOnly;
}"
7938,"/** 
 * Find out whether the instantiator has a spec for a named data set
 * @param name the name of the data set
 * @return whether the instantiator knows the spec for the data set
 */
public boolean hasDataSet(String name){
  return this.datasets.containsKey(name);
}","/** 
 * Find out whether the instantiator has a spec for a named data set.
 * @param name the name of the data set
 * @return whether the instantiator knows the spec for the data set
 */
public boolean hasDataSet(String name){
  return this.datasets.containsKey(name);
}"
7939,"/** 
 * helper method to cast the created data set object to its correct class. This method is to isolate the unchecked cast (it has to be unchecked because T is a type parameter, we cannot do instanceof or isAssignableFrom on type parameters...) into a small method, that we can annotate with a SuppressWarnings of small scope.
 * @param o The object to be cast
 * @param className the name of the class of that object, for error messages
 * @param < T > The type to cast to
 * @return The cast object of type T
 * @throws DataSetInstantiationException if the cast fails.
 */
@SuppressWarnings(""String_Node_Str"") private <T extends DataSet>T convert(Object o,String className) throws DataSetInstantiationException {
  try {
    return (T)o;
  }
 catch (  ClassCastException e) {
    throw logAndException(e,""String_Node_Str"",className);
  }
}","/** 
 * Helper method to cast the created data set object to its correct class. This method is to isolate the unchecked cast (it has to be unchecked because T is a type parameter, we cannot do instanceof or isAssignableFrom on type parameters...) into a small method, that we can annotate with a SuppressWarnings of small scope.
 * @param o The object to be cast
 * @param className the name of the class of that object, for error messages
 * @param < T > The type to cast to
 * @return The cast object of type T
 * @throws DataSetInstantiationException if the cast fails.
 */
@SuppressWarnings(""String_Node_Str"") private <T extends DataSet>T convert(Object o,String className) throws DataSetInstantiationException {
  try {
    return (T)o;
  }
 catch (  ClassCastException e) {
    throw logAndException(e,""String_Node_Str"",className);
  }
}"
7940,"/** 
 * a constructor from data fabric and transaction proxy
 * @param fabric the data fabric
 * @param transactionProxy the transaction proxy to use for all data sets
 * @param classLoader the class loader to use for loading data set classes.If null, then the default class loader is used
 */
public DataSetInstantiator(DataFabric fabric,TransactionProxy transactionProxy,ClassLoader classLoader){
  super(classLoader);
  this.fabric=fabric;
  this.transactionProxy=transactionProxy;
}","/** 
 * Constructor from data fabric and transaction proxy.
 * @param fabric the data fabric
 * @param transactionProxy the transaction proxy to use for all data sets
 * @param classLoader the class loader to use for loading data set classes.If null, then the default class loader is used
 */
public DataSetInstantiator(DataFabric fabric,TransactionProxy transactionProxy,ClassLoader classLoader){
  super(classLoader);
  this.fabric=fabric;
  this.transactionProxy=transactionProxy;
}"
7941,"/** 
 * package-protected constructor, only to be called from @see #setReadOnlyTable() and @see ReadWriteTable constructor
 * @param table the original table
 * @param proxy transaction proxy for all operations
 * @param fabric the data fabric
 */
ReadOnlyTable(Table table,DataFabric fabric,TransactionProxy proxy){
  super(table,fabric,proxy);
}","/** 
 * Package-protected constructor, only to be called from @see #setReadOnlyTable() and @see ReadWriteTable constructor.
 * @param table the original table
 * @param proxy transaction proxy for all operations
 * @param fabric the data fabric
 */
ReadOnlyTable(Table table,DataFabric fabric,TransactionProxy proxy){
  super(table,fabric,proxy);
}"
7942,"/** 
 * package-protected constructor, only to be called from @see #setReadOnlyTable() and @see ReadWriteTable constructor
 * @param table the original table
 * @param fabric the data fabric
 */
RuntimeTable(Table table,DataFabric fabric,TransactionProxy proxy){
  super(table.getName());
  this.dataFabric=fabric;
  this.proxy=proxy;
}","/** 
 * package-protected constructor, only to be called from @see #setReadOnlyTable() and @see ReadWriteTable constructor.
 * @param table the original table
 * @param fabric the data fabric
 */
RuntimeTable(Table table,DataFabric fabric,TransactionProxy proxy){
  super(table.getName());
  this.dataFabric=fabric;
  this.proxy=proxy;
}"
7943,"/** 
 * set the name to use for metrics
 * @param metricName the name to use for emitting metrics
 */
protected void setMetricName(String metricName){
  this.metricName=metricName;
}","/** 
 * Set the name to use for metrics.
 * @param metricName the name to use for emitting metrics
 */
protected void setMetricName(String metricName){
  this.metricName=metricName;
}"
7944,"/** 
 * Helper to convert an increment operation
 * @param increment the table increment
 * @return a corresponding data fabric increment operation
 */
private com.continuuity.data.operation.Increment toOperation(Increment increment){
  com.continuuity.data.operation.Increment operation=new com.continuuity.data.operation.Increment(this.tableName(),increment.getRow(),increment.getColumns(),increment.getValues());
  operation.setMetricName(getMetricName());
  return operation;
}","/** 
 * Helper to convert an increment operation.
 * @param increment the table increment
 * @return a corresponding data fabric increment operation
 */
private com.continuuity.data.operation.Increment toOperation(Increment increment){
  com.continuuity.data.operation.Increment operation=new com.continuuity.data.operation.Increment(this.tableName(),increment.getRow(),increment.getColumns(),increment.getValues());
  operation.setMetricName(getMetricName());
  return operation;
}"
7945,"/** 
 * open the table in the data fabric, to ensure it exists and is accessible.
 * @throws com.continuuity.api.data.OperationException if something goes wrong
 */
public void open() throws OperationException {
  this.dataFabric.openTable(this.getName());
}","/** 
 * Open the table in the data fabric, to ensure it exists and is accessible.
 * @throws OperationException if something goes wrong
 */
public void open() throws OperationException {
  this.dataFabric.openTable(this.getName());
}"
7946,"/** 
 * Get Application Id
 * @return application id
 */
public String getApplication(){
  return application;
}","/** 
 * Get Application Id.
 * @return application id
 */
public String getApplication(){
  return application;
}"
7947,"/** 
 * Returns all binary key fields
 * @return Set of keys
 */
public Set<String> getBinaryFields(){
  return this.binaryFields.keySet();
}","/** 
 * Returns the keys of all binary fields.
 * @return Set of keys
 */
public Set<String> getBinaryFields(){
  return this.binaryFields.keySet();
}"
7948,"/** 
 * Get Metadata type
 * @return metadata type
 */
public String getType(){
  return this.type;
}","/** 
 * Get Metadata type.
 * @return metadata type
 */
public String getType(){
  return this.type;
}"
7949,"/** 
 * Get Account id
 * @return id
 */
public String getAccount(){
  return account;
}","/** 
 * Get Account id.
 * @return id
 */
public String getAccount(){
  return account;
}"
7950,"/** 
 * Returns Field value as a String
 * @param field field key
 * @return value
 */
public String getTextField(String field){
  if (field == null)   throw new IllegalArgumentException(""String_Node_Str"");
  return this.textFields.get(field);
}","/** 
 * Returns Field value as a String.
 * @param field field key
 * @return value
 */
public String getTextField(String field){
  if (field == null) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  return this.textFields.get(field);
}"
7951,"/** 
 * Comparison function
 * @param o  Object to be compared
 * @return boolean true if the objects passed is equal, false otherwise
 */
public boolean equals(Object o){
  if (this == o)   return true;
  if (!(o instanceof MetaDataEntry))   return false;
  MetaDataEntry other=(MetaDataEntry)o;
  if (!this.account.equals(other.account))   return false;
  if (this.application == null && other.application != null)   return false;
  if (this.application != null && !this.application.equals(other.application))   return false;
  if (!this.id.equals(other.id))   return false;
  if (!this.type.equals(other.type))   return false;
  if (!this.textFields.equals(other.textFields))   return false;
  if (!this.getBinaryFields().equals(other.getBinaryFields()))   return false;
  for (  String key : this.getBinaryFields())   if (!Arrays.equals(this.getBinaryField(key),other.getBinaryField(key)))   return false;
  return true;
}","/** 
 * Comparison function.
 * @param o Object to be compared
 * @return boolean true if the objects passed is equal, false otherwise
 */
public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (!(o instanceof MetaDataEntry)) {
    return false;
  }
  MetaDataEntry other=(MetaDataEntry)o;
  if (!this.account.equals(other.account)) {
    return false;
  }
  if (this.application == null && other.application != null) {
    return false;
  }
  if (this.application != null && !this.application.equals(other.application)) {
    return false;
  }
  if (!this.id.equals(other.id)) {
    return false;
  }
  if (!this.type.equals(other.type)) {
    return false;
  }
  if (!this.textFields.equals(other.textFields)) {
    return false;
  }
  if (!this.getBinaryFields().equals(other.getBinaryFields())) {
    return false;
  }
  for (  String key : this.getBinaryFields()) {
    if (!Arrays.equals(this.getBinaryField(key),other.getBinaryField(key))) {
      return false;
    }
  }
  return true;
}"
7952,"/** 
 * Adds a field to the metadata
 * @param field field name
 * @param value field value
 */
public void addField(String field,byte[] value){
  if (field == null)   throw new IllegalArgumentException(""String_Node_Str"");
  if (field.isEmpty())   throw new IllegalArgumentException(""String_Node_Str"");
  if (value == null)   throw new IllegalArgumentException(""String_Node_Str"");
  this.binaryFields.put(field,value);
}","/** 
 * Adds a binary field to the metadata.
 * @param field field name
 * @param value field value
 */
public void addField(String field,byte[] value){
  if (field == null) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (field.isEmpty()) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (value == null) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  this.binaryFields.put(field,value);
}"
7953,"/** 
 * Get Metadata id
 * @return metadata id
 */
public String getId(){
  return this.id;
}","/** 
 * Get Metadata id.
 * @return metadata id
 */
public String getId(){
  return this.id;
}"
7954,"/** 
 * Returns binary field value
 * @param field field key
 * @return value
 */
public byte[] getBinaryField(String field){
  if (field == null)   throw new IllegalArgumentException(""String_Node_Str"");
  return this.binaryFields.get(field);
}","/** 
 * Returns binary field value.
 * @param field field key
 * @return value
 */
public byte[] getBinaryField(String field){
  if (field == null) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  return this.binaryFields.get(field);
}"
7955,"/** 
 * Returns all text key fields
 * @return Set of keys
 */
public Set<String> getTextFields(){
  return this.textFields.keySet();
}","/** 
 * Returns the keys of all text fields.
 * @return Set of keys
 */
public Set<String> getTextFields(){
  return this.textFields.keySet();
}"
7956,"/** 
 * Constructor
 * @param account account id related to the meta data entry
 * @param application application id related to the meta data entry
 * @param type Type Meta data type
 * @param id Meta data id
 */
public MetaDataEntry(String account,String application,String type,String id){
  if (account == null)   throw new IllegalArgumentException(""String_Node_Str"");
  if (account.isEmpty())   throw new IllegalArgumentException(""String_Node_Str"");
  if (id == null)   throw new IllegalArgumentException(""String_Node_Str"");
  if (id.isEmpty())   throw new IllegalArgumentException(""String_Node_Str"");
  if (application != null && application.isEmpty())   throw new IllegalArgumentException(""String_Node_Str"");
  if (type == null)   throw new IllegalArgumentException(""String_Node_Str"");
  if (type.isEmpty())   throw new IllegalArgumentException(""String_Node_Str"");
  this.account=account;
  this.application=application;
  this.id=id;
  this.type=type;
  this.textFields=Maps.newTreeMap();
  this.binaryFields=Maps.newTreeMap();
}","/** 
 * @param account     account id related to the meta data entry
 * @param application application id related to the meta data entry
 * @param type        Type Meta data type
 * @param id          Meta data id
 */
public MetaDataEntry(String account,String application,String type,String id){
  if (account == null) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (account.isEmpty()) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (id == null) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (id.isEmpty()) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (application != null && application.isEmpty()) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (type == null) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (type.isEmpty()) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  this.account=account;
  this.application=application;
  this.id=id;
  this.type=type;
  this.textFields=Maps.newTreeMap();
  this.binaryFields=Maps.newTreeMap();
}"
7957,"/** 
 * Serialize a meta data entry
 * @param meta the meta data to be serialized
 * @return the serialized meta data as a byte array
 * @throws MetaDataException if serialization fails
 */
public byte[] serialize(MetaDataEntry meta) throws MetaDataException {
  ByteArrayOutputStream outStream=new ByteArrayOutputStream();
  output.setOutputStream(outStream);
  try {
    kryo.writeObject(output,meta);
    output.flush();
    return outStream.toByteArray();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw new MetaDataException(""String_Node_Str"",e);
  }
}","/** 
 * Serialize a meta data entry.
 * @param meta the meta data to be serialized
 * @return the serialized meta data as a byte array
 * @throws MetaDataException if serialization fails
 */
public byte[] serialize(MetaDataEntry meta) throws MetaDataException {
  ByteArrayOutputStream outStream=new ByteArrayOutputStream();
  output.setOutputStream(outStream);
  try {
    kryo.writeObject(output,meta);
    output.flush();
    return outStream.toByteArray();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw new MetaDataException(""String_Node_Str"",e);
  }
}"
7958,"/** 
 * Deserialize an meta data entry
 * @param bytes the serialized representation of the meta data
 * @return the deserialized meta data
 * @throws MetaDataException if deserialization fails
 */
public MetaDataEntry deserialize(byte[] bytes) throws MetaDataException {
  ByteArrayInputStream inputStream=new ByteArrayInputStream(bytes);
  input.setInputStream(inputStream);
  try {
    return kryo.readObject(input,MetaData.class);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw new MetaDataException(""String_Node_Str"",e);
  }
}","/** 
 * Deserialize an meta data entry.
 * @param bytes the serialized representation of the meta data
 * @return the deserialized meta data
 * @throws MetaDataException if deserialization fails
 */
public MetaDataEntry deserialize(byte[] bytes) throws MetaDataException {
  ByteArrayInputStream inputStream=new ByteArrayInputStream(bytes);
  input.setInputStream(inputStream);
  try {
    return kryo.readObject(input,MetaData.class);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw new MetaDataException(""String_Node_Str"",e);
  }
}"
7959,"private byte[][] generateDeleteVals(int length){
  byte[][] values=new byte[length][];
  for (int i=0; i < values.length; i++)   values[i]=NULL_VAL;
  return values;
}","private byte[][] generateDeleteVals(int length){
  byte[][] values=new byte[length][];
  for (int i=0; i < values.length; i++) {
    values[i]=NULL_VAL;
  }
  return values;
}"
7960,"@Override public synchronized long incrementAtomicDirtily(byte[] row,byte[] column,long amount) throws OperationException {
  PreparedStatement ps=null;
  try {
    this.connection.setAutoCommit(false);
    ps=this.connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    ps.setBytes(1,row);
    ps.setBytes(2,column);
    ResultSet result=ps.executeQuery();
    ps.close();
    ImmutablePair<Long,byte[]> latest=latest(result);
    long newAmount=amount;
    if (latest == null) {
      ps=this.connection.prepareStatement(""String_Node_Str"" + this.quotedTableName + ""String_Node_Str"");
      ps.setBytes(1,row);
      ps.setBytes(2,column);
      ps.setLong(3,TransactionOracle.DIRTY_WRITE_VERSION);
      ps.setInt(4,Type.VALUE.i);
      ps.setBytes(5,Bytes.toBytes(newAmount));
      ps.executeUpdate();
    }
 else {
      ps=this.connection.prepareStatement(""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str"");
      newAmount=Bytes.toLong(latest.getSecond()) + amount;
      ps.setBytes(1,Bytes.toBytes(newAmount));
      ps.setBytes(2,row);
      ps.setBytes(3,column);
      ps.setLong(4,TransactionOracle.DIRTY_WRITE_VERSION);
      ps.executeUpdate();
    }
    this.connection.commit();
    return newAmount;
  }
 catch (  SQLException e) {
    throw createOperationException(e,""String_Node_Str"");
  }
 finally {
    try {
      this.connection.setAutoCommit(true);
      if (ps != null) {
        ps.close();
      }
    }
 catch (    SQLException e) {
      throw createOperationException(e,""String_Node_Str"");
    }
  }
}","@Override public synchronized long incrementAtomicDirtily(byte[] row,byte[] column,long amount) throws OperationException {
  PreparedStatement ps=null;
  try {
    this.connection.setAutoCommit(false);
    ps=this.connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    ps.setBytes(1,row);
    ps.setBytes(2,column);
    ResultSet result=ps.executeQuery();
    ps.close();
    ImmutablePair<Long,byte[]> latest=latest(result);
    long newAmount=amount;
    if (latest == null) {
      ps=this.connection.prepareStatement(""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str"");
      ps.setBytes(1,row);
      ps.setBytes(2,column);
      ps.setLong(3,TransactionOracle.DIRTY_WRITE_VERSION);
      ps.setInt(4,Type.VALUE.i);
      ps.setBytes(5,Bytes.toBytes(newAmount));
      ps.executeUpdate();
    }
 else {
      ps=this.connection.prepareStatement(""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str"");
      newAmount=Bytes.toLong(latest.getSecond()) + amount;
      ps.setBytes(1,Bytes.toBytes(newAmount));
      ps.setBytes(2,row);
      ps.setBytes(3,column);
      ps.setLong(4,TransactionOracle.DIRTY_WRITE_VERSION);
      ps.executeUpdate();
    }
    this.connection.commit();
    return newAmount;
  }
 catch (  SQLException e) {
    throw createOperationException(e,""String_Node_Str"");
  }
 finally {
    try {
      this.connection.setAutoCommit(true);
      if (ps != null) {
        ps.close();
      }
    }
 catch (    SQLException e) {
      throw createOperationException(e,""String_Node_Str"");
    }
  }
}"
7961,"@Override public OperationResult<ImmutablePair<byte[],Long>> getWithVersion(byte[] row,byte[] column,ReadPointer readPointer) throws OperationException {
  PreparedStatement ps=null;
  try {
    ps=this.connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    ps.setBytes(1,row);
    ps.setBytes(2,column);
    ResultSet result=ps.executeQuery();
    ImmutablePair<Long,byte[]> latest=filteredLatest(result,readPointer);
    if (latest == null)     return new OperationResult<ImmutablePair<byte[],Long>>(StatusCode.KEY_NOT_FOUND);
    return new OperationResult<ImmutablePair<byte[],Long>>(new ImmutablePair<byte[],Long>(latest.getSecond(),latest.getFirst()));
  }
 catch (  SQLException e) {
    throw createOperationException(e,""String_Node_Str"",ps);
  }
 finally {
    if (ps != null) {
      try {
        ps.close();
      }
 catch (      SQLException e) {
        throw createOperationException(e,""String_Node_Str"");
      }
    }
  }
}","@Override public OperationResult<ImmutablePair<byte[],Long>> getWithVersion(byte[] row,byte[] column,ReadPointer readPointer) throws OperationException {
  PreparedStatement ps=null;
  try {
    ps=this.connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    ps.setBytes(1,row);
    ps.setBytes(2,column);
    ResultSet result=ps.executeQuery();
    ImmutablePair<Long,byte[]> latest=filteredLatest(result,readPointer);
    if (latest == null) {
      return new OperationResult<ImmutablePair<byte[],Long>>(StatusCode.KEY_NOT_FOUND);
    }
    return new OperationResult<ImmutablePair<byte[],Long>>(new ImmutablePair<byte[],Long>(latest.getSecond(),latest.getFirst()));
  }
 catch (  SQLException e) {
    throw createOperationException(e,""String_Node_Str"",ps);
  }
 finally {
    if (ps != null) {
      try {
        ps.close();
      }
 catch (      SQLException e) {
        throw createOperationException(e,""String_Node_Str"");
      }
    }
  }
}"
7962,"@Override public void compareAndSwap(byte[] row,byte[] column,byte[] expectedValue,byte[] newValue,ReadPointer readPointer,long writeVersion) throws OperationException {
  PreparedStatement ps=null;
  try {
    ps=this.connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    ps.setBytes(1,row);
    ps.setBytes(2,column);
    ResultSet result=ps.executeQuery();
    ImmutablePair<Long,byte[]> latest=filteredLatest(result,readPointer);
    byte[] existingValue=latest == null ? null : latest.getSecond();
    ps.close();
    ps=null;
    if (existingValue == null && expectedValue != null)     throw new OperationException(StatusCode.WRITE_CONFLICT,""String_Node_Str"");
    if (existingValue != null && expectedValue == null)     throw new OperationException(StatusCode.WRITE_CONFLICT,""String_Node_Str"");
    if (expectedValue == null) {
      put(row,column,writeVersion,newValue);
      return;
    }
    if (!Bytes.equals(expectedValue,existingValue))     throw new OperationException(StatusCode.WRITE_CONFLICT,""String_Node_Str"");
    if (newValue == null) {
      deleteAll(row,column,latest.getFirst());
      return;
    }
    ps=this.connection.prepareStatement(""String_Node_Str"" + this.quotedTableName + ""String_Node_Str"");
    ps.setBytes(1,row);
    ps.setBytes(2,column);
    ps.setLong(3,writeVersion);
    ps.setInt(4,Type.VALUE.i);
    ps.setBytes(5,newValue);
    ps.executeUpdate();
  }
 catch (  SQLException e) {
    throw createOperationException(e,""String_Node_Str"");
  }
 finally {
    if (ps != null) {
      try {
        ps.close();
      }
 catch (      SQLException e) {
        throw createOperationException(e,""String_Node_Str"");
      }
    }
  }
}","@Override public void compareAndSwap(byte[] row,byte[] column,byte[] expectedValue,byte[] newValue,ReadPointer readPointer,long writeVersion) throws OperationException {
  PreparedStatement ps=null;
  try {
    ps=this.connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    ps.setBytes(1,row);
    ps.setBytes(2,column);
    ResultSet result=ps.executeQuery();
    ImmutablePair<Long,byte[]> latest=filteredLatest(result,readPointer);
    byte[] existingValue=latest == null ? null : latest.getSecond();
    ps.close();
    ps=null;
    if (existingValue == null && expectedValue != null) {
      throw new OperationException(StatusCode.WRITE_CONFLICT,""String_Node_Str"");
    }
    if (existingValue != null && expectedValue == null) {
      throw new OperationException(StatusCode.WRITE_CONFLICT,""String_Node_Str"");
    }
    if (expectedValue == null) {
      put(row,column,writeVersion,newValue);
      return;
    }
    if (!Bytes.equals(expectedValue,existingValue)) {
      throw new OperationException(StatusCode.WRITE_CONFLICT,""String_Node_Str"");
    }
    if (newValue == null) {
      deleteAll(row,column,latest.getFirst());
      return;
    }
    ps=this.connection.prepareStatement(""String_Node_Str"" + this.quotedTableName + ""String_Node_Str"");
    ps.setBytes(1,row);
    ps.setBytes(2,column);
    ps.setLong(3,writeVersion);
    ps.setInt(4,Type.VALUE.i);
    ps.setBytes(5,newValue);
    ps.executeUpdate();
  }
 catch (  SQLException e) {
    throw createOperationException(e,""String_Node_Str"");
  }
 finally {
    if (ps != null) {
      try {
        ps.close();
      }
 catch (      SQLException e) {
        throw createOperationException(e,""String_Node_Str"");
      }
    }
  }
}"
7963,"/** 
 * Result has (row, column, version, kvtype, id, value)
 * @throws SQLException
 */
private Map<byte[],Map<byte[],byte[]>> filteredLatestColumnsWithKey(ResultSet result,ReadPointer readPointer,int limit) throws SQLException {
  if (limit <= 0) {
    limit=Integer.MAX_VALUE;
  }
  Map<byte[],Map<byte[],byte[]>> map=new TreeMap<byte[],Map<byte[],byte[]>>(Bytes.BYTES_COMPARATOR);
  if (result == null) {
    return map;
  }
  boolean newRow=true;
  byte[] lastRow=new byte[0];
  byte[] curCol=new byte[0];
  byte[] lastCol=new byte[0];
  long lastDelete=-1;
  long undeleted=-1;
  while (result.next()) {
    byte[] rowKey=result.getBytes(1);
    if (!Bytes.equals(lastRow,rowKey)) {
      newRow=true;
      lastRow=rowKey;
    }
    long curVersion=result.getLong(3);
    if (!readPointer.isVisible(curVersion)) {
      continue;
    }
    byte[] column=result.getBytes(2);
    if (!newRow && Bytes.equals(lastCol,column)) {
      continue;
    }
    if (newRow || !Bytes.equals(curCol,column)) {
      curCol=column;
      lastDelete=-1;
      undeleted=-1;
    }
    Type type=Type.from(result.getInt(4));
    if (type.isUndeleteAll()) {
      undeleted=curVersion;
      continue;
    }
    if (type.isDeleteAll()) {
      if (undeleted == curVersion) {
        continue;
      }
 else {
        lastCol=column;
        continue;
      }
    }
    if (type.isDelete()) {
      lastDelete=curVersion;
      continue;
    }
    if (curVersion == lastDelete) {
      continue;
    }
    lastCol=column;
    Map<byte[],byte[]> colMap=map.get(rowKey);
    if (colMap == null) {
      colMap=new TreeMap<byte[],byte[]>(Bytes.BYTES_COMPARATOR);
      map.put(rowKey,colMap);
    }
    colMap.put(column,result.getBytes(6));
    if (map.size() >= limit)     break;
  }
  return map;
}","/** 
 * Result has (row, column, version, kvtype, id, value).
 * @throws SQLException
 */
private Map<byte[],Map<byte[],byte[]>> filteredLatestColumnsWithKey(ResultSet result,ReadPointer readPointer,int limit) throws SQLException {
  if (limit <= 0) {
    limit=Integer.MAX_VALUE;
  }
  Map<byte[],Map<byte[],byte[]>> map=new TreeMap<byte[],Map<byte[],byte[]>>(Bytes.BYTES_COMPARATOR);
  if (result == null) {
    return map;
  }
  boolean newRow=true;
  byte[] lastRow=new byte[0];
  byte[] curCol=new byte[0];
  byte[] lastCol=new byte[0];
  long lastDelete=-1;
  long undeleted=-1;
  while (result.next()) {
    byte[] rowKey=result.getBytes(1);
    if (!Bytes.equals(lastRow,rowKey)) {
      newRow=true;
      lastRow=rowKey;
    }
    long curVersion=result.getLong(3);
    if (!readPointer.isVisible(curVersion)) {
      continue;
    }
    byte[] column=result.getBytes(2);
    if (!newRow && Bytes.equals(lastCol,column)) {
      continue;
    }
    if (newRow || !Bytes.equals(curCol,column)) {
      curCol=column;
      lastDelete=-1;
      undeleted=-1;
    }
    Type type=Type.from(result.getInt(4));
    if (type.isUndeleteAll()) {
      undeleted=curVersion;
      continue;
    }
    if (type.isDeleteAll()) {
      if (undeleted == curVersion) {
        continue;
      }
 else {
        lastCol=column;
        continue;
      }
    }
    if (type.isDelete()) {
      lastDelete=curVersion;
      continue;
    }
    if (curVersion == lastDelete) {
      continue;
    }
    lastCol=column;
    Map<byte[],byte[]> colMap=map.get(rowKey);
    if (colMap == null) {
      colMap=new TreeMap<byte[],byte[]>(Bytes.BYTES_COMPARATOR);
      map.put(rowKey,colMap);
    }
    colMap.put(column,result.getBytes(6));
    if (map.size() >= limit) {
      break;
    }
  }
  return map;
}"
7964,"/** 
 * Result has (column, version, kvtype, id, value)
 * @throws SQLException
 */
private Map<byte[],byte[]> filteredLatestColumns(ResultSet result,ReadPointer readPointer,int limit) throws SQLException {
  if (limit <= 0)   limit=Integer.MAX_VALUE;
  Map<byte[],byte[]> map=new TreeMap<byte[],byte[]>(Bytes.BYTES_COMPARATOR);
  if (result == null)   return map;
  byte[] curCol=new byte[0];
  byte[] lastCol=new byte[0];
  long lastDelete=-1;
  long undeleted=-1;
  while (result.next()) {
    long curVersion=result.getLong(2);
    if (!readPointer.isVisible(curVersion))     continue;
    byte[] column=result.getBytes(1);
    if (Bytes.equals(lastCol,column)) {
      continue;
    }
    if (!Bytes.equals(curCol,column)) {
      curCol=column;
      lastDelete=-1;
      undeleted=-1;
    }
    Type type=Type.from(result.getInt(3));
    if (type.isUndeleteAll()) {
      undeleted=curVersion;
      continue;
    }
    if (type.isDeleteAll()) {
      if (undeleted == curVersion)       continue;
 else {
        lastCol=column;
        continue;
      }
    }
    if (type.isDelete()) {
      lastDelete=curVersion;
      continue;
    }
    if (curVersion == lastDelete)     continue;
    lastCol=column;
    map.put(column,result.getBytes(5));
    if (map.size() >= limit)     break;
  }
  return map;
}","/** 
 * Result has (column, version, kvtype, id, value).
 * @throws SQLException
 */
private Map<byte[],byte[]> filteredLatestColumns(ResultSet result,ReadPointer readPointer,int limit) throws SQLException {
  if (limit <= 0) {
    limit=Integer.MAX_VALUE;
  }
  Map<byte[],byte[]> map=new TreeMap<byte[],byte[]>(Bytes.BYTES_COMPARATOR);
  if (result == null) {
    return map;
  }
  byte[] curCol=new byte[0];
  byte[] lastCol=new byte[0];
  long lastDelete=-1;
  long undeleted=-1;
  while (result.next()) {
    long curVersion=result.getLong(2);
    if (!readPointer.isVisible(curVersion)) {
      continue;
    }
    byte[] column=result.getBytes(1);
    if (Bytes.equals(lastCol,column)) {
      continue;
    }
    if (!Bytes.equals(curCol,column)) {
      curCol=column;
      lastDelete=-1;
      undeleted=-1;
    }
    Type type=Type.from(result.getInt(3));
    if (type.isUndeleteAll()) {
      undeleted=curVersion;
      continue;
    }
    if (type.isDeleteAll()) {
      if (undeleted == curVersion) {
        continue;
      }
 else {
        lastCol=column;
        continue;
      }
    }
    if (type.isDelete()) {
      lastDelete=curVersion;
      continue;
    }
    if (curVersion == lastDelete) {
      continue;
    }
    lastCol=column;
    map.put(column,result.getBytes(5));
    if (map.size() >= limit) {
      break;
    }
  }
  return map;
}"
7965,"/** 
 * Result has (version, kvtype, id, value)
 * @throws SQLException
 */
private ImmutablePair<Long,byte[]> filteredLatest(ResultSet result,ReadPointer readPointer) throws SQLException {
  if (result == null)   return null;
  long lastDelete=-1;
  long undeleted=-1;
  while (result.next()) {
    long curVersion=result.getLong(1);
    if (!readPointer.isVisible(curVersion))     continue;
    Type type=Type.from(result.getInt(2));
    if (type.isUndeleteAll()) {
      undeleted=curVersion;
      continue;
    }
    if (type.isDeleteAll()) {
      if (undeleted == curVersion)       continue;
 else       break;
    }
    if (type.isDelete()) {
      lastDelete=curVersion;
      continue;
    }
    if (curVersion == lastDelete)     continue;
    return new ImmutablePair<Long,byte[]>(curVersion,result.getBytes(4));
  }
  return null;
}","/** 
 * Result has (version, kvtype, id, value).
 * @throws SQLException
 */
private ImmutablePair<Long,byte[]> filteredLatest(ResultSet result,ReadPointer readPointer) throws SQLException {
  if (result == null) {
    return null;
  }
  long lastDelete=-1;
  long undeleted=-1;
  while (result.next()) {
    long curVersion=result.getLong(1);
    if (!readPointer.isVisible(curVersion)) {
      continue;
    }
    Type type=Type.from(result.getInt(2));
    if (type.isUndeleteAll()) {
      undeleted=curVersion;
      continue;
    }
    if (type.isDeleteAll()) {
      if (undeleted == curVersion) {
        continue;
      }
 else {
        break;
      }
    }
    if (type.isDelete()) {
      lastDelete=curVersion;
      continue;
    }
    if (curVersion == lastDelete) {
      continue;
    }
    return new ImmutablePair<Long,byte[]>(curVersion,result.getBytes(4));
  }
  return null;
}"
7966,"/** 
 * Result has (version, kvtype, id, value)
 * @throws SQLException
 */
private ImmutablePair<Long,byte[]> latest(ResultSet result) throws SQLException {
  if (result == null)   return null;
  long lastDelete=-1;
  long undeleted=-1;
  while (result.next()) {
    long curVersion=result.getLong(1);
    Type type=Type.from(result.getInt(2));
    if (type.isUndeleteAll()) {
      undeleted=curVersion;
      continue;
    }
    if (type.isDeleteAll()) {
      if (undeleted == curVersion)       continue;
 else       break;
    }
    if (type.isDelete()) {
      lastDelete=curVersion;
      continue;
    }
    if (curVersion == lastDelete)     continue;
    return new ImmutablePair<Long,byte[]>(curVersion,result.getBytes(4));
  }
  return null;
}","/** 
 * Result has (version, kvtype, id, value).
 * @throws SQLException
 */
private ImmutablePair<Long,byte[]> latest(ResultSet result) throws SQLException {
  if (result == null) {
    return null;
  }
  long lastDelete=-1;
  long undeleted=-1;
  while (result.next()) {
    long curVersion=result.getLong(1);
    Type type=Type.from(result.getInt(2));
    if (type.isUndeleteAll()) {
      undeleted=curVersion;
      continue;
    }
    if (type.isDeleteAll()) {
      if (undeleted == curVersion) {
        continue;
      }
 else {
        break;
      }
    }
    if (type.isDelete()) {
      lastDelete=curVersion;
      continue;
    }
    if (curVersion == lastDelete) {
      continue;
    }
    return new ImmutablePair<Long,byte[]>(curVersion,result.getBytes(4));
  }
  return null;
}"
7967,"@Override public List<byte[]> getKeys(int limit,int offset,ReadPointer readPointer) throws OperationException {
  PreparedStatement ps=null;
  try {
    ps=this.connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str"");
    ResultSet result=ps.executeQuery();
    List<byte[]> keys=new ArrayList<byte[]>(limit > 1024 ? 1024 : limit);
    int returned=0;
    int skipped=0;
    long lastDelete=-1;
    long undeleted=-1;
    byte[] lastRow=new byte[0];
    byte[] curRow=new byte[0];
    byte[] curCol=new byte[0];
    byte[] lastCol=new byte[0];
    while (result.next() && returned < limit) {
      byte[] row=result.getBytes(1);
      if (Bytes.equals(lastRow,row))       continue;
      if (!Bytes.equals(curRow,row)) {
        lastCol=new byte[0];
        curCol=new byte[0];
        lastDelete=-1;
        undeleted=-1;
      }
      curRow=row;
      long curVersion=result.getLong(3);
      if (!readPointer.isVisible(curVersion))       continue;
      byte[] column=result.getBytes(2);
      if (Bytes.equals(lastCol,column)) {
        continue;
      }
      if (!Bytes.equals(curCol,column)) {
        curCol=column;
        lastDelete=-1;
        undeleted=-1;
      }
      Type type=Type.from(result.getInt(4));
      if (type.isUndeleteAll()) {
        undeleted=curVersion;
        continue;
      }
      if (type.isDeleteAll()) {
        if (undeleted == curVersion)         continue;
 else {
          lastCol=column;
          continue;
        }
      }
      if (type.isDelete()) {
        lastDelete=curVersion;
        continue;
      }
      if (curVersion == lastDelete)       continue;
      lastRow=row;
      if (skipped < offset) {
        skipped++;
      }
 else {
        keys.add(row);
        returned++;
      }
    }
    return keys;
  }
 catch (  SQLException e) {
    throw createOperationException(e,""String_Node_Str"");
  }
 finally {
    if (ps != null) {
      try {
        ps.close();
      }
 catch (      SQLException e) {
        throw createOperationException(e,""String_Node_Str"");
      }
    }
  }
}","@Override public List<byte[]> getKeys(int limit,int offset,ReadPointer readPointer) throws OperationException {
  PreparedStatement ps=null;
  try {
    ps=this.connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str"");
    ResultSet result=ps.executeQuery();
    List<byte[]> keys=new ArrayList<byte[]>(limit > 1024 ? 1024 : limit);
    int returned=0;
    int skipped=0;
    long lastDelete=-1;
    long undeleted=-1;
    byte[] lastRow=new byte[0];
    byte[] curRow=new byte[0];
    byte[] curCol=new byte[0];
    byte[] lastCol=new byte[0];
    while (result.next() && returned < limit) {
      byte[] row=result.getBytes(1);
      if (Bytes.equals(lastRow,row)) {
        continue;
      }
      if (!Bytes.equals(curRow,row)) {
        lastCol=new byte[0];
        curCol=new byte[0];
        lastDelete=-1;
        undeleted=-1;
      }
      curRow=row;
      long curVersion=result.getLong(3);
      if (!readPointer.isVisible(curVersion)) {
        continue;
      }
      byte[] column=result.getBytes(2);
      if (Bytes.equals(lastCol,column)) {
        continue;
      }
      if (!Bytes.equals(curCol,column)) {
        curCol=column;
        lastDelete=-1;
        undeleted=-1;
      }
      Type type=Type.from(result.getInt(4));
      if (type.isUndeleteAll()) {
        undeleted=curVersion;
        continue;
      }
      if (type.isDeleteAll()) {
        if (undeleted == curVersion) {
          continue;
        }
 else {
          lastCol=column;
          continue;
        }
      }
      if (type.isDelete()) {
        lastDelete=curVersion;
        continue;
      }
      if (curVersion == lastDelete) {
        continue;
      }
      lastRow=row;
      if (skipped < offset) {
        skipped++;
      }
 else {
        keys.add(row);
        returned++;
      }
    }
    return keys;
  }
 catch (  SQLException e) {
    throw createOperationException(e,""String_Node_Str"");
  }
 finally {
    if (ps != null) {
      try {
        ps.close();
      }
 catch (      SQLException e) {
        throw createOperationException(e,""String_Node_Str"");
      }
    }
  }
}"
7968,"@Override public boolean compareAndSwapDirty(byte[] row,byte[] column,byte[] expectedValue,byte[] newValue) throws OperationException {
  PreparedStatement ps=null;
  Boolean oldAutoCommitValue=null;
  Integer oldTransactionIsolation=null;
  try {
    oldAutoCommitValue=this.connection.getAutoCommit();
    oldTransactionIsolation=this.connection.getTransactionIsolation();
    this.connection.setAutoCommit(false);
    this.connection.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);
    ps=this.connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    ps.setBytes(1,row);
    ps.setBytes(2,column);
    ResultSet result=ps.executeQuery();
    ImmutablePair<Long,byte[]> latest=latest(result);
    byte[] existingValue=latest == null ? null : latest.getSecond();
    ps.close();
    ps=null;
    if ((existingValue == null && expectedValue == null) || Bytes.equals(existingValue,expectedValue)) {
      if (newValue == null || newValue.length == 0) {
        deleteAll(row,column,latest.getFirst());
        ps=this.connection.prepareStatement(""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str"");
        ps.setBytes(1,row);
        ps.setBytes(2,column);
        ps.setLong(3,TransactionOracle.DIRTY_WRITE_VERSION);
        ps.executeUpdate();
      }
 else {
        if (existingValue == null) {
          ps=this.connection.prepareStatement(""String_Node_Str"" + this.quotedTableName + ""String_Node_Str"");
          ps.setBytes(1,row);
          ps.setBytes(2,column);
          ps.setLong(3,TransactionOracle.DIRTY_WRITE_VERSION);
          ps.setInt(4,Type.VALUE.i);
          ps.setBytes(5,newValue);
          ps.executeUpdate();
        }
 else {
          ps=this.connection.prepareStatement(""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str"");
          ps.setBytes(1,newValue);
          ps.setBytes(2,row);
          ps.setBytes(3,column);
          ps.setLong(4,TransactionOracle.DIRTY_WRITE_VERSION);
          ps.executeUpdate();
        }
      }
      this.connection.commit();
      return true;
    }
    return false;
  }
 catch (  SQLException e) {
    throw createOperationException(e,""String_Node_Str"");
  }
 finally {
    try {
      if (oldAutoCommitValue != null) {
        this.connection.setAutoCommit(oldAutoCommitValue);
      }
      if (oldTransactionIsolation != null) {
        this.connection.setTransactionIsolation(oldTransactionIsolation);
      }
      if (ps != null) {
        ps.close();
      }
    }
 catch (    SQLException e) {
      throw createOperationException(e,""String_Node_Str"");
    }
  }
}","@Override public boolean compareAndSwapDirty(byte[] row,byte[] column,byte[] expectedValue,byte[] newValue) throws OperationException {
  PreparedStatement ps=null;
  Boolean oldAutoCommitValue=null;
  Integer oldTransactionIsolation=null;
  try {
    oldAutoCommitValue=this.connection.getAutoCommit();
    oldTransactionIsolation=this.connection.getTransactionIsolation();
    this.connection.setAutoCommit(false);
    this.connection.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);
    ps=this.connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    ps.setBytes(1,row);
    ps.setBytes(2,column);
    ResultSet result=ps.executeQuery();
    ImmutablePair<Long,byte[]> latest=latest(result);
    byte[] existingValue=latest == null ? null : latest.getSecond();
    ps.close();
    ps=null;
    if ((existingValue == null && expectedValue == null) || Bytes.equals(existingValue,expectedValue)) {
      if (newValue == null || newValue.length == 0) {
        deleteAll(row,column,latest.getFirst());
        ps=this.connection.prepareStatement(""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str"");
        ps.setBytes(1,row);
        ps.setBytes(2,column);
        ps.setLong(3,TransactionOracle.DIRTY_WRITE_VERSION);
        ps.executeUpdate();
      }
 else {
        if (existingValue == null) {
          ps=this.connection.prepareStatement(""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str"");
          ps.setBytes(1,row);
          ps.setBytes(2,column);
          ps.setLong(3,TransactionOracle.DIRTY_WRITE_VERSION);
          ps.setInt(4,Type.VALUE.i);
          ps.setBytes(5,newValue);
          ps.executeUpdate();
        }
 else {
          ps=this.connection.prepareStatement(""String_Node_Str"" + this.quotedTableName + ""String_Node_Str""+ ""String_Node_Str"");
          ps.setBytes(1,newValue);
          ps.setBytes(2,row);
          ps.setBytes(3,column);
          ps.setLong(4,TransactionOracle.DIRTY_WRITE_VERSION);
          ps.executeUpdate();
        }
      }
      this.connection.commit();
      return true;
    }
    return false;
  }
 catch (  SQLException e) {
    throw createOperationException(e,""String_Node_Str"");
  }
 finally {
    try {
      if (oldAutoCommitValue != null) {
        this.connection.setAutoCommit(oldAutoCommitValue);
      }
      if (oldTransactionIsolation != null) {
        this.connection.setTransactionIsolation(oldTransactionIsolation);
      }
      if (ps != null) {
        ps.close();
      }
    }
 catch (    SQLException e) {
      throw createOperationException(e,""String_Node_Str"");
    }
  }
}"
7969,"@Override public OrderedVersionedColumnarTable openTable(byte[] tableName) throws OperationException {
  HyperSQLOVCTable table=new HyperSQLOVCTable(Bytes.toString(tableName),this.connection);
  if (table.openTable())   return table;
 else   return null;
}","@Override public OrderedVersionedColumnarTable openTable(byte[] tableName) throws OperationException {
  HyperSQLOVCTable table=new HyperSQLOVCTable(Bytes.toString(tableName),this.connection);
  if (table.openTable()) {
    return table;
  }
 else {
    return null;
  }
}"
7970,"@Inject public HyperSQLOVCTableHandle(@Named(""String_Node_Str"") String hyperSqlJDBCString) throws SQLException {
  this.hyperSqlJDBCString=hyperSqlJDBCString;
  this.connection=DriverManager.getConnection(this.hyperSqlJDBCString);
}","@Inject public HyperSQLOVCTableHandle(@Named(""String_Node_Str"") String hyperSqlJDBCString) throws SQLException {
  this.connection=DriverManager.getConnection(hyperSqlJDBCString);
}"
7971,"/** 
 * clears the listed scopes (user data, tables, meta data, queues, streams, or all).
 * @param id explicit unique id of this operation
 * @param whatToClear list of scopes to clear
 */
public ClearFabric(long id,List<ToClear> whatToClear){
  super(id);
  setToClear(whatToClear);
}","/** 
 * Clears the listed scopes (user data, tables, meta data, queues, streams, or all).
 * @param id explicit unique id of this operation
 * @param whatToClear list of scopes to clear
 */
public ClearFabric(long id,List<ToClear> whatToClear){
  super(id);
  setToClear(whatToClear);
}"
7972,"public String toString(){
  return Objects.toStringHelper(this).add(""String_Node_Str"",Boolean.toString(clearData)).add(""String_Node_Str"",Boolean.toString(clearMeta)).add(""String_Node_Str"",Boolean.toString(clearQueues)).add(""String_Node_Str"",Boolean.toString(clearStreams)).toString();
}","@Override public String toString(){
  return Objects.toStringHelper(this).add(""String_Node_Str"",Boolean.toString(clearData)).add(""String_Node_Str"",Boolean.toString(clearMeta)).add(""String_Node_Str"",Boolean.toString(clearQueues)).add(""String_Node_Str"",Boolean.toString(clearStreams)).toString();
}"
7973,"@Override public int getSize(){
  if (newValue == null || newValue.length == 0)   return 0;
  int diff=newValue.length - (expectedValue == null ? 0 : expectedValue.length);
  if (diff < 0)   return 0;
  return diff;
}","@Override public int getSize(){
  if (newValue == null || newValue.length == 0) {
    return 0;
  }
  int diff=newValue.length - (expectedValue == null ? 0 : expectedValue.length);
  if (diff < 0) {
    return 0;
  }
  return diff;
}"
7974,"/** 
 * Checks the specified columns and amounts arguments for validity.
 * @param columns the columns to increment
 * @param amounts the amounts to increment, in the same order as the columns
 * @throws IllegalArgumentException if no columns specified, no amountsspecified, or number of columns does not match number of amounts.
 */
public static void checkColumnArgs(final Object[] columns,final long[] amounts){
  if (columns == null || columns.length == 0)   throw new IllegalArgumentException(""String_Node_Str"");
  if (amounts == null || amounts.length == 0)   throw new IllegalArgumentException(""String_Node_Str"");
  if (columns.length != amounts.length)   throw new IllegalArgumentException(""String_Node_Str"" + columns.length + ""String_Node_Str""+ amounts.length+ ""String_Node_Str"");
}","/** 
 * Checks the specified columns and amounts arguments for validity.
 * @param columns the columns to increment
 * @param amounts the amounts to increment, in the same order as the columns
 * @throws IllegalArgumentException if no columns specified, no amountsspecified, or number of columns does not match number of amounts.
 */
public static void checkColumnArgs(final Object[] columns,final long[] amounts){
  if (columns == null || columns.length == 0) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (amounts == null || amounts.length == 0) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (columns.length != amounts.length) {
    throw new IllegalArgumentException(""String_Node_Str"" + columns.length + ""String_Node_Str""+ amounts.length+ ""String_Node_Str"");
  }
}"
7975,"@Override public int getSize(){
  if (key == null || columns == null)   return 0;
  int size=key.length;
  for (int i=0; i < columns.length; i++) {
    size+=columns[i].length + 8;
  }
  return size;
}","@Override public int getSize(){
  if (key == null || columns == null) {
    return 0;
  }
  int size=key.length;
  for (int i=0; i < columns.length; i++) {
    size+=columns[i].length + 8;
  }
  return size;
}"
7976,"public String getTableName(){
  return table;
}","/** 
 * Get the table name.
 * @return the table name
 */
public String getTableName(){
  return table;
}"
7977,"public String toString(){
  return ""String_Node_Str"" + table + ""String_Node_Str"";
}","@Override public String toString(){
  return ""String_Node_Str"" + table + ""String_Node_Str"";
}"
7978,"/** 
 * To open the named table
 * @param id explicit unique id of this operation
 * @param tableName the name of the table to open
 */
public OpenTable(long id,String tableName){
  super(id);
  this.table=tableName;
}","/** 
 * To open the named table.
 * @param id explicit unique id of this operation
 * @param tableName the name of the table to open
 */
public OpenTable(long id,String tableName){
  super(id);
  this.table=tableName;
}"
7979,"/** 
 * set a name to use for the data metrics - ypically the name of the data set that emitted the operation
 * @param name the name to use
 */
public void setMetricName(String name){
  this.metricName=name;
}","/** 
 * set a name to use for the data metrics - typically the name of the data set that emitted the operation.
 * @param name the name to use
 */
public void setMetricName(String name){
  this.metricName=name;
}"
7980,"/** 
 * @return a name to use for the data metrics - ypically the name ofthe data set that emitted the operation
 */
public @Nullable String getMetricName(){
  return this.metricName;
}","/** 
 * @return a name to use for the data metrics - ypically the name ofthe data set that emitted the operation
 */
@Nullable public String getMetricName(){
  return this.metricName;
}"
7981,"/** 
 * Constructor with id - typically used for deserialization
 */
protected Operation(long id){
  this.id=id;
}","/** 
 * Constructor with id - typically used for deserialization.
 */
protected Operation(long id){
  this.id=id;
}"
7982,"/** 
 * Constructor for operation context
 * @param account  account Id
 */
public OperationContext(String account){
  this(account,null);
}","/** 
 * Constructor for operation context.
 * @param account  account Id
 */
public OperationContext(String account){
  this(account,null);
}"
7983,"/** 
 * getApplicationId
 * @return String application id
 */
public String getApplication(){
  return this.application;
}","/** 
 * @return String application id
 */
public String getApplication(){
  return this.application;
}"
7984,"/** 
 * getAccountId
 * @return String account Id
 */
public String getAccount(){
  return account;
}","/** 
 * @return String account Id
 */
public String getAccount(){
  return account;
}"
7985,"public byte[] getKey(){
  return this.key;
}","/** 
 * @return the row key for the read
 */
public byte[] getKey(){
  return this.key;
}"
7986,"public String getTable(){
  return this.table;
}","/** 
 * @return the table name
 */
public String getTable(){
  return this.table;
}"
7987,"public String toString(){
  StringBuilder builder=new StringBuilder();
  char sep='[';
  for (  byte[] column : this.columns) {
    builder.append(sep);
    builder.append(new String(column));
    sep=',';
  }
  builder.append(']');
  String columnsStr=builder.toString();
  return Objects.toStringHelper(this).add(""String_Node_Str"",new String(this.key)).add(""String_Node_Str"",columnsStr).toString();
}","@Override public String toString(){
  StringBuilder builder=new StringBuilder();
  char sep='[';
  for (  byte[] column : this.columns) {
    builder.append(sep);
    builder.append(new String(column));
    sep=',';
  }
  builder.append(']');
  String columnsStr=builder.toString();
  return Objects.toStringHelper(this).add(""String_Node_Str"",new String(this.key)).add(""String_Node_Str"",columnsStr).toString();
}"
7988,"public byte[][] getColumns(){
  return this.columns;
}","/** 
 * @return the columns to read
 */
public byte[][] getColumns(){
  return this.columns;
}"
7989,"public ReportConsoleThread(AgentGroup[] groups,BenchmarkMetric[] metrics,CConfiguration config){
  this.groupMetrics=metrics;
  this.groups=groups;
  this.reportInterval=config.getInt(""String_Node_Str"",reportInterval);
}","public ReportConsoleThread(AgentGroup[] groups,BenchmarkMetric[] metrics,CConfiguration config){
  super(groups,metrics);
  this.reportInterval=config.getInt(""String_Node_Str"",reportInterval);
}"
7990,"public ReportFileAppenderThread(String benchmarkName,AgentGroup[] groups,BenchmarkMetric[] metrics,CConfiguration config){
  this.groupMetrics=metrics;
  this.groups=groups;
  this.reportInterval=config.getInt(""String_Node_Str"",reportInterval);
  this.fileName=config.get(""String_Node_Str"");
  int pos=benchmarkName.lastIndexOf(""String_Node_Str"");
  if (pos != -1) {
    this.benchmarkName=benchmarkName.substring(pos + 1,benchmarkName.length());
  }
 else {
    this.benchmarkName=benchmarkName;
  }
  this.metrics=new HashMap<String,ArrayList<Double>>(groups.length);
  for (  AgentGroup group : groups) {
    this.metrics.put(group.getName(),new ArrayList<Double>());
  }
}","public ReportFileAppenderThread(String benchmarkName,AgentGroup[] groups,BenchmarkMetric[] metrics,CConfiguration config){
  super(groups,metrics);
  this.fileName=config.get(""String_Node_Str"");
  int pos=benchmarkName.lastIndexOf(""String_Node_Str"");
  if (pos != -1) {
    this.benchmarkName=benchmarkName.substring(pos + 1,benchmarkName.length());
  }
 else {
    this.benchmarkName=benchmarkName;
  }
  this.metrics=new HashMap<String,ArrayList<Double>>(groups.length);
  for (  AgentGroup group : groups) {
    this.metrics.put(group.getName(),new ArrayList<Double>());
  }
}"
7991,"@Override public void processGroupMetricsInterval(long unixTime,AgentGroup group,long previousMillis,long millis,Map<String,Long> prevMetrics,Map<String,Long> latestMetrics,boolean interrupt){
  if (prevMetrics != null) {
    for (    Map.Entry<String,Long> singleMetric : prevMetrics.entrySet()) {
      String key=singleMetric.getKey();
      long value=singleMetric.getValue();
      if (!interrupt) {
        Long previousValue=prevMetrics.get(key);
        if (previousValue == null)         previousValue=0L;
        long valueSince=value - previousValue;
        long millisSince=millis - previousMillis;
        metrics.get(group.getName()).add(valueSince * 1000.0 / millisSince);
        String metricValue=String.format(""String_Node_Str"",valueSince * 1000.0 / millisSince);
        String metric=MensaUtils.buildMetric(OPS_PER_SEC_ONE_MIN,Long.toString(unixTime),metricValue,benchmarkName,group.getName(),Integer.toString(group.getNumAgents()),""String_Node_Str"");
        LOG.debug(""String_Node_Str"",metric);
      }
    }
  }
}","@Override public void processGroupMetricsInterval(long unixTime,AgentGroup group,long previousMillis,long millis,Map<String,Long> prevMetrics,Map<String,Long> latestMetrics,boolean interrupt){
  if (prevMetrics != null) {
    for (    Map.Entry<String,Long> singleMetric : latestMetrics.entrySet()) {
      String key=singleMetric.getKey();
      long value=singleMetric.getValue();
      if (!interrupt) {
        Long previousValue=prevMetrics.get(key);
        if (previousValue == null) {
          previousValue=0L;
        }
        long valueSince=value - previousValue;
        long millisSince=millis - previousMillis;
        metrics.get(group.getName()).add(valueSince * 1000.0 / millisSince);
        String metricValue=String.format(""String_Node_Str"",valueSince * 1000.0 / millisSince);
        String metric=MensaUtils.buildMetric(OPS_PER_SEC_ONE_MIN,Long.toString(unixTime),metricValue,benchmarkName,group.getName(),Integer.toString(group.getNumAgents()),""String_Node_Str"");
        LOG.debug(""String_Node_Str"",metric);
      }
    }
  }
}"
7992,"public ReportMensaWriterThread(String benchmarkName,AgentGroup[] groups,BenchmarkMetric[] metrics,CConfiguration config,String extraTags){
  this.groupMetrics=metrics;
  this.groups=groups;
  this.reportInterval=config.getInt(""String_Node_Str"",reportInterval);
  this.fileName=config.get(""String_Node_Str"");
  int pos=benchmarkName.lastIndexOf(""String_Node_Str"");
  if (pos != -1) {
    this.benchmarkName=benchmarkName.substring(pos + 1,benchmarkName.length());
  }
 else {
    this.benchmarkName=benchmarkName;
  }
  String mensa=config.get(""String_Node_Str"");
  if (mensa != null && mensa.length() != 0) {
    String[] hostPort=mensa.split(""String_Node_Str"");
    this.mensaHost=hostPort[0];
    this.mensaPort=Integer.valueOf(hostPort[1]);
  }
  mensaTags=config.get(""String_Node_Str"");
  if (mensaTags != null) {
    mensaTags=mensaTags.replace(""String_Node_Str"",""String_Node_Str"");
  }
  if (extraTags != null && extraTags.length() != 0) {
    if (mensaTags != null && mensaTags.length() != 0) {
      mensaTags=mensaTags + ""String_Node_Str"" + extraTags;
    }
 else {
      mensaTags=extraTags;
    }
  }
  this.metrics=new HashMap<String,ArrayList<Double>>(groups.length);
  for (  AgentGroup group : groups) {
    this.metrics.put(group.getName(),new ArrayList<Double>());
  }
}","public ReportMensaWriterThread(String benchmarkName,AgentGroup[] groups,BenchmarkMetric[] metrics,CConfiguration config,String extraTags){
  super(groups,metrics);
  this.fileName=config.get(""String_Node_Str"");
  int pos=benchmarkName.lastIndexOf(""String_Node_Str"");
  if (pos != -1) {
    this.benchmarkName=benchmarkName.substring(pos + 1,benchmarkName.length());
  }
 else {
    this.benchmarkName=benchmarkName;
  }
  String mensa=config.get(""String_Node_Str"");
  if (mensa != null && mensa.length() != 0) {
    String[] hostPort=mensa.split(""String_Node_Str"");
    this.mensaHost=hostPort[0];
    this.mensaPort=Integer.valueOf(hostPort[1]);
  }
  mensaTags=config.get(""String_Node_Str"");
  if (mensaTags != null) {
    mensaTags=mensaTags.replace(""String_Node_Str"",""String_Node_Str"");
  }
  if (extraTags != null && extraTags.length() != 0) {
    if (mensaTags != null && mensaTags.length() != 0) {
      mensaTags=mensaTags + ""String_Node_Str"" + extraTags;
    }
 else {
      mensaTags=extraTags;
    }
  }
  this.metrics=new HashMap<String,ArrayList<Double>>(groups.length);
  for (  AgentGroup group : groups) {
    this.metrics.put(group.getName(),new ArrayList<Double>());
  }
}"
7993,"@Override public void processGroupMetricsInterval(long unixTime,AgentGroup group,long previousMillis,long millis,Map<String,Long> prevMetrics,Map<String,Long> latestMetrics,boolean interrupt) throws BenchmarkException {
  if (prevMetrics != null) {
    for (    Map.Entry<String,Long> singleMetric : prevMetrics.entrySet()) {
      String key=singleMetric.getKey();
      long value=singleMetric.getValue();
      if (!interrupt) {
        Long previousValue=prevMetrics.get(key);
        if (previousValue == null)         previousValue=0L;
        long valueSince=value - previousValue;
        long millisSince=millis - previousMillis;
        metrics.get(group.getName()).add(valueSince * 1000.0 / millisSince);
        String metricValue=String.format(""String_Node_Str"",valueSince * 1000.0 / millisSince);
        String metric=MensaUtils.buildMetric(OPS_PER_SEC_ONE_MIN,Long.toString(unixTime),metricValue,benchmarkName,group.getName(),Integer.toString(group.getNumAgents()),mensaTags);
        try {
          MensaUtils.uploadMetric(mensaHost,mensaPort,metric);
          LOG.debug(""String_Node_Str"",metric,mensaHost,mensaPort);
        }
 catch (        IOException e) {
          throw new BenchmarkException(""String_Node_Str"" + metric + ""String_Node_Str""+ mensaHost+ ""String_Node_Str""+ mensaPort+ ""String_Node_Str"",e);
        }
      }
    }
  }
}","@Override public void processGroupMetricsInterval(long unixTime,AgentGroup group,long previousMillis,long millis,Map<String,Long> prevMetrics,Map<String,Long> latestMetrics,boolean interrupt) throws BenchmarkException {
  if (prevMetrics != null) {
    for (    Map.Entry<String,Long> singleMetric : latestMetrics.entrySet()) {
      String metricName=singleMetric.getKey();
      long latestValue=singleMetric.getValue();
      if (!interrupt) {
        Long previousValue=prevMetrics.get(metricName);
        if (previousValue == null) {
          previousValue=0L;
        }
        long valueSince=latestValue - previousValue;
        long millisSince=millis - previousMillis;
        metrics.get(group.getName()).add(valueSince * 1000.0 / millisSince);
        String metricValue=String.format(""String_Node_Str"",valueSince * 1000.0 / millisSince);
        String metric=MensaUtils.buildMetric(OPS_PER_SEC_10_SEC,Long.toString(unixTime),metricValue,benchmarkName,group.getName(),Integer.toString(group.getNumAgents()),mensaTags);
        try {
          MensaUtils.uploadMetric(mensaHost,mensaPort,metric);
          LOG.debug(""String_Node_Str"",metric,mensaHost,mensaPort);
        }
 catch (        IOException e) {
          throw new BenchmarkException(""String_Node_Str"" + metric + ""String_Node_Str""+ mensaHost+ ""String_Node_Str""+ mensaPort+ ""String_Node_Str"",e);
        }
      }
    }
  }
}"
7994,"public final void run(){
  try {
    init();
    long start=System.currentTimeMillis();
    long unixTime;
    boolean interrupt=false;
    ArrayList<Map<String,Long>> previousMetrics=new ArrayList<Map<String,Long>>(groups.length);
    for (int i=0; i < groups.length; i++) {
      previousMetrics.add(null);
    }
    long[] previousMillis=new long[groups.length];
    for (int seconds=reportInterval; !interrupt; seconds+=reportInterval) {
      long wakeup=start + (seconds * 1000);
      long currentTime=System.currentTimeMillis();
      unixTime=currentTime / 1000L;
      try {
        if (wakeup > currentTime) {
          Thread.sleep(wakeup - currentTime);
        }
      }
 catch (      InterruptedException e) {
        interrupt=true;
      }
      long latestMillis;
      if (interrupt) {
        latestMillis=System.currentTimeMillis() - start;
      }
 else {
        latestMillis=seconds * 1000L;
      }
      LOG.debug(""String_Node_Str"",time2String(latestMillis));
      for (int i=0; i < groups.length; i++) {
        Map<String,Long> latestGrpMetrics=groupMetrics[i].list();
        Map<String,Long> previousGrpMetrics=previousMetrics.get(i);
        processGroupMetricsInterval(unixTime,groups[i],previousMillis[i],latestMillis,previousGrpMetrics,latestGrpMetrics,interrupt);
        previousMetrics.set(i,latestGrpMetrics);
        previousMillis[i]=latestMillis;
      }
    }
    LOG.debug(""String_Node_Str"");
    unixTime=System.currentTimeMillis() / 1000L;
    for (    AgentGroup group : groups) {
      processGroupMetricsFinal(unixTime,group);
    }
  }
 catch (  Exception e) {
    LOG.error(e.getMessage(),e);
  }
 finally {
    shutdown();
  }
}","public final void run(){
  try {
    init();
    long start=System.currentTimeMillis();
    long unixTime;
    boolean interrupt=false;
    ArrayList<Map<String,Long>> previousMetrics=new ArrayList<Map<String,Long>>(groups.length);
    for (int i=0; i < groups.length; i++) {
      previousMetrics.add(null);
    }
    long[] previousMillis=new long[groups.length];
    int interval=getInterval();
    for (int seconds=interval; !interrupt; seconds+=interval) {
      long wakeup=start + (seconds * 1000);
      long currentTime=System.currentTimeMillis();
      unixTime=currentTime / 1000L;
      try {
        if (wakeup > currentTime) {
          Thread.sleep(wakeup - currentTime);
        }
      }
 catch (      InterruptedException e) {
        interrupt=true;
      }
      long latestMillis;
      if (interrupt) {
        latestMillis=System.currentTimeMillis() - start;
      }
 else {
        latestMillis=seconds * 1000L;
      }
      LOG.debug(""String_Node_Str"",time2String(latestMillis));
      for (int i=0; i < groups.length; i++) {
        Map<String,Long> latestGrpMetrics=groupMetrics[i].list();
        Map<String,Long> previousGrpMetrics=previousMetrics.get(i);
        processGroupMetricsInterval(unixTime,groups[i],previousMillis[i],latestMillis,previousGrpMetrics,latestGrpMetrics,interrupt);
        previousMetrics.set(i,latestGrpMetrics);
        previousMillis[i]=latestMillis;
      }
    }
    LOG.debug(""String_Node_Str"");
    unixTime=System.currentTimeMillis() / 1000L;
    for (    AgentGroup group : groups) {
      processGroupMetricsFinal(unixTime,group);
    }
  }
 catch (  Exception e) {
    LOG.error(e.getMessage(),e);
  }
 finally {
    shutdown();
  }
}"
7995,"public boolean validate(String reportLine){
  String[] split=reportLine.split(""String_Node_Str"");
  String metric=split[1];
  String ts=split[2];
  double newMetricValueAvg=Double.valueOf(split[3]);
  StringBuilder sb=new StringBuilder();
  for (int i=4; i < split.length; i++) {
    if (!split[i].contains(""String_Node_Str"")) {
      sb.append(split[i]);
      sb.append(""String_Node_Str"");
    }
  }
  sb.setLength(sb.length() - 1);
  String tags=sb.toString();
  MetricsResult result=queryTSDB(host,port,startts,endts,metric,tags);
  double oldMetricValueAvg=0;
  if (result != null && result.getMetrics().size() != 0) {
    oldMetricValueAvg=result.getMetric(0).avg(7);
  }
  if (newMetricValueAvg < 0.95 * oldMetricValueAvg) {
    System.out.println(""String_Node_Str"" + oldMetricValueAvg + ""String_Node_Str""+ newMetricValueAvg);
    return false;
  }
 else {
    return true;
  }
}","public boolean validate(String reportLine){
  String[] split=reportLine.split(""String_Node_Str"");
  String metric=split[1];
  String ts=split[2];
  double newMetricValueAvg=Double.valueOf(split[3]);
  StringBuilder sb=new StringBuilder();
  for (int i=4; i < split.length; i++) {
    if (!split[i].contains(""String_Node_Str"")) {
      sb.append(split[i]);
      sb.append(""String_Node_Str"");
    }
  }
  sb.setLength(sb.length() - 1);
  String tags=sb.toString();
  MetricsResult result=queryTSDB(host,port,startts,endts,metric,tags);
  double oldMetricValueAvg=0;
  if (result != null && result.getMetrics().size() != 0 && result.getMetric(0).getNumDataPoints() >= 7) {
    oldMetricValueAvg=result.getMetric(0).avg(7);
  }
  if (newMetricValueAvg < 0.95 * oldMetricValueAvg) {
    System.out.println(""String_Node_Str"" + oldMetricValueAvg + ""String_Node_Str""+ newMetricValueAvg);
    return false;
  }
 else {
    return true;
  }
}"
7996,"/** 
 * Executes a   {@link com.continuuity.data.operation.Increment} operation. Ifa non-null transaction is passed in, the operation is performed in that client-side transaction. Otherwise it is performed and committed as an anonymous transaction.
 * @param context the operation context
 * @param transaction an existing transaction, or null to perform an anonymoustransaction
 * @param increment the operation
 * @return a result object containing a map of columns to the new, incrementedvalues.
 * @throws OperationException is something goes wrong
 */
public Map<byte[],Long> increment(OperationContext context,@Nullable Transaction transaction,Increment increment) throws OperationException ;","/** 
 * Executes a   {@link com.continuuity.data.operation.Increment} operation.A valid transaction must be passed in, and the operation is performed in that client-side transaction.
 * @param context the operation context
 * @param transaction an existing, valid transaction
 * @param increment the operation
 * @return a result object containing a map of columns to the new, incrementedvalues.
 * @throws OperationException is something goes wrong
 */
public Map<byte[],Long> increment(OperationContext context,Transaction transaction,Increment increment) throws OperationException ;"
7997,"@Override public Map<byte[],Long> increment(OperationContext context,Transaction transaction,Increment increment) throws OperationException {
  if (transaction == null) {
    return increment(context,increment);
  }
  WriteTransactionResult writeTxReturn=write(context,increment,transaction);
  List<Undo> undos=writeTxReturn.undos;
  if (null != undos && !undos.isEmpty()) {
    addToTransaction(transaction,undos);
  }
  if (writeTxReturn.success) {
    return writeTxReturn.incrementResult;
  }
 else {
    cmetric.meter(METRIC_PREFIX + ""String_Node_Str"",1);
    abort(context,transaction);
    throw new OmidTransactionException(writeTxReturn.statusCode,writeTxReturn.message);
  }
}","@Override public Map<byte[],Long> increment(OperationContext context,Transaction transaction,Increment increment) throws OperationException {
  if (transaction == null) {
    throw new OmidTransactionException(StatusCode.INVALID_TRANSACTION,""String_Node_Str"");
  }
 else {
    oracle.validateTransaction(transaction);
  }
  WriteTransactionResult writeTxReturn=write(context,increment,transaction);
  List<Undo> undos=writeTxReturn.undos;
  if (null != undos && !undos.isEmpty()) {
    addToTransaction(transaction,undos);
  }
  if (writeTxReturn.success) {
    return writeTxReturn.incrementResult;
  }
 else {
    cmetric.meter(METRIC_PREFIX + ""String_Node_Str"",1);
    abort(context,transaction);
    throw new OmidTransactionException(writeTxReturn.statusCode,writeTxReturn.message);
  }
}"
7998,"public void abort(OperationContext context,Transaction transaction) throws OperationException, TException {
  MetricsHelper helper=newHelper(""String_Node_Str"");
  if (Log.isTraceEnabled())   Log.trace(""String_Node_Str"");
  try {
    TOperationContext tcontext=wrap(context);
    client.commit(tcontext,wrap(transaction));
    if (Log.isTraceEnabled())     Log.trace(""String_Node_Str"");
    helper.success();
  }
 catch (  TOperationException te) {
    helper.failure();
    throw unwrap(te);
  }
catch (  TException te) {
    helper.failure();
    throw te;
  }
}","public void abort(OperationContext context,Transaction transaction) throws OperationException, TException {
  MetricsHelper helper=newHelper(""String_Node_Str"");
  if (Log.isTraceEnabled())   Log.trace(""String_Node_Str"");
  try {
    TOperationContext tcontext=wrap(context);
    client.abort(tcontext,wrap(transaction));
    if (Log.isTraceEnabled())     Log.trace(""String_Node_Str"");
    helper.success();
  }
 catch (  TOperationException te) {
    helper.failure();
    throw unwrap(te);
  }
catch (  TException te) {
    helper.failure();
    throw te;
  }
}"
7999,"public Map<byte[],Long> increment(OperationContext context,Increment increment) throws TException, OperationException {
  MetricsHelper helper=newHelper(""String_Node_Str"",increment.getTable());
  try {
    if (Log.isTraceEnabled())     Log.trace(""String_Node_Str"");
    TOperationContext tcontext=wrap(context);
    TIncrement tReadColumnRange=wrap(increment);
    if (Log.isTraceEnabled())     Log.trace(""String_Node_Str"");
    Map<ByteBuffer,Long> tResult=client.increment(tcontext,tReadColumnRange);
    if (Log.isTraceEnabled())     Log.trace(""String_Node_Str"");
    Map<byte[],Long> result=unwrapLongMap(tResult);
    helper.finish(result.isEmpty() ? NoData : Success);
    return result;
  }
 catch (  TOperationException te) {
    helper.failure();
    throw unwrap(te);
  }
catch (  TException te) {
    helper.failure();
    throw te;
  }
}","public Map<byte[],Long> increment(OperationContext context,Increment increment) throws TException, OperationException {
  MetricsHelper helper=newHelper(""String_Node_Str"",increment.getTable());
  try {
    if (Log.isTraceEnabled())     Log.trace(""String_Node_Str"");
    TOperationContext tcontext=wrap(context);
    TIncrement tIncrement=wrap(increment);
    if (Log.isTraceEnabled())     Log.trace(""String_Node_Str"");
    Map<ByteBuffer,Long> tResult=client.increment(tcontext,tIncrement);
    if (Log.isTraceEnabled())     Log.trace(""String_Node_Str"");
    Map<byte[],Long> result=unwrapLongMap(tResult);
    helper.finish(result.isEmpty() ? NoData : Success);
    return result;
  }
 catch (  TOperationException te) {
    helper.failure();
    throw unwrap(te);
  }
catch (  TException te) {
    helper.failure();
    throw te;
  }
}"
8000,"@Override public TTQueueTable getQueueTable(byte[] queueTableName) throws OperationException {
  TTQueueTable queueTable=this.queueTables.get(queueTableName);
  if (queueTable != null)   return queueTable;
  OrderedVersionedColumnarTable table=getTable(queueOVCTable);
  queueTable=new TTQueueTableOnVCTable(table,oracle,conf);
  TTQueueTable existing=this.queueTables.putIfAbsent(queueTableName,queueTable);
  return existing != null ? existing : queueTable;
}","@Override public TTQueueTable getQueueTable(byte[] queueTableName) throws OperationException {
  TTQueueTable queueTable=this.queueTables.get(queueTableName);
  if (queueTable != null)   return queueTable;
  OrderedVersionedColumnarTable table=getTable(queueOVCTable);
  queueTable=new TTQueueTableNewOnVCTable(table,oracle,conf);
  TTQueueTable existing=this.queueTables.putIfAbsent(queueTableName,queueTable);
  return existing != null ? existing : queueTable;
}"
