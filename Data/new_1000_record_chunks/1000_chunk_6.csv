record_number,buggy_code,fixed_code
5001,"/** 
 * Prepares the Spark 2 framework on the location.
 * @param sparkConf the spark configuration
 * @param locationFactory the {@link LocationFactory} for saving the spark framework jar
 * @param tempDir directory for temporary file creation
 * @return A {@link SparkFramework} containing information about the spark framework in localization context.
 * @throws IOException If failed to prepare the framework.
 */
private static SparkFramework prepareSpark2Framework(Properties sparkConf,LocationFactory locationFactory,File tempDir) throws IOException {
  String sparkYarnArchive=sparkConf.getProperty(SPARK_YARN_ARCHIVE);
  if (sparkYarnArchive != null) {
    Location frameworkLocation=locationFactory.create(URI.create(sparkYarnArchive));
    if (frameworkLocation.exists()) {
      return new SparkFramework(new LocalizeResource(resolveURI(frameworkLocation),true),SPARK_YARN_ARCHIVE);
    }
    LOG.warn(""String_Node_Str"",frameworkLocation,SPARK_YARN_ARCHIVE);
  }
  String sparkVersion=System.getenv(SPARK_VERSION);
  sparkVersion=sparkVersion == null ? SparkCompat.SPARK2_2_11.getCompat() : sparkVersion;
  String archiveName=""String_Node_Str"" + sparkVersion + ""String_Node_Str"";
  Location frameworkDir=locationFactory.create(""String_Node_Str"");
  Location frameworkLocation=frameworkDir.append(archiveName);
  if (!frameworkLocation.exists()) {
    File archive=new File(tempDir,archiveName);
    try {
      try (ZipOutputStream zipOutput=new ZipOutputStream(new BufferedOutputStream(new FileOutputStream(archive)))){
        zipOutput.setLevel(Deflater.NO_COMPRESSION);
        for (        File file : getLocalSparkLibrary(SparkCompat.SPARK2_2_11)) {
          zipOutput.putNextEntry(new ZipEntry(file.getName()));
          Files.copy(file.toPath(),zipOutput);
          zipOutput.closeEntry();
        }
      }
       frameworkDir.mkdirs(""String_Node_Str"");
      try (OutputStream os=frameworkLocation.getOutputStream(""String_Node_Str"")){
        Files.copy(archive.toPath(),os);
      }
     }
  finally {
      archive.delete();
    }
  }
  return new SparkFramework(new LocalizeResource(resolveURI(frameworkLocation),true),SPARK_YARN_ARCHIVE);
}","/** 
 * Prepares the Spark 2 framework on the location.
 * @param sparkConf the spark configuration
 * @param locationFactory the {@link LocationFactory} for saving the spark framework jar
 * @param tempDir directory for temporary file creation
 * @return A {@link SparkFramework} containing information about the spark framework in localization context.
 * @throws IOException If failed to prepare the framework.
 */
private static SparkFramework prepareSpark2Framework(Properties sparkConf,LocationFactory locationFactory,File tempDir) throws IOException {
  String sparkYarnArchive=sparkConf.getProperty(SPARK_YARN_ARCHIVE);
  if (sparkYarnArchive != null) {
    URI sparkYarnArchiveURI=URI.create(sparkYarnArchive);
    if (locationFactory.getHomeLocation().toURI().getScheme().equals(sparkYarnArchiveURI.getScheme())) {
      Location frameworkLocation=locationFactory.create(URI.create(sparkYarnArchive));
      if (frameworkLocation.exists()) {
        return new SparkFramework(new LocalizeResource(resolveURI(frameworkLocation),true),SPARK_YARN_ARCHIVE);
      }
      LOG.warn(""String_Node_Str"",frameworkLocation,SPARK_YARN_ARCHIVE);
    }
  }
  String sparkVersion=System.getenv(SPARK_VERSION);
  sparkVersion=sparkVersion == null ? SparkCompat.SPARK2_2_11.getCompat() : sparkVersion;
  String archiveName=""String_Node_Str"" + sparkVersion + ""String_Node_Str"";
  Location frameworkDir=locationFactory.create(""String_Node_Str"");
  Location frameworkLocation=frameworkDir.append(archiveName);
  if (!frameworkLocation.exists()) {
    File archive=new File(tempDir,archiveName);
    try {
      try (ZipOutputStream zipOutput=new ZipOutputStream(new BufferedOutputStream(new FileOutputStream(archive)))){
        zipOutput.setLevel(Deflater.NO_COMPRESSION);
        for (        File file : getLocalSparkLibrary(SparkCompat.SPARK2_2_11)) {
          zipOutput.putNextEntry(new ZipEntry(file.getName()));
          Files.copy(file.toPath(),zipOutput);
          zipOutput.closeEntry();
        }
      }
       frameworkDir.mkdirs(""String_Node_Str"");
      try (OutputStream os=frameworkLocation.getOutputStream(""String_Node_Str"")){
        Files.copy(archive.toPath(),os);
      }
     }
  finally {
      archive.delete();
    }
  }
  return new SparkFramework(new LocalizeResource(resolveURI(frameworkLocation),true),SPARK_YARN_ARCHIVE);
}"
5002,"/** 
 * Returns an array of URLs to be used for creation of classloader for Spark, based on the urls used by the given   {@link ClassLoader}.
 */
private URL[] getSparkClassloaderURLs(ClassLoader classLoader) throws IOException {
  List<URL> urls=ClassLoaders.getClassLoaderURLs(classLoader,new LinkedList<URL>());
  if (classLoader.getResource(""String_Node_Str"") == null) {
    Iterator<URL> itor=urls.iterator();
    while (itor.hasNext()) {
      URL url=itor.next();
      if (url.getPath().contains(""String_Node_Str"")) {
        itor.remove();
      }
    }
    for (    File file : SparkPackageUtils.getLocalSparkLibrary(providerSparkCompat)) {
      urls.add(file.toURI().toURL());
    }
  }
  return urls.toArray(new URL[urls.size()]);
}","/** 
 * Returns an array of URLs to be used for creation of classloader for Spark, based on the urls used by the given   {@link ClassLoader}.
 */
private URL[] getSparkClassloaderURLs(ClassLoader classLoader) throws IOException {
  List<URL> urls=ClassLoaders.getClassLoaderURLs(classLoader,new LinkedList<URL>());
  if (classLoader.getResource(""String_Node_Str"") == null) {
    Iterator<URL> itor=urls.iterator();
    while (itor.hasNext()) {
      URL url=itor.next();
      String filename=Paths.get(url.getPath()).getFileName().toString();
      if (filename.startsWith(""String_Node_Str"") || filename.startsWith(""String_Node_Str"")) {
        itor.remove();
      }
    }
    for (    File file : SparkPackageUtils.getLocalSparkLibrary(providerSparkCompat)) {
      urls.add(file.toURI().toURL());
    }
  }
  return urls.toArray(new URL[urls.size()]);
}"
5003,"@Override public boolean acceptPackage(String packageName){
  return !packageName.startsWith(""String_Node_Str"");
}","@Override public boolean acceptPackage(String packageName){
  return !packageName.startsWith(""String_Node_Str"") && !packageName.startsWith(""String_Node_Str"") && !packageName.startsWith(""String_Node_Str"");
}"
5004,"@Override public boolean acceptResource(String resource){
  return !resource.startsWith(""String_Node_Str"") && !""String_Node_Str"".equals(resource);
}","@Override public boolean acceptResource(String resource){
  return !resource.startsWith(""String_Node_Str"") && !resource.startsWith(""String_Node_Str"") && !resource.startsWith(""String_Node_Str"")&& !""String_Node_Str"".equals(resource);
}"
5005,"ScalaFilterClassLoader(ClassLoader parent){
  super(new FilterClassLoader(parent,new FilterClassLoader.Filter(){
    @Override public boolean acceptResource(    String resource){
      return !resource.startsWith(""String_Node_Str"") && !""String_Node_Str"".equals(resource);
    }
    @Override public boolean acceptPackage(    String packageName){
      return !packageName.startsWith(""String_Node_Str"");
    }
  }
));
}","ScalaFilterClassLoader(ClassLoader parent){
  super(new FilterClassLoader(parent,new FilterClassLoader.Filter(){
    @Override public boolean acceptResource(    String resource){
      return !resource.startsWith(""String_Node_Str"") && !resource.startsWith(""String_Node_Str"") && !resource.startsWith(""String_Node_Str"")&& !""String_Node_Str"".equals(resource);
    }
    @Override public boolean acceptPackage(    String packageName){
      return !packageName.startsWith(""String_Node_Str"") && !packageName.startsWith(""String_Node_Str"") && !packageName.startsWith(""String_Node_Str"");
    }
  }
));
}"
5006,"@Override protected void setupLaunchConfig(LaunchConfig launchConfig,Program program,ProgramOptions options,CConfiguration cConf,Configuration hConf,File tempDir) throws IOException {
  hConf.setBoolean(SparkRuntimeContextConfig.HCONF_ATTR_CLUSTER_MODE,true);
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    hConf.setLong(SparkRuntimeContextConfig.HCONF_ATTR_CREDENTIALS_UPDATE_INTERVAL_MS,(long)((secureStoreRenewer.getUpdateInterval() + 5000) / 0.8));
  }
  ApplicationSpecification appSpec=program.getApplicationSpecification();
  SparkSpecification spec=appSpec.getSpark().get(program.getName());
  Map<String,String> clientArgs=RuntimeArguments.extractScope(""String_Node_Str"",""String_Node_Str"",options.getUserArguments().asMap());
  Resources resources=SystemArguments.getResources(clientArgs,spec.getClientResources());
  launchConfig.addRunnable(spec.getName(),new SparkTwillRunnable(spec.getName()),resources,1);
  Map<String,LocalizeResource> localizeResources=new HashMap<>();
  Map<String,String> extraEnv=new HashMap<>(SparkPackageUtils.getSparkClientEnv());
  SparkPackageUtils.prepareSparkResources(sparkCompat,locationFactory,tempDir,localizeResources,extraEnv);
  extraEnv.put(Constants.SPARK_COMPAT_ENV,sparkCompat.getCompat());
  launchConfig.addExtraResources(localizeResources).addExtraDependencies(SparkProgramRuntimeProvider.class).addExtraEnv(extraEnv).setClassAcceptor(createBundlerClassAcceptor());
}","@Override protected void setupLaunchConfig(LaunchConfig launchConfig,Program program,ProgramOptions options,CConfiguration cConf,Configuration hConf,File tempDir) throws IOException {
  hConf.setBoolean(SparkRuntimeContextConfig.HCONF_ATTR_CLUSTER_MODE,true);
  hConf.set(""String_Node_Str"",HiveAuthFactory.HS2_CLIENT_TOKEN);
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    hConf.setLong(SparkRuntimeContextConfig.HCONF_ATTR_CREDENTIALS_UPDATE_INTERVAL_MS,(long)((secureStoreRenewer.getUpdateInterval() + 5000) / 0.8));
  }
  ApplicationSpecification appSpec=program.getApplicationSpecification();
  SparkSpecification spec=appSpec.getSpark().get(program.getName());
  Map<String,String> clientArgs=RuntimeArguments.extractScope(""String_Node_Str"",""String_Node_Str"",options.getUserArguments().asMap());
  Resources resources=SystemArguments.getResources(clientArgs,spec.getClientResources());
  launchConfig.addRunnable(spec.getName(),new SparkTwillRunnable(spec.getName()),resources,1);
  Map<String,LocalizeResource> localizeResources=new HashMap<>();
  Map<String,String> extraEnv=new HashMap<>(SparkPackageUtils.getSparkClientEnv());
  SparkPackageUtils.prepareSparkResources(sparkCompat,locationFactory,tempDir,localizeResources,extraEnv);
  extraEnv.put(Constants.SPARK_COMPAT_ENV,sparkCompat.getCompat());
  launchConfig.addExtraResources(localizeResources).addExtraDependencies(SparkProgramRuntimeProvider.class).addExtraEnv(extraEnv).setClassAcceptor(createBundlerClassAcceptor());
}"
5007,"/** 
 * Creates the list of arguments that will be used for calling   {@link SparkSubmit#main(String[])}.
 * @param spec the {@link SparkSpecification} of the program
 * @param configs set of Spark configurations
 * @param resources list of resources that needs to be localized to Spark containers
 * @param jobJar the job jar file for Spark
 * @return a list of arguments
 */
private List<String> createSubmitArguments(SparkSpecification spec,Map<String,String> configs,List<LocalizeResource> resources,File jobJar){
  ImmutableList.Builder<String> builder=ImmutableList.builder();
  builder.add(""String_Node_Str"").add(getMaster(configs));
  builder.add(""String_Node_Str"").add(SparkMainWrapper.class.getName());
  builder.add(""String_Node_Str"").add(""String_Node_Str"" + spec.getName());
  for (  Map.Entry<String,String> entry : configs.entrySet()) {
    builder.add(""String_Node_Str"").add(entry.getKey() + ""String_Node_Str"" + entry.getValue());
  }
  for (  Map.Entry<String,String> entry : getSubmitConf().entrySet()) {
    builder.add(""String_Node_Str"").add(entry.getKey() + ""String_Node_Str"" + entry.getValue());
  }
  String archives=Joiner.on(',').join(Iterables.transform(Iterables.filter(resources,ARCHIVE_FILTER),RESOURCE_TO_PATH));
  String files=Joiner.on(',').join(Iterables.transform(Iterables.filter(resources,Predicates.not(ARCHIVE_FILTER)),RESOURCE_TO_PATH));
  if (!archives.isEmpty()) {
    builder.add(""String_Node_Str"").add(archives);
  }
  if (!files.isEmpty()) {
    builder.add(""String_Node_Str"").add(files);
  }
  return builder.add(jobJar.getAbsolutePath()).add(""String_Node_Str"" + SparkMainWrapper.ARG_USER_CLASS() + ""String_Node_Str""+ spec.getMainClassName()).build();
}","/** 
 * Creates the list of arguments that will be used for calling   {@link SparkSubmit#main(String[])}.
 * @param spec the {@link SparkSpecification} of the program
 * @param configs set of Spark configurations
 * @param resources list of resources that needs to be localized to Spark containers
 * @param jobJar the job jar file for Spark
 * @return a list of arguments
 */
private List<String> createSubmitArguments(SparkSpecification spec,Map<String,String> configs,List<LocalizeResource> resources,File jobJar){
  ImmutableList.Builder<String> builder=ImmutableList.builder();
  addMaster(configs,builder);
  builder.add(""String_Node_Str"").add(SparkMainWrapper.class.getName());
  builder.add(""String_Node_Str"").add(""String_Node_Str"" + spec.getName());
  for (  Map.Entry<String,String> entry : configs.entrySet()) {
    builder.add(""String_Node_Str"").add(entry.getKey() + ""String_Node_Str"" + entry.getValue());
  }
  for (  Map.Entry<String,String> entry : getSubmitConf().entrySet()) {
    builder.add(""String_Node_Str"").add(entry.getKey() + ""String_Node_Str"" + entry.getValue());
  }
  String archives=Joiner.on(',').join(Iterables.transform(Iterables.filter(resources,ARCHIVE_FILTER),RESOURCE_TO_PATH));
  String files=Joiner.on(',').join(Iterables.transform(Iterables.filter(resources,Predicates.not(ARCHIVE_FILTER)),RESOURCE_TO_PATH));
  if (!archives.isEmpty()) {
    builder.add(""String_Node_Str"").add(archives);
  }
  if (!files.isEmpty()) {
    builder.add(""String_Node_Str"").add(files);
  }
  return builder.add(jobJar.getAbsolutePath()).add(""String_Node_Str"" + SparkMainWrapper.ARG_USER_CLASS() + ""String_Node_Str""+ spec.getMainClassName()).build();
}"
5008,"/** 
 * Create the given streams and the Hive tables for the streams if explore is enabled.
 * @param namespaceId the namespace to have the stream created in
 * @param streamSpecs the set of stream specifications for streams to be created
 * @param ownerPrincipal the principal of the stream owner if one exists else null
 * @throws Exception if there was an exception creating a stream
 */
void createStreams(NamespaceId namespaceId,Iterable<StreamSpecification> streamSpecs,@Nullable KerberosPrincipalId ownerPrincipal) throws Exception {
  for (  StreamSpecification spec : streamSpecs) {
    Properties props=new Properties();
    if (spec.getDescription() != null) {
      props.put(Constants.Stream.DESCRIPTION,spec.getDescription());
    }
    if (ownerPrincipal != null) {
      props.put(Constants.Security.PRINCIPAL,ownerPrincipal.getPrincipal());
    }
    streamAdmin.create(namespaceId.stream(spec.getName()),props);
  }
}","/** 
 * Create the given streams and the Hive tables for the streams if explore is enabled.
 * @param namespaceId the namespace to have the stream created in
 * @param streamSpecs the set of stream specifications for streams to be created
 * @param ownerPrincipal the principal of the stream owner if one exists else null
 * @throws Exception if there was an exception creating a stream
 */
void createStreams(NamespaceId namespaceId,Iterable<StreamSpecification> streamSpecs,@Nullable KerberosPrincipalId ownerPrincipal) throws Exception {
  for (  StreamSpecification spec : streamSpecs) {
    Properties props=new Properties();
    if (spec.getDescription() != null) {
      props.put(Constants.Stream.DESCRIPTION,spec.getDescription());
    }
    if (ownerPrincipal != null) {
      props.put(Constants.Security.PRINCIPAL,ownerPrincipal.getPrincipal());
    }
    if (streamAdmin.create(namespaceId.stream(spec.getName()),props) != null) {
      LOG.info(""String_Node_Str"",namespaceId.getNamespace(),spec.getName());
    }
  }
}"
5009,"@Override public void execute(JobExecutionContext context) throws JobExecutionException {
  LOG.debug(""String_Node_Str"",context.getJobDetail().getKey().toString(),context.getTrigger().getKey().toString());
  Trigger trigger=context.getTrigger();
  String key=trigger.getKey().getName();
  String[] parts=key.split(""String_Node_Str"");
  Preconditions.checkArgument(parts.length == 6,String.format(""String_Node_Str"",key,parts.length));
  String namespaceId=parts[0];
  String applicationId=parts[1];
  String appVersion=parts[2];
  String scheduleName=parts[5];
  LOG.debug(""String_Node_Str"",key);
  ImmutableMap.Builder<String,String> builder=ImmutableMap.builder();
  builder.put(ProgramOptionConstants.SCHEDULE_NAME,scheduleName);
  Map<String,String> userOverrides=ImmutableMap.of(ProgramOptionConstants.LOGICAL_START_TIME,Long.toString(context.getScheduledFireTime().getTime()));
  ScheduleId scheduleId=new ApplicationId(namespaceId,applicationId,appVersion).schedule(scheduleName);
  try {
    taskPublisher.publishNotification(Notification.Type.TIME,scheduleId,builder.build(),userOverrides);
  }
 catch (  Throwable t) {
    LOG.warn(""String_Node_Str"",scheduleId,t);
    throw new JobExecutionException(t.getMessage(),t.getCause(),false);
  }
}","@Override public void execute(JobExecutionContext context) throws JobExecutionException {
  LOG.debug(""String_Node_Str"",context.getJobDetail().getKey().toString(),context.getTrigger().getKey().toString());
  Trigger trigger=context.getTrigger();
  String key=trigger.getKey().getName();
  String[] parts=key.split(""String_Node_Str"");
  Preconditions.checkArgument(parts.length == 6,String.format(""String_Node_Str"",key,parts.length));
  String namespaceId=parts[0];
  String applicationId=parts[1];
  String appVersion=parts[2];
  String scheduleName=parts[5];
  ImmutableMap.Builder<String,String> builder=ImmutableMap.builder();
  builder.put(ProgramOptionConstants.SCHEDULE_NAME,scheduleName);
  Map<String,String> userOverrides=ImmutableMap.of(ProgramOptionConstants.LOGICAL_START_TIME,Long.toString(context.getScheduledFireTime().getTime()));
  ScheduleId scheduleId=new ApplicationId(namespaceId,applicationId,appVersion).schedule(scheduleName);
  try {
    taskPublisher.publishNotification(Notification.Type.TIME,scheduleId,builder.build(),userOverrides);
  }
 catch (  Throwable t) {
    LOG.warn(""String_Node_Str"",scheduleId,t);
    throw new JobExecutionException(t.getMessage(),t.getCause(),false);
  }
}"
5010,"private void alterExploreStream(StreamId stream,boolean enable,@Nullable FormatSpecification format){
  if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED)) {
    Preconditions.checkNotNull(exploreFacade,""String_Node_Str"");
    try {
      if (enable) {
        exploreFacade.enableExploreStream(stream,tableNaming.getTableName(stream),format);
      }
 else {
        exploreFacade.disableExploreStream(stream,tableNaming.getTableName(stream));
      }
    }
 catch (    Exception e) {
      String msg=String.format(""String_Node_Str"",enable,stream,e.getMessage());
      LOG.error(msg,e);
    }
  }
}","private void alterExploreStream(StreamId stream,boolean enable,@Nullable FormatSpecification format){
  if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED)) {
    Preconditions.checkNotNull(exploreFacade,""String_Node_Str"");
    try {
      if (enable) {
        exploreFacade.enableExploreStream(stream,tableNaming.getTableName(stream),format);
        LOG.info(""String_Node_Str"",stream.getStream());
      }
 else {
        exploreFacade.disableExploreStream(stream,tableNaming.getTableName(stream));
      }
    }
 catch (    Exception e) {
      String msg=String.format(""String_Node_Str"",enable,stream,e.getMessage());
      LOG.error(msg,e);
    }
  }
}"
5011,"@Override public FactTable getOrCreateFactTable(int resolution){
  String tableName=cConf.get(Constants.Metrics.METRICS_TABLE_PREFIX,Constants.Metrics.DEFAULT_METRIC_TABLE_PREFIX) + ""String_Node_Str"" + resolution;
  int ttl=cConf.getInt(Constants.Metrics.RETENTION_SECONDS + ""String_Node_Str"" + resolution+ ""String_Node_Str"",-1);
  TableProperties.Builder props=TableProperties.builder();
  if (ttl > 0 && resolution != Integer.MAX_VALUE) {
    props.setTTL(ttl);
  }
  props.setReadlessIncrementSupport(true);
  props.add(HBaseTableAdmin.PROPERTY_SPLITS,GSON.toJson(FactTable.getSplits(DefaultMetricStore.AGGREGATIONS.size())));
  MetricsTable table=getOrCreateMetricsTable(tableName,props.build());
  LOG.debug(""String_Node_Str"",tableName);
  return new FactTable(table,entityTable.get(),resolution,getRollTime(resolution));
}","@Override public FactTable getOrCreateFactTable(int resolution){
  String tableName=cConf.get(Constants.Metrics.METRICS_TABLE_PREFIX,Constants.Metrics.DEFAULT_METRIC_TABLE_PREFIX) + ""String_Node_Str"" + resolution;
  int ttl=cConf.getInt(Constants.Metrics.RETENTION_SECONDS + ""String_Node_Str"" + resolution+ ""String_Node_Str"",-1);
  TableProperties.Builder props=TableProperties.builder();
  if (ttl > 0 && resolution != Integer.MAX_VALUE) {
    props.setTTL(ttl);
  }
  props.setReadlessIncrementSupport(true);
  props.add(HBaseTableAdmin.PROPERTY_SPLITS,GSON.toJson(FactTable.getSplits(DefaultMetricStore.AGGREGATIONS.size())));
  MetricsTable table=getOrCreateMetricsTable(tableName,props.build());
  return new FactTable(table,entityTable.get(),resolution,getRollTime(resolution));
}"
5012,"@Override public MetricsConsumerMetaTable createConsumerMeta(){
  String tableName=cConf.get(Constants.Metrics.KAFKA_META_TABLE);
  MetricsTable table=getOrCreateMetricsTable(tableName,DatasetProperties.EMPTY);
  LOG.debug(""String_Node_Str"",tableName);
  return new MetricsConsumerMetaTable(table);
}","@Override public MetricsConsumerMetaTable createConsumerMeta(){
  String tableName=cConf.get(Constants.Metrics.KAFKA_META_TABLE);
  MetricsTable table=getOrCreateMetricsTable(tableName,DatasetProperties.EMPTY);
  return new MetricsConsumerMetaTable(table);
}"
5013,"@Override public void destroy(){
  WorkflowContext workflowContext=getContext();
  if (!workflowContext.getDataTracer(PostAction.PLUGIN_TYPE).isEnabled()) {
    BasicArguments arguments=new BasicArguments(workflowContext.getToken(),workflowContext.getRuntimeArguments());
    for (    Map.Entry<String,PostAction> endingActionEntry : postActions.entrySet()) {
      String name=endingActionEntry.getKey();
      PostAction action=endingActionEntry.getValue();
      StageInfo stageInfo=StageInfo.builder(name,PostAction.PLUGIN_TYPE).setStageLoggingEnabled(spec.isStageLoggingEnabled()).setProcessTimingEnabled(spec.isProcessTimingEnabled()).build();
      BatchActionContext context=new WorkflowBackedActionContext(workflowContext,workflowMetrics,stageInfo,arguments);
      try {
        action.run(context);
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",name,t);
      }
    }
  }
  ProgramStatus status=getContext().getState().getStatus();
  WRAPPERLOGGER.info(""String_Node_Str"",getContext().getApplicationSpecification().getName(),status == ProgramStatus.COMPLETED ? ""String_Node_Str"" : status.name().toLowerCase());
}","@Override public void destroy(){
  WorkflowContext workflowContext=getContext();
  if (!workflowContext.getDataTracer(PostAction.PLUGIN_TYPE).isEnabled()) {
    BasicArguments arguments=new BasicArguments(workflowContext.getToken(),workflowContext.getRuntimeArguments());
    for (    Map.Entry<String,PostAction> endingActionEntry : postActions.entrySet()) {
      String name=endingActionEntry.getKey();
      PostAction action=endingActionEntry.getValue();
      StageInfo stageInfo=StageInfo.builder(name,PostAction.PLUGIN_TYPE).setStageLoggingEnabled(spec.isStageLoggingEnabled()).setProcessTimingEnabled(spec.isProcessTimingEnabled()).build();
      BatchActionContext context=new WorkflowBackedActionContext(workflowContext,workflowMetrics,stageInfo,arguments);
      try {
        action.run(context);
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"",name,t);
      }
    }
  }
  ProgramStatus status=getContext().getState().getStatus();
  if (status == ProgramStatus.FAILED) {
    WRAPPERLOGGER.error(""String_Node_Str"",getContext().getApplicationSpecification().getName());
  }
 else {
    WRAPPERLOGGER.info(""String_Node_Str"",getContext().getApplicationSpecification().getName(),status == ProgramStatus.COMPLETED ? ""String_Node_Str"" : status.name().toLowerCase());
  }
}"
5014,"public void process(KeyValue<String,Object> value){
  try {
    if (removeStageName) {
      transformation.transform(value.getValue(),emitter);
    }
 else {
      transformation.transform(value,emitter);
    }
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
}","public void process(KeyValue<String,Object> value){
  try {
    if (removeStageName) {
      transformation.transform(value.getValue(),emitter);
    }
 else {
      transformation.transform(value,emitter);
    }
  }
 catch (  StageFailureException e) {
    throw e;
  }
catch (  Exception e) {
    Throwable rootCause=Throwables.getRootCause(e);
    throw new StageFailureException(String.format(""String_Node_Str"" + ""String_Node_Str"",stageName,rootCause.getMessage()),rootCause);
  }
}"
5015,"@Override protected void reduce(Object key,Iterable values,Context context) throws IOException, InterruptedException {
  try {
    transformRunner.transform(key,values.iterator());
  }
 catch (  Exception e) {
    Throwables.propagate(e);
  }
}","@Override protected void reduce(Object key,Iterable values,Context context) throws IOException, InterruptedException {
  try {
    transformRunner.transform(key,values.iterator());
  }
 catch (  StageFailureException e) {
    PIPELINE_LOG.error(""String_Node_Str"",e.getMessage(),e.getCause());
    Throwables.propagate(e.getCause());
  }
catch (  Exception e) {
    Throwables.propagate(e);
  }
}"
5016,"@Override public void map(Object key,Object value,Mapper.Context context) throws IOException, InterruptedException {
  try {
    transformRunner.transform(key,value);
  }
 catch (  Exception e) {
    Throwables.propagate(e);
  }
}","@Override public void map(Object key,Object value,Mapper.Context context) throws IOException, InterruptedException {
  try {
    transformRunner.transform(key,value);
  }
 catch (  StageFailureException e) {
    PIPELINE_LOG.error(""String_Node_Str"",e.getMessage(),e.getCause());
    Throwables.propagate(e.getCause());
  }
catch (  Exception e) {
    Throwables.propagate(e);
  }
}"
5017,"public void runOneIteration(IN input) throws Exception {
  for (  String stageName : startingPoints) {
    PipeTransformDetail transformDetail=transformDetailMap.get(stageName);
    try {
      transformDetail.process(new KeyValue<String,Object>(stageName,input));
    }
 catch (    Exception e) {
      PIPELINE_LOG.error(""String_Node_Str"" + ""String_Node_Str"",stageName,Throwables.getRootCause(e).getMessage(),Throwables.getRootCause(e));
      throw e;
    }
  }
}","public void runOneIteration(IN input){
  for (  String stageName : startingPoints) {
    PipeTransformDetail transformDetail=transformDetailMap.get(stageName);
    transformDetail.process(new KeyValue<String,Object>(stageName,input));
  }
}"
5018,"private void createLocation(NamespaceMeta namespaceMeta) throws IOException {
  NamespaceId namespaceId=namespaceMeta.getNamespaceId();
  boolean createdHome=false;
  Location namespaceHome;
  if (hasCustomLocation(namespaceMeta)) {
    namespaceHome=validateCustomLocation(namespaceMeta);
  }
 else {
    namespaceHome=namespacedLocationFactory.get(namespaceMeta);
    if (namespaceHome.exists()) {
      throw new FileAlreadyExistsException(namespaceHome.toString());
    }
    createdHome=createNamespaceDir(namespaceHome,""String_Node_Str"",namespaceId);
  }
  Location dataLoc=namespaceHome.append(Constants.Dataset.DEFAULT_DATA_DIR);
  Location tempLoc=namespaceHome.append(cConf.get(Constants.AppFabric.TEMP_DIR));
  Location streamsLoc=namespaceHome.append(cConf.get(Constants.Stream.BASE_DIR));
  Location deletedLoc=streamsLoc.append(StreamUtils.DELETED);
  String configuredGroupName=namespaceMeta.getConfig().getGroupName();
  boolean createdData=false;
  boolean createdTemp=false;
  boolean createdStreams=false;
  try {
    if (createdHome && SecurityUtil.isKerberosEnabled(cConf)) {
      String groupToSet=configuredGroupName;
      if (groupToSet == null) {
        String[] groups=UserGroupInformation.getCurrentUser().getGroupNames();
        if (groups != null && groups.length > 0) {
          groupToSet=groups[0];
        }
      }
      if (groupToSet != null) {
        namespaceHome.setGroup(groupToSet);
      }
    }
    createdData=createNamespaceDir(dataLoc,""String_Node_Str"",namespaceId);
    createdTemp=createNamespaceDir(tempLoc,""String_Node_Str"",namespaceId);
    createdStreams=createNamespaceDir(streamsLoc,""String_Node_Str"",namespaceId);
    createNamespaceDir(deletedLoc,""String_Node_Str"",namespaceId);
    if (SecurityUtil.isKerberosEnabled(cConf)) {
      String groupToSet=configuredGroupName != null ? configuredGroupName : namespaceHome.getGroup();
      for (      Location loc : new Location[]{dataLoc,tempLoc,streamsLoc,deletedLoc}) {
        loc.setGroup(groupToSet);
        if (configuredGroupName != null) {
          String permissions=loc.getPermissions();
          loc.setPermissions(permissions.substring(0,3) + ""String_Node_Str"" + permissions.substring(6));
        }
      }
    }
  }
 catch (  Throwable t) {
    if (createdHome) {
      deleteDirSilently(namespaceHome,t,""String_Node_Str"",namespaceMeta.getNamespaceId());
    }
 else {
      if (createdData) {
        deleteDirSilently(dataLoc,t,""String_Node_Str"",namespaceMeta.getNamespaceId());
      }
      if (createdTemp) {
        deleteDirSilently(tempLoc,t,""String_Node_Str"",namespaceMeta.getNamespaceId());
      }
      if (createdStreams) {
        deleteDirSilently(streamsLoc,t,""String_Node_Str"",namespaceMeta.getNamespaceId());
      }
    }
    throw t;
  }
}","private void createLocation(NamespaceMeta namespaceMeta) throws IOException {
  NamespaceId namespaceId=namespaceMeta.getNamespaceId();
  boolean createdHome=false;
  Location namespaceHome;
  if (hasCustomLocation(namespaceMeta)) {
    namespaceHome=validateCustomLocation(namespaceMeta);
  }
 else {
    namespaceHome=namespacedLocationFactory.get(namespaceMeta);
    if (namespaceHome.exists()) {
      throw new FileAlreadyExistsException(null,null,String.format(""String_Node_Str"",namespaceHome,namespaceId));
    }
    createdHome=createNamespaceDir(namespaceHome,""String_Node_Str"",namespaceId);
  }
  Location dataLoc=namespaceHome.append(Constants.Dataset.DEFAULT_DATA_DIR);
  Location tempLoc=namespaceHome.append(cConf.get(Constants.AppFabric.TEMP_DIR));
  Location streamsLoc=namespaceHome.append(cConf.get(Constants.Stream.BASE_DIR));
  Location deletedLoc=streamsLoc.append(StreamUtils.DELETED);
  String configuredGroupName=namespaceMeta.getConfig().getGroupName();
  boolean createdData=false;
  boolean createdTemp=false;
  boolean createdStreams=false;
  try {
    if (createdHome && SecurityUtil.isKerberosEnabled(cConf)) {
      String groupToSet=configuredGroupName;
      if (groupToSet == null) {
        String[] groups=UserGroupInformation.getCurrentUser().getGroupNames();
        if (groups != null && groups.length > 0) {
          groupToSet=groups[0];
        }
      }
      if (groupToSet != null) {
        namespaceHome.setGroup(groupToSet);
      }
    }
    createdData=createNamespaceDir(dataLoc,""String_Node_Str"",namespaceId);
    createdTemp=createNamespaceDir(tempLoc,""String_Node_Str"",namespaceId);
    createdStreams=createNamespaceDir(streamsLoc,""String_Node_Str"",namespaceId);
    createNamespaceDir(deletedLoc,""String_Node_Str"",namespaceId);
    if (SecurityUtil.isKerberosEnabled(cConf)) {
      String groupToSet=configuredGroupName != null ? configuredGroupName : namespaceHome.getGroup();
      for (      Location loc : new Location[]{dataLoc,tempLoc,streamsLoc,deletedLoc}) {
        loc.setGroup(groupToSet);
        if (configuredGroupName != null) {
          String permissions=loc.getPermissions();
          loc.setPermissions(permissions.substring(0,3) + ""String_Node_Str"" + permissions.substring(6));
        }
      }
    }
  }
 catch (  Throwable t) {
    if (createdHome) {
      deleteDirSilently(namespaceHome,t,""String_Node_Str"",namespaceMeta.getNamespaceId());
    }
 else {
      if (createdData) {
        deleteDirSilently(dataLoc,t,""String_Node_Str"",namespaceMeta.getNamespaceId());
      }
      if (createdTemp) {
        deleteDirSilently(tempLoc,t,""String_Node_Str"",namespaceMeta.getNamespaceId());
      }
      if (createdStreams) {
        deleteDirSilently(streamsLoc,t,""String_Node_Str"",namespaceMeta.getNamespaceId());
      }
    }
    throw t;
  }
}"
5019,"@Override public void create(NamespaceMeta namespaceMeta) throws IOException, ExploreException, SQLException {
  super.create(namespaceMeta);
  if (NamespaceId.DEFAULT.equals(namespaceMeta.getNamespaceId())) {
    return;
  }
  String hbaseNamespace=tableUtil.getHBaseNamespace(namespaceMeta);
  if (Strings.isNullOrEmpty(namespaceMeta.getConfig().getHbaseNamespace())) {
    try (HBaseDDLExecutor executor=hBaseDDLExecutorFactory.get()){
      boolean created=executor.createNamespaceIfNotExists(hbaseNamespace);
      if (namespaceMeta.getConfig().getGroupName() != null) {
        try {
          executor.grantPermissions(hbaseNamespace,null,ImmutableMap.of(""String_Node_Str"" + namespaceMeta.getConfig().getGroupName(),""String_Node_Str""));
        }
 catch (        IOException|RuntimeException e) {
          if (created) {
            try {
              executor.deleteNamespaceIfExists(hbaseNamespace);
            }
 catch (            Throwable t) {
              e.addSuppressed(t);
            }
          }
          throw e;
        }
      }
    }
 catch (    Throwable t) {
      try {
        super.delete(namespaceMeta.getNamespaceId());
      }
 catch (      Exception e) {
        t.addSuppressed(e);
      }
      throw t;
    }
  }
  try (HBaseAdmin admin=new HBaseAdmin(hConf)){
    if (!tableUtil.hasNamespace(admin,hbaseNamespace)) {
      throw new IOException(String.format(""String_Node_Str"",hbaseNamespace,namespaceMeta.getName()));
    }
  }
 }","@Override public void create(NamespaceMeta namespaceMeta) throws IOException, ExploreException, SQLException {
  super.create(namespaceMeta);
  if (NamespaceId.DEFAULT.equals(namespaceMeta.getNamespaceId())) {
    return;
  }
  String hbaseNamespace=tableUtil.getHBaseNamespace(namespaceMeta);
  if (Strings.isNullOrEmpty(namespaceMeta.getConfig().getHbaseNamespace())) {
    try (HBaseDDLExecutor executor=hBaseDDLExecutorFactory.get()){
      boolean created=executor.createNamespaceIfNotExists(hbaseNamespace);
      if (namespaceMeta.getConfig().getGroupName() != null) {
        try {
          executor.grantPermissions(hbaseNamespace,null,ImmutableMap.of(""String_Node_Str"" + namespaceMeta.getConfig().getGroupName(),""String_Node_Str""));
        }
 catch (        IOException|RuntimeException e) {
          if (created) {
            try {
              executor.deleteNamespaceIfExists(hbaseNamespace);
            }
 catch (            Throwable t) {
              e.addSuppressed(t);
            }
          }
          throw e;
        }
      }
    }
 catch (    Throwable t) {
      try {
        super.delete(namespaceMeta.getNamespaceId());
      }
 catch (      Exception e) {
        t.addSuppressed(e);
      }
      throw t;
    }
  }
  try (HBaseAdmin admin=new HBaseAdmin(hConf)){
    if (!tableUtil.hasNamespace(admin,hbaseNamespace)) {
      throw new IOException(String.format(""String_Node_Str"" + ""String_Node_Str"",hbaseNamespace,namespaceMeta.getName()));
    }
  }
 }"
5020,"@Override protected Entry computeNext(){
  if (closed || (!scanner.hasNext())) {
    return endOfData();
  }
  RawPayloadTableEntry entry=scanner.next();
  if (skipStartRow != null) {
    byte[] row=skipStartRow;
    skipStartRow=null;
    if (Bytes.equals(row,entry.getKey()) && !scanner.hasNext()) {
      return endOfData();
    }
    entry=scanner.next();
  }
  return new ImmutablePayloadTableEntry(entry.getKey(),entry.getValue());
}","@Override protected Entry computeNext(){
  if (closed || (!scanner.hasNext())) {
    return endOfData();
  }
  RawPayloadTableEntry entry=scanner.next();
  if (skipFirstRow) {
    skipFirstRow=false;
    if (!scanner.hasNext()) {
      return endOfData();
    }
    entry=scanner.next();
  }
  return new ImmutablePayloadTableEntry(entry.getKey(),entry.getValue());
}"
5021,"@Override public CloseableIterator<Entry> fetch(TopicMetadata metadata,long transactionWritePointer,MessageId messageId,final boolean inclusive,int limit) throws IOException {
  byte[] topic=MessagingUtils.toDataKeyPrefix(metadata.getTopicId(),metadata.getGeneration());
  final byte[] startRow=new byte[topic.length + (2 * Bytes.SIZEOF_LONG) + Bytes.SIZEOF_SHORT];
  byte[] stopRow=new byte[topic.length + Bytes.SIZEOF_LONG];
  Bytes.putBytes(startRow,0,topic,0,topic.length);
  Bytes.putBytes(stopRow,0,topic,0,topic.length);
  Bytes.putLong(startRow,topic.length,transactionWritePointer);
  Bytes.putLong(stopRow,topic.length,transactionWritePointer);
  Bytes.putLong(startRow,topic.length + Bytes.SIZEOF_LONG,messageId.getPayloadWriteTimestamp());
  Bytes.putShort(startRow,topic.length + (2 * Bytes.SIZEOF_LONG),messageId.getPayloadSequenceId());
  stopRow=Bytes.stopKeyForPrefix(stopRow);
  final CloseableIterator<RawPayloadTableEntry> scanner=read(startRow,stopRow,limit);
  return new AbstractCloseableIterator<Entry>(){
    private boolean closed=false;
    private byte[] skipStartRow=inclusive ? null : startRow;
    @Override protected Entry computeNext(){
      if (closed || (!scanner.hasNext())) {
        return endOfData();
      }
      RawPayloadTableEntry entry=scanner.next();
      if (skipStartRow != null) {
        byte[] row=skipStartRow;
        skipStartRow=null;
        if (Bytes.equals(row,entry.getKey()) && !scanner.hasNext()) {
          return endOfData();
        }
        entry=scanner.next();
      }
      return new ImmutablePayloadTableEntry(entry.getKey(),entry.getValue());
    }
    @Override public void close(){
      try {
        scanner.close();
      }
  finally {
        endOfData();
        closed=true;
      }
    }
  }
;
}","@Override public CloseableIterator<Entry> fetch(TopicMetadata metadata,long transactionWritePointer,MessageId messageId,final boolean inclusive,int limit) throws IOException {
  byte[] topic=MessagingUtils.toDataKeyPrefix(metadata.getTopicId(),metadata.getGeneration());
  final byte[] startRow=new byte[topic.length + (2 * Bytes.SIZEOF_LONG) + Bytes.SIZEOF_SHORT];
  byte[] stopRow=new byte[topic.length + Bytes.SIZEOF_LONG];
  Bytes.putBytes(startRow,0,topic,0,topic.length);
  Bytes.putBytes(stopRow,0,topic,0,topic.length);
  Bytes.putLong(startRow,topic.length,transactionWritePointer);
  Bytes.putLong(stopRow,topic.length,transactionWritePointer);
  Bytes.putLong(startRow,topic.length + Bytes.SIZEOF_LONG,messageId.getPayloadWriteTimestamp());
  Bytes.putShort(startRow,topic.length + (2 * Bytes.SIZEOF_LONG),messageId.getPayloadSequenceId());
  stopRow=Bytes.stopKeyForPrefix(stopRow);
  final CloseableIterator<RawPayloadTableEntry> scanner=read(startRow,stopRow,limit);
  return new AbstractCloseableIterator<Entry>(){
    private boolean closed=false;
    private boolean skipFirstRow=!inclusive;
    @Override protected Entry computeNext(){
      if (closed || (!scanner.hasNext())) {
        return endOfData();
      }
      RawPayloadTableEntry entry=scanner.next();
      if (skipFirstRow) {
        skipFirstRow=false;
        if (!scanner.hasNext()) {
          return endOfData();
        }
        entry=scanner.next();
      }
      return new ImmutablePayloadTableEntry(entry.getKey(),entry.getValue());
    }
    @Override public void close(){
      try {
        scanner.close();
      }
  finally {
        endOfData();
        closed=true;
      }
    }
  }
;
}"
5022,"@Test public void testSingleMessage() throws Exception {
  TopicId topicId=NamespaceId.DEFAULT.topic(""String_Node_Str"");
  TopicMetadata metadata=new TopicMetadata(topicId,DEFAULT_PROPERTY);
  String payload=""String_Node_Str"";
  long txWritePtr=123L;
  try (MetadataTable metadataTable=getMetadataTable();PayloadTable table=getPayloadTable()){
    metadataTable.createTopic(metadata);
    List<PayloadTable.Entry> entryList=new ArrayList<>();
    entryList.add(new TestPayloadEntry(topicId,GENERATION,txWritePtr,0L,(short)0,Bytes.toBytes(payload)));
    table.store(entryList.iterator());
    byte[] messageId=new byte[MessageId.RAW_ID_SIZE];
    MessageId.putRawId(0L,(short)0,0L,(short)0,messageId,0);
    try (CloseableIterator<PayloadTable.Entry> iterator=table.fetch(metadata,txWritePtr,new MessageId(messageId),false,Integer.MAX_VALUE)){
      Assert.assertFalse(iterator.hasNext());
    }
     try (CloseableIterator<PayloadTable.Entry> iterator=table.fetch(metadata,txWritePtr,new MessageId(messageId),true,Integer.MAX_VALUE)){
      Assert.assertTrue(iterator.hasNext());
      PayloadTable.Entry entry=iterator.next();
      Assert.assertArrayEquals(Bytes.toBytes(payload),entry.getPayload());
      Assert.assertEquals(txWritePtr,entry.getTransactionWritePointer());
      Assert.assertFalse(iterator.hasNext());
    }
   }
 }","@Test public void testSingleMessage() throws Exception {
  TopicId topicId=NamespaceId.DEFAULT.topic(""String_Node_Str"");
  TopicMetadata metadata=new TopicMetadata(topicId,DEFAULT_PROPERTY);
  String payload=""String_Node_Str"";
  long txWritePtr=123L;
  try (MetadataTable metadataTable=getMetadataTable();PayloadTable table=getPayloadTable()){
    metadataTable.createTopic(metadata);
    List<PayloadTable.Entry> entryList=new ArrayList<>();
    entryList.add(new TestPayloadEntry(topicId,GENERATION,txWritePtr,1L,(short)1,Bytes.toBytes(payload)));
    table.store(entryList.iterator());
    byte[] messageId=new byte[MessageId.RAW_ID_SIZE];
    MessageId.putRawId(0L,(short)0,0L,(short)0,messageId,0);
    try (CloseableIterator<PayloadTable.Entry> iterator=table.fetch(metadata,txWritePtr,new MessageId(messageId),false,Integer.MAX_VALUE)){
      Assert.assertFalse(iterator.hasNext());
    }
     try (CloseableIterator<PayloadTable.Entry> iterator=table.fetch(metadata,txWritePtr,new MessageId(messageId),true,Integer.MAX_VALUE)){
      Assert.assertTrue(iterator.hasNext());
      PayloadTable.Entry entry=iterator.next();
      Assert.assertArrayEquals(Bytes.toBytes(payload),entry.getPayload());
      Assert.assertEquals(txWritePtr,entry.getTransactionWritePointer());
      Assert.assertFalse(iterator.hasNext());
    }
   }
 }"
5023,"/** 
 * Returns a singleton instance of the transaction state cache, performing lazy initialization if necessary.
 * @return A shared instance of the transaction state cache.
 */
@Override public TransactionStateCache get(){
  if (instance == null) {
synchronized (lock) {
      if (instance == null) {
        instance=new DefaultTransactionStateCache(sysConfigTablePrefix);
        instance.setConf(conf);
        instance.start();
      }
    }
  }
  return instance;
}","@Override public TransactionStateCache get(){
  TransactionStateCache cache=new DefaultTransactionStateCache(sysConfigTablePrefix);
  cache.setConf(conf);
  return cache;
}"
5024,"public DefaultTransactionStateCacheSupplier(String sysConfigTablePrefix,Configuration conf){
  super(conf);
  this.sysConfigTablePrefix=sysConfigTablePrefix;
}","public DefaultTransactionStateCacheSupplier(final String sysConfigTablePrefix,final Configuration conf){
  super(new Supplier<TransactionStateCache>(){
    @Override public TransactionStateCache get(){
      TransactionStateCache cache=new DefaultTransactionStateCache(sysConfigTablePrefix);
      cache.setConf(conf);
      return cache;
    }
  }
);
}"
5025,"@Override protected Supplier<TransactionStateCache> getTransactionStateCacheSupplier(RegionCoprocessorEnvironment env){
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,env.getConfiguration());
}","@Override protected CacheSupplier<TransactionStateCache> getTransactionStateCacheSupplier(RegionCoprocessorEnvironment env){
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,env.getConfiguration());
}"
5026,"@Override protected Supplier<TransactionStateCache> getTransactionStateCacheSupplier(RegionCoprocessorEnvironment env){
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,env.getConfiguration());
}","@Override protected CacheSupplier<TransactionStateCache> getTransactionStateCacheSupplier(RegionCoprocessorEnvironment env){
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,env.getConfiguration());
}"
5027,"@Override protected Supplier<TransactionStateCache> getTransactionStateCacheSupplier(RegionCoprocessorEnvironment env){
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,env.getConfiguration());
}","@Override protected CacheSupplier<TransactionStateCache> getTransactionStateCacheSupplier(RegionCoprocessorEnvironment env){
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,env.getConfiguration());
}"
5028,"@Override protected Supplier<TransactionStateCache> getTransactionStateCacheSupplier(RegionCoprocessorEnvironment env){
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,env.getConfiguration());
}","@Override protected CacheSupplier<TransactionStateCache> getTransactionStateCacheSupplier(RegionCoprocessorEnvironment env){
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,env.getConfiguration());
}"
5029,"@Override protected Supplier<TransactionStateCache> getTransactionStateCacheSupplier(RegionCoprocessorEnvironment env){
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,env.getConfiguration());
}","@Override protected CacheSupplier<TransactionStateCache> getTransactionStateCacheSupplier(RegionCoprocessorEnvironment env){
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,env.getConfiguration());
}"
5030,"@Override protected Supplier<TransactionStateCache> getTransactionStateCacheSupplier(RegionCoprocessorEnvironment env){
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,env.getConfiguration());
}","@Override protected CacheSupplier<TransactionStateCache> getTransactionStateCacheSupplier(RegionCoprocessorEnvironment env){
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,env.getConfiguration());
}"
5031,"@Override protected Supplier<TransactionStateCache> getTransactionStateCacheSupplier(RegionCoprocessorEnvironment env){
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,env.getConfiguration());
}","@Override protected CacheSupplier<TransactionStateCache> getTransactionStateCacheSupplier(RegionCoprocessorEnvironment env){
  return new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,env.getConfiguration());
}"
5032,"@Test public void testOldGenCleanup() throws Exception {
  try (MetadataTable metadataTable=getMetadataTable();MessageTable messageTable=getMessageTable();PayloadTable payloadTable=getPayloadTable()){
    int txWritePtr=100;
    TopicId topicId=NamespaceId.DEFAULT.topic(""String_Node_Str"");
    TopicMetadata topic=new TopicMetadata(topicId,TopicMetadata.TTL_KEY,""String_Node_Str"",TopicMetadata.GENERATION_KEY,Integer.toString(GENERATION));
    metadataTable.createTopic(topic);
    List<MessageTable.Entry> entries=new ArrayList<>();
    List<PayloadTable.Entry> pentries=new ArrayList<>();
    byte[] messageId=new byte[MessageId.RAW_ID_SIZE];
    MessageId.putRawId(0L,(short)0,0L,(short)0,messageId,0);
    entries.add(new TestMessageEntry(topicId,GENERATION,""String_Node_Str"",txWritePtr,(short)0));
    pentries.add(new TestPayloadEntry(topicId,GENERATION,""String_Node_Str"",txWritePtr,(short)0));
    messageTable.store(entries.iterator());
    payloadTable.store(pentries.iterator());
    try (CloseableIterator<MessageTable.Entry> iterator=messageTable.fetch(topic,0,Integer.MAX_VALUE,null)){
      checkMessageEntry(iterator,txWritePtr);
    }
     try (CloseableIterator<PayloadTable.Entry> iterator=payloadTable.fetch(topic,txWritePtr,new MessageId(messageId),true,100)){
      checkPayloadEntry(iterator,txWritePtr);
    }
     forceFlushAndCompact(Table.MESSAGE);
    forceFlushAndCompact(Table.PAYLOAD);
    try (CloseableIterator<MessageTable.Entry> iterator=messageTable.fetch(topic,0,Integer.MAX_VALUE,null)){
      checkMessageEntry(iterator,txWritePtr);
    }
     try (CloseableIterator<PayloadTable.Entry> iterator=payloadTable.fetch(topic,txWritePtr,new MessageId(messageId),true,100)){
      checkPayloadEntry(iterator,txWritePtr);
    }
     metadataTable.deleteTopic(topicId);
    TimeUnit.SECONDS.sleep(1);
    forceFlushAndCompact(Table.MESSAGE);
    forceFlushAndCompact(Table.PAYLOAD);
    try (CloseableIterator<MessageTable.Entry> iterator=messageTable.fetch(topic,0,Integer.MAX_VALUE,null)){
      Assert.assertFalse(iterator.hasNext());
    }
     try (CloseableIterator<PayloadTable.Entry> iterator=payloadTable.fetch(topic,txWritePtr,new MessageId(messageId),true,100)){
      Assert.assertFalse(iterator.hasNext());
    }
   }
 }","@Test public void testOldGenCleanup() throws Exception {
  try (MetadataTable metadataTable=getMetadataTable();MessageTable messageTable=getMessageTable();PayloadTable payloadTable=getPayloadTable()){
    int txWritePtr=100;
    TopicId topicId=NamespaceId.DEFAULT.topic(""String_Node_Str"");
    TopicMetadata topic=new TopicMetadata(topicId,TopicMetadata.TTL_KEY,""String_Node_Str"",TopicMetadata.GENERATION_KEY,Integer.toString(GENERATION));
    metadataTable.createTopic(topic);
    List<MessageTable.Entry> entries=new ArrayList<>();
    List<PayloadTable.Entry> pentries=new ArrayList<>();
    byte[] messageId=new byte[MessageId.RAW_ID_SIZE];
    MessageId.putRawId(0L,(short)0,0L,(short)0,messageId,0);
    entries.add(new TestMessageEntry(topicId,GENERATION,""String_Node_Str"",txWritePtr,(short)0));
    pentries.add(new TestPayloadEntry(topicId,GENERATION,""String_Node_Str"",txWritePtr,(short)0));
    messageTable.store(entries.iterator());
    payloadTable.store(pentries.iterator());
    try (CloseableIterator<MessageTable.Entry> iterator=messageTable.fetch(topic,0,Integer.MAX_VALUE,null)){
      checkMessageEntry(iterator,txWritePtr);
    }
     try (CloseableIterator<PayloadTable.Entry> iterator=payloadTable.fetch(topic,txWritePtr,new MessageId(messageId),true,100)){
      checkPayloadEntry(iterator,txWritePtr);
    }
     forceFlushAndCompact(Table.MESSAGE);
    forceFlushAndCompact(Table.PAYLOAD);
    try (CloseableIterator<MessageTable.Entry> iterator=messageTable.fetch(topic,0,Integer.MAX_VALUE,null)){
      checkMessageEntry(iterator,txWritePtr);
    }
     try (CloseableIterator<PayloadTable.Entry> iterator=payloadTable.fetch(topic,txWritePtr,new MessageId(messageId),true,100)){
      checkPayloadEntry(iterator,txWritePtr);
    }
     metadataTable.deleteTopic(topicId);
    TimeUnit.SECONDS.sleep(3);
    forceFlushAndCompact(Table.MESSAGE);
    forceFlushAndCompact(Table.PAYLOAD);
    try (CloseableIterator<MessageTable.Entry> iterator=messageTable.fetch(topic,0,Integer.MAX_VALUE,null)){
      Assert.assertFalse(iterator.hasNext());
    }
     try (CloseableIterator<PayloadTable.Entry> iterator=payloadTable.fetch(topic,txWritePtr,new MessageId(messageId),true,100)){
      Assert.assertFalse(iterator.hasNext());
    }
   }
 }"
5033,"@POST @Path(""String_Node_Str"") public void listPrivileges(HttpRequest request,HttpResponder responder) throws Exception {
  Iterator<MethodArgument> arguments=parseArguments(request);
  Principal principal=deserializeNext(arguments);
  LOG.trace(""String_Node_Str"",principal);
  Set<Privilege> privileges=authorizer.listPrivileges(principal);
  LOG.debug(""String_Node_Str"",principal,privileges,authorizer);
  responder.sendJson(HttpResponseStatus.OK,privileges);
}","@POST @Path(""String_Node_Str"") public void listPrivileges(HttpRequest request,HttpResponder responder) throws Exception {
  Iterator<MethodArgument> arguments=parseArguments(request);
  Principal principal=deserializeNext(arguments);
  LOG.trace(""String_Node_Str"",principal);
  Set<Privilege> privileges=privilegesManager.listPrivileges(principal);
  LOG.debug(""String_Node_Str"",principal,privileges);
  responder.sendJson(HttpResponseStatus.OK,privileges);
}"
5034,"@POST @Path(""String_Node_Str"") public void revokeAll(HttpRequest request,HttpResponder responder) throws Exception {
  Iterator<MethodArgument> arguments=parseArguments(request);
  EntityId entityId=deserializeNext(arguments);
  LOG.trace(""String_Node_Str"",entityId);
  authorizer.revoke(entityId);
  LOG.debug(""String_Node_Str"",entityId);
  responder.sendStatus(HttpResponseStatus.OK);
}","@POST @Path(""String_Node_Str"") public void revokeAll(HttpRequest request,HttpResponder responder) throws Exception {
  Iterator<MethodArgument> arguments=parseArguments(request);
  EntityId entityId=deserializeNext(arguments);
  LOG.trace(""String_Node_Str"",entityId);
  privilegesManager.revoke(entityId);
  LOG.info(""String_Node_Str"",entityId);
  responder.sendStatus(HttpResponseStatus.OK);
}"
5035,"@POST @Path(""String_Node_Str"") public void enforce(HttpRequest request,HttpResponder responder) throws Exception {
  Iterator<MethodArgument> arguments=parseArguments(request);
  EntityId entityId=deserializeNext(arguments);
  Principal principal=deserializeNext(arguments);
  Action action=deserializeNext(arguments);
  LOG.trace(""String_Node_Str"",action,entityId,principal);
  authorizer.enforce(entityId,principal,action);
  responder.sendStatus(HttpResponseStatus.OK);
}","@POST @Path(""String_Node_Str"") public void enforce(HttpRequest request,HttpResponder responder) throws Exception {
  Iterator<MethodArgument> arguments=parseArguments(request);
  EntityId entityId=deserializeNext(arguments);
  Principal principal=deserializeNext(arguments);
  Action action=deserializeNext(arguments);
  LOG.debug(""String_Node_Str"",action,entityId,principal);
  authorizationEnforcer.enforce(entityId,principal,action);
  responder.sendStatus(HttpResponseStatus.OK);
}"
5036,"@POST @Path(""String_Node_Str"") public void revoke(HttpRequest request,HttpResponder responder) throws Exception {
  Iterator<MethodArgument> arguments=parseArguments(request);
  EntityId entityId=deserializeNext(arguments);
  Principal principal=deserializeNext(arguments);
  Set<Action> actions=deserializeNext(arguments,SET_OF_ACTIONS);
  LOG.trace(""String_Node_Str"",actions,entityId,principal);
  authorizer.revoke(entityId,principal,actions);
  LOG.debug(""String_Node_Str"",actions,entityId,principal);
  responder.sendStatus(HttpResponseStatus.OK);
}","@POST @Path(""String_Node_Str"") public void revoke(HttpRequest request,HttpResponder responder) throws Exception {
  Iterator<MethodArgument> arguments=parseArguments(request);
  EntityId entityId=deserializeNext(arguments);
  Principal principal=deserializeNext(arguments);
  Set<Action> actions=deserializeNext(arguments,SET_OF_ACTIONS);
  LOG.trace(""String_Node_Str"",actions,entityId,principal);
  privilegesManager.revoke(entityId,principal,actions);
  LOG.info(""String_Node_Str"",actions,entityId,principal);
  responder.sendStatus(HttpResponseStatus.OK);
}"
5037,"@Inject RemotePrivilegesHandler(AuthorizerInstantiator authorizerInstantiator){
  this.authorizer=authorizerInstantiator.get();
}","@Inject RemotePrivilegesHandler(PrivilegesManager privilegesManager,AuthorizationEnforcer authorizationEnforcer){
  this.privilegesManager=privilegesManager;
  this.authorizationEnforcer=authorizationEnforcer;
}"
5038,"@POST @Path(""String_Node_Str"") public void grant(HttpRequest request,HttpResponder responder) throws Exception {
  Iterator<MethodArgument> arguments=parseArguments(request);
  EntityId entityId=deserializeNext(arguments);
  Principal principal=deserializeNext(arguments);
  Set<Action> actions=deserializeNext(arguments,SET_OF_ACTIONS);
  LOG.trace(""String_Node_Str"",actions,entityId,principal);
  authorizer.grant(entityId,principal,actions);
  LOG.debug(""String_Node_Str"",actions,entityId,principal);
  responder.sendStatus(HttpResponseStatus.OK);
}","@POST @Path(""String_Node_Str"") public void grant(HttpRequest request,HttpResponder responder) throws Exception {
  Iterator<MethodArgument> arguments=parseArguments(request);
  EntityId entityId=deserializeNext(arguments);
  Principal principal=deserializeNext(arguments);
  Set<Action> actions=deserializeNext(arguments,SET_OF_ACTIONS);
  LOG.debug(""String_Node_Str"",actions,entityId,principal);
  privilegesManager.grant(entityId,principal,actions);
  LOG.info(""String_Node_Str"",actions,entityId,principal);
  responder.sendStatus(HttpResponseStatus.OK);
}"
5039,"@Test public void testPrivilegesManager() throws Exception {
  privilegesManager.grant(NS,ALICE,EnumSet.allOf(Action.class));
  privilegesManager.grant(APP,ALICE,Collections.singleton(Action.ADMIN));
  privilegesManager.grant(PROGRAM,ALICE,Collections.singleton(Action.EXECUTE));
  authorizationEnforcer.enforce(NS,ALICE,EnumSet.allOf(Action.class));
  authorizationEnforcer.enforce(APP,ALICE,Action.ADMIN);
  authorizationEnforcer.enforce(PROGRAM,ALICE,Action.EXECUTE);
  try {
    authorizer.enforce(APP,ALICE,EnumSet.allOf(Action.class));
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  privilegesManager.revoke(PROGRAM);
  privilegesManager.revoke(APP,ALICE,EnumSet.allOf(Action.class));
  privilegesManager.revoke(NS,ALICE,EnumSet.allOf(Action.class));
  Set<Privilege> privileges=authorizer.listPrivileges(ALICE);
  Assert.assertTrue(String.format(""String_Node_Str"",privileges),privileges.isEmpty());
}","@Test public void testPrivilegesManager() throws Exception {
  privilegesManager.grant(NS,ALICE,EnumSet.allOf(Action.class));
  privilegesManager.grant(APP,ALICE,Collections.singleton(Action.ADMIN));
  privilegesManager.grant(PROGRAM,ALICE,Collections.singleton(Action.EXECUTE));
  authorizationEnforcer.enforce(NS,ALICE,EnumSet.allOf(Action.class));
  authorizationEnforcer.enforce(APP,ALICE,Action.ADMIN);
  authorizationEnforcer.enforce(PROGRAM,ALICE,Action.EXECUTE);
  try {
    authorizationEnforcer.enforce(APP,ALICE,EnumSet.allOf(Action.class));
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  privilegesManager.revoke(PROGRAM);
  privilegesManager.revoke(APP,ALICE,EnumSet.allOf(Action.class));
  privilegesManager.revoke(NS,ALICE,EnumSet.allOf(Action.class));
  Set<Privilege> privileges=privilegesManager.listPrivileges(ALICE);
  Assert.assertTrue(String.format(""String_Node_Str"",privileges),privileges.isEmpty());
}"
5040,"@BeforeClass public static void setup() throws IOException, InterruptedException {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TEMPORARY_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,true);
  cConf.setInt(Constants.Security.Authorization.CACHE_TTL_SECS,CACHE_TIMEOUT);
  Manifest manifest=new Manifest();
  manifest.getMainAttributes().put(Attributes.Name.MAIN_CLASS,InMemoryAuthorizer.class.getName());
  LocationFactory locationFactory=new LocalLocationFactory(TEMPORARY_FOLDER.newFolder());
  Location externalAuthJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class,manifest);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,externalAuthJar.toString());
  Injector injector=AppFabricTestHelper.getInjector(cConf);
  discoveryService=injector.getInstance(DiscoveryServiceClient.class);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  waitForService(Constants.Service.APP_FABRIC_HTTP);
  authorizationEnforcer=injector.getInstance(RemoteAuthorizationEnforcer.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  privilegesManager=injector.getInstance(PrivilegesManager.class);
}","@BeforeClass public static void setup() throws IOException, InterruptedException {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TEMPORARY_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,true);
  cConf.setInt(Constants.Security.Authorization.CACHE_TTL_SECS,CACHE_TIMEOUT);
  Manifest manifest=new Manifest();
  manifest.getMainAttributes().put(Attributes.Name.MAIN_CLASS,InMemoryAuthorizer.class.getName());
  LocationFactory locationFactory=new LocalLocationFactory(TEMPORARY_FOLDER.newFolder());
  Location externalAuthJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class,manifest);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,externalAuthJar.toString());
  Injector injector=AppFabricTestHelper.getInjector(cConf);
  discoveryService=injector.getInstance(DiscoveryServiceClient.class);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  waitForService(Constants.Service.APP_FABRIC_HTTP);
  authorizationEnforcer=injector.getInstance(RemoteAuthorizationEnforcer.class);
  privilegesManager=injector.getInstance(PrivilegesManager.class);
}"
5041,"@Test public void test() throws Exception {
  final Principal systemUser=new Principal(UserGroupInformation.getCurrentUser().getShortUserName(),Principal.PrincipalType.USER);
  Predicate<EntityId> systemUserFilter=authorizationEnforcer.createFilter(systemUser);
  Predicate<EntityId> adminUserFilter=authorizationEnforcer.createFilter(ADMIN_USER);
  Assert.assertFalse(systemUserFilter.apply(instanceId));
  Assert.assertFalse(systemUserFilter.apply(NamespaceId.SYSTEM));
  Assert.assertFalse(adminUserFilter.apply(NamespaceId.DEFAULT));
  authorizationBootstrapper.run();
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      Predicate<EntityId> systemUserFilter=authorizationEnforcer.createFilter(systemUser);
      Predicate<EntityId> adminUserFilter=authorizationEnforcer.createFilter(ADMIN_USER);
      return systemUserFilter.apply(instanceId) && systemUserFilter.apply(NamespaceId.SYSTEM) && adminUserFilter.apply(NamespaceId.DEFAULT);
    }
  }
,10,TimeUnit.SECONDS);
  txManager.startAndWait();
  datasetService.startAndWait();
  waitForService(Constants.Service.DATASET_MANAGER);
  defaultNamespaceEnsurer.startAndWait();
  systemArtifactLoader.startAndWait();
  waitForService(defaultNamespaceEnsurer);
  waitForService(systemArtifactLoader);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      try {
        return namespaceQueryAdmin.exists(NamespaceId.DEFAULT);
      }
 catch (      Exception e) {
        return false;
      }
    }
  }
,10,TimeUnit.SECONDS);
  Assert.assertTrue(defaultNamespaceEnsurer.isRunning());
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      try {
        artifactRepository.getArtifact(SYSTEM_ARTIFACT.toId());
        return true;
      }
 catch (      Exception e) {
        return false;
      }
    }
  }
,20,TimeUnit.SECONDS);
  Assert.assertTrue(systemArtifactLoader.isRunning());
  Dataset systemDataset=DatasetsUtil.getOrCreateDataset(dsFramework,NamespaceId.SYSTEM.dataset(""String_Node_Str""),Table.class.getName(),DatasetProperties.EMPTY,Collections.<String,String>emptyMap(),this.getClass().getClassLoader());
  Assert.assertNotNull(systemDataset);
  SecurityRequestContext.setUserId(ADMIN_USER.getName());
  namespaceAdmin.create(new NamespaceMeta.Builder().setName(""String_Node_Str"").build());
  SecurityRequestContext.setUserId(""String_Node_Str"");
  try {
    namespaceAdmin.create(new NamespaceMeta.Builder().setName(""String_Node_Str"").build());
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
}","@Test public void test() throws Exception {
  final Principal systemUser=new Principal(UserGroupInformation.getCurrentUser().getShortUserName(),Principal.PrincipalType.USER);
  Predicate<EntityId> systemUserFilter=authorizationEnforcer.createFilter(systemUser);
  Predicate<EntityId> adminUserFilter=authorizationEnforcer.createFilter(ADMIN_USER);
  Assert.assertFalse(systemUserFilter.apply(instanceId));
  Assert.assertFalse(systemUserFilter.apply(NamespaceId.SYSTEM));
  Assert.assertFalse(adminUserFilter.apply(NamespaceId.DEFAULT));
  authorizationBootstrapper.run();
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      Predicate<EntityId> systemUserFilter=authorizationEnforcer.createFilter(systemUser);
      Predicate<EntityId> adminUserFilter=authorizationEnforcer.createFilter(ADMIN_USER);
      return systemUserFilter.apply(instanceId) && systemUserFilter.apply(NamespaceId.SYSTEM) && adminUserFilter.apply(NamespaceId.DEFAULT);
    }
  }
,10,TimeUnit.SECONDS);
  txManager.startAndWait();
  datasetService.startAndWait();
  waitForService(Constants.Service.DATASET_MANAGER);
  defaultNamespaceEnsurer.startAndWait();
  systemArtifactLoader.startAndWait();
  waitForService(defaultNamespaceEnsurer);
  waitForService(systemArtifactLoader);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      try {
        return namespaceQueryAdmin.exists(NamespaceId.DEFAULT);
      }
 catch (      Exception e) {
        return false;
      }
    }
  }
,10,TimeUnit.SECONDS);
  Assert.assertTrue(defaultNamespaceEnsurer.isRunning());
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      try {
        artifactRepository.getArtifact(SYSTEM_ARTIFACT.toId());
        return true;
      }
 catch (      Exception e) {
        return false;
      }
    }
  }
,20,TimeUnit.SECONDS);
  Assert.assertTrue(systemArtifactLoader.isRunning());
  Dataset systemDataset=DatasetsUtil.getOrCreateDataset(dsFramework,NamespaceId.SYSTEM.dataset(""String_Node_Str""),Table.class.getName(),DatasetProperties.EMPTY,Collections.<String,String>emptyMap());
  Assert.assertNotNull(systemDataset);
  SecurityRequestContext.setUserId(ADMIN_USER.getName());
  namespaceAdmin.create(new NamespaceMeta.Builder().setName(""String_Node_Str"").build());
  SecurityRequestContext.setUserId(""String_Node_Str"");
  try {
    namespaceAdmin.create(new NamespaceMeta.Builder().setName(""String_Node_Str"").build());
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
}"
5042,"private void addPluginsInRangeToMap(final NamespaceId namespace,List<Id.Artifact> parentArtifacts,Map<byte[],byte[]> columns,SortedMap<ArtifactDescriptor,PluginClass> plugins,@Nullable Predicate<co.cask.cdap.proto.id.ArtifactId> range,int limit){
  range=range != null ? range : new Predicate<co.cask.cdap.proto.id.ArtifactId>(){
    @Override public boolean apply(    co.cask.cdap.proto.id.ArtifactId input){
      return NamespaceId.SYSTEM.equals(input.getParent()) || input.getParent().equals(namespace);
    }
  }
;
  for (  Map.Entry<byte[],byte[]> column : columns.entrySet()) {
    if (limit != Integer.MAX_VALUE && limit == plugins.size()) {
      break;
    }
    ArtifactColumn artifactColumn=ArtifactColumn.parse(column.getKey());
    if (!range.apply(artifactColumn.artifactId.toEntityId())) {
      continue;
    }
    PluginData pluginData=GSON.fromJson(Bytes.toString(column.getValue()),PluginData.class);
    for (    Id.Artifact parentArtifactId : parentArtifacts) {
      if (pluginData.usableBy.versionIsInRange(parentArtifactId.getVersion())) {
        plugins.put(new ArtifactDescriptor(artifactColumn.artifactId.toArtifactId(),Locations.getLocationFromAbsolutePath(locationFactory,pluginData.getArtifactLocationPath())),pluginData.pluginClass);
        break;
      }
    }
  }
}","private void addPluginsInRangeToMap(final NamespaceId namespace,List<Id.Artifact> parentArtifacts,Map<byte[],byte[]> columns,SortedMap<ArtifactDescriptor,PluginClass> plugins,@Nullable Predicate<co.cask.cdap.proto.id.ArtifactId> range,int limit){
  range=range != null ? range : new Predicate<co.cask.cdap.proto.id.ArtifactId>(){
    @Override public boolean apply(    co.cask.cdap.proto.id.ArtifactId input){
      return NamespaceId.SYSTEM.equals(input.getParent()) || input.getParent().equals(namespace);
    }
  }
;
  for (  Map.Entry<byte[],byte[]> column : columns.entrySet()) {
    ArtifactColumn artifactColumn=ArtifactColumn.parse(column.getKey());
    if (!range.apply(artifactColumn.artifactId.toEntityId())) {
      continue;
    }
    PluginData pluginData=GSON.fromJson(Bytes.toString(column.getValue()),PluginData.class);
    for (    Id.Artifact parentArtifactId : parentArtifacts) {
      if (pluginData.usableBy.versionIsInRange(parentArtifactId.getVersion())) {
        plugins.put(new ArtifactDescriptor(artifactColumn.artifactId.toArtifactId(),Locations.getLocationFromAbsolutePath(locationFactory,pluginData.getArtifactLocationPath())),pluginData.pluginClass);
        break;
      }
    }
    if (limit < plugins.size()) {
      plugins.remove(plugins.lastKey());
    }
  }
}"
5043,"/** 
 * Get all plugin classes of the given type and name that extend the given parent artifact. Results are returned as a map from plugin artifact to plugins in that artifact.
 * @param parentArtifactRange the parent artifact range to find plugins for
 * @param type the type of plugin to look for
 * @param name the name of the plugin to look for
 * @param pluginRange the predicate for the plugins
 * @param limit the limit number of the result
 * @param order the order of the result
 * @return an unmodifiable map of plugin artifact to plugin classes of the given type and name, accessible by thegiven artifact. The map will never be null, and will never be empty.
 * @throws PluginNotExistsException if no plugin with the given type and name exists in the namespace
 * @throws IOException if there was an exception reading metadata from the metastore
 */
public SortedMap<ArtifactDescriptor,PluginClass> getPluginClasses(final NamespaceId namespace,final ArtifactRange parentArtifactRange,final String type,final String name,@Nullable final Predicate<co.cask.cdap.proto.id.ArtifactId> pluginRange,final int limit,final ArtifactSortOrder order) throws IOException, ArtifactNotFoundException, PluginNotExistsException {
  try {
    SortedMap<ArtifactDescriptor,PluginClass> result=Transactions.execute(transactional,new TxCallable<SortedMap<ArtifactDescriptor,PluginClass>>(){
      @Override public SortedMap<ArtifactDescriptor,PluginClass> call(      DatasetContext context) throws Exception {
        Table metaTable=getMetaTable(context);
        List<ArtifactDetail> parentArtifactDetails=getArtifacts(metaTable,parentArtifactRange,limit,null);
        if (parentArtifactDetails.isEmpty()) {
          throw new ArtifactNotFoundException(parentArtifactRange.getNamespace(),parentArtifactRange.getName());
        }
        SortedMap<ArtifactDescriptor,PluginClass> plugins=order == ArtifactSortOrder.DESC ? new TreeMap<ArtifactDescriptor,PluginClass>(Collections.<ArtifactDescriptor>reverseOrder()) : new TreeMap<ArtifactDescriptor,PluginClass>();
        List<Id.Artifact> parentArtifacts=new ArrayList<>();
        for (        ArtifactDetail parentArtifactDetail : parentArtifactDetails) {
          Id.Artifact parentArtifactId=Id.Artifact.from(namespace.toId(),parentArtifactDetail.getDescriptor().getArtifactId());
          parentArtifacts.add(parentArtifactId);
          Set<PluginClass> parentPlugins=parentArtifactDetail.getMeta().getClasses().getPlugins();
          for (          PluginClass pluginClass : parentPlugins) {
            if (pluginClass.getName().equals(name) && pluginClass.getType().equals(type)) {
              plugins.put(parentArtifactDetail.getDescriptor(),pluginClass);
              break;
            }
          }
        }
        PluginKey pluginKey=new PluginKey(parentArtifactRange.getNamespace().toId(),parentArtifactRange.getName(),type,name);
        Row row=metaTable.get(pluginKey.getRowKey());
        if (!row.isEmpty()) {
          addPluginsInRangeToMap(namespace,parentArtifacts,row.getColumns(),plugins,pluginRange,limit);
        }
        return Collections.unmodifiableSortedMap(plugins);
      }
    }
);
    if (result.isEmpty()) {
      throw new PluginNotExistsException(parentArtifactRange.getNamespace().toId(),type,name);
    }
    return result;
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,IOException.class,ArtifactNotFoundException.class);
  }
}","/** 
 * Get all plugin classes of the given type and name that extend the given parent artifact. Results are returned as a map from plugin artifact to plugins in that artifact.
 * @param parentArtifactRange the parent artifact range to find plugins for
 * @param type the type of plugin to look for
 * @param name the name of the plugin to look for
 * @param pluginRange the predicate for the plugins
 * @param limit the limit number of the result
 * @param order the order of the result
 * @return an unmodifiable map of plugin artifact to plugin classes of the given type and name, accessible by thegiven artifact. The map will never be null, and will never be empty.
 * @throws PluginNotExistsException if no plugin with the given type and name exists in the namespace
 * @throws IOException if there was an exception reading metadata from the metastore
 */
public SortedMap<ArtifactDescriptor,PluginClass> getPluginClasses(final NamespaceId namespace,final ArtifactRange parentArtifactRange,final String type,final String name,@Nullable final Predicate<co.cask.cdap.proto.id.ArtifactId> pluginRange,final int limit,final ArtifactSortOrder order) throws IOException, ArtifactNotFoundException, PluginNotExistsException {
  try {
    SortedMap<ArtifactDescriptor,PluginClass> result=Transactions.execute(transactional,new TxCallable<SortedMap<ArtifactDescriptor,PluginClass>>(){
      @Override public SortedMap<ArtifactDescriptor,PluginClass> call(      DatasetContext context) throws Exception {
        Table metaTable=getMetaTable(context);
        List<ArtifactDetail> parentArtifactDetails=getArtifacts(metaTable,parentArtifactRange,Integer.MAX_VALUE,null);
        if (parentArtifactDetails.isEmpty()) {
          throw new ArtifactNotFoundException(parentArtifactRange.getNamespace(),parentArtifactRange.getName());
        }
        SortedMap<ArtifactDescriptor,PluginClass> plugins=order == ArtifactSortOrder.DESC ? new TreeMap<ArtifactDescriptor,PluginClass>(Collections.<ArtifactDescriptor>reverseOrder()) : new TreeMap<ArtifactDescriptor,PluginClass>();
        List<Id.Artifact> parentArtifacts=new ArrayList<>();
        for (        ArtifactDetail parentArtifactDetail : parentArtifactDetails) {
          Id.Artifact parentArtifactId=Id.Artifact.from(namespace.toId(),parentArtifactDetail.getDescriptor().getArtifactId());
          parentArtifacts.add(parentArtifactId);
          Set<PluginClass> parentPlugins=parentArtifactDetail.getMeta().getClasses().getPlugins();
          for (          PluginClass pluginClass : parentPlugins) {
            if (pluginClass.getName().equals(name) && pluginClass.getType().equals(type)) {
              plugins.put(parentArtifactDetail.getDescriptor(),pluginClass);
              break;
            }
          }
        }
        PluginKey pluginKey=new PluginKey(parentArtifactRange.getNamespace().toId(),parentArtifactRange.getName(),type,name);
        Row row=metaTable.get(pluginKey.getRowKey());
        if (!row.isEmpty()) {
          addPluginsInRangeToMap(namespace,parentArtifacts,row.getColumns(),plugins,pluginRange,limit);
        }
        return Collections.unmodifiableSortedMap(plugins);
      }
    }
);
    if (result.isEmpty()) {
      throw new PluginNotExistsException(parentArtifactRange.getNamespace().toId(),type,name);
    }
    return result;
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,IOException.class,ArtifactNotFoundException.class);
  }
}"
5044,"@Test public void testGetPlugins() throws Exception {
  ArtifactRange parentArtifacts=new ArtifactRange(NamespaceId.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""));
  PluginClass pluginA1=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false),""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",false,false)));
  PluginClass pluginA2=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false)));
  PluginClass pluginB1=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",false,false)));
  PluginClass pluginB2=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false),""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false)));
  String contents=""String_Node_Str"";
  Id.Artifact parentArtifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta parentMeta=new ArtifactMeta(ArtifactClasses.builder().build());
  writeArtifact(parentArtifactId,parentMeta,contents);
  Id.Artifact artifactXv100=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaXv100=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginA1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactXv100,metaXv100,contents);
  ArtifactDescriptor artifactXv100Info=artifactStore.getArtifact(artifactXv100).getDescriptor();
  Id.Artifact artifactXv110=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaXv110=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginA1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactXv110,metaXv110,contents);
  ArtifactDescriptor artifactXv110Info=artifactStore.getArtifact(artifactXv110).getDescriptor();
  Id.Artifact artifactXv200=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaXv200=new ArtifactMeta(ArtifactClasses.builder().addPlugins(pluginA1,pluginA2).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactXv200,metaXv200,contents);
  ArtifactDescriptor artifactXv200Info=artifactStore.getArtifact(artifactXv200).getDescriptor();
  Id.Artifact artifactYv100=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaYv100=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginB1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactYv100,metaYv100,contents);
  ArtifactDescriptor artifactYv100Info=artifactStore.getArtifact(artifactYv100).getDescriptor();
  Id.Artifact artifactYv200=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaYv200=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginB2).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactYv200,metaYv200,contents);
  ArtifactDescriptor artifactYv200Info=artifactStore.getArtifact(artifactYv200).getDescriptor();
  Id.Artifact artifactZv100=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaZv100=new ArtifactMeta(ArtifactClasses.builder().addPlugins(pluginA1,pluginB1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactZv100,metaZv100,contents);
  ArtifactDescriptor artifactZv100Info=artifactStore.getArtifact(artifactZv100).getDescriptor();
  Id.Artifact artifactZv200=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaZv200=new ArtifactMeta(ArtifactClasses.builder().addPlugins(pluginA1,pluginA2,pluginB1,pluginB2).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactZv200,metaZv200,contents);
  ArtifactDescriptor artifactZv200Info=artifactStore.getArtifact(artifactZv200).getDescriptor();
  Map<ArtifactDescriptor,Set<PluginClass>> expected=Maps.newHashMap();
  expected.put(artifactXv100Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv110Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv200Info,ImmutableSet.of(pluginA1,pluginA2));
  expected.put(artifactYv100Info,ImmutableSet.of(pluginB1));
  expected.put(artifactYv200Info,ImmutableSet.of(pluginB2));
  expected.put(artifactZv100Info,ImmutableSet.of(pluginA1,pluginB1));
  expected.put(artifactZv200Info,ImmutableSet.of(pluginA1,pluginA2,pluginB1,pluginB2));
  Map<ArtifactDescriptor,Set<PluginClass>> actual=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId);
  Assert.assertEquals(expected,actual);
  expected=Maps.newHashMap();
  expected.put(artifactXv100Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv110Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv200Info,ImmutableSet.of(pluginA1,pluginA2));
  expected.put(artifactZv100Info,ImmutableSet.of(pluginA1));
  expected.put(artifactZv200Info,ImmutableSet.of(pluginA1,pluginA2));
  actual=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"");
  Assert.assertEquals(expected,actual);
  expected=Maps.newHashMap();
  expected.put(artifactYv100Info,ImmutableSet.of(pluginB1));
  expected.put(artifactYv200Info,ImmutableSet.of(pluginB2));
  expected.put(artifactZv100Info,ImmutableSet.of(pluginB1));
  expected.put(artifactZv200Info,ImmutableSet.of(pluginB1,pluginB2));
  actual=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"");
  Assert.assertEquals(expected,actual);
  Map<ArtifactDescriptor,PluginClass> expectedMap=Maps.newHashMap();
  expectedMap.put(artifactXv100Info,pluginA1);
  expectedMap.put(artifactXv110Info,pluginA1);
  expectedMap.put(artifactXv200Info,pluginA1);
  expectedMap.put(artifactZv100Info,pluginA1);
  expectedMap.put(artifactZv200Info,pluginA1);
  Map<ArtifactDescriptor,PluginClass> actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,1,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(ImmutableMap.of(artifactXv100Info,pluginA1),actualMap);
  actualMap=new TreeMap<>(artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.DESC));
  Assert.assertEquals(expectedMap,new TreeMap<>(actualMap).descendingMap());
  Predicate<ArtifactId> predicate=new Predicate<ArtifactId>(){
    @Override public boolean apply(    ArtifactId input){
      try {
        return input.getParent().equals(NamespaceId.DEFAULT) && input.getArtifact().equals(""String_Node_Str"") && ArtifactVersionRange.parse(""String_Node_Str"").versionIsInRange(new ArtifactVersion(input.getVersion()));
      }
 catch (      InvalidArtifactRangeException e) {
        return false;
      }
    }
  }
;
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactXv100Info,pluginA1);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",predicate,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactXv200Info,pluginA2);
  expectedMap.put(artifactZv200Info,pluginA2);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactYv100Info,pluginB1);
  expectedMap.put(artifactZv100Info,pluginB1);
  expectedMap.put(artifactZv200Info,pluginB1);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactYv200Info,pluginB2);
  expectedMap.put(artifactZv200Info,pluginB2);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
}","@Test public void testGetPlugins() throws Exception {
  ArtifactRange parentArtifacts=new ArtifactRange(NamespaceId.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""));
  PluginClass pluginA1=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false),""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",false,false)));
  PluginClass pluginA2=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false)));
  PluginClass pluginB1=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",false,false)));
  PluginClass pluginB2=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false),""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false)));
  String contents=""String_Node_Str"";
  Id.Artifact parentArtifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta parentMeta=new ArtifactMeta(ArtifactClasses.builder().build());
  writeArtifact(parentArtifactId,parentMeta,contents);
  Id.Artifact artifactXv100=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaXv100=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginA1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactXv100,metaXv100,contents);
  ArtifactDescriptor artifactXv100Info=artifactStore.getArtifact(artifactXv100).getDescriptor();
  Id.Artifact artifactXv110=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaXv110=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginA1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactXv110,metaXv110,contents);
  ArtifactDescriptor artifactXv110Info=artifactStore.getArtifact(artifactXv110).getDescriptor();
  Id.Artifact artifactXv200=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaXv200=new ArtifactMeta(ArtifactClasses.builder().addPlugins(pluginA1,pluginA2).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactXv200,metaXv200,contents);
  ArtifactDescriptor artifactXv200Info=artifactStore.getArtifact(artifactXv200).getDescriptor();
  Id.Artifact artifactYv100=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaYv100=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginB1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactYv100,metaYv100,contents);
  ArtifactDescriptor artifactYv100Info=artifactStore.getArtifact(artifactYv100).getDescriptor();
  Id.Artifact artifactYv200=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaYv200=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginB2).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactYv200,metaYv200,contents);
  ArtifactDescriptor artifactYv200Info=artifactStore.getArtifact(artifactYv200).getDescriptor();
  Id.Artifact artifactZv100=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaZv100=new ArtifactMeta(ArtifactClasses.builder().addPlugins(pluginA1,pluginB1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactZv100,metaZv100,contents);
  ArtifactDescriptor artifactZv100Info=artifactStore.getArtifact(artifactZv100).getDescriptor();
  Id.Artifact artifactZv200=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaZv200=new ArtifactMeta(ArtifactClasses.builder().addPlugins(pluginA1,pluginA2,pluginB1,pluginB2).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactZv200,metaZv200,contents);
  ArtifactDescriptor artifactZv200Info=artifactStore.getArtifact(artifactZv200).getDescriptor();
  Map<ArtifactDescriptor,Set<PluginClass>> expected=Maps.newHashMap();
  expected.put(artifactXv100Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv110Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv200Info,ImmutableSet.of(pluginA1,pluginA2));
  expected.put(artifactYv100Info,ImmutableSet.of(pluginB1));
  expected.put(artifactYv200Info,ImmutableSet.of(pluginB2));
  expected.put(artifactZv100Info,ImmutableSet.of(pluginA1,pluginB1));
  expected.put(artifactZv200Info,ImmutableSet.of(pluginA1,pluginA2,pluginB1,pluginB2));
  Map<ArtifactDescriptor,Set<PluginClass>> actual=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId);
  Assert.assertEquals(expected,actual);
  expected=Maps.newHashMap();
  expected.put(artifactXv100Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv110Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv200Info,ImmutableSet.of(pluginA1,pluginA2));
  expected.put(artifactZv100Info,ImmutableSet.of(pluginA1));
  expected.put(artifactZv200Info,ImmutableSet.of(pluginA1,pluginA2));
  actual=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"");
  Assert.assertEquals(expected,actual);
  expected=Maps.newHashMap();
  expected.put(artifactYv100Info,ImmutableSet.of(pluginB1));
  expected.put(artifactYv200Info,ImmutableSet.of(pluginB2));
  expected.put(artifactZv100Info,ImmutableSet.of(pluginB1));
  expected.put(artifactZv200Info,ImmutableSet.of(pluginB1,pluginB2));
  actual=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"");
  Assert.assertEquals(expected,actual);
  Map<ArtifactDescriptor,PluginClass> expectedMap=Maps.newHashMap();
  expectedMap.put(artifactXv100Info,pluginA1);
  expectedMap.put(artifactXv110Info,pluginA1);
  expectedMap.put(artifactXv200Info,pluginA1);
  expectedMap.put(artifactZv100Info,pluginA1);
  expectedMap.put(artifactZv200Info,pluginA1);
  Map<ArtifactDescriptor,PluginClass> actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,1,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(ImmutableMap.of(artifactXv100Info,pluginA1),actualMap);
  actualMap=new TreeMap<>(artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.DESC));
  Assert.assertEquals(expectedMap,new TreeMap<>(actualMap).descendingMap());
  Predicate<ArtifactId> predicate=new Predicate<ArtifactId>(){
    @Override public boolean apply(    ArtifactId input){
      try {
        return input.getParent().equals(NamespaceId.DEFAULT) && input.getArtifact().equals(""String_Node_Str"") && ArtifactVersionRange.parse(""String_Node_Str"").versionIsInRange(new ArtifactVersion(input.getVersion()));
      }
 catch (      InvalidArtifactRangeException e) {
        return false;
      }
    }
  }
;
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactXv100Info,pluginA1);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",predicate,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,1,ArtifactSortOrder.DESC);
  Assert.assertEquals(ImmutableMap.of(artifactZv200Info,pluginA1),actualMap);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",predicate,1,ArtifactSortOrder.DESC);
  Assert.assertEquals(ImmutableMap.of(artifactXv100Info,pluginA1),actualMap);
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactXv200Info,pluginA2);
  expectedMap.put(artifactZv200Info,pluginA2);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactYv100Info,pluginB1);
  expectedMap.put(artifactZv100Info,pluginB1);
  expectedMap.put(artifactZv200Info,pluginB1);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactYv200Info,pluginB2);
  expectedMap.put(artifactZv200Info,pluginB2);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
}"
5045,"@Test public void testGetPlugins() throws Exception {
  ArtifactRange parentArtifacts=new ArtifactRange(NamespaceId.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""));
  PluginClass pluginA1=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false),""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",false,false)));
  PluginClass pluginA2=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false)));
  PluginClass pluginB1=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",false,false)));
  PluginClass pluginB2=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false),""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false)));
  String contents=""String_Node_Str"";
  Id.Artifact parentArtifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta parentMeta=new ArtifactMeta(ArtifactClasses.builder().build());
  writeArtifact(parentArtifactId,parentMeta,contents);
  Id.Artifact artifactXv100=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaXv100=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginA1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactXv100,metaXv100,contents);
  ArtifactDescriptor artifactXv100Info=artifactStore.getArtifact(artifactXv100).getDescriptor();
  Id.Artifact artifactXv110=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaXv110=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginA1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactXv110,metaXv110,contents);
  ArtifactDescriptor artifactXv110Info=artifactStore.getArtifact(artifactXv110).getDescriptor();
  Id.Artifact artifactXv200=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaXv200=new ArtifactMeta(ArtifactClasses.builder().addPlugins(pluginA1,pluginA2).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactXv200,metaXv200,contents);
  ArtifactDescriptor artifactXv200Info=artifactStore.getArtifact(artifactXv200).getDescriptor();
  Id.Artifact artifactYv100=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaYv100=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginB1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactYv100,metaYv100,contents);
  ArtifactDescriptor artifactYv100Info=artifactStore.getArtifact(artifactYv100).getDescriptor();
  Id.Artifact artifactYv200=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaYv200=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginB2).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactYv200,metaYv200,contents);
  ArtifactDescriptor artifactYv200Info=artifactStore.getArtifact(artifactYv200).getDescriptor();
  Id.Artifact artifactZv100=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaZv100=new ArtifactMeta(ArtifactClasses.builder().addPlugins(pluginA1,pluginB1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactZv100,metaZv100,contents);
  ArtifactDescriptor artifactZv100Info=artifactStore.getArtifact(artifactZv100).getDescriptor();
  Id.Artifact artifactZv200=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaZv200=new ArtifactMeta(ArtifactClasses.builder().addPlugins(pluginA1,pluginA2,pluginB1,pluginB2).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactZv200,metaZv200,contents);
  ArtifactDescriptor artifactZv200Info=artifactStore.getArtifact(artifactZv200).getDescriptor();
  Map<ArtifactDescriptor,Set<PluginClass>> expected=Maps.newHashMap();
  expected.put(artifactXv100Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv110Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv200Info,ImmutableSet.of(pluginA1,pluginA2));
  expected.put(artifactYv100Info,ImmutableSet.of(pluginB1));
  expected.put(artifactYv200Info,ImmutableSet.of(pluginB2));
  expected.put(artifactZv100Info,ImmutableSet.of(pluginA1,pluginB1));
  expected.put(artifactZv200Info,ImmutableSet.of(pluginA1,pluginA2,pluginB1,pluginB2));
  Map<ArtifactDescriptor,Set<PluginClass>> actual=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId);
  Assert.assertEquals(expected,actual);
  expected=Maps.newHashMap();
  expected.put(artifactXv100Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv110Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv200Info,ImmutableSet.of(pluginA1,pluginA2));
  expected.put(artifactZv100Info,ImmutableSet.of(pluginA1));
  expected.put(artifactZv200Info,ImmutableSet.of(pluginA1,pluginA2));
  actual=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"");
  Assert.assertEquals(expected,actual);
  expected=Maps.newHashMap();
  expected.put(artifactYv100Info,ImmutableSet.of(pluginB1));
  expected.put(artifactYv200Info,ImmutableSet.of(pluginB2));
  expected.put(artifactZv100Info,ImmutableSet.of(pluginB1));
  expected.put(artifactZv200Info,ImmutableSet.of(pluginB1,pluginB2));
  actual=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"");
  Assert.assertEquals(expected,actual);
  Map<ArtifactDescriptor,PluginClass> expectedMap=Maps.newHashMap();
  expectedMap.put(artifactXv100Info,pluginA1);
  expectedMap.put(artifactXv110Info,pluginA1);
  expectedMap.put(artifactXv200Info,pluginA1);
  expectedMap.put(artifactZv100Info,pluginA1);
  expectedMap.put(artifactZv200Info,pluginA1);
  Map<ArtifactDescriptor,PluginClass> actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,1,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(ImmutableMap.of(artifactXv100Info,pluginA1),actualMap);
  actualMap=new TreeMap<>(artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.DESC));
  Assert.assertEquals(expectedMap,new TreeMap<>(actualMap).descendingMap());
  Predicate<ArtifactId> predicate=new Predicate<ArtifactId>(){
    @Override public boolean apply(    ArtifactId input){
      try {
        return input.getParent().equals(NamespaceId.DEFAULT) && input.getArtifact().equals(""String_Node_Str"") && ArtifactVersionRange.parse(""String_Node_Str"").versionIsInRange(new ArtifactVersion(input.getVersion()));
      }
 catch (      InvalidArtifactRangeException e) {
        return false;
      }
    }
  }
;
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactXv100Info,pluginA1);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",predicate,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactXv200Info,pluginA2);
  expectedMap.put(artifactZv200Info,pluginA2);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactYv100Info,pluginB1);
  expectedMap.put(artifactZv100Info,pluginB1);
  expectedMap.put(artifactZv200Info,pluginB1);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactYv200Info,pluginB2);
  expectedMap.put(artifactZv200Info,pluginB2);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
}","@Test public void testGetPlugins() throws Exception {
  ArtifactRange parentArtifacts=new ArtifactRange(NamespaceId.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""));
  PluginClass pluginA1=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false),""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",false,false)));
  PluginClass pluginA2=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false)));
  PluginClass pluginB1=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",false,false)));
  PluginClass pluginB2=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false),""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true,false)));
  String contents=""String_Node_Str"";
  Id.Artifact parentArtifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta parentMeta=new ArtifactMeta(ArtifactClasses.builder().build());
  writeArtifact(parentArtifactId,parentMeta,contents);
  Id.Artifact artifactXv100=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaXv100=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginA1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactXv100,metaXv100,contents);
  ArtifactDescriptor artifactXv100Info=artifactStore.getArtifact(artifactXv100).getDescriptor();
  Id.Artifact artifactXv110=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaXv110=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginA1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactXv110,metaXv110,contents);
  ArtifactDescriptor artifactXv110Info=artifactStore.getArtifact(artifactXv110).getDescriptor();
  Id.Artifact artifactXv200=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaXv200=new ArtifactMeta(ArtifactClasses.builder().addPlugins(pluginA1,pluginA2).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactXv200,metaXv200,contents);
  ArtifactDescriptor artifactXv200Info=artifactStore.getArtifact(artifactXv200).getDescriptor();
  Id.Artifact artifactYv100=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaYv100=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginB1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactYv100,metaYv100,contents);
  ArtifactDescriptor artifactYv100Info=artifactStore.getArtifact(artifactYv100).getDescriptor();
  Id.Artifact artifactYv200=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaYv200=new ArtifactMeta(ArtifactClasses.builder().addPlugin(pluginB2).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactYv200,metaYv200,contents);
  ArtifactDescriptor artifactYv200Info=artifactStore.getArtifact(artifactYv200).getDescriptor();
  Id.Artifact artifactZv100=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaZv100=new ArtifactMeta(ArtifactClasses.builder().addPlugins(pluginA1,pluginB1).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactZv100,metaZv100,contents);
  ArtifactDescriptor artifactZv100Info=artifactStore.getArtifact(artifactZv100).getDescriptor();
  Id.Artifact artifactZv200=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta metaZv200=new ArtifactMeta(ArtifactClasses.builder().addPlugins(pluginA1,pluginA2,pluginB1,pluginB2).build(),ImmutableSet.of(parentArtifacts));
  writeArtifact(artifactZv200,metaZv200,contents);
  ArtifactDescriptor artifactZv200Info=artifactStore.getArtifact(artifactZv200).getDescriptor();
  Map<ArtifactDescriptor,Set<PluginClass>> expected=Maps.newHashMap();
  expected.put(artifactXv100Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv110Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv200Info,ImmutableSet.of(pluginA1,pluginA2));
  expected.put(artifactYv100Info,ImmutableSet.of(pluginB1));
  expected.put(artifactYv200Info,ImmutableSet.of(pluginB2));
  expected.put(artifactZv100Info,ImmutableSet.of(pluginA1,pluginB1));
  expected.put(artifactZv200Info,ImmutableSet.of(pluginA1,pluginA2,pluginB1,pluginB2));
  Map<ArtifactDescriptor,Set<PluginClass>> actual=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId);
  Assert.assertEquals(expected,actual);
  expected=Maps.newHashMap();
  expected.put(artifactXv100Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv110Info,ImmutableSet.of(pluginA1));
  expected.put(artifactXv200Info,ImmutableSet.of(pluginA1,pluginA2));
  expected.put(artifactZv100Info,ImmutableSet.of(pluginA1));
  expected.put(artifactZv200Info,ImmutableSet.of(pluginA1,pluginA2));
  actual=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"");
  Assert.assertEquals(expected,actual);
  expected=Maps.newHashMap();
  expected.put(artifactYv100Info,ImmutableSet.of(pluginB1));
  expected.put(artifactYv200Info,ImmutableSet.of(pluginB2));
  expected.put(artifactZv100Info,ImmutableSet.of(pluginB1));
  expected.put(artifactZv200Info,ImmutableSet.of(pluginB1,pluginB2));
  actual=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"");
  Assert.assertEquals(expected,actual);
  Map<ArtifactDescriptor,PluginClass> expectedMap=Maps.newHashMap();
  expectedMap.put(artifactXv100Info,pluginA1);
  expectedMap.put(artifactXv110Info,pluginA1);
  expectedMap.put(artifactXv200Info,pluginA1);
  expectedMap.put(artifactZv100Info,pluginA1);
  expectedMap.put(artifactZv200Info,pluginA1);
  Map<ArtifactDescriptor,PluginClass> actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,1,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(ImmutableMap.of(artifactXv100Info,pluginA1),actualMap);
  actualMap=new TreeMap<>(artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.DESC));
  Assert.assertEquals(expectedMap,new TreeMap<>(actualMap).descendingMap());
  Predicate<ArtifactId> predicate=new Predicate<ArtifactId>(){
    @Override public boolean apply(    ArtifactId input){
      try {
        return input.getParent().equals(NamespaceId.DEFAULT) && input.getArtifact().equals(""String_Node_Str"") && ArtifactVersionRange.parse(""String_Node_Str"").versionIsInRange(new ArtifactVersion(input.getVersion()));
      }
 catch (      InvalidArtifactRangeException e) {
        return false;
      }
    }
  }
;
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactXv100Info,pluginA1);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",predicate,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,1,ArtifactSortOrder.DESC);
  Assert.assertEquals(ImmutableMap.of(artifactZv200Info,pluginA1),actualMap);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",predicate,1,ArtifactSortOrder.DESC);
  Assert.assertEquals(ImmutableMap.of(artifactXv100Info,pluginA1),actualMap);
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactXv200Info,pluginA2);
  expectedMap.put(artifactZv200Info,pluginA2);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactYv100Info,pluginB1);
  expectedMap.put(artifactZv100Info,pluginB1);
  expectedMap.put(artifactZv200Info,pluginB1);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
  expectedMap=Maps.newHashMap();
  expectedMap.put(artifactYv200Info,pluginB2);
  expectedMap.put(artifactZv200Info,pluginB2);
  actualMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,parentArtifactId,""String_Node_Str"",""String_Node_Str"",null,Integer.MAX_VALUE,ArtifactSortOrder.UNORDERED);
  Assert.assertEquals(expectedMap,actualMap);
}"
5046,"@Override public void failed(Service.State from,Throwable failure){
  LOG.error(""String_Node_Str"",failure);
  serviceStoppedLatch.countDown();
  error(failure);
}","@Override public void failed(Service.State from,Throwable failure){
  LOG.error(""String_Node_Str"",getProgramRunId().getType(),getProgramRunId().getProgram(),failure);
  serviceStoppedLatch.countDown();
  error(failure);
}"
5047,"private void listenToRuntimeState(Service service){
  service.addListener(new ServiceListenerAdapter(){
    @Override public void running(){
      started();
    }
    @Override public void failed(    Service.State from,    Throwable failure){
      LOG.error(""String_Node_Str"",failure);
      serviceStoppedLatch.countDown();
      error(failure);
    }
    @Override public void terminated(    Service.State from){
      serviceStoppedLatch.countDown();
      if (from != Service.State.STOPPING) {
        complete();
      }
 else {
        stop();
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
}","private void listenToRuntimeState(Service service){
  service.addListener(new ServiceListenerAdapter(){
    @Override public void running(){
      started();
    }
    @Override public void failed(    Service.State from,    Throwable failure){
      LOG.error(""String_Node_Str"",getProgramRunId().getType(),getProgramRunId().getProgram(),failure);
      serviceStoppedLatch.countDown();
      error(failure);
    }
    @Override public void terminated(    Service.State from){
      serviceStoppedLatch.countDown();
      if (from != Service.State.STOPPING) {
        complete();
      }
 else {
        stop();
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
}"
5048,"@Override public void run(){
  try {
    String programName=getContext().getSpecification().getProperties().get(PROGRAM_NAME);
    if (programWorkflowRunner == null) {
      throw new UnsupportedOperationException(""String_Node_Str"");
    }
    Runnable programRunner=programWorkflowRunner.create(programName);
    LOG.info(""String_Node_Str"",programName);
    programRunner.run();
    LOG.info(""String_Node_Str"",programType != null ? programType.name() : null,programName);
  }
 catch (  Exception e) {
    LOG.info(""String_Node_Str"",programType,programName,e);
    throw e;
  }
}","@Override public void run(){
  String prettyProgramType=ProgramType.valueOf(programType.name()).getPrettyName();
  String programName=getContext().getSpecification().getProperties().get(PROGRAM_NAME);
  if (programWorkflowRunner == null) {
    throw new UnsupportedOperationException(""String_Node_Str"");
  }
  Runnable programRunner=programWorkflowRunner.create(programName);
  LOG.info(""String_Node_Str"",prettyProgramType,programName);
  programRunner.run();
  LOG.info(""String_Node_Str"",prettyProgramType,programName);
}"
5049,"@Override protected void startUp() throws Exception {
  LoggingContextAccessor.setLoggingContext(loggingContext);
  httpService=NettyHttpService.builder().setWorkerThreadPoolSize(2).setExecThreadPoolSize(4).setHost(hostname.getHostName()).addHttpHandlers(ImmutableList.of(new WorkflowServiceHandler(createStatusSupplier()))).build();
  httpService.startAndWait();
  runningThread=Thread.currentThread();
  createLocalDatasets();
  workflow=initializeWorkflow();
}","@Override protected void startUp() throws Exception {
  LoggingContextAccessor.setLoggingContext(loggingContext);
  httpService=NettyHttpService.builder(workflowRunId.getProgram() + ""String_Node_Str"").setWorkerThreadPoolSize(2).setExecThreadPoolSize(4).setHost(hostname.getHostName()).addHttpHandlers(ImmutableList.of(new WorkflowServiceHandler(createStatusSupplier()))).build();
  httpService.startAndWait();
  runningThread=Thread.currentThread();
  createLocalDatasets();
  workflow=initializeWorkflow();
}"
5050,"@Override public void running(){
  InetSocketAddress endpoint=driver.getServiceEndpoint();
  cancelAnnounce=serviceAnnouncer.announce(serviceName,endpoint.getPort());
  LOG.info(""String_Node_Str"",serviceName,endpoint);
  started();
}","@Override public void running(){
  InetSocketAddress endpoint=driver.getServiceEndpoint();
  cancelAnnounce=serviceAnnouncer.announce(serviceName,endpoint.getPort());
  LOG.debug(""String_Node_Str"",serviceName,endpoint);
  started();
}"
5051,"private void startListen(Service service){
  service.addListener(new ServiceListenerAdapter(){
    @Override public void running(){
      InetSocketAddress endpoint=driver.getServiceEndpoint();
      cancelAnnounce=serviceAnnouncer.announce(serviceName,endpoint.getPort());
      LOG.info(""String_Node_Str"",serviceName,endpoint);
      started();
    }
    @Override public void terminated(    Service.State from){
      LOG.info(""String_Node_Str"",from,serviceName);
      cancelAnnounce.cancel();
      LOG.info(""String_Node_Str"",serviceName);
      if (getState() != State.STOPPING) {
        complete();
      }
 else {
        stop();
      }
    }
    @Override public void failed(    Service.State from,    Throwable failure){
      LOG.info(""String_Node_Str"",from,serviceName,failure);
      if (cancelAnnounce != null) {
        cancelAnnounce.cancel();
      }
      LOG.info(""String_Node_Str"",serviceName);
      error(failure);
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
}","private void startListen(Service service){
  service.addListener(new ServiceListenerAdapter(){
    @Override public void running(){
      InetSocketAddress endpoint=driver.getServiceEndpoint();
      cancelAnnounce=serviceAnnouncer.announce(serviceName,endpoint.getPort());
      LOG.debug(""String_Node_Str"",serviceName,endpoint);
      started();
    }
    @Override public void terminated(    Service.State from){
      LOG.debug(""String_Node_Str"",from,serviceName);
      cancelAnnounce.cancel();
      LOG.debug(""String_Node_Str"",serviceName);
      if (getState() != State.STOPPING) {
        complete();
      }
 else {
        stop();
      }
    }
    @Override public void failed(    Service.State from,    Throwable failure){
      LOG.info(""String_Node_Str"",from,serviceName,failure);
      if (cancelAnnounce != null) {
        cancelAnnounce.cancel();
      }
      LOG.info(""String_Node_Str"",serviceName);
      error(failure);
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
}"
5052,"@Override public void terminated(Service.State from){
  LOG.info(""String_Node_Str"",from,serviceName);
  cancelAnnounce.cancel();
  LOG.info(""String_Node_Str"",serviceName);
  if (getState() != State.STOPPING) {
    complete();
  }
 else {
    stop();
  }
}","@Override public void terminated(Service.State from){
  LOG.debug(""String_Node_Str"",from,serviceName);
  cancelAnnounce.cancel();
  LOG.debug(""String_Node_Str"",serviceName);
  if (getState() != State.STOPPING) {
    complete();
  }
 else {
    stop();
  }
}"
5053,"@Override public void run(){
  JobID jobId=profile.getJobID();
  JobContext jContext=new JobContextImpl(job,jobId);
  org.apache.hadoop.mapreduce.OutputCommitter outputCommitter=null;
  try {
    outputCommitter=createOutputCommitter(conf.getUseNewMapper(),jobId,conf);
  }
 catch (  Exception e) {
    LOG.info(""String_Node_Str"",e);
    return;
  }
  try {
    TaskSplitMetaInfo[] taskSplitMetaInfos=SplitMetaInfoReader.readSplitMetaInfo(jobId,localFs,conf,systemJobDir);
    int numReduceTasks=job.getNumReduceTasks();
    outputCommitter.setupJob(jContext);
    status.setSetupProgress(1.0f);
    Map<TaskAttemptID,MapOutputFile> mapOutputFiles=Collections.synchronizedMap(new HashMap<TaskAttemptID,MapOutputFile>());
    List<RunnableWithThrowable> mapRunnables=getMapTaskRunnables(taskSplitMetaInfos,jobId,mapOutputFiles);
    initCounters(mapRunnables.size(),numReduceTasks);
    ExecutorService mapService=createMapExecutor();
    runTasks(mapRunnables,mapService,""String_Node_Str"");
    try {
      if (numReduceTasks > 0) {
        List<RunnableWithThrowable> reduceRunnables=getReduceTaskRunnables(jobId,mapOutputFiles);
        ExecutorService reduceService=createReduceExecutor();
        runTasks(reduceRunnables,reduceService,""String_Node_Str"");
      }
    }
  finally {
      for (      MapOutputFile output : mapOutputFiles.values()) {
        output.removeAll();
      }
    }
    outputCommitter.commitJob(jContext);
    status.setCleanupProgress(1.0f);
    if (killed) {
      this.status.setRunState(JobStatus.KILLED);
    }
 else {
      this.status.setRunState(JobStatus.SUCCEEDED);
    }
    JobEndNotifier.localRunnerNotification(job,status);
  }
 catch (  Throwable t) {
    try {
      outputCommitter.abortJob(jContext,org.apache.hadoop.mapreduce.JobStatus.State.FAILED);
    }
 catch (    IOException ioe) {
      LOG.info(""String_Node_Str"",id);
    }
    status.setCleanupProgress(1.0f);
    if (killed) {
      this.status.setRunState(JobStatus.KILLED);
      LOG.warn(""String_Node_Str"",id,t);
    }
 else {
      this.status.setRunState(JobStatus.FAILED);
      LOG.error(""String_Node_Str"",id,t);
    }
    JobEndNotifier.localRunnerNotification(job,status);
  }
 finally {
    try {
      fs.delete(systemJobFile.getParent(),true);
      localFs.delete(localJobFile,true);
      localDistributedCacheManager.close();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",id,e);
    }
  }
}","@Override public void run(){
  JobID jobId=profile.getJobID();
  JobContext jContext=new JobContextImpl(job,jobId);
  org.apache.hadoop.mapreduce.OutputCommitter outputCommitter=null;
  try {
    outputCommitter=createOutputCommitter(conf.getUseNewMapper(),jobId,conf);
  }
 catch (  Exception e) {
    LOG.info(""String_Node_Str"",e);
    return;
  }
  try {
    TaskSplitMetaInfo[] taskSplitMetaInfos=SplitMetaInfoReader.readSplitMetaInfo(jobId,localFs,conf,systemJobDir);
    int numReduceTasks=job.getNumReduceTasks();
    outputCommitter.setupJob(jContext);
    status.setSetupProgress(1.0f);
    Map<TaskAttemptID,MapOutputFile> mapOutputFiles=Collections.synchronizedMap(new HashMap<TaskAttemptID,MapOutputFile>());
    List<RunnableWithThrowable> mapRunnables=getMapTaskRunnables(taskSplitMetaInfos,jobId,mapOutputFiles);
    initCounters(mapRunnables.size(),numReduceTasks);
    ExecutorService mapService=createMapExecutor();
    runTasks(mapRunnables,mapService,""String_Node_Str"");
    try {
      if (numReduceTasks > 0) {
        List<RunnableWithThrowable> reduceRunnables=getReduceTaskRunnables(jobId,mapOutputFiles);
        ExecutorService reduceService=createReduceExecutor();
        runTasks(reduceRunnables,reduceService,""String_Node_Str"");
      }
    }
  finally {
      for (      MapOutputFile output : mapOutputFiles.values()) {
        output.removeAll();
      }
    }
    outputCommitter.commitJob(jContext);
    status.setCleanupProgress(1.0f);
    if (killed) {
      this.status.setRunState(JobStatus.KILLED);
    }
 else {
      this.status.setRunState(JobStatus.SUCCEEDED);
    }
    JobEndNotifier.localRunnerNotification(job,status);
  }
 catch (  Throwable t) {
    try {
      outputCommitter.abortJob(jContext,org.apache.hadoop.mapreduce.JobStatus.State.FAILED);
    }
 catch (    IOException ioe) {
      LOG.info(""String_Node_Str"",id);
    }
    status.setCleanupProgress(1.0f);
    if (killed) {
      this.status.setRunState(JobStatus.KILLED);
      LOG.warn(""String_Node_Str"",id,t);
    }
 else {
      this.status.setRunState(JobStatus.FAILED);
      LOG.error(""String_Node_Str"",id.getId(),Throwables.getRootCause(t));
    }
    JobEndNotifier.localRunnerNotification(job,status);
  }
 finally {
    try {
      fs.delete(systemJobFile.getParent(),true);
      localFs.delete(localJobFile,true);
      localDistributedCacheManager.close();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",id,e);
    }
  }
}"
5054,"private void initFlowlet() throws InterruptedException {
  LOG.info(""String_Node_Str"" + flowletContext);
  try {
    try {
      flowletContext.initializeProgram(flowlet,flowletContext,Transactions.getTransactionControl(TransactionControl.IMPLICIT,Flowlet.class,flowlet,""String_Node_Str"",FlowletContext.class),false);
      LOG.info(""String_Node_Str"" + flowletContext);
    }
 catch (    TransactionFailureException e) {
      throw e.getCause() == null ? e : e.getCause();
    }
  }
 catch (  Throwable cause) {
    LOG.error(""String_Node_Str"" + flowletContext,cause);
    throw Throwables.propagate(cause);
  }
}","private void initFlowlet() throws InterruptedException {
  LOG.debug(""String_Node_Str"",flowletContext);
  try {
    try {
      flowletContext.initializeProgram(flowlet,flowletContext,Transactions.getTransactionControl(TransactionControl.IMPLICIT,Flowlet.class,flowlet,""String_Node_Str"",FlowletContext.class),false);
      LOG.debug(""String_Node_Str"",flowletContext);
    }
 catch (    TransactionFailureException e) {
      throw e.getCause() == null ? e : e.getCause();
    }
  }
 catch (  Throwable cause) {
    LOG.error(""String_Node_Str"",flowletContext,cause);
    throw Throwables.propagate(cause);
  }
}"
5055,"@Override protected void startUp() throws Exception {
  LoggingContextAccessor.setLoggingContext(flowletContext.getLoggingContext());
  flowletContext.getProgramMetrics().increment(""String_Node_Str"",1);
  flowletProcessDriver=new FlowletProcessDriver(flowletContext,dataFabricFacade,txCallback,processSpecs);
  serviceHook.startAndWait();
  initFlowlet();
  flowletProcessDriver.startAndWait();
}","@Override protected void startUp() throws Exception {
  LoggingContextAccessor.setLoggingContext(flowletContext.getLoggingContext());
  flowletContext.getProgramMetrics().increment(""String_Node_Str"",1);
  flowletProcessDriver=new FlowletProcessDriver(flowletContext,dataFabricFacade,txCallback,processSpecs);
  serviceHook.startAndWait();
  initFlowlet();
  flowletProcessDriver.startAndWait();
  LOG.info(""String_Node_Str"",flowletContext.getFlowletId(),flowletContext.getFlowId(),flowletContext);
}"
5056,"private void destroyFlowlet(){
  LOG.info(""String_Node_Str"" + flowletContext);
  try {
    try {
      flowletContext.destroyProgram(flowlet,flowletContext,Transactions.getTransactionControl(TransactionControl.IMPLICIT,Flowlet.class,flowlet,""String_Node_Str""),false);
      LOG.info(""String_Node_Str"" + flowletContext);
    }
 catch (    TransactionFailureException e) {
      throw e.getCause() == null ? e : e.getCause();
    }
  }
 catch (  Throwable cause) {
    LOG.error(""String_Node_Str"" + flowletContext,cause);
    throw Throwables.propagate(cause);
  }
}","private void destroyFlowlet(){
  LOG.debug(""String_Node_Str"",flowletContext);
  try {
    try {
      flowletContext.destroyProgram(flowlet,flowletContext,Transactions.getTransactionControl(TransactionControl.IMPLICIT,Flowlet.class,flowlet,""String_Node_Str""),false);
      LOG.debug(""String_Node_Str"",flowletContext);
    }
 catch (    TransactionFailureException e) {
      throw e.getCause() == null ? e : e.getCause();
    }
  }
 catch (  Throwable cause) {
    LOG.error(""String_Node_Str"",flowletContext,cause);
    throw Throwables.propagate(cause);
  }
}"
5057,"@Override protected void shutDown() throws Exception {
  LoggingContextAccessor.setLoggingContext(flowletContext.getLoggingContext());
  if (flowletProcessDriver != null) {
    stopService(flowletProcessDriver);
  }
  destroyFlowlet();
  stopService(serviceHook);
}","@Override protected void shutDown() throws Exception {
  LoggingContextAccessor.setLoggingContext(flowletContext.getLoggingContext());
  if (flowletProcessDriver != null) {
    stopService(flowletProcessDriver);
  }
  destroyFlowlet();
  LOG.info(""String_Node_Str"",flowletContext.getFlowletId(),flowletContext.getInstanceId(),flowletContext.getFlowId());
  stopService(serviceHook);
}"
5058,"@Override public void cancel(){
  if (Thread.currentThread() == saveCurrentThread && !cancelled) {
    MDC.setContextMap(saveContextMap);
    loggingContext.set(saveLoggingContext);
    cancelled=true;
  }
}","@Override public void cancel(){
  if (Thread.currentThread() == saveCurrentThread && !cancelled) {
    MDC.setContextMap(saveContextMap == null ? Collections.emptyMap() : saveContextMap);
    loggingContext.set(saveLoggingContext);
    cancelled=true;
  }
}"
5059,"/** 
 * Sets the logging context. <p> NOTE: in work execution frameworks where threads are shared between workers (like Akka) we would have to init context very frequently (before every chunk of work is started). In that case we really want to re-use logging context object instance. </p>
 * @param context context to set
 * @return Cancellable that can be used to revert the logging context and MDC Map to its original value
 */
public static Cancellable setLoggingContext(LoggingContext context){
  final LoggingContext saveLoggingContext=loggingContext.get();
  final Map saveContextMap=MDC.getCopyOfContextMap();
  final Thread saveCurrentThread=Thread.currentThread();
  loggingContext.set(context);
  try {
    MDC.setContextMap(context.getSystemTagsAsString());
  }
 catch (  IllegalStateException e) {
  }
  return new Cancellable(){
    private boolean cancelled;
    @Override public void cancel(){
      if (Thread.currentThread() == saveCurrentThread && !cancelled) {
        MDC.setContextMap(saveContextMap);
        loggingContext.set(saveLoggingContext);
        cancelled=true;
      }
    }
  }
;
}","/** 
 * Sets the logging context. <p> NOTE: in work execution frameworks where threads are shared between workers (like Akka) we would have to init context very frequently (before every chunk of work is started). In that case we really want to re-use logging context object instance. </p>
 * @param context context to set
 * @return Cancellable that can be used to revert the logging context and MDC Map to its original value
 */
public static Cancellable setLoggingContext(LoggingContext context){
  final LoggingContext saveLoggingContext=loggingContext.get();
  final Map saveContextMap=MDC.getCopyOfContextMap();
  final Thread saveCurrentThread=Thread.currentThread();
  loggingContext.set(context);
  try {
    MDC.setContextMap(context.getSystemTagsAsString());
  }
 catch (  IllegalStateException e) {
  }
  return new Cancellable(){
    private boolean cancelled;
    @Override public void cancel(){
      if (Thread.currentThread() == saveCurrentThread && !cancelled) {
        MDC.setContextMap(saveContextMap == null ? Collections.emptyMap() : saveContextMap);
        loggingContext.set(saveLoggingContext);
        cancelled=true;
      }
    }
  }
;
}"
5060,"@Test public void testReset(){
  LoggingContextAccessor.setLoggingContext(new GenericLoggingContext(OLD_NS,OLD_APP,OLD_ENTITY));
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),OLD_NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),OLD_APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),OLD_ENTITY);
  final Cancellable cancellable=LoggingContextAccessor.setLoggingContext(new GenericLoggingContext(NS,APP,ENTITY));
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),ENTITY);
  Thread thread=new Thread(new Runnable(){
    @Override public void run(){
      cancellable.cancel();
    }
  }
);
  thread.start();
  try {
    thread.join();
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
    Assert.fail();
  }
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),ENTITY);
  cancellable.cancel();
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),OLD_NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),OLD_APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),OLD_ENTITY);
}","@Test public void testReset(){
  Cancellable cancellable=LoggingContextAccessor.setLoggingContext(new GenericLoggingContext(OLD_NS,OLD_APP,OLD_ENTITY));
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),OLD_NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),OLD_APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),OLD_ENTITY);
  final Cancellable cancellable2=LoggingContextAccessor.setLoggingContext(new GenericLoggingContext(NS,APP,ENTITY));
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),ENTITY);
  Thread thread=new Thread(new Runnable(){
    @Override public void run(){
      cancellable2.cancel();
    }
  }
);
  thread.start();
  try {
    thread.join();
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
    Assert.fail();
  }
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),ENTITY);
  cancellable2.cancel();
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),OLD_NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),OLD_APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),OLD_ENTITY);
  cancellable.cancel();
  Assert.assertTrue(MDC.getCopyOfContextMap().isEmpty());
}"
5061,"@Override public void run(){
  cancellable.cancel();
}","@Override public void run(){
  cancellable2.cancel();
}"
5062,"@Override public void cancel(){
  if (Thread.currentThread() == saveCurrentThread && !cancelled) {
    MDC.setContextMap(saveContextMap);
    loggingContext.set(saveLoggingContext);
    cancelled=true;
  }
}","@Override public void cancel(){
  if (Thread.currentThread() == saveCurrentThread && !cancelled) {
    MDC.setContextMap(saveContextMap == null ? Collections.emptyMap() : saveContextMap);
    loggingContext.set(saveLoggingContext);
    cancelled=true;
  }
}"
5063,"/** 
 * Sets the logging context. <p> NOTE: in work execution frameworks where threads are shared between workers (like Akka) we would have to init context very frequently (before every chunk of work is started). In that case we really want to re-use logging context object instance. </p>
 * @param context context to set
 * @return Cancellable that can be used to revert the logging context and MDC Map to its original value
 */
public static Cancellable setLoggingContext(LoggingContext context){
  final LoggingContext saveLoggingContext=loggingContext.get();
  final Map saveContextMap=MDC.getCopyOfContextMap();
  final Thread saveCurrentThread=Thread.currentThread();
  loggingContext.set(context);
  try {
    MDC.setContextMap(context.getSystemTagsAsString());
  }
 catch (  IllegalStateException e) {
  }
  return new Cancellable(){
    private boolean cancelled;
    @Override public void cancel(){
      if (Thread.currentThread() == saveCurrentThread && !cancelled) {
        MDC.setContextMap(saveContextMap);
        loggingContext.set(saveLoggingContext);
        cancelled=true;
      }
    }
  }
;
}","/** 
 * Sets the logging context. <p> NOTE: in work execution frameworks where threads are shared between workers (like Akka) we would have to init context very frequently (before every chunk of work is started). In that case we really want to re-use logging context object instance. </p>
 * @param context context to set
 * @return Cancellable that can be used to revert the logging context and MDC Map to its original value
 */
public static Cancellable setLoggingContext(LoggingContext context){
  final LoggingContext saveLoggingContext=loggingContext.get();
  final Map saveContextMap=MDC.getCopyOfContextMap();
  final Thread saveCurrentThread=Thread.currentThread();
  loggingContext.set(context);
  try {
    MDC.setContextMap(context.getSystemTagsAsString());
  }
 catch (  IllegalStateException e) {
  }
  return new Cancellable(){
    private boolean cancelled;
    @Override public void cancel(){
      if (Thread.currentThread() == saveCurrentThread && !cancelled) {
        MDC.setContextMap(saveContextMap == null ? Collections.emptyMap() : saveContextMap);
        loggingContext.set(saveLoggingContext);
        cancelled=true;
      }
    }
  }
;
}"
5064,"@Test public void testReset(){
  LoggingContextAccessor.setLoggingContext(new GenericLoggingContext(OLD_NS,OLD_APP,OLD_ENTITY));
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),OLD_NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),OLD_APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),OLD_ENTITY);
  final Cancellable cancellable=LoggingContextAccessor.setLoggingContext(new GenericLoggingContext(NS,APP,ENTITY));
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),ENTITY);
  Thread thread=new Thread(new Runnable(){
    @Override public void run(){
      cancellable.cancel();
    }
  }
);
  thread.start();
  try {
    thread.join();
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
    Assert.fail();
  }
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),ENTITY);
  cancellable.cancel();
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),OLD_NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),OLD_APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),OLD_ENTITY);
}","@Test public void testReset(){
  Cancellable cancellable=LoggingContextAccessor.setLoggingContext(new GenericLoggingContext(OLD_NS,OLD_APP,OLD_ENTITY));
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),OLD_NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),OLD_APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),OLD_ENTITY);
  final Cancellable cancellable2=LoggingContextAccessor.setLoggingContext(new GenericLoggingContext(NS,APP,ENTITY));
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),ENTITY);
  Thread thread=new Thread(new Runnable(){
    @Override public void run(){
      cancellable2.cancel();
    }
  }
);
  thread.start();
  try {
    thread.join();
  }
 catch (  InterruptedException e) {
    e.printStackTrace();
    Assert.fail();
  }
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),ENTITY);
  cancellable2.cancel();
  Assert.assertEquals(MDC.get(NamespaceLoggingContext.TAG_NAMESPACE_ID),OLD_NS);
  Assert.assertEquals(MDC.get(ApplicationLoggingContext.TAG_APPLICATION_ID),OLD_APP);
  Assert.assertEquals(MDC.get(GenericLoggingContext.TAG_ENTITY_ID),OLD_ENTITY);
  cancellable.cancel();
  Assert.assertTrue(MDC.getCopyOfContextMap().isEmpty());
}"
5065,"@Override public void run(){
  cancellable.cancel();
}","@Override public void run(){
  cancellable2.cancel();
}"
5066,"/** 
 * Save program runtime args.
 */
@PUT @Path(""String_Node_Str"") @AuditPolicy(AuditDetail.REQUEST_BODY) public void saveProgramRuntimeArgs(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appName,@PathParam(""String_Node_Str"") String type,@PathParam(""String_Node_Str"") String programName) throws Exception {
  ProgramType programType=getProgramType(type);
  if (programType == null || programType == ProgramType.WEBAPP) {
    throw new NotFoundException(String.format(""String_Node_Str"" + ""String_Node_Str"",programType));
  }
  lifecycleService.saveRuntimeArgs(new ProgramId(namespaceId,appName,programType,programName),decodeArguments(request));
  responder.sendStatus(HttpResponseStatus.OK);
}","/** 
 * Save runtime args of program with app version.
 */
@PUT @Path(""String_Node_Str"") @AuditPolicy(AuditDetail.REQUEST_BODY) public void saveProgramRuntimeArgs(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appName,@PathParam(""String_Node_Str"") String appVersion,@PathParam(""String_Node_Str"") String type,@PathParam(""String_Node_Str"") String programName) throws Exception {
  ProgramType programType=getProgramType(type);
  ProgramId programId=new ApplicationId(namespaceId,appName,appVersion).program(programType,programName);
  saveProgramIdRuntimeArgs(programId,request,responder);
}"
5067,"/** 
 * Get program runtime args.
 */
@GET @Path(""String_Node_Str"") public void getProgramRuntimeArgs(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appName,@PathParam(""String_Node_Str"") String type,@PathParam(""String_Node_Str"") String programName) throws BadRequestException, NotImplementedException, NotFoundException, UnauthorizedException {
  ProgramType programType=getProgramType(type);
  if (programType == null || programType == ProgramType.WEBAPP) {
    throw new NotFoundException(String.format(""String_Node_Str"" + ""String_Node_Str"",type));
  }
  responder.sendJson(HttpResponseStatus.OK,lifecycleService.getRuntimeArgs(new ProgramId(namespaceId,appName,programType,programName)));
}","/** 
 * Get runtime args of a program with app version.
 */
@GET @Path(""String_Node_Str"") public void getProgramRuntimeArgs(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appName,@PathParam(""String_Node_Str"") String appVersion,@PathParam(""String_Node_Str"") String type,@PathParam(""String_Node_Str"") String programName) throws BadRequestException, NotImplementedException, NotFoundException, UnauthorizedException {
  ProgramType programType=getProgramType(type);
  ProgramId programId=new ApplicationId(namespaceId,appName,appVersion).program(programType,programName);
  getProgramIdRuntimeArgs(programId,responder);
}"
5068,"private void testRuntimeArgs(Class<?> app,String namespace,String appId,String programType,String programId) throws Exception {
  deploy(app,Constants.Gateway.API_VERSION_3_TOKEN,namespace);
  Map<String,String> args=Maps.newHashMap();
  args.put(""String_Node_Str"",""String_Node_Str"");
  args.put(""String_Node_Str"",""String_Node_Str"");
  args.put(""String_Node_Str"",""String_Node_Str"");
  HttpResponse response;
  String argString=GSON.toJson(args,new TypeToken<Map<String,String>>(){
  }
.getType());
  String versionedRuntimeArgsUrl=getVersionedAPIPath(""String_Node_Str"" + appId + ""String_Node_Str""+ programType+ ""String_Node_Str""+ programId+ ""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,namespace);
  response=doPut(versionedRuntimeArgsUrl,argString);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=doGet(versionedRuntimeArgsUrl);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Map<String,String> argsRead=GSON.fromJson(EntityUtils.toString(response.getEntity()),new TypeToken<Map<String,String>>(){
  }
.getType());
  Assert.assertEquals(args.size(),argsRead.size());
  for (  Map.Entry<String,String> entry : args.entrySet()) {
    Assert.assertEquals(entry.getValue(),argsRead.get(entry.getKey()));
  }
  response=doPut(versionedRuntimeArgsUrl,""String_Node_Str"");
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=doGet(versionedRuntimeArgsUrl);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  argsRead=GSON.fromJson(EntityUtils.toString(response.getEntity()),new TypeToken<Map<String,String>>(){
  }
.getType());
  Assert.assertEquals(0,argsRead.size());
  response=doPut(versionedRuntimeArgsUrl,null);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=doGet(versionedRuntimeArgsUrl);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  argsRead=GSON.fromJson(EntityUtils.toString(response.getEntity()),new TypeToken<Map<String,String>>(){
  }
.getType());
  Assert.assertEquals(0,argsRead.size());
}","private void testRuntimeArgs(Class<?> app,String namespace,String appId,String programType,String programId) throws Exception {
  deploy(app,Constants.Gateway.API_VERSION_3_TOKEN,namespace);
  String versionedRuntimeArgsUrl=getVersionedAPIPath(""String_Node_Str"" + appId + ""String_Node_Str""+ programType+ ""String_Node_Str""+ programId+ ""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,namespace);
  verifyRuntimeArgs(versionedRuntimeArgsUrl);
  String versionedRuntimeArgsAppVersionUrl=getVersionedAPIPath(""String_Node_Str"" + appId + ""String_Node_Str""+ ApplicationId.DEFAULT_VERSION+ ""String_Node_Str""+ programType+ ""String_Node_Str""+ programId+ ""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,namespace);
  verifyRuntimeArgs(versionedRuntimeArgsAppVersionUrl);
}"
5069,"@Test public void testVersionedProgramStartStopStatus() throws Exception {
  Id.Artifact wordCountArtifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",VERSION1);
  addAppArtifact(wordCountArtifactId,WordCountApp.class);
  AppRequest<? extends Config> wordCountRequest=new AppRequest<>(new ArtifactSummary(wordCountArtifactId.getName(),wordCountArtifactId.getVersion().getVersion()));
  ApplicationId wordCountApp1=NamespaceId.DEFAULT.app(""String_Node_Str"",VERSION1);
  ProgramId wordcountFlow1=wordCountApp1.program(ProgramType.FLOW,""String_Node_Str"");
  Id.Application wordCountAppDefault=wordCountApp1.toId();
  Id.Program wordcountFlowDefault=wordcountFlow1.toId();
  ApplicationId wordCountApp2=NamespaceId.DEFAULT.app(""String_Node_Str"",VERSION2);
  ProgramId wordcountFlow2=wordCountApp2.program(ProgramType.FLOW,""String_Node_Str"");
  Assert.assertEquals(200,deploy(wordCountApp1,wordCountRequest).getStatusLine().getStatusCode());
  Assert.assertEquals(200,deploy(wordCountAppDefault,wordCountRequest).getStatusLine().getStatusCode());
  Assert.assertEquals(STOPPED,getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow1,200);
  waitState(wordcountFlow1,RUNNING);
  startProgram(wordcountFlow1,409);
  startProgram(new NamespaceId(TEST_NAMESPACE1).app(wordcountFlow1.getApplication(),wordcountFlow1.getVersion()).program(wordcountFlow1.getType(),wordcountFlow1.getProgram()),404);
  Assert.assertEquals(200,deploy(wordCountApp2,wordCountRequest).getStatusLine().getStatusCode());
  startProgram(wordcountFlow2,409);
  startProgram(wordcountFlowDefault,409);
  stopProgram(wordcountFlow1,null,200,null);
  waitState(wordcountFlow1,""String_Node_Str"");
  startProgram(wordcountFlow2,200);
  stopProgram(wordcountFlow2,null,200,null);
  ProgramId wordFrequencyService1=wordCountApp1.program(ProgramType.SERVICE,""String_Node_Str"");
  ProgramId wordFrequencyService2=wordCountApp2.program(ProgramType.SERVICE,""String_Node_Str"");
  Id.Program wordFrequencyServiceDefault=wordFrequencyService1.toId();
  Assert.assertEquals(STOPPED,getProgramStatus(wordFrequencyService1));
  startProgram(wordFrequencyService1,200);
  waitState(wordFrequencyService1,RUNNING);
  Assert.assertEquals(STOPPED,getProgramStatus(wordFrequencyService2));
  startProgram(wordFrequencyService2,200);
  waitState(wordFrequencyService2,RUNNING);
  Assert.assertEquals(STOPPED,getProgramStatus(wordFrequencyServiceDefault));
  startProgram(wordFrequencyServiceDefault,200);
  waitState(wordFrequencyServiceDefault,RUNNING);
  startProgram(wordFrequencyService1,409);
  stopProgram(wordFrequencyService1,null,200,null);
  Assert.assertEquals(STOPPED,getProgramStatus(wordFrequencyService1));
  startProgram(wordFrequencyService1,200);
  stopProgram(wordFrequencyService1,null,200,null);
  stopProgram(wordFrequencyService2,null,200,null);
  stopProgram(wordFrequencyServiceDefault,null,200,null);
  Id.Artifact sleepWorkflowArtifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",VERSION1);
  addAppArtifact(sleepWorkflowArtifactId,SleepingWorkflowApp.class);
  AppRequest<? extends Config> sleepWorkflowRequest=new AppRequest<>(new ArtifactSummary(sleepWorkflowArtifactId.getName(),sleepWorkflowArtifactId.getVersion().getVersion()));
  ApplicationId sleepWorkflowApp1=new ApplicationId(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",VERSION1);
  ProgramId sleepWorkflow1=sleepWorkflowApp1.program(ProgramType.WORKFLOW,""String_Node_Str"");
  ApplicationId sleepWorkflowApp2=new ApplicationId(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",VERSION2);
  ProgramId sleepWorkflow2=sleepWorkflowApp2.program(ProgramType.WORKFLOW,""String_Node_Str"");
  Assert.assertEquals(200,deploy(sleepWorkflowApp1,sleepWorkflowRequest).getStatusLine().getStatusCode());
  Assert.assertEquals(STOPPED,getProgramStatus(sleepWorkflow1));
  startProgram(sleepWorkflow2,404);
  Assert.assertEquals(200,deploy(sleepWorkflowApp2,sleepWorkflowRequest).getStatusLine().getStatusCode());
  startProgram(sleepWorkflow1,200);
  startProgram(sleepWorkflow2,200);
  startProgram(sleepWorkflow1,200);
  startProgram(sleepWorkflow2,200);
  stopProgram(sleepWorkflow1,null,200,null);
  stopProgram(sleepWorkflow2,null,200,null);
  Assert.assertEquals(STOPPED,getProgramStatus(sleepWorkflow1));
  Assert.assertEquals(STOPPED,getProgramStatus(sleepWorkflow2));
  deleteApp(wordCountApp1,200);
  deleteApp(wordCountApp2,200);
  deleteApp(wordCountAppDefault,200);
  deleteApp(sleepWorkflowApp1,200);
  deleteApp(sleepWorkflowApp2,200);
}","@Test public void testVersionedProgramStartStopStatus() throws Exception {
  Id.Artifact wordCountArtifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",VERSION1);
  addAppArtifact(wordCountArtifactId,WordCountApp.class);
  AppRequest<? extends Config> wordCountRequest=new AppRequest<>(new ArtifactSummary(wordCountArtifactId.getName(),wordCountArtifactId.getVersion().getVersion()));
  ApplicationId wordCountApp1=NamespaceId.DEFAULT.app(""String_Node_Str"",VERSION1);
  ProgramId wordcountFlow1=wordCountApp1.program(ProgramType.FLOW,""String_Node_Str"");
  Id.Application wordCountAppDefault=wordCountApp1.toId();
  Id.Program wordcountFlowDefault=wordcountFlow1.toId();
  ApplicationId wordCountApp2=NamespaceId.DEFAULT.app(""String_Node_Str"",VERSION2);
  ProgramId wordcountFlow2=wordCountApp2.program(ProgramType.FLOW,""String_Node_Str"");
  Assert.assertEquals(200,deploy(wordCountApp1,wordCountRequest).getStatusLine().getStatusCode());
  Assert.assertEquals(200,deploy(wordCountAppDefault,wordCountRequest).getStatusLine().getStatusCode());
  Assert.assertEquals(STOPPED,getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow1,200);
  waitState(wordcountFlow1,RUNNING);
  startProgram(wordcountFlow1,409);
  startProgram(new NamespaceId(TEST_NAMESPACE1).app(wordcountFlow1.getApplication(),wordcountFlow1.getVersion()).program(wordcountFlow1.getType(),wordcountFlow1.getProgram()),404);
  Assert.assertEquals(200,deploy(wordCountApp2,wordCountRequest).getStatusLine().getStatusCode());
  startProgram(wordcountFlow2,409);
  startProgram(wordcountFlowDefault,409);
  stopProgram(wordcountFlow1,null,200,null);
  waitState(wordcountFlow1,""String_Node_Str"");
  startProgram(wordcountFlow2,200);
  stopProgram(wordcountFlow2,null,200,null);
  ProgramId wordFrequencyService1=wordCountApp1.program(ProgramType.SERVICE,""String_Node_Str"");
  ProgramId wordFrequencyService2=wordCountApp2.program(ProgramType.SERVICE,""String_Node_Str"");
  Id.Program wordFrequencyServiceDefault=wordFrequencyService1.toId();
  Assert.assertEquals(STOPPED,getProgramStatus(wordFrequencyService1));
  startProgram(wordFrequencyService1,200);
  waitState(wordFrequencyService1,RUNNING);
  Assert.assertEquals(STOPPED,getProgramStatus(wordFrequencyService2));
  startProgram(wordFrequencyService2,200);
  waitState(wordFrequencyService2,RUNNING);
  Assert.assertEquals(STOPPED,getProgramStatus(wordFrequencyServiceDefault));
  startProgram(wordFrequencyServiceDefault,200);
  waitState(wordFrequencyServiceDefault,RUNNING);
  startProgram(wordFrequencyService1,409);
  stopProgram(wordFrequencyService1,null,200,null);
  Assert.assertEquals(STOPPED,getProgramStatus(wordFrequencyService1));
  startProgram(wordFrequencyService1,200);
  stopProgram(wordFrequencyService1,null,200,null);
  stopProgram(wordFrequencyService2,null,200,null);
  stopProgram(wordFrequencyServiceDefault,null,200,null);
  Id.Artifact sleepWorkflowArtifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",VERSION1);
  addAppArtifact(sleepWorkflowArtifactId,SleepingWorkflowApp.class);
  AppRequest<? extends Config> sleepWorkflowRequest=new AppRequest<>(new ArtifactSummary(sleepWorkflowArtifactId.getName(),sleepWorkflowArtifactId.getVersion().getVersion()));
  ApplicationId sleepWorkflowApp1=new ApplicationId(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",VERSION1);
  ProgramId sleepWorkflow1=sleepWorkflowApp1.program(ProgramType.WORKFLOW,""String_Node_Str"");
  ApplicationId sleepWorkflowApp2=new ApplicationId(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",VERSION2);
  ProgramId sleepWorkflow2=sleepWorkflowApp2.program(ProgramType.WORKFLOW,""String_Node_Str"");
  Assert.assertEquals(200,deploy(sleepWorkflowApp1,sleepWorkflowRequest).getStatusLine().getStatusCode());
  Assert.assertEquals(STOPPED,getProgramStatus(sleepWorkflow1));
  startProgram(sleepWorkflow2,404);
  Assert.assertEquals(200,deploy(sleepWorkflowApp2,sleepWorkflowRequest).getStatusLine().getStatusCode());
  startProgram(sleepWorkflow1,200);
  startProgram(sleepWorkflow2,200);
  startProgram(sleepWorkflow1,200);
  startProgram(sleepWorkflow2,200);
  stopProgram(sleepWorkflow1,null,200,null);
  stopProgram(sleepWorkflow2,null,200,null);
  Assert.assertEquals(STOPPED,getProgramStatus(sleepWorkflow1));
  Assert.assertEquals(STOPPED,getProgramStatus(sleepWorkflow2));
  testVersionedProgramRuntimeArgs(sleepWorkflow1);
  deleteApp(wordCountApp1,200);
  deleteApp(wordCountApp2,200);
  deleteApp(wordCountAppDefault,200);
  deleteApp(sleepWorkflowApp1,200);
  deleteApp(sleepWorkflowApp2,200);
}"
5070,"@Test public void testRuntimeArgs() throws Exception {
  String qualifiedServiceId=String.format(""String_Node_Str"",FakeApp.NAME,PrefixedEchoHandler.NAME);
  ServiceId service=NamespaceId.DEFAULT.app(FakeApp.NAME).service(PrefixedEchoHandler.NAME);
  Map<String,String> runtimeArgs=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  String runtimeArgsKV=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(runtimeArgs);
  testCommandOutputContains(cli,""String_Node_Str"" + qualifiedServiceId + ""String_Node_Str""+ runtimeArgsKV+ ""String_Node_Str"",""String_Node_Str"");
  try {
    assertProgramStatus(programClient,service,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + qualifiedServiceId + ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + qualifiedServiceId,""String_Node_Str"");
    assertProgramStatus(programClient,service,""String_Node_Str"");
    Map<String,String> runtimeArgs2=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    String runtimeArgs2Json=GSON.toJson(runtimeArgs2);
    String runtimeArgs2KV=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(runtimeArgs2);
    testCommandOutputContains(cli,""String_Node_Str"" + qualifiedServiceId + ""String_Node_Str""+ runtimeArgs2KV+ ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + qualifiedServiceId,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + qualifiedServiceId,runtimeArgs2Json);
    testCommandOutputContains(cli,""String_Node_Str"" + qualifiedServiceId + ""String_Node_Str"",""String_Node_Str"");
  }
  finally {
    testCommandOutputContains(cli,""String_Node_Str"" + qualifiedServiceId,""String_Node_Str"");
    assertProgramStatus(programClient,service,""String_Node_Str"");
  }
}","@Test public void testRuntimeArgs() throws Exception {
  String qualifiedServiceId=String.format(""String_Node_Str"",FakeApp.NAME,PrefixedEchoHandler.NAME);
  ServiceId service=NamespaceId.DEFAULT.app(FakeApp.NAME).service(PrefixedEchoHandler.NAME);
  testServiceRuntimeArgs(qualifiedServiceId,service);
}"
5071,"@Override public String getPattern(){
  return String.format(""String_Node_Str"",elementType.getShortName(),elementType.getArgumentName());
}","@Override public String getPattern(){
  return String.format(""String_Node_Str"",elementType.getShortName(),elementType.getArgumentName(),ArgumentName.APP_VERSION);
}"
5072,"@Override public String getPattern(){
  return String.format(""String_Node_Str"",elementType.getShortName(),elementType.getArgumentName(),ArgumentName.RUNTIME_ARGS);
}","@Override public String getPattern(){
  return String.format(""String_Node_Str"",elementType.getShortName(),elementType.getArgumentName(),ArgumentName.APP_VERSION,ArgumentName.RUNTIME_ARGS);
}"
5073,"/** 
 * Sets the runtime args of a program.
 * @param program the program
 * @param runtimeArgs args of the program
 * @throws IOException if a network error occurred
 * @throws ProgramNotFoundException if the application or program could not be found
 * @throws UnauthenticatedException if the request is not authorized successfully in the gateway server
 */
public void setRuntimeArgs(ProgramId program,Map<String,String> runtimeArgs) throws IOException, UnauthenticatedException, ProgramNotFoundException, UnauthorizedException {
  String path=String.format(""String_Node_Str"",program.getApplication(),program.getType().getCategoryName(),program.getProgram());
  URL url=config.resolveNamespacedURLV3(program.getNamespaceId(),path);
  HttpRequest request=HttpRequest.put(url).withBody(GSON.toJson(runtimeArgs)).build();
  HttpResponse response=restClient.execute(request,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new ProgramNotFoundException(program);
  }
}","/** 
 * Sets the runtime args of a program.
 * @param program the program
 * @param runtimeArgs args of the program
 * @throws IOException if a network error occurred
 * @throws ProgramNotFoundException if the application or program could not be found
 * @throws UnauthenticatedException if the request is not authorized successfully in the gateway server
 */
public void setRuntimeArgs(ProgramId program,Map<String,String> runtimeArgs) throws IOException, UnauthenticatedException, ProgramNotFoundException, UnauthorizedException {
  String path=String.format(""String_Node_Str"",program.getApplication(),program.getVersion(),program.getType().getCategoryName(),program.getProgram());
  URL url=config.resolveNamespacedURLV3(program.getNamespaceId(),path);
  HttpRequest request=HttpRequest.put(url).withBody(GSON.toJson(runtimeArgs)).build();
  HttpResponse response=restClient.execute(request,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new ProgramNotFoundException(program);
  }
}"
5074,"/** 
 * Gets the runtime args of a program.
 * @param program the program
 * @return runtime args of the program
 * @throws IOException if a network error occurred
 * @throws ProgramNotFoundException if the application or program could not be found
 * @throws UnauthenticatedException if the request is not authorized successfully in the gateway server
 */
public Map<String,String> getRuntimeArgs(ProgramId program) throws IOException, UnauthenticatedException, ProgramNotFoundException, UnauthorizedException {
  String path=String.format(""String_Node_Str"",program.getApplication(),program.getType().getCategoryName(),program.getProgram());
  URL url=config.resolveNamespacedURLV3(program.getNamespaceId(),path);
  HttpResponse response=restClient.execute(HttpMethod.GET,url,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new ProgramNotFoundException(program);
  }
  return ObjectResponse.fromJsonBody(response,new TypeToken<Map<String,String>>(){
  }
).getResponseObject();
}","/** 
 * Gets the runtime args of a program.
 * @param program the program
 * @return runtime args of the program
 * @throws IOException if a network error occurred
 * @throws ProgramNotFoundException if the application or program could not be found
 * @throws UnauthenticatedException if the request is not authorized successfully in the gateway server
 */
public Map<String,String> getRuntimeArgs(ProgramId program) throws IOException, UnauthenticatedException, ProgramNotFoundException, UnauthorizedException {
  String path=String.format(""String_Node_Str"",program.getApplication(),program.getVersion(),program.getType().getCategoryName(),program.getProgram());
  URL url=config.resolveNamespacedURLV3(program.getNamespaceId(),path);
  HttpResponse response=restClient.execute(HttpMethod.GET,url,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new ProgramNotFoundException(program);
  }
  return ObjectResponse.fromJsonBody(response,new TypeToken<Map<String,String>>(){
  }
).getResponseObject();
}"
5075,"@VisibleForTesting static Injector createInjector(CConfiguration cConf,Configuration hConf){
  return Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new IOModule(),new KafkaClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(FileContext.class).toProvider(FileContextProvider.class).in(Scopes.SINGLETON);
    }
    @Provides @Singleton private LocationFactory providesLocationFactory(    Configuration hConf,    CConfiguration cConf,    FileContext fc){
      String namespace=cConf.get(Constants.CFG_HDFS_NAMESPACE);
      return new FileContextLocationFactory(hConf,fc,namespace);
    }
  }
);
}","@VisibleForTesting static Injector createInjector(CConfiguration cConf,Configuration hConf){
  return Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new IOModule(),new KafkaClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(FileContext.class).toProvider(FileContextProvider.class).in(Scopes.SINGLETON);
    }
    @Provides @Singleton private LocationFactory providesLocationFactory(    Configuration hConf,    CConfiguration cConf,    FileContext fc){
      final String namespace=cConf.get(Constants.CFG_HDFS_NAMESPACE);
      if (UserGroupInformation.isSecurityEnabled()) {
        return new FileContextLocationFactory(hConf,namespace);
      }
      return new InsecureFileContextLocationFactory(hConf,namespace,fc);
    }
  }
);
}"
5076,"@Provides @Singleton private LocationFactory providesLocationFactory(Configuration hConf,CConfiguration cConf,FileContext fc){
  String namespace=cConf.get(Constants.CFG_HDFS_NAMESPACE);
  return new FileContextLocationFactory(hConf,fc,namespace);
}","@Provides @Singleton private LocationFactory providesLocationFactory(Configuration hConf,CConfiguration cConf,FileContext fc){
  final String namespace=cConf.get(Constants.CFG_HDFS_NAMESPACE);
  if (UserGroupInformation.isSecurityEnabled()) {
    return new FileContextLocationFactory(hConf,namespace);
  }
  return new InsecureFileContextLocationFactory(hConf,namespace,fc);
}"
5077,"/** 
 * Method to add version in StreamSizeSchedule row key in SchedulerStore.
 * @throws InterruptedException
 * @throws IOException
 * @throws DatasetManagementException
 */
public void upgrade() throws InterruptedException, IOException, DatasetManagementException {
  while (!storeInitialized.get()) {
    TimeUnit.SECONDS.sleep(10);
  }
  if (isUpgradeComplete()) {
    LOG.info(""String_Node_Str"",NAME);
    return;
  }
  final AtomicInteger maxNumberUpdateRows=new AtomicInteger(1000);
  final AtomicInteger sleepTimeInSecs=new AtomicInteger(60);
  LOG.info(""String_Node_Str"",NAME);
  while (!isUpgradeComplete()) {
    sleepTimeInSecs.set(60);
    try {
      factory.createExecutor(ImmutableList.of((TransactionAware)table)).execute(new TransactionExecutor.Subroutine(){
        @Override public void apply(){
          if (upgradeVersionKeys(table,maxNumberUpdateRows.get())) {
            table.put(APP_VERSION_UPGRADE_KEY,COLUMN,Bytes.toBytes(ProjectInfo.getVersion().toString()));
          }
        }
      }
);
    }
 catch (    TransactionFailureException e) {
      if (e instanceof TransactionConflictException) {
        LOG.debug(""String_Node_Str"",e);
        sleepTimeInSecs.set(10);
      }
 else       if (e instanceof TransactionNotInProgressException) {
        int currMaxRows=maxNumberUpdateRows.get();
        if (currMaxRows > 500) {
          maxNumberUpdateRows.decrementAndGet();
        }
 else {
          LOG.warn(""String_Node_Str"",NAME);
          return;
        }
        sleepTimeInSecs.set(10);
        LOG.debug(""String_Node_Str"" + ""String_Node_Str"",maxNumberUpdateRows.get(),e);
      }
 else {
        LOG.error(""String_Node_Str"",e);
        sleepTimeInSecs.set(60);
      }
    }
    TimeUnit.SECONDS.sleep(sleepTimeInSecs.get());
  }
  LOG.info(""String_Node_Str"",NAME);
}","/** 
 * Method to add version in StreamSizeSchedule row key in SchedulerStore.
 * @throws InterruptedException
 * @throws IOException
 * @throws DatasetManagementException
 */
public void upgrade() throws InterruptedException, IOException, DatasetManagementException {
  Table metaTable=null;
  while (metaTable == null) {
    try {
      metaTable=tableUtil.getMetaTable();
    }
 catch (    Exception e) {
    }
    TimeUnit.SECONDS.sleep(10);
  }
  if (isUpgradeComplete()) {
    LOG.info(""String_Node_Str"",NAME);
    return;
  }
  final AtomicInteger maxNumberUpdateRows=new AtomicInteger(1000);
  final AtomicInteger sleepTimeInSecs=new AtomicInteger(60);
  LOG.info(""String_Node_Str"",NAME);
  while (!isUpgradeComplete()) {
    sleepTimeInSecs.set(60);
    try {
      final Table finalMetaTable=metaTable;
      factory.createExecutor(ImmutableList.of((TransactionAware)finalMetaTable)).execute(new TransactionExecutor.Subroutine(){
        @Override public void apply() throws Exception {
          if (upgradeVersionKeys(finalMetaTable,maxNumberUpdateRows.get())) {
            finalMetaTable.put(APP_VERSION_UPGRADE_KEY,COLUMN,Bytes.toBytes(ProjectInfo.getVersion().toString()));
          }
        }
      }
);
    }
 catch (    TransactionFailureException e) {
      if (e instanceof TransactionConflictException) {
        LOG.debug(""String_Node_Str"",e);
        sleepTimeInSecs.set(10);
      }
 else       if (e instanceof TransactionNotInProgressException) {
        int currMaxRows=maxNumberUpdateRows.get();
        if (currMaxRows > 500) {
          maxNumberUpdateRows.decrementAndGet();
        }
 else {
          LOG.warn(""String_Node_Str"",NAME);
          return;
        }
        sleepTimeInSecs.set(10);
        LOG.debug(""String_Node_Str"" + ""String_Node_Str"",maxNumberUpdateRows.get(),e);
      }
 else {
        LOG.error(""String_Node_Str"",e);
        sleepTimeInSecs.set(60);
      }
    }
    TimeUnit.SECONDS.sleep(sleepTimeInSecs.get());
  }
  LOG.info(""String_Node_Str"",NAME);
}"
5078,"@Override public void apply(){
  if (upgradeVersionKeys(table,maxNumberUpdateRows.get())) {
    table.put(APP_VERSION_UPGRADE_KEY,COLUMN,Bytes.toBytes(ProjectInfo.getVersion().toString()));
  }
}","@Override public void apply() throws Exception {
  if (upgradeVersionKeys(finalMetaTable,maxNumberUpdateRows.get())) {
    finalMetaTable.put(APP_VERSION_UPGRADE_KEY,COLUMN,Bytes.toBytes(ProjectInfo.getVersion().toString()));
  }
}"
5079,"private Scanner getScannerWithPrefix(String keyPrefix){
  byte[] startKey=Bytes.toBytes(keyPrefix);
  byte[] endKey=Bytes.stopKeyForPrefix(startKey);
  return table.scan(startKey,endKey);
}","private Scanner getScannerWithPrefix(Table table,String keyPrefix){
  byte[] startKey=Bytes.toBytes(keyPrefix);
  byte[] endKey=Bytes.stopKeyForPrefix(startKey);
  return table.scan(startKey,endKey);
}"
5080,"/** 
 * @return a list of all the schedules and their states present in the store
 */
public synchronized List<StreamSizeScheduleState> list() throws InterruptedException, TransactionFailureException {
  final List<StreamSizeScheduleState> scheduleStates=Lists.newArrayList();
  factory.createExecutor(ImmutableList.of((TransactionAware)table)).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      try (Scanner scan=getScannerWithPrefix(KEY_PREFIX)){
        Row row;
        while ((row=scan.next()) != null) {
          byte[] scheduleBytes=row.get(SCHEDULE_COL);
          byte[] baseSizeBytes=row.get(BASE_SIZE_COL);
          byte[] baseTsBytes=row.get(BASE_TS_COL);
          byte[] lastRunSizeBytes=row.get(LAST_RUN_SIZE_COL);
          byte[] lastRunTsBytes=row.get(LAST_RUN_TS_COL);
          byte[] activeBytes=row.get(ACTIVE_COL);
          byte[] propertyBytes=row.get(PROPERTIES_COL);
          if (isInvalidRow(row)) {
            LIMITED_LOG.debug(""String_Node_Str"",Bytes.toString(row.getRow()));
            continue;
          }
          String rowKey=Bytes.toString(row.getRow());
          String[] splits=rowKey.split(""String_Node_Str"");
          ProgramId program;
          if (splits.length == 7) {
            program=new ApplicationId(splits[1],splits[2],splits[3]).program(ProgramType.valueOf(splits[4]),splits[5]);
          }
 else           if (splits.length == 6) {
            program=new ApplicationId(splits[1],splits[2]).program(ProgramType.valueOf(splits[3]),splits[4]);
          }
 else {
            continue;
          }
          SchedulableProgramType programType=program.getType().getSchedulableType();
          StreamSizeSchedule schedule=GSON.fromJson(Bytes.toString(scheduleBytes),StreamSizeSchedule.class);
          long baseSize=Bytes.toLong(baseSizeBytes);
          long baseTs=Bytes.toLong(baseTsBytes);
          long lastRunSize=Bytes.toLong(lastRunSizeBytes);
          long lastRunTs=Bytes.toLong(lastRunTsBytes);
          boolean active=Bytes.toBoolean(activeBytes);
          Map<String,String> properties=Maps.newHashMap();
          if (propertyBytes != null) {
            properties=GSON.fromJson(Bytes.toString(propertyBytes),STRING_MAP_TYPE);
          }
          StreamSizeScheduleState scheduleState=new StreamSizeScheduleState(program,programType,schedule,properties,baseSize,baseTs,lastRunSize,lastRunTs,active);
          scheduleStates.add(scheduleState);
          LOG.debug(""String_Node_Str"",scheduleState);
        }
      }
     }
  }
);
  return scheduleStates;
}","/** 
 * @return a list of all the schedules and their states present in the store
 */
public synchronized List<StreamSizeScheduleState> list() throws InterruptedException, TransactionFailureException {
  final List<StreamSizeScheduleState> scheduleStates=Lists.newArrayList();
  factory.createExecutor(ImmutableList.of((TransactionAware)table)).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      try (Scanner scan=getScannerWithPrefix(table,KEY_PREFIX)){
        Row row;
        while ((row=scan.next()) != null) {
          byte[] scheduleBytes=row.get(SCHEDULE_COL);
          byte[] baseSizeBytes=row.get(BASE_SIZE_COL);
          byte[] baseTsBytes=row.get(BASE_TS_COL);
          byte[] lastRunSizeBytes=row.get(LAST_RUN_SIZE_COL);
          byte[] lastRunTsBytes=row.get(LAST_RUN_TS_COL);
          byte[] activeBytes=row.get(ACTIVE_COL);
          byte[] propertyBytes=row.get(PROPERTIES_COL);
          if (isInvalidRow(row)) {
            LIMITED_LOG.debug(""String_Node_Str"",Bytes.toString(row.getRow()));
            continue;
          }
          String rowKey=Bytes.toString(row.getRow());
          String[] splits=rowKey.split(""String_Node_Str"");
          ProgramId program;
          if (splits.length == 7) {
            program=new ApplicationId(splits[1],splits[2],splits[3]).program(ProgramType.valueOf(splits[4]),splits[5]);
          }
 else           if (splits.length == 6) {
            program=new ApplicationId(splits[1],splits[2]).program(ProgramType.valueOf(splits[3]),splits[4]);
          }
 else {
            continue;
          }
          SchedulableProgramType programType=program.getType().getSchedulableType();
          StreamSizeSchedule schedule=GSON.fromJson(Bytes.toString(scheduleBytes),StreamSizeSchedule.class);
          long baseSize=Bytes.toLong(baseSizeBytes);
          long baseTs=Bytes.toLong(baseTsBytes);
          long lastRunSize=Bytes.toLong(lastRunSizeBytes);
          long lastRunTs=Bytes.toLong(lastRunTsBytes);
          boolean active=Bytes.toBoolean(activeBytes);
          Map<String,String> properties=Maps.newHashMap();
          if (propertyBytes != null) {
            properties=GSON.fromJson(Bytes.toString(propertyBytes),STRING_MAP_TYPE);
          }
          StreamSizeScheduleState scheduleState=new StreamSizeScheduleState(program,programType,schedule,properties,baseSize,baseTs,lastRunSize,lastRunTs,active);
          scheduleStates.add(scheduleState);
          LOG.debug(""String_Node_Str"",scheduleState);
        }
      }
     }
  }
);
  return scheduleStates;
}"
5081,"private boolean upgradeVersionKeys(Table table,int maxNumberUpdateRows){
  int numRowsUpgraded=0;
  try (Scanner scan=getScannerWithPrefix(KEY_PREFIX)){
    Row next;
    while (((next=scan.next()) != null) && (numRowsUpgraded < maxNumberUpdateRows)) {
      if (isInvalidRow(next)) {
        LIMITED_LOG.debug(""String_Node_Str"",Bytes.toString(next.getRow()));
        continue;
      }
      byte[] oldRowKey=next.getRow();
      String oldRowKeyString=Bytes.toString(next.getRow());
      String[] splits=oldRowKeyString.split(""String_Node_Str"");
      if (splits.length != 6) {
        LIMITED_LOG.debug(""String_Node_Str"" + ""String_Node_Str"",oldRowKeyString);
        continue;
      }
      byte[] newRowKey=Bytes.toBytes(ScheduleUpgradeUtil.getNameWithDefaultVersion(splits,3));
      Row row=table.get(newRowKey);
      if (!row.isEmpty()) {
        table.delete(oldRowKey);
        numRowsUpgraded++;
        continue;
      }
      Put put=new Put(newRowKey);
      for (      Map.Entry<byte[],byte[]> colValEntry : next.getColumns().entrySet()) {
        put.add(colValEntry.getKey(),colValEntry.getValue());
      }
      table.put(put);
      table.delete(oldRowKey);
      numRowsUpgraded++;
    }
  }
   return (numRowsUpgraded == 0);
}","private boolean upgradeVersionKeys(Table table,int maxNumberUpdateRows){
  int numRowsUpgraded=0;
  try (Scanner scan=getScannerWithPrefix(table,KEY_PREFIX)){
    Row next;
    while (((next=scan.next()) != null) && (numRowsUpgraded < maxNumberUpdateRows)) {
      if (isInvalidRow(next)) {
        LIMITED_LOG.debug(""String_Node_Str"",Bytes.toString(next.getRow()));
        continue;
      }
      byte[] oldRowKey=next.getRow();
      String oldRowKeyString=Bytes.toString(next.getRow());
      String[] splits=oldRowKeyString.split(""String_Node_Str"");
      if (splits.length != 6) {
        LIMITED_LOG.debug(""String_Node_Str"" + ""String_Node_Str"",oldRowKeyString);
        continue;
      }
      byte[] newRowKey=Bytes.toBytes(ScheduleUpgradeUtil.getNameWithDefaultVersion(splits,3));
      Row row=table.get(newRowKey);
      if (!row.isEmpty()) {
        table.delete(oldRowKey);
        numRowsUpgraded++;
        continue;
      }
      Put put=new Put(newRowKey);
      for (      Map.Entry<byte[],byte[]> colValEntry : next.getColumns().entrySet()) {
        put.add(colValEntry.getKey(),colValEntry.getValue());
      }
      table.put(put);
      table.delete(oldRowKey);
      numRowsUpgraded++;
    }
  }
   return (numRowsUpgraded == 0);
}"
5082,"/** 
 * Initialize this persistent store.
 */
public synchronized void initialize() throws IOException, DatasetManagementException {
  table=tableUtil.getMetaTable();
  Preconditions.checkNotNull(table,""String_Node_Str"",ScheduleStoreTableUtil.SCHEDULE_STORE_DATASET_NAME);
  upgradeCacheLoader=CacheBuilder.newBuilder().expireAfterWrite(1,TimeUnit.MINUTES).build(new UpgradeValueLoader(NAME,factory,table,storeInitialized));
  storeInitialized.set(true);
}","/** 
 * Initialize this persistent store.
 */
public synchronized void initialize() throws IOException, DatasetManagementException {
  table=tableUtil.getMetaTable();
  Preconditions.checkNotNull(table,""String_Node_Str"",ScheduleStoreTableUtil.SCHEDULE_STORE_DATASET_NAME);
  upgradeCacheLoader=CacheBuilder.newBuilder().expireAfterWrite(1,TimeUnit.MINUTES).build(new UpgradeValueLoader(NAME,factory,tableUtil.getMetaTable(),storeInitialized));
  storeInitialized.set(true);
}"
5083,"private void initializeScheduleTable() throws IOException, DatasetManagementException {
  table=tableUtil.getMetaTable();
  Preconditions.checkNotNull(table,""String_Node_Str"",ScheduleStoreTableUtil.SCHEDULE_STORE_DATASET_NAME);
  if (cacheLoaderInitialized.compareAndSet(false,true)) {
    upgradeCacheLoader=CacheBuilder.newBuilder().expireAfterWrite(1,TimeUnit.MINUTES).build(new UpgradeValueLoader(NAME,factory,table));
  }
}","private void initializeScheduleTable() throws IOException, DatasetManagementException {
  table=tableUtil.getMetaTable();
  Preconditions.checkNotNull(table,""String_Node_Str"",ScheduleStoreTableUtil.SCHEDULE_STORE_DATASET_NAME);
  if (cacheLoaderInitialized.compareAndSet(false,true)) {
    upgradeCacheLoader=CacheBuilder.newBuilder().expireAfterWrite(1,TimeUnit.MINUTES).build(new UpgradeValueLoader(NAME,factory,tableUtil.getMetaTable()));
  }
}"
5084,"private void doAddSchedule(HttpRequest request,HttpResponder responder,String namespaceId,String appId,String appVersion,String scheduleName) throws IOException, BadRequestException, NotFoundException, SchedulerException, AlreadyExistsException {
  ApplicationId applicationId=new ApplicationId(namespaceId,appId,appVersion);
  ScheduleSpecification scheduleSpecFromRequest;
  try (Reader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()),Charsets.UTF_8)){
    scheduleSpecFromRequest=GSON.fromJson(reader,ScheduleSpecification.class);
  }
 catch (  IOException e) {
    LOG.debug(""String_Node_Str"",e);
    throw new IOException(""String_Node_Str"");
  }
catch (  JsonSyntaxException e) {
    throw new BadRequestException(""String_Node_Str"" + e.getMessage());
  }
  if (scheduleSpecFromRequest.getSchedule().getName() != null && !scheduleName.equals(scheduleSpecFromRequest.getSchedule().getName())) {
    throw new BadRequestException(String.format(""String_Node_Str"",scheduleSpecFromRequest.getSchedule().getName(),scheduleName));
  }
  lifecycleService.addSchedule(applicationId,scheduleSpecFromRequest);
  responder.sendStatus(HttpResponseStatus.OK);
}","private void doAddSchedule(HttpRequest request,HttpResponder responder,String namespaceId,String appId,String appVersion,String scheduleName) throws IOException, BadRequestException, NotFoundException, SchedulerException, AlreadyExistsException {
  ApplicationId applicationId=new ApplicationId(namespaceId,appId,appVersion);
  ScheduleSpecification scheduleSpecFromRequest;
  try (Reader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()),Charsets.UTF_8)){
    scheduleSpecFromRequest=GSON.fromJson(reader,ScheduleSpecification.class);
  }
 catch (  IOException e) {
    LOG.debug(""String_Node_Str"",e);
    throw new IOException(""String_Node_Str"");
  }
catch (  JsonSyntaxException e) {
    throw new BadRequestException(""String_Node_Str"" + e.getMessage());
  }
  if (scheduleSpecFromRequest.getSchedule().getName() != null && !scheduleName.equals(scheduleSpecFromRequest.getSchedule().getName())) {
    throw new BadRequestException(String.format(""String_Node_Str"",scheduleSpecFromRequest.getSchedule().getName(),scheduleName));
  }
  if (scheduleSpecFromRequest.getSchedule().getName() == null) {
    scheduleSpecFromRequest=addNameToSpec(scheduleSpecFromRequest,scheduleName);
  }
  lifecycleService.addSchedule(applicationId,scheduleSpecFromRequest);
  responder.sendStatus(HttpResponseStatus.OK);
}"
5085,"private void testAddSchedule(ApplicationId appV2Id,String scheduleName) throws Exception {
  TimeSchedule timeSchedule=(TimeSchedule)Schedules.builder(scheduleName).setDescription(""String_Node_Str"").createTimeSchedule(""String_Node_Str"");
  ScheduleProgramInfo programInfo=new ScheduleProgramInfo(SchedulableProgramType.WORKFLOW,AppWithSchedule.WORKFLOW_NAME);
  ImmutableMap<String,String> properties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  ScheduleSpecification specification=new ScheduleSpecification(timeSchedule,programInfo,properties);
  HttpResponse response=addSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,null,""String_Node_Str"",specification);
  Assert.assertEquals(HttpResponseStatus.BAD_REQUEST.getCode(),response.getStatusLine().getStatusCode());
  response=addSchedule(TEST_NAMESPACE1,""String_Node_Str"",null,scheduleName,specification);
  Assert.assertEquals(HttpResponseStatus.NOT_FOUND.getCode(),response.getStatusLine().getStatusCode());
  ScheduleProgramInfo invalidScheduleProgramInfo=new ScheduleProgramInfo(SchedulableProgramType.SPARK,""String_Node_Str"");
  ScheduleSpecification invalidSpecification=new ScheduleSpecification(timeSchedule,invalidScheduleProgramInfo,properties);
  response=addSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,null,scheduleName,invalidSpecification);
  Assert.assertEquals(HttpResponseStatus.BAD_REQUEST.getCode(),response.getStatusLine().getStatusCode());
  TimeSchedule invalidTimeSchedule=(TimeSchedule)Schedules.builder(""String_Node_Str"").setDescription(""String_Node_Str"").createTimeSchedule(""String_Node_Str"");
  invalidSpecification=new ScheduleSpecification(invalidTimeSchedule,programInfo,properties);
  response=addSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,null,""String_Node_Str"",invalidSpecification);
  Assert.assertEquals(HttpResponseStatus.BAD_REQUEST.getCode(),response.getStatusLine().getStatusCode());
  response=addSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,null,scheduleName,specification);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  List<ScheduleSpecification> schedules=getSchedules(TEST_NAMESPACE1,AppWithSchedule.NAME,AppWithSchedule.WORKFLOW_NAME);
  Assert.assertEquals(2,schedules.size());
  response=addSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,null,scheduleName,specification);
  Assert.assertEquals(HttpResponseStatus.CONFLICT.getCode(),response.getStatusLine().getStatusCode());
  response=addSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,appV2Id.getVersion(),scheduleName,specification);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  List<ScheduleSpecification> actualSchSpecs=listSchedules(TEST_NAMESPACE1,appV2Id.getApplication(),appV2Id.getVersion());
  Assert.assertEquals(2,actualSchSpecs.size());
  Assert.assertTrue(actualSchSpecs.contains(specification));
}","private void testAddSchedule(ApplicationId appV2Id,String scheduleName) throws Exception {
  TimeSchedule timeSchedule=(TimeSchedule)Schedules.builder(scheduleName).setDescription(""String_Node_Str"").createTimeSchedule(""String_Node_Str"");
  ScheduleProgramInfo programInfo=new ScheduleProgramInfo(SchedulableProgramType.WORKFLOW,AppWithSchedule.WORKFLOW_NAME);
  ImmutableMap<String,String> properties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  ScheduleSpecification specification=new ScheduleSpecification(timeSchedule,programInfo,properties);
  HttpResponse response=addSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,null,""String_Node_Str"",specification);
  Assert.assertEquals(HttpResponseStatus.BAD_REQUEST.getCode(),response.getStatusLine().getStatusCode());
  response=addSchedule(TEST_NAMESPACE1,""String_Node_Str"",null,scheduleName,specification);
  Assert.assertEquals(HttpResponseStatus.NOT_FOUND.getCode(),response.getStatusLine().getStatusCode());
  ScheduleProgramInfo invalidScheduleProgramInfo=new ScheduleProgramInfo(SchedulableProgramType.SPARK,""String_Node_Str"");
  ScheduleSpecification invalidSpecification=new ScheduleSpecification(timeSchedule,invalidScheduleProgramInfo,properties);
  response=addSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,null,scheduleName,invalidSpecification);
  Assert.assertEquals(HttpResponseStatus.BAD_REQUEST.getCode(),response.getStatusLine().getStatusCode());
  TimeSchedule invalidTimeSchedule=(TimeSchedule)Schedules.builder(""String_Node_Str"").setDescription(""String_Node_Str"").createTimeSchedule(""String_Node_Str"");
  invalidSpecification=new ScheduleSpecification(invalidTimeSchedule,programInfo,properties);
  response=addSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,null,""String_Node_Str"",invalidSpecification);
  Assert.assertEquals(HttpResponseStatus.BAD_REQUEST.getCode(),response.getStatusLine().getStatusCode());
  response=addSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,null,scheduleName,specification);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  List<ScheduleSpecification> schedules=getSchedules(TEST_NAMESPACE1,AppWithSchedule.NAME,AppWithSchedule.WORKFLOW_NAME);
  Assert.assertEquals(2,schedules.size());
  response=addSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,null,scheduleName,specification);
  Assert.assertEquals(HttpResponseStatus.CONFLICT.getCode(),response.getStatusLine().getStatusCode());
  response=addSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,appV2Id.getVersion(),scheduleName,specification);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  List<ScheduleSpecification> actualSchSpecs=listSchedules(TEST_NAMESPACE1,appV2Id.getApplication(),appV2Id.getVersion());
  Assert.assertEquals(2,actualSchSpecs.size());
  Assert.assertTrue(actualSchSpecs.contains(specification));
  TimeSchedule timeSchedule2=(TimeSchedule)Schedules.builder(null).setDescription(""String_Node_Str"").createTimeSchedule(""String_Node_Str"");
  ScheduleSpecification specification2=new ScheduleSpecification(timeSchedule2,programInfo,properties);
  response=addSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,appV2Id.getVersion(),""String_Node_Str"",specification2);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  ScheduleSpecification schedule2=getSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,appV2Id.getVersion(),""String_Node_Str"");
  Assert.assertNotNull(schedule2);
}"
5086,"private void testDeleteSchedule(ApplicationId appV2Id,String scheduleName) throws Exception {
  HttpResponse response=deleteSchedule(TEST_NAMESPACE1,""String_Node_Str"",null,scheduleName);
  Assert.assertEquals(HttpResponseStatus.NOT_FOUND.getCode(),response.getStatusLine().getStatusCode());
  response=deleteSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,null,""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.NOT_FOUND.getCode(),response.getStatusLine().getStatusCode());
  response=deleteSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,null,scheduleName);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  List<ScheduleSpecification> schedules=getSchedules(TEST_NAMESPACE1,AppWithSchedule.NAME,AppWithSchedule.WORKFLOW_NAME);
  Assert.assertEquals(1,schedules.size());
  schedules=getSchedules(TEST_NAMESPACE1,AppWithSchedule.NAME,appV2Id.getVersion(),AppWithSchedule.WORKFLOW_NAME);
  Assert.assertEquals(2,schedules.size());
  boolean foundSchedule=false;
  for (  ScheduleSpecification schedule : schedules) {
    if (schedule.getSchedule().getName().equals(scheduleName)) {
      foundSchedule=true;
    }
  }
  Assert.assertTrue(String.format(""String_Node_Str"",scheduleName),foundSchedule);
  response=deleteSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,appV2Id.getVersion(),scheduleName);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  schedules=getSchedules(TEST_NAMESPACE1,AppWithSchedule.NAME,appV2Id.getVersion(),AppWithSchedule.WORKFLOW_NAME);
  Assert.assertEquals(1,schedules.size());
}","private void testDeleteSchedule(ApplicationId appV2Id,String scheduleName) throws Exception {
  HttpResponse response=deleteSchedule(TEST_NAMESPACE1,""String_Node_Str"",null,scheduleName);
  Assert.assertEquals(HttpResponseStatus.NOT_FOUND.getCode(),response.getStatusLine().getStatusCode());
  response=deleteSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,null,""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.NOT_FOUND.getCode(),response.getStatusLine().getStatusCode());
  response=deleteSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,null,scheduleName);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  List<ScheduleSpecification> schedules=getSchedules(TEST_NAMESPACE1,AppWithSchedule.NAME,AppWithSchedule.WORKFLOW_NAME);
  Assert.assertEquals(1,schedules.size());
  schedules=getSchedules(TEST_NAMESPACE1,AppWithSchedule.NAME,appV2Id.getVersion(),AppWithSchedule.WORKFLOW_NAME);
  Assert.assertEquals(3,schedules.size());
  boolean foundSchedule=false;
  for (  ScheduleSpecification schedule : schedules) {
    if (schedule.getSchedule().getName().equals(scheduleName)) {
      foundSchedule=true;
    }
  }
  Assert.assertTrue(String.format(""String_Node_Str"",scheduleName),foundSchedule);
  response=deleteSchedule(TEST_NAMESPACE1,AppWithSchedule.NAME,appV2Id.getVersion(),scheduleName);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  schedules=getSchedules(TEST_NAMESPACE1,AppWithSchedule.NAME,appV2Id.getVersion(),AppWithSchedule.WORKFLOW_NAME);
  Assert.assertEquals(2,schedules.size());
}"
5087,"@Override protected void configure(){
  bind(PipelineFactory.class).to(SynchronousPipelineFactory.class);
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<AppDeploymentInfo,ApplicationWithPrograms>>(){
  }
,new TypeLiteral<LocalApplicationManager<AppDeploymentInfo,ApplicationWithPrograms>>(){
  }
).build(new TypeLiteral<ManagerFactory<AppDeploymentInfo,ApplicationWithPrograms>>(){
  }
));
  bind(Store.class).to(DefaultStore.class);
  bind(RuntimeStore.class).to(DefaultStore.class);
  bind(ArtifactStore.class).in(Scopes.SINGLETON);
  bind(ProgramLifecycleService.class).in(Scopes.SINGLETON);
  bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
  Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.AppFabric.HANDLERS_BINDING));
  CommonHandlers.add(handlerBinder);
  handlerBinder.addBinding().to(ConfigHandler.class);
  handlerBinder.addBinding().to(VersionHandler.class);
  handlerBinder.addBinding().to(MonitorHandler.class);
  handlerBinder.addBinding().to(UsageHandler.class);
  handlerBinder.addBinding().to(NamespaceHttpHandler.class);
  handlerBinder.addBinding().to(NotificationFeedHttpHandler.class);
  handlerBinder.addBinding().to(AppLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(DashboardHttpHandler.class);
  handlerBinder.addBinding().to(ProgramLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(PreferencesHttpHandler.class);
  handlerBinder.addBinding().to(ConsoleSettingsHttpHandler.class);
  handlerBinder.addBinding().to(TransactionHttpHandler.class);
  handlerBinder.addBinding().to(WorkflowHttpHandler.class);
  handlerBinder.addBinding().to(ArtifactHttpHandler.class);
  handlerBinder.addBinding().to(WorkflowStatsSLAHttpHandler.class);
  handlerBinder.addBinding().to(AuthorizationHandler.class);
  handlerBinder.addBinding().to(SecureStoreHandler.class);
  handlerBinder.addBinding().to(RemotePrivilegesHandler.class);
  handlerBinder.addBinding().to(RouteConfigHttpHandler.class);
  handlerBinder.addBinding().to(OperationalStatsHttpHandler.class);
  for (  Class<? extends HttpHandler> handlerClass : handlerClasses) {
    handlerBinder.addBinding().to(handlerClass);
  }
}","@Override protected void configure(){
  bind(PipelineFactory.class).to(SynchronousPipelineFactory.class);
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<AppDeploymentInfo,ApplicationWithPrograms>>(){
  }
,new TypeLiteral<LocalApplicationManager<AppDeploymentInfo,ApplicationWithPrograms>>(){
  }
).build(new TypeLiteral<ManagerFactory<AppDeploymentInfo,ApplicationWithPrograms>>(){
  }
));
  bind(Store.class).to(DefaultStore.class);
  bind(RuntimeStore.class).to(DefaultStore.class);
  bind(ArtifactStore.class).in(Scopes.SINGLETON);
  bind(ProgramLifecycleService.class).in(Scopes.SINGLETON);
  bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
  Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.AppFabric.HANDLERS_BINDING));
  CommonHandlers.add(handlerBinder);
  handlerBinder.addBinding().to(ConfigHandler.class);
  handlerBinder.addBinding().to(VersionHandler.class);
  handlerBinder.addBinding().to(MonitorHandler.class);
  handlerBinder.addBinding().to(UsageHandler.class);
  handlerBinder.addBinding().to(NamespaceHttpHandler.class);
  handlerBinder.addBinding().to(NotificationFeedHttpHandler.class);
  handlerBinder.addBinding().to(AppLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(DashboardHttpHandler.class);
  handlerBinder.addBinding().to(ProgramLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(PreferencesHttpHandler.class);
  handlerBinder.addBinding().to(ConsoleSettingsHttpHandler.class);
  handlerBinder.addBinding().to(TransactionHttpHandler.class);
  handlerBinder.addBinding().to(WorkflowHttpHandler.class);
  handlerBinder.addBinding().to(ArtifactHttpHandler.class);
  handlerBinder.addBinding().to(WorkflowStatsSLAHttpHandler.class);
  handlerBinder.addBinding().to(AuthorizationHandler.class);
  handlerBinder.addBinding().to(SecureStoreHandler.class);
  handlerBinder.addBinding().to(RemotePrivilegesHandler.class);
  handlerBinder.addBinding().to(UpgradeHttpHandler.class);
  handlerBinder.addBinding().to(RouteConfigHttpHandler.class);
  handlerBinder.addBinding().to(OperationalStatsHttpHandler.class);
  for (  Class<? extends HttpHandler> handlerClass : handlerClasses) {
    handlerBinder.addBinding().to(handlerClass);
  }
}"
5088,"/** 
 * Method to add version in StreamSizeSchedule row key in SchedulerStore.
 * @throws Exception
 */
public void upgrade() throws InterruptedException, TransactionFailureException, IOException, DatasetManagementException {
  initialize();
  factory.createExecutor(ImmutableList.of((TransactionAware)table)).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply(){
      upgradeVersionKeys();
    }
  }
);
}","/** 
 * Method to add version in StreamSizeSchedule row key in SchedulerStore.
 * @throws InterruptedException
 * @throws IOException
 * @throws DatasetManagementException
 */
public void upgrade() throws InterruptedException, IOException, DatasetManagementException {
  while (!storeInitialized.get()) {
    TimeUnit.SECONDS.sleep(10);
  }
  if (isUpgradeComplete()) {
    LOG.info(""String_Node_Str"",NAME);
    return;
  }
  final AtomicInteger maxNumberUpdateRows=new AtomicInteger(1000);
  final AtomicInteger sleepTimeInSecs=new AtomicInteger(60);
  LOG.info(""String_Node_Str"",NAME);
  while (!isUpgradeComplete()) {
    sleepTimeInSecs.set(60);
    try {
      factory.createExecutor(ImmutableList.of((TransactionAware)table)).execute(new TransactionExecutor.Subroutine(){
        @Override public void apply(){
          if (upgradeVersionKeys(table,maxNumberUpdateRows.get())) {
            table.put(APP_VERSION_UPGRADE_KEY,COLUMN,Bytes.toBytes(ProjectInfo.getVersion().toString()));
          }
        }
      }
);
    }
 catch (    TransactionFailureException e) {
      if (e instanceof TransactionConflictException) {
        LOG.debug(""String_Node_Str"",e);
        sleepTimeInSecs.set(10);
      }
 else       if (e instanceof TransactionNotInProgressException) {
        int currMaxRows=maxNumberUpdateRows.get();
        if (currMaxRows > 500) {
          maxNumberUpdateRows.decrementAndGet();
        }
 else {
          LOG.warn(""String_Node_Str"",NAME);
          return;
        }
        sleepTimeInSecs.set(10);
        LOG.debug(""String_Node_Str"" + ""String_Node_Str"",maxNumberUpdateRows.get(),e);
      }
 else {
        LOG.error(""String_Node_Str"",e);
        sleepTimeInSecs.set(60);
      }
    }
    TimeUnit.SECONDS.sleep(sleepTimeInSecs.get());
  }
  LOG.info(""String_Node_Str"",NAME);
}"
5089,"@Override public void apply(){
  upgradeVersionKeys();
}","@Override public void apply(){
  if (upgradeVersionKeys(table,maxNumberUpdateRows.get())) {
    table.put(APP_VERSION_UPGRADE_KEY,COLUMN,Bytes.toBytes(ProjectInfo.getVersion().toString()));
  }
}"
5090,"/** 
 * Remove a schedule from the store.
 * @param programId program id the schedule is running for
 * @param programType program type the schedule is running for
 * @param scheduleName name of the schedule
 */
public synchronized void delete(final ProgramId programId,final SchedulableProgramType programType,final String scheduleName) throws InterruptedException, TransactionFailureException {
  factory.createExecutor(ImmutableList.of((TransactionAware)table)).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      String rowKey=getRowKey(programId,programType,scheduleName);
      String versionLessRowKey=removeAppVersion(rowKey);
      if (versionLessRowKey != null) {
        table.delete(Bytes.toBytes(versionLessRowKey));
      }
      table.delete(Bytes.toBytes(rowKey));
    }
  }
);
}","/** 
 * Remove a schedule from the store.
 * @param programId program id the schedule is running for
 * @param programType program type the schedule is running for
 * @param scheduleName name of the schedule
 */
public synchronized void delete(final ProgramId programId,final SchedulableProgramType programType,final String scheduleName) throws InterruptedException, TransactionFailureException {
  final boolean needVersionLessDelete=!isUpgradeComplete();
  factory.createExecutor(ImmutableList.of((TransactionAware)table)).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      String rowKey=getRowKey(programId,programType,scheduleName);
      if (needVersionLessDelete) {
        String versionLessRowKey=removeAppVersion(rowKey);
        if (versionLessRowKey != null) {
          table.delete(Bytes.toBytes(versionLessRowKey));
        }
      }
      table.delete(Bytes.toBytes(rowKey));
    }
  }
);
}"
5091,"private void upgradeVersionKeys(){
  Joiner joiner=Joiner.on(""String_Node_Str"");
  try (Scanner scan=getScannerWithPrefix(KEY_PREFIX)){
    Row next;
    while ((next=scan.next()) != null) {
      if (isInvalidRow(next)) {
        LIMITED_LOG.debug(""String_Node_Str"",Bytes.toString(next.getRow()));
        continue;
      }
      byte[] oldRowKey=next.getRow();
      String oldRowKeyString=Bytes.toString(next.getRow());
      String[] splits=oldRowKeyString.split(""String_Node_Str"");
      if (splits.length != 6) {
        LOG.debug(""String_Node_Str"" + ""String_Node_Str"",oldRowKeyString);
        continue;
      }
      List<String> splitsList=new ArrayList<>(Arrays.asList(splits));
      splitsList.add(3,ApplicationId.DEFAULT_VERSION);
      String newRowKeyString=joiner.join(splitsList);
      byte[] newRowKey=Bytes.toBytes(newRowKeyString);
      Put put=new Put(newRowKey);
      for (      Map.Entry<byte[],byte[]> colValEntry : next.getColumns().entrySet()) {
        put.add(colValEntry.getKey(),colValEntry.getValue());
      }
      table.put(put);
      table.delete(oldRowKey);
    }
  }
 }","private boolean upgradeVersionKeys(Table table,int maxNumberUpdateRows){
  int numRowsUpgraded=0;
  try (Scanner scan=getScannerWithPrefix(KEY_PREFIX)){
    Row next;
    while (((next=scan.next()) != null) && (numRowsUpgraded < maxNumberUpdateRows)) {
      if (isInvalidRow(next)) {
        LIMITED_LOG.debug(""String_Node_Str"",Bytes.toString(next.getRow()));
        continue;
      }
      byte[] oldRowKey=next.getRow();
      String oldRowKeyString=Bytes.toString(next.getRow());
      String[] splits=oldRowKeyString.split(""String_Node_Str"");
      if (splits.length != 6) {
        LIMITED_LOG.debug(""String_Node_Str"" + ""String_Node_Str"",oldRowKeyString);
        continue;
      }
      byte[] newRowKey=Bytes.toBytes(ScheduleUpgradeUtil.getNameWithDefaultVersion(splits,3));
      Row row=table.get(newRowKey);
      if (!row.isEmpty()) {
        table.delete(oldRowKey);
        numRowsUpgraded++;
        continue;
      }
      Put put=new Put(newRowKey);
      for (      Map.Entry<byte[],byte[]> colValEntry : next.getColumns().entrySet()) {
        put.add(colValEntry.getKey(),colValEntry.getValue());
      }
      table.put(put);
      table.delete(oldRowKey);
      numRowsUpgraded++;
    }
  }
   return (numRowsUpgraded == 0);
}"
5092,"/** 
 * Initialize this persistent store.
 */
public synchronized void initialize() throws IOException, DatasetManagementException {
  table=tableUtil.getMetaTable();
  Preconditions.checkNotNull(table,""String_Node_Str"",ScheduleStoreTableUtil.SCHEDULE_STORE_DATASET_NAME);
}","/** 
 * Initialize this persistent store.
 */
public synchronized void initialize() throws IOException, DatasetManagementException {
  table=tableUtil.getMetaTable();
  Preconditions.checkNotNull(table,""String_Node_Str"",ScheduleStoreTableUtil.SCHEDULE_STORE_DATASET_NAME);
  upgradeCacheLoader=CacheBuilder.newBuilder().expireAfterWrite(1,TimeUnit.MINUTES).build(new UpgradeValueLoader(NAME,factory,table,storeInitialized));
  storeInitialized.set(true);
}"
5093,"@Inject public DatasetBasedStreamSizeScheduleStore(TransactionExecutorFactory factory,ScheduleStoreTableUtil tableUtil){
  this.tableUtil=tableUtil;
  this.factory=factory;
}","@Inject public DatasetBasedStreamSizeScheduleStore(TransactionExecutorFactory factory,ScheduleStoreTableUtil tableUtil){
  this.tableUtil=tableUtil;
  this.factory=factory;
  this.storeInitialized=new AtomicBoolean(false);
}"
5094,"@Override public void execute(JobExecutionContext context) throws JobExecutionException {
  LOG.debug(""String_Node_Str"",context.getJobDetail().getKey().toString(),context.getTrigger().getKey().toString());
  Trigger trigger=context.getTrigger();
  String key=trigger.getKey().getName();
  String[] parts=key.split(""String_Node_Str"");
  Preconditions.checkArgument(parts.length == 6,""String_Node_Str"",key,parts.length);
  String namespaceId=parts[0];
  String applicationId=parts[1];
  String appVersion=parts[2];
  ProgramType programType=ProgramType.valueOf(parts[3]);
  String programName=parts[4];
  String scheduleName=parts[5];
  LOG.debug(""String_Node_Str"",key);
  ImmutableMap.Builder<String,String> builder=ImmutableMap.builder();
  builder.put(ProgramOptionConstants.RETRY_COUNT,Integer.toString(context.getRefireCount()));
  builder.put(ProgramOptionConstants.SCHEDULE_NAME,scheduleName);
  Map<String,String> userOverrides=ImmutableMap.of(ProgramOptionConstants.LOGICAL_START_TIME,Long.toString(context.getScheduledFireTime().getTime()));
  JobDataMap jobDataMap=trigger.getJobDataMap();
  for (  Map.Entry<String,Object> entry : jobDataMap.entrySet()) {
    builder.put(entry.getKey(),jobDataMap.getString(entry.getKey()));
  }
  ProgramId programId=new ApplicationId(namespaceId,applicationId,appVersion).program(programType,programName);
  try {
    taskRunner.run(programId,builder.build(),userOverrides).get();
  }
 catch (  TaskExecutionException e) {
    LOG.warn(""String_Node_Str"",programId,e);
    throw new JobExecutionException(e.getMessage(),e.getCause(),e.isRefireImmediately());
  }
catch (  Throwable t) {
    LOG.warn(""String_Node_Str"",programId,t);
    throw new JobExecutionException(t.getMessage(),t.getCause(),false);
  }
}","@Override public void execute(JobExecutionContext context) throws JobExecutionException {
  LOG.debug(""String_Node_Str"",context.getJobDetail().getKey().toString(),context.getTrigger().getKey().toString());
  Trigger trigger=context.getTrigger();
  String key=trigger.getKey().getName();
  String[] parts=key.split(""String_Node_Str"");
  Preconditions.checkArgument(parts.length == 6,String.format(""String_Node_Str"",key,parts.length));
  String namespaceId=parts[0];
  String applicationId=parts[1];
  String appVersion=parts[2];
  ProgramType programType=ProgramType.valueOf(parts[3]);
  String programName=parts[4];
  String scheduleName=parts[5];
  LOG.debug(""String_Node_Str"",key);
  ImmutableMap.Builder<String,String> builder=ImmutableMap.builder();
  builder.put(ProgramOptionConstants.RETRY_COUNT,Integer.toString(context.getRefireCount()));
  builder.put(ProgramOptionConstants.SCHEDULE_NAME,scheduleName);
  Map<String,String> userOverrides=ImmutableMap.of(ProgramOptionConstants.LOGICAL_START_TIME,Long.toString(context.getScheduledFireTime().getTime()));
  JobDataMap jobDataMap=trigger.getJobDataMap();
  for (  Map.Entry<String,Object> entry : jobDataMap.entrySet()) {
    builder.put(entry.getKey(),jobDataMap.getString(entry.getKey()));
  }
  ProgramId programId=new ApplicationId(namespaceId,applicationId,appVersion).program(programType,programName);
  try {
    taskRunner.run(programId,builder.build(),userOverrides).get();
  }
 catch (  TaskExecutionException e) {
    LOG.warn(""String_Node_Str"",programId,e);
    throw new JobExecutionException(e.getMessage(),e.getCause(),e.isRefireImmediately());
  }
catch (  Throwable t) {
    LOG.warn(""String_Node_Str"",programId,t);
    throw new JobExecutionException(t.getMessage(),t.getCause(),false);
  }
}"
5095,"@Override public void initialize() throws Exception {
  SparkClientContext context=getContext();
  SparkConf sparkConf=new SparkConf();
  Map<String,String> programProperties=context.getSpecification().getProperties();
  String extraOpts=programProperties.get(EXTRA_OPTS);
  if (extraOpts != null && !extraOpts.isEmpty()) {
    sparkConf.set(""String_Node_Str"",extraOpts);
    sparkConf.set(""String_Node_Str"",extraOpts);
  }
  Integer numSources=Integer.valueOf(programProperties.get(NUM_SOURCES));
  sparkConf.set(""String_Node_Str"",String.valueOf(numSources + 2));
  Boolean isUnitTest=Boolean.valueOf(programProperties.get(IS_UNIT_TEST));
  if (isUnitTest) {
    sparkConf.setMaster(String.format(""String_Node_Str"",numSources + 1));
  }
  context.setSparkConf(sparkConf);
  boolean checkpointsDisabled=Boolean.valueOf(programProperties.get(CHECKPOINTS_DISABLED));
  if (!checkpointsDisabled) {
    FileSet checkpointFileSet=context.getDataset(DataStreamsApp.CHECKPOINT_FILESET);
    String pipelineName=context.getApplicationSpecification().getName();
    String checkpointDir=context.getSpecification().getProperty(CHECKPOINT_DIR);
    Location pipelineCheckpointBase=checkpointFileSet.getBaseLocation().append(pipelineName);
    Location pipelineCheckpointDir=pipelineCheckpointBase.append(checkpointDir);
    if (!ensureDirExists(pipelineCheckpointBase)) {
      throw new IOException(String.format(""String_Node_Str"",pipelineCheckpointBase));
    }
    try {
      for (      Location child : pipelineCheckpointBase.list()) {
        if (!child.equals(pipelineCheckpointDir) && !child.delete(true)) {
          LOG.warn(""String_Node_Str"",child);
        }
      }
    }
 catch (    Exception e) {
      LOG.warn(""String_Node_Str"",e);
    }
    if (!ensureDirExists(pipelineCheckpointDir)) {
      throw new IOException(String.format(""String_Node_Str"",pipelineCheckpointDir));
    }
  }
}","@Override public void initialize() throws Exception {
  SparkClientContext context=getContext();
  String arguments=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(context.getRuntimeArguments());
  WRAPPERLOGGER.info(""String_Node_Str"",context.getApplicationSpecification().getName(),UserGroupInformation.getCurrentUser().getShortUserName(),arguments);
  SparkConf sparkConf=new SparkConf();
  Map<String,String> programProperties=context.getSpecification().getProperties();
  String extraOpts=programProperties.get(EXTRA_OPTS);
  if (extraOpts != null && !extraOpts.isEmpty()) {
    sparkConf.set(""String_Node_Str"",extraOpts);
    sparkConf.set(""String_Node_Str"",extraOpts);
  }
  Integer numSources=Integer.valueOf(programProperties.get(NUM_SOURCES));
  sparkConf.set(""String_Node_Str"",String.valueOf(numSources + 2));
  Boolean isUnitTest=Boolean.valueOf(programProperties.get(IS_UNIT_TEST));
  if (isUnitTest) {
    sparkConf.setMaster(String.format(""String_Node_Str"",numSources + 1));
  }
  context.setSparkConf(sparkConf);
  boolean checkpointsDisabled=Boolean.valueOf(programProperties.get(CHECKPOINTS_DISABLED));
  if (!checkpointsDisabled) {
    FileSet checkpointFileSet=context.getDataset(DataStreamsApp.CHECKPOINT_FILESET);
    String pipelineName=context.getApplicationSpecification().getName();
    String checkpointDir=context.getSpecification().getProperty(CHECKPOINT_DIR);
    Location pipelineCheckpointBase=checkpointFileSet.getBaseLocation().append(pipelineName);
    Location pipelineCheckpointDir=pipelineCheckpointBase.append(checkpointDir);
    if (!ensureDirExists(pipelineCheckpointBase)) {
      throw new IOException(String.format(""String_Node_Str"",pipelineCheckpointBase));
    }
    try {
      for (      Location child : pipelineCheckpointBase.list()) {
        if (!child.equals(pipelineCheckpointDir) && !child.delete(true)) {
          LOG.warn(""String_Node_Str"",child);
        }
      }
    }
 catch (    Exception e) {
      LOG.warn(""String_Node_Str"",e);
    }
    if (!ensureDirExists(pipelineCheckpointDir)) {
      throw new IOException(String.format(""String_Node_Str"",pipelineCheckpointDir));
    }
  }
  WRAPPERLOGGER.info(""String_Node_Str"",context.getApplicationSpecification().getName());
}"
5096,"/** 
 * Set the field to the given value.
 * @param fieldName Name of the field to set.
 * @param value Value for the field.
 * @return This builder.
 * @throws UnexpectedFormatException if the field is not in the schema, or the field is not nullable but a nullvalue is given.
 */
public Builder set(String fieldName,Object value){
  validateAndGetField(fieldName,value);
  fields.put(fieldName,value);
  return this;
}","/** 
 * Set the field to the given value.
 * @param fieldName Name of the field to set
 * @param value Value for the field
 * @return This builder
 * @throws UnexpectedFormatException if the field is not in the schema, or the field is not nullable but a nullvalue is given
 */
public Builder set(String fieldName,@Nullable Object value){
  validateAndGetField(fieldName,value);
  fields.put(fieldName,value);
  return this;
}"
5097,"/** 
 * Build a   {@link StructuredRecord} with the fields set by this builder.
 * @return A {@link StructuredRecord} with the fields set by this builder.
 * @throws UnexpectedFormatException if there is at least one non-nullable field without a value.
 */
public StructuredRecord build() throws UnexpectedFormatException {
  for (  Schema.Field field : schema.getFields()) {
    String fieldName=field.getName();
    if (!fields.containsKey(fieldName)) {
      if (!field.getSchema().isNullable()) {
        throw new UnexpectedFormatException(""String_Node_Str"" + fieldName + ""String_Node_Str"");
      }
 else {
        fields.put(fieldName,null);
      }
    }
  }
  return new StructuredRecord(schema,fields);
}","/** 
 * Build a   {@link StructuredRecord} with the fields set by this builder.
 * @return A {@link StructuredRecord} with the fields set by this builder
 * @throws UnexpectedFormatException if there is at least one non-nullable field without a value
 */
public StructuredRecord build() throws UnexpectedFormatException {
  for (  Schema.Field field : schema.getFields()) {
    String fieldName=field.getName();
    if (!fields.containsKey(fieldName)) {
      if (!field.getSchema().isNullable()) {
        throw new UnexpectedFormatException(""String_Node_Str"" + fieldName + ""String_Node_Str"");
      }
 else {
        fields.put(fieldName,null);
      }
    }
  }
  return new StructuredRecord(schema,fields);
}"
5098,"/** 
 * Convert the given string into the type of the given field, and set the value for that field. A String can be converted to a boolean, int, long, float, double, bytes, string, or null.
 * @param fieldName Name of the field to set.
 * @param strVal String value for the field.
 * @return This builder.
 * @throws UnexpectedFormatException if the field is not in the schema, or the field is not nullable but a nullvalue is given, or the string cannot be converted to the type for the field.
 */
public Builder convertAndSet(String fieldName,String strVal) throws UnexpectedFormatException {
  Schema.Field field=validateAndGetField(fieldName,strVal);
  fields.put(fieldName,convertString(field.getSchema(),strVal));
  return this;
}","/** 
 * Convert the given string into the type of the given field, and set the value for that field. A String can be converted to a boolean, int, long, float, double, bytes, string, or null.
 * @param fieldName Name of the field to set
 * @param strVal String value for the field
 * @return This builder
 * @throws UnexpectedFormatException if the field is not in the schema, or the field is not nullable but a nullvalue is given, or the string cannot be converted to the type for the field
 */
public Builder convertAndSet(String fieldName,@Nullable String strVal) throws UnexpectedFormatException {
  Schema.Field field=validateAndGetField(fieldName,strVal);
  fields.put(fieldName,convertString(field.getSchema(),strVal));
  return this;
}"
5099,"public WorkflowBackedActionContext(WorkflowContext workflowContext,Metrics metrics,LookupProvider lookup,long logicalStartTime,Map<String,String> runtimeArgs,StageInfo stageInfo){
  super(workflowContext,workflowContext,metrics,lookup,logicalStartTime,runtimeArgs,workflowContext.getAdmin(),stageInfo);
  this.workflowContext=workflowContext;
}","public WorkflowBackedActionContext(WorkflowContext workflowContext,Metrics metrics,LookupProvider lookup,long logicalStartTime,Map<String,String> runtimeArgs,StageInfo stageInfo){
  super(workflowContext,workflowContext,workflowContext,metrics,lookup,logicalStartTime,runtimeArgs,workflowContext.getAdmin(),stageInfo);
  this.workflowContext=workflowContext;
}"
5100,"public BasicActionContext(CustomActionContext context,Metrics metrics,String stageName){
  super(context,metrics,StageInfo.builder(stageName,Action.PLUGIN_TYPE).build());
  this.context=context;
  this.arguments=new BasicSettableArguments(context.getRuntimeArguments());
}","public BasicActionContext(CustomActionContext context,Metrics metrics,String stageName){
  super(context,context,metrics,StageInfo.builder(stageName,Action.PLUGIN_TYPE).build());
  this.context=context;
  this.arguments=new BasicSettableArguments(context.getRuntimeArguments());
}"
5101,"protected AbstractAggregatorContext(PluginContext pluginContext,DatasetContext datasetContext,Metrics metrics,LookupProvider lookup,long logicalStartTime,Map<String,String> runtimeArgs,Admin admin,StageInfo stageInfo){
  super(pluginContext,datasetContext,metrics,lookup,logicalStartTime,runtimeArgs,admin,stageInfo);
}","protected AbstractAggregatorContext(PluginContext pluginContext,ServiceDiscoverer serviceDiscoverer,DatasetContext datasetContext,Metrics metrics,LookupProvider lookup,long logicalStartTime,Map<String,String> runtimeArgs,Admin admin,StageInfo stageInfo){
  super(pluginContext,serviceDiscoverer,datasetContext,metrics,lookup,logicalStartTime,runtimeArgs,admin,stageInfo);
}"
5102,"protected <T extends PluginContext & DatasetContext>AbstractBatchContext(T context,Metrics metrics,LookupProvider lookup,long logicalStartTime,Map<String,String> runtimeArgs,Admin admin,StageInfo stageInfo){
  super(context,metrics,lookup,stageInfo);
  this.datasetContext=context;
  this.logicalStartTime=logicalStartTime;
  this.runtimeArgs=runtimeArgs;
  this.admin=admin;
}","protected <T extends PluginContext & DatasetContext>AbstractBatchContext(T context,ServiceDiscoverer serviceDiscoverer,Metrics metrics,LookupProvider lookup,long logicalStartTime,Map<String,String> runtimeArgs,Admin admin,StageInfo stageInfo){
  super(context,serviceDiscoverer,metrics,lookup,stageInfo);
  this.datasetContext=context;
  this.logicalStartTime=logicalStartTime;
  this.runtimeArgs=runtimeArgs;
  this.admin=admin;
}"
5103,"protected AbstractJoinerContext(PluginContext pluginContext,DatasetContext datasetContext,Metrics metrics,LookupProvider lookup,long logicalStartTime,Map<String,String> runtimeArgs,Admin admin,StageInfo stageInfo){
  super(pluginContext,datasetContext,metrics,lookup,logicalStartTime,runtimeArgs,admin,stageInfo);
}","protected AbstractJoinerContext(PluginContext pluginContext,ServiceDiscoverer serviceDiscoverer,DatasetContext datasetContext,Metrics metrics,LookupProvider lookup,long logicalStartTime,Map<String,String> runtimeArgs,Admin admin,StageInfo stageInfo){
  super(pluginContext,serviceDiscoverer,datasetContext,metrics,lookup,logicalStartTime,runtimeArgs,admin,stageInfo);
}"
5104,"public MapReduceAggregatorContext(MapReduceContext context,Metrics metrics,LookupProvider lookup,Map<String,String> runtimeArgs,StageInfo stageInfo){
  super(context,context,metrics,lookup,context.getLogicalStartTime(),runtimeArgs,context.getAdmin(),stageInfo);
  this.mrContext=context;
}","public MapReduceAggregatorContext(MapReduceContext context,Metrics metrics,LookupProvider lookup,Map<String,String> runtimeArgs,StageInfo stageInfo){
  super(context,context,context,metrics,lookup,context.getLogicalStartTime(),runtimeArgs,context.getAdmin(),stageInfo);
  this.mrContext=context;
}"
5105,"public MapReduceBatchContext(MapReduceContext context,Metrics metrics,LookupProvider lookup,Map<String,String> runtimeArguments,StageInfo stageInfo){
  super(context,metrics,lookup,context.getLogicalStartTime(),runtimeArguments,context.getAdmin(),stageInfo);
  this.mrContext=context;
}","public MapReduceBatchContext(MapReduceContext context,Metrics metrics,LookupProvider lookup,Map<String,String> runtimeArguments,StageInfo stageInfo){
  super(context,context,metrics,lookup,context.getLogicalStartTime(),runtimeArguments,context.getAdmin(),stageInfo);
  this.mrContext=context;
}"
5106,"public MapReduceRuntimeContext(MapReduceTaskContext context,Metrics metrics,LookupProvider lookup,Map<String,String> runtimeArgs,StageInfo stageInfo){
  super(context,metrics,lookup,stageInfo);
  this.context=context;
  this.runtimeArgs=ImmutableMap.copyOf(runtimeArgs);
}","public MapReduceRuntimeContext(MapReduceTaskContext context,Metrics metrics,LookupProvider lookup,Map<String,String> runtimeArgs,StageInfo stageInfo){
  super(context,context,metrics,lookup,stageInfo);
  this.context=context;
  this.runtimeArgs=ImmutableMap.copyOf(runtimeArgs);
}"
5107,"/** 
 * This method is used to generate the logs for program which are used for testing. Single call to this method would add   {@link #MAX} number of events.First 20 events are generated without  {@link ApplicationLoggingContext#TAG_RUN_ID} tag.For next 40 events, alternate event is tagged with  {@code ApplicationLoggingContext#TAG_RUN_ID}. Last 20 events are not tagged with   {@code ApplicationLoggingContext#TAG_RUN_ID}. All events are alternately marked as   {@link Level#ERROR} and {@link Level#WARN}. All events are alternately tagged with ""plugin"", ""program"" and ""system"" as value of MDC property "".origin"" All events are alternately tagged with ""lifecycle"" as value of MDC property ""MDC:eventType
 */
private void generateLogs(LoggingContext loggingContext,ProgramId programId,ProgramRunStatus runStatus) throws InterruptedException {
  String[] origins={""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  String entityId=LoggingContextHelper.getEntityId(loggingContext).getValue();
  RunId runId=null;
  Long stopTs=null;
  for (int i=0; i < MAX; ++i) {
    if (i == 20) {
      runId=RunIds.generate(TimeUnit.SECONDS.toMillis(getMockTimeSecs(i)));
    }
 else     if (i == 60 && runStatus != ProgramRunStatus.RUNNING && runStatus != ProgramRunStatus.SUSPENDED) {
      stopTs=getMockTimeSecs(i);
    }
    LoggingEvent event=new LoggingEvent(""String_Node_Str"",(ch.qos.logback.classic.Logger)LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),i % 2 == 0 ? Level.ERROR : Level.WARN,entityId + ""String_Node_Str"" + i,null,null);
    event.setTimeStamp(TimeUnit.SECONDS.toMillis(getMockTimeSecs(i)));
    Map<String,String> tagMap=Maps.newHashMap(Maps.transformValues(loggingContext.getSystemTagsMap(),TAG_TO_STRING_FUNCTION));
    if (runId != null && stopTs == null && i % 2 == 0) {
      tagMap.put(ApplicationLoggingContext.TAG_RUN_ID,runId.getId());
    }
    tagMap.put(""String_Node_Str"",origins[i % 3]);
    if (i % 2 == 0) {
      tagMap.put(""String_Node_Str"",""String_Node_Str"");
    }
    event.setMDCPropertyMap(tagMap);
    logEvents.add(new LogEvent(event,new LogOffset(i,i)));
  }
  long startTs=RunIds.getTime(runId,TimeUnit.SECONDS);
  if (programId != null) {
    runRecordMap.put(programId,new RunRecord(runId.getId(),startTs,stopTs,runStatus,null));
    store.setStart(programId,runId.getId(),startTs);
    if (stopTs != null) {
      store.setStop(programId,runId.getId(),stopTs,runStatus);
    }
  }
}","/** 
 * This method is used to generate the logs for program which are used for testing. Single call to this method would add   {@link #MAX} number of events.First 20 events are generated without  {@link ApplicationLoggingContext#TAG_RUN_ID} tag.For next 40 events, alternate event is tagged with  {@code ApplicationLoggingContext#TAG_RUN_ID}. Last 20 events are not tagged with   {@code ApplicationLoggingContext#TAG_RUN_ID}. All events are alternately marked as   {@link Level#ERROR} and {@link Level#WARN}. All events are alternately tagged with ""plugin"", ""program"" and ""system"" as value of MDC property "".origin"" All events are alternately tagged with ""lifecycle"" as value of MDC property ""MDC:eventType
 */
private void generateLogs(LoggingContext loggingContext,ProgramId programId,ProgramRunStatus runStatus) throws InterruptedException {
  String[] origins={""String_Node_Str"",""String_Node_Str"",""String_Node_Str""};
  String entityId=LoggingContextHelper.getEntityId(loggingContext).getValue();
  StackTraceElement stackTraceElementNative=new StackTraceElement(""String_Node_Str"",""String_Node_Str"",null,-2);
  RunId runId=null;
  Long stopTs=null;
  for (int i=0; i < MAX; ++i) {
    if (i == 20) {
      runId=RunIds.generate(TimeUnit.SECONDS.toMillis(getMockTimeSecs(i)));
    }
 else     if (i == 60 && runStatus != ProgramRunStatus.RUNNING && runStatus != ProgramRunStatus.SUSPENDED) {
      stopTs=getMockTimeSecs(i);
    }
    LoggingEvent event=new LoggingEvent(""String_Node_Str"",(ch.qos.logback.classic.Logger)LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),i % 2 == 0 ? Level.ERROR : Level.WARN,entityId + ""String_Node_Str"" + i,null,null);
    event.setTimeStamp(TimeUnit.SECONDS.toMillis(getMockTimeSecs(i)));
    Map<String,String> tagMap=Maps.newHashMap(Maps.transformValues(loggingContext.getSystemTagsMap(),TAG_TO_STRING_FUNCTION));
    if (runId != null && stopTs == null && i % 2 == 0) {
      tagMap.put(ApplicationLoggingContext.TAG_RUN_ID,runId.getId());
    }
    tagMap.put(""String_Node_Str"",origins[i % 3]);
    if (i % 2 == 0) {
      tagMap.put(""String_Node_Str"",""String_Node_Str"");
    }
    if (i == 30) {
      event.setCallerData(new StackTraceElement[]{stackTraceElementNative});
    }
    event.setMDCPropertyMap(tagMap);
    logEvents.add(new LogEvent(event,new LogOffset(i,i)));
  }
  long startTs=RunIds.getTime(runId,TimeUnit.SECONDS);
  if (programId != null) {
    runRecordMap.put(programId,new RunRecord(runId.getId(),startTs,stopTs,runStatus,null));
    store.setStart(programId,runId.getId(),startTs);
    if (stopTs != null) {
      store.setStop(programId,runId.getId(),stopTs,runStatus);
    }
  }
}"
5108,"LogData(Long timestamp,String logLevel,String threadName,String className,String simpleClassName,Integer lineNumber,String message,String stackTrace,String loggerName,Map<String,String> mdc){
  this.timestamp=timestamp;
  this.logLevel=logLevel;
  this.threadName=threadName;
  this.className=className;
  this.simpleClassName=simpleClassName;
  this.lineNumber=lineNumber;
  this.message=message;
  this.stackTrace=stackTrace;
  this.loggerName=loggerName;
  this.mdc=mdc;
}","LogData(Long timestamp,String logLevel,String threadName,String className,String simpleClassName,Integer lineNumber,String message,String stackTrace,String loggerName,Map<String,String> mdc,boolean isNativeMethod){
  this.timestamp=timestamp;
  this.logLevel=logLevel;
  this.threadName=threadName;
  this.className=className;
  this.simpleClassName=simpleClassName;
  this.lineNumber=lineNumber;
  this.message=message;
  this.stackTrace=stackTrace;
  this.loggerName=loggerName;
  this.mdc=mdc;
  this.isNativeMethod=isNativeMethod;
}"
5109,"@Override public Object encodeSend(LogEvent logEvent){
  ILoggingEvent event=logEvent.getLoggingEvent();
  StackTraceElement[] stackTraceElements=event.getCallerData();
  String className=""String_Node_Str"";
  String simpleClassName=""String_Node_Str"";
  int lineNumber=0;
  if (stackTraceElements != null && stackTraceElements.length > 0) {
    StackTraceElement first=stackTraceElements[0];
    className=first.getClassName();
    simpleClassName=(className.indexOf('.') >= 0) ? className.substring(className.lastIndexOf('.') + 1) : className;
    lineNumber=first.getLineNumber();
  }
  LogData logData=new LogData(event.getTimeStamp(),event.getLevel().toString(),event.getThreadName(),className,simpleClassName,lineNumber,event.getFormattedMessage(),ThrowableProxyUtil.asString(event.getThrowableProxy()),event.getLoggerName(),event.getMDCPropertyMap());
  return modifyLogJsonElememnt(GSON.toJsonTree(new FormattedLogDataEvent(logData,logEvent.getOffset())));
}","@Override public Object encodeSend(LogEvent logEvent){
  ILoggingEvent event=logEvent.getLoggingEvent();
  StackTraceElement[] stackTraceElements=event.getCallerData();
  String className=""String_Node_Str"";
  String simpleClassName=""String_Node_Str"";
  int lineNumber=0;
  boolean isNativeMethod=false;
  if (stackTraceElements != null && stackTraceElements.length > 0) {
    StackTraceElement first=stackTraceElements[0];
    className=first.getClassName();
    simpleClassName=(className.indexOf('.') >= 0) ? className.substring(className.lastIndexOf('.') + 1) : className;
    lineNumber=first.getLineNumber();
    isNativeMethod=first.isNativeMethod();
  }
  LogData logData=new LogData(event.getTimeStamp(),event.getLevel().toString(),event.getThreadName(),className,simpleClassName,lineNumber,event.getFormattedMessage(),ThrowableProxyUtil.asString(event.getThrowableProxy()),event.getLoggerName(),event.getMDCPropertyMap(),isNativeMethod);
  return modifyLogJsonElememnt(GSON.toJsonTree(new FormattedLogDataEvent(logData,logEvent.getOffset())));
}"
5110,"@Override public Object encodeSend(LogEvent logEvent){
  ILoggingEvent event=logEvent.getLoggingEvent();
  StackTraceElement[] stackTraceElements=event.getCallerData();
  String className=""String_Node_Str"";
  String simpleClassName=""String_Node_Str"";
  int lineNumber=0;
  if (stackTraceElements != null && stackTraceElements.length > 0) {
    StackTraceElement first=stackTraceElements[0];
    className=first.getClassName();
    simpleClassName=(className.indexOf('.') >= 0) ? className.substring(className.lastIndexOf('.') + 1) : className;
    lineNumber=first.getLineNumber();
  }
  LogData logData=new LogData(event.getTimeStamp(),event.getLevel().toString(),event.getThreadName(),className,simpleClassName,lineNumber,event.getFormattedMessage(),ThrowableProxyUtil.asString(event.getThrowableProxy()),event.getLoggerName(),event.getMDCPropertyMap());
  return modifyLogJsonElememnt(GSON.toJsonTree(new FormattedLogDataEvent(logData,logEvent.getOffset())));
}","@Override public Object encodeSend(LogEvent logEvent){
  ILoggingEvent event=logEvent.getLoggingEvent();
  StackTraceElement[] stackTraceElements=event.getCallerData();
  String className=""String_Node_Str"";
  String simpleClassName=""String_Node_Str"";
  int lineNumber=0;
  boolean isNativeMethod=false;
  if (stackTraceElements != null && stackTraceElements.length > 0) {
    StackTraceElement first=stackTraceElements[0];
    className=first.getClassName();
    simpleClassName=(className.indexOf('.') >= 0) ? className.substring(className.lastIndexOf('.') + 1) : className;
    lineNumber=first.getLineNumber();
    isNativeMethod=first.isNativeMethod();
  }
  LogData logData=new LogData(event.getTimeStamp(),event.getLevel().toString(),event.getThreadName(),className,simpleClassName,lineNumber,event.getFormattedMessage(),ThrowableProxyUtil.asString(event.getThrowableProxy()),event.getLoggerName(),event.getMDCPropertyMap(),isNativeMethod);
  return modifyLogJsonElememnt(GSON.toJsonTree(new FormattedLogDataEvent(logData,logEvent.getOffset())));
}"
5111,"private void deleteLocalDatasets(){
  for (  Map.Entry<String,String> entry : datasetFramework.getDatasetNameMapping().entrySet()) {
    Map<String,String> datasetArguments=RuntimeArguments.extractScope(Scope.DATASET,entry.getKey(),basicWorkflowContext.getRuntimeArguments());
    if (Boolean.parseBoolean(datasetArguments.get(""String_Node_Str""))) {
      continue;
    }
    String localInstanceName=entry.getValue();
    DatasetId instanceId=new DatasetId(workflowRunId.getNamespace(),localInstanceName);
    LOG.debug(""String_Node_Str"",localInstanceName);
    try {
      datasetFramework.deleteInstance(instanceId);
    }
 catch (    Throwable t) {
      LOG.warn(""String_Node_Str"",localInstanceName,t);
    }
  }
}","private void deleteLocalDatasets(){
  for (  final Map.Entry<String,String> entry : datasetFramework.getDatasetNameMapping().entrySet()) {
    final Map<String,String> datasetArguments=RuntimeArguments.extractScope(Scope.DATASET,entry.getKey(),basicWorkflowContext.getRuntimeArguments());
    if (Boolean.parseBoolean(datasetArguments.get(""String_Node_Str""))) {
      continue;
    }
    final String localInstanceName=entry.getValue();
    final DatasetId instanceId=new DatasetId(workflowRunId.getNamespace(),localInstanceName);
    LOG.debug(""String_Node_Str"",localInstanceName);
    try {
      Retries.callWithRetries(new Retries.Callable<Void,Exception>(){
        @Override public Void call() throws Exception {
          datasetFramework.deleteInstance(instanceId);
          return null;
        }
      }
,RetryStrategies.fixDelay(Constants.Retry.LOCAL_DATASET_OPERATION_RETRY_DELAY_SECONDS,TimeUnit.SECONDS));
    }
 catch (    Exception e) {
      LOG.warn(""String_Node_Str"",localInstanceName,e);
    }
  }
}"
5112,"private void createLocalDatasets() throws IOException, DatasetManagementException {
  for (  Map.Entry<String,String> entry : datasetFramework.getDatasetNameMapping().entrySet()) {
    String localInstanceName=entry.getValue();
    DatasetId instanceId=new DatasetId(workflowRunId.getNamespace(),localInstanceName);
    DatasetCreationSpec instanceSpec=workflowSpec.getLocalDatasetSpecs().get(entry.getKey());
    LOG.debug(""String_Node_Str"",localInstanceName);
    datasetFramework.addInstance(instanceSpec.getTypeName(),instanceId,addLocalDatasetProperty(instanceSpec.getProperties()));
  }
}","private void createLocalDatasets() throws IOException, DatasetManagementException {
  for (  final Map.Entry<String,String> entry : datasetFramework.getDatasetNameMapping().entrySet()) {
    final String localInstanceName=entry.getValue();
    final DatasetId instanceId=new DatasetId(workflowRunId.getNamespace(),localInstanceName);
    final DatasetCreationSpec instanceSpec=workflowSpec.getLocalDatasetSpecs().get(entry.getKey());
    LOG.debug(""String_Node_Str"",localInstanceName);
    try {
      Retries.callWithRetries(new Retries.Callable<Void,Exception>(){
        @Override public Void call() throws Exception {
          datasetFramework.addInstance(instanceSpec.getTypeName(),instanceId,addLocalDatasetProperty(instanceSpec.getProperties()));
          return null;
        }
      }
,RetryStrategies.fixDelay(Constants.Retry.LOCAL_DATASET_OPERATION_RETRY_DELAY_SECONDS,TimeUnit.SECONDS));
    }
 catch (    IOException|DatasetManagementException e) {
      throw e;
    }
catch (    Exception e) {
      throw new IllegalStateException(e);
    }
  }
}"
5113,"@Override public Map.Entry<String,WorkflowToken> call() throws Exception {
  WorkflowToken copiedToken=((BasicWorkflowToken)token).deepCopy();
  executeAll(branch.iterator(),appSpec,instantiator,classLoader,copiedToken);
  return Maps.immutableEntry(branch.toString(),copiedToken);
}","@Override public Void call() throws Exception {
  datasetFramework.deleteInstance(instanceId);
  return null;
}"
5114,"/** 
 * Adds extra MDC tags to the given event.
 */
private void addExtraTags(ILoggingEvent event){
  StackTraceElement[] callerData=event.getCallerData();
  if (callerData == null || callerData.length == 0) {
    return;
  }
  String callerClass=callerData[0].getClassName();
  Map<String,String> tags=loggerExtraTags.getIfPresent(callerClass);
  if (tags == null) {
    tags=Collections.emptyMap();
    for (    Class<?> cls : CallerClassSecurityManager.getCallerClasses()) {
      if (cls.getName().equals(callerClass)) {
        String classLoaderName=cls.getClassLoader().getClass().getName();
switch (classLoaderName) {
case ""String_Node_Str"":
          tags=Collections.singletonMap(ORIGIN_KEY,""String_Node_Str"");
        break;
case ""String_Node_Str"":
      tags=Collections.singletonMap(ORIGIN_KEY,""String_Node_Str"");
    break;
default :
  tags=Collections.singletonMap(ORIGIN_KEY,""String_Node_Str"");
}
break;
}
}
loggerExtraTags.put(callerClass,tags);
}
event.getMDCPropertyMap().putAll(tags);
}","/** 
 * Adds extra MDC tags to the given event.
 */
private void addExtraTags(ILoggingEvent event){
  StackTraceElement[] callerData=event.getCallerData();
  if (callerData == null || callerData.length == 0) {
    return;
  }
  String callerClass=callerData[0].getClassName();
  Map<String,String> tags=loggerExtraTags.getIfPresent(callerClass);
  Class[] callerClasses=CallerClassSecurityManager.getCallerClasses();
  if (tags == null) {
    tags=addTagsForClass(callerClass,callerClasses);
    loggerExtraTags.put(callerClass,tags);
  }
  if (tags.isEmpty()) {
    tags=loggerExtraTags.getIfPresent(event.getLoggerName());
    if (tags == null) {
      tags=addTagsForClass(event.getLoggerName(),callerClasses);
      loggerExtraTags.put(event.getLoggerName(),tags);
    }
  }
  event.getMDCPropertyMap().putAll(tags);
}"
5115,"/** 
 * Create run constraints for a   {@link Schedule}. When a schedule is triggered, the constraints will be checked before launching a run.
 * @param maxConcurrentRuns the maximum number of concurrent active runs for a schedule.If null, no limit is enforced.
 */
RunConstraints(@Nullable Integer maxConcurrentRuns){
  this.maxConcurrentRuns=maxConcurrentRuns;
}","/** 
 * Create run constraints for a   {@link Schedule}. When a schedule is triggered, the constraints will be checked before launching a run.
 * @param maxConcurrentRuns the maximum number of concurrent active runs for a schedule.If null, no limit is enforced.
 */
public RunConstraints(@Nullable Integer maxConcurrentRuns){
  this.maxConcurrentRuns=maxConcurrentRuns;
}"
5116,"private void doUpdateSchedule(HttpRequest request,HttpResponder responder,String namespaceId,String appId,String appVersion,String scheduleName) throws IOException, BadRequestException, NotFoundException, SchedulerException, AlreadyExistsException {
  ApplicationId applicationId=new ApplicationId(namespaceId,appId,appVersion);
  ScheduleSpecification scheduleSpecFromRequest;
  try (Reader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()),Charsets.UTF_8)){
    scheduleSpecFromRequest=GSON.fromJson(reader,ScheduleSpecification.class);
  }
 catch (  IOException e) {
    LOG.debug(""String_Node_Str"",e);
    throw new IOException(""String_Node_Str"");
  }
catch (  JsonSyntaxException e) {
    throw new BadRequestException(""String_Node_Str"" + e.getMessage());
  }
  if (!scheduleName.equals(scheduleSpecFromRequest.getSchedule().getName())) {
    throw new BadRequestException(String.format(""String_Node_Str"",scheduleSpecFromRequest.getSchedule().getName(),scheduleName));
  }
  lifecycleService.updateSchedule(applicationId,scheduleSpecFromRequest);
  responder.sendStatus(HttpResponseStatus.OK);
}","private void doUpdateSchedule(HttpRequest request,HttpResponder responder,String namespaceId,String appId,String appVersion,String scheduleName) throws IOException, BadRequestException, NotFoundException, SchedulerException, AlreadyExistsException {
  ApplicationId applicationId=new ApplicationId(namespaceId,appId,appVersion);
  ScheduleUpdateDetail scheduleUpdateDetail;
  try (Reader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()),Charsets.UTF_8)){
    scheduleUpdateDetail=GSON.fromJson(reader,ScheduleUpdateDetail.class);
  }
 catch (  IOException e) {
    LOG.debug(""String_Node_Str"",e);
    throw new IOException(""String_Node_Str"");
  }
catch (  JsonSyntaxException e) {
    throw new BadRequestException(""String_Node_Str"" + e.getMessage());
  }
  lifecycleService.updateSchedule(applicationId,scheduleName,scheduleUpdateDetail);
  responder.sendStatus(HttpResponseStatus.OK);
}"
5117,"/** 
 * Update the schedule in an application.
 * @param applicationId the application containing the schedule
 * @param scheduleSpecUpdate updated schedule specification
 * @throws NotFoundException when application is not found
 * @throws SchedulerException on an exception when updating the schedule
 * @throws BadRequestException when existing schedule type does not match the updated schedule type
 */
public void updateSchedule(ApplicationId applicationId,ScheduleSpecification scheduleSpecUpdate) throws NotFoundException, SchedulerException, AlreadyExistsException, BadRequestException {
  ApplicationSpecification appSpec=store.getApplication(applicationId);
  if (appSpec == null) {
    throw new ApplicationNotFoundException(applicationId);
  }
  String scheduleName=scheduleSpecUpdate.getSchedule().getName();
  ScheduleSpecification existingScheduleSpec=appSpec.getSchedules().get(scheduleName);
  if (existingScheduleSpec == null) {
    throw new NotFoundException(new ScheduleId(applicationId.getNamespace(),applicationId.getApplication(),scheduleName));
  }
  ScheduleType existingType=ScheduleType.fromSchedule(existingScheduleSpec.getSchedule());
  ScheduleType newType=ScheduleType.fromSchedule(scheduleSpecUpdate.getSchedule());
  if (!existingType.equals(newType)) {
    throw new BadRequestException(String.format(""String_Node_Str"",newType,existingType));
  }
  ProgramType existingProgramType=ProgramType.valueOfSchedulableType(existingScheduleSpec.getProgram().getProgramType());
  ProgramType programType=getSchedulableProgramType(scheduleSpecUpdate);
  String programName=scheduleSpecUpdate.getProgram().getProgramName();
  ProgramId programId=applicationId.program(programType,programName);
  if (!existingProgramType.equals(programType)) {
    throw new BadRequestException(String.format(""String_Node_Str"",programType,existingProgramType));
  }
  try {
    scheduler.updateSchedule(programId,scheduleSpecUpdate.getProgram().getProgramType(),scheduleSpecUpdate.getSchedule(),scheduleSpecUpdate.getProperties());
  }
 catch (  IllegalArgumentException e) {
    throw new BadRequestException(e);
  }
  store.addSchedule(programId,scheduleSpecUpdate,true);
}","/** 
 * Update the schedule in an application.
 * @param applicationId the application containing the schedule
 * @param scheduleName the name of the schedule which needs to updated
 * @param scheduleUpdateDetail updated schedule details
 * @throws NotFoundException when application is not found
 * @throws SchedulerException on an exception when updating the schedule
 * @throws BadRequestException when existing schedule type does not match the updated schedule type
 */
public void updateSchedule(ApplicationId applicationId,String scheduleName,ScheduleUpdateDetail scheduleUpdateDetail) throws NotFoundException, SchedulerException, AlreadyExistsException, BadRequestException {
  ApplicationSpecification appSpec=store.getApplication(applicationId);
  if (appSpec == null) {
    throw new ApplicationNotFoundException(applicationId);
  }
  ScheduleSpecification existingScheduleSpec=appSpec.getSchedules().get(scheduleName);
  if (existingScheduleSpec == null) {
    throw new NotFoundException(new ScheduleId(applicationId.getNamespace(),applicationId.getApplication(),scheduleName));
  }
  ProgramType programType=ProgramType.valueOfSchedulableType(existingScheduleSpec.getProgram().getProgramType());
  String programName=existingScheduleSpec.getProgram().getProgramName();
  ProgramId programId=applicationId.program(programType,programName);
  ScheduleSpecification updatedScheduleSpec=getUpdatedScheduleSpecification(existingScheduleSpec,scheduleUpdateDetail);
  try {
    scheduler.updateSchedule(programId,existingScheduleSpec.getProgram().getProgramType(),updatedScheduleSpec.getSchedule(),updatedScheduleSpec.getProperties());
  }
 catch (  IllegalArgumentException e) {
    throw new BadRequestException(e);
  }
  store.addSchedule(programId,updatedScheduleSpec,true);
}"
5118,"protected HttpResponse updateSchedule(String namespace,String appName,@Nullable String appVersion,String scheduleName,ScheduleSpecification scheduleSpec) throws Exception {
  appVersion=appVersion == null ? ApplicationId.DEFAULT_VERSION : appVersion;
  String path=String.format(""String_Node_Str"",appName,appVersion,scheduleName);
  return doPost(getVersionedAPIPath(path,namespace),GSON.toJson(scheduleSpec));
}","protected HttpResponse updateSchedule(String namespace,String appName,@Nullable String appVersion,String scheduleName,ScheduleUpdateDetail scheduleUpdateDetail) throws Exception {
  appVersion=appVersion == null ? ApplicationId.DEFAULT_VERSION : appVersion;
  String path=String.format(""String_Node_Str"",appName,appVersion,scheduleName);
  return doPost(getVersionedAPIPath(path,namespace),GSON.toJson(scheduleUpdateDetail));
}"
5119,"@Override protected QueryStatus doFetchStatus(OperationHandle operationHandle) throws HiveSQLException, ExploreException, HandleNotFoundException {
  OperationStatus operationStatus=getCliService().getOperationStatus(operationHandle);
  @SuppressWarnings(""String_Node_Str"") HiveSQLException hiveExn=operationStatus.getOperationException();
  if (hiveExn != null) {
    return new QueryStatus(hiveExn.getMessage(),hiveExn.getSQLState());
  }
  return new QueryStatus(QueryStatus.OpStatus.valueOf(operationStatus.getState().toString()),operationHandle.hasResultSet());
}","@Override protected QueryStatus doFetchStatus(OperationHandle operationHandle) throws HiveSQLException, ExploreException, HandleNotFoundException {
  OperationStatus operationStatus;
  CLIService cliService=getCliService();
  try {
    if (getOperationStatus.getParameterTypes().length == 2) {
      operationStatus=(OperationStatus)getOperationStatus.invoke(cliService,operationHandle,true);
    }
 else {
      operationStatus=(OperationStatus)getOperationStatus.invoke(cliService,operationHandle);
    }
  }
 catch (  IndexOutOfBoundsException|IllegalAccessException|InvocationTargetException e) {
    throw new RuntimeException(""String_Node_Str"",e);
  }
  @SuppressWarnings(""String_Node_Str"") HiveSQLException hiveExn=operationStatus.getOperationException();
  if (hiveExn != null) {
    return new QueryStatus(hiveExn.getMessage(),hiveExn.getSQLState());
  }
  return new QueryStatus(QueryStatus.OpStatus.valueOf(operationStatus.getState().toString()),operationHandle.hasResultSet());
}"
5120,"@Override public ExploreService get(){
  File hiveDataDir=new File(cConf.get(Constants.Explore.LOCAL_DATA_DIR));
  System.setProperty(HiveConf.ConfVars.SCRATCHDIR.toString(),new File(hiveDataDir,cConf.get(Constants.AppFabric.TEMP_DIR)).getAbsolutePath());
  System.setProperty(""String_Node_Str"",hConf.get(""String_Node_Str""));
  File warehouseDir=new File(cConf.get(Constants.Explore.LOCAL_DATA_DIR),""String_Node_Str"");
  File databaseDir=new File(cConf.get(Constants.Explore.LOCAL_DATA_DIR),""String_Node_Str"");
  if (isInMemory) {
    warehouseDir=new File(warehouseDir,Long.toString(seed));
    databaseDir=new File(databaseDir,Long.toString(seed));
  }
  LOG.debug(""String_Node_Str"",HiveConf.ConfVars.METASTOREWAREHOUSE.toString(),warehouseDir.getAbsoluteFile());
  System.setProperty(HiveConf.ConfVars.METASTOREWAREHOUSE.toString(),warehouseDir.getAbsolutePath());
  System.setProperty(""String_Node_Str"",cConf.get(Constants.Explore.LOCAL_DATA_DIR) + File.separator + ""String_Node_Str"");
  String connectUrl=String.format(""String_Node_Str"",databaseDir.getAbsoluteFile());
  LOG.debug(""String_Node_Str"",HiveConf.ConfVars.METASTORECONNECTURLKEY.toString(),connectUrl);
  System.setProperty(HiveConf.ConfVars.METASTORECONNECTURLKEY.toString(),connectUrl);
  System.setProperty(HiveConf.ConfVars.LOCALMODEAUTO.toString(),""String_Node_Str"");
  System.setProperty(HiveConf.ConfVars.SUBMITVIACHILD.toString(),""String_Node_Str"");
  System.setProperty(MRConfig.FRAMEWORK_NAME,""String_Node_Str"");
  System.setProperty(HiveConf.ConfVars.HIVE_SERVER2_AUTHENTICATION.toString(),""String_Node_Str"");
  System.setProperty(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS.toString(),""String_Node_Str"");
  System.setProperty(HiveConf.ConfVars.METASTORE_USE_THRIFT_SASL.toString(),""String_Node_Str"");
  return exploreService;
}","@Override public ExploreService get(){
  File hiveDataDir=new File(cConf.get(Constants.Explore.LOCAL_DATA_DIR));
  File defaultScratchDir=new File(hiveDataDir,cConf.get(Constants.AppFabric.TEMP_DIR));
  if (System.getProperty(HiveConf.ConfVars.SCRATCHDIR.toString()) == null) {
    System.setProperty(HiveConf.ConfVars.SCRATCHDIR.toString(),defaultScratchDir.getAbsolutePath());
  }
  System.setProperty(""String_Node_Str"",hConf.get(""String_Node_Str""));
  File warehouseDir=new File(cConf.get(Constants.Explore.LOCAL_DATA_DIR),""String_Node_Str"");
  File databaseDir=new File(cConf.get(Constants.Explore.LOCAL_DATA_DIR),""String_Node_Str"");
  if (isInMemory) {
    warehouseDir=new File(warehouseDir,Long.toString(seed));
    databaseDir=new File(databaseDir,Long.toString(seed));
  }
  LOG.debug(""String_Node_Str"",HiveConf.ConfVars.METASTOREWAREHOUSE.toString(),warehouseDir.getAbsoluteFile());
  System.setProperty(HiveConf.ConfVars.METASTOREWAREHOUSE.toString(),warehouseDir.getAbsolutePath());
  System.setProperty(""String_Node_Str"",cConf.get(Constants.Explore.LOCAL_DATA_DIR) + File.separator + ""String_Node_Str"");
  String connectUrl=String.format(""String_Node_Str"",databaseDir.getAbsoluteFile());
  LOG.debug(""String_Node_Str"",HiveConf.ConfVars.METASTORECONNECTURLKEY.toString(),connectUrl);
  System.setProperty(HiveConf.ConfVars.METASTORECONNECTURLKEY.toString(),connectUrl);
  System.setProperty(HiveConf.ConfVars.LOCALMODEAUTO.toString(),""String_Node_Str"");
  System.setProperty(HiveConf.ConfVars.SUBMITVIACHILD.toString(),""String_Node_Str"");
  System.setProperty(MRConfig.FRAMEWORK_NAME,""String_Node_Str"");
  System.setProperty(HiveConf.ConfVars.HIVE_SERVER2_AUTHENTICATION.toString(),""String_Node_Str"");
  System.setProperty(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS.toString(),""String_Node_Str"");
  System.setProperty(HiveConf.ConfVars.METASTORE_USE_THRIFT_SASL.toString(),""String_Node_Str"");
  return exploreService;
}"
5121,"/** 
 * Add a schedule to an application.
 * @param applicationId the application id for which the schedule needs to be added
 * @param scheduleSpec the schedule specification
 * @throws NotFoundException when application is not found
 * @throws SchedulerException on an exception when updating the schedule
 * @throws BadRequestException if the program type is not workflow
 */
public void addSchedule(ApplicationId applicationId,ScheduleSpecification scheduleSpec) throws NotFoundException, SchedulerException, AlreadyExistsException, BadRequestException {
  ApplicationSpecification appSpec=store.getApplication(applicationId);
  if (appSpec == null) {
    throw new ApplicationNotFoundException(applicationId);
  }
  String scheduleName=scheduleSpec.getSchedule().getName();
  ScheduleSpecification existingScheduleSpec=appSpec.getSchedules().get(scheduleName);
  if (existingScheduleSpec != null) {
    throw new AlreadyExistsException(new ScheduleId(applicationId.getNamespace(),applicationId.getApplication(),scheduleName));
  }
  ProgramType programType=ProgramType.valueOfSchedulableType(scheduleSpec.getProgram().getProgramType());
  String programName=scheduleSpec.getProgram().getProgramName();
  ProgramId programId=applicationId.program(programType,programName);
  if (!programType.equals(ProgramType.WORKFLOW)) {
    throw new BadRequestException(""String_Node_Str"");
  }
  scheduler.schedule(programId,scheduleSpec.getProgram().getProgramType(),scheduleSpec.getSchedule(),scheduleSpec.getProperties());
  store.addSchedule(programId,scheduleSpec,false);
}","/** 
 * Add a schedule to an application.
 * @param applicationId the application id for which the schedule needs to be added
 * @param scheduleSpec the schedule specification
 * @throws NotFoundException when application is not found
 * @throws SchedulerException on an exception when updating the schedule
 * @throws BadRequestException if the program type is not workflow
 */
public void addSchedule(ApplicationId applicationId,ScheduleSpecification scheduleSpec) throws NotFoundException, SchedulerException, AlreadyExistsException, BadRequestException {
  ApplicationSpecification appSpec=store.getApplication(applicationId);
  if (appSpec == null) {
    throw new ApplicationNotFoundException(applicationId);
  }
  String scheduleName=scheduleSpec.getSchedule().getName();
  ScheduleSpecification existingScheduleSpec=appSpec.getSchedules().get(scheduleName);
  if (existingScheduleSpec != null) {
    throw new AlreadyExistsException(new ScheduleId(applicationId.getNamespace(),applicationId.getApplication(),scheduleName));
  }
  ProgramType programType=getSchedulableProgramType(scheduleSpec);
  String programName=scheduleSpec.getProgram().getProgramName();
  ProgramId programId=applicationId.program(programType,programName);
  if (!programType.equals(ProgramType.WORKFLOW)) {
    throw new BadRequestException(""String_Node_Str"");
  }
  try {
    scheduler.schedule(programId,scheduleSpec.getProgram().getProgramType(),scheduleSpec.getSchedule(),scheduleSpec.getProperties());
  }
 catch (  IllegalArgumentException e) {
    throw new BadRequestException(e);
  }
  store.addSchedule(programId,scheduleSpec,false);
}"
5122,"/** 
 * Update the schedule in an application.
 * @param applicationId the application containing the schedule
 * @param scheduleSpecUpdate updated schedule specification
 * @throws NotFoundException when application is not found
 * @throws SchedulerException on an exception when updating the schedule
 * @throws BadRequestException when existing schedule type does not match the updated schedule type
 */
public void updateSchedule(ApplicationId applicationId,ScheduleSpecification scheduleSpecUpdate) throws NotFoundException, SchedulerException, AlreadyExistsException, BadRequestException {
  ApplicationSpecification appSpec=store.getApplication(applicationId);
  if (appSpec == null) {
    throw new ApplicationNotFoundException(applicationId);
  }
  String scheduleName=scheduleSpecUpdate.getSchedule().getName();
  ScheduleSpecification existingScheduleSpec=appSpec.getSchedules().get(scheduleName);
  if (existingScheduleSpec == null) {
    throw new NotFoundException(new ScheduleId(applicationId.getNamespace(),applicationId.getApplication(),scheduleName));
  }
  ScheduleType existingType=ScheduleType.fromSchedule(existingScheduleSpec.getSchedule());
  ScheduleType newType=ScheduleType.fromSchedule(scheduleSpecUpdate.getSchedule());
  if (!existingType.equals(newType)) {
    throw new BadRequestException(String.format(""String_Node_Str"",newType,existingType));
  }
  ProgramType existingProgramType=ProgramType.valueOfSchedulableType(existingScheduleSpec.getProgram().getProgramType());
  ProgramType programType=ProgramType.valueOfSchedulableType(scheduleSpecUpdate.getProgram().getProgramType());
  String programName=scheduleSpecUpdate.getProgram().getProgramName();
  ProgramId programId=applicationId.program(programType,programName);
  if (!existingProgramType.equals(programType)) {
    throw new BadRequestException(String.format(""String_Node_Str"",programType,existingProgramType));
  }
  scheduler.updateSchedule(programId,scheduleSpecUpdate.getProgram().getProgramType(),scheduleSpecUpdate.getSchedule(),scheduleSpecUpdate.getProperties());
  store.addSchedule(programId,scheduleSpecUpdate,true);
}","/** 
 * Update the schedule in an application.
 * @param applicationId the application containing the schedule
 * @param scheduleSpecUpdate updated schedule specification
 * @throws NotFoundException when application is not found
 * @throws SchedulerException on an exception when updating the schedule
 * @throws BadRequestException when existing schedule type does not match the updated schedule type
 */
public void updateSchedule(ApplicationId applicationId,ScheduleSpecification scheduleSpecUpdate) throws NotFoundException, SchedulerException, AlreadyExistsException, BadRequestException {
  ApplicationSpecification appSpec=store.getApplication(applicationId);
  if (appSpec == null) {
    throw new ApplicationNotFoundException(applicationId);
  }
  String scheduleName=scheduleSpecUpdate.getSchedule().getName();
  ScheduleSpecification existingScheduleSpec=appSpec.getSchedules().get(scheduleName);
  if (existingScheduleSpec == null) {
    throw new NotFoundException(new ScheduleId(applicationId.getNamespace(),applicationId.getApplication(),scheduleName));
  }
  ScheduleType existingType=ScheduleType.fromSchedule(existingScheduleSpec.getSchedule());
  ScheduleType newType=ScheduleType.fromSchedule(scheduleSpecUpdate.getSchedule());
  if (!existingType.equals(newType)) {
    throw new BadRequestException(String.format(""String_Node_Str"",newType,existingType));
  }
  ProgramType existingProgramType=ProgramType.valueOfSchedulableType(existingScheduleSpec.getProgram().getProgramType());
  ProgramType programType=getSchedulableProgramType(scheduleSpecUpdate);
  String programName=scheduleSpecUpdate.getProgram().getProgramName();
  ProgramId programId=applicationId.program(programType,programName);
  if (!existingProgramType.equals(programType)) {
    throw new BadRequestException(String.format(""String_Node_Str"",programType,existingProgramType));
  }
  try {
    scheduler.updateSchedule(programId,scheduleSpecUpdate.getProgram().getProgramType(),scheduleSpecUpdate.getSchedule(),scheduleSpecUpdate.getProperties());
  }
 catch (  IllegalArgumentException e) {
    throw new BadRequestException(e);
  }
  store.addSchedule(programId,scheduleSpecUpdate,true);
}"
5123,"private ApplicationSpecification getAppSpecOrFail(AppMetadataStore mds,ProgramId id){
  ApplicationSpecification appSpec=getApplicationSpec(mds,id.getParent());
  if (appSpec == null) {
    throw new NoSuchElementException(""String_Node_Str"" + id.getNamespaceId() + ""String_Node_Str""+ id.getApplication());
  }
  return appSpec;
}","private ApplicationSpecification getAppSpecOrFail(AppMetadataStore mds,ApplicationId id){
  ApplicationSpecification appSpec=getApplicationSpec(mds,id);
  if (appSpec == null) {
    throw new NoSuchElementException(""String_Node_Str"" + id.getNamespaceId() + ""String_Node_Str""+ id.getApplication());
  }
  return appSpec;
}"
5124,"@Inject public HBaseQueueDebugger(HBaseTableUtil tableUtil,HBaseQueueAdmin queueAdmin,HBaseQueueClientFactory queueClientFactory,ZKClientService zkClientService,TransactionExecutorFactory txExecutorFactory,NamespaceQueryAdmin namespaceQueryAdmin,Store store,Impersonator impersonator,AuthorizationEnforcementService authorizationEnforcementService){
  this.tableUtil=tableUtil;
  this.queueAdmin=queueAdmin;
  this.queueClientFactory=queueClientFactory;
  this.zkClientService=zkClientService;
  this.txExecutorFactory=txExecutorFactory;
  this.namespaceQueryAdmin=namespaceQueryAdmin;
  this.store=store;
  this.impersonator=impersonator;
  this.authorizationEnforcementService=authorizationEnforcementService;
}","@Inject public HBaseQueueDebugger(HBaseTableUtil tableUtil,HBaseQueueAdmin queueAdmin,HBaseQueueClientFactory queueClientFactory,ZKClientService zkClientService,TransactionExecutorFactory txExecutorFactory,Store store,Impersonator impersonator,AuthorizationEnforcementService authorizationEnforcementService){
  this.tableUtil=tableUtil;
  this.queueAdmin=queueAdmin;
  this.queueClientFactory=queueClientFactory;
  this.zkClientService=zkClientService;
  this.txExecutorFactory=txExecutorFactory;
  this.store=store;
  this.impersonator=impersonator;
  this.authorizationEnforcementService=authorizationEnforcementService;
}"
5125,"public static void main(String[] args) throws Exception {
  if (args.length >= 1 && args[0].equals(""String_Node_Str"")) {
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println();
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"" + PROP_SHOW_PROGRESS + ""String_Node_Str"");
    System.out.println(""String_Node_Str"" + PROP_ROWS_CACHE + ""String_Node_Str""+ ""String_Node_Str"");
    System.exit(1);
  }
  final QueueName queueName=args.length >= 1 ? QueueName.from(URI.create(args[0])) : null;
  Long consumerGroupId=null;
  if (args.length >= 2) {
    Preconditions.checkNotNull(queueName);
    String consumerFlowlet=args[1];
    FlowId flowId=new FlowId(queueName.getFirstComponent(),queueName.getSecondComponent(),queueName.getThirdComponent());
    consumerGroupId=FlowUtils.generateConsumerGroupId(flowId,consumerFlowlet);
  }
  HBaseQueueDebugger debugger=createDebugger();
  debugger.startAndWait();
  if (queueName != null) {
    debugger.scanQueue(queueName,consumerGroupId);
  }
 else {
    debugger.scanAllQueues();
  }
  debugger.stopAndWait();
}","public static void main(String[] args) throws Exception {
  if (args.length >= 1 && args[0].equals(""String_Node_Str"")) {
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println();
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"" + PROP_SHOW_PROGRESS + ""String_Node_Str"");
    System.out.println(""String_Node_Str"" + PROP_ROWS_CACHE + ""String_Node_Str""+ ""String_Node_Str"");
    System.exit(1);
  }
  final QueueName queueName=args.length >= 1 ? QueueName.from(URI.create(args[0])) : null;
  Long consumerGroupId=null;
  if (args.length >= 2) {
    Preconditions.checkNotNull(queueName);
    String consumerFlowlet=args[1];
    FlowId flowId=new FlowId(queueName.getFirstComponent(),queueName.getSecondComponent(),queueName.getThirdComponent());
    consumerGroupId=FlowUtils.generateConsumerGroupId(flowId,consumerFlowlet);
  }
  final HBaseQueueDebugger debugger=createDebugger();
  debugger.startAndWait();
  Injector injector=createInjector(true);
  NoAuthService noAuthService=injector.getInstance(NoAuthService.class);
  noAuthService.startAndWait();
  NamespaceQueryAdmin namespaceQueryAdmin=noAuthService.getNamespaceQueryAdmin();
  Impersonator impersonator=noAuthService.getImpersonator();
  if (queueName != null) {
    final Long finalConsumerGroupId=consumerGroupId;
    impersonator.doAs(new NamespaceId(queueName.getFirstComponent()),new Callable<Void>(){
      @Override public Void call() throws Exception {
        debugger.scanQueue(queueName,finalConsumerGroupId);
        return null;
      }
    }
);
  }
 else {
    debugger.scanQueues(namespaceQueryAdmin.list());
  }
  noAuthService.stopAndWait();
  debugger.stopAndWait();
}"
5126,"public static HBaseQueueDebugger createDebugger() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  if (cConf.getBoolean(Constants.Security.Authorization.ENABLED)) {
    System.out.println(String.format(""String_Node_Str"",HBaseQueueDebugger.class.getSimpleName()));
    cConf.setBoolean(Constants.Security.Authorization.ENABLED,false);
  }
  SecurityUtil.loginForMasterService(cConf);
  Injector injector=Guice.createInjector(new ConfigModule(cConf,HBaseConfiguration.create()),new IOModule(),new ZKClientModule(),new LocationRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new TwillModule(),new ExploreClientModule(),new DataFabricModules().getDistributedModules(),new ServiceStoreModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new AppFabricServiceRuntimeModule().getDistributedModules(),new ProgramRunnerRuntimeModule().getDistributedModules(),new SystemDatasetRuntimeModule().getDistributedModules(),new NotificationServiceRuntimeModule().getDistributedModules(),new MetricsClientRuntimeModule().getDistributedModules(),new MetricsStoreModule(),new KafkaClientModule(),new NamespaceStoreModule().getDistributedModules(),new AuthorizationModule(),new AuthorizationEnforcementModule().getMasterModule(),new SecureStoreModules().getDistributedModules(),new MessagingClientModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(QueueClientFactory.class).to(HBaseQueueClientFactory.class).in(Singleton.class);
      bind(QueueAdmin.class).to(HBaseQueueAdmin.class).in(Singleton.class);
      bind(HBaseTableUtil.class).toProvider(HBaseTableUtilFactory.class);
      bind(Store.class).annotatedWith(Names.named(""String_Node_Str"")).to(DefaultStore.class).in(Singleton.class);
      bind(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str"")).to(DatasetFramework.class).in(Singleton.class);
    }
  }
);
  return injector.getInstance(HBaseQueueDebugger.class);
}","@VisibleForTesting static HBaseQueueDebugger createDebugger() throws Exception {
  return createInjector(false).getInstance(HBaseQueueDebugger.class);
}"
5127,"@Override protected void shutDown() throws Exception {
  authorizationEnforcementService.stopAndWait();
  zkClientService.stopAndWait();
}","@Override protected void shutDown() throws Exception {
  zkClientService.stopAndWait();
}"
5128,"@Override public Void call() throws Exception {
  SimpleQueueSpecificationGenerator queueSpecGenerator=new SimpleQueueSpecificationGenerator(flowId.getParent());
  Table<QueueSpecificationGenerator.Node,String,Set<QueueSpecification>> table=queueSpecGenerator.create(flow);
  for (  Table.Cell<QueueSpecificationGenerator.Node,String,Set<QueueSpecification>> cell : table.cellSet()) {
    if (cell.getRowKey().getType() == FlowletConnection.Type.FLOWLET) {
      for (      QueueSpecification queue : cell.getValue()) {
        QueueStatistics queueStats=scanQueue(queue.getQueueName(),null);
        totalStats.add(queueStats);
      }
    }
  }
  return null;
}","@Override public Void call() throws Exception {
  debugger.scanQueue(queueName,finalConsumerGroupId);
  return null;
}"
5129,"@Override protected void startUp() throws Exception {
  zkClientService.startAndWait();
  authorizationEnforcementService.startAndWait();
}","@Override protected void startUp() throws Exception {
  zkClientService.startAndWait();
}"
5130,"@Beta @POST @Path(""String_Node_Str"" + ""String_Node_Str"") @AuditPolicy({AuditDetail.REQUEST_BODY,AuditDetail.RESPONSE_BODY}) public void callArtifactPluginMethod(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginName,@PathParam(""String_Node_Str"") String pluginType,@PathParam(""String_Node_Str"") String methodName,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws Exception {
  String requestBody=request.getContent().toString(Charsets.UTF_8);
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  if (requestBody.isEmpty()) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  try {
    PluginEndpoint pluginEndpoint=pluginService.getPluginEndpoint(namespace,artifactId,pluginType,pluginName,methodName);
    Object response=pluginEndpoint.invoke(GSON.fromJson(requestBody,pluginEndpoint.getMethodParameterType()));
    responder.sendString(HttpResponseStatus.OK,GSON.toJson(response));
  }
 catch (  JsonSyntaxException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
  }
catch (  InvocationTargetException e) {
    if (e.getCause() instanceof javax.ws.rs.NotFoundException) {
      throw new NotFoundException(e.getCause());
    }
 else     if (e.getCause() instanceof javax.ws.rs.BadRequestException) {
      throw new BadRequestException(e.getCause());
    }
 else     if (e.getCause() instanceof IllegalArgumentException && e.getCause() != null) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getCause().getMessage());
    }
 else {
      Throwable rootCause=Throwables.getRootCause(e);
      String message=""String_Node_Str"";
      if (rootCause != null && rootCause.getMessage() != null) {
        message=String.format(""String_Node_Str"",message,rootCause.getMessage());
      }
      responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,message);
    }
  }
}","@Beta @POST @Path(""String_Node_Str"" + ""String_Node_Str"") @AuditPolicy({AuditDetail.REQUEST_BODY,AuditDetail.RESPONSE_BODY}) public void callArtifactPluginMethod(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginName,@PathParam(""String_Node_Str"") String pluginType,@PathParam(""String_Node_Str"") String methodName,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws Exception {
  String requestBody=request.getContent().toString(Charsets.UTF_8);
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  if (requestBody.isEmpty()) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  try {
    PluginEndpoint pluginEndpoint=pluginService.getPluginEndpoint(namespace,artifactId,pluginType,pluginName,methodName);
    Object response=pluginEndpoint.invoke(GSON.fromJson(requestBody,pluginEndpoint.getMethodParameterType()));
    responder.sendString(HttpResponseStatus.OK,GSON.toJson(response));
  }
 catch (  JsonSyntaxException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
  }
catch (  InvocationTargetException e) {
    if (e.getCause() instanceof javax.ws.rs.NotFoundException) {
      throw new NotFoundException(e.getCause());
    }
 else     if (e.getCause() instanceof javax.ws.rs.BadRequestException) {
      throw new BadRequestException(e.getCause());
    }
 else     if (e.getCause() instanceof IllegalArgumentException && e.getCause() != null) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getCause().getMessage());
    }
 else {
      Throwable rootCause=Throwables.getRootCause(e);
      String message=String.format(""String_Node_Str"",methodName);
      if (rootCause != null && rootCause.getMessage() != null) {
        message=String.format(""String_Node_Str"",message,rootCause.getMessage());
      }
      responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,message);
    }
  }
}"
5131,"@Beta @POST @Path(""String_Node_Str"" + ""String_Node_Str"") @AuditPolicy({AuditDetail.REQUEST_BODY,AuditDetail.RESPONSE_BODY}) public void callArtifactPluginMethod(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginName,@PathParam(""String_Node_Str"") String pluginType,@PathParam(""String_Node_Str"") String methodName,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws Exception {
  String requestBody=request.getContent().toString(Charsets.UTF_8);
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  if (requestBody.isEmpty()) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  try {
    PluginEndpoint pluginEndpoint=pluginService.getPluginEndpoint(namespace,artifactId,pluginType,pluginName,methodName);
    Object response=pluginEndpoint.invoke(GSON.fromJson(requestBody,pluginEndpoint.getMethodParameterType()));
    responder.sendString(HttpResponseStatus.OK,GSON.toJson(response));
  }
 catch (  JsonSyntaxException e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
  }
catch (  InvocationTargetException e) {
    LOG.error(""String_Node_Str"",e);
    if (e.getCause() instanceof javax.ws.rs.NotFoundException) {
      throw new NotFoundException(e.getCause());
    }
 else     if (e.getCause() instanceof javax.ws.rs.BadRequestException) {
      throw new BadRequestException(e.getCause());
    }
 else     if (e.getCause() instanceof IllegalArgumentException && e.getCause() != null) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getCause().getMessage());
    }
 else {
      Throwable rootCause=Throwables.getRootCause(e);
      String message=""String_Node_Str"";
      if (rootCause != null && rootCause.getMessage() != null) {
        message=String.format(""String_Node_Str"",message,rootCause.getMessage());
      }
      responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,message);
    }
  }
}","@Beta @POST @Path(""String_Node_Str"" + ""String_Node_Str"") @AuditPolicy({AuditDetail.REQUEST_BODY,AuditDetail.RESPONSE_BODY}) public void callArtifactPluginMethod(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginName,@PathParam(""String_Node_Str"") String pluginType,@PathParam(""String_Node_Str"") String methodName,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws Exception {
  String requestBody=request.getContent().toString(Charsets.UTF_8);
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  if (requestBody.isEmpty()) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  try {
    PluginEndpoint pluginEndpoint=pluginService.getPluginEndpoint(namespace,artifactId,pluginType,pluginName,methodName);
    Object response=pluginEndpoint.invoke(GSON.fromJson(requestBody,pluginEndpoint.getMethodParameterType()));
    responder.sendString(HttpResponseStatus.OK,GSON.toJson(response));
  }
 catch (  JsonSyntaxException e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
  }
catch (  InvocationTargetException e) {
    LOG.error(""String_Node_Str"",e);
    if (e.getCause() instanceof javax.ws.rs.NotFoundException) {
      throw new NotFoundException(e.getCause());
    }
 else     if (e.getCause() instanceof javax.ws.rs.BadRequestException) {
      throw new BadRequestException(e.getCause());
    }
 else     if (e.getCause() instanceof IllegalArgumentException && e.getCause() != null) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getCause().getMessage());
    }
 else {
      Throwable rootCause=Throwables.getRootCause(e);
      String message=String.format(""String_Node_Str"",methodName);
      if (rootCause != null && rootCause.getMessage() != null) {
        message=String.format(""String_Node_Str"",message,rootCause.getMessage());
      }
      responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,message);
    }
  }
}"
5132,"private ETLRealtimeConfig convertRealtimeConfig(int minorVersion,String configStr){
  UpgradeableConfig config;
  if (minorVersion == 2) {
    config=GSON.fromJson(configStr,co.cask.cdap.etl.proto.v0.ETLRealtimeConfig.class);
  }
 else   if (minorVersion == 3) {
    config=GSON.fromJson(configStr,co.cask.cdap.etl.proto.v1.ETLRealtimeConfig.class);
  }
 else {
    ETLRealtimeConfig realtimeConfig=GSON.fromJson(configStr,ETLRealtimeConfig.class);
    ETLRealtimeConfig.Builder builder=ETLRealtimeConfig.builder().addConnections(realtimeConfig.getConnections()).setInstances(realtimeConfig.getInstances()).setResources(realtimeConfig.getResources());
    for (    ETLStage stage : realtimeConfig.getStages()) {
      builder.addStage(stage.upgradeStage(etlRealtimeContext));
    }
    return builder.build();
  }
  while (config.canUpgrade()) {
    config=config.upgrade(etlRealtimeContext);
  }
  return (ETLRealtimeConfig)config;
}","private ETLRealtimeConfig convertRealtimeConfig(int majorVersion,int minorVersion,String configStr){
  UpgradeableConfig config;
  if (majorVersion == 3 && minorVersion == 2) {
    config=GSON.fromJson(configStr,co.cask.cdap.etl.proto.v0.ETLRealtimeConfig.class);
  }
 else   if (majorVersion == 3 && minorVersion == 3) {
    config=GSON.fromJson(configStr,co.cask.cdap.etl.proto.v1.ETLRealtimeConfig.class);
  }
 else {
    ETLRealtimeConfig realtimeConfig=GSON.fromJson(configStr,ETLRealtimeConfig.class);
    ETLRealtimeConfig.Builder builder=ETLRealtimeConfig.builder().addConnections(realtimeConfig.getConnections()).setInstances(realtimeConfig.getInstances()).setResources(realtimeConfig.getResources());
    for (    ETLStage stage : realtimeConfig.getStages()) {
      builder.addStage(stage.upgradeStage(etlRealtimeContext));
    }
    return builder.build();
  }
  while (config.canUpgrade()) {
    config=config.upgrade(etlRealtimeContext);
  }
  return (ETLRealtimeConfig)config;
}"
5133,"private ETLBatchConfig convertBatchConfig(int minorVersion,String configStr,UpgradeContext upgradeContext){
  UpgradeableConfig config;
  if (minorVersion == 2) {
    config=GSON.fromJson(configStr,co.cask.cdap.etl.proto.v0.ETLBatchConfig.class);
  }
 else   if (minorVersion == 3) {
    config=GSON.fromJson(configStr,co.cask.cdap.etl.proto.v1.ETLBatchConfig.class);
  }
 else {
    ETLBatchConfig batchConfig=GSON.fromJson(configStr,ETLBatchConfig.class);
    ETLBatchConfig.Builder builder=ETLBatchConfig.builder(batchConfig.getSchedule()).addConnections(batchConfig.getConnections()).setResources(batchConfig.getResources()).setDriverResources(batchConfig.getDriverResources()).setEngine(batchConfig.getEngine());
    for (    ETLStage postAction : batchConfig.getPostActions()) {
      builder.addPostAction(postAction.upgradeStage(upgradeContext));
    }
    for (    ETLStage stage : batchConfig.getStages()) {
      builder.addStage(stage.upgradeStage(upgradeContext));
    }
    return builder.build();
  }
  while (config.canUpgrade()) {
    config=config.upgrade(upgradeContext);
  }
  return (ETLBatchConfig)config;
}","private ETLBatchConfig convertBatchConfig(int majorVersion,int minorVersion,String configStr,UpgradeContext upgradeContext){
  UpgradeableConfig config;
  if (majorVersion == 3 && minorVersion == 2) {
    config=GSON.fromJson(configStr,co.cask.cdap.etl.proto.v0.ETLBatchConfig.class);
  }
 else   if (majorVersion == 3 && minorVersion == 3) {
    config=GSON.fromJson(configStr,co.cask.cdap.etl.proto.v1.ETLBatchConfig.class);
  }
 else {
    ETLBatchConfig batchConfig=GSON.fromJson(configStr,ETLBatchConfig.class);
    ETLBatchConfig.Builder builder=ETLBatchConfig.builder(batchConfig.getSchedule()).addConnections(batchConfig.getConnections()).setResources(batchConfig.getResources()).setDriverResources(batchConfig.getDriverResources()).setEngine(batchConfig.getEngine());
    for (    ETLStage postAction : batchConfig.getPostActions()) {
      builder.addPostAction(postAction.upgradeStage(upgradeContext));
    }
    for (    ETLStage stage : batchConfig.getStages()) {
      builder.addStage(stage.upgradeStage(upgradeContext));
    }
    return builder.build();
  }
  while (config.canUpgrade()) {
    config=config.upgrade(upgradeContext);
  }
  return (ETLBatchConfig)config;
}"
5134,"public boolean shouldUpgrade(ArtifactSummary artifactSummary){
  if (artifactSummary.getScope() != ArtifactScope.SYSTEM) {
    return false;
  }
  if (!Upgrader.ARTIFACT_NAMES.contains(artifactSummary.getName())) {
    return false;
  }
  ArtifactVersion artifactVersion=new ArtifactVersion(artifactSummary.getVersion());
  Integer majorVersion=artifactVersion.getMajor();
  Integer minorVersion=artifactVersion.getMinor();
  return majorVersion != null && majorVersion == 3 && minorVersion != null && minorVersion >= 2;
}","public boolean shouldUpgrade(ArtifactSummary artifactSummary){
  if (artifactSummary.getScope() != ArtifactScope.SYSTEM) {
    return false;
  }
  if (!Upgrader.ARTIFACT_NAMES.contains(artifactSummary.getName())) {
    return false;
  }
  ArtifactVersion artifactVersion=new ArtifactVersion(artifactSummary.getVersion());
  return LOWEST_VERSION.compareTo(artifactVersion) <= 0 && CURRENT_VERSION.compareTo(artifactVersion) > 0;
}"
5135,"@Override public void getLogNext(LoggingContext loggingContext,ReadRange readRange,int maxEvents,Filter filter,Callback callback){
  if (readRange.getKafkaOffset() < 0) {
    getLogPrev(loggingContext,readRange,maxEvents,filter,callback);
    return;
  }
  Filter contextFilter=LoggingContextHelper.createFilter(loggingContext);
  callback.init();
  try {
    int count=0;
    for (    LogEvent logLine : logEvents) {
      if (logLine.getOffset().getKafkaOffset() >= readRange.getKafkaOffset()) {
        long logTime=logLine.getLoggingEvent().getTimeStamp();
        if (!contextFilter.match(logLine.getLoggingEvent()) || logTime < readRange.getFromMillis() || logTime >= readRange.getToMillis()) {
          continue;
        }
        if (++count > maxEvents) {
          break;
        }
        if (filter != Filter.EMPTY_FILTER && logLine.getOffset().getKafkaOffset() % 2 != 0) {
          continue;
        }
        callback.handle(logLine);
      }
    }
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
  }
 finally {
    callback.close();
  }
}","@Override public void getLogNext(LoggingContext loggingContext,ReadRange readRange,int maxEvents,Filter filter,Callback callback){
  if (readRange.getKafkaOffset() < 0) {
    getLogPrev(loggingContext,readRange,maxEvents,filter,callback);
    return;
  }
  Filter contextFilter=LoggingContextHelper.createFilter(loggingContext);
  callback.init();
  try {
    int count=0;
    for (    LogEvent logLine : logEvents) {
      if (logLine.getOffset().getKafkaOffset() >= readRange.getKafkaOffset()) {
        long logTime=logLine.getLoggingEvent().getTimeStamp();
        if (!contextFilter.match(logLine.getLoggingEvent()) || logTime < readRange.getFromMillis() || logTime >= readRange.getToMillis()) {
          continue;
        }
        if (++count > maxEvents) {
          break;
        }
        if (!filter.match(logLine.getLoggingEvent())) {
          continue;
        }
        callback.handle(logLine);
      }
    }
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
  }
 finally {
    callback.close();
  }
}"
5136,"@Override public void uncaughtException(Thread t,Throwable e){
  StackTraceElement[] stackTrace=e.getStackTrace();
  if (stackTrace.length > 0) {
    Logger logger=LoggerFactory.getLogger(stackTrace[0].getClassName());
    logger.error(""String_Node_Str"",t,e);
  }
 else {
    LOG.error(""String_Node_Str"",t,e);
  }
}","@Override public void uncaughtException(Thread t,Throwable e){
  StackTraceElement[] stackTrace=e.getStackTrace();
  if (stackTrace.length > 0) {
    Logger logger=LoggerFactory.getLogger(stackTrace[0].getClassName());
    logger.debug(""String_Node_Str"",t,e);
  }
 else {
    LOG.debug(""String_Node_Str"",t,e);
  }
}"
5137,"LogData(Long timestamp,String logLevel,String threadName,String className,String simpleClassName,Integer lineNumber,String message,String stackTrace){
  this.timestamp=timestamp;
  this.logLevel=logLevel;
  this.threadName=threadName;
  this.className=className;
  this.simpleClassName=simpleClassName;
  this.lineNumber=lineNumber;
  this.message=message;
  this.stackTrace=stackTrace;
}","LogData(Long timestamp,String logLevel,String threadName,String className,String simpleClassName,Integer lineNumber,String message,String stackTrace,String loggerName){
  this.timestamp=timestamp;
  this.logLevel=logLevel;
  this.threadName=threadName;
  this.className=className;
  this.simpleClassName=simpleClassName;
  this.lineNumber=lineNumber;
  this.message=message;
  this.stackTrace=stackTrace;
  this.loggerName=loggerName;
}"
5138,"@Override public Object encodeSend(LogEvent logEvent){
  ILoggingEvent event=logEvent.getLoggingEvent();
  StackTraceElement[] stackTraceElements=event.getCallerData();
  String className=""String_Node_Str"";
  String simpleClassName=""String_Node_Str"";
  int lineNumber=0;
  if (stackTraceElements != null && stackTraceElements.length > 0) {
    StackTraceElement first=stackTraceElements[0];
    className=first.getClassName();
    simpleClassName=(className.indexOf('.') >= 0) ? className.substring(className.lastIndexOf('.') + 1) : className;
    lineNumber=first.getLineNumber();
  }
  LogData logData=new LogData(event.getTimeStamp(),event.getLevel().toString(),event.getThreadName(),className,simpleClassName,lineNumber,event.getFormattedMessage(),ThrowableProxyUtil.asString(event.getThrowableProxy()));
  return modifyLogJsonElememnt(GSON.toJsonTree(new FormattedLogDataEvent(logData,logEvent.getOffset())));
}","@Override public Object encodeSend(LogEvent logEvent){
  ILoggingEvent event=logEvent.getLoggingEvent();
  StackTraceElement[] stackTraceElements=event.getCallerData();
  String className=""String_Node_Str"";
  String simpleClassName=""String_Node_Str"";
  int lineNumber=0;
  if (stackTraceElements != null && stackTraceElements.length > 0) {
    StackTraceElement first=stackTraceElements[0];
    className=first.getClassName();
    simpleClassName=(className.indexOf('.') >= 0) ? className.substring(className.lastIndexOf('.') + 1) : className;
    lineNumber=first.getLineNumber();
  }
  LogData logData=new LogData(event.getTimeStamp(),event.getLevel().toString(),event.getThreadName(),className,simpleClassName,lineNumber,event.getFormattedMessage(),ThrowableProxyUtil.asString(event.getThrowableProxy()),event.getLoggerName());
  return modifyLogJsonElememnt(GSON.toJsonTree(new FormattedLogDataEvent(logData,logEvent.getOffset())));
}"
5139,"@Override public Object encodeSend(LogEvent logEvent){
  ILoggingEvent event=logEvent.getLoggingEvent();
  StackTraceElement[] stackTraceElements=event.getCallerData();
  String className=""String_Node_Str"";
  String simpleClassName=""String_Node_Str"";
  int lineNumber=0;
  if (stackTraceElements != null && stackTraceElements.length > 0) {
    StackTraceElement first=stackTraceElements[0];
    className=first.getClassName();
    simpleClassName=(className.indexOf('.') >= 0) ? className.substring(className.lastIndexOf('.') + 1) : className;
    lineNumber=first.getLineNumber();
  }
  LogData logData=new LogData(event.getTimeStamp(),event.getLevel().toString(),event.getThreadName(),className,simpleClassName,lineNumber,event.getFormattedMessage(),ThrowableProxyUtil.asString(event.getThrowableProxy()));
  return modifyLogJsonElememnt(GSON.toJsonTree(new FormattedLogDataEvent(logData,logEvent.getOffset())));
}","@Override public Object encodeSend(LogEvent logEvent){
  ILoggingEvent event=logEvent.getLoggingEvent();
  StackTraceElement[] stackTraceElements=event.getCallerData();
  String className=""String_Node_Str"";
  String simpleClassName=""String_Node_Str"";
  int lineNumber=0;
  if (stackTraceElements != null && stackTraceElements.length > 0) {
    StackTraceElement first=stackTraceElements[0];
    className=first.getClassName();
    simpleClassName=(className.indexOf('.') >= 0) ? className.substring(className.lastIndexOf('.') + 1) : className;
    lineNumber=first.getLineNumber();
  }
  LogData logData=new LogData(event.getTimeStamp(),event.getLevel().toString(),event.getThreadName(),className,simpleClassName,lineNumber,event.getFormattedMessage(),ThrowableProxyUtil.asString(event.getThrowableProxy()),event.getLoggerName());
  return modifyLogJsonElememnt(GSON.toJsonTree(new FormattedLogDataEvent(logData,logEvent.getOffset())));
}"
5140,"@Inject public HBaseQueueClientFactory(CConfiguration cConf,Configuration hConf,HBaseTableUtil hBaseTableUtil,QueueAdmin queueAdmin,TransactionExecutorFactory txExecutorFactory){
  this.cConf=cConf;
  this.hConf=hConf;
  this.queueAdmin=(HBaseQueueAdmin)queueAdmin;
  this.queueUtil=new HBaseQueueUtilFactory().get();
  this.hBaseTableUtil=hBaseTableUtil;
  this.txExecutorFactory=txExecutorFactory;
  this.txMaxLifeTimeInMillis=TimeUnit.SECONDS.toMillis(cConf.getLong(Constants.Tephra.CFG_TX_MAX_LIFETIME,Constants.Tephra.DEFAULT_TX_MAX_LIFETIME));
}","@Inject public HBaseQueueClientFactory(CConfiguration cConf,Configuration hConf,HBaseTableUtil hBaseTableUtil,QueueAdmin queueAdmin,TransactionExecutorFactory txExecutorFactory){
  this.cConf=cConf;
  this.hConf=hConf;
  this.queueAdmin=(HBaseQueueAdmin)queueAdmin;
  this.queueUtil=new HBaseQueueUtilFactory().get();
  this.hBaseTableUtil=hBaseTableUtil;
  this.txExecutorFactory=txExecutorFactory;
  this.txMaxLifeTimeInMillis=TimeUnit.SECONDS.toMillis(cConf.getLong(TxConstants.Manager.CFG_TX_MAX_LIFETIME,TxConstants.Manager.DEFAULT_TX_MAX_LIFETIME));
}"
5141,"@Beta @POST @Path(""String_Node_Str"" + ""String_Node_Str"") @AuditPolicy({AuditDetail.REQUEST_BODY,AuditDetail.RESPONSE_BODY}) public void callArtifactPluginMethod(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginName,@PathParam(""String_Node_Str"") String pluginType,@PathParam(""String_Node_Str"") String methodName,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws Exception {
  String requestBody=request.getContent().toString(Charsets.UTF_8);
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  if (requestBody.isEmpty()) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  try {
    PluginEndpoint pluginEndpoint=pluginService.getPluginEndpoint(namespace,artifactId,pluginType,pluginName,methodName);
    Object response=pluginEndpoint.invoke(GSON.fromJson(requestBody,pluginEndpoint.getMethodParameterType()));
    responder.sendString(HttpResponseStatus.OK,GSON.toJson(response));
  }
 catch (  JsonSyntaxException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
  }
catch (  InvocationTargetException e) {
    if (e.getCause() instanceof javax.ws.rs.NotFoundException) {
      throw new NotFoundException(e.getCause());
    }
 else     if (e.getCause() instanceof javax.ws.rs.BadRequestException) {
      throw new BadRequestException(e.getCause());
    }
 else     if (e.getCause() instanceof IllegalArgumentException && e.getCause() != null) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getCause().getMessage());
    }
 else {
      Throwable rootCause=Throwables.getRootCause(e);
      String message=""String_Node_Str"";
      if (rootCause != null && rootCause.getMessage() != null) {
        message=String.format(""String_Node_Str"",message,rootCause.getMessage());
      }
      responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,message);
    }
  }
}","@Beta @POST @Path(""String_Node_Str"" + ""String_Node_Str"") @AuditPolicy({AuditDetail.REQUEST_BODY,AuditDetail.RESPONSE_BODY}) public void callArtifactPluginMethod(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginName,@PathParam(""String_Node_Str"") String pluginType,@PathParam(""String_Node_Str"") String methodName,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws Exception {
  String requestBody=request.getContent().toString(Charsets.UTF_8);
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  if (requestBody.isEmpty()) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  try {
    PluginEndpoint pluginEndpoint=pluginService.getPluginEndpoint(namespace,artifactId,pluginType,pluginName,methodName);
    Object response=pluginEndpoint.invoke(GSON.fromJson(requestBody,pluginEndpoint.getMethodParameterType()));
    responder.sendString(HttpResponseStatus.OK,GSON.toJson(response));
  }
 catch (  JsonSyntaxException e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
  }
catch (  InvocationTargetException e) {
    LOG.error(""String_Node_Str"",e);
    if (e.getCause() instanceof javax.ws.rs.NotFoundException) {
      throw new NotFoundException(e.getCause());
    }
 else     if (e.getCause() instanceof javax.ws.rs.BadRequestException) {
      throw new BadRequestException(e.getCause());
    }
 else     if (e.getCause() instanceof IllegalArgumentException && e.getCause() != null) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getCause().getMessage());
    }
 else {
      Throwable rootCause=Throwables.getRootCause(e);
      String message=""String_Node_Str"";
      if (rootCause != null && rootCause.getMessage() != null) {
        message=String.format(""String_Node_Str"",message,rootCause.getMessage());
      }
      responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,message);
    }
  }
}"
5142,"/** 
 * Get all artifacts that match artifacts in the given ranges.
 * @param range the range to match artifacts in
 * @return an unmodifiable list of all artifacts that match the given ranges. If none exist, an empty list is returned
 */
public List<ArtifactDetail> getArtifacts(final ArtifactRange range) throws Exception {
  List<ArtifactDetail> artifacts=artifactStore.getArtifacts(range);
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter=authorizationEnforcer.createFilter(principal);
  return Lists.newArrayList(Iterables.filter(artifacts,new com.google.common.base.Predicate<ArtifactDetail>(){
    @Override public boolean apply(    ArtifactDetail artifactDetail){
      ArtifactId artifactId=artifactDetail.getDescriptor().getArtifactId();
      return filter.apply(range.getNamespace().artifact(artifactId.getName(),artifactId.getVersion().getVersion()));
    }
  }
));
}","/** 
 * Get all artifacts that match artifacts in the given ranges.
 * @param range the range to match artifacts in
 * @return an unmodifiable list of all artifacts that match the given ranges. If none exist, an empty list is returned
 */
public List<ArtifactDetail> getArtifacts(final ArtifactRange range) throws Exception {
  List<ArtifactDetail> artifacts=artifactStore.getArtifacts(range);
  if (NamespaceId.SYSTEM.equals(range.getNamespace())) {
    return artifacts;
  }
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter=authorizationEnforcer.createFilter(principal);
  return Lists.newArrayList(Iterables.filter(artifacts,new com.google.common.base.Predicate<ArtifactDetail>(){
    @Override public boolean apply(    ArtifactDetail artifactDetail){
      ArtifactId artifactId=artifactDetail.getDescriptor().getArtifactId();
      return filter.apply(range.getNamespace().artifact(artifactId.getName(),artifactId.getVersion().getVersion()));
    }
  }
));
}"
5143,"@Inject public HBaseQueueDebugger(HBaseTableUtil tableUtil,HBaseQueueAdmin queueAdmin,HBaseQueueClientFactory queueClientFactory,ZKClientService zkClientService,TransactionExecutorFactory txExecutorFactory,NamespaceQueryAdmin namespaceQueryAdmin,Store store,Impersonator impersonator,AuthorizationEnforcementService authorizationEnforcementService){
  this.tableUtil=tableUtil;
  this.queueAdmin=queueAdmin;
  this.queueClientFactory=queueClientFactory;
  this.zkClientService=zkClientService;
  this.txExecutorFactory=txExecutorFactory;
  this.namespaceQueryAdmin=namespaceQueryAdmin;
  this.store=store;
  this.impersonator=impersonator;
  this.authorizationEnforcementService=authorizationEnforcementService;
}","@Inject public HBaseQueueDebugger(HBaseTableUtil tableUtil,HBaseQueueAdmin queueAdmin,HBaseQueueClientFactory queueClientFactory,ZKClientService zkClientService,TransactionExecutorFactory txExecutorFactory,Store store,Impersonator impersonator,AuthorizationEnforcementService authorizationEnforcementService){
  this.tableUtil=tableUtil;
  this.queueAdmin=queueAdmin;
  this.queueClientFactory=queueClientFactory;
  this.zkClientService=zkClientService;
  this.txExecutorFactory=txExecutorFactory;
  this.store=store;
  this.impersonator=impersonator;
  this.authorizationEnforcementService=authorizationEnforcementService;
}"
5144,"public static void main(String[] args) throws Exception {
  if (args.length >= 1 && args[0].equals(""String_Node_Str"")) {
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println();
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"" + PROP_SHOW_PROGRESS + ""String_Node_Str"");
    System.out.println(""String_Node_Str"" + PROP_ROWS_CACHE + ""String_Node_Str""+ ""String_Node_Str"");
    System.exit(1);
  }
  final QueueName queueName=args.length >= 1 ? QueueName.from(URI.create(args[0])) : null;
  Long consumerGroupId=null;
  if (args.length >= 2) {
    Preconditions.checkNotNull(queueName);
    String consumerFlowlet=args[1];
    FlowId flowId=new FlowId(queueName.getFirstComponent(),queueName.getSecondComponent(),queueName.getThirdComponent());
    consumerGroupId=FlowUtils.generateConsumerGroupId(flowId,consumerFlowlet);
  }
  HBaseQueueDebugger debugger=createDebugger();
  debugger.startAndWait();
  if (queueName != null) {
    debugger.scanQueue(queueName,consumerGroupId);
  }
 else {
    debugger.scanAllQueues();
  }
  debugger.stopAndWait();
}","public static void main(String[] args) throws Exception {
  if (args.length >= 1 && args[0].equals(""String_Node_Str"")) {
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println();
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"" + PROP_SHOW_PROGRESS + ""String_Node_Str"");
    System.out.println(""String_Node_Str"" + PROP_ROWS_CACHE + ""String_Node_Str""+ ""String_Node_Str"");
    System.exit(1);
  }
  final QueueName queueName=args.length >= 1 ? QueueName.from(URI.create(args[0])) : null;
  Long consumerGroupId=null;
  if (args.length >= 2) {
    Preconditions.checkNotNull(queueName);
    String consumerFlowlet=args[1];
    FlowId flowId=new FlowId(queueName.getFirstComponent(),queueName.getSecondComponent(),queueName.getThirdComponent());
    consumerGroupId=FlowUtils.generateConsumerGroupId(flowId,consumerFlowlet);
  }
  final HBaseQueueDebugger debugger=createDebugger();
  debugger.startAndWait();
  Injector injector=createInjector(true);
  NoAuthService noAuthService=injector.getInstance(NoAuthService.class);
  noAuthService.startAndWait();
  NamespaceQueryAdmin namespaceQueryAdmin=noAuthService.getNamespaceQueryAdmin();
  Impersonator impersonator=noAuthService.getImpersonator();
  if (queueName != null) {
    final Long finalConsumerGroupId=consumerGroupId;
    impersonator.doAs(new NamespaceId(queueName.getFirstComponent()),new Callable<Void>(){
      @Override public Void call() throws Exception {
        debugger.scanQueue(queueName,finalConsumerGroupId);
        return null;
      }
    }
);
  }
 else {
    debugger.scanQueues(namespaceQueryAdmin.list());
  }
  noAuthService.stopAndWait();
  debugger.stopAndWait();
}"
5145,"public static HBaseQueueDebugger createDebugger() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  if (cConf.getBoolean(Constants.Security.Authorization.ENABLED)) {
    System.out.println(String.format(""String_Node_Str"",HBaseQueueDebugger.class.getSimpleName()));
    cConf.setBoolean(Constants.Security.Authorization.ENABLED,false);
  }
  SecurityUtil.loginForMasterService(cConf);
  Injector injector=Guice.createInjector(new ConfigModule(cConf,HBaseConfiguration.create()),new IOModule(),new ZKClientModule(),new LocationRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new TwillModule(),new ExploreClientModule(),new DataFabricModules().getDistributedModules(),new ServiceStoreModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new AppFabricServiceRuntimeModule().getDistributedModules(),new ProgramRunnerRuntimeModule().getDistributedModules(),new SystemDatasetRuntimeModule().getDistributedModules(),new NotificationServiceRuntimeModule().getDistributedModules(),new MetricsClientRuntimeModule().getDistributedModules(),new MetricsStoreModule(),new KafkaClientModule(),new NamespaceStoreModule().getDistributedModules(),new AuthorizationModule(),new AuthorizationEnforcementModule().getMasterModule(),new SecureStoreModules().getDistributedModules(),new MessagingClientModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(QueueClientFactory.class).to(HBaseQueueClientFactory.class).in(Singleton.class);
      bind(QueueAdmin.class).to(HBaseQueueAdmin.class).in(Singleton.class);
      bind(HBaseTableUtil.class).toProvider(HBaseTableUtilFactory.class);
      bind(Store.class).annotatedWith(Names.named(""String_Node_Str"")).to(DefaultStore.class).in(Singleton.class);
      bind(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str"")).to(DatasetFramework.class).in(Singleton.class);
    }
  }
);
  return injector.getInstance(HBaseQueueDebugger.class);
}","@VisibleForTesting static HBaseQueueDebugger createDebugger() throws Exception {
  return createInjector(false).getInstance(HBaseQueueDebugger.class);
}"
5146,"@Override protected void shutDown() throws Exception {
  authorizationEnforcementService.stopAndWait();
  zkClientService.stopAndWait();
}","@Override protected void shutDown() throws Exception {
  zkClientService.stopAndWait();
}"
5147,"@Override public Void call() throws Exception {
  SimpleQueueSpecificationGenerator queueSpecGenerator=new SimpleQueueSpecificationGenerator(flowId.getParent());
  Table<QueueSpecificationGenerator.Node,String,Set<QueueSpecification>> table=queueSpecGenerator.create(flow);
  for (  Table.Cell<QueueSpecificationGenerator.Node,String,Set<QueueSpecification>> cell : table.cellSet()) {
    if (cell.getRowKey().getType() == FlowletConnection.Type.FLOWLET) {
      for (      QueueSpecification queue : cell.getValue()) {
        QueueStatistics queueStats=scanQueue(queue.getQueueName(),null);
        totalStats.add(queueStats);
      }
    }
  }
  return null;
}","@Override public Void call() throws Exception {
  debugger.scanQueue(queueName,finalConsumerGroupId);
  return null;
}"
5148,"@Override protected void startUp() throws Exception {
  zkClientService.startAndWait();
  authorizationEnforcementService.startAndWait();
}","@Override protected void startUp() throws Exception {
  zkClientService.startAndWait();
}"
5149,"/** 
 * Launches the given main class. The main class will be loaded through the   {@link MapReduceClassLoader}.
 * @param mainClassName the main class to launch
 * @param args          arguments for the main class
 */
@SuppressWarnings(""String_Node_Str"") public static void launch(String mainClassName,String[] args){
  ClassLoader systemClassLoader=ClassLoader.getSystemClassLoader();
  List<URL> urls=ClassLoaders.getClassLoaderURLs(systemClassLoader,new ArrayList<URL>());
  URL resource=systemClassLoader.getResource(mainClassName.replace('.','/') + ""String_Node_Str"");
  if (resource == null) {
    throw new IllegalStateException(""String_Node_Str"" + mainClassName);
  }
  if (!urls.remove(ClassLoaders.getClassPathURL(mainClassName,resource))) {
    throw new IllegalStateException(""String_Node_Str"" + resource);
  }
  URL[] classLoaderUrls=urls.toArray(new URL[urls.size()]);
  ClassLoader mainClassLoader=new MainClassLoader(classLoaderUrls,systemClassLoader.getParent());
  ClassLoaders.setContextClassLoader(mainClassLoader);
  try {
    final ClassLoader classLoader=(ClassLoader)mainClassLoader.loadClass(MapReduceClassLoader.class.getName()).newInstance();
    Runtime.getRuntime().addShutdownHook(new Thread(){
      @Override public void run(){
        if (classLoader instanceof AutoCloseable) {
          try {
            ((AutoCloseable)classLoader).close();
          }
 catch (          Exception e) {
            System.err.println(""String_Node_Str"" + classLoader);
            e.printStackTrace();
          }
        }
      }
    }
);
    Thread.currentThread().setContextClassLoader(classLoader);
    classLoader.getClass().getDeclaredMethod(""String_Node_Str"").invoke(classLoader);
    classLoader.loadClass(""String_Node_Str"").getDeclaredMethod(""String_Node_Str"",String.class).invoke(null,mainClassName);
    Class<?> mainClass=classLoader.loadClass(mainClassName);
    Method mainMethod=mainClass.getMethod(""String_Node_Str"",String[].class);
    mainMethod.setAccessible(true);
    LOG.info(""String_Node_Str"",mainClassName,Arrays.toString(args));
    mainMethod.invoke(null,new Object[]{args});
    LOG.info(""String_Node_Str"",mainClassName);
  }
 catch (  Exception e) {
    throw new RuntimeException(""String_Node_Str"" + mainClassName + ""String_Node_Str"",e);
  }
}","/** 
 * Launches the given main class. The main class will be loaded through the   {@link MapReduceClassLoader}.
 * @param mainClassName the main class to launch
 * @param args          arguments for the main class
 */
@SuppressWarnings(""String_Node_Str"") public static void launch(String mainClassName,String[] args){
  Thread.setDefaultUncaughtExceptionHandler(new UncaughtExceptionHandler());
  ClassLoader systemClassLoader=ClassLoader.getSystemClassLoader();
  List<URL> urls=ClassLoaders.getClassLoaderURLs(systemClassLoader,new ArrayList<URL>());
  URL resource=systemClassLoader.getResource(mainClassName.replace('.','/') + ""String_Node_Str"");
  if (resource == null) {
    throw new IllegalStateException(""String_Node_Str"" + mainClassName);
  }
  if (!urls.remove(ClassLoaders.getClassPathURL(mainClassName,resource))) {
    throw new IllegalStateException(""String_Node_Str"" + resource);
  }
  URL[] classLoaderUrls=urls.toArray(new URL[urls.size()]);
  ClassLoader mainClassLoader=new MainClassLoader(classLoaderUrls,systemClassLoader.getParent());
  ClassLoaders.setContextClassLoader(mainClassLoader);
  try {
    final ClassLoader classLoader=(ClassLoader)mainClassLoader.loadClass(MapReduceClassLoader.class.getName()).newInstance();
    Runtime.getRuntime().addShutdownHook(new Thread(){
      @Override public void run(){
        if (classLoader instanceof AutoCloseable) {
          try {
            ((AutoCloseable)classLoader).close();
          }
 catch (          Exception e) {
            System.err.println(""String_Node_Str"" + classLoader);
            e.printStackTrace();
          }
        }
      }
    }
);
    Thread.currentThread().setContextClassLoader(classLoader);
    classLoader.getClass().getDeclaredMethod(""String_Node_Str"").invoke(classLoader);
    classLoader.loadClass(""String_Node_Str"").getDeclaredMethod(""String_Node_Str"",String.class).invoke(null,mainClassName);
    Class<?> mainClass=classLoader.loadClass(mainClassName);
    Method mainMethod=mainClass.getMethod(""String_Node_Str"",String[].class);
    mainMethod.setAccessible(true);
    LOG.info(""String_Node_Str"",mainClassName,Arrays.toString(args));
    mainMethod.invoke(null,new Object[]{args});
    LOG.info(""String_Node_Str"",mainClassName);
  }
 catch (  Exception e) {
    throw new RuntimeException(""String_Node_Str"" + mainClassName + ""String_Node_Str"",e);
  }
}"
5150,"@Override public void initialize(TwillContext context){
  System.setSecurityManager(new RunnableSecurityManager(System.getSecurityManager()));
  runLatch=new CountDownLatch(1);
  coreServices=new ArrayList<>();
  name=context.getSpecification().getName();
  LOG.info(""String_Node_Str"" + name);
  try {
    CommandLine cmdLine=parseArgs(context.getApplicationArguments());
    hConf=new Configuration();
    hConf.clear();
    hConf.addResource(new File(cmdLine.getOptionValue(RunnableOptions.HADOOP_CONF_FILE)).toURI().toURL());
    UserGroupInformation.setConfiguration(hConf);
    cConf=CConfiguration.create(new File(cmdLine.getOptionValue(RunnableOptions.CDAP_CONF_FILE)));
    programOpts=createProgramOptions(cmdLine,context,context.getSpecification().getConfigs());
    String principal=programOpts.getArguments().getOption(ProgramOptionConstants.PRINCIPAL);
    ProgramId programId=GSON.fromJson(cmdLine.getOptionValue(RunnableOptions.PROGRAM_ID),ProgramId.class);
    Injector injector=Guice.createInjector(createModule(context,programId,principal));
    coreServices.add(injector.getInstance(ZKClientService.class));
    coreServices.add(injector.getInstance(KafkaClientService.class));
    coreServices.add(injector.getInstance(BrokerService.class));
    coreServices.add(injector.getInstance(MetricsCollectionService.class));
    coreServices.add(injector.getInstance(StreamCoordinatorClient.class));
    coreServices.add(injector.getInstance(AuthorizationEnforcementService.class));
    logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
    logAppenderInitializer.initialize();
    programRunner=createProgramRunner(injector);
    try {
      Location programJarLocation=Locations.toLocation(new File(cmdLine.getOptionValue(RunnableOptions.JAR)));
      ApplicationSpecification appSpec=readAppSpec(new File(cmdLine.getOptionValue(RunnableOptions.APP_SPEC_FILE)));
      program=Programs.create(cConf,programRunner,new ProgramDescriptor(programId,appSpec),programJarLocation,BundleJarUtil.unJar(programJarLocation,Files.createTempDir()));
    }
 catch (    IOException e) {
      throw Throwables.propagate(e);
    }
    coreServices.add(new ProgramRunnableResourceReporter(program.getId(),injector.getInstance(MetricsCollectionService.class),context));
    LOG.info(""String_Node_Str"",name);
  }
 catch (  Throwable t) {
    LOG.error(t.getMessage(),t);
    throw Throwables.propagate(t);
  }
}","@Override public void initialize(TwillContext context){
  Thread.setDefaultUncaughtExceptionHandler(new UncaughtExceptionHandler());
  System.setSecurityManager(new RunnableSecurityManager(System.getSecurityManager()));
  runLatch=new CountDownLatch(1);
  coreServices=new ArrayList<>();
  name=context.getSpecification().getName();
  LOG.info(""String_Node_Str"" + name);
  try {
    CommandLine cmdLine=parseArgs(context.getApplicationArguments());
    hConf=new Configuration();
    hConf.clear();
    hConf.addResource(new File(cmdLine.getOptionValue(RunnableOptions.HADOOP_CONF_FILE)).toURI().toURL());
    UserGroupInformation.setConfiguration(hConf);
    cConf=CConfiguration.create(new File(cmdLine.getOptionValue(RunnableOptions.CDAP_CONF_FILE)));
    programOpts=createProgramOptions(cmdLine,context,context.getSpecification().getConfigs());
    String principal=programOpts.getArguments().getOption(ProgramOptionConstants.PRINCIPAL);
    ProgramId programId=GSON.fromJson(cmdLine.getOptionValue(RunnableOptions.PROGRAM_ID),ProgramId.class);
    Injector injector=Guice.createInjector(createModule(context,programId,principal));
    coreServices.add(injector.getInstance(ZKClientService.class));
    coreServices.add(injector.getInstance(KafkaClientService.class));
    coreServices.add(injector.getInstance(BrokerService.class));
    coreServices.add(injector.getInstance(MetricsCollectionService.class));
    coreServices.add(injector.getInstance(StreamCoordinatorClient.class));
    coreServices.add(injector.getInstance(AuthorizationEnforcementService.class));
    logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
    logAppenderInitializer.initialize();
    programRunner=createProgramRunner(injector);
    try {
      Location programJarLocation=Locations.toLocation(new File(cmdLine.getOptionValue(RunnableOptions.JAR)));
      ApplicationSpecification appSpec=readAppSpec(new File(cmdLine.getOptionValue(RunnableOptions.APP_SPEC_FILE)));
      program=Programs.create(cConf,programRunner,new ProgramDescriptor(programId,appSpec),programJarLocation,BundleJarUtil.unJar(programJarLocation,Files.createTempDir()));
    }
 catch (    IOException e) {
      throw Throwables.propagate(e);
    }
    coreServices.add(new ProgramRunnableResourceReporter(program.getId(),injector.getInstance(MetricsCollectionService.class),context));
    LOG.info(""String_Node_Str"",name);
  }
 catch (  Throwable t) {
    LOG.error(t.getMessage(),t);
    throw Throwables.propagate(t);
  }
}"
5151,"/** 
 * The main method. It simply call methods in the same sequence as if the program is started by jsvc.
 */
protected void doMain(final String[] args) throws Exception {
  init(args);
  final CountDownLatch shutdownLatch=new CountDownLatch(1);
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        try {
          DaemonMain.this.stop();
        }
  finally {
          try {
            DaemonMain.this.destroy();
          }
  finally {
            shutdownLatch.countDown();
          }
        }
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"" + t.getMessage(),t);
      }
    }
  }
);
  start();
  shutdownLatch.await();
}","/** 
 * The main method. It simply call methods in the same sequence as if the program is started by jsvc.
 */
protected void doMain(final String[] args) throws Exception {
  Thread.setDefaultUncaughtExceptionHandler(new UncaughtExceptionHandler());
  init(args);
  final CountDownLatch shutdownLatch=new CountDownLatch(1);
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        try {
          DaemonMain.this.stop();
        }
  finally {
          try {
            DaemonMain.this.destroy();
          }
  finally {
            shutdownLatch.countDown();
          }
        }
      }
 catch (      Throwable t) {
        LOG.error(""String_Node_Str"" + t.getMessage(),t);
      }
    }
  }
);
  start();
  shutdownLatch.await();
}"
5152,"@Override public final void initialize(TwillContext context){
  super.initialize(context);
  name=context.getSpecification().getName();
  LOG.info(""String_Node_Str"",name);
  Map<String,String> configs=context.getSpecification().getConfigs();
  try {
    hConf=new Configuration();
    hConf.clear();
    hConf.addResource(new File(configs.get(""String_Node_Str"")).toURI().toURL());
    UserGroupInformation.setConfiguration(hConf);
    cConf=CConfiguration.create(new File(configs.get(""String_Node_Str"")));
    LOG.debug(""String_Node_Str"",name,cConf);
    LOG.debug(""String_Node_Str"",name,hConf);
    Injector injector=doInit(context);
    services=Lists.newArrayList();
    services.add(injector.getInstance(ZKClientService.class));
    services.add(injector.getInstance(KafkaClientService.class));
    services.add(injector.getInstance(BrokerService.class));
    services.add(injector.getInstance(MetricsCollectionService.class));
    addServices(services);
    Preconditions.checkArgument(!services.isEmpty(),""String_Node_Str"");
    LOG.info(""String_Node_Str"",name);
  }
 catch (  Throwable t) {
    throw Throwables.propagate(t);
  }
}","@Override public final void initialize(TwillContext context){
  Thread.setDefaultUncaughtExceptionHandler(new UncaughtExceptionHandler());
  super.initialize(context);
  name=context.getSpecification().getName();
  LOG.info(""String_Node_Str"",name);
  Map<String,String> configs=context.getSpecification().getConfigs();
  try {
    hConf=new Configuration();
    hConf.clear();
    hConf.addResource(new File(configs.get(""String_Node_Str"")).toURI().toURL());
    UserGroupInformation.setConfiguration(hConf);
    cConf=CConfiguration.create(new File(configs.get(""String_Node_Str"")));
    LOG.debug(""String_Node_Str"",name,cConf);
    LOG.debug(""String_Node_Str"",name,hConf);
    Injector injector=doInit(context);
    services=Lists.newArrayList();
    services.add(injector.getInstance(ZKClientService.class));
    services.add(injector.getInstance(KafkaClientService.class));
    services.add(injector.getInstance(BrokerService.class));
    services.add(injector.getInstance(MetricsCollectionService.class));
    addServices(services);
    Preconditions.checkArgument(!services.isEmpty(),""String_Node_Str"");
    LOG.info(""String_Node_Str"",name);
  }
 catch (  Throwable t) {
    throw Throwables.propagate(t);
  }
}"
5153,"@Override protected void startUp() throws Exception {
  executorServer.startAndWait();
}","@Override protected void startUp() throws Exception {
  executorServer.startAndWait();
  LOG.debug(""String_Node_Str"");
}"
5154,"@Override protected void shutDown() throws Exception {
  executorServer.stopAndWait();
}","@Override protected void shutDown() throws Exception {
  executorServer.stopAndWait();
  LOG.debug(""String_Node_Str"");
}"
5155,"@Override public void execute(Runnable runnable){
  Thread t=new Thread(runnable,String.format(""String_Node_Str"",id.incrementAndGet()));
  t.setUncaughtExceptionHandler(h);
  t.start();
}","@Override public void execute(Runnable runnable){
  Thread t=new Thread(runnable,String.format(""String_Node_Str"",id.incrementAndGet()));
  t.start();
}"
5156,"/** 
 * @noinspection NullableProblems 
 */
@Override protected Executor executor(final State state){
  final AtomicInteger id=new AtomicInteger();
  final Thread.UncaughtExceptionHandler h=new Thread.UncaughtExceptionHandler(){
    @Override public void uncaughtException(    Thread t,    Throwable e){
    }
  }
;
  return new Executor(){
    @Override public void execute(    Runnable runnable){
      Thread t=new Thread(runnable,String.format(""String_Node_Str"",id.incrementAndGet()));
      t.setUncaughtExceptionHandler(h);
      t.start();
    }
  }
;
}","/** 
 * @noinspection NullableProblems 
 */
@Override protected Executor executor(final State state){
  final AtomicInteger id=new AtomicInteger();
  return new Executor(){
    @Override public void execute(    Runnable runnable){
      Thread t=new Thread(runnable,String.format(""String_Node_Str"",id.incrementAndGet()));
      t.start();
    }
  }
;
}"
5157,"@Override protected Executor executor(State state){
  final AtomicInteger id=new AtomicInteger();
  final Thread.UncaughtExceptionHandler h=new Thread.UncaughtExceptionHandler(){
    @Override public void uncaughtException(    Thread t,    Throwable e){
    }
  }
;
  return new Executor(){
    @Override public void execute(    Runnable runnable){
      Thread t=new Thread(runnable,String.format(""String_Node_Str"",id.incrementAndGet()));
      t.setUncaughtExceptionHandler(h);
      t.start();
    }
  }
;
}","@Override protected Executor executor(State state){
  final AtomicInteger id=new AtomicInteger();
  return new Executor(){
    @Override public void execute(    Runnable runnable){
      Thread t=new Thread(runnable,String.format(""String_Node_Str"",id.incrementAndGet()));
      t.start();
    }
  }
;
}"
5158,"@Override public void execute(Runnable runnable){
  Thread t=new Thread(runnable,String.format(""String_Node_Str"",id.incrementAndGet()));
  t.setUncaughtExceptionHandler(h);
  t.start();
}","@Override public void execute(Runnable runnable){
  Thread t=new Thread(runnable,String.format(""String_Node_Str"",id.incrementAndGet()));
  t.start();
}"
5159,"/** 
 * Launches the given main class. The main class will be loaded through the   {@link SparkRunnerClassLoader}.
 * @param mainClassName the main class to launch
 * @param args arguments for the main class
 */
@SuppressWarnings(""String_Node_Str"") public static void launch(String mainClassName,String[] args){
  ClassLoader systemClassLoader=ClassLoader.getSystemClassLoader();
  List<URL> urls=ClassLoaders.getClassLoaderURLs(systemClassLoader,new ArrayList<URL>());
  URL resource=systemClassLoader.getResource(mainClassName.replace('.','/') + ""String_Node_Str"");
  if (resource == null) {
    throw new IllegalStateException(""String_Node_Str"" + mainClassName);
  }
  if (!urls.remove(ClassLoaders.getClassPathURL(mainClassName,resource))) {
    throw new IllegalStateException(""String_Node_Str"" + resource);
  }
  URL[] classLoaderUrls=urls.toArray(new URL[urls.size()]);
  ClassLoader classLoader=new SparkRunnerClassLoader(classLoaderUrls,new MainClassLoader(classLoaderUrls,systemClassLoader.getParent()),false);
  Thread.currentThread().setContextClassLoader(classLoader);
  try {
    classLoader.loadClass(SparkRuntimeContextProvider.class.getName()).getMethod(""String_Node_Str"").invoke(null);
    classLoader.loadClass(StandardOutErrorRedirector.class.getName()).getDeclaredMethod(""String_Node_Str"",String.class).invoke(null,mainClassName);
    LOG.info(""String_Node_Str"",mainClassName,Arrays.toString(args));
    classLoader.loadClass(mainClassName).getMethod(""String_Node_Str"",String[].class).invoke(null,new Object[]{args});
    LOG.info(""String_Node_Str"",mainClassName);
  }
 catch (  Exception e) {
    throw new RuntimeException(""String_Node_Str"" + mainClassName + ""String_Node_Str"",e);
  }
}","/** 
 * Launches the given main class. The main class will be loaded through the   {@link SparkRunnerClassLoader}.
 * @param mainClassName the main class to launch
 * @param args arguments for the main class
 */
@SuppressWarnings(""String_Node_Str"") public static void launch(String mainClassName,String[] args){
  Thread.setDefaultUncaughtExceptionHandler(new UncaughtExceptionHandler());
  ClassLoader systemClassLoader=ClassLoader.getSystemClassLoader();
  List<URL> urls=ClassLoaders.getClassLoaderURLs(systemClassLoader,new ArrayList<URL>());
  URL resource=systemClassLoader.getResource(mainClassName.replace('.','/') + ""String_Node_Str"");
  if (resource == null) {
    throw new IllegalStateException(""String_Node_Str"" + mainClassName);
  }
  if (!urls.remove(ClassLoaders.getClassPathURL(mainClassName,resource))) {
    throw new IllegalStateException(""String_Node_Str"" + resource);
  }
  URL[] classLoaderUrls=urls.toArray(new URL[urls.size()]);
  ClassLoader classLoader=new SparkRunnerClassLoader(classLoaderUrls,new MainClassLoader(classLoaderUrls,systemClassLoader.getParent()),false);
  Thread.currentThread().setContextClassLoader(classLoader);
  try {
    classLoader.loadClass(SparkRuntimeContextProvider.class.getName()).getMethod(""String_Node_Str"").invoke(null);
    classLoader.loadClass(StandardOutErrorRedirector.class.getName()).getDeclaredMethod(""String_Node_Str"",String.class).invoke(null,mainClassName);
    LOG.info(""String_Node_Str"",mainClassName,Arrays.toString(args));
    classLoader.loadClass(mainClassName).getMethod(""String_Node_Str"",String[].class).invoke(null,new Object[]{args});
    LOG.info(""String_Node_Str"",mainClassName);
  }
 catch (  Exception e) {
    throw new RuntimeException(""String_Node_Str"" + mainClassName + ""String_Node_Str"",e);
  }
}"
5160,"public void testMacroEvaluationActionPipeline(Engine engine) throws Exception {
  ETLStage action1=new ETLStage(""String_Node_Str"",MockAction.getPlugin(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  ETLStage action2=new ETLStage(""String_Node_Str"",MockAction.getPlugin(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  ETLBatchConfig etlConfig=co.cask.cdap.etl.proto.v2.ETLBatchConfig.builder(""String_Node_Str"").addStage(action1).addStage(action2).addConnection(new Connection(action1.getName(),action2.getName())).setEngine(engine).build();
  Map<String,String> runtimeArguments=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  AppRequest<co.cask.cdap.etl.proto.v2.ETLBatchConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationId appId=NamespaceId.DEFAULT.app(""String_Node_Str"" + engine);
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkflowManager manager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  manager.setRuntimeArgs(runtimeArguments);
  manager.start(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
  manager.waitForRun(ProgramRunStatus.COMPLETED,3,TimeUnit.MINUTES);
  DataSetManager<Table> actionTableDS=getDataset(""String_Node_Str"");
  Assert.assertEquals(""String_Node_Str"",MockAction.readOutput(actionTableDS,""String_Node_Str"",""String_Node_Str""));
  appManager.getHistory(appId.workflow(SmartWorkflow.NAME).toId(),ProgramRunStatus.FAILED);
}","public void testMacroEvaluationActionPipeline(Engine engine) throws Exception {
  ETLStage action1=new ETLStage(""String_Node_Str"",MockAction.getPlugin(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  ETLBatchConfig etlConfig=co.cask.cdap.etl.proto.v2.ETLBatchConfig.builder(""String_Node_Str"").addStage(action1).setEngine(engine).build();
  Map<String,String> runtimeArguments=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  AppRequest<co.cask.cdap.etl.proto.v2.ETLBatchConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationId appId=NamespaceId.DEFAULT.app(""String_Node_Str"" + engine);
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkflowManager manager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  manager.setRuntimeArgs(runtimeArguments);
  manager.start(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
  manager.waitForRun(ProgramRunStatus.COMPLETED,3,TimeUnit.MINUTES);
  DataSetManager<Table> actionTableDS=getDataset(""String_Node_Str"");
  Assert.assertEquals(""String_Node_Str"",MockAction.readOutput(actionTableDS,""String_Node_Str"",""String_Node_Str""));
  appManager.getHistory(appId.workflow(SmartWorkflow.NAME).toId(),ProgramRunStatus.FAILED);
}"
5161,"/** 
 * Validate that this is a valid pipeline. A valid pipeline has the following properties: All stages in the pipeline have a unique name. Source stages have at least one output and no inputs. Sink stages have at least one input and no outputs. There are no cycles in the pipeline. All inputs into a stage have the same schema. ErrorTransforms only have BatchSource, Transform, or BatchAggregator as input stages Returns the stages in the order they should be configured to ensure that all input stages are configured before their output.
 * @param config the user provided configuration
 * @return the order to configure the stages in
 * @throws IllegalArgumentException if the pipeline is invalid
 */
private List<StageConnections> validateConfig(ETLConfig config){
  config.validate();
  if (config.getStages().isEmpty()) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  Set<String> actionStages=new HashSet<>();
  Map<String,String> stageTypes=new HashMap<>();
  Set<String> stageNames=new HashSet<>();
  for (  ETLStage stage : config.getStages()) {
    if (!stageNames.add(stage.getName())) {
      throw new IllegalArgumentException(String.format(""String_Node_Str"",stage.getName()));
    }
    if (Action.PLUGIN_TYPE.equals(stage.getPlugin().getType())) {
      actionStages.add(stage.getName());
    }
    stageTypes.put(stage.getName(),stage.getPlugin().getType());
  }
  for (  Connection connection : config.getConnections()) {
    if (!stageNames.contains(connection.getFrom())) {
      throw new IllegalArgumentException(String.format(""String_Node_Str"",connection,connection.getFrom()));
    }
    if (!stageNames.contains(connection.getTo())) {
      throw new IllegalArgumentException(String.format(""String_Node_Str"",connection,connection.getTo()));
    }
  }
  Dag dag=new Dag(config.getConnections());
  Map<String,StageConnections> stages=new HashMap<>();
  for (  ETLStage stage : config.getStages()) {
    String stageName=stage.getName();
    Set<String> stageInputs=dag.getNodeInputs(stageName);
    Set<String> stageOutputs=dag.getNodeOutputs(stageName);
    String stageType=stage.getPlugin().getType();
    if (isSource(stageType)) {
      if (!stageInputs.isEmpty() && !actionStages.containsAll(stageInputs)) {
        throw new IllegalArgumentException(String.format(""String_Node_Str"",stageName,Joiner.on(',').join(stageInputs)));
      }
    }
 else     if (isSink(stageType)) {
      if (!stageOutputs.isEmpty() && !actionStages.containsAll(stageOutputs)) {
        throw new IllegalArgumentException(String.format(""String_Node_Str"",stageName,Joiner.on(',').join(stageOutputs)));
      }
    }
 else {
      boolean isAction=Action.PLUGIN_TYPE.equals(stageType);
      if (!isAction) {
        if (stageInputs.isEmpty()) {
          throw new IllegalArgumentException(String.format(""String_Node_Str"",stageName));
        }
        if (stageOutputs.isEmpty()) {
          throw new IllegalArgumentException(String.format(""String_Node_Str"",stageName));
        }
      }
      boolean isErrorTransform=ErrorTransform.PLUGIN_TYPE.equals(stageType);
      if (isErrorTransform) {
        for (        String inputStage : stageInputs) {
          String inputType=stageTypes.get(inputStage);
          if (!VALID_ERROR_INPUTS.contains(inputType)) {
            throw new IllegalArgumentException(String.format(""String_Node_Str"",stageName,inputStage,inputType,Joiner.on(',').join(VALID_ERROR_INPUTS)));
          }
        }
      }
    }
    stages.put(stageName,new StageConnections(stage,stageInputs,stageOutputs));
  }
  List<StageConnections> traversalOrder=new ArrayList<>(stages.size());
  for (  String stageName : dag.getTopologicalOrder()) {
    traversalOrder.add(stages.get(stageName));
  }
  return traversalOrder;
}","/** 
 * Validate that this is a valid pipeline. A valid pipeline has the following properties: All stages in the pipeline have a unique name. Source stages have at least one output and no inputs. Sink stages have at least one input and no outputs. There are no cycles in the pipeline. All inputs into a stage have the same schema. ErrorTransforms only have BatchSource, Transform, or BatchAggregator as input stages Returns the stages in the order they should be configured to ensure that all input stages are configured before their output.
 * @param config the user provided configuration
 * @return the order to configure the stages in
 * @throws IllegalArgumentException if the pipeline is invalid
 */
private List<StageConnections> validateConfig(ETLConfig config){
  config.validate();
  if (config.getStages().isEmpty()) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  Set<String> actionStages=new HashSet<>();
  Map<String,String> stageTypes=new HashMap<>();
  Set<String> stageNames=new HashSet<>();
  for (  ETLStage stage : config.getStages()) {
    if (!stageNames.add(stage.getName())) {
      throw new IllegalArgumentException(String.format(""String_Node_Str"",stage.getName()));
    }
    if (isAction(stage.getPlugin().getType())) {
      actionStages.add(stage.getName());
    }
    stageTypes.put(stage.getName(),stage.getPlugin().getType());
  }
  for (  Connection connection : config.getConnections()) {
    if (!stageNames.contains(connection.getFrom())) {
      throw new IllegalArgumentException(String.format(""String_Node_Str"",connection,connection.getFrom()));
    }
    if (!stageNames.contains(connection.getTo())) {
      throw new IllegalArgumentException(String.format(""String_Node_Str"",connection,connection.getTo()));
    }
  }
  List<StageConnections> traversalOrder=new ArrayList<>(stageNames.size());
  if (config.getConnections().isEmpty()) {
    if (actionStages.size() == 1 && stageNames.size() == 1) {
      traversalOrder.add(new StageConnections(config.getStages().iterator().next(),Collections.<String>emptyList(),Collections.<String>emptyList()));
      return traversalOrder;
    }
 else {
      throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"");
    }
  }
  Dag dag=new Dag(config.getConnections());
  Map<String,StageConnections> stages=new HashMap<>();
  for (  ETLStage stage : config.getStages()) {
    String stageName=stage.getName();
    Set<String> stageInputs=dag.getNodeInputs(stageName);
    Set<String> stageOutputs=dag.getNodeOutputs(stageName);
    String stageType=stage.getPlugin().getType();
    if (isSource(stageType)) {
      if (!stageInputs.isEmpty() && !actionStages.containsAll(stageInputs)) {
        throw new IllegalArgumentException(String.format(""String_Node_Str"",stageName,Joiner.on(',').join(stageInputs)));
      }
    }
 else     if (isSink(stageType)) {
      if (!stageOutputs.isEmpty() && !actionStages.containsAll(stageOutputs)) {
        throw new IllegalArgumentException(String.format(""String_Node_Str"",stageName,Joiner.on(',').join(stageOutputs)));
      }
    }
 else {
      if (!isAction(stageType)) {
        if (stageInputs.isEmpty()) {
          throw new IllegalArgumentException(String.format(""String_Node_Str"",stageName));
        }
        if (stageOutputs.isEmpty()) {
          throw new IllegalArgumentException(String.format(""String_Node_Str"",stageName));
        }
      }
      boolean isErrorTransform=ErrorTransform.PLUGIN_TYPE.equals(stageType);
      if (isErrorTransform) {
        for (        String inputStage : stageInputs) {
          String inputType=stageTypes.get(inputStage);
          if (!VALID_ERROR_INPUTS.contains(inputType)) {
            throw new IllegalArgumentException(String.format(""String_Node_Str"",stageName,inputStage,inputType,Joiner.on(',').join(VALID_ERROR_INPUTS)));
          }
        }
      }
    }
    stages.put(stageName,new StageConnections(stage,stageInputs,stageOutputs));
  }
  for (  String stageName : dag.getTopologicalOrder()) {
    traversalOrder.add(stages.get(stageName));
  }
  return traversalOrder;
}"
5162,"@Test public void testDifferentInputSchemasForAction(){
  ETLPlugin mockAction=new ETLPlugin(""String_Node_Str"",Action.PLUGIN_TYPE,ImmutableMap.<String,String>of(),null);
  ETLBatchConfig config=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MOCK_SOURCE)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_A)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_B)).addStage(new ETLStage(""String_Node_Str"",MOCK_SINK)).addStage(new ETLStage(""String_Node_Str"",MOCK_SINK)).addStage(new ETLStage(""String_Node_Str"",mockAction)).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").build();
  PipelineSpec actual=specGenerator.generateSpec(config);
  Map<String,String> emptyMap=ImmutableMap.of();
  PipelineSpec expected=BatchPipelineSpec.builder().addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSource.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).setOutputSchema(SCHEMA_A).addOutputs(""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSink.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_A).addInputs(""String_Node_Str"").addOutputs(""String_Node_Str"").setErrorSchema(SCHEMA_A).build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSink.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_B).addInputs(""String_Node_Str"").addOutputs(""String_Node_Str"").setErrorSchema(SCHEMA_B).build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_A).setOutputSchema(SCHEMA_A).addInputs(""String_Node_Str"").addOutputs(""String_Node_Str"").setErrorSchema(SCHEMA_B).build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_A).setOutputSchema(SCHEMA_B).addInputs(""String_Node_Str"").addOutputs(""String_Node_Str"").setErrorSchema(SCHEMA_A).build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Action.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputs(""String_Node_Str"",""String_Node_Str"").build()).addConnections(config.getConnections()).setResources(config.getResources()).setDriverResources(config.getDriverResources()).setClientResources(config.getClientResources()).setStageLoggingEnabled(config.isStageLoggingEnabled()).build();
  Assert.assertEquals(expected,actual);
}","@Test public void testDifferentInputSchemasForAction(){
  ETLBatchConfig config=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MOCK_SOURCE)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_A)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_B)).addStage(new ETLStage(""String_Node_Str"",MOCK_SINK)).addStage(new ETLStage(""String_Node_Str"",MOCK_SINK)).addStage(new ETLStage(""String_Node_Str"",MOCK_ACTION)).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").build();
  PipelineSpec actual=specGenerator.generateSpec(config);
  Map<String,String> emptyMap=ImmutableMap.of();
  PipelineSpec expected=BatchPipelineSpec.builder().addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSource.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).setOutputSchema(SCHEMA_A).addOutputs(""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSink.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_A).addInputs(""String_Node_Str"").addOutputs(""String_Node_Str"").setErrorSchema(SCHEMA_A).build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSink.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_B).addInputs(""String_Node_Str"").addOutputs(""String_Node_Str"").setErrorSchema(SCHEMA_B).build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_A).setOutputSchema(SCHEMA_A).addInputs(""String_Node_Str"").addOutputs(""String_Node_Str"").setErrorSchema(SCHEMA_B).build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_A).setOutputSchema(SCHEMA_B).addInputs(""String_Node_Str"").addOutputs(""String_Node_Str"").setErrorSchema(SCHEMA_A).build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Action.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputs(""String_Node_Str"",""String_Node_Str"").build()).addConnections(config.getConnections()).setResources(config.getResources()).setDriverResources(config.getDriverResources()).setClientResources(config.getClientResources()).setStageLoggingEnabled(config.isStageLoggingEnabled()).build();
  Assert.assertEquals(expected,actual);
}"
5163,"@Beta @POST @Path(""String_Node_Str"" + ""String_Node_Str"") @AuditPolicy({AuditDetail.REQUEST_BODY,AuditDetail.RESPONSE_BODY}) public void callArtifactPluginMethod(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginName,@PathParam(""String_Node_Str"") String pluginType,@PathParam(""String_Node_Str"") String methodName,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws Exception {
  String requestBody=request.getContent().toString(Charsets.UTF_8);
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  if (requestBody.isEmpty()) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  try {
    PluginEndpoint pluginEndpoint=pluginService.getPluginEndpoint(namespace,artifactId,pluginType,pluginName,methodName);
    Object response=pluginEndpoint.invoke(GSON.fromJson(requestBody,pluginEndpoint.getMethodParameterType()));
    responder.sendString(HttpResponseStatus.OK,GSON.toJson(response));
  }
 catch (  JsonSyntaxException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
  }
catch (  InvocationTargetException e) {
    if (e.getCause() instanceof javax.ws.rs.NotFoundException) {
      throw new NotFoundException(e.getCause());
    }
 else     if (e.getCause() instanceof javax.ws.rs.BadRequestException) {
      throw new BadRequestException(e.getCause());
    }
 else     if (e.getCause() instanceof IllegalArgumentException && e.getCause() != null) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getCause().getMessage());
    }
 else {
      responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    }
  }
}","@Beta @POST @Path(""String_Node_Str"" + ""String_Node_Str"") @AuditPolicy({AuditDetail.REQUEST_BODY,AuditDetail.RESPONSE_BODY}) public void callArtifactPluginMethod(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginName,@PathParam(""String_Node_Str"") String pluginType,@PathParam(""String_Node_Str"") String methodName,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws Exception {
  String requestBody=request.getContent().toString(Charsets.UTF_8);
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  if (requestBody.isEmpty()) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  try {
    PluginEndpoint pluginEndpoint=pluginService.getPluginEndpoint(namespace,artifactId,pluginType,pluginName,methodName);
    Object response=pluginEndpoint.invoke(GSON.fromJson(requestBody,pluginEndpoint.getMethodParameterType()));
    responder.sendString(HttpResponseStatus.OK,GSON.toJson(response));
  }
 catch (  JsonSyntaxException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
  }
catch (  InvocationTargetException e) {
    if (e.getCause() instanceof javax.ws.rs.NotFoundException) {
      throw new NotFoundException(e.getCause());
    }
 else     if (e.getCause() instanceof javax.ws.rs.BadRequestException) {
      throw new BadRequestException(e.getCause());
    }
 else     if (e.getCause() instanceof IllegalArgumentException && e.getCause() != null) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getCause().getMessage());
    }
 else {
      Throwable rootCause=Throwables.getRootCause(e);
      String message=""String_Node_Str"";
      if (rootCause != null && rootCause.getMessage() != null) {
        message=String.format(""String_Node_Str"",message,rootCause.getMessage());
      }
      responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,message);
    }
  }
}"
5164,"@Beta @POST @Path(""String_Node_Str"" + ""String_Node_Str"") @AuditPolicy({AuditDetail.REQUEST_BODY,AuditDetail.RESPONSE_BODY}) public void callArtifactPluginMethod(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginName,@PathParam(""String_Node_Str"") String pluginType,@PathParam(""String_Node_Str"") String methodName,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws Exception {
  String requestBody=request.getContent().toString(Charsets.UTF_8);
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  if (requestBody.isEmpty()) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  try {
    PluginEndpoint pluginEndpoint=pluginService.getPluginEndpoint(namespace,artifactId,pluginType,pluginName,methodName);
    Object response=pluginEndpoint.invoke(GSON.fromJson(requestBody,pluginEndpoint.getMethodParameterType()));
    responder.sendString(HttpResponseStatus.OK,GSON.toJson(response));
  }
 catch (  JsonSyntaxException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
  }
catch (  InvocationTargetException e) {
    if (e.getCause() instanceof javax.ws.rs.NotFoundException) {
      throw new NotFoundException(e.getCause());
    }
 else     if (e.getCause() instanceof javax.ws.rs.BadRequestException) {
      throw new BadRequestException(e.getCause());
    }
 else     if (e.getCause() instanceof IllegalArgumentException && e.getCause() != null) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getCause().getMessage());
    }
 else {
      responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    }
  }
}","@Beta @POST @Path(""String_Node_Str"" + ""String_Node_Str"") @AuditPolicy({AuditDetail.REQUEST_BODY,AuditDetail.RESPONSE_BODY}) public void callArtifactPluginMethod(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginName,@PathParam(""String_Node_Str"") String pluginType,@PathParam(""String_Node_Str"") String methodName,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws Exception {
  String requestBody=request.getContent().toString(Charsets.UTF_8);
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  if (requestBody.isEmpty()) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  try {
    PluginEndpoint pluginEndpoint=pluginService.getPluginEndpoint(namespace,artifactId,pluginType,pluginName,methodName);
    Object response=pluginEndpoint.invoke(GSON.fromJson(requestBody,pluginEndpoint.getMethodParameterType()));
    responder.sendString(HttpResponseStatus.OK,GSON.toJson(response));
  }
 catch (  JsonSyntaxException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
  }
catch (  InvocationTargetException e) {
    if (e.getCause() instanceof javax.ws.rs.NotFoundException) {
      throw new NotFoundException(e.getCause());
    }
 else     if (e.getCause() instanceof javax.ws.rs.BadRequestException) {
      throw new BadRequestException(e.getCause());
    }
 else     if (e.getCause() instanceof IllegalArgumentException && e.getCause() != null) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getCause().getMessage());
    }
 else {
      Throwable rootCause=Throwables.getRootCause(e);
      String message=""String_Node_Str"";
      if (rootCause != null && rootCause.getMessage() != null) {
        message=String.format(""String_Node_Str"",message,rootCause.getMessage());
      }
      responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,message);
    }
  }
}"
5165,"/** 
 * Adds a schedule for a particular program. If the schedule with the name already exists, the method will throw RuntimeException.
 * @param program defines program to which a schedule is being added
 * @param scheduleSpecification defines the schedule to be added for the program
 */
void addSchedule(ProgramId program,ScheduleSpecification scheduleSpecification);","/** 
 * Adds a schedule for a particular program. If the schedule with the name already exists, the method will throw AlreadyExistsException unless overwrite is true. If overwrite is true then the existing schedule is updated.
 * @param program defines program to which a schedule is being added
 * @param scheduleSpecification defines the schedule to be added for the program
 * @param allowOverwrite whether to overwrite an existing schedule
 * @throws AlreadyExistsException when schedule already exists and overwrite is false
 */
void addSchedule(ProgramId program,ScheduleSpecification scheduleSpecification,boolean allowOverwrite) throws AlreadyExistsException ;"
5166,"/** 
 * Creates a temporary directory through the   {@link LocationFactory} provided to this class.
 */
private Location createTempLocationDirectory() throws IOException {
  ProgramId programId=context.getProgram().getId();
  String tempLocationName=String.format(""String_Node_Str"",cConf.get(Constants.AppFabric.TEMP_DIR),programId.getType().name().toLowerCase(),programId.getNamespaceId().getEntityName(),programId.getApplication(),programId.getProgram(),context.getRunId().getId());
  Location location=locationFactory.get(programId.getNamespaceId()).append(tempLocationName);
  location.mkdirs();
  return location;
}","/** 
 * Creates a temporary directory through the   {@link LocationFactory} provided to this class.
 */
private Location createTempLocationDirectory() throws IOException {
  ProgramId programId=context.getProgram().getId();
  String tempLocationName=String.format(""String_Node_Str"",cConf.get(Constants.AppFabric.TEMP_DIR),programId.getType().name().toLowerCase(),programId.getNamespace(),programId.getApplication(),programId.getProgram(),context.getRunId().getId());
  Location location=locationFactory.get(programId.getNamespaceId()).append(tempLocationName);
  location.mkdirs();
  return location;
}"
5167,"/** 
 * Creates a temporary directory through the   {@link LocationFactory} provided to this class.
 */
private Location createTempLocationDirectory() throws IOException {
  ProgramId programId=context.getProgram().getId();
  String tempLocationName=String.format(""String_Node_Str"",cConf.get(Constants.AppFabric.TEMP_DIR),programId.getType().name().toLowerCase(),programId.getNamespaceId(),programId.getApplication(),programId.getProgram(),context.getRunId().getId());
  Location location=locationFactory.get(programId.getNamespaceId()).append(tempLocationName);
  location.mkdirs();
  return location;
}","/** 
 * Creates a temporary directory through the   {@link LocationFactory} provided to this class.
 */
private Location createTempLocationDirectory() throws IOException {
  ProgramId programId=context.getProgram().getId();
  String tempLocationName=String.format(""String_Node_Str"",cConf.get(Constants.AppFabric.TEMP_DIR),programId.getType().name().toLowerCase(),programId.getNamespaceId().getEntityName(),programId.getApplication(),programId.getProgram(),context.getRunId().getId());
  Location location=locationFactory.get(programId.getNamespaceId()).append(tempLocationName);
  location.mkdirs();
  return location;
}"
5168,"/** 
 * Get all applications in the specified namespace that satisfy the specified predicate.
 * @param namespace the namespace to get apps from
 * @param predicate the predicate that must be satisfied in order to be returned
 * @return list of all applications in the namespace that satisfy the specified predicate
 */
public List<ApplicationRecord> getApps(final NamespaceId namespace,com.google.common.base.Predicate<ApplicationRecord> predicate) throws Exception {
  List<ApplicationRecord> appRecords=new ArrayList<>();
  Set<ApplicationId> appIds=new HashSet<>();
  for (  ApplicationSpecification appSpec : store.getAllApplications(namespace)) {
    appIds.add(namespace.app(appSpec.getName(),appSpec.getAppVersion()));
  }
  for (  ApplicationId appId : appIds) {
    ApplicationSpecification appSpec=store.getApplication(appId);
    if (appSpec == null) {
      continue;
    }
    ArtifactId artifactId=appSpec.getArtifactId();
    ArtifactSummary artifactSummary=artifactId == null ? new ArtifactSummary(appSpec.getName(),null) : ArtifactSummary.from(artifactId);
    ApplicationRecord record=new ApplicationRecord(artifactSummary,appId,appSpec.getDescription(),ownerAdmin.getImpersonationPrincipal(appId));
    if (predicate.apply(record)) {
      appRecords.add(record);
    }
  }
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter=authorizationEnforcer.createFilter(principal);
  return Lists.newArrayList(Iterables.filter(appRecords,new com.google.common.base.Predicate<ApplicationRecord>(){
    @Override public boolean apply(    ApplicationRecord appRecord){
      return filter.apply(namespace.app(appRecord.getName()));
    }
  }
));
}","/** 
 * Get all applications in the specified namespace that satisfy the specified predicate.
 * @param namespace the namespace to get apps from
 * @param predicate the predicate that must be satisfied in order to be returned
 * @return list of all applications in the namespace that satisfy the specified predicate
 */
public List<ApplicationRecord> getApps(final NamespaceId namespace,com.google.common.base.Predicate<ApplicationRecord> predicate) throws Exception {
  List<ApplicationRecord> appRecords=new ArrayList<>();
  Set<ApplicationId> appIds=new HashSet<>();
  for (  ApplicationSpecification appSpec : store.getAllApplications(namespace)) {
    appIds.add(namespace.app(appSpec.getName(),appSpec.getAppVersion()));
  }
  for (  ApplicationId appId : appIds) {
    ApplicationSpecification appSpec=store.getApplication(appId);
    if (appSpec == null) {
      continue;
    }
    ArtifactId artifactId=appSpec.getArtifactId();
    ArtifactSummary artifactSummary=artifactId == null ? new ArtifactSummary(appSpec.getName(),null) : ArtifactSummary.from(artifactId);
    ApplicationRecord record=new ApplicationRecord(artifactSummary,appId,appSpec.getDescription(),ownerAdmin.getOwnerPrincipal(appId));
    if (predicate.apply(record)) {
      appRecords.add(record);
    }
  }
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter=authorizationEnforcer.createFilter(principal);
  return Lists.newArrayList(Iterables.filter(appRecords,new com.google.common.base.Predicate<ApplicationRecord>(){
    @Override public boolean apply(    ApplicationRecord appRecord){
      return filter.apply(namespace.app(appRecord.getName()));
    }
  }
));
}"
5169,"/** 
 * Get detail about the specified application
 * @param appId the id of the application to get
 * @return detail about the specified application
 * @throws ApplicationNotFoundException if the specified application does not exist
 */
public ApplicationDetail getAppDetail(ApplicationId appId) throws Exception {
  ApplicationSpecification appSpec=store.getApplication(appId);
  if (appSpec == null) {
    throw new ApplicationNotFoundException(appId);
  }
  ensureAccess(appId);
  String ownerPrincipal=ownerAdmin.getImpersonationPrincipal(appId);
  return ApplicationDetail.fromSpec(appSpec,ownerPrincipal);
}","/** 
 * Get detail about the specified application
 * @param appId the id of the application to get
 * @return detail about the specified application
 * @throws ApplicationNotFoundException if the specified application does not exist
 */
public ApplicationDetail getAppDetail(ApplicationId appId) throws Exception {
  ApplicationSpecification appSpec=store.getApplication(appId);
  if (appSpec == null) {
    throw new ApplicationNotFoundException(appId);
  }
  ensureAccess(appId);
  String ownerPrincipal=ownerAdmin.getOwnerPrincipal(appId);
  return ApplicationDetail.fromSpec(appSpec,ownerPrincipal);
}"
5170,"/** 
 * Read the dataset meta data (instance and type) from MDS.
 */
private DatasetMeta getFromMds(DatasetId instance) throws Exception {
  DatasetSpecification spec=instanceManager.get(instance);
  if (spec == null) {
    throw new NotFoundException(instance);
  }
  spec=DatasetsUtil.fixOriginalProperties(spec);
  DatasetTypeId datasetTypeId=instance.getParent().datasetType(spec.getType());
  DatasetTypeMeta typeMeta=getTypeInfo(instance.getParent(),spec.getType());
  if (typeMeta == null) {
    throw new NotFoundException(datasetTypeId);
  }
  String ownerPrincipal=null;
  if (!NamespaceId.SYSTEM.equals(instance.getNamespaceId())) {
    ownerPrincipal=ownerAdmin.getImpersonationPrincipal(instance);
  }
  return new DatasetMeta(spec,typeMeta,null,ownerPrincipal);
}","/** 
 * Read the dataset meta data (instance and type) from MDS.
 */
private DatasetMeta getFromMds(DatasetId instance) throws Exception {
  DatasetSpecification spec=instanceManager.get(instance);
  if (spec == null) {
    throw new NotFoundException(instance);
  }
  spec=DatasetsUtil.fixOriginalProperties(spec);
  DatasetTypeId datasetTypeId=instance.getParent().datasetType(spec.getType());
  DatasetTypeMeta typeMeta=getTypeInfo(instance.getParent(),spec.getType());
  if (typeMeta == null) {
    throw new NotFoundException(datasetTypeId);
  }
  String ownerPrincipal=null;
  if (!NamespaceId.SYSTEM.equals(instance.getNamespaceId())) {
    ownerPrincipal=ownerAdmin.getOwnerPrincipal(instance);
  }
  return new DatasetMeta(spec,typeMeta,null,ownerPrincipal);
}"
5171,"@Override public StreamProperties getProperties(StreamId streamId) throws Exception {
  ensureAccess(streamId);
  String ownerPrincipal=ownerAdmin.getImpersonationPrincipal(streamId);
  StreamConfig config=getConfig(streamId);
  StreamSpecification spec=streamMetaStore.getStream(streamId);
  return new StreamProperties(config.getTTL(),config.getFormat(),config.getNotificationThresholdMB(),spec.getDescription(),ownerPrincipal);
}","@Override public StreamProperties getProperties(StreamId streamId) throws Exception {
  ensureAccess(streamId);
  String ownerPrincipal=ownerAdmin.getOwnerPrincipal(streamId);
  StreamConfig config=getConfig(streamId);
  StreamSpecification spec=streamMetaStore.getStream(streamId);
  return new StreamProperties(config.getTTL(),config.getFormat(),config.getNotificationThresholdMB(),spec.getDescription(),ownerPrincipal);
}"
5172,"private void handleException(Exception ex){
  LOG.warn(""String_Node_Str"" + tableName.getNamespaceAsString() + ""String_Node_Str""+ tableName.getNameAsString(),ex);
  if (ex instanceof IOException) {
    Thread.currentThread().interrupt();
  }
}","private void handleException(Exception ex){
  LOG.warn(""String_Node_Str"" + tableName.getNamespaceAsString() + ""String_Node_Str""+ tableName.getNameAsString(),ex);
  if (ex instanceof InterruptedException) {
    Thread.currentThread().interrupt();
  }
}"
5173,"private void handleException(Exception ex){
  LOG.warn(""String_Node_Str"" + tableName.getNamespaceAsString() + ""String_Node_Str""+ tableName.getNameAsString(),ex);
  if (ex instanceof IOException) {
    Thread.currentThread().interrupt();
  }
}","private void handleException(Exception ex){
  LOG.warn(""String_Node_Str"" + tableName.getNamespaceAsString() + ""String_Node_Str""+ tableName.getNameAsString(),ex);
  if (ex instanceof InterruptedException) {
    Thread.currentThread().interrupt();
  }
}"
5174,"private void handleException(Exception ex){
  LOG.warn(""String_Node_Str"" + tableName.getNamespaceAsString() + ""String_Node_Str""+ tableName.getNameAsString(),ex);
  if (ex instanceof IOException) {
    Thread.currentThread().interrupt();
  }
}","private void handleException(Exception ex){
  LOG.warn(""String_Node_Str"" + tableName.getNamespaceAsString() + ""String_Node_Str""+ tableName.getNameAsString(),ex);
  if (ex instanceof InterruptedException) {
    Thread.currentThread().interrupt();
  }
}"
5175,"private void handleException(Exception ex){
  LOG.warn(""String_Node_Str"" + tableName.getNamespaceAsString() + ""String_Node_Str""+ tableName.getNameAsString(),ex);
  if (ex instanceof IOException) {
    Thread.currentThread().interrupt();
  }
}","private void handleException(Exception ex){
  LOG.warn(""String_Node_Str"" + tableName.getNamespaceAsString() + ""String_Node_Str""+ tableName.getNameAsString(),ex);
  if (ex instanceof InterruptedException) {
    Thread.currentThread().interrupt();
  }
}"
5176,"private void handleException(Exception ex){
  LOG.warn(""String_Node_Str"" + tableName.getNamespaceAsString() + ""String_Node_Str""+ tableName.getNameAsString(),ex);
  if (ex instanceof IOException) {
    Thread.currentThread().interrupt();
  }
}","private void handleException(Exception ex){
  LOG.warn(""String_Node_Str"" + tableName.getNamespaceAsString() + ""String_Node_Str""+ tableName.getNameAsString(),ex);
  if (ex instanceof InterruptedException) {
    Thread.currentThread().interrupt();
  }
}"
5177,"private void startFlushThread(){
  flushThread=new Thread(""String_Node_Str""){
    @Override public void run(){
      while ((!isInterrupted()) && (!stopped)) {
        long now=System.currentTimeMillis();
        if (now > (lastChecked + pruneFlushInterval)) {
          try {
            User.runAsLoginUser(new PrivilegedExceptionAction<Void>(){
              @Override public Void run() throws Exception {
                while (!pruneEntries.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=pruneEntries.firstEntry();
                  dataJanitorState.savePruneUpperBoundForRegion(firstEntry.getKey(),firstEntry.getValue());
                  pruneEntries.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                while (!emptyRegions.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=emptyRegions.firstEntry();
                  dataJanitorState.saveEmptyRegionForTime(firstEntry.getValue(),firstEntry.getKey());
                  emptyRegions.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                return null;
              }
            }
);
          }
 catch (          IOException ex) {
            LOG.warn(""String_Node_Str"" + tableName.getNamespaceAsString() + ""String_Node_Str""+ tableName.getNameAsString(),ex);
          }
          lastChecked=now;
        }
        try {
          TimeUnit.SECONDS.sleep(1);
        }
 catch (        InterruptedException ex) {
          interrupt();
          break;
        }
      }
      LOG.info(""String_Node_Str"");
    }
  }
;
  flushThread.setDaemon(true);
  flushThread.start();
}","private void startFlushThread(){
  flushThread=new Thread(""String_Node_Str""){
    @Override public void run(){
      while ((!isInterrupted()) && (!stopped)) {
        long now=System.currentTimeMillis();
        if (now > (lastChecked + pruneFlushInterval)) {
          try {
            UserGroupInformation.getLoginUser().doAs(new PrivilegedExceptionAction<Void>(){
              @Override public Void run() throws Exception {
                while (!pruneEntries.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=pruneEntries.firstEntry();
                  dataJanitorState.savePruneUpperBoundForRegion(firstEntry.getKey(),firstEntry.getValue());
                  pruneEntries.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                while (!emptyRegions.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=emptyRegions.firstEntry();
                  dataJanitorState.saveEmptyRegionForTime(firstEntry.getValue(),firstEntry.getKey());
                  emptyRegions.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                return null;
              }
            }
);
          }
 catch (          IOException|InterruptedException ex) {
            handleException(ex);
          }
          lastChecked=now;
        }
        try {
          TimeUnit.SECONDS.sleep(1);
        }
 catch (        InterruptedException ex) {
          interrupt();
          break;
        }
      }
      LOG.info(""String_Node_Str"");
    }
  }
;
  flushThread.setDaemon(true);
  flushThread.start();
}"
5178,"private void startFlushThread(){
  flushThread=new Thread(""String_Node_Str""){
    @Override public void run(){
      while ((!isInterrupted()) && (!stopped)) {
        long now=System.currentTimeMillis();
        if (now > (lastChecked + pruneFlushInterval)) {
          try {
            User.runAsLoginUser(new PrivilegedExceptionAction<Void>(){
              @Override public Void run() throws Exception {
                while (!pruneEntries.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=pruneEntries.firstEntry();
                  dataJanitorState.savePruneUpperBoundForRegion(firstEntry.getKey(),firstEntry.getValue());
                  pruneEntries.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                while (!emptyRegions.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=emptyRegions.firstEntry();
                  dataJanitorState.saveEmptyRegionForTime(firstEntry.getValue(),firstEntry.getKey());
                  emptyRegions.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                return null;
              }
            }
);
          }
 catch (          IOException ex) {
            LOG.warn(""String_Node_Str"" + tableName.getNameWithNamespaceInclAsString(),ex);
          }
          lastChecked=now;
        }
        try {
          TimeUnit.SECONDS.sleep(1);
        }
 catch (        InterruptedException ex) {
          interrupt();
          break;
        }
      }
      LOG.info(""String_Node_Str"");
    }
  }
;
  flushThread.setDaemon(true);
  flushThread.start();
}","private void startFlushThread(){
  flushThread=new Thread(""String_Node_Str""){
    @Override public void run(){
      while ((!isInterrupted()) && (!stopped)) {
        long now=System.currentTimeMillis();
        if (now > (lastChecked + pruneFlushInterval)) {
          try {
            UserGroupInformation.getLoginUser().doAs(new PrivilegedExceptionAction<Void>(){
              @Override public Void run() throws Exception {
                while (!pruneEntries.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=pruneEntries.firstEntry();
                  dataJanitorState.savePruneUpperBoundForRegion(firstEntry.getKey(),firstEntry.getValue());
                  pruneEntries.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                while (!emptyRegions.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=emptyRegions.firstEntry();
                  dataJanitorState.saveEmptyRegionForTime(firstEntry.getValue(),firstEntry.getKey());
                  emptyRegions.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                return null;
              }
            }
);
          }
 catch (          IOException|InterruptedException ex) {
            handleException(ex);
          }
          lastChecked=now;
        }
        try {
          TimeUnit.SECONDS.sleep(1);
        }
 catch (        InterruptedException ex) {
          interrupt();
          break;
        }
      }
      LOG.info(""String_Node_Str"");
    }
  }
;
  flushThread.setDaemon(true);
  flushThread.start();
}"
5179,"private void startFlushThread(){
  flushThread=new Thread(""String_Node_Str""){
    @Override public void run(){
      while ((!isInterrupted()) && (!stopped)) {
        long now=System.currentTimeMillis();
        if (now > (lastChecked + pruneFlushInterval)) {
          try {
            User.runAsLoginUser(new PrivilegedExceptionAction<Void>(){
              @Override public Void run() throws Exception {
                while (!pruneEntries.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=pruneEntries.firstEntry();
                  dataJanitorState.savePruneUpperBoundForRegion(firstEntry.getKey(),firstEntry.getValue());
                  pruneEntries.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                while (!emptyRegions.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=emptyRegions.firstEntry();
                  dataJanitorState.saveEmptyRegionForTime(firstEntry.getValue(),firstEntry.getKey());
                  emptyRegions.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                return null;
              }
            }
);
          }
 catch (          IOException ex) {
            LOG.warn(""String_Node_Str"" + tableName.getNameWithNamespaceInclAsString(),ex);
          }
          lastChecked=now;
        }
        try {
          TimeUnit.SECONDS.sleep(1);
        }
 catch (        InterruptedException ex) {
          interrupt();
          break;
        }
      }
      LOG.info(""String_Node_Str"");
    }
  }
;
  flushThread.setDaemon(true);
  flushThread.start();
}","private void startFlushThread(){
  flushThread=new Thread(""String_Node_Str""){
    @Override public void run(){
      while ((!isInterrupted()) && (!stopped)) {
        long now=System.currentTimeMillis();
        if (now > (lastChecked + pruneFlushInterval)) {
          try {
            UserGroupInformation.getLoginUser().doAs(new PrivilegedExceptionAction<Void>(){
              @Override public Void run() throws Exception {
                while (!pruneEntries.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=pruneEntries.firstEntry();
                  dataJanitorState.savePruneUpperBoundForRegion(firstEntry.getKey(),firstEntry.getValue());
                  pruneEntries.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                while (!emptyRegions.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=emptyRegions.firstEntry();
                  dataJanitorState.saveEmptyRegionForTime(firstEntry.getValue(),firstEntry.getKey());
                  emptyRegions.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                return null;
              }
            }
);
          }
 catch (          IOException|InterruptedException ex) {
            handleException(ex);
          }
          lastChecked=now;
        }
        try {
          TimeUnit.SECONDS.sleep(1);
        }
 catch (        InterruptedException ex) {
          interrupt();
          break;
        }
      }
      LOG.info(""String_Node_Str"");
    }
  }
;
  flushThread.setDaemon(true);
  flushThread.start();
}"
5180,"private void startFlushThread(){
  flushThread=new Thread(""String_Node_Str""){
    @Override public void run(){
      while ((!isInterrupted()) && (!stopped)) {
        long now=System.currentTimeMillis();
        if (now > (lastChecked + pruneFlushInterval)) {
          try {
            User.runAsLoginUser(new PrivilegedExceptionAction<Void>(){
              @Override public Void run() throws Exception {
                while (!pruneEntries.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=pruneEntries.firstEntry();
                  dataJanitorState.savePruneUpperBoundForRegion(firstEntry.getKey(),firstEntry.getValue());
                  pruneEntries.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                while (!emptyRegions.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=emptyRegions.firstEntry();
                  dataJanitorState.saveEmptyRegionForTime(firstEntry.getValue(),firstEntry.getKey());
                  emptyRegions.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                return null;
              }
            }
);
          }
 catch (          IOException ex) {
            LOG.warn(""String_Node_Str"" + tableName.getNamespaceAsString() + ""String_Node_Str""+ tableName.getNameAsString(),ex);
          }
          lastChecked=now;
        }
        try {
          TimeUnit.SECONDS.sleep(1);
        }
 catch (        InterruptedException ex) {
          interrupt();
          break;
        }
      }
      LOG.info(""String_Node_Str"");
    }
  }
;
  flushThread.setDaemon(true);
  flushThread.start();
}","private void startFlushThread(){
  flushThread=new Thread(""String_Node_Str""){
    @Override public void run(){
      while ((!isInterrupted()) && (!stopped)) {
        long now=System.currentTimeMillis();
        if (now > (lastChecked + pruneFlushInterval)) {
          try {
            UserGroupInformation.getLoginUser().doAs(new PrivilegedExceptionAction<Void>(){
              @Override public Void run() throws Exception {
                while (!pruneEntries.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=pruneEntries.firstEntry();
                  dataJanitorState.savePruneUpperBoundForRegion(firstEntry.getKey(),firstEntry.getValue());
                  pruneEntries.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                while (!emptyRegions.isEmpty()) {
                  Map.Entry<byte[],Long> firstEntry=emptyRegions.firstEntry();
                  dataJanitorState.saveEmptyRegionForTime(firstEntry.getValue(),firstEntry.getKey());
                  emptyRegions.remove(firstEntry.getKey(),firstEntry.getValue());
                }
                return null;
              }
            }
);
          }
 catch (          IOException|InterruptedException ex) {
            handleException(ex);
          }
          lastChecked=now;
        }
        try {
          TimeUnit.SECONDS.sleep(1);
        }
 catch (        InterruptedException ex) {
          interrupt();
          break;
        }
      }
      LOG.info(""String_Node_Str"");
    }
  }
;
  flushThread.setDaemon(true);
  flushThread.start();
}"
5181,"/** 
 * Return a list of RegionPruneInfo. These regions are the ones that have the lowest prune upper bounds. If -1 is passed in, all the regions and their prune upper bound will be returned.
 * @param numRegions number of regions
 * @return Map of region name and its prune upper bound
 */
public Queue<RegionPruneInfo> getIdleRegions(Integer numRegions) throws IOException {
  List<RegionPruneInfo> regionPruneInfos=dataJanitorState.getPruneInfoForRegions(null);
  if (regionPruneInfos.isEmpty()) {
    return new LinkedList<>();
  }
  if (numRegions < 0) {
    numRegions=regionPruneInfos.size();
  }
  Queue<RegionPruneInfo> lowestPrunes=MinMaxPriorityQueue.orderedBy(new Comparator<RegionPruneInfo>(){
    @Override public int compare(    RegionPruneInfo o1,    RegionPruneInfo o2){
      return (int)(o1.getPruneUpperBound() - o2.getPruneUpperBound());
    }
  }
).maximumSize(numRegions).create();
  for (  RegionPruneInfo pruneInfo : regionPruneInfos) {
    lowestPrunes.add(pruneInfo);
  }
  return lowestPrunes;
}","/** 
 * Return a list of RegionPruneInfo. These regions are the ones that have the lowest prune upper bounds. If -1 is passed in, all the regions and their prune upper bound will be returned. Note that only the regions that are known to be live will be returned.
 * @param numRegions number of regions
 * @return Map of region name and its prune upper bound
 */
public Queue<RegionPruneInfo> getIdleRegions(Integer numRegions) throws IOException {
  List<RegionPruneInfo> regionPruneInfos=dataJanitorState.getPruneInfoForRegions(null);
  if (regionPruneInfos.isEmpty()) {
    return new LinkedList<>();
  }
  Set<String> pruneRegionNameSet=new HashSet<>();
  for (  RegionPruneInfo regionPruneInfo : regionPruneInfos) {
    pruneRegionNameSet.add(regionPruneInfo.getRegionNameAsString());
  }
  Map<Long,SortedSet<String>> latestTimeRegion=getRegionsOnOrBeforeTime(System.currentTimeMillis());
  if (!latestTimeRegion.isEmpty()) {
    SortedSet<String> liveRegions=latestTimeRegion.values().iterator().next();
    Set<String> liveRegionsWithPruneInfo=Sets.intersection(liveRegions,pruneRegionNameSet);
    List<RegionPruneInfo> liveRegionWithPruneInfoList=new ArrayList<>();
    for (    RegionPruneInfo regionPruneInfo : regionPruneInfos) {
      if (liveRegionsWithPruneInfo.contains(regionPruneInfo.getRegionNameAsString())) {
        liveRegionWithPruneInfoList.add(regionPruneInfo);
      }
    }
    regionPruneInfos=liveRegionWithPruneInfoList;
  }
  if (numRegions < 0) {
    numRegions=regionPruneInfos.size();
  }
  Queue<RegionPruneInfo> lowestPrunes=MinMaxPriorityQueue.orderedBy(new Comparator<RegionPruneInfo>(){
    @Override public int compare(    RegionPruneInfo o1,    RegionPruneInfo o2){
      return (int)(o1.getPruneUpperBound() - o2.getPruneUpperBound());
    }
  }
).maximumSize(numRegions).create();
  for (  RegionPruneInfo pruneInfo : regionPruneInfos) {
    lowestPrunes.add(pruneInfo);
  }
  return lowestPrunes;
}"
5182,"/** 
 * Return a list of RegionPruneInfo. These regions are the ones that have the lowest prune upper bounds. If -1 is passed in, all the regions and their prune upper bound will be returned.
 * @param numRegions number of regions
 * @return Map of region name and its prune upper bound
 */
public Queue<RegionPruneInfo> getIdleRegions(Integer numRegions) throws IOException {
  List<RegionPruneInfo> regionPruneInfos=dataJanitorState.getPruneInfoForRegions(null);
  if (regionPruneInfos.isEmpty()) {
    return new LinkedList<>();
  }
  if (numRegions < 0) {
    numRegions=regionPruneInfos.size();
  }
  Queue<RegionPruneInfo> lowestPrunes=MinMaxPriorityQueue.orderedBy(new Comparator<RegionPruneInfo>(){
    @Override public int compare(    RegionPruneInfo o1,    RegionPruneInfo o2){
      return (int)(o1.getPruneUpperBound() - o2.getPruneUpperBound());
    }
  }
).maximumSize(numRegions).create();
  for (  RegionPruneInfo pruneInfo : regionPruneInfos) {
    lowestPrunes.add(pruneInfo);
  }
  return lowestPrunes;
}","/** 
 * Return a list of RegionPruneInfo. These regions are the ones that have the lowest prune upper bounds. If -1 is passed in, all the regions and their prune upper bound will be returned. Note that only the regions that are known to be live will be returned.
 * @param numRegions number of regions
 * @return Map of region name and its prune upper bound
 */
public Queue<RegionPruneInfo> getIdleRegions(Integer numRegions) throws IOException {
  List<RegionPruneInfo> regionPruneInfos=dataJanitorState.getPruneInfoForRegions(null);
  if (regionPruneInfos.isEmpty()) {
    return new LinkedList<>();
  }
  Set<String> pruneRegionNameSet=new HashSet<>();
  for (  RegionPruneInfo regionPruneInfo : regionPruneInfos) {
    pruneRegionNameSet.add(regionPruneInfo.getRegionNameAsString());
  }
  Map<Long,SortedSet<String>> latestTimeRegion=getRegionsOnOrBeforeTime(System.currentTimeMillis());
  if (!latestTimeRegion.isEmpty()) {
    SortedSet<String> liveRegions=latestTimeRegion.values().iterator().next();
    Set<String> liveRegionsWithPruneInfo=Sets.intersection(liveRegions,pruneRegionNameSet);
    List<RegionPruneInfo> liveRegionWithPruneInfoList=new ArrayList<>();
    for (    RegionPruneInfo regionPruneInfo : regionPruneInfos) {
      if (liveRegionsWithPruneInfo.contains(regionPruneInfo.getRegionNameAsString())) {
        liveRegionWithPruneInfoList.add(regionPruneInfo);
      }
    }
    regionPruneInfos=liveRegionWithPruneInfoList;
  }
  if (numRegions < 0) {
    numRegions=regionPruneInfos.size();
  }
  Queue<RegionPruneInfo> lowestPrunes=MinMaxPriorityQueue.orderedBy(new Comparator<RegionPruneInfo>(){
    @Override public int compare(    RegionPruneInfo o1,    RegionPruneInfo o2){
      return (int)(o1.getPruneUpperBound() - o2.getPruneUpperBound());
    }
  }
).maximumSize(numRegions).create();
  for (  RegionPruneInfo pruneInfo : regionPruneInfos) {
    lowestPrunes.add(pruneInfo);
  }
  return lowestPrunes;
}"
5183,"/** 
 * Return a list of RegionPruneInfo. These regions are the ones that have the lowest prune upper bounds. If -1 is passed in, all the regions and their prune upper bound will be returned.
 * @param numRegions number of regions
 * @return Map of region name and its prune upper bound
 */
public Queue<RegionPruneInfo> getIdleRegions(Integer numRegions) throws IOException {
  List<RegionPruneInfo> regionPruneInfos=dataJanitorState.getPruneInfoForRegions(null);
  if (regionPruneInfos.isEmpty()) {
    return new LinkedList<>();
  }
  if (numRegions < 0) {
    numRegions=regionPruneInfos.size();
  }
  Queue<RegionPruneInfo> lowestPrunes=MinMaxPriorityQueue.orderedBy(new Comparator<RegionPruneInfo>(){
    @Override public int compare(    RegionPruneInfo o1,    RegionPruneInfo o2){
      return (int)(o1.getPruneUpperBound() - o2.getPruneUpperBound());
    }
  }
).maximumSize(numRegions).create();
  for (  RegionPruneInfo pruneInfo : regionPruneInfos) {
    lowestPrunes.add(pruneInfo);
  }
  return lowestPrunes;
}","/** 
 * Return a list of RegionPruneInfo. These regions are the ones that have the lowest prune upper bounds. If -1 is passed in, all the regions and their prune upper bound will be returned. Note that only the regions that are known to be live will be returned.
 * @param numRegions number of regions
 * @return Map of region name and its prune upper bound
 */
public Queue<RegionPruneInfo> getIdleRegions(Integer numRegions) throws IOException {
  List<RegionPruneInfo> regionPruneInfos=dataJanitorState.getPruneInfoForRegions(null);
  if (regionPruneInfos.isEmpty()) {
    return new LinkedList<>();
  }
  Set<String> pruneRegionNameSet=new HashSet<>();
  for (  RegionPruneInfo regionPruneInfo : regionPruneInfos) {
    pruneRegionNameSet.add(regionPruneInfo.getRegionNameAsString());
  }
  Map<Long,SortedSet<String>> latestTimeRegion=getRegionsOnOrBeforeTime(System.currentTimeMillis());
  if (!latestTimeRegion.isEmpty()) {
    SortedSet<String> liveRegions=latestTimeRegion.values().iterator().next();
    Set<String> liveRegionsWithPruneInfo=Sets.intersection(liveRegions,pruneRegionNameSet);
    List<RegionPruneInfo> liveRegionWithPruneInfoList=new ArrayList<>();
    for (    RegionPruneInfo regionPruneInfo : regionPruneInfos) {
      if (liveRegionsWithPruneInfo.contains(regionPruneInfo.getRegionNameAsString())) {
        liveRegionWithPruneInfoList.add(regionPruneInfo);
      }
    }
    regionPruneInfos=liveRegionWithPruneInfoList;
  }
  if (numRegions < 0) {
    numRegions=regionPruneInfos.size();
  }
  Queue<RegionPruneInfo> lowestPrunes=MinMaxPriorityQueue.orderedBy(new Comparator<RegionPruneInfo>(){
    @Override public int compare(    RegionPruneInfo o1,    RegionPruneInfo o2){
      return (int)(o1.getPruneUpperBound() - o2.getPruneUpperBound());
    }
  }
).maximumSize(numRegions).create();
  for (  RegionPruneInfo pruneInfo : regionPruneInfos) {
    lowestPrunes.add(pruneInfo);
  }
  return lowestPrunes;
}"
5184,"@Override public void run(){
  LOG.info(""String_Node_Str"");
  try {
    new HBaseTableUtilFactory(cConf).get();
  }
 catch (  ProvisionException e) {
    throw new RuntimeException(""String_Node_Str"" + HBaseVersion.getVersionString());
  }
  LOG.info(""String_Node_Str"");
  LOG.info(""String_Node_Str"");
  try (HConnection hbaseConnection=HConnectionManager.createConnection(hConf)){
    hbaseConnection.listTables();
    LOG.info(""String_Node_Str"");
  }
 catch (  IOException e) {
    throw new RuntimeException(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",e);
  }
}","@Override public void run(){
  LOG.info(""String_Node_Str"");
  HBaseTableUtil hBaseTableUtil;
  try {
    hBaseTableUtil=new HBaseTableUtilFactory(cConf).get();
  }
 catch (  ProvisionException e) {
    throw new RuntimeException(""String_Node_Str"" + HBaseVersion.getVersionString());
  }
  LOG.info(""String_Node_Str"");
  LOG.info(""String_Node_Str"");
  try (HConnection hbaseConnection=HConnectionManager.createConnection(hConf)){
    hbaseConnection.listTables();
    LOG.info(""String_Node_Str"");
  }
 catch (  IOException e) {
    throw new RuntimeException(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",e);
  }
  if (hConf.getBoolean(""String_Node_Str"",false)) {
    if (cConf.getBoolean(TxConstants.TransactionPruning.PRUNE_ENABLE)) {
      LOG.info(""String_Node_Str"");
      try {
        boolean isGlobalAdmin=hBaseTableUtil.isGlobalAdmin(hConf);
        LOG.info(""String_Node_Str"",isGlobalAdmin);
        if (isGlobalAdmin) {
          return;
        }
        if (cConf.getBoolean(Constants.Startup.TX_PRUNE_ACL_CHECK,false)) {
          LOG.info(""String_Node_Str"" + ""String_Node_Str"",Constants.Startup.TX_PRUNE_ACL_CHECK);
          return;
        }
        StringBuilder builder=new StringBuilder(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
        builder.append(Constants.Startup.TX_PRUNE_ACL_CHECK);
        builder.append(""String_Node_Str"");
        if (HBaseVersion.get().equals(HBaseVersion.Version.HBASE_96) || HBaseVersion.get().equals(HBaseVersion.Version.HBASE_98)) {
          builder.append(""String_Node_Str"");
          builder.append(HBaseVersion.get());
          builder.append(""String_Node_Str"");
          builder.append(""String_Node_Str"");
          builder.append(Constants.Startup.TX_PRUNE_ACL_CHECK);
        }
        throw new RuntimeException(builder.toString());
      }
 catch (      IOException e) {
        throw new RuntimeException(""String_Node_Str"");
      }
    }
  }
  LOG.info(""String_Node_Str"");
}"
5185,"private BodyConsumer deployApplication(final HttpResponder responder,final NamespaceId namespace,final String appId,final String archiveName,final String configString,@Nullable final String ownerPrincipal) throws IOException {
  Id.Namespace idNamespace=namespace.toId();
  Location namespaceHomeLocation=namespacedLocationFactory.get(idNamespace);
  if (!namespaceHomeLocation.exists()) {
    String msg=String.format(""String_Node_Str"",namespaceHomeLocation,namespace.getNamespace());
    LOG.error(msg);
    responder.sendString(HttpResponseStatus.NOT_FOUND,msg);
    return null;
  }
  if (archiveName == null || archiveName.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,String.format(""String_Node_Str"",ARCHIVE_NAME_HEADER),ImmutableMultimap.of(HttpHeaders.Names.CONNECTION,HttpHeaders.Values.CLOSE));
    return null;
  }
  final Id.Artifact artifactId;
  try {
    artifactId=Id.Artifact.parse(idNamespace,archiveName);
  }
 catch (  IllegalArgumentException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    return null;
  }
  KerberosPrincipalId ownerPrincipalId=ownerPrincipal == null ? null : new KerberosPrincipalId(ownerPrincipal);
  String namespacesDir=configuration.get(Constants.Namespace.NAMESPACES_DIR);
  File localDataDir=new File(configuration.get(Constants.CFG_LOCAL_DATA_DIR));
  File namespaceBase=new File(localDataDir,namespacesDir);
  File tempDir=new File(new File(namespaceBase,namespace.getNamespace()),configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  if (!DirUtils.mkdirs(tempDir)) {
    throw new IOException(""String_Node_Str"" + tempDir);
  }
  final KerberosPrincipalId finalOwnerPrincipalId=ownerPrincipalId;
  return new AbstractBodyConsumer(File.createTempFile(""String_Node_Str"",""String_Node_Str"",tempDir)){
    @Override protected void onFinish(    HttpResponder responder,    File uploadedFile){
      try {
        ApplicationWithPrograms app=applicationLifecycleService.deployAppAndArtifact(namespace,appId,artifactId,uploadedFile,configString,finalOwnerPrincipalId,createProgramTerminator());
        LOG.info(""String_Node_Str"" + ""String_Node_Str"",app.getApplicationId().getApplication(),namespace.getNamespace(),artifactId,configString,finalOwnerPrincipalId);
        responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",app.getApplicationId().getApplication()));
      }
 catch (      InvalidArtifactException e) {
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
catch (      ArtifactAlreadyExistsException e) {
        responder.sendString(HttpResponseStatus.CONFLICT,String.format(""String_Node_Str"" + ""String_Node_Str"",artifactId));
      }
catch (      WriteConflictException e) {
        LOG.warn(""String_Node_Str"",artifactId,e);
        responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + ""String_Node_Str"");
      }
catch (      UnauthorizedException e) {
        responder.sendString(HttpResponseStatus.FORBIDDEN,e.getMessage());
      }
catch (      Exception e) {
        LOG.error(""String_Node_Str"",e);
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
    }
  }
;
}","private BodyConsumer deployApplication(final HttpResponder responder,final NamespaceId namespace,final String appId,final String archiveName,final String configString,@Nullable final String ownerPrincipal) throws IOException {
  Id.Namespace idNamespace=namespace.toId();
  Location namespaceHomeLocation=namespacedLocationFactory.get(idNamespace);
  if (!namespaceHomeLocation.exists()) {
    String msg=String.format(""String_Node_Str"",namespaceHomeLocation,namespace.getNamespace());
    LOG.error(msg);
    responder.sendString(HttpResponseStatus.NOT_FOUND,msg);
    return null;
  }
  if (archiveName == null || archiveName.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,String.format(""String_Node_Str"",ARCHIVE_NAME_HEADER),ImmutableMultimap.of(HttpHeaders.Names.CONNECTION,HttpHeaders.Values.CLOSE));
    return null;
  }
  final Id.Artifact artifactId;
  try {
    artifactId=Id.Artifact.parse(idNamespace,archiveName);
  }
 catch (  IllegalArgumentException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    return null;
  }
  KerberosPrincipalId ownerPrincipalId=ownerPrincipal == null ? null : new KerberosPrincipalId(ownerPrincipal);
  String namespacesDir=configuration.get(Constants.Namespace.NAMESPACES_DIR);
  File localDataDir=new File(configuration.get(Constants.CFG_LOCAL_DATA_DIR));
  File namespaceBase=new File(localDataDir,namespacesDir);
  File tempDir=new File(new File(namespaceBase,namespace.getNamespace()),configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  if (!DirUtils.mkdirs(tempDir)) {
    throw new IOException(""String_Node_Str"" + tempDir);
  }
  final KerberosPrincipalId finalOwnerPrincipalId=ownerPrincipalId;
  return new AbstractBodyConsumer(File.createTempFile(""String_Node_Str"",""String_Node_Str"",tempDir)){
    @Override protected void onFinish(    HttpResponder responder,    File uploadedFile){
      try {
        ApplicationWithPrograms app=applicationLifecycleService.deployAppAndArtifact(namespace,appId,artifactId,uploadedFile,configString,finalOwnerPrincipalId,createProgramTerminator());
        LOG.info(""String_Node_Str"" + ""String_Node_Str"",app.getApplicationId().getApplication(),namespace.getNamespace(),artifactId,configString,finalOwnerPrincipalId);
        responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",app.getApplicationId().getApplication()));
      }
 catch (      InvalidArtifactException e) {
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
catch (      ArtifactAlreadyExistsException e) {
        responder.sendString(HttpResponseStatus.CONFLICT,String.format(""String_Node_Str"" + ""String_Node_Str"",artifactId));
      }
catch (      WriteConflictException e) {
        LOG.warn(""String_Node_Str"",artifactId,e);
        responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + ""String_Node_Str"");
      }
catch (      UnauthorizedException e) {
        responder.sendString(HttpResponseStatus.FORBIDDEN,e.getMessage());
      }
catch (      ConflictException e) {
        responder.sendString(HttpResponseStatus.CONFLICT,e.getMessage());
      }
catch (      Exception e) {
        LOG.error(""String_Node_Str"",e);
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
    }
  }
;
}"
5186,"/** 
 * Updates an existing application.
 */
@POST @Path(""String_Node_Str"") @AuditPolicy(AuditDetail.REQUEST_BODY) public void updateApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String namespaceId,@PathParam(""String_Node_Str"") final String appName) throws NotFoundException, BadRequestException, UnauthorizedException, IOException {
  ApplicationId appId=validateApplicationId(namespaceId,appName);
  AppRequest appRequest;
  try (Reader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()),Charsets.UTF_8)){
    appRequest=GSON.fromJson(reader,AppRequest.class);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",appName,namespaceId,e);
    throw new IOException(""String_Node_Str"");
  }
catch (  JsonSyntaxException e) {
    throw new BadRequestException(""String_Node_Str"" + e.getMessage());
  }
  try {
    applicationLifecycleService.updateApp(appId,appRequest,createProgramTerminator());
    responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
  }
 catch (  InvalidArtifactException e) {
    throw new BadRequestException(e.getMessage());
  }
catch (  NotFoundException|UnauthorizedException e) {
    throw e;
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
}","/** 
 * Updates an existing application.
 */
@POST @Path(""String_Node_Str"") @AuditPolicy(AuditDetail.REQUEST_BODY) public void updateApp(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") final String namespaceId,@PathParam(""String_Node_Str"") final String appName) throws NotFoundException, BadRequestException, UnauthorizedException, IOException {
  ApplicationId appId=validateApplicationId(namespaceId,appName);
  AppRequest appRequest;
  try (Reader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()),Charsets.UTF_8)){
    appRequest=GSON.fromJson(reader,AppRequest.class);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",appName,namespaceId,e);
    throw new IOException(""String_Node_Str"");
  }
catch (  JsonSyntaxException e) {
    throw new BadRequestException(""String_Node_Str"" + e.getMessage());
  }
  try {
    applicationLifecycleService.updateApp(appId,appRequest,createProgramTerminator());
    responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
  }
 catch (  InvalidArtifactException e) {
    throw new BadRequestException(e.getMessage());
  }
catch (  ConflictException e) {
    responder.sendString(HttpResponseStatus.CONFLICT,e.getMessage());
  }
catch (  NotFoundException|UnauthorizedException e) {
    throw e;
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
}"
5187,"private BodyConsumer deployAppFromArtifact(final ApplicationId appId) throws IOException {
  return new AbstractBodyConsumer(File.createTempFile(""String_Node_Str"" + appId,""String_Node_Str"",tmpDir)){
    @Override protected void onFinish(    HttpResponder responder,    File uploadedFile){
      try (FileReader fileReader=new FileReader(uploadedFile)){
        AppRequest<?> appRequest=GSON.fromJson(fileReader,AppRequest.class);
        ArtifactSummary artifactSummary=appRequest.getArtifact();
        NamespaceId artifactNamespace=ArtifactScope.SYSTEM.equals(artifactSummary.getScope()) ? NamespaceId.SYSTEM : appId.getParent();
        Id.Artifact artifactId=Id.Artifact.from(artifactNamespace.toId(),artifactSummary.getName(),artifactSummary.getVersion());
        KerberosPrincipalId ownerPrincipalId=appRequest.getOwnerPrincipal() == null ? null : new KerberosPrincipalId(appRequest.getOwnerPrincipal());
        String configString=appRequest.getConfig() == null ? null : GSON.toJson(appRequest.getConfig());
        applicationLifecycleService.deployApp(appId.getParent(),appId.getApplication(),appId.getVersion(),artifactId,configString,createProgramTerminator(),ownerPrincipalId);
        responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
      }
 catch (      ArtifactNotFoundException e) {
        responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
      }
catch (      InvalidArtifactException e) {
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
catch (      IOException e) {
        LOG.error(""String_Node_Str"",appId);
        responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,String.format(""String_Node_Str"",appId));
      }
catch (      Exception e) {
        LOG.error(""String_Node_Str"",e);
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
    }
  }
;
}","private BodyConsumer deployAppFromArtifact(final ApplicationId appId) throws IOException {
  return new AbstractBodyConsumer(File.createTempFile(""String_Node_Str"" + appId,""String_Node_Str"",tmpDir)){
    @Override protected void onFinish(    HttpResponder responder,    File uploadedFile){
      try (FileReader fileReader=new FileReader(uploadedFile)){
        AppRequest<?> appRequest=GSON.fromJson(fileReader,AppRequest.class);
        ArtifactSummary artifactSummary=appRequest.getArtifact();
        NamespaceId artifactNamespace=ArtifactScope.SYSTEM.equals(artifactSummary.getScope()) ? NamespaceId.SYSTEM : appId.getParent();
        Id.Artifact artifactId=Id.Artifact.from(artifactNamespace.toId(),artifactSummary.getName(),artifactSummary.getVersion());
        KerberosPrincipalId ownerPrincipalId=appRequest.getOwnerPrincipal() == null ? null : new KerberosPrincipalId(appRequest.getOwnerPrincipal());
        String configString=appRequest.getConfig() == null ? null : GSON.toJson(appRequest.getConfig());
        applicationLifecycleService.deployApp(appId.getParent(),appId.getApplication(),appId.getVersion(),artifactId,configString,createProgramTerminator(),ownerPrincipalId);
        responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
      }
 catch (      ArtifactNotFoundException e) {
        responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
      }
catch (      ConflictException e) {
        responder.sendString(HttpResponseStatus.CONFLICT,e.getMessage());
      }
catch (      UnauthorizedException e) {
        responder.sendString(HttpResponseStatus.FORBIDDEN,e.getMessage());
      }
catch (      InvalidArtifactException e) {
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
catch (      IOException e) {
        LOG.error(""String_Node_Str"",appId);
        responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,String.format(""String_Node_Str"",appId));
      }
catch (      Exception e) {
        LOG.error(""String_Node_Str"",e);
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
    }
  }
;
}"
5188,"@Override protected void onFinish(HttpResponder responder,File uploadedFile){
  try {
    ApplicationWithPrograms app=applicationLifecycleService.deployAppAndArtifact(namespace,appId,artifactId,uploadedFile,configString,finalOwnerPrincipalId,createProgramTerminator());
    LOG.info(""String_Node_Str"" + ""String_Node_Str"",app.getApplicationId().getApplication(),namespace.getNamespace(),artifactId,configString,finalOwnerPrincipalId);
    responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",app.getApplicationId().getApplication()));
  }
 catch (  InvalidArtifactException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
catch (  ArtifactAlreadyExistsException e) {
    responder.sendString(HttpResponseStatus.CONFLICT,String.format(""String_Node_Str"" + ""String_Node_Str"",artifactId));
  }
catch (  WriteConflictException e) {
    LOG.warn(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + ""String_Node_Str"");
  }
catch (  UnauthorizedException e) {
    responder.sendString(HttpResponseStatus.FORBIDDEN,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
}","@Override protected void onFinish(HttpResponder responder,File uploadedFile){
  try {
    ApplicationWithPrograms app=applicationLifecycleService.deployAppAndArtifact(namespace,appId,artifactId,uploadedFile,configString,finalOwnerPrincipalId,createProgramTerminator());
    LOG.info(""String_Node_Str"" + ""String_Node_Str"",app.getApplicationId().getApplication(),namespace.getNamespace(),artifactId,configString,finalOwnerPrincipalId);
    responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",app.getApplicationId().getApplication()));
  }
 catch (  InvalidArtifactException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
catch (  ArtifactAlreadyExistsException e) {
    responder.sendString(HttpResponseStatus.CONFLICT,String.format(""String_Node_Str"" + ""String_Node_Str"",artifactId));
  }
catch (  WriteConflictException e) {
    LOG.warn(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + ""String_Node_Str"");
  }
catch (  UnauthorizedException e) {
    responder.sendString(HttpResponseStatus.FORBIDDEN,e.getMessage());
  }
catch (  ConflictException e) {
    responder.sendString(HttpResponseStatus.CONFLICT,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
}"
5189,"protected void verifyData(ApplicationId appId,ApplicationSpecification specification,@Nullable KerberosPrincipalId specifiedOwnerPrincipal) throws DatasetManagementException, ConflictException {
  VerifyResult result;
  for (  DatasetCreationSpec dataSetCreateSpec : specification.getDatasets().values()) {
    result=getVerifier(DatasetCreationSpec.class).verify(appId,dataSetCreateSpec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
    String dsName=dataSetCreateSpec.getInstanceName();
    DatasetId datasetInstanceId=appId.getParent().dataset(dsName);
    DatasetSpecification existingSpec=dsFramework.getDatasetSpec(datasetInstanceId);
    if (existingSpec != null && !existingSpec.getType().equals(dataSetCreateSpec.getTypeName())) {
      throw new DataSetException(String.format(""String_Node_Str"",dsName,dataSetCreateSpec.getTypeName()));
    }
    if (existingSpec != null) {
      verifyOwner(datasetInstanceId,specifiedOwnerPrincipal);
    }
  }
  for (  StreamSpecification spec : specification.getStreams().values()) {
    result=getVerifier(StreamSpecification.class).verify(appId,spec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
    if (store.getStream(appId.getNamespaceId(),spec.getName()) != null) {
      verifyOwner(appId.getParent().stream(spec.getName()),specifiedOwnerPrincipal);
    }
  }
}","protected void verifyData(ApplicationId appId,ApplicationSpecification specification,@Nullable KerberosPrincipalId specifiedOwnerPrincipal) throws DatasetManagementException, UnauthorizedException {
  VerifyResult result;
  for (  DatasetCreationSpec dataSetCreateSpec : specification.getDatasets().values()) {
    result=getVerifier(DatasetCreationSpec.class).verify(appId,dataSetCreateSpec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
    String dsName=dataSetCreateSpec.getInstanceName();
    DatasetId datasetInstanceId=appId.getParent().dataset(dsName);
    DatasetSpecification existingSpec=dsFramework.getDatasetSpec(datasetInstanceId);
    if (existingSpec != null && !existingSpec.getType().equals(dataSetCreateSpec.getTypeName())) {
      throw new DataSetException(String.format(""String_Node_Str"",dsName,dataSetCreateSpec.getTypeName()));
    }
    if (existingSpec != null) {
      verifyOwner(datasetInstanceId,specifiedOwnerPrincipal);
    }
  }
  for (  StreamSpecification spec : specification.getStreams().values()) {
    result=getVerifier(StreamSpecification.class).verify(appId,spec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
    if (store.getStream(appId.getNamespaceId(),spec.getName()) != null) {
      verifyOwner(appId.getParent().stream(spec.getName()),specifiedOwnerPrincipal);
    }
  }
}"
5190,"private void verifyOwner(NamespacedEntityId entityId,@Nullable KerberosPrincipalId specifiedOwnerPrincipal) throws DatasetManagementException, ConflictException {
  String ownerPrincipal;
  try {
    ownerPrincipal=ownerAdmin.getImpersonationPrincipal(entityId);
    if (!Objects.equals(ownerPrincipal,specifiedOwnerPrincipal == null ? null : specifiedOwnerPrincipal.getPrincipal())) {
      throw new ConflictException(String.format(""String_Node_Str"" + ""String_Node_Str"",entityId.getEntityType(),entityId.getEntityName(),Constants.Security.PRINCIPAL,specifiedOwnerPrincipal,Constants.Security.PRINCIPAL));
    }
  }
 catch (  IOException e) {
    throw new DatasetManagementException(e.getMessage(),e);
  }
}","private void verifyOwner(NamespacedEntityId entityId,@Nullable KerberosPrincipalId specifiedOwnerPrincipal) throws DatasetManagementException, UnauthorizedException {
  try {
    SecurityUtil.verifyOwnerPrincipal(entityId,specifiedOwnerPrincipal == null ? null : specifiedOwnerPrincipal.getPrincipal(),ownerAdmin);
  }
 catch (  IOException e) {
    throw new DatasetManagementException(e.getMessage(),e);
  }
}"
5191,"/** 
 * Update an existing application. An application's configuration and artifact version can be updated.
 * @param appId the id of the application to update
 * @param appRequest the request to update the application, including new config and artifact
 * @param programTerminator a program terminator that will stop programs that are removed when updating an app.For example, if an update removes a flow, the terminator defines how to stop that flow.
 * @return information about the deployed application
 * @throws ApplicationNotFoundException if the specified application does not exist
 * @throws ArtifactNotFoundException if the requested artifact does not exist
 * @throws InvalidArtifactException if the specified artifact is invalid. For example, if the artifact name changed,if the version is an invalid version, or the artifact contains no app classes
 * @throws Exception if there was an exception during the deployment pipeline. This exception will often wrapthe actual exception
 */
public ApplicationWithPrograms updateApp(ApplicationId appId,AppRequest appRequest,ProgramTerminator programTerminator) throws Exception {
  ApplicationSpecification currentSpec=store.getApplication(appId);
  if (currentSpec == null) {
    throw new ApplicationNotFoundException(appId);
  }
  authorizationEnforcer.enforce(appId,authenticationContext.getPrincipal(),Action.ADMIN);
  ArtifactId currentArtifact=currentSpec.getArtifactId();
  ArtifactId newArtifactId=currentArtifact;
  ArtifactSummary requestedArtifact=appRequest.getArtifact();
  if (requestedArtifact != null) {
    if (!currentArtifact.getName().equals(requestedArtifact.getName())) {
      throw new InvalidArtifactException(String.format(""String_Node_Str"",currentArtifact.getName(),requestedArtifact.getName()));
    }
    if (!currentArtifact.getScope().equals(requestedArtifact.getScope())) {
      throw new InvalidArtifactException(""String_Node_Str"" + ""String_Node_Str"");
    }
    ArtifactVersion requestedVersion=new ArtifactVersion(requestedArtifact.getVersion());
    if (requestedVersion.getVersion() == null) {
      throw new InvalidArtifactException(String.format(""String_Node_Str"",requestedArtifact.getVersion()));
    }
    newArtifactId=new ArtifactId(currentArtifact.getName(),requestedVersion,currentArtifact.getScope());
  }
  if (!Objects.equals(ownerAdmin.getImpersonationPrincipal(appId),appRequest.getOwnerPrincipal())) {
    throw new ConflictException(String.format(""String_Node_Str"" + ""String_Node_Str"",appId.getEntityType(),appId.getApplication(),Constants.Security.PRINCIPAL,appRequest.getOwnerPrincipal(),Constants.Security.PRINCIPAL));
  }
  Object requestedConfigObj=appRequest.getConfig();
  String requestedConfigStr=requestedConfigObj == null ? currentSpec.getConfiguration() : new Gson().toJson(requestedConfigObj);
  Id.Artifact artifactId=Artifacts.toArtifactId(appId.getParent(),newArtifactId).toId();
  return deployApp(appId.getParent(),appId.getApplication(),null,artifactId,requestedConfigStr,programTerminator);
}","/** 
 * Update an existing application. An application's configuration and artifact version can be updated.
 * @param appId the id of the application to update
 * @param appRequest the request to update the application, including new config and artifact
 * @param programTerminator a program terminator that will stop programs that are removed when updating an app.For example, if an update removes a flow, the terminator defines how to stop that flow.
 * @return information about the deployed application
 * @throws ApplicationNotFoundException if the specified application does not exist
 * @throws ArtifactNotFoundException if the requested artifact does not exist
 * @throws InvalidArtifactException if the specified artifact is invalid. For example, if the artifact name changed,if the version is an invalid version, or the artifact contains no app classes
 * @throws Exception if there was an exception during the deployment pipeline. This exception will often wrapthe actual exception
 */
public ApplicationWithPrograms updateApp(ApplicationId appId,AppRequest appRequest,ProgramTerminator programTerminator) throws Exception {
  ApplicationSpecification currentSpec=store.getApplication(appId);
  if (currentSpec == null) {
    throw new ApplicationNotFoundException(appId);
  }
  authorizationEnforcer.enforce(appId,authenticationContext.getPrincipal(),Action.ADMIN);
  ArtifactId currentArtifact=currentSpec.getArtifactId();
  ArtifactId newArtifactId=currentArtifact;
  ArtifactSummary requestedArtifact=appRequest.getArtifact();
  if (requestedArtifact != null) {
    if (!currentArtifact.getName().equals(requestedArtifact.getName())) {
      throw new InvalidArtifactException(String.format(""String_Node_Str"",currentArtifact.getName(),requestedArtifact.getName()));
    }
    if (!currentArtifact.getScope().equals(requestedArtifact.getScope())) {
      throw new InvalidArtifactException(""String_Node_Str"" + ""String_Node_Str"");
    }
    ArtifactVersion requestedVersion=new ArtifactVersion(requestedArtifact.getVersion());
    if (requestedVersion.getVersion() == null) {
      throw new InvalidArtifactException(String.format(""String_Node_Str"",requestedArtifact.getVersion()));
    }
    newArtifactId=new ArtifactId(currentArtifact.getName(),requestedVersion,currentArtifact.getScope());
  }
  SecurityUtil.verifyOwnerPrincipal(appId,appRequest.getOwnerPrincipal(),ownerAdmin);
  Object requestedConfigObj=appRequest.getConfig();
  String requestedConfigStr=requestedConfigObj == null ? currentSpec.getConfiguration() : new Gson().toJson(requestedConfigObj);
  Id.Artifact artifactId=Artifacts.toArtifactId(appId.getParent(),newArtifactId).toId();
  return deployApp(appId.getParent(),appId.getApplication(),null,artifactId,requestedConfigStr,programTerminator);
}"
5192,"private ApplicationWithPrograms deployApp(NamespaceId namespaceId,@Nullable String appName,@Nullable String appVersion,@Nullable String configStr,ProgramTerminator programTerminator,ArtifactDetail artifactDetail,@Nullable KerberosPrincipalId ownerPrincipal) throws Exception {
  authorizationEnforcer.enforce(namespaceId,authenticationContext.getPrincipal(),Action.WRITE);
  ApplicationClass appClass=Iterables.getFirst(artifactDetail.getMeta().getClasses().getApps(),null);
  if (appClass == null) {
    throw new InvalidArtifactException(String.format(""String_Node_Str"",artifactDetail.getDescriptor().getArtifactId(),namespaceId));
  }
  AppDeploymentInfo deploymentInfo=new AppDeploymentInfo(artifactDetail.getDescriptor(),namespaceId,appClass.getClassName(),appName,appVersion,configStr,ownerPrincipal);
  Manager<AppDeploymentInfo,ApplicationWithPrograms> manager=managerFactory.create(programTerminator);
  ApplicationWithPrograms applicationWithPrograms=manager.deploy(deploymentInfo).get();
  privilegesManager.grant(applicationWithPrograms.getApplicationId(),authenticationContext.getPrincipal(),EnumSet.allOf(Action.class));
  return applicationWithPrograms;
}","private ApplicationWithPrograms deployApp(NamespaceId namespaceId,@Nullable String appName,@Nullable String appVersion,@Nullable String configStr,ProgramTerminator programTerminator,ArtifactDetail artifactDetail,@Nullable KerberosPrincipalId ownerPrincipal) throws Exception {
  authorizationEnforcer.enforce(namespaceId,authenticationContext.getPrincipal(),Action.WRITE);
  ApplicationClass appClass=Iterables.getFirst(artifactDetail.getMeta().getClasses().getApps(),null);
  if (appClass == null) {
    throw new InvalidArtifactException(String.format(""String_Node_Str"",artifactDetail.getDescriptor().getArtifactId(),namespaceId));
  }
  AppDeploymentInfo deploymentInfo=new AppDeploymentInfo(artifactDetail.getDescriptor(),namespaceId,appClass.getClassName(),appName,appVersion,configStr,ownerPrincipal);
  Manager<AppDeploymentInfo,ApplicationWithPrograms> manager=managerFactory.create(programTerminator);
  ApplicationWithPrograms applicationWithPrograms;
  try {
    applicationWithPrograms=manager.deploy(deploymentInfo).get();
  }
 catch (  ExecutionException e) {
    Throwables.propagateIfPossible(e.getCause(),Exception.class);
    throw Throwables.propagate(e.getCause());
  }
  privilegesManager.grant(applicationWithPrograms.getApplicationId(),authenticationContext.getPrincipal(),EnumSet.allOf(Action.class));
  return applicationWithPrograms;
}"
5193,"/** 
 * Some tests for owner information storage/propagation during app deployment. More tests at handler level   {@link co.cask.cdap.internal.app.services.http.handlers.AppLifecycleHttpHandlerTest}
 */
@Test public void testOwner() throws Exception {
  String ownerPrincipal=""String_Node_Str"";
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1,ownerPrincipal);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1,""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.BAD_REQUEST.getCode(),response.getStatusLine().getStatusCode());
  response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1,ownerPrincipal);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
}","/** 
 * Some tests for owner information storage/propagation during app deployment. More tests at handler level   {@link co.cask.cdap.internal.app.services.http.handlers.AppLifecycleHttpHandlerTest}
 */
@Test public void testOwner() throws Exception {
  String ownerPrincipal=""String_Node_Str"";
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1,ownerPrincipal);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1,""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.FORBIDDEN.getCode(),response.getStatusLine().getStatusCode());
  response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1,ownerPrincipal);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
}"
5194,"@Test public void testOwnerUsingArtifact() throws Exception {
  ArtifactId artifactId=new ArtifactId(NamespaceId.DEFAULT.getNamespace(),""String_Node_Str"",""String_Node_Str"");
  addAppArtifact(artifactId.toId(),WordCountApp.class);
  ApplicationId applicationId=new ApplicationId(NamespaceId.DEFAULT.getNamespace(),""String_Node_Str"");
  String ownerPrincipal=""String_Node_Str"";
  AppRequest<ConfigTestApp.ConfigClass> appRequest=new AppRequest<>(new ArtifactSummary(artifactId.getArtifact(),artifactId.getVersion()),null,ownerPrincipal);
  Assert.assertEquals(HttpResponseCodes.SC_OK,deploy(applicationId,appRequest).getStatusLine().getStatusCode());
  JsonObject appDetails=getAppDetails(NamespaceId.DEFAULT.getNamespace(),applicationId.getApplication());
  Assert.assertEquals(ownerPrincipal,appDetails.get(Constants.Security.PRINCIPAL).getAsString());
  Assert.assertEquals(ownerPrincipal,getStreamConfig(applicationId.getNamespaceId().stream(""String_Node_Str"")).getOwnerPrincipal());
  Assert.assertEquals(ownerPrincipal,getDatasetMeta(applicationId.getNamespaceId().dataset(""String_Node_Str"")).getOwnerPrincipal());
  String bobPrincipal=""String_Node_Str"";
  appRequest=new AppRequest<>(new ArtifactSummary(artifactId.getArtifact(),artifactId.getVersion()),null,bobPrincipal);
  Assert.assertEquals(HttpResponseCodes.SC_BAD_REQUEST,deploy(applicationId,appRequest).getStatusLine().getStatusCode());
  appRequest=new AppRequest<>(new ArtifactSummary(artifactId.getArtifact(),artifactId.getVersion()),null,bobPrincipal);
  Assert.assertEquals(HttpResponseCodes.SC_BAD_REQUEST,deploy(new ApplicationId(applicationId.getNamespace(),applicationId.getApplication(),""String_Node_Str""),appRequest).getStatusLine().getStatusCode());
  appRequest=new AppRequest<>(new ArtifactSummary(artifactId.getArtifact(),artifactId.getVersion()),null,ownerPrincipal);
  Assert.assertEquals(HttpResponseCodes.SC_OK,deploy(applicationId,appRequest).getStatusLine().getStatusCode());
  appRequest=new AppRequest<>(new ArtifactSummary(artifactId.getArtifact(),artifactId.getVersion()),null,ownerPrincipal);
  Assert.assertEquals(HttpResponseCodes.SC_OK,deploy(new ApplicationId(applicationId.getNamespace(),applicationId.getApplication(),""String_Node_Str""),appRequest).getStatusLine().getStatusCode());
  Assert.assertEquals(200,doDelete(getVersionedAPIPath(""String_Node_Str"" + applicationId.getApplication(),applicationId.getNamespace())).getStatusLine().getStatusCode());
  Assert.assertEquals(ownerPrincipal,getStreamConfig(applicationId.getNamespaceId().stream(""String_Node_Str"")).getOwnerPrincipal());
  Assert.assertEquals(ownerPrincipal,getDatasetMeta(applicationId.getNamespaceId().dataset(""String_Node_Str"")).getOwnerPrincipal());
  deleteNamespace(NamespaceId.DEFAULT.getNamespace());
}","@Test public void testOwnerUsingArtifact() throws Exception {
  ArtifactId artifactId=new ArtifactId(NamespaceId.DEFAULT.getNamespace(),""String_Node_Str"",""String_Node_Str"");
  addAppArtifact(artifactId.toId(),WordCountApp.class);
  ApplicationId applicationId=new ApplicationId(NamespaceId.DEFAULT.getNamespace(),""String_Node_Str"");
  String ownerPrincipal=""String_Node_Str"";
  AppRequest<ConfigTestApp.ConfigClass> appRequest=new AppRequest<>(new ArtifactSummary(artifactId.getArtifact(),artifactId.getVersion()),null,ownerPrincipal);
  Assert.assertEquals(HttpResponseCodes.SC_OK,deploy(applicationId,appRequest).getStatusLine().getStatusCode());
  JsonObject appDetails=getAppDetails(NamespaceId.DEFAULT.getNamespace(),applicationId.getApplication());
  Assert.assertEquals(ownerPrincipal,appDetails.get(Constants.Security.PRINCIPAL).getAsString());
  Assert.assertEquals(ownerPrincipal,getStreamConfig(applicationId.getNamespaceId().stream(""String_Node_Str"")).getOwnerPrincipal());
  Assert.assertEquals(ownerPrincipal,getDatasetMeta(applicationId.getNamespaceId().dataset(""String_Node_Str"")).getOwnerPrincipal());
  String bobPrincipal=""String_Node_Str"";
  appRequest=new AppRequest<>(new ArtifactSummary(artifactId.getArtifact(),artifactId.getVersion()),null,bobPrincipal);
  Assert.assertEquals(HttpResponseCodes.SC_FORBIDDEN,deploy(applicationId,appRequest).getStatusLine().getStatusCode());
  appRequest=new AppRequest<>(new ArtifactSummary(artifactId.getArtifact(),artifactId.getVersion()),null,bobPrincipal);
  Assert.assertEquals(HttpResponseCodes.SC_FORBIDDEN,deploy(new ApplicationId(applicationId.getNamespace(),applicationId.getApplication(),""String_Node_Str""),appRequest).getStatusLine().getStatusCode());
  appRequest=new AppRequest<>(new ArtifactSummary(artifactId.getArtifact(),artifactId.getVersion()),null,ownerPrincipal);
  Assert.assertEquals(HttpResponseCodes.SC_OK,deploy(applicationId,appRequest).getStatusLine().getStatusCode());
  appRequest=new AppRequest<>(new ArtifactSummary(artifactId.getArtifact(),artifactId.getVersion()),null,ownerPrincipal);
  Assert.assertEquals(HttpResponseCodes.SC_OK,deploy(new ApplicationId(applicationId.getNamespace(),applicationId.getApplication(),""String_Node_Str""),appRequest).getStatusLine().getStatusCode());
  Assert.assertEquals(200,doDelete(getVersionedAPIPath(""String_Node_Str"" + applicationId.getApplication(),applicationId.getNamespace())).getStatusLine().getStatusCode());
  Assert.assertEquals(ownerPrincipal,getStreamConfig(applicationId.getNamespaceId().stream(""String_Node_Str"")).getOwnerPrincipal());
  Assert.assertEquals(ownerPrincipal,getDatasetMeta(applicationId.getNamespaceId().dataset(""String_Node_Str"")).getOwnerPrincipal());
  deleteNamespace(NamespaceId.DEFAULT.getNamespace());
}"
5195,"@Test public void testOwner() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  OwnerAdmin ownerAdmin=getOwnerAdmin();
  grantAndAssertSuccess(FOO_NAMESPACE,USER,ImmutableSet.of(Action.WRITE));
  StreamId stream=FOO_NAMESPACE.stream(""String_Node_Str"");
  Properties properties=new Properties();
  String ownerPrincipal=""String_Node_Str"";
  properties.put(Constants.Security.PRINCIPAL,ownerPrincipal);
  streamAdmin.create(stream,properties);
  Assert.assertTrue(streamAdmin.exists(stream));
  Assert.assertTrue(ownerAdmin.exists(stream));
  Assert.assertEquals(ownerPrincipal,streamAdmin.getProperties(stream).getOwnerPrincipal());
  try {
    streamAdmin.updateConfig(stream,new StreamProperties(1L,null,null,null,""String_Node_Str""));
    Assert.fail();
  }
 catch (  ConflictException e) {
  }
  properties.put(Constants.Security.PRINCIPAL,""String_Node_Str"");
  try {
    streamAdmin.create(stream,properties);
    Assert.fail(""String_Node_Str"");
  }
 catch (  ConflictException e) {
  }
  Assert.assertEquals(ownerPrincipal,streamAdmin.getProperties(stream).getOwnerPrincipal());
  streamAdmin.drop(stream);
  Assert.assertFalse(ownerAdmin.exists(stream));
}","@Test public void testOwner() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  OwnerAdmin ownerAdmin=getOwnerAdmin();
  grantAndAssertSuccess(FOO_NAMESPACE,USER,ImmutableSet.of(Action.WRITE));
  StreamId stream=FOO_NAMESPACE.stream(""String_Node_Str"");
  Properties properties=new Properties();
  String ownerPrincipal=""String_Node_Str"";
  properties.put(Constants.Security.PRINCIPAL,ownerPrincipal);
  streamAdmin.create(stream,properties);
  Assert.assertTrue(streamAdmin.exists(stream));
  Assert.assertTrue(ownerAdmin.exists(stream));
  Assert.assertEquals(ownerPrincipal,streamAdmin.getProperties(stream).getOwnerPrincipal());
  try {
    streamAdmin.updateConfig(stream,new StreamProperties(1L,null,null,null,""String_Node_Str""));
    Assert.fail();
  }
 catch (  UnauthorizedException e) {
  }
  properties.put(Constants.Security.PRINCIPAL,""String_Node_Str"");
  try {
    streamAdmin.create(stream,properties);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  Assert.assertEquals(ownerPrincipal,streamAdmin.getProperties(stream).getOwnerPrincipal());
  streamAdmin.drop(stream);
  Assert.assertFalse(ownerAdmin.exists(stream));
}"
5196,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override @AuthEnforce(entities=""String_Node_Str"",enforceOn=InstanceId.class,actions=Action.ADMIN) public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=metadata.getNamespaceId();
  if (exists(namespace)) {
    throw new NamespaceAlreadyExistsException(namespace);
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  boolean hasValidKerberosConf=false;
  if (metadata.getConfig() != null) {
    String configuredPrincipal=metadata.getConfig().getPrincipal();
    String configuredKeytabURI=metadata.getConfig().getKeytabURI();
    if ((!Strings.isNullOrEmpty(configuredPrincipal) && Strings.isNullOrEmpty(configuredKeytabURI)) || (Strings.isNullOrEmpty(configuredPrincipal) && !Strings.isNullOrEmpty(configuredKeytabURI))) {
      throw new BadRequestException(String.format(""String_Node_Str"" + ""String_Node_Str"",configuredPrincipal,configuredKeytabURI));
    }
    hasValidKerberosConf=true;
  }
  if (!metadata.getConfig().isExploreAsPrincipal() && !hasValidKerberosConf) {
    throw new BadRequestException(String.format(""String_Node_Str"",NamespaceConfig.EXPLORE_AS_PRINCIPAL));
  }
  Principal principal=authenticationContext.getPrincipal();
  privilegesManager.grant(namespace,principal,EnumSet.allOf(Action.class));
  String executionUserName;
  if (SecurityUtil.isKerberosEnabled(cConf) && !NamespaceId.SYSTEM.equals(namespace)) {
    String namespacePrincipal=metadata.getConfig().getPrincipal();
    if (Strings.isNullOrEmpty(namespacePrincipal)) {
      executionUserName=SecurityUtil.getMasterPrincipal(cConf);
    }
 else {
      executionUserName=new KerberosName(namespacePrincipal).getShortName();
    }
  }
 else {
    executionUserName=UserGroupInformation.getCurrentUser().getShortUserName();
  }
  Principal executionUser=new Principal(executionUserName,Principal.PrincipalType.USER);
  privilegesManager.grant(namespace,executionUser,EnumSet.allOf(Action.class));
  nsStore.create(metadata);
  try {
    UserGroupInformation ugi;
    if (NamespaceId.DEFAULT.equals(namespace)) {
      ugi=UserGroupInformation.getCurrentUser();
    }
 else {
      ugi=impersonator.getUGI(namespace);
    }
    ImpersonationUtils.doAs(ugi,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.get().create(metadata);
        return null;
      }
    }
);
  }
 catch (  Throwable t) {
    deleteNamespaceMeta(metadata.getNamespaceId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace,t);
  }
  LOG.info(""String_Node_Str"",metadata.getNamespaceId(),metadata);
}","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override @AuthEnforce(entities=""String_Node_Str"",enforceOn=InstanceId.class,actions=Action.ADMIN) public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=metadata.getNamespaceId();
  if (exists(namespace)) {
    throw new NamespaceAlreadyExistsException(namespace);
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  boolean hasValidKerberosConf=false;
  if (metadata.getConfig() != null) {
    String configuredPrincipal=metadata.getConfig().getPrincipal();
    String configuredKeytabURI=metadata.getConfig().getKeytabURI();
    if ((!Strings.isNullOrEmpty(configuredPrincipal) && Strings.isNullOrEmpty(configuredKeytabURI)) || (Strings.isNullOrEmpty(configuredPrincipal) && !Strings.isNullOrEmpty(configuredKeytabURI))) {
      throw new BadRequestException(String.format(""String_Node_Str"" + ""String_Node_Str"",configuredPrincipal,configuredKeytabURI));
    }
    hasValidKerberosConf=true;
  }
  if (!metadata.getConfig().isExploreAsPrincipal() && !hasValidKerberosConf) {
    throw new BadRequestException(String.format(""String_Node_Str"",NamespaceConfig.EXPLORE_AS_PRINCIPAL));
  }
  Principal principal=authenticationContext.getPrincipal();
  privilegesManager.grant(namespace,principal,EnumSet.allOf(Action.class));
  String executionUserName;
  if (SecurityUtil.isKerberosEnabled(cConf) && !NamespaceId.SYSTEM.equals(namespace)) {
    String namespacePrincipal=metadata.getConfig().getPrincipal();
    if (Strings.isNullOrEmpty(namespacePrincipal)) {
      executionUserName=new KerberosName(SecurityUtil.getMasterPrincipal(cConf)).getShortName();
    }
 else {
      executionUserName=new KerberosName(namespacePrincipal).getShortName();
    }
  }
 else {
    executionUserName=UserGroupInformation.getCurrentUser().getShortUserName();
  }
  Principal executionUser=new Principal(executionUserName,Principal.PrincipalType.USER);
  privilegesManager.grant(namespace,executionUser,EnumSet.allOf(Action.class));
  nsStore.create(metadata);
  try {
    UserGroupInformation ugi;
    if (NamespaceId.DEFAULT.equals(namespace)) {
      ugi=UserGroupInformation.getCurrentUser();
    }
 else {
      ugi=impersonator.getUGI(namespace);
    }
    ImpersonationUtils.doAs(ugi,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.get().create(metadata);
        return null;
      }
    }
);
  }
 catch (  Throwable t) {
    deleteNamespaceMeta(metadata.getNamespaceId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace,t);
  }
  LOG.info(""String_Node_Str"",metadata.getNamespaceId(),metadata);
}"
5197,"public DefaultStreamingContext(StageInfo stageInfo,JavaSparkExecutionContext sec,JavaStreamingContext jsc){
  super(sec.getPluginContext(),sec.getMetrics(),stageInfo);
  this.sec=sec;
  this.jsc=jsc;
}","public DefaultStreamingContext(StageInfo stageInfo,JavaSparkExecutionContext sec,JavaStreamingContext jsc){
  super(sec.getPluginContext(),sec.getMetrics(),stageInfo);
  this.sec=sec;
  this.jsc=jsc;
  this.admin=sec.getAdmin();
}"
5198,"@Override protected void startUp() throws Exception {
  File tempDir=createTempDirectory();
  cleanupTask=createCleanupTask(tempDir);
  try {
    Job job=createJob(new File(tempDir,""String_Node_Str""));
    Configuration mapredConf=job.getConfiguration();
    classLoader=new MapReduceClassLoader(injector,cConf,mapredConf,context.getProgram().getClassLoader(),context.getApplicationSpecification().getPlugins(),context.getPluginInstantiator());
    cleanupTask=createCleanupTask(cleanupTask,classLoader);
    mapredConf.setClassLoader(new WeakReferenceDelegatorClassLoader(classLoader));
    ClassLoaders.setContextClassLoader(mapredConf.getClassLoader());
    context.setJob(job);
    beforeSubmit(job);
    Map<String,String> localizedUserResources=localizeUserResources(job,tempDir);
    String jobName=job.getJobName();
    if (!jobName.isEmpty()) {
      LOG.warn(""String_Node_Str"",jobName);
    }
    job.setJobName(getJobName(context));
    Location tempLocation=createTempLocationDirectory();
    cleanupTask=createCleanupTask(cleanupTask,tempLocation);
    if (!MapReduceTaskContextProvider.isLocal(mapredConf)) {
      Location pluginArchive=createPluginArchive(tempLocation);
      if (pluginArchive != null) {
        job.addCacheArchive(pluginArchive.toURI());
        mapredConf.set(Constants.Plugin.ARCHIVE,pluginArchive.getName());
      }
    }
    TaskType.MAP.setResources(mapredConf,context.getMapperResources());
    TaskType.REDUCE.setResources(mapredConf,context.getReducerResources());
    MapperWrapper.wrap(job);
    ReducerWrapper.wrap(job);
    PartitionerWrapper.wrap(job);
    RawComparatorWrapper.CombinerGroupComparatorWrapper.wrap(job);
    RawComparatorWrapper.GroupComparatorWrapper.wrap(job);
    RawComparatorWrapper.KeyComparatorWrapper.wrap(job);
    File jobJar=buildJobJar(job,tempDir);
    job.setJar(jobJar.toURI().toString());
    Location programJar=programJarLocation;
    if (!MapReduceTaskContextProvider.isLocal(mapredConf)) {
      programJar=copyProgramJar(tempLocation);
      job.addCacheFile(programJar.toURI());
      Location launcherJar=createLauncherJar(tempLocation);
      job.addCacheFile(launcherJar.toURI());
      List<String> classpath=new ArrayList<>();
      classpath.add(launcherJar.getName());
      Location logbackLocation=ProgramRunners.createLogbackJar(tempLocation);
      if (logbackLocation != null) {
        job.addCacheFile(logbackLocation.toURI());
        classpath.add(logbackLocation.getName());
        mapredConf.set(""String_Node_Str"",""String_Node_Str"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR);
        mapredConf.set(""String_Node_Str"",""String_Node_Str"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR);
        mapredConf.set(""String_Node_Str"",""String_Node_Str"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR);
      }
      List<String> jarFiles=new ArrayList<>();
      try (JarFile jobJarFile=new JarFile(jobJar)){
        Enumeration<JarEntry> entries=jobJarFile.entries();
        while (entries.hasMoreElements()) {
          JarEntry entry=entries.nextElement();
          if (entry.getName().startsWith(""String_Node_Str"") && entry.getName().endsWith(""String_Node_Str"")) {
            jarFiles.add(""String_Node_Str"" + entry.getName());
          }
        }
      }
       Collections.sort(jarFiles);
      classpath.addAll(jarFiles);
      classpath.add(""String_Node_Str"");
      for (      URI jarURI : CConfigurationUtil.getExtraJars(cConf)) {
        if (""String_Node_Str"".equals(jarURI.getScheme())) {
          Location extraJarLocation=copyFileToLocation(new File(jarURI.getPath()),tempLocation);
          job.addCacheFile(extraJarLocation.toURI());
        }
 else {
          job.addCacheFile(jarURI);
        }
        classpath.add(LocalizationUtils.getLocalizedName(jarURI));
      }
      MapReduceContainerHelper.addMapReduceClassPath(mapredConf,classpath);
      mapredConf.set(MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(classpath));
      mapredConf.set(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(classpath));
    }
    MapReduceContextConfig contextConfig=new MapReduceContextConfig(mapredConf);
    Transaction tx=txClient.startLong();
    try {
      CConfiguration cConfCopy=cConf;
      contextConfig.set(context,cConfCopy,tx,programJar.toURI(),localizedUserResources);
      job.submit();
      LOG.info(""String_Node_Str"",context);
      this.job=job;
      this.transaction=tx;
    }
 catch (    Throwable t) {
      Transactions.invalidateQuietly(txClient,tx);
      throw t;
    }
  }
 catch (  Throwable t) {
    cleanupTask.run();
    if (t instanceof TransactionFailureException && t.getCause() instanceof Exception && !(t instanceof TransactionConflictException)) {
      throw (Exception)t.getCause();
    }
    throw t;
  }
}","@Override protected void startUp() throws Exception {
  File tempDir=createTempDirectory();
  cleanupTask=createCleanupTask(tempDir);
  try {
    Job job=createJob(new File(tempDir,""String_Node_Str""));
    Configuration mapredConf=job.getConfiguration();
    classLoader=new MapReduceClassLoader(injector,cConf,mapredConf,context.getProgram().getClassLoader(),context.getApplicationSpecification().getPlugins(),context.getPluginInstantiator());
    cleanupTask=createCleanupTask(cleanupTask,classLoader);
    mapredConf.setClassLoader(new WeakReferenceDelegatorClassLoader(classLoader));
    ClassLoaders.setContextClassLoader(mapredConf.getClassLoader());
    context.setJob(job);
    beforeSubmit(job);
    Map<String,String> localizedUserResources=localizeUserResources(job,tempDir);
    String jobName=job.getJobName();
    if (!jobName.isEmpty()) {
      LOG.warn(""String_Node_Str"",jobName);
    }
    job.setJobName(getJobName(context));
    Location tempLocation=createTempLocationDirectory();
    cleanupTask=createCleanupTask(cleanupTask,tempLocation);
    if (!MapReduceTaskContextProvider.isLocal(mapredConf)) {
      Location pluginArchive=createPluginArchive(tempLocation);
      if (pluginArchive != null) {
        job.addCacheArchive(pluginArchive.toURI());
        mapredConf.set(Constants.Plugin.ARCHIVE,pluginArchive.getName());
      }
    }
    TaskType.MAP.setResources(mapredConf,context.getMapperResources());
    TaskType.REDUCE.setResources(mapredConf,context.getReducerResources());
    MapperWrapper.wrap(job);
    ReducerWrapper.wrap(job);
    PartitionerWrapper.wrap(job);
    RawComparatorWrapper.CombinerGroupComparatorWrapper.wrap(job);
    RawComparatorWrapper.GroupComparatorWrapper.wrap(job);
    RawComparatorWrapper.KeyComparatorWrapper.wrap(job);
    File jobJar=buildJobJar(job,tempDir);
    job.setJar(jobJar.toURI().toString());
    Location programJar=programJarLocation;
    if (!MapReduceTaskContextProvider.isLocal(mapredConf)) {
      programJar=copyProgramJar(tempLocation);
      job.addCacheFile(programJar.toURI());
      Location launcherJar=createLauncherJar(tempLocation);
      job.addCacheFile(launcherJar.toURI());
      List<String> classpath=new ArrayList<>();
      classpath.add(launcherJar.getName());
      Location logbackLocation=ProgramRunners.createLogbackJar(tempLocation);
      if (logbackLocation != null) {
        job.addCacheFile(logbackLocation.toURI());
        classpath.add(logbackLocation.getName());
        mapredConf.set(""String_Node_Str"",""String_Node_Str"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR);
        mapredConf.set(""String_Node_Str"",""String_Node_Str"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR);
        mapredConf.set(""String_Node_Str"",""String_Node_Str"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR);
      }
      List<String> jarFiles=new ArrayList<>();
      try (JarFile jobJarFile=new JarFile(jobJar)){
        Enumeration<JarEntry> entries=jobJarFile.entries();
        while (entries.hasMoreElements()) {
          JarEntry entry=entries.nextElement();
          if (entry.getName().startsWith(""String_Node_Str"") && entry.getName().endsWith(""String_Node_Str"")) {
            jarFiles.add(""String_Node_Str"" + entry.getName());
          }
        }
      }
       Collections.sort(jarFiles);
      classpath.addAll(jarFiles);
      classpath.add(""String_Node_Str"");
      for (      URI jarURI : CConfigurationUtil.getExtraJars(cConf)) {
        if (""String_Node_Str"".equals(jarURI.getScheme())) {
          Location extraJarLocation=copyFileToLocation(new File(jarURI.getPath()),tempLocation);
          job.addCacheFile(extraJarLocation.toURI());
        }
 else {
          job.addCacheFile(jarURI);
        }
        classpath.add(LocalizationUtils.getLocalizedName(jarURI));
      }
      MapReduceContainerHelper.addMapReduceClassPath(mapredConf,classpath);
      mapredConf.set(MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(classpath));
      mapredConf.set(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(classpath));
    }
    MapReduceContextConfig contextConfig=new MapReduceContextConfig(mapredConf);
    Transaction tx=txClient.startLong();
    try {
      CConfiguration cConfCopy=cConf;
      contextConfig.set(context,cConfCopy,tx,programJar.toURI(),localizedUserResources);
      job.submit();
      LOG.info(""String_Node_Str"",context);
      this.job=job;
      this.transaction=tx;
    }
 catch (    Throwable t) {
      Transactions.invalidateQuietly(txClient,tx);
      throw t;
    }
  }
 catch (  LinkageError e) {
    throw new Exception(e.getMessage(),e);
  }
catch (  Throwable t) {
    cleanupTask.run();
    if (t instanceof TransactionFailureException && t.getCause() instanceof Exception && !(t instanceof TransactionConflictException)) {
      throw (Exception)t.getCause();
    }
    throw t;
  }
}"
5199,"@Override protected void startUp() throws Exception {
  LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
  Class<?> workerClass=program.getClassLoader().loadClass(spec.getClassName());
  @SuppressWarnings(""String_Node_Str"") TypeToken<Worker> workerType=(TypeToken<Worker>)TypeToken.of(workerClass);
  worker=new InstantiatorFactory(false).get(workerType).create();
  Reflections.visit(worker,workerType.getType(),new MetricsFieldSetter(context.getMetrics()),new PropertyFieldSetter(spec.getProperties()));
  LOG.debug(""String_Node_Str"",program.getId());
  TransactionControl txControl=Transactions.getTransactionControl(TransactionControl.EXPLICIT,Worker.class,worker,""String_Node_Str"",WorkerContext.class);
  context.initializeProgram(worker,context,txControl,false);
}","@Override protected void startUp() throws Exception {
  LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
  Class<?> workerClass=program.getClassLoader().loadClass(spec.getClassName());
  @SuppressWarnings(""String_Node_Str"") TypeToken<Worker> workerType=(TypeToken<Worker>)TypeToken.of(workerClass);
  worker=new InstantiatorFactory(false).get(workerType).create();
  Reflections.visit(worker,workerType.getType(),new MetricsFieldSetter(context.getMetrics()),new PropertyFieldSetter(spec.getProperties()));
  LOG.debug(""String_Node_Str"",program.getId());
  TransactionControl txControl=Transactions.getTransactionControl(TransactionControl.EXPLICIT,Worker.class,worker,""String_Node_Str"",WorkerContext.class);
  try {
    context.initializeProgram(worker,context,txControl,false);
  }
 catch (  LinkageError e) {
    throw new Exception(e.getMessage(),e);
  }
}"
5200,"@Override protected void startUp() throws Exception {
  Reflections.visit(spark,spark.getClass(),new PropertyFieldSetter(runtimeContext.getSparkSpecification().getProperties()),new DataSetFieldSetter(runtimeContext.getDatasetCache()),new MetricsFieldSetter(runtimeContext));
  initialize();
  File tempDir=DirUtils.createTempDir(new File(cConf.get(Constants.CFG_LOCAL_DATA_DIR),cConf.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile());
  tempDir.mkdirs();
  this.cleanupTask=createCleanupTask(tempDir,System.getProperties());
  try {
    SparkRuntimeContextConfig contextConfig=new SparkRuntimeContextConfig(runtimeContext.getConfiguration());
    final File jobJar=generateJobJar(tempDir);
    final List<LocalizeResource> localizeResources=new ArrayList<>();
    String metricsConfPath;
    String logbackJarName=null;
    File sparkJar=null;
    List<String> extraJars=new ArrayList<>();
    if (contextConfig.isLocal()) {
      copyUserResources(context.getLocalizeResources(),tempDir);
      File metricsConf=SparkMetricsSink.writeConfig(File.createTempFile(""String_Node_Str"",""String_Node_Str"",tempDir));
      metricsConfPath=metricsConf.getAbsolutePath();
    }
 else {
      distributedUserResources(context.getLocalizeResources(),localizeResources);
      File programJar=Locations.linkOrCopy(runtimeContext.getProgram().getJarLocation(),new File(tempDir,SparkRuntimeContextProvider.PROGRAM_JAR_NAME));
      File expandedProgramJar=Locations.linkOrCopy(runtimeContext.getProgram().getJarLocation(),new File(tempDir,SparkRuntimeContextProvider.PROGRAM_JAR_EXPANDED_NAME));
      localizeResources.add(new LocalizeResource(programJar));
      localizeResources.add(new LocalizeResource(expandedProgramJar,true));
      localizeResources.add(new LocalizeResource(createLauncherJar(tempDir)));
      sparkJar=buildDependencyJar(tempDir);
      localizeResources.add(new LocalizeResource(sparkJar,true));
      localizeResources.add(new LocalizeResource(saveCConf(cConf,tempDir)));
      if (pluginArchive != null) {
        localizeResources.add(new LocalizeResource(pluginArchive,true));
      }
      File logbackJar=ProgramRunners.createLogbackJar(tempDir);
      if (logbackJar != null) {
        localizeResources.add(new LocalizeResource(logbackJar));
        logbackJarName=logbackJar.getName();
      }
      File metricsConf=SparkMetricsSink.writeConfig(File.createTempFile(""String_Node_Str"",""String_Node_Str"",new File(System.getProperty(""String_Node_Str""))));
      metricsConfPath=metricsConf.getName();
      localizeResources.add(new LocalizeResource(metricsConf));
      Configuration hConf=contextConfig.set(runtimeContext,pluginArchive).getConfiguration();
      localizeResources.add(new LocalizeResource(saveHConf(hConf,tempDir)));
      for (      URI jarURI : CConfigurationUtil.getExtraJars(cConf)) {
        extraJars.add(LocalizationUtils.getLocalizedName(jarURI));
        localizeResources.add(new LocalizeResource(jarURI,false));
      }
    }
    final Map<String,String> configs=createSubmitConfigs(sparkJar,tempDir,metricsConfPath,logbackJarName,context.getLocalizeResources(),extraJars,contextConfig.isLocal());
    submitSpark=new Callable<ListenableFuture<RunId>>(){
      @Override public ListenableFuture<RunId> call() throws Exception {
        if (!isRunning()) {
          return immediateCancelledFuture();
        }
        return sparkSubmitter.submit(runtimeContext,configs,localizeResources,jobJar,runtimeContext.getRunId());
      }
    }
;
  }
 catch (  Throwable t) {
    cleanupTask.run();
    throw t;
  }
}","@Override protected void startUp() throws Exception {
  Reflections.visit(spark,spark.getClass(),new PropertyFieldSetter(runtimeContext.getSparkSpecification().getProperties()),new DataSetFieldSetter(runtimeContext.getDatasetCache()),new MetricsFieldSetter(runtimeContext));
  File tempDir=DirUtils.createTempDir(new File(cConf.get(Constants.CFG_LOCAL_DATA_DIR),cConf.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile());
  tempDir.mkdirs();
  this.cleanupTask=createCleanupTask(tempDir,System.getProperties());
  try {
    initialize();
    SparkRuntimeContextConfig contextConfig=new SparkRuntimeContextConfig(runtimeContext.getConfiguration());
    final File jobJar=generateJobJar(tempDir);
    final List<LocalizeResource> localizeResources=new ArrayList<>();
    String metricsConfPath;
    String logbackJarName=null;
    File sparkJar=null;
    List<String> extraJars=new ArrayList<>();
    if (contextConfig.isLocal()) {
      copyUserResources(context.getLocalizeResources(),tempDir);
      File metricsConf=SparkMetricsSink.writeConfig(File.createTempFile(""String_Node_Str"",""String_Node_Str"",tempDir));
      metricsConfPath=metricsConf.getAbsolutePath();
    }
 else {
      distributedUserResources(context.getLocalizeResources(),localizeResources);
      File programJar=Locations.linkOrCopy(runtimeContext.getProgram().getJarLocation(),new File(tempDir,SparkRuntimeContextProvider.PROGRAM_JAR_NAME));
      File expandedProgramJar=Locations.linkOrCopy(runtimeContext.getProgram().getJarLocation(),new File(tempDir,SparkRuntimeContextProvider.PROGRAM_JAR_EXPANDED_NAME));
      localizeResources.add(new LocalizeResource(programJar));
      localizeResources.add(new LocalizeResource(expandedProgramJar,true));
      localizeResources.add(new LocalizeResource(createLauncherJar(tempDir)));
      sparkJar=buildDependencyJar(tempDir);
      localizeResources.add(new LocalizeResource(sparkJar,true));
      localizeResources.add(new LocalizeResource(saveCConf(cConf,tempDir)));
      if (pluginArchive != null) {
        localizeResources.add(new LocalizeResource(pluginArchive,true));
      }
      File logbackJar=ProgramRunners.createLogbackJar(tempDir);
      if (logbackJar != null) {
        localizeResources.add(new LocalizeResource(logbackJar));
        logbackJarName=logbackJar.getName();
      }
      File metricsConf=SparkMetricsSink.writeConfig(File.createTempFile(""String_Node_Str"",""String_Node_Str"",new File(System.getProperty(""String_Node_Str""))));
      metricsConfPath=metricsConf.getName();
      localizeResources.add(new LocalizeResource(metricsConf));
      Configuration hConf=contextConfig.set(runtimeContext,pluginArchive).getConfiguration();
      localizeResources.add(new LocalizeResource(saveHConf(hConf,tempDir)));
      for (      URI jarURI : CConfigurationUtil.getExtraJars(cConf)) {
        extraJars.add(LocalizationUtils.getLocalizedName(jarURI));
        localizeResources.add(new LocalizeResource(jarURI,false));
      }
    }
    final Map<String,String> configs=createSubmitConfigs(sparkJar,tempDir,metricsConfPath,logbackJarName,context.getLocalizeResources(),extraJars,contextConfig.isLocal());
    submitSpark=new Callable<ListenableFuture<RunId>>(){
      @Override public ListenableFuture<RunId> call() throws Exception {
        if (!isRunning()) {
          return immediateCancelledFuture();
        }
        return sparkSubmitter.submit(runtimeContext,configs,localizeResources,jobJar,runtimeContext.getRunId());
      }
    }
;
  }
 catch (  LinkageError e) {
    throw new Exception(e.getMessage(),e);
  }
catch (  Throwable t) {
    cleanupTask.run();
    throw t;
  }
}"
5201,"private void doGetLogs(HttpResponder responder,LoggingContext loggingContext,long fromTimeSecsParam,long toTimeSecsParam,boolean escape,String filterStr,@Nullable RunRecordMeta runRecord,String format,List<String> fieldsToSuppress){
  try {
    TimeRange timeRange=parseTime(fromTimeSecsParam,toTimeSecsParam,responder);
    if (timeRange == null) {
      return;
    }
    Filter filter=FilterParser.parse(filterStr);
    ReadRange readRange=new ReadRange(timeRange.getFromMillis(),timeRange.getToMillis(),LogOffset.INVALID_KAFKA_OFFSET);
    readRange=adjustReadRange(readRange,runRecord,fromTimeSecsParam != -1);
    try {
      CloseableIterator<LogEvent> logIter=logReader.getLog(loggingContext,readRange.getFromMillis(),readRange.getToMillis(),filter);
      AbstractChunkedLogProducer logsProducer=getFullLogsProducer(format,logIter,fieldsToSuppress,escape);
      responder.sendContent(HttpResponseStatus.OK,logsProducer,logsProducer.getResponseHeaders());
    }
 catch (    Exception ex) {
      LOG.debug(""String_Node_Str"",loggingContext,ex);
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  IllegalArgumentException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
}","private void doGetLogs(HttpResponder responder,LoggingContext loggingContext,long fromTimeSecsParam,long toTimeSecsParam,boolean escape,String filterStr,@Nullable RunRecordMeta runRecord,String format,List<String> fieldsToSuppress){
  try {
    TimeRange timeRange=parseTime(fromTimeSecsParam,toTimeSecsParam,responder);
    if (timeRange == null) {
      return;
    }
    Filter filter=FilterParser.parse(filterStr);
    ReadRange readRange=new ReadRange(timeRange.getFromMillis(),timeRange.getToMillis(),LogOffset.INVALID_KAFKA_OFFSET);
    readRange=adjustReadRange(readRange,runRecord,fromTimeSecsParam != -1);
    AbstractChunkedLogProducer logsProducer=null;
    try {
      CloseableIterator<LogEvent> logIter=logReader.getLog(loggingContext,readRange.getFromMillis(),readRange.getToMillis(),filter);
      logsProducer=getFullLogsProducer(format,logIter,fieldsToSuppress,escape);
    }
 catch (    Exception ex) {
      LOG.debug(""String_Node_Str"",loggingContext,ex);
      if (logsProducer != null) {
        logsProducer.close();
      }
      responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
      return;
    }
    responder.sendContent(HttpResponseStatus.OK,logsProducer,logsProducer.getResponseHeaders());
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  IllegalArgumentException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
}"
5202,"private void verifyOwner(NamespacedEntityId entityId,@Nullable KerberosPrincipalId specifiedOwnerPrincipal) throws DatasetManagementException, ConflictException {
  String ownerPrincipal;
  try {
    ownerPrincipal=ownerAdmin.getImpersonationPrincipal(entityId);
    if (!Objects.equals(ownerPrincipal,specifiedOwnerPrincipal == null ? null : specifiedOwnerPrincipal.getPrincipal())) {
      throw new ConflictException(String.format(""String_Node_Str"" + ""String_Node_Str"",Constants.Security.PRINCIPAL,specifiedOwnerPrincipal,Constants.Security.PRINCIPAL,entityId.getEntityType()));
    }
  }
 catch (  IOException e) {
    throw new DatasetManagementException(e.getMessage(),e);
  }
}","private void verifyOwner(NamespacedEntityId entityId,@Nullable KerberosPrincipalId specifiedOwnerPrincipal) throws DatasetManagementException, ConflictException {
  String ownerPrincipal;
  try {
    ownerPrincipal=ownerAdmin.getImpersonationPrincipal(entityId);
    if (!Objects.equals(ownerPrincipal,specifiedOwnerPrincipal == null ? null : specifiedOwnerPrincipal.getPrincipal())) {
      throw new ConflictException(String.format(""String_Node_Str"" + ""String_Node_Str"",entityId.getEntityType(),entityId.getEntityName(),Constants.Security.PRINCIPAL,specifiedOwnerPrincipal,Constants.Security.PRINCIPAL));
    }
  }
 catch (  IOException e) {
    throw new DatasetManagementException(e.getMessage(),e);
  }
}"
5203,"/** 
 * Update an existing application. An application's configuration and artifact version can be updated.
 * @param appId the id of the application to update
 * @param appRequest the request to update the application, including new config and artifact
 * @param programTerminator a program terminator that will stop programs that are removed when updating an app.For example, if an update removes a flow, the terminator defines how to stop that flow.
 * @return information about the deployed application
 * @throws ApplicationNotFoundException if the specified application does not exist
 * @throws ArtifactNotFoundException if the requested artifact does not exist
 * @throws InvalidArtifactException if the specified artifact is invalid. For example, if the artifact name changed,if the version is an invalid version, or the artifact contains no app classes
 * @throws Exception if there was an exception during the deployment pipeline. This exception will often wrapthe actual exception
 */
public ApplicationWithPrograms updateApp(ApplicationId appId,AppRequest appRequest,ProgramTerminator programTerminator) throws Exception {
  ApplicationSpecification currentSpec=store.getApplication(appId);
  if (currentSpec == null) {
    throw new ApplicationNotFoundException(appId);
  }
  authorizationEnforcer.enforce(appId,authenticationContext.getPrincipal(),Action.ADMIN);
  ArtifactId currentArtifact=currentSpec.getArtifactId();
  ArtifactId newArtifactId=currentArtifact;
  ArtifactSummary requestedArtifact=appRequest.getArtifact();
  if (requestedArtifact != null) {
    if (!currentArtifact.getName().equals(requestedArtifact.getName())) {
      throw new InvalidArtifactException(String.format(""String_Node_Str"",currentArtifact.getName(),requestedArtifact.getName()));
    }
    if (!currentArtifact.getScope().equals(requestedArtifact.getScope())) {
      throw new InvalidArtifactException(""String_Node_Str"" + ""String_Node_Str"");
    }
    ArtifactVersion requestedVersion=new ArtifactVersion(requestedArtifact.getVersion());
    if (requestedVersion.getVersion() == null) {
      throw new InvalidArtifactException(String.format(""String_Node_Str"",requestedArtifact.getVersion()));
    }
    newArtifactId=new ArtifactId(currentArtifact.getName(),requestedVersion,currentArtifact.getScope());
  }
  if (!Objects.equals(ownerAdmin.getImpersonationPrincipal(appId),appRequest.getOwnerPrincipal())) {
    throw new ConflictException(String.format(""String_Node_Str"" + ""String_Node_Str"",Constants.Security.PRINCIPAL,appRequest.getOwnerPrincipal(),Constants.Security.PRINCIPAL,appId.getEntityType()));
  }
  Object requestedConfigObj=appRequest.getConfig();
  String requestedConfigStr=requestedConfigObj == null ? currentSpec.getConfiguration() : new Gson().toJson(requestedConfigObj);
  Id.Artifact artifactId=Artifacts.toArtifactId(appId.getParent(),newArtifactId).toId();
  return deployApp(appId.getParent(),appId.getApplication(),null,artifactId,requestedConfigStr,programTerminator);
}","/** 
 * Update an existing application. An application's configuration and artifact version can be updated.
 * @param appId the id of the application to update
 * @param appRequest the request to update the application, including new config and artifact
 * @param programTerminator a program terminator that will stop programs that are removed when updating an app.For example, if an update removes a flow, the terminator defines how to stop that flow.
 * @return information about the deployed application
 * @throws ApplicationNotFoundException if the specified application does not exist
 * @throws ArtifactNotFoundException if the requested artifact does not exist
 * @throws InvalidArtifactException if the specified artifact is invalid. For example, if the artifact name changed,if the version is an invalid version, or the artifact contains no app classes
 * @throws Exception if there was an exception during the deployment pipeline. This exception will often wrapthe actual exception
 */
public ApplicationWithPrograms updateApp(ApplicationId appId,AppRequest appRequest,ProgramTerminator programTerminator) throws Exception {
  ApplicationSpecification currentSpec=store.getApplication(appId);
  if (currentSpec == null) {
    throw new ApplicationNotFoundException(appId);
  }
  authorizationEnforcer.enforce(appId,authenticationContext.getPrincipal(),Action.ADMIN);
  ArtifactId currentArtifact=currentSpec.getArtifactId();
  ArtifactId newArtifactId=currentArtifact;
  ArtifactSummary requestedArtifact=appRequest.getArtifact();
  if (requestedArtifact != null) {
    if (!currentArtifact.getName().equals(requestedArtifact.getName())) {
      throw new InvalidArtifactException(String.format(""String_Node_Str"",currentArtifact.getName(),requestedArtifact.getName()));
    }
    if (!currentArtifact.getScope().equals(requestedArtifact.getScope())) {
      throw new InvalidArtifactException(""String_Node_Str"" + ""String_Node_Str"");
    }
    ArtifactVersion requestedVersion=new ArtifactVersion(requestedArtifact.getVersion());
    if (requestedVersion.getVersion() == null) {
      throw new InvalidArtifactException(String.format(""String_Node_Str"",requestedArtifact.getVersion()));
    }
    newArtifactId=new ArtifactId(currentArtifact.getName(),requestedVersion,currentArtifact.getScope());
  }
  if (!Objects.equals(ownerAdmin.getImpersonationPrincipal(appId),appRequest.getOwnerPrincipal())) {
    throw new ConflictException(String.format(""String_Node_Str"" + ""String_Node_Str"",appId.getEntityType(),appId.getApplication(),Constants.Security.PRINCIPAL,appRequest.getOwnerPrincipal(),Constants.Security.PRINCIPAL));
  }
  Object requestedConfigObj=appRequest.getConfig();
  String requestedConfigStr=requestedConfigObj == null ? currentSpec.getConfiguration() : new Gson().toJson(requestedConfigObj);
  Id.Artifact artifactId=Artifacts.toArtifactId(appId.getParent(),newArtifactId).toId();
  return deployApp(appId.getParent(),appId.getApplication(),null,artifactId,requestedConfigStr,programTerminator);
}"
5204,"private void verifyOwner(StreamId streamId,@Nullable String specifiedOwnerPrincipal) throws IOException, ConflictException {
  if (!Objects.equals(specifiedOwnerPrincipal,ownerAdmin.getImpersonationPrincipal(streamId))) {
    throw new ConflictException(String.format(""String_Node_Str"" + ""String_Node_Str"",Constants.Security.PRINCIPAL,specifiedOwnerPrincipal,Constants.Security.PRINCIPAL,streamId.getEntityType()));
  }
}","private void verifyOwner(StreamId streamId,@Nullable String specifiedOwnerPrincipal) throws IOException, ConflictException {
  if (!Objects.equals(specifiedOwnerPrincipal,ownerAdmin.getImpersonationPrincipal(streamId))) {
    throw new ConflictException(String.format(""String_Node_Str"" + ""String_Node_Str"",streamId.getEntityType(),streamId.getStream(),Constants.Security.PRINCIPAL,specifiedOwnerPrincipal,Constants.Security.PRINCIPAL));
  }
}"
5205,"/** 
 * Collects stats from all   {@link OperationalStats}.
 */
private void collectOperationalStats() throws InterruptedException {
  LOG.debug(""String_Node_Str"");
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    if (!isRunning()) {
      return;
    }
    OperationalStats stats=entry.getValue();
    LOG.debug(""String_Node_Str"",stats.getServiceName(),stats.getStatType());
    try {
      stats.collect();
    }
 catch (    Throwable t) {
      Throwables.propagateIfInstanceOf(t,InterruptedException.class);
      Throwable rootCause=Throwables.getRootCause(t);
      if (rootCause instanceof ServiceUnavailableException || rootCause instanceof TException) {
        return;
      }
      if (rootCause instanceof InterruptedException) {
        throw (InterruptedException)rootCause;
      }
      LOG.warn(""String_Node_Str"",stats.getServiceName(),stats.getStatType(),rootCause.getMessage());
    }
  }
}","/** 
 * Collects stats from all   {@link OperationalStats}.
 */
private void collectOperationalStats() throws InterruptedException {
  LOG.trace(""String_Node_Str"");
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    if (!isRunning()) {
      return;
    }
    OperationalStats stats=entry.getValue();
    LOG.trace(""String_Node_Str"",stats.getServiceName(),stats.getStatType());
    try {
      stats.collect();
    }
 catch (    Throwable t) {
      Throwables.propagateIfInstanceOf(t,InterruptedException.class);
      Throwable rootCause=Throwables.getRootCause(t);
      if (rootCause instanceof ServiceUnavailableException || rootCause instanceof TException) {
        return;
      }
      if (rootCause instanceof InterruptedException) {
        throw (InterruptedException)rootCause;
      }
      LOG.warn(""String_Node_Str"",stats.getServiceName(),stats.getStatType(),rootCause.getMessage());
    }
  }
}"
5206,"@Override protected void shutDown() throws Exception {
  MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    OperationalStats operationalStats=entry.getValue();
    ObjectName objectName=getObjectName(operationalStats);
    if (objectName == null) {
      LOG.warn(""String_Node_Str"" + ""String_Node_Str"",operationalStats.getClass().getName());
      continue;
    }
    try {
      mbs.unregisterMBean(objectName);
    }
 catch (    InstanceNotFoundException e) {
      LOG.debug(""String_Node_Str"",objectName);
    }
catch (    MBeanRegistrationException e) {
      LOG.warn(""String_Node_Str"",e);
    }
    operationalStats.destroy();
  }
  LOG.info(""String_Node_Str"");
}","@Override protected void shutDown() throws Exception {
  MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    OperationalStats operationalStats=entry.getValue();
    ObjectName objectName=getObjectName(operationalStats);
    if (objectName == null) {
      LOG.warn(""String_Node_Str"" + ""String_Node_Str"",operationalStats.getClass().getName());
      continue;
    }
    try {
      mbs.unregisterMBean(objectName);
    }
 catch (    InstanceNotFoundException e) {
      LOG.warn(""String_Node_Str"",objectName);
    }
catch (    MBeanRegistrationException e) {
      LOG.warn(""String_Node_Str"",e);
    }
    operationalStats.destroy();
  }
  LOG.info(""String_Node_Str"");
}"
5207,"private BodyConsumer deployApplication(final HttpResponder responder,final NamespaceId namespace,final String appId,final String archiveName,final String configString,@Nullable final String ownerPrincipal) throws IOException {
  Id.Namespace idNamespace=namespace.toId();
  Location namespaceHomeLocation=namespacedLocationFactory.get(idNamespace);
  if (!namespaceHomeLocation.exists()) {
    String msg=String.format(""String_Node_Str"",namespaceHomeLocation,namespace.getNamespace());
    LOG.error(msg);
    responder.sendString(HttpResponseStatus.NOT_FOUND,msg);
    return null;
  }
  if (archiveName == null || archiveName.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,String.format(""String_Node_Str"",ARCHIVE_NAME_HEADER),ImmutableMultimap.of(HttpHeaders.Names.CONNECTION,HttpHeaders.Values.CLOSE));
    return null;
  }
  final Id.Artifact artifactId;
  try {
    artifactId=Id.Artifact.parse(idNamespace,archiveName);
  }
 catch (  IllegalArgumentException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    return null;
  }
  KerberosPrincipalId ownerPrincipalId=ownerPrincipal == null ? null : new KerberosPrincipalId(ownerPrincipal);
  String namespacesDir=configuration.get(Constants.Namespace.NAMESPACES_DIR);
  File localDataDir=new File(configuration.get(Constants.CFG_LOCAL_DATA_DIR));
  File namespaceBase=new File(localDataDir,namespacesDir);
  File tempDir=new File(new File(namespaceBase,namespace.getNamespace()),configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  if (!DirUtils.mkdirs(tempDir)) {
    throw new IOException(""String_Node_Str"" + tempDir);
  }
  final KerberosPrincipalId finalOwnerPrincipalId=ownerPrincipalId;
  return new AbstractBodyConsumer(File.createTempFile(""String_Node_Str"",""String_Node_Str"",tempDir)){
    @Override protected void onFinish(    HttpResponder responder,    File uploadedFile){
      try {
        ApplicationWithPrograms app=applicationLifecycleService.deployAppAndArtifact(namespace,appId,artifactId,uploadedFile,configString,finalOwnerPrincipalId,createProgramTerminator());
        responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",app.getApplicationId().getApplication()));
      }
 catch (      InvalidArtifactException e) {
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
catch (      ArtifactAlreadyExistsException e) {
        responder.sendString(HttpResponseStatus.CONFLICT,String.format(""String_Node_Str"" + ""String_Node_Str"",artifactId));
      }
catch (      WriteConflictException e) {
        LOG.warn(""String_Node_Str"",artifactId,e);
        responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + ""String_Node_Str"");
      }
catch (      UnauthorizedException e) {
        responder.sendString(HttpResponseStatus.FORBIDDEN,e.getMessage());
      }
catch (      Exception e) {
        LOG.error(""String_Node_Str"",e);
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
    }
  }
;
}","private BodyConsumer deployApplication(final HttpResponder responder,final NamespaceId namespace,final String appId,final String archiveName,final String configString,@Nullable final String ownerPrincipal) throws IOException {
  Id.Namespace idNamespace=namespace.toId();
  Location namespaceHomeLocation=namespacedLocationFactory.get(idNamespace);
  if (!namespaceHomeLocation.exists()) {
    String msg=String.format(""String_Node_Str"",namespaceHomeLocation,namespace.getNamespace());
    LOG.error(msg);
    responder.sendString(HttpResponseStatus.NOT_FOUND,msg);
    return null;
  }
  if (archiveName == null || archiveName.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,String.format(""String_Node_Str"",ARCHIVE_NAME_HEADER),ImmutableMultimap.of(HttpHeaders.Names.CONNECTION,HttpHeaders.Values.CLOSE));
    return null;
  }
  final Id.Artifact artifactId;
  try {
    artifactId=Id.Artifact.parse(idNamespace,archiveName);
  }
 catch (  IllegalArgumentException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    return null;
  }
  KerberosPrincipalId ownerPrincipalId=ownerPrincipal == null ? null : new KerberosPrincipalId(ownerPrincipal);
  String namespacesDir=configuration.get(Constants.Namespace.NAMESPACES_DIR);
  File localDataDir=new File(configuration.get(Constants.CFG_LOCAL_DATA_DIR));
  File namespaceBase=new File(localDataDir,namespacesDir);
  File tempDir=new File(new File(namespaceBase,namespace.getNamespace()),configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  if (!DirUtils.mkdirs(tempDir)) {
    throw new IOException(""String_Node_Str"" + tempDir);
  }
  final KerberosPrincipalId finalOwnerPrincipalId=ownerPrincipalId;
  return new AbstractBodyConsumer(File.createTempFile(""String_Node_Str"",""String_Node_Str"",tempDir)){
    @Override protected void onFinish(    HttpResponder responder,    File uploadedFile){
      try {
        ApplicationWithPrograms app=applicationLifecycleService.deployAppAndArtifact(namespace,appId,artifactId,uploadedFile,configString,finalOwnerPrincipalId,createProgramTerminator());
        LOG.info(""String_Node_Str"" + ""String_Node_Str"",app.getApplicationId().getApplication(),namespace.getNamespace(),artifactId,configString,finalOwnerPrincipalId);
        responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",app.getApplicationId().getApplication()));
      }
 catch (      InvalidArtifactException e) {
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
catch (      ArtifactAlreadyExistsException e) {
        responder.sendString(HttpResponseStatus.CONFLICT,String.format(""String_Node_Str"" + ""String_Node_Str"",artifactId));
      }
catch (      WriteConflictException e) {
        LOG.warn(""String_Node_Str"",artifactId,e);
        responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + ""String_Node_Str"");
      }
catch (      UnauthorizedException e) {
        responder.sendString(HttpResponseStatus.FORBIDDEN,e.getMessage());
      }
catch (      Exception e) {
        LOG.error(""String_Node_Str"",e);
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
    }
  }
;
}"
5208,"@Override protected void onFinish(HttpResponder responder,File uploadedFile){
  try {
    ApplicationWithPrograms app=applicationLifecycleService.deployAppAndArtifact(namespace,appId,artifactId,uploadedFile,configString,finalOwnerPrincipalId,createProgramTerminator());
    responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",app.getApplicationId().getApplication()));
  }
 catch (  InvalidArtifactException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
catch (  ArtifactAlreadyExistsException e) {
    responder.sendString(HttpResponseStatus.CONFLICT,String.format(""String_Node_Str"" + ""String_Node_Str"",artifactId));
  }
catch (  WriteConflictException e) {
    LOG.warn(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + ""String_Node_Str"");
  }
catch (  UnauthorizedException e) {
    responder.sendString(HttpResponseStatus.FORBIDDEN,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
}","@Override protected void onFinish(HttpResponder responder,File uploadedFile){
  try {
    ApplicationWithPrograms app=applicationLifecycleService.deployAppAndArtifact(namespace,appId,artifactId,uploadedFile,configString,finalOwnerPrincipalId,createProgramTerminator());
    LOG.info(""String_Node_Str"" + ""String_Node_Str"",app.getApplicationId().getApplication(),namespace.getNamespace(),artifactId,configString,finalOwnerPrincipalId);
    responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",app.getApplicationId().getApplication()));
  }
 catch (  InvalidArtifactException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
catch (  ArtifactAlreadyExistsException e) {
    responder.sendString(HttpResponseStatus.CONFLICT,String.format(""String_Node_Str"" + ""String_Node_Str"",artifactId));
  }
catch (  WriteConflictException e) {
    LOG.warn(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + ""String_Node_Str"");
  }
catch (  UnauthorizedException e) {
    responder.sendString(HttpResponseStatus.FORBIDDEN,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
}"
5209,"protected void verifyData(ApplicationId appId,ApplicationSpecification specification,@Nullable KerberosPrincipalId specifiedOwnerPrincipal) throws DatasetManagementException {
  VerifyResult result;
  for (  DatasetCreationSpec dataSetCreateSpec : specification.getDatasets().values()) {
    result=getVerifier(DatasetCreationSpec.class).verify(appId,dataSetCreateSpec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
    String dsName=dataSetCreateSpec.getInstanceName();
    DatasetId datasetInstanceId=appId.getParent().dataset(dsName);
    DatasetSpecification existingSpec=dsFramework.getDatasetSpec(datasetInstanceId);
    if (existingSpec != null && !existingSpec.getType().equals(dataSetCreateSpec.getTypeName())) {
      throw new DataSetException(String.format(""String_Node_Str"",dsName,dataSetCreateSpec.getTypeName()));
    }
    if (existingSpec != null) {
      verifyOwner(datasetInstanceId,specifiedOwnerPrincipal);
    }
  }
  for (  StreamSpecification spec : specification.getStreams().values()) {
    result=getVerifier(StreamSpecification.class).verify(appId,spec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
    if (store.getStream(appId.getNamespaceId(),spec.getName()) != null) {
      verifyOwner(appId.getParent().stream(spec.getName()),specifiedOwnerPrincipal);
    }
  }
}","protected void verifyData(ApplicationId appId,ApplicationSpecification specification,@Nullable KerberosPrincipalId specifiedOwnerPrincipal) throws DatasetManagementException, ConflictException {
  VerifyResult result;
  for (  DatasetCreationSpec dataSetCreateSpec : specification.getDatasets().values()) {
    result=getVerifier(DatasetCreationSpec.class).verify(appId,dataSetCreateSpec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
    String dsName=dataSetCreateSpec.getInstanceName();
    DatasetId datasetInstanceId=appId.getParent().dataset(dsName);
    DatasetSpecification existingSpec=dsFramework.getDatasetSpec(datasetInstanceId);
    if (existingSpec != null && !existingSpec.getType().equals(dataSetCreateSpec.getTypeName())) {
      throw new DataSetException(String.format(""String_Node_Str"",dsName,dataSetCreateSpec.getTypeName()));
    }
    if (existingSpec != null) {
      verifyOwner(datasetInstanceId,specifiedOwnerPrincipal);
    }
  }
  for (  StreamSpecification spec : specification.getStreams().values()) {
    result=getVerifier(StreamSpecification.class).verify(appId,spec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
    if (store.getStream(appId.getNamespaceId(),spec.getName()) != null) {
      verifyOwner(appId.getParent().stream(spec.getName()),specifiedOwnerPrincipal);
    }
  }
}"
5210,"private void verifyOwner(NamespacedEntityId entityId,@Nullable KerberosPrincipalId specifiedOwnerPrincipal) throws DatasetManagementException {
  KerberosPrincipalId existingOwnerPrincipal;
  try {
    existingOwnerPrincipal=ownerAdmin.getOwner(entityId);
    boolean equals=Objects.equals(existingOwnerPrincipal,specifiedOwnerPrincipal);
    Preconditions.checkArgument(equals,String.format(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",entityId,Constants.Security.PRINCIPAL,specifiedOwnerPrincipal,Constants.Security.PRINCIPAL));
  }
 catch (  IOException e) {
    throw new DatasetManagementException(e.getMessage(),e);
  }
}","private void verifyOwner(NamespacedEntityId entityId,@Nullable KerberosPrincipalId specifiedOwnerPrincipal) throws DatasetManagementException, ConflictException {
  String ownerPrincipal;
  try {
    ownerPrincipal=ownerAdmin.getImpersonationPrincipal(entityId);
    if (!Objects.equals(ownerPrincipal,specifiedOwnerPrincipal == null ? null : specifiedOwnerPrincipal.getPrincipal())) {
      throw new ConflictException(String.format(""String_Node_Str"" + ""String_Node_Str"",Constants.Security.PRINCIPAL,specifiedOwnerPrincipal,Constants.Security.PRINCIPAL,entityId.getEntityType()));
    }
  }
 catch (  IOException e) {
    throw new DatasetManagementException(e.getMessage(),e);
  }
}"
5211,"/** 
 * Update an existing application. An application's configuration and artifact version can be updated.
 * @param appId the id of the application to update
 * @param appRequest the request to update the application, including new config and artifact
 * @param programTerminator a program terminator that will stop programs that are removed when updating an app.For example, if an update removes a flow, the terminator defines how to stop that flow.
 * @return information about the deployed application
 * @throws ApplicationNotFoundException if the specified application does not exist
 * @throws ArtifactNotFoundException if the requested artifact does not exist
 * @throws InvalidArtifactException if the specified artifact is invalid. For example, if the artifact name changed,if the version is an invalid version, or the artifact contains no app classes
 * @throws Exception if there was an exception during the deployment pipeline. This exception will often wrapthe actual exception
 */
public ApplicationWithPrograms updateApp(ApplicationId appId,AppRequest appRequest,ProgramTerminator programTerminator) throws Exception {
  ApplicationSpecification currentSpec=store.getApplication(appId);
  if (currentSpec == null) {
    throw new ApplicationNotFoundException(appId);
  }
  authorizationEnforcer.enforce(appId,authenticationContext.getPrincipal(),Action.ADMIN);
  ArtifactId currentArtifact=currentSpec.getArtifactId();
  ArtifactId newArtifactId=currentArtifact;
  ArtifactSummary requestedArtifact=appRequest.getArtifact();
  if (requestedArtifact != null) {
    if (!currentArtifact.getName().equals(requestedArtifact.getName())) {
      throw new InvalidArtifactException(String.format(""String_Node_Str"",currentArtifact.getName(),requestedArtifact.getName()));
    }
    if (!currentArtifact.getScope().equals(requestedArtifact.getScope())) {
      throw new InvalidArtifactException(""String_Node_Str"" + ""String_Node_Str"");
    }
    ArtifactVersion requestedVersion=new ArtifactVersion(requestedArtifact.getVersion());
    if (requestedVersion.getVersion() == null) {
      throw new InvalidArtifactException(String.format(""String_Node_Str"",requestedArtifact.getVersion()));
    }
    newArtifactId=new ArtifactId(currentArtifact.getName(),requestedVersion,currentArtifact.getScope());
  }
  boolean equals=Objects.equals(ownerAdmin.getOwnerPrincipal(appId),appRequest.getOwnerPrincipal());
  Preconditions.checkArgument(equals,String.format(""String_Node_Str"",Constants.Security.PRINCIPAL));
  Object requestedConfigObj=appRequest.getConfig();
  String requestedConfigStr=requestedConfigObj == null ? currentSpec.getConfiguration() : new Gson().toJson(requestedConfigObj);
  Id.Artifact artifactId=Artifacts.toArtifactId(appId.getParent(),newArtifactId).toId();
  return deployApp(appId.getParent(),appId.getApplication(),null,artifactId,requestedConfigStr,programTerminator);
}","/** 
 * Update an existing application. An application's configuration and artifact version can be updated.
 * @param appId the id of the application to update
 * @param appRequest the request to update the application, including new config and artifact
 * @param programTerminator a program terminator that will stop programs that are removed when updating an app.For example, if an update removes a flow, the terminator defines how to stop that flow.
 * @return information about the deployed application
 * @throws ApplicationNotFoundException if the specified application does not exist
 * @throws ArtifactNotFoundException if the requested artifact does not exist
 * @throws InvalidArtifactException if the specified artifact is invalid. For example, if the artifact name changed,if the version is an invalid version, or the artifact contains no app classes
 * @throws Exception if there was an exception during the deployment pipeline. This exception will often wrapthe actual exception
 */
public ApplicationWithPrograms updateApp(ApplicationId appId,AppRequest appRequest,ProgramTerminator programTerminator) throws Exception {
  ApplicationSpecification currentSpec=store.getApplication(appId);
  if (currentSpec == null) {
    throw new ApplicationNotFoundException(appId);
  }
  authorizationEnforcer.enforce(appId,authenticationContext.getPrincipal(),Action.ADMIN);
  ArtifactId currentArtifact=currentSpec.getArtifactId();
  ArtifactId newArtifactId=currentArtifact;
  ArtifactSummary requestedArtifact=appRequest.getArtifact();
  if (requestedArtifact != null) {
    if (!currentArtifact.getName().equals(requestedArtifact.getName())) {
      throw new InvalidArtifactException(String.format(""String_Node_Str"",currentArtifact.getName(),requestedArtifact.getName()));
    }
    if (!currentArtifact.getScope().equals(requestedArtifact.getScope())) {
      throw new InvalidArtifactException(""String_Node_Str"" + ""String_Node_Str"");
    }
    ArtifactVersion requestedVersion=new ArtifactVersion(requestedArtifact.getVersion());
    if (requestedVersion.getVersion() == null) {
      throw new InvalidArtifactException(String.format(""String_Node_Str"",requestedArtifact.getVersion()));
    }
    newArtifactId=new ArtifactId(currentArtifact.getName(),requestedVersion,currentArtifact.getScope());
  }
  if (!Objects.equals(ownerAdmin.getImpersonationPrincipal(appId),appRequest.getOwnerPrincipal())) {
    throw new ConflictException(String.format(""String_Node_Str"" + ""String_Node_Str"",Constants.Security.PRINCIPAL,appRequest.getOwnerPrincipal(),Constants.Security.PRINCIPAL,appId.getEntityType()));
  }
  Object requestedConfigObj=appRequest.getConfig();
  String requestedConfigStr=requestedConfigObj == null ? currentSpec.getConfiguration() : new Gson().toJson(requestedConfigObj);
  Id.Artifact artifactId=Artifacts.toArtifactId(appId.getParent(),newArtifactId).toId();
  return deployApp(appId.getParent(),appId.getApplication(),null,artifactId,requestedConfigStr,programTerminator);
}"
5212,"/** 
 * Get detail about the specified application
 * @param appId the id of the application to get
 * @return detail about the specified application
 * @throws ApplicationNotFoundException if the specified application does not exist
 */
public ApplicationDetail getAppDetail(ApplicationId appId) throws Exception {
  ApplicationSpecification appSpec=store.getApplication(appId);
  if (appSpec == null) {
    throw new ApplicationNotFoundException(appId);
  }
  ensureAccess(appId);
  String ownerPrincipal=ownerAdmin.getOwnerPrincipal(appId);
  return ApplicationDetail.fromSpec(appSpec,ownerPrincipal);
}","/** 
 * Get detail about the specified application
 * @param appId the id of the application to get
 * @return detail about the specified application
 * @throws ApplicationNotFoundException if the specified application does not exist
 */
public ApplicationDetail getAppDetail(ApplicationId appId) throws Exception {
  ApplicationSpecification appSpec=store.getApplication(appId);
  if (appSpec == null) {
    throw new ApplicationNotFoundException(appId);
  }
  ensureAccess(appId);
  String ownerPrincipal=ownerAdmin.getImpersonationPrincipal(appId);
  return ApplicationDetail.fromSpec(appSpec,ownerPrincipal);
}"
5213,"@Nullable @Override public ImpersonationInfo getImpersonationInfo(NamespacedEntityId entityId) throws IOException {
  if (entityId.getEntityType().equals(EntityType.PROGRAM)) {
    entityId=((ProgramId)entityId).getParent();
  }
  if (!entityId.getEntityType().equals(EntityType.NAMESPACE)) {
    KerberosPrincipalId effectiveOwner=ownerStore.getOwner(entityId);
    if (effectiveOwner != null) {
      return new ImpersonationInfo(effectiveOwner.getPrincipal(),SecurityUtil.getKeytabURIforPrincipal(effectiveOwner.getPrincipal(),cConf));
    }
  }
  try {
    NamespaceConfig nsConfig=namespaceQueryAdmin.get(entityId.getNamespaceId()).getConfig();
    String nsPrincipal=nsConfig.getPrincipal();
    return nsPrincipal == null ? null : new ImpersonationInfo(nsPrincipal,nsConfig.getKeytabURI());
  }
 catch (  IOException e) {
    throw e;
  }
catch (  Exception e) {
    throw new IOException(e);
  }
}","@Nullable @Override public ImpersonationInfo getImpersonationInfo(NamespacedEntityId entityId) throws IOException {
  entityId=getEffectiveEntity(entityId);
  if (!entityId.getEntityType().equals(EntityType.NAMESPACE)) {
    KerberosPrincipalId effectiveOwner=ownerStore.getOwner(entityId);
    if (effectiveOwner != null) {
      return new ImpersonationInfo(effectiveOwner.getPrincipal(),SecurityUtil.getKeytabURIforPrincipal(effectiveOwner.getPrincipal(),cConf));
    }
  }
  NamespaceConfig nsConfig=getNamespaceConfig(entityId.getNamespaceId());
  return nsConfig.getPrincipal() == null ? null : new ImpersonationInfo(nsConfig.getPrincipal(),nsConfig.getKeytabURI());
}"
5214,"@BeforeClass public static void init() throws Exception {
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new DiscoveryRuntimeModule().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),Modules.override(new DataSetsModules().getInMemoryModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(OwnerStore.class).to(InMemoryOwnerStore.class).in(Scopes.SINGLETON);
    }
  }
),new DataFabricModules().getInMemoryModules(),new NonCustomLocationUnitTestModule().getModule(),new NamespaceClientRuntimeModule().getInMemoryModules(),new TransactionMetricsModule(),new NotificationFeedServiceRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new ViewAdminModules().getInMemoryModules(),new AuthorizationTestModule(),new AuthenticationContextModules().getNoOpModule(),new AuthorizationEnforcementModule().getInMemoryModules(),Modules.override(new StreamAdminModules().getInMemoryModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
    }
  }
));
  setupNamespaces(injector.getInstance(NamespacedLocationFactory.class));
  streamAdmin=injector.getInstance(StreamAdmin.class);
  coordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  coordinatorClient.startAndWait();
}","@BeforeClass public static void init() throws Exception {
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new DiscoveryRuntimeModule().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),Modules.override(new DataSetsModules().getInMemoryModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(OwnerStore.class).to(InMemoryOwnerStore.class).in(Scopes.SINGLETON);
    }
  }
),new DataFabricModules().getInMemoryModules(),new NonCustomLocationUnitTestModule().getModule(),new TransactionMetricsModule(),new NotificationFeedServiceRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new ViewAdminModules().getInMemoryModules(),new AuthorizationTestModule(),new AuthenticationContextModules().getNoOpModule(),new AuthorizationEnforcementModule().getInMemoryModules(),Modules.override(new StreamAdminModules().getInMemoryModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
      bind(NamespaceQueryAdmin.class).to(SimpleNamespaceQueryAdmin.class);
    }
  }
));
  setupNamespaces(injector.getInstance(NamespacedLocationFactory.class));
  streamAdmin=injector.getInstance(StreamAdmin.class);
  coordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  coordinatorClient.startAndWait();
}"
5215,"@Override protected void configure(){
  bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
  bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
  bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
}","@Override protected void configure(){
  bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
  bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
  bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
  bind(NamespaceQueryAdmin.class).to(SimpleNamespaceQueryAdmin.class);
}"
5216,"@Test public void testOwner() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  OwnerAdmin ownerAdmin=getOwnerAdmin();
  grantAndAssertSuccess(FOO_NAMESPACE,USER,ImmutableSet.of(Action.WRITE));
  StreamId stream=FOO_NAMESPACE.stream(""String_Node_Str"");
  Properties properties=new Properties();
  String ownerPrincipal=""String_Node_Str"";
  properties.put(Constants.Security.PRINCIPAL,ownerPrincipal);
  streamAdmin.create(stream,properties);
  Assert.assertTrue(streamAdmin.exists(stream));
  Assert.assertTrue(ownerAdmin.exists(stream));
  Assert.assertEquals(ownerPrincipal,streamAdmin.getProperties(stream).getOwnerPrincipal());
  try {
    streamAdmin.updateConfig(stream,new StreamProperties(1L,null,null,null,""String_Node_Str""));
    Assert.fail();
  }
 catch (  IllegalArgumentException e) {
  }
  properties.put(Constants.Security.PRINCIPAL,""String_Node_Str"");
  try {
    streamAdmin.create(stream,properties);
    Assert.fail(""String_Node_Str"");
  }
 catch (  IllegalArgumentException e) {
  }
  Assert.assertEquals(ownerPrincipal,streamAdmin.getProperties(stream).getOwnerPrincipal());
  streamAdmin.drop(stream);
  Assert.assertFalse(ownerAdmin.exists(stream));
}","@Test public void testOwner() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  OwnerAdmin ownerAdmin=getOwnerAdmin();
  grantAndAssertSuccess(FOO_NAMESPACE,USER,ImmutableSet.of(Action.WRITE));
  StreamId stream=FOO_NAMESPACE.stream(""String_Node_Str"");
  Properties properties=new Properties();
  String ownerPrincipal=""String_Node_Str"";
  properties.put(Constants.Security.PRINCIPAL,ownerPrincipal);
  streamAdmin.create(stream,properties);
  Assert.assertTrue(streamAdmin.exists(stream));
  Assert.assertTrue(ownerAdmin.exists(stream));
  Assert.assertEquals(ownerPrincipal,streamAdmin.getProperties(stream).getOwnerPrincipal());
  try {
    streamAdmin.updateConfig(stream,new StreamProperties(1L,null,null,null,""String_Node_Str""));
    Assert.fail();
  }
 catch (  ConflictException e) {
  }
  properties.put(Constants.Security.PRINCIPAL,""String_Node_Str"");
  try {
    streamAdmin.create(stream,properties);
    Assert.fail(""String_Node_Str"");
  }
 catch (  ConflictException e) {
  }
  Assert.assertEquals(ownerPrincipal,streamAdmin.getProperties(stream).getOwnerPrincipal());
  streamAdmin.drop(stream);
  Assert.assertFalse(ownerAdmin.exists(stream));
}"
5217,"@BeforeClass public static void init() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  addCConfProperties(cConf);
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new NonCustomLocationUnitTestModule().getModule(),new SystemDatasetRuntimeModule().getInMemoryModules(),new DataSetsModules().getInMemoryModules(),new DataFabricLevelDBModule(),new TransactionMetricsModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new ViewAdminModules().getInMemoryModules(),new AuditModule().getInMemoryModules(),new NamespaceClientRuntimeModule().getInMemoryModules(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getNoOpModule(),Modules.override(new StreamAdminModules().getStandaloneModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
    }
  }
));
  streamAdmin=injector.getInstance(StreamAdmin.class);
  txManager=injector.getInstance(TransactionManager.class);
  fileWriterFactory=injector.getInstance(StreamFileWriterFactory.class);
  streamCoordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  inMemoryAuditPublisher=injector.getInstance(InMemoryAuditPublisher.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  authorizationEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  ownerAdmin=injector.getInstance(OwnerAdmin.class);
  streamCoordinatorClient.startAndWait();
  setupNamespaces(injector.getInstance(NamespacedLocationFactory.class));
  txManager.startAndWait();
  authorizationEnforcementService.startAndWait();
}","@BeforeClass public static void init() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  addCConfProperties(cConf);
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new NonCustomLocationUnitTestModule().getModule(),new SystemDatasetRuntimeModule().getInMemoryModules(),new DataSetsModules().getInMemoryModules(),new DataFabricLevelDBModule(),new TransactionMetricsModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new ViewAdminModules().getInMemoryModules(),new AuditModule().getInMemoryModules(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getNoOpModule(),Modules.override(new StreamAdminModules().getStandaloneModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
      bind(NamespaceQueryAdmin.class).to(SimpleNamespaceQueryAdmin.class);
    }
  }
));
  streamAdmin=injector.getInstance(StreamAdmin.class);
  txManager=injector.getInstance(TransactionManager.class);
  fileWriterFactory=injector.getInstance(StreamFileWriterFactory.class);
  streamCoordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  inMemoryAuditPublisher=injector.getInstance(InMemoryAuditPublisher.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  authorizationEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  ownerAdmin=injector.getInstance(OwnerAdmin.class);
  streamCoordinatorClient.startAndWait();
  setupNamespaces(injector.getInstance(NamespacedLocationFactory.class));
  txManager.startAndWait();
  authorizationEnforcementService.startAndWait();
}"
5218,"@Override protected void configure(){
  bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
  bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
  bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
  bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
}","@Override protected void configure(){
  bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
  bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
  bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
  bind(OwnerAdmin.class).to(DefaultOwnerAdmin.class);
  bind(NamespaceQueryAdmin.class).to(SimpleNamespaceQueryAdmin.class);
}"
5219,"/** 
 * Read the dataset meta data (instance and type) from MDS.
 */
private DatasetMeta getFromMds(DatasetId instance) throws Exception {
  DatasetSpecification spec=instanceManager.get(instance);
  if (spec == null) {
    throw new NotFoundException(instance);
  }
  spec=DatasetsUtil.fixOriginalProperties(spec);
  DatasetTypeId datasetTypeId=instance.getParent().datasetType(spec.getType());
  DatasetTypeMeta typeMeta=getTypeInfo(instance.getParent(),spec.getType());
  if (typeMeta == null) {
    throw new NotFoundException(datasetTypeId);
  }
  String ownerPrincipal=null;
  if (!NamespaceId.SYSTEM.equals(instance.getNamespaceId())) {
    ownerPrincipal=ownerAdmin.getOwnerPrincipal(instance);
  }
  return new DatasetMeta(spec,typeMeta,null,ownerPrincipal);
}","/** 
 * Read the dataset meta data (instance and type) from MDS.
 */
private DatasetMeta getFromMds(DatasetId instance) throws Exception {
  DatasetSpecification spec=instanceManager.get(instance);
  if (spec == null) {
    throw new NotFoundException(instance);
  }
  spec=DatasetsUtil.fixOriginalProperties(spec);
  DatasetTypeId datasetTypeId=instance.getParent().datasetType(spec.getType());
  DatasetTypeMeta typeMeta=getTypeInfo(instance.getParent(),spec.getType());
  if (typeMeta == null) {
    throw new NotFoundException(datasetTypeId);
  }
  String ownerPrincipal=null;
  if (!NamespaceId.SYSTEM.equals(instance.getNamespaceId())) {
    ownerPrincipal=ownerAdmin.getImpersonationPrincipal(instance);
  }
  return new DatasetMeta(spec,typeMeta,null,ownerPrincipal);
}"
5220,"private void verifyOwner(StreamId streamId,@Nullable String specifiedOwnerPrincipal) throws IOException {
  boolean equals=Objects.equals(specifiedOwnerPrincipal,ownerAdmin.getOwnerPrincipal(streamId));
  Preconditions.checkArgument(equals,String.format(""String_Node_Str"",Constants.Security.PRINCIPAL));
}","private void verifyOwner(StreamId streamId,@Nullable String specifiedOwnerPrincipal) throws IOException, ConflictException {
  if (!Objects.equals(specifiedOwnerPrincipal,ownerAdmin.getImpersonationPrincipal(streamId))) {
    throw new ConflictException(String.format(""String_Node_Str"" + ""String_Node_Str"",Constants.Security.PRINCIPAL,specifiedOwnerPrincipal,Constants.Security.PRINCIPAL,streamId.getEntityType()));
  }
}"
5221,"@Override public StreamProperties getProperties(StreamId streamId) throws Exception {
  ensureAccess(streamId);
  String ownerPrincipal=ownerAdmin.getOwnerPrincipal(streamId);
  StreamConfig config=getConfig(streamId);
  StreamSpecification spec=streamMetaStore.getStream(streamId);
  return new StreamProperties(config.getTTL(),config.getFormat(),config.getNotificationThresholdMB(),spec.getDescription(),ownerPrincipal);
}","@Override public StreamProperties getProperties(StreamId streamId) throws Exception {
  ensureAccess(streamId);
  String ownerPrincipal=ownerAdmin.getImpersonationPrincipal(streamId);
  StreamConfig config=getConfig(streamId);
  StreamSpecification spec=streamMetaStore.getStream(streamId);
  return new StreamProperties(config.getTTL(),config.getFormat(),config.getNotificationThresholdMB(),spec.getDescription(),ownerPrincipal);
}"
5222,"private void verifyUpdateOwnerFailure(String streamName,@Nullable String ownerPrincipal) throws IOException, URISyntaxException {
  StreamProperties newProps=new StreamProperties(1L,null,null,null,ownerPrincipal);
  HttpURLConnection urlConn=openURL(createPropertiesURL(streamName),HttpMethod.PUT);
  urlConn.setDoOutput(true);
  urlConn.getOutputStream().write(GSON.toJson(newProps).getBytes(Charsets.UTF_8));
  Assert.assertEquals(HttpResponseStatus.INTERNAL_SERVER_ERROR.getCode(),urlConn.getResponseCode());
  urlConn.disconnect();
}","private void verifyUpdateOwnerFailure(String streamName,@Nullable String ownerPrincipal) throws IOException, URISyntaxException {
  StreamProperties newProps=new StreamProperties(1L,null,null,null,ownerPrincipal);
  HttpURLConnection urlConn=openURL(createPropertiesURL(streamName),HttpMethod.PUT);
  urlConn.setDoOutput(true);
  urlConn.getOutputStream().write(GSON.toJson(newProps).getBytes(Charsets.UTF_8));
  Assert.assertEquals(HttpResponseStatus.CONFLICT.getCode(),urlConn.getResponseCode());
  urlConn.disconnect();
}"
5223,"@Override public StreamConfig create(StreamId streamId,@Nullable Properties props) throws Exception {
  return null;
}","@Override @Nullable public StreamConfig create(StreamId streamId,@Nullable Properties props) throws Exception {
  return null;
}"
5224,"@Test public void testOwner() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  OwnerAdmin ownerAdmin=getOwnerAdmin();
  grantAndAssertSuccess(FOO_NAMESPACE,USER,ImmutableSet.of(Action.WRITE));
  StreamId stream=FOO_NAMESPACE.stream(""String_Node_Str"");
  Properties properties=new Properties();
  String ownerPrincipal=""String_Node_Str"";
  properties.put(Constants.Security.PRINCIPAL,ownerPrincipal);
  streamAdmin.create(stream,properties);
  Assert.assertTrue(streamAdmin.exists(stream));
  Assert.assertTrue(ownerAdmin.exists(stream));
  Assert.assertEquals(ownerPrincipal,streamAdmin.getProperties(stream).getOwnerPrincipal());
  try {
    streamAdmin.updateConfig(stream,new StreamProperties(1L,null,null,null,""String_Node_Str""));
    Assert.fail();
  }
 catch (  IllegalArgumentException e) {
  }
  streamAdmin.drop(stream);
  Assert.assertFalse(ownerAdmin.exists(stream));
}","@Test public void testOwner() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  OwnerAdmin ownerAdmin=getOwnerAdmin();
  grantAndAssertSuccess(FOO_NAMESPACE,USER,ImmutableSet.of(Action.WRITE));
  StreamId stream=FOO_NAMESPACE.stream(""String_Node_Str"");
  Properties properties=new Properties();
  String ownerPrincipal=""String_Node_Str"";
  properties.put(Constants.Security.PRINCIPAL,ownerPrincipal);
  streamAdmin.create(stream,properties);
  Assert.assertTrue(streamAdmin.exists(stream));
  Assert.assertTrue(ownerAdmin.exists(stream));
  Assert.assertEquals(ownerPrincipal,streamAdmin.getProperties(stream).getOwnerPrincipal());
  try {
    streamAdmin.updateConfig(stream,new StreamProperties(1L,null,null,null,""String_Node_Str""));
    Assert.fail();
  }
 catch (  IllegalArgumentException e) {
  }
  properties.put(Constants.Security.PRINCIPAL,""String_Node_Str"");
  try {
    streamAdmin.create(stream,properties);
    Assert.fail(""String_Node_Str"");
  }
 catch (  IllegalArgumentException e) {
  }
  Assert.assertEquals(ownerPrincipal,streamAdmin.getProperties(stream).getOwnerPrincipal());
  streamAdmin.drop(stream);
  Assert.assertFalse(ownerAdmin.exists(stream));
}"
5225,"@Override public StreamConfig create(StreamId streamId,@Nullable Properties props) throws Exception {
  create(QueueName.fromStream(streamId),props);
  String description=(props != null) ? props.getProperty(Constants.Stream.DESCRIPTION) : null;
  streamMetaStore.addStream(streamId,description);
  publishAudit(streamId,AuditType.CREATE);
  return null;
}","@Override @Nullable public StreamConfig create(StreamId streamId,@Nullable Properties props) throws Exception {
  create(QueueName.fromStream(streamId),props);
  String description=(props != null) ? props.getProperty(Constants.Stream.DESCRIPTION) : null;
  streamMetaStore.addStream(streamId,description);
  publishAudit(streamId,AuditType.CREATE);
  return null;
}"
5226,"@Override public StreamConfig create(final StreamId streamId,@Nullable final Properties props) throws Exception {
  NamespaceId streamNamespace=streamId.getParent();
  ensureAccess(streamNamespace,Action.WRITE);
  privilegesManager.revoke(streamId);
  final Properties properties=(props == null) ? new Properties() : props;
  try {
    privilegesManager.grant(streamId,authenticationContext.getPrincipal(),EnumSet.allOf(Action.class));
    String ownerPrincipal=properties.containsKey(Constants.Security.PRINCIPAL) ? properties.getProperty(Constants.Security.PRINCIPAL) : null;
    if (exists(streamId)) {
      verifyOwner(streamId,ownerPrincipal);
    }
 else {
      if (ownerPrincipal != null) {
        ownerAdmin.add(streamId,new KerberosPrincipalId(ownerPrincipal));
      }
    }
    final Location streamLocation=impersonator.doAs(streamId,new Callable<Location>(){
      @Override public Location call() throws Exception {
        assertNamespaceHomeExists(streamId.getParent());
        Location streamLocation=getStreamLocation(streamId);
        Locations.mkdirsIfNotExists(streamLocation);
        return streamLocation;
      }
    }
);
    return streamCoordinatorClient.createStream(streamId,new Callable<StreamConfig>(){
      @Override public StreamConfig call() throws Exception {
        if (exists(streamId)) {
          return null;
        }
        long createTime=System.currentTimeMillis();
        long partitionDuration=Long.parseLong(properties.getProperty(Constants.Stream.PARTITION_DURATION,cConf.get(Constants.Stream.PARTITION_DURATION)));
        long indexInterval=Long.parseLong(properties.getProperty(Constants.Stream.INDEX_INTERVAL,cConf.get(Constants.Stream.INDEX_INTERVAL)));
        long ttl=Long.parseLong(properties.getProperty(Constants.Stream.TTL,cConf.get(Constants.Stream.TTL)));
        int threshold=Integer.parseInt(properties.getProperty(Constants.Stream.NOTIFICATION_THRESHOLD,cConf.get(Constants.Stream.NOTIFICATION_THRESHOLD)));
        String description=properties.getProperty(Constants.Stream.DESCRIPTION);
        FormatSpecification formatSpec=null;
        if (properties.containsKey(Constants.Stream.FORMAT_SPECIFICATION)) {
          formatSpec=GSON.fromJson(properties.getProperty(Constants.Stream.FORMAT_SPECIFICATION),FormatSpecification.class);
        }
        final StreamConfig config=new StreamConfig(streamId,partitionDuration,indexInterval,ttl,streamLocation,formatSpec,threshold);
        impersonator.doAs(streamId,new Callable<Void>(){
          @Override public Void call() throws Exception {
            writeConfig(config);
            return null;
          }
        }
);
        createStreamFeeds(config);
        alterExploreStream(streamId,true,config.getFormat());
        streamMetaStore.addStream(streamId,description);
        publishAudit(streamId,AuditType.CREATE);
        SystemMetadataWriter systemMetadataWriter=new StreamSystemMetadataWriter(metadataStore,streamId,config,createTime,description);
        systemMetadataWriter.write();
        return config;
      }
    }
);
  }
 catch (  Exception e) {
    privilegesManager.revoke(streamId);
    ownerAdmin.delete(streamId);
    throw e;
  }
}","@Override @Nullable public StreamConfig create(final StreamId streamId,@Nullable final Properties props) throws Exception {
  NamespaceId streamNamespace=streamId.getParent();
  ensureAccess(streamNamespace,Action.WRITE);
  final Properties properties=(props == null) ? new Properties() : props;
  String ownerPrincipal=properties.containsKey(Constants.Security.PRINCIPAL) ? properties.getProperty(Constants.Security.PRINCIPAL) : null;
  if (exists(streamId)) {
    verifyOwner(streamId,ownerPrincipal);
    return null;
  }
  try {
    privilegesManager.revoke(streamId);
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"" + ""String_Node_Str"");
  }
  if (ownerPrincipal != null) {
    ownerAdmin.add(streamId,new KerberosPrincipalId(ownerPrincipal));
  }
  try {
    privilegesManager.grant(streamId,authenticationContext.getPrincipal(),EnumSet.allOf(Action.class));
    try {
      final Location streamLocation=impersonator.doAs(streamId,new Callable<Location>(){
        @Override public Location call() throws Exception {
          assertNamespaceHomeExists(streamId.getParent());
          Location streamLocation=getStreamLocation(streamId);
          Locations.mkdirsIfNotExists(streamLocation);
          return streamLocation;
        }
      }
);
      return createStream(streamId,properties,streamLocation);
    }
 catch (    Exception e) {
      privilegesManager.revoke(streamId);
      throw e;
    }
  }
 catch (  Exception e) {
    ownerAdmin.delete(streamId);
    throw e;
  }
}"
5227,"/** 
 * Creates stream if doesn't exist. If stream exists, does nothing.
 * @param streamId Id of the stream to create
 * @param props additional properties
 * @return The {@link StreamConfig} associated with the new stream
 * @throws Exception if creation fails
 */
StreamConfig create(StreamId streamId,@Nullable Properties props) throws Exception ;","/** 
 * Creates stream if doesn't exist. If stream exists, does nothing.
 * @param streamId Id of the stream to create
 * @param props additional properties
 * @return The {@link StreamConfig} associated with the new stream or null if the stream already exists
 * @throws Exception if creation fails
 */
@Nullable StreamConfig create(StreamId streamId,@Nullable Properties props) throws Exception ;"
5228,"private void assertNoAccess(final EntityId entityId) throws Exception {
  Authorizer authorizer=getAuthorizer();
  Predicate<Privilege> entityFilter=new Predicate<Privilege>(){
    @Override public boolean apply(    Privilege input){
      return entityId.equals(input.getEntity());
    }
  }
;
  Assert.assertTrue(Sets.filter(authorizer.listPrivileges(ALICE),entityFilter).isEmpty());
  Assert.assertTrue(Sets.filter(authorizer.listPrivileges(BOB),entityFilter).isEmpty());
}","private void assertNoAccess(final EntityId entityId) throws Exception {
  assertNoAccess(ALICE,entityId);
  assertNoAccess(BOB,entityId);
}"
5229,"private SearchResults searchByCustomIndex(String namespaceId,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,@Nullable String cursor,boolean showHidden,Set<EntityScope> entityScope){
  List<MetadataEntry> returnedResults=new LinkedList<>();
  List<MetadataEntry> allResults=new LinkedList<>();
  String indexColumn=getIndexColumn(sortInfo.getSortBy(),sortInfo.getSortOrder());
  int fetchSize=offset + ((numCursors + 1) * limit);
  List<String> cursors=new ArrayList<>(numCursors);
  for (  String searchTerm : getSearchTerms(namespaceId,""String_Node_Str"",entityScope)) {
    byte[] startKey=Bytes.toBytes(searchTerm.substring(0,searchTerm.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    if (!Strings.isNullOrEmpty(cursor)) {
      String namespaceInStartKey=searchTerm.substring(0,searchTerm.indexOf(KEYVALUE_SEPARATOR));
      startKey=Bytes.toBytes(namespaceInStartKey + KEYVALUE_SEPARATOR + cursor);
    }
    int mod=(limit == 1) ? 0 : 1;
    try (Scanner scanner=indexedTable.scanByIndex(Bytes.toBytes(indexColumn),startKey,stopKey)){
      Row next;
      while ((next=scanner.next()) != null) {
        Optional<MetadataEntry> metadataEntry=parseRow(next,indexColumn,types,showHidden);
        if (!metadataEntry.isPresent()) {
          continue;
        }
        allResults.add(metadataEntry.get());
        if (allResults.size() <= offset || allResults.size() > fetchSize) {
          continue;
        }
        if (returnedResults.size() < limit) {
          returnedResults.add(metadataEntry.get());
        }
 else {
          if ((allResults.size() - offset) % limit == mod) {
            String cursorWithNamespace=Bytes.toString(next.get(indexColumn));
            cursors.add(cursorWithNamespace.substring(cursorWithNamespace.indexOf(KEYVALUE_SEPARATOR) + 1));
          }
        }
      }
    }
   }
  return new SearchResults(returnedResults,cursors,allResults);
}","private SearchResults searchByCustomIndex(String namespaceId,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,@Nullable String cursor,boolean showHidden,Set<EntityScope> entityScope){
  List<MetadataEntry> returnedResults=new LinkedList<>();
  List<MetadataEntry> allResults=new LinkedList<>();
  String indexColumn=getIndexColumn(sortInfo.getSortBy(),sortInfo.getSortOrder());
  int fetchSize=(int)Math.min(offset + ((numCursors + 1) * (long)limit),Integer.MAX_VALUE);
  List<String> cursors=new ArrayList<>(numCursors);
  for (  String searchTerm : getSearchTerms(namespaceId,""String_Node_Str"",entityScope)) {
    byte[] startKey=Bytes.toBytes(searchTerm.substring(0,searchTerm.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    if (!Strings.isNullOrEmpty(cursor)) {
      String namespaceInStartKey=searchTerm.substring(0,searchTerm.indexOf(KEYVALUE_SEPARATOR));
      startKey=Bytes.toBytes(namespaceInStartKey + KEYVALUE_SEPARATOR + cursor);
    }
    int mod=(limit == 1) ? 0 : 1;
    try (Scanner scanner=indexedTable.scanByIndex(Bytes.toBytes(indexColumn),startKey,stopKey)){
      Row next;
      while ((next=scanner.next()) != null && allResults.size() < fetchSize) {
        Optional<MetadataEntry> metadataEntry=parseRow(next,indexColumn,types,showHidden);
        if (!metadataEntry.isPresent()) {
          continue;
        }
        allResults.add(metadataEntry.get());
        if (allResults.size() <= offset) {
          continue;
        }
        if (returnedResults.size() < limit) {
          returnedResults.add(metadataEntry.get());
        }
 else {
          if ((allResults.size() - offset) % limit == mod) {
            String cursorWithNamespace=Bytes.toString(next.get(indexColumn));
            cursors.add(cursorWithNamespace.substring(cursorWithNamespace.indexOf(KEYVALUE_SEPARATOR) + 1));
          }
        }
      }
    }
   }
  return new SearchResults(returnedResults,cursors,allResults);
}"
5230,"@Test public void testPagination() throws Exception {
  final MetadataDataset dataset=getDataset(DatasetFrameworkTestUtil.NAMESPACE_ID.dataset(""String_Node_Str""),MetadataScope.SYSTEM);
  TransactionExecutor txnl=dsFrameworkUtil.newInMemoryTransactionExecutor((TransactionAware)dataset);
  final String flowName=""String_Node_Str"";
  final String dsName=""String_Node_Str"";
  final String appName=""String_Node_Str"";
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      dataset.setProperty(flow1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,flowName);
      dataset.setProperty(dataset1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,dsName);
      dataset.setProperty(app1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,appName);
    }
  }
);
  final String namespaceId=flow1.getNamespace();
  final EnumSet<EntityTypeSimpleName> targets=EnumSet.allOf(EntityTypeSimpleName.class);
  final MetadataEntry flowEntry=new MetadataEntry(flow1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,flowName);
  final MetadataEntry dsEntry=new MetadataEntry(dataset1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,dsName);
  final MetadataEntry appEntry=new MetadataEntry(app1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,appName);
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      SearchResults searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,SortInfo.DEFAULT,0,3,1,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,dsEntry,dsEntry,appEntry,appEntry,appEntry,appEntry),searchResults.getResults());
      SortInfo nameAsc=new SortInfo(AbstractSystemMetadataWriter.ENTITY_NAME_KEY,SortInfo.SortOrder.ASC);
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,2,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,appEntry),searchResults.getAllResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,1,2,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(dsEntry,appEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,appEntry),searchResults.getAllResults());
      SortInfo nameDesc=new SortInfo(AbstractSystemMetadataWriter.ENTITY_NAME_KEY,SortInfo.SortOrder.DESC);
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,0,2,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(appEntry,dsEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(appEntry,dsEntry,flowEntry),searchResults.getAllResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,2,1,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(appEntry,dsEntry,flowEntry),searchResults.getAllResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,2,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,appEntry),searchResults.getAllResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,1,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(ImmutableList.of(appEntry,dsEntry,flowEntry),searchResults.getAllResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,4,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,appEntry),searchResults.getAllResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,100,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(ImmutableList.of(appEntry,dsEntry,flowEntry),searchResults.getAllResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(dsName,appName),searchResults.getCursors());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,searchResults.getCursors().get(0),false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(dsEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(appName),searchResults.getCursors());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,searchResults.getCursors().get(0),false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(appEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(),searchResults.getCursors());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,2,3,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(appName),searchResults.getCursors());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,3,1,2,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(),searchResults.getCursors());
    }
  }
);
}","@Test public void testPagination() throws Exception {
  final MetadataDataset dataset=getDataset(DatasetFrameworkTestUtil.NAMESPACE_ID.dataset(""String_Node_Str""),MetadataScope.SYSTEM);
  TransactionExecutor txnl=dsFrameworkUtil.newInMemoryTransactionExecutor((TransactionAware)dataset);
  final String flowName=""String_Node_Str"";
  final String dsName=""String_Node_Str"";
  final String appName=""String_Node_Str"";
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      dataset.setProperty(flow1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,flowName);
      dataset.setProperty(dataset1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,dsName);
      dataset.setProperty(app1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,appName);
    }
  }
);
  final String namespaceId=flow1.getNamespace();
  final EnumSet<EntityTypeSimpleName> targets=EnumSet.allOf(EntityTypeSimpleName.class);
  final MetadataEntry flowEntry=new MetadataEntry(flow1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,flowName);
  final MetadataEntry dsEntry=new MetadataEntry(dataset1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,dsName);
  final MetadataEntry appEntry=new MetadataEntry(app1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,appName);
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      SearchResults searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,SortInfo.DEFAULT,0,3,1,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,dsEntry,dsEntry,appEntry,appEntry,appEntry,appEntry),searchResults.getResults());
      SortInfo nameAsc=new SortInfo(AbstractSystemMetadataWriter.ENTITY_NAME_KEY,SortInfo.SortOrder.ASC);
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,2,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry),searchResults.getResults());
      Assert.assertEquals(2,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,1,2,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(dsEntry,appEntry),searchResults.getResults());
      Assert.assertEquals(3,searchResults.getAllResults().size());
      SortInfo nameDesc=new SortInfo(AbstractSystemMetadataWriter.ENTITY_NAME_KEY,SortInfo.SortOrder.DESC);
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,0,2,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(appEntry,dsEntry),searchResults.getResults());
      Assert.assertEquals(2,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,2,1,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry),searchResults.getResults());
      Assert.assertEquals(3,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,2,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(2,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,1,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(1,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,4,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(3,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,100,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(3,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(dsName,appName),searchResults.getCursors());
      Assert.assertEquals(3,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,searchResults.getCursors().get(0),false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(dsEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(appName),searchResults.getCursors());
      Assert.assertEquals(2,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,searchResults.getCursors().get(0),false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(appEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(),searchResults.getCursors());
      Assert.assertEquals(1,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,2,3,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(appName),searchResults.getCursors());
      Assert.assertEquals(3,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,3,1,2,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(),searchResults.getCursors());
      Assert.assertEquals(3,searchResults.getAllResults().size());
    }
  }
);
}"
5231,"@Override public void process(ApplicationWithPrograms input) throws Exception {
  Collection<ApplicationId> allAppVersionsAppIds=store.getAllAppVersionsAppIds(input.getApplicationId());
  if (allAppVersionsAppIds.isEmpty() && input.getOwnerPrincipal() != null) {
    addOwner(input.getApplicationId(),input.getOwnerPrincipal());
  }
  store.addApplication(input.getApplicationId(),input.getSpecification());
  registerDatasets(input);
  emit(input);
}","@Override public void process(ApplicationWithPrograms input) throws Exception {
  Collection<ApplicationId> allAppVersionsAppIds=store.getAllAppVersionsAppIds(input.getApplicationId());
  boolean ownerAdded=addOwnerIfRequired(input,allAppVersionsAppIds);
  try {
    store.addApplication(input.getApplicationId(),input.getSpecification());
  }
 catch (  Exception e) {
    if (ownerAdded) {
      ownerAdmin.delete(input.getApplicationId());
    }
    throw e;
  }
  registerDatasets(input);
  emit(input);
}"
5232,"private SearchResults searchByCustomIndex(String namespaceId,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,@Nullable String cursor,boolean showHidden,Set<EntityScope> entityScope){
  List<MetadataEntry> returnedResults=new LinkedList<>();
  List<MetadataEntry> allResults=new LinkedList<>();
  String indexColumn=getIndexColumn(sortInfo.getSortBy(),sortInfo.getSortOrder());
  int fetchSize=(int)Math.min(offset + ((numCursors + 1) * (long)limit),Integer.MAX_VALUE);
  List<String> cursors=new ArrayList<>(numCursors);
  for (  String searchTerm : getSearchTerms(namespaceId,""String_Node_Str"",entityScope)) {
    byte[] startKey=Bytes.toBytes(searchTerm.substring(0,searchTerm.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    if (!Strings.isNullOrEmpty(cursor)) {
      String namespaceInStartKey=searchTerm.substring(0,searchTerm.indexOf(KEYVALUE_SEPARATOR));
      startKey=Bytes.toBytes(namespaceInStartKey + KEYVALUE_SEPARATOR + cursor);
    }
    int mod=(limit == 1) ? 0 : 1;
    try (Scanner scanner=indexedTable.scanByIndex(Bytes.toBytes(indexColumn),startKey,stopKey)){
      Row next;
      while ((next=scanner.next()) != null && allResults.size() < fetchSize) {
        Optional<MetadataEntry> metadataEntry=parseRow(next,indexColumn,types,showHidden);
        if (!metadataEntry.isPresent()) {
          continue;
        }
        allResults.add(metadataEntry.get());
        if (allResults.size() <= offset) {
          continue;
        }
        if (returnedResults.size() < limit) {
          returnedResults.add(metadataEntry.get());
        }
 else {
          if ((allResults.size() - offset) % limit == mod) {
            String cursorWithNamespace=Bytes.toString(next.get(indexColumn));
            cursors.add(cursorWithNamespace.substring(cursorWithNamespace.indexOf(KEYVALUE_SEPARATOR) + 1));
          }
        }
      }
    }
   }
  return new SearchResults(returnedResults,cursors,allResults);
}","private SearchResults searchByCustomIndex(String namespaceId,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,@Nullable String cursor,boolean showHidden,Set<EntityScope> entityScope){
  List<MetadataEntry> resultsFromOffset=new LinkedList<>();
  List<MetadataEntry> resultsFromBeginning=new LinkedList<>();
  String indexColumn=getIndexColumn(sortInfo.getSortBy(),sortInfo.getSortOrder());
  int fetchSize=(int)Math.min(offset + ((numCursors + 1) * (long)limit),Integer.MAX_VALUE);
  List<String> cursors=new ArrayList<>(numCursors);
  for (  String searchTerm : getSearchTerms(namespaceId,""String_Node_Str"",entityScope)) {
    byte[] startKey=Bytes.toBytes(searchTerm.substring(0,searchTerm.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    if (!Strings.isNullOrEmpty(cursor)) {
      String namespaceInStartKey=searchTerm.substring(0,searchTerm.indexOf(KEYVALUE_SEPARATOR));
      startKey=Bytes.toBytes(namespaceInStartKey + KEYVALUE_SEPARATOR + cursor);
    }
    int mod=(limit == 1) ? 0 : 1;
    try (Scanner scanner=indexedTable.scanByIndex(Bytes.toBytes(indexColumn),startKey,stopKey)){
      Row next;
      while ((next=scanner.next()) != null && resultsFromBeginning.size() < fetchSize) {
        Optional<MetadataEntry> metadataEntry=parseRow(next,indexColumn,types,showHidden);
        if (!metadataEntry.isPresent()) {
          continue;
        }
        resultsFromBeginning.add(metadataEntry.get());
        if (resultsFromBeginning.size() <= offset) {
          continue;
        }
        if (resultsFromOffset.size() < limit) {
          resultsFromOffset.add(metadataEntry.get());
        }
 else {
          if ((resultsFromBeginning.size() - offset) % limit == mod) {
            String cursorWithNamespace=Bytes.toString(next.get(indexColumn));
            cursors.add(cursorWithNamespace.substring(cursorWithNamespace.indexOf(KEYVALUE_SEPARATOR) + 1));
          }
        }
      }
    }
   }
  return new SearchResults(resultsFromOffset,cursors,resultsFromBeginning);
}"
5233,"SearchResults(List<MetadataEntry> results,List<String> cursors,List<MetadataEntry> allResults){
  this.results=results;
  this.cursors=cursors;
  this.allResults=allResults;
}","SearchResults(List<MetadataEntry> results,List<String> cursors,List<MetadataEntry> allResults){
  this.resultsFromOffset=results;
  this.cursors=cursors;
  this.resultsFromBeginning=allResults;
}"
5234,"private MetadataSearchResponse search(Set<MetadataScope> scopes,String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,String cursor,boolean showHidden,Set<EntityScope> entityScope) throws BadRequestException {
  if (offset < 0) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (limit < 0) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  List<MetadataEntry> results=new LinkedList<>();
  List<String> cursors=new LinkedList<>();
  List<MetadataEntry> allResults=new LinkedList<>();
  for (  MetadataScope scope : scopes) {
    SearchResults searchResults=getSearchResults(scope,namespaceId,searchQuery,types,sortInfo,offset,limit,numCursors,cursor,showHidden,entityScope);
    results.addAll(searchResults.getResults());
    cursors.addAll(searchResults.getCursors());
    allResults.addAll(searchResults.getAllResults());
  }
  Set<NamespacedEntityId> sortedEntities=getSortedEntities(results,sortInfo);
  int total=getSortedEntities(allResults,sortInfo).size();
  if (SortInfo.DEFAULT.equals(sortInfo)) {
    int startIndex=Math.min(offset,sortedEntities.size());
    int endIndex=(int)Math.min(Integer.MAX_VALUE,(long)offset + limit);
    endIndex=Math.min(endIndex,sortedEntities.size());
    sortedEntities=new LinkedHashSet<>(ImmutableList.copyOf(sortedEntities).subList(startIndex,endIndex));
  }
  Map<NamespacedEntityId,Metadata> systemMetadata=fetchMetadata(sortedEntities,MetadataScope.SYSTEM);
  Map<NamespacedEntityId,Metadata> userMetadata=fetchMetadata(sortedEntities,MetadataScope.USER);
  return new MetadataSearchResponse(sortInfo.getSortBy() + ""String_Node_Str"" + sortInfo.getSortOrder(),offset,limit,numCursors,total,addMetadataToEntities(sortedEntities,systemMetadata,userMetadata),cursors,showHidden,entityScope);
}","private MetadataSearchResponse search(Set<MetadataScope> scopes,String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,String cursor,boolean showHidden,Set<EntityScope> entityScope) throws BadRequestException {
  if (offset < 0) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (limit < 0) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  List<MetadataEntry> resultsFromOffset=new LinkedList<>();
  List<String> cursors=new LinkedList<>();
  List<MetadataEntry> resultsFromBeginning=new LinkedList<>();
  for (  MetadataScope scope : scopes) {
    SearchResults searchResults=getSearchResults(scope,namespaceId,searchQuery,types,sortInfo,offset,limit,numCursors,cursor,showHidden,entityScope);
    resultsFromOffset.addAll(searchResults.getResultsFromOffset());
    cursors.addAll(searchResults.getCursors());
    resultsFromBeginning.addAll(searchResults.getResultsFromBeginning());
  }
  Set<NamespacedEntityId> sortedEntities=getSortedEntities(resultsFromOffset,sortInfo);
  int total=getSortedEntities(resultsFromBeginning,sortInfo).size();
  if (SortInfo.DEFAULT.equals(sortInfo)) {
    int startIndex=Math.min(offset,sortedEntities.size());
    int endIndex=(int)Math.min(Integer.MAX_VALUE,(long)offset + limit);
    endIndex=Math.min(endIndex,sortedEntities.size());
    sortedEntities=new LinkedHashSet<>(ImmutableList.copyOf(sortedEntities).subList(startIndex,endIndex));
  }
  Map<NamespacedEntityId,Metadata> systemMetadata=fetchMetadata(sortedEntities,MetadataScope.SYSTEM);
  Map<NamespacedEntityId,Metadata> userMetadata=fetchMetadata(sortedEntities,MetadataScope.USER);
  return new MetadataSearchResponse(sortInfo.getSortBy() + ""String_Node_Str"" + sortInfo.getSortOrder(),offset,limit,numCursors,total,addMetadataToEntities(sortedEntities,systemMetadata,userMetadata),cursors,showHidden,entityScope);
}"
5235,"@Test public void testPagination() throws Exception {
  final MetadataDataset dataset=getDataset(DatasetFrameworkTestUtil.NAMESPACE_ID.dataset(""String_Node_Str""),MetadataScope.SYSTEM);
  TransactionExecutor txnl=dsFrameworkUtil.newInMemoryTransactionExecutor((TransactionAware)dataset);
  final String flowName=""String_Node_Str"";
  final String dsName=""String_Node_Str"";
  final String appName=""String_Node_Str"";
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      dataset.setProperty(flow1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,flowName);
      dataset.setProperty(dataset1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,dsName);
      dataset.setProperty(app1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,appName);
    }
  }
);
  final String namespaceId=flow1.getNamespace();
  final EnumSet<EntityTypeSimpleName> targets=EnumSet.allOf(EntityTypeSimpleName.class);
  final MetadataEntry flowEntry=new MetadataEntry(flow1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,flowName);
  final MetadataEntry dsEntry=new MetadataEntry(dataset1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,dsName);
  final MetadataEntry appEntry=new MetadataEntry(app1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,appName);
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      SearchResults searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,SortInfo.DEFAULT,0,3,1,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,dsEntry,dsEntry,appEntry,appEntry,appEntry,appEntry),searchResults.getResults());
      SortInfo nameAsc=new SortInfo(AbstractSystemMetadataWriter.ENTITY_NAME_KEY,SortInfo.SortOrder.ASC);
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,2,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry),searchResults.getResults());
      Assert.assertEquals(2,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,1,2,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(dsEntry,appEntry),searchResults.getResults());
      Assert.assertEquals(3,searchResults.getAllResults().size());
      SortInfo nameDesc=new SortInfo(AbstractSystemMetadataWriter.ENTITY_NAME_KEY,SortInfo.SortOrder.DESC);
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,0,2,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(appEntry,dsEntry),searchResults.getResults());
      Assert.assertEquals(2,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,2,1,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry),searchResults.getResults());
      Assert.assertEquals(3,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,2,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(2,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,1,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(1,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,4,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(3,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,100,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(3,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(dsName,appName),searchResults.getCursors());
      Assert.assertEquals(3,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,searchResults.getCursors().get(0),false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(dsEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(appName),searchResults.getCursors());
      Assert.assertEquals(2,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,searchResults.getCursors().get(0),false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(appEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(),searchResults.getCursors());
      Assert.assertEquals(1,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,2,3,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(appName),searchResults.getCursors());
      Assert.assertEquals(3,searchResults.getAllResults().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,3,1,2,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(),searchResults.getCursors());
      Assert.assertEquals(3,searchResults.getAllResults().size());
    }
  }
);
}","@Test public void testPagination() throws Exception {
  final MetadataDataset dataset=getDataset(DatasetFrameworkTestUtil.NAMESPACE_ID.dataset(""String_Node_Str""),MetadataScope.SYSTEM);
  TransactionExecutor txnl=dsFrameworkUtil.newInMemoryTransactionExecutor((TransactionAware)dataset);
  final String flowName=""String_Node_Str"";
  final String dsName=""String_Node_Str"";
  final String appName=""String_Node_Str"";
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      dataset.setProperty(flow1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,flowName);
      dataset.setProperty(dataset1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,dsName);
      dataset.setProperty(app1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,appName);
    }
  }
);
  final String namespaceId=flow1.getNamespace();
  final EnumSet<EntityTypeSimpleName> targets=EnumSet.allOf(EntityTypeSimpleName.class);
  final MetadataEntry flowEntry=new MetadataEntry(flow1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,flowName);
  final MetadataEntry dsEntry=new MetadataEntry(dataset1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,dsName);
  final MetadataEntry appEntry=new MetadataEntry(app1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,appName);
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      SearchResults searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,SortInfo.DEFAULT,0,3,1,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,dsEntry,dsEntry,appEntry,appEntry,appEntry,appEntry),searchResults.getResultsFromOffset());
      SortInfo nameAsc=new SortInfo(AbstractSystemMetadataWriter.ENTITY_NAME_KEY,SortInfo.SortOrder.ASC);
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,2,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry),searchResults.getResultsFromOffset());
      Assert.assertEquals(2,searchResults.getResultsFromBeginning().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,1,2,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(dsEntry,appEntry),searchResults.getResultsFromOffset());
      Assert.assertEquals(3,searchResults.getResultsFromBeginning().size());
      SortInfo nameDesc=new SortInfo(AbstractSystemMetadataWriter.ENTITY_NAME_KEY,SortInfo.SortOrder.DESC);
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,0,2,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(appEntry,dsEntry),searchResults.getResultsFromOffset());
      Assert.assertEquals(2,searchResults.getResultsFromBeginning().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,2,1,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry),searchResults.getResultsFromOffset());
      Assert.assertEquals(3,searchResults.getResultsFromBeginning().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,2,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResultsFromOffset().isEmpty());
      Assert.assertEquals(2,searchResults.getResultsFromBeginning().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,1,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResultsFromOffset().isEmpty());
      Assert.assertEquals(1,searchResults.getResultsFromBeginning().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,4,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResultsFromOffset().isEmpty());
      Assert.assertEquals(3,searchResults.getResultsFromBeginning().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,100,0,0,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertTrue(searchResults.getResultsFromOffset().isEmpty());
      Assert.assertEquals(3,searchResults.getResultsFromBeginning().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry),searchResults.getResultsFromOffset());
      Assert.assertEquals(ImmutableList.of(dsName,appName),searchResults.getCursors());
      Assert.assertEquals(3,searchResults.getResultsFromBeginning().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,searchResults.getCursors().get(0),false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(dsEntry),searchResults.getResultsFromOffset());
      Assert.assertEquals(ImmutableList.of(appName),searchResults.getCursors());
      Assert.assertEquals(2,searchResults.getResultsFromBeginning().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,searchResults.getCursors().get(0),false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(appEntry),searchResults.getResultsFromOffset());
      Assert.assertEquals(ImmutableList.of(),searchResults.getCursors());
      Assert.assertEquals(1,searchResults.getResultsFromBeginning().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,2,3,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry),searchResults.getResultsFromOffset());
      Assert.assertEquals(ImmutableList.of(appName),searchResults.getCursors());
      Assert.assertEquals(3,searchResults.getResultsFromBeginning().size());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,3,1,2,null,false,EnumSet.allOf(EntityScope.class));
      Assert.assertEquals(ImmutableList.of(),searchResults.getResultsFromOffset());
      Assert.assertEquals(ImmutableList.of(),searchResults.getCursors());
      Assert.assertEquals(3,searchResults.getResultsFromBeginning().size());
    }
  }
);
}"
5236,"private List<MetadataEntry> searchByDefaultIndex(MetadataDataset dataset,String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types) throws BadRequestException {
  return dataset.search(namespaceId,searchQuery,types,SortInfo.DEFAULT,0,Integer.MAX_VALUE,1,null,false,EnumSet.allOf(EntityScope.class)).getResults();
}","private List<MetadataEntry> searchByDefaultIndex(MetadataDataset dataset,String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types) throws BadRequestException {
  return dataset.search(namespaceId,searchQuery,types,SortInfo.DEFAULT,0,Integer.MAX_VALUE,1,null,false,EnumSet.allOf(EntityScope.class)).getResultsFromOffset();
}"
5237,"@Test public void testSearchDifferentEntityScope() throws InterruptedException, TransactionFailureException {
  final ArtifactId sysArtifact=NamespaceId.SYSTEM.artifact(""String_Node_Str"",""String_Node_Str"");
  final ArtifactId nsArtifact=new ArtifactId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  final String multiWordKey=""String_Node_Str"";
  final String multiWordValue=""String_Node_Str"";
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      dataset.setProperty(nsArtifact,multiWordKey,multiWordValue);
      dataset.setProperty(sysArtifact,multiWordKey,multiWordValue);
    }
  }
);
  final MetadataEntry systemArtifactEntry=new MetadataEntry(sysArtifact,multiWordKey,multiWordValue);
  final MetadataEntry nsArtifactEntry=new MetadataEntry(nsArtifact,multiWordKey,multiWordValue);
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      List<MetadataEntry> results=dataset.search(""String_Node_Str"",""String_Node_Str"",ImmutableSet.of(EntityTypeSimpleName.ALL),SortInfo.DEFAULT,0,Integer.MAX_VALUE,1,null,false,EnumSet.of(EntityScope.USER)).getResults();
      Assert.assertEquals(Sets.newHashSet(nsArtifactEntry),Sets.newHashSet(results));
      results=dataset.search(""String_Node_Str"",""String_Node_Str"",ImmutableSet.of(EntityTypeSimpleName.ALL),SortInfo.DEFAULT,0,Integer.MAX_VALUE,1,null,false,EnumSet.of(EntityScope.SYSTEM)).getResults();
      Assert.assertEquals(Sets.newHashSet(systemArtifactEntry),Sets.newHashSet(results));
      results=dataset.search(""String_Node_Str"",""String_Node_Str"",ImmutableSet.of(EntityTypeSimpleName.ALL),SortInfo.DEFAULT,0,Integer.MAX_VALUE,1,null,false,EnumSet.allOf(EntityScope.class)).getResults();
      Assert.assertEquals(Sets.newHashSet(nsArtifactEntry,systemArtifactEntry),Sets.newHashSet(results));
    }
  }
);
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      dataset.removeProperties(nsArtifact);
      dataset.removeProperties(sysArtifact);
    }
  }
);
}","@Test public void testSearchDifferentEntityScope() throws InterruptedException, TransactionFailureException {
  final ArtifactId sysArtifact=NamespaceId.SYSTEM.artifact(""String_Node_Str"",""String_Node_Str"");
  final ArtifactId nsArtifact=new ArtifactId(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  final String multiWordKey=""String_Node_Str"";
  final String multiWordValue=""String_Node_Str"";
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      dataset.setProperty(nsArtifact,multiWordKey,multiWordValue);
      dataset.setProperty(sysArtifact,multiWordKey,multiWordValue);
    }
  }
);
  final MetadataEntry systemArtifactEntry=new MetadataEntry(sysArtifact,multiWordKey,multiWordValue);
  final MetadataEntry nsArtifactEntry=new MetadataEntry(nsArtifact,multiWordKey,multiWordValue);
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      List<MetadataEntry> results=dataset.search(""String_Node_Str"",""String_Node_Str"",ImmutableSet.of(EntityTypeSimpleName.ALL),SortInfo.DEFAULT,0,Integer.MAX_VALUE,1,null,false,EnumSet.of(EntityScope.USER)).getResultsFromOffset();
      Assert.assertEquals(Sets.newHashSet(nsArtifactEntry),Sets.newHashSet(results));
      results=dataset.search(""String_Node_Str"",""String_Node_Str"",ImmutableSet.of(EntityTypeSimpleName.ALL),SortInfo.DEFAULT,0,Integer.MAX_VALUE,1,null,false,EnumSet.of(EntityScope.SYSTEM)).getResultsFromOffset();
      Assert.assertEquals(Sets.newHashSet(systemArtifactEntry),Sets.newHashSet(results));
      results=dataset.search(""String_Node_Str"",""String_Node_Str"",ImmutableSet.of(EntityTypeSimpleName.ALL),SortInfo.DEFAULT,0,Integer.MAX_VALUE,1,null,false,EnumSet.allOf(EntityScope.class)).getResultsFromOffset();
      Assert.assertEquals(Sets.newHashSet(nsArtifactEntry,systemArtifactEntry),Sets.newHashSet(results));
    }
  }
);
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      dataset.removeProperties(nsArtifact);
      dataset.removeProperties(sysArtifact);
    }
  }
);
}"
5238,"@Override public void alive(){
  executor.execute(new Runnable(){
    @Override public void run(){
      listener.alive();
    }
  }
);
}","@Override public void alive(){
  execute(new Runnable(){
    @Override public void run(){
      listener.alive();
    }
  }
,""String_Node_Str"");
}"
5239,"@Override public void error(final Throwable cause){
  executor.execute(new Runnable(){
    @Override public void run(){
      listener.error(cause);
    }
  }
);
}","@Override public void error(final Throwable cause){
  execute(new Runnable(){
    @Override public void run(){
      listener.error(cause);
    }
  }
,""String_Node_Str"");
}"
5240,"@Override public void resuming(){
  executor.execute(new Runnable(){
    @Override public void run(){
      listener.resuming();
    }
  }
);
}","@Override public void resuming(){
  execute(new Runnable(){
    @Override public void run(){
      listener.resuming();
    }
  }
,""String_Node_Str"");
}"
5241,"@Override public void suspending(){
  executor.execute(new Runnable(){
    @Override public void run(){
      listener.suspending();
    }
  }
);
}","@Override public void suspending(){
  execute(new Runnable(){
    @Override public void run(){
      listener.suspending();
    }
  }
,""String_Node_Str"");
}"
5242,"@Override public void init(final State currentState,@Nullable final Throwable cause){
  executor.execute(new Runnable(){
    @Override public void run(){
      listener.init(currentState,cause);
    }
  }
);
}","@Override public void init(final State currentState,@Nullable final Throwable cause){
  execute(new Runnable(){
    @Override public void run(){
      listener.init(currentState,cause);
    }
  }
,""String_Node_Str"");
}"
5243,"@Override public void completed(){
  executor.execute(new Runnable(){
    @Override public void run(){
      listener.completed();
    }
  }
);
}","@Override public void completed(){
  execute(new Runnable(){
    @Override public void run(){
      listener.completed();
    }
  }
,""String_Node_Str"");
}"
5244,"@Override public void killed(){
  executor.execute(new Runnable(){
    @Override public void run(){
      listener.killed();
    }
  }
);
}","@Override public void killed(){
  execute(new Runnable(){
    @Override public void run(){
      listener.killed();
    }
  }
,""String_Node_Str"");
}"
5245,"@Override public void suspended(){
  executor.execute(new Runnable(){
    @Override public void run(){
      listener.suspended();
    }
  }
);
}","@Override public void suspended(){
  execute(new Runnable(){
    @Override public void run(){
      listener.suspended();
    }
  }
,""String_Node_Str"");
}"
5246,"@Override public void stopping(){
  executor.execute(new Runnable(){
    @Override public void run(){
      listener.stopping();
    }
  }
);
}","@Override public void stopping(){
  execute(new Runnable(){
    @Override public void run(){
      listener.stopping();
    }
  }
,""String_Node_Str"");
}"
5247,"/** 
 * Collects stats from all   {@link OperationalStats}.
 */
private void collectOperationalStats() throws InterruptedException {
  LOG.debug(""String_Node_Str"");
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    if (!isRunning()) {
      return;
    }
    OperationalStats stats=entry.getValue();
    LOG.debug(""String_Node_Str"",stats.getServiceName(),stats.getStatType());
    try {
      stats.collect();
    }
 catch (    InterruptedException e) {
      throw e;
    }
catch (    Throwable t) {
      Throwable rootCause=Throwables.getRootCause(t);
      if (rootCause instanceof InterruptedException) {
        throw (InterruptedException)rootCause;
      }
      LOG.warn(""String_Node_Str"",stats.getServiceName(),stats.getStatType(),rootCause.getMessage());
    }
  }
}","/** 
 * Collects stats from all   {@link OperationalStats}.
 */
private void collectOperationalStats() throws InterruptedException {
  LOG.debug(""String_Node_Str"");
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    if (!isRunning()) {
      return;
    }
    OperationalStats stats=entry.getValue();
    LOG.debug(""String_Node_Str"",stats.getServiceName(),stats.getStatType());
    try {
      stats.collect();
    }
 catch (    Throwable t) {
      Throwables.propagateIfInstanceOf(t,InterruptedException.class);
      Throwable rootCause=Throwables.getRootCause(t);
      if (rootCause instanceof ServiceUnavailableException || rootCause instanceof TException) {
        return;
      }
      if (rootCause instanceof InterruptedException) {
        throw (InterruptedException)rootCause;
      }
      LOG.warn(""String_Node_Str"",stats.getServiceName(),stats.getStatType(),rootCause.getMessage());
    }
  }
}"
5248,"private MetricsConsumerMetaTable getMetaTable(){
  while (metaTable == null) {
    if (stopping) {
      LOG.info(""String_Node_Str"");
      break;
    }
    try {
      metaTable=metricDatasetFactory.createConsumerMeta();
    }
 catch (    Exception e) {
      LOG.warn(""String_Node_Str"");
      try {
        TimeUnit.SECONDS.sleep(1);
      }
 catch (      InterruptedException ie) {
        Thread.currentThread().interrupt();
        break;
      }
    }
  }
  return metaTable;
}","private MetricsConsumerMetaTable getMetaTable(){
  while (metaTable == null) {
    if (stopping) {
      LOG.info(""String_Node_Str"");
      break;
    }
    try {
      metaTable=metricDatasetFactory.createConsumerMeta();
    }
 catch (    ServiceUnavailableException e) {
    }
catch (    Exception e) {
      LOG.warn(""String_Node_Str"");
      try {
        TimeUnit.SECONDS.sleep(1);
      }
 catch (      InterruptedException ie) {
        Thread.currentThread().interrupt();
        break;
      }
    }
  }
  return metaTable;
}"
5249,"private MetricsConsumerMetaTable getMetaTable(){
  while (metaTable == null) {
    if (stopping) {
      LOG.info(""String_Node_Str"");
      break;
    }
    try {
      metaTable=metricDatasetFactory.createConsumerMeta();
    }
 catch (    Exception e) {
      LOG.warn(""String_Node_Str"");
      try {
        TimeUnit.SECONDS.sleep(1);
      }
 catch (      InterruptedException ie) {
        Thread.currentThread().interrupt();
        break;
      }
    }
  }
  return metaTable;
}","private MetricsConsumerMetaTable getMetaTable(){
  while (metaTable == null) {
    if (stopping) {
      LOG.info(""String_Node_Str"");
      break;
    }
    try {
      metaTable=metricDatasetFactory.createConsumerMeta();
    }
 catch (    ServiceUnavailableException e) {
    }
catch (    Exception e) {
      LOG.warn(""String_Node_Str"");
      try {
        TimeUnit.SECONDS.sleep(1);
      }
 catch (      InterruptedException ie) {
        Thread.currentThread().interrupt();
        break;
      }
    }
  }
  return metaTable;
}"
5250,"private DefaultMetricDatasetFactory(DatasetFramework namespacedDsFramework,final CConfiguration cConf){
  this.cConf=cConf;
  this.dsFramework=namespacedDsFramework;
  this.entityTable=Suppliers.memoize(new Supplier<EntityTable>(){
    @Override public EntityTable get(){
      String tableName=cConf.get(Constants.Metrics.ENTITY_TABLE_NAME,Constants.Metrics.DEFAULT_ENTITY_TABLE_NAME);
      return new EntityTable(getOrCreateMetricsTable(tableName,DatasetProperties.EMPTY));
    }
  }
);
}","@Inject public DefaultMetricDatasetFactory(final CConfiguration cConf,DatasetFramework dsFramework){
  this.cConf=cConf;
  this.dsFramework=dsFramework;
  this.entityTable=Suppliers.memoize(new Supplier<EntityTable>(){
    @Override public EntityTable get(){
      String tableName=cConf.get(Constants.Metrics.ENTITY_TABLE_NAME,Constants.Metrics.DEFAULT_ENTITY_TABLE_NAME);
      return new EntityTable(getOrCreateMetricsTable(tableName,DatasetProperties.EMPTY));
    }
  }
);
}"
5251,"@Override public FactTable getOrCreateFactTable(int resolution){
  String tableName=cConf.get(Constants.Metrics.METRICS_TABLE_PREFIX,Constants.Metrics.DEFAULT_METRIC_TABLE_PREFIX) + ""String_Node_Str"" + resolution;
  int ttl=cConf.getInt(Constants.Metrics.RETENTION_SECONDS + ""String_Node_Str"" + resolution+ ""String_Node_Str"",-1);
  TableProperties.Builder props=TableProperties.builder();
  if (ttl > 0 && resolution != Integer.MAX_VALUE) {
    props.setTTL(ttl);
  }
  props.setReadlessIncrementSupport(true);
  props.add(HBaseTableAdmin.PROPERTY_SPLITS,GSON.toJson(FactTable.getSplits(DefaultMetricStore.AGGREGATIONS.size())));
  MetricsTable table=getOrCreateMetricsTable(tableName,props.build());
  LOG.info(""String_Node_Str"",tableName);
  return new FactTable(table,entityTable.get(),resolution,getRollTime(resolution));
}","@Override public FactTable getOrCreateFactTable(int resolution){
  String tableName=cConf.get(Constants.Metrics.METRICS_TABLE_PREFIX,Constants.Metrics.DEFAULT_METRIC_TABLE_PREFIX) + ""String_Node_Str"" + resolution;
  int ttl=cConf.getInt(Constants.Metrics.RETENTION_SECONDS + ""String_Node_Str"" + resolution+ ""String_Node_Str"",-1);
  TableProperties.Builder props=TableProperties.builder();
  if (ttl > 0 && resolution != Integer.MAX_VALUE) {
    props.setTTL(ttl);
  }
  props.setReadlessIncrementSupport(true);
  props.add(HBaseTableAdmin.PROPERTY_SPLITS,GSON.toJson(FactTable.getSplits(DefaultMetricStore.AGGREGATIONS.size())));
  MetricsTable table=getOrCreateMetricsTable(tableName,props.build());
  LOG.debug(""String_Node_Str"",tableName);
  return new FactTable(table,entityTable.get(),resolution,getRollTime(resolution));
}"
5252,"private MetricsTable getOrCreateMetricsTable(String tableName,DatasetProperties props){
  MetricsTable table=null;
  DatasetId metricsDatasetInstanceId=NamespaceId.SYSTEM.dataset(tableName);
  while (table == null) {
    try {
      table=DatasetsUtil.getOrCreateDataset(dsFramework,metricsDatasetInstanceId,MetricsTable.class.getName(),props,null,null);
    }
 catch (    DatasetManagementException|ServiceUnavailableException e) {
      LOG.warn(""String_Node_Str"",tableName);
      try {
        TimeUnit.SECONDS.sleep(1);
      }
 catch (      InterruptedException ie) {
        Thread.currentThread().interrupt();
        break;
      }
    }
catch (    IOException e) {
      LOG.error(""String_Node_Str"",tableName,e);
      throw Throwables.propagate(e);
    }
  }
  return table;
}","private MetricsTable getOrCreateMetricsTable(String tableName,DatasetProperties props){
  DatasetId metricsDatasetInstanceId=NamespaceId.SYSTEM.dataset(tableName);
  MetricsTable table=null;
  try {
    table=DatasetsUtil.getOrCreateDataset(dsFramework,metricsDatasetInstanceId,MetricsTable.class.getName(),props,null,null);
  }
 catch (  Exception e) {
    Throwables.propagate(e);
  }
  return table;
}"
5253,"@Override public MetricsConsumerMetaTable createConsumerMeta(){
  String tableName=cConf.get(Constants.Metrics.KAFKA_META_TABLE);
  try {
    MetricsTable table=getOrCreateMetricsTable(tableName,DatasetProperties.EMPTY);
    LOG.info(""String_Node_Str"",tableName);
    return new MetricsConsumerMetaTable(table);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}","@Override public MetricsConsumerMetaTable createConsumerMeta(){
  String tableName=cConf.get(Constants.Metrics.KAFKA_META_TABLE);
  MetricsTable table=getOrCreateMetricsTable(tableName,DatasetProperties.EMPTY);
  LOG.debug(""String_Node_Str"",tableName);
  return new MetricsConsumerMetaTable(table);
}"
5254,"public static TableDescriptor getTableDescriptor(HTableDescriptor descriptor){
  Set<ColumnFamilyDescriptor> families=new HashSet<>();
  for (  HColumnDescriptor family : descriptor.getColumnFamilies()) {
    families.add(getColumnFamilyDescriptor(family));
  }
  Set<CoprocessorDescriptor> coprocessors=new HashSet<>();
  coprocessors.addAll(CoprocessorUtil.getCoprocessors(descriptor).values());
  Map<String,String> properties=new HashMap<>();
  for (  Map.Entry<ImmutableBytesWritable,ImmutableBytesWritable> value : descriptor.getValues().entrySet()) {
    properties.put(org.apache.hadoop.hbase.util.Bytes.toString(value.getKey().get()),org.apache.hadoop.hbase.util.Bytes.toString(value.getValue().get()));
  }
  return new TableDescriptor(descriptor.getTableName().getNamespaceAsString(),descriptor.getTableName().getQualifierAsString(),families,coprocessors,properties);
}","public static TableDescriptor getTableDescriptor(HTableDescriptor descriptor){
  Set<ColumnFamilyDescriptor> families=new HashSet<>();
  for (  HColumnDescriptor family : descriptor.getColumnFamilies()) {
    families.add(getColumnFamilyDescriptor(family));
  }
  Set<CoprocessorDescriptor> coprocessors=new HashSet<>();
  coprocessors.addAll(CoprocessorUtil.getCoprocessors(descriptor).values());
  Map<String,String> properties=CoprocessorUtil.getNonCoprocessorProperties(descriptor);
  return new TableDescriptor(descriptor.getTableName().getNamespaceAsString(),descriptor.getTableName().getQualifierAsString(),families,coprocessors,properties);
}"
5255,"public static TableDescriptor getTableDescriptor(HTableDescriptor descriptor){
  Set<ColumnFamilyDescriptor> families=new HashSet<>();
  for (  HColumnDescriptor family : descriptor.getColumnFamilies()) {
    families.add(getColumnFamilyDescriptor(family));
  }
  Set<CoprocessorDescriptor> coprocessors=new HashSet<>();
  coprocessors.addAll(CoprocessorUtil.getCoprocessors(descriptor).values());
  Map<String,String> properties=new HashMap<>();
  for (  Map.Entry<ImmutableBytesWritable,ImmutableBytesWritable> value : descriptor.getValues().entrySet()) {
    properties.put(org.apache.hadoop.hbase.util.Bytes.toString(value.getKey().get()),org.apache.hadoop.hbase.util.Bytes.toString(value.getValue().get()));
  }
  return new TableDescriptor(descriptor.getTableName().getNamespaceAsString(),descriptor.getTableName().getQualifierAsString(),families,coprocessors,properties);
}","public static TableDescriptor getTableDescriptor(HTableDescriptor descriptor){
  Set<ColumnFamilyDescriptor> families=new HashSet<>();
  for (  HColumnDescriptor family : descriptor.getColumnFamilies()) {
    families.add(getColumnFamilyDescriptor(family));
  }
  Set<CoprocessorDescriptor> coprocessors=new HashSet<>();
  coprocessors.addAll(CoprocessorUtil.getCoprocessors(descriptor).values());
  Map<String,String> properties=CoprocessorUtil.getNonCoprocessorProperties(descriptor);
  return new TableDescriptor(descriptor.getTableName().getNamespaceAsString(),descriptor.getTableName().getQualifierAsString(),families,coprocessors,properties);
}"
5256,"public static TableDescriptor getTableDescriptor(HTableDescriptor descriptor){
  Set<ColumnFamilyDescriptor> families=new HashSet<>();
  for (  HColumnDescriptor family : descriptor.getColumnFamilies()) {
    families.add(getColumnFamilyDescriptor(family));
  }
  Set<CoprocessorDescriptor> coprocessors=new HashSet<>();
  coprocessors.addAll(CoprocessorUtil.getCoprocessors(descriptor).values());
  Map<String,String> properties=new HashMap<>();
  for (  Map.Entry<ImmutableBytesWritable,ImmutableBytesWritable> value : descriptor.getValues().entrySet()) {
    properties.put(org.apache.hadoop.hbase.util.Bytes.toString(value.getKey().get()),org.apache.hadoop.hbase.util.Bytes.toString(value.getValue().get()));
  }
  return new TableDescriptor(descriptor.getTableName().getNamespaceAsString(),descriptor.getTableName().getQualifierAsString(),families,coprocessors,properties);
}","public static TableDescriptor getTableDescriptor(HTableDescriptor descriptor){
  Set<ColumnFamilyDescriptor> families=new HashSet<>();
  for (  HColumnDescriptor family : descriptor.getColumnFamilies()) {
    families.add(getColumnFamilyDescriptor(family));
  }
  Set<CoprocessorDescriptor> coprocessors=new HashSet<>();
  coprocessors.addAll(CoprocessorUtil.getCoprocessors(descriptor).values());
  Map<String,String> properties=CoprocessorUtil.getNonCoprocessorProperties(descriptor);
  return new TableDescriptor(descriptor.getTableName().getNamespaceAsString(),descriptor.getTableName().getQualifierAsString(),families,coprocessors,properties);
}"
5257,"public static TableDescriptor getTableDescriptor(HTableDescriptor descriptor){
  Set<ColumnFamilyDescriptor> families=new HashSet<>();
  for (  HColumnDescriptor family : descriptor.getColumnFamilies()) {
    families.add(getColumnFamilyDescriptor(family));
  }
  Set<CoprocessorDescriptor> coprocessors=new HashSet<>();
  coprocessors.addAll(CoprocessorUtil.getCoprocessors(descriptor).values());
  Map<String,String> properties=new HashMap<>();
  for (  Map.Entry<ImmutableBytesWritable,ImmutableBytesWritable> value : descriptor.getValues().entrySet()) {
    properties.put(org.apache.hadoop.hbase.util.Bytes.toString(value.getKey().get()),org.apache.hadoop.hbase.util.Bytes.toString(value.getValue().get()));
  }
  return new TableDescriptor(descriptor.getTableName().getNamespaceAsString(),descriptor.getTableName().getQualifierAsString(),families,coprocessors,properties);
}","public static TableDescriptor getTableDescriptor(HTableDescriptor descriptor){
  Set<ColumnFamilyDescriptor> families=new HashSet<>();
  for (  HColumnDescriptor family : descriptor.getColumnFamilies()) {
    families.add(getColumnFamilyDescriptor(family));
  }
  Set<CoprocessorDescriptor> coprocessors=new HashSet<>();
  coprocessors.addAll(CoprocessorUtil.getCoprocessors(descriptor).values());
  Map<String,String> properties=CoprocessorUtil.getNonCoprocessorProperties(descriptor);
  return new TableDescriptor(descriptor.getTableName().getNamespaceAsString(),descriptor.getTableName().getQualifierAsString(),families,coprocessors,properties);
}"
5258,"public static TableDescriptor getTableDescriptor(HTableDescriptor descriptor){
  Set<ColumnFamilyDescriptor> families=new HashSet<>();
  for (  HColumnDescriptor family : descriptor.getColumnFamilies()) {
    families.add(getColumnFamilyDescriptor(family));
  }
  Set<CoprocessorDescriptor> coprocessors=new HashSet<>();
  coprocessors.addAll(CoprocessorUtil.getCoprocessors(descriptor).values());
  Map<String,String> properties=new HashMap<>();
  for (  Map.Entry<ImmutableBytesWritable,ImmutableBytesWritable> value : descriptor.getValues().entrySet()) {
    properties.put(org.apache.hadoop.hbase.util.Bytes.toString(value.getKey().get()),org.apache.hadoop.hbase.util.Bytes.toString(value.getValue().get()));
  }
  return new TableDescriptor(descriptor.getTableName().getNamespaceAsString(),descriptor.getTableName().getQualifierAsString(),families,coprocessors,properties);
}","public static TableDescriptor getTableDescriptor(HTableDescriptor descriptor){
  Set<ColumnFamilyDescriptor> families=new HashSet<>();
  for (  HColumnDescriptor family : descriptor.getColumnFamilies()) {
    families.add(getColumnFamilyDescriptor(family));
  }
  Set<CoprocessorDescriptor> coprocessors=new HashSet<>();
  coprocessors.addAll(CoprocessorUtil.getCoprocessors(descriptor).values());
  Map<String,String> properties=CoprocessorUtil.getNonCoprocessorProperties(descriptor);
  return new TableDescriptor(descriptor.getTableName().getNamespaceAsString(),descriptor.getTableName().getQualifierAsString(),families,coprocessors,properties);
}"
5259,"public static TableDescriptor getTableDescriptor(HTableDescriptor descriptor){
  Set<ColumnFamilyDescriptor> families=new HashSet<>();
  for (  HColumnDescriptor family : descriptor.getColumnFamilies()) {
    families.add(getColumnFamilyDescriptor(family));
  }
  Set<CoprocessorDescriptor> coprocessors=new HashSet<>();
  coprocessors.addAll(CoprocessorUtil.getCoprocessors(descriptor).values());
  Map<String,String> properties=new HashMap<>();
  for (  Map.Entry<ImmutableBytesWritable,ImmutableBytesWritable> value : descriptor.getValues().entrySet()) {
    properties.put(org.apache.hadoop.hbase.util.Bytes.toString(value.getKey().get()),org.apache.hadoop.hbase.util.Bytes.toString(value.getValue().get()));
  }
  return new TableDescriptor(descriptor.getTableName().getNamespaceAsString(),descriptor.getTableName().getQualifierAsString(),families,coprocessors,properties);
}","public static TableDescriptor getTableDescriptor(HTableDescriptor descriptor){
  Set<ColumnFamilyDescriptor> families=new HashSet<>();
  for (  HColumnDescriptor family : descriptor.getColumnFamilies()) {
    families.add(getColumnFamilyDescriptor(family));
  }
  Set<CoprocessorDescriptor> coprocessors=new HashSet<>();
  coprocessors.addAll(CoprocessorUtil.getCoprocessors(descriptor).values());
  Map<String,String> properties=CoprocessorUtil.getNonCoprocessorProperties(descriptor);
  return new TableDescriptor(descriptor.getTableName().getNamespaceAsString(),descriptor.getTableName().getQualifierAsString(),families,coprocessors,properties);
}"
5260,"public static TableDescriptor getTableDescriptor(HTableDescriptor descriptor){
  Set<ColumnFamilyDescriptor> families=new HashSet<>();
  for (  HColumnDescriptor family : descriptor.getColumnFamilies()) {
    families.add(getColumnFamilyDescriptor(family));
  }
  Set<CoprocessorDescriptor> coprocessors=new HashSet<>();
  coprocessors.addAll(CoprocessorUtil.getCoprocessors(descriptor).values());
  Map<String,String> properties=new HashMap<>();
  for (  Map.Entry<ImmutableBytesWritable,ImmutableBytesWritable> value : descriptor.getValues().entrySet()) {
    properties.put(org.apache.hadoop.hbase.util.Bytes.toString(value.getKey().get()),org.apache.hadoop.hbase.util.Bytes.toString(value.getValue().get()));
  }
  return new TableDescriptor(descriptor.getTableName().getNamespaceAsString(),descriptor.getTableName().getQualifierAsString(),families,coprocessors,properties);
}","public static TableDescriptor getTableDescriptor(HTableDescriptor descriptor){
  Set<ColumnFamilyDescriptor> families=new HashSet<>();
  for (  HColumnDescriptor family : descriptor.getColumnFamilies()) {
    families.add(getColumnFamilyDescriptor(family));
  }
  Set<CoprocessorDescriptor> coprocessors=new HashSet<>();
  coprocessors.addAll(CoprocessorUtil.getCoprocessors(descriptor).values());
  Map<String,String> properties=CoprocessorUtil.getNonCoprocessorProperties(descriptor);
  return new TableDescriptor(descriptor.getTableName().getNamespaceAsString(),descriptor.getTableName().getQualifierAsString(),families,coprocessors,properties);
}"
5261,"@Inject public HBaseQueueClientFactory(CConfiguration cConf,Configuration hConf,HBaseTableUtil hBaseTableUtil,QueueAdmin queueAdmin,TransactionExecutorFactory txExecutorFactory){
  this.cConf=cConf;
  this.hConf=hConf;
  this.queueAdmin=(HBaseQueueAdmin)queueAdmin;
  this.queueUtil=new HBaseQueueUtilFactory().get();
  this.hBaseTableUtil=hBaseTableUtil;
  this.txExecutorFactory=txExecutorFactory;
}","@Inject public HBaseQueueClientFactory(CConfiguration cConf,Configuration hConf,HBaseTableUtil hBaseTableUtil,QueueAdmin queueAdmin,TransactionExecutorFactory txExecutorFactory){
  this.cConf=cConf;
  this.hConf=hConf;
  this.queueAdmin=(HBaseQueueAdmin)queueAdmin;
  this.queueUtil=new HBaseQueueUtilFactory().get();
  this.hBaseTableUtil=hBaseTableUtil;
  this.txExecutorFactory=txExecutorFactory;
  this.txMaxLifeTimeInMillis=TimeUnit.SECONDS.toMillis(cConf.getLong(Constants.Tephra.CFG_TX_MAX_LIFETIME,Constants.Tephra.DEFAULT_TX_MAX_LIFETIME));
}"
5262,"private HBaseQueueProducer createProducer(HTable hTable,QueueName queueName,QueueMetrics queueMetrics,HBaseQueueStrategy queueStrategy,Iterable<? extends ConsumerGroupConfig> groupConfigs) throws IOException {
  return new HBaseQueueProducer(hTable,queueName,queueMetrics,queueStrategy,groupConfigs);
}","private HBaseQueueProducer createProducer(HTable hTable,QueueName queueName,QueueMetrics queueMetrics,HBaseQueueStrategy queueStrategy,Iterable<? extends ConsumerGroupConfig> groupConfigs) throws IOException {
  return new HBaseQueueProducer(hTable,queueName,queueMetrics,queueStrategy,groupConfigs,txMaxLifeTimeInMillis);
}"
5263,"public HBaseQueueProducer(HTable hTable,QueueName queueName,QueueMetrics queueMetrics,HBaseQueueStrategy queueStrategy,Iterable<? extends ConsumerGroupConfig> consumerGroupConfigs){
  super(queueMetrics,queueName);
  this.queueStrategy=queueStrategy;
  this.consumerGroupConfigs=ImmutableList.copyOf(Iterables.filter(consumerGroupConfigs,new Predicate<ConsumerGroupConfig>(){
    private final Set<Long> seenGroups=Sets.newHashSet();
    @Override public boolean apply(    ConsumerGroupConfig config){
      return seenGroups.add(config.getGroupId());
    }
  }
));
  this.queueRowPrefix=QueueEntryRow.getQueueRowPrefix(queueName);
  this.rollbackKeys=Lists.newArrayList();
  this.hTable=hTable;
}","public HBaseQueueProducer(HTable hTable,QueueName queueName,QueueMetrics queueMetrics,HBaseQueueStrategy queueStrategy,Iterable<? extends ConsumerGroupConfig> consumerGroupConfigs,long txMaxLifeTimeInMillis){
  super(queueMetrics,queueName);
  this.queueStrategy=queueStrategy;
  this.consumerGroupConfigs=ImmutableList.copyOf(Iterables.filter(consumerGroupConfigs,new Predicate<ConsumerGroupConfig>(){
    private final Set<Long> seenGroups=Sets.newHashSet();
    @Override public boolean apply(    ConsumerGroupConfig config){
      return seenGroups.add(config.getGroupId());
    }
  }
));
  this.queueRowPrefix=QueueEntryRow.getQueueRowPrefix(queueName);
  this.rollbackKeys=Lists.newArrayList();
  this.hTable=hTable;
  this.txMaxLifeTimeInMillis=txMaxLifeTimeInMillis;
}"
5264,"/** 
 * Persist queue entries into HBase.
 */
protected int persist(Iterable<QueueEntry> entries,Transaction transaction) throws IOException {
  int count=0;
  List<Put> puts=Lists.newArrayList();
  int bytes=0;
  List<byte[]> rowKeys=Lists.newArrayList();
  long writePointer=transaction.getWritePointer();
  for (  QueueEntry entry : entries) {
    rowKeys.clear();
    queueStrategy.getRowKeys(consumerGroupConfigs,entry,queueRowPrefix,writePointer,count,rowKeys);
    rollbackKeys.addAll(rowKeys);
    byte[] metaData=QueueEntry.serializeHashKeys(entry.getHashKeys());
    for (    byte[] rowKey : rowKeys) {
      Put put=new Put(rowKey);
      put.add(QueueEntryRow.COLUMN_FAMILY,QueueEntryRow.DATA_COLUMN,entry.getData());
      put.add(QueueEntryRow.COLUMN_FAMILY,QueueEntryRow.META_COLUMN,metaData);
      puts.add(put);
      bytes+=entry.getData().length;
    }
    count++;
  }
  hTable.put(puts);
  hTable.flushCommits();
  return bytes;
}","/** 
 * Persist queue entries into HBase.
 */
protected int persist(Iterable<QueueEntry> entries,Transaction transaction) throws IOException {
  int count=0;
  List<Put> puts=Lists.newArrayList();
  int bytes=0;
  List<byte[]> rowKeys=Lists.newArrayList();
  long writePointer=transaction.getWritePointer();
  ensureValidTxLifetime(writePointer);
  for (  QueueEntry entry : entries) {
    rowKeys.clear();
    queueStrategy.getRowKeys(consumerGroupConfigs,entry,queueRowPrefix,writePointer,count,rowKeys);
    rollbackKeys.addAll(rowKeys);
    byte[] metaData=QueueEntry.serializeHashKeys(entry.getHashKeys());
    for (    byte[] rowKey : rowKeys) {
      Put put=new Put(rowKey);
      put.add(QueueEntryRow.COLUMN_FAMILY,QueueEntryRow.DATA_COLUMN,entry.getData());
      put.add(QueueEntryRow.COLUMN_FAMILY,QueueEntryRow.META_COLUMN,metaData);
      puts.add(put);
      bytes+=entry.getData().length;
    }
    count++;
  }
  hTable.put(puts);
  hTable.flushCommits();
  return bytes;
}"
5265,"@VisibleForTesting CoreMessagingService(CConfiguration cConf,TableFactory tableFactory,TimeProvider timeProvider,MetricsCollectionService metricsCollectionService){
  this.cConf=cConf;
  this.tableFactory=tableFactory;
  this.topicCache=createTopicCache();
  this.messageTableWriterCache=createTableWriterCache(true,cConf);
  this.payloadTableWriterCache=createTableWriterCache(false,cConf);
  this.timeProvider=timeProvider;
  this.metricsCollectionService=metricsCollectionService;
}","@VisibleForTesting CoreMessagingService(CConfiguration cConf,TableFactory tableFactory,TimeProvider timeProvider,MetricsCollectionService metricsCollectionService){
  this.cConf=cConf;
  this.tableFactory=tableFactory;
  this.topicCache=createTopicCache();
  this.messageTableWriterCache=createTableWriterCache(true,cConf);
  this.payloadTableWriterCache=createTableWriterCache(false,cConf);
  this.timeProvider=timeProvider;
  this.metricsCollectionService=metricsCollectionService;
  this.txMaxLifeTimeInMillis=TimeUnit.SECONDS.toMillis(cConf.getLong(Constants.Tephra.CFG_TX_MAX_LIFETIME,Constants.Tephra.DEFAULT_TX_MAX_LIFETIME));
}"
5266,"@Nullable @Override public RollbackDetail publish(StoreRequest request) throws TopicNotFoundException, IOException {
  try {
    TopicMetadata metadata=topicCache.get(request.getTopicId());
    return messageTableWriterCache.get(request.getTopicId()).persist(request,metadata);
  }
 catch (  ExecutionException e) {
    Throwable cause=Objects.firstNonNull(e.getCause(),e);
    Throwables.propagateIfPossible(cause,TopicNotFoundException.class,IOException.class);
    throw Throwables.propagate(e);
  }
}","@Nullable @Override public RollbackDetail publish(StoreRequest request) throws TopicNotFoundException, IOException {
  try {
    TopicMetadata metadata=topicCache.get(request.getTopicId());
    if (request.isTransactional()) {
      ensureValidTxLifetime(request.getTransactionWritePointer());
    }
    return messageTableWriterCache.get(request.getTopicId()).persist(request,metadata);
  }
 catch (  ExecutionException e) {
    Throwable cause=Objects.firstNonNull(e.getCause(),e);
    Throwables.propagateIfPossible(cause,TopicNotFoundException.class,IOException.class);
    throw Throwables.propagate(e);
  }
}"
5267,"@BeforeClass public static void init() throws IOException {
  cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TEMP_FOLDER.newFolder().getAbsolutePath());
  cConf.setInt(Constants.MessagingSystem.HTTP_SERVER_CONSUME_CHUNK_SIZE,128);
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new DiscoveryRuntimeModule().getInMemoryModules(),new MessagingServerRuntimeModule().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).toInstance(new NoOpMetricsCollectionService());
    }
  }
);
  httpService=injector.getInstance(MessagingHttpService.class);
  httpService.startAndWait();
  client=new ClientMessagingService(injector.getInstance(DiscoveryServiceClient.class));
}","@BeforeClass public static void init() throws IOException {
  cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TEMP_FOLDER.newFolder().getAbsolutePath());
  cConf.setInt(Constants.MessagingSystem.HTTP_SERVER_CONSUME_CHUNK_SIZE,128);
  cConf.setLong(Constants.Tephra.CFG_TX_MAX_LIFETIME,10000000000L);
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new DiscoveryRuntimeModule().getInMemoryModules(),new MessagingServerRuntimeModule().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).toInstance(new NoOpMetricsCollectionService());
    }
  }
);
  httpService=injector.getInstance(MessagingHttpService.class);
  httpService.startAndWait();
  client=new ClientMessagingService(injector.getInstance(DiscoveryServiceClient.class));
}"
5268,"@Override public void flush() throws IOException {
  out.flush();
}","@Override public void flush() throws IOException {
  if (out instanceof org.apache.hadoop.fs.Syncable) {
    ((org.apache.hadoop.fs.Syncable)out).hflush();
  }
 else {
    out.flush();
  }
}"
5269,"@Override public void sync() throws IOException {
  if (out instanceof Syncable) {
    ((org.apache.hadoop.fs.Syncable)out).hsync();
  }
}","@Override public void sync() throws IOException {
  if (out instanceof org.apache.hadoop.fs.Syncable) {
    ((org.apache.hadoop.fs.Syncable)out).hsync();
  }
 else {
    out.flush();
  }
}"
5270,"private HiveConf getHiveConf(){
  HiveConf conf=new HiveConf();
  if (UserGroupInformation.isSecurityEnabled()) {
    conf.set(HIVE_METASTORE_TOKEN_KEY,HiveAuthFactory.HS2_CLIENT_TOKEN);
  }
  String whiteListAppend=conf.getVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST_APPEND);
  if (whiteListAppend != null && !whiteListAppend.trim().isEmpty()) {
    whiteListAppend=whiteListAppend + ""String_Node_Str"" + PARAMS_EXPLORE_MODIFIES;
  }
 else {
    whiteListAppend=PARAMS_EXPLORE_MODIFIES;
  }
  conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST_APPEND,whiteListAppend);
  conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,UserGroupInformation.AuthenticationMethod.SIMPLE.name());
  conf.unset(""String_Node_Str"");
  conf.unset(""String_Node_Str"");
  return conf;
}","private HiveConf getHiveConf(){
  HiveConf conf=new HiveConf();
  if (UserGroupInformation.isSecurityEnabled()) {
    conf.set(HIVE_METASTORE_TOKEN_KEY,HiveAuthFactory.HS2_CLIENT_TOKEN);
  }
  String whiteListAppend=conf.get(Constants.Explore.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST_APPEND);
  if (whiteListAppend != null && !whiteListAppend.trim().isEmpty()) {
    whiteListAppend=whiteListAppend + ""String_Node_Str"" + PARAMS_EXPLORE_MODIFIES;
  }
 else {
    whiteListAppend=PARAMS_EXPLORE_MODIFIES;
  }
  conf.set(Constants.Explore.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST_APPEND,whiteListAppend);
  conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,UserGroupInformation.AuthenticationMethod.SIMPLE.name());
  conf.unset(Constants.Explore.HIVE_SERVER2_SPNEGO_KEYTAB);
  conf.unset(Constants.Explore.HIVE_SERVER2_SPNEGO_PRINCIPAL);
  return conf;
}"
5271,"private Map<String,String> doStartSession(@Nullable NamespaceId namespace,@Nullable Map<String,String> additionalSessionConf) throws IOException, ExploreException, NamespaceNotFoundException {
  Map<String,String> sessionConf=new HashMap<>();
  QueryHandle queryHandle=QueryHandle.generate();
  sessionConf.put(Constants.Explore.QUERY_ID,queryHandle.getHandle());
  String schedulerQueue=namespace != null ? schedulerQueueResolver.getQueue(namespace.toId()) : schedulerQueueResolver.getDefaultQueue();
  if (schedulerQueue != null && !schedulerQueue.isEmpty()) {
    sessionConf.put(JobContext.QUEUE_NAME,schedulerQueue);
  }
  Transaction tx=startTransaction();
  ConfigurationUtil.set(sessionConf,Constants.Explore.TX_QUERY_KEY,TxnCodec.INSTANCE,tx);
  ConfigurationUtil.set(sessionConf,Constants.Explore.CCONF_KEY,CConfCodec.INSTANCE,cConf);
  ConfigurationUtil.set(sessionConf,Constants.Explore.HCONF_KEY,HConfCodec.INSTANCE,hConf);
  HiveConf hiveConf=getHiveConf();
  if (ExploreServiceUtils.isSparkEngine(hiveConf,additionalSessionConf)) {
    sessionConf.putAll(sparkConf);
  }
  if (UserGroupInformation.isSecurityEnabled()) {
    sessionConf.put(""String_Node_Str"",""String_Node_Str"");
    sessionConf.put(""String_Node_Str"",""String_Node_Str"");
    File credentialsFile=writeCredentialsFile(queryHandle);
    String credentialsFilePath=credentialsFile.getAbsolutePath();
    sessionConf.put(MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY,credentialsFilePath);
    sessionConf.put(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,UserGroupInformation.AuthenticationMethod.KERBEROS.name());
    sessionConf.put(""String_Node_Str"",Boolean.FALSE.toString());
    sessionConf.put(""String_Node_Str"",Boolean.FALSE.toString());
    if (ExploreServiceUtils.isTezEngine(hiveConf,additionalSessionConf)) {
      sessionConf.put(""String_Node_Str"",credentialsFilePath);
    }
  }
  if (additionalSessionConf != null) {
    sessionConf.putAll(additionalSessionConf);
  }
  return sessionConf;
}","private Map<String,String> doStartSession(@Nullable NamespaceId namespace,@Nullable Map<String,String> additionalSessionConf) throws IOException, ExploreException, NamespaceNotFoundException {
  Map<String,String> sessionConf=new HashMap<>();
  QueryHandle queryHandle=QueryHandle.generate();
  sessionConf.put(Constants.Explore.QUERY_ID,queryHandle.getHandle());
  String schedulerQueue=namespace != null ? schedulerQueueResolver.getQueue(namespace.toId()) : schedulerQueueResolver.getDefaultQueue();
  if (schedulerQueue != null && !schedulerQueue.isEmpty()) {
    sessionConf.put(JobContext.QUEUE_NAME,schedulerQueue);
  }
  Transaction tx=startTransaction();
  ConfigurationUtil.set(sessionConf,Constants.Explore.TX_QUERY_KEY,TxnCodec.INSTANCE,tx);
  ConfigurationUtil.set(sessionConf,Constants.Explore.CCONF_KEY,CConfCodec.INSTANCE,cConf);
  ConfigurationUtil.set(sessionConf,Constants.Explore.HCONF_KEY,HConfCodec.INSTANCE,hConf);
  HiveConf hiveConf=getHiveConf();
  if (ExploreServiceUtils.isSparkEngine(hiveConf,additionalSessionConf)) {
    sessionConf.putAll(sparkConf);
  }
  if (UserGroupInformation.isSecurityEnabled()) {
    sessionConf.put(""String_Node_Str"",""String_Node_Str"");
    sessionConf.put(""String_Node_Str"",""String_Node_Str"");
    File credentialsFile=writeCredentialsFile(queryHandle);
    String credentialsFilePath=credentialsFile.getAbsolutePath();
    sessionConf.put(MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY,credentialsFilePath);
    sessionConf.put(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,UserGroupInformation.AuthenticationMethod.KERBEROS.name());
    sessionConf.put(Constants.Explore.SUBMITLOCALTASKVIACHILD,Boolean.FALSE.toString());
    sessionConf.put(Constants.Explore.SUBMITVIACHILD,Boolean.FALSE.toString());
    if (ExploreServiceUtils.isTezEngine(hiveConf,additionalSessionConf)) {
      sessionConf.put(""String_Node_Str"",credentialsFilePath);
    }
  }
  if (additionalSessionConf != null) {
    sessionConf.putAll(additionalSessionConf);
  }
  return sessionConf;
}"
5272,"@Override public void stop(){
}","/** 
 * By default, this method is a no-op. This method should be overridden to provide actual   {@code stop} functionality.
 */
@Override public void stop(){
}"
5273,"@Override @TransactionPolicy(TransactionControl.EXPLICIT) public void destroy(){
}","/** 
 * By default, this method is a no-op. This method should be overridden to provide actual   {@code destroy}functionality.
 */
@Override @TransactionPolicy(TransactionControl.EXPLICIT) public void destroy(){
}"
5274,"/** 
 * Request to stop the running worker. This method will be invoked from a different thread than the one calling the   {@link #run()} method.
 */
void stop();","/** 
 * Stop the   {@code Worker}. This method will be invoked whenever the worker is externally stopped by CDAP. This method will be invoked from a different thread than the one calling the   {@link #run()} method.
 */
void stop();"
5275,"/** 
 * Destroy the Worker. Note that unlike most program types, this method is not called within an implicit transaction, but instead it can start its own transactions using   {@link WorkerContext#execute(TxRunnable)}.
 */
@Override @TransactionPolicy(TransactionControl.EXPLICIT) void destroy();","/** 
 * Destroy the   {@code Worker}. Note that unlike most program types, this method is not called within an implicit transaction, but instead it can start its own transactions using   {@link WorkerContext#execute(TxRunnable)}.
 */
@Override @TransactionPolicy(TransactionControl.EXPLICIT) void destroy();"
5276,"/** 
 * Configure a Worker.
 */
void configure(WorkerConfigurer configurer);","/** 
 * Configure a   {@code Worker}.
 */
void configure(WorkerConfigurer configurer);"
5277,"/** 
 * Initialize the Worker. Note that unlike most program types, this method is not called within an implicit transaction, but instead it can start its own transactions using   {@link WorkerContext#execute(TxRunnable)}. methods.
 */
@Override @TransactionPolicy(TransactionControl.EXPLICIT) void initialize(WorkerContext context) throws Exception ;","/** 
 * Initialize the   {@code Worker}. Note that unlike most program types, this method is not called within an implicit transaction, but instead it can start its own transactions using   {@link WorkerContext#execute(TxRunnable)}. methods.
 */
@Override @TransactionPolicy(TransactionControl.EXPLICIT) void initialize(WorkerContext context) throws Exception ;"
5278,"/** 
 * @param principal The principal whose KeytabURI is being looked up
 * @param cConf To lookup the configured path for the keytabs
 * @return The location of the keytab
 * @throws IOException If the principal is not a valid kerberos principal
 */
static String getKeytabURIforPrincipal(String principal,CConfiguration cConf) throws IOException {
  String confPath=cConf.getRaw(Constants.Security.KEYTAB_PATH);
  String name=new KerberosName(principal).getShortName();
  return confPath.replace(Constants.USER_NAME_SPECIFIER,name);
}","/** 
 * @param principal The principal whose KeytabURI is being looked up
 * @param cConf To lookup the configured path for the keytabs
 * @return The location of the keytab
 * @throws IOException If the principal is not a valid kerberos principal
 */
static String getKeytabURIforPrincipal(String principal,CConfiguration cConf) throws IOException {
  String confPath=cConf.getRaw(Constants.Security.KEYTAB_PATH);
  Preconditions.checkNotNull(confPath,String.format(""String_Node_Str"" + ""String_Node_Str"",Constants.Security.KEYTAB_PATH));
  String name=new KerberosName(principal).getShortName();
  return confPath.replace(Constants.USER_NAME_SPECIFIER,name);
}"
5279,"@Test public void testPreferences() throws Exception {
  testPreferencesOutput(cli,""String_Node_Str"",ImmutableMap.<String,String>of());
  Map<String,String> propMap=Maps.newHashMap();
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  testPreferencesOutput(cli,""String_Node_Str"",propMap);
  testPreferencesOutput(cli,""String_Node_Str"",propMap);
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  propMap.clear();
  testPreferencesOutput(cli,""String_Node_Str"",propMap);
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,String.format(""String_Node_Str"",FakeApp.NAME,FakeFlow.NAME),""String_Node_Str"");
  testPreferencesOutput(cli,String.format(""String_Node_Str"",FakeApp.NAME,FakeFlow.NAME),propMap);
  testCommandOutputContains(cli,String.format(""String_Node_Str"",FakeApp.NAME,FakeFlow.NAME),""String_Node_Str"");
  propMap.clear();
  testPreferencesOutput(cli,String.format(""String_Node_Str"",FakeApp.NAME),propMap);
  testPreferencesOutput(cli,""String_Node_Str"",propMap);
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  File file=new File(TMP_FOLDER.newFolder(),""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + file.getParentFile().getAbsolutePath() + ""String_Node_Str"",""String_Node_Str"");
  BufferedWriter writer=Files.newWriter(file,Charsets.UTF_8);
  try {
    writer.write(""String_Node_Str"");
  }
  finally {
    writer.close();
  }
  testCommandOutputContains(cli,""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"",""String_Node_Str"");
  propMap.clear();
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  testPreferencesOutput(cli,""String_Node_Str"",propMap);
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  file=new File(TMP_FOLDER.newFolder(),""String_Node_Str"");
  writer=Files.newWriter(file,Charsets.UTF_8);
  try {
    writer.write(""String_Node_Str"");
  }
  finally {
    writer.close();
  }
  testCommandOutputContains(cli,""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
}","@Test public void testPreferences() throws Exception {
  testPreferencesOutput(cli,""String_Node_Str"",ImmutableMap.<String,String>of());
  Map<String,String> propMap=Maps.newHashMap();
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  testPreferencesOutput(cli,""String_Node_Str"",propMap);
  testPreferencesOutput(cli,""String_Node_Str"",propMap);
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  propMap.clear();
  testPreferencesOutput(cli,""String_Node_Str"",propMap);
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,String.format(""String_Node_Str"",FakeApp.NAME,FakeFlow.NAME),""String_Node_Str"");
  testPreferencesOutput(cli,String.format(""String_Node_Str"",FakeApp.NAME,FakeFlow.NAME),propMap);
  testCommandOutputContains(cli,String.format(""String_Node_Str"",FakeApp.NAME,FakeFlow.NAME),""String_Node_Str"");
  propMap.clear();
  testPreferencesOutput(cli,String.format(""String_Node_Str"",FakeApp.NAME),propMap);
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  testPreferencesOutput(cli,""String_Node_Str"",propMap);
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  File file=new File(TMP_FOLDER.newFolder(),""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + file.getParentFile().getAbsolutePath() + ""String_Node_Str"",""String_Node_Str"");
  BufferedWriter writer=Files.newWriter(file,Charsets.UTF_8);
  try {
    writer.write(""String_Node_Str"");
  }
  finally {
    writer.close();
  }
  testCommandOutputContains(cli,""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"",""String_Node_Str"");
  propMap.clear();
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  testPreferencesOutput(cli,""String_Node_Str"",propMap);
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  file=new File(TMP_FOLDER.newFolder(),""String_Node_Str"");
  writer=Files.newWriter(file,Charsets.UTF_8);
  try {
    writer.write(""String_Node_Str"");
  }
  finally {
    writer.close();
  }
  testCommandOutputContains(cli,""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
}"
5280,"protected void setPreferences(Arguments arguments,PrintStream printStream,Map<String,String> args) throws Exception {
  String[] programIdParts=new String[0];
  if (arguments.hasArgument(type.getArgumentName().toString())) {
    programIdParts=arguments.get(type.getArgumentName().toString()).split(""String_Node_Str"");
  }
switch (type) {
case INSTANCE:
    checkInputLength(programIdParts,0);
  client.setInstancePreferences(args);
printSuccessMessage(printStream,type);
break;
case NAMESPACE:
checkInputLength(programIdParts,0);
client.setNamespacePreferences(cliConfig.getCurrentNamespace(),args);
printSuccessMessage(printStream,type);
break;
case APP:
client.setApplicationPreferences(parseApplicationId(arguments),args);
printSuccessMessage(printStream,type);
break;
case FLOW:
case MAPREDUCE:
case WORKFLOW:
case SERVICE:
case SPARK:
client.setProgramPreferences(parseProgramId(arguments,type),args);
printSuccessMessage(printStream,type);
break;
default :
throw new IllegalArgumentException(""String_Node_Str"" + type.getShortName());
}
}","protected void setPreferences(Arguments arguments,PrintStream printStream,Map<String,String> args,String[] programIdParts) throws Exception {
switch (type) {
case INSTANCE:
    checkInputLength(programIdParts,0);
  client.setInstancePreferences(args);
printSuccessMessage(printStream,type);
break;
case NAMESPACE:
checkInputLength(programIdParts,0);
client.setNamespacePreferences(cliConfig.getCurrentNamespace(),args);
printSuccessMessage(printStream,type);
break;
case APP:
client.setApplicationPreferences(parseApplicationId(arguments),args);
printSuccessMessage(printStream,type);
break;
case FLOW:
case MAPREDUCE:
case WORKFLOW:
case SERVICE:
case SPARK:
client.setProgramPreferences(parseProgramId(arguments,type),args);
printSuccessMessage(printStream,type);
break;
default :
throw new IllegalArgumentException(""String_Node_Str"" + type.getShortName());
}
}"
5281,"protected String determinePattern(String action){
switch (action) {
case ""String_Node_Str"":
    return determinePatternSetHelper();
case ""String_Node_Str"":
  return determinePatternLoadHelper();
}
throw new RuntimeException(""String_Node_Str"" + type.getShortName());
}","protected String determinePattern(String action){
switch (action) {
case ""String_Node_Str"":
    return determineSetPatternHelper();
case ""String_Node_Str"":
  return determineLoadPatternHelper();
}
throw new RuntimeException(""String_Node_Str"" + action);
}"
5282,"private static UserGroupInformation getUgiForDataset(Impersonator impersonator,DatasetId datasetInstanceId) throws IOException, NamespaceNotFoundException {
  UserGroupInformation ugi;
  if (NamespaceId.SYSTEM.equals(datasetInstanceId.getParent())) {
    ugi=UserGroupInformation.getCurrentUser();
  }
 else {
    ugi=impersonator.getUGI(datasetInstanceId);
  }
  LOG.debug(""String_Node_Str"",ugi.getUserName(),datasetInstanceId);
  return ugi;
}","private static UserGroupInformation getUgiForDataset(Impersonator impersonator,DatasetId datasetInstanceId) throws IOException {
  UserGroupInformation ugi;
  if (NamespaceId.SYSTEM.equals(datasetInstanceId.getParent())) {
    ugi=UserGroupInformation.getCurrentUser();
  }
 else {
    ugi=impersonator.getUGI(datasetInstanceId);
  }
  LOG.debug(""String_Node_Str"",ugi.getUserName(),datasetInstanceId);
  return ugi;
}"
5283,"private UserGroupInformation getUGI(NamespacedEntityId entityId,ImpersonatedOpType impersonatedOpType) throws IOException {
  if (!kerberosEnabled || NamespaceId.SYSTEM.equals(entityId.getNamespaceId())) {
    return UserGroupInformation.getCurrentUser();
  }
  return ugiProvider.getConfiguredUGI(new ImpersonationRequest(entityId,impersonatedOpType)).getUGI();
}","private UserGroupInformation getUGI(NamespacedEntityId entityId,ImpersonatedOpType impersonatedOpType) throws IOException {
  if (!kerberosEnabled || NamespaceId.SYSTEM.equals(entityId.getNamespaceId())) {
    return UserGroupInformation.getCurrentUser();
  }
  ImpersonationRequest impersonationRequest=new ImpersonationRequest(entityId,impersonatedOpType);
  if (!UserGroupInformation.getCurrentUser().getShortUserName().equals(masterShortUsername)) {
    LOG.trace(""String_Node_Str"",impersonationRequest,UserGroupInformation.getCurrentUser());
    return UserGroupInformation.getCurrentUser();
  }
  return ugiProvider.getConfiguredUGI(impersonationRequest).getUGI();
}"
5284,"@Inject @VisibleForTesting public DefaultImpersonator(CConfiguration cConf,UGIProvider ugiProvider){
  this.ugiProvider=ugiProvider;
  this.kerberosEnabled=SecurityUtil.isKerberosEnabled(cConf);
}","@Inject @VisibleForTesting public DefaultImpersonator(CConfiguration cConf,UGIProvider ugiProvider){
  this.ugiProvider=ugiProvider;
  this.kerberosEnabled=SecurityUtil.isKerberosEnabled(cConf);
  String masterPrincipal=SecurityUtil.getMasterPrincipal(cConf);
  try {
    masterShortUsername=masterPrincipal == null ? null : new KerberosName(masterPrincipal).getShortName();
  }
 catch (  IOException e) {
    Throwables.propagate(e);
  }
}"
5285,"/** 
 * Retrieve the   {@link UserGroupInformation} for the given {@link NamespaceId}
 * @param entityId Entity whose effective owner's UGI will be returned
 * @return {@link UserGroupInformation}
 * @throws IOException if there was any error fetching the {@link UserGroupInformation}
 * @throws NamespaceNotFoundException if namespaceId does not exist
 */
UserGroupInformation getUGI(NamespacedEntityId entityId) throws IOException, NamespaceNotFoundException ;","/** 
 * Retrieve the   {@link UserGroupInformation} for the given {@link NamespaceId}
 * @param entityId Entity whose effective owner's UGI will be returned
 * @return {@link UserGroupInformation}
 * @throws IOException if there was any error fetching the {@link UserGroupInformation}
 */
UserGroupInformation getUGI(NamespacedEntityId entityId) throws IOException ;"
5286,"public Transaction checkpoint(Transaction originalTx) throws TransactionNotInProgressException {
  txMetricsCollector.rate(""String_Node_Str"");
  Stopwatch timer=new Stopwatch().start();
  Transaction checkpointedTx=null;
  long txId=originalTx.getTransactionId();
  long newWritePointer=0;
  this.logReadLock.lock();
  try {
synchronized (this) {
      ensureAvailable();
      InProgressTx parentTx=inProgress.get(txId);
      if (parentTx == null) {
        if (invalid.contains(txId)) {
          throw new TransactionNotInProgressException(String.format(""String_Node_Str"",txId));
        }
 else {
          throw new TransactionNotInProgressException(String.format(""String_Node_Str"",txId));
        }
      }
      newWritePointer=getNextWritePointer();
      doCheckpoint(newWritePointer,txId);
      checkpointedTx=new Transaction(originalTx,newWritePointer,parentTx.getCheckpointWritePointers().toLongArray());
    }
    appendToLog(TransactionEdit.createCheckpoint(newWritePointer,txId));
  }
  finally {
    this.logReadLock.unlock();
  }
  txMetricsCollector.histogram(""String_Node_Str"",(int)timer.elapsedMillis());
  return checkpointedTx;
}","public Transaction checkpoint(Transaction originalTx) throws TransactionNotInProgressException {
  txMetricsCollector.rate(""String_Node_Str"");
  Stopwatch timer=new Stopwatch().start();
  Transaction checkpointedTx=null;
  long txId=originalTx.getTransactionId();
  long newWritePointer=0;
  this.logReadLock.lock();
  try {
synchronized (this) {
      ensureAvailable();
      InProgressTx parentTx=inProgress.get(txId);
      if (parentTx == null) {
        if (invalidTxList.contains(txId)) {
          throw new TransactionNotInProgressException(String.format(""String_Node_Str"",txId));
        }
 else {
          throw new TransactionNotInProgressException(String.format(""String_Node_Str"",txId));
        }
      }
      newWritePointer=getNextWritePointer();
      doCheckpoint(newWritePointer,txId);
      checkpointedTx=new Transaction(originalTx,newWritePointer,parentTx.getCheckpointWritePointers().toLongArray());
    }
    appendToLog(TransactionEdit.createCheckpoint(newWritePointer,txId));
  }
  finally {
    this.logReadLock.unlock();
  }
  txMetricsCollector.histogram(""String_Node_Str"",(int)timer.elapsedMillis());
  return checkpointedTx;
}"
5287,"private void cleanupTimedOutTransactions(){
  List<TransactionEdit> invalidEdits=null;
  logReadLock.lock();
  try {
synchronized (this) {
      if (!isRunning()) {
        return;
      }
      long currentTime=System.currentTimeMillis();
      Map<Long,InProgressType> timedOut=Maps.newHashMap();
      for (      Map.Entry<Long,InProgressTx> tx : inProgress.entrySet()) {
        long expiration=tx.getValue().getExpiration();
        if (expiration >= 0L && currentTime > expiration) {
          timedOut.put(tx.getKey(),tx.getValue().getType());
          LOG.info(""String_Node_Str"",tx.getKey());
        }
 else         if (expiration < 0) {
          LOG.warn(""String_Node_Str"" + ""String_Node_Str"",tx.getKey(),expiration);
          timedOut.put(tx.getKey(),InProgressType.LONG);
        }
      }
      if (!timedOut.isEmpty()) {
        invalidEdits=Lists.newArrayListWithCapacity(timedOut.size());
        invalid.addAll(timedOut.keySet());
        for (        Map.Entry<Long,InProgressType> tx : timedOut.entrySet()) {
          inProgress.remove(tx.getKey());
          if (!InProgressType.CHECKPOINT.equals(tx.getValue())) {
            committingChangeSets.remove(tx.getKey());
            invalidEdits.add(TransactionEdit.createInvalid(tx.getKey()));
          }
        }
        Collections.sort(invalid);
        invalidArray=invalid.toLongArray();
        LOG.info(""String_Node_Str"",timedOut.size());
      }
    }
    if (invalidEdits != null) {
      appendToLog(invalidEdits);
    }
  }
  finally {
    this.logReadLock.unlock();
  }
}","private void cleanupTimedOutTransactions(){
  List<TransactionEdit> invalidEdits=null;
  logReadLock.lock();
  try {
synchronized (this) {
      if (!isRunning()) {
        return;
      }
      long currentTime=System.currentTimeMillis();
      Map<Long,InProgressType> timedOut=Maps.newHashMap();
      for (      Map.Entry<Long,InProgressTx> tx : inProgress.entrySet()) {
        long expiration=tx.getValue().getExpiration();
        if (expiration >= 0L && currentTime > expiration) {
          timedOut.put(tx.getKey(),tx.getValue().getType());
          LOG.info(""String_Node_Str"",tx.getKey());
        }
 else         if (expiration < 0) {
          LOG.warn(""String_Node_Str"" + ""String_Node_Str"",tx.getKey(),expiration);
          timedOut.put(tx.getKey(),InProgressType.LONG);
        }
      }
      if (!timedOut.isEmpty()) {
        invalidEdits=Lists.newArrayListWithCapacity(timedOut.size());
        invalidTxList.addAll(timedOut.keySet());
        for (        Map.Entry<Long,InProgressType> tx : timedOut.entrySet()) {
          inProgress.remove(tx.getKey());
          if (!InProgressType.CHECKPOINT.equals(tx.getValue())) {
            committingChangeSets.remove(tx.getKey());
            invalidEdits.add(TransactionEdit.createInvalid(tx.getKey()));
          }
        }
        LOG.info(""String_Node_Str"",timedOut.size());
      }
    }
    if (invalidEdits != null) {
      appendToLog(invalidEdits);
    }
  }
  finally {
    this.logReadLock.unlock();
  }
}"
5288,"public boolean commit(Transaction tx) throws TransactionNotInProgressException {
  txMetricsCollector.rate(""String_Node_Str"");
  Stopwatch timer=new Stopwatch().start();
  Set<ChangeId> changeSet=null;
  boolean addToCommitted=true;
  long commitPointer;
  this.logReadLock.lock();
  try {
synchronized (this) {
      ensureAvailable();
      commitPointer=lastWritePointer + 1;
      if (inProgress.get(tx.getTransactionId()) == null) {
        if (invalid.contains(tx.getTransactionId())) {
          throw new TransactionNotInProgressException(String.format(""String_Node_Str"" + ""String_Node_Str"",tx.getTransactionId()));
        }
 else {
          throw new TransactionNotInProgressException(String.format(""String_Node_Str"",tx.getTransactionId()));
        }
      }
      changeSet=committingChangeSets.remove(tx.getTransactionId());
      if (changeSet != null) {
        if (hasConflicts(tx,changeSet)) {
          return false;
        }
      }
 else {
        addToCommitted=false;
      }
      doCommit(tx.getTransactionId(),tx.getWritePointer(),changeSet,commitPointer,addToCommitted);
    }
    appendToLog(TransactionEdit.createCommitted(tx.getTransactionId(),changeSet,commitPointer,addToCommitted));
  }
  finally {
    this.logReadLock.unlock();
  }
  txMetricsCollector.histogram(""String_Node_Str"",(int)timer.elapsedMillis());
  return true;
}","public boolean commit(Transaction tx) throws TransactionNotInProgressException {
  txMetricsCollector.rate(""String_Node_Str"");
  Stopwatch timer=new Stopwatch().start();
  Set<ChangeId> changeSet=null;
  boolean addToCommitted=true;
  long commitPointer;
  this.logReadLock.lock();
  try {
synchronized (this) {
      ensureAvailable();
      commitPointer=lastWritePointer + 1;
      if (inProgress.get(tx.getTransactionId()) == null) {
        if (invalidTxList.contains(tx.getTransactionId())) {
          throw new TransactionNotInProgressException(String.format(""String_Node_Str"" + ""String_Node_Str"",tx.getTransactionId()));
        }
 else {
          throw new TransactionNotInProgressException(String.format(""String_Node_Str"",tx.getTransactionId()));
        }
      }
      changeSet=committingChangeSets.remove(tx.getTransactionId());
      if (changeSet != null) {
        if (hasConflicts(tx,changeSet)) {
          return false;
        }
      }
 else {
        addToCommitted=false;
      }
      doCommit(tx.getTransactionId(),tx.getWritePointer(),changeSet,commitPointer,addToCommitted);
    }
    appendToLog(TransactionEdit.createCommitted(tx.getTransactionId(),changeSet,commitPointer,addToCommitted));
  }
  finally {
    this.logReadLock.unlock();
  }
  txMetricsCollector.histogram(""String_Node_Str"",(int)timer.elapsedMillis());
  return true;
}"
5289,"public int getExcludedListSize(){
  return invalid.size() + inProgress.size();
}","public int getExcludedListSize(){
  return getInvalidSize() + inProgress.size();
}"
5290,"private boolean doInvalidate(long writePointer){
  Set<ChangeId> previousChangeSet=committingChangeSets.remove(writePointer);
  InProgressTx previous=inProgress.remove(writePointer);
  if (previous != null || previousChangeSet != null) {
    invalid.add(writePointer);
    if (previous == null) {
      LOG.debug(""String_Node_Str"",writePointer);
    }
 else {
      LongArrayList childWritePointers=previous.getCheckpointWritePointers();
      if (!childWritePointers.isEmpty()) {
        invalid.addAll(childWritePointers);
        inProgress.keySet().removeAll(childWritePointers);
      }
    }
    LOG.info(""String_Node_Str"",writePointer);
    Collections.sort(invalid);
    invalidArray=invalid.toLongArray();
    if (previous != null && !previous.isLongRunning()) {
      moveReadPointerIfNeeded(writePointer);
    }
    return true;
  }
  return false;
}","private boolean doInvalidate(long writePointer){
  Set<ChangeId> previousChangeSet=committingChangeSets.remove(writePointer);
  InProgressTx previous=inProgress.remove(writePointer);
  if (previous != null || previousChangeSet != null) {
    invalidTxList.add(writePointer);
    if (previous == null) {
      LOG.debug(""String_Node_Str"",writePointer);
    }
 else {
      LongArrayList childWritePointers=previous.getCheckpointWritePointers();
      if (!childWritePointers.isEmpty()) {
        invalidTxList.addAll(childWritePointers);
        inProgress.keySet().removeAll(childWritePointers);
      }
    }
    LOG.info(""String_Node_Str"",writePointer);
    if (previous != null && !previous.isLongRunning()) {
      moveReadPointerIfNeeded(writePointer);
    }
    return true;
  }
  return false;
}"
5291,"/** 
 * Restore the initial in-memory transaction state from a snapshot.
 */
private void restoreSnapshot(TransactionSnapshot snapshot){
  LOG.info(""String_Node_Str"" + snapshot.getTimestamp());
  Preconditions.checkState(lastSnapshotTime == 0,""String_Node_Str"");
  Preconditions.checkState(readPointer == 0,""String_Node_Str"");
  Preconditions.checkState(lastWritePointer == 0,""String_Node_Str"");
  Preconditions.checkState(invalid.isEmpty(),""String_Node_Str"");
  Preconditions.checkState(inProgress.isEmpty(),""String_Node_Str"");
  Preconditions.checkState(committingChangeSets.isEmpty(),""String_Node_Str"");
  Preconditions.checkState(committedChangeSets.isEmpty(),""String_Node_Str"");
  LOG.info(""String_Node_Str"" + snapshot);
  lastSnapshotTime=snapshot.getTimestamp();
  readPointer=snapshot.getReadPointer();
  lastWritePointer=snapshot.getWritePointer();
  invalid.addAll(snapshot.getInvalid());
  inProgress.putAll(txnBackwardsCompatCheck(defaultLongTimeout,longTimeoutTolerance,snapshot.getInProgress()));
  committingChangeSets.putAll(snapshot.getCommittingChangeSets());
  committedChangeSets.putAll(snapshot.getCommittedChangeSets());
}","/** 
 * Restore the initial in-memory transaction state from a snapshot.
 */
private void restoreSnapshot(TransactionSnapshot snapshot){
  LOG.info(""String_Node_Str"" + snapshot.getTimestamp());
  Preconditions.checkState(lastSnapshotTime == 0,""String_Node_Str"");
  Preconditions.checkState(readPointer == 0,""String_Node_Str"");
  Preconditions.checkState(lastWritePointer == 0,""String_Node_Str"");
  Preconditions.checkState(invalidTxList.isEmpty(),""String_Node_Str"");
  Preconditions.checkState(inProgress.isEmpty(),""String_Node_Str"");
  Preconditions.checkState(committingChangeSets.isEmpty(),""String_Node_Str"");
  Preconditions.checkState(committedChangeSets.isEmpty(),""String_Node_Str"");
  LOG.info(""String_Node_Str"" + snapshot);
  lastSnapshotTime=snapshot.getTimestamp();
  readPointer=snapshot.getReadPointer();
  lastWritePointer=snapshot.getWritePointer();
  invalidTxList.addAll(snapshot.getInvalid());
  inProgress.putAll(txnBackwardsCompatCheck(defaultLongTimeout,longTimeoutTolerance,snapshot.getInProgress()));
  committingChangeSets.putAll(snapshot.getCommittingChangeSets());
  committedChangeSets.putAll(snapshot.getCommittedChangeSets());
}"
5292,"private void startMetricsThread(){
  LOG.info(""String_Node_Str"" + METRICS_POLL_INTERVAL);
  this.metricsThread=new DaemonThreadExecutor(""String_Node_Str""){
    @Override public void doRun(){
      txMetricsCollector.gauge(""String_Node_Str"",committingChangeSets.size());
      txMetricsCollector.gauge(""String_Node_Str"",committedChangeSets.size());
      txMetricsCollector.gauge(""String_Node_Str"",inProgress.size());
      txMetricsCollector.gauge(""String_Node_Str"",invalidArray.length);
    }
    @Override protected void onShutdown(){
      txMetricsCollector.gauge(""String_Node_Str"",committingChangeSets.size());
      txMetricsCollector.gauge(""String_Node_Str"",committedChangeSets.size());
      txMetricsCollector.gauge(""String_Node_Str"",inProgress.size());
      txMetricsCollector.gauge(""String_Node_Str"",invalidArray.length);
    }
    @Override public long getSleepMillis(){
      return METRICS_POLL_INTERVAL;
    }
  }
;
  metricsThread.start();
}","private void startMetricsThread(){
  LOG.info(""String_Node_Str"" + METRICS_POLL_INTERVAL);
  this.metricsThread=new DaemonThreadExecutor(""String_Node_Str""){
    @Override public void doRun(){
      txMetricsCollector.gauge(""String_Node_Str"",committingChangeSets.size());
      txMetricsCollector.gauge(""String_Node_Str"",committedChangeSets.size());
      txMetricsCollector.gauge(""String_Node_Str"",inProgress.size());
      txMetricsCollector.gauge(""String_Node_Str"",getInvalidSize());
    }
    @Override protected void onShutdown(){
      txMetricsCollector.gauge(""String_Node_Str"",committingChangeSets.size());
      txMetricsCollector.gauge(""String_Node_Str"",committedChangeSets.size());
      txMetricsCollector.gauge(""String_Node_Str"",inProgress.size());
      txMetricsCollector.gauge(""String_Node_Str"",getInvalidSize());
    }
    @Override public long getSleepMillis(){
      return METRICS_POLL_INTERVAL;
    }
  }
;
  metricsThread.start();
}"
5293,"private void doCommit(long transactionId,long writePointer,Set<ChangeId> changes,long commitPointer,boolean addToCommitted){
  committingChangeSets.remove(transactionId);
  if (addToCommitted && !changes.isEmpty()) {
    Set<ChangeId> changeIds=committedChangeSets.get(commitPointer);
    if (changeIds != null) {
      changes.addAll(changeIds);
    }
    committedChangeSets.put(commitPointer,changes);
  }
  InProgressTx previous=inProgress.remove(transactionId);
  if (previous == null) {
    if (invalid.rem(transactionId)) {
      invalidArray=invalid.toLongArray();
      LOG.info(""String_Node_Str"",transactionId);
    }
  }
 else {
    LongArrayList checkpointPointers=previous.getCheckpointWritePointers();
    if (!checkpointPointers.isEmpty()) {
      writePointer=checkpointPointers.getLong(checkpointPointers.size() - 1);
      inProgress.keySet().removeAll(previous.getCheckpointWritePointers());
    }
  }
  moveReadPointerIfNeeded(writePointer);
  committedChangeSets.headMap(TxUtils.getFirstShortInProgress(inProgress)).clear();
}","private void doCommit(long transactionId,long writePointer,Set<ChangeId> changes,long commitPointer,boolean addToCommitted){
  committingChangeSets.remove(transactionId);
  if (addToCommitted && !changes.isEmpty()) {
    Set<ChangeId> changeIds=committedChangeSets.get(commitPointer);
    if (changeIds != null) {
      changes.addAll(changeIds);
    }
    committedChangeSets.put(commitPointer,changes);
  }
  InProgressTx previous=inProgress.remove(transactionId);
  if (previous == null) {
    if (invalidTxList.remove(transactionId)) {
      LOG.info(""String_Node_Str"",transactionId);
    }
  }
 else {
    LongArrayList checkpointPointers=previous.getCheckpointWritePointers();
    if (!checkpointPointers.isEmpty()) {
      writePointer=checkpointPointers.getLong(checkpointPointers.size() - 1);
      inProgress.keySet().removeAll(previous.getCheckpointWritePointers());
    }
  }
  moveReadPointerIfNeeded(writePointer);
  committedChangeSets.headMap(TxUtils.getFirstShortInProgress(inProgress)).clear();
}"
5294,"public synchronized TransactionSnapshot getCurrentState(){
  return TransactionSnapshot.copyFrom(System.currentTimeMillis(),readPointer,lastWritePointer,invalid,inProgress,committingChangeSets,committedChangeSets);
}","public synchronized TransactionSnapshot getCurrentState(){
  return TransactionSnapshot.copyFrom(System.currentTimeMillis(),readPointer,lastWritePointer,invalidTxList.toRawList(),inProgress,committingChangeSets,committedChangeSets);
}"
5295,"/** 
 * @return the size of invalid list
 */
public int getInvalidSize(){
  return this.invalid.size();
}","/** 
 * @return the size of invalid list
 */
public synchronized int getInvalidSize(){
  return this.invalidTxList.size();
}"
5296,"private void clear(){
  invalid.clear();
  invalidArray=NO_INVALID_TX;
  inProgress.clear();
  committedChangeSets.clear();
  committingChangeSets.clear();
  lastWritePointer=0;
  readPointer=0;
  lastSnapshotTime=0;
}","private void clear(){
  invalidTxList.clear();
  inProgress.clear();
  committedChangeSets.clear();
  committingChangeSets.clear();
  lastWritePointer=0;
  readPointer=0;
  lastSnapshotTime=0;
}"
5297,"private boolean doTruncateInvalidTx(Set<Long> invalidTxIds){
  LOG.info(""String_Node_Str"",invalidTxIds);
  boolean success=invalid.removeAll(invalidTxIds);
  if (success) {
    invalidArray=invalid.toLongArray();
  }
  return success;
}","private boolean doTruncateInvalidTx(Set<Long> toRemove){
  LOG.info(""String_Node_Str"",toRemove);
  return invalidTxList.removeAll(toRemove);
}"
5298,"/** 
 * Called from the tx service every 10 seconds. This hack is needed because current metrics system is not flexible when it comes to adding new metrics.
 */
public void logStatistics(){
  LOG.info(""String_Node_Str"" + lastWritePointer + ""String_Node_Str""+ invalid.size()+ ""String_Node_Str""+ inProgress.size()+ ""String_Node_Str""+ committingChangeSets.size()+ ""String_Node_Str""+ committedChangeSets.size());
}","/** 
 * Called from the tx service every 10 seconds. This hack is needed because current metrics system is not flexible when it comes to adding new metrics.
 */
public void logStatistics(){
  LOG.info(""String_Node_Str"" + lastWritePointer + ""String_Node_Str""+ getInvalidSize()+ ""String_Node_Str""+ inProgress.size()+ ""String_Node_Str""+ committingChangeSets.size()+ ""String_Node_Str""+ committedChangeSets.size());
}"
5299,"public boolean canCommit(Transaction tx,Collection<byte[]> changeIds) throws TransactionNotInProgressException {
  txMetricsCollector.rate(""String_Node_Str"");
  Stopwatch timer=new Stopwatch().start();
  if (inProgress.get(tx.getTransactionId()) == null) {
    if (invalid.contains(tx.getTransactionId())) {
      throw new TransactionNotInProgressException(String.format(""String_Node_Str"",tx.getTransactionId()));
    }
 else {
      throw new TransactionNotInProgressException(String.format(""String_Node_Str"",tx.getTransactionId()));
    }
  }
  Set<ChangeId> set=Sets.newHashSetWithExpectedSize(changeIds.size());
  for (  byte[] change : changeIds) {
    set.add(new ChangeId(change));
  }
  if (hasConflicts(tx,set)) {
    return false;
  }
  this.logReadLock.lock();
  try {
synchronized (this) {
      ensureAvailable();
      addCommittingChangeSet(tx.getTransactionId(),set);
    }
    appendToLog(TransactionEdit.createCommitting(tx.getTransactionId(),set));
  }
  finally {
    this.logReadLock.unlock();
  }
  txMetricsCollector.histogram(""String_Node_Str"",(int)timer.elapsedMillis());
  return true;
}","public boolean canCommit(Transaction tx,Collection<byte[]> changeIds) throws TransactionNotInProgressException {
  txMetricsCollector.rate(""String_Node_Str"");
  Stopwatch timer=new Stopwatch().start();
  if (inProgress.get(tx.getTransactionId()) == null) {
synchronized (this) {
      if (invalidTxList.contains(tx.getTransactionId())) {
        throw new TransactionNotInProgressException(String.format(""String_Node_Str"",tx.getTransactionId()));
      }
 else {
        throw new TransactionNotInProgressException(String.format(""String_Node_Str"",tx.getTransactionId()));
      }
    }
  }
  Set<ChangeId> set=Sets.newHashSetWithExpectedSize(changeIds.size());
  for (  byte[] change : changeIds) {
    set.add(new ChangeId(change));
  }
  if (hasConflicts(tx,set)) {
    return false;
  }
  this.logReadLock.lock();
  try {
synchronized (this) {
      ensureAvailable();
      addCommittingChangeSet(tx.getTransactionId(),set);
    }
    appendToLog(TransactionEdit.createCommitting(tx.getTransactionId(),set));
  }
  finally {
    this.logReadLock.unlock();
  }
  txMetricsCollector.histogram(""String_Node_Str"",(int)timer.elapsedMillis());
  return true;
}"
5300,"/** 
 * Creates a new Transaction. This method only get called from start transaction, which is already synchronized.
 */
private Transaction createTransaction(long writePointer,TransactionType type){
  long firstShortTx=Transaction.NO_TX_IN_PROGRESS;
  LongArrayList inProgressIds=new LongArrayList(inProgress.size());
  for (  Map.Entry<Long,InProgressTx> entry : inProgress.entrySet()) {
    long txId=entry.getKey();
    inProgressIds.add(txId);
    if (firstShortTx == Transaction.NO_TX_IN_PROGRESS && !entry.getValue().isLongRunning()) {
      firstShortTx=txId;
    }
  }
  return new Transaction(readPointer,writePointer,invalidArray,inProgressIds.toLongArray(),firstShortTx,type);
}","/** 
 * Creates a new Transaction. This method only get called from start transaction, which is already synchronized.
 */
private Transaction createTransaction(long writePointer,TransactionType type){
  long firstShortTx=Transaction.NO_TX_IN_PROGRESS;
  LongArrayList inProgressIds=new LongArrayList(inProgress.size());
  for (  Map.Entry<Long,InProgressTx> entry : inProgress.entrySet()) {
    long txId=entry.getKey();
    inProgressIds.add(txId);
    if (firstShortTx == Transaction.NO_TX_IN_PROGRESS && !entry.getValue().isLongRunning()) {
      firstShortTx=txId;
    }
  }
  return new Transaction(readPointer,writePointer,invalidTxList.toSortedArray(),inProgressIds.toLongArray(),firstShortTx,type);
}"
5301,"private void doAbort(long writePointer,long[] checkpointWritePointers,TransactionType type){
  committingChangeSets.remove(writePointer);
  if (type == TransactionType.LONG) {
    doInvalidate(writePointer);
    return;
  }
  InProgressTx removed=inProgress.remove(writePointer);
  boolean removeInProgressCheckpoints=true;
  if (removed == null) {
    if (invalid.rem(writePointer)) {
      removeInProgressCheckpoints=false;
      if (checkpointWritePointers != null) {
        for (        long checkpointWritePointer : checkpointWritePointers) {
          invalid.rem(checkpointWritePointer);
        }
      }
      invalidArray=invalid.toLongArray();
      LOG.info(""String_Node_Str"",writePointer);
    }
  }
  if (removeInProgressCheckpoints && checkpointWritePointers != null) {
    for (    long checkpointWritePointer : checkpointWritePointers) {
      inProgress.remove(checkpointWritePointer);
    }
  }
  moveReadPointerIfNeeded(writePointer);
}","private void doAbort(long writePointer,long[] checkpointWritePointers,TransactionType type){
  committingChangeSets.remove(writePointer);
  if (type == TransactionType.LONG) {
    doInvalidate(writePointer);
    return;
  }
  InProgressTx removed=inProgress.remove(writePointer);
  boolean removeInProgressCheckpoints=true;
  if (removed == null) {
    if (invalidTxList.remove(writePointer)) {
      removeInProgressCheckpoints=false;
      if (checkpointWritePointers != null) {
        for (        long checkpointWritePointer : checkpointWritePointers) {
          invalidTxList.remove(checkpointWritePointer);
        }
      }
      LOG.info(""String_Node_Str"",writePointer);
    }
  }
  if (removeInProgressCheckpoints && checkpointWritePointers != null) {
    for (    long checkpointWritePointer : checkpointWritePointers) {
      inProgress.remove(checkpointWritePointer);
    }
  }
  moveReadPointerIfNeeded(writePointer);
}"
5302,"private boolean doTruncateInvalidTxBefore(long time) throws InvalidTruncateTimeException {
  LOG.info(""String_Node_Str"",time);
  long truncateWp=time * TxConstants.MAX_TX_PER_MS;
  if (inProgress.lowerKey(truncateWp) != null) {
    throw new InvalidTruncateTimeException(""String_Node_Str"" + time + ""String_Node_Str"");
  }
  Set<Long> toTruncate=Sets.newHashSet();
  for (  long wp : invalid) {
    if (wp >= truncateWp) {
      break;
    }
    toTruncate.add(wp);
  }
  return doTruncateInvalidTx(toTruncate);
}","private boolean doTruncateInvalidTxBefore(long time) throws InvalidTruncateTimeException {
  LOG.info(""String_Node_Str"",time);
  long truncateWp=time * TxConstants.MAX_TX_PER_MS;
  if (inProgress.lowerKey(truncateWp) != null) {
    throw new InvalidTruncateTimeException(""String_Node_Str"" + time + ""String_Node_Str"");
  }
  LongSet toTruncate=new LongArraySet();
  LongIterator it=invalidTxList.toRawList().iterator();
  while (it.hasNext()) {
    long wp=it.nextLong();
    if (wp < truncateWp) {
      toTruncate.add(wp);
    }
  }
  LOG.info(""String_Node_Str"",toTruncate);
  return invalidTxList.removeAll(toTruncate);
}"
5303,"private HiveConf getHiveConf(){
  HiveConf conf=new HiveConf();
  if (UserGroupInformation.isSecurityEnabled()) {
    conf.set(HIVE_METASTORE_TOKEN_KEY,HiveAuthFactory.HS2_CLIENT_TOKEN);
  }
  conf.unset(""String_Node_Str"");
  conf.unset(""String_Node_Str"");
  return conf;
}","private HiveConf getHiveConf(){
  HiveConf conf=new HiveConf();
  if (UserGroupInformation.isSecurityEnabled()) {
    conf.set(HIVE_METASTORE_TOKEN_KEY,HiveAuthFactory.HS2_CLIENT_TOKEN);
  }
  String whiteListAppend=conf.getVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST_APPEND);
  if (whiteListAppend != null && !whiteListAppend.trim().isEmpty()) {
    whiteListAppend=whiteListAppend + ""String_Node_Str"" + PARAMS_EXPLORE_MODIFIES;
  }
 else {
    whiteListAppend=PARAMS_EXPLORE_MODIFIES;
  }
  conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_SQL_STD_AUTH_CONFIG_WHITELIST_APPEND,whiteListAppend);
  conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,UserGroupInformation.AuthenticationMethod.SIMPLE.name());
  conf.unset(""String_Node_Str"");
  conf.unset(""String_Node_Str"");
  return conf;
}"
5304,"private Map<String,String> doStartSession(@Nullable NamespaceId namespace,@Nullable Map<String,String> additionalSessionConf) throws IOException, ExploreException, NamespaceNotFoundException {
  Map<String,String> sessionConf=new HashMap<>();
  QueryHandle queryHandle=QueryHandle.generate();
  sessionConf.put(Constants.Explore.QUERY_ID,queryHandle.getHandle());
  String schedulerQueue=namespace != null ? schedulerQueueResolver.getQueue(namespace.toId()) : schedulerQueueResolver.getDefaultQueue();
  if (schedulerQueue != null && !schedulerQueue.isEmpty()) {
    sessionConf.put(JobContext.QUEUE_NAME,schedulerQueue);
  }
  Transaction tx=startTransaction();
  ConfigurationUtil.set(sessionConf,Constants.Explore.TX_QUERY_KEY,TxnCodec.INSTANCE,tx);
  ConfigurationUtil.set(sessionConf,Constants.Explore.CCONF_KEY,CConfCodec.INSTANCE,cConf);
  ConfigurationUtil.set(sessionConf,Constants.Explore.HCONF_KEY,HConfCodec.INSTANCE,hConf);
  HiveConf hiveConf=getHiveConf();
  if (ExploreServiceUtils.isSparkEngine(hiveConf,additionalSessionConf)) {
    sessionConf.putAll(sparkConf);
  }
  if (UserGroupInformation.isSecurityEnabled()) {
    sessionConf.put(""String_Node_Str"",""String_Node_Str"");
    sessionConf.put(""String_Node_Str"",""String_Node_Str"");
    File credentialsFile=writeCredentialsFile(queryHandle);
    String credentialsFilePath=credentialsFile.getAbsolutePath();
    sessionConf.put(MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY,credentialsFilePath);
    sessionConf.put(""String_Node_Str"",Boolean.FALSE.toString());
    sessionConf.put(""String_Node_Str"",Boolean.FALSE.toString());
    if (ExploreServiceUtils.isTezEngine(hiveConf,additionalSessionConf)) {
      sessionConf.put(""String_Node_Str"",credentialsFilePath);
    }
  }
  if (additionalSessionConf != null) {
    sessionConf.putAll(additionalSessionConf);
  }
  return sessionConf;
}","private Map<String,String> doStartSession(@Nullable NamespaceId namespace,@Nullable Map<String,String> additionalSessionConf) throws IOException, ExploreException, NamespaceNotFoundException {
  Map<String,String> sessionConf=new HashMap<>();
  QueryHandle queryHandle=QueryHandle.generate();
  sessionConf.put(Constants.Explore.QUERY_ID,queryHandle.getHandle());
  String schedulerQueue=namespace != null ? schedulerQueueResolver.getQueue(namespace.toId()) : schedulerQueueResolver.getDefaultQueue();
  if (schedulerQueue != null && !schedulerQueue.isEmpty()) {
    sessionConf.put(JobContext.QUEUE_NAME,schedulerQueue);
  }
  Transaction tx=startTransaction();
  ConfigurationUtil.set(sessionConf,Constants.Explore.TX_QUERY_KEY,TxnCodec.INSTANCE,tx);
  ConfigurationUtil.set(sessionConf,Constants.Explore.CCONF_KEY,CConfCodec.INSTANCE,cConf);
  ConfigurationUtil.set(sessionConf,Constants.Explore.HCONF_KEY,HConfCodec.INSTANCE,hConf);
  HiveConf hiveConf=getHiveConf();
  if (ExploreServiceUtils.isSparkEngine(hiveConf,additionalSessionConf)) {
    sessionConf.putAll(sparkConf);
  }
  if (UserGroupInformation.isSecurityEnabled()) {
    sessionConf.put(""String_Node_Str"",""String_Node_Str"");
    sessionConf.put(""String_Node_Str"",""String_Node_Str"");
    File credentialsFile=writeCredentialsFile(queryHandle);
    String credentialsFilePath=credentialsFile.getAbsolutePath();
    sessionConf.put(MRJobConfig.MAPREDUCE_JOB_CREDENTIALS_BINARY,credentialsFilePath);
    sessionConf.put(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,UserGroupInformation.AuthenticationMethod.KERBEROS.name());
    sessionConf.put(""String_Node_Str"",Boolean.FALSE.toString());
    sessionConf.put(""String_Node_Str"",Boolean.FALSE.toString());
    if (ExploreServiceUtils.isTezEngine(hiveConf,additionalSessionConf)) {
      sessionConf.put(""String_Node_Str"",credentialsFilePath);
    }
  }
  if (additionalSessionConf != null) {
    sessionConf.putAll(additionalSessionConf);
  }
  return sessionConf;
}"
5305,"private void rewriteHiveConfig(){
  URL resourceURL=getLocalResourceURL(""String_Node_Str"");
  if (resourceURL == null) {
    LOG.warn(""String_Node_Str"");
    return;
  }
  try (InputStream is=new FileInputStream(new File(resourceURL.toURI()))){
    Configuration conf=new Configuration(false);
    conf.addResource(is);
    conf.setBoolean(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,false);
    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER,false);
    conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,UserGroupInformation.AuthenticationMethod.SIMPLE.name());
    String sparkHome=System.getenv(Constants.SPARK_HOME);
    if (sparkHome != null) {
      LOG.debug(""String_Node_Str"",sparkHome);
      conf.set(""String_Node_Str"",sparkHome);
    }
    File newConfFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    try (FileOutputStream os=new FileOutputStream(newConfFile)){
      conf.writeXml(os);
    }
     Files.move(newConfFile.toPath(),Paths.get(resourceURL.toURI()),StandardCopyOption.REPLACE_EXISTING);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",resourceURL,e);
    throw Throwables.propagate(e);
  }
}","private void rewriteHiveConfig(){
  URL resourceURL=getLocalResourceURL(""String_Node_Str"");
  if (resourceURL == null) {
    LOG.warn(""String_Node_Str"");
    return;
  }
  try (InputStream is=new FileInputStream(new File(resourceURL.toURI()))){
    Configuration conf=new Configuration(false);
    conf.addResource(is);
    conf.setBoolean(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,false);
    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER,false);
    String sparkHome=System.getenv(Constants.SPARK_HOME);
    if (sparkHome != null) {
      LOG.debug(""String_Node_Str"",sparkHome);
      conf.set(""String_Node_Str"",sparkHome);
    }
    File newConfFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    try (FileOutputStream os=new FileOutputStream(newConfFile)){
      conf.writeXml(os);
    }
     Files.move(newConfFile.toPath(),Paths.get(resourceURL.toURI()),StandardCopyOption.REPLACE_EXISTING);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",resourceURL,e);
    throw Throwables.propagate(e);
  }
}"
5306,"/** 
 * Generate an X.509 certificate
 * @param dn Distinguished name for the owner of the certificate, it will also be the signer of the certificate.
 * @param pair Key pair used for signing the certificate.
 * @param days Validity of the certificate.
 * @param algorithm Name of the signature algorithm used.
 * @return A X.509 certificate
 */
private static X509Certificate getCertificate(String dn,KeyPair pair,int days,String algorithm) throws IOException, CertificateException, NoSuchProviderException, NoSuchAlgorithmException, InvalidKeyException, SignatureException {
  Date from=new Date();
  Date to=DateUtils.addDays(from,days);
  CertificateValidity interval=new CertificateValidity(from,to);
  BigInteger sn=new BigInteger(64,new SecureRandom());
  X500Name owner=new X500Name(dn);
  X509CertInfo info=new X509CertInfo();
  info.set(X509CertInfo.VALIDITY,interval);
  info.set(X509CertInfo.SERIAL_NUMBER,new CertificateSerialNumber(sn));
  info.set(X509CertInfo.SUBJECT,new CertificateSubjectName(owner));
  info.set(X509CertInfo.ISSUER,new CertificateIssuerName(owner));
  info.set(X509CertInfo.KEY,new CertificateX509Key(pair.getPublic()));
  info.set(X509CertInfo.VERSION,new CertificateVersion(CertificateVersion.V3));
  AlgorithmId algo=new AlgorithmId(AlgorithmId.md5WithRSAEncryption_oid);
  info.set(X509CertInfo.ALGORITHM_ID,new CertificateAlgorithmId(algo));
  X509CertImpl cert=new X509CertImpl(info);
  PrivateKey privateKey=pair.getPrivate();
  cert.sign(privateKey,algorithm);
  return cert;
}","/** 
 * Generate an X.509 certificate
 * @param dn Distinguished name for the owner of the certificate, it will also be the signer of the certificate.
 * @param pair Key pair used for signing the certificate.
 * @param days Validity of the certificate.
 * @param algorithm Name of the signature algorithm used.
 * @return A X.509 certificate
 */
private static X509Certificate getCertificate(String dn,KeyPair pair,int days,String algorithm) throws IOException, CertificateException, NoSuchProviderException, NoSuchAlgorithmException, InvalidKeyException, SignatureException {
  Date from=new Date();
  Date to=DateUtils.addDays(from,days);
  CertificateValidity interval=new CertificateValidity(from,to);
  BigInteger sn=new BigInteger(64,new SecureRandom());
  X500Name owner=new X500Name(dn);
  X509CertInfo info=new X509CertInfo();
  info.set(X509CertInfo.VALIDITY,interval);
  info.set(X509CertInfo.SERIAL_NUMBER,new CertificateSerialNumber(sn));
  Field subjectField=null;
  try {
    subjectField=info.getClass().getDeclaredField(""String_Node_Str"");
    if (subjectField.getType().equals(X500Name.class)) {
      info.set(X509CertInfo.SUBJECT,owner);
      info.set(X509CertInfo.ISSUER,owner);
    }
 else {
      info.set(X509CertInfo.SUBJECT,new CertificateSubjectName(owner));
      info.set(X509CertInfo.ISSUER,new CertificateIssuerName(owner));
    }
  }
 catch (  NoSuchFieldException e) {
    info.set(X509CertInfo.SUBJECT,owner);
    info.set(X509CertInfo.ISSUER,owner);
  }
  info.set(X509CertInfo.KEY,new CertificateX509Key(pair.getPublic()));
  info.set(X509CertInfo.VERSION,new CertificateVersion(CertificateVersion.V3));
  AlgorithmId algo=new AlgorithmId(AlgorithmId.md5WithRSAEncryption_oid);
  info.set(X509CertInfo.ALGORITHM_ID,new CertificateAlgorithmId(algo));
  X509CertImpl cert=new X509CertImpl(info);
  PrivateKey privateKey=pair.getPrivate();
  cert.sign(privateKey,algorithm);
  return cert;
}"
5307,"private EndpointStrategy loadCache(CacheKey cacheKey) throws Exception {
  EndpointStrategy endpointStrategy;
  RouteDestination routeDestination=cacheKey.getRouteDestination();
  if (routeDestination.getServiceName().contains(""String_Node_Str"")) {
    endpointStrategy=discoverService(cacheKey);
    if (endpointStrategy.pick() == null) {
      endpointStrategy=discoverDefaultService(cacheKey);
    }
  }
 else {
    endpointStrategy=discover(routeDestination);
  }
  if (endpointStrategy.pick() == null) {
    String message=String.format(""String_Node_Str"",cacheKey);
    LOG.error(message);
    throw new Exception(message);
  }
  return endpointStrategy;
}","private EndpointStrategy loadCache(CacheKey cacheKey) throws Exception {
  EndpointStrategy endpointStrategy;
  RouteDestination routeDestination=cacheKey.getRouteDestination();
  if (routeDestination.getServiceName().contains(""String_Node_Str"")) {
    endpointStrategy=discoverService(cacheKey);
    if (endpointStrategy.pick() == null) {
      endpointStrategy=discoverDefaultService(cacheKey);
    }
  }
 else {
    endpointStrategy=discover(routeDestination);
  }
  if (endpointStrategy.pick() == null) {
    String message=String.format(""String_Node_Str"",cacheKey);
    LOG.debug(message);
    throw new Exception(message);
  }
  return endpointStrategy;
}"
5308,"@Override public void exceptionCaught(ChannelHandlerContext ctx,ExceptionEvent e){
  Throwable cause=e.getCause();
switch (exceptionsHandled.incrementAndGet()) {
case 1:
    break;
case 2:
  LOG.error(""String_Node_Str"",ctx.getChannel(),cause);
default :
return;
}
LOG.error(""String_Node_Str"",ctx.getChannel(),cause);
if (ctx.getChannel().isConnected() && !channelClosed) {
HttpResponse response=(cause instanceof HandlerException) ? ((HandlerException)cause).createFailureResponse() : new DefaultHttpResponse(HttpVersion.HTTP_1_1,HttpResponseStatus.INTERNAL_SERVER_ERROR);
Channels.write(ctx,e.getFuture(),response);
e.getFuture().addListener(ChannelFutureListener.CLOSE);
}
}","@Override public void exceptionCaught(ChannelHandlerContext ctx,ExceptionEvent e){
  Throwable cause=e.getCause();
switch (exceptionsHandled.incrementAndGet()) {
case 1:
    break;
case 2:
  LOG.error(""String_Node_Str"",ctx.getChannel(),cause);
default :
return;
}
if (cause instanceof HandlerException && ((HandlerException)cause).getFailureStatus() != HttpResponseStatus.INTERNAL_SERVER_ERROR) {
LOG.debug(""String_Node_Str"",ctx.getChannel(),cause);
}
 else {
LOG.error(""String_Node_Str"",ctx.getChannel(),cause);
}
if (ctx.getChannel().isConnected() && !channelClosed) {
HttpResponse response=cause instanceof HandlerException ? ((HandlerException)cause).createFailureResponse() : new DefaultHttpResponse(HttpVersion.HTTP_1_1,HttpResponseStatus.INTERNAL_SERVER_ERROR);
Channels.write(ctx,e.getFuture(),response);
e.getFuture().addListener(ChannelFutureListener.CLOSE);
}
}"
5309,"@Override public void messageReceived(final ChannelHandlerContext ctx,MessageEvent event) throws Exception {
  if (channelClosed) {
    return;
  }
  final Channel inboundChannel=event.getChannel();
  Object msg=event.getMessage();
  if (msg instanceof HttpChunk) {
    if (chunkSender == null) {
      throw new HandlerException(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    }
    chunkSender.send(msg);
  }
 else   if (msg instanceof HttpRequest) {
    HttpRequest request=(HttpRequest)msg;
    request=applyProxyRules(request);
    inboundChannel.setReadable(false);
    WrappedDiscoverable discoverable=getDiscoverable(request,(InetSocketAddress)inboundChannel.getLocalAddress());
    MessageSender sender=discoveryLookup.get(discoverable);
    if (sender == null || !sender.isConnected()) {
      InetSocketAddress address=discoverable.getSocketAddress();
      ChannelFuture future=clientBootstrap.connect(address);
      final Channel outboundChannel=future.getChannel();
      outboundChannel.getPipeline().addAfter(""String_Node_Str"",""String_Node_Str"",new OutboundHandler(inboundChannel));
      if (Arrays.equals(Constants.Security.SSL_URI_SCHEME.getBytes(),discoverable.getPayload())) {
        SSLContext clientContext=null;
        try {
          clientContext=SSLContext.getInstance(""String_Node_Str"");
          clientContext.init(null,PermissiveTrustManagerFactory.getTrustManagers(),null);
        }
 catch (        NoSuchAlgorithmException|KeyManagementException e) {
          throw new RuntimeException(""String_Node_Str"" + ""String_Node_Str"",e);
        }
        SSLEngine engine=clientContext.createSSLEngine();
        engine.setUseClientMode(true);
        engine.setEnabledProtocols(new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str""});
        outboundChannel.getPipeline().addFirst(""String_Node_Str"",new SslHandler(engine));
        LOG.trace(""String_Node_Str"");
      }
      sender=new MessageSender(inboundChannel,future);
      discoveryLookup.put(discoverable,sender);
      inboundChannel.setAttachment(outboundChannel);
      outboundChannel.getCloseFuture().addListener(new ChannelFutureListener(){
        @Override public void operationComplete(        ChannelFuture future) throws Exception {
          inboundChannel.getPipeline().execute(new Runnable(){
            @Override public void run(){
              if (outboundChannel.equals(inboundChannel.getAttachment())) {
                closeOnFlush(inboundChannel);
              }
            }
          }
);
        }
      }
);
    }
 else {
      Channel outboundChannel=(Channel)inboundChannel.getAttachment();
      if (outboundChannel != null) {
        outboundChannel.setReadable(true);
      }
    }
    sender.send(request);
    inboundChannel.setReadable(true);
    if (request.isChunked()) {
      chunkSender=sender;
    }
  }
 else {
    super.messageReceived(ctx,event);
  }
}","@Override public void messageReceived(final ChannelHandlerContext ctx,MessageEvent event) throws Exception {
  if (channelClosed) {
    return;
  }
  final Channel inboundChannel=event.getChannel();
  Object msg=event.getMessage();
  if (msg instanceof HttpChunk) {
    if (chunkSender == null) {
      throw new HandlerException(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    }
    chunkSender.send(msg);
  }
 else   if (msg instanceof HttpRequest) {
    HttpRequest request=(HttpRequest)msg;
    request=applyProxyRules(request);
    inboundChannel.setReadable(false);
    WrappedDiscoverable discoverable=getDiscoverable(request,(InetSocketAddress)inboundChannel.getLocalAddress());
    MessageSender sender=discoveryLookup.get(discoverable);
    if (sender == null || !sender.isConnected()) {
      InetSocketAddress address=discoverable.getSocketAddress();
      ChannelFuture future=clientBootstrap.connect(address);
      final Channel outboundChannel=future.getChannel();
      outboundChannel.getPipeline().addAfter(""String_Node_Str"",""String_Node_Str"",new OutboundHandler(inboundChannel));
      if (Arrays.equals(Constants.Security.SSL_URI_SCHEME.getBytes(),discoverable.getPayload())) {
        SSLContext clientContext;
        try {
          clientContext=SSLContext.getInstance(""String_Node_Str"");
          clientContext.init(null,PermissiveTrustManagerFactory.getTrustManagers(),null);
        }
 catch (        NoSuchAlgorithmException|KeyManagementException e) {
          throw new RuntimeException(""String_Node_Str"" + ""String_Node_Str"",e);
        }
        SSLEngine engine=clientContext.createSSLEngine();
        engine.setUseClientMode(true);
        engine.setEnabledProtocols(new String[]{""String_Node_Str"",""String_Node_Str"",""String_Node_Str""});
        outboundChannel.getPipeline().addFirst(""String_Node_Str"",new SslHandler(engine));
        LOG.trace(""String_Node_Str"");
      }
      sender=new MessageSender(inboundChannel,future);
      discoveryLookup.put(discoverable,sender);
      inboundChannel.setAttachment(outboundChannel);
      outboundChannel.getCloseFuture().addListener(new ChannelFutureListener(){
        @Override public void operationComplete(        ChannelFuture future) throws Exception {
          inboundChannel.getPipeline().execute(new Runnable(){
            @Override public void run(){
              if (outboundChannel.equals(inboundChannel.getAttachment())) {
                closeOnFlush(inboundChannel);
              }
            }
          }
);
        }
      }
);
    }
 else {
      Channel outboundChannel=(Channel)inboundChannel.getAttachment();
      if (outboundChannel != null) {
        outboundChannel.setReadable(true);
      }
    }
    sender.send(request);
    inboundChannel.setReadable(true);
    if (request.isChunked()) {
      chunkSender=sender;
    }
  }
 else {
    super.messageReceived(ctx,event);
  }
}"
5310,"@Inject public TransactionHttpHandler(CConfiguration cConf,TransactionSystemClient txClient){
  this.txClient=new TransactionSystemClientAdapter(txClient);
  try {
    this.debugClazz=getClass().getClassLoader().loadClass(""String_Node_Str"");
    this.debugObject=debugClazz.newInstance();
    Configuration hConf=new Configuration();
    for (    Map.Entry<String,String> entry : cConf) {
      hConf.set(entry.getKey(),entry.getValue());
    }
    Method initMethod=debugClazz.getMethod(""String_Node_Str"",Configuration.class);
    initMethod.setAccessible(true);
    initMethod.invoke(debugObject,hConf);
  }
 catch (  ClassNotFoundException|IllegalAccessException|InstantiationException ex) {
    LOG.warn(""String_Node_Str"",ex);
    this.debugClazz=null;
  }
catch (  NoSuchMethodException|InvocationTargetException ex) {
    LOG.warn(""String_Node_Str"",ex);
    this.debugClazz=null;
  }
}","@Inject public TransactionHttpHandler(CConfiguration cConf,TransactionSystemClient txClient){
  this.cConf=cConf;
  this.txClient=new TransactionSystemClientAdapter(txClient);
  this.pruneEnable=cConf.getBoolean(TxConstants.TransactionPruning.PRUNE_ENABLE,TxConstants.TransactionPruning.DEFAULT_PRUNE_ENABLE);
}"
5311,"@Path(""String_Node_Str"") @GET public void getPruneInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String regionName){
  if (debugClazz == null || debugObject == null) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    return;
  }
  try {
    Method method=debugClazz.getMethod(""String_Node_Str"",String.class);
    method.setAccessible(true);
    Object response=method.invoke(debugObject,regionName);
    if (response == null) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
      return;
    }
    RegionPruneInfo pruneInfo=(RegionPruneInfo)response;
    responder.sendJson(HttpResponseStatus.OK,pruneInfo);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}","@Path(""String_Node_Str"") @GET public void getPruneInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String regionName){
  if (!initializePruningDebug(responder)) {
    return;
  }
  try {
    Method method=debugClazz.getMethod(""String_Node_Str"",String.class);
    method.setAccessible(true);
    Object response=method.invoke(debugObject,regionName);
    if (response == null) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
      return;
    }
    RegionPruneInfo pruneInfo=(RegionPruneInfo)response;
    responder.sendJson(HttpResponseStatus.OK,pruneInfo);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}"
5312,"@Path(""String_Node_Str"") @GET public void getTimeRegions(HttpRequest request,HttpResponder responder,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long time){
  if (debugClazz == null || debugObject == null) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    return;
  }
  try {
    Method method=debugClazz.getMethod(""String_Node_Str"",Long.class);
    method.setAccessible(true);
    Object response=method.invoke(debugObject,time);
    if (response == null) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",time));
      return;
    }
    Map<Long,SortedSet<String>> timeRegionInfo=(Map<Long,SortedSet<String>>)response;
    responder.sendJson(HttpResponseStatus.OK,timeRegionInfo);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}","@Path(""String_Node_Str"") @GET public void getTimeRegions(HttpRequest request,HttpResponder responder,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long time){
  if (!initializePruningDebug(responder)) {
    return;
  }
  try {
    Method method=debugClazz.getMethod(""String_Node_Str"",Long.class);
    method.setAccessible(true);
    Object response=method.invoke(debugObject,time);
    if (response == null) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",time));
      return;
    }
    Map<Long,SortedSet<String>> timeRegionInfo=(Map<Long,SortedSet<String>>)response;
    responder.sendJson(HttpResponseStatus.OK,timeRegionInfo);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}"
5313,"@Path(""String_Node_Str"") @GET public void getIdleRegions(HttpRequest request,HttpResponder responder,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numRegions){
  if (debugClazz == null || debugObject == null) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    return;
  }
  try {
    Method method=debugClazz.getMethod(""String_Node_Str"",Integer.class);
    method.setAccessible(true);
    Object response=method.invoke(debugObject,numRegions);
    Queue<RegionPruneInfo> pruneInfos=(Queue<RegionPruneInfo>)response;
    responder.sendJson(HttpResponseStatus.OK,pruneInfos);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}","@Path(""String_Node_Str"") @GET public void getIdleRegions(HttpRequest request,HttpResponder responder,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numRegions){
  if (!initializePruningDebug(responder)) {
    return;
  }
  try {
    Method method=debugClazz.getMethod(""String_Node_Str"",Integer.class);
    method.setAccessible(true);
    Object response=method.invoke(debugObject,numRegions);
    Queue<RegionPruneInfo> pruneInfos=(Queue<RegionPruneInfo>)response;
    responder.sendJson(HttpResponseStatus.OK,pruneInfos);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}"
5314,"@POST @Path(""String_Node_Str"") public void getCredentials(HttpRequest request,HttpResponder responder) throws Exception {
  String requestContent=request.getContent().toString(Charsets.UTF_8);
  if (requestContent == null) {
    throw new BadRequestException(""String_Node_Str"");
  }
  ImpersonationRequest impersonationRequest=GSON.fromJson(requestContent,ImpersonationRequest.class);
  LOG.info(""String_Node_Str"",impersonationRequest);
  UGIWithPrincipal ugiWithPrincipal=ugiProvider.getConfiguredUGI(impersonationRequest);
  Credentials credentials=ImpersonationUtils.doAs(ugiWithPrincipal.getUGI(),new Callable<Credentials>(){
    @Override public Credentials call() throws Exception {
      SecureStore update=tokenSecureStoreUpdater.update();
      return update.getStore();
    }
  }
);
  Location credentialsDir=locationFactory.create(""String_Node_Str"");
  if (credentialsDir.isDirectory() || credentialsDir.mkdirs() || credentialsDir.isDirectory()) {
    Location credentialsFile=credentialsDir.append(""String_Node_Str"").getTempFile(""String_Node_Str"");
    try (DataOutputStream os=new DataOutputStream(new BufferedOutputStream(credentialsFile.getOutputStream(""String_Node_Str"")))){
      credentials.writeTokenStorageToStream(os);
    }
     LOG.debug(""String_Node_Str"",ugiWithPrincipal.getPrincipal(),credentialsFile);
    PrincipalCredentials principalCredentials=new PrincipalCredentials(ugiWithPrincipal.getPrincipal(),credentialsFile.toURI().toString());
    responder.sendJson(HttpResponseStatus.OK,principalCredentials);
  }
 else {
    throw new IllegalStateException(""String_Node_Str"");
  }
}","@POST @Path(""String_Node_Str"") public void getCredentials(HttpRequest request,HttpResponder responder) throws Exception {
  String requestContent=request.getContent().toString(Charsets.UTF_8);
  if (requestContent == null) {
    throw new BadRequestException(""String_Node_Str"");
  }
  ImpersonationRequest impersonationRequest=GSON.fromJson(requestContent,ImpersonationRequest.class);
  LOG.debug(""String_Node_Str"",impersonationRequest);
  UGIWithPrincipal ugiWithPrincipal=ugiProvider.getConfiguredUGI(impersonationRequest);
  Credentials credentials=ImpersonationUtils.doAs(ugiWithPrincipal.getUGI(),new Callable<Credentials>(){
    @Override public Credentials call() throws Exception {
      SecureStore update=tokenSecureStoreUpdater.update();
      return update.getStore();
    }
  }
);
  Location credentialsDir=locationFactory.create(""String_Node_Str"");
  if (credentialsDir.isDirectory() || credentialsDir.mkdirs() || credentialsDir.isDirectory()) {
    Location credentialsFile=credentialsDir.append(""String_Node_Str"").getTempFile(""String_Node_Str"");
    try (DataOutputStream os=new DataOutputStream(new BufferedOutputStream(credentialsFile.getOutputStream(""String_Node_Str"")))){
      credentials.writeTokenStorageToStream(os);
    }
     LOG.debug(""String_Node_Str"",ugiWithPrincipal.getPrincipal(),credentialsFile);
    PrincipalCredentials principalCredentials=new PrincipalCredentials(ugiWithPrincipal.getPrincipal(),credentialsFile.toURI().toString());
    responder.sendJson(HttpResponseStatus.OK,principalCredentials);
  }
 else {
    throw new IllegalStateException(""String_Node_Str"");
  }
}"
5315,"/** 
 * Helper method to get delegation tokens for the given LocationFactory.
 * @param config The hadoop configuration.
 * @param locationFactory The LocationFactory for generating tokens.
 * @param credentials Credentials for storing tokens acquired.
 * @return List of delegation Tokens acquired.TODO: copied from Twill 0.6 YarnUtils for CDAP-5350. Remove after this fix is moved to Twill.
 */
private static List<Token<?>> addDelegationTokens(Configuration config,LocationFactory locationFactory,Credentials credentials) throws IOException {
  if (!UserGroupInformation.isSecurityEnabled()) {
    LOG.debug(""String_Node_Str"");
    return ImmutableList.of();
  }
  FileSystem fileSystem=getFileSystem(locationFactory,config);
  if (fileSystem == null) {
    LOG.warn(""String_Node_Str"");
    return ImmutableList.of();
  }
  String renewer=YarnUtils.getYarnTokenRenewer(config);
  Token<?>[] tokens=fileSystem.addDelegationTokens(renewer,credentials);
  LOG.info(""String_Node_Str"",Arrays.toString(tokens));
  return tokens == null ? ImmutableList.<Token<?>>of() : ImmutableList.copyOf(tokens);
}","/** 
 * Helper method to get delegation tokens for the given LocationFactory.
 * @param config The hadoop configuration.
 * @param locationFactory The LocationFactory for generating tokens.
 * @param credentials Credentials for storing tokens acquired.
 * @return List of delegation Tokens acquired.TODO: copied from Twill 0.6 YarnUtils for CDAP-5350. Remove after this fix is moved to Twill.
 */
private static List<Token<?>> addDelegationTokens(Configuration config,LocationFactory locationFactory,Credentials credentials) throws IOException {
  if (!UserGroupInformation.isSecurityEnabled()) {
    LOG.debug(""String_Node_Str"");
    return ImmutableList.of();
  }
  FileSystem fileSystem=getFileSystem(locationFactory,config);
  if (fileSystem == null) {
    LOG.warn(""String_Node_Str"");
    return ImmutableList.of();
  }
  String renewer=YarnUtils.getYarnTokenRenewer(config);
  Token<?>[] tokens=fileSystem.addDelegationTokens(renewer,credentials);
  LOG.debug(""String_Node_Str"",Arrays.toString(tokens));
  return tokens == null ? ImmutableList.<Token<?>>of() : ImmutableList.copyOf(tokens);
}"
5316,"/** 
 * Invoked when an update to secure store is needed.
 */
public SecureStore update(){
  Credentials credentials=refreshCredentials();
  LOG.info(""String_Node_Str"",credentials.getAllTokens());
  try {
    return YarnSecureStore.create(credentials,UserGroupInformation.getCurrentUser());
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}","/** 
 * Invoked when an update to secure store is needed.
 */
public SecureStore update(){
  Credentials credentials=refreshCredentials();
  LOG.debug(""String_Node_Str"",credentials.getAllTokens());
  try {
    return YarnSecureStore.create(credentials,UserGroupInformation.getCurrentUser());
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}"
5317,"public static Credentials obtainToken(Credentials credentials){
  ClassLoader hiveClassloader=ExploreUtils.getExploreClassloader();
  ClassLoader contextClassloader=Thread.currentThread().getContextClassLoader();
  Thread.currentThread().setContextClassLoader(hiveClassloader);
  try {
    LOG.info(""String_Node_Str"");
    Class hiveConfClass=hiveClassloader.loadClass(""String_Node_Str"");
    Object hiveConf=hiveConfClass.newInstance();
    Class hiveClass=hiveClassloader.loadClass(""String_Node_Str"");
    @SuppressWarnings(""String_Node_Str"") Method hiveGet=hiveClass.getMethod(""String_Node_Str"",hiveConfClass);
    Object hiveObject=hiveGet.invoke(null,hiveConf);
    String user=UserGroupInformation.getCurrentUser().getShortUserName();
    @SuppressWarnings(""String_Node_Str"") Method getDelegationToken=hiveClass.getMethod(""String_Node_Str"",String.class,String.class);
    String tokenStr=(String)getDelegationToken.invoke(hiveObject,user,user);
    Token<DelegationTokenIdentifier> delegationToken=new Token<>();
    delegationToken.decodeFromUrlString(tokenStr);
    delegationToken.setService(new Text(HiveAuthFactory.HS2_CLIENT_TOKEN));
    LOG.info(""String_Node_Str"",delegationToken,delegationToken.getService(),user);
    credentials.addToken(delegationToken.getService(),delegationToken);
    return credentials;
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
 finally {
    Thread.currentThread().setContextClassLoader(contextClassloader);
  }
}","public static Credentials obtainToken(Credentials credentials){
  ClassLoader hiveClassloader=ExploreUtils.getExploreClassloader();
  ClassLoader contextClassloader=Thread.currentThread().getContextClassLoader();
  Thread.currentThread().setContextClassLoader(hiveClassloader);
  try {
    Class hiveConfClass=hiveClassloader.loadClass(""String_Node_Str"");
    Object hiveConf=hiveConfClass.newInstance();
    Class hiveClass=hiveClassloader.loadClass(""String_Node_Str"");
    @SuppressWarnings(""String_Node_Str"") Method hiveGet=hiveClass.getMethod(""String_Node_Str"",hiveConfClass);
    Object hiveObject=hiveGet.invoke(null,hiveConf);
    String user=UserGroupInformation.getCurrentUser().getShortUserName();
    @SuppressWarnings(""String_Node_Str"") Method getDelegationToken=hiveClass.getMethod(""String_Node_Str"",String.class,String.class);
    String tokenStr=(String)getDelegationToken.invoke(hiveObject,user,user);
    Token<DelegationTokenIdentifier> delegationToken=new Token<>();
    delegationToken.decodeFromUrlString(tokenStr);
    delegationToken.setService(new Text(HiveAuthFactory.HS2_CLIENT_TOKEN));
    LOG.debug(""String_Node_Str"",delegationToken,delegationToken.getService(),user);
    credentials.addToken(delegationToken.getService(),delegationToken);
    return credentials;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
 finally {
    Thread.currentThread().setContextClassLoader(contextClassloader);
  }
}"
5318,"/** 
 * Gets a JHS delegation token and stores it in the given Credentials.
 * @return the same Credentials instance as the one given in parameter.
 */
public static Credentials obtainToken(Configuration configuration,Credentials credentials){
  if (!UserGroupInformation.isSecurityEnabled()) {
    return credentials;
  }
  String historyServerAddress=configuration.get(""String_Node_Str"");
  HostAndPort hostAndPort=HostAndPort.fromString(historyServerAddress);
  try {
    LOG.info(""String_Node_Str"");
    ResourceMgrDelegate resourceMgrDelegate=new ResourceMgrDelegate(new YarnConfiguration(configuration));
    MRClientCache clientCache=new MRClientCache(configuration,resourceMgrDelegate);
    MRClientProtocol hsProxy=clientCache.getInitializedHSProxy();
    GetDelegationTokenRequest request=new GetDelegationTokenRequestPBImpl();
    request.setRenewer(YarnUtils.getYarnTokenRenewer(configuration));
    InetSocketAddress address=new InetSocketAddress(hostAndPort.getHostText(),hostAndPort.getPort());
    Token<TokenIdentifier> token=ConverterUtils.convertFromYarn(hsProxy.getDelegationToken(request).getDelegationToken(),address);
    credentials.addToken(new Text(token.getService()),token);
    return credentials;
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",hostAndPort,e);
    throw Throwables.propagate(e);
  }
}","/** 
 * Gets a JHS delegation token and stores it in the given Credentials.
 * @return the same Credentials instance as the one given in parameter.
 */
public static Credentials obtainToken(Configuration configuration,Credentials credentials){
  if (!UserGroupInformation.isSecurityEnabled()) {
    return credentials;
  }
  String historyServerAddress=configuration.get(""String_Node_Str"");
  HostAndPort hostAndPort=HostAndPort.fromString(historyServerAddress);
  try {
    ResourceMgrDelegate resourceMgrDelegate=new ResourceMgrDelegate(new YarnConfiguration(configuration));
    MRClientCache clientCache=new MRClientCache(configuration,resourceMgrDelegate);
    MRClientProtocol hsProxy=clientCache.getInitializedHSProxy();
    GetDelegationTokenRequest request=new GetDelegationTokenRequestPBImpl();
    request.setRenewer(YarnUtils.getYarnTokenRenewer(configuration));
    InetSocketAddress address=new InetSocketAddress(hostAndPort.getHostText(),hostAndPort.getPort());
    Token<TokenIdentifier> token=ConverterUtils.convertFromYarn(hsProxy.getDelegationToken(request).getDelegationToken(),address);
    credentials.addToken(new Text(token.getService()),token);
    LOG.debug(""String_Node_Str"",token);
    return credentials;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
5319,"/** 
 * Gets a Yarn delegation token and stores it in the given Credentials.
 * @return the same Credentials instance as the one given in parameter.
 */
public static Credentials obtainToken(YarnConfiguration configuration,Credentials credentials){
  if (!UserGroupInformation.isSecurityEnabled()) {
    return credentials;
  }
  try {
    YarnClient yarnClient=YarnClient.createYarnClient();
    yarnClient.init(configuration);
    yarnClient.start();
    try {
      Text renewer=new Text(UserGroupInformation.getCurrentUser().getShortUserName());
      org.apache.hadoop.yarn.api.records.Token rmDelegationToken=yarnClient.getRMDelegationToken(renewer);
      List<String> services=new ArrayList<>();
      if (HAUtil.isHAEnabled(configuration)) {
        YarnConfiguration yarnConf=new YarnConfiguration(configuration);
        for (        String rmId : HAUtil.getRMHAIds(configuration)) {
          yarnConf.set(YarnConfiguration.RM_HA_ID,rmId);
          InetSocketAddress address=yarnConf.getSocketAddr(YarnConfiguration.RM_ADDRESS,YarnConfiguration.DEFAULT_RM_ADDRESS,YarnConfiguration.DEFAULT_RM_PORT);
          services.add(SecurityUtil.buildTokenService(address).toString());
        }
      }
 else {
        services.add(SecurityUtil.buildTokenService(YarnUtils.getRMAddress(configuration)).toString());
      }
      Token<TokenIdentifier> token=ConverterUtils.convertFromYarn(rmDelegationToken,(InetSocketAddress)null);
      token.setService(new Text(Joiner.on(',').join(services)));
      credentials.addToken(new Text(token.getService()),token);
      LOG.info(""String_Node_Str"",token);
    }
  finally {
      yarnClient.stop();
    }
    return credentials;
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}","/** 
 * Gets a Yarn delegation token and stores it in the given Credentials.
 * @return the same Credentials instance as the one given in parameter.
 */
public static Credentials obtainToken(YarnConfiguration configuration,Credentials credentials){
  if (!UserGroupInformation.isSecurityEnabled()) {
    return credentials;
  }
  try {
    YarnClient yarnClient=YarnClient.createYarnClient();
    yarnClient.init(configuration);
    yarnClient.start();
    try {
      Text renewer=new Text(UserGroupInformation.getCurrentUser().getShortUserName());
      org.apache.hadoop.yarn.api.records.Token rmDelegationToken=yarnClient.getRMDelegationToken(renewer);
      List<String> services=new ArrayList<>();
      if (HAUtil.isHAEnabled(configuration)) {
        YarnConfiguration yarnConf=new YarnConfiguration(configuration);
        for (        String rmId : HAUtil.getRMHAIds(configuration)) {
          yarnConf.set(YarnConfiguration.RM_HA_ID,rmId);
          InetSocketAddress address=yarnConf.getSocketAddr(YarnConfiguration.RM_ADDRESS,YarnConfiguration.DEFAULT_RM_ADDRESS,YarnConfiguration.DEFAULT_RM_PORT);
          services.add(SecurityUtil.buildTokenService(address).toString());
        }
      }
 else {
        services.add(SecurityUtil.buildTokenService(YarnUtils.getRMAddress(configuration)).toString());
      }
      Token<TokenIdentifier> token=ConverterUtils.convertFromYarn(rmDelegationToken,null);
      token.setService(new Text(Joiner.on(',').join(services)));
      credentials.addToken(new Text(token.getService()),token);
      LOG.debug(""String_Node_Str"",token);
    }
  finally {
      yarnClient.stop();
    }
    return credentials;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
5320,"/** 
 * Gets a HBase delegation token and stores it in the given Credentials.
 * @return the same Credentials instance as the one given in parameter.
 */
public static Credentials obtainToken(Configuration hConf,Credentials credentials){
  if (!User.isHBaseSecurityEnabled(hConf)) {
    return credentials;
  }
  try {
    Class c=Class.forName(""String_Node_Str"");
    Method method=c.getMethod(""String_Node_Str"",Configuration.class);
    Token<? extends TokenIdentifier> token=castToken(method.invoke(null,hConf));
    credentials.addToken(token.getService(),token);
    return credentials;
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}","/** 
 * Gets a HBase delegation token and stores it in the given Credentials.
 * @return the same Credentials instance as the one given in parameter.
 */
public static Credentials obtainToken(Configuration hConf,Credentials credentials){
  if (!User.isHBaseSecurityEnabled(hConf)) {
    return credentials;
  }
  try {
    Class c=Class.forName(""String_Node_Str"");
    Method method=c.getMethod(""String_Node_Str"",Configuration.class);
    Token<? extends TokenIdentifier> token=castToken(method.invoke(null,hConf));
    credentials.addToken(token.getService(),token);
    return credentials;
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
5321,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override @AuthEnforce(entities=""String_Node_Str"",enforceOn=InstanceId.class,actions=Action.ADMIN) public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=metadata.getNamespaceId();
  if (exists(namespace)) {
    throw new NamespaceAlreadyExistsException(namespace);
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  boolean hasValidKerberosConf=false;
  if (metadata.getConfig() != null) {
    String configuredPrincipal=metadata.getConfig().getPrincipal();
    String configuredKeytabURI=metadata.getConfig().getKeytabURI();
    if ((!Strings.isNullOrEmpty(configuredPrincipal) && Strings.isNullOrEmpty(configuredKeytabURI)) || (Strings.isNullOrEmpty(configuredPrincipal) && !Strings.isNullOrEmpty(configuredKeytabURI))) {
      throw new BadRequestException(String.format(""String_Node_Str"" + ""String_Node_Str"",configuredPrincipal,configuredKeytabURI));
    }
    hasValidKerberosConf=true;
  }
  if (!metadata.getConfig().isExploreAsPrincipal() && !hasValidKerberosConf) {
    throw new BadRequestException(String.format(""String_Node_Str"",NamespaceConfig.EXPLORE_AS_PRINCIPAL));
  }
  Principal principal=authenticationContext.getPrincipal();
  privilegesManager.grant(namespace,principal,EnumSet.allOf(Action.class));
  String executionUserName;
  if (SecurityUtil.isKerberosEnabled(cConf) && !NamespaceId.SYSTEM.equals(namespace)) {
    String namespacePrincipal=metadata.getConfig().getPrincipal();
    if (Strings.isNullOrEmpty(namespacePrincipal)) {
      executionUserName=SecurityUtil.getMasterPrincipal(cConf);
    }
 else {
      executionUserName=new KerberosName(namespacePrincipal).getShortName();
    }
  }
 else {
    executionUserName=UserGroupInformation.getCurrentUser().getShortUserName();
  }
  Principal executionUser=new Principal(executionUserName,Principal.PrincipalType.USER);
  privilegesManager.grant(namespace,executionUser,EnumSet.allOf(Action.class));
  nsStore.create(metadata);
  UserGroupInformation ugi;
  if (NamespaceId.DEFAULT.equals(namespace)) {
    ugi=UserGroupInformation.getCurrentUser();
  }
 else {
    ugi=impersonator.getUGI(namespace);
  }
  try {
    ImpersonationUtils.doAs(ugi,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.get().create(metadata);
        return null;
      }
    }
);
  }
 catch (  Throwable t) {
    deleteNamespaceMeta(metadata.getNamespaceId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace,t);
  }
  LOG.info(""String_Node_Str"",metadata.getNamespaceId(),metadata);
}","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override @AuthEnforce(entities=""String_Node_Str"",enforceOn=InstanceId.class,actions=Action.ADMIN) public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=metadata.getNamespaceId();
  if (exists(namespace)) {
    throw new NamespaceAlreadyExistsException(namespace);
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  boolean hasValidKerberosConf=false;
  if (metadata.getConfig() != null) {
    String configuredPrincipal=metadata.getConfig().getPrincipal();
    String configuredKeytabURI=metadata.getConfig().getKeytabURI();
    if ((!Strings.isNullOrEmpty(configuredPrincipal) && Strings.isNullOrEmpty(configuredKeytabURI)) || (Strings.isNullOrEmpty(configuredPrincipal) && !Strings.isNullOrEmpty(configuredKeytabURI))) {
      throw new BadRequestException(String.format(""String_Node_Str"" + ""String_Node_Str"",configuredPrincipal,configuredKeytabURI));
    }
    hasValidKerberosConf=true;
  }
  if (!metadata.getConfig().isExploreAsPrincipal() && !hasValidKerberosConf) {
    throw new BadRequestException(String.format(""String_Node_Str"",NamespaceConfig.EXPLORE_AS_PRINCIPAL));
  }
  Principal principal=authenticationContext.getPrincipal();
  privilegesManager.grant(namespace,principal,EnumSet.allOf(Action.class));
  String executionUserName;
  if (SecurityUtil.isKerberosEnabled(cConf) && !NamespaceId.SYSTEM.equals(namespace)) {
    String namespacePrincipal=metadata.getConfig().getPrincipal();
    if (Strings.isNullOrEmpty(namespacePrincipal)) {
      executionUserName=SecurityUtil.getMasterPrincipal(cConf);
    }
 else {
      executionUserName=new KerberosName(namespacePrincipal).getShortName();
    }
  }
 else {
    executionUserName=UserGroupInformation.getCurrentUser().getShortUserName();
  }
  Principal executionUser=new Principal(executionUserName,Principal.PrincipalType.USER);
  privilegesManager.grant(namespace,executionUser,EnumSet.allOf(Action.class));
  nsStore.create(metadata);
  try {
    UserGroupInformation ugi;
    if (NamespaceId.DEFAULT.equals(namespace)) {
      ugi=UserGroupInformation.getCurrentUser();
    }
 else {
      ugi=impersonator.getUGI(namespace);
    }
    ImpersonationUtils.doAs(ugi,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.get().create(metadata);
        return null;
      }
    }
);
  }
 catch (  Throwable t) {
    deleteNamespaceMeta(metadata.getNamespaceId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace,t);
  }
  LOG.info(""String_Node_Str"",metadata.getNamespaceId(),metadata);
}"
5322,"@GET @Path(""String_Node_Str"") public void searchMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String searchQuery,@QueryParam(""String_Node_Str"") List<String> targets,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String sort,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int offset,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int limit,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numCursors,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String cursor,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") boolean showHidden,@Nullable @QueryParam(""String_Node_Str"") String entityScope) throws Exception {
  Set<EntityTypeSimpleName> types=Collections.emptySet();
  if (targets != null) {
    types=ImmutableSet.copyOf(Iterables.transform(targets,STRING_TO_TARGET_TYPE));
  }
  SortInfo sortInfo=SortInfo.of(URLDecoder.decode(sort,""String_Node_Str""));
  if (SortInfo.DEFAULT.equals(sortInfo)) {
    if (!(cursor.isEmpty()) || 0 != numCursors) {
      throw new BadRequestException(""String_Node_Str"");
    }
  }
  try {
    MetadataSearchResponse response=metadataAdmin.search(namespaceId,URLDecoder.decode(searchQuery,""String_Node_Str""),types,sortInfo,offset,limit,numCursors,cursor,showHidden,validateEntityScope(entityScope));
    responder.sendJson(HttpResponseStatus.OK,response,MetadataSearchResponse.class,GSON);
  }
 catch (  Exception e) {
    if (Throwables.getRootCause(e) instanceof IllegalArgumentException) {
      throw new BadRequestException(e.getMessage(),e);
    }
    throw e;
  }
}","@GET @Path(""String_Node_Str"") public void searchMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String searchQuery,@QueryParam(""String_Node_Str"") List<String> targets,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String sort,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int offset,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int limit,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numCursors,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String cursor,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") boolean showHidden,@Nullable @QueryParam(""String_Node_Str"") String entityScope) throws Exception {
  Set<EntityTypeSimpleName> types=Collections.emptySet();
  if (targets != null) {
    types=ImmutableSet.copyOf(Iterables.transform(targets,STRING_TO_TARGET_TYPE));
  }
  SortInfo sortInfo=SortInfo.of(URLDecoder.decode(sort,""String_Node_Str""));
  if (SortInfo.DEFAULT.equals(sortInfo)) {
    if (!(cursor.isEmpty()) || 0 != numCursors) {
      throw new BadRequestException(""String_Node_Str"");
    }
  }
  try {
    MetadataSearchResponse response=metadataAdmin.search(namespaceId,URLDecoder.decode(searchQuery,""String_Node_Str""),types,sortInfo,offset,limit,numCursors,cursor,showHidden,validateEntityScope(entityScope));
    responder.sendJson(HttpResponseStatus.OK,response,MetadataSearchResponse.class,GSON);
  }
 catch (  Exception e) {
    if (Throwables.getRootCause(e) instanceof BadRequestException) {
      throw new BadRequestException(e.getMessage(),e);
    }
    throw e;
  }
}"
5323,"/** 
 * Searches entities that match the specified search query in the specified namespace and   {@link NamespaceId#SYSTEM}for the specified   {@link EntityTypeSimpleName}.
 * @param namespaceId the namespace to search in
 * @param searchQuery the search query, which could be of two forms: [key]:[value] or just [value] and can have '*'at the end for a prefix search
 * @param types the {@link EntityTypeSimpleName} to restrict the search to, if empty all types are searched
 * @param sortInfo the {@link SortInfo} to sort the results by
 * @param offset index to start with in the search results. To return results from the beginning, pass {@code 0}. Only applies when #sortInfo is not   {@link SortInfo#DEFAULT}
 * @param limit number of results to return, starting from #offset. To return all, pass {@link Integer#MAX_VALUE}. Only applies when #sortInfo is not   {@link SortInfo#DEFAULT}
 * @param numCursors number of cursors to return in the response. A cursor identifies the first index of thenext page for pagination purposes. Only applies when #sortInfo is not  {@link SortInfo#DEFAULT}. Defaults to   {@code 0}
 * @param cursor the cursor that acts as the starting index for the requested page. This is only applicable when#sortInfo is not  {@link SortInfo#DEFAULT}. If offset is also specified, it is applied starting at the cursor. If   {@code null}, the first row is used as the cursor
 * @param showHidden boolean which specifies whether to display hidden entities (entity whose name start with ""_"")or not.
 * @param entityScope a set which specifies which scope of entities to display.
 * @return a {@link SearchResults} object containing a list of {@link MetadataEntry} containing each matching{@link NamespacedEntityId} with its associated metadata. It also optionally contains a list of cursorsfor subsequent queries to start with, if the specified #sortInfo is not  {@link SortInfo#DEFAULT}.
 */
public SearchResults search(String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,@Nullable String cursor,boolean showHidden,Set<EntityScope> entityScope){
  if (!SortInfo.DEFAULT.equals(sortInfo)) {
    if (!""String_Node_Str"".equals(searchQuery)) {
      throw new IllegalArgumentException(""String_Node_Str"");
    }
    return searchByCustomIndex(namespaceId,types,sortInfo,offset,limit,numCursors,cursor,showHidden,entityScope);
  }
  return searchByDefaultIndex(namespaceId,searchQuery,types,showHidden,entityScope);
}","/** 
 * Searches entities that match the specified search query in the specified namespace and   {@link NamespaceId#SYSTEM}for the specified   {@link EntityTypeSimpleName}.
 * @param namespaceId the namespace to search in
 * @param searchQuery the search query, which could be of two forms: [key]:[value] or just [value] and can have '*'at the end for a prefix search
 * @param types the {@link EntityTypeSimpleName} to restrict the search to, if empty all types are searched
 * @param sortInfo the {@link SortInfo} to sort the results by
 * @param offset index to start with in the search results. To return results from the beginning, pass {@code 0}. Only applies when #sortInfo is not   {@link SortInfo#DEFAULT}
 * @param limit number of results to return, starting from #offset. To return all, pass {@link Integer#MAX_VALUE}. Only applies when #sortInfo is not   {@link SortInfo#DEFAULT}
 * @param numCursors number of cursors to return in the response. A cursor identifies the first index of thenext page for pagination purposes. Only applies when #sortInfo is not  {@link SortInfo#DEFAULT}. Defaults to   {@code 0}
 * @param cursor the cursor that acts as the starting index for the requested page. This is only applicable when#sortInfo is not  {@link SortInfo#DEFAULT}. If offset is also specified, it is applied starting at the cursor. If   {@code null}, the first row is used as the cursor
 * @param showHidden boolean which specifies whether to display hidden entities (entity whose name start with ""_"")or not.
 * @param entityScope a set which specifies which scope of entities to display.
 * @return a {@link SearchResults} object containing a list of {@link MetadataEntry} containing each matching{@link NamespacedEntityId} with its associated metadata. It also optionally contains a list of cursorsfor subsequent queries to start with, if the specified #sortInfo is not  {@link SortInfo#DEFAULT}.
 */
public SearchResults search(String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,@Nullable String cursor,boolean showHidden,Set<EntityScope> entityScope) throws BadRequestException {
  if (!SortInfo.DEFAULT.equals(sortInfo)) {
    if (!""String_Node_Str"".equals(searchQuery)) {
      throw new BadRequestException(""String_Node_Str"");
    }
    return searchByCustomIndex(namespaceId,types,sortInfo,offset,limit,numCursors,cursor,showHidden,entityScope);
  }
  return searchByDefaultIndex(namespaceId,searchQuery,types,showHidden,entityScope);
}"
5324,"private List<MetadataEntry> searchByDefaultIndex(MetadataDataset dataset,String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types){
  return dataset.search(namespaceId,searchQuery,types,SortInfo.DEFAULT,0,Integer.MAX_VALUE,1,null,false,EnumSet.allOf(EntityScope.class)).getResults();
}","private List<MetadataEntry> searchByDefaultIndex(MetadataDataset dataset,String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types) throws BadRequestException {
  return dataset.search(namespaceId,searchQuery,types,SortInfo.DEFAULT,0,Integer.MAX_VALUE,1,null,false,EnumSet.allOf(EntityScope.class)).getResults();
}"
5325,"@GET @Path(""String_Node_Str"") public void searchMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String searchQuery,@QueryParam(""String_Node_Str"") List<String> targets,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String sort,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int offset,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int limit,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numCursors,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String cursor,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") boolean showHidden,@Nullable @QueryParam(""String_Node_Str"") String entityScope) throws Exception {
  Set<EntityTypeSimpleName> types=Collections.emptySet();
  if (targets != null) {
    types=ImmutableSet.copyOf(Iterables.transform(targets,STRING_TO_TARGET_TYPE));
  }
  SortInfo sortInfo=SortInfo.of(URLDecoder.decode(sort,""String_Node_Str""));
  if (SortInfo.DEFAULT.equals(sortInfo)) {
    if (!(cursor.isEmpty()) || 0 != numCursors) {
      throw new BadRequestException(""String_Node_Str"");
    }
  }
  try {
    MetadataSearchResponse response=metadataAdmin.search(namespaceId,URLDecoder.decode(searchQuery,""String_Node_Str""),types,sortInfo,offset,limit,numCursors,cursor,showHidden,validateEntityScope(entityScope));
    responder.sendJson(HttpResponseStatus.OK,response,MetadataSearchResponse.class,GSON);
  }
 catch (  Exception e) {
    if (Throwables.getRootCause(e) instanceof BadRequestException) {
      throw new BadRequestException(e.getMessage(),e);
    }
    throw e;
  }
}","@GET @Path(""String_Node_Str"") public void searchMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@QueryParam(""String_Node_Str"") String searchQuery,@QueryParam(""String_Node_Str"") List<String> targets,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String sort,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int offset,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int limit,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numCursors,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String cursor,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") boolean showHidden,@Nullable @QueryParam(""String_Node_Str"") String entityScope) throws Exception {
  if (searchQuery == null || searchQuery.isEmpty()) {
    throw new BadRequestException(""String_Node_Str"");
  }
  Set<EntityTypeSimpleName> types=Collections.emptySet();
  if (targets != null) {
    types=ImmutableSet.copyOf(Iterables.transform(targets,STRING_TO_TARGET_TYPE));
  }
  SortInfo sortInfo=SortInfo.of(URLDecoder.decode(sort,""String_Node_Str""));
  if (SortInfo.DEFAULT.equals(sortInfo)) {
    if (!(cursor.isEmpty()) || 0 != numCursors) {
      throw new BadRequestException(""String_Node_Str"");
    }
  }
  try {
    MetadataSearchResponse response=metadataAdmin.search(namespaceId,URLDecoder.decode(searchQuery,""String_Node_Str""),types,sortInfo,offset,limit,numCursors,cursor,showHidden,validateEntityScope(entityScope));
    responder.sendJson(HttpResponseStatus.OK,response,MetadataSearchResponse.class,GSON);
  }
 catch (  Exception e) {
    if (Throwables.getRootCause(e) instanceof BadRequestException) {
      throw new BadRequestException(e.getMessage(),e);
    }
    throw e;
  }
}"
5326,"@Test public void testInvalidSearchParams() throws Exception {
  NamespaceId namespace=new NamespaceId(""String_Node_Str"");
  Set<EntityTypeSimpleName> targets=EnumSet.allOf(EntityTypeSimpleName.class);
  try {
    searchMetadata(namespace,""String_Node_Str"",targets,AbstractSystemMetadataWriter.ENTITY_NAME_KEY);
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    searchMetadata(namespace,""String_Node_Str"",targets,""String_Node_Str"");
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    searchMetadata(namespace,""String_Node_Str"",targets,AbstractSystemMetadataWriter.ENTITY_NAME_KEY + ""String_Node_Str"");
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    searchMetadata(NamespaceId.DEFAULT,""String_Node_Str"",targets,null,0,Integer.MAX_VALUE,1,null);
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    searchMetadata(NamespaceId.DEFAULT,""String_Node_Str"",targets,null,0,Integer.MAX_VALUE,0,""String_Node_Str"");
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
}","@Test public void testInvalidSearchParams() throws Exception {
  NamespaceId namespace=new NamespaceId(""String_Node_Str"");
  Set<EntityTypeSimpleName> targets=EnumSet.allOf(EntityTypeSimpleName.class);
  try {
    searchMetadata(namespace,""String_Node_Str"",targets,AbstractSystemMetadataWriter.ENTITY_NAME_KEY);
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    searchMetadata(namespace,""String_Node_Str"",targets,""String_Node_Str"");
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    searchMetadata(namespace,""String_Node_Str"",targets,AbstractSystemMetadataWriter.ENTITY_NAME_KEY + ""String_Node_Str"");
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    searchMetadata(NamespaceId.DEFAULT,""String_Node_Str"",targets,null,0,Integer.MAX_VALUE,1,null);
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    searchMetadata(NamespaceId.DEFAULT,""String_Node_Str"",targets,null,0,Integer.MAX_VALUE,0,""String_Node_Str"");
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    searchMetadata(NamespaceId.DEFAULT,""String_Node_Str"");
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
}"
5327,"/** 
 * Patch hive classes by bytecode rewriting in the given source jar. Currently it rewrite the following classes: <ul> <li>  {@link HiveAuthFactory} - This is for skipping kerberos authentication from the explore service container.{@link SessionState} - This is to workaround a native memory leakage bug due tounclosed URLClassloaders, introduced by HIVE-14037. In normal Java process this leakage won't be a problem as eventually those URLClassLoaders will get GC and have the memory released. However, since explore container runs in YARN and YARN monitor the RSS memory usage, it is highly possible that the URLClassLoader won't get GC due to low heap memory usage, while already taken up all the allowed RSS memory. We don't need aux jars added inside the explore JVM since all CDAP classes are already in the classloader. We only use aux jars config to tell hive to localize CDAP jars to task containers. </li> </ul>
 * @param sourceJar the source jar to look for the {@link HiveAuthFactory} class.
 * @param targetJar the target jar to write to if rewrite happened
 * @return the source jar if there is no rewrite happened; the target jar if rewrite happened.
 * @throws IOException if failed to read/write to the jar files.
 */
public static File patchHiveClasses(File sourceJar,File targetJar) throws IOException {
  try (JarFile input=new JarFile(sourceJar)){
    boolean needPatch=false;
    for (    String classFile : HIVE_CLASS_FILES_TO_PATCH.keySet()) {
      needPatch=needPatch || (input.getEntry(classFile) != null);
    }
    if (!needPatch) {
      return sourceJar;
    }
    try (JarOutputStream output=new JarOutputStream(new FileOutputStream(targetJar))){
      Enumeration<JarEntry> sourceEntries=input.entries();
      while (sourceEntries.hasMoreElements()) {
        JarEntry entry=sourceEntries.nextElement();
        output.putNextEntry(new JarEntry(entry.getName()));
        try (InputStream entryInputStream=input.getInputStream(entry)){
          Set<String> patchMethods=HIVE_CLASS_FILES_TO_PATCH.get(entry.getName());
          if (patchMethods == null) {
            ByteStreams.copy(entryInputStream,output);
            continue;
          }
          output.write(rewriteMethodToNoop(entry.getName(),entryInputStream,patchMethods));
        }
       }
    }
     return targetJar;
  }
 }","/** 
 * Patch hive classes by bytecode rewriting in the given source jar. Currently it rewrite the following classes: <ul> <li>  {@link HiveAuthFactory} - This is for skipping kerberos authentication from the explore service container.{@link SessionState} - This is to workaround a native memory leakage bug due tounclosed URLClassloaders, introduced by HIVE-14037. In normal Java process this leakage won't be a problem as eventually those URLClassLoaders will get GC and have the memory released. However, since explore container runs in YARN and YARN monitor the RSS memory usage, it is highly possible that the URLClassLoader won't get GC due to low heap memory usage, while already taken up all the allowed RSS memory. We don't need aux jars added inside the explore JVM since all CDAP classes are already in the classloader. We only use aux jars config to tell hive to localize CDAP jars to task containers. </li> </ul>
 * @param sourceJar the source jar to look for the {@link HiveAuthFactory} class.
 * @param targetJar the target jar to write to if rewrite happened
 * @return the source jar if there is no rewrite happened; the target jar if rewrite happened.
 * @throws IOException if failed to read/write to the jar files.
 */
public static File patchHiveClasses(File sourceJar,File targetJar) throws IOException {
  try (JarFile input=new JarFile(sourceJar)){
    boolean needPatch=false;
    for (    String classFile : HIVE_CLASS_FILES_TO_PATCH.keySet()) {
      needPatch=needPatch || (input.getEntry(classFile) != null);
    }
    if (!needPatch) {
      return sourceJar;
    }
    try (JarOutputStream output=new JarOutputStream(new FileOutputStream(targetJar))){
      Enumeration<JarEntry> sourceEntries=input.entries();
      while (sourceEntries.hasMoreElements()) {
        JarEntry entry=sourceEntries.nextElement();
        output.putNextEntry(new JarEntry(entry.getName()));
        try (InputStream entryInputStream=input.getInputStream(entry)){
          Set<String> patchMethods=HIVE_CLASS_FILES_TO_PATCH.get(entry.getName());
          if (patchMethods == null) {
            ByteStreams.copy(entryInputStream,output);
            continue;
          }
          output.write(Classes.rewriteMethodToNoop(entry.getName(),entryInputStream,patchMethods));
        }
       }
    }
     return targetJar;
  }
 }"
5328,"@Override protected Class<?> loadClass(String name,boolean resolve) throws ClassNotFoundException {
  if (API_CLASSES.contains(name) || (!name.startsWith(""String_Node_Str"") && !name.startsWith(""String_Node_Str"") && !name.startsWith(""String_Node_Str"")&& !name.startsWith(""String_Node_Str"")&& !name.startsWith(""String_Node_Str"")&& !name.startsWith(""String_Node_Str"")&& !name.startsWith(""String_Node_Str""))) {
    return super.loadClass(name,resolve);
  }
  Class<?> cls=findLoadedClass(name);
  if (cls != null) {
    return cls;
  }
  try (InputStream is=openResource(name.replace('.','/') + ""String_Node_Str"")){
    if (is == null) {
      throw new ClassNotFoundException(""String_Node_Str"" + name);
    }
    if (name.equals(SPARK_CONTEXT_TYPE.getClassName())) {
      cls=defineContext(SPARK_CONTEXT_TYPE,is);
    }
 else     if (name.equals(SPARK_STREAMING_CONTEXT_TYPE.getClassName())) {
      cls=defineContext(SPARK_STREAMING_CONTEXT_TYPE,is);
    }
 else     if (name.equals(SPARK_CONF_TYPE.getClassName())) {
      cls=defineSparkConf(SPARK_CONF_TYPE,is);
    }
 else     if (name.startsWith(SPARK_SUBMIT_TYPE.getClassName())) {
      cls=rewriteSetPropertiesAndDefineClass(name,is);
    }
 else     if (name.equals(SPARK_YARN_CLIENT_TYPE.getClassName()) && rewriteYarnClient) {
      cls=defineClient(name,is);
    }
 else     if (name.equals(SPARK_DSTREAM_GRAPH_TYPE.getClassName())) {
      cls=defineDStreamGraph(name,is);
    }
 else     if (name.equals(AKKA_REMOTING_TYPE.getClassName())) {
      cls=defineAkkaRemoting(name,is);
    }
 else {
      cls=findClass(name);
    }
    if (resolve) {
      resolveClass(cls);
    }
    return cls;
  }
 catch (  IOException e) {
    throw new ClassNotFoundException(""String_Node_Str"" + name,e);
  }
}","@Override protected Class<?> loadClass(String name,boolean resolve) throws ClassNotFoundException {
  if (API_CLASSES.contains(name) || (!name.startsWith(""String_Node_Str"") && !name.startsWith(""String_Node_Str"") && !name.startsWith(""String_Node_Str"")&& !name.startsWith(""String_Node_Str"")&& !name.startsWith(""String_Node_Str"")&& !name.startsWith(""String_Node_Str"")&& !name.startsWith(""String_Node_Str""))) {
    return super.loadClass(name,resolve);
  }
  Class<?> cls=findLoadedClass(name);
  if (cls != null) {
    return cls;
  }
  try (InputStream is=openResource(name.replace('.','/') + ""String_Node_Str"")){
    if (is == null) {
      throw new ClassNotFoundException(""String_Node_Str"" + name);
    }
    if (name.equals(SPARK_CONTEXT_TYPE.getClassName())) {
      cls=defineContext(SPARK_CONTEXT_TYPE,is);
    }
 else     if (name.equals(SPARK_STREAMING_CONTEXT_TYPE.getClassName())) {
      cls=defineContext(SPARK_STREAMING_CONTEXT_TYPE,is);
    }
 else     if (name.equals(SPARK_CONF_TYPE.getClassName())) {
      cls=defineSparkConf(SPARK_CONF_TYPE,is);
    }
 else     if (name.startsWith(SPARK_SUBMIT_TYPE.getClassName())) {
      cls=rewriteSetPropertiesAndDefineClass(name,is);
    }
 else     if (name.equals(SPARK_YARN_CLIENT_TYPE.getClassName()) && rewriteYarnClient) {
      cls=defineClient(name,is);
    }
 else     if (name.equals(SPARK_DSTREAM_GRAPH_TYPE.getClassName())) {
      cls=defineDStreamGraph(name,is);
    }
 else     if (name.equals(AKKA_REMOTING_TYPE.getClassName())) {
      cls=defineAkkaRemoting(name,is);
    }
 else     if (name.equals(YARNSPARKHADOOPUTIL_TYPE.getClassName())) {
      cls=defineHadoopSparkHadoopUtil(name,is);
    }
 else {
      cls=findClass(name);
    }
    if (resolve) {
      resolveClass(cls);
    }
    return cls;
  }
 catch (  IOException e) {
    throw new ClassNotFoundException(""String_Node_Str"" + name,e);
  }
}"
5329,"@Override protected void startUp() throws Exception {
  LOG.info(""String_Node_Str"");
  httpService=new CommonNettyHttpServiceBuilder(cConf,Constants.Service.METADATA_SERVICE).addHttpHandlers(handlers).setHandlerHooks(ImmutableList.of(new MetricsReporterHook(metricsCollectionService,Constants.Service.METADATA_SERVICE))).setHost(cConf.get(Constants.Metadata.SERVICE_BIND_ADDRESS)).setPort(cConf.getInt(Constants.Metadata.SERVICE_BIND_PORT)).setWorkerThreadPoolSize(cConf.getInt(Constants.Metadata.SERVICE_WORKER_THREADS)).setExecThreadPoolSize(cConf.getInt(Constants.Metadata.SERVICE_EXEC_THREADS)).setConnectionBacklog(20000).build();
  httpService.addListener(new ServiceListenerAdapter(){
    private Cancellable cancellable;
    @Override public void running(){
      final InetSocketAddress socketAddress=httpService.getBindAddress();
      LOG.info(""String_Node_Str"",socketAddress);
      cancellable=discoveryService.register(ResolvingDiscoverable.of(new Discoverable(Constants.Service.METADATA_SERVICE,socketAddress)));
    }
    @Override public void terminated(    State from){
      LOG.info(""String_Node_Str"");
      cancellable.cancel();
    }
    @Override public void failed(    State from,    Throwable failure){
      LOG.info(""String_Node_Str"",failure);
      cancellable.cancel();
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  httpService.startAndWait();
}","@Override protected void startUp() throws Exception {
  LOG.info(""String_Node_Str"");
  metadataUpgrader.createOrUpgradeIfNecessary();
  httpService=new CommonNettyHttpServiceBuilder(cConf,Constants.Service.METADATA_SERVICE).addHttpHandlers(handlers).setHandlerHooks(ImmutableList.of(new MetricsReporterHook(metricsCollectionService,Constants.Service.METADATA_SERVICE))).setHost(cConf.get(Constants.Metadata.SERVICE_BIND_ADDRESS)).setPort(cConf.getInt(Constants.Metadata.SERVICE_BIND_PORT)).setWorkerThreadPoolSize(cConf.getInt(Constants.Metadata.SERVICE_WORKER_THREADS)).setExecThreadPoolSize(cConf.getInt(Constants.Metadata.SERVICE_EXEC_THREADS)).setConnectionBacklog(20000).build();
  httpService.addListener(new ServiceListenerAdapter(){
    private Cancellable cancellable;
    @Override public void running(){
      final InetSocketAddress socketAddress=httpService.getBindAddress();
      LOG.info(""String_Node_Str"",socketAddress);
      cancellable=discoveryService.register(ResolvingDiscoverable.of(new Discoverable(Constants.Service.METADATA_SERVICE,socketAddress)));
    }
    @Override public void terminated(    State from){
      LOG.info(""String_Node_Str"");
      cancellable.cancel();
    }
    @Override public void failed(    State from,    Throwable failure){
      LOG.info(""String_Node_Str"",failure);
      cancellable.cancel();
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  httpService.startAndWait();
}"
5330,"@Inject MetadataService(CConfiguration cConf,MetricsCollectionService metricsCollectionService,DiscoveryService discoveryService,@Named(Constants.Metadata.HANDLERS_NAME) Set<HttpHandler> handlers){
  this.cConf=cConf;
  this.metricsCollectionService=metricsCollectionService;
  this.discoveryService=discoveryService;
  this.handlers=handlers;
}","@Inject MetadataService(CConfiguration cConf,MetricsCollectionService metricsCollectionService,DiscoveryService discoveryService,@Named(Constants.Metadata.HANDLERS_NAME) Set<HttpHandler> handlers,MetadataUpgrader metadataUpgrader){
  this.cConf=cConf;
  this.metricsCollectionService=metricsCollectionService;
  this.discoveryService=discoveryService;
  this.handlers=handlers;
  this.metadataUpgrader=metadataUpgrader;
}"
5331,"@Override public DatasetSpecification reconfigure(String instanceName,DatasetProperties newProperties,DatasetSpecification currentSpec) throws IncompatibleUpdateException {
  DatasetSpecification indexSpec=currentSpec.getSpecification(METADATA_INDEX_TABLE_NAME);
  String indexColumn=indexSpec.getProperty(IndexedTable.INDEX_COLUMNS_CONF_KEY);
  return DatasetSpecification.builder(instanceName,getName()).properties(newProperties.getProperties()).datasets(AbstractDatasetDefinition.reconfigure(indexedTableDef,METADATA_INDEX_TABLE_NAME,addIndexColumns(newProperties,indexColumn),indexSpec)).build();
}","@Override public DatasetSpecification reconfigure(String instanceName,DatasetProperties newProperties,DatasetSpecification currentSpec) throws IncompatibleUpdateException {
  return configure(instanceName,newProperties);
}"
5332,"@Inject public DefaultConfigStore(DatasetFramework datasetFramework,TransactionSystemClient txClient){
  this.datasetFramework=datasetFramework;
  this.transactional=Transactions.createTransactionalWithRetry(Transactions.createTransactional(new MultiThreadDatasetCache(new SystemDatasetInstantiator(datasetFramework),txClient,NamespaceId.SYSTEM,ImmutableMap.<String,String>of(),null,null)),RetryStrategies.retryOnConflict(20,100));
}","@Inject public DefaultConfigStore(DatasetFramework datasetFramework,TransactionSystemClient txClient){
  this.datasetFramework=datasetFramework;
  this.transactional=Transactions.createTransactionalWithRetry(Transactions.createTransactional(new MultiThreadDatasetCache(new SystemDatasetInstantiator(datasetFramework),new TransactionSystemClientAdapter(txClient),NamespaceId.SYSTEM,ImmutableMap.<String,String>of(),null,null)),RetryStrategies.retryOnConflict(20,100));
}"
5333,"@Inject public TransactionHttpHandler(CConfiguration cConf,TransactionSystemClient txClient){
  this.txClient=txClient;
  try {
    this.debugClazz=getClass().getClassLoader().loadClass(""String_Node_Str"");
    this.debugObject=debugClazz.newInstance();
    Configuration hConf=new Configuration();
    for (    Map.Entry<String,String> entry : cConf) {
      hConf.set(entry.getKey(),entry.getValue());
    }
    Method initMethod=debugClazz.getMethod(""String_Node_Str"",Configuration.class);
    initMethod.setAccessible(true);
    initMethod.invoke(debugObject,hConf);
  }
 catch (  ClassNotFoundException|IllegalAccessException|InstantiationException ex) {
    LOG.warn(""String_Node_Str"",ex);
    this.debugClazz=null;
  }
catch (  NoSuchMethodException|InvocationTargetException ex) {
    LOG.warn(""String_Node_Str"",ex);
    this.debugClazz=null;
  }
}","@Inject public TransactionHttpHandler(CConfiguration cConf,TransactionSystemClient txClient){
  this.txClient=new TransactionSystemClientAdapter(txClient);
  try {
    this.debugClazz=getClass().getClassLoader().loadClass(""String_Node_Str"");
    this.debugObject=debugClazz.newInstance();
    Configuration hConf=new Configuration();
    for (    Map.Entry<String,String> entry : cConf) {
      hConf.set(entry.getKey(),entry.getValue());
    }
    Method initMethod=debugClazz.getMethod(""String_Node_Str"",Configuration.class);
    initMethod.setAccessible(true);
    initMethod.invoke(debugObject,hConf);
  }
 catch (  ClassNotFoundException|IllegalAccessException|InstantiationException ex) {
    LOG.warn(""String_Node_Str"",ex);
    this.debugClazz=null;
  }
catch (  NoSuchMethodException|InvocationTargetException ex) {
    LOG.warn(""String_Node_Str"",ex);
    this.debugClazz=null;
  }
}"
5334,"@Inject ArtifactStore(DatasetFramework datasetFramework,NamespacedLocationFactory namespacedLocationFactory,LocationFactory locationFactory,TransactionSystemClient txClient,Impersonator impersonator){
  this.locationFactory=locationFactory;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.datasetFramework=datasetFramework;
  this.transactional=Transactions.createTransactionalWithRetry(Transactions.createTransactional(new MultiThreadDatasetCache(new SystemDatasetInstantiator(datasetFramework),txClient,META_ID.getParent(),Collections.<String,String>emptyMap(),null,null)),RetryStrategies.retryOnConflict(20,100));
  this.impersonator=impersonator;
}","@Inject ArtifactStore(DatasetFramework datasetFramework,NamespacedLocationFactory namespacedLocationFactory,LocationFactory locationFactory,TransactionSystemClient txClient,Impersonator impersonator){
  this.locationFactory=locationFactory;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.datasetFramework=datasetFramework;
  this.transactional=Transactions.createTransactionalWithRetry(Transactions.createTransactional(new MultiThreadDatasetCache(new SystemDatasetInstantiator(datasetFramework),new TransactionSystemClientAdapter(txClient),META_ID.getParent(),Collections.<String,String>emptyMap(),null,null)),RetryStrategies.retryOnConflict(20,100));
  this.impersonator=impersonator;
}"
5335,"@Inject public DefaultStore(CConfiguration conf,DatasetFramework framework,TransactionSystemClient txClient){
  this.configuration=conf;
  this.dsFramework=framework;
  this.transactional=Transactions.createTransactionalWithRetry(Transactions.createTransactional(new MultiThreadDatasetCache(new SystemDatasetInstantiator(framework),txClient,NamespaceId.SYSTEM,ImmutableMap.<String,String>of(),null,null)),RetryStrategies.retryOnConflict(20,100));
}","@Inject public DefaultStore(CConfiguration conf,DatasetFramework framework,TransactionSystemClient txClient){
  this.configuration=conf;
  this.dsFramework=framework;
  this.transactional=Transactions.createTransactionalWithRetry(Transactions.createTransactional(new MultiThreadDatasetCache(new SystemDatasetInstantiator(framework),new TransactionSystemClientAdapter(txClient),NamespaceId.SYSTEM,ImmutableMap.<String,String>of(),null,null)),RetryStrategies.retryOnConflict(20,100));
}"
5336,"@Inject public MDSStreamMetaStore(DatasetFramework dsFramework,TransactionSystemClient txClient){
  this.datasetFramework=dsFramework;
  this.transactional=Transactions.createTransactionalWithRetry(Transactions.createTransactional(new MultiThreadDatasetCache(new SystemDatasetInstantiator(datasetFramework),txClient,NamespaceId.SYSTEM,ImmutableMap.<String,String>of(),null,null)),RetryStrategies.retryOnConflict(20,100));
}","@Inject public MDSStreamMetaStore(DatasetFramework dsFramework,TransactionSystemClient txClient){
  this.datasetFramework=dsFramework;
  this.transactional=Transactions.createTransactionalWithRetry(Transactions.createTransactional(new MultiThreadDatasetCache(new SystemDatasetInstantiator(datasetFramework),new TransactionSystemClientAdapter(txClient),NamespaceId.SYSTEM,ImmutableMap.<String,String>of(),null,null)),RetryStrategies.retryOnConflict(20,100));
}"
5337,"@Inject public DatasetInstanceManager(TransactionSystemClientService txClientService,TransactionExecutorFactory txExecutorFactory,@Named(""String_Node_Str"") DatasetFramework datasetFramework){
  this.txExecutorFactory=txExecutorFactory;
  Map<String,String> emptyArgs=Collections.emptyMap();
  this.datasetCache=new MultiThreadDatasetCache(new SystemDatasetInstantiator(datasetFramework),txClientService,NamespaceId.SYSTEM,emptyArgs,null,ImmutableMap.of(DatasetMetaTableUtil.INSTANCE_TABLE_NAME,emptyArgs));
}","@Inject public DatasetInstanceManager(TransactionSystemClientService txClientService,TransactionExecutorFactory txExecutorFactory,@Named(""String_Node_Str"") DatasetFramework datasetFramework){
  this.txExecutorFactory=txExecutorFactory;
  Map<String,String> emptyArgs=Collections.emptyMap();
  this.datasetCache=new MultiThreadDatasetCache(new SystemDatasetInstantiator(datasetFramework),new TransactionSystemClientAdapter(txClientService),NamespaceId.SYSTEM,emptyArgs,null,ImmutableMap.of(DatasetMetaTableUtil.INSTANCE_TABLE_NAME,emptyArgs));
}"
5338,"/** 
 * Executes the given runnable with a transaction.
 * @param transactional the {@link Transactional} used to submit the task
 * @param runnable task
 * @throws ServiceUnavailableException when the transaction service is not running
 * @throws RuntimeException for any other errors that occurs
 */
public static void executeUnchecked(Transactional transactional,final TxRunnable runnable){
  executeUnchecked(transactional,new TxCallable<Void>(){
    @Override public Void call(    DatasetContext context) throws Exception {
      runnable.run(context);
      return null;
    }
  }
);
}","/** 
 * Executes the given runnable with a transaction. Think twice before you call this. Usages of this method likely indicate poor exception handling.
 * @param transactional the {@link Transactional} used to submit the task
 * @param runnable task
 * @throws RuntimeException for errors that occur
 */
public static void executeUnchecked(Transactional transactional,final TxRunnable runnable){
  executeUnchecked(transactional,new TxCallable<Void>(){
    @Override public Void call(    DatasetContext context) throws Exception {
      runnable.run(context);
      return null;
    }
  }
);
}"
5339,"@Override @Nullable protected Configuration getConfiguration(CoprocessorEnvironment env){
  CConfiguration cConf=getCConfCache((RegionCoprocessorEnvironment)env).getCConf();
  if (cConf == null) {
    return null;
  }
  Configuration hConf=new Configuration();
  for (  Map.Entry<String,String> entry : cConf) {
    hConf.set(entry.getKey(),entry.getValue());
  }
  return hConf;
}","@Override @Nullable protected Configuration getConfiguration(CoprocessorEnvironment env){
  CConfiguration cConf=cConfCache.getCConf();
  if (cConf == null) {
    return null;
  }
  Configuration hConf=new Configuration();
  for (  Map.Entry<String,String> entry : cConf) {
    hConf.set(entry.getKey(),entry.getValue());
  }
  return hConf;
}"
5340,"@Override public void stop(CoprocessorEnvironment e) throws IOException {
  super.stop(e);
  if (cConfCache != null) {
    cConfCache.stop();
  }
}","@Override public void stop(CoprocessorEnvironment e) throws IOException {
  super.stop(e);
  cConfCacheSupplier.release();
}"
5341,"@Override public void start(CoprocessorEnvironment e) throws IOException {
  if (e instanceof RegionCoprocessorEnvironment) {
    RegionCoprocessorEnvironment env=(RegionCoprocessorEnvironment)e;
    HTableDescriptor tableDesc=env.getRegion().getTableDesc();
    String hbaseNamespacePrefix=tableDesc.getValue(Constants.Dataset.TABLE_PREFIX);
    this.sysConfigTablePrefix=HTableNameConverter.getSysConfigTablePrefix(hbaseNamespacePrefix);
    this.cConfCache=createCConfCache(env);
  }
  super.start(e);
}","@Override public void start(CoprocessorEnvironment e) throws IOException {
  if (e instanceof RegionCoprocessorEnvironment) {
    RegionCoprocessorEnvironment env=(RegionCoprocessorEnvironment)e;
    HTableDescriptor tableDesc=env.getRegion().getTableDesc();
    String hbaseNamespacePrefix=tableDesc.getValue(Constants.Dataset.TABLE_PREFIX);
    this.sysConfigTablePrefix=HTableNameConverter.getSysConfigTablePrefix(hbaseNamespacePrefix);
    this.cConfCacheSupplier=new CConfigurationCacheSupplier(env.getConfiguration(),sysConfigTablePrefix);
    this.cConfCache=cConfCacheSupplier.get();
  }
  super.start(e);
}"
5342,"@Override public void start(CoprocessorEnvironment env) throws IOException {
  if (env instanceof RegionCoprocessorEnvironment) {
    HTableDescriptor tableDesc=((RegionCoprocessorEnvironment)env).getRegion().getTableDesc();
    metadataTableNamespace=tableDesc.getValue(Constants.MessagingSystem.HBASE_METADATA_TABLE_NAMESPACE);
    hbaseNamespacePrefix=tableDesc.getValue(Constants.Dataset.TABLE_PREFIX);
    prefixLength=Integer.valueOf(tableDesc.getValue(Constants.MessagingSystem.HBASE_MESSAGING_TABLE_PREFIX_NUM_BYTES));
    String sysConfigTablePrefix=HTableNameConverter.getSysConfigTablePrefix(hbaseNamespacePrefix);
    cConfReader=new CConfigurationReader(env.getConfiguration(),sysConfigTablePrefix);
    Supplier<TransactionStateCache> cacheSupplier=getTransactionStateCacheSupplier(hbaseNamespacePrefix,env.getConfiguration());
    txStateCache=cacheSupplier.get();
    topicMetadataCache=createTopicMetadataCache((RegionCoprocessorEnvironment)env);
  }
}","@Override public void start(CoprocessorEnvironment e) throws IOException {
  if (e instanceof RegionCoprocessorEnvironment) {
    RegionCoprocessorEnvironment env=(RegionCoprocessorEnvironment)e;
    HTableDescriptor tableDesc=env.getRegion().getTableDesc();
    String metadataTableNamespace=tableDesc.getValue(Constants.MessagingSystem.HBASE_METADATA_TABLE_NAMESPACE);
    String hbaseNamespacePrefix=tableDesc.getValue(Constants.Dataset.TABLE_PREFIX);
    prefixLength=Integer.valueOf(tableDesc.getValue(Constants.MessagingSystem.HBASE_MESSAGING_TABLE_PREFIX_NUM_BYTES));
    String sysConfigTablePrefix=HTableNameConverter.getSysConfigTablePrefix(hbaseNamespacePrefix);
    CConfigurationReader cConfReader=new CConfigurationReader(env.getConfiguration(),sysConfigTablePrefix);
    Supplier<TransactionStateCache> cacheSupplier=getTransactionStateCacheSupplier(hbaseNamespacePrefix,env.getConfiguration());
    txStateCache=cacheSupplier.get();
    topicMetadataCacheSupplier=new TopicMetadataCacheSupplier(env,cConfReader,hbaseNamespacePrefix,metadataTableNamespace,new DefaultScanBuilder());
    topicMetadataCache=topicMetadataCacheSupplier.get();
  }
}"
5343,"@Override public InternalScanner preFlushScannerOpen(ObserverContext<RegionCoprocessorEnvironment> c,Store store,KeyValueScanner memstoreScanner,InternalScanner s) throws IOException {
  TopicMetadataCache metadataCache=getTopicMetadataCache(c.getEnvironment());
  LOG.info(""String_Node_Str"");
  TransactionVisibilityState txVisibilityState=txStateCache.getLatestState();
  Scan scan=new Scan();
  scan.setFilter(new MessageDataFilter(c.getEnvironment(),System.currentTimeMillis(),prefixLength,metadataCache,txVisibilityState));
  return new LoggingInternalScanner(""String_Node_Str"",""String_Node_Str"",new StoreScanner(store,store.getScanInfo(),scan,Collections.singletonList(memstoreScanner),ScanType.COMPACT_DROP_DELETES,store.getSmallestReadPoint(),HConstants.OLDEST_TIMESTAMP),txVisibilityState);
}","@Override public InternalScanner preFlushScannerOpen(ObserverContext<RegionCoprocessorEnvironment> c,Store store,KeyValueScanner memstoreScanner,InternalScanner s) throws IOException {
  LOG.info(""String_Node_Str"");
  TransactionVisibilityState txVisibilityState=txStateCache.getLatestState();
  Scan scan=new Scan();
  scan.setFilter(new MessageDataFilter(c.getEnvironment(),System.currentTimeMillis(),prefixLength,topicMetadataCache,txVisibilityState));
  return new LoggingInternalScanner(""String_Node_Str"",""String_Node_Str"",new StoreScanner(store,store.getScanInfo(),scan,Collections.singletonList(memstoreScanner),ScanType.COMPACT_DROP_DELETES,store.getSmallestReadPoint(),HConstants.OLDEST_TIMESTAMP),txVisibilityState);
}"
5344,"@Override public InternalScanner preCompactScannerOpen(ObserverContext<RegionCoprocessorEnvironment> c,Store store,List<? extends KeyValueScanner> scanners,ScanType scanType,long earliestPutTs,InternalScanner s,CompactionRequest request) throws IOException {
  TopicMetadataCache metadataCache=getTopicMetadataCache(c.getEnvironment());
  LOG.info(""String_Node_Str"");
  TransactionVisibilityState txVisibilityState=txStateCache.getLatestState();
  if (pruneEnable == null) {
    CConfiguration cConf=metadataCache.getCConfiguration();
    if (cConf != null) {
      pruneEnable=cConf.getBoolean(TxConstants.TransactionPruning.PRUNE_ENABLE,TxConstants.TransactionPruning.DEFAULT_PRUNE_ENABLE);
      if (Boolean.TRUE.equals(pruneEnable)) {
        String pruneTable=cConf.get(TxConstants.TransactionPruning.PRUNE_STATE_TABLE,TxConstants.TransactionPruning.DEFAULT_PRUNE_STATE_TABLE);
        long pruneFlushInterval=TimeUnit.SECONDS.toMillis(cConf.getLong(TxConstants.TransactionPruning.PRUNE_FLUSH_INTERVAL,TxConstants.TransactionPruning.DEFAULT_PRUNE_FLUSH_INTERVAL));
        compactionState=new CompactionState(c.getEnvironment(),TableName.valueOf(pruneTable),pruneFlushInterval);
        LOG.debug(""String_Node_Str"" + pruneTable);
      }
    }
  }
  if (Boolean.TRUE.equals(pruneEnable)) {
    compactionState.record(request,txVisibilityState);
  }
  Scan scan=new Scan();
  scan.setFilter(new MessageDataFilter(c.getEnvironment(),System.currentTimeMillis(),prefixLength,metadataCache,txVisibilityState));
  return new LoggingInternalScanner(""String_Node_Str"",""String_Node_Str"",new StoreScanner(store,store.getScanInfo(),scan,scanners,scanType,store.getSmallestReadPoint(),earliestPutTs),txVisibilityState);
}","@Override public InternalScanner preCompactScannerOpen(ObserverContext<RegionCoprocessorEnvironment> c,Store store,List<? extends KeyValueScanner> scanners,ScanType scanType,long earliestPutTs,InternalScanner s,CompactionRequest request) throws IOException {
  LOG.info(""String_Node_Str"");
  TransactionVisibilityState txVisibilityState=txStateCache.getLatestState();
  if (pruneEnable == null) {
    CConfiguration cConf=topicMetadataCache.getCConfiguration();
    if (cConf != null) {
      pruneEnable=cConf.getBoolean(TxConstants.TransactionPruning.PRUNE_ENABLE,TxConstants.TransactionPruning.DEFAULT_PRUNE_ENABLE);
      if (Boolean.TRUE.equals(pruneEnable)) {
        String pruneTable=cConf.get(TxConstants.TransactionPruning.PRUNE_STATE_TABLE,TxConstants.TransactionPruning.DEFAULT_PRUNE_STATE_TABLE);
        long pruneFlushInterval=TimeUnit.SECONDS.toMillis(cConf.getLong(TxConstants.TransactionPruning.PRUNE_FLUSH_INTERVAL,TxConstants.TransactionPruning.DEFAULT_PRUNE_FLUSH_INTERVAL));
        compactionState=new CompactionState(c.getEnvironment(),TableName.valueOf(pruneTable),pruneFlushInterval);
        LOG.debug(""String_Node_Str"" + pruneTable);
      }
    }
  }
  if (Boolean.TRUE.equals(pruneEnable)) {
    compactionState.record(request,txVisibilityState);
  }
  Scan scan=new Scan();
  scan.setFilter(new MessageDataFilter(c.getEnvironment(),System.currentTimeMillis(),prefixLength,topicMetadataCache,txVisibilityState));
  return new LoggingInternalScanner(""String_Node_Str"",""String_Node_Str"",new StoreScanner(store,store.getScanInfo(),scan,scanners,scanType,store.getSmallestReadPoint(),earliestPutTs),txVisibilityState);
}"
5345,"@Override public void stop(CoprocessorEnvironment e){
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
  if (compactionState != null) {
    compactionState.stop();
  }
}","@Override public void stop(CoprocessorEnvironment e){
  topicMetadataCacheSupplier.release();
  if (compactionState != null) {
    compactionState.stop();
  }
}"
5346,"@Override public void stop(CoprocessorEnvironment e) throws IOException {
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
}","@Override public void stop(CoprocessorEnvironment e) throws IOException {
  topicMetadataCacheSupplier.release();
}"
5347,"@Override public void start(CoprocessorEnvironment env) throws IOException {
  if (env instanceof RegionCoprocessorEnvironment) {
    HTableDescriptor tableDesc=((RegionCoprocessorEnvironment)env).getRegion().getTableDesc();
    metadataTableNamespace=tableDesc.getValue(Constants.MessagingSystem.HBASE_METADATA_TABLE_NAMESPACE);
    hbaseNamespacePrefix=tableDesc.getValue(Constants.Dataset.TABLE_PREFIX);
    prefixLength=Integer.valueOf(tableDesc.getValue(Constants.MessagingSystem.HBASE_MESSAGING_TABLE_PREFIX_NUM_BYTES));
    String sysConfigTablePrefix=HTableNameConverter.getSysConfigTablePrefix(hbaseNamespacePrefix);
    cConfReader=new CConfigurationReader(env.getConfiguration(),sysConfigTablePrefix);
    topicMetadataCache=createTopicMetadataCache((RegionCoprocessorEnvironment)env);
  }
}","@Override public void start(CoprocessorEnvironment e) throws IOException {
  if (e instanceof RegionCoprocessorEnvironment) {
    RegionCoprocessorEnvironment env=(RegionCoprocessorEnvironment)e;
    HTableDescriptor tableDesc=env.getRegion().getTableDesc();
    String metadataTableNamespace=tableDesc.getValue(Constants.MessagingSystem.HBASE_METADATA_TABLE_NAMESPACE);
    String hbaseNamespacePrefix=tableDesc.getValue(Constants.Dataset.TABLE_PREFIX);
    prefixLength=Integer.valueOf(tableDesc.getValue(Constants.MessagingSystem.HBASE_MESSAGING_TABLE_PREFIX_NUM_BYTES));
    String sysConfigTablePrefix=HTableNameConverter.getSysConfigTablePrefix(hbaseNamespacePrefix);
    CConfigurationReader cConfReader=new CConfigurationReader(env.getConfiguration(),sysConfigTablePrefix);
    topicMetadataCacheSupplier=new TopicMetadataCacheSupplier(env,cConfReader,hbaseNamespacePrefix,metadataTableNamespace,new DefaultScanBuilder());
    topicMetadataCache=topicMetadataCacheSupplier.get();
  }
}"
5348,"@Override public InternalScanner preFlushScannerOpen(ObserverContext<RegionCoprocessorEnvironment> c,Store store,KeyValueScanner memstoreScanner,InternalScanner s) throws IOException {
  TopicMetadataCache metadataCache=getTopicMetadataCache(c.getEnvironment());
  LOG.info(""String_Node_Str"");
  Scan scan=new Scan();
  scan.setFilter(new PayloadDataFilter(c.getEnvironment(),System.currentTimeMillis(),prefixLength,metadataCache));
  return new StoreScanner(store,store.getScanInfo(),scan,Collections.singletonList(memstoreScanner),ScanType.COMPACT_DROP_DELETES,store.getSmallestReadPoint(),HConstants.OLDEST_TIMESTAMP);
}","@Override public InternalScanner preFlushScannerOpen(ObserverContext<RegionCoprocessorEnvironment> c,Store store,KeyValueScanner memstoreScanner,InternalScanner s) throws IOException {
  LOG.info(""String_Node_Str"");
  Scan scan=new Scan();
  scan.setFilter(new PayloadDataFilter(c.getEnvironment(),System.currentTimeMillis(),prefixLength,topicMetadataCache));
  return new StoreScanner(store,store.getScanInfo(),scan,Collections.singletonList(memstoreScanner),ScanType.COMPACT_DROP_DELETES,store.getSmallestReadPoint(),HConstants.OLDEST_TIMESTAMP);
}"
5349,"@Override public InternalScanner preCompactScannerOpen(ObserverContext<RegionCoprocessorEnvironment> c,Store store,List<? extends KeyValueScanner> scanners,ScanType scanType,long earliestPutTs,InternalScanner s,CompactionRequest request) throws IOException {
  TopicMetadataCache metadataCache=getTopicMetadataCache(c.getEnvironment());
  LOG.info(""String_Node_Str"");
  Scan scan=new Scan();
  scan.setFilter(new PayloadDataFilter(c.getEnvironment(),System.currentTimeMillis(),prefixLength,metadataCache));
  return new StoreScanner(store,store.getScanInfo(),scan,scanners,scanType,store.getSmallestReadPoint(),earliestPutTs);
}","@Override public InternalScanner preCompactScannerOpen(ObserverContext<RegionCoprocessorEnvironment> c,Store store,List<? extends KeyValueScanner> scanners,ScanType scanType,long earliestPutTs,InternalScanner s,CompactionRequest request) throws IOException {
  LOG.info(""String_Node_Str"");
  Scan scan=new Scan();
  scan.setFilter(new PayloadDataFilter(c.getEnvironment(),System.currentTimeMillis(),prefixLength,topicMetadataCache));
  return new StoreScanner(store,store.getScanInfo(),scan,scanners,scanType,store.getSmallestReadPoint(),earliestPutTs);
}"
5350,"@Override @Nullable protected Configuration getConfiguration(CoprocessorEnvironment env){
  CConfiguration cConf=getCConfCache((RegionCoprocessorEnvironment)env).getCConf();
  if (cConf == null) {
    return null;
  }
  Configuration hConf=new Configuration();
  for (  Map.Entry<String,String> entry : cConf) {
    hConf.set(entry.getKey(),entry.getValue());
  }
  return hConf;
}","@Override @Nullable protected Configuration getConfiguration(CoprocessorEnvironment env){
  CConfiguration cConf=cConfCache.getCConf();
  if (cConf == null) {
    return null;
  }
  Configuration hConf=new Configuration();
  for (  Map.Entry<String,String> entry : cConf) {
    hConf.set(entry.getKey(),entry.getValue());
  }
  return hConf;
}"
5351,"@Override public void stop(CoprocessorEnvironment e) throws IOException {
  super.stop(e);
  if (cConfCache != null) {
    cConfCache.stop();
  }
}","@Override public void stop(CoprocessorEnvironment e) throws IOException {
  super.stop(e);
  cConfCacheSupplier.release();
}"
5352,"@Override public void start(CoprocessorEnvironment e) throws IOException {
  if (e instanceof RegionCoprocessorEnvironment) {
    RegionCoprocessorEnvironment env=(RegionCoprocessorEnvironment)e;
    HTableDescriptor tableDesc=env.getRegion().getTableDesc();
    String hbaseNamespacePrefix=tableDesc.getValue(Constants.Dataset.TABLE_PREFIX);
    this.sysConfigTablePrefix=HTableNameConverter.getSysConfigTablePrefix(hbaseNamespacePrefix);
    this.cConfCache=createCConfCache(env);
  }
  super.start(e);
}","@Override public void start(CoprocessorEnvironment e) throws IOException {
  if (e instanceof RegionCoprocessorEnvironment) {
    RegionCoprocessorEnvironment env=(RegionCoprocessorEnvironment)e;
    HTableDescriptor tableDesc=env.getRegion().getTableDesc();
    String hbaseNamespacePrefix=tableDesc.getValue(Constants.Dataset.TABLE_PREFIX);
    this.sysConfigTablePrefix=HTableNameConverter.getSysConfigTablePrefix(hbaseNamespacePrefix);
    this.cConfCacheSupplier=new CConfigurationCacheSupplier(env.getConfiguration(),sysConfigTablePrefix);
    this.cConfCache=cConfCacheSupplier.get();
  }
  super.start(e);
}"
5353,"@Override protected void startUp() throws Exception {
  DatasetId serviceStoreDatasetInstanceId=NamespaceId.SYSTEM.dataset(Constants.Service.SERVICE_INSTANCE_TABLE_NAME);
  table=DatasetsUtil.getOrCreateDataset(dsFramework,serviceStoreDatasetInstanceId,NoTxKeyValueTable.class.getName(),DatasetProperties.EMPTY,null,null);
}","@Override protected void startUp() throws Exception {
  final DatasetId serviceStoreDatasetInstanceId=NamespaceId.SYSTEM.dataset(Constants.Service.SERVICE_INSTANCE_TABLE_NAME);
  table=Retries.supplyWithRetries(new Supplier<NoTxKeyValueTable>(){
    @Override public NoTxKeyValueTable get(){
      try {
        return DatasetsUtil.getOrCreateDataset(dsFramework,serviceStoreDatasetInstanceId,NoTxKeyValueTable.class.getName(),DatasetProperties.EMPTY,null,null);
      }
 catch (      Exception e) {
        LOG.warn(""String_Node_Str"",serviceStoreDatasetInstanceId,e.getMessage());
        throw new RetryableException(e);
      }
    }
  }
,RetryStrategies.exponentialDelay(1,30,TimeUnit.SECONDS));
}"
5354,"/** 
 * Creates a service listener to reactor on state changes on   {@link MapReduceRuntimeService}.
 */
private Service.Listener createRuntimeServiceListener(final ProgramId programId,final RunId runId,final Iterable<Closeable> closeables,final Arguments arguments,final Arguments userArgs){
  final String twillRunId=arguments.getOption(ProgramOptionConstants.TWILL_RUN_ID);
  return new ServiceListenerAdapter(){
    @Override public void starting(){
      long startTimeInSeconds=RunIds.getTime(runId,TimeUnit.SECONDS);
      if (startTimeInSeconds == -1) {
        startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
      }
      runtimeStore.setStart(programId,runId.getId(),startTimeInSeconds,twillRunId,userArgs.asMap(),arguments.asMap());
    }
    @Override public void terminated(    Service.State from){
      closeAllQuietly(closeables);
      ProgramRunStatus runStatus=ProgramController.State.COMPLETED.getRunStatus();
      if (from == Service.State.STOPPING) {
        runStatus=ProgramController.State.KILLED.getRunStatus();
      }
      runtimeStore.setStop(programId,runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),runStatus);
    }
    @Override public void failed(    Service.State from,    @Nullable Throwable failure){
      closeAllQuietly(closeables);
      runtimeStore.setStop(programId,runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus(),new BasicThrowable(failure));
    }
  }
;
}","/** 
 * Creates a service listener to reactor on state changes on   {@link MapReduceRuntimeService}.
 */
private Service.Listener createRuntimeServiceListener(final ProgramId programId,final RunId runId,final Iterable<Closeable> closeables,final Arguments arguments,final Arguments userArgs){
  final String twillRunId=arguments.getOption(ProgramOptionConstants.TWILL_RUN_ID);
  return new ServiceListenerAdapter(){
    @Override public void starting(){
      long startTimeInSeconds=RunIds.getTime(runId,TimeUnit.SECONDS);
      if (startTimeInSeconds == -1) {
        startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
      }
      final long finalStartTimeInSeconds=startTimeInSeconds;
      Retries.supplyWithRetries(new Supplier<Void>(){
        @Override public Void get(){
          runtimeStore.setStart(programId,runId.getId(),finalStartTimeInSeconds,twillRunId,userArgs.asMap(),arguments.asMap());
          return null;
        }
      }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
    }
    @Override public void terminated(    Service.State from){
      closeAllQuietly(closeables);
      ProgramRunStatus runStatus=ProgramController.State.COMPLETED.getRunStatus();
      if (from == Service.State.STOPPING) {
        runStatus=ProgramController.State.KILLED.getRunStatus();
      }
      final ProgramRunStatus finalRunStatus=runStatus;
      Retries.supplyWithRetries(new Supplier<Void>(){
        @Override public Void get(){
          runtimeStore.setStop(programId,runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),finalRunStatus);
          return null;
        }
      }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
    }
    @Override public void failed(    Service.State from,    @Nullable final Throwable failure){
      closeAllQuietly(closeables);
      Retries.supplyWithRetries(new Supplier<Void>(){
        @Override public Void get(){
          runtimeStore.setStop(programId,runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus(),new BasicThrowable(failure));
          return null;
        }
      }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
    }
  }
;
}"
5355,"@Override public ProgramController run(final Program program,ProgramOptions options){
  ApplicationSpecification appSpec=program.getApplicationSpecification();
  Preconditions.checkNotNull(appSpec,""String_Node_Str"");
  ProgramType processorType=program.getType();
  Preconditions.checkNotNull(processorType,""String_Node_Str"");
  Preconditions.checkArgument(processorType == ProgramType.MAPREDUCE,""String_Node_Str"");
  MapReduceSpecification spec=appSpec.getMapReduce().get(program.getName());
  Preconditions.checkNotNull(spec,""String_Node_Str"",program.getName());
  Arguments arguments=options.getArguments();
  RunId runId=ProgramRunners.getRunId(options);
  WorkflowProgramInfo workflowInfo=WorkflowProgramInfo.create(arguments);
  DatasetFramework programDatasetFramework=workflowInfo == null ? datasetFramework : NameMappedDatasetFramework.createFromWorkflowProgramInfo(datasetFramework,workflowInfo,appSpec);
  if (programDatasetFramework instanceof ProgramContextAware) {
    ProgramId programId=program.getId();
    ((ProgramContextAware)programDatasetFramework).initContext(programId.run(runId));
  }
  MapReduce mapReduce;
  try {
    mapReduce=new InstantiatorFactory(false).get(TypeToken.of(program.<MapReduce>getMainClass())).create();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",spec.getClassName(),e);
    throw Throwables.propagate(e);
  }
  List<Closeable> closeables=new ArrayList<>();
  try {
    PluginInstantiator pluginInstantiator=createPluginInstantiator(options,program.getClassLoader());
    if (pluginInstantiator != null) {
      closeables.add(pluginInstantiator);
    }
    final BasicMapReduceContext context=new BasicMapReduceContext(program,options,cConf,spec,workflowInfo,discoveryServiceClient,metricsCollectionService,txSystemClient,programDatasetFramework,streamAdmin,getPluginArchive(options),pluginInstantiator,secureStore,secureStoreManager,messagingService);
    Reflections.visit(mapReduce,mapReduce.getClass(),new PropertyFieldSetter(context.getSpecification().getProperties()),new MetricsFieldSetter(context.getMetrics()),new DataSetFieldSetter(context));
    LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
    final Service mapReduceRuntimeService=new MapReduceRuntimeService(injector,cConf,hConf,mapReduce,spec,context,program.getJarLocation(),locationFactory,streamAdmin,txSystemClient,authorizationEnforcer,authenticationContext);
    mapReduceRuntimeService.addListener(createRuntimeServiceListener(program.getId(),runId,closeables,arguments,options.getUserArguments()),Threads.SAME_THREAD_EXECUTOR);
    final ProgramController controller=new MapReduceProgramController(mapReduceRuntimeService,context);
    LOG.info(""String_Node_Str"",context.toString());
    if (MapReduceTaskContextProvider.isLocal(hConf) || UserGroupInformation.isSecurityEnabled()) {
      mapReduceRuntimeService.start();
    }
 else {
      ProgramRunners.startAsUser(cConf.get(Constants.CFG_HDFS_USER),mapReduceRuntimeService);
    }
    return controller;
  }
 catch (  Exception e) {
    closeAllQuietly(closeables);
    throw Throwables.propagate(e);
  }
}","@Override public ProgramController run(final Program program,ProgramOptions options){
  ApplicationSpecification appSpec=program.getApplicationSpecification();
  Preconditions.checkNotNull(appSpec,""String_Node_Str"");
  ProgramType processorType=program.getType();
  Preconditions.checkNotNull(processorType,""String_Node_Str"");
  Preconditions.checkArgument(processorType == ProgramType.MAPREDUCE,""String_Node_Str"");
  MapReduceSpecification spec=appSpec.getMapReduce().get(program.getName());
  Preconditions.checkNotNull(spec,""String_Node_Str"",program.getName());
  Arguments arguments=options.getArguments();
  RunId runId=ProgramRunners.getRunId(options);
  WorkflowProgramInfo workflowInfo=WorkflowProgramInfo.create(arguments);
  DatasetFramework programDatasetFramework=workflowInfo == null ? datasetFramework : NameMappedDatasetFramework.createFromWorkflowProgramInfo(datasetFramework,workflowInfo,appSpec);
  if (programDatasetFramework instanceof ProgramContextAware) {
    ProgramId programId=program.getId();
    ((ProgramContextAware)programDatasetFramework).initContext(programId.run(runId));
  }
  MapReduce mapReduce;
  try {
    mapReduce=new InstantiatorFactory(false).get(TypeToken.of(program.<MapReduce>getMainClass())).create();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",spec.getClassName(),e);
    throw Throwables.propagate(e);
  }
  List<Closeable> closeables=new ArrayList<>();
  try {
    PluginInstantiator pluginInstantiator=createPluginInstantiator(options,program.getClassLoader());
    if (pluginInstantiator != null) {
      closeables.add(pluginInstantiator);
    }
    final BasicMapReduceContext context=new BasicMapReduceContext(program,options,cConf,spec,workflowInfo,discoveryServiceClient,metricsCollectionService,txSystemClient,programDatasetFramework,streamAdmin,getPluginArchive(options),pluginInstantiator,secureStore,secureStoreManager,messagingService);
    Reflections.visit(mapReduce,mapReduce.getClass(),new PropertyFieldSetter(context.getSpecification().getProperties()),new MetricsFieldSetter(context.getMetrics()),new DataSetFieldSetter(context));
    LoggingContextAccessor.setLoggingContext(context.getLoggingContext());
    Service mapReduceRuntimeService=new MapReduceRuntimeService(injector,cConf,hConf,mapReduce,spec,context,program.getJarLocation(),locationFactory,streamAdmin,txSystemClient,authorizationEnforcer,authenticationContext);
    mapReduceRuntimeService.addListener(createRuntimeServiceListener(program.getId(),runId,closeables,arguments,options.getUserArguments()),Threads.SAME_THREAD_EXECUTOR);
    final ProgramController controller=new MapReduceProgramController(mapReduceRuntimeService,context);
    LOG.info(""String_Node_Str"",context.toString());
    if (MapReduceTaskContextProvider.isLocal(hConf) || UserGroupInformation.isSecurityEnabled()) {
      mapReduceRuntimeService.start();
    }
 else {
      ProgramRunners.startAsUser(cConf.get(Constants.CFG_HDFS_USER),mapReduceRuntimeService);
    }
    return controller;
  }
 catch (  Exception e) {
    closeAllQuietly(closeables);
    throw Throwables.propagate(e);
  }
}"
5356,"@Override public void failed(Service.State from,@Nullable Throwable failure){
  closeAllQuietly(closeables);
  runtimeStore.setStop(programId,runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus(),new BasicThrowable(failure));
}","@Override public void failed(Service.State from,@Nullable final Throwable failure){
  closeAllQuietly(closeables);
  Retries.supplyWithRetries(new Supplier<Void>(){
    @Override public Void get(){
      runtimeStore.setStop(programId,runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus(),new BasicThrowable(failure));
      return null;
    }
  }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
}"
5357,"@Override public void starting(){
  long startTimeInSeconds=RunIds.getTime(runId,TimeUnit.SECONDS);
  if (startTimeInSeconds == -1) {
    startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
  }
  runtimeStore.setStart(programId,runId.getId(),startTimeInSeconds,twillRunId,userArgs.asMap(),arguments.asMap());
}","@Override public void starting(){
  long startTimeInSeconds=RunIds.getTime(runId,TimeUnit.SECONDS);
  if (startTimeInSeconds == -1) {
    startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
  }
  final long finalStartTimeInSeconds=startTimeInSeconds;
  Retries.supplyWithRetries(new Supplier<Void>(){
    @Override public Void get(){
      runtimeStore.setStart(programId,runId.getId(),finalStartTimeInSeconds,twillRunId,userArgs.asMap(),arguments.asMap());
      return null;
    }
  }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
}"
5358,"@Override public void terminated(Service.State from){
  closeAllQuietly(closeables);
  ProgramRunStatus runStatus=ProgramController.State.COMPLETED.getRunStatus();
  if (from == Service.State.STOPPING) {
    runStatus=ProgramController.State.KILLED.getRunStatus();
  }
  runtimeStore.setStop(programId,runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),runStatus);
}","@Override public void terminated(Service.State from){
  closeAllQuietly(closeables);
  ProgramRunStatus runStatus=ProgramController.State.COMPLETED.getRunStatus();
  if (from == Service.State.STOPPING) {
    runStatus=ProgramController.State.KILLED.getRunStatus();
  }
  final ProgramRunStatus finalRunStatus=runStatus;
  Retries.supplyWithRetries(new Supplier<Void>(){
    @Override public Void get(){
      runtimeStore.setStop(programId,runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),finalRunStatus);
      return null;
    }
  }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
}"
5359,"@Override public void init(ProgramController.State state,@Nullable Throwable cause){
  long startTimeInSeconds=RunIds.getTime(controller.getRunId(),TimeUnit.SECONDS);
  if (startTimeInSeconds == -1) {
    startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
  }
  runtimeStore.setStart(program.getId(),runId.getId(),startTimeInSeconds,twillRunId,options.getUserArguments().asMap(),options.getArguments().asMap());
  if (state == ProgramController.State.COMPLETED) {
    completed();
  }
  if (state == ProgramController.State.ERROR) {
    error(controller.getFailureCause());
  }
}","@Override public void init(ProgramController.State state,@Nullable Throwable cause){
  long startTimeInSeconds=RunIds.getTime(controller.getRunId(),TimeUnit.SECONDS);
  if (startTimeInSeconds == -1) {
    startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
  }
  final long finalStartTimeInSeconds=startTimeInSeconds;
  Retries.supplyWithRetries(new Supplier<Void>(){
    @Override public Void get(){
      runtimeStore.setStart(program.getId(),runId.getId(),finalStartTimeInSeconds,twillRunId,options.getUserArguments().asMap(),options.getArguments().asMap());
      return null;
    }
  }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
  if (state == ProgramController.State.COMPLETED) {
    completed();
  }
  if (state == ProgramController.State.ERROR) {
    error(controller.getFailureCause());
  }
}"
5360,"@Override public void resuming(){
  LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
  runtimeStore.setResume(program.getId(),runId.getId());
}","@Override public void resuming(){
  LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
  Retries.supplyWithRetries(new Supplier<Void>(){
    @Override public Void get(){
      runtimeStore.setResume(program.getId(),runId.getId());
      return null;
    }
  }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
}"
5361,"@Override public ProgramController run(final Program program,final ProgramOptions options){
  ApplicationSpecification appSpec=program.getApplicationSpecification();
  Preconditions.checkNotNull(appSpec,""String_Node_Str"");
  ProgramType processorType=program.getType();
  Preconditions.checkNotNull(processorType,""String_Node_Str"");
  Preconditions.checkArgument(processorType == ProgramType.WORKFLOW,""String_Node_Str"");
  WorkflowSpecification workflowSpec=appSpec.getWorkflows().get(program.getName());
  Preconditions.checkNotNull(workflowSpec,""String_Node_Str"",program.getName());
  final RunId runId=ProgramRunners.getRunId(options);
  if (datasetFramework instanceof ProgramContextAware) {
    ProgramId programId=program.getId();
    ((ProgramContextAware)datasetFramework).initContext(programId.run(runId));
  }
  final List<Closeable> closeables=new ArrayList<>();
  try {
    PluginInstantiator pluginInstantiator=createPluginInstantiator(options,program.getClassLoader());
    if (pluginInstantiator != null) {
      closeables.add(pluginInstantiator);
    }
    WorkflowDriver driver=new WorkflowDriver(program,options,hostname,workflowSpec,programRunnerFactory,metricsCollectionService,datasetFramework,discoveryServiceClient,txClient,runtimeStore,cConf,pluginInstantiator,secureStore,secureStoreManager,messagingService);
    final ProgramController controller=new WorkflowProgramController(program,driver,serviceAnnouncer,runId);
    final String twillRunId=options.getArguments().getOption(ProgramOptionConstants.TWILL_RUN_ID);
    controller.addListener(new AbstractListener(){
      @Override public void init(      ProgramController.State state,      @Nullable Throwable cause){
        long startTimeInSeconds=RunIds.getTime(controller.getRunId(),TimeUnit.SECONDS);
        if (startTimeInSeconds == -1) {
          startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
        }
        runtimeStore.setStart(program.getId(),runId.getId(),startTimeInSeconds,twillRunId,options.getUserArguments().asMap(),options.getArguments().asMap());
        if (state == ProgramController.State.COMPLETED) {
          completed();
        }
        if (state == ProgramController.State.ERROR) {
          error(controller.getFailureCause());
        }
      }
      @Override public void completed(){
        LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
        runtimeStore.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.COMPLETED.getRunStatus());
      }
      @Override public void killed(){
        LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
        runtimeStore.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.KILLED.getRunStatus());
      }
      @Override public void suspended(){
        LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
        runtimeStore.setSuspend(program.getId(),runId.getId());
      }
      @Override public void resuming(){
        LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
        runtimeStore.setResume(program.getId(),runId.getId());
      }
      @Override public void error(      Throwable cause){
        LOG.info(""String_Node_Str"",program.getId(),runId.getId(),cause);
        closeAllQuietly(closeables);
        runtimeStore.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus(),new BasicThrowable(cause));
      }
    }
,Threads.SAME_THREAD_EXECUTOR);
    driver.start();
    return controller;
  }
 catch (  Exception e) {
    closeAllQuietly(closeables);
    throw Throwables.propagate(e);
  }
}","@Override public ProgramController run(final Program program,final ProgramOptions options){
  ApplicationSpecification appSpec=program.getApplicationSpecification();
  Preconditions.checkNotNull(appSpec,""String_Node_Str"");
  ProgramType processorType=program.getType();
  Preconditions.checkNotNull(processorType,""String_Node_Str"");
  Preconditions.checkArgument(processorType == ProgramType.WORKFLOW,""String_Node_Str"");
  WorkflowSpecification workflowSpec=appSpec.getWorkflows().get(program.getName());
  Preconditions.checkNotNull(workflowSpec,""String_Node_Str"",program.getName());
  final RunId runId=ProgramRunners.getRunId(options);
  if (datasetFramework instanceof ProgramContextAware) {
    ProgramId programId=program.getId();
    ((ProgramContextAware)datasetFramework).initContext(programId.run(runId));
  }
  final List<Closeable> closeables=new ArrayList<>();
  try {
    PluginInstantiator pluginInstantiator=createPluginInstantiator(options,program.getClassLoader());
    if (pluginInstantiator != null) {
      closeables.add(pluginInstantiator);
    }
    WorkflowDriver driver=new WorkflowDriver(program,options,hostname,workflowSpec,programRunnerFactory,metricsCollectionService,datasetFramework,discoveryServiceClient,txClient,runtimeStore,cConf,pluginInstantiator,secureStore,secureStoreManager,messagingService);
    final ProgramController controller=new WorkflowProgramController(program,driver,serviceAnnouncer,runId);
    final String twillRunId=options.getArguments().getOption(ProgramOptionConstants.TWILL_RUN_ID);
    controller.addListener(new AbstractListener(){
      @Override public void init(      ProgramController.State state,      @Nullable Throwable cause){
        long startTimeInSeconds=RunIds.getTime(controller.getRunId(),TimeUnit.SECONDS);
        if (startTimeInSeconds == -1) {
          startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
        }
        final long finalStartTimeInSeconds=startTimeInSeconds;
        Retries.supplyWithRetries(new Supplier<Void>(){
          @Override public Void get(){
            runtimeStore.setStart(program.getId(),runId.getId(),finalStartTimeInSeconds,twillRunId,options.getUserArguments().asMap(),options.getArguments().asMap());
            return null;
          }
        }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
        if (state == ProgramController.State.COMPLETED) {
          completed();
        }
        if (state == ProgramController.State.ERROR) {
          error(controller.getFailureCause());
        }
      }
      @Override public void completed(){
        LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
        Retries.supplyWithRetries(new Supplier<Void>(){
          @Override public Void get(){
            runtimeStore.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.COMPLETED.getRunStatus());
            return null;
          }
        }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
      }
      @Override public void killed(){
        LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
        Retries.supplyWithRetries(new Supplier<Void>(){
          @Override public Void get(){
            runtimeStore.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.KILLED.getRunStatus());
            return null;
          }
        }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
      }
      @Override public void suspended(){
        LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
        Retries.supplyWithRetries(new Supplier<Void>(){
          @Override public Void get(){
            runtimeStore.setSuspend(program.getId(),runId.getId());
            return null;
          }
        }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
      }
      @Override public void resuming(){
        LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
        Retries.supplyWithRetries(new Supplier<Void>(){
          @Override public Void get(){
            runtimeStore.setResume(program.getId(),runId.getId());
            return null;
          }
        }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
      }
      @Override public void error(      final Throwable cause){
        LOG.info(""String_Node_Str"",program.getId(),runId.getId(),cause);
        closeAllQuietly(closeables);
        Retries.supplyWithRetries(new Supplier<Void>(){
          @Override public Void get(){
            runtimeStore.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus(),new BasicThrowable(cause));
            return null;
          }
        }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
      }
    }
,Threads.SAME_THREAD_EXECUTOR);
    driver.start();
    return controller;
  }
 catch (  Exception e) {
    closeAllQuietly(closeables);
    throw Throwables.propagate(e);
  }
}"
5362,"@Override public void completed(){
  LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
  runtimeStore.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.COMPLETED.getRunStatus());
}","@Override public void completed(){
  LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
  Retries.supplyWithRetries(new Supplier<Void>(){
    @Override public Void get(){
      runtimeStore.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.COMPLETED.getRunStatus());
      return null;
    }
  }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
}"
5363,"@Override public void error(Throwable cause){
  LOG.info(""String_Node_Str"",program.getId(),runId.getId(),cause);
  closeAllQuietly(closeables);
  runtimeStore.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus(),new BasicThrowable(cause));
}","@Override public void error(final Throwable cause){
  LOG.info(""String_Node_Str"",program.getId(),runId.getId(),cause);
  closeAllQuietly(closeables);
  Retries.supplyWithRetries(new Supplier<Void>(){
    @Override public Void get(){
      runtimeStore.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus(),new BasicThrowable(cause));
      return null;
    }
  }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
}"
5364,"@Override public void killed(){
  LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
  runtimeStore.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.KILLED.getRunStatus());
}","@Override public void killed(){
  LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
  Retries.supplyWithRetries(new Supplier<Void>(){
    @Override public Void get(){
      runtimeStore.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.KILLED.getRunStatus());
      return null;
    }
  }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
}"
5365,"@Override public void suspended(){
  LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
  runtimeStore.setSuspend(program.getId(),runId.getId());
}","@Override public void suspended(){
  LOG.debug(""String_Node_Str"",program.getId(),runId.getId());
  Retries.supplyWithRetries(new Supplier<Void>(){
    @Override public Void get(){
      runtimeStore.setSuspend(program.getId(),runId.getId());
      return null;
    }
  }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
}"
5366,"@Override public void error(Throwable cause){
  LOG.info(""String_Node_Str"",programId,runId,cause);
  store.setStop(programId,runId,TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus(),new BasicThrowable(cause));
}","@Override public void error(final Throwable cause){
  LOG.info(""String_Node_Str"",programId,runId,cause);
  Retries.supplyWithRetries(new Supplier<Void>(){
    @Override public Void get(){
      store.setStop(programId,runId,TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus(),new BasicThrowable(cause));
      return null;
    }
  }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
}"
5367,"@Override public void resuming(){
  LOG.debug(""String_Node_Str"",programId,runId);
  store.setResume(programId,runId);
}","@Override public void resuming(){
  LOG.debug(""String_Node_Str"",programId,runId);
  Retries.supplyWithRetries(new Supplier<Void>(){
    @Override public Void get(){
      store.setResume(programId,runId);
      return null;
    }
  }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
}"
5368,"@Override public void init(ProgramController.State state,@Nullable Throwable cause){
  long startTimeInSeconds=RunIds.getTime(controller.getRunId(),TimeUnit.SECONDS);
  if (startTimeInSeconds == -1) {
    startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
  }
  store.setStart(programId,runId,startTimeInSeconds,twillRunId,userArgs,systemArgs);
  if (state == ProgramController.State.COMPLETED) {
    completed();
  }
  if (state == ProgramController.State.ERROR) {
    error(controller.getFailureCause());
  }
}","@Override public void init(ProgramController.State state,@Nullable Throwable cause){
  long startTimeInSeconds=RunIds.getTime(controller.getRunId(),TimeUnit.SECONDS);
  if (startTimeInSeconds == -1) {
    startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
  }
  final long finalStartTimeInSeconds=startTimeInSeconds;
  Retries.supplyWithRetries(new Supplier<Void>(){
    @Override public Void get(){
      store.setStart(programId,runId,finalStartTimeInSeconds,twillRunId,userArgs,systemArgs);
      return null;
    }
  }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
  if (state == ProgramController.State.COMPLETED) {
    completed();
  }
  if (state == ProgramController.State.ERROR) {
    error(controller.getFailureCause());
  }
}"
5369,"/** 
 * Start a Program.
 * @param programId  the {@link ProgramId program} to start
 * @param systemArgs system arguments
 * @param userArgs user arguments
 * @param debug enable debug mode
 * @return {@link ProgramRuntimeService.RuntimeInfo}
 * @throws IOException if there is an error starting the program
 * @throws ProgramNotFoundException if program is not found
 * @throws UnauthorizedException if the logged in user is not authorized to start the program. To start a program,a user requires  {@link Action#EXECUTE} on the program
 * @throws Exception if there were other exceptions checking if the current user is authorized to start the program
 */
public ProgramRuntimeService.RuntimeInfo start(final ProgramId programId,final Map<String,String> systemArgs,final Map<String,String> userArgs,boolean debug) throws Exception {
  authorizationEnforcer.enforce(programId,authenticationContext.getPrincipal(),Action.EXECUTE);
  ProgramDescriptor programDescriptor=store.loadProgram(programId);
  BasicArguments systemArguments=new BasicArguments(systemArgs);
  BasicArguments userArguments=new BasicArguments(userArgs);
  ProgramRuntimeService.RuntimeInfo runtimeInfo=runtimeService.run(programDescriptor,new SimpleProgramOptions(programId.getProgram(),systemArguments,userArguments,debug));
  final ProgramController controller=runtimeInfo.getController();
  final String runId=controller.getRunId().getId();
  final String twillRunId=runtimeInfo.getTwillRunId() == null ? null : runtimeInfo.getTwillRunId().getId();
  if (programId.getType() != ProgramType.MAPREDUCE && programId.getType() != ProgramType.SPARK && programId.getType() != ProgramType.WORKFLOW) {
    controller.addListener(new AbstractListener(){
      @Override public void init(      ProgramController.State state,      @Nullable Throwable cause){
        long startTimeInSeconds=RunIds.getTime(controller.getRunId(),TimeUnit.SECONDS);
        if (startTimeInSeconds == -1) {
          startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
        }
        store.setStart(programId,runId,startTimeInSeconds,twillRunId,userArgs,systemArgs);
        if (state == ProgramController.State.COMPLETED) {
          completed();
        }
        if (state == ProgramController.State.ERROR) {
          error(controller.getFailureCause());
        }
      }
      @Override public void completed(){
        LOG.debug(""String_Node_Str"",programId);
        store.setStop(programId,runId,TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.COMPLETED.getRunStatus());
      }
      @Override public void killed(){
        LOG.debug(""String_Node_Str"",programId);
        store.setStop(programId,runId,TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.KILLED.getRunStatus());
      }
      @Override public void suspended(){
        LOG.debug(""String_Node_Str"",programId,runId);
        store.setSuspend(programId,runId);
      }
      @Override public void resuming(){
        LOG.debug(""String_Node_Str"",programId,runId);
        store.setResume(programId,runId);
      }
      @Override public void error(      Throwable cause){
        LOG.info(""String_Node_Str"",programId,runId,cause);
        store.setStop(programId,runId,TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus(),new BasicThrowable(cause));
      }
    }
,Threads.SAME_THREAD_EXECUTOR);
  }
  return runtimeInfo;
}","/** 
 * Start a Program.
 * @param programId  the {@link ProgramId program} to start
 * @param systemArgs system arguments
 * @param userArgs user arguments
 * @param debug enable debug mode
 * @return {@link ProgramRuntimeService.RuntimeInfo}
 * @throws IOException if there is an error starting the program
 * @throws ProgramNotFoundException if program is not found
 * @throws UnauthorizedException if the logged in user is not authorized to start the program. To start a program,a user requires  {@link Action#EXECUTE} on the program
 * @throws Exception if there were other exceptions checking if the current user is authorized to start the program
 */
public ProgramRuntimeService.RuntimeInfo start(final ProgramId programId,final Map<String,String> systemArgs,final Map<String,String> userArgs,boolean debug) throws Exception {
  authorizationEnforcer.enforce(programId,authenticationContext.getPrincipal(),Action.EXECUTE);
  ProgramDescriptor programDescriptor=store.loadProgram(programId);
  BasicArguments systemArguments=new BasicArguments(systemArgs);
  BasicArguments userArguments=new BasicArguments(userArgs);
  ProgramRuntimeService.RuntimeInfo runtimeInfo=runtimeService.run(programDescriptor,new SimpleProgramOptions(programId.getProgram(),systemArguments,userArguments,debug));
  final ProgramController controller=runtimeInfo.getController();
  final String runId=controller.getRunId().getId();
  final String twillRunId=runtimeInfo.getTwillRunId() == null ? null : runtimeInfo.getTwillRunId().getId();
  if (programId.getType() != ProgramType.MAPREDUCE && programId.getType() != ProgramType.SPARK && programId.getType() != ProgramType.WORKFLOW) {
    controller.addListener(new AbstractListener(){
      @Override public void init(      ProgramController.State state,      @Nullable Throwable cause){
        long startTimeInSeconds=RunIds.getTime(controller.getRunId(),TimeUnit.SECONDS);
        if (startTimeInSeconds == -1) {
          startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
        }
        final long finalStartTimeInSeconds=startTimeInSeconds;
        Retries.supplyWithRetries(new Supplier<Void>(){
          @Override public Void get(){
            store.setStart(programId,runId,finalStartTimeInSeconds,twillRunId,userArgs,systemArgs);
            return null;
          }
        }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
        if (state == ProgramController.State.COMPLETED) {
          completed();
        }
        if (state == ProgramController.State.ERROR) {
          error(controller.getFailureCause());
        }
      }
      @Override public void completed(){
        LOG.debug(""String_Node_Str"",programId);
        Retries.supplyWithRetries(new Supplier<Void>(){
          @Override public Void get(){
            store.setStop(programId,runId,TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.COMPLETED.getRunStatus());
            return null;
          }
        }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
      }
      @Override public void killed(){
        LOG.debug(""String_Node_Str"",programId);
        Retries.supplyWithRetries(new Supplier<Void>(){
          @Override public Void get(){
            store.setStop(programId,runId,TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.KILLED.getRunStatus());
            return null;
          }
        }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
      }
      @Override public void suspended(){
        LOG.debug(""String_Node_Str"",programId,runId);
        Retries.supplyWithRetries(new Supplier<Void>(){
          @Override public Void get(){
            store.setSuspend(programId,runId);
            return null;
          }
        }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
      }
      @Override public void resuming(){
        LOG.debug(""String_Node_Str"",programId,runId);
        Retries.supplyWithRetries(new Supplier<Void>(){
          @Override public Void get(){
            store.setResume(programId,runId);
            return null;
          }
        }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
      }
      @Override public void error(      final Throwable cause){
        LOG.info(""String_Node_Str"",programId,runId,cause);
        Retries.supplyWithRetries(new Supplier<Void>(){
          @Override public Void get(){
            store.setStop(programId,runId,TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus(),new BasicThrowable(cause));
            return null;
          }
        }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
      }
    }
,Threads.SAME_THREAD_EXECUTOR);
  }
  return runtimeInfo;
}"
5370,"@Override public void completed(){
  LOG.debug(""String_Node_Str"",programId);
  store.setStop(programId,runId,TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.COMPLETED.getRunStatus());
}","@Override public void completed(){
  LOG.debug(""String_Node_Str"",programId);
  Retries.supplyWithRetries(new Supplier<Void>(){
    @Override public Void get(){
      store.setStop(programId,runId,TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.COMPLETED.getRunStatus());
      return null;
    }
  }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
}"
5371,"@Override public void killed(){
  LOG.debug(""String_Node_Str"",programId);
  store.setStop(programId,runId,TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.KILLED.getRunStatus());
}","@Override public void killed(){
  LOG.debug(""String_Node_Str"",programId);
  Retries.supplyWithRetries(new Supplier<Void>(){
    @Override public Void get(){
      store.setStop(programId,runId,TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.KILLED.getRunStatus());
      return null;
    }
  }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
}"
5372,"@Override public void suspended(){
  LOG.debug(""String_Node_Str"",programId,runId);
  store.setSuspend(programId,runId);
}","@Override public void suspended(){
  LOG.debug(""String_Node_Str"",programId,runId);
  Retries.supplyWithRetries(new Supplier<Void>(){
    @Override public Void get(){
      store.setSuspend(programId,runId);
      return null;
    }
  }
,RetryStrategies.fixDelay(Constants.Retry.RUN_RECORD_UPDATE_RETRY_DELAY_SECS,TimeUnit.SECONDS));
}"
5373,"private void upgradeCoProcessor(TableId tableId,Class<? extends Coprocessor> coprocessor) throws IOException {
  try (HBaseDDLExecutor ddlExecutor=ddlExecutorFactory.get()){
    HTableDescriptor tableDescriptor;
    try (HBaseAdmin admin=new HBaseAdmin(hConf)){
      if (!tableUtil.tableExists(admin,tableId)) {
        LOG.debug(""String_Node_Str"",tableId);
        return;
      }
      tableDescriptor=tableUtil.getHTableDescriptor(admin,tableId);
    }
     ProjectInfo.Version version=HBaseTableUtil.getVersion(tableDescriptor);
    if (version.compareTo(ProjectInfo.getVersion()) >= 0) {
      LOG.info(""String_Node_Str"",tableId,version,ProjectInfo.getVersion());
      return;
    }
    HTableDescriptorBuilder newDescriptor=tableUtil.buildHTableDescriptor(tableDescriptor);
    Map<String,HBaseTableUtil.CoprocessorInfo> coprocessorInfo=HBaseTableUtil.getCoprocessorInfo(tableDescriptor);
    for (    Map.Entry<String,HBaseTableUtil.CoprocessorInfo> coprocessorEntry : coprocessorInfo.entrySet()) {
      newDescriptor.removeCoprocessor(coprocessorEntry.getValue().getClassName());
    }
    CoprocessorDescriptor coprocessorDescriptor=coprocessorManager.getCoprocessorDescriptor(coprocessor,Coprocessor.PRIORITY_USER);
    Path path=coprocessorDescriptor.getPath() == null ? null : new Path(coprocessorDescriptor.getPath());
    newDescriptor.addCoprocessor(coprocessorDescriptor.getClassName(),path,coprocessorDescriptor.getPriority(),coprocessorDescriptor.getProperties());
    HBaseTableUtil.setVersion(newDescriptor);
    HBaseTableUtil.setTablePrefix(newDescriptor,cConf);
    disableTable(ddlExecutor,tableId);
    tableUtil.modifyTable(ddlExecutor,newDescriptor.build());
    LOG.debug(""String_Node_Str"",tableId);
    enableTable(ddlExecutor,tableId);
  }
   LOG.info(""String_Node_Str"",tableId);
}","private void upgradeCoProcessor(TableId tableId,Class<? extends Coprocessor> coprocessor) throws IOException {
  try (HBaseDDLExecutor ddlExecutor=ddlExecutorFactory.get()){
    HTableDescriptor tableDescriptor;
    try (HBaseAdmin admin=new HBaseAdmin(hConf)){
      if (!tableUtil.tableExists(admin,tableId)) {
        LOG.debug(""String_Node_Str"",tableId);
        return;
      }
      tableDescriptor=tableUtil.getHTableDescriptor(admin,tableId);
    }
     ProjectInfo.Version version=HBaseTableUtil.getVersion(tableDescriptor);
    if (version.compareTo(ProjectInfo.getVersion()) >= 0) {
      LOG.info(""String_Node_Str"",tableId,version,ProjectInfo.getVersion());
      enableTable(ddlExecutor,tableId);
      return;
    }
    HTableDescriptorBuilder newDescriptor=tableUtil.buildHTableDescriptor(tableDescriptor);
    Map<String,HBaseTableUtil.CoprocessorInfo> coprocessorInfo=HBaseTableUtil.getCoprocessorInfo(tableDescriptor);
    for (    Map.Entry<String,HBaseTableUtil.CoprocessorInfo> coprocessorEntry : coprocessorInfo.entrySet()) {
      newDescriptor.removeCoprocessor(coprocessorEntry.getValue().getClassName());
    }
    CoprocessorDescriptor coprocessorDescriptor=coprocessorManager.getCoprocessorDescriptor(coprocessor,Coprocessor.PRIORITY_USER);
    Path path=coprocessorDescriptor.getPath() == null ? null : new Path(coprocessorDescriptor.getPath());
    newDescriptor.addCoprocessor(coprocessorDescriptor.getClassName(),path,coprocessorDescriptor.getPriority(),coprocessorDescriptor.getProperties());
    HBaseTableUtil.setVersion(newDescriptor);
    HBaseTableUtil.setTablePrefix(newDescriptor,cConf);
    disableTable(ddlExecutor,tableId);
    tableUtil.modifyTable(ddlExecutor,newDescriptor.build());
    LOG.debug(""String_Node_Str"",tableId);
    enableTable(ddlExecutor,tableId);
  }
   LOG.info(""String_Node_Str"",tableId);
}"
5374,"@Path(""String_Node_Str"") @GET public void getIdleRegions(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numRegions){
  if (debugClazz == null || debugObject == null) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    return;
  }
  try {
    Method method=debugClazz.getMethod(""String_Node_Str"",Integer.class);
    method.setAccessible(true);
    Object response=method.invoke(debugObject,numRegions);
    Queue<RegionPruneInfo> pruneInfos=(Queue<RegionPruneInfo>)response;
    responder.sendJson(HttpResponseStatus.OK,pruneInfos);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}","@Path(""String_Node_Str"") @GET public void getIdleRegions(HttpRequest request,HttpResponder responder,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numRegions){
  if (debugClazz == null || debugObject == null) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    return;
  }
  try {
    Method method=debugClazz.getMethod(""String_Node_Str"",Integer.class);
    method.setAccessible(true);
    Object response=method.invoke(debugObject,numRegions);
    Queue<RegionPruneInfo> pruneInfos=(Queue<RegionPruneInfo>)response;
    responder.sendJson(HttpResponseStatus.OK,pruneInfos);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    LOG.debug(""String_Node_Str"",e);
  }
}"
5375,"@Inject public TransactionHttpHandler(TransactionSystemClient txClient){
  this.txClient=txClient;
}","@Inject public TransactionHttpHandler(CConfiguration cConf,TransactionSystemClient txClient){
  this.txClient=txClient;
  try {
    this.debugClazz=getClass().getClassLoader().loadClass(""String_Node_Str"");
    this.debugObject=debugClazz.newInstance();
    Configuration hConf=new Configuration();
    for (    Map.Entry<String,String> entry : cConf) {
      hConf.set(entry.getKey(),entry.getValue());
    }
    Method initMethod=debugClazz.getMethod(""String_Node_Str"",Configuration.class);
    initMethod.setAccessible(true);
    initMethod.invoke(debugObject,hConf);
  }
 catch (  ClassNotFoundException|IllegalAccessException|InstantiationException ex) {
    LOG.warn(""String_Node_Str"",ex);
    this.debugClazz=null;
  }
catch (  NoSuchMethodException|InvocationTargetException ex) {
    LOG.warn(""String_Node_Str"",ex);
    this.debugClazz=null;
  }
}"
5376,"@Override protected void doStop(){
  startupThread.interrupt();
  Uninterruptibles.joinUninterruptibly(startupThread);
  Service service=currentDelegate;
  if (service != null) {
    Futures.addCallback(service.stop(),new FutureCallback<State>(){
      @Override public void onSuccess(      State result){
        notifyStopped();
      }
      @Override public void onFailure(      Throwable t){
        notifyFailed(t);
      }
    }
,Threads.SAME_THREAD_EXECUTOR);
  }
 else {
    notifyStopped();
  }
}","@Override protected void doStop(){
  stopped=true;
  startupThread.interrupt();
  Uninterruptibles.joinUninterruptibly(startupThread);
  Service service=currentDelegate;
  if (service != null) {
    Futures.addCallback(service.stop(),new FutureCallback<State>(){
      @Override public void onSuccess(      State result){
        notifyStopped();
      }
      @Override public void onFailure(      Throwable t){
        notifyFailed(t);
      }
    }
,Threads.SAME_THREAD_EXECUTOR);
  }
 else {
    notifyStopped();
  }
}"
5377,"/** 
 * Creates a new instance.
 * @param delegate a {@link Supplier} that gives new instance of the delegating Service.
 * @param retryStrategy strategy to use for retrying
 */
public RetryOnStartFailureService(final Supplier<Service> delegate,final RetryStrategy retryStrategy){
  final Service service=delegate.get();
  this.delegateServiceName=service.getClass().getSimpleName();
  this.startupThread=new Thread(""String_Node_Str"" + delegateServiceName){
    @Override public void run(){
      int failures=0;
      long startTime=System.currentTimeMillis();
      long delay=0L;
      Service delegateService=service;
      while (delay >= 0 && !isInterrupted()) {
        try {
          currentDelegate=delegateService;
          delegateService.start().get();
          break;
        }
 catch (        InterruptedException e) {
          interrupt();
          break;
        }
catch (        Throwable t) {
          LOG.debug(""String_Node_Str"",delegateServiceName,t);
          delay=retryStrategy.nextRetry(++failures,startTime);
          if (delay < 0) {
            LOG.error(""String_Node_Str"",delegateServiceName,failures,System.currentTimeMillis() - startTime);
            notifyFailed(t);
            break;
          }
          try {
            TimeUnit.MILLISECONDS.sleep(delay);
            LOG.debug(""String_Node_Str"",delegateServiceName);
            delegateService=delegate.get();
          }
 catch (          InterruptedException e) {
            interrupt();
          }
        }
      }
      if (isInterrupted()) {
        LOG.warn(""String_Node_Str"",delegateServiceName);
      }
    }
  }
;
}","/** 
 * Creates a new instance.
 * @param delegate a {@link Supplier} that gives new instance of the delegating Service.
 * @param retryStrategy strategy to use for retrying
 */
public RetryOnStartFailureService(final Supplier<Service> delegate,final RetryStrategy retryStrategy){
  final Service service=delegate.get();
  this.delegateServiceName=service.getClass().getSimpleName();
  this.startupThread=new Thread(""String_Node_Str"" + delegateServiceName){
    @Override public void run(){
      int failures=0;
      long startTime=System.currentTimeMillis();
      long delay=0L;
      Service delegateService=service;
      while (delay >= 0 && !isInterrupted() && !stopped) {
        try {
          currentDelegate=delegateService;
          delegateService.start().get();
          break;
        }
 catch (        InterruptedException e) {
          interrupt();
          break;
        }
catch (        Throwable t) {
          LOG.debug(""String_Node_Str"",delegateServiceName,t);
          delay=retryStrategy.nextRetry(++failures,startTime);
          if (delay < 0) {
            LOG.error(""String_Node_Str"",delegateServiceName,failures,System.currentTimeMillis() - startTime);
            notifyFailed(t);
            break;
          }
          try {
            TimeUnit.MILLISECONDS.sleep(delay);
            LOG.debug(""String_Node_Str"",delegateServiceName);
            delegateService=delegate.get();
          }
 catch (          InterruptedException e) {
            interrupt();
          }
        }
      }
      if (isInterrupted()) {
        LOG.warn(""String_Node_Str"",delegateServiceName);
      }
    }
  }
;
}"
5378,"@Override public void run(){
  int failures=0;
  long startTime=System.currentTimeMillis();
  long delay=0L;
  Service delegateService=service;
  while (delay >= 0 && !isInterrupted()) {
    try {
      currentDelegate=delegateService;
      delegateService.start().get();
      break;
    }
 catch (    InterruptedException e) {
      interrupt();
      break;
    }
catch (    Throwable t) {
      LOG.debug(""String_Node_Str"",delegateServiceName,t);
      delay=retryStrategy.nextRetry(++failures,startTime);
      if (delay < 0) {
        LOG.error(""String_Node_Str"",delegateServiceName,failures,System.currentTimeMillis() - startTime);
        notifyFailed(t);
        break;
      }
      try {
        TimeUnit.MILLISECONDS.sleep(delay);
        LOG.debug(""String_Node_Str"",delegateServiceName);
        delegateService=delegate.get();
      }
 catch (      InterruptedException e) {
        interrupt();
      }
    }
  }
  if (isInterrupted()) {
    LOG.warn(""String_Node_Str"",delegateServiceName);
  }
}","@Override public void run(){
  int failures=0;
  long startTime=System.currentTimeMillis();
  long delay=0L;
  Service delegateService=service;
  while (delay >= 0 && !isInterrupted() && !stopped) {
    try {
      currentDelegate=delegateService;
      delegateService.start().get();
      break;
    }
 catch (    InterruptedException e) {
      interrupt();
      break;
    }
catch (    Throwable t) {
      LOG.debug(""String_Node_Str"",delegateServiceName,t);
      delay=retryStrategy.nextRetry(++failures,startTime);
      if (delay < 0) {
        LOG.error(""String_Node_Str"",delegateServiceName,failures,System.currentTimeMillis() - startTime);
        notifyFailed(t);
        break;
      }
      try {
        TimeUnit.MILLISECONDS.sleep(delay);
        LOG.debug(""String_Node_Str"",delegateServiceName);
        delegateService=delegate.get();
      }
 catch (      InterruptedException e) {
        interrupt();
      }
    }
  }
  if (isInterrupted()) {
    LOG.warn(""String_Node_Str"",delegateServiceName);
  }
}"
5379,"@BeforeClass public static void init() throws Exception {
  cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TEMP_FOLDER.newFolder().getAbsolutePath());
  miniKdc=new MiniKdc(MiniKdc.createConf(),TEMP_FOLDER.newFolder());
  miniKdc.start();
  System.setProperty(""String_Node_Str"",miniKdc.getKrb5conf().getAbsolutePath());
  keytabFile=TEMP_FOLDER.newFile();
  miniKdc.createPrincipal(keytabFile,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Configuration hConf=new Configuration();
  hConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR,TEMP_FOLDER.newFolder().getAbsolutePath());
  miniDFSCluster=new MiniDFSCluster.Builder(hConf).numDataNodes(1).build();
  miniDFSCluster.waitClusterUp();
  locationFactory=new FileContextLocationFactory(miniDFSCluster.getFileSystem().getConf());
  hConf=new Configuration();
  hConf.set(""String_Node_Str"",""String_Node_Str"");
  UserGroupInformation.setConfiguration(hConf);
}","@BeforeClass public static void init() throws Exception {
  cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TEMP_FOLDER.newFolder().getAbsolutePath());
  miniKdc=new MiniKdc(MiniKdc.createConf(),TEMP_FOLDER.newFolder());
  miniKdc.start();
  System.setProperty(""String_Node_Str"",miniKdc.getKrb5conf().getAbsolutePath());
  keytabFile=TEMP_FOLDER.newFile();
  miniKdc.createPrincipal(keytabFile,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Configuration hConf=new Configuration();
  hConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR,TEMP_FOLDER.newFolder().getAbsolutePath());
  hConf.setBoolean(""String_Node_Str"",true);
  miniDFSCluster=new MiniDFSCluster.Builder(hConf).numDataNodes(1).build();
  miniDFSCluster.waitClusterUp();
  locationFactory=new FileContextLocationFactory(miniDFSCluster.getFileSystem().getConf());
  hConf=new Configuration();
  hConf.set(""String_Node_Str"",""String_Node_Str"");
  UserGroupInformation.setConfiguration(hConf);
}"
5380,"@Path(""String_Node_Str"") @POST public void getCredentials(HttpRequest request,HttpResponder responder) throws IOException {
  ImpersonationInfo impersonationInfo=new Gson().fromJson(request.getContent().toString(StandardCharsets.UTF_8),ImpersonationInfo.class);
  Credentials credentials=new Credentials();
  credentials.addToken(new Text(""String_Node_Str""),new Token<>(impersonationInfo.getPrincipal().getBytes(StandardCharsets.UTF_8),impersonationInfo.getPrincipal().getBytes(StandardCharsets.UTF_8),new Text(""String_Node_Str""),new Text(""String_Node_Str"")));
  credentials.addToken(new Text(""String_Node_Str""),new Token<>(impersonationInfo.getKeytabURI().getBytes(StandardCharsets.UTF_8),impersonationInfo.getKeytabURI().getBytes(StandardCharsets.UTF_8),new Text(""String_Node_Str""),new Text(""String_Node_Str"")));
  Location credentialsDir=locationFactory.create(""String_Node_Str"");
  Preconditions.checkState(credentialsDir.mkdirs());
  Location credentialsFile=credentialsDir.append(""String_Node_Str"").getTempFile(""String_Node_Str"");
  try (DataOutputStream os=new DataOutputStream(new BufferedOutputStream(credentialsFile.getOutputStream()))){
    credentials.writeTokenStorageToStream(os);
  }
   responder.sendString(HttpResponseStatus.OK,credentialsFile.toURI().toString());
}","@Path(""String_Node_Str"") @POST public void getCredentials(HttpRequest request,HttpResponder responder) throws IOException {
  ImpersonationInfo impersonationInfo=new Gson().fromJson(request.getContent().toString(StandardCharsets.UTF_8),ImpersonationInfo.class);
  Credentials credentials=new Credentials();
  credentials.addToken(new Text(""String_Node_Str""),new Token<>(impersonationInfo.getPrincipal().getBytes(StandardCharsets.UTF_8),impersonationInfo.getPrincipal().getBytes(StandardCharsets.UTF_8),new Text(""String_Node_Str""),new Text(""String_Node_Str"")));
  credentials.addToken(new Text(""String_Node_Str""),new Token<>(impersonationInfo.getKeytabURI().getBytes(StandardCharsets.UTF_8),impersonationInfo.getKeytabURI().getBytes(StandardCharsets.UTF_8),new Text(""String_Node_Str""),new Text(""String_Node_Str"")));
  Location credentialsDir=locationFactory.create(""String_Node_Str"");
  if (!credentialsDir.exists()) {
    Preconditions.checkState(credentialsDir.mkdirs());
  }
  Location credentialsFile=credentialsDir.append(""String_Node_Str"").getTempFile(""String_Node_Str"");
  try (DataOutputStream os=new DataOutputStream(new BufferedOutputStream(credentialsFile.getOutputStream()))){
    credentials.writeTokenStorageToStream(os);
  }
   responder.sendString(HttpResponseStatus.OK,credentialsFile.toURI().toString());
}"
5381,"@Ignore @Test public void testRemoteUGIProvider() throws Exception {
  final NettyHttpService httpService=NettyHttpService.builder(""String_Node_Str"").addHttpHandlers(Collections.singleton(new UGIProviderTestHandler())).build();
  httpService.startAndWait();
  try {
    InMemoryDiscoveryService discoveryService=new InMemoryDiscoveryService();
    discoveryService.register(new Discoverable(Constants.Service.APP_FABRIC_HTTP,httpService.getBindAddress()));
    RemoteUGIProvider ugiProvider=new RemoteUGIProvider(cConf,discoveryService,locationFactory);
    ImpersonationInfo aliceInfo=new ImpersonationInfo(getPrincipal(""String_Node_Str""),keytabFile.toURI().toString());
    UserGroupInformation aliceUGI=ugiProvider.getConfiguredUGI(aliceInfo);
    Assert.assertFalse(aliceUGI.hasKerberosCredentials());
    Token<? extends TokenIdentifier> token=aliceUGI.getCredentials().getToken(new Text(""String_Node_Str""));
    Assert.assertArrayEquals(aliceInfo.getPrincipal().getBytes(StandardCharsets.UTF_8),token.getIdentifier());
    Assert.assertArrayEquals(aliceInfo.getPrincipal().getBytes(StandardCharsets.UTF_8),token.getPassword());
    Assert.assertEquals(new Text(""String_Node_Str""),token.getKind());
    Assert.assertEquals(new Text(""String_Node_Str""),token.getService());
    token=aliceUGI.getCredentials().getToken(new Text(""String_Node_Str""));
    Assert.assertArrayEquals(aliceInfo.getKeytabURI().getBytes(StandardCharsets.UTF_8),token.getIdentifier());
    Assert.assertArrayEquals(aliceInfo.getKeytabURI().getBytes(StandardCharsets.UTF_8),token.getPassword());
    Assert.assertEquals(new Text(""String_Node_Str""),token.getKind());
    Assert.assertEquals(new Text(""String_Node_Str""),token.getService());
    Assert.assertSame(aliceUGI,ugiProvider.getConfiguredUGI(aliceInfo));
    ugiProvider.invalidCache();
    Assert.assertNotSame(aliceUGI,ugiProvider.getConfiguredUGI(aliceInfo));
  }
  finally {
    httpService.stopAndWait();
  }
}","@Test public void testRemoteUGIProvider() throws Exception {
  final NettyHttpService httpService=NettyHttpService.builder(""String_Node_Str"").addHttpHandlers(Collections.singleton(new UGIProviderTestHandler())).build();
  httpService.startAndWait();
  try {
    InMemoryDiscoveryService discoveryService=new InMemoryDiscoveryService();
    discoveryService.register(new Discoverable(Constants.Service.APP_FABRIC_HTTP,httpService.getBindAddress()));
    RemoteUGIProvider ugiProvider=new RemoteUGIProvider(cConf,discoveryService,locationFactory);
    ImpersonationInfo aliceInfo=new ImpersonationInfo(getPrincipal(""String_Node_Str""),keytabFile.toURI().toString());
    UserGroupInformation aliceUGI=ugiProvider.getConfiguredUGI(aliceInfo);
    Assert.assertFalse(aliceUGI.hasKerberosCredentials());
    Token<? extends TokenIdentifier> token=aliceUGI.getCredentials().getToken(new Text(""String_Node_Str""));
    Assert.assertArrayEquals(aliceInfo.getPrincipal().getBytes(StandardCharsets.UTF_8),token.getIdentifier());
    Assert.assertArrayEquals(aliceInfo.getPrincipal().getBytes(StandardCharsets.UTF_8),token.getPassword());
    Assert.assertEquals(new Text(""String_Node_Str""),token.getKind());
    Assert.assertEquals(new Text(""String_Node_Str""),token.getService());
    token=aliceUGI.getCredentials().getToken(new Text(""String_Node_Str""));
    Assert.assertArrayEquals(aliceInfo.getKeytabURI().getBytes(StandardCharsets.UTF_8),token.getIdentifier());
    Assert.assertArrayEquals(aliceInfo.getKeytabURI().getBytes(StandardCharsets.UTF_8),token.getPassword());
    Assert.assertEquals(new Text(""String_Node_Str""),token.getKind());
    Assert.assertEquals(new Text(""String_Node_Str""),token.getService());
    Assert.assertSame(aliceUGI,ugiProvider.getConfiguredUGI(aliceInfo));
    ugiProvider.invalidCache();
    Assert.assertNotSame(aliceUGI,ugiProvider.getConfiguredUGI(aliceInfo));
  }
  finally {
    httpService.stopAndWait();
  }
}"
5382,"/** 
 * Sets the logging context. <p> NOTE: in work execution frameworks where threads are shared between workers (like Akka) we would have to init context very frequently (before every chunk of work is started). In that case we really want to re-use logging context object instance. </p>
 * @param context context to set
 */
public static void setLoggingContext(LoggingContext context){
  loggingContext.set(context);
}","/** 
 * Sets the logging context. <p> NOTE: in work execution frameworks where threads are shared between workers (like Akka) we would have to init context very frequently (before every chunk of work is started). In that case we really want to re-use logging context object instance. </p>
 * @param context context to set
 */
public static void setLoggingContext(LoggingContext context){
  loggingContext.set(context);
  try {
    MDC.setContextMap(context.getSystemTagsAsString());
  }
 catch (  IllegalStateException e) {
  }
}"
5383,"public DefaultStreamBatchWriter(HttpURLConnection connection,Id.Stream stream) throws IOException {
  this.connection=connection;
  this.outputStream=connection.getOutputStream();
  this.stream=stream;
  this.open=true;
}","public DefaultStreamBatchWriter(HttpURLConnection connection,StreamId stream) throws IOException {
  this.connection=connection;
  this.outputStream=connection.getOutputStream();
  this.stream=stream;
  this.open=true;
}"
5384,"private void registerStream(Id.Stream stream){
  if (!isStreamRegistered.containsKey(stream)) {
    runtimeUsageRegistry.registerAll(owners,stream.toEntityId());
    isStreamRegistered.put(stream,true);
  }
  lineageWriter.addAccess(run.toEntityId(),stream.toEntityId(),AccessType.WRITE);
}","private void registerStream(StreamId stream){
  if (!isStreamRegistered.containsKey(stream)) {
    runtimeUsageRegistry.registerAll(owners,stream);
    isStreamRegistered.put(stream,true);
  }
  lineageWriter.addAccess(run,stream,AccessType.WRITE);
}"
5385,"private void writeToStream(Id.Stream stream,HttpRequest.Builder builder) throws IOException {
  if (authorizationEnabled) {
    builder.addHeader(Constants.Security.Headers.USER_ID,authenticationContext.getPrincipal().getName());
  }
  HttpResponse response=HttpRequests.execute(builder.build(),new DefaultHttpRequestConfig(false));
  int responseCode=response.getResponseCode();
  if (responseCode == HttpResponseStatus.NOT_FOUND.getCode()) {
    throw new IOException(String.format(""String_Node_Str"",stream));
  }
  registerStream(stream);
  if (responseCode < 200 || responseCode >= 300) {
    throw new IOException(String.format(""String_Node_Str"",stream,responseCode));
  }
}","private void writeToStream(StreamId stream,HttpRequest.Builder builder) throws IOException {
  if (authorizationEnabled) {
    builder.addHeader(Constants.Security.Headers.USER_ID,authenticationContext.getPrincipal().getName());
  }
  HttpResponse response=remoteClient.execute(builder.build());
  int responseCode=response.getResponseCode();
  if (responseCode == HttpResponseStatus.NOT_FOUND.getCode()) {
    throw new IOException(String.format(""String_Node_Str"",stream));
  }
  registerStream(stream);
  if (responseCode < 200 || responseCode >= 300) {
    throw new IOException(String.format(""String_Node_Str"",stream,responseCode));
  }
}"
5386,"@Override public StreamBatchWriter createBatchWriter(String stream,String contentType) throws IOException {
  URL url=getStreamURL(stream,true);
  HttpURLConnection connection=(HttpURLConnection)url.openConnection();
  connection.setRequestMethod(HttpMethod.POST.name());
  connection.setReadTimeout(15000);
  connection.setConnectTimeout(15000);
  connection.setRequestProperty(HttpHeaders.CONTENT_TYPE,contentType);
  if (authorizationEnabled) {
    connection.setRequestProperty(Constants.Security.Headers.USER_ID,authenticationContext.getPrincipal().getName());
  }
  connection.setDoOutput(true);
  connection.setChunkedStreamingMode(0);
  connection.connect();
  try {
    Id.Stream streamId=Id.Stream.from(namespace,stream);
    registerStream(streamId);
    return new DefaultStreamBatchWriter(connection,streamId);
  }
 catch (  IOException e) {
    connection.disconnect();
    throw e;
  }
}","@Override public StreamBatchWriter createBatchWriter(final String stream,String contentType) throws IOException {
  URL url=Retries.callWithRetries(new Retries.Callable<URL,IOException>(){
    @Override public URL call() throws IOException {
      return remoteClient.resolve(stream + ""String_Node_Str"");
    }
  }
,retryStrategy);
  HttpURLConnection connection=(HttpURLConnection)url.openConnection();
  connection.setRequestMethod(HttpMethod.POST.name());
  connection.setReadTimeout(15000);
  connection.setConnectTimeout(15000);
  connection.setRequestProperty(HttpHeaders.CONTENT_TYPE,contentType);
  if (authorizationEnabled) {
    connection.setRequestProperty(Constants.Security.Headers.USER_ID,authenticationContext.getPrincipal().getName());
  }
  connection.setDoOutput(true);
  connection.setChunkedStreamingMode(0);
  connection.connect();
  try {
    StreamId streamId=namespace.stream(stream);
    registerStream(streamId);
    return new DefaultStreamBatchWriter(connection,streamId);
  }
 catch (  IOException e) {
    connection.disconnect();
    throw e;
  }
}"
5387,"@Inject public DefaultStreamWriter(@Assisted(""String_Node_Str"") Id.Run run,@Assisted(""String_Node_Str"") Iterable<? extends EntityId> owners,@Assisted(""String_Node_Str"") RetryStrategy retryStrategy,RuntimeUsageRegistry runtimeUsageRegistry,LineageWriter lineageWriter,DiscoveryServiceClient discoveryServiceClient,AuthenticationContext authenticationContext,CConfiguration cConf){
  this.run=run;
  this.namespace=run.getNamespace();
  this.owners=owners;
  this.lineageWriter=lineageWriter;
  this.endpointStrategy=new RandomEndpointStrategy(discoveryServiceClient.discover(Constants.Service.STREAMS));
  this.isStreamRegistered=Maps.newConcurrentMap();
  this.runtimeUsageRegistry=runtimeUsageRegistry;
  this.authenticationContext=authenticationContext;
  this.authorizationEnabled=cConf.getBoolean(Constants.Security.Authorization.ENABLED);
  this.retryStrategy=retryStrategy;
}","@Inject public DefaultStreamWriter(@Assisted(""String_Node_Str"") Id.Run run,@Assisted(""String_Node_Str"") Iterable<? extends EntityId> owners,@Assisted(""String_Node_Str"") RetryStrategy retryStrategy,RuntimeUsageRegistry runtimeUsageRegistry,LineageWriter lineageWriter,DiscoveryServiceClient discoveryServiceClient,AuthenticationContext authenticationContext,CConfiguration cConf){
  this.run=run.toEntityId();
  this.namespace=run.getNamespace().toEntityId();
  this.owners=owners;
  this.lineageWriter=lineageWriter;
  this.isStreamRegistered=Maps.newConcurrentMap();
  this.runtimeUsageRegistry=runtimeUsageRegistry;
  this.authenticationContext=authenticationContext;
  this.authorizationEnabled=cConf.getBoolean(Constants.Security.Authorization.ENABLED);
  this.retryStrategy=retryStrategy;
  this.remoteClient=new RemoteClient(discoveryServiceClient,Constants.Service.STREAMS,new DefaultHttpRequestConfig(false),String.format(""String_Node_Str"",Constants.Gateway.API_VERSION_3,namespace.getNamespace()));
}"
5388,"@Override public void writeFile(String stream,File file,String contentType) throws IOException {
  URL url=getStreamURL(stream,true);
  HttpRequest.Builder requestBuilder=HttpRequest.post(url).withBody(file).addHeader(HttpHeaders.CONTENT_TYPE,contentType);
  writeToStream(Id.Stream.from(namespace,stream),requestBuilder);
}","@Override public void writeFile(final String stream,final File file,final String contentType) throws IOException {
  Retries.callWithRetries(new Retries.Callable<Void,IOException>(){
    @Override public Void call() throws IOException {
      HttpRequest.Builder requestBuilder=remoteClient.requestBuilder(HttpMethod.POST,stream + ""String_Node_Str"").withBody(file).addHeader(HttpHeaders.CONTENT_TYPE,contentType);
      writeToStream(namespace.stream(stream),requestBuilder);
      return null;
    }
  }
,retryStrategy);
}"
5389,"@Override public void handle(Throwable t,HttpRequest request,HttpResponder responder){
  if (Iterables.size(Iterables.filter(Throwables.getCausalChain(t),ServiceUnavailableException.class)) > 0) {
    responder.sendString(HttpResponseStatus.SERVICE_UNAVAILABLE,t.getMessage());
    return;
  }
  if (t instanceof HttpErrorStatusProvider) {
    logWithTrace(request,t);
    responder.sendString(HttpResponseStatus.valueOf(((HttpErrorStatusProvider)t).getStatusCode()),t.getMessage());
    return;
  }
  if (t.getClass().getName().endsWith(""String_Node_Str"")) {
    logWithTrace(request,t);
    responder.sendString(HttpResponseStatus.NOT_FOUND,t.getMessage());
    return;
  }
  if (t.getClass().getName().endsWith(""String_Node_Str"")) {
    logWithTrace(request,t);
    responder.sendString(HttpResponseStatus.CONFLICT,t.getMessage());
    return;
  }
  LOG.error(""String_Node_Str"",request.getMethod().getName(),request.getUri(),Objects.firstNonNull(SecurityRequestContext.getUserId(),""String_Node_Str""),t);
  responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,Throwables.getRootCause(t).getMessage());
}","@Override public void handle(Throwable t,HttpRequest request,HttpResponder responder){
  for (  Throwable cause : Throwables.getCausalChain(t)) {
    if (cause instanceof ServiceUnavailableException) {
      responder.sendString(HttpResponseStatus.SERVICE_UNAVAILABLE,cause.getMessage());
      return;
    }
  }
  if (t instanceof HttpErrorStatusProvider) {
    logWithTrace(request,t);
    responder.sendString(HttpResponseStatus.valueOf(((HttpErrorStatusProvider)t).getStatusCode()),t.getMessage());
    return;
  }
  if (t.getClass().getName().endsWith(""String_Node_Str"")) {
    logWithTrace(request,t);
    responder.sendString(HttpResponseStatus.NOT_FOUND,t.getMessage());
    return;
  }
  if (t.getClass().getName().endsWith(""String_Node_Str"")) {
    logWithTrace(request,t);
    responder.sendString(HttpResponseStatus.CONFLICT,t.getMessage());
    return;
  }
  LOG.error(""String_Node_Str"",request.getMethod().getName(),request.getUri(),Objects.firstNonNull(SecurityRequestContext.getUserId(),""String_Node_Str""),t);
  responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,Throwables.getRootCause(t).getMessage());
}"
5390,"/** 
 * Gets stream properties from the request. If there is request is invalid, response will be made and   {@code null}will be return.
 */
private StreamProperties getAndValidateConfig(HttpRequest request,HttpResponder responder){
  Reader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()));
  StreamProperties properties;
  try {
    properties=GSON.fromJson(reader,StreamProperties.class);
  }
 catch (  Exception e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"" + ""String_Node_Str"");
    return null;
  }
  Long ttl=properties.getTTL();
  if (ttl != null && ttl < 0) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return null;
  }
  FormatSpecification formatSpec=properties.getFormat();
  if (formatSpec != null) {
    String formatName=formatSpec.getName();
    if (formatName == null) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return null;
    }
    try {
      RecordFormat<?,?> format=RecordFormats.createInitializedFormat(formatSpec);
      formatSpec=new FormatSpecification(formatSpec.getName(),format.getSchema(),formatSpec.getSettings());
    }
 catch (    UnsupportedTypeException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"" + formatName + ""String_Node_Str"");
      return null;
    }
catch (    Exception e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"" + formatName);
      return null;
    }
  }
  Integer threshold=properties.getNotificationThresholdMB();
  if (threshold != null && threshold <= 0) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return null;
  }
  if (properties.getOwnerPrincipal() != null) {
    SecurityUtil.validateKerberosPrincipal(properties.getOwnerPrincipal());
  }
  return new StreamProperties(ttl,formatSpec,threshold,properties.getDescription(),properties.getOwnerPrincipal());
}","/** 
 * Gets stream properties from the request. If there is request is invalid, a BadRequestException will be thrown.
 */
private StreamProperties getAndValidateConfig(HttpRequest request) throws BadRequestException {
  Reader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()));
  StreamProperties properties;
  try {
    properties=GSON.fromJson(reader,StreamProperties.class);
  }
 catch (  Exception e) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"" + e.getMessage());
  }
  Long ttl=properties.getTTL();
  if (ttl != null && ttl < 0) {
    throw new BadRequestException(""String_Node_Str"" + ttl + ""String_Node_Str"");
  }
  FormatSpecification formatSpec=properties.getFormat();
  if (formatSpec != null) {
    String formatName=formatSpec.getName();
    if (formatName == null) {
      throw new BadRequestException(""String_Node_Str"");
    }
    try {
      RecordFormat<?,?> format=RecordFormats.createInitializedFormat(formatSpec);
      formatSpec=new FormatSpecification(formatSpec.getName(),format.getSchema(),formatSpec.getSettings());
    }
 catch (    UnsupportedTypeException e) {
      throw new BadRequestException(""String_Node_Str"" + formatName + ""String_Node_Str"");
    }
catch (    Exception e) {
      throw new BadRequestException(""String_Node_Str"" + formatName);
    }
  }
  Integer threshold=properties.getNotificationThresholdMB();
  if (threshold != null && threshold <= 0) {
    throw new BadRequestException(""String_Node_Str"" + threshold + ""String_Node_Str"");
  }
  if (properties.getOwnerPrincipal() != null) {
    SecurityUtil.validateKerberosPrincipal(properties.getOwnerPrincipal());
  }
  return new StreamProperties(ttl,formatSpec,threshold,properties.getDescription(),properties.getOwnerPrincipal());
}"
5391,"@PUT @Path(""String_Node_Str"") @AuditPolicy(AuditDetail.REQUEST_BODY) public void setConfig(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream) throws Exception {
  StreamId streamId=validateAndGetStreamId(namespaceId,stream);
  checkStreamExists(streamId);
  StreamProperties properties=getAndValidateConfig(request,responder);
  if (properties == null) {
    return;
  }
  streamAdmin.updateConfig(streamId,properties);
  responder.sendStatus(HttpResponseStatus.OK);
}","@PUT @Path(""String_Node_Str"") @AuditPolicy(AuditDetail.REQUEST_BODY) public void setConfig(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream) throws Exception {
  StreamId streamId=validateAndGetStreamId(namespaceId,stream);
  checkStreamExists(streamId);
  StreamProperties properties=getAndValidateConfig(request);
  streamAdmin.updateConfig(streamId,properties);
  responder.sendStatus(HttpResponseStatus.OK);
}"
5392,"@PUT @Path(""String_Node_Str"") @AuditPolicy(AuditDetail.REQUEST_BODY) public void create(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream) throws Exception {
  namespaceQueryAdmin.get(new NamespaceId(namespaceId));
  StreamId streamId=validateAndGetStreamId(namespaceId,stream);
  Properties props=new Properties();
  StreamProperties streamProperties;
  if (request.getContent().readable()) {
    streamProperties=getAndValidateConfig(request,responder);
    if (streamProperties == null) {
      return;
    }
    if (streamProperties.getTTL() != null) {
      props.put(Constants.Stream.TTL,Long.toString(streamProperties.getTTL()));
    }
    if (streamProperties.getNotificationThresholdMB() != null) {
      props.put(Constants.Stream.NOTIFICATION_THRESHOLD,Integer.toString(streamProperties.getNotificationThresholdMB()));
    }
    if (streamProperties.getDescription() != null) {
      props.put(Constants.Stream.DESCRIPTION,streamProperties.getDescription());
    }
    if (streamProperties.getFormat() != null) {
      props.put(Constants.Stream.FORMAT_SPECIFICATION,GSON.toJson(streamProperties.getFormat()));
    }
    if (streamProperties.getOwnerPrincipal() != null) {
      props.put(Constants.Security.PRINCIPAL,streamProperties.getOwnerPrincipal());
    }
  }
  streamAdmin.create(streamId,props);
  responder.sendStatus(HttpResponseStatus.OK);
}","@PUT @Path(""String_Node_Str"") @AuditPolicy(AuditDetail.REQUEST_BODY) public void create(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream) throws Exception {
  namespaceQueryAdmin.get(new NamespaceId(namespaceId));
  StreamId streamId=validateAndGetStreamId(namespaceId,stream);
  Properties props=new Properties();
  StreamProperties streamProperties;
  if (request.getContent().readable()) {
    streamProperties=getAndValidateConfig(request);
    if (streamProperties.getTTL() != null) {
      props.put(Constants.Stream.TTL,Long.toString(streamProperties.getTTL()));
    }
    if (streamProperties.getNotificationThresholdMB() != null) {
      props.put(Constants.Stream.NOTIFICATION_THRESHOLD,Integer.toString(streamProperties.getNotificationThresholdMB()));
    }
    if (streamProperties.getDescription() != null) {
      props.put(Constants.Stream.DESCRIPTION,streamProperties.getDescription());
    }
    if (streamProperties.getFormat() != null) {
      props.put(Constants.Stream.FORMAT_SPECIFICATION,GSON.toJson(streamProperties.getFormat()));
    }
    if (streamProperties.getOwnerPrincipal() != null) {
      props.put(Constants.Security.PRINCIPAL,streamProperties.getOwnerPrincipal());
    }
  }
  streamAdmin.create(streamId,props);
  responder.sendStatus(HttpResponseStatus.OK);
}"
5393,"@POST @Path(""String_Node_Str"") public void enqueue(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream) throws Exception {
  StreamId streamId=validateAndGetStreamId(namespaceId,stream);
  authorizationEnforcer.enforce(streamId,authenticationContext.getPrincipal(),Action.WRITE);
  try {
    streamWriter.enqueue(streamId,getHeaders(request,stream),request.getContent().toByteBuffer());
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",stream,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}","@POST @Path(""String_Node_Str"") public void enqueue(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream) throws Exception {
  StreamId streamId=validateAndGetStreamId(namespaceId,stream);
  authorizationEnforcer.enforce(streamId,authenticationContext.getPrincipal(),Action.WRITE);
  streamWriter.enqueue(streamId,getHeaders(request,stream),request.getContent().toByteBuffer());
  responder.sendStatus(HttpResponseStatus.OK);
}"
5394,"@Override public void abort(@Nullable TransactionFailureException cause) throws TransactionFailureException {
  if (currentTx == null) {
    return;
  }
  try {
    boolean success=true;
    for (    TransactionAware txAware : getTransactionAwares()) {
      try {
        success=txAware.rollbackTx() && success;
      }
 catch (      Throwable e) {
        if (cause == null) {
          cause=new TransactionFailureException(String.format(""String_Node_Str"",txAware.getTransactionAwareName(),currentTx.getTransactionId()),e);
        }
 else {
          cause.addSuppressed(e);
        }
        success=false;
      }
    }
    if (success) {
      txClient.abort(currentTx);
    }
 else {
      txClient.invalidate(currentTx.getTransactionId());
    }
    if (cause != null) {
      throw cause;
    }
  }
  finally {
    currentTx=null;
    cleanup();
  }
}","@Override public void abort(@Nullable TransactionFailureException cause) throws TransactionFailureException {
  if (currentTx == null) {
    return;
  }
  try {
    boolean success=true;
    for (    TransactionAware txAware : getTransactionAwares()) {
      try {
        success=txAware.rollbackTx() && success;
      }
 catch (      Throwable e) {
        if (cause == null) {
          cause=new TransactionFailureException(String.format(""String_Node_Str"",txAware.getTransactionAwareName(),currentTx.getTransactionId()),e);
        }
 else {
          cause.addSuppressed(e);
        }
        success=false;
      }
    }
    try {
      if (success) {
        txClient.abort(currentTx);
      }
 else {
        txClient.invalidate(currentTx.getTransactionId());
      }
    }
 catch (    Throwable t) {
      if (cause == null) {
        cause=new TransactionFailureException(String.format(""String_Node_Str"",success ? ""String_Node_Str"" : ""String_Node_Str"",currentTx.getTransactionId()));
      }
 else {
        cause.addSuppressed(t);
      }
    }
    if (cause != null) {
      throw cause;
    }
  }
  finally {
    currentTx=null;
    cleanup();
  }
}"
5395,"@Override public void abort(Transaction tx){
  state=CommitState.Aborted;
  super.abort(tx);
}","@Override public void abort(Transaction transaction){
  throw new RuntimeException();
}"
5396,"@Override public boolean canCommit(Transaction tx,Collection<byte[]> changeIds) throws TransactionNotInProgressException {
  if (failCanCommitOnce) {
    failCanCommitOnce=false;
    return false;
  }
 else {
    return super.canCommit(tx,changeIds);
  }
}","@Override public boolean canCommit(Transaction transaction,Collection<byte[]> collection) throws TransactionNotInProgressException {
  throw new RuntimeException();
}"
5397,"@Inject RemoteLineageWriter(CConfiguration cConf,DiscoveryServiceClient discoveryClient){
  super(cConf,discoveryClient,Constants.Service.REMOTE_SYSTEM_OPERATION);
}","@Inject RemoteLineageWriter(DiscoveryServiceClient discoveryClient){
  super(discoveryClient,Constants.Service.REMOTE_SYSTEM_OPERATION);
}"
5398,"@Inject RemoteRuntimeStore(CConfiguration cConf,DiscoveryServiceClient discoveryClient){
  super(cConf,discoveryClient,Constants.Service.REMOTE_SYSTEM_OPERATION);
}","@Inject RemoteRuntimeStore(DiscoveryServiceClient discoveryClient){
  super(discoveryClient,Constants.Service.REMOTE_SYSTEM_OPERATION);
}"
5399,"@Inject RemoteRuntimeUsageRegistry(CConfiguration cConf,DiscoveryServiceClient discoveryClient){
  super(cConf,discoveryClient,Constants.Service.REMOTE_SYSTEM_OPERATION);
}","@Inject RemoteRuntimeUsageRegistry(DiscoveryServiceClient discoveryClient){
  super(discoveryClient,Constants.Service.REMOTE_SYSTEM_OPERATION);
}"
5400,"public ServiceUnavailableException(String serviceName,Throwable cause){
  super(""String_Node_Str"" + serviceName + ""String_Node_Str"",cause);
  this.serviceName=serviceName;
}","public ServiceUnavailableException(String serviceName,String message,Throwable cause){
  super(message,cause);
  this.serviceName=serviceName;
}"
5401,"protected RemoteOpsClient(CConfiguration cConf,final DiscoveryServiceClient discoveryClient,final String discoverableServiceName){
  this.endpointStrategySupplier=Suppliers.memoize(new Supplier<EndpointStrategy>(){
    @Override public EndpointStrategy get(){
      return new RandomEndpointStrategy(discoveryClient.discover(discoverableServiceName));
    }
  }
);
  this.httpRequestConfig=new DefaultHttpRequestConfig(false);
  this.discoverableServiceName=discoverableServiceName;
}","protected RemoteOpsClient(final DiscoveryServiceClient discoveryClient,final String discoverableServiceName){
  this.remoteClient=new RemoteClient(discoveryClient,discoverableServiceName,new DefaultHttpRequestConfig(false),""String_Node_Str"");
}"
5402,"protected HttpResponse executeRequest(String methodName,Map<String,String> headers,Object... arguments){
  return doRequest(""String_Node_Str"" + methodName,HttpMethod.POST,headers,GSON.toJson(createArguments(arguments)));
}","protected HttpResponse executeRequest(String methodName,Map<String,String> headers,Object... arguments){
  String body=GSON.toJson(createBody(arguments));
  HttpRequest.Builder builder=remoteClient.requestBuilder(HttpMethod.POST,methodName).addHeaders(headers);
  if (body != null) {
    builder.withBody(body);
  }
  HttpRequest request=builder.build();
  try {
    HttpResponse response=remoteClient.execute(request);
    if (response.getResponseCode() == HttpURLConnection.HTTP_OK) {
      return response;
    }
    throw new RuntimeException(String.format(""String_Node_Str"",remoteClient.createErrorMessage(request,body),response));
  }
 catch (  IOException e) {
    throw new RuntimeException(remoteClient.createErrorMessage(request,body),e);
  }
}"
5403,"private void checkLogPartitionKey(Set<String> problemKeys){
  if (!isValidPartitionKey(cConf.get(Constants.Logging.LOG_PUBLISH_PARTITION_KEY))) {
    problemKeys.add(Constants.Logging.LOG_PUBLISH_PARTITION_KEY);
  }
}","private void checkLogPartitionKey(Set<String> problemKeys){
  validatePartitionKey(Constants.Logging.LOG_PUBLISH_PARTITION_KEY,problemKeys);
}"
5404,"private void checkPotentialPortConflicts(){
  Multimap<Integer,String> services=HashMultimap.create();
  if (cConf.getBoolean(Constants.Security.SSL.EXTERNAL_ENABLED)) {
    services.put(cConf.getInt(Constants.Router.ROUTER_SSL_PORT),""String_Node_Str"");
    services.put(cConf.getInt(Constants.Security.AuthenticationServer.SSL_PORT),""String_Node_Str"");
  }
 else {
    services.put(cConf.getInt(Constants.Router.ROUTER_PORT),""String_Node_Str"");
    services.put(cConf.getInt(Constants.Security.AUTH_SERVER_BIND_PORT),""String_Node_Str"");
  }
  for (  Integer port : services.keySet()) {
    Collection<String> conflictingServices=services.get(port);
    if (conflictingServices.size() > 1) {
      LOG.warn(""String_Node_Str"",port,Joiner.on(""String_Node_Str"").join(conflictingServices));
    }
  }
}","private void checkPotentialPortConflicts(Set<String> problemKeys){
  Multimap<Integer,String> services=HashMultimap.create();
  String sslKey=Constants.Security.SSL.EXTERNAL_ENABLED;
  boolean isSSL;
  try {
    isSSL=cConf.getBoolean(sslKey);
  }
 catch (  Exception e) {
    logProblem(""String_Node_Str"",sslKey,cConf.get(sslKey),e);
    problemKeys.add(Constants.Security.SSL.EXTERNAL_ENABLED);
    return;
  }
  if (isSSL) {
    services.put(cConf.getInt(Constants.Router.ROUTER_SSL_PORT),""String_Node_Str"");
    services.put(cConf.getInt(Constants.Security.AuthenticationServer.SSL_PORT),""String_Node_Str"");
  }
 else {
    services.put(cConf.getInt(Constants.Router.ROUTER_PORT),""String_Node_Str"");
    services.put(cConf.getInt(Constants.Security.AUTH_SERVER_BIND_PORT),""String_Node_Str"");
  }
  for (  Integer port : services.keySet()) {
    Collection<String> conflictingServices=services.get(port);
    if (conflictingServices.size() > 1) {
      LOG.warn(""String_Node_Str"",port,Joiner.on(""String_Node_Str"").join(conflictingServices));
    }
  }
}"
5405,"@Override public void run(){
  LOG.info(""String_Node_Str"");
  Set<String> problemKeys=new HashSet<>();
  checkServiceResources(problemKeys);
  checkBindAddresses();
  checkPotentialPortConflicts();
  checkKafkaTopic(problemKeys);
  checkMessagingTopics(problemKeys);
  checkLogPartitionKey(problemKeys);
  if (!problemKeys.isEmpty()) {
    throw new RuntimeException(""String_Node_Str"" + Joiner.on(',').join(problemKeys));
  }
  LOG.info(""String_Node_Str"");
}","@Override public void run(){
  LOG.info(""String_Node_Str"");
  Set<String> problemKeys=new HashSet<>();
  checkServiceResources(problemKeys);
  checkBindAddresses();
  checkPotentialPortConflicts(problemKeys);
  checkKafkaTopic(problemKeys);
  checkMessagingTopics(problemKeys);
  checkLogPartitionKey(problemKeys);
  if (!problemKeys.isEmpty()) {
    throw new RuntimeException(""String_Node_Str"" + Joiner.on(',').join(problemKeys));
  }
  LOG.info(""String_Node_Str"");
}"
5406,"private void checkServiceResources(Set<String> problemKeys){
  for (  ServiceResourceKeys serviceResourceKeys : systemServicesResourceKeys) {
    verifyResources(serviceResourceKeys,problemKeys);
    boolean instancesIsPositive=!problemKeys.contains(serviceResourceKeys.getInstancesKey());
    boolean maxInstancesIsPositive=!problemKeys.contains(serviceResourceKeys.getMaxInstancesKey());
    if (instancesIsPositive && maxInstancesIsPositive) {
      int instances=serviceResourceKeys.getInstances();
      int maxInstances=serviceResourceKeys.getMaxInstances();
      if (instances > maxInstances) {
        LOG.error(""String_Node_Str"",serviceResourceKeys.getInstancesKey(),instances,serviceResourceKeys.getMaxInstancesKey(),maxInstances);
        problemKeys.add(serviceResourceKeys.getInstancesKey());
      }
    }
  }
}","private void checkServiceResources(Set<String> problemKeys){
  for (  ServiceResourceKeys serviceResourceKeys : systemServicesResourceKeys) {
    validatePositiveInteger(serviceResourceKeys.getMemoryKey(),problemKeys);
    validatePositiveInteger(serviceResourceKeys.getVcoresKey(),problemKeys);
    Integer instances=validatePositiveInteger(serviceResourceKeys.getInstancesKey(),problemKeys);
    Integer maxInstances=validatePositiveInteger(serviceResourceKeys.getMaxInstancesKey(),problemKeys);
    if (instances != null && maxInstances != null && instances > maxInstances) {
      LOG.error(""String_Node_Str"",serviceResourceKeys.getInstancesKey(),instances,serviceResourceKeys.getMaxInstancesKey(),maxInstances);
      problemKeys.add(serviceResourceKeys.getInstancesKey());
    }
  }
}"
5407,"private void checkMessagingTopics(Set<String> problemKeys){
  if (!EntityId.isValidId(cConf.get(Constants.Audit.TOPIC))) {
    problemKeys.add(Constants.Audit.TOPIC);
  }
  if (!EntityId.isValidId(cConf.get(Constants.Notification.TOPIC))) {
    problemKeys.add(Constants.Notification.TOPIC);
  }
}","private void checkMessagingTopics(Set<String> problemKeys){
  validateMessagingTopic(Constants.Audit.TOPIC,problemKeys);
  validateMessagingTopic(Constants.Notification.TOPIC,problemKeys);
}"
5408,"private void checkBindAddresses(){
  Set<String> bindAddressKeys=ImmutableSet.of(Constants.Service.MASTER_SERVICES_BIND_ADDRESS,Constants.Router.ADDRESS);
  for (  String bindAddressKey : bindAddressKeys) {
    String bindAddress=cConf.get(bindAddressKey);
    try {
      if (InetAddress.getByName(bindAddress).isLoopbackAddress()) {
        LOG.warn(""String_Node_Str"",bindAddressKey,bindAddress);
      }
    }
 catch (    UnknownHostException e) {
      LOG.warn(""String_Node_Str"",bindAddressKey,e);
    }
  }
}","private void checkBindAddresses(){
  Set<String> bindAddressKeys=ImmutableSet.of(Constants.Service.MASTER_SERVICES_BIND_ADDRESS,Constants.Router.ADDRESS);
  for (  String bindAddressKey : bindAddressKeys) {
    String bindAddress=cConf.get(bindAddressKey);
    try {
      if (InetAddress.getByName(bindAddress).isLoopbackAddress()) {
        LOG.warn(""String_Node_Str"",bindAddressKey,bindAddress);
      }
    }
 catch (    UnknownHostException e) {
      LOG.warn(""String_Node_Str"",bindAddressKey,bindAddress,e);
    }
  }
}"
5409,"private void checkKafkaTopic(Set<String> problemKeys){
  if (!isValidKafkaTopic(Constants.Logging.KAFKA_TOPIC)) {
    problemKeys.add(Constants.Logging.KAFKA_TOPIC);
  }
}","private void checkKafkaTopic(Set<String> problemKeys){
  validateKafkaTopic(Constants.Logging.KAFKA_TOPIC,problemKeys);
}"
5410,"public boolean canStartMaster(){
  List<CheckRunner.Failure> failures=checkRunner.runChecks();
  if (!failures.isEmpty()) {
    for (    CheckRunner.Failure failure : failures) {
      LOG.error(""String_Node_Str"",failure.getName(),failure.getException().getClass().getSimpleName(),failure.getException().getMessage());
      if (failure.getException().getCause() != null) {
        LOG.error(""String_Node_Str"",ExceptionUtils.getRootCauseMessage(failure.getException().getCause()));
      }
    }
    LOG.error(""String_Node_Str"" + ""String_Node_Str"");
    return false;
  }
  return true;
}","public boolean canStartMaster(){
  List<CheckRunner.Failure> failures=checkRunner.runChecks();
  if (!failures.isEmpty()) {
    for (    CheckRunner.Failure failure : failures) {
      LOG.error(""String_Node_Str"",failure.getName(),failure.getException().getClass().getSimpleName(),failure.getException().getMessage(),failure.getException());
      if (failure.getException().getCause() != null) {
        LOG.error(""String_Node_Str"",ExceptionUtils.getRootCauseMessage(failure.getException().getCause()));
      }
    }
    LOG.error(""String_Node_Str"" + ""String_Node_Str"");
    return false;
  }
  return true;
}"
5411,"private boolean isValidPartitionKey(String key){
  try {
    LogPartitionType.valueOf(key);
  }
 catch (  IllegalArgumentException e) {
    LOG.error(""String_Node_Str"",key,e.getMessage());
    return false;
  }
  return true;
}","private boolean isValidPartitionKey(String key){
  try {
    LogPartitionType.valueOf(key.toUpperCase());
  }
 catch (  IllegalArgumentException|NullPointerException e) {
    LOG.error(""String_Node_Str"",key,e.getMessage());
    return false;
  }
  return true;
}"
5412,"private void checkLogPartitionKey(Set<String> problemKeys){
  if (!isValidPartitionKey(cConf.get(Constants.Logging.LOG_PUBLISH_PARTITION_KEY).toUpperCase())) {
    problemKeys.add(Constants.Logging.LOG_PUBLISH_PARTITION_KEY);
  }
}","private void checkLogPartitionKey(Set<String> problemKeys){
  if (!isValidPartitionKey(cConf.get(Constants.Logging.LOG_PUBLISH_PARTITION_KEY))) {
    problemKeys.add(Constants.Logging.LOG_PUBLISH_PARTITION_KEY);
  }
}"
5413,"public boolean canStartMaster(){
  List<CheckRunner.Failure> failures=checkRunner.runChecks();
  if (!failures.isEmpty()) {
    for (    CheckRunner.Failure failure : failures) {
      LOG.error(""String_Node_Str"",failure.getName(),failure.getException().getMessage());
      if (failure.getException().getCause() != null) {
        LOG.error(""String_Node_Str"",ExceptionUtils.getRootCauseMessage(failure.getException().getCause()));
      }
    }
    LOG.error(""String_Node_Str"" + ""String_Node_Str"");
    return false;
  }
  return true;
}","public boolean canStartMaster(){
  List<CheckRunner.Failure> failures=checkRunner.runChecks();
  if (!failures.isEmpty()) {
    for (    CheckRunner.Failure failure : failures) {
      LOG.error(""String_Node_Str"",failure.getName(),failure.getException().getClass().getSimpleName(),failure.getException().getMessage());
      if (failure.getException().getCause() != null) {
        LOG.error(""String_Node_Str"",ExceptionUtils.getRootCauseMessage(failure.getException().getCause()));
      }
    }
    LOG.error(""String_Node_Str"" + ""String_Node_Str"");
    return false;
  }
  return true;
}"
5414,"private void rewriteHiveConfig(){
  URL resourceURL=getLocalResourceURL(""String_Node_Str"");
  if (resourceURL == null) {
    LOG.warn(""String_Node_Str"");
    return;
  }
  try (InputStream is=new FileInputStream(new File(resourceURL.toURI()))){
    Configuration conf=new Configuration(false);
    conf.addResource(is);
    conf.setBoolean(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,false);
    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER,false);
    conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,UserGroupInformation.AuthenticationMethod.SIMPLE.name());
    String sparkHome=System.getenv(Constants.SPARK_HOME);
    if (sparkHome != null) {
      LOG.debug(""String_Node_Str"",sparkHome);
      conf.set(""String_Node_Str"",sparkHome);
    }
    File newConfFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    try (FileOutputStream os=new FileOutputStream(newConfFile)){
      conf.writeXml(os);
    }
     Files.move(newConfFile.toPath(),Paths.get(resourceURL.toURI()),StandardCopyOption.REPLACE_EXISTING);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",resourceURL,e);
    throw Throwables.propagate(e);
  }
}","private void rewriteHiveConfig(){
  URL resourceURL=getLocalResourceURL(""String_Node_Str"");
  if (resourceURL == null) {
    LOG.warn(""String_Node_Str"");
    return;
  }
  try (InputStream is=new FileInputStream(new File(resourceURL.toURI()))){
    Configuration conf=new Configuration(false);
    conf.addResource(is);
    conf.setBoolean(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,false);
    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER,false);
    String sparkHome=System.getenv(Constants.SPARK_HOME);
    if (sparkHome != null) {
      LOG.debug(""String_Node_Str"",sparkHome);
      conf.set(""String_Node_Str"",sparkHome);
    }
    File newConfFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    try (FileOutputStream os=new FileOutputStream(newConfFile)){
      conf.writeXml(os);
    }
     Files.move(newConfFile.toPath(),Paths.get(resourceURL.toURI()),StandardCopyOption.REPLACE_EXISTING);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",resourceURL,e);
    throw Throwables.propagate(e);
  }
}"
5415,"@Before public void setupTest() throws Exception {
  Assert.assertEquals(ImmutableSet.<Privilege>of(),getAuthorizer().listPrivileges(ALICE));
}","@Before public void setupTest() throws Exception {
  Assert.assertEquals(ImmutableSet.<Privilege>of(),getAuthorizer().listPrivileges(ALICE));
  SecurityRequestContext.setUserId(ALICE.getName());
}"
5416,"@BeforeClass public static void setup(){
  instance=new InstanceId(getConfiguration().get(Constants.INSTANCE_NAME));
  oldUser=SecurityRequestContext.getUserId();
  SecurityRequestContext.setUserId(ALICE.getName());
}","@BeforeClass public static void setup(){
  instance=new InstanceId(getConfiguration().get(Constants.INSTANCE_NAME));
  oldUser=SecurityRequestContext.getUserId();
}"
5417,"private void rewriteHiveConfig(){
  URL resourceURL=getLocalResourceURL(""String_Node_Str"");
  if (resourceURL == null) {
    LOG.warn(""String_Node_Str"");
    return;
  }
  try (InputStream is=new FileInputStream(new File(resourceURL.toURI()))){
    Configuration conf=new Configuration(false);
    conf.addResource(is);
    conf.setBoolean(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,false);
    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER,false);
    conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,UserGroupInformation.AuthenticationMethod.SIMPLE.name());
    String sparkHome=System.getenv(Constants.SPARK_HOME);
    if (sparkHome != null) {
      LOG.debug(""String_Node_Str"",sparkHome);
      conf.set(""String_Node_Str"",sparkHome);
    }
    File newConfFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    try (FileOutputStream os=new FileOutputStream(newConfFile)){
      conf.writeXml(os);
    }
     Files.move(newConfFile.toPath(),Paths.get(resourceURL.toURI()),StandardCopyOption.REPLACE_EXISTING);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",resourceURL,e);
    throw Throwables.propagate(e);
  }
}","private void rewriteHiveConfig(){
  URL resourceURL=getLocalResourceURL(""String_Node_Str"");
  if (resourceURL == null) {
    LOG.warn(""String_Node_Str"");
    return;
  }
  try (InputStream is=new FileInputStream(new File(resourceURL.toURI()))){
    Configuration conf=new Configuration(false);
    conf.addResource(is);
    conf.setBoolean(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,false);
    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER,false);
    String sparkHome=System.getenv(Constants.SPARK_HOME);
    if (sparkHome != null) {
      LOG.debug(""String_Node_Str"",sparkHome);
      conf.set(""String_Node_Str"",sparkHome);
    }
    File newConfFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    try (FileOutputStream os=new FileOutputStream(newConfFile)){
      conf.writeXml(os);
    }
     Files.move(newConfFile.toPath(),Paths.get(resourceURL.toURI()),StandardCopyOption.REPLACE_EXISTING);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",resourceURL,e);
    throw Throwables.propagate(e);
  }
}"
5418,"@GET @Path(""String_Node_Str"") public void searchMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String searchQuery,@QueryParam(""String_Node_Str"") List<String> targets,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String sort,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int offset,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int limit,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numCursors,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String cursor,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") boolean showHidden) throws Exception {
  Set<EntityTypeSimpleName> types=Collections.emptySet();
  if (targets != null) {
    types=ImmutableSet.copyOf(Iterables.transform(targets,STRING_TO_TARGET_TYPE));
  }
  SortInfo sortInfo=SortInfo.of(URLDecoder.decode(sort,""String_Node_Str""));
  if (SortInfo.DEFAULT.equals(sortInfo)) {
    if (!(cursor.isEmpty()) || 0 != numCursors) {
      throw new BadRequestException(""String_Node_Str"");
    }
  }
  MetadataSearchResponse response=metadataAdmin.search(namespaceId,URLDecoder.decode(searchQuery,""String_Node_Str""),types,sortInfo,offset,limit,numCursors,cursor,showHidden);
  responder.sendJson(HttpResponseStatus.OK,response,MetadataSearchResponse.class,GSON);
}","@GET @Path(""String_Node_Str"") public void searchMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String searchQuery,@QueryParam(""String_Node_Str"") List<String> targets,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String sort,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int offset,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int limit,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numCursors,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String cursor,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") boolean showHidden) throws Exception {
  Set<EntityTypeSimpleName> types=Collections.emptySet();
  if (targets != null) {
    types=ImmutableSet.copyOf(Iterables.transform(targets,STRING_TO_TARGET_TYPE));
  }
  SortInfo sortInfo=SortInfo.of(URLDecoder.decode(sort,""String_Node_Str""));
  if (SortInfo.DEFAULT.equals(sortInfo)) {
    if (!(cursor.isEmpty()) || 0 != numCursors) {
      throw new BadRequestException(""String_Node_Str"");
    }
  }
  try {
    MetadataSearchResponse response=metadataAdmin.search(namespaceId,URLDecoder.decode(searchQuery,""String_Node_Str""),types,sortInfo,offset,limit,numCursors,cursor,showHidden);
    responder.sendJson(HttpResponseStatus.OK,response,MetadataSearchResponse.class,GSON);
  }
 catch (  Exception e) {
    if (Throwables.getRootCause(e) instanceof IllegalArgumentException) {
      throw new BadRequestException(e.getMessage(),e);
    }
    throw e;
  }
}"
5419,"/** 
 * Searches entities that match the specified search query in the specified namespace and   {@link NamespaceId#SYSTEM}for the specified   {@link EntityTypeSimpleName}.
 * @param namespaceId the namespace to search in
 * @param searchQuery the search query, which could be of two forms: [key]:[value] or just [value] and can have '*'at the end for a prefix search
 * @param types the {@link EntityTypeSimpleName} to restrict the search to, if empty all types are searched
 * @param sortInfo the {@link SortInfo} to sort the results by
 * @param offset index to start with in the search results. To return results from the beginning, pass {@code 0}. Only applies when #sortInfo is not   {@link SortInfo#DEFAULT}
 * @param limit number of results to return, starting from #offset. To return all, pass {@link Integer#MAX_VALUE}. Only applies when #sortInfo is not   {@link SortInfo#DEFAULT}
 * @param numCursors number of cursors to return in the response. A cursor identifies the first index of thenext page for pagination purposes. Only applies when #sortInfo is not  {@link SortInfo#DEFAULT}. Defaults to   {@code 0}
 * @param cursor the cursor that acts as the starting index for the requested page. This is only applicable when#sortInfo is not  {@link SortInfo#DEFAULT}. If offset is also specified, it is applied starting at the cursor. If   {@code null}, the first row is used as the cursor
 * @param showHidden boolean which specifies whether to display hidden entities (entity whose name start with ""_"")or not.
 * @return a {@link SearchResults} object containing a list of {@link MetadataEntry} containing each matching{@link NamespacedEntityId} with its associated metadata. It also optionally contains a list of cursorsfor subsequent queries to start with, if the specified #sortInfo is not  {@link SortInfo#DEFAULT}.
 */
public SearchResults search(String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,@Nullable String cursor,boolean showHidden){
  return SortInfo.DEFAULT.equals(sortInfo) ? searchByDefaultIndex(namespaceId,searchQuery,types,showHidden) : searchByCustomIndex(namespaceId,types,sortInfo,offset,limit,numCursors,cursor,showHidden);
}","/** 
 * Searches entities that match the specified search query in the specified namespace and   {@link NamespaceId#SYSTEM}for the specified   {@link EntityTypeSimpleName}.
 * @param namespaceId the namespace to search in
 * @param searchQuery the search query, which could be of two forms: [key]:[value] or just [value] and can have '*'at the end for a prefix search
 * @param types the {@link EntityTypeSimpleName} to restrict the search to, if empty all types are searched
 * @param sortInfo the {@link SortInfo} to sort the results by
 * @param offset index to start with in the search results. To return results from the beginning, pass {@code 0}. Only applies when #sortInfo is not   {@link SortInfo#DEFAULT}
 * @param limit number of results to return, starting from #offset. To return all, pass {@link Integer#MAX_VALUE}. Only applies when #sortInfo is not   {@link SortInfo#DEFAULT}
 * @param numCursors number of cursors to return in the response. A cursor identifies the first index of thenext page for pagination purposes. Only applies when #sortInfo is not  {@link SortInfo#DEFAULT}. Defaults to   {@code 0}
 * @param cursor the cursor that acts as the starting index for the requested page. This is only applicable when#sortInfo is not  {@link SortInfo#DEFAULT}. If offset is also specified, it is applied starting at the cursor. If   {@code null}, the first row is used as the cursor
 * @param showHidden boolean which specifies whether to display hidden entities (entity whose name start with ""_"")or not.
 * @return a {@link SearchResults} object containing a list of {@link MetadataEntry} containing each matching{@link NamespacedEntityId} with its associated metadata. It also optionally contains a list of cursorsfor subsequent queries to start with, if the specified #sortInfo is not  {@link SortInfo#DEFAULT}.
 */
public SearchResults search(String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,@Nullable String cursor,boolean showHidden){
  if (!SortInfo.DEFAULT.equals(sortInfo)) {
    if (!""String_Node_Str"".equals(searchQuery)) {
      throw new IllegalArgumentException(""String_Node_Str"");
    }
    return searchByCustomIndex(namespaceId,types,sortInfo,offset,limit,numCursors,cursor,showHidden);
  }
  return searchByDefaultIndex(namespaceId,searchQuery,types,showHidden);
}"
5420,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override @AuthEnforce(entities=""String_Node_Str"",enforceOn=InstanceId.class,actions=Action.ADMIN) public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=metadata.getNamespaceId();
  if (exists(namespace)) {
    throw new NamespaceAlreadyExistsException(namespace);
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  privilegesManager.grant(namespace,principal,EnumSet.allOf(Action.class));
  String executionUserName;
  if (SecurityUtil.isKerberosEnabled(cConf) && !NamespaceId.SYSTEM.equals(namespace)) {
    String namespacePrincipal=metadata.getConfig().getPrincipal();
    if (Strings.isNullOrEmpty(namespacePrincipal)) {
      executionUserName=ImpersonationInfo.getMasterImpersonationInfo(cConf).getPrincipal();
    }
 else {
      executionUserName=new KerberosName(namespacePrincipal).getShortName();
    }
  }
 else {
    executionUserName=UserGroupInformation.getCurrentUser().getShortUserName();
  }
  Principal executionUser=new Principal(executionUserName,Principal.PrincipalType.USER);
  privilegesManager.grant(namespace,executionUser,EnumSet.allOf(Action.class));
  nsStore.create(metadata);
  UserGroupInformation ugi;
  if (NamespaceId.DEFAULT.equals(namespace)) {
    ugi=UserGroupInformation.getCurrentUser();
  }
 else {
    ugi=impersonator.getUGI(namespace);
  }
  try {
    ImpersonationUtils.doAs(ugi,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.get().create(metadata);
        return null;
      }
    }
);
  }
 catch (  Throwable t) {
    deleteNamespaceMeta(metadata.getNamespaceId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace,t);
  }
  LOG.info(""String_Node_Str"",metadata.getNamespaceId());
}","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override @AuthEnforce(entities=""String_Node_Str"",enforceOn=InstanceId.class,actions=Action.ADMIN) public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=metadata.getNamespaceId();
  if (exists(namespace)) {
    throw new NamespaceAlreadyExistsException(namespace);
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  privilegesManager.grant(namespace,principal,EnumSet.allOf(Action.class));
  String executionUserName;
  if (SecurityUtil.isKerberosEnabled(cConf) && !NamespaceId.SYSTEM.equals(namespace)) {
    String namespacePrincipal=metadata.getConfig().getPrincipal();
    if (Strings.isNullOrEmpty(namespacePrincipal)) {
      executionUserName=SecurityUtil.getMasterPrincipal(cConf);
    }
 else {
      executionUserName=new KerberosName(namespacePrincipal).getShortName();
    }
  }
 else {
    executionUserName=UserGroupInformation.getCurrentUser().getShortUserName();
  }
  Principal executionUser=new Principal(executionUserName,Principal.PrincipalType.USER);
  privilegesManager.grant(namespace,executionUser,EnumSet.allOf(Action.class));
  nsStore.create(metadata);
  UserGroupInformation ugi;
  if (NamespaceId.DEFAULT.equals(namespace)) {
    ugi=UserGroupInformation.getCurrentUser();
  }
 else {
    ugi=impersonator.getUGI(namespace);
  }
  try {
    ImpersonationUtils.doAs(ugi,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.get().create(metadata);
        return null;
      }
    }
);
  }
 catch (  Throwable t) {
    deleteNamespaceMeta(metadata.getNamespaceId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace,t);
  }
  LOG.info(""String_Node_Str"",metadata.getNamespaceId());
}"
5421,"@Override public void run(){
  Futures.getUnchecked(Services.chainStart(zkClientService,kafkaClientService,metricsCollectionService,streamCoordinatorClient,resourceReporter,authEnforcementService));
  LOG.info(""String_Node_Str"",name);
  controller=programRunner.run(program,programOpts);
  final SettableFuture<ProgramController.State> state=SettableFuture.create();
  controller.addListener(new AbstractListener(){
    @Override public void alive(){
      runlatch.countDown();
    }
    @Override public void init(    ProgramController.State currentState,    @Nullable Throwable cause){
      if (currentState == ProgramController.State.ALIVE) {
        alive();
      }
 else {
        super.init(currentState,cause);
      }
    }
    @Override public void completed(){
      state.set(ProgramController.State.COMPLETED);
    }
    @Override public void killed(){
      state.set(ProgramController.State.KILLED);
    }
    @Override public void error(    Throwable cause){
      LOG.error(""String_Node_Str"",cause);
      state.setException(cause);
    }
  }
,MoreExecutors.sameThreadExecutor());
  try {
    state.get();
    LOG.info(""String_Node_Str"",name);
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",name,e);
    Thread.currentThread().interrupt();
  }
catch (  ExecutionException e) {
    LOG.error(""String_Node_Str"",name,e);
    if (propagateServiceError()) {
      throw Throwables.propagate(Throwables.getRootCause(e));
    }
  }
 finally {
    if (programRunner instanceof Closeable) {
      Closeables.closeQuietly((Closeable)programRunner);
    }
    runlatch.countDown();
  }
}","@Override public void run(){
  Futures.getUnchecked(Services.chainStart(zkClientService,kafkaClientService,metricsCollectionService,streamCoordinatorClient,resourceReporter,authEnforcementService));
  LOG.info(""String_Node_Str"",name);
  controller=programRunner.run(program,programOpts);
  final SettableFuture<ProgramController.State> state=SettableFuture.create();
  controller.addListener(new AbstractListener(){
    @Override public void alive(){
      runlatch.countDown();
    }
    @Override public void init(    ProgramController.State currentState,    @Nullable Throwable cause){
      if (currentState == ProgramController.State.ALIVE) {
        alive();
      }
 else {
        super.init(currentState,cause);
      }
    }
    @Override public void completed(){
      state.set(ProgramController.State.COMPLETED);
    }
    @Override public void killed(){
      state.set(ProgramController.State.KILLED);
    }
    @Override public void error(    Throwable cause){
      LOG.error(""String_Node_Str"",cause);
      state.setException(cause);
    }
  }
,MoreExecutors.sameThreadExecutor());
  try {
    state.get();
    LOG.info(""String_Node_Str"",name);
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",name,e);
  }
catch (  ExecutionException e) {
    LOG.error(""String_Node_Str"",name,e);
    if (propagateServiceError()) {
      throw Throwables.propagate(Throwables.getRootCause(e));
    }
  }
 finally {
    if (programRunner instanceof Closeable) {
      Closeables.closeQuietly((Closeable)programRunner);
    }
    runlatch.countDown();
  }
}"
5422,"public Map<String,String> getSystemProperties(Id.Program id) throws Exception {
  Map<String,String> systemArgs=Maps.newHashMap();
  systemArgs.put(Constants.CLUSTER_NAME,cConf.get(Constants.CLUSTER_NAME,""String_Node_Str""));
  systemArgs.put(Constants.AppFabric.APP_SCHEDULER_QUEUE,queueResolver.getQueue(id.getNamespace()));
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    ImpersonationInfo impersonationInfo;
    KerberosPrincipalId principalId=ownerAdmin.getEffectiveOwner(id.toEntityId());
    if (principalId == null) {
      impersonationInfo=ImpersonationInfo.getMasterImpersonationInfo(cConf);
    }
 else {
      impersonationInfo=new ImpersonationInfo(principalId.getPrincipal(),cConf);
    }
    systemArgs.put(ProgramOptionConstants.PRINCIPAL,impersonationInfo.getPrincipal());
    systemArgs.put(ProgramOptionConstants.KEYTAB_URI,impersonationInfo.getKeytabURI());
  }
  return systemArgs;
}","public Map<String,String> getSystemProperties(Id.Program id) throws Exception {
  Map<String,String> systemArgs=Maps.newHashMap();
  systemArgs.put(Constants.CLUSTER_NAME,cConf.get(Constants.CLUSTER_NAME,""String_Node_Str""));
  systemArgs.put(Constants.AppFabric.APP_SCHEDULER_QUEUE,queueResolver.getQueue(id.getNamespace()));
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    ImpersonationInfo impersonationInfo=SecurityUtil.createImpersonationInfo(ownerAdmin,cConf,id.toEntityId());
    systemArgs.put(ProgramOptionConstants.PRINCIPAL,impersonationInfo.getPrincipal());
    systemArgs.put(ProgramOptionConstants.KEYTAB_URI,impersonationInfo.getKeytabURI());
  }
  return systemArgs;
}"
5423,"private Properties generateKafkaConfig(String kafkaZKConnect){
  int port=Networks.getRandomPort();
  Preconditions.checkState(port > 0,""String_Node_Str"");
  Properties prop=new Properties();
  prop.setProperty(""String_Node_Str"",new File(""String_Node_Str"").getAbsolutePath());
  prop.setProperty(""String_Node_Str"",Integer.toString(port));
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",kafkaZKConnect);
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  return prop;
}","private Properties generateKafkaConfig(String kafkaZKConnect){
  Properties prop=new Properties();
  prop.setProperty(""String_Node_Str"",new File(""String_Node_Str"").getAbsolutePath());
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",kafkaZKConnect);
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  return prop;
}"
5424,"@Test public void testWorkflows() throws Exception {
  String workflow=String.format(""String_Node_Str"",FakeApp.NAME,FakeWorkflow.NAME);
  File doneFile=TMP_FOLDER.newFile(""String_Node_Str"");
  Map<String,String> runtimeArgs=ImmutableMap.of(""String_Node_Str"",doneFile.getAbsolutePath());
  String runtimeArgsKV=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(runtimeArgs);
  testCommandOutputContains(cli,""String_Node_Str"" + workflow + ""String_Node_Str""+ runtimeArgsKV+ ""String_Node_Str"",""String_Node_Str"");
  assertProgramStatus(programClient,fakeWorkflowId,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  String commandOutput=getCommandOutput(cli,""String_Node_Str"" + workflow);
  String[] lines=commandOutput.split(""String_Node_Str"");
  Assert.assertEquals(2,lines.length);
  String[] split=lines[1].split(""String_Node_Str"");
  String runId=split[0];
  List<WorkflowTokenDetail.NodeValueDetail> tokenValues=new ArrayList<>();
  tokenValues.add(new WorkflowTokenDetail.NodeValueDetail(FakeWorkflow.FakeAction.class.getSimpleName(),FakeWorkflow.FakeAction.TOKEN_VALUE));
  tokenValues.add(new WorkflowTokenDetail.NodeValueDetail(FakeWorkflow.FakeAction.ANOTHER_FAKE_NAME,FakeWorkflow.FakeAction.TOKEN_VALUE));
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId),Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,GSON.toJson(tokenValues)));
  testCommandOutputNotContains(cli,String.format(""String_Node_Str"",workflow,runId),Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,GSON.toJson(tokenValues)));
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.TOKEN_KEY),Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,GSON.toJson(tokenValues)));
  String fakeNodeValue=Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,FakeWorkflow.FakeAction.TOKEN_VALUE);
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.class.getSimpleName()),fakeNodeValue);
  testCommandOutputNotContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.ANOTHER_FAKE_NAME),fakeNodeValue);
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.ANOTHER_FAKE_NAME,FakeWorkflow.FakeAction.TOKEN_KEY),fakeNodeValue);
  testCommandOutputContains(cli,""String_Node_Str"" + workflow,FakeWorkflow.FAKE_LOG);
  testCommandOutputContains(cli,""String_Node_Str"" + workflow,String.format(""String_Node_Str"",fakeWorkflowId));
}","@Test public void testWorkflows() throws Exception {
  String workflow=String.format(""String_Node_Str"",FakeApp.NAME,FakeWorkflow.NAME);
  File doneFile=TMP_FOLDER.newFile(""String_Node_Str"");
  Map<String,String> runtimeArgs=ImmutableMap.of(""String_Node_Str"",doneFile.getAbsolutePath());
  String runtimeArgsKV=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(runtimeArgs);
  testCommandOutputContains(cli,""String_Node_Str"" + workflow + ""String_Node_Str""+ runtimeArgsKV+ ""String_Node_Str"",""String_Node_Str"");
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return programClient.getProgramRuns(fakeWorkflowId,ProgramRunStatus.COMPLETED.name(),0,Long.MAX_VALUE,Integer.MAX_VALUE).size();
    }
  }
,180,TimeUnit.SECONDS);
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  String commandOutput=getCommandOutput(cli,""String_Node_Str"" + workflow);
  String[] lines=commandOutput.split(""String_Node_Str"");
  Assert.assertEquals(2,lines.length);
  String[] split=lines[1].split(""String_Node_Str"");
  String runId=split[0];
  List<WorkflowTokenDetail.NodeValueDetail> tokenValues=new ArrayList<>();
  tokenValues.add(new WorkflowTokenDetail.NodeValueDetail(FakeWorkflow.FakeAction.class.getSimpleName(),FakeWorkflow.FakeAction.TOKEN_VALUE));
  tokenValues.add(new WorkflowTokenDetail.NodeValueDetail(FakeWorkflow.FakeAction.ANOTHER_FAKE_NAME,FakeWorkflow.FakeAction.TOKEN_VALUE));
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId),Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,GSON.toJson(tokenValues)));
  testCommandOutputNotContains(cli,String.format(""String_Node_Str"",workflow,runId),Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,GSON.toJson(tokenValues)));
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.TOKEN_KEY),Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,GSON.toJson(tokenValues)));
  String fakeNodeValue=Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,FakeWorkflow.FakeAction.TOKEN_VALUE);
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.class.getSimpleName()),fakeNodeValue);
  testCommandOutputNotContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.ANOTHER_FAKE_NAME),fakeNodeValue);
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.ANOTHER_FAKE_NAME,FakeWorkflow.FakeAction.TOKEN_KEY),fakeNodeValue);
  testCommandOutputContains(cli,""String_Node_Str"" + workflow,FakeWorkflow.FAKE_LOG);
  testCommandOutputContains(cli,""String_Node_Str"" + workflow,String.format(""String_Node_Str"",fakeWorkflowId));
}"
5425,"private Properties generateKafkaConfig(String kafkaZKConnect){
  int port=Networks.getRandomPort();
  Preconditions.checkState(port > 0,""String_Node_Str"");
  Properties prop=new Properties();
  prop.setProperty(""String_Node_Str"",new File(""String_Node_Str"").getAbsolutePath());
  prop.setProperty(""String_Node_Str"",Integer.toString(port));
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",kafkaZKConnect);
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  return prop;
}","private Properties generateKafkaConfig(String kafkaZKConnect){
  Properties prop=new Properties();
  prop.setProperty(""String_Node_Str"",new File(""String_Node_Str"").getAbsolutePath());
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",kafkaZKConnect);
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  return prop;
}"
5426,"private void rewriteHiveConfig(){
  URL resourceURL=getLocalResourceURL(""String_Node_Str"");
  if (resourceURL == null) {
    LOG.warn(""String_Node_Str"");
    return;
  }
  try (InputStream is=new FileInputStream(new File(resourceURL.toURI()))){
    Configuration conf=new Configuration(false);
    conf.addResource(is);
    conf.setBoolean(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,false);
    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER,false);
    String sparkHome=System.getenv(Constants.SPARK_HOME);
    if (sparkHome != null) {
      LOG.debug(""String_Node_Str"",sparkHome);
      conf.set(""String_Node_Str"",sparkHome);
    }
    File newConfFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    try (FileOutputStream os=new FileOutputStream(newConfFile)){
      conf.writeXml(os);
    }
     Files.move(newConfFile.toPath(),Paths.get(resourceURL.toURI()),StandardCopyOption.REPLACE_EXISTING);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",resourceURL,e);
    throw Throwables.propagate(e);
  }
}","private void rewriteHiveConfig(){
  URL resourceURL=getLocalResourceURL(""String_Node_Str"");
  if (resourceURL == null) {
    LOG.warn(""String_Node_Str"");
    return;
  }
  try (InputStream is=new FileInputStream(new File(resourceURL.toURI()))){
    Configuration conf=new Configuration(false);
    conf.addResource(is);
    conf.setBoolean(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,false);
    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER,false);
    conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,UserGroupInformation.AuthenticationMethod.SIMPLE.name());
    String sparkHome=System.getenv(Constants.SPARK_HOME);
    if (sparkHome != null) {
      LOG.debug(""String_Node_Str"",sparkHome);
      conf.set(""String_Node_Str"",sparkHome);
    }
    File newConfFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    try (FileOutputStream os=new FileOutputStream(newConfFile)){
      conf.writeXml(os);
    }
     Files.move(newConfFile.toPath(),Paths.get(resourceURL.toURI()),StandardCopyOption.REPLACE_EXISTING);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",resourceURL,e);
    throw Throwables.propagate(e);
  }
}"
5427,"/** 
 * Searches entities that match the specified search query in the specified namespace and   {@link NamespaceId#SYSTEM}for the specified   {@link EntityTypeSimpleName}.
 * @param namespaceId the namespace to search in
 * @param searchQuery the search query, which could be of two forms: [key]:[value] or just [value] and can have '*'at the end for a prefix search
 * @param types the {@link EntityTypeSimpleName} to restrict the search to, if empty all types are searched
 * @param sortInfo the {@link SortInfo} to sort the results by
 * @param offset index to start with in the search results. To return results from the beginning, pass {@code 0}. Only applies when #sortInfo is not   {@link SortInfo#DEFAULT}
 * @param limit number of results to return, starting from #offset. To return all, pass {@link Integer#MAX_VALUE}. Only applies when #sortInfo is not   {@link SortInfo#DEFAULT}
 * @param numCursors number of cursors to return in the response. A cursor identifies the first index of thenext page for pagination purposes. Only applies when #sortInfo is not  {@link SortInfo#DEFAULT}. Defaults to   {@code 0}
 * @param cursor the cursor that acts as the starting index for the requested page. This is only applicable when#sortInfo is not  {@link SortInfo#DEFAULT}. If offset is also specified, it is applied starting at the cursor. If   {@code null}, the first row is used as the cursor
 * @param showHidden boolean which specifies whether to display hidden entities (entity whose name start with ""_"")or not.
 * @return a {@link SearchResults} object containing a list of {@link MetadataEntry} containing each matching{@link NamespacedEntityId} with its associated metadata. It also optionally contains a list of cursorsfor subsequent queries to start with, if the specified #sortInfo is not  {@link SortInfo#DEFAULT}.
 */
public SearchResults search(String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,@Nullable String cursor,boolean showHidden){
  if (SortInfo.DEFAULT.equals(sortInfo)) {
    return searchByDefaultIndex(namespaceId,searchQuery,types,showHidden);
  }
  return searchByCustomIndex(namespaceId,types,sortInfo,offset,limit,numCursors,cursor,showHidden);
}","/** 
 * Searches entities that match the specified search query in the specified namespace and   {@link NamespaceId#SYSTEM}for the specified   {@link EntityTypeSimpleName}.
 * @param namespaceId the namespace to search in
 * @param searchQuery the search query, which could be of two forms: [key]:[value] or just [value] and can have '*'at the end for a prefix search
 * @param types the {@link EntityTypeSimpleName} to restrict the search to, if empty all types are searched
 * @param sortInfo the {@link SortInfo} to sort the results by
 * @param offset index to start with in the search results. To return results from the beginning, pass {@code 0}. Only applies when #sortInfo is not   {@link SortInfo#DEFAULT}
 * @param limit number of results to return, starting from #offset. To return all, pass {@link Integer#MAX_VALUE}. Only applies when #sortInfo is not   {@link SortInfo#DEFAULT}
 * @param numCursors number of cursors to return in the response. A cursor identifies the first index of thenext page for pagination purposes. Only applies when #sortInfo is not  {@link SortInfo#DEFAULT}. Defaults to   {@code 0}
 * @param cursor the cursor that acts as the starting index for the requested page. This is only applicable when#sortInfo is not  {@link SortInfo#DEFAULT}. If offset is also specified, it is applied starting at the cursor. If   {@code null}, the first row is used as the cursor
 * @param showHidden boolean which specifies whether to display hidden entities (entity whose name start with ""_"")or not.
 * @return a {@link SearchResults} object containing a list of {@link MetadataEntry} containing each matching{@link NamespacedEntityId} with its associated metadata. It also optionally contains a list of cursorsfor subsequent queries to start with, if the specified #sortInfo is not  {@link SortInfo#DEFAULT}.
 */
public SearchResults search(String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,@Nullable String cursor,boolean showHidden){
  return SortInfo.DEFAULT.equals(sortInfo) ? searchByDefaultIndex(namespaceId,searchQuery,types,showHidden) : searchByCustomIndex(namespaceId,types,sortInfo,offset,limit,numCursors,cursor,showHidden);
}"
5428,"private SearchResults searchByCustomIndex(String namespaceId,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,@Nullable String cursor,boolean showHidden){
  List<MetadataEntry> results=new ArrayList<>();
  String indexColumn=getIndexColumn(sortInfo.getSortBy(),sortInfo.getSortOrder());
  int fetchSize=offset + ((numCursors + 1) * limit);
  List<String> cursors=new ArrayList<>(numCursors);
  int count=0;
  for (  String searchTerm : getSearchTerms(namespaceId,""String_Node_Str"")) {
    byte[] startKey=Bytes.toBytes(searchTerm.substring(0,searchTerm.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    if (!Strings.isNullOrEmpty(cursor)) {
      String namespaceInStartKey=searchTerm.substring(0,searchTerm.indexOf(KEYVALUE_SEPARATOR));
      startKey=Bytes.toBytes(namespaceInStartKey + KEYVALUE_SEPARATOR + cursor);
    }
    int mod=limit == 1 ? 0 : 1;
    try (Scanner scanner=indexedTable.scanByIndex(Bytes.toBytes(indexColumn),startKey,stopKey)){
      Row next;
      while ((next=scanner.next()) != null && count < fetchSize) {
        if (count < offset) {
          if (parseRow(next,indexColumn,types,showHidden).isPresent()) {
            count++;
          }
          continue;
        }
        Optional<MetadataEntry> metadataEntry=parseRow(next,indexColumn,types,showHidden);
        if (metadataEntry.isPresent()) {
          count++;
          results.add(metadataEntry.get());
        }
        if (results.size() % limit == mod && results.size() > limit) {
          String cursorWithNamespace=Bytes.toString(next.get(indexColumn));
          cursors.add(cursorWithNamespace.substring(cursorWithNamespace.indexOf(KEYVALUE_SEPARATOR) + 1));
        }
      }
    }
   }
  return new SearchResults(results,cursors);
}","private SearchResults searchByCustomIndex(String namespaceId,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,@Nullable String cursor,boolean showHidden){
  List<MetadataEntry> returnedResults=new LinkedList<>();
  List<MetadataEntry> allResults=new LinkedList<>();
  String indexColumn=getIndexColumn(sortInfo.getSortBy(),sortInfo.getSortOrder());
  int fetchSize=offset + ((numCursors + 1) * limit);
  List<String> cursors=new ArrayList<>(numCursors);
  for (  String searchTerm : getSearchTerms(namespaceId,""String_Node_Str"")) {
    byte[] startKey=Bytes.toBytes(searchTerm.substring(0,searchTerm.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    if (!Strings.isNullOrEmpty(cursor)) {
      String namespaceInStartKey=searchTerm.substring(0,searchTerm.indexOf(KEYVALUE_SEPARATOR));
      startKey=Bytes.toBytes(namespaceInStartKey + KEYVALUE_SEPARATOR + cursor);
    }
    int mod=(limit == 1) ? 0 : 1;
    try (Scanner scanner=indexedTable.scanByIndex(Bytes.toBytes(indexColumn),startKey,stopKey)){
      Row next;
      while ((next=scanner.next()) != null) {
        Optional<MetadataEntry> metadataEntry=parseRow(next,indexColumn,types,showHidden);
        if (!metadataEntry.isPresent()) {
          continue;
        }
        allResults.add(metadataEntry.get());
        if (allResults.size() <= offset || allResults.size() > fetchSize) {
          continue;
        }
        if (returnedResults.size() < limit) {
          returnedResults.add(metadataEntry.get());
        }
 else {
          if ((allResults.size() - offset) % limit == mod) {
            String cursorWithNamespace=Bytes.toString(next.get(indexColumn));
            cursors.add(cursorWithNamespace.substring(cursorWithNamespace.indexOf(KEYVALUE_SEPARATOR) + 1));
          }
        }
      }
    }
   }
  return new SearchResults(returnedResults,cursors,allResults);
}"
5429,"/** 
 * Prepares search terms from the specified search query by <ol> <li>Splitting on   {@link #SPACE_SEPARATOR_PATTERN} and trimming</li><li>Handling  {@link #KEYVALUE_SEPARATOR}, so searches of the pattern key:value* can be supported</li> <li>Prepending the result with the specified namespaceId and   {@link NamespaceId#SYSTEM} so the search canbe restricted to entities in the specified namespace and  {@link NamespaceId#SYSTEM}.</li> </ol>t
 * @param namespaceId the namespaceId to search in
 * @param searchQuery the user specified search query. If {@code *}, returns a singleton list containing  {@code *} which matches everything.
 * @return formatted search query which is namespaced
 */
private Iterable<String> getSearchTerms(String namespaceId,String searchQuery){
  List<String> searchTerms=new ArrayList<>();
  for (  String term : Splitter.on(SPACE_SEPARATOR_PATTERN).omitEmptyStrings().trimResults().split(searchQuery)) {
    String formattedSearchTerm=term.toLowerCase();
    if (formattedSearchTerm.contains(KEYVALUE_SEPARATOR)) {
      String[] split=formattedSearchTerm.split(KEYVALUE_SEPARATOR,2);
      formattedSearchTerm=split[0].trim() + KEYVALUE_SEPARATOR + split[1].trim();
    }
    searchTerms.add(namespaceId + KEYVALUE_SEPARATOR + formattedSearchTerm);
    if (!NamespaceId.SYSTEM.getEntityName().equals(namespaceId)) {
      searchTerms.add(NamespaceId.SYSTEM.getEntityName() + KEYVALUE_SEPARATOR + formattedSearchTerm);
    }
  }
  return searchTerms;
}","/** 
 * Prepares search terms from the specified search query by <ol> <li>Splitting on   {@link #SPACE_SEPARATOR_PATTERN} and trimming</li><li>Handling  {@link #KEYVALUE_SEPARATOR}, so searches of the pattern key:value* can be supported</li> <li>Prepending the result with the specified namespaceId and   {@link NamespaceId#SYSTEM} so the search canbe restricted to entities in the specified namespace and  {@link NamespaceId#SYSTEM}.</li> </ol>t
 * @param namespaceId the namespaceId to search in
 * @param searchQuery the user specified search query. If {@code *}, returns a singleton list containing  {@code *} which matches everything.
 * @return formatted search query which is namespaced
 */
private Iterable<String> getSearchTerms(String namespaceId,String searchQuery){
  List<String> searchTerms=new LinkedList<>();
  for (  String term : Splitter.on(SPACE_SEPARATOR_PATTERN).omitEmptyStrings().trimResults().split(searchQuery)) {
    String formattedSearchTerm=term.toLowerCase();
    if (formattedSearchTerm.contains(KEYVALUE_SEPARATOR)) {
      String[] split=formattedSearchTerm.split(KEYVALUE_SEPARATOR,2);
      formattedSearchTerm=split[0].trim() + KEYVALUE_SEPARATOR + split[1].trim();
    }
    searchTerms.add(namespaceId + KEYVALUE_SEPARATOR + formattedSearchTerm);
    if (!NamespaceId.SYSTEM.getEntityName().equals(namespaceId)) {
      searchTerms.add(NamespaceId.SYSTEM.getEntityName() + KEYVALUE_SEPARATOR + formattedSearchTerm);
    }
  }
  return searchTerms;
}"
5430,"private SearchResults searchByDefaultIndex(String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types,boolean showHidden){
  List<MetadataEntry> results=new ArrayList<>();
  for (  String searchTerm : getSearchTerms(namespaceId,searchQuery)) {
    Scanner scanner;
    if (searchTerm.endsWith(""String_Node_Str"")) {
      byte[] startKey=Bytes.toBytes(searchTerm.substring(0,searchTerm.lastIndexOf(""String_Node_Str"")));
      byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
      scanner=indexedTable.scanByIndex(Bytes.toBytes(DEFAULT_INDEX_COLUMN),startKey,stopKey);
    }
 else {
      byte[] value=Bytes.toBytes(searchTerm);
      scanner=indexedTable.readByIndex(Bytes.toBytes(DEFAULT_INDEX_COLUMN),value);
    }
    try {
      Row next;
      while ((next=scanner.next()) != null) {
        Optional<MetadataEntry> metadataEntry=parseRow(next,DEFAULT_INDEX_COLUMN,types,showHidden);
        if (metadataEntry.isPresent()) {
          results.add(metadataEntry.get());
        }
      }
    }
  finally {
      scanner.close();
    }
  }
  return new SearchResults(results,Collections.<String>emptyList());
}","private SearchResults searchByDefaultIndex(String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types,boolean showHidden){
  List<MetadataEntry> results=new LinkedList<>();
  for (  String searchTerm : getSearchTerms(namespaceId,searchQuery)) {
    Scanner scanner;
    if (searchTerm.endsWith(""String_Node_Str"")) {
      byte[] startKey=Bytes.toBytes(searchTerm.substring(0,searchTerm.lastIndexOf(""String_Node_Str"")));
      byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
      scanner=indexedTable.scanByIndex(Bytes.toBytes(DEFAULT_INDEX_COLUMN),startKey,stopKey);
    }
 else {
      byte[] value=Bytes.toBytes(searchTerm);
      scanner=indexedTable.readByIndex(Bytes.toBytes(DEFAULT_INDEX_COLUMN),value);
    }
    try {
      Row next;
      while ((next=scanner.next()) != null) {
        Optional<MetadataEntry> metadataEntry=parseRow(next,DEFAULT_INDEX_COLUMN,types,showHidden);
        if (metadataEntry.isPresent()) {
          results.add(metadataEntry.get());
        }
      }
    }
  finally {
      scanner.close();
    }
  }
  return new SearchResults(results,Collections.<String>emptyList(),results);
}"
5431,"/** 
 * Returns metadata for a given set of entities
 * @param targetIds entities for which metadata is required
 * @return map of entitiyId to set of metadata for that entity
 */
public Set<Metadata> getMetadata(Set<? extends NamespacedEntityId> targetIds){
  if (targetIds.isEmpty()) {
    return Collections.emptySet();
  }
  List<ImmutablePair<byte[],byte[]>> fuzzyKeys=new ArrayList<>();
  for (  NamespacedEntityId targetId : targetIds) {
    fuzzyKeys.add(getFuzzyKeyFor(targetId));
  }
  Collections.sort(fuzzyKeys,FUZZY_KEY_COMPARATOR);
  Multimap<NamespacedEntityId,MetadataEntry> metadataMap=HashMultimap.create();
  byte[] start=fuzzyKeys.get(0).getFirst();
  byte[] end=Bytes.stopKeyForPrefix(fuzzyKeys.get(fuzzyKeys.size() - 1).getFirst());
  try (Scanner scan=indexedTable.scan(new Scan(start,end,new FuzzyRowFilter(fuzzyKeys)))){
    Row next;
    while ((next=scan.next()) != null) {
      MetadataEntry metadataEntry=convertRow(next);
      if (metadataEntry != null) {
        metadataMap.put(metadataEntry.getTargetId(),metadataEntry);
      }
    }
  }
   Set<Metadata> metadataSet=new HashSet<>();
  for (  Map.Entry<NamespacedEntityId,Collection<MetadataEntry>> entry : metadataMap.asMap().entrySet()) {
    Map<String,String> properties=new HashMap<>();
    Set<String> tags=Collections.emptySet();
    for (    MetadataEntry metadataEntry : entry.getValue()) {
      if (TAGS_KEY.equals(metadataEntry.getKey())) {
        tags=splitTags(metadataEntry.getValue());
      }
 else {
        properties.put(metadataEntry.getKey(),metadataEntry.getValue());
      }
    }
    metadataSet.add(new Metadata(entry.getKey(),properties,tags));
  }
  return metadataSet;
}","/** 
 * Returns metadata for a given set of entities
 * @param targetIds entities for which metadata is required
 * @return map of entitiyId to set of metadata for that entity
 */
public Set<Metadata> getMetadata(Set<? extends NamespacedEntityId> targetIds){
  if (targetIds.isEmpty()) {
    return Collections.emptySet();
  }
  List<ImmutablePair<byte[],byte[]>> fuzzyKeys=new ArrayList<>(targetIds.size());
  for (  NamespacedEntityId targetId : targetIds) {
    fuzzyKeys.add(getFuzzyKeyFor(targetId));
  }
  Collections.sort(fuzzyKeys,FUZZY_KEY_COMPARATOR);
  Multimap<NamespacedEntityId,MetadataEntry> metadataMap=HashMultimap.create();
  byte[] start=fuzzyKeys.get(0).getFirst();
  byte[] end=Bytes.stopKeyForPrefix(fuzzyKeys.get(fuzzyKeys.size() - 1).getFirst());
  try (Scanner scan=indexedTable.scan(new Scan(start,end,new FuzzyRowFilter(fuzzyKeys)))){
    Row next;
    while ((next=scan.next()) != null) {
      MetadataEntry metadataEntry=convertRow(next);
      if (metadataEntry != null) {
        metadataMap.put(metadataEntry.getTargetId(),metadataEntry);
      }
    }
  }
   Set<Metadata> metadataSet=new HashSet<>();
  for (  Map.Entry<NamespacedEntityId,Collection<MetadataEntry>> entry : metadataMap.asMap().entrySet()) {
    Map<String,String> properties=new HashMap<>();
    Set<String> tags=Collections.emptySet();
    for (    MetadataEntry metadataEntry : entry.getValue()) {
      if (TAGS_KEY.equals(metadataEntry.getKey())) {
        tags=splitTags(metadataEntry.getValue());
      }
 else {
        properties.put(metadataEntry.getKey(),metadataEntry.getValue());
      }
    }
    metadataSet.add(new Metadata(entry.getKey(),properties,tags));
  }
  return metadataSet;
}"
5432,"SearchResults(List<MetadataEntry> results,List<String> cursors){
  this.results=results;
  this.cursors=cursors;
}","SearchResults(List<MetadataEntry> results,List<String> cursors,List<MetadataEntry> allResults){
  this.results=results;
  this.cursors=cursors;
  this.allResults=allResults;
}"
5433,"private MetadataSearchResponse search(Set<MetadataScope> scopes,String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,String cursor,boolean showHidden) throws BadRequestException {
  List<MetadataEntry> results=new ArrayList<>();
  List<String> cursors=new ArrayList<>();
  for (  MetadataScope scope : scopes) {
    SearchResults searchResults=getSearchResults(scope,namespaceId,searchQuery,types,sortInfo,offset,limit,numCursors,cursor,showHidden);
    results.addAll(searchResults.getResults());
    cursors.addAll(searchResults.getCursors());
  }
  Set<NamespacedEntityId> sortedEntities=getSortedEntities(results,sortInfo);
  int startIndex=0;
  int maxEndIndex;
  int total=sortedEntities.size();
  if (SortInfo.DEFAULT.equals(sortInfo)) {
    if (offset > sortedEntities.size()) {
      maxEndIndex=0;
    }
 else {
      startIndex=offset;
      maxEndIndex=(int)Math.min(Integer.MAX_VALUE,(long)offset + limit);
    }
  }
 else {
    maxEndIndex=limit;
    total+=offset;
  }
  sortedEntities=new LinkedHashSet<>(ImmutableList.copyOf(sortedEntities).subList(startIndex,Math.min(maxEndIndex,sortedEntities.size())));
  Map<NamespacedEntityId,Metadata> systemMetadata=fetchMetadata(sortedEntities,MetadataScope.SYSTEM);
  Map<NamespacedEntityId,Metadata> userMetadata=fetchMetadata(sortedEntities,MetadataScope.USER);
  return new MetadataSearchResponse(sortInfo.getSortBy() + ""String_Node_Str"" + sortInfo.getSortOrder(),offset,limit,numCursors,total,addMetadataToEntities(sortedEntities,systemMetadata,userMetadata),cursors,showHidden);
}","private MetadataSearchResponse search(Set<MetadataScope> scopes,String namespaceId,String searchQuery,Set<EntityTypeSimpleName> types,SortInfo sortInfo,int offset,int limit,int numCursors,String cursor,boolean showHidden) throws BadRequestException {
  if (offset < 0) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  if (limit < 0) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  List<MetadataEntry> results=new LinkedList<>();
  List<String> cursors=new LinkedList<>();
  List<MetadataEntry> allResults=new LinkedList<>();
  for (  MetadataScope scope : scopes) {
    SearchResults searchResults=getSearchResults(scope,namespaceId,searchQuery,types,sortInfo,offset,limit,numCursors,cursor,showHidden);
    results.addAll(searchResults.getResults());
    cursors.addAll(searchResults.getCursors());
    allResults.addAll(searchResults.getAllResults());
  }
  Set<NamespacedEntityId> sortedEntities=getSortedEntities(results,sortInfo);
  int total=getSortedEntities(allResults,sortInfo).size();
  if (SortInfo.DEFAULT.equals(sortInfo)) {
    int startIndex=Math.min(offset,sortedEntities.size());
    int endIndex=(int)Math.min(Integer.MAX_VALUE,(long)offset + limit);
    endIndex=Math.min(endIndex,sortedEntities.size());
    sortedEntities=new LinkedHashSet<>(ImmutableList.copyOf(sortedEntities).subList(startIndex,endIndex));
  }
  Map<NamespacedEntityId,Metadata> systemMetadata=fetchMetadata(sortedEntities,MetadataScope.SYSTEM);
  Map<NamespacedEntityId,Metadata> userMetadata=fetchMetadata(sortedEntities,MetadataScope.USER);
  return new MetadataSearchResponse(sortInfo.getSortBy() + ""String_Node_Str"" + sortInfo.getSortOrder(),offset,limit,numCursors,total,addMetadataToEntities(sortedEntities,systemMetadata,userMetadata),cursors,showHidden);
}"
5434,"private Set<NamespacedEntityId> getSortedEntities(List<MetadataEntry> results,SortInfo sortInfo){
  if (SortInfo.SortOrder.WEIGHTED != sortInfo.getSortOrder()) {
    Set<NamespacedEntityId> entities=new LinkedHashSet<>(results.size());
    for (    MetadataEntry metadataEntry : results) {
      if (metadataEntry != null) {
        entities.add(metadataEntry.getTargetId());
      }
    }
    return entities;
  }
  final Map<NamespacedEntityId,Integer> weightedResults=new HashMap<>();
  for (  MetadataEntry metadataEntry : results) {
    if (metadataEntry != null) {
      Integer score=weightedResults.get(metadataEntry.getTargetId());
      score=score == null ? 0 : score;
      weightedResults.put(metadataEntry.getTargetId(),score + 1);
    }
  }
  List<Map.Entry<NamespacedEntityId,Integer>> resultList=new ArrayList<>(weightedResults.entrySet());
  Collections.sort(resultList,SEARCH_RESULT_DESC_SCORE_COMPARATOR);
  Set<NamespacedEntityId> result=new LinkedHashSet<>(resultList.size());
  for (  Map.Entry<NamespacedEntityId,Integer> entry : resultList) {
    result.add(entry.getKey());
  }
  return result;
}","private Set<NamespacedEntityId> getSortedEntities(List<MetadataEntry> results,SortInfo sortInfo){
  if (SortInfo.SortOrder.WEIGHTED != sortInfo.getSortOrder()) {
    Set<NamespacedEntityId> entities=new LinkedHashSet<>(results.size());
    for (    MetadataEntry metadataEntry : results) {
      if (metadataEntry != null) {
        entities.add(metadataEntry.getTargetId());
      }
    }
    return entities;
  }
  final Map<NamespacedEntityId,Integer> weightedResults=new HashMap<>();
  for (  MetadataEntry metadataEntry : results) {
    if (metadataEntry != null) {
      Integer score=weightedResults.get(metadataEntry.getTargetId());
      score=(score == null) ? 0 : score;
      weightedResults.put(metadataEntry.getTargetId(),score + 1);
    }
  }
  List<Map.Entry<NamespacedEntityId,Integer>> resultList=new ArrayList<>(weightedResults.entrySet());
  Collections.sort(resultList,SEARCH_RESULT_DESC_SCORE_COMPARATOR);
  Set<NamespacedEntityId> result=new LinkedHashSet<>(resultList.size());
  for (  Map.Entry<NamespacedEntityId,Integer> entry : resultList) {
    result.add(entry.getKey());
  }
  return result;
}"
5435,"@Test public void testPagination() throws Exception {
  final MetadataDataset dataset=getDataset(DatasetFrameworkTestUtil.NAMESPACE_ID.dataset(""String_Node_Str""),MetadataScope.SYSTEM);
  TransactionExecutor txnl=dsFrameworkUtil.newInMemoryTransactionExecutor((TransactionAware)dataset);
  final String flowName=""String_Node_Str"";
  final String dsName=""String_Node_Str"";
  final String appName=""String_Node_Str"";
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      dataset.setProperty(flow1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,flowName);
      dataset.setProperty(dataset1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,dsName);
      dataset.setProperty(app1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,appName);
    }
  }
);
  final String namespaceId=flow1.getNamespace();
  final EnumSet<EntityTypeSimpleName> targets=EnumSet.allOf(EntityTypeSimpleName.class);
  final MetadataEntry flowEntry=new MetadataEntry(flow1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,flowName);
  final MetadataEntry dsEntry=new MetadataEntry(dataset1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,dsName);
  final MetadataEntry appEntry=new MetadataEntry(app1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,appName);
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      SearchResults searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,SortInfo.DEFAULT,0,3,1,null,false);
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,dsEntry,dsEntry,appEntry,appEntry,appEntry,appEntry),searchResults.getResults());
      SortInfo nameAsc=new SortInfo(AbstractSystemMetadataWriter.ENTITY_NAME_KEY,SortInfo.SortOrder.ASC);
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,2,0,null,false);
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry),searchResults.getResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,1,2,0,null,false);
      Assert.assertEquals(ImmutableList.of(dsEntry,appEntry),searchResults.getResults());
      SortInfo nameDesc=new SortInfo(AbstractSystemMetadataWriter.ENTITY_NAME_KEY,SortInfo.SortOrder.DESC);
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,0,2,0,null,false);
      Assert.assertEquals(ImmutableList.of(appEntry,dsEntry),searchResults.getResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,2,1,0,null,false);
      Assert.assertEquals(ImmutableList.of(flowEntry),searchResults.getResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,2,0,0,null,false);
      Assert.assertTrue(searchResults.getResults().isEmpty());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,1,0,0,null,false);
      Assert.assertTrue(searchResults.getResults().isEmpty());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,4,0,0,null,false);
      Assert.assertTrue(searchResults.getResults().isEmpty());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,100,0,0,null,false);
      Assert.assertTrue(searchResults.getResults().isEmpty());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,null,false);
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,appEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(dsName,appName),searchResults.getCursors());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,searchResults.getCursors().get(0),false);
      Assert.assertEquals(ImmutableList.of(dsEntry,appEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(appName),searchResults.getCursors());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,searchResults.getCursors().get(0),false);
      Assert.assertEquals(ImmutableList.of(appEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(),searchResults.getCursors());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,2,3,null,false);
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,appEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(appName),searchResults.getCursors());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,3,1,2,null,false);
      Assert.assertEquals(ImmutableList.of(),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(),searchResults.getCursors());
    }
  }
);
}","@Test public void testPagination() throws Exception {
  final MetadataDataset dataset=getDataset(DatasetFrameworkTestUtil.NAMESPACE_ID.dataset(""String_Node_Str""),MetadataScope.SYSTEM);
  TransactionExecutor txnl=dsFrameworkUtil.newInMemoryTransactionExecutor((TransactionAware)dataset);
  final String flowName=""String_Node_Str"";
  final String dsName=""String_Node_Str"";
  final String appName=""String_Node_Str"";
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      dataset.setProperty(flow1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,flowName);
      dataset.setProperty(dataset1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,dsName);
      dataset.setProperty(app1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,appName);
    }
  }
);
  final String namespaceId=flow1.getNamespace();
  final EnumSet<EntityTypeSimpleName> targets=EnumSet.allOf(EntityTypeSimpleName.class);
  final MetadataEntry flowEntry=new MetadataEntry(flow1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,flowName);
  final MetadataEntry dsEntry=new MetadataEntry(dataset1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,dsName);
  final MetadataEntry appEntry=new MetadataEntry(app1,AbstractSystemMetadataWriter.ENTITY_NAME_KEY,appName);
  txnl.execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      SearchResults searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,SortInfo.DEFAULT,0,3,1,null,false);
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,dsEntry,dsEntry,appEntry,appEntry,appEntry,appEntry),searchResults.getResults());
      SortInfo nameAsc=new SortInfo(AbstractSystemMetadataWriter.ENTITY_NAME_KEY,SortInfo.SortOrder.ASC);
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,2,0,null,false);
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,appEntry),searchResults.getAllResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,1,2,0,null,false);
      Assert.assertEquals(ImmutableList.of(dsEntry,appEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,appEntry),searchResults.getAllResults());
      SortInfo nameDesc=new SortInfo(AbstractSystemMetadataWriter.ENTITY_NAME_KEY,SortInfo.SortOrder.DESC);
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,0,2,0,null,false);
      Assert.assertEquals(ImmutableList.of(appEntry,dsEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(appEntry,dsEntry,flowEntry),searchResults.getAllResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,2,1,0,null,false);
      Assert.assertEquals(ImmutableList.of(flowEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(appEntry,dsEntry,flowEntry),searchResults.getAllResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,2,0,0,null,false);
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,appEntry),searchResults.getAllResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,1,0,0,null,false);
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(ImmutableList.of(appEntry,dsEntry,flowEntry),searchResults.getAllResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,4,0,0,null,false);
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry,appEntry),searchResults.getAllResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameDesc,100,0,0,null,false);
      Assert.assertTrue(searchResults.getResults().isEmpty());
      Assert.assertEquals(ImmutableList.of(appEntry,dsEntry,flowEntry),searchResults.getAllResults());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,null,false);
      Assert.assertEquals(ImmutableList.of(flowEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(dsName,appName),searchResults.getCursors());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,searchResults.getCursors().get(0),false);
      Assert.assertEquals(ImmutableList.of(dsEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(appName),searchResults.getCursors());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,1,3,searchResults.getCursors().get(0),false);
      Assert.assertEquals(ImmutableList.of(appEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(),searchResults.getCursors());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,0,2,3,null,false);
      Assert.assertEquals(ImmutableList.of(flowEntry,dsEntry),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(appName),searchResults.getCursors());
      searchResults=dataset.search(namespaceId,""String_Node_Str"",targets,nameAsc,3,1,2,null,false);
      Assert.assertEquals(ImmutableList.of(),searchResults.getResults());
      Assert.assertEquals(ImmutableList.of(),searchResults.getCursors());
    }
  }
);
}"
5436,"private MetadataSearchResponse search(String ns,String searchQuery,int offset,int limit,int numCursors,boolean showHidden) throws BadRequestException {
  return store.search(ns,searchQuery,EnumSet.allOf(EntityTypeSimpleName.class),SortInfo.DEFAULT,offset,limit,numCursors,null,showHidden);
}","private MetadataSearchResponse search(String ns,String searchQuery,int offset,int limit,int numCursors,boolean showHidden,SortInfo sortInfo) throws BadRequestException {
  return store.search(ns,searchQuery,EnumSet.allOf(EntityTypeSimpleName.class),sortInfo,offset,limit,numCursors,""String_Node_Str"",showHidden);
}"
5437,"private void rewriteHiveConfig(){
  URL resourceURL=getLocalResourceURL(""String_Node_Str"");
  if (resourceURL == null) {
    LOG.warn(""String_Node_Str"");
    return;
  }
  try (InputStream is=new FileInputStream(new File(resourceURL.toURI()))){
    Configuration conf=new Configuration(false);
    conf.addResource(is);
    conf.setBoolean(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,false);
    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER,false);
    String sparkHome=System.getenv(Constants.SPARK_HOME);
    if (sparkHome != null) {
      LOG.debug(""String_Node_Str"",sparkHome);
      conf.set(""String_Node_Str"",sparkHome);
    }
    File newConfFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    try (FileOutputStream os=new FileOutputStream(newConfFile)){
      conf.writeXml(os);
    }
     Files.move(newConfFile.toPath(),Paths.get(resourceURL.toURI()),StandardCopyOption.REPLACE_EXISTING);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",resourceURL,e);
    throw Throwables.propagate(e);
  }
}","private void rewriteHiveConfig(){
  URL resourceURL=getLocalResourceURL(""String_Node_Str"");
  if (resourceURL == null) {
    LOG.warn(""String_Node_Str"");
    return;
  }
  try (InputStream is=new FileInputStream(new File(resourceURL.toURI()))){
    Configuration conf=new Configuration(false);
    conf.addResource(is);
    conf.setBoolean(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,false);
    conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER,false);
    conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,UserGroupInformation.AuthenticationMethod.SIMPLE.name());
    String sparkHome=System.getenv(Constants.SPARK_HOME);
    if (sparkHome != null) {
      LOG.debug(""String_Node_Str"",sparkHome);
      conf.set(""String_Node_Str"",sparkHome);
    }
    File newConfFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    try (FileOutputStream os=new FileOutputStream(newConfFile)){
      conf.writeXml(os);
    }
     Files.move(newConfFile.toPath(),Paths.get(resourceURL.toURI()),StandardCopyOption.REPLACE_EXISTING);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",resourceURL,e);
    throw Throwables.propagate(e);
  }
}"
5438,"private Properties generateKafkaConfig(String kafkaZKConnect){
  int port=Networks.getRandomPort();
  Preconditions.checkState(port > 0,""String_Node_Str"");
  Properties prop=new Properties();
  prop.setProperty(""String_Node_Str"",new File(""String_Node_Str"").getAbsolutePath());
  prop.setProperty(""String_Node_Str"",Integer.toString(port));
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",kafkaZKConnect);
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  return prop;
}","private Properties generateKafkaConfig(String kafkaZKConnect){
  Properties prop=new Properties();
  prop.setProperty(""String_Node_Str"",new File(""String_Node_Str"").getAbsolutePath());
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",kafkaZKConnect);
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  prop.setProperty(""String_Node_Str"",""String_Node_Str"");
  return prop;
}"
5439,"@Override public Boolean call() throws Exception {
  try {
    getDSFramework().getInstances(NamespaceId.DEFAULT);
    return true;
  }
 catch (  ServiceUnavailableException sue) {
    return false;
  }
}","@Override public Boolean call() throws Exception {
  try {
    getDSFramework().getInstances(NamespaceId.SYSTEM);
    return true;
  }
 catch (  ServiceUnavailableException sue) {
    return false;
  }
}"
5440,"@Override protected void startUp() throws Exception {
  if (!zkClientService.isRunning()) {
    zkClientService.startAndWait();
  }
  datasetOpExecutorService.startAndWait();
  remoteSystemOperationsService.startAndWait();
  datasetService.startAndWait();
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      try {
        getDSFramework().getInstances(NamespaceId.DEFAULT);
        return true;
      }
 catch (      ServiceUnavailableException sue) {
        return false;
      }
    }
  }
,5,TimeUnit.MINUTES,10,TimeUnit.SECONDS);
}","@Override protected void startUp() throws Exception {
  if (!zkClientService.isRunning()) {
    zkClientService.startAndWait();
  }
  datasetOpExecutorService.startAndWait();
  remoteSystemOperationsService.startAndWait();
  datasetService.startAndWait();
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      try {
        getDSFramework().getInstances(NamespaceId.SYSTEM);
        return true;
      }
 catch (      ServiceUnavailableException sue) {
        return false;
      }
    }
  }
,5,TimeUnit.MINUTES,10,TimeUnit.SECONDS);
}"
5441,"/** 
 * Constructs a context. To have plugin support, the   {@code pluginInstantiator} must not be null.
 */
protected AbstractContext(Program program,ProgramOptions programOptions,CConfiguration cConf,Set<String> datasets,DatasetFramework dsFramework,TransactionSystemClient txClient,DiscoveryServiceClient discoveryServiceClient,boolean multiThreaded,@Nullable MetricsCollectionService metricsService,Map<String,String> metricsTags,SecureStore secureStore,SecureStoreManager secureStoreManager,MessagingService messagingService,@Nullable PluginInstantiator pluginInstantiator){
  super(program.getId());
  this.program=program;
  this.programOptions=programOptions;
  this.runId=ProgramRunners.getRunId(programOptions);
  this.discoveryServiceClient=discoveryServiceClient;
  this.owners=createOwners(program.getId());
  this.programMetrics=createProgramMetrics(program,runId,metricsService,metricsTags);
  this.userMetrics=new ProgramUserMetrics(programMetrics);
  this.retryStrategy=SystemArguments.getRetryStrategy(programOptions.getUserArguments().asMap(),program.getType(),cConf);
  Map<String,String> runtimeArgs=new HashMap<>(programOptions.getUserArguments().asMap());
  this.logicalStartTime=ProgramRunners.updateLogicalStartTime(runtimeArgs);
  this.runtimeArguments=Collections.unmodifiableMap(runtimeArgs);
  Map<String,Map<String,String>> staticDatasets=new HashMap<>();
  for (  String name : datasets) {
    staticDatasets.put(name,runtimeArguments);
  }
  SystemDatasetInstantiator instantiator=new SystemDatasetInstantiator(dsFramework,program.getClassLoader(),owners);
  this.messagingContext=new MultiThreadMessagingContext(messagingService);
  this.datasetCache=multiThreaded ? new MultiThreadDatasetCache(instantiator,txClient,new NamespaceId(program.getId().getNamespace()),runtimeArguments,programMetrics,staticDatasets,messagingContext) : new SingleThreadDatasetCache(instantiator,txClient,new NamespaceId(program.getId().getNamespace()),runtimeArguments,programMetrics,staticDatasets);
  this.pluginInstantiator=pluginInstantiator;
  this.pluginContext=new DefaultPluginContext(pluginInstantiator,program.getId(),program.getApplicationSpecification().getPlugins());
  this.admin=new DefaultAdmin(dsFramework,program.getId().getNamespaceId(),secureStoreManager,new BasicMessagingAdmin(messagingService,program.getId().getNamespaceId()),retryStrategy);
  this.secureStore=secureStore;
  this.defaultTxTimeout=determineTransactionTimeout(cConf);
  this.transactional=Transactions.createTransactional(getDatasetCache(),defaultTxTimeout);
  if (!multiThreaded) {
    datasetCache.addExtraTransactionAware(messagingContext);
  }
}","/** 
 * Constructs a context. To have plugin support, the   {@code pluginInstantiator} must not be null.
 */
protected AbstractContext(Program program,ProgramOptions programOptions,CConfiguration cConf,Set<String> datasets,DatasetFramework dsFramework,TransactionSystemClient txClient,DiscoveryServiceClient discoveryServiceClient,boolean multiThreaded,@Nullable MetricsCollectionService metricsService,Map<String,String> metricsTags,SecureStore secureStore,SecureStoreManager secureStoreManager,MessagingService messagingService,@Nullable PluginInstantiator pluginInstantiator){
  super(program.getId());
  this.program=program;
  this.programOptions=programOptions;
  this.runId=ProgramRunners.getRunId(programOptions);
  this.discoveryServiceClient=discoveryServiceClient;
  this.owners=createOwners(program.getId());
  this.programMetrics=createProgramMetrics(program,runId,metricsService,metricsTags);
  this.userMetrics=new ProgramUserMetrics(programMetrics);
  this.retryStrategy=SystemArguments.getRetryStrategy(programOptions.getUserArguments().asMap(),program.getType(),cConf);
  Map<String,String> runtimeArgs=new HashMap<>(programOptions.getUserArguments().asMap());
  this.logicalStartTime=ProgramRunners.updateLogicalStartTime(runtimeArgs);
  this.runtimeArguments=Collections.unmodifiableMap(runtimeArgs);
  Map<String,Map<String,String>> staticDatasets=new HashMap<>();
  for (  String name : datasets) {
    staticDatasets.put(name,runtimeArguments);
  }
  SystemDatasetInstantiator instantiator=new SystemDatasetInstantiator(dsFramework,program.getClassLoader(),owners);
  this.messagingContext=new MultiThreadMessagingContext(messagingService);
  TransactionSystemClient retryingTxClient=new RetryingShortTransactionSystemClient(txClient,retryStrategy);
  this.datasetCache=multiThreaded ? new MultiThreadDatasetCache(instantiator,retryingTxClient,new NamespaceId(program.getId().getNamespace()),runtimeArguments,programMetrics,staticDatasets,messagingContext) : new SingleThreadDatasetCache(instantiator,retryingTxClient,new NamespaceId(program.getId().getNamespace()),runtimeArguments,programMetrics,staticDatasets);
  this.pluginInstantiator=pluginInstantiator;
  this.pluginContext=new DefaultPluginContext(pluginInstantiator,program.getId(),program.getApplicationSpecification().getPlugins());
  this.admin=new DefaultAdmin(dsFramework,program.getId().getNamespaceId(),secureStoreManager,new BasicMessagingAdmin(messagingService,program.getId().getNamespaceId()),retryStrategy);
  this.secureStore=secureStore;
  this.defaultTxTimeout=determineTransactionTimeout(cConf);
  this.transactional=Transactions.createTransactional(getDatasetCache(),defaultTxTimeout);
  if (!multiThreaded) {
    datasetCache.addExtraTransactionAware(messagingContext);
  }
}"
5442,"/** 
 * Creates an instance with all Admin functions supported.
 */
public DefaultAdmin(DatasetFramework dsFramework,NamespaceId namespace,SecureStoreManager secureStoreManager,@Nullable MessagingAdmin messagingAdmin,RetryStrategy retryStrategy){
  this.dsFramework=dsFramework;
  this.namespace=namespace;
  this.secureStoreManager=secureStoreManager;
  this.messagingAdmin=messagingAdmin;
  this.retryStrategy=retryStrategy;
}","/** 
 * Creates an instance with all Admin functions supported.
 */
public DefaultAdmin(DatasetFramework dsFramework,NamespaceId namespace,SecureStoreManager secureStoreManager,@Nullable MessagingAdmin messagingAdmin,RetryStrategy retryStrategy){
  super(dsFramework,namespace,retryStrategy);
  this.secureStoreManager=secureStoreManager;
  this.messagingAdmin=messagingAdmin;
  this.retryStrategy=retryStrategy;
}"
5443,"/** 
 * Get the retry strategy for a program given its arguments and the CDAP defaults for the program type.
 * @return the retry strategy to use for internal calls
 * @throws IllegalArgumentException if there is an invalid value for an argument
 */
public static RetryStrategy getRetryStrategy(Map<String,String> args,ProgramType programType,CConfiguration cConf){
  String keyPrefix;
switch (programType) {
case MAPREDUCE:
    keyPrefix=Constants.Retry.MAPREDUCE_PREFIX;
  break;
case SPARK:
keyPrefix=Constants.Retry.SPARK_PREFIX;
break;
case WORKFLOW:
keyPrefix=Constants.Retry.WORKFLOW_PREFIX;
break;
case WORKER:
keyPrefix=Constants.Retry.WORKER_PREFIX;
break;
case SERVICE:
keyPrefix=Constants.Retry.SERVICE_PREFIX;
break;
case FLOW:
keyPrefix=Constants.Retry.FLOW_PREFIX;
break;
case CUSTOM_ACTION:
keyPrefix=Constants.Retry.CUSTOM_ACTION_PREFIX;
break;
default :
throw new IllegalArgumentException(""String_Node_Str"" + programType);
}
String typeKey=keyPrefix + Constants.Retry.TYPE;
String maxRetriesKey=keyPrefix + Constants.Retry.MAX_RETRIES;
String maxTimeKey=keyPrefix + Constants.Retry.MAX_TIME_SECS;
String baseDelayKey=keyPrefix + Constants.Retry.DELAY_BASE_MS;
String maxDelayKey=keyPrefix + Constants.Retry.DELAY_MAX_MS;
String typeStr=args.get(RETRY_POLICY_TYPE);
if (typeStr == null) {
typeStr=cConf.get(typeKey);
}
RetryStrategyType type=RetryStrategyType.from(typeStr);
if (type == RetryStrategyType.NONE) {
return RetryStrategies.noRetry();
}
int maxRetries=getNonNegativeInt(args,RETRY_POLICY_MAX_RETRIES,RETRY_POLICY_MAX_RETRIES,cConf.getInt(maxRetriesKey));
long maxTimeSecs=getNonNegativeLong(args,RETRY_POLICY_MAX_TIME_SECS,RETRY_POLICY_MAX_TIME_SECS,cConf.getLong(maxTimeKey));
long baseDelay=getNonNegativeLong(args,RETRY_POLICY_DELAY_BASE_MS,RETRY_POLICY_DELAY_BASE_MS,cConf.getLong(baseDelayKey));
RetryStrategy baseStrategy;
switch (type) {
case FIXED_DELAY:
baseStrategy=RetryStrategies.fixDelay(baseDelay,TimeUnit.MILLISECONDS);
break;
case EXPONENTIAL_BACKOFF:
long maxDelay=getNonNegativeLong(args,RETRY_POLICY_DELAY_MAX_MS,RETRY_POLICY_DELAY_MAX_MS,cConf.getLong(maxDelayKey));
baseStrategy=RetryStrategies.exponentialDelay(baseDelay,maxDelay,TimeUnit.MILLISECONDS);
break;
default :
throw new IllegalStateException(""String_Node_Str"" + type);
}
return RetryStrategies.limit(maxRetries,RetryStrategies.timeLimit(maxTimeSecs,TimeUnit.SECONDS,baseStrategy));
}","/** 
 * Get the retry strategy for a program given its arguments and the CDAP defaults for the program type.
 * @return the retry strategy to use for internal calls
 * @throws IllegalArgumentException if there is an invalid value for an argument
 */
public static RetryStrategy getRetryStrategy(Map<String,String> args,ProgramType programType,CConfiguration cConf){
  String keyPrefix;
switch (programType) {
case MAPREDUCE:
    keyPrefix=Constants.Retry.MAPREDUCE_PREFIX;
  break;
case SPARK:
keyPrefix=Constants.Retry.SPARK_PREFIX;
break;
case WORKFLOW:
keyPrefix=Constants.Retry.WORKFLOW_PREFIX;
break;
case WORKER:
keyPrefix=Constants.Retry.WORKER_PREFIX;
break;
case SERVICE:
keyPrefix=Constants.Retry.SERVICE_PREFIX;
break;
case FLOW:
keyPrefix=Constants.Retry.FLOW_PREFIX;
break;
case CUSTOM_ACTION:
keyPrefix=Constants.Retry.CUSTOM_ACTION_PREFIX;
break;
default :
throw new IllegalArgumentException(""String_Node_Str"" + programType);
}
CConfiguration policyConf=CConfiguration.copy(cConf);
String typeStr=args.get(RETRY_POLICY_TYPE);
if (typeStr != null) {
policyConf.set(keyPrefix + Constants.Retry.TYPE,typeStr);
}
int maxRetries=getNonNegativeInt(args,RETRY_POLICY_MAX_RETRIES,RETRY_POLICY_MAX_RETRIES,-1);
if (maxRetries >= 0) {
policyConf.setInt(keyPrefix + Constants.Retry.MAX_RETRIES,maxRetries);
}
long maxTimeSecs=getNonNegativeLong(args,RETRY_POLICY_MAX_TIME_SECS,RETRY_POLICY_MAX_TIME_SECS,-1L);
if (maxTimeSecs >= 0) {
policyConf.setLong(keyPrefix + Constants.Retry.MAX_TIME_SECS,maxTimeSecs);
}
long baseDelay=getNonNegativeLong(args,RETRY_POLICY_DELAY_BASE_MS,RETRY_POLICY_DELAY_BASE_MS,-1L);
if (baseDelay >= 0) {
policyConf.setLong(keyPrefix + Constants.Retry.DELAY_BASE_MS,baseDelay);
}
long maxDelay=getNonNegativeLong(args,RETRY_POLICY_DELAY_MAX_MS,RETRY_POLICY_DELAY_MAX_MS,-1L);
if (maxDelay >= 0) {
policyConf.setLong(keyPrefix + Constants.Retry.DELAY_MAX_MS,maxDelay);
}
return RetryStrategies.fromConfiguration(policyConf,keyPrefix);
}"
5444,"MapReduceRuntimeService(Injector injector,CConfiguration cConf,Configuration hConf,MapReduce mapReduce,MapReduceSpecification specification,BasicMapReduceContext context,Location programJarLocation,NamespacedLocationFactory locationFactory,StreamAdmin streamAdmin,TransactionSystemClient txClient,AuthorizationEnforcer authorizationEnforcer,AuthenticationContext authenticationContext){
  this.injector=injector;
  this.cConf=cConf;
  this.hConf=hConf;
  this.mapReduce=mapReduce;
  this.specification=specification;
  this.programJarLocation=programJarLocation;
  this.locationFactory=locationFactory;
  this.streamAdmin=streamAdmin;
  this.txClient=txClient;
  this.context=context;
  this.authorizationEnforcer=authorizationEnforcer;
  this.authenticationContext=authenticationContext;
}","MapReduceRuntimeService(Injector injector,CConfiguration cConf,Configuration hConf,MapReduce mapReduce,MapReduceSpecification specification,BasicMapReduceContext context,Location programJarLocation,NamespacedLocationFactory locationFactory,StreamAdmin streamAdmin,TransactionSystemClient txClient,AuthorizationEnforcer authorizationEnforcer,AuthenticationContext authenticationContext){
  this.injector=injector;
  this.cConf=cConf;
  this.hConf=hConf;
  this.mapReduce=mapReduce;
  this.specification=specification;
  this.programJarLocation=programJarLocation;
  this.locationFactory=locationFactory;
  this.streamAdmin=streamAdmin;
  this.txClient=new RetryingLongTransactionSystemClient(txClient,context.getRetryStrategy());
  this.context=context;
  this.authorizationEnforcer=authorizationEnforcer;
  this.authenticationContext=authenticationContext;
}"
5445,"@Override public void initialize(TwillContext context){
  System.setSecurityManager(new RunnableSecurityManager(System.getSecurityManager()));
  runlatch=new CountDownLatch(1);
  name=context.getSpecification().getName();
  LOG.info(""String_Node_Str"" + name);
  try {
    CommandLine cmdLine=parseArgs(context.getApplicationArguments());
    hConf=new Configuration();
    hConf.clear();
    hConf.addResource(new File(cmdLine.getOptionValue(RunnableOptions.HADOOP_CONF_FILE)).toURI().toURL());
    UserGroupInformation.setConfiguration(hConf);
    cConf=CConfiguration.create(new File(cmdLine.getOptionValue(RunnableOptions.CDAP_CONF_FILE)));
    Injector injector=Guice.createInjector(createModule(context));
    zkClientService=injector.getInstance(ZKClientService.class);
    kafkaClientService=injector.getInstance(KafkaClientService.class);
    metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
    streamCoordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
    programOpts=createProgramOptions(cmdLine,context,context.getSpecification().getConfigs());
    logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
    logAppenderInitializer.initialize();
    programRunner=createProgramRunner(injector);
    try {
      Location programJarLocation=Locations.toLocation(new File(cmdLine.getOptionValue(RunnableOptions.JAR)));
      ProgramId programId=GSON.fromJson(cmdLine.getOptionValue(RunnableOptions.PROGRAM_ID),ProgramId.class);
      ApplicationSpecification appSpec=readAppSpec(new File(cmdLine.getOptionValue(RunnableOptions.APP_SPEC_FILE)));
      program=Programs.create(cConf,programRunner,new ProgramDescriptor(programId,appSpec),programJarLocation,BundleJarUtil.unJar(programJarLocation,Files.createTempDir()));
    }
 catch (    IOException e) {
      throw Throwables.propagate(e);
    }
    resourceReporter=new ProgramRunnableResourceReporter(program.getId(),metricsCollectionService,context);
    authEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
    LOG.info(""String_Node_Str"",name);
  }
 catch (  Throwable t) {
    LOG.error(t.getMessage(),t);
    throw Throwables.propagate(t);
  }
}","@Override public void initialize(TwillContext context){
  System.setSecurityManager(new RunnableSecurityManager(System.getSecurityManager()));
  runlatch=new CountDownLatch(1);
  name=context.getSpecification().getName();
  LOG.info(""String_Node_Str"" + name);
  try {
    CommandLine cmdLine=parseArgs(context.getApplicationArguments());
    hConf=new Configuration();
    hConf.clear();
    hConf.addResource(new File(cmdLine.getOptionValue(RunnableOptions.HADOOP_CONF_FILE)).toURI().toURL());
    hConf.set(TxConstants.Service.CFG_DATA_TX_CLIENT_RETRY_STRATEGY,""String_Node_Str"");
    hConf.setInt(TxConstants.Service.CFG_DATA_TX_CLIENT_ATTEMPTS,0);
    UserGroupInformation.setConfiguration(hConf);
    cConf=CConfiguration.create(new File(cmdLine.getOptionValue(RunnableOptions.CDAP_CONF_FILE)));
    Injector injector=Guice.createInjector(createModule(context));
    zkClientService=injector.getInstance(ZKClientService.class);
    kafkaClientService=injector.getInstance(KafkaClientService.class);
    metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
    streamCoordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
    programOpts=createProgramOptions(cmdLine,context,context.getSpecification().getConfigs());
    logAppenderInitializer=injector.getInstance(LogAppenderInitializer.class);
    logAppenderInitializer.initialize();
    programRunner=createProgramRunner(injector);
    try {
      Location programJarLocation=Locations.toLocation(new File(cmdLine.getOptionValue(RunnableOptions.JAR)));
      ProgramId programId=GSON.fromJson(cmdLine.getOptionValue(RunnableOptions.PROGRAM_ID),ProgramId.class);
      ApplicationSpecification appSpec=readAppSpec(new File(cmdLine.getOptionValue(RunnableOptions.APP_SPEC_FILE)));
      program=Programs.create(cConf,programRunner,new ProgramDescriptor(programId,appSpec),programJarLocation,BundleJarUtil.unJar(programJarLocation,Files.createTempDir()));
    }
 catch (    IOException e) {
      throw Throwables.propagate(e);
    }
    resourceReporter=new ProgramRunnableResourceReporter(program.getId(),metricsCollectionService,context);
    authEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
    LOG.info(""String_Node_Str"",name);
  }
 catch (  Throwable t) {
    LOG.error(t.getMessage(),t);
    throw Throwables.propagate(t);
  }
}"
5446,"/** 
 * Start the scheduler services, by initializing them and starting them
 */
protected final void startSchedulers() throws SchedulerException {
  try {
    timeScheduler.init();
    timeScheduler.start();
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    Throwables.propagateIfInstanceOf(t,SchedulerException.class);
    throw new SchedulerException(t);
  }
  try {
    streamSizeScheduler.init();
    streamSizeScheduler.start();
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    Throwables.propagateIfInstanceOf(t,SchedulerException.class);
    throw new SchedulerException(t);
  }
}","/** 
 * Start the scheduler services, by initializing them and starting them
 */
protected final void startSchedulers() throws SchedulerException {
  try {
    timeScheduler.init();
    timeScheduler.start();
    LOG.info(""String_Node_Str"");
  }
 catch (  SchedulerException t) {
    Throwables.propagateIfPossible(t,SchedulerException.class);
    throw new SchedulerException(t);
  }
  try {
    streamSizeScheduler.init();
    streamSizeScheduler.start();
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    Throwables.propagateIfPossible(t,SchedulerException.class);
    throw new SchedulerException(t);
  }
}"
5447,"/** 
 * Stop the quartz scheduler service.
 */
protected final void stopScheduler() throws SchedulerException {
  try {
    streamSizeScheduler.stop();
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",t);
    Throwables.propagateIfInstanceOf(t,SchedulerException.class);
    throw new SchedulerException(t);
  }
 finally {
    try {
      timeScheduler.stop();
      LOG.info(""String_Node_Str"");
    }
 catch (    Throwable t) {
      LOG.error(""String_Node_Str"",t);
      Throwables.propagateIfInstanceOf(t,SchedulerException.class);
      throw new SchedulerException(t);
    }
  }
}","/** 
 * Stop the quartz scheduler service.
 */
protected final void stopScheduler() throws SchedulerException {
  try {
    streamSizeScheduler.stop();
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",t);
    Throwables.propagateIfPossible(t,SchedulerException.class);
    throw new SchedulerException(t);
  }
 finally {
    try {
      timeScheduler.stop();
      LOG.info(""String_Node_Str"");
    }
 catch (    Throwable t) {
      LOG.error(""String_Node_Str"",t);
      Throwables.propagateIfPossible(t,SchedulerException.class);
      throw new SchedulerException(t);
    }
  }
}"
5448,"@Inject public DistributedSchedulerService(TimeScheduler timeScheduler,StreamSizeScheduler streamSizeScheduler,Store store){
  super(timeScheduler,streamSizeScheduler,store);
  this.serviceDelegate=new RetryOnStartFailureService(new Supplier<Service>(){
    @Override public Service get(){
      return new AbstractService(){
        @Override protected void doStart(){
          try {
            startSchedulers();
            notifyStarted();
          }
 catch (          SchedulerException e) {
            LOG.warn(""String_Node_Str"",e);
            notifyFailed(e);
          }
        }
        @Override protected void doStop(){
          try {
            stopScheduler();
            notifyStopped();
          }
 catch (          SchedulerException e) {
            notifyFailed(e);
          }
        }
      }
;
    }
  }
,RetryStrategies.exponentialDelay(200,5000,TimeUnit.MILLISECONDS));
}","@Inject public DistributedSchedulerService(TimeScheduler timeScheduler,StreamSizeScheduler streamSizeScheduler,Store store){
  super(timeScheduler,streamSizeScheduler,store);
  this.serviceDelegate=new RetryOnStartFailureService(new Supplier<Service>(){
    @Override public Service get(){
      return new AbstractService(){
        @Override protected void doStart(){
          try {
            startSchedulers();
            notifyStarted();
          }
 catch (          ServiceUnavailableException e) {
            LOG.debug(""String_Node_Str"",e.getMessage());
            notifyFailed(e);
          }
catch (          SchedulerException e) {
            LOG.warn(""String_Node_Str"",e);
            notifyFailed(e);
          }
        }
        @Override protected void doStop(){
          try {
            stopScheduler();
            notifyStopped();
          }
 catch (          SchedulerException e) {
            notifyFailed(e);
          }
        }
      }
;
    }
  }
,RetryStrategies.exponentialDelay(200,5000,TimeUnit.MILLISECONDS));
}"
5449,"@Override public Service get(){
  return new AbstractService(){
    @Override protected void doStart(){
      try {
        startSchedulers();
        notifyStarted();
      }
 catch (      SchedulerException e) {
        LOG.warn(""String_Node_Str"",e);
        notifyFailed(e);
      }
    }
    @Override protected void doStop(){
      try {
        stopScheduler();
        notifyStopped();
      }
 catch (      SchedulerException e) {
        notifyFailed(e);
      }
    }
  }
;
}","@Override public Service get(){
  return new AbstractService(){
    @Override protected void doStart(){
      try {
        startSchedulers();
        notifyStarted();
      }
 catch (      ServiceUnavailableException e) {
        LOG.debug(""String_Node_Str"",e.getMessage());
        notifyFailed(e);
      }
catch (      SchedulerException e) {
        LOG.warn(""String_Node_Str"",e);
        notifyFailed(e);
      }
    }
    @Override protected void doStop(){
      try {
        stopScheduler();
        notifyStopped();
      }
 catch (      SchedulerException e) {
        notifyFailed(e);
      }
    }
  }
;
}"
5450,"@Override protected void doStart(){
  try {
    startSchedulers();
    notifyStarted();
  }
 catch (  SchedulerException e) {
    LOG.warn(""String_Node_Str"",e);
    notifyFailed(e);
  }
}","@Override protected void doStart(){
  try {
    startSchedulers();
    notifyStarted();
  }
 catch (  ServiceUnavailableException e) {
    LOG.debug(""String_Node_Str"",e.getMessage());
    notifyFailed(e);
  }
catch (  SchedulerException e) {
    LOG.warn(""String_Node_Str"",e);
    notifyFailed(e);
  }
}"
5451,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String datasetType=arguments.get(ArgumentName.DATASET_TYPE.toString());
  String datasetName=arguments.get(ArgumentName.NEW_DATASET.toString());
  String datasetPropertiesString=arguments.getOptional(ArgumentName.DATASET_PROPERTIES.toString(),""String_Node_Str"");
  String datasetDescription=arguments.getOptional(ArgumentName.DATASET_DESCRIPTON.toString(),null);
  Map<String,String> datasetProperties=ArgumentParser.parseMap(datasetPropertiesString);
  DatasetInstanceConfiguration datasetConfig=new DatasetInstanceConfiguration(datasetType,datasetProperties,datasetDescription,null);
  datasetClient.create(cliConfig.getCurrentNamespace().dataset(datasetName),datasetConfig);
  output.printf(""String_Node_Str"",datasetName,datasetType,GSON.toJson(datasetProperties));
  output.println();
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String datasetType=arguments.get(ArgumentName.DATASET_TYPE.toString());
  String datasetName=arguments.get(ArgumentName.NEW_DATASET.toString());
  String datasetPropertiesString=arguments.getOptional(ArgumentName.DATASET_PROPERTIES.toString(),""String_Node_Str"");
  String datasetDescription=arguments.getOptional(ArgumentName.DATASET_DESCRIPTON.toString(),null);
  Map<String,String> datasetProperties=ArgumentParser.parseMap(datasetPropertiesString,ArgumentName.DATASET_PROPERTIES.toString());
  DatasetInstanceConfiguration datasetConfig=new DatasetInstanceConfiguration(datasetType,datasetProperties,datasetDescription,null);
  datasetClient.create(cliConfig.getCurrentNamespace().dataset(datasetName),datasetConfig);
  output.printf(""String_Node_Str"",datasetName,datasetType,GSON.toJson(datasetProperties));
  output.println();
}"
5452,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetId instance=cliConfig.getCurrentNamespace().dataset(arguments.get(ArgumentName.DATASET.toString()));
  Map<String,String> properties=ArgumentParser.parseMap(arguments.get(ArgumentName.DATASET_PROPERTIES.toString()));
  datasetClient.updateExisting(instance,properties);
  output.printf(""String_Node_Str"",instance.getEntityName(),GSON.toJson(properties));
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetId instance=cliConfig.getCurrentNamespace().dataset(arguments.get(ArgumentName.DATASET.toString()));
  Map<String,String> properties=ArgumentParser.parseMap(arguments.get(ArgumentName.DATASET_PROPERTIES.toString()),ArgumentName.DATASET_PROPERTIES.toString());
  datasetClient.updateExisting(instance,properties);
  output.printf(""String_Node_Str"",instance.getEntityName(),GSON.toJson(properties));
}"
5453,"@Override public String getDescription(){
  return String.format(""String_Node_Str"",Fragment.of(Article.A,ElementType.DATASET.getName()));
}","@Override public String getDescription(){
  return String.format(""String_Node_Str"",Fragment.of(Article.A,ElementType.DATASET.getName()),ArgumentName.DATASET_PROPERTIES);
}"
5454,"@Override public void perform(Arguments arguments,PrintStream printStream) throws Exception {
  String runtimeArgs=arguments.get(ArgumentName.RUNTIME_ARGS.toString());
  Map<String,String> args=ArgumentParser.parseMap(runtimeArgs);
  setPreferences(arguments,printStream,args);
}","@Override public void perform(Arguments arguments,PrintStream printStream) throws Exception {
  String runtimeArgs=arguments.get(ArgumentName.RUNTIME_ARGS.toString());
  Map<String,String> args=ArgumentParser.parseMap(runtimeArgs,ArgumentName.RUNTIME_ARGS.toString());
  setPreferences(arguments,printStream,args);
}"
5455,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  ProgramId programId=parseProgramId(arguments,elementType);
  String appName=programId.getApplication();
  String appVersion=programId.getVersion();
  String programName=programId.getProgram();
  String runtimeArgsString=arguments.get(ArgumentName.RUNTIME_ARGS.toString());
  Map<String,String> runtimeArgs=ArgumentParser.parseMap(runtimeArgsString);
  programClient.setRuntimeArgs(programId,runtimeArgs);
  output.printf(""String_Node_Str"",elementType.getName(),programName,appName,appVersion,runtimeArgsString);
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  ProgramId programId=parseProgramId(arguments,elementType);
  String appName=programId.getApplication();
  String appVersion=programId.getVersion();
  String programName=programId.getProgram();
  String runtimeArgsString=arguments.get(ArgumentName.RUNTIME_ARGS.toString());
  Map<String,String> runtimeArgs=ArgumentParser.parseMap(runtimeArgsString,ArgumentName.RUNTIME_ARGS.toString());
  programClient.setRuntimeArgs(programId,runtimeArgs);
  output.printf(""String_Node_Str"",elementType.getName(),programName,appName,appVersion,runtimeArgsString);
}"
5456,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  ServiceId serviceId=new ServiceId(parseProgramId(arguments,ElementType.SERVICE));
  String routeConfig=arguments.get(ArgumentName.ROUTE_CONFIG.getName());
  serviceClient.storeRouteConfig(serviceId,ArgumentParser.parseStringIntegerMap(routeConfig));
  output.printf(""String_Node_Str"",ElementType.SERVICE.getName(),serviceId.getProgram(),serviceId.getApplication(),routeConfig);
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  ServiceId serviceId=new ServiceId(parseProgramId(arguments,ElementType.SERVICE));
  String routeConfig=arguments.get(ArgumentName.ROUTE_CONFIG.getName());
  serviceClient.storeRouteConfig(serviceId,ArgumentParser.parseStringIntegerMap(routeConfig,ArgumentName.ROUTE_CONFIG.toString()));
  output.printf(""String_Node_Str"",ElementType.SERVICE.getName(),serviceId.getProgram(),serviceId.getApplication(),routeConfig);
}"
5457,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  StreamId streamId=cliConfig.getCurrentNamespace().stream(arguments.get(ArgumentName.STREAM.toString()));
  StreamProperties currentProperties=streamClient.getConfig(streamId);
  String formatName=arguments.get(ArgumentName.FORMAT.toString());
  Schema schema=getSchema(arguments);
  Map<String,String> settings=Collections.emptyMap();
  if (arguments.hasArgument(ArgumentName.SETTINGS.toString())) {
    settings=ArgumentParser.parseMap(arguments.get(ArgumentName.SETTINGS.toString()));
  }
  FormatSpecification formatSpecification=new FormatSpecification(formatName,schema,settings);
  StreamProperties streamProperties=new StreamProperties(currentProperties.getTTL(),formatSpecification,currentProperties.getNotificationThresholdMB(),currentProperties.getDescription());
  streamClient.setStreamProperties(streamId,streamProperties);
  output.printf(""String_Node_Str"",streamId.getEntityName());
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  StreamId streamId=cliConfig.getCurrentNamespace().stream(arguments.get(ArgumentName.STREAM.toString()));
  StreamProperties currentProperties=streamClient.getConfig(streamId);
  String formatName=arguments.get(ArgumentName.FORMAT.toString());
  Schema schema=getSchema(arguments);
  Map<String,String> settings=Collections.emptyMap();
  if (arguments.hasArgument(ArgumentName.SETTINGS.toString())) {
    settings=ArgumentParser.parseMap(arguments.get(ArgumentName.SETTINGS.toString()),ArgumentName.SETTINGS.toString());
  }
  FormatSpecification formatSpecification=new FormatSpecification(formatName,schema,settings);
  StreamProperties streamProperties=new StreamProperties(currentProperties.getTTL(),formatSpecification,currentProperties.getNotificationThresholdMB(),currentProperties.getDescription());
  streamClient.setStreamProperties(streamId,streamProperties);
  output.printf(""String_Node_Str"",streamId.getEntityName());
}"
5458,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] programIdParts=arguments.get(elementType.getArgumentName().toString()).split(""String_Node_Str"");
  if (programIdParts.length < 2) {
    throw new CommandInputError(this);
  }
  ProgramId programId=parseProgramId(arguments,elementType);
  String appName=programId.getApplication();
  String appVersion=programId.getVersion();
  String programName=programId.getProgram();
  String runtimeArgsString=arguments.getOptional(ArgumentName.RUNTIME_ARGS.toString(),""String_Node_Str"");
  if (runtimeArgsString == null || runtimeArgsString.isEmpty()) {
    programClient.start(programId,isDebug,null);
    runtimeArgsString=GSON.toJson(programClient.getRuntimeArgs(programId));
    output.printf(""String_Node_Str"",elementType.getName(),programName,appName,appVersion,runtimeArgsString);
  }
 else {
    Map<String,String> runtimeArgs=ArgumentParser.parseMap(runtimeArgsString);
    programClient.start(programId,isDebug,runtimeArgs);
    output.printf(""String_Node_Str"",elementType.getName(),programName,appName,appVersion,runtimeArgsString);
  }
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] programIdParts=arguments.get(elementType.getArgumentName().toString()).split(""String_Node_Str"");
  if (programIdParts.length < 2) {
    throw new CommandInputError(this);
  }
  ProgramId programId=parseProgramId(arguments,elementType);
  String appName=programId.getApplication();
  String appVersion=programId.getVersion();
  String programName=programId.getProgram();
  String runtimeArgsString=arguments.getOptional(ArgumentName.RUNTIME_ARGS.toString(),""String_Node_Str"");
  if (runtimeArgsString == null || runtimeArgsString.isEmpty()) {
    programClient.start(programId,isDebug,null);
    runtimeArgsString=GSON.toJson(programClient.getRuntimeArgs(programId));
    output.printf(""String_Node_Str"",elementType.getName(),programName,appName,appVersion,runtimeArgsString);
  }
 else {
    Map<String,String> runtimeArgs=ArgumentParser.parseMap(runtimeArgsString,ArgumentName.RUNTIME_ARGS.toString());
    programClient.start(programId,isDebug,runtimeArgs);
    output.printf(""String_Node_Str"",elementType.getName(),programName,appName,appVersion,runtimeArgsString);
  }
}"
5459,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  EntityId entity=EntityId.fromString(arguments.get(ArgumentName.ENTITY.toString()));
  Map<String,String> properties=parseMap(arguments.get(""String_Node_Str""));
  client.addProperties(entity.toId(),properties);
  output.println(""String_Node_Str"");
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  EntityId entity=EntityId.fromString(arguments.get(ArgumentName.ENTITY.toString()));
  Map<String,String> properties=parseMap(arguments.get(""String_Node_Str""),""String_Node_Str"");
  client.addProperties(entity.toId(),properties);
  output.println(""String_Node_Str"");
}"
5460,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String metric=arguments.get(""String_Node_Str"");
  Map<String,String> tags=ArgumentParser.parseMap(arguments.getOptional(""String_Node_Str"",""String_Node_Str""));
  String start=arguments.getOptional(""String_Node_Str"",""String_Node_Str"");
  String end=arguments.getOptional(""String_Node_Str"",""String_Node_Str"");
  MetricQueryResult result=client.query(tags,ImmutableList.of(metric),ImmutableList.<String>of(),start.isEmpty() ? null : start,end.isEmpty() ? null : end);
  output.printf(""String_Node_Str"",result.getStartTime());
  output.printf(""String_Node_Str"",result.getEndTime());
  for (  MetricQueryResult.TimeSeries series : result.getSeries()) {
    output.println();
    output.printf(""String_Node_Str"",series.getMetricName());
    if (!series.getGrouping().isEmpty()) {
      output.printf(""String_Node_Str"",Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(series.getGrouping()));
    }
    Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"").setRows(ImmutableList.copyOf(series.getData()),new RowMaker<MetricQueryResult.TimeValue>(){
      @Override public List<?> makeRow(      MetricQueryResult.TimeValue object){
        return Lists.newArrayList(object.getTime(),object.getValue());
      }
    }
).build();
    cliConfig.getTableRenderer().render(cliConfig,output,table);
  }
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String metric=arguments.get(""String_Node_Str"");
  Map<String,String> tags=ArgumentParser.parseMap(arguments.getOptional(""String_Node_Str"",""String_Node_Str""),""String_Node_Str"");
  String start=arguments.getOptional(""String_Node_Str"",""String_Node_Str"");
  String end=arguments.getOptional(""String_Node_Str"",""String_Node_Str"");
  MetricQueryResult result=client.query(tags,ImmutableList.of(metric),ImmutableList.<String>of(),start.isEmpty() ? null : start,end.isEmpty() ? null : end);
  output.printf(""String_Node_Str"",result.getStartTime());
  output.printf(""String_Node_Str"",result.getEndTime());
  for (  MetricQueryResult.TimeSeries series : result.getSeries()) {
    output.println();
    output.printf(""String_Node_Str"",series.getMetricName());
    if (!series.getGrouping().isEmpty()) {
      output.printf(""String_Node_Str"",Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(series.getGrouping()));
    }
    Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"").setRows(ImmutableList.copyOf(series.getData()),new RowMaker<MetricQueryResult.TimeValue>(){
      @Override public List<?> makeRow(      MetricQueryResult.TimeValue object){
        return Lists.newArrayList(object.getTime(),object.getValue());
      }
    }
).build();
    cliConfig.getTableRenderer().render(cliConfig,output,table);
  }
}"
5461,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  Map<String,String> tags=ArgumentParser.parseMap(arguments.getOptional(""String_Node_Str"",""String_Node_Str""));
  List<String> results=client.searchMetrics(tags);
  for (  String result : results) {
    output.println(result);
  }
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  Map<String,String> tags=ArgumentParser.parseMap(arguments.getOptional(""String_Node_Str"",""String_Node_Str""),""String_Node_Str"");
  List<String> results=client.searchMetrics(tags);
  for (  String result : results) {
    output.println(result);
  }
}"
5462,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  Map<String,String> tags=ArgumentParser.parseMap(arguments.getOptional(""String_Node_Str"",""String_Node_Str""));
  List<MetricTagValue> results=client.searchTags(tags);
  for (  MetricTagValue result : results) {
    output.printf(""String_Node_Str"",result.getName(),result.getValue());
  }
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  Map<String,String> tags=ArgumentParser.parseMap(arguments.getOptional(""String_Node_Str"",""String_Node_Str""),""String_Node_Str"");
  List<MetricTagValue> results=client.searchTags(tags);
  for (  MetricTagValue result : results) {
    output.printf(""String_Node_Str"",result.getName(),result.getValue());
  }
}"
5463,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  StreamId streamId=cliConfig.getCurrentNamespace().stream(arguments.get(ArgumentName.STREAM.toString()));
  StreamViewId viewId=streamId.view(arguments.get(ArgumentName.VIEW.toString()));
  String formatName=arguments.get(ArgumentName.FORMAT.toString());
  Schema schema=getSchema(arguments);
  Map<String,String> settings=Collections.emptyMap();
  if (arguments.hasArgument(ArgumentName.SETTINGS.toString())) {
    settings=ArgumentParser.parseMap(arguments.get(ArgumentName.SETTINGS.toString()));
  }
  FormatSpecification formatSpecification=new FormatSpecification(formatName,schema,settings);
  ViewSpecification viewSpecification=new ViewSpecification(formatSpecification);
  boolean created=client.createOrUpdate(viewId,viewSpecification);
  if (created) {
    output.printf(""String_Node_Str"",viewId.getEntityName());
  }
 else {
    output.printf(""String_Node_Str"",viewId.getEntityName());
  }
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  StreamId streamId=cliConfig.getCurrentNamespace().stream(arguments.get(ArgumentName.STREAM.toString()));
  StreamViewId viewId=streamId.view(arguments.get(ArgumentName.VIEW.toString()));
  String formatName=arguments.get(ArgumentName.FORMAT.toString());
  Schema schema=getSchema(arguments);
  Map<String,String> settings=Collections.emptyMap();
  if (arguments.hasArgument(ArgumentName.SETTINGS.toString())) {
    settings=ArgumentParser.parseMap(arguments.get(ArgumentName.SETTINGS.toString()),ArgumentName.SETTINGS.toString());
  }
  FormatSpecification formatSpecification=new FormatSpecification(formatName,schema,settings);
  ViewSpecification viewSpecification=new ViewSpecification(formatSpecification);
  boolean created=client.createOrUpdate(viewId,viewSpecification);
  if (created) {
    output.printf(""String_Node_Str"",viewId.getEntityName());
  }
 else {
    output.printf(""String_Node_Str"",viewId.getEntityName());
  }
}"
5464,"protected Map<String,String> parseMap(@Nullable String value){
  return ArgumentParser.parseMap(value);
}","protected Map<String,String> parseMap(@Nullable String value,String description){
  return ArgumentParser.parseMap(value,description);
}"
5465,"/** 
 * Parses a map in the format: ""key1=a key2=b ..""
 * @param mapString {@link String} representation of the map
 * @return a {@link Map} with {@link String} keys and {@link String} values
 */
public static Map<String,String> parseMap(String mapString){
  if (mapString == null || mapString.isEmpty()) {
    return ImmutableMap.of();
  }
  ImmutableMap.Builder<String,String> result=ImmutableMap.builder();
  List<String> tokens=Parser.parseInput(mapString);
  for (  String token : tokens) {
    int firstEquals=token.indexOf('=');
    if (firstEquals > 0) {
      String key=token.substring(0,firstEquals);
      String value=token.substring(firstEquals + 1,token.length());
      result.put(extractValue(key),extractValue(value));
    }
  }
  return result.build();
}","/** 
 * Parses a map in the format: ""key1=a key2=b ..""
 * @param mapString {@link String} representation of the map
 * @return a {@link Map} with {@link String} keys and {@link String} values
 */
public static Map<String,String> parseMap(String mapString,String description){
  if (mapString == null || mapString.isEmpty()) {
    return ImmutableMap.of();
  }
  ImmutableMap.Builder<String,String> result=ImmutableMap.builder();
  List<String> tokens=Parser.parseInput(mapString);
  for (  String token : tokens) {
    int firstEquals=token.indexOf('=');
    if (firstEquals > 0) {
      String key=token.substring(0,firstEquals);
      String value=token.substring(firstEquals + 1,token.length());
      result.put(extractValue(key),extractValue(value));
    }
 else {
      throw new IllegalArgumentException(description + ""String_Node_Str"");
    }
  }
  return result.build();
}"
5466,"/** 
 * Parses a map in the format: ""key1=a key2=b .."" where a, b and etc. are integers
 * @param mapString {@link String} representation of the map
 * @return a {@link Map} with {@link String} keys and {@link Integer} values
 */
public static Map<String,Integer> parseStringIntegerMap(String mapString){
  return Maps.transformValues(parseMap(mapString),new Function<String,Integer>(){
    @Override public Integer apply(    String input){
      return Integer.valueOf(input);
    }
  }
);
}","/** 
 * Parses a map in the format: ""key1=a key2=b .."" where a, b and etc. are integers
 * @param mapString {@link String} representation of the map
 * @return a {@link Map} with {@link String} keys and {@link Integer} values
 */
public static Map<String,Integer> parseStringIntegerMap(String mapString,String description){
  return Maps.transformValues(parseMap(mapString,description),new Function<String,Integer>(){
    @Override public Integer apply(    String input){
      return Integer.valueOf(input);
    }
  }
);
}"
5467,"@Test public void testParseMap(){
  String argValue=""String_Node_Str"";
  String mapString=""String_Node_Str"" + argValue + ""String_Node_Str"";
  Map<String,String> actual=ArgumentParser.parseMap(mapString);
  Assert.assertEquals(""String_Node_Str"",actual.get(""String_Node_Str""));
  Assert.assertEquals(argValue,actual.get(""String_Node_Str""));
}","@Test public void testParseMap(){
  String argValue=""String_Node_Str"";
  String mapString=""String_Node_Str"" + argValue + ""String_Node_Str"";
  Map<String,String> actual=ArgumentParser.parseMap(mapString,""String_Node_Str"");
  Assert.assertEquals(""String_Node_Str"",actual.get(""String_Node_Str""));
  Assert.assertEquals(argValue,actual.get(""String_Node_Str""));
}"
5468,"@Override public void stop(CoprocessorEnvironment e) throws IOException {
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
}","@Override public void stop(CoprocessorEnvironment e){
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
  if (compactionState != null) {
    compactionState.stop();
  }
}"
5469,"@Override public void stop(CoprocessorEnvironment e) throws IOException {
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
}","@Override public void stop(CoprocessorEnvironment e){
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
  if (compactionState != null) {
    compactionState.stop();
  }
}"
5470,"@Override public void stop(CoprocessorEnvironment e) throws IOException {
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
}","@Override public void stop(CoprocessorEnvironment e){
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
  if (compactionState != null) {
    compactionState.stop();
  }
}"
5471,"@Override public void stop(CoprocessorEnvironment e) throws IOException {
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
}","@Override public void stop(CoprocessorEnvironment e){
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
  if (compactionState != null) {
    compactionState.stop();
  }
}"
5472,"@Override public void stop(CoprocessorEnvironment e) throws IOException {
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
}","@Override public void stop(CoprocessorEnvironment e){
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
  if (compactionState != null) {
    compactionState.stop();
  }
}"
5473,"@Override public void stop(CoprocessorEnvironment e) throws IOException {
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
}","@Override public void stop(CoprocessorEnvironment e){
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
  if (compactionState != null) {
    compactionState.stop();
  }
}"
5474,"@Override public void start(CoprocessorEnvironment env) throws IOException {
  if (env instanceof RegionCoprocessorEnvironment) {
    HTableDescriptor tableDesc=((RegionCoprocessorEnvironment)env).getRegion().getTableDesc();
    metadataTableNamespace=tableDesc.getValue(Constants.MessagingSystem.HBASE_METADATA_TABLE_NAMESPACE);
    hbaseNamespacePrefix=tableDesc.getValue(Constants.Dataset.TABLE_PREFIX);
    prefixLength=Integer.valueOf(tableDesc.getValue(Constants.MessagingSystem.HBASE_MESSAGING_TABLE_PREFIX_NUM_BYTES));
    String sysConfigTablePrefix=HTableNameConverter.getSysConfigTablePrefix(hbaseNamespacePrefix);
    cConfReader=new CConfigurationReader(env.getConfiguration(),sysConfigTablePrefix);
    Supplier<TransactionStateCache> cacheSupplier=getTransactionStateCacheSupplier(hbaseNamespacePrefix,env.getConfiguration());
    txStateCache=cacheSupplier.get();
    topicMetadataCache=createTopicMetadataCache((RegionCoprocessorEnvironment)env);
  }
}","@Override public void start(CoprocessorEnvironment env){
  if (env instanceof RegionCoprocessorEnvironment) {
    HTableDescriptor tableDesc=((RegionCoprocessorEnvironment)env).getRegion().getTableDesc();
    metadataTableNamespace=tableDesc.getValue(Constants.MessagingSystem.HBASE_METADATA_TABLE_NAMESPACE);
    hbaseNamespacePrefix=tableDesc.getValue(Constants.Dataset.TABLE_PREFIX);
    prefixLength=Integer.valueOf(tableDesc.getValue(Constants.MessagingSystem.HBASE_MESSAGING_TABLE_PREFIX_NUM_BYTES));
    String sysConfigTablePrefix=HTableNameConverter.getSysConfigTablePrefix(hbaseNamespacePrefix);
    cConfReader=new CConfigurationReader(env.getConfiguration(),sysConfigTablePrefix);
    Supplier<TransactionStateCache> cacheSupplier=getTransactionStateCacheSupplier(hbaseNamespacePrefix,env.getConfiguration());
    txStateCache=cacheSupplier.get();
    topicMetadataCache=createTopicMetadataCache((RegionCoprocessorEnvironment)env);
  }
}"
5475,"@Override public void stop(CoprocessorEnvironment e) throws IOException {
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
}","@Override public void stop(CoprocessorEnvironment e) throws IOException {
  if (e instanceof RegionCoprocessorEnvironment) {
    getTopicMetadataCache((RegionCoprocessorEnvironment)e).stop();
  }
  if (compactionState != null) {
    compactionState.stop();
  }
}"
5476,"/** 
 * Returns a list of services associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllServices(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,lifecycleService.list(new NamespaceId(namespaceId),ProgramType.SERVICE));
}","/** 
 * Returns a list of services associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllServices(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,lifecycleService.list(validateAndGetNamespace(namespaceId),ProgramType.SERVICE));
}"
5477,"@GET @Path(""String_Node_Str"") public void getAllWorkers(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,lifecycleService.list(new NamespaceId(namespaceId),ProgramType.WORKER));
}","@GET @Path(""String_Node_Str"") public void getAllWorkers(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,lifecycleService.list(validateAndGetNamespace(namespaceId),ProgramType.WORKER));
}"
5478,"@Inject ProgramLifecycleHttpHandler(Store store,ProgramRuntimeService runtimeService,DiscoveryServiceClient discoveryServiceClient,ProgramLifecycleService lifecycleService,QueueAdmin queueAdmin,PreferencesStore preferencesStore,MRJobInfoFetcher mrJobInfoFetcher,MetricStore metricStore){
  this.store=store;
  this.runtimeService=runtimeService;
  this.discoveryServiceClient=discoveryServiceClient;
  this.lifecycleService=lifecycleService;
  this.metricStore=metricStore;
  this.queueAdmin=queueAdmin;
  this.preferencesStore=preferencesStore;
  this.mrJobInfoFetcher=mrJobInfoFetcher;
}","@Inject ProgramLifecycleHttpHandler(Store store,ProgramRuntimeService runtimeService,DiscoveryServiceClient discoveryServiceClient,ProgramLifecycleService lifecycleService,QueueAdmin queueAdmin,PreferencesStore preferencesStore,MRJobInfoFetcher mrJobInfoFetcher,MetricStore metricStore,NamespaceQueryAdmin namespaceQueryAdmin){
  this.store=store;
  this.runtimeService=runtimeService;
  this.discoveryServiceClient=discoveryServiceClient;
  this.lifecycleService=lifecycleService;
  this.metricStore=metricStore;
  this.queueAdmin=queueAdmin;
  this.preferencesStore=preferencesStore;
  this.mrJobInfoFetcher=mrJobInfoFetcher;
  this.namespaceQueryAdmin=namespaceQueryAdmin;
}"
5479,"@DELETE @Path(""String_Node_Str"") public synchronized void deleteQueues(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  NamespaceId namespace=new NamespaceId(namespaceId);
  try {
    List<ProgramRecord> flows=lifecycleService.list(new NamespaceId(namespaceId),ProgramType.FLOW);
    for (    ProgramRecord flow : flows) {
      String appId=flow.getApp();
      String flowId=flow.getName();
      ProgramId programId=new ProgramId(namespaceId,appId,ProgramType.FLOW,flowId);
      ProgramStatus status=lifecycleService.getProgramStatus(programId);
      if (ProgramStatus.STOPPED != status) {
        responder.sendString(HttpResponseStatus.FORBIDDEN,String.format(""String_Node_Str"" + ""String_Node_Str"",flowId,appId,namespaceId));
        return;
      }
    }
    queueAdmin.dropAllInNamespace(namespace);
    FlowUtils.deleteFlowPendingMetrics(metricStore,namespaceId,null,null);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"" + namespace,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}","@DELETE @Path(""String_Node_Str"") public synchronized void deleteQueues(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws NamespaceNotFoundException {
  NamespaceId namespace=validateAndGetNamespace(namespaceId);
  try {
    List<ProgramRecord> flows=lifecycleService.list(validateAndGetNamespace(namespaceId),ProgramType.FLOW);
    for (    ProgramRecord flow : flows) {
      String appId=flow.getApp();
      String flowId=flow.getName();
      ProgramId programId=new ProgramId(namespaceId,appId,ProgramType.FLOW,flowId);
      ProgramStatus status=lifecycleService.getProgramStatus(programId);
      if (ProgramStatus.STOPPED != status) {
        responder.sendString(HttpResponseStatus.FORBIDDEN,String.format(""String_Node_Str"" + ""String_Node_Str"",flowId,appId,namespaceId));
        return;
      }
    }
    queueAdmin.dropAllInNamespace(namespace);
    FlowUtils.deleteFlowPendingMetrics(metricStore,namespaceId,null,null);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"" + namespace,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}"
5480,"/** 
 * Returns a list of workflows associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllWorkflows(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,lifecycleService.list(new NamespaceId(namespaceId),ProgramType.WORKFLOW));
}","/** 
 * Returns a list of workflows associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllWorkflows(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,lifecycleService.list(validateAndGetNamespace(namespaceId),ProgramType.WORKFLOW));
}"
5481,"/** 
 * Returns number of instances of a worker.
 */
@GET @Path(""String_Node_Str"") public void getWorkerInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workerId){
  try {
    int count=store.getWorkerInstances(new NamespaceId(namespaceId).app(appId).worker(workerId));
    responder.sendJson(HttpResponseStatus.OK,new Instances(count));
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    throw e;
  }
}","/** 
 * Returns number of instances of a worker.
 */
@GET @Path(""String_Node_Str"") public void getWorkerInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workerId) throws Exception {
  try {
    int count=store.getWorkerInstances(validateAndGetNamespace(namespaceId).app(appId).worker(workerId));
    responder.sendJson(HttpResponseStatus.OK,new Instances(count));
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    throw e;
  }
}"
5482,"/** 
 * Returns a list of spark jobs associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllSpark(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,lifecycleService.list(new NamespaceId(namespaceId),ProgramType.SPARK));
}","/** 
 * Returns a list of spark jobs associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllSpark(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,lifecycleService.list(validateAndGetNamespace(namespaceId),ProgramType.SPARK));
}"
5483,"/** 
 * Returns a list of map/reduces associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllMapReduce(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,lifecycleService.list(new NamespaceId(namespaceId),ProgramType.MAPREDUCE));
}","/** 
 * Returns a list of map/reduces associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllMapReduce(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,lifecycleService.list(validateAndGetNamespace(namespaceId),ProgramType.MAPREDUCE));
}"
5484,"/** 
 * Returns a list of flows associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllFlows(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,lifecycleService.list(new NamespaceId(namespaceId),ProgramType.FLOW));
}","/** 
 * Returns a list of flows associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllFlows(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,lifecycleService.list(validateAndGetNamespace(namespaceId),ProgramType.FLOW));
}"
5485,"@Inject WorkflowHttpHandler(Store store,WorkflowClient workflowClient,ProgramRuntimeService runtimeService,QueueAdmin queueAdmin,Scheduler scheduler,PreferencesStore preferencesStore,MRJobInfoFetcher mrJobInfoFetcher,ProgramLifecycleService lifecycleService,MetricStore metricStore,DatasetFramework datasetFramework,DiscoveryServiceClient discoveryServiceClient){
  super(store,runtimeService,discoveryServiceClient,lifecycleService,queueAdmin,preferencesStore,mrJobInfoFetcher,metricStore);
  this.workflowClient=workflowClient;
  this.datasetFramework=datasetFramework;
  this.scheduler=scheduler;
}","@Inject WorkflowHttpHandler(Store store,WorkflowClient workflowClient,ProgramRuntimeService runtimeService,QueueAdmin queueAdmin,Scheduler scheduler,PreferencesStore preferencesStore,MRJobInfoFetcher mrJobInfoFetcher,ProgramLifecycleService lifecycleService,MetricStore metricStore,NamespaceQueryAdmin namespaceQueryAdmin,DatasetFramework datasetFramework,DiscoveryServiceClient discoveryServiceClient){
  super(store,runtimeService,discoveryServiceClient,lifecycleService,queueAdmin,preferencesStore,mrJobInfoFetcher,metricStore,namespaceQueryAdmin);
  this.workflowClient=workflowClient;
  this.datasetFramework=datasetFramework;
  this.scheduler=scheduler;
}"
5486,"public Instances getWorkerInstances(String namespaceId,String appId,String workerId){
  MockResponder responder=new MockResponder();
  String uri=String.format(""String_Node_Str"",getNamespacePath(namespaceId),appId,workerId);
  HttpRequest request=new DefaultHttpRequest(HttpVersion.HTTP_1_1,HttpMethod.GET,uri);
  programLifecycleHttpHandler.getWorkerInstances(request,responder,namespaceId,appId,workerId);
  verifyResponse(HttpResponseStatus.OK,responder.getStatus(),""String_Node_Str"");
  return responder.decodeResponseContent(Instances.class);
}","public Instances getWorkerInstances(String namespaceId,String appId,String workerId) throws Exception {
  MockResponder responder=new MockResponder();
  String uri=String.format(""String_Node_Str"",getNamespacePath(namespaceId),appId,workerId);
  HttpRequest request=new DefaultHttpRequest(HttpVersion.HTTP_1_1,HttpMethod.GET,uri);
  programLifecycleHttpHandler.getWorkerInstances(request,responder,namespaceId,appId,workerId);
  verifyResponse(HttpResponseStatus.OK,responder.getStatus(),""String_Node_Str"");
  return responder.decodeResponseContent(Instances.class);
}"
5487,"@Override public int getInstances(){
  return appFabricClient.getWorkerInstances(programId.getNamespace(),programId.getApplication(),programId.getProgram()).getInstances();
}","@Override public int getInstances(){
  try {
    return appFabricClient.getWorkerInstances(programId.getNamespace(),programId.getApplication(),programId.getProgram()).getInstances();
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
5488,"/** 
 * Inspect the given artifact to determine the classes contained in the artifact.
 * @param artifactId the id of the artifact to inspect
 * @param artifactFile the artifact file
 * @param parentClassLoader the parent classloader to use when inspecting plugins contained in the artifact.For example, a ProgramClassLoader created from the artifact the input artifact extends
 * @return metadata about the classes contained in the artifact
 * @throws IOException if there was an exception opening the jar file
 * @throws InvalidArtifactException if the artifact is invalid. For example, if the application main class is notactually an Application.
 */
ArtifactClasses inspectArtifact(Id.Artifact artifactId,File artifactFile,@Nullable ClassLoader parentClassLoader) throws IOException, InvalidArtifactException {
  Path tmpDir=Paths.get(cConf.get(Constants.CFG_LOCAL_DATA_DIR),cConf.get(Constants.AppFabric.TEMP_DIR)).toAbsolutePath();
  Files.createDirectories(tmpDir);
  Location artifactLocation=Locations.toLocation(artifactFile);
  Path stageDir=Files.createTempDirectory(tmpDir,artifactFile.getName());
  try {
    File unpackedDir=BundleJarUtil.unJar(artifactLocation,Files.createTempDirectory(stageDir,""String_Node_Str"").toFile());
    try (CloseableClassLoader artifactClassLoader=artifactClassLoaderFactory.createClassLoader(unpackedDir)){
      ArtifactClasses.Builder builder=inspectApplications(artifactId,ArtifactClasses.builder(),artifactLocation,artifactClassLoader);
      try (PluginInstantiator pluginInstantiator=new PluginInstantiator(cConf,parentClassLoader == null ? artifactClassLoader : parentClassLoader,Files.createTempDirectory(stageDir,""String_Node_Str"").toFile())){
        pluginInstantiator.addArtifact(artifactLocation,artifactId.toArtifactId());
        inspectPlugins(builder,artifactFile,artifactId.toArtifactId(),pluginInstantiator);
      }
       return builder.build();
    }
   }
  finally {
    try {
      DirUtils.deleteDirectoryContents(stageDir.toFile());
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",stageDir,e);
    }
  }
}","/** 
 * Inspect the given artifact to determine the classes contained in the artifact.
 * @param artifactId the id of the artifact to inspect
 * @param artifactFile the artifact file
 * @param parentClassLoader the parent classloader to use when inspecting plugins contained in the artifact.For example, a ProgramClassLoader created from the artifact the input artifact extends
 * @return metadata about the classes contained in the artifact
 * @throws IOException if there was an exception opening the jar file
 * @throws InvalidArtifactException if the artifact is invalid. For example, if the application main class is notactually an Application.
 */
ArtifactClasses inspectArtifact(Id.Artifact artifactId,File artifactFile,@Nullable ClassLoader parentClassLoader) throws IOException, InvalidArtifactException {
  Path tmpDir=Paths.get(cConf.get(Constants.CFG_LOCAL_DATA_DIR),cConf.get(Constants.AppFabric.TEMP_DIR)).toAbsolutePath();
  Files.createDirectories(tmpDir);
  Location artifactLocation=Locations.toLocation(artifactFile);
  Path stageDir=Files.createTempDirectory(tmpDir,artifactFile.getName());
  try {
    File unpackedDir=BundleJarUtil.unJar(artifactLocation,Files.createTempDirectory(stageDir,""String_Node_Str"").toFile());
    try (CloseableClassLoader artifactClassLoader=artifactClassLoaderFactory.createClassLoader(unpackedDir)){
      ArtifactClasses.Builder builder=inspectApplications(artifactId,ArtifactClasses.builder(),artifactLocation,artifactClassLoader);
      try (PluginInstantiator pluginInstantiator=new PluginInstantiator(cConf,parentClassLoader == null ? artifactClassLoader : parentClassLoader,Files.createTempDirectory(stageDir,""String_Node_Str"").toFile())){
        pluginInstantiator.addArtifact(artifactLocation,artifactId.toArtifactId());
        inspectPlugins(builder,artifactFile,artifactId.toArtifactId(),pluginInstantiator);
      }
       return builder.build();
    }
   }
 catch (  EOFException|ZipException e) {
    throw new InvalidArtifactException(""String_Node_Str"" + artifactId + ""String_Node_Str"",e);
  }
 finally {
    try {
      DirUtils.deleteDirectoryContents(stageDir.toFile());
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",stageDir,e);
    }
  }
}"
5489,"@Test public void testHBaseVersionToCompatMapping() throws ParseException {
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_96,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH55,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH56,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
}","@Test public void testHBaseVersionToCompatMapping() throws ParseException {
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_96,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_98,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH55,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_10_CDH56,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_11,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
  assertCompatModuleMapping(HBaseVersion.Version.HBASE_12_CDH57,""String_Node_Str"");
}"
5490,"@VisibleForTesting static Version determineVersionFromVersionString(String versionString) throws ParseException {
  if (versionString.startsWith(HBASE_94_VERSION)) {
    return Version.HBASE_94;
  }
 else   if (versionString.startsWith(HBASE_96_VERSION)) {
    return Version.HBASE_96;
  }
 else   if (versionString.startsWith(HBASE_98_VERSION)) {
    return Version.HBASE_98;
  }
 else   if (versionString.startsWith(HBASE_10_VERSION)) {
    VersionNumber ver=VersionNumber.create(versionString);
    if (ver.getClassifier() != null && ver.getClassifier().startsWith(CDH55_CLASSIFIER)) {
      return Version.HBASE_10_CDH55;
    }
 else     if (ver.getClassifier() != null && ver.getClassifier().startsWith(CDH56_CLASSIFIER)) {
      return Version.HBASE_10_CDH56;
    }
 else     if (ver.getClassifier() != null && ver.getClassifier().startsWith(CDH_CLASSIFIER)) {
      return Version.HBASE_10_CDH;
    }
 else {
      return Version.HBASE_10;
    }
  }
 else   if (versionString.startsWith(HBASE_11_VERSION)) {
    return Version.HBASE_11;
  }
 else   if (versionString.startsWith(HBASE_12_VERSION)) {
    VersionNumber ver=VersionNumber.create(versionString);
    if (ver.getClassifier() != null && (ver.getClassifier().startsWith(CDH57_CLASSIFIER) || ver.getClassifier().startsWith(CDH58_CLASSIFIER) || ver.getClassifier().startsWith(CDH59_CLASSIFIER))) {
      return Version.HBASE_12_CDH57;
    }
 else {
      return Version.HBASE_11;
    }
  }
 else {
    return Version.UNKNOWN;
  }
}","@VisibleForTesting static Version determineVersionFromVersionString(String versionString) throws ParseException {
  if (versionString.startsWith(HBASE_94_VERSION)) {
    return Version.HBASE_94;
  }
 else   if (versionString.startsWith(HBASE_96_VERSION)) {
    return Version.HBASE_96;
  }
 else   if (versionString.startsWith(HBASE_98_VERSION)) {
    return Version.HBASE_98;
  }
 else   if (versionString.startsWith(HBASE_10_VERSION)) {
    VersionNumber ver=VersionNumber.create(versionString);
    if (ver.getClassifier() != null && ver.getClassifier().startsWith(CDH55_CLASSIFIER)) {
      return Version.HBASE_10_CDH55;
    }
 else     if (ver.getClassifier() != null && ver.getClassifier().startsWith(CDH56_CLASSIFIER)) {
      return Version.HBASE_10_CDH56;
    }
 else     if (ver.getClassifier() != null && ver.getClassifier().startsWith(CDH_CLASSIFIER)) {
      return Version.HBASE_10_CDH;
    }
 else {
      return Version.HBASE_10;
    }
  }
 else   if (versionString.startsWith(HBASE_11_VERSION)) {
    return Version.HBASE_11;
  }
 else   if (versionString.startsWith(HBASE_12_VERSION)) {
    VersionNumber ver=VersionNumber.create(versionString);
    if (ver.getClassifier() != null && (ver.getClassifier().startsWith(CDH57_CLASSIFIER) || ver.getClassifier().startsWith(CDH58_CLASSIFIER) || ver.getClassifier().startsWith(CDH59_CLASSIFIER)|| ver.getClassifier().startsWith(CDH510_CLASSIFIER))) {
      return Version.HBASE_12_CDH57;
    }
 else {
      return Version.HBASE_11;
    }
  }
 else {
    return Version.UNKNOWN;
  }
}"
5491,"private void testCreateAddAlterDrop(@Nullable String dbName,@Nullable String tableName) throws Exception {
  DatasetId datasetInstanceId=NAMESPACE_ID.dataset(""String_Node_Str"");
  String hiveTableName=getDatasetHiveName(datasetInstanceId);
  String showTablesCommand=""String_Node_Str"";
  FileSetProperties.Builder props=FileSetProperties.builder().setBasePath(""String_Node_Str"").setEnableExploreOnCreate(true).setSerDe(""String_Node_Str"").setExploreInputFormat(""String_Node_Str"").setExploreOutputFormat(""String_Node_Str"").setTableProperty(""String_Node_Str"",SCHEMA.toString());
  if (tableName != null) {
    props.setExploreTableName(tableName);
    hiveTableName=tableName;
  }
  String queryTableName=hiveTableName;
  if (dbName != null) {
    props.setExploreDatabaseName(dbName);
    runCommand(NAMESPACE_ID,""String_Node_Str"" + dbName,false,null,null);
    showTablesCommand+=""String_Node_Str"" + dbName;
    queryTableName=dbName + ""String_Node_Str"" + queryTableName;
  }
  datasetFramework.addInstance(""String_Node_Str"",datasetInstanceId,props.build());
  runCommand(NAMESPACE_ID,showTablesCommand,true,null,Lists.newArrayList(new QueryResult(Lists.<Object>newArrayList(hiveTableName))));
  FileSet fileSet=datasetFramework.getDataset(datasetInstanceId,DatasetDefinition.NO_ARGUMENTS,null);
  Assert.assertNotNull(fileSet);
  FileWriterHelper.generateAvroFile(fileSet.getLocation(""String_Node_Str"").getOutputStream(),""String_Node_Str"",0,3);
  runCommand(NAMESPACE_ID,""String_Node_Str"" + queryTableName,true,Lists.newArrayList(new ColumnDesc(hiveTableName + ""String_Node_Str"",""String_Node_Str"",1,null),new ColumnDesc(hiveTableName + ""String_Node_Str"",""String_Node_Str"",2,null)),Lists.newArrayList(new QueryResult(Lists.<Object>newArrayList(""String_Node_Str"",""String_Node_Str"")),new QueryResult(Lists.<Object>newArrayList(""String_Node_Str"",""String_Node_Str"")),new QueryResult(Lists.<Object>newArrayList(""String_Node_Str"",""String_Node_Str""))));
  FileWriterHelper.generateAvroFile(fileSet.getLocation(""String_Node_Str"").getOutputStream(),""String_Node_Str"",3,5);
  runCommand(NAMESPACE_ID,""String_Node_Str"" + queryTableName,true,Lists.newArrayList(new ColumnDesc(""String_Node_Str"",""String_Node_Str"",1,null)),Lists.newArrayList(new QueryResult(Lists.<Object>newArrayList(5L))));
  datasetFramework.updateInstance(datasetInstanceId,props.setEnableExploreOnCreate(false).build());
  runCommand(NAMESPACE_ID,showTablesCommand,false,null,Collections.<QueryResult>emptyList());
  datasetFramework.updateInstance(datasetInstanceId,props.setEnableExploreOnCreate(true).build());
  runCommand(NAMESPACE_ID,""String_Node_Str"" + queryTableName,true,Lists.newArrayList(new ColumnDesc(""String_Node_Str"",""String_Node_Str"",1,null)),Lists.newArrayList(new QueryResult(Lists.<Object>newArrayList(5L))));
  datasetFramework.updateInstance(datasetInstanceId,props.setTableProperty(""String_Node_Str"",K_SCHEMA.toString()).build());
  runCommand(NAMESPACE_ID,""String_Node_Str"" + queryTableName,true,Lists.newArrayList(new ColumnDesc(hiveTableName + ""String_Node_Str"",""String_Node_Str"",1,null)),Lists.newArrayList(new QueryResult(Lists.<Object>newArrayList(""String_Node_Str"")),new QueryResult(Lists.<Object>newArrayList(""String_Node_Str"")),new QueryResult(Lists.<Object>newArrayList(""String_Node_Str"")),new QueryResult(Lists.<Object>newArrayList(""String_Node_Str"")),new QueryResult(Lists.<Object>newArrayList(""String_Node_Str""))));
  datasetFramework.deleteInstance(datasetInstanceId);
  runCommand(NAMESPACE_ID,showTablesCommand,false,null,Collections.<QueryResult>emptyList());
  if (dbName != null) {
    runCommand(NAMESPACE_ID,""String_Node_Str"" + dbName,false,null,null);
  }
}","private void testCreateAddAlterDrop(@Nullable String dbName,@Nullable String tableName) throws Exception {
  DatasetId datasetInstanceId=NAMESPACE_ID.dataset(""String_Node_Str"");
  String hiveTableName=getDatasetHiveName(datasetInstanceId);
  String showTablesCommand=""String_Node_Str"";
  FileSetProperties.Builder props=FileSetProperties.builder().setBasePath(""String_Node_Str"").setEnableExploreOnCreate(true).setSerDe(""String_Node_Str"").setExploreInputFormat(""String_Node_Str"").setExploreOutputFormat(""String_Node_Str"").setTableProperty(""String_Node_Str"",SCHEMA.toString());
  if (tableName != null) {
    props.setExploreTableName(tableName);
    hiveTableName=tableName;
  }
  String queryTableName=hiveTableName;
  if (dbName != null) {
    props.setExploreDatabaseName(dbName);
    runCommand(NAMESPACE_ID,""String_Node_Str"" + dbName,false,null,null);
    showTablesCommand+=""String_Node_Str"" + dbName;
    queryTableName=dbName + ""String_Node_Str"" + queryTableName;
  }
  datasetFramework.addInstance(""String_Node_Str"",datasetInstanceId,props.build());
  runCommand(NAMESPACE_ID,showTablesCommand,true,null,Lists.newArrayList(new QueryResult(Lists.<Object>newArrayList(hiveTableName))));
  FileSet fileSet=datasetFramework.getDataset(datasetInstanceId,DatasetDefinition.NO_ARGUMENTS,null);
  Assert.assertNotNull(fileSet);
  FileWriterHelper.generateAvroFile(fileSet.getLocation(""String_Node_Str"").getOutputStream(),""String_Node_Str"",0,3);
  runCommand(NAMESPACE_ID,""String_Node_Str"" + queryTableName,true,Lists.newArrayList(new ColumnDesc(hiveTableName + ""String_Node_Str"",""String_Node_Str"",1,null),new ColumnDesc(hiveTableName + ""String_Node_Str"",""String_Node_Str"",2,null)),Lists.newArrayList(new QueryResult(Lists.<Object>newArrayList(""String_Node_Str"",""String_Node_Str"")),new QueryResult(Lists.<Object>newArrayList(""String_Node_Str"",""String_Node_Str"")),new QueryResult(Lists.<Object>newArrayList(""String_Node_Str"",""String_Node_Str""))));
  FileWriterHelper.generateAvroFile(fileSet.getLocation(""String_Node_Str"").getOutputStream(),""String_Node_Str"",3,5);
  runCommand(NAMESPACE_ID,""String_Node_Str"" + queryTableName,true,Lists.newArrayList(new ColumnDesc(""String_Node_Str"",""String_Node_Str"",1,null)),Lists.newArrayList(new QueryResult(Lists.<Object>newArrayList(5L))));
  datasetFramework.updateInstance(datasetInstanceId,props.setEnableExploreOnCreate(false).build());
  runCommand(NAMESPACE_ID,showTablesCommand,false,null,Collections.<QueryResult>emptyList());
  datasetFramework.updateInstance(datasetInstanceId,props.setEnableExploreOnCreate(true).build());
  runCommand(NAMESPACE_ID,""String_Node_Str"" + queryTableName,true,Lists.newArrayList(new ColumnDesc(""String_Node_Str"",""String_Node_Str"",1,null)),Lists.newArrayList(new QueryResult(Lists.<Object>newArrayList(5L))));
  datasetFramework.updateInstance(datasetInstanceId,props.setTableProperty(""String_Node_Str"",K_SCHEMA.toString()).build());
  runCommand(NAMESPACE_ID,""String_Node_Str"" + queryTableName + ""String_Node_Str"",true,Lists.newArrayList(new ColumnDesc(hiveTableName + ""String_Node_Str"",""String_Node_Str"",1,null)),Lists.newArrayList(new QueryResult(Lists.<Object>newArrayList(""String_Node_Str"")),new QueryResult(Lists.<Object>newArrayList(""String_Node_Str"")),new QueryResult(Lists.<Object>newArrayList(""String_Node_Str"")),new QueryResult(Lists.<Object>newArrayList(""String_Node_Str"")),new QueryResult(Lists.<Object>newArrayList(""String_Node_Str""))));
  datasetFramework.deleteInstance(datasetInstanceId);
  runCommand(NAMESPACE_ID,showTablesCommand,false,null,Collections.<QueryResult>emptyList());
  if (dbName != null) {
    runCommand(NAMESPACE_ID,""String_Node_Str"" + dbName,false,null,null);
  }
}"
5492,"/** 
 * Given a locationPath which is derived from URI.getPath() and a locationFactory this method generates a Location with URI generated from the given Absolute path
 * @param locationFactory locationFactory to create Location from given Path
 * @param absolutePath Path to be used for Location
 * @return Location resulting from absolute locationPath
 */
public static Location getLocationFromAbsolutePath(LocationFactory locationFactory,String absolutePath){
  URI basePathURI=URI.create(locationFactory.create(""String_Node_Str"").toURI().getPath());
  URI locationURI=URI.create(absolutePath);
  URI relativePathURI=basePathURI.relativize(locationURI);
  return locationFactory.create(relativePathURI);
}","/** 
 * Get a Location using a specified location factory and absolute path
 * @param locationFactory locationFactory to create Location from given Path
 * @param absolutePath Path to be used for Location
 * @return Location resulting from absolute locationPath
 */
public static Location getLocationFromAbsolutePath(LocationFactory locationFactory,String absolutePath){
  URI homeURI=locationFactory.getHomeLocation().toURI();
  try {
    return locationFactory.create(new URI(homeURI.getScheme(),homeURI.getAuthority(),absolutePath,null,null));
  }
 catch (  URISyntaxException e) {
    throw Throwables.propagate(e);
  }
}"
5493,"/** 
 * Given a locationPath which is derived from URI.getPath() and a locationFactory this method generates a Location with URI generated from the given Absolute path
 * @param locationFactory locationFactory to create Location from given Path
 * @param absolutePath Path to be used for Location
 * @return Location resulting from absolute locationPath
 */
public static Location getLocationFromAbsolutePath(LocationFactory locationFactory,String absolutePath){
  URI basePathURI=URI.create(locationFactory.create(""String_Node_Str"").toURI().getPath());
  URI locationURI=URI.create(absolutePath);
  URI relativePathURI=basePathURI.relativize(locationURI);
  return locationFactory.create(relativePathURI);
}","/** 
 * Get a Location using a specified location factory and absolute path
 * @param locationFactory locationFactory to create Location from given Path
 * @param absolutePath Path to be used for Location
 * @return Location resulting from absolute locationPath
 */
public static Location getLocationFromAbsolutePath(LocationFactory locationFactory,String absolutePath){
  URI homeURI=locationFactory.getHomeLocation().toURI();
  try {
    return locationFactory.create(new URI(homeURI.getScheme(),homeURI.getAuthority(),absolutePath,null,null));
  }
 catch (  URISyntaxException e) {
    throw Throwables.propagate(e);
  }
}"
5494,"private void createLocation(NamespaceMeta namespaceMeta) throws IOException {
  boolean createdHome=false;
  Location namespaceHome;
  if (hasCustomLocation(namespaceMeta)) {
    namespaceHome=validateCustomLocation(namespaceMeta);
  }
 else {
    namespaceHome=namespacedLocationFactory.get(namespaceMeta);
    if (namespaceHome.exists()) {
      throw new FileAlreadyExistsException(namespaceHome.toString());
    }
    if (!namespaceHome.mkdirs()) {
      throw new IOException(String.format(""String_Node_Str"",namespaceHome,namespaceMeta.getNamespaceId()));
    }
    createdHome=true;
  }
  Location namespaceData=namespaceHome.append(Constants.Dataset.DEFAULT_DATA_DIR);
  String configuredGroupName=namespaceMeta.getConfig().getGroupName();
  boolean createdData=false;
  try {
    if (createdHome) {
      String groupName=configuredGroupName != null ? configuredGroupName : UserGroupInformation.getCurrentUser().getPrimaryGroupName();
      namespaceHome.setGroup(groupName);
    }
    if (!namespaceData.mkdirs()) {
      throw new IOException(String.format(""String_Node_Str"",namespaceData,namespaceMeta.getNamespaceId()));
    }
    createdData=true;
    String dataGroup=configuredGroupName != null ? configuredGroupName : namespaceHome.getGroup();
    namespaceData.setGroup(dataGroup);
    if (configuredGroupName != null) {
      String permissions=namespaceData.getPermissions();
      namespaceData.setPermissions(permissions.substring(0,3) + ""String_Node_Str"" + permissions.substring(6));
    }
  }
 catch (  Throwable t) {
    try {
      if (createdHome) {
        namespaceHome.delete(true);
      }
 else       if (createdData) {
        namespaceData.delete(true);
      }
    }
 catch (    Throwable t1) {
      LOG.warn(""String_Node_Str"",namespaceHome,namespaceMeta.getNamespaceId());
      t.addSuppressed(t1);
    }
    Throwables.propagateIfInstanceOf(t,IOException.class);
    throw Throwables.propagate(t);
  }
}","private void createLocation(NamespaceMeta namespaceMeta) throws IOException {
  boolean createdHome=false;
  Location namespaceHome;
  if (hasCustomLocation(namespaceMeta)) {
    namespaceHome=validateCustomLocation(namespaceMeta);
  }
 else {
    namespaceHome=namespacedLocationFactory.get(namespaceMeta);
    if (namespaceHome.exists()) {
      throw new FileAlreadyExistsException(namespaceHome.toString());
    }
    if (!namespaceHome.mkdirs()) {
      throw new IOException(String.format(""String_Node_Str"",namespaceHome,namespaceMeta.getNamespaceId()));
    }
    createdHome=true;
  }
  Location namespaceData=namespaceHome.append(Constants.Dataset.DEFAULT_DATA_DIR);
  String configuredGroupName=namespaceMeta.getConfig().getGroupName();
  boolean createdData=false;
  try {
    if (createdHome && SecurityUtil.isKerberosEnabled(cConf)) {
      String groupName=configuredGroupName != null ? configuredGroupName : UserGroupInformation.getCurrentUser().getPrimaryGroupName();
      namespaceHome.setGroup(groupName);
    }
    if (!namespaceData.mkdirs()) {
      throw new IOException(String.format(""String_Node_Str"",namespaceData,namespaceMeta.getNamespaceId()));
    }
    createdData=true;
    if (SecurityUtil.isKerberosEnabled(cConf)) {
      String dataGroup=configuredGroupName != null ? configuredGroupName : namespaceHome.getGroup();
      namespaceData.setGroup(dataGroup);
      if (configuredGroupName != null) {
        String permissions=namespaceData.getPermissions();
        namespaceData.setPermissions(permissions.substring(0,3) + ""String_Node_Str"" + permissions.substring(6));
      }
    }
  }
 catch (  Throwable t) {
    try {
      if (createdHome) {
        namespaceHome.delete(true);
      }
 else       if (createdData) {
        namespaceData.delete(true);
      }
    }
 catch (    Throwable t1) {
      LOG.warn(""String_Node_Str"",namespaceHome,namespaceMeta.getNamespaceId());
      t.addSuppressed(t1);
    }
    Throwables.propagateIfInstanceOf(t,IOException.class);
    throw Throwables.propagate(t);
  }
}"
5495,"protected static void initializeAndStartServices(CConfiguration cConf,@Nullable SConfiguration sConf) throws Exception {
  injector=Guice.createInjector(new AppFabricTestModule(cConf,sConf));
  txManager=injector.getInstance(TransactionManager.class);
  txManager.startAndWait();
  dsOpService=injector.getInstance(DatasetOpExecutor.class);
  dsOpService.startAndWait();
  remoteSysOpService=injector.getInstance(RemoteSystemOperationsService.class);
  remoteSysOpService.startAndWait();
  datasetService=injector.getInstance(DatasetService.class);
  datasetService.startAndWait();
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  DiscoveryServiceClient discoveryClient=injector.getInstance(DiscoveryServiceClient.class);
  ServiceDiscovered appFabricHttpDiscovered=discoveryClient.discover(Constants.Service.APP_FABRIC_HTTP);
  EndpointStrategy endpointStrategy=new RandomEndpointStrategy(appFabricHttpDiscovered);
  port=endpointStrategy.pick(1,TimeUnit.SECONDS).getSocketAddress().getPort();
  txClient=injector.getInstance(TransactionSystemClient.class);
  metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  metricsCollectionService.startAndWait();
  metricsService=injector.getInstance(MetricsQueryService.class);
  metricsService.startAndWait();
  streamService=injector.getInstance(StreamService.class);
  streamService.startAndWait();
  serviceStore=injector.getInstance(ServiceStore.class);
  serviceStore.startAndWait();
  metadataService=injector.getInstance(MetadataService.class);
  metadataService.startAndWait();
  locationFactory=getInjector().getInstance(LocationFactory.class);
  streamClient=new StreamClient(getClientConfig(discoveryClient,Constants.Service.STREAMS));
  streamViewClient=new StreamViewClient(getClientConfig(discoveryClient,Constants.Service.STREAMS));
  datasetClient=new DatasetClient(getClientConfig(discoveryClient,Constants.Service.DATASET_MANAGER));
  createNamespaces();
}","protected static void initializeAndStartServices(CConfiguration cConf,@Nullable SConfiguration sConf) throws Exception {
  injector=Guice.createInjector(Modules.override(new AppFabricTestModule(cConf,sConf)).with(new AbstractModule(){
    @Override protected void configure(){
      bind(UGIProvider.class).to(CurrentUGIProvider.class);
    }
  }
));
  txManager=injector.getInstance(TransactionManager.class);
  txManager.startAndWait();
  dsOpService=injector.getInstance(DatasetOpExecutor.class);
  dsOpService.startAndWait();
  remoteSysOpService=injector.getInstance(RemoteSystemOperationsService.class);
  remoteSysOpService.startAndWait();
  datasetService=injector.getInstance(DatasetService.class);
  datasetService.startAndWait();
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  DiscoveryServiceClient discoveryClient=injector.getInstance(DiscoveryServiceClient.class);
  ServiceDiscovered appFabricHttpDiscovered=discoveryClient.discover(Constants.Service.APP_FABRIC_HTTP);
  EndpointStrategy endpointStrategy=new RandomEndpointStrategy(appFabricHttpDiscovered);
  port=endpointStrategy.pick(1,TimeUnit.SECONDS).getSocketAddress().getPort();
  txClient=injector.getInstance(TransactionSystemClient.class);
  metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  metricsCollectionService.startAndWait();
  metricsService=injector.getInstance(MetricsQueryService.class);
  metricsService.startAndWait();
  streamService=injector.getInstance(StreamService.class);
  streamService.startAndWait();
  serviceStore=injector.getInstance(ServiceStore.class);
  serviceStore.startAndWait();
  metadataService=injector.getInstance(MetadataService.class);
  metadataService.startAndWait();
  locationFactory=getInjector().getInstance(LocationFactory.class);
  createNamespaces();
}"
5496,"ArtifactData(Location location,ArtifactMeta meta){
  this.locationURI=null;
  this.locationPath=Locations.getRelativePath(location.getLocationFactory(),location.toURI());
  this.meta=meta;
}","ArtifactData(Location location,ArtifactMeta meta){
  this.locationURI=null;
  this.locationPath=location.toURI().getPath();
  this.meta=meta;
}"
5497,"/** 
 * Get information about the given artifact.
 * @param artifactId the artifact to get
 * @return information about the artifact
 * @throws ArtifactNotFoundException if the given artifact does not exist
 * @throws IOException if there was an exception reading the artifact information from the metastore
 */
public ArtifactDetail getArtifact(final Id.Artifact artifactId) throws ArtifactNotFoundException, IOException {
  try {
    final ArtifactData artifactData=Transactions.execute(transactional,new TxCallable<ArtifactData>(){
      @Override public ArtifactData call(      DatasetContext context) throws Exception {
        ArtifactCell artifactCell=new ArtifactCell(artifactId);
        byte[] value=getMetaTable(context).get(artifactCell.rowkey,artifactCell.column);
        if (value == null) {
          throw new ArtifactNotFoundException(artifactId.toEntityId());
        }
        return GSON.fromJson(Bytes.toString(value),ArtifactData.class);
      }
    }
);
    Location artifactLocation=impersonator.doAs(artifactId.getNamespace().toEntityId(),new Callable<Location>(){
      @Override public Location call() throws Exception {
        return Locations.getCompatibleLocation(locationFactory,artifactData.locationPath,artifactData.locationURI);
      }
    }
);
    return new ArtifactDetail(new ArtifactDescriptor(artifactId.toArtifactId(),artifactLocation),artifactData.meta);
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,IOException.class,ArtifactNotFoundException.class);
  }
catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","/** 
 * Get information about the given artifact.
 * @param artifactId the artifact to get
 * @return information about the artifact
 * @throws ArtifactNotFoundException if the given artifact does not exist
 * @throws IOException if there was an exception reading the artifact information from the metastore
 */
public ArtifactDetail getArtifact(final Id.Artifact artifactId) throws ArtifactNotFoundException, IOException {
  try {
    final ArtifactData artifactData=Transactions.execute(transactional,new TxCallable<ArtifactData>(){
      @Override public ArtifactData call(      DatasetContext context) throws Exception {
        ArtifactCell artifactCell=new ArtifactCell(artifactId);
        byte[] value=getMetaTable(context).get(artifactCell.rowkey,artifactCell.column);
        if (value == null) {
          throw new ArtifactNotFoundException(artifactId.toEntityId());
        }
        return GSON.fromJson(Bytes.toString(value),ArtifactData.class);
      }
    }
);
    Location artifactLocation=impersonator.doAs(artifactId.getNamespace().toEntityId(),new Callable<Location>(){
      @Override public Location call() throws Exception {
        return Locations.getLocationFromAbsolutePath(locationFactory,artifactData.getLocationPath());
      }
    }
);
    return new ArtifactDetail(new ArtifactDescriptor(artifactId.toArtifactId(),artifactLocation),artifactData.meta);
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,IOException.class,ArtifactNotFoundException.class);
  }
catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
5498,"AppData(ApplicationClass appClass,Location artifactLocation){
  this.appClass=appClass;
  this.artifactLocationURI=null;
  this.artifactLocationPath=Locations.getRelativePath(artifactLocation.getLocationFactory(),artifactLocation.toURI());
}","AppData(ApplicationClass appClass,Location artifactLocation){
  this.appClass=appClass;
  this.artifactLocationURI=null;
  this.artifactLocationPath=artifactLocation.toURI().getPath();
}"
5499,"/** 
 * Get all plugin classes of the given type and name that extend the given parent artifact. Results are returned as a map from plugin artifact to plugins in that artifact.
 * @param parentArtifactId the id of the artifact to find plugins for
 * @param type the type of plugin to look for
 * @param name the name of the plugin to look for
 * @return an unmodifiable map of plugin artifact to plugin classes of the given type and name, accessible by thegiven artifact. The map will never be null, and will never be empty.
 * @throws PluginNotExistsException if no plugin with the given type and name exists in the namespace
 * @throws IOException if there was an exception reading metadata from the metastore
 */
public SortedMap<ArtifactDescriptor,PluginClass> getPluginClasses(final NamespaceId namespace,final Id.Artifact parentArtifactId,final String type,final String name) throws IOException, ArtifactNotFoundException, PluginNotExistsException {
  try {
    SortedMap<ArtifactDescriptor,PluginClass> result=Transactions.execute(transactional,new TxCallable<SortedMap<ArtifactDescriptor,PluginClass>>(){
      @Override public SortedMap<ArtifactDescriptor,PluginClass> call(      DatasetContext context) throws Exception {
        Table metaTable=getMetaTable(context);
        ArtifactCell parentCell=new ArtifactCell(parentArtifactId);
        byte[] parentDataBytes=metaTable.get(parentCell.rowkey,parentCell.column);
        if (parentDataBytes == null) {
          throw new ArtifactNotFoundException(parentArtifactId.toEntityId());
        }
        SortedMap<ArtifactDescriptor,PluginClass> plugins=new TreeMap<>();
        ArtifactData parentData=GSON.fromJson(Bytes.toString(parentDataBytes),ArtifactData.class);
        Set<PluginClass> parentPlugins=parentData.meta.getClasses().getPlugins();
        for (        PluginClass pluginClass : parentPlugins) {
          if (pluginClass.getName().equals(name) && pluginClass.getType().equals(type)) {
            ArtifactDescriptor parentDescriptor=new ArtifactDescriptor(parentArtifactId.toArtifactId(),Locations.getCompatibleLocation(locationFactory,parentData.locationPath,parentData.locationURI));
            plugins.put(parentDescriptor,pluginClass);
            break;
          }
        }
        PluginKey pluginKey=new PluginKey(parentArtifactId.getNamespace(),parentArtifactId.getName(),type,name);
        Row row=metaTable.get(pluginKey.getRowKey());
        if (!row.isEmpty()) {
          for (          Map.Entry<byte[],byte[]> column : row.getColumns().entrySet()) {
            ImmutablePair<ArtifactDescriptor,PluginClass> pluginEntry=getPluginEntry(namespace,parentArtifactId,column);
            if (pluginEntry != null) {
              plugins.put(pluginEntry.getFirst(),pluginEntry.getSecond());
            }
          }
        }
        return Collections.unmodifiableSortedMap(plugins);
      }
    }
);
    if (result.isEmpty()) {
      throw new PluginNotExistsException(parentArtifactId.getNamespace(),type,name);
    }
    return result;
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,IOException.class,ArtifactNotFoundException.class);
  }
}","/** 
 * Get all plugin classes of the given type and name that extend the given parent artifact. Results are returned as a map from plugin artifact to plugins in that artifact.
 * @param parentArtifactId the id of the artifact to find plugins for
 * @param type the type of plugin to look for
 * @param name the name of the plugin to look for
 * @return an unmodifiable map of plugin artifact to plugin classes of the given type and name, accessible by thegiven artifact. The map will never be null, and will never be empty.
 * @throws PluginNotExistsException if no plugin with the given type and name exists in the namespace
 * @throws IOException if there was an exception reading metadata from the metastore
 */
public SortedMap<ArtifactDescriptor,PluginClass> getPluginClasses(final NamespaceId namespace,final Id.Artifact parentArtifactId,final String type,final String name) throws IOException, ArtifactNotFoundException, PluginNotExistsException {
  try {
    SortedMap<ArtifactDescriptor,PluginClass> result=Transactions.execute(transactional,new TxCallable<SortedMap<ArtifactDescriptor,PluginClass>>(){
      @Override public SortedMap<ArtifactDescriptor,PluginClass> call(      DatasetContext context) throws Exception {
        Table metaTable=getMetaTable(context);
        ArtifactCell parentCell=new ArtifactCell(parentArtifactId);
        byte[] parentDataBytes=metaTable.get(parentCell.rowkey,parentCell.column);
        if (parentDataBytes == null) {
          throw new ArtifactNotFoundException(parentArtifactId.toEntityId());
        }
        SortedMap<ArtifactDescriptor,PluginClass> plugins=new TreeMap<>();
        ArtifactData parentData=GSON.fromJson(Bytes.toString(parentDataBytes),ArtifactData.class);
        Set<PluginClass> parentPlugins=parentData.meta.getClasses().getPlugins();
        for (        PluginClass pluginClass : parentPlugins) {
          if (pluginClass.getName().equals(name) && pluginClass.getType().equals(type)) {
            ArtifactDescriptor parentDescriptor=new ArtifactDescriptor(parentArtifactId.toArtifactId(),Locations.getLocationFromAbsolutePath(locationFactory,parentData.getLocationPath()));
            plugins.put(parentDescriptor,pluginClass);
            break;
          }
        }
        PluginKey pluginKey=new PluginKey(parentArtifactId.getNamespace(),parentArtifactId.getName(),type,name);
        Row row=metaTable.get(pluginKey.getRowKey());
        if (!row.isEmpty()) {
          for (          Map.Entry<byte[],byte[]> column : row.getColumns().entrySet()) {
            ImmutablePair<ArtifactDescriptor,PluginClass> pluginEntry=getPluginEntry(namespace,parentArtifactId,column);
            if (pluginEntry != null) {
              plugins.put(pluginEntry.getFirst(),pluginEntry.getSecond());
            }
          }
        }
        return Collections.unmodifiableSortedMap(plugins);
      }
    }
);
    if (result.isEmpty()) {
      throw new PluginNotExistsException(parentArtifactId.getNamespace(),type,name);
    }
    return result;
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,IOException.class,ArtifactNotFoundException.class);
  }
}"
5500,"private void addArtifactsToList(List<ArtifactDetail> artifactDetails,Row row) throws IOException {
  ArtifactKey artifactKey=ArtifactKey.parse(row.getRow());
  for (  Map.Entry<byte[],byte[]> columnVal : row.getColumns().entrySet()) {
    String version=Bytes.toString(columnVal.getKey());
    ArtifactData data=GSON.fromJson(Bytes.toString(columnVal.getValue()),ArtifactData.class);
    Id.Artifact artifactId=Id.Artifact.from(artifactKey.namespace.toId(),artifactKey.name,version);
    artifactDetails.add(new ArtifactDetail(new ArtifactDescriptor(artifactId.toArtifactId(),Locations.getCompatibleLocation(locationFactory,data.locationPath,data.locationURI)),data.meta));
  }
}","private void addArtifactsToList(List<ArtifactDetail> artifactDetails,Row row) throws IOException {
  ArtifactKey artifactKey=ArtifactKey.parse(row.getRow());
  for (  Map.Entry<byte[],byte[]> columnVal : row.getColumns().entrySet()) {
    String version=Bytes.toString(columnVal.getKey());
    ArtifactData data=GSON.fromJson(Bytes.toString(columnVal.getValue()),ArtifactData.class);
    Id.Artifact artifactId=Id.Artifact.from(artifactKey.namespace.toId(),artifactKey.name,version);
    artifactDetails.add(new ArtifactDetail(new ArtifactDescriptor(artifactId.toArtifactId(),Locations.getLocationFromAbsolutePath(locationFactory,data.getLocationPath())),data.meta));
  }
}"
5501,"PluginData(PluginClass pluginClass,ArtifactRange usableBy,Location artifactLocation){
  this.pluginClass=pluginClass;
  this.usableBy=usableBy;
  this.artifactLocationURI=null;
  this.artifactLocationPath=Locations.getRelativePath(artifactLocation.getLocationFactory(),artifactLocation.toURI());
}","PluginData(PluginClass pluginClass,ArtifactRange usableBy,Location artifactLocation){
  this.pluginClass=pluginClass;
  this.usableBy=usableBy;
  this.artifactLocationURI=null;
  this.artifactLocationPath=artifactLocation.toURI().getPath();
}"
5502,"/** 
 * Update artifact properties using an update function. Functions will receive an immutable map.
 * @param artifactId the id of the artifact to add
 * @param updateFunction the function used to update existing properties
 * @throws ArtifactNotFoundException if the artifact does not exist
 * @throws IOException if there was an exception writing the properties to the metastore
 */
public void updateArtifactProperties(final Id.Artifact artifactId,final Function<Map<String,String>,Map<String,String>> updateFunction) throws ArtifactNotFoundException, IOException {
  try {
    transactional.execute(new TxRunnable(){
      @Override public void run(      DatasetContext context) throws Exception {
        ArtifactCell artifactCell=new ArtifactCell(artifactId);
        Table metaTable=getMetaTable(context);
        byte[] existingMetaBytes=metaTable.get(artifactCell.rowkey,artifactCell.column);
        if (existingMetaBytes == null) {
          throw new ArtifactNotFoundException(artifactId.toEntityId());
        }
        ArtifactData old=GSON.fromJson(Bytes.toString(existingMetaBytes),ArtifactData.class);
        ArtifactMeta updatedMeta=new ArtifactMeta(old.meta.getClasses(),old.meta.getUsableBy(),updateFunction.apply(old.meta.getProperties()));
        ArtifactData updatedData=new ArtifactData(Locations.getCompatibleLocation(locationFactory,old.locationPath,old.locationURI),updatedMeta);
        metaTable.put(artifactCell.rowkey,artifactCell.column,Bytes.toBytes(GSON.toJson(updatedData)));
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,ArtifactNotFoundException.class,IOException.class);
  }
}","/** 
 * Update artifact properties using an update function. Functions will receive an immutable map.
 * @param artifactId the id of the artifact to add
 * @param updateFunction the function used to update existing properties
 * @throws ArtifactNotFoundException if the artifact does not exist
 * @throws IOException if there was an exception writing the properties to the metastore
 */
public void updateArtifactProperties(final Id.Artifact artifactId,final Function<Map<String,String>,Map<String,String>> updateFunction) throws ArtifactNotFoundException, IOException {
  try {
    transactional.execute(new TxRunnable(){
      @Override public void run(      DatasetContext context) throws Exception {
        ArtifactCell artifactCell=new ArtifactCell(artifactId);
        Table metaTable=getMetaTable(context);
        byte[] existingMetaBytes=metaTable.get(artifactCell.rowkey,artifactCell.column);
        if (existingMetaBytes == null) {
          throw new ArtifactNotFoundException(artifactId.toEntityId());
        }
        ArtifactData old=GSON.fromJson(Bytes.toString(existingMetaBytes),ArtifactData.class);
        ArtifactMeta updatedMeta=new ArtifactMeta(old.meta.getClasses(),old.meta.getUsableBy(),updateFunction.apply(old.meta.getProperties()));
        ArtifactData updatedData=new ArtifactData(Locations.getLocationFromAbsolutePath(locationFactory,old.getLocationPath()),updatedMeta);
        metaTable.put(artifactCell.rowkey,artifactCell.column,Bytes.toBytes(GSON.toJson(updatedData)));
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,ArtifactNotFoundException.class,IOException.class);
  }
}"
5503,"/** 
 * Decode the PluginClass from the table column if it is from an artifact in the given namespace and extends the given parent artifact. If the plugin's artifact is not in the given namespace, or it does not extend the given parent artifact, return null.
 */
private ImmutablePair<ArtifactDescriptor,PluginClass> getPluginEntry(NamespaceId namespace,Id.Artifact parentArtifactId,Map.Entry<byte[],byte[]> column){
  ArtifactColumn artifactColumn=ArtifactColumn.parse(column.getKey());
  Id.Namespace artifactNamespace=artifactColumn.artifactId.getNamespace();
  if (!Id.Namespace.SYSTEM.equals(artifactNamespace) && !artifactNamespace.equals(namespace.toId())) {
    return null;
  }
  PluginData pluginData=GSON.fromJson(Bytes.toString(column.getValue()),PluginData.class);
  if (pluginData.usableBy.versionIsInRange(parentArtifactId.getVersion())) {
    ArtifactDescriptor artifactDescriptor=new ArtifactDescriptor(artifactColumn.artifactId.toArtifactId(),Locations.getCompatibleLocation(locationFactory,pluginData.artifactLocationPath,pluginData.artifactLocationURI));
    return ImmutablePair.of(artifactDescriptor,pluginData.pluginClass);
  }
  return null;
}","/** 
 * Decode the PluginClass from the table column if it is from an artifact in the given namespace and extends the given parent artifact. If the plugin's artifact is not in the given namespace, or it does not extend the given parent artifact, return null.
 */
private ImmutablePair<ArtifactDescriptor,PluginClass> getPluginEntry(NamespaceId namespace,Id.Artifact parentArtifactId,Map.Entry<byte[],byte[]> column){
  ArtifactColumn artifactColumn=ArtifactColumn.parse(column.getKey());
  Id.Namespace artifactNamespace=artifactColumn.artifactId.getNamespace();
  if (!Id.Namespace.SYSTEM.equals(artifactNamespace) && !artifactNamespace.equals(namespace.toId())) {
    return null;
  }
  PluginData pluginData=GSON.fromJson(Bytes.toString(column.getValue()),PluginData.class);
  if (pluginData.usableBy.versionIsInRange(parentArtifactId.getVersion())) {
    ArtifactDescriptor artifactDescriptor=new ArtifactDescriptor(artifactColumn.artifactId.toArtifactId(),Locations.getLocationFromAbsolutePath(locationFactory,pluginData.getArtifactLocationPath()));
    return ImmutablePair.of(artifactDescriptor,pluginData.pluginClass);
  }
  return null;
}"
5504,"private void writeMeta(Table table,Id.Artifact artifactId,ArtifactData data) throws IOException {
  ArtifactCell artifactCell=new ArtifactCell(artifactId);
  table.put(artifactCell.rowkey,artifactCell.column,Bytes.toBytes(GSON.toJson(data)));
  byte[] artifactColumn=new ArtifactColumn(artifactId).getColumn();
  ArtifactClasses classes=data.meta.getClasses();
  Location artifactLocation=Locations.getCompatibleLocation(locationFactory,data.locationPath,data.locationURI);
  for (  PluginClass pluginClass : classes.getPlugins()) {
    for (    ArtifactRange artifactRange : data.meta.getUsableBy()) {
      PluginKey pluginKey=new PluginKey(artifactRange.getNamespace().toId(),artifactRange.getName(),pluginClass.getType(),pluginClass.getName());
      byte[] pluginDataBytes=Bytes.toBytes(GSON.toJson(new PluginData(pluginClass,artifactRange,artifactLocation)));
      table.put(pluginKey.getRowKey(),artifactColumn,pluginDataBytes);
    }
  }
  for (  ApplicationClass appClass : classes.getApps()) {
    AppClassKey appClassKey=new AppClassKey(artifactId.getNamespace().toEntityId(),appClass.getClassName());
    byte[] appDataBytes=Bytes.toBytes(GSON.toJson(new AppData(appClass,artifactLocation)));
    table.put(appClassKey.getRowKey(),artifactColumn,appDataBytes);
  }
}","private void writeMeta(Table table,Id.Artifact artifactId,ArtifactData data) throws IOException {
  ArtifactCell artifactCell=new ArtifactCell(artifactId);
  table.put(artifactCell.rowkey,artifactCell.column,Bytes.toBytes(GSON.toJson(data)));
  byte[] artifactColumn=new ArtifactColumn(artifactId).getColumn();
  ArtifactClasses classes=data.meta.getClasses();
  Location artifactLocation=Locations.getLocationFromAbsolutePath(locationFactory,data.getLocationPath());
  for (  PluginClass pluginClass : classes.getPlugins()) {
    for (    ArtifactRange artifactRange : data.meta.getUsableBy()) {
      PluginKey pluginKey=new PluginKey(artifactRange.getNamespace().toId(),artifactRange.getName(),pluginClass.getType(),pluginClass.getName());
      byte[] pluginDataBytes=Bytes.toBytes(GSON.toJson(new PluginData(pluginClass,artifactRange,artifactLocation)));
      table.put(pluginKey.getRowKey(),artifactColumn,pluginDataBytes);
    }
  }
  for (  ApplicationClass appClass : classes.getApps()) {
    AppClassKey appClassKey=new AppClassKey(artifactId.getNamespace().toEntityId(),appClass.getClassName());
    byte[] appDataBytes=Bytes.toBytes(GSON.toJson(new AppData(appClass,artifactLocation)));
    table.put(appClassKey.getRowKey(),artifactColumn,appDataBytes);
  }
}"
5505,"@Override public Void call() throws Exception {
  Locations.getCompatibleLocation(locationFactory,oldMeta.locationPath,oldMeta.locationURI).delete();
  return null;
}","@Override public Void call() throws Exception {
  Locations.getLocationFromAbsolutePath(locationFactory,oldMeta.getLocationPath()).delete();
  return null;
}"
5506,"private SortedMap<ArtifactDescriptor,Set<PluginClass>> getPluginsInArtifact(Table table,Id.Artifact artifactId,Predicate<PluginClass> filter){
  SortedMap<ArtifactDescriptor,Set<PluginClass>> result=new TreeMap<>();
  ArtifactCell parentCell=new ArtifactCell(artifactId);
  byte[] parentDataBytes=table.get(parentCell.rowkey,parentCell.column);
  if (parentDataBytes == null) {
    return null;
  }
  ArtifactData parentData=GSON.fromJson(Bytes.toString(parentDataBytes),ArtifactData.class);
  Set<PluginClass> parentPlugins=parentData.meta.getClasses().getPlugins();
  Set<PluginClass> filteredPlugins=Sets.newLinkedHashSet(Iterables.filter(parentPlugins,filter));
  if (!filteredPlugins.isEmpty()) {
    Location parentLocation=Locations.getCompatibleLocation(locationFactory,parentData.locationPath,parentData.locationURI);
    ArtifactDescriptor descriptor=new ArtifactDescriptor(artifactId.toArtifactId(),parentLocation);
    result.put(descriptor,filteredPlugins);
  }
  return result;
}","private SortedMap<ArtifactDescriptor,Set<PluginClass>> getPluginsInArtifact(Table table,Id.Artifact artifactId,Predicate<PluginClass> filter){
  SortedMap<ArtifactDescriptor,Set<PluginClass>> result=new TreeMap<>();
  ArtifactCell parentCell=new ArtifactCell(artifactId);
  byte[] parentDataBytes=table.get(parentCell.rowkey,parentCell.column);
  if (parentDataBytes == null) {
    return null;
  }
  ArtifactData parentData=GSON.fromJson(Bytes.toString(parentDataBytes),ArtifactData.class);
  Set<PluginClass> parentPlugins=parentData.meta.getClasses().getPlugins();
  Set<PluginClass> filteredPlugins=Sets.newLinkedHashSet(Iterables.filter(parentPlugins,filter));
  if (!filteredPlugins.isEmpty()) {
    Location parentLocation=Locations.getLocationFromAbsolutePath(locationFactory,parentData.getLocationPath());
    ArtifactDescriptor descriptor=new ArtifactDescriptor(artifactId.toArtifactId(),parentLocation);
    result.put(descriptor,filteredPlugins);
  }
  return result;
}"
5507,"private void deleteMeta(Table table,Id.Artifact artifactId,byte[] oldData) throws IOException {
  ArtifactCell artifactCell=new ArtifactCell(artifactId);
  table.delete(artifactCell.rowkey,artifactCell.column);
  final ArtifactData oldMeta=GSON.fromJson(Bytes.toString(oldData),ArtifactData.class);
  byte[] artifactColumn=new ArtifactColumn(artifactId).getColumn();
  for (  PluginClass pluginClass : oldMeta.meta.getClasses().getPlugins()) {
    for (    ArtifactRange artifactRange : oldMeta.meta.getUsableBy()) {
      PluginKey pluginKey=new PluginKey(artifactRange.getNamespace().toId(),artifactRange.getName(),pluginClass.getType(),pluginClass.getName());
      table.delete(pluginKey.getRowKey(),artifactColumn);
    }
  }
  for (  ApplicationClass appClass : oldMeta.meta.getClasses().getApps()) {
    AppClassKey appClassKey=new AppClassKey(artifactId.getNamespace().toEntityId(),appClass.getClassName());
    table.delete(appClassKey.getRowKey(),artifactColumn);
  }
  try {
    new NamespacedImpersonator(artifactId.getNamespace().toEntityId(),impersonator).impersonate(new Callable<Void>(){
      @Override public Void call() throws Exception {
        Locations.getCompatibleLocation(locationFactory,oldMeta.locationPath,oldMeta.locationURI).delete();
        return null;
      }
    }
);
  }
 catch (  IOException ioe) {
    throw ioe;
  }
catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","private void deleteMeta(Table table,Id.Artifact artifactId,byte[] oldData) throws IOException {
  ArtifactCell artifactCell=new ArtifactCell(artifactId);
  table.delete(artifactCell.rowkey,artifactCell.column);
  final ArtifactData oldMeta=GSON.fromJson(Bytes.toString(oldData),ArtifactData.class);
  byte[] artifactColumn=new ArtifactColumn(artifactId).getColumn();
  for (  PluginClass pluginClass : oldMeta.meta.getClasses().getPlugins()) {
    for (    ArtifactRange artifactRange : oldMeta.meta.getUsableBy()) {
      PluginKey pluginKey=new PluginKey(artifactRange.getNamespace().toId(),artifactRange.getName(),pluginClass.getType(),pluginClass.getName());
      table.delete(pluginKey.getRowKey(),artifactColumn);
    }
  }
  for (  ApplicationClass appClass : oldMeta.meta.getClasses().getApps()) {
    AppClassKey appClassKey=new AppClassKey(artifactId.getNamespace().toEntityId(),appClass.getClassName());
    table.delete(appClassKey.getRowKey(),artifactColumn);
  }
  try {
    new NamespacedImpersonator(artifactId.getNamespace().toEntityId(),impersonator).impersonate(new Callable<Void>(){
      @Override public Void call() throws Exception {
        Locations.getLocationFromAbsolutePath(locationFactory,oldMeta.getLocationPath()).delete();
        return null;
      }
    }
);
  }
 catch (  IOException ioe) {
    throw ioe;
  }
catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
5508,"/** 
 * Get all application classes that belong to the specified namespace of the specified classname. Results are returned as a sorted map from artifact to application classes in that artifact. Map entries are sorted by the artifact.
 * @param namespace the namespace from which to get application classes
 * @param className the classname of application classes to get
 * @return an unmodifiable map of artifact the application classes in that artifact.The map will never be null. If there are no application classes, an empty map will be returned.
 */
public SortedMap<ArtifactDescriptor,ApplicationClass> getApplicationClasses(final NamespaceId namespace,final String className){
  try {
    return Transactions.execute(transactional,new TxCallable<SortedMap<ArtifactDescriptor,ApplicationClass>>(){
      @Override public SortedMap<ArtifactDescriptor,ApplicationClass> call(      DatasetContext context) throws Exception {
        SortedMap<ArtifactDescriptor,ApplicationClass> result=Maps.newTreeMap();
        Row row=getMetaTable(context).get(new AppClassKey(namespace,className).getRowKey());
        if (!row.isEmpty()) {
          for (          Map.Entry<byte[],byte[]> column : row.getColumns().entrySet()) {
            ArtifactColumn artifactColumn=ArtifactColumn.parse(column.getKey());
            AppData appData=GSON.fromJson(Bytes.toString(column.getValue()),AppData.class);
            ArtifactDescriptor artifactDescriptor=new ArtifactDescriptor(artifactColumn.artifactId.toArtifactId(),Locations.getCompatibleLocation(locationFactory,appData.artifactLocationPath,appData.artifactLocationURI));
            result.put(artifactDescriptor,appData.appClass);
          }
        }
        return Collections.unmodifiableSortedMap(result);
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e);
  }
}","/** 
 * Get all application classes that belong to the specified namespace of the specified classname. Results are returned as a sorted map from artifact to application classes in that artifact. Map entries are sorted by the artifact.
 * @param namespace the namespace from which to get application classes
 * @param className the classname of application classes to get
 * @return an unmodifiable map of artifact the application classes in that artifact.The map will never be null. If there are no application classes, an empty map will be returned.
 */
public SortedMap<ArtifactDescriptor,ApplicationClass> getApplicationClasses(final NamespaceId namespace,final String className){
  try {
    return Transactions.execute(transactional,new TxCallable<SortedMap<ArtifactDescriptor,ApplicationClass>>(){
      @Override public SortedMap<ArtifactDescriptor,ApplicationClass> call(      DatasetContext context) throws Exception {
        SortedMap<ArtifactDescriptor,ApplicationClass> result=Maps.newTreeMap();
        Row row=getMetaTable(context).get(new AppClassKey(namespace,className).getRowKey());
        if (!row.isEmpty()) {
          for (          Map.Entry<byte[],byte[]> column : row.getColumns().entrySet()) {
            ArtifactColumn artifactColumn=ArtifactColumn.parse(column.getKey());
            AppData appData=GSON.fromJson(Bytes.toString(column.getValue()),AppData.class);
            ArtifactDescriptor artifactDescriptor=new ArtifactDescriptor(artifactColumn.artifactId.toArtifactId(),Locations.getLocationFromAbsolutePath(locationFactory,appData.getArtifactLocationPath()));
            result.put(artifactDescriptor,appData.appClass);
          }
        }
        return Collections.unmodifiableSortedMap(result);
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e);
  }
}"
5509,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetModuleId moduleId=cliConfig.getCurrentNamespace().datasetModule(arguments.get(ArgumentName.DATASET_MODULE.toString()));
  DatasetModuleMeta datasetModuleMeta=datasetModuleClient.get(moduleId);
  Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(ImmutableList.of(datasetModuleMeta),new RowMaker<DatasetModuleMeta>(){
    @Override public List<?> makeRow(    DatasetModuleMeta object){
      return Lists.newArrayList(object.getName(),object.getClassName(),object.getJarLocation(),Joiner.on(""String_Node_Str"").join(object.getTypes()),Joiner.on(""String_Node_Str"").join(object.getUsesModules()),Joiner.on(""String_Node_Str"").join(object.getUsedByModules()));
    }
  }
).build();
  cliConfig.getTableRenderer().render(cliConfig,output,table);
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetModuleId moduleId=cliConfig.getCurrentNamespace().datasetModule(arguments.get(ArgumentName.DATASET_MODULE.toString()));
  DatasetModuleMeta datasetModuleMeta=datasetModuleClient.get(moduleId);
  Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(ImmutableList.of(datasetModuleMeta),new RowMaker<DatasetModuleMeta>(){
    @Override public List<?> makeRow(    DatasetModuleMeta object){
      return Lists.newArrayList(object.getName(),object.getClassName(),object.getJarLocationPath(),Joiner.on(""String_Node_Str"").join(object.getTypes()),Joiner.on(""String_Node_Str"").join(object.getUsesModules()),Joiner.on(""String_Node_Str"").join(object.getUsedByModules()));
    }
  }
).build();
  cliConfig.getTableRenderer().render(cliConfig,output,table);
}"
5510,"@Override public List<?> makeRow(DatasetModuleMeta object){
  return Lists.newArrayList(object.getName(),object.getClassName(),object.getJarLocation(),Joiner.on(""String_Node_Str"").join(object.getTypes()),Joiner.on(""String_Node_Str"").join(object.getUsesModules()),Joiner.on(""String_Node_Str"").join(object.getUsedByModules()));
}","@Override public List<?> makeRow(DatasetModuleMeta object){
  return Lists.newArrayList(object.getName(),object.getClassName(),object.getJarLocationPath(),Joiner.on(""String_Node_Str"").join(object.getTypes()),Joiner.on(""String_Node_Str"").join(object.getUsesModules()),Joiner.on(""String_Node_Str"").join(object.getUsedByModules()));
}"
5511,"@Override public void apply() throws Exception {
  Collection<DatasetModuleMeta> allDatasets=datasetTypeMDS.getModules(NamespaceId.SYSTEM);
  for (  DatasetModuleMeta ds : allDatasets) {
    if (ds.getJarLocation() == null) {
      LOG.debug(""String_Node_Str"",ds.toString());
      DatasetModuleId moduleId=NamespaceId.SYSTEM.datasetModule(ds.getName());
      datasetTypeMDS.deleteModule(moduleId);
      revokeAllPrivilegesOnModule(moduleId,ds);
    }
  }
}","@Override public void apply() throws Exception {
  Collection<DatasetModuleMeta> allDatasets=datasetTypeMDS.getModules(NamespaceId.SYSTEM);
  for (  DatasetModuleMeta ds : allDatasets) {
    if (ds.getJarLocationPath() == null) {
      LOG.debug(""String_Node_Str"",ds.toString());
      DatasetModuleId moduleId=NamespaceId.SYSTEM.datasetModule(ds.getName());
      datasetTypeMDS.deleteModule(moduleId);
      revokeAllPrivilegesOnModule(moduleId,ds);
    }
  }
}"
5512,"private void deleteSystemModules() throws InterruptedException, TransactionFailureException {
  final DatasetTypeMDS datasetTypeMDS=datasetCache.getDataset(DatasetMetaTableUtil.META_TABLE_NAME);
  txExecutorFactory.createExecutor(datasetCache).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      Collection<DatasetModuleMeta> allDatasets=datasetTypeMDS.getModules(NamespaceId.SYSTEM);
      for (      DatasetModuleMeta ds : allDatasets) {
        if (ds.getJarLocation() == null) {
          LOG.debug(""String_Node_Str"",ds.toString());
          DatasetModuleId moduleId=NamespaceId.SYSTEM.datasetModule(ds.getName());
          datasetTypeMDS.deleteModule(moduleId);
          revokeAllPrivilegesOnModule(moduleId,ds);
        }
      }
    }
  }
);
}","private void deleteSystemModules() throws InterruptedException, TransactionFailureException {
  final DatasetTypeMDS datasetTypeMDS=datasetCache.getDataset(DatasetMetaTableUtil.META_TABLE_NAME);
  txExecutorFactory.createExecutor(datasetCache).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      Collection<DatasetModuleMeta> allDatasets=datasetTypeMDS.getModules(NamespaceId.SYSTEM);
      for (      DatasetModuleMeta ds : allDatasets) {
        if (ds.getJarLocationPath() == null) {
          LOG.debug(""String_Node_Str"",ds.toString());
          DatasetModuleId moduleId=NamespaceId.SYSTEM.datasetModule(ds.getName());
          datasetTypeMDS.deleteModule(moduleId);
          revokeAllPrivilegesOnModule(moduleId,ds);
        }
      }
    }
  }
);
}"
5513,"@Override public void apply() throws DatasetModuleConflictException, IOException {
  final Set<String> typesToDelete=new HashSet<String>();
  final List<Location> moduleLocations=new ArrayList<>();
  final Collection<DatasetModuleMeta> modules=datasetTypeMDS.getModules(namespaceId);
  try {
    impersonator.doAs(namespaceId,new Callable<Void>(){
      @Override public Void call() throws Exception {
        for (        DatasetModuleMeta module : modules) {
          typesToDelete.addAll(module.getTypes());
          moduleLocations.add(locationFactory.create(module.getJarLocation()));
        }
        return null;
      }
    }
);
  }
 catch (  Exception e) {
    Throwables.propagate(e);
  }
  Collection<DatasetSpecification> instances=datasetInstanceMDS.getByTypes(namespaceId,typesToDelete);
  if (!instances.isEmpty()) {
    throw new DatasetModuleConflictException(""String_Node_Str"");
  }
  datasetTypeMDS.deleteModules(namespaceId);
  for (  Location moduleLocation : moduleLocations) {
    if (!moduleLocation.delete()) {
      LOG.debug(""String_Node_Str"",moduleLocation);
    }
  }
}","@Override public void apply() throws DatasetModuleConflictException, IOException {
  final Set<String> typesToDelete=new HashSet<String>();
  final List<Location> moduleLocations=new ArrayList<>();
  final Collection<DatasetModuleMeta> modules=datasetTypeMDS.getModules(namespaceId);
  try {
    impersonator.doAs(namespaceId,new Callable<Void>(){
      @Override public Void call() throws Exception {
        for (        DatasetModuleMeta module : modules) {
          typesToDelete.addAll(module.getTypes());
          moduleLocations.add(Locations.getLocationFromAbsolutePath(locationFactory,module.getJarLocationPath()));
        }
        return null;
      }
    }
);
  }
 catch (  Exception e) {
    Throwables.propagate(e);
  }
  Collection<DatasetSpecification> instances=datasetInstanceMDS.getByTypes(namespaceId,typesToDelete);
  if (!instances.isEmpty()) {
    throw new DatasetModuleConflictException(""String_Node_Str"");
  }
  datasetTypeMDS.deleteModules(namespaceId);
  for (  Location moduleLocation : moduleLocations) {
    if (!moduleLocation.delete()) {
      LOG.debug(""String_Node_Str"",moduleLocation);
    }
  }
}"
5514,"/** 
 * Deletes all modules in a namespace, other than system. Presumes that the namespace has already been checked to be non-system.
 * @param namespaceId the {@link NamespaceId} to delete modules from.
 */
public void deleteModules(final NamespaceId namespaceId) throws DatasetModuleConflictException {
  Preconditions.checkArgument(namespaceId != null && !NamespaceId.SYSTEM.equals(namespaceId),""String_Node_Str"");
  LOG.warn(""String_Node_Str"",namespaceId);
  try {
    final DatasetTypeMDS datasetTypeMDS=datasetCache.getDataset(DatasetMetaTableUtil.META_TABLE_NAME);
    final DatasetInstanceMDS datasetInstanceMDS=datasetCache.getDataset(DatasetMetaTableUtil.INSTANCE_TABLE_NAME);
    txExecutorFactory.createExecutor(datasetCache).execute(new TransactionExecutor.Subroutine(){
      @Override public void apply() throws DatasetModuleConflictException, IOException {
        final Set<String> typesToDelete=new HashSet<String>();
        final List<Location> moduleLocations=new ArrayList<>();
        final Collection<DatasetModuleMeta> modules=datasetTypeMDS.getModules(namespaceId);
        try {
          impersonator.doAs(namespaceId,new Callable<Void>(){
            @Override public Void call() throws Exception {
              for (              DatasetModuleMeta module : modules) {
                typesToDelete.addAll(module.getTypes());
                moduleLocations.add(locationFactory.create(module.getJarLocation()));
              }
              return null;
            }
          }
);
        }
 catch (        Exception e) {
          Throwables.propagate(e);
        }
        Collection<DatasetSpecification> instances=datasetInstanceMDS.getByTypes(namespaceId,typesToDelete);
        if (!instances.isEmpty()) {
          throw new DatasetModuleConflictException(""String_Node_Str"");
        }
        datasetTypeMDS.deleteModules(namespaceId);
        for (        Location moduleLocation : moduleLocations) {
          if (!moduleLocation.delete()) {
            LOG.debug(""String_Node_Str"",moduleLocation);
          }
        }
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    if (e.getCause() != null && e.getCause() instanceof DatasetModuleConflictException) {
      throw (DatasetModuleConflictException)e.getCause();
    }
    LOG.error(""String_Node_Str"",namespaceId);
    throw Throwables.propagate(e);
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}","/** 
 * Deletes all modules in a namespace, other than system. Presumes that the namespace has already been checked to be non-system.
 * @param namespaceId the {@link NamespaceId} to delete modules from.
 */
public void deleteModules(final NamespaceId namespaceId) throws DatasetModuleConflictException {
  Preconditions.checkArgument(namespaceId != null && !NamespaceId.SYSTEM.equals(namespaceId),""String_Node_Str"");
  LOG.warn(""String_Node_Str"",namespaceId);
  try {
    final DatasetTypeMDS datasetTypeMDS=datasetCache.getDataset(DatasetMetaTableUtil.META_TABLE_NAME);
    final DatasetInstanceMDS datasetInstanceMDS=datasetCache.getDataset(DatasetMetaTableUtil.INSTANCE_TABLE_NAME);
    txExecutorFactory.createExecutor(datasetCache).execute(new TransactionExecutor.Subroutine(){
      @Override public void apply() throws DatasetModuleConflictException, IOException {
        final Set<String> typesToDelete=new HashSet<String>();
        final List<Location> moduleLocations=new ArrayList<>();
        final Collection<DatasetModuleMeta> modules=datasetTypeMDS.getModules(namespaceId);
        try {
          impersonator.doAs(namespaceId,new Callable<Void>(){
            @Override public Void call() throws Exception {
              for (              DatasetModuleMeta module : modules) {
                typesToDelete.addAll(module.getTypes());
                moduleLocations.add(Locations.getLocationFromAbsolutePath(locationFactory,module.getJarLocationPath()));
              }
              return null;
            }
          }
);
        }
 catch (        Exception e) {
          Throwables.propagate(e);
        }
        Collection<DatasetSpecification> instances=datasetInstanceMDS.getByTypes(namespaceId,typesToDelete);
        if (!instances.isEmpty()) {
          throw new DatasetModuleConflictException(""String_Node_Str"");
        }
        datasetTypeMDS.deleteModules(namespaceId);
        for (        Location moduleLocation : moduleLocations) {
          if (!moduleLocation.delete()) {
            LOG.debug(""String_Node_Str"",moduleLocation);
          }
        }
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    if (e.getCause() != null && e.getCause() instanceof DatasetModuleConflictException) {
      throw (DatasetModuleConflictException)e.getCause();
    }
    LOG.error(""String_Node_Str"",namespaceId);
    throw Throwables.propagate(e);
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}"
5515,"@Override public Void call() throws Exception {
  for (  DatasetModuleMeta module : modules) {
    typesToDelete.addAll(module.getTypes());
    moduleLocations.add(locationFactory.create(module.getJarLocation()));
  }
  return null;
}","@Override public Void call() throws Exception {
  for (  DatasetModuleMeta module : modules) {
    typesToDelete.addAll(module.getTypes());
    moduleLocations.add(Locations.getLocationFromAbsolutePath(locationFactory,module.getJarLocationPath()));
  }
  return null;
}"
5516,"/** 
 * Deletes specified dataset module
 * @param datasetModuleId {@link DatasetModuleId} of the dataset module to delete
 * @return true if deleted successfully, false if module didn't exist: nothing to delete
 * @throws DatasetModuleConflictException when there are other modules depend on the specified one, in which casedeletion does NOT happen
 */
public boolean deleteModule(final DatasetModuleId datasetModuleId) throws DatasetModuleConflictException {
  LOG.info(""String_Node_Str"",datasetModuleId);
  try {
    final DatasetTypeMDS datasetTypeMDS=datasetCache.getDataset(DatasetMetaTableUtil.META_TABLE_NAME);
    final DatasetInstanceMDS datasetInstanceMDS=datasetCache.getDataset(DatasetMetaTableUtil.INSTANCE_TABLE_NAME);
    return txExecutorFactory.createExecutor(datasetCache).execute(new Callable<Boolean>(){
      @Override public Boolean call() throws DatasetModuleConflictException, IOException {
        final DatasetModuleMeta module=datasetTypeMDS.getModule(datasetModuleId);
        if (module == null) {
          return false;
        }
        if (module.getUsedByModules().size() > 0) {
          String msg=String.format(""String_Node_Str"",module);
          throw new DatasetModuleConflictException(msg);
        }
        Collection<DatasetSpecification> instances=datasetInstanceMDS.getByTypes(datasetModuleId.getParent(),ImmutableSet.copyOf(module.getTypes()));
        if (!instances.isEmpty()) {
          String msg=String.format(""String_Node_Str"",module);
          throw new DatasetModuleConflictException(msg);
        }
        for (        String usedModuleName : module.getUsesModules()) {
          DatasetModuleId usedModuleId=new DatasetModuleId(datasetModuleId.getNamespace(),usedModuleName);
          DatasetModuleMeta usedModule=datasetTypeMDS.getModule(usedModuleId);
          if (usedModule == null) {
            usedModuleId=NamespaceId.SYSTEM.datasetModule(usedModuleName);
            usedModule=datasetTypeMDS.getModule(usedModuleId);
            Preconditions.checkState(usedModule != null,""String_Node_Str"",usedModuleName,datasetModuleId.getEntityName());
          }
          usedModule.removeUsedByModule(datasetModuleId.getEntityName());
          datasetTypeMDS.writeModule(usedModuleId.getParent(),usedModule);
        }
        datasetTypeMDS.deleteModule(datasetModuleId);
        try {
          Location moduleJarLocation=impersonator.doAs(datasetModuleId.getParent(),new Callable<Location>(){
            @Override public Location call() throws Exception {
              return locationFactory.create(module.getJarLocation());
            }
          }
);
          if (!moduleJarLocation.delete()) {
            LOG.debug(""String_Node_Str"");
          }
        }
 catch (        Exception e) {
          Throwables.propagateIfInstanceOf(e,IOException.class);
          Throwables.propagate(e);
        }
        return true;
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    if (e.getCause() != null && e.getCause() instanceof DatasetModuleConflictException) {
      throw (DatasetModuleConflictException)e.getCause();
    }
    throw Throwables.propagate(e);
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}","/** 
 * Deletes specified dataset module
 * @param datasetModuleId {@link DatasetModuleId} of the dataset module to delete
 * @return true if deleted successfully, false if module didn't exist: nothing to delete
 * @throws DatasetModuleConflictException when there are other modules depend on the specified one, in which casedeletion does NOT happen
 */
public boolean deleteModule(final DatasetModuleId datasetModuleId) throws DatasetModuleConflictException {
  LOG.info(""String_Node_Str"",datasetModuleId);
  try {
    final DatasetTypeMDS datasetTypeMDS=datasetCache.getDataset(DatasetMetaTableUtil.META_TABLE_NAME);
    final DatasetInstanceMDS datasetInstanceMDS=datasetCache.getDataset(DatasetMetaTableUtil.INSTANCE_TABLE_NAME);
    return txExecutorFactory.createExecutor(datasetCache).execute(new Callable<Boolean>(){
      @Override public Boolean call() throws DatasetModuleConflictException, IOException {
        final DatasetModuleMeta module=datasetTypeMDS.getModule(datasetModuleId);
        if (module == null) {
          return false;
        }
        if (module.getUsedByModules().size() > 0) {
          String msg=String.format(""String_Node_Str"",module);
          throw new DatasetModuleConflictException(msg);
        }
        Collection<DatasetSpecification> instances=datasetInstanceMDS.getByTypes(datasetModuleId.getParent(),ImmutableSet.copyOf(module.getTypes()));
        if (!instances.isEmpty()) {
          String msg=String.format(""String_Node_Str"",module);
          throw new DatasetModuleConflictException(msg);
        }
        for (        String usedModuleName : module.getUsesModules()) {
          DatasetModuleId usedModuleId=new DatasetModuleId(datasetModuleId.getNamespace(),usedModuleName);
          DatasetModuleMeta usedModule=datasetTypeMDS.getModule(usedModuleId);
          if (usedModule == null) {
            usedModuleId=NamespaceId.SYSTEM.datasetModule(usedModuleName);
            usedModule=datasetTypeMDS.getModule(usedModuleId);
            Preconditions.checkState(usedModule != null,""String_Node_Str"",usedModuleName,datasetModuleId.getEntityName());
          }
          usedModule.removeUsedByModule(datasetModuleId.getEntityName());
          datasetTypeMDS.writeModule(usedModuleId.getParent(),usedModule);
        }
        datasetTypeMDS.deleteModule(datasetModuleId);
        try {
          Location moduleJarLocation=impersonator.doAs(datasetModuleId.getParent(),new Callable<Location>(){
            @Override public Location call() throws Exception {
              return Locations.getLocationFromAbsolutePath(locationFactory,module.getJarLocationPath());
            }
          }
);
          if (!moduleJarLocation.delete()) {
            LOG.debug(""String_Node_Str"");
          }
        }
 catch (        Exception e) {
          Throwables.propagateIfInstanceOf(e,IOException.class);
          Throwables.propagate(e);
        }
        return true;
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    if (e.getCause() != null && e.getCause() instanceof DatasetModuleConflictException) {
      throw (DatasetModuleConflictException)e.getCause();
    }
    throw Throwables.propagate(e);
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}"
5517,"@Override public ClassLoader load(CacheKey key) throws Exception {
  if (key.uri == null) {
    return key.parentClassLoader;
  }
  Location jarLocation=locationFactory.create(key.uri);
  File unpackedDir=DirUtils.createTempDir(tmpDir);
  BundleJarUtil.unJar(jarLocation,unpackedDir);
  LOG.trace(""String_Node_Str"",key.uri.toString(),unpackedDir.getAbsolutePath());
  return new DirectoryClassLoader(unpackedDir,key.parentClassLoader,""String_Node_Str"");
}","@Override public ClassLoader load(CacheKey key) throws Exception {
  if (key.uri == null) {
    return key.parentClassLoader;
  }
  Location jarLocation=Locations.getLocationFromAbsolutePath(locationFactory,key.uri.getPath());
  File unpackedDir=DirUtils.createTempDir(tmpDir);
  BundleJarUtil.unJar(jarLocation,unpackedDir);
  LOG.trace(""String_Node_Str"",key.uri.toString(),unpackedDir.getAbsolutePath());
  return new DirectoryClassLoader(unpackedDir,key.parentClassLoader,""String_Node_Str"");
}"
5518,"@Override public ClassLoader get(DatasetModuleMeta moduleMeta,ClassLoader parentClassLoader) throws IOException {
  URI jarLocation=moduleMeta.getJarLocation();
  return jarLocation == null ? parentClassLoader : classLoaders.getUnchecked(new CacheKey(jarLocation,parentClassLoader));
}","@Override public ClassLoader get(DatasetModuleMeta moduleMeta,ClassLoader parentClassLoader) throws IOException {
  URI jarLocation=moduleMeta.getJarLocationPath() == null ? null : URI.create(moduleMeta.getJarLocationPath());
  return jarLocation == null ? parentClassLoader : classLoaders.getUnchecked(new CacheKey(jarLocation,parentClassLoader));
}"
5519,"static void verify(DatasetModuleMeta moduleMeta,String moduleName,Class moduleClass,List<String> types,List<String> usesModules,Collection<String> usedByModules){
  Assert.assertEquals(moduleName,moduleMeta.getName());
  Assert.assertEquals(moduleClass.getName(),moduleMeta.getClassName());
  Assert.assertArrayEquals(types.toArray(),moduleMeta.getTypes().toArray());
  Assert.assertArrayEquals(usesModules.toArray(),moduleMeta.getUsesModules().toArray());
  Assert.assertArrayEquals(Sets.newTreeSet(usedByModules).toArray(),Sets.newTreeSet(moduleMeta.getUsedByModules()).toArray());
  Assert.assertNotNull(moduleMeta.getJarLocation());
  Assert.assertTrue(new File(moduleMeta.getJarLocation()).exists());
}","static void verify(DatasetModuleMeta moduleMeta,String moduleName,Class moduleClass,List<String> types,List<String> usesModules,Collection<String> usedByModules){
  Assert.assertEquals(moduleName,moduleMeta.getName());
  Assert.assertEquals(moduleClass.getName(),moduleMeta.getClassName());
  Assert.assertArrayEquals(types.toArray(),moduleMeta.getTypes().toArray());
  Assert.assertArrayEquals(usesModules.toArray(),moduleMeta.getUsesModules().toArray());
  Assert.assertArrayEquals(Sets.newTreeSet(usedByModules).toArray(),Sets.newTreeSet(moduleMeta.getUsedByModules()).toArray());
  Assert.assertNotNull(moduleMeta.getJarLocationPath());
  Assert.assertTrue(new File(moduleMeta.getJarLocationPath()).exists());
}"
5520,"/** 
 * Creates instance of   {@link DatasetModuleMeta}
 * @param name name of the dataset module
 * @param className class name of the dataset module
 * @param jarLocation location of the dataset module jar. {@code null} means this is ""system module"" which classesalways present in classpath. This helps to minimize redundant copying of jars.
 * @param types list of types announced by this module in the order they are announced
 * @param usesModules list of modules that this module depends on, ordered in a way they must beloaded and initialized
 * @param usedByModules list of modules that depend on this module
 */
public DatasetModuleMeta(String name,String className,@Nullable URI jarLocation,Collection<String> types,Collection<String> usesModules,Collection<String> usedByModules){
  this.name=name;
  this.className=className;
  this.jarLocation=jarLocation;
  this.types=Collections.unmodifiableList(new ArrayList<>(types));
  this.usesModules=Collections.unmodifiableList(new ArrayList<>(usesModules));
  this.usedByModules=new ArrayList<>(usedByModules);
}","/** 
 * Creates instance of   {@link DatasetModuleMeta}
 * @param name name of the dataset module
 * @param className class name of the dataset module
 * @param jarLocation location of the dataset module jar. {@code null} means this is ""system module"" which classesalways present in classpath. This helps to minimize redundant copying of jars.
 * @param types list of types announced by this module in the order they are announced
 * @param usesModules list of modules that this module depends on, ordered in a way they must beloaded and initialized
 * @param usedByModules list of modules that depend on this module
 */
public DatasetModuleMeta(String name,String className,@Nullable URI jarLocation,Collection<String> types,Collection<String> usesModules,Collection<String> usedByModules){
  this.name=name;
  this.className=className;
  this.jarLocation=null;
  this.jarLocationPath=jarLocation == null ? null : jarLocation.getPath();
  this.types=Collections.unmodifiableList(new ArrayList<>(types));
  this.usesModules=Collections.unmodifiableList(new ArrayList<>(usesModules));
  this.usedByModules=new ArrayList<>(usedByModules);
}"
5521,"@Override public String toString(){
  return ""String_Node_Str"" + ""String_Node_Str"" + name + '\''+ ""String_Node_Str""+ className+ '\''+ ""String_Node_Str""+ jarLocation+ ""String_Node_Str""+ types+ ""String_Node_Str""+ usesModules+ ""String_Node_Str""+ usedByModules+ '}';
}","@Override public String toString(){
  return ""String_Node_Str"" + ""String_Node_Str"" + name + '\''+ ""String_Node_Str""+ className+ '\''+ ""String_Node_Str""+ jarLocation+ ""String_Node_Str""+ jarLocationPath+ ""String_Node_Str""+ types+ ""String_Node_Str""+ usesModules+ ""String_Node_Str""+ usedByModules+ '}';
}"
5522,"ArtifactData(Location location,ArtifactMeta meta){
  this.locationURI=null;
  this.locationPath=Locations.getRelativePath(location);
  this.meta=meta;
}","ArtifactData(Location location,ArtifactMeta meta){
  this.locationURI=null;
  this.locationPath=Locations.getRelativePath(location.getLocationFactory(),location.toURI());
  this.meta=meta;
}"
5523,"AppData(ApplicationClass appClass,Location artifactLocation){
  this.appClass=appClass;
  this.artifactLocationURI=null;
  this.artifactLocationPath=Locations.getRelativePath(artifactLocation);
}","AppData(ApplicationClass appClass,Location artifactLocation){
  this.appClass=appClass;
  this.artifactLocationURI=null;
  this.artifactLocationPath=Locations.getRelativePath(artifactLocation.getLocationFactory(),artifactLocation.toURI());
}"
5524,"PluginData(PluginClass pluginClass,ArtifactRange usableBy,Location artifactLocation){
  this.pluginClass=pluginClass;
  this.usableBy=usableBy;
  this.artifactLocationURI=null;
  this.artifactLocationPath=Locations.getRelativePath(artifactLocation);
}","PluginData(PluginClass pluginClass,ArtifactRange usableBy,Location artifactLocation){
  this.pluginClass=pluginClass;
  this.usableBy=usableBy;
  this.artifactLocationURI=null;
  this.artifactLocationPath=Locations.getRelativePath(artifactLocation.getLocationFactory(),artifactLocation.toURI());
}"
5525,"/** 
 * For backward compatibility with URIs, this method creates a location based on uri or path one of them should be non-null
 * @param locationFactory corresponding to the uri and path
 * @param path if path is available
 * @param uri if uri is available
 * @return
 */
public static Location getCompatibleLocation(LocationFactory locationFactory,@Nullable String path,@Nullable URI uri){
  Location artifactLocation=uri != null ? locationFactory.create(Locations.getRelativePath(locationFactory.create(uri))) : locationFactory.create(path);
  return artifactLocation;
}","/** 
 * For backward compatibility with URIs, this method creates a location based on uri or path one of them should be non-null
 * @param locationFactory corresponding to the uri and path
 * @param path if path is available
 * @param uri if uri is available
 * @return Backward compatible Location
 */
public static Location getCompatibleLocation(LocationFactory locationFactory,@Nullable String path,@Nullable URI uri){
  Location artifactLocation=uri != null ? locationFactory.create(Locations.getRelativePath(locationFactory,uri)) : locationFactory.create(path);
  return artifactLocation;
}"
5526,"/** 
 * Returns the relative path for a given location
 * @param location
 * @return
 */
public static String getRelativePath(Location location){
  URI baseURI=location.getLocationFactory().create(""String_Node_Str"").toURI();
  URI locationURI=location.toURI();
  URI relativeURI=baseURI.relativize(locationURI);
  return relativeURI.getPath();
}","/** 
 * Returns the relative path for a given locationURI. If locationURI was created with a different locationFactory the same locationURI may be returned.
 * @param locationFactory locationFactory to be used to create base URI
 * @param locationURI URI to be used for base URI
 * @return Path relative to the Base path of the given locationFactory
 */
public static String getRelativePath(LocationFactory locationFactory,URI locationURI){
  URI baseURI=URI.create(locationFactory.create(""String_Node_Str"").toURI().getPath());
  URI relativeURI=baseURI.relativize(URI.create(locationURI.getPath()));
  return relativeURI.getPath();
}"
5527,"@Override public Integer apply(Table table) throws Exception {
  int deletedColumns=0;
  try (Scanner scanner=table.scan(ROW_KEY_PREFIX,ROW_KEY_PREFIX_END)){
    Row row;
    while ((row=scanner.next()) != null) {
      byte[] rowKey=row.getRow();
      final NamespaceId namespaceId=getNamespaceId(rowKey);
      String namespacedLogDir=null;
      try {
        namespacedLogDir=impersonator.doAs(namespaceId,new Callable<String>(){
          @Override public String call() throws Exception {
            return LoggingContextHelper.getNamespacedBaseDirLocation(namespacedLocationFactory,logBaseDir,namespaceId,impersonator).toString();
          }
        }
);
      }
 catch (      Exception e) {
        if (e instanceof NamespaceNotFoundException) {
          LOG.warn(""String_Node_Str"",namespaceId.getEntityName(),e);
        }
 else {
          LOG.warn(""String_Node_Str"",namespaceId.getEntityName(),e);
          continue;
        }
      }
      for (      final Map.Entry<byte[],byte[]> entry : row.getColumns().entrySet()) {
        try {
          byte[] colName=entry.getKey();
          URI file=new URI(Bytes.toString(entry.getValue()));
          if (LOG.isDebugEnabled()) {
            LOG.debug(""String_Node_Str"",file,Bytes.toLong(colName));
          }
          if (Strings.isNullOrEmpty(namespacedLogDir)) {
            LOG.warn(""String_Node_Str"" + ""String_Node_Str"",Bytes.toString(entry.getValue()),namespaceId.getEntityName());
            table.delete(rowKey,colName);
            deletedColumns++;
            continue;
          }
          Location fileLocation=impersonator.doAs(namespaceId,new Callable<Location>(){
            @Override public Location call() throws Exception {
              return rootLocationFactory.create(new URI(Bytes.toString(entry.getValue())));
            }
          }
);
          if (!fileLocation.exists()) {
            LOG.warn(""String_Node_Str"",file);
            table.delete(rowKey,colName);
            deletedColumns++;
          }
 else           if (fileLocation.lastModified() < untilTime) {
            callback.handle(namespaceId,fileLocation,namespacedLogDir);
            table.delete(rowKey,colName);
            deletedColumns++;
          }
        }
 catch (        Exception e) {
          if (e instanceof NamespaceNotFoundException) {
            LOG.warn(""String_Node_Str"",Bytes.toString(entry.getValue()),namespaceId.getEntityName());
          }
 else {
            LOG.error(""String_Node_Str"",Bytes.toString(entry.getValue()),e);
          }
        }
      }
    }
  }
   return deletedColumns;
}","@Override public Integer apply(Table table) throws Exception {
  int deletedColumns=0;
  try (Scanner scanner=table.scan(ROW_KEY_PREFIX,ROW_KEY_PREFIX_END)){
    Row row;
    while ((row=scanner.next()) != null) {
      byte[] rowKey=row.getRow();
      final NamespaceId namespaceId=getNamespaceId(rowKey);
      String namespacedLogDir=null;
      try {
        namespacedLogDir=impersonator.doAs(namespaceId,new Callable<String>(){
          @Override public String call() throws Exception {
            return LoggingContextHelper.getNamespacedBaseDirLocation(namespacedLocationFactory,logBaseDir,namespaceId,impersonator).toString();
          }
        }
);
      }
 catch (      Exception e) {
        if (e instanceof NamespaceNotFoundException) {
          LOG.warn(""String_Node_Str"",namespaceId.getEntityName(),e);
        }
 else {
          LOG.warn(""String_Node_Str"",namespaceId.getEntityName(),e);
          continue;
        }
      }
      for (      final Map.Entry<byte[],byte[]> entry : row.getColumns().entrySet()) {
        try {
          byte[] colName=entry.getKey();
          URI file=new URI(Bytes.toString(entry.getValue()));
          if (LOG.isDebugEnabled()) {
            LOG.debug(""String_Node_Str"",file,Bytes.toLong(colName));
          }
          if (Strings.isNullOrEmpty(namespacedLogDir)) {
            LOG.warn(""String_Node_Str"" + ""String_Node_Str"",Bytes.toString(entry.getValue()),namespaceId.getEntityName());
            table.delete(rowKey,colName);
            deletedColumns++;
            continue;
          }
          Location fileLocation=impersonator.doAs(namespaceId,new Callable<Location>(){
            @Override public Location call() throws Exception {
              return getCompatibleLocationFromValue(entry.getValue());
            }
          }
);
          if (!fileLocation.exists()) {
            LOG.warn(""String_Node_Str"",file);
            table.delete(rowKey,colName);
            deletedColumns++;
          }
 else           if (fileLocation.lastModified() < untilTime) {
            callback.handle(namespaceId,fileLocation,namespacedLogDir);
            table.delete(rowKey,colName);
            deletedColumns++;
          }
        }
 catch (        Exception e) {
          if (e instanceof NamespaceNotFoundException) {
            LOG.warn(""String_Node_Str"",Bytes.toString(entry.getValue()),namespaceId.getEntityName());
          }
 else {
            LOG.error(""String_Node_Str"",Bytes.toString(entry.getValue()),e);
          }
        }
      }
    }
  }
   return deletedColumns;
}"
5528,"@Override public Location call() throws Exception {
  return rootLocationFactory.create(new URI(Bytes.toString(entry.getValue())));
}","@Override public Location call() throws Exception {
  return getCompatibleLocationFromValue(entry.getValue());
}"
5529,"/** 
 * Deletes meta data until a given time, while keeping the latest meta data even if less than the given time.
 * @param untilTime time until the meta data will be deleted.
 * @param callback callback called before deleting a meta data column.
 * @return total number of columns deleted.
 */
public int cleanMetaData(final long untilTime,final DeleteCallback callback) throws Exception {
  return execute(new TransactionExecutor.Function<Table,Integer>(){
    @Override public Integer apply(    Table table) throws Exception {
      int deletedColumns=0;
      try (Scanner scanner=table.scan(ROW_KEY_PREFIX,ROW_KEY_PREFIX_END)){
        Row row;
        while ((row=scanner.next()) != null) {
          byte[] rowKey=row.getRow();
          final NamespaceId namespaceId=getNamespaceId(rowKey);
          String namespacedLogDir=null;
          try {
            namespacedLogDir=impersonator.doAs(namespaceId,new Callable<String>(){
              @Override public String call() throws Exception {
                return LoggingContextHelper.getNamespacedBaseDirLocation(namespacedLocationFactory,logBaseDir,namespaceId,impersonator).toString();
              }
            }
);
          }
 catch (          Exception e) {
            if (e instanceof NamespaceNotFoundException) {
              LOG.warn(""String_Node_Str"",namespaceId.getEntityName(),e);
            }
 else {
              LOG.warn(""String_Node_Str"",namespaceId.getEntityName(),e);
              continue;
            }
          }
          for (          final Map.Entry<byte[],byte[]> entry : row.getColumns().entrySet()) {
            try {
              byte[] colName=entry.getKey();
              URI file=new URI(Bytes.toString(entry.getValue()));
              if (LOG.isDebugEnabled()) {
                LOG.debug(""String_Node_Str"",file,Bytes.toLong(colName));
              }
              if (Strings.isNullOrEmpty(namespacedLogDir)) {
                LOG.warn(""String_Node_Str"" + ""String_Node_Str"",Bytes.toString(entry.getValue()),namespaceId.getEntityName());
                table.delete(rowKey,colName);
                deletedColumns++;
                continue;
              }
              Location fileLocation=impersonator.doAs(namespaceId,new Callable<Location>(){
                @Override public Location call() throws Exception {
                  return rootLocationFactory.create(new URI(Bytes.toString(entry.getValue())));
                }
              }
);
              if (!fileLocation.exists()) {
                LOG.warn(""String_Node_Str"",file);
                table.delete(rowKey,colName);
                deletedColumns++;
              }
 else               if (fileLocation.lastModified() < untilTime) {
                callback.handle(namespaceId,fileLocation,namespacedLogDir);
                table.delete(rowKey,colName);
                deletedColumns++;
              }
            }
 catch (            Exception e) {
              if (e instanceof NamespaceNotFoundException) {
                LOG.warn(""String_Node_Str"",Bytes.toString(entry.getValue()),namespaceId.getEntityName());
              }
 else {
                LOG.error(""String_Node_Str"",Bytes.toString(entry.getValue()),e);
              }
            }
          }
        }
      }
       return deletedColumns;
    }
  }
);
}","/** 
 * Deletes meta data until a given time, while keeping the latest meta data even if less than the given time.
 * @param untilTime time until the meta data will be deleted.
 * @param callback callback called before deleting a meta data column.
 * @return total number of columns deleted.
 */
public int cleanMetaData(final long untilTime,final DeleteCallback callback) throws Exception {
  return execute(new TransactionExecutor.Function<Table,Integer>(){
    @Override public Integer apply(    Table table) throws Exception {
      int deletedColumns=0;
      try (Scanner scanner=table.scan(ROW_KEY_PREFIX,ROW_KEY_PREFIX_END)){
        Row row;
        while ((row=scanner.next()) != null) {
          byte[] rowKey=row.getRow();
          final NamespaceId namespaceId=getNamespaceId(rowKey);
          String namespacedLogDir=null;
          try {
            namespacedLogDir=impersonator.doAs(namespaceId,new Callable<String>(){
              @Override public String call() throws Exception {
                return LoggingContextHelper.getNamespacedBaseDirLocation(namespacedLocationFactory,logBaseDir,namespaceId,impersonator).toString();
              }
            }
);
          }
 catch (          Exception e) {
            if (e instanceof NamespaceNotFoundException) {
              LOG.warn(""String_Node_Str"",namespaceId.getEntityName(),e);
            }
 else {
              LOG.warn(""String_Node_Str"",namespaceId.getEntityName(),e);
              continue;
            }
          }
          for (          final Map.Entry<byte[],byte[]> entry : row.getColumns().entrySet()) {
            try {
              byte[] colName=entry.getKey();
              URI file=new URI(Bytes.toString(entry.getValue()));
              if (LOG.isDebugEnabled()) {
                LOG.debug(""String_Node_Str"",file,Bytes.toLong(colName));
              }
              if (Strings.isNullOrEmpty(namespacedLogDir)) {
                LOG.warn(""String_Node_Str"" + ""String_Node_Str"",Bytes.toString(entry.getValue()),namespaceId.getEntityName());
                table.delete(rowKey,colName);
                deletedColumns++;
                continue;
              }
              Location fileLocation=impersonator.doAs(namespaceId,new Callable<Location>(){
                @Override public Location call() throws Exception {
                  return getCompatibleLocationFromValue(entry.getValue());
                }
              }
);
              if (!fileLocation.exists()) {
                LOG.warn(""String_Node_Str"",file);
                table.delete(rowKey,colName);
                deletedColumns++;
              }
 else               if (fileLocation.lastModified() < untilTime) {
                callback.handle(namespaceId,fileLocation,namespacedLogDir);
                table.delete(rowKey,colName);
                deletedColumns++;
              }
            }
 catch (            Exception e) {
              if (e instanceof NamespaceNotFoundException) {
                LOG.warn(""String_Node_Str"",Bytes.toString(entry.getValue()),namespaceId.getEntityName());
              }
 else {
                LOG.error(""String_Node_Str"",Bytes.toString(entry.getValue()),e);
              }
            }
          }
        }
      }
       return deletedColumns;
    }
  }
);
}"
5530,"/** 
 * Persists meta data associated with a log file.
 * @param logPartition partition name that is used to group log messages
 * @param startTimeMs start log time associated with the file.
 * @param location log file.
 */
private void writeMetaData(final String logPartition,final long startTimeMs,final Location location) throws Exception {
  LOG.debug(""String_Node_Str"",logPartition,startTimeMs,location);
  execute(new TransactionExecutor.Procedure<Table>(){
    @Override public void apply(    Table table) throws Exception {
      table.put(getRowKey(logPartition),Bytes.toBytes(startTimeMs),Bytes.toBytes(location.toURI().toString()));
    }
  }
);
}","/** 
 * Persists meta data associated with a log file.
 * @param logPartition partition name that is used to group log messages
 * @param startTimeMs start log time associated with the file.
 * @param location log file.
 */
private void writeMetaData(final String logPartition,final long startTimeMs,final Location location) throws Exception {
  LOG.debug(""String_Node_Str"",logPartition,startTimeMs,location);
  execute(new TransactionExecutor.Procedure<Table>(){
    @Override public void apply(    Table table) throws Exception {
      String locationPath=Locations.getRelativePath(rootLocationFactory,location.toURI());
      table.put(getRowKey(logPartition),Bytes.toBytes(startTimeMs),Bytes.toBytes(locationPath));
    }
  }
);
}"
5531,"ArtifactData(Location location,ArtifactMeta meta){
  this.locationURI=location.toURI();
  this.meta=meta;
}","ArtifactData(Location location,ArtifactMeta meta){
  this.locationURI=null;
  this.locationPath=Locations.getRelativePath(location);
  this.meta=meta;
}"
5532,"/** 
 * Get information about the given artifact.
 * @param artifactId the artifact to get
 * @return information about the artifact
 * @throws ArtifactNotFoundException if the given artifact does not exist
 * @throws IOException if there was an exception reading the artifact information from the metastore
 */
public ArtifactDetail getArtifact(final Id.Artifact artifactId) throws ArtifactNotFoundException, IOException {
  try {
    final ArtifactData artifactData=Transactions.execute(transactional,new TxCallable<ArtifactData>(){
      @Override public ArtifactData call(      DatasetContext context) throws Exception {
        ArtifactCell artifactCell=new ArtifactCell(artifactId);
        byte[] value=getMetaTable(context).get(artifactCell.rowkey,artifactCell.column);
        if (value == null) {
          throw new ArtifactNotFoundException(artifactId.toEntityId());
        }
        return GSON.fromJson(Bytes.toString(value),ArtifactData.class);
      }
    }
);
    Location artifactLocation=impersonator.doAs(artifactId.getNamespace().toEntityId(),new Callable<Location>(){
      @Override public Location call() throws Exception {
        return locationFactory.create(artifactData.locationURI);
      }
    }
);
    return new ArtifactDetail(new ArtifactDescriptor(artifactId.toArtifactId(),artifactLocation),artifactData.meta);
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,IOException.class,ArtifactNotFoundException.class);
  }
catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","/** 
 * Get information about the given artifact.
 * @param artifactId the artifact to get
 * @return information about the artifact
 * @throws ArtifactNotFoundException if the given artifact does not exist
 * @throws IOException if there was an exception reading the artifact information from the metastore
 */
public ArtifactDetail getArtifact(final Id.Artifact artifactId) throws ArtifactNotFoundException, IOException {
  try {
    final ArtifactData artifactData=Transactions.execute(transactional,new TxCallable<ArtifactData>(){
      @Override public ArtifactData call(      DatasetContext context) throws Exception {
        ArtifactCell artifactCell=new ArtifactCell(artifactId);
        byte[] value=getMetaTable(context).get(artifactCell.rowkey,artifactCell.column);
        if (value == null) {
          throw new ArtifactNotFoundException(artifactId.toEntityId());
        }
        return GSON.fromJson(Bytes.toString(value),ArtifactData.class);
      }
    }
);
    Location artifactLocation=impersonator.doAs(artifactId.getNamespace().toEntityId(),new Callable<Location>(){
      @Override public Location call() throws Exception {
        return Locations.getCompatibleLocation(locationFactory,artifactData.locationPath,artifactData.locationURI);
      }
    }
);
    return new ArtifactDetail(new ArtifactDescriptor(artifactId.toArtifactId(),artifactLocation),artifactData.meta);
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,IOException.class,ArtifactNotFoundException.class);
  }
catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
5533,"AppData(ApplicationClass appClass,Location artifactLocation){
  this.appClass=appClass;
  this.artifactLocationURI=artifactLocation.toURI();
}","AppData(ApplicationClass appClass,Location artifactLocation){
  this.appClass=appClass;
  this.artifactLocationURI=null;
  this.artifactLocationPath=Locations.getRelativePath(artifactLocation);
}"
5534,"/** 
 * Get all plugin classes of the given type and name that extend the given parent artifact. Results are returned as a map from plugin artifact to plugins in that artifact.
 * @param parentArtifactId the id of the artifact to find plugins for
 * @param type the type of plugin to look for
 * @param name the name of the plugin to look for
 * @return an unmodifiable map of plugin artifact to plugin classes of the given type and name, accessible by thegiven artifact. The map will never be null, and will never be empty.
 * @throws PluginNotExistsException if no plugin with the given type and name exists in the namespace
 * @throws IOException if there was an exception reading metadata from the metastore
 */
public SortedMap<ArtifactDescriptor,PluginClass> getPluginClasses(final NamespaceId namespace,final Id.Artifact parentArtifactId,final String type,final String name) throws IOException, ArtifactNotFoundException, PluginNotExistsException {
  try {
    SortedMap<ArtifactDescriptor,PluginClass> result=Transactions.execute(transactional,new TxCallable<SortedMap<ArtifactDescriptor,PluginClass>>(){
      @Override public SortedMap<ArtifactDescriptor,PluginClass> call(      DatasetContext context) throws Exception {
        Table metaTable=getMetaTable(context);
        ArtifactCell parentCell=new ArtifactCell(parentArtifactId);
        byte[] parentDataBytes=metaTable.get(parentCell.rowkey,parentCell.column);
        if (parentDataBytes == null) {
          throw new ArtifactNotFoundException(parentArtifactId.toEntityId());
        }
        SortedMap<ArtifactDescriptor,PluginClass> plugins=new TreeMap<>();
        ArtifactData parentData=GSON.fromJson(Bytes.toString(parentDataBytes),ArtifactData.class);
        Set<PluginClass> parentPlugins=parentData.meta.getClasses().getPlugins();
        for (        PluginClass pluginClass : parentPlugins) {
          if (pluginClass.getName().equals(name) && pluginClass.getType().equals(type)) {
            ArtifactDescriptor parentDescriptor=new ArtifactDescriptor(parentArtifactId.toArtifactId(),locationFactory.create(parentData.locationURI));
            plugins.put(parentDescriptor,pluginClass);
            break;
          }
        }
        PluginKey pluginKey=new PluginKey(parentArtifactId.getNamespace(),parentArtifactId.getName(),type,name);
        Row row=metaTable.get(pluginKey.getRowKey());
        if (!row.isEmpty()) {
          for (          Map.Entry<byte[],byte[]> column : row.getColumns().entrySet()) {
            ImmutablePair<ArtifactDescriptor,PluginClass> pluginEntry=getPluginEntry(namespace,parentArtifactId,column);
            if (pluginEntry != null) {
              plugins.put(pluginEntry.getFirst(),pluginEntry.getSecond());
            }
          }
        }
        return Collections.unmodifiableSortedMap(plugins);
      }
    }
);
    if (result.isEmpty()) {
      throw new PluginNotExistsException(parentArtifactId.getNamespace(),type,name);
    }
    return result;
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,IOException.class,ArtifactNotFoundException.class);
  }
}","/** 
 * Get all plugin classes of the given type and name that extend the given parent artifact. Results are returned as a map from plugin artifact to plugins in that artifact.
 * @param parentArtifactId the id of the artifact to find plugins for
 * @param type the type of plugin to look for
 * @param name the name of the plugin to look for
 * @return an unmodifiable map of plugin artifact to plugin classes of the given type and name, accessible by thegiven artifact. The map will never be null, and will never be empty.
 * @throws PluginNotExistsException if no plugin with the given type and name exists in the namespace
 * @throws IOException if there was an exception reading metadata from the metastore
 */
public SortedMap<ArtifactDescriptor,PluginClass> getPluginClasses(final NamespaceId namespace,final Id.Artifact parentArtifactId,final String type,final String name) throws IOException, ArtifactNotFoundException, PluginNotExistsException {
  try {
    SortedMap<ArtifactDescriptor,PluginClass> result=Transactions.execute(transactional,new TxCallable<SortedMap<ArtifactDescriptor,PluginClass>>(){
      @Override public SortedMap<ArtifactDescriptor,PluginClass> call(      DatasetContext context) throws Exception {
        Table metaTable=getMetaTable(context);
        ArtifactCell parentCell=new ArtifactCell(parentArtifactId);
        byte[] parentDataBytes=metaTable.get(parentCell.rowkey,parentCell.column);
        if (parentDataBytes == null) {
          throw new ArtifactNotFoundException(parentArtifactId.toEntityId());
        }
        SortedMap<ArtifactDescriptor,PluginClass> plugins=new TreeMap<>();
        ArtifactData parentData=GSON.fromJson(Bytes.toString(parentDataBytes),ArtifactData.class);
        Set<PluginClass> parentPlugins=parentData.meta.getClasses().getPlugins();
        for (        PluginClass pluginClass : parentPlugins) {
          if (pluginClass.getName().equals(name) && pluginClass.getType().equals(type)) {
            ArtifactDescriptor parentDescriptor=new ArtifactDescriptor(parentArtifactId.toArtifactId(),Locations.getCompatibleLocation(locationFactory,parentData.locationPath,parentData.locationURI));
            plugins.put(parentDescriptor,pluginClass);
            break;
          }
        }
        PluginKey pluginKey=new PluginKey(parentArtifactId.getNamespace(),parentArtifactId.getName(),type,name);
        Row row=metaTable.get(pluginKey.getRowKey());
        if (!row.isEmpty()) {
          for (          Map.Entry<byte[],byte[]> column : row.getColumns().entrySet()) {
            ImmutablePair<ArtifactDescriptor,PluginClass> pluginEntry=getPluginEntry(namespace,parentArtifactId,column);
            if (pluginEntry != null) {
              plugins.put(pluginEntry.getFirst(),pluginEntry.getSecond());
            }
          }
        }
        return Collections.unmodifiableSortedMap(plugins);
      }
    }
);
    if (result.isEmpty()) {
      throw new PluginNotExistsException(parentArtifactId.getNamespace(),type,name);
    }
    return result;
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,IOException.class,ArtifactNotFoundException.class);
  }
}"
5535,"private void addArtifactsToList(List<ArtifactDetail> artifactDetails,Row row) throws IOException {
  ArtifactKey artifactKey=ArtifactKey.parse(row.getRow());
  for (  Map.Entry<byte[],byte[]> columnVal : row.getColumns().entrySet()) {
    String version=Bytes.toString(columnVal.getKey());
    ArtifactData data=GSON.fromJson(Bytes.toString(columnVal.getValue()),ArtifactData.class);
    Id.Artifact artifactId=Id.Artifact.from(artifactKey.namespace.toId(),artifactKey.name,version);
    artifactDetails.add(new ArtifactDetail(new ArtifactDescriptor(artifactId.toArtifactId(),locationFactory.create(data.locationURI)),data.meta));
  }
}","private void addArtifactsToList(List<ArtifactDetail> artifactDetails,Row row) throws IOException {
  ArtifactKey artifactKey=ArtifactKey.parse(row.getRow());
  for (  Map.Entry<byte[],byte[]> columnVal : row.getColumns().entrySet()) {
    String version=Bytes.toString(columnVal.getKey());
    ArtifactData data=GSON.fromJson(Bytes.toString(columnVal.getValue()),ArtifactData.class);
    Id.Artifact artifactId=Id.Artifact.from(artifactKey.namespace.toId(),artifactKey.name,version);
    artifactDetails.add(new ArtifactDetail(new ArtifactDescriptor(artifactId.toArtifactId(),Locations.getCompatibleLocation(locationFactory,data.locationPath,data.locationURI)),data.meta));
  }
}"
5536,"PluginData(PluginClass pluginClass,ArtifactRange usableBy,Location artifactLocation){
  this.pluginClass=pluginClass;
  this.usableBy=usableBy;
  this.artifactLocationURI=artifactLocation.toURI();
}","PluginData(PluginClass pluginClass,ArtifactRange usableBy,Location artifactLocation){
  this.pluginClass=pluginClass;
  this.usableBy=usableBy;
  this.artifactLocationURI=null;
  this.artifactLocationPath=Locations.getRelativePath(artifactLocation);
}"
5537,"/** 
 * Update artifact properties using an update function. Functions will receive an immutable map.
 * @param artifactId the id of the artifact to add
 * @param updateFunction the function used to update existing properties
 * @throws ArtifactNotFoundException if the artifact does not exist
 * @throws IOException if there was an exception writing the properties to the metastore
 */
public void updateArtifactProperties(final Id.Artifact artifactId,final Function<Map<String,String>,Map<String,String>> updateFunction) throws ArtifactNotFoundException, IOException {
  try {
    transactional.execute(new TxRunnable(){
      @Override public void run(      DatasetContext context) throws Exception {
        ArtifactCell artifactCell=new ArtifactCell(artifactId);
        Table metaTable=getMetaTable(context);
        byte[] existingMetaBytes=metaTable.get(artifactCell.rowkey,artifactCell.column);
        if (existingMetaBytes == null) {
          throw new ArtifactNotFoundException(artifactId.toEntityId());
        }
        ArtifactData old=GSON.fromJson(Bytes.toString(existingMetaBytes),ArtifactData.class);
        ArtifactMeta updatedMeta=new ArtifactMeta(old.meta.getClasses(),old.meta.getUsableBy(),updateFunction.apply(old.meta.getProperties()));
        ArtifactData updatedData=new ArtifactData(locationFactory.create(old.locationURI),updatedMeta);
        metaTable.put(artifactCell.rowkey,artifactCell.column,Bytes.toBytes(GSON.toJson(updatedData)));
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,ArtifactNotFoundException.class,IOException.class);
  }
}","/** 
 * Update artifact properties using an update function. Functions will receive an immutable map.
 * @param artifactId the id of the artifact to add
 * @param updateFunction the function used to update existing properties
 * @throws ArtifactNotFoundException if the artifact does not exist
 * @throws IOException if there was an exception writing the properties to the metastore
 */
public void updateArtifactProperties(final Id.Artifact artifactId,final Function<Map<String,String>,Map<String,String>> updateFunction) throws ArtifactNotFoundException, IOException {
  try {
    transactional.execute(new TxRunnable(){
      @Override public void run(      DatasetContext context) throws Exception {
        ArtifactCell artifactCell=new ArtifactCell(artifactId);
        Table metaTable=getMetaTable(context);
        byte[] existingMetaBytes=metaTable.get(artifactCell.rowkey,artifactCell.column);
        if (existingMetaBytes == null) {
          throw new ArtifactNotFoundException(artifactId.toEntityId());
        }
        ArtifactData old=GSON.fromJson(Bytes.toString(existingMetaBytes),ArtifactData.class);
        ArtifactMeta updatedMeta=new ArtifactMeta(old.meta.getClasses(),old.meta.getUsableBy(),updateFunction.apply(old.meta.getProperties()));
        ArtifactData updatedData=new ArtifactData(Locations.getCompatibleLocation(locationFactory,old.locationPath,old.locationURI),updatedMeta);
        metaTable.put(artifactCell.rowkey,artifactCell.column,Bytes.toBytes(GSON.toJson(updatedData)));
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,ArtifactNotFoundException.class,IOException.class);
  }
}"
5538,"/** 
 * Decode the PluginClass from the table column if it is from an artifact in the given namespace and extends the given parent artifact. If the plugin's artifact is not in the given namespace, or it does not extend the given parent artifact, return null.
 */
private ImmutablePair<ArtifactDescriptor,PluginClass> getPluginEntry(NamespaceId namespace,Id.Artifact parentArtifactId,Map.Entry<byte[],byte[]> column){
  ArtifactColumn artifactColumn=ArtifactColumn.parse(column.getKey());
  Id.Namespace artifactNamespace=artifactColumn.artifactId.getNamespace();
  if (!Id.Namespace.SYSTEM.equals(artifactNamespace) && !artifactNamespace.equals(namespace.toId())) {
    return null;
  }
  PluginData pluginData=GSON.fromJson(Bytes.toString(column.getValue()),PluginData.class);
  if (pluginData.usableBy.versionIsInRange(parentArtifactId.getVersion())) {
    ArtifactDescriptor artifactDescriptor=new ArtifactDescriptor(artifactColumn.artifactId.toArtifactId(),locationFactory.create(pluginData.artifactLocationURI));
    return ImmutablePair.of(artifactDescriptor,pluginData.pluginClass);
  }
  return null;
}","/** 
 * Decode the PluginClass from the table column if it is from an artifact in the given namespace and extends the given parent artifact. If the plugin's artifact is not in the given namespace, or it does not extend the given parent artifact, return null.
 */
private ImmutablePair<ArtifactDescriptor,PluginClass> getPluginEntry(NamespaceId namespace,Id.Artifact parentArtifactId,Map.Entry<byte[],byte[]> column){
  ArtifactColumn artifactColumn=ArtifactColumn.parse(column.getKey());
  Id.Namespace artifactNamespace=artifactColumn.artifactId.getNamespace();
  if (!Id.Namespace.SYSTEM.equals(artifactNamespace) && !artifactNamespace.equals(namespace.toId())) {
    return null;
  }
  PluginData pluginData=GSON.fromJson(Bytes.toString(column.getValue()),PluginData.class);
  if (pluginData.usableBy.versionIsInRange(parentArtifactId.getVersion())) {
    ArtifactDescriptor artifactDescriptor=new ArtifactDescriptor(artifactColumn.artifactId.toArtifactId(),Locations.getCompatibleLocation(locationFactory,pluginData.artifactLocationPath,pluginData.artifactLocationURI));
    return ImmutablePair.of(artifactDescriptor,pluginData.pluginClass);
  }
  return null;
}"
5539,"private void writeMeta(Table table,Id.Artifact artifactId,ArtifactData data) throws IOException {
  ArtifactCell artifactCell=new ArtifactCell(artifactId);
  table.put(artifactCell.rowkey,artifactCell.column,Bytes.toBytes(GSON.toJson(data)));
  byte[] artifactColumn=new ArtifactColumn(artifactId).getColumn();
  ArtifactClasses classes=data.meta.getClasses();
  Location artifactLocation=locationFactory.create(data.locationURI);
  for (  PluginClass pluginClass : classes.getPlugins()) {
    for (    ArtifactRange artifactRange : data.meta.getUsableBy()) {
      PluginKey pluginKey=new PluginKey(artifactRange.getNamespace().toId(),artifactRange.getName(),pluginClass.getType(),pluginClass.getName());
      byte[] pluginDataBytes=Bytes.toBytes(GSON.toJson(new PluginData(pluginClass,artifactRange,artifactLocation)));
      table.put(pluginKey.getRowKey(),artifactColumn,pluginDataBytes);
    }
  }
  for (  ApplicationClass appClass : classes.getApps()) {
    AppClassKey appClassKey=new AppClassKey(artifactId.getNamespace().toEntityId(),appClass.getClassName());
    byte[] appDataBytes=Bytes.toBytes(GSON.toJson(new AppData(appClass,artifactLocation)));
    table.put(appClassKey.getRowKey(),artifactColumn,appDataBytes);
  }
}","private void writeMeta(Table table,Id.Artifact artifactId,ArtifactData data) throws IOException {
  ArtifactCell artifactCell=new ArtifactCell(artifactId);
  table.put(artifactCell.rowkey,artifactCell.column,Bytes.toBytes(GSON.toJson(data)));
  byte[] artifactColumn=new ArtifactColumn(artifactId).getColumn();
  ArtifactClasses classes=data.meta.getClasses();
  Location artifactLocation=Locations.getCompatibleLocation(locationFactory,data.locationPath,data.locationURI);
  for (  PluginClass pluginClass : classes.getPlugins()) {
    for (    ArtifactRange artifactRange : data.meta.getUsableBy()) {
      PluginKey pluginKey=new PluginKey(artifactRange.getNamespace().toId(),artifactRange.getName(),pluginClass.getType(),pluginClass.getName());
      byte[] pluginDataBytes=Bytes.toBytes(GSON.toJson(new PluginData(pluginClass,artifactRange,artifactLocation)));
      table.put(pluginKey.getRowKey(),artifactColumn,pluginDataBytes);
    }
  }
  for (  ApplicationClass appClass : classes.getApps()) {
    AppClassKey appClassKey=new AppClassKey(artifactId.getNamespace().toEntityId(),appClass.getClassName());
    byte[] appDataBytes=Bytes.toBytes(GSON.toJson(new AppData(appClass,artifactLocation)));
    table.put(appClassKey.getRowKey(),artifactColumn,appDataBytes);
  }
}"
5540,"@Override public Void call() throws Exception {
  locationFactory.create(oldMeta.locationURI).delete();
  return null;
}","@Override public Void call() throws Exception {
  Locations.getCompatibleLocation(locationFactory,oldMeta.locationPath,oldMeta.locationURI).delete();
  return null;
}"
5541,"private SortedMap<ArtifactDescriptor,Set<PluginClass>> getPluginsInArtifact(Table table,Id.Artifact artifactId,Predicate<PluginClass> filter){
  SortedMap<ArtifactDescriptor,Set<PluginClass>> result=new TreeMap<>();
  ArtifactCell parentCell=new ArtifactCell(artifactId);
  byte[] parentDataBytes=table.get(parentCell.rowkey,parentCell.column);
  if (parentDataBytes == null) {
    return null;
  }
  ArtifactData parentData=GSON.fromJson(Bytes.toString(parentDataBytes),ArtifactData.class);
  Set<PluginClass> parentPlugins=parentData.meta.getClasses().getPlugins();
  Set<PluginClass> filteredPlugins=Sets.newLinkedHashSet(Iterables.filter(parentPlugins,filter));
  if (!filteredPlugins.isEmpty()) {
    Location parentLocation=locationFactory.create(parentData.locationURI);
    ArtifactDescriptor descriptor=new ArtifactDescriptor(artifactId.toArtifactId(),parentLocation);
    result.put(descriptor,filteredPlugins);
  }
  return result;
}","private SortedMap<ArtifactDescriptor,Set<PluginClass>> getPluginsInArtifact(Table table,Id.Artifact artifactId,Predicate<PluginClass> filter){
  SortedMap<ArtifactDescriptor,Set<PluginClass>> result=new TreeMap<>();
  ArtifactCell parentCell=new ArtifactCell(artifactId);
  byte[] parentDataBytes=table.get(parentCell.rowkey,parentCell.column);
  if (parentDataBytes == null) {
    return null;
  }
  ArtifactData parentData=GSON.fromJson(Bytes.toString(parentDataBytes),ArtifactData.class);
  Set<PluginClass> parentPlugins=parentData.meta.getClasses().getPlugins();
  Set<PluginClass> filteredPlugins=Sets.newLinkedHashSet(Iterables.filter(parentPlugins,filter));
  if (!filteredPlugins.isEmpty()) {
    Location parentLocation=Locations.getCompatibleLocation(locationFactory,parentData.locationPath,parentData.locationURI);
    ArtifactDescriptor descriptor=new ArtifactDescriptor(artifactId.toArtifactId(),parentLocation);
    result.put(descriptor,filteredPlugins);
  }
  return result;
}"
5542,"private void deleteMeta(Table table,Id.Artifact artifactId,byte[] oldData) throws IOException {
  ArtifactCell artifactCell=new ArtifactCell(artifactId);
  table.delete(artifactCell.rowkey,artifactCell.column);
  final ArtifactData oldMeta=GSON.fromJson(Bytes.toString(oldData),ArtifactData.class);
  byte[] artifactColumn=new ArtifactColumn(artifactId).getColumn();
  for (  PluginClass pluginClass : oldMeta.meta.getClasses().getPlugins()) {
    for (    ArtifactRange artifactRange : oldMeta.meta.getUsableBy()) {
      PluginKey pluginKey=new PluginKey(artifactRange.getNamespace().toId(),artifactRange.getName(),pluginClass.getType(),pluginClass.getName());
      table.delete(pluginKey.getRowKey(),artifactColumn);
    }
  }
  for (  ApplicationClass appClass : oldMeta.meta.getClasses().getApps()) {
    AppClassKey appClassKey=new AppClassKey(artifactId.getNamespace().toEntityId(),appClass.getClassName());
    table.delete(appClassKey.getRowKey(),artifactColumn);
  }
  try {
    new NamespacedImpersonator(artifactId.getNamespace().toEntityId(),impersonator).impersonate(new Callable<Void>(){
      @Override public Void call() throws Exception {
        locationFactory.create(oldMeta.locationURI).delete();
        return null;
      }
    }
);
  }
 catch (  IOException ioe) {
    throw ioe;
  }
catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","private void deleteMeta(Table table,Id.Artifact artifactId,byte[] oldData) throws IOException {
  ArtifactCell artifactCell=new ArtifactCell(artifactId);
  table.delete(artifactCell.rowkey,artifactCell.column);
  final ArtifactData oldMeta=GSON.fromJson(Bytes.toString(oldData),ArtifactData.class);
  byte[] artifactColumn=new ArtifactColumn(artifactId).getColumn();
  for (  PluginClass pluginClass : oldMeta.meta.getClasses().getPlugins()) {
    for (    ArtifactRange artifactRange : oldMeta.meta.getUsableBy()) {
      PluginKey pluginKey=new PluginKey(artifactRange.getNamespace().toId(),artifactRange.getName(),pluginClass.getType(),pluginClass.getName());
      table.delete(pluginKey.getRowKey(),artifactColumn);
    }
  }
  for (  ApplicationClass appClass : oldMeta.meta.getClasses().getApps()) {
    AppClassKey appClassKey=new AppClassKey(artifactId.getNamespace().toEntityId(),appClass.getClassName());
    table.delete(appClassKey.getRowKey(),artifactColumn);
  }
  try {
    new NamespacedImpersonator(artifactId.getNamespace().toEntityId(),impersonator).impersonate(new Callable<Void>(){
      @Override public Void call() throws Exception {
        Locations.getCompatibleLocation(locationFactory,oldMeta.locationPath,oldMeta.locationURI).delete();
        return null;
      }
    }
);
  }
 catch (  IOException ioe) {
    throw ioe;
  }
catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
5543,"/** 
 * Get all application classes that belong to the specified namespace of the specified classname. Results are returned as a sorted map from artifact to application classes in that artifact. Map entries are sorted by the artifact.
 * @param namespace the namespace from which to get application classes
 * @param className the classname of application classes to get
 * @return an unmodifiable map of artifact the application classes in that artifact.The map will never be null. If there are no application classes, an empty map will be returned.
 */
public SortedMap<ArtifactDescriptor,ApplicationClass> getApplicationClasses(final NamespaceId namespace,final String className){
  try {
    return Transactions.execute(transactional,new TxCallable<SortedMap<ArtifactDescriptor,ApplicationClass>>(){
      @Override public SortedMap<ArtifactDescriptor,ApplicationClass> call(      DatasetContext context) throws Exception {
        SortedMap<ArtifactDescriptor,ApplicationClass> result=Maps.newTreeMap();
        Row row=getMetaTable(context).get(new AppClassKey(namespace,className).getRowKey());
        if (!row.isEmpty()) {
          for (          Map.Entry<byte[],byte[]> column : row.getColumns().entrySet()) {
            ArtifactColumn artifactColumn=ArtifactColumn.parse(column.getKey());
            AppData appData=GSON.fromJson(Bytes.toString(column.getValue()),AppData.class);
            ArtifactDescriptor artifactDescriptor=new ArtifactDescriptor(artifactColumn.artifactId.toArtifactId(),locationFactory.create(appData.artifactLocationURI));
            result.put(artifactDescriptor,appData.appClass);
          }
        }
        return Collections.unmodifiableSortedMap(result);
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e);
  }
}","/** 
 * Get all application classes that belong to the specified namespace of the specified classname. Results are returned as a sorted map from artifact to application classes in that artifact. Map entries are sorted by the artifact.
 * @param namespace the namespace from which to get application classes
 * @param className the classname of application classes to get
 * @return an unmodifiable map of artifact the application classes in that artifact.The map will never be null. If there are no application classes, an empty map will be returned.
 */
public SortedMap<ArtifactDescriptor,ApplicationClass> getApplicationClasses(final NamespaceId namespace,final String className){
  try {
    return Transactions.execute(transactional,new TxCallable<SortedMap<ArtifactDescriptor,ApplicationClass>>(){
      @Override public SortedMap<ArtifactDescriptor,ApplicationClass> call(      DatasetContext context) throws Exception {
        SortedMap<ArtifactDescriptor,ApplicationClass> result=Maps.newTreeMap();
        Row row=getMetaTable(context).get(new AppClassKey(namespace,className).getRowKey());
        if (!row.isEmpty()) {
          for (          Map.Entry<byte[],byte[]> column : row.getColumns().entrySet()) {
            ArtifactColumn artifactColumn=ArtifactColumn.parse(column.getKey());
            AppData appData=GSON.fromJson(Bytes.toString(column.getValue()),AppData.class);
            ArtifactDescriptor artifactDescriptor=new ArtifactDescriptor(artifactColumn.artifactId.toArtifactId(),Locations.getCompatibleLocation(locationFactory,appData.artifactLocationPath,appData.artifactLocationURI));
            result.put(artifactDescriptor,appData.appClass);
          }
        }
        return Collections.unmodifiableSortedMap(result);
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e);
  }
}"
5544,"/** 
 * Retrieve the   {@link UserGroupInformation} for the given {@link NamespaceId}
 * @param namespaceId namespace to lookup the user
 * @return {@link UserGroupInformation}
 * @throws IOException if there was any error fetching the {@link UserGroupInformation}
 */
UserGroupInformation getUGI(NamespaceId namespaceId) throws IOException ;","/** 
 * Retrieve the   {@link UserGroupInformation} for the given {@link NamespaceId}
 * @param namespaceId namespace to lookup the user
 * @return {@link UserGroupInformation}
 * @throws IOException if there was any error fetching the {@link UserGroupInformation}
 * @throws NamespaceNotFoundException if namespaceId does not exist
 */
UserGroupInformation getUGI(NamespaceId namespaceId) throws IOException, NamespaceNotFoundException ;"
5545,"@Override public StreamConfig getConfig(final StreamId streamId) throws IOException {
  UserGroupInformation ugi=impersonator.getUGI(streamId.getParent());
  try {
    return ImpersonationUtils.doAs(ugi,new Callable<StreamConfig>(){
      @Override public StreamConfig call() throws IOException {
        Location configLocation=getConfigLocation(streamId);
        if (!configLocation.exists()) {
          throw new FileNotFoundException(String.format(""String_Node_Str"",configLocation.toURI().getPath(),streamId));
        }
        StreamConfig config=GSON.fromJson(CharStreams.toString(CharStreams.newReaderSupplier(Locations.newInputSupplier(configLocation),Charsets.UTF_8)),StreamConfig.class);
        int threshold=config.getNotificationThresholdMB();
        if (threshold <= 0) {
          threshold=cConf.getInt(Constants.Stream.NOTIFICATION_THRESHOLD);
        }
        return new StreamConfig(streamId,config.getPartitionDuration(),config.getIndexInterval(),config.getTTL(),getStreamLocation(streamId),config.getFormat(),threshold);
      }
    }
);
  }
 catch (  Exception ex) {
    Throwables.propagateIfPossible(ex,IOException.class);
    throw new IOException(ex);
  }
}","@Override public StreamConfig getConfig(final StreamId streamId) throws IOException {
  try {
    UserGroupInformation ugi=impersonator.getUGI(streamId.getParent());
    return ImpersonationUtils.doAs(ugi,new Callable<StreamConfig>(){
      @Override public StreamConfig call() throws IOException {
        Location configLocation=getConfigLocation(streamId);
        if (!configLocation.exists()) {
          throw new FileNotFoundException(String.format(""String_Node_Str"",configLocation.toURI().getPath(),streamId));
        }
        StreamConfig config=GSON.fromJson(CharStreams.toString(CharStreams.newReaderSupplier(Locations.newInputSupplier(configLocation),Charsets.UTF_8)),StreamConfig.class);
        int threshold=config.getNotificationThresholdMB();
        if (threshold <= 0) {
          threshold=cConf.getInt(Constants.Stream.NOTIFICATION_THRESHOLD);
        }
        return new StreamConfig(streamId,config.getPartitionDuration(),config.getIndexInterval(),config.getTTL(),getStreamLocation(streamId),config.getFormat(),threshold);
      }
    }
);
  }
 catch (  Exception ex) {
    Throwables.propagateIfPossible(ex,IOException.class);
    throw new IOException(ex);
  }
}"
5546,"private void writeSystemMetadataForDatasets(NamespaceId namespace,DatasetFramework dsFramework) throws DatasetManagementException, IOException {
  SystemDatasetInstantiatorFactory systemDatasetInstantiatorFactory=new SystemDatasetInstantiatorFactory(locationFactory,dsFramework,cConf);
  try (SystemDatasetInstantiator systemDatasetInstantiator=systemDatasetInstantiatorFactory.create()){
    UserGroupInformation ugi=impersonator.getUGI(namespace);
    for (    DatasetSpecificationSummary summary : dsFramework.getInstances(namespace)) {
      final DatasetId dsInstance=namespace.dataset(summary.getName());
      DatasetProperties dsProperties=DatasetProperties.of(summary.getProperties());
      String dsType=summary.getType();
      Dataset dataset=null;
      try {
        try {
          dataset=ImpersonationUtils.doAs(ugi,new Callable<Dataset>(){
            @Override public Dataset call() throws Exception {
              return systemDatasetInstantiator.getDataset(dsInstance);
            }
          }
);
        }
 catch (        Exception e) {
          LOG.warn(""String_Node_Str"",dsInstance,e);
        }
        SystemMetadataWriter writer=new DatasetSystemMetadataWriter(metadataStore,dsInstance,dsProperties,dataset,dsType,summary.getDescription());
        writer.write();
      }
  finally {
        if (dataset != null) {
          dataset.close();
        }
      }
    }
  }
 }","private void writeSystemMetadataForDatasets(NamespaceId namespace,DatasetFramework dsFramework) throws DatasetManagementException, IOException, NamespaceNotFoundException {
  SystemDatasetInstantiatorFactory systemDatasetInstantiatorFactory=new SystemDatasetInstantiatorFactory(locationFactory,dsFramework,cConf);
  try (SystemDatasetInstantiator systemDatasetInstantiator=systemDatasetInstantiatorFactory.create()){
    UserGroupInformation ugi=impersonator.getUGI(namespace);
    for (    DatasetSpecificationSummary summary : dsFramework.getInstances(namespace)) {
      final DatasetId dsInstance=namespace.dataset(summary.getName());
      DatasetProperties dsProperties=DatasetProperties.of(summary.getProperties());
      String dsType=summary.getType();
      Dataset dataset=null;
      try {
        try {
          dataset=ImpersonationUtils.doAs(ugi,new Callable<Dataset>(){
            @Override public Dataset call() throws Exception {
              return systemDatasetInstantiator.getDataset(dsInstance);
            }
          }
);
        }
 catch (        Exception e) {
          LOG.warn(""String_Node_Str"",dsInstance,e);
        }
        SystemMetadataWriter writer=new DatasetSystemMetadataWriter(metadataStore,dsInstance,dsProperties,dataset,dsType,summary.getDescription());
        writer.write();
      }
  finally {
        if (dataset != null) {
          dataset.close();
        }
      }
    }
  }
 }"
5547,"@Override public Integer apply(Table table) throws Exception {
  int deletedColumns=0;
  try (Scanner scanner=table.scan(ROW_KEY_PREFIX,ROW_KEY_PREFIX_END)){
    Row row;
    while ((row=scanner.next()) != null) {
      byte[] rowKey=row.getRow();
      final NamespaceId namespaceId=getNamespaceId(rowKey);
      String namespacedLogDir=impersonator.doAs(namespaceId,new Callable<String>(){
        @Override public String call() throws Exception {
          return LoggingContextHelper.getNamespacedBaseDirLocation(namespacedLocationFactory,logBaseDir,namespaceId,impersonator).toString();
        }
      }
);
      for (      final Map.Entry<byte[],byte[]> entry : row.getColumns().entrySet()) {
        try {
          byte[] colName=entry.getKey();
          URI file=new URI(Bytes.toString(entry.getValue()));
          if (LOG.isDebugEnabled()) {
            LOG.debug(""String_Node_Str"",file,Bytes.toLong(colName));
          }
          Location fileLocation=impersonator.doAs(namespaceId,new Callable<Location>(){
            @Override public Location call() throws Exception {
              return rootLocationFactory.create(new URI(Bytes.toString(entry.getValue())));
            }
          }
);
          if (!fileLocation.exists()) {
            LOG.warn(""String_Node_Str"",file);
            table.delete(rowKey,colName);
            deletedColumns++;
          }
 else           if (fileLocation.lastModified() < tillTime) {
            callback.handle(namespaceId,fileLocation,namespacedLogDir);
            table.delete(rowKey,colName);
            deletedColumns++;
          }
        }
 catch (        Exception e) {
          LOG.error(""String_Node_Str"",Bytes.toString(entry.getValue()),e);
        }
      }
    }
  }
   return deletedColumns;
}","@Override public Integer apply(Table table) throws Exception {
  int deletedColumns=0;
  try (Scanner scanner=table.scan(ROW_KEY_PREFIX,ROW_KEY_PREFIX_END)){
    Row row;
    while ((row=scanner.next()) != null) {
      byte[] rowKey=row.getRow();
      final NamespaceId namespaceId=getNamespaceId(rowKey);
      String namespacedLogDir=null;
      try {
        namespacedLogDir=impersonator.doAs(namespaceId,new Callable<String>(){
          @Override public String call() throws Exception {
            return LoggingContextHelper.getNamespacedBaseDirLocation(namespacedLocationFactory,logBaseDir,namespaceId,impersonator).toString();
          }
        }
);
      }
 catch (      Exception e) {
        if (e instanceof NamespaceNotFoundException) {
          LOG.warn(""String_Node_Str"",namespaceId.getEntityName(),e);
        }
 else {
          LOG.warn(""String_Node_Str"",namespaceId.getEntityName(),e);
          continue;
        }
      }
      for (      final Map.Entry<byte[],byte[]> entry : row.getColumns().entrySet()) {
        try {
          byte[] colName=entry.getKey();
          URI file=new URI(Bytes.toString(entry.getValue()));
          if (LOG.isDebugEnabled()) {
            LOG.debug(""String_Node_Str"",file,Bytes.toLong(colName));
          }
          if (Strings.isNullOrEmpty(namespacedLogDir)) {
            LOG.warn(""String_Node_Str"" + ""String_Node_Str"",Bytes.toString(entry.getValue()),namespaceId.getEntityName());
            table.delete(rowKey,colName);
            deletedColumns++;
            continue;
          }
          Location fileLocation=impersonator.doAs(namespaceId,new Callable<Location>(){
            @Override public Location call() throws Exception {
              return rootLocationFactory.create(new URI(Bytes.toString(entry.getValue())));
            }
          }
);
          if (!fileLocation.exists()) {
            LOG.warn(""String_Node_Str"",file);
            table.delete(rowKey,colName);
            deletedColumns++;
          }
 else           if (fileLocation.lastModified() < tillTime) {
            callback.handle(namespaceId,fileLocation,namespacedLogDir);
            table.delete(rowKey,colName);
            deletedColumns++;
          }
        }
 catch (        Exception e) {
          if (e instanceof NamespaceNotFoundException) {
            LOG.warn(""String_Node_Str"",Bytes.toString(entry.getValue()),namespaceId.getEntityName());
          }
 else {
            LOG.error(""String_Node_Str"",Bytes.toString(entry.getValue()),e);
          }
        }
      }
    }
  }
   return deletedColumns;
}"
5548,"/** 
 * Deletes meta data until a given time, while keeping the latest meta data even if less than tillTime.
 * @param tillTime time till the meta data will be deleted.
 * @param callback callback called before deleting a meta data column.
 * @return total number of columns deleted.
 */
public int cleanMetaData(final long tillTime,final DeleteCallback callback) throws Exception {
  return execute(new TransactionExecutor.Function<Table,Integer>(){
    @Override public Integer apply(    Table table) throws Exception {
      int deletedColumns=0;
      try (Scanner scanner=table.scan(ROW_KEY_PREFIX,ROW_KEY_PREFIX_END)){
        Row row;
        while ((row=scanner.next()) != null) {
          byte[] rowKey=row.getRow();
          final NamespaceId namespaceId=getNamespaceId(rowKey);
          String namespacedLogDir=impersonator.doAs(namespaceId,new Callable<String>(){
            @Override public String call() throws Exception {
              return LoggingContextHelper.getNamespacedBaseDirLocation(namespacedLocationFactory,logBaseDir,namespaceId,impersonator).toString();
            }
          }
);
          for (          final Map.Entry<byte[],byte[]> entry : row.getColumns().entrySet()) {
            try {
              byte[] colName=entry.getKey();
              URI file=new URI(Bytes.toString(entry.getValue()));
              if (LOG.isDebugEnabled()) {
                LOG.debug(""String_Node_Str"",file,Bytes.toLong(colName));
              }
              Location fileLocation=impersonator.doAs(namespaceId,new Callable<Location>(){
                @Override public Location call() throws Exception {
                  return rootLocationFactory.create(new URI(Bytes.toString(entry.getValue())));
                }
              }
);
              if (!fileLocation.exists()) {
                LOG.warn(""String_Node_Str"",file);
                table.delete(rowKey,colName);
                deletedColumns++;
              }
 else               if (fileLocation.lastModified() < tillTime) {
                callback.handle(namespaceId,fileLocation,namespacedLogDir);
                table.delete(rowKey,colName);
                deletedColumns++;
              }
            }
 catch (            Exception e) {
              LOG.error(""String_Node_Str"",Bytes.toString(entry.getValue()),e);
            }
          }
        }
      }
       return deletedColumns;
    }
  }
);
}","/** 
 * Deletes meta data until a given time, while keeping the latest meta data even if less than tillTime.
 * @param tillTime time till the meta data will be deleted.
 * @param callback callback called before deleting a meta data column.
 * @return total number of columns deleted.
 */
public int cleanMetaData(final long tillTime,final DeleteCallback callback) throws Exception {
  return execute(new TransactionExecutor.Function<Table,Integer>(){
    @Override public Integer apply(    Table table) throws Exception {
      int deletedColumns=0;
      try (Scanner scanner=table.scan(ROW_KEY_PREFIX,ROW_KEY_PREFIX_END)){
        Row row;
        while ((row=scanner.next()) != null) {
          byte[] rowKey=row.getRow();
          final NamespaceId namespaceId=getNamespaceId(rowKey);
          String namespacedLogDir=null;
          try {
            namespacedLogDir=impersonator.doAs(namespaceId,new Callable<String>(){
              @Override public String call() throws Exception {
                return LoggingContextHelper.getNamespacedBaseDirLocation(namespacedLocationFactory,logBaseDir,namespaceId,impersonator).toString();
              }
            }
);
          }
 catch (          Exception e) {
            if (e instanceof NamespaceNotFoundException) {
              LOG.warn(""String_Node_Str"",namespaceId.getEntityName(),e);
            }
 else {
              LOG.warn(""String_Node_Str"",namespaceId.getEntityName(),e);
              continue;
            }
          }
          for (          final Map.Entry<byte[],byte[]> entry : row.getColumns().entrySet()) {
            try {
              byte[] colName=entry.getKey();
              URI file=new URI(Bytes.toString(entry.getValue()));
              if (LOG.isDebugEnabled()) {
                LOG.debug(""String_Node_Str"",file,Bytes.toLong(colName));
              }
              if (Strings.isNullOrEmpty(namespacedLogDir)) {
                LOG.warn(""String_Node_Str"" + ""String_Node_Str"",Bytes.toString(entry.getValue()),namespaceId.getEntityName());
                table.delete(rowKey,colName);
                deletedColumns++;
                continue;
              }
              Location fileLocation=impersonator.doAs(namespaceId,new Callable<Location>(){
                @Override public Location call() throws Exception {
                  return rootLocationFactory.create(new URI(Bytes.toString(entry.getValue())));
                }
              }
);
              if (!fileLocation.exists()) {
                LOG.warn(""String_Node_Str"",file);
                table.delete(rowKey,colName);
                deletedColumns++;
              }
 else               if (fileLocation.lastModified() < tillTime) {
                callback.handle(namespaceId,fileLocation,namespacedLogDir);
                table.delete(rowKey,colName);
                deletedColumns++;
              }
            }
 catch (            Exception e) {
              if (e instanceof NamespaceNotFoundException) {
                LOG.warn(""String_Node_Str"",Bytes.toString(entry.getValue()),namespaceId.getEntityName());
              }
 else {
                LOG.error(""String_Node_Str"",Bytes.toString(entry.getValue()),e);
              }
            }
          }
        }
      }
       return deletedColumns;
    }
  }
);
}"
5549,"private MetadataSearchResponse search(Set<MetadataScope> scopes,String namespaceId,String searchQuery,Set<MetadataSearchTargetType> types,SortInfo sortInfo,int offset,int limit,int numCursors,String cursor,boolean showHidden) throws BadRequestException {
  List<MetadataEntry> results=new ArrayList<>();
  List<String> cursors=new ArrayList<>();
  for (  MetadataScope scope : scopes) {
    SearchResults searchResults=getSearchResults(scope,namespaceId,searchQuery,types,sortInfo,offset,limit,numCursors,cursor,showHidden);
    results.addAll(searchResults.getResults());
    cursors.addAll(searchResults.getCursors());
  }
  Set<NamespacedEntityId> sortedEntities=getSortedEntities(results,sortInfo);
  int startIndex=0;
  int maxEndIndex;
  int total=sortedEntities.size();
  if (SortInfo.DEFAULT.equals(sortInfo)) {
    if (offset > sortedEntities.size()) {
      maxEndIndex=0;
    }
 else {
      startIndex=offset;
      maxEndIndex=offset + limit;
    }
  }
 else {
    maxEndIndex=limit;
    total+=offset;
  }
  sortedEntities=new LinkedHashSet<>(ImmutableList.copyOf(sortedEntities).subList(startIndex,Math.min(maxEndIndex,sortedEntities.size())));
  Map<NamespacedEntityId,Metadata> systemMetadata=fetchMetadata(sortedEntities,MetadataScope.SYSTEM);
  Map<NamespacedEntityId,Metadata> userMetadata=fetchMetadata(sortedEntities,MetadataScope.USER);
  return new MetadataSearchResponse(sortInfo.getSortBy() + ""String_Node_Str"" + sortInfo.getSortOrder(),offset,limit,numCursors,total,addMetadataToEntities(sortedEntities,systemMetadata,userMetadata),cursors,showHidden);
}","private MetadataSearchResponse search(Set<MetadataScope> scopes,String namespaceId,String searchQuery,Set<MetadataSearchTargetType> types,SortInfo sortInfo,int offset,int limit,int numCursors,String cursor,boolean showHidden) throws BadRequestException {
  List<MetadataEntry> results=new ArrayList<>();
  List<String> cursors=new ArrayList<>();
  for (  MetadataScope scope : scopes) {
    SearchResults searchResults=getSearchResults(scope,namespaceId,searchQuery,types,sortInfo,offset,limit,numCursors,cursor,showHidden);
    results.addAll(searchResults.getResults());
    cursors.addAll(searchResults.getCursors());
  }
  Set<NamespacedEntityId> sortedEntities=getSortedEntities(results,sortInfo);
  int startIndex=0;
  int maxEndIndex;
  int total=sortedEntities.size();
  if (SortInfo.DEFAULT.equals(sortInfo)) {
    if (offset > sortedEntities.size()) {
      maxEndIndex=0;
    }
 else {
      startIndex=offset;
      maxEndIndex=(int)Math.min(Integer.MAX_VALUE,(long)offset + limit);
    }
  }
 else {
    maxEndIndex=limit;
    total+=offset;
  }
  sortedEntities=new LinkedHashSet<>(ImmutableList.copyOf(sortedEntities).subList(startIndex,Math.min(maxEndIndex,sortedEntities.size())));
  Map<NamespacedEntityId,Metadata> systemMetadata=fetchMetadata(sortedEntities,MetadataScope.SYSTEM);
  Map<NamespacedEntityId,Metadata> userMetadata=fetchMetadata(sortedEntities,MetadataScope.USER);
  return new MetadataSearchResponse(sortInfo.getSortBy() + ""String_Node_Str"" + sortInfo.getSortOrder(),offset,limit,numCursors,total,addMetadataToEntities(sortedEntities,systemMetadata,userMetadata),cursors,showHidden);
}"
5550,"@Test public void testSearchPagination() throws BadRequestException {
  NamespaceId ns=new NamespaceId(""String_Node_Str"");
  ProgramId flow=ns.app(""String_Node_Str"").flow(""String_Node_Str"");
  StreamId stream=ns.stream(""String_Node_Str"");
  DatasetId dataset=ns.dataset(""String_Node_Str"");
  DatasetId trackerDataset=ns.dataset(""String_Node_Str"");
  store.addTags(MetadataScope.USER,flow,""String_Node_Str"",""String_Node_Str"");
  store.addTags(MetadataScope.USER,stream,""String_Node_Str"",""String_Node_Str"");
  store.addTags(MetadataScope.USER,dataset,""String_Node_Str"",""String_Node_Str"");
  store.addTags(MetadataScope.USER,trackerDataset,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  MetadataSearchResultRecord flowSearchResult=new MetadataSearchResultRecord(flow);
  MetadataSearchResultRecord streamSearchResult=new MetadataSearchResultRecord(stream);
  MetadataSearchResultRecord datasetSearchResult=new MetadataSearchResultRecord(dataset);
  MetadataSearchResultRecord trackerDatasetSearchResult=new MetadataSearchResultRecord(trackerDataset);
  MetadataSearchResponse response=search(ns.getNamespace(),""String_Node_Str"",0,Integer.MAX_VALUE,1);
  Assert.assertEquals(3,response.getTotal());
  Assert.assertEquals(ImmutableList.of(datasetSearchResult,streamSearchResult,flowSearchResult),ImmutableList.copyOf(stripMetadata(response.getResults())));
  response=search(ns.getNamespace(),""String_Node_Str"",0,Integer.MAX_VALUE,1,true);
  Assert.assertEquals(4,response.getTotal());
  Assert.assertEquals(ImmutableList.of(trackerDatasetSearchResult,datasetSearchResult,streamSearchResult,flowSearchResult),ImmutableList.copyOf(stripMetadata(response.getResults())));
  response=search(ns.getNamespace(),""String_Node_Str"",0,2,1);
  Assert.assertEquals(3,response.getTotal());
  Assert.assertEquals(ImmutableList.of(datasetSearchResult,streamSearchResult),ImmutableList.copyOf(stripMetadata(response.getResults())));
  response=search(ns.getNamespace(),""String_Node_Str"",1,2,1);
  Assert.assertEquals(3,response.getTotal());
  Assert.assertEquals(ImmutableList.of(streamSearchResult,flowSearchResult),ImmutableList.copyOf(stripMetadata(response.getResults())));
  response=search(ns.getNamespace(),""String_Node_Str"",1,3,1,true);
  Assert.assertEquals(4,response.getTotal());
  Assert.assertEquals(ImmutableList.of(datasetSearchResult,streamSearchResult,flowSearchResult),ImmutableList.copyOf(stripMetadata(response.getResults())));
  response=search(ns.getNamespace(),""String_Node_Str"",2,2,1);
  Assert.assertEquals(3,response.getTotal());
  Assert.assertEquals(ImmutableList.of(flowSearchResult),ImmutableList.copyOf(stripMetadata(response.getResults())));
  response=search(ns.getNamespace(),""String_Node_Str"",4,2,1);
  Assert.assertEquals(3,response.getTotal());
  Assert.assertEquals(ImmutableList.<MetadataSearchResultRecord>of(),ImmutableList.copyOf(stripMetadata(response.getResults())));
}","@Test public void testSearchPagination() throws BadRequestException {
  NamespaceId ns=new NamespaceId(""String_Node_Str"");
  ProgramId flow=ns.app(""String_Node_Str"").flow(""String_Node_Str"");
  StreamId stream=ns.stream(""String_Node_Str"");
  DatasetId dataset=ns.dataset(""String_Node_Str"");
  DatasetId trackerDataset=ns.dataset(""String_Node_Str"");
  store.addTags(MetadataScope.USER,flow,""String_Node_Str"",""String_Node_Str"");
  store.addTags(MetadataScope.USER,stream,""String_Node_Str"",""String_Node_Str"");
  store.addTags(MetadataScope.USER,dataset,""String_Node_Str"",""String_Node_Str"");
  store.addTags(MetadataScope.USER,trackerDataset,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  MetadataSearchResultRecord flowSearchResult=new MetadataSearchResultRecord(flow);
  MetadataSearchResultRecord streamSearchResult=new MetadataSearchResultRecord(stream);
  MetadataSearchResultRecord datasetSearchResult=new MetadataSearchResultRecord(dataset);
  MetadataSearchResultRecord trackerDatasetSearchResult=new MetadataSearchResultRecord(trackerDataset);
  MetadataSearchResponse response=search(ns.getNamespace(),""String_Node_Str"",0,Integer.MAX_VALUE,1);
  Assert.assertEquals(3,response.getTotal());
  Assert.assertEquals(ImmutableList.of(datasetSearchResult,streamSearchResult,flowSearchResult),ImmutableList.copyOf(stripMetadata(response.getResults())));
  response=search(ns.getNamespace(),""String_Node_Str"",0,Integer.MAX_VALUE,1,true);
  Assert.assertEquals(4,response.getTotal());
  Assert.assertEquals(ImmutableList.of(trackerDatasetSearchResult,datasetSearchResult,streamSearchResult,flowSearchResult),ImmutableList.copyOf(stripMetadata(response.getResults())));
  response=search(ns.getNamespace(),""String_Node_Str"",0,2,1);
  Assert.assertEquals(3,response.getTotal());
  Assert.assertEquals(ImmutableList.of(datasetSearchResult,streamSearchResult),ImmutableList.copyOf(stripMetadata(response.getResults())));
  response=search(ns.getNamespace(),""String_Node_Str"",1,2,1);
  Assert.assertEquals(3,response.getTotal());
  Assert.assertEquals(ImmutableList.of(streamSearchResult,flowSearchResult),ImmutableList.copyOf(stripMetadata(response.getResults())));
  response=search(ns.getNamespace(),""String_Node_Str"",1,3,1,true);
  Assert.assertEquals(4,response.getTotal());
  Assert.assertEquals(ImmutableList.of(datasetSearchResult,streamSearchResult,flowSearchResult),ImmutableList.copyOf(stripMetadata(response.getResults())));
  response=search(ns.getNamespace(),""String_Node_Str"",2,2,1);
  Assert.assertEquals(3,response.getTotal());
  Assert.assertEquals(ImmutableList.of(flowSearchResult),ImmutableList.copyOf(stripMetadata(response.getResults())));
  response=search(ns.getNamespace(),""String_Node_Str"",4,2,1);
  Assert.assertEquals(3,response.getTotal());
  Assert.assertEquals(ImmutableList.<MetadataSearchResultRecord>of(),ImmutableList.copyOf(stripMetadata(response.getResults())));
  response=search(ns.getNamespace(),""String_Node_Str"",1,Integer.MAX_VALUE,0);
  Assert.assertEquals(3,response.getTotal());
  Assert.assertEquals(ImmutableList.of(streamSearchResult,flowSearchResult),ImmutableList.copyOf(stripMetadata(response.getResults())));
}"
5551,"public Boolean getStopGracefully(){
  return stopGracefully == null ? false : stopGracefully;
}","public Boolean getStopGracefully(){
  return stopGracefully == null ? true : stopGracefully;
}"
5552,"public Builder(){
  this.isUnitTest=true;
  this.batchInterval=""String_Node_Str"";
  this.stopGraceFully=false;
}","public Builder(){
  this.isUnitTest=true;
  this.batchInterval=""String_Node_Str"";
  this.stopGraceFully=true;
}"
5553,"@GET @Path(""String_Node_Str"") public void getProperty(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String key) throws Exception {
  NamespaceId namespace=NamespaceId.SYSTEM.getNamespace().equalsIgnoreCase(namespaceId) ? NamespaceId.SYSTEM : validateAndGetNamespace(namespaceId);
  Id.Artifact artifactId=validateAndGetArtifactId(namespace,artifactName,artifactVersion);
  try {
    ArtifactDetail detail=artifactRepository.getArtifact(artifactId);
    responder.sendString(HttpResponseStatus.OK,detail.getMeta().getProperties().get(key));
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
  }
}","@GET @Path(""String_Node_Str"") public void getProperty(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String key,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws Exception {
  NamespaceId namespace=validateAndGetScopedNamespace(Ids.namespace(namespaceId),scope);
  Id.Artifact artifactId=validateAndGetArtifactId(namespace,artifactName,artifactVersion);
  try {
    ArtifactDetail detail=artifactRepository.getArtifact(artifactId);
    responder.sendString(HttpResponseStatus.OK,detail.getMeta().getProperties().get(key));
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
  }
}"
5554,"/** 
 * Delete the specified artifact. Programs that use the artifact will not be able to start.
 * @param artifactId the artifact to delete
 * @throws IOException if there was some IO error deleting the artifact
 * @throws UnauthorizedException if the current user is not authorized to delete the artifact. To delete an artifact,a user needs  {@link Action#ADMIN} permission on the artifact.
 */
public void deleteArtifact(Id.Artifact artifactId) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(artifactId.toEntityId(),principal,Action.ADMIN);
  artifactStore.delete(artifactId);
  metadataStore.removeMetadata(artifactId.toEntityId());
  privilegesManager.revoke(artifactId.toEntityId());
}","/** 
 * Delete the specified artifact. Programs that use the artifact will not be able to start.
 * @param artifactId the artifact to delete
 * @throws IOException if there was some IO error deleting the artifact
 * @throws ArtifactNotFoundException if the given artifact does not exist
 * @throws UnauthorizedException if the current user is not authorized to delete the artifact. To delete an artifact,a user needs  {@link Action#ADMIN} permission on the artifact.
 */
public void deleteArtifact(Id.Artifact artifactId) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(artifactId.toEntityId(),principal,Action.ADMIN);
  artifactStore.delete(artifactId);
  metadataStore.removeMetadata(artifactId.toEntityId());
  privilegesManager.revoke(artifactId.toEntityId());
}"
5555,"/** 
 * Delete the specified artifact. Programs that use the artifact will no longer be able to start.
 * @param artifactId the id of the artifact to delete
 * @throws IOException if there was an IO error deleting the metadata or the actual artifact
 */
public void delete(final Id.Artifact artifactId) throws IOException {
  try {
    transactional.execute(new TxRunnable(){
      @Override public void run(      DatasetContext context) throws Exception {
        ArtifactCell artifactCell=new ArtifactCell(artifactId);
        Table metaTable=getMetaTable(context);
        byte[] detailBytes=metaTable.get(artifactCell.rowkey,artifactCell.column);
        if (detailBytes == null) {
          return;
        }
        deleteMeta(metaTable,artifactId,detailBytes);
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,IOException.class);
  }
}","/** 
 * Delete the specified artifact. Programs that use the artifact will no longer be able to start.
 * @param artifactId the id of the artifact to delete
 * @throws IOException if there was an IO error deleting the metadata or the actual artifact
 */
public void delete(final Id.Artifact artifactId) throws ArtifactNotFoundException, IOException {
  try {
    transactional.execute(new TxRunnable(){
      @Override public void run(      DatasetContext context) throws Exception {
        ArtifactCell artifactCell=new ArtifactCell(artifactId);
        Table metaTable=getMetaTable(context);
        byte[] detailBytes=metaTable.get(artifactCell.rowkey,artifactCell.column);
        if (detailBytes == null) {
          throw new ArtifactNotFoundException(artifactId.toEntityId());
        }
        deleteMeta(metaTable,artifactId,detailBytes);
      }
    }
);
  }
 catch (  TransactionFailureException e) {
    throw Transactions.propagate(e,IOException.class,ArtifactNotFoundException.class);
  }
}"
5556,"private static List<Module> createPersistentModules(CConfiguration cConf,Configuration hConf){
  cConf.setIfUnset(Constants.CFG_DATA_LEVELDB_DIR,Constants.DEFAULT_DATA_LEVELDB_DIR);
  cConf.set(Constants.CFG_DATA_INMEMORY_PERSISTENCE,Constants.InMemoryPersistenceType.LEVELDB.name());
  String localhost=InetAddress.getLoopbackAddress().getHostAddress();
  cConf.set(Constants.Service.MASTER_SERVICES_BIND_ADDRESS,localhost);
  cConf.set(Constants.Transaction.Container.ADDRESS,localhost);
  cConf.set(Constants.Dataset.Executor.ADDRESS,localhost);
  cConf.set(Constants.Stream.ADDRESS,localhost);
  cConf.set(Constants.Metrics.ADDRESS,localhost);
  cConf.set(Constants.Metrics.SERVER_ADDRESS,localhost);
  cConf.set(Constants.MetricsProcessor.ADDRESS,localhost);
  cConf.set(Constants.LogSaver.ADDRESS,localhost);
  cConf.set(Constants.Security.AUTH_SERVER_BIND_ADDRESS,localhost);
  cConf.set(Constants.Explore.SERVER_ADDRESS,localhost);
  cConf.set(Constants.Metadata.SERVICE_BIND_ADDRESS,localhost);
  cConf.set(Constants.Preview.ADDRESS,localhost);
  return ImmutableList.of(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MetricsHandlerModule(),new DiscoveryRuntimeModule().getStandaloneModules(),new LocationRuntimeModule().getStandaloneModules(),new AppFabricServiceRuntimeModule().getStandaloneModules(),new ProgramRunnerRuntimeModule().getStandaloneModules(),new DataFabricModules().getStandaloneModules(),new DataSetsModules().getStandaloneModules(),new DataSetServiceModules().getStandaloneModules(),new MetricsClientRuntimeModule().getStandaloneModules(),new LoggingModules().getStandaloneModules(),new LogReaderRuntimeModules().getStandaloneModules(),new RouterModules().getStandaloneModules(),new SecurityModules().getStandaloneModules(),new SecureStoreModules().getStandaloneModules(),new StreamServiceRuntimeModule().getStandaloneModules(),new ExploreRuntimeModule().getStandaloneModules(),new ServiceStoreModules().getStandaloneModules(),new ExploreClientModule(),new NotificationFeedServiceRuntimeModule().getStandaloneModules(),new NotificationServiceRuntimeModule().getStandaloneModules(),new ViewAdminModules().getStandaloneModules(),new StreamAdminModules().getStandaloneModules(),new NamespaceStoreModule().getStandaloneModules(),new MetadataServiceModule(),new RemoteSystemOperationsServiceModule(),new AuditModule().getStandaloneModules(),new AuthorizationModule(),new AuthorizationEnforcementModule().getStandaloneModules(),new PreviewHttpModule(),new MessagingServerRuntimeModule().getStandaloneModules(),new PrivateModule(){
    @Override protected void configure(){
      bind(OperationalStatsLoader.class).in(Scopes.SINGLETON);
      bind(OperationalStatsService.class).in(Scopes.SINGLETON);
      expose(OperationalStatsService.class);
    }
  }
);
}","private static List<Module> createPersistentModules(CConfiguration cConf,Configuration hConf){
  cConf.setIfUnset(Constants.CFG_DATA_LEVELDB_DIR,Constants.DEFAULT_DATA_LEVELDB_DIR);
  cConf.set(Constants.CFG_DATA_INMEMORY_PERSISTENCE,Constants.InMemoryPersistenceType.LEVELDB.name());
  String localhost=InetAddress.getLoopbackAddress().getHostAddress();
  cConf.set(Constants.Service.MASTER_SERVICES_BIND_ADDRESS,localhost);
  cConf.set(Constants.Transaction.Container.ADDRESS,localhost);
  cConf.set(Constants.Dataset.Executor.ADDRESS,localhost);
  cConf.set(Constants.Stream.ADDRESS,localhost);
  cConf.set(Constants.Metrics.ADDRESS,localhost);
  cConf.set(Constants.Metrics.SERVER_ADDRESS,localhost);
  cConf.set(Constants.MetricsProcessor.ADDRESS,localhost);
  cConf.set(Constants.LogSaver.ADDRESS,localhost);
  cConf.set(Constants.Explore.SERVER_ADDRESS,localhost);
  cConf.set(Constants.Metadata.SERVICE_BIND_ADDRESS,localhost);
  cConf.set(Constants.Preview.ADDRESS,localhost);
  return ImmutableList.of(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MetricsHandlerModule(),new DiscoveryRuntimeModule().getStandaloneModules(),new LocationRuntimeModule().getStandaloneModules(),new AppFabricServiceRuntimeModule().getStandaloneModules(),new ProgramRunnerRuntimeModule().getStandaloneModules(),new DataFabricModules().getStandaloneModules(),new DataSetsModules().getStandaloneModules(),new DataSetServiceModules().getStandaloneModules(),new MetricsClientRuntimeModule().getStandaloneModules(),new LoggingModules().getStandaloneModules(),new LogReaderRuntimeModules().getStandaloneModules(),new RouterModules().getStandaloneModules(),new SecurityModules().getStandaloneModules(),new SecureStoreModules().getStandaloneModules(),new StreamServiceRuntimeModule().getStandaloneModules(),new ExploreRuntimeModule().getStandaloneModules(),new ServiceStoreModules().getStandaloneModules(),new ExploreClientModule(),new NotificationFeedServiceRuntimeModule().getStandaloneModules(),new NotificationServiceRuntimeModule().getStandaloneModules(),new ViewAdminModules().getStandaloneModules(),new StreamAdminModules().getStandaloneModules(),new NamespaceStoreModule().getStandaloneModules(),new MetadataServiceModule(),new RemoteSystemOperationsServiceModule(),new AuditModule().getStandaloneModules(),new AuthorizationModule(),new AuthorizationEnforcementModule().getStandaloneModules(),new PreviewHttpModule(),new MessagingServerRuntimeModule().getStandaloneModules(),new PrivateModule(){
    @Override protected void configure(){
      bind(OperationalStatsLoader.class).in(Scopes.SINGLETON);
      bind(OperationalStatsService.class).in(Scopes.SINGLETON);
      expose(OperationalStatsService.class);
    }
  }
);
}"
5557,"/** 
 * Returns the set of jar files used by hive. The set is constructed based on the system property  {@link #EXPLORE_CLASSPATH}. The   {@link #EXPLORE_CLASSPATH} is expected to contains one or more file paths,separated by the  {@link File#pathSeparatorChar}. Only jar files will be included in the result set and paths ended with a '*' will be expanded to include all jars under the given path.
 * @throws IllegalArgumentException if the system property {@link #EXPLORE_CLASSPATH} is missing.
 */
public static Iterable<File> getExploreClasspathJarFiles(){
  String property=System.getProperty(EXPLORE_CLASSPATH);
  if (property == null) {
    throw new RuntimeException(""String_Node_Str"" + EXPLORE_CLASSPATH + ""String_Node_Str"");
  }
  Set<File> result=new LinkedHashSet<>();
  for (  String path : Splitter.on(File.pathSeparator).split(property)) {
    List<File> jarFiles;
    if (path.endsWith(""String_Node_Str"")) {
      jarFiles=DirUtils.listFiles(new File(path.substring(0,path.length() - 1)).getAbsoluteFile(),""String_Node_Str"");
    }
 else     if (path.endsWith(""String_Node_Str"")) {
      jarFiles=Collections.singletonList(new File(path));
    }
 else {
      continue;
    }
    for (    File jarFile : jarFiles) {
      try {
        Path jarPath=jarFile.toPath().toRealPath();
        if (Files.isRegularFile(jarPath) && Files.isReadable(jarPath)) {
          result.add(jarPath.toFile());
        }
      }
 catch (      IOException e) {
        LOG.debug(""String_Node_Str"",jarFile);
      }
    }
  }
  return Collections.unmodifiableSet(result);
}","/** 
 * Returns the set of jar files used by hive. The set is constructed based on the system property  {@link #EXPLORE_CLASSPATH}. The   {@link #EXPLORE_CLASSPATH} is expected to contains one or more file paths,separated by the  {@link File#pathSeparatorChar}. Only jar files will be included in the result set and paths ended with a '*' will be expanded to include all jars under the given path.
 * @param extraExtensions if provided, the set of file extensions that is also accepted when resolving theclasspath wildcard
 * @throws IllegalArgumentException if the system property {@link #EXPLORE_CLASSPATH} is missing.
 */
public static Iterable<File> getExploreClasspathJarFiles(String... extraExtensions){
  String property=System.getProperty(EXPLORE_CLASSPATH);
  if (property == null) {
    throw new RuntimeException(""String_Node_Str"" + EXPLORE_CLASSPATH + ""String_Node_Str"");
  }
  Set<String> acceptedExts=Sets.newHashSet(extraExtensions);
  acceptedExts.add(""String_Node_Str"");
  Set<File> result=new LinkedHashSet<>();
  for (  String path : Splitter.on(File.pathSeparator).split(property)) {
    List<File> jarFiles;
    if (path.endsWith(""String_Node_Str"")) {
      jarFiles=DirUtils.listFiles(new File(path.substring(0,path.length() - 1)).getAbsoluteFile(),acceptedExts);
    }
 else     if (path.endsWith(""String_Node_Str"")) {
      jarFiles=Collections.singletonList(new File(path));
    }
 else {
      continue;
    }
    for (    File jarFile : jarFiles) {
      try {
        Path jarPath=jarFile.toPath().toRealPath();
        if (Files.isRegularFile(jarPath) && Files.isReadable(jarPath)) {
          result.add(jarPath.toFile());
        }
      }
 catch (      IOException e) {
        LOG.debug(""String_Node_Str"",jarFile);
      }
    }
  }
  return Collections.unmodifiableSet(result);
}"
5558,"/** 
 * Setup the environment needed by the embedded HiveServer2.
 */
private void setupHive(){
  File tmpDir=new File(System.getProperty(""String_Node_Str"")).getAbsoluteFile();
  File localScratchFile=new File(tmpDir,""String_Node_Str"" + System.getProperty(""String_Node_Str""));
  System.setProperty(HiveConf.ConfVars.LOCALSCRATCHDIR.toString(),localScratchFile.getAbsolutePath());
  LOG.info(""String_Node_Str"",HiveConf.ConfVars.LOCALSCRATCHDIR.toString(),System.getProperty(HiveConf.ConfVars.LOCALSCRATCHDIR.toString()));
  ClassLoader classLoader=Objects.firstNonNull(Thread.currentThread().getContextClassLoader(),getClass().getClassLoader());
  if (!(classLoader instanceof URLClassLoader)) {
    LOG.warn(""String_Node_Str"" + ""String_Node_Str"",classLoader);
    return;
  }
  List<URL> urls=Arrays.asList(((URLClassLoader)classLoader).getURLs());
  LOG.debug(""String_Node_Str"",urls);
  Map<String,URL> hiveExtraJars=new LinkedHashMap<>();
  String userDir=System.getProperty(""String_Node_Str"");
  for (  URL url : urls) {
    String path=url.getPath();
    if (!path.endsWith(""String_Node_Str"") || !path.startsWith(userDir) || new File(path).getParent().equals(userDir)) {
      continue;
    }
    String fileName=getFileName(url);
    if (!hiveExtraJars.containsKey(fileName)) {
      hiveExtraJars.put(fileName,url);
    }
 else {
      LOG.info(""String_Node_Str"",fileName,url);
    }
  }
  System.setProperty(HiveConf.ConfVars.HIVEAUXJARS.toString(),Joiner.on(',').join(Iterables.transform(hiveExtraJars.values(),Functions.toStringFunction())));
  LOG.debug(""String_Node_Str"",HiveConf.ConfVars.HIVEAUXJARS.toString(),System.getProperty(HiveConf.ConfVars.HIVEAUXJARS.toString()));
  System.setProperty(BaseHiveExploreService.SPARK_YARN_DIST_FILES,Joiner.on(',').join(Iterables.transform(hiveExtraJars.values(),URL_TO_PATH)));
  LOG.debug(""String_Node_Str"",BaseHiveExploreService.SPARK_YARN_DIST_FILES,System.getProperty(BaseHiveExploreService.SPARK_YARN_DIST_FILES));
  String extraClassPath=Joiner.on(',').join(Iterables.transform(hiveExtraJars.keySet(),new Function<String,String>(){
    @Override public String apply(    String name){
      return ""String_Node_Str"" + name;
    }
  }
));
  extraClassPath+=""String_Node_Str"";
  rewriteConfigClasspath(""String_Node_Str"",YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH),extraClassPath);
  rewriteConfigClasspath(""String_Node_Str"",MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH,MRJobConfig.DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH,extraClassPath);
  rewriteConfigClasspath(""String_Node_Str"",TezConfiguration.TEZ_CLUSTER_ADDITIONAL_CLASSPATH_PREFIX,null,extraClassPath);
  rewriteHiveConfig();
  String hiveExecJar=new JobConf(org.apache.hadoop.hive.ql.exec.Task.class).getJar();
  Preconditions.checkNotNull(hiveExecJar,""String_Node_Str"" + ""String_Node_Str"");
  LOG.debug(""String_Node_Str"",hiveExecJar);
  try {
    setupHadoopBin(Iterables.concat(hiveExtraJars.values(),Collections.singleton(new File(hiveExecJar).toURI().toURL())));
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}","/** 
 * Setup the environment needed by the embedded HiveServer2.
 */
private void setupHive(){
  File tmpDir=new File(System.getProperty(""String_Node_Str"")).getAbsoluteFile();
  File localScratchFile=new File(tmpDir,""String_Node_Str"" + System.getProperty(""String_Node_Str""));
  System.setProperty(HiveConf.ConfVars.LOCALSCRATCHDIR.toString(),localScratchFile.getAbsolutePath());
  LOG.info(""String_Node_Str"",HiveConf.ConfVars.LOCALSCRATCHDIR.toString(),System.getProperty(HiveConf.ConfVars.LOCALSCRATCHDIR.toString()));
  ClassLoader classLoader=Objects.firstNonNull(Thread.currentThread().getContextClassLoader(),getClass().getClassLoader());
  if (!(classLoader instanceof URLClassLoader)) {
    LOG.warn(""String_Node_Str"" + ""String_Node_Str"",classLoader);
    return;
  }
  List<URL> urls=Arrays.asList(((URLClassLoader)classLoader).getURLs());
  LOG.debug(""String_Node_Str"",urls);
  Map<String,URL> hiveExtraJars=new LinkedHashMap<>();
  String userDir=System.getProperty(""String_Node_Str"");
  for (  URL url : urls) {
    String path=url.getPath();
    if (!path.endsWith(""String_Node_Str"") || !path.startsWith(userDir) || new File(path).getParent().equals(userDir)) {
      continue;
    }
    String fileName=getFileName(url);
    if (!hiveExtraJars.containsKey(fileName)) {
      hiveExtraJars.put(fileName,url);
    }
 else {
      LOG.info(""String_Node_Str"",fileName,url);
    }
  }
  System.setProperty(HiveConf.ConfVars.HIVEAUXJARS.toString(),Joiner.on(',').join(Iterables.transform(hiveExtraJars.values(),Functions.toStringFunction())));
  LOG.debug(""String_Node_Str"",HiveConf.ConfVars.HIVEAUXJARS.toString(),System.getProperty(HiveConf.ConfVars.HIVEAUXJARS.toString()));
  System.setProperty(BaseHiveExploreService.SPARK_YARN_DIST_FILES,Joiner.on(',').join(Iterables.transform(hiveExtraJars.values(),URL_TO_PATH)));
  LOG.debug(""String_Node_Str"",BaseHiveExploreService.SPARK_YARN_DIST_FILES,System.getProperty(BaseHiveExploreService.SPARK_YARN_DIST_FILES));
  Iterable<String> extraClassPath=Iterables.concat(Iterables.transform(hiveExtraJars.keySet(),new Function<String,String>(){
    @Override public String apply(    String name){
      return ""String_Node_Str"" + name;
    }
  }
),Collections.singleton(""String_Node_Str""));
  rewriteConfigClasspath(""String_Node_Str"",YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH),Joiner.on(""String_Node_Str"").join(extraClassPath));
  rewriteConfigClasspath(""String_Node_Str"",MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH,MRJobConfig.DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(extraClassPath));
  rewriteConfigClasspath(""String_Node_Str"",TezConfiguration.TEZ_CLUSTER_ADDITIONAL_CLASSPATH_PREFIX,null,Joiner.on(File.pathSeparatorChar).join(extraClassPath));
  rewriteHiveConfig();
  String hiveExecJar=new JobConf(org.apache.hadoop.hive.ql.exec.Task.class).getJar();
  Preconditions.checkNotNull(hiveExecJar,""String_Node_Str"" + ""String_Node_Str"");
  LOG.debug(""String_Node_Str"",hiveExecJar);
  try {
    setupHadoopBin(Iterables.concat(hiveExtraJars.values(),Collections.singleton(new File(hiveExecJar).toURI().toURL())));
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}"
5559,"private Map<String,LocalizeResource> getExploreDependencies(Path tempDir,List<String> extraClassPaths) throws IOException {
  final Set<File> masterJars=new HashSet<>();
  for (  URL url : ClassLoaders.getClassLoaderURLs(getClass().getClassLoader(),new HashSet<URL>())) {
    String path=url.getPath();
    if (!""String_Node_Str"".equals(url.getProtocol()) || !path.endsWith(""String_Node_Str"")) {
      continue;
    }
    try {
      masterJars.add(new File(url.toURI()));
    }
 catch (    URISyntaxException e) {
      LOG.warn(""String_Node_Str"",e);
    }
  }
  Iterable<File> exploreJars=Iterables.filter(ExploreUtils.getExploreClasspathJarFiles(),new Predicate<File>(){
    @Override public boolean apply(    File file){
      return !masterJars.contains(file);
    }
  }
);
  Map<String,LocalizeResource> resources=new HashMap<>();
  for (  File exploreJar : exploreJars) {
    File targetJar=tempDir.resolve(System.currentTimeMillis() + ""String_Node_Str"" + exploreJar.getName()).toFile();
    File resultFile=ExploreServiceUtils.rewriteHiveAuthFactory(exploreJar,targetJar);
    if (resultFile == targetJar) {
      LOG.info(""String_Node_Str"",exploreJar,resultFile);
    }
    resources.put(resultFile.getName(),new LocalizeResource(resultFile));
    extraClassPaths.add(resultFile.getName());
  }
  extraClassPaths.addAll(MapReduceContainerHelper.localizeFramework(hConf,resources));
  return resources;
}","private Map<String,LocalizeResource> getExploreDependencies(Path tempDir,List<String> extraClassPaths) throws IOException {
  final Set<File> masterJars=new HashSet<>();
  for (  URL url : ClassLoaders.getClassLoaderURLs(getClass().getClassLoader(),new HashSet<URL>())) {
    String path=url.getPath();
    if (!""String_Node_Str"".equals(url.getProtocol()) || !path.endsWith(""String_Node_Str"")) {
      continue;
    }
    try {
      masterJars.add(new File(url.toURI()));
    }
 catch (    URISyntaxException e) {
      LOG.warn(""String_Node_Str"",e);
    }
  }
  Iterable<File> exploreFiles=Iterables.filter(ExploreUtils.getExploreClasspathJarFiles(""String_Node_Str"",""String_Node_Str""),new Predicate<File>(){
    @Override public boolean apply(    File file){
      return !masterJars.contains(file);
    }
  }
);
  Map<String,LocalizeResource> resources=new HashMap<>();
  for (  File file : exploreFiles) {
    if (file.getName().endsWith(""String_Node_Str"") || file.getName().endsWith(""String_Node_Str"")) {
      resources.put(file.getName(),new LocalizeResource(file,true));
      extraClassPaths.add(file.getName());
      extraClassPaths.add(file.getName() + ""String_Node_Str"");
      extraClassPaths.add(file.getName() + ""String_Node_Str"");
    }
 else {
      File targetFile=tempDir.resolve(System.currentTimeMillis() + ""String_Node_Str"" + file.getName()).toFile();
      File resultFile=ExploreServiceUtils.rewriteHiveAuthFactory(file,targetFile);
      if (resultFile == targetFile) {
        LOG.info(""String_Node_Str"",file,resultFile);
      }
      resources.put(resultFile.getName(),new LocalizeResource(resultFile));
      extraClassPaths.add(resultFile.getName());
    }
  }
  extraClassPaths.addAll(MapReduceContainerHelper.localizeFramework(hConf,resources));
  return resources;
}"
5560,"@Override public String getWebURL(){
  try {
    if (HAUtil.isHAEnabled(conf,getNameService())) {
      URL haWebURL=getHAWebURL();
      if (haWebURL != null) {
        return haWebURL.toString();
      }
    }
 else {
      try (FileSystem fs=FileSystem.get(conf)){
        URL webUrl=rpcToHttpAddress(fs.getUri());
        if (webUrl != null) {
          return webUrl.toString();
        }
      }
     }
  }
 catch (  IOException e) {
    LOG.warn(""String_Node_Str"",e);
  }
  return null;
}","@Override public String getWebURL(){
  try {
    if (HAUtil.isHAEnabled(conf,getNameService())) {
      URL haWebURL=getHAWebURL();
      if (haWebURL != null) {
        return haWebURL.toString();
      }
    }
 else {
      try (FileSystem fs=FileSystem.get(conf)){
        URL webUrl=rpcToHttpAddress(fs.getUri());
        if (webUrl != null) {
          return webUrl.toString();
        }
      }
     }
    lastCollectFailed=false;
  }
 catch (  Exception e) {
    if (!lastCollectFailed) {
      LOG.warn(""String_Node_Str"",e);
    }
    lastCollectFailed=true;
  }
  return null;
}"
5561,"@Nullable private URL getHAWebURL() throws IOException {
  String activeNamenode=null;
  String nameService=getNameService();
  for (  String nnId : DFSUtil.getNameNodeIds(conf,nameService)) {
    HAServiceTarget haServiceTarget=new NNHAServiceTarget(conf,nameService,nnId);
    HAServiceProtocol proxy=haServiceTarget.getProxy(conf,10000);
    HAServiceStatus serviceStatus=proxy.getServiceStatus();
    if (HAServiceProtocol.HAServiceState.ACTIVE != serviceStatus.getState()) {
      continue;
    }
    activeNamenode=DFSUtil.getNamenodeServiceAddr(conf,nameService,nnId);
  }
  if (activeNamenode == null) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  return rpcToHttpAddress(URI.create(activeNamenode));
}","@Nullable private URL getHAWebURL() throws IOException {
  String activeNamenode=null;
  String nameService=getNameService();
  HdfsConfiguration hdfsConf=new HdfsConfiguration(conf);
  String nameNodePrincipal=conf.get(DFSConfigKeys.DFS_NAMENODE_USER_NAME_KEY,""String_Node_Str"");
  hdfsConf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY,nameNodePrincipal);
  for (  String nnId : DFSUtil.getNameNodeIds(conf,nameService)) {
    HAServiceTarget haServiceTarget=new NNHAServiceTarget(hdfsConf,nameService,nnId);
    HAServiceProtocol proxy=haServiceTarget.getProxy(hdfsConf,10000);
    HAServiceStatus serviceStatus=proxy.getServiceStatus();
    if (HAServiceProtocol.HAServiceState.ACTIVE != serviceStatus.getState()) {
      continue;
    }
    activeNamenode=DFSUtil.getNamenodeServiceAddr(hdfsConf,nameService,nnId);
  }
  if (activeNamenode == null) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  return rpcToHttpAddress(URI.create(activeNamenode));
}"
5562,"/** 
 * Should only be called when HA is enabled.
 */
private URL getHAWebURL() throws IOException {
  InetSocketAddress activeRM=null;
  Collection<String> rmIds=HAUtil.getRMHAIds(conf);
  if (rmIds.isEmpty()) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  for (  String rmId : rmIds) {
    YarnConfiguration yarnConf=new YarnConfiguration(conf);
    yarnConf.set(YarnConfiguration.RM_HA_ID,rmId);
    RMHAServiceTarget rmhaServiceTarget=new RMHAServiceTarget(yarnConf);
    HAServiceProtocol proxy=rmhaServiceTarget.getProxy(yarnConf,10000);
    HAServiceStatus serviceStatus=proxy.getServiceStatus();
    if (HAServiceProtocol.HAServiceState.ACTIVE != serviceStatus.getState()) {
      continue;
    }
    activeRM=rmhaServiceTarget.getAddress();
  }
  if (activeRM == null) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  return adminToWebappAddress(activeRM);
}","/** 
 * Should only be called when HA is enabled.
 */
private URL getHAWebURL() throws IOException {
  InetSocketAddress activeRM=null;
  Collection<String> rmIds=HAUtil.getRMHAIds(conf);
  if (rmIds.isEmpty()) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  for (  String rmId : rmIds) {
    YarnConfiguration yarnConf=new YarnConfiguration(conf);
    yarnConf.set(YarnConfiguration.RM_HA_ID,rmId);
    yarnConf.set(CommonConfigurationKeys.HADOOP_SECURITY_SERVICE_USER_NAME_KEY,conf.get(YarnConfiguration.RM_PRINCIPAL,""String_Node_Str""));
    RMHAServiceTarget rmhaServiceTarget=new RMHAServiceTarget(yarnConf);
    HAServiceProtocol proxy=rmhaServiceTarget.getProxy(yarnConf,10000);
    HAServiceStatus serviceStatus=proxy.getServiceStatus();
    if (HAServiceProtocol.HAServiceState.ACTIVE != serviceStatus.getState()) {
      continue;
    }
    activeRM=rmhaServiceTarget.getAddress();
  }
  if (activeRM == null) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  return adminToWebappAddress(activeRM);
}"
5563,"@Override public synchronized void collect() throws IOException {
  webUrl=getResourceManager().toString();
  logsUrl=webUrl + ""String_Node_Str"";
}","@Override public synchronized void collect() throws IOException {
  try {
    webUrl=getResourceManager().toString();
    logsUrl=webUrl + ""String_Node_Str"";
    lastCollectFailed=false;
  }
 catch (  Exception e) {
    if (!lastCollectFailed) {
      LOG.warn(""String_Node_Str"",e);
    }
    lastCollectFailed=true;
  }
}"
5564,"/** 
 * Parses a   {@link SortInfo} object from the specified string. The supported format is<pre>[sortBy][whitespace][sortOrder]</pre>.
 * @param sort the string to parse into a {@link SortInfo}. If   {@code null},   {@link #DEFAULT} is returned
 * @return the parsed {@link SortInfo}
 * @throws BadRequestException if the string does not conform to the expected format
 */
public static SortInfo of(@Nullable String sort) throws BadRequestException {
  if (Strings.isNullOrEmpty(sort)) {
    return SortInfo.DEFAULT;
  }
  Iterable<String> sortSplit=Splitter.on(SPACE_SPLIT_PATTERN).trimResults().omitEmptyStrings().split(sort);
  if (Iterables.size(sortSplit) != 2) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"" + sort);
  }
  Iterator<String> iterator=sortSplit.iterator();
  String sortBy=iterator.next();
  String sortOrder=iterator.next();
  if (!AbstractSystemMetadataWriter.ENTITY_NAME_KEY.equalsIgnoreCase(sortBy) && !AbstractSystemMetadataWriter.CREATION_TIME_KEY.equalsIgnoreCase(sortBy)) {
    throw new BadRequestException(""String_Node_Str"" + sortBy);
  }
  if (!""String_Node_Str"".equalsIgnoreCase(sortOrder) && !""String_Node_Str"".equalsIgnoreCase(sortOrder)) {
    throw new BadRequestException(""String_Node_Str"" + sortOrder);
  }
  return new SortInfo(sortBy,SortInfo.SortOrder.valueOf(sortOrder.toUpperCase()));
}","/** 
 * Parses a   {@link SortInfo} object from the specified string. The supported format is<pre>[sortBy][whitespace][sortOrder]</pre>.
 * @param sort the string to parse into a {@link SortInfo}. If   {@code null},   {@link #DEFAULT} is returned
 * @return the parsed {@link SortInfo}
 * @throws BadRequestException if the string does not conform to the expected format
 */
public static SortInfo of(@Nullable String sort) throws BadRequestException {
  if (Strings.isNullOrEmpty(sort)) {
    return SortInfo.DEFAULT;
  }
  Iterable<String> sortSplit=Splitter.on(SPACE_SPLIT_PATTERN).trimResults().omitEmptyStrings().split(sort);
  if (Iterables.size(sortSplit) != 2) {
    throw new BadRequestException(String.format(""String_Node_Str"" + ""String_Node_Str"",AbstractSystemMetadataWriter.ENTITY_NAME_KEY,AbstractSystemMetadataWriter.CREATION_TIME_KEY,SortOrder.ASC,SortOrder.DESC,sort));
  }
  Iterator<String> iterator=sortSplit.iterator();
  String sortBy=iterator.next();
  String sortOrder=iterator.next();
  if (!AbstractSystemMetadataWriter.ENTITY_NAME_KEY.equalsIgnoreCase(sortBy) && !AbstractSystemMetadataWriter.CREATION_TIME_KEY.equalsIgnoreCase(sortBy)) {
    throw new BadRequestException(String.format(""String_Node_Str"",AbstractSystemMetadataWriter.ENTITY_NAME_KEY,AbstractSystemMetadataWriter.CREATION_TIME_KEY,sortBy));
  }
  if (!""String_Node_Str"".equalsIgnoreCase(sortOrder) && !""String_Node_Str"".equalsIgnoreCase(sortOrder)) {
    throw new BadRequestException(String.format(""String_Node_Str"",SortOrder.ASC,SortOrder.DESC,sortOrder));
  }
  return new SortInfo(sortBy,SortInfo.SortOrder.valueOf(sortOrder.toUpperCase()));
}"
5565,"public DatasetFramework getDSFramework(){
  return datasetFramework;
}","DatasetFramework getDSFramework(){
  return datasetFramework;
}"
5566,"@Inject UpgradeDatasetServiceManager(CConfiguration cConf,Configuration hConf,AuthorizationEnforcementService authorizationEnforcementService){
  Injector injector=createInjector(cConf,hConf,authorizationEnforcementService);
  this.datasetService=injector.getInstance(DatasetService.class);
  this.zkClientService=injector.getInstance(ZKClientService.class);
  this.datasetFramework=injector.getInstance(DatasetFramework.class);
  this.datasetOpExecutorService=injector.getInstance(DatasetOpExecutorService.class);
  this.remoteSystemOperationsService=injector.getInstance(RemoteSystemOperationsService.class);
}","@Inject UpgradeDatasetServiceManager(CConfiguration cConf,Configuration hConf,AuthorizationEnforcementService authorizationEnforcementService){
  Injector injector=createInjector(cConf,hConf,authorizationEnforcementService);
  this.datasetService=injector.getInstance(DatasetService.class);
  this.zkClientService=injector.getInstance(ZKClientService.class);
  this.datasetFramework=injector.getInstance(DatasetFramework.class);
  this.datasetOpExecutorService=injector.getInstance(DatasetOpExecutorService.class);
  this.remoteSystemOperationsService=injector.getInstance(RemoteSystemOperationsService.class);
  this.metadataStore=injector.getInstance(MetadataStore.class);
}"
5567,"UpgradeTool() throws Exception {
  this.cConf=CConfiguration.create();
  if (this.cConf.getBoolean(Constants.Security.Authorization.ENABLED)) {
    LOG.info(""String_Node_Str"",getClass().getSimpleName());
    this.cConf.setBoolean(Constants.Security.Authorization.ENABLED,false);
  }
  SecurityUtil.loginForMasterService(cConf);
  this.hConf=HBaseConfiguration.create();
  Injector injector=createInjector();
  this.txService=injector.getInstance(TransactionService.class);
  this.zkClientService=injector.getInstance(ZKClientService.class);
  this.dsFramework=injector.getInstance(DatasetFramework.class);
  this.metadataStore=injector.getInstance(MetadataStore.class);
  this.streamStateStoreUpgrader=injector.getInstance(StreamStateStoreUpgrader.class);
  this.dsUpgrade=injector.getInstance(DatasetUpgrader.class);
  this.dsSpecUpgrader=injector.getInstance(DatasetSpecificationUpgrader.class);
  this.queueAdmin=injector.getInstance(QueueAdmin.class);
  this.nsStore=injector.getInstance(NamespaceStore.class);
  this.authorizationService=injector.getInstance(AuthorizationEnforcementService.class);
  this.datasetBasedStreamSizeScheduleStore=injector.getInstance(DatasetBasedStreamSizeScheduleStore.class);
  this.datasetBasedTimeScheduleStore=injector.getInstance(DatasetBasedTimeScheduleStore.class);
  this.store=injector.getInstance(DefaultStore.class);
  this.datasetInstanceManager=injector.getInstance(Key.get(DatasetInstanceManager.class,Names.named(""String_Node_Str"")));
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        UpgradeTool.this.stop();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
);
  this.existingEntitySystemMetadataWriter=injector.getInstance(ExistingEntitySystemMetadataWriter.class);
  this.upgradeDatasetServiceManager=injector.getInstance(UpgradeDatasetServiceManager.class);
}","UpgradeTool() throws Exception {
  this.cConf=CConfiguration.create();
  if (this.cConf.getBoolean(Constants.Security.Authorization.ENABLED)) {
    LOG.info(""String_Node_Str"",getClass().getSimpleName());
    this.cConf.setBoolean(Constants.Security.Authorization.ENABLED,false);
  }
  SecurityUtil.loginForMasterService(cConf);
  this.hConf=HBaseConfiguration.create();
  Injector injector=createInjector();
  this.txService=injector.getInstance(TransactionService.class);
  this.zkClientService=injector.getInstance(ZKClientService.class);
  this.dsFramework=injector.getInstance(DatasetFramework.class);
  this.streamStateStoreUpgrader=injector.getInstance(StreamStateStoreUpgrader.class);
  this.dsUpgrade=injector.getInstance(DatasetUpgrader.class);
  this.dsSpecUpgrader=injector.getInstance(DatasetSpecificationUpgrader.class);
  this.queueAdmin=injector.getInstance(QueueAdmin.class);
  this.nsStore=injector.getInstance(NamespaceStore.class);
  this.authorizationService=injector.getInstance(AuthorizationEnforcementService.class);
  this.datasetBasedStreamSizeScheduleStore=injector.getInstance(DatasetBasedStreamSizeScheduleStore.class);
  this.datasetBasedTimeScheduleStore=injector.getInstance(DatasetBasedTimeScheduleStore.class);
  this.store=injector.getInstance(DefaultStore.class);
  this.datasetInstanceManager=injector.getInstance(Key.get(DatasetInstanceManager.class,Names.named(""String_Node_Str"")));
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        UpgradeTool.this.stop();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
);
  this.existingEntitySystemMetadataWriter=injector.getInstance(ExistingEntitySystemMetadataWriter.class);
  this.upgradeDatasetServiceManager=injector.getInstance(UpgradeDatasetServiceManager.class);
}"
5568,"private void upgradeMetadataDatasetSpec(DatasetId metadataDatasetId){
  DatasetSpecification oldMetadataDatasetSpec=datasetInstanceManager.get(metadataDatasetId);
  if (oldMetadataDatasetSpec == null) {
    LOG.info(""String_Node_Str"",metadataDatasetId);
    return;
  }
  Gson gson=new Gson();
  JsonObject jsonObject=gson.toJsonTree(oldMetadataDatasetSpec,DatasetSpecification.class).getAsJsonObject();
  JsonObject metadataIndexObject=jsonObject.get(""String_Node_Str"").getAsJsonObject().get(""String_Node_Str"").getAsJsonObject();
  JsonObject properties=metadataIndexObject.get(""String_Node_Str"").getAsJsonObject();
  properties.addProperty(""String_Node_Str"",MetadataDataset.COLUMNS_TO_INDEX);
  JsonObject dProperties=metadataIndexObject.get(""String_Node_Str"").getAsJsonObject().get(""String_Node_Str"").getAsJsonObject().get(""String_Node_Str"").getAsJsonObject();
  JsonObject iProperties=metadataIndexObject.get(""String_Node_Str"").getAsJsonObject().get(""String_Node_Str"").getAsJsonObject().get(""String_Node_Str"").getAsJsonObject();
  dProperties.addProperty(""String_Node_Str"",MetadataDataset.COLUMNS_TO_INDEX);
  iProperties.addProperty(""String_Node_Str"",MetadataDataset.COLUMNS_TO_INDEX);
  DatasetSpecification newMetadataDatasetSpec=gson.fromJson(jsonObject,DatasetSpecification.class);
  datasetInstanceManager.delete(metadataDatasetId);
  datasetInstanceManager.add(NamespaceId.SYSTEM,newMetadataDatasetSpec);
  LOG.info(""String_Node_Str"",oldMetadataDatasetSpec,newMetadataDatasetSpec);
}","private void upgradeMetadataDatasetSpec(MetadataScope scope,DatasetId metadataDatasetId){
  DatasetSpecification oldMetadataDatasetSpec=datasetInstanceManager.get(metadataDatasetId);
  if (oldMetadataDatasetSpec == null) {
    LOG.info(""String_Node_Str"",metadataDatasetId);
    return;
  }
  Gson gson=new Gson();
  JsonObject jsonObject=gson.toJsonTree(oldMetadataDatasetSpec,DatasetSpecification.class).getAsJsonObject();
  JsonObject metadataDatasetProperties=jsonObject.get(""String_Node_Str"").getAsJsonObject();
  metadataDatasetProperties.addProperty(""String_Node_Str"",scope.name());
  JsonObject metadataIndexObject=jsonObject.get(""String_Node_Str"").getAsJsonObject().get(""String_Node_Str"").getAsJsonObject();
  JsonObject properties=metadataIndexObject.get(""String_Node_Str"").getAsJsonObject();
  properties.addProperty(""String_Node_Str"",MetadataDataset.COLUMNS_TO_INDEX);
  properties.addProperty(""String_Node_Str"",scope.name());
  JsonObject dProperties=metadataIndexObject.get(""String_Node_Str"").getAsJsonObject().get(""String_Node_Str"").getAsJsonObject().get(""String_Node_Str"").getAsJsonObject();
  JsonObject iProperties=metadataIndexObject.get(""String_Node_Str"").getAsJsonObject().get(""String_Node_Str"").getAsJsonObject().get(""String_Node_Str"").getAsJsonObject();
  dProperties.addProperty(""String_Node_Str"",MetadataDataset.COLUMNS_TO_INDEX);
  dProperties.addProperty(""String_Node_Str"",scope.name());
  iProperties.addProperty(""String_Node_Str"",MetadataDataset.COLUMNS_TO_INDEX);
  iProperties.addProperty(""String_Node_Str"",scope.name());
  DatasetSpecification newMetadataDatasetSpec=gson.fromJson(jsonObject,DatasetSpecification.class);
  datasetInstanceManager.delete(metadataDatasetId);
  datasetInstanceManager.add(NamespaceId.SYSTEM,newMetadataDatasetSpec);
  LOG.info(""String_Node_Str"",oldMetadataDatasetSpec,newMetadataDatasetSpec);
}"
5569,"private void performUpgrade() throws Exception {
  performCoprocessorUpgrade();
  LOG.info(""String_Node_Str"");
  store.upgradeAppVersion();
  LOG.info(""String_Node_Str"");
  dsSpecUpgrader.upgrade();
  LOG.info(""String_Node_Str"");
  streamStateStoreUpgrader.upgrade();
  LOG.info(""String_Node_Str"");
  upgradeMetadataDatasetSpecs();
  upgradeDatasetServiceManager.startUp();
  LOG.info(""String_Node_Str"");
  datasetBasedStreamSizeScheduleStore.upgrade();
  LOG.info(""String_Node_Str"");
  datasetBasedTimeScheduleStore.upgrade();
  LOG.info(""String_Node_Str"");
  try {
    existingEntitySystemMetadataWriter.write(upgradeDatasetServiceManager.getDSFramework());
    LOG.info(""String_Node_Str"");
    DeletedDatasetMetadataRemover datasetMetadataRemover=new DeletedDatasetMetadataRemover(nsStore,metadataStore,upgradeDatasetServiceManager.getDSFramework());
    datasetMetadataRemover.remove();
    LOG.info(""String_Node_Str"");
    metadataStore.deleteAllIndexes();
    LOG.info(""String_Node_Str"");
    metadataStore.rebuildIndexes();
  }
  finally {
    upgradeDatasetServiceManager.shutDown();
  }
}","private void performUpgrade() throws Exception {
  performCoprocessorUpgrade();
  LOG.info(""String_Node_Str"");
  store.upgradeAppVersion();
  LOG.info(""String_Node_Str"");
  dsSpecUpgrader.upgrade();
  LOG.info(""String_Node_Str"");
  streamStateStoreUpgrader.upgrade();
  LOG.info(""String_Node_Str"");
  upgradeMetadataDatasetSpecs();
  upgradeDatasetServiceManager.startUp();
  LOG.info(""String_Node_Str"");
  datasetBasedStreamSizeScheduleStore.upgrade();
  LOG.info(""String_Node_Str"");
  datasetBasedTimeScheduleStore.upgrade();
  LOG.info(""String_Node_Str"");
  MetadataStore metadataStore=upgradeDatasetServiceManager.getMetadataStore();
  try {
    existingEntitySystemMetadataWriter.write(upgradeDatasetServiceManager.getDSFramework());
    LOG.info(""String_Node_Str"");
    DeletedDatasetMetadataRemover datasetMetadataRemover=new DeletedDatasetMetadataRemover(nsStore,metadataStore,upgradeDatasetServiceManager.getDSFramework());
    datasetMetadataRemover.remove();
    LOG.info(""String_Node_Str"");
    metadataStore.deleteAllIndexes();
    LOG.info(""String_Node_Str"");
    metadataStore.rebuildIndexes();
  }
  finally {
    upgradeDatasetServiceManager.shutDown();
  }
}"
5570,"private void upgradeMetadataDatasetSpecs(){
  upgradeMetadataDatasetSpec(DefaultMetadataStore.BUSINESS_METADATA_INSTANCE_ID);
  upgradeMetadataDatasetSpec(DefaultMetadataStore.SYSTEM_METADATA_INSTANCE_ID);
}","private void upgradeMetadataDatasetSpecs(){
  upgradeMetadataDatasetSpec(MetadataScope.USER,DefaultMetadataStore.BUSINESS_METADATA_INSTANCE_ID);
  upgradeMetadataDatasetSpec(MetadataScope.SYSTEM,DefaultMetadataStore.SYSTEM_METADATA_INSTANCE_ID);
}"
5571,"/** 
 * Parses a   {@link SortInfo} object from the specified string. The supported format is<pre>[sortBy][whitespace][sortOrder]</pre>.
 * @param sort the string to parse into a {@link SortInfo}. If   {@code null},   {@link #DEFAULT} is returned
 * @return the parsed {@link SortInfo}
 * @throws BadRequestException if the string does not conform to the expected format
 */
public static SortInfo of(@Nullable String sort) throws BadRequestException {
  if (Strings.isNullOrEmpty(sort)) {
    return SortInfo.DEFAULT;
  }
  Iterable<String> sortSplit=Splitter.on(SPACE_SPLIT_PATTERN).trimResults().omitEmptyStrings().split(sort);
  if (Iterables.size(sortSplit) != 2) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"" + sort);
  }
  Iterator<String> iterator=sortSplit.iterator();
  String sortBy=iterator.next();
  String sortOrder=iterator.next();
  if (!AbstractSystemMetadataWriter.ENTITY_NAME_KEY.equalsIgnoreCase(sortBy) && !AbstractSystemMetadataWriter.CREATION_TIME_KEY.equalsIgnoreCase(sortBy)) {
    throw new BadRequestException(""String_Node_Str"" + sortBy);
  }
  if (!""String_Node_Str"".equalsIgnoreCase(sortOrder) && !""String_Node_Str"".equalsIgnoreCase(sortOrder)) {
    throw new BadRequestException(""String_Node_Str"" + sortOrder);
  }
  return new SortInfo(sortBy,SortInfo.SortOrder.valueOf(sortOrder.toUpperCase()));
}","/** 
 * Parses a   {@link SortInfo} object from the specified string. The supported format is<pre>[sortBy][whitespace][sortOrder]</pre>.
 * @param sort the string to parse into a {@link SortInfo}. If   {@code null},   {@link #DEFAULT} is returned
 * @return the parsed {@link SortInfo}
 * @throws BadRequestException if the string does not conform to the expected format
 */
public static SortInfo of(@Nullable String sort) throws BadRequestException {
  if (Strings.isNullOrEmpty(sort)) {
    return SortInfo.DEFAULT;
  }
  Iterable<String> sortSplit=Splitter.on(SPACE_SPLIT_PATTERN).trimResults().omitEmptyStrings().split(sort);
  if (Iterables.size(sortSplit) != 2) {
    throw new BadRequestException(String.format(""String_Node_Str"" + ""String_Node_Str"",AbstractSystemMetadataWriter.ENTITY_NAME_KEY,AbstractSystemMetadataWriter.CREATION_TIME_KEY,SortOrder.ASC,SortOrder.DESC,sort));
  }
  Iterator<String> iterator=sortSplit.iterator();
  String sortBy=iterator.next();
  String sortOrder=iterator.next();
  if (!AbstractSystemMetadataWriter.ENTITY_NAME_KEY.equalsIgnoreCase(sortBy) && !AbstractSystemMetadataWriter.CREATION_TIME_KEY.equalsIgnoreCase(sortBy)) {
    throw new BadRequestException(String.format(""String_Node_Str"",AbstractSystemMetadataWriter.ENTITY_NAME_KEY,AbstractSystemMetadataWriter.CREATION_TIME_KEY,sortBy));
  }
  if (!""String_Node_Str"".equalsIgnoreCase(sortOrder) && !""String_Node_Str"".equalsIgnoreCase(sortOrder)) {
    throw new BadRequestException(String.format(""String_Node_Str"",SortOrder.ASC,SortOrder.DESC,sortOrder));
  }
  return new SortInfo(sortBy,SortInfo.SortOrder.valueOf(sortOrder.toUpperCase()));
}"
5572,"public ProgramControllerServiceAdapter(Service service,ProgramId programId,RunId runId,@Nullable String componentName){
  super(programId,runId,componentName);
  this.service=service;
  listenToRuntimeState(service);
}","public ProgramControllerServiceAdapter(Service service,ProgramId programId,RunId runId,@Nullable String componentName){
  super(programId,runId,componentName);
  this.service=service;
  this.serviceStoppedLatch=new CountDownLatch(1);
  listenToRuntimeState(service);
}"
5573,"@Override protected void doStop() throws Exception {
  if (service.state() != Service.State.TERMINATED && service.state() != Service.State.FAILED) {
    service.stopAndWait();
  }
}","@Override protected void doStop() throws Exception {
  if (service.state() != Service.State.TERMINATED && service.state() != Service.State.FAILED) {
    LOG.debug(""String_Node_Str"",getProgramRunId());
    service.stopAndWait();
    LOG.debug(""String_Node_Str"",getProgramRunId());
    serviceStoppedLatch.await(30,TimeUnit.SECONDS);
    LOG.debug(""String_Node_Str"",getProgramRunId());
  }
}"
5574,"@Override public void failed(Service.State from,Throwable failure){
  LOG.error(""String_Node_Str"",failure);
  error(failure);
}","@Override public void failed(Service.State from,Throwable failure){
  LOG.error(""String_Node_Str"",failure);
  serviceStoppedLatch.countDown();
  error(failure);
}"
5575,"private void listenToRuntimeState(Service service){
  service.addListener(new ServiceListenerAdapter(){
    @Override public void running(){
      started();
    }
    @Override public void failed(    Service.State from,    Throwable failure){
      LOG.error(""String_Node_Str"",failure);
      error(failure);
    }
    @Override public void terminated(    Service.State from){
      if (from != Service.State.STOPPING) {
        complete();
      }
 else {
        stop();
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
}","private void listenToRuntimeState(Service service){
  service.addListener(new ServiceListenerAdapter(){
    @Override public void running(){
      started();
    }
    @Override public void failed(    Service.State from,    Throwable failure){
      LOG.error(""String_Node_Str"",failure);
      serviceStoppedLatch.countDown();
      error(failure);
    }
    @Override public void terminated(    Service.State from){
      serviceStoppedLatch.countDown();
      if (from != Service.State.STOPPING) {
        complete();
      }
 else {
        stop();
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
}"
5576,"@Override public void terminated(Service.State from){
  if (from != Service.State.STOPPING) {
    complete();
  }
 else {
    stop();
  }
}","@Override public void terminated(Service.State from){
  serviceStoppedLatch.countDown();
  if (from != Service.State.STOPPING) {
    complete();
  }
 else {
    stop();
  }
}"
5577,"public DataStreamsPipelineSpec build(){
  return new DataStreamsPipelineSpec(stages,connections,resources,driverResources,clientResources,stageLoggingEnabled,batchIntervalMillis,extraJavaOpts,numOfRecordsPreview);
}","public DataStreamsPipelineSpec build(){
  return new DataStreamsPipelineSpec(stages,connections,resources,driverResources,clientResources,stageLoggingEnabled,batchIntervalMillis,extraJavaOpts,numOfRecordsPreview,stopGracefully);
}"
5578,"@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  if (!super.equals(o)) {
    return false;
  }
  DataStreamsPipelineSpec that=(DataStreamsPipelineSpec)o;
  return batchIntervalMillis == that.batchIntervalMillis && Objects.equals(extraJavaOpts,that.extraJavaOpts);
}","@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  if (!super.equals(o)) {
    return false;
  }
  DataStreamsPipelineSpec that=(DataStreamsPipelineSpec)o;
  return batchIntervalMillis == that.batchIntervalMillis && Objects.equals(extraJavaOpts,that.extraJavaOpts) && stopGracefully == that.stopGracefully;
}"
5579,"public Builder(long batchIntervalMillis){
  this.batchIntervalMillis=batchIntervalMillis;
}","public Builder(long batchIntervalMillis){
  this.batchIntervalMillis=batchIntervalMillis;
  this.stopGracefully=false;
}"
5580,"private DataStreamsPipelineSpec(Set<StageSpec> stages,Set<Connection> connections,Resources resources,Resources driverResources,Resources clientResources,boolean stageLoggingEnabled,long batchIntervalMillis,String extraJavaOpts,int numOfRecordsPreview){
  super(stages,connections,resources,driverResources,clientResources,stageLoggingEnabled,numOfRecordsPreview);
  this.batchIntervalMillis=batchIntervalMillis;
  this.extraJavaOpts=extraJavaOpts;
}","private DataStreamsPipelineSpec(Set<StageSpec> stages,Set<Connection> connections,Resources resources,Resources driverResources,Resources clientResources,boolean stageLoggingEnabled,long batchIntervalMillis,String extraJavaOpts,int numOfRecordsPreview,boolean stopGracefully){
  super(stages,connections,resources,driverResources,clientResources,stageLoggingEnabled,numOfRecordsPreview);
  this.batchIntervalMillis=batchIntervalMillis;
  this.extraJavaOpts=extraJavaOpts;
  this.stopGracefully=stopGracefully;
}"
5581,"@Override public String toString(){
  return ""String_Node_Str"" + ""String_Node_Str"" + batchIntervalMillis + ""String_Node_Str""+ extraJavaOpts+ '\''+ ""String_Node_Str""+ super.toString();
}","@Override public String toString(){
  return ""String_Node_Str"" + ""String_Node_Str"" + batchIntervalMillis + ""String_Node_Str""+ extraJavaOpts+ '\''+ ""String_Node_Str""+ stopGracefully+ ""String_Node_Str""+ super.toString();
}"
5582,"@Override public DataStreamsPipelineSpec generateSpec(DataStreamsConfig config){
  long batchIntervalMillis;
  try {
    batchIntervalMillis=TimeParser.parseDuration(config.getBatchInterval());
  }
 catch (  Exception e) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",config.getBatchInterval()));
  }
  DataStreamsPipelineSpec.Builder specBuilder=DataStreamsPipelineSpec.builder(batchIntervalMillis).setExtraJavaOpts(config.getExtraJavaOpts());
  configureStages(config,specBuilder);
  return specBuilder.build();
}","@Override public DataStreamsPipelineSpec generateSpec(DataStreamsConfig config){
  long batchIntervalMillis;
  try {
    batchIntervalMillis=TimeParser.parseDuration(config.getBatchInterval());
  }
 catch (  Exception e) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",config.getBatchInterval()));
  }
  DataStreamsPipelineSpec.Builder specBuilder=DataStreamsPipelineSpec.builder(batchIntervalMillis).setExtraJavaOpts(config.getExtraJavaOpts()).setStopGracefully(config.getStopGracefully());
  configureStages(config,specBuilder);
  return specBuilder.build();
}"
5583,"public Builder(){
  this.isUnitTest=true;
  this.batchInterval=""String_Node_Str"";
  this.driverResources=new Resources();
}","public Builder(){
  this.isUnitTest=true;
  this.batchInterval=""String_Node_Str"";
  this.stopGraceFully=false;
}"
5584,"private DataStreamsConfig(Set<ETLStage> stages,Set<Connection> connections,Resources resources,Resources driverResources,Resources clientResources,boolean stageLoggingEnabled,String batchInterval,boolean isUnitTest,boolean disableCheckpoints,@Nullable String checkpointDir,int numOfRecordsPreview){
  super(stages,connections,resources,driverResources,clientResources,stageLoggingEnabled,numOfRecordsPreview);
  this.batchInterval=batchInterval;
  this.isUnitTest=isUnitTest;
  this.extraJavaOpts=""String_Node_Str"";
  this.disableCheckpoints=disableCheckpoints;
  this.checkpointDir=checkpointDir;
}","private DataStreamsConfig(Set<ETLStage> stages,Set<Connection> connections,Resources resources,Resources driverResources,Resources clientResources,boolean stageLoggingEnabled,String batchInterval,boolean isUnitTest,boolean disableCheckpoints,@Nullable String checkpointDir,int numOfRecordsPreview,boolean stopGracefully){
  super(stages,connections,resources,driverResources,clientResources,stageLoggingEnabled,numOfRecordsPreview);
  this.batchInterval=batchInterval;
  this.isUnitTest=isUnitTest;
  this.extraJavaOpts=""String_Node_Str"";
  this.disableCheckpoints=disableCheckpoints;
  this.checkpointDir=checkpointDir;
  this.stopGracefully=stopGracefully;
}"
5585,"public DataStreamsConfig build(){
  return new DataStreamsConfig(stages,connections,resources,driverResources,clientResources,stageLoggingEnabled,batchInterval,isUnitTest,false,checkpointDir,numOfRecordsPreview);
}","public DataStreamsConfig build(){
  return new DataStreamsConfig(stages,connections,resources,driverResources,clientResources,stageLoggingEnabled,batchInterval,isUnitTest,false,checkpointDir,numOfRecordsPreview,stopGraceFully);
}"
5586,"@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  if (!super.equals(o)) {
    return false;
  }
  DataStreamsConfig that=(DataStreamsConfig)o;
  return Objects.equals(batchInterval,that.batchInterval) && Objects.equals(extraJavaOpts,that.extraJavaOpts) && Objects.equals(disableCheckpoints,that.disableCheckpoints)&& Objects.equals(checkpointDir,that.checkpointDir);
}","@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  if (!super.equals(o)) {
    return false;
  }
  DataStreamsConfig that=(DataStreamsConfig)o;
  return Objects.equals(batchInterval,that.batchInterval) && Objects.equals(extraJavaOpts,that.extraJavaOpts) && Objects.equals(disableCheckpoints,that.disableCheckpoints)&& Objects.equals(checkpointDir,that.checkpointDir)&& Objects.equals(stopGracefully,that.stopGracefully);
}"
5587,"@Override public String toString(){
  return ""String_Node_Str"" + ""String_Node_Str"" + batchInterval + '\''+ ""String_Node_Str""+ extraJavaOpts+ '\''+ ""String_Node_Str""+ disableCheckpoints+ ""String_Node_Str""+ checkpointDir+ '\''+ ""String_Node_Str""+ isUnitTest+ ""String_Node_Str""+ super.toString();
}","@Override public String toString(){
  return ""String_Node_Str"" + ""String_Node_Str"" + batchInterval + '\''+ ""String_Node_Str""+ extraJavaOpts+ '\''+ ""String_Node_Str""+ disableCheckpoints+ ""String_Node_Str""+ checkpointDir+ '\''+ ""String_Node_Str""+ stopGracefully+ ""String_Node_Str""+ isUnitTest+ ""String_Node_Str""+ super.toString();
}"
5588,"@Test public void testSearchResultPagination() throws Exception {
  NamespaceId namespace=new NamespaceId(""String_Node_Str"");
  namespaceClient.create(new NamespaceMeta.Builder().setName(namespace).build());
  StreamId stream=namespace.stream(""String_Node_Str"");
  DatasetId dataset=namespace.dataset(""String_Node_Str"");
  StreamViewId view=stream.view(""String_Node_Str"");
  streamClient.create(stream);
  streamViewClient.createOrUpdate(view,new ViewSpecification(new FormatSpecification(""String_Node_Str"",null,null)));
  datasetClient.create(dataset,new DatasetInstanceConfiguration(Table.class.getName(),Collections.<String,String>emptyMap()));
  EnumSet<MetadataSearchTargetType> targets=EnumSet.allOf(MetadataSearchTargetType.class);
  String sort=AbstractSystemMetadataWriter.ENTITY_NAME_KEY + ""String_Node_Str"";
  MetadataSearchResponse searchResponse=searchMetadata(namespace,""String_Node_Str"",targets,sort,0,1,0,null);
  List<MetadataSearchResultRecord> expectedResults=ImmutableList.of(new MetadataSearchResultRecord(dataset));
  List<String> expectedCursors=ImmutableList.of();
  Assert.assertEquals(expectedResults,new ArrayList<>(searchResponse.getResults()));
  Assert.assertEquals(expectedCursors,searchResponse.getCursors());
  searchResponse=searchMetadata(namespace,""String_Node_Str"",targets,sort,0,1,2,null);
  expectedResults=ImmutableList.of(new MetadataSearchResultRecord(dataset));
  expectedCursors=ImmutableList.of(stream.getEntityName(),view.getEntityName());
  Assert.assertEquals(expectedResults,new ArrayList<>(searchResponse.getResults()));
  Assert.assertEquals(expectedCursors,searchResponse.getCursors());
  searchResponse=searchMetadata(namespace,""String_Node_Str"",targets,sort,1,1,2,null);
  expectedResults=ImmutableList.of(new MetadataSearchResultRecord(stream));
  expectedCursors=ImmutableList.of(view.getEntityName());
  Assert.assertEquals(expectedResults,new ArrayList<>(searchResponse.getResults()));
  Assert.assertEquals(expectedCursors,searchResponse.getCursors());
  searchResponse=searchMetadata(namespace,""String_Node_Str"",targets,sort,2,1,2,null);
  expectedResults=ImmutableList.of(new MetadataSearchResultRecord(view));
  Assert.assertEquals(expectedResults,new ArrayList<>(searchResponse.getResults()));
  Assert.assertTrue(searchResponse.getCursors().isEmpty());
  searchResponse=searchMetadata(namespace,""String_Node_Str"",targets,sort,3,1,2,null);
  Assert.assertTrue(searchResponse.getResults().isEmpty());
  Assert.assertTrue(searchResponse.getCursors().isEmpty());
  searchResponse=searchMetadata(namespace,""String_Node_Str"",targets,sort,0,Integer.MAX_VALUE,4,null);
  expectedResults=ImmutableList.of(new MetadataSearchResultRecord(dataset),new MetadataSearchResultRecord(stream),new MetadataSearchResultRecord(view));
  Assert.assertEquals(expectedResults,new ArrayList<>(searchResponse.getResults()));
  Assert.assertTrue(searchResponse.getCursors().isEmpty());
  namespaceClient.delete(namespace);
}","public void testSearchResultPagination() throws Exception {
  NamespaceId namespace=new NamespaceId(""String_Node_Str"");
  namespaceClient.create(new NamespaceMeta.Builder().setName(namespace).build());
  StreamId stream=namespace.stream(""String_Node_Str"");
  DatasetId dataset=namespace.dataset(""String_Node_Str"");
  StreamViewId view=stream.view(""String_Node_Str"");
  streamClient.create(stream);
  streamViewClient.createOrUpdate(view,new ViewSpecification(new FormatSpecification(""String_Node_Str"",null,null)));
  datasetClient.create(dataset,new DatasetInstanceConfiguration(Table.class.getName(),Collections.<String,String>emptyMap()));
  EnumSet<MetadataSearchTargetType> targets=EnumSet.allOf(MetadataSearchTargetType.class);
  String sort=AbstractSystemMetadataWriter.ENTITY_NAME_KEY + ""String_Node_Str"";
  MetadataSearchResponse searchResponse=searchMetadata(namespace,""String_Node_Str"",targets,sort,0,1,0,null);
  List<MetadataSearchResultRecord> expectedResults=ImmutableList.of(new MetadataSearchResultRecord(dataset));
  List<String> expectedCursors=ImmutableList.of();
  Assert.assertEquals(expectedResults,new ArrayList<>(searchResponse.getResults()));
  Assert.assertEquals(expectedCursors,searchResponse.getCursors());
  searchResponse=searchMetadata(namespace,""String_Node_Str"",targets,sort,0,1,2,null);
  expectedResults=ImmutableList.of(new MetadataSearchResultRecord(dataset));
  expectedCursors=ImmutableList.of(stream.getEntityName(),view.getEntityName());
  Assert.assertEquals(expectedResults,new ArrayList<>(searchResponse.getResults()));
  Assert.assertEquals(expectedCursors,searchResponse.getCursors());
  searchResponse=searchMetadata(namespace,""String_Node_Str"",targets,sort,1,1,2,null);
  expectedResults=ImmutableList.of(new MetadataSearchResultRecord(stream));
  expectedCursors=ImmutableList.of(view.getEntityName());
  Assert.assertEquals(expectedResults,new ArrayList<>(searchResponse.getResults()));
  Assert.assertEquals(expectedCursors,searchResponse.getCursors());
  searchResponse=searchMetadata(namespace,""String_Node_Str"",targets,sort,2,1,2,null);
  expectedResults=ImmutableList.of(new MetadataSearchResultRecord(view));
  Assert.assertEquals(expectedResults,new ArrayList<>(searchResponse.getResults()));
  Assert.assertTrue(searchResponse.getCursors().isEmpty());
  searchResponse=searchMetadata(namespace,""String_Node_Str"",targets,sort,3,1,2,null);
  Assert.assertTrue(searchResponse.getResults().isEmpty());
  Assert.assertTrue(searchResponse.getCursors().isEmpty());
  searchResponse=searchMetadata(namespace,""String_Node_Str"",targets,sort,0,Integer.MAX_VALUE,4,null);
  expectedResults=ImmutableList.of(new MetadataSearchResultRecord(dataset),new MetadataSearchResultRecord(stream),new MetadataSearchResultRecord(view));
  Assert.assertEquals(expectedResults,new ArrayList<>(searchResponse.getResults()));
  Assert.assertTrue(searchResponse.getCursors().isEmpty());
  namespaceClient.delete(namespace);
}"
5589,"private SearchResults searchByCustomIndex(String namespaceId,Set<MetadataSearchTargetType> types,SortInfo sortInfo,int offset,int limit,int numCursors,@Nullable String cursor){
  List<MetadataEntry> results=new ArrayList<>();
  String indexColumn=getIndexColumn(sortInfo.getSortBy(),sortInfo.getSortOrder());
  int fetchSize=offset + ((numCursors + 1) * limit);
  List<String> cursors=new ArrayList<>(numCursors);
  for (  String searchTerm : getSearchTerms(namespaceId,""String_Node_Str"")) {
    byte[] startKey=Bytes.toBytes(searchTerm.substring(0,searchTerm.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    if (!Strings.isNullOrEmpty(cursor)) {
      String namespaceInStartKey=searchTerm.substring(0,searchTerm.indexOf(KEYVALUE_SEPARATOR));
      startKey=Bytes.toBytes(namespaceInStartKey + KEYVALUE_SEPARATOR + cursor);
    }
    int mod=limit == 1 ? 0 : 1;
    try (Scanner scanner=indexedTable.scanByIndex(Bytes.toBytes(indexColumn),startKey,stopKey)){
      Row next;
      int count=0;
      while ((next=scanner.next()) != null && count < fetchSize) {
        if (count++ < offset) {
          continue;
        }
        processRow(results,next,indexColumn,types);
        if (results.size() % limit == mod && results.size() > limit) {
          String cursorWithNamespace=Bytes.toString(next.get(indexColumn));
          cursors.add(cursorWithNamespace.substring(cursorWithNamespace.indexOf(KEYVALUE_SEPARATOR) + 1));
        }
      }
    }
   }
  return new SearchResults(results,cursors);
}","private SearchResults searchByCustomIndex(String namespaceId,Set<MetadataSearchTargetType> types,SortInfo sortInfo,int offset,int limit,int numCursors,@Nullable String cursor){
  List<MetadataEntry> results=new ArrayList<>();
  String indexColumn=getIndexColumn(sortInfo.getSortBy(),sortInfo.getSortOrder());
  int fetchSize=offset + ((numCursors + 1) * limit);
  List<String> cursors=new ArrayList<>(numCursors);
  for (  String searchTerm : getSearchTerms(namespaceId,""String_Node_Str"")) {
    byte[] startKey=Bytes.toBytes(searchTerm.substring(0,searchTerm.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    if (!Strings.isNullOrEmpty(cursor)) {
      String namespaceInStartKey=searchTerm.substring(0,searchTerm.indexOf(KEYVALUE_SEPARATOR));
      startKey=Bytes.toBytes(namespaceInStartKey + KEYVALUE_SEPARATOR + cursor);
    }
    int mod=limit == 1 ? 0 : 1;
    try (Scanner scanner=indexedTable.scanByIndex(Bytes.toBytes(indexColumn),startKey,stopKey)){
      Row next;
      int count=0;
      while ((next=scanner.next()) != null && count < fetchSize) {
        if (count < offset) {
          if (parseRow(next,indexColumn,types).isPresent()) {
            count++;
          }
          continue;
        }
        Optional<MetadataEntry> metadataEntry=parseRow(next,indexColumn,types);
        if (metadataEntry.isPresent()) {
          count++;
          results.add(metadataEntry.get());
        }
        if (results.size() % limit == mod && results.size() > limit) {
          String cursorWithNamespace=Bytes.toString(next.get(indexColumn));
          cursors.add(cursorWithNamespace.substring(cursorWithNamespace.indexOf(KEYVALUE_SEPARATOR) + 1));
        }
      }
    }
   }
  return new SearchResults(results,cursors);
}"
5590,"private SearchResults searchByDefaultIndex(String namespaceId,String searchQuery,Set<MetadataSearchTargetType> types){
  List<MetadataEntry> results=new ArrayList<>();
  for (  String searchTerm : getSearchTerms(namespaceId,searchQuery)) {
    Scanner scanner;
    if (searchTerm.endsWith(""String_Node_Str"")) {
      byte[] startKey=Bytes.toBytes(searchTerm.substring(0,searchTerm.lastIndexOf(""String_Node_Str"")));
      byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
      scanner=indexedTable.scanByIndex(Bytes.toBytes(DEFAULT_INDEX_COLUMN),startKey,stopKey);
    }
 else {
      byte[] value=Bytes.toBytes(searchTerm);
      scanner=indexedTable.readByIndex(Bytes.toBytes(DEFAULT_INDEX_COLUMN),value);
    }
    try {
      Row next;
      while ((next=scanner.next()) != null) {
        processRow(results,next,DEFAULT_INDEX_COLUMN,types);
      }
    }
  finally {
      scanner.close();
    }
  }
  return new SearchResults(results,Collections.<String>emptyList());
}","private SearchResults searchByDefaultIndex(String namespaceId,String searchQuery,Set<MetadataSearchTargetType> types){
  List<MetadataEntry> results=new ArrayList<>();
  for (  String searchTerm : getSearchTerms(namespaceId,searchQuery)) {
    Scanner scanner;
    if (searchTerm.endsWith(""String_Node_Str"")) {
      byte[] startKey=Bytes.toBytes(searchTerm.substring(0,searchTerm.lastIndexOf(""String_Node_Str"")));
      byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
      scanner=indexedTable.scanByIndex(Bytes.toBytes(DEFAULT_INDEX_COLUMN),startKey,stopKey);
    }
 else {
      byte[] value=Bytes.toBytes(searchTerm);
      scanner=indexedTable.readByIndex(Bytes.toBytes(DEFAULT_INDEX_COLUMN),value);
    }
    try {
      Row next;
      while ((next=scanner.next()) != null) {
        Optional<MetadataEntry> metadataEntry=parseRow(next,DEFAULT_INDEX_COLUMN,types);
        if (metadataEntry.isPresent()) {
          results.add(metadataEntry.get());
        }
      }
    }
  finally {
      scanner.close();
    }
  }
  return new SearchResults(results,Collections.<String>emptyList());
}"
5591,"/** 
 * Drop an existing file set.
 * @param set the name of the file set to drop
 */
@POST @Path(""String_Node_Str"") public void drop(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") final String set) throws DatasetManagementException {
  try {
    getContext().getAdmin().dropDataset(set);
  }
 catch (  InstanceNotFoundException e) {
    responder.sendError(404,""String_Node_Str"" + set + ""String_Node_Str"");
    return;
  }
  responder.sendStatus(200);
}","/** 
 * Drop an existing file set.
 * @param set the name of the file set to drop
 */
@POST @Path(""String_Node_Str"") @TransactionPolicy(TransactionControl.EXPLICIT) public void drop(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") final String set) throws DatasetManagementException {
  try {
    getContext().getAdmin().dropDataset(set);
  }
 catch (  InstanceNotFoundException e) {
    responder.sendError(404,""String_Node_Str"" + set + ""String_Node_Str"");
    return;
  }
  responder.sendStatus(200);
}"
5592,"/** 
 * Responds with the content of the file specified by the request.
 * @param set the name of the file set
 * @param filePath the relative path within the file set
 */
@GET @Path(""String_Node_Str"") public void read(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") String set,@QueryParam(""String_Node_Str"") String filePath){
  FileSet fileSet;
  try {
    fileSet=getContext().getDataset(set);
  }
 catch (  DatasetInstantiationException e) {
    LOG.warn(""String_Node_Str"",set,e);
    responder.sendError(400,String.format(""String_Node_Str"",set));
    return;
  }
  Location location=fileSet.getLocation(filePath);
  getContext().discardDataset(fileSet);
  try {
    responder.send(200,location,""String_Node_Str"");
  }
 catch (  IOException e) {
    responder.sendError(400,String.format(""String_Node_Str"",filePath,set));
  }
}","/** 
 * Responds with the content of the file specified by the request.
 * @param set the name of the file set
 * @param filePath the relative path within the file set
 */
@GET @Path(""String_Node_Str"") @TransactionPolicy(TransactionControl.EXPLICIT) public void read(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") String set,@QueryParam(""String_Node_Str"") String filePath){
  FileSet fileSet;
  try {
    fileSet=getContext().getDataset(set);
  }
 catch (  DatasetInstantiationException e) {
    LOG.warn(""String_Node_Str"",set,e);
    responder.sendError(400,String.format(""String_Node_Str"",set));
    return;
  }
  Location location=fileSet.getLocation(filePath);
  getContext().discardDataset(fileSet);
  try {
    responder.send(200,location,""String_Node_Str"");
  }
 catch (  IOException e) {
    responder.sendError(400,String.format(""String_Node_Str"",filePath,set));
  }
}"
5593,"/** 
 * Truncate an existing file set. This will delete all files under the file set's base path.
 * @param set the name of the file set to truncate
 */
@POST @Path(""String_Node_Str"") public void truncate(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") final String set) throws DatasetManagementException {
  try {
    getContext().getAdmin().truncateDataset(set);
  }
 catch (  InstanceNotFoundException e) {
    responder.sendError(404,""String_Node_Str"" + set + ""String_Node_Str"");
    return;
  }
  responder.sendStatus(200);
}","/** 
 * Truncate an existing file set. This will delete all files under the file set's base path.
 * @param set the name of the file set to truncate
 */
@POST @Path(""String_Node_Str"") @TransactionPolicy(TransactionControl.EXPLICIT) public void truncate(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") final String set) throws DatasetManagementException {
  try {
    getContext().getAdmin().truncateDataset(set);
  }
 catch (  InstanceNotFoundException e) {
    responder.sendError(404,""String_Node_Str"" + set + ""String_Node_Str"");
    return;
  }
  responder.sendStatus(200);
}"
5594,"/** 
 * Create a new file set. The properties for the new dataset can be given as JSON in the body of the request. Alternatively the request can specify the name of an existing dataset as a query parameter; in that case, a copy of the properties of that dataset is used to create the new file set. If neither a body nor a clone parameter is present, the dataset is created with empty (that is, default) properties.
 * @param set the name of the file set
 * @param clone the name of an existing dataset. If present, its properties are used for the new dataset.
 */
@POST @Path(""String_Node_Str"") public void create(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") final String set,@Nullable @QueryParam(""String_Node_Str"") final String clone) throws DatasetManagementException {
  DatasetProperties properties=DatasetProperties.EMPTY;
  ByteBuffer content=request.getContent();
  if (clone != null) {
    try {
      properties=getContext().getAdmin().getDatasetProperties(clone);
    }
 catch (    InstanceNotFoundException e) {
      responder.sendError(404,""String_Node_Str"" + clone + ""String_Node_Str"");
      return;
    }
  }
 else   if (content != null && content.hasRemaining()) {
    try {
      properties=GSON.fromJson(Bytes.toString(content),DatasetProperties.class);
    }
 catch (    Exception e) {
      responder.sendError(400,""String_Node_Str"" + e.getMessage());
      return;
    }
  }
  try {
    getContext().getAdmin().createDataset(set,""String_Node_Str"",properties);
  }
 catch (  InstanceConflictException e) {
    responder.sendError(409,""String_Node_Str"" + set + ""String_Node_Str"");
    return;
  }
  responder.sendStatus(200);
}","/** 
 * Create a new file set. The properties for the new dataset can be given as JSON in the body of the request. Alternatively the request can specify the name of an existing dataset as a query parameter; in that case, a copy of the properties of that dataset is used to create the new file set. If neither a body nor a clone parameter is present, the dataset is created with empty (that is, default) properties.
 * @param set the name of the file set
 * @param clone the name of an existing dataset. If present, its properties are used for the new dataset.
 */
@POST @Path(""String_Node_Str"") @TransactionPolicy(TransactionControl.EXPLICIT) public void create(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") final String set,@Nullable @QueryParam(""String_Node_Str"") final String clone) throws DatasetManagementException {
  DatasetProperties properties=DatasetProperties.EMPTY;
  ByteBuffer content=request.getContent();
  if (clone != null) {
    try {
      properties=getContext().getAdmin().getDatasetProperties(clone);
    }
 catch (    InstanceNotFoundException e) {
      responder.sendError(404,""String_Node_Str"" + clone + ""String_Node_Str"");
      return;
    }
  }
 else   if (content != null && content.hasRemaining()) {
    try {
      properties=GSON.fromJson(Bytes.toString(content),DatasetProperties.class);
    }
 catch (    Exception e) {
      responder.sendError(400,""String_Node_Str"" + e.getMessage());
      return;
    }
  }
  try {
    getContext().getAdmin().createDataset(set,""String_Node_Str"",properties);
  }
 catch (  InstanceConflictException e) {
    responder.sendError(409,""String_Node_Str"" + set + ""String_Node_Str"");
    return;
  }
  responder.sendStatus(200);
}"
5595,"/** 
 * Update the properties of a file set. The new properties must be be given as JSON in the body of the request. If no properties are given, the dataset is updated with empty properties.
 * @param set the name of the file set
 */
@POST @Path(""String_Node_Str"") public void update(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") final String set) throws DatasetManagementException {
  DatasetProperties properties=DatasetProperties.EMPTY;
  ByteBuffer content=request.getContent();
  if (content != null && content.hasRemaining()) {
    try {
      properties=GSON.fromJson(Bytes.toString(content),DatasetProperties.class);
    }
 catch (    Exception e) {
      responder.sendError(400,""String_Node_Str"" + e.getMessage());
      return;
    }
  }
  try {
    getContext().getAdmin().updateDataset(set,properties);
  }
 catch (  InstanceNotFoundException e) {
    responder.sendError(404,""String_Node_Str"" + set + ""String_Node_Str"");
    return;
  }
  responder.sendStatus(200);
}","/** 
 * Update the properties of a file set. The new properties must be be given as JSON in the body of the request. If no properties are given, the dataset is updated with empty properties.
 * @param set the name of the file set
 */
@POST @Path(""String_Node_Str"") @TransactionPolicy(TransactionControl.EXPLICIT) public void update(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") final String set) throws DatasetManagementException {
  DatasetProperties properties=DatasetProperties.EMPTY;
  ByteBuffer content=request.getContent();
  if (content != null && content.hasRemaining()) {
    try {
      properties=GSON.fromJson(Bytes.toString(content),DatasetProperties.class);
    }
 catch (    Exception e) {
      responder.sendError(400,""String_Node_Str"" + e.getMessage());
      return;
    }
  }
  try {
    getContext().getAdmin().updateDataset(set,properties);
  }
 catch (  InstanceNotFoundException e) {
    responder.sendError(404,""String_Node_Str"" + set + ""String_Node_Str"");
    return;
  }
  responder.sendStatus(200);
}"
5596,"/** 
 * Upload the content for a new file at the location specified by thee request.
 * @param set the name of the file set
 * @param filePath the relative path within the file set
 */
@PUT @Path(""String_Node_Str"") public HttpContentConsumer write(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") final String set,@QueryParam(""String_Node_Str"") final String filePath){
  FileSet fileSet;
  try {
    fileSet=getContext().getDataset(set);
  }
 catch (  DatasetInstantiationException e) {
    LOG.warn(""String_Node_Str"",set,e);
    responder.sendError(400,String.format(""String_Node_Str"",set));
    return null;
  }
  final Location location=fileSet.getLocation(filePath);
  getContext().discardDataset(fileSet);
  try {
    final WritableByteChannel channel=Channels.newChannel(location.getOutputStream());
    return new HttpContentConsumer(){
      @Override public void onReceived(      ByteBuffer chunk,      Transactional transactional) throws Exception {
        channel.write(chunk);
      }
      @Override public void onFinish(      HttpServiceResponder responder) throws Exception {
        channel.close();
        responder.sendStatus(200);
      }
      @Override public void onError(      HttpServiceResponder responder,      Throwable failureCause){
        Closeables.closeQuietly(channel);
        try {
          location.delete();
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"",location,e);
        }
        LOG.debug(""String_Node_Str"",filePath,set,failureCause);
        responder.sendError(400,String.format(""String_Node_Str"",filePath,set,failureCause.getMessage()));
      }
    }
;
  }
 catch (  IOException e) {
    responder.sendError(400,String.format(""String_Node_Str"",filePath,set,e.getMessage()));
    return null;
  }
}","/** 
 * Upload the content for a new file at the location specified by thee request.
 * @param set the name of the file set
 * @param filePath the relative path within the file set
 */
@PUT @Path(""String_Node_Str"") @TransactionPolicy(TransactionControl.EXPLICIT) public HttpContentConsumer write(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") final String set,@QueryParam(""String_Node_Str"") final String filePath){
  FileSet fileSet;
  try {
    fileSet=getContext().getDataset(set);
  }
 catch (  DatasetInstantiationException e) {
    LOG.warn(""String_Node_Str"",set,e);
    responder.sendError(400,String.format(""String_Node_Str"",set));
    return null;
  }
  final Location location=fileSet.getLocation(filePath);
  getContext().discardDataset(fileSet);
  try {
    final WritableByteChannel channel=Channels.newChannel(location.getOutputStream());
    return new HttpContentConsumer(){
      @Override public void onReceived(      ByteBuffer chunk,      Transactional transactional) throws Exception {
        channel.write(chunk);
      }
      @Override public void onFinish(      HttpServiceResponder responder) throws Exception {
        channel.close();
        responder.sendStatus(200);
      }
      @Override public void onError(      HttpServiceResponder responder,      Throwable failureCause){
        Closeables.closeQuietly(channel);
        try {
          location.delete();
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"",location,e);
        }
        LOG.debug(""String_Node_Str"",filePath,set,failureCause);
        responder.sendError(400,String.format(""String_Node_Str"",filePath,set,failureCause.getMessage()));
      }
    }
;
  }
 catch (  IOException e) {
    responder.sendError(400,String.format(""String_Node_Str"",filePath,set,e.getMessage()));
    return null;
  }
}"
5597,"/** 
 * Responds with the properties of an existing file set. The properties are returned in JSON format.
 * @param set the name of the file set
 */
@POST @Path(""String_Node_Str"") public void properties(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") final String set) throws DatasetManagementException {
  try {
    DatasetProperties props=getContext().getAdmin().getDatasetProperties(set);
    responder.sendJson(200,props);
  }
 catch (  InstanceNotFoundException e) {
    responder.sendError(404,""String_Node_Str"" + set + ""String_Node_Str"");
  }
}","/** 
 * Responds with the properties of an existing file set. The properties are returned in JSON format.
 * @param set the name of the file set
 */
@POST @Path(""String_Node_Str"") @TransactionPolicy(TransactionControl.EXPLICIT) public void properties(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") final String set) throws DatasetManagementException {
  try {
    DatasetProperties props=getContext().getAdmin().getDatasetProperties(set);
    responder.sendJson(200,props);
  }
 catch (  InstanceNotFoundException e) {
    responder.sendError(404,""String_Node_Str"" + set + ""String_Node_Str"");
  }
}"
5598,"@GET @Path(""String_Node_Str"") public void read(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") String league,@PathParam(""String_Node_Str"") int season){
  PartitionDetail partitionDetail=results.getPartition(PartitionKey.builder().addStringField(""String_Node_Str"",league).addIntField(""String_Node_Str"",season).build());
  if (partitionDetail == null) {
    responder.sendString(404,""String_Node_Str"",Charsets.UTF_8);
    return;
  }
  try {
    responder.send(200,partitionDetail.getLocation().append(""String_Node_Str""),""String_Node_Str"");
  }
 catch (  IOException e) {
    responder.sendError(400,String.format(""String_Node_Str"",partitionDetail.getRelativePath()));
  }
}","@GET @Path(""String_Node_Str"") @TransactionPolicy(TransactionControl.EXPLICIT) public void read(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") final String league,@PathParam(""String_Node_Str"") final int season) throws TransactionFailureException {
  final PartitionKey key=PartitionKey.builder().addStringField(""String_Node_Str"",league).addIntField(""String_Node_Str"",season).build();
  final AtomicReference<PartitionDetail> partitionDetail=new AtomicReference<>();
  getContext().execute(new TxRunnable(){
    @Override public void run(    DatasetContext context) throws Exception {
      partitionDetail.set(results.getPartition(key));
    }
  }
);
  if (partitionDetail.get() == null) {
    responder.sendString(404,""String_Node_Str"",Charsets.UTF_8);
    return;
  }
  try {
    responder.send(200,partitionDetail.get().getLocation().append(""String_Node_Str""),""String_Node_Str"");
  }
 catch (  IOException e) {
    responder.sendError(400,String.format(""String_Node_Str"",partitionDetail.get().getRelativePath()));
  }
}"
5599,"@PUT @Path(""String_Node_Str"") public HttpContentConsumer write(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") String league,@PathParam(""String_Node_Str"") int season){
  PartitionKey key=PartitionKey.builder().addStringField(""String_Node_Str"",league).addIntField(""String_Node_Str"",season).build();
  if (results.getPartition(key) != null) {
    responder.sendString(409,""String_Node_Str"",Charsets.UTF_8);
    return null;
  }
  final PartitionOutput output=results.getPartitionOutput(key);
  try {
    final Location partitionDir=output.getLocation();
    if (!partitionDir.mkdirs()) {
      responder.sendString(409,""String_Node_Str"",Charsets.UTF_8);
      return null;
    }
    final Location location=partitionDir.append(""String_Node_Str"");
    final WritableByteChannel channel=Channels.newChannel(location.getOutputStream());
    return new HttpContentConsumer(){
      @Override public void onReceived(      ByteBuffer chunk,      Transactional transactional) throws Exception {
        channel.write(chunk);
      }
      @Override public void onFinish(      HttpServiceResponder responder) throws Exception {
        channel.close();
        output.addPartition();
        responder.sendStatus(200);
      }
      @Override public void onError(      HttpServiceResponder responder,      Throwable failureCause){
        Closeables.closeQuietly(channel);
        try {
          partitionDir.delete(true);
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"",partitionDir,e);
        }
        LOG.debug(""String_Node_Str"",location,failureCause);
        responder.sendError(400,String.format(""String_Node_Str"",location,failureCause.getMessage()));
      }
    }
;
  }
 catch (  IOException e) {
    responder.sendError(400,String.format(""String_Node_Str"",output.getRelativePath(),e.getMessage()));
    return null;
  }
}","@PUT @Path(""String_Node_Str"") @TransactionPolicy(TransactionControl.EXPLICIT) public HttpContentConsumer write(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") String league,@PathParam(""String_Node_Str"") int season) throws TransactionFailureException {
  final PartitionKey key=PartitionKey.builder().addStringField(""String_Node_Str"",league).addIntField(""String_Node_Str"",season).build();
  final AtomicReference<PartitionDetail> partitionDetail=new AtomicReference<>();
  getContext().execute(new TxRunnable(){
    @Override public void run(    DatasetContext context) throws Exception {
      partitionDetail.set(results.getPartition(key));
    }
  }
);
  if (partitionDetail.get() != null) {
    responder.sendString(409,""String_Node_Str"",Charsets.UTF_8);
    return null;
  }
  final PartitionOutput output=results.getPartitionOutput(key);
  try {
    final Location partitionDir=output.getLocation();
    if (!partitionDir.mkdirs()) {
      responder.sendString(409,""String_Node_Str"",Charsets.UTF_8);
      return null;
    }
    final Location location=partitionDir.append(""String_Node_Str"");
    final WritableByteChannel channel=Channels.newChannel(location.getOutputStream());
    return new HttpContentConsumer(){
      @Override public void onReceived(      ByteBuffer chunk,      Transactional transactional) throws Exception {
        channel.write(chunk);
      }
      @Override public void onFinish(      HttpServiceResponder responder) throws Exception {
        channel.close();
        output.addPartition();
        responder.sendStatus(200);
      }
      @Override public void onError(      HttpServiceResponder responder,      Throwable failureCause){
        Closeables.closeQuietly(channel);
        try {
          partitionDir.delete(true);
        }
 catch (        IOException e) {
          LOG.warn(""String_Node_Str"",partitionDir,e);
        }
        LOG.debug(""String_Node_Str"",location,failureCause);
        responder.sendError(400,String.format(""String_Node_Str"",location,failureCause.getMessage()));
      }
    }
;
  }
 catch (  IOException e) {
    responder.sendError(400,String.format(""String_Node_Str"",output.getRelativePath(),e.getMessage()));
    return null;
  }
}"
5600,"public UpgradeTool() throws Exception {
  this.cConf=CConfiguration.create();
  if (this.cConf.getBoolean(Constants.Security.Authorization.ENABLED)) {
    LOG.info(""String_Node_Str"",getClass().getSimpleName());
    this.cConf.setBoolean(Constants.Security.Authorization.ENABLED,false);
  }
  SecurityUtil.loginForMasterService(cConf);
  this.hConf=HBaseConfiguration.create();
  Injector injector=createInjector();
  this.txService=injector.getInstance(TransactionService.class);
  this.zkClientService=injector.getInstance(ZKClientService.class);
  this.dsFramework=injector.getInstance(DatasetFramework.class);
  this.metadataStore=injector.getInstance(MetadataStore.class);
  this.streamStateStoreUpgrader=injector.getInstance(StreamStateStoreUpgrader.class);
  this.dsUpgrade=injector.getInstance(DatasetUpgrader.class);
  this.dsSpecUpgrader=injector.getInstance(DatasetSpecificationUpgrader.class);
  this.queueAdmin=injector.getInstance(QueueAdmin.class);
  this.nsStore=injector.getInstance(NamespaceStore.class);
  this.authorizationService=injector.getInstance(AuthorizationEnforcementService.class);
  this.datasetBasedStreamSizeScheduleStore=injector.getInstance(DatasetBasedStreamSizeScheduleStore.class);
  this.datasetBasedTimeScheduleStore=injector.getInstance(DatasetBasedTimeScheduleStore.class);
  this.store=injector.getInstance(DefaultStore.class);
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        UpgradeTool.this.stop();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
);
  this.existingEntitySystemMetadataWriter=injector.getInstance(ExistingEntitySystemMetadataWriter.class);
  this.upgradeDatasetServiceManager=injector.getInstance(UpgradeDatasetServiceManager.class);
}","UpgradeTool() throws Exception {
  this.cConf=CConfiguration.create();
  if (this.cConf.getBoolean(Constants.Security.Authorization.ENABLED)) {
    LOG.info(""String_Node_Str"",getClass().getSimpleName());
    this.cConf.setBoolean(Constants.Security.Authorization.ENABLED,false);
  }
  SecurityUtil.loginForMasterService(cConf);
  this.hConf=HBaseConfiguration.create();
  Injector injector=createInjector();
  this.txService=injector.getInstance(TransactionService.class);
  this.zkClientService=injector.getInstance(ZKClientService.class);
  this.dsFramework=injector.getInstance(DatasetFramework.class);
  this.metadataStore=injector.getInstance(MetadataStore.class);
  this.streamStateStoreUpgrader=injector.getInstance(StreamStateStoreUpgrader.class);
  this.dsUpgrade=injector.getInstance(DatasetUpgrader.class);
  this.dsSpecUpgrader=injector.getInstance(DatasetSpecificationUpgrader.class);
  this.queueAdmin=injector.getInstance(QueueAdmin.class);
  this.nsStore=injector.getInstance(NamespaceStore.class);
  this.authorizationService=injector.getInstance(AuthorizationEnforcementService.class);
  this.datasetBasedStreamSizeScheduleStore=injector.getInstance(DatasetBasedStreamSizeScheduleStore.class);
  this.datasetBasedTimeScheduleStore=injector.getInstance(DatasetBasedTimeScheduleStore.class);
  this.store=injector.getInstance(DefaultStore.class);
  this.datasetInstanceManager=injector.getInstance(Key.get(DatasetInstanceManager.class,Names.named(""String_Node_Str"")));
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        UpgradeTool.this.stop();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
);
  this.existingEntitySystemMetadataWriter=injector.getInstance(ExistingEntitySystemMetadataWriter.class);
  this.upgradeDatasetServiceManager=injector.getInstance(UpgradeDatasetServiceManager.class);
}"
5601,"private void performUpgrade() throws Exception {
  performCoprocessorUpgrade();
  LOG.info(""String_Node_Str"");
  store.upgradeAppVersion();
  LOG.info(""String_Node_Str"");
  dsSpecUpgrader.upgrade();
  LOG.info(""String_Node_Str"");
  streamStateStoreUpgrader.upgrade();
  upgradeDatasetServiceManager.startUp();
  LOG.info(""String_Node_Str"");
  datasetBasedStreamSizeScheduleStore.upgrade();
  LOG.info(""String_Node_Str"");
  datasetBasedTimeScheduleStore.upgrade();
  LOG.info(""String_Node_Str"");
  try {
    existingEntitySystemMetadataWriter.write(upgradeDatasetServiceManager.getDSFramework());
    LOG.info(""String_Node_Str"");
    DeletedDatasetMetadataRemover datasetMetadataRemover=new DeletedDatasetMetadataRemover(nsStore,metadataStore,upgradeDatasetServiceManager.getDSFramework());
    datasetMetadataRemover.remove();
    LOG.info(""String_Node_Str"");
    metadataStore.deleteAllIndexes();
    LOG.info(""String_Node_Str"");
    metadataStore.rebuildIndexes();
  }
  finally {
    upgradeDatasetServiceManager.shutDown();
  }
}","private void performUpgrade() throws Exception {
  performCoprocessorUpgrade();
  LOG.info(""String_Node_Str"");
  store.upgradeAppVersion();
  LOG.info(""String_Node_Str"");
  dsSpecUpgrader.upgrade();
  LOG.info(""String_Node_Str"");
  streamStateStoreUpgrader.upgrade();
  LOG.info(""String_Node_Str"");
  upgradeMetadataDatasetSpecs();
  upgradeDatasetServiceManager.startUp();
  LOG.info(""String_Node_Str"");
  datasetBasedStreamSizeScheduleStore.upgrade();
  LOG.info(""String_Node_Str"");
  datasetBasedTimeScheduleStore.upgrade();
  LOG.info(""String_Node_Str"");
  try {
    existingEntitySystemMetadataWriter.write(upgradeDatasetServiceManager.getDSFramework());
    LOG.info(""String_Node_Str"");
    DeletedDatasetMetadataRemover datasetMetadataRemover=new DeletedDatasetMetadataRemover(nsStore,metadataStore,upgradeDatasetServiceManager.getDSFramework());
    datasetMetadataRemover.remove();
    LOG.info(""String_Node_Str"");
    metadataStore.deleteAllIndexes();
    LOG.info(""String_Node_Str"");
    metadataStore.rebuildIndexes();
  }
  finally {
    upgradeDatasetServiceManager.shutDown();
  }
}"
5602,"/** 
 * Create a quartz scheduler. Quartz factory method is not used, because inflexible in allowing custom jobstore and turning off check for new versions.
 * @param store JobStore.
 * @param cConf CConfiguration.
 * @return an instance of {@link org.quartz.Scheduler}
 */
private org.quartz.Scheduler getScheduler(JobStore store,CConfiguration cConf) throws SchedulerException {
  int threadPoolSize=cConf.getInt(Constants.Scheduler.CFG_SCHEDULER_MAX_THREAD_POOL_SIZE);
  ExecutorThreadPool threadPool=new ExecutorThreadPool(threadPoolSize);
  threadPool.initialize();
  String schedulerName=DirectSchedulerFactory.DEFAULT_SCHEDULER_NAME;
  String schedulerInstanceId=DirectSchedulerFactory.DEFAULT_INSTANCE_ID;
  QuartzSchedulerResources qrs=new QuartzSchedulerResources();
  JobRunShellFactory jrsf=new StdJobRunShellFactory();
  qrs.setName(schedulerName);
  qrs.setInstanceId(schedulerInstanceId);
  qrs.setJobRunShellFactory(jrsf);
  qrs.setThreadPool(threadPool);
  qrs.setThreadExecutor(new DefaultThreadExecutor());
  qrs.setJobStore(store);
  qrs.setRunUpdateCheck(false);
  QuartzScheduler qs=new QuartzScheduler(qrs,-1,-1);
  ClassLoadHelper cch=new CascadingClassLoadHelper();
  cch.initialize();
  store.initialize(cch,qs.getSchedulerSignaler());
  org.quartz.Scheduler scheduler=new StdScheduler(qs);
  jrsf.initialize(scheduler);
  qs.initialize();
  return scheduler;
}","/** 
 * Create a quartz scheduler. Quartz factory method is not used, because inflexible in allowing custom jobstore and turning off check for new versions.
 * @param store JobStore.
 * @param cConf CConfiguration.
 * @return an instance of {@link org.quartz.Scheduler}
 */
private org.quartz.Scheduler getScheduler(JobStore store,CConfiguration cConf) throws SchedulerException {
  int threadPoolSize=cConf.getInt(Constants.Scheduler.CFG_SCHEDULER_MAX_THREAD_POOL_SIZE);
  ExecutorThreadPool threadPool=new ExecutorThreadPool(threadPoolSize);
  threadPool.initialize();
  String schedulerName=DirectSchedulerFactory.DEFAULT_SCHEDULER_NAME;
  String schedulerInstanceId=DirectSchedulerFactory.DEFAULT_INSTANCE_ID;
  QuartzSchedulerResources qrs=new QuartzSchedulerResources();
  JobRunShellFactory jrsf=new StdJobRunShellFactory();
  qrs.setName(schedulerName);
  qrs.setInstanceId(schedulerInstanceId);
  qrs.setJobRunShellFactory(jrsf);
  qrs.setThreadPool(threadPool);
  qrs.setThreadExecutor(new DefaultThreadExecutor());
  qrs.setJobStore(store);
  qrs.setRunUpdateCheck(false);
  QuartzScheduler qs=new QuartzScheduler(qrs,-1,-1);
  ClassLoadHelper cch=new CascadingClassLoadHelper();
  cch.initialize();
  store.initialize(cch,qs.getSchedulerSignaler());
  org.quartz.Scheduler scheduler=new StdScheduler(qs);
  jrsf.initialize(scheduler);
  qs.initialize();
  scheduler.getListenerManager().addTriggerListener(new TriggerMisfireLogger());
  return scheduler;
}"
5603,"@Override public void execute(JobExecutionContext context) throws JobExecutionException {
  LOG.debug(""String_Node_Str"",context.getJobDetail().getKey().toString(),context.getTrigger().getKey().toString());
  Trigger trigger=context.getTrigger();
  String key=trigger.getKey().getName();
  String[] parts=key.split(""String_Node_Str"");
  Preconditions.checkArgument(parts.length == 6,""String_Node_Str"",key,parts.length);
  String namespaceId=parts[0];
  String applicationId=parts[1];
  String appVersion=parts[2];
  ProgramType programType=ProgramType.valueOf(parts[3]);
  String programName=parts[4];
  String scheduleName=parts[5];
  LOG.debug(""String_Node_Str"",key);
  ImmutableMap.Builder<String,String> builder=ImmutableMap.builder();
  builder.put(ProgramOptionConstants.RETRY_COUNT,Integer.toString(context.getRefireCount()));
  builder.put(ProgramOptionConstants.SCHEDULE_NAME,scheduleName);
  Map<String,String> userOverrides=ImmutableMap.of(ProgramOptionConstants.LOGICAL_START_TIME,Long.toString(context.getScheduledFireTime().getTime()));
  JobDataMap jobDataMap=trigger.getJobDataMap();
  for (  Map.Entry<String,Object> entry : jobDataMap.entrySet()) {
    builder.put(entry.getKey(),jobDataMap.getString(entry.getKey()));
  }
  ProgramId programId=new ApplicationId(namespaceId,applicationId,appVersion).program(programType,programName);
  try {
    taskRunner.run(programId,builder.build(),userOverrides).get();
  }
 catch (  TaskExecutionException e) {
    throw new JobExecutionException(e.getMessage(),e.getCause(),e.isRefireImmediately());
  }
catch (  Throwable t) {
    LOG.info(""String_Node_Str"",programId,t);
    throw new JobExecutionException(t.getMessage(),t.getCause(),false);
  }
}","@Override public void execute(JobExecutionContext context) throws JobExecutionException {
  LOG.debug(""String_Node_Str"",context.getJobDetail().getKey().toString(),context.getTrigger().getKey().toString());
  Trigger trigger=context.getTrigger();
  String key=trigger.getKey().getName();
  String[] parts=key.split(""String_Node_Str"");
  Preconditions.checkArgument(parts.length == 6,""String_Node_Str"",key,parts.length);
  String namespaceId=parts[0];
  String applicationId=parts[1];
  String appVersion=parts[2];
  ProgramType programType=ProgramType.valueOf(parts[3]);
  String programName=parts[4];
  String scheduleName=parts[5];
  LOG.debug(""String_Node_Str"",key);
  ImmutableMap.Builder<String,String> builder=ImmutableMap.builder();
  builder.put(ProgramOptionConstants.RETRY_COUNT,Integer.toString(context.getRefireCount()));
  builder.put(ProgramOptionConstants.SCHEDULE_NAME,scheduleName);
  Map<String,String> userOverrides=ImmutableMap.of(ProgramOptionConstants.LOGICAL_START_TIME,Long.toString(context.getScheduledFireTime().getTime()));
  JobDataMap jobDataMap=trigger.getJobDataMap();
  for (  Map.Entry<String,Object> entry : jobDataMap.entrySet()) {
    builder.put(entry.getKey(),jobDataMap.getString(entry.getKey()));
  }
  ProgramId programId=new ApplicationId(namespaceId,applicationId,appVersion).program(programType,programName);
  try {
    taskRunner.run(programId,builder.build(),userOverrides).get();
  }
 catch (  TaskExecutionException e) {
    LOG.warn(""String_Node_Str"",programId,e);
    throw new JobExecutionException(e.getMessage(),e.getCause(),e.isRefireImmediately());
  }
catch (  Throwable t) {
    LOG.warn(""String_Node_Str"",programId,t);
    throw new JobExecutionException(t.getMessage(),t.getCause(),false);
  }
}"
5604,"@Inject public DatasetBasedTimeScheduleStore(TransactionExecutorFactory factory,ScheduleStoreTableUtil tableUtil){
  this.tableUtil=tableUtil;
  this.factory=factory;
}","@Inject DatasetBasedTimeScheduleStore(TransactionExecutorFactory factory,ScheduleStoreTableUtil tableUtil,CConfiguration cConf){
  this.tableUtil=tableUtil;
  this.factory=factory;
  this.cConf=cConf;
}"
5605,"@Override public void initialize(ClassLoadHelper loadHelper,SchedulerSignaler schedSignaler){
  super.initialize(loadHelper,schedSignaler);
  try {
    initializeScheduleTable();
    readSchedulesFromPersistentStore();
  }
 catch (  Throwable th) {
    throw Throwables.propagate(th);
  }
}","@Override public void initialize(ClassLoadHelper loadHelper,SchedulerSignaler schedSignaler){
  super.initialize(loadHelper,schedSignaler);
  try {
    setMisfireThreshold(cConf.getLong(Constants.Scheduler.CFG_SCHEDULER_MISFIRE_THRESHOLD_MS));
    initializeScheduleTable();
    readSchedulesFromPersistentStore();
  }
 catch (  Throwable th) {
    throw Throwables.propagate(th);
  }
}"
5606,"private static void schedulerSetup(boolean enablePersistence) throws SchedulerException {
  JobStore js;
  if (enablePersistence) {
    CConfiguration conf=injector.getInstance(CConfiguration.class);
    js=new DatasetBasedTimeScheduleStore(factory,new ScheduleStoreTableUtil(dsFramework,conf));
  }
 else {
    js=new RAMJobStore();
  }
  SimpleThreadPool threadPool=new SimpleThreadPool(10,Thread.NORM_PRIORITY);
  threadPool.initialize();
  DirectSchedulerFactory.getInstance().createScheduler(DUMMY_SCHEDULER_NAME,""String_Node_Str"",threadPool,js);
  scheduler=DirectSchedulerFactory.getInstance().getScheduler(DUMMY_SCHEDULER_NAME);
  scheduler.start();
}","private static void schedulerSetup(boolean enablePersistence) throws SchedulerException {
  JobStore js;
  if (enablePersistence) {
    CConfiguration conf=injector.getInstance(CConfiguration.class);
    js=new DatasetBasedTimeScheduleStore(factory,new ScheduleStoreTableUtil(dsFramework,conf),conf);
  }
 else {
    js=new RAMJobStore();
  }
  SimpleThreadPool threadPool=new SimpleThreadPool(10,Thread.NORM_PRIORITY);
  threadPool.initialize();
  DirectSchedulerFactory.getInstance().createScheduler(DUMMY_SCHEDULER_NAME,""String_Node_Str"",threadPool,js);
  scheduler=DirectSchedulerFactory.getInstance().getScheduler(DUMMY_SCHEDULER_NAME);
  scheduler.start();
}"
5607,"@Override @Nullable public Schema getInputSchema(){
  return inputSchemas.entrySet().iterator().next().getValue();
}","@Override @Nullable public Schema getInputSchema(){
  return inputSchemas.isEmpty() ? null : inputSchemas.entrySet().iterator().next().getValue();
}"
5608,"public void addInputSchema(String stageType,String inputStageName,@Nullable Schema inputSchema){
  if (stageType.equalsIgnoreCase(BatchJoiner.PLUGIN_TYPE) && inputSchema == null) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"" + ""String_Node_Str"",inputStageName));
  }
  if (!stageType.equalsIgnoreCase(BatchJoiner.PLUGIN_TYPE) && !hasSameSchema(inputSchema)) {
    throw new IllegalArgumentException(""String_Node_Str"" + this.stageName);
  }
  inputSchemas.put(inputStageName,inputSchema);
}","public void addInputSchema(String inputStageName,@Nullable Schema inputSchema){
  inputSchemas.put(inputStageName,inputSchema);
}"
5609,"/** 
 * Performs most of the validation and configuration needed by a pipeline. Handles stages, connections, resources, and stage logging settings.
 * @param config user provided ETL config
 * @param specBuilder builder for creating a pipeline spec.
 */
protected void configureStages(ETLConfig config,PipelineSpec.Builder specBuilder){
  List<StageConnections> traversalOrder=validateConfig(config);
  Map<String,DefaultPipelineConfigurer> pluginConfigurers=new HashMap<>(traversalOrder.size());
  Map<String,String> pluginTypes=new HashMap<>(traversalOrder.size());
  for (  StageConnections stageConnections : traversalOrder) {
    String stageName=stageConnections.getStage().getName();
    pluginTypes.put(stageName,stageConnections.getStage().getPlugin().getType());
    pluginConfigurers.put(stageName,new DefaultPipelineConfigurer(configurer,stageName));
  }
  for (  StageConnections stageConnections : traversalOrder) {
    ETLStage stage=stageConnections.getStage();
    String stageName=stage.getName();
    DefaultPipelineConfigurer pluginConfigurer=pluginConfigurers.get(stageName);
    StageSpec stageSpec=configureStage(stageConnections,pluginConfigurer);
    Schema outputSchema=stageSpec.getOutputSchema();
    for (    String outputStageName : stageConnections.getOutputs()) {
      pluginConfigurers.get(outputStageName).getStageConfigurer().addInputSchema(pluginTypes.get(outputStageName),stageName,outputSchema);
    }
    specBuilder.addStage(stageSpec);
  }
  specBuilder.addConnections(config.getConnections()).setResources(config.getResources()).setStageLoggingEnabled(config.isStageLoggingEnabled()).setNumOfRecordsPreview(config.getNumOfRecordsPreview());
}","/** 
 * Performs most of the validation and configuration needed by a pipeline. Handles stages, connections, resources, and stage logging settings.
 * @param config user provided ETL config
 * @param specBuilder builder for creating a pipeline spec.
 */
protected void configureStages(ETLConfig config,PipelineSpec.Builder specBuilder){
  List<StageConnections> traversalOrder=validateConfig(config);
  Map<String,DefaultPipelineConfigurer> pluginConfigurers=new HashMap<>(traversalOrder.size());
  Map<String,String> pluginTypes=new HashMap<>(traversalOrder.size());
  for (  StageConnections stageConnections : traversalOrder) {
    String stageName=stageConnections.getStage().getName();
    pluginTypes.put(stageName,stageConnections.getStage().getPlugin().getType());
    pluginConfigurers.put(stageName,new DefaultPipelineConfigurer(configurer,stageName));
  }
  for (  StageConnections stageConnections : traversalOrder) {
    ETLStage stage=stageConnections.getStage();
    String stageName=stage.getName();
    DefaultPipelineConfigurer pluginConfigurer=pluginConfigurers.get(stageName);
    StageSpec stageSpec=configureStage(stageConnections,pluginConfigurer);
    Schema outputSchema=stageSpec.getOutputSchema();
    for (    String outputStageName : stageConnections.getOutputs()) {
      String outputStageType=pluginTypes.get(outputStageName);
      if (Action.PLUGIN_TYPE.equals(outputStageType)) {
        continue;
      }
      DefaultStageConfigurer outputStageConfigurer=pluginConfigurers.get(outputStageName).getStageConfigurer();
      if (BatchJoiner.PLUGIN_TYPE.equals(outputStageType) && outputSchema == null) {
        throw new IllegalArgumentException(String.format(""String_Node_Str"" + ""String_Node_Str"",stageName));
      }
      if (!outputStageType.equals(BatchJoiner.PLUGIN_TYPE) && !hasSameSchema(outputStageConfigurer.getInputSchemas(),outputSchema)) {
        throw new IllegalArgumentException(""String_Node_Str"" + outputStageName);
      }
      outputStageConfigurer.addInputSchema(stageName,outputSchema);
    }
    specBuilder.addStage(stageSpec);
  }
  specBuilder.addConnections(config.getConnections()).setResources(config.getResources()).setStageLoggingEnabled(config.isStageLoggingEnabled()).setNumOfRecordsPreview(config.getNumOfRecordsPreview());
}"
5610,"@Test public void testGenerateSpec(){
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MOCK_SOURCE)).addStage(new ETLStage(""String_Node_Str"",MOCK_SINK)).addStage(new ETLStage(""String_Node_Str"",MOCK_SINK)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_A)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_A)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_B)).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").build();
  BatchPipelineSpec actual=specGenerator.generateSpec(etlConfig);
  Map<String,String> emptyMap=ImmutableMap.of();
  PipelineSpec expected=BatchPipelineSpec.builder().addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSource.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).setOutputSchema(SCHEMA_A).addOutputs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSink.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_B).addInputs(""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSink.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchemas(ImmutableMap.<String,Schema>of(""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A)).addInputs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_A).setOutputSchema(SCHEMA_A).addInputs(""String_Node_Str"").addOutputs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchemas(ImmutableMap.<String,Schema>of(""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A)).setOutputSchema(SCHEMA_A).addInputs(""String_Node_Str"",""String_Node_Str"").addOutputs(""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchemas(ImmutableMap.<String,Schema>of(""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A)).setOutputSchema(SCHEMA_B).addInputs(""String_Node_Str"",""String_Node_Str"").addOutputs(""String_Node_Str"").build()).addConnections(etlConfig.getConnections()).setResources(etlConfig.getResources()).setStageLoggingEnabled(etlConfig.isStageLoggingEnabled()).build();
  Assert.assertEquals(expected,actual);
}","@Test public void testGenerateSpec(){
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MOCK_SOURCE)).addStage(new ETLStage(""String_Node_Str"",MOCK_SINK)).addStage(new ETLStage(""String_Node_Str"",MOCK_SINK)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_A)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_A)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_B)).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").build();
  BatchPipelineSpec actual=specGenerator.generateSpec(etlConfig);
  Map<String,String> emptyMap=ImmutableMap.of();
  PipelineSpec expected=BatchPipelineSpec.builder().addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSource.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).setOutputSchema(SCHEMA_A).addOutputs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSink.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_B).addInputs(""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSink.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchemas(ImmutableMap.of(""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A)).addInputs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_A).setOutputSchema(SCHEMA_A).addInputs(""String_Node_Str"").addOutputs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchemas(ImmutableMap.of(""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A)).setOutputSchema(SCHEMA_A).addInputs(""String_Node_Str"",""String_Node_Str"").addOutputs(""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchemas(ImmutableMap.of(""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A)).setOutputSchema(SCHEMA_B).addInputs(""String_Node_Str"",""String_Node_Str"").addOutputs(""String_Node_Str"").build()).addConnections(etlConfig.getConnections()).setResources(etlConfig.getResources()).setStageLoggingEnabled(etlConfig.isStageLoggingEnabled()).build();
  Assert.assertEquals(expected,actual);
}"
5611,"@BeforeClass public static void setupTests(){
  MockPluginConfigurer pluginConfigurer=new MockPluginConfigurer();
  Set<ArtifactId> artifactIds=ImmutableSet.of(ARTIFACT_ID);
  pluginConfigurer.addMockPlugin(BatchSource.PLUGIN_TYPE,""String_Node_Str"",new MockPlugin(SCHEMA_A),artifactIds);
  pluginConfigurer.addMockPlugin(Transform.PLUGIN_TYPE,""String_Node_Str"",new MockPlugin(SCHEMA_A),artifactIds);
  pluginConfigurer.addMockPlugin(Transform.PLUGIN_TYPE,""String_Node_Str"",new MockPlugin(SCHEMA_B),artifactIds);
  pluginConfigurer.addMockPlugin(BatchSink.PLUGIN_TYPE,""String_Node_Str"",new MockPlugin(),artifactIds);
  specGenerator=new BatchPipelineSpecGenerator(pluginConfigurer,ImmutableSet.of(BatchSource.PLUGIN_TYPE),ImmutableSet.of(BatchSink.PLUGIN_TYPE),FileSet.class,DatasetProperties.EMPTY);
}","@BeforeClass public static void setupTests(){
  MockPluginConfigurer pluginConfigurer=new MockPluginConfigurer();
  Set<ArtifactId> artifactIds=ImmutableSet.of(ARTIFACT_ID);
  pluginConfigurer.addMockPlugin(BatchSource.PLUGIN_TYPE,""String_Node_Str"",new MockPlugin(SCHEMA_A),artifactIds);
  pluginConfigurer.addMockPlugin(Transform.PLUGIN_TYPE,""String_Node_Str"",new MockPlugin(SCHEMA_A),artifactIds);
  pluginConfigurer.addMockPlugin(Transform.PLUGIN_TYPE,""String_Node_Str"",new MockPlugin(SCHEMA_B),artifactIds);
  pluginConfigurer.addMockPlugin(BatchSink.PLUGIN_TYPE,""String_Node_Str"",new MockPlugin(),artifactIds);
  pluginConfigurer.addMockPlugin(Action.PLUGIN_TYPE,""String_Node_Str"",new MockPlugin(),artifactIds);
  specGenerator=new BatchPipelineSpecGenerator(pluginConfigurer,ImmutableSet.of(BatchSource.PLUGIN_TYPE),ImmutableSet.of(BatchSink.PLUGIN_TYPE),FileSet.class,DatasetProperties.EMPTY);
}"
5612,"public StreamViewId getId(){
  return id;
}","public StreamViewId getId(){
  return id.toEntityId();
}"
5613,"private StreamViewEntry(StreamViewId id,ViewSpecification spec){
  this.id=id;
  this.spec=spec;
}","private StreamViewEntry(StreamViewId id,ViewSpecification spec){
  this.id=id.toId();
  this.spec=spec;
}"
5614,"/** 
 * Create a quartz scheduler. Quartz factory method is not used, because inflexible in allowing custom jobstore and turning off check for new versions.
 * @param store JobStore.
 * @param cConf CConfiguration.
 * @return an instance of {@link org.quartz.Scheduler}
 */
private org.quartz.Scheduler getScheduler(JobStore store,CConfiguration cConf) throws SchedulerException {
  int threadPoolSize=cConf.getInt(Constants.Scheduler.CFG_SCHEDULER_MAX_THREAD_POOL_SIZE);
  ExecutorThreadPool threadPool=new ExecutorThreadPool(threadPoolSize);
  threadPool.initialize();
  String schedulerName=DirectSchedulerFactory.DEFAULT_SCHEDULER_NAME;
  String schedulerInstanceId=DirectSchedulerFactory.DEFAULT_INSTANCE_ID;
  QuartzSchedulerResources qrs=new QuartzSchedulerResources();
  JobRunShellFactory jrsf=new StdJobRunShellFactory();
  qrs.setName(schedulerName);
  qrs.setInstanceId(schedulerInstanceId);
  qrs.setJobRunShellFactory(jrsf);
  qrs.setThreadPool(threadPool);
  qrs.setThreadExecutor(new DefaultThreadExecutor());
  qrs.setJobStore(store);
  qrs.setRunUpdateCheck(false);
  QuartzScheduler qs=new QuartzScheduler(qrs,-1,-1);
  ClassLoadHelper cch=new CascadingClassLoadHelper();
  cch.initialize();
  store.initialize(cch,qs.getSchedulerSignaler());
  org.quartz.Scheduler scheduler=new StdScheduler(qs);
  jrsf.initialize(scheduler);
  qs.initialize();
  return scheduler;
}","/** 
 * Create a quartz scheduler. Quartz factory method is not used, because inflexible in allowing custom jobstore and turning off check for new versions.
 * @param store JobStore.
 * @param cConf CConfiguration.
 * @return an instance of {@link org.quartz.Scheduler}
 */
private org.quartz.Scheduler getScheduler(JobStore store,CConfiguration cConf) throws SchedulerException {
  int threadPoolSize=cConf.getInt(Constants.Scheduler.CFG_SCHEDULER_MAX_THREAD_POOL_SIZE);
  ExecutorThreadPool threadPool=new ExecutorThreadPool(threadPoolSize);
  threadPool.initialize();
  String schedulerName=DirectSchedulerFactory.DEFAULT_SCHEDULER_NAME;
  String schedulerInstanceId=DirectSchedulerFactory.DEFAULT_INSTANCE_ID;
  QuartzSchedulerResources qrs=new QuartzSchedulerResources();
  JobRunShellFactory jrsf=new StdJobRunShellFactory();
  qrs.setName(schedulerName);
  qrs.setInstanceId(schedulerInstanceId);
  qrs.setJobRunShellFactory(jrsf);
  qrs.setThreadPool(threadPool);
  qrs.setThreadExecutor(new DefaultThreadExecutor());
  qrs.setJobStore(store);
  qrs.setRunUpdateCheck(false);
  QuartzScheduler qs=new QuartzScheduler(qrs,-1,-1);
  ClassLoadHelper cch=new CascadingClassLoadHelper();
  cch.initialize();
  store.initialize(cch,qs.getSchedulerSignaler());
  org.quartz.Scheduler scheduler=new StdScheduler(qs);
  jrsf.initialize(scheduler);
  qs.initialize();
  scheduler.getListenerManager().addTriggerListener(new TriggerMisfireLogger());
  return scheduler;
}"
5615,"@Override public void execute(JobExecutionContext context) throws JobExecutionException {
  LOG.debug(""String_Node_Str"",context.getJobDetail().getKey().toString(),context.getTrigger().getKey().toString());
  Trigger trigger=context.getTrigger();
  String key=trigger.getKey().getName();
  String[] parts=key.split(""String_Node_Str"");
  Preconditions.checkArgument(parts.length == 5);
  String namespaceId=parts[0];
  String applicationId=parts[1];
  ProgramType programType=ProgramType.valueOf(parts[2]);
  String programName=parts[3];
  String scheduleName=parts[4];
  LOG.debug(""String_Node_Str"",key);
  ImmutableMap.Builder<String,String> builder=ImmutableMap.builder();
  builder.put(ProgramOptionConstants.RETRY_COUNT,Integer.toString(context.getRefireCount()));
  builder.put(ProgramOptionConstants.SCHEDULE_NAME,scheduleName);
  Map<String,String> userOverrides=ImmutableMap.of(ProgramOptionConstants.LOGICAL_START_TIME,Long.toString(context.getScheduledFireTime().getTime()));
  JobDataMap jobDataMap=trigger.getJobDataMap();
  for (  Map.Entry<String,Object> entry : jobDataMap.entrySet()) {
    builder.put(entry.getKey(),jobDataMap.getString(entry.getKey()));
  }
  ProgramId programId=new ProgramId(namespaceId,applicationId,programType,programName);
  try {
    taskRunner.run(programId.toId(),builder.build(),userOverrides).get();
  }
 catch (  TaskExecutionException e) {
    throw new JobExecutionException(e.getMessage(),e.getCause(),e.isRefireImmediately());
  }
catch (  Throwable t) {
    LOG.info(""String_Node_Str"",programId,t);
    throw new JobExecutionException(t.getMessage(),t.getCause(),false);
  }
}","@Override public void execute(JobExecutionContext context) throws JobExecutionException {
  LOG.debug(""String_Node_Str"",context.getJobDetail().getKey().toString(),context.getTrigger().getKey().toString());
  Trigger trigger=context.getTrigger();
  String key=trigger.getKey().getName();
  String[] parts=key.split(""String_Node_Str"");
  Preconditions.checkArgument(parts.length == 5);
  String namespaceId=parts[0];
  String applicationId=parts[1];
  ProgramType programType=ProgramType.valueOf(parts[2]);
  String programName=parts[3];
  String scheduleName=parts[4];
  LOG.debug(""String_Node_Str"",key);
  ImmutableMap.Builder<String,String> builder=ImmutableMap.builder();
  builder.put(ProgramOptionConstants.RETRY_COUNT,Integer.toString(context.getRefireCount()));
  builder.put(ProgramOptionConstants.SCHEDULE_NAME,scheduleName);
  Map<String,String> userOverrides=ImmutableMap.of(ProgramOptionConstants.LOGICAL_START_TIME,Long.toString(context.getScheduledFireTime().getTime()));
  JobDataMap jobDataMap=trigger.getJobDataMap();
  for (  Map.Entry<String,Object> entry : jobDataMap.entrySet()) {
    builder.put(entry.getKey(),jobDataMap.getString(entry.getKey()));
  }
  ProgramId programId=new ProgramId(namespaceId,applicationId,programType,programName);
  try {
    taskRunner.run(programId.toId(),builder.build(),userOverrides).get();
  }
 catch (  TaskExecutionException e) {
    LOG.warn(""String_Node_Str"",programId,e);
    throw new JobExecutionException(e.getMessage(),e.getCause(),e.isRefireImmediately());
  }
catch (  Throwable t) {
    LOG.warn(""String_Node_Str"",programId,t);
    throw new JobExecutionException(t.getMessage(),t.getCause(),false);
  }
}"
5616,"@Inject public DatasetBasedTimeScheduleStore(TransactionExecutorFactory factory,ScheduleStoreTableUtil tableUtil){
  this.tableUtil=tableUtil;
  this.factory=factory;
}","@Inject DatasetBasedTimeScheduleStore(TransactionExecutorFactory factory,ScheduleStoreTableUtil tableUtil,CConfiguration cConf){
  this.tableUtil=tableUtil;
  this.factory=factory;
  this.cConf=cConf;
}"
5617,"@Override public void initialize(ClassLoadHelper loadHelper,SchedulerSignaler schedSignaler){
  super.initialize(loadHelper,schedSignaler);
  try {
    initializeScheduleTable();
    readSchedulesFromPersistentStore();
  }
 catch (  Throwable th) {
    throw Throwables.propagate(th);
  }
}","@Override public void initialize(ClassLoadHelper loadHelper,SchedulerSignaler schedSignaler){
  super.initialize(loadHelper,schedSignaler);
  try {
    setMisfireThreshold(cConf.getLong(Constants.Scheduler.CFG_SCHEDULER_MISFIRE_THRESHOLD_MS));
    initializeScheduleTable();
    readSchedulesFromPersistentStore();
  }
 catch (  Throwable th) {
    throw Throwables.propagate(th);
  }
}"
5618,"private static void schedulerSetup(boolean enablePersistence) throws SchedulerException {
  JobStore js;
  if (enablePersistence) {
    CConfiguration conf=injector.getInstance(CConfiguration.class);
    js=new DatasetBasedTimeScheduleStore(factory,new ScheduleStoreTableUtil(dsFramework,conf));
  }
 else {
    js=new RAMJobStore();
  }
  SimpleThreadPool threadPool=new SimpleThreadPool(10,Thread.NORM_PRIORITY);
  threadPool.initialize();
  DirectSchedulerFactory.getInstance().createScheduler(DUMMY_SCHEDULER_NAME,""String_Node_Str"",threadPool,js);
  scheduler=DirectSchedulerFactory.getInstance().getScheduler(DUMMY_SCHEDULER_NAME);
  scheduler.start();
}","private static void schedulerSetup(boolean enablePersistence) throws SchedulerException {
  JobStore js;
  if (enablePersistence) {
    CConfiguration conf=injector.getInstance(CConfiguration.class);
    js=new DatasetBasedTimeScheduleStore(factory,new ScheduleStoreTableUtil(dsFramework,conf),conf);
  }
 else {
    js=new RAMJobStore();
  }
  SimpleThreadPool threadPool=new SimpleThreadPool(10,Thread.NORM_PRIORITY);
  threadPool.initialize();
  DirectSchedulerFactory.getInstance().createScheduler(DUMMY_SCHEDULER_NAME,""String_Node_Str"",threadPool,js);
  scheduler=DirectSchedulerFactory.getInstance().getScheduler(DUMMY_SCHEDULER_NAME);
  scheduler.start();
}"
5619,"private void addProgram(String phaseName,WorkflowProgramAdder programAdder){
  PipelinePhase phase=plan.getPhase(phaseName);
  if (phase == null) {
    return;
  }
  String programName=""String_Node_Str"" + phaseNum;
  phaseNum++;
  for (  StageInfo connectorInfo : phase.getStagesOfType(Constants.CONNECTOR_TYPE)) {
    String connectorName=connectorInfo.getName();
    String datasetName=connectorDatasets.get(connectorName);
    if (datasetName == null) {
      datasetName=""String_Node_Str"" + connectorNum++;
      connectorDatasets.put(connectorName,datasetName);
      ConnectorSource connectorSource=new ConnectorSource(datasetName,null);
      connectorSource.configure(getConfigurer());
    }
  }
  Map<String,String> phaseConnectorDatasets=new HashMap<>();
  for (  StageInfo connectorStage : phase.getStagesOfType(Constants.CONNECTOR_TYPE)) {
    phaseConnectorDatasets.put(connectorStage.getName(),connectorDatasets.get(connectorStage.getName()));
  }
  BatchPhaseSpec batchPhaseSpec=new BatchPhaseSpec(programName,phase,spec.getResources(),spec.getDriverResources(),spec.isStageLoggingEnabled(),phaseConnectorDatasets,spec.getNumOfRecordsPreview());
  boolean hasCustomAction=batchPhaseSpec.getPhase().getSources().isEmpty() && batchPhaseSpec.getPhase().getSinks().isEmpty();
  if (hasCustomAction) {
    programAdder.addAction(new PipelineAction(batchPhaseSpec));
    return;
  }
  if (useSpark) {
    applicationConfigurer.addSpark(new ETLSpark(batchPhaseSpec));
    programAdder.addSpark(programName);
  }
 else {
    applicationConfigurer.addMapReduce(new ETLMapReduce(batchPhaseSpec));
    programAdder.addMapReduce(programName);
  }
}","private void addProgram(String phaseName,WorkflowProgramAdder programAdder){
  PipelinePhase phase=plan.getPhase(phaseName);
  if (phase == null) {
    return;
  }
  String programName=""String_Node_Str"" + phaseNum;
  phaseNum++;
  for (  StageInfo connectorInfo : phase.getStagesOfType(Constants.CONNECTOR_TYPE)) {
    String connectorName=connectorInfo.getName();
    String datasetName=connectorDatasets.get(connectorName);
    if (datasetName == null) {
      datasetName=""String_Node_Str"" + connectorNum++;
      connectorDatasets.put(connectorName,datasetName);
      ConnectorSource connectorSource=new ConnectorSource(datasetName,null);
      connectorSource.configure(getConfigurer());
    }
  }
  Map<String,String> phaseConnectorDatasets=new HashMap<>();
  for (  StageInfo connectorStage : phase.getStagesOfType(Constants.CONNECTOR_TYPE)) {
    phaseConnectorDatasets.put(connectorStage.getName(),connectorDatasets.get(connectorStage.getName()));
  }
  BatchPhaseSpec batchPhaseSpec=new BatchPhaseSpec(programName,phase,spec.getResources(),spec.getDriverResources(),spec.getClientResources(),spec.isStageLoggingEnabled(),phaseConnectorDatasets,spec.getNumOfRecordsPreview());
  boolean hasCustomAction=batchPhaseSpec.getPhase().getSources().isEmpty() && batchPhaseSpec.getPhase().getSinks().isEmpty();
  if (hasCustomAction) {
    programAdder.addAction(new PipelineAction(batchPhaseSpec));
    return;
  }
  if (useSpark) {
    applicationConfigurer.addSpark(new ETLSpark(batchPhaseSpec));
    programAdder.addSpark(programName);
  }
 else {
    applicationConfigurer.addMapReduce(new ETLMapReduce(batchPhaseSpec));
    programAdder.addMapReduce(programName);
  }
}"
5620,"public DataStreamsPipelineSpec build(){
  return new DataStreamsPipelineSpec(stages,connections,resources,driverResources == null ? resources : driverResources,stageLoggingEnabled,batchIntervalMillis,extraJavaOpts,numOfRecordsPreview);
}","public DataStreamsPipelineSpec build(){
  return new DataStreamsPipelineSpec(stages,connections,resources,driverResources,clientResources,stageLoggingEnabled,batchIntervalMillis,extraJavaOpts,numOfRecordsPreview);
}"
5621,"@Override public int hashCode(){
  return Objects.hash(super.hashCode(),driverResources,batchIntervalMillis,extraJavaOpts);
}","@Override public int hashCode(){
  return Objects.hash(super.hashCode(),batchIntervalMillis,extraJavaOpts);
}"
5622,"@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  if (!super.equals(o)) {
    return false;
  }
  DataStreamsPipelineSpec that=(DataStreamsPipelineSpec)o;
  return batchIntervalMillis == that.batchIntervalMillis && Objects.equals(driverResources,that.driverResources) && Objects.equals(extraJavaOpts,that.extraJavaOpts);
}","@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  if (!super.equals(o)) {
    return false;
  }
  DataStreamsPipelineSpec that=(DataStreamsPipelineSpec)o;
  return batchIntervalMillis == that.batchIntervalMillis && Objects.equals(extraJavaOpts,that.extraJavaOpts);
}"
5623,"private DataStreamsPipelineSpec(Set<StageSpec> stages,Set<Connection> connections,Resources resources,Resources driverResources,boolean stageLoggingEnabled,long batchIntervalMillis,String extraJavaOpts,int numOfRecordsPreview){
  super(stages,connections,resources,stageLoggingEnabled,numOfRecordsPreview);
  this.driverResources=driverResources;
  this.batchIntervalMillis=batchIntervalMillis;
  this.extraJavaOpts=extraJavaOpts;
}","private DataStreamsPipelineSpec(Set<StageSpec> stages,Set<Connection> connections,Resources resources,Resources driverResources,Resources clientResources,boolean stageLoggingEnabled,long batchIntervalMillis,String extraJavaOpts,int numOfRecordsPreview){
  super(stages,connections,resources,driverResources,clientResources,stageLoggingEnabled,numOfRecordsPreview);
  this.batchIntervalMillis=batchIntervalMillis;
  this.extraJavaOpts=extraJavaOpts;
}"
5624,"@Override public String toString(){
  return ""String_Node_Str"" + ""String_Node_Str"" + batchIntervalMillis + ""String_Node_Str""+ driverResources+ ""String_Node_Str""+ extraJavaOpts+ '\''+ ""String_Node_Str""+ super.toString();
}","@Override public String toString(){
  return ""String_Node_Str"" + ""String_Node_Str"" + batchIntervalMillis + ""String_Node_Str""+ extraJavaOpts+ '\''+ ""String_Node_Str""+ super.toString();
}"
5625,"@Override public DataStreamsPipelineSpec generateSpec(DataStreamsConfig config){
  long batchIntervalMillis;
  try {
    batchIntervalMillis=TimeParser.parseDuration(config.getBatchInterval());
  }
 catch (  Exception e) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",config.getBatchInterval()));
  }
  DataStreamsPipelineSpec.Builder specBuilder=DataStreamsPipelineSpec.builder(batchIntervalMillis).setDriverResources(config.getDriverResources()).setExtraJavaOpts(config.getExtraJavaOpts());
  configureStages(config,specBuilder);
  return specBuilder.build();
}","@Override public DataStreamsPipelineSpec generateSpec(DataStreamsConfig config){
  long batchIntervalMillis;
  try {
    batchIntervalMillis=TimeParser.parseDuration(config.getBatchInterval());
  }
 catch (  Exception e) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",config.getBatchInterval()));
  }
  DataStreamsPipelineSpec.Builder specBuilder=DataStreamsPipelineSpec.builder(batchIntervalMillis).setExtraJavaOpts(config.getExtraJavaOpts());
  configureStages(config,specBuilder);
  return specBuilder.build();
}"
5626,"@Override protected void configure(){
  setName(NAME);
  setMainClass(SparkStreamingPipelineDriver.class);
  setExecutorResources(pipelineSpec.getResources());
  setDriverResources(pipelineSpec.getDriverResources());
  int numSources=0;
  for (  StageSpec stageSpec : pipelineSpec.getStages()) {
    if (StreamingSource.PLUGIN_TYPE.equals(stageSpec.getPlugin().getType())) {
      numSources++;
    }
  }
  Map<String,String> properties=new HashMap<>();
  properties.put(Constants.PIPELINEID,GSON.toJson(pipelineSpec));
  properties.put(IS_UNIT_TEST,String.valueOf(config.isUnitTest()));
  properties.put(NUM_SOURCES,String.valueOf(numSources));
  properties.put(EXTRA_OPTS,pipelineSpec.getExtraJavaOpts());
  properties.put(CHECKPOINT_DIR,config.getCheckpointDir() == null ? UUID.randomUUID().toString() : config.getCheckpointDir());
  properties.put(CHECKPOINTS_DISABLED,String.valueOf(config.checkpointsDisabled()));
  setProperties(properties);
}","@Override protected void configure(){
  setName(NAME);
  setMainClass(SparkStreamingPipelineDriver.class);
  setExecutorResources(pipelineSpec.getResources());
  setDriverResources(pipelineSpec.getDriverResources());
  setClientResources(pipelineSpec.getClientResources());
  int numSources=0;
  for (  StageSpec stageSpec : pipelineSpec.getStages()) {
    if (StreamingSource.PLUGIN_TYPE.equals(stageSpec.getPlugin().getType())) {
      numSources++;
    }
  }
  Map<String,String> properties=new HashMap<>();
  properties.put(Constants.PIPELINEID,GSON.toJson(pipelineSpec));
  properties.put(IS_UNIT_TEST,String.valueOf(config.isUnitTest()));
  properties.put(NUM_SOURCES,String.valueOf(numSources));
  properties.put(EXTRA_OPTS,pipelineSpec.getExtraJavaOpts());
  properties.put(CHECKPOINT_DIR,config.getCheckpointDir() == null ? UUID.randomUUID().toString() : config.getCheckpointDir());
  properties.put(CHECKPOINTS_DISABLED,String.valueOf(config.checkpointsDisabled()));
  setProperties(properties);
}"
5627,"@Override public void configure(){
  ETLBatchConfig config=getConfig().convertOldConfig();
  setDescription(DEFAULT_DESCRIPTION);
  PipelineSpecGenerator<ETLBatchConfig,BatchPipelineSpec> specGenerator=new BatchPipelineSpecGenerator(getConfigurer(),ImmutableSet.of(BatchSource.PLUGIN_TYPE),ImmutableSet.of(BatchSink.PLUGIN_TYPE),TimePartitionedFileSet.class,FileSetProperties.builder().setInputFormat(AvroKeyInputFormat.class).setOutputFormat(AvroKeyOutputFormat.class).setEnableExploreOnCreate(true).setSerDe(""String_Node_Str"").setExploreInputFormat(""String_Node_Str"").setExploreOutputFormat(""String_Node_Str"").setTableProperty(""String_Node_Str"",Constants.ERROR_SCHEMA.toString()).build());
  BatchPipelineSpec spec=specGenerator.generateSpec(config);
  int sourceCount=0;
  for (  StageSpec stageSpec : spec.getStages()) {
    if (BatchSource.PLUGIN_TYPE.equals(stageSpec.getPlugin().getType())) {
      sourceCount++;
    }
  }
  if (sourceCount != 1) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  PipelinePlanner planner=new PipelinePlanner(SUPPORTED_PLUGIN_TYPES,ImmutableSet.<String>of(),ImmutableSet.<String>of());
  PipelinePlan plan=planner.plan(spec);
  if (plan.getPhases().size() != 1) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  PipelinePhase pipeline=plan.getPhases().values().iterator().next();
switch (config.getEngine()) {
case MAPREDUCE:
    BatchPhaseSpec batchPhaseSpec=new BatchPhaseSpec(ETLMapReduce.NAME,pipeline,config.getResources(),config.getDriverResources(),config.isStageLoggingEnabled(),new HashMap<String,String>(),config.getNumOfRecordsPreview());
  addMapReduce(new ETLMapReduce(batchPhaseSpec));
break;
case SPARK:
batchPhaseSpec=new BatchPhaseSpec(ETLSpark.class.getSimpleName(),pipeline,config.getResources(),config.getDriverResources(),config.isStageLoggingEnabled(),new HashMap<String,String>(),config.getNumOfRecordsPreview());
addSpark(new ETLSpark(batchPhaseSpec));
break;
default :
throw new IllegalArgumentException(String.format(""String_Node_Str"",config.getEngine(),Joiner.on(',').join(Engine.values())));
}
addWorkflow(new ETLWorkflow(spec,config.getEngine()));
scheduleWorkflow(Schedules.builder(SCHEDULE_NAME).setDescription(""String_Node_Str"").createTimeSchedule(config.getSchedule()),ETLWorkflow.NAME);
}","@Override public void configure(){
  ETLBatchConfig config=getConfig().convertOldConfig();
  setDescription(DEFAULT_DESCRIPTION);
  PipelineSpecGenerator<ETLBatchConfig,BatchPipelineSpec> specGenerator=new BatchPipelineSpecGenerator(getConfigurer(),ImmutableSet.of(BatchSource.PLUGIN_TYPE),ImmutableSet.of(BatchSink.PLUGIN_TYPE),TimePartitionedFileSet.class,FileSetProperties.builder().setInputFormat(AvroKeyInputFormat.class).setOutputFormat(AvroKeyOutputFormat.class).setEnableExploreOnCreate(true).setSerDe(""String_Node_Str"").setExploreInputFormat(""String_Node_Str"").setExploreOutputFormat(""String_Node_Str"").setTableProperty(""String_Node_Str"",Constants.ERROR_SCHEMA.toString()).build());
  BatchPipelineSpec spec=specGenerator.generateSpec(config);
  int sourceCount=0;
  for (  StageSpec stageSpec : spec.getStages()) {
    if (BatchSource.PLUGIN_TYPE.equals(stageSpec.getPlugin().getType())) {
      sourceCount++;
    }
  }
  if (sourceCount != 1) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  PipelinePlanner planner=new PipelinePlanner(SUPPORTED_PLUGIN_TYPES,ImmutableSet.<String>of(),ImmutableSet.<String>of());
  PipelinePlan plan=planner.plan(spec);
  if (plan.getPhases().size() != 1) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  PipelinePhase pipeline=plan.getPhases().values().iterator().next();
switch (config.getEngine()) {
case MAPREDUCE:
    BatchPhaseSpec batchPhaseSpec=new BatchPhaseSpec(ETLMapReduce.NAME,pipeline,config.getResources(),config.getDriverResources(),config.getClientResources(),config.isStageLoggingEnabled(),new HashMap<String,String>(),config.getNumOfRecordsPreview());
  addMapReduce(new ETLMapReduce(batchPhaseSpec));
break;
case SPARK:
batchPhaseSpec=new BatchPhaseSpec(ETLSpark.class.getSimpleName(),pipeline,config.getResources(),config.getDriverResources(),config.getClientResources(),config.isStageLoggingEnabled(),new HashMap<String,String>(),config.getNumOfRecordsPreview());
addSpark(new ETLSpark(batchPhaseSpec));
break;
default :
throw new IllegalArgumentException(String.format(""String_Node_Str"",config.getEngine(),Joiner.on(',').join(Engine.values())));
}
addWorkflow(new ETLWorkflow(spec,config.getEngine()));
scheduleWorkflow(Schedules.builder(SCHEDULE_NAME).setDescription(""String_Node_Str"").createTimeSchedule(config.getSchedule()),ETLWorkflow.NAME);
}"
5628,"@Test public void testDescription() throws Exception {
  PipelinePhase.Builder builder=PipelinePhase.builder(ImmutableSet.of(BatchSource.PLUGIN_TYPE,Constants.CONNECTOR_TYPE)).addStage(StageInfo.builder(""String_Node_Str"",BatchSource.PLUGIN_TYPE).build()).addStage(StageInfo.builder(""String_Node_Str"",BatchSource.PLUGIN_TYPE).addInputSchema(""String_Node_Str"",Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)))).build()).addStage(StageInfo.builder(""String_Node_Str"",Constants.CONNECTOR_TYPE).build()).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"");
  BatchPhaseSpec phaseSpec=new BatchPhaseSpec(""String_Node_Str"",builder.build(),new Resources(),new Resources(),false,Collections.<String,String>emptyMap(),0);
  String phaseSpecStr=GSON.toJson(phaseSpec);
  Assert.assertEquals(""String_Node_Str"",phaseSpec.getDescription());
}","@Test public void testDescription() throws Exception {
  PipelinePhase.Builder builder=PipelinePhase.builder(ImmutableSet.of(BatchSource.PLUGIN_TYPE,Constants.CONNECTOR_TYPE)).addStage(StageInfo.builder(""String_Node_Str"",BatchSource.PLUGIN_TYPE).build()).addStage(StageInfo.builder(""String_Node_Str"",BatchSource.PLUGIN_TYPE).addInputSchema(""String_Node_Str"",Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)))).build()).addStage(StageInfo.builder(""String_Node_Str"",Constants.CONNECTOR_TYPE).build()).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"");
  BatchPhaseSpec phaseSpec=new BatchPhaseSpec(""String_Node_Str"",builder.build(),new Resources(),new Resources(),new Resources(),false,Collections.<String,String>emptyMap(),0);
  String phaseSpecStr=GSON.toJson(phaseSpec);
  Assert.assertEquals(""String_Node_Str"",phaseSpec.getDescription());
}"
5629,"public BatchPhaseSpec(String phaseName,PipelinePhase phase,Resources resources,Resources driverResources,boolean isStageLoggingEnabled,Map<String,String> connectorDatasets,int numOfRecordsPreview){
  this.phaseName=phaseName;
  this.phase=phase;
  this.resources=resources;
  this.driverResources=driverResources;
  this.isStageLoggingEnabled=isStageLoggingEnabled;
  this.connectorDatasets=connectorDatasets;
  this.description=createDescription();
  this.numOfRecordsPreview=numOfRecordsPreview;
}","public BatchPhaseSpec(String phaseName,PipelinePhase phase,Resources resources,Resources driverResources,Resources clientResources,boolean isStageLoggingEnabled,Map<String,String> connectorDatasets,int numOfRecordsPreview){
  this.phaseName=phaseName;
  this.phase=phase;
  this.resources=resources;
  this.driverResources=driverResources;
  this.clientResources=clientResources;
  this.isStageLoggingEnabled=isStageLoggingEnabled;
  this.connectorDatasets=connectorDatasets;
  this.description=createDescription();
  this.numOfRecordsPreview=numOfRecordsPreview;
}"
5630,"private BatchPipelineSpec(Set<StageSpec> stages,Set<Connection> connections,Resources resources,Resources driverResources,boolean stageLoggingEnabled,List<ActionSpec> endingActions,int numOfRecordsPreview){
  super(stages,connections,resources,stageLoggingEnabled,numOfRecordsPreview);
  this.endingActions=ImmutableList.copyOf(endingActions);
  this.driverResources=driverResources;
}","private BatchPipelineSpec(Set<StageSpec> stages,Set<Connection> connections,Resources resources,Resources driverResources,Resources clientResources,boolean stageLoggingEnabled,List<ActionSpec> endingActions,int numOfRecordsPreview){
  super(stages,connections,resources,driverResources,clientResources,stageLoggingEnabled,numOfRecordsPreview);
  this.endingActions=ImmutableList.copyOf(endingActions);
}"
5631,"public BatchPipelineSpec build(){
  return new BatchPipelineSpec(stages,connections,resources,driverResources == null ? resources : driverResources,stageLoggingEnabled,endingActions,numOfRecordsPreview);
}","public BatchPipelineSpec build(){
  return new BatchPipelineSpec(stages,connections,resources,driverResources,clientResources,stageLoggingEnabled,endingActions,numOfRecordsPreview);
}"
5632,"@Override public int hashCode(){
  return Objects.hash(super.hashCode(),endingActions,driverResources);
}","@Override public int hashCode(){
  return Objects.hash(super.hashCode(),endingActions);
}"
5633,"@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  if (!super.equals(o)) {
    return false;
  }
  BatchPipelineSpec that=(BatchPipelineSpec)o;
  return Objects.equals(endingActions,that.endingActions) && Objects.equals(driverResources,that.driverResources);
}","@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  if (!super.equals(o)) {
    return false;
  }
  BatchPipelineSpec that=(BatchPipelineSpec)o;
  return Objects.equals(endingActions,that.endingActions);
}"
5634,"@Override public BatchPipelineSpec generateSpec(ETLBatchConfig config){
  BatchPipelineSpec.Builder specBuilder=BatchPipelineSpec.builder().setDriverResources(config.getDriverResources());
  for (  ETLStage endingAction : config.getPostActions()) {
    String name=endingAction.getName();
    DefaultPipelineConfigurer pipelineConfigurer=new DefaultPipelineConfigurer(configurer,name);
    PluginSpec pluginSpec=configurePlugin(endingAction.getName(),endingAction.getPlugin(),pipelineConfigurer);
    specBuilder.addAction(new ActionSpec(name,pluginSpec));
  }
  configureStages(config,specBuilder);
  return specBuilder.build();
}","@Override public BatchPipelineSpec generateSpec(ETLBatchConfig config){
  BatchPipelineSpec.Builder specBuilder=BatchPipelineSpec.builder();
  for (  ETLStage endingAction : config.getPostActions()) {
    String name=endingAction.getName();
    DefaultPipelineConfigurer pipelineConfigurer=new DefaultPipelineConfigurer(configurer,name);
    PluginSpec pluginSpec=configurePlugin(endingAction.getName(),endingAction.getPlugin(),pipelineConfigurer);
    specBuilder.addAction(new ActionSpec(name,pluginSpec));
  }
  configureStages(config,specBuilder);
  return specBuilder.build();
}"
5635,"public PipelineSpec build(){
  return new PipelineSpec(stages,connections,resources,stageLoggingEnabled,numOfRecordsPreview);
}","public PipelineSpec build(){
  return new PipelineSpec(stages,connections,resources,driverResources,clientResources,stageLoggingEnabled,numOfRecordsPreview);
}"
5636,"protected PipelineSpec(Set<StageSpec> stages,Set<Connection> connections,Resources resources,boolean stageLoggingEnabled,int numOfRecordsPreview){
  this.stages=ImmutableSet.copyOf(stages);
  this.connections=ImmutableSet.copyOf(connections);
  this.resources=resources;
  this.stageLoggingEnabled=stageLoggingEnabled;
  this.numOfRecordsPreview=numOfRecordsPreview;
}","protected PipelineSpec(Set<StageSpec> stages,Set<Connection> connections,Resources resources,Resources driverResources,Resources clientResources,boolean stageLoggingEnabled,int numOfRecordsPreview){
  this.stages=ImmutableSet.copyOf(stages);
  this.connections=ImmutableSet.copyOf(connections);
  this.resources=resources;
  this.driverResources=driverResources;
  this.clientResources=clientResources;
  this.stageLoggingEnabled=stageLoggingEnabled;
  this.numOfRecordsPreview=numOfRecordsPreview;
}"
5637,"@Override public int hashCode(){
  return Objects.hash(stages,connections,resources,stageLoggingEnabled);
}","@Override public int hashCode(){
  return Objects.hash(stages,connections,resources,driverResources,clientResources,stageLoggingEnabled);
}"
5638,"@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  PipelineSpec that=(PipelineSpec)o;
  return Objects.equals(stages,that.stages) && Objects.equals(connections,that.connections) && Objects.equals(resources,that.resources)&& Objects.equals(stageLoggingEnabled,that.stageLoggingEnabled);
}","@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  PipelineSpec that=(PipelineSpec)o;
  return Objects.equals(stages,that.stages) && Objects.equals(connections,that.connections) && Objects.equals(resources,that.resources)&& Objects.equals(driverResources,that.driverResources)&& Objects.equals(clientResources,that.clientResources)&& Objects.equals(stageLoggingEnabled,that.stageLoggingEnabled);
}"
5639,"@Override public String toString(){
  return ""String_Node_Str"" + ""String_Node_Str"" + stages + ""String_Node_Str""+ connections+ ""String_Node_Str""+ resources+ ""String_Node_Str""+ stageLoggingEnabled+ '}';
}","@Override public String toString(){
  return ""String_Node_Str"" + ""String_Node_Str"" + stages + ""String_Node_Str""+ connections+ ""String_Node_Str""+ resources+ ""String_Node_Str""+ driverResources+ ""String_Node_Str""+ clientResources+ ""String_Node_Str""+ stageLoggingEnabled+ ""String_Node_Str""+ numOfRecordsPreview+ '}';
}"
5640,"/** 
 * Performs most of the validation and configuration needed by a pipeline. Handles stages, connections, resources, and stage logging settings.
 * @param config user provided ETL config
 * @param specBuilder builder for creating a pipeline spec.
 */
protected void configureStages(ETLConfig config,PipelineSpec.Builder specBuilder){
  List<StageConnections> traversalOrder=validateConfig(config);
  Map<String,DefaultPipelineConfigurer> pluginConfigurers=new HashMap<>(traversalOrder.size());
  Map<String,String> pluginTypes=new HashMap<>(traversalOrder.size());
  for (  StageConnections stageConnections : traversalOrder) {
    String stageName=stageConnections.getStage().getName();
    pluginTypes.put(stageName,stageConnections.getStage().getPlugin().getType());
    pluginConfigurers.put(stageName,new DefaultPipelineConfigurer(configurer,stageName));
  }
  for (  StageConnections stageConnections : traversalOrder) {
    ETLStage stage=stageConnections.getStage();
    String stageName=stage.getName();
    DefaultPipelineConfigurer pluginConfigurer=pluginConfigurers.get(stageName);
    StageSpec stageSpec=configureStage(stageConnections,pluginConfigurer);
    Schema outputSchema=stageSpec.getOutputSchema();
    for (    String outputStageName : stageConnections.getOutputs()) {
      String outputStageType=pluginTypes.get(outputStageName);
      if (Action.PLUGIN_TYPE.equals(outputStageType)) {
        continue;
      }
      DefaultStageConfigurer outputStageConfigurer=pluginConfigurers.get(outputStageName).getStageConfigurer();
      if (BatchJoiner.PLUGIN_TYPE.equals(outputStageType) && outputSchema == null) {
        throw new IllegalArgumentException(String.format(""String_Node_Str"" + ""String_Node_Str"",stageName));
      }
      if (!outputStageType.equals(BatchJoiner.PLUGIN_TYPE) && !hasSameSchema(outputStageConfigurer.getInputSchemas(),outputSchema)) {
        throw new IllegalArgumentException(""String_Node_Str"" + outputStageName);
      }
      outputStageConfigurer.addInputSchema(stageName,outputSchema);
    }
    specBuilder.addStage(stageSpec);
  }
  specBuilder.addConnections(config.getConnections()).setResources(config.getResources()).setStageLoggingEnabled(config.isStageLoggingEnabled()).setNumOfRecordsPreview(config.getNumOfRecordsPreview());
}","/** 
 * Performs most of the validation and configuration needed by a pipeline. Handles stages, connections, resources, and stage logging settings.
 * @param config user provided ETL config
 * @param specBuilder builder for creating a pipeline spec.
 */
protected void configureStages(ETLConfig config,PipelineSpec.Builder specBuilder){
  List<StageConnections> traversalOrder=validateConfig(config);
  Map<String,DefaultPipelineConfigurer> pluginConfigurers=new HashMap<>(traversalOrder.size());
  Map<String,String> pluginTypes=new HashMap<>(traversalOrder.size());
  for (  StageConnections stageConnections : traversalOrder) {
    String stageName=stageConnections.getStage().getName();
    pluginTypes.put(stageName,stageConnections.getStage().getPlugin().getType());
    pluginConfigurers.put(stageName,new DefaultPipelineConfigurer(configurer,stageName));
  }
  for (  StageConnections stageConnections : traversalOrder) {
    ETLStage stage=stageConnections.getStage();
    String stageName=stage.getName();
    DefaultPipelineConfigurer pluginConfigurer=pluginConfigurers.get(stageName);
    StageSpec stageSpec=configureStage(stageConnections,pluginConfigurer);
    Schema outputSchema=stageSpec.getOutputSchema();
    for (    String outputStageName : stageConnections.getOutputs()) {
      String outputStageType=pluginTypes.get(outputStageName);
      if (Action.PLUGIN_TYPE.equals(outputStageType)) {
        continue;
      }
      DefaultStageConfigurer outputStageConfigurer=pluginConfigurers.get(outputStageName).getStageConfigurer();
      if (BatchJoiner.PLUGIN_TYPE.equals(outputStageType) && outputSchema == null) {
        throw new IllegalArgumentException(String.format(""String_Node_Str"" + ""String_Node_Str"",stageName));
      }
      if (!outputStageType.equals(BatchJoiner.PLUGIN_TYPE) && !hasSameSchema(outputStageConfigurer.getInputSchemas(),outputSchema)) {
        throw new IllegalArgumentException(""String_Node_Str"" + outputStageName);
      }
      outputStageConfigurer.addInputSchema(stageName,outputSchema);
    }
    specBuilder.addStage(stageSpec);
  }
  specBuilder.addConnections(config.getConnections()).setResources(config.getResources()).setDriverResources(config.getDriverResources()).setClientResources(config.getClientResources()).setStageLoggingEnabled(config.isStageLoggingEnabled()).setNumOfRecordsPreview(config.getNumOfRecordsPreview());
}"
5641,"@Test public void testGenerateSpec(){
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MOCK_SOURCE)).addStage(new ETLStage(""String_Node_Str"",MOCK_SINK)).addStage(new ETLStage(""String_Node_Str"",MOCK_SINK)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_A)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_A)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_B)).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").build();
  BatchPipelineSpec actual=specGenerator.generateSpec(etlConfig);
  Map<String,String> emptyMap=ImmutableMap.of();
  PipelineSpec expected=BatchPipelineSpec.builder().addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSource.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).setOutputSchema(SCHEMA_A).addOutputs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSink.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_B).addInputs(""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSink.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchemas(ImmutableMap.of(""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A)).addInputs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_A).setOutputSchema(SCHEMA_A).addInputs(""String_Node_Str"").addOutputs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchemas(ImmutableMap.of(""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A)).setOutputSchema(SCHEMA_A).addInputs(""String_Node_Str"",""String_Node_Str"").addOutputs(""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchemas(ImmutableMap.of(""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A)).setOutputSchema(SCHEMA_B).addInputs(""String_Node_Str"",""String_Node_Str"").addOutputs(""String_Node_Str"").build()).addConnections(etlConfig.getConnections()).setResources(etlConfig.getResources()).setStageLoggingEnabled(etlConfig.isStageLoggingEnabled()).build();
  Assert.assertEquals(expected,actual);
}","@Test public void testGenerateSpec(){
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MOCK_SOURCE)).addStage(new ETLStage(""String_Node_Str"",MOCK_SINK)).addStage(new ETLStage(""String_Node_Str"",MOCK_SINK)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_A)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_A)).addStage(new ETLStage(""String_Node_Str"",MOCK_TRANSFORM_B)).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").build();
  BatchPipelineSpec actual=specGenerator.generateSpec(etlConfig);
  Map<String,String> emptyMap=ImmutableMap.of();
  PipelineSpec expected=BatchPipelineSpec.builder().addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSource.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).setOutputSchema(SCHEMA_A).addOutputs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSink.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_B).addInputs(""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(BatchSink.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchemas(ImmutableMap.of(""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A)).addInputs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchema(""String_Node_Str"",SCHEMA_A).setOutputSchema(SCHEMA_A).addInputs(""String_Node_Str"").addOutputs(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchemas(ImmutableMap.of(""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A)).setOutputSchema(SCHEMA_A).addInputs(""String_Node_Str"",""String_Node_Str"").addOutputs(""String_Node_Str"",""String_Node_Str"").build()).addStage(StageSpec.builder(""String_Node_Str"",new PluginSpec(Transform.PLUGIN_TYPE,""String_Node_Str"",emptyMap,ARTIFACT_ID)).addInputSchemas(ImmutableMap.of(""String_Node_Str"",SCHEMA_A,""String_Node_Str"",SCHEMA_A)).setOutputSchema(SCHEMA_B).addInputs(""String_Node_Str"",""String_Node_Str"").addOutputs(""String_Node_Str"").build()).addConnections(etlConfig.getConnections()).setResources(etlConfig.getResources()).setDriverResources(new Resources(1024,1)).setClientResources(new Resources(1024,1)).setStageLoggingEnabled(etlConfig.isStageLoggingEnabled()).build();
  Assert.assertEquals(expected,actual);
}"
5642,"/** 
 * Collects the stats that are reported by this object.
 */
void collect() throws IOException ;","/** 
 * Collects the stats that are reported by this object.
 */
void collect() throws Exception ;"
5643,"@Inject OperationalStatsService(OperationalStatsLoader operationalStatsLoader,CConfiguration cConf){
  this.operationalStatsLoader=operationalStatsLoader;
  this.statsRefreshInterval=cConf.getInt(Constants.OperationalStats.REFRESH_INTERVAL_SECS);
}","@Inject OperationalStatsService(OperationalStatsLoader operationalStatsLoader,CConfiguration cConf,Injector injector){
  this.operationalStatsLoader=operationalStatsLoader;
  this.statsRefreshInterval=cConf.getInt(Constants.OperationalStats.REFRESH_INTERVAL_SECS);
  this.injector=injector;
}"
5644,"/** 
 * Registers all JMX   {@link MXBean MXBeans} from {@link OperationalStats} extensions in the extensions directory.
 */
@Override protected void startUp() throws Exception {
  MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    ObjectName objectName=getObjectName(entry.getValue());
    if (objectName == null) {
      LOG.warn(""String_Node_Str"",OperationalStats.class.getName());
      continue;
    }
    LOG.debug(""String_Node_Str"",entry.getValue(),entry.getKey());
    mbs.registerMBean(entry.getValue(),objectName);
  }
  LOG.info(""String_Node_Str"");
}","/** 
 * Registers all JMX   {@link MXBean MXBeans} from {@link OperationalStats} extensions in the extensions directory.
 */
@Override protected void startUp() throws Exception {
  MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    OperationalStats operationalStats=entry.getValue();
    ObjectName objectName=getObjectName(operationalStats);
    if (objectName == null) {
      LOG.warn(""String_Node_Str"",OperationalStats.class.getName());
      continue;
    }
    LOG.debug(""String_Node_Str"",operationalStats,entry.getKey());
    operationalStats.initialize(injector);
    mbs.registerMBean(operationalStats,objectName);
  }
  LOG.info(""String_Node_Str"");
}"
5645,"@Override protected void shutDown() throws Exception {
  MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    ObjectName objectName=getObjectName(entry.getValue());
    if (objectName == null) {
      LOG.warn(""String_Node_Str"" + ""String_Node_Str"",entry.getValue().getClass().getName());
      continue;
    }
    try {
      mbs.unregisterMBean(objectName);
    }
 catch (    InstanceNotFoundException e) {
      LOG.debug(""String_Node_Str"",objectName);
    }
catch (    MBeanRegistrationException e) {
      LOG.warn(""String_Node_Str"",e);
    }
  }
  if (executor != null) {
    executor.shutdownNow();
  }
  LOG.info(""String_Node_Str"");
}","@Override protected void shutDown() throws Exception {
  MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    OperationalStats operationalStats=entry.getValue();
    ObjectName objectName=getObjectName(operationalStats);
    if (objectName == null) {
      LOG.warn(""String_Node_Str"" + ""String_Node_Str"",operationalStats.getClass().getName());
      continue;
    }
    try {
      mbs.unregisterMBean(objectName);
    }
 catch (    InstanceNotFoundException e) {
      LOG.debug(""String_Node_Str"",objectName);
    }
catch (    MBeanRegistrationException e) {
      LOG.warn(""String_Node_Str"",e);
    }
    operationalStats.destroy();
  }
  if (executor != null) {
    executor.shutdownNow();
  }
  LOG.info(""String_Node_Str"");
}"
5646,"@Override public void initialize(WorkflowContext context) throws Exception {
  super.initialize(context);
  postActions=new LinkedHashMap<>();
  BatchPipelineSpec batchPipelineSpec=GSON.fromJson(context.getWorkflowSpecification().getProperty(Constants.PIPELINE_SPEC_KEY),BatchPipelineSpec.class);
  for (  ActionSpec actionSpec : batchPipelineSpec.getEndingActions()) {
    postActions.put(actionSpec.getName(),(PostAction)context.newPluginInstance(actionSpec.getName()));
  }
}","@Override public void initialize(WorkflowContext context) throws Exception {
  super.initialize(context);
  postActions=new LinkedHashMap<>();
  BatchPipelineSpec batchPipelineSpec=GSON.fromJson(context.getWorkflowSpecification().getProperty(Constants.PIPELINE_SPEC_KEY),BatchPipelineSpec.class);
  MacroEvaluator macroEvaluator=new DefaultMacroEvaluator(context.getToken(),context.getRuntimeArguments(),context.getLogicalStartTime(),context,context.getNamespace());
  for (  ActionSpec actionSpec : batchPipelineSpec.getEndingActions()) {
    postActions.put(actionSpec.getName(),(PostAction)context.newPluginInstance(actionSpec.getName(),macroEvaluator));
  }
}"
5647,"@Inject TokenSecureStoreUpdater(YarnConfiguration hConf,CConfiguration cConf,LocationFactory locationFactory,co.cask.cdap.api.security.store.SecureStore secureStore){
  this.hConf=hConf;
  this.locationFactory=locationFactory;
  this.secureStore=secureStore;
  secureExplore=cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED) && UserGroupInformation.isSecurityEnabled();
  updateInterval=calculateUpdateInterval();
}","@Inject TokenSecureStoreUpdater(YarnConfiguration hConf,CConfiguration cConf,LocationFactory locationFactory,co.cask.cdap.api.security.store.SecureStore secureStore){
  this.hConf=hConf;
  this.locationFactory=locationFactory;
  this.secureStore=secureStore;
  secureExplore=cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED) && UserGroupInformation.isSecurityEnabled();
}"
5648,"/** 
 * Returns the minimum update interval for the delegation tokens.
 * @return The update interval in milliseconds.
 */
public long getUpdateInterval(){
  return updateInterval;
}","/** 
 * Returns the minimum update interval for the delegation tokens.
 * @return The update interval in milliseconds.
 */
public long getUpdateInterval(){
  if (updateInterval == null) {
    updateInterval=calculateUpdateInterval();
  }
  return updateInterval;
}"
5649,"private long calculateUpdateInterval(){
  List<Long> renewalTimes=Lists.newArrayList();
  renewalTimes.add(hConf.getLong(DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_RENEW_INTERVAL_KEY,DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT));
  renewalTimes.add(hConf.getLong(Constants.HBase.AUTH_KEY_UPDATE_INTERVAL,TimeUnit.MILLISECONDS.convert(1,TimeUnit.DAYS)));
  if (secureExplore) {
    renewalTimes.add(hConf.getLong(YarnConfiguration.DELEGATION_TOKEN_RENEW_INTERVAL_KEY,YarnConfiguration.DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT));
    Configuration hiveConf=getHiveConf();
    if (hiveConf != null) {
      renewalTimes.add(hiveConf.getLong(HadoopThriftAuthBridge.Server.DELEGATION_TOKEN_RENEW_INTERVAL_KEY,HadoopThriftAuthBridge.Server.DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT));
    }
 else {
      renewalTimes.add(HadoopThriftAuthBridge.Server.DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT);
    }
    renewalTimes.add(hConf.getLong(MRConfig.DELEGATION_TOKEN_RENEW_INTERVAL_KEY,MRConfig.DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT));
  }
  Long minimumInterval=Collections.min(renewalTimes);
  long delay=minimumInterval - TimeUnit.MINUTES.toMillis(5);
  if (delay <= 0) {
    delay=(minimumInterval <= 2) ? 1 : minimumInterval / 2;
  }
  LOG.info(""String_Node_Str"",delay);
  return delay;
}","private long calculateUpdateInterval(){
  List<Long> renewalTimes=Lists.newArrayList();
  renewalTimes.add(hConf.getLong(DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_RENEW_INTERVAL_KEY,DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT));
  renewalTimes.add(hConf.getLong(Constants.HBase.AUTH_KEY_UPDATE_INTERVAL,TimeUnit.MILLISECONDS.convert(1,TimeUnit.DAYS)));
  if (secureExplore) {
    renewalTimes.add(hConf.getLong(YarnConfiguration.DELEGATION_TOKEN_RENEW_INTERVAL_KEY,YarnConfiguration.DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT));
    Configuration hiveConf=getHiveConf();
    if (hiveConf != null) {
      renewalTimes.add(hiveConf.getLong(HadoopThriftAuthBridge.Server.DELEGATION_TOKEN_RENEW_INTERVAL_KEY,HadoopThriftAuthBridge.Server.DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT));
    }
 else {
      renewalTimes.add(HadoopThriftAuthBridge.Server.DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT);
    }
    renewalTimes.add(hConf.getLong(MRConfig.DELEGATION_TOKEN_RENEW_INTERVAL_KEY,MRConfig.DELEGATION_TOKEN_RENEW_INTERVAL_DEFAULT));
  }
  Long minimumInterval=Collections.min(renewalTimes);
  long delay=minimumInterval - TimeUnit.HOURS.toMillis(1);
  if (delay <= 0) {
    delay=(minimumInterval <= 2) ? 1 : minimumInterval / 2;
  }
  LOG.info(""String_Node_Str"",delay);
  return delay;
}"
5650,"@Override public void initialize(WorkflowContext context) throws Exception {
  super.initialize(context);
  postActions=new LinkedHashMap<>();
  BatchPipelineSpec batchPipelineSpec=GSON.fromJson(context.getWorkflowSpecification().getProperty(Constants.PIPELINE_SPEC_KEY),BatchPipelineSpec.class);
  for (  ActionSpec actionSpec : batchPipelineSpec.getEndingActions()) {
    postActions.put(actionSpec.getName(),(PostAction)context.newPluginInstance(actionSpec.getName()));
  }
}","@Override public void initialize(WorkflowContext context) throws Exception {
  super.initialize(context);
  postActions=new LinkedHashMap<>();
  BatchPipelineSpec batchPipelineSpec=GSON.fromJson(context.getWorkflowSpecification().getProperty(Constants.PIPELINE_SPEC_KEY),BatchPipelineSpec.class);
  MacroEvaluator macroEvaluator=new DefaultMacroEvaluator(context.getToken(),context.getRuntimeArguments(),context.getLogicalStartTime(),context,context.getNamespace());
  for (  ActionSpec actionSpec : batchPipelineSpec.getEndingActions()) {
    postActions.put(actionSpec.getName(),(PostAction)context.newPluginInstance(actionSpec.getName(),macroEvaluator));
  }
}"
5651,"@Override protected <T extends Dataset>T getDataset(String namespace,String name,Map<String,String> arguments,AccessType accessType) throws DatasetInstantiationException {
  T dataset=super.getDataset(namespace,name,arguments,accessType);
  startDatasetTransaction(dataset);
  return dataset;
}","@Override protected <T extends Dataset>T getDataset(String namespace,String name,Map<String,String> arguments,AccessType accessType) throws DatasetInstantiationException {
  T dataset=super.getDataset(namespace,name,adjustRuntimeArguments(arguments),accessType);
  startDatasetTransaction(dataset);
  return dataset;
}"
5652,"@Override protected BufferingTable getTable(DatasetContext datasetContext,String name,DatasetProperties props) throws Exception {
  DatasetSpecification spec=TABLE_DEFINITION.configure(name,props);
  return new HBaseTable(datasetContext,spec,cConf,TEST_HBASE.getConfiguration(),hBaseTableUtil);
}","@Override protected BufferingTable getTable(DatasetContext datasetContext,String name,DatasetProperties props,Map<String,String> args) throws Exception {
  DatasetSpecification spec=TABLE_DEFINITION.configure(name,props);
  return new HBaseTable(datasetContext,spec,args,cConf,TEST_HBASE.getConfiguration(),hBaseTableUtil);
}"
5653,"@Test public void testTTL() throws Exception {
  int ttl=1;
  String ttlTable=""String_Node_Str"";
  String noTtlTable=""String_Node_Str"";
  DatasetProperties props=DatasetProperties.builder().add(Table.PROPERTY_TTL,String.valueOf(ttl)).build();
  getTableAdmin(CONTEXT1,ttlTable,props).create();
  DatasetSpecification ttlTableSpec=DatasetSpecification.builder(ttlTable,HBaseTable.class.getName()).properties(props.getProperties()).build();
  HBaseTable table=new HBaseTable(CONTEXT1,ttlTableSpec,cConf,TEST_HBASE.getConfiguration(),hBaseTableUtil);
  DetachedTxSystemClient txSystemClient=new DetachedTxSystemClient();
  Transaction tx=txSystemClient.startShort();
  table.startTx(tx);
  table.put(b(""String_Node_Str""),b(""String_Node_Str""),b(""String_Node_Str""));
  table.commitTx();
  TimeUnit.MILLISECONDS.sleep(1010);
  tx=txSystemClient.startShort();
  table.startTx(tx);
  table.put(b(""String_Node_Str""),b(""String_Node_Str""),b(""String_Node_Str""));
  table.commitTx();
  tx=txSystemClient.startShort();
  table.startTx(tx);
  byte[] val=table.get(b(""String_Node_Str""),b(""String_Node_Str""));
  if (val != null) {
    LOG.info(""String_Node_Str"" + Bytes.toStringBinary(val));
  }
  Assert.assertNull(val);
  Assert.assertArrayEquals(b(""String_Node_Str""),table.get(b(""String_Node_Str""),b(""String_Node_Str"")));
  DatasetProperties props2=DatasetProperties.builder().add(Table.PROPERTY_TTL,String.valueOf(Tables.NO_TTL)).build();
  getTableAdmin(CONTEXT1,noTtlTable,props2).create();
  DatasetSpecification noTtlTableSpec=DatasetSpecification.builder(noTtlTable,HBaseTable.class.getName()).properties(props2.getProperties()).build();
  HBaseTable table2=new HBaseTable(CONTEXT1,noTtlTableSpec,cConf,TEST_HBASE.getConfiguration(),hBaseTableUtil);
  tx=txSystemClient.startShort();
  table2.startTx(tx);
  table2.put(b(""String_Node_Str""),b(""String_Node_Str""),b(""String_Node_Str""));
  table2.commitTx();
  TimeUnit.SECONDS.sleep(2);
  tx=txSystemClient.startShort();
  table2.startTx(tx);
  table2.put(b(""String_Node_Str""),b(""String_Node_Str""),b(""String_Node_Str""));
  table2.commitTx();
  tx=txSystemClient.startShort();
  table2.startTx(tx);
  Assert.assertArrayEquals(b(""String_Node_Str""),table2.get(b(""String_Node_Str""),b(""String_Node_Str"")));
  Assert.assertArrayEquals(b(""String_Node_Str""),table2.get(b(""String_Node_Str""),b(""String_Node_Str"")));
}","@Test public void testTTL() throws Exception {
  int ttl=1;
  String ttlTable=""String_Node_Str"";
  String noTtlTable=""String_Node_Str"";
  DatasetProperties props=DatasetProperties.builder().add(Table.PROPERTY_TTL,String.valueOf(ttl)).build();
  getTableAdmin(CONTEXT1,ttlTable,props).create();
  DatasetSpecification ttlTableSpec=DatasetSpecification.builder(ttlTable,HBaseTable.class.getName()).properties(props.getProperties()).build();
  HBaseTable table=new HBaseTable(CONTEXT1,ttlTableSpec,Collections.<String,String>emptyMap(),cConf,TEST_HBASE.getConfiguration(),hBaseTableUtil);
  DetachedTxSystemClient txSystemClient=new DetachedTxSystemClient();
  Transaction tx=txSystemClient.startShort();
  table.startTx(tx);
  table.put(b(""String_Node_Str""),b(""String_Node_Str""),b(""String_Node_Str""));
  table.commitTx();
  TimeUnit.MILLISECONDS.sleep(1010);
  tx=txSystemClient.startShort();
  table.startTx(tx);
  table.put(b(""String_Node_Str""),b(""String_Node_Str""),b(""String_Node_Str""));
  table.commitTx();
  tx=txSystemClient.startShort();
  table.startTx(tx);
  byte[] val=table.get(b(""String_Node_Str""),b(""String_Node_Str""));
  if (val != null) {
    LOG.info(""String_Node_Str"" + Bytes.toStringBinary(val));
  }
  Assert.assertNull(val);
  Assert.assertArrayEquals(b(""String_Node_Str""),table.get(b(""String_Node_Str""),b(""String_Node_Str"")));
  DatasetProperties props2=DatasetProperties.builder().add(Table.PROPERTY_TTL,String.valueOf(Tables.NO_TTL)).build();
  getTableAdmin(CONTEXT1,noTtlTable,props2).create();
  DatasetSpecification noTtlTableSpec=DatasetSpecification.builder(noTtlTable,HBaseTable.class.getName()).properties(props2.getProperties()).build();
  HBaseTable table2=new HBaseTable(CONTEXT1,noTtlTableSpec,Collections.<String,String>emptyMap(),cConf,TEST_HBASE.getConfiguration(),hBaseTableUtil);
  tx=txSystemClient.startShort();
  table2.startTx(tx);
  table2.put(b(""String_Node_Str""),b(""String_Node_Str""),b(""String_Node_Str""));
  table2.commitTx();
  TimeUnit.SECONDS.sleep(2);
  tx=txSystemClient.startShort();
  table2.startTx(tx);
  table2.put(b(""String_Node_Str""),b(""String_Node_Str""),b(""String_Node_Str""));
  table2.commitTx();
  tx=txSystemClient.startShort();
  table2.startTx(tx);
  Assert.assertArrayEquals(b(""String_Node_Str""),table2.get(b(""String_Node_Str""),b(""String_Node_Str"")));
  Assert.assertArrayEquals(b(""String_Node_Str""),table2.get(b(""String_Node_Str""),b(""String_Node_Str"")));
}"
5654,"public HBaseTable(DatasetContext datasetContext,DatasetSpecification spec,CConfiguration cConf,Configuration hConf,HBaseTableUtil tableUtil) throws IOException {
  super(PrefixedNamespaces.namespace(cConf,datasetContext.getNamespaceId(),spec.getName()),TableProperties.supportsReadlessIncrements(spec.getProperties()),spec.getProperties());
  TableId hBaseTableId=tableUtil.createHTableId(new NamespaceId(datasetContext.getNamespaceId()),spec.getName());
  HTable hTable=tableUtil.createHTable(hConf,hBaseTableId);
  hTable.setWriteBufferSize(HBaseTableUtil.DEFAULT_WRITE_BUFFER_SIZE);
  hTable.setAutoFlush(false);
  this.tableUtil=tableUtil;
  this.hTable=hTable;
  this.hTableName=Bytes.toStringBinary(hTable.getTableName());
  this.columnFamily=TableProperties.getColumnFamily(spec.getProperties());
  this.txCodec=new TransactionCodec();
  this.nameAsTxChangePrefix=Bytes.add(new byte[]{(byte)this.hTableName.length()},Bytes.toBytes(this.hTableName));
}","public HBaseTable(DatasetContext datasetContext,DatasetSpecification spec,Map<String,String> args,CConfiguration cConf,Configuration hConf,HBaseTableUtil tableUtil) throws IOException {
  super(PrefixedNamespaces.namespace(cConf,datasetContext.getNamespaceId(),spec.getName()),TableProperties.supportsReadlessIncrements(spec.getProperties()),spec.getProperties());
  TableId hBaseTableId=tableUtil.createHTableId(new NamespaceId(datasetContext.getNamespaceId()),spec.getName());
  HTable hTable=tableUtil.createHTable(hConf,hBaseTableId);
  hTable.setWriteBufferSize(HBaseTableUtil.DEFAULT_WRITE_BUFFER_SIZE);
  hTable.setAutoFlush(false);
  this.tableUtil=tableUtil;
  this.hTable=hTable;
  this.hTableName=Bytes.toStringBinary(hTable.getTableName());
  this.columnFamily=TableProperties.getColumnFamily(spec.getProperties());
  this.txCodec=new TransactionCodec();
  this.nameAsTxChangePrefix=Bytes.add(new byte[]{(byte)this.hTableName.length()},Bytes.toBytes(this.hTableName));
  this.safeReadlessIncrements=args.containsKey(SAFE_INCREMENTS) && Boolean.valueOf(args.get(SAFE_INCREMENTS));
}"
5655,"@Override protected void persist(NavigableMap<byte[],NavigableMap<byte[],Update>> updates) throws Exception {
  if (updates.isEmpty()) {
    return;
  }
  List<Put> puts=Lists.newArrayList();
  for (  Map.Entry<byte[],NavigableMap<byte[],Update>> row : updates.entrySet()) {
    PutBuilder put=tableUtil.buildPut(row.getKey());
    Put incrementPut=null;
    for (    Map.Entry<byte[],Update> column : row.getValue().entrySet()) {
      if (tx != null) {
        Update val=column.getValue();
        if (val instanceof IncrementValue) {
          incrementPut=getIncrementalPut(incrementPut,row.getKey());
          incrementPut.add(columnFamily,column.getKey(),tx.getWritePointer(),Bytes.toBytes(((IncrementValue)val).getValue()));
        }
 else         if (val instanceof PutValue) {
          put.add(columnFamily,column.getKey(),tx.getWritePointer(),wrapDeleteIfNeeded(((PutValue)val).getValue()));
        }
      }
 else {
        Update val=column.getValue();
        if (val instanceof IncrementValue) {
          incrementPut=getIncrementalPut(incrementPut,row.getKey());
          incrementPut.add(columnFamily,column.getKey(),Bytes.toBytes(((IncrementValue)val).getValue()));
        }
 else         if (val instanceof PutValue) {
          put.add(columnFamily,column.getKey(),((PutValue)val).getValue());
        }
      }
    }
    if (incrementPut != null) {
      puts.add(incrementPut);
    }
    if (!put.isEmpty()) {
      puts.add(put.build());
    }
  }
  if (!puts.isEmpty()) {
    hbasePut(puts);
  }
 else {
    LOG.info(""String_Node_Str"");
  }
}","@Override protected void persist(NavigableMap<byte[],NavigableMap<byte[],Update>> updates) throws Exception {
  if (updates.isEmpty()) {
    return;
  }
  List<Mutation> mutations=new ArrayList<>();
  for (  Map.Entry<byte[],NavigableMap<byte[],Update>> row : updates.entrySet()) {
    PutBuilder put=null;
    PutBuilder incrementPut=null;
    IncrementBuilder increment=null;
    for (    Map.Entry<byte[],Update> column : row.getValue().entrySet()) {
      if (tx != null) {
        Update val=column.getValue();
        if (val instanceof IncrementValue) {
          if (safeReadlessIncrements) {
            increment=getIncrement(increment,row.getKey(),true);
            increment.add(columnFamily,column.getKey(),tx.getWritePointer(),((IncrementValue)val).getValue());
          }
 else {
            incrementPut=getPutForIncrement(incrementPut,row.getKey());
            incrementPut.add(columnFamily,column.getKey(),tx.getWritePointer(),Bytes.toBytes(((IncrementValue)val).getValue()));
          }
        }
 else         if (val instanceof PutValue) {
          put=getPut(put,row.getKey());
          put.add(columnFamily,column.getKey(),tx.getWritePointer(),wrapDeleteIfNeeded(((PutValue)val).getValue()));
        }
      }
 else {
        Update val=column.getValue();
        if (val instanceof IncrementValue) {
          incrementPut=getPutForIncrement(incrementPut,row.getKey());
          incrementPut.add(columnFamily,column.getKey(),Bytes.toBytes(((IncrementValue)val).getValue()));
        }
 else         if (val instanceof PutValue) {
          put=getPut(put,row.getKey());
          put.add(columnFamily,column.getKey(),((PutValue)val).getValue());
        }
      }
    }
    if (incrementPut != null) {
      mutations.add(incrementPut.build());
    }
    if (increment != null) {
      mutations.add(increment.build());
    }
    if (put != null) {
      mutations.add(put.build());
    }
  }
  if (!hbaseFlush(mutations)) {
    LOG.info(""String_Node_Str"");
  }
}"
5656,"@Override public Table getDataset(DatasetContext datasetContext,DatasetSpecification spec,Map<String,String> arguments,ClassLoader classLoader) throws IOException {
  return new HBaseTable(datasetContext,spec,cConf,hConf,hBaseTableUtil);
}","@Override public Table getDataset(DatasetContext datasetContext,DatasetSpecification spec,Map<String,String> arguments,ClassLoader classLoader) throws IOException {
  return new HBaseTable(datasetContext,spec,arguments,cConf,hConf,hBaseTableUtil);
}"
5657,"protected SessionHandle doOpenHiveSession(Map<String,String> sessionConf) throws HiveSQLException {
  try {
    return cliService.openSession(UserGroupInformation.getCurrentUser().getShortUserName(),""String_Node_Str"",sessionConf);
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}","protected SessionHandle doOpenHiveSession(Map<String,String> sessionConf) throws HiveSQLException {
  return cliService.openSession(""String_Node_Str"",""String_Node_Str"",sessionConf);
}"
5658,"/** 
 * Registers all JMX   {@link MXBean MXBeans} from {@link OperationalStats} extensions in the extensions directory.
 */
@Override protected void startUp() throws Exception {
  MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    ObjectName objectName=getObjectName(entry.getValue());
    if (objectName == null) {
      LOG.warn(""String_Node_Str"",OperationalStats.class.getName());
      continue;
    }
    LOG.debug(""String_Node_Str"",entry.getValue());
    mbs.registerMBean(entry.getValue(),objectName);
  }
  LOG.info(""String_Node_Str"");
}","/** 
 * Registers all JMX   {@link MXBean MXBeans} from {@link OperationalStats} extensions in the extensions directory.
 */
@Override protected void startUp() throws Exception {
  MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    ObjectName objectName=getObjectName(entry.getValue());
    if (objectName == null) {
      LOG.warn(""String_Node_Str"",OperationalStats.class.getName());
      continue;
    }
    LOG.debug(""String_Node_Str"",entry.getValue(),entry.getKey());
    mbs.registerMBean(entry.getValue(),objectName);
  }
  LOG.info(""String_Node_Str"");
}"
5659,"@Override protected ProgramController launch(Program program,ProgramOptions options,Map<String,LocalizeResource> localizeResources,File tempDir,ApplicationLauncher launcher){
  ApplicationSpecification appSpec=program.getApplicationSpecification();
  Preconditions.checkNotNull(appSpec,""String_Node_Str"");
  ProgramType processorType=program.getType();
  Preconditions.checkNotNull(processorType,""String_Node_Str"");
  Preconditions.checkArgument(processorType == ProgramType.WORKFLOW,""String_Node_Str"");
  WorkflowSpecification workflowSpec=appSpec.getWorkflows().get(program.getName());
  Preconditions.checkNotNull(workflowSpec,""String_Node_Str"",program.getName());
  List<String> extraClassPaths=new ArrayList<>();
  List<Class<?>> extraDependencies=new ArrayList<>();
  extraDependencies.add(YarnClientProtocolProvider.class);
  DriverMeta driverMeta=findDriverResources(program.getApplicationSpecification().getSpark(),program.getApplicationSpecification().getMapReduce(),workflowSpec);
  ProgramRuntimeProvider provider=runtimeProviderLoader.get(ProgramType.SPARK);
  if (provider != null) {
    try {
      String sparkAssemblyJarName=SparkUtils.prepareSparkResources(tempDir,localizeResources);
      extraClassPaths.add(sparkAssemblyJarName);
      extraDependencies.add(provider.getClass());
    }
 catch (    Exception e) {
      if (driverMeta.hasSpark) {
        throw e;
      }
      LOG.debug(""String_Node_Str"",program.getId(),e);
    }
  }
 else   if (driverMeta.hasSpark) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  extraClassPaths.addAll(MapReduceContainerHelper.localizeFramework(hConf,localizeResources));
  LOG.info(""String_Node_Str"" + program.getName() + ""String_Node_Str""+ workflowSpec.getName());
  TwillController controller=launcher.launch(new WorkflowTwillApplication(program,options.getUserArguments(),workflowSpec,localizeResources,eventHandler,driverMeta.resources),extraClassPaths,extraDependencies);
  return createProgramController(controller,program.getId(),ProgramRunners.getRunId(options));
}","@Override protected ProgramController launch(Program program,ProgramOptions options,Map<String,LocalizeResource> localizeResources,File tempDir,ApplicationLauncher launcher){
  ApplicationSpecification appSpec=program.getApplicationSpecification();
  Preconditions.checkNotNull(appSpec,""String_Node_Str"");
  ProgramType processorType=program.getType();
  Preconditions.checkNotNull(processorType,""String_Node_Str"");
  Preconditions.checkArgument(processorType == ProgramType.WORKFLOW,""String_Node_Str"");
  WorkflowSpecification workflowSpec=appSpec.getWorkflows().get(program.getName());
  Preconditions.checkNotNull(workflowSpec,""String_Node_Str"",program.getName());
  List<String> extraClassPaths=new ArrayList<>();
  List<Class<?>> extraDependencies=new ArrayList<>();
  extraDependencies.add(YarnClientProtocolProvider.class);
  if (SecureStoreUtils.isKMSBacked(cConf) && SecureStoreUtils.isKMSCapable()) {
    extraDependencies.add(SecureStoreUtils.getKMSSecureStore());
  }
  DriverMeta driverMeta=findDriverResources(program.getApplicationSpecification().getSpark(),program.getApplicationSpecification().getMapReduce(),workflowSpec);
  ProgramRuntimeProvider provider=runtimeProviderLoader.get(ProgramType.SPARK);
  if (provider != null) {
    try {
      String sparkAssemblyJarName=SparkUtils.prepareSparkResources(tempDir,localizeResources);
      extraClassPaths.add(sparkAssemblyJarName);
      extraDependencies.add(provider.getClass());
    }
 catch (    Exception e) {
      if (driverMeta.hasSpark) {
        throw e;
      }
      LOG.debug(""String_Node_Str"",program.getId(),e);
    }
  }
 else   if (driverMeta.hasSpark) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  extraClassPaths.addAll(MapReduceContainerHelper.localizeFramework(hConf,localizeResources));
  LOG.info(""String_Node_Str"" + program.getName() + ""String_Node_Str""+ workflowSpec.getName());
  TwillController controller=launcher.launch(new WorkflowTwillApplication(program,options.getUserArguments(),workflowSpec,localizeResources,eventHandler,driverMeta.resources),extraClassPaths,extraDependencies);
  return createProgramController(controller,program.getId(),ProgramRunners.getRunId(options));
}"
5660,"/** 
 * Private constructor, only to be used by static method setOriginalProperties.
 * @param name the name of the dataset
 * @param type the type of the dataset
 * @param description the description of dataset
 * @param properties the custom properties
 * @param datasetSpecs the specs of embedded datasets
 */
private DatasetSpecification(String name,String type,@Nullable String description,@Nullable Map<String,String> originalProperties,SortedMap<String,String> properties,SortedMap<String,DatasetSpecification> datasetSpecs){
  this.name=name;
  this.type=type;
  this.description=description;
  this.properties=Collections.unmodifiableSortedMap(new TreeMap<>(properties));
  this.originalProperties=originalProperties == null ? null : Collections.unmodifiableMap(new TreeMap<String,String>(originalProperties));
  this.datasetSpecs=Collections.unmodifiableSortedMap(new TreeMap<>(datasetSpecs));
}","/** 
 * Private constructor, only to be used by static method setOriginalProperties.
 * @param name the name of the dataset
 * @param type the type of the dataset
 * @param description the description of dataset
 * @param properties the custom properties
 * @param datasetSpecs the specs of embedded datasets
 */
private DatasetSpecification(String name,String type,@Nullable String description,@Nullable Map<String,String> originalProperties,SortedMap<String,String> properties,SortedMap<String,DatasetSpecification> datasetSpecs){
  this.name=name;
  this.type=type;
  this.description=description;
  this.properties=Collections.unmodifiableSortedMap(new TreeMap<>(properties));
  this.originalProperties=originalProperties == null ? null : Collections.unmodifiableMap(new TreeMap<>(originalProperties));
  this.datasetSpecs=Collections.unmodifiableSortedMap(new TreeMap<>(datasetSpecs));
}"
5661,"/** 
 * Lookup a custom property of the dataset.
 * @param key the name of the property
 * @param defaultValue the value to return if property does not exist
 * @return the value of the property or defaultValue if the property does not exist
 */
public long getLongProperty(String key,long defaultValue){
  return properties.containsKey(key) ? Long.parseLong(getProperty(key)) : defaultValue;
}","/** 
 * Lookup a custom property of the dataset.
 * @param key the name of the property
 * @param defaultValue the value to return if property does not exist
 * @return the value of the property or defaultValue if the property does not exist
 */
public long getLongProperty(String key,long defaultValue){
  return properties.containsKey(key) ? Long.parseLong(properties.get(key)) : defaultValue;
}"
5662,"/** 
 * Lookup a custom property of the dataset.
 * @param key the name of the property
 * @param defaultValue the value to return if property does not exist
 * @return the value of the property or defaultValue if the property does not exist
 */
public int getIntProperty(String key,int defaultValue){
  return properties.containsKey(key) ? Integer.parseInt(getProperty(key)) : defaultValue;
}","/** 
 * Lookup a custom property of the dataset.
 * @param key the name of the property
 * @param defaultValue the value to return if property does not exist
 * @return the value of the property or defaultValue if the property does not exist
 */
public int getIntProperty(String key,int defaultValue){
  return properties.containsKey(key) ? Integer.parseInt(properties.get(key)) : defaultValue;
}"
5663,"@Override public void process(ApplicationWithPrograms input) throws Exception {
  ApplicationId appId=input.getApplicationId();
  ApplicationSpecification appSpec=input.getSpecification();
  SystemMetadataWriter appSystemMetadataWriter=new AppSystemMetadataWriter(metadataStore,appId,appSpec);
  appSystemMetadataWriter.write();
  writeProgramSystemMetadata(appId,ProgramType.FLOW,appSpec.getFlows().values());
  writeProgramSystemMetadata(appId,ProgramType.MAPREDUCE,appSpec.getMapReduce().values());
  writeProgramSystemMetadata(appId,ProgramType.SERVICE,appSpec.getServices().values());
  writeProgramSystemMetadata(appId,ProgramType.SPARK,appSpec.getSpark().values());
  writeProgramSystemMetadata(appId,ProgramType.WORKER,appSpec.getWorkers().values());
  writeProgramSystemMetadata(appId,ProgramType.WORKFLOW,appSpec.getWorkflows().values());
  emit(input);
}","@Override public void process(ApplicationWithPrograms input) throws Exception {
  ApplicationId appId=input.getApplicationId();
  ApplicationSpecification appSpec=input.getSpecification();
  Map<String,String> properties=metadataStore.getProperties(MetadataScope.SYSTEM,appId);
  SystemMetadataWriter appSystemMetadataWriter=new AppSystemMetadataWriter(metadataStore,appId,appSpec,!properties.isEmpty());
  appSystemMetadataWriter.write();
  writeProgramSystemMetadata(appId,ProgramType.FLOW,appSpec.getFlows().values());
  writeProgramSystemMetadata(appId,ProgramType.MAPREDUCE,appSpec.getMapReduce().values());
  writeProgramSystemMetadata(appId,ProgramType.SERVICE,appSpec.getServices().values());
  writeProgramSystemMetadata(appId,ProgramType.SPARK,appSpec.getSpark().values());
  writeProgramSystemMetadata(appId,ProgramType.WORKER,appSpec.getWorkers().values());
  writeProgramSystemMetadata(appId,ProgramType.WORKFLOW,appSpec.getWorkflows().values());
  emit(input);
}"
5664,"private void writeProgramSystemMetadata(ApplicationId appId,ProgramType programType,Iterable<? extends ProgramSpecification> specs){
  for (  ProgramSpecification spec : specs) {
    ProgramId programId=appId.program(programType,spec.getName());
    ProgramSystemMetadataWriter writer=new ProgramSystemMetadataWriter(metadataStore,programId,spec);
    writer.write();
  }
}","private void writeProgramSystemMetadata(ApplicationId appId,ProgramType programType,Iterable<? extends ProgramSpecification> specs){
  for (  ProgramSpecification spec : specs) {
    ProgramId programId=appId.program(programType,spec.getName());
    Map<String,String> properties=metadataStore.getProperties(MetadataScope.SYSTEM,programId);
    ProgramSystemMetadataWriter writer=new ProgramSystemMetadataWriter(metadataStore,programId,spec,!properties.isEmpty());
    writer.write();
  }
}"
5665,"/** 
 * Filter a list of   {@link MetadataSearchResultRecord} that ensures the logged-in user has a privilege on
 * @param results the {@link Set<MetadataSearchResultRecord>} to filter with
 * @return filtered list of {@link MetadataSearchResultRecord}
 */
private Set<MetadataSearchResultRecord> filterAuthorizedSearchResult(Set<MetadataSearchResultRecord> results) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter=authorizationEnforcer.createFilter(principal);
  return ImmutableSet.copyOf(Iterables.filter(results,new com.google.common.base.Predicate<MetadataSearchResultRecord>(){
    @Override public boolean apply(    MetadataSearchResultRecord metadataSearchResultRecord){
      return filter.apply(metadataSearchResultRecord.getEntityId());
    }
  }
));
}","/** 
 * Filter a list of   {@link MetadataSearchResultRecord} that ensures the logged-in user has a privilege on
 * @param results the {@link MetadataSearchResponse} to filter
 * @return filtered {@link MetadataSearchResponse}
 */
private MetadataSearchResponse filterAuthorizedSearchResult(MetadataSearchResponse results) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter=authorizationEnforcer.createFilter(principal);
  return new MetadataSearchResponse(results.getSort(),results.getOffset(),results.getLimit(),results.getNumCursors(),results.getTotal(),ImmutableSet.copyOf(Iterables.filter(results.getResults(),new com.google.common.base.Predicate<MetadataSearchResultRecord>(){
    @Override public boolean apply(    MetadataSearchResultRecord metadataSearchResultRecord){
      return filter.apply(metadataSearchResultRecord.getEntityId());
    }
  }
)),results.getCursors());
}"
5666,"/** 
 * Filter a list of   {@link MetadataSearchResultRecord} that ensures the logged-in user has a privilege on
 * @param results the {@link MetadataSearchResponse} to filter
 * @return filtered {@link MetadataSearchResponse}
 */
private MetadataSearchResponse filterAuthorizedSearchResult(MetadataSearchResponse results) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter=authorizationEnforcer.createFilter(principal);
  return new MetadataSearchResponse(results.getSort(),results.getOffset(),results.getLimit(),results.getTotal(),ImmutableSet.copyOf(Iterables.filter(results.getResults(),new com.google.common.base.Predicate<MetadataSearchResultRecord>(){
    @Override public boolean apply(    MetadataSearchResultRecord metadataSearchResultRecord){
      return filter.apply(metadataSearchResultRecord.getEntityId());
    }
  }
)),results.getCursors());
}","/** 
 * Filter a list of   {@link MetadataSearchResultRecord} that ensures the logged-in user has a privilege on
 * @param results the {@link MetadataSearchResponse} to filter
 * @return filtered {@link MetadataSearchResponse}
 */
private MetadataSearchResponse filterAuthorizedSearchResult(MetadataSearchResponse results) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter=authorizationEnforcer.createFilter(principal);
  return new MetadataSearchResponse(results.getSort(),results.getOffset(),results.getLimit(),results.getNumCursors(),results.getTotal(),ImmutableSet.copyOf(Iterables.filter(results.getResults(),new com.google.common.base.Predicate<MetadataSearchResultRecord>(){
    @Override public boolean apply(    MetadataSearchResultRecord metadataSearchResultRecord){
      return filter.apply(metadataSearchResultRecord.getEntityId());
    }
  }
)),results.getCursors());
}"
5667,"/** 
 * Executes a search for CDAP entities in the specified namespace with the specified search query and an optional set of   {@link MetadataSearchTargetType entity types} in the specified {@link MetadataScope}.
 * @param namespaceId the namespace id to filter the search by
 * @param searchQuery the search query
 * @param types the types of CDAP entity to be searched. If empty all possible types will be searched
 * @param sortInfo represents sorting information. Use {@link SortInfo#DEFAULT} to return search results withoutsorting (which implies that the sort order is by relevance to the search query)
 * @param offset the index to start with in the search results. To return results from the beginning, pass {@code 0}
 * @param limit the number of results to return, starting from #offset. To return all, pass {@link Integer#MAX_VALUE}
 * @param numCursors the number of cursors to return in the response. A cursor identifies the first index of thenext page for pagination purposes
 * @param cursor the cursor that acts as the starting index for the requested page. This is only applicable when#sortInfo is not  {@link SortInfo#DEFAULT}. If offset is also specified, it is applied starting at the cursor. If   {@code null}, the first row is used as the cursor
 * @return the {@link MetadataSearchResponse} containing search results for the specified search query and filters
 */
MetadataSearchResponse search(String namespaceId,String searchQuery,Set<MetadataSearchTargetType> types,SortInfo sortInfo,int offset,int limit,int numCursors,String cursor) throws Exception ;","/** 
 * Executes a search for CDAP entities in the specified namespace with the specified search query and an optional set of   {@link MetadataSearchTargetType entity types} in the specified {@link MetadataScope}.
 * @param namespaceId the namespace id to filter the search by
 * @param searchQuery the search query
 * @param types the types of CDAP entity to be searched. If empty all possible types will be searched
 * @param sortInfo represents sorting information. Use {@link SortInfo#DEFAULT} to return search results withoutsorting (which implies that the sort order is by relevance to the search query)
 * @param offset the index to start with in the search results. To return results from the beginning, pass {@code 0}
 * @param limit the number of results to return, starting from #offset. To return all, pass {@link Integer#MAX_VALUE}
 * @param numCursors the number of cursors to return in the response. A cursor identifies the first index of thenext page for pagination purposes. Defaults to  {@code 0}
 * @param cursor the cursor that acts as the starting index for the requested page. This is only applicable when#sortInfo is not  {@link SortInfo#DEFAULT}. If offset is also specified, it is applied starting at the cursor. If   {@code null}, the first row is used as the cursor
 * @return the {@link MetadataSearchResponse} containing search results for the specified search query and filters
 */
MetadataSearchResponse search(String namespaceId,String searchQuery,Set<MetadataSearchTargetType> types,SortInfo sortInfo,int offset,int limit,int numCursors,String cursor) throws Exception ;"
5668,"@GET @Path(""String_Node_Str"") public void searchMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String searchQuery,@QueryParam(""String_Node_Str"") List<String> targets,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String sort,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int offset,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int limit,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numCursors,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String cursor) throws Exception {
  Set<MetadataSearchTargetType> types=Collections.emptySet();
  if (targets != null) {
    types=ImmutableSet.copyOf(Iterables.transform(targets,STRING_TO_TARGET_TYPE));
  }
  SortInfo sortInfo=SortInfo.of(URLDecoder.decode(sort,""String_Node_Str""));
  if (SortInfo.DEFAULT.equals(sortInfo) && !(cursor.isEmpty())) {
    throw new BadRequestException(""String_Node_Str"");
  }
  MetadataSearchResponse response=metadataAdmin.search(namespaceId,URLDecoder.decode(searchQuery,""String_Node_Str""),types,sortInfo,offset,limit,numCursors,cursor);
  responder.sendJson(HttpResponseStatus.OK,response,MetadataSearchResponse.class,GSON);
}","@GET @Path(""String_Node_Str"") public void searchMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String searchQuery,@QueryParam(""String_Node_Str"") List<String> targets,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String sort,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int offset,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int limit,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int numCursors,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String cursor) throws Exception {
  Set<MetadataSearchTargetType> types=Collections.emptySet();
  if (targets != null) {
    types=ImmutableSet.copyOf(Iterables.transform(targets,STRING_TO_TARGET_TYPE));
  }
  SortInfo sortInfo=SortInfo.of(URLDecoder.decode(sort,""String_Node_Str""));
  if (SortInfo.DEFAULT.equals(sortInfo)) {
    if (!(cursor.isEmpty()) || 0 != numCursors) {
      throw new BadRequestException(""String_Node_Str"");
    }
  }
  MetadataSearchResponse response=metadataAdmin.search(namespaceId,URLDecoder.decode(searchQuery,""String_Node_Str""),types,sortInfo,offset,limit,numCursors,cursor);
  responder.sendJson(HttpResponseStatus.OK,response,MetadataSearchResponse.class,GSON);
}"
5669,"@Test public void testSearch() throws Exception {
  SecurityRequestContext.setUserId(ALICE.getName());
  authorizer.grant(NamespaceId.DEFAULT,ALICE,Collections.singleton(Action.WRITE));
  AppFabricTestHelper.deployApplication(Id.Namespace.DEFAULT,AllProgramsApp.class,""String_Node_Str"",cConf);
  EnumSet<MetadataSearchTargetType> types=EnumSet.allOf(MetadataSearchTargetType.class);
  Assert.assertFalse(metadataAdmin.search(NamespaceId.DEFAULT.getNamespace(),""String_Node_Str"",types,SortInfo.DEFAULT,0,Integer.MAX_VALUE,1,null).getResults().isEmpty());
  SecurityRequestContext.setUserId(""String_Node_Str"");
  Assert.assertTrue(metadataAdmin.search(NamespaceId.DEFAULT.getNamespace(),""String_Node_Str"",types,SortInfo.DEFAULT,0,Integer.MAX_VALUE,1,null).getResults().isEmpty());
}","@Test public void testSearch() throws Exception {
  SecurityRequestContext.setUserId(ALICE.getName());
  authorizer.grant(NamespaceId.DEFAULT,ALICE,Collections.singleton(Action.WRITE));
  AppFabricTestHelper.deployApplication(Id.Namespace.DEFAULT,AllProgramsApp.class,""String_Node_Str"",cConf);
  EnumSet<MetadataSearchTargetType> types=EnumSet.allOf(MetadataSearchTargetType.class);
  Assert.assertFalse(metadataAdmin.search(NamespaceId.DEFAULT.getNamespace(),""String_Node_Str"",types,SortInfo.DEFAULT,0,Integer.MAX_VALUE,0,null).getResults().isEmpty());
  SecurityRequestContext.setUserId(""String_Node_Str"");
  Assert.assertTrue(metadataAdmin.search(NamespaceId.DEFAULT.getNamespace(),""String_Node_Str"",types,SortInfo.DEFAULT,0,Integer.MAX_VALUE,0,null).getResults().isEmpty());
}"
5670,"/** 
 * strips metadata from search results
 */
@Override protected Set<MetadataSearchResultRecord> searchMetadata(NamespaceId namespaceId,String query,Set<MetadataSearchTargetType> targets,@Nullable String sort) throws Exception {
  Set<MetadataSearchResultRecord> results=super.searchMetadata(namespaceId,query,targets,sort);
  Set<MetadataSearchResultRecord> transformed=new LinkedHashSet<>();
  for (  MetadataSearchResultRecord result : results) {
    transformed.add(new MetadataSearchResultRecord(result.getEntityId()));
  }
  return transformed;
}","/** 
 * strips metadata from search results
 */
@Override protected MetadataSearchResponse searchMetadata(NamespaceId namespaceId,String query,Set<MetadataSearchTargetType> targets,@Nullable String sort,int offset,int limit,int numCursors,@Nullable String cursor) throws Exception {
  MetadataSearchResponse searchResponse=super.searchMetadata(namespaceId,query,targets,sort,offset,limit,numCursors,cursor);
  Set<MetadataSearchResultRecord> transformed=new LinkedHashSet<>();
  for (  MetadataSearchResultRecord result : searchResponse.getResults()) {
    transformed.add(new MetadataSearchResultRecord(result.getEntityId()));
  }
  return new MetadataSearchResponse(searchResponse.getSort(),searchResponse.getOffset(),searchResponse.getLimit(),searchResponse.getNumCursors(),searchResponse.getTotal(),transformed,searchResponse.getCursors());
}"
5671,"@Test public void testSearchResultSorting() throws Exception {
  NamespaceId namespace=new NamespaceId(""String_Node_Str"");
  namespaceClient.create(new NamespaceMeta.Builder().setName(namespace).build());
  StreamId stream=namespace.stream(""String_Node_Str"");
  DatasetId dataset=namespace.dataset(""String_Node_Str"");
  StreamViewId view=stream.view(""String_Node_Str"");
  streamClient.create(stream);
  TimeUnit.MILLISECONDS.sleep(1);
  streamViewClient.createOrUpdate(view,new ViewSpecification(new FormatSpecification(""String_Node_Str"",null,null)));
  TimeUnit.MILLISECONDS.sleep(1);
  datasetClient.create(dataset,new DatasetInstanceConfiguration(Table.class.getName(),Collections.<String,String>emptyMap()));
  EnumSet<MetadataSearchTargetType> targets=EnumSet.allOf(MetadataSearchTargetType.class);
  try {
    searchMetadata(namespace,""String_Node_Str"",targets,AbstractSystemMetadataWriter.ENTITY_NAME_KEY);
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    searchMetadata(namespace,""String_Node_Str"",targets,""String_Node_Str"");
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    searchMetadata(namespace,""String_Node_Str"",targets,AbstractSystemMetadataWriter.ENTITY_NAME_KEY + ""String_Node_Str"");
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    searchMetadata(NamespaceId.DEFAULT,""String_Node_Str"",EnumSet.allOf(MetadataSearchTargetType.class),null,1,""String_Node_Str"");
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  Set<MetadataSearchResultRecord> searchResults=searchMetadata(namespace,""String_Node_Str"",targets,AbstractSystemMetadataWriter.ENTITY_NAME_KEY + ""String_Node_Str"");
  List<MetadataSearchResultRecord> expected=ImmutableList.of(new MetadataSearchResultRecord(dataset),new MetadataSearchResultRecord(stream),new MetadataSearchResultRecord(view));
  Assert.assertEquals(expected,new ArrayList<>(searchResults));
  searchResults=searchMetadata(namespace,""String_Node_Str"",targets,AbstractSystemMetadataWriter.ENTITY_NAME_KEY + ""String_Node_Str"");
  expected=ImmutableList.of(new MetadataSearchResultRecord(view),new MetadataSearchResultRecord(stream),new MetadataSearchResultRecord(dataset));
  Assert.assertEquals(expected,new ArrayList<>(searchResults));
  searchResults=searchMetadata(namespace,""String_Node_Str"",targets,AbstractSystemMetadataWriter.CREATION_TIME_KEY + ""String_Node_Str"");
  expected=ImmutableList.of(new MetadataSearchResultRecord(stream),new MetadataSearchResultRecord(view),new MetadataSearchResultRecord(dataset));
  Assert.assertEquals(expected,new ArrayList<>(searchResults));
  searchResults=searchMetadata(namespace,""String_Node_Str"",targets,AbstractSystemMetadataWriter.CREATION_TIME_KEY + ""String_Node_Str"");
  expected=ImmutableList.of(new MetadataSearchResultRecord(dataset),new MetadataSearchResultRecord(view),new MetadataSearchResultRecord(stream));
  Assert.assertEquals(expected,new ArrayList<>(searchResults));
  namespaceClient.delete(namespace);
}","@Test public void testSearchResultSorting() throws Exception {
  NamespaceId namespace=new NamespaceId(""String_Node_Str"");
  namespaceClient.create(new NamespaceMeta.Builder().setName(namespace).build());
  StreamId stream=namespace.stream(""String_Node_Str"");
  DatasetId dataset=namespace.dataset(""String_Node_Str"");
  StreamViewId view=stream.view(""String_Node_Str"");
  streamClient.create(stream);
  TimeUnit.MILLISECONDS.sleep(1);
  streamViewClient.createOrUpdate(view,new ViewSpecification(new FormatSpecification(""String_Node_Str"",null,null)));
  TimeUnit.MILLISECONDS.sleep(1);
  datasetClient.create(dataset,new DatasetInstanceConfiguration(Table.class.getName(),Collections.<String,String>emptyMap()));
  EnumSet<MetadataSearchTargetType> targets=EnumSet.allOf(MetadataSearchTargetType.class);
  Set<MetadataSearchResultRecord> searchResults=searchMetadata(namespace,""String_Node_Str"",targets,AbstractSystemMetadataWriter.ENTITY_NAME_KEY + ""String_Node_Str"");
  List<MetadataSearchResultRecord> expected=ImmutableList.of(new MetadataSearchResultRecord(dataset),new MetadataSearchResultRecord(stream),new MetadataSearchResultRecord(view));
  Assert.assertEquals(expected,new ArrayList<>(searchResults));
  searchResults=searchMetadata(namespace,""String_Node_Str"",targets,AbstractSystemMetadataWriter.ENTITY_NAME_KEY + ""String_Node_Str"");
  expected=ImmutableList.of(new MetadataSearchResultRecord(view),new MetadataSearchResultRecord(stream),new MetadataSearchResultRecord(dataset));
  Assert.assertEquals(expected,new ArrayList<>(searchResults));
  searchResults=searchMetadata(namespace,""String_Node_Str"",targets,AbstractSystemMetadataWriter.CREATION_TIME_KEY + ""String_Node_Str"");
  expected=ImmutableList.of(new MetadataSearchResultRecord(stream),new MetadataSearchResultRecord(view),new MetadataSearchResultRecord(dataset));
  Assert.assertEquals(expected,new ArrayList<>(searchResults));
  searchResults=searchMetadata(namespace,""String_Node_Str"",targets,AbstractSystemMetadataWriter.CREATION_TIME_KEY + ""String_Node_Str"");
  expected=ImmutableList.of(new MetadataSearchResultRecord(dataset),new MetadataSearchResultRecord(view),new MetadataSearchResultRecord(stream));
  Assert.assertEquals(expected,new ArrayList<>(searchResults));
  namespaceClient.delete(namespace);
}"
5672,"protected Set<MetadataSearchResultRecord> searchMetadata(NamespaceId namespaceId,String query,Set<MetadataSearchTargetType> targets,@Nullable String sort,int numCursors,@Nullable String cursor) throws Exception {
  return metadataClient.searchMetadata(namespaceId.toId(),query,targets,sort,cursor).getResults();
}","protected MetadataSearchResponse searchMetadata(NamespaceId namespaceId,String query,Set<MetadataSearchTargetType> targets,@Nullable String sort,int offset,int limit,int numCursors,@Nullable String cursor) throws Exception {
  return metadataClient.searchMetadata(namespaceId.toId(),query,targets,sort,offset,limit,numCursors,cursor);
}"
5673,"/** 
 * Searches entities in the specified namespace whose metadata matches the specified query.
 * @param namespace the namespace to search in
 * @param query the query string with which to search
 * @param targets {@link MetadataSearchTargetType}s to search. If empty, all possible types will be searched
 * @param sort specifies sort field and sort order. If {@code null}, the sort order is by relevance
 * @return A set of {@link MetadataSearchResultRecord} for the given query.
 */
public MetadataSearchResponse searchMetadata(Id.Namespace namespace,String query,Set<MetadataSearchTargetType> targets,@Nullable String sort,@Nullable String cursor) throws IOException, UnauthenticatedException, UnauthorizedException, BadRequestException {
  String path=String.format(""String_Node_Str"",query);
  for (  MetadataSearchTargetType t : targets) {
    path+=""String_Node_Str"" + t;
  }
  if (sort != null) {
    path+=""String_Node_Str"" + URLEncoder.encode(sort,""String_Node_Str"");
  }
  if (cursor != null) {
    path+=""String_Node_Str"" + cursor;
  }
  URL searchURL=resolve(namespace,path);
  HttpResponse response=execute(HttpRequest.get(searchURL).build(),HttpResponseStatus.BAD_REQUEST.getCode());
  if (HttpResponseStatus.BAD_REQUEST.getCode() == response.getResponseCode()) {
    throw new BadRequestException(response.getResponseBodyAsString());
  }
  return GSON.fromJson(response.getResponseBodyAsString(),MetadataSearchResponse.class);
}","/** 
 * Searches entities in the specified namespace whose metadata matches the specified query.
 * @param namespace the namespace to search in
 * @param query the query string with which to search
 * @param targets {@link MetadataSearchTargetType}s to search. If empty, all possible types will be searched
 * @param sort specifies sort field and sort order. If {@code null}, the sort order is by relevance
 * @param offset the index to start with in the search results. To return results from the beginning, pass {@code 0}
 * @param limit the number of results to return, starting from #offset. To return all, pass {@link Integer#MAX_VALUE}
 * @param numCursors the number of cursors to return in the response. A cursor identifies the first index of thenext page for pagination purposes
 * @param cursor the cursor that acts as the starting index for the requested page. This is only applicable when#sortInfo is not default. If offset is also specified, it is applied starting at the cursor. If  {@code null}, the first row is used as the cursor
 * @return A set of {@link MetadataSearchResultRecord} for the given query.
 */
public MetadataSearchResponse searchMetadata(Id.Namespace namespace,String query,Set<MetadataSearchTargetType> targets,@Nullable String sort,int offset,int limit,int numCursors,@Nullable String cursor) throws IOException, UnauthenticatedException, UnauthorizedException, BadRequestException {
  String path=String.format(""String_Node_Str"",query);
  for (  MetadataSearchTargetType t : targets) {
    path+=""String_Node_Str"" + t;
  }
  if (sort != null) {
    path+=""String_Node_Str"" + URLEncoder.encode(sort,""String_Node_Str"");
  }
  path+=""String_Node_Str"" + offset;
  path+=""String_Node_Str"" + limit;
  path+=""String_Node_Str"" + numCursors;
  if (cursor != null) {
    path+=""String_Node_Str"" + cursor;
  }
  URL searchURL=resolve(namespace,path);
  HttpResponse response=execute(HttpRequest.get(searchURL).build(),HttpResponseStatus.BAD_REQUEST.getCode());
  if (HttpResponseStatus.BAD_REQUEST.getCode() == response.getResponseCode()) {
    throw new BadRequestException(response.getResponseBodyAsString());
  }
  return GSON.fromJson(response.getResponseBodyAsString(),MetadataSearchResponse.class);
}"
5674,"/** 
 * Also schedules asynchronous stats collection for all   {@link MXBean MXBeans} by calling the{@link OperationalStats#collect()} method.
 */
@Override protected void runOneIteration() throws Exception {
  LOG.debug(""String_Node_Str"");
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    LOG.debug(""String_Node_Str"",entry.getValue().getStatType(),entry.getValue().getServiceName());
    try {
      entry.getValue().collect();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",entry.getValue().getServiceName(),entry.getValue().getStatType(),e);
    }
  }
}","/** 
 * Also schedules asynchronous stats collection for all   {@link MXBean MXBeans} by calling the{@link OperationalStats#collect()} method.
 */
@Override protected void runOneIteration() throws Exception {
  LOG.debug(""String_Node_Str"");
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    LOG.debug(""String_Node_Str"",entry.getValue().getStatType(),entry.getValue().getServiceName());
    try {
      entry.getValue().collect();
    }
 catch (    Throwable t) {
      LOG.warn(""String_Node_Str"",entry.getValue().getServiceName(),entry.getValue().getStatType(),t);
    }
  }
}"
5675,"/** 
 * Registers all JMX   {@link MXBean MXBeans} from {@link OperationalStats} extensions in the extensions directory.
 */
@Override protected void startUp() throws Exception {
  LOG.info(""String_Node_Str"");
  MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    ObjectName objectName=getObjectName(entry.getValue());
    if (objectName == null) {
      LOG.warn(""String_Node_Str"",OperationalStats.class.getName());
      continue;
    }
    LOG.debug(""String_Node_Str"",entry.getValue());
    mbs.registerMBean(entry.getValue(),objectName);
  }
}","/** 
 * Registers all JMX   {@link MXBean MXBeans} from {@link OperationalStats} extensions in the extensions directory.
 */
@Override protected void startUp() throws Exception {
  MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();
  for (  Map.Entry<OperationalExtensionId,OperationalStats> entry : operationalStatsLoader.getAll().entrySet()) {
    ObjectName objectName=getObjectName(entry.getValue());
    if (objectName == null) {
      LOG.warn(""String_Node_Str"",OperationalStats.class.getName());
      continue;
    }
    LOG.debug(""String_Node_Str"",entry.getValue());
    mbs.registerMBean(entry.getValue(),objectName);
  }
  LOG.info(""String_Node_Str"");
}"
5676,"private URL getResourceManager() throws IOException {
  if (HAUtil.isHAEnabled(conf)) {
    return getHAWebURL();
  }
  return getNonHAWebURL();
}","private URL getResourceManager() throws IOException {
  if (conf.getBoolean(YarnConfiguration.RM_HA_ENABLED,YarnConfiguration.DEFAULT_RM_HA_ENABLED)) {
    return getHAWebURL();
  }
  return getNonHAWebURL();
}"
5677,"@Override public void run(){
  JobID jobId=profile.getJobID();
  JobContext jContext=new JobContextImpl(job,jobId);
  org.apache.hadoop.mapreduce.OutputCommitter outputCommitter=null;
  try {
    outputCommitter=createOutputCommitter(conf.getUseNewMapper(),jobId,conf);
  }
 catch (  Exception e) {
    LOG.info(""String_Node_Str"",e);
    return;
  }
  try {
    TaskSplitMetaInfo[] taskSplitMetaInfos=SplitMetaInfoReader.readSplitMetaInfo(jobId,localFs,conf,systemJobDir);
    int numReduceTasks=job.getNumReduceTasks();
    outputCommitter.setupJob(jContext);
    status.setSetupProgress(1.0f);
    Map<TaskAttemptID,MapOutputFile> mapOutputFiles=Collections.synchronizedMap(new HashMap<TaskAttemptID,MapOutputFile>());
    List<RunnableWithThrowable> mapRunnables=getMapTaskRunnables(taskSplitMetaInfos,jobId,mapOutputFiles);
    initCounters(mapRunnables.size(),numReduceTasks);
    ExecutorService mapService=createMapExecutor();
    runTasks(mapRunnables,mapService,""String_Node_Str"");
    try {
      if (numReduceTasks > 0) {
        List<RunnableWithThrowable> reduceRunnables=getReduceTaskRunnables(jobId,mapOutputFiles);
        ExecutorService reduceService=createReduceExecutor();
        runTasks(reduceRunnables,reduceService,""String_Node_Str"");
      }
    }
  finally {
      for (      MapOutputFile output : mapOutputFiles.values()) {
        output.removeAll();
      }
    }
    outputCommitter.commitJob(jContext);
    status.setCleanupProgress(1.0f);
    if (killed) {
      this.status.setRunState(JobStatus.KILLED);
    }
 else {
      this.status.setRunState(JobStatus.SUCCEEDED);
    }
    JobEndNotifier.localRunnerNotification(job,status);
  }
 catch (  Throwable t) {
    try {
      outputCommitter.abortJob(jContext,org.apache.hadoop.mapreduce.JobStatus.State.FAILED);
    }
 catch (    IOException ioe) {
      LOG.info(""String_Node_Str"",id);
    }
    status.setCleanupProgress(1.0f);
    if (killed) {
      this.status.setRunState(JobStatus.KILLED);
    }
 else {
      this.status.setRunState(JobStatus.FAILED);
    }
    LOG.warn(""String_Node_Str"",id,t);
    JobEndNotifier.localRunnerNotification(job,status);
  }
 finally {
    try {
      fs.delete(systemJobFile.getParent(),true);
      localFs.delete(localJobFile,true);
      localDistributedCacheManager.close();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",id,e);
    }
  }
}","@Override public void run(){
  JobID jobId=profile.getJobID();
  JobContext jContext=new JobContextImpl(job,jobId);
  org.apache.hadoop.mapreduce.OutputCommitter outputCommitter=null;
  try {
    outputCommitter=createOutputCommitter(conf.getUseNewMapper(),jobId,conf);
  }
 catch (  Exception e) {
    LOG.info(""String_Node_Str"",e);
    return;
  }
  try {
    TaskSplitMetaInfo[] taskSplitMetaInfos=SplitMetaInfoReader.readSplitMetaInfo(jobId,localFs,conf,systemJobDir);
    int numReduceTasks=job.getNumReduceTasks();
    outputCommitter.setupJob(jContext);
    status.setSetupProgress(1.0f);
    Map<TaskAttemptID,MapOutputFile> mapOutputFiles=Collections.synchronizedMap(new HashMap<TaskAttemptID,MapOutputFile>());
    List<RunnableWithThrowable> mapRunnables=getMapTaskRunnables(taskSplitMetaInfos,jobId,mapOutputFiles);
    initCounters(mapRunnables.size(),numReduceTasks);
    ExecutorService mapService=createMapExecutor();
    runTasks(mapRunnables,mapService,""String_Node_Str"");
    try {
      if (numReduceTasks > 0) {
        List<RunnableWithThrowable> reduceRunnables=getReduceTaskRunnables(jobId,mapOutputFiles);
        ExecutorService reduceService=createReduceExecutor();
        runTasks(reduceRunnables,reduceService,""String_Node_Str"");
      }
    }
  finally {
      for (      MapOutputFile output : mapOutputFiles.values()) {
        output.removeAll();
      }
    }
    outputCommitter.commitJob(jContext);
    status.setCleanupProgress(1.0f);
    if (killed) {
      this.status.setRunState(JobStatus.KILLED);
    }
 else {
      this.status.setRunState(JobStatus.SUCCEEDED);
    }
    JobEndNotifier.localRunnerNotification(job,status);
  }
 catch (  Throwable t) {
    try {
      outputCommitter.abortJob(jContext,org.apache.hadoop.mapreduce.JobStatus.State.FAILED);
    }
 catch (    IOException ioe) {
      LOG.info(""String_Node_Str"",id);
    }
    status.setCleanupProgress(1.0f);
    if (killed) {
      this.status.setRunState(JobStatus.KILLED);
      LOG.warn(""String_Node_Str"",id,t);
    }
 else {
      this.status.setRunState(JobStatus.FAILED);
      LOG.error(""String_Node_Str"",id,t);
    }
    JobEndNotifier.localRunnerNotification(job,status);
  }
 finally {
    try {
      fs.delete(systemJobFile.getParent(),true);
      localFs.delete(localJobFile,true);
      localDistributedCacheManager.close();
    }
 catch (    IOException e) {
      LOG.warn(""String_Node_Str"",id,e);
    }
  }
}"
5678,"@Override public int complete(String buffer,int cursor,List<CharSequence> candidates){
  Map<String,String> arguments=ArgumentParser.getArguments(buffer,PATTERN);
  ProgramIdArgument programIdArgument=ArgumentParser.parseProgramId(arguments.get(PROGRAM_ID));
  if (programIdArgument != null) {
    ServiceId service=cliConfig.getCurrentNamespace().app(programIdArgument.getAppId()).service(programIdArgument.getProgramId());
    completer.setEndpoints(getEndpoints(service,arguments.get(METHOD)));
  }
 else {
    completer.setEndpoints(Collections.<String>emptyList());
  }
  return super.complete(buffer,cursor,candidates);
}","@Override public int complete(String buffer,int cursor,List<CharSequence> candidates){
  Map<String,String> arguments=ArgumentParser.getArguments(buffer,PATTERN);
  ProgramIdArgument programIdArgument=ArgumentParser.parseProgramId(arguments.get(SERVICE_ID));
  if (programIdArgument != null) {
    ServiceId service;
    if (arguments.get(APP_VERSION) == null) {
      service=cliConfig.getCurrentNamespace().app(programIdArgument.getAppId()).service(programIdArgument.getProgramId());
    }
 else {
      service=cliConfig.getCurrentNamespace().app(programIdArgument.getAppId(),arguments.get(APP_VERSION)).service(programIdArgument.getProgramId());
    }
    completer.setEndpoints(getEndpoints(service,arguments.get(METHOD)));
  }
 else {
    completer.setEndpoints(Collections.<String>emptyList());
  }
  return super.complete(buffer,cursor,candidates);
}"
5679,"@Override public int complete(String buffer,int cursor,List<CharSequence> candidates){
  Map<String,String> arguments=ArgumentParser.getArguments(buffer,PATTERN);
  ProgramIdArgument programIdArgument=ArgumentParser.parseProgramId(arguments.get(PROGRAM_ID));
  if (programIdArgument != null) {
    ServiceId service=cliConfig.getCurrentNamespace().app(programIdArgument.getAppId()).service(programIdArgument.getProgramId());
    completer.setEndpoints(getMethods(service));
  }
 else {
    completer.setEndpoints(Collections.<String>emptyList());
  }
  return super.complete(buffer,cursor,candidates);
}","@Override public int complete(String buffer,int cursor,List<CharSequence> candidates){
  Map<String,String> arguments=ArgumentParser.getArguments(buffer,PATTERN);
  ProgramIdArgument programIdArgument=ArgumentParser.parseProgramId(arguments.get(SERVICE_ID));
  if (programIdArgument != null) {
    ServiceId service;
    if (arguments.get(APP_VERSION) == null) {
      service=cliConfig.getCurrentNamespace().app(programIdArgument.getAppId()).service(programIdArgument.getProgramId());
    }
 else {
      service=cliConfig.getCurrentNamespace().app(programIdArgument.getAppId(),arguments.get(APP_VERSION)).service(programIdArgument.getProgramId());
    }
    completer.setEndpoints(getMethods(service));
  }
 else {
    completer.setEndpoints(Collections.<String>emptyList());
  }
  return super.complete(buffer,cursor,candidates);
}"
5680,"@Override public Completer getCompleter(String prefix,Completer completer){
  if (prefix != null && !prefix.isEmpty()) {
    String prefixMatch=prefix.replaceAll(""String_Node_Str"",""String_Node_Str"");
    if (METHOD_PREFIX.equals(prefixMatch)) {
      return new HttpMethodPrefixCompleter(serviceClient,cliConfig,prefix,(EndpointCompleter)completer);
    }
 else     if (ENDPOINT_PREFIX.equals(prefixMatch)) {
      return new HttpEndpointPrefixCompleter(serviceClient,cliConfig,prefix,(EndpointCompleter)completer);
    }
  }
  return null;
}","@Override public Completer getCompleter(String prefix,Completer completer){
  if (prefix != null && !prefix.isEmpty()) {
    String prefixMatch=prefix.replaceAll(""String_Node_Str"",""String_Node_Str"");
    if ((METHOD_PREFIX.equals(prefixMatch) || METHOD_PREFIX_WITH_APP_VERSION.equals(prefixMatch)) && completer instanceof EndpointCompleter) {
      return new HttpMethodPrefixCompleter(serviceClient,cliConfig,prefix,(EndpointCompleter)completer);
    }
 else     if (ENDPOINT_PREFIX.equals(prefixMatch) || ENDPOINT_PREFIX_WITH_APP_VERSION.equals(prefixMatch)) {
      return new HttpEndpointPrefixCompleter(serviceClient,cliConfig,prefix,(EndpointCompleter)completer);
    }
  }
  return null;
}"
5681,"/** 
 * In-progress LONG transactions written with DefaultSnapshotCodec will not have the type serialized as part of the data.  Since these transactions also contain a non-negative expiration, we need to ensure we reset the type correctly when the snapshot is loaded.
 */
@Test public void testV2ToTephraV3Compatibility() throws Exception {
  long now=System.currentTimeMillis();
  long nowWritePointer=now * TxConstants.MAX_TX_PER_MS;
  long tInvalid=nowWritePointer - 5;
  long readPtr=nowWritePointer - 4;
  long tLong=nowWritePointer - 3;
  long tCommitted=nowWritePointer - 2;
  long tShort=nowWritePointer - 1;
  TreeMap<Long,TransactionManager.InProgressTx> inProgress=Maps.newTreeMap(ImmutableSortedMap.of(tLong,new TransactionManager.InProgressTx(readPtr,TransactionManager.getTxExpirationFromWritePointer(tLong,TxConstants.Manager.DEFAULT_TX_LONG_TIMEOUT),TransactionType.LONG),tShort,new TransactionManager.InProgressTx(readPtr,now + 1000,TransactionType.SHORT)));
  TransactionSnapshot snapshot=new TransactionSnapshot(now,readPtr,nowWritePointer,Lists.newArrayList(tInvalid),inProgress,ImmutableMap.<Long,Set<ChangeId>>of(tShort,Sets.newHashSet(new ChangeId(new byte[]{'r','3'}),new ChangeId(new byte[]{'r','4'}))),ImmutableMap.<Long,Set<ChangeId>>of(tCommitted,Sets.newHashSet(new ChangeId(new byte[]{'r','1'}),new ChangeId(new byte[]{'r','2'}))));
  Configuration conf1=new Configuration();
  conf1.set(TxConstants.Persist.CFG_TX_SNAPHOT_CODEC_CLASSES,SnapshotCodecV2.class.getName());
  SnapshotCodecProvider provider1=new SnapshotCodecProvider(conf1);
  ByteArrayOutputStream out=new ByteArrayOutputStream();
  try {
    provider1.encode(out,snapshot);
  }
  finally {
    out.close();
  }
  TransactionSnapshot snapshot2=provider1.decode(new ByteArrayInputStream(out.toByteArray()));
  assertEquals(snapshot.getReadPointer(),snapshot2.getReadPointer());
  assertEquals(snapshot.getWritePointer(),snapshot2.getWritePointer());
  assertEquals(snapshot.getInvalid(),snapshot2.getInvalid());
  assertNotEquals(snapshot.getInProgress(),snapshot2.getInProgress());
  assertEquals(snapshot.getCommittingChangeSets(),snapshot2.getCommittingChangeSets());
  assertEquals(snapshot.getCommittedChangeSets(),snapshot2.getCommittedChangeSets());
  Map<Long,TransactionManager.InProgressTx> fixedInProgress=TransactionManager.txnBackwardsCompatCheck(TxConstants.Manager.DEFAULT_TX_LONG_TIMEOUT,10000L,snapshot2.getInProgress());
  assertEquals(snapshot.getInProgress(),fixedInProgress);
  assertEquals(snapshot,snapshot2);
}","/** 
 * In-progress LONG transactions written with DefaultSnapshotCodec will not have the type serialized as part of the data.  Since these transactions also contain a non-negative expiration, we need to ensure we reset the type correctly when the snapshot is loaded.
 */
@Test public void testV2ToTephraV3Compatibility() throws Exception {
  long now=System.currentTimeMillis();
  long nowWritePointer=now * TxConstants.MAX_TX_PER_MS;
  long tInvalid=nowWritePointer - 5;
  long readPtr=nowWritePointer - 4;
  long tLong=nowWritePointer - 3;
  long tCommitted=nowWritePointer - 2;
  long tShort=nowWritePointer - 1;
  TreeMap<Long,TransactionManager.InProgressTx> inProgress=Maps.newTreeMap(ImmutableSortedMap.of(tLong,new TransactionManager.InProgressTx(readPtr,TransactionManager.getTxExpirationFromWritePointer(tLong,TxConstants.Manager.DEFAULT_TX_LONG_TIMEOUT),TransactionManager.InProgressType.LONG),tShort,new TransactionManager.InProgressTx(readPtr,now + 1000,TransactionManager.InProgressType.SHORT)));
  TransactionSnapshot snapshot=new TransactionSnapshot(now,readPtr,nowWritePointer,Lists.newArrayList(tInvalid),inProgress,ImmutableMap.<Long,Set<ChangeId>>of(tShort,Sets.newHashSet(new ChangeId(new byte[]{'r','3'}),new ChangeId(new byte[]{'r','4'}))),ImmutableMap.<Long,Set<ChangeId>>of(tCommitted,Sets.newHashSet(new ChangeId(new byte[]{'r','1'}),new ChangeId(new byte[]{'r','2'}))));
  Configuration conf1=new Configuration();
  conf1.set(TxConstants.Persist.CFG_TX_SNAPHOT_CODEC_CLASSES,SnapshotCodecV2.class.getName());
  SnapshotCodecProvider provider1=new SnapshotCodecProvider(conf1);
  ByteArrayOutputStream out=new ByteArrayOutputStream();
  try {
    provider1.encode(out,snapshot);
  }
  finally {
    out.close();
  }
  TransactionSnapshot snapshot2=provider1.decode(new ByteArrayInputStream(out.toByteArray()));
  assertEquals(snapshot.getReadPointer(),snapshot2.getReadPointer());
  assertEquals(snapshot.getWritePointer(),snapshot2.getWritePointer());
  assertEquals(snapshot.getInvalid(),snapshot2.getInvalid());
  assertNotEquals(snapshot.getInProgress(),snapshot2.getInProgress());
  assertEquals(snapshot.getCommittingChangeSets(),snapshot2.getCommittingChangeSets());
  assertEquals(snapshot.getCommittedChangeSets(),snapshot2.getCommittedChangeSets());
  Map<Long,TransactionManager.InProgressTx> fixedInProgress=TransactionManager.txnBackwardsCompatCheck(TxConstants.Manager.DEFAULT_TX_LONG_TIMEOUT,10000L,snapshot2.getInProgress());
  assertEquals(snapshot.getInProgress(),fixedInProgress);
  assertEquals(snapshot,snapshot2);
}"
5682,"@Override public void run(){
  LOG.info(""String_Node_Str"");
  try (final HConnection hbaseConnection=HConnectionManager.createConnection(hConf)){
    hbaseConnection.listTables();
    LOG.info(""String_Node_Str"");
  }
 catch (  IOException e) {
    throw new RuntimeException(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",e);
  }
}","@Override public void run(){
  LOG.info(""String_Node_Str"");
  try {
    new HTableNameConverterFactory().get();
  }
 catch (  ProvisionException e) {
    throw new RuntimeException(""String_Node_Str"" + HBaseVersion.getVersionString());
  }
  LOG.info(""String_Node_Str"");
  LOG.info(""String_Node_Str"");
  try (final HConnection hbaseConnection=HConnectionManager.createConnection(hConf)){
    hbaseConnection.listTables();
    LOG.info(""String_Node_Str"");
  }
 catch (  IOException e) {
    throw new RuntimeException(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",e);
  }
}"
5683,"/** 
 * In-progress LONG transactions written with DefaultSnapshotCodec will not have the type serialized as part of the data.  Since these transactions also contain a non-negative expiration, we need to ensure we reset the type correctly when the snapshot is loaded.
 */
@Test public void testV2ToTephraV3Compatibility() throws Exception {
  long now=System.currentTimeMillis();
  long nowWritePointer=now * TxConstants.MAX_TX_PER_MS;
  long tInvalid=nowWritePointer - 5;
  long readPtr=nowWritePointer - 4;
  long tLong=nowWritePointer - 3;
  long tCommitted=nowWritePointer - 2;
  long tShort=nowWritePointer - 1;
  TreeMap<Long,TransactionManager.InProgressTx> inProgress=Maps.newTreeMap(ImmutableSortedMap.of(tLong,new TransactionManager.InProgressTx(readPtr,TransactionManager.getTxExpirationFromWritePointer(tLong,TxConstants.Manager.DEFAULT_TX_LONG_TIMEOUT),TransactionType.LONG),tShort,new TransactionManager.InProgressTx(readPtr,now + 1000,TransactionType.SHORT)));
  TransactionSnapshot snapshot=new TransactionSnapshot(now,readPtr,nowWritePointer,Lists.newArrayList(tInvalid),inProgress,ImmutableMap.<Long,Set<ChangeId>>of(tShort,Sets.newHashSet(new ChangeId(new byte[]{'r','3'}),new ChangeId(new byte[]{'r','4'}))),ImmutableMap.<Long,Set<ChangeId>>of(tCommitted,Sets.newHashSet(new ChangeId(new byte[]{'r','1'}),new ChangeId(new byte[]{'r','2'}))));
  Configuration conf1=new Configuration();
  conf1.set(TxConstants.Persist.CFG_TX_SNAPHOT_CODEC_CLASSES,SnapshotCodecV2.class.getName());
  SnapshotCodecProvider provider1=new SnapshotCodecProvider(conf1);
  ByteArrayOutputStream out=new ByteArrayOutputStream();
  try {
    provider1.encode(out,snapshot);
  }
  finally {
    out.close();
  }
  TransactionSnapshot snapshot2=provider1.decode(new ByteArrayInputStream(out.toByteArray()));
  assertEquals(snapshot.getReadPointer(),snapshot2.getReadPointer());
  assertEquals(snapshot.getWritePointer(),snapshot2.getWritePointer());
  assertEquals(snapshot.getInvalid(),snapshot2.getInvalid());
  assertNotEquals(snapshot.getInProgress(),snapshot2.getInProgress());
  assertEquals(snapshot.getCommittingChangeSets(),snapshot2.getCommittingChangeSets());
  assertEquals(snapshot.getCommittedChangeSets(),snapshot2.getCommittedChangeSets());
  Map<Long,TransactionManager.InProgressTx> fixedInProgress=TransactionManager.txnBackwardsCompatCheck(TxConstants.Manager.DEFAULT_TX_LONG_TIMEOUT,10000L,snapshot2.getInProgress());
  assertEquals(snapshot.getInProgress(),fixedInProgress);
  assertEquals(snapshot,snapshot2);
}","/** 
 * In-progress LONG transactions written with DefaultSnapshotCodec will not have the type serialized as part of the data.  Since these transactions also contain a non-negative expiration, we need to ensure we reset the type correctly when the snapshot is loaded.
 */
@Test public void testV2ToTephraV3Compatibility() throws Exception {
  long now=System.currentTimeMillis();
  long nowWritePointer=now * TxConstants.MAX_TX_PER_MS;
  long tInvalid=nowWritePointer - 5;
  long readPtr=nowWritePointer - 4;
  long tLong=nowWritePointer - 3;
  long tCommitted=nowWritePointer - 2;
  long tShort=nowWritePointer - 1;
  TreeMap<Long,TransactionManager.InProgressTx> inProgress=Maps.newTreeMap(ImmutableSortedMap.of(tLong,new TransactionManager.InProgressTx(readPtr,TransactionManager.getTxExpirationFromWritePointer(tLong,TxConstants.Manager.DEFAULT_TX_LONG_TIMEOUT),TransactionManager.InProgressType.LONG),tShort,new TransactionManager.InProgressTx(readPtr,now + 1000,TransactionManager.InProgressType.SHORT)));
  TransactionSnapshot snapshot=new TransactionSnapshot(now,readPtr,nowWritePointer,Lists.newArrayList(tInvalid),inProgress,ImmutableMap.<Long,Set<ChangeId>>of(tShort,Sets.newHashSet(new ChangeId(new byte[]{'r','3'}),new ChangeId(new byte[]{'r','4'}))),ImmutableMap.<Long,Set<ChangeId>>of(tCommitted,Sets.newHashSet(new ChangeId(new byte[]{'r','1'}),new ChangeId(new byte[]{'r','2'}))));
  Configuration conf1=new Configuration();
  conf1.set(TxConstants.Persist.CFG_TX_SNAPHOT_CODEC_CLASSES,SnapshotCodecV2.class.getName());
  SnapshotCodecProvider provider1=new SnapshotCodecProvider(conf1);
  ByteArrayOutputStream out=new ByteArrayOutputStream();
  try {
    provider1.encode(out,snapshot);
  }
  finally {
    out.close();
  }
  TransactionSnapshot snapshot2=provider1.decode(new ByteArrayInputStream(out.toByteArray()));
  assertEquals(snapshot.getReadPointer(),snapshot2.getReadPointer());
  assertEquals(snapshot.getWritePointer(),snapshot2.getWritePointer());
  assertEquals(snapshot.getInvalid(),snapshot2.getInvalid());
  assertNotEquals(snapshot.getInProgress(),snapshot2.getInProgress());
  assertEquals(snapshot.getCommittingChangeSets(),snapshot2.getCommittingChangeSets());
  assertEquals(snapshot.getCommittedChangeSets(),snapshot2.getCommittedChangeSets());
  Map<Long,TransactionManager.InProgressTx> fixedInProgress=TransactionManager.txnBackwardsCompatCheck(TxConstants.Manager.DEFAULT_TX_LONG_TIMEOUT,10000L,snapshot2.getInProgress());
  assertEquals(snapshot.getInProgress(),fixedInProgress);
  assertEquals(snapshot,snapshot2);
}"
5684,"@Override public void onReceived(Iterator<FetchedMessage> messages){
  final ByteBufferInputStream is=new ByteBufferInputStream(null);
  List<MetricValues> records=Lists.newArrayList();
  while (messages.hasNext()) {
    FetchedMessage input=messages.next();
    try {
      MetricValues metricValues=recordReader.read(new BinaryDecoder(is.reset(input.getPayload())),recordSchema);
      records.add(metricValues);
    }
 catch (    IOException e) {
      LOG.info(""String_Node_Str"",e.getMessage());
    }
  }
  if (records.isEmpty()) {
    LOG.info(""String_Node_Str"");
    return;
  }
  try {
    addProcessingStats(records);
    metricStore.add(records);
  }
 catch (  Exception e) {
    String msg=""String_Node_Str"";
    LOG.error(msg);
    throw new RuntimeException(msg,e);
  }
  recordProcessed+=records.size();
  if (recordProcessed % 1000 == 0) {
    LOG.info(""String_Node_Str"",recordProcessed);
    LOG.info(""String_Node_Str"",records.get(records.size() - 1).getTimestamp());
  }
}","@Override public void onReceived(Iterator<FetchedMessage> messages){
  ByteBufferInputStream is=new ByteBufferInputStream(null);
  List<MetricValues> records=Lists.newArrayList();
  while (messages.hasNext()) {
    FetchedMessage input=messages.next();
    try {
      MetricValues metricValues=recordReader.read(new BinaryDecoder(is.reset(input.getPayload())),recordSchema);
      records.add(metricValues);
    }
 catch (    IOException e) {
      LOG.info(""String_Node_Str"",e.getMessage());
    }
  }
  if (records.isEmpty()) {
    LOG.info(""String_Node_Str"");
    return;
  }
  try {
    addProcessingStats(records);
    metricStore.add(records);
  }
 catch (  Exception e) {
    throw new RuntimeException(""String_Node_Str"",e);
  }
  recordsProcessed+=records.size();
  if (System.currentTimeMillis() > lastLoggedMillis + TimeUnit.MINUTES.toMillis(1)) {
    lastLoggedMillis=System.currentTimeMillis();
    LOG.info(""String_Node_Str"",recordsProcessed,records.get(records.size() - 1).getTimestamp());
  }
}"
5685,"protected SessionHandle doOpenHiveSession(Map<String,String> sessionConf) throws HiveSQLException {
  return cliService.openSession(""String_Node_Str"",""String_Node_Str"",sessionConf);
}","protected SessionHandle doOpenHiveSession(Map<String,String> sessionConf) throws HiveSQLException {
  try {
    return cliService.openSession(UserGroupInformation.getCurrentUser().getShortUserName(),""String_Node_Str"",sessionConf);
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}"
5686,"protected SessionHandle doOpenHiveSession(Map<String,String> sessionConf) throws HiveSQLException {
  return cliService.openSession(""String_Node_Str"",""String_Node_Str"",sessionConf);
}","protected SessionHandle doOpenHiveSession(Map<String,String> sessionConf) throws HiveSQLException {
  try {
    return cliService.openSession(UserGroupInformation.getCurrentUser().getShortUserName(),""String_Node_Str"",sessionConf);
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}"
5687,"private void validateMetric(long expected,Id.Application appId,String metric) throws TimeoutException, InterruptedException {
  Map<String,String> tags=ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE,appId.getNamespaceId(),Constants.Metrics.Tag.APP,appId.getId(),Constants.Metrics.Tag.WORKER,ETLWorker.NAME);
  getMetricsManager().waitForTotalMetricCount(tags,""String_Node_Str"" + metric,expected,20,TimeUnit.SECONDS);
  Assert.assertEquals(expected,getMetricsManager().getTotalMetric(tags,""String_Node_Str"" + metric));
}","private void validateMetric(long expected,ApplicationId appId,String metric) throws TimeoutException, InterruptedException {
  Map<String,String> tags=ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE,appId.getNamespace(),Constants.Metrics.Tag.APP,appId.getApplication(),Constants.Metrics.Tag.WORKER,ETLWorker.NAME);
  getMetricsManager().waitForTotalMetricCount(tags,""String_Node_Str"" + metric,expected,20,TimeUnit.SECONDS);
  Assert.assertEquals(expected,getMetricsManager().getTotalMetric(tags,""String_Node_Str"" + metric));
}"
5688,"@Test @Category(SlowTests.class) public void testOneSourceOneSink() throws Exception {
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  List<StructuredRecord> input=new ArrayList<>();
  input.add(StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build());
  input.add(StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build());
  File tmpDir=TMP_FOLDER.newFolder();
  ETLRealtimeConfig etlConfig=ETLRealtimeConfig.builder().addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(input))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(tmpDir))).addConnection(""String_Node_Str"",""String_Node_Str"").build();
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  AppRequest<ETLRealtimeConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkerManager workerManager=appManager.getWorkerManager(ETLWorker.NAME);
  workerManager.start();
  workerManager.waitForStatus(true,10,1);
  try {
    List<StructuredRecord> written=MockSink.getRecords(tmpDir,0,10,TimeUnit.SECONDS);
    Assert.assertEquals(input,written);
  }
  finally {
    stopWorker(workerManager);
  }
  validateMetric(2,appId,""String_Node_Str"");
  validateMetric(2,appId,""String_Node_Str"");
}","@Test @Category(SlowTests.class) public void testOneSourceOneSink() throws Exception {
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  List<StructuredRecord> input=new ArrayList<>();
  input.add(StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build());
  input.add(StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build());
  File tmpDir=TMP_FOLDER.newFolder();
  ETLRealtimeConfig etlConfig=ETLRealtimeConfig.builder().addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(input))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(tmpDir))).addConnection(""String_Node_Str"",""String_Node_Str"").build();
  ApplicationId appId=NamespaceId.DEFAULT.app(""String_Node_Str"");
  AppRequest<ETLRealtimeConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkerManager workerManager=appManager.getWorkerManager(ETLWorker.NAME);
  workerManager.start();
  workerManager.waitForStatus(true,10,1);
  try {
    List<StructuredRecord> written=MockSink.getRecords(tmpDir,0,10,TimeUnit.SECONDS);
    Assert.assertEquals(input,written);
  }
  finally {
    stopWorker(workerManager);
  }
  validateMetric(2,appId,""String_Node_Str"");
  validateMetric(2,appId,""String_Node_Str"");
}"
5689,"@Test public void testEmptyProperties() throws Exception {
  ETLRealtimeConfig etlConfig=ETLRealtimeConfig.builder().addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(null))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(null))).addConnection(""String_Node_Str"",""String_Node_Str"").setInstances(2).build();
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  AppRequest<ETLRealtimeConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationManager appManager=deployApplication(appId,appRequest);
  Assert.assertNotNull(appManager);
  WorkerManager workerManager=appManager.getWorkerManager(ETLWorker.NAME);
  workerManager.start();
  workerManager.waitForStatus(true,10,1);
  try {
    Assert.assertEquals(2,workerManager.getInstances());
  }
  finally {
    stopWorker(workerManager);
  }
}","@Test public void testEmptyProperties() throws Exception {
  ETLRealtimeConfig etlConfig=ETLRealtimeConfig.builder().addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(null))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(null))).addConnection(""String_Node_Str"",""String_Node_Str"").setInstances(2).build();
  ApplicationId appId=NamespaceId.DEFAULT.app(""String_Node_Str"");
  AppRequest<ETLRealtimeConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationManager appManager=deployApplication(appId,appRequest);
  Assert.assertNotNull(appManager);
  WorkerManager workerManager=appManager.getWorkerManager(ETLWorker.NAME);
  workerManager.start();
  workerManager.waitForStatus(true,10,1);
  try {
    Assert.assertEquals(2,workerManager.getInstances());
  }
  finally {
    stopWorker(workerManager);
  }
}"
5690,"@Test public void testLookup() throws Exception {
  addDatasetInstance(KeyValueTable.class.getName(),""String_Node_Str"");
  DataSetManager<KeyValueTable> lookupTable=getDataset(""String_Node_Str"");
  lookupTable.get().write(""String_Node_Str"".getBytes(Charsets.UTF_8),""String_Node_Str"".getBytes(Charsets.UTF_8));
  lookupTable.flush();
  File outDir=TMP_FOLDER.newFolder();
  ETLRealtimeConfig etlConfig=ETLRealtimeConfig.builder().addStage(new ETLStage(""String_Node_Str"",LookupSource.getPlugin(ImmutableSet.of(""String_Node_Str"",""String_Node_Str""),""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(outDir))).addConnection(""String_Node_Str"",""String_Node_Str"").build();
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  AppRequest<ETLRealtimeConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkerManager workerManager=appManager.getWorkerManager(ETLWorker.NAME);
  workerManager.start();
  workerManager.waitForStatus(true,10,1);
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))));
  List<StructuredRecord> expected=new ArrayList<>();
  expected.add(StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build());
  try {
    List<StructuredRecord> actual=MockSink.getRecords(outDir,0,10,TimeUnit.SECONDS);
    Assert.assertEquals(expected,actual);
  }
  finally {
    stopWorker(workerManager);
  }
  validateMetric(1,appId,""String_Node_Str"");
  validateMetric(1,appId,""String_Node_Str"");
}","@Test public void testLookup() throws Exception {
  addDatasetInstance(KeyValueTable.class.getName(),""String_Node_Str"");
  DataSetManager<KeyValueTable> lookupTable=getDataset(""String_Node_Str"");
  lookupTable.get().write(""String_Node_Str"".getBytes(Charsets.UTF_8),""String_Node_Str"".getBytes(Charsets.UTF_8));
  lookupTable.flush();
  File outDir=TMP_FOLDER.newFolder();
  ETLRealtimeConfig etlConfig=ETLRealtimeConfig.builder().addStage(new ETLStage(""String_Node_Str"",LookupSource.getPlugin(ImmutableSet.of(""String_Node_Str"",""String_Node_Str""),""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(outDir))).addConnection(""String_Node_Str"",""String_Node_Str"").build();
  ApplicationId appId=NamespaceId.DEFAULT.app(""String_Node_Str"");
  AppRequest<ETLRealtimeConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkerManager workerManager=appManager.getWorkerManager(ETLWorker.NAME);
  workerManager.start();
  workerManager.waitForStatus(true,10,1);
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))));
  List<StructuredRecord> expected=new ArrayList<>();
  expected.add(StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build());
  try {
    List<StructuredRecord> actual=MockSink.getRecords(outDir,0,10,TimeUnit.SECONDS);
    Assert.assertEquals(expected,actual);
  }
  finally {
    stopWorker(workerManager);
  }
  validateMetric(1,appId,""String_Node_Str"");
  validateMetric(1,appId,""String_Node_Str"");
}"
5691,"@Test public void testDAG() throws Exception {
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)));
  StructuredRecord record1=StructuredRecord.builder(schema).set(""String_Node_Str"",1).build();
  StructuredRecord record2=StructuredRecord.builder(schema).set(""String_Node_Str"",2).build();
  StructuredRecord record3=StructuredRecord.builder(schema).set(""String_Node_Str"",3).build();
  List<StructuredRecord> input=ImmutableList.of(record1,record2,record3);
  File sink1Out=TMP_FOLDER.newFolder();
  File sink2Out=TMP_FOLDER.newFolder();
  ETLRealtimeConfig etlConfig=ETLRealtimeConfig.builder().addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(input))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(sink1Out))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(sink2Out))).addStage(new ETLStage(""String_Node_Str"",IntValueFilterTransform.getPlugin(""String_Node_Str"",2))).addStage(new ETLStage(""String_Node_Str"",DoubleTransform.getPlugin())).addStage(new ETLStage(""String_Node_Str"",IdentityTransform.getPlugin())).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").build();
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  AppRequest<ETLRealtimeConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationManager appManager=deployApplication(appId,appRequest);
  Assert.assertNotNull(appManager);
  WorkerManager workerManager=appManager.getWorkerManager(ETLWorker.NAME);
  workerManager.start();
  workerManager.waitForStatus(true,10,1);
  try {
    List<StructuredRecord> sink1output=MockSink.getRecords(sink1Out,0,10,TimeUnit.SECONDS);
    List<StructuredRecord> sink1expected=ImmutableList.of(record1,record3);
    Assert.assertEquals(sink1expected,sink1output);
    List<StructuredRecord> sink2output=MockSink.getRecords(sink2Out,0,10,TimeUnit.SECONDS);
    Assert.assertEquals(9,sink2output.size());
  }
  finally {
    stopWorker(workerManager);
  }
  validateMetric(3,appId,""String_Node_Str"");
  validateMetric(3,appId,""String_Node_Str"");
  validateMetric(2,appId,""String_Node_Str"");
  validateMetric(3,appId,""String_Node_Str"");
  validateMetric(6,appId,""String_Node_Str"");
  validateMetric(3,appId,""String_Node_Str"");
  validateMetric(3,appId,""String_Node_Str"");
  validateMetric(2,appId,""String_Node_Str"");
  validateMetric(9,appId,""String_Node_Str"");
}","@Test public void testDAG() throws Exception {
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)));
  StructuredRecord record1=StructuredRecord.builder(schema).set(""String_Node_Str"",1).build();
  StructuredRecord record2=StructuredRecord.builder(schema).set(""String_Node_Str"",2).build();
  StructuredRecord record3=StructuredRecord.builder(schema).set(""String_Node_Str"",3).build();
  List<StructuredRecord> input=ImmutableList.of(record1,record2,record3);
  File sink1Out=TMP_FOLDER.newFolder();
  File sink2Out=TMP_FOLDER.newFolder();
  ETLRealtimeConfig etlConfig=ETLRealtimeConfig.builder().addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(input))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(sink1Out))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(sink2Out))).addStage(new ETLStage(""String_Node_Str"",IntValueFilterTransform.getPlugin(""String_Node_Str"",2))).addStage(new ETLStage(""String_Node_Str"",DoubleTransform.getPlugin())).addStage(new ETLStage(""String_Node_Str"",IdentityTransform.getPlugin())).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").build();
  ApplicationId appId=NamespaceId.DEFAULT.app(""String_Node_Str"");
  AppRequest<ETLRealtimeConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationManager appManager=deployApplication(appId,appRequest);
  Assert.assertNotNull(appManager);
  WorkerManager workerManager=appManager.getWorkerManager(ETLWorker.NAME);
  workerManager.start();
  workerManager.waitForStatus(true,10,1);
  try {
    List<StructuredRecord> sink1output=MockSink.getRecords(sink1Out,0,10,TimeUnit.SECONDS);
    List<StructuredRecord> sink1expected=ImmutableList.of(record1,record3);
    Assert.assertEquals(sink1expected,sink1output);
    List<StructuredRecord> sink2output=MockSink.getRecords(sink2Out,0,10,TimeUnit.SECONDS);
    Assert.assertEquals(9,sink2output.size());
  }
  finally {
    stopWorker(workerManager);
  }
  validateMetric(3,appId,""String_Node_Str"");
  validateMetric(3,appId,""String_Node_Str"");
  validateMetric(2,appId,""String_Node_Str"");
  validateMetric(3,appId,""String_Node_Str"");
  validateMetric(6,appId,""String_Node_Str"");
  validateMetric(3,appId,""String_Node_Str"");
  validateMetric(3,appId,""String_Node_Str"");
  validateMetric(2,appId,""String_Node_Str"");
  validateMetric(9,appId,""String_Node_Str"");
}"
5692,"private <T extends ETLConfig>void upgrade(ApplicationId appId,ArtifactSummary appArtifact,T config) throws IOException {
  AppRequest<T> updateRequest=new AppRequest<>(appArtifact,config);
  try {
    appClient.update(appId.toId(),updateRequest);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",appId,e);
    if (errorDir != null) {
      File errorFile=new File(errorDir,String.format(""String_Node_Str"",appId.getParent(),appId.getEntityName()));
      LOG.error(""String_Node_Str"",appId,errorFile.getAbsolutePath());
      try (OutputStreamWriter outputStreamWriter=new OutputStreamWriter(new FileOutputStream(errorFile))){
        outputStreamWriter.write(GSON.toJson(updateRequest));
      }
     }
  }
}","private <T extends ETLConfig>void upgrade(ApplicationId appId,ArtifactSummary appArtifact,T config) throws IOException {
  AppRequest<T> updateRequest=new AppRequest<>(appArtifact,config);
  try {
    appClient.update(appId,updateRequest);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",appId,e);
    if (errorDir != null) {
      File errorFile=new File(errorDir,String.format(""String_Node_Str"",appId.getParent(),appId.getEntityName()));
      LOG.error(""String_Node_Str"",appId,errorFile.getAbsolutePath());
      try (OutputStreamWriter outputStreamWriter=new OutputStreamWriter(new FileOutputStream(errorFile))){
        outputStreamWriter.write(GSON.toJson(updateRequest));
      }
     }
  }
}"
5693,"@Nullable @Override public ArtifactSelectorConfig getPluginArtifact(String pluginType,String pluginName){
  try {
    List<PluginInfo> plugins=artifactClient.getPluginInfo(artifactId.toId(),pluginType,pluginName,ArtifactScope.SYSTEM);
    if (plugins.isEmpty()) {
      return null;
    }
    ArtifactSummary chosenArtifact=plugins.get(plugins.size() - 1).getArtifact();
    return new ArtifactSelectorConfig(chosenArtifact.getScope().name(),chosenArtifact.getName(),chosenArtifact.getVersion());
  }
 catch (  Exception e) {
    return null;
  }
}","@Nullable @Override public ArtifactSelectorConfig getPluginArtifact(String pluginType,String pluginName){
  try {
    List<PluginInfo> plugins=artifactClient.getPluginInfo(artifactId,pluginType,pluginName,ArtifactScope.SYSTEM);
    if (plugins.isEmpty()) {
      return null;
    }
    ArtifactSummary chosenArtifact=plugins.get(plugins.size() - 1).getArtifact();
    return new ArtifactSelectorConfig(chosenArtifact.getScope().name(),chosenArtifact.getName(),chosenArtifact.getVersion());
  }
 catch (  Exception e) {
    return null;
  }
}"
5694,"@Test public void testRuntimeArgs() throws Exception {
  ServiceId service=fakeAppId.service(PrefixedEchoHandler.NAME);
  ServiceId serviceV1=fakeAppIdV1.service(PrefixedEchoHandler.NAME);
  String serviceName=String.format(""String_Node_Str"",FakeApp.NAME,PrefixedEchoHandler.NAME);
  String serviceArgument=String.format(""String_Node_Str"",serviceName,ApplicationId.DEFAULT_VERSION);
  String serviceV1Argument=String.format(""String_Node_Str"",serviceName,V1_SNAPSHOT);
  Map<String,String> runtimeArgs=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  String runtimeArgsJson=GSON.toJson(runtimeArgs);
  String runtimeArgsKV=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(runtimeArgs);
  testCommandOutputContains(cli,""String_Node_Str"" + serviceArgument + ""String_Node_Str""+ runtimeArgsKV+ ""String_Node_Str"",""String_Node_Str"");
  assertProgramStatus(programClient,service,""String_Node_Str"");
  try {
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    assertProgramStatus(programClient,service,""String_Node_Str"");
    Map<String,String> runtimeArgs1=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    String runtimeArgs1Json=GSON.toJson(runtimeArgs1);
    String runtimeArgs1KV=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(runtimeArgs1);
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument + ""String_Node_Str""+ runtimeArgs1KV+ ""String_Node_Str"",""String_Node_Str"");
    assertProgramStatus(programClient,serviceV1,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument + ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument,""String_Node_Str"");
    assertProgramStatus(programClient,serviceV1,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str""+ runtimeArgs1KV+ ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,runtimeArgs1Json);
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument + ""String_Node_Str""+ runtimeArgsKV+ ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument,runtimeArgsJson);
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument + ""String_Node_Str"",""String_Node_Str"");
  }
  finally {
    programClient.stopAll(NamespaceId.DEFAULT.toId());
  }
}","@Test public void testRuntimeArgs() throws Exception {
  ServiceId service=fakeAppId.service(PrefixedEchoHandler.NAME);
  ServiceId serviceV1=fakeAppIdV1.service(PrefixedEchoHandler.NAME);
  String serviceName=String.format(""String_Node_Str"",FakeApp.NAME,PrefixedEchoHandler.NAME);
  String serviceArgument=String.format(""String_Node_Str"",serviceName,ApplicationId.DEFAULT_VERSION);
  String serviceV1Argument=String.format(""String_Node_Str"",serviceName,V1_SNAPSHOT);
  Map<String,String> runtimeArgs=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  String runtimeArgsJson=GSON.toJson(runtimeArgs);
  String runtimeArgsKV=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(runtimeArgs);
  testCommandOutputContains(cli,""String_Node_Str"" + serviceArgument + ""String_Node_Str""+ runtimeArgsKV+ ""String_Node_Str"",""String_Node_Str"");
  assertProgramStatus(programClient,service,""String_Node_Str"");
  try {
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    assertProgramStatus(programClient,service,""String_Node_Str"");
    Map<String,String> runtimeArgs1=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    String runtimeArgs1Json=GSON.toJson(runtimeArgs1);
    String runtimeArgs1KV=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(runtimeArgs1);
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument + ""String_Node_Str""+ runtimeArgs1KV+ ""String_Node_Str"",""String_Node_Str"");
    assertProgramStatus(programClient,serviceV1,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument + ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument,""String_Node_Str"");
    assertProgramStatus(programClient,serviceV1,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str""+ runtimeArgs1KV+ ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,runtimeArgs1Json);
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument + ""String_Node_Str""+ runtimeArgsKV+ ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument,runtimeArgsJson);
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument + ""String_Node_Str"",""String_Node_Str"");
  }
  finally {
    programClient.stopAll(NamespaceId.DEFAULT);
  }
}"
5695,"@Test public void testRouteConfig() throws Exception {
  ServiceId service=fakeAppId.service(PrefixedEchoHandler.NAME);
  ServiceId serviceV1=fakeAppIdV1.service(PrefixedEchoHandler.NAME);
  String serviceName=String.format(""String_Node_Str"",FakeApp.NAME,PrefixedEchoHandler.NAME);
  String serviceArgument=String.format(""String_Node_Str"",serviceName,ApplicationId.DEFAULT_VERSION);
  String serviceV1Argument=String.format(""String_Node_Str"",serviceName,V1_SNAPSHOT);
  try {
    Map<String,String> runtimeArgs=ImmutableMap.of(""String_Node_Str"",ApplicationId.DEFAULT_VERSION);
    String runtimeArgsKV=SPACE_JOINER.withKeyValueSeparator(""String_Node_Str"").join(runtimeArgs);
    testCommandOutputContains(cli,""String_Node_Str"" + serviceArgument + ""String_Node_Str""+ runtimeArgsKV+ ""String_Node_Str"",""String_Node_Str"");
    assertProgramStatus(programClient,service,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,GSON.toJson(ImmutableMap.of()));
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str"",String.format(""String_Node_Str"",ApplicationId.DEFAULT_VERSION));
    Map<String,String> runtimeArgs1=ImmutableMap.of(""String_Node_Str"",V1_SNAPSHOT);
    String runtimeArgs1KV=SPACE_JOINER.withKeyValueSeparator(""String_Node_Str"").join(runtimeArgs1);
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument + ""String_Node_Str""+ runtimeArgs1KV+ ""String_Node_Str"",""String_Node_Str"");
    assertProgramStatus(programClient,serviceV1,""String_Node_Str"");
    Map<String,Integer> routeConfig=ImmutableMap.of(ApplicationId.DEFAULT_VERSION,100,V1_SNAPSHOT,0);
    String routeConfigKV=SPACE_JOINER.withKeyValueSeparator(""String_Node_Str"").join(routeConfig);
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str""+ routeConfigKV+ ""String_Node_Str"",""String_Node_Str"");
    for (int i=0; i < 10; i++) {
      testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str"",String.format(""String_Node_Str"",ApplicationId.DEFAULT_VERSION));
    }
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,GSON.toJson(routeConfig));
    routeConfig=ImmutableMap.of(ApplicationId.DEFAULT_VERSION,0,V1_SNAPSHOT,100);
    routeConfigKV=SPACE_JOINER.withKeyValueSeparator(""String_Node_Str"").join(routeConfig);
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str""+ routeConfigKV+ ""String_Node_Str"",""String_Node_Str"");
    for (int i=0; i < 10; i++) {
      testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str"",String.format(""String_Node_Str"",V1_SNAPSHOT));
    }
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,GSON.toJson(routeConfig));
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,GSON.toJson(ImmutableMap.of()));
  }
  finally {
    programClient.stopAll(NamespaceId.DEFAULT.toId());
  }
}","@Test public void testRouteConfig() throws Exception {
  ServiceId service=fakeAppId.service(PrefixedEchoHandler.NAME);
  ServiceId serviceV1=fakeAppIdV1.service(PrefixedEchoHandler.NAME);
  String serviceName=String.format(""String_Node_Str"",FakeApp.NAME,PrefixedEchoHandler.NAME);
  String serviceArgument=String.format(""String_Node_Str"",serviceName,ApplicationId.DEFAULT_VERSION);
  String serviceV1Argument=String.format(""String_Node_Str"",serviceName,V1_SNAPSHOT);
  try {
    Map<String,String> runtimeArgs=ImmutableMap.of(""String_Node_Str"",ApplicationId.DEFAULT_VERSION);
    String runtimeArgsKV=SPACE_JOINER.withKeyValueSeparator(""String_Node_Str"").join(runtimeArgs);
    testCommandOutputContains(cli,""String_Node_Str"" + serviceArgument + ""String_Node_Str""+ runtimeArgsKV+ ""String_Node_Str"",""String_Node_Str"");
    assertProgramStatus(programClient,service,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,GSON.toJson(ImmutableMap.of()));
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str"",String.format(""String_Node_Str"",ApplicationId.DEFAULT_VERSION));
    Map<String,String> runtimeArgs1=ImmutableMap.of(""String_Node_Str"",V1_SNAPSHOT);
    String runtimeArgs1KV=SPACE_JOINER.withKeyValueSeparator(""String_Node_Str"").join(runtimeArgs1);
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument + ""String_Node_Str""+ runtimeArgs1KV+ ""String_Node_Str"",""String_Node_Str"");
    assertProgramStatus(programClient,serviceV1,""String_Node_Str"");
    Map<String,Integer> routeConfig=ImmutableMap.of(ApplicationId.DEFAULT_VERSION,100,V1_SNAPSHOT,0);
    String routeConfigKV=SPACE_JOINER.withKeyValueSeparator(""String_Node_Str"").join(routeConfig);
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str""+ routeConfigKV+ ""String_Node_Str"",""String_Node_Str"");
    for (int i=0; i < 10; i++) {
      testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str"",String.format(""String_Node_Str"",ApplicationId.DEFAULT_VERSION));
    }
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,GSON.toJson(routeConfig));
    routeConfig=ImmutableMap.of(ApplicationId.DEFAULT_VERSION,0,V1_SNAPSHOT,100);
    routeConfigKV=SPACE_JOINER.withKeyValueSeparator(""String_Node_Str"").join(routeConfig);
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str""+ routeConfigKV+ ""String_Node_Str"",""String_Node_Str"");
    for (int i=0; i < 10; i++) {
      testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str"",String.format(""String_Node_Str"",V1_SNAPSHOT));
    }
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,GSON.toJson(routeConfig));
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,GSON.toJson(ImmutableMap.of()));
  }
  finally {
    programClient.stopAll(NamespaceId.DEFAULT);
  }
}"
5696,"@Test public void testDataset() throws Exception {
  String datasetName=PREFIX + ""String_Node_Str"";
  DatasetTypeClient datasetTypeClient=new DatasetTypeClient(cliConfig.getClientConfig());
  DatasetTypeMeta datasetType=datasetTypeClient.list(NamespaceId.DEFAULT.toId()).get(0);
  testCommandOutputContains(cli,""String_Node_Str"" + datasetType.getName() + ""String_Node_Str""+ datasetName+ ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",FakeDataset.class.getSimpleName());
  testCommandOutputContains(cli,""String_Node_Str"" + datasetName,""String_Node_Str"");
  NamespaceClient namespaceClient=new NamespaceClient(cliConfig.getClientConfig());
  NamespaceId barspace=new NamespaceId(""String_Node_Str"");
  namespaceClient.create(new NamespaceMeta.Builder().setName(barspace.toId()).build());
  cliConfig.setNamespace(barspace);
  testCommandOutputNotContains(cli,""String_Node_Str"",FakeDataset.class.getSimpleName());
  DatasetTypeId datasetType1=barspace.datasetType(datasetType.getName());
  testCommandOutputContains(cli,""String_Node_Str"" + datasetType.getName() + ""String_Node_Str""+ datasetName,new DatasetTypeNotFoundException(datasetType1).getMessage());
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  try {
    testCommandOutputContains(cli,""String_Node_Str"" + datasetName,""String_Node_Str"");
  }
  finally {
    testCommandOutputContains(cli,""String_Node_Str"" + datasetName,""String_Node_Str"");
  }
  String datasetName2=PREFIX + ""String_Node_Str"";
  String description=""String_Node_Str"" + datasetName2;
  testCommandOutputContains(cli,""String_Node_Str"" + datasetType.getName() + ""String_Node_Str""+ datasetName2+ ""String_Node_Str""+ ""String_Node_Str""+ description,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",description);
  testCommandOutputContains(cli,""String_Node_Str"" + datasetName2,""String_Node_Str"");
}","@Test public void testDataset() throws Exception {
  String datasetName=PREFIX + ""String_Node_Str"";
  DatasetTypeClient datasetTypeClient=new DatasetTypeClient(cliConfig.getClientConfig());
  DatasetTypeMeta datasetType=datasetTypeClient.list(NamespaceId.DEFAULT).get(0);
  testCommandOutputContains(cli,""String_Node_Str"" + datasetType.getName() + ""String_Node_Str""+ datasetName+ ""String_Node_Str"",""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",FakeDataset.class.getSimpleName());
  testCommandOutputContains(cli,""String_Node_Str"" + datasetName,""String_Node_Str"");
  NamespaceClient namespaceClient=new NamespaceClient(cliConfig.getClientConfig());
  NamespaceId barspace=new NamespaceId(""String_Node_Str"");
  namespaceClient.create(new NamespaceMeta.Builder().setName(barspace).build());
  cliConfig.setNamespace(barspace);
  testCommandOutputNotContains(cli,""String_Node_Str"",FakeDataset.class.getSimpleName());
  DatasetTypeId datasetType1=barspace.datasetType(datasetType.getName());
  testCommandOutputContains(cli,""String_Node_Str"" + datasetType.getName() + ""String_Node_Str""+ datasetName,new DatasetTypeNotFoundException(datasetType1).getMessage());
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  try {
    testCommandOutputContains(cli,""String_Node_Str"" + datasetName,""String_Node_Str"");
  }
  finally {
    testCommandOutputContains(cli,""String_Node_Str"" + datasetName,""String_Node_Str"");
  }
  String datasetName2=PREFIX + ""String_Node_Str"";
  String description=""String_Node_Str"" + datasetName2;
  testCommandOutputContains(cli,""String_Node_Str"" + datasetType.getName() + ""String_Node_Str""+ datasetName2+ ""String_Node_Str""+ ""String_Node_Str""+ description,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",description);
  testCommandOutputContains(cli,""String_Node_Str"" + datasetName2,""String_Node_Str"");
}"
5697,"@Test public void testService() throws Exception {
  ServiceId service=fakeAppId.service(PrefixedEchoHandler.NAME);
  ServiceId serviceV1=fakeAppIdV1.service(PrefixedEchoHandler.NAME);
  String serviceName=String.format(""String_Node_Str"",FakeApp.NAME,PrefixedEchoHandler.NAME);
  String serviceArgument=String.format(""String_Node_Str"",serviceName,ApplicationId.DEFAULT_VERSION);
  String serviceV1Argument=String.format(""String_Node_Str"",serviceName,V1_SNAPSHOT);
  try {
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    assertProgramStatus(programClient,service,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceArgument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument,""String_Node_Str"");
    assertProgramStatus(programClient,service,""String_Node_Str"");
    assertProgramStatus(programClient,serviceV1,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceArgument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceArgument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceArgument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceArgument + ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument + ""String_Node_Str"",""String_Node_Str"");
  }
  finally {
    programClient.stopAll(NamespaceId.DEFAULT.toId());
  }
}","@Test public void testService() throws Exception {
  ServiceId service=fakeAppId.service(PrefixedEchoHandler.NAME);
  ServiceId serviceV1=fakeAppIdV1.service(PrefixedEchoHandler.NAME);
  String serviceName=String.format(""String_Node_Str"",FakeApp.NAME,PrefixedEchoHandler.NAME);
  String serviceArgument=String.format(""String_Node_Str"",serviceName,ApplicationId.DEFAULT_VERSION);
  String serviceV1Argument=String.format(""String_Node_Str"",serviceName,V1_SNAPSHOT);
  try {
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName + ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceName,""String_Node_Str"");
    assertProgramStatus(programClient,service,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceArgument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument,""String_Node_Str"");
    assertProgramStatus(programClient,service,""String_Node_Str"");
    assertProgramStatus(programClient,serviceV1,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceArgument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceArgument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceArgument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument,""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceArgument + ""String_Node_Str"",""String_Node_Str"");
    testCommandOutputContains(cli,""String_Node_Str"" + serviceV1Argument + ""String_Node_Str"",""String_Node_Str"");
  }
  finally {
    programClient.stopAll(NamespaceId.DEFAULT);
  }
}"
5698,"private Map<String,String> parsePreferences(String[] programIdParts) throws IOException, UnauthenticatedException, NotFoundException, UnauthorizedException {
switch (type) {
case INSTANCE:
    checkInputLength(programIdParts,0);
  return client.getInstancePreferences();
case NAMESPACE:
checkInputLength(programIdParts,0);
return client.getNamespacePreferences(cliConfig.getCurrentNamespace().toId(),resolved);
case APP:
return client.getApplicationPreferences(parseAppId(programIdParts).toId(),resolved);
case FLOW:
case MAPREDUCE:
case WORKFLOW:
case SERVICE:
case SPARK:
return client.getProgramPreferences(parseProgramId(programIdParts,type.getProgramType()).toId(),resolved);
default :
throw new IllegalArgumentException(""String_Node_Str"" + type.getShortName());
}
}","private Map<String,String> parsePreferences(String[] programIdParts) throws IOException, UnauthenticatedException, NotFoundException, UnauthorizedException {
switch (type) {
case INSTANCE:
    checkInputLength(programIdParts,0);
  return client.getInstancePreferences();
case NAMESPACE:
checkInputLength(programIdParts,0);
return client.getNamespacePreferences(cliConfig.getCurrentNamespace(),resolved);
case APP:
return client.getApplicationPreferences(parseAppId(programIdParts),resolved);
case FLOW:
case MAPREDUCE:
case WORKFLOW:
case SERVICE:
case SPARK:
return client.getProgramPreferences(parseProgramId(programIdParts,type.getProgramType()),resolved);
default :
throw new IllegalArgumentException(""String_Node_Str"" + type.getShortName());
}
}"
5699,"protected void setPreferences(String[] programIdParts,PrintStream printStream,Map<String,String> args) throws Exception {
switch (type) {
case INSTANCE:
    checkInputLength(programIdParts,0);
  client.setInstancePreferences(args);
printSuccessMessage(printStream,type);
break;
case NAMESPACE:
checkInputLength(programIdParts,0);
client.setNamespacePreferences(cliConfig.getCurrentNamespace().toId(),args);
printSuccessMessage(printStream,type);
break;
case APP:
client.setApplicationPreferences(parseAppId(programIdParts).toId(),args);
printSuccessMessage(printStream,type);
break;
case FLOW:
case MAPREDUCE:
case WORKFLOW:
case SERVICE:
case SPARK:
client.setProgramPreferences(parseProgramId(programIdParts,type.getProgramType()).toId(),args);
printSuccessMessage(printStream,type);
break;
default :
throw new IllegalArgumentException(""String_Node_Str"" + type.getShortName());
}
}","protected void setPreferences(String[] programIdParts,PrintStream printStream,Map<String,String> args) throws Exception {
switch (type) {
case INSTANCE:
    checkInputLength(programIdParts,0);
  client.setInstancePreferences(args);
printSuccessMessage(printStream,type);
break;
case NAMESPACE:
checkInputLength(programIdParts,0);
client.setNamespacePreferences(cliConfig.getCurrentNamespace(),args);
printSuccessMessage(printStream,type);
break;
case APP:
client.setApplicationPreferences(parseAppId(programIdParts),args);
printSuccessMessage(printStream,type);
break;
case FLOW:
case MAPREDUCE:
case WORKFLOW:
case SERVICE:
case SPARK:
client.setProgramPreferences(parseProgramId(programIdParts,type.getProgramType()),args);
printSuccessMessage(printStream,type);
break;
default :
throw new IllegalArgumentException(""String_Node_Str"" + type.getShortName());
}
}"
5700,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String datasetType=arguments.get(ArgumentName.DATASET_TYPE.toString());
  String datasetName=arguments.get(ArgumentName.NEW_DATASET.toString());
  String datasetPropertiesString=arguments.getOptional(ArgumentName.DATASET_PROPERTIES.toString(),""String_Node_Str"");
  String datasetDescription=arguments.getOptional(ArgumentName.DATASET_DESCRIPTON.toString(),null);
  Map<String,String> datasetProperties=ArgumentParser.parseMap(datasetPropertiesString);
  DatasetInstanceConfiguration datasetConfig=new DatasetInstanceConfiguration(datasetType,datasetProperties,datasetDescription);
  datasetClient.create(cliConfig.getCurrentNamespace().dataset(datasetName).toId(),datasetConfig);
  output.printf(""String_Node_Str"",datasetName,datasetType,GSON.toJson(datasetProperties));
  output.println();
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String datasetType=arguments.get(ArgumentName.DATASET_TYPE.toString());
  String datasetName=arguments.get(ArgumentName.NEW_DATASET.toString());
  String datasetPropertiesString=arguments.getOptional(ArgumentName.DATASET_PROPERTIES.toString(),""String_Node_Str"");
  String datasetDescription=arguments.getOptional(ArgumentName.DATASET_DESCRIPTON.toString(),null);
  Map<String,String> datasetProperties=ArgumentParser.parseMap(datasetPropertiesString);
  DatasetInstanceConfiguration datasetConfig=new DatasetInstanceConfiguration(datasetType,datasetProperties,datasetDescription);
  datasetClient.create(cliConfig.getCurrentNamespace().dataset(datasetName),datasetConfig);
  output.printf(""String_Node_Str"",datasetName,datasetType,GSON.toJson(datasetProperties));
  output.println();
}"
5701,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String streamId=arguments.get(ArgumentName.NEW_STREAM.toString());
  StreamProperties streamProperties=null;
  if (arguments.hasArgument(ArgumentName.LOCAL_FILE_PATH.toString())) {
    File file=new File(arguments.get(ArgumentName.LOCAL_FILE_PATH.toString()));
    try (Reader reader=Files.newReader(file,Charsets.UTF_8)){
      streamProperties=GSON.fromJson(reader,StreamProperties.class);
    }
 catch (    FileNotFoundException e) {
      throw new IllegalArgumentException(""String_Node_Str"" + file);
    }
catch (    Exception e) {
      throw new IllegalArgumentException(""String_Node_Str"",e);
    }
  }
  streamClient.create(cliConfig.getCurrentNamespace().stream(streamId).toId(),streamProperties);
  output.printf(""String_Node_Str"",streamId);
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String streamId=arguments.get(ArgumentName.NEW_STREAM.toString());
  StreamProperties streamProperties=null;
  if (arguments.hasArgument(ArgumentName.LOCAL_FILE_PATH.toString())) {
    File file=new File(arguments.get(ArgumentName.LOCAL_FILE_PATH.toString()));
    try (Reader reader=Files.newReader(file,Charsets.UTF_8)){
      streamProperties=GSON.fromJson(reader,StreamProperties.class);
    }
 catch (    FileNotFoundException e) {
      throw new IllegalArgumentException(""String_Node_Str"" + file);
    }
catch (    Exception e) {
      throw new IllegalArgumentException(""String_Node_Str"",e);
    }
  }
  streamClient.create(cliConfig.getCurrentNamespace().stream(streamId),streamProperties);
  output.printf(""String_Node_Str"",streamId);
}"
5702,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetId dataset=cliConfig.getCurrentNamespace().dataset(arguments.get(ArgumentName.DATASET.toString()));
  datasetClient.delete(dataset.toId());
  output.printf(""String_Node_Str"",dataset.getEntityName());
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetId dataset=cliConfig.getCurrentNamespace().dataset(arguments.get(ArgumentName.DATASET.toString()));
  datasetClient.delete(dataset);
  output.printf(""String_Node_Str"",dataset.getEntityName());
}"
5703,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetModuleId module=cliConfig.getCurrentNamespace().datasetModule(arguments.get(ArgumentName.DATASET_MODULE.toString()));
  datasetClient.delete(module.toId());
  output.printf(""String_Node_Str"",module.getEntityName());
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetModuleId module=cliConfig.getCurrentNamespace().datasetModule(arguments.get(ArgumentName.DATASET_MODULE.toString()));
  datasetClient.delete(module);
  output.printf(""String_Node_Str"",module.getEntityName());
}"
5704,"@Override public void perform(Arguments arguments,PrintStream printStream) throws Exception {
  String[] programIdParts=new String[0];
  if (arguments.hasArgument(type.getArgumentName().toString())) {
    programIdParts=arguments.get(type.getArgumentName().toString()).split(""String_Node_Str"");
  }
switch (type) {
case INSTANCE:
    checkInputLength(programIdParts,0);
  client.deleteInstancePreferences();
printStream.printf(SUCCESS + ""String_Node_Str"",type.getName());
break;
case NAMESPACE:
checkInputLength(programIdParts,0);
client.deleteNamespacePreferences(cliConfig.getCurrentNamespace().toId());
printStream.printf(SUCCESS + ""String_Node_Str"",type.getName());
break;
case APP:
client.deleteApplicationPreferences(parseAppId(programIdParts).toId());
printStream.printf(SUCCESS + ""String_Node_Str"",type.getName());
break;
case FLOW:
case MAPREDUCE:
case WORKFLOW:
case SERVICE:
case SPARK:
checkInputLength(programIdParts,2);
client.deleteProgramPreferences(parseProgramId(programIdParts,type.getProgramType()).toId());
printStream.printf(SUCCESS + ""String_Node_Str"",type.getName());
break;
default :
throw new IllegalArgumentException(""String_Node_Str"" + type.getName());
}
}","@Override public void perform(Arguments arguments,PrintStream printStream) throws Exception {
  String[] programIdParts=new String[0];
  if (arguments.hasArgument(type.getArgumentName().toString())) {
    programIdParts=arguments.get(type.getArgumentName().toString()).split(""String_Node_Str"");
  }
switch (type) {
case INSTANCE:
    checkInputLength(programIdParts,0);
  client.deleteInstancePreferences();
printStream.printf(SUCCESS + ""String_Node_Str"",type.getName());
break;
case NAMESPACE:
checkInputLength(programIdParts,0);
client.deleteNamespacePreferences(cliConfig.getCurrentNamespace());
printStream.printf(SUCCESS + ""String_Node_Str"",type.getName());
break;
case APP:
client.deleteApplicationPreferences(parseAppId(programIdParts));
printStream.printf(SUCCESS + ""String_Node_Str"",type.getName());
break;
case FLOW:
case MAPREDUCE:
case WORKFLOW:
case SERVICE:
case SPARK:
checkInputLength(programIdParts,2);
client.deleteProgramPreferences(parseProgramId(programIdParts,type.getProgramType()));
printStream.printf(SUCCESS + ""String_Node_Str"",type.getName());
break;
default :
throw new IllegalArgumentException(""String_Node_Str"" + type.getName());
}
}"
5705,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  StreamId streamId=cliConfig.getCurrentNamespace().stream(arguments.get(ArgumentName.STREAM.toString()));
  streamClient.delete(streamId.toId());
  output.printf(""String_Node_Str"",streamId.getEntityName());
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  StreamId streamId=cliConfig.getCurrentNamespace().stream(arguments.get(ArgumentName.STREAM.toString()));
  streamClient.delete(streamId);
  output.printf(""String_Node_Str"",streamId.getEntityName());
}"
5706,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  File moduleJarFile=resolver.resolvePathToFile(arguments.get(ArgumentName.DATASET_MODULE_JAR_FILE.toString()));
  Preconditions.checkArgument(moduleJarFile.exists(),""String_Node_Str"" + moduleJarFile.getAbsolutePath() + ""String_Node_Str"");
  Preconditions.checkArgument(moduleJarFile.canRead());
  String moduleName=arguments.get(ArgumentName.NEW_DATASET_MODULE.toString());
  String moduleJarClassname=arguments.get(ArgumentName.DATASET_MODULE_JAR_CLASSNAME.toString());
  datasetModuleClient.add(cliConfig.getCurrentNamespace().datasetModule(moduleName).toId(),moduleJarClassname,moduleJarFile);
  output.printf(""String_Node_Str"",moduleName);
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  File moduleJarFile=resolver.resolvePathToFile(arguments.get(ArgumentName.DATASET_MODULE_JAR_FILE.toString()));
  Preconditions.checkArgument(moduleJarFile.exists(),""String_Node_Str"" + moduleJarFile.getAbsolutePath() + ""String_Node_Str"");
  Preconditions.checkArgument(moduleJarFile.canRead());
  String moduleName=arguments.get(ArgumentName.NEW_DATASET_MODULE.toString());
  String moduleJarClassname=arguments.get(ArgumentName.DATASET_MODULE_JAR_CLASSNAME.toString());
  datasetModuleClient.add(cliConfig.getCurrentNamespace().datasetModule(moduleName),moduleJarClassname,moduleJarFile);
  output.printf(""String_Node_Str"",moduleName);
}"
5707,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetId instance=cliConfig.getCurrentNamespace().dataset(arguments.get(ArgumentName.DATASET.toString()));
  DatasetMeta meta=datasetClient.get(instance.toId());
  Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(ImmutableList.of(meta),new RowMaker<DatasetMeta>(){
    @Override public List<?> makeRow(    DatasetMeta object){
      return Lists.newArrayList(object.getHiveTableName(),GSON.toJson(object.getSpec()),GSON.toJson(object.getType()));
    }
  }
).build();
  cliConfig.getTableRenderer().render(cliConfig,output,table);
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetId instance=cliConfig.getCurrentNamespace().dataset(arguments.get(ArgumentName.DATASET.toString()));
  DatasetMeta meta=datasetClient.get(instance);
  Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(ImmutableList.of(meta),new RowMaker<DatasetMeta>(){
    @Override public List<?> makeRow(    DatasetMeta object){
      return Lists.newArrayList(object.getHiveTableName(),GSON.toJson(object.getSpec()),GSON.toJson(object.getType()));
    }
  }
).build();
  cliConfig.getTableRenderer().render(cliConfig,output,table);
}"
5708,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetModuleId moduleId=cliConfig.getCurrentNamespace().datasetModule(arguments.get(ArgumentName.DATASET_MODULE.toString()));
  DatasetModuleMeta datasetModuleMeta=datasetModuleClient.get(moduleId.toId());
  Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(ImmutableList.of(datasetModuleMeta),new RowMaker<DatasetModuleMeta>(){
    @Override public List<?> makeRow(    DatasetModuleMeta object){
      return Lists.newArrayList(object.getName(),object.getClassName(),object.getJarLocation(),Joiner.on(""String_Node_Str"").join(object.getTypes()),Joiner.on(""String_Node_Str"").join(object.getUsesModules()),Joiner.on(""String_Node_Str"").join(object.getUsedByModules()));
    }
  }
).build();
  cliConfig.getTableRenderer().render(cliConfig,output,table);
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetModuleId moduleId=cliConfig.getCurrentNamespace().datasetModule(arguments.get(ArgumentName.DATASET_MODULE.toString()));
  DatasetModuleMeta datasetModuleMeta=datasetModuleClient.get(moduleId);
  Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(ImmutableList.of(datasetModuleMeta),new RowMaker<DatasetModuleMeta>(){
    @Override public List<?> makeRow(    DatasetModuleMeta object){
      return Lists.newArrayList(object.getName(),object.getClassName(),object.getJarLocation(),Joiner.on(""String_Node_Str"").join(object.getTypes()),Joiner.on(""String_Node_Str"").join(object.getUsesModules()),Joiner.on(""String_Node_Str"").join(object.getUsedByModules()));
    }
  }
).build();
  cliConfig.getTableRenderer().render(cliConfig,output,table);
}"
5709,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetTypeId type=cliConfig.getCurrentNamespace().datasetType(arguments.get(ArgumentName.DATASET_TYPE.toString()));
  DatasetTypeMeta datasetTypeMeta=datasetTypeClient.get(type.toId());
  Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"").setRows(ImmutableList.of(datasetTypeMeta),new RowMaker<DatasetTypeMeta>(){
    @Override public List<?> makeRow(    DatasetTypeMeta object){
      return Lists.newArrayList(object.getName(),Joiner.on(""String_Node_Str"").join(object.getModules()));
    }
  }
).build();
  cliConfig.getTableRenderer().render(cliConfig,output,table);
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetTypeId type=cliConfig.getCurrentNamespace().datasetType(arguments.get(ArgumentName.DATASET_TYPE.toString()));
  DatasetTypeMeta datasetTypeMeta=datasetTypeClient.get(type);
  Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"").setRows(ImmutableList.of(datasetTypeMeta),new RowMaker<DatasetTypeMeta>(){
    @Override public List<?> makeRow(    DatasetTypeMeta object){
      return Lists.newArrayList(object.getName(),Joiner.on(""String_Node_Str"").join(object.getModules()));
    }
  }
).build();
  cliConfig.getTableRenderer().render(cliConfig,output,table);
}"
5710,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  StreamId streamId=cliConfig.getCurrentNamespace().stream(arguments.get(ArgumentName.STREAM.toString()));
  StreamProperties config=streamClient.getConfig(streamId.toId());
  Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(ImmutableList.of(config),new RowMaker<StreamProperties>(){
    @Override public List<?> makeRow(    StreamProperties object){
      FormatSpecification format=object.getFormat();
      return Lists.newArrayList(object.getTTL(),format.getName(),format.getSchema().toString(),object.getNotificationThresholdMB(),object.getDescription());
    }
  }
).build();
  cliConfig.getTableRenderer().render(cliConfig,output,table);
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  StreamId streamId=cliConfig.getCurrentNamespace().stream(arguments.get(ArgumentName.STREAM.toString()));
  StreamProperties config=streamClient.getConfig(streamId);
  Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(ImmutableList.of(config),new RowMaker<StreamProperties>(){
    @Override public List<?> makeRow(    StreamProperties object){
      FormatSpecification format=object.getFormat();
      return Lists.newArrayList(object.getTTL(),format.getName(),format.getSchema().toString(),object.getNotificationThresholdMB(),object.getDescription());
    }
  }
).build();
  cliConfig.getTableRenderer().render(cliConfig,output,table);
}"
5711,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetId instance=cliConfig.getCurrentNamespace().dataset(arguments.get(ArgumentName.DATASET.toString()));
  Map<String,String> properties=datasetClient.getProperties(instance.toId());
  output.printf(GSON.toJson(properties));
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  DatasetId instance=cliConfig.getCurrentNamespace().dataset(arguments.get(ArgumentName.DATASET.toString()));
  Map<String,String> properties=datasetClient.getProperties(instance);
  output.printf(GSON.toJson(properties));
}"
5712,"@Override @SuppressWarnings(""String_Node_Str"") public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] programIdParts=arguments.get(elementType.getArgumentName().toString()).split(""String_Node_Str"");
  ApplicationId appId=cliConfig.getCurrentNamespace().app(programIdParts[0]);
  int instances;
switch (elementType) {
case FLOWLET:
    if (programIdParts.length < 3) {
      throw new CommandInputError(this);
    }
  String flowId=programIdParts[1];
String flowletName=programIdParts[2];
FlowletId flowlet=appId.flow(flowId).flowlet(flowletName);
instances=programClient.getFlowletInstances(flowlet.toId());
break;
case WORKER:
if (programIdParts.length < 2) {
throw new CommandInputError(this);
}
String workerId=programIdParts[1];
ProgramId worker=appId.worker(workerId);
instances=programClient.getWorkerInstances(Id.Worker.from(worker.getParent().toId(),workerId));
break;
case SERVICE:
if (programIdParts.length < 2) {
throw new CommandInputError(this);
}
String serviceName=programIdParts[1];
instances=programClient.getServiceInstances(appId.service(serviceName).toId());
break;
default :
throw new IllegalArgumentException(""String_Node_Str"" + elementType);
}
output.println(instances);
}","@Override @SuppressWarnings(""String_Node_Str"") public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] programIdParts=arguments.get(elementType.getArgumentName().toString()).split(""String_Node_Str"");
  ApplicationId appId=cliConfig.getCurrentNamespace().app(programIdParts[0]);
  int instances;
switch (elementType) {
case FLOWLET:
    if (programIdParts.length < 3) {
      throw new CommandInputError(this);
    }
  String flowId=programIdParts[1];
String flowletName=programIdParts[2];
FlowletId flowlet=appId.flow(flowId).flowlet(flowletName);
instances=programClient.getFlowletInstances(flowlet);
break;
case WORKER:
if (programIdParts.length < 2) {
throw new CommandInputError(this);
}
String workerId=programIdParts[1];
ProgramId worker=appId.worker(workerId);
instances=programClient.getWorkerInstances(worker);
break;
case SERVICE:
if (programIdParts.length < 2) {
throw new CommandInputError(this);
}
String serviceName=programIdParts[1];
instances=programClient.getServiceInstances(appId.service(serviceName));
break;
default :
throw new IllegalArgumentException(""String_Node_Str"" + elementType);
}
output.println(instances);
}"
5713,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] programIdParts=arguments.get(elementType.getArgumentName().toString()).split(""String_Node_Str"");
  if (programIdParts.length < 2) {
    throw new CommandInputError(this);
  }
  String appId=programIdParts[0];
  String programName=programIdParts[1];
  ProgramId program=cliConfig.getCurrentNamespace().app(appId).program(elementType.getProgramType(),programName);
  DistributedProgramLiveInfo liveInfo=programClient.getLiveInfo(program.toId());
  if (liveInfo == null) {
    output.println(""String_Node_Str"");
    return;
  }
  Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(ImmutableList.of(liveInfo),new RowMaker<DistributedProgramLiveInfo>(){
    @Override public List<?> makeRow(    DistributedProgramLiveInfo object){
      return Lists.newArrayList(object.getApp(),object.getType(),object.getName(),object.getRuntime(),object.getYarnAppId());
    }
  }
).build();
  cliConfig.getTableRenderer().render(cliConfig,output,table);
  if (liveInfo.getContainers() != null) {
    Table containersTable=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(liveInfo.getContainers(),new RowMaker<Containers.ContainerInfo>(){
      @Override public List<?> makeRow(      Containers.ContainerInfo object){
        return Lists.newArrayList(""String_Node_Str"",object.getInstance(),object.getHost(),object.getContainer(),object.getMemory(),object.getVirtualCores(),object.getDebugPort());
      }
    }
).build();
    cliConfig.getTableRenderer().render(cliConfig,output,containersTable);
  }
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] programIdParts=arguments.get(elementType.getArgumentName().toString()).split(""String_Node_Str"");
  if (programIdParts.length < 2) {
    throw new CommandInputError(this);
  }
  String appId=programIdParts[0];
  String programName=programIdParts[1];
  ProgramId program=cliConfig.getCurrentNamespace().app(appId).program(elementType.getProgramType(),programName);
  DistributedProgramLiveInfo liveInfo=programClient.getLiveInfo(program);
  if (liveInfo == null) {
    output.println(""String_Node_Str"");
    return;
  }
  Table table=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(ImmutableList.of(liveInfo),new RowMaker<DistributedProgramLiveInfo>(){
    @Override public List<?> makeRow(    DistributedProgramLiveInfo object){
      return Lists.newArrayList(object.getApp(),object.getType(),object.getName(),object.getRuntime(),object.getYarnAppId());
    }
  }
).build();
  cliConfig.getTableRenderer().render(cliConfig,output,table);
  if (liveInfo.getContainers() != null) {
    Table containersTable=Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(liveInfo.getContainers(),new RowMaker<Containers.ContainerInfo>(){
      @Override public List<?> makeRow(      Containers.ContainerInfo object){
        return Lists.newArrayList(""String_Node_Str"",object.getInstance(),object.getHost(),object.getContainer(),object.getMemory(),object.getVirtualCores(),object.getDebugPort());
      }
    }
).build();
    cliConfig.getTableRenderer().render(cliConfig,output,containersTable);
  }
}"
5714,"@Test public void testStringCaseTransform() throws Exception {
  String inputName=""String_Node_Str"";
  String outputName=""String_Node_Str"";
  ETLStage source=new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputName));
  ETLStage sink=new ETLStage(""String_Node_Str"",MockSink.getPlugin(outputName));
  Map<String,String> transformProperties=new HashMap<>();
  transformProperties.put(""String_Node_Str"",""String_Node_Str"");
  transformProperties.put(""String_Node_Str"",""String_Node_Str"");
  ETLStage transform=new ETLStage(""String_Node_Str"",new ETLPlugin(StringCaseTransform.NAME,Transform.PLUGIN_TYPE,transformProperties,null));
  ETLBatchConfig pipelineConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(source).addStage(sink).addStage(transform).addConnection(source.getName(),transform.getName()).addConnection(transform.getName(),sink.getName()).build();
  Id.Application pipelineId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(pipelineId,new AppRequest<>(APP_ARTIFACT,pipelineConfig));
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  DataSetManager<Table> inputManager=getDataset(inputName);
  List<StructuredRecord> inputRecords=new ArrayList<>();
  inputRecords.add(StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build());
  MockSource.writeInput(inputManager,inputRecords);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start();
  workflowManager.waitForFinish(4,TimeUnit.MINUTES);
  DataSetManager<Table> outputManager=getDataset(outputName);
  List<StructuredRecord> outputRecords=MockSink.readOutput(outputManager);
  List<StructuredRecord> expected=new ArrayList<>();
  expected.add(StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build());
  Assert.assertEquals(expected,outputRecords);
}","@Test public void testStringCaseTransform() throws Exception {
  String inputName=""String_Node_Str"";
  String outputName=""String_Node_Str"";
  ETLStage source=new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputName));
  ETLStage sink=new ETLStage(""String_Node_Str"",MockSink.getPlugin(outputName));
  Map<String,String> transformProperties=new HashMap<>();
  transformProperties.put(""String_Node_Str"",""String_Node_Str"");
  transformProperties.put(""String_Node_Str"",""String_Node_Str"");
  ETLStage transform=new ETLStage(""String_Node_Str"",new ETLPlugin(StringCaseTransform.NAME,Transform.PLUGIN_TYPE,transformProperties,null));
  ETLBatchConfig pipelineConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(source).addStage(sink).addStage(transform).addConnection(source.getName(),transform.getName()).addConnection(transform.getName(),sink.getName()).build();
  ApplicationId pipelineId=NamespaceId.DEFAULT.app(""String_Node_Str"");
  ApplicationManager appManager=deployApplication(pipelineId,new AppRequest<>(APP_ARTIFACT,pipelineConfig));
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  DataSetManager<Table> inputManager=getDataset(inputName);
  List<StructuredRecord> inputRecords=new ArrayList<>();
  inputRecords.add(StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build());
  MockSource.writeInput(inputManager,inputRecords);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start();
  workflowManager.waitForFinish(4,TimeUnit.MINUTES);
  DataSetManager<Table> outputManager=getDataset(outputName);
  List<StructuredRecord> outputRecords=MockSink.readOutput(outputManager);
  List<StructuredRecord> expected=new ArrayList<>();
  expected.add(StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build());
  Assert.assertEquals(expected,outputRecords);
}"
5715,"@Test public void testTextFileSource() throws Exception {
  String inputName=""String_Node_Str"";
  String outputName=""String_Node_Str"";
  Map<String,String> sourceProperties=new HashMap<>();
  sourceProperties.put(TextFileSetSource.Conf.FILESET_NAME,inputName);
  sourceProperties.put(TextFileSetSource.Conf.CREATE_IF_NOT_EXISTS,""String_Node_Str"");
  sourceProperties.put(TextFileSetSource.Conf.DELETE_INPUT_ON_SUCCESS,""String_Node_Str"");
  ETLStage source=new ETLStage(""String_Node_Str"",new ETLPlugin(TextFileSetSource.NAME,BatchSource.PLUGIN_TYPE,sourceProperties,null));
  ETLStage sink=new ETLStage(""String_Node_Str"",MockSink.getPlugin(outputName));
  ETLBatchConfig pipelineConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(source).addStage(sink).addConnection(source.getName(),sink.getName()).build();
  Id.Application pipelineId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(pipelineId,new AppRequest<>(APP_ARTIFACT,pipelineConfig));
  DataSetManager<FileSet> inputManager=getDataset(inputName);
  Location inputFile=inputManager.get().getBaseLocation().append(""String_Node_Str"");
  String line1=""String_Node_Str"";
  String line2=""String_Node_Str"";
  String line3=""String_Node_Str"";
  String inputText=line1 + ""String_Node_Str"" + line2+ ""String_Node_Str""+ line3;
  try (OutputStream outputStream=inputFile.getOutputStream()){
    outputStream.write(inputText.getBytes(Charset.forName(""String_Node_Str"")));
  }
   Map<String,String> runtimeArgs=new HashMap<>();
  runtimeArgs.put(String.format(""String_Node_Str"",inputName),inputFile.getName());
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start(runtimeArgs);
  workflowManager.waitForFinish(4,TimeUnit.MINUTES);
  DataSetManager<Table> outputManager=getDataset(outputName);
  Set<StructuredRecord> outputRecords=new HashSet<>();
  outputRecords.addAll(MockSink.readOutput(outputManager));
  Set<StructuredRecord> expected=new HashSet<>();
  expected.add(StructuredRecord.builder(TextFileSetSource.OUTPUT_SCHEMA).set(""String_Node_Str"",(long)inputText.indexOf(line1)).set(""String_Node_Str"",line1).build());
  expected.add(StructuredRecord.builder(TextFileSetSource.OUTPUT_SCHEMA).set(""String_Node_Str"",(long)inputText.indexOf(line2)).set(""String_Node_Str"",line2).build());
  expected.add(StructuredRecord.builder(TextFileSetSource.OUTPUT_SCHEMA).set(""String_Node_Str"",(long)inputText.indexOf(line3)).set(""String_Node_Str"",line3).build());
  Assert.assertEquals(expected,outputRecords);
  Assert.assertFalse(inputFile.exists());
}","@Test public void testTextFileSource() throws Exception {
  String inputName=""String_Node_Str"";
  String outputName=""String_Node_Str"";
  Map<String,String> sourceProperties=new HashMap<>();
  sourceProperties.put(TextFileSetSource.Conf.FILESET_NAME,inputName);
  sourceProperties.put(TextFileSetSource.Conf.CREATE_IF_NOT_EXISTS,""String_Node_Str"");
  sourceProperties.put(TextFileSetSource.Conf.DELETE_INPUT_ON_SUCCESS,""String_Node_Str"");
  ETLStage source=new ETLStage(""String_Node_Str"",new ETLPlugin(TextFileSetSource.NAME,BatchSource.PLUGIN_TYPE,sourceProperties,null));
  ETLStage sink=new ETLStage(""String_Node_Str"",MockSink.getPlugin(outputName));
  ETLBatchConfig pipelineConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(source).addStage(sink).addConnection(source.getName(),sink.getName()).build();
  ApplicationId pipelineId=NamespaceId.DEFAULT.app(""String_Node_Str"");
  ApplicationManager appManager=deployApplication(pipelineId,new AppRequest<>(APP_ARTIFACT,pipelineConfig));
  DataSetManager<FileSet> inputManager=getDataset(inputName);
  Location inputFile=inputManager.get().getBaseLocation().append(""String_Node_Str"");
  String line1=""String_Node_Str"";
  String line2=""String_Node_Str"";
  String line3=""String_Node_Str"";
  String inputText=line1 + ""String_Node_Str"" + line2+ ""String_Node_Str""+ line3;
  try (OutputStream outputStream=inputFile.getOutputStream()){
    outputStream.write(inputText.getBytes(Charset.forName(""String_Node_Str"")));
  }
   Map<String,String> runtimeArgs=new HashMap<>();
  runtimeArgs.put(String.format(""String_Node_Str"",inputName),inputFile.getName());
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start(runtimeArgs);
  workflowManager.waitForFinish(4,TimeUnit.MINUTES);
  DataSetManager<Table> outputManager=getDataset(outputName);
  Set<StructuredRecord> outputRecords=new HashSet<>();
  outputRecords.addAll(MockSink.readOutput(outputManager));
  Set<StructuredRecord> expected=new HashSet<>();
  expected.add(StructuredRecord.builder(TextFileSetSource.OUTPUT_SCHEMA).set(""String_Node_Str"",(long)inputText.indexOf(line1)).set(""String_Node_Str"",line1).build());
  expected.add(StructuredRecord.builder(TextFileSetSource.OUTPUT_SCHEMA).set(""String_Node_Str"",(long)inputText.indexOf(line2)).set(""String_Node_Str"",line2).build());
  expected.add(StructuredRecord.builder(TextFileSetSource.OUTPUT_SCHEMA).set(""String_Node_Str"",(long)inputText.indexOf(line3)).set(""String_Node_Str"",line3).build());
  Assert.assertEquals(expected,outputRecords);
  Assert.assertFalse(inputFile.exists());
}"
5716,"@SuppressWarnings(""String_Node_Str"") @Test public void testWordCountSparkSink() throws Exception {
  String inputName=""String_Node_Str"";
  String outputName=""String_Node_Str"";
  ETLStage source=new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputName));
  Map<String,String> sinkProperties=new HashMap<>();
  sinkProperties.put(""String_Node_Str"",""String_Node_Str"");
  sinkProperties.put(""String_Node_Str"",outputName);
  ETLStage sink=new ETLStage(""String_Node_Str"",new ETLPlugin(WordCountSink.NAME,SparkSink.PLUGIN_TYPE,sinkProperties,null));
  ETLBatchConfig pipelineConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(source).addStage(sink).addConnection(source.getName(),sink.getName()).build();
  Id.Application pipelineId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(pipelineId,new AppRequest<>(APP_ARTIFACT,pipelineConfig));
  Schema inputSchema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  DataSetManager<Table> inputManager=getDataset(inputName);
  List<StructuredRecord> inputRecords=new ArrayList<>();
  inputRecords.add(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").build());
  inputRecords.add(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").build());
  inputRecords.add(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").build());
  MockSource.writeInput(inputManager,inputRecords);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start();
  workflowManager.waitForFinish(4,TimeUnit.MINUTES);
  DataSetManager<KeyValueTable> outputManager=getDataset(outputName);
  KeyValueTable output=outputManager.get();
  Assert.assertEquals(3L,Bytes.toLong(output.read(""String_Node_Str"")));
  Assert.assertEquals(1L,Bytes.toLong(output.read(""String_Node_Str"")));
  Assert.assertEquals(2L,Bytes.toLong(output.read(""String_Node_Str"")));
  Assert.assertEquals(2L,Bytes.toLong(output.read(""String_Node_Str"")));
  Assert.assertEquals(2L,Bytes.toLong(output.read(""String_Node_Str"")));
  Assert.assertEquals(1L,Bytes.toLong(output.read(""String_Node_Str"")));
  Assert.assertEquals(1L,Bytes.toLong(output.read(""String_Node_Str"")));
}","@SuppressWarnings(""String_Node_Str"") @Test public void testWordCountSparkSink() throws Exception {
  String inputName=""String_Node_Str"";
  String outputName=""String_Node_Str"";
  ETLStage source=new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputName));
  Map<String,String> sinkProperties=new HashMap<>();
  sinkProperties.put(""String_Node_Str"",""String_Node_Str"");
  sinkProperties.put(""String_Node_Str"",outputName);
  ETLStage sink=new ETLStage(""String_Node_Str"",new ETLPlugin(WordCountSink.NAME,SparkSink.PLUGIN_TYPE,sinkProperties,null));
  ETLBatchConfig pipelineConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(source).addStage(sink).addConnection(source.getName(),sink.getName()).build();
  ApplicationId pipelineId=NamespaceId.DEFAULT.app(""String_Node_Str"");
  ApplicationManager appManager=deployApplication(pipelineId,new AppRequest<>(APP_ARTIFACT,pipelineConfig));
  Schema inputSchema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  DataSetManager<Table> inputManager=getDataset(inputName);
  List<StructuredRecord> inputRecords=new ArrayList<>();
  inputRecords.add(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").build());
  inputRecords.add(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").build());
  inputRecords.add(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").build());
  MockSource.writeInput(inputManager,inputRecords);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start();
  workflowManager.waitForFinish(4,TimeUnit.MINUTES);
  DataSetManager<KeyValueTable> outputManager=getDataset(outputName);
  KeyValueTable output=outputManager.get();
  Assert.assertEquals(3L,Bytes.toLong(output.read(""String_Node_Str"")));
  Assert.assertEquals(1L,Bytes.toLong(output.read(""String_Node_Str"")));
  Assert.assertEquals(2L,Bytes.toLong(output.read(""String_Node_Str"")));
  Assert.assertEquals(2L,Bytes.toLong(output.read(""String_Node_Str"")));
  Assert.assertEquals(2L,Bytes.toLong(output.read(""String_Node_Str"")));
  Assert.assertEquals(1L,Bytes.toLong(output.read(""String_Node_Str"")));
  Assert.assertEquals(1L,Bytes.toLong(output.read(""String_Node_Str"")));
}"
5717,"@Test public void testTextFileSink() throws Exception {
  String inputName=""String_Node_Str"";
  String outputName=""String_Node_Str"";
  ETLStage source=new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputName));
  Map<String,String> sinkProperties=new HashMap<>();
  sinkProperties.put(TextFileSetSink.Conf.FILESET_NAME,outputName);
  sinkProperties.put(TextFileSetSink.Conf.FIELD_SEPARATOR,""String_Node_Str"");
  ETLStage sink=new ETLStage(""String_Node_Str"",new ETLPlugin(TextFileSetSink.NAME,BatchSink.PLUGIN_TYPE,sinkProperties,null));
  ETLBatchConfig pipelineConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(source).addStage(sink).addConnection(source.getName(),sink.getName()).build();
  Id.Application pipelineId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(pipelineId,new AppRequest<>(APP_ARTIFACT,pipelineConfig));
  Schema inputSchema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  Map<String,String> users=new HashMap<>();
  users.put(""String_Node_Str"",""String_Node_Str"");
  users.put(""String_Node_Str"",""String_Node_Str"");
  users.put(""String_Node_Str"",""String_Node_Str"");
  List<StructuredRecord> inputRecords=new ArrayList<>();
  for (  Map.Entry<String,String> userEntry : users.entrySet()) {
    String name=userEntry.getKey();
    String item=userEntry.getValue();
    inputRecords.add(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",name).set(""String_Node_Str"",item).build());
  }
  DataSetManager<Table> inputManager=getDataset(inputName);
  MockSource.writeInput(inputManager,inputRecords);
  Map<String,String> runtimeArgs=new HashMap<>();
  String outputPath=""String_Node_Str"";
  runtimeArgs.put(String.format(""String_Node_Str"",outputName),outputPath);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start(runtimeArgs);
  workflowManager.waitForFinish(4,TimeUnit.MINUTES);
  DataSetManager<FileSet> outputManager=getDataset(outputName);
  FileSet output=outputManager.get();
  Location outputDir=output.getBaseLocation().append(outputPath);
  Map<String,String> actual=new HashMap<>();
  for (  Location outputFile : outputDir.list()) {
    if (outputFile.getName().endsWith(""String_Node_Str"") || ""String_Node_Str"".equals(outputFile.getName())) {
      continue;
    }
    try (BufferedReader reader=new BufferedReader(new InputStreamReader(outputFile.getInputStream()))){
      String line;
      while ((line=reader.readLine()) != null) {
        String[] parts=line.split(""String_Node_Str"");
        actual.put(parts[0],parts[1]);
      }
    }
   }
  Assert.assertEquals(actual,users);
}","@Test public void testTextFileSink() throws Exception {
  String inputName=""String_Node_Str"";
  String outputName=""String_Node_Str"";
  ETLStage source=new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputName));
  Map<String,String> sinkProperties=new HashMap<>();
  sinkProperties.put(TextFileSetSink.Conf.FILESET_NAME,outputName);
  sinkProperties.put(TextFileSetSink.Conf.FIELD_SEPARATOR,""String_Node_Str"");
  ETLStage sink=new ETLStage(""String_Node_Str"",new ETLPlugin(TextFileSetSink.NAME,BatchSink.PLUGIN_TYPE,sinkProperties,null));
  ETLBatchConfig pipelineConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(source).addStage(sink).addConnection(source.getName(),sink.getName()).build();
  ApplicationId pipelineId=NamespaceId.DEFAULT.app(""String_Node_Str"");
  ApplicationManager appManager=deployApplication(pipelineId,new AppRequest<>(APP_ARTIFACT,pipelineConfig));
  Schema inputSchema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  Map<String,String> users=new HashMap<>();
  users.put(""String_Node_Str"",""String_Node_Str"");
  users.put(""String_Node_Str"",""String_Node_Str"");
  users.put(""String_Node_Str"",""String_Node_Str"");
  List<StructuredRecord> inputRecords=new ArrayList<>();
  for (  Map.Entry<String,String> userEntry : users.entrySet()) {
    String name=userEntry.getKey();
    String item=userEntry.getValue();
    inputRecords.add(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",name).set(""String_Node_Str"",item).build());
  }
  DataSetManager<Table> inputManager=getDataset(inputName);
  MockSource.writeInput(inputManager,inputRecords);
  Map<String,String> runtimeArgs=new HashMap<>();
  String outputPath=""String_Node_Str"";
  runtimeArgs.put(String.format(""String_Node_Str"",outputName),outputPath);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start(runtimeArgs);
  workflowManager.waitForFinish(4,TimeUnit.MINUTES);
  DataSetManager<FileSet> outputManager=getDataset(outputName);
  FileSet output=outputManager.get();
  Location outputDir=output.getBaseLocation().append(outputPath);
  Map<String,String> actual=new HashMap<>();
  for (  Location outputFile : outputDir.list()) {
    if (outputFile.getName().endsWith(""String_Node_Str"") || ""String_Node_Str"".equals(outputFile.getName())) {
      continue;
    }
    try (BufferedReader reader=new BufferedReader(new InputStreamReader(outputFile.getInputStream()))){
      String line;
      while ((line=reader.readLine()) != null) {
        String[] parts=line.split(""String_Node_Str"");
        actual.put(parts[0],parts[1]);
      }
    }
   }
  Assert.assertEquals(actual,users);
}"
5718,"public void testWordCount(String pluginType) throws Exception {
  String inputName=""String_Node_Str"" + pluginType;
  String outputName=""String_Node_Str"" + pluginType;
  ETLStage source=new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputName));
  ETLStage sink=new ETLStage(""String_Node_Str"",MockSink.getPlugin(outputName));
  Map<String,String> aggProperties=new HashMap<>();
  aggProperties.put(""String_Node_Str"",""String_Node_Str"");
  ETLStage agg=new ETLStage(""String_Node_Str"",new ETLPlugin(""String_Node_Str"",pluginType,aggProperties,null));
  ETLBatchConfig pipelineConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(source).addStage(sink).addStage(agg).addConnection(source.getName(),agg.getName()).addConnection(agg.getName(),sink.getName()).build();
  Id.Application pipelineId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"" + pluginType);
  ApplicationManager appManager=deployApplication(pipelineId,new AppRequest<>(APP_ARTIFACT,pipelineConfig));
  Schema inputSchema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  DataSetManager<Table> inputManager=getDataset(inputName);
  List<StructuredRecord> inputRecords=new ArrayList<>();
  inputRecords.add(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").build());
  inputRecords.add(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").build());
  inputRecords.add(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").build());
  MockSource.writeInput(inputManager,inputRecords);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start();
  workflowManager.waitForFinish(4,TimeUnit.MINUTES);
  DataSetManager<Table> outputManager=getDataset(outputName);
  Set<StructuredRecord> outputRecords=new HashSet<>();
  outputRecords.addAll(MockSink.readOutput(outputManager));
  Set<StructuredRecord> expected=new HashSet<>();
  expected.add(StructuredRecord.builder(WordCountAggregator.OUTPUT_SCHEMA).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",3L).build());
  expected.add(StructuredRecord.builder(WordCountAggregator.OUTPUT_SCHEMA).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build());
  expected.add(StructuredRecord.builder(WordCountAggregator.OUTPUT_SCHEMA).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build());
  expected.add(StructuredRecord.builder(WordCountAggregator.OUTPUT_SCHEMA).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build());
  expected.add(StructuredRecord.builder(WordCountAggregator.OUTPUT_SCHEMA).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build());
  expected.add(StructuredRecord.builder(WordCountAggregator.OUTPUT_SCHEMA).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build());
  expected.add(StructuredRecord.builder(WordCountAggregator.OUTPUT_SCHEMA).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build());
  Assert.assertEquals(expected,outputRecords);
}","public void testWordCount(String pluginType) throws Exception {
  String inputName=""String_Node_Str"" + pluginType;
  String outputName=""String_Node_Str"" + pluginType;
  ETLStage source=new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputName));
  ETLStage sink=new ETLStage(""String_Node_Str"",MockSink.getPlugin(outputName));
  Map<String,String> aggProperties=new HashMap<>();
  aggProperties.put(""String_Node_Str"",""String_Node_Str"");
  ETLStage agg=new ETLStage(""String_Node_Str"",new ETLPlugin(""String_Node_Str"",pluginType,aggProperties,null));
  ETLBatchConfig pipelineConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(source).addStage(sink).addStage(agg).addConnection(source.getName(),agg.getName()).addConnection(agg.getName(),sink.getName()).build();
  ApplicationId pipelineId=NamespaceId.DEFAULT.app(""String_Node_Str"" + pluginType);
  ApplicationManager appManager=deployApplication(pipelineId,new AppRequest<>(APP_ARTIFACT,pipelineConfig));
  Schema inputSchema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  DataSetManager<Table> inputManager=getDataset(inputName);
  List<StructuredRecord> inputRecords=new ArrayList<>();
  inputRecords.add(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").build());
  inputRecords.add(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").build());
  inputRecords.add(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").build());
  MockSource.writeInput(inputManager,inputRecords);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start();
  workflowManager.waitForFinish(4,TimeUnit.MINUTES);
  DataSetManager<Table> outputManager=getDataset(outputName);
  Set<StructuredRecord> outputRecords=new HashSet<>();
  outputRecords.addAll(MockSink.readOutput(outputManager));
  Set<StructuredRecord> expected=new HashSet<>();
  expected.add(StructuredRecord.builder(WordCountAggregator.OUTPUT_SCHEMA).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",3L).build());
  expected.add(StructuredRecord.builder(WordCountAggregator.OUTPUT_SCHEMA).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build());
  expected.add(StructuredRecord.builder(WordCountAggregator.OUTPUT_SCHEMA).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build());
  expected.add(StructuredRecord.builder(WordCountAggregator.OUTPUT_SCHEMA).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build());
  expected.add(StructuredRecord.builder(WordCountAggregator.OUTPUT_SCHEMA).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build());
  expected.add(StructuredRecord.builder(WordCountAggregator.OUTPUT_SCHEMA).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build());
  expected.add(StructuredRecord.builder(WordCountAggregator.OUTPUT_SCHEMA).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build());
  Assert.assertEquals(expected,outputRecords);
}"
5719,"/** 
 * Returns a Set of resources name that are visible through the cdap-api module as well as Hadoop classes. This includes all classes+resources in cdap-api plus all classes+resources that cdap-api depends on (for example, sl4j, guava, gson, etc).
 */
private static Set<String> createBaseResources() throws IOException {
  ClassLoader classLoader=ProgramResources.class.getClassLoader();
  Set<String> result=ClassPathResources.getResourcesWithDependencies(classLoader,Application.class);
  Iterables.addAll(result,Iterables.transform(ClassPathResources.getClassPathResources(classLoader,Path.class),ClassPathResources.RESOURCE_INFO_TO_RESOURCE_NAME));
  getResources(ClassPath.from(classLoader,JAR_ONLY_URI),HADOOP_PACKAGES,HBASE_PACKAGES,ClassPathResources.RESOURCE_INFO_TO_RESOURCE_NAME,result);
  return Collections.unmodifiableSet(result);
}","/** 
 * Returns a Set of resources name that are visible through the cdap-api module as well as Hadoop classes. This includes all classes+resources in cdap-api plus all classes+resources that cdap-api depends on (for example, sl4j, gson, etc).
 */
private static Set<String> createBaseResources() throws IOException {
  ClassLoader classLoader=ProgramResources.class.getClassLoader();
  Set<String> result=ClassPathResources.getResourcesWithDependencies(classLoader,Application.class);
  Iterables.addAll(result,Iterables.transform(ClassPathResources.getClassPathResources(classLoader,Path.class),ClassPathResources.RESOURCE_INFO_TO_RESOURCE_NAME));
  getResources(ClassPath.from(classLoader,JAR_ONLY_URI),HADOOP_PACKAGES,EXCLUDE_PACKAGES,ClassPathResources.RESOURCE_INFO_TO_RESOURCE_NAME,result);
  return Collections.unmodifiableSet(result);
}"
5720,"private Set<ArtifactRange> parseExtendsHeader(NamespaceId namespace,String extendsHeader) throws BadRequestException {
  Set<ArtifactRange> parentArtifacts=Sets.newHashSet();
  if (extendsHeader != null) {
    for (    String parent : Splitter.on('/').split(extendsHeader)) {
      parent=parent.trim();
      ArtifactRange range;
      try {
        range=ArtifactRange.parse(parent);
        if (!range.getNamespace().equals(Id.Namespace.SYSTEM) && !range.getNamespace().equals(namespace.toId())) {
          throw new BadRequestException(String.format(""String_Node_Str"",parent));
        }
      }
 catch (      InvalidArtifactRangeException e) {
        try {
          range=ArtifactRange.parse(namespace.toId(),parent);
        }
 catch (        InvalidArtifactRangeException e1) {
          throw new BadRequestException(String.format(""String_Node_Str"",parent,e1.getMessage()));
        }
      }
      parentArtifacts.add(range);
    }
  }
  return parentArtifacts;
}","private Set<ArtifactRange> parseExtendsHeader(NamespaceId namespace,String extendsHeader) throws BadRequestException {
  Set<ArtifactRange> parentArtifacts=Sets.newHashSet();
  if (extendsHeader != null) {
    for (    String parent : Splitter.on('/').split(extendsHeader)) {
      parent=parent.trim();
      ArtifactRange range;
      try {
        range=ArtifactRange.parse(parent);
        if (!NamespaceId.SYSTEM.equals(range.getNamespace()) && !namespace.equals(range.getNamespace())) {
          throw new BadRequestException(String.format(""String_Node_Str"",parent));
        }
      }
 catch (      InvalidArtifactRangeException e) {
        try {
          range=ArtifactRange.parse(namespace,parent);
        }
 catch (        InvalidArtifactRangeException e1) {
          throw new BadRequestException(String.format(""String_Node_Str"",parent,e1.getMessage()));
        }
      }
      parentArtifacts.add(range);
    }
  }
  return parentArtifacts;
}"
5721,"/** 
 * Get all artifacts that match artifacts in the given ranges.
 * @param range the range to match artifacts in
 * @return an unmodifiable list of all artifacts that match the given ranges. If none exist, an empty list is returned
 */
public List<ArtifactDetail> getArtifacts(final ArtifactRange range) throws Exception {
  List<ArtifactDetail> artifacts=artifactStore.getArtifacts(range);
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter=authorizationEnforcer.createFilter(principal);
  return Lists.newArrayList(Iterables.filter(artifacts,new com.google.common.base.Predicate<ArtifactDetail>(){
    @Override public boolean apply(    ArtifactDetail artifactDetail){
      ArtifactId artifactId=artifactDetail.getDescriptor().getArtifactId();
      return filter.apply(range.getNamespace().toEntityId().artifact(artifactId.getName(),artifactId.getVersion().getVersion()));
    }
  }
));
}","/** 
 * Get all artifacts that match artifacts in the given ranges.
 * @param range the range to match artifacts in
 * @return an unmodifiable list of all artifacts that match the given ranges. If none exist, an empty list is returned
 */
public List<ArtifactDetail> getArtifacts(final ArtifactRange range) throws Exception {
  List<ArtifactDetail> artifacts=artifactStore.getArtifacts(range);
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter=authorizationEnforcer.createFilter(principal);
  return Lists.newArrayList(Iterables.filter(artifacts,new com.google.common.base.Predicate<ArtifactDetail>(){
    @Override public boolean apply(    ArtifactDetail artifactDetail){
      ArtifactId artifactId=artifactDetail.getDescriptor().getArtifactId();
      return filter.apply(range.getNamespace().artifact(artifactId.getName(),artifactId.getVersion().getVersion()));
    }
  }
));
}"
5722,"/** 
 * Validates the parents of an artifact. Checks that each artifact only appears with a single version range.
 * @param parents the set of parent ranges to validate
 * @throws InvalidArtifactException if there is more than one version range for an artifact
 */
@VisibleForTesting static void validateParentSet(Id.Artifact artifactId,Set<ArtifactRange> parents) throws InvalidArtifactException {
  boolean isInvalid=false;
  StringBuilder errMsg=new StringBuilder(""String_Node_Str"");
  Set<String> parentNames=new HashSet<>();
  Set<String> dupes=new HashSet<>();
  for (  ArtifactRange parent : parents) {
    String parentName=parent.getName();
    if (!parentNames.add(parentName) && !dupes.contains(parentName)) {
      errMsg.append(""String_Node_Str"");
      errMsg.append(parentName);
      errMsg.append(""String_Node_Str"");
      dupes.add(parentName);
      isInvalid=true;
    }
    if (artifactId.getName().equals(parentName) && artifactId.getNamespace().equals(parent.getNamespace())) {
      throw new InvalidArtifactException(String.format(""String_Node_Str"",parent,artifactId));
    }
  }
  if (isInvalid) {
    throw new InvalidArtifactException(errMsg.toString());
  }
}","/** 
 * Validates the parents of an artifact. Checks that each artifact only appears with a single version range.
 * @param parents the set of parent ranges to validate
 * @throws InvalidArtifactException if there is more than one version range for an artifact
 */
@VisibleForTesting static void validateParentSet(Id.Artifact artifactId,Set<ArtifactRange> parents) throws InvalidArtifactException {
  boolean isInvalid=false;
  StringBuilder errMsg=new StringBuilder(""String_Node_Str"");
  Set<String> parentNames=new HashSet<>();
  Set<String> dupes=new HashSet<>();
  for (  ArtifactRange parent : parents) {
    String parentName=parent.getName();
    if (!parentNames.add(parentName) && !dupes.contains(parentName)) {
      errMsg.append(""String_Node_Str"");
      errMsg.append(parentName);
      errMsg.append(""String_Node_Str"");
      dupes.add(parentName);
      isInvalid=true;
    }
    if (artifactId.getName().equals(parentName) && artifactId.getNamespace().toEntityId().equals(parent.getNamespace())) {
      throw new InvalidArtifactException(String.format(""String_Node_Str"",parent,artifactId));
    }
  }
  if (isInvalid) {
    throw new InvalidArtifactException(errMsg.toString());
  }
}"
5723,"private void writeMeta(Table table,Id.Artifact artifactId,ArtifactData data) throws IOException {
  ArtifactCell artifactCell=new ArtifactCell(artifactId);
  table.put(artifactCell.rowkey,artifactCell.column,Bytes.toBytes(GSON.toJson(data)));
  byte[] artifactColumn=new ArtifactColumn(artifactId).getColumn();
  ArtifactClasses classes=data.meta.getClasses();
  Location artifactLocation=locationFactory.create(data.locationURI);
  for (  PluginClass pluginClass : classes.getPlugins()) {
    for (    ArtifactRange artifactRange : data.meta.getUsableBy()) {
      PluginKey pluginKey=new PluginKey(artifactRange.getNamespace(),artifactRange.getName(),pluginClass.getType(),pluginClass.getName());
      byte[] pluginDataBytes=Bytes.toBytes(GSON.toJson(new PluginData(pluginClass,artifactRange,artifactLocation)));
      table.put(pluginKey.getRowKey(),artifactColumn,pluginDataBytes);
    }
  }
  for (  ApplicationClass appClass : classes.getApps()) {
    AppClassKey appClassKey=new AppClassKey(artifactId.getNamespace().toEntityId(),appClass.getClassName());
    byte[] appDataBytes=Bytes.toBytes(GSON.toJson(new AppData(appClass,artifactLocation)));
    table.put(appClassKey.getRowKey(),artifactColumn,appDataBytes);
  }
}","private void writeMeta(Table table,Id.Artifact artifactId,ArtifactData data) throws IOException {
  ArtifactCell artifactCell=new ArtifactCell(artifactId);
  table.put(artifactCell.rowkey,artifactCell.column,Bytes.toBytes(GSON.toJson(data)));
  byte[] artifactColumn=new ArtifactColumn(artifactId).getColumn();
  ArtifactClasses classes=data.meta.getClasses();
  Location artifactLocation=locationFactory.create(data.locationURI);
  for (  PluginClass pluginClass : classes.getPlugins()) {
    for (    ArtifactRange artifactRange : data.meta.getUsableBy()) {
      PluginKey pluginKey=new PluginKey(artifactRange.getNamespace().toId(),artifactRange.getName(),pluginClass.getType(),pluginClass.getName());
      byte[] pluginDataBytes=Bytes.toBytes(GSON.toJson(new PluginData(pluginClass,artifactRange,artifactLocation)));
      table.put(pluginKey.getRowKey(),artifactColumn,pluginDataBytes);
    }
  }
  for (  ApplicationClass appClass : classes.getApps()) {
    AppClassKey appClassKey=new AppClassKey(artifactId.getNamespace().toEntityId(),appClass.getClassName());
    byte[] appDataBytes=Bytes.toBytes(GSON.toJson(new AppData(appClass,artifactLocation)));
    table.put(appClassKey.getRowKey(),artifactColumn,appDataBytes);
  }
}"
5724,"private void deleteMeta(Table table,Id.Artifact artifactId,byte[] oldData) throws IOException {
  ArtifactCell artifactCell=new ArtifactCell(artifactId);
  table.delete(artifactCell.rowkey,artifactCell.column);
  final ArtifactData oldMeta=GSON.fromJson(Bytes.toString(oldData),ArtifactData.class);
  byte[] artifactColumn=new ArtifactColumn(artifactId).getColumn();
  for (  PluginClass pluginClass : oldMeta.meta.getClasses().getPlugins()) {
    for (    ArtifactRange artifactRange : oldMeta.meta.getUsableBy()) {
      PluginKey pluginKey=new PluginKey(artifactRange.getNamespace(),artifactRange.getName(),pluginClass.getType(),pluginClass.getName());
      table.delete(pluginKey.getRowKey(),artifactColumn);
    }
  }
  for (  ApplicationClass appClass : oldMeta.meta.getClasses().getApps()) {
    AppClassKey appClassKey=new AppClassKey(artifactId.getNamespace().toEntityId(),appClass.getClassName());
    table.delete(appClassKey.getRowKey(),artifactColumn);
  }
  try {
    new NamespacedImpersonator(artifactId.getNamespace().toEntityId(),impersonator).impersonate(new Callable<Void>(){
      @Override public Void call() throws Exception {
        locationFactory.create(oldMeta.locationURI).delete();
        return null;
      }
    }
);
  }
 catch (  IOException ioe) {
    throw ioe;
  }
catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","private void deleteMeta(Table table,Id.Artifact artifactId,byte[] oldData) throws IOException {
  ArtifactCell artifactCell=new ArtifactCell(artifactId);
  table.delete(artifactCell.rowkey,artifactCell.column);
  final ArtifactData oldMeta=GSON.fromJson(Bytes.toString(oldData),ArtifactData.class);
  byte[] artifactColumn=new ArtifactColumn(artifactId).getColumn();
  for (  PluginClass pluginClass : oldMeta.meta.getClasses().getPlugins()) {
    for (    ArtifactRange artifactRange : oldMeta.meta.getUsableBy()) {
      PluginKey pluginKey=new PluginKey(artifactRange.getNamespace().toId(),artifactRange.getName(),pluginClass.getType(),pluginClass.getName());
      table.delete(pluginKey.getRowKey(),artifactColumn);
    }
  }
  for (  ApplicationClass appClass : oldMeta.meta.getClasses().getApps()) {
    AppClassKey appClassKey=new AppClassKey(artifactId.getNamespace().toEntityId(),appClass.getClassName());
    table.delete(appClassKey.getRowKey(),artifactColumn);
  }
  try {
    new NamespacedImpersonator(artifactId.getNamespace().toEntityId(),impersonator).impersonate(new Callable<Void>(){
      @Override public Void call() throws Exception {
        locationFactory.create(oldMeta.locationURI).delete();
        return null;
      }
    }
);
  }
 catch (  IOException ioe) {
    throw ioe;
  }
catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
5725,"@Test public void testVersionParse() throws InvalidArtifactRangeException {
  ArtifactRange expected=new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),true,new ArtifactVersion(""String_Node_Str""),false);
  ArtifactRange actual=ArtifactRange.parse(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expected,actual);
  expected=new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),false,new ArtifactVersion(""String_Node_Str""),true);
  actual=ArtifactRange.parse(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expected,actual);
  Assert.assertEquals(expected,ArtifactRange.parse(expected.toString()));
}","@Test public void testVersionParse() throws InvalidArtifactRangeException {
  ArtifactRange expected=new ArtifactRange(NamespaceId.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),true,new ArtifactVersion(""String_Node_Str""),false);
  ArtifactRange actual=ArtifactRange.parse(NamespaceId.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expected,actual);
  expected=new ArtifactRange(NamespaceId.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),false,new ArtifactVersion(""String_Node_Str""),true);
  actual=ArtifactRange.parse(NamespaceId.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expected,actual);
  Assert.assertEquals(expected,ArtifactRange.parse(expected.toString()));
}"
5726,"@Test public void testLowerVersionGreaterThanUpper(){
  List<String> invalidRanges=Lists.newArrayList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  for (  String invalidRange : invalidRanges) {
    try {
      ArtifactRange.parse(Id.Namespace.DEFAULT,invalidRange);
      Assert.fail();
    }
 catch (    InvalidArtifactRangeException e) {
    }
    try {
      ArtifactRange.parse(""String_Node_Str"" + invalidRange);
      Assert.fail();
    }
 catch (    InvalidArtifactRangeException e) {
    }
  }
}","@Test public void testLowerVersionGreaterThanUpper(){
  List<String> invalidRanges=Lists.newArrayList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  for (  String invalidRange : invalidRanges) {
    try {
      ArtifactRange.parse(NamespaceId.DEFAULT,invalidRange);
      Assert.fail();
    }
 catch (    InvalidArtifactRangeException e) {
    }
    try {
      ArtifactRange.parse(""String_Node_Str"" + invalidRange);
      Assert.fail();
    }
 catch (    InvalidArtifactRangeException e) {
    }
  }
}"
5727,"@Test public void testParseInvalid(){
  List<String> invalidRanges=Lists.newArrayList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  for (  String invalidRange : invalidRanges) {
    try {
      ArtifactRange.parse(Id.Namespace.DEFAULT,invalidRange);
      Assert.fail();
    }
 catch (    InvalidArtifactRangeException e) {
    }
    try {
      ArtifactRange.parse(""String_Node_Str"" + invalidRange);
      Assert.fail();
    }
 catch (    InvalidArtifactRangeException e) {
    }
  }
}","@Test public void testParseInvalid(){
  List<String> invalidRanges=Lists.newArrayList(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  for (  String invalidRange : invalidRanges) {
    try {
      ArtifactRange.parse(NamespaceId.DEFAULT,invalidRange);
      Assert.fail();
    }
 catch (    InvalidArtifactRangeException e) {
    }
    try {
      ArtifactRange.parse(""String_Node_Str"" + invalidRange);
      Assert.fail();
    }
 catch (    InvalidArtifactRangeException e) {
    }
  }
}"
5728,"@Test public void testIsInRange(){
  ArtifactRange range=new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertTrue(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertTrue(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertTrue(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertTrue(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertTrue(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertTrue(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertTrue(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
}","@Test public void testIsInRange(){
  ArtifactRange range=new ArtifactRange(NamespaceId.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertTrue(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertTrue(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertTrue(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertTrue(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertTrue(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertTrue(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertTrue(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
  Assert.assertFalse(range.versionIsInRange(new ArtifactVersion(""String_Node_Str"")));
}"
5729,"@Test public void testWhitespace() throws InvalidArtifactRangeException {
  ArtifactRange range=ArtifactRange.parse(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),true,new ArtifactVersion(""String_Node_Str""),false),range);
}","@Test public void testWhitespace() throws InvalidArtifactRangeException {
  ArtifactRange range=ArtifactRange.parse(NamespaceId.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(new ArtifactRange(NamespaceId.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),true,new ArtifactVersion(""String_Node_Str""),false),range);
}"
5730,"@Test(expected=InvalidArtifactException.class) public void testSelfExtendingArtifact() throws InvalidArtifactException {
  Id.Artifact child=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  ArtifactRepository.validateParentSet(child,ImmutableSet.of(new ArtifactRange(Id.Namespace.SYSTEM,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""))));
}","@Test(expected=InvalidArtifactException.class) public void testSelfExtendingArtifact() throws InvalidArtifactException {
  Id.Artifact child=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  ArtifactRepository.validateParentSet(child,ImmutableSet.of(new ArtifactRange(NamespaceId.SYSTEM,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""))));
}"
5731,"@Test public void testPluginSelector() throws Exception {
  try {
    artifactRepository.findPlugin(NamespaceId.DEFAULT,APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector());
    Assert.fail();
  }
 catch (  PluginNotExistsException e) {
  }
  File pluginDir=DirUtils.createTempDir(tmpDir);
  Id.Artifact artifact1Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File jarFile=createPluginJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(APP_ARTIFACT_ID.getNamespace(),APP_ARTIFACT_ID.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  artifactRepository.addArtifact(artifact1Id,jarFile,parents);
  Map.Entry<ArtifactDescriptor,PluginClass> plugin=artifactRepository.findPlugin(NamespaceId.DEFAULT,APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector());
  Assert.assertNotNull(plugin);
  Assert.assertEquals(new ArtifactVersion(""String_Node_Str""),plugin.getKey().getArtifactId().getVersion());
  Assert.assertEquals(""String_Node_Str"",plugin.getValue().getName());
  Files.copy(Locations.newInputSupplier(plugin.getKey().getLocation()),new File(pluginDir,Artifacts.getFileName(plugin.getKey().getArtifactId())));
  Id.Artifact artifact2Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  jarFile=createPluginJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
  artifactRepository.addArtifact(artifact2Id,jarFile,parents);
  plugin=artifactRepository.findPlugin(NamespaceId.DEFAULT,APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector());
  Assert.assertNotNull(plugin);
  Assert.assertEquals(new ArtifactVersion(""String_Node_Str""),plugin.getKey().getArtifactId().getVersion());
  Assert.assertEquals(""String_Node_Str"",plugin.getValue().getName());
  Files.copy(Locations.newInputSupplier(plugin.getKey().getLocation()),new File(pluginDir,Artifacts.getFileName(plugin.getKey().getArtifactId())));
  try (PluginInstantiator instantiator=new PluginInstantiator(cConf,appClassLoader,pluginDir)){
    ClassLoader pluginClassLoader=instantiator.getArtifactClassLoader(plugin.getKey().getArtifactId());
    Class<?> pluginClass=pluginClassLoader.loadClass(TestPlugin2.class.getName());
    plugin=artifactRepository.findPlugin(NamespaceId.DEFAULT,APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector(){
      @Override public Map.Entry<ArtifactId,PluginClass> select(      SortedMap<ArtifactId,PluginClass> plugins){
        return plugins.entrySet().iterator().next();
      }
    }
);
    Assert.assertNotNull(plugin);
    Assert.assertEquals(new ArtifactVersion(""String_Node_Str""),plugin.getKey().getArtifactId().getVersion());
    Assert.assertEquals(""String_Node_Str"",plugin.getValue().getName());
    pluginClassLoader=instantiator.getArtifactClassLoader(plugin.getKey().getArtifactId());
    Assert.assertNotSame(pluginClass,pluginClassLoader.loadClass(TestPlugin2.class.getName()));
    Class<?> cls=pluginClassLoader.loadClass(PluginTestRunnable.class.getName());
    Assert.assertSame(appClassLoader.loadClass(PluginTestRunnable.class.getName()),cls);
    cls=pluginClassLoader.loadClass(Application.class.getName());
    Assert.assertSame(Application.class,cls);
  }
 }","@Test public void testPluginSelector() throws Exception {
  try {
    artifactRepository.findPlugin(NamespaceId.DEFAULT,APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector());
    Assert.fail();
  }
 catch (  PluginNotExistsException e) {
  }
  File pluginDir=DirUtils.createTempDir(tmpDir);
  Id.Artifact artifact1Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File jarFile=createPluginJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(APP_ARTIFACT_ID.getNamespace().toEntityId(),APP_ARTIFACT_ID.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  artifactRepository.addArtifact(artifact1Id,jarFile,parents);
  Map.Entry<ArtifactDescriptor,PluginClass> plugin=artifactRepository.findPlugin(NamespaceId.DEFAULT,APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector());
  Assert.assertNotNull(plugin);
  Assert.assertEquals(new ArtifactVersion(""String_Node_Str""),plugin.getKey().getArtifactId().getVersion());
  Assert.assertEquals(""String_Node_Str"",plugin.getValue().getName());
  Files.copy(Locations.newInputSupplier(plugin.getKey().getLocation()),new File(pluginDir,Artifacts.getFileName(plugin.getKey().getArtifactId())));
  Id.Artifact artifact2Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  jarFile=createPluginJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
  artifactRepository.addArtifact(artifact2Id,jarFile,parents);
  plugin=artifactRepository.findPlugin(NamespaceId.DEFAULT,APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector());
  Assert.assertNotNull(plugin);
  Assert.assertEquals(new ArtifactVersion(""String_Node_Str""),plugin.getKey().getArtifactId().getVersion());
  Assert.assertEquals(""String_Node_Str"",plugin.getValue().getName());
  Files.copy(Locations.newInputSupplier(plugin.getKey().getLocation()),new File(pluginDir,Artifacts.getFileName(plugin.getKey().getArtifactId())));
  try (PluginInstantiator instantiator=new PluginInstantiator(cConf,appClassLoader,pluginDir)){
    ClassLoader pluginClassLoader=instantiator.getArtifactClassLoader(plugin.getKey().getArtifactId());
    Class<?> pluginClass=pluginClassLoader.loadClass(TestPlugin2.class.getName());
    plugin=artifactRepository.findPlugin(NamespaceId.DEFAULT,APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector(){
      @Override public Map.Entry<ArtifactId,PluginClass> select(      SortedMap<ArtifactId,PluginClass> plugins){
        return plugins.entrySet().iterator().next();
      }
    }
);
    Assert.assertNotNull(plugin);
    Assert.assertEquals(new ArtifactVersion(""String_Node_Str""),plugin.getKey().getArtifactId().getVersion());
    Assert.assertEquals(""String_Node_Str"",plugin.getValue().getName());
    pluginClassLoader=instantiator.getArtifactClassLoader(plugin.getKey().getArtifactId());
    Assert.assertNotSame(pluginClass,pluginClassLoader.loadClass(TestPlugin2.class.getName()));
    Class<?> cls=pluginClassLoader.loadClass(PluginTestRunnable.class.getName());
    Assert.assertSame(appClassLoader.loadClass(PluginTestRunnable.class.getName()),cls);
    cls=pluginClassLoader.loadClass(Application.class.getName());
    Assert.assertSame(Application.class,cls);
  }
 }"
5732,"@Test(expected=InvalidArtifactException.class) public void testGrandparentsAreInvalid() throws Exception {
  Id.Artifact childId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,Plugin1.class.getPackage().getName());
  File jarFile=createPluginJar(Plugin1.class,new File(tmpDir,""String_Node_Str""),manifest);
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(APP_ARTIFACT_ID.getNamespace(),APP_ARTIFACT_ID.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  artifactRepository.addArtifact(childId,jarFile,parents);
  Id.Artifact grandchildId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  manifest=createManifest(ManifestFields.EXPORT_PACKAGE,Plugin2.class.getPackage().getName());
  jarFile=createPluginJar(Plugin2.class,new File(tmpDir,""String_Node_Str""),manifest);
  parents=ImmutableSet.of(new ArtifactRange(childId.getNamespace(),childId.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  artifactRepository.addArtifact(grandchildId,jarFile,parents);
}","@Test(expected=InvalidArtifactException.class) public void testGrandparentsAreInvalid() throws Exception {
  Id.Artifact childId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,Plugin1.class.getPackage().getName());
  File jarFile=createPluginJar(Plugin1.class,new File(tmpDir,""String_Node_Str""),manifest);
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(APP_ARTIFACT_ID.getNamespace().toEntityId(),APP_ARTIFACT_ID.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  artifactRepository.addArtifact(childId,jarFile,parents);
  Id.Artifact grandchildId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  manifest=createManifest(ManifestFields.EXPORT_PACKAGE,Plugin2.class.getPackage().getName());
  jarFile=createPluginJar(Plugin2.class,new File(tmpDir,""String_Node_Str""),manifest);
  parents=ImmutableSet.of(new ArtifactRange(childId.getNamespace().toEntityId(),childId.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  artifactRepository.addArtifact(grandchildId,jarFile,parents);
}"
5733,"private static File getFile() throws Exception {
  File pluginDir=DirUtils.createTempDir(tmpDir);
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File jarFile=createPluginJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(APP_ARTIFACT_ID.getNamespace(),APP_ARTIFACT_ID.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  artifactRepository.addArtifact(artifactId,jarFile,parents);
  return pluginDir;
}","private static File getFile() throws Exception {
  File pluginDir=DirUtils.createTempDir(tmpDir);
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File jarFile=createPluginJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(APP_ARTIFACT_ID.getNamespace().toEntityId(),APP_ARTIFACT_ID.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  artifactRepository.addArtifact(artifactId,jarFile,parents);
  return pluginDir;
}"
5734,"@Test(expected=InvalidArtifactException.class) public void testMultipleParentVersions() throws InvalidArtifactException {
  Id.Artifact child=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  ArtifactRepository.validateParentSet(child,ImmutableSet.of(new ArtifactRange(Id.Namespace.SYSTEM,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")),new ArtifactRange(Id.Namespace.SYSTEM,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""))));
}","@Test(expected=InvalidArtifactException.class) public void testMultipleParentVersions() throws InvalidArtifactException {
  Id.Artifact child=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  ArtifactRepository.validateParentSet(child,ImmutableSet.of(new ArtifactRange(NamespaceId.SYSTEM,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")),new ArtifactRange(NamespaceId.SYSTEM,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""))));
}"
5735,"@Test public void testAddSystemArtifacts() throws Exception {
  Id.Artifact systemAppArtifactId=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  File systemAppJar=createAppJar(PluginTestApp.class,new File(systemArtifactsDir1,""String_Node_Str""),createManifest(ManifestFields.EXPORT_PACKAGE,PluginTestRunnable.class.getPackage().getName()));
  Id.Artifact pluginArtifactId1=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File pluginJar1=createPluginJar(TestPlugin.class,new File(systemArtifactsDir1,""String_Node_Str""),manifest);
  Map<String,PluginPropertyField> emptyMap=Collections.emptyMap();
  Set<PluginClass> manuallyAddedPlugins1=ImmutableSet.of(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap),new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap));
  File pluginConfigFile=new File(systemArtifactsDir1,""String_Node_Str"");
  ArtifactConfig pluginConfig1=new ArtifactConfig(ImmutableSet.of(new ArtifactRange(Id.Namespace.SYSTEM,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""))),manuallyAddedPlugins1,ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  try (BufferedWriter writer=Files.newWriter(pluginConfigFile,Charsets.UTF_8)){
    writer.write(pluginConfig1.toString());
  }
   Id.Artifact pluginArtifactId2=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File pluginJar2=createPluginJar(TestPlugin.class,new File(systemArtifactsDir2,""String_Node_Str""),manifest);
  Set<PluginClass> manuallyAddedPlugins2=ImmutableSet.of(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap),new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap));
  pluginConfigFile=new File(systemArtifactsDir2,""String_Node_Str"");
  ArtifactConfig pluginConfig2=new ArtifactConfig(ImmutableSet.of(new ArtifactRange(Id.Namespace.SYSTEM,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""))),manuallyAddedPlugins2,ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
  try (BufferedWriter writer=Files.newWriter(pluginConfigFile,Charsets.UTF_8)){
    writer.write(pluginConfig2.toString());
  }
   artifactRepository.addSystemArtifacts();
  Assert.assertTrue(systemAppJar.delete());
  Assert.assertTrue(pluginJar1.delete());
  Assert.assertTrue(pluginJar2.delete());
  try {
    ArtifactDetail appArtifactDetail=artifactRepository.getArtifact(systemAppArtifactId);
    Map<ArtifactDescriptor,Set<PluginClass>> plugins=artifactRepository.getPlugins(NamespaceId.DEFAULT,systemAppArtifactId);
    Assert.assertEquals(2,plugins.size());
    Set<PluginClass> pluginClasses=plugins.values().iterator().next();
    Set<String> pluginNames=Sets.newHashSet();
    for (    PluginClass pluginClass : pluginClasses) {
      pluginNames.add(pluginClass.getName());
    }
    Assert.assertEquals(Sets.newHashSet(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),pluginNames);
    Assert.assertEquals(systemAppArtifactId.getName(),appArtifactDetail.getDescriptor().getArtifactId().getName());
    Assert.assertEquals(systemAppArtifactId.getVersion(),appArtifactDetail.getDescriptor().getArtifactId().getVersion());
    ArtifactDetail pluginArtifactDetail=artifactRepository.getArtifact(pluginArtifactId1);
    Assert.assertEquals(pluginArtifactId1.getName(),pluginArtifactDetail.getDescriptor().getArtifactId().getName());
    Assert.assertEquals(pluginArtifactId1.getVersion(),pluginArtifactDetail.getDescriptor().getArtifactId().getVersion());
    Assert.assertTrue(pluginArtifactDetail.getMeta().getClasses().getPlugins().containsAll(manuallyAddedPlugins1));
    Assert.assertEquals(pluginConfig1.getProperties(),pluginArtifactDetail.getMeta().getProperties());
    pluginArtifactDetail=artifactRepository.getArtifact(pluginArtifactId2);
    Assert.assertEquals(pluginArtifactId2.getName(),pluginArtifactDetail.getDescriptor().getArtifactId().getName());
    Assert.assertEquals(pluginArtifactId2.getVersion(),pluginArtifactDetail.getDescriptor().getArtifactId().getVersion());
    Assert.assertTrue(pluginArtifactDetail.getMeta().getClasses().getPlugins().containsAll(manuallyAddedPlugins2));
    Assert.assertEquals(pluginConfig2.getProperties(),pluginArtifactDetail.getMeta().getProperties());
  }
  finally {
    artifactRepository.clear(NamespaceId.SYSTEM);
  }
}","@Test public void testAddSystemArtifacts() throws Exception {
  Id.Artifact systemAppArtifactId=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  File systemAppJar=createAppJar(PluginTestApp.class,new File(systemArtifactsDir1,""String_Node_Str""),createManifest(ManifestFields.EXPORT_PACKAGE,PluginTestRunnable.class.getPackage().getName()));
  Id.Artifact pluginArtifactId1=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File pluginJar1=createPluginJar(TestPlugin.class,new File(systemArtifactsDir1,""String_Node_Str""),manifest);
  Map<String,PluginPropertyField> emptyMap=Collections.emptyMap();
  Set<PluginClass> manuallyAddedPlugins1=ImmutableSet.of(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap),new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap));
  File pluginConfigFile=new File(systemArtifactsDir1,""String_Node_Str"");
  ArtifactConfig pluginConfig1=new ArtifactConfig(ImmutableSet.of(new ArtifactRange(NamespaceId.SYSTEM,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""))),manuallyAddedPlugins1,ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  try (BufferedWriter writer=Files.newWriter(pluginConfigFile,Charsets.UTF_8)){
    writer.write(pluginConfig1.toString());
  }
   Id.Artifact pluginArtifactId2=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File pluginJar2=createPluginJar(TestPlugin.class,new File(systemArtifactsDir2,""String_Node_Str""),manifest);
  Set<PluginClass> manuallyAddedPlugins2=ImmutableSet.of(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap),new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap));
  pluginConfigFile=new File(systemArtifactsDir2,""String_Node_Str"");
  ArtifactConfig pluginConfig2=new ArtifactConfig(ImmutableSet.of(new ArtifactRange(NamespaceId.SYSTEM,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""))),manuallyAddedPlugins2,ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
  try (BufferedWriter writer=Files.newWriter(pluginConfigFile,Charsets.UTF_8)){
    writer.write(pluginConfig2.toString());
  }
   artifactRepository.addSystemArtifacts();
  Assert.assertTrue(systemAppJar.delete());
  Assert.assertTrue(pluginJar1.delete());
  Assert.assertTrue(pluginJar2.delete());
  try {
    ArtifactDetail appArtifactDetail=artifactRepository.getArtifact(systemAppArtifactId);
    Map<ArtifactDescriptor,Set<PluginClass>> plugins=artifactRepository.getPlugins(NamespaceId.DEFAULT,systemAppArtifactId);
    Assert.assertEquals(2,plugins.size());
    Set<PluginClass> pluginClasses=plugins.values().iterator().next();
    Set<String> pluginNames=Sets.newHashSet();
    for (    PluginClass pluginClass : pluginClasses) {
      pluginNames.add(pluginClass.getName());
    }
    Assert.assertEquals(Sets.newHashSet(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),pluginNames);
    Assert.assertEquals(systemAppArtifactId.getName(),appArtifactDetail.getDescriptor().getArtifactId().getName());
    Assert.assertEquals(systemAppArtifactId.getVersion(),appArtifactDetail.getDescriptor().getArtifactId().getVersion());
    ArtifactDetail pluginArtifactDetail=artifactRepository.getArtifact(pluginArtifactId1);
    Assert.assertEquals(pluginArtifactId1.getName(),pluginArtifactDetail.getDescriptor().getArtifactId().getName());
    Assert.assertEquals(pluginArtifactId1.getVersion(),pluginArtifactDetail.getDescriptor().getArtifactId().getVersion());
    Assert.assertTrue(pluginArtifactDetail.getMeta().getClasses().getPlugins().containsAll(manuallyAddedPlugins1));
    Assert.assertEquals(pluginConfig1.getProperties(),pluginArtifactDetail.getMeta().getProperties());
    pluginArtifactDetail=artifactRepository.getArtifact(pluginArtifactId2);
    Assert.assertEquals(pluginArtifactId2.getName(),pluginArtifactDetail.getDescriptor().getArtifactId().getName());
    Assert.assertEquals(pluginArtifactId2.getVersion(),pluginArtifactDetail.getDescriptor().getArtifactId().getVersion());
    Assert.assertTrue(pluginArtifactDetail.getMeta().getClasses().getPlugins().containsAll(manuallyAddedPlugins2));
    Assert.assertEquals(pluginConfig2.getProperties(),pluginArtifactDetail.getMeta().getProperties());
  }
  finally {
    artifactRepository.clear(NamespaceId.SYSTEM);
  }
}"
5736,"@Test public void testNamespaceIsolation() throws Exception {
  Id.Artifact systemAppArtifactId=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  File jar=createAppJar(PluginTestApp.class,new File(systemArtifactsDir1,""String_Node_Str""),createManifest(ManifestFields.EXPORT_PACKAGE,PluginTestRunnable.class.getPackage().getName()));
  artifactRepository.addSystemArtifacts();
  Assert.assertTrue(jar.delete());
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(systemAppArtifactId.getNamespace(),systemAppArtifactId.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  NamespaceId namespace1=Ids.namespace(""String_Node_Str"");
  NamespaceId namespace2=Ids.namespace(""String_Node_Str"");
  Id.Artifact pluginArtifactId1=Id.Artifact.from(namespace1.toId(),""String_Node_Str"",""String_Node_Str"");
  Id.Artifact pluginArtifactId2=Id.Artifact.from(namespace2.toId(),""String_Node_Str"",""String_Node_Str"");
  try {
    Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
    File jarFile=createPluginJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
    artifactRepository.addArtifact(pluginArtifactId1,jarFile,parents);
    artifactRepository.addArtifact(pluginArtifactId2,jarFile,parents);
    SortedMap<ArtifactDescriptor,Set<PluginClass>> extensions=artifactRepository.getPlugins(namespace1,systemAppArtifactId);
    Assert.assertEquals(1,extensions.keySet().size());
    Assert.assertEquals(2,extensions.values().iterator().next().size());
    extensions=artifactRepository.getPlugins(namespace2,systemAppArtifactId);
    Assert.assertEquals(1,extensions.keySet().size());
    Assert.assertEquals(2,extensions.values().iterator().next().size());
  }
  finally {
    artifactRepository.clear(NamespaceId.SYSTEM);
    artifactRepository.clear(namespace1);
    artifactRepository.clear(namespace2);
  }
}","@Test public void testNamespaceIsolation() throws Exception {
  Id.Artifact systemAppArtifactId=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  File jar=createAppJar(PluginTestApp.class,new File(systemArtifactsDir1,""String_Node_Str""),createManifest(ManifestFields.EXPORT_PACKAGE,PluginTestRunnable.class.getPackage().getName()));
  artifactRepository.addSystemArtifacts();
  Assert.assertTrue(jar.delete());
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(systemAppArtifactId.getNamespace().toEntityId(),systemAppArtifactId.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  NamespaceId namespace1=Ids.namespace(""String_Node_Str"");
  NamespaceId namespace2=Ids.namespace(""String_Node_Str"");
  Id.Artifact pluginArtifactId1=Id.Artifact.from(namespace1.toId(),""String_Node_Str"",""String_Node_Str"");
  Id.Artifact pluginArtifactId2=Id.Artifact.from(namespace2.toId(),""String_Node_Str"",""String_Node_Str"");
  try {
    Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
    File jarFile=createPluginJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
    artifactRepository.addArtifact(pluginArtifactId1,jarFile,parents);
    artifactRepository.addArtifact(pluginArtifactId2,jarFile,parents);
    SortedMap<ArtifactDescriptor,Set<PluginClass>> extensions=artifactRepository.getPlugins(namespace1,systemAppArtifactId);
    Assert.assertEquals(1,extensions.keySet().size());
    Assert.assertEquals(2,extensions.values().iterator().next().size());
    extensions=artifactRepository.getPlugins(namespace2,systemAppArtifactId);
    Assert.assertEquals(1,extensions.keySet().size());
    Assert.assertEquals(2,extensions.values().iterator().next().size());
  }
  finally {
    artifactRepository.clear(NamespaceId.SYSTEM);
    artifactRepository.clear(namespace1);
    artifactRepository.clear(namespace2);
  }
}"
5737,"public static Comparable convertFieldValue(String where,String kind,String fieldName,FieldType fieldType,String stringValue,boolean acceptNull){
  if (null == stringValue) {
    if (acceptNull) {
      return null;
    }
 else {
      throw new IllegalArgumentException(String.format(""String_Node_Str"",where,kind,fieldName));
    }
  }
  try {
    return fieldType.parse(stringValue);
  }
 catch (  Exception e) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",where,kind,stringValue,fieldName,fieldType.name()),e);
  }
}","private static Comparable convertFieldValue(String where,String kind,String fieldName,FieldType fieldType,String stringValue,boolean acceptNull){
  if (null == stringValue) {
    if (acceptNull) {
      return null;
    }
 else {
      throw new IllegalArgumentException(String.format(""String_Node_Str"",where,kind,fieldName));
    }
  }
  try {
    return fieldType.parse(stringValue);
  }
 catch (  Exception e) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",where,kind,stringValue,fieldName,fieldType.name()),e);
  }
}"
5738,"/** 
 * Sets a partition as input for a PartitionedFileSet. If both a PartitionFilter and Partition(s) are specified, the PartitionFilter takes precedence and the specified Partition(s) will be ignored.
 * @param arguments the runtime arguments for a partitioned dataset
 * @param partition the partition to add as input
 */
public static void addInputPartition(Map<String,String> arguments,Partition partition){
  FileSetArguments.addInputPath(arguments,partition.getRelativePath());
}","/** 
 * Sets a partition as input for a PartitionedFileSet. If both a PartitionFilter and Partition(s) are specified, the PartitionFilter takes precedence and the specified Partition(s) will be ignored.
 * @param arguments the runtime arguments for a partitioned dataset
 * @param partition the partition to add as input
 */
public static void addInputPartition(Map<String,String> arguments,Partition partition){
  addInputPartitions(arguments,Collections.singletonList(partition));
}"
5739,"/** 
 * Returns the name of the input configured for this task. Returns null, if this task is a Reducer or no inputs were configured through CDAP APIs.
 */
@Nullable String getInputName();","/** 
 * Returns the name of the input configured for this task. Returns null, if this task is a Reducer or no inputs were configured through CDAP APIs.
 * @deprecated Instead, use {@link #getInputContext()}.
 */
@Deprecated @Nullable String getInputName();"
5740,"@Nullable @Override public String getInputName(){
  return inputName;
}","@Nullable @Override public String getInputName(){
  if (inputContext == null) {
    return null;
  }
  return inputContext.getInputName();
}"
5741,"private WrappedMapper.Context createAutoFlushingContext(final Context context,final BasicMapReduceTaskContext basicMapReduceContext){
  final int flushFreq=context.getConfiguration().getInt(""String_Node_Str"",10000);
  @SuppressWarnings(""String_Node_Str"") WrappedMapper.Context flushingContext=new WrappedMapper().new Context(context){
    private int processedRecords=0;
    @Override public boolean nextKeyValue() throws IOException, InterruptedException {
      boolean result=super.nextKeyValue();
      if (++processedRecords > flushFreq) {
        try {
          LOG.trace(""String_Node_Str"");
          basicMapReduceContext.flushOperations();
        }
 catch (        Exception e) {
          LOG.error(""String_Node_Str"",e);
          throw Throwables.propagate(e);
        }
        processedRecords=0;
      }
      return result;
    }
    @Override public InputSplit getInputSplit(){
      InputSplit inputSplit=super.getInputSplit();
      if (inputSplit instanceof TaggedInputSplit) {
        inputSplit=((TaggedInputSplit)inputSplit).getInputSplit();
      }
      return inputSplit;
    }
    @Override public Class<? extends InputFormat<?,?>> getInputFormatClass() throws ClassNotFoundException {
      InputSplit inputSplit=super.getInputSplit();
      if (inputSplit instanceof TaggedInputSplit) {
        return ((TaggedInputSplit)inputSplit).getInputFormatClass();
      }
      return super.getInputFormatClass();
    }
  }
;
  return flushingContext;
}","private WrappedMapper.Context createAutoFlushingContext(final Context context,final BasicMapReduceTaskContext basicMapReduceContext){
  final int flushFreq=context.getConfiguration().getInt(""String_Node_Str"",10000);
  @SuppressWarnings(""String_Node_Str"") WrappedMapper.Context flushingContext=new WrappedMapper().new Context(context){
    private int processedRecords=0;
    @Override public boolean nextKeyValue() throws IOException, InterruptedException {
      boolean result=super.nextKeyValue();
      if (++processedRecords > flushFreq) {
        try {
          LOG.trace(""String_Node_Str"");
          basicMapReduceContext.flushOperations();
        }
 catch (        Exception e) {
          LOG.error(""String_Node_Str"",e);
          throw Throwables.propagate(e);
        }
        processedRecords=0;
      }
      return result;
    }
    @Override public InputSplit getInputSplit(){
      InputSplit inputSplit=super.getInputSplit();
      if (inputSplit instanceof TaggedInputSplit) {
        inputSplit=((TaggedInputSplit)inputSplit).getInputSplit();
      }
      return inputSplit;
    }
    @Override public Class<? extends InputFormat<?,?>> getInputFormatClass() throws ClassNotFoundException {
      InputSplit inputSplit=super.getInputSplit();
      if (inputSplit instanceof MultiInputTaggedSplit) {
        return ((MultiInputTaggedSplit)inputSplit).getInputFormatClass();
      }
      return super.getInputFormatClass();
    }
  }
;
  return flushingContext;
}"
5742,"@Override public Class<? extends InputFormat<?,?>> getInputFormatClass() throws ClassNotFoundException {
  InputSplit inputSplit=super.getInputSplit();
  if (inputSplit instanceof TaggedInputSplit) {
    return ((TaggedInputSplit)inputSplit).getInputFormatClass();
  }
  return super.getInputFormatClass();
}","@Override public Class<? extends InputFormat<?,?>> getInputFormatClass() throws ClassNotFoundException {
  InputSplit inputSplit=super.getInputSplit();
  if (inputSplit instanceof MultiInputTaggedSplit) {
    return ((MultiInputTaggedSplit)inputSplit).getInputFormatClass();
  }
  return super.getInputFormatClass();
}"
5743,"@SuppressWarnings(""String_Node_Str"") @Override public void run(Context context) throws IOException, InterruptedException {
  MapReduceClassLoader classLoader=MapReduceClassLoader.getFromConfiguration(context.getConfiguration());
  BasicMapReduceTaskContext basicMapReduceContext=classLoader.getTaskContextProvider().get(context);
  WrappedMapper.Context flushingContext=createAutoFlushingContext(context,basicMapReduceContext);
  basicMapReduceContext.setHadoopContext(flushingContext);
  InputSplit inputSplit=context.getInputSplit();
  if (inputSplit instanceof TaggedInputSplit) {
    basicMapReduceContext.setInputName(((TaggedInputSplit)inputSplit).getName());
  }
  ClassLoader programClassLoader=classLoader.getProgramClassLoader();
  Mapper delegate=createMapperInstance(programClassLoader,getWrappedMapper(context.getConfiguration()),context);
  try {
    Reflections.visit(delegate,delegate.getClass(),new PropertyFieldSetter(basicMapReduceContext.getSpecification().getProperties()),new MetricsFieldSetter(basicMapReduceContext.getMetrics()),new DataSetFieldSetter(basicMapReduceContext));
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",delegate.getClass(),t);
    throw Throwables.propagate(t);
  }
  ClassLoader oldClassLoader;
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(classLoader);
    try {
      ((ProgramLifecycle)delegate).initialize(new MapReduceLifecycleContext(basicMapReduceContext));
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
      throw Throwables.propagate(e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
  oldClassLoader=ClassLoaders.setContextClassLoader(classLoader);
  try {
    delegate.run(flushingContext);
  }
  finally {
    ClassLoaders.setContextClassLoader(oldClassLoader);
  }
  try {
    basicMapReduceContext.flushOperations();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",basicMapReduceContext,e);
    throw Throwables.propagate(e);
  }
  basicMapReduceContext.closeMultiOutputs();
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(classLoader);
    try {
      ((ProgramLifecycle<? extends RuntimeContext>)delegate).destroy();
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
}","@SuppressWarnings(""String_Node_Str"") @Override public void run(Context context) throws IOException, InterruptedException {
  MapReduceClassLoader classLoader=MapReduceClassLoader.getFromConfiguration(context.getConfiguration());
  BasicMapReduceTaskContext basicMapReduceContext=classLoader.getTaskContextProvider().get(context);
  WrappedMapper.Context flushingContext=createAutoFlushingContext(context,basicMapReduceContext);
  basicMapReduceContext.setHadoopContext(flushingContext);
  InputSplit inputSplit=context.getInputSplit();
  if (inputSplit instanceof MultiInputTaggedSplit) {
    basicMapReduceContext.setInputContext(InputContexts.create((MultiInputTaggedSplit)inputSplit));
  }
  ClassLoader programClassLoader=classLoader.getProgramClassLoader();
  Mapper delegate=createMapperInstance(programClassLoader,getWrappedMapper(context.getConfiguration()),context);
  try {
    Reflections.visit(delegate,delegate.getClass(),new PropertyFieldSetter(basicMapReduceContext.getSpecification().getProperties()),new MetricsFieldSetter(basicMapReduceContext.getMetrics()),new DataSetFieldSetter(basicMapReduceContext));
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",delegate.getClass(),t);
    throw Throwables.propagate(t);
  }
  ClassLoader oldClassLoader;
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(classLoader);
    try {
      ((ProgramLifecycle)delegate).initialize(new MapReduceLifecycleContext(basicMapReduceContext));
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
      throw Throwables.propagate(e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
  oldClassLoader=ClassLoaders.setContextClassLoader(classLoader);
  try {
    delegate.run(flushingContext);
  }
  finally {
    ClassLoaders.setContextClassLoader(oldClassLoader);
  }
  try {
    basicMapReduceContext.flushOperations();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",basicMapReduceContext,e);
    throw Throwables.propagate(e);
  }
  basicMapReduceContext.closeMultiOutputs();
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(classLoader);
    try {
      ((ProgramLifecycle<? extends RuntimeContext>)delegate).destroy();
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
}"
5744,"private Mapper createMapperInstance(ClassLoader classLoader,String userMapper,Context context){
  if (context.getInputSplit() instanceof TaggedInputSplit) {
    userMapper=((TaggedInputSplit)context.getInputSplit()).getMapperClassName();
  }
  try {
    return (Mapper)classLoader.loadClass(userMapper).newInstance();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"" + userMapper);
    throw Throwables.propagate(e);
  }
}","private Mapper createMapperInstance(ClassLoader classLoader,String userMapper,Context context){
  if (context.getInputSplit() instanceof MultiInputTaggedSplit) {
    userMapper=((MultiInputTaggedSplit)context.getInputSplit()).getMapperClassName();
  }
  try {
    return (Mapper)classLoader.loadClass(userMapper).newInstance();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"" + userMapper);
    throw Throwables.propagate(e);
  }
}"
5745,"@Override public ProgramController call() throws Exception {
  return launch(program,options,localizeResources,tempDir,new ApplicationLauncher(){
    @Override public TwillController launch(    TwillApplication twillApplication,    Iterable<String> extraClassPaths,    Iterable<? extends Class<?>> extraDependencies){
      TwillPreparer twillPreparer=twillRunner.prepare(twillApplication);
      Iterables.addAll(additionalClassPaths,extraClassPaths);
      twillPreparer.withEnv(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ApplicationConstants.LOG_DIR_EXPANSION_VAR));
      if (options.isDebug()) {
        twillPreparer.enableDebugging();
      }
      LOG.info(""String_Node_Str"",program.getId(),options.isDebug(),programOptions,logbackURI);
      if (schedulerQueueName != null && !schedulerQueueName.isEmpty()) {
        LOG.info(""String_Node_Str"",program.getId(),schedulerQueueName);
        twillPreparer.setSchedulerQueue(schedulerQueueName);
      }
      if (logbackURI != null) {
        twillPreparer.addJVMOptions(""String_Node_Str"" + LOGBACK_FILE_NAME);
      }
      String logLevelConf=cConf.get(Constants.COLLECT_APP_CONTAINER_LOG_LEVEL).toUpperCase();
      if (""String_Node_Str"".equals(logLevelConf)) {
        twillPreparer.addJVMOptions(""String_Node_Str"");
      }
 else {
        LogEntry.Level logLevel=LogEntry.Level.ERROR;
        if (""String_Node_Str"".equals(logLevelConf)) {
          logLevel=LogEntry.Level.TRACE;
        }
 else {
          try {
            logLevel=LogEntry.Level.valueOf(logLevelConf.toUpperCase());
          }
 catch (          Exception e) {
            LOG.warn(""String_Node_Str"",logLevelConf);
          }
        }
        twillPreparer.addLogHandler(new ApplicationLogHandler(new PrinterLogHandler(new PrintWriter(System.out)),logLevel));
      }
      if (User.isHBaseSecurityEnabled(hConf) || UserGroupInformation.isSecurityEnabled()) {
        twillPreparer.addSecureStore(secureStoreUpdater.update());
      }
      Iterable<Class<?>> dependencies=Iterables.concat(Collections.singletonList(HBaseTableUtilFactory.getHBaseTableUtilClass()),getKMSSecureStore(cConf),extraDependencies);
      Iterable<String> yarnAppClassPath=Arrays.asList(hConf.getStrings(YarnConfiguration.YARN_APPLICATION_CLASSPATH,YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
      twillPreparer.withDependencies(dependencies).withClassPaths(Iterables.concat(additionalClassPaths,yarnAppClassPath)).withApplicationClassPaths(yarnAppClassPath).withBundlerClassAcceptor(new HadoopClassExcluder(){
        @Override public boolean accept(        String className,        URL classUrl,        URL classPathUrl){
          return super.accept(className,classUrl,classPathUrl) && !className.startsWith(""String_Node_Str"");
        }
      }
).withApplicationArguments(""String_Node_Str"" + RunnableOptions.JAR,programJarName,""String_Node_Str"" + RunnableOptions.HADOOP_CONF_FILE,HADOOP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.CDAP_CONF_FILE,CDAP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.APP_SPEC_FILE,APP_SPEC_FILE_NAME,""String_Node_Str"" + RunnableOptions.PROGRAM_OPTIONS,programOptions,""String_Node_Str"" + RunnableOptions.PROGRAM_ID,GSON.toJson(program.getId()));
      File tmpDir=new File(cConf.get(Constants.CFG_LOCAL_DATA_DIR),cConf.get(Constants.AppFabric.TEMP_DIR));
      File jarCacheDir=new File(tmpDir,""String_Node_Str"");
      File programTypeDir=new File(jarCacheDir,program.getType().name().toLowerCase());
      DirUtils.mkdirs(programTypeDir);
      twillPreparer.withApplicationArguments(""String_Node_Str"" + programTypeDir.getAbsolutePath());
      jarCacheTracker.registerLaunch(programTypeDir,program.getType());
      twillPreparer.withApplicationArguments(""String_Node_Str"" + cConf.get(Constants.AppFabric.PROGRAM_MAX_START_SECONDS),""String_Node_Str"" + cConf.get(Constants.AppFabric.PROGRAM_MAX_STOP_SECONDS));
      TwillController twillController;
      ClassLoader oldClassLoader=ClassLoaders.setContextClassLoader(new CombineClassLoader(AbstractDistributedProgramRunner.this.getClass().getClassLoader(),Iterables.transform(dependencies,new Function<Class<?>,ClassLoader>(){
        @Override public ClassLoader apply(        Class<?> input){
          return input.getClassLoader();
        }
      }
)));
      try {
        twillController=twillPreparer.start();
      }
  finally {
        ClassLoaders.setContextClassLoader(oldClassLoader);
      }
      return addCleanupListener(twillController,program,tempDir);
    }
  }
);
}","@Override public ProgramController call() throws Exception {
  return launch(program,options,localizeResources,tempDir,new ApplicationLauncher(){
    @Override public TwillController launch(    TwillApplication twillApplication,    Iterable<String> extraClassPaths,    Iterable<? extends Class<?>> extraDependencies){
      TwillPreparer twillPreparer=twillRunner.prepare(twillApplication);
      Iterables.addAll(additionalClassPaths,extraClassPaths);
      twillPreparer.withEnv(ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ApplicationConstants.LOG_DIR_EXPANSION_VAR));
      if (options.isDebug()) {
        twillPreparer.enableDebugging();
      }
      LOG.info(""String_Node_Str"",program.getId(),options.isDebug(),programOptions,logbackURI);
      if (schedulerQueueName != null && !schedulerQueueName.isEmpty()) {
        LOG.info(""String_Node_Str"",program.getId(),schedulerQueueName);
        twillPreparer.setSchedulerQueue(schedulerQueueName);
      }
      if (logbackURI != null) {
        twillPreparer.addJVMOptions(""String_Node_Str"" + LOGBACK_FILE_NAME);
      }
      String logLevelConf=cConf.get(Constants.COLLECT_APP_CONTAINER_LOG_LEVEL).toUpperCase();
      if (""String_Node_Str"".equals(logLevelConf)) {
        twillPreparer.addJVMOptions(""String_Node_Str"");
      }
 else {
        LogEntry.Level logLevel=LogEntry.Level.ERROR;
        if (""String_Node_Str"".equals(logLevelConf)) {
          logLevel=LogEntry.Level.TRACE;
        }
 else {
          try {
            logLevel=LogEntry.Level.valueOf(logLevelConf.toUpperCase());
          }
 catch (          Exception e) {
            LOG.warn(""String_Node_Str"",logLevelConf);
          }
        }
        twillPreparer.addLogHandler(new ApplicationLogHandler(new PrinterLogHandler(new PrintWriter(System.out)),logLevel));
      }
      if (User.isHBaseSecurityEnabled(hConf) || UserGroupInformation.isSecurityEnabled()) {
        twillPreparer.addSecureStore(secureStoreUpdater.update());
      }
      Iterable<Class<?>> dependencies=Iterables.concat(Collections.singletonList(HBaseTableUtilFactory.getHBaseTableUtilClass()),getKMSSecureStore(cConf),extraDependencies);
      Iterable<String> yarnAppClassPath=Arrays.asList(hConf.getTrimmedStrings(YarnConfiguration.YARN_APPLICATION_CLASSPATH,YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
      twillPreparer.withDependencies(dependencies).withClassPaths(Iterables.concat(additionalClassPaths,yarnAppClassPath)).withApplicationClassPaths(yarnAppClassPath).withBundlerClassAcceptor(new HadoopClassExcluder(){
        @Override public boolean accept(        String className,        URL classUrl,        URL classPathUrl){
          return super.accept(className,classUrl,classPathUrl) && !className.startsWith(""String_Node_Str"");
        }
      }
).withApplicationArguments(""String_Node_Str"" + RunnableOptions.JAR,programJarName,""String_Node_Str"" + RunnableOptions.HADOOP_CONF_FILE,HADOOP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.CDAP_CONF_FILE,CDAP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.APP_SPEC_FILE,APP_SPEC_FILE_NAME,""String_Node_Str"" + RunnableOptions.PROGRAM_OPTIONS,programOptions,""String_Node_Str"" + RunnableOptions.PROGRAM_ID,GSON.toJson(program.getId()));
      File tmpDir=new File(cConf.get(Constants.CFG_LOCAL_DATA_DIR),cConf.get(Constants.AppFabric.TEMP_DIR));
      File jarCacheDir=new File(tmpDir,""String_Node_Str"");
      File programTypeDir=new File(jarCacheDir,program.getType().name().toLowerCase());
      DirUtils.mkdirs(programTypeDir);
      twillPreparer.withApplicationArguments(""String_Node_Str"" + programTypeDir.getAbsolutePath());
      jarCacheTracker.registerLaunch(programTypeDir,program.getType());
      twillPreparer.withApplicationArguments(""String_Node_Str"" + cConf.get(Constants.AppFabric.PROGRAM_MAX_START_SECONDS),""String_Node_Str"" + cConf.get(Constants.AppFabric.PROGRAM_MAX_STOP_SECONDS));
      TwillController twillController;
      ClassLoader oldClassLoader=ClassLoaders.setContextClassLoader(new CombineClassLoader(AbstractDistributedProgramRunner.this.getClass().getClassLoader(),Iterables.transform(dependencies,new Function<Class<?>,ClassLoader>(){
        @Override public ClassLoader apply(        Class<?> input){
          return input.getClassLoader();
        }
      }
)));
      try {
        twillController=twillPreparer.start();
      }
  finally {
        ClassLoaders.setContextClassLoader(oldClassLoader);
      }
      return addCleanupListener(twillController,program,tempDir);
    }
  }
);
}"
5746,"/** 
 * Returns the path to a keytab file, after copying it to the local file system, if necessary
 * @param keytabLocation the keytabLocation of the keytab file
 * @return local keytab file
 */
private File localizeKeytab(Location keytabLocation) throws IOException {
  if (!DirUtils.mkdirs(tempDir)) {
    throw new IOException(String.format(""String_Node_Str"",tempDir));
  }
  Path localKeytabFile=Files.createTempFile(tempDir.toPath(),null,""String_Node_Str"",OWNER_ONLY_ATTRS);
  LOG.debug(""String_Node_Str"",keytabLocation,localKeytabFile);
  try (InputStream is=keytabLocation.getInputStream()){
    Files.copy(is,localKeytabFile,StandardCopyOption.REPLACE_EXISTING);
  }
   return localKeytabFile.toFile();
}","/** 
 * Returns the path to a keytab file, after copying it to the local file system, if necessary
 * @param keytabLocation the keytabLocation of the keytab file
 * @return local keytab file
 */
private File localizeKeytab(Location keytabLocation) throws IOException {
  if (!DirUtils.mkdirs(tempDir)) {
    throw new IOException(String.format(""String_Node_Str"",tempDir));
  }
  Path localKeytabFile=Files.createTempFile(tempDir.toPath(),null,""String_Node_Str"",FileUtils.OWNER_ONLY_RW);
  LOG.debug(""String_Node_Str"",keytabLocation,localKeytabFile);
  try (InputStream is=keytabLocation.getInputStream()){
    Files.copy(is,localKeytabFile,StandardCopyOption.REPLACE_EXISTING);
  }
   return localKeytabFile.toFile();
}"
5747,"@Override public TableInfo getTableInfo(@Nullable String database,String table) throws ExploreException, TableNotFoundException {
  HttpResponse response=doGet(String.format(""String_Node_Str"",database,table));
  if (response.getResponseCode() == HttpURLConnection.HTTP_OK) {
    return parseJson(response,TableInfo.class);
  }
 else   if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new TableNotFoundException(""String_Node_Str"" + database + table+ ""String_Node_Str"");
  }
  throw new ExploreException(""String_Node_Str"" + database + table+ ""String_Node_Str""+ response);
}","@Override public TableInfo getTableInfo(String namespace,String table) throws ExploreException, TableNotFoundException {
  HttpResponse response=doGet(String.format(""String_Node_Str"",namespace,table));
  if (response.getResponseCode() == HttpURLConnection.HTTP_OK) {
    return parseJson(response,TableInfo.class);
  }
 else   if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new TableNotFoundException(String.format(""String_Node_Str"",namespace,table));
  }
  throw new ExploreException(String.format(""String_Node_Str"",namespace,table,response));
}"
5748,"@Override public List<TableNameInfo> getTables(@Nullable String database) throws ExploreException {
  HttpResponse response=doGet(String.format(""String_Node_Str"",database));
  if (response.getResponseCode() == HttpURLConnection.HTTP_OK) {
    return parseJson(response,TABLES_TYPE);
  }
  throw new ExploreException(""String_Node_Str"" + response);
}","@Override public List<TableNameInfo> getTables(String namespace) throws ExploreException {
  HttpResponse response=doGet(String.format(""String_Node_Str"",namespace));
  if (response.getResponseCode() == HttpURLConnection.HTTP_OK) {
    return parseJson(response,TABLES_TYPE);
  }
  throw new ExploreException(""String_Node_Str"" + response);
}"
5749,"@Override public QueryHandle getFunctions(String catalog,String schemaPattern,String functionNamePattern) throws ExploreException, SQLException {
  String body=GSON.toJson(new FunctionsArgs(catalog,schemaPattern,functionNamePattern));
  String resource=String.format(""String_Node_Str"",schemaPattern);
  HttpResponse response=doPost(resource,body,null);
  if (response.getResponseCode() == HttpURLConnection.HTTP_OK) {
    return QueryHandle.fromId(parseResponseAsMap(response,""String_Node_Str""));
  }
  throw new ExploreException(""String_Node_Str"" + response);
}","@Override public QueryHandle getFunctions(@Nullable String catalog,@Nullable String schemaPattern,String functionNamePattern) throws ExploreException, SQLException {
  String body=GSON.toJson(new FunctionsArgs(catalog,schemaPattern,functionNamePattern));
  String resource=String.format(""String_Node_Str"",schemaPattern);
  HttpResponse response=doPost(resource,body,null);
  if (response.getResponseCode() == HttpURLConnection.HTTP_OK) {
    return QueryHandle.fromId(parseResponseAsMap(response,""String_Node_Str""));
  }
  throw new ExploreException(""String_Node_Str"" + response);
}"
5750,"@Override public QueryHandle getSchemas(String catalog,String schemaPattern) throws ExploreException, SQLException {
  String body=GSON.toJson(new SchemasArgs(catalog,schemaPattern));
  String resource=String.format(""String_Node_Str"",schemaPattern);
  HttpResponse response=doPost(resource,body,null);
  if (response.getResponseCode() == HttpURLConnection.HTTP_OK) {
    return QueryHandle.fromId(parseResponseAsMap(response,""String_Node_Str""));
  }
  throw new ExploreException(""String_Node_Str"" + response);
}","@Override public QueryHandle getSchemas(@Nullable String catalog,@Nullable String schemaPattern) throws ExploreException, SQLException {
  String body=GSON.toJson(new SchemasArgs(catalog,schemaPattern));
  String resource=String.format(""String_Node_Str"",schemaPattern);
  HttpResponse response=doPost(resource,body,null);
  if (response.getResponseCode() == HttpURLConnection.HTTP_OK) {
    return QueryHandle.fromId(parseResponseAsMap(response,""String_Node_Str""));
  }
  throw new ExploreException(""String_Node_Str"" + response);
}"
5751,"/** 
 * Get information about a Hive table.
 * @param database name of the database the table belongs to.
 * @param table table name for which to get the schema.
 * @return information about a table.
 * @throws ExploreException on any error getting the tables.
 */
TableInfo getTableInfo(@Nullable String database,String table) throws ExploreException, TableNotFoundException ;","/** 
 * Get information about a Hive table.
 * @param namespace name of the namespace the table belongs to.
 * @param table table name for which to get the schema.
 * @return information about a table.
 * @throws ExploreException on any error getting the tables.
 */
TableInfo getTableInfo(String namespace,String table) throws ExploreException, TableNotFoundException ;"
5752,"/** 
 * Retrieve a list of all the tables present in Hive Metastore that match the given database name.
 * @param database database name from which to list the tables. The database has to be accessible by the currentuser. If it is null, all the databases the user has access to will be inspected.
 * @return list of table names present in the database.
 * @throws ExploreException on any error getting the tables.
 */
List<TableNameInfo> getTables(@Nullable String database) throws ExploreException ;","/** 
 * Retrieve a list of all the tables present in Hive Metastore that match the given database name.
 * @param namespace namespace name from which to list the tables. The database has to be accessible by the currentuser.
 * @return list of table names present in the database.
 * @throws ExploreException on any error getting the tables.
 */
List<TableNameInfo> getTables(String namespace) throws ExploreException ;"
5753,"@Override public Service get(){
  return new AbstractService(){
    @Override protected void doStart(){
      try {
        namespaceAdmin.create(NamespaceMeta.DEFAULT);
        LOG.info(""String_Node_Str"");
        notifyStarted();
      }
 catch (      AlreadyExistsException e) {
        LOG.info(""String_Node_Str"");
        notifyStarted();
      }
catch (      Exception e) {
        notifyFailed(e);
      }
    }
    @Override protected void doStop(){
      notifyStopped();
    }
  }
;
}","@Override public Service get(){
  return new AbstractService(){
    @Override protected void doStart(){
      try {
        namespaceAdmin.create(NamespaceMeta.DEFAULT);
        LOG.info(""String_Node_Str"",NamespaceMeta.DEFAULT);
        notifyStarted();
      }
 catch (      AlreadyExistsException e) {
        LOG.info(""String_Node_Str"");
        notifyStarted();
      }
catch (      Exception e) {
        notifyFailed(e);
      }
    }
    @Override protected void doStop(){
      notifyStopped();
    }
  }
;
}"
5754,"@Inject public DefaultNamespaceEnsurer(final NamespaceAdmin namespaceAdmin){
  this.serviceDelegate=new RetryOnStartFailureService(new Supplier<Service>(){
    @Override public Service get(){
      return new AbstractService(){
        @Override protected void doStart(){
          try {
            namespaceAdmin.create(NamespaceMeta.DEFAULT);
            LOG.info(""String_Node_Str"");
            notifyStarted();
          }
 catch (          AlreadyExistsException e) {
            LOG.info(""String_Node_Str"");
            notifyStarted();
          }
catch (          Exception e) {
            notifyFailed(e);
          }
        }
        @Override protected void doStop(){
          notifyStopped();
        }
      }
;
    }
  }
,RetryStrategies.exponentialDelay(200,5000,TimeUnit.MILLISECONDS));
}","@Inject public DefaultNamespaceEnsurer(final NamespaceAdmin namespaceAdmin){
  this.serviceDelegate=new RetryOnStartFailureService(new Supplier<Service>(){
    @Override public Service get(){
      return new AbstractService(){
        @Override protected void doStart(){
          try {
            namespaceAdmin.create(NamespaceMeta.DEFAULT);
            LOG.info(""String_Node_Str"",NamespaceMeta.DEFAULT);
            notifyStarted();
          }
 catch (          AlreadyExistsException e) {
            LOG.info(""String_Node_Str"");
            notifyStarted();
          }
catch (          Exception e) {
            notifyFailed(e);
          }
        }
        @Override protected void doStop(){
          notifyStopped();
        }
      }
;
    }
  }
,RetryStrategies.exponentialDelay(200,5000,TimeUnit.MILLISECONDS));
}"
5755,"/** 
 * Construct the AppFabricServer with service factory and configuration coming from guice injection.
 */
@Inject public AppFabricServer(CConfiguration configuration,DiscoveryService discoveryService,SchedulerService schedulerService,NotificationService notificationService,@Named(Constants.Service.MASTER_SERVICES_BIND_ADDRESS) InetAddress hostname,@Named(Constants.AppFabric.HANDLERS_BINDING) Set<HttpHandler> handlers,@Nullable MetricsCollectionService metricsCollectionService,ProgramRuntimeService programRuntimeService,ApplicationLifecycleService applicationLifecycleService,ProgramLifecycleService programLifecycleService,StreamCoordinatorClient streamCoordinatorClient,@Named(""String_Node_Str"") Set<String> servicesNames,@Named(""String_Node_Str"") Set<String> handlerHookNames,DefaultNamespaceEnsurer defaultNamespaceEnsurer,SystemArtifactLoader systemArtifactLoader,PluginService pluginService,PrivilegesFetcherProxyService privilegesFetcherProxyService){
  this.hostname=hostname;
  this.discoveryService=discoveryService;
  this.schedulerService=schedulerService;
  this.handlers=handlers;
  this.configuration=configuration;
  this.metricsCollectionService=metricsCollectionService;
  this.programRuntimeService=programRuntimeService;
  this.notificationService=notificationService;
  this.servicesNames=servicesNames;
  this.handlerHookNames=handlerHookNames;
  this.applicationLifecycleService=applicationLifecycleService;
  this.streamCoordinatorClient=streamCoordinatorClient;
  this.programLifecycleService=programLifecycleService;
  this.defaultNamespaceEnsurer=defaultNamespaceEnsurer;
  this.systemArtifactLoader=systemArtifactLoader;
  this.pluginService=pluginService;
  this.privilegesFetcherProxyService=privilegesFetcherProxyService;
}","/** 
 * Construct the AppFabricServer with service factory and configuration coming from guice injection.
 */
@Inject public AppFabricServer(CConfiguration configuration,DiscoveryService discoveryService,SchedulerService schedulerService,NotificationService notificationService,@Named(Constants.Service.MASTER_SERVICES_BIND_ADDRESS) InetAddress hostname,@Named(Constants.AppFabric.HANDLERS_BINDING) Set<HttpHandler> handlers,@Nullable MetricsCollectionService metricsCollectionService,ProgramRuntimeService programRuntimeService,ApplicationLifecycleService applicationLifecycleService,ProgramLifecycleService programLifecycleService,StreamCoordinatorClient streamCoordinatorClient,@Named(""String_Node_Str"") Set<String> servicesNames,@Named(""String_Node_Str"") Set<String> handlerHookNames,NamespaceAdmin namespaceAdmin,SystemArtifactLoader systemArtifactLoader,PluginService pluginService,PrivilegesFetcherProxyService privilegesFetcherProxyService){
  this.hostname=hostname;
  this.discoveryService=discoveryService;
  this.schedulerService=schedulerService;
  this.handlers=handlers;
  this.configuration=configuration;
  this.metricsCollectionService=metricsCollectionService;
  this.programRuntimeService=programRuntimeService;
  this.notificationService=notificationService;
  this.servicesNames=servicesNames;
  this.handlerHookNames=handlerHookNames;
  this.applicationLifecycleService=applicationLifecycleService;
  this.streamCoordinatorClient=streamCoordinatorClient;
  this.programLifecycleService=programLifecycleService;
  this.systemArtifactLoader=systemArtifactLoader;
  this.pluginService=pluginService;
  this.privilegesFetcherProxyService=privilegesFetcherProxyService;
  this.defaultNamespaceEnsurer=new DefaultNamespaceEnsurer(namespaceAdmin);
}"
5756,"/** 
 * Construct the Standalone AppFabricServer with service factory and configuration coming from guice injection.
 */
@Inject public StandaloneAppFabricServer(CConfiguration configuration,DiscoveryService discoveryService,SchedulerService schedulerService,NotificationService notificationService,@Named(Constants.Service.MASTER_SERVICES_BIND_ADDRESS) InetAddress hostname,@Named(Constants.AppFabric.HANDLERS_BINDING) Set<HttpHandler> handlers,@Nullable MetricsCollectionService metricsCollectionService,ProgramRuntimeService programRuntimeService,ApplicationLifecycleService applicationLifecycleService,ProgramLifecycleService programLifecycleService,StreamCoordinatorClient streamCoordinatorClient,@Named(""String_Node_Str"") Set<String> servicesNames,@Named(""String_Node_Str"") Set<String> handlerHookNames,DefaultNamespaceEnsurer defaultNamespaceEnsurer,MetricStore metricStore,SystemArtifactLoader systemArtifactLoader,PluginService pluginService,PrivilegesFetcherProxyService privilegesFetcherProxyService){
  super(configuration,discoveryService,schedulerService,notificationService,hostname,handlers,metricsCollectionService,programRuntimeService,applicationLifecycleService,programLifecycleService,streamCoordinatorClient,servicesNames,handlerHookNames,defaultNamespaceEnsurer,systemArtifactLoader,pluginService,privilegesFetcherProxyService);
  this.metricStore=metricStore;
}","/** 
 * Construct the Standalone AppFabricServer with service factory and configuration coming from guice injection.
 */
@Inject public StandaloneAppFabricServer(CConfiguration configuration,DiscoveryService discoveryService,SchedulerService schedulerService,NotificationService notificationService,@Named(Constants.Service.MASTER_SERVICES_BIND_ADDRESS) InetAddress hostname,@Named(Constants.AppFabric.HANDLERS_BINDING) Set<HttpHandler> handlers,@Nullable MetricsCollectionService metricsCollectionService,ProgramRuntimeService programRuntimeService,ApplicationLifecycleService applicationLifecycleService,ProgramLifecycleService programLifecycleService,StreamCoordinatorClient streamCoordinatorClient,@Named(""String_Node_Str"") Set<String> servicesNames,@Named(""String_Node_Str"") Set<String> handlerHookNames,NamespaceAdmin namespaceAdmin,MetricStore metricStore,SystemArtifactLoader systemArtifactLoader,PluginService pluginService,PrivilegesFetcherProxyService privilegesFetcherProxyService){
  super(configuration,discoveryService,schedulerService,notificationService,hostname,handlers,metricsCollectionService,programRuntimeService,applicationLifecycleService,programLifecycleService,streamCoordinatorClient,servicesNames,handlerHookNames,namespaceAdmin,systemArtifactLoader,pluginService,privilegesFetcherProxyService);
  this.metricStore=metricStore;
}"
5757,"@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  Location deploymentJar=AppJarHelper.createDeploymentJar(new LocalLocationFactory(TMP_FOLDER.newFolder()),InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,deploymentJar.toURI().getPath());
  cConf.set(Constants.Security.Authorization.ADMIN_USERS,ADMIN_USER.getName());
  instanceId=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
  File systemArtifactsDir=TMP_FOLDER.newFolder();
  cConf.set(Constants.AppFabric.SYSTEM_ARTIFACTS_DIR,systemArtifactsDir.getAbsolutePath());
  createSystemArtifact(systemArtifactsDir);
  Injector injector=Guice.createInjector(new AppFabricTestModule(cConf));
  defaultNamespaceEnsurer=injector.getInstance(DefaultNamespaceEnsurer.class);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  txManager=injector.getInstance(TransactionManager.class);
  datasetService=injector.getInstance(DatasetService.class);
  authorizationEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  authorizationEnforcementService.startAndWait();
  systemArtifactLoader=injector.getInstance(SystemArtifactLoader.class);
  authorizationBootstrapper=injector.getInstance(AuthorizationBootstrapper.class);
  namespaceQueryAdmin=injector.getInstance(NamespaceQueryAdmin.class);
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  artifactRepository=injector.getInstance(ArtifactRepository.class);
  dsFramework=injector.getInstance(DatasetFramework.class);
}","@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  Location deploymentJar=AppJarHelper.createDeploymentJar(new LocalLocationFactory(TMP_FOLDER.newFolder()),InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,deploymentJar.toURI().getPath());
  cConf.set(Constants.Security.Authorization.ADMIN_USERS,ADMIN_USER.getName());
  instanceId=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
  File systemArtifactsDir=TMP_FOLDER.newFolder();
  cConf.set(Constants.AppFabric.SYSTEM_ARTIFACTS_DIR,systemArtifactsDir.getAbsolutePath());
  createSystemArtifact(systemArtifactsDir);
  Injector injector=Guice.createInjector(new AppFabricTestModule(cConf));
  namespaceQueryAdmin=injector.getInstance(NamespaceQueryAdmin.class);
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  defaultNamespaceEnsurer=new DefaultNamespaceEnsurer(namespaceAdmin);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  txManager=injector.getInstance(TransactionManager.class);
  datasetService=injector.getInstance(DatasetService.class);
  authorizationEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  authorizationEnforcementService.startAndWait();
  systemArtifactLoader=injector.getInstance(SystemArtifactLoader.class);
  authorizationBootstrapper=injector.getInstance(AuthorizationBootstrapper.class);
  artifactRepository=injector.getInstance(ArtifactRepository.class);
  dsFramework=injector.getInstance(DatasetFramework.class);
}"
5758,"/** 
 * Creates a new instance.
 * @param delegate a {@link Supplier} that gives new instance of the delegating Service.
 * @param retryStrategy strategy to use for retrying
 */
public RetryOnStartFailureService(final Supplier<Service> delegate,final RetryStrategy retryStrategy){
  final Service service=delegate.get();
  this.delegateServiceName=service.getClass().getSimpleName();
  this.startupThread=new Thread(""String_Node_Str"" + delegateServiceName){
    @Override public void run(){
      int failures=0;
      long startTime=System.currentTimeMillis();
      long delay=0L;
      Service delegateService=service;
      while (delay >= 0 && !isInterrupted()) {
        try {
          delegateService.startAndWait();
          currentDelegate=delegateService;
          break;
        }
 catch (        Throwable t) {
          LOG.debug(""String_Node_Str"",delegateServiceName,t);
          delay=retryStrategy.nextRetry(++failures,startTime);
          if (delay < 0) {
            LOG.error(""String_Node_Str"",delegateServiceName,failures,System.currentTimeMillis() - startTime);
            notifyFailed(t);
            break;
          }
          try {
            TimeUnit.MILLISECONDS.sleep(delay);
            LOG.debug(""String_Node_Str"",delegateServiceName);
            delegateService=delegate.get();
          }
 catch (          InterruptedException e) {
            interrupt();
          }
        }
      }
      if (isInterrupted()) {
        LOG.warn(""String_Node_Str"",delegateServiceName);
      }
    }
  }
;
}","/** 
 * Creates a new instance.
 * @param delegate a {@link Supplier} that gives new instance of the delegating Service.
 * @param retryStrategy strategy to use for retrying
 */
public RetryOnStartFailureService(final Supplier<Service> delegate,final RetryStrategy retryStrategy){
  final Service service=delegate.get();
  this.delegateServiceName=service.getClass().getSimpleName();
  this.startupThread=new Thread(""String_Node_Str"" + delegateServiceName){
    @Override public void run(){
      int failures=0;
      long startTime=System.currentTimeMillis();
      long delay=0L;
      Service delegateService=service;
      while (delay >= 0 && !isInterrupted()) {
        try {
          currentDelegate=delegateService;
          delegateService.start().get();
          break;
        }
 catch (        InterruptedException e) {
          interrupt();
          break;
        }
catch (        Throwable t) {
          LOG.debug(""String_Node_Str"",delegateServiceName,t);
          delay=retryStrategy.nextRetry(++failures,startTime);
          if (delay < 0) {
            LOG.error(""String_Node_Str"",delegateServiceName,failures,System.currentTimeMillis() - startTime);
            notifyFailed(t);
            break;
          }
          try {
            TimeUnit.MILLISECONDS.sleep(delay);
            LOG.debug(""String_Node_Str"",delegateServiceName);
            delegateService=delegate.get();
          }
 catch (          InterruptedException e) {
            interrupt();
          }
        }
      }
      if (isInterrupted()) {
        LOG.warn(""String_Node_Str"",delegateServiceName);
      }
    }
  }
;
}"
5759,"@Override public void run(){
  int failures=0;
  long startTime=System.currentTimeMillis();
  long delay=0L;
  Service delegateService=service;
  while (delay >= 0 && !isInterrupted()) {
    try {
      delegateService.startAndWait();
      currentDelegate=delegateService;
      break;
    }
 catch (    Throwable t) {
      LOG.debug(""String_Node_Str"",delegateServiceName,t);
      delay=retryStrategy.nextRetry(++failures,startTime);
      if (delay < 0) {
        LOG.error(""String_Node_Str"",delegateServiceName,failures,System.currentTimeMillis() - startTime);
        notifyFailed(t);
        break;
      }
      try {
        TimeUnit.MILLISECONDS.sleep(delay);
        LOG.debug(""String_Node_Str"",delegateServiceName);
        delegateService=delegate.get();
      }
 catch (      InterruptedException e) {
        interrupt();
      }
    }
  }
  if (isInterrupted()) {
    LOG.warn(""String_Node_Str"",delegateServiceName);
  }
}","@Override public void run(){
  int failures=0;
  long startTime=System.currentTimeMillis();
  long delay=0L;
  Service delegateService=service;
  while (delay >= 0 && !isInterrupted()) {
    try {
      currentDelegate=delegateService;
      delegateService.start().get();
      break;
    }
 catch (    InterruptedException e) {
      interrupt();
      break;
    }
catch (    Throwable t) {
      LOG.debug(""String_Node_Str"",delegateServiceName,t);
      delay=retryStrategy.nextRetry(++failures,startTime);
      if (delay < 0) {
        LOG.error(""String_Node_Str"",delegateServiceName,failures,System.currentTimeMillis() - startTime);
        notifyFailed(t);
        break;
      }
      try {
        TimeUnit.MILLISECONDS.sleep(delay);
        LOG.debug(""String_Node_Str"",delegateServiceName);
        delegateService=delegate.get();
      }
 catch (      InterruptedException e) {
        interrupt();
      }
    }
  }
  if (isInterrupted()) {
    LOG.warn(""String_Node_Str"",delegateServiceName);
  }
}"
5760,"@Test(timeout=5000) public void testFailureStop() throws InterruptedException {
  CountDownLatch failureLatch=new CountDownLatch(1);
  Service service=new RetryOnStartFailureService(createServiceSupplier(1000,new CountDownLatch(1),failureLatch),RetryStrategies.fixDelay(10,TimeUnit.MILLISECONDS));
  service.startAndWait();
  Assert.assertTrue(failureLatch.await(1,TimeUnit.SECONDS));
  service.stopAndWait();
}","@Test(timeout=5000) public void testFailureStop() throws InterruptedException {
  CountDownLatch failureLatch=new CountDownLatch(1);
  Service service=new RetryOnStartFailureService(createServiceSupplier(1000,new CountDownLatch(1),failureLatch),RetryStrategies.fixDelay(10,TimeUnit.MILLISECONDS));
  service.startAndWait();
  Assert.assertTrue(failureLatch.await(1,TimeUnit.SECONDS));
  try {
    service.stopAndWait();
    Assert.fail();
  }
 catch (  UncheckedExecutionException e) {
    Throwable rootCause=Throwables.getRootCause(e);
    Assert.assertEquals(RuntimeException.class,rootCause.getClass());
    Assert.assertEquals(""String_Node_Str"",rootCause.getMessage());
  }
}"
5761,"/** 
 * Create a composite record writer that can write key/value data to different output files.
 * @return a composite record writer
 * @throws IOException
 */
@Override public RecordWriter<K,V> getRecordWriter(final TaskAttemptContext job) throws IOException {
  final String outputName=FileOutputFormat.getOutputName(job);
  Configuration configuration=job.getConfiguration();
  Class<? extends DynamicPartitioner> partitionerClass=configuration.getClass(PartitionedFileSetArguments.DYNAMIC_PARTITIONER_CLASS_NAME,null,DynamicPartitioner.class);
  @SuppressWarnings(""String_Node_Str"") final DynamicPartitioner<K,V> dynamicPartitioner=new InstantiatorFactory(false).get(TypeToken.of(partitionerClass)).create();
  MapReduceClassLoader classLoader=MapReduceClassLoader.getFromConfiguration(configuration);
  final BasicMapReduceTaskContext<K,V> taskContext=classLoader.getTaskContextProvider().get(job);
  String outputDatasetName=configuration.get(Constants.Dataset.Partitioned.HCONF_ATTR_OUTPUT_DATASET);
  PartitionedFileSet outputDataset=taskContext.getDataset(outputDatasetName);
  final Partitioning partitioning=outputDataset.getPartitioning();
  dynamicPartitioner.initialize(taskContext);
  return new RecordWriter<K,V>(){
    Map<PartitionKey,RecordWriter<K,V>> recordWriters=new HashMap<>();
    Map<PartitionKey,TaskAttemptContext> contexts=new HashMap<>();
    public void write(    K key,    V value) throws IOException, InterruptedException {
      PartitionKey partitionKey=dynamicPartitioner.getPartitionKey(key,value);
      RecordWriter<K,V> rw=this.recordWriters.get(partitionKey);
      if (rw == null) {
        String relativePath=PartitionedFileSetDataset.getOutputPath(partitionKey,partitioning);
        String finalPath=relativePath + ""String_Node_Str"" + outputName;
        TaskAttemptContext taskAttemptContext=getTaskAttemptContext(job,finalPath);
        rw=getBaseRecordWriter(taskAttemptContext);
        this.recordWriters.put(partitionKey,rw);
        this.contexts.put(partitionKey,taskAttemptContext);
      }
      rw.write(key,value);
    }
    @Override public void close(    TaskAttemptContext context) throws IOException, InterruptedException {
      try {
        Map<PartitionKey,RecordWriter<?,?>> recordWriters=new HashMap<>();
        recordWriters.putAll(this.recordWriters);
        MultipleOutputs.closeRecordWriters(recordWriters,contexts);
        taskContext.flushOperations();
      }
 catch (      Exception e) {
        throw new IOException(e);
      }
 finally {
        dynamicPartitioner.destroy();
      }
    }
  }
;
}","/** 
 * Create a composite record writer that can write key/value data to different output files.
 * @return a composite record writer
 * @throws IOException
 */
@Override public RecordWriter<K,V> getRecordWriter(final TaskAttemptContext job) throws IOException {
  boolean concurrencyAllowed=job.getConfiguration().getBoolean(PartitionedFileSetArguments.DYNAMIC_PARTITIONER_ALLOW_CONCURRENCY,true);
  if (concurrencyAllowed) {
    return new MultiWriter<>(job);
  }
 else {
    return new SingleWriter<>(job);
  }
}"
5762,"private static Path createJobSpecificPath(Path path,JobContext jobContext){
  String outputPathSuffix=""String_Node_Str"" + jobContext.getJobID().getId();
  return new Path(path,outputPathSuffix);
}","static Path createJobSpecificPath(Path path,JobContext jobContext){
  String outputPathSuffix=""String_Node_Str"" + jobContext.getJobID().getId();
  return new Path(path,outputPathSuffix);
}"
5763,"@Override protected void setup(Reducer.Context context) throws IOException, InterruptedException {
  metrics.gauge(""String_Node_Str"",1);
  LOG.info(""String_Node_Str"");
  long reducersCount=counters.incrementAndGet(new Increment(""String_Node_Str"",""String_Node_Str"",1L)).getLong(""String_Node_Str"",0);
  Assert.assertEquals(reducersCount,countersFromContext.incrementAndGet(new Increment(""String_Node_Str"",""String_Node_Str"",1L)).getLong(""String_Node_Str"",0));
  LOG.info(""String_Node_Str"" + reducersCount);
  metrics.gauge(""String_Node_Str"",1);
}","@Override protected void setup(Reducer.Context context) throws IOException, InterruptedException {
  metrics.gauge(""String_Node_Str"",1);
  LOG.info(""String_Node_Str"");
  long reducersCount=counters.incrementAndGet(new Increment(""String_Node_Str"",""String_Node_Str"",1L)).getLong(""String_Node_Str"",0);
  LOG.info(""String_Node_Str"" + reducersCount);
  metrics.gauge(""String_Node_Str"",1);
}"
5764,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] appAndServiceId=arguments.get(ArgumentName.SERVICE.toString()).split(""String_Node_Str"");
  if (appAndServiceId.length < 2) {
    throw new CommandInputError(this);
  }
  String appId=appAndServiceId[0];
  String serviceName=appAndServiceId[1];
  Id.Service serviceId=Id.Service.from(cliConfig.getCurrentNamespace(),appId,serviceName);
  output.println(serviceClient.getAvailability(serviceId));
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] appAndServiceId=arguments.get(ArgumentName.SERVICE.toString()).split(""String_Node_Str"");
  if (appAndServiceId.length < 2) {
    throw new CommandInputError(this);
  }
  String appId=appAndServiceId[0];
  String serviceName=appAndServiceId[1];
  Id.Service serviceId=Id.Service.from(cliConfig.getCurrentNamespace(),appId,serviceName);
  serviceClient.checkAvailability(serviceId);
  output.println(""String_Node_Str"");
}"
5765,"@Before public void setUp() throws Throwable {
  super.setUp();
  appClient=new ApplicationClient(clientConfig);
  serviceClient=new ServiceClient(clientConfig);
  programClient=new ProgramClient(clientConfig);
  try {
    serviceClient.getAvailability(service);
    Assert.fail();
  }
 catch (  NotFoundException ex) {
  }
  appClient.deploy(namespace,createAppJarFile(FakeApp.class));
  try {
    serviceClient.getAvailability(service);
    Assert.fail();
  }
 catch (  ServiceUnavailableException ex) {
  }
  programClient.start(service);
  assertProgramRunning(programClient,service);
}","@Before public void setUp() throws Throwable {
  super.setUp();
  appClient=new ApplicationClient(clientConfig);
  serviceClient=new ServiceClient(clientConfig);
  programClient=new ProgramClient(clientConfig);
  try {
    serviceClient.checkAvailability(service);
    Assert.fail();
  }
 catch (  NotFoundException ex) {
  }
  appClient.deploy(namespace,createAppJarFile(FakeApp.class));
  try {
    serviceClient.checkAvailability(service);
    Assert.fail();
  }
 catch (  ServiceUnavailableException ex) {
  }
  programClient.start(service);
  assertProgramRunning(programClient,service);
}"
5766,"@Test public void testActiveStatus() throws Exception {
  String responseBody=serviceClient.getAvailability(service);
  Assert.assertTrue(responseBody.contains(""String_Node_Str""));
}","@Test public void testActiveStatus() throws Exception {
  serviceClient.checkAvailability(service);
}"
5767,"@Override public URL getServiceURL(long timeout,TimeUnit timeoutUnit){
  return getServiceURL();
}","@Override public URL getServiceURL(long timeout,TimeUnit timeoutUnit){
  try {
    Tasks.waitFor(true,new Callable<Boolean>(){
      @Override public Boolean call() throws Exception {
        try {
          serviceClient.checkAvailability(serviceId);
          return true;
        }
 catch (        ServiceUnavailableException e) {
          return false;
        }
      }
    }
,timeout,timeoutUnit);
    return serviceClient.getServiceURL(serviceId);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
5768,"@Override public Connection connect(String url,Properties info) throws SQLException {
  if (!acceptsURL(url)) {
    return null;
  }
  ExploreConnectionParams params=ExploreConnectionParams.parseConnectionUrl(url);
  String authToken=getString(params,ExploreConnectionParams.Info.EXPLORE_AUTH_TOKEN,null);
  String namespace=getString(params,ExploreConnectionParams.Info.NAMESPACE,Id.Namespace.DEFAULT.getId());
  boolean sslEnabled=getBoolean(params,ExploreConnectionParams.Info.SSL_ENABLED,false);
  boolean verifySSLCert=getBoolean(params,ExploreConnectionParams.Info.VERIFY_SSL_CERT,true);
  ExploreClient exploreClient=new FixedAddressExploreClient(params.getHost(),params.getPort(),authToken,sslEnabled,verifySSLCert);
  if (!exploreClient.isServiceAvailable()) {
    throw new SQLException(""String_Node_Str"" + url + ""String_Node_Str"");
  }
  return new ExploreConnection(exploreClient,namespace,params);
}","@Override public Connection connect(String url,Properties info) throws SQLException {
  if (!acceptsURL(url)) {
    return null;
  }
  ExploreConnectionParams params=ExploreConnectionParams.parseConnectionUrl(url);
  String authToken=getString(params,ExploreConnectionParams.Info.EXPLORE_AUTH_TOKEN,null);
  String namespace=getString(params,ExploreConnectionParams.Info.NAMESPACE,Id.Namespace.DEFAULT.getId());
  boolean sslEnabled=getBoolean(params,ExploreConnectionParams.Info.SSL_ENABLED,false);
  boolean verifySSLCert=getBoolean(params,ExploreConnectionParams.Info.VERIFY_SSL_CERT,true);
  ExploreClient exploreClient=new FixedAddressExploreClient(params.getHost(),params.getPort(),authToken,sslEnabled,verifySSLCert);
  try {
    exploreClient.ping();
  }
 catch (  UnauthenticatedException e) {
    throw new SQLException(""String_Node_Str"" + url + ""String_Node_Str"");
  }
catch (  ServiceUnavailableException|ExploreException e) {
    throw new SQLException(""String_Node_Str"" + url + ""String_Node_Str"");
  }
  return new ExploreConnection(exploreClient,namespace,params);
}"
5769,"protected static void initialize(CConfiguration cConf,TemporaryFolder tmpFolder,boolean useStandalone,boolean enableAuthorization) throws Exception {
  if (!runBefore) {
    return;
  }
  Configuration hConf=new Configuration();
  if (enableAuthorization) {
    LocationFactory locationFactory=new LocalLocationFactory(tmpFolder.newFolder());
    Location authExtensionJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class);
    cConf.setBoolean(Constants.Security.ENABLED,true);
    cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
    cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,authExtensionJar.toURI().getPath());
    cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
    cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  }
  List<Module> modules=useStandalone ? createStandaloneModules(cConf,hConf,tmpFolder) : createInMemoryModules(cConf,hConf,tmpFolder);
  injector=Guice.createInjector(modules);
  authorizationEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  if (enableAuthorization) {
    injector.getInstance(AuthorizationBootstrapper.class).run();
    authorizationEnforcementService.startAndWait();
  }
  transactionManager=injector.getInstance(TransactionManager.class);
  transactionManager.startAndWait();
  dsOpService=injector.getInstance(DatasetOpExecutor.class);
  dsOpService.startAndWait();
  datasetService=injector.getInstance(DatasetService.class);
  datasetService.startAndWait();
  exploreExecutorService=injector.getInstance(ExploreExecutorService.class);
  exploreExecutorService.startAndWait();
  datasetFramework=injector.getInstance(DatasetFramework.class);
  exploreClient=injector.getInstance(ExploreClient.class);
  exploreService=injector.getInstance(ExploreService.class);
  Assert.assertTrue(exploreClient.isServiceAvailable());
  notificationService=injector.getInstance(NotificationService.class);
  notificationService.startAndWait();
  streamService=injector.getInstance(StreamService.class);
  streamService.startAndWait();
  streamHttpService=injector.getInstance(StreamHttpService.class);
  streamHttpService.startAndWait();
  exploreTableManager=injector.getInstance(ExploreTableManager.class);
  streamAdmin=injector.getInstance(StreamAdmin.class);
  streamMetaStore=injector.getInstance(StreamMetaStore.class);
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  createNamespace(NamespaceId.DEFAULT);
  createNamespace(NAMESPACE_ID.toEntityId());
  createNamespace(OTHER_NAMESPACE_ID.toEntityId());
}","protected static void initialize(CConfiguration cConf,TemporaryFolder tmpFolder,boolean useStandalone,boolean enableAuthorization) throws Exception {
  if (!runBefore) {
    return;
  }
  Configuration hConf=new Configuration();
  if (enableAuthorization) {
    LocationFactory locationFactory=new LocalLocationFactory(tmpFolder.newFolder());
    Location authExtensionJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class);
    cConf.setBoolean(Constants.Security.ENABLED,true);
    cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
    cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,authExtensionJar.toURI().getPath());
    cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
    cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  }
  List<Module> modules=useStandalone ? createStandaloneModules(cConf,hConf,tmpFolder) : createInMemoryModules(cConf,hConf,tmpFolder);
  injector=Guice.createInjector(modules);
  authorizationEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  if (enableAuthorization) {
    injector.getInstance(AuthorizationBootstrapper.class).run();
    authorizationEnforcementService.startAndWait();
  }
  transactionManager=injector.getInstance(TransactionManager.class);
  transactionManager.startAndWait();
  dsOpService=injector.getInstance(DatasetOpExecutor.class);
  dsOpService.startAndWait();
  datasetService=injector.getInstance(DatasetService.class);
  datasetService.startAndWait();
  exploreExecutorService=injector.getInstance(ExploreExecutorService.class);
  exploreExecutorService.startAndWait();
  datasetFramework=injector.getInstance(DatasetFramework.class);
  exploreClient=injector.getInstance(ExploreClient.class);
  exploreService=injector.getInstance(ExploreService.class);
  exploreClient.ping();
  notificationService=injector.getInstance(NotificationService.class);
  notificationService.startAndWait();
  streamService=injector.getInstance(StreamService.class);
  streamService.startAndWait();
  streamHttpService=injector.getInstance(StreamHttpService.class);
  streamHttpService.startAndWait();
  exploreTableManager=injector.getInstance(ExploreTableManager.class);
  streamAdmin=injector.getInstance(StreamAdmin.class);
  streamMetaStore=injector.getInstance(StreamMetaStore.class);
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  createNamespace(NamespaceId.DEFAULT);
  createNamespace(NAMESPACE_ID.toEntityId());
  createNamespace(OTHER_NAMESPACE_ID.toEntityId());
}"
5770,"@BeforeClass public static void start() throws Exception {
  Injector injector=Guice.createInjector(createInMemoryModules(CConfiguration.create(),new Configuration()));
  transactionManager=injector.getInstance(TransactionManager.class);
  transactionManager.startAndWait();
  dsOpExecutor=injector.getInstance(DatasetOpExecutor.class);
  dsOpExecutor.startAndWait();
  datasetService=injector.getInstance(DatasetService.class);
  datasetService.startAndWait();
  exploreClient=injector.getInstance(DiscoveryExploreClient.class);
  Assert.assertFalse(exploreClient.isServiceAvailable());
  datasetFramework=injector.getInstance(DatasetFramework.class);
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  NamespacedLocationFactory namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  NamespaceMeta namespaceMeta=new NamespaceMeta.Builder().setName(namespaceId).build();
  namespaceAdmin.create(namespaceMeta);
  namespacedLocationFactory.get(namespaceId).mkdirs();
  exploreClient.addNamespace(namespaceMeta);
}","@BeforeClass public static void start() throws Exception {
  Injector injector=Guice.createInjector(createInMemoryModules(CConfiguration.create(),new Configuration()));
  transactionManager=injector.getInstance(TransactionManager.class);
  transactionManager.startAndWait();
  dsOpExecutor=injector.getInstance(DatasetOpExecutor.class);
  dsOpExecutor.startAndWait();
  datasetService=injector.getInstance(DatasetService.class);
  datasetService.startAndWait();
  exploreClient=injector.getInstance(DiscoveryExploreClient.class);
  try {
    exploreClient.ping();
    Assert.fail(""String_Node_Str"");
  }
 catch (  Exception e) {
    Assert.assertTrue(e.getMessage().contains(""String_Node_Str"" + Constants.Service.EXPLORE_HTTP_USER_SERVICE));
  }
  datasetFramework=injector.getInstance(DatasetFramework.class);
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  NamespacedLocationFactory namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  NamespaceMeta namespaceMeta=new NamespaceMeta.Builder().setName(namespaceId).build();
  namespaceAdmin.create(namespaceMeta);
  namespacedLocationFactory.get(namespaceId).mkdirs();
  exploreClient.addNamespace(namespaceMeta);
}"
5771,"@Override public Connection getQueryClient(Id.Namespace namespace) throws Exception {
  ConnectionConfig connConfig=clientConfig.getConnectionConfig();
  String url=String.format(""String_Node_Str"",Constants.Explore.Jdbc.URL_PREFIX,connConfig.getHostname(),connConfig.getPort(),namespace.getId());
  return new ExploreDriver().connect(url,new Properties());
}","@Override public Connection getQueryClient(Id.Namespace namespace) throws Exception {
  Map<String,String> connParams=new HashMap<>();
  connParams.put(ExploreConnectionParams.Info.NAMESPACE.getName(),namespace.getId());
  AccessToken accessToken=clientConfig.getAccessToken();
  if (accessToken != null) {
    connParams.put(ExploreConnectionParams.Info.EXPLORE_AUTH_TOKEN.getName(),accessToken.getValue());
  }
  connParams.put(ExploreConnectionParams.Info.SSL_ENABLED.getName(),Boolean.toString(clientConfig.getConnectionConfig().isSSLEnabled()));
  connParams.put(ExploreConnectionParams.Info.VERIFY_SSL_CERT.getName(),Boolean.toString(clientConfig.isVerifySSLCert()));
  ConnectionConfig connConfig=clientConfig.getConnectionConfig();
  String url=String.format(""String_Node_Str"",Constants.Explore.Jdbc.URL_PREFIX,connConfig.getHostname(),connConfig.getPort(),Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(connParams));
  return new ExploreDriver().connect(url,new Properties());
}"
5772,"/** 
 * Create a namespace in the File System and Hive.
 * @param namespaceMeta {@link NamespaceMeta} for the namespace to create
 * @throws IOException if there are errors while creating the namespace in the File System
 * @throws ExploreException if there are errors while creating the namespace in Hive
 * @throws SQLException if there are errors while creating the namespace in Hive
 */
@Override public void create(NamespaceMeta namespaceMeta) throws IOException, ExploreException, SQLException {
  createLocation(namespaceMeta);
  if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED)) {
    try {
      exploreFacade.createNamespace(namespaceMeta);
    }
 catch (    ExploreException|SQLException e) {
      deleteLocation(namespaceMeta.getNamespaceId());
      throw e;
    }
  }
}","/** 
 * Create a namespace in the File System and Hive.
 * @param namespaceMeta {@link NamespaceMeta} for the namespace to create
 * @throws IOException if there are errors while creating the namespace in the File System
 * @throws ExploreException if there are errors while creating the namespace in Hive
 * @throws SQLException if there are errors while creating the namespace in Hive
 */
@Override public void create(NamespaceMeta namespaceMeta) throws IOException, ExploreException, SQLException {
  createLocation(namespaceMeta);
  if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED)) {
    try {
      exploreFacade.createNamespace(namespaceMeta);
    }
 catch (    ExploreException|SQLException e) {
      try {
        deleteLocation(namespaceMeta.getNamespaceId());
      }
 catch (      Exception e2) {
        e.addSuppressed(e2);
      }
      throw e;
    }
  }
}"
5773,"@Override public void create(NamespaceMeta namespaceMeta) throws IOException, ExploreException, SQLException {
  super.create(namespaceMeta);
  if (NamespaceId.DEFAULT.equals(namespaceMeta.getNamespaceId())) {
    return;
  }
  String hbaseNamespace=tableUtil.getHBaseNamespace(namespaceMeta);
  try (HBaseAdmin admin=new HBaseAdmin(hConf)){
    if (Strings.isNullOrEmpty(namespaceMeta.getConfig().getHbaseNamespace())) {
      try {
        tableUtil.createNamespaceIfNotExists(admin,hbaseNamespace);
      }
 catch (      Throwable t) {
        super.delete(namespaceMeta.getNamespaceId());
        throw t;
      }
    }
    if (!tableUtil.hasNamespace(admin,hbaseNamespace)) {
      throw new IOException(String.format(""String_Node_Str"",hbaseNamespace,namespaceMeta.getName()));
    }
  }
 }","@Override public void create(NamespaceMeta namespaceMeta) throws IOException, ExploreException, SQLException {
  super.create(namespaceMeta);
  if (NamespaceId.DEFAULT.equals(namespaceMeta.getNamespaceId())) {
    return;
  }
  String hbaseNamespace=tableUtil.getHBaseNamespace(namespaceMeta);
  try (HBaseAdmin admin=new HBaseAdmin(hConf)){
    if (Strings.isNullOrEmpty(namespaceMeta.getConfig().getHbaseNamespace())) {
      try {
        tableUtil.createNamespaceIfNotExists(admin,hbaseNamespace);
      }
 catch (      Throwable t) {
        try {
          super.delete(namespaceMeta.getNamespaceId());
        }
 catch (        Exception e) {
          t.addSuppressed(e);
        }
        throw t;
      }
    }
    if (!tableUtil.hasNamespace(admin,hbaseNamespace)) {
      throw new IOException(String.format(""String_Node_Str"",hbaseNamespace,namespaceMeta.getName()));
    }
  }
 }"
5774,"public String getPrompt(CLIConnectionConfig config){
  try {
    return ""String_Node_Str"" + config.getURI().resolve(""String_Node_Str"" + config.getNamespace()) + ""String_Node_Str"";
  }
 catch (  DisconnectedException e) {
    return ""String_Node_Str"";
  }
}","public String getPrompt(CLIConnectionConfig config){
  try {
    return ""String_Node_Str"" + config.getURI().resolve(""String_Node_Str"" + config.getNamespace().getId()) + ""String_Node_Str"";
  }
 catch (  DisconnectedException e) {
    return ""String_Node_Str"";
  }
}"
5775,"@Override public void onReceived(Iterator<FetchedMessage> messages){
  try {
    if (stopLatch.await(1,TimeUnit.NANOSECONDS)) {
      LOG.debug(""String_Node_Str"");
      return;
    }
  }
 catch (  InterruptedException e) {
    LOG.error(""String_Node_Str"",e);
    Thread.currentThread().interrupt();
    return;
  }
  long oldestProcessed=Long.MAX_VALUE;
  List<KafkaLogEvent> events=Lists.newArrayList();
  while (messages.hasNext()) {
    FetchedMessage message=messages.next();
    try {
      GenericRecord genericRecord=serializer.toGenericRecord(message.getPayload());
      ILoggingEvent event=serializer.fromGenericRecord(genericRecord);
      LoggingContext loggingContext=LoggingContextHelper.getLoggingContext(event.getMDCPropertyMap());
      KafkaLogEvent logEvent=new KafkaLogEvent(genericRecord,event,loggingContext,message.getTopicPartition().getPartition(),message.getNextOffset());
      events.add(logEvent);
      if (event.getTimeStamp() < oldestProcessed) {
        oldestProcessed=event.getTimeStamp();
      }
    }
 catch (    Throwable th) {
      LOG.error(""String_Node_Str"",message.getTopicPartition().getTopic(),message.getTopicPartition().getPartition());
    }
  }
  int count=events.size();
  if (!events.isEmpty()) {
    for (    KafkaLogProcessor processor : kafkaLogProcessors) {
      try {
        processor.process(events.iterator());
      }
 catch (      Throwable th) {
        LOG.error(""String_Node_Str"",processor.getClass().getSimpleName());
      }
    }
    metricsContext.gauge(delayMetric,System.currentTimeMillis() - oldestProcessed);
    metricsContext.increment(Constants.Metrics.Name.Log.PROCESS_MESSAGES_COUNT,count);
  }
  LOG.trace(""String_Node_Str"",count);
}","@Override public void onReceived(Iterator<FetchedMessage> messages){
  try {
    if (stopLatch.await(1,TimeUnit.NANOSECONDS)) {
      LOG.debug(""String_Node_Str"");
      return;
    }
  }
 catch (  InterruptedException e) {
    LOG.error(""String_Node_Str"",e);
    Thread.currentThread().interrupt();
    return;
  }
  long oldestProcessed=Long.MAX_VALUE;
  List<KafkaLogEvent> events=Lists.newArrayList();
  while (messages.hasNext()) {
    FetchedMessage message=messages.next();
    try {
      GenericRecord genericRecord=serializer.toGenericRecord(message.getPayload());
      ILoggingEvent event=serializer.fromGenericRecord(genericRecord);
      LOG.trace(""String_Node_Str"",event,partition);
      LoggingContext loggingContext=LoggingContextHelper.getLoggingContext(event.getMDCPropertyMap());
      KafkaLogEvent logEvent=new KafkaLogEvent(genericRecord,event,loggingContext,message.getTopicPartition().getPartition(),message.getNextOffset());
      events.add(logEvent);
      if (event.getTimeStamp() < oldestProcessed) {
        oldestProcessed=event.getTimeStamp();
      }
    }
 catch (    Throwable th) {
      LOG.warn(""String_Node_Str"",message.getNextOffset(),message.getTopicPartition().getTopic(),message.getTopicPartition().getPartition(),th);
    }
  }
  int count=events.size();
  if (!events.isEmpty()) {
    for (    KafkaLogProcessor processor : kafkaLogProcessors) {
      try {
        processor.process(events.iterator());
      }
 catch (      Throwable th) {
        LOG.warn(""String_Node_Str"",events.size(),processor.getClass().getSimpleName(),th);
      }
    }
    metricsContext.gauge(delayMetric,System.currentTimeMillis() - oldestProcessed);
    metricsContext.increment(Constants.Metrics.Name.Log.PROCESS_MESSAGES_COUNT,count);
  }
  LOG.trace(""String_Node_Str"",count);
}"
5776,"@Override public CloseableIterator<LogEvent> getLog(LoggingContext loggingContext,final long fromTimeMs,final long toTimeMs,Filter filter){
  try {
    final Filter logFilter=new AndFilter(ImmutableList.of(LoggingContextHelper.createFilter(loggingContext),filter));
    LOG.trace(""String_Node_Str"",fromTimeMs,toTimeMs);
    NavigableMap<Long,Location> sortedFiles=fileMetaDataManager.listFiles(loggingContext);
    if (sortedFiles.isEmpty()) {
      return null;
    }
    List<Location> filesInRange=getFilesInRange(sortedFiles,fromTimeMs,toTimeMs);
    final AvroFileReader avroFileReader=new AvroFileReader(schema);
    final Iterator<Location> filesIter=filesInRange.iterator();
    final NamespaceId namespaceId=LoggingContextHelper.getNamespaceId(loggingContext);
    CloseableIterator closeableIterator=new CloseableIterator(){
      private CloseableIterator<LogEvent> curr=null;
      @Override public void close(){
        if (curr != null) {
          curr.close();
        }
      }
      @Override public boolean hasNext(){
        return filesIter.hasNext();
      }
      @Override public CloseableIterator<LogEvent> next(){
        if (curr != null) {
          curr.close();
        }
        Location file=filesIter.next();
        LOG.trace(""String_Node_Str"",file);
        curr=avroFileReader.readLog(file,logFilter,fromTimeMs,toTimeMs,Integer.MAX_VALUE,namespaceId,impersonator);
        return curr;
      }
      @Override public void remove(){
        throw new UnsupportedOperationException(""String_Node_Str"");
      }
    }
;
    return concat(closeableIterator);
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}","@Override public CloseableIterator<LogEvent> getLog(LoggingContext loggingContext,final long fromTimeMs,final long toTimeMs,Filter filter){
  try {
    final Filter logFilter=new AndFilter(ImmutableList.of(LoggingContextHelper.createFilter(loggingContext),filter));
    LOG.trace(""String_Node_Str"",fromTimeMs,toTimeMs);
    NavigableMap<Long,Location> sortedFiles=fileMetaDataManager.listFiles(loggingContext);
    if (sortedFiles.isEmpty()) {
      return new AbstractCloseableIterator<LogEvent>(){
        @Override protected LogEvent computeNext(){
          endOfData();
          return null;
        }
        @Override public void close(){
        }
      }
;
    }
    List<Location> filesInRange=getFilesInRange(sortedFiles,fromTimeMs,toTimeMs);
    final AvroFileReader avroFileReader=new AvroFileReader(schema);
    final Iterator<Location> filesIter=filesInRange.iterator();
    final NamespaceId namespaceId=LoggingContextHelper.getNamespaceId(loggingContext);
    CloseableIterator closeableIterator=new CloseableIterator(){
      private CloseableIterator<LogEvent> curr=null;
      @Override public void close(){
        if (curr != null) {
          curr.close();
        }
      }
      @Override public boolean hasNext(){
        return filesIter.hasNext();
      }
      @Override public CloseableIterator<LogEvent> next(){
        if (curr != null) {
          curr.close();
        }
        Location file=filesIter.next();
        LOG.trace(""String_Node_Str"",file);
        curr=avroFileReader.readLog(file,logFilter,fromTimeMs,toTimeMs,Integer.MAX_VALUE,namespaceId,impersonator);
        return curr;
      }
      @Override public void remove(){
        throw new UnsupportedOperationException(""String_Node_Str"");
      }
    }
;
    return concat(closeableIterator);
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}"
5777,"@Test public void testPlugin() throws Exception {
  try {
    startLogSaver();
    publishLogs();
    LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
    String logBaseDir=cConf.get(LoggingConfiguration.LOG_BASE_DIR);
    Location ns1LogBaseDir=locationFactory.create(namespaceDir).append(""String_Node_Str"").append(logBaseDir);
    FlowletLoggingContext loggingContext=new FlowletLoggingContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
    FileMetaDataManager fileMetaDataManager=injector.getInstance(FileMetaDataManager.class);
    waitTillLogSaverDone(fileMetaDataManager,loggingContext,""String_Node_Str"");
    testLogRead(loggingContext,logBaseDir);
    FileLogReader fileLogReader=injector.getInstance(FileLogReader.class);
    List<LogEvent> events=Lists.newArrayList(fileLogReader.getLog(new FlowletLoggingContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str""),0,Long.MAX_VALUE,Filter.EMPTY_FILTER));
    Assert.assertEquals(60,events.size());
    stopLogSaver();
    verifyCheckpoint();
    verifyMetricsPlugin(60L);
    LogSaverTableUtilOverride.setLogMetaTableName(""String_Node_Str"");
    resetLogSaverPluginCheckpoint(10);
    String logBaseDir1=logBaseDir + ""String_Node_Str"";
    cConf.set(LoggingConfiguration.LOG_BASE_DIR,logBaseDir1);
    Location ns1LogBaseDir1=locationFactory.create(namespaceDir).append(""String_Node_Str"").append(logBaseDir1);
    startLogSaver();
    cConf.set(LoggingConfiguration.LOG_BASE_DIR,logBaseDir);
    fileMetaDataManager=injector.getInstance(FileMetaDataManager.class);
    waitTillLogSaverDone(fileMetaDataManager,loggingContext,""String_Node_Str"");
    stopLogSaver();
    fileLogReader=injector.getInstance(FileLogReader.class);
    events=Lists.newArrayList(fileLogReader.getLog(new FlowletLoggingContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str""),0,Long.MAX_VALUE,Filter.EMPTY_FILTER));
    Assert.assertEquals(50,events.size());
    verifyCheckpoint();
    verifyMetricsPlugin(110L);
  }
 catch (  Throwable t) {
    try {
      final Multimap<String,String> contextMessages=getPublishedKafkaMessages();
      LOG.info(""String_Node_Str"",contextMessages);
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",e);
    }
    throw t;
  }
}","@Test public void testPlugin() throws Exception {
  try {
    startLogSaver();
    FlowletLoggingContext loggingContext=new FlowletLoggingContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
    FileLogReader fileLogReader=injector.getInstance(FileLogReader.class);
    try (CloseableIterator<LogEvent> events=fileLogReader.getLog(loggingContext,0,Long.MAX_VALUE,Filter.EMPTY_FILTER)){
      Assert.assertFalse(events.hasNext());
    }
     publishLogs();
    LocationFactory locationFactory=injector.getInstance(LocationFactory.class);
    String logBaseDir=cConf.get(LoggingConfiguration.LOG_BASE_DIR);
    Location ns1LogBaseDir=locationFactory.create(namespaceDir).append(""String_Node_Str"").append(logBaseDir);
    FileMetaDataManager fileMetaDataManager=injector.getInstance(FileMetaDataManager.class);
    waitTillLogSaverDone(fileMetaDataManager,loggingContext,""String_Node_Str"");
    testLogRead(loggingContext,logBaseDir);
    List<LogEvent> events=Lists.newArrayList(fileLogReader.getLog(new FlowletLoggingContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str""),0,Long.MAX_VALUE,Filter.EMPTY_FILTER));
    Assert.assertEquals(60,events.size());
    stopLogSaver();
    verifyCheckpoint();
    verifyMetricsPlugin(60L);
    LogSaverTableUtilOverride.setLogMetaTableName(""String_Node_Str"");
    resetLogSaverPluginCheckpoint(10);
    String logBaseDir1=logBaseDir + ""String_Node_Str"";
    cConf.set(LoggingConfiguration.LOG_BASE_DIR,logBaseDir1);
    Location ns1LogBaseDir1=locationFactory.create(namespaceDir).append(""String_Node_Str"").append(logBaseDir1);
    startLogSaver();
    cConf.set(LoggingConfiguration.LOG_BASE_DIR,logBaseDir);
    fileMetaDataManager=injector.getInstance(FileMetaDataManager.class);
    waitTillLogSaverDone(fileMetaDataManager,loggingContext,""String_Node_Str"");
    stopLogSaver();
    fileLogReader=injector.getInstance(FileLogReader.class);
    events=Lists.newArrayList(fileLogReader.getLog(new FlowletLoggingContext(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str""),0,Long.MAX_VALUE,Filter.EMPTY_FILTER));
    Assert.assertEquals(50,events.size());
    verifyCheckpoint();
    verifyMetricsPlugin(110L);
  }
 catch (  Throwable t) {
    try {
      final Multimap<String,String> contextMessages=getPublishedKafkaMessages();
      LOG.info(""String_Node_Str"",contextMessages);
    }
 catch (    Throwable e) {
      LOG.error(""String_Node_Str"",e);
    }
    throw t;
  }
}"
5778,"/** 
 * Creates a jar that contains everything that are needed for running the MapReduce program by Hadoop.
 * @return a new {@link File} containing the job jar
 */
private File buildJobJar(Job job,File tempDir) throws IOException, URISyntaxException {
  File jobJar=new File(tempDir,""String_Node_Str"");
  LOG.debug(""String_Node_Str"",jobJar);
  if (MapReduceTaskContextProvider.isLocal(job.getConfiguration())) {
    JarOutputStream output=new JarOutputStream(new FileOutputStream(jobJar));
    output.close();
    return jobJar;
  }
  final HadoopClassExcluder hadoopClassExcluder=new HadoopClassExcluder();
  ApplicationBundler appBundler=new ApplicationBundler(new ClassAcceptor(){
    @Override public boolean accept(    String className,    URL classUrl,    URL classPathUrl){
      if (className.startsWith(""String_Node_Str"") || classPathUrl.toString().contains(""String_Node_Str"")) {
        return false;
      }
      return hadoopClassExcluder.accept(className,classUrl,classPathUrl);
    }
  }
);
  Set<Class<?>> classes=Sets.newHashSet();
  classes.add(MapReduce.class);
  classes.add(MapperWrapper.class);
  classes.add(ReducerWrapper.class);
  if (cConf.getBoolean(Constants.AppFabric.MAPREDUCE_INCLUDE_CUSTOM_CLASSES)) {
    try {
      Class<? extends InputFormat<?,?>> inputFormatClass=job.getInputFormatClass();
      LOG.info(""String_Node_Str"",inputFormatClass,inputFormatClass.getClassLoader());
      classes.add(inputFormatClass);
      if (MapReduceStreamInputFormat.class.isAssignableFrom(inputFormatClass)) {
        Class<? extends StreamEventDecoder> decoderType=MapReduceStreamInputFormat.getDecoderClass(job.getConfiguration());
        if (decoderType != null) {
          classes.add(decoderType);
        }
      }
    }
 catch (    Throwable t) {
      LOG.info(""String_Node_Str"",t.getMessage(),t);
    }
    try {
      Class<? extends OutputFormat<?,?>> outputFormatClass=job.getOutputFormatClass();
      LOG.info(""String_Node_Str"",outputFormatClass,outputFormatClass.getClassLoader());
      classes.add(outputFormatClass);
    }
 catch (    Throwable t) {
      LOG.info(""String_Node_Str"",t.getMessage(),t);
    }
  }
  try {
    Class<?> hbaseTableUtilClass=HBaseTableUtilFactory.getHBaseTableUtilClass();
    classes.add(hbaseTableUtilClass);
  }
 catch (  ProvisionException e) {
    LOG.warn(""String_Node_Str"");
  }
  ClassLoader oldCLassLoader=ClassLoaders.setContextClassLoader(job.getConfiguration().getClassLoader());
  appBundler.createBundle(Locations.toLocation(jobJar),classes);
  ClassLoaders.setContextClassLoader(oldCLassLoader);
  LOG.info(""String_Node_Str"",jobJar.toURI());
  return jobJar;
}","/** 
 * Creates a jar that contains everything that are needed for running the MapReduce program by Hadoop.
 * @return a new {@link File} containing the job jar
 */
private File buildJobJar(Job job,File tempDir) throws IOException, URISyntaxException {
  File jobJar=new File(tempDir,""String_Node_Str"");
  LOG.debug(""String_Node_Str"",jobJar);
  if (MapReduceTaskContextProvider.isLocal(job.getConfiguration())) {
    JarOutputStream output=new JarOutputStream(new FileOutputStream(jobJar));
    output.close();
    return jobJar;
  }
  final HadoopClassExcluder hadoopClassExcluder=new HadoopClassExcluder();
  ApplicationBundler appBundler=new ApplicationBundler(new ClassAcceptor(){
    @Override public boolean accept(    String className,    URL classUrl,    URL classPathUrl){
      if (className.startsWith(""String_Node_Str"") || classPathUrl.toString().contains(""String_Node_Str"")) {
        return false;
      }
      return hadoopClassExcluder.accept(className,classUrl,classPathUrl);
    }
  }
);
  Set<Class<?>> classes=Sets.newHashSet();
  classes.add(MapReduce.class);
  classes.add(MapperWrapper.class);
  classes.add(ReducerWrapper.class);
  if (cConf.getBoolean(Constants.AppFabric.MAPREDUCE_INCLUDE_CUSTOM_CLASSES)) {
    try {
      Class<? extends InputFormat<?,?>> inputFormatClass=job.getInputFormatClass();
      LOG.info(""String_Node_Str"",inputFormatClass,inputFormatClass.getClassLoader());
      classes.add(inputFormatClass);
      if (MapReduceStreamInputFormat.class.isAssignableFrom(inputFormatClass)) {
        Class<? extends StreamEventDecoder> decoderType=MapReduceStreamInputFormat.getDecoderClass(job.getConfiguration());
        if (decoderType != null) {
          classes.add(decoderType);
        }
      }
    }
 catch (    Throwable t) {
      LOG.info(""String_Node_Str"",t.getMessage(),t);
    }
    try {
      Class<? extends OutputFormat<?,?>> outputFormatClass=job.getOutputFormatClass();
      LOG.info(""String_Node_Str"",outputFormatClass,outputFormatClass.getClassLoader());
      classes.add(outputFormatClass);
    }
 catch (    Throwable t) {
      LOG.info(""String_Node_Str"",t.getMessage(),t);
    }
  }
  if (SecureStoreUtils.isKMSBacked(cConf) && SecureStoreUtils.isKMSCapable()) {
    classes.add(SecureStoreUtils.getKMSSecureStore());
  }
  try {
    Class<?> hbaseTableUtilClass=HBaseTableUtilFactory.getHBaseTableUtilClass();
    classes.add(hbaseTableUtilClass);
  }
 catch (  ProvisionException e) {
    LOG.warn(""String_Node_Str"");
  }
  ClassLoader oldCLassLoader=ClassLoaders.setContextClassLoader(job.getConfiguration().getClassLoader());
  appBundler.createBundle(Locations.toLocation(jobJar),classes);
  ClassLoaders.setContextClassLoader(oldCLassLoader);
  LOG.info(""String_Node_Str"",jobJar.toURI());
  return jobJar;
}"
5779,"/** 
 * Packages all the dependencies of the Spark job. It contains all CDAP classes that are needed to run the user spark program.
 * @param targetDir directory for the file to be created in
 * @return {@link File} of the dependency jar in the given target directory
 * @throws IOException if failed to package the jar
 */
private File buildDependencyJar(File targetDir) throws IOException {
  Location tempLocation=new LocalLocationFactory(targetDir).create(CDAP_SPARK_JAR);
  final HadoopClassExcluder hadoopClassExcluder=new HadoopClassExcluder();
  ApplicationBundler appBundler=new ApplicationBundler(new ClassAcceptor(){
    @Override public boolean accept(    String className,    URL classUrl,    URL classPathUrl){
      if (className.startsWith(""String_Node_Str"") || className.startsWith(""String_Node_Str"") || classPathUrl.toString().contains(""String_Node_Str"")) {
        return false;
      }
      return hadoopClassExcluder.accept(className,classUrl,classPathUrl);
    }
  }
);
  appBundler.createBundle(tempLocation,SparkMainWrapper.class,HBaseTableUtilFactory.getHBaseTableUtilClass());
  return new File(tempLocation.toURI());
}","/** 
 * Packages all the dependencies of the Spark job. It contains all CDAP classes that are needed to run the user spark program.
 * @param targetDir directory for the file to be created in
 * @return {@link File} of the dependency jar in the given target directory
 * @throws IOException if failed to package the jar
 */
private File buildDependencyJar(File targetDir) throws IOException {
  Location tempLocation=new LocalLocationFactory(targetDir).create(CDAP_SPARK_JAR);
  final HadoopClassExcluder hadoopClassExcluder=new HadoopClassExcluder();
  ApplicationBundler appBundler=new ApplicationBundler(new ClassAcceptor(){
    @Override public boolean accept(    String className,    URL classUrl,    URL classPathUrl){
      if (className.startsWith(""String_Node_Str"") || className.startsWith(""String_Node_Str"") || classPathUrl.toString().contains(""String_Node_Str"")) {
        return false;
      }
      return hadoopClassExcluder.accept(className,classUrl,classPathUrl);
    }
  }
);
  List<Class<?>> classes=new ArrayList<>();
  classes.add(SparkMainWrapper.class);
  classes.add(HBaseTableUtilFactory.getHBaseTableUtilClass());
  if (SecureStoreUtils.isKMSBacked(cConf) && SecureStoreUtils.isKMSCapable()) {
    classes.add(SecureStoreUtils.getKMSSecureStore());
  }
  appBundler.createBundle(tempLocation,classes);
  return new File(tempLocation.toURI());
}"
5780,"/** 
 * Serialize the   {@link ApplicationSpecification} to the configuration.
 */
private void setApplicationSpecification(ApplicationSpecification spec){
  hConf.set(HCONF_ATTR_APP_SPEC,GSON.toJson(spec,ApplicationSpecification.class));
}","/** 
 * Serialize the   {@link ApplicationSpecification} to the configuration.
 */
@VisibleForTesting void setApplicationSpecification(ApplicationSpecification spec){
  hConf.set(HCONF_ATTR_APP_SPEC,GSON.toJson(spec,ApplicationSpecification.class));
}"
5781,"/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.get(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}","/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.getRaw(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}"
5782,"/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.get(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}","/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.getRaw(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}"
5783,"/** 
 * Serialize the   {@link ApplicationSpecification} to the configuration.
 */
private void setApplicationSpecification(ApplicationSpecification spec){
  hConf.set(HCONF_ATTR_APP_SPEC,GSON.toJson(spec,ApplicationSpecification.class));
}","/** 
 * Serialize the   {@link ApplicationSpecification} to the configuration.
 */
@VisibleForTesting void setApplicationSpecification(ApplicationSpecification spec){
  hConf.set(HCONF_ATTR_APP_SPEC,GSON.toJson(spec,ApplicationSpecification.class));
}"
5784,"/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.get(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}","/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.getRaw(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}"
5785,"/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.get(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}","/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.getRaw(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}"
5786,"@Inject public HBaseQueueDebugger(HBaseTableUtil tableUtil,HBaseQueueAdmin queueAdmin,HBaseQueueClientFactory queueClientFactory,ZKClientService zkClientService,TransactionExecutorFactory txExecutorFactory,NamespaceQueryAdmin namespaceQueryAdmin,Store store){
  this.tableUtil=tableUtil;
  this.queueAdmin=queueAdmin;
  this.queueClientFactory=queueClientFactory;
  this.zkClientService=zkClientService;
  this.txExecutorFactory=txExecutorFactory;
  this.namespaceQueryAdmin=namespaceQueryAdmin;
  this.store=store;
}","@Inject public HBaseQueueDebugger(HBaseTableUtil tableUtil,HBaseQueueAdmin queueAdmin,HBaseQueueClientFactory queueClientFactory,ZKClientService zkClientService,TransactionExecutorFactory txExecutorFactory,NamespaceQueryAdmin namespaceQueryAdmin,Store store,Impersonator impersonator){
  this.tableUtil=tableUtil;
  this.queueAdmin=queueAdmin;
  this.queueClientFactory=queueClientFactory;
  this.zkClientService=zkClientService;
  this.txExecutorFactory=txExecutorFactory;
  this.namespaceQueryAdmin=namespaceQueryAdmin;
  this.store=store;
  this.impersonator=impersonator;
}"
5787,"public void scanAllQueues() throws Exception {
  QueueStatistics totalStats=new QueueStatistics();
  List<NamespaceMeta> namespaceMetas=namespaceQueryAdmin.list();
  for (  NamespaceMeta namespaceMeta : namespaceMetas) {
    Id.Namespace namespaceId=Id.Namespace.from(namespaceMeta.getName());
    Collection<ApplicationSpecification> apps=store.getAllApplications(namespaceId);
    for (    ApplicationSpecification app : apps) {
      Id.Application appId=Id.Application.from(namespaceId,app.getName());
      for (      FlowSpecification flow : app.getFlows().values()) {
        Id.Flow flowId=Id.Flow.from(appId,flow.getName());
        SimpleQueueSpecificationGenerator queueSpecGenerator=new SimpleQueueSpecificationGenerator(flowId.getApplication());
        Table<QueueSpecificationGenerator.Node,String,Set<QueueSpecification>> table=queueSpecGenerator.create(flow);
        for (        Table.Cell<QueueSpecificationGenerator.Node,String,Set<QueueSpecification>> cell : table.cellSet()) {
          if (cell.getRowKey().getType() == FlowletConnection.Type.FLOWLET) {
            for (            QueueSpecification queue : cell.getValue()) {
              QueueStatistics queueStats=scanQueue(queue.getQueueName(),null);
              totalStats.add(queueStats);
            }
          }
        }
      }
    }
  }
  System.out.printf(""String_Node_Str"",totalStats.getReport(showTxTimestampOnly()));
}","public void scanAllQueues() throws Exception {
  final QueueStatistics totalStats=new QueueStatistics();
  List<NamespaceMeta> namespaceMetas=namespaceQueryAdmin.list();
  for (  NamespaceMeta namespaceMeta : namespaceMetas) {
    final Id.Namespace namespaceId=Id.Namespace.from(namespaceMeta.getName());
    final Collection<ApplicationSpecification> apps=store.getAllApplications(namespaceId);
    impersonator.doAs(namespaceMeta,new Callable<Void>(){
      @Override public Void call() throws Exception {
        for (        ApplicationSpecification app : apps) {
          Id.Application appId=Id.Application.from(namespaceId,app.getName());
          for (          FlowSpecification flow : app.getFlows().values()) {
            Id.Flow flowId=Id.Flow.from(appId,flow.getName());
            SimpleQueueSpecificationGenerator queueSpecGenerator=new SimpleQueueSpecificationGenerator(flowId.getApplication());
            Table<QueueSpecificationGenerator.Node,String,Set<QueueSpecification>> table=queueSpecGenerator.create(flow);
            for (            Table.Cell<QueueSpecificationGenerator.Node,String,Set<QueueSpecification>> cell : table.cellSet()) {
              if (cell.getRowKey().getType() == FlowletConnection.Type.FLOWLET) {
                for (                QueueSpecification queue : cell.getValue()) {
                  QueueStatistics queueStats=scanQueue(queue.getQueueName(),null);
                  totalStats.add(queueStats);
                }
              }
            }
          }
        }
        return null;
      }
    }
);
  }
  System.out.printf(""String_Node_Str"",totalStats.getReport(showTxTimestampOnly()));
}"
5788,"public static HBaseQueueDebugger createDebugger(){
  Injector injector=Guice.createInjector(new ConfigModule(CConfiguration.create(),HBaseConfiguration.create()),new IOModule(),new ZKClientModule(),new LocationRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new TwillModule(),new ExploreClientModule(),new DataFabricModules().getDistributedModules(),new ServiceStoreModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new AppFabricServiceRuntimeModule().getDistributedModules(),new ProgramRunnerRuntimeModule().getDistributedModules(),new SystemDatasetRuntimeModule().getDistributedModules(),new NotificationServiceRuntimeModule().getDistributedModules(),new MetricsClientRuntimeModule().getDistributedModules(),new MetricsStoreModule(),new KafkaClientModule(),new NamespaceStoreModule().getDistributedModules(),new AuthorizationModule(),new AuthorizationEnforcementModule().getMasterModule(),new SecureStoreModules().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(QueueClientFactory.class).to(HBaseQueueClientFactory.class).in(Singleton.class);
      bind(QueueAdmin.class).to(HBaseQueueAdmin.class).in(Singleton.class);
      bind(HBaseTableUtil.class).toProvider(HBaseTableUtilFactory.class);
      bind(Store.class).annotatedWith(Names.named(""String_Node_Str"")).to(DefaultStore.class).in(Singleton.class);
      bind(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str"")).to(DatasetFramework.class).in(Singleton.class);
    }
  }
);
  return injector.getInstance(HBaseQueueDebugger.class);
}","public static HBaseQueueDebugger createDebugger() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  SecurityUtil.loginForMasterService(cConf);
  Injector injector=Guice.createInjector(new ConfigModule(cConf,HBaseConfiguration.create()),new IOModule(),new ZKClientModule(),new LocationRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new TwillModule(),new ExploreClientModule(),new DataFabricModules().getDistributedModules(),new ServiceStoreModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new AppFabricServiceRuntimeModule().getDistributedModules(),new ProgramRunnerRuntimeModule().getDistributedModules(),new SystemDatasetRuntimeModule().getDistributedModules(),new NotificationServiceRuntimeModule().getDistributedModules(),new MetricsClientRuntimeModule().getDistributedModules(),new MetricsStoreModule(),new KafkaClientModule(),new NamespaceStoreModule().getDistributedModules(),new AuthorizationModule(),new AuthorizationEnforcementModule().getMasterModule(),new SecureStoreModules().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(QueueClientFactory.class).to(HBaseQueueClientFactory.class).in(Singleton.class);
      bind(QueueAdmin.class).to(HBaseQueueAdmin.class).in(Singleton.class);
      bind(HBaseTableUtil.class).toProvider(HBaseTableUtilFactory.class);
      bind(Store.class).annotatedWith(Names.named(""String_Node_Str"")).to(DefaultStore.class).in(Singleton.class);
      bind(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str"")).to(DatasetFramework.class).in(Singleton.class);
    }
  }
);
  return injector.getInstance(HBaseQueueDebugger.class);
}"
5789,"public UpgradeTool() throws Exception {
  this.cConf=CConfiguration.create();
  this.hConf=HBaseConfiguration.create();
  Injector injector=createInjector();
  this.txService=injector.getInstance(TransactionService.class);
  this.zkClientService=injector.getInstance(ZKClientService.class);
  this.dsFramework=injector.getInstance(DatasetFramework.class);
  this.metadataStore=injector.getInstance(MetadataStore.class);
  this.streamStateStoreUpgrader=injector.getInstance(StreamStateStoreUpgrader.class);
  this.dsUpgrade=injector.getInstance(DatasetUpgrader.class);
  this.dsSpecUpgrader=injector.getInstance(DatasetSpecificationUpgrader.class);
  this.queueAdmin=injector.getInstance(QueueAdmin.class);
  this.nsStore=injector.getInstance(NamespaceStore.class);
  this.authorizationService=injector.getInstance(AuthorizationEnforcementService.class);
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        UpgradeTool.this.stop();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
);
  this.existingEntitySystemMetadataWriter=injector.getInstance(ExistingEntitySystemMetadataWriter.class);
  this.upgradeDatasetServiceManager=injector.getInstance(UpgradeDatasetServiceManager.class);
}","public UpgradeTool() throws Exception {
  this.cConf=CConfiguration.create();
  SecurityUtil.loginForMasterService(cConf);
  this.hConf=HBaseConfiguration.create();
  Injector injector=createInjector();
  this.txService=injector.getInstance(TransactionService.class);
  this.zkClientService=injector.getInstance(ZKClientService.class);
  this.dsFramework=injector.getInstance(DatasetFramework.class);
  this.metadataStore=injector.getInstance(MetadataStore.class);
  this.streamStateStoreUpgrader=injector.getInstance(StreamStateStoreUpgrader.class);
  this.dsUpgrade=injector.getInstance(DatasetUpgrader.class);
  this.dsSpecUpgrader=injector.getInstance(DatasetSpecificationUpgrader.class);
  this.queueAdmin=injector.getInstance(QueueAdmin.class);
  this.nsStore=injector.getInstance(NamespaceStore.class);
  this.authorizationService=injector.getInstance(AuthorizationEnforcementService.class);
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        UpgradeTool.this.stop();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
);
  this.existingEntitySystemMetadataWriter=injector.getInstance(ExistingEntitySystemMetadataWriter.class);
  this.upgradeDatasetServiceManager=injector.getInstance(UpgradeDatasetServiceManager.class);
}"
5790,"/** 
 * Serialize the   {@link ApplicationSpecification} to the configuration.
 */
private void setApplicationSpecification(ApplicationSpecification spec){
  hConf.set(HCONF_ATTR_APP_SPEC,GSON.toJson(spec,ApplicationSpecification.class));
}","/** 
 * Serialize the   {@link ApplicationSpecification} to the configuration.
 */
@VisibleForTesting void setApplicationSpecification(ApplicationSpecification spec){
  hConf.set(HCONF_ATTR_APP_SPEC,GSON.toJson(spec,ApplicationSpecification.class));
}"
5791,"/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.get(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}","/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.getRaw(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}"
5792,"/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.get(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}","/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.getRaw(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}"
5793,"/** 
 * Serialize the   {@link ApplicationSpecification} to the configuration.
 */
private void setApplicationSpecification(ApplicationSpecification spec){
  hConf.set(HCONF_ATTR_APP_SPEC,GSON.toJson(spec,ApplicationSpecification.class));
}","/** 
 * Serialize the   {@link ApplicationSpecification} to the configuration.
 */
@VisibleForTesting void setApplicationSpecification(ApplicationSpecification spec){
  hConf.set(HCONF_ATTR_APP_SPEC,GSON.toJson(spec,ApplicationSpecification.class));
}"
5794,"/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.get(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}","/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.getRaw(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}"
5795,"/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.get(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}","/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.getRaw(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}"
5796,"@Inject public HBaseQueueDebugger(HBaseTableUtil tableUtil,HBaseQueueAdmin queueAdmin,HBaseQueueClientFactory queueClientFactory,ZKClientService zkClientService,TransactionExecutorFactory txExecutorFactory,NamespaceQueryAdmin namespaceQueryAdmin,Store store,Impersonator impersonator){
  this.tableUtil=tableUtil;
  this.queueAdmin=queueAdmin;
  this.queueClientFactory=queueClientFactory;
  this.zkClientService=zkClientService;
  this.txExecutorFactory=txExecutorFactory;
  this.namespaceQueryAdmin=namespaceQueryAdmin;
  this.store=store;
  this.impersonator=impersonator;
}","@Inject public HBaseQueueDebugger(HBaseTableUtil tableUtil,HBaseQueueAdmin queueAdmin,HBaseQueueClientFactory queueClientFactory,ZKClientService zkClientService,TransactionExecutorFactory txExecutorFactory,NamespaceQueryAdmin namespaceQueryAdmin,Store store,Impersonator impersonator,AuthorizationEnforcementService authorizationEnforcementService){
  this.tableUtil=tableUtil;
  this.queueAdmin=queueAdmin;
  this.queueClientFactory=queueClientFactory;
  this.zkClientService=zkClientService;
  this.txExecutorFactory=txExecutorFactory;
  this.namespaceQueryAdmin=namespaceQueryAdmin;
  this.store=store;
  this.impersonator=impersonator;
  this.authorizationEnforcementService=authorizationEnforcementService;
}"
5797,"@Override protected void shutDown() throws Exception {
  zkClientService.stopAndWait();
}","@Override protected void shutDown() throws Exception {
  authorizationEnforcementService.stopAndWait();
  zkClientService.stopAndWait();
}"
5798,"@Override protected void startUp() throws Exception {
  zkClientService.startAndWait();
}","@Override protected void startUp() throws Exception {
  zkClientService.startAndWait();
  authorizationEnforcementService.startAndWait();
}"
5799,"/** 
 * Creates a jar that contains everything that are needed for running the MapReduce program by Hadoop.
 * @return a new {@link File} containing the job jar
 */
private File buildJobJar(Job job,File tempDir) throws IOException, URISyntaxException {
  File jobJar=new File(tempDir,""String_Node_Str"");
  LOG.debug(""String_Node_Str"",jobJar);
  if (MapReduceTaskContextProvider.isLocal(job.getConfiguration())) {
    JarOutputStream output=new JarOutputStream(new FileOutputStream(jobJar));
    output.close();
    return jobJar;
  }
  final HadoopClassExcluder hadoopClassExcluder=new HadoopClassExcluder();
  ApplicationBundler appBundler=new ApplicationBundler(new ClassAcceptor(){
    @Override public boolean accept(    String className,    URL classUrl,    URL classPathUrl){
      if (className.startsWith(""String_Node_Str"") || classPathUrl.toString().contains(""String_Node_Str"")) {
        return false;
      }
      return hadoopClassExcluder.accept(className,classUrl,classPathUrl);
    }
  }
);
  Set<Class<?>> classes=Sets.newHashSet();
  classes.add(MapReduce.class);
  classes.add(MapperWrapper.class);
  classes.add(ReducerWrapper.class);
  if (cConf.getBoolean(Constants.AppFabric.MAPREDUCE_INCLUDE_CUSTOM_CLASSES)) {
    try {
      Class<? extends InputFormat<?,?>> inputFormatClass=job.getInputFormatClass();
      LOG.info(""String_Node_Str"",inputFormatClass,inputFormatClass.getClassLoader());
      classes.add(inputFormatClass);
      if (MapReduceStreamInputFormat.class.isAssignableFrom(inputFormatClass)) {
        Class<? extends StreamEventDecoder> decoderType=MapReduceStreamInputFormat.getDecoderClass(job.getConfiguration());
        if (decoderType != null) {
          classes.add(decoderType);
        }
      }
    }
 catch (    Throwable t) {
      LOG.info(""String_Node_Str"",t.getMessage(),t);
    }
    try {
      Class<? extends OutputFormat<?,?>> outputFormatClass=job.getOutputFormatClass();
      LOG.info(""String_Node_Str"",outputFormatClass,outputFormatClass.getClassLoader());
      classes.add(outputFormatClass);
    }
 catch (    Throwable t) {
      LOG.info(""String_Node_Str"",t.getMessage(),t);
    }
  }
  try {
    Class<?> hbaseTableUtilClass=HBaseTableUtilFactory.getHBaseTableUtilClass();
    classes.add(hbaseTableUtilClass);
  }
 catch (  ProvisionException e) {
    LOG.warn(""String_Node_Str"");
  }
  ClassLoader oldCLassLoader=ClassLoaders.setContextClassLoader(job.getConfiguration().getClassLoader());
  appBundler.createBundle(Locations.toLocation(jobJar),classes);
  ClassLoaders.setContextClassLoader(oldCLassLoader);
  LOG.info(""String_Node_Str"",jobJar.toURI());
  return jobJar;
}","/** 
 * Creates a jar that contains everything that are needed for running the MapReduce program by Hadoop.
 * @return a new {@link File} containing the job jar
 */
private File buildJobJar(Job job,File tempDir) throws IOException, URISyntaxException {
  File jobJar=new File(tempDir,""String_Node_Str"");
  LOG.debug(""String_Node_Str"",jobJar);
  if (MapReduceTaskContextProvider.isLocal(job.getConfiguration())) {
    JarOutputStream output=new JarOutputStream(new FileOutputStream(jobJar));
    output.close();
    return jobJar;
  }
  final HadoopClassExcluder hadoopClassExcluder=new HadoopClassExcluder();
  ApplicationBundler appBundler=new ApplicationBundler(new ClassAcceptor(){
    @Override public boolean accept(    String className,    URL classUrl,    URL classPathUrl){
      if (className.startsWith(""String_Node_Str"") || classPathUrl.toString().contains(""String_Node_Str"")) {
        return false;
      }
      return hadoopClassExcluder.accept(className,classUrl,classPathUrl);
    }
  }
);
  Set<Class<?>> classes=Sets.newHashSet();
  classes.add(MapReduce.class);
  classes.add(MapperWrapper.class);
  classes.add(ReducerWrapper.class);
  if (cConf.getBoolean(Constants.AppFabric.MAPREDUCE_INCLUDE_CUSTOM_CLASSES)) {
    try {
      Class<? extends InputFormat<?,?>> inputFormatClass=job.getInputFormatClass();
      LOG.info(""String_Node_Str"",inputFormatClass,inputFormatClass.getClassLoader());
      classes.add(inputFormatClass);
      if (MapReduceStreamInputFormat.class.isAssignableFrom(inputFormatClass)) {
        Class<? extends StreamEventDecoder> decoderType=MapReduceStreamInputFormat.getDecoderClass(job.getConfiguration());
        if (decoderType != null) {
          classes.add(decoderType);
        }
      }
    }
 catch (    Throwable t) {
      LOG.info(""String_Node_Str"",t.getMessage(),t);
    }
    try {
      Class<? extends OutputFormat<?,?>> outputFormatClass=job.getOutputFormatClass();
      LOG.info(""String_Node_Str"",outputFormatClass,outputFormatClass.getClassLoader());
      classes.add(outputFormatClass);
    }
 catch (    Throwable t) {
      LOG.info(""String_Node_Str"",t.getMessage(),t);
    }
  }
  if (SecureStoreUtils.isKMSBacked(cConf) && SecureStoreUtils.isKMSCapable()) {
    classes.add(SecureStoreUtils.getKMSSecureStore());
  }
  try {
    Class<?> hbaseTableUtilClass=HBaseTableUtilFactory.getHBaseTableUtilClass();
    classes.add(hbaseTableUtilClass);
  }
 catch (  ProvisionException e) {
    LOG.warn(""String_Node_Str"");
  }
  ClassLoader oldCLassLoader=ClassLoaders.setContextClassLoader(job.getConfiguration().getClassLoader());
  appBundler.createBundle(Locations.toLocation(jobJar),classes);
  ClassLoaders.setContextClassLoader(oldCLassLoader);
  LOG.info(""String_Node_Str"",jobJar.toURI());
  return jobJar;
}"
5800,"/** 
 * Packages all the dependencies of the Spark job. It contains all CDAP classes that are needed to run the user spark program.
 * @param targetDir directory for the file to be created in
 * @return {@link File} of the dependency jar in the given target directory
 * @throws IOException if failed to package the jar
 */
private File buildDependencyJar(File targetDir) throws IOException {
  Location tempLocation=new LocalLocationFactory(targetDir).create(CDAP_SPARK_JAR);
  final HadoopClassExcluder hadoopClassExcluder=new HadoopClassExcluder();
  ApplicationBundler appBundler=new ApplicationBundler(new ClassAcceptor(){
    @Override public boolean accept(    String className,    URL classUrl,    URL classPathUrl){
      if (className.startsWith(""String_Node_Str"") || className.startsWith(""String_Node_Str"") || classPathUrl.toString().contains(""String_Node_Str"")) {
        return false;
      }
      return hadoopClassExcluder.accept(className,classUrl,classPathUrl);
    }
  }
);
  appBundler.createBundle(tempLocation,SparkMainWrapper.class,HBaseTableUtilFactory.getHBaseTableUtilClass());
  return new File(tempLocation.toURI());
}","/** 
 * Packages all the dependencies of the Spark job. It contains all CDAP classes that are needed to run the user spark program.
 * @param targetDir directory for the file to be created in
 * @return {@link File} of the dependency jar in the given target directory
 * @throws IOException if failed to package the jar
 */
private File buildDependencyJar(File targetDir) throws IOException {
  Location tempLocation=new LocalLocationFactory(targetDir).create(CDAP_SPARK_JAR);
  final HadoopClassExcluder hadoopClassExcluder=new HadoopClassExcluder();
  ApplicationBundler appBundler=new ApplicationBundler(new ClassAcceptor(){
    @Override public boolean accept(    String className,    URL classUrl,    URL classPathUrl){
      if (className.startsWith(""String_Node_Str"") || className.startsWith(""String_Node_Str"") || classPathUrl.toString().contains(""String_Node_Str"")) {
        return false;
      }
      return hadoopClassExcluder.accept(className,classUrl,classPathUrl);
    }
  }
);
  List<Class<?>> classes=new ArrayList<>();
  classes.add(SparkMainWrapper.class);
  classes.add(HBaseTableUtilFactory.getHBaseTableUtilClass());
  if (SecureStoreUtils.isKMSBacked(cConf) && SecureStoreUtils.isKMSCapable()) {
    classes.add(SecureStoreUtils.getKMSSecureStore());
  }
  appBundler.createBundle(tempLocation,classes);
  return new File(tempLocation.toURI());
}"
5801,"@Override public void run(){
  if (outboundChannel.equals(inboundChannel.getAttachment())) {
    closeOnFlush(inboundChannel);
  }
}","@Override public void run(){
  if (e.getChannel().isWritable()) {
    LOG.trace(""String_Node_Str"");
    outboundChannel.setReadable(true);
  }
 else {
    LOG.trace(""String_Node_Str"");
    outboundChannel.setReadable(false);
  }
}"
5802,"@Override public void messageReceived(final ChannelHandlerContext ctx,MessageEvent event) throws Exception {
  if (channelClosed) {
    return;
  }
  final Channel inboundChannel=event.getChannel();
  Object msg=event.getMessage();
  if (msg instanceof HttpChunk) {
    if (chunkSender == null) {
      throw new HandlerException(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    }
    chunkSender.send(msg);
  }
 else   if (msg instanceof HttpRequest) {
    HttpRequest request=(HttpRequest)msg;
    request=applyProxyRules(request);
    inboundChannel.setReadable(false);
    WrappedDiscoverable discoverable=getDiscoverable(request,(InetSocketAddress)inboundChannel.getLocalAddress());
    MessageSender sender=discoveryLookup.get(discoverable);
    if (sender == null || !sender.isConnected()) {
      InetSocketAddress address=discoverable.getSocketAddress();
      ChannelFuture future=clientBootstrap.connect(address);
      final Channel outboundChannel=future.getChannel();
      outboundChannel.getPipeline().addAfter(""String_Node_Str"",""String_Node_Str"",new OutboundHandler(inboundChannel));
      sender=new MessageSender(inboundChannel,future);
      discoveryLookup.put(discoverable,sender);
      inboundChannel.setAttachment(outboundChannel);
      outboundChannel.getCloseFuture().addListener(new ChannelFutureListener(){
        @Override public void operationComplete(        ChannelFuture future) throws Exception {
          inboundChannel.getPipeline().execute(new Runnable(){
            @Override public void run(){
              if (outboundChannel.equals(inboundChannel.getAttachment())) {
                closeOnFlush(inboundChannel);
              }
            }
          }
);
        }
      }
);
    }
    sender.send(request);
    inboundChannel.setReadable(true);
    if (request.isChunked()) {
      chunkSender=sender;
    }
  }
 else {
    super.messageReceived(ctx,event);
  }
}","@Override public void messageReceived(final ChannelHandlerContext ctx,MessageEvent event) throws Exception {
  if (channelClosed) {
    return;
  }
  final Channel inboundChannel=event.getChannel();
  Object msg=event.getMessage();
  if (msg instanceof HttpChunk) {
    if (chunkSender == null) {
      throw new HandlerException(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
    }
    chunkSender.send(msg);
  }
 else   if (msg instanceof HttpRequest) {
    HttpRequest request=(HttpRequest)msg;
    request=applyProxyRules(request);
    inboundChannel.setReadable(false);
    WrappedDiscoverable discoverable=getDiscoverable(request,(InetSocketAddress)inboundChannel.getLocalAddress());
    MessageSender sender=discoveryLookup.get(discoverable);
    if (sender == null || !sender.isConnected()) {
      InetSocketAddress address=discoverable.getSocketAddress();
      ChannelFuture future=clientBootstrap.connect(address);
      final Channel outboundChannel=future.getChannel();
      outboundChannel.getPipeline().addAfter(""String_Node_Str"",""String_Node_Str"",new OutboundHandler(inboundChannel));
      sender=new MessageSender(inboundChannel,future);
      discoveryLookup.put(discoverable,sender);
      inboundChannel.setAttachment(outboundChannel);
      outboundChannel.getCloseFuture().addListener(new ChannelFutureListener(){
        @Override public void operationComplete(        ChannelFuture future) throws Exception {
          inboundChannel.getPipeline().execute(new Runnable(){
            @Override public void run(){
              if (outboundChannel.equals(inboundChannel.getAttachment())) {
                closeOnFlush(inboundChannel);
              }
            }
          }
);
        }
      }
);
    }
 else {
      Channel outboundChannel=(Channel)inboundChannel.getAttachment();
      if (outboundChannel != null) {
        outboundChannel.setReadable(true);
      }
    }
    sender.send(request);
    inboundChannel.setReadable(true);
    if (request.isChunked()) {
      chunkSender=sender;
    }
  }
 else {
    super.messageReceived(ctx,event);
  }
}"
5803,"@Override public void getLog(LoggingContext loggingContext,long fromTimeMs,long toTimeMs,Filter filter,Callback callback){
  long fromOffset=getOffset(fromTimeMs / 1000);
  long toOffset=getOffset(toTimeMs / 1000);
  getLogNext(loggingContext,new ReadRange(fromTimeMs,toTimeMs,fromOffset),(int)(toOffset - fromOffset),filter,callback);
}","@Override public CloseableIterator<LogEvent> getLog(LoggingContext loggingContext,long fromTimeMs,long toTimeMs,Filter filter){
  CollectingCallback collectingCallback=new CollectingCallback();
  long fromOffset=getOffset(fromTimeMs / 1000);
  long toOffset=getOffset(toTimeMs / 1000);
  getLogNext(loggingContext,new ReadRange(fromTimeMs,toTimeMs,fromOffset),(int)(toOffset - fromOffset),filter,collectingCallback);
  final Iterator<LogEvent> iterator=collectingCallback.getLogEvents().iterator();
  return new CloseableIterator<LogEvent>(){
    @Override public boolean hasNext(){
      return iterator.hasNext();
    }
    @Override public LogEvent next(){
      return iterator.next();
    }
    @Override public void remove(){
      iterator.remove();
    }
    @Override public void close(){
    }
  }
;
}"
5804,"@Test public void testRouterAsync() throws Exception {
  int numElements=123;
  AsyncHttpClientConfig.Builder configBuilder=new AsyncHttpClientConfig.Builder();
  final AsyncHttpClient asyncHttpClient=new AsyncHttpClient(new NettyAsyncHttpProvider(configBuilder.build()),configBuilder.build());
  final CountDownLatch latch=new CountDownLatch(numElements);
  final AtomicInteger numSuccessfulRequests=new AtomicInteger(0);
  for (int i=0; i < numElements; ++i) {
    final int elem=i;
    final Request request=new RequestBuilder(""String_Node_Str"").setUrl(resolveURI(DEFAULT_SERVICE,String.format(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",i))).build();
    asyncHttpClient.executeRequest(request,new AsyncCompletionHandler<Void>(){
      @Override public Void onCompleted(      Response response) throws Exception {
        latch.countDown();
        Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusCode());
        numSuccessfulRequests.incrementAndGet();
        return null;
      }
      @Override public void onThrowable(      Throwable t){
        LOG.error(""String_Node_Str"",elem,t);
        latch.countDown();
      }
    }
);
    TimeUnit.MILLISECONDS.sleep(1);
  }
  latch.await();
  asyncHttpClient.close();
  Assert.assertEquals(numElements,numSuccessfulRequests.get());
  Assert.assertTrue(numElements == (defaultServer1.getNumRequests() + defaultServer2.getNumRequests()));
}","@Test public void testRouterAsync() throws Exception {
  int numElements=123;
  AsyncHttpClientConfig.Builder configBuilder=new AsyncHttpClientConfig.Builder();
  final AsyncHttpClient asyncHttpClient=new AsyncHttpClient(new NettyAsyncHttpProvider(configBuilder.build()),configBuilder.build());
  final CountDownLatch latch=new CountDownLatch(numElements);
  final AtomicInteger numSuccessfulRequests=new AtomicInteger(0);
  for (int i=0; i < numElements; ++i) {
    final int elem=i;
    final Request request=new RequestBuilder(""String_Node_Str"").setUrl(resolveURI(DEFAULT_SERVICE,String.format(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",i))).build();
    asyncHttpClient.executeRequest(request,new AsyncCompletionHandler<Void>(){
      @Override public Void onCompleted(      Response response) throws Exception {
        latch.countDown();
        Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusCode());
        String responseBody=response.getResponseBody();
        LOG.trace(""String_Node_Str"",responseBody);
        Assert.assertEquals(""String_Node_Str"" + elem,responseBody);
        numSuccessfulRequests.incrementAndGet();
        return null;
      }
      @Override public void onThrowable(      Throwable t){
        LOG.error(""String_Node_Str"",elem,t);
        latch.countDown();
      }
    }
);
    TimeUnit.MILLISECONDS.sleep(1);
  }
  latch.await();
  asyncHttpClient.close();
  Assert.assertEquals(numElements,numSuccessfulRequests.get());
  Assert.assertTrue(numElements == (defaultServer1.getNumRequests() + defaultServer2.getNumRequests()));
}"
5805,"/** 
 * Serialize the   {@link ApplicationSpecification} to the configuration.
 */
private void setApplicationSpecification(ApplicationSpecification spec){
  hConf.set(HCONF_ATTR_APP_SPEC,GSON.toJson(spec,ApplicationSpecification.class));
}","/** 
 * Serialize the   {@link ApplicationSpecification} to the configuration.
 */
@VisibleForTesting void setApplicationSpecification(ApplicationSpecification spec){
  hConf.set(HCONF_ATTR_APP_SPEC,GSON.toJson(spec,ApplicationSpecification.class));
}"
5806,"/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.get(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}","/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.getRaw(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}"
5807,"/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.get(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}","/** 
 * @return the {@link ApplicationSpecification} stored in the configuration.
 */
public ApplicationSpecification getApplicationSpecification(){
  return GSON.fromJson(hConf.getRaw(HCONF_ATTR_APP_SPEC),ApplicationSpecification.class);
}"
5808,"/** 
 * Gets a TableId for stream consumer state stores within a given namespace.
 * @param namespace the namespace for which the table is for.
 * @return constructed TableId
 */
public static TableId getStateStoreTableId(Id.Namespace namespace){
  String tableName=String.format(""String_Node_Str"",Id.Namespace.SYSTEM.getId(),QueueConstants.QueueType.STREAM.toString());
  return TableId.from(namespace.getId(),tableName);
}","/** 
 * Gets a TableId for stream consumer state stores within a given namespace.
 * @param namespace the namespace for which the table is for.
 * @return constructed TableId. Note that the namespace in the returned TableId is the CDAP namespace (CDAP-7344).
 */
public static TableId getStateStoreTableId(Id.Namespace namespace){
  String tableName=String.format(""String_Node_Str"",Id.Namespace.SYSTEM.getId(),QueueConstants.QueueType.STREAM.toString());
  return TableId.from(namespace.getId(),tableName);
}"
5809,"/** 
 * Lists all tables in the specified namespace
 * @param admin the {@link HBaseAdmin} to use to communicate with HBase
 * @param namespaceId namespace for which the tables are being requested
 */
public abstract List<TableId> listTablesInNamespace(HBaseAdmin admin,String namespaceId) throws IOException ;","/** 
 * Lists all tables in the specified namespace
 * @param admin the {@link HBaseAdmin} to use to communicate with HBase
 * @param namespaceId HBase namespace for which the tables are being requested
 */
public abstract List<TableId> listTablesInNamespace(HBaseAdmin admin,String namespaceId) throws IOException ;"
5810,"/** 
 * Gets the system configuration table prefix.
 * @param prefix Prefix string
 * @return System configuration table prefix (full table name minus the table qualifier).Example input: ""cdap_ns.table.name""  -->  output: ""cdap_system.""   (hbase 94) Example input: ""cdap.table.name""     -->  output: ""cdap_system.""   (hbase 94. input table is in default namespace) Example input: ""cdap_ns:table.name""  -->  output: ""cdap_system:""   (hbase 96, 98)
 */
public String getSysConfigTablePrefix(String prefix){
  return prefix + ""String_Node_Str"" + NamespaceId.SYSTEM.getNamespace()+ ""String_Node_Str"";
}","/** 
 * Gets the system configuration table prefix.
 * @param prefix Prefix string
 * @return System configuration table prefix (full table name minus the table qualifier).Example input: ""cdap.table.name""     -->  output: ""cdap_system.""   (input table is in default namespace) Example input: ""cdap_ns:table.name""  -->  output: ""cdap_system:""   (input table is in a custom namespace)
 */
public String getSysConfigTablePrefix(String prefix){
  return prefix + ""String_Node_Str"" + NamespaceId.SYSTEM.getNamespace()+ ""String_Node_Str"";
}"
5811,"@Test public void testNonTransactionalMixed() throws Exception {
  TableId tableId=TableId.from(Id.Namespace.DEFAULT.getId(),""String_Node_Str"");
  byte[] row1=Bytes.toBytes(""String_Node_Str"");
  byte[] col=Bytes.toBytes(""String_Node_Str"");
  try (HTable table=createTable(tableId)){
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
    table.put(tableUtil.buildPut(row1).add(FAMILY,col,Bytes.toBytes(11L)).build());
    assertColumn(table,row1,col,11);
    Delete delete=tableUtil.buildDelete(row1).deleteColumns(FAMILY,col).build();
    table.batch(Lists.newArrayList(delete));
    Get get=tableUtil.buildGet(row1).build();
    Result result=table.get(get);
    LOG.info(""String_Node_Str"" + result);
    assertTrue(result.isEmpty());
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
    delete=tableUtil.buildDelete(row1).deleteFamily(FAMILY).build();
    table.batch(Lists.newArrayList(delete));
    get=tableUtil.buildGet(row1).build();
    result=table.get(get);
    LOG.info(""String_Node_Str"" + result);
    assertTrue(result.isEmpty());
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
    delete=tableUtil.buildDelete(row1).build();
    table.batch(Lists.newArrayList(delete));
    get=tableUtil.buildGet(row1).build();
    result=table.get(get);
    LOG.info(""String_Node_Str"" + result);
    assertTrue(result.isEmpty());
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
  }
 }","@Test public void testNonTransactionalMixed() throws Exception {
  TableId tableId=TableId.from(Id.Namespace.DEFAULT.getId(),""String_Node_Str"");
  byte[] row1=Bytes.toBytes(""String_Node_Str"");
  byte[] col=Bytes.toBytes(""String_Node_Str"");
  try (HTable table=createTable(tableId)){
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
    table.put(tableUtil.buildPut(row1).add(FAMILY,col,Bytes.toBytes(11L)).build());
    assertColumn(table,row1,col,11);
    Delete delete=tableUtil.buildDelete(row1).deleteColumns(FAMILY,col).build();
    table.delete(delete);
    Get get=tableUtil.buildGet(row1).build();
    Result result=table.get(get);
    LOG.info(""String_Node_Str"" + result);
    assertTrue(result.isEmpty());
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
    delete=tableUtil.buildDelete(row1).deleteFamily(FAMILY).build();
    table.batch(Lists.newArrayList(delete));
    get=tableUtil.buildGet(row1).build();
    result=table.get(get);
    LOG.info(""String_Node_Str"" + result);
    assertTrue(result.isEmpty());
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
    delete=tableUtil.buildDelete(row1).build();
    table.delete(delete);
    get=tableUtil.buildGet(row1).build();
    result=table.get(get);
    LOG.info(""String_Node_Str"" + result);
    assertTrue(result.isEmpty());
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
  }
 }"
5812,"/** 
 * Updates the TTL in the   {@link co.cask.cdap.data2.datafabric.dataset.service.mds.DatasetInstanceMDS}table for CDAP versions prior to 3.3. <p> The TTL for   {@link DatasetSpecification} was stored in milliseconds.Since the spec (as of CDAP version 3.3) is in seconds, the instance MDS entries must be updated. This is to be called only if the current CDAP version is < 3.3. </p>
 * @throws Exception
 */
public void upgrade() throws Exception {
  TableId datasetSpecId=TableId.from(Id.Namespace.SYSTEM.getId(),DatasetMetaTableUtil.INSTANCE_TABLE_NAME);
  HBaseAdmin hBaseAdmin=new HBaseAdmin(conf);
  if (!tableUtil.tableExists(hBaseAdmin,datasetSpecId)) {
    LOG.error(""String_Node_Str"",datasetSpecId);
    return;
  }
  HTable specTable=tableUtil.createHTable(conf,datasetSpecId);
  try {
    ScanBuilder scanBuilder=tableUtil.buildScan();
    scanBuilder.setTimeRange(0,HConstants.LATEST_TIMESTAMP);
    scanBuilder.setMaxVersions();
    try (ResultScanner resultScanner=specTable.getScanner(scanBuilder.build())){
      Result result;
      while ((result=resultScanner.next()) != null) {
        Put put=new Put(result.getRow());
        for (        Map.Entry<byte[],NavigableMap<byte[],NavigableMap<Long,byte[]>>> familyMap : result.getMap().entrySet()) {
          for (          Map.Entry<byte[],NavigableMap<Long,byte[]>> columnMap : familyMap.getValue().entrySet()) {
            for (            Map.Entry<Long,byte[]> columnEntry : columnMap.getValue().entrySet()) {
              Long timeStamp=columnEntry.getKey();
              byte[] colVal=columnEntry.getValue();
              if (colVal == null || colVal.length == 0) {
                continue;
              }
              String specEntry=Bytes.toString(colVal);
              DatasetSpecification specification=GSON.fromJson(specEntry,DatasetSpecification.class);
              DatasetSpecification updatedSpec=updateTTLInSpecification(specification,null);
              colVal=Bytes.toBytes(GSON.toJson(updatedSpec));
              put.add(familyMap.getKey(),columnMap.getKey(),timeStamp,colVal);
            }
          }
        }
        if (put.size() > 0) {
          specTable.put(put);
        }
      }
    }
   }
  finally {
    specTable.flushCommits();
    specTable.close();
  }
}","/** 
 * Updates the TTL in the   {@link co.cask.cdap.data2.datafabric.dataset.service.mds.DatasetInstanceMDS}table for CDAP versions prior to 3.3. <p> The TTL for   {@link DatasetSpecification} was stored in milliseconds.Since the spec (as of CDAP version 3.3) is in seconds, the instance MDS entries must be updated. This is to be called only if the current CDAP version is < 3.3. </p>
 * @throws Exception
 */
public void upgrade() throws Exception {
  TableId datasetSpecId=tableUtil.createHTableId(NamespaceId.SYSTEM,DatasetMetaTableUtil.INSTANCE_TABLE_NAME);
  HBaseAdmin hBaseAdmin=new HBaseAdmin(conf);
  if (!tableUtil.tableExists(hBaseAdmin,datasetSpecId)) {
    LOG.error(""String_Node_Str"",datasetSpecId);
    return;
  }
  HTable specTable=tableUtil.createHTable(conf,datasetSpecId);
  try {
    ScanBuilder scanBuilder=tableUtil.buildScan();
    scanBuilder.setTimeRange(0,HConstants.LATEST_TIMESTAMP);
    scanBuilder.setMaxVersions();
    try (ResultScanner resultScanner=specTable.getScanner(scanBuilder.build())){
      Result result;
      while ((result=resultScanner.next()) != null) {
        Put put=new Put(result.getRow());
        for (        Map.Entry<byte[],NavigableMap<byte[],NavigableMap<Long,byte[]>>> familyMap : result.getMap().entrySet()) {
          for (          Map.Entry<byte[],NavigableMap<Long,byte[]>> columnMap : familyMap.getValue().entrySet()) {
            for (            Map.Entry<Long,byte[]> columnEntry : columnMap.getValue().entrySet()) {
              Long timeStamp=columnEntry.getKey();
              byte[] colVal=columnEntry.getValue();
              if (colVal == null || colVal.length == 0) {
                continue;
              }
              String specEntry=Bytes.toString(colVal);
              DatasetSpecification specification=GSON.fromJson(specEntry,DatasetSpecification.class);
              DatasetSpecification updatedSpec=updateTTLInSpecification(specification,null);
              colVal=Bytes.toBytes(GSON.toJson(updatedSpec));
              put.add(familyMap.getKey(),columnMap.getKey(),timeStamp,colVal);
            }
          }
        }
        if (put.size() > 0) {
          specTable.put(put);
        }
      }
    }
   }
  finally {
    specTable.flushCommits();
    specTable.close();
  }
}"
5813,"@Override public TableId apply(NamespaceMeta input){
  return StreamUtils.getStateStoreTableId(Id.Namespace.from(input.getName()));
}","@Override public TableId apply(NamespaceMeta input){
  try {
    TableId tableId=StreamUtils.getStateStoreTableId(input.getNamespaceId().toId());
    return tableUtil.createHTableId(new NamespaceId(tableId.getNamespace()),tableId.getTableName());
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}"
5814,"@Override protected Iterable<TableId> getTableIds() throws Exception {
  return Lists.transform(namespaceQueryAdmin.list(),new Function<NamespaceMeta,TableId>(){
    @Override public TableId apply(    NamespaceMeta input){
      return StreamUtils.getStateStoreTableId(Id.Namespace.from(input.getName()));
    }
  }
);
}","@Override protected Iterable<TableId> getTableIds() throws Exception {
  return Lists.transform(namespaceQueryAdmin.list(),new Function<NamespaceMeta,TableId>(){
    @Override public TableId apply(    NamespaceMeta input){
      try {
        TableId tableId=StreamUtils.getStateStoreTableId(input.getNamespaceId().toId());
        return tableUtil.createHTableId(new NamespaceId(tableId.getNamespace()),tableId.getTableName());
      }
 catch (      IOException e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
}"
5815,"/** 
 * Gets a TableId for stream consumer state stores within a given namespace.
 * @param namespace the namespace for which the table is for.
 * @return constructed TableId
 */
public static TableId getStateStoreTableId(Id.Namespace namespace){
  String tableName=String.format(""String_Node_Str"",Id.Namespace.SYSTEM.getId(),QueueConstants.QueueType.STREAM.toString());
  return TableId.from(namespace.getId(),tableName);
}","/** 
 * Gets a TableId for stream consumer state stores within a given namespace.
 * @param namespace the namespace for which the table is for.
 * @return constructed TableId. Note that the namespace in the returned TableId is the CDAP namespace (CDAP-7344).
 */
public static TableId getStateStoreTableId(Id.Namespace namespace){
  String tableName=String.format(""String_Node_Str"",Id.Namespace.SYSTEM.getId(),QueueConstants.QueueType.STREAM.toString());
  return TableId.from(namespace.getId(),tableName);
}"
5816,"/** 
 * Lists all tables in the specified namespace
 * @param admin the {@link HBaseAdmin} to use to communicate with HBase
 * @param namespaceId namespace for which the tables are being requested
 */
public abstract List<TableId> listTablesInNamespace(HBaseAdmin admin,String namespaceId) throws IOException ;","/** 
 * Lists all tables in the specified namespace
 * @param admin the {@link HBaseAdmin} to use to communicate with HBase
 * @param namespaceId HBase namespace for which the tables are being requested
 */
public abstract List<TableId> listTablesInNamespace(HBaseAdmin admin,String namespaceId) throws IOException ;"
5817,"/** 
 * Gets the system configuration table prefix.
 * @param prefix Prefix string
 * @return System configuration table prefix (full table name minus the table qualifier).Example input: ""cdap_ns.table.name""  -->  output: ""cdap_system.""   (hbase 94) Example input: ""cdap.table.name""     -->  output: ""cdap_system.""   (hbase 94. input table is in default namespace) Example input: ""cdap_ns:table.name""  -->  output: ""cdap_system:""   (hbase 96, 98)
 */
public String getSysConfigTablePrefix(String prefix){
  return prefix + ""String_Node_Str"" + NamespaceId.SYSTEM.getNamespace()+ ""String_Node_Str"";
}","/** 
 * Gets the system configuration table prefix.
 * @param prefix Prefix string
 * @return System configuration table prefix (full table name minus the table qualifier).Example input: ""cdap.table.name""     -->  output: ""cdap_system.""   (input table is in default namespace) Example input: ""cdap_ns:table.name""  -->  output: ""cdap_system:""   (input table is in a custom namespace)
 */
public String getSysConfigTablePrefix(String prefix){
  return prefix + ""String_Node_Str"" + NamespaceId.SYSTEM.getNamespace()+ ""String_Node_Str"";
}"
5818,"@Test public void testNonTransactionalMixed() throws Exception {
  TableId tableId=TableId.from(Id.Namespace.DEFAULT.getId(),""String_Node_Str"");
  byte[] row1=Bytes.toBytes(""String_Node_Str"");
  byte[] col=Bytes.toBytes(""String_Node_Str"");
  try (HTable table=createTable(tableId)){
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
    table.put(tableUtil.buildPut(row1).add(FAMILY,col,Bytes.toBytes(11L)).build());
    assertColumn(table,row1,col,11);
    Delete delete=tableUtil.buildDelete(row1).deleteColumns(FAMILY,col).build();
    table.batch(Lists.newArrayList(delete));
    Get get=tableUtil.buildGet(row1).build();
    Result result=table.get(get);
    LOG.info(""String_Node_Str"" + result);
    assertTrue(result.isEmpty());
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
    delete=tableUtil.buildDelete(row1).deleteFamily(FAMILY).build();
    table.batch(Lists.newArrayList(delete));
    get=tableUtil.buildGet(row1).build();
    result=table.get(get);
    LOG.info(""String_Node_Str"" + result);
    assertTrue(result.isEmpty());
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
    delete=tableUtil.buildDelete(row1).build();
    table.batch(Lists.newArrayList(delete));
    get=tableUtil.buildGet(row1).build();
    result=table.get(get);
    LOG.info(""String_Node_Str"" + result);
    assertTrue(result.isEmpty());
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
  }
 }","@Test public void testNonTransactionalMixed() throws Exception {
  TableId tableId=TableId.from(Id.Namespace.DEFAULT.getId(),""String_Node_Str"");
  byte[] row1=Bytes.toBytes(""String_Node_Str"");
  byte[] col=Bytes.toBytes(""String_Node_Str"");
  try (HTable table=createTable(tableId)){
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
    table.put(tableUtil.buildPut(row1).add(FAMILY,col,Bytes.toBytes(11L)).build());
    assertColumn(table,row1,col,11);
    Delete delete=tableUtil.buildDelete(row1).deleteColumns(FAMILY,col).build();
    table.delete(delete);
    Get get=tableUtil.buildGet(row1).build();
    Result result=table.get(get);
    LOG.info(""String_Node_Str"" + result);
    assertTrue(result.isEmpty());
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
    delete=tableUtil.buildDelete(row1).deleteFamily(FAMILY).build();
    table.batch(Lists.newArrayList(delete));
    get=tableUtil.buildGet(row1).build();
    result=table.get(get);
    LOG.info(""String_Node_Str"" + result);
    assertTrue(result.isEmpty());
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
    delete=tableUtil.buildDelete(row1).build();
    table.delete(delete);
    get=tableUtil.buildGet(row1).build();
    result=table.get(get);
    LOG.info(""String_Node_Str"" + result);
    assertTrue(result.isEmpty());
    for (int i=0; i < 100; i++) {
      table.put(newIncrement(row1,col,1));
    }
    assertColumn(table,row1,col,100);
  }
 }"
5819,"/** 
 * Updates the TTL in the   {@link co.cask.cdap.data2.datafabric.dataset.service.mds.DatasetInstanceMDS}table for CDAP versions prior to 3.3. <p> The TTL for   {@link DatasetSpecification} was stored in milliseconds.Since the spec (as of CDAP version 3.3) is in seconds, the instance MDS entries must be updated. This is to be called only if the current CDAP version is < 3.3. </p>
 * @throws Exception
 */
public void upgrade() throws Exception {
  TableId datasetSpecId=TableId.from(Id.Namespace.SYSTEM.getId(),DatasetMetaTableUtil.INSTANCE_TABLE_NAME);
  HBaseAdmin hBaseAdmin=new HBaseAdmin(conf);
  if (!tableUtil.tableExists(hBaseAdmin,datasetSpecId)) {
    LOG.error(""String_Node_Str"",datasetSpecId);
    return;
  }
  HTable specTable=tableUtil.createHTable(conf,datasetSpecId);
  try {
    ScanBuilder scanBuilder=tableUtil.buildScan();
    scanBuilder.setTimeRange(0,HConstants.LATEST_TIMESTAMP);
    scanBuilder.setMaxVersions();
    try (ResultScanner resultScanner=specTable.getScanner(scanBuilder.build())){
      Result result;
      while ((result=resultScanner.next()) != null) {
        Put put=new Put(result.getRow());
        for (        Map.Entry<byte[],NavigableMap<byte[],NavigableMap<Long,byte[]>>> familyMap : result.getMap().entrySet()) {
          for (          Map.Entry<byte[],NavigableMap<Long,byte[]>> columnMap : familyMap.getValue().entrySet()) {
            for (            Map.Entry<Long,byte[]> columnEntry : columnMap.getValue().entrySet()) {
              Long timeStamp=columnEntry.getKey();
              byte[] colVal=columnEntry.getValue();
              if (colVal == null || colVal.length == 0) {
                continue;
              }
              String specEntry=Bytes.toString(colVal);
              DatasetSpecification specification=GSON.fromJson(specEntry,DatasetSpecification.class);
              DatasetSpecification updatedSpec=updateTTLInSpecification(specification,null);
              colVal=Bytes.toBytes(GSON.toJson(updatedSpec));
              put.add(familyMap.getKey(),columnMap.getKey(),timeStamp,colVal);
            }
          }
        }
        if (put.size() > 0) {
          specTable.put(put);
        }
      }
    }
   }
  finally {
    specTable.flushCommits();
    specTable.close();
  }
}","/** 
 * Updates the TTL in the   {@link co.cask.cdap.data2.datafabric.dataset.service.mds.DatasetInstanceMDS}table for CDAP versions prior to 3.3. <p> The TTL for   {@link DatasetSpecification} was stored in milliseconds.Since the spec (as of CDAP version 3.3) is in seconds, the instance MDS entries must be updated. This is to be called only if the current CDAP version is < 3.3. </p>
 * @throws Exception
 */
public void upgrade() throws Exception {
  TableId datasetSpecId=tableUtil.createHTableId(NamespaceId.SYSTEM,DatasetMetaTableUtil.INSTANCE_TABLE_NAME);
  HBaseAdmin hBaseAdmin=new HBaseAdmin(conf);
  if (!tableUtil.tableExists(hBaseAdmin,datasetSpecId)) {
    LOG.error(""String_Node_Str"",datasetSpecId);
    return;
  }
  HTable specTable=tableUtil.createHTable(conf,datasetSpecId);
  try {
    ScanBuilder scanBuilder=tableUtil.buildScan();
    scanBuilder.setTimeRange(0,HConstants.LATEST_TIMESTAMP);
    scanBuilder.setMaxVersions();
    try (ResultScanner resultScanner=specTable.getScanner(scanBuilder.build())){
      Result result;
      while ((result=resultScanner.next()) != null) {
        Put put=new Put(result.getRow());
        for (        Map.Entry<byte[],NavigableMap<byte[],NavigableMap<Long,byte[]>>> familyMap : result.getMap().entrySet()) {
          for (          Map.Entry<byte[],NavigableMap<Long,byte[]>> columnMap : familyMap.getValue().entrySet()) {
            for (            Map.Entry<Long,byte[]> columnEntry : columnMap.getValue().entrySet()) {
              Long timeStamp=columnEntry.getKey();
              byte[] colVal=columnEntry.getValue();
              if (colVal == null || colVal.length == 0) {
                continue;
              }
              String specEntry=Bytes.toString(colVal);
              DatasetSpecification specification=GSON.fromJson(specEntry,DatasetSpecification.class);
              DatasetSpecification updatedSpec=updateTTLInSpecification(specification,null);
              colVal=Bytes.toBytes(GSON.toJson(updatedSpec));
              put.add(familyMap.getKey(),columnMap.getKey(),timeStamp,colVal);
            }
          }
        }
        if (put.size() > 0) {
          specTable.put(put);
        }
      }
    }
   }
  finally {
    specTable.flushCommits();
    specTable.close();
  }
}"
5820,"@Override public TableId apply(NamespaceMeta input){
  return StreamUtils.getStateStoreTableId(Id.Namespace.from(input.getName()));
}","@Override public TableId apply(NamespaceMeta input){
  try {
    TableId tableId=StreamUtils.getStateStoreTableId(input.getNamespaceId().toId());
    return tableUtil.createHTableId(new NamespaceId(tableId.getNamespace()),tableId.getTableName());
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}"
5821,"@Override protected Iterable<TableId> getTableIds() throws Exception {
  return Lists.transform(namespaceQueryAdmin.list(),new Function<NamespaceMeta,TableId>(){
    @Override public TableId apply(    NamespaceMeta input){
      return StreamUtils.getStateStoreTableId(Id.Namespace.from(input.getName()));
    }
  }
);
}","@Override protected Iterable<TableId> getTableIds() throws Exception {
  return Lists.transform(namespaceQueryAdmin.list(),new Function<NamespaceMeta,TableId>(){
    @Override public TableId apply(    NamespaceMeta input){
      try {
        TableId tableId=StreamUtils.getStateStoreTableId(input.getNamespaceId().toId());
        return tableUtil.createHTableId(new NamespaceId(tableId.getNamespace()),tableId.getTableName());
      }
 catch (      IOException e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
}"
5822,"@Override public void run(DatasetContext context) throws Exception {
  BatchPhaseSpec phaseSpec=GSON.fromJson(sec.getSpecification().getProperty(Constants.PIPELINEID),BatchPhaseSpec.class);
  try (InputStream is=new FileInputStream(sec.getLocalizationContext().getLocalFile(""String_Node_Str""))){
    sourceFactory=SparkBatchSourceFactory.deserialize(is);
    sinkFactory=SparkBatchSinkFactory.deserialize(is);
    DataInputStream dataInputStream=new DataInputStream(is);
    stagePartitions=GSON.fromJson(dataInputStream.readUTF(),MAP_TYPE);
  }
   datasetContext=context;
  runPipeline(phaseSpec.getPhase(),BatchSource.PLUGIN_TYPE,sec,stagePartitions);
}","@Override public void run(DatasetContext context) throws Exception {
  BatchPhaseSpec phaseSpec=GSON.fromJson(sec.getSpecification().getProperty(Constants.PIPELINEID),BatchPhaseSpec.class);
  Path configFile=sec.getLocalizationContext().getLocalFile(""String_Node_Str"").toPath();
  try (BufferedReader reader=Files.newBufferedReader(configFile,StandardCharsets.UTF_8)){
    String object=reader.readLine();
    SparkBatchSourceSinkFactoryInfo sourceSinkInfo=GSON.fromJson(object,SparkBatchSourceSinkFactoryInfo.class);
    sourceFactory=sourceSinkInfo.getSparkBatchSourceFactory();
    sinkFactory=sourceSinkInfo.getSparkBatchSinkFactory();
    stagePartitions=sourceSinkInfo.getStagePartitions();
  }
   datasetContext=context;
  runPipeline(phaseSpec.getPhase(),BatchSource.PLUGIN_TYPE,sec,stagePartitions);
}"
5823,"@Override public void initialize() throws Exception {
  SparkClientContext context=getContext();
  cleanupFiles=new ArrayList<>();
  CompositeFinisher.Builder finishers=CompositeFinisher.builder();
  SparkConf sparkConf=new SparkConf();
  sparkConf.set(""String_Node_Str"",""String_Node_Str"");
  sparkConf.set(""String_Node_Str"",""String_Node_Str"");
  context.setSparkConf(sparkConf);
  Map<String,String> properties=context.getSpecification().getProperties();
  BatchPhaseSpec phaseSpec=GSON.fromJson(properties.get(Constants.PIPELINEID),BatchPhaseSpec.class);
  DatasetContextLookupProvider lookProvider=new DatasetContextLookupProvider(context);
  MacroEvaluator evaluator=new DefaultMacroEvaluator(context.getWorkflowToken(),context.getRuntimeArguments(),context.getLogicalStartTime(),context,context.getNamespace());
  SparkBatchSourceFactory sourceFactory=new SparkBatchSourceFactory();
  SparkBatchSinkFactory sinkFactory=new SparkBatchSinkFactory();
  Map<String,Integer> stagePartitions=new HashMap<>();
  for (  StageInfo stageInfo : phaseSpec.getPhase()) {
    String stageName=stageInfo.getName();
    String pluginType=stageInfo.getPluginType();
    if (BatchSource.PLUGIN_TYPE.equals(pluginType)) {
      BatchConfigurable<BatchSourceContext> batchSource=context.newPluginInstance(stageName,evaluator);
      BatchSourceContext sourceContext=new SparkBatchSourceContext(sourceFactory,context,lookProvider,stageName);
      batchSource.prepareRun(sourceContext);
      finishers.add(batchSource,sourceContext);
    }
 else     if (BatchSink.PLUGIN_TYPE.equals(pluginType)) {
      BatchConfigurable<BatchSinkContext> batchSink=context.newPluginInstance(stageName,evaluator);
      BatchSinkContext sinkContext=new SparkBatchSinkContext(sinkFactory,context,null,stageName);
      batchSink.prepareRun(sinkContext);
      finishers.add(batchSink,sinkContext);
    }
 else     if (SparkSink.PLUGIN_TYPE.equals(pluginType)) {
      BatchConfigurable<SparkPluginContext> sparkSink=context.newPluginInstance(stageName,evaluator);
      SparkPluginContext sparkPluginContext=new BasicSparkPluginContext(context,lookProvider,stageName);
      sparkSink.prepareRun(sparkPluginContext);
      finishers.add(sparkSink,sparkPluginContext);
    }
 else     if (BatchAggregator.PLUGIN_TYPE.equals(pluginType)) {
      BatchAggregator aggregator=context.newPluginInstance(stageName,evaluator);
      AbstractAggregatorContext aggregatorContext=new SparkAggregatorContext(context,new DatasetContextLookupProvider(context),stageName);
      aggregator.prepareRun(aggregatorContext);
      finishers.add(aggregator,aggregatorContext);
      stagePartitions.put(stageName,aggregatorContext.getNumPartitions());
    }
 else     if (BatchJoiner.PLUGIN_TYPE.equals(pluginType)) {
      BatchJoiner joiner=context.newPluginInstance(stageName,evaluator);
      SparkJoinerContext sparkJoinerContext=new SparkJoinerContext(stageName,context);
      joiner.prepareRun(sparkJoinerContext);
      finishers.add(joiner,sparkJoinerContext);
      stagePartitions.put(stageName,sparkJoinerContext.getNumPartitions());
    }
  }
  File configFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
  cleanupFiles.add(configFile);
  try (OutputStream os=new FileOutputStream(configFile)){
    sourceFactory.serialize(os);
    sinkFactory.serialize(os);
    DataOutput dataOutput=new DataOutputStream(os);
    dataOutput.writeUTF(GSON.toJson(stagePartitions));
  }
   finisher=finishers.build();
  context.localize(""String_Node_Str"",configFile.toURI());
}","@Override public void initialize() throws Exception {
  SparkClientContext context=getContext();
  cleanupFiles=new ArrayList<>();
  CompositeFinisher.Builder finishers=CompositeFinisher.builder();
  SparkConf sparkConf=new SparkConf();
  sparkConf.set(""String_Node_Str"",""String_Node_Str"");
  sparkConf.set(""String_Node_Str"",""String_Node_Str"");
  context.setSparkConf(sparkConf);
  Map<String,String> properties=context.getSpecification().getProperties();
  BatchPhaseSpec phaseSpec=GSON.fromJson(properties.get(Constants.PIPELINEID),BatchPhaseSpec.class);
  DatasetContextLookupProvider lookProvider=new DatasetContextLookupProvider(context);
  MacroEvaluator evaluator=new DefaultMacroEvaluator(context.getWorkflowToken(),context.getRuntimeArguments(),context.getLogicalStartTime(),context,context.getNamespace());
  SparkBatchSourceFactory sourceFactory=new SparkBatchSourceFactory();
  SparkBatchSinkFactory sinkFactory=new SparkBatchSinkFactory();
  Map<String,Integer> stagePartitions=new HashMap<>();
  for (  StageInfo stageInfo : phaseSpec.getPhase()) {
    String stageName=stageInfo.getName();
    String pluginType=stageInfo.getPluginType();
    if (BatchSource.PLUGIN_TYPE.equals(pluginType)) {
      BatchConfigurable<BatchSourceContext> batchSource=context.newPluginInstance(stageName,evaluator);
      BatchSourceContext sourceContext=new SparkBatchSourceContext(sourceFactory,context,lookProvider,stageName);
      batchSource.prepareRun(sourceContext);
      finishers.add(batchSource,sourceContext);
    }
 else     if (BatchSink.PLUGIN_TYPE.equals(pluginType)) {
      BatchConfigurable<BatchSinkContext> batchSink=context.newPluginInstance(stageName,evaluator);
      BatchSinkContext sinkContext=new SparkBatchSinkContext(sinkFactory,context,null,stageName);
      batchSink.prepareRun(sinkContext);
      finishers.add(batchSink,sinkContext);
    }
 else     if (SparkSink.PLUGIN_TYPE.equals(pluginType)) {
      BatchConfigurable<SparkPluginContext> sparkSink=context.newPluginInstance(stageName,evaluator);
      SparkPluginContext sparkPluginContext=new BasicSparkPluginContext(context,lookProvider,stageName);
      sparkSink.prepareRun(sparkPluginContext);
      finishers.add(sparkSink,sparkPluginContext);
    }
 else     if (BatchAggregator.PLUGIN_TYPE.equals(pluginType)) {
      BatchAggregator aggregator=context.newPluginInstance(stageName,evaluator);
      AbstractAggregatorContext aggregatorContext=new SparkAggregatorContext(context,new DatasetContextLookupProvider(context),stageName);
      aggregator.prepareRun(aggregatorContext);
      finishers.add(aggregator,aggregatorContext);
      stagePartitions.put(stageName,aggregatorContext.getNumPartitions());
    }
 else     if (BatchJoiner.PLUGIN_TYPE.equals(pluginType)) {
      BatchJoiner joiner=context.newPluginInstance(stageName,evaluator);
      SparkJoinerContext sparkJoinerContext=new SparkJoinerContext(stageName,context);
      joiner.prepareRun(sparkJoinerContext);
      finishers.add(joiner,sparkJoinerContext);
      stagePartitions.put(stageName,sparkJoinerContext.getNumPartitions());
    }
  }
  File configFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
  cleanupFiles.add(configFile);
  try (Writer writer=Files.newBufferedWriter(configFile.toPath(),StandardCharsets.UTF_8)){
    SparkBatchSourceSinkFactoryInfo sourceSinkInfo=new SparkBatchSourceSinkFactoryInfo(sourceFactory,sinkFactory,stagePartitions);
    writer.write(GSON.toJson(sourceSinkInfo));
  }
   finisher=finishers.build();
  context.localize(""String_Node_Str"",configFile.toURI());
}"
5824,"/** 
 * Unregisters all usage information of an application.
 * @param applicationId application
 */
@Override public void unregister(final Id.Application applicationId){
  execute(new TransactionExecutor.Procedure<UsageDataset>(){
    @Override public void apply(    UsageDataset usageDataset) throws Exception {
      usageDataset.unregister(applicationId);
    }
  }
);
  for (  DatasetUsageKey key : usageCache.asMap().keySet()) {
    if (applicationId.equals(key.getOwner().getApplication())) {
      usageCache.invalidate(key);
    }
  }
}","/** 
 * Unregisters all usage information of an application.
 * @param applicationId application
 */
@Override public void unregister(final Id.Application applicationId){
  execute(new TransactionExecutor.Procedure<UsageDataset>(){
    @Override public void apply(    UsageDataset usageDataset) throws Exception {
      usageDataset.unregister(applicationId);
    }
  }
);
}"
5825,"@Inject public DefaultUsageRegistry(TransactionExecutorFactory executorFactory,DatasetFramework datasetFramework){
  this.executorFactory=executorFactory;
  this.datasetFramework=datasetFramework;
  this.usageCache=CacheBuilder.newBuilder().maximumSize(1024).build(new CacheLoader<DatasetUsageKey,Boolean>(){
    @Override public Boolean load(    DatasetUsageKey key) throws Exception {
      doRegister(key.getOwner(),key.getDataset());
      return true;
    }
  }
);
}","@Inject DefaultUsageRegistry(TransactionExecutorFactory executorFactory,DatasetFramework datasetFramework){
  this.executorFactory=executorFactory;
  this.datasetFramework=datasetFramework;
}"
5826,"@Test public void testUsageRegistry(){
  UsageRegistry registry=new DefaultUsageRegistry(new TransactionExecutorFactory(){
    @Override public TransactionExecutor createExecutor(    Iterable<TransactionAware> iterable){
      return dsFrameworkUtil.newInMemoryTransactionExecutor(iterable);
    }
  }
,new ForwardingDatasetFramework(dsFrameworkUtil.getFramework()){
    @Nullable @Override public <T extends Dataset>T getDataset(    Id.DatasetInstance datasetInstanceId,    @Nullable Map<String,String> arguments,    @Nullable ClassLoader classLoader) throws DatasetManagementException, IOException {
      T t=super.getDataset(datasetInstanceId,arguments,classLoader);
      if (t instanceof UsageDataset) {
        @SuppressWarnings(""String_Node_Str"") T t1=(T)new WrappedUsageDataset((UsageDataset)t);
        return t1;
      }
      return t;
    }
  }
);
  registry.register(flow11,datasetInstance1);
  registry.register(flow12,stream1);
  registry.registerAll(ImmutableList.of(flow21,flow22),datasetInstance2);
  registry.registerAll(ImmutableList.of(flow21,flow22),stream1);
  int count=WrappedUsageDataset.registerCount;
  Assert.assertEquals(ImmutableSet.of(datasetInstance1),registry.getDatasets(flow11));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow12));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow21));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow22));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow21));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow22));
  Assert.assertEquals(ImmutableSet.of(flow11),registry.getPrograms(datasetInstance1));
  Assert.assertEquals(ImmutableSet.of(flow21,flow22),registry.getPrograms(datasetInstance2));
  Assert.assertEquals(ImmutableSet.of(flow12,flow21,flow22),registry.getPrograms(stream1));
  registry.register(flow11,datasetInstance1);
  registry.registerAll(ImmutableList.of(flow21,flow22),datasetInstance2);
  Assert.assertEquals(count,WrappedUsageDataset.registerCount);
  Assert.assertEquals(ImmutableSet.of(datasetInstance1),registry.getDatasets(flow11));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow12));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow21));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow22));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow21));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow22));
  Assert.assertEquals(ImmutableSet.of(flow11),registry.getPrograms(datasetInstance1));
  Assert.assertEquals(ImmutableSet.of(flow21,flow22),registry.getPrograms(datasetInstance2));
  Assert.assertEquals(ImmutableSet.of(flow12,flow21,flow22),registry.getPrograms(stream1));
  registry.unregister(flow11.getApplication());
  Assert.assertEquals(ImmutableSet.of(),registry.getDatasets(flow11));
  Assert.assertEquals(ImmutableSet.of(),registry.getStreams(flow12));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow21));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow22));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow21));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow22));
  Assert.assertEquals(ImmutableSet.of(),registry.getPrograms(datasetInstance1));
  Assert.assertEquals(ImmutableSet.of(flow21,flow22),registry.getPrograms(datasetInstance2));
  Assert.assertEquals(ImmutableSet.of(flow21,flow22),registry.getPrograms(stream1));
  registry.register(flow11,datasetInstance1);
  registry.register(flow12,stream1);
  Assert.assertEquals(ImmutableSet.of(datasetInstance1),registry.getDatasets(flow11));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow12));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow21));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow22));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow21));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow22));
  Assert.assertEquals(ImmutableSet.of(flow11),registry.getPrograms(datasetInstance1));
  Assert.assertEquals(ImmutableSet.of(flow21,flow22),registry.getPrograms(datasetInstance2));
  Assert.assertEquals(ImmutableSet.of(flow12,flow21,flow22),registry.getPrograms(stream1));
  Assert.assertEquals(count + 2,WrappedUsageDataset.registerCount);
}","@Test public void testUsageRegistry(){
  UsageRegistry registry=new DefaultUsageRegistry(new TransactionExecutorFactory(){
    @Override public TransactionExecutor createExecutor(    Iterable<TransactionAware> iterable){
      return dsFrameworkUtil.newInMemoryTransactionExecutor(iterable);
    }
  }
,new ForwardingDatasetFramework(dsFrameworkUtil.getFramework()){
    @Nullable @Override public <T extends Dataset>T getDataset(    Id.DatasetInstance datasetInstanceId,    @Nullable Map<String,String> arguments,    @Nullable ClassLoader classLoader) throws DatasetManagementException, IOException {
      T t=super.getDataset(datasetInstanceId,arguments,classLoader);
      if (t instanceof UsageDataset) {
        @SuppressWarnings(""String_Node_Str"") T t1=(T)new WrappedUsageDataset((UsageDataset)t);
        return t1;
      }
      return t;
    }
  }
);
  registry.register(flow11,datasetInstance1);
  registry.register(flow12,stream1);
  registry.registerAll(ImmutableList.of(flow21,flow22),datasetInstance2);
  registry.registerAll(ImmutableList.of(flow21,flow22),stream1);
  int count=WrappedUsageDataset.registerCount;
  Assert.assertEquals(ImmutableSet.of(datasetInstance1),registry.getDatasets(flow11));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow12));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow21));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow22));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow21));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow22));
  Assert.assertEquals(ImmutableSet.of(flow11),registry.getPrograms(datasetInstance1));
  Assert.assertEquals(ImmutableSet.of(flow21,flow22),registry.getPrograms(datasetInstance2));
  Assert.assertEquals(ImmutableSet.of(flow12,flow21,flow22),registry.getPrograms(stream1));
  registry.register(flow11,datasetInstance1);
  registry.registerAll(ImmutableList.of(flow21,flow22),datasetInstance2);
  count+=3;
  Assert.assertEquals(count,WrappedUsageDataset.registerCount);
  Assert.assertEquals(ImmutableSet.of(datasetInstance1),registry.getDatasets(flow11));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow12));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow21));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow22));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow21));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow22));
  Assert.assertEquals(ImmutableSet.of(flow11),registry.getPrograms(datasetInstance1));
  Assert.assertEquals(ImmutableSet.of(flow21,flow22),registry.getPrograms(datasetInstance2));
  Assert.assertEquals(ImmutableSet.of(flow12,flow21,flow22),registry.getPrograms(stream1));
  registry.unregister(flow11.getApplication());
  Assert.assertEquals(ImmutableSet.of(),registry.getDatasets(flow11));
  Assert.assertEquals(ImmutableSet.of(),registry.getStreams(flow12));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow21));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow22));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow21));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow22));
  Assert.assertEquals(ImmutableSet.of(),registry.getPrograms(datasetInstance1));
  Assert.assertEquals(ImmutableSet.of(flow21,flow22),registry.getPrograms(datasetInstance2));
  Assert.assertEquals(ImmutableSet.of(flow21,flow22),registry.getPrograms(stream1));
  registry.register(flow11,datasetInstance1);
  registry.register(flow12,stream1);
  Assert.assertEquals(ImmutableSet.of(datasetInstance1),registry.getDatasets(flow11));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow12));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow21));
  Assert.assertEquals(ImmutableSet.of(datasetInstance2),registry.getDatasets(flow22));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow21));
  Assert.assertEquals(ImmutableSet.of(stream1),registry.getStreams(flow22));
  Assert.assertEquals(ImmutableSet.of(flow11),registry.getPrograms(datasetInstance1));
  Assert.assertEquals(ImmutableSet.of(flow21,flow22),registry.getPrograms(datasetInstance2));
  Assert.assertEquals(ImmutableSet.of(flow12,flow21,flow22),registry.getPrograms(stream1));
  Assert.assertEquals(count + 2,WrappedUsageDataset.registerCount);
}"
5827,"private ResolvingDiscoverable(Discoverable discoverable){
  super(discoverable.getName(),discoverable.getSocketAddress());
}","private ResolvingDiscoverable(Discoverable discoverable){
  super(discoverable.getName(),discoverable.getSocketAddress(),discoverable.getPayload());
}"
5828,"@Override public JsonElement serialize(Discoverable src,Type typeOfSrc,JsonSerializationContext context){
  JsonObject jsonObj=new JsonObject();
  jsonObj.addProperty(""String_Node_Str"",src.getName());
  jsonObj.addProperty(""String_Node_Str"",src.getSocketAddress().getHostName());
  jsonObj.addProperty(""String_Node_Str"",src.getSocketAddress().getPort());
  return jsonObj;
}","@Override public JsonElement serialize(Discoverable src,Type typeOfSrc,JsonSerializationContext context){
  JsonObject jsonObj=new JsonObject();
  jsonObj.addProperty(""String_Node_Str"",src.getName());
  jsonObj.addProperty(""String_Node_Str"",src.getSocketAddress().getHostName());
  jsonObj.addProperty(""String_Node_Str"",src.getSocketAddress().getPort());
  jsonObj.add(""String_Node_Str"",context.serialize(src.getPayload()));
  return jsonObj;
}"
5829,"@Override public Discoverable deserialize(JsonElement json,Type typeOfT,JsonDeserializationContext context) throws JsonParseException {
  JsonObject jsonObj=json.getAsJsonObject();
  String service=jsonObj.get(""String_Node_Str"").getAsString();
  String hostname=jsonObj.get(""String_Node_Str"").getAsString();
  int port=jsonObj.get(""String_Node_Str"").getAsInt();
  InetSocketAddress address=new InetSocketAddress(hostname,port);
  return new Discoverable(service,address);
}","@Override public Discoverable deserialize(JsonElement json,Type typeOfT,JsonDeserializationContext context) throws JsonParseException {
  JsonObject jsonObj=json.getAsJsonObject();
  String service=jsonObj.get(""String_Node_Str"").getAsString();
  String hostname=jsonObj.get(""String_Node_Str"").getAsString();
  int port=jsonObj.get(""String_Node_Str"").getAsInt();
  InetSocketAddress address=new InetSocketAddress(hostname,port);
  byte[] payload=context.deserialize(jsonObj.get(""String_Node_Str""),BYTE_ARRAY_TYPE);
  return new Discoverable(service,address,payload);
}"
5830,"@Override public boolean accept(String className,URL classUrl,URL classPathUrl){
  String resourceName=className.replace('.','/') + ""String_Node_Str"";
  if (visibleResources.contains(resourceName)) {
    return false;
  }
  if (resourceName.startsWith(""String_Node_Str"")) {
    return true;
  }
  try {
    getClass().getClassLoader().loadClass(""String_Node_Str"");
    return !SparkRuntimeUtils.SPARK_PROGRAM_CLASS_LOADER_FILTER.acceptResource(resourceName);
  }
 catch (  ClassNotFoundException e) {
    LOG.warn(""String_Node_Str"",e);
    return true;
  }
}","@Override public boolean accept(String className,URL classUrl,URL classPathUrl){
  String resourceName=className.replace('.','/') + ""String_Node_Str"";
  if (visibleResources.contains(resourceName)) {
    return false;
  }
  if (resourceName.startsWith(""String_Node_Str"")) {
    return true;
  }
  try {
    getClass().getClassLoader().loadClass(""String_Node_Str"");
    return !SparkRuntimeUtils.SPARK_PROGRAM_CLASS_LOADER_FILTER.acceptResource(resourceName);
  }
 catch (  ClassNotFoundException e) {
    if (logWarnOnce.compareAndSet(false,true)) {
      LOG.warn(""String_Node_Str"");
    }
    return true;
  }
}"
5831,"@Override public boolean accept(String className,URL classUrl,URL classPathUrl){
  String resourceName=className.replace('.','/') + ""String_Node_Str"";
  if (visibleResources.contains(resourceName)) {
    return false;
  }
  if (resourceName.startsWith(""String_Node_Str"")) {
    return true;
  }
  return !SparkRuntimeUtils.SPARK_PROGRAM_CLASS_LOADER_FILTER.acceptResource(resourceName);
}","@Override public boolean accept(String className,URL classUrl,URL classPathUrl){
  String resourceName=className.replace('.','/') + ""String_Node_Str"";
  if (visibleResources.contains(resourceName)) {
    return false;
  }
  if (resourceName.startsWith(""String_Node_Str"")) {
    return true;
  }
  try {
    getClass().getClassLoader().loadClass(""String_Node_Str"");
    return !SparkRuntimeUtils.SPARK_PROGRAM_CLASS_LOADER_FILTER.acceptResource(resourceName);
  }
 catch (  ClassNotFoundException e) {
    LOG.warn(""String_Node_Str"",e);
    return true;
  }
}"
5832,"/** 
 * Split this dag into multiple dags. Each subdag will contain at most a single reduce node.
 * @return list of subdags.
 */
public List<Dag> split(){
  List<Dag> dags=new ArrayList<>();
  Set<String> remainingNodes=new HashSet<>();
  remainingNodes.addAll(nodes);
  Set<String> possibleNewSources=Sets.union(sources,connectors);
  Set<String> possibleNewSinks=Sets.union(sinks,connectors);
  for (  String reduceNode : reduceNodes) {
    Dag subdag=subsetAround(reduceNode,possibleNewSources,possibleNewSinks);
    remainingNodes.removeAll(subdag.getNodes());
    dags.add(subdag);
  }
  Set<String> remainingSources=Sets.intersection(remainingNodes,possibleNewSources);
  if (!remainingSources.isEmpty()) {
    dags.add(subsetFrom(remainingSources,possibleNewSinks));
  }
  return dags;
}","/** 
 * Split this dag into multiple dags. Each subdag will contain at most a single reduce node.
 * @return list of subdags.
 */
public List<Dag> split(){
  List<Dag> dags=new ArrayList<>();
  Set<String> remainingNodes=new HashSet<>();
  remainingNodes.addAll(nodes);
  Set<String> possibleNewSources=Sets.union(sources,connectors);
  Set<String> possibleNewSinks=Sets.union(sinks,connectors);
  for (  String reduceNode : reduceNodes) {
    Dag subdag=subsetAround(reduceNode,possibleNewSources,possibleNewSinks);
    remainingNodes.removeAll(subdag.getNodes());
    dags.add(subdag);
  }
  Set<String> remainingSources=Sets.intersection(remainingNodes,possibleNewSources);
  Set<String> processedNodes=new HashSet<>();
  if (!remainingSources.isEmpty()) {
    Map<String,Set<String>> nodesAccessibleBySources=new HashMap<>();
    for (    String remainingSource : remainingSources) {
      Dag remainingNodesDag=subsetFrom(remainingSource,possibleNewSinks);
      nodesAccessibleBySources.put(remainingSource,remainingNodesDag.getNodes());
    }
    for (    String remainingSource : remainingSources) {
      if (processedNodes.contains(remainingSource)) {
        continue;
      }
      Set<String> remainingAccessibleNodes=nodesAccessibleBySources.get(remainingSource);
      Set<String> islandNodes=new HashSet<>();
      islandNodes.addAll(remainingAccessibleNodes);
      for (      String otherSource : remainingSources) {
        if (remainingSource.equals(otherSource)) {
          continue;
        }
        Set<String> otherAccessibleNodes=nodesAccessibleBySources.get(otherSource);
        if (!Sets.intersection(remainingAccessibleNodes,otherAccessibleNodes).isEmpty()) {
          islandNodes.addAll(otherAccessibleNodes);
        }
      }
      dags.add(createSubDag(islandNodes));
      processedNodes.addAll(islandNodes);
    }
  }
  return dags;
}"
5833,"public UpgradeTool() throws Exception {
  this.cConf=CConfiguration.create();
  this.hConf=HBaseConfiguration.create();
  Injector injector=createInjector();
  this.txService=injector.getInstance(TransactionService.class);
  this.zkClientService=injector.getInstance(ZKClientService.class);
  this.dsFramework=injector.getInstance(DatasetFramework.class);
  this.metadataStore=injector.getInstance(MetadataStore.class);
  this.streamStateStoreUpgrader=injector.getInstance(StreamStateStoreUpgrader.class);
  this.dsUpgrade=injector.getInstance(DatasetUpgrader.class);
  this.dsSpecUpgrader=injector.getInstance(DatasetSpecificationUpgrader.class);
  this.queueAdmin=injector.getInstance(QueueAdmin.class);
  this.nsStore=injector.getInstance(NamespaceStore.class);
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        UpgradeTool.this.stop();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
);
  this.existingEntitySystemMetadataWriter=injector.getInstance(ExistingEntitySystemMetadataWriter.class);
  this.datasetServiceManager=injector.getInstance(DatasetServiceManager.class);
}","public UpgradeTool() throws Exception {
  this.cConf=CConfiguration.create();
  this.hConf=HBaseConfiguration.create();
  Injector injector=createInjector();
  this.txService=injector.getInstance(TransactionService.class);
  this.zkClientService=injector.getInstance(ZKClientService.class);
  this.dsFramework=injector.getInstance(DatasetFramework.class);
  this.metadataStore=injector.getInstance(MetadataStore.class);
  this.streamStateStoreUpgrader=injector.getInstance(StreamStateStoreUpgrader.class);
  this.dsUpgrade=injector.getInstance(DatasetUpgrader.class);
  this.dsSpecUpgrader=injector.getInstance(DatasetSpecificationUpgrader.class);
  this.queueAdmin=injector.getInstance(QueueAdmin.class);
  this.nsStore=injector.getInstance(NamespaceStore.class);
  this.authorizationService=injector.getInstance(AuthorizationEnforcementService.class);
  Runtime.getRuntime().addShutdownHook(new Thread(){
    @Override public void run(){
      try {
        UpgradeTool.this.stop();
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
  }
);
  this.existingEntitySystemMetadataWriter=injector.getInstance(ExistingEntitySystemMetadataWriter.class);
  this.upgradeDatasetServiceManager=injector.getInstance(UpgradeDatasetServiceManager.class);
}"
5834,"private void performUpgrade() throws Exception {
  performCoprocessorUpgrade();
  LOG.info(""String_Node_Str"");
  dsSpecUpgrader.upgrade();
  LOG.info(""String_Node_Str"");
  streamStateStoreUpgrader.upgrade();
  datasetServiceManager.startUp();
  LOG.info(""String_Node_Str"");
  try {
    existingEntitySystemMetadataWriter.write(datasetServiceManager.getDSFramework());
    LOG.info(""String_Node_Str"");
    DeletedDatasetMetadataRemover datasetMetadataRemover=new DeletedDatasetMetadataRemover(nsStore,metadataStore,datasetServiceManager.getDSFramework());
    datasetMetadataRemover.remove();
    LOG.info(""String_Node_Str"");
    metadataStore.deleteAllIndexes();
    LOG.info(""String_Node_Str"");
    metadataStore.rebuildIndexes();
  }
  finally {
    datasetServiceManager.shutDown();
  }
}","private void performUpgrade() throws Exception {
  performCoprocessorUpgrade();
  LOG.info(""String_Node_Str"");
  dsSpecUpgrader.upgrade();
  LOG.info(""String_Node_Str"");
  streamStateStoreUpgrader.upgrade();
  upgradeDatasetServiceManager.startUp();
  LOG.info(""String_Node_Str"");
  try {
    existingEntitySystemMetadataWriter.write(upgradeDatasetServiceManager.getDSFramework());
    LOG.info(""String_Node_Str"");
    DeletedDatasetMetadataRemover datasetMetadataRemover=new DeletedDatasetMetadataRemover(nsStore,metadataStore,upgradeDatasetServiceManager.getDSFramework());
    datasetMetadataRemover.remove();
    LOG.info(""String_Node_Str"");
    metadataStore.deleteAllIndexes();
    LOG.info(""String_Node_Str"");
    metadataStore.rebuildIndexes();
  }
  finally {
    upgradeDatasetServiceManager.shutDown();
  }
}"
5835,"/** 
 * Stop services and
 */
private void stop(){
  try {
    txService.stopAndWait();
    zkClientService.stopAndWait();
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    Runtime.getRuntime().halt(1);
  }
}","/** 
 * Stop services and
 */
private void stop(){
  try {
    txService.stopAndWait();
    zkClientService.stopAndWait();
    authorizationService.stopAndWait();
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    Runtime.getRuntime().halt(1);
  }
}"
5836,"private Injector createInjector() throws Exception {
  return Guice.createInjector(new ConfigModule(cConf,hConf),new LocationRuntimeModule().getDistributedModules(),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),Modules.override(new DataSetsModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(DatasetFramework.class).to(InMemoryDatasetFramework.class).in(Scopes.SINGLETON);
      bind(DatasetFramework.class).annotatedWith(Names.named(DataSetsModules.BASE_DATASET_FRAMEWORK)).to(DatasetFramework.class);
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(LineageWriter.class).to(NoOpLineageWriter.class);
      bind(MetadataChangePublisher.class).to(NoOpMetadataChangePublisher.class);
    }
  }
),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new TwillModule(),new ExploreClientModule(),new AppFabricServiceRuntimeModule().getDistributedModules(),new ProgramRunnerRuntimeModule().getDistributedModules(),new ServiceStoreModules().getDistributedModules(),new SystemDatasetRuntimeModule().getDistributedModules(),new NotificationServiceRuntimeModule().getInMemoryModules(),new KafkaClientModule(),new NamespaceStoreModule().getDistributedModules(),new AuthenticationContextModules().getMasterModule(),new AuthorizationModule(),new AuthorizationEnforcementModule().getMasterModule(),new SecureStoreModules().getDistributedModules(),new DataFabricModules().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Scopes.SINGLETON);
      bind(MetricDatasetFactory.class).to(DefaultMetricDatasetFactory.class).in(Scopes.SINGLETON);
      bind(MetricStore.class).to(DefaultMetricStore.class);
    }
    @Provides @Singleton @Named(""String_Node_Str"") @SuppressWarnings(""String_Node_Str"") public DatasetInstanceManager getDatasetInstanceManager(    TransactionSystemClientService txClient,    TransactionExecutorFactory txExecutorFactory,    @Named(""String_Node_Str"") DatasetFramework framework){
      return new DatasetInstanceManager(txClient,txExecutorFactory,framework);
    }
    @Provides @Singleton @Named(""String_Node_Str"") @SuppressWarnings(""String_Node_Str"") public DatasetFramework getInDsFramework(    DatasetFramework dsFramework){
      return dsFramework;
    }
  }
);
}","@VisibleForTesting Injector createInjector() throws Exception {
  return Guice.createInjector(new ConfigModule(cConf,hConf),new LocationRuntimeModule().getDistributedModules(),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),Modules.override(new DataSetsModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(DatasetFramework.class).to(InMemoryDatasetFramework.class).in(Scopes.SINGLETON);
      bind(DatasetFramework.class).annotatedWith(Names.named(DataSetsModules.BASE_DATASET_FRAMEWORK)).to(DatasetFramework.class);
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(LineageWriter.class).to(NoOpLineageWriter.class);
      bind(MetadataChangePublisher.class).to(NoOpMetadataChangePublisher.class);
    }
  }
),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new TwillModule(),new ExploreClientModule(),new AppFabricServiceRuntimeModule().getDistributedModules(),new ProgramRunnerRuntimeModule().getDistributedModules(),new ServiceStoreModules().getDistributedModules(),new SystemDatasetRuntimeModule().getDistributedModules(),new NotificationServiceRuntimeModule().getInMemoryModules(),new KafkaClientModule(),new NamespaceStoreModule().getDistributedModules(),new AuthenticationContextModules().getMasterModule(),new AuthorizationModule(),new AuthorizationEnforcementModule().getMasterModule(),new SecureStoreModules().getDistributedModules(),new DataFabricModules().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Scopes.SINGLETON);
      bind(MetricDatasetFactory.class).to(DefaultMetricDatasetFactory.class).in(Scopes.SINGLETON);
      bind(MetricStore.class).to(DefaultMetricStore.class);
    }
    @Provides @Singleton @Named(""String_Node_Str"") @SuppressWarnings(""String_Node_Str"") public DatasetInstanceManager getDatasetInstanceManager(    TransactionSystemClientService txClient,    TransactionExecutorFactory txExecutorFactory,    @Named(""String_Node_Str"") DatasetFramework framework){
      return new DatasetInstanceManager(txClient,txExecutorFactory,framework);
    }
    @Provides @Singleton @Named(""String_Node_Str"") @SuppressWarnings(""String_Node_Str"") public DatasetFramework getInDsFramework(    DatasetFramework dsFramework){
      return dsFramework;
    }
  }
);
}"
5837,"/** 
 * Do the start up work
 */
private void startUp() throws Exception {
  zkClientService.startAndWait();
  txService.startAndWait();
  initializeDSFramework(cConf,dsFramework);
}","/** 
 * Do the start up work
 */
private void startUp() throws Exception {
  zkClientService.startAndWait();
  txService.startAndWait();
  authorizationService.startAndWait();
  initializeDSFramework(cConf,dsFramework);
}"
5838,"@Test public void testInjector() throws Exception {
  new UpgradeTool();
}","@Test public void testInjector() throws Exception {
  Injector upgradeToolInjector=new UpgradeTool().createInjector();
  upgradeToolInjector.getInstance(UpgradeDatasetServiceManager.class);
}"
5839,"@Override public PartitionConsumerResult doConsume(ConsumerWorkingSet workingSet,PartitionAcceptor acceptor){
  doExpiry(workingSet);
  workingSet.populate(getPartitionedFileSet(),getConfiguration());
  List<PartitionDetail> toConsume=selectPartitions(acceptor,workingSet.getPartitions());
  return new PartitionConsumerResult(toConsume,removeDiscardedPartitions(workingSet));
}","@Override public PartitionConsumerResult doConsume(ConsumerWorkingSet workingSet,PartitionAcceptor acceptor){
  doExpiry(workingSet);
  workingSet.populate(getPartitionedFileSet(),getConfiguration());
  List<PartitionDetail> toConsume=selectPartitions(acceptor,workingSet);
  return new PartitionConsumerResult(toConsume,removeDiscardedPartitions(workingSet));
}"
5840,"private List<PartitionDetail> selectPartitions(PartitionAcceptor acceptor,List<? extends ConsumablePartition> partitions){
  long now=System.currentTimeMillis();
  List<PartitionDetail> toConsume=new ArrayList<>();
  for (  ConsumablePartition consumablePartition : partitions) {
    if (ProcessState.AVAILABLE != consumablePartition.getProcessState()) {
      continue;
    }
    PartitionDetail partition=getPartitionedFileSet().getPartition(consumablePartition.getPartitionKey());
    if (partition == null) {
      continue;
    }
    PartitionAcceptor.Return accept=acceptor.accept(partition);
switch (accept) {
case ACCEPT:
      consumablePartition.take();
    consumablePartition.setTimestamp(now);
  toConsume.add(partition);
continue;
case SKIP:
continue;
case STOP:
return toConsume;
}
}
return toConsume;
}","private List<PartitionDetail> selectPartitions(PartitionAcceptor acceptor,ConsumerWorkingSet workingSet){
  long now=System.currentTimeMillis();
  List<PartitionDetail> toConsume=new ArrayList<>();
  Iterator<ConsumablePartition> iter=workingSet.getPartitions().iterator();
  while (iter.hasNext()) {
    ConsumablePartition consumablePartition=iter.next();
    if (ProcessState.AVAILABLE != consumablePartition.getProcessState()) {
      continue;
    }
    PartitionDetail partition=getPartitionedFileSet().getPartition(consumablePartition.getPartitionKey());
    if (partition == null) {
      iter.remove();
      continue;
    }
    PartitionAcceptor.Return accept=acceptor.accept(partition);
switch (accept) {
case ACCEPT:
      consumablePartition.take();
    consumablePartition.setTimestamp(now);
  toConsume.add(partition);
continue;
case SKIP:
continue;
case STOP:
return toConsume;
}
}
return toConsume;
}"
5841,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(instanceId,principal,Action.ADMIN);
  privilegesManager.grant(namespace,principal,EnumSet.allOf(Action.class));
  String executionUserName;
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    ImpersonationInfo impersonationInfo=new ImpersonationInfo(metadata,cConf);
    String namespacePrincipal=impersonationInfo.getPrincipal();
    executionUserName=new KerberosName(namespacePrincipal).getShortName();
  }
 else {
    executionUserName=UserGroupInformation.getCurrentUser().getShortUserName();
  }
  Principal executionUser=new Principal(executionUserName,Principal.PrincipalType.USER);
  privilegesManager.grant(namespace,executionUser,EnumSet.allOf(Action.class));
  nsStore.create(metadata);
  try {
    impersonator.doAs(metadata,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.get().create(metadata);
        return null;
      }
    }
);
  }
 catch (  IOException|ExploreException|SQLException e) {
    deleteNamespaceMeta(metadata.getNamespaceId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace.toId(),e);
  }
}","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(instanceId,principal,Action.ADMIN);
  privilegesManager.grant(namespace,principal,EnumSet.allOf(Action.class));
  String executionUserName;
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    ImpersonationInfo impersonationInfo=new ImpersonationInfo(metadata,cConf);
    String namespacePrincipal=impersonationInfo.getPrincipal();
    executionUserName=new KerberosName(namespacePrincipal).getShortName();
  }
 else {
    executionUserName=UserGroupInformation.getCurrentUser().getShortUserName();
  }
  Principal executionUser=new Principal(executionUserName,Principal.PrincipalType.USER);
  privilegesManager.grant(namespace,executionUser,EnumSet.allOf(Action.class));
  nsStore.create(metadata);
  try {
    impersonator.doAs(metadata,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.get().create(metadata);
        return null;
      }
    }
);
  }
 catch (  Throwable t) {
    deleteNamespaceMeta(metadata.getNamespaceId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace.toId(),t);
  }
}"
5842,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(instanceId,principal,Action.ADMIN);
  privilegesManager.grant(namespace,principal,EnumSet.allOf(Action.class));
  String executionUserName;
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    ImpersonationInfo impersonationInfo=new ImpersonationInfo(metadata,cConf);
    String namespacePrincipal=impersonationInfo.getPrincipal();
    executionUserName=new KerberosName(namespacePrincipal).getShortName();
  }
 else {
    executionUserName=UserGroupInformation.getCurrentUser().getShortUserName();
  }
  Principal executionUser=new Principal(executionUserName,Principal.PrincipalType.USER);
  privilegesManager.grant(namespace,executionUser,EnumSet.allOf(Action.class));
  nsStore.create(metadata);
  try {
    impersonator.doAs(metadata,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.get().create(metadata);
        return null;
      }
    }
);
  }
 catch (  IOException|ExploreException|SQLException e) {
    deleteNamespaceMeta(metadata.getNamespaceId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace.toId(),e);
  }
}","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(instanceId,principal,Action.ADMIN);
  privilegesManager.grant(namespace,principal,EnumSet.allOf(Action.class));
  String executionUserName;
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    ImpersonationInfo impersonationInfo=new ImpersonationInfo(metadata,cConf);
    String namespacePrincipal=impersonationInfo.getPrincipal();
    executionUserName=new KerberosName(namespacePrincipal).getShortName();
  }
 else {
    executionUserName=UserGroupInformation.getCurrentUser().getShortUserName();
  }
  Principal executionUser=new Principal(executionUserName,Principal.PrincipalType.USER);
  privilegesManager.grant(namespace,executionUser,EnumSet.allOf(Action.class));
  nsStore.create(metadata);
  try {
    impersonator.doAs(metadata,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.get().create(metadata);
        return null;
      }
    }
);
  }
 catch (  Throwable t) {
    deleteNamespaceMeta(metadata.getNamespaceId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace.toId(),t);
  }
}"
5843,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(instanceId,principal,Action.ADMIN);
  privilegesManager.grant(namespace,principal,EnumSet.allOf(Action.class));
  String executionUserName;
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    ImpersonationInfo impersonationInfo=new ImpersonationInfo(metadata,cConf);
    String namespacePrincipal=impersonationInfo.getPrincipal();
    executionUserName=new KerberosName(namespacePrincipal).getShortName();
  }
 else {
    executionUserName=UserGroupInformation.getCurrentUser().getShortUserName();
  }
  Principal executionUser=new Principal(executionUserName,Principal.PrincipalType.USER);
  privilegesManager.grant(namespace,executionUser,EnumSet.allOf(Action.class));
  nsStore.create(metadata);
  try {
    impersonator.doAs(metadata,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.get().create(metadata);
        return null;
      }
    }
);
  }
 catch (  IOException|ExploreException|SQLException e) {
    deleteNamespaceMeta(metadata.getNamespaceId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace.toId(),e);
  }
}","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(instanceId,principal,Action.ADMIN);
  privilegesManager.grant(namespace,principal,EnumSet.allOf(Action.class));
  String executionUserName;
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    ImpersonationInfo impersonationInfo=new ImpersonationInfo(metadata,cConf);
    String namespacePrincipal=impersonationInfo.getPrincipal();
    executionUserName=new KerberosName(namespacePrincipal).getShortName();
  }
 else {
    executionUserName=UserGroupInformation.getCurrentUser().getShortUserName();
  }
  Principal executionUser=new Principal(executionUserName,Principal.PrincipalType.USER);
  privilegesManager.grant(namespace,executionUser,EnumSet.allOf(Action.class));
  nsStore.create(metadata);
  try {
    impersonator.doAs(metadata,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.get().create(metadata);
        return null;
      }
    }
);
  }
 catch (  Throwable t) {
    deleteNamespaceMeta(metadata.getNamespaceId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace.toId(),t);
  }
}"
5844,"@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  ETLConfig that=(ETLConfig)o;
  return Objects.equals(source,that.source) && Objects.equals(sinks,that.sinks) && Objects.equals(transforms,that.transforms)&& Objects.equals(connections,that.connections)&& Objects.equals(resources,that.resources)&& isStageLoggingEnabled() == that.isStageLoggingEnabled();
}","@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  ETLConfig that=(ETLConfig)o;
  return Objects.equals(source,that.source) && Objects.equals(sinks,that.sinks) && Objects.equals(transforms,that.transforms)&& Objects.equals(connections,that.connections)&& Objects.equals(resources,that.resources)&& Objects.equals(isStageLoggingEnabled(),that.isStageLoggingEnabled());
}"
5845,"/** 
 * Gets the value of the given metrics.
 * @param tags tags for the request
 * @param metrics names of the metrics
 * @param groupBys groupBys for the request
 * @return values of the metrics
 * @throws IOException if a network error occurred
 * @throws UnauthenticatedException if the request is not authorized successfully in the gateway server
 */
public MetricQueryResult query(Map<String,String> tags,List<String> metrics,List<String> groupBys,@Nullable Map<String,String> timeRangeParams) throws IOException, UnauthenticatedException, UnauthorizedException {
  List<String> queryParts=Lists.newArrayList();
  queryParts.add(""String_Node_Str"");
  add(""String_Node_Str"",metrics,queryParts);
  add(""String_Node_Str"",groupBys,queryParts);
  addTags(tags,queryParts);
  addTimeRangeParametersToQuery(timeRangeParams,queryParts);
  URL url=config.resolveURLV3(String.format(""String_Node_Str"",Joiner.on(""String_Node_Str"").join(queryParts)));
  HttpResponse response=restClient.execute(HttpMethod.POST,url,config.getAccessToken());
  return ObjectResponse.fromJsonBody(response,MetricQueryResult.class).getResponseObject();
}","/** 
 * Gets the value of the given metrics.
 * @param tags tags for the request
 * @param metrics names of the metrics
 * @param groupBys groupBys for the request
 * @param timeRangeParams parameters specifying the time range
 * @return values of the metrics
 * @throws IOException if a network error occurred
 * @throws UnauthenticatedException if the request is not authorized successfully in the gateway server
 */
public MetricQueryResult query(Map<String,String> tags,List<String> metrics,List<String> groupBys,Map<String,String> timeRangeParams) throws IOException, UnauthenticatedException, UnauthorizedException {
  List<String> queryParts=Lists.newArrayList();
  queryParts.add(""String_Node_Str"");
  add(""String_Node_Str"",metrics,queryParts);
  add(""String_Node_Str"",groupBys,queryParts);
  addTags(tags,queryParts);
  addTimeRangeParametersToQuery(timeRangeParams,queryParts);
  URL url=config.resolveURLV3(String.format(""String_Node_Str"",Joiner.on(""String_Node_Str"").join(queryParts)));
  HttpResponse response=restClient.execute(HttpMethod.POST,url,config.getAccessToken());
  return ObjectResponse.fromJsonBody(response,MetricQueryResult.class).getResponseObject();
}"
5846,"@Override public boolean equals(Object obj){
  if (this == obj) {
    return true;
  }
  if (KeyRange.class != obj.getClass()) {
    return false;
  }
  KeyRange other=(KeyRange)obj;
  return Arrays.equals(this.start,other.start) && Arrays.equals(this.stop,other.stop);
}","@Override public boolean equals(Object obj){
  if (this == obj) {
    return true;
  }
  if (obj == null || KeyRange.class != obj.getClass()) {
    return false;
  }
  KeyRange other=(KeyRange)obj;
  return Arrays.equals(this.start,other.start) && Arrays.equals(this.stop,other.stop);
}"
5847,"/** 
 * Given a key prefix, return the smallest key that is greater than all keys starting with that prefix.
 */
static byte[] rowAfterPrefix(byte[] prefix){
  Preconditions.checkNotNull(""String_Node_Str"",prefix);
  for (int i=prefix.length - 1; i >= 0; i--) {
    if (prefix[i] != (byte)0xff) {
      byte[] after=Arrays.copyOf(prefix,i + 1);
      ++after[i];
      return after;
    }
  }
  return null;
}","/** 
 * Given a key prefix, return the smallest key that is greater than all keys starting with that prefix.
 */
static byte[] rowAfterPrefix(byte[] prefix){
  Preconditions.checkNotNull(prefix,""String_Node_Str"");
  for (int i=prefix.length - 1; i >= 0; i--) {
    if (prefix[i] != (byte)0xff) {
      byte[] after=Arrays.copyOf(prefix,i + 1);
      ++after[i];
      return after;
    }
  }
  return null;
}"
5848,"@Override public boolean equals(Object o){
  if (!(o instanceof TimeseriesId) || o == null) {
    return false;
  }
  TimeseriesId other=(TimeseriesId)o;
  return Objects.equal(context,other.context) && Objects.equal(metric,other.metric) && Objects.equal(tag,other.tag)&& Objects.equal(runId,other.runId);
}","@Override public boolean equals(Object o){
  if (!(o instanceof TimeseriesId)) {
    return false;
  }
  TimeseriesId other=(TimeseriesId)o;
  return Objects.equal(context,other.context) && Objects.equal(metric,other.metric) && Objects.equal(tag,other.tag)&& Objects.equal(runId,other.runId);
}"
5849,"@SuppressWarnings(""String_Node_Str"") @Override public void write(DataOutput out) throws IOException {
  String schemaStr=record.getSchema().toString();
  String recordStr=StructuredRecordStringConverter.toJsonString(record);
  out.writeInt(schemaStr.length());
  out.write(Bytes.toBytes(schemaStr));
  out.writeInt(recordStr.length());
  out.write(Bytes.toBytes(recordStr));
}","@SuppressWarnings(""String_Node_Str"") @Override public void write(DataOutput out) throws IOException {
  byte[] schemaBytes=Bytes.toBytes(record.getSchema().toString());
  out.writeInt(schemaBytes.length);
  out.write(schemaBytes);
  byte[] recordBytes=Bytes.toBytes(StructuredRecordStringConverter.toJsonString(record));
  out.writeInt(recordBytes.length);
  out.write(recordBytes);
}"
5850,"@Override public ProgramController call() throws Exception {
  return launch(program,options,localizeResources,tempDir,new ApplicationLauncher(){
    @Override public TwillController launch(    TwillApplication twillApplication,    Iterable<String> extraClassPaths,    Iterable<? extends Class<?>> extraDependencies){
      TwillPreparer twillPreparer=twillRunner.prepare(twillApplication);
      twillPreparer.withEnv(Collections.singletonMap(""String_Node_Str"",""String_Node_Str""));
      if (options.isDebug()) {
        twillPreparer.enableDebugging();
      }
      LOG.info(""String_Node_Str"",program.getId(),options.isDebug(),programOptions,logbackURI);
      if (schedulerQueueName != null && !schedulerQueueName.isEmpty()) {
        LOG.info(""String_Node_Str"",program.getId(),schedulerQueueName);
        twillPreparer.setSchedulerQueue(schedulerQueueName);
      }
      if (logbackURI != null) {
        twillPreparer.withResources(logbackURI);
      }
      String logLevelConf=cConf.get(Constants.COLLECT_APP_CONTAINER_LOG_LEVEL).toUpperCase();
      if (""String_Node_Str"".equals(logLevelConf)) {
        twillPreparer.addJVMOptions(""String_Node_Str"");
      }
 else {
        LogEntry.Level logLevel=LogEntry.Level.ERROR;
        if (""String_Node_Str"".equals(logLevelConf)) {
          logLevel=LogEntry.Level.TRACE;
        }
 else {
          try {
            logLevel=LogEntry.Level.valueOf(logLevelConf.toUpperCase());
          }
 catch (          Exception e) {
            LOG.warn(""String_Node_Str"",logLevelConf);
          }
        }
        twillPreparer.addLogHandler(new ApplicationLogHandler(new PrinterLogHandler(new PrintWriter(System.out)),logLevel));
      }
      String yarnAppClassPath=hConf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
      if (User.isHBaseSecurityEnabled(hConf) || UserGroupInformation.isSecurityEnabled()) {
        twillPreparer.addSecureStore(secureStoreUpdater.update());
      }
      Iterable<Class<?>> dependencies=Iterables.concat(Collections.singletonList(HBaseTableUtilFactory.getHBaseTableUtilClass()),getKMSSecureStore(cConf),extraDependencies);
      twillPreparer.withDependencies(dependencies).withClassPaths(Iterables.concat(extraClassPaths,Splitter.on(',').trimResults().split(hConf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,""String_Node_Str"")))).withApplicationClassPaths(Splitter.on(""String_Node_Str"").trimResults().split(yarnAppClassPath)).withBundlerClassAcceptor(new HadoopClassExcluder(){
        @Override public boolean accept(        String className,        URL classUrl,        URL classPathUrl){
          return super.accept(className,classUrl,classPathUrl) && !className.startsWith(""String_Node_Str"");
        }
      }
).withApplicationArguments(""String_Node_Str"" + RunnableOptions.JAR,programJarName,""String_Node_Str"" + RunnableOptions.HADOOP_CONF_FILE,HADOOP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.CDAP_CONF_FILE,CDAP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.APP_SPEC_FILE,APP_SPEC_FILE_NAME,""String_Node_Str"" + RunnableOptions.PROGRAM_OPTIONS,programOptions,""String_Node_Str"" + RunnableOptions.PROGRAM_ID,GSON.toJson(program.getId().toEntityId()));
      TwillController twillController;
      ClassLoader oldClassLoader=ClassLoaders.setContextClassLoader(new CombineClassLoader(AbstractDistributedProgramRunner.this.getClass().getClassLoader(),Iterables.transform(dependencies,new Function<Class<?>,ClassLoader>(){
        @Override public ClassLoader apply(        Class<?> input){
          return input.getClassLoader();
        }
      }
)));
      try {
        twillController=twillPreparer.start();
      }
  finally {
        ClassLoaders.setContextClassLoader(oldClassLoader);
      }
      return addCleanupListener(twillController,program,tempDir);
    }
  }
);
}","@Override public ProgramController call() throws Exception {
  return launch(program,options,localizeResources,tempDir,new ApplicationLauncher(){
    @Override public TwillController launch(    TwillApplication twillApplication,    Iterable<String> extraClassPaths,    Iterable<? extends Class<?>> extraDependencies){
      TwillPreparer twillPreparer=twillRunner.prepare(twillApplication);
      twillPreparer.withEnv(Collections.singletonMap(""String_Node_Str"",""String_Node_Str""));
      if (options.isDebug()) {
        twillPreparer.enableDebugging();
      }
      LOG.info(""String_Node_Str"",program.getId(),options.isDebug(),programOptions,logbackURI);
      if (schedulerQueueName != null && !schedulerQueueName.isEmpty()) {
        LOG.info(""String_Node_Str"",program.getId(),schedulerQueueName);
        twillPreparer.setSchedulerQueue(schedulerQueueName);
      }
      if (logbackURI != null) {
        twillPreparer.withResources(logbackURI);
      }
      String logLevelConf=cConf.get(Constants.COLLECT_APP_CONTAINER_LOG_LEVEL).toUpperCase();
      if (""String_Node_Str"".equals(logLevelConf)) {
        twillPreparer.addJVMOptions(""String_Node_Str"");
      }
 else {
        LogEntry.Level logLevel=LogEntry.Level.ERROR;
        if (""String_Node_Str"".equals(logLevelConf)) {
          logLevel=LogEntry.Level.TRACE;
        }
 else {
          try {
            logLevel=LogEntry.Level.valueOf(logLevelConf.toUpperCase());
          }
 catch (          Exception e) {
            LOG.warn(""String_Node_Str"",logLevelConf);
          }
        }
        twillPreparer.addLogHandler(new ApplicationLogHandler(new PrinterLogHandler(new PrintWriter(System.out)),logLevel));
      }
      String yarnAppClassPath=hConf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
      if (User.isHBaseSecurityEnabled(hConf) || UserGroupInformation.isSecurityEnabled()) {
        twillPreparer.addSecureStore(secureStoreUpdater.update());
      }
      Iterable<Class<?>> dependencies=Iterables.concat(Collections.singletonList(HBaseTableUtilFactory.getHBaseTableUtilClass()),getKMSSecureStore(cConf),extraDependencies);
      twillPreparer.withDependencies(dependencies).withClassPaths(Iterables.concat(extraClassPaths,Splitter.on(',').trimResults().split(hConf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,""String_Node_Str"")))).withApplicationClassPaths(Splitter.on(""String_Node_Str"").trimResults().split(yarnAppClassPath)).withBundlerClassAcceptor(new HadoopClassExcluder(){
        @Override public boolean accept(        String className,        URL classUrl,        URL classPathUrl){
          return super.accept(className,classUrl,classPathUrl) && !className.startsWith(""String_Node_Str"");
        }
      }
).withApplicationArguments(""String_Node_Str"" + RunnableOptions.JAR,programJarName,""String_Node_Str"" + RunnableOptions.HADOOP_CONF_FILE,HADOOP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.CDAP_CONF_FILE,CDAP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.APP_SPEC_FILE,APP_SPEC_FILE_NAME,""String_Node_Str"" + RunnableOptions.PROGRAM_OPTIONS,programOptions,""String_Node_Str"" + RunnableOptions.PROGRAM_ID,GSON.toJson(program.getId().toEntityId()));
      File tmpDir=new File(cConf.get(Constants.CFG_LOCAL_DATA_DIR),cConf.get(Constants.AppFabric.TEMP_DIR));
      File jarCacheDir=new File(tmpDir,""String_Node_Str"");
      File programTypeDir=new File(jarCacheDir,program.getType().name().toLowerCase());
      DirUtils.mkdirs(programTypeDir);
      twillPreparer.withApplicationArguments(""String_Node_Str"" + programTypeDir.getAbsolutePath());
      jarCacheTracker.registerLaunch(programTypeDir,program.getType());
      twillPreparer.withApplicationArguments(""String_Node_Str"" + cConf.get(Constants.AppFabric.PROGRAM_MAX_START_SECONDS),""String_Node_Str"" + cConf.get(Constants.AppFabric.PROGRAM_MAX_STOP_SECONDS));
      TwillController twillController;
      ClassLoader oldClassLoader=ClassLoaders.setContextClassLoader(new CombineClassLoader(AbstractDistributedProgramRunner.this.getClass().getClassLoader(),Iterables.transform(dependencies,new Function<Class<?>,ClassLoader>(){
        @Override public ClassLoader apply(        Class<?> input){
          return input.getClassLoader();
        }
      }
)));
      try {
        twillController=twillPreparer.start();
      }
  finally {
        ClassLoaders.setContextClassLoader(oldClassLoader);
      }
      return addCleanupListener(twillController,program,tempDir);
    }
  }
);
}"
5851,"@Override protected synchronized void doShutDown(){
  if (processController == null) {
    LOG.warn(""String_Node_Str"");
    return;
  }
  stopPollStatus();
  try {
    Uninterruptibles.getUninterruptibly(getStopMessageFuture(),Constants.APPLICATION_MAX_STOP_SECONDS,TimeUnit.SECONDS);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    kill();
  }
  FinalApplicationStatus finalStatus=null;
  try {
    StopWatch stopWatch=new StopWatch();
    stopWatch.start();
    stopWatch.split();
    long maxTime=TimeUnit.MILLISECONDS.convert(Constants.APPLICATION_MAX_STOP_SECONDS,TimeUnit.SECONDS);
    YarnApplicationReport report=processController.getReport();
    finalStatus=report.getFinalApplicationStatus();
    ApplicationId appId=report.getApplicationId();
    while (finalStatus == FinalApplicationStatus.UNDEFINED && stopWatch.getSplitTime() < maxTime) {
      LOG.debug(""String_Node_Str"",appName,appId,finalStatus);
      TimeUnit.SECONDS.sleep(1);
      stopWatch.split();
      finalStatus=processController.getReport().getFinalApplicationStatus();
    }
    LOG.debug(""String_Node_Str"",appName,appId,finalStatus);
    if (finalStatus == FinalApplicationStatus.UNDEFINED) {
      kill();
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e.getMessage(),e);
    kill();
  }
  super.doShutDown();
  if (finalStatus == FinalApplicationStatus.FAILED) {
    setTerminationStatus(finalStatus);
    throw new RuntimeException(String.format(""String_Node_Str"",appName,getRunId(),finalStatus.name().toLowerCase()));
  }
}","@Override protected synchronized void doShutDown(){
  if (processController == null) {
    LOG.warn(""String_Node_Str"");
    return;
  }
  stopPollStatus();
  try {
    Uninterruptibles.getUninterruptibly(getStopMessageFuture(),maxStopSeconds,TimeUnit.SECONDS);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    kill();
  }
  FinalApplicationStatus finalStatus=null;
  try {
    StopWatch stopWatch=new StopWatch();
    stopWatch.start();
    stopWatch.split();
    long maxTime=TimeUnit.MILLISECONDS.convert(maxStopSeconds,TimeUnit.SECONDS);
    YarnApplicationReport report=processController.getReport();
    finalStatus=report.getFinalApplicationStatus();
    ApplicationId appId=report.getApplicationId();
    while (finalStatus == FinalApplicationStatus.UNDEFINED && stopWatch.getSplitTime() < maxTime) {
      LOG.debug(""String_Node_Str"",appName,appId,finalStatus);
      TimeUnit.SECONDS.sleep(1);
      stopWatch.split();
      finalStatus=processController.getReport().getFinalApplicationStatus();
    }
    LOG.debug(""String_Node_Str"",appName,appId,finalStatus);
    if (finalStatus == FinalApplicationStatus.UNDEFINED) {
      kill();
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e.getMessage(),e);
    kill();
  }
  super.doShutDown();
  if (finalStatus == FinalApplicationStatus.FAILED) {
    setTerminationStatus(finalStatus);
    throw new RuntimeException(String.format(""String_Node_Str"",appName,getRunId(),finalStatus.name().toLowerCase()));
  }
}"
5852,"@Override protected void doStartUp(){
  super.doStartUp();
  try {
    processController=startUp.call();
    YarnApplicationReport report=processController.getReport();
    ApplicationId appId=report.getApplicationId();
    LOG.debug(""String_Node_Str"",appName,appId);
    YarnApplicationState state=report.getYarnApplicationState();
    StopWatch stopWatch=new StopWatch();
    stopWatch.start();
    stopWatch.split();
    long maxTime=TimeUnit.MILLISECONDS.convert(Constants.APPLICATION_MAX_START_SECONDS,TimeUnit.SECONDS);
    LOG.debug(""String_Node_Str"",appName,appId);
    while (!hasRun(state) && stopWatch.getSplitTime() < maxTime) {
      report=processController.getReport();
      state=report.getYarnApplicationState();
      LOG.debug(""String_Node_Str"",appName,appId,state);
      TimeUnit.SECONDS.sleep(1);
      stopWatch.split();
    }
    LOG.info(""String_Node_Str"",appName,appId,state);
    if (state != YarnApplicationState.RUNNING) {
      LOG.info(""String_Node_Str"",appName,appId,Constants.APPLICATION_MAX_START_SECONDS);
      forceShutDown();
    }
 else {
      try {
        URL resourceUrl=URI.create(String.format(""String_Node_Str"",report.getHost(),report.getRpcPort())).resolve(TrackerService.PATH).toURL();
        resourcesClient=new ResourceReportClient(resourceUrl);
      }
 catch (      IOException e) {
        resourcesClient=null;
      }
    }
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","@Override protected void doStartUp(){
  super.doStartUp();
  try {
    processController=startUp.call();
    YarnApplicationReport report=processController.getReport();
    ApplicationId appId=report.getApplicationId();
    LOG.debug(""String_Node_Str"",appName,appId);
    YarnApplicationState state=report.getYarnApplicationState();
    StopWatch stopWatch=new StopWatch();
    stopWatch.start();
    stopWatch.split();
    long maxTime=TimeUnit.MILLISECONDS.convert(maxStartSeconds,TimeUnit.SECONDS);
    LOG.debug(""String_Node_Str"",appName,appId);
    while (!hasRun(state) && stopWatch.getSplitTime() < maxTime) {
      report=processController.getReport();
      state=report.getYarnApplicationState();
      LOG.debug(""String_Node_Str"",appName,appId,state);
      TimeUnit.SECONDS.sleep(1);
      stopWatch.split();
    }
    LOG.info(""String_Node_Str"",appName,appId,state);
    if (state != YarnApplicationState.RUNNING) {
      LOG.info(""String_Node_Str"",appName,appId,Constants.APPLICATION_MAX_START_SECONDS);
      forceShutDown();
    }
 else {
      try {
        URL resourceUrl=URI.create(String.format(""String_Node_Str"",report.getHost(),report.getRpcPort())).resolve(TrackerService.PATH).toURL();
        resourcesClient=new ResourceReportClient(resourceUrl);
      }
 catch (      IOException e) {
        resourcesClient=null;
      }
    }
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
5853,"@Test public void test() throws Exception {
  authorizationBootstrapper.run();
  final Principal systemUser=new Principal(UserGroupInformation.getCurrentUser().getShortUserName(),Principal.PrincipalType.USER);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      Predicate<EntityId> systemUserFilter=authorizationEnforcementService.createFilter(systemUser);
      return systemUserFilter.apply(instanceId) && systemUserFilter.apply(NamespaceId.SYSTEM);
    }
  }
,10,TimeUnit.SECONDS);
  txManager.startAndWait();
  datasetService.startAndWait();
  waitForService(Constants.Service.DATASET_MANAGER);
  defaultNamespaceEnsurer.startAndWait();
  systemArtifactLoader.startAndWait();
  waitForService(defaultNamespaceEnsurer);
  waitForService(systemArtifactLoader);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      try {
        return namespaceQueryAdmin.exists(NamespaceId.DEFAULT.toId());
      }
 catch (      Exception e) {
        return false;
      }
    }
  }
,10,TimeUnit.SECONDS);
  Assert.assertTrue(defaultNamespaceEnsurer.isRunning());
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      try {
        artifactRepository.getArtifact(SYSTEM_ARTIFACT.toId());
        return true;
      }
 catch (      Exception e) {
        return false;
      }
    }
  }
,20,TimeUnit.SECONDS);
  Assert.assertTrue(systemArtifactLoader.isRunning());
  Dataset systemDataset=DatasetsUtil.getOrCreateDataset(dsFramework,NamespaceId.SYSTEM.dataset(""String_Node_Str"").toId(),Table.class.getName(),DatasetProperties.EMPTY,Collections.<String,String>emptyMap(),this.getClass().getClassLoader());
  Assert.assertNotNull(systemDataset);
  SecurityRequestContext.setUserId(""String_Node_Str"");
  namespaceAdmin.create(new NamespaceMeta.Builder().setName(""String_Node_Str"").build());
  SecurityRequestContext.setUserId(""String_Node_Str"");
  try {
    namespaceAdmin.create(new NamespaceMeta.Builder().setName(""String_Node_Str"").build());
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
}","@Test public void test() throws Exception {
  final Principal systemUser=new Principal(UserGroupInformation.getCurrentUser().getShortUserName(),Principal.PrincipalType.USER);
  Predicate<EntityId> systemUserFilter=authorizationEnforcementService.createFilter(systemUser);
  Predicate<EntityId> adminUserFilter=authorizationEnforcementService.createFilter(ADMIN_USER);
  Assert.assertFalse(systemUserFilter.apply(instanceId));
  Assert.assertFalse(systemUserFilter.apply(NamespaceId.SYSTEM));
  Assert.assertFalse(adminUserFilter.apply(NamespaceId.DEFAULT));
  authorizationBootstrapper.run();
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      Predicate<EntityId> systemUserFilter=authorizationEnforcementService.createFilter(systemUser);
      Predicate<EntityId> adminUserFilter=authorizationEnforcementService.createFilter(ADMIN_USER);
      return systemUserFilter.apply(instanceId) && systemUserFilter.apply(NamespaceId.SYSTEM) && adminUserFilter.apply(NamespaceId.DEFAULT);
    }
  }
,10,TimeUnit.SECONDS);
  txManager.startAndWait();
  datasetService.startAndWait();
  waitForService(Constants.Service.DATASET_MANAGER);
  defaultNamespaceEnsurer.startAndWait();
  systemArtifactLoader.startAndWait();
  waitForService(defaultNamespaceEnsurer);
  waitForService(systemArtifactLoader);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      try {
        return namespaceQueryAdmin.exists(NamespaceId.DEFAULT.toId());
      }
 catch (      Exception e) {
        return false;
      }
    }
  }
,10,TimeUnit.SECONDS);
  Assert.assertTrue(defaultNamespaceEnsurer.isRunning());
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      try {
        artifactRepository.getArtifact(SYSTEM_ARTIFACT.toId());
        return true;
      }
 catch (      Exception e) {
        return false;
      }
    }
  }
,20,TimeUnit.SECONDS);
  Assert.assertTrue(systemArtifactLoader.isRunning());
  Dataset systemDataset=DatasetsUtil.getOrCreateDataset(dsFramework,NamespaceId.SYSTEM.dataset(""String_Node_Str"").toId(),Table.class.getName(),DatasetProperties.EMPTY,Collections.<String,String>emptyMap(),this.getClass().getClassLoader());
  Assert.assertNotNull(systemDataset);
  SecurityRequestContext.setUserId(ADMIN_USER.getName());
  namespaceAdmin.create(new NamespaceMeta.Builder().setName(""String_Node_Str"").build());
  SecurityRequestContext.setUserId(""String_Node_Str"");
  try {
    namespaceAdmin.create(new NamespaceMeta.Builder().setName(""String_Node_Str"").build());
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
}"
5854,"@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  Location deploymentJar=AppJarHelper.createDeploymentJar(new LocalLocationFactory(TMP_FOLDER.newFolder()),InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,deploymentJar.toURI().getPath());
  cConf.set(Constants.Security.Authorization.ADMIN_USERS,""String_Node_Str"");
  instanceId=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
  File systemArtifactsDir=TMP_FOLDER.newFolder();
  cConf.set(Constants.AppFabric.SYSTEM_ARTIFACTS_DIR,systemArtifactsDir.getAbsolutePath());
  createSystemArtifact(systemArtifactsDir);
  Injector injector=Guice.createInjector(new AppFabricTestModule(cConf));
  defaultNamespaceEnsurer=injector.getInstance(DefaultNamespaceEnsurer.class);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  txManager=injector.getInstance(TransactionManager.class);
  datasetService=injector.getInstance(DatasetService.class);
  authorizationEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  authorizationEnforcementService.startAndWait();
  systemArtifactLoader=injector.getInstance(SystemArtifactLoader.class);
  authorizationBootstrapper=injector.getInstance(AuthorizationBootstrapper.class);
  namespaceQueryAdmin=injector.getInstance(NamespaceQueryAdmin.class);
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  artifactRepository=injector.getInstance(ArtifactRepository.class);
  dsFramework=injector.getInstance(DatasetFramework.class);
}","@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  Location deploymentJar=AppJarHelper.createDeploymentJar(new LocalLocationFactory(TMP_FOLDER.newFolder()),InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,deploymentJar.toURI().getPath());
  cConf.set(Constants.Security.Authorization.ADMIN_USERS,ADMIN_USER.getName());
  instanceId=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
  File systemArtifactsDir=TMP_FOLDER.newFolder();
  cConf.set(Constants.AppFabric.SYSTEM_ARTIFACTS_DIR,systemArtifactsDir.getAbsolutePath());
  createSystemArtifact(systemArtifactsDir);
  Injector injector=Guice.createInjector(new AppFabricTestModule(cConf));
  defaultNamespaceEnsurer=injector.getInstance(DefaultNamespaceEnsurer.class);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  txManager=injector.getInstance(TransactionManager.class);
  datasetService=injector.getInstance(DatasetService.class);
  authorizationEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  authorizationEnforcementService.startAndWait();
  systemArtifactLoader=injector.getInstance(SystemArtifactLoader.class);
  authorizationBootstrapper=injector.getInstance(AuthorizationBootstrapper.class);
  namespaceQueryAdmin=injector.getInstance(NamespaceQueryAdmin.class);
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  artifactRepository=injector.getInstance(ArtifactRepository.class);
  dsFramework=injector.getInstance(DatasetFramework.class);
}"
5855,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(instanceId,principal,Action.ADMIN);
  privilegesManager.grant(namespace,principal,EnumSet.allOf(Action.class));
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    ImpersonationInfo impersonationInfo=new ImpersonationInfo(metadata,cConf);
    String namespacePrincipal=impersonationInfo.getPrincipal();
    String namespaceUserName=new KerberosName(namespacePrincipal).getShortName();
    Principal namespaceUser=new Principal(namespaceUserName,Principal.PrincipalType.USER);
    privilegesManager.grant(namespace,namespaceUser,EnumSet.allOf(Action.class));
  }
  nsStore.create(metadata);
  try {
    impersonator.doAs(metadata,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.create(metadata);
        return null;
      }
    }
);
  }
 catch (  IOException|ExploreException|SQLException e) {
    nsStore.delete(metadata.getNamespaceId().toId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace.toId(),e);
  }
}","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(instanceId,principal,Action.ADMIN);
  privilegesManager.grant(namespace,principal,EnumSet.allOf(Action.class));
  String executionUserName;
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    ImpersonationInfo impersonationInfo=new ImpersonationInfo(metadata,cConf);
    String namespacePrincipal=impersonationInfo.getPrincipal();
    executionUserName=new KerberosName(namespacePrincipal).getShortName();
  }
 else {
    executionUserName=UserGroupInformation.getCurrentUser().getShortUserName();
  }
  Principal executionUser=new Principal(executionUserName,Principal.PrincipalType.USER);
  privilegesManager.grant(namespace,executionUser,EnumSet.allOf(Action.class));
  nsStore.create(metadata);
  try {
    impersonator.doAs(metadata,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.create(metadata);
        return null;
      }
    }
);
  }
 catch (  IOException|ExploreException|SQLException e) {
    nsStore.delete(metadata.getNamespaceId().toId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace.toId(),e);
  }
}"
5856,"/** 
 * Return the original properties of a dataset instance, that is, the properties with which the dataset was created or last reconfigured.
 * @param instance the id of the dataset
 * @return The original properties as stored in the dataset's spec, or if they are not available, a best effortto derive the original properties from the top-level properties of the spec
 * @throws UnauthorizedException if permimeter security and authorization are enabled, and the current user does nothave any privileges on the #instance
 */
Map<String,String> getOriginalProperties(Id.DatasetInstance instance) throws Exception {
  DatasetSpecification spec=instanceManager.get(instance);
  if (spec == null) {
    throw new NotFoundException(instance);
  }
  ensureAccess(instance.toEntityId());
  return DatasetsUtil.fixOriginalProperties(spec).getOriginalProperties();
}","/** 
 * Return the original properties of a dataset instance, that is, the properties with which the dataset was created or last reconfigured.
 * @param instance the id of the dataset
 * @return The original properties as stored in the dataset's spec, or if they are not available, a best effortto derive the original properties from the top-level properties of the spec
 * @throws UnauthorizedException if perimeter security and authorization are enabled, and the current user does nothave any privileges on the #instance
 */
Map<String,String> getOriginalProperties(Id.DatasetInstance instance) throws Exception {
  DatasetSpecification spec=instanceManager.get(instance);
  if (spec == null) {
    throw new NotFoundException(instance);
  }
  ensureAccess(instance.toEntityId());
  return DatasetsUtil.fixOriginalProperties(spec).getOriginalProperties();
}"
5857,"@POST @Path(""String_Node_Str"") public void drop(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String instanceName) throws Exception {
  InternalDatasetDropParams params=GSON.fromJson(request.getContent().toString(Charsets.UTF_8),InternalDatasetDropParams.class);
  Preconditions.checkArgument(params.getInstanceSpec() != null,""String_Node_Str"");
  Preconditions.checkArgument(params.getTypeMeta() != null,""String_Node_Str"");
  DatasetSpecification spec=params.getInstanceSpec();
  DatasetTypeMeta typeMeta=params.getTypeMeta();
  try {
    datasetAdminService.drop(Id.DatasetInstance.from(namespaceId,instanceName),typeMeta,spec);
    responder.sendJson(HttpResponseStatus.OK,spec);
  }
 catch (  BadRequestException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
}","@POST @Path(""String_Node_Str"") public void drop(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String instanceName) throws Exception {
  propagateUserId(request);
  InternalDatasetDropParams params=GSON.fromJson(request.getContent().toString(Charsets.UTF_8),InternalDatasetDropParams.class);
  Preconditions.checkArgument(params.getInstanceSpec() != null,""String_Node_Str"");
  Preconditions.checkArgument(params.getTypeMeta() != null,""String_Node_Str"");
  DatasetSpecification spec=params.getInstanceSpec();
  DatasetTypeMeta typeMeta=params.getTypeMeta();
  try {
    datasetAdminService.drop(Id.DatasetInstance.from(namespaceId,instanceName),typeMeta,spec);
    responder.sendJson(HttpResponseStatus.OK,spec);
  }
 catch (  BadRequestException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
}"
5858,"@Inject public DatasetAdminOpHTTPHandler(DatasetAdminService datasetAdminService){
  this.datasetAdminService=datasetAdminService;
}","@Inject @VisibleForTesting public DatasetAdminOpHTTPHandler(DatasetAdminService datasetAdminService){
  this.datasetAdminService=datasetAdminService;
}"
5859,"@POST @Path(""String_Node_Str"") public void truncate(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String instanceName){
  try {
    Id.DatasetInstance instanceId=Id.DatasetInstance.from(namespaceId,instanceName);
    datasetAdminService.truncate(instanceId);
    responder.sendJson(HttpResponseStatus.OK,new DatasetAdminOpResponse(null,null));
  }
 catch (  NotFoundException e) {
    LOG.debug(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.NOT_FOUND,StringUtils.defaultIfEmpty(e.getMessage(),""String_Node_Str""));
  }
catch (  Exception e) {
    LOG.error(getAdminOpErrorMessage(""String_Node_Str"",instanceName),e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,getAdminOpErrorMessage(""String_Node_Str"",instanceName));
  }
}","@POST @Path(""String_Node_Str"") public void truncate(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String instanceName){
  propagateUserId(request);
  try {
    Id.DatasetInstance instanceId=Id.DatasetInstance.from(namespaceId,instanceName);
    datasetAdminService.truncate(instanceId);
    responder.sendJson(HttpResponseStatus.OK,new DatasetAdminOpResponse(null,null));
  }
 catch (  NotFoundException e) {
    LOG.debug(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.NOT_FOUND,StringUtils.defaultIfEmpty(e.getMessage(),""String_Node_Str""));
  }
catch (  Exception e) {
    LOG.error(getAdminOpErrorMessage(""String_Node_Str"",instanceName),e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,getAdminOpErrorMessage(""String_Node_Str"",instanceName));
  }
}"
5860,"@POST @Path(""String_Node_Str"") public void upgrade(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String instanceName){
  try {
    Id.DatasetInstance instanceId=Id.DatasetInstance.from(namespaceId,instanceName);
    datasetAdminService.upgrade(instanceId);
    responder.sendJson(HttpResponseStatus.OK,new DatasetAdminOpResponse(null,null));
  }
 catch (  NotFoundException e) {
    LOG.debug(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.NOT_FOUND,StringUtils.defaultIfEmpty(e.getMessage(),""String_Node_Str""));
  }
catch (  Exception e) {
    LOG.error(getAdminOpErrorMessage(""String_Node_Str"",instanceName),e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,getAdminOpErrorMessage(""String_Node_Str"",instanceName));
  }
}","@POST @Path(""String_Node_Str"") public void upgrade(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String instanceName){
  propagateUserId(request);
  try {
    Id.DatasetInstance instanceId=Id.DatasetInstance.from(namespaceId,instanceName);
    datasetAdminService.upgrade(instanceId);
    responder.sendJson(HttpResponseStatus.OK,new DatasetAdminOpResponse(null,null));
  }
 catch (  NotFoundException e) {
    LOG.debug(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.NOT_FOUND,StringUtils.defaultIfEmpty(e.getMessage(),""String_Node_Str""));
  }
catch (  Exception e) {
    LOG.error(getAdminOpErrorMessage(""String_Node_Str"",instanceName),e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,getAdminOpErrorMessage(""String_Node_Str"",instanceName));
  }
}"
5861,"@POST @Path(""String_Node_Str"") public void exists(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String instanceName){
  Id.Namespace namespace=Id.Namespace.from(namespaceId);
  try {
    Id.DatasetInstance instanceId=Id.DatasetInstance.from(namespace,instanceName);
    responder.sendJson(HttpResponseStatus.OK,new DatasetAdminOpResponse(datasetAdminService.exists(instanceId),null));
  }
 catch (  NotFoundException e) {
    LOG.debug(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.NOT_FOUND,StringUtils.defaultIfEmpty(e.getMessage(),""String_Node_Str""));
  }
catch (  Exception e) {
    LOG.error(getAdminOpErrorMessage(""String_Node_Str"",instanceName),e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,getAdminOpErrorMessage(""String_Node_Str"",instanceName));
  }
}","@POST @Path(""String_Node_Str"") public void exists(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String instanceName){
  propagateUserId(request);
  Id.Namespace namespace=Id.Namespace.from(namespaceId);
  try {
    Id.DatasetInstance instanceId=Id.DatasetInstance.from(namespace,instanceName);
    responder.sendJson(HttpResponseStatus.OK,new DatasetAdminOpResponse(datasetAdminService.exists(instanceId),null));
  }
 catch (  NotFoundException e) {
    LOG.debug(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.NOT_FOUND,StringUtils.defaultIfEmpty(e.getMessage(),""String_Node_Str""));
  }
catch (  Exception e) {
    LOG.error(getAdminOpErrorMessage(""String_Node_Str"",instanceName),e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,getAdminOpErrorMessage(""String_Node_Str"",instanceName));
  }
}"
5862,"@POST @Path(""String_Node_Str"") public void create(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String name){
  InternalDatasetCreationParams params=GSON.fromJson(request.getContent().toString(Charsets.UTF_8),InternalDatasetCreationParams.class);
  Preconditions.checkArgument(params.getProperties() != null,""String_Node_Str"");
  Preconditions.checkArgument(params.getTypeMeta() != null,""String_Node_Str"");
  DatasetProperties props=params.getProperties();
  DatasetTypeMeta typeMeta=params.getTypeMeta();
  try {
    Id.DatasetInstance instanceId=Id.DatasetInstance.from(namespaceId,name);
    DatasetSpecification spec=datasetAdminService.createOrUpdate(instanceId,typeMeta,props,null);
    responder.sendJson(HttpResponseStatus.OK,spec);
  }
 catch (  BadRequestException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
catch (  Exception e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}","@POST @Path(""String_Node_Str"") public void create(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String name){
  propagateUserId(request);
  InternalDatasetCreationParams params=GSON.fromJson(request.getContent().toString(Charsets.UTF_8),InternalDatasetCreationParams.class);
  Preconditions.checkArgument(params.getProperties() != null,""String_Node_Str"");
  Preconditions.checkArgument(params.getTypeMeta() != null,""String_Node_Str"");
  DatasetProperties props=params.getProperties();
  DatasetTypeMeta typeMeta=params.getTypeMeta();
  try {
    Id.DatasetInstance instanceId=Id.DatasetInstance.from(namespaceId,name);
    DatasetSpecification spec=datasetAdminService.createOrUpdate(instanceId,typeMeta,props,null);
    responder.sendJson(HttpResponseStatus.OK,spec);
  }
 catch (  BadRequestException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
catch (  Exception e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}"
5863,"@POST @Path(""String_Node_Str"") public void update(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String name){
  InternalDatasetUpdateParams params=GSON.fromJson(request.getContent().toString(Charsets.UTF_8),InternalDatasetUpdateParams.class);
  Preconditions.checkArgument(params.getProperties() != null,""String_Node_Str"");
  Preconditions.checkArgument(params.getTypeMeta() != null,""String_Node_Str"");
  Preconditions.checkArgument(params.getExistingSpec() != null,""String_Node_Str"");
  DatasetProperties props=params.getProperties();
  DatasetSpecification existing=params.getExistingSpec();
  DatasetTypeMeta typeMeta=params.getTypeMeta();
  try {
    Id.DatasetInstance instanceId=Id.DatasetInstance.from(namespaceId,name);
    DatasetSpecification spec=datasetAdminService.createOrUpdate(instanceId,typeMeta,props,existing);
    responder.sendJson(HttpResponseStatus.OK,spec);
  }
 catch (  NotFoundException e) {
    LOG.debug(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.NOT_FOUND,StringUtils.defaultIfEmpty(e.getMessage(),""String_Node_Str""));
  }
catch (  BadRequestException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
catch (  IncompatibleUpdateException e) {
    responder.sendString(HttpResponseStatus.CONFLICT,e.getMessage());
  }
catch (  Exception e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}","@POST @Path(""String_Node_Str"") public void update(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String name){
  propagateUserId(request);
  InternalDatasetUpdateParams params=GSON.fromJson(request.getContent().toString(Charsets.UTF_8),InternalDatasetUpdateParams.class);
  Preconditions.checkArgument(params.getProperties() != null,""String_Node_Str"");
  Preconditions.checkArgument(params.getTypeMeta() != null,""String_Node_Str"");
  Preconditions.checkArgument(params.getExistingSpec() != null,""String_Node_Str"");
  DatasetProperties props=params.getProperties();
  DatasetSpecification existing=params.getExistingSpec();
  DatasetTypeMeta typeMeta=params.getTypeMeta();
  try {
    Id.DatasetInstance instanceId=Id.DatasetInstance.from(namespaceId,name);
    DatasetSpecification spec=datasetAdminService.createOrUpdate(instanceId,typeMeta,props,existing);
    responder.sendJson(HttpResponseStatus.OK,spec);
  }
 catch (  NotFoundException e) {
    LOG.debug(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.NOT_FOUND,StringUtils.defaultIfEmpty(e.getMessage(),""String_Node_Str""));
  }
catch (  BadRequestException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
  }
catch (  IncompatibleUpdateException e) {
    responder.sendString(HttpResponseStatus.CONFLICT,e.getMessage());
  }
catch (  Exception e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}"
5864,"@Inject public LocalDatasetOpExecutor(CConfiguration cConf,DiscoveryServiceClient discoveryClient,DatasetOpExecutorService executorServer){
  super(cConf,discoveryClient);
  this.executorServer=executorServer;
}","@Inject @VisibleForTesting public LocalDatasetOpExecutor(CConfiguration cConf,DiscoveryServiceClient discoveryClient,DatasetOpExecutorService executorServer,AuthenticationContext authenticationContext){
  super(cConf,discoveryClient,authenticationContext);
  this.executorServer=executorServer;
}"
5865,"@Override public void drop(Id.DatasetInstance datasetInstanceId,DatasetTypeMeta typeMeta,DatasetSpecification spec) throws Exception {
  InternalDatasetDropParams dropParams=new InternalDatasetDropParams(typeMeta,spec);
  HttpRequest request=HttpRequest.post(resolve(datasetInstanceId,""String_Node_Str"")).withBody(GSON.toJson(dropParams)).build();
  HttpResponse response=HttpRequests.execute(request,httpRequestConfig);
  verifyResponse(response);
}","@Override public void drop(Id.DatasetInstance datasetInstanceId,DatasetTypeMeta typeMeta,DatasetSpecification spec) throws Exception {
  InternalDatasetDropParams dropParams=new InternalDatasetDropParams(typeMeta,spec);
  doRequest(datasetInstanceId,""String_Node_Str"",GSON.toJson(dropParams));
}"
5866,"@Override public void upgrade(Id.DatasetInstance datasetInstanceId) throws Exception {
  executeAdminOp(datasetInstanceId,""String_Node_Str"");
}","@Override public void upgrade(Id.DatasetInstance datasetInstanceId) throws Exception {
  executeAdminOp(datasetInstanceId,""String_Node_Str"",null);
}"
5867,"@Override public DatasetSpecification update(Id.DatasetInstance datasetInstanceId,DatasetTypeMeta typeMeta,DatasetProperties props,DatasetSpecification existing) throws Exception {
  InternalDatasetCreationParams updateParams=new InternalDatasetUpdateParams(typeMeta,existing,props);
  HttpRequest request=HttpRequest.post(resolve(datasetInstanceId,""String_Node_Str"")).withBody(GSON.toJson(updateParams)).build();
  HttpResponse response=HttpRequests.execute(request,httpRequestConfig);
  verifyResponse(response);
  return ObjectResponse.fromJsonBody(response,DatasetSpecification.class).getResponseObject();
}","@Override public DatasetSpecification update(Id.DatasetInstance datasetInstanceId,DatasetTypeMeta typeMeta,DatasetProperties props,DatasetSpecification existing) throws Exception {
  InternalDatasetCreationParams updateParams=new InternalDatasetUpdateParams(typeMeta,existing,props);
  HttpResponse response=doRequest(datasetInstanceId,""String_Node_Str"",GSON.toJson(updateParams));
  return ObjectResponse.fromJsonBody(response,DatasetSpecification.class).getResponseObject();
}"
5868,"@Override public void truncate(Id.DatasetInstance datasetInstanceId) throws Exception {
  executeAdminOp(datasetInstanceId,""String_Node_Str"");
}","@Override public void truncate(Id.DatasetInstance datasetInstanceId) throws Exception {
  executeAdminOp(datasetInstanceId,""String_Node_Str"",null);
}"
5869,"@Override public boolean exists(Id.DatasetInstance datasetInstanceId) throws Exception {
  return (Boolean)executeAdminOp(datasetInstanceId,""String_Node_Str"").getResult();
}","@Override public boolean exists(Id.DatasetInstance datasetInstanceId) throws Exception {
  return (Boolean)executeAdminOp(datasetInstanceId,""String_Node_Str"",null).getResult();
}"
5870,"@Override public DatasetSpecification create(Id.DatasetInstance datasetInstanceId,DatasetTypeMeta typeMeta,DatasetProperties props) throws Exception {
  InternalDatasetCreationParams creationParams=new InternalDatasetCreationParams(typeMeta,props);
  HttpRequest request=HttpRequest.post(resolve(datasetInstanceId,""String_Node_Str"")).withBody(GSON.toJson(creationParams)).build();
  HttpResponse response=HttpRequests.execute(request,httpRequestConfig);
  verifyResponse(response);
  return ObjectResponse.fromJsonBody(response,DatasetSpecification.class).getResponseObject();
}","@Override public DatasetSpecification create(Id.DatasetInstance datasetInstanceId,DatasetTypeMeta typeMeta,DatasetProperties props) throws Exception {
  InternalDatasetCreationParams creationParams=new InternalDatasetCreationParams(typeMeta,props);
  HttpResponse response=doRequest(datasetInstanceId,""String_Node_Str"",GSON.toJson(creationParams));
  return ObjectResponse.fromJsonBody(response,DatasetSpecification.class).getResponseObject();
}"
5871,"@Inject public RemoteDatasetOpExecutor(CConfiguration cConf,final DiscoveryServiceClient discoveryClient){
  this.cConf=cConf;
  this.endpointStrategySupplier=Suppliers.memoize(new Supplier<EndpointStrategy>(){
    @Override public EndpointStrategy get(){
      return new RandomEndpointStrategy(discoveryClient.discover(Constants.Service.DATASET_EXECUTOR));
    }
  }
);
  this.httpRequestConfig=new DefaultHttpRequestConfig();
}","@Inject RemoteDatasetOpExecutor(CConfiguration cConf,final DiscoveryServiceClient discoveryClient,AuthenticationContext authenticationContext){
  this.cConf=cConf;
  this.authenticationContext=authenticationContext;
  this.endpointStrategySupplier=Suppliers.memoize(new Supplier<EndpointStrategy>(){
    @Override public EndpointStrategy get(){
      return new RandomEndpointStrategy(discoveryClient.discover(Constants.Service.DATASET_EXECUTOR));
    }
  }
);
  this.httpRequestConfig=new DefaultHttpRequestConfig();
}"
5872,"private DatasetAdminOpResponse executeAdminOp(Id.DatasetInstance datasetInstanceId,String opName) throws IOException, HandlerException, ConflictException {
  HttpResponse httpResponse=HttpRequests.execute(HttpRequest.post(resolve(datasetInstanceId,opName)).build(),httpRequestConfig);
  verifyResponse(httpResponse);
  return GSON.fromJson(new String(httpResponse.getResponseBody()),DatasetAdminOpResponse.class);
}","private DatasetAdminOpResponse executeAdminOp(Id.DatasetInstance datasetInstanceId,String opName,@Nullable String body) throws IOException, HandlerException, ConflictException {
  HttpResponse httpResponse=doRequest(datasetInstanceId,opName,body);
  return GSON.fromJson(Bytes.toString(httpResponse.getResponseBody()),DatasetAdminOpResponse.class);
}"
5873,"@Inject public YarnDatasetOpExecutor(CConfiguration cConf,DiscoveryServiceClient discoveryClient){
  super(cConf,discoveryClient);
}","@Inject YarnDatasetOpExecutor(CConfiguration cConf,DiscoveryServiceClient discoveryClient,AuthenticationContext authenticationContext){
  super(cConf,discoveryClient,authenticationContext);
}"
5874,"@Before public void before() throws Exception {
  cConf.set(Constants.Dataset.Manager.ADDRESS,""String_Node_Str"");
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  Configuration txConf=HBaseConfiguration.create();
  CConfigurationUtil.copyTxProperties(cConf,txConf);
  Impersonator impersonator=new DefaultImpersonator(cConf,null,null);
  Injector injector=Guice.createInjector(new ConfigModule(cConf,txConf),new DiscoveryRuntimeModule().getInMemoryModules(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getMasterModule(),new TransactionInMemoryModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Singleton.class);
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(RemoteDatasetFramework.class);
    }
  }
);
  txManager=new TransactionManager(txConf);
  txManager.startAndWait();
  InMemoryTxSystemClient txSystemClient=new InMemoryTxSystemClient(txManager);
  TransactionSystemClientService txSystemClientService=new DelegatingTransactionSystemClientService(txSystemClient);
  DiscoveryService discoveryService=injector.getInstance(DiscoveryService.class);
  DiscoveryServiceClient discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  MetricsCollectionService metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  AuthenticationContext authenticationContext=injector.getInstance(AuthenticationContext.class);
  framework=new RemoteDatasetFramework(cConf,discoveryServiceClient,registryFactory,authenticationContext);
  SystemDatasetInstantiatorFactory datasetInstantiatorFactory=new SystemDatasetInstantiatorFactory(locationFactory,framework,cConf);
  DatasetAdminService datasetAdminService=new DatasetAdminService(framework,cConf,locationFactory,datasetInstantiatorFactory,new NoOpMetadataStore(),impersonator);
  ImmutableSet<HttpHandler> handlers=ImmutableSet.<HttpHandler>of(new DatasetAdminOpHTTPHandler(datasetAdminService));
  opExecutorService=new DatasetOpExecutorService(cConf,discoveryService,metricsCollectionService,handlers);
  opExecutorService.startAndWait();
  ImmutableMap<String,DatasetModule> modules=ImmutableMap.<String,DatasetModule>builder().put(""String_Node_Str"",new InMemoryTableModule()).put(""String_Node_Str"",new CoreDatasetsModule()).putAll(DatasetMetaTableUtil.getModules()).build();
  InMemoryDatasetFramework mdsFramework=new InMemoryDatasetFramework(registryFactory,modules);
  ExploreFacade exploreFacade=new ExploreFacade(new DiscoveryExploreClient(cConf,discoveryServiceClient),cConf);
  TransactionExecutorFactory txExecutorFactory=new DynamicTransactionExecutorFactory(txSystemClient);
  AuthorizationEnforcer authorizationEnforcer=injector.getInstance(AuthorizationEnforcer.class);
  DatasetTypeManager typeManager=new DatasetTypeManager(cConf,locationFactory,txSystemClientService,txExecutorFactory,mdsFramework,impersonator);
  DatasetInstanceManager instanceManager=new DatasetInstanceManager(txSystemClientService,txExecutorFactory,mdsFramework);
  PrivilegesManager privilegesManager=injector.getInstance(PrivilegesManager.class);
  DatasetTypeService typeService=new DatasetTypeService(typeManager,namespaceQueryAdmin,namespacedLocationFactory,authorizationEnforcer,privilegesManager,authenticationContext,cConf,impersonator,txSystemClientService,mdsFramework,txExecutorFactory,DEFAULT_MODULES);
  DatasetOpExecutor opExecutor=new LocalDatasetOpExecutor(cConf,discoveryServiceClient,opExecutorService);
  DatasetInstanceService instanceService=new DatasetInstanceService(typeService,instanceManager,opExecutor,exploreFacade,namespaceQueryAdmin,authorizationEnforcer,privilegesManager,authenticationContext);
  instanceService.setAuditPublisher(inMemoryAuditPublisher);
  service=new DatasetService(cConf,discoveryService,discoveryServiceClient,metricsCollectionService,new InMemoryDatasetOpExecutor(framework),new HashSet<DatasetMetricsReporter>(),typeService,instanceService);
  service.startAndWait();
  EndpointStrategy endpointStrategy=new RandomEndpointStrategy(discoveryServiceClient.discover(Constants.Service.DATASET_MANAGER));
  Preconditions.checkNotNull(endpointStrategy.pick(5,TimeUnit.SECONDS),""String_Node_Str"",service);
  createNamespace(Id.Namespace.SYSTEM);
  createNamespace(NAMESPACE_ID);
}","@Before public void before() throws Exception {
  cConf.set(Constants.Dataset.Manager.ADDRESS,""String_Node_Str"");
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  Configuration txConf=HBaseConfiguration.create();
  CConfigurationUtil.copyTxProperties(cConf,txConf);
  Impersonator impersonator=new DefaultImpersonator(cConf,null,null);
  Injector injector=Guice.createInjector(new ConfigModule(cConf,txConf),new DiscoveryRuntimeModule().getInMemoryModules(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getMasterModule(),new TransactionInMemoryModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Singleton.class);
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(RemoteDatasetFramework.class);
    }
  }
);
  txManager=new TransactionManager(txConf);
  txManager.startAndWait();
  InMemoryTxSystemClient txSystemClient=new InMemoryTxSystemClient(txManager);
  TransactionSystemClientService txSystemClientService=new DelegatingTransactionSystemClientService(txSystemClient);
  DiscoveryService discoveryService=injector.getInstance(DiscoveryService.class);
  DiscoveryServiceClient discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  MetricsCollectionService metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  AuthenticationContext authenticationContext=injector.getInstance(AuthenticationContext.class);
  framework=new RemoteDatasetFramework(cConf,discoveryServiceClient,registryFactory,authenticationContext);
  SystemDatasetInstantiatorFactory datasetInstantiatorFactory=new SystemDatasetInstantiatorFactory(locationFactory,framework,cConf);
  DatasetAdminService datasetAdminService=new DatasetAdminService(framework,cConf,locationFactory,datasetInstantiatorFactory,new NoOpMetadataStore(),impersonator);
  ImmutableSet<HttpHandler> handlers=ImmutableSet.<HttpHandler>of(new DatasetAdminOpHTTPHandler(datasetAdminService));
  opExecutorService=new DatasetOpExecutorService(cConf,discoveryService,metricsCollectionService,handlers);
  opExecutorService.startAndWait();
  ImmutableMap<String,DatasetModule> modules=ImmutableMap.<String,DatasetModule>builder().put(""String_Node_Str"",new InMemoryTableModule()).put(""String_Node_Str"",new CoreDatasetsModule()).putAll(DatasetMetaTableUtil.getModules()).build();
  InMemoryDatasetFramework mdsFramework=new InMemoryDatasetFramework(registryFactory,modules);
  DiscoveryExploreClient exploreClient=new DiscoveryExploreClient(discoveryServiceClient,authenticationContext);
  ExploreFacade exploreFacade=new ExploreFacade(exploreClient,cConf);
  TransactionExecutorFactory txExecutorFactory=new DynamicTransactionExecutorFactory(txSystemClient);
  AuthorizationEnforcer authorizationEnforcer=injector.getInstance(AuthorizationEnforcer.class);
  DatasetTypeManager typeManager=new DatasetTypeManager(cConf,locationFactory,txSystemClientService,txExecutorFactory,mdsFramework,impersonator);
  DatasetInstanceManager instanceManager=new DatasetInstanceManager(txSystemClientService,txExecutorFactory,mdsFramework);
  PrivilegesManager privilegesManager=injector.getInstance(PrivilegesManager.class);
  DatasetTypeService typeService=new DatasetTypeService(typeManager,namespaceQueryAdmin,namespacedLocationFactory,authorizationEnforcer,privilegesManager,authenticationContext,cConf,impersonator,txSystemClientService,mdsFramework,txExecutorFactory,DEFAULT_MODULES);
  DatasetOpExecutor opExecutor=new LocalDatasetOpExecutor(cConf,discoveryServiceClient,opExecutorService,authenticationContext);
  DatasetInstanceService instanceService=new DatasetInstanceService(typeService,instanceManager,opExecutor,exploreFacade,namespaceQueryAdmin,authorizationEnforcer,privilegesManager,authenticationContext);
  instanceService.setAuditPublisher(inMemoryAuditPublisher);
  service=new DatasetService(cConf,discoveryService,discoveryServiceClient,metricsCollectionService,new InMemoryDatasetOpExecutor(framework),new HashSet<DatasetMetricsReporter>(),typeService,instanceService);
  service.startAndWait();
  EndpointStrategy endpointStrategy=new RandomEndpointStrategy(discoveryServiceClient.discover(Constants.Service.DATASET_MANAGER));
  Preconditions.checkNotNull(endpointStrategy.pick(5,TimeUnit.SECONDS),""String_Node_Str"",service);
  createNamespace(Id.Namespace.SYSTEM);
  createNamespace(NAMESPACE_ID);
}"
5875,"protected static void initializeAndStartService(CConfiguration cConf) throws Exception {
  injector=Guice.createInjector(new ConfigModule(cConf),new DiscoveryRuntimeModule().getInMemoryModules(),new NonCustomLocationUnitTestModule().getModule(),new NamespaceClientRuntimeModule().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),new TransactionInMemoryModule(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getMasterModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Singleton.class);
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(RemoteDatasetFramework.class);
    }
  }
);
  AuthorizationEnforcer authEnforcer=injector.getInstance(AuthorizationEnforcer.class);
  authEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  authEnforcementService.startAndWait();
  AuthenticationContext authenticationContext=injector.getInstance(AuthenticationContext.class);
  DiscoveryService discoveryService=injector.getInstance(DiscoveryService.class);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  dsFramework=injector.getInstance(RemoteDatasetFramework.class);
  txManager=injector.getInstance(TransactionManager.class);
  txManager.startAndWait();
  TransactionSystemClient txSystemClient=injector.getInstance(TransactionSystemClient.class);
  TransactionSystemClientService txSystemClientService=new DelegatingTransactionSystemClientService(txSystemClient);
  NamespacedLocationFactory namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  SystemDatasetInstantiatorFactory datasetInstantiatorFactory=new SystemDatasetInstantiatorFactory(locationFactory,dsFramework,cConf);
  Impersonator impersonator=new DefaultImpersonator(cConf,null,null);
  DatasetAdminService datasetAdminService=new DatasetAdminService(dsFramework,cConf,locationFactory,datasetInstantiatorFactory,new NoOpMetadataStore(),impersonator);
  ImmutableSet<HttpHandler> handlers=ImmutableSet.<HttpHandler>of(new DatasetAdminOpHTTPHandler(datasetAdminService));
  MetricsCollectionService metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  opExecutorService=new DatasetOpExecutorService(cConf,discoveryService,metricsCollectionService,handlers);
  opExecutorService.startAndWait();
  Map<String,DatasetModule> defaultModules=injector.getInstance(Key.get(new TypeLiteral<Map<String,DatasetModule>>(){
  }
,Names.named(""String_Node_Str"")));
  ImmutableMap<String,DatasetModule> modules=ImmutableMap.<String,DatasetModule>builder().putAll(defaultModules).putAll(DatasetMetaTableUtil.getModules()).build();
  registryFactory=injector.getInstance(DatasetDefinitionRegistryFactory.class);
  inMemoryDatasetFramework=new InMemoryDatasetFramework(registryFactory,modules);
  ExploreFacade exploreFacade=new ExploreFacade(new DiscoveryExploreClient(cConf,discoveryServiceClient),cConf);
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  namespaceAdmin.create(NamespaceMeta.DEFAULT);
  NamespaceQueryAdmin namespaceQueryAdmin=injector.getInstance(NamespaceQueryAdmin.class);
  TransactionExecutorFactory txExecutorFactory=new DynamicTransactionExecutorFactory(txSystemClient);
  DatasetTypeManager typeManager=new DatasetTypeManager(cConf,locationFactory,txSystemClientService,txExecutorFactory,inMemoryDatasetFramework,impersonator);
  DatasetOpExecutor opExecutor=new InMemoryDatasetOpExecutor(dsFramework);
  DatasetInstanceManager instanceManager=new DatasetInstanceManager(txSystemClientService,txExecutorFactory,inMemoryDatasetFramework);
  PrivilegesManager privilegesManager=injector.getInstance(PrivilegesManager.class);
  DatasetTypeService typeService=new DatasetTypeService(typeManager,namespaceAdmin,namespacedLocationFactory,authEnforcer,privilegesManager,authenticationContext,cConf,impersonator,txSystemClientService,inMemoryDatasetFramework,txExecutorFactory,defaultModules);
  instanceService=new DatasetInstanceService(typeService,instanceManager,opExecutor,exploreFacade,namespaceQueryAdmin,authEnforcer,privilegesManager,authenticationContext);
  service=new DatasetService(cConf,discoveryService,discoveryServiceClient,metricsCollectionService,opExecutor,new HashSet<DatasetMetricsReporter>(),typeService,instanceService);
  service.startAndWait();
  waitForService(Constants.Service.DATASET_EXECUTOR);
  waitForService(Constants.Service.DATASET_MANAGER);
  Locations.mkdirsIfNotExists(namespacedLocationFactory.get(Id.Namespace.DEFAULT));
}","protected static void initializeAndStartService(CConfiguration cConf) throws Exception {
  injector=Guice.createInjector(new ConfigModule(cConf),new DiscoveryRuntimeModule().getInMemoryModules(),new NonCustomLocationUnitTestModule().getModule(),new NamespaceClientRuntimeModule().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),new TransactionInMemoryModule(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getMasterModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Singleton.class);
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(RemoteDatasetFramework.class);
    }
  }
);
  AuthorizationEnforcer authEnforcer=injector.getInstance(AuthorizationEnforcer.class);
  authEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  authEnforcementService.startAndWait();
  AuthenticationContext authenticationContext=injector.getInstance(AuthenticationContext.class);
  DiscoveryService discoveryService=injector.getInstance(DiscoveryService.class);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  dsFramework=injector.getInstance(RemoteDatasetFramework.class);
  txManager=injector.getInstance(TransactionManager.class);
  txManager.startAndWait();
  TransactionSystemClient txSystemClient=injector.getInstance(TransactionSystemClient.class);
  TransactionSystemClientService txSystemClientService=new DelegatingTransactionSystemClientService(txSystemClient);
  NamespacedLocationFactory namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  SystemDatasetInstantiatorFactory datasetInstantiatorFactory=new SystemDatasetInstantiatorFactory(locationFactory,dsFramework,cConf);
  Impersonator impersonator=new DefaultImpersonator(cConf,null,null);
  DatasetAdminService datasetAdminService=new DatasetAdminService(dsFramework,cConf,locationFactory,datasetInstantiatorFactory,new NoOpMetadataStore(),impersonator);
  ImmutableSet<HttpHandler> handlers=ImmutableSet.<HttpHandler>of(new DatasetAdminOpHTTPHandler(datasetAdminService));
  MetricsCollectionService metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  opExecutorService=new DatasetOpExecutorService(cConf,discoveryService,metricsCollectionService,handlers);
  opExecutorService.startAndWait();
  Map<String,DatasetModule> defaultModules=injector.getInstance(Key.get(new TypeLiteral<Map<String,DatasetModule>>(){
  }
,Names.named(""String_Node_Str"")));
  ImmutableMap<String,DatasetModule> modules=ImmutableMap.<String,DatasetModule>builder().putAll(defaultModules).putAll(DatasetMetaTableUtil.getModules()).build();
  registryFactory=injector.getInstance(DatasetDefinitionRegistryFactory.class);
  inMemoryDatasetFramework=new InMemoryDatasetFramework(registryFactory,modules);
  DiscoveryExploreClient exploreClient=new DiscoveryExploreClient(discoveryServiceClient,authenticationContext);
  ExploreFacade exploreFacade=new ExploreFacade(exploreClient,cConf);
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  namespaceAdmin.create(NamespaceMeta.DEFAULT);
  NamespaceQueryAdmin namespaceQueryAdmin=injector.getInstance(NamespaceQueryAdmin.class);
  TransactionExecutorFactory txExecutorFactory=new DynamicTransactionExecutorFactory(txSystemClient);
  DatasetTypeManager typeManager=new DatasetTypeManager(cConf,locationFactory,txSystemClientService,txExecutorFactory,inMemoryDatasetFramework,impersonator);
  DatasetOpExecutor opExecutor=new InMemoryDatasetOpExecutor(dsFramework);
  DatasetInstanceManager instanceManager=new DatasetInstanceManager(txSystemClientService,txExecutorFactory,inMemoryDatasetFramework);
  PrivilegesManager privilegesManager=injector.getInstance(PrivilegesManager.class);
  DatasetTypeService typeService=new DatasetTypeService(typeManager,namespaceAdmin,namespacedLocationFactory,authEnforcer,privilegesManager,authenticationContext,cConf,impersonator,txSystemClientService,inMemoryDatasetFramework,txExecutorFactory,defaultModules);
  instanceService=new DatasetInstanceService(typeService,instanceManager,opExecutor,exploreFacade,namespaceQueryAdmin,authEnforcer,privilegesManager,authenticationContext);
  service=new DatasetService(cConf,discoveryService,discoveryServiceClient,metricsCollectionService,opExecutor,new HashSet<DatasetMetricsReporter>(),typeService,instanceService);
  service.startAndWait();
  waitForService(Constants.Service.DATASET_EXECUTOR);
  waitForService(Constants.Service.DATASET_MANAGER);
  Locations.mkdirsIfNotExists(namespacedLocationFactory.get(Id.Namespace.DEFAULT));
}"
5876,"@Override public void initialize() throws Exception {
  Job job=getContext().getHadoopJob();
  job.setMapperClass(Emitter.class);
  job.setReducerClass(Counter.class);
  job.setNumReduceTasks(1);
  context.addInput(Input.ofDataset(""String_Node_Str""));
  context.addOutput(Output.ofDataset(""String_Node_Str""));
}","@Override public void initialize() throws Exception {
  MapReduceContext context=getContext();
  Job job=context.getHadoopJob();
  job.setMapperClass(Emitter.class);
  job.setReducerClass(Counter.class);
  job.setNumReduceTasks(1);
  context.addInput(Input.ofDataset(""String_Node_Str""));
  context.addOutput(Output.ofDataset(""String_Node_Str""));
}"
5877,"@Override public void start(){
  super.start();
  try {
    logSchema=new LogSchema().getAvroSchema();
    FileMetaDataManager fileMetaDataManager=new FileMetaDataManager(tableUtil,txExecutorFactory,rootLocationFactory,namespacedLocationFactory,cConf,impersonator);
    AvroFileWriter avroFileWriter=new AvroFileWriter(fileMetaDataManager,namespacedLocationFactory,logBaseDir,logSchema,maxLogFileSizeBytes,syncIntervalBytes,inactiveIntervalMs,impersonator);
    logFileWriter=new SimpleLogFileWriter(avroFileWriter,checkpointIntervalMs);
    LogCleanup logCleanup=new LogCleanup(fileMetaDataManager,rootLocationFactory,retentionDurationMs,impersonator);
    scheduledExecutor.scheduleAtFixedRate(logCleanup,10,logCleanupIntervalMins,TimeUnit.MINUTES);
  }
 catch (  Exception e) {
    close();
    throw Throwables.propagate(e);
  }
}","@Override public void start(){
  super.start();
  try {
    logSchema=new LogSchema().getAvroSchema();
    FileMetaDataManager fileMetaDataManager=new FileMetaDataManager(tableUtil,txExecutorFactory,rootLocationFactory,namespacedLocationFactory,cConf,impersonator);
    AvroFileWriter avroFileWriter=new AvroFileWriter(fileMetaDataManager,namespacedLocationFactory,logBaseDir,logSchema,maxLogFileSizeBytes,syncIntervalBytes,maxFileLifetimeMs,impersonator);
    logFileWriter=new SimpleLogFileWriter(avroFileWriter,checkpointIntervalMs);
    LogCleanup logCleanup=new LogCleanup(fileMetaDataManager,rootLocationFactory,retentionDurationMs,impersonator);
    scheduledExecutor.scheduleAtFixedRate(logCleanup,10,logCleanupIntervalMins,TimeUnit.MINUTES);
  }
 catch (  Exception e) {
    close();
    throw Throwables.propagate(e);
  }
}"
5878,"@Inject public FileLogAppender(CConfiguration cConfig,DatasetFramework dsFramework,TransactionExecutorFactory txExecutorFactory,NamespacedLocationFactory namespacedLocationFactory,RootLocationFactory rootLocationFactory,Impersonator impersonator){
  setName(APPENDER_NAME);
  this.cConf=cConfig;
  this.tableUtil=new LogSaverTableUtil(dsFramework,cConfig);
  this.txExecutorFactory=txExecutorFactory;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.rootLocationFactory=rootLocationFactory;
  this.impersonator=impersonator;
  this.logBaseDir=cConfig.get(LoggingConfiguration.LOG_BASE_DIR);
  Preconditions.checkNotNull(logBaseDir,""String_Node_Str"");
  this.syncIntervalBytes=cConfig.getInt(LoggingConfiguration.LOG_FILE_SYNC_INTERVAL_BYTES,2 * 1024 * 1024);
  Preconditions.checkArgument(this.syncIntervalBytes > 0,""String_Node_Str"",this.syncIntervalBytes);
  long retentionDurationDays=cConfig.getLong(LoggingConfiguration.LOG_RETENTION_DURATION_DAYS,-1);
  Preconditions.checkArgument(retentionDurationDays > 0,""String_Node_Str"",retentionDurationDays);
  this.retentionDurationMs=TimeUnit.MILLISECONDS.convert(retentionDurationDays,TimeUnit.DAYS);
  maxLogFileSizeBytes=cConfig.getLong(LoggingConfiguration.LOG_MAX_FILE_SIZE_BYTES,20 * 1024 * 1024);
  Preconditions.checkArgument(maxLogFileSizeBytes > 0,""String_Node_Str"",maxLogFileSizeBytes);
  inactiveIntervalMs=cConfig.getLong(LoggingConfiguration.LOG_SAVER_INACTIVE_FILE_INTERVAL_MS,LoggingConfiguration.DEFAULT_LOG_SAVER_INACTIVE_FILE_INTERVAL_MS);
  Preconditions.checkArgument(inactiveIntervalMs > 0,""String_Node_Str"",inactiveIntervalMs);
  checkpointIntervalMs=cConfig.getLong(LoggingConfiguration.LOG_SAVER_CHECKPOINT_INTERVAL_MS,LoggingConfiguration.DEFAULT_LOG_SAVER_CHECKPOINT_INTERVAL_MS);
  Preconditions.checkArgument(checkpointIntervalMs > 0,""String_Node_Str"",checkpointIntervalMs);
  logCleanupIntervalMins=cConfig.getInt(LoggingConfiguration.LOG_CLEANUP_RUN_INTERVAL_MINS,LoggingConfiguration.DEFAULT_LOG_CLEANUP_RUN_INTERVAL_MINS);
  Preconditions.checkArgument(logCleanupIntervalMins > 0,""String_Node_Str"",logCleanupIntervalMins);
  this.scheduledExecutor=MoreExecutors.listeningDecorator(Executors.newSingleThreadScheduledExecutor(Threads.createDaemonThreadFactory(""String_Node_Str"")));
}","@Inject public FileLogAppender(CConfiguration cConfig,DatasetFramework dsFramework,TransactionExecutorFactory txExecutorFactory,NamespacedLocationFactory namespacedLocationFactory,RootLocationFactory rootLocationFactory,Impersonator impersonator){
  setName(APPENDER_NAME);
  this.cConf=cConfig;
  this.tableUtil=new LogSaverTableUtil(dsFramework,cConfig);
  this.txExecutorFactory=txExecutorFactory;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.rootLocationFactory=rootLocationFactory;
  this.impersonator=impersonator;
  this.logBaseDir=cConfig.get(LoggingConfiguration.LOG_BASE_DIR);
  Preconditions.checkNotNull(logBaseDir,""String_Node_Str"");
  this.syncIntervalBytes=cConfig.getInt(LoggingConfiguration.LOG_FILE_SYNC_INTERVAL_BYTES,2 * 1024 * 1024);
  Preconditions.checkArgument(this.syncIntervalBytes > 0,""String_Node_Str"",this.syncIntervalBytes);
  long retentionDurationDays=cConfig.getLong(LoggingConfiguration.LOG_RETENTION_DURATION_DAYS,-1);
  Preconditions.checkArgument(retentionDurationDays > 0,""String_Node_Str"",retentionDurationDays);
  this.retentionDurationMs=TimeUnit.MILLISECONDS.convert(retentionDurationDays,TimeUnit.DAYS);
  maxLogFileSizeBytes=cConfig.getLong(LoggingConfiguration.LOG_MAX_FILE_SIZE_BYTES,20 * 1024 * 1024);
  Preconditions.checkArgument(maxLogFileSizeBytes > 0,""String_Node_Str"",maxLogFileSizeBytes);
  maxFileLifetimeMs=cConfig.getLong(LoggingConfiguration.LOG_SAVER_MAX_FILE_LIFETIME,LoggingConfiguration.DEFAULT_LOG_SAVER_MAX_FILE_LIFETIME_MS);
  Preconditions.checkArgument(maxFileLifetimeMs > 0,""String_Node_Str"",maxFileLifetimeMs);
  if (cConf.get(LoggingConfiguration.LOG_SAVER_INACTIVE_FILE_INTERVAL_MS) != null) {
    LOG.warn(""String_Node_Str"",LoggingConfiguration.LOG_SAVER_INACTIVE_FILE_INTERVAL_MS,LoggingConfiguration.LOG_SAVER_MAX_FILE_LIFETIME);
  }
  checkpointIntervalMs=cConfig.getLong(LoggingConfiguration.LOG_SAVER_CHECKPOINT_INTERVAL_MS,LoggingConfiguration.DEFAULT_LOG_SAVER_CHECKPOINT_INTERVAL_MS);
  Preconditions.checkArgument(checkpointIntervalMs > 0,""String_Node_Str"",checkpointIntervalMs);
  logCleanupIntervalMins=cConfig.getInt(LoggingConfiguration.LOG_CLEANUP_RUN_INTERVAL_MINS,LoggingConfiguration.DEFAULT_LOG_CLEANUP_RUN_INTERVAL_MINS);
  Preconditions.checkArgument(logCleanupIntervalMins > 0,""String_Node_Str"",logCleanupIntervalMins);
  this.scheduledExecutor=MoreExecutors.listeningDecorator(Executors.newSingleThreadScheduledExecutor(Threads.createDaemonThreadFactory(""String_Node_Str"")));
}"
5879,"private void flush(boolean force) throws Exception {
  long currentTs=System.currentTimeMillis();
  if (!force && currentTs - lastCheckpointTime < flushIntervalMs) {
    return;
  }
  avroFileWriter.flush();
  checkpointManager.saveCheckpoint(partitionCheckpointMap);
  lastCheckpointTime=currentTs;
}","@Override public void flush(boolean force) throws IOException {
  try {
    long currentTs=System.currentTimeMillis();
    if (!force && currentTs - lastCheckpointTime < flushIntervalMs) {
      return;
    }
    avroFileWriter.flush();
    checkpointManager.saveCheckpoint(partitionCheckpointMap);
    lastCheckpointTime=currentTs;
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw new IOException(e);
  }
}"
5880,"@Override public void append(List<KafkaLogEvent> events) throws Exception {
  if (events.isEmpty()) {
    return;
  }
  KafkaLogEvent event=events.get(0);
  int partition=event.getPartition();
  Checkpoint maxCheckpoint=partitionCheckpointMap.get(partition);
  maxCheckpoint=maxCheckpoint == null ? new Checkpoint(-1,-1) : maxCheckpoint;
  for (  KafkaLogEvent e : events) {
    if (e.getNextOffset() > maxCheckpoint.getNextOffset()) {
      maxCheckpoint=new Checkpoint(e.getNextOffset(),e.getLogEvent().getTimeStamp());
    }
  }
  partitionCheckpointMap.put(partition,maxCheckpoint);
  avroFileWriter.append(events);
  flush(false);
}","@Override public void append(List<KafkaLogEvent> events) throws Exception {
  if (events.isEmpty()) {
    return;
  }
  KafkaLogEvent event=events.get(0);
  int partition=event.getPartition();
  Checkpoint maxCheckpoint=partitionCheckpointMap.get(partition);
  maxCheckpoint=maxCheckpoint == null ? new Checkpoint(-1,-1) : maxCheckpoint;
  for (  KafkaLogEvent e : events) {
    if (e.getNextOffset() > maxCheckpoint.getNextOffset()) {
      maxCheckpoint=new Checkpoint(e.getNextOffset(),e.getLogEvent().getTimeStamp());
    }
  }
  partitionCheckpointMap.put(partition,maxCheckpoint);
  avroFileWriter.append(events);
}"
5881,"@Inject KafkaLogWriterPlugin(CConfiguration cConf,FileMetaDataManager fileMetaDataManager,CheckpointManagerFactory checkpointManagerFactory,RootLocationFactory rootLocationFactory,NamespacedLocationFactory namespacedLocationFactory,Impersonator impersonator) throws Exception {
  this.serializer=new LoggingEventSerializer();
  this.messageTable=TreeBasedTable.create();
  this.logBaseDir=cConf.get(LoggingConfiguration.LOG_BASE_DIR);
  Preconditions.checkNotNull(this.logBaseDir,""String_Node_Str"");
  LOG.debug(String.format(""String_Node_Str"",this.logBaseDir));
  long retentionDurationDays=cConf.getLong(LoggingConfiguration.LOG_RETENTION_DURATION_DAYS,LoggingConfiguration.DEFAULT_LOG_RETENTION_DURATION_DAYS);
  Preconditions.checkArgument(retentionDurationDays > 0,""String_Node_Str"",retentionDurationDays);
  long maxLogFileSizeBytes=cConf.getLong(LoggingConfiguration.LOG_MAX_FILE_SIZE_BYTES,100 * 1000 * 1000);
  Preconditions.checkArgument(maxLogFileSizeBytes > 0,""String_Node_Str"",maxLogFileSizeBytes);
  int syncIntervalBytes=cConf.getInt(LoggingConfiguration.LOG_FILE_SYNC_INTERVAL_BYTES,10 * 1000 * 1000);
  Preconditions.checkArgument(syncIntervalBytes > 0,""String_Node_Str"",syncIntervalBytes);
  long checkpointIntervalMs=cConf.getLong(LoggingConfiguration.LOG_SAVER_CHECKPOINT_INTERVAL_MS,LoggingConfiguration.DEFAULT_LOG_SAVER_CHECKPOINT_INTERVAL_MS);
  Preconditions.checkArgument(checkpointIntervalMs > 0,""String_Node_Str"",checkpointIntervalMs);
  long inactiveIntervalMs=cConf.getLong(LoggingConfiguration.LOG_SAVER_INACTIVE_FILE_INTERVAL_MS,LoggingConfiguration.DEFAULT_LOG_SAVER_INACTIVE_FILE_INTERVAL_MS);
  Preconditions.checkArgument(inactiveIntervalMs > 0,""String_Node_Str"",inactiveIntervalMs);
  this.eventBucketIntervalMs=cConf.getLong(LoggingConfiguration.LOG_SAVER_EVENT_BUCKET_INTERVAL_MS,LoggingConfiguration.DEFAULT_LOG_SAVER_EVENT_BUCKET_INTERVAL_MS);
  Preconditions.checkArgument(this.eventBucketIntervalMs > 0,""String_Node_Str"",this.eventBucketIntervalMs);
  this.maxNumberOfBucketsInTable=cConf.getLong(LoggingConfiguration.LOG_SAVER_MAXIMUM_INMEMORY_EVENT_BUCKETS,LoggingConfiguration.DEFAULT_LOG_SAVER_MAXIMUM_INMEMORY_EVENT_BUCKETS);
  Preconditions.checkArgument(this.maxNumberOfBucketsInTable > 0,""String_Node_Str"",this.maxNumberOfBucketsInTable);
  long topicCreationSleepMs=cConf.getLong(LoggingConfiguration.LOG_SAVER_TOPIC_WAIT_SLEEP_MS,LoggingConfiguration.DEFAULT_LOG_SAVER_TOPIC_WAIT_SLEEP_MS);
  Preconditions.checkArgument(topicCreationSleepMs > 0,""String_Node_Str"",topicCreationSleepMs);
  logCleanupIntervalMins=cConf.getInt(LoggingConfiguration.LOG_CLEANUP_RUN_INTERVAL_MINS,LoggingConfiguration.DEFAULT_LOG_CLEANUP_RUN_INTERVAL_MINS);
  Preconditions.checkArgument(logCleanupIntervalMins > 0,""String_Node_Str"",logCleanupIntervalMins);
  AvroFileWriter avroFileWriter=new AvroFileWriter(fileMetaDataManager,namespacedLocationFactory,logBaseDir,serializer.getAvroSchema(),maxLogFileSizeBytes,syncIntervalBytes,inactiveIntervalMs,impersonator);
  checkpointManager=checkpointManagerFactory.create(cConf.get(Constants.Logging.KAFKA_TOPIC),CHECKPOINT_ROW_KEY_PREFIX);
  this.logFileWriter=new CheckpointingLogFileWriter(avroFileWriter,checkpointManager,checkpointIntervalMs);
  long retentionDurationMs=TimeUnit.MILLISECONDS.convert(retentionDurationDays,TimeUnit.DAYS);
  this.logCleanup=new LogCleanup(fileMetaDataManager,rootLocationFactory,retentionDurationMs,impersonator);
}","@Inject KafkaLogWriterPlugin(CConfiguration cConf,FileMetaDataManager fileMetaDataManager,CheckpointManagerFactory checkpointManagerFactory,RootLocationFactory rootLocationFactory,NamespacedLocationFactory namespacedLocationFactory,Impersonator impersonator) throws Exception {
  this.serializer=new LoggingEventSerializer();
  this.messageTable=TreeBasedTable.create();
  this.logBaseDir=cConf.get(LoggingConfiguration.LOG_BASE_DIR);
  Preconditions.checkNotNull(this.logBaseDir,""String_Node_Str"");
  LOG.debug(String.format(""String_Node_Str"",this.logBaseDir));
  long retentionDurationDays=cConf.getLong(LoggingConfiguration.LOG_RETENTION_DURATION_DAYS,LoggingConfiguration.DEFAULT_LOG_RETENTION_DURATION_DAYS);
  Preconditions.checkArgument(retentionDurationDays > 0,""String_Node_Str"",retentionDurationDays);
  long maxLogFileSizeBytes=cConf.getLong(LoggingConfiguration.LOG_MAX_FILE_SIZE_BYTES,100 * 1000 * 1000);
  Preconditions.checkArgument(maxLogFileSizeBytes > 0,""String_Node_Str"",maxLogFileSizeBytes);
  int syncIntervalBytes=cConf.getInt(LoggingConfiguration.LOG_FILE_SYNC_INTERVAL_BYTES,10 * 1000 * 1000);
  Preconditions.checkArgument(syncIntervalBytes > 0,""String_Node_Str"",syncIntervalBytes);
  long checkpointIntervalMs=cConf.getLong(LoggingConfiguration.LOG_SAVER_CHECKPOINT_INTERVAL_MS,LoggingConfiguration.DEFAULT_LOG_SAVER_CHECKPOINT_INTERVAL_MS);
  Preconditions.checkArgument(checkpointIntervalMs > 0,""String_Node_Str"",checkpointIntervalMs);
  long maxFileLifetimeMs=cConf.getLong(LoggingConfiguration.LOG_SAVER_MAX_FILE_LIFETIME,LoggingConfiguration.DEFAULT_LOG_SAVER_MAX_FILE_LIFETIME_MS);
  Preconditions.checkArgument(maxFileLifetimeMs > 0,""String_Node_Str"",maxFileLifetimeMs);
  if (cConf.get(LoggingConfiguration.LOG_SAVER_INACTIVE_FILE_INTERVAL_MS) != null) {
    LOG.warn(""String_Node_Str"",LoggingConfiguration.LOG_SAVER_INACTIVE_FILE_INTERVAL_MS,LoggingConfiguration.LOG_SAVER_MAX_FILE_LIFETIME);
  }
  this.eventBucketIntervalMs=cConf.getLong(LoggingConfiguration.LOG_SAVER_EVENT_BUCKET_INTERVAL_MS,LoggingConfiguration.DEFAULT_LOG_SAVER_EVENT_BUCKET_INTERVAL_MS);
  Preconditions.checkArgument(this.eventBucketIntervalMs > 0,""String_Node_Str"",this.eventBucketIntervalMs);
  this.maxNumberOfBucketsInTable=cConf.getLong(LoggingConfiguration.LOG_SAVER_MAXIMUM_INMEMORY_EVENT_BUCKETS,LoggingConfiguration.DEFAULT_LOG_SAVER_MAXIMUM_INMEMORY_EVENT_BUCKETS);
  Preconditions.checkArgument(this.maxNumberOfBucketsInTable > 0,""String_Node_Str"",this.maxNumberOfBucketsInTable);
  long topicCreationSleepMs=cConf.getLong(LoggingConfiguration.LOG_SAVER_TOPIC_WAIT_SLEEP_MS,LoggingConfiguration.DEFAULT_LOG_SAVER_TOPIC_WAIT_SLEEP_MS);
  Preconditions.checkArgument(topicCreationSleepMs > 0,""String_Node_Str"",topicCreationSleepMs);
  logCleanupIntervalMins=cConf.getInt(LoggingConfiguration.LOG_CLEANUP_RUN_INTERVAL_MINS,LoggingConfiguration.DEFAULT_LOG_CLEANUP_RUN_INTERVAL_MINS);
  Preconditions.checkArgument(logCleanupIntervalMins > 0,""String_Node_Str"",logCleanupIntervalMins);
  AvroFileWriter avroFileWriter=new AvroFileWriter(fileMetaDataManager,namespacedLocationFactory,logBaseDir,serializer.getAvroSchema(),maxLogFileSizeBytes,syncIntervalBytes,maxFileLifetimeMs,impersonator);
  checkpointManager=checkpointManagerFactory.create(cConf.get(Constants.Logging.KAFKA_TOPIC),CHECKPOINT_ROW_KEY_PREFIX);
  this.logFileWriter=new CheckpointingLogFileWriter(avroFileWriter,checkpointManager,checkpointIntervalMs);
  long retentionDurationMs=TimeUnit.MILLISECONDS.convert(retentionDurationDays,TimeUnit.DAYS);
  this.logCleanup=new LogCleanup(fileMetaDataManager,rootLocationFactory,retentionDurationMs,impersonator);
}"
5882,"@Override public void run(){
  while (true) {
    try {
      if (writeListMap.isEmpty()) {
        int messages=0;
        long limitKey=(System.currentTimeMillis() / eventBucketIntervalMs) - maxNumberOfBucketsInTable;
synchronized (messageTable) {
          SortedSet<Long> rowKeySet=messageTable.rowKeySet();
          if (!rowKeySet.isEmpty()) {
            int numBuckets=rowKeySet.size();
            long oldestBucketKey=rowKeySet.first();
            Map<String,Entry<Long,List<KafkaLogEvent>>> row=messageTable.row(oldestBucketKey);
            for (Iterator<Map.Entry<String,Entry<Long,List<KafkaLogEvent>>>> it=row.entrySet().iterator(); it.hasNext(); ) {
              Map.Entry<String,Entry<Long,List<KafkaLogEvent>>> mapEntry=it.next();
              if (numBuckets < maxNumberOfBucketsInTable && limitKey < mapEntry.getValue().getKey()) {
                break;
              }
              writeListMap.putAll(mapEntry.getKey(),mapEntry.getValue().getValue());
              messages+=mapEntry.getValue().getValue().size();
              it.remove();
            }
          }
        }
        LOG.trace(""String_Node_Str"",messages);
      }
      long sleepTimeNanos=writeListMap.isEmpty() ? SLEEP_TIME_NS : 1;
      if (stopLatch.await(sleepTimeNanos,TimeUnit.NANOSECONDS)) {
        LOG.debug(""String_Node_Str"");
        return;
      }
 else {
        LOG.trace(""String_Node_Str"",sleepTimeNanos);
      }
      for (Iterator<Entry<String,Collection<KafkaLogEvent>>> it=writeListMap.asMap().entrySet().iterator(); it.hasNext(); ) {
        Entry<String,Collection<KafkaLogEvent>> mapEntry=it.next();
        List<KafkaLogEvent> list=(List<KafkaLogEvent>)mapEntry.getValue();
        Collections.sort(list);
        logFileWriter.append(list);
        it.remove();
      }
      exponentialBackoff.reset();
    }
 catch (    Throwable e) {
      LOG.error(""String_Node_Str"",e);
      try {
        exponentialBackoff.backoff();
      }
 catch (      InterruptedException e1) {
      }
    }
  }
}","@Override public void run(){
  while (true) {
    try {
      if (writeListMap.isEmpty()) {
        int messages=0;
        long limitKey=(System.currentTimeMillis() / eventBucketIntervalMs) - maxNumberOfBucketsInTable;
synchronized (messageTable) {
          SortedSet<Long> rowKeySet=messageTable.rowKeySet();
          if (!rowKeySet.isEmpty()) {
            int numBuckets=rowKeySet.size();
            long oldestBucketKey=rowKeySet.first();
            Map<String,Entry<Long,List<KafkaLogEvent>>> row=messageTable.row(oldestBucketKey);
            for (Iterator<Map.Entry<String,Entry<Long,List<KafkaLogEvent>>>> it=row.entrySet().iterator(); it.hasNext(); ) {
              Map.Entry<String,Entry<Long,List<KafkaLogEvent>>> mapEntry=it.next();
              if (numBuckets < maxNumberOfBucketsInTable && limitKey < mapEntry.getValue().getKey()) {
                break;
              }
              writeListMap.putAll(mapEntry.getKey(),mapEntry.getValue().getValue());
              messages+=mapEntry.getValue().getValue().size();
              it.remove();
            }
          }
        }
        LOG.trace(""String_Node_Str"",messages);
      }
      long sleepTimeNanos=writeListMap.isEmpty() ? SLEEP_TIME_NS : 1;
      if (stopLatch.await(sleepTimeNanos,TimeUnit.NANOSECONDS)) {
        LOG.debug(""String_Node_Str"");
        return;
      }
 else {
        LOG.trace(""String_Node_Str"",sleepTimeNanos);
      }
      for (Iterator<Entry<String,Collection<KafkaLogEvent>>> it=writeListMap.asMap().entrySet().iterator(); it.hasNext(); ) {
        Entry<String,Collection<KafkaLogEvent>> mapEntry=it.next();
        List<KafkaLogEvent> list=(List<KafkaLogEvent>)mapEntry.getValue();
        Collections.sort(list);
        logFileWriter.append(list);
        it.remove();
      }
      logFileWriter.flush(false);
      exponentialBackoff.reset();
    }
 catch (    Throwable e) {
      LOG.error(""String_Node_Str"",e);
      try {
        exponentialBackoff.backoff();
      }
 catch (      InterruptedException e1) {
      }
    }
  }
}"
5883,"/** 
 * Constructs an AvroFileWriter object.
 * @param fileMetaDataManager used to store file meta data.
 * @param namespacedLocationFactory the namespaced location factory
 * @param logBaseDir the basedirectory for logs as defined in configuration
 * @param schema schema of the Avro data to be written.
 * @param maxFileSize Avro files greater than maxFileSize will get rotated.
 * @param syncIntervalBytes the approximate number of uncompressed bytes to write in each block.
 * @param inactiveIntervalMs files that have no data written for more than inactiveIntervalMs will be closed.
 */
public AvroFileWriter(FileMetaDataManager fileMetaDataManager,NamespacedLocationFactory namespacedLocationFactory,String logBaseDir,Schema schema,long maxFileSize,int syncIntervalBytes,long inactiveIntervalMs,Impersonator impersonator){
  this.fileMetaDataManager=fileMetaDataManager;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.logBaseDir=logBaseDir;
  this.schema=schema;
  this.syncIntervalBytes=syncIntervalBytes;
  this.fileMap=Maps.newHashMap();
  this.maxFileSize=maxFileSize;
  this.inactiveIntervalMs=inactiveIntervalMs;
  this.impersonator=impersonator;
}","/** 
 * Constructs an AvroFileWriter object.
 * @param fileMetaDataManager used to store file meta data.
 * @param namespacedLocationFactory the namespaced location factory
 * @param logBaseDir the basedirectory for logs as defined in configuration
 * @param schema schema of the Avro data to be written.
 * @param maxFileSize Avro files greater than maxFileSize will get rotated.
 * @param syncIntervalBytes the approximate number of uncompressed bytes to write in each block.
 * @param maxFileLifetimeMs files that are older than maxFileLifetimeMs will be closed.
 */
public AvroFileWriter(FileMetaDataManager fileMetaDataManager,NamespacedLocationFactory namespacedLocationFactory,String logBaseDir,Schema schema,long maxFileSize,int syncIntervalBytes,long maxFileLifetimeMs,Impersonator impersonator){
  this.fileMetaDataManager=fileMetaDataManager;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.logBaseDir=logBaseDir;
  this.schema=schema;
  this.syncIntervalBytes=syncIntervalBytes;
  this.fileMap=Maps.newHashMap();
  this.maxFileSize=maxFileSize;
  this.maxFileLifetimeMs=maxFileLifetimeMs;
  this.impersonator=impersonator;
}"
5884,"public void append(LogWriteEvent event) throws IOException {
  try {
    dataFileWriter.append(event.getGenericRecord());
    lastModifiedTs=System.currentTimeMillis();
  }
 catch (  Exception e) {
    close();
    throw new IOException(""String_Node_Str"" + location,e);
  }
}","public void append(LogWriteEvent event) throws IOException {
  try {
    dataFileWriter.append(event.getGenericRecord());
  }
 catch (  Exception e) {
    close();
    throw new IOException(""String_Node_Str"" + location,e);
  }
}"
5885,"/** 
 * Opens the underlying file for writing. If open throws an exception then underlying file may still need to be deleted.
 * @throws IOException
 */
void open() throws IOException {
  try {
    this.outputStream=new FSDataOutputStream(location.getOutputStream(),null);
    this.dataFileWriter=new DataFileWriter<>(new GenericDatumWriter<GenericRecord>(schema));
    this.dataFileWriter.create(schema,this.outputStream);
    this.dataFileWriter.setSyncInterval(syncIntervalBytes);
    this.lastModifiedTs=System.currentTimeMillis();
  }
 catch (  Exception e) {
    close();
    throw new IOException(""String_Node_Str"" + location,e);
  }
  this.isOpen=true;
}","/** 
 * Opens the underlying file for writing. If open throws an exception then underlying file may still need to be deleted.
 * @throws IOException
 */
void open() throws IOException {
  try {
    this.outputStream=new FSDataOutputStream(location.getOutputStream(),null);
    this.dataFileWriter=new DataFileWriter<>(new GenericDatumWriter<GenericRecord>(schema));
    this.dataFileWriter.create(schema,this.outputStream);
    this.dataFileWriter.setSyncInterval(syncIntervalBytes);
    this.createTime=System.currentTimeMillis();
  }
 catch (  Exception e) {
    close();
    throw new IOException(""String_Node_Str"" + location,e);
  }
  this.isOpen=true;
}"
5886,"private void flush(boolean force) throws Exception {
  long currentTs=System.currentTimeMillis();
  if (!force && currentTs - lastCheckpointTime < flushIntervalMs) {
    return;
  }
  avroFileWriter.flush();
  lastCheckpointTime=currentTs;
}","public void flush(boolean force) throws IOException {
  try {
    long currentTs=System.currentTimeMillis();
    if (!force && currentTs - lastCheckpointTime < flushIntervalMs) {
      return;
    }
    avroFileWriter.flush();
    lastCheckpointTime=currentTs;
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw new IOException(e);
  }
}"
5887,"/** 
 * List of all the entries in the secure store.
 * @param namespace The namespace that this key belongs to.
 * @return A list of {@link SecureStoreMetadata} objects representing the data stored in the store.
 * @throws IOException If there was a problem reading from the keystore.
 * @throws Exception If the specified namespace does not exist.
 */
List<SecureStoreMetadata> listSecureData(String namespace) throws Exception ;","/** 
 * List of all the entries in the secure store.
 * @param namespace The namespace that this key belongs to.
 * @return A list of {@link SecureStoreMetadata} objects representing the data stored in the store.
 * @throws IOException If there was a problem reading from the keystore.
 * @throws Exception If the specified namespace does not exist.
 */
Map<String,String> listSecureData(String namespace) throws Exception ;"
5888,"@Override public Module getStandaloneModules(){
  return Modules.combine(new AppFabricServiceModule(StreamHandler.class,StreamFetchHandler.class,StreamViewHttpHandler.class),new ConfigStoreModule().getStandaloneModule(),new SecureStoreModules().getStandaloneModules(),new EntityVerifierModule(),new AuthenticationContextModules().getMasterModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(AppFabricServer.class).to(StandaloneAppFabricServer.class).in(Scopes.SINGLETON);
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      bind(MRJobInfoFetcher.class).to(LocalMRJobInfoFetcher.class);
      bind(StorageProviderNamespaceAdmin.class).to(LocalStorageProviderNamespaceAdmin.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      addInMemoryBindings(binder());
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      servicesNamesBinder.addBinding().toInstance(Constants.Service.STREAMS);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      handlerHookNamesBinder.addBinding().toInstance(Constants.Stream.STREAM_HANDLER);
    }
  }
);
}","@Override public Module getStandaloneModules(){
  return Modules.combine(new AppFabricServiceModule(StreamHandler.class,StreamFetchHandler.class,StreamViewHttpHandler.class),new ConfigStoreModule().getStandaloneModule(),new EntityVerifierModule(),new AuthenticationContextModules().getMasterModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(AppFabricServer.class).to(StandaloneAppFabricServer.class).in(Scopes.SINGLETON);
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      bind(MRJobInfoFetcher.class).to(LocalMRJobInfoFetcher.class);
      bind(StorageProviderNamespaceAdmin.class).to(LocalStorageProviderNamespaceAdmin.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      addInMemoryBindings(binder());
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      servicesNamesBinder.addBinding().toInstance(Constants.Service.STREAMS);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      handlerHookNamesBinder.addBinding().toInstance(Constants.Stream.STREAM_HANDLER);
    }
  }
);
}"
5889,"@Override public Module getDistributedModules(){
  return Modules.combine(new AppFabricServiceModule(ImpersonationHandler.class),new ConfigStoreModule().getDistributedModule(),new SecureStoreModules().getDistributedModules(),new EntityVerifierModule(),new AuthenticationContextModules().getMasterModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(DistributedSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      bind(MRJobInfoFetcher.class).to(DistributedMRJobInfoFetcher.class);
      bind(StorageProviderNamespaceAdmin.class).to(DistributedStorageProviderNamespaceAdmin.class);
      bind(UGIProvider.class).to(DefaultUGIProvider.class);
      MapBinder<String,MasterServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,MasterServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(LogSaverStatusServiceManager.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(TransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(MetricsProcessorStatusServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(MetricsServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(AppFabricServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(StreamServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(DatasetExecutorServiceManager.class);
      mapBinder.addBinding(Constants.Service.METADATA_SERVICE).to(MetadataServiceManager.class);
      mapBinder.addBinding(Constants.Service.REMOTE_SYSTEM_OPERATION).to(RemoteSystemOperationServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(ExploreServiceManager.class);
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
    }
  }
);
}","@Override public Module getDistributedModules(){
  return Modules.combine(new AppFabricServiceModule(ImpersonationHandler.class),new ConfigStoreModule().getDistributedModule(),new EntityVerifierModule(),new AuthenticationContextModules().getMasterModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(DistributedSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      bind(MRJobInfoFetcher.class).to(DistributedMRJobInfoFetcher.class);
      bind(StorageProviderNamespaceAdmin.class).to(DistributedStorageProviderNamespaceAdmin.class);
      bind(UGIProvider.class).to(DefaultUGIProvider.class);
      MapBinder<String,MasterServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,MasterServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(LogSaverStatusServiceManager.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(TransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(MetricsProcessorStatusServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(MetricsServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(AppFabricServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(StreamServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(DatasetExecutorServiceManager.class);
      mapBinder.addBinding(Constants.Service.METADATA_SERVICE).to(MetadataServiceManager.class);
      mapBinder.addBinding(Constants.Service.REMOTE_SYSTEM_OPERATION).to(RemoteSystemOperationServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(ExploreServiceManager.class);
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
    }
  }
);
}"
5890,"@Override public Module getInMemoryModules(){
  return Modules.combine(new AppFabricServiceModule(StreamHandler.class,StreamFetchHandler.class,StreamViewHttpHandler.class),new ConfigStoreModule().getInMemoryModule(),new SecureStoreModules().getInMemoryModules(),new EntityVerifierModule(),new AuthenticationContextModules().getMasterModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      bind(MRJobInfoFetcher.class).to(LocalMRJobInfoFetcher.class);
      bind(StorageProviderNamespaceAdmin.class).to(LocalStorageProviderNamespaceAdmin.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      addInMemoryBindings(binder());
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      servicesNamesBinder.addBinding().toInstance(Constants.Service.STREAMS);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      handlerHookNamesBinder.addBinding().toInstance(Constants.Stream.STREAM_HANDLER);
    }
  }
);
}","@Override public Module getInMemoryModules(){
  return Modules.combine(new AppFabricServiceModule(StreamHandler.class,StreamFetchHandler.class,StreamViewHttpHandler.class),new ConfigStoreModule().getInMemoryModule(),new EntityVerifierModule(),new AuthenticationContextModules().getMasterModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      bind(MRJobInfoFetcher.class).to(LocalMRJobInfoFetcher.class);
      bind(StorageProviderNamespaceAdmin.class).to(LocalStorageProviderNamespaceAdmin.class);
      bind(UGIProvider.class).to(UnsupportedUGIProvider.class);
      addInMemoryBindings(binder());
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      servicesNamesBinder.addBinding().toInstance(Constants.Service.STREAMS);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      handlerHookNamesBinder.addBinding().toInstance(Constants.Stream.STREAM_HANDLER);
    }
  }
);
}"
5891,"@Override protected void configure(){
  bind(PipelineFactory.class).to(SynchronousPipelineFactory.class);
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<AppDeploymentInfo,ApplicationWithPrograms>>(){
  }
,new TypeLiteral<LocalApplicationManager<AppDeploymentInfo,ApplicationWithPrograms>>(){
  }
).build(new TypeLiteral<ManagerFactory<AppDeploymentInfo,ApplicationWithPrograms>>(){
  }
));
  bind(Store.class).to(DefaultStore.class);
  bind(RuntimeStore.class).to(DefaultStore.class);
  bind(ArtifactStore.class).in(Scopes.SINGLETON);
  bind(ProgramLifecycleService.class).in(Scopes.SINGLETON);
  bind(NamespaceAdmin.class).to(DefaultNamespaceAdmin.class).in(Scopes.SINGLETON);
  bind(NamespaceQueryAdmin.class).to(DefaultNamespaceQueryAdmin.class).in(Scopes.SINGLETON);
  bind(SecureStoreService.class).to(DefaultSecureStoreService.class);
  Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.AppFabric.HANDLERS_BINDING));
  CommonHandlers.add(handlerBinder);
  handlerBinder.addBinding().to(ConfigHandler.class);
  handlerBinder.addBinding().to(VersionHandler.class);
  handlerBinder.addBinding().to(MonitorHandler.class);
  handlerBinder.addBinding().to(UsageHandler.class);
  handlerBinder.addBinding().to(NamespaceHttpHandler.class);
  handlerBinder.addBinding().to(NotificationFeedHttpHandler.class);
  handlerBinder.addBinding().to(AppLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(DashboardHttpHandler.class);
  handlerBinder.addBinding().to(ProgramLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(PreferencesHttpHandler.class);
  handlerBinder.addBinding().to(ConsoleSettingsHttpHandler.class);
  handlerBinder.addBinding().to(TransactionHttpHandler.class);
  handlerBinder.addBinding().to(WorkflowHttpHandler.class);
  handlerBinder.addBinding().to(ArtifactHttpHandler.class);
  handlerBinder.addBinding().to(WorkflowStatsSLAHttpHandler.class);
  handlerBinder.addBinding().to(AuthorizationHandler.class);
  handlerBinder.addBinding().to(SecureStoreHandler.class);
  handlerBinder.addBinding().to(RemotePrivilegesHandler.class);
  for (  Class<? extends HttpHandler> handlerClass : handlerClasses) {
    handlerBinder.addBinding().to(handlerClass);
  }
}","@Override protected void configure(){
  bind(PipelineFactory.class).to(SynchronousPipelineFactory.class);
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<AppDeploymentInfo,ApplicationWithPrograms>>(){
  }
,new TypeLiteral<LocalApplicationManager<AppDeploymentInfo,ApplicationWithPrograms>>(){
  }
).build(new TypeLiteral<ManagerFactory<AppDeploymentInfo,ApplicationWithPrograms>>(){
  }
));
  bind(Store.class).to(DefaultStore.class);
  bind(RuntimeStore.class).to(DefaultStore.class);
  bind(ArtifactStore.class).in(Scopes.SINGLETON);
  bind(ProgramLifecycleService.class).in(Scopes.SINGLETON);
  bind(NamespaceAdmin.class).to(DefaultNamespaceAdmin.class).in(Scopes.SINGLETON);
  bind(NamespaceQueryAdmin.class).to(DefaultNamespaceQueryAdmin.class).in(Scopes.SINGLETON);
  Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.AppFabric.HANDLERS_BINDING));
  CommonHandlers.add(handlerBinder);
  handlerBinder.addBinding().to(ConfigHandler.class);
  handlerBinder.addBinding().to(VersionHandler.class);
  handlerBinder.addBinding().to(MonitorHandler.class);
  handlerBinder.addBinding().to(UsageHandler.class);
  handlerBinder.addBinding().to(NamespaceHttpHandler.class);
  handlerBinder.addBinding().to(NotificationFeedHttpHandler.class);
  handlerBinder.addBinding().to(AppLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(DashboardHttpHandler.class);
  handlerBinder.addBinding().to(ProgramLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(PreferencesHttpHandler.class);
  handlerBinder.addBinding().to(ConsoleSettingsHttpHandler.class);
  handlerBinder.addBinding().to(TransactionHttpHandler.class);
  handlerBinder.addBinding().to(WorkflowHttpHandler.class);
  handlerBinder.addBinding().to(ArtifactHttpHandler.class);
  handlerBinder.addBinding().to(WorkflowStatsSLAHttpHandler.class);
  handlerBinder.addBinding().to(AuthorizationHandler.class);
  handlerBinder.addBinding().to(SecureStoreHandler.class);
  handlerBinder.addBinding().to(RemotePrivilegesHandler.class);
  for (  Class<? extends HttpHandler> handlerClass : handlerClasses) {
    handlerBinder.addBinding().to(handlerClass);
  }
}"
5892,"@Path(""String_Node_Str"") @GET public void getMetadata(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String namespace,@PathParam(""String_Node_Str"") String name) throws Exception {
  SecureKeyId secureKeyId=new SecureKeyId(namespace,name);
  SecureStoreData secureStoreData=secureStoreService.get(secureKeyId);
  httpResponder.sendJson(HttpResponseStatus.OK,secureStoreData.getMetadata());
}","@Path(""String_Node_Str"") @GET public void getMetadata(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String namespace,@PathParam(""String_Node_Str"") String name) throws Exception {
  SecureStoreData secureStoreData=secureStore.getSecureData(namespace,name);
  httpResponder.sendJson(HttpResponseStatus.OK,secureStoreData.getMetadata());
}"
5893,"@Path(""String_Node_Str"") @GET public void get(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String namespace,@PathParam(""String_Node_Str"") String name) throws Exception {
  SecureKeyId secureKeyId=new SecureKeyId(namespace,name);
  String data=new String(secureStoreService.get(secureKeyId).get(),StandardCharsets.UTF_8);
  httpResponder.sendString(HttpResponseStatus.OK,data);
}","@Path(""String_Node_Str"") @GET public void get(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String namespace,@PathParam(""String_Node_Str"") String name) throws Exception {
  SecureKeyId secureKeyId=new SecureKeyId(namespace,name);
  httpResponder.sendContent(HttpResponseStatus.OK,ChannelBuffers.wrappedBuffer(secureStore.getSecureData(namespace,name).get()),""String_Node_Str"",null);
}"
5894,"@Path(""String_Node_Str"") @PUT public void create(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String namespace,@PathParam(""String_Node_Str"") String name) throws Exception {
  SecureKeyId secureKeyId=new SecureKeyId(namespace,name);
  SecureKeyCreateRequest secureKeyCreateRequest=parseBody(httpRequest,SecureKeyCreateRequest.class);
  if (secureKeyCreateRequest == null) {
    SecureKeyCreateRequest dummy=new SecureKeyCreateRequest(""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"" + GSON.toJson(dummy));
  }
  secureStoreService.put(secureKeyId,secureKeyCreateRequest);
  httpResponder.sendStatus(HttpResponseStatus.OK);
}","@Path(""String_Node_Str"") @PUT public void create(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String namespace,@PathParam(""String_Node_Str"") String name) throws Exception {
  SecureKeyId secureKeyId=new SecureKeyId(namespace,name);
  SecureKeyCreateRequest secureKeyCreateRequest=parseBody(httpRequest,SecureKeyCreateRequest.class);
  if (secureKeyCreateRequest == null) {
    SecureKeyCreateRequest dummy=new SecureKeyCreateRequest(""String_Node_Str"",""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"" + GSON.toJson(dummy));
  }
  secureStoreManager.putSecureData(namespace,name,secureKeyCreateRequest.getData(),secureKeyCreateRequest.getDescription(),secureKeyCreateRequest.getProperties());
  httpResponder.sendStatus(HttpResponseStatus.OK);
}"
5895,"@Inject SecureStoreHandler(SecureStoreService secureStoreService){
  this.secureStoreService=secureStoreService;
}","@Inject SecureStoreHandler(SecureStore secureStore,SecureStoreManager secureStoreManager){
  this.secureStore=secureStore;
  this.secureStoreManager=secureStoreManager;
}"
5896,"@Path(""String_Node_Str"") @GET public void list(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String namespace) throws Exception {
  NamespaceId namespaceId=new NamespaceId(namespace);
  httpResponder.sendJson(HttpResponseStatus.OK,secureStoreService.list(namespaceId));
}","@Path(""String_Node_Str"") @GET public void list(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String namespace) throws Exception {
  httpResponder.sendJson(HttpResponseStatus.OK,secureStore.listSecureData(namespace));
}"
5897,"@Path(""String_Node_Str"") @DELETE public void delete(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String namespace,@PathParam(""String_Node_Str"") String name) throws Exception {
  SecureKeyId secureKeyId=new SecureKeyId(namespace,name);
  secureStoreService.delete(secureKeyId);
  httpResponder.sendStatus(HttpResponseStatus.OK);
}","@Path(""String_Node_Str"") @DELETE public void delete(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String namespace,@PathParam(""String_Node_Str"") String name) throws Exception {
  secureStoreManager.deleteSecureData(namespace,name);
  httpResponder.sendStatus(HttpResponseStatus.OK);
}"
5898,"@Override public List<SecureStoreMetadata> listSecureData(String namespace) throws Exception {
  return secureStore.listSecureData(namespace);
}","@Override public Map<String,String> listSecureData(String namespace) throws Exception {
  return secureStore.listSecureData(namespace);
}"
5899,"@Override public List<SecureStoreMetadata> listSecureData(String namespace) throws Exception {
  return delegate.listSecureData(namespace);
}","@Override public Map<String,String> listSecureData(String namespace) throws Exception {
  return delegate.listSecureData(namespace);
}"
5900,"@Path(""String_Node_Str"") @GET public void list(HttpServiceRequest request,HttpServiceResponder responder) throws Exception {
  String name=getContext().listSecureData(namespace).get(0).getName();
  responder.sendString(name);
}","@Path(""String_Node_Str"") @GET public void list(HttpServiceRequest request,HttpServiceResponder responder) throws Exception {
  String name=(String)getContext().listSecureData(namespace).keySet().toArray()[0];
  responder.sendString(name);
}"
5901,"@Override public List<SecureStoreMetadata> listSecureData(String namespace) throws Exception {
  return null;
}","@Override public Map<String,String> listSecureData(String namespace) throws Exception {
  return null;
}"
5902,"@Override public void initialize() throws Exception {
  SparkClientContext context=getContext();
  SparkConf sparkConf=new SparkConf();
  Map<String,String> programProperties=context.getSpecification().getProperties();
  String extraOpts=programProperties.get(""String_Node_Str"");
  if (extraOpts != null && !extraOpts.isEmpty()) {
    sparkConf.set(""String_Node_Str"",extraOpts);
    sparkConf.set(""String_Node_Str"",extraOpts);
  }
  Boolean isUnitTest=Boolean.valueOf(programProperties.get(""String_Node_Str""));
  if (isUnitTest) {
    Integer numSources=Integer.valueOf(programProperties.get(""String_Node_Str""));
    sparkConf.setMaster(String.format(""String_Node_Str"",numSources + 1));
    sparkConf.set(""String_Node_Str"",String.valueOf(numSources + 2));
  }
  context.setSparkConf(sparkConf);
}","@Override public void initialize() throws Exception {
  SparkClientContext context=getContext();
  SparkConf sparkConf=new SparkConf();
  Map<String,String> programProperties=context.getSpecification().getProperties();
  String extraOpts=programProperties.get(""String_Node_Str"");
  if (extraOpts != null && !extraOpts.isEmpty()) {
    sparkConf.set(""String_Node_Str"",extraOpts);
    sparkConf.set(""String_Node_Str"",extraOpts);
  }
  Integer numSources=Integer.valueOf(programProperties.get(""String_Node_Str""));
  sparkConf.set(""String_Node_Str"",String.valueOf(numSources + 2));
  Boolean isUnitTest=Boolean.valueOf(programProperties.get(""String_Node_Str""));
  if (isUnitTest) {
    sparkConf.setMaster(String.format(""String_Node_Str"",numSources + 1));
  }
  context.setSparkConf(sparkConf);
}"
5903,"@Override protected SparkCollection<Object> getSource(String stageName,PluginFunctionContext pluginFunctionContext) throws Exception {
  StreamingSource<Object> source=sec.getPluginContext().newPluginInstance(stageName);
  StreamingContext context=new DefaultStreamingContext(stageName,sec,streamingContext);
  return new DStreamCollection<>(sec,sparkContext,source.getStream(context));
}","@Override protected SparkCollection<Object> getSource(final String stageName,PluginFunctionContext pluginFunctionContext) throws Exception {
  StreamingSource<Object> source=sec.getPluginContext().newPluginInstance(stageName);
  StreamingContext context=new DefaultStreamingContext(stageName,sec,streamingContext);
  return new DStreamCollection<>(sec,sparkContext,source.getStream(context).transform(new Function<JavaRDD<Object>,JavaRDD<Object>>(){
    @Override public JavaRDD<Object> call(    JavaRDD<Object> input) throws Exception {
      return input.map(new CountingFunction<>(stageName,sec.getMetrics(),""String_Node_Str""));
    }
  }
));
}"
5904,"@Test public void testTransformCompute() throws Exception {
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  List<StructuredRecord> input=new ArrayList<>();
  StructuredRecord samuelRecord=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord jacksonRecord=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord dwayneRecord=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord johnsonRecord=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  input.add(samuelRecord);
  input.add(jacksonRecord);
  input.add(dwayneRecord);
  input.add(johnsonRecord);
  DataStreamsConfig etlConfig=DataStreamsConfig.builder().addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(schema,input))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",StringValueFilterTransform.getPlugin(""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",StringValueFilterCompute.getPlugin(""String_Node_Str"",""String_Node_Str""))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").setBatchInterval(""String_Node_Str"").build();
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  AppRequest<DataStreamsConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationManager appManager=deployApplication(appId,appRequest);
  SparkManager sparkManager=appManager.getSparkManager(DataStreamsSparkLauncher.NAME);
  sparkManager.start();
  sparkManager.waitForStatus(true,10,1);
  final DataSetManager<Table> outputManager=getDataset(""String_Node_Str"");
  final Set<StructuredRecord> expected=new HashSet<>();
  expected.add(samuelRecord);
  expected.add(johnsonRecord);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      outputManager.flush();
      Set<StructuredRecord> outputRecords=new HashSet<>();
      outputRecords.addAll(MockSink.readOutput(outputManager));
      return expected.equals(outputRecords);
    }
  }
,1,TimeUnit.MINUTES);
  sparkManager.stop();
  sparkManager.waitForStatus(false,10,1);
}","@Test public void testTransformCompute() throws Exception {
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  List<StructuredRecord> input=new ArrayList<>();
  StructuredRecord samuelRecord=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord jacksonRecord=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord dwayneRecord=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord johnsonRecord=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  input.add(samuelRecord);
  input.add(jacksonRecord);
  input.add(dwayneRecord);
  input.add(johnsonRecord);
  DataStreamsConfig etlConfig=DataStreamsConfig.builder().addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(schema,input))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",StringValueFilterTransform.getPlugin(""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",StringValueFilterCompute.getPlugin(""String_Node_Str"",""String_Node_Str""))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").setBatchInterval(""String_Node_Str"").build();
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  AppRequest<DataStreamsConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationManager appManager=deployApplication(appId,appRequest);
  SparkManager sparkManager=appManager.getSparkManager(DataStreamsSparkLauncher.NAME);
  sparkManager.start();
  sparkManager.waitForStatus(true,10,1);
  final DataSetManager<Table> outputManager=getDataset(""String_Node_Str"");
  final Set<StructuredRecord> expected=new HashSet<>();
  expected.add(samuelRecord);
  expected.add(johnsonRecord);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      outputManager.flush();
      Set<StructuredRecord> outputRecords=new HashSet<>();
      outputRecords.addAll(MockSink.readOutput(outputManager));
      return expected.equals(outputRecords);
    }
  }
,1,TimeUnit.MINUTES);
  sparkManager.stop();
  sparkManager.waitForStatus(false,10,1);
  validateMetric(appId,""String_Node_Str"",4);
  validateMetric(appId,""String_Node_Str"",4);
  validateMetric(appId,""String_Node_Str"",3);
  validateMetric(appId,""String_Node_Str"",3);
  validateMetric(appId,""String_Node_Str"",2);
  validateMetric(appId,""String_Node_Str"",2);
}"
5905,"@Test public void testJoin() throws Exception {
  Schema inputSchema1=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  Schema inputSchema2=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  Schema inputSchema3=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  Schema outSchema1=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  Schema outSchema2=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.LONG))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))));
  StructuredRecord recordSamuel=StructuredRecord.builder(inputSchema1).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordBob=StructuredRecord.builder(inputSchema1).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordJane=StructuredRecord.builder(inputSchema1).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordCar=StructuredRecord.builder(inputSchema2).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",10000L).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordBike=StructuredRecord.builder(inputSchema2).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",100L).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordTrasCar=StructuredRecord.builder(inputSchema3).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordTrasBike=StructuredRecord.builder(inputSchema3).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordTrasPlane=StructuredRecord.builder(inputSchema3).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  List<StructuredRecord> input1=ImmutableList.of(recordSamuel,recordBob,recordJane);
  List<StructuredRecord> input2=ImmutableList.of(recordCar,recordBike);
  List<StructuredRecord> input3=ImmutableList.of(recordTrasCar,recordTrasBike,recordTrasPlane);
  String outputName=""String_Node_Str"";
  DataStreamsConfig etlConfig=DataStreamsConfig.builder().addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputSchema1,input1))).addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputSchema2,input2))).addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputSchema3,input3))).addStage(new ETLStage(""String_Node_Str"",FieldsPrefixTransform.getPlugin(""String_Node_Str"",inputSchema1.toString()))).addStage(new ETLStage(""String_Node_Str"",FieldsPrefixTransform.getPlugin(""String_Node_Str"",inputSchema2.toString()))).addStage(new ETLStage(""String_Node_Str"",FieldsPrefixTransform.getPlugin(""String_Node_Str"",inputSchema3.toString()))).addStage(new ETLStage(""String_Node_Str"",FieldsPrefixTransform.getPlugin(""String_Node_Str"",outSchema1.toString()))).addStage(new ETLStage(""String_Node_Str"",MockJoiner.getPlugin(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockJoiner.getPlugin(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(outputName))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").setBatchInterval(""String_Node_Str"").build();
  AppRequest<DataStreamsConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  SparkManager sparkManager=appManager.getSparkManager(DataStreamsSparkLauncher.NAME);
  sparkManager.start();
  sparkManager.waitForStatus(true,10,1);
  StructuredRecord joinRecordSamuel=StructuredRecord.builder(outSchema2).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",10000L).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord joinRecordJane=StructuredRecord.builder(outSchema2).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",100L).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord joinRecordPlane=StructuredRecord.builder(outSchema2).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  final Set<StructuredRecord> expected=ImmutableSet.of(joinRecordSamuel,joinRecordJane,joinRecordPlane);
  final DataSetManager<Table> outputManager=getDataset(outputName);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      outputManager.flush();
      Set<StructuredRecord> outputRecords=new HashSet<>();
      outputRecords.addAll(MockSink.readOutput(outputManager));
      return expected.equals(outputRecords);
    }
  }
,4,TimeUnit.MINUTES);
  sparkManager.stop();
  sparkManager.waitForStatus(false,10,1);
}","@Test public void testJoin() throws Exception {
  Schema inputSchema1=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  Schema inputSchema2=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  Schema inputSchema3=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  Schema outSchema1=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  Schema outSchema2=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.LONG))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))),Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.STRING))));
  StructuredRecord recordSamuel=StructuredRecord.builder(inputSchema1).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordBob=StructuredRecord.builder(inputSchema1).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordJane=StructuredRecord.builder(inputSchema1).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordCar=StructuredRecord.builder(inputSchema2).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",10000L).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordBike=StructuredRecord.builder(inputSchema2).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",100L).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordTrasCar=StructuredRecord.builder(inputSchema3).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordTrasBike=StructuredRecord.builder(inputSchema3).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordTrasPlane=StructuredRecord.builder(inputSchema3).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  List<StructuredRecord> input1=ImmutableList.of(recordSamuel,recordBob,recordJane);
  List<StructuredRecord> input2=ImmutableList.of(recordCar,recordBike);
  List<StructuredRecord> input3=ImmutableList.of(recordTrasCar,recordTrasBike,recordTrasPlane);
  String outputName=""String_Node_Str"";
  DataStreamsConfig etlConfig=DataStreamsConfig.builder().addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputSchema1,input1))).addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputSchema2,input2))).addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputSchema3,input3))).addStage(new ETLStage(""String_Node_Str"",FieldsPrefixTransform.getPlugin(""String_Node_Str"",inputSchema1.toString()))).addStage(new ETLStage(""String_Node_Str"",FieldsPrefixTransform.getPlugin(""String_Node_Str"",inputSchema2.toString()))).addStage(new ETLStage(""String_Node_Str"",FieldsPrefixTransform.getPlugin(""String_Node_Str"",inputSchema3.toString()))).addStage(new ETLStage(""String_Node_Str"",FieldsPrefixTransform.getPlugin(""String_Node_Str"",outSchema1.toString()))).addStage(new ETLStage(""String_Node_Str"",MockJoiner.getPlugin(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockJoiner.getPlugin(""String_Node_Str"",""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(outputName))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").setBatchInterval(""String_Node_Str"").build();
  AppRequest<DataStreamsConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  SparkManager sparkManager=appManager.getSparkManager(DataStreamsSparkLauncher.NAME);
  sparkManager.start();
  sparkManager.waitForStatus(true,10,1);
  StructuredRecord joinRecordSamuel=StructuredRecord.builder(outSchema2).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",10000L).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord joinRecordJane=StructuredRecord.builder(outSchema2).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",100L).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord joinRecordPlane=StructuredRecord.builder(outSchema2).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",""String_Node_Str"").build();
  final Set<StructuredRecord> expected=ImmutableSet.of(joinRecordSamuel,joinRecordJane,joinRecordPlane);
  final DataSetManager<Table> outputManager=getDataset(outputName);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      outputManager.flush();
      Set<StructuredRecord> outputRecords=new HashSet<>();
      outputRecords.addAll(MockSink.readOutput(outputManager));
      return expected.equals(outputRecords);
    }
  }
,4,TimeUnit.MINUTES);
  sparkManager.stop();
  sparkManager.waitForStatus(false,10,1);
  validateMetric(appId,""String_Node_Str"",3);
  validateMetric(appId,""String_Node_Str"",2);
  validateMetric(appId,""String_Node_Str"",3);
  validateMetric(appId,""String_Node_Str"",3);
  validateMetric(appId,""String_Node_Str"",3);
  validateMetric(appId,""String_Node_Str"",2);
  validateMetric(appId,""String_Node_Str"",2);
  validateMetric(appId,""String_Node_Str"",3);
  validateMetric(appId,""String_Node_Str"",3);
  validateMetric(appId,""String_Node_Str"",2);
  validateMetric(appId,""String_Node_Str"",2);
  validateMetric(appId,""String_Node_Str"",5);
  validateMetric(appId,""String_Node_Str"",2);
  validateMetric(appId,""String_Node_Str"",5);
  validateMetric(appId,""String_Node_Str"",3);
  validateMetric(appId,""String_Node_Str"",3);
}"
5906,"@Test public void testParallelAggregators() throws Exception {
  String sink1Name=""String_Node_Str"";
  String sink2Name=""String_Node_Str"";
  Schema inputSchema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)));
  List<StructuredRecord> input1=ImmutableList.of(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build(),StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build());
  List<StructuredRecord> input2=ImmutableList.of(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",3L).build(),StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",4L).build(),StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",3L).build());
  DataStreamsConfig pipelineConfig=DataStreamsConfig.builder().setBatchInterval(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputSchema,input1))).addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputSchema,input2))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(sink1Name))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(sink2Name))).addStage(new ETLStage(""String_Node_Str"",FieldCountAggregator.getPlugin(""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",FieldCountAggregator.getPlugin(""String_Node_Str"",""String_Node_Str""))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").build();
  AppRequest<DataStreamsConfig> appRequest=new AppRequest<>(APP_ARTIFACT,pipelineConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  SparkManager sparkManager=appManager.getSparkManager(DataStreamsSparkLauncher.NAME);
  sparkManager.start();
  sparkManager.waitForStatus(true,10,1);
  Schema outputSchema1=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)));
  Schema outputSchema2=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)));
  final DataSetManager<Table> sinkManager1=getDataset(sink1Name);
  final Set<StructuredRecord> expected1=ImmutableSet.of(StructuredRecord.builder(outputSchema1).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",5L).build(),StructuredRecord.builder(outputSchema1).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",3L).build(),StructuredRecord.builder(outputSchema1).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build());
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      sinkManager1.flush();
      Set<StructuredRecord> outputRecords=new HashSet<>();
      outputRecords.addAll(MockSink.readOutput(sinkManager1));
      return expected1.equals(outputRecords);
    }
  }
,1,TimeUnit.MINUTES);
  final DataSetManager<Table> sinkManager2=getDataset(sink2Name);
  final Set<StructuredRecord> expected2=ImmutableSet.of(StructuredRecord.builder(outputSchema2).set(""String_Node_Str"",0L).set(""String_Node_Str"",5L).build(),StructuredRecord.builder(outputSchema2).set(""String_Node_Str"",1L).set(""String_Node_Str"",1L).build(),StructuredRecord.builder(outputSchema2).set(""String_Node_Str"",2L).set(""String_Node_Str"",1L).build(),StructuredRecord.builder(outputSchema2).set(""String_Node_Str"",3L).set(""String_Node_Str"",2L).build(),StructuredRecord.builder(outputSchema2).set(""String_Node_Str"",4L).set(""String_Node_Str"",1L).build());
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      sinkManager2.flush();
      Set<StructuredRecord> outputRecords=new HashSet<>();
      outputRecords.addAll(MockSink.readOutput(sinkManager2));
      return expected2.equals(outputRecords);
    }
  }
,1,TimeUnit.MINUTES);
  sparkManager.stop();
  sparkManager.waitForStatus(false,10,1);
}","@Test public void testParallelAggregators() throws Exception {
  String sink1Name=""String_Node_Str"";
  String sink2Name=""String_Node_Str"";
  Schema inputSchema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)));
  List<StructuredRecord> input1=ImmutableList.of(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build(),StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build());
  List<StructuredRecord> input2=ImmutableList.of(StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",3L).build(),StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",4L).build(),StructuredRecord.builder(inputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",3L).build());
  DataStreamsConfig pipelineConfig=DataStreamsConfig.builder().setBatchInterval(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputSchema,input1))).addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(inputSchema,input2))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(sink1Name))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(sink2Name))).addStage(new ETLStage(""String_Node_Str"",FieldCountAggregator.getPlugin(""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",FieldCountAggregator.getPlugin(""String_Node_Str"",""String_Node_Str""))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").build();
  AppRequest<DataStreamsConfig> appRequest=new AppRequest<>(APP_ARTIFACT,pipelineConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  SparkManager sparkManager=appManager.getSparkManager(DataStreamsSparkLauncher.NAME);
  sparkManager.start();
  sparkManager.waitForStatus(true,10,1);
  Schema outputSchema1=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)));
  Schema outputSchema2=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)));
  final DataSetManager<Table> sinkManager1=getDataset(sink1Name);
  final Set<StructuredRecord> expected1=ImmutableSet.of(StructuredRecord.builder(outputSchema1).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",5L).build(),StructuredRecord.builder(outputSchema1).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",3L).build(),StructuredRecord.builder(outputSchema1).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build());
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      sinkManager1.flush();
      Set<StructuredRecord> outputRecords=new HashSet<>();
      outputRecords.addAll(MockSink.readOutput(sinkManager1));
      return expected1.equals(outputRecords);
    }
  }
,1,TimeUnit.MINUTES);
  final DataSetManager<Table> sinkManager2=getDataset(sink2Name);
  final Set<StructuredRecord> expected2=ImmutableSet.of(StructuredRecord.builder(outputSchema2).set(""String_Node_Str"",0L).set(""String_Node_Str"",5L).build(),StructuredRecord.builder(outputSchema2).set(""String_Node_Str"",1L).set(""String_Node_Str"",1L).build(),StructuredRecord.builder(outputSchema2).set(""String_Node_Str"",2L).set(""String_Node_Str"",1L).build(),StructuredRecord.builder(outputSchema2).set(""String_Node_Str"",3L).set(""String_Node_Str"",2L).build(),StructuredRecord.builder(outputSchema2).set(""String_Node_Str"",4L).set(""String_Node_Str"",1L).build());
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      sinkManager2.flush();
      Set<StructuredRecord> outputRecords=new HashSet<>();
      outputRecords.addAll(MockSink.readOutput(sinkManager2));
      return expected2.equals(outputRecords);
    }
  }
,1,TimeUnit.MINUTES);
  sparkManager.stop();
  sparkManager.waitForStatus(false,10,1);
  validateMetric(appId,""String_Node_Str"",2);
  validateMetric(appId,""String_Node_Str"",3);
  validateMetric(appId,""String_Node_Str"",5);
  validateMetric(appId,""String_Node_Str"",3);
  validateMetric(appId,""String_Node_Str"",5);
  validateMetric(appId,""String_Node_Str"",5);
  validateMetric(appId,""String_Node_Str"",3);
  validateMetric(appId,""String_Node_Str"",5);
}"
5907,SparkCollection<T> window(Windower windower);,"SparkCollection<T> window(String stageName,Windower windower);"
5908,"public void runPipeline(PipelinePhase pipelinePhase,String sourcePluginType,JavaSparkExecutionContext sec,Map<String,Integer> stagePartitions) throws Exception {
  MacroEvaluator macroEvaluator=new DefaultMacroEvaluator(sec.getWorkflowToken(),sec.getRuntimeArguments(),sec.getLogicalStartTime(),sec,sec.getNamespace());
  Map<String,SparkCollection<Object>> stageDataCollections=new HashMap<>();
  if (pipelinePhase.getDag() == null) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  for (  String stageName : pipelinePhase.getDag().getTopologicalOrder()) {
    StageInfo stageInfo=pipelinePhase.getStage(stageName);
    String pluginType=stageInfo.getPluginType();
    SparkCollection<Object> stageData=null;
    Map<String,SparkCollection<Object>> inputDataCollections=new HashMap<>();
    for (    String inputStageName : stageInfo.getInputs()) {
      inputDataCollections.put(inputStageName,stageDataCollections.get(inputStageName));
    }
    if (!inputDataCollections.isEmpty()) {
      Iterator<SparkCollection<Object>> inputCollectionIter=inputDataCollections.values().iterator();
      stageData=inputCollectionIter.next();
      while (!BatchJoiner.PLUGIN_TYPE.equals(pluginType) && inputCollectionIter.hasNext()) {
        stageData=stageData.union(inputCollectionIter.next());
      }
    }
    PluginFunctionContext pluginFunctionContext=new PluginFunctionContext(stageName,sec,pipelinePhase);
    if (stageData == null) {
      if (sourcePluginType.equals(pluginType)) {
        stageData=getSource(stageName,pluginFunctionContext);
      }
 else {
        throw new IllegalStateException(String.format(""String_Node_Str"",stageName));
      }
    }
 else     if (BatchSink.PLUGIN_TYPE.equals(pluginType)) {
      stageData.store(stageName,new BatchSinkFunction(pluginFunctionContext));
    }
 else     if (Transform.PLUGIN_TYPE.equals(pluginType)) {
      stageData=stageData.flatMap(new TransformFunction(pluginFunctionContext));
    }
 else     if (SparkCompute.PLUGIN_TYPE.equals(pluginType)) {
      SparkCompute<Object,Object> sparkCompute=sec.getPluginContext().newPluginInstance(stageName,macroEvaluator);
      stageData=stageData.compute(stageName,sparkCompute);
    }
 else     if (SparkSink.PLUGIN_TYPE.equals(pluginType)) {
      SparkSink<Object> sparkSink=sec.getPluginContext().newPluginInstance(stageName,macroEvaluator);
      stageData.store(stageName,sparkSink);
    }
 else     if (BatchAggregator.PLUGIN_TYPE.equals(pluginType)) {
      PairFlatMapFunction<Object,Object,Object> groupByFunction=new AggregatorGroupByFunction(pluginFunctionContext);
      FlatMapFunction<Tuple2<Object,Iterable<Object>>,Object> aggregateFunction=new AggregatorAggregateFunction(pluginFunctionContext);
      Integer partitions=stagePartitions.get(stageName);
      SparkPairCollection<Object,Object> keyedCollection=stageData.flatMapToPair(groupByFunction);
      SparkPairCollection<Object,Iterable<Object>> groupedCollection=partitions == null ? keyedCollection.groupByKey() : keyedCollection.groupByKey(partitions);
      stageData=groupedCollection.flatMap(aggregateFunction);
    }
 else     if (BatchJoiner.PLUGIN_TYPE.equals(pluginType)) {
      BatchJoiner<Object,Object,Object> joiner=sec.getPluginContext().newPluginInstance(stageName,macroEvaluator);
      BatchJoinerRuntimeContext joinerRuntimeContext=pluginFunctionContext.createJoinerRuntimeContext();
      joiner.initialize(joinerRuntimeContext);
      Map<String,SparkPairCollection<Object,Object>> preJoinStreams=new HashMap<>();
      for (      Map.Entry<String,SparkCollection<Object>> inputStreamEntry : inputDataCollections.entrySet()) {
        String inputStage=inputStreamEntry.getKey();
        SparkCollection<Object> inputStream=inputStreamEntry.getValue();
        preJoinStreams.put(inputStage,inputStream.flatMapToPair(new JoinOnFunction(pluginFunctionContext,inputStage)));
      }
      Set<String> remainingInputs=new HashSet<>();
      remainingInputs.addAll(inputDataCollections.keySet());
      Integer numPartitions=stagePartitions.get(stageName);
      SparkPairCollection<Object,List<JoinElement<Object>>> joinedInputs=null;
      for (      final String inputStageName : joiner.getJoinConfig().getRequiredInputs()) {
        SparkPairCollection<Object,Object> preJoinCollection=preJoinStreams.get(inputStageName);
        if (joinedInputs == null) {
          joinedInputs=preJoinCollection.mapValues(new InitialJoinFunction(inputStageName));
        }
 else {
          JoinFlattenFunction joinFlattenFunction=new JoinFlattenFunction(inputStageName);
          joinedInputs=numPartitions == null ? joinedInputs.join(preJoinCollection).mapValues(joinFlattenFunction) : joinedInputs.join(preJoinCollection,numPartitions).mapValues(joinFlattenFunction);
        }
        remainingInputs.remove(inputStageName);
      }
      boolean isFullOuter=joinedInputs == null;
      for (      final String inputStageName : remainingInputs) {
        SparkPairCollection<Object,Object> preJoinStream=preJoinStreams.get(inputStageName);
        if (joinedInputs == null) {
          joinedInputs=preJoinStream.mapValues(new InitialJoinFunction(inputStageName));
        }
 else {
          if (isFullOuter) {
            OuterJoinFlattenFunction outerJoinFlattenFunction=new OuterJoinFlattenFunction(inputStageName);
            joinedInputs=numPartitions == null ? joinedInputs.fullOuterJoin(preJoinStream).mapValues(outerJoinFlattenFunction) : joinedInputs.fullOuterJoin(preJoinStream,numPartitions).mapValues(outerJoinFlattenFunction);
          }
 else {
            LeftJoinFlattenFunction joinFlattenFunction=new LeftJoinFlattenFunction(inputStageName);
            joinedInputs=numPartitions == null ? joinedInputs.leftOuterJoin(preJoinStream).mapValues(joinFlattenFunction) : joinedInputs.leftOuterJoin(preJoinStream,numPartitions).mapValues(joinFlattenFunction);
          }
        }
      }
      if (joinedInputs == null) {
        throw new IllegalStateException(""String_Node_Str"" + stageName);
      }
      stageData=joinedInputs.flatMap(new JoinMergeFunction(pluginFunctionContext)).cache();
    }
 else     if (Windower.PLUGIN_TYPE.equals(pluginType)) {
      Windower windower=sec.getPluginContext().newPluginInstance(stageName,macroEvaluator);
      stageData=stageData.window(windower);
    }
 else {
      throw new IllegalStateException(String.format(""String_Node_Str"",stageName,pluginType));
    }
    if (shouldCache(pipelinePhase,stageInfo)) {
      stageData=stageData.cache();
    }
    stageDataCollections.put(stageName,stageData);
  }
}","public void runPipeline(PipelinePhase pipelinePhase,String sourcePluginType,JavaSparkExecutionContext sec,Map<String,Integer> stagePartitions) throws Exception {
  MacroEvaluator macroEvaluator=new DefaultMacroEvaluator(sec.getWorkflowToken(),sec.getRuntimeArguments(),sec.getLogicalStartTime(),sec,sec.getNamespace());
  Map<String,SparkCollection<Object>> stageDataCollections=new HashMap<>();
  if (pipelinePhase.getDag() == null) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  for (  String stageName : pipelinePhase.getDag().getTopologicalOrder()) {
    StageInfo stageInfo=pipelinePhase.getStage(stageName);
    String pluginType=stageInfo.getPluginType();
    SparkCollection<Object> stageData=null;
    Map<String,SparkCollection<Object>> inputDataCollections=new HashMap<>();
    for (    String inputStageName : stageInfo.getInputs()) {
      inputDataCollections.put(inputStageName,stageDataCollections.get(inputStageName));
    }
    if (!inputDataCollections.isEmpty()) {
      Iterator<SparkCollection<Object>> inputCollectionIter=inputDataCollections.values().iterator();
      stageData=inputCollectionIter.next();
      while (!BatchJoiner.PLUGIN_TYPE.equals(pluginType) && inputCollectionIter.hasNext()) {
        stageData=stageData.union(inputCollectionIter.next());
      }
    }
    PluginFunctionContext pluginFunctionContext=new PluginFunctionContext(stageName,sec,pipelinePhase);
    if (stageData == null) {
      if (sourcePluginType.equals(pluginType)) {
        stageData=getSource(stageName,pluginFunctionContext);
      }
 else {
        throw new IllegalStateException(String.format(""String_Node_Str"",stageName));
      }
    }
 else     if (BatchSink.PLUGIN_TYPE.equals(pluginType)) {
      stageData.store(stageName,new BatchSinkFunction(pluginFunctionContext));
    }
 else     if (Transform.PLUGIN_TYPE.equals(pluginType)) {
      stageData=stageData.flatMap(new TransformFunction(pluginFunctionContext));
    }
 else     if (SparkCompute.PLUGIN_TYPE.equals(pluginType)) {
      SparkCompute<Object,Object> sparkCompute=sec.getPluginContext().newPluginInstance(stageName,macroEvaluator);
      stageData=stageData.compute(stageName,sparkCompute);
    }
 else     if (SparkSink.PLUGIN_TYPE.equals(pluginType)) {
      SparkSink<Object> sparkSink=sec.getPluginContext().newPluginInstance(stageName,macroEvaluator);
      stageData.store(stageName,sparkSink);
    }
 else     if (BatchAggregator.PLUGIN_TYPE.equals(pluginType)) {
      PairFlatMapFunction<Object,Object,Object> groupByFunction=new AggregatorGroupByFunction(pluginFunctionContext);
      FlatMapFunction<Tuple2<Object,Iterable<Object>>,Object> aggregateFunction=new AggregatorAggregateFunction(pluginFunctionContext);
      Integer partitions=stagePartitions.get(stageName);
      SparkPairCollection<Object,Object> keyedCollection=stageData.flatMapToPair(groupByFunction);
      SparkPairCollection<Object,Iterable<Object>> groupedCollection=partitions == null ? keyedCollection.groupByKey() : keyedCollection.groupByKey(partitions);
      stageData=groupedCollection.flatMap(aggregateFunction);
    }
 else     if (BatchJoiner.PLUGIN_TYPE.equals(pluginType)) {
      BatchJoiner<Object,Object,Object> joiner=sec.getPluginContext().newPluginInstance(stageName,macroEvaluator);
      BatchJoinerRuntimeContext joinerRuntimeContext=pluginFunctionContext.createJoinerRuntimeContext();
      joiner.initialize(joinerRuntimeContext);
      Map<String,SparkPairCollection<Object,Object>> preJoinStreams=new HashMap<>();
      for (      Map.Entry<String,SparkCollection<Object>> inputStreamEntry : inputDataCollections.entrySet()) {
        String inputStage=inputStreamEntry.getKey();
        SparkCollection<Object> inputStream=inputStreamEntry.getValue();
        preJoinStreams.put(inputStage,inputStream.flatMapToPair(new JoinOnFunction(pluginFunctionContext,inputStage)));
      }
      Set<String> remainingInputs=new HashSet<>();
      remainingInputs.addAll(inputDataCollections.keySet());
      Integer numPartitions=stagePartitions.get(stageName);
      SparkPairCollection<Object,List<JoinElement<Object>>> joinedInputs=null;
      for (      final String inputStageName : joiner.getJoinConfig().getRequiredInputs()) {
        SparkPairCollection<Object,Object> preJoinCollection=preJoinStreams.get(inputStageName);
        if (joinedInputs == null) {
          joinedInputs=preJoinCollection.mapValues(new InitialJoinFunction(inputStageName));
        }
 else {
          JoinFlattenFunction joinFlattenFunction=new JoinFlattenFunction(inputStageName);
          joinedInputs=numPartitions == null ? joinedInputs.join(preJoinCollection).mapValues(joinFlattenFunction) : joinedInputs.join(preJoinCollection,numPartitions).mapValues(joinFlattenFunction);
        }
        remainingInputs.remove(inputStageName);
      }
      boolean isFullOuter=joinedInputs == null;
      for (      final String inputStageName : remainingInputs) {
        SparkPairCollection<Object,Object> preJoinStream=preJoinStreams.get(inputStageName);
        if (joinedInputs == null) {
          joinedInputs=preJoinStream.mapValues(new InitialJoinFunction(inputStageName));
        }
 else {
          if (isFullOuter) {
            OuterJoinFlattenFunction outerJoinFlattenFunction=new OuterJoinFlattenFunction(inputStageName);
            joinedInputs=numPartitions == null ? joinedInputs.fullOuterJoin(preJoinStream).mapValues(outerJoinFlattenFunction) : joinedInputs.fullOuterJoin(preJoinStream,numPartitions).mapValues(outerJoinFlattenFunction);
          }
 else {
            LeftJoinFlattenFunction joinFlattenFunction=new LeftJoinFlattenFunction(inputStageName);
            joinedInputs=numPartitions == null ? joinedInputs.leftOuterJoin(preJoinStream).mapValues(joinFlattenFunction) : joinedInputs.leftOuterJoin(preJoinStream,numPartitions).mapValues(joinFlattenFunction);
          }
        }
      }
      if (joinedInputs == null) {
        throw new IllegalStateException(""String_Node_Str"" + stageName);
      }
      stageData=joinedInputs.flatMap(new JoinMergeFunction(pluginFunctionContext)).cache();
    }
 else     if (Windower.PLUGIN_TYPE.equals(pluginType)) {
      Windower windower=sec.getPluginContext().newPluginInstance(stageName,macroEvaluator);
      stageData=stageData.window(stageName,windower);
    }
 else {
      throw new IllegalStateException(String.format(""String_Node_Str"",stageName,pluginType));
    }
    if (shouldCache(pipelinePhase,stageInfo)) {
      stageData=stageData.cache();
    }
    stageDataCollections.put(stageName,stageData);
  }
}"
5909,"@Override public <U>SparkCollection<U> compute(String stageName,SparkCompute<T,U> compute) throws Exception {
  SparkExecutionPluginContext sparkPluginContext=new BasicSparkExecutionPluginContext(sec,jsc,datasetContext,stageName);
  compute.initialize(sparkPluginContext);
  long recordsIn=rdd.cache().count();
  StageMetrics stageMetrics=new DefaultStageMetrics(sec.getMetrics(),stageName);
  stageMetrics.gauge(""String_Node_Str"",recordsIn);
  JavaRDD<U> computedRDD=compute.transform(sparkPluginContext,rdd).cache();
  long recordsOut=computedRDD.count();
  stageMetrics.gauge(""String_Node_Str"",recordsOut);
  return wrap(computedRDD);
}","@Override public <U>SparkCollection<U> compute(String stageName,SparkCompute<T,U> compute) throws Exception {
  SparkExecutionPluginContext sparkPluginContext=new BasicSparkExecutionPluginContext(sec,jsc,datasetContext,stageName);
  compute.initialize(sparkPluginContext);
  JavaRDD<T> countedInput=rdd.map(new CountingFunction<T>(stageName,sec.getMetrics(),""String_Node_Str"")).cache();
  return wrap(compute.transform(sparkPluginContext,countedInput).map(new CountingFunction<U>(stageName,sec.getMetrics(),""String_Node_Str"")));
}"
5910,"@Override public void store(String stageName,SparkSink<T> sink) throws Exception {
  SparkExecutionPluginContext sparkPluginContext=new BasicSparkExecutionPluginContext(sec,jsc,datasetContext,stageName);
  long recordsIn=rdd.cache().count();
  StageMetrics stageMetrics=new DefaultStageMetrics(sec.getMetrics(),stageName);
  stageMetrics.gauge(""String_Node_Str"",recordsIn);
  sink.run(sparkPluginContext,rdd);
}","@Override public void store(String stageName,SparkSink<T> sink) throws Exception {
  SparkExecutionPluginContext sparkPluginContext=new BasicSparkExecutionPluginContext(sec,jsc,datasetContext,stageName);
  JavaRDD<T> countedRDD=rdd.map(new CountingFunction<T>(stageName,sec.getMetrics(),""String_Node_Str"")).cache();
  sink.run(sparkPluginContext,countedRDD);
}"
5911,"@Override public SparkCollection<T> window(Windower windower){
  throw new UnsupportedOperationException(""String_Node_Str"");
}","@Override public SparkCollection<T> window(String stageName,Windower windower){
  throw new UnsupportedOperationException(""String_Node_Str"");
}"
5912,"@Override public <U>SparkCollection<U> compute(final String stageName,final SparkCompute<T,U> compute) throws Exception {
  sec.execute(new TxRunnable(){
    @Override public void run(    DatasetContext datasetContext) throws Exception {
      SparkExecutionPluginContext sparkPluginContext=new BasicSparkExecutionPluginContext(sec,sparkContext,datasetContext,stageName);
      compute.initialize(sparkPluginContext);
    }
  }
);
  return wrap(stream.transform(new Function2<JavaRDD<T>,Time,JavaRDD<U>>(){
    @Override public JavaRDD<U> call(    JavaRDD<T> data,    Time batchTime) throws Exception {
      SparkExecutionPluginContext sparkPluginContext=new SparkStreamingExecutionContext(sec,sparkContext,stageName,batchTime.milliseconds());
      return compute.transform(sparkPluginContext,data);
    }
  }
));
}","@Override public <U>SparkCollection<U> compute(final String stageName,final SparkCompute<T,U> compute) throws Exception {
  sec.execute(new TxRunnable(){
    @Override public void run(    DatasetContext datasetContext) throws Exception {
      SparkExecutionPluginContext sparkPluginContext=new BasicSparkExecutionPluginContext(sec,sparkContext,datasetContext,stageName);
      compute.initialize(sparkPluginContext);
    }
  }
);
  return wrap(stream.transform(new Function2<JavaRDD<T>,Time,JavaRDD<U>>(){
    @Override public JavaRDD<U> call(    JavaRDD<T> data,    Time batchTime) throws Exception {
      SparkExecutionPluginContext sparkPluginContext=new SparkStreamingExecutionContext(sec,sparkContext,stageName,batchTime.milliseconds());
      data=data.map(new CountingFunction<T>(stageName,sec.getMetrics(),""String_Node_Str""));
      return compute.transform(sparkPluginContext,data).map(new CountingFunction<U>(stageName,sec.getMetrics(),""String_Node_Str""));
    }
  }
));
}"
5913,"@Override public Void call(JavaRDD<T> data,Time batchTime) throws Exception {
  final long logicalStartTime=batchTime.milliseconds();
  MacroEvaluator evaluator=new DefaultMacroEvaluator(sec.getWorkflowToken(),sec.getRuntimeArguments(),logicalStartTime,sec.getSecureStore(),sec.getNamespace());
  final SparkBatchSinkFactory sinkFactory=new SparkBatchSinkFactory();
  final BatchSink<Object,Object,Object> batchSink=sec.getPluginContext().newPluginInstance(stageName,evaluator);
  boolean isPrepared=false;
  boolean isDone=false;
  try {
    sec.execute(new TxRunnable(){
      @Override public void run(      DatasetContext datasetContext) throws Exception {
        SparkBatchSinkContext sinkContext=new SparkBatchSinkContext(sinkFactory,sec,datasetContext,stageName,logicalStartTime);
        batchSink.prepareRun(sinkContext);
      }
    }
);
    isPrepared=true;
    sinkFactory.writeFromRDD(data.flatMapToPair(sinkFunction),sec,stageName,Object.class,Object.class);
    isDone=true;
    sec.execute(new TxRunnable(){
      @Override public void run(      DatasetContext datasetContext) throws Exception {
        SparkBatchSinkContext sinkContext=new SparkBatchSinkContext(sinkFactory,sec,datasetContext,stageName,logicalStartTime);
        batchSink.onRunFinish(true,sinkContext);
      }
    }
);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",stageName,logicalStartTime,e);
  }
 finally {
    if (isPrepared && !isDone) {
      sec.execute(new TxRunnable(){
        @Override public void run(        DatasetContext datasetContext) throws Exception {
          SparkBatchSinkContext sinkContext=new SparkBatchSinkContext(sinkFactory,sec,datasetContext,stageName,logicalStartTime);
          batchSink.onRunFinish(false,sinkContext);
        }
      }
);
    }
  }
  return null;
}","@Override public JavaRDD<T> call(JavaRDD<T> in) throws Exception {
  return in.map(new CountingFunction<T>(stageName,sec.getMetrics(),""String_Node_Str""));
}"
5914,"@Override public SparkCollection<T> window(Windower windower){
  return wrap(stream.window(Durations.seconds(windower.getWidth()),Durations.seconds(windower.getSlideInterval())));
}","@Override public SparkCollection<T> window(final String stageName,Windower windower){
  return wrap(stream.transform(new Function<JavaRDD<T>,JavaRDD<T>>(){
    @Override public JavaRDD<T> call(    JavaRDD<T> in) throws Exception {
      return in.map(new CountingFunction<T>(stageName,sec.getMetrics(),""String_Node_Str""));
    }
  }
).window(Durations.seconds(windower.getWidth()),Durations.seconds(windower.getSlideInterval())).transform(new Function<JavaRDD<T>,JavaRDD<T>>(){
    @Override public JavaRDD<T> call(    JavaRDD<T> in) throws Exception {
      return in.map(new CountingFunction<T>(stageName,sec.getMetrics(),""String_Node_Str""));
    }
  }
));
}"
5915,"@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  Location deploymentJar=AppJarHelper.createDeploymentJar(new LocalLocationFactory(TMP_FOLDER.newFolder()),InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,deploymentJar.toURI().getPath());
  File systemArtifactsDir=TMP_FOLDER.newFolder();
  cConf.set(Constants.AppFabric.SYSTEM_ARTIFACTS_DIR,systemArtifactsDir.getAbsolutePath());
  createSystemArtifact(systemArtifactsDir);
  systemUser=new AuthenticationTestContext().getPrincipal().getName();
  cConf.set(Constants.Security.Authorization.SYSTEM_USER,systemUser);
  Injector injector=AppFabricTestHelper.getInjector(cConf);
  artifactRepository=injector.getInstance(ArtifactRepository.class);
  AuthorizerInstantiator instantiatorService=injector.getInstance(AuthorizerInstantiator.class);
  authorizer=instantiatorService.get();
  authEnforcerService=injector.getInstance(AuthorizationEnforcementService.class);
  authEnforcerService.startAndWait();
  instance=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
}","@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  Location deploymentJar=AppJarHelper.createDeploymentJar(new LocalLocationFactory(TMP_FOLDER.newFolder()),InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,deploymentJar.toURI().getPath());
  File systemArtifactsDir=TMP_FOLDER.newFolder();
  cConf.set(Constants.AppFabric.SYSTEM_ARTIFACTS_DIR,systemArtifactsDir.getAbsolutePath());
  createSystemArtifact(systemArtifactsDir);
  Injector injector=AppFabricTestHelper.getInjector(cConf);
  artifactRepository=injector.getInstance(ArtifactRepository.class);
  AuthorizerInstantiator instantiatorService=injector.getInstance(AuthorizerInstantiator.class);
  authorizer=instantiatorService.get();
  authEnforcerService=injector.getInstance(AuthorizationEnforcementService.class);
  authEnforcerService.startAndWait();
  instance=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
}"
5916,"@Test public void testAuthorizationForSystemArtifacts() throws Exception {
  SecurityRequestContext.setUserId(systemUser);
  artifactRepository.addSystemArtifacts();
  SecurityRequestContext.setUserId(ALICE.getName());
  try {
    artifactRepository.addSystemArtifacts();
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  authorizer.grant(NamespaceId.SYSTEM,ALICE,Collections.singleton(Action.WRITE));
  Assert.assertEquals(Collections.singleton(new Privilege(NamespaceId.SYSTEM,Action.WRITE)),authorizer.listPrivileges(ALICE));
  artifactRepository.addSystemArtifacts();
  SecurityRequestContext.setUserId(""String_Node_Str"");
  try {
    artifactRepository.deleteArtifact(SYSTEM_ARTIFACT.toId());
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  SecurityRequestContext.setUserId(ALICE.getName());
  authorizer.grant(instance,ALICE,Collections.singleton(Action.ADMIN));
  NamespaceId namespaceId=new NamespaceId(""String_Node_Str"");
  namespaceAdmin.create(new NamespaceMeta.Builder().setName(namespaceId.getNamespace()).build());
  authorizer.revoke(instance);
  List<ArtifactSummary> artifacts=artifactRepository.getArtifacts(namespaceId,true);
  Assert.assertEquals(1,artifacts.size());
  ArtifactSummary artifactSummary=artifacts.get(0);
  Assert.assertEquals(SYSTEM_ARTIFACT.getArtifact(),artifactSummary.getName());
  Assert.assertEquals(SYSTEM_ARTIFACT.getVersion(),artifactSummary.getVersion());
  Assert.assertEquals(SYSTEM_ARTIFACT.getNamespace(),artifactSummary.getScope().name().toLowerCase());
  ArtifactDetail artifactDetail=artifactRepository.getArtifact(SYSTEM_ARTIFACT.toId());
  co.cask.cdap.api.artifact.ArtifactId artifactId=artifactDetail.getDescriptor().getArtifactId();
  Assert.assertEquals(SYSTEM_ARTIFACT.getArtifact(),artifactId.getName());
  Assert.assertEquals(SYSTEM_ARTIFACT.getVersion(),artifactId.getVersion().getVersion());
  Assert.assertEquals(SYSTEM_ARTIFACT.getNamespace(),artifactId.getScope().name().toLowerCase());
  namespaceAdmin.delete(namespaceId.toId());
  authorizer.enforce(SYSTEM_ARTIFACT,ALICE,EnumSet.allOf(Action.class));
  authorizer.enforce(NamespaceId.SYSTEM,ALICE,Action.WRITE);
  artifactRepository.deleteArtifact(SYSTEM_ARTIFACT.toId());
}","@Test public void testAuthorizationForSystemArtifacts() throws Exception {
  artifactRepository.addSystemArtifacts();
  SecurityRequestContext.setUserId(ALICE.getName());
  try {
    artifactRepository.addSystemArtifacts();
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  authorizer.grant(NamespaceId.SYSTEM,ALICE,Collections.singleton(Action.WRITE));
  Assert.assertEquals(Collections.singleton(new Privilege(NamespaceId.SYSTEM,Action.WRITE)),authorizer.listPrivileges(ALICE));
  artifactRepository.addSystemArtifacts();
  SecurityRequestContext.setUserId(""String_Node_Str"");
  try {
    artifactRepository.deleteArtifact(SYSTEM_ARTIFACT.toId());
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  SecurityRequestContext.setUserId(ALICE.getName());
  authorizer.grant(instance,ALICE,Collections.singleton(Action.ADMIN));
  NamespaceId namespaceId=new NamespaceId(""String_Node_Str"");
  namespaceAdmin.create(new NamespaceMeta.Builder().setName(namespaceId.getNamespace()).build());
  authorizer.revoke(instance);
  List<ArtifactSummary> artifacts=artifactRepository.getArtifacts(namespaceId,true);
  Assert.assertEquals(1,artifacts.size());
  ArtifactSummary artifactSummary=artifacts.get(0);
  Assert.assertEquals(SYSTEM_ARTIFACT.getArtifact(),artifactSummary.getName());
  Assert.assertEquals(SYSTEM_ARTIFACT.getVersion(),artifactSummary.getVersion());
  Assert.assertEquals(SYSTEM_ARTIFACT.getNamespace(),artifactSummary.getScope().name().toLowerCase());
  ArtifactDetail artifactDetail=artifactRepository.getArtifact(SYSTEM_ARTIFACT.toId());
  co.cask.cdap.api.artifact.ArtifactId artifactId=artifactDetail.getDescriptor().getArtifactId();
  Assert.assertEquals(SYSTEM_ARTIFACT.getArtifact(),artifactId.getName());
  Assert.assertEquals(SYSTEM_ARTIFACT.getVersion(),artifactId.getVersion().getVersion());
  Assert.assertEquals(SYSTEM_ARTIFACT.getNamespace(),artifactId.getScope().name().toLowerCase());
  namespaceAdmin.delete(namespaceId.toId());
  authorizer.enforce(SYSTEM_ARTIFACT,ALICE,EnumSet.allOf(Action.class));
  authorizer.enforce(NamespaceId.SYSTEM,ALICE,Action.WRITE);
  artifactRepository.deleteArtifact(SYSTEM_ARTIFACT.toId());
}"
5917,"private static CConfiguration createCConf() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TEMPORARY_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  cConf.set(Constants.Security.Authorization.SYSTEM_USER,new MasterAuthenticationContext().getPrincipal().getName());
  LocationFactory locationFactory=new LocalLocationFactory(TEMPORARY_FOLDER.newFolder());
  Location authorizerJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,authorizerJar.toURI().getPath());
  cConf.set(Constants.Security.Store.PROVIDER,""String_Node_Str"");
  return cConf;
}","private static CConfiguration createCConf() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TEMPORARY_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  LocationFactory locationFactory=new LocalLocationFactory(TEMPORARY_FOLDER.newFolder());
  Location authorizerJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,authorizerJar.toURI().getPath());
  cConf.set(Constants.Security.Store.PROVIDER,""String_Node_Str"");
  return cConf;
}"
5918,"@Test public void test() throws Exception {
  authorizationBootstrapper.run();
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      Predicate<EntityId> systemUserFilter=authorizationEnforcementService.createFilter(systemUser);
      return systemUserFilter.apply(instanceId) && systemUserFilter.apply(NamespaceId.SYSTEM);
    }
  }
,10,TimeUnit.SECONDS);
  txManager.startAndWait();
  datasetService.startAndWait();
  waitForService(Constants.Service.DATASET_MANAGER);
  defaultNamespaceEnsurer.startAndWait();
  systemArtifactLoader.startAndWait();
  waitForService(defaultNamespaceEnsurer);
  waitForService(systemArtifactLoader);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      try {
        return namespaceQueryAdmin.exists(NamespaceId.DEFAULT.toId());
      }
 catch (      Exception e) {
        return false;
      }
    }
  }
,10,TimeUnit.SECONDS);
  Assert.assertTrue(defaultNamespaceEnsurer.isRunning());
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      try {
        artifactRepository.getArtifact(SYSTEM_ARTIFACT.toId());
        return true;
      }
 catch (      Exception e) {
        return false;
      }
    }
  }
,20,TimeUnit.SECONDS);
  Assert.assertTrue(systemArtifactLoader.isRunning());
  Dataset systemDataset=DatasetsUtil.getOrCreateDataset(dsFramework,NamespaceId.SYSTEM.dataset(""String_Node_Str"").toId(),Table.class.getName(),DatasetProperties.EMPTY,Collections.<String,String>emptyMap(),this.getClass().getClassLoader());
  Assert.assertNotNull(systemDataset);
  SecurityRequestContext.setUserId(""String_Node_Str"");
  namespaceAdmin.create(new NamespaceMeta.Builder().setName(""String_Node_Str"").build());
  SecurityRequestContext.setUserId(""String_Node_Str"");
  try {
    namespaceAdmin.create(new NamespaceMeta.Builder().setName(""String_Node_Str"").build());
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
}","@Test public void test() throws Exception {
  authorizationBootstrapper.run();
  final Principal systemUser=new Principal(UserGroupInformation.getCurrentUser().getShortUserName(),Principal.PrincipalType.USER);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      Predicate<EntityId> systemUserFilter=authorizationEnforcementService.createFilter(systemUser);
      return systemUserFilter.apply(instanceId) && systemUserFilter.apply(NamespaceId.SYSTEM);
    }
  }
,10,TimeUnit.SECONDS);
  txManager.startAndWait();
  datasetService.startAndWait();
  waitForService(Constants.Service.DATASET_MANAGER);
  defaultNamespaceEnsurer.startAndWait();
  systemArtifactLoader.startAndWait();
  waitForService(defaultNamespaceEnsurer);
  waitForService(systemArtifactLoader);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      try {
        return namespaceQueryAdmin.exists(NamespaceId.DEFAULT.toId());
      }
 catch (      Exception e) {
        return false;
      }
    }
  }
,10,TimeUnit.SECONDS);
  Assert.assertTrue(defaultNamespaceEnsurer.isRunning());
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      try {
        artifactRepository.getArtifact(SYSTEM_ARTIFACT.toId());
        return true;
      }
 catch (      Exception e) {
        return false;
      }
    }
  }
,20,TimeUnit.SECONDS);
  Assert.assertTrue(systemArtifactLoader.isRunning());
  Dataset systemDataset=DatasetsUtil.getOrCreateDataset(dsFramework,NamespaceId.SYSTEM.dataset(""String_Node_Str"").toId(),Table.class.getName(),DatasetProperties.EMPTY,Collections.<String,String>emptyMap(),this.getClass().getClassLoader());
  Assert.assertNotNull(systemDataset);
  SecurityRequestContext.setUserId(""String_Node_Str"");
  namespaceAdmin.create(new NamespaceMeta.Builder().setName(""String_Node_Str"").build());
  SecurityRequestContext.setUserId(""String_Node_Str"");
  try {
    namespaceAdmin.create(new NamespaceMeta.Builder().setName(""String_Node_Str"").build());
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
}"
5919,"@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  Location deploymentJar=AppJarHelper.createDeploymentJar(new LocalLocationFactory(TMP_FOLDER.newFolder()),InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,deploymentJar.toURI().getPath());
  systemUser=new MasterAuthenticationContext().getPrincipal();
  cConf.set(Constants.Security.Authorization.SYSTEM_USER,systemUser.getName());
  cConf.set(Constants.Security.Authorization.ADMIN_USERS,""String_Node_Str"");
  instanceId=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
  File systemArtifactsDir=TMP_FOLDER.newFolder();
  cConf.set(Constants.AppFabric.SYSTEM_ARTIFACTS_DIR,systemArtifactsDir.getAbsolutePath());
  createSystemArtifact(systemArtifactsDir);
  Injector injector=Guice.createInjector(new AppFabricTestModule(cConf));
  defaultNamespaceEnsurer=injector.getInstance(DefaultNamespaceEnsurer.class);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  txManager=injector.getInstance(TransactionManager.class);
  datasetService=injector.getInstance(DatasetService.class);
  authorizationEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  authorizationEnforcementService.startAndWait();
  systemArtifactLoader=injector.getInstance(SystemArtifactLoader.class);
  authorizationBootstrapper=injector.getInstance(AuthorizationBootstrapper.class);
  namespaceQueryAdmin=injector.getInstance(NamespaceQueryAdmin.class);
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  artifactRepository=injector.getInstance(ArtifactRepository.class);
  dsFramework=injector.getInstance(DatasetFramework.class);
}","@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  Location deploymentJar=AppJarHelper.createDeploymentJar(new LocalLocationFactory(TMP_FOLDER.newFolder()),InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,deploymentJar.toURI().getPath());
  cConf.set(Constants.Security.Authorization.ADMIN_USERS,""String_Node_Str"");
  instanceId=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
  File systemArtifactsDir=TMP_FOLDER.newFolder();
  cConf.set(Constants.AppFabric.SYSTEM_ARTIFACTS_DIR,systemArtifactsDir.getAbsolutePath());
  createSystemArtifact(systemArtifactsDir);
  Injector injector=Guice.createInjector(new AppFabricTestModule(cConf));
  defaultNamespaceEnsurer=injector.getInstance(DefaultNamespaceEnsurer.class);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  txManager=injector.getInstance(TransactionManager.class);
  datasetService=injector.getInstance(DatasetService.class);
  authorizationEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  authorizationEnforcementService.startAndWait();
  systemArtifactLoader=injector.getInstance(SystemArtifactLoader.class);
  authorizationBootstrapper=injector.getInstance(AuthorizationBootstrapper.class);
  namespaceQueryAdmin=injector.getInstance(NamespaceQueryAdmin.class);
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  artifactRepository=injector.getInstance(ArtifactRepository.class);
  dsFramework=injector.getInstance(DatasetFramework.class);
}"
5920,"@BeforeClass public static void setup() throws IOException, InterruptedException {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TEMPORARY_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  cConf.set(Constants.Security.Authorization.SYSTEM_USER,new MasterAuthenticationContext().getPrincipal().getName());
  Manifest manifest=new Manifest();
  manifest.getMainAttributes().put(Attributes.Name.MAIN_CLASS,InMemoryAuthorizer.class.getName());
  LocationFactory locationFactory=new LocalLocationFactory(TEMPORARY_FOLDER.newFolder());
  Location externalAuthJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class,manifest);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,externalAuthJar.toString());
  Injector injector=AppFabricTestHelper.getInjector(cConf);
  discoveryService=injector.getInstance(DiscoveryServiceClient.class);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  waitForService(Constants.Service.APP_FABRIC_HTTP);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  privilegesFetcher=injector.getInstance(PrivilegesFetcher.class);
  privilegesManager=injector.getInstance(PrivilegesManager.class);
}","@BeforeClass public static void setup() throws IOException, InterruptedException {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TEMPORARY_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  Manifest manifest=new Manifest();
  manifest.getMainAttributes().put(Attributes.Name.MAIN_CLASS,InMemoryAuthorizer.class.getName());
  LocationFactory locationFactory=new LocalLocationFactory(TEMPORARY_FOLDER.newFolder());
  Location externalAuthJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class,manifest);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,externalAuthJar.toString());
  Injector injector=AppFabricTestHelper.getInjector(cConf);
  discoveryService=injector.getInstance(DiscoveryServiceClient.class);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  waitForService(Constants.Service.APP_FABRIC_HTTP);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  privilegesFetcher=injector.getInstance(PrivilegesFetcher.class);
  privilegesManager=injector.getInstance(PrivilegesManager.class);
}"
5921,"private static CConfiguration createCConf() throws IOException {
  CConfiguration cConf=CConfiguration.create();
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  LocationFactory locationFactory=new LocalLocationFactory(new File(TEMPORARY_FOLDER.newFolder().toURI()));
  Location authorizerJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,authorizerJar.toURI().getPath());
  cConf.set(Constants.Security.Authorization.SYSTEM_USER,new MasterAuthenticationContext().getPrincipal().getName());
  return cConf;
}","private static CConfiguration createCConf() throws IOException {
  CConfiguration cConf=CConfiguration.create();
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  LocationFactory locationFactory=new LocalLocationFactory(new File(TEMPORARY_FOLDER.newFolder().toURI()));
  Location authorizerJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,authorizerJar.toURI().getPath());
  return cConf;
}"
5922,"private static String[] getAuthConfigs(File tmpDir) throws IOException {
  LocationFactory locationFactory=new LocalLocationFactory(tmpDir);
  Location authExtensionJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class);
  return new String[]{Constants.Security.ENABLED,""String_Node_Str"",Constants.Security.AUTH_HANDLER_CLASS,BasicAuthenticationHandler.class.getName(),Constants.Security.Router.BYPASS_AUTHENTICATION_REGEX,""String_Node_Str"",Constants.Security.Authorization.ENABLED,""String_Node_Str"",Constants.Security.Authorization.CACHE_ENABLED,""String_Node_Str"",Constants.Security.Authorization.EXTENSION_JAR_PATH,authExtensionJar.toURI().getPath(),Constants.Security.Authorization.SYSTEM_USER,new MasterAuthenticationContext().getPrincipal().getName(),Constants.Security.Authorization.EXTENSION_CONFIG_PREFIX + ""String_Node_Str"",""String_Node_Str"",Constants.Security.KERBEROS_ENABLED,""String_Node_Str"",Constants.Explore.EXPLORE_ENABLED,""String_Node_Str""};
}","private static String[] getAuthConfigs(File tmpDir) throws IOException {
  LocationFactory locationFactory=new LocalLocationFactory(tmpDir);
  Location authExtensionJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class);
  return new String[]{Constants.Security.ENABLED,""String_Node_Str"",Constants.Security.AUTH_HANDLER_CLASS,BasicAuthenticationHandler.class.getName(),Constants.Security.Router.BYPASS_AUTHENTICATION_REGEX,""String_Node_Str"",Constants.Security.Authorization.ENABLED,""String_Node_Str"",Constants.Security.Authorization.CACHE_ENABLED,""String_Node_Str"",Constants.Security.Authorization.EXTENSION_JAR_PATH,authExtensionJar.toURI().getPath(),Constants.Security.Authorization.EXTENSION_CONFIG_PREFIX + ""String_Node_Str"",""String_Node_Str"",Constants.Security.KERBEROS_ENABLED,""String_Node_Str"",Constants.Explore.EXPLORE_ENABLED,""String_Node_Str""};
}"
5923,"/** 
 * Get all artifacts that match artifacts in the given ranges.
 * @namespace namespace
 * @param range the range to match artifacts in
 * @return an unmodifiable list of all artifacts that match the given ranges. If none exist, an empty listis returned
 */
public List<ArtifactDetail> getArtifacts(final ArtifactRange range) throws Exception {
  List<ArtifactDetail> artifacts=artifactStore.getArtifacts(range);
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter=authorizationEnforcer.createFilter(principal);
  return Lists.newArrayList(Iterables.filter(artifacts,new com.google.common.base.Predicate<ArtifactDetail>(){
    @Override public boolean apply(    ArtifactDetail artifactDetail){
      ArtifactId artifactId=artifactDetail.getDescriptor().getArtifactId();
      return filter.apply(range.getNamespace().toEntityId().artifact(artifactId.getName(),artifactId.getVersion().getVersion()));
    }
  }
));
}","/** 
 * Get all artifacts that match artifacts in the given ranges.
 * @param range the range to match artifacts in
 * @return an unmodifiable list of all artifacts that match the given ranges. If none exist, an empty list is returned
 */
public List<ArtifactDetail> getArtifacts(final ArtifactRange range) throws Exception {
  List<ArtifactDetail> artifacts=artifactStore.getArtifacts(range);
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter=authorizationEnforcer.createFilter(principal);
  return Lists.newArrayList(Iterables.filter(artifacts,new com.google.common.base.Predicate<ArtifactDetail>(){
    @Override public boolean apply(    ArtifactDetail artifactDetail){
      ArtifactId artifactId=artifactDetail.getDescriptor().getArtifactId();
      return filter.apply(range.getNamespace().toEntityId().artifact(artifactId.getName(),artifactId.getVersion().getVersion()));
    }
  }
));
}"
5924,"private void addProgram(String phaseName,WorkflowProgramAdder programAdder){
  PipelinePhase phase=plan.getPhase(phaseName);
  if (phase == null) {
    return;
  }
  String programName=""String_Node_Str"" + phaseNum;
  phaseNum++;
  for (  StageInfo connectorInfo : phase.getStagesOfType(Constants.CONNECTOR_TYPE)) {
    String connectorName=connectorInfo.getName();
    String datasetName=connectorDatasets.get(connectorName);
    if (datasetName == null) {
      datasetName=UUID.randomUUID().toString();
      connectorDatasets.put(connectorName,datasetName);
      ConnectorSource connectorSource=new ConnectorSource(datasetName,null);
      connectorSource.configure(getConfigurer());
    }
  }
  Map<String,String> phaseConnectorDatasets=new HashMap<>();
  for (  StageInfo connectorStage : phase.getStagesOfType(Constants.CONNECTOR_TYPE)) {
    phaseConnectorDatasets.put(connectorStage.getName(),connectorDatasets.get(connectorStage.getName()));
  }
  BatchPhaseSpec batchPhaseSpec=new BatchPhaseSpec(programName,phase,spec.getResources(),spec.getDriverResources(),spec.isStageLoggingEnabled(),phaseConnectorDatasets);
  boolean hasCustomAction=batchPhaseSpec.getPhase().getSources().isEmpty() && batchPhaseSpec.getPhase().getSinks().isEmpty();
  if (hasCustomAction) {
    programAdder.addAction(new PipelineAction(batchPhaseSpec));
    return;
  }
  if (useSpark) {
    applicationConfigurer.addSpark(new ETLSpark(batchPhaseSpec));
    programAdder.addSpark(programName);
  }
 else {
    applicationConfigurer.addMapReduce(new ETLMapReduce(batchPhaseSpec));
    programAdder.addMapReduce(programName);
  }
}","private void addProgram(String phaseName,WorkflowProgramAdder programAdder){
  PipelinePhase phase=plan.getPhase(phaseName);
  if (phase == null) {
    return;
  }
  String programName=""String_Node_Str"" + phaseNum;
  phaseNum++;
  for (  StageInfo connectorInfo : phase.getStagesOfType(Constants.CONNECTOR_TYPE)) {
    String connectorName=connectorInfo.getName();
    String datasetName=connectorDatasets.get(connectorName);
    if (datasetName == null) {
      datasetName=""String_Node_Str"" + connectorNum++;
      connectorDatasets.put(connectorName,datasetName);
      ConnectorSource connectorSource=new ConnectorSource(datasetName,null);
      connectorSource.configure(getConfigurer());
    }
  }
  Map<String,String> phaseConnectorDatasets=new HashMap<>();
  for (  StageInfo connectorStage : phase.getStagesOfType(Constants.CONNECTOR_TYPE)) {
    phaseConnectorDatasets.put(connectorStage.getName(),connectorDatasets.get(connectorStage.getName()));
  }
  BatchPhaseSpec batchPhaseSpec=new BatchPhaseSpec(programName,phase,spec.getResources(),spec.getDriverResources(),spec.isStageLoggingEnabled(),phaseConnectorDatasets);
  boolean hasCustomAction=batchPhaseSpec.getPhase().getSources().isEmpty() && batchPhaseSpec.getPhase().getSinks().isEmpty();
  if (hasCustomAction) {
    programAdder.addAction(new PipelineAction(batchPhaseSpec));
    return;
  }
  if (useSpark) {
    applicationConfigurer.addSpark(new ETLSpark(batchPhaseSpec));
    programAdder.addSpark(programName);
  }
 else {
    applicationConfigurer.addMapReduce(new ETLMapReduce(batchPhaseSpec));
    programAdder.addMapReduce(programName);
  }
}"
5925,"@SuppressWarnings(""String_Node_Str"") @Override public void run(Context context) throws IOException, InterruptedException {
  MapReduceClassLoader classLoader=MapReduceClassLoader.getFromConfiguration(context.getConfiguration());
  BasicMapReduceTaskContext basicMapReduceContext=classLoader.getTaskContextProvider().get(context);
  WrappedMapper.Context flushingContext=createAutoFlushingContext(context,basicMapReduceContext);
  basicMapReduceContext.setHadoopContext(flushingContext);
  InputSplit inputSplit=context.getInputSplit();
  if (inputSplit instanceof TaggedInputSplit) {
    basicMapReduceContext.setInputName(((TaggedInputSplit)inputSplit).getName());
  }
  ClassLoader programClassLoader=classLoader.getProgramClassLoader();
  Mapper delegate=createMapperInstance(programClassLoader,getWrappedMapper(context.getConfiguration()),context);
  try {
    Reflections.visit(delegate,delegate.getClass(),new PropertyFieldSetter(basicMapReduceContext.getSpecification().getProperties()),new MetricsFieldSetter(basicMapReduceContext.getMetrics()),new DataSetFieldSetter(basicMapReduceContext));
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",delegate.getClass(),t);
    throw Throwables.propagate(t);
  }
  ClassLoader oldClassLoader;
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(programClassLoader);
    try {
      ((ProgramLifecycle)delegate).initialize(new MapReduceLifecycleContext(basicMapReduceContext));
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
      throw Throwables.propagate(e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
  oldClassLoader=ClassLoaders.setContextClassLoader(programClassLoader);
  try {
    delegate.run(flushingContext);
  }
  finally {
    ClassLoaders.setContextClassLoader(oldClassLoader);
  }
  try {
    basicMapReduceContext.flushOperations();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",basicMapReduceContext,e);
    throw Throwables.propagate(e);
  }
  basicMapReduceContext.closeMultiOutputs();
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(programClassLoader);
    try {
      ((ProgramLifecycle<? extends RuntimeContext>)delegate).destroy();
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
}","@SuppressWarnings(""String_Node_Str"") @Override public void run(Context context) throws IOException, InterruptedException {
  MapReduceClassLoader classLoader=MapReduceClassLoader.getFromConfiguration(context.getConfiguration());
  BasicMapReduceTaskContext basicMapReduceContext=classLoader.getTaskContextProvider().get(context);
  WrappedMapper.Context flushingContext=createAutoFlushingContext(context,basicMapReduceContext);
  basicMapReduceContext.setHadoopContext(flushingContext);
  InputSplit inputSplit=context.getInputSplit();
  if (inputSplit instanceof TaggedInputSplit) {
    basicMapReduceContext.setInputName(((TaggedInputSplit)inputSplit).getName());
  }
  ClassLoader programClassLoader=classLoader.getProgramClassLoader();
  Mapper delegate=createMapperInstance(programClassLoader,getWrappedMapper(context.getConfiguration()),context);
  try {
    Reflections.visit(delegate,delegate.getClass(),new PropertyFieldSetter(basicMapReduceContext.getSpecification().getProperties()),new MetricsFieldSetter(basicMapReduceContext.getMetrics()),new DataSetFieldSetter(basicMapReduceContext));
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",delegate.getClass(),t);
    throw Throwables.propagate(t);
  }
  ClassLoader oldClassLoader;
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(classLoader);
    try {
      ((ProgramLifecycle)delegate).initialize(new MapReduceLifecycleContext(basicMapReduceContext));
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
      throw Throwables.propagate(e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
  oldClassLoader=ClassLoaders.setContextClassLoader(classLoader);
  try {
    delegate.run(flushingContext);
  }
  finally {
    ClassLoaders.setContextClassLoader(oldClassLoader);
  }
  try {
    basicMapReduceContext.flushOperations();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",basicMapReduceContext,e);
    throw Throwables.propagate(e);
  }
  basicMapReduceContext.closeMultiOutputs();
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(classLoader);
    try {
      ((ProgramLifecycle<? extends RuntimeContext>)delegate).destroy();
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
}"
5926,"@SuppressWarnings(""String_Node_Str"") @Override public void run(Context context) throws IOException, InterruptedException {
  MapReduceClassLoader classLoader=MapReduceClassLoader.getFromConfiguration(context.getConfiguration());
  BasicMapReduceTaskContext basicMapReduceContext=classLoader.getTaskContextProvider().get(context);
  WrappedReducer.Context flushingContext=createAutoFlushingContext(context,basicMapReduceContext);
  basicMapReduceContext.setHadoopContext(flushingContext);
  String userReducer=context.getConfiguration().get(ATTR_REDUCER_CLASS);
  ClassLoader programClassLoader=classLoader.getProgramClassLoader();
  Reducer delegate=createReducerInstance(programClassLoader,userReducer);
  try {
    Reflections.visit(delegate,delegate.getClass(),new PropertyFieldSetter(basicMapReduceContext.getSpecification().getProperties()),new MetricsFieldSetter(basicMapReduceContext.getMetrics()),new DataSetFieldSetter(basicMapReduceContext));
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",delegate.getClass(),t);
    throw Throwables.propagate(t);
  }
  ClassLoader oldClassLoader;
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(programClassLoader);
    try {
      ((ProgramLifecycle)delegate).initialize(new MapReduceLifecycleContext(basicMapReduceContext));
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
      throw Throwables.propagate(e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
  oldClassLoader=ClassLoaders.setContextClassLoader(programClassLoader);
  try {
    delegate.run(flushingContext);
  }
  finally {
    ClassLoaders.setContextClassLoader(oldClassLoader);
  }
  try {
    basicMapReduceContext.flushOperations();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"" + basicMapReduceContext,e);
    throw Throwables.propagate(e);
  }
  basicMapReduceContext.closeMultiOutputs();
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(programClassLoader);
    try {
      ((ProgramLifecycle<? extends RuntimeContext>)delegate).destroy();
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
}","@SuppressWarnings(""String_Node_Str"") @Override public void run(Context context) throws IOException, InterruptedException {
  MapReduceClassLoader classLoader=MapReduceClassLoader.getFromConfiguration(context.getConfiguration());
  BasicMapReduceTaskContext basicMapReduceContext=classLoader.getTaskContextProvider().get(context);
  WrappedReducer.Context flushingContext=createAutoFlushingContext(context,basicMapReduceContext);
  basicMapReduceContext.setHadoopContext(flushingContext);
  String userReducer=context.getConfiguration().get(ATTR_REDUCER_CLASS);
  ClassLoader programClassLoader=classLoader.getProgramClassLoader();
  Reducer delegate=createReducerInstance(programClassLoader,userReducer);
  try {
    Reflections.visit(delegate,delegate.getClass(),new PropertyFieldSetter(basicMapReduceContext.getSpecification().getProperties()),new MetricsFieldSetter(basicMapReduceContext.getMetrics()),new DataSetFieldSetter(basicMapReduceContext));
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",delegate.getClass(),t);
    throw Throwables.propagate(t);
  }
  ClassLoader oldClassLoader;
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(classLoader);
    try {
      ((ProgramLifecycle)delegate).initialize(new MapReduceLifecycleContext(basicMapReduceContext));
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
      throw Throwables.propagate(e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
  oldClassLoader=ClassLoaders.setContextClassLoader(classLoader);
  try {
    delegate.run(flushingContext);
  }
  finally {
    ClassLoaders.setContextClassLoader(oldClassLoader);
  }
  try {
    basicMapReduceContext.flushOperations();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"" + basicMapReduceContext,e);
    throw Throwables.propagate(e);
  }
  basicMapReduceContext.closeMultiOutputs();
  if (delegate instanceof ProgramLifecycle) {
    oldClassLoader=ClassLoaders.setContextClassLoader(classLoader);
    try {
      ((ProgramLifecycle<? extends RuntimeContext>)delegate).destroy();
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",basicMapReduceContext,e);
    }
 finally {
      ClassLoaders.setContextClassLoader(oldClassLoader);
    }
  }
}"
5927,"private void assertDataEntitySearch() throws Exception {
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME);
  Id.DatasetInstance datasetInstance2=Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME2);
  Id.DatasetInstance datasetInstance3=Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME3);
  Id.DatasetInstance dsWithSchema=Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DS_WITH_SCHEMA_NAME);
  Id.Stream streamId=Id.Stream.from(Id.Namespace.DEFAULT,AllProgramsApp.STREAM_NAME);
  Id.Stream.View view=Id.Stream.View.from(streamId,""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(streamId),new MetadataSearchResultRecord(mystream));
  Set<MetadataSearchResultRecord> expectedWithView=ImmutableSet.<MetadataSearchResultRecord>builder().addAll(expected).add(new MetadataSearchResultRecord(myview)).build();
  Set<MetadataSearchResultRecord> metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expectedWithView,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"" + Schema.Type.STRING.toString());
  Assert.assertEquals(expected,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expectedWithView,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expected,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.<MetadataSearchResultRecord>builder().addAll(expected).add(new MetadataSearchResultRecord(dsWithSchema)).build(),metadataSearchResultRecords);
  Schema viewSchema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.BYTES))));
  streamViewClient.createOrUpdate(view,new ViewSpecification(new FormatSpecification(""String_Node_Str"",viewSchema)));
  Map<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  addProperties(datasetInstance,datasetProperties);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.<MetadataSearchResultRecord>builder().addAll(expectedWithView).add(new MetadataSearchResultRecord(datasetInstance)).add(new MetadataSearchResultRecord(dsWithSchema)).add(new MetadataSearchResultRecord(view)).build(),metadataSearchResultRecords);
  ImmutableSet<MetadataSearchResultRecord> expectedKvTables=ImmutableSet.of(new MetadataSearchResultRecord(datasetInstance),new MetadataSearchResultRecord(datasetInstance2),new MetadataSearchResultRecord(datasetInstance3),new MetadataSearchResultRecord(myds));
  ImmutableSet<MetadataSearchResultRecord> expectedAllDatasets=ImmutableSet.<MetadataSearchResultRecord>builder().addAll(expectedKvTables).add(new MetadataSearchResultRecord(dsWithSchema)).build();
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expectedAllDatasets,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,KeyValueTable.class.getName());
  Assert.assertEquals(expectedKvTables,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expectedAllDatasets,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expected,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.STREAM_NAME);
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(streamId),new MetadataSearchResultRecord(view)),metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.STREAM_NAME,MetadataSearchTargetType.STREAM);
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(streamId)),metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.STREAM_NAME,MetadataSearchTargetType.VIEW);
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(view)),metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"",MetadataSearchTargetType.VIEW);
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(view)),metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME);
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(datasetInstance)),metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.DS_WITH_SCHEMA_NAME);
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(dsWithSchema)),metadataSearchResultRecords);
}","private void assertDataEntitySearch() throws Exception {
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME);
  Id.DatasetInstance datasetInstance2=Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME2);
  Id.DatasetInstance datasetInstance3=Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME3);
  Id.DatasetInstance datasetInstance4=Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME4);
  Id.DatasetInstance datasetInstance5=Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME5);
  Id.DatasetInstance datasetInstance6=Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME6);
  Id.DatasetInstance datasetInstance7=Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME7);
  Id.DatasetInstance dsWithSchema=Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DS_WITH_SCHEMA_NAME);
  Id.Stream streamId=Id.Stream.from(Id.Namespace.DEFAULT,AllProgramsApp.STREAM_NAME);
  Id.Stream.View view=Id.Stream.View.from(streamId,""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(streamId),new MetadataSearchResultRecord(mystream));
  Set<MetadataSearchResultRecord> expectedWithView=ImmutableSet.<MetadataSearchResultRecord>builder().addAll(expected).add(new MetadataSearchResultRecord(myview)).build();
  Set<MetadataSearchResultRecord> metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expectedWithView,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"" + Schema.Type.STRING.toString());
  Assert.assertEquals(expected,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expectedWithView,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expected,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.<MetadataSearchResultRecord>builder().addAll(expected).add(new MetadataSearchResultRecord(dsWithSchema)).build(),metadataSearchResultRecords);
  Schema viewSchema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.nullableOf(Schema.of(Schema.Type.BYTES))));
  streamViewClient.createOrUpdate(view,new ViewSpecification(new FormatSpecification(""String_Node_Str"",viewSchema)));
  Map<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  addProperties(datasetInstance,datasetProperties);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.<MetadataSearchResultRecord>builder().addAll(expectedWithView).add(new MetadataSearchResultRecord(datasetInstance)).add(new MetadataSearchResultRecord(dsWithSchema)).add(new MetadataSearchResultRecord(view)).build(),metadataSearchResultRecords);
  ImmutableSet<MetadataSearchResultRecord> expectedKvTables=ImmutableSet.of(new MetadataSearchResultRecord(datasetInstance),new MetadataSearchResultRecord(datasetInstance2),new MetadataSearchResultRecord(datasetInstance3),new MetadataSearchResultRecord(myds));
  ImmutableSet<MetadataSearchResultRecord> expectedExplorableDatasets=ImmutableSet.<MetadataSearchResultRecord>builder().addAll(expectedKvTables).add(new MetadataSearchResultRecord(datasetInstance4)).add(new MetadataSearchResultRecord(datasetInstance5)).add(new MetadataSearchResultRecord(dsWithSchema)).build();
  ImmutableSet<MetadataSearchResultRecord> expectedAllDatasets=ImmutableSet.<MetadataSearchResultRecord>builder().addAll(expectedExplorableDatasets).add(new MetadataSearchResultRecord(datasetInstance6)).add(new MetadataSearchResultRecord(datasetInstance7)).build();
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expectedExplorableDatasets,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,KeyValueTable.class.getName());
  Assert.assertEquals(expectedKvTables,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expectedAllDatasets,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"");
  Assert.assertEquals(expected,metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.STREAM_NAME);
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(streamId),new MetadataSearchResultRecord(view)),metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.STREAM_NAME,MetadataSearchTargetType.STREAM);
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(streamId)),metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.STREAM_NAME,MetadataSearchTargetType.VIEW);
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(view)),metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str"",MetadataSearchTargetType.VIEW);
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(view)),metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME);
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(datasetInstance)),metadataSearchResultRecords);
  metadataSearchResultRecords=searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.DS_WITH_SCHEMA_NAME);
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(dsWithSchema)),metadataSearchResultRecords);
}"
5928,"private void assertProgramSearch(Id.Application app) throws Exception {
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR.NAME)),new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR2.NAME)),new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.WORKFLOW,AllProgramsApp.NoOpWorkflow.NAME)),new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.SPARK,AllProgramsApp.NoOpSpark.NAME)),new MetadataSearchResultRecord(Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME)),new MetadataSearchResultRecord(Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME2)),new MetadataSearchResultRecord(Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME3)),new MetadataSearchResultRecord(Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DS_WITH_SCHEMA_NAME)),new MetadataSearchResultRecord(myds)),searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str""));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.FLOW,AllProgramsApp.NoOpFlow.NAME)),new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.SERVICE,AllProgramsApp.NoOpService.NAME)),new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.WORKER,AllProgramsApp.NoOpWorker.NAME)),new MetadataSearchResultRecord(Id.Program.from(Id.Application.from(Id.Namespace.DEFAULT,AppWithDataset.class.getSimpleName()),ProgramType.SERVICE,""String_Node_Str""))),searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str""));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.FLOW,AllProgramsApp.NoOpFlow.NAME))),searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.NoOpFlow.NAME,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR.NAME)),new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.WORKFLOW,AllProgramsApp.NoOpWorkflow.NAME))),searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.NoOpMR.NAME,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.SERVICE,AllProgramsApp.NoOpService.NAME))),searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.NoOpService.NAME,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.SPARK,AllProgramsApp.NoOpSpark.NAME))),searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.NoOpSpark.NAME,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.WORKER,AllProgramsApp.NoOpWorker.NAME))),searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.NoOpWorker.NAME,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.WORKFLOW,AllProgramsApp.NoOpWorkflow.NAME))),searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.NoOpWorkflow.NAME,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.FLOW,AllProgramsApp.NoOpFlow.NAME))),searchMetadata(Id.Namespace.DEFAULT,ProgramType.FLOW.getPrettyName(),MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR.NAME)),new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR2.NAME))),searchMetadata(Id.Namespace.DEFAULT,ProgramType.MAPREDUCE.getPrettyName(),MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.SERVICE,AllProgramsApp.NoOpService.NAME)),new MetadataSearchResultRecord(pingService)),searchMetadata(Id.Namespace.DEFAULT,ProgramType.SERVICE.getPrettyName(),MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.SPARK,AllProgramsApp.NoOpSpark.NAME))),searchMetadata(Id.Namespace.DEFAULT,ProgramType.SPARK.getPrettyName(),MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.WORKER,AllProgramsApp.NoOpWorker.NAME))),searchMetadata(Id.Namespace.DEFAULT,ProgramType.WORKER.getPrettyName(),MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.WORKFLOW,AllProgramsApp.NoOpWorkflow.NAME))),searchMetadata(Id.Namespace.DEFAULT,ProgramType.WORKFLOW.getPrettyName(),MetadataSearchTargetType.PROGRAM));
}","private void assertProgramSearch(Id.Application app) throws Exception {
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR.NAME)),new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR2.NAME)),new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.WORKFLOW,AllProgramsApp.NoOpWorkflow.NAME)),new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.SPARK,AllProgramsApp.NoOpSpark.NAME)),new MetadataSearchResultRecord(Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME)),new MetadataSearchResultRecord(Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME2)),new MetadataSearchResultRecord(Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME3)),new MetadataSearchResultRecord(Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME4)),new MetadataSearchResultRecord(Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME5)),new MetadataSearchResultRecord(Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME6)),new MetadataSearchResultRecord(Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DATASET_NAME7)),new MetadataSearchResultRecord(Id.DatasetInstance.from(Id.Namespace.DEFAULT,AllProgramsApp.DS_WITH_SCHEMA_NAME)),new MetadataSearchResultRecord(myds)),searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str""));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.FLOW,AllProgramsApp.NoOpFlow.NAME)),new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.SERVICE,AllProgramsApp.NoOpService.NAME)),new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.WORKER,AllProgramsApp.NoOpWorker.NAME)),new MetadataSearchResultRecord(Id.Program.from(Id.Application.from(Id.Namespace.DEFAULT,AppWithDataset.class.getSimpleName()),ProgramType.SERVICE,""String_Node_Str""))),searchMetadata(Id.Namespace.DEFAULT,""String_Node_Str""));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.FLOW,AllProgramsApp.NoOpFlow.NAME))),searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.NoOpFlow.NAME,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR.NAME)),new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.WORKFLOW,AllProgramsApp.NoOpWorkflow.NAME))),searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.NoOpMR.NAME,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.SERVICE,AllProgramsApp.NoOpService.NAME))),searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.NoOpService.NAME,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.SPARK,AllProgramsApp.NoOpSpark.NAME))),searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.NoOpSpark.NAME,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.WORKER,AllProgramsApp.NoOpWorker.NAME))),searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.NoOpWorker.NAME,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.WORKFLOW,AllProgramsApp.NoOpWorkflow.NAME))),searchMetadata(Id.Namespace.DEFAULT,AllProgramsApp.NoOpWorkflow.NAME,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.FLOW,AllProgramsApp.NoOpFlow.NAME))),searchMetadata(Id.Namespace.DEFAULT,ProgramType.FLOW.getPrettyName(),MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR.NAME)),new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR2.NAME))),searchMetadata(Id.Namespace.DEFAULT,ProgramType.MAPREDUCE.getPrettyName(),MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.SERVICE,AllProgramsApp.NoOpService.NAME)),new MetadataSearchResultRecord(pingService)),searchMetadata(Id.Namespace.DEFAULT,ProgramType.SERVICE.getPrettyName(),MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.SPARK,AllProgramsApp.NoOpSpark.NAME))),searchMetadata(Id.Namespace.DEFAULT,ProgramType.SPARK.getPrettyName(),MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.WORKER,AllProgramsApp.NoOpWorker.NAME))),searchMetadata(Id.Namespace.DEFAULT,ProgramType.WORKER.getPrettyName(),MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(ImmutableSet.of(new MetadataSearchResultRecord(Id.Program.from(app,ProgramType.WORKFLOW,AllProgramsApp.NoOpWorkflow.NAME))),searchMetadata(Id.Namespace.DEFAULT,ProgramType.WORKFLOW.getPrettyName(),MetadataSearchTargetType.PROGRAM));
}"
5929,"@Override protected void configure(){
  bind(DynamicDatasetCache.class).toProvider(DynamicDatasetCacheProvider.class).in(Scopes.SINGLETON);
  bind(DatasetContext.class).to(DynamicDatasetCache.class).in(Scopes.SINGLETON);
  bind(Admin.class).toProvider(AdminProvider.class);
  bind(Transactional.class).toProvider(TransactionalProvider.class);
  install(new FactoryModuleBuilder().implement(AuthorizationContext.class,DefaultAuthorizationContext.class).build(AuthorizationContextFactory.class));
  bind(AuthorizerInstantiator.class).in(Scopes.SINGLETON);
  expose(AuthorizerInstantiator.class);
  bind(PrivilegesManager.class).to(AuthorizerAsPrivilegesManager.class);
  expose(PrivilegesManager.class);
}","@Override protected void configure(){
  bind(DynamicDatasetCache.class).toProvider(DynamicDatasetCacheProvider.class).in(Scopes.SINGLETON);
  bind(DatasetContext.class).to(DynamicDatasetCache.class).in(Scopes.SINGLETON);
  bind(Admin.class).toProvider(AdminProvider.class);
  bind(Transactional.class).toProvider(TransactionalProvider.class);
  install(new FactoryModuleBuilder().implement(AuthorizationContext.class,DefaultAuthorizationContext.class).build(AuthorizationContextFactory.class));
  bind(AuthorizerInstantiator.class).in(Scopes.SINGLETON);
  expose(AuthorizerInstantiator.class);
  bind(PrivilegesManager.class).to(DefaultPrivilegesManager.class);
  expose(PrivilegesManager.class);
}"
5930,"@Inject AuthorizationHandler(AuthorizerInstantiator authorizerInstantiator,CConfiguration cConf,AuthorizationEnforcer authorizationEnforcer,AuthenticationContext authenticationContext,EntityExistenceVerifier entityExistenceVerifier){
  this.authorizer=authorizerInstantiator.get();
  this.authorizationEnforcer=authorizationEnforcer;
  this.authenticationContext=authenticationContext;
  this.authenticationEnabled=cConf.getBoolean(Constants.Security.ENABLED);
  this.authorizationEnabled=cConf.getBoolean(Constants.Security.Authorization.ENABLED);
  this.entityExistenceVerifier=entityExistenceVerifier;
}","@Inject AuthorizationHandler(PrivilegesManager privilegesManager,AuthorizerInstantiator authorizerInstantiator,CConfiguration cConf,AuthorizationEnforcer authorizationEnforcer,AuthenticationContext authenticationContext,EntityExistenceVerifier entityExistenceVerifier){
  this.privilegesManager=privilegesManager;
  this.authorizer=authorizerInstantiator.get();
  this.authorizationEnforcer=authorizationEnforcer;
  this.authenticationContext=authenticationContext;
  this.authenticationEnabled=cConf.getBoolean(Constants.Security.ENABLED);
  this.authorizationEnabled=cConf.getBoolean(Constants.Security.Authorization.ENABLED);
  this.entityExistenceVerifier=entityExistenceVerifier;
}"
5931,"@Path(""String_Node_Str"") @POST public void revoke(HttpRequest httpRequest,HttpResponder httpResponder) throws Exception {
  ensureSecurityEnabled();
  RevokeRequest request=parseBody(httpRequest,RevokeRequest.class);
  verifyAuthRequest(request);
  authorizationEnforcer.enforce(request.getEntity(),authenticationContext.getPrincipal(),Action.ADMIN);
  if (request.getPrincipal() == null && request.getActions() == null) {
    authorizer.revoke(request.getEntity());
  }
 else {
    Set<Action> actions=request.getActions() == null ? EnumSet.allOf(Action.class) : request.getActions();
    authorizer.revoke(request.getEntity(),request.getPrincipal(),actions);
  }
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,request,HttpResponseStatus.OK);
}","@Path(""String_Node_Str"") @POST public void revoke(HttpRequest httpRequest,HttpResponder httpResponder) throws Exception {
  ensureSecurityEnabled();
  RevokeRequest request=parseBody(httpRequest,RevokeRequest.class);
  verifyAuthRequest(request);
  authorizationEnforcer.enforce(request.getEntity(),authenticationContext.getPrincipal(),Action.ADMIN);
  if (request.getPrincipal() == null && request.getActions() == null) {
    privilegesManager.revoke(request.getEntity());
  }
 else {
    Set<Action> actions=request.getActions() == null ? EnumSet.allOf(Action.class) : request.getActions();
    privilegesManager.revoke(request.getEntity(),request.getPrincipal(),actions);
  }
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,request,HttpResponseStatus.OK);
}"
5932,"@Path(""String_Node_Str"") @POST public void grant(HttpRequest httpRequest,HttpResponder httpResponder) throws Exception {
  ensureSecurityEnabled();
  GrantRequest request=parseBody(httpRequest,GrantRequest.class);
  verifyAuthRequest(request);
  Set<Action> actions=request.getActions() == null ? EnumSet.allOf(Action.class) : request.getActions();
  authorizationEnforcer.enforce(request.getEntity(),authenticationContext.getPrincipal(),Action.ADMIN);
  authorizer.grant(request.getEntity(),request.getPrincipal(),actions);
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,request,HttpResponseStatus.OK);
}","@Path(""String_Node_Str"") @POST public void grant(HttpRequest httpRequest,HttpResponder httpResponder) throws Exception {
  ensureSecurityEnabled();
  GrantRequest request=parseBody(httpRequest,GrantRequest.class);
  verifyAuthRequest(request);
  Set<Action> actions=request.getActions() == null ? EnumSet.allOf(Action.class) : request.getActions();
  authorizationEnforcer.enforce(request.getEntity(),authenticationContext.getPrincipal(),Action.ADMIN);
  privilegesManager.grant(request.getEntity(),request.getPrincipal(),actions);
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,request,HttpResponseStatus.OK);
}"
5933,"@Inject LocalApplicationManager(CConfiguration configuration,PipelineFactory pipelineFactory,NamespacedLocationFactory namespacedLocationFactory,Store store,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,DatasetFramework datasetFramework,@Named(""String_Node_Str"") DatasetFramework inMemoryDatasetFramework,StreamAdmin streamAdmin,Scheduler scheduler,@Assisted ProgramTerminator programTerminator,MetricStore metricStore,UsageRegistry usageRegistry,ArtifactRepository artifactRepository,MetadataStore metadataStore,AuthorizerInstantiator authorizerInstantiator,Impersonator impersonator){
  this.configuration=configuration;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.pipelineFactory=pipelineFactory;
  this.store=store;
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.programTerminator=programTerminator;
  this.datasetFramework=datasetFramework;
  this.inMemoryDatasetFramework=inMemoryDatasetFramework;
  this.streamAdmin=streamAdmin;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
  this.usageRegistry=usageRegistry;
  this.artifactRepository=artifactRepository;
  this.metadataStore=metadataStore;
  this.authorizer=authorizerInstantiator.get();
  this.impersonator=impersonator;
}","@Inject LocalApplicationManager(CConfiguration configuration,PipelineFactory pipelineFactory,Store store,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,DatasetFramework datasetFramework,@Named(""String_Node_Str"") DatasetFramework inMemoryDatasetFramework,StreamAdmin streamAdmin,Scheduler scheduler,@Assisted ProgramTerminator programTerminator,MetricStore metricStore,UsageRegistry usageRegistry,ArtifactRepository artifactRepository,MetadataStore metadataStore,PrivilegesManager privilegesManager,Impersonator impersonator,AuthenticationContext authenticationContext){
  this.configuration=configuration;
  this.pipelineFactory=pipelineFactory;
  this.store=store;
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.programTerminator=programTerminator;
  this.datasetFramework=datasetFramework;
  this.inMemoryDatasetFramework=inMemoryDatasetFramework;
  this.streamAdmin=streamAdmin;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
  this.usageRegistry=usageRegistry;
  this.artifactRepository=artifactRepository;
  this.metadataStore=metadataStore;
  this.privilegesManager=privilegesManager;
  this.impersonator=impersonator;
  this.authenticationContext=authenticationContext;
}"
5934,"@Override public ListenableFuture<O> deploy(I input) throws Exception {
  Pipeline<O> pipeline=pipelineFactory.getPipeline();
  pipeline.addLast(new LocalArtifactLoaderStage(configuration,store,artifactRepository,impersonator));
  pipeline.addLast(new ApplicationVerificationStage(store,datasetFramework));
  pipeline.addLast(new DeployDatasetModulesStage(configuration,datasetFramework,inMemoryDatasetFramework));
  pipeline.addLast(new CreateDatasetInstancesStage(configuration,datasetFramework));
  pipeline.addLast(new CreateStreamsStage(streamAdmin));
  pipeline.addLast(new DeletedProgramHandlerStage(store,programTerminator,streamConsumerFactory,queueAdmin,metricStore,metadataStore,authorizer,impersonator));
  pipeline.addLast(new ProgramGenerationStage(configuration,namespacedLocationFactory,authorizer));
  pipeline.addLast(new ApplicationRegistrationStage(store,usageRegistry));
  pipeline.addLast(new CreateSchedulesStage(scheduler));
  pipeline.addLast(new SystemMetadataWriterStage(metadataStore));
  pipeline.setFinally(new DeploymentCleanupStage());
  return pipeline.execute(input);
}","@Override public ListenableFuture<O> deploy(I input) throws Exception {
  Pipeline<O> pipeline=pipelineFactory.getPipeline();
  pipeline.addLast(new LocalArtifactLoaderStage(configuration,store,artifactRepository,impersonator));
  pipeline.addLast(new ApplicationVerificationStage(store,datasetFramework));
  pipeline.addLast(new DeployDatasetModulesStage(configuration,datasetFramework,inMemoryDatasetFramework));
  pipeline.addLast(new CreateDatasetInstancesStage(configuration,datasetFramework));
  pipeline.addLast(new CreateStreamsStage(streamAdmin));
  pipeline.addLast(new DeletedProgramHandlerStage(store,programTerminator,streamConsumerFactory,queueAdmin,metricStore,metadataStore,privilegesManager,impersonator));
  pipeline.addLast(new ProgramGenerationStage(privilegesManager,authenticationContext));
  pipeline.addLast(new ApplicationRegistrationStage(store,usageRegistry));
  pipeline.addLast(new CreateSchedulesStage(scheduler));
  pipeline.addLast(new SystemMetadataWriterStage(metadataStore));
  pipeline.setFinally(new DeploymentCleanupStage());
  return pipeline.execute(input);
}"
5935,"public DeletedProgramHandlerStage(Store store,ProgramTerminator programTerminator,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,MetricStore metricStore,MetadataStore metadataStore,Authorizer authorizer,Impersonator impersonator){
  super(TypeToken.of(ApplicationDeployable.class));
  this.store=store;
  this.programTerminator=programTerminator;
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.metricStore=metricStore;
  this.metadataStore=metadataStore;
  this.authorizer=authorizer;
  this.impersonator=impersonator;
}","public DeletedProgramHandlerStage(Store store,ProgramTerminator programTerminator,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,MetricStore metricStore,MetadataStore metadataStore,PrivilegesManager privilegesManager,Impersonator impersonator){
  super(TypeToken.of(ApplicationDeployable.class));
  this.store=store;
  this.programTerminator=programTerminator;
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.metricStore=metricStore;
  this.metadataStore=metadataStore;
  this.privilegesManager=privilegesManager;
  this.impersonator=impersonator;
}"
5936,"@Override public void process(ApplicationDeployable appSpec) throws Exception {
  List<ProgramSpecification> deletedSpecs=store.getDeletedProgramSpecifications(appSpec.getApplicationId().toId(),appSpec.getSpecification());
  List<String> deletedFlows=Lists.newArrayList();
  for (  ProgramSpecification spec : deletedSpecs) {
    ProgramType type=ProgramTypes.fromSpecification(spec);
    final Id.Program programId=appSpec.getApplicationId().program(type,spec.getName()).toId();
    programTerminator.stop(programId);
    authorizer.revoke(programId.toEntityId());
    if (ProgramType.FLOW.equals(type)) {
      FlowSpecification flowSpecification=(FlowSpecification)spec;
      final Multimap<String,Long> streamGroups=HashMultimap.create();
      for (      FlowletConnection connection : flowSpecification.getConnections()) {
        if (connection.getSourceType() == FlowletConnection.Type.STREAM) {
          long groupId=FlowUtils.generateConsumerGroupId(programId,connection.getTargetName());
          streamGroups.put(connection.getSourceName(),groupId);
        }
      }
      final String namespace=String.format(""String_Node_Str"",programId.getApplicationId(),programId.getId());
      final NamespaceId namespaceId=appSpec.getApplicationId().getParent();
      impersonator.doAs(namespaceId,new Callable<Void>(){
        @Override public Void call() throws Exception {
          for (          Map.Entry<String,Collection<Long>> entry : streamGroups.asMap().entrySet()) {
            streamConsumerFactory.dropAll(namespaceId.stream(entry.getKey()).toId(),namespace,entry.getValue());
          }
          queueAdmin.dropAllForFlow(Id.Flow.from(programId.getApplication(),programId.getId()));
          return null;
        }
      }
);
      deletedFlows.add(programId.getId());
    }
    metadataStore.removeMetadata(programId);
  }
  if (!deletedFlows.isEmpty()) {
    deleteMetrics(appSpec.getApplicationId(),deletedFlows);
  }
  emit(appSpec);
}","@Override public void process(ApplicationDeployable appSpec) throws Exception {
  List<ProgramSpecification> deletedSpecs=store.getDeletedProgramSpecifications(appSpec.getApplicationId().toId(),appSpec.getSpecification());
  List<String> deletedFlows=Lists.newArrayList();
  for (  ProgramSpecification spec : deletedSpecs) {
    ProgramType type=ProgramTypes.fromSpecification(spec);
    final Id.Program programId=appSpec.getApplicationId().program(type,spec.getName()).toId();
    programTerminator.stop(programId);
    privilegesManager.revoke(programId.toEntityId());
    if (ProgramType.FLOW.equals(type)) {
      FlowSpecification flowSpecification=(FlowSpecification)spec;
      final Multimap<String,Long> streamGroups=HashMultimap.create();
      for (      FlowletConnection connection : flowSpecification.getConnections()) {
        if (connection.getSourceType() == FlowletConnection.Type.STREAM) {
          long groupId=FlowUtils.generateConsumerGroupId(programId,connection.getTargetName());
          streamGroups.put(connection.getSourceName(),groupId);
        }
      }
      final String namespace=String.format(""String_Node_Str"",programId.getApplicationId(),programId.getId());
      final NamespaceId namespaceId=appSpec.getApplicationId().getParent();
      impersonator.doAs(namespaceId,new Callable<Void>(){
        @Override public Void call() throws Exception {
          for (          Map.Entry<String,Collection<Long>> entry : streamGroups.asMap().entrySet()) {
            streamConsumerFactory.dropAll(namespaceId.stream(entry.getKey()).toId(),namespace,entry.getValue());
          }
          queueAdmin.dropAllForFlow(Id.Flow.from(programId.getApplication(),programId.getId()));
          return null;
        }
      }
);
      deletedFlows.add(programId.getId());
    }
    metadataStore.removeMetadata(programId);
  }
  if (!deletedFlows.isEmpty()) {
    deleteMetrics(appSpec.getApplicationId(),deletedFlows);
  }
  emit(appSpec);
}"
5937,"@Override public void process(final ApplicationDeployable input) throws Exception {
  List<ProgramDescriptor> programDescriptors=new ArrayList<>();
  final ApplicationSpecification appSpec=input.getSpecification();
  Iterable<ProgramSpecification> specifications=Iterables.concat(appSpec.getMapReduce().values(),appSpec.getFlows().values(),appSpec.getWorkflows().values(),appSpec.getServices().values(),appSpec.getSpark().values(),appSpec.getWorkers().values());
  for (  final ProgramSpecification spec : specifications) {
    ProgramType type=ProgramTypes.fromSpecification(spec);
    ProgramId programId=input.getApplicationId().program(type,spec.getName());
    authorizer.grant(programId,SecurityRequestContext.toPrincipal(),ImmutableSet.of(Action.ALL));
    programDescriptors.add(new ProgramDescriptor(programId,appSpec));
  }
  emit(new ApplicationWithPrograms(input,programDescriptors));
}","@Override public void process(final ApplicationDeployable input) throws Exception {
  List<ProgramDescriptor> programDescriptors=new ArrayList<>();
  final ApplicationSpecification appSpec=input.getSpecification();
  Iterable<ProgramSpecification> specifications=Iterables.concat(appSpec.getMapReduce().values(),appSpec.getFlows().values(),appSpec.getWorkflows().values(),appSpec.getServices().values(),appSpec.getSpark().values(),appSpec.getWorkers().values());
  for (  final ProgramSpecification spec : specifications) {
    ProgramType type=ProgramTypes.fromSpecification(spec);
    ProgramId programId=input.getApplicationId().program(type,spec.getName());
    privilegesManager.grant(programId,authenticationContext.getPrincipal(),ImmutableSet.of(Action.ALL));
    programDescriptors.add(new ProgramDescriptor(programId,appSpec));
  }
  emit(new ApplicationWithPrograms(input,programDescriptors));
}"
5938,"public ProgramGenerationStage(CConfiguration configuration,NamespacedLocationFactory namespacedLocationFactory,Authorizer authorizer){
  super(TypeToken.of(ApplicationDeployable.class));
  this.configuration=configuration;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.authorizer=authorizer;
}","public ProgramGenerationStage(PrivilegesManager privilegesManager,AuthenticationContext authenticationContext){
  super(TypeToken.of(ApplicationDeployable.class));
  this.privilegesManager=privilegesManager;
  this.authenticationContext=authenticationContext;
}"
5939,"private void validateCustomMapping(NamespaceMeta metadata) throws Exception {
  for (  NamespaceMeta existingNamespaceMeta : list()) {
    NamespaceConfig existingConfig=existingNamespaceMeta.getConfig();
    if (!Strings.isNullOrEmpty(metadata.getConfig().getHbaseNamespace()) && metadata.getConfig().getHbaseNamespace().equals(existingConfig.getHbaseNamespace())) {
      throw new NamespaceAlreadyExistsException(existingNamespaceMeta.getNamespaceId().toId(),String.format(""String_Node_Str"" + ""String_Node_Str"",existingNamespaceMeta.getName(),existingConfig.getHbaseNamespace()));
    }
    if (!Strings.isNullOrEmpty(metadata.getConfig().getHiveDatabase()) && metadata.getConfig().getHiveDatabase().equals(existingConfig.getHiveDatabase())) {
      throw new NamespaceAlreadyExistsException(existingNamespaceMeta.getNamespaceId().toId(),String.format(""String_Node_Str"" + ""String_Node_Str"",existingNamespaceMeta.getName(),existingConfig.getHiveDatabase()));
    }
    if (!Strings.isNullOrEmpty(metadata.getConfig().getRootDirectory())) {
      validatePath(metadata);
      if (hasSubDirRelationship(existingConfig.getRootDirectory(),metadata.getConfig().getRootDirectory())) {
        throw new NamespaceAlreadyExistsException(existingNamespaceMeta.getNamespaceId().toId(),String.format(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",metadata.getName(),metadata.getConfig().getRootDirectory(),existingNamespaceMeta.getName(),existingConfig.getRootDirectory()));
      }
    }
  }
}","private void validateCustomMapping(NamespaceMeta metadata) throws Exception {
  for (  NamespaceMeta existingNamespaceMeta : list()) {
    NamespaceConfig existingConfig=existingNamespaceMeta.getConfig();
    if (!Strings.isNullOrEmpty(metadata.getConfig().getHbaseNamespace()) && metadata.getConfig().getHbaseNamespace().equals(existingConfig.getHbaseNamespace())) {
      throw new NamespaceAlreadyExistsException(existingNamespaceMeta.getNamespaceId().toId(),String.format(""String_Node_Str"" + ""String_Node_Str"",existingNamespaceMeta.getName(),existingConfig.getHbaseNamespace()));
    }
    if (!Strings.isNullOrEmpty(metadata.getConfig().getHiveDatabase()) && metadata.getConfig().getHiveDatabase().equals(existingConfig.getHiveDatabase())) {
      throw new NamespaceAlreadyExistsException(existingNamespaceMeta.getNamespaceId().toId(),String.format(""String_Node_Str"" + ""String_Node_Str"",existingNamespaceMeta.getName(),existingConfig.getHiveDatabase()));
    }
    if (!Strings.isNullOrEmpty(metadata.getConfig().getRootDirectory())) {
      validatePath(metadata.getName(),metadata.getConfig().getRootDirectory());
      if (hasSubDirRelationship(existingConfig.getRootDirectory(),metadata.getConfig().getRootDirectory())) {
        throw new NamespaceAlreadyExistsException(existingNamespaceMeta.getNamespaceId().toId(),String.format(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",metadata.getName(),metadata.getConfig().getRootDirectory(),existingNamespaceMeta.getName(),existingConfig.getRootDirectory()));
      }
    }
  }
}"
5940,"private void validatePath(NamespaceMeta namespaceMeta) throws IOException {
  File customLocation=new File(namespaceMeta.getConfig().getRootDirectory());
  if (!customLocation.isAbsolute()) {
    throw new IOException(String.format(""String_Node_Str"",namespaceMeta.getName(),customLocation));
  }
}","private void validatePath(String namespace,String rootDir) throws IOException {
  File customLocation=new File(rootDir);
  if (!customLocation.isAbsolute()) {
    throw new IOException(String.format(""String_Node_Str"",namespace,customLocation));
  }
}"
5941,"@Inject DefaultNamespaceAdmin(Store store,NamespaceStore nsStore,PreferencesStore preferencesStore,DashboardStore dashboardStore,DatasetFramework dsFramework,ProgramRuntimeService runtimeService,QueueAdmin queueAdmin,StreamAdmin streamAdmin,MetricStore metricStore,Scheduler scheduler,ApplicationLifecycleService applicationLifecycleService,ArtifactRepository artifactRepository,AuthorizerInstantiator authorizerInstantiator,CConfiguration cConf,StorageProviderNamespaceAdmin storageProviderNamespaceAdmin,Impersonator impersonator,AuthorizationEnforcer authorizationEnforcer,AuthenticationContext authenticationContext){
  super(nsStore,authorizationEnforcer,authenticationContext);
  this.queueAdmin=queueAdmin;
  this.streamAdmin=streamAdmin;
  this.store=store;
  this.preferencesStore=preferencesStore;
  this.dashboardStore=dashboardStore;
  this.dsFramework=dsFramework;
  this.runtimeService=runtimeService;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
  this.applicationLifecycleService=applicationLifecycleService;
  this.artifactRepository=artifactRepository;
  this.authorizer=authorizerInstantiator.get();
  this.authorizationEnforcer=authorizationEnforcer;
  this.instanceId=createInstanceId(cConf);
  this.storageProviderNamespaceAdmin=storageProviderNamespaceAdmin;
  this.impersonator=impersonator;
  this.cConf=cConf;
}","@Inject DefaultNamespaceAdmin(Store store,NamespaceStore nsStore,PreferencesStore preferencesStore,DashboardStore dashboardStore,DatasetFramework dsFramework,ProgramRuntimeService runtimeService,QueueAdmin queueAdmin,StreamAdmin streamAdmin,MetricStore metricStore,Scheduler scheduler,ApplicationLifecycleService applicationLifecycleService,ArtifactRepository artifactRepository,PrivilegesManager privilegesManager,CConfiguration cConf,StorageProviderNamespaceAdmin storageProviderNamespaceAdmin,Impersonator impersonator,AuthorizationEnforcer authorizationEnforcer,AuthenticationContext authenticationContext){
  super(nsStore,authorizationEnforcer,authenticationContext);
  this.queueAdmin=queueAdmin;
  this.streamAdmin=streamAdmin;
  this.store=store;
  this.preferencesStore=preferencesStore;
  this.dashboardStore=dashboardStore;
  this.dsFramework=dsFramework;
  this.runtimeService=runtimeService;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
  this.applicationLifecycleService=applicationLifecycleService;
  this.artifactRepository=artifactRepository;
  this.privilegesManager=privilegesManager;
  this.authorizationEnforcer=authorizationEnforcer;
  this.instanceId=createInstanceId(cConf);
  this.storageProviderNamespaceAdmin=storageProviderNamespaceAdmin;
  this.impersonator=impersonator;
  this.cConf=cConf;
}"
5942,"/** 
 * Deletes the specified namespace
 * @param namespaceId the {@link Id.Namespace} of the specified namespace
 * @throws NamespaceCannotBeDeletedException if the specified namespace cannot be deleted
 * @throws NamespaceNotFoundException if the specified namespace does not exist
 */
@Override public synchronized void delete(final Id.Namespace namespaceId) throws Exception {
  NamespaceId namespace=namespaceId.toEntityId();
  if (!exists(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (checkProgramsRunning(namespaceId.toEntityId())) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  authorizationEnforcer.enforce(namespace,authenticationContext.getPrincipal(),Action.ADMIN);
  LOG.info(""String_Node_Str"",namespaceId);
  try {
    preferencesStore.deleteProperties(namespaceId.getId());
    dashboardStore.delete(namespaceId.getId());
    applicationLifecycleService.removeAll(namespaceId);
    scheduler.deleteAllSchedules(namespaceId);
    dsFramework.deleteAllInstances(namespaceId);
    dsFramework.deleteAllModules(namespaceId);
    queueAdmin.dropAllInNamespace(namespaceId);
    streamAdmin.dropAllInNamespace(namespaceId);
    store.removeAll(namespaceId);
    deleteMetrics(namespaceId.toEntityId());
    artifactRepository.clear(namespaceId.toEntityId());
    if (!Id.Namespace.DEFAULT.equals(namespaceId)) {
      try {
        impersonator.doAs(namespace,new Callable<Void>(){
          @Override public Void call() throws Exception {
            storageProviderNamespaceAdmin.delete(namespaceId.toEntityId());
            return null;
          }
        }
);
      }
  finally {
        nsStore.delete(namespaceId);
      }
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
 finally {
    authorizer.revoke(namespace);
  }
  LOG.info(""String_Node_Str"",namespaceId);
  authorizer.revoke(namespace);
}","/** 
 * Deletes the specified namespace
 * @param namespaceId the {@link Id.Namespace} of the specified namespace
 * @throws NamespaceCannotBeDeletedException if the specified namespace cannot be deleted
 * @throws NamespaceNotFoundException if the specified namespace does not exist
 */
@Override public synchronized void delete(final Id.Namespace namespaceId) throws Exception {
  NamespaceId namespace=namespaceId.toEntityId();
  if (!exists(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (checkProgramsRunning(namespaceId.toEntityId())) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  authorizationEnforcer.enforce(namespace,authenticationContext.getPrincipal(),Action.ADMIN);
  LOG.info(""String_Node_Str"",namespaceId);
  try {
    preferencesStore.deleteProperties(namespaceId.getId());
    dashboardStore.delete(namespaceId.getId());
    applicationLifecycleService.removeAll(namespaceId);
    scheduler.deleteAllSchedules(namespaceId);
    dsFramework.deleteAllInstances(namespaceId);
    dsFramework.deleteAllModules(namespaceId);
    queueAdmin.dropAllInNamespace(namespaceId);
    streamAdmin.dropAllInNamespace(namespaceId);
    store.removeAll(namespaceId);
    deleteMetrics(namespaceId.toEntityId());
    artifactRepository.clear(namespaceId.toEntityId());
    if (!Id.Namespace.DEFAULT.equals(namespaceId)) {
      try {
        impersonator.doAs(namespace,new Callable<Void>(){
          @Override public Void call() throws Exception {
            storageProviderNamespaceAdmin.delete(namespaceId.toEntityId());
            return null;
          }
        }
);
      }
  finally {
        nsStore.delete(namespaceId);
      }
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
 finally {
    privilegesManager.revoke(namespace);
  }
  LOG.info(""String_Node_Str"",namespaceId);
  privilegesManager.revoke(namespace);
}"
5943,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(instanceId,principal,Action.ADMIN);
  authorizer.grant(namespace,principal,ImmutableSet.of(Action.ALL));
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    ImpersonationInfo impersonationInfo=new ImpersonationInfo(metadata,cConf);
    String namespacePrincipal=impersonationInfo.getPrincipal();
    String namespaceUserName=new KerberosName(namespacePrincipal).getShortName();
    Principal namespaceUser=new Principal(namespaceUserName,Principal.PrincipalType.USER);
    authorizer.grant(namespace,namespaceUser,EnumSet.allOf(Action.class));
  }
  nsStore.create(metadata);
  try {
    impersonator.doAs(metadata,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.create(metadata);
        return null;
      }
    }
);
  }
 catch (  IOException|ExploreException|SQLException e) {
    nsStore.delete(metadata.getNamespaceId().toId());
    authorizer.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace.toId(),e);
  }
}","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(instanceId,principal,Action.ADMIN);
  privilegesManager.grant(namespace,principal,ImmutableSet.of(Action.ALL));
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    ImpersonationInfo impersonationInfo=new ImpersonationInfo(metadata,cConf);
    String namespacePrincipal=impersonationInfo.getPrincipal();
    String namespaceUserName=new KerberosName(namespacePrincipal).getShortName();
    Principal namespaceUser=new Principal(namespaceUserName,Principal.PrincipalType.USER);
    privilegesManager.grant(namespace,namespaceUser,EnumSet.allOf(Action.class));
  }
  nsStore.create(metadata);
  try {
    impersonator.doAs(metadata,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.create(metadata);
        return null;
      }
    }
);
  }
 catch (  IOException|ExploreException|SQLException e) {
    nsStore.delete(metadata.getNamespaceId().toId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace.toId(),e);
  }
}"
5944,"@Override public void process(final ApplicationDeployable input) throws Exception {
  List<ProgramDescriptor> programDescriptors=new ArrayList<>();
  final ApplicationSpecification appSpec=input.getSpecification();
  Iterable<ProgramSpecification> specifications=Iterables.concat(appSpec.getMapReduce().values(),appSpec.getFlows().values(),appSpec.getWorkflows().values(),appSpec.getServices().values(),appSpec.getSpark().values(),appSpec.getWorkers().values());
  for (  final ProgramSpecification spec : specifications) {
    ProgramType type=ProgramTypes.fromSpecification(spec);
    ProgramId programId=input.getApplicationId().program(type,spec.getName());
    privilegesManager.grant(programId,authenticationContext.getPrincipal(),ImmutableSet.of(Action.ALL));
    programDescriptors.add(new ProgramDescriptor(programId,appSpec));
  }
  emit(new ApplicationWithPrograms(input,programDescriptors));
}","@Override public void process(final ApplicationDeployable input) throws Exception {
  List<ProgramDescriptor> programDescriptors=new ArrayList<>();
  final ApplicationSpecification appSpec=input.getSpecification();
  Iterable<ProgramSpecification> specifications=Iterables.concat(appSpec.getMapReduce().values(),appSpec.getFlows().values(),appSpec.getWorkflows().values(),appSpec.getServices().values(),appSpec.getSpark().values(),appSpec.getWorkers().values());
  for (  ProgramSpecification spec : specifications) {
    ProgramType type=ProgramTypes.fromSpecification(spec);
    ProgramId programId=input.getApplicationId().program(type,spec.getName());
    privilegesManager.grant(programId,authenticationContext.getPrincipal(),EnumSet.allOf(Action.class));
    programDescriptors.add(new ProgramDescriptor(programId,appSpec));
  }
  emit(new ApplicationWithPrograms(input,programDescriptors));
}"
5945,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(instanceId,principal,Action.ADMIN);
  privilegesManager.grant(namespace,principal,ImmutableSet.of(Action.ALL));
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    ImpersonationInfo impersonationInfo=new ImpersonationInfo(metadata,cConf);
    String namespacePrincipal=impersonationInfo.getPrincipal();
    String namespaceUserName=new KerberosName(namespacePrincipal).getShortName();
    Principal namespaceUser=new Principal(namespaceUserName,Principal.PrincipalType.USER);
    privilegesManager.grant(namespace,namespaceUser,EnumSet.allOf(Action.class));
  }
  nsStore.create(metadata);
  try {
    impersonator.doAs(metadata,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.create(metadata);
        return null;
      }
    }
);
  }
 catch (  IOException|ExploreException|SQLException e) {
    nsStore.delete(metadata.getNamespaceId().toId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace.toId(),e);
  }
}","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(instanceId,principal,Action.ADMIN);
  privilegesManager.grant(namespace,principal,EnumSet.allOf(Action.class));
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    ImpersonationInfo impersonationInfo=new ImpersonationInfo(metadata,cConf);
    String namespacePrincipal=impersonationInfo.getPrincipal();
    String namespaceUserName=new KerberosName(namespacePrincipal).getShortName();
    Principal namespaceUser=new Principal(namespaceUserName,Principal.PrincipalType.USER);
    privilegesManager.grant(namespace,namespaceUser,EnumSet.allOf(Action.class));
  }
  nsStore.create(metadata);
  try {
    impersonator.doAs(metadata,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.create(metadata);
        return null;
      }
    }
);
  }
 catch (  IOException|ExploreException|SQLException e) {
    nsStore.delete(metadata.getNamespaceId().toId());
    privilegesManager.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace.toId(),e);
  }
}"
5946,"/** 
 * Scan all files in the local system artifact directory, looking for jar files and adding them as system artifacts. If the artifact already exists it will not be added again unless it is a snapshot version.
 * @throws IOException if there was some IO error adding the system artifacts
 */
public void addSystemArtifacts() throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(NamespaceId.SYSTEM,principal,Action.WRITE);
  List<SystemArtifactInfo> systemArtifacts=new ArrayList<>();
  for (  File systemArtifactDir : systemArtifactDirs) {
    for (    File jarFile : DirUtils.listFiles(systemArtifactDir,""String_Node_Str"")) {
      Id.Artifact artifactId;
      try {
        artifactId=Id.Artifact.parse(Id.Namespace.SYSTEM,jarFile.getName());
      }
 catch (      IllegalArgumentException e) {
        LOG.warn(String.format(""String_Node_Str"",e.getMessage()));
        continue;
      }
      co.cask.cdap.proto.id.ArtifactId artifact=artifactId.toEntityId();
      privilegesManager.revoke(artifact);
      privilegesManager.grant(artifact,principal,Collections.singleton(Action.ALL));
      String artifactFileName=jarFile.getName();
      String configFileName=artifactFileName.substring(0,artifactFileName.length() - ""String_Node_Str"".length()) + ""String_Node_Str"";
      File configFile=new File(systemArtifactDir,configFileName);
      try {
        ArtifactConfig artifactConfig=configFile.isFile() ? configReader.read(artifactId.getNamespace(),configFile) : new ArtifactConfig();
        validateParentSet(artifactId,artifactConfig.getParents());
        validatePluginSet(artifactConfig.getPlugins());
        systemArtifacts.add(new SystemArtifactInfo(artifactId,jarFile,artifactConfig));
      }
 catch (      InvalidArtifactException e) {
        LOG.warn(String.format(""String_Node_Str"",artifactFileName),e);
        privilegesManager.revoke(artifact);
      }
    }
  }
  Set<Id.Artifact> parents=new HashSet<>();
  for (  SystemArtifactInfo child : systemArtifacts) {
    Id.Artifact childId=child.getArtifactId();
    for (    SystemArtifactInfo potentialParent : systemArtifacts) {
      Id.Artifact potentialParentId=potentialParent.getArtifactId();
      if (childId.equals(potentialParentId)) {
        continue;
      }
      if (child.getConfig().hasParent(potentialParentId)) {
        parents.add(potentialParentId);
      }
    }
  }
  for (  SystemArtifactInfo systemArtifact : systemArtifacts) {
    if (parents.contains(systemArtifact.getArtifactId())) {
      addSystemArtifact(systemArtifact);
    }
  }
  for (  SystemArtifactInfo systemArtifact : systemArtifacts) {
    if (!parents.contains(systemArtifact.getArtifactId())) {
      addSystemArtifact(systemArtifact);
    }
  }
}","/** 
 * Scan all files in the local system artifact directory, looking for jar files and adding them as system artifacts. If the artifact already exists it will not be added again unless it is a snapshot version.
 * @throws IOException if there was some IO error adding the system artifacts
 */
public void addSystemArtifacts() throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(NamespaceId.SYSTEM,principal,Action.WRITE);
  List<SystemArtifactInfo> systemArtifacts=new ArrayList<>();
  for (  File systemArtifactDir : systemArtifactDirs) {
    for (    File jarFile : DirUtils.listFiles(systemArtifactDir,""String_Node_Str"")) {
      Id.Artifact artifactId;
      try {
        artifactId=Id.Artifact.parse(Id.Namespace.SYSTEM,jarFile.getName());
      }
 catch (      IllegalArgumentException e) {
        LOG.warn(String.format(""String_Node_Str"",e.getMessage()));
        continue;
      }
      co.cask.cdap.proto.id.ArtifactId artifact=artifactId.toEntityId();
      privilegesManager.revoke(artifact);
      privilegesManager.grant(artifact,principal,EnumSet.allOf(Action.class));
      String artifactFileName=jarFile.getName();
      String configFileName=artifactFileName.substring(0,artifactFileName.length() - ""String_Node_Str"".length()) + ""String_Node_Str"";
      File configFile=new File(systemArtifactDir,configFileName);
      try {
        ArtifactConfig artifactConfig=configFile.isFile() ? configReader.read(artifactId.getNamespace(),configFile) : new ArtifactConfig();
        validateParentSet(artifactId,artifactConfig.getParents());
        validatePluginSet(artifactConfig.getPlugins());
        systemArtifacts.add(new SystemArtifactInfo(artifactId,jarFile,artifactConfig));
      }
 catch (      InvalidArtifactException e) {
        LOG.warn(String.format(""String_Node_Str"",artifactFileName),e);
        privilegesManager.revoke(artifact);
      }
    }
  }
  Set<Id.Artifact> parents=new HashSet<>();
  for (  SystemArtifactInfo child : systemArtifacts) {
    Id.Artifact childId=child.getArtifactId();
    for (    SystemArtifactInfo potentialParent : systemArtifacts) {
      Id.Artifact potentialParentId=potentialParent.getArtifactId();
      if (childId.equals(potentialParentId)) {
        continue;
      }
      if (child.getConfig().hasParent(potentialParentId)) {
        parents.add(potentialParentId);
      }
    }
  }
  for (  SystemArtifactInfo systemArtifact : systemArtifacts) {
    if (parents.contains(systemArtifact.getArtifactId())) {
      addSystemArtifact(systemArtifact);
    }
  }
  for (  SystemArtifactInfo systemArtifact : systemArtifacts) {
    if (!parents.contains(systemArtifact.getArtifactId())) {
      addSystemArtifact(systemArtifact);
    }
  }
}"
5947,"/** 
 * Inspects and builds plugin and application information for the given artifact, adding an additional set of plugin classes to the plugins found through inspection. This method is used when all plugin classes cannot be derived by inspecting the artifact but need to be explicitly set. This is true for 3rd party plugins like jdbc drivers.
 * @param artifactId the id of the artifact to inspect and store
 * @param artifactFile the artifact to inspect and store
 * @param parentArtifacts artifacts the given artifact extends.If null, the given artifact does not extend another artifact
 * @param additionalPlugins the set of additional plugin classes to add to the plugins found through inspection.If null, no additional plugin classes will be added
 * @param properties properties for the artifact
 * @throws IOException if there was an exception reading from the artifact store
 * @throws ArtifactRangeNotFoundException if none of the parent artifacts could be found
 * @throws UnauthorizedException if the user is not authorized to add an artifact in the specified namespace. To addan artifact, a user must have  {@link Action#WRITE} on the namespace in whichthe artifact is being added. If authorization is successful, and the artifact is added successfully, then the user gets  {@link Action#ALL} privilegeson the added artifact.
 */
@VisibleForTesting public ArtifactDetail addArtifact(final Id.Artifact artifactId,final File artifactFile,@Nullable Set<ArtifactRange> parentArtifacts,@Nullable Set<PluginClass> additionalPlugins,Map<String,String> properties) throws Exception {
  if (additionalPlugins != null) {
    validatePluginSet(additionalPlugins);
  }
  parentArtifacts=parentArtifacts == null ? Collections.<ArtifactRange>emptySet() : parentArtifacts;
  CloseableClassLoader parentClassLoader;
  NamespacedImpersonator namespacedImpersonator=new NamespacedImpersonator(artifactId.getNamespace().toEntityId(),impersonator);
  if (parentArtifacts.isEmpty()) {
    parentClassLoader=createArtifactClassLoader(Locations.toLocation(artifactFile),namespacedImpersonator);
  }
 else {
    validateParentSet(artifactId,parentArtifacts);
    parentClassLoader=createParentClassLoader(artifactId,parentArtifacts,namespacedImpersonator);
  }
  try {
    ArtifactClasses artifactClasses=inspectArtifact(artifactId,artifactFile,additionalPlugins,parentClassLoader);
    ArtifactMeta meta=new ArtifactMeta(artifactClasses,parentArtifacts,properties);
    ArtifactDetail artifactDetail=artifactStore.write(artifactId,meta,Files.newInputStreamSupplier(artifactFile),namespacedImpersonator);
    ArtifactDescriptor descriptor=artifactDetail.getDescriptor();
    ArtifactInfo artifactInfo=new ArtifactInfo(descriptor.getArtifactId(),artifactDetail.getMeta().getClasses(),artifactDetail.getMeta().getProperties());
    writeSystemMetadata(artifactId,artifactInfo);
    return artifactDetail;
  }
  finally {
    parentClassLoader.close();
  }
}","/** 
 * Inspects and builds plugin and application information for the given artifact, adding an additional set of plugin classes to the plugins found through inspection. This method is used when all plugin classes cannot be derived by inspecting the artifact but need to be explicitly set. This is true for 3rd party plugins like jdbc drivers.
 * @param artifactId the id of the artifact to inspect and store
 * @param artifactFile the artifact to inspect and store
 * @param parentArtifacts artifacts the given artifact extends.If null, the given artifact does not extend another artifact
 * @param additionalPlugins the set of additional plugin classes to add to the plugins found through inspection.If null, no additional plugin classes will be added
 * @param properties properties for the artifact
 * @throws IOException if there was an exception reading from the artifact store
 * @throws ArtifactRangeNotFoundException if none of the parent artifacts could be found
 * @throws UnauthorizedException if the user is not authorized to add an artifact in the specified namespace. To addan artifact, a user must have  {@link Action#WRITE} on the namespace in whichthe artifact is being added. If authorization is successful, and the artifact is added successfully, then the user gets all  {@link Action privileges}on the added artifact.
 */
@VisibleForTesting public ArtifactDetail addArtifact(final Id.Artifact artifactId,final File artifactFile,@Nullable Set<ArtifactRange> parentArtifacts,@Nullable Set<PluginClass> additionalPlugins,Map<String,String> properties) throws Exception {
  if (additionalPlugins != null) {
    validatePluginSet(additionalPlugins);
  }
  parentArtifacts=parentArtifacts == null ? Collections.<ArtifactRange>emptySet() : parentArtifacts;
  CloseableClassLoader parentClassLoader;
  NamespacedImpersonator namespacedImpersonator=new NamespacedImpersonator(artifactId.getNamespace().toEntityId(),impersonator);
  if (parentArtifacts.isEmpty()) {
    parentClassLoader=createArtifactClassLoader(Locations.toLocation(artifactFile),namespacedImpersonator);
  }
 else {
    validateParentSet(artifactId,parentArtifacts);
    parentClassLoader=createParentClassLoader(artifactId,parentArtifacts,namespacedImpersonator);
  }
  try {
    ArtifactClasses artifactClasses=inspectArtifact(artifactId,artifactFile,additionalPlugins,parentClassLoader);
    ArtifactMeta meta=new ArtifactMeta(artifactClasses,parentArtifacts,properties);
    ArtifactDetail artifactDetail=artifactStore.write(artifactId,meta,Files.newInputStreamSupplier(artifactFile),namespacedImpersonator);
    ArtifactDescriptor descriptor=artifactDetail.getDescriptor();
    ArtifactInfo artifactInfo=new ArtifactInfo(descriptor.getArtifactId(),artifactDetail.getMeta().getClasses(),artifactDetail.getMeta().getProperties());
    writeSystemMetadata(artifactId,artifactInfo);
    return artifactDetail;
  }
  finally {
    parentClassLoader.close();
  }
}"
5948,"private ApplicationWithPrograms deployApp(NamespaceId namespaceId,@Nullable String appName,@Nullable String configStr,ProgramTerminator programTerminator,ArtifactDetail artifactDetail) throws Exception {
  authorizationEnforcer.enforce(namespaceId,authenticationContext.getPrincipal(),Action.WRITE);
  ApplicationClass appClass=Iterables.getFirst(artifactDetail.getMeta().getClasses().getApps(),null);
  if (appClass == null) {
    throw new InvalidArtifactException(String.format(""String_Node_Str"",artifactDetail.getDescriptor().getArtifactId(),namespaceId));
  }
  AppDeploymentInfo deploymentInfo=new AppDeploymentInfo(artifactDetail.getDescriptor(),namespaceId,appClass.getClassName(),appName,configStr);
  Manager<AppDeploymentInfo,ApplicationWithPrograms> manager=managerFactory.create(programTerminator);
  ApplicationWithPrograms applicationWithPrograms=manager.deploy(deploymentInfo).get();
  privilegesManager.grant(applicationWithPrograms.getApplicationId(),authenticationContext.getPrincipal(),ImmutableSet.of(Action.ALL));
  return applicationWithPrograms;
}","private ApplicationWithPrograms deployApp(NamespaceId namespaceId,@Nullable String appName,@Nullable String configStr,ProgramTerminator programTerminator,ArtifactDetail artifactDetail) throws Exception {
  authorizationEnforcer.enforce(namespaceId,authenticationContext.getPrincipal(),Action.WRITE);
  ApplicationClass appClass=Iterables.getFirst(artifactDetail.getMeta().getClasses().getApps(),null);
  if (appClass == null) {
    throw new InvalidArtifactException(String.format(""String_Node_Str"",artifactDetail.getDescriptor().getArtifactId(),namespaceId));
  }
  AppDeploymentInfo deploymentInfo=new AppDeploymentInfo(artifactDetail.getDescriptor(),namespaceId,appClass.getClassName(),appName,configStr);
  Manager<AppDeploymentInfo,ApplicationWithPrograms> manager=managerFactory.create(programTerminator);
  ApplicationWithPrograms applicationWithPrograms=manager.deploy(deploymentInfo).get();
  privilegesManager.grant(applicationWithPrograms.getApplicationId(),authenticationContext.getPrincipal(),EnumSet.allOf(Action.class));
  return applicationWithPrograms;
}"
5949,"/** 
 * Puts the user provided data in the secure store, if the user has write access to the namespace. Grants the user all access to the newly created entity.
 * @param secureKeyId The Id for the key that needs to be stored.
 * @param secureKeyCreateRequest The request containing the data to be stored in the secure store.
 * @throws BadRequestException If the request does not contain the value to be stored.
 * @throws UnauthorizedException If the user does not have write permissions on the namespace.
 * @throws NamespaceNotFoundException If the specified namespace does not exist.
 * @throws AlreadyExistsException If the key already exists in the namespace. Updating is not supported.
 * @throws IOException If there was a problem storing the key to underlying provider.
 */
@Override public synchronized void put(SecureKeyId secureKeyId,SecureKeyCreateRequest secureKeyCreateRequest) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  NamespaceId namespaceId=new NamespaceId(secureKeyId.getNamespace());
  authorizationEnforcer.enforce(namespaceId,principal,Action.WRITE);
  String description=secureKeyCreateRequest.getDescription();
  String value=secureKeyCreateRequest.getData();
  if (Strings.isNullOrEmpty(value)) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  secureStoreManager.putSecureData(secureKeyId.getNamespace(),secureKeyId.getName(),value,description,secureKeyCreateRequest.getProperties());
  authorizer.grant(secureKeyId,principal,ImmutableSet.of(Action.ALL));
}","/** 
 * Puts the user provided data in the secure store, if the user has write access to the namespace. Grants the user all access to the newly created entity.
 * @param secureKeyId The Id for the key that needs to be stored.
 * @param secureKeyCreateRequest The request containing the data to be stored in the secure store.
 * @throws BadRequestException If the request does not contain the value to be stored.
 * @throws UnauthorizedException If the user does not have write permissions on the namespace.
 * @throws NamespaceNotFoundException If the specified namespace does not exist.
 * @throws AlreadyExistsException If the key already exists in the namespace. Updating is not supported.
 * @throws IOException If there was a problem storing the key to underlying provider.
 */
@Override public synchronized void put(SecureKeyId secureKeyId,SecureKeyCreateRequest secureKeyCreateRequest) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  NamespaceId namespaceId=new NamespaceId(secureKeyId.getNamespace());
  authorizationEnforcer.enforce(namespaceId,principal,Action.WRITE);
  String description=secureKeyCreateRequest.getDescription();
  String value=secureKeyCreateRequest.getData();
  if (Strings.isNullOrEmpty(value)) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  secureStoreManager.putSecureData(secureKeyId.getNamespace(),secureKeyId.getName(),value,description,secureKeyCreateRequest.getProperties());
  authorizer.grant(secureKeyId,principal,EnumSet.allOf(Action.class));
}"
5950,"@Test public void testAuthorizationForPrivileges() throws Exception {
  Principal bob=new Principal(""String_Node_Str"",Principal.PrincipalType.USER);
  Principal alice=new Principal(""String_Node_Str"",Principal.PrincipalType.USER);
  String oldUser=getCurrentUser();
  setCurrentUser(alice.getName());
  try {
    try {
      client.grant(ns1,bob,ImmutableSet.of(Action.ALL));
      Assert.fail(String.format(""String_Node_Str"" + ""String_Node_Str"",ns1));
    }
 catch (    UnauthorizedException expected) {
    }
    setCurrentUser(oldUser);
    client.grant(ns1,alice,ImmutableSet.of(Action.ADMIN));
    setCurrentUser(alice.getName());
    client.grant(ns1,bob,ImmutableSet.of(Action.ALL));
    setCurrentUser(oldUser);
    client.revoke(ns1);
    setCurrentUser(alice.getName());
    try {
      client.revoke(ns1,bob,ImmutableSet.of(Action.ALL));
      Assert.fail(String.format(""String_Node_Str"" + ""String_Node_Str"",ns1));
    }
 catch (    UnauthorizedException expected) {
    }
    setCurrentUser(oldUser);
    client.grant(ns1,alice,ImmutableSet.of(Action.ALL));
    setCurrentUser(alice.getName());
    client.revoke(ns1,bob,ImmutableSet.of(Action.ALL));
  }
  finally {
    setCurrentUser(oldUser);
  }
}","@Test public void testAuthorizationForPrivileges() throws Exception {
  Principal bob=new Principal(""String_Node_Str"",Principal.PrincipalType.USER);
  Principal alice=new Principal(""String_Node_Str"",Principal.PrincipalType.USER);
  String oldUser=getCurrentUser();
  setCurrentUser(alice.getName());
  try {
    try {
      client.grant(ns1,bob,EnumSet.allOf(Action.class));
      Assert.fail(String.format(""String_Node_Str"" + ""String_Node_Str"",ns1));
    }
 catch (    UnauthorizedException expected) {
    }
    setCurrentUser(oldUser);
    client.grant(ns1,alice,ImmutableSet.of(Action.ADMIN));
    setCurrentUser(alice.getName());
    client.grant(ns1,bob,EnumSet.allOf(Action.class));
    setCurrentUser(oldUser);
    client.revoke(ns1);
    setCurrentUser(alice.getName());
    try {
      client.revoke(ns1,bob,EnumSet.allOf(Action.class));
      Assert.fail(String.format(""String_Node_Str"" + ""String_Node_Str"",ns1));
    }
 catch (    UnauthorizedException expected) {
    }
    setCurrentUser(oldUser);
    client.grant(ns1,alice,EnumSet.allOf(Action.class));
    setCurrentUser(alice.getName());
    client.revoke(ns1,bob,EnumSet.allOf(Action.class));
  }
  finally {
    setCurrentUser(oldUser);
  }
}"
5951,"@Test public void testAuthorizationForSystemArtifacts() throws Exception {
  SecurityRequestContext.setUserId(systemUser);
  artifactRepository.addSystemArtifacts();
  SecurityRequestContext.setUserId(ALICE.getName());
  try {
    artifactRepository.addSystemArtifacts();
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  authorizer.grant(NamespaceId.SYSTEM,ALICE,Collections.singleton(Action.WRITE));
  Assert.assertEquals(Collections.singleton(new Privilege(NamespaceId.SYSTEM,Action.WRITE)),authorizer.listPrivileges(ALICE));
  artifactRepository.addSystemArtifacts();
  SecurityRequestContext.setUserId(""String_Node_Str"");
  try {
    artifactRepository.deleteArtifact(SYSTEM_ARTIFACT.toId());
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  SecurityRequestContext.setUserId(ALICE.getName());
  authorizer.grant(instance,ALICE,Collections.singleton(Action.ADMIN));
  NamespaceId namespaceId=new NamespaceId(""String_Node_Str"");
  namespaceAdmin.create(new NamespaceMeta.Builder().setName(namespaceId.getNamespace()).build());
  authorizer.revoke(instance);
  List<ArtifactSummary> artifacts=artifactRepository.getArtifacts(namespaceId,true);
  Assert.assertEquals(1,artifacts.size());
  ArtifactSummary artifactSummary=artifacts.get(0);
  Assert.assertEquals(SYSTEM_ARTIFACT.getArtifact(),artifactSummary.getName());
  Assert.assertEquals(SYSTEM_ARTIFACT.getVersion(),artifactSummary.getVersion());
  Assert.assertEquals(SYSTEM_ARTIFACT.getNamespace(),artifactSummary.getScope().name().toLowerCase());
  ArtifactDetail artifactDetail=artifactRepository.getArtifact(SYSTEM_ARTIFACT.toId());
  co.cask.cdap.api.artifact.ArtifactId artifactId=artifactDetail.getDescriptor().getArtifactId();
  Assert.assertEquals(SYSTEM_ARTIFACT.getArtifact(),artifactId.getName());
  Assert.assertEquals(SYSTEM_ARTIFACT.getVersion(),artifactId.getVersion().getVersion());
  Assert.assertEquals(SYSTEM_ARTIFACT.getNamespace(),artifactId.getScope().name().toLowerCase());
  namespaceAdmin.delete(namespaceId.toId());
  Assert.assertEquals(ImmutableSet.of(new Privilege(SYSTEM_ARTIFACT,Action.ALL),new Privilege(NamespaceId.SYSTEM,Action.WRITE)),authorizer.listPrivileges(ALICE));
  artifactRepository.deleteArtifact(SYSTEM_ARTIFACT.toId());
}","@Test public void testAuthorizationForSystemArtifacts() throws Exception {
  SecurityRequestContext.setUserId(systemUser);
  artifactRepository.addSystemArtifacts();
  SecurityRequestContext.setUserId(ALICE.getName());
  try {
    artifactRepository.addSystemArtifacts();
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  authorizer.grant(NamespaceId.SYSTEM,ALICE,Collections.singleton(Action.WRITE));
  Assert.assertEquals(Collections.singleton(new Privilege(NamespaceId.SYSTEM,Action.WRITE)),authorizer.listPrivileges(ALICE));
  artifactRepository.addSystemArtifacts();
  SecurityRequestContext.setUserId(""String_Node_Str"");
  try {
    artifactRepository.deleteArtifact(SYSTEM_ARTIFACT.toId());
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  SecurityRequestContext.setUserId(ALICE.getName());
  authorizer.grant(instance,ALICE,Collections.singleton(Action.ADMIN));
  NamespaceId namespaceId=new NamespaceId(""String_Node_Str"");
  namespaceAdmin.create(new NamespaceMeta.Builder().setName(namespaceId.getNamespace()).build());
  authorizer.revoke(instance);
  List<ArtifactSummary> artifacts=artifactRepository.getArtifacts(namespaceId,true);
  Assert.assertEquals(1,artifacts.size());
  ArtifactSummary artifactSummary=artifacts.get(0);
  Assert.assertEquals(SYSTEM_ARTIFACT.getArtifact(),artifactSummary.getName());
  Assert.assertEquals(SYSTEM_ARTIFACT.getVersion(),artifactSummary.getVersion());
  Assert.assertEquals(SYSTEM_ARTIFACT.getNamespace(),artifactSummary.getScope().name().toLowerCase());
  ArtifactDetail artifactDetail=artifactRepository.getArtifact(SYSTEM_ARTIFACT.toId());
  co.cask.cdap.api.artifact.ArtifactId artifactId=artifactDetail.getDescriptor().getArtifactId();
  Assert.assertEquals(SYSTEM_ARTIFACT.getArtifact(),artifactId.getName());
  Assert.assertEquals(SYSTEM_ARTIFACT.getVersion(),artifactId.getVersion().getVersion());
  Assert.assertEquals(SYSTEM_ARTIFACT.getNamespace(),artifactId.getScope().name().toLowerCase());
  namespaceAdmin.delete(namespaceId.toId());
  authorizer.enforce(SYSTEM_ARTIFACT,ALICE,EnumSet.allOf(Action.class));
  authorizer.enforce(NamespaceId.SYSTEM,ALICE,Action.WRITE);
  artifactRepository.deleteArtifact(SYSTEM_ARTIFACT.toId());
}"
5952,"@Test public void testPrivilegesManager() throws Exception {
  privilegesManager.grant(NS,ALICE,Collections.singleton(Action.ALL));
  privilegesManager.grant(APP,ALICE,Collections.singleton(Action.ADMIN));
  privilegesManager.grant(PROGRAM,ALICE,Collections.singleton(Action.EXECUTE));
  authorizer.enforce(NS,ALICE,Action.ALL);
  authorizer.enforce(APP,ALICE,Action.ADMIN);
  authorizer.enforce(PROGRAM,ALICE,Action.EXECUTE);
  try {
    authorizer.enforce(APP,ALICE,Action.ALL);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  privilegesManager.revoke(PROGRAM);
  privilegesManager.revoke(APP,ALICE,EnumSet.allOf(Action.class));
  privilegesManager.revoke(NS,ALICE,Collections.singleton(Action.ALL));
  Set<Privilege> privileges=authorizer.listPrivileges(ALICE);
  Assert.assertTrue(String.format(""String_Node_Str"",privileges),privileges.isEmpty());
}","@Test public void testPrivilegesManager() throws Exception {
  privilegesManager.grant(NS,ALICE,EnumSet.allOf(Action.class));
  privilegesManager.grant(APP,ALICE,Collections.singleton(Action.ADMIN));
  privilegesManager.grant(PROGRAM,ALICE,Collections.singleton(Action.EXECUTE));
  authorizer.enforce(NS,ALICE,EnumSet.allOf(Action.class));
  authorizer.enforce(APP,ALICE,Action.ADMIN);
  authorizer.enforce(PROGRAM,ALICE,Action.EXECUTE);
  try {
    authorizer.enforce(APP,ALICE,EnumSet.allOf(Action.class));
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  privilegesManager.revoke(PROGRAM);
  privilegesManager.revoke(APP,ALICE,EnumSet.allOf(Action.class));
  privilegesManager.revoke(NS,ALICE,EnumSet.allOf(Action.class));
  Set<Privilege> privileges=authorizer.listPrivileges(ALICE);
  Assert.assertTrue(String.format(""String_Node_Str"",privileges),privileges.isEmpty());
}"
5953,"@Test public void testAuditPublish() throws Exception {
  grantAndAssertSuccess(new NamespaceId(FOO_NAMESPACE),USER,EnumSet.of(Action.ALL));
  getInMemoryAuditPublisher().popMessages();
  final List<AuditMessage> expectedMessages=new ArrayList<>();
  StreamAdmin streamAdmin=getStreamAdmin();
  Id.Stream stream1=Id.Stream.from(FOO_NAMESPACE,""String_Node_Str"");
  streamAdmin.create(stream1);
  expectedMessages.add(new AuditMessage(0,stream1.toEntityId(),""String_Node_Str"",AuditType.CREATE,AuditPayload.EMPTY_PAYLOAD));
  Id.Stream stream2=Id.Stream.from(FOO_NAMESPACE,""String_Node_Str"");
  streamAdmin.create(stream2);
  expectedMessages.add(new AuditMessage(0,stream2.toEntityId(),""String_Node_Str"",AuditType.CREATE,AuditPayload.EMPTY_PAYLOAD));
  streamAdmin.truncate(stream1);
  expectedMessages.add(new AuditMessage(0,stream1.toEntityId(),""String_Node_Str"",AuditType.TRUNCATE,AuditPayload.EMPTY_PAYLOAD));
  streamAdmin.updateConfig(stream1,new StreamProperties(100L,new FormatSpecification(""String_Node_Str"",null),100));
  expectedMessages.add(new AuditMessage(0,stream1.toEntityId(),""String_Node_Str"",AuditType.UPDATE,AuditPayload.EMPTY_PAYLOAD));
  Id.Run run=new Id.Run(Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.FLOW,""String_Node_Str""),RunIds.generate().getId());
  streamAdmin.addAccess(run,stream1,AccessType.READ);
  expectedMessages.add(new AuditMessage(0,stream1.toEntityId(),""String_Node_Str"",AuditType.ACCESS,new AccessPayload(co.cask.cdap.proto.audit.payload.access.AccessType.READ,run.toEntityId())));
  streamAdmin.drop(stream1);
  expectedMessages.add(new AuditMessage(0,stream1.toEntityId(),""String_Node_Str"",AuditType.DELETE,AuditPayload.EMPTY_PAYLOAD));
  streamAdmin.dropAllInNamespace(Id.Namespace.from(FOO_NAMESPACE));
  expectedMessages.add(new AuditMessage(0,stream2.toEntityId(),""String_Node_Str"",AuditType.DELETE,AuditPayload.EMPTY_PAYLOAD));
  final String systemNs=NamespaceId.SYSTEM.getNamespace();
  final Iterable<AuditMessage> actualMessages=Iterables.filter(getInMemoryAuditPublisher().popMessages(),new Predicate<AuditMessage>(){
    @Override public boolean apply(    AuditMessage input){
      return !(input.getEntityId() instanceof NamespacedId && ((NamespacedId)input.getEntityId()).getNamespace().equals(systemNs));
    }
  }
);
  Assert.assertEquals(expectedMessages,Lists.newArrayList(actualMessages));
}","@Test public void testAuditPublish() throws Exception {
  grantAndAssertSuccess(new NamespaceId(FOO_NAMESPACE),USER,EnumSet.allOf(Action.class));
  getInMemoryAuditPublisher().popMessages();
  final List<AuditMessage> expectedMessages=new ArrayList<>();
  StreamAdmin streamAdmin=getStreamAdmin();
  Id.Stream stream1=Id.Stream.from(FOO_NAMESPACE,""String_Node_Str"");
  streamAdmin.create(stream1);
  expectedMessages.add(new AuditMessage(0,stream1.toEntityId(),""String_Node_Str"",AuditType.CREATE,AuditPayload.EMPTY_PAYLOAD));
  Id.Stream stream2=Id.Stream.from(FOO_NAMESPACE,""String_Node_Str"");
  streamAdmin.create(stream2);
  expectedMessages.add(new AuditMessage(0,stream2.toEntityId(),""String_Node_Str"",AuditType.CREATE,AuditPayload.EMPTY_PAYLOAD));
  streamAdmin.truncate(stream1);
  expectedMessages.add(new AuditMessage(0,stream1.toEntityId(),""String_Node_Str"",AuditType.TRUNCATE,AuditPayload.EMPTY_PAYLOAD));
  streamAdmin.updateConfig(stream1,new StreamProperties(100L,new FormatSpecification(""String_Node_Str"",null),100));
  expectedMessages.add(new AuditMessage(0,stream1.toEntityId(),""String_Node_Str"",AuditType.UPDATE,AuditPayload.EMPTY_PAYLOAD));
  Id.Run run=new Id.Run(Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.FLOW,""String_Node_Str""),RunIds.generate().getId());
  streamAdmin.addAccess(run,stream1,AccessType.READ);
  expectedMessages.add(new AuditMessage(0,stream1.toEntityId(),""String_Node_Str"",AuditType.ACCESS,new AccessPayload(co.cask.cdap.proto.audit.payload.access.AccessType.READ,run.toEntityId())));
  streamAdmin.drop(stream1);
  expectedMessages.add(new AuditMessage(0,stream1.toEntityId(),""String_Node_Str"",AuditType.DELETE,AuditPayload.EMPTY_PAYLOAD));
  streamAdmin.dropAllInNamespace(Id.Namespace.from(FOO_NAMESPACE));
  expectedMessages.add(new AuditMessage(0,stream2.toEntityId(),""String_Node_Str"",AuditType.DELETE,AuditPayload.EMPTY_PAYLOAD));
  final String systemNs=NamespaceId.SYSTEM.getNamespace();
  final Iterable<AuditMessage> actualMessages=Iterables.filter(getInMemoryAuditPublisher().popMessages(),new Predicate<AuditMessage>(){
    @Override public boolean apply(    AuditMessage input){
      return !(input.getEntityId() instanceof NamespacedId && ((NamespacedId)input.getEntityId()).getNamespace().equals(systemNs));
    }
  }
);
  Assert.assertEquals(expectedMessages,Lists.newArrayList(actualMessages));
}"
5954,"@Test public void testConfigAndTruncate() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  grantAndAssertSuccess(new NamespaceId(FOO_NAMESPACE),USER,ImmutableSet.of(Action.WRITE));
  Id.Stream stream=Id.Stream.from(FOO_NAMESPACE,""String_Node_Str"");
  streamAdmin.create(stream);
  Assert.assertTrue(streamAdmin.exists(stream));
  writeEvent(stream);
  streamAdmin.getConfig(stream);
  streamAdmin.getProperties(stream);
  revokeAndAssertSuccess(new NamespaceId(FOO_NAMESPACE),USER,ImmutableSet.of(Action.WRITE));
  revokeAndAssertSuccess(stream.toEntityId(),USER,ImmutableSet.of(Action.ALL));
  streamAdmin.getConfig(stream);
  try {
    streamAdmin.getProperties(stream);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  grantAndAssertSuccess(stream.toEntityId(),USER,ImmutableSet.of(Action.READ));
  streamAdmin.getConfig(stream);
  StreamProperties properties=streamAdmin.getProperties(stream);
  try {
    streamAdmin.updateConfig(stream,properties);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  writeEvent(stream);
  grantAndAssertSuccess(stream.toEntityId(),USER,ImmutableSet.of(Action.WRITE));
  writeEvent(stream);
  try {
    streamAdmin.updateConfig(stream,properties);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  try {
    streamAdmin.truncate(stream);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  try {
    streamAdmin.drop(stream);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  grantAndAssertSuccess(stream.toEntityId(),USER,ImmutableSet.of(Action.ADMIN));
  streamAdmin.updateConfig(stream,properties);
  streamAdmin.truncate(stream);
  Assert.assertEquals(0,getStreamSize(stream));
  streamAdmin.drop(stream);
}","@Test public void testConfigAndTruncate() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  grantAndAssertSuccess(new NamespaceId(FOO_NAMESPACE),USER,ImmutableSet.of(Action.WRITE));
  Id.Stream stream=Id.Stream.from(FOO_NAMESPACE,""String_Node_Str"");
  streamAdmin.create(stream);
  Assert.assertTrue(streamAdmin.exists(stream));
  writeEvent(stream);
  streamAdmin.getConfig(stream);
  streamAdmin.getProperties(stream);
  revokeAndAssertSuccess(new NamespaceId(FOO_NAMESPACE),USER,ImmutableSet.of(Action.WRITE));
  revokeAndAssertSuccess(stream.toEntityId(),USER,EnumSet.allOf(Action.class));
  streamAdmin.getConfig(stream);
  try {
    streamAdmin.getProperties(stream);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  grantAndAssertSuccess(stream.toEntityId(),USER,ImmutableSet.of(Action.READ));
  streamAdmin.getConfig(stream);
  StreamProperties properties=streamAdmin.getProperties(stream);
  try {
    streamAdmin.updateConfig(stream,properties);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  writeEvent(stream);
  grantAndAssertSuccess(stream.toEntityId(),USER,ImmutableSet.of(Action.WRITE));
  writeEvent(stream);
  try {
    streamAdmin.updateConfig(stream,properties);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  try {
    streamAdmin.truncate(stream);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  try {
    streamAdmin.drop(stream);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  grantAndAssertSuccess(stream.toEntityId(),USER,ImmutableSet.of(Action.ADMIN));
  streamAdmin.updateConfig(stream,properties);
  streamAdmin.truncate(stream);
  Assert.assertEquals(0,getStreamSize(stream));
  streamAdmin.drop(stream);
}"
5955,"@Test public void testCreateExist() throws Exception {
  SecurityRequestContext.setUserId(USER.getName());
  StreamAdmin streamAdmin=getStreamAdmin();
  String streamName=""String_Node_Str"";
  Id.Stream streamId=Id.Stream.from(FOO_NAMESPACE,streamName);
  Id.Stream otherStreamId=Id.Stream.from(OTHER_NAMESPACE,streamName);
  Assert.assertFalse(streamAdmin.exists(streamId));
  Assert.assertFalse(streamAdmin.exists(otherStreamId));
  try {
    streamAdmin.create(streamId);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  grantAndAssertSuccess(streamId.getNamespace().toEntityId(),USER,ImmutableSet.of(Action.WRITE));
  streamAdmin.create(streamId);
  Assert.assertTrue(streamAdmin.exists(streamId));
  Assert.assertFalse(streamAdmin.exists(otherStreamId));
  try {
    streamAdmin.create(otherStreamId);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  grantAndAssertSuccess(otherStreamId.getNamespace().toEntityId(),USER,ImmutableSet.of(Action.WRITE));
  streamAdmin.create(otherStreamId);
  Assert.assertTrue(streamAdmin.exists(otherStreamId));
  streamAdmin.drop(otherStreamId);
  Assert.assertFalse(streamAdmin.exists(otherStreamId));
  revokeAndAssertSuccess(streamId.toEntityId(),USER,ImmutableSet.of(Action.ADMIN,Action.ALL));
  try {
    streamAdmin.drop(streamId);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  grantAndAssertSuccess(streamId.toEntityId(),USER,ImmutableSet.of(Action.WRITE));
  try {
    streamAdmin.drop(streamId);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  grantAndAssertSuccess(streamId.toEntityId(),USER,ImmutableSet.of(Action.ADMIN));
  streamAdmin.drop(streamId);
  Assert.assertFalse(streamAdmin.exists(streamId));
}","@Test public void testCreateExist() throws Exception {
  SecurityRequestContext.setUserId(USER.getName());
  StreamAdmin streamAdmin=getStreamAdmin();
  String streamName=""String_Node_Str"";
  Id.Stream streamId=Id.Stream.from(FOO_NAMESPACE,streamName);
  Id.Stream otherStreamId=Id.Stream.from(OTHER_NAMESPACE,streamName);
  Assert.assertFalse(streamAdmin.exists(streamId));
  Assert.assertFalse(streamAdmin.exists(otherStreamId));
  try {
    streamAdmin.create(streamId);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  grantAndAssertSuccess(streamId.getNamespace().toEntityId(),USER,ImmutableSet.of(Action.WRITE));
  streamAdmin.create(streamId);
  Assert.assertTrue(streamAdmin.exists(streamId));
  Assert.assertFalse(streamAdmin.exists(otherStreamId));
  try {
    streamAdmin.create(otherStreamId);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  grantAndAssertSuccess(otherStreamId.getNamespace().toEntityId(),USER,ImmutableSet.of(Action.WRITE));
  streamAdmin.create(otherStreamId);
  Assert.assertTrue(streamAdmin.exists(otherStreamId));
  streamAdmin.drop(otherStreamId);
  Assert.assertFalse(streamAdmin.exists(otherStreamId));
  revokeAndAssertSuccess(streamId.toEntityId(),USER,EnumSet.allOf(Action.class));
  try {
    streamAdmin.drop(streamId);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  grantAndAssertSuccess(streamId.toEntityId(),USER,ImmutableSet.of(Action.WRITE));
  try {
    streamAdmin.drop(streamId);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException e) {
  }
  grantAndAssertSuccess(streamId.toEntityId(),USER,ImmutableSet.of(Action.ADMIN));
  streamAdmin.drop(streamId);
  Assert.assertFalse(streamAdmin.exists(streamId));
}"
5956,"@Test public void testListStreams() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  NamespaceId nsId=new NamespaceId(FOO_NAMESPACE);
  grantAndAssertSuccess(nsId,USER,EnumSet.allOf(Action.class));
  StreamId s1=nsId.stream(""String_Node_Str"");
  StreamId s2=nsId.stream(""String_Node_Str"");
  List<StreamSpecification> specifications=streamAdmin.listStreams(nsId);
  Assert.assertTrue(specifications.isEmpty());
  streamAdmin.create(s1.toId());
  streamAdmin.create(s2.toId());
  specifications=streamAdmin.listStreams(nsId);
  Assert.assertEquals(2,specifications.size());
  revokeAndAssertSuccess(s1,USER,EnumSet.allOf(Action.class));
  specifications=streamAdmin.listStreams(nsId);
  Assert.assertEquals(2,specifications.size());
  Assert.assertEquals(s2.getStream(),specifications.get(0).getName());
  revokeAndAssertSuccess(s2,USER,EnumSet.allOf(Action.class));
  specifications=streamAdmin.listStreams(nsId);
  Assert.assertEquals(2,specifications.size());
  revokeAndAssertSuccess(nsId,USER,EnumSet.allOf(Action.class));
  specifications=streamAdmin.listStreams(nsId);
  Assert.assertTrue(specifications.isEmpty());
  grantAndAssertSuccess(s1,USER,ImmutableSet.of(Action.ALL));
  grantAndAssertSuccess(s2,USER,ImmutableSet.of(Action.ALL));
  streamAdmin.drop(s1.toId());
  streamAdmin.drop(s2.toId());
}","@Test public void testListStreams() throws Exception {
  StreamAdmin streamAdmin=getStreamAdmin();
  NamespaceId nsId=new NamespaceId(FOO_NAMESPACE);
  grantAndAssertSuccess(nsId,USER,EnumSet.allOf(Action.class));
  StreamId s1=nsId.stream(""String_Node_Str"");
  StreamId s2=nsId.stream(""String_Node_Str"");
  List<StreamSpecification> specifications=streamAdmin.listStreams(nsId);
  Assert.assertTrue(specifications.isEmpty());
  streamAdmin.create(s1.toId());
  streamAdmin.create(s2.toId());
  specifications=streamAdmin.listStreams(nsId);
  Assert.assertEquals(2,specifications.size());
  revokeAndAssertSuccess(s1,USER,EnumSet.allOf(Action.class));
  specifications=streamAdmin.listStreams(nsId);
  Assert.assertEquals(2,specifications.size());
  Set<String> streamNames=ImmutableSet.of(s1.getStream(),s2.getStream());
  Assert.assertTrue(streamNames.contains(specifications.get(0).getName()));
  Assert.assertTrue(streamNames.contains(specifications.get(1).getName()));
  revokeAndAssertSuccess(s2,USER,EnumSet.allOf(Action.class));
  specifications=streamAdmin.listStreams(nsId);
  Assert.assertEquals(2,specifications.size());
  Assert.assertTrue(streamNames.contains(specifications.get(0).getName()));
  Assert.assertTrue(streamNames.contains(specifications.get(1).getName()));
  revokeAndAssertSuccess(nsId,USER,EnumSet.allOf(Action.class));
  specifications=streamAdmin.listStreams(nsId);
  Assert.assertTrue(specifications.isEmpty());
  grantAndAssertSuccess(s1,USER,EnumSet.allOf(Action.class));
  grantAndAssertSuccess(s2,USER,EnumSet.allOf(Action.class));
  streamAdmin.drop(s1.toId());
  streamAdmin.drop(s2.toId());
}"
5957,"/** 
 * Creates a dataset instance.
 * @param namespaceId the namespace to create the dataset instance in
 * @param name the name of the new dataset instance
 * @param props the properties for the new dataset instance
 * @throws NamespaceNotFoundException if the specified namespace was not found
 * @throws DatasetAlreadyExistsException if a dataset with the same name already exists
 * @throws DatasetTypeNotFoundException if the dataset type was not found
 * @throws UnauthorizedException if perimeter security and authorization are enabled, and the current user does nothave  {@link Action#WRITE} privilege on the #instance's namespace
 */
void create(String namespaceId,String name,DatasetInstanceConfiguration props) throws Exception {
  Id.Namespace namespace=ConversionHelpers.toNamespaceId(namespaceId);
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(namespace.toEntityId(),principal,Action.WRITE);
  ensureNamespaceExists(namespace);
  Id.DatasetInstance newInstance=ConversionHelpers.toDatasetInstanceId(namespaceId,name);
  DatasetSpecification existing=instanceManager.get(newInstance);
  if (existing != null) {
    throw new DatasetAlreadyExistsException(newInstance);
  }
  DatasetTypeMeta typeMeta=getTypeInfo(namespace,props.getTypeName());
  if (typeMeta == null) {
    throw new DatasetTypeNotFoundException(ConversionHelpers.toDatasetTypeId(namespace,props.getTypeName()));
  }
  DatasetId datasetId=newInstance.toEntityId();
  privilegesManager.revoke(datasetId);
  privilegesManager.grant(datasetId,principal,ImmutableSet.of(Action.ALL));
  LOG.info(""String_Node_Str"",namespaceId,name,props.getTypeName(),props.getProperties());
  try {
    DatasetSpecification spec=opExecutorClient.create(newInstance,typeMeta,DatasetProperties.builder().addAll(props.getProperties()).setDescription(props.getDescription()).build());
    instanceManager.add(namespace,spec);
    metaCache.invalidate(newInstance);
    publishAudit(newInstance,AuditType.CREATE);
    enableExplore(newInstance,spec,props);
  }
 catch (  Exception e) {
    privilegesManager.revoke(datasetId);
    throw e;
  }
}","/** 
 * Creates a dataset instance.
 * @param namespaceId the namespace to create the dataset instance in
 * @param name the name of the new dataset instance
 * @param props the properties for the new dataset instance
 * @throws NamespaceNotFoundException if the specified namespace was not found
 * @throws DatasetAlreadyExistsException if a dataset with the same name already exists
 * @throws DatasetTypeNotFoundException if the dataset type was not found
 * @throws UnauthorizedException if perimeter security and authorization are enabled, and the current user does nothave  {@link Action#WRITE} privilege on the #instance's namespace
 */
void create(String namespaceId,String name,DatasetInstanceConfiguration props) throws Exception {
  Id.Namespace namespace=ConversionHelpers.toNamespaceId(namespaceId);
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(namespace.toEntityId(),principal,Action.WRITE);
  ensureNamespaceExists(namespace);
  Id.DatasetInstance newInstance=ConversionHelpers.toDatasetInstanceId(namespaceId,name);
  DatasetSpecification existing=instanceManager.get(newInstance);
  if (existing != null) {
    throw new DatasetAlreadyExistsException(newInstance);
  }
  DatasetTypeMeta typeMeta=getTypeInfo(namespace,props.getTypeName());
  if (typeMeta == null) {
    throw new DatasetTypeNotFoundException(ConversionHelpers.toDatasetTypeId(namespace,props.getTypeName()));
  }
  DatasetId datasetId=newInstance.toEntityId();
  privilegesManager.revoke(datasetId);
  privilegesManager.grant(datasetId,principal,EnumSet.allOf(Action.class));
  LOG.info(""String_Node_Str"",namespaceId,name,props.getTypeName(),props.getProperties());
  try {
    DatasetSpecification spec=opExecutorClient.create(newInstance,typeMeta,DatasetProperties.builder().addAll(props.getProperties()).setDescription(props.getDescription()).build());
    instanceManager.add(namespace,spec);
    metaCache.invalidate(newInstance);
    publishAudit(newInstance,AuditType.CREATE);
    enableExplore(newInstance,spec,props);
  }
 catch (  Exception e) {
    privilegesManager.revoke(datasetId);
    throw e;
  }
}"
5958,"private void grantAllPrivilegesOnModule(DatasetModuleId moduleId,Principal principal,@Nullable DatasetModuleMeta moduleMeta) throws Exception {
  Set<Action> allActions=ImmutableSet.of(Action.ALL);
  privilegesManager.grant(moduleId,principal,allActions);
  if (moduleMeta == null) {
    moduleMeta=typeManager.getModule(moduleId.toId());
  }
  if (moduleMeta == null) {
    LOG.debug(""String_Node_Str"",moduleId);
    return;
  }
  for (  String type : moduleMeta.getTypes()) {
    DatasetTypeId datasetTypeId=moduleId.getParent().datasetType(type);
    privilegesManager.grant(datasetTypeId,principal,allActions);
  }
}","private void grantAllPrivilegesOnModule(DatasetModuleId moduleId,Principal principal,@Nullable DatasetModuleMeta moduleMeta) throws Exception {
  privilegesManager.grant(moduleId,principal,EnumSet.allOf(Action.class));
  if (moduleMeta == null) {
    moduleMeta=typeManager.getModule(moduleId.toId());
  }
  if (moduleMeta == null) {
    LOG.debug(""String_Node_Str"",moduleId);
    return;
  }
  for (  String type : moduleMeta.getTypes()) {
    DatasetTypeId datasetTypeId=moduleId.getParent().datasetType(type);
    privilegesManager.grant(datasetTypeId,principal,EnumSet.allOf(Action.class));
  }
}"
5959,"@VisibleForTesting static ClassLoader createParent(){
  ClassLoader baseClassLoader=AuthorizerClassLoader.class.getClassLoader();
  Set<String> authorizerResources;
  try {
    authorizerResources=ClassPathResources.getResourcesWithDependencies(baseClassLoader,Authorizer.class);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"" + ""String_Node_Str"",e);
    authorizerResources=ImmutableSet.of();
  }
  Set<String> apiResources;
  try {
    apiResources=ClassPathResources.getResourcesWithDependencies(baseClassLoader,Application.class);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"" + ""String_Node_Str"",e);
    apiResources=ImmutableSet.of();
  }
  final Set<String> finalAuthorizerResources=Sets.union(authorizerResources,apiResources);
  return new FilterClassLoader(baseClassLoader,new FilterClassLoader.Filter(){
    @Override public boolean acceptResource(    String resource){
      return finalAuthorizerResources.contains(resource);
    }
    @Override public boolean acceptPackage(    String packageName){
      return true;
    }
  }
);
}","@VisibleForTesting static ClassLoader createParent(){
  ClassLoader baseClassLoader=AuthorizerClassLoader.class.getClassLoader();
  final Set<String> authorizerResources=traceSecurityDependencies(baseClassLoader);
  final FilterClassLoader.Filter defaultFilter=FilterClassLoader.defaultFilter();
  return new FilterClassLoader(baseClassLoader,new FilterClassLoader.Filter(){
    @Override public boolean acceptResource(    String resource){
      return defaultFilter.acceptResource(resource) || authorizerResources.contains(resource);
    }
    @Override public boolean acceptPackage(    String packageName){
      return true;
    }
  }
);
}"
5960,"@Override public boolean acceptResource(String resource){
  return finalAuthorizerResources.contains(resource);
}","@Override public boolean acceptResource(String resource){
  return defaultFilter.acceptResource(resource) || authorizerResources.contains(resource);
}"
5961,"@Test public void testAuthorizerClassLoaderParentUnavailableClasses(){
  assertClassUnavailable(ImmutableList.class);
  assertClassUnavailable(Configuration.class);
  assertClassUnavailable(HTable.class);
  assertClassUnavailable(""String_Node_Str"");
  assertClassUnavailable(ClassPathResources.class);
  assertClassUnavailable(AuthorizerClassLoader.class);
  assertClassUnavailable(""String_Node_Str"");
  assertClassUnavailable(""String_Node_Str"");
}","@Test public void testAuthorizerClassLoaderParentUnavailableClasses(){
  assertClassUnavailable(ImmutableList.class);
  assertClassUnavailable(HTable.class);
  assertClassUnavailable(""String_Node_Str"");
  assertClassUnavailable(ClassPathResources.class);
  assertClassUnavailable(AuthorizerClassLoader.class);
  assertClassUnavailable(""String_Node_Str"");
  assertClassUnavailable(""String_Node_Str"");
}"
5962,"@Test public void testAuthorizerClassLoaderParentAvailableClasses() throws ClassNotFoundException {
  parent.loadClass(List.class.getName());
  parent.loadClass(Nullable.class.getName());
  parent.loadClass(Gson.class.getName());
  parent.loadClass(Application.class.getName());
  parent.loadClass(LocationFactory.class.getName());
  parent.loadClass(Logger.class.getName());
  parent.loadClass(Principal.class.getName());
  parent.loadClass(Authorizer.class.getName());
  parent.loadClass(UnauthorizedException.class.getName());
}","@Test public void testAuthorizerClassLoaderParentAvailableClasses() throws ClassNotFoundException {
  parent.loadClass(List.class.getName());
  parent.loadClass(Nullable.class.getName());
  parent.loadClass(Gson.class.getName());
  parent.loadClass(Application.class.getName());
  parent.loadClass(LocationFactory.class.getName());
  parent.loadClass(Logger.class.getName());
  parent.loadClass(Principal.class.getName());
  parent.loadClass(Authorizer.class.getName());
  parent.loadClass(UnauthorizedException.class.getName());
  parent.loadClass(Configuration.class.getName());
  parent.loadClass(UserGroupInformation.class.getName());
}"
5963,"/** 
 * Creates   {@link ImpersonationInfo} for the specified namespace. If the info is not configured at the namespacelevel is empty, returns the info configured at the cdap level.
 */
public ImpersonationInfo(NamespaceMeta namespaceMeta,CConfiguration cConf){
  NamespaceConfig namespaceConfig=namespaceMeta.getConfig();
  String configuredPrincipal=namespaceConfig.getPrincipal();
  String configuredKeytabURI=namespaceConfig.getKeytabURI();
  if (configuredPrincipal != null && configuredKeytabURI != null) {
    this.principal=configuredPrincipal;
    this.keytabURI=configuredKeytabURI;
  }
 else   if (configuredPrincipal == null && configuredKeytabURI == null) {
    this.principal=cConf.get(Constants.Security.CFG_CDAP_MASTER_KRB_PRINCIPAL);
    this.keytabURI=cConf.get(Constants.Security.CFG_CDAP_MASTER_KRB_KEYTAB_PATH);
  }
  throw new IllegalStateException(String.format(""String_Node_Str"" + ""String_Node_Str"",configuredPrincipal,configuredKeytabURI));
}","/** 
 * Creates   {@link ImpersonationInfo} for the specified namespace. If the info is not configured at the namespacelevel is empty, returns the info configured at the cdap level.
 */
public ImpersonationInfo(NamespaceMeta namespaceMeta,CConfiguration cConf){
  NamespaceConfig namespaceConfig=namespaceMeta.getConfig();
  String configuredPrincipal=namespaceConfig.getPrincipal();
  String configuredKeytabURI=namespaceConfig.getKeytabURI();
  if (configuredPrincipal != null && configuredKeytabURI != null) {
    this.principal=configuredPrincipal;
    this.keytabURI=configuredKeytabURI;
  }
 else   if (configuredPrincipal == null && configuredKeytabURI == null) {
    this.principal=cConf.get(Constants.Security.CFG_CDAP_MASTER_KRB_PRINCIPAL);
    this.keytabURI=cConf.get(Constants.Security.CFG_CDAP_MASTER_KRB_KEYTAB_PATH);
  }
 else {
    throw new IllegalStateException(String.format(""String_Node_Str"" + ""String_Node_Str"",configuredPrincipal,configuredKeytabURI));
  }
}"
5964,"@Override public ProgramController call() throws Exception {
  return launch(program,options,localizeResources,tempDir,new ApplicationLauncher(){
    @Override public TwillController launch(    TwillApplication twillApplication,    Iterable<String> extraClassPaths,    Iterable<? extends Class<?>> extraDependencies){
      TwillPreparer twillPreparer=twillRunner.prepare(twillApplication);
      twillPreparer.withEnv(Collections.singletonMap(""String_Node_Str"",""String_Node_Str""));
      if (options.isDebug()) {
        twillPreparer.enableDebugging();
      }
      LOG.info(""String_Node_Str"",program.getId(),options.isDebug(),programOptions,logbackURI);
      if (schedulerQueueName != null && !schedulerQueueName.isEmpty()) {
        LOG.info(""String_Node_Str"",program.getId(),schedulerQueueName);
        twillPreparer.setSchedulerQueue(schedulerQueueName);
      }
      if (logbackURI != null) {
        twillPreparer.withResources(logbackURI);
      }
      String logLevelConf=cConf.get(Constants.COLLECT_APP_CONTAINER_LOG_LEVEL).toUpperCase();
      if (""String_Node_Str"".equals(logLevelConf)) {
        twillPreparer.addJVMOptions(""String_Node_Str"");
      }
 else {
        LogEntry.Level logLevel=LogEntry.Level.ERROR;
        if (""String_Node_Str"".equals(logLevelConf)) {
          logLevel=LogEntry.Level.TRACE;
        }
 else {
          try {
            logLevel=LogEntry.Level.valueOf(logLevelConf.toUpperCase());
          }
 catch (          Exception e) {
            LOG.warn(""String_Node_Str"",logLevelConf);
          }
        }
        twillPreparer.addLogHandler(new ApplicationLogHandler(new PrinterLogHandler(new PrintWriter(System.out)),logLevel));
      }
      String yarnAppClassPath=hConf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
      if (User.isHBaseSecurityEnabled(hConf) || UserGroupInformation.isSecurityEnabled()) {
        twillPreparer.addSecureStore(secureStoreUpdater.update(null,null));
      }
      Iterable<Class<?>> dependencies=Iterables.concat(Collections.singletonList(HBaseTableUtilFactory.getHBaseTableUtilClass()),extraDependencies);
      twillPreparer.withDependencies(dependencies).withClassPaths(Iterables.concat(extraClassPaths,Splitter.on(',').trimResults().split(hConf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,""String_Node_Str"")))).withApplicationClassPaths(Splitter.on(""String_Node_Str"").trimResults().split(yarnAppClassPath)).withBundlerClassAcceptor(new HadoopClassExcluder(){
        @Override public boolean accept(        String className,        URL classUrl,        URL classPathUrl){
          return super.accept(className,classUrl,classPathUrl) && !className.startsWith(""String_Node_Str"");
        }
      }
).withApplicationArguments(""String_Node_Str"" + RunnableOptions.JAR,programJarName,""String_Node_Str"" + RunnableOptions.HADOOP_CONF_FILE,HADOOP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.CDAP_CONF_FILE,CDAP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.APP_SPEC_FILE,APP_SPEC_FILE_NAME,""String_Node_Str"" + RunnableOptions.PROGRAM_OPTIONS,programOptions,""String_Node_Str"" + RunnableOptions.PROGRAM_ID,GSON.toJson(program.getId().toEntityId()));
      TwillController twillController;
      ClassLoader oldClassLoader=ClassLoaders.setContextClassLoader(new CombineClassLoader(AbstractDistributedProgramRunner.this.getClass().getClassLoader(),Iterables.transform(dependencies,new Function<Class<?>,ClassLoader>(){
        @Override public ClassLoader apply(        Class<?> input){
          return input.getClassLoader();
        }
      }
)));
      try {
        twillController=twillPreparer.start();
      }
  finally {
        ClassLoaders.setContextClassLoader(oldClassLoader);
      }
      return addCleanupListener(twillController,program,tempDir);
    }
  }
);
}","@Override public ProgramController call() throws Exception {
  return launch(program,options,localizeResources,tempDir,new ApplicationLauncher(){
    @Override public TwillController launch(    TwillApplication twillApplication,    Iterable<String> extraClassPaths,    Iterable<? extends Class<?>> extraDependencies){
      TwillPreparer twillPreparer=twillRunner.prepare(twillApplication);
      twillPreparer.withEnv(Collections.singletonMap(""String_Node_Str"",""String_Node_Str""));
      if (options.isDebug()) {
        twillPreparer.enableDebugging();
      }
      LOG.info(""String_Node_Str"",program.getId(),options.isDebug(),programOptions,logbackURI);
      if (schedulerQueueName != null && !schedulerQueueName.isEmpty()) {
        LOG.info(""String_Node_Str"",program.getId(),schedulerQueueName);
        twillPreparer.setSchedulerQueue(schedulerQueueName);
      }
      if (logbackURI != null) {
        twillPreparer.withResources(logbackURI);
      }
      String logLevelConf=cConf.get(Constants.COLLECT_APP_CONTAINER_LOG_LEVEL).toUpperCase();
      if (""String_Node_Str"".equals(logLevelConf)) {
        twillPreparer.addJVMOptions(""String_Node_Str"");
      }
 else {
        LogEntry.Level logLevel=LogEntry.Level.ERROR;
        if (""String_Node_Str"".equals(logLevelConf)) {
          logLevel=LogEntry.Level.TRACE;
        }
 else {
          try {
            logLevel=LogEntry.Level.valueOf(logLevelConf.toUpperCase());
          }
 catch (          Exception e) {
            LOG.warn(""String_Node_Str"",logLevelConf);
          }
        }
        twillPreparer.addLogHandler(new ApplicationLogHandler(new PrinterLogHandler(new PrintWriter(System.out)),logLevel));
      }
      String yarnAppClassPath=hConf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
      if (User.isHBaseSecurityEnabled(hConf) || UserGroupInformation.isSecurityEnabled()) {
        twillPreparer.addSecureStore(secureStoreUpdater.update(null,null));
      }
      Iterable<Class<?>> dependencies=Iterables.concat(Collections.singletonList(HBaseTableUtilFactory.getHBaseTableUtilClass()),getKMSSecureStore(cConf),extraDependencies);
      twillPreparer.withDependencies(dependencies).withClassPaths(Iterables.concat(extraClassPaths,Splitter.on(',').trimResults().split(hConf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,""String_Node_Str"")))).withApplicationClassPaths(Splitter.on(""String_Node_Str"").trimResults().split(yarnAppClassPath)).withBundlerClassAcceptor(new HadoopClassExcluder(){
        @Override public boolean accept(        String className,        URL classUrl,        URL classPathUrl){
          return super.accept(className,classUrl,classPathUrl) && !className.startsWith(""String_Node_Str"");
        }
      }
).withApplicationArguments(""String_Node_Str"" + RunnableOptions.JAR,programJarName,""String_Node_Str"" + RunnableOptions.HADOOP_CONF_FILE,HADOOP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.CDAP_CONF_FILE,CDAP_CONF_FILE_NAME,""String_Node_Str"" + RunnableOptions.APP_SPEC_FILE,APP_SPEC_FILE_NAME,""String_Node_Str"" + RunnableOptions.PROGRAM_OPTIONS,programOptions,""String_Node_Str"" + RunnableOptions.PROGRAM_ID,GSON.toJson(program.getId().toEntityId()));
      TwillController twillController;
      ClassLoader oldClassLoader=ClassLoaders.setContextClassLoader(new CombineClassLoader(AbstractDistributedProgramRunner.this.getClass().getClassLoader(),Iterables.transform(dependencies,new Function<Class<?>,ClassLoader>(){
        @Override public ClassLoader apply(        Class<?> input){
          return input.getClassLoader();
        }
      }
)));
      try {
        twillController=twillPreparer.start();
      }
  finally {
        ClassLoaders.setContextClassLoader(oldClassLoader);
      }
      return addCleanupListener(twillController,program,tempDir);
    }
  }
);
}"
5965,"@Override @SuppressWarnings(""String_Node_Str"") public T get(){
  boolean kmsBacked=KMS_BACKED.equalsIgnoreCase(cConf.get(Constants.Security.Store.PROVIDER));
  boolean fileBacked=FILE_BACKED.equalsIgnoreCase(cConf.get(Constants.Security.Store.PROVIDER));
  boolean validPassword=!Strings.isNullOrEmpty(sConf.get(Constants.Security.Store.FILE_PASSWORD));
  if (fileBacked && validPassword) {
    return (T)injector.getInstance(FileSecureStore.class);
  }
  if (fileBacked) {
    throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"");
  }
  if (kmsBacked) {
    throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"");
  }
  return (T)injector.getInstance(DummySecureStore.class);
}","@Override @SuppressWarnings(""String_Node_Str"") public T get(){
  boolean fileBacked=SecureStoreUtils.isFileBacked(cConf);
  boolean validPassword=!Strings.isNullOrEmpty(sConf.get(Constants.Security.Store.FILE_PASSWORD));
  if (fileBacked && validPassword) {
    return (T)injector.getInstance(FileSecureStore.class);
  }
  if (fileBacked) {
    throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"");
  }
  if (SecureStoreUtils.isKMSBacked(cConf)) {
    throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"");
  }
  return (T)injector.getInstance(DummySecureStore.class);
}"
5966,"/** 
 * Create a namespace in the File System and Hive.
 * @param namespaceMeta {@link NamespaceMeta} for the namespace to create
 * @throws IOException if there are errors while creating the namespace in the File System
 * @throws ExploreException if there are errors while deleting the namespace in Hive
 * @throws SQLException if there are errors while deleting the namespace in Hive
 */
@Override public void create(NamespaceMeta namespaceMeta) throws IOException, ExploreException, SQLException {
  createLocation(namespaceMeta);
  if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED)) {
    try {
      exploreFacade.createNamespace(namespaceMeta);
    }
 catch (    ExploreException|SQLException e) {
      deleteLocation(namespaceMeta.getNamespaceId());
      throw e;
    }
  }
}","/** 
 * Create a namespace in the File System and Hive. The hive database is only created for non-default namespaces.
 * @param namespaceMeta {@link NamespaceMeta} for the namespace to create
 * @throws IOException if there are errors while creating the namespace in the File System
 * @throws ExploreException if there are errors while creating the namespace in Hive
 * @throws SQLException if there are errors while creating the namespace in Hive
 */
@Override public void create(NamespaceMeta namespaceMeta) throws IOException, ExploreException, SQLException {
  createLocation(namespaceMeta);
  if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED) && !NamespaceId.DEFAULT.equals(namespaceMeta.getNamespaceId())) {
    try {
      exploreFacade.createNamespace(namespaceMeta);
    }
 catch (    ExploreException|SQLException e) {
      deleteLocation(namespaceMeta.getNamespaceId());
      throw e;
    }
  }
}"
5967,"/** 
 * Deletes the namespace directory on the FileSystem and Hive.
 * @param namespaceId {@link NamespaceId} for the namespace to delete
 * @throws IOException if there are errors while deleting the namespace in the File System
 * @throws ExploreException if there are errors while deleting the namespace in Hive
 * @throws SQLException if there are errors while deleting the namespace in Hive
 */
@Override public void delete(NamespaceId namespaceId) throws IOException, ExploreException, SQLException {
  deleteLocation(namespaceId);
  if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED)) {
    exploreFacade.removeNamespace(namespaceId.toId());
  }
}","/** 
 * Deletes the namespace directory on the FileSystem and Hive.
 * @param namespaceId {@link NamespaceId} for the namespace to delete
 * @throws IOException if there are errors while deleting the namespace in the File System
 * @throws ExploreException if there are errors while deleting the namespace in Hive
 * @throws SQLException if there are errors while deleting the namespace in Hive
 */
@Override public void delete(NamespaceId namespaceId) throws IOException, ExploreException, SQLException {
  deleteLocation(namespaceId);
  if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED) && !NamespaceId.DEFAULT.equals(namespaceId)) {
    exploreFacade.removeNamespace(namespaceId.toId());
  }
}"
5968,"@Inject DefaultNamespaceAdmin(Store store,NamespaceStore nsStore,PreferencesStore preferencesStore,DashboardStore dashboardStore,DatasetFramework dsFramework,ProgramRuntimeService runtimeService,QueueAdmin queueAdmin,StreamAdmin streamAdmin,MetricStore metricStore,Scheduler scheduler,ApplicationLifecycleService applicationLifecycleService,ArtifactRepository artifactRepository,AuthorizerInstantiator authorizerInstantiator,CConfiguration cConf,StorageProviderNamespaceAdmin storageProviderNamespaceAdmin,Impersonator impersonator,AuthorizationEnforcer authorizationEnforcer,AuthenticationContext authenticationContext){
  super(nsStore,authorizationEnforcer,authenticationContext);
  this.queueAdmin=queueAdmin;
  this.streamAdmin=streamAdmin;
  this.store=store;
  this.preferencesStore=preferencesStore;
  this.dashboardStore=dashboardStore;
  this.dsFramework=dsFramework;
  this.runtimeService=runtimeService;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
  this.applicationLifecycleService=applicationLifecycleService;
  this.artifactRepository=artifactRepository;
  this.authorizer=authorizerInstantiator.get();
  this.authorizationEnforcer=authorizationEnforcer;
  this.instanceId=createInstanceId(cConf);
  this.storageProviderNamespaceAdmin=storageProviderNamespaceAdmin;
  this.impersonator=impersonator;
}","@Inject DefaultNamespaceAdmin(Store store,NamespaceStore nsStore,PreferencesStore preferencesStore,DashboardStore dashboardStore,DatasetFramework dsFramework,ProgramRuntimeService runtimeService,QueueAdmin queueAdmin,StreamAdmin streamAdmin,MetricStore metricStore,Scheduler scheduler,ApplicationLifecycleService applicationLifecycleService,ArtifactRepository artifactRepository,AuthorizerInstantiator authorizerInstantiator,CConfiguration cConf,StorageProviderNamespaceAdmin storageProviderNamespaceAdmin,Impersonator impersonator,AuthorizationEnforcer authorizationEnforcer,AuthenticationContext authenticationContext){
  super(nsStore,authorizationEnforcer,authenticationContext);
  this.queueAdmin=queueAdmin;
  this.streamAdmin=streamAdmin;
  this.store=store;
  this.preferencesStore=preferencesStore;
  this.dashboardStore=dashboardStore;
  this.dsFramework=dsFramework;
  this.runtimeService=runtimeService;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
  this.applicationLifecycleService=applicationLifecycleService;
  this.artifactRepository=artifactRepository;
  this.authorizer=authorizerInstantiator.get();
  this.authorizationEnforcer=authorizationEnforcer;
  this.instanceId=createInstanceId(cConf);
  this.storageProviderNamespaceAdmin=storageProviderNamespaceAdmin;
  this.impersonator=impersonator;
  this.cConf=cConf;
}"
5969,"@Override public synchronized void updateProperties(Id.Namespace namespaceId,NamespaceMeta namespaceMeta) throws Exception {
  if (!exists(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  authorizationEnforcer.enforce(namespaceId.toEntityId(),authenticationContext.getPrincipal(),Action.ADMIN);
  NamespaceMeta existingMeta=nsStore.get(namespaceId);
  Preconditions.checkNotNull(existingMeta);
  NamespaceMeta.Builder builder=new NamespaceMeta.Builder(existingMeta);
  if (namespaceMeta.getDescription() != null) {
    builder.setDescription(namespaceMeta.getDescription());
  }
  NamespaceConfig config=namespaceMeta.getConfig();
  if (config != null && !Strings.isNullOrEmpty(config.getSchedulerQueueName())) {
    builder.setSchedulerQueueName(config.getSchedulerQueueName());
  }
  if (config != null && config.getRootDirectory() != null) {
    if (!config.getRootDirectory().equals(existingMeta.getConfig().getRootDirectory())) {
      throw new BadRequestException(String.format(""String_Node_Str"",NamespaceConfig.ROOT_DIRECTORY,existingMeta.getConfig().getRootDirectory(),config.getRootDirectory()));
    }
    if (config.getHbaseNamespace() != null && (!config.getHbaseNamespace().equals(existingMeta.getConfig().getHbaseNamespace()))) {
      throw new BadRequestException(String.format(""String_Node_Str"",NamespaceConfig.HBASE_NAMESPACE,existingMeta.getConfig().getHbaseNamespace(),config.getHbaseNamespace()));
    }
  }
  if (config != null && config.getHiveDatabase() != null) {
    if (!config.getHiveDatabase().equals(existingMeta.getConfig().getHiveDatabase())) {
      throw new BadRequestException(String.format(""String_Node_Str"",NamespaceConfig.HIVE_DATABASE,existingMeta.getConfig().getHiveDatabase(),config.getHiveDatabase()));
    }
  }
  nsStore.update(builder.build());
}","@Override public synchronized void updateProperties(Id.Namespace namespaceId,NamespaceMeta namespaceMeta) throws Exception {
  if (!exists(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  authorizationEnforcer.enforce(namespaceId.toEntityId(),authenticationContext.getPrincipal(),Action.ADMIN);
  NamespaceMeta existingMeta=nsStore.get(namespaceId);
  Preconditions.checkNotNull(existingMeta);
  NamespaceMeta.Builder builder=new NamespaceMeta.Builder(existingMeta);
  if (namespaceMeta.getDescription() != null) {
    builder.setDescription(namespaceMeta.getDescription());
  }
  NamespaceConfig config=namespaceMeta.getConfig();
  if (config != null && !Strings.isNullOrEmpty(config.getSchedulerQueueName())) {
    builder.setSchedulerQueueName(config.getSchedulerQueueName());
  }
  Set<String> difference=existingMeta.getConfig().getDifference(config);
  if (!difference.isEmpty()) {
    throw new BadRequestException(String.format(""String_Node_Str"" + ""String_Node_Str"",difference,namespaceId));
  }
  nsStore.update(builder.build());
}"
5970,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(instanceId,principal,Action.ADMIN);
  authorizer.grant(namespace,principal,ImmutableSet.of(Action.ALL));
  nsStore.create(metadata);
  try {
    impersonator.doAs(metadata,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.create(metadata);
        return null;
      }
    }
);
  }
 catch (  IOException|ExploreException|SQLException e) {
    nsStore.delete(metadata.getNamespaceId().toId());
    authorizer.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace.toId(),e);
  }
}","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(final NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  if (hasCustomMapping(metadata)) {
    validateCustomMapping(metadata);
  }
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(instanceId,principal,Action.ADMIN);
  authorizer.grant(namespace,principal,ImmutableSet.of(Action.ALL));
  if (SecurityUtil.isKerberosEnabled(cConf)) {
    ImpersonationInfo impersonationInfo=new ImpersonationInfo(metadata,cConf);
    String namespacePrincipal=impersonationInfo.getPrincipal();
    String namespaceUserName=new KerberosName(namespacePrincipal).getShortName();
    Principal namespaceUser=new Principal(namespaceUserName,Principal.PrincipalType.USER);
    authorizer.grant(namespace,namespaceUser,EnumSet.allOf(Action.class));
  }
  nsStore.create(metadata);
  try {
    impersonator.doAs(metadata,new Callable<Void>(){
      @Override public Void call() throws Exception {
        storageProviderNamespaceAdmin.create(metadata);
        return null;
      }
    }
);
  }
 catch (  IOException|ExploreException|SQLException e) {
    nsStore.delete(metadata.getNamespaceId().toId());
    authorizer.revoke(namespace);
    throw new NamespaceCannotBeCreatedException(namespace.toId(),e);
  }
}"
5971,"/** 
 * Checks if the specified namespace exists
 * @param namespaceId the {@link Id.Namespace} to check for existence
 * @return true, if the specifed namespace exists, false otherwise
 */
@Override public boolean exists(Id.Namespace namespaceId) throws Exception {
  try {
    get(namespaceId);
  }
 catch (  NotFoundException e) {
    return false;
  }
  return true;
}","/** 
 * Checks if the specified namespace exists
 * @param namespaceId the {@link Id.Namespace} to check for existence
 * @return true, if the specifed namespace exists, false otherwise
 */
@Override public boolean exists(Id.Namespace namespaceId) throws Exception {
  return nsStore.get(namespaceId) != null;
}"
5972,"@Override public void create(NamespaceMeta namespaceMeta) throws IOException, ExploreException, SQLException {
  super.create(namespaceMeta);
  String hbaseNamespace=tableUtil.getHBaseNamespace(namespaceMeta);
  try (HBaseAdmin admin=new HBaseAdmin(hConf)){
    if (Strings.isNullOrEmpty(namespaceMeta.getConfig().getHbaseNamespace())) {
      try {
        tableUtil.createNamespaceIfNotExists(admin,hbaseNamespace);
      }
 catch (      IOException e) {
        super.delete(namespaceMeta.getNamespaceId());
        throw e;
      }
    }
    if (!tableUtil.hasNamespace(admin,hbaseNamespace)) {
      throw new IOException(String.format(""String_Node_Str"",hbaseNamespace,namespaceMeta.getName()));
    }
  }
 }","@Override public void create(NamespaceMeta namespaceMeta) throws IOException, ExploreException, SQLException {
  super.create(namespaceMeta);
  if (NamespaceId.DEFAULT.equals(namespaceMeta.getNamespaceId())) {
    return;
  }
  String hbaseNamespace=tableUtil.getHBaseNamespace(namespaceMeta);
  try (HBaseAdmin admin=new HBaseAdmin(hConf)){
    if (Strings.isNullOrEmpty(namespaceMeta.getConfig().getHbaseNamespace())) {
      try {
        tableUtil.createNamespaceIfNotExists(admin,hbaseNamespace);
      }
 catch (      IOException e) {
        super.delete(namespaceMeta.getNamespaceId());
        throw e;
      }
    }
    if (!tableUtil.hasNamespace(admin,hbaseNamespace)) {
      throw new IOException(String.format(""String_Node_Str"",hbaseNamespace,namespaceMeta.getName()));
    }
  }
 }"
5973,"@SuppressWarnings(""String_Node_Str"") @Override public void delete(NamespaceId namespaceId) throws IOException, ExploreException, SQLException {
  super.delete(namespaceId);
  NamespaceConfig namespaceConfig;
  try {
    namespaceConfig=namespaceQueryAdmin.get(namespaceId.toId()).getConfig();
  }
 catch (  Exception ex) {
    throw new IOException(""String_Node_Str"",ex);
  }
  if (Strings.isNullOrEmpty(namespaceConfig.getHbaseNamespace())) {
    String namespace=tableUtil.getHBaseNamespace(namespaceId);
    try (HBaseAdmin admin=new HBaseAdmin(hConf)){
      tableUtil.deleteNamespaceIfExists(admin,namespace);
    }
   }
 else {
    LOG.debug(""String_Node_Str"",namespaceConfig.getHbaseNamespace(),namespaceId);
  }
}","@SuppressWarnings(""String_Node_Str"") @Override public void delete(NamespaceId namespaceId) throws IOException, ExploreException, SQLException {
  super.delete(namespaceId);
  if (NamespaceId.DEFAULT.equals(namespaceId)) {
    return;
  }
  NamespaceConfig namespaceConfig;
  try {
    namespaceConfig=namespaceQueryAdmin.get(namespaceId.toId()).getConfig();
  }
 catch (  Exception ex) {
    throw new IOException(""String_Node_Str"",ex);
  }
  if (!Strings.isNullOrEmpty(namespaceConfig.getHbaseNamespace())) {
    LOG.debug(""String_Node_Str"",namespaceConfig.getHbaseNamespace(),namespaceId);
    return;
  }
  String namespace=tableUtil.getHBaseNamespace(namespaceId);
  try (HBaseAdmin admin=new HBaseAdmin(hConf)){
    tableUtil.deleteNamespaceIfExists(admin,namespace);
  }
 }"
5974,"@Test public void testConfigUpdateFailures() throws Exception {
  String namespace=""String_Node_Str"";
  Id.Namespace namespaceId=Id.Namespace.from(namespace);
  Location customlocation=namespacedLocationFactory.get(namespaceId);
  Assert.assertTrue(customlocation.mkdirs());
  NamespaceMeta nsMeta=new NamespaceMeta.Builder().setName(namespaceId).setRootDirectory(customlocation.toString()).build();
  namespaceAdmin.create(nsMeta);
  Assert.assertTrue(namespaceAdmin.exists(namespaceId));
  try {
    namespaceAdmin.updateProperties(nsMeta.getNamespaceId().toId(),new NamespaceMeta.Builder(nsMeta).setRootDirectory(""String_Node_Str"").build());
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    namespaceAdmin.updateProperties(nsMeta.getNamespaceId().toId(),new NamespaceMeta.Builder(nsMeta).setHBaseNamespace(""String_Node_Str"").build());
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    namespaceAdmin.updateProperties(nsMeta.getNamespaceId().toId(),new NamespaceMeta.Builder(nsMeta).setHiveDatabase(""String_Node_Str"").build());
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    namespaceAdmin.updateProperties(nsMeta.getNamespaceId().toId(),new NamespaceMeta.Builder(nsMeta).setRootDirectory(""String_Node_Str"").build());
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  namespaceAdmin.delete(namespaceId);
  Locations.deleteQuietly(customlocation);
}","@Test public void testConfigUpdateFailures() throws Exception {
  String namespace=""String_Node_Str"";
  Id.Namespace namespaceId=Id.Namespace.from(namespace);
  Location customlocation=namespacedLocationFactory.get(namespaceId);
  Assert.assertTrue(customlocation.mkdirs());
  NamespaceMeta nsMeta=new NamespaceMeta.Builder().setName(namespaceId).setRootDirectory(customlocation.toString()).build();
  namespaceAdmin.create(nsMeta);
  Assert.assertTrue(namespaceAdmin.exists(namespaceId));
  try {
    namespaceAdmin.updateProperties(nsMeta.getNamespaceId().toId(),new NamespaceMeta.Builder(nsMeta).setRootDirectory(""String_Node_Str"").build());
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    namespaceAdmin.updateProperties(nsMeta.getNamespaceId().toId(),new NamespaceMeta.Builder(nsMeta).setHBaseNamespace(""String_Node_Str"").build());
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    namespaceAdmin.updateProperties(nsMeta.getNamespaceId().toId(),new NamespaceMeta.Builder(nsMeta).setHiveDatabase(""String_Node_Str"").build());
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    namespaceAdmin.updateProperties(nsMeta.getNamespaceId().toId(),new NamespaceMeta.Builder(nsMeta).setRootDirectory(""String_Node_Str"").build());
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    namespaceAdmin.updateProperties(nsMeta.getNamespaceId().toId(),new NamespaceMeta.Builder(nsMeta).setPrincipal(""String_Node_Str"").build());
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  try {
    namespaceAdmin.updateProperties(nsMeta.getNamespaceId().toId(),new NamespaceMeta.Builder(nsMeta).setKeytabURI(""String_Node_Str"").build());
    Assert.fail();
  }
 catch (  BadRequestException e) {
  }
  namespaceAdmin.delete(namespaceId);
  Locations.deleteQuietly(customlocation);
}"
5975,"@Override public boolean exists(Id.Namespace namespaceId) throws Exception {
  try {
    get(namespaceId);
  }
 catch (  NamespaceNotFoundException e) {
    return false;
  }
  return true;
}","@Override public boolean exists(Id.Namespace namespaceId) throws Exception {
  try {
    get(namespaceId);
  }
 catch (  NamespaceNotFoundException e) {
    return false;
  }
catch (  UnauthorizedException e) {
  }
  return true;
}"
5976,"public ImpersonationInfo(String principal,String keytabURI){
  this.principal=principal;
  this.keytabURI=keytabURI;
}","/** 
 * Creates   {@link ImpersonationInfo} for the specified namespace. If the info is not configured at the namespacelevel is empty, returns the info configured at the cdap level.
 */
public ImpersonationInfo(NamespaceMeta namespaceMeta,CConfiguration cConf){
  NamespaceConfig namespaceConfig=namespaceMeta.getConfig();
  String configuredPrincipal=namespaceConfig.getPrincipal();
  String configuredKeytabURI=namespaceConfig.getKeytabURI();
  if (configuredPrincipal != null && configuredKeytabURI != null) {
    this.principal=configuredPrincipal;
    this.keytabURI=configuredKeytabURI;
  }
 else   if (configuredPrincipal == null && configuredKeytabURI == null) {
    this.principal=cConf.get(Constants.Security.CFG_CDAP_MASTER_KRB_PRINCIPAL);
    this.keytabURI=cConf.get(Constants.Security.CFG_CDAP_MASTER_KRB_KEYTAB_PATH);
  }
  throw new IllegalStateException(String.format(""String_Node_Str"" + ""String_Node_Str"",configuredPrincipal,configuredKeytabURI));
}"
5977,"@Inject ImpersonationUserResolver(NamespaceQueryAdmin namespaceQueryAdmin,CConfiguration cConf){
  this.namespaceQueryAdmin=namespaceQueryAdmin;
  this.defaultPrincipal=cConf.get(Constants.Security.CFG_CDAP_MASTER_KRB_PRINCIPAL);
  this.defaultKeytabPath=cConf.get(Constants.Security.CFG_CDAP_MASTER_KRB_KEYTAB_PATH);
}","@Inject ImpersonationUserResolver(NamespaceQueryAdmin namespaceQueryAdmin,CConfiguration cConf){
  this.namespaceQueryAdmin=namespaceQueryAdmin;
  this.cConf=cConf;
}"
5978,"/** 
 * Get impersonation info for a given namespace. If the info configured at the namespace level is empty, returns the info configured at the cdap level.
 * @return configured {@link ImpersonationInfo}.
 */
public ImpersonationInfo getImpersonationInfo(NamespaceMeta meta){
  NamespaceConfig namespaceConfig=meta.getConfig();
  String configuredPrincipal=namespaceConfig.getPrincipal();
  String configuredKeytabURI=namespaceConfig.getKeytabURI();
  if (configuredPrincipal != null && configuredKeytabURI != null) {
    return new ImpersonationInfo(configuredPrincipal,configuredKeytabURI);
  }
  if (configuredPrincipal == null && configuredKeytabURI == null) {
    return new ImpersonationInfo(defaultPrincipal,defaultKeytabPath);
  }
  throw new IllegalStateException(String.format(""String_Node_Str"" + ""String_Node_Str"",configuredPrincipal,configuredKeytabURI));
}","/** 
 * Get impersonation info for a given namespace. If the info configured at the namespace level is empty, returns the info configured at the cdap level.
 * @return configured {@link ImpersonationInfo}.
 */
public ImpersonationInfo getImpersonationInfo(NamespaceMeta meta){
  return new ImpersonationInfo(meta,cConf);
}"
5979,"private UserGroupInformation getUGI(ImpersonationInfo impersonationInfo) throws IOException {
  if (UserGroupInformation.getCurrentUser().getUserName().equals(impersonationInfo.getPrincipal())) {
    LOG.debug(""String_Node_Str"",UserGroupInformation.getCurrentUser());
    return UserGroupInformation.getCurrentUser();
  }
  return ugiProvider.getConfiguredUGI(impersonationInfo);
}","private UserGroupInformation getUGI(ImpersonationInfo impersonationInfo) throws IOException {
  String configuredPrincipalShortName=new KerberosName(impersonationInfo.getPrincipal()).getShortName();
  if (UserGroupInformation.getCurrentUser().getShortUserName().equals(configuredPrincipalShortName)) {
    LOG.debug(""String_Node_Str"",impersonationInfo.getPrincipal(),UserGroupInformation.getCurrentUser());
    return UserGroupInformation.getCurrentUser();
  }
  return ugiProvider.getConfiguredUGI(impersonationInfo);
}"
5980,"/** 
 * Returns the data stored in the secure store. Makes two calls to the provider, one to get the metadata and another to get the data.
 * @param namespace The namespace this key belongs to.
 * @param name Name of the key.
 * @return An object representing the securely stored data associated with the name.
 * @throws NamespaceNotFoundException If the specified namespace does not exist.
 * @throws IOException If there was a problem getting the key or the metadata from the underlying key provider.
 */
@Override public SecureStoreData getSecureData(String namespace,String name) throws Exception {
  checkNamespaceExists(namespace);
  String keyName=getKeyName(namespace,name);
  KeyProvider.Metadata metadata=provider.getMetadata(keyName);
  SecureStoreMetadata meta=SecureStoreMetadata.of(name,metadata.getDescription(),metadata.getAttributes());
  KeyProvider.KeyVersion keyVersion=provider.getCurrentKey(keyName);
  return new SecureStoreData(meta,keyVersion.getMaterial());
}","/** 
 * Returns the data stored in the secure store. Makes two calls to the provider, one to get the metadata and another to get the data.
 * @param namespace The namespace this key belongs to.
 * @param name Name of the key.
 * @return An object representing the securely stored data associated with the name.
 * @throws NamespaceNotFoundException If the specified namespace does not exist.
 * @throws IOException If there was a problem getting the key or the metadata from the underlying key provider.
 */
@Override public SecureStoreData getSecureData(String namespace,String name) throws Exception {
  checkNamespaceExists(namespace);
  String keyName=getKeyName(namespace,name);
  KeyProvider.Metadata metadata=provider.getMetadata(keyName);
  if (metadata == null) {
    throw new NotFoundException(new SecureKeyId(namespace,name));
  }
  SecureStoreMetadata meta=SecureStoreMetadata.of(name,metadata.getDescription(),metadata.getAttributes());
  KeyProvider.KeyVersion keyVersion=provider.getCurrentKey(keyName);
  return new SecureStoreData(meta,keyVersion.getMaterial());
}"
5981,"private void revokeAndAssertSuccess(EntityId entityId,Principal principal,Set<Action> actions) throws Exception {
  Set<Privilege> existingPrivileges=authorizer.listPrivileges(principal);
  authorizer.revoke(entityId,principal,actions);
  for (  Action action : actions) {
    existingPrivileges.remove(new Privilege(entityId,action));
  }
  Assert.assertEquals(existingPrivileges,authorizer.listPrivileges(principal));
}","private void revokeAndAssertSuccess(EntityId entityId,Principal principal,Set<Action> actions) throws Exception {
  Set<Privilege> existingPrivileges=authorizer.listPrivileges(principal);
  authorizer.revoke(entityId,principal,actions);
  Set<Privilege> revokedPrivileges=new HashSet<>();
  for (  Action action : actions) {
    revokedPrivileges.add(new Privilege(entityId,action));
  }
  Assert.assertEquals(Sets.difference(existingPrivileges,revokedPrivileges),authorizer.listPrivileges(principal));
}"
5982,"private void revokeAndAssertSuccess(EntityId entityId,Principal principal,Set<Action> actions) throws Exception {
  Authorizer authorizer=getAuthorizer();
  Set<Privilege> existingPrivileges=authorizer.listPrivileges(principal);
  authorizer.revoke(entityId,principal,actions);
  for (  Action action : actions) {
    existingPrivileges.remove(new Privilege(entityId,action));
  }
  Assert.assertEquals(existingPrivileges,authorizer.listPrivileges(principal));
}","private void revokeAndAssertSuccess(EntityId entityId,Principal principal,Set<Action> actions) throws Exception {
  Authorizer authorizer=getAuthorizer();
  Set<Privilege> existingPrivileges=authorizer.listPrivileges(principal);
  authorizer.revoke(entityId,principal,actions);
  Set<Privilege> revokedPrivileges=new HashSet<>();
  for (  Action action : actions) {
    revokedPrivileges.add(new Privilege(entityId,action));
  }
  Assert.assertEquals(Sets.difference(existingPrivileges,revokedPrivileges),authorizer.listPrivileges(principal));
}"
5983,"@Test public void testRBAC() throws Exception {
  Authorizer authorizer=get();
  Role admins=new Role(""String_Node_Str"");
  Role engineers=new Role(""String_Node_Str"");
  authorizer.createRole(admins);
  authorizer.createRole(engineers);
  Set<Role> roles=authorizer.listAllRoles();
  Assert.assertEquals(Collections.singleton(Arrays.asList(admins,engineers)),roles);
  try {
    authorizer.createRole(admins);
    Assert.fail(String.format(""String_Node_Str"",admins.getName()));
  }
 catch (  RoleAlreadyExistsException expected) {
  }
  authorizer.dropRole(admins);
  roles=authorizer.listAllRoles();
  Assert.assertEquals(Collections.singleton(engineers),roles);
  try {
    authorizer.dropRole(admins);
    Assert.fail(String.format(""String_Node_Str"",admins.getName()));
  }
 catch (  RoleNotFoundException expected) {
  }
  Principal spiderman=new Principal(""String_Node_Str"",Principal.PrincipalType.USER);
  authorizer.addRoleToPrincipal(engineers,spiderman);
  try {
    authorizer.addRoleToPrincipal(admins,spiderman);
    Assert.fail(String.format(""String_Node_Str"",admins,spiderman));
  }
 catch (  RoleNotFoundException expected) {
  }
  Assert.assertEquals(Collections.singleton(engineers),authorizer.listRoles(spiderman));
  NamespaceId ns1=Ids.namespace(""String_Node_Str"");
  verifyAuthFailure(ns1,spiderman,Action.READ);
  authorizer.grant(ns1,engineers,Collections.singleton(Action.READ));
  authorizer.enforce(ns1,spiderman,Action.READ);
  Assert.assertEquals((Collections.singleton(new Privilege(ns1,Action.READ))),authorizer.listPrivileges(spiderman));
  authorizer.revoke(ns1,engineers,(Collections.singleton(Action.READ)));
  Assert.assertEquals(Collections.EMPTY_SET,authorizer.listPrivileges(spiderman));
  verifyAuthFailure(ns1,spiderman,Action.READ);
  authorizer.removeRoleFromPrincipal(engineers,spiderman);
  Assert.assertEquals(Collections.EMPTY_SET,authorizer.listRoles(spiderman));
  try {
    authorizer.removeRoleFromPrincipal(admins,spiderman);
    Assert.fail(String.format(""String_Node_Str"",admins,spiderman));
  }
 catch (  RoleNotFoundException expected) {
  }
}","@Test public void testRBAC() throws Exception {
  Authorizer authorizer=get();
  Role admins=new Role(""String_Node_Str"");
  Role engineers=new Role(""String_Node_Str"");
  authorizer.createRole(admins);
  authorizer.createRole(engineers);
  Set<Role> roles=authorizer.listAllRoles();
  Set<Role> expectedRoles=new HashSet<>();
  expectedRoles.add(admins);
  expectedRoles.add(engineers);
  Assert.assertEquals(expectedRoles,roles);
  try {
    authorizer.createRole(admins);
    Assert.fail(String.format(""String_Node_Str"",admins.getName()));
  }
 catch (  RoleAlreadyExistsException expected) {
  }
  authorizer.dropRole(admins);
  roles=authorizer.listAllRoles();
  Assert.assertEquals(Collections.singleton(engineers),roles);
  try {
    authorizer.dropRole(admins);
    Assert.fail(String.format(""String_Node_Str"",admins.getName()));
  }
 catch (  RoleNotFoundException expected) {
  }
  Principal spiderman=new Principal(""String_Node_Str"",Principal.PrincipalType.USER);
  authorizer.addRoleToPrincipal(engineers,spiderman);
  try {
    authorizer.addRoleToPrincipal(admins,spiderman);
    Assert.fail(String.format(""String_Node_Str"",admins,spiderman));
  }
 catch (  RoleNotFoundException expected) {
  }
  Assert.assertEquals(Collections.singleton(engineers),authorizer.listRoles(spiderman));
  NamespaceId ns1=new NamespaceId(""String_Node_Str"");
  verifyAuthFailure(ns1,spiderman,Action.READ);
  authorizer.grant(ns1,engineers,Collections.singleton(Action.READ));
  authorizer.enforce(ns1,spiderman,Action.READ);
  Assert.assertEquals(Collections.singleton(new Privilege(ns1,Action.READ)),authorizer.listPrivileges(spiderman));
  authorizer.revoke(ns1,engineers,Collections.singleton(Action.READ));
  Assert.assertEquals(Collections.EMPTY_SET,authorizer.listPrivileges(spiderman));
  verifyAuthFailure(ns1,spiderman,Action.READ);
  authorizer.removeRoleFromPrincipal(engineers,spiderman);
  Assert.assertEquals(Collections.EMPTY_SET,authorizer.listRoles(spiderman));
  try {
    authorizer.removeRoleFromPrincipal(admins,spiderman);
    Assert.fail(String.format(""String_Node_Str"",admins,spiderman));
  }
 catch (  RoleNotFoundException expected) {
  }
}"
5984,"@Test public void testHierarchy() throws Exception {
  Authorizer authorizer=get();
  DatasetId dataset=namespace.dataset(""String_Node_Str"");
  verifyAuthFailure(namespace,user,Action.READ);
  authorizer.grant(namespace,user,Collections.singleton(Action.READ));
  authorizer.enforce(dataset,user,Action.READ);
  authorizer.grant(dataset,user,Collections.singleton(Action.WRITE));
  verifyAuthFailure(namespace,user,Action.WRITE);
  authorizer.revoke(namespace,user,Collections.singleton(Action.READ));
  authorizer.revoke(dataset);
  verifyAuthFailure(namespace,user,Action.READ);
}","@Test @Ignore public void testHierarchy() throws Exception {
  Authorizer authorizer=get();
  DatasetId dataset=namespace.dataset(""String_Node_Str"");
  verifyAuthFailure(namespace,user,Action.READ);
  authorizer.grant(namespace,user,Collections.singleton(Action.READ));
  authorizer.enforce(dataset,user,Action.READ);
  authorizer.grant(dataset,user,Collections.singleton(Action.WRITE));
  verifyAuthFailure(namespace,user,Action.WRITE);
  authorizer.revoke(namespace,user,Collections.singleton(Action.READ));
  authorizer.revoke(dataset);
  verifyAuthFailure(namespace,user,Action.READ);
}"
5985,"@AfterClass public static void cleanup(){
  appFabricServer.stopAndWait();
  remoteSystemOperationsService.stopAndWait();
  authorizationEnforcementService.stopAndWait();
}","@AfterClass public static void cleanup(){
  appFabricServer.stopAndWait();
  authorizationEnforcementService.stopAndWait();
}"
5986,"@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=createCConf();
  SConfiguration sConf=SConfiguration.create();
  sConf.set(Constants.Security.Store.FILE_PASSWORD,""String_Node_Str"");
  final Injector injector=Guice.createInjector(new AppFabricTestModule(cConf,sConf));
  injector.getInstance(TransactionManager.class).startAndWait();
  injector.getInstance(DatasetOpExecutor.class).startAndWait();
  injector.getInstance(DatasetService.class).startAndWait();
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  authorizationEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  authorizationEnforcementService.startAndWait();
  remoteSystemOperationsService=injector.getInstance(RemoteSystemOperationsService.class);
  remoteSystemOperationsService.startAndWait();
  secureStoreService=injector.getInstance(SecureStoreService.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  SecurityRequestContext.setUserId(ALICE.getName());
  secureStoreService=injector.getInstance(SecureStoreService.class);
  authorizer.grant(NamespaceId.DEFAULT,ALICE,Collections.singleton(Action.READ));
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return injector.getInstance(NamespaceAdmin.class).exists(Id.Namespace.DEFAULT);
    }
  }
,5,TimeUnit.SECONDS);
  authorizer.revoke(NamespaceId.DEFAULT,ALICE,Collections.singleton(Action.READ));
}","@BeforeClass public static void setup() throws Exception {
  SConfiguration sConf=SConfiguration.create();
  sConf.set(Constants.Security.Store.FILE_PASSWORD,""String_Node_Str"");
  final Injector injector=Guice.createInjector(new AppFabricTestModule(createCConf(),sConf));
  injector.getInstance(TransactionManager.class).startAndWait();
  injector.getInstance(DatasetOpExecutor.class).startAndWait();
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  DatasetService datasetService=injector.getInstance(DatasetService.class);
  datasetService.startAndWait();
  waitForService(Constants.Service.DATASET_EXECUTOR);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  authorizationEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  authorizationEnforcementService.startAndWait();
  secureStoreService=injector.getInstance(SecureStoreService.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  SecurityRequestContext.setUserId(ALICE.getName());
  secureStoreService=injector.getInstance(SecureStoreService.class);
  authorizer.grant(NamespaceId.DEFAULT,ALICE,Collections.singleton(Action.READ));
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return injector.getInstance(NamespaceAdmin.class).exists(Id.Namespace.DEFAULT);
    }
  }
,5,TimeUnit.SECONDS);
  authorizer.revoke(NamespaceId.DEFAULT,ALICE,Collections.singleton(Action.READ));
}"
5987,"/** 
 * Filter a list of   {@link ArtifactSummary} that ensures the logged-in user has a {@link Action privilege} on
 * @param artifacts the {@link List<ArtifactSummary>} to filter with
 * @param namespace namespace of the artifacts
 * @return filtered list of {@link ArtifactSummary}
 */
private List<ArtifactSummary> filterAuthorizedArtifacts(List<ArtifactSummary> artifacts,final NamespaceId namespace) throws Exception {
  final Predicate<EntityId> filter=authorizationEnforcer.createFilter(authenticationContext.getPrincipal());
  return Lists.newArrayList(Iterables.filter(artifacts,new com.google.common.base.Predicate<ArtifactSummary>(){
    @Override public boolean apply(    ArtifactSummary artifactSummary){
      return filter.apply(namespace.artifact(artifactSummary.getName(),artifactSummary.getVersion()));
    }
  }
));
}","/** 
 * Filter a list of   {@link ArtifactSummary} that ensures the logged-in user has a {@link Action privilege} on
 * @param artifacts the {@link List<ArtifactSummary>} to filter with
 * @param namespace namespace of the artifacts
 * @return filtered list of {@link ArtifactSummary}
 */
private List<ArtifactSummary> filterAuthorizedArtifacts(List<ArtifactSummary> artifacts,final NamespaceId namespace) throws Exception {
  final Predicate<EntityId> filter=authorizationEnforcer.createFilter(authenticationContext.getPrincipal());
  return Lists.newArrayList(Iterables.filter(artifacts,new com.google.common.base.Predicate<ArtifactSummary>(){
    @Override public boolean apply(    ArtifactSummary artifactSummary){
      return ArtifactScope.SYSTEM.equals(artifactSummary.getScope()) || filter.apply(namespace.artifact(artifactSummary.getName(),artifactSummary.getVersion()));
    }
  }
));
}"
5988,"@Override public boolean apply(ArtifactSummary artifactSummary){
  return filter.apply(namespace.artifact(artifactSummary.getName(),artifactSummary.getVersion()));
}","@Override public boolean apply(ArtifactSummary artifactSummary){
  return ArtifactScope.SYSTEM.equals(artifactSummary.getScope()) || filter.apply(namespace.artifact(artifactSummary.getName(),artifactSummary.getVersion()));
}"
5989,"@AfterClass public static void cleanup() throws Exception {
  authorizer.revoke(instance);
  authEnforcerService.stopAndWait();
  Assert.assertEquals(ImmutableSet.<Privilege>of(),authorizer.listPrivileges(ALICE));
  SecurityRequestContext.setUserId(OLD_USER_ID);
}","@AfterClass public static void cleanup() throws Exception {
  authorizer.revoke(instance);
  authEnforcerService.stopAndWait();
  Assert.assertEquals(Collections.emptySet(),authorizer.listPrivileges(ALICE));
  SecurityRequestContext.setUserId(OLD_USER_ID);
}"
5990,"@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  Location deploymentJar=AppJarHelper.createDeploymentJar(new LocalLocationFactory(TMP_FOLDER.newFolder()),InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,deploymentJar.toURI().getPath());
  cConf.set(Constants.Security.Authorization.SUPERUSERS,""String_Node_Str"");
  Injector injector=AppFabricTestHelper.getInjector(cConf);
  artifactRepository=injector.getInstance(ArtifactRepository.class);
  AuthorizerInstantiator instantiatorService=injector.getInstance(AuthorizerInstantiator.class);
  authorizer=instantiatorService.get();
  authEnforcerService=injector.getInstance(AuthorizationEnforcementService.class);
  authEnforcerService.startAndWait();
  instance=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
}","@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  Location deploymentJar=AppJarHelper.createDeploymentJar(new LocalLocationFactory(TMP_FOLDER.newFolder()),InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,deploymentJar.toURI().getPath());
  cConf.set(Constants.Security.Authorization.SUPERUSERS,""String_Node_Str"");
  File systemArtifactsDir=TMP_FOLDER.newFolder();
  cConf.set(Constants.AppFabric.SYSTEM_ARTIFACTS_DIR,systemArtifactsDir.getAbsolutePath());
  createSystemArtifact(systemArtifactsDir);
  Injector injector=AppFabricTestHelper.getInjector(cConf);
  artifactRepository=injector.getInstance(ArtifactRepository.class);
  AuthorizerInstantiator instantiatorService=injector.getInstance(AuthorizerInstantiator.class);
  authorizer=instantiatorService.get();
  authEnforcerService=injector.getInstance(AuthorizationEnforcementService.class);
  authEnforcerService.startAndWait();
  instance=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
}"
5991,"@Test public void testAuthorizationForSystemArtifacts() throws Exception {
  SecurityRequestContext.setUserId(Principal.SYSTEM.getName());
  artifactRepository.addSystemArtifacts();
  SecurityRequestContext.setUserId(ALICE.getName());
  try {
    artifactRepository.addSystemArtifacts();
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  authorizer.grant(instance,ALICE,Collections.singleton(Action.WRITE));
  Assert.assertEquals(Collections.singleton(new Privilege(instance,Action.WRITE)),authorizer.listPrivileges(ALICE));
  artifactRepository.addSystemArtifacts();
  ArtifactId systemArtifact=NamespaceId.SYSTEM.artifact(""String_Node_Str"",""String_Node_Str"");
  try {
    artifactRepository.deleteArtifact(systemArtifact.toId());
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  authorizer.grant(instance,ALICE,Collections.singleton(Action.ADMIN));
  Assert.assertEquals(ImmutableSet.of(new Privilege(instance,Action.WRITE),new Privilege(instance,Action.ADMIN)),authorizer.listPrivileges(ALICE));
  artifactRepository.deleteArtifact(systemArtifact.toId());
}","@Test public void testAuthorizationForSystemArtifacts() throws Exception {
  SecurityRequestContext.setUserId(Principal.SYSTEM.getName());
  artifactRepository.addSystemArtifacts();
  SecurityRequestContext.setUserId(ALICE.getName());
  try {
    artifactRepository.addSystemArtifacts();
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  authorizer.grant(instance,ALICE,Collections.singleton(Action.WRITE));
  Assert.assertEquals(Collections.singleton(new Privilege(instance,Action.WRITE)),authorizer.listPrivileges(ALICE));
  artifactRepository.addSystemArtifacts();
  try {
    artifactRepository.deleteArtifact(SYSTEM_ARTIFACT.toId());
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  authorizer.grant(instance,ALICE,Collections.singleton(Action.ADMIN));
  NamespaceId namespaceId=new NamespaceId(""String_Node_Str"");
  namespaceAdmin.create(new NamespaceMeta.Builder().setName(namespaceId.getNamespace()).build());
  authorizer.revoke(instance);
  List<ArtifactSummary> artifacts=artifactRepository.getArtifacts(namespaceId,true);
  Assert.assertEquals(1,artifacts.size());
  ArtifactSummary artifactSummary=artifacts.get(0);
  Assert.assertTrue(SYSTEM_ARTIFACT.getArtifact().equals(artifactSummary.getName()));
  Assert.assertTrue(SYSTEM_ARTIFACT.getVersion().equals(artifactSummary.getVersion()));
  Assert.assertTrue(SYSTEM_ARTIFACT.getNamespace().equals(artifactSummary.getScope().name().toLowerCase()));
  namespaceAdmin.delete(namespaceId.toId());
  Assert.assertEquals(Collections.emptySet(),authorizer.listPrivileges(ALICE));
  authorizer.grant(instance,ALICE,Collections.singleton(Action.ADMIN));
  artifactRepository.deleteArtifact(SYSTEM_ARTIFACT.toId());
}"
5992,"protected void assertProgramStatus(ProgramClient programClient,Id.Program programId,String programStatus) throws IOException, ProgramNotFoundException, UnauthenticatedException {
  assertProgramStatus(programClient,programId,programStatus,180);
}","protected void assertProgramStatus(ProgramClient programClient,Id.Program programId,String programStatus) throws IOException, ProgramNotFoundException, UnauthenticatedException, UnauthorizedException {
  assertProgramStatus(programClient,programId,programStatus,180);
}"
5993,"private void checkConnection(ClientConfig baseClientConfig,ConnectionConfig connectionInfo,AccessToken accessToken) throws IOException, UnauthenticatedException {
  ClientConfig clientConfig=new ClientConfig.Builder(baseClientConfig).setConnectionConfig(connectionInfo).setAccessToken(accessToken).build();
  MetaClient metaClient=new MetaClient(clientConfig);
  try {
    metaClient.ping();
  }
 catch (  IOException e) {
    throw new IOException(""String_Node_Str"",e);
  }
}","private void checkConnection(ClientConfig baseClientConfig,ConnectionConfig connectionInfo,AccessToken accessToken) throws IOException, UnauthenticatedException, UnauthorizedException {
  ClientConfig clientConfig=new ClientConfig.Builder(baseClientConfig).setConnectionConfig(connectionInfo).setAccessToken(accessToken).build();
  MetaClient metaClient=new MetaClient(clientConfig);
  try {
    metaClient.ping();
  }
 catch (  IOException e) {
    throw new IOException(""String_Node_Str"",e);
  }
}"
5994,"@Nullable private UserAccessToken acquireAccessToken(ClientConfig clientConfig,ConnectionConfig connectionInfo,PrintStream output,boolean debug) throws IOException {
  if (!isAuthenticationEnabled(connectionInfo)) {
    return null;
  }
  try {
    UserAccessToken savedToken=getSavedAccessToken(connectionInfo.getHostname());
    if (savedToken == null) {
      throw new UnauthenticatedException();
    }
    checkConnection(clientConfig,connectionInfo,savedToken.getAccessToken());
    return savedToken;
  }
 catch (  UnauthenticatedException ignored) {
  }
  return getNewAccessToken(connectionInfo,output,debug);
}","@Nullable private UserAccessToken acquireAccessToken(ClientConfig clientConfig,ConnectionConfig connectionInfo,PrintStream output,boolean debug) throws IOException, UnauthorizedException {
  if (!isAuthenticationEnabled(connectionInfo)) {
    return null;
  }
  try {
    UserAccessToken savedToken=getSavedAccessToken(connectionInfo.getHostname());
    if (savedToken == null) {
      throw new UnauthenticatedException();
    }
    checkConnection(clientConfig,connectionInfo,savedToken.getAccessToken());
    return savedToken;
  }
 catch (  UnauthenticatedException ignored) {
  }
  return getNewAccessToken(connectionInfo,output,debug);
}"
5995,"private Map<String,String> parsePreferences(String[] programIdParts) throws IOException, UnauthenticatedException, NotFoundException {
switch (type) {
case INSTANCE:
    checkInputLength(programIdParts,0);
  return client.getInstancePreferences();
case NAMESPACE:
checkInputLength(programIdParts,0);
return client.getNamespacePreferences(cliConfig.getCurrentNamespace(),resolved);
case APP:
return client.getApplicationPreferences(parseAppId(programIdParts),resolved);
case FLOW:
case MAPREDUCE:
case WORKFLOW:
case SERVICE:
case SPARK:
return client.getProgramPreferences(parseProgramId(programIdParts,type.getProgramType()),resolved);
default :
throw new IllegalArgumentException(""String_Node_Str"" + type.getShortName());
}
}","private Map<String,String> parsePreferences(String[] programIdParts) throws IOException, UnauthenticatedException, NotFoundException, UnauthorizedException {
switch (type) {
case INSTANCE:
    checkInputLength(programIdParts,0);
  return client.getInstancePreferences();
case NAMESPACE:
checkInputLength(programIdParts,0);
return client.getNamespacePreferences(cliConfig.getCurrentNamespace(),resolved);
case APP:
return client.getApplicationPreferences(parseAppId(programIdParts),resolved);
case FLOW:
case MAPREDUCE:
case WORKFLOW:
case SERVICE:
case SPARK:
return client.getProgramPreferences(parseProgramId(programIdParts,type.getProgramType()),resolved);
default :
throw new IllegalArgumentException(""String_Node_Str"" + type.getShortName());
}
}"
5996,"private Table getWorkflowLocalDatasets(ProgramRunId programRunId) throws UnauthenticatedException, IOException, NotFoundException {
  Map<String,DatasetSpecificationSummary> workflowLocalDatasets=workflowClient.getWorkflowLocalDatasets(programRunId);
  List<Map.Entry<String,DatasetSpecificationSummary>> localDatasetSummaries=new ArrayList<>();
  localDatasetSummaries.addAll(workflowLocalDatasets.entrySet());
  return Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(localDatasetSummaries,new RowMaker<Map.Entry<String,DatasetSpecificationSummary>>(){
    @Override public List<?> makeRow(    Map.Entry<String,DatasetSpecificationSummary> object){
      return Lists.newArrayList(object.getKey(),object.getValue().getName(),object.getValue().getType());
    }
  }
).build();
}","private Table getWorkflowLocalDatasets(ProgramRunId programRunId) throws UnauthenticatedException, IOException, NotFoundException, UnauthorizedException {
  Map<String,DatasetSpecificationSummary> workflowLocalDatasets=workflowClient.getWorkflowLocalDatasets(programRunId);
  List<Map.Entry<String,DatasetSpecificationSummary>> localDatasetSummaries=new ArrayList<>();
  localDatasetSummaries.addAll(workflowLocalDatasets.entrySet());
  return Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(localDatasetSummaries,new RowMaker<Map.Entry<String,DatasetSpecificationSummary>>(){
    @Override public List<?> makeRow(    Map.Entry<String,DatasetSpecificationSummary> object){
      return Lists.newArrayList(object.getKey(),object.getValue().getName(),object.getValue().getType());
    }
  }
).build();
}"
5997,"private Table getWorkflowNodeStates(ProgramRunId programRunId) throws UnauthenticatedException, IOException, NotFoundException {
  Map<String,WorkflowNodeStateDetail> workflowNodeStates=workflowClient.getWorkflowNodeStates(programRunId);
  List<Map.Entry<String,WorkflowNodeStateDetail>> nodeStates=new ArrayList<>();
  nodeStates.addAll(workflowNodeStates.entrySet());
  return Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(nodeStates,new RowMaker<Map.Entry<String,WorkflowNodeStateDetail>>(){
    @Override public List<?> makeRow(    Map.Entry<String,WorkflowNodeStateDetail> object){
      return Lists.newArrayList(object.getValue().getNodeId(),object.getValue().getNodeStatus(),object.getValue().getRunId(),object.getValue().getFailureCause());
    }
  }
).build();
}","private Table getWorkflowNodeStates(ProgramRunId programRunId) throws UnauthenticatedException, IOException, NotFoundException, UnauthorizedException {
  Map<String,WorkflowNodeStateDetail> workflowNodeStates=workflowClient.getWorkflowNodeStates(programRunId);
  List<Map.Entry<String,WorkflowNodeStateDetail>> nodeStates=new ArrayList<>();
  nodeStates.addAll(workflowNodeStates.entrySet());
  return Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").setRows(nodeStates,new RowMaker<Map.Entry<String,WorkflowNodeStateDetail>>(){
    @Override public List<?> makeRow(    Map.Entry<String,WorkflowNodeStateDetail> object){
      return Lists.newArrayList(object.getValue().getNodeId(),object.getValue().getNodeStatus(),object.getValue().getRunId(),object.getValue().getFailureCause());
    }
  }
).build();
}"
5998,"private Table getWorkflowToken(Id.Run runId,WorkflowToken.Scope workflowTokenScope,String key,String nodeName) throws UnauthenticatedException, IOException, NotFoundException {
  WorkflowTokenNodeDetail workflowToken=workflowClient.getWorkflowTokenAtNode(runId,nodeName,workflowTokenScope,key);
  List<Map.Entry<String,String>> tokenKeys=new ArrayList<>();
  tokenKeys.addAll(workflowToken.getTokenDataAtNode().entrySet());
  return Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"").setRows(tokenKeys,new RowMaker<Map.Entry<String,String>>(){
    @Override public List<?> makeRow(    Map.Entry<String,String> object){
      return Lists.newArrayList(object.getKey(),object.getValue());
    }
  }
).build();
}","private Table getWorkflowToken(Id.Run runId,WorkflowToken.Scope workflowTokenScope,String key,String nodeName) throws UnauthenticatedException, IOException, NotFoundException, UnauthorizedException {
  WorkflowTokenNodeDetail workflowToken=workflowClient.getWorkflowTokenAtNode(runId,nodeName,workflowTokenScope,key);
  List<Map.Entry<String,String>> tokenKeys=new ArrayList<>();
  tokenKeys.addAll(workflowToken.getTokenDataAtNode().entrySet());
  return Table.builder().setHeader(""String_Node_Str"",""String_Node_Str"").setRows(tokenKeys,new RowMaker<Map.Entry<String,String>>(){
    @Override public List<?> makeRow(    Map.Entry<String,String> object){
      return Lists.newArrayList(object.getKey(),object.getValue());
    }
  }
).build();
}"
5999,"/** 
 * Reads arguments to get app id, program types, and list of input programs.
 */
protected Args<T> readArgs(Arguments arguments) throws ApplicationNotFoundException, UnauthenticatedException, IOException {
  String appName=arguments.get(ArgumentName.APP.getName());
  Id.Application appId=Id.Application.from(cliConfig.getCurrentNamespace(),appName);
  Set<ProgramType> programTypes=getDefaultProgramTypes();
  if (arguments.hasArgument(ArgumentName.PROGRAM_TYPES.getName())) {
    programTypes.clear();
    String programTypesStr=arguments.get(ArgumentName.PROGRAM_TYPES.getName());
    for (    String programTypeStr : Splitter.on(',').trimResults().split(programTypesStr)) {
      ProgramType programType=ProgramType.valueOf(programTypeStr.toUpperCase());
      programTypes.add(programType);
    }
  }
  List<T> programs=new ArrayList<>();
  Map<ProgramType,List<ProgramRecord>> appPrograms=appClient.listProgramsByType(appId);
  for (  ProgramType programType : programTypes) {
    List<ProgramRecord> programRecords=appPrograms.get(programType);
    if (programRecords != null) {
      for (      ProgramRecord programRecord : programRecords) {
        programs.add(createProgram(programRecord));
      }
    }
  }
  return new Args<>(appId,programTypes,programs);
}","/** 
 * Reads arguments to get app id, program types, and list of input programs.
 */
protected Args<T> readArgs(Arguments arguments) throws ApplicationNotFoundException, UnauthenticatedException, IOException, UnauthorizedException {
  String appName=arguments.get(ArgumentName.APP.getName());
  Id.Application appId=Id.Application.from(cliConfig.getCurrentNamespace(),appName);
  Set<ProgramType> programTypes=getDefaultProgramTypes();
  if (arguments.hasArgument(ArgumentName.PROGRAM_TYPES.getName())) {
    programTypes.clear();
    String programTypesStr=arguments.get(ArgumentName.PROGRAM_TYPES.getName());
    for (    String programTypeStr : Splitter.on(',').trimResults().split(programTypesStr)) {
      ProgramType programType=ProgramType.valueOf(programTypeStr.toUpperCase());
      programTypes.add(programType);
    }
  }
  List<T> programs=new ArrayList<>();
  Map<ProgramType,List<ProgramRecord>> appPrograms=appClient.listProgramsByType(appId);
  for (  ProgramType programType : programTypes) {
    List<ProgramRecord> programRecords=appPrograms.get(programType);
    if (programRecords != null) {
      for (      ProgramRecord programRecord : programRecords) {
        programs.add(createProgram(programRecord));
      }
    }
  }
  return new Args<>(appId,programTypes,programs);
}"
6000,"@Override public Collection<String> get(){
  try {
    List<ApplicationRecord> appsList=applicationClient.list(cliConfig.getCurrentNamespace());
    List<String> appIds=Lists.newArrayList();
    for (    ApplicationRecord item : appsList) {
      appIds.add(item.getName());
    }
    return appIds;
  }
 catch (  IOException e) {
    return Lists.newArrayList();
  }
catch (  UnauthenticatedException e) {
    return Lists.newArrayList();
  }
}","@Override public Collection<String> get(){
  try {
    List<ApplicationRecord> appsList=applicationClient.list(cliConfig.getCurrentNamespace());
    List<String> appIds=new ArrayList<>();
    for (    ApplicationRecord item : appsList) {
      appIds.add(item.getName());
    }
    return appIds;
  }
 catch (  IOException|UnauthenticatedException|UnauthorizedException e) {
    return new ArrayList<>();
  }
}"
