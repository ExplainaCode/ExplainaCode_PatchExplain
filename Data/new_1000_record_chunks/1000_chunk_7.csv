record_number,buggy_code,fixed_code
6001,"@Inject public AppIdCompleter(final ApplicationClient applicationClient,final CLIConfig cliConfig){
  super(new Supplier<Collection<String>>(){
    @Override public Collection<String> get(){
      try {
        List<ApplicationRecord> appsList=applicationClient.list(cliConfig.getCurrentNamespace());
        List<String> appIds=Lists.newArrayList();
        for (        ApplicationRecord item : appsList) {
          appIds.add(item.getName());
        }
        return appIds;
      }
 catch (      IOException e) {
        return Lists.newArrayList();
      }
catch (      UnauthenticatedException e) {
        return Lists.newArrayList();
      }
    }
  }
);
}","@Inject public AppIdCompleter(final ApplicationClient applicationClient,final CLIConfig cliConfig){
  super(new Supplier<Collection<String>>(){
    @Override public Collection<String> get(){
      try {
        List<ApplicationRecord> appsList=applicationClient.list(cliConfig.getCurrentNamespace());
        List<String> appIds=new ArrayList<>();
        for (        ApplicationRecord item : appsList) {
          appIds.add(item.getName());
        }
        return appIds;
      }
 catch (      IOException|UnauthenticatedException|UnauthorizedException e) {
        return new ArrayList<>();
      }
    }
  }
);
}"
6002,"@Override public Collection<String> get(){
  try {
    List<DatasetModuleMeta> list=datasetModuleClient.list(cliConfig.getCurrentNamespace());
    return Lists.newArrayList(Iterables.transform(list,new Function<DatasetModuleMeta,String>(){
      @Override public String apply(      DatasetModuleMeta input){
        return input.getName();
      }
    }
));
  }
 catch (  IOException e) {
    return Lists.newArrayList();
  }
catch (  UnauthenticatedException e) {
    return Lists.newArrayList();
  }
}","@Override public Collection<String> get(){
  try {
    List<DatasetModuleMeta> list=datasetModuleClient.list(cliConfig.getCurrentNamespace());
    return Lists.newArrayList(Iterables.transform(list,new Function<DatasetModuleMeta,String>(){
      @Override public String apply(      DatasetModuleMeta input){
        return input.getName();
      }
    }
));
  }
 catch (  IOException|UnauthenticatedException|UnauthorizedException e) {
    return new ArrayList<>();
  }
}"
6003,"@Inject public DatasetModuleNameCompleter(final DatasetModuleClient datasetModuleClient,final CLIConfig cliConfig){
  super(new Supplier<Collection<String>>(){
    @Override public Collection<String> get(){
      try {
        List<DatasetModuleMeta> list=datasetModuleClient.list(cliConfig.getCurrentNamespace());
        return Lists.newArrayList(Iterables.transform(list,new Function<DatasetModuleMeta,String>(){
          @Override public String apply(          DatasetModuleMeta input){
            return input.getName();
          }
        }
));
      }
 catch (      IOException e) {
        return Lists.newArrayList();
      }
catch (      UnauthenticatedException e) {
        return Lists.newArrayList();
      }
    }
  }
);
}","@Inject public DatasetModuleNameCompleter(final DatasetModuleClient datasetModuleClient,final CLIConfig cliConfig){
  super(new Supplier<Collection<String>>(){
    @Override public Collection<String> get(){
      try {
        List<DatasetModuleMeta> list=datasetModuleClient.list(cliConfig.getCurrentNamespace());
        return Lists.newArrayList(Iterables.transform(list,new Function<DatasetModuleMeta,String>(){
          @Override public String apply(          DatasetModuleMeta input){
            return input.getName();
          }
        }
));
      }
 catch (      IOException|UnauthenticatedException|UnauthorizedException e) {
        return new ArrayList<>();
      }
    }
  }
);
}"
6004,"@Inject public DatasetNameCompleter(final DatasetClient datasetClient,final CLIConfig cliConfig){
  super(new Supplier<Collection<String>>(){
    @Override public Collection<String> get(){
      try {
        List<DatasetSpecificationSummary> list=datasetClient.list(cliConfig.getCurrentNamespace());
        return Lists.newArrayList(Iterables.transform(list,new Function<DatasetSpecificationSummary,String>(){
          @Override public String apply(          DatasetSpecificationSummary input){
            String[] tokens=input.getName().split(""String_Node_Str"");
            return tokens[tokens.length - 1];
          }
        }
));
      }
 catch (      IOException e) {
        return Lists.newArrayList();
      }
catch (      UnauthenticatedException e) {
        return Lists.newArrayList();
      }
    }
  }
);
}","@Inject public DatasetNameCompleter(final DatasetClient datasetClient,final CLIConfig cliConfig){
  super(new Supplier<Collection<String>>(){
    @Override public Collection<String> get(){
      try {
        List<DatasetSpecificationSummary> list=datasetClient.list(cliConfig.getCurrentNamespace());
        return Lists.newArrayList(Iterables.transform(list,new Function<DatasetSpecificationSummary,String>(){
          @Override public String apply(          DatasetSpecificationSummary input){
            String[] tokens=input.getName().split(""String_Node_Str"");
            return tokens[tokens.length - 1];
          }
        }
));
      }
 catch (      IOException|UnauthenticatedException|UnauthorizedException e) {
        return new ArrayList<>();
      }
    }
  }
);
}"
6005,"@Override public Collection<String> get(){
  try {
    List<DatasetSpecificationSummary> list=datasetClient.list(cliConfig.getCurrentNamespace());
    return Lists.newArrayList(Iterables.transform(list,new Function<DatasetSpecificationSummary,String>(){
      @Override public String apply(      DatasetSpecificationSummary input){
        String[] tokens=input.getName().split(""String_Node_Str"");
        return tokens[tokens.length - 1];
      }
    }
));
  }
 catch (  IOException e) {
    return Lists.newArrayList();
  }
catch (  UnauthenticatedException e) {
    return Lists.newArrayList();
  }
}","@Override public Collection<String> get(){
  try {
    List<DatasetSpecificationSummary> list=datasetClient.list(cliConfig.getCurrentNamespace());
    return Lists.newArrayList(Iterables.transform(list,new Function<DatasetSpecificationSummary,String>(){
      @Override public String apply(      DatasetSpecificationSummary input){
        String[] tokens=input.getName().split(""String_Node_Str"");
        return tokens[tokens.length - 1];
      }
    }
));
  }
 catch (  IOException|UnauthenticatedException|UnauthorizedException e) {
    return new ArrayList<>();
  }
}"
6006,"@Inject public DatasetTypeNameCompleter(final DatasetTypeClient datasetTypeClient,final CLIConfig cliConfig){
  super(new Supplier<Collection<String>>(){
    @Override public Collection<String> get(){
      try {
        List<DatasetTypeMeta> list=datasetTypeClient.list(cliConfig.getCurrentNamespace());
        return Lists.newArrayList(Iterables.transform(list,new Function<DatasetTypeMeta,String>(){
          @Override public String apply(          DatasetTypeMeta input){
            return input.getName();
          }
        }
));
      }
 catch (      IOException e) {
        return Lists.newArrayList();
      }
catch (      UnauthenticatedException e) {
        return Lists.newArrayList();
      }
    }
  }
);
}","@Inject public DatasetTypeNameCompleter(final DatasetTypeClient datasetTypeClient,final CLIConfig cliConfig){
  super(new Supplier<Collection<String>>(){
    @Override public Collection<String> get(){
      try {
        List<DatasetTypeMeta> list=datasetTypeClient.list(cliConfig.getCurrentNamespace());
        return Lists.newArrayList(Iterables.transform(list,new Function<DatasetTypeMeta,String>(){
          @Override public String apply(          DatasetTypeMeta input){
            return input.getName();
          }
        }
));
      }
 catch (      IOException|UnauthenticatedException|UnauthorizedException e) {
        return new ArrayList<>();
      }
    }
  }
);
}"
6007,"@Override public Collection<String> get(){
  try {
    List<DatasetTypeMeta> list=datasetTypeClient.list(cliConfig.getCurrentNamespace());
    return Lists.newArrayList(Iterables.transform(list,new Function<DatasetTypeMeta,String>(){
      @Override public String apply(      DatasetTypeMeta input){
        return input.getName();
      }
    }
));
  }
 catch (  IOException e) {
    return Lists.newArrayList();
  }
catch (  UnauthenticatedException e) {
    return Lists.newArrayList();
  }
}","@Override public Collection<String> get(){
  try {
    List<DatasetTypeMeta> list=datasetTypeClient.list(cliConfig.getCurrentNamespace());
    return Lists.newArrayList(Iterables.transform(list,new Function<DatasetTypeMeta,String>(){
      @Override public String apply(      DatasetTypeMeta input){
        return input.getName();
      }
    }
));
  }
 catch (  IOException|UnauthenticatedException|UnauthorizedException e) {
    return new ArrayList<>();
  }
}"
6008,"public Collection<String> getEndpoints(Id.Service serviceId,String method){
  Collection<String> httpEndpoints=Lists.newArrayList();
  try {
    for (    ServiceHttpEndpoint endpoint : serviceClient.getEndpoints(serviceId)) {
      if (endpoint.getMethod().equals(method)) {
        httpEndpoints.add(endpoint.getPath());
      }
    }
  }
 catch (  IOException|NotFoundException|UnauthenticatedException ignored) {
  }
  return httpEndpoints;
}","public Collection<String> getEndpoints(Id.Service serviceId,String method){
  Collection<String> httpEndpoints=Lists.newArrayList();
  try {
    for (    ServiceHttpEndpoint endpoint : serviceClient.getEndpoints(serviceId)) {
      if (endpoint.getMethod().equals(method)) {
        httpEndpoints.add(endpoint.getPath());
      }
    }
  }
 catch (  IOException|NotFoundException|UnauthenticatedException|UnauthorizedException ignored) {
  }
  return httpEndpoints;
}"
6009,"public Collection<String> getMethods(Id.Service serviceId){
  Collection<String> httpMethods=Lists.newArrayList();
  try {
    for (    ServiceHttpEndpoint endpoint : serviceClient.getEndpoints(serviceId)) {
      String method=endpoint.getMethod();
      if (!httpMethods.contains(method)) {
        httpMethods.add(method);
      }
    }
  }
 catch (  IOException|UnauthenticatedException|NotFoundException ignored) {
  }
  return httpMethods;
}","public Collection<String> getMethods(Id.Service serviceId){
  Collection<String> httpMethods=Lists.newArrayList();
  try {
    for (    ServiceHttpEndpoint endpoint : serviceClient.getEndpoints(serviceId)) {
      String method=endpoint.getMethod();
      if (!httpMethods.contains(method)) {
        httpMethods.add(method);
      }
    }
  }
 catch (  IOException|UnauthenticatedException|NotFoundException|UnauthorizedException ignored) {
  }
  return httpMethods;
}"
6010,"public ProgramIdCompleter(final ApplicationClient appClient,final CLIConfig cliConfig,final ProgramType programType){
  super(new Supplier<Collection<String>>(){
    @Override public Collection<String> get(){
      try {
        List<ProgramRecord> programs=appClient.listAllPrograms(cliConfig.getCurrentNamespace(),programType);
        List<String> programIds=Lists.newArrayList();
        for (        ProgramRecord programRecord : programs) {
          programIds.add(programRecord.getApp() + ""String_Node_Str"" + programRecord.getName());
        }
        return programIds;
      }
 catch (      IOException e) {
        return Lists.newArrayList();
      }
catch (      UnauthenticatedException e) {
        return Lists.newArrayList();
      }
    }
  }
);
}","public ProgramIdCompleter(final ApplicationClient appClient,final CLIConfig cliConfig,final ProgramType programType){
  super(new Supplier<Collection<String>>(){
    @Override public Collection<String> get(){
      try {
        List<ProgramRecord> programs=appClient.listAllPrograms(cliConfig.getCurrentNamespace(),programType);
        List<String> programIds=new ArrayList<>();
        for (        ProgramRecord programRecord : programs) {
          programIds.add(programRecord.getApp() + ""String_Node_Str"" + programRecord.getName());
        }
        return programIds;
      }
 catch (      IOException|UnauthenticatedException|UnauthorizedException e) {
        return new ArrayList<>();
      }
    }
  }
);
}"
6011,"@Override public Collection<String> get(){
  try {
    List<ProgramRecord> programs=appClient.listAllPrograms(cliConfig.getCurrentNamespace(),programType);
    List<String> programIds=Lists.newArrayList();
    for (    ProgramRecord programRecord : programs) {
      programIds.add(programRecord.getApp() + ""String_Node_Str"" + programRecord.getName());
    }
    return programIds;
  }
 catch (  IOException e) {
    return Lists.newArrayList();
  }
catch (  UnauthenticatedException e) {
    return Lists.newArrayList();
  }
}","@Override public Collection<String> get(){
  try {
    List<ProgramRecord> programs=appClient.listAllPrograms(cliConfig.getCurrentNamespace(),programType);
    List<String> programIds=new ArrayList<>();
    for (    ProgramRecord programRecord : programs) {
      programIds.add(programRecord.getApp() + ""String_Node_Str"" + programRecord.getName());
    }
    return programIds;
  }
 catch (  IOException|UnauthenticatedException|UnauthorizedException e) {
    return new ArrayList<>();
  }
}"
6012,"/** 
 * Ensures that the logged-in user has a   {@link Action privilege} on the specified dataset instance.
 * @param artifactId the {@link co.cask.cdap.proto.id.ArtifactId} to check for privileges
 * @throws UnauthorizedException if the logged in user has no {@link Action privileges} on the specified dataset
 */
private void ensureAccess(co.cask.cdap.proto.id.ArtifactId artifactId) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  Predicate<EntityId> filter=authorizationEnforcer.createFilter(principal);
  if (!Principal.SYSTEM.equals(principal) && !filter.apply(artifactId)) {
    throw new UnauthorizedException(principal,artifactId);
  }
}","/** 
 * Ensures that the logged-in user has a   {@link Action privilege} on the specified dataset instance.
 * @param artifactId the {@link co.cask.cdap.proto.id.ArtifactId} to check for privileges
 * @throws UnauthorizedException if the logged in user has no {@link Action privileges} on the specified dataset
 */
private void ensureAccess(co.cask.cdap.proto.id.ArtifactId artifactId) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  Predicate<EntityId> filter=authorizationEnforcer.createFilter(principal);
  if (Principal.SYSTEM.equals(principal) || NamespaceId.SYSTEM.equals(artifactId.getParent())) {
    return;
  }
  if (!filter.apply(artifactId)) {
    throw new UnauthorizedException(principal,artifactId);
  }
}"
6013,"@Test public void testAuthorizationForSystemArtifacts() throws Exception {
  SecurityRequestContext.setUserId(Principal.SYSTEM.getName());
  artifactRepository.addSystemArtifacts();
  SecurityRequestContext.setUserId(ALICE.getName());
  try {
    artifactRepository.addSystemArtifacts();
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  authorizer.grant(instance,ALICE,Collections.singleton(Action.WRITE));
  Assert.assertEquals(Collections.singleton(new Privilege(instance,Action.WRITE)),authorizer.listPrivileges(ALICE));
  artifactRepository.addSystemArtifacts();
  try {
    artifactRepository.deleteArtifact(SYSTEM_ARTIFACT.toId());
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  authorizer.grant(instance,ALICE,Collections.singleton(Action.ADMIN));
  NamespaceId namespaceId=new NamespaceId(""String_Node_Str"");
  namespaceAdmin.create(new NamespaceMeta.Builder().setName(namespaceId.getNamespace()).build());
  authorizer.revoke(instance);
  List<ArtifactSummary> artifacts=artifactRepository.getArtifacts(namespaceId,true);
  Assert.assertEquals(1,artifacts.size());
  ArtifactSummary artifactSummary=artifacts.get(0);
  Assert.assertTrue(SYSTEM_ARTIFACT.getArtifact().equals(artifactSummary.getName()));
  Assert.assertTrue(SYSTEM_ARTIFACT.getVersion().equals(artifactSummary.getVersion()));
  Assert.assertTrue(SYSTEM_ARTIFACT.getNamespace().equals(artifactSummary.getScope().name().toLowerCase()));
  namespaceAdmin.delete(namespaceId.toId());
  Assert.assertEquals(Collections.emptySet(),authorizer.listPrivileges(ALICE));
  authorizer.grant(instance,ALICE,Collections.singleton(Action.ADMIN));
  artifactRepository.deleteArtifact(SYSTEM_ARTIFACT.toId());
}","@Test public void testAuthorizationForSystemArtifacts() throws Exception {
  SecurityRequestContext.setUserId(Principal.SYSTEM.getName());
  artifactRepository.addSystemArtifacts();
  SecurityRequestContext.setUserId(ALICE.getName());
  try {
    artifactRepository.addSystemArtifacts();
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  authorizer.grant(instance,ALICE,Collections.singleton(Action.WRITE));
  Assert.assertEquals(Collections.singleton(new Privilege(instance,Action.WRITE)),authorizer.listPrivileges(ALICE));
  artifactRepository.addSystemArtifacts();
  try {
    artifactRepository.deleteArtifact(SYSTEM_ARTIFACT.toId());
    Assert.fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  authorizer.grant(instance,ALICE,Collections.singleton(Action.ADMIN));
  NamespaceId namespaceId=new NamespaceId(""String_Node_Str"");
  namespaceAdmin.create(new NamespaceMeta.Builder().setName(namespaceId.getNamespace()).build());
  authorizer.revoke(instance);
  List<ArtifactSummary> artifacts=artifactRepository.getArtifacts(namespaceId,true);
  Assert.assertEquals(1,artifacts.size());
  ArtifactSummary artifactSummary=artifacts.get(0);
  Assert.assertEquals(SYSTEM_ARTIFACT.getArtifact(),artifactSummary.getName());
  Assert.assertEquals(SYSTEM_ARTIFACT.getVersion(),artifactSummary.getVersion());
  Assert.assertEquals(SYSTEM_ARTIFACT.getNamespace(),artifactSummary.getScope().name().toLowerCase());
  ArtifactDetail artifactDetail=artifactRepository.getArtifact(SYSTEM_ARTIFACT.toId());
  co.cask.cdap.api.artifact.ArtifactId artifactId=artifactDetail.getDescriptor().getArtifactId();
  Assert.assertEquals(SYSTEM_ARTIFACT.getArtifact(),artifactId.getName());
  Assert.assertEquals(SYSTEM_ARTIFACT.getVersion(),artifactId.getVersion().getVersion());
  Assert.assertEquals(SYSTEM_ARTIFACT.getNamespace(),artifactId.getScope().name().toLowerCase());
  namespaceAdmin.delete(namespaceId.toId());
  Assert.assertEquals(Collections.emptySet(),authorizer.listPrivileges(ALICE));
  authorizer.grant(instance,ALICE,Collections.singleton(Action.ADMIN));
  artifactRepository.deleteArtifact(SYSTEM_ARTIFACT.toId());
}"
6014,"@VisibleForTesting static Injector createInjector(CConfiguration cConf,Configuration hConf){
  return Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new LocationRuntimeModule().getDistributedModules(),new NamespaceClientRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new ExploreClientModule(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new KafkaClientModule(),new AuditModule().getDistributedModules(),new AuthorizationModule(),new AuthorizationEnforcementModule().getDistributedModules(),new SecureStoreModules().getDistributedModules(),new AuthenticationContextModules().getMasterModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Scopes.SINGLETON);
    }
  }
);
}","@VisibleForTesting static Injector createInjector(CConfiguration cConf,Configuration hConf){
  return Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new LocationRuntimeModule().getDistributedModules(),new NamespaceClientRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new ExploreClientModule(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new KafkaClientModule(),new AuditModule().getDistributedModules(),new AuthorizationEnforcementModule().getDistributedModules(),new SecureStoreModules().getDistributedModules(),new AuthenticationContextModules().getMasterModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Scopes.SINGLETON);
      bind(PrivilegesManager.class).to(RemotePrivilegesManager.class);
    }
  }
);
}"
6015,"@Override protected void configure(){
  bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
  bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Scopes.SINGLETON);
}","@Override protected void configure(){
  bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
  bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Scopes.SINGLETON);
  bind(PrivilegesManager.class).to(RemotePrivilegesManager.class);
}"
6016,"private boolean updateWriter(long length) throws IOException {
  bodySize+=length;
  if (bodySize >= bufferThreshold) {
    ContentWriter fileContentWriter;
    try {
      fileContentWriter=impersonator.doAs(streamId.getParent(),new Callable<ContentWriter>(){
        @Override public ContentWriter call() throws Exception {
          return fileContentWriterFactory.create(ImmutableMap.<String,String>of());
        }
      }
);
    }
 catch (    Exception e) {
      Throwables.propagateIfPossible(e,IOException.class);
      throw new IOException(e);
    }
    fileContentWriter.appendAll(bufferedContentWriter.iterator(),true);
    bufferedContentWriter.cancel();
    return true;
  }
  return false;
}","private boolean updateWriter(long length) throws IOException {
  bodySize+=length;
  if (bodySize >= bufferThreshold) {
    try {
      fileContentWriter=impersonator.doAs(streamId.getParent(),new Callable<ContentWriter>(){
        @Override public ContentWriter call() throws Exception {
          return fileContentWriterFactory.create(ImmutableMap.<String,String>of());
        }
      }
);
    }
 catch (    Exception e) {
      Throwables.propagateIfPossible(e,IOException.class);
      throw new IOException(e);
    }
    fileContentWriter.appendAll(bufferedContentWriter.iterator(),true);
    bufferedContentWriter.cancel();
    return true;
  }
  return false;
}"
6017,"@Test public void testWindower() throws Exception {
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  List<StructuredRecord> input=ImmutableList.of(StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build(),StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build(),StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build(),StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build());
  String sink1Name=""String_Node_Str"";
  String sink2Name=""String_Node_Str"";
  String sink3Name=""String_Node_Str"";
  DataStreamsConfig etlConfig=DataStreamsConfig.builder().addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(schema,input,1000L))).addStage(new ETLStage(""String_Node_Str"",Window.getPlugin(1,1))).addStage(new ETLStage(""String_Node_Str"",Window.getPlugin(2,1))).addStage(new ETLStage(""String_Node_Str"",Window.getPlugin(2,2))).addStage(new ETLStage(""String_Node_Str"",FieldCountAggregator.getPlugin(""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",FieldCountAggregator.getPlugin(""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",FieldCountAggregator.getPlugin(""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",StringValueFilterTransform.getPlugin(""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",StringValueFilterTransform.getPlugin(""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",StringValueFilterTransform.getPlugin(""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(sink1Name))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(sink2Name))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(sink3Name))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").setBatchInterval(""String_Node_Str"").build();
  AppRequest<DataStreamsConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  SparkManager sparkManager=appManager.getSparkManager(DataStreamsSparkLauncher.NAME);
  sparkManager.start();
  sparkManager.waitForStatus(true,10,1);
  Schema outputSchema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.LONG)));
  final List<StructuredRecord> expected1=ImmutableList.of(StructuredRecord.builder(outputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build(),StructuredRecord.builder(outputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build(),StructuredRecord.builder(outputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build(),StructuredRecord.builder(outputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build());
  final DataSetManager<Table> outputManager1=getDataset(sink1Name);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      outputManager1.flush();
      return expected1.equals(MockSink.readOutput(outputManager1));
    }
  }
,4,TimeUnit.MINUTES);
  final List<StructuredRecord> expected2=ImmutableList.of(StructuredRecord.builder(outputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build(),StructuredRecord.builder(outputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build(),StructuredRecord.builder(outputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build(),StructuredRecord.builder(outputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build(),StructuredRecord.builder(outputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build());
  final DataSetManager<Table> outputManager2=getDataset(sink2Name);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      outputManager2.flush();
      return expected2.equals(MockSink.readOutput(outputManager2));
    }
  }
,4,TimeUnit.MINUTES);
  final List<StructuredRecord> possible1=ImmutableList.of(StructuredRecord.builder(outputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build(),StructuredRecord.builder(outputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build(),StructuredRecord.builder(outputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",1L).build());
  final List<StructuredRecord> possible2=ImmutableList.of(StructuredRecord.builder(outputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build(),StructuredRecord.builder(outputSchema).set(""String_Node_Str"",""String_Node_Str"").set(""String_Node_Str"",2L).build());
  final DataSetManager<Table> outputManager3=getDataset(sink3Name);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      outputManager3.flush();
      List<StructuredRecord> actual=MockSink.readOutput(outputManager3);
      return possible1.equals(actual) || possible2.equals(actual);
    }
  }
,4,TimeUnit.MINUTES);
  sparkManager.stop();
  sparkManager.waitForStatus(false,10,1);
}","@Test public void testWindower() throws Exception {
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  List<StructuredRecord> input=ImmutableList.of(StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build(),StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build(),StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build());
  String sinkName=""String_Node_Str"";
  DataStreamsConfig etlConfig=DataStreamsConfig.builder().addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(schema,input,1000L))).addStage(new ETLStage(""String_Node_Str"",Window.getPlugin(30,1))).addStage(new ETLStage(""String_Node_Str"",FieldCountAggregator.getPlugin(""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",StringValueFilterTransform.getPlugin(""String_Node_Str"",""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(sinkName))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").setBatchInterval(""String_Node_Str"").build();
  AppRequest<DataStreamsConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  SparkManager sparkManager=appManager.getSparkManager(DataStreamsSparkLauncher.NAME);
  sparkManager.start();
  sparkManager.waitForStatus(true,10,1);
  final DataSetManager<Table> outputManager=getDataset(sinkName);
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      outputManager.flush();
      boolean sawThree=false;
      for (      StructuredRecord record : MockSink.readOutput(outputManager)) {
        long count=record.get(""String_Node_Str"");
        if (count == 3L) {
          sawThree=true;
        }
        Assert.assertTrue(count <= 3L);
      }
      return sawThree;
    }
  }
,2,TimeUnit.MINUTES);
  sparkManager.stop();
  sparkManager.waitForStatus(false,10,1);
}"
6018,"private static CConfiguration createCConf() throws Exception {
  LocationFactory locationFactory=new LocalLocationFactory(TEMPORARY_FOLDER.newFolder());
  String secureStoreLocation=locationFactory.create(""String_Node_Str"").toURI().getPath();
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Security.Store.FILE_PATH,secureStoreLocation);
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  cConf.set(Constants.Security.Authorization.SUPERUSERS,""String_Node_Str"");
  Location authorizerJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,authorizerJar.toURI().getPath());
  return cConf;
}","private static CConfiguration createCConf() throws Exception {
  LocationFactory locationFactory=new LocalLocationFactory(TEMPORARY_FOLDER.newFolder());
  String secureStoreLocation=locationFactory.create(""String_Node_Str"").toURI().getPath();
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Security.Store.FILE_PATH,secureStoreLocation);
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  cConf.set(Constants.Security.Authorization.SUPERUSERS,""String_Node_Str"");
  Location authorizerJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,authorizerJar.toURI().getPath());
  cConf.set(Constants.Security.Store.PROVIDER,""String_Node_Str"");
  return cConf;
}"
6019,"@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=createCConf();
  final Injector injector=AppFabricTestHelper.getInjector(cConf);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  authorizationEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  authorizationEnforcementService.startAndWait();
  remoteSystemOperationsService=injector.getInstance(RemoteSystemOperationsService.class);
  remoteSystemOperationsService.startAndWait();
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  SecurityRequestContext.setUserId(ALICE.getName());
  secureStoreService=injector.getInstance(SecureStoreService.class);
  authorizer.grant(NamespaceId.DEFAULT,ALICE,Collections.singleton(Action.READ));
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return injector.getInstance(NamespaceAdmin.class).exists(Id.Namespace.DEFAULT);
    }
  }
,5,TimeUnit.SECONDS);
  authorizer.revoke(NamespaceId.DEFAULT,ALICE,Collections.singleton(Action.READ));
}","@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=createCConf();
  SConfiguration sConf=SConfiguration.create();
  sConf.set(Constants.Security.Store.FILE_PASSWORD,""String_Node_Str"");
  final Injector injector=Guice.createInjector(new AppFabricTestModule(createCConf(),sConf));
  injector.getInstance(TransactionManager.class).startAndWait();
  injector.getInstance(DatasetOpExecutor.class).startAndWait();
  injector.getInstance(DatasetService.class).startAndWait();
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  authorizationEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  authorizationEnforcementService.startAndWait();
  remoteSystemOperationsService=injector.getInstance(RemoteSystemOperationsService.class);
  remoteSystemOperationsService.startAndWait();
  secureStoreService=injector.getInstance(SecureStoreService.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  SecurityRequestContext.setUserId(ALICE.getName());
  secureStoreService=injector.getInstance(SecureStoreService.class);
  authorizer.grant(NamespaceId.DEFAULT,ALICE,Collections.singleton(Action.READ));
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return injector.getInstance(NamespaceAdmin.class).exists(Id.Namespace.DEFAULT);
    }
  }
,5,TimeUnit.SECONDS);
  authorizer.revoke(NamespaceId.DEFAULT,ALICE,Collections.singleton(Action.READ));
}"
6020,"@BeforeClass public static void beforeClass() throws Throwable {
  CConfiguration conf=CConfiguration.create();
  conf.set(Constants.AppFabric.SERVER_ADDRESS,hostname);
  conf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder(""String_Node_Str"").getAbsolutePath());
  conf.set(Constants.AppFabric.OUTPUT_DIR,System.getProperty(""String_Node_Str""));
  conf.set(Constants.AppFabric.TEMP_DIR,System.getProperty(""String_Node_Str""));
  conf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  injector=Guice.createInjector(new AppFabricTestModule(conf));
  txManager=injector.getInstance(TransactionManager.class);
  txManager.startAndWait();
  dsOpService=injector.getInstance(DatasetOpExecutor.class);
  dsOpService.startAndWait();
  remoteSysOpService=injector.getInstance(RemoteSystemOperationsService.class);
  remoteSysOpService.startAndWait();
  datasetService=injector.getInstance(DatasetService.class);
  datasetService.startAndWait();
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  DiscoveryServiceClient discoveryClient=injector.getInstance(DiscoveryServiceClient.class);
  ServiceDiscovered appFabricHttpDiscovered=discoveryClient.discover(Constants.Service.APP_FABRIC_HTTP);
  EndpointStrategy endpointStrategy=new RandomEndpointStrategy(appFabricHttpDiscovered);
  port=endpointStrategy.pick(1,TimeUnit.SECONDS).getSocketAddress().getPort();
  txClient=injector.getInstance(TransactionSystemClient.class);
  metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  metricsCollectionService.startAndWait();
  metricsService=injector.getInstance(MetricsQueryService.class);
  metricsService.startAndWait();
  streamService=injector.getInstance(StreamService.class);
  streamService.startAndWait();
  serviceStore=injector.getInstance(ServiceStore.class);
  serviceStore.startAndWait();
  metadataService=injector.getInstance(MetadataService.class);
  metadataService.startAndWait();
  locationFactory=getInjector().getInstance(LocationFactory.class);
  streamClient=new StreamClient(getClientConfig(discoveryClient,Constants.Service.STREAMS));
  streamViewClient=new StreamViewClient(getClientConfig(discoveryClient,Constants.Service.STREAMS));
  datasetClient=new DatasetClient(getClientConfig(discoveryClient,Constants.Service.DATASET_MANAGER));
  createNamespaces();
}","@BeforeClass public static void beforeClass() throws Throwable {
  initializeAndStartServices(createBasicCConf(),null);
}"
6021,"public AppFabricTestModule(CConfiguration configuration){
  this.cConf=configuration;
  File localDataDir=new File(cConf.get(Constants.CFG_LOCAL_DATA_DIR));
  hConf=new Configuration();
  hConf.addResource(""String_Node_Str"");
  hConf.reloadConfiguration();
  hConf.set(""String_Node_Str"",new File(localDataDir,cConf.get(Constants.AppFabric.TEMP_DIR)).getAbsolutePath());
  hConf.set(Constants.CFG_LOCAL_DATA_DIR,localDataDir.getAbsolutePath());
  hConf.set(Constants.AppFabric.OUTPUT_DIR,cConf.get(Constants.AppFabric.OUTPUT_DIR));
}","public AppFabricTestModule(CConfiguration cConf,@Nullable SConfiguration sConf){
  this.cConf=cConf;
  File localDataDir=new File(cConf.get(Constants.CFG_LOCAL_DATA_DIR));
  hConf=new Configuration();
  hConf.addResource(""String_Node_Str"");
  hConf.reloadConfiguration();
  hConf.set(""String_Node_Str"",new File(localDataDir,cConf.get(Constants.AppFabric.TEMP_DIR)).getAbsolutePath());
  hConf.set(Constants.CFG_LOCAL_DATA_DIR,localDataDir.getAbsolutePath());
  hConf.set(Constants.AppFabric.OUTPUT_DIR,cConf.get(Constants.AppFabric.OUTPUT_DIR));
  this.sConf=sConf == null ? SConfiguration.create() : sConf;
}"
6022,"@Inject public DatasetService(CConfiguration cConf,DiscoveryService discoveryService,DiscoveryServiceClient discoveryServiceClient,DatasetTypeManager typeManager,MetricsCollectionService metricsCollectionService,DatasetOpExecutor opExecutorClient,Set<DatasetMetricsReporter> metricReporters,DatasetTypeService datasetTypeService,DatasetInstanceService datasetInstanceService) throws Exception {
  this.typeManager=typeManager;
  DatasetTypeHandler datasetTypeHandler=new DatasetTypeHandler(datasetTypeService);
  DatasetInstanceHandler datasetInstanceHandler=new DatasetInstanceHandler(datasetInstanceService);
  NettyHttpService.Builder builder=new CommonNettyHttpServiceBuilder(cConf);
  builder.addHttpHandlers(ImmutableList.of(datasetTypeHandler,datasetInstanceHandler));
  builder.setHandlerHooks(ImmutableList.of(new MetricsReporterHook(metricsCollectionService,Constants.Service.DATASET_MANAGER)));
  builder.setHost(cConf.get(Constants.Dataset.Manager.ADDRESS));
  builder.setConnectionBacklog(cConf.getInt(Constants.Dataset.Manager.BACKLOG_CONNECTIONS,Constants.Dataset.Manager.DEFAULT_BACKLOG));
  builder.setExecThreadPoolSize(cConf.getInt(Constants.Dataset.Manager.EXEC_THREADS,Constants.Dataset.Manager.DEFAULT_EXEC_THREADS));
  builder.setBossThreadPoolSize(cConf.getInt(Constants.Dataset.Manager.BOSS_THREADS,Constants.Dataset.Manager.DEFAULT_BOSS_THREADS));
  builder.setWorkerThreadPoolSize(cConf.getInt(Constants.Dataset.Manager.WORKER_THREADS,Constants.Dataset.Manager.DEFAULT_WORKER_THREADS));
  this.httpService=builder.build();
  this.discoveryService=discoveryService;
  this.discoveryServiceClient=discoveryServiceClient;
  this.opExecutorClient=opExecutorClient;
  this.metricReporters=metricReporters;
}","@Inject public DatasetService(CConfiguration cConf,DiscoveryService discoveryService,DiscoveryServiceClient discoveryServiceClient,MetricsCollectionService metricsCollectionService,DatasetOpExecutor opExecutorClient,Set<DatasetMetricsReporter> metricReporters,DatasetTypeService datasetTypeService,DatasetInstanceService datasetInstanceService) throws Exception {
  this.typeService=datasetTypeService;
  DatasetTypeHandler datasetTypeHandler=new DatasetTypeHandler(datasetTypeService);
  DatasetInstanceHandler datasetInstanceHandler=new DatasetInstanceHandler(datasetInstanceService);
  NettyHttpService.Builder builder=new CommonNettyHttpServiceBuilder(cConf);
  builder.addHttpHandlers(ImmutableList.of(datasetTypeHandler,datasetInstanceHandler));
  builder.setHandlerHooks(ImmutableList.of(new MetricsReporterHook(metricsCollectionService,Constants.Service.DATASET_MANAGER)));
  builder.setHost(cConf.get(Constants.Dataset.Manager.ADDRESS));
  builder.setConnectionBacklog(cConf.getInt(Constants.Dataset.Manager.BACKLOG_CONNECTIONS,Constants.Dataset.Manager.DEFAULT_BACKLOG));
  builder.setExecThreadPoolSize(cConf.getInt(Constants.Dataset.Manager.EXEC_THREADS,Constants.Dataset.Manager.DEFAULT_EXEC_THREADS));
  builder.setBossThreadPoolSize(cConf.getInt(Constants.Dataset.Manager.BOSS_THREADS,Constants.Dataset.Manager.DEFAULT_BOSS_THREADS));
  builder.setWorkerThreadPoolSize(cConf.getInt(Constants.Dataset.Manager.WORKER_THREADS,Constants.Dataset.Manager.DEFAULT_WORKER_THREADS));
  this.httpService=builder.build();
  this.discoveryService=discoveryService;
  this.discoveryServiceClient=discoveryServiceClient;
  this.opExecutorClient=opExecutorClient;
  this.metricReporters=metricReporters;
}"
6023,"@Override protected void shutDown() throws Exception {
  LOG.info(""String_Node_Str"");
  for (  DatasetMetricsReporter metricsReporter : metricReporters) {
    metricsReporter.stop();
  }
  if (opExecutorServiceWatch != null) {
    opExecutorServiceWatch.cancel();
  }
  typeManager.stopAndWait();
  if (cancelDiscovery != null) {
    cancelDiscovery.cancel();
  }
  try {
    TimeUnit.SECONDS.sleep(3);
  }
 catch (  InterruptedException e) {
    LOG.error(""String_Node_Str"",e);
  }
  httpService.stopAndWait();
  opExecutorClient.stopAndWait();
}","@Override protected void shutDown() throws Exception {
  LOG.info(""String_Node_Str"");
  for (  DatasetMetricsReporter metricsReporter : metricReporters) {
    metricsReporter.stop();
  }
  if (opExecutorServiceWatch != null) {
    opExecutorServiceWatch.cancel();
  }
  typeService.stopAndWait();
  if (cancelDiscovery != null) {
    cancelDiscovery.cancel();
  }
  try {
    TimeUnit.SECONDS.sleep(3);
  }
 catch (  InterruptedException e) {
    LOG.error(""String_Node_Str"",e);
  }
  httpService.stopAndWait();
  opExecutorClient.stopAndWait();
}"
6024,"@Override protected void startUp() throws Exception {
  LOG.info(""String_Node_Str"");
  typeManager.startAndWait();
  opExecutorClient.startAndWait();
  httpService.startAndWait();
  ServiceDiscovered discover=discoveryServiceClient.discover(Constants.Service.DATASET_EXECUTOR);
  opExecutorDiscovered=SettableFuture.create();
  opExecutorServiceWatch=discover.watchChanges(new ServiceDiscovered.ChangeListener(){
    @Override public void onChange(    ServiceDiscovered serviceDiscovered){
      if (!Iterables.isEmpty(serviceDiscovered)) {
        LOG.info(""String_Node_Str"",Constants.Service.DATASET_EXECUTOR);
        opExecutorDiscovered.set(serviceDiscovered);
      }
    }
  }
,MoreExecutors.sameThreadExecutor());
  for (  DatasetMetricsReporter metricsReporter : metricReporters) {
    metricsReporter.start();
  }
}","@Override protected void startUp() throws Exception {
  LOG.info(""String_Node_Str"");
  typeService.startAndWait();
  opExecutorClient.startAndWait();
  httpService.startAndWait();
  ServiceDiscovered discover=discoveryServiceClient.discover(Constants.Service.DATASET_EXECUTOR);
  opExecutorDiscovered=SettableFuture.create();
  opExecutorServiceWatch=discover.watchChanges(new ServiceDiscovered.ChangeListener(){
    @Override public void onChange(    ServiceDiscovered serviceDiscovered){
      if (!Iterables.isEmpty(serviceDiscovered)) {
        LOG.info(""String_Node_Str"",Constants.Service.DATASET_EXECUTOR);
        opExecutorDiscovered.set(serviceDiscovered);
      }
    }
  }
,MoreExecutors.sameThreadExecutor());
  for (  DatasetMetricsReporter metricsReporter : metricReporters) {
    metricsReporter.start();
  }
}"
6025,"@Override public boolean apply(DatasetTypeMeta datasetTypeMeta){
  DatasetTypeId datasetTypeId=namespaceId.datasetType(datasetTypeMeta.getName());
  return authFilter.apply(datasetTypeId);
}","@Override public void apply() throws Exception {
  Collection<DatasetModuleMeta> allDatasets=datasetTypeMDS.getModules(Id.Namespace.SYSTEM);
  for (  DatasetModuleMeta ds : allDatasets) {
    if (ds.getJarLocation() == null) {
      LOG.debug(""String_Node_Str"",ds.toString());
      Id.DatasetModule moduleId=Id.DatasetModule.from(Id.Namespace.SYSTEM,ds.getName());
      datasetTypeMDS.deleteModule(moduleId);
      revokeAllPrivilegesOnModule(moduleId.toEntityId(),ds);
    }
  }
}"
6026,"@Inject @VisibleForTesting public DatasetTypeService(DatasetTypeManager typeManager,NamespaceQueryAdmin namespaceQueryAdmin,NamespacedLocationFactory namespacedLocationFactory,AuthorizationEnforcer authorizationEnforcer,PrivilegesManager privilegesManager,AuthenticationContext authenticationContext,CConfiguration cConf,Impersonator impersonator){
  this.typeManager=typeManager;
  this.namespaceQueryAdmin=namespaceQueryAdmin;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.authorizationEnforcer=authorizationEnforcer;
  this.privilegesManager=privilegesManager;
  this.authenticationContext=authenticationContext;
  this.cConf=cConf;
  this.impersonator=impersonator;
}","@Inject @VisibleForTesting public DatasetTypeService(DatasetTypeManager typeManager,NamespaceQueryAdmin namespaceQueryAdmin,NamespacedLocationFactory namespacedLocationFactory,AuthorizationEnforcer authorizationEnforcer,PrivilegesManager privilegesManager,AuthenticationContext authenticationContext,CConfiguration cConf,Impersonator impersonator,TransactionSystemClientService txClientService,@Named(""String_Node_Str"") DatasetFramework datasetFramework,TransactionExecutorFactory txExecutorFactory,@Named(""String_Node_Str"") Map<String,DatasetModule> defaultModules){
  this.typeManager=typeManager;
  this.namespaceQueryAdmin=namespaceQueryAdmin;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.authorizationEnforcer=authorizationEnforcer;
  this.privilegesManager=privilegesManager;
  this.authenticationContext=authenticationContext;
  this.cConf=cConf;
  this.impersonator=impersonator;
  this.txClientService=txClientService;
  this.datasetFramework=datasetFramework;
  this.txExecutorFactory=txExecutorFactory;
  Map<String,String> emptyArgs=Collections.emptyMap();
  this.datasetCache=new MultiThreadDatasetCache(new SystemDatasetInstantiator(datasetFramework,null,null),txClientService,NamespaceId.SYSTEM,emptyArgs,null,ImmutableMap.of(DatasetMetaTableUtil.META_TABLE_NAME,emptyArgs,DatasetMetaTableUtil.INSTANCE_TABLE_NAME,emptyArgs));
  this.defaultModules=new LinkedHashMap<>(defaultModules);
  this.extensionModules=getExtensionModules(cConf);
}"
6027,"private void grantAllPrivilegesOnModule(DatasetModuleId moduleId,Principal principal) throws Exception {
  Set<Action> allActions=ImmutableSet.of(Action.ALL);
  privilegesManager.grant(moduleId,principal,allActions);
  DatasetModuleMeta moduleMeta=typeManager.getModule(moduleId.toId());
  if (moduleMeta == null) {
    LOG.debug(""String_Node_Str"",moduleId);
    return;
  }
  for (  String type : moduleMeta.getTypes()) {
    DatasetTypeId datasetTypeId=moduleId.getParent().datasetType(type);
    privilegesManager.grant(datasetTypeId,principal,allActions);
  }
}","private void grantAllPrivilegesOnModule(DatasetModuleId moduleId,Principal principal,@Nullable DatasetModuleMeta moduleMeta) throws Exception {
  Set<Action> allActions=ImmutableSet.of(Action.ALL);
  privilegesManager.grant(moduleId,principal,allActions);
  if (moduleMeta == null) {
    moduleMeta=typeManager.getModule(moduleId.toId());
  }
  if (moduleMeta == null) {
    LOG.debug(""String_Node_Str"",moduleId);
    return;
  }
  for (  String type : moduleMeta.getTypes()) {
    DatasetTypeId datasetTypeId=moduleId.getParent().datasetType(type);
    privilegesManager.grant(datasetTypeId,principal,allActions);
  }
}"
6028,"/** 
 * Drops the specified dataset instance.
 * @param instance the {@link Id.DatasetInstance} to drop
 * @throws NamespaceNotFoundException if the namespace was not found
 * @throws DatasetNotFoundException if the dataset instance was not found
 * @throws IOException if there was a problem in checking if the namespace exists over HTTP
 * @throws UnauthorizedException if perimeter security and authorization are enabled, and the current user does nothave  {@link Action#ADMIN} privileges on the #instance
 */
void drop(Id.DatasetInstance instance) throws Exception {
  DatasetId datasetId=instance.toEntityId();
  ensureNamespaceExists(instance.getNamespace());
  DatasetSpecification spec=instanceManager.get(instance);
  if (spec == null) {
    throw new DatasetNotFoundException(instance);
  }
  authorizationEnforcer.enforce(datasetId,authenticationContext.getPrincipal(),Action.ADMIN);
  LOG.info(""String_Node_Str"",instance.getNamespaceId(),instance.getId());
  dropDataset(instance,spec);
  publishAudit(instance,AuditType.DELETE);
  authorizer.revoke(datasetId);
}","/** 
 * Drops the specified dataset instance.
 * @param instance the {@link Id.DatasetInstance} to drop
 * @throws NamespaceNotFoundException if the namespace was not found
 * @throws DatasetNotFoundException if the dataset instance was not found
 * @throws IOException if there was a problem in checking if the namespace exists over HTTP
 * @throws UnauthorizedException if perimeter security and authorization are enabled, and the current user does nothave  {@link Action#ADMIN} privileges on the #instance
 */
void drop(Id.DatasetInstance instance) throws Exception {
  DatasetId datasetId=instance.toEntityId();
  ensureNamespaceExists(instance.getNamespace());
  DatasetSpecification spec=instanceManager.get(instance);
  if (spec == null) {
    throw new DatasetNotFoundException(instance);
  }
  authorizationEnforcer.enforce(datasetId,authenticationContext.getPrincipal(),Action.ADMIN);
  LOG.info(""String_Node_Str"",instance.getNamespaceId(),instance.getId());
  dropDataset(instance,spec);
  publishAudit(instance,AuditType.DELETE);
  privilegesManager.revoke(datasetId);
}"
6029,"@Inject public DatasetInstanceService(DatasetTypeManager typeManager,DatasetInstanceManager instanceManager,DatasetOpExecutor opExecutorClient,ExploreFacade exploreFacade,NamespaceQueryAdmin namespaceQueryAdmin,AuthorizationEnforcer authorizationEnforcer,AuthorizerInstantiator authorizerInstantiator,AuthenticationContext authenticationContext){
  this.opExecutorClient=opExecutorClient;
  this.typeManager=typeManager;
  this.instanceManager=instanceManager;
  this.exploreFacade=exploreFacade;
  this.namespaceQueryAdmin=namespaceQueryAdmin;
  this.metaCache=CacheBuilder.newBuilder().build(new CacheLoader<Id.DatasetInstance,DatasetMeta>(){
    @Override public DatasetMeta load(    Id.DatasetInstance datasetId) throws Exception {
      return getFromMds(datasetId);
    }
  }
);
  this.authorizationEnforcer=authorizationEnforcer;
  this.authorizer=authorizerInstantiator.get();
  this.authenticationContext=authenticationContext;
}","@Inject public DatasetInstanceService(DatasetTypeManager typeManager,DatasetInstanceManager instanceManager,DatasetOpExecutor opExecutorClient,ExploreFacade exploreFacade,NamespaceQueryAdmin namespaceQueryAdmin,AuthorizationEnforcer authorizationEnforcer,PrivilegesManager privilegesManager,AuthenticationContext authenticationContext){
  this.opExecutorClient=opExecutorClient;
  this.typeManager=typeManager;
  this.instanceManager=instanceManager;
  this.exploreFacade=exploreFacade;
  this.namespaceQueryAdmin=namespaceQueryAdmin;
  this.metaCache=CacheBuilder.newBuilder().build(new CacheLoader<Id.DatasetInstance,DatasetMeta>(){
    @Override public DatasetMeta load(    Id.DatasetInstance datasetId) throws Exception {
      return getFromMds(datasetId);
    }
  }
);
  this.authorizationEnforcer=authorizationEnforcer;
  this.privilegesManager=privilegesManager;
  this.authenticationContext=authenticationContext;
}"
6030,"/** 
 * Creates a dataset instance.
 * @param namespaceId the namespace to create the dataset instance in
 * @param name the name of the new dataset instance
 * @param props the properties for the new dataset instance
 * @throws NamespaceNotFoundException if the specified namespace was not found
 * @throws DatasetAlreadyExistsException if a dataset with the same name already exists
 * @throws DatasetTypeNotFoundException if the dataset type was not found
 * @throws UnauthorizedException if perimeter security and authorization are enabled, and the current user does nothave  {@link Action#WRITE} privilege on the #instance's namespace
 */
void create(String namespaceId,String name,DatasetInstanceConfiguration props) throws Exception {
  Id.Namespace namespace=ConversionHelpers.toNamespaceId(namespaceId);
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(namespace.toEntityId(),principal,Action.WRITE);
  ensureNamespaceExists(namespace);
  Id.DatasetInstance newInstance=ConversionHelpers.toDatasetInstanceId(namespaceId,name);
  DatasetSpecification existing=instanceManager.get(newInstance);
  if (existing != null) {
    throw new DatasetAlreadyExistsException(newInstance);
  }
  DatasetTypeMeta typeMeta=getTypeInfo(namespace,props.getTypeName());
  if (typeMeta == null) {
    throw new DatasetTypeNotFoundException(ConversionHelpers.toDatasetTypeId(namespace,props.getTypeName()));
  }
  DatasetId datasetId=newInstance.toEntityId();
  authorizer.revoke(datasetId);
  authorizer.grant(datasetId,principal,ImmutableSet.of(Action.ALL));
  LOG.info(""String_Node_Str"",namespaceId,name,props.getTypeName(),props.getProperties());
  try {
    DatasetSpecification spec=opExecutorClient.create(newInstance,typeMeta,DatasetProperties.builder().addAll(props.getProperties()).setDescription(props.getDescription()).build());
    instanceManager.add(namespace,spec);
    metaCache.invalidate(newInstance);
    publishAudit(newInstance,AuditType.CREATE);
    enableExplore(newInstance,spec,props);
  }
 catch (  Exception e) {
    authorizer.revoke(datasetId);
    throw e;
  }
}","/** 
 * Creates a dataset instance.
 * @param namespaceId the namespace to create the dataset instance in
 * @param name the name of the new dataset instance
 * @param props the properties for the new dataset instance
 * @throws NamespaceNotFoundException if the specified namespace was not found
 * @throws DatasetAlreadyExistsException if a dataset with the same name already exists
 * @throws DatasetTypeNotFoundException if the dataset type was not found
 * @throws UnauthorizedException if perimeter security and authorization are enabled, and the current user does nothave  {@link Action#WRITE} privilege on the #instance's namespace
 */
void create(String namespaceId,String name,DatasetInstanceConfiguration props) throws Exception {
  Id.Namespace namespace=ConversionHelpers.toNamespaceId(namespaceId);
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(namespace.toEntityId(),principal,Action.WRITE);
  ensureNamespaceExists(namespace);
  Id.DatasetInstance newInstance=ConversionHelpers.toDatasetInstanceId(namespaceId,name);
  DatasetSpecification existing=instanceManager.get(newInstance);
  if (existing != null) {
    throw new DatasetAlreadyExistsException(newInstance);
  }
  DatasetTypeMeta typeMeta=getTypeInfo(namespace,props.getTypeName());
  if (typeMeta == null) {
    throw new DatasetTypeNotFoundException(ConversionHelpers.toDatasetTypeId(namespace,props.getTypeName()));
  }
  DatasetId datasetId=newInstance.toEntityId();
  privilegesManager.revoke(datasetId);
  privilegesManager.grant(datasetId,principal,ImmutableSet.of(Action.ALL));
  LOG.info(""String_Node_Str"",namespaceId,name,props.getTypeName(),props.getProperties());
  try {
    DatasetSpecification spec=opExecutorClient.create(newInstance,typeMeta,DatasetProperties.builder().addAll(props.getProperties()).setDescription(props.getDescription()).build());
    instanceManager.add(namespace,spec);
    metaCache.invalidate(newInstance);
    publishAudit(newInstance,AuditType.CREATE);
    enableExplore(newInstance,spec,props);
  }
 catch (  Exception e) {
    privilegesManager.revoke(datasetId);
    throw e;
  }
}"
6031,"/** 
 * Finds the   {@link DatasetTypeMeta} for the specified dataset type name.Search order - first in the specified namespace, then in the 'system' namespace from defaultModules
 * @param namespaceId {@link Id.Namespace} for the specified namespace
 * @param typeName the name of the dataset type to search
 * @return {@link DatasetTypeMeta} for the type if found in either the specified namespace or in the system namespace,null otherwise. TODO: This may need to move to a util class eventually
 */
@Nullable private DatasetTypeMeta getTypeInfo(Id.Namespace namespaceId,String typeName) throws Exception {
  Id.DatasetType datasetTypeId=ConversionHelpers.toDatasetTypeId(namespaceId,typeName);
  DatasetTypeMeta typeMeta=typeManager.getTypeInfo(datasetTypeId);
  if (typeMeta == null) {
    Id.DatasetType systemDatasetTypeId=ConversionHelpers.toDatasetTypeId(Id.Namespace.SYSTEM,typeName);
    typeMeta=typeManager.getTypeInfo(systemDatasetTypeId);
  }
 else {
    DatasetTypeId typeId=datasetTypeId.toEntityId();
    Principal principal=authenticationContext.getPrincipal();
    Predicate<EntityId> filter=authorizer.createFilter(principal);
    if (!Principal.SYSTEM.equals(principal) && !filter.apply(typeId)) {
      throw new UnauthorizedException(principal,typeId);
    }
  }
  return typeMeta;
}","/** 
 * Finds the   {@link DatasetTypeMeta} for the specified dataset type name.Search order - first in the specified namespace, then in the 'system' namespace from defaultModules
 * @param namespaceId {@link Id.Namespace} for the specified namespace
 * @param typeName the name of the dataset type to search
 * @return {@link DatasetTypeMeta} for the type if found in either the specified namespace or in the system namespace,null otherwise. TODO: This may need to move to a util class eventually
 */
@Nullable private DatasetTypeMeta getTypeInfo(Id.Namespace namespaceId,String typeName) throws Exception {
  Id.DatasetType datasetTypeId=ConversionHelpers.toDatasetTypeId(namespaceId,typeName);
  DatasetTypeMeta typeMeta=typeManager.getTypeInfo(datasetTypeId);
  if (typeMeta == null) {
    Id.DatasetType systemDatasetTypeId=ConversionHelpers.toDatasetTypeId(Id.Namespace.SYSTEM,typeName);
    typeMeta=typeManager.getTypeInfo(systemDatasetTypeId);
  }
 else {
    DatasetTypeId typeId=datasetTypeId.toEntityId();
    Principal principal=authenticationContext.getPrincipal();
    Predicate<EntityId> filter=authorizationEnforcer.createFilter(principal);
    if (!Principal.SYSTEM.equals(principal) && !filter.apply(typeId)) {
      throw new UnauthorizedException(principal,typeId);
    }
  }
  return typeMeta;
}"
6032,"/** 
 * Deletes all   {@link DatasetModuleMeta dataset modules} in the specified {@link NamespaceId namespace}.
 */
void deleteAll(NamespaceId namespaceId) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(namespaceId,principal,Action.ADMIN);
  if (NamespaceId.SYSTEM.equals(namespaceId)) {
    throw new UnauthorizedException(String.format(""String_Node_Str"",namespaceId));
  }
  ensureNamespaceExists(namespaceId);
  Id.Namespace namespace=namespaceId.toId();
  for (  DatasetModuleMeta meta : typeManager.getModules(namespace)) {
    authorizer.revoke(namespaceId.datasetModule(meta.getName()));
  }
  try {
    typeManager.deleteModules(namespace);
  }
 catch (  DatasetModuleConflictException e) {
    throw new ConflictException(e.getMessage(),e);
  }
}","/** 
 * Deletes all   {@link DatasetModuleMeta dataset modules} in the specified {@link NamespaceId namespace}.
 */
void deleteAll(NamespaceId namespaceId) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  authorizationEnforcer.enforce(namespaceId,principal,Action.ADMIN);
  if (NamespaceId.SYSTEM.equals(namespaceId)) {
    throw new UnauthorizedException(String.format(""String_Node_Str"",namespaceId));
  }
  ensureNamespaceExists(namespaceId);
  Id.Namespace namespace=namespaceId.toId();
  for (  DatasetModuleMeta meta : typeManager.getModules(namespace)) {
    privilegesManager.revoke(namespaceId.datasetModule(meta.getName()));
  }
  try {
    typeManager.deleteModules(namespace);
  }
 catch (  DatasetModuleConflictException e) {
    throw new ConflictException(e.getMessage(),e);
  }
}"
6033,"private void revokeAllPrivilegesOnModule(DatasetModuleId moduleId,@Nullable DatasetModuleMeta moduleMeta) throws Exception {
  authorizer.revoke(moduleId);
  moduleMeta=moduleMeta == null ? typeManager.getModule(moduleId.toId()) : moduleMeta;
  if (moduleMeta == null) {
    LOG.debug(""String_Node_Str"",moduleId);
    return;
  }
  for (  String type : moduleMeta.getTypes()) {
    DatasetTypeId datasetTypeId=moduleId.getParent().datasetType(type);
    authorizer.revoke(datasetTypeId);
  }
}","private void revokeAllPrivilegesOnModule(DatasetModuleId moduleId,@Nullable DatasetModuleMeta moduleMeta) throws Exception {
  privilegesManager.revoke(moduleId);
  moduleMeta=moduleMeta == null ? typeManager.getModule(moduleId.toId()) : moduleMeta;
  if (moduleMeta == null) {
    LOG.debug(""String_Node_Str"",moduleId);
    return;
  }
  for (  String type : moduleMeta.getTypes()) {
    DatasetTypeId datasetTypeId=moduleId.getParent().datasetType(type);
    privilegesManager.revoke(datasetTypeId);
  }
}"
6034,"@Inject @VisibleForTesting public DatasetTypeService(DatasetTypeManager typeManager,NamespaceQueryAdmin namespaceQueryAdmin,NamespacedLocationFactory namespacedLocationFactory,AuthorizationEnforcer authorizationEnforcer,AuthorizerInstantiator authorizerInstantiator,AuthenticationContext authenticationContext,CConfiguration cConf,Impersonator impersonator){
  this.typeManager=typeManager;
  this.namespaceQueryAdmin=namespaceQueryAdmin;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.authorizationEnforcer=authorizationEnforcer;
  this.authorizer=authorizerInstantiator.get();
  this.authenticationContext=authenticationContext;
  this.cConf=cConf;
  this.impersonator=impersonator;
}","@Inject @VisibleForTesting public DatasetTypeService(DatasetTypeManager typeManager,NamespaceQueryAdmin namespaceQueryAdmin,NamespacedLocationFactory namespacedLocationFactory,AuthorizationEnforcer authorizationEnforcer,PrivilegesManager privilegesManager,AuthenticationContext authenticationContext,CConfiguration cConf,Impersonator impersonator){
  this.typeManager=typeManager;
  this.namespaceQueryAdmin=namespaceQueryAdmin;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.authorizationEnforcer=authorizationEnforcer;
  this.privilegesManager=privilegesManager;
  this.authenticationContext=authenticationContext;
  this.cConf=cConf;
  this.impersonator=impersonator;
}"
6035,"private void grantAllPrivilegesOnModule(DatasetModuleId moduleId,Principal principal) throws Exception {
  Set<Action> allActions=ImmutableSet.of(Action.ALL);
  authorizer.grant(moduleId,principal,allActions);
  DatasetModuleMeta moduleMeta=typeManager.getModule(moduleId.toId());
  if (moduleMeta == null) {
    LOG.debug(""String_Node_Str"",moduleId);
    return;
  }
  for (  String type : moduleMeta.getTypes()) {
    DatasetTypeId datasetTypeId=moduleId.getParent().datasetType(type);
    authorizer.grant(datasetTypeId,principal,allActions);
  }
}","private void grantAllPrivilegesOnModule(DatasetModuleId moduleId,Principal principal) throws Exception {
  Set<Action> allActions=ImmutableSet.of(Action.ALL);
  privilegesManager.grant(moduleId,principal,allActions);
  DatasetModuleMeta moduleMeta=typeManager.getModule(moduleId.toId());
  if (moduleMeta == null) {
    LOG.debug(""String_Node_Str"",moduleId);
    return;
  }
  for (  String type : moduleMeta.getTypes()) {
    DatasetTypeId datasetTypeId=moduleId.getParent().datasetType(type);
    privilegesManager.grant(datasetTypeId,principal,allActions);
  }
}"
6036,"@Before public void before() throws Exception {
  cConf.set(Constants.Dataset.Manager.ADDRESS,""String_Node_Str"");
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  Configuration txConf=HBaseConfiguration.create();
  CConfigurationUtil.copyTxProperties(cConf,txConf);
  Impersonator impersonator=new Impersonator(cConf,null,null);
  Injector injector=Guice.createInjector(new ConfigModule(cConf,txConf),new DiscoveryRuntimeModule().getInMemoryModules(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getMasterModule(),new TransactionInMemoryModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Singleton.class);
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(RemoteDatasetFramework.class);
    }
  }
);
  txManager=new TransactionManager(txConf);
  txManager.startAndWait();
  InMemoryTxSystemClient txSystemClient=new InMemoryTxSystemClient(txManager);
  TransactionSystemClientService txSystemClientService=new DelegatingTransactionSystemClientService(txSystemClient);
  DiscoveryService discoveryService=injector.getInstance(DiscoveryService.class);
  DiscoveryServiceClient discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  MetricsCollectionService metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  AuthenticationContext authenticationContext=injector.getInstance(AuthenticationContext.class);
  framework=new RemoteDatasetFramework(cConf,discoveryServiceClient,registryFactory,authenticationContext);
  SystemDatasetInstantiatorFactory datasetInstantiatorFactory=new SystemDatasetInstantiatorFactory(locationFactory,framework,cConf);
  DatasetAdminService datasetAdminService=new DatasetAdminService(framework,cConf,locationFactory,datasetInstantiatorFactory,new NoOpMetadataStore(),impersonator);
  ImmutableSet<HttpHandler> handlers=ImmutableSet.<HttpHandler>of(new DatasetAdminOpHTTPHandler(datasetAdminService));
  opExecutorService=new DatasetOpExecutorService(cConf,discoveryService,metricsCollectionService,handlers);
  opExecutorService.startAndWait();
  ImmutableMap<String,DatasetModule> modules=ImmutableMap.<String,DatasetModule>builder().put(""String_Node_Str"",new InMemoryTableModule()).put(""String_Node_Str"",new CoreDatasetsModule()).putAll(DatasetMetaTableUtil.getModules()).build();
  InMemoryDatasetFramework mdsFramework=new InMemoryDatasetFramework(registryFactory,modules);
  ExploreFacade exploreFacade=new ExploreFacade(new DiscoveryExploreClient(cConf,discoveryServiceClient),cConf);
  TransactionExecutorFactory txExecutorFactory=new DynamicTransactionExecutorFactory(txSystemClient);
  AuthorizationEnforcer authorizationEnforcer=injector.getInstance(AuthorizationEnforcer.class);
  DatasetTypeManager typeManager=new DatasetTypeManager(cConf,locationFactory,txSystemClientService,txExecutorFactory,mdsFramework,DEFAULT_MODULES,impersonator);
  DatasetInstanceManager instanceManager=new DatasetInstanceManager(txSystemClientService,txExecutorFactory,mdsFramework);
  AuthorizerInstantiator authorizerInstantiator=injector.getInstance(AuthorizerInstantiator.class);
  DatasetTypeService typeService=new DatasetTypeService(typeManager,namespaceQueryAdmin,namespacedLocationFactory,authorizationEnforcer,authorizerInstantiator,authenticationContext,cConf,impersonator);
  DatasetOpExecutor opExecutor=new LocalDatasetOpExecutor(cConf,discoveryServiceClient,opExecutorService);
  DatasetInstanceService instanceService=new DatasetInstanceService(typeManager,instanceManager,opExecutor,exploreFacade,namespaceQueryAdmin,authorizationEnforcer,authorizerInstantiator,authenticationContext);
  instanceService.setAuditPublisher(inMemoryAuditPublisher);
  service=new DatasetService(cConf,discoveryService,discoveryServiceClient,typeManager,metricsCollectionService,new InMemoryDatasetOpExecutor(framework),new HashSet<DatasetMetricsReporter>(),typeService,instanceService);
  service.startAndWait();
  EndpointStrategy endpointStrategy=new RandomEndpointStrategy(discoveryServiceClient.discover(Constants.Service.DATASET_MANAGER));
  Preconditions.checkNotNull(endpointStrategy.pick(5,TimeUnit.SECONDS),""String_Node_Str"",service);
  createNamespace(Id.Namespace.SYSTEM);
  createNamespace(NAMESPACE_ID);
}","@Before public void before() throws Exception {
  cConf.set(Constants.Dataset.Manager.ADDRESS,""String_Node_Str"");
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  Configuration txConf=HBaseConfiguration.create();
  CConfigurationUtil.copyTxProperties(cConf,txConf);
  Impersonator impersonator=new Impersonator(cConf,null,null);
  Injector injector=Guice.createInjector(new ConfigModule(cConf,txConf),new DiscoveryRuntimeModule().getInMemoryModules(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getMasterModule(),new TransactionInMemoryModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Singleton.class);
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(RemoteDatasetFramework.class);
    }
  }
);
  txManager=new TransactionManager(txConf);
  txManager.startAndWait();
  InMemoryTxSystemClient txSystemClient=new InMemoryTxSystemClient(txManager);
  TransactionSystemClientService txSystemClientService=new DelegatingTransactionSystemClientService(txSystemClient);
  DiscoveryService discoveryService=injector.getInstance(DiscoveryService.class);
  DiscoveryServiceClient discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  MetricsCollectionService metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  AuthenticationContext authenticationContext=injector.getInstance(AuthenticationContext.class);
  framework=new RemoteDatasetFramework(cConf,discoveryServiceClient,registryFactory,authenticationContext);
  SystemDatasetInstantiatorFactory datasetInstantiatorFactory=new SystemDatasetInstantiatorFactory(locationFactory,framework,cConf);
  DatasetAdminService datasetAdminService=new DatasetAdminService(framework,cConf,locationFactory,datasetInstantiatorFactory,new NoOpMetadataStore(),impersonator);
  ImmutableSet<HttpHandler> handlers=ImmutableSet.<HttpHandler>of(new DatasetAdminOpHTTPHandler(datasetAdminService));
  opExecutorService=new DatasetOpExecutorService(cConf,discoveryService,metricsCollectionService,handlers);
  opExecutorService.startAndWait();
  ImmutableMap<String,DatasetModule> modules=ImmutableMap.<String,DatasetModule>builder().put(""String_Node_Str"",new InMemoryTableModule()).put(""String_Node_Str"",new CoreDatasetsModule()).putAll(DatasetMetaTableUtil.getModules()).build();
  InMemoryDatasetFramework mdsFramework=new InMemoryDatasetFramework(registryFactory,modules);
  ExploreFacade exploreFacade=new ExploreFacade(new DiscoveryExploreClient(cConf,discoveryServiceClient),cConf);
  TransactionExecutorFactory txExecutorFactory=new DynamicTransactionExecutorFactory(txSystemClient);
  AuthorizationEnforcer authorizationEnforcer=injector.getInstance(AuthorizationEnforcer.class);
  DatasetTypeManager typeManager=new DatasetTypeManager(cConf,locationFactory,txSystemClientService,txExecutorFactory,mdsFramework,DEFAULT_MODULES,impersonator);
  DatasetInstanceManager instanceManager=new DatasetInstanceManager(txSystemClientService,txExecutorFactory,mdsFramework);
  PrivilegesManager privilegesManager=injector.getInstance(PrivilegesManager.class);
  DatasetTypeService typeService=new DatasetTypeService(typeManager,namespaceQueryAdmin,namespacedLocationFactory,authorizationEnforcer,privilegesManager,authenticationContext,cConf,impersonator);
  DatasetOpExecutor opExecutor=new LocalDatasetOpExecutor(cConf,discoveryServiceClient,opExecutorService);
  DatasetInstanceService instanceService=new DatasetInstanceService(typeManager,instanceManager,opExecutor,exploreFacade,namespaceQueryAdmin,authorizationEnforcer,privilegesManager,authenticationContext);
  instanceService.setAuditPublisher(inMemoryAuditPublisher);
  service=new DatasetService(cConf,discoveryService,discoveryServiceClient,typeManager,metricsCollectionService,new InMemoryDatasetOpExecutor(framework),new HashSet<DatasetMetricsReporter>(),typeService,instanceService);
  service.startAndWait();
  EndpointStrategy endpointStrategy=new RandomEndpointStrategy(discoveryServiceClient.discover(Constants.Service.DATASET_MANAGER));
  Preconditions.checkNotNull(endpointStrategy.pick(5,TimeUnit.SECONDS),""String_Node_Str"",service);
  createNamespace(Id.Namespace.SYSTEM);
  createNamespace(NAMESPACE_ID);
}"
6037,"protected static void initializeAndStartService(CConfiguration cConf) throws Exception {
  injector=Guice.createInjector(new ConfigModule(cConf),new DiscoveryRuntimeModule().getInMemoryModules(),new NonCustomLocationUnitTestModule().getModule(),new NamespaceClientRuntimeModule().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),new TransactionInMemoryModule(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getMasterModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Singleton.class);
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(RemoteDatasetFramework.class);
    }
  }
);
  AuthorizationEnforcer authEnforcer=injector.getInstance(AuthorizationEnforcer.class);
  authEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  authEnforcementService.startAndWait();
  AuthorizerInstantiator authorizerInstantiator=injector.getInstance(AuthorizerInstantiator.class);
  AuthenticationContext authenticationContext=injector.getInstance(AuthenticationContext.class);
  DiscoveryService discoveryService=injector.getInstance(DiscoveryService.class);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  dsFramework=injector.getInstance(RemoteDatasetFramework.class);
  txManager=injector.getInstance(TransactionManager.class);
  txManager.startAndWait();
  TransactionSystemClient txSystemClient=injector.getInstance(TransactionSystemClient.class);
  TransactionSystemClientService txSystemClientService=new DelegatingTransactionSystemClientService(txSystemClient);
  NamespacedLocationFactory namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  SystemDatasetInstantiatorFactory datasetInstantiatorFactory=new SystemDatasetInstantiatorFactory(locationFactory,dsFramework,cConf);
  Impersonator impersonator=new Impersonator(cConf,null,null);
  DatasetAdminService datasetAdminService=new DatasetAdminService(dsFramework,cConf,locationFactory,datasetInstantiatorFactory,new NoOpMetadataStore(),impersonator);
  ImmutableSet<HttpHandler> handlers=ImmutableSet.<HttpHandler>of(new DatasetAdminOpHTTPHandler(datasetAdminService));
  MetricsCollectionService metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  opExecutorService=new DatasetOpExecutorService(cConf,discoveryService,metricsCollectionService,handlers);
  opExecutorService.startAndWait();
  Map<String,DatasetModule> defaultModules=injector.getInstance(Key.get(new TypeLiteral<Map<String,DatasetModule>>(){
  }
,Names.named(""String_Node_Str"")));
  ImmutableMap<String,DatasetModule> modules=ImmutableMap.<String,DatasetModule>builder().putAll(defaultModules).putAll(DatasetMetaTableUtil.getModules()).build();
  registryFactory=injector.getInstance(DatasetDefinitionRegistryFactory.class);
  inMemoryDatasetFramework=new InMemoryDatasetFramework(registryFactory,modules);
  ExploreFacade exploreFacade=new ExploreFacade(new DiscoveryExploreClient(cConf,discoveryServiceClient),cConf);
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  namespaceAdmin.create(NamespaceMeta.DEFAULT);
  NamespaceQueryAdmin namespaceQueryAdmin=injector.getInstance(NamespaceQueryAdmin.class);
  TransactionExecutorFactory txExecutorFactory=new DynamicTransactionExecutorFactory(txSystemClient);
  DatasetTypeManager typeManager=new DatasetTypeManager(cConf,locationFactory,txSystemClientService,txExecutorFactory,inMemoryDatasetFramework,defaultModules,impersonator);
  DatasetOpExecutor opExecutor=new InMemoryDatasetOpExecutor(dsFramework);
  DatasetInstanceManager instanceManager=new DatasetInstanceManager(txSystemClientService,txExecutorFactory,inMemoryDatasetFramework);
  instanceService=new DatasetInstanceService(typeManager,instanceManager,opExecutor,exploreFacade,namespaceQueryAdmin,authEnforcer,authorizerInstantiator,authenticationContext);
  DatasetTypeService typeService=new DatasetTypeService(typeManager,namespaceAdmin,namespacedLocationFactory,authEnforcer,authorizerInstantiator,authenticationContext,cConf,impersonator);
  service=new DatasetService(cConf,discoveryService,discoveryServiceClient,typeManager,metricsCollectionService,opExecutor,new HashSet<DatasetMetricsReporter>(),typeService,instanceService);
  service.startAndWait();
  waitForService(Constants.Service.DATASET_EXECUTOR);
  waitForService(Constants.Service.DATASET_MANAGER);
  Locations.mkdirsIfNotExists(namespacedLocationFactory.get(Id.Namespace.DEFAULT));
}","protected static void initializeAndStartService(CConfiguration cConf) throws Exception {
  injector=Guice.createInjector(new ConfigModule(cConf),new DiscoveryRuntimeModule().getInMemoryModules(),new NonCustomLocationUnitTestModule().getModule(),new NamespaceClientRuntimeModule().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),new TransactionInMemoryModule(),new AuthorizationTestModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AuthenticationContextModules().getMasterModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class).in(Singleton.class);
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(RemoteDatasetFramework.class);
    }
  }
);
  AuthorizationEnforcer authEnforcer=injector.getInstance(AuthorizationEnforcer.class);
  authEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  authEnforcementService.startAndWait();
  AuthenticationContext authenticationContext=injector.getInstance(AuthenticationContext.class);
  DiscoveryService discoveryService=injector.getInstance(DiscoveryService.class);
  discoveryServiceClient=injector.getInstance(DiscoveryServiceClient.class);
  dsFramework=injector.getInstance(RemoteDatasetFramework.class);
  txManager=injector.getInstance(TransactionManager.class);
  txManager.startAndWait();
  TransactionSystemClient txSystemClient=injector.getInstance(TransactionSystemClient.class);
  TransactionSystemClientService txSystemClientService=new DelegatingTransactionSystemClientService(txSystemClient);
  NamespacedLocationFactory namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  SystemDatasetInstantiatorFactory datasetInstantiatorFactory=new SystemDatasetInstantiatorFactory(locationFactory,dsFramework,cConf);
  Impersonator impersonator=new Impersonator(cConf,null,null);
  DatasetAdminService datasetAdminService=new DatasetAdminService(dsFramework,cConf,locationFactory,datasetInstantiatorFactory,new NoOpMetadataStore(),impersonator);
  ImmutableSet<HttpHandler> handlers=ImmutableSet.<HttpHandler>of(new DatasetAdminOpHTTPHandler(datasetAdminService));
  MetricsCollectionService metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  opExecutorService=new DatasetOpExecutorService(cConf,discoveryService,metricsCollectionService,handlers);
  opExecutorService.startAndWait();
  Map<String,DatasetModule> defaultModules=injector.getInstance(Key.get(new TypeLiteral<Map<String,DatasetModule>>(){
  }
,Names.named(""String_Node_Str"")));
  ImmutableMap<String,DatasetModule> modules=ImmutableMap.<String,DatasetModule>builder().putAll(defaultModules).putAll(DatasetMetaTableUtil.getModules()).build();
  registryFactory=injector.getInstance(DatasetDefinitionRegistryFactory.class);
  inMemoryDatasetFramework=new InMemoryDatasetFramework(registryFactory,modules);
  ExploreFacade exploreFacade=new ExploreFacade(new DiscoveryExploreClient(cConf,discoveryServiceClient),cConf);
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  namespaceAdmin.create(NamespaceMeta.DEFAULT);
  NamespaceQueryAdmin namespaceQueryAdmin=injector.getInstance(NamespaceQueryAdmin.class);
  TransactionExecutorFactory txExecutorFactory=new DynamicTransactionExecutorFactory(txSystemClient);
  DatasetTypeManager typeManager=new DatasetTypeManager(cConf,locationFactory,txSystemClientService,txExecutorFactory,inMemoryDatasetFramework,defaultModules,impersonator);
  DatasetOpExecutor opExecutor=new InMemoryDatasetOpExecutor(dsFramework);
  DatasetInstanceManager instanceManager=new DatasetInstanceManager(txSystemClientService,txExecutorFactory,inMemoryDatasetFramework);
  PrivilegesManager privilegesManager=injector.getInstance(PrivilegesManager.class);
  instanceService=new DatasetInstanceService(typeManager,instanceManager,opExecutor,exploreFacade,namespaceQueryAdmin,authEnforcer,privilegesManager,authenticationContext);
  DatasetTypeService typeService=new DatasetTypeService(typeManager,namespaceAdmin,namespacedLocationFactory,authEnforcer,privilegesManager,authenticationContext,cConf,impersonator);
  service=new DatasetService(cConf,discoveryService,discoveryServiceClient,typeManager,metricsCollectionService,opExecutor,new HashSet<DatasetMetricsReporter>(),typeService,instanceService);
  service.startAndWait();
  waitForService(Constants.Service.DATASET_EXECUTOR);
  waitForService(Constants.Service.DATASET_MANAGER);
  Locations.mkdirsIfNotExists(namespacedLocationFactory.get(Id.Namespace.DEFAULT));
}"
6038,"@VisibleForTesting static Injector createInjector(CConfiguration cConf,Configuration hConf){
  return Guice.createInjector(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MetricsClientRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new LocationRuntimeModule().getDistributedModules(),new NamespaceClientRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new DataSetServiceModules().getDistributedModules(),new LoggingModules().getDistributedModules(),new ExploreClientModule(),new NamespaceClientRuntimeModule().getDistributedModules(),new NamespaceStoreModule().getDistributedModules(),new MetadataServiceModule(),new RemoteSystemOperationsServiceModule(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new AuditModule().getDistributedModules(),new EntityVerifierModule(),new SecureStoreModules().getDistributedModules(),new AuthorizationModule(),new AuthorizationEnforcementModule().getDistributedModules(),new AuthenticationContextModules().getProgramContainerModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(Store.class).to(DefaultStore.class);
      bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
    }
  }
);
}","@VisibleForTesting static Injector createInjector(CConfiguration cConf,Configuration hConf){
  return Guice.createInjector(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MetricsClientRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new LocationRuntimeModule().getDistributedModules(),new NamespaceClientRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new DataSetServiceModules().getDistributedModules(),new LoggingModules().getDistributedModules(),new ExploreClientModule(),new NamespaceClientRuntimeModule().getDistributedModules(),new NamespaceStoreModule().getDistributedModules(),new MetadataServiceModule(),new RemoteSystemOperationsServiceModule(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new AuditModule().getDistributedModules(),new EntityVerifierModule(),new SecureStoreModules().getDistributedModules(),new AuthorizationEnforcementModule().getDistributedModules(),new AuthenticationContextModules().getProgramContainerModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(Store.class).to(DefaultStore.class);
      bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
      bind(PrivilegesManager.class).to(RemotePrivilegesManager.class);
    }
  }
);
}"
6039,"@Override protected void configure(){
  bind(Store.class).to(DefaultStore.class);
  bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
}","@Override protected void configure(){
  bind(Store.class).to(DefaultStore.class);
  bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
  bind(PrivilegesManager.class).to(RemotePrivilegesManager.class);
}"
6040,"@VisibleForTesting static Injector createInjector(CConfiguration cConf,Configuration hConf){
  return Guice.createInjector(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MetricsClientRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new LocationRuntimeModule().getDistributedModules(),new NamespaceClientRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new LoggingModules().getDistributedModules(),new ExploreRuntimeModule().getDistributedModules(),new ExploreClientModule(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new AuditModule().getDistributedModules(),new AuthorizationEnforcementModule().getDistributedModules(),new AuthenticationContextModules().getMasterModule(),new SecureStoreModules().getDistributedModules(),new AuthorizationEnforcementModule().getDistributedModules(),new AuthorizationModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(Store.class).to(DefaultStore.class);
      bind(NamespaceStore.class).to(DefaultNamespaceStore.class);
      bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
    }
  }
);
}","@VisibleForTesting static Injector createInjector(CConfiguration cConf,Configuration hConf){
  return Guice.createInjector(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MetricsClientRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new LocationRuntimeModule().getDistributedModules(),new NamespaceClientRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new LoggingModules().getDistributedModules(),new ExploreRuntimeModule().getDistributedModules(),new ExploreClientModule(),new ViewAdminModules().getDistributedModules(),new StreamAdminModules().getDistributedModules(),new NotificationFeedClientModule(),new AuditModule().getDistributedModules(),new AuthenticationContextModules().getMasterModule(),new SecureStoreModules().getDistributedModules(),new AuthorizationEnforcementModule().getDistributedModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(Store.class).to(DefaultStore.class);
      bind(NamespaceStore.class).to(DefaultNamespaceStore.class);
      bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
      bind(PrivilegesManager.class).to(RemotePrivilegesManager.class);
    }
  }
);
}"
6041,"@Override protected void doInit(TwillContext context){
  CConfiguration cConf=getCConfiguration();
  Configuration hConf=getConfiguration();
  URL hiveSiteURL=getClass().getClassLoader().getResource(""String_Node_Str"");
  if (hiveSiteURL == null) {
    LOG.warn(""String_Node_Str"");
  }
 else {
    hConf.addResource(hiveSiteURL);
  }
  injector=createInjector(cConf,hConf);
  injector.getInstance(LogAppenderInitializer.class).initialize();
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(Id.Namespace.SYSTEM.getId(),Constants.Logging.COMPONENT_NAME,Constants.Service.EXPLORE_HTTP_USER_SERVICE));
  LOG.info(""String_Node_Str"",name);
  cConf.set(Constants.Explore.SERVER_ADDRESS,context.getHost().getHostName());
}","@Override protected void doInit(TwillContext context){
  CConfiguration cConf=getCConfiguration();
  Configuration hConf=getConfiguration();
  URL hiveSiteURL=getClass().getClassLoader().getResource(""String_Node_Str"");
  if (hiveSiteURL == null) {
    LOG.warn(""String_Node_Str"");
  }
 else {
    hConf.addResource(hiveSiteURL);
  }
  injector=createInjector(cConf,hConf);
  injector.getInstance(LogAppenderInitializer.class).initialize();
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(NamespaceId.SYSTEM.getNamespace(),Constants.Logging.COMPONENT_NAME,Constants.Service.EXPLORE_HTTP_USER_SERVICE));
  LOG.info(""String_Node_Str"",name);
  cConf.set(Constants.Explore.SERVER_ADDRESS,context.getHost().getHostName());
}"
6042,"@Override protected void configure(){
  bind(Store.class).to(DefaultStore.class);
  bind(NamespaceStore.class).to(DefaultNamespaceStore.class);
  bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
}","@Override protected void configure(){
  bind(Store.class).to(DefaultStore.class);
  bind(NamespaceStore.class).to(DefaultNamespaceStore.class);
  bind(UGIProvider.class).to(RemoteUGIProvider.class).in(Scopes.SINGLETON);
  bind(PrivilegesManager.class).to(RemotePrivilegesManager.class);
}"
6043,"/** 
 * Stores an element in the secure store.
 * @param namespace The namespace that this key belongs to.
 * @param name This is the identifier that will be used to retrieve this element.
 * @param data The sensitive data that has to be securely stored. Passed in as utf-8 formatted byte array.
 * @param description User provided description of the entry.
 * @param properties associated with this element.
 * @throws IOException If the attempt to store the element failed.
 * @throws Exception If the specified namespace does not exist or the name already exists. Updating is not supported.
 */
void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws Exception ;","/** 
 * Stores an element in the secure store.
 * @param namespace The namespace that this key belongs to.
 * @param name This is the identifier that will be used to retrieve this element.
 * @param data The sensitive data that has to be securely stored
 * @param description User provided description of the entry.
 * @param properties associated with this element.
 * @throws IOException If the attempt to store the element failed.
 * @throws Exception If the specified namespace does not exist or the name already exists. Updating is not supported.
 */
void putSecureData(String namespace,String name,String data,String description,Map<String,String> properties) throws Exception ;"
6044,"@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws Exception {
  secureStoreManager.putSecureData(namespace,name,data,description,properties);
}","@Override public void putSecureData(String namespace,String name,String data,String description,Map<String,String> properties) throws Exception {
  secureStoreManager.putSecureData(namespace,name,data,description,properties);
}"
6045,"/** 
 * Puts the user provided data in the secure store, if the user has write access to the namespace. Grants the user all access to the newly created entity.
 * @param secureKeyId The Id for the key that needs to be stored.
 * @param secureKeyCreateRequest The request containing the data to be stored in the secure store.
 * @throws BadRequestException If the request does not contain the value to be stored.
 * @throws UnauthorizedException If the user does not have write permissions on the namespace.
 * @throws NamespaceNotFoundException If the specified namespace does not exist.
 * @throws AlreadyExistsException If the key already exists in the namespace. Updating is not supported.
 * @throws IOException If there was a problem storing the key to underlying provider.
 */
@Override public synchronized void put(SecureKeyId secureKeyId,SecureKeyCreateRequest secureKeyCreateRequest) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  NamespaceId namespaceId=new NamespaceId(secureKeyId.getNamespace());
  authorizer.enforce(namespaceId,principal,Action.WRITE);
  String description=secureKeyCreateRequest.getDescription();
  String value=secureKeyCreateRequest.getData();
  if (Strings.isNullOrEmpty(value)) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  byte[] data=value.getBytes(StandardCharsets.UTF_8);
  secureStoreManager.putSecureData(secureKeyId.getNamespace(),secureKeyId.getName(),data,description,secureKeyCreateRequest.getProperties());
  authorizer.grant(secureKeyId,principal,ImmutableSet.of(Action.ALL));
}","/** 
 * Puts the user provided data in the secure store, if the user has write access to the namespace. Grants the user all access to the newly created entity.
 * @param secureKeyId The Id for the key that needs to be stored.
 * @param secureKeyCreateRequest The request containing the data to be stored in the secure store.
 * @throws BadRequestException If the request does not contain the value to be stored.
 * @throws UnauthorizedException If the user does not have write permissions on the namespace.
 * @throws NamespaceNotFoundException If the specified namespace does not exist.
 * @throws AlreadyExistsException If the key already exists in the namespace. Updating is not supported.
 * @throws IOException If there was a problem storing the key to underlying provider.
 */
@Override public synchronized void put(SecureKeyId secureKeyId,SecureKeyCreateRequest secureKeyCreateRequest) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  NamespaceId namespaceId=new NamespaceId(secureKeyId.getNamespace());
  authorizer.enforce(namespaceId,principal,Action.WRITE);
  String description=secureKeyCreateRequest.getDescription();
  String value=secureKeyCreateRequest.getData();
  if (Strings.isNullOrEmpty(value)) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  secureStoreManager.putSecureData(secureKeyId.getNamespace(),secureKeyId.getName(),value,description,secureKeyCreateRequest.getProperties());
  authorizer.grant(secureKeyId,principal,ImmutableSet.of(Action.ALL));
}"
6046,"@Override public void run(JavaSparkExecutionContext sec) throws Exception {
  final SecureStore secureStore=sec.getSecureStore();
  sec.getAdmin().putSecureData(NAMESPACE,KEY,VALUE.getBytes(),""String_Node_Str"",new HashMap<String,String>());
  Assert.assertEquals(new String(sec.getSecureData(NAMESPACE,KEY).get()),VALUE);
  Assert.assertEquals(new String(secureStore.getSecureData(NAMESPACE,KEY).get()),VALUE);
  sec.listSecureData(NAMESPACE);
  JavaSparkContext jsc=new JavaSparkContext();
  JavaPairRDD<Long,String> rdd=sec.fromStream(STREAM_NAME,String.class);
  JavaPairRDD<byte[],byte[]> resultRDD=rdd.mapToPair(new PairFunction<Tuple2<Long,String>,byte[],byte[]>(){
    @Override public Tuple2<byte[],byte[]> call(    Tuple2<Long,String> tuple2) throws Exception {
      return new Tuple2<>(Bytes.toBytes(tuple2._2()),secureStore.getSecureData(NAMESPACE,KEY).get());
    }
  }
);
  sec.saveAsDataset(resultRDD,""String_Node_Str"");
  sec.getAdmin().deleteSecureData(NAMESPACE,KEY);
}","@Override public void run(JavaSparkExecutionContext sec) throws Exception {
  final SecureStore secureStore=sec.getSecureStore();
  sec.getAdmin().putSecureData(NAMESPACE,KEY,VALUE,""String_Node_Str"",new HashMap<String,String>());
  Assert.assertEquals(new String(sec.getSecureData(NAMESPACE,KEY).get()),VALUE);
  Assert.assertEquals(new String(secureStore.getSecureData(NAMESPACE,KEY).get()),VALUE);
  sec.listSecureData(NAMESPACE);
  JavaSparkContext jsc=new JavaSparkContext();
  JavaPairRDD<Long,String> rdd=sec.fromStream(STREAM_NAME,String.class);
  JavaPairRDD<byte[],byte[]> resultRDD=rdd.mapToPair(new PairFunction<Tuple2<Long,String>,byte[],byte[]>(){
    @Override public Tuple2<byte[],byte[]> call(    Tuple2<Long,String> tuple2) throws Exception {
      return new Tuple2<>(Bytes.toBytes(tuple2._2()),secureStore.getSecureData(NAMESPACE,KEY).get());
    }
  }
);
  sec.saveAsDataset(resultRDD,""String_Node_Str"");
  sec.getAdmin().deleteSecureData(NAMESPACE,KEY);
}"
6047,"@Path(""String_Node_Str"") @PUT public void put(HttpServiceRequest request,HttpServiceResponder responder) throws Exception {
  byte[] value=new byte[request.getContent().remaining()];
  request.getContent().get(value);
  getContext().getAdmin().putSecureData(namespace,KEY,value,""String_Node_Str"",new HashMap<String,String>());
  responder.sendStatus(200);
}","@Path(""String_Node_Str"") @PUT public void put(HttpServiceRequest request,HttpServiceResponder responder) throws Exception {
  byte[] value=new byte[request.getContent().remaining()];
  request.getContent().get(value);
  getContext().getAdmin().putSecureData(namespace,KEY,new String(value),""String_Node_Str"",new HashMap<String,String>());
  responder.sendStatus(200);
}"
6048,"@Override public Admin getAdmin(){
  return new Admin(){
    @Override public boolean datasetExists(    String name){
      return false;
    }
    @Nullable @Override public String getDatasetType(    String name) throws InstanceNotFoundException {
      throw new InstanceNotFoundException(name);
    }
    @Nullable @Override public DatasetProperties getDatasetProperties(    String name) throws InstanceNotFoundException {
      throw new InstanceNotFoundException(name);
    }
    @Override public void createDataset(    String name,    String type,    DatasetProperties properties){
    }
    @Override public void updateDataset(    String name,    DatasetProperties properties){
    }
    @Override public void dropDataset(    String name){
    }
    @Override public void truncateDataset(    String name){
    }
    @Override public void putSecureData(    String namespace,    String name,    byte[] data,    String description,    Map<String,String> properties) throws Exception {
    }
    @Override public void deleteSecureData(    String namespace,    String name) throws Exception {
    }
  }
;
}","@Override public Admin getAdmin(){
  return new Admin(){
    @Override public boolean datasetExists(    String name){
      return false;
    }
    @Nullable @Override public String getDatasetType(    String name) throws InstanceNotFoundException {
      throw new InstanceNotFoundException(name);
    }
    @Nullable @Override public DatasetProperties getDatasetProperties(    String name) throws InstanceNotFoundException {
      throw new InstanceNotFoundException(name);
    }
    @Override public void createDataset(    String name,    String type,    DatasetProperties properties){
    }
    @Override public void updateDataset(    String name,    DatasetProperties properties){
    }
    @Override public void dropDataset(    String name){
    }
    @Override public void truncateDataset(    String name){
    }
    @Override public void putSecureData(    String namespace,    String name,    String data,    String description,    Map<String,String> properties) throws Exception {
    }
    @Override public void deleteSecureData(    String namespace,    String name) throws Exception {
    }
  }
;
}"
6049,"@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws Exception {
}","@Override public void putSecureData(String namespace,String name,String data,String description,Map<String,String> properties) throws Exception {
}"
6050,"/** 
 * Tests the secure storage macro function in a pipelines by creating datasets from the secure store data.
 */
private void testSecureStorePipeline(Engine engine,String prefix) throws Exception {
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MockRuntimeDatasetSource.getPlugin(""String_Node_Str"",""String_Node_Str"" + prefix + ""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockRuntimeDatasetSink.getPlugin(""String_Node_Str"",""String_Node_Str"" + prefix + ""String_Node_Str""))).addConnection(""String_Node_Str"",""String_Node_Str"").setEngine(engine).build();
  getSecureStoreManager().putSecureData(""String_Node_Str"",prefix + ""String_Node_Str"",(prefix + ""String_Node_Str"").getBytes(),""String_Node_Str"",new HashMap<String,String>());
  getSecureStoreManager().putSecureData(""String_Node_Str"",prefix + ""String_Node_Str"",(prefix + ""String_Node_Str"").getBytes(),""String_Node_Str"",new HashMap<String,String>());
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  Assert.assertNull(getDataset(prefix + ""String_Node_Str"").get());
  Assert.assertNull(getDataset(prefix + ""String_Node_Str"").get());
  workflowManager.start();
  workflowManager.waitForFinish(5,TimeUnit.MINUTES);
  Assert.assertNotNull(getDataset(prefix + ""String_Node_Str"").get());
  Assert.assertNotNull(getDataset(prefix + ""String_Node_Str"").get());
}","/** 
 * Tests the secure storage macro function in a pipelines by creating datasets from the secure store data.
 */
private void testSecureStorePipeline(Engine engine,String prefix) throws Exception {
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MockRuntimeDatasetSource.getPlugin(""String_Node_Str"",""String_Node_Str"" + prefix + ""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockRuntimeDatasetSink.getPlugin(""String_Node_Str"",""String_Node_Str"" + prefix + ""String_Node_Str""))).addConnection(""String_Node_Str"",""String_Node_Str"").setEngine(engine).build();
  getSecureStoreManager().putSecureData(""String_Node_Str"",prefix + ""String_Node_Str"",prefix + ""String_Node_Str"",""String_Node_Str"",new HashMap<String,String>());
  getSecureStoreManager().putSecureData(""String_Node_Str"",prefix + ""String_Node_Str"",prefix + ""String_Node_Str"",""String_Node_Str"",new HashMap<String,String>());
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  Assert.assertNull(getDataset(prefix + ""String_Node_Str"").get());
  Assert.assertNull(getDataset(prefix + ""String_Node_Str"").get());
  workflowManager.start();
  workflowManager.waitForFinish(5,TimeUnit.MINUTES);
  Assert.assertNotNull(getDataset(prefix + ""String_Node_Str"").get());
  Assert.assertNotNull(getDataset(prefix + ""String_Node_Str"").get());
}"
6051,"@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws Exception {
  context.getAdmin().putSecureData(namespace,name,data,description,properties);
}","@Override public void putSecureData(String namespace,String name,String data,String description,Map<String,String> properties) throws Exception {
  context.getAdmin().putSecureData(namespace,name,data,description,properties);
}"
6052,"/** 
 * Stores an element in the secure store. The key is stored as namespace:name in the backing store, assuming "":"" is the name separator.
 * @param namespace The namespace this key belongs to.
 * @param name Name of the element to store.
 * @param data The data that needs to be securely stored.
 * @param description User provided description of the entry.
 * @param properties Metadata associated with the data
 * @throws NamespaceNotFoundException If the specified namespace does not exist.
 * @throws IOException If it failed to store the key in the store.
 */
@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws Exception {
  checkNamespaceExists(namespace);
  KeyProvider.Options options=new KeyProvider.Options(conf);
  options.setDescription(description);
  options.setAttributes(properties);
  options.setBitLength(data.length * Byte.SIZE);
  String keyName=getKeyName(namespace,name);
  try {
    provider.createKey(keyName,data,options);
  }
 catch (  IOException e) {
    throw new IOException(""String_Node_Str"" + name + ""String_Node_Str""+ namespace,e);
  }
}","/** 
 * Stores an element in the secure store. The key is stored as namespace:name in the backing store, assuming "":"" is the name separator.
 * @param namespace The namespace this key belongs to.
 * @param name Name of the element to store.
 * @param data The data that needs to be securely stored.
 * @param description User provided description of the entry.
 * @param properties Metadata associated with the data
 * @throws NamespaceNotFoundException If the specified namespace does not exist.
 * @throws IOException If it failed to store the key in the store.
 */
@Override public void putSecureData(String namespace,String name,String data,String description,Map<String,String> properties) throws Exception {
  checkNamespaceExists(namespace);
  KeyProvider.Options options=new KeyProvider.Options(conf);
  options.setDescription(description);
  options.setAttributes(properties);
  byte[] buff=data.getBytes(Charsets.UTF_8);
  options.setBitLength(buff.length * Byte.SIZE);
  String keyName=getKeyName(namespace,name);
  try {
    provider.createKey(keyName,buff,options);
  }
 catch (  IOException e) {
    throw new IOException(""String_Node_Str"" + name + ""String_Node_Str""+ namespace,e);
  }
}"
6053,"@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws Exception {
  delegateAdmin.putSecureData(namespace,name,data,description,properties);
}","@Override public void putSecureData(String namespace,String name,String data,String description,Map<String,String> properties) throws Exception {
  delegateAdmin.putSecureData(namespace,name,data,description,properties);
}"
6054,"@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws IOException {
  throw new UnsupportedOperationException(UNSUPPORTED_ERROR_MSG);
}","@Override public void putSecureData(String namespace,String name,String data,String description,Map<String,String> properties) throws IOException {
  throw new UnsupportedOperationException(UNSUPPORTED_ERROR_MSG);
}"
6055,"/** 
 * Stores an element in the secure store. Although JCEKS supports overwriting keys the interface currently does not support it. If the key already exists then this method throws an AlreadyExistsException.
 * @param namespace The namespace this key belongs to.
 * @param name Name of the element to store.
 * @param data The data that needs to be securely stored.
 * @param description User provided description of the entry.
 * @param properties Metadata associated with the data.
 * @throws NamespaceNotFoundException If the specified namespace does not exist.
 * @throws AlreadyExistsException If the key already exists in the namespace. Updating is not supported.
 * @throws IOException If there was a problem storing the key to the in memory keystoreor if there was problem persisting the keystore.
 */
@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws Exception {
  checkNamespaceExists(namespace);
  String keyName=getKeyName(namespace,name);
  SecureStoreMetadata meta=SecureStoreMetadata.of(name,description,properties);
  SecureStoreData secureStoreData=new SecureStoreData(meta,data);
  writeLock.lock();
  try {
    if (keyStore.containsAlias(keyName)) {
      throw new AlreadyExistsException((new SecureKeyId(namespace,name)).toId());
    }
    keyStore.setKeyEntry(keyName,new KeyStoreEntry(secureStoreData,meta),password,null);
    flush();
    LOG.debug(String.format(""String_Node_Str"",name,namespace));
  }
 catch (  KeyStoreException e) {
    throw new IOException(""String_Node_Str"",e);
  }
 finally {
    writeLock.unlock();
  }
}","/** 
 * Stores an element in the secure store. Although JCEKS supports overwriting keys the interface currently does not support it. If the key already exists then this method throws an AlreadyExistsException.
 * @param namespace The namespace this key belongs to.
 * @param name Name of the element to store.
 * @param data The data that needs to be securely stored.
 * @param description User provided description of the entry.
 * @param properties Metadata associated with the data.
 * @throws NamespaceNotFoundException If the specified namespace does not exist.
 * @throws AlreadyExistsException If the key already exists in the namespace. Updating is not supported.
 * @throws IOException If there was a problem storing the key to the in memory keystoreor if there was problem persisting the keystore.
 */
@Override public void putSecureData(String namespace,String name,String data,String description,Map<String,String> properties) throws Exception {
  checkNamespaceExists(namespace);
  String keyName=getKeyName(namespace,name);
  SecureStoreMetadata meta=SecureStoreMetadata.of(name,description,properties);
  SecureStoreData secureStoreData=new SecureStoreData(meta,data.getBytes(Charsets.UTF_8));
  writeLock.lock();
  try {
    if (keyStore.containsAlias(keyName)) {
      throw new AlreadyExistsException((new SecureKeyId(namespace,name)).toId());
    }
    keyStore.setKeyEntry(keyName,new KeyStoreEntry(secureStoreData,meta),password,null);
    flush();
    LOG.debug(String.format(""String_Node_Str"",name,namespace));
  }
 catch (  KeyStoreException e) {
    throw new IOException(""String_Node_Str"",e);
  }
 finally {
    writeLock.unlock();
  }
}"
6056,"@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws Exception {
}","@Override public void putSecureData(String namespace,String name,String data,String description,Map<String,String> properties) throws Exception {
}"
6057,"@Test(expected=Exception.class) public void testOverwrite() throws Exception {
  secureStoreManager.putSecureData(NAMESPACE1,KEY1,VALUE1.getBytes(Charsets.UTF_8),DESCRIPTION1,PROPERTIES_1);
  SecureStoreData oldData=secureStore.getSecureData(NAMESPACE1,KEY1);
  Assert.assertArrayEquals(VALUE1.getBytes(Charsets.UTF_8),oldData.get());
  String newVal=""String_Node_Str"";
  secureStoreManager.putSecureData(NAMESPACE1,KEY1,newVal.getBytes(Charsets.UTF_8),DESCRIPTION1,PROPERTIES_1);
}","@Test(expected=Exception.class) public void testOverwrite() throws Exception {
  secureStoreManager.putSecureData(NAMESPACE1,KEY1,VALUE1,DESCRIPTION1,PROPERTIES_1);
  SecureStoreData oldData=secureStore.getSecureData(NAMESPACE1,KEY1);
  Assert.assertArrayEquals(VALUE1.getBytes(Charsets.UTF_8),oldData.get());
  String newVal=""String_Node_Str"";
  secureStoreManager.putSecureData(NAMESPACE1,KEY1,newVal,DESCRIPTION1,PROPERTIES_1);
}"
6058,"@Test public void testMultipleNamespaces() throws Exception {
  populateStore();
  secureStoreManager.putSecureData(NAMESPACE2,KEY1,VALUE1.getBytes(Charsets.UTF_8),DESCRIPTION1,PROPERTIES_1);
  List<SecureStoreMetadata> expectedList=ImmutableList.of(secureStore.getSecureData(NAMESPACE1,KEY2).getMetadata(),secureStore.getSecureData(NAMESPACE1,KEY1).getMetadata());
  Assert.assertEquals(expectedList,secureStore.listSecureData(NAMESPACE1));
  Assert.assertNotEquals(expectedList,secureStore.listSecureData(NAMESPACE2));
  List<SecureStoreMetadata> expectedList2=ImmutableList.of(secureStore.getSecureData(NAMESPACE2,KEY1).getMetadata());
  Assert.assertEquals(expectedList2,secureStore.listSecureData(NAMESPACE2));
  Assert.assertNotEquals(expectedList2,secureStore.listSecureData(NAMESPACE1));
}","@Test public void testMultipleNamespaces() throws Exception {
  populateStore();
  String ns=""String_Node_Str"";
  secureStoreManager.putSecureData(ns,KEY1,VALUE1,DESCRIPTION1,PROPERTIES_1);
  List<SecureStoreMetadata> expectedList=ImmutableList.of(secureStore.getSecureData(NAMESPACE1,KEY2).getMetadata(),secureStore.getSecureData(NAMESPACE1,KEY1).getMetadata());
  Assert.assertEquals(expectedList,secureStore.listSecureData(NAMESPACE1));
  Assert.assertNotEquals(expectedList,secureStore.listSecureData(NAMESPACE2));
  List<SecureStoreMetadata> expectedList2=ImmutableList.of(secureStore.getSecureData(NAMESPACE2,KEY1).getMetadata());
  Assert.assertEquals(expectedList2,secureStore.listSecureData(NAMESPACE2));
  Assert.assertNotEquals(expectedList2,secureStore.listSecureData(NAMESPACE1));
}"
6059,"private void populateStore() throws Exception {
  secureStoreManager.putSecureData(NAMESPACE1,KEY1,VALUE1.getBytes(Charsets.UTF_8),DESCRIPTION1,PROPERTIES_1);
  secureStoreManager.putSecureData(NAMESPACE1,KEY2,VALUE2.getBytes(Charsets.UTF_8),DESCRIPTION2,PROPERTIES_2);
}","private void populateStore() throws Exception {
  secureStoreManager.putSecureData(NAMESPACE1,KEY1,VALUE1,DESCRIPTION1,PROPERTIES_1);
  secureStoreManager.putSecureData(NAMESPACE1,KEY2,VALUE2,DESCRIPTION2,PROPERTIES_2);
}"
6060,"/** 
 * @param namespace The namespace that this key belongs to.
 * @param name Name of the data element.
 * @return An object representing the securely stored data associated with the name.
 */
SecureStoreData getSecureData(String namespace,String name) throws IOException ;","/** 
 * Returns the data stored in the secure store.
 * @param namespace The namespace that this key belongs to.
 * @param name Name of the data element.
 * @return An object representing the securely stored data associated with the name.
 * @throws IOException If there was a problem reading from the store.
 * @throws Exception if the specified namespace or name does not exist.
 */
SecureStoreData getSecureData(String namespace,String name) throws Exception ;"
6061,"/** 
 * @param namespace The namespace that this key belongs to.
 * @return A list of {@link SecureStoreMetadata} objects representing the data stored in the store.
 */
List<SecureStoreMetadata> listSecureData(String namespace) throws IOException ;","/** 
 * List of all the entries in the secure store.
 * @param namespace The namespace that this key belongs to.
 * @return A list of {@link SecureStoreMetadata} objects representing the data stored in the store.
 * @throws IOException If there was a problem reading from the keystore.
 * @throws Exception If the specified namespace does not exist.
 */
List<SecureStoreMetadata> listSecureData(String namespace) throws Exception ;"
6062,"/** 
 * @param namespace The namespace that this key belongs to.
 * @param name of the element to delete.
 * @throws IOException If the store is not initialized or if the key could not be removed.
 */
void deleteSecureData(String namespace,String name) throws IOException ;","/** 
 * Deletes the element with the given name.
 * @param namespace The namespace that this key belongs to.
 * @param name of the element to delete.
 * @throws IOException If the store is not initialized or if the key could not be removed.
 * @throws Exception If the specified namespace or name does not exist.
 */
void deleteSecureData(String namespace,String name) throws Exception ;"
6063,"/** 
 * @param namespace The namespace that this key belongs to.
 * @param name This is the identifier that will be used to retrieve this element.
 * @param data The sensitive data that has to be securely stored. Passed in as utf-8 formatted byte array.
 * @param description User provided description of the entry.
 * @param properties associated with this element.
 * @throws IOException If the attempt to store the element failed.
 */
void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws IOException ;","/** 
 * Stores an element in the secure store.
 * @param namespace The namespace that this key belongs to.
 * @param name This is the identifier that will be used to retrieve this element.
 * @param data The sensitive data that has to be securely stored. Passed in as utf-8 formatted byte array.
 * @param description User provided description of the entry.
 * @param properties associated with this element.
 * @throws IOException If the attempt to store the element failed.
 * @throws Exception If the specified namespace does not exist or the name already exists. Updating is not supported.
 */
void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws Exception ;"
6064,"@Override public List<SecureStoreMetadata> listSecureData(String namespace) throws IOException {
  return secureStore.listSecureData(namespace);
}","@Override public List<SecureStoreMetadata> listSecureData(String namespace) throws Exception {
  return secureStore.listSecureData(namespace);
}"
6065,"@Override public SecureStoreData getSecureData(String namespace,String name) throws IOException {
  return secureStore.getSecureData(namespace,name);
}","@Override public SecureStoreData getSecureData(String namespace,String name) throws Exception {
  return secureStore.getSecureData(namespace,name);
}"
6066,"@Override public void deleteSecureData(String namespace,String name) throws IOException {
  secureStoreManager.deleteSecureData(namespace,name);
}","@Override public void deleteSecureData(String namespace,String name) throws Exception {
  secureStoreManager.deleteSecureData(namespace,name);
}"
6067,"@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws IOException {
  secureStoreManager.putSecureData(namespace,name,data,description,properties);
}","@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws Exception {
  secureStoreManager.putSecureData(namespace,name,data,description,properties);
}"
6068,"@Override public List<SecureStoreMetadata> listSecureData(String namespace) throws IOException {
  return delegate.listSecureData(namespace);
}","@Override public List<SecureStoreMetadata> listSecureData(String namespace) throws Exception {
  return delegate.listSecureData(namespace);
}"
6069,"@Override public SecureStoreData getSecureData(String namespace,String name) throws IOException {
  return delegate.getSecureData(namespace,name);
}","@Override public SecureStoreData getSecureData(String namespace,String name) throws Exception {
  return delegate.getSecureData(namespace,name);
}"
6070,"/** 
 * Checks if the user has access to read the secure key and returns the data associated with the key if they do.
 * @param secureKeyId Id of the key that the user is trying to read.
 * @return Data associated with the key if the user has read access.
 * @throws Exception If we fail to create a filter for the current principalor the user does not have READ permissions on the secure key. or if there was a problem getting the data from the underlying provider.
 */
@Override public SecureStoreData get(SecureKeyId secureKeyId) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter;
  filter=authorizer.createFilter(principal);
  if (filter.apply(secureKeyId)) {
    return secureStore.getSecureData(secureKeyId.getNamespace(),secureKeyId.getName());
  }
  throw new UnauthorizedException(principal,Action.READ,secureKeyId);
}","/** 
 * Checks if the user has access to read the secure key and returns the   {@link SecureStoreData} associatedwith the key if they do.
 * @param secureKeyId Id of the key that the user is trying to read.
 * @return Data associated with the key if the user has read access.
 * @throws NamespaceNotFoundException If the specified namespace does not exist.
 * @throws NotFoundException If the key is not found in the store.
 * @throws IOException If there was a problem reading from the store.
 * @throws UnauthorizedException If the user does not have READ permissions on the secure key.
 */
@Override public SecureStoreData get(SecureKeyId secureKeyId) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter;
  filter=authorizer.createFilter(principal);
  if (filter.apply(secureKeyId)) {
    return secureStore.getSecureData(secureKeyId.getNamespace(),secureKeyId.getName());
  }
  throw new UnauthorizedException(principal,Action.READ,secureKeyId);
}"
6071,"/** 
 * Lists all the secure keys in the given namespace that the user has access to.
 * @param namespaceId Id of the namespace we want the key list for.
 * @return A list of {@link SecureKeyListEntry} of all the keys visible to the user under the given namespace.
 * @throws Exception If there was a problem getting the list from the underlying provider.or if we fail to create a filter for the current principal.
 */
@Override public List<SecureKeyListEntry> list(NamespaceId namespaceId) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter;
  filter=authorizer.createFilter(principal);
  List<SecureStoreMetadata> metadatas=secureStore.listSecureData(namespaceId.getNamespace());
  List<SecureKeyListEntry> result=new ArrayList<>(metadatas.size());
  String namespace=namespaceId.getNamespace();
  for (  SecureStoreMetadata metadata : metadatas) {
    String name=metadata.getName();
    if (filter.apply(new SecureKeyId(namespace,name))) {
      result.add(new SecureKeyListEntry(name,metadata.getDescription()));
    }
  }
  return result;
}","/** 
 * Lists all the secure keys in the given namespace that the user has access to. Returns an empty list if the user does not have access to the namespace or any of the keys in the namespace.
 * @param namespaceId Id of the namespace we want the key list for.
 * @return A list of {@link SecureKeyListEntry} for all the keys visible to the user under the given namespace.
 * @throws NamespaceNotFoundException If the specified namespace does not exist.
 * @throws IOException If there was a problem reading from the store.
 */
@Override public List<SecureKeyListEntry> list(NamespaceId namespaceId) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  final Predicate<EntityId> filter;
  filter=authorizer.createFilter(principal);
  List<SecureStoreMetadata> metadatas=secureStore.listSecureData(namespaceId.getNamespace());
  List<SecureKeyListEntry> result=new ArrayList<>(metadatas.size());
  String namespace=namespaceId.getNamespace();
  for (  SecureStoreMetadata metadata : metadatas) {
    String name=metadata.getName();
    if (filter.apply(new SecureKeyId(namespace,name))) {
      result.add(new SecureKeyListEntry(name,metadata.getDescription()));
    }
  }
  return result;
}"
6072,"/** 
 * Deletes the key if the user has ADMIN privileges to the key.
 * @param secureKeyId Id of the key to be deleted.
 * @throws UnauthorizedException If the user does not have admin privilages required to delete the secure key.
 * @throws IOException If there was a problem deleting it from the underlying provider.
 */
@Override public void delete(SecureKeyId secureKeyId) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  authorizer.enforce(secureKeyId,principal,Action.ADMIN);
  secureStoreManager.deleteSecureData(secureKeyId.getNamespace(),secureKeyId.getName());
  authorizer.revoke(secureKeyId);
}","/** 
 * Deletes the key if the user has ADMIN privileges to the key. Clears all the privileges associated with the key.
 * @param secureKeyId Id of the key to be deleted.
 * @throws UnauthorizedException If the user does not have admin privileges required to delete the secure key.
 * @throws NamespaceNotFoundException If the specified namespace does not exist.
 * @throws NotFoundException If the key to be deleted is not found.
 * @throws IOException If there was a problem deleting it from the underlying provider.
 */
@Override public void delete(SecureKeyId secureKeyId) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  authorizer.enforce(secureKeyId,principal,Action.ADMIN);
  secureStoreManager.deleteSecureData(secureKeyId.getNamespace(),secureKeyId.getName());
  authorizer.revoke(secureKeyId);
}"
6073,"/** 
 * Puts the user provided data in the secure store, if the user has write access to the namespace.
 * @param secureKeyId The Id for the key that needs to be stored.
 * @param secureKeyCreateRequest The request containing the data to be stored in the secure store.
 * @throws BadRequestException If the request does not contain the value to be stored.
 * @throws UnauthorizedException If the user does not have write permissions on the namespace.
 * @throws IOException If there was a problem storing the data to the underlying provider.
 */
@Override public synchronized void put(SecureKeyId secureKeyId,SecureKeyCreateRequest secureKeyCreateRequest) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  NamespaceId namespaceId=new NamespaceId(secureKeyId.getNamespace());
  authorizer.enforce(namespaceId,principal,Action.WRITE);
  String description=secureKeyCreateRequest.getDescription();
  String value=secureKeyCreateRequest.getData();
  if (Strings.isNullOrEmpty(value)) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  byte[] data=value.getBytes(StandardCharsets.UTF_8);
  secureStoreManager.putSecureData(secureKeyId.getNamespace(),secureKeyId.getName(),data,description,secureKeyCreateRequest.getProperties());
  try {
    authorizer.grant(secureKeyId,principal,ImmutableSet.of(Action.ALL));
  }
 catch (  Exception e) {
    throw new UnauthorizedException(principal,Action.ADMIN,secureKeyId);
  }
}","/** 
 * Puts the user provided data in the secure store, if the user has write access to the namespace. Grants the user all access to the newly created entity.
 * @param secureKeyId The Id for the key that needs to be stored.
 * @param secureKeyCreateRequest The request containing the data to be stored in the secure store.
 * @throws BadRequestException If the request does not contain the value to be stored.
 * @throws UnauthorizedException If the user does not have write permissions on the namespace.
 * @throws NamespaceNotFoundException If the specified namespace does not exist.
 * @throws AlreadyExistsException If the key already exists in the namespace. Updating is not supported.
 * @throws IOException If there was a problem storing the key to underlying provider.
 */
@Override public synchronized void put(SecureKeyId secureKeyId,SecureKeyCreateRequest secureKeyCreateRequest) throws Exception {
  Principal principal=authenticationContext.getPrincipal();
  NamespaceId namespaceId=new NamespaceId(secureKeyId.getNamespace());
  authorizer.enforce(namespaceId,principal,Action.WRITE);
  String description=secureKeyCreateRequest.getDescription();
  String value=secureKeyCreateRequest.getData();
  if (Strings.isNullOrEmpty(value)) {
    throw new BadRequestException(""String_Node_Str"" + ""String_Node_Str"");
  }
  byte[] data=value.getBytes(StandardCharsets.UTF_8);
  secureStoreManager.putSecureData(secureKeyId.getNamespace(),secureKeyId.getName(),data,description,secureKeyCreateRequest.getProperties());
  authorizer.grant(secureKeyId,principal,ImmutableSet.of(Action.ALL));
}"
6074,"@Path(""String_Node_Str"") @GET public void get(HttpServiceRequest request,HttpServiceResponder responder) throws IOException {
  try {
    byte[] bytes=getContext().getSecureData(namespace,KEY).get();
    responder.sendString(new String(bytes));
  }
 catch (  IOException e) {
    responder.sendError(500,e.getMessage());
  }
}","@Path(""String_Node_Str"") @GET public void get(HttpServiceRequest request,HttpServiceResponder responder){
  try {
    byte[] bytes=getContext().getSecureData(namespace,KEY).get();
    responder.sendString(new String(bytes));
  }
 catch (  Exception e) {
    responder.sendError(500,e.getMessage());
  }
}"
6075,"@Path(""String_Node_Str"") @GET public void list(HttpServiceRequest request,HttpServiceResponder responder) throws IOException {
  String name=getContext().listSecureData(namespace).get(0).getName();
  responder.sendString(name);
}","@Path(""String_Node_Str"") @GET public void list(HttpServiceRequest request,HttpServiceResponder responder) throws Exception {
  String name=getContext().listSecureData(namespace).get(0).getName();
  responder.sendString(name);
}"
6076,"@Path(""String_Node_Str"") @GET public void delete(HttpServiceRequest request,HttpServiceResponder responder) throws IOException {
  getContext().getAdmin().deleteSecureData(namespace,KEY);
  responder.sendStatus(200);
}","@Path(""String_Node_Str"") @GET public void delete(HttpServiceRequest request,HttpServiceResponder responder) throws Exception {
  getContext().getAdmin().deleteSecureData(namespace,KEY);
  responder.sendStatus(200);
}"
6077,"@Path(""String_Node_Str"") @PUT public void put(HttpServiceRequest request,HttpServiceResponder responder) throws IOException {
  byte[] value=new byte[request.getContent().remaining()];
  request.getContent().get(value);
  getContext().getAdmin().putSecureData(namespace,KEY,value,""String_Node_Str"",new HashMap<String,String>());
  responder.sendStatus(200);
}","@Path(""String_Node_Str"") @PUT public void put(HttpServiceRequest request,HttpServiceResponder responder) throws Exception {
  byte[] value=new byte[request.getContent().remaining()];
  request.getContent().get(value);
  getContext().getAdmin().putSecureData(namespace,KEY,value,""String_Node_Str"",new HashMap<String,String>());
  responder.sendStatus(200);
}"
6078,"@Override public void deleteSecureData(String namespace,String name) throws IOException {
}","@Override public void deleteSecureData(String namespace,String name) throws Exception {
}"
6079,"@Override public Admin getAdmin(){
  return new Admin(){
    @Override public boolean datasetExists(    String name){
      return false;
    }
    @Nullable @Override public String getDatasetType(    String name) throws InstanceNotFoundException {
      throw new InstanceNotFoundException(name);
    }
    @Nullable @Override public DatasetProperties getDatasetProperties(    String name) throws InstanceNotFoundException {
      throw new InstanceNotFoundException(name);
    }
    @Override public void createDataset(    String name,    String type,    DatasetProperties properties){
    }
    @Override public void updateDataset(    String name,    DatasetProperties properties){
    }
    @Override public void dropDataset(    String name){
    }
    @Override public void truncateDataset(    String name){
    }
    @Override public void putSecureData(    String namespace,    String name,    byte[] data,    String description,    Map<String,String> properties) throws IOException {
    }
    @Override public void deleteSecureData(    String namespace,    String name) throws IOException {
    }
  }
;
}","@Override public Admin getAdmin(){
  return new Admin(){
    @Override public boolean datasetExists(    String name){
      return false;
    }
    @Nullable @Override public String getDatasetType(    String name) throws InstanceNotFoundException {
      throw new InstanceNotFoundException(name);
    }
    @Nullable @Override public DatasetProperties getDatasetProperties(    String name) throws InstanceNotFoundException {
      throw new InstanceNotFoundException(name);
    }
    @Override public void createDataset(    String name,    String type,    DatasetProperties properties){
    }
    @Override public void updateDataset(    String name,    DatasetProperties properties){
    }
    @Override public void dropDataset(    String name){
    }
    @Override public void truncateDataset(    String name){
    }
    @Override public void putSecureData(    String namespace,    String name,    byte[] data,    String description,    Map<String,String> properties) throws Exception {
    }
    @Override public void deleteSecureData(    String namespace,    String name) throws Exception {
    }
  }
;
}"
6080,"@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws IOException {
}","@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws Exception {
}"
6081,"@Override public List<SecureStoreMetadata> listSecureData(String namespace) throws IOException {
  return null;
}","@Override public List<SecureStoreMetadata> listSecureData(String namespace) throws Exception {
  return null;
}"
6082,"@Override public SecureStoreData getSecureData(String namespace,String name) throws IOException {
  return null;
}","@Override public SecureStoreData getSecureData(String namespace,String name) throws Exception {
  return null;
}"
6083,"private static CConfiguration createCConf() throws IOException {
  File rootLocationFactoryPath=TEMPORARY_FOLDER.newFolder();
  String secureStoreLocation=TEMPORARY_FOLDER.newFolder().getAbsolutePath();
  CConfiguration cConf=CConfiguration.create();
  cConf.setStrings(Constants.Security.Store.FILE_PATH,secureStoreLocation);
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  cConf.set(Constants.Security.Authorization.SUPERUSERS,""String_Node_Str"");
  LocationFactory locationFactory=new LocalLocationFactory(rootLocationFactoryPath);
  Location authorizerJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,authorizerJar.toURI().getPath());
  return cConf;
}","private static CConfiguration createCConf() throws Exception {
  LocationFactory locationFactory=new LocalLocationFactory(TEMPORARY_FOLDER.newFolder());
  String secureStoreLocation=locationFactory.create(""String_Node_Str"").toURI().getPath();
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Security.Store.FILE_PATH,secureStoreLocation);
  cConf.setBoolean(Constants.Security.ENABLED,true);
  cConf.setBoolean(Constants.Security.Authorization.ENABLED,true);
  cConf.setBoolean(Constants.Security.KERBEROS_ENABLED,false);
  cConf.setBoolean(Constants.Security.Authorization.CACHE_ENABLED,false);
  cConf.set(Constants.Security.Authorization.SUPERUSERS,""String_Node_Str"");
  Location authorizerJar=AppJarHelper.createDeploymentJar(locationFactory,InMemoryAuthorizer.class);
  cConf.set(Constants.Security.Authorization.EXTENSION_JAR_PATH,authorizerJar.toURI().getPath());
  return cConf;
}"
6084,"@Test public void testSecureStoreAccess() throws Exception {
  final SecureKeyId secureKeyId1=NamespaceId.DEFAULT.secureKey(KEY1);
  SecurityRequestContext.setUserId(ALICE.getName());
  final SecureKeyCreateRequest createRequest=new SecureKeyCreateRequest(DESCRIPTION1,VALUE1,Collections.<String,String>emptyMap());
  try {
    secureStoreService.put(secureKeyId1,createRequest);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  grantAndAssertSuccess(NamespaceId.DEFAULT,ALICE,ImmutableSet.of(Action.WRITE));
  secureStoreService.put(secureKeyId1,createRequest);
  List<SecureKeyListEntry> secureKeyListEntries=secureStoreService.list(NamespaceId.DEFAULT);
  Assert.assertEquals(secureKeyListEntries.size(),1);
  Assert.assertEquals(secureKeyListEntries.get(0).getName(),KEY1);
  Assert.assertEquals(secureKeyListEntries.get(0).getDescription(),DESCRIPTION1);
  revokeAndAssertSuccess(secureKeyId1,ALICE,ImmutableSet.of(Action.ALL));
  secureKeyListEntries=secureStoreService.list(NamespaceId.DEFAULT);
  Assert.assertEquals(0,secureKeyListEntries.size());
  SecurityRequestContext.setUserId(BOB.getName());
  grantAndAssertSuccess(secureKeyId1,BOB,ImmutableSet.of(Action.READ));
  Assert.assertArrayEquals(secureStoreService.get(secureKeyId1).get(),VALUE1.getBytes());
  secureKeyListEntries=secureStoreService.list(NamespaceId.DEFAULT);
  Assert.assertEquals(1,secureKeyListEntries.size());
  try {
    secureStoreService.delete(secureKeyId1);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  grantAndAssertSuccess(secureKeyId1,BOB,ImmutableSet.of(Action.ADMIN));
  secureStoreService.delete(secureKeyId1);
  Assert.assertEquals(0,secureStoreService.list(NamespaceId.DEFAULT).size());
  Predicate<Privilege> secureKeyIdFilter=new Predicate<Privilege>(){
    @Override public boolean apply(    Privilege input){
      return input.getEntity().equals(secureKeyId1);
    }
  }
;
  Assert.assertTrue(Sets.filter(authorizer.listPrivileges(ALICE),secureKeyIdFilter).isEmpty());
  Assert.assertTrue(Sets.filter(authorizer.listPrivileges(BOB),secureKeyIdFilter).isEmpty());
}","@Test public void testSecureStoreAccess() throws Exception {
  final SecureKeyId secureKeyId1=NamespaceId.DEFAULT.secureKey(KEY1);
  SecurityRequestContext.setUserId(ALICE.getName());
  final SecureKeyCreateRequest createRequest=new SecureKeyCreateRequest(DESCRIPTION1,VALUE1,Collections.<String,String>emptyMap());
  try {
    secureStoreService.put(secureKeyId1,createRequest);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  grantAndAssertSuccess(NamespaceId.DEFAULT,ALICE,ImmutableSet.of(Action.WRITE));
  secureStoreService.put(secureKeyId1,createRequest);
  List<SecureKeyListEntry> secureKeyListEntries=secureStoreService.list(NamespaceId.DEFAULT);
  Assert.assertEquals(secureKeyListEntries.size(),1);
  Assert.assertEquals(secureKeyListEntries.get(0).getName(),KEY1);
  Assert.assertEquals(secureKeyListEntries.get(0).getDescription(),DESCRIPTION1);
  revokeAndAssertSuccess(secureKeyId1,ALICE,ImmutableSet.of(Action.ALL));
  secureKeyListEntries=secureStoreService.list(NamespaceId.DEFAULT);
  Assert.assertEquals(0,secureKeyListEntries.size());
  SecurityRequestContext.setUserId(BOB.getName());
  grantAndAssertSuccess(NamespaceId.DEFAULT,BOB,ImmutableSet.of(Action.READ));
  grantAndAssertSuccess(secureKeyId1,BOB,ImmutableSet.of(Action.READ));
  Assert.assertArrayEquals(secureStoreService.get(secureKeyId1).get(),VALUE1.getBytes());
  secureKeyListEntries=secureStoreService.list(NamespaceId.DEFAULT);
  Assert.assertEquals(1,secureKeyListEntries.size());
  try {
    secureStoreService.delete(secureKeyId1);
    Assert.fail(""String_Node_Str"");
  }
 catch (  UnauthorizedException expected) {
  }
  grantAndAssertSuccess(secureKeyId1,BOB,ImmutableSet.of(Action.ADMIN));
  secureStoreService.delete(secureKeyId1);
  Assert.assertEquals(0,secureStoreService.list(NamespaceId.DEFAULT).size());
  Predicate<Privilege> secureKeyIdFilter=new Predicate<Privilege>(){
    @Override public boolean apply(    Privilege input){
      return input.getEntity().equals(secureKeyId1);
    }
  }
;
  Assert.assertTrue(Sets.filter(authorizer.listPrivileges(ALICE),secureKeyIdFilter).isEmpty());
  Assert.assertTrue(Sets.filter(authorizer.listPrivileges(BOB),secureKeyIdFilter).isEmpty());
}"
6085,"@BeforeClass public static void setup() throws Exception {
  Injector injector=Guice.createInjector(new AppFabricTestModule(createCConf()));
  secureStoreService=injector.getInstance(SecureStoreService.class);
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
}","@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=createCConf();
  final Injector injector=AppFabricTestHelper.getInjector(cConf);
  appFabricServer=injector.getInstance(AppFabricServer.class);
  appFabricServer.startAndWait();
  authorizationEnforcementService=injector.getInstance(AuthorizationEnforcementService.class);
  authorizationEnforcementService.startAndWait();
  remoteSystemOperationsService=injector.getInstance(RemoteSystemOperationsService.class);
  remoteSystemOperationsService.startAndWait();
  authorizer=injector.getInstance(AuthorizerInstantiator.class).get();
  SecurityRequestContext.setUserId(ALICE.getName());
  secureStoreService=injector.getInstance(SecureStoreService.class);
  authorizer.grant(NamespaceId.DEFAULT,ALICE,Collections.singleton(Action.READ));
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return injector.getInstance(NamespaceAdmin.class).exists(Id.Namespace.DEFAULT);
    }
  }
,5,TimeUnit.SECONDS);
  authorizer.revoke(NamespaceId.DEFAULT,ALICE,Collections.singleton(Action.READ));
}"
6086,"@Override public void deleteSecureData(String namespace,String name) throws Exception {
  context.getAdmin().deleteSecureData(namespace,name);
}","@Override public void deleteSecureData(String namespace,String name) throws IOException {
  context.getAdmin().deleteSecureData(namespace,name);
}"
6087,"@Override public SecureStoreData getSecureData(String namespace,String name) throws Exception {
  return context.getSecureData(namespace,name);
}","@Override public SecureStoreData getSecureData(String namespace,String name) throws IOException {
  return context.getSecureData(namespace,name);
}"
6088,"@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws Exception {
  context.getAdmin().putSecureData(namespace,name,data,description,properties);
}","@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws IOException {
  context.getAdmin().putSecureData(namespace,name,data,description,properties);
}"
6089,"@Override public List<SecureStoreMetadata> listSecureData(String namespace) throws Exception {
  return context.listSecureData(namespace);
}","@Override public List<SecureStoreMetadata> listSecureData(String namespace) throws IOException {
  return context.listSecureData(namespace);
}"
6090,"private void revoke(RevokeRequest revokeRequest) throws IOException, UnauthenticatedException, FeatureDisabledException, UnauthorizedException, NotFoundException {
  URL url=config.resolveURLV3(AUTHORIZATION_BASE + ""String_Node_Str"");
  HttpRequest request=HttpRequest.post(url).withBody(GSON.toJson(revokeRequest)).build();
  executePrivilegeRequest(revokeRequest.getEntity(),request);
}","private void revoke(RevokeRequest revokeRequest) throws IOException, UnauthenticatedException, FeatureDisabledException, UnauthorizedException, NotFoundException {
  URL url=config.resolveURLV3(AUTHORIZATION_BASE + ""String_Node_Str"");
  HttpRequest request=HttpRequest.post(url).withBody(GSON.toJson(revokeRequest)).build();
  executePrivilegeRequest(request);
}"
6091,"@Override public void grant(EntityId entity,Principal principal,Set<Action> actions) throws IOException, UnauthenticatedException, FeatureDisabledException, UnauthorizedException, NotFoundException {
  GrantRequest grantRequest=new GrantRequest(entity,principal,actions);
  URL url=config.resolveURLV3(AUTHORIZATION_BASE + ""String_Node_Str"");
  HttpRequest request=HttpRequest.post(url).withBody(GSON.toJson(grantRequest)).build();
  executePrivilegeRequest(entity,request);
}","@Override public void grant(EntityId entity,Principal principal,Set<Action> actions) throws IOException, UnauthenticatedException, FeatureDisabledException, UnauthorizedException, NotFoundException {
  GrantRequest grantRequest=new GrantRequest(entity,principal,actions);
  URL url=config.resolveURLV3(AUTHORIZATION_BASE + ""String_Node_Str"");
  HttpRequest request=HttpRequest.post(url).withBody(GSON.toJson(grantRequest)).build();
  executePrivilegeRequest(request);
}"
6092,"private HttpResponse executePrivilegeRequest(EntityId entityId,HttpRequest request) throws FeatureDisabledException, UnauthenticatedException, IOException, NotFoundException, UnauthorizedException {
  HttpResponse httpResponse=doExecuteRequest(request,HttpURLConnection.HTTP_NOT_FOUND);
  if (HttpURLConnection.HTTP_NOT_FOUND == httpResponse.getResponseCode()) {
    throw new NotFoundException(entityId);
  }
  return httpResponse;
}","private HttpResponse executePrivilegeRequest(HttpRequest request) throws FeatureDisabledException, UnauthenticatedException, IOException, NotFoundException, UnauthorizedException {
  HttpResponse httpResponse=doExecuteRequest(request,HttpURLConnection.HTTP_NOT_FOUND);
  if (HttpURLConnection.HTTP_NOT_FOUND == httpResponse.getResponseCode()) {
    throw new NotFoundException(httpResponse.getResponseBodyAsString());
  }
  return httpResponse;
}"
6093,"private LineageRecord getLineage(Id.NamespacedId namespacedId,String path) throws IOException, UnauthenticatedException, NotFoundException, BadRequestException {
  URL lineageURL=config.resolveNamespacedURLV3(namespacedId.getNamespace(),path);
  HttpResponse response=restClient.execute(HttpRequest.get(lineageURL).build(),config.getAccessToken(),HttpURLConnection.HTTP_BAD_REQUEST,HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_BAD_REQUEST) {
    throw new BadRequestException(response.getResponseBodyAsString());
  }
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new NotFoundException(namespacedId);
  }
  return GSON.fromJson(response.getResponseBodyAsString(),LineageRecord.class);
}","private LineageRecord getLineage(Id.NamespacedId namespacedId,String path) throws IOException, UnauthenticatedException, NotFoundException, BadRequestException {
  URL lineageURL=config.resolveNamespacedURLV3(namespacedId.getNamespace(),path);
  HttpResponse response=restClient.execute(HttpRequest.get(lineageURL).build(),config.getAccessToken(),HttpURLConnection.HTTP_BAD_REQUEST,HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_BAD_REQUEST) {
    throw new BadRequestException(response.getResponseBodyAsString());
  }
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new NotFoundException(response.getResponseBodyAsString());
  }
  return GSON.fromJson(response.getResponseBodyAsString(),LineageRecord.class);
}"
6094,"@Override public void deleteSecureData(String namespace,String name) throws IOException {
  context.getAdmin().deleteSecureData(namespace,name);
}","@Override public void deleteSecureData(String namespace,String name) throws Exception {
  context.getAdmin().deleteSecureData(namespace,name);
}"
6095,"@Override public SecureStoreData getSecureData(String namespace,String name) throws IOException {
  return context.getSecureData(namespace,name);
}","@Override public SecureStoreData getSecureData(String namespace,String name) throws Exception {
  return context.getSecureData(namespace,name);
}"
6096,"@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws IOException {
  context.getAdmin().putSecureData(namespace,name,data,description,properties);
}","@Override public void putSecureData(String namespace,String name,byte[] data,String description,Map<String,String> properties) throws Exception {
  context.getAdmin().putSecureData(namespace,name,data,description,properties);
}"
6097,"@Override public List<SecureStoreMetadata> listSecureData(String namespace) throws IOException {
  return context.listSecureData(namespace);
}","@Override public List<SecureStoreMetadata> listSecureData(String namespace) throws Exception {
  return context.listSecureData(namespace);
}"
6098,"private void revoke(RevokeRequest revokeRequest) throws IOException, UnauthenticatedException, FeatureDisabledException, UnauthorizedException, NotFoundException {
  URL url=config.resolveURLV3(AUTHORIZATION_BASE + ""String_Node_Str"");
  HttpRequest request=HttpRequest.post(url).withBody(GSON.toJson(revokeRequest)).build();
  executePrivilegeRequest(revokeRequest.getEntity(),request);
}","private void revoke(RevokeRequest revokeRequest) throws IOException, UnauthenticatedException, FeatureDisabledException, UnauthorizedException, NotFoundException {
  URL url=config.resolveURLV3(AUTHORIZATION_BASE + ""String_Node_Str"");
  HttpRequest request=HttpRequest.post(url).withBody(GSON.toJson(revokeRequest)).build();
  executePrivilegeRequest(request);
}"
6099,"@Override public void grant(EntityId entity,Principal principal,Set<Action> actions) throws IOException, UnauthenticatedException, FeatureDisabledException, UnauthorizedException, NotFoundException {
  GrantRequest grantRequest=new GrantRequest(entity,principal,actions);
  URL url=config.resolveURLV3(AUTHORIZATION_BASE + ""String_Node_Str"");
  HttpRequest request=HttpRequest.post(url).withBody(GSON.toJson(grantRequest)).build();
  executePrivilegeRequest(entity,request);
}","@Override public void grant(EntityId entity,Principal principal,Set<Action> actions) throws IOException, UnauthenticatedException, FeatureDisabledException, UnauthorizedException, NotFoundException {
  GrantRequest grantRequest=new GrantRequest(entity,principal,actions);
  URL url=config.resolveURLV3(AUTHORIZATION_BASE + ""String_Node_Str"");
  HttpRequest request=HttpRequest.post(url).withBody(GSON.toJson(grantRequest)).build();
  executePrivilegeRequest(request);
}"
6100,"private HttpResponse executePrivilegeRequest(EntityId entityId,HttpRequest request) throws FeatureDisabledException, UnauthenticatedException, IOException, NotFoundException, UnauthorizedException {
  HttpResponse httpResponse=doExecuteRequest(request,HttpURLConnection.HTTP_NOT_FOUND);
  if (HttpURLConnection.HTTP_NOT_FOUND == httpResponse.getResponseCode()) {
    throw new NotFoundException(entityId);
  }
  return httpResponse;
}","private HttpResponse executePrivilegeRequest(HttpRequest request) throws FeatureDisabledException, UnauthenticatedException, IOException, NotFoundException, UnauthorizedException {
  HttpResponse httpResponse=doExecuteRequest(request,HttpURLConnection.HTTP_NOT_FOUND);
  if (HttpURLConnection.HTTP_NOT_FOUND == httpResponse.getResponseCode()) {
    throw new NotFoundException(httpResponse.getResponseBodyAsString());
  }
  return httpResponse;
}"
6101,"private LineageRecord getLineage(Id.NamespacedId namespacedId,String path) throws IOException, UnauthenticatedException, NotFoundException, BadRequestException {
  URL lineageURL=config.resolveNamespacedURLV3(namespacedId.getNamespace(),path);
  HttpResponse response=restClient.execute(HttpRequest.get(lineageURL).build(),config.getAccessToken(),HttpURLConnection.HTTP_BAD_REQUEST,HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_BAD_REQUEST) {
    throw new BadRequestException(response.getResponseBodyAsString());
  }
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new NotFoundException(namespacedId);
  }
  return GSON.fromJson(response.getResponseBodyAsString(),LineageRecord.class);
}","private LineageRecord getLineage(Id.NamespacedId namespacedId,String path) throws IOException, UnauthenticatedException, NotFoundException, BadRequestException {
  URL lineageURL=config.resolveNamespacedURLV3(namespacedId.getNamespace(),path);
  HttpResponse response=restClient.execute(HttpRequest.get(lineageURL).build(),config.getAccessToken(),HttpURLConnection.HTTP_BAD_REQUEST,HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_BAD_REQUEST) {
    throw new BadRequestException(response.getResponseBodyAsString());
  }
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new NotFoundException(response.getResponseBodyAsString());
  }
  return GSON.fromJson(response.getResponseBodyAsString(),LineageRecord.class);
}"
6102,"@AfterClass public static void finish() throws Exception {
  if (--startCount != 0) {
    return;
  }
  if (cConf.getBoolean(Constants.Security.Authorization.ENABLED)) {
    InstanceId instance=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
    Principal principal=new Principal(System.getProperty(""String_Node_Str""),Principal.PrincipalType.USER);
    authorizerInstantiator.get().grant(instance,principal,ImmutableSet.of(Action.ADMIN));
    authorizerInstantiator.get().grant(NamespaceId.DEFAULT,principal,ImmutableSet.of(Action.ADMIN));
  }
  namespaceAdmin.delete(Id.Namespace.DEFAULT);
  authorizerInstantiator.close();
  streamCoordinatorClient.stopAndWait();
  metricsQueryService.stopAndWait();
  metricsCollectionService.startAndWait();
  schedulerService.stopAndWait();
  if (exploreClient != null) {
    Closeables.closeQuietly(exploreClient);
  }
  if (exploreExecutorService != null) {
    exploreExecutorService.stopAndWait();
  }
  datasetService.stopAndWait();
  dsOpService.stopAndWait();
  txService.stopAndWait();
}","@AfterClass public static void finish() throws Exception {
  if (--nestedStartCount != 0) {
    return;
  }
  if (cConf.getBoolean(Constants.Security.Authorization.ENABLED)) {
    InstanceId instance=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
    Principal principal=new Principal(System.getProperty(""String_Node_Str""),Principal.PrincipalType.USER);
    authorizerInstantiator.get().grant(instance,principal,ImmutableSet.of(Action.ADMIN));
    authorizerInstantiator.get().grant(NamespaceId.DEFAULT,principal,ImmutableSet.of(Action.ADMIN));
  }
  namespaceAdmin.delete(Id.Namespace.DEFAULT);
  authorizerInstantiator.close();
  streamCoordinatorClient.stopAndWait();
  metricsQueryService.stopAndWait();
  metricsCollectionService.startAndWait();
  schedulerService.stopAndWait();
  if (exploreClient != null) {
    Closeables.closeQuietly(exploreClient);
  }
  if (exploreExecutorService != null) {
    exploreExecutorService.stopAndWait();
  }
  datasetService.stopAndWait();
  dsOpService.stopAndWait();
  txService.stopAndWait();
}"
6103,"@BeforeClass public static void initialize() throws Exception {
  if (startCount++ > 0) {
    return;
  }
  File localDataDir=TMP_FOLDER.newFolder();
  cConf=createCConf(localDataDir);
  org.apache.hadoop.conf.Configuration hConf=new org.apache.hadoop.conf.Configuration();
  hConf.addResource(""String_Node_Str"");
  hConf.reloadConfiguration();
  hConf.set(Constants.CFG_LOCAL_DATA_DIR,localDataDir.getAbsolutePath());
  hConf.set(Constants.AppFabric.OUTPUT_DIR,cConf.get(Constants.AppFabric.OUTPUT_DIR));
  hConf.set(""String_Node_Str"",new File(localDataDir,cConf.get(Constants.AppFabric.TEMP_DIR)).getAbsolutePath());
  if (OSDetector.isWindows()) {
    File tmpDir=TMP_FOLDER.newFolder();
    File binDir=new File(tmpDir,""String_Node_Str"");
    Assert.assertTrue(binDir.mkdirs());
    copyTempFile(""String_Node_Str"",tmpDir);
    copyTempFile(""String_Node_Str"",binDir);
    System.setProperty(""String_Node_Str"",tmpDir.getAbsolutePath());
    System.load(new File(tmpDir,""String_Node_Str"").getAbsolutePath());
  }
  Injector injector=Guice.createInjector(createDataFabricModule(),new TransactionExecutorModule(),new DataSetsModules().getStandaloneModules(),new DataSetServiceModules().getInMemoryModules(),new ConfigModule(cConf,hConf),new IOModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new AppFabricServiceRuntimeModule().getInMemoryModules(),new ServiceStoreModules().getInMemoryModules(),new InMemoryProgramRunnerModule(LocalStreamWriter.class),new AbstractModule(){
    @Override protected void configure(){
      bind(StreamHandler.class).in(Scopes.SINGLETON);
      bind(StreamFetchHandler.class).in(Scopes.SINGLETON);
      bind(StreamViewHttpHandler.class).in(Scopes.SINGLETON);
      bind(StreamFileJanitorService.class).to(LocalStreamFileJanitorService.class).in(Scopes.SINGLETON);
      bind(StreamWriterSizeCollector.class).to(BasicStreamWriterSizeCollector.class).in(Scopes.SINGLETON);
      bind(StreamCoordinatorClient.class).to(InMemoryStreamCoordinatorClient.class).in(Scopes.SINGLETON);
      bind(MetricsManager.class).toProvider(MetricsManagerProvider.class);
    }
  }
,new MetricsHandlerModule(),new MetricsClientRuntimeModule().getInMemoryModules(),new LoggingModules().getInMemoryModules(),new LogReaderRuntimeModules().getInMemoryModules(),new ExploreRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new NotificationFeedServiceRuntimeModule().getInMemoryModules(),new NotificationServiceRuntimeModule().getInMemoryModules(),new NamespaceStoreModule().getStandaloneModules(),new AuthorizationModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AbstractModule(){
    @Override @SuppressWarnings(""String_Node_Str"") protected void configure(){
      install(new FactoryModuleBuilder().implement(ApplicationManager.class,DefaultApplicationManager.class).build(ApplicationManagerFactory.class));
      install(new FactoryModuleBuilder().implement(ArtifactManager.class,DefaultArtifactManager.class).build(ArtifactManagerFactory.class));
      install(new FactoryModuleBuilder().implement(StreamManager.class,DefaultStreamManager.class).build(StreamManagerFactory.class));
      bind(TemporaryFolder.class).toInstance(TMP_FOLDER);
      bind(AuthorizationHandler.class).in(Scopes.SINGLETON);
    }
  }
);
  txService=injector.getInstance(TransactionManager.class);
  txService.startAndWait();
  dsOpService=injector.getInstance(DatasetOpExecutor.class);
  dsOpService.startAndWait();
  datasetService=injector.getInstance(DatasetService.class);
  datasetService.startAndWait();
  metricsQueryService=injector.getInstance(MetricsQueryService.class);
  metricsQueryService.startAndWait();
  metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  metricsCollectionService.startAndWait();
  schedulerService=injector.getInstance(SchedulerService.class);
  schedulerService.startAndWait();
  if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED)) {
    exploreExecutorService=injector.getInstance(ExploreExecutorService.class);
    exploreExecutorService.startAndWait();
    exploreClient=injector.getInstance(ExploreClient.class);
  }
  streamCoordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  streamCoordinatorClient.startAndWait();
  testManager=injector.getInstance(UnitTestManager.class);
  metricsManager=injector.getInstance(MetricsManager.class);
  authorizerInstantiator=injector.getInstance(AuthorizerInstantiator.class);
  if (cConf.getBoolean(Constants.Security.Authorization.ENABLED)) {
    String user=System.getProperty(""String_Node_Str"");
    SecurityRequestContext.setUserId(user);
    InstanceId instance=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
    Principal principal=new Principal(user,Principal.PrincipalType.USER);
    authorizerInstantiator.get().grant(instance,principal,ImmutableSet.of(Action.ADMIN));
    authorizerInstantiator.get().grant(NamespaceId.DEFAULT,principal,ImmutableSet.of(Action.ADMIN));
  }
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  namespaceAdmin.create(NamespaceMeta.DEFAULT);
  secureStore=injector.getInstance(SecureStore.class);
  secureStoreManager=injector.getInstance(SecureStoreManager.class);
}","@BeforeClass public static void initialize() throws Exception {
  if (nestedStartCount++ > 0) {
    return;
  }
  File localDataDir=TMP_FOLDER.newFolder();
  cConf=createCConf(localDataDir);
  org.apache.hadoop.conf.Configuration hConf=new org.apache.hadoop.conf.Configuration();
  hConf.addResource(""String_Node_Str"");
  hConf.reloadConfiguration();
  hConf.set(Constants.CFG_LOCAL_DATA_DIR,localDataDir.getAbsolutePath());
  hConf.set(Constants.AppFabric.OUTPUT_DIR,cConf.get(Constants.AppFabric.OUTPUT_DIR));
  hConf.set(""String_Node_Str"",new File(localDataDir,cConf.get(Constants.AppFabric.TEMP_DIR)).getAbsolutePath());
  if (OSDetector.isWindows()) {
    File tmpDir=TMP_FOLDER.newFolder();
    File binDir=new File(tmpDir,""String_Node_Str"");
    Assert.assertTrue(binDir.mkdirs());
    copyTempFile(""String_Node_Str"",tmpDir);
    copyTempFile(""String_Node_Str"",binDir);
    System.setProperty(""String_Node_Str"",tmpDir.getAbsolutePath());
    System.load(new File(tmpDir,""String_Node_Str"").getAbsolutePath());
  }
  Injector injector=Guice.createInjector(createDataFabricModule(),new TransactionExecutorModule(),new DataSetsModules().getStandaloneModules(),new DataSetServiceModules().getInMemoryModules(),new ConfigModule(cConf,hConf),new IOModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getInMemoryModules(),new AppFabricServiceRuntimeModule().getInMemoryModules(),new ServiceStoreModules().getInMemoryModules(),new InMemoryProgramRunnerModule(LocalStreamWriter.class),new AbstractModule(){
    @Override protected void configure(){
      bind(StreamHandler.class).in(Scopes.SINGLETON);
      bind(StreamFetchHandler.class).in(Scopes.SINGLETON);
      bind(StreamViewHttpHandler.class).in(Scopes.SINGLETON);
      bind(StreamFileJanitorService.class).to(LocalStreamFileJanitorService.class).in(Scopes.SINGLETON);
      bind(StreamWriterSizeCollector.class).to(BasicStreamWriterSizeCollector.class).in(Scopes.SINGLETON);
      bind(StreamCoordinatorClient.class).to(InMemoryStreamCoordinatorClient.class).in(Scopes.SINGLETON);
      bind(MetricsManager.class).toProvider(MetricsManagerProvider.class);
    }
  }
,new MetricsHandlerModule(),new MetricsClientRuntimeModule().getInMemoryModules(),new LoggingModules().getInMemoryModules(),new LogReaderRuntimeModules().getInMemoryModules(),new ExploreRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new NotificationFeedServiceRuntimeModule().getInMemoryModules(),new NotificationServiceRuntimeModule().getInMemoryModules(),new NamespaceStoreModule().getStandaloneModules(),new AuthorizationModule(),new AuthorizationEnforcementModule().getInMemoryModules(),new AbstractModule(){
    @Override @SuppressWarnings(""String_Node_Str"") protected void configure(){
      install(new FactoryModuleBuilder().implement(ApplicationManager.class,DefaultApplicationManager.class).build(ApplicationManagerFactory.class));
      install(new FactoryModuleBuilder().implement(ArtifactManager.class,DefaultArtifactManager.class).build(ArtifactManagerFactory.class));
      install(new FactoryModuleBuilder().implement(StreamManager.class,DefaultStreamManager.class).build(StreamManagerFactory.class));
      bind(TemporaryFolder.class).toInstance(TMP_FOLDER);
      bind(AuthorizationHandler.class).in(Scopes.SINGLETON);
    }
  }
);
  txService=injector.getInstance(TransactionManager.class);
  txService.startAndWait();
  dsOpService=injector.getInstance(DatasetOpExecutor.class);
  dsOpService.startAndWait();
  datasetService=injector.getInstance(DatasetService.class);
  datasetService.startAndWait();
  metricsQueryService=injector.getInstance(MetricsQueryService.class);
  metricsQueryService.startAndWait();
  metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  metricsCollectionService.startAndWait();
  schedulerService=injector.getInstance(SchedulerService.class);
  schedulerService.startAndWait();
  if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED)) {
    exploreExecutorService=injector.getInstance(ExploreExecutorService.class);
    exploreExecutorService.startAndWait();
    exploreClient=injector.getInstance(ExploreClient.class);
  }
  streamCoordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  streamCoordinatorClient.startAndWait();
  testManager=injector.getInstance(UnitTestManager.class);
  metricsManager=injector.getInstance(MetricsManager.class);
  authorizerInstantiator=injector.getInstance(AuthorizerInstantiator.class);
  if (cConf.getBoolean(Constants.Security.Authorization.ENABLED)) {
    String user=System.getProperty(""String_Node_Str"");
    SecurityRequestContext.setUserId(user);
    InstanceId instance=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
    Principal principal=new Principal(user,Principal.PrincipalType.USER);
    authorizerInstantiator.get().grant(instance,principal,ImmutableSet.of(Action.ADMIN));
    authorizerInstantiator.get().grant(NamespaceId.DEFAULT,principal,ImmutableSet.of(Action.ADMIN));
  }
  namespaceAdmin=injector.getInstance(NamespaceAdmin.class);
  if (firstInit) {
    namespaceAdmin.create(NamespaceMeta.DEFAULT);
  }
  secureStore=injector.getInstance(SecureStore.class);
  secureStoreManager=injector.getInstance(SecureStoreManager.class);
  firstInit=false;
}"
6104,"/** 
 * Tests the secure storage macro function in a pipelines by creating datasets from the secure store data.
 */
private void testSecureStorePipeline(Engine engine,String prefix) throws Exception {
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MockRuntimeDatasetSource.getPlugin(""String_Node_Str"",""String_Node_Str"" + prefix + ""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockRuntimeDatasetSink.getPlugin(""String_Node_Str"",""String_Node_Str"" + prefix + ""String_Node_Str""))).addConnection(""String_Node_Str"",""String_Node_Str"").setEngine(engine).build();
  getSecureStoreManager().put(""String_Node_Str"",prefix + ""String_Node_Str"",(prefix + ""String_Node_Str"").getBytes(),""String_Node_Str"",new HashMap<String,String>());
  getSecureStoreManager().put(""String_Node_Str"",prefix + ""String_Node_Str"",(prefix + ""String_Node_Str"").getBytes(),""String_Node_Str"",new HashMap<String,String>());
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  Assert.assertNull(getDataset(prefix + ""String_Node_Str"").get());
  Assert.assertNull(getDataset(prefix + ""String_Node_Str"").get());
  workflowManager.start();
  workflowManager.waitForFinish(5,TimeUnit.MINUTES);
  Assert.assertNotNull(getDataset(prefix + ""String_Node_Str"").get());
  Assert.assertNotNull(getDataset(prefix + ""String_Node_Str"").get());
}","/** 
 * Tests the secure storage macro function in a pipelines by creating datasets from the secure store data.
 */
private void testSecureStorePipeline(Engine engine,String prefix) throws Exception {
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MockRuntimeDatasetSource.getPlugin(""String_Node_Str"",""String_Node_Str"" + prefix + ""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockRuntimeDatasetSink.getPlugin(""String_Node_Str"",""String_Node_Str"" + prefix + ""String_Node_Str""))).addConnection(""String_Node_Str"",""String_Node_Str"").setEngine(engine).build();
  getSecureStoreManager().putSecureData(""String_Node_Str"",prefix + ""String_Node_Str"",(prefix + ""String_Node_Str"").getBytes(),""String_Node_Str"",new HashMap<String,String>());
  getSecureStoreManager().putSecureData(""String_Node_Str"",prefix + ""String_Node_Str"",(prefix + ""String_Node_Str"").getBytes(),""String_Node_Str"",new HashMap<String,String>());
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  Assert.assertNull(getDataset(prefix + ""String_Node_Str"").get());
  Assert.assertNull(getDataset(prefix + ""String_Node_Str"").get());
  workflowManager.start();
  workflowManager.waitForFinish(5,TimeUnit.MINUTES);
  Assert.assertNotNull(getDataset(prefix + ""String_Node_Str"").get());
  Assert.assertNotNull(getDataset(prefix + ""String_Node_Str"").get());
}"
6105,"@Test public void testPipelineWithAllActions() throws Exception {
  String actionTable=""String_Node_Str"";
  String action1RowKey=""String_Node_Str"";
  String action1ColumnKey=""String_Node_Str"";
  String action1Value=""String_Node_Str"";
  String action2RowKey=""String_Node_Str"";
  String action2ColumnKey=""String_Node_Str"";
  String action2Value=""String_Node_Str"";
  String action3RowKey=""String_Node_Str"";
  String action3ColumnKey=""String_Node_Str"";
  String action3Value=""String_Node_Str"";
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(actionTable,action1RowKey,action1ColumnKey,action1Value))).addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(actionTable,action2RowKey,action2ColumnKey,action2Value))).addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(actionTable,action3RowKey,action3ColumnKey,action3Value))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").setEngine(Engine.MAPREDUCE).build();
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start();
  workflowManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<Table> actionTableDS=getDataset(actionTable);
  Assert.assertEquals(action1Value,MockAction.readOutput(actionTableDS,action1RowKey,action1ColumnKey));
  Assert.assertEquals(action2Value,MockAction.readOutput(actionTableDS,action2RowKey,action2ColumnKey));
  Assert.assertEquals(action3Value,MockAction.readOutput(actionTableDS,action3RowKey,action3ColumnKey));
}","@Test public void testPipelineWithAllActions() throws Exception {
  String actionTable=""String_Node_Str"";
  String action1RowKey=""String_Node_Str"";
  String action1ColumnKey=""String_Node_Str"";
  String action1Value=""String_Node_Str"";
  String action2RowKey=""String_Node_Str"";
  String action2ColumnKey=""String_Node_Str"";
  String action2Value=""String_Node_Str"";
  String action3RowKey=""String_Node_Str"";
  String action3ColumnKey=""String_Node_Str"";
  String action3Value=""String_Node_Str"";
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(actionTable,action1RowKey,action1ColumnKey,action1Value))).addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(actionTable,action2RowKey,action2ColumnKey,action2Value))).addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(actionTable,action3RowKey,action3ColumnKey,action3Value))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").setEngine(Engine.MAPREDUCE).build();
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start();
  workflowManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<Table> actionTableDS=getDataset(actionTable);
  Assert.assertEquals(action1Value,MockAction.readOutput(actionTableDS,action1RowKey,action1ColumnKey));
  Assert.assertEquals(action2Value,MockAction.readOutput(actionTableDS,action2RowKey,action2ColumnKey));
  Assert.assertEquals(action3Value,MockAction.readOutput(actionTableDS,action3RowKey,action3ColumnKey));
  List<RunRecord> history=workflowManager.getHistory(ProgramRunStatus.COMPLETED);
  Assert.assertEquals(1,history.size());
  String runId=history.get(0).getPid();
  WorkflowTokenDetail tokenDetail=workflowManager.getToken(runId,WorkflowToken.Scope.USER,action1RowKey + action1ColumnKey);
  validateToken(tokenDetail,action1RowKey + action1ColumnKey,action1Value);
  tokenDetail=workflowManager.getToken(runId,WorkflowToken.Scope.USER,action2RowKey + action2ColumnKey);
  validateToken(tokenDetail,action2RowKey + action2ColumnKey,action2Value);
  tokenDetail=workflowManager.getToken(runId,WorkflowToken.Scope.USER,action3RowKey + action3ColumnKey);
  validateToken(tokenDetail,action3RowKey + action3ColumnKey,action3Value);
}"
6106,"public BasicActionContext(CustomActionContext context){
  this.context=context;
}","public BasicActionContext(CustomActionContext context){
  this.context=context;
  this.arguments=new BasicSettableArguments(context.getRuntimeArguments());
}"
6107,"@Override public SettableArguments getArguments(){
  return new BasicSettableArguments(context.getRuntimeArguments());
}","@Override public SettableArguments getArguments(){
  return arguments;
}"
6108,"@Override public void run(){
  checkpoint();
}","@Override public void run(){
  try {
    checkpoint();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
  }
}"
6109,"@Override public void run(){
  try {
    if (writeListMap.isEmpty()) {
      int messages=0;
      long limitKey=System.currentTimeMillis() / eventBucketIntervalMs;
synchronized (messageTable) {
        SortedSet<Long> rowKeySet=messageTable.rowKeySet();
        if (!rowKeySet.isEmpty()) {
          long oldestBucketKey=rowKeySet.first();
          Map<String,Entry<Long,List<KafkaLogEvent>>> row=messageTable.row(oldestBucketKey);
          for (Iterator<Map.Entry<String,Entry<Long,List<KafkaLogEvent>>>> it=row.entrySet().iterator(); it.hasNext(); ) {
            Map.Entry<String,Entry<Long,List<KafkaLogEvent>>> mapEntry=it.next();
            if (limitKey < (mapEntry.getValue().getKey() + maxNumberOfBucketsInTable)) {
              break;
            }
            writeListMap.putAll(mapEntry.getKey(),mapEntry.getValue().getValue());
            messages+=mapEntry.getValue().getValue().size();
            it.remove();
          }
        }
      }
      LOG.trace(""String_Node_Str"",messages);
      for (Iterator<Map.Entry<String,Collection<KafkaLogEvent>>> it=writeListMap.asMap().entrySet().iterator(); it.hasNext(); ) {
        Map.Entry<String,Collection<KafkaLogEvent>> mapEntry=it.next();
        List<KafkaLogEvent> list=(List<KafkaLogEvent>)mapEntry.getValue();
        Collections.sort(list);
        logFileWriter.append(list);
        it.remove();
      }
    }
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
  }
}","@Override public void run(){
  try {
    if (writeListMap.isEmpty()) {
      int messages=0;
      long limitKey=System.currentTimeMillis() / eventBucketIntervalMs;
synchronized (messageTable) {
        SortedSet<Long> rowKeySet=messageTable.rowKeySet();
        if (!rowKeySet.isEmpty()) {
          long oldestBucketKey=rowKeySet.first();
          Map<String,Entry<Long,List<KafkaLogEvent>>> row=messageTable.row(oldestBucketKey);
          for (Iterator<Map.Entry<String,Entry<Long,List<KafkaLogEvent>>>> it=row.entrySet().iterator(); it.hasNext(); ) {
            Map.Entry<String,Entry<Long,List<KafkaLogEvent>>> mapEntry=it.next();
            if (limitKey < (mapEntry.getValue().getKey() + maxNumberOfBucketsInTable)) {
              break;
            }
            writeListMap.putAll(mapEntry.getKey(),mapEntry.getValue().getValue());
            messages+=mapEntry.getValue().getValue().size();
            it.remove();
          }
        }
      }
      LOG.trace(""String_Node_Str"",messages);
    }
    for (Iterator<Map.Entry<String,Collection<KafkaLogEvent>>> it=writeListMap.asMap().entrySet().iterator(); it.hasNext(); ) {
      Map.Entry<String,Collection<KafkaLogEvent>> mapEntry=it.next();
      List<KafkaLogEvent> list=(List<KafkaLogEvent>)mapEntry.getValue();
      Collections.sort(list);
      logFileWriter.append(list);
      it.remove();
    }
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
  }
}"
6110,"private <T>T execute(TransactionExecutor.Function<Table,T> func){
  try {
    Table table=tableUtil.getMetaTable();
    if (table instanceof TransactionAware) {
      TransactionExecutor txExecutor=Transactions.createTransactionExecutor(transactionExecutorFactory,(TransactionAware)table);
      return txExecutor.execute(func,table);
    }
 else {
      throw new RuntimeException(String.format(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",table));
    }
  }
 catch (  Exception e) {
    throw new RuntimeException(String.format(""String_Node_Str"",Constants.Stream.View.STORE_TABLE),e);
  }
}","private <T>T execute(TransactionExecutor.Function<Table,T> func){
  try {
    Table table=tableUtil.getMetaTable();
    if (table instanceof TransactionAware) {
      TransactionExecutor txExecutor=Transactions.createTransactionExecutor(transactionExecutorFactory,(TransactionAware)table);
      return txExecutor.execute(func,table);
    }
 else {
      throw new RuntimeException(String.format(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",table));
    }
  }
 catch (  Exception e) {
    throw new RuntimeException(String.format(""String_Node_Str"",tableUtil.getMetaTableName()),e);
  }
}"
6111,"@Test public void testPipelineWithAllActions() throws Exception {
  String actionTable=""String_Node_Str"";
  String action1RowKey=""String_Node_Str"";
  String action1ColumnKey=""String_Node_Str"";
  String action1Value=""String_Node_Str"";
  String action2RowKey=""String_Node_Str"";
  String action2ColumnKey=""String_Node_Str"";
  String action2Value=""String_Node_Str"";
  String action3RowKey=""String_Node_Str"";
  String action3ColumnKey=""String_Node_Str"";
  String action3Value=""String_Node_Str"";
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(actionTable,action1RowKey,action1ColumnKey,action1Value))).addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(actionTable,action2RowKey,action2ColumnKey,action2Value))).addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(actionTable,action3RowKey,action3ColumnKey,action3Value))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").setEngine(Engine.MAPREDUCE).build();
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start();
  workflowManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<Table> actionTableDS=getDataset(actionTable);
  Assert.assertEquals(action1Value,MockAction.readOutput(actionTableDS,action1RowKey,action1ColumnKey));
  Assert.assertEquals(action2Value,MockAction.readOutput(actionTableDS,action2RowKey,action2ColumnKey));
  Assert.assertEquals(action3Value,MockAction.readOutput(actionTableDS,action3RowKey,action3ColumnKey));
}","@Test public void testPipelineWithAllActions() throws Exception {
  String actionTable=""String_Node_Str"";
  String action1RowKey=""String_Node_Str"";
  String action1ColumnKey=""String_Node_Str"";
  String action1Value=""String_Node_Str"";
  String action2RowKey=""String_Node_Str"";
  String action2ColumnKey=""String_Node_Str"";
  String action2Value=""String_Node_Str"";
  String action3RowKey=""String_Node_Str"";
  String action3ColumnKey=""String_Node_Str"";
  String action3Value=""String_Node_Str"";
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(actionTable,action1RowKey,action1ColumnKey,action1Value))).addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(actionTable,action2RowKey,action2ColumnKey,action2Value))).addStage(new ETLStage(""String_Node_Str"",MockAction.getPlugin(actionTable,action3RowKey,action3ColumnKey,action3Value))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").setEngine(Engine.MAPREDUCE).build();
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start();
  workflowManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<Table> actionTableDS=getDataset(actionTable);
  Assert.assertEquals(action1Value,MockAction.readOutput(actionTableDS,action1RowKey,action1ColumnKey));
  Assert.assertEquals(action2Value,MockAction.readOutput(actionTableDS,action2RowKey,action2ColumnKey));
  Assert.assertEquals(action3Value,MockAction.readOutput(actionTableDS,action3RowKey,action3ColumnKey));
  List<RunRecord> history=workflowManager.getHistory(ProgramRunStatus.COMPLETED);
  Assert.assertEquals(1,history.size());
  String runId=history.get(0).getPid();
  WorkflowTokenDetail tokenDetail=workflowManager.getToken(runId,WorkflowToken.Scope.USER,action1RowKey + action1ColumnKey);
  validateToken(tokenDetail,action1RowKey + action1ColumnKey,action1Value);
  tokenDetail=workflowManager.getToken(runId,WorkflowToken.Scope.USER,action2RowKey + action2ColumnKey);
  validateToken(tokenDetail,action2RowKey + action2ColumnKey,action2Value);
  tokenDetail=workflowManager.getToken(runId,WorkflowToken.Scope.USER,action3RowKey + action3ColumnKey);
  validateToken(tokenDetail,action3RowKey + action3ColumnKey,action3Value);
}"
6112,"public BasicActionContext(CustomActionContext context){
  this.context=context;
}","public BasicActionContext(CustomActionContext context){
  this.context=context;
  this.arguments=new BasicSettableArguments(context.getRuntimeArguments());
}"
6113,"@Override public SettableArguments getArguments(){
  return new BasicSettableArguments(context.getRuntimeArguments());
}","@Override public SettableArguments getArguments(){
  return arguments;
}"
6114,"private PluginProperties substituteMacroFields(Plugin plugin,MacroEvaluator macroEvaluator){
  Map<String,String> properties=new HashMap<>();
  Map<String,PluginPropertyField> pluginPropertyFieldMap=plugin.getPluginClass().getProperties();
  MacroParser macroParser=new MacroParser(macroEvaluator);
  for (  Map.Entry<String,PluginPropertyField> pluginEntry : pluginPropertyFieldMap.entrySet()) {
    if (pluginEntry.getValue().isMacroSupported()) {
      String macroValue=plugin.getProperties().getProperties().get(pluginEntry.getKey());
      properties.put(pluginEntry.getKey(),macroParser.parse(macroValue));
    }
 else {
      properties.put(pluginEntry.getKey(),plugin.getProperties().getProperties().get(pluginEntry.getKey()));
    }
  }
  return PluginProperties.builder().addAll(properties).build();
}","private PluginProperties substituteMacroFields(Plugin plugin,MacroEvaluator macroEvaluator){
  Map<String,String> properties=new HashMap<>();
  Map<String,PluginPropertyField> pluginPropertyFieldMap=plugin.getPluginClass().getProperties();
  MacroParser macroParser=new MacroParser(macroEvaluator);
  for (  Map.Entry<String,String> property : plugin.getProperties().getProperties().entrySet()) {
    PluginPropertyField field=pluginPropertyFieldMap.get(property.getKey());
    if (field != null && field.isMacroSupported()) {
      properties.put(property.getKey(),macroParser.parse(property.getValue()));
    }
 else {
      properties.put(property.getKey(),property.getValue());
    }
  }
  return PluginProperties.builder().addAll(properties).build();
}"
6115,"private Dag(SetMultimap<String,String> outgoingConnections,SetMultimap<String,String> incomingConnections){
  this.outgoingConnections=HashMultimap.create(outgoingConnections);
  this.incomingConnections=HashMultimap.create(incomingConnections);
  this.sources=new HashSet<>();
  this.sinks=new HashSet<>();
  this.nodes=new HashSet<>();
  init();
}","protected Dag(SetMultimap<String,String> outgoingConnections,SetMultimap<String,String> incomingConnections){
  this.outgoingConnections=HashMultimap.create(outgoingConnections);
  this.incomingConnections=HashMultimap.create(incomingConnections);
  this.sources=new HashSet<>();
  this.sinks=new HashSet<>();
  this.nodes=new HashSet<>();
  init();
}"
6116,"private Set<String> traverse(String stage,Set<String> alreadySeen,Set<String> stopNodes){
  if (!alreadySeen.add(stage)) {
    return alreadySeen;
  }
  Collection<String> outputs=outgoingConnections.get(stage);
  if (outputs.isEmpty()) {
    return alreadySeen;
  }
  for (  String output : outputs) {
    if (stopNodes.contains(output)) {
      alreadySeen.add(output);
      continue;
    }
    alreadySeen.addAll(traverse(output,alreadySeen,stopNodes));
  }
  return alreadySeen;
}","private Set<String> traverse(String stage,Set<String> alreadySeen,Set<String> stopNodes,SetMultimap<String,String> connections){
  if (!alreadySeen.add(stage)) {
    return alreadySeen;
  }
  Collection<String> outputs=connections.get(stage);
  if (outputs.isEmpty()) {
    return alreadySeen;
  }
  for (  String output : outputs) {
    if (stopNodes.contains(output)) {
      alreadySeen.add(output);
      continue;
    }
    alreadySeen.addAll(traverse(output,alreadySeen,stopNodes,connections));
  }
  return alreadySeen;
}"
6117,"/** 
 * Return all stages accessible from the specified stage, without going past any node in stopNodes.
 * @param stages the stages to start at
 * @param stopNodes set of nodes to stop traversal on
 * @return all stages accessible from that stage
 */
public Set<String> accessibleFrom(Set<String> stages,Set<String> stopNodes){
  Set<String> accessible=new HashSet<>();
  for (  String stage : stages) {
    accessible.addAll(traverse(stage,accessible,stopNodes));
  }
  return accessible;
}","/** 
 * Return all stages accessible from the starting stages, without going past any node in stopNodes.
 * @param stages the stages to start at
 * @param stopNodes set of nodes to stop traversal on
 * @return all stages accessible from that stage
 */
public Set<String> accessibleFrom(Set<String> stages,Set<String> stopNodes){
  Set<String> accessible=new HashSet<>();
  for (  String stage : stages) {
    accessible.addAll(traverseForwards(stage,accessible,stopNodes));
  }
  return accessible;
}"
6118,"/** 
 * Create an execution plan for the given logical pipeline. This is used for batch pipelines. Though it may eventually be useful to mark windowing points for realtime pipelines. A plan consists of one or more phases, with connections between phases. A connection between a phase indicates control flow, and not necessarily data flow. This class assumes that it receives a valid pipeline spec. That is, the pipeline has no cycles, all its nodes have unique names, sources don't have any input, sinks don't have any output, everything else has both an input and an output, etc. We start by inserting connector nodes into the logical dag, which are used to mark boundaries between mapreduce jobs. Each connector represents a node where we will need to write to a local dataset. Next, the logical pipeline is broken up into phases, using the connectors as sinks in one phase, and a source in another. After this point, connections between phases do not indicate data flow, but control flow.
 * @param spec the pipeline spec, representing a logical pipeline
 * @return the execution plan
 */
public PipelinePlan plan(PipelineSpec spec){
  Set<String> reduceNodes=new HashSet<>();
  Set<String> isolationNodes=new HashSet<>();
  Set<String> actionNodes=new HashSet<>();
  Map<String,StageSpec> specs=new HashMap<>();
  for (  StageSpec stage : spec.getStages()) {
    if (reduceTypes.contains(stage.getPlugin().getType())) {
      reduceNodes.add(stage.getName());
    }
    if (isolationTypes.contains(stage.getPlugin().getType())) {
      isolationNodes.add(stage.getName());
    }
    if (Action.PLUGIN_TYPE.equals(stage.getPlugin().getType())) {
      actionNodes.add(stage.getName());
    }
    specs.put(stage.getName(),stage);
  }
  SetMultimap<String,String> outgoingActionConnections=HashMultimap.create();
  SetMultimap<String,String> incomingActionConnections=HashMultimap.create();
  Set<Connection> connectionsWithoutAction=new HashSet<>();
  for (  Connection connection : spec.getConnections()) {
    if (actionNodes.contains(connection.getFrom()) || actionNodes.contains(connection.getTo())) {
      if (actionNodes.contains(connection.getFrom())) {
        outgoingActionConnections.put(connection.getFrom(),connection.getTo());
      }
      if (actionNodes.contains(connection.getTo())) {
        incomingActionConnections.put(connection.getTo(),connection.getFrom());
      }
      continue;
    }
    connectionsWithoutAction.add(connection);
  }
  ConnectorDag cdag=ConnectorDag.builder().addConnections(connectionsWithoutAction).addReduceNodes(reduceNodes).addIsolationNodes(isolationNodes).build();
  cdag.insertConnectors();
  Set<String> connectorNodes=cdag.getConnectors();
  Map<String,Dag> subdags=new HashMap<>();
  for (  Dag subdag : cdag.splitOnConnectors()) {
    String name=getPhaseName(subdag.getSources(),subdag.getSinks());
    subdags.put(name,subdag);
  }
  Set<Connection> phaseConnections=new HashSet<>();
  for (  Map.Entry<String,Dag> subdagEntry1 : subdags.entrySet()) {
    String dag1Name=subdagEntry1.getKey();
    Dag dag1=subdagEntry1.getValue();
    for (    Map.Entry<String,Dag> subdagEntry2 : subdags.entrySet()) {
      String dag2Name=subdagEntry2.getKey();
      Dag dag2=subdagEntry2.getValue();
      if (dag1Name.equals(dag2Name)) {
        continue;
      }
      if (Sets.intersection(dag1.getSinks(),dag2.getSources()).size() > 0) {
        phaseConnections.add(new Connection(dag1Name,dag2Name));
      }
    }
  }
  Map<String,PipelinePhase> phases=new HashMap<>();
  for (  Map.Entry<String,Dag> dagEntry : subdags.entrySet()) {
    phases.put(dagEntry.getKey(),dagToPipeline(dagEntry.getValue(),connectorNodes,specs));
  }
  populateActionPhases(specs,actionNodes,phases,phaseConnections,outgoingActionConnections,incomingActionConnections,subdags);
  return new PipelinePlan(phases,phaseConnections);
}","/** 
 * Create an execution plan for the given logical pipeline. This is used for batch pipelines. Though it may eventually be useful to mark windowing points for realtime pipelines. A plan consists of one or more phases, with connections between phases. A connection between a phase indicates control flow, and not necessarily data flow. This class assumes that it receives a valid pipeline spec. That is, the pipeline has no cycles, all its nodes have unique names, sources don't have any input, sinks don't have any output, everything else has both an input and an output, etc. We start by inserting connector nodes into the logical dag, which are used to mark boundaries between mapreduce jobs. Each connector represents a node where we will need to write to a local dataset. Next, the logical pipeline is broken up into phases, using the connectors as sinks in one phase, and a source in another. After this point, connections between phases do not indicate data flow, but control flow.
 * @param spec the pipeline spec, representing a logical pipeline
 * @return the execution plan
 */
public PipelinePlan plan(PipelineSpec spec){
  Set<String> reduceNodes=new HashSet<>();
  Set<String> isolationNodes=new HashSet<>();
  Set<String> actionNodes=new HashSet<>();
  Map<String,StageSpec> specs=new HashMap<>();
  for (  StageSpec stage : spec.getStages()) {
    if (reduceTypes.contains(stage.getPlugin().getType())) {
      reduceNodes.add(stage.getName());
    }
    if (isolationTypes.contains(stage.getPlugin().getType())) {
      isolationNodes.add(stage.getName());
    }
    if (Action.PLUGIN_TYPE.equals(stage.getPlugin().getType())) {
      actionNodes.add(stage.getName());
    }
    specs.put(stage.getName(),stage);
  }
  SetMultimap<String,String> outgoingActionConnections=HashMultimap.create();
  SetMultimap<String,String> incomingActionConnections=HashMultimap.create();
  Set<Connection> connectionsWithoutAction=new HashSet<>();
  for (  Connection connection : spec.getConnections()) {
    if (actionNodes.contains(connection.getFrom()) || actionNodes.contains(connection.getTo())) {
      if (actionNodes.contains(connection.getFrom())) {
        outgoingActionConnections.put(connection.getFrom(),connection.getTo());
      }
      if (actionNodes.contains(connection.getTo())) {
        incomingActionConnections.put(connection.getTo(),connection.getFrom());
      }
      continue;
    }
    connectionsWithoutAction.add(connection);
  }
  ConnectorDag cdag=ConnectorDag.builder().addConnections(connectionsWithoutAction).addReduceNodes(reduceNodes).addIsolationNodes(isolationNodes).build();
  cdag.insertConnectors();
  Set<String> connectorNodes=cdag.getConnectors();
  Map<String,Dag> subdags=new HashMap<>();
  for (  Dag subdag : cdag.split()) {
    String name=getPhaseName(subdag.getSources(),subdag.getSinks());
    subdags.put(name,subdag);
  }
  Set<Connection> phaseConnections=new HashSet<>();
  for (  Map.Entry<String,Dag> subdagEntry1 : subdags.entrySet()) {
    String dag1Name=subdagEntry1.getKey();
    Dag dag1=subdagEntry1.getValue();
    for (    Map.Entry<String,Dag> subdagEntry2 : subdags.entrySet()) {
      String dag2Name=subdagEntry2.getKey();
      Dag dag2=subdagEntry2.getValue();
      if (dag1Name.equals(dag2Name)) {
        continue;
      }
      if (Sets.intersection(dag1.getSinks(),dag2.getSources()).size() > 0) {
        phaseConnections.add(new Connection(dag1Name,dag2Name));
      }
    }
  }
  Map<String,PipelinePhase> phases=new HashMap<>();
  for (  Map.Entry<String,Dag> dagEntry : subdags.entrySet()) {
    phases.put(dagEntry.getKey(),dagToPipeline(dagEntry.getValue(),connectorNodes,specs));
  }
  populateActionPhases(specs,actionNodes,phases,phaseConnections,outgoingActionConnections,incomingActionConnections,subdags);
  return new PipelinePlan(phases,phaseConnections);
}"
6119,"@Test public void testSplitDag(){
  ConnectorDag cdag=ConnectorDag.builder().addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addReduceNodes(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build();
  cdag.insertConnectors();
  Set<Dag> actual=new HashSet<>(cdag.splitOnConnectors());
  Dag dag1=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Dag dag2=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Dag dag3=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Dag dag4=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Dag dag5=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Dag dag6=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Set<Dag> expected=ImmutableSet.of(dag1,dag2,dag3,dag4,dag5,dag6);
  Assert.assertEquals(expected,actual);
  cdag=ConnectorDag.builder().addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addReduceNodes(""String_Node_Str"",""String_Node_Str"").build();
  cdag.insertConnectors();
  actual=new HashSet<>(cdag.splitOnConnectors());
  dag1=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  dag2=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  dag3=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  expected=ImmutableSet.of(dag1,dag2,dag3);
  Assert.assertEquals(expected,actual);
}","@Test public void testSplitDag(){
  ConnectorDag cdag=ConnectorDag.builder().addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addReduceNodes(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"").build();
  cdag.insertConnectors();
  Set<Dag> actual=new HashSet<>(cdag.split());
  Dag dag1=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Dag dag2=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Dag dag3=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Dag dag4=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Dag dag5=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Dag dag6=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  Set<Dag> expected=ImmutableSet.of(dag1,dag2,dag3,dag4,dag5,dag6);
  Assert.assertEquals(expected,actual);
  cdag=ConnectorDag.builder().addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addReduceNodes(""String_Node_Str"",""String_Node_Str"").build();
  cdag.insertConnectors();
  actual=new HashSet<>(cdag.split());
  dag1=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  dag2=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  dag3=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  expected=ImmutableSet.of(dag1,dag2,dag3);
  Assert.assertEquals(expected,actual);
  cdag=ConnectorDag.builder().addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addReduceNodes(""String_Node_Str"",""String_Node_Str"").build();
  cdag.insertConnectors();
  actual=new HashSet<>(cdag.split());
  dag1=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  dag2=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str"")));
  dag3=new Dag(ImmutableSet.of(new Connection(""String_Node_Str"",""String_Node_Str""),new Connection(""String_Node_Str"",""String_Node_Str"")));
  expected=ImmutableSet.of(dag1,dag2,dag3);
  Assert.assertEquals(expected,actual);
}"
6120,"/** 
 * Split this dag into multiple dags. Each subdag will contain at most a single reduce node.
 * @return list of subdags.
 */
public List<Dag> split(){
  List<Dag> dags=new ArrayList<>();
  Set<String> remainingNodes=new HashSet<>();
  remainingNodes.addAll(nodes);
  Set<String> possibleNewSources=Sets.union(sources,connectors);
  Set<String> possibleNewSinks=Sets.union(sinks,connectors);
  for (  String reduceNode : reduceNodes) {
    Dag subdag=subsetAround(reduceNode,possibleNewSources,possibleNewSinks);
    remainingNodes.removeAll(subdag.getNodes());
    dags.add(subdag);
  }
  Set<String> remainingSources=Sets.intersection(remainingNodes,possibleNewSources);
  dags.add(subsetFrom(remainingSources,possibleNewSinks));
  return dags;
}","/** 
 * Split this dag into multiple dags. Each subdag will contain at most a single reduce node.
 * @return list of subdags.
 */
public List<Dag> split(){
  List<Dag> dags=new ArrayList<>();
  Set<String> remainingNodes=new HashSet<>();
  remainingNodes.addAll(nodes);
  Set<String> possibleNewSources=Sets.union(sources,connectors);
  Set<String> possibleNewSinks=Sets.union(sinks,connectors);
  for (  String reduceNode : reduceNodes) {
    Dag subdag=subsetAround(reduceNode,possibleNewSources,possibleNewSinks);
    remainingNodes.removeAll(subdag.getNodes());
    dags.add(subdag);
  }
  Set<String> remainingSources=Sets.intersection(remainingNodes,possibleNewSources);
  if (!remainingSources.isEmpty()) {
    dags.add(subsetFrom(remainingSources,possibleNewSinks));
  }
  return dags;
}"
6121,"@Override public void drop(Id.DatasetInstance datasetInstanceId,DatasetTypeMeta typeMeta,DatasetSpecification spec) throws Exception {
  InternalDatasetDropParams dropParams=new InternalDatasetDropParams(typeMeta,spec);
  HttpRequest request=HttpRequest.post(resolve(datasetInstanceId,""String_Node_Str"")).withBody(GSON.toJson(dropParams)).build();
  HttpResponse response=HttpRequests.execute(request,httpRequestConfig);
  verifyResponse(response);
}","@Override public void drop(Id.DatasetInstance datasetInstanceId,DatasetTypeMeta typeMeta,DatasetSpecification spec) throws Exception {
  InternalDatasetDropParams dropParams=new InternalDatasetDropParams(typeMeta,spec);
  HttpRequest request=HttpRequest.post(resolve(datasetInstanceId,""String_Node_Str"")).withBody(GSON.toJson(dropParams)).build();
  HttpResponse response=HttpRequests.execute(request);
  verifyResponse(response);
}"
6122,"@Override public DatasetSpecification update(Id.DatasetInstance datasetInstanceId,DatasetTypeMeta typeMeta,DatasetProperties props,DatasetSpecification existing) throws Exception {
  InternalDatasetCreationParams updateParams=new InternalDatasetUpdateParams(typeMeta,existing,props);
  HttpRequest request=HttpRequest.post(resolve(datasetInstanceId,""String_Node_Str"")).withBody(GSON.toJson(updateParams)).build();
  HttpResponse response=HttpRequests.execute(request,httpRequestConfig);
  verifyResponse(response);
  return ObjectResponse.fromJsonBody(response,DatasetSpecification.class).getResponseObject();
}","@Override public DatasetSpecification update(Id.DatasetInstance datasetInstanceId,DatasetTypeMeta typeMeta,DatasetProperties props,DatasetSpecification existing) throws Exception {
  InternalDatasetCreationParams updateParams=new InternalDatasetUpdateParams(typeMeta,existing,props);
  HttpRequest request=HttpRequest.post(resolve(datasetInstanceId,""String_Node_Str"")).withBody(GSON.toJson(updateParams)).build();
  HttpResponse response=HttpRequests.execute(request);
  verifyResponse(response);
  return ObjectResponse.fromJsonBody(response,DatasetSpecification.class).getResponseObject();
}"
6123,"@Override public DatasetSpecification create(Id.DatasetInstance datasetInstanceId,DatasetTypeMeta typeMeta,DatasetProperties props) throws Exception {
  InternalDatasetCreationParams creationParams=new InternalDatasetCreationParams(typeMeta,props);
  HttpRequest request=HttpRequest.post(resolve(datasetInstanceId,""String_Node_Str"")).withBody(GSON.toJson(creationParams)).build();
  HttpResponse response=HttpRequests.execute(request,httpRequestConfig);
  verifyResponse(response);
  return ObjectResponse.fromJsonBody(response,DatasetSpecification.class).getResponseObject();
}","@Override public DatasetSpecification create(Id.DatasetInstance datasetInstanceId,DatasetTypeMeta typeMeta,DatasetProperties props) throws Exception {
  InternalDatasetCreationParams creationParams=new InternalDatasetCreationParams(typeMeta,props);
  HttpRequest request=HttpRequest.post(resolve(datasetInstanceId,""String_Node_Str"")).withBody(GSON.toJson(creationParams)).build();
  HttpResponse response=HttpRequests.execute(request);
  verifyResponse(response);
  return ObjectResponse.fromJsonBody(response,DatasetSpecification.class).getResponseObject();
}"
6124,"@Inject public RemoteDatasetOpExecutor(CConfiguration cConf,final DiscoveryServiceClient discoveryClient){
  this.cConf=cConf;
  this.endpointStrategySupplier=Suppliers.memoize(new Supplier<EndpointStrategy>(){
    @Override public EndpointStrategy get(){
      return new RandomEndpointStrategy(discoveryClient.discover(Constants.Service.DATASET_EXECUTOR));
    }
  }
);
  int httpTimeoutMs=cConf.getInt(Constants.HTTP_CLIENT_TIMEOUT_MS);
  this.httpRequestConfig=new HttpRequestConfig(httpTimeoutMs,httpTimeoutMs);
}","@Inject public RemoteDatasetOpExecutor(CConfiguration cConf,final DiscoveryServiceClient discoveryClient){
  this.cConf=cConf;
  this.endpointStrategySupplier=Suppliers.memoize(new Supplier<EndpointStrategy>(){
    @Override public EndpointStrategy get(){
      return new RandomEndpointStrategy(discoveryClient.discover(Constants.Service.DATASET_EXECUTOR));
    }
  }
);
}"
6125,"private DatasetAdminOpResponse executeAdminOp(Id.DatasetInstance datasetInstanceId,String opName) throws IOException, HandlerException, ConflictException {
  HttpResponse httpResponse=HttpRequests.execute(HttpRequest.post(resolve(datasetInstanceId,opName)).build(),httpRequestConfig);
  verifyResponse(httpResponse);
  return GSON.fromJson(new String(httpResponse.getResponseBody()),DatasetAdminOpResponse.class);
}","private DatasetAdminOpResponse executeAdminOp(Id.DatasetInstance datasetInstanceId,String opName) throws IOException, HandlerException, ConflictException {
  HttpResponse httpResponse=HttpRequests.execute(HttpRequest.post(resolve(datasetInstanceId,opName)).build());
  verifyResponse(httpResponse);
  return GSON.fromJson(new String(httpResponse.getResponseBody()),DatasetAdminOpResponse.class);
}"
6126,"@Override protected void startUp() throws Exception {
  scheduler=Executors.newSingleThreadScheduledExecutor(Threads.createDaemonThreadFactory(""String_Node_Str""));
  long retention=cConf.getLong(Constants.Metrics.RETENTION_SECONDS + ""String_Node_Str"",Constants.Metrics.DEFAULT_RETENTION_HOURS);
  scheduler.schedule(createCleanupTask(retention),1,TimeUnit.SECONDS);
}","@Override protected void startUp() throws Exception {
  scheduler=Executors.newSingleThreadScheduledExecutor(Threads.createDaemonThreadFactory(""String_Node_Str""));
  long retentionSecs=cConf.getLong(Constants.Metrics.RETENTION_SECONDS + ""String_Node_Str"",TimeUnit.HOURS.toSeconds(Constants.Metrics.DEFAULT_RETENTION_HOURS));
  scheduler.schedule(createCleanupTask(retentionSecs),1,TimeUnit.SECONDS);
}"
6127,"/** 
 * Notifies the given job execution completed.
 * @param jobId the unique id that identifies the job.
 * @param succeeded {@code true} if the job execution completed successfully.
 */
void jobEnded(Integer jobId,boolean succeeded) throws TransactionFailureException {
  JobTransaction jobTransaction=jobTransactions.remove(jobId);
  if (jobTransaction == null) {
    LOG.error(""String_Node_Str"",jobId);
    return;
  }
  stageToJob.keySet().removeAll(jobTransaction.getStageIds());
  jobTransaction.completed(succeeded);
}","/** 
 * Notifies the given job execution completed.
 * @param jobId the unique id that identifies the job.
 * @param succeeded {@code true} if the job execution completed successfully.
 */
void jobEnded(Integer jobId,boolean succeeded) throws TransactionFailureException {
  JobTransaction jobTransaction=jobTransactions.remove(jobId);
  if (jobTransaction == null) {
    LOG.error(""String_Node_Str"",jobId);
    return;
  }
  LOG.debug(""String_Node_Str"",jobTransaction);
  stageToJob.keySet().removeAll(jobTransaction.getStageIds());
  jobTransaction.completed(succeeded);
}"
6128,"private TransactionalDatasetContext(Transaction transaction,DynamicDatasetCache datasetCache,boolean asyncCommit){
  this.transaction=transaction;
  this.datasetCache=datasetCache;
  this.datasets=Collections.synchronizedSet(new HashSet<Dataset>());
  this.discardDatasets=Collections.synchronizedSet(new HashSet<Dataset>());
  this.completion=asyncCommit ? new CountDownLatch(1) : null;
}","private TransactionalDatasetContext(Transaction transaction,DynamicDatasetCache datasetCache,TransactionType transactionType){
  this.transaction=transaction;
  this.datasetCache=datasetCache;
  this.datasets=Collections.synchronizedSet(new HashSet<Dataset>());
  this.discardDatasets=Collections.synchronizedSet(new HashSet<Dataset>());
  this.transactionType=transactionType;
  this.completion=new CountDownLatch(1);
}"
6129,"@Override public boolean commitOnJobEnded(){
  return completion != null;
}","@Override public boolean commitOnJobEnded(){
  return transactionType == TransactionType.IMPLICIT_COMMIT_ON_JOB_END;
}"
6130,"/** 
 * Executes the given   {@link TxRunnable} with a long {@link Transaction}. All Spark RDD operations performed inside the given   {@link TxRunnable} will be using the same {@link Transaction}.
 * @param runnable the runnable to be executed in the transaction
 * @throws TransactionFailureException if there is failure during execution. The actual cause of the failuremaybe wrapped inside the  {@link TransactionFailureException} (bothuser exception from the  {@link TxRunnable#run(DatasetContext)} methodor transaction exception from Tephra).
 */
@Override public void execute(TxRunnable runnable) throws TransactionFailureException {
  executeLong(wrap(runnable),false);
}","/** 
 * Executes the given runnable with transactionally. If there is an opened transaction that can be used, then the runnable will be executed with that existing transaction. Otherwise, a new long transaction will be created to exeucte the given runnable.
 * @param runnable The {@link TxRunnable} to be executed inside a transaction
 * @param transactionType The {@link TransactionType} of the Spark transaction.
 */
void execute(SparkTxRunnable runnable,TransactionType transactionType) throws TransactionFailureException {
  TransactionalDatasetContext txDatasetContext=activeDatasetContext.get();
  boolean needCommit=false;
  if (txDatasetContext != null) {
    TransactionType currentTransactionType=txDatasetContext.getTransactionType();
    if (currentTransactionType == TransactionType.EXPLICIT && transactionType == TransactionType.EXPLICIT) {
      throw new TransactionFailureException(""String_Node_Str"" + txDatasetContext.getTransaction());
    }
    if (currentTransactionType == TransactionType.IMPLICIT_COMMIT_ON_JOB_END) {
      if (txDatasetContext.isJobStarted()) {
        try {
          txDatasetContext.awaitCompletion();
          txDatasetContext=null;
        }
 catch (        InterruptedException e) {
          Thread.currentThread().interrupt();
          return;
        }
      }
 else       if (transactionType != TransactionType.IMPLICIT_COMMIT_ON_JOB_END) {
        txDatasetContext.setTransactionType(transactionType);
        needCommit=true;
      }
    }
  }
  if (txDatasetContext == null) {
    txDatasetContext=new TransactionalDatasetContext(txClient.startLong(),datasetCache,transactionType);
    activeDatasetContext.set(txDatasetContext);
    needCommit=transactionType != TransactionType.IMPLICIT_COMMIT_ON_JOB_END;
  }
  Transaction transaction=txDatasetContext.getTransaction();
  try {
    runnable.run(txDatasetContext);
    txDatasetContext.flush();
    if (needCommit) {
      if (!txClient.commit(transaction)) {
        throw new TransactionFailureException(""String_Node_Str"" + transaction);
      }
      activeDatasetContext.remove();
      txDatasetContext.postCommit();
      txDatasetContext.discardDatasets();
    }
  }
 catch (  Throwable t) {
    activeDatasetContext.remove();
    Transactions.invalidateQuietly(txClient,transaction);
    throw Transactions.asTransactionFailure(t);
  }
}"
6131,void close();,@Override void close();
6132,"/** 
 * Closes the scanner and releases any resources.
 */
void close();","/** 
 * Closes the scanner and releases any resources.
 */
@Override void close();"
6133,"/** 
 * Gets the corresponding aggregation for a given aggregation type, field name, sourceID, and time interval
 */
@Path(""String_Node_Str"") @GET public void basicAggregationGetter(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") String fieldName,@PathParam(""String_Node_Str"") String aggregationType,@PathParam(""String_Node_Str"") String sourceID,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long startTimestamp,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long endTimestamp) throws IOException {
  ValuesRowKey valuesRowKeyStart=new ValuesRowKey(startTimestamp,fieldName,sourceID);
  ValuesRowKey valuesRowKeyEnd=new ValuesRowKey(endTimestamp + 1,fieldName,sourceID);
  List<TimestampValue> timestampValueList=new ArrayList<>();
  try {
    Class<?> aggregationClass=Class.forName(""String_Node_Str"" + aggregationType);
    BasicAggregationFunction aggregationClassInstance=(BasicAggregationFunction)aggregationClass.newInstance();
    Scanner scanner=dataStore.scan(valuesRowKeyStart.getTableRowKey(),valuesRowKeyEnd.getTableRowKey());
    Row row;
    byte[] aggregationTypeBytes=Bytes.toBytes(aggregationType);
    try {
      while ((row=scanner.next()) != null) {
        byte[] rowBytes=row.getRow();
        Long timestamp=Bytes.toLong(rowBytes,rowBytes.length - Bytes.SIZEOF_LONG);
        Map<byte[],byte[]> columnsMapBytes=row.getColumns();
        byte[] output=columnsMapBytes.get(aggregationTypeBytes);
        if (output != null) {
          Object deserializedOutput=aggregationClassInstance.deserialize(output);
          TimestampValue tsValue=new TimestampValue(timestamp,deserializedOutput);
          timestampValueList.add(tsValue);
        }
      }
    }
  finally {
      scanner.close();
    }
    if (timestampValueList.isEmpty()) {
      responder.sendString(HttpURLConnection.HTTP_NOT_FOUND,""String_Node_Str"",Charsets.UTF_8);
    }
 else {
      responder.sendJson(HttpURLConnection.HTTP_OK,timestampValueList);
    }
  }
 catch (  ClassNotFoundException|InstantiationException|IllegalAccessException e) {
    responder.sendString(HttpURLConnection.HTTP_NOT_FOUND,String.format(""String_Node_Str"",aggregationType,sourceID,fieldName),Charsets.UTF_8);
  }
catch (  ClassCastException e) {
    responder.sendString(HttpURLConnection.HTTP_BAD_REQUEST,""String_Node_Str"",Charsets.UTF_8);
  }
}","/** 
 * Gets the corresponding aggregation for a given aggregation type, field name, sourceID, and time interval
 */
@Path(""String_Node_Str"") @GET public void basicAggregationGetter(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") String fieldName,@PathParam(""String_Node_Str"") String aggregationType,@PathParam(""String_Node_Str"") String sourceID,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long startTimestamp,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long endTimestamp) throws IOException {
  ValuesRowKey valuesRowKeyStart=new ValuesRowKey(startTimestamp,fieldName,sourceID);
  ValuesRowKey valuesRowKeyEnd=new ValuesRowKey(endTimestamp + 1,fieldName,sourceID);
  List<TimestampValue> timestampValueList=new ArrayList<>();
  try {
    Class<?> aggregationClass=Class.forName(""String_Node_Str"" + aggregationType);
    BasicAggregationFunction aggregationClassInstance=(BasicAggregationFunction)aggregationClass.newInstance();
    Row row;
    byte[] aggregationTypeBytes=Bytes.toBytes(aggregationType);
    try (Scanner scanner=dataStore.scan(valuesRowKeyStart.getTableRowKey(),valuesRowKeyEnd.getTableRowKey())){
      while ((row=scanner.next()) != null) {
        byte[] rowBytes=row.getRow();
        Long timestamp=Bytes.toLong(rowBytes,rowBytes.length - Bytes.SIZEOF_LONG);
        Map<byte[],byte[]> columnsMapBytes=row.getColumns();
        byte[] output=columnsMapBytes.get(aggregationTypeBytes);
        if (output != null) {
          Object deserializedOutput=aggregationClassInstance.deserialize(output);
          TimestampValue tsValue=new TimestampValue(timestamp,deserializedOutput);
          timestampValueList.add(tsValue);
        }
      }
    }
     if (timestampValueList.isEmpty()) {
      responder.sendString(HttpURLConnection.HTTP_NOT_FOUND,""String_Node_Str"",Charsets.UTF_8);
    }
 else {
      responder.sendJson(HttpURLConnection.HTTP_OK,timestampValueList);
    }
  }
 catch (  ClassNotFoundException|InstantiationException|IllegalAccessException e) {
    responder.sendString(HttpURLConnection.HTTP_NOT_FOUND,String.format(""String_Node_Str"",aggregationType,sourceID,fieldName),Charsets.UTF_8);
  }
catch (  ClassCastException e) {
    responder.sendString(HttpURLConnection.HTTP_BAD_REQUEST,""String_Node_Str"",Charsets.UTF_8);
  }
}"
6134,"/** 
 * Gets the corresponding aggregation for a given aggregation type, field name, sourceID, and time interval
 */
@Path(""String_Node_Str"") @GET public void combinableAggregationGetter(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") String fieldName,@PathParam(""String_Node_Str"") String aggregationType,@PathParam(""String_Node_Str"") String sourceID,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long startTimestamp,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long endTimestamp) throws IOException {
  ValuesRowKey valuesRowKeyStart=new ValuesRowKey(startTimestamp,fieldName,sourceID);
  ValuesRowKey valuesRowKeyEnd=new ValuesRowKey(endTimestamp + 1,fieldName,sourceID);
  try {
    Class<?> aggregationClass=Class.forName(""String_Node_Str"" + aggregationType);
    CombinableAggregationFunction aggregationClassInstance=(CombinableAggregationFunction)aggregationClass.newInstance();
    Scanner scanner=dataStore.scan(valuesRowKeyStart.getTableRowKey(),valuesRowKeyEnd.getTableRowKey());
    Row row;
    byte[] aggregationTypeBytes=Bytes.toBytes(aggregationType);
    try {
      while ((row=scanner.next()) != null) {
        Map<byte[],byte[]> columnsMapBytes=row.getColumns();
        byte[] output=columnsMapBytes.get(aggregationTypeBytes);
        if (output != null) {
          aggregationClassInstance.combine(output);
        }
      }
    }
  finally {
      scanner.close();
    }
    Object output=aggregationClassInstance.retrieveAggregation();
    if (output == null) {
      responder.sendString(HttpURLConnection.HTTP_NOT_FOUND,""String_Node_Str"",Charsets.UTF_8);
    }
 else {
      responder.sendJson(HttpURLConnection.HTTP_OK,output);
    }
  }
 catch (  ClassNotFoundException|InstantiationException|IllegalAccessException e) {
    responder.sendString(HttpURLConnection.HTTP_NOT_FOUND,String.format(""String_Node_Str"",sourceID,fieldName),Charsets.UTF_8);
  }
catch (  ClassCastException e) {
    responder.sendString(HttpURLConnection.HTTP_BAD_REQUEST,""String_Node_Str"",Charsets.UTF_8);
  }
}","/** 
 * Gets the corresponding aggregation for a given aggregation type, field name, sourceID, and time interval
 */
@Path(""String_Node_Str"") @GET public void combinableAggregationGetter(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") String fieldName,@PathParam(""String_Node_Str"") String aggregationType,@PathParam(""String_Node_Str"") String sourceID,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long startTimestamp,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long endTimestamp) throws IOException {
  ValuesRowKey valuesRowKeyStart=new ValuesRowKey(startTimestamp,fieldName,sourceID);
  ValuesRowKey valuesRowKeyEnd=new ValuesRowKey(endTimestamp + 1,fieldName,sourceID);
  try {
    Class<?> aggregationClass=Class.forName(""String_Node_Str"" + aggregationType);
    CombinableAggregationFunction aggregationClassInstance=(CombinableAggregationFunction)aggregationClass.newInstance();
    Row row;
    byte[] aggregationTypeBytes=Bytes.toBytes(aggregationType);
    try (Scanner scanner=dataStore.scan(valuesRowKeyStart.getTableRowKey(),valuesRowKeyEnd.getTableRowKey())){
      while ((row=scanner.next()) != null) {
        Map<byte[],byte[]> columnsMapBytes=row.getColumns();
        byte[] output=columnsMapBytes.get(aggregationTypeBytes);
        if (output != null) {
          aggregationClassInstance.combine(output);
        }
      }
    }
     Object output=aggregationClassInstance.retrieveAggregation();
    if (output == null) {
      responder.sendString(HttpURLConnection.HTTP_NOT_FOUND,""String_Node_Str"",Charsets.UTF_8);
    }
 else {
      responder.sendJson(HttpURLConnection.HTTP_OK,output);
    }
  }
 catch (  ClassNotFoundException|InstantiationException|IllegalAccessException e) {
    responder.sendString(HttpURLConnection.HTTP_NOT_FOUND,String.format(""String_Node_Str"",sourceID,fieldName),Charsets.UTF_8);
  }
catch (  ClassCastException e) {
    responder.sendString(HttpURLConnection.HTTP_BAD_REQUEST,""String_Node_Str"",Charsets.UTF_8);
  }
}"
6135,"/** 
 * Gets the aggregation functions that are queryable for a given time range, sourceID, and field name
 */
@Path(""String_Node_Str"") @GET public void aggregationTypesGetter(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") String fieldName,@PathParam(""String_Node_Str"") String sourceID,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long startTimestamp,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long endTimestamp) throws IOException {
  AggregationsRowKey aggregationsRowKeyStart=new AggregationsRowKey(startTimestamp,sourceID);
  AggregationsRowKey aggregationsRowKeyEnd=new AggregationsRowKey(endTimestamp + 1,sourceID);
  Scanner scanner=dataStore.scan(aggregationsRowKeyStart.getTableRowKey(),aggregationsRowKeyEnd.getTableRowKey());
  Row row;
  byte[] fieldNameBytes=Bytes.toBytes(fieldName);
  Set<AggregationTypeValue> commonAggregationTypeValues=new HashSet<>();
  try {
    while ((row=scanner.next()) != null) {
      Map<byte[],byte[]> columnsMapBytes=row.getColumns();
      byte[] output=columnsMapBytes.get(fieldNameBytes);
      String outputString=Bytes.toString(output);
      Set<AggregationTypeValue> aggregationTypeValuesSet=GSON.fromJson(outputString,TOKEN_TYPE_SET_AGGREGATION_TYPE_VALUES);
      commonAggregationTypeValues.addAll(aggregationTypeValuesSet);
    }
  }
  finally {
    scanner.close();
  }
  if (commonAggregationTypeValues.isEmpty()) {
    responder.sendString(HttpURLConnection.HTTP_NOT_FOUND,String.format(""String_Node_Str"",sourceID,fieldName),Charsets.UTF_8);
  }
 else {
    responder.sendJson(HttpURLConnection.HTTP_OK,commonAggregationTypeValues);
  }
}","/** 
 * Gets the aggregation functions that are queryable for a given time range, sourceID, and field name
 */
@Path(""String_Node_Str"") @GET public void aggregationTypesGetter(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") String fieldName,@PathParam(""String_Node_Str"") String sourceID,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long startTimestamp,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long endTimestamp) throws IOException {
  AggregationsRowKey aggregationsRowKeyStart=new AggregationsRowKey(startTimestamp,sourceID);
  AggregationsRowKey aggregationsRowKeyEnd=new AggregationsRowKey(endTimestamp + 1,sourceID);
  Row row;
  byte[] fieldNameBytes=Bytes.toBytes(fieldName);
  Set<AggregationTypeValue> commonAggregationTypeValues=new HashSet<>();
  try (Scanner scanner=dataStore.scan(aggregationsRowKeyStart.getTableRowKey(),aggregationsRowKeyEnd.getTableRowKey())){
    while ((row=scanner.next()) != null) {
      Map<byte[],byte[]> columnsMapBytes=row.getColumns();
      byte[] output=columnsMapBytes.get(fieldNameBytes);
      String outputString=Bytes.toString(output);
      Set<AggregationTypeValue> aggregationTypeValuesSet=GSON.fromJson(outputString,TOKEN_TYPE_SET_AGGREGATION_TYPE_VALUES);
      commonAggregationTypeValues.addAll(aggregationTypeValuesSet);
    }
  }
   if (commonAggregationTypeValues.isEmpty()) {
    responder.sendString(HttpURLConnection.HTTP_NOT_FOUND,String.format(""String_Node_Str"",sourceID,fieldName),Charsets.UTF_8);
  }
 else {
    responder.sendJson(HttpURLConnection.HTTP_OK,commonAggregationTypeValues);
  }
}"
6136,"/** 
 * Gets the fields that are queryable for a given time range and sourceID for combinable aggregation functions
 */
@Path(""String_Node_Str"") @GET public void fieldsGetter(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") String sourceID,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long startTimestamp,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long endTimestamp) throws IOException {
  AggregationsRowKey aggregationsRowKeyStart=new AggregationsRowKey(startTimestamp,sourceID);
  AggregationsRowKey aggregationsRowKeyEnd=new AggregationsRowKey(endTimestamp + 1,sourceID);
  Scanner scanner=dataStore.scan(aggregationsRowKeyStart.getTableRowKey(),aggregationsRowKeyEnd.getTableRowKey());
  Row row;
  Map<String,FieldDetail> fieldDetailMap=new HashMap<>();
  try {
    while ((row=scanner.next()) != null) {
      Map<byte[],byte[]> columnsMapBytes=row.getColumns();
      List<FieldDetail> timestampSpecificFieldDetailList=new ArrayList<>();
      for (      Map.Entry<byte[],byte[]> columnMapEntry : columnsMapBytes.entrySet()) {
        String fieldName=Bytes.toString(columnMapEntry.getKey());
        byte[] output=columnMapEntry.getValue();
        String outputString=Bytes.toString(output);
        Set<AggregationTypeValue> aggregationTypeValuesSet=GSON.fromJson(outputString,TOKEN_TYPE_SET_AGGREGATION_TYPE_VALUES);
        FieldDetail fieldDetail=new FieldDetail(fieldName,aggregationTypeValuesSet);
        timestampSpecificFieldDetailList.add(fieldDetail);
      }
      for (      FieldDetail fdTimestampSpecific : timestampSpecificFieldDetailList) {
        String fdTimestampSpecificFieldName=fdTimestampSpecific.getFieldName();
        if (fieldDetailMap.containsKey(fdTimestampSpecificFieldName)) {
          FieldDetail fdCombined=fieldDetailMap.get(fdTimestampSpecificFieldName);
          fdCombined.addAggregations(fdTimestampSpecific.getAggregationTypeSet());
        }
 else {
          fieldDetailMap.put(fdTimestampSpecificFieldName,fdTimestampSpecific);
        }
      }
    }
  }
  finally {
    scanner.close();
  }
  if (fieldDetailMap.isEmpty()) {
    responder.sendString(HttpURLConnection.HTTP_NOT_FOUND,String.format(""String_Node_Str"",sourceID),Charsets.UTF_8);
  }
 else {
    responder.sendJson(HttpURLConnection.HTTP_OK,fieldDetailMap.values());
  }
}","/** 
 * Gets the fields that are queryable for a given time range and sourceID for combinable aggregation functions
 */
@Path(""String_Node_Str"") @GET public void fieldsGetter(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") String sourceID,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long startTimestamp,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long endTimestamp) throws IOException {
  AggregationsRowKey aggregationsRowKeyStart=new AggregationsRowKey(startTimestamp,sourceID);
  AggregationsRowKey aggregationsRowKeyEnd=new AggregationsRowKey(endTimestamp + 1,sourceID);
  Row row;
  Map<String,FieldDetail> fieldDetailMap=new HashMap<>();
  try (Scanner scanner=dataStore.scan(aggregationsRowKeyStart.getTableRowKey(),aggregationsRowKeyEnd.getTableRowKey())){
    while ((row=scanner.next()) != null) {
      Map<byte[],byte[]> columnsMapBytes=row.getColumns();
      List<FieldDetail> timestampSpecificFieldDetailList=new ArrayList<>();
      for (      Map.Entry<byte[],byte[]> columnMapEntry : columnsMapBytes.entrySet()) {
        String fieldName=Bytes.toString(columnMapEntry.getKey());
        byte[] output=columnMapEntry.getValue();
        String outputString=Bytes.toString(output);
        Set<AggregationTypeValue> aggregationTypeValuesSet=GSON.fromJson(outputString,TOKEN_TYPE_SET_AGGREGATION_TYPE_VALUES);
        FieldDetail fieldDetail=new FieldDetail(fieldName,aggregationTypeValuesSet);
        timestampSpecificFieldDetailList.add(fieldDetail);
      }
      for (      FieldDetail fdTimestampSpecific : timestampSpecificFieldDetailList) {
        String fdTimestampSpecificFieldName=fdTimestampSpecific.getFieldName();
        if (fieldDetailMap.containsKey(fdTimestampSpecificFieldName)) {
          FieldDetail fdCombined=fieldDetailMap.get(fdTimestampSpecificFieldName);
          fdCombined.addAggregations(fdTimestampSpecific.getAggregationTypeSet());
        }
 else {
          fieldDetailMap.put(fdTimestampSpecificFieldName,fdTimestampSpecific);
        }
      }
    }
  }
   if (fieldDetailMap.isEmpty()) {
    responder.sendString(HttpURLConnection.HTTP_NOT_FOUND,String.format(""String_Node_Str"",sourceID),Charsets.UTF_8);
  }
 else {
    responder.sendJson(HttpURLConnection.HTTP_OK,fieldDetailMap.values());
  }
}"
6137,"@Test public void testDefaultConfig() throws Exception {
  Map<String,Set<String>> testMap=new HashMap<>();
  Set<String> testSet=new HashSet<>();
  testSet.add(""String_Node_Str"");
  testMap.put(""String_Node_Str"",testSet);
  DataQualityApp.DataQualityConfig config=new DataQualityApp.DataQualityConfig(WORKFLOW_SCHEDULE_MINUTES,getStreamSource(),""String_Node_Str"",testMap);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  AppRequest<DataQualityApp.DataQualityConfig> appRequest=new AppRequest<>(new ArtifactSummary(appArtifact.getName(),appArtifact.getVersion().getVersion()),config);
  ApplicationManager applicationManager=deployApplication(appId,appRequest);
  MapReduceManager mrManager=applicationManager.getMapReduceManager(""String_Node_Str"").start();
  mrManager.waitForFinish(180,TimeUnit.SECONDS);
  Table logDataStore=(Table)getDataset(""String_Node_Str"").get();
  Scanner scanner=logDataStore.scan(null,null);
  DiscreteValuesHistogram discreteValuesHistogramAggregationFunction=new DiscreteValuesHistogram();
  Row row;
  try {
    while ((row=scanner.next()) != null) {
      if (Bytes.toString(row.getRow()).contains(""String_Node_Str"")) {
        Map<byte[],byte[]> columnsMapBytes=row.getColumns();
        byte[] output=columnsMapBytes.get(Bytes.toBytes(""String_Node_Str""));
        if (output != null) {
          discreteValuesHistogramAggregationFunction.combine(output);
        }
      }
    }
  }
  finally {
    scanner.close();
  }
  Map<String,Integer> outputMap=discreteValuesHistogramAggregationFunction.retrieveAggregation();
  Map<String,Integer> expectedMap=Maps.newHashMap();
  expectedMap.put(""String_Node_Str"",3);
  Assert.assertEquals(expectedMap,outputMap);
}","@Test public void testDefaultConfig() throws Exception {
  Map<String,Set<String>> testMap=new HashMap<>();
  Set<String> testSet=new HashSet<>();
  testSet.add(""String_Node_Str"");
  testMap.put(""String_Node_Str"",testSet);
  DataQualityApp.DataQualityConfig config=new DataQualityApp.DataQualityConfig(WORKFLOW_SCHEDULE_MINUTES,getStreamSource(),""String_Node_Str"",testMap);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  AppRequest<DataQualityApp.DataQualityConfig> appRequest=new AppRequest<>(new ArtifactSummary(appArtifact.getName(),appArtifact.getVersion().getVersion()),config);
  ApplicationManager applicationManager=deployApplication(appId,appRequest);
  MapReduceManager mrManager=applicationManager.getMapReduceManager(""String_Node_Str"").start();
  mrManager.waitForFinish(180,TimeUnit.SECONDS);
  Table logDataStore=(Table)getDataset(""String_Node_Str"").get();
  DiscreteValuesHistogram discreteValuesHistogramAggregationFunction=new DiscreteValuesHistogram();
  Row row;
  try (Scanner scanner=logDataStore.scan(null,null)){
    while ((row=scanner.next()) != null) {
      if (Bytes.toString(row.getRow()).contains(""String_Node_Str"")) {
        Map<byte[],byte[]> columnsMapBytes=row.getColumns();
        byte[] output=columnsMapBytes.get(Bytes.toBytes(""String_Node_Str""));
        if (output != null) {
          discreteValuesHistogramAggregationFunction.combine(output);
        }
      }
    }
  }
   Map<String,Integer> outputMap=discreteValuesHistogramAggregationFunction.retrieveAggregation();
  Map<String,Integer> expectedMap=Maps.newHashMap();
  expectedMap.put(""String_Node_Str"",3);
  Assert.assertEquals(expectedMap,outputMap);
}"
6138,"@Override public void initialize(final WorkerContext context) throws Exception {
  if (Boolean.valueOf(context.getSpecification().getProperty(Constants.STAGE_LOGGING_ENABLED))) {
    LogStageInjector.start();
  }
  super.initialize(context);
  Map<String,String> properties=context.getSpecification().getProperties();
  appName=context.getApplicationSpecification().getName();
  Preconditions.checkArgument(properties.containsKey(Constants.PIPELINEID));
  Preconditions.checkArgument(properties.containsKey(UNIQUE_ID));
  String uniqueId=properties.get(UNIQUE_ID);
  final String appName=context.getApplicationSpecification().getName();
  stateStoreKey=String.format(""String_Node_Str"",appName,SEPARATOR,uniqueId,SEPARATOR,context.getInstanceId());
  stateStoreKeyBytes=Bytes.toBytes(stateStoreKey);
  getContext().execute(new TxRunnable(){
    @Override public void run(    DatasetContext dsContext) throws Exception {
      KeyValueTable stateTable=dsContext.getDataset(ETLRealtimeApplication.STATE_TABLE);
      byte[] startKey=Bytes.toBytes(String.format(""String_Node_Str"",appName,SEPARATOR));
      CloseableIterator<KeyValue<byte[],byte[]>> rows=stateTable.scan(startKey,Bytes.stopKeyForPrefix(startKey));
      try {
        while (rows.hasNext()) {
          KeyValue<byte[],byte[]> row=rows.next();
          if (Bytes.compareTo(stateStoreKeyBytes,row.getKey()) != 0) {
            stateTable.delete(row.getKey());
          }
        }
      }
  finally {
        rows.close();
      }
    }
  }
);
  PipelinePhase pipeline=GSON.fromJson(properties.get(Constants.PIPELINEID),PipelinePhase.class);
  Map<String,TransformDetail> transformationMap=new HashMap<>();
  initializeSource(context,pipeline);
  initializeTransforms(context,transformationMap,pipeline);
  initializeSinks(context,transformationMap,pipeline);
  Set<String> startStages=new HashSet<>();
  startStages.addAll(pipeline.getStageOutputs(sourceStageName));
  transformExecutor=new TransformExecutor(transformationMap,startStages);
}","@Override public void initialize(final WorkerContext context) throws Exception {
  if (Boolean.valueOf(context.getSpecification().getProperty(Constants.STAGE_LOGGING_ENABLED))) {
    LogStageInjector.start();
  }
  super.initialize(context);
  Map<String,String> properties=context.getSpecification().getProperties();
  appName=context.getApplicationSpecification().getName();
  Preconditions.checkArgument(properties.containsKey(Constants.PIPELINEID));
  Preconditions.checkArgument(properties.containsKey(UNIQUE_ID));
  String uniqueId=properties.get(UNIQUE_ID);
  final String appName=context.getApplicationSpecification().getName();
  stateStoreKey=String.format(""String_Node_Str"",appName,SEPARATOR,uniqueId,SEPARATOR,context.getInstanceId());
  stateStoreKeyBytes=Bytes.toBytes(stateStoreKey);
  getContext().execute(new TxRunnable(){
    @Override public void run(    DatasetContext dsContext) throws Exception {
      KeyValueTable stateTable=dsContext.getDataset(ETLRealtimeApplication.STATE_TABLE);
      byte[] startKey=Bytes.toBytes(String.format(""String_Node_Str"",appName,SEPARATOR));
      try (CloseableIterator<KeyValue<byte[],byte[]>> rows=stateTable.scan(startKey,Bytes.stopKeyForPrefix(startKey))){
        while (rows.hasNext()) {
          KeyValue<byte[],byte[]> row=rows.next();
          if (Bytes.compareTo(stateStoreKeyBytes,row.getKey()) != 0) {
            stateTable.delete(row.getKey());
          }
        }
      }
     }
  }
);
  PipelinePhase pipeline=GSON.fromJson(properties.get(Constants.PIPELINEID),PipelinePhase.class);
  Map<String,TransformDetail> transformationMap=new HashMap<>();
  initializeSource(context,pipeline);
  initializeTransforms(context,transformationMap,pipeline);
  initializeSinks(context,transformationMap,pipeline);
  Set<String> startStages=new HashSet<>();
  startStages.addAll(pipeline.getStageOutputs(sourceStageName));
  transformExecutor=new TransformExecutor(transformationMap,startStages);
}"
6139,"/** 
 * Used to read the records written by this sink.
 * @param tableManager dataset manager used to get the sink dataset to read from
 */
public static List<StructuredRecord> readOutput(DataSetManager<Table> tableManager) throws Exception {
  Table table=tableManager.get();
  Scanner scanner=table.scan(null,null);
  try {
    List<StructuredRecord> records=new ArrayList<>();
    Row row;
    while ((row=scanner.next()) != null) {
      Schema schema=Schema.parseJson(row.getString(SCHEMA_COL));
      String recordStr=row.getString(RECORD_COL);
      records.add(StructuredRecordStringConverter.fromJsonString(recordStr,schema));
    }
    return records;
  }
  finally {
    scanner.close();
  }
}","/** 
 * Used to read the records written by this sink.
 * @param tableManager dataset manager used to get the sink dataset to read from
 */
public static List<StructuredRecord> readOutput(DataSetManager<Table> tableManager) throws Exception {
  Table table=tableManager.get();
  try (Scanner scanner=table.scan(null,null)){
    List<StructuredRecord> records=new ArrayList<>();
    Row row;
    while ((row=scanner.next()) != null) {
      Schema schema=Schema.parseJson(row.getString(SCHEMA_COL));
      String recordStr=row.getString(RECORD_COL);
      records.add(StructuredRecordStringConverter.fromJsonString(recordStr,schema));
    }
    return records;
  }
 }"
6140,"protected void getPartitions(@Nullable PartitionFilter filter,PartitionConsumer consumer,boolean decodeMetadata){
  byte[] startKey=generateStartKey(filter);
  byte[] endKey=generateStopKey(filter);
  Scanner scanner=partitionsTable.scan(startKey,endKey);
  try {
    while (true) {
      Row row=scanner.next();
      if (row == null) {
        break;
      }
      PartitionKey key;
      try {
        key=parseRowKey(row.getRow(),partitioning);
      }
 catch (      IllegalArgumentException e) {
        if (!ignoreInvalidRowsSilently) {
          LOG.debug(String.format(""String_Node_Str"",getName(),Bytes.toStringBinary(row.getRow())));
        }
        continue;
      }
      if (filter != null && !filter.match(key)) {
        continue;
      }
      byte[] pathBytes=row.get(RELATIVE_PATH);
      if (pathBytes != null) {
        consumer.consume(key,Bytes.toString(pathBytes),decodeMetadata ? metadataFromRow(row) : null);
      }
    }
  }
  finally {
    scanner.close();
  }
}","protected void getPartitions(@Nullable PartitionFilter filter,PartitionConsumer consumer,boolean decodeMetadata){
  byte[] startKey=generateStartKey(filter);
  byte[] endKey=generateStopKey(filter);
  try (Scanner scanner=partitionsTable.scan(startKey,endKey)){
    while (true) {
      Row row=scanner.next();
      if (row == null) {
        break;
      }
      PartitionKey key;
      try {
        key=parseRowKey(row.getRow(),partitioning);
      }
 catch (      IllegalArgumentException e) {
        if (!ignoreInvalidRowsSilently) {
          LOG.debug(String.format(""String_Node_Str"",getName(),Bytes.toStringBinary(row.getRow())));
        }
        continue;
      }
      if (filter != null && !filter.match(key)) {
        continue;
      }
      byte[] pathBytes=row.get(RELATIVE_PATH);
      if (pathBytes != null) {
        consumer.consume(key,Bytes.toString(pathBytes),decodeMetadata ? metadataFromRow(row) : null);
      }
    }
  }
 }"
6141,"@Override public PartitionConsumerResult consumePartitions(PartitionConsumerState partitionConsumerState,int limit,Predicate<PartitionDetail> predicate){
  List<Long> previousInProgress=partitionConsumerState.getVersionsToCheck();
  Set<Long> noLongerInProgress=setDiff(previousInProgress,tx.getInProgress());
  List<PartitionDetail> partitions=Lists.newArrayList();
  Iterator<Long> iter=noLongerInProgress.iterator();
  while (iter.hasNext()) {
    Long txId=iter.next();
    if (partitions.size() >= limit) {
      break;
    }
    Scanner scanner=partitionsTable.readByIndex(WRITE_PTR_COL,Bytes.toBytes(txId));
    try {
      scannerToPartitions(scanner,partitions,limit,predicate);
    }
  finally {
      scanner.close();
    }
    iter.remove();
  }
  long scanUpTo;
  if (partitions.size() < limit) {
    scanUpTo=Math.min(tx.getWritePointer(),tx.getReadPointer() + 1);
    Scanner scanner=partitionsTable.scanByIndex(WRITE_PTR_COL,Bytes.toBytes(partitionConsumerState.getStartVersion()),Bytes.toBytes(scanUpTo));
    Long endTxId;
    try {
      endTxId=scannerToPartitions(scanner,partitions,limit,predicate);
    }
  finally {
      scanner.close();
    }
    if (endTxId != null) {
      scanUpTo=endTxId;
    }
  }
 else {
    scanUpTo=partitionConsumerState.getStartVersion();
  }
  List<Long> inProgressBeforeScanEnd=Lists.newArrayList(noLongerInProgress);
  for (  long txId : tx.getInProgress()) {
    if (txId >= scanUpTo) {
      break;
    }
    inProgressBeforeScanEnd.add(txId);
  }
  return new PartitionConsumerResult(new PartitionConsumerState(scanUpTo,inProgressBeforeScanEnd),partitions);
}","@Override public PartitionConsumerResult consumePartitions(PartitionConsumerState partitionConsumerState,int limit,Predicate<PartitionDetail> predicate){
  List<Long> previousInProgress=partitionConsumerState.getVersionsToCheck();
  Set<Long> noLongerInProgress=setDiff(previousInProgress,tx.getInProgress());
  List<PartitionDetail> partitions=Lists.newArrayList();
  Iterator<Long> iter=noLongerInProgress.iterator();
  while (iter.hasNext()) {
    Long txId=iter.next();
    if (partitions.size() >= limit) {
      break;
    }
    try (Scanner scanner=partitionsTable.readByIndex(WRITE_PTR_COL,Bytes.toBytes(txId))){
      scannerToPartitions(scanner,partitions,limit,predicate);
    }
     iter.remove();
  }
  long scanUpTo;
  if (partitions.size() < limit) {
    scanUpTo=Math.min(tx.getWritePointer(),tx.getReadPointer() + 1);
    Long endTxId;
    try (Scanner scanner=partitionsTable.scanByIndex(WRITE_PTR_COL,Bytes.toBytes(partitionConsumerState.getStartVersion()),Bytes.toBytes(scanUpTo))){
      endTxId=scannerToPartitions(scanner,partitions,limit,predicate);
    }
     if (endTxId != null) {
      scanUpTo=endTxId;
    }
  }
 else {
    scanUpTo=partitionConsumerState.getStartVersion();
  }
  List<Long> inProgressBeforeScanEnd=Lists.newArrayList(noLongerInProgress);
  for (  long txId : tx.getInProgress()) {
    if (txId >= scanUpTo) {
      break;
    }
    inProgressBeforeScanEnd.add(txId);
  }
  return new PartitionConsumerResult(new PartitionConsumerState(scanUpTo,inProgressBeforeScanEnd),partitions);
}"
6142,"public void deleteRange(byte[] startRow,byte[] stopRow,@Nullable FuzzyRowFilter filter,@Nullable byte[][] columns) throws IOException {
  if (columns != null) {
    if (columns.length == 0) {
      return;
    }
    columns=Arrays.copyOf(columns,columns.length);
    Arrays.sort(columns,Bytes.BYTES_COMPARATOR);
  }
  DB db=getDB();
  DBIterator iterator=db.iterator();
  seekToStart(iterator,startRow);
  byte[] endKey=stopRow == null ? null : createEndKey(stopRow);
  Scanner scanner=new LevelDBScanner(iterator,endKey,filter,columns,null);
  DBIterator deleteIterator=db.iterator();
  seekToStart(deleteIterator,startRow);
  final int deletesPerRound=1024;
  try {
    Row rowValues;
    WriteBatch batch=db.createWriteBatch();
    int deletesInBatch=0;
    while ((rowValues=scanner.next()) != null) {
      byte[] row=rowValues.getRow();
      for (      byte[] column : rowValues.getColumns().keySet()) {
        addToDeleteBatch(batch,deleteIterator,row,column);
        deletesInBatch++;
        if (deletesInBatch >= deletesPerRound) {
          db.write(batch,getWriteOptions());
          batch=db.createWriteBatch();
          deletesInBatch=0;
        }
      }
    }
    if (deletesInBatch > 0) {
      db.write(batch,getWriteOptions());
    }
  }
  finally {
    scanner.close();
    deleteIterator.close();
  }
}","public void deleteRange(byte[] startRow,byte[] stopRow,@Nullable FuzzyRowFilter filter,@Nullable byte[][] columns) throws IOException {
  if (columns != null) {
    if (columns.length == 0) {
      return;
    }
    columns=Arrays.copyOf(columns,columns.length);
    Arrays.sort(columns,Bytes.BYTES_COMPARATOR);
  }
  DB db=getDB();
  DBIterator iterator=db.iterator();
  seekToStart(iterator,startRow);
  byte[] endKey=stopRow == null ? null : createEndKey(stopRow);
  DBIterator deleteIterator=db.iterator();
  seekToStart(deleteIterator,startRow);
  final int deletesPerRound=1024;
  try (Scanner scanner=new LevelDBScanner(iterator,endKey,filter,columns,null)){
    Row rowValues;
    WriteBatch batch=db.createWriteBatch();
    int deletesInBatch=0;
    while ((rowValues=scanner.next()) != null) {
      byte[] row=rowValues.getRow();
      for (      byte[] column : rowValues.getColumns().keySet()) {
        addToDeleteBatch(batch,deleteIterator,row,column);
        deletesInBatch++;
        if (deletesInBatch >= deletesPerRound) {
          db.write(batch,getWriteOptions());
          batch=db.createWriteBatch();
          deletesInBatch=0;
        }
      }
    }
    if (deletesInBatch > 0) {
      db.write(batch,getWriteOptions());
    }
  }
  finally {
    deleteIterator.close();
  }
}"
6143,"/** 
 * Delete entries in fact table.
 * @param scan specifies deletion criteria
 */
public void delete(FactScan scan){
  Scanner scanner=getScanner(scan);
  try {
    Row row;
    while ((row=scanner.next()) != null) {
      List<byte[]> columns=Lists.newArrayList();
      boolean exhausted=false;
      for (      byte[] column : row.getColumns().keySet()) {
        long ts=codec.getTimestamp(row.getRow(),column);
        if (ts < scan.getStartTs()) {
          continue;
        }
        if (ts > scan.getEndTs()) {
          exhausted=true;
          break;
        }
        columns.add(column);
      }
      timeSeriesTable.delete(row.getRow(),columns.toArray(new byte[columns.size()][]));
      if (exhausted) {
        break;
      }
    }
  }
  finally {
    scanner.close();
  }
}","/** 
 * Delete entries in fact table.
 * @param scan specifies deletion criteria
 */
public void delete(FactScan scan){
  try (Scanner scanner=getScanner(scan)){
    Row row;
    while ((row=scanner.next()) != null) {
      List<byte[]> columns=Lists.newArrayList();
      boolean exhausted=false;
      for (      byte[] column : row.getColumns().keySet()) {
        long ts=codec.getTimestamp(row.getRow(),column);
        if (ts < scan.getStartTs()) {
          continue;
        }
        if (ts > scan.getEndTs()) {
          exhausted=true;
          break;
        }
        columns.add(column);
      }
      timeSeriesTable.delete(row.getRow(),columns.toArray(new byte[columns.size()][]));
      if (exhausted) {
        break;
      }
    }
  }
 }"
6144,"/** 
 * Finds all measure names of the facts that match given   {@link DimensionValue}s and time range.
 * @param allDimensionNames list of all dimension names to be present in the fact record
 * @param dimensionSlice dimension values to filter by, {@code null} means any non-null value.
 * @param startTs start timestamp, in sec
 * @param endTs end timestamp, in sec
 * @return {@link Set} of measure names
 */
public Set<String> findMeasureNames(List<String> allDimensionNames,Map<String,String> dimensionSlice,long startTs,long endTs){
  List<DimensionValue> allDimensions=Lists.newArrayList();
  for (  String dimensionName : allDimensionNames) {
    allDimensions.add(new DimensionValue(dimensionName,dimensionSlice.get(dimensionName)));
  }
  byte[] startRow=codec.createStartRowKey(allDimensions,null,startTs,false);
  byte[] endRow=codec.createEndRowKey(allDimensions,null,endTs,false);
  endRow=Bytes.stopKeyForPrefix(endRow);
  FuzzyRowFilter fuzzyRowFilter=createFuzzyRowFilter(new FactScan(startTs,endTs,ImmutableList.<String>of(),allDimensions),startRow);
  Set<String> measureNames=Sets.newHashSet();
  int scannedRecords=0;
  Scanner scanner=timeSeriesTable.scan(startRow,endRow,fuzzyRowFilter);
  try {
    Row rowResult;
    while ((rowResult=scanner.next()) != null) {
      scannedRecords++;
      if (scannedRecords > MAX_RECORDS_TO_SCAN_DURING_SEARCH) {
        break;
      }
      byte[] rowKey=rowResult.getRow();
      if (codec.getTimestamp(rowKey,codec.createColumn(startTs)) < startTs) {
        continue;
      }
      if (codec.getTimestamp(rowKey,codec.createColumn(endTs)) > endTs) {
        break;
      }
      measureNames.add(codec.getMeasureName(rowResult.getRow()));
    }
  }
  finally {
    scanner.close();
  }
  LOG.trace(""String_Node_Str"",scannedRecords);
  return measureNames;
}","/** 
 * Finds all measure names of the facts that match given   {@link DimensionValue}s and time range.
 * @param allDimensionNames list of all dimension names to be present in the fact record
 * @param dimensionSlice dimension values to filter by, {@code null} means any non-null value.
 * @param startTs start timestamp, in sec
 * @param endTs end timestamp, in sec
 * @return {@link Set} of measure names
 */
public Set<String> findMeasureNames(List<String> allDimensionNames,Map<String,String> dimensionSlice,long startTs,long endTs){
  List<DimensionValue> allDimensions=Lists.newArrayList();
  for (  String dimensionName : allDimensionNames) {
    allDimensions.add(new DimensionValue(dimensionName,dimensionSlice.get(dimensionName)));
  }
  byte[] startRow=codec.createStartRowKey(allDimensions,null,startTs,false);
  byte[] endRow=codec.createEndRowKey(allDimensions,null,endTs,false);
  endRow=Bytes.stopKeyForPrefix(endRow);
  FuzzyRowFilter fuzzyRowFilter=createFuzzyRowFilter(new FactScan(startTs,endTs,ImmutableList.<String>of(),allDimensions),startRow);
  Set<String> measureNames=Sets.newHashSet();
  int scannedRecords=0;
  try (Scanner scanner=timeSeriesTable.scan(startRow,endRow,fuzzyRowFilter)){
    Row rowResult;
    while ((rowResult=scanner.next()) != null) {
      scannedRecords++;
      if (scannedRecords > MAX_RECORDS_TO_SCAN_DURING_SEARCH) {
        break;
      }
      byte[] rowKey=rowResult.getRow();
      if (codec.getTimestamp(rowKey,codec.createColumn(startTs)) < startTs) {
        continue;
      }
      if (codec.getTimestamp(rowKey,codec.createColumn(endTs)) > endTs) {
        break;
      }
      measureNames.add(codec.getMeasureName(rowResult.getRow()));
    }
  }
   LOG.trace(""String_Node_Str"",scannedRecords);
  return measureNames;
}"
6145,"/** 
 * Rebuilds all the indexes in the   {@link MetadataDataset} in batches.
 * @param startRowKey the key of the row to start the scan for the current batch with
 * @param limit the batch size
 * @return the row key of the last row scanned in the current batch, {@code null} if there are no more rows to scan.
 */
@Nullable public byte[] rebuildIndexes(@Nullable byte[] startRowKey,int limit){
  byte[] valueRowPrefix=MdsKey.getValueRowPrefix();
  startRowKey=startRowKey == null ? valueRowPrefix : startRowKey;
  byte[] stopRowKey=Bytes.stopKeyForPrefix(valueRowPrefix);
  Scanner scanner=indexedTable.scan(startRowKey,stopRowKey);
  Row row;
  try {
    while ((limit > 0) && (row=scanner.next()) != null) {
      byte[] rowKey=row.getRow();
      String targetType=MdsKey.getTargetType(rowKey);
      Id.NamespacedId namespacedId=MdsKey.getNamespacedIdFromKey(targetType,rowKey);
      String metadataKey=MdsKey.getMetadataKey(targetType,rowKey);
      Indexer indexer=getIndexerForKey(metadataKey);
      MetadataEntry metadataEntry=getMetadata(namespacedId,metadataKey);
      if (metadataEntry == null) {
        LOG.warn(""String_Node_Str"" + ""String_Node_Str"",metadataKey,namespacedId);
        continue;
      }
      Set<String> indexes=indexer.getIndexes(metadataEntry);
      storeIndexes(namespacedId,metadataKey,indexes);
      limit--;
    }
    Row startRowForNextBatch=scanner.next();
    if (startRowForNextBatch == null) {
      return null;
    }
    return startRowForNextBatch.getRow();
  }
  finally {
    scanner.close();
  }
}","/** 
 * Rebuilds all the indexes in the   {@link MetadataDataset} in batches.
 * @param startRowKey the key of the row to start the scan for the current batch with
 * @param limit the batch size
 * @return the row key of the last row scanned in the current batch, {@code null} if there are no more rows to scan.
 */
@Nullable public byte[] rebuildIndexes(@Nullable byte[] startRowKey,int limit){
  byte[] valueRowPrefix=MdsKey.getValueRowPrefix();
  startRowKey=startRowKey == null ? valueRowPrefix : startRowKey;
  byte[] stopRowKey=Bytes.stopKeyForPrefix(valueRowPrefix);
  Row row;
  try (Scanner scanner=indexedTable.scan(startRowKey,stopRowKey)){
    while ((limit > 0) && (row=scanner.next()) != null) {
      byte[] rowKey=row.getRow();
      String targetType=MdsKey.getTargetType(rowKey);
      Id.NamespacedId namespacedId=MdsKey.getNamespacedIdFromKey(targetType,rowKey);
      String metadataKey=MdsKey.getMetadataKey(targetType,rowKey);
      Indexer indexer=getIndexerForKey(metadataKey);
      MetadataEntry metadataEntry=getMetadata(namespacedId,metadataKey);
      if (metadataEntry == null) {
        LOG.warn(""String_Node_Str"" + ""String_Node_Str"",metadataKey,namespacedId);
        continue;
      }
      Set<String> indexes=indexer.getIndexes(metadataEntry);
      storeIndexes(namespacedId,metadataKey,indexes);
      limit--;
    }
    Row startRowForNextBatch=scanner.next();
    if (startRowForNextBatch == null) {
      return null;
    }
    return startRowForNextBatch.getRow();
  }
 }"
6146,"private Metadata getSnapshotBeforeTime(Id.NamespacedId targetId,long timeMillis){
  byte[] scanStartKey=MdsHistoryKey.getMdsScanStartKey(targetId,timeMillis).getKey();
  byte[] scanEndKey=MdsHistoryKey.getMdsScanEndKey(targetId).getKey();
  Scanner scanner=indexedTable.scan(scanStartKey,scanEndKey);
  try {
    Row next=scanner.next();
    if (next != null) {
      return GSON.fromJson(next.getString(HISTORY_COLUMN),Metadata.class);
    }
 else {
      return new Metadata(targetId);
    }
  }
  finally {
    scanner.close();
  }
}","private Metadata getSnapshotBeforeTime(Id.NamespacedId targetId,long timeMillis){
  byte[] scanStartKey=MdsHistoryKey.getMdsScanStartKey(targetId,timeMillis).getKey();
  byte[] scanEndKey=MdsHistoryKey.getMdsScanEndKey(targetId).getKey();
  try (Scanner scanner=indexedTable.scan(scanStartKey,scanEndKey)){
    Row next=scanner.next();
    if (next != null) {
      return GSON.fromJson(next.getString(HISTORY_COLUMN),Metadata.class);
    }
 else {
      return new Metadata(targetId);
    }
  }
 }"
6147,"/** 
 * Delete all indexes in the metadata dataset.
 * @param limit the number of rows (indexes) to delete
 * @return the offset at which to start deletion
 */
public int deleteAllIndexes(int limit){
  byte[] indexStartPrefix=MdsKey.getIndexRowPrefix();
  byte[] indexStopPrefix=Bytes.stopKeyForPrefix(indexStartPrefix);
  int count=0;
  Scanner scanner=indexedTable.scan(indexStartPrefix,indexStopPrefix);
  Row row;
  try {
    while (count < limit && ((row=scanner.next()) != null)) {
      if (deleteIndexRow(row)) {
        count++;
      }
    }
  }
  finally {
    scanner.close();
  }
  return count;
}","/** 
 * Delete all indexes in the metadata dataset.
 * @param limit the number of rows (indexes) to delete
 * @return the offset at which to start deletion
 */
public int deleteAllIndexes(int limit){
  byte[] indexStartPrefix=MdsKey.getIndexRowPrefix();
  byte[] indexStopPrefix=Bytes.stopKeyForPrefix(indexStartPrefix);
  int count=0;
  Row row;
  try (Scanner scanner=indexedTable.scan(indexStartPrefix,indexStopPrefix)){
    while (count < limit && ((row=scanner.next()) != null)) {
      if (deleteIndexRow(row)) {
        count++;
      }
    }
  }
   return count;
}"
6148,"@Override protected void configure(){
  bind(DynamicDatasetCache.class).toProvider(DynamicDatasetCacheProvider.class).in(Scopes.SINGLETON);
  bind(DatasetContext.class).to(DynamicDatasetCache.class).in(Scopes.SINGLETON);
  bind(Admin.class).toProvider(AdminProvider.class);
  bind(Transactional.class).toProvider(TransactionalProvider.class);
  install(new FactoryModuleBuilder().implement(AuthorizationContext.class,DefaultAuthorizationContext.class).build(AuthorizationContextFactory.class));
  bind(AuthorizerInstantiatorService.class).in(Scopes.SINGLETON);
  expose(AuthorizerInstantiatorService.class);
}","@Override protected void configure(){
  bind(DynamicDatasetCache.class).toProvider(DynamicDatasetCacheProvider.class).in(Scopes.SINGLETON);
  bind(DatasetContext.class).to(DynamicDatasetCache.class).in(Scopes.SINGLETON);
  bind(Admin.class).toProvider(AdminProvider.class);
  bind(Transactional.class).toProvider(TransactionalProvider.class);
  install(new FactoryModuleBuilder().implement(AuthorizationContext.class,DefaultAuthorizationContext.class).build(AuthorizationContextFactory.class));
  bind(AuthorizerInstantiator.class).in(Scopes.SINGLETON);
  expose(AuthorizerInstantiator.class);
}"
6149,"@Inject AuthorizationHandler(AuthorizerInstantiatorService authorizerInstantiatorService,CConfiguration cConf,EntityExistenceVerifier entityExistenceVerifier){
  this.authorizerInstantiatorService=authorizerInstantiatorService;
  this.authenticationEnabled=cConf.getBoolean(Constants.Security.ENABLED);
  this.authorizationEnabled=cConf.getBoolean(Constants.Security.Authorization.ENABLED);
  this.entityExistenceVerifier=entityExistenceVerifier;
}","@Inject AuthorizationHandler(AuthorizerInstantiator authorizerInstantiator,CConfiguration cConf,EntityExistenceVerifier entityExistenceVerifier){
  this.authorizerInstantiator=authorizerInstantiator;
  this.authenticationEnabled=cConf.getBoolean(Constants.Security.ENABLED);
  this.authorizationEnabled=cConf.getBoolean(Constants.Security.Authorization.ENABLED);
  this.entityExistenceVerifier=entityExistenceVerifier;
}"
6150,"@Path(""String_Node_Str"") @PUT public void addRoleToPrincipal(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String principalType,@PathParam(""String_Node_Str"") String principalName,@PathParam(""String_Node_Str"") String roleName) throws Exception {
  ensureSecurityEnabled();
  authorizerInstantiatorService.get().addRoleToPrincipal(new Role(roleName),new Principal(principalName,Principal.PrincipalType.valueOf(principalType.toUpperCase())));
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,null,HttpResponseStatus.OK);
}","@Path(""String_Node_Str"") @PUT public void addRoleToPrincipal(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String principalType,@PathParam(""String_Node_Str"") String principalName,@PathParam(""String_Node_Str"") String roleName) throws Exception {
  ensureSecurityEnabled();
  authorizerInstantiator.get().addRoleToPrincipal(new Role(roleName),new Principal(principalName,Principal.PrincipalType.valueOf(principalType.toUpperCase())));
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,null,HttpResponseStatus.OK);
}"
6151,"@Path(""String_Node_Str"") @POST public void revoke(HttpRequest httpRequest,HttpResponder httpResponder) throws Exception {
  ensureSecurityEnabled();
  RevokeRequest request=parseBody(httpRequest,RevokeRequest.class);
  verifyAuthRequest(request);
  authorizerInstantiatorService.get().enforce(request.getEntity(),SecurityRequestContext.toPrincipal(),Action.ADMIN);
  if (request.getPrincipal() == null && request.getActions() == null) {
    authorizerInstantiatorService.get().revoke(request.getEntity());
  }
 else {
    Set<Action> actions=request.getActions() == null ? EnumSet.allOf(Action.class) : request.getActions();
    authorizerInstantiatorService.get().revoke(request.getEntity(),request.getPrincipal(),actions);
  }
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,request,HttpResponseStatus.OK);
}","@Path(""String_Node_Str"") @POST public void revoke(HttpRequest httpRequest,HttpResponder httpResponder) throws Exception {
  ensureSecurityEnabled();
  RevokeRequest request=parseBody(httpRequest,RevokeRequest.class);
  verifyAuthRequest(request);
  authorizerInstantiator.get().enforce(request.getEntity(),SecurityRequestContext.toPrincipal(),Action.ADMIN);
  if (request.getPrincipal() == null && request.getActions() == null) {
    authorizerInstantiator.get().revoke(request.getEntity());
  }
 else {
    Set<Action> actions=request.getActions() == null ? EnumSet.allOf(Action.class) : request.getActions();
    authorizerInstantiator.get().revoke(request.getEntity(),request.getPrincipal(),actions);
  }
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,request,HttpResponseStatus.OK);
}"
6152,"/** 
 * Role Management : For Role Based Access Control
 */
@Path(""String_Node_Str"") @PUT public void createRole(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String roleName) throws Exception {
  ensureSecurityEnabled();
  authorizerInstantiatorService.get().createRole(new Role(roleName));
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,null,HttpResponseStatus.OK);
}","/** 
 * Role Management : For Role Based Access Control
 */
@Path(""String_Node_Str"") @PUT public void createRole(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String roleName) throws Exception {
  ensureSecurityEnabled();
  authorizerInstantiator.get().createRole(new Role(roleName));
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,null,HttpResponseStatus.OK);
}"
6153,"@Path(""String_Node_Str"") @DELETE public void removeRoleFromPrincipal(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String principalType,@PathParam(""String_Node_Str"") String principalName,@PathParam(""String_Node_Str"") String roleName) throws Exception {
  ensureSecurityEnabled();
  authorizerInstantiatorService.get().removeRoleFromPrincipal(new Role(roleName),new Principal(principalName,Principal.PrincipalType.valueOf(principalType.toUpperCase())));
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,null,HttpResponseStatus.OK);
}","@Path(""String_Node_Str"") @DELETE public void removeRoleFromPrincipal(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String principalType,@PathParam(""String_Node_Str"") String principalName,@PathParam(""String_Node_Str"") String roleName) throws Exception {
  ensureSecurityEnabled();
  authorizerInstantiator.get().removeRoleFromPrincipal(new Role(roleName),new Principal(principalName,Principal.PrincipalType.valueOf(principalType.toUpperCase())));
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,null,HttpResponseStatus.OK);
}"
6154,"@Path(""String_Node_Str"") @GET public void listRoles(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String principalType,@PathParam(""String_Node_Str"") String principalName) throws Exception {
  ensureSecurityEnabled();
  Principal principal=new Principal(principalName,Principal.PrincipalType.valueOf(principalType.toUpperCase()));
  httpResponder.sendJson(HttpResponseStatus.OK,authorizerInstantiatorService.get().listRoles(principal));
  createLogEntry(httpRequest,null,HttpResponseStatus.OK);
}","@Path(""String_Node_Str"") @GET public void listRoles(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String principalType,@PathParam(""String_Node_Str"") String principalName) throws Exception {
  ensureSecurityEnabled();
  Principal principal=new Principal(principalName,Principal.PrincipalType.valueOf(principalType.toUpperCase()));
  httpResponder.sendJson(HttpResponseStatus.OK,authorizerInstantiator.get().listRoles(principal));
  createLogEntry(httpRequest,null,HttpResponseStatus.OK);
}"
6155,"@Path(""String_Node_Str"") @GET public void listPrivileges(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String principalType,@PathParam(""String_Node_Str"") String principalName) throws Exception {
  ensureSecurityEnabled();
  Principal principal=new Principal(principalName,Principal.PrincipalType.valueOf(principalType.toUpperCase()));
  httpResponder.sendJson(HttpResponseStatus.OK,authorizerInstantiatorService.get().listPrivileges(principal),PRIVILEGE_SET_TYPE,GSON);
  createLogEntry(httpRequest,null,HttpResponseStatus.OK);
}","@Path(""String_Node_Str"") @GET public void listPrivileges(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String principalType,@PathParam(""String_Node_Str"") String principalName) throws Exception {
  ensureSecurityEnabled();
  Principal principal=new Principal(principalName,Principal.PrincipalType.valueOf(principalType.toUpperCase()));
  httpResponder.sendJson(HttpResponseStatus.OK,authorizerInstantiator.get().listPrivileges(principal),PRIVILEGE_SET_TYPE,GSON);
  createLogEntry(httpRequest,null,HttpResponseStatus.OK);
}"
6156,"@Path(""String_Node_Str"") @DELETE public void dropRole(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String roleName) throws Exception {
  ensureSecurityEnabled();
  authorizerInstantiatorService.get().dropRole(new Role(roleName));
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,null,HttpResponseStatus.OK);
}","@Path(""String_Node_Str"") @DELETE public void dropRole(HttpRequest httpRequest,HttpResponder httpResponder,@PathParam(""String_Node_Str"") String roleName) throws Exception {
  ensureSecurityEnabled();
  authorizerInstantiator.get().dropRole(new Role(roleName));
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,null,HttpResponseStatus.OK);
}"
6157,"@Path(""String_Node_Str"") @GET public void listAllRoles(HttpRequest httpRequest,HttpResponder httpResponder) throws Exception {
  ensureSecurityEnabled();
  httpResponder.sendJson(HttpResponseStatus.OK,authorizerInstantiatorService.get().listAllRoles());
  createLogEntry(httpRequest,null,HttpResponseStatus.OK);
}","@Path(""String_Node_Str"") @GET public void listAllRoles(HttpRequest httpRequest,HttpResponder httpResponder) throws Exception {
  ensureSecurityEnabled();
  httpResponder.sendJson(HttpResponseStatus.OK,authorizerInstantiator.get().listAllRoles());
  createLogEntry(httpRequest,null,HttpResponseStatus.OK);
}"
6158,"@Path(""String_Node_Str"") @POST public void grant(HttpRequest httpRequest,HttpResponder httpResponder) throws Exception {
  ensureSecurityEnabled();
  GrantRequest request=parseBody(httpRequest,GrantRequest.class);
  verifyAuthRequest(request);
  Set<Action> actions=request.getActions() == null ? EnumSet.allOf(Action.class) : request.getActions();
  authorizerInstantiatorService.get().enforce(request.getEntity(),SecurityRequestContext.toPrincipal(),Action.ADMIN);
  authorizerInstantiatorService.get().grant(request.getEntity(),request.getPrincipal(),actions);
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,request,HttpResponseStatus.OK);
}","@Path(""String_Node_Str"") @POST public void grant(HttpRequest httpRequest,HttpResponder httpResponder) throws Exception {
  ensureSecurityEnabled();
  GrantRequest request=parseBody(httpRequest,GrantRequest.class);
  verifyAuthRequest(request);
  Set<Action> actions=request.getActions() == null ? EnumSet.allOf(Action.class) : request.getActions();
  authorizerInstantiator.get().enforce(request.getEntity(),SecurityRequestContext.toPrincipal(),Action.ADMIN);
  authorizerInstantiator.get().grant(request.getEntity(),request.getPrincipal(),actions);
  httpResponder.sendStatus(HttpResponseStatus.OK);
  createLogEntry(httpRequest,request,HttpResponseStatus.OK);
}"
6159,"@Inject LocalApplicationManager(CConfiguration configuration,PipelineFactory pipelineFactory,NamespacedLocationFactory namespacedLocationFactory,Store store,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,DatasetFramework datasetFramework,@Named(""String_Node_Str"") DatasetFramework inMemoryDatasetFramework,StreamAdmin streamAdmin,Scheduler scheduler,@Assisted ProgramTerminator programTerminator,MetricStore metricStore,UsageRegistry usageRegistry,ArtifactRepository artifactRepository,MetadataStore metadataStore,AuthorizerInstantiatorService authorizerInstantiatorService){
  this.configuration=configuration;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.pipelineFactory=pipelineFactory;
  this.store=store;
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.programTerminator=programTerminator;
  this.datasetFramework=datasetFramework;
  this.inMemoryDatasetFramework=inMemoryDatasetFramework;
  this.streamAdmin=streamAdmin;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
  this.usageRegistry=usageRegistry;
  this.artifactRepository=artifactRepository;
  this.metadataStore=metadataStore;
  this.authorizerInstantiatorService=authorizerInstantiatorService;
}","@Inject LocalApplicationManager(CConfiguration configuration,PipelineFactory pipelineFactory,NamespacedLocationFactory namespacedLocationFactory,Store store,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,DatasetFramework datasetFramework,@Named(""String_Node_Str"") DatasetFramework inMemoryDatasetFramework,StreamAdmin streamAdmin,Scheduler scheduler,@Assisted ProgramTerminator programTerminator,MetricStore metricStore,UsageRegistry usageRegistry,ArtifactRepository artifactRepository,MetadataStore metadataStore,AuthorizerInstantiator authorizerInstantiator){
  this.configuration=configuration;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.pipelineFactory=pipelineFactory;
  this.store=store;
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.programTerminator=programTerminator;
  this.datasetFramework=datasetFramework;
  this.inMemoryDatasetFramework=inMemoryDatasetFramework;
  this.streamAdmin=streamAdmin;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
  this.usageRegistry=usageRegistry;
  this.artifactRepository=artifactRepository;
  this.metadataStore=metadataStore;
  this.authorizerInstantiator=authorizerInstantiator;
}"
6160,"@Override public ListenableFuture<O> deploy(Id.Namespace namespace,@Nullable String appId,I input) throws Exception {
  Pipeline<O> pipeline=pipelineFactory.getPipeline();
  pipeline.addLast(new LocalArtifactLoaderStage(configuration,store,namespace,appId,artifactRepository));
  pipeline.addLast(new ApplicationVerificationStage(store,datasetFramework));
  pipeline.addLast(new DeployDatasetModulesStage(configuration,namespace,datasetFramework,inMemoryDatasetFramework));
  pipeline.addLast(new CreateDatasetInstancesStage(configuration,datasetFramework,namespace));
  pipeline.addLast(new CreateStreamsStage(namespace,streamAdmin));
  pipeline.addLast(new DeletedProgramHandlerStage(store,programTerminator,streamConsumerFactory,queueAdmin,metricStore,metadataStore,authorizerInstantiatorService.get()));
  pipeline.addLast(new ProgramGenerationStage(configuration,namespacedLocationFactory,authorizerInstantiatorService.get()));
  pipeline.addLast(new ApplicationRegistrationStage(store,usageRegistry));
  pipeline.addLast(new CreateSchedulesStage(scheduler));
  pipeline.addLast(new SystemMetadataWriterStage(metadataStore));
  return pipeline.execute(input);
}","@Override public ListenableFuture<O> deploy(Id.Namespace namespace,@Nullable String appId,I input) throws Exception {
  Pipeline<O> pipeline=pipelineFactory.getPipeline();
  pipeline.addLast(new LocalArtifactLoaderStage(configuration,store,namespace,appId,artifactRepository));
  pipeline.addLast(new ApplicationVerificationStage(store,datasetFramework));
  pipeline.addLast(new DeployDatasetModulesStage(configuration,namespace,datasetFramework,inMemoryDatasetFramework));
  pipeline.addLast(new CreateDatasetInstancesStage(configuration,datasetFramework,namespace));
  pipeline.addLast(new CreateStreamsStage(namespace,streamAdmin));
  pipeline.addLast(new DeletedProgramHandlerStage(store,programTerminator,streamConsumerFactory,queueAdmin,metricStore,metadataStore,authorizerInstantiator.get()));
  pipeline.addLast(new ProgramGenerationStage(configuration,namespacedLocationFactory,authorizerInstantiator.get()));
  pipeline.addLast(new ApplicationRegistrationStage(store,usageRegistry));
  pipeline.addLast(new CreateSchedulesStage(scheduler));
  pipeline.addLast(new SystemMetadataWriterStage(metadataStore));
  return pipeline.execute(input);
}"
6161,"@Override public synchronized void deleteDatasets(Id.Namespace namespaceId) throws Exception {
  if (!exists(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (checkProgramsRunning(namespaceId.toEntityId())) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  authorizerInstantiatorService.get().enforce(namespaceId.toEntityId(),SecurityRequestContext.toPrincipal(),Action.ADMIN);
  try {
    dsFramework.deleteAllInstances(namespaceId);
  }
 catch (  DatasetManagementException|IOException e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
  LOG.debug(""String_Node_Str"",namespaceId);
}","@Override public synchronized void deleteDatasets(Id.Namespace namespaceId) throws Exception {
  if (!exists(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (checkProgramsRunning(namespaceId.toEntityId())) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  authorizerInstantiator.get().enforce(namespaceId.toEntityId(),SecurityRequestContext.toPrincipal(),Action.ADMIN);
  try {
    dsFramework.deleteAllInstances(namespaceId);
  }
 catch (  DatasetManagementException|IOException e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
  LOG.debug(""String_Node_Str"",namespaceId);
}"
6162,"@Override public synchronized void updateProperties(Id.Namespace namespaceId,NamespaceMeta namespaceMeta) throws Exception {
  if (!exists(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  authorizerInstantiatorService.get().enforce(namespaceId.toEntityId(),SecurityRequestContext.toPrincipal(),Action.ADMIN);
  NamespaceMeta metadata=nsStore.get(namespaceId);
  NamespaceMeta.Builder builder=new NamespaceMeta.Builder(metadata);
  if (namespaceMeta.getDescription() != null) {
    builder.setDescription(namespaceMeta.getDescription());
  }
  NamespaceConfig config=namespaceMeta.getConfig();
  if (config != null && !Strings.isNullOrEmpty(config.getSchedulerQueueName())) {
    builder.setSchedulerQueueName(config.getSchedulerQueueName());
  }
  nsStore.update(builder.build());
}","@Override public synchronized void updateProperties(Id.Namespace namespaceId,NamespaceMeta namespaceMeta) throws Exception {
  if (!exists(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  authorizerInstantiator.get().enforce(namespaceId.toEntityId(),SecurityRequestContext.toPrincipal(),Action.ADMIN);
  NamespaceMeta metadata=nsStore.get(namespaceId);
  NamespaceMeta.Builder builder=new NamespaceMeta.Builder(metadata);
  if (namespaceMeta.getDescription() != null) {
    builder.setDescription(namespaceMeta.getDescription());
  }
  NamespaceConfig config=namespaceMeta.getConfig();
  if (config != null && !Strings.isNullOrEmpty(config.getSchedulerQueueName())) {
    builder.setSchedulerQueueName(config.getSchedulerQueueName());
  }
  nsStore.update(builder.build());
}"
6163,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  Principal principal=SecurityRequestContext.toPrincipal();
  if (!(Principal.SYSTEM.equals(principal) && NamespaceId.DEFAULT.equals(namespace))) {
    authorizerInstantiatorService.get().enforce(instanceId,principal,Action.ADMIN);
  }
  try {
    dsFramework.createNamespace(namespace.toId());
  }
 catch (  DatasetManagementException e) {
    throw new NamespaceCannotBeCreatedException(namespace.toId(),e);
  }
  nsStore.create(metadata);
  if (!(Principal.SYSTEM.equals(principal) && NamespaceId.DEFAULT.equals(namespace))) {
    authorizerInstantiatorService.get().grant(namespace,principal,ImmutableSet.of(Action.ALL));
  }
}","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
@Override public synchronized void create(NamespaceMeta metadata) throws Exception {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  NamespaceId namespace=new NamespaceId(metadata.getName());
  if (exists(namespace.toId())) {
    throw new NamespaceAlreadyExistsException(namespace.toId());
  }
  Principal principal=SecurityRequestContext.toPrincipal();
  if (!(Principal.SYSTEM.equals(principal) && NamespaceId.DEFAULT.equals(namespace))) {
    authorizerInstantiator.get().enforce(instanceId,principal,Action.ADMIN);
  }
  try {
    dsFramework.createNamespace(namespace.toId());
  }
 catch (  DatasetManagementException e) {
    throw new NamespaceCannotBeCreatedException(namespace.toId(),e);
  }
  nsStore.create(metadata);
  if (!(Principal.SYSTEM.equals(principal) && NamespaceId.DEFAULT.equals(namespace))) {
    authorizerInstantiator.get().grant(namespace,principal,ImmutableSet.of(Action.ALL));
  }
}"
6164,"@Inject DefaultNamespaceAdmin(Store store,NamespaceStore nsStore,PreferencesStore preferencesStore,DashboardStore dashboardStore,DatasetFramework dsFramework,ProgramRuntimeService runtimeService,QueueAdmin queueAdmin,StreamAdmin streamAdmin,MetricStore metricStore,Scheduler scheduler,ApplicationLifecycleService applicationLifecycleService,ArtifactRepository artifactRepository,AuthorizerInstantiatorService authorizerInstantiatorService,CConfiguration cConf){
  this.queueAdmin=queueAdmin;
  this.streamAdmin=streamAdmin;
  this.store=store;
  this.nsStore=nsStore;
  this.preferencesStore=preferencesStore;
  this.dashboardStore=dashboardStore;
  this.dsFramework=dsFramework;
  this.runtimeService=runtimeService;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
  this.applicationLifecycleService=applicationLifecycleService;
  this.artifactRepository=artifactRepository;
  this.authorizerInstantiatorService=authorizerInstantiatorService;
  this.instanceId=createInstanceId(cConf);
}","@Inject DefaultNamespaceAdmin(Store store,NamespaceStore nsStore,PreferencesStore preferencesStore,DashboardStore dashboardStore,DatasetFramework dsFramework,ProgramRuntimeService runtimeService,QueueAdmin queueAdmin,StreamAdmin streamAdmin,MetricStore metricStore,Scheduler scheduler,ApplicationLifecycleService applicationLifecycleService,ArtifactRepository artifactRepository,AuthorizerInstantiator authorizerInstantiator,CConfiguration cConf){
  this.queueAdmin=queueAdmin;
  this.streamAdmin=streamAdmin;
  this.store=store;
  this.nsStore=nsStore;
  this.preferencesStore=preferencesStore;
  this.dashboardStore=dashboardStore;
  this.dsFramework=dsFramework;
  this.runtimeService=runtimeService;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
  this.applicationLifecycleService=applicationLifecycleService;
  this.artifactRepository=artifactRepository;
  this.authorizerInstantiator=authorizerInstantiator;
  this.instanceId=createInstanceId(cConf);
}"
6165,"/** 
 * Deletes the specified namespace
 * @param namespaceId the {@link Id.Namespace} of the specified namespace
 * @throws NamespaceCannotBeDeletedException if the specified namespace cannot be deleted
 * @throws NamespaceNotFoundException if the specified namespace does not exist
 */
@Override public synchronized void delete(final Id.Namespace namespaceId) throws Exception {
  NamespaceId namespace=namespaceId.toEntityId();
  if (!exists(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (checkProgramsRunning(namespaceId.toEntityId())) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  authorizerInstantiatorService.get().enforce(namespace,SecurityRequestContext.toPrincipal(),Action.ADMIN);
  authorizerInstantiatorService.get().revoke(namespace);
  LOG.info(""String_Node_Str"",namespaceId);
  try {
    preferencesStore.deleteProperties(namespaceId.getId());
    dashboardStore.delete(namespaceId.getId());
    applicationLifecycleService.removeAll(namespaceId);
    scheduler.deleteAllSchedules(namespaceId);
    dsFramework.deleteAllInstances(namespaceId);
    dsFramework.deleteAllModules(namespaceId);
    queueAdmin.dropAllInNamespace(namespaceId);
    streamAdmin.dropAllInNamespace(namespaceId);
    store.removeAll(namespaceId);
    deleteMetrics(namespaceId.toEntityId());
    artifactRepository.clear(namespaceId.toEntityId());
    if (!Id.Namespace.DEFAULT.equals(namespaceId)) {
      nsStore.delete(namespaceId);
      dsFramework.deleteNamespace(namespaceId);
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
  LOG.info(""String_Node_Str"",namespaceId);
}","/** 
 * Deletes the specified namespace
 * @param namespaceId the {@link Id.Namespace} of the specified namespace
 * @throws NamespaceCannotBeDeletedException if the specified namespace cannot be deleted
 * @throws NamespaceNotFoundException if the specified namespace does not exist
 */
@Override public synchronized void delete(final Id.Namespace namespaceId) throws Exception {
  NamespaceId namespace=namespaceId.toEntityId();
  if (!exists(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (checkProgramsRunning(namespaceId.toEntityId())) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  authorizerInstantiator.get().enforce(namespace,SecurityRequestContext.toPrincipal(),Action.ADMIN);
  authorizerInstantiator.get().revoke(namespace);
  LOG.info(""String_Node_Str"",namespaceId);
  try {
    preferencesStore.deleteProperties(namespaceId.getId());
    dashboardStore.delete(namespaceId.getId());
    applicationLifecycleService.removeAll(namespaceId);
    scheduler.deleteAllSchedules(namespaceId);
    dsFramework.deleteAllInstances(namespaceId);
    dsFramework.deleteAllModules(namespaceId);
    queueAdmin.dropAllInNamespace(namespaceId);
    streamAdmin.dropAllInNamespace(namespaceId);
    store.removeAll(namespaceId);
    deleteMetrics(namespaceId.toEntityId());
    artifactRepository.clear(namespaceId.toEntityId());
    if (!Id.Namespace.DEFAULT.equals(namespaceId)) {
      nsStore.delete(namespaceId);
      dsFramework.deleteNamespace(namespaceId);
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
  LOG.info(""String_Node_Str"",namespaceId);
}"
6166,"/** 
 * Helper method to get   {@link ProgramId} for a RunRecord for type of program
 * @param programType Type of program to search
 * @param runId The target id of the {@link RunRecord} to find
 * @return the program id of the run record or {@code null} if does not exist.
 */
@Nullable private ProgramId retrieveProgramIdForRunRecord(ProgramType programType,String runId){
  List<NamespaceMeta> namespaceMetas=nsStore.list();
  ProgramId targetProgramId=null;
  for (  NamespaceMeta nm : namespaceMetas) {
    NamespaceId namespace=Ids.namespace(nm.getName());
    Collection<ApplicationSpecification> appSpecs=store.getAllApplications(namespace.toId());
    for (    ApplicationSpecification appSpec : appSpecs) {
switch (programType) {
case FLOW:
        for (        String programName : appSpec.getFlows().keySet()) {
          ProgramId programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
          if (programId != null) {
            targetProgramId=programId;
            break;
          }
        }
      break;
case MAPREDUCE:
    for (    String programName : appSpec.getMapReduce().keySet()) {
      ProgramId programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
      if (programId != null) {
        targetProgramId=programId;
        break;
      }
    }
  break;
case SPARK:
for (String programName : appSpec.getSpark().keySet()) {
  ProgramId programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
  if (programId != null) {
    targetProgramId=programId;
    break;
  }
}
break;
case SERVICE:
for (String programName : appSpec.getServices().keySet()) {
ProgramId programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case WORKER:
for (String programName : appSpec.getWorkers().keySet()) {
ProgramId programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case WORKFLOW:
for (String programName : appSpec.getWorkflows().keySet()) {
ProgramId programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
default :
LOG.debug(""String_Node_Str"" + programType.name());
break;
}
if (targetProgramId != null) {
break;
}
}
if (targetProgramId != null) {
break;
}
}
return targetProgramId;
}","/** 
 * Helper method to get   {@link ProgramId} for a RunRecord for type of program
 * @param programType Type of program to search
 * @param runId The target id of the {@link RunRecord} to find
 * @return the program id of the run record or {@code null} if does not exist.
 */
@Nullable private ProgramId retrieveProgramIdForRunRecord(ProgramType programType,String runId){
  List<NamespaceMeta> namespaceMetas=nsStore.list();
  ProgramId targetProgramId=null;
  for (  NamespaceMeta nm : namespaceMetas) {
    NamespaceId namespace=Ids.namespace(nm.getName());
    Collection<ApplicationSpecification> appSpecs=store.getAllApplications(namespace.toId());
    for (    ApplicationSpecification appSpec : appSpecs) {
switch (programType) {
case FLOW:
        for (        String programName : appSpec.getFlows().keySet()) {
          ProgramId programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
          if (programId != null) {
            targetProgramId=programId;
            break;
          }
        }
      break;
case MAPREDUCE:
    for (    String programName : appSpec.getMapReduce().keySet()) {
      ProgramId programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
      if (programId != null) {
        targetProgramId=programId;
        break;
      }
    }
  break;
case SPARK:
for (String programName : appSpec.getSpark().keySet()) {
  ProgramId programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
  if (programId != null) {
    targetProgramId=programId;
    break;
  }
}
break;
case SERVICE:
for (String programName : appSpec.getServices().keySet()) {
ProgramId programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case WORKER:
for (String programName : appSpec.getWorkers().keySet()) {
ProgramId programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case WORKFLOW:
for (String programName : appSpec.getWorkflows().keySet()) {
ProgramId programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case CUSTOM_ACTION:
case WEBAPP:
break;
default :
LOG.debug(""String_Node_Str"" + programType.name());
break;
}
if (targetProgramId != null) {
break;
}
}
if (targetProgramId != null) {
break;
}
}
return targetProgramId;
}"
6167,"@Test public void testCompatibleType() throws SerDeException, IOException {
  TextStringMapHolder o1=new TextStringMapHolder();
  StructObjectInspector oi1=(StructObjectInspector)ObjectInspectorFactory.getReflectionObjectInspector(TextStringMapHolder.class);
  LazySimpleSerDe serde=new LazySimpleSerDe();
  Configuration conf=new Configuration();
  Properties tbl=new Properties();
  tbl.setProperty(serdeConstants.LIST_COLUMNS,ObjectInspectorUtils.getFieldNames(oi1));
  tbl.setProperty(serdeConstants.LIST_COLUMN_TYPES,ObjectInspectorUtils.getFieldTypes(oi1));
  SerDeParameters serdeParams=LazySimpleSerDe.initSerdeParams(conf,tbl,LazySimpleSerDe.class.getName());
  serde.initialize(conf,tbl);
  ObjectInspector oi2=serde.getObjectInspector();
  Object o2=serializeAndDeserialize(o1,oi1,serde,serdeParams);
  int rc=ObjectInspectorUtils.compare(o1,oi1,o2,oi2,new SimpleMapEqualComparer());
  Assert.assertEquals(0,rc);
  o1.mMap.put(new Text(""String_Node_Str""),""String_Node_Str"");
  o1.mMap.put(new Text(""String_Node_Str""),""String_Node_Str"");
  o2=serializeAndDeserialize(o1,oi1,serde,serdeParams);
  rc=ObjectInspectorUtils.compare(o1,oi1,o2,oi2,new SimpleMapEqualComparer());
  Assert.assertEquals(0,rc);
  o1.mMap.put(new Text(""String_Node_Str""),""String_Node_Str"");
  rc=ObjectInspectorUtils.compare(o1,oi1,o2,oi2,new SimpleMapEqualComparer());
  Assert.assertFalse(0 == rc);
}","@Test public void testCompatibleType() throws SerDeException, IOException {
  TextStringMapHolder o1=new TextStringMapHolder();
  StructObjectInspector oi1=(StructObjectInspector)ObjectInspectorFactory.getReflectionObjectInspector(TextStringMapHolder.class);
  LazySimpleSerDe serde=new LazySimpleSerDe();
  Configuration conf=new Configuration();
  Properties tbl=new Properties();
  tbl.setProperty(serdeConstants.LIST_COLUMNS,ObjectInspectorUtils.getFieldNames(oi1));
  tbl.setProperty(serdeConstants.LIST_COLUMN_TYPES,ObjectInspectorUtils.getFieldTypes(oi1));
  LazySerDeParameters serdeParams=new LazySerDeParameters(conf,tbl,LazySimpleSerDe.class.getName());
  serde.initialize(conf,tbl);
  ObjectInspector oi2=serde.getObjectInspector();
  Object o2=serializeAndDeserialize(o1,oi1,serde,serdeParams);
  int rc=ObjectInspectorUtils.compare(o1,oi1,o2,oi2,new SimpleMapEqualComparer());
  Assert.assertEquals(0,rc);
  o1.mMap.put(new Text(""String_Node_Str""),""String_Node_Str"");
  o1.mMap.put(new Text(""String_Node_Str""),""String_Node_Str"");
  o2=serializeAndDeserialize(o1,oi1,serde,serdeParams);
  rc=ObjectInspectorUtils.compare(o1,oi1,o2,oi2,new SimpleMapEqualComparer());
  Assert.assertEquals(0,rc);
  o1.mMap.put(new Text(""String_Node_Str""),""String_Node_Str"");
  rc=ObjectInspectorUtils.compare(o1,oi1,o2,oi2,new SimpleMapEqualComparer());
  Assert.assertFalse(0 == rc);
}"
6168,"Object serializeAndDeserialize(StringTextMapHolder o1,StructObjectInspector oi1,LazySimpleSerDe serde,SerDeParameters serdeParams) throws IOException, SerDeException {
  ByteStream.Output serializeStream=new ByteStream.Output();
  LazySimpleSerDe.serialize(serializeStream,o1,oi1,serdeParams.getSeparators(),0,serdeParams.getNullSequence(),serdeParams.isEscaped(),serdeParams.getEscapeChar(),serdeParams.getNeedsEscape());
  Text t=new Text(serializeStream.toByteArray());
  return serde.deserialize(t);
}","Object serializeAndDeserialize(StringTextMapHolder o1,StructObjectInspector oi1,LazySimpleSerDe serde,LazySerDeParameters serdeParams) throws IOException, SerDeException {
  ByteStream.Output serializeStream=new ByteStream.Output();
  LazySimpleSerDe.serialize(serializeStream,o1,oi1,serdeParams.getSeparators(),0,serdeParams.getNullSequence(),serdeParams.isEscaped(),serdeParams.getEscapeChar(),serdeParams.getNeedsEscape());
  Text t=new Text(serializeStream.toByteArray());
  return serde.deserialize(t);
}"
6169,"@Test public void testIncompatibleType() throws SerDeException, IOException {
  StringTextMapHolder o1=new StringTextMapHolder();
  StructObjectInspector oi1=(StructObjectInspector)ObjectInspectorFactory.getReflectionObjectInspector(StringTextMapHolder.class);
  LazySimpleSerDe serde=new LazySimpleSerDe();
  Configuration conf=new Configuration();
  Properties tbl=new Properties();
  tbl.setProperty(serdeConstants.LIST_COLUMNS,ObjectInspectorUtils.getFieldNames(oi1));
  tbl.setProperty(serdeConstants.LIST_COLUMN_TYPES,ObjectInspectorUtils.getFieldTypes(oi1));
  SerDeParameters serdeParams=LazySimpleSerDe.initSerdeParams(conf,tbl,LazySimpleSerDe.class.getName());
  serde.initialize(conf,tbl);
  ObjectInspector oi2=serde.getObjectInspector();
  Object o2=serializeAndDeserialize(o1,oi1,serde,serdeParams);
  int rc=ObjectInspectorUtils.compare(o1,oi1,o2,oi2,new SimpleMapEqualComparer());
  Assert.assertEquals(0,rc);
  o1.mMap.put(""String_Node_Str"",new Text(""String_Node_Str""));
  o1.mMap.put(""String_Node_Str"",new Text(""String_Node_Str""));
  o2=serializeAndDeserialize(o1,oi1,serde,serdeParams);
  rc=ObjectInspectorUtils.compare(o1,oi1,o2,oi2,new SimpleMapEqualComparer());
  Assert.assertFalse(0 == rc);
}","@Test public void testIncompatibleType() throws SerDeException, IOException {
  StringTextMapHolder o1=new StringTextMapHolder();
  StructObjectInspector oi1=(StructObjectInspector)ObjectInspectorFactory.getReflectionObjectInspector(StringTextMapHolder.class);
  LazySimpleSerDe serde=new LazySimpleSerDe();
  Configuration conf=new Configuration();
  Properties tbl=new Properties();
  tbl.setProperty(serdeConstants.LIST_COLUMNS,ObjectInspectorUtils.getFieldNames(oi1));
  tbl.setProperty(serdeConstants.LIST_COLUMN_TYPES,ObjectInspectorUtils.getFieldTypes(oi1));
  LazySerDeParameters serdeParams=new LazySerDeParameters(conf,tbl,LazySimpleSerDe.class.getName());
  serde.initialize(conf,tbl);
  ObjectInspector oi2=serde.getObjectInspector();
  Object o2=serializeAndDeserialize(o1,oi1,serde,serdeParams);
  int rc=ObjectInspectorUtils.compare(o1,oi1,o2,oi2,new SimpleMapEqualComparer());
  Assert.assertEquals(0,rc);
  o1.mMap.put(""String_Node_Str"",new Text(""String_Node_Str""));
  o1.mMap.put(""String_Node_Str"",new Text(""String_Node_Str""));
  o2=serializeAndDeserialize(o1,oi1,serde,serdeParams);
  rc=ObjectInspectorUtils.compare(o1,oi1,o2,oi2,new SimpleMapEqualComparer());
  Assert.assertFalse(0 == rc);
}"
6170,"@Override protected void before() throws Throwable {
  tmpFolder.create();
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  cConf.set(Constants.Router.ADDRESS,getLocalHostname());
  cConf.setInt(Constants.Router.ROUTER_PORT,Networks.getRandomPort());
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  cConf.setBoolean(Constants.Explore.EXPLORE_ENABLED,true);
  cConf.setBoolean(Constants.Explore.START_ON_DEMAND,true);
  cConf.setBoolean(StandaloneMain.DISABLE_UI,true);
  for (int i=0; i < configs.length; i+=2) {
    cConf.set(configs[i].toString(),configs[i + 1].toString());
  }
  this.cConf=cConf;
  standaloneMain=StandaloneMain.create(cConf,new Configuration());
  standaloneMain.startUp();
  try {
    waitForStandalone();
  }
 catch (  Throwable t) {
    standaloneMain.shutDown();
    throw t;
  }
}","@Override protected void before() throws Throwable {
  tmpFolder.create();
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  cConf.set(Constants.Router.ADDRESS,getLocalHostname());
  cConf.setInt(Constants.Router.ROUTER_PORT,Networks.getRandomPort());
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  cConf.setBoolean(Constants.Explore.EXPLORE_ENABLED,true);
  cConf.setBoolean(Constants.Explore.START_ON_DEMAND,true);
  cConf.setBoolean(StandaloneMain.DISABLE_UI,true);
  cConf.setBoolean(Constants.Audit.ENABLED,false);
  for (int i=0; i < configs.length; i+=2) {
    cConf.set(configs[i].toString(),configs[i + 1].toString());
  }
  this.cConf=cConf;
  standaloneMain=StandaloneMain.create(cConf,new Configuration());
  standaloneMain.startUp();
  try {
    waitForStandalone();
  }
 catch (  Throwable t) {
    standaloneMain.shutDown();
    throw t;
  }
}"
6171,"@Override protected void runOneIteration() throws Exception {
  for (  Map.Entry<Id.Namespace,StreamSpecification> streamSpecEntry : streamMetaStore.listStreams().entries()) {
    Id.Stream streamId=Id.Stream.from(streamSpecEntry.getKey(),streamSpecEntry.getValue().getName());
    StreamSizeAggregator streamSizeAggregator=aggregators.get(streamId);
    try {
      if (streamSizeAggregator == null) {
        StreamConfig config=streamAdmin.getConfig(streamId);
        streamSizeAggregator=createSizeAggregator(streamId,0,config.getNotificationThresholdMB());
      }
      streamSizeAggregator.checkAggregatedSize();
    }
 catch (    Exception e) {
      LOG.warn(""String_Node_Str"",streamId,e);
    }
  }
}","@Override protected void runOneIteration() throws Exception {
  for (  Map.Entry<Id.Namespace,StreamSpecification> streamSpecEntry : streamMetaStore.listStreams().entries()) {
    Id.Stream streamId=Id.Stream.from(streamSpecEntry.getKey(),streamSpecEntry.getValue().getName());
    StreamSizeAggregator streamSizeAggregator=aggregators.get(streamId);
    try {
      if (streamSizeAggregator == null) {
        StreamConfig config;
        try {
          config=streamAdmin.getConfig(streamId);
        }
 catch (        FileNotFoundException e) {
          continue;
        }
        streamSizeAggregator=createSizeAggregator(streamId,0,config.getNotificationThresholdMB());
      }
      streamSizeAggregator.checkAggregatedSize();
    }
 catch (    Exception e) {
      LOG.warn(""String_Node_Str"",streamId,e);
    }
  }
}"
6172,"@Override protected void initialize() throws Exception {
  for (  Map.Entry<Id.Namespace,StreamSpecification> streamSpecEntry : streamMetaStore.listStreams().entries()) {
    Id.Stream streamId=Id.Stream.from(streamSpecEntry.getKey(),streamSpecEntry.getValue().getName());
    StreamConfig config=streamAdmin.getConfig(streamId);
    long eventsSizes=getStreamEventsSize(streamId);
    createSizeAggregator(streamId,eventsSizes,config.getNotificationThresholdMB());
  }
}","@Override protected void initialize() throws Exception {
  for (  Map.Entry<Id.Namespace,StreamSpecification> streamSpecEntry : streamMetaStore.listStreams().entries()) {
    Id.Stream streamId=Id.Stream.from(streamSpecEntry.getKey(),streamSpecEntry.getValue().getName());
    StreamConfig config;
    try {
      config=streamAdmin.getConfig(streamId);
    }
 catch (    FileNotFoundException e) {
      LOG.warn(""String_Node_Str"" + ""String_Node_Str"",streamId);
      continue;
    }
catch (    Exception e) {
      LOG.warn(""String_Node_Str"" + ""String_Node_Str"",streamId,e);
      continue;
    }
    long eventsSizes=getStreamEventsSize(streamId);
    createSizeAggregator(streamId,eventsSizes,config.getNotificationThresholdMB());
  }
}"
6173,"@DELETE @Path(""String_Node_Str"") public void delete(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream) throws Exception {
  Id.Stream streamId=Id.Stream.from(namespaceId,stream);
  checkStreamExists(streamId);
  streamAdmin.drop(streamId);
  responder.sendStatus(HttpResponseStatus.OK);
}","@DELETE @Path(""String_Node_Str"") public void delete(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream) throws Exception {
  Id.Stream streamId=Id.Stream.from(namespaceId,stream);
  checkStreamExists(streamId);
  streamWriter.close(streamId);
  streamAdmin.drop(streamId);
  responder.sendStatus(HttpResponseStatus.OK);
}"
6174,"@Override public StreamConfig getConfig(Id.Stream streamId) throws IOException {
  Location configLocation=getConfigLocation(streamId);
  Preconditions.checkArgument(configLocation.exists(),""String_Node_Str"",streamId);
  StreamConfig config=GSON.fromJson(CharStreams.toString(CharStreams.newReaderSupplier(Locations.newInputSupplier(configLocation),Charsets.UTF_8)),StreamConfig.class);
  int threshold=config.getNotificationThresholdMB();
  if (threshold <= 0) {
    threshold=cConf.getInt(Constants.Stream.NOTIFICATION_THRESHOLD);
  }
  return new StreamConfig(streamId,config.getPartitionDuration(),config.getIndexInterval(),config.getTTL(),getStreamLocation(streamId),config.getFormat(),threshold);
}","@Override public StreamConfig getConfig(Id.Stream streamId) throws IOException {
  Location configLocation=getConfigLocation(streamId);
  if (!configLocation.exists()) {
    throw new FileNotFoundException(String.format(""String_Node_Str"",configLocation.toURI().getPath(),streamId));
  }
  StreamConfig config=GSON.fromJson(CharStreams.toString(CharStreams.newReaderSupplier(Locations.newInputSupplier(configLocation),Charsets.UTF_8)),StreamConfig.class);
  int threshold=config.getNotificationThresholdMB();
  if (threshold <= 0) {
    threshold=cConf.getInt(Constants.Stream.NOTIFICATION_THRESHOLD);
  }
  return new StreamConfig(streamId,config.getPartitionDuration(),config.getIndexInterval(),config.getTTL(),getStreamLocation(streamId),config.getFormat(),threshold);
}"
6175,"private static MetricsContext createMetricsContext(Program program,String runId,MetricsCollectionService service,@Nullable WorkflowProgramInfo workflowProgramInfo){
  Map<String,String> tags=Maps.newHashMap();
  tags.putAll(getMetricsContext(program,runId));
  MetricsContext programMetricsContext=service.getContext(tags);
  if (workflowProgramInfo == null) {
    return programMetricsContext;
  }
  tags=Maps.newHashMap();
  tags.put(Constants.Metrics.Tag.NAMESPACE,program.getNamespaceId());
  tags.put(Constants.Metrics.Tag.APP,program.getApplicationId());
  tags.put(Constants.Metrics.Tag.WORKFLOW,workflowProgramInfo.getName());
  tags.put(Constants.Metrics.Tag.RUN_ID,workflowProgramInfo.getRunId().getId());
  tags.put(Constants.Metrics.Tag.NODE,workflowProgramInfo.getNodeId());
  return new MultiMetricsContext(programMetricsContext,service.getContext(tags));
}","private static MetricsContext createMetricsContext(Program program,String runId,MetricsCollectionService service,@Nullable WorkflowProgramInfo workflowProgramInfo){
  Map<String,String> tags=Maps.newHashMap();
  tags.putAll(getMetricsContext(program,runId));
  if (workflowProgramInfo != null) {
    tags.put(Constants.Metrics.Tag.WORKFLOW,workflowProgramInfo.getName());
    tags.put(Constants.Metrics.Tag.WORKFLOW_RUN_ID,workflowProgramInfo.getRunId().getId());
    tags.put(Constants.Metrics.Tag.NODE,workflowProgramInfo.getNodeId());
  }
  return service.getContext(tags);
}"
6176,"private static MetricsContext createMetricsContext(Program program,String runId,MetricsCollectionService service,String taskId,@Nullable MapReduceMetrics.TaskType type,@Nullable WorkflowProgramInfo workflowProgramInfo){
  Map<String,String> tags=Maps.newHashMap();
  tags.putAll(getMetricsContext(program,runId));
  if (type != null) {
    tags.put(Constants.Metrics.Tag.MR_TASK_TYPE,type.getId());
    tags.put(Constants.Metrics.Tag.INSTANCE_ID,taskId);
  }
  MetricsContext programMetricsContext=service.getContext(tags);
  if (workflowProgramInfo == null) {
    return programMetricsContext;
  }
  tags=Maps.newHashMap();
  tags.put(Constants.Metrics.Tag.NAMESPACE,program.getNamespaceId());
  tags.put(Constants.Metrics.Tag.APP,program.getApplicationId());
  tags.put(Constants.Metrics.Tag.WORKFLOW,workflowProgramInfo.getName());
  tags.put(Constants.Metrics.Tag.RUN_ID,workflowProgramInfo.getRunId().getId());
  tags.put(Constants.Metrics.Tag.NODE,workflowProgramInfo.getNodeId());
  return new MultiMetricsContext(programMetricsContext,service.getContext(tags));
}","private static MetricsContext createMetricsContext(Program program,String runId,MetricsCollectionService service,String taskId,@Nullable MapReduceMetrics.TaskType type,@Nullable WorkflowProgramInfo workflowProgramInfo){
  Map<String,String> tags=Maps.newHashMap();
  tags.putAll(getMetricsContext(program,runId));
  if (type != null) {
    tags.put(Constants.Metrics.Tag.MR_TASK_TYPE,type.getId());
    tags.put(Constants.Metrics.Tag.INSTANCE_ID,taskId);
  }
  if (workflowProgramInfo != null) {
    tags.put(Constants.Metrics.Tag.WORKFLOW,workflowProgramInfo.getName());
    tags.put(Constants.Metrics.Tag.WORKFLOW_RUN_ID,workflowProgramInfo.getRunId().getId());
    tags.put(Constants.Metrics.Tag.NODE,workflowProgramInfo.getNodeId());
  }
  return service.getContext(tags);
}"
6177,"@Nullable private static MetricsContext getMetricCollector(Program program,String runId,@Nullable MetricsCollectionService service){
  if (service == null) {
    return null;
  }
  Map<String,String> tags=Maps.newHashMap(getMetricsContext(program,runId));
  return service.getContext(tags);
}","@Nullable private static MetricsContext getMetricCollector(Program program,String runId,@Nullable MetricsCollectionService service){
  if (service == null) {
    return null;
  }
  Map<String,String> tags=Maps.newHashMap();
  tags.put(Constants.Metrics.Tag.NAMESPACE,program.getNamespaceId());
  tags.put(Constants.Metrics.Tag.APP,program.getApplicationId());
  tags.put(ProgramTypeMetricTag.getTagName(program.getType()),program.getName());
  tags.put(Constants.Metrics.Tag.WORKFLOW_RUN_ID,runId);
  return service.getContext(tags);
}"
6178,"public CubeDataset(String name,MetricsTable entityTable,Map<Integer,Table> resolutionTables,Map<String,? extends Aggregation> aggregations){
  super(name,entityTable,resolutionTables.values().toArray(new Dataset[resolutionTables.values().size()]));
  this.entityTable=entityTable;
  this.resolutionTables=resolutionTables;
  int[] resolutions=new int[resolutionTables.keySet().size()];
  int index=0;
  for (  Integer resolution : resolutionTables.keySet()) {
    resolutions[index++]=resolution;
  }
  this.cube=new DefaultCube(resolutions,new FactTableSupplierImpl(entityTable,resolutionTables),aggregations);
}","public CubeDataset(String name,MetricsTable entityTable,Map<Integer,Table> resolutionTables,Map<String,? extends Aggregation> aggregations){
  super(name,entityTable,resolutionTables.values().toArray(new Dataset[resolutionTables.values().size()]));
  this.entityTable=entityTable;
  this.resolutionTables=resolutionTables;
  int[] resolutions=new int[resolutionTables.keySet().size()];
  int index=0;
  for (  Integer resolution : resolutionTables.keySet()) {
    resolutions[index++]=resolution;
  }
  this.cube=new DefaultCube(resolutions,new FactTableSupplierImpl(entityTable,resolutionTables),aggregations,ImmutableMap.<String,AggregationAlias>of());
}"
6179,"public DefaultCube(int[] resolutions,FactTableSupplier factTableSupplier,Map<String,? extends Aggregation> aggregations){
  this.aggregations=aggregations;
  this.resolutionToFactTable=Maps.newHashMap();
  for (  int resolution : resolutions) {
    resolutionToFactTable.put(resolution,factTableSupplier.get(resolution,3600));
  }
}","public DefaultCube(int[] resolutions,FactTableSupplier factTableSupplier,Map<String,? extends Aggregation> aggregations,Map<String,AggregationAlias> aggregationAliasMap){
  this.aggregations=aggregations;
  this.resolutionToFactTable=Maps.newHashMap();
  for (  int resolution : resolutions) {
    resolutionToFactTable.put(resolution,factTableSupplier.get(resolution,3600));
  }
  this.aggregationAliasMap=aggregationAliasMap;
}"
6180,"@Override public void add(Collection<? extends CubeFact> facts){
  List<Fact> toWrite=Lists.newArrayList();
  int dimValuesCount=0;
  for (  CubeFact fact : facts) {
    for (    Aggregation agg : aggregations.values()) {
      if (agg.accept(fact)) {
        List<DimensionValue> dimensionValues=Lists.newArrayList();
        for (        String dimensionName : agg.getDimensionNames()) {
          dimensionValues.add(new DimensionValue(dimensionName,fact.getDimensionValues().get(dimensionName)));
          dimValuesCount++;
        }
        toWrite.add(new Fact(fact.getTimestamp(),dimensionValues,fact.getMeasurements()));
      }
    }
  }
  for (  FactTable table : resolutionToFactTable.values()) {
    table.add(toWrite);
  }
  incrementMetric(""String_Node_Str"",1);
  incrementMetric(""String_Node_Str"",facts.size());
  incrementMetric(""String_Node_Str"",toWrite.size());
  incrementMetric(""String_Node_Str"",dimValuesCount);
  incrementMetric(""String_Node_Str"",toWrite.size() * resolutionToFactTable.size());
}","@Override public void add(Collection<? extends CubeFact> facts){
  List<Fact> toWrite=Lists.newArrayList();
  int dimValuesCount=0;
  for (  CubeFact fact : facts) {
    for (    Map.Entry<String,? extends Aggregation> aggEntry : aggregations.entrySet()) {
      Aggregation agg=aggEntry.getValue();
      AggregationAlias aggregationAlias=null;
      if (aggregationAliasMap.containsKey(aggEntry.getKey())) {
        aggregationAlias=aggregationAliasMap.get(aggEntry.getKey());
      }
      if (agg.accept(fact)) {
        List<DimensionValue> dimensionValues=Lists.newArrayList();
        for (        String dimensionName : agg.getDimensionNames()) {
          String dimensionValueKey=aggregationAlias == null ? dimensionName : aggregationAlias.getAlias(dimensionName);
          dimensionValues.add(new DimensionValue(dimensionName,fact.getDimensionValues().get(dimensionValueKey)));
          dimValuesCount++;
        }
        toWrite.add(new Fact(fact.getTimestamp(),dimensionValues,fact.getMeasurements()));
      }
    }
  }
  for (  FactTable table : resolutionToFactTable.values()) {
    table.add(toWrite);
  }
  incrementMetric(""String_Node_Str"",1);
  incrementMetric(""String_Node_Str"",facts.size());
  incrementMetric(""String_Node_Str"",toWrite.size());
  incrementMetric(""String_Node_Str"",dimValuesCount);
  incrementMetric(""String_Node_Str"",toWrite.size() * resolutionToFactTable.size());
}"
6181,"@Override protected Cube getCube(final String name,int[] resolutions,Map<String,? extends Aggregation> aggregations){
  FactTableSupplier supplier=new FactTableSupplier(){
    @Override public FactTable get(    int resolution,    int rollTime){
      String entityTableName=""String_Node_Str"" + name;
      InMemoryTableService.create(entityTableName);
      String dataTableName=""String_Node_Str"" + name + ""String_Node_Str""+ resolution;
      InMemoryTableService.create(dataTableName);
      return new FactTable(new InMemoryMetricsTable(dataTableName),new EntityTable(new InMemoryMetricsTable(entityTableName)),resolution,rollTime);
    }
  }
;
  return new DefaultCube(resolutions,supplier,aggregations);
}","@Override protected Cube getCube(final String name,int[] resolutions,Map<String,? extends Aggregation> aggregations){
  FactTableSupplier supplier=new FactTableSupplier(){
    @Override public FactTable get(    int resolution,    int rollTime){
      String entityTableName=""String_Node_Str"" + name;
      InMemoryTableService.create(entityTableName);
      String dataTableName=""String_Node_Str"" + name + ""String_Node_Str""+ resolution;
      InMemoryTableService.create(dataTableName);
      return new FactTable(new InMemoryMetricsTable(dataTableName),new EntityTable(new InMemoryMetricsTable(entityTableName)),resolution,rollTime);
    }
  }
;
  return new DefaultCube(resolutions,supplier,aggregations,ImmutableMap.<String,AggregationAlias>of());
}"
6182,"@Override public Map<String,WorkflowNodeStateDetail> getWorkflowNodeStates(ProgramRunId workflowRunId) throws NotFoundException {
  try {
    return workflowClient.getWorkflowNodeStates(workflowRunId);
  }
 catch (  IOException|UnauthenticatedException e) {
    throw Throwables.propagate(e);
  }
}","@Override public Map<String,WorkflowNodeStateDetail> getWorkflowNodeStates(String workflowRunId) throws NotFoundException {
  try {
    ProgramRunId programRunId=new ProgramRunId(workflowId.getNamespaceId(),workflowId.getApplicationId(),workflowId.getType(),workflowId.getId(),workflowRunId);
    return workflowClient.getWorkflowNodeStates(programRunId);
  }
 catch (  IOException|UnauthenticatedException e) {
    throw Throwables.propagate(e);
  }
}"
6183,"/** 
 * Creates a   {@link MetricsContext} to be used for the Spark execution.
 */
private static MetricsContext createMetricsContext(MetricsCollectionService service,ProgramId programId,RunId runId,@Nullable WorkflowProgramInfo workflowProgramInfo){
  Map<String,String> tags=Maps.newHashMap();
  tags.put(Constants.Metrics.Tag.NAMESPACE,programId.getNamespace());
  tags.put(Constants.Metrics.Tag.APP,programId.getApplication());
  tags.put(ProgramTypeMetricTag.getTagName(ProgramType.SPARK),programId.getProgram());
  tags.put(Constants.Metrics.Tag.RUN_ID,runId.getId());
  tags.put(Constants.Metrics.Tag.INSTANCE_ID,""String_Node_Str"");
  MetricsContext programMetricsContext=service.getContext(tags);
  if (workflowProgramInfo == null) {
    return programMetricsContext;
  }
  tags=Maps.newHashMap();
  tags.put(Constants.Metrics.Tag.NAMESPACE,programId.getNamespace());
  tags.put(Constants.Metrics.Tag.APP,programId.getApplication());
  tags.put(Constants.Metrics.Tag.WORKFLOW,workflowProgramInfo.getName());
  tags.put(Constants.Metrics.Tag.RUN_ID,workflowProgramInfo.getRunId().getId());
  tags.put(Constants.Metrics.Tag.NODE,workflowProgramInfo.getNodeId());
  return new MultiMetricsContext(programMetricsContext,service.getContext(tags));
}","/** 
 * Creates a   {@link MetricsContext} to be used for the Spark execution.
 */
private static MetricsContext createMetricsContext(MetricsCollectionService service,ProgramId programId,RunId runId,@Nullable WorkflowProgramInfo workflowProgramInfo){
  Map<String,String> tags=Maps.newHashMap();
  tags.put(Constants.Metrics.Tag.NAMESPACE,programId.getNamespace());
  tags.put(Constants.Metrics.Tag.APP,programId.getApplication());
  tags.put(ProgramTypeMetricTag.getTagName(ProgramType.SPARK),programId.getProgram());
  tags.put(Constants.Metrics.Tag.RUN_ID,runId.getId());
  tags.put(Constants.Metrics.Tag.INSTANCE_ID,""String_Node_Str"");
  if (workflowProgramInfo != null) {
    tags.put(Constants.Metrics.Tag.WORKFLOW,workflowProgramInfo.getName());
    tags.put(Constants.Metrics.Tag.WORKFLOW_RUN_ID,workflowProgramInfo.getRunId().getId());
    tags.put(Constants.Metrics.Tag.NODE,workflowProgramInfo.getNodeId());
  }
  return service.getContext(tags);
}"
6184,"/** 
 * Get node stated for the specified Workflow run.
 * @param workflowRunId the Workflow run for which node states to be returned
 * @return {@link Map} of node name to the {@link WorkflowNodeStateDetail}
 * @throws NotFoundException when the specified Workflow run is not found
 */
Map<String,WorkflowNodeStateDetail> getWorkflowNodeStates(ProgramRunId workflowRunId) throws NotFoundException ;","/** 
 * Get node stated for the specified Workflow run.
 * @param workflowRunId the Workflow run for which node states to be returned
 * @return {@link Map} of node name to the {@link WorkflowNodeStateDetail}
 * @throws NotFoundException when the specified Workflow run is not found
 */
Map<String,WorkflowNodeStateDetail> getWorkflowNodeStates(String workflowRunId) throws NotFoundException ;"
6185,"@Override public Map<String,WorkflowNodeStateDetail> getWorkflowNodeStates(ProgramRunId workflowRunId) throws NotFoundException {
  return appFabricClient.getWorkflowNodeStates(workflowRunId);
}","@Override public Map<String,WorkflowNodeStateDetail> getWorkflowNodeStates(String workflowRunId) throws NotFoundException {
  return appFabricClient.getWorkflowNodeStates(new ProgramRunId(programId.getNamespaceId(),programId.getApplicationId(),programId.getType(),programId.getId(),workflowRunId));
}"
6186,"public DefaultWorkflowManager(Id.Program programId,AppFabricClient appFabricClient,DefaultApplicationManager applicationManager){
  super(programId,applicationManager);
  this.appFabricClient=appFabricClient;
}","public DefaultWorkflowManager(Id.Program programId,AppFabricClient appFabricClient,DefaultApplicationManager applicationManager){
  super(programId,applicationManager);
  this.programId=programId;
  this.appFabricClient=appFabricClient;
}"
6187,"private String executeWorkflow(ApplicationManager applicationManager,Map<String,String> additionalParams) throws Exception {
  WorkflowManager wfManager=applicationManager.getWorkflowManager(WorkflowAppWithLocalDatasets.WORKFLOW_NAME);
  Map<String,String> runtimeArgs=new HashMap<>();
  File waitFile=new File(TMP_FOLDER.newFolder(),""String_Node_Str"");
  File doneFile=new File(TMP_FOLDER.newFolder(),""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",waitFile.getAbsolutePath());
  runtimeArgs.put(""String_Node_Str"",doneFile.getAbsolutePath());
  runtimeArgs.putAll(additionalParams);
  wfManager.start(runtimeArgs);
  while (!waitFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  List<RunRecord> history=wfManager.getHistory(ProgramRunStatus.RUNNING);
  Assert.assertEquals(1,history.size());
  String runId=history.get(0).getPid();
  DataSetManager<KeyValueTable> localDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.WORDCOUNT_DATASET + ""String_Node_Str"" + runId);
  Assert.assertEquals(""String_Node_Str"",Bytes.toString(localDataset.get().read(""String_Node_Str"")));
  DataSetManager<FileSet> fileSetDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.CSV_FILESET_DATASET + ""String_Node_Str"" + runId);
  Assert.assertNotNull(fileSetDataset.get());
  localDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.WORDCOUNT_DATASET);
  Assert.assertNull(localDataset.get());
  fileSetDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.CSV_FILESET_DATASET);
  Assert.assertNull(fileSetDataset.get());
  doneFile.createNewFile();
  wfManager.waitForFinish(1,TimeUnit.MINUTES);
  Map<String,String> workflowMetricsContext=new HashMap<>();
  workflowMetricsContext.put(Constants.Metrics.Tag.NAMESPACE,testSpace.getId());
  workflowMetricsContext.put(Constants.Metrics.Tag.APP,applicationManager.getInfo().getName());
  workflowMetricsContext.put(Constants.Metrics.Tag.WORKFLOW,WorkflowAppWithLocalDatasets.WORKFLOW_NAME);
  workflowMetricsContext.put(Constants.Metrics.Tag.RUN_ID,runId);
  Map<String,String> writerContext=new HashMap<>(workflowMetricsContext);
  writerContext.put(Constants.Metrics.Tag.NODE,WorkflowAppWithLocalDatasets.LocalDatasetWriter.class.getSimpleName());
  Assert.assertEquals(2,getMetricsManager().getTotalMetric(writerContext,""String_Node_Str""));
  Map<String,String> sparkMetricsContext=new HashMap<>(workflowMetricsContext);
  sparkMetricsContext.put(Constants.Metrics.Tag.NODE,""String_Node_Str"");
  Assert.assertEquals(2,getMetricsManager().getTotalMetric(sparkMetricsContext,""String_Node_Str""));
  Map<String,String> mrMetricsContext=new HashMap<>(workflowMetricsContext);
  mrMetricsContext.put(Constants.Metrics.Tag.NODE,""String_Node_Str"");
  Assert.assertEquals(7,getMetricsManager().getTotalMetric(mrMetricsContext,""String_Node_Str""));
  Map<String,String> readerContext=new HashMap<>(workflowMetricsContext);
  readerContext.put(Constants.Metrics.Tag.NODE,""String_Node_Str"");
  Assert.assertEquals(6,getMetricsManager().getTotalMetric(readerContext,""String_Node_Str""));
  return runId;
}","private String executeWorkflow(ApplicationManager applicationManager,Map<String,String> additionalParams) throws Exception {
  WorkflowManager wfManager=applicationManager.getWorkflowManager(WorkflowAppWithLocalDatasets.WORKFLOW_NAME);
  Map<String,String> runtimeArgs=new HashMap<>();
  File waitFile=new File(TMP_FOLDER.newFolder(),""String_Node_Str"");
  File doneFile=new File(TMP_FOLDER.newFolder(),""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",waitFile.getAbsolutePath());
  runtimeArgs.put(""String_Node_Str"",doneFile.getAbsolutePath());
  runtimeArgs.putAll(additionalParams);
  wfManager.start(runtimeArgs);
  while (!waitFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  List<RunRecord> history=wfManager.getHistory(ProgramRunStatus.RUNNING);
  Assert.assertEquals(1,history.size());
  String runId=history.get(0).getPid();
  DataSetManager<KeyValueTable> localDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.WORDCOUNT_DATASET + ""String_Node_Str"" + runId);
  Assert.assertEquals(""String_Node_Str"",Bytes.toString(localDataset.get().read(""String_Node_Str"")));
  DataSetManager<FileSet> fileSetDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.CSV_FILESET_DATASET + ""String_Node_Str"" + runId);
  Assert.assertNotNull(fileSetDataset.get());
  localDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.WORDCOUNT_DATASET);
  Assert.assertNull(localDataset.get());
  fileSetDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.CSV_FILESET_DATASET);
  Assert.assertNull(fileSetDataset.get());
  doneFile.createNewFile();
  wfManager.waitForFinish(1,TimeUnit.MINUTES);
  Map<String,WorkflowNodeStateDetail> nodeStateDetailMap=wfManager.getWorkflowNodeStates(runId);
  Map<String,String> workflowMetricsContext=new HashMap<>();
  workflowMetricsContext.put(Constants.Metrics.Tag.NAMESPACE,testSpace.getId());
  workflowMetricsContext.put(Constants.Metrics.Tag.APP,applicationManager.getInfo().getName());
  workflowMetricsContext.put(Constants.Metrics.Tag.WORKFLOW,WorkflowAppWithLocalDatasets.WORKFLOW_NAME);
  workflowMetricsContext.put(Constants.Metrics.Tag.RUN_ID,runId);
  Map<String,String> writerContext=new HashMap<>(workflowMetricsContext);
  writerContext.put(Constants.Metrics.Tag.NODE,WorkflowAppWithLocalDatasets.LocalDatasetWriter.class.getSimpleName());
  Assert.assertEquals(2,getMetricsManager().getTotalMetric(writerContext,""String_Node_Str""));
  Map<String,String> wfSparkMetricsContext=new HashMap<>(workflowMetricsContext);
  wfSparkMetricsContext.put(Constants.Metrics.Tag.NODE,""String_Node_Str"");
  Assert.assertEquals(2,getMetricsManager().getTotalMetric(wfSparkMetricsContext,""String_Node_Str""));
  Map<String,String> sparkMetricsContext=new HashMap<>();
  sparkMetricsContext.put(Constants.Metrics.Tag.NAMESPACE,testSpace.getId());
  sparkMetricsContext.put(Constants.Metrics.Tag.APP,applicationManager.getInfo().getName());
  sparkMetricsContext.put(Constants.Metrics.Tag.SPARK,""String_Node_Str"");
  sparkMetricsContext.put(Constants.Metrics.Tag.RUN_ID,nodeStateDetailMap.get(""String_Node_Str"").getRunId());
  Assert.assertEquals(2,getMetricsManager().getTotalMetric(sparkMetricsContext,""String_Node_Str""));
  Map<String,String> appMetricsContext=new HashMap<>();
  appMetricsContext.put(Constants.Metrics.Tag.NAMESPACE,testSpace.getId());
  appMetricsContext.put(Constants.Metrics.Tag.APP,applicationManager.getInfo().getName());
  Assert.assertEquals(4,getMetricsManager().getTotalMetric(appMetricsContext,""String_Node_Str""));
  Map<String,String> wfMRMetricsContext=new HashMap<>(workflowMetricsContext);
  wfMRMetricsContext.put(Constants.Metrics.Tag.NODE,""String_Node_Str"");
  Assert.assertEquals(7,getMetricsManager().getTotalMetric(wfMRMetricsContext,""String_Node_Str""));
  Map<String,String> mrMetricsContext=new HashMap<>();
  mrMetricsContext.put(Constants.Metrics.Tag.NAMESPACE,testSpace.getId());
  mrMetricsContext.put(Constants.Metrics.Tag.APP,applicationManager.getInfo().getName());
  mrMetricsContext.put(Constants.Metrics.Tag.MAPREDUCE,""String_Node_Str"");
  mrMetricsContext.put(Constants.Metrics.Tag.RUN_ID,nodeStateDetailMap.get(""String_Node_Str"").getRunId());
  Assert.assertEquals(7,getMetricsManager().getTotalMetric(mrMetricsContext,""String_Node_Str""));
  Map<String,String> readerContext=new HashMap<>(workflowMetricsContext);
  readerContext.put(Constants.Metrics.Tag.NODE,""String_Node_Str"");
  Assert.assertEquals(6,getMetricsManager().getTotalMetric(readerContext,""String_Node_Str""));
  return runId;
}"
6188,"private static MetricsContext createMetricsContext(Program program,String runId,MetricsCollectionService service,@Nullable WorkflowProgramInfo workflowProgramInfo){
  Map<String,String> tags=Maps.newHashMap();
  tags.putAll(getMetricsContext(program,runId));
  MetricsContext programMetricsContext=service.getContext(tags);
  if (workflowProgramInfo == null) {
    return programMetricsContext;
  }
  tags=Maps.newHashMap();
  tags.put(Constants.Metrics.Tag.NAMESPACE,program.getNamespaceId());
  tags.put(Constants.Metrics.Tag.APP,program.getApplicationId());
  tags.put(Constants.Metrics.Tag.WORKFLOW,workflowProgramInfo.getName());
  tags.put(Constants.Metrics.Tag.RUN_ID,workflowProgramInfo.getRunId().getId());
  tags.put(Constants.Metrics.Tag.NODE,workflowProgramInfo.getNodeId());
  return new MultiMetricsContext(programMetricsContext,service.getContext(tags));
}","private static MetricsContext createMetricsContext(Program program,String runId,MetricsCollectionService service,@Nullable WorkflowProgramInfo workflowProgramInfo){
  Map<String,String> tags=Maps.newHashMap();
  tags.putAll(getMetricsContext(program,runId));
  if (workflowProgramInfo != null) {
    tags.put(Constants.Metrics.Tag.WORKFLOW,workflowProgramInfo.getName());
    tags.put(Constants.Metrics.Tag.WORKFLOW_RUN_ID,workflowProgramInfo.getRunId().getId());
    tags.put(Constants.Metrics.Tag.NODE,workflowProgramInfo.getNodeId());
  }
  return service.getContext(tags);
}"
6189,"private static MetricsContext createMetricsContext(Program program,String runId,MetricsCollectionService service,String taskId,@Nullable MapReduceMetrics.TaskType type,@Nullable WorkflowProgramInfo workflowProgramInfo){
  Map<String,String> tags=Maps.newHashMap();
  tags.putAll(getMetricsContext(program,runId));
  if (type != null) {
    tags.put(Constants.Metrics.Tag.MR_TASK_TYPE,type.getId());
    tags.put(Constants.Metrics.Tag.INSTANCE_ID,taskId);
  }
  MetricsContext programMetricsContext=service.getContext(tags);
  if (workflowProgramInfo == null) {
    return programMetricsContext;
  }
  tags=Maps.newHashMap();
  tags.put(Constants.Metrics.Tag.NAMESPACE,program.getNamespaceId());
  tags.put(Constants.Metrics.Tag.APP,program.getApplicationId());
  tags.put(Constants.Metrics.Tag.WORKFLOW,workflowProgramInfo.getName());
  tags.put(Constants.Metrics.Tag.RUN_ID,workflowProgramInfo.getRunId().getId());
  tags.put(Constants.Metrics.Tag.NODE,workflowProgramInfo.getNodeId());
  return new MultiMetricsContext(programMetricsContext,service.getContext(tags));
}","private static MetricsContext createMetricsContext(Program program,String runId,MetricsCollectionService service,String taskId,@Nullable MapReduceMetrics.TaskType type,@Nullable WorkflowProgramInfo workflowProgramInfo){
  Map<String,String> tags=Maps.newHashMap();
  tags.putAll(getMetricsContext(program,runId));
  if (type != null) {
    tags.put(Constants.Metrics.Tag.MR_TASK_TYPE,type.getId());
    tags.put(Constants.Metrics.Tag.INSTANCE_ID,taskId);
  }
  if (workflowProgramInfo != null) {
    tags.put(Constants.Metrics.Tag.WORKFLOW,workflowProgramInfo.getName());
    tags.put(Constants.Metrics.Tag.WORKFLOW_RUN_ID,workflowProgramInfo.getRunId().getId());
    tags.put(Constants.Metrics.Tag.NODE,workflowProgramInfo.getNodeId());
  }
  return service.getContext(tags);
}"
6190,"@Nullable private static MetricsContext getMetricCollector(Program program,String runId,@Nullable MetricsCollectionService service){
  if (service == null) {
    return null;
  }
  Map<String,String> tags=Maps.newHashMap(getMetricsContext(program,runId));
  return service.getContext(tags);
}","@Nullable private static MetricsContext getMetricCollector(Program program,String runId,@Nullable MetricsCollectionService service){
  if (service == null) {
    return null;
  }
  Map<String,String> tags=Maps.newHashMap();
  tags.put(Constants.Metrics.Tag.NAMESPACE,program.getNamespaceId());
  tags.put(Constants.Metrics.Tag.APP,program.getApplicationId());
  tags.put(ProgramTypeMetricTag.getTagName(program.getType()),program.getName());
  tags.put(Constants.Metrics.Tag.WORKFLOW_RUN_ID,runId);
  return service.getContext(tags);
}"
6191,"public CubeDataset(String name,MetricsTable entityTable,Map<Integer,Table> resolutionTables,Map<String,? extends Aggregation> aggregations){
  super(name,entityTable,resolutionTables.values().toArray(new Dataset[resolutionTables.values().size()]));
  this.entityTable=entityTable;
  this.resolutionTables=resolutionTables;
  int[] resolutions=new int[resolutionTables.keySet().size()];
  int index=0;
  for (  Integer resolution : resolutionTables.keySet()) {
    resolutions[index++]=resolution;
  }
  this.cube=new DefaultCube(resolutions,new FactTableSupplierImpl(entityTable,resolutionTables),aggregations);
}","public CubeDataset(String name,MetricsTable entityTable,Map<Integer,Table> resolutionTables,Map<String,? extends Aggregation> aggregations){
  super(name,entityTable,resolutionTables.values().toArray(new Dataset[resolutionTables.values().size()]));
  this.entityTable=entityTable;
  this.resolutionTables=resolutionTables;
  int[] resolutions=new int[resolutionTables.keySet().size()];
  int index=0;
  for (  Integer resolution : resolutionTables.keySet()) {
    resolutions[index++]=resolution;
  }
  this.cube=new DefaultCube(resolutions,new FactTableSupplierImpl(entityTable,resolutionTables),aggregations,ImmutableMap.<String,AggregationAlias>of());
}"
6192,"public DefaultCube(int[] resolutions,FactTableSupplier factTableSupplier,Map<String,? extends Aggregation> aggregations){
  this.aggregations=aggregations;
  this.resolutionToFactTable=Maps.newHashMap();
  for (  int resolution : resolutions) {
    resolutionToFactTable.put(resolution,factTableSupplier.get(resolution,3600));
  }
}","public DefaultCube(int[] resolutions,FactTableSupplier factTableSupplier,Map<String,? extends Aggregation> aggregations,Map<String,AggregationAlias> aggregationAliasMap){
  this.aggregations=aggregations;
  this.resolutionToFactTable=Maps.newHashMap();
  for (  int resolution : resolutions) {
    resolutionToFactTable.put(resolution,factTableSupplier.get(resolution,3600));
  }
  this.aggregationAliasMap=aggregationAliasMap;
}"
6193,"@Override public void add(Collection<? extends CubeFact> facts){
  List<Fact> toWrite=Lists.newArrayList();
  int dimValuesCount=0;
  for (  CubeFact fact : facts) {
    for (    Aggregation agg : aggregations.values()) {
      if (agg.accept(fact)) {
        List<DimensionValue> dimensionValues=Lists.newArrayList();
        for (        String dimensionName : agg.getDimensionNames()) {
          dimensionValues.add(new DimensionValue(dimensionName,fact.getDimensionValues().get(dimensionName)));
          dimValuesCount++;
        }
        toWrite.add(new Fact(fact.getTimestamp(),dimensionValues,fact.getMeasurements()));
      }
    }
  }
  for (  FactTable table : resolutionToFactTable.values()) {
    table.add(toWrite);
  }
  incrementMetric(""String_Node_Str"",1);
  incrementMetric(""String_Node_Str"",facts.size());
  incrementMetric(""String_Node_Str"",toWrite.size());
  incrementMetric(""String_Node_Str"",dimValuesCount);
  incrementMetric(""String_Node_Str"",toWrite.size() * resolutionToFactTable.size());
}","@Override public void add(Collection<? extends CubeFact> facts){
  List<Fact> toWrite=Lists.newArrayList();
  int dimValuesCount=0;
  for (  CubeFact fact : facts) {
    for (    Map.Entry<String,? extends Aggregation> aggEntry : aggregations.entrySet()) {
      Aggregation agg=aggEntry.getValue();
      AggregationAlias aggregationAlias=null;
      if (aggregationAliasMap.containsKey(aggEntry.getKey())) {
        aggregationAlias=aggregationAliasMap.get(aggEntry.getKey());
      }
      if (agg.accept(fact)) {
        List<DimensionValue> dimensionValues=Lists.newArrayList();
        for (        String dimensionName : agg.getDimensionNames()) {
          String dimensionValueKey=aggregationAlias == null ? dimensionName : aggregationAlias.getAlias(dimensionName);
          dimensionValues.add(new DimensionValue(dimensionName,fact.getDimensionValues().get(dimensionValueKey)));
          dimValuesCount++;
        }
        toWrite.add(new Fact(fact.getTimestamp(),dimensionValues,fact.getMeasurements()));
      }
    }
  }
  for (  FactTable table : resolutionToFactTable.values()) {
    table.add(toWrite);
  }
  incrementMetric(""String_Node_Str"",1);
  incrementMetric(""String_Node_Str"",facts.size());
  incrementMetric(""String_Node_Str"",toWrite.size());
  incrementMetric(""String_Node_Str"",dimValuesCount);
  incrementMetric(""String_Node_Str"",toWrite.size() * resolutionToFactTable.size());
}"
6194,"@Override protected Cube getCube(final String name,int[] resolutions,Map<String,? extends Aggregation> aggregations){
  FactTableSupplier supplier=new FactTableSupplier(){
    @Override public FactTable get(    int resolution,    int rollTime){
      String entityTableName=""String_Node_Str"" + name;
      InMemoryTableService.create(entityTableName);
      String dataTableName=""String_Node_Str"" + name + ""String_Node_Str""+ resolution;
      InMemoryTableService.create(dataTableName);
      return new FactTable(new InMemoryMetricsTable(dataTableName),new EntityTable(new InMemoryMetricsTable(entityTableName)),resolution,rollTime);
    }
  }
;
  return new DefaultCube(resolutions,supplier,aggregations);
}","@Override protected Cube getCube(final String name,int[] resolutions,Map<String,? extends Aggregation> aggregations){
  FactTableSupplier supplier=new FactTableSupplier(){
    @Override public FactTable get(    int resolution,    int rollTime){
      String entityTableName=""String_Node_Str"" + name;
      InMemoryTableService.create(entityTableName);
      String dataTableName=""String_Node_Str"" + name + ""String_Node_Str""+ resolution;
      InMemoryTableService.create(dataTableName);
      return new FactTable(new InMemoryMetricsTable(dataTableName),new EntityTable(new InMemoryMetricsTable(entityTableName)),resolution,rollTime);
    }
  }
;
  return new DefaultCube(resolutions,supplier,aggregations,ImmutableMap.<String,AggregationAlias>of());
}"
6195,"@Override public Map<String,WorkflowNodeStateDetail> getWorkflowNodeStates(ProgramRunId workflowRunId) throws NotFoundException {
  try {
    return workflowClient.getWorkflowNodeStates(workflowRunId);
  }
 catch (  IOException|UnauthenticatedException e) {
    throw Throwables.propagate(e);
  }
}","@Override public Map<String,WorkflowNodeStateDetail> getWorkflowNodeStates(String workflowRunId) throws NotFoundException {
  try {
    ProgramRunId programRunId=new ProgramRunId(workflowId.getNamespaceId(),workflowId.getApplicationId(),workflowId.getType(),workflowId.getId(),workflowRunId);
    return workflowClient.getWorkflowNodeStates(programRunId);
  }
 catch (  IOException|UnauthenticatedException e) {
    throw Throwables.propagate(e);
  }
}"
6196,"/** 
 * Creates a   {@link MetricsContext} to be used for the Spark execution.
 */
private static MetricsContext createMetricsContext(MetricsCollectionService service,ProgramId programId,RunId runId,@Nullable WorkflowProgramInfo workflowProgramInfo){
  Map<String,String> tags=Maps.newHashMap();
  tags.put(Constants.Metrics.Tag.NAMESPACE,programId.getNamespace());
  tags.put(Constants.Metrics.Tag.APP,programId.getApplication());
  tags.put(ProgramTypeMetricTag.getTagName(ProgramType.SPARK),programId.getProgram());
  tags.put(Constants.Metrics.Tag.RUN_ID,runId.getId());
  tags.put(Constants.Metrics.Tag.INSTANCE_ID,""String_Node_Str"");
  MetricsContext programMetricsContext=service.getContext(tags);
  if (workflowProgramInfo == null) {
    return programMetricsContext;
  }
  tags=Maps.newHashMap();
  tags.put(Constants.Metrics.Tag.NAMESPACE,programId.getNamespace());
  tags.put(Constants.Metrics.Tag.APP,programId.getApplication());
  tags.put(Constants.Metrics.Tag.WORKFLOW,workflowProgramInfo.getName());
  tags.put(Constants.Metrics.Tag.RUN_ID,workflowProgramInfo.getRunId().getId());
  tags.put(Constants.Metrics.Tag.NODE,workflowProgramInfo.getNodeId());
  return new MultiMetricsContext(programMetricsContext,service.getContext(tags));
}","/** 
 * Creates a   {@link MetricsContext} to be used for the Spark execution.
 */
private static MetricsContext createMetricsContext(MetricsCollectionService service,ProgramId programId,RunId runId,@Nullable WorkflowProgramInfo workflowProgramInfo){
  Map<String,String> tags=Maps.newHashMap();
  tags.put(Constants.Metrics.Tag.NAMESPACE,programId.getNamespace());
  tags.put(Constants.Metrics.Tag.APP,programId.getApplication());
  tags.put(ProgramTypeMetricTag.getTagName(ProgramType.SPARK),programId.getProgram());
  tags.put(Constants.Metrics.Tag.RUN_ID,runId.getId());
  tags.put(Constants.Metrics.Tag.INSTANCE_ID,""String_Node_Str"");
  if (workflowProgramInfo != null) {
    tags.put(Constants.Metrics.Tag.WORKFLOW,workflowProgramInfo.getName());
    tags.put(Constants.Metrics.Tag.WORKFLOW_RUN_ID,workflowProgramInfo.getRunId().getId());
    tags.put(Constants.Metrics.Tag.NODE,workflowProgramInfo.getNodeId());
  }
  return service.getContext(tags);
}"
6197,"/** 
 * Get node stated for the specified Workflow run.
 * @param workflowRunId the Workflow run for which node states to be returned
 * @return {@link Map} of node name to the {@link WorkflowNodeStateDetail}
 * @throws NotFoundException when the specified Workflow run is not found
 */
Map<String,WorkflowNodeStateDetail> getWorkflowNodeStates(ProgramRunId workflowRunId) throws NotFoundException ;","/** 
 * Get node stated for the specified Workflow run.
 * @param workflowRunId the Workflow run for which node states to be returned
 * @return {@link Map} of node name to the {@link WorkflowNodeStateDetail}
 * @throws NotFoundException when the specified Workflow run is not found
 */
Map<String,WorkflowNodeStateDetail> getWorkflowNodeStates(String workflowRunId) throws NotFoundException ;"
6198,"@Override public Map<String,WorkflowNodeStateDetail> getWorkflowNodeStates(ProgramRunId workflowRunId) throws NotFoundException {
  return appFabricClient.getWorkflowNodeStates(workflowRunId);
}","@Override public Map<String,WorkflowNodeStateDetail> getWorkflowNodeStates(String workflowRunId) throws NotFoundException {
  return appFabricClient.getWorkflowNodeStates(new ProgramRunId(programId.getNamespaceId(),programId.getApplicationId(),programId.getType(),programId.getId(),workflowRunId));
}"
6199,"public DefaultWorkflowManager(Id.Program programId,AppFabricClient appFabricClient,DefaultApplicationManager applicationManager){
  super(programId,applicationManager);
  this.appFabricClient=appFabricClient;
}","public DefaultWorkflowManager(Id.Program programId,AppFabricClient appFabricClient,DefaultApplicationManager applicationManager){
  super(programId,applicationManager);
  this.programId=programId;
  this.appFabricClient=appFabricClient;
}"
6200,"private String executeWorkflow(ApplicationManager applicationManager,Map<String,String> additionalParams) throws Exception {
  WorkflowManager wfManager=applicationManager.getWorkflowManager(WorkflowAppWithLocalDatasets.WORKFLOW_NAME);
  Map<String,String> runtimeArgs=new HashMap<>();
  File waitFile=new File(TMP_FOLDER.newFolder(),""String_Node_Str"");
  File doneFile=new File(TMP_FOLDER.newFolder(),""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",waitFile.getAbsolutePath());
  runtimeArgs.put(""String_Node_Str"",doneFile.getAbsolutePath());
  runtimeArgs.putAll(additionalParams);
  wfManager.start(runtimeArgs);
  while (!waitFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  List<RunRecord> history=wfManager.getHistory(ProgramRunStatus.RUNNING);
  Assert.assertEquals(1,history.size());
  String runId=history.get(0).getPid();
  DataSetManager<KeyValueTable> localDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.WORDCOUNT_DATASET + ""String_Node_Str"" + runId);
  Assert.assertEquals(""String_Node_Str"",Bytes.toString(localDataset.get().read(""String_Node_Str"")));
  DataSetManager<FileSet> fileSetDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.CSV_FILESET_DATASET + ""String_Node_Str"" + runId);
  Assert.assertNotNull(fileSetDataset.get());
  localDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.WORDCOUNT_DATASET);
  Assert.assertNull(localDataset.get());
  fileSetDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.CSV_FILESET_DATASET);
  Assert.assertNull(fileSetDataset.get());
  doneFile.createNewFile();
  wfManager.waitForFinish(1,TimeUnit.MINUTES);
  Map<String,String> workflowMetricsContext=new HashMap<>();
  workflowMetricsContext.put(Constants.Metrics.Tag.NAMESPACE,testSpace.getId());
  workflowMetricsContext.put(Constants.Metrics.Tag.APP,applicationManager.getInfo().getName());
  workflowMetricsContext.put(Constants.Metrics.Tag.WORKFLOW,WorkflowAppWithLocalDatasets.WORKFLOW_NAME);
  workflowMetricsContext.put(Constants.Metrics.Tag.RUN_ID,runId);
  Map<String,String> writerContext=new HashMap<>(workflowMetricsContext);
  writerContext.put(Constants.Metrics.Tag.NODE,WorkflowAppWithLocalDatasets.LocalDatasetWriter.class.getSimpleName());
  Assert.assertEquals(2,getMetricsManager().getTotalMetric(writerContext,""String_Node_Str""));
  Map<String,String> sparkMetricsContext=new HashMap<>(workflowMetricsContext);
  sparkMetricsContext.put(Constants.Metrics.Tag.NODE,""String_Node_Str"");
  Assert.assertEquals(2,getMetricsManager().getTotalMetric(sparkMetricsContext,""String_Node_Str""));
  Map<String,String> mrMetricsContext=new HashMap<>(workflowMetricsContext);
  mrMetricsContext.put(Constants.Metrics.Tag.NODE,""String_Node_Str"");
  Assert.assertEquals(7,getMetricsManager().getTotalMetric(mrMetricsContext,""String_Node_Str""));
  Map<String,String> readerContext=new HashMap<>(workflowMetricsContext);
  readerContext.put(Constants.Metrics.Tag.NODE,""String_Node_Str"");
  Assert.assertEquals(6,getMetricsManager().getTotalMetric(readerContext,""String_Node_Str""));
  return runId;
}","private String executeWorkflow(ApplicationManager applicationManager,Map<String,String> additionalParams) throws Exception {
  WorkflowManager wfManager=applicationManager.getWorkflowManager(WorkflowAppWithLocalDatasets.WORKFLOW_NAME);
  Map<String,String> runtimeArgs=new HashMap<>();
  File waitFile=new File(TMP_FOLDER.newFolder(),""String_Node_Str"");
  File doneFile=new File(TMP_FOLDER.newFolder(),""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArgs.put(""String_Node_Str"",waitFile.getAbsolutePath());
  runtimeArgs.put(""String_Node_Str"",doneFile.getAbsolutePath());
  runtimeArgs.putAll(additionalParams);
  wfManager.start(runtimeArgs);
  while (!waitFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  List<RunRecord> history=wfManager.getHistory(ProgramRunStatus.RUNNING);
  Assert.assertEquals(1,history.size());
  String runId=history.get(0).getPid();
  DataSetManager<KeyValueTable> localDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.WORDCOUNT_DATASET + ""String_Node_Str"" + runId);
  Assert.assertEquals(""String_Node_Str"",Bytes.toString(localDataset.get().read(""String_Node_Str"")));
  DataSetManager<FileSet> fileSetDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.CSV_FILESET_DATASET + ""String_Node_Str"" + runId);
  Assert.assertNotNull(fileSetDataset.get());
  localDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.WORDCOUNT_DATASET);
  Assert.assertNull(localDataset.get());
  fileSetDataset=getDataset(testSpace,WorkflowAppWithLocalDatasets.CSV_FILESET_DATASET);
  Assert.assertNull(fileSetDataset.get());
  doneFile.createNewFile();
  wfManager.waitForFinish(1,TimeUnit.MINUTES);
  Map<String,WorkflowNodeStateDetail> nodeStateDetailMap=wfManager.getWorkflowNodeStates(runId);
  Map<String,String> workflowMetricsContext=new HashMap<>();
  workflowMetricsContext.put(Constants.Metrics.Tag.NAMESPACE,testSpace.getId());
  workflowMetricsContext.put(Constants.Metrics.Tag.APP,applicationManager.getInfo().getName());
  workflowMetricsContext.put(Constants.Metrics.Tag.WORKFLOW,WorkflowAppWithLocalDatasets.WORKFLOW_NAME);
  workflowMetricsContext.put(Constants.Metrics.Tag.RUN_ID,runId);
  Map<String,String> writerContext=new HashMap<>(workflowMetricsContext);
  writerContext.put(Constants.Metrics.Tag.NODE,WorkflowAppWithLocalDatasets.LocalDatasetWriter.class.getSimpleName());
  Assert.assertEquals(2,getMetricsManager().getTotalMetric(writerContext,""String_Node_Str""));
  Map<String,String> wfSparkMetricsContext=new HashMap<>(workflowMetricsContext);
  wfSparkMetricsContext.put(Constants.Metrics.Tag.NODE,""String_Node_Str"");
  Assert.assertEquals(2,getMetricsManager().getTotalMetric(wfSparkMetricsContext,""String_Node_Str""));
  Map<String,String> sparkMetricsContext=new HashMap<>();
  sparkMetricsContext.put(Constants.Metrics.Tag.NAMESPACE,testSpace.getId());
  sparkMetricsContext.put(Constants.Metrics.Tag.APP,applicationManager.getInfo().getName());
  sparkMetricsContext.put(Constants.Metrics.Tag.SPARK,""String_Node_Str"");
  sparkMetricsContext.put(Constants.Metrics.Tag.RUN_ID,nodeStateDetailMap.get(""String_Node_Str"").getRunId());
  Assert.assertEquals(2,getMetricsManager().getTotalMetric(sparkMetricsContext,""String_Node_Str""));
  Map<String,String> appMetricsContext=new HashMap<>();
  appMetricsContext.put(Constants.Metrics.Tag.NAMESPACE,testSpace.getId());
  appMetricsContext.put(Constants.Metrics.Tag.APP,applicationManager.getInfo().getName());
  Assert.assertEquals(4,getMetricsManager().getTotalMetric(appMetricsContext,""String_Node_Str""));
  Map<String,String> wfMRMetricsContext=new HashMap<>(workflowMetricsContext);
  wfMRMetricsContext.put(Constants.Metrics.Tag.NODE,""String_Node_Str"");
  Assert.assertEquals(7,getMetricsManager().getTotalMetric(wfMRMetricsContext,""String_Node_Str""));
  Map<String,String> mrMetricsContext=new HashMap<>();
  mrMetricsContext.put(Constants.Metrics.Tag.NAMESPACE,testSpace.getId());
  mrMetricsContext.put(Constants.Metrics.Tag.APP,applicationManager.getInfo().getName());
  mrMetricsContext.put(Constants.Metrics.Tag.MAPREDUCE,""String_Node_Str"");
  mrMetricsContext.put(Constants.Metrics.Tag.RUN_ID,nodeStateDetailMap.get(""String_Node_Str"").getRunId());
  Assert.assertEquals(7,getMetricsManager().getTotalMetric(mrMetricsContext,""String_Node_Str""));
  Map<String,String> readerContext=new HashMap<>(workflowMetricsContext);
  readerContext.put(Constants.Metrics.Tag.NODE,""String_Node_Str"");
  Assert.assertEquals(6,getMetricsManager().getTotalMetric(readerContext,""String_Node_Str""));
  return runId;
}"
6201,"private static void usage(){
  HelpFormatter formatter=new HelpFormatter();
  String args=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
  formatter.printHelp(""String_Node_Str"" + args,getOptions());
  System.exit(0);
}","private static void usage(){
  String toolName=TOOL_NAME + (OSDetector.isWindows() ? ""String_Node_Str"" : ""String_Node_Str"");
  HelpFormatter formatter=new HelpFormatter();
  String args=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
  formatter.printHelp(toolName + ""String_Node_Str"" + args,getOptions());
  System.exit(0);
}"
6202,"@Override public ProgramRunner create(ProgramType programType){
  ProgramRuntimeProvider provider=runtimeProviderLoader.get(programType);
  if (provider != null) {
    LOG.debug(""String_Node_Str"",provider,programType);
    return provider.createProgramRunner(programType,mode,injector);
  }
  Provider<ProgramRunner> defaultProvider=defaultRunnerProviders.get(programType);
  Preconditions.checkNotNull(defaultProvider,""String_Node_Str"" + programType);
  return defaultProvider.get();
}","@Override public ProgramRunner create(ProgramType programType){
  ProgramRuntimeProvider provider=runtimeProviderLoader.get(programType);
  if (provider != null) {
    LOG.debug(""String_Node_Str"",provider,programType);
    return provider.createProgramRunner(programType,mode,injector);
  }
  Provider<ProgramRunner> defaultProvider=defaultRunnerProviders.get(programType);
  if (defaultProvider == null) {
    throw new IllegalArgumentException(""String_Node_Str"" + programType);
  }
  return defaultProvider.get();
}"
6203,"@Override protected void configure(){
  MapBinder<ProgramType,ProgramRunner> defaultProgramRunnerBinder=MapBinder.newMapBinder(binder(),ProgramType.class,ProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.FLOW).to(DistributedFlowProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.MAPREDUCE).to(DistributedMapReduceProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.WORKFLOW).to(DistributedWorkflowProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.WEBAPP).to(DistributedWebappProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.SERVICE).to(DistributedServiceProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.WORKER).to(DistributedWorkerProgramRunner.class);
  bind(ProgramRuntimeProvider.Mode.class).toInstance(ProgramRuntimeProvider.Mode.DISTRIBUTED);
  bind(ProgramRunnerFactory.class).to(DefaultProgramRunnerFactory.class).in(Scopes.SINGLETON);
  bind(ProgramRuntimeService.class).to(DistributedProgramRuntimeService.class).in(Scopes.SINGLETON);
  expose(ProgramRuntimeService.class);
}","@Override protected void configure(){
  MapBinder<ProgramType,ProgramRunner> defaultProgramRunnerBinder=MapBinder.newMapBinder(binder(),ProgramType.class,ProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.FLOW).to(DistributedFlowProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.MAPREDUCE).to(DistributedMapReduceProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.WORKFLOW).to(DistributedWorkflowProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.WEBAPP).to(DistributedWebappProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.SERVICE).to(DistributedServiceProgramRunner.class);
  defaultProgramRunnerBinder.addBinding(ProgramType.WORKER).to(DistributedWorkerProgramRunner.class);
  bind(ProgramRuntimeProvider.Mode.class).toInstance(ProgramRuntimeProvider.Mode.DISTRIBUTED);
  bind(ProgramRunnerFactory.class).to(DefaultProgramRunnerFactory.class).in(Scopes.SINGLETON);
  expose(ProgramRunnerFactory.class);
  bind(ProgramRuntimeService.class).to(DistributedProgramRuntimeService.class).in(Scopes.SINGLETON);
  expose(ProgramRuntimeService.class);
}"
6204,"/** 
 * Creates   {@link Program} that can be executed by the given {@link ProgramRunner}.
 * @param cConf the CDAP configuration
 * @param programRunner the {@link ProgramRunner} for executing the program
 * @param programJarLocation the {@link Location} of the program jar file
 * @param unpackedDir a directory that the program jar file was unpacked to
 * @return a new {@link Program} instance.
 * @throws IOException If failed to create the program
 */
public static Program create(CConfiguration cConf,ProgramRunner programRunner,Location programJarLocation,File unpackedDir) throws IOException {
  FilterClassLoader.Filter filter;
  if (programRunner instanceof ProgramClassLoaderFilterProvider) {
    filter=((ProgramClassLoaderFilterProvider)programRunner).getFilter();
  }
 else {
    filter=FilterClassLoader.defaultFilter();
  }
  if (filter == null) {
    throw new IOException(""String_Node_Str"");
  }
  FilterClassLoader parentClassLoader=new FilterClassLoader(programRunner.getClass().getClassLoader(),filter);
  final ProgramClassLoader programClassLoader=new ProgramClassLoader(cConf,unpackedDir,parentClassLoader);
  return new ForwardingProgram(Programs.create(programJarLocation,programClassLoader)){
    @Override public void close() throws IOException {
      Closeables.closeQuietly(programClassLoader);
      super.close();
    }
  }
;
}","/** 
 * Creates a   {@link Program} that can be executed by the given {@link ProgramRunner}.
 * @param cConf the CDAP configuration
 * @param programRunner the {@link ProgramRunner} for executing the program
 * @param programJarLocation the {@link Location} of the program jar file
 * @param unpackedDir a directory that the program jar file was unpacked to
 * @return a new {@link Program} instance.
 * @throws IOException If failed to create the program
 */
public static Program create(CConfiguration cConf,ProgramRunner programRunner,Location programJarLocation,File unpackedDir) throws IOException {
  final ProgramClassLoader programClassLoader;
  if (programRunner instanceof ProgramClassLoaderProvider) {
    programClassLoader=((ProgramClassLoaderProvider)programRunner).createProgramClassLoader(cConf,unpackedDir);
  }
 else {
    programClassLoader=new ProgramClassLoader(cConf,unpackedDir,FilterClassLoader.create(Programs.class.getClassLoader()));
  }
  if (programClassLoader == null) {
    throw new IOException(""String_Node_Str"");
  }
  return new ForwardingProgram(Programs.create(programJarLocation,programClassLoader)){
    @Override public void close() throws IOException {
      Closeables.closeQuietly(programClassLoader);
      super.close();
    }
  }
;
}"
6205,ProgramRunner create(ProgramType programType);,"/** 
 * Creates a   {@link ProgramRunner} for the given {@link ProgramType}.
 * @param programType type of program
 * @return a {@link ProgramRunner} that can execute the given program type.
 * @throws IllegalArgumentException if no {@link ProgramRunner} is found for the given program type
 */
ProgramRunner create(ProgramType programType);"
6206,"ArtifactClassLoaderFactory(CConfiguration cConf,ProgramRuntimeProviderLoader runtimeProviderLoader){
  this.cConf=cConf;
  this.runtimeProviderLoader=runtimeProviderLoader;
  this.tmpDir=new File(cConf.get(Constants.CFG_LOCAL_DATA_DIR),cConf.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
}","ArtifactClassLoaderFactory(CConfiguration cConf,ProgramRunnerFactory programRunnerFactory){
  this.cConf=cConf;
  this.programRunnerFactory=programRunnerFactory;
  this.tmpDir=new File(cConf.get(Constants.CFG_LOCAL_DATA_DIR),cConf.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
}"
6207,"/** 
 * Create a classloader that uses the artifact at the specified location to load classes, with access to packages that all program type has access to. The classloader created is only for artifact inspection purpose and shouldn't be used for program execution as it doesn't have the proper class filtering for the specific program type for the program being executed.
 * @param artifactLocation the location of the artifact to create the classloader from
 * @return a closeable classloader based off the specified artifact; on closing the returned {@link ClassLoader}, all temporary resources created for the classloader will be removed
 * @throws IOException if there was an error copying or unpacking the artifact
 */
CloseableClassLoader createClassLoader(Location artifactLocation) throws IOException {
  final File unpackDir=BundleJarUtil.unJar(artifactLocation,DirUtils.createTempDir(tmpDir));
  ClassLoader parentClassLoader;
  ProgramRuntimeProvider sparkRuntimeProvider=runtimeProviderLoader.get(ProgramType.SPARK);
  if (sparkRuntimeProvider != null) {
    parentClassLoader=new FilterClassLoader(sparkRuntimeProvider.getClass().getClassLoader(),sparkRuntimeProvider.createProgramClassLoaderFilter(ProgramType.SPARK));
  }
 else {
    parentClassLoader=new FilterClassLoader(getClass().getClassLoader(),FilterClassLoader.defaultFilter());
  }
  final ProgramClassLoader programClassLoader=new ProgramClassLoader(cConf,unpackDir,parentClassLoader);
  return new CloseableClassLoader(programClassLoader,new Closeable(){
    @Override public void close(){
      try {
        Closeables.closeQuietly(programClassLoader);
        DirUtils.deleteDirectoryContents(unpackDir);
      }
 catch (      IOException e) {
        LOG.warn(""String_Node_Str"",unpackDir,e);
      }
    }
  }
);
}","/** 
 * Create a classloader that uses the artifact at the specified location to load classes, with access to packages that all program type has access to. The classloader created is only for artifact inspection purpose and shouldn't be used for program execution as it doesn't have the proper class filtering for the specific program type for the program being executed.
 * @param artifactLocation the location of the artifact to create the classloader from
 * @return a closeable classloader based off the specified artifact; on closing the returned {@link ClassLoader}, all temporary resources created for the classloader will be removed
 * @throws IOException if there was an error copying or unpacking the artifact
 */
CloseableClassLoader createClassLoader(Location artifactLocation) throws IOException {
  final File unpackDir=BundleJarUtil.unJar(artifactLocation,DirUtils.createTempDir(tmpDir));
  ProgramClassLoader programClassLoader=null;
  try {
    ProgramRunner programRunner=programRunnerFactory.create(ProgramType.SPARK);
    if (programRunner instanceof ProgramClassLoaderProvider) {
      programClassLoader=((ProgramClassLoaderProvider)programRunner).createProgramClassLoader(cConf,unpackDir);
    }
  }
 catch (  Exception e) {
    LOG.trace(""String_Node_Str"",e);
  }
  if (programClassLoader == null) {
    programClassLoader=new ProgramClassLoader(cConf,unpackDir,FilterClassLoader.create(getClass().getClassLoader()));
  }
  final ProgramClassLoader finalProgramClassLoader=programClassLoader;
  return new CloseableClassLoader(programClassLoader,new Closeable(){
    @Override public void close(){
      try {
        Closeables.closeQuietly(finalProgramClassLoader);
        DirUtils.deleteDirectoryContents(unpackDir);
      }
 catch (      IOException e) {
        LOG.warn(""String_Node_Str"",unpackDir,e);
      }
    }
  }
);
}"
6208,"@Override public void close(){
  try {
    Closeables.closeQuietly(programClassLoader);
    DirUtils.deleteDirectoryContents(unpackDir);
  }
 catch (  IOException e) {
    LOG.warn(""String_Node_Str"",unpackDir,e);
  }
}","@Override public void close(){
  try {
    Closeables.closeQuietly(finalProgramClassLoader);
    DirUtils.deleteDirectoryContents(unpackDir);
  }
 catch (  IOException e) {
    LOG.warn(""String_Node_Str"",unpackDir,e);
  }
}"
6209,"@VisibleForTesting @Inject public ArtifactRepository(CConfiguration cConf,ArtifactStore artifactStore,MetadataStore metadataStore,AuthorizerInstantiatorService authorizerInstantiatorService,ProgramRuntimeProviderLoader programRuntimeProviderLoader){
  this.artifactStore=artifactStore;
  this.artifactClassLoaderFactory=new ArtifactClassLoaderFactory(cConf,programRuntimeProviderLoader);
  this.artifactInspector=new ArtifactInspector(cConf,artifactClassLoaderFactory);
  this.systemArtifactDirs=new ArrayList<>();
  for (  String dir : cConf.get(Constants.AppFabric.SYSTEM_ARTIFACTS_DIR).split(""String_Node_Str"")) {
    File file=new File(dir);
    if (!file.isDirectory()) {
      LOG.warn(""String_Node_Str"",file);
      continue;
    }
    systemArtifactDirs.add(file);
  }
  this.configReader=new ArtifactConfigReader();
  this.metadataStore=metadataStore;
  this.authorizerInstantiatorService=authorizerInstantiatorService;
  this.instanceId=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
}","@VisibleForTesting @Inject public ArtifactRepository(CConfiguration cConf,ArtifactStore artifactStore,MetadataStore metadataStore,AuthorizerInstantiatorService authorizerInstantiatorService,ProgramRunnerFactory programRunnerFactory){
  this.artifactStore=artifactStore;
  this.artifactClassLoaderFactory=new ArtifactClassLoaderFactory(cConf,programRunnerFactory);
  this.artifactInspector=new ArtifactInspector(cConf,artifactClassLoaderFactory);
  this.systemArtifactDirs=new ArrayList<>();
  for (  String dir : cConf.get(Constants.AppFabric.SYSTEM_ARTIFACTS_DIR).split(""String_Node_Str"")) {
    File file=new File(dir);
    if (!file.isDirectory()) {
      LOG.warn(""String_Node_Str"",file);
      continue;
    }
    systemArtifactDirs.add(file);
  }
  this.configReader=new ArtifactConfigReader();
  this.metadataStore=metadataStore;
  this.authorizerInstantiatorService=authorizerInstantiatorService;
  this.instanceId=new InstanceId(cConf.get(Constants.INSTANCE_NAME));
}"
6210,"@Test public void testInMemoryConfigurator() throws Exception {
  LocationFactory locationFactory=new LocalLocationFactory(TMP_FOLDER.newFolder());
  Location appJar=AppJarHelper.createDeploymentJar(locationFactory,WordCountApp.class);
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,WordCountApp.class.getSimpleName(),""String_Node_Str"");
  ArtifactRepository artifactRepo=new ArtifactRepository(conf,null,null,null,new ProgramRuntimeProviderLoader(conf));
  Configurator configurator=new InMemoryConfigurator(conf,Id.Namespace.DEFAULT,artifactId,WordCountApp.class.getName(),appJar,""String_Node_Str"",artifactRepo);
  ListenableFuture<ConfigResponse> result=configurator.config();
  ConfigResponse response=result.get(10,TimeUnit.SECONDS);
  Assert.assertNotNull(response);
  ApplicationSpecificationAdapter adapter=ApplicationSpecificationAdapter.create(new ReflectionSchemaGenerator());
  ApplicationSpecification specification=adapter.fromJson(response.get());
  Assert.assertNotNull(specification);
  Assert.assertTrue(specification.getName().equals(""String_Node_Str""));
  Assert.assertTrue(specification.getFlows().size() == 1);
}","@Test public void testInMemoryConfigurator() throws Exception {
  LocationFactory locationFactory=new LocalLocationFactory(TMP_FOLDER.newFolder());
  Location appJar=AppJarHelper.createDeploymentJar(locationFactory,WordCountApp.class);
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,WordCountApp.class.getSimpleName(),""String_Node_Str"");
  ArtifactRepository artifactRepo=new ArtifactRepository(conf,null,null,null,new DummyProgramRunnerFactory());
  Configurator configurator=new InMemoryConfigurator(conf,Id.Namespace.DEFAULT,artifactId,WordCountApp.class.getName(),appJar,""String_Node_Str"",artifactRepo);
  ListenableFuture<ConfigResponse> result=configurator.config();
  ConfigResponse response=result.get(10,TimeUnit.SECONDS);
  Assert.assertNotNull(response);
  ApplicationSpecificationAdapter adapter=ApplicationSpecificationAdapter.create(new ReflectionSchemaGenerator());
  ApplicationSpecification specification=adapter.fromJson(response.get());
  Assert.assertNotNull(specification);
  Assert.assertTrue(specification.getName().equals(""String_Node_Str""));
  Assert.assertTrue(specification.getFlows().size() == 1);
}"
6211,"@Test public void testAppWithConfig() throws Exception {
  LocationFactory locationFactory=new LocalLocationFactory(TMP_FOLDER.newFolder());
  Location appJar=AppJarHelper.createDeploymentJar(locationFactory,ConfigTestApp.class);
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,ConfigTestApp.class.getSimpleName(),""String_Node_Str"");
  ArtifactRepository artifactRepo=new ArtifactRepository(conf,null,null,null,new ProgramRuntimeProviderLoader(conf));
  ConfigTestApp.ConfigClass config=new ConfigTestApp.ConfigClass(""String_Node_Str"",""String_Node_Str"");
  Configurator configuratorWithConfig=new InMemoryConfigurator(conf,Id.Namespace.DEFAULT,artifactId,ConfigTestApp.class.getName(),appJar,new Gson().toJson(config),artifactRepo);
  ListenableFuture<ConfigResponse> result=configuratorWithConfig.config();
  ConfigResponse response=result.get(10,TimeUnit.SECONDS);
  Assert.assertNotNull(response);
  ApplicationSpecificationAdapter adapter=ApplicationSpecificationAdapter.create(new ReflectionSchemaGenerator());
  ApplicationSpecification specification=adapter.fromJson(response.get());
  Assert.assertNotNull(specification);
  Assert.assertTrue(specification.getStreams().size() == 1);
  Assert.assertTrue(specification.getStreams().containsKey(""String_Node_Str""));
  Assert.assertTrue(specification.getDatasets().size() == 1);
  Assert.assertTrue(specification.getDatasets().containsKey(""String_Node_Str""));
  Configurator configuratorWithoutConfig=new InMemoryConfigurator(conf,Id.Namespace.DEFAULT,artifactId,ConfigTestApp.class.getName(),appJar,null,artifactRepo);
  result=configuratorWithoutConfig.config();
  response=result.get(10,TimeUnit.SECONDS);
  Assert.assertNotNull(response);
  specification=adapter.fromJson(response.get());
  Assert.assertNotNull(specification);
  Assert.assertTrue(specification.getStreams().size() == 1);
  Assert.assertTrue(specification.getStreams().containsKey(ConfigTestApp.DEFAULT_STREAM));
  Assert.assertTrue(specification.getDatasets().size() == 1);
  Assert.assertTrue(specification.getDatasets().containsKey(ConfigTestApp.DEFAULT_TABLE));
}","@Test public void testAppWithConfig() throws Exception {
  LocationFactory locationFactory=new LocalLocationFactory(TMP_FOLDER.newFolder());
  Location appJar=AppJarHelper.createDeploymentJar(locationFactory,ConfigTestApp.class);
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,ConfigTestApp.class.getSimpleName(),""String_Node_Str"");
  ArtifactRepository artifactRepo=new ArtifactRepository(conf,null,null,null,new DummyProgramRunnerFactory());
  ConfigTestApp.ConfigClass config=new ConfigTestApp.ConfigClass(""String_Node_Str"",""String_Node_Str"");
  Configurator configuratorWithConfig=new InMemoryConfigurator(conf,Id.Namespace.DEFAULT,artifactId,ConfigTestApp.class.getName(),appJar,new Gson().toJson(config),artifactRepo);
  ListenableFuture<ConfigResponse> result=configuratorWithConfig.config();
  ConfigResponse response=result.get(10,TimeUnit.SECONDS);
  Assert.assertNotNull(response);
  ApplicationSpecificationAdapter adapter=ApplicationSpecificationAdapter.create(new ReflectionSchemaGenerator());
  ApplicationSpecification specification=adapter.fromJson(response.get());
  Assert.assertNotNull(specification);
  Assert.assertTrue(specification.getStreams().size() == 1);
  Assert.assertTrue(specification.getStreams().containsKey(""String_Node_Str""));
  Assert.assertTrue(specification.getDatasets().size() == 1);
  Assert.assertTrue(specification.getDatasets().containsKey(""String_Node_Str""));
  Configurator configuratorWithoutConfig=new InMemoryConfigurator(conf,Id.Namespace.DEFAULT,artifactId,ConfigTestApp.class.getName(),appJar,null,artifactRepo);
  result=configuratorWithoutConfig.config();
  response=result.get(10,TimeUnit.SECONDS);
  Assert.assertNotNull(response);
  specification=adapter.fromJson(response.get());
  Assert.assertNotNull(specification);
  Assert.assertTrue(specification.getStreams().size() == 1);
  Assert.assertTrue(specification.getStreams().containsKey(ConfigTestApp.DEFAULT_STREAM));
  Assert.assertTrue(specification.getDatasets().size() == 1);
  Assert.assertTrue(specification.getDatasets().containsKey(ConfigTestApp.DEFAULT_TABLE));
}"
6212,"@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  classLoaderFactory=new ArtifactClassLoaderFactory(cConf,new ProgramRuntimeProviderLoader(cConf));
  artifactInspector=new ArtifactInspector(cConf,classLoaderFactory);
}","@BeforeClass public static void setup() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TMP_FOLDER.newFolder().getAbsolutePath());
  classLoaderFactory=new ArtifactClassLoaderFactory(cConf,new DummyProgramRunnerFactory());
  artifactInspector=new ArtifactInspector(cConf,classLoaderFactory);
}"
6213,"@GET @Path(""String_Node_Str"") public void getArtifactPlugins(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginType,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws NamespaceNotFoundException, BadRequestException, ArtifactNotFoundException {
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  try {
    SortedMap<ArtifactDescriptor,List<PluginClass>> plugins=artifactRepository.getPlugins(namespace,artifactId,pluginType);
    List<PluginSummary> pluginSummaries=Lists.newArrayList();
    for (    Map.Entry<ArtifactDescriptor,List<PluginClass>> pluginsEntry : plugins.entrySet()) {
      ArtifactDescriptor pluginArtifact=pluginsEntry.getKey();
      ArtifactSummary pluginArtifactSummary=ArtifactSummary.from(pluginArtifact.getArtifactId());
      for (      PluginClass pluginClass : pluginsEntry.getValue()) {
        pluginSummaries.add(new PluginSummary(pluginClass.getName(),pluginClass.getType(),pluginClass.getDescription(),pluginClass.getClassName(),pluginArtifactSummary));
      }
    }
    responder.sendJson(HttpResponseStatus.OK,pluginSummaries);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
  }
}","@GET @Path(""String_Node_Str"") public void getArtifactPlugins(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginType,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws NamespaceNotFoundException, BadRequestException, ArtifactNotFoundException {
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  try {
    SortedMap<ArtifactDescriptor,Set<PluginClass>> plugins=artifactRepository.getPlugins(namespace,artifactId,pluginType);
    List<PluginSummary> pluginSummaries=Lists.newArrayList();
    for (    Map.Entry<ArtifactDescriptor,Set<PluginClass>> pluginsEntry : plugins.entrySet()) {
      ArtifactDescriptor pluginArtifact=pluginsEntry.getKey();
      ArtifactSummary pluginArtifactSummary=ArtifactSummary.from(pluginArtifact.getArtifactId());
      for (      PluginClass pluginClass : pluginsEntry.getValue()) {
        pluginSummaries.add(new PluginSummary(pluginClass.getName(),pluginClass.getType(),pluginClass.getDescription(),pluginClass.getClassName(),pluginArtifactSummary));
      }
    }
    responder.sendJson(HttpResponseStatus.OK,pluginSummaries);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
  }
}"
6214,"@GET @Path(""String_Node_Str"") public void getArtifactPluginTypes(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws NamespaceNotFoundException, BadRequestException, ArtifactNotFoundException {
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  try {
    SortedMap<ArtifactDescriptor,List<PluginClass>> plugins=artifactRepository.getPlugins(namespace,artifactId);
    Set<String> pluginTypes=Sets.newHashSet();
    for (    List<PluginClass> pluginClasses : plugins.values()) {
      for (      PluginClass pluginClass : pluginClasses) {
        pluginTypes.add(pluginClass.getType());
      }
    }
    responder.sendJson(HttpResponseStatus.OK,pluginTypes);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
  }
}","@GET @Path(""String_Node_Str"") public void getArtifactPluginTypes(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws NamespaceNotFoundException, BadRequestException, ArtifactNotFoundException {
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  try {
    SortedMap<ArtifactDescriptor,Set<PluginClass>> plugins=artifactRepository.getPlugins(namespace,artifactId);
    Set<String> pluginTypes=Sets.newHashSet();
    for (    Set<PluginClass> pluginClasses : plugins.values()) {
      for (      PluginClass pluginClass : pluginClasses) {
        pluginTypes.add(pluginClass.getType());
      }
    }
    responder.sendJson(HttpResponseStatus.OK,pluginTypes);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
  }
}"
6215,"private void addPluginsToMap(NamespaceId namespace,Id.Artifact parentArtifactId,SortedMap<ArtifactDescriptor,List<PluginClass>> map,Row row) throws IOException {
  for (  Map.Entry<byte[],byte[]> column : row.getColumns().entrySet()) {
    ImmutablePair<ArtifactDescriptor,PluginClass> pluginEntry=getPluginEntry(namespace,parentArtifactId,column);
    if (pluginEntry != null) {
      ArtifactDescriptor artifactDescriptor=pluginEntry.getFirst();
      if (!map.containsKey(artifactDescriptor)) {
        map.put(artifactDescriptor,Lists.<PluginClass>newArrayList());
      }
      map.get(artifactDescriptor).add(pluginEntry.getSecond());
    }
  }
}","private void addPluginsToMap(NamespaceId namespace,Id.Artifact parentArtifactId,SortedMap<ArtifactDescriptor,Set<PluginClass>> map,Row row) throws IOException {
  for (  Map.Entry<byte[],byte[]> column : row.getColumns().entrySet()) {
    ImmutablePair<ArtifactDescriptor,PluginClass> pluginEntry=getPluginEntry(namespace,parentArtifactId,column);
    if (pluginEntry != null) {
      ArtifactDescriptor artifactDescriptor=pluginEntry.getFirst();
      if (!map.containsKey(artifactDescriptor)) {
        map.put(artifactDescriptor,Sets.<PluginClass>newHashSet());
      }
      map.get(artifactDescriptor).add(pluginEntry.getSecond());
    }
  }
}"
6216,"private SortedMap<ArtifactDescriptor,List<PluginClass>> getPluginsInArtifact(Table table,Id.Artifact artifactId){
  SortedMap<ArtifactDescriptor,List<PluginClass>> result=new TreeMap<>();
  ArtifactCell parentCell=new ArtifactCell(artifactId);
  byte[] parentDataBytes=table.get(parentCell.rowkey,parentCell.column);
  if (parentDataBytes == null) {
    return null;
  }
  ArtifactData parentData=gson.fromJson(Bytes.toString(parentDataBytes),ArtifactData.class);
  Set<PluginClass> parentPlugins=parentData.meta.getClasses().getPlugins();
  if (!parentPlugins.isEmpty()) {
    Location parentLocation=locationFactory.create(parentData.locationURI);
    ArtifactDescriptor descriptor=new ArtifactDescriptor(artifactId.toArtifactId(),parentLocation);
    result.put(descriptor,Lists.newArrayList(parentPlugins));
  }
  return result;
}","private SortedMap<ArtifactDescriptor,Set<PluginClass>> getPluginsInArtifact(Table table,Id.Artifact artifactId,Predicate<PluginClass> filter){
  SortedMap<ArtifactDescriptor,Set<PluginClass>> result=new TreeMap<>();
  ArtifactCell parentCell=new ArtifactCell(artifactId);
  byte[] parentDataBytes=table.get(parentCell.rowkey,parentCell.column);
  if (parentDataBytes == null) {
    return null;
  }
  ArtifactData parentData=gson.fromJson(Bytes.toString(parentDataBytes),ArtifactData.class);
  Set<PluginClass> parentPlugins=parentData.meta.getClasses().getPlugins();
  Set<PluginClass> filteredPlugins=Sets.newLinkedHashSet(Iterables.filter(parentPlugins,filter));
  if (!filteredPlugins.isEmpty()) {
    Location parentLocation=locationFactory.create(parentData.locationURI);
    ArtifactDescriptor descriptor=new ArtifactDescriptor(artifactId.toArtifactId(),parentLocation);
    result.put(descriptor,filteredPlugins);
  }
  return result;
}"
6217,"@Test public void testPlugin() throws Exception {
  File pluginDir=DirUtils.createTempDir(tmpDir);
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File jarFile=createPluginJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(APP_ARTIFACT_ID.getNamespace(),APP_ARTIFACT_ID.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  artifactRepository.addArtifact(artifactId,jarFile,parents);
  SortedMap<ArtifactDescriptor,List<PluginClass>> plugins=artifactRepository.getPlugins(NamespaceId.DEFAULT,APP_ARTIFACT_ID);
  Assert.assertEquals(1,plugins.size());
  Assert.assertEquals(2,plugins.get(plugins.firstKey()).size());
  ArtifactDescriptor descriptor=plugins.firstKey();
  Files.copy(Locations.newInputSupplier(descriptor.getLocation()),new File(pluginDir,Artifacts.getFileName(descriptor.getArtifactId())));
  try (PluginInstantiator instantiator=new PluginInstantiator(cConf,appClassLoader,pluginDir)){
    for (    Map.Entry<ArtifactDescriptor,List<PluginClass>> entry : plugins.entrySet()) {
      for (      PluginClass pluginClass : entry.getValue()) {
        Plugin pluginInfo=new Plugin(entry.getKey().getArtifactId(),pluginClass,PluginProperties.builder().add(""String_Node_Str"",TEST_EMPTY_CLASS).add(""String_Node_Str"",""String_Node_Str"").build());
        Callable<String> plugin=instantiator.newInstance(pluginInfo);
        Assert.assertEquals(TEST_EMPTY_CLASS,plugin.call());
      }
    }
  }
 }","@Test public void testPlugin() throws Exception {
  File pluginDir=DirUtils.createTempDir(tmpDir);
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File jarFile=createPluginJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(APP_ARTIFACT_ID.getNamespace(),APP_ARTIFACT_ID.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  artifactRepository.addArtifact(artifactId,jarFile,parents);
  SortedMap<ArtifactDescriptor,Set<PluginClass>> plugins=artifactRepository.getPlugins(NamespaceId.DEFAULT,APP_ARTIFACT_ID);
  Assert.assertEquals(1,plugins.size());
  Assert.assertEquals(2,plugins.get(plugins.firstKey()).size());
  ArtifactDescriptor descriptor=plugins.firstKey();
  Files.copy(Locations.newInputSupplier(descriptor.getLocation()),new File(pluginDir,Artifacts.getFileName(descriptor.getArtifactId())));
  try (PluginInstantiator instantiator=new PluginInstantiator(cConf,appClassLoader,pluginDir)){
    for (    Map.Entry<ArtifactDescriptor,Set<PluginClass>> entry : plugins.entrySet()) {
      for (      PluginClass pluginClass : entry.getValue()) {
        Plugin pluginInfo=new Plugin(entry.getKey().getArtifactId(),pluginClass,PluginProperties.builder().add(""String_Node_Str"",TEST_EMPTY_CLASS).add(""String_Node_Str"",""String_Node_Str"").build());
        Callable<String> plugin=instantiator.newInstance(pluginInfo);
        Assert.assertEquals(TEST_EMPTY_CLASS,plugin.call());
      }
    }
  }
 }"
6218,"@Test public void testAddSystemArtifacts() throws Exception {
  Id.Artifact systemAppArtifactId=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  File systemAppJar=createAppJar(PluginTestApp.class,new File(systemArtifactsDir1,""String_Node_Str""),createManifest(ManifestFields.EXPORT_PACKAGE,PluginTestRunnable.class.getPackage().getName()));
  Id.Artifact pluginArtifactId1=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File pluginJar1=createPluginJar(TestPlugin.class,new File(systemArtifactsDir1,""String_Node_Str""),manifest);
  Map<String,PluginPropertyField> emptyMap=Collections.emptyMap();
  Set<PluginClass> manuallyAddedPlugins1=ImmutableSet.of(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap),new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap));
  File pluginConfigFile=new File(systemArtifactsDir1,""String_Node_Str"");
  ArtifactConfig pluginConfig1=new ArtifactConfig(ImmutableSet.of(new ArtifactRange(Id.Namespace.SYSTEM,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""))),manuallyAddedPlugins1,ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  try (BufferedWriter writer=Files.newWriter(pluginConfigFile,Charsets.UTF_8)){
    writer.write(pluginConfig1.toString());
  }
   Id.Artifact pluginArtifactId2=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File pluginJar2=createPluginJar(TestPlugin.class,new File(systemArtifactsDir2,""String_Node_Str""),manifest);
  Set<PluginClass> manuallyAddedPlugins2=ImmutableSet.of(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap),new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap));
  pluginConfigFile=new File(systemArtifactsDir2,""String_Node_Str"");
  ArtifactConfig pluginConfig2=new ArtifactConfig(ImmutableSet.of(new ArtifactRange(Id.Namespace.SYSTEM,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""))),manuallyAddedPlugins2,ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
  try (BufferedWriter writer=Files.newWriter(pluginConfigFile,Charsets.UTF_8)){
    writer.write(pluginConfig2.toString());
  }
   artifactRepository.addSystemArtifacts();
  Assert.assertTrue(systemAppJar.delete());
  Assert.assertTrue(pluginJar1.delete());
  Assert.assertTrue(pluginJar2.delete());
  try {
    ArtifactDetail appArtifactDetail=artifactRepository.getArtifact(systemAppArtifactId);
    Map<ArtifactDescriptor,List<PluginClass>> plugins=artifactRepository.getPlugins(NamespaceId.DEFAULT,systemAppArtifactId);
    Assert.assertEquals(2,plugins.size());
    List<PluginClass> pluginClasses=plugins.values().iterator().next();
    Set<String> pluginNames=Sets.newHashSet();
    for (    PluginClass pluginClass : pluginClasses) {
      pluginNames.add(pluginClass.getName());
    }
    Assert.assertEquals(Sets.newHashSet(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),pluginNames);
    Assert.assertEquals(systemAppArtifactId.getName(),appArtifactDetail.getDescriptor().getArtifactId().getName());
    Assert.assertEquals(systemAppArtifactId.getVersion(),appArtifactDetail.getDescriptor().getArtifactId().getVersion());
    ArtifactDetail pluginArtifactDetail=artifactRepository.getArtifact(pluginArtifactId1);
    Assert.assertEquals(pluginArtifactId1.getName(),pluginArtifactDetail.getDescriptor().getArtifactId().getName());
    Assert.assertEquals(pluginArtifactId1.getVersion(),pluginArtifactDetail.getDescriptor().getArtifactId().getVersion());
    Assert.assertTrue(pluginArtifactDetail.getMeta().getClasses().getPlugins().containsAll(manuallyAddedPlugins1));
    Assert.assertEquals(pluginConfig1.getProperties(),pluginArtifactDetail.getMeta().getProperties());
    pluginArtifactDetail=artifactRepository.getArtifact(pluginArtifactId2);
    Assert.assertEquals(pluginArtifactId2.getName(),pluginArtifactDetail.getDescriptor().getArtifactId().getName());
    Assert.assertEquals(pluginArtifactId2.getVersion(),pluginArtifactDetail.getDescriptor().getArtifactId().getVersion());
    Assert.assertTrue(pluginArtifactDetail.getMeta().getClasses().getPlugins().containsAll(manuallyAddedPlugins2));
    Assert.assertEquals(pluginConfig2.getProperties(),pluginArtifactDetail.getMeta().getProperties());
  }
  finally {
    artifactRepository.clear(NamespaceId.SYSTEM);
  }
}","@Test public void testAddSystemArtifacts() throws Exception {
  Id.Artifact systemAppArtifactId=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  File systemAppJar=createAppJar(PluginTestApp.class,new File(systemArtifactsDir1,""String_Node_Str""),createManifest(ManifestFields.EXPORT_PACKAGE,PluginTestRunnable.class.getPackage().getName()));
  Id.Artifact pluginArtifactId1=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File pluginJar1=createPluginJar(TestPlugin.class,new File(systemArtifactsDir1,""String_Node_Str""),manifest);
  Map<String,PluginPropertyField> emptyMap=Collections.emptyMap();
  Set<PluginClass> manuallyAddedPlugins1=ImmutableSet.of(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap),new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap));
  File pluginConfigFile=new File(systemArtifactsDir1,""String_Node_Str"");
  ArtifactConfig pluginConfig1=new ArtifactConfig(ImmutableSet.of(new ArtifactRange(Id.Namespace.SYSTEM,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""))),manuallyAddedPlugins1,ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  try (BufferedWriter writer=Files.newWriter(pluginConfigFile,Charsets.UTF_8)){
    writer.write(pluginConfig1.toString());
  }
   Id.Artifact pluginArtifactId2=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File pluginJar2=createPluginJar(TestPlugin.class,new File(systemArtifactsDir2,""String_Node_Str""),manifest);
  Set<PluginClass> manuallyAddedPlugins2=ImmutableSet.of(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap),new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,emptyMap));
  pluginConfigFile=new File(systemArtifactsDir2,""String_Node_Str"");
  ArtifactConfig pluginConfig2=new ArtifactConfig(ImmutableSet.of(new ArtifactRange(Id.Namespace.SYSTEM,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""))),manuallyAddedPlugins2,ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
  try (BufferedWriter writer=Files.newWriter(pluginConfigFile,Charsets.UTF_8)){
    writer.write(pluginConfig2.toString());
  }
   artifactRepository.addSystemArtifacts();
  Assert.assertTrue(systemAppJar.delete());
  Assert.assertTrue(pluginJar1.delete());
  Assert.assertTrue(pluginJar2.delete());
  try {
    ArtifactDetail appArtifactDetail=artifactRepository.getArtifact(systemAppArtifactId);
    Map<ArtifactDescriptor,Set<PluginClass>> plugins=artifactRepository.getPlugins(NamespaceId.DEFAULT,systemAppArtifactId);
    Assert.assertEquals(2,plugins.size());
    Set<PluginClass> pluginClasses=plugins.values().iterator().next();
    Set<String> pluginNames=Sets.newHashSet();
    for (    PluginClass pluginClass : pluginClasses) {
      pluginNames.add(pluginClass.getName());
    }
    Assert.assertEquals(Sets.newHashSet(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),pluginNames);
    Assert.assertEquals(systemAppArtifactId.getName(),appArtifactDetail.getDescriptor().getArtifactId().getName());
    Assert.assertEquals(systemAppArtifactId.getVersion(),appArtifactDetail.getDescriptor().getArtifactId().getVersion());
    ArtifactDetail pluginArtifactDetail=artifactRepository.getArtifact(pluginArtifactId1);
    Assert.assertEquals(pluginArtifactId1.getName(),pluginArtifactDetail.getDescriptor().getArtifactId().getName());
    Assert.assertEquals(pluginArtifactId1.getVersion(),pluginArtifactDetail.getDescriptor().getArtifactId().getVersion());
    Assert.assertTrue(pluginArtifactDetail.getMeta().getClasses().getPlugins().containsAll(manuallyAddedPlugins1));
    Assert.assertEquals(pluginConfig1.getProperties(),pluginArtifactDetail.getMeta().getProperties());
    pluginArtifactDetail=artifactRepository.getArtifact(pluginArtifactId2);
    Assert.assertEquals(pluginArtifactId2.getName(),pluginArtifactDetail.getDescriptor().getArtifactId().getName());
    Assert.assertEquals(pluginArtifactId2.getVersion(),pluginArtifactDetail.getDescriptor().getArtifactId().getVersion());
    Assert.assertTrue(pluginArtifactDetail.getMeta().getClasses().getPlugins().containsAll(manuallyAddedPlugins2));
    Assert.assertEquals(pluginConfig2.getProperties(),pluginArtifactDetail.getMeta().getProperties());
  }
  finally {
    artifactRepository.clear(NamespaceId.SYSTEM);
  }
}"
6219,"@Test public void testNamespaceIsolation() throws Exception {
  Id.Artifact systemAppArtifactId=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  File jar=createAppJar(PluginTestApp.class,new File(systemArtifactsDir1,""String_Node_Str""),createManifest(ManifestFields.EXPORT_PACKAGE,PluginTestRunnable.class.getPackage().getName()));
  artifactRepository.addSystemArtifacts();
  Assert.assertTrue(jar.delete());
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(systemAppArtifactId.getNamespace(),systemAppArtifactId.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  NamespaceId namespace1=Ids.namespace(""String_Node_Str"");
  NamespaceId namespace2=Ids.namespace(""String_Node_Str"");
  Id.Artifact pluginArtifactId1=Id.Artifact.from(namespace1.toId(),""String_Node_Str"",""String_Node_Str"");
  Id.Artifact pluginArtifactId2=Id.Artifact.from(namespace2.toId(),""String_Node_Str"",""String_Node_Str"");
  try {
    Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
    File jarFile=createPluginJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
    artifactRepository.addArtifact(pluginArtifactId1,jarFile,parents);
    artifactRepository.addArtifact(pluginArtifactId2,jarFile,parents);
    SortedMap<ArtifactDescriptor,List<PluginClass>> extensions=artifactRepository.getPlugins(namespace1,systemAppArtifactId);
    Assert.assertEquals(1,extensions.keySet().size());
    Assert.assertEquals(2,extensions.values().iterator().next().size());
    extensions=artifactRepository.getPlugins(namespace2,systemAppArtifactId);
    Assert.assertEquals(1,extensions.keySet().size());
    Assert.assertEquals(2,extensions.values().iterator().next().size());
  }
  finally {
    artifactRepository.clear(NamespaceId.SYSTEM);
    artifactRepository.clear(namespace1);
    artifactRepository.clear(namespace2);
  }
}","@Test public void testNamespaceIsolation() throws Exception {
  Id.Artifact systemAppArtifactId=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  File jar=createAppJar(PluginTestApp.class,new File(systemArtifactsDir1,""String_Node_Str""),createManifest(ManifestFields.EXPORT_PACKAGE,PluginTestRunnable.class.getPackage().getName()));
  artifactRepository.addSystemArtifacts();
  Assert.assertTrue(jar.delete());
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(systemAppArtifactId.getNamespace(),systemAppArtifactId.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  NamespaceId namespace1=Ids.namespace(""String_Node_Str"");
  NamespaceId namespace2=Ids.namespace(""String_Node_Str"");
  Id.Artifact pluginArtifactId1=Id.Artifact.from(namespace1.toId(),""String_Node_Str"",""String_Node_Str"");
  Id.Artifact pluginArtifactId2=Id.Artifact.from(namespace2.toId(),""String_Node_Str"",""String_Node_Str"");
  try {
    Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
    File jarFile=createPluginJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
    artifactRepository.addArtifact(pluginArtifactId1,jarFile,parents);
    artifactRepository.addArtifact(pluginArtifactId2,jarFile,parents);
    SortedMap<ArtifactDescriptor,Set<PluginClass>> extensions=artifactRepository.getPlugins(namespace1,systemAppArtifactId);
    Assert.assertEquals(1,extensions.keySet().size());
    Assert.assertEquals(2,extensions.values().iterator().next().size());
    extensions=artifactRepository.getPlugins(namespace2,systemAppArtifactId);
    Assert.assertEquals(1,extensions.keySet().size());
    Assert.assertEquals(2,extensions.values().iterator().next().size());
  }
  finally {
    artifactRepository.clear(NamespaceId.SYSTEM);
    artifactRepository.clear(namespace1);
    artifactRepository.clear(namespace2);
  }
}"
6220,"private static void usage(){
  HelpFormatter formatter=new HelpFormatter();
  String args=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
  formatter.printHelp(""String_Node_Str"" + args,getOptions());
  System.exit(0);
}","private static void usage(){
  String TOOL_NAME;
  String TOOL_NAME_BASE=""String_Node_Str"";
  String OS=System.getProperty(""String_Node_Str"").toLowerCase();
  if (OS.indexOf(""String_Node_Str"") >= 0) {
    TOOL_NAME=TOOL_NAME_BASE + ""String_Node_Str"";
  }
 else {
    TOOL_NAME=TOOL_NAME_BASE + ""String_Node_Str"";
  }
  HelpFormatter formatter=new HelpFormatter();
  String args=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
  formatter.printHelp(TOOL_NAME + ""String_Node_Str"" + args,getOptions());
  System.exit(0);
}"
6221,"private SortedMap<ArtifactDescriptor,List<PluginClass>> getPluginsInArtifact(Table table,Id.Artifact artifactId){
  SortedMap<ArtifactDescriptor,List<PluginClass>> result=new TreeMap<>();
  ArtifactCell parentCell=new ArtifactCell(artifactId);
  byte[] parentDataBytes=table.get(parentCell.rowkey,parentCell.column);
  if (parentDataBytes == null) {
    return null;
  }
  ArtifactData parentData=gson.fromJson(Bytes.toString(parentDataBytes),ArtifactData.class);
  Set<PluginClass> parentPlugins=parentData.meta.getClasses().getPlugins();
  if (!parentPlugins.isEmpty()) {
    Location parentLocation=locationFactory.create(parentData.locationURI);
    ArtifactDescriptor descriptor=new ArtifactDescriptor(artifactId.toArtifactId(),parentLocation);
    result.put(descriptor,Lists.newArrayList(parentPlugins));
  }
  return result;
}","private SortedMap<ArtifactDescriptor,List<PluginClass>> getPluginsInArtifact(Table table,Id.Artifact artifactId,Predicate<PluginClass> filter){
  SortedMap<ArtifactDescriptor,List<PluginClass>> result=new TreeMap<>();
  ArtifactCell parentCell=new ArtifactCell(artifactId);
  byte[] parentDataBytes=table.get(parentCell.rowkey,parentCell.column);
  if (parentDataBytes == null) {
    return null;
  }
  ArtifactData parentData=gson.fromJson(Bytes.toString(parentDataBytes),ArtifactData.class);
  Set<PluginClass> parentPlugins=parentData.meta.getClasses().getPlugins();
  Set<PluginClass> filteredPlugins=new HashSet<>();
  for (  PluginClass pluginClass : parentPlugins) {
    if (filter.apply(pluginClass)) {
      filteredPlugins.add(pluginClass);
    }
  }
  if (!filteredPlugins.isEmpty()) {
    Location parentLocation=locationFactory.create(parentData.locationURI);
    ArtifactDescriptor descriptor=new ArtifactDescriptor(artifactId.toArtifactId(),parentLocation);
    result.put(descriptor,Lists.newArrayList(filteredPlugins));
  }
  return result;
}"
6222,"@Test public void testAddGetSingleArtifact() throws Exception {
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  PluginClass plugin1=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of());
  PluginClass plugin2=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of());
  List<PluginClass> plugins=ImmutableList.of(plugin1,plugin2);
  ApplicationClass appClass=new ApplicationClass(InspectionApp.class.getName(),""String_Node_Str"",new ReflectionSchemaGenerator().generate(InspectionApp.AConfig.class));
  ArtifactMeta artifactMeta=new ArtifactMeta(ArtifactClasses.builder().addPlugins(plugins).addApp(appClass).build());
  String artifactContents=""String_Node_Str"";
  writeArtifact(artifactId,artifactMeta,artifactContents);
  ArtifactDetail artifactDetail=artifactStore.getArtifact(artifactId);
  assertEqual(artifactId,artifactMeta,artifactContents,artifactDetail);
  Map<ArtifactDescriptor,List<PluginClass>> pluginsMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,artifactId);
  Assert.assertEquals(1,pluginsMap.size());
  Assert.assertTrue(pluginsMap.containsKey(artifactDetail.getDescriptor()));
  Set<PluginClass> expected=ImmutableSet.copyOf(plugins);
  Set<PluginClass> actual=ImmutableSet.copyOf(pluginsMap.get(artifactDetail.getDescriptor()));
  Assert.assertEquals(expected,actual);
  pluginsMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,artifactId,""String_Node_Str"");
  Assert.assertEquals(1,pluginsMap.size());
  Assert.assertTrue(pluginsMap.containsKey(artifactDetail.getDescriptor()));
  expected=ImmutableSet.copyOf(plugins);
  actual=ImmutableSet.copyOf(pluginsMap.get(artifactDetail.getDescriptor()));
  Assert.assertEquals(expected,actual);
  Map<ArtifactDescriptor,PluginClass> pluginClasses=artifactStore.getPluginClasses(NamespaceId.DEFAULT,artifactId,""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(1,pluginClasses.size());
  Assert.assertTrue(pluginClasses.containsKey(artifactDetail.getDescriptor()));
  Assert.assertEquals(plugin2,pluginClasses.get(artifactDetail.getDescriptor()));
}","@Test public void testAddGetSingleArtifact() throws Exception {
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  PluginClass plugin1=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of());
  PluginClass plugin2=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of());
  PluginClass plugin3=new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of());
  List<PluginClass> plugins=ImmutableList.of(plugin1,plugin2,plugin3);
  ApplicationClass appClass=new ApplicationClass(InspectionApp.class.getName(),""String_Node_Str"",new ReflectionSchemaGenerator().generate(InspectionApp.AConfig.class));
  ArtifactMeta artifactMeta=new ArtifactMeta(ArtifactClasses.builder().addPlugins(plugins).addApp(appClass).build());
  String artifactContents=""String_Node_Str"";
  writeArtifact(artifactId,artifactMeta,artifactContents);
  ArtifactDetail artifactDetail=artifactStore.getArtifact(artifactId);
  assertEqual(artifactId,artifactMeta,artifactContents,artifactDetail);
  Map<ArtifactDescriptor,List<PluginClass>> pluginsMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,artifactId);
  Assert.assertEquals(1,pluginsMap.size());
  Assert.assertTrue(pluginsMap.containsKey(artifactDetail.getDescriptor()));
  Set<PluginClass> expected=ImmutableSet.copyOf(plugins);
  Set<PluginClass> actual=ImmutableSet.copyOf(pluginsMap.get(artifactDetail.getDescriptor()));
  Assert.assertEquals(expected,actual);
  pluginsMap=artifactStore.getPluginClasses(NamespaceId.DEFAULT,artifactId,""String_Node_Str"");
  Assert.assertEquals(1,pluginsMap.size());
  Assert.assertTrue(pluginsMap.containsKey(artifactDetail.getDescriptor()));
  expected=ImmutableSet.of(plugin1,plugin2);
  actual=ImmutableSet.copyOf(pluginsMap.get(artifactDetail.getDescriptor()));
  Assert.assertEquals(expected,actual);
  Map<ArtifactDescriptor,PluginClass> pluginClasses=artifactStore.getPluginClasses(NamespaceId.DEFAULT,artifactId,""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(1,pluginClasses.size());
  Assert.assertTrue(pluginClasses.containsKey(artifactDetail.getDescriptor()));
  Assert.assertEquals(plugin3,pluginClasses.get(artifactDetail.getDescriptor()));
}"
6223,"@GET @Path(""String_Node_Str"" + ""String_Node_Str"") public void getArtifactPlugin(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginType,@PathParam(""String_Node_Str"") String pluginName,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws NamespaceNotFoundException, BadRequestException, ArtifactNotFoundException {
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  try {
    SortedMap<ArtifactDescriptor,PluginClass> plugins=artifactRepository.getPlugins(namespace,artifactId,pluginType,pluginName);
    List<PluginInfo> pluginInfos=Lists.newArrayList();
    for (    Map.Entry<ArtifactDescriptor,PluginClass> pluginsEntry : plugins.entrySet()) {
      ArtifactDescriptor pluginArtifact=pluginsEntry.getKey();
      ArtifactSummary pluginArtifactSummary=ArtifactSummary.from(pluginArtifact.getArtifactId());
      PluginClass pluginClass=pluginsEntry.getValue();
      pluginInfos.add(new PluginInfo(pluginClass.getName(),pluginClass.getType(),pluginClass.getDescription(),pluginClass.getClassName(),pluginArtifactSummary,pluginClass.getProperties()));
    }
    responder.sendJson(HttpResponseStatus.OK,pluginInfos);
  }
 catch (  PluginNotExistsException e) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
  }
catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
  }
}","@GET @Path(""String_Node_Str"" + ""String_Node_Str"") public void getArtifactPlugin(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginType,@PathParam(""String_Node_Str"") String pluginName,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws NamespaceNotFoundException, BadRequestException, ArtifactNotFoundException {
  NamespaceId namespace=Ids.namespace(namespaceId);
  NamespaceId artifactNamespace=validateAndGetScopedNamespace(namespace,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(artifactNamespace,artifactName,artifactVersion);
  try {
    SortedMap<ArtifactDescriptor,PluginClass> plugins=artifactRepository.getPlugins(namespace,artifactId,pluginType,pluginName);
    List<PluginInfo> pluginInfos=Lists.newArrayList();
    for (    Map.Entry<ArtifactDescriptor,PluginClass> pluginsEntry : plugins.entrySet()) {
      ArtifactDescriptor pluginArtifact=pluginsEntry.getKey();
      ArtifactSummary pluginArtifactSummary=ArtifactSummary.from(pluginArtifact.getArtifactId());
      PluginClass pluginClass=pluginsEntry.getValue();
      pluginInfos.add(new PluginInfo(pluginClass.getName(),pluginClass.getType(),pluginClass.getDescription(),pluginClass.getClassName(),pluginArtifactSummary,pluginClass.getProperties(),pluginClass.getEndpoints()));
    }
    responder.sendJson(HttpResponseStatus.OK,pluginInfos);
  }
 catch (  PluginNotExistsException e) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
  }
catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
  }
}"
6224,"@Test public void testPluginWithEndpoints() throws Exception {
  Id.Artifact wordCount1Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addAppArtifact(wordCount1Id,WordCountApp.class).getStatusLine().getStatusCode());
  Manifest manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,CallablePlugin.class.getPackage().getName());
  Id.Artifact plugins3Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Set<ArtifactRange> plugins3Parents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(plugins3Id,CallablePlugin.class,manifest,plugins3Parents).getStatusLine().getStatusCode());
  Assert.assertEquals(""String_Node_Str"",GSON.fromJson(callPluginMethod(plugins3Id,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ArtifactScope.USER,200).getResponseBodyAsString(),String.class));
  manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,CallingPlugin.class.getPackage().getName());
  Id.Artifact plugins4Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Set<ArtifactRange> plugins4Parents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(plugins4Id,CallingPlugin.class,manifest,plugins4Parents).getStatusLine().getStatusCode());
  Assert.assertEquals(""String_Node_Str"",GSON.fromJson(callPluginMethod(plugins4Id,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ArtifactScope.USER,200).getResponseBodyAsString(),String.class));
  manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,PluginWithPojo.class.getPackage().getName());
  Id.Artifact plugins5Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Set<ArtifactRange> plugins5Parents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(plugins5Id,PluginWithPojo.class,manifest,plugins5Parents).getStatusLine().getStatusCode());
  List<TestData> data=ImmutableList.of(new TestData(1,10),new TestData(1,20),new TestData(3,15),new TestData(4,5),new TestData(3,15));
  Map<Long,Long> expectedResult=new HashMap<>();
  expectedResult.put(1L,30L);
  expectedResult.put(3L,30L);
  expectedResult.put(4L,5L);
  String response=callPluginMethod(plugins5Id,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",GSON.toJson(data),ArtifactScope.USER,200).getResponseBodyAsString();
  Assert.assertEquals(expectedResult,GSON.fromJson(response,new TypeToken<Map<Long,Long>>(){
  }
.getType()));
  callPluginMethod(plugins4Id,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ArtifactScope.USER,404);
  manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,InvalidPlugin.class.getPackage().getName());
  Id.Artifact invalidPluginId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Set<ArtifactRange> invalidPluginParents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.BAD_REQUEST.getCode(),addPluginArtifact(invalidPluginId,InvalidPlugin.class,manifest,invalidPluginParents).getStatusLine().getStatusCode());
  manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,InvalidPluginMethodParams.class.getPackage().getName());
  invalidPluginId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  invalidPluginParents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.BAD_REQUEST.getCode(),addPluginArtifact(invalidPluginId,InvalidPluginMethodParams.class,manifest,invalidPluginParents).getStatusLine().getStatusCode());
  manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,InvalidPluginMethodParamType.class.getPackage().getName());
  invalidPluginId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  invalidPluginParents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.BAD_REQUEST.getCode(),addPluginArtifact(invalidPluginId,InvalidPluginMethodParamType.class,manifest,invalidPluginParents).getStatusLine().getStatusCode());
  manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,PluginEndpointContextTestPlugin.class.getPackage().getName());
  Id.Artifact validPluginId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Set<ArtifactRange> validPluginParents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(validPluginId,PluginEndpointContextTestPlugin.class,manifest,validPluginParents).getStatusLine().getStatusCode());
}","@Test public void testPluginWithEndpoints() throws Exception {
  Id.Artifact wordCount1Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addAppArtifact(wordCount1Id,WordCountApp.class).getStatusLine().getStatusCode());
  Manifest manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,CallablePlugin.class.getPackage().getName());
  Id.Artifact plugins3Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Set<ArtifactRange> plugins3Parents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(plugins3Id,CallablePlugin.class,manifest,plugins3Parents).getStatusLine().getStatusCode());
  Set<PluginInfo> expectedInfos=Sets.newHashSet(new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",CallablePlugin.class.getName(),new ArtifactSummary(""String_Node_Str"",""String_Node_Str""),ImmutableMap.<String,PluginPropertyField>of(),ImmutableSet.<String>of(""String_Node_Str"")));
  Assert.assertEquals(expectedInfos,getPluginInfos(wordCount1Id,""String_Node_Str"",""String_Node_Str""));
  Assert.assertEquals(""String_Node_Str"",GSON.fromJson(callPluginMethod(plugins3Id,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ArtifactScope.USER,200).getResponseBodyAsString(),String.class));
  manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,CallingPlugin.class.getPackage().getName());
  Id.Artifact plugins4Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Set<ArtifactRange> plugins4Parents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(plugins4Id,CallingPlugin.class,manifest,plugins4Parents).getStatusLine().getStatusCode());
  Assert.assertEquals(""String_Node_Str"",GSON.fromJson(callPluginMethod(plugins4Id,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ArtifactScope.USER,200).getResponseBodyAsString(),String.class));
  manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,PluginWithPojo.class.getPackage().getName());
  Id.Artifact plugins5Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Set<ArtifactRange> plugins5Parents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(plugins5Id,PluginWithPojo.class,manifest,plugins5Parents).getStatusLine().getStatusCode());
  List<TestData> data=ImmutableList.of(new TestData(1,10),new TestData(1,20),new TestData(3,15),new TestData(4,5),new TestData(3,15));
  Map<Long,Long> expectedResult=new HashMap<>();
  expectedResult.put(1L,30L);
  expectedResult.put(3L,30L);
  expectedResult.put(4L,5L);
  String response=callPluginMethod(plugins5Id,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",GSON.toJson(data),ArtifactScope.USER,200).getResponseBodyAsString();
  Assert.assertEquals(expectedResult,GSON.fromJson(response,new TypeToken<Map<Long,Long>>(){
  }
.getType()));
  callPluginMethod(plugins4Id,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ArtifactScope.USER,404);
  manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,InvalidPlugin.class.getPackage().getName());
  Id.Artifact invalidPluginId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Set<ArtifactRange> invalidPluginParents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.BAD_REQUEST.getCode(),addPluginArtifact(invalidPluginId,InvalidPlugin.class,manifest,invalidPluginParents).getStatusLine().getStatusCode());
  manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,InvalidPluginMethodParams.class.getPackage().getName());
  invalidPluginId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  invalidPluginParents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.BAD_REQUEST.getCode(),addPluginArtifact(invalidPluginId,InvalidPluginMethodParams.class,manifest,invalidPluginParents).getStatusLine().getStatusCode());
  manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,InvalidPluginMethodParamType.class.getPackage().getName());
  invalidPluginId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  invalidPluginParents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.BAD_REQUEST.getCode(),addPluginArtifact(invalidPluginId,InvalidPluginMethodParamType.class,manifest,invalidPluginParents).getStatusLine().getStatusCode());
  manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,PluginEndpointContextTestPlugin.class.getPackage().getName());
  Id.Artifact validPluginId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Set<ArtifactRange> validPluginParents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(validPluginId,PluginEndpointContextTestPlugin.class,manifest,validPluginParents).getStatusLine().getStatusCode());
}"
6225,"@Test public void testGetPlugins() throws Exception {
  Id.Artifact wordCount1Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addAppArtifact(wordCount1Id,WordCountApp.class).getStatusLine().getStatusCode());
  Id.Artifact wordCount2Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addAppArtifact(wordCount2Id,WordCountApp.class).getStatusLine().getStatusCode());
  Manifest manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,Plugin1.class.getPackage().getName());
  Id.Artifact pluginsId1=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Set<ArtifactRange> plugins1Parents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(pluginsId1,Plugin1.class,manifest,plugins1Parents).getStatusLine().getStatusCode());
  Id.Artifact pluginsId2=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Set<ArtifactRange> plugins2Parents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(pluginsId2,Plugin1.class,manifest,plugins2Parents).getStatusLine().getStatusCode());
  ArtifactSummary plugins1Artifact=new ArtifactSummary(""String_Node_Str"",""String_Node_Str"");
  ArtifactSummary plugins2Artifact=new ArtifactSummary(""String_Node_Str"",""String_Node_Str"");
  Set<String> expectedTypes=Sets.newHashSet(""String_Node_Str"",""String_Node_Str"");
  Set<String> actualTypes=getPluginTypes(wordCount1Id);
  Assert.assertEquals(expectedTypes,actualTypes);
  actualTypes=getPluginTypes(wordCount2Id);
  Assert.assertEquals(expectedTypes,actualTypes);
  Set<PluginSummary> expectedSummaries=Sets.newHashSet(new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),plugins1Artifact),new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),plugins2Artifact));
  Set<PluginSummary> actualSummaries=getPluginSummaries(wordCount1Id,""String_Node_Str"");
  Assert.assertEquals(expectedSummaries,actualSummaries);
  expectedSummaries=Sets.newHashSet(new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),plugins1Artifact),new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),plugins2Artifact));
  actualSummaries=getPluginSummaries(wordCount1Id,""String_Node_Str"");
  Assert.assertEquals(expectedSummaries,actualSummaries);
  expectedSummaries=Sets.newHashSet(new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),plugins2Artifact));
  actualSummaries=getPluginSummaries(wordCount2Id,""String_Node_Str"");
  Assert.assertEquals(expectedSummaries,actualSummaries);
  expectedSummaries=Sets.newHashSet(new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),plugins2Artifact));
  actualSummaries=getPluginSummaries(wordCount2Id,""String_Node_Str"");
  Assert.assertEquals(expectedSummaries,actualSummaries);
  Map<String,PluginPropertyField> p1Properties=ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true),""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true));
  Map<String,PluginPropertyField> p2Properties=ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true));
  Set<PluginInfo> expectedInfos=Sets.newHashSet(new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),plugins1Artifact,p1Properties),new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),plugins2Artifact,p1Properties));
  Assert.assertEquals(expectedInfos,getPluginInfos(wordCount1Id,""String_Node_Str"",""String_Node_Str""));
  expectedInfos=Sets.newHashSet(new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),plugins1Artifact,p2Properties),new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),plugins2Artifact,p2Properties));
  Assert.assertEquals(expectedInfos,getPluginInfos(wordCount1Id,""String_Node_Str"",""String_Node_Str""));
  expectedInfos=Sets.newHashSet(new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),plugins2Artifact,p1Properties));
  Assert.assertEquals(expectedInfos,getPluginInfos(wordCount2Id,""String_Node_Str"",""String_Node_Str""));
  expectedInfos=Sets.newHashSet(new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),plugins2Artifact,p2Properties));
  Assert.assertEquals(expectedInfos,getPluginInfos(wordCount2Id,""String_Node_Str"",""String_Node_Str""));
}","@Test public void testGetPlugins() throws Exception {
  Id.Artifact wordCount1Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addAppArtifact(wordCount1Id,WordCountApp.class).getStatusLine().getStatusCode());
  Id.Artifact wordCount2Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addAppArtifact(wordCount2Id,WordCountApp.class).getStatusLine().getStatusCode());
  Manifest manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,Plugin1.class.getPackage().getName());
  Id.Artifact pluginsId1=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Set<ArtifactRange> plugins1Parents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(pluginsId1,Plugin1.class,manifest,plugins1Parents).getStatusLine().getStatusCode());
  Id.Artifact pluginsId2=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Set<ArtifactRange> plugins2Parents=Sets.newHashSet(new ArtifactRange(Id.Namespace.DEFAULT,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(pluginsId2,Plugin1.class,manifest,plugins2Parents).getStatusLine().getStatusCode());
  ArtifactSummary plugins1Artifact=new ArtifactSummary(""String_Node_Str"",""String_Node_Str"");
  ArtifactSummary plugins2Artifact=new ArtifactSummary(""String_Node_Str"",""String_Node_Str"");
  Set<String> expectedTypes=Sets.newHashSet(""String_Node_Str"",""String_Node_Str"");
  Set<String> actualTypes=getPluginTypes(wordCount1Id);
  Assert.assertEquals(expectedTypes,actualTypes);
  actualTypes=getPluginTypes(wordCount2Id);
  Assert.assertEquals(expectedTypes,actualTypes);
  Set<PluginSummary> expectedSummaries=Sets.newHashSet(new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),plugins1Artifact),new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),plugins2Artifact));
  Set<PluginSummary> actualSummaries=getPluginSummaries(wordCount1Id,""String_Node_Str"");
  Assert.assertEquals(expectedSummaries,actualSummaries);
  expectedSummaries=Sets.newHashSet(new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),plugins1Artifact),new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),plugins2Artifact));
  actualSummaries=getPluginSummaries(wordCount1Id,""String_Node_Str"");
  Assert.assertEquals(expectedSummaries,actualSummaries);
  expectedSummaries=Sets.newHashSet(new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),plugins2Artifact));
  actualSummaries=getPluginSummaries(wordCount2Id,""String_Node_Str"");
  Assert.assertEquals(expectedSummaries,actualSummaries);
  expectedSummaries=Sets.newHashSet(new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),plugins2Artifact));
  actualSummaries=getPluginSummaries(wordCount2Id,""String_Node_Str"");
  Assert.assertEquals(expectedSummaries,actualSummaries);
  Map<String,PluginPropertyField> p1Properties=ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true),""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true));
  Map<String,PluginPropertyField> p2Properties=ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true));
  Set<PluginInfo> expectedInfos=Sets.newHashSet(new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),plugins1Artifact,p1Properties,new HashSet<String>()),new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),plugins2Artifact,p1Properties,new HashSet<String>()));
  Assert.assertEquals(expectedInfos,getPluginInfos(wordCount1Id,""String_Node_Str"",""String_Node_Str""));
  expectedInfos=Sets.newHashSet(new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),plugins1Artifact,p2Properties,new HashSet<String>()),new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),plugins2Artifact,p2Properties,new HashSet<String>()));
  Assert.assertEquals(expectedInfos,getPluginInfos(wordCount1Id,""String_Node_Str"",""String_Node_Str""));
  expectedInfos=Sets.newHashSet(new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),plugins2Artifact,p1Properties,new HashSet<String>()));
  Assert.assertEquals(expectedInfos,getPluginInfos(wordCount2Id,""String_Node_Str"",""String_Node_Str""));
  expectedInfos=Sets.newHashSet(new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),plugins2Artifact,p2Properties,new HashSet<String>()));
  Assert.assertEquals(expectedInfos,getPluginInfos(wordCount2Id,""String_Node_Str"",""String_Node_Str""));
}"
6226,"@Test public void testPluginNamespaceIsolation() throws Exception {
  Id.Artifact systemId=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  File systemArtifact=buildAppArtifact(WordCountApp.class,""String_Node_Str"");
  artifactRepository.addArtifact(systemId,systemArtifact,Sets.<ArtifactRange>newHashSet());
  Set<ArtifactRange> parents=Sets.newHashSet(new ArtifactRange(systemId.getNamespace(),systemId.getName(),systemId.getVersion(),true,systemId.getVersion(),true));
  Id.Namespace namespace1=Id.Namespace.from(""String_Node_Str"");
  Id.Namespace namespace2=Id.Namespace.from(""String_Node_Str"");
  createNamespace(namespace1.getId());
  createNamespace(namespace2.getId());
  try {
    Manifest manifest=new Manifest();
    manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,Plugin1.class.getPackage().getName());
    Id.Artifact pluginsId1=Id.Artifact.from(namespace1,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(pluginsId1,Plugin1.class,manifest,parents).getStatusLine().getStatusCode());
    manifest=new Manifest();
    manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,Plugin1.class.getPackage().getName());
    Id.Artifact pluginsId2=Id.Artifact.from(namespace2,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(pluginsId2,Plugin1.class,manifest,parents).getStatusLine().getStatusCode());
    ArtifactSummary artifact1=new ArtifactSummary(pluginsId1.getName(),pluginsId1.getVersion().getVersion(),ArtifactScope.USER);
    ArtifactSummary artifact2=new ArtifactSummary(pluginsId2.getName(),pluginsId2.getVersion().getVersion(),ArtifactScope.USER);
    PluginSummary summary1Namespace1=new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),artifact1);
    PluginSummary summary2Namespace1=new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),artifact1);
    PluginSummary summary1Namespace2=new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),artifact2);
    PluginSummary summary2Namespace2=new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),artifact2);
    PluginInfo info1Namespace1=new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),artifact1,ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true),""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true)));
    PluginInfo info2Namespace1=new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),artifact1,ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true)));
    PluginInfo info1Namespace2=new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),artifact2,ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true),""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true)));
    PluginInfo info2Namespace2=new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),artifact2,ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true)));
    Id.Artifact namespace1Artifact=Id.Artifact.from(namespace1,systemId.getName(),systemId.getVersion());
    Id.Artifact namespace2Artifact=Id.Artifact.from(namespace2,systemId.getName(),systemId.getVersion());
    Assert.assertEquals(ImmutableSet.of(""String_Node_Str"",""String_Node_Str""),getPluginTypes(namespace1Artifact,ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(""String_Node_Str"",""String_Node_Str""),getPluginTypes(namespace2Artifact,ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(summary1Namespace1),getPluginSummaries(namespace1Artifact,""String_Node_Str"",ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(summary2Namespace1),getPluginSummaries(namespace1Artifact,""String_Node_Str"",ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(info1Namespace1),getPluginInfos(namespace1Artifact,""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(info2Namespace1),getPluginInfos(namespace1Artifact,""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(summary1Namespace2),getPluginSummaries(namespace2Artifact,""String_Node_Str"",ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(summary2Namespace2),getPluginSummaries(namespace2Artifact,""String_Node_Str"",ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(info1Namespace2),getPluginInfos(namespace2Artifact,""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(info2Namespace2),getPluginInfos(namespace2Artifact,""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM));
  }
  finally {
    deleteNamespace(""String_Node_Str"");
    deleteNamespace(""String_Node_Str"");
  }
}","@Test public void testPluginNamespaceIsolation() throws Exception {
  Id.Artifact systemId=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  File systemArtifact=buildAppArtifact(WordCountApp.class,""String_Node_Str"");
  artifactRepository.addArtifact(systemId,systemArtifact,Sets.<ArtifactRange>newHashSet());
  Set<ArtifactRange> parents=Sets.newHashSet(new ArtifactRange(systemId.getNamespace(),systemId.getName(),systemId.getVersion(),true,systemId.getVersion(),true));
  Id.Namespace namespace1=Id.Namespace.from(""String_Node_Str"");
  Id.Namespace namespace2=Id.Namespace.from(""String_Node_Str"");
  createNamespace(namespace1.getId());
  createNamespace(namespace2.getId());
  try {
    Manifest manifest=new Manifest();
    manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,Plugin1.class.getPackage().getName());
    Id.Artifact pluginsId1=Id.Artifact.from(namespace1,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(pluginsId1,Plugin1.class,manifest,parents).getStatusLine().getStatusCode());
    manifest=new Manifest();
    manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,Plugin1.class.getPackage().getName());
    Id.Artifact pluginsId2=Id.Artifact.from(namespace2,""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(HttpResponseStatus.OK.getCode(),addPluginArtifact(pluginsId2,Plugin1.class,manifest,parents).getStatusLine().getStatusCode());
    ArtifactSummary artifact1=new ArtifactSummary(pluginsId1.getName(),pluginsId1.getVersion().getVersion(),ArtifactScope.USER);
    ArtifactSummary artifact2=new ArtifactSummary(pluginsId2.getName(),pluginsId2.getVersion().getVersion(),ArtifactScope.USER);
    PluginSummary summary1Namespace1=new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),artifact1);
    PluginSummary summary2Namespace1=new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),artifact1);
    PluginSummary summary1Namespace2=new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),artifact2);
    PluginSummary summary2Namespace2=new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),artifact2);
    PluginInfo info1Namespace1=new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),artifact1,ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true),""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true)),new HashSet<String>());
    PluginInfo info2Namespace1=new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),artifact1,ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true)),new HashSet<String>());
    PluginInfo info1Namespace2=new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),artifact2,ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true),""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true)),new HashSet<String>());
    PluginInfo info2Namespace2=new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin2.class.getName(),artifact2,ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true)),new HashSet<String>());
    Id.Artifact namespace1Artifact=Id.Artifact.from(namespace1,systemId.getName(),systemId.getVersion());
    Id.Artifact namespace2Artifact=Id.Artifact.from(namespace2,systemId.getName(),systemId.getVersion());
    Assert.assertEquals(ImmutableSet.of(""String_Node_Str"",""String_Node_Str""),getPluginTypes(namespace1Artifact,ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(""String_Node_Str"",""String_Node_Str""),getPluginTypes(namespace2Artifact,ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(summary1Namespace1),getPluginSummaries(namespace1Artifact,""String_Node_Str"",ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(summary2Namespace1),getPluginSummaries(namespace1Artifact,""String_Node_Str"",ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(info1Namespace1),getPluginInfos(namespace1Artifact,""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(info2Namespace1),getPluginInfos(namespace1Artifact,""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(summary1Namespace2),getPluginSummaries(namespace2Artifact,""String_Node_Str"",ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(summary2Namespace2),getPluginSummaries(namespace2Artifact,""String_Node_Str"",ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(info1Namespace2),getPluginInfos(namespace2Artifact,""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM));
    Assert.assertEquals(ImmutableSet.of(info2Namespace2),getPluginInfos(namespace2Artifact,""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM));
  }
  finally {
    deleteNamespace(""String_Node_Str"");
    deleteNamespace(""String_Node_Str"");
  }
}"
6227,"@Test public void testArtifacts() throws Exception {
  Id.Artifact myapp1Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Id.Artifact myapp2Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  LocalLocationFactory locationFactory=new LocalLocationFactory(tmpFolder.newFolder());
  Manifest manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.BUNDLE_VERSION,""String_Node_Str"");
  final Location appJarLoc=AppJarHelper.createDeploymentJar(locationFactory,MyApp.class,manifest);
  InputSupplier<InputStream> inputSupplier=new InputSupplier<InputStream>(){
    @Override public InputStream getInput() throws IOException {
      return appJarLoc.getInputStream();
    }
  }
;
  artifactClient.add(myapp1Id.getNamespace(),myapp1Id.getName(),inputSupplier,myapp1Id.getVersion().getVersion());
  Map<String,String> myapp1Properties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  artifactClient.writeProperties(myapp1Id,myapp1Properties);
  artifactClient.add(myapp2Id.getNamespace(),myapp2Id.getName(),inputSupplier,null,null);
  Map<String,String> myapp2Properties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  artifactClient.writeProperties(myapp2Id,myapp2Properties);
  Id.Artifact pluginId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,Plugin1.class.getPackage().getName());
  final Location pluginJarLoc=PluginJarHelper.createPluginJar(locationFactory,manifest,Plugin1.class);
  inputSupplier=new InputSupplier<InputStream>(){
    @Override public InputStream getInput() throws IOException {
      return pluginJarLoc.getInputStream();
    }
  }
;
  Set<ArtifactRange> parents=Sets.newHashSet(new ArtifactRange(myapp2Id.getNamespace(),myapp2Id.getName(),myapp2Id.getVersion(),new ArtifactVersion(""String_Node_Str"")));
  Set<PluginClass> additionalPlugins=Sets.newHashSet(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,Collections.<String,PluginPropertyField>emptyMap()));
  artifactClient.add(pluginId.getNamespace(),pluginId.getName(),inputSupplier,pluginId.getVersion().getVersion(),parents,additionalPlugins);
  ArtifactSummary myapp1Summary=new ArtifactSummary(myapp1Id.getName(),myapp1Id.getVersion().getVersion());
  ArtifactSummary myapp2Summary=new ArtifactSummary(myapp2Id.getName(),myapp2Id.getVersion().getVersion());
  ArtifactSummary pluginArtifactSummary=new ArtifactSummary(pluginId.getName(),pluginId.getVersion().getVersion());
  Set<ArtifactSummary> artifacts=Sets.newHashSet(artifactClient.list(Id.Namespace.DEFAULT));
  Assert.assertEquals(Sets.newHashSet(myapp1Summary,myapp2Summary,pluginArtifactSummary),artifacts);
  Assert.assertEquals(Sets.newHashSet(myapp1Summary,myapp2Summary),Sets.newHashSet(artifactClient.listVersions(Id.Namespace.DEFAULT,myapp1Id.getName())));
  Assert.assertEquals(Sets.newHashSet(pluginArtifactSummary),Sets.newHashSet(artifactClient.listVersions(Id.Namespace.DEFAULT,pluginId.getName())));
  try {
    artifactClient.listVersions(Id.Namespace.DEFAULT,pluginId.getName(),ArtifactScope.SYSTEM);
    Assert.fail();
  }
 catch (  ArtifactNotFoundException e) {
  }
  Schema myAppConfigSchema=new ReflectionSchemaGenerator(false).generate(MyApp.Conf.class);
  ArtifactClasses myAppClasses=ArtifactClasses.builder().addApp(new ApplicationClass(MyApp.class.getName(),""String_Node_Str"",myAppConfigSchema)).build();
  ArtifactInfo myapp1Info=new ArtifactInfo(myapp1Id.getName(),myapp1Id.getVersion().getVersion(),ArtifactScope.USER,myAppClasses,myapp1Properties);
  Assert.assertEquals(myapp1Info,artifactClient.getArtifactInfo(myapp1Id));
  ArtifactInfo myapp2Info=new ArtifactInfo(myapp2Id.getName(),myapp2Id.getVersion().getVersion(),ArtifactScope.USER,myAppClasses,myapp2Properties);
  Assert.assertEquals(myapp2Info,artifactClient.getArtifactInfo(myapp2Id));
  myapp2Properties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  artifactClient.writeProperties(myapp2Id,myapp2Properties);
  Assert.assertEquals(myapp2Properties,artifactClient.getArtifactInfo(myapp2Id).getProperties());
  artifactClient.deleteProperty(myapp2Id,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),artifactClient.getArtifactInfo(myapp2Id).getProperties());
  artifactClient.writeProperty(myapp2Id,""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),artifactClient.getArtifactInfo(myapp2Id).getProperties());
  artifactClient.deleteProperties(myapp2Id);
  Assert.assertTrue(artifactClient.getArtifactInfo(myapp2Id).getProperties().isEmpty());
  Map<String,PluginPropertyField> props=ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true));
  ArtifactClasses pluginClasses=ArtifactClasses.builder().addPlugin(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),""String_Node_Str"",props)).addPlugins(additionalPlugins).build();
  ArtifactInfo pluginArtifactInfo=new ArtifactInfo(pluginId.getName(),pluginId.getVersion().getVersion(),ArtifactScope.USER,pluginClasses,ImmutableMap.<String,String>of());
  Assert.assertEquals(pluginArtifactInfo,artifactClient.getArtifactInfo(pluginId));
  Set<ApplicationClassSummary> expectedSummaries=ImmutableSet.of(new ApplicationClassSummary(myapp1Summary,MyApp.class.getName()),new ApplicationClassSummary(myapp2Summary,MyApp.class.getName()));
  Set<ApplicationClassSummary> appClassSummaries=Sets.newHashSet(artifactClient.getApplicationClasses(Id.Namespace.DEFAULT));
  Assert.assertEquals(expectedSummaries,appClassSummaries);
  Set<ApplicationClassInfo> appClassInfos=Sets.newHashSet(artifactClient.getApplicationClasses(Id.Namespace.DEFAULT,MyApp.class.getName()));
  Set<ApplicationClassInfo> expectedInfos=ImmutableSet.of(new ApplicationClassInfo(myapp1Summary,MyApp.class.getName(),myAppConfigSchema),new ApplicationClassInfo(myapp2Summary,MyApp.class.getName(),myAppConfigSchema));
  Assert.assertEquals(expectedInfos,appClassInfos);
  Assert.assertTrue(artifactClient.getPluginTypes(myapp1Id).isEmpty());
  Assert.assertEquals(Lists.newArrayList(""String_Node_Str"",""String_Node_Str""),artifactClient.getPluginTypes(myapp2Id));
  PluginSummary pluginSummary=new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),pluginArtifactSummary);
  Assert.assertEquals(Sets.newHashSet(pluginSummary),Sets.newHashSet(artifactClient.getPluginSummaries(myapp2Id,""String_Node_Str"")));
  Assert.assertTrue(artifactClient.getPluginSummaries(myapp2Id,""String_Node_Str"").isEmpty());
  PluginInfo pluginInfo=new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),pluginArtifactSummary,props);
  Assert.assertEquals(Sets.newHashSet(pluginInfo),Sets.newHashSet(artifactClient.getPluginInfo(myapp2Id,""String_Node_Str"",""String_Node_Str"")));
}","@Test public void testArtifacts() throws Exception {
  Id.Artifact myapp1Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Id.Artifact myapp2Id=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  LocalLocationFactory locationFactory=new LocalLocationFactory(tmpFolder.newFolder());
  Manifest manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.BUNDLE_VERSION,""String_Node_Str"");
  final Location appJarLoc=AppJarHelper.createDeploymentJar(locationFactory,MyApp.class,manifest);
  InputSupplier<InputStream> inputSupplier=new InputSupplier<InputStream>(){
    @Override public InputStream getInput() throws IOException {
      return appJarLoc.getInputStream();
    }
  }
;
  artifactClient.add(myapp1Id.getNamespace(),myapp1Id.getName(),inputSupplier,myapp1Id.getVersion().getVersion());
  Map<String,String> myapp1Properties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  artifactClient.writeProperties(myapp1Id,myapp1Properties);
  artifactClient.add(myapp2Id.getNamespace(),myapp2Id.getName(),inputSupplier,null,null);
  Map<String,String> myapp2Properties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  artifactClient.writeProperties(myapp2Id,myapp2Properties);
  Id.Artifact pluginId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  manifest=new Manifest();
  manifest.getMainAttributes().put(ManifestFields.EXPORT_PACKAGE,Plugin1.class.getPackage().getName());
  final Location pluginJarLoc=PluginJarHelper.createPluginJar(locationFactory,manifest,Plugin1.class);
  inputSupplier=new InputSupplier<InputStream>(){
    @Override public InputStream getInput() throws IOException {
      return pluginJarLoc.getInputStream();
    }
  }
;
  Set<ArtifactRange> parents=Sets.newHashSet(new ArtifactRange(myapp2Id.getNamespace(),myapp2Id.getName(),myapp2Id.getVersion(),new ArtifactVersion(""String_Node_Str"")));
  Set<PluginClass> additionalPlugins=Sets.newHashSet(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,Collections.<String,PluginPropertyField>emptyMap()));
  artifactClient.add(pluginId.getNamespace(),pluginId.getName(),inputSupplier,pluginId.getVersion().getVersion(),parents,additionalPlugins);
  ArtifactSummary myapp1Summary=new ArtifactSummary(myapp1Id.getName(),myapp1Id.getVersion().getVersion());
  ArtifactSummary myapp2Summary=new ArtifactSummary(myapp2Id.getName(),myapp2Id.getVersion().getVersion());
  ArtifactSummary pluginArtifactSummary=new ArtifactSummary(pluginId.getName(),pluginId.getVersion().getVersion());
  Set<ArtifactSummary> artifacts=Sets.newHashSet(artifactClient.list(Id.Namespace.DEFAULT));
  Assert.assertEquals(Sets.newHashSet(myapp1Summary,myapp2Summary,pluginArtifactSummary),artifacts);
  Assert.assertEquals(Sets.newHashSet(myapp1Summary,myapp2Summary),Sets.newHashSet(artifactClient.listVersions(Id.Namespace.DEFAULT,myapp1Id.getName())));
  Assert.assertEquals(Sets.newHashSet(pluginArtifactSummary),Sets.newHashSet(artifactClient.listVersions(Id.Namespace.DEFAULT,pluginId.getName())));
  try {
    artifactClient.listVersions(Id.Namespace.DEFAULT,pluginId.getName(),ArtifactScope.SYSTEM);
    Assert.fail();
  }
 catch (  ArtifactNotFoundException e) {
  }
  Schema myAppConfigSchema=new ReflectionSchemaGenerator(false).generate(MyApp.Conf.class);
  ArtifactClasses myAppClasses=ArtifactClasses.builder().addApp(new ApplicationClass(MyApp.class.getName(),""String_Node_Str"",myAppConfigSchema)).build();
  ArtifactInfo myapp1Info=new ArtifactInfo(myapp1Id.getName(),myapp1Id.getVersion().getVersion(),ArtifactScope.USER,myAppClasses,myapp1Properties);
  Assert.assertEquals(myapp1Info,artifactClient.getArtifactInfo(myapp1Id));
  ArtifactInfo myapp2Info=new ArtifactInfo(myapp2Id.getName(),myapp2Id.getVersion().getVersion(),ArtifactScope.USER,myAppClasses,myapp2Properties);
  Assert.assertEquals(myapp2Info,artifactClient.getArtifactInfo(myapp2Id));
  myapp2Properties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  artifactClient.writeProperties(myapp2Id,myapp2Properties);
  Assert.assertEquals(myapp2Properties,artifactClient.getArtifactInfo(myapp2Id).getProperties());
  artifactClient.deleteProperty(myapp2Id,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),artifactClient.getArtifactInfo(myapp2Id).getProperties());
  artifactClient.writeProperty(myapp2Id,""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),artifactClient.getArtifactInfo(myapp2Id).getProperties());
  artifactClient.deleteProperties(myapp2Id);
  Assert.assertTrue(artifactClient.getArtifactInfo(myapp2Id).getProperties().isEmpty());
  Map<String,PluginPropertyField> props=ImmutableMap.of(""String_Node_Str"",new PluginPropertyField(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",true));
  ArtifactClasses pluginClasses=ArtifactClasses.builder().addPlugin(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),""String_Node_Str"",props)).addPlugins(additionalPlugins).build();
  ArtifactInfo pluginArtifactInfo=new ArtifactInfo(pluginId.getName(),pluginId.getVersion().getVersion(),ArtifactScope.USER,pluginClasses,ImmutableMap.<String,String>of());
  Assert.assertEquals(pluginArtifactInfo,artifactClient.getArtifactInfo(pluginId));
  Set<ApplicationClassSummary> expectedSummaries=ImmutableSet.of(new ApplicationClassSummary(myapp1Summary,MyApp.class.getName()),new ApplicationClassSummary(myapp2Summary,MyApp.class.getName()));
  Set<ApplicationClassSummary> appClassSummaries=Sets.newHashSet(artifactClient.getApplicationClasses(Id.Namespace.DEFAULT));
  Assert.assertEquals(expectedSummaries,appClassSummaries);
  Set<ApplicationClassInfo> appClassInfos=Sets.newHashSet(artifactClient.getApplicationClasses(Id.Namespace.DEFAULT,MyApp.class.getName()));
  Set<ApplicationClassInfo> expectedInfos=ImmutableSet.of(new ApplicationClassInfo(myapp1Summary,MyApp.class.getName(),myAppConfigSchema),new ApplicationClassInfo(myapp2Summary,MyApp.class.getName(),myAppConfigSchema));
  Assert.assertEquals(expectedInfos,appClassInfos);
  Assert.assertTrue(artifactClient.getPluginTypes(myapp1Id).isEmpty());
  Assert.assertEquals(Lists.newArrayList(""String_Node_Str"",""String_Node_Str""),artifactClient.getPluginTypes(myapp2Id));
  PluginSummary pluginSummary=new PluginSummary(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),pluginArtifactSummary);
  Assert.assertEquals(Sets.newHashSet(pluginSummary),Sets.newHashSet(artifactClient.getPluginSummaries(myapp2Id,""String_Node_Str"")));
  Assert.assertTrue(artifactClient.getPluginSummaries(myapp2Id,""String_Node_Str"").isEmpty());
  PluginInfo pluginInfo=new PluginInfo(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",Plugin1.class.getName(),pluginArtifactSummary,props,new HashSet<String>());
  Assert.assertEquals(Sets.newHashSet(pluginInfo),Sets.newHashSet(artifactClient.getPluginInfo(myapp2Id,""String_Node_Str"",""String_Node_Str"")));
}"
6228,"@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  PluginInfo that=(PluginInfo)o;
  return super.equals(that) && Objects.equals(properties,that.properties);
}","@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  PluginInfo that=(PluginInfo)o;
  return super.equals(that) && Objects.equals(properties,that.properties) && Objects.equals(endpoints,that.endpoints);
}"
6229,"public PluginInfo(String name,String type,String description,String className,ArtifactSummary artifact,Map<String,PluginPropertyField> properties){
  super(name,type,description,className,artifact);
  this.properties=properties;
}","public PluginInfo(String name,String type,String description,String className,ArtifactSummary artifact,Map<String,PluginPropertyField> properties,Set<String> endpoints){
  super(name,type,description,className,artifact);
  this.properties=properties;
  this.endpoints=endpoints;
}"
6230,"@Override public String toString(){
  return ""String_Node_Str"" + ""String_Node_Str"" + name + '\''+ ""String_Node_Str""+ type+ '\''+ ""String_Node_Str""+ description+ '\''+ ""String_Node_Str""+ className+ '\''+ ""String_Node_Str""+ properties+ ""String_Node_Str""+ artifact+ '}';
}","@Override public String toString(){
  return ""String_Node_Str"" + ""String_Node_Str"" + name + '\''+ ""String_Node_Str""+ type+ '\''+ ""String_Node_Str""+ description+ '\''+ ""String_Node_Str""+ className+ '\''+ ""String_Node_Str""+ properties+ ""String_Node_Str""+ artifact+ ""String_Node_Str""+ endpoints+ '}';
}"
6231,"/** 
 * Change the number of instances of the running runnable.
 * @param runnableName Name of the runnable
 * @param newCount New instance count
 * @throws java.util.concurrent.ExecutionException
 * @throws InterruptedException
 */
private void changeInstances(String runnableName,final int newCount) throws Exception {
  Map<Integer,ProgramController> liveRunnables=components.row(runnableName);
  int liveCount=liveRunnables.size();
  if (liveCount == newCount) {
    return;
  }
  if (liveCount > newCount) {
    List<ListenableFuture<ProgramController>> futures=Lists.newArrayListWithCapacity(liveCount - newCount);
    for (int instanceId=liveCount - 1; instanceId >= newCount; instanceId--) {
      futures.add(components.remove(runnableName,instanceId).stop());
    }
    Futures.allAsList(futures).get();
  }
  for (int instanceId=liveCount; instanceId < newCount; instanceId++) {
    ProgramOptions programOptions=createComponentOptions(runnableName,instanceId,newCount,getRunId(),options);
    ProgramController controller=createProgramRunner().run(program,programOptions);
    components.put(runnableName,instanceId,controller);
  }
  liveRunnables=components.row(runnableName);
  for (  Map.Entry<Integer,ProgramController> entry : liveRunnables.entrySet()) {
    entry.getValue().command(ProgramOptionConstants.INSTANCES,newCount);
  }
}","/** 
 * Change the number of instances of the running runnable.
 * @param runnableName Name of the runnable
 * @param newCount New instance count
 * @param oldCount Old instance count
 * @throws java.util.concurrent.ExecutionException
 * @throws InterruptedException
 */
private void changeInstances(String runnableName,final int newCount,@SuppressWarnings(""String_Node_Str"") final int oldCount) throws Exception {
  Map<Integer,ProgramController> liveRunnables=components.row(runnableName);
  int liveCount=liveRunnables.size();
  if (liveCount == newCount) {
    return;
  }
  if (liveCount > newCount) {
    List<ListenableFuture<ProgramController>> futures=Lists.newArrayListWithCapacity(liveCount - newCount);
    for (int instanceId=liveCount - 1; instanceId >= newCount; instanceId--) {
      futures.add(components.remove(runnableName,instanceId).stop());
    }
    Futures.allAsList(futures).get();
  }
  for (int instanceId=liveCount; instanceId < newCount; instanceId++) {
    ProgramOptions programOptions=createComponentOptions(runnableName,instanceId,newCount,getRunId(),options);
    ProgramController controller=createProgramRunner().run(program,programOptions);
    components.put(runnableName,instanceId,controller);
  }
  liveRunnables=components.row(runnableName);
  for (  Map.Entry<Integer,ProgramController> entry : liveRunnables.entrySet()) {
    entry.getValue().command(ProgramOptionConstants.INSTANCES,newCount);
  }
}"
6232,"@Override @SuppressWarnings(""String_Node_Str"") protected void doCommand(String name,Object value) throws Exception {
  if (!ProgramOptionConstants.INSTANCES.equals(name) || !(value instanceof Map)) {
    return;
  }
  Map<String,String> command=(Map<String,String>)value;
  lock.lock();
  try {
    for (    Map.Entry<String,String> entry : command.entrySet()) {
      changeInstances(entry.getKey(),Integer.valueOf(entry.getValue()));
    }
  }
 catch (  Throwable t) {
    LOG.error(String.format(""String_Node_Str"",command),t);
  }
 finally {
    lock.unlock();
  }
}","@Override @SuppressWarnings(""String_Node_Str"") protected void doCommand(String name,Object value) throws Exception {
  if (!ProgramOptionConstants.INSTANCES.equals(name) || !(value instanceof Map)) {
    return;
  }
  Map<String,String> command=(Map<String,String>)value;
  lock.lock();
  try {
    changeInstances(command.get(""String_Node_Str""),Integer.valueOf(command.get(""String_Node_Str"")),Integer.valueOf(command.get(""String_Node_Str"")));
  }
 catch (  Throwable t) {
    LOG.error(String.format(""String_Node_Str"",command),t);
    throw t;
  }
 finally {
    lock.unlock();
  }
}"
6233,"@SuppressWarnings(""String_Node_Str"") @Override protected void doCommand(String name,Object value) throws Exception {
  if (!ProgramOptionConstants.INSTANCES.equals(name) || !(value instanceof Map)) {
    return;
  }
  Map<String,String> command=(Map<String,String>)value;
  lock.lock();
  try {
    changeInstances(command.get(""String_Node_Str""),Integer.valueOf(command.get(""String_Node_Str"")),GSON.fromJson(command.get(""String_Node_Str""),FlowSpecification.class));
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",command,t);
    stop();
  }
 finally {
    lock.unlock();
  }
}","@SuppressWarnings(""String_Node_Str"") @Override protected void doCommand(String name,Object value) throws Exception {
  if (!ProgramOptionConstants.INSTANCES.equals(name) || !(value instanceof Map)) {
    return;
  }
  Map<String,String> command=(Map<String,String>)value;
  lock.lock();
  try {
    changeInstances(command.get(""String_Node_Str""),Integer.valueOf(command.get(""String_Node_Str"")),GSON.fromJson(command.get(""String_Node_Str""),FlowSpecification.class));
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",command,t);
    stop();
    throw t;
  }
 finally {
    lock.unlock();
  }
}"
6234,"@SuppressWarnings(""String_Node_Str"") @Override protected void doCommand(String name,Object value) throws Exception {
  if (!ProgramOptionConstants.INSTANCES.equals(name) || !(value instanceof Map)) {
    return;
  }
  Map<String,String> command=(Map<String,String>)value;
  lock.lock();
  try {
    changeInstances(command.get(""String_Node_Str""),Integer.valueOf(command.get(""String_Node_Str"")),Integer.valueOf(command.get(""String_Node_Str"")));
  }
 catch (  Throwable t) {
    LOG.error(String.format(""String_Node_Str"",command),t);
  }
 finally {
    lock.unlock();
  }
}","@SuppressWarnings(""String_Node_Str"") @Override protected void doCommand(String name,Object value) throws Exception {
  if (!ProgramOptionConstants.INSTANCES.equals(name) || !(value instanceof Map)) {
    return;
  }
  Map<String,String> command=(Map<String,String>)value;
  lock.lock();
  try {
    changeInstances(command.get(""String_Node_Str""),Integer.valueOf(command.get(""String_Node_Str"")),Integer.valueOf(command.get(""String_Node_Str"")));
  }
 catch (  Throwable t) {
    LOG.error(String.format(""String_Node_Str"",command),t);
    throw t;
  }
 finally {
    lock.unlock();
  }
}"
6235,"@SuppressWarnings(""String_Node_Str"") @Override protected void doCommand(String name,Object value) throws Exception {
  if (!ProgramOptionConstants.INSTANCES.equals(name) || !(value instanceof Map)) {
    return;
  }
  Map<String,String> command=(Map<String,String>)value;
  try {
    for (    Map.Entry<String,String> entry : command.entrySet()) {
      LOG.info(""String_Node_Str"",entry.getKey(),entry.getValue());
      changeInstances(entry.getKey(),Integer.valueOf(entry.getValue()));
      LOG.info(""String_Node_Str"",entry.getKey(),entry.getValue());
    }
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",command,t);
  }
}","@SuppressWarnings(""String_Node_Str"") @Override protected void doCommand(String name,Object value) throws Exception {
  if (!ProgramOptionConstants.INSTANCES.equals(name) || !(value instanceof Map)) {
    return;
  }
  Map<String,String> command=(Map<String,String>)value;
  try {
    for (    Map.Entry<String,String> entry : command.entrySet()) {
      LOG.info(""String_Node_Str"",entry.getKey(),entry.getValue());
      changeInstances(entry.getKey(),Integer.valueOf(entry.getValue()));
      LOG.info(""String_Node_Str"",entry.getKey(),entry.getValue());
    }
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",command,t);
    throw t;
  }
}"
6236,"@Override @SuppressWarnings(""String_Node_Str"") protected void doCommand(String name,Object value) throws Exception {
  if (!ProgramOptionConstants.INSTANCES.equals(name) || !(value instanceof Map)) {
    return;
  }
  Map<String,String> command=(Map<String,String>)value;
  lock.lock();
  try {
    changeInstances(command.get(""String_Node_Str""),Integer.valueOf(command.get(""String_Node_Str"")));
  }
 catch (  Throwable t) {
    LOG.error(String.format(""String_Node_Str"",command),t);
  }
 finally {
    lock.unlock();
  }
}","@Override @SuppressWarnings(""String_Node_Str"") protected void doCommand(String name,Object value) throws Exception {
  if (!ProgramOptionConstants.INSTANCES.equals(name) || !(value instanceof Map)) {
    return;
  }
  Map<String,String> command=(Map<String,String>)value;
  lock.lock();
  try {
    changeInstances(command.get(""String_Node_Str""),Integer.valueOf(command.get(""String_Node_Str"")));
  }
 catch (  Throwable t) {
    LOG.error(String.format(""String_Node_Str"",command),t);
    stop();
    throw t;
  }
 finally {
    lock.unlock();
  }
}"
6237,"private void setServiceInstances(ProgramId programId,int instances) throws ExecutionException, InterruptedException {
  int oldInstances=store.getServiceInstances(programId.toId());
  if (oldInstances != instances) {
    store.setServiceInstances(programId.toId(),instances);
    ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(programId);
    if (runtimeInfo != null) {
      runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(programId.getProgram(),String.valueOf(instances))).get();
    }
  }
}","private void setServiceInstances(ProgramId programId,int instances) throws ExecutionException, InterruptedException {
  int oldInstances=store.getServiceInstances(programId.toId());
  if (oldInstances != instances) {
    store.setServiceInstances(programId.toId(),instances);
    ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(programId);
    if (runtimeInfo != null) {
      runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(""String_Node_Str"",programId.getProgram(),""String_Node_Str"",String.valueOf(instances),""String_Node_Str"",String.valueOf(oldInstances))).get();
    }
  }
}"
6238,"private void setWorkerInstances(ProgramId programId,int instances) throws ExecutionException, InterruptedException {
  int oldInstances=store.getWorkerInstances(programId.toId());
  if (oldInstances != instances) {
    store.setWorkerInstances(programId.toId(),instances);
    ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(programId);
    if (runtimeInfo != null) {
      runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(programId.getProgram(),String.valueOf(instances))).get();
    }
  }
}","private void setWorkerInstances(ProgramId programId,int instances) throws ExecutionException, InterruptedException {
  int oldInstances=store.getWorkerInstances(programId.toId());
  if (oldInstances != instances) {
    store.setWorkerInstances(programId.toId(),instances);
    ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(programId);
    if (runtimeInfo != null) {
      runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(""String_Node_Str"",programId.getProgram(),""String_Node_Str"",String.valueOf(instances),""String_Node_Str"",String.valueOf(oldInstances))).get();
    }
  }
}"
6239,"/** 
 * Wraps the given   {@link Throwable} as a {@link TransactionFailureException} if it is not already an instance of{@link TransactionFailureException}.
 */
public static TransactionFailureException asTransactionFailure(Throwable t){
  if (t instanceof TransactionFailureException) {
    return (TransactionFailureException)t;
  }
  return new TransactionFailureException(""String_Node_Str"" + t.getMessage(),t);
}","/** 
 * Wraps the given   {@link Throwable} as a {@link TransactionFailureException} if it is not already an instance of{@link TransactionFailureException}.
 * @param t the original exception
 * @param message the exception message to use in case wrapping is needed
 */
public static TransactionFailureException asTransactionFailure(Throwable t,String message){
  if (t instanceof TransactionFailureException) {
    return (TransactionFailureException)t;
  }
  return new TransactionFailureException(message,t);
}"
6240,"@Override protected Scanner scanPersisted(Scan scan){
  byte[] startRow=scan.getStartRow();
  byte[] stopRow=scan.getStopRow();
  NavigableMap<byte[],NavigableMap<byte[],NavigableMap<Long,byte[]>>> rowRange=InMemoryTableService.getRowRange(getTableName(),startRow,stopRow,tx == null ? null : tx.getReadPointer());
  NavigableMap<byte[],NavigableMap<byte[],byte[]>> visibleRowRange=getLatestNotExcludedRows(rowRange,tx);
  NavigableMap<byte[],NavigableMap<byte[],byte[]>> rows=unwrapDeletesForRows(visibleRowRange);
  rows=applyFilter(rows,scan.getFilter());
  return new InMemoryScanner(rows.entrySet().iterator());
}","@Override protected Scanner scanPersisted(Scan scan){
  byte[] startRow=scan.getStartRow();
  byte[] stopRow=scan.getStopRow();
  NavigableMap<byte[],NavigableMap<byte[],NavigableMap<Long,byte[]>>> rowRange=InMemoryTableService.getRowRange(getTableName(),startRow,stopRow,tx == null ? null : tx);
  NavigableMap<byte[],NavigableMap<byte[],byte[]>> visibleRowRange=getLatestNotExcludedRows(rowRange,tx);
  NavigableMap<byte[],NavigableMap<byte[],byte[]>> rows=unwrapDeletesForRows(visibleRowRange);
  rows=applyFilter(rows,scan.getFilter());
  return new InMemoryScanner(rows.entrySet().iterator());
}"
6241,"private NavigableMap<byte[],byte[]> getInternal(byte[] row,@Nullable byte[][] columns) throws IOException {
  if (tx == null) {
    NavigableMap<byte[],NavigableMap<Long,byte[]>> rowMap=InMemoryTableService.get(getTableName(),row,NO_TX_VERSION);
    return unwrapDeletes(filterByColumns(getLatest(rowMap),columns));
  }
  NavigableMap<byte[],NavigableMap<Long,byte[]>> rowMap=InMemoryTableService.get(getTableName(),row,tx.getReadPointer());
  if (rowMap == null) {
    return EMPTY_ROW_MAP;
  }
  if (!tx.hasExcludes()) {
    return unwrapDeletes(filterByColumns(getLatest(rowMap),columns));
  }
  NavigableMap<byte[],byte[]> result=filterByColumns(getLatestNotExcluded(rowMap,tx),columns);
  return unwrapDeletes(result);
}","private NavigableMap<byte[],byte[]> getInternal(byte[] row,@Nullable byte[][] columns) throws IOException {
  if (tx == null) {
    NavigableMap<byte[],NavigableMap<Long,byte[]>> rowMap=InMemoryTableService.get(getTableName(),row,tx);
    return unwrapDeletes(filterByColumns(getLatest(rowMap),columns));
  }
  NavigableMap<byte[],NavigableMap<Long,byte[]>> rowMap=InMemoryTableService.get(getTableName(),row,tx);
  if (rowMap == null) {
    return EMPTY_ROW_MAP;
  }
  if (!tx.hasExcludes()) {
    return unwrapDeletes(filterByColumns(getLatest(rowMap),columns));
  }
  NavigableMap<byte[],byte[]> result=filterByColumns(getLatestNotExcluded(rowMap,tx),columns);
  return unwrapDeletes(result);
}"
6242,"private static NavigableMap<byte[],NavigableMap<Long,Update>> getVisible(NavigableMap<byte[],NavigableMap<Long,Update>> rowMap,Long version){
  if (rowMap == null) {
    return null;
  }
  NavigableMap<byte[],NavigableMap<Long,Update>> result=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  for (  Map.Entry<byte[],NavigableMap<Long,Update>> column : rowMap.entrySet()) {
    NavigableMap<Long,Update> visbleValues=column.getValue();
    if (version != null) {
      visbleValues=visbleValues.headMap(version,true);
    }
    if (visbleValues.size() > 0) {
      NavigableMap<Long,Update> colMap=createVersionedValuesMap(visbleValues);
      result.put(column.getKey(),colMap);
    }
  }
  return result;
}","private static NavigableMap<byte[],NavigableMap<Long,Update>> getVisible(NavigableMap<byte[],NavigableMap<Long,Update>> rowMap,final Transaction tx){
  if (rowMap == null) {
    return null;
  }
  NavigableMap<byte[],NavigableMap<Long,Update>> result=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  for (  Map.Entry<byte[],NavigableMap<Long,Update>> column : rowMap.entrySet()) {
    SortedMap<Long,Update> visbleValues=column.getValue();
    if (tx != null) {
      visbleValues=Maps.filterKeys(visbleValues,new Predicate<Long>(){
        @Override public boolean apply(        Long version){
          return tx.isVisible(version);
        }
      }
);
    }
    if (visbleValues.size() > 0) {
      NavigableMap<Long,Update> colMap=createVersionedValuesMap(visbleValues);
      result.put(column.getKey(),colMap);
    }
  }
  return result;
}"
6243,"public static synchronized NavigableMap<byte[],NavigableMap<byte[],NavigableMap<Long,byte[]>>> getRowRange(String tableName,byte[] startRow,byte[] stopRow,Long version){
  ConcurrentNavigableMap<byte[],NavigableMap<byte[],NavigableMap<Long,Update>>> tableData=tables.get(tableName);
  NavigableMap<byte[],NavigableMap<byte[],NavigableMap<Long,Update>>> rows;
  if (startRow == null && stopRow == null) {
    rows=tableData;
  }
 else   if (startRow == null) {
    rows=tableData.headMap(stopRow,false);
  }
 else   if (stopRow == null) {
    rows=tableData.tailMap(startRow,true);
  }
 else {
    rows=tableData.subMap(startRow,true,stopRow,false);
  }
  NavigableMap<byte[],NavigableMap<byte[],NavigableMap<Long,byte[]>>> result=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  for (  Map.Entry<byte[],NavigableMap<byte[],NavigableMap<Long,Update>>> rowMap : rows.entrySet()) {
    NavigableMap<byte[],NavigableMap<Long,Update>> columns=version == null ? rowMap.getValue() : getVisible(rowMap.getValue(),version);
    result.put(copy(rowMap.getKey()),deepCopy(Updates.rowToBytes(columns)));
  }
  return result;
}","public static synchronized NavigableMap<byte[],NavigableMap<byte[],NavigableMap<Long,byte[]>>> getRowRange(String tableName,byte[] startRow,byte[] stopRow,@Nullable Transaction tx){
  ConcurrentNavigableMap<byte[],NavigableMap<byte[],NavigableMap<Long,Update>>> tableData=tables.get(tableName);
  NavigableMap<byte[],NavigableMap<byte[],NavigableMap<Long,Update>>> rows;
  if (startRow == null && stopRow == null) {
    rows=tableData;
  }
 else   if (startRow == null) {
    rows=tableData.headMap(stopRow,false);
  }
 else   if (stopRow == null) {
    rows=tableData.tailMap(startRow,true);
  }
 else {
    rows=tableData.subMap(startRow,true,stopRow,false);
  }
  NavigableMap<byte[],NavigableMap<byte[],NavigableMap<Long,byte[]>>> result=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  for (  Map.Entry<byte[],NavigableMap<byte[],NavigableMap<Long,Update>>> rowMap : rows.entrySet()) {
    NavigableMap<byte[],NavigableMap<Long,Update>> columns=tx == null ? rowMap.getValue() : getVisible(rowMap.getValue(),tx);
    result.put(copy(rowMap.getKey()),deepCopy(Updates.rowToBytes(columns)));
  }
  return result;
}"
6244,"public static synchronized NavigableMap<byte[],NavigableMap<Long,byte[]>> get(String tableName,byte[] row,Long version){
  ConcurrentNavigableMap<byte[],NavigableMap<byte[],NavigableMap<Long,Update>>> table=tables.get(tableName);
  Preconditions.checkArgument(table != null,""String_Node_Str"" + tableName);
  NavigableMap<byte[],NavigableMap<Long,Update>> rowMap=table.get(row);
  return deepCopy(Updates.rowToBytes(getVisible(rowMap,version)));
}","public static synchronized NavigableMap<byte[],NavigableMap<Long,byte[]>> get(String tableName,byte[] row,@Nullable Transaction tx){
  ConcurrentNavigableMap<byte[],NavigableMap<byte[],NavigableMap<Long,Update>>> table=tables.get(tableName);
  Preconditions.checkArgument(table != null,""String_Node_Str"" + tableName);
  NavigableMap<byte[],NavigableMap<Long,Update>> rowMap=table.get(row);
  return deepCopy(Updates.rowToBytes(getVisible(rowMap,tx)));
}"
6245,"private static NavigableMap<Long,Update> createVersionedValuesMap(NavigableMap<Long,Update> copy){
  NavigableMap<Long,Update> map=Maps.newTreeMap(VERSIONED_VALUE_MAP_COMPARATOR);
  map.putAll(copy);
  return map;
}","private static NavigableMap<Long,Update> createVersionedValuesMap(SortedMap<Long,Update> copy){
  NavigableMap<Long,Update> map=Maps.newTreeMap(VERSIONED_VALUE_MAP_COMPARATOR);
  map.putAll(copy);
  return map;
}"
6246,"private void verify123(){
  NavigableMap<byte[],NavigableMap<Long,byte[]>> rowFromGet=InMemoryTableService.get(""String_Node_Str"",new byte[]{1},1L);
  Assert.assertEquals(1,rowFromGet.size());
  Assert.assertArrayEquals(new byte[]{2},rowFromGet.firstEntry().getKey());
  Assert.assertArrayEquals(new byte[]{3},rowFromGet.firstEntry().getValue().get(1L));
}","private void verify123(){
  NavigableMap<byte[],NavigableMap<Long,byte[]>> rowFromGet=InMemoryTableService.get(""String_Node_Str"",new byte[]{1},new Transaction(1L,2L,new long[0],new long[0],1L));
  Assert.assertEquals(1,rowFromGet.size());
  Assert.assertArrayEquals(new byte[]{2},rowFromGet.firstEntry().getKey());
  Assert.assertArrayEquals(new byte[]{3},rowFromGet.firstEntry().getValue().get(1L));
}"
6247,"@Test public void testInternalsNotLeaking(){
  InMemoryTableService.create(""String_Node_Str"");
  NavigableMap<byte[],NavigableMap<byte[],Update>> updates=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  NavigableMap<byte[],Update> rowUpdate=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  byte[] rowParam=new byte[]{1};
  byte[] columnParam=new byte[]{2};
  byte[] valParam=new byte[]{3};
  rowUpdate.put(columnParam,new PutValue(valParam));
  updates.put(rowParam,rowUpdate);
  InMemoryTableService.merge(""String_Node_Str"",updates,1L);
  verify123();
  updates.remove(rowParam);
  rowUpdate.remove(columnParam);
  rowParam[0]++;
  columnParam[0]++;
  valParam[0]++;
  verify123();
  NavigableMap<byte[],NavigableMap<Long,byte[]>> rowFromGet=InMemoryTableService.get(""String_Node_Str"",new byte[]{1},1L);
  Assert.assertEquals(1,rowFromGet.size());
  byte[] columnFromGet=rowFromGet.firstEntry().getKey();
  Assert.assertArrayEquals(new byte[]{2},columnFromGet);
  byte[] valFromGet=rowFromGet.firstEntry().getValue().get(1L);
  Assert.assertArrayEquals(new byte[]{3},valFromGet);
  rowFromGet.firstEntry().getValue().remove(1L);
  rowFromGet.remove(columnFromGet);
  columnFromGet[0]++;
  valFromGet[0]++;
  verify123();
  NavigableMap<byte[],NavigableMap<byte[],NavigableMap<Long,byte[]>>> fromGetRange=InMemoryTableService.getRowRange(""String_Node_Str"",null,null,1L);
  Assert.assertEquals(1,fromGetRange.size());
  byte[] keyFromGetRange=fromGetRange.firstEntry().getKey();
  Assert.assertArrayEquals(new byte[]{1},keyFromGetRange);
  NavigableMap<byte[],NavigableMap<Long,byte[]>> rowFromGetRange=fromGetRange.get(new byte[]{1});
  Assert.assertEquals(1,rowFromGetRange.size());
  byte[] columnFromGetRange=rowFromGetRange.firstEntry().getKey();
  Assert.assertArrayEquals(new byte[]{2},columnFromGetRange);
  byte[] valFromGetRange=rowFromGetRange.firstEntry().getValue().get(1L);
  Assert.assertArrayEquals(new byte[]{3},valFromGetRange);
  rowFromGetRange.firstEntry().getValue().remove(1L);
  rowFromGetRange.remove(columnFromGetRange);
  fromGetRange.remove(keyFromGetRange);
  keyFromGetRange[0]++;
  columnFromGetRange[0]++;
  valFromGet[0]++;
  verify123();
}","@Test public void testInternalsNotLeaking(){
  InMemoryTableService.create(""String_Node_Str"");
  NavigableMap<byte[],NavigableMap<byte[],Update>> updates=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  NavigableMap<byte[],Update> rowUpdate=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  byte[] rowParam=new byte[]{1};
  byte[] columnParam=new byte[]{2};
  byte[] valParam=new byte[]{3};
  rowUpdate.put(columnParam,new PutValue(valParam));
  updates.put(rowParam,rowUpdate);
  InMemoryTableService.merge(""String_Node_Str"",updates,1L);
  verify123();
  updates.remove(rowParam);
  rowUpdate.remove(columnParam);
  rowParam[0]++;
  columnParam[0]++;
  valParam[0]++;
  verify123();
  NavigableMap<byte[],NavigableMap<Long,byte[]>> rowFromGet=InMemoryTableService.get(""String_Node_Str"",new byte[]{1},new Transaction(1L,2L,new long[0],new long[0],1L));
  Assert.assertEquals(1,rowFromGet.size());
  byte[] columnFromGet=rowFromGet.firstEntry().getKey();
  Assert.assertArrayEquals(new byte[]{2},columnFromGet);
  byte[] valFromGet=rowFromGet.firstEntry().getValue().get(1L);
  Assert.assertArrayEquals(new byte[]{3},valFromGet);
  rowFromGet.firstEntry().getValue().remove(1L);
  rowFromGet.remove(columnFromGet);
  columnFromGet[0]++;
  valFromGet[0]++;
  verify123();
  NavigableMap<byte[],NavigableMap<byte[],NavigableMap<Long,byte[]>>> fromGetRange=InMemoryTableService.getRowRange(""String_Node_Str"",null,null,new Transaction(1L,2L,new long[0],new long[0],1L));
  Assert.assertEquals(1,fromGetRange.size());
  byte[] keyFromGetRange=fromGetRange.firstEntry().getKey();
  Assert.assertArrayEquals(new byte[]{1},keyFromGetRange);
  NavigableMap<byte[],NavigableMap<Long,byte[]>> rowFromGetRange=fromGetRange.get(new byte[]{1});
  Assert.assertEquals(1,rowFromGetRange.size());
  byte[] columnFromGetRange=rowFromGetRange.firstEntry().getKey();
  Assert.assertArrayEquals(new byte[]{2},columnFromGetRange);
  byte[] valFromGetRange=rowFromGetRange.firstEntry().getValue().get(1L);
  Assert.assertArrayEquals(new byte[]{3},valFromGetRange);
  rowFromGetRange.firstEntry().getValue().remove(1L);
  rowFromGetRange.remove(columnFromGetRange);
  fromGetRange.remove(keyFromGetRange);
  keyFromGetRange[0]++;
  columnFromGetRange[0]++;
  valFromGet[0]++;
  verify123();
}"
6248,"/** 
 * Gets all the plugins of the given type and name available to the given artifact.
 * @param artifactId the id of the artifact to get
 * @param pluginType the type of plugins to get
 * @param pluginName the name of the plugins to get
 * @param scope the scope of the artifact
 * @return list of {@link PluginInfo}
 * @throws NotFoundException if the given artifact does not exist or plugins for that artifact do not exist
 * @throws IOException if a network error occurred
 * @throws UnauthenticatedException if the request is not authorized successfully in the gateway server
 */
public List<PluginInfo> getPluginInfo(Id.Artifact artifactId,String pluginType,String pluginName,ArtifactScope scope) throws IOException, UnauthenticatedException, NotFoundException {
  String path=String.format(""String_Node_Str"",artifactId.getName(),artifactId.getVersion().getVersion(),pluginType,pluginName);
  URL url=config.resolveNamespacedURLV3(artifactId.getNamespace(),path);
  HttpResponse response=restClient.execute(HttpMethod.GET,url,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new NotFoundException(response.getResponseBodyAsString());
  }
  return ObjectResponse.<List<PluginInfo>>fromJsonBody(response,PLUGIN_INFOS_TYPE).getResponseObject();
}","/** 
 * Gets all the plugins of the given type and name available to the given artifact.
 * @param artifactId the id of the artifact to get
 * @param pluginType the type of plugins to get
 * @param pluginName the name of the plugins to get
 * @param scope the scope of the artifact
 * @return list of {@link PluginInfo}
 * @throws NotFoundException if the given artifact does not exist or plugins for that artifact do not exist
 * @throws IOException if a network error occurred
 * @throws UnauthenticatedException if the request is not authorized successfully in the gateway server
 */
public List<PluginInfo> getPluginInfo(Id.Artifact artifactId,String pluginType,String pluginName,ArtifactScope scope) throws IOException, UnauthenticatedException, NotFoundException {
  String path=String.format(""String_Node_Str"",artifactId.getName(),artifactId.getVersion().getVersion(),pluginType,pluginName,scope.name());
  URL url=config.resolveNamespacedURLV3(artifactId.getNamespace(),path);
  HttpResponse response=restClient.execute(HttpMethod.GET,url,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new NotFoundException(response.getResponseBodyAsString());
  }
  return ObjectResponse.<List<PluginInfo>>fromJsonBody(response,PLUGIN_INFOS_TYPE).getResponseObject();
}"
6249,"public SmartWorkflow(PipelineSpec spec,PipelinePlan plan,ApplicationConfigurer applicationConfigurer){
  this.spec=spec;
  this.plan=plan;
  this.applicationConfigurer=applicationConfigurer;
  this.phaseNum=1;
  this.dag=new ControlDag(plan.getPhaseConnections());
  this.dag.flatten();
  this.connectorDatasets=new HashMap<>();
}","public SmartWorkflow(PipelineSpec spec,PipelinePlan plan,ApplicationConfigurer applicationConfigurer){
  this.spec=spec;
  this.plan=plan;
  this.applicationConfigurer=applicationConfigurer;
  this.phaseNum=1;
  this.connectorDatasets=new HashMap<>();
}"
6250,"@Override protected void configure(){
  setName(NAME);
  setDescription(DESCRIPTION);
  Map<String,String> properties=new HashMap<>();
  properties.put(""String_Node_Str"",GSON.toJson(spec));
  setProperties(properties);
  String start=dag.getSources().iterator().next();
  addPrograms(start,getConfigurer());
}","@Override protected void configure(){
  setName(NAME);
  setDescription(DESCRIPTION);
  Map<String,String> properties=new HashMap<>();
  properties.put(""String_Node_Str"",GSON.toJson(spec));
  setProperties(properties);
  if (plan.getPhaseConnections().isEmpty()) {
    if (plan.getPhases().size() == 1) {
      addProgram(plan.getPhases().keySet().iterator().next(),new TrunkProgramAdder(getConfigurer()));
      return;
    }
    WorkflowForkConfigurer forkConfigurer=getConfigurer().fork();
    for (    String phaseName : plan.getPhases().keySet()) {
      addProgram(phaseName,new BranchProgramAdder(forkConfigurer));
    }
    forkConfigurer.join();
    return;
  }
  dag=new ControlDag(plan.getPhaseConnections());
  dag.flatten();
  String start=dag.getSources().iterator().next();
  addPrograms(start,getConfigurer());
}"
6251,"@Test public void testMultiSource() throws Exception {
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",IdentityTransform.getPlugin())).addStage(new ETLStage(""String_Node_Str"",IdentityTransform.getPlugin())).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(""String_Node_Str""))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").build();
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(ETLBATCH_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  StructuredRecord recordSamuel=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordBob=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordJane=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build();
  DataSetManager<Table> inputManager=getDataset(Id.Namespace.DEFAULT,""String_Node_Str"");
  MockSource.writeInput(inputManager,ImmutableList.of(recordSamuel));
  inputManager=getDataset(Id.Namespace.DEFAULT,""String_Node_Str"");
  MockSource.writeInput(inputManager,ImmutableList.of(recordBob));
  inputManager=getDataset(Id.Namespace.DEFAULT,""String_Node_Str"");
  MockSource.writeInput(inputManager,ImmutableList.of(recordJane));
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start();
  workflowManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<Table> sinkManager=getDataset(""String_Node_Str"");
  Set<StructuredRecord> expected=ImmutableSet.of(recordSamuel,recordBob);
  Set<StructuredRecord> actual=Sets.newHashSet(MockSink.readOutput(sinkManager));
  Assert.assertEquals(expected,actual);
  sinkManager=getDataset(""String_Node_Str"");
  expected=ImmutableSet.of(recordSamuel,recordBob,recordJane);
  actual=Sets.newHashSet(MockSink.readOutput(sinkManager));
  Assert.assertEquals(expected,actual);
}","@Test public void testMultiSource() throws Exception {
  ETLBatchConfig etlConfig=ETLBatchConfig.builder(""String_Node_Str"").addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockSource.getPlugin(""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",IdentityTransform.getPlugin())).addStage(new ETLStage(""String_Node_Str"",IdentityTransform.getPlugin())).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(""String_Node_Str""))).addStage(new ETLStage(""String_Node_Str"",MockSink.getPlugin(""String_Node_Str""))).addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").addConnection(""String_Node_Str"",""String_Node_Str"").build();
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  StructuredRecord recordSamuel=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordBob=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build();
  StructuredRecord recordJane=StructuredRecord.builder(schema).set(""String_Node_Str"",""String_Node_Str"").build();
  DataSetManager<Table> inputManager=getDataset(Id.Namespace.DEFAULT,""String_Node_Str"");
  MockSource.writeInput(inputManager,ImmutableList.of(recordSamuel));
  inputManager=getDataset(Id.Namespace.DEFAULT,""String_Node_Str"");
  MockSource.writeInput(inputManager,ImmutableList.of(recordBob));
  inputManager=getDataset(Id.Namespace.DEFAULT,""String_Node_Str"");
  MockSource.writeInput(inputManager,ImmutableList.of(recordJane));
  WorkflowManager workflowManager=appManager.getWorkflowManager(SmartWorkflow.NAME);
  workflowManager.start();
  workflowManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<Table> sinkManager=getDataset(""String_Node_Str"");
  Set<StructuredRecord> expected=ImmutableSet.of(recordSamuel,recordBob);
  Set<StructuredRecord> actual=Sets.newHashSet(MockSink.readOutput(sinkManager));
  Assert.assertEquals(expected,actual);
  sinkManager=getDataset(""String_Node_Str"");
  expected=ImmutableSet.of(recordSamuel,recordBob,recordJane);
  actual=Sets.newHashSet(MockSink.readOutput(sinkManager));
  Assert.assertEquals(expected,actual);
}"
6252,"@Override public PluginClass deserialize(JsonElement json,Type typeOfT,JsonDeserializationContext context) throws JsonParseException {
  if (!json.isJsonObject()) {
    throw new JsonParseException(""String_Node_Str"");
  }
  JsonObject jsonObj=json.getAsJsonObject();
  String type=jsonObj.has(""String_Node_Str"") ? jsonObj.get(""String_Node_Str"").getAsString() : Plugin.DEFAULT_TYPE;
  String name=getRequired(jsonObj,""String_Node_Str"").getAsString();
  String description=jsonObj.has(""String_Node_Str"") ? jsonObj.get(""String_Node_Str"").getAsString() : ""String_Node_Str"";
  String className=getRequired(jsonObj,""String_Node_Str"").getAsString();
  JsonArray endpoints=getRequired(jsonObj,""String_Node_Str"").getAsJsonArray();
  Set<String> endpointsSet=new HashSet<>();
  Iterator<JsonElement> iterator=endpoints.iterator();
  while (iterator.hasNext()) {
    endpointsSet.add(iterator.next().getAsString());
  }
  Map<String,PluginPropertyField> properties=jsonObj.has(""String_Node_Str"") ? context.<Map<String,PluginPropertyField>>deserialize(jsonObj.get(""String_Node_Str""),PROPERTIES_TYPE) : ImmutableMap.<String,PluginPropertyField>of();
  return new PluginClass(type,name,description,className,null,properties,endpointsSet);
}","@Override public PluginClass deserialize(JsonElement json,Type typeOfT,JsonDeserializationContext context) throws JsonParseException {
  if (!json.isJsonObject()) {
    throw new JsonParseException(""String_Node_Str"");
  }
  JsonObject jsonObj=json.getAsJsonObject();
  String type=jsonObj.has(""String_Node_Str"") ? jsonObj.get(""String_Node_Str"").getAsString() : Plugin.DEFAULT_TYPE;
  String name=getRequired(jsonObj,""String_Node_Str"").getAsString();
  String description=jsonObj.has(""String_Node_Str"") ? jsonObj.get(""String_Node_Str"").getAsString() : ""String_Node_Str"";
  String className=getRequired(jsonObj,""String_Node_Str"").getAsString();
  Set<String> endpointsSet=new HashSet<>();
  if (jsonObj.has(""String_Node_Str"")) {
    endpointsSet=context.deserialize(jsonObj.get(""String_Node_Str""),ENDPOINTS_TYPE);
  }
  Map<String,PluginPropertyField> properties=jsonObj.has(""String_Node_Str"") ? context.<Map<String,PluginPropertyField>>deserialize(jsonObj.get(""String_Node_Str""),PROPERTIES_TYPE) : ImmutableMap.<String,PluginPropertyField>of();
  return new PluginClass(type,name,description,className,null,properties,endpointsSet);
}"
6253,"@Override public PartitionConsumerResult doConsume(ConsumerWorkingSet workingSet,PartitionAcceptor acceptor){
  doExpiry(workingSet);
  workingSet.populate(getPartitionedFileSet(),getConfiguration());
  long now=System.currentTimeMillis();
  List<PartitionDetail> toConsume=new ArrayList<>();
  List<? extends ConsumablePartition> partitions=workingSet.getPartitions();
  for (  ConsumablePartition consumablePartition : partitions) {
    if (ProcessState.AVAILABLE != consumablePartition.getProcessState()) {
      continue;
    }
    PartitionDetail partition=getPartitionedFileSet().getPartition(consumablePartition.getPartitionKey());
    PartitionAcceptor.Return accept=acceptor.accept(partition);
switch (accept) {
case ACCEPT:
      consumablePartition.take();
    consumablePartition.setTimestamp(now);
  toConsume.add(partition);
continue;
case SKIP:
continue;
case STOP:
break;
}
}
return new PartitionConsumerResult(toConsume,removeDiscardedPartitions(workingSet));
}","@Override public PartitionConsumerResult doConsume(ConsumerWorkingSet workingSet,PartitionAcceptor acceptor){
  doExpiry(workingSet);
  workingSet.populate(getPartitionedFileSet(),getConfiguration());
  long now=System.currentTimeMillis();
  List<PartitionDetail> toConsume=new ArrayList<>();
  List<? extends ConsumablePartition> partitions=workingSet.getPartitions();
  for (  ConsumablePartition consumablePartition : partitions) {
    if (ProcessState.AVAILABLE != consumablePartition.getProcessState()) {
      continue;
    }
    PartitionDetail partition=getPartitionedFileSet().getPartition(consumablePartition.getPartitionKey());
    if (partition == null) {
      continue;
    }
    PartitionAcceptor.Return accept=acceptor.accept(partition);
switch (accept) {
case ACCEPT:
      consumablePartition.take();
    consumablePartition.setTimestamp(now);
  toConsume.add(partition);
continue;
case SKIP:
continue;
case STOP:
break;
}
}
return new PartitionConsumerResult(toConsume,removeDiscardedPartitions(workingSet));
}"
6254,"@Test public void testDeployApplicationInNamespace() throws Exception {
  Id.Namespace namespace=createNamespace(""String_Node_Str"");
  ClientConfig clientConfig=new ClientConfig.Builder(getClientConfig()).build();
  deployApplication(namespace,TestApplication.class);
  ClientConfig defaultClientConfig=new ClientConfig.Builder(getClientConfig()).build();
  Assert.assertEquals(0,new ApplicationClient(defaultClientConfig).list(Id.Namespace.DEFAULT).size());
  ApplicationClient applicationClient=new ApplicationClient(clientConfig);
  Assert.assertEquals(""String_Node_Str"",applicationClient.list(namespace).get(0).getName());
  applicationClient.delete(Id.Application.from(namespace,""String_Node_Str""));
  Assert.assertEquals(0,new ApplicationClient(clientConfig).list(namespace).size());
}","@Test public void testDeployApplicationInNamespace() throws Exception {
  Id.Namespace namespace=createNamespace(""String_Node_Str"");
  ClientConfig clientConfig=new ClientConfig.Builder(getClientConfig()).build();
  deployApplication(namespace,TestApplication.class);
  ClientConfig defaultClientConfig=new ClientConfig.Builder(getClientConfig()).build();
  Assert.assertEquals(0,new ApplicationClient(defaultClientConfig).list(Id.Namespace.DEFAULT).size());
  ApplicationClient applicationClient=new ApplicationClient(clientConfig);
  Assert.assertEquals(TestApplication.NAME,applicationClient.list(namespace).get(0).getName());
  applicationClient.delete(Id.Application.from(namespace,TestApplication.NAME));
  Assert.assertEquals(0,new ApplicationClient(clientConfig).list(namespace).size());
}"
6255,"@Override protected void configureFlow(){
  setName(NAME);
  setDescription(""String_Node_Str"");
  addFlowlet(""String_Node_Str"",new TestFlowlet());
  connectStream(INPUT_STREAM,""String_Node_Str"");
}","@Override protected void configureFlow(){
  setName(NAME);
  setDescription(""String_Node_Str"");
  addFlowlet(TestFlowlet.NAME,new TestFlowlet());
  connectStream(INPUT_STREAM,""String_Node_Str"");
}"
6256,"@Override public ApplicationManager deployApplication(Id.Application appId,AppRequest appRequest) throws Exception {
  appFabricClient.deployApplication(appId,appRequest);
  ArtifactSummary requestedArtifact=appRequest.getArtifact();
  Id.Artifact artifactId=Id.Artifact.from(ArtifactScope.SYSTEM.equals(requestedArtifact.getScope()) ? Id.Namespace.SYSTEM : appId.getNamespace(),requestedArtifact.getName(),requestedArtifact.getVersion());
  ArtifactDetail artifactDetail=artifactRepository.getArtifact(artifactId);
  return appManagerFactory.create(appId,artifactDetail.getDescriptor().getLocation());
}","@Override public ApplicationManager deployApplication(Id.Application appId,AppRequest appRequest) throws Exception {
  appFabricClient.deployApplication(appId,appRequest);
  return appManagerFactory.create(appId);
}"
6257,"ApplicationManager create(@Assisted(""String_Node_Str"") Id.Application applicationId,Location deployedJar);","ApplicationManager create(@Assisted(""String_Node_Str"") Id.Application applicationId);"
6258,"/** 
 * Constructs a   {@link Schema} from the ""key"":""schema"" pair.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param key The json property name that need to match.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} object representing the schema of the json input.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readInnerSchema(JsonReader reader,String key,Set<String> knownRecords) throws IOException {
  if (!key.equals(reader.nextName())) {
    throw new IOException(""String_Node_Str"" + key + ""String_Node_Str"");
  }
  return read(reader,knownRecords);
}","/** 
 * Constructs a   {@link Schema} from the ""key"":""schema"" pair.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param key The json property name that need to match.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} object representing the schema of the json input.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readInnerSchema(JsonReader reader,String key,Map<String,Schema> knownRecords) throws IOException {
  if (!key.equals(reader.nextName())) {
    throw new IOException(""String_Node_Str"" + key + ""String_Node_Str"");
  }
  return read(reader,knownRecords);
}"
6259,"/** 
 * Reads json value and convert it into   {@link Schema} object.
 * @param reader Source of json
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} reflecting the json.
 * @throws IOException Any error during reading.
 */
private Schema read(JsonReader reader,Set<String> knownRecords) throws IOException {
  JsonToken token=reader.peek();
switch (token) {
case NULL:
    return null;
case STRING:
{
    String name=reader.nextString();
    if (knownRecords.contains(name)) {
      return Schema.recordOf(name);
    }
    return Schema.of(Schema.Type.valueOf(name.toUpperCase()));
  }
case BEGIN_ARRAY:
return readUnion(reader,knownRecords);
case BEGIN_OBJECT:
{
reader.beginObject();
String name=reader.nextName();
if (!""String_Node_Str"".equals(name)) {
  throw new IOException(""String_Node_Str"");
}
Schema.Type schemaType=Schema.Type.valueOf(reader.nextString().toUpperCase());
Schema schema;
switch (schemaType) {
case ENUM:
  schema=readEnum(reader);
break;
case ARRAY:
schema=readArray(reader,knownRecords);
break;
case MAP:
schema=readMap(reader,knownRecords);
break;
case RECORD:
schema=readRecord(reader,knownRecords);
break;
default :
schema=Schema.of(schemaType);
}
reader.endObject();
return schema;
}
}
throw new IOException(""String_Node_Str"");
}","/** 
 * Reads json value and convert it into   {@link Schema} object.
 * @param reader Source of json
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} reflecting the json.
 * @throws IOException Any error during reading.
 */
private Schema read(JsonReader reader,Map<String,Schema> knownRecords) throws IOException {
  JsonToken token=reader.peek();
switch (token) {
case NULL:
    return null;
case STRING:
{
    String name=reader.nextString();
    if (knownRecords.containsKey(name)) {
      Schema schema=knownRecords.get(name);
      return schema == null ? Schema.recordOf(name) : schema;
    }
    return Schema.of(Schema.Type.valueOf(name.toUpperCase()));
  }
case BEGIN_ARRAY:
return readUnion(reader,knownRecords);
case BEGIN_OBJECT:
{
reader.beginObject();
String name=reader.nextName();
if (!""String_Node_Str"".equals(name)) {
  throw new IOException(""String_Node_Str"");
}
Schema.Type schemaType=Schema.Type.valueOf(reader.nextString().toUpperCase());
Schema schema;
switch (schemaType) {
case ENUM:
  schema=readEnum(reader);
break;
case ARRAY:
schema=readArray(reader,knownRecords);
break;
case MAP:
schema=readMap(reader,knownRecords);
break;
case RECORD:
schema=readRecord(reader,knownRecords);
break;
default :
schema=Schema.of(schemaType);
}
reader.endObject();
return schema;
}
}
throw new IOException(""String_Node_Str"");
}"
6260,"/** 
 * Constructs   {@link Schema.Type#MAP MAP} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#MAP MAP}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readMap(JsonReader reader,Set<String> knownRecords) throws IOException {
  return Schema.mapOf(readInnerSchema(reader,""String_Node_Str"",knownRecords),readInnerSchema(reader,""String_Node_Str"",knownRecords));
}","/** 
 * Constructs   {@link Schema.Type#MAP MAP} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#MAP MAP}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readMap(JsonReader reader,Map<String,Schema> knownRecords) throws IOException {
  return Schema.mapOf(readInnerSchema(reader,""String_Node_Str"",knownRecords),readInnerSchema(reader,""String_Node_Str"",knownRecords));
}"
6261,"/** 
 * Constructs   {@link Schema.Type#RECORD RECORD} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#RECORD RECORD}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readRecord(JsonReader reader,Set<String> knownRecords) throws IOException {
  if (!""String_Node_Str"".equals(reader.nextName())) {
    throw new IOException(""String_Node_Str"");
  }
  String recordName=reader.nextString();
  if (!""String_Node_Str"".equals(reader.nextName())) {
    throw new IOException(""String_Node_Str"");
  }
  knownRecords.add(recordName);
  List<Schema.Field> fieldBuilder=new ArrayList<>();
  reader.beginArray();
  while (reader.peek() != JsonToken.END_ARRAY) {
    reader.beginObject();
    if (!""String_Node_Str"".equals(reader.nextName())) {
      throw new IOException(""String_Node_Str"");
    }
    String fieldName=reader.nextString();
    fieldBuilder.add(Schema.Field.of(fieldName,readInnerSchema(reader,""String_Node_Str"",knownRecords)));
    reader.endObject();
  }
  reader.endArray();
  return Schema.recordOf(recordName,fieldBuilder);
}","/** 
 * Constructs   {@link Schema.Type#RECORD RECORD} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#RECORD RECORD}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readRecord(JsonReader reader,Map<String,Schema> knownRecords) throws IOException {
  if (!""String_Node_Str"".equals(reader.nextName())) {
    throw new IOException(""String_Node_Str"");
  }
  String recordName=reader.nextString();
  if (!""String_Node_Str"".equals(reader.nextName())) {
    throw new IOException(""String_Node_Str"");
  }
  knownRecords.put(recordName,null);
  List<Schema.Field> fieldBuilder=new ArrayList<>();
  reader.beginArray();
  while (reader.peek() != JsonToken.END_ARRAY) {
    reader.beginObject();
    if (!""String_Node_Str"".equals(reader.nextName())) {
      throw new IOException(""String_Node_Str"");
    }
    String fieldName=reader.nextString();
    fieldBuilder.add(Schema.Field.of(fieldName,readInnerSchema(reader,""String_Node_Str"",knownRecords)));
    reader.endObject();
  }
  reader.endArray();
  Schema schema=Schema.recordOf(recordName,fieldBuilder);
  knownRecords.put(recordName,schema);
  return schema;
}"
6262,"/** 
 * Constructs   {@link Schema.Type#ARRAY ARRAY} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#ARRAY ARRAY}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readArray(JsonReader reader,Set<String> knownRecords) throws IOException {
  return Schema.arrayOf(readInnerSchema(reader,""String_Node_Str"",knownRecords));
}","/** 
 * Constructs   {@link Schema.Type#ARRAY ARRAY} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#ARRAY ARRAY}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readArray(JsonReader reader,Map<String,Schema> knownRecords) throws IOException {
  return Schema.arrayOf(readInnerSchema(reader,""String_Node_Str"",knownRecords));
}"
6263,"/** 
 * Constructs   {@link Schema.Type#UNION UNION} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#UNION UNION}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readUnion(JsonReader reader,Set<String> knownRecords) throws IOException {
  List<Schema> unionSchemas=new ArrayList<>();
  reader.beginArray();
  while (reader.peek() != JsonToken.END_ARRAY) {
    unionSchemas.add(read(reader,knownRecords));
  }
  reader.endArray();
  return Schema.unionOf(unionSchemas);
}","/** 
 * Constructs   {@link Schema.Type#UNION UNION} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#UNION UNION}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readUnion(JsonReader reader,Map<String,Schema> knownRecords) throws IOException {
  List<Schema> unionSchemas=new ArrayList<>();
  reader.beginArray();
  while (reader.peek() != JsonToken.END_ARRAY) {
    unionSchemas.add(read(reader,knownRecords));
  }
  reader.endArray();
  return Schema.unionOf(unionSchemas);
}"
6264,"/** 
 * Gets a Yarn delegation token and stores it in the given Credentials.
 * @return the same Credentials instance as the one given in parameter.
 */
public static Credentials obtainToken(YarnConfiguration configuration,Credentials credentials){
  if (!UserGroupInformation.isSecurityEnabled()) {
    return credentials;
  }
  try {
    YarnClient yarnClient=YarnClient.createYarnClient();
    yarnClient.init(configuration);
    yarnClient.start();
    try {
      Text renewer=new Text(UserGroupInformation.getCurrentUser().getShortUserName());
      org.apache.hadoop.yarn.api.records.Token rmDelegationToken=yarnClient.getRMDelegationToken(renewer);
      List<String> services=new ArrayList<>();
      if (HAUtil.isHAEnabled(configuration)) {
        YarnConfiguration yarnConf=new YarnConfiguration(configuration);
        for (        String rmId : HAUtil.getRMHAIds(configuration)) {
          yarnConf.set(YarnConfiguration.RM_HA_ID,rmId);
          InetSocketAddress address=yarnConf.getSocketAddr(YarnConfiguration.RM_ADDRESS,YarnConfiguration.DEFAULT_RM_ADDRESS,YarnConfiguration.DEFAULT_RM_PORT);
          services.add(SecurityUtil.buildTokenService(address).toString());
        }
      }
 else {
        services.add(SecurityUtil.buildTokenService(YarnUtils.getRMAddress(configuration)).toString());
      }
      Token<TokenIdentifier> token=ConverterUtils.convertFromYarn(rmDelegationToken,null);
      token.setService(new Text(Joiner.on(',').join(services)));
      credentials.addToken(new Text(token.getService()),token);
      LOG.info(""String_Node_Str"",token);
    }
  finally {
      yarnClient.stop();
    }
    return credentials;
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}","/** 
 * Gets a Yarn delegation token and stores it in the given Credentials.
 * @return the same Credentials instance as the one given in parameter.
 */
public static Credentials obtainToken(YarnConfiguration configuration,Credentials credentials){
  if (!UserGroupInformation.isSecurityEnabled()) {
    return credentials;
  }
  try {
    YarnClient yarnClient=YarnClient.createYarnClient();
    yarnClient.init(configuration);
    yarnClient.start();
    try {
      Text renewer=new Text(UserGroupInformation.getCurrentUser().getShortUserName());
      org.apache.hadoop.yarn.api.records.Token rmDelegationToken=yarnClient.getRMDelegationToken(renewer);
      List<String> services=new ArrayList<>();
      if (HAUtil.isHAEnabled(configuration)) {
        YarnConfiguration yarnConf=new YarnConfiguration(configuration);
        for (        String rmId : HAUtil.getRMHAIds(configuration)) {
          yarnConf.set(YarnConfiguration.RM_HA_ID,rmId);
          InetSocketAddress address=yarnConf.getSocketAddr(YarnConfiguration.RM_ADDRESS,YarnConfiguration.DEFAULT_RM_ADDRESS,YarnConfiguration.DEFAULT_RM_PORT);
          services.add(SecurityUtil.buildTokenService(address).toString());
        }
      }
 else {
        services.add(SecurityUtil.buildTokenService(YarnUtils.getRMAddress(configuration)).toString());
      }
      Token<TokenIdentifier> token=ConverterUtils.convertFromYarn(rmDelegationToken,(InetSocketAddress)null);
      token.setService(new Text(Joiner.on(',').join(services)));
      credentials.addToken(new Text(token.getService()),token);
      LOG.info(""String_Node_Str"",token);
    }
  finally {
      yarnClient.stop();
    }
    return credentials;
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}"
6265,"@Override public ProcessLauncher<ApplicationId> createLauncher(String user,TwillSpecification twillSpec,@Nullable String schedulerQueue) throws Exception {
  return createLauncher(twillSpec,schedulerQueue);
}","@Override public ProcessLauncher<ApplicationMasterInfo> createLauncher(String user,TwillSpecification twillSpec,@Nullable String schedulerQueue) throws Exception {
  return createLauncher(twillSpec,schedulerQueue);
}"
6266,"@Override public ProcessController<YarnApplicationReport> submit(YarnLaunchContext context,Resource capability){
  ContainerLaunchContext launchContext=context.getLaunchContext();
  appSubmissionContext.setAMContainerSpec(launchContext);
  appSubmissionContext.setResource(adjustMemory(response,capability));
  appSubmissionContext.setMaxAppAttempts(2);
  try {
    yarnClient.submitApplication(appSubmissionContext);
    return new ProcessControllerImpl(yarnClient,appId);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",appId,e);
    throw Throwables.propagate(e);
  }
}","@Override public ProcessController<YarnApplicationReport> submit(YarnLaunchContext context){
  ContainerLaunchContext launchContext=context.getLaunchContext();
  appSubmissionContext.setAMContainerSpec(launchContext);
  appSubmissionContext.setResource(capability);
  appSubmissionContext.setMaxAppAttempts(2);
  try {
    yarnClient.submitApplication(appSubmissionContext);
    return new ProcessControllerImpl(yarnClient,appId);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",appId,e);
    throw Throwables.propagate(e);
  }
}"
6267,"@Inject public TokenSecureStoreUpdater(YarnConfiguration hConf,CConfiguration cConf,LocationFactory locationFactory){
  this.hConf=hConf;
  this.locationFactory=locationFactory;
  secureExplore=cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED) && UserGroupInformation.isSecurityEnabled();
  credentials=new Credentials();
  updateInterval=calculateUpdateInterval();
}","@Inject public TokenSecureStoreUpdater(YarnConfiguration hConf,CConfiguration cConf,LocationFactory locationFactory){
  this.hConf=hConf;
  this.locationFactory=locationFactory;
  secureExplore=cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED) && UserGroupInformation.isSecurityEnabled();
  updateInterval=calculateUpdateInterval();
}"
6268,"private void refreshCredentials(){
  try {
    Credentials refreshedCredentials=new Credentials();
    if (User.isSecurityEnabled()) {
      YarnTokenUtils.obtainToken(hConf,refreshedCredentials);
    }
    if (User.isHBaseSecurityEnabled(hConf)) {
      HBaseTokenUtils.obtainToken(hConf,refreshedCredentials);
    }
    if (secureExplore) {
      HiveTokenUtils.obtainToken(refreshedCredentials);
      JobHistoryServerTokenUtils.obtainToken(hConf,refreshedCredentials);
    }
    if (locationFactory instanceof HDFSLocationFactory) {
      YarnUtils.addDelegationTokens(hConf,locationFactory,refreshedCredentials);
    }
 else     if (locationFactory instanceof FileContextLocationFactory) {
      List<Token<?>> tokens=((FileContextLocationFactory)locationFactory).getFileContext().getDelegationTokens(new Path(Locations.toURI(locationFactory.getHomeLocation())),YarnUtils.getYarnTokenRenewer(hConf));
      for (      Token<?> token : tokens) {
        refreshedCredentials.addToken(token.getService(),token);
      }
    }
    credentials=refreshedCredentials;
  }
 catch (  IOException ioe) {
    throw Throwables.propagate(ioe);
  }
}","private Credentials refreshCredentials(){
  try {
    Credentials refreshedCredentials=new Credentials();
    if (User.isSecurityEnabled()) {
      YarnTokenUtils.obtainToken(hConf,refreshedCredentials);
    }
    if (User.isHBaseSecurityEnabled(hConf)) {
      HBaseTokenUtils.obtainToken(hConf,refreshedCredentials);
    }
    if (secureExplore) {
      HiveTokenUtils.obtainToken(refreshedCredentials);
      JobHistoryServerTokenUtils.obtainToken(hConf,refreshedCredentials);
    }
    if (locationFactory instanceof HDFSLocationFactory) {
      YarnUtils.addDelegationTokens(hConf,locationFactory,refreshedCredentials);
    }
 else     if (locationFactory instanceof FileContextLocationFactory) {
      List<Token<?>> tokens=((FileContextLocationFactory)locationFactory).getFileContext().getDelegationTokens(new Path(Locations.toURI(locationFactory.getHomeLocation())),YarnUtils.getYarnTokenRenewer(hConf));
      for (      Token<?> token : tokens) {
        refreshedCredentials.addToken(token.getService(),token);
      }
    }
    return refreshedCredentials;
  }
 catch (  IOException ioe) {
    throw Throwables.propagate(ioe);
  }
}"
6269,"@Override public SecureStore update(String application,RunId runId){
  long now=System.currentTimeMillis();
  if (now >= nextUpdateTime) {
    nextUpdateTime=now + getUpdateInterval();
    refreshCredentials();
  }
  return YarnSecureStore.create(credentials);
}","@Override public SecureStore update(String application,RunId runId){
  Credentials credentials=refreshCredentials();
  LOG.info(""String_Node_Str"",credentials.getAllTokens());
  return YarnSecureStore.create(credentials);
}"
6270,"/** 
 * Gets a Yarn delegation token and stores it in the given Credentials.
 * @return the same Credentials instance as the one given in parameter.
 */
public static Credentials obtainToken(YarnConfiguration configuration,Credentials credentials){
  if (!UserGroupInformation.isSecurityEnabled()) {
    return credentials;
  }
  try {
    YarnClient yarnClient=YarnClient.createYarnClient();
    yarnClient.init(configuration);
    yarnClient.start();
    try {
      Text renewer=new Text(UserGroupInformation.getCurrentUser().getShortUserName());
      org.apache.hadoop.yarn.api.records.Token rmDelegationToken=yarnClient.getRMDelegationToken(renewer);
      if (HAUtil.isHAEnabled(configuration)) {
        YarnConfiguration yarnConf=new YarnConfiguration(configuration);
        for (        String rmId : HAUtil.getRMHAIds(configuration)) {
          yarnConf.set(YarnConfiguration.RM_HA_ID,rmId);
          InetSocketAddress address=yarnConf.getSocketAddr(YarnConfiguration.RM_ADDRESS,YarnConfiguration.DEFAULT_RM_ADDRESS,YarnConfiguration.DEFAULT_RM_PORT);
          Token<TokenIdentifier> token=ConverterUtils.convertFromYarn(rmDelegationToken,address);
          credentials.addToken(new Text(token.getService()),token);
          LOG.info(""String_Node_Str"",token);
        }
      }
 else {
        Token<TokenIdentifier> token=ConverterUtils.convertFromYarn(rmDelegationToken,YarnUtils.getRMAddress(configuration));
        credentials.addToken(new Text(token.getService()),token);
        LOG.info(""String_Node_Str"",token);
      }
    }
  finally {
      yarnClient.stop();
    }
    return credentials;
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}","/** 
 * Gets a Yarn delegation token and stores it in the given Credentials.
 * @return the same Credentials instance as the one given in parameter.
 */
public static Credentials obtainToken(YarnConfiguration configuration,Credentials credentials){
  if (!UserGroupInformation.isSecurityEnabled()) {
    return credentials;
  }
  try {
    YarnClient yarnClient=YarnClient.createYarnClient();
    yarnClient.init(configuration);
    yarnClient.start();
    try {
      Text renewer=new Text(UserGroupInformation.getCurrentUser().getShortUserName());
      org.apache.hadoop.yarn.api.records.Token rmDelegationToken=yarnClient.getRMDelegationToken(renewer);
      List<String> services=new ArrayList<>();
      if (HAUtil.isHAEnabled(configuration)) {
        YarnConfiguration yarnConf=new YarnConfiguration(configuration);
        for (        String rmId : HAUtil.getRMHAIds(configuration)) {
          yarnConf.set(YarnConfiguration.RM_HA_ID,rmId);
          InetSocketAddress address=yarnConf.getSocketAddr(YarnConfiguration.RM_ADDRESS,YarnConfiguration.DEFAULT_RM_ADDRESS,YarnConfiguration.DEFAULT_RM_PORT);
          services.add(SecurityUtil.buildTokenService(address).toString());
        }
      }
 else {
        services.add(SecurityUtil.buildTokenService(YarnUtils.getRMAddress(configuration)).toString());
      }
      Token<TokenIdentifier> token=ConverterUtils.convertFromYarn(rmDelegationToken,null);
      token.setService(new Text(Joiner.on(',').join(services)));
      credentials.addToken(new Text(token.getService()),token);
      LOG.info(""String_Node_Str"",token);
    }
  finally {
      yarnClient.stop();
    }
    return credentials;
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}"
6271,"/** 
 * Gets a Yarn delegation token and stores it in the given Credentials.
 * @return the same Credentials instance as the one given in parameter.
 */
public static Credentials obtainToken(YarnConfiguration configuration,Credentials credentials){
  if (!UserGroupInformation.isSecurityEnabled()) {
    return credentials;
  }
  try {
    YarnClient yarnClient=YarnClient.createYarnClient();
    yarnClient.init(configuration);
    yarnClient.start();
    try {
      Text renewer=new Text(UserGroupInformation.getCurrentUser().getShortUserName());
      org.apache.hadoop.yarn.api.records.Token rmDelegationToken=yarnClient.getRMDelegationToken(renewer);
      List<String> services=new ArrayList<>();
      if (HAUtil.isHAEnabled(configuration)) {
        YarnConfiguration yarnConf=new YarnConfiguration(configuration);
        for (        String rmId : HAUtil.getRMHAIds(configuration)) {
          yarnConf.set(YarnConfiguration.RM_HA_ID,rmId);
          InetSocketAddress address=yarnConf.getSocketAddr(YarnConfiguration.RM_ADDRESS,YarnConfiguration.DEFAULT_RM_ADDRESS,YarnConfiguration.DEFAULT_RM_PORT);
          services.add(SecurityUtil.buildTokenService(address).toString());
        }
      }
 else {
        services.add(SecurityUtil.buildTokenService(YarnUtils.getRMAddress(configuration)).toString());
      }
      Token<TokenIdentifier> token=ConverterUtils.convertFromYarn(rmDelegationToken,null);
      token.setService(new Text(Joiner.on(',').join(services)));
      credentials.addToken(new Text(token.getService()),token);
      LOG.info(""String_Node_Str"",token);
    }
  finally {
      yarnClient.stop();
    }
    return credentials;
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}","/** 
 * Gets a Yarn delegation token and stores it in the given Credentials.
 * @return the same Credentials instance as the one given in parameter.
 */
public static Credentials obtainToken(YarnConfiguration configuration,Credentials credentials){
  if (!UserGroupInformation.isSecurityEnabled()) {
    return credentials;
  }
  try {
    YarnClient yarnClient=YarnClient.createYarnClient();
    yarnClient.init(configuration);
    yarnClient.start();
    try {
      Text renewer=new Text(UserGroupInformation.getCurrentUser().getShortUserName());
      org.apache.hadoop.yarn.api.records.Token rmDelegationToken=yarnClient.getRMDelegationToken(renewer);
      List<String> services=new ArrayList<>();
      if (HAUtil.isHAEnabled(configuration)) {
        YarnConfiguration yarnConf=new YarnConfiguration(configuration);
        for (        String rmId : HAUtil.getRMHAIds(configuration)) {
          yarnConf.set(YarnConfiguration.RM_HA_ID,rmId);
          InetSocketAddress address=yarnConf.getSocketAddr(YarnConfiguration.RM_ADDRESS,YarnConfiguration.DEFAULT_RM_ADDRESS,YarnConfiguration.DEFAULT_RM_PORT);
          services.add(SecurityUtil.buildTokenService(address).toString());
        }
      }
 else {
        services.add(SecurityUtil.buildTokenService(YarnUtils.getRMAddress(configuration)).toString());
      }
      Token<TokenIdentifier> token=ConverterUtils.convertFromYarn(rmDelegationToken,(InetSocketAddress)null);
      token.setService(new Text(Joiner.on(',').join(services)));
      credentials.addToken(new Text(token.getService()),token);
      LOG.info(""String_Node_Str"",token);
    }
  finally {
      yarnClient.stop();
    }
    return credentials;
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
}"
6272,"@Override public ProcessLauncher<ApplicationId> createLauncher(String user,TwillSpecification twillSpec,@Nullable String schedulerQueue) throws Exception {
  return createLauncher(twillSpec,schedulerQueue);
}","@Override public ProcessLauncher<ApplicationMasterInfo> createLauncher(String user,TwillSpecification twillSpec,@Nullable String schedulerQueue) throws Exception {
  return createLauncher(twillSpec,schedulerQueue);
}"
6273,"@Override public ProcessController<YarnApplicationReport> submit(YarnLaunchContext context,Resource capability){
  ContainerLaunchContext launchContext=context.getLaunchContext();
  appSubmissionContext.setAMContainerSpec(launchContext);
  appSubmissionContext.setResource(adjustMemory(response,capability));
  appSubmissionContext.setMaxAppAttempts(2);
  try {
    yarnClient.submitApplication(appSubmissionContext);
    return new ProcessControllerImpl(yarnClient,appId);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",appId,e);
    throw Throwables.propagate(e);
  }
}","@Override public ProcessController<YarnApplicationReport> submit(YarnLaunchContext context){
  ContainerLaunchContext launchContext=context.getLaunchContext();
  appSubmissionContext.setAMContainerSpec(launchContext);
  appSubmissionContext.setResource(capability);
  appSubmissionContext.setMaxAppAttempts(2);
  try {
    yarnClient.submitApplication(appSubmissionContext);
    return new ProcessControllerImpl(yarnClient,appId);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",appId,e);
    throw Throwables.propagate(e);
  }
}"
6274,"/** 
 * Constructs a   {@link Schema} from the ""key"":""schema"" pair.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param key The json property name that need to match.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} object representing the schema of the json input.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readInnerSchema(JsonReader reader,String key,Set<String> knownRecords) throws IOException {
  if (!key.equals(reader.nextName())) {
    throw new IOException(""String_Node_Str"" + key + ""String_Node_Str"");
  }
  return read(reader,knownRecords);
}","/** 
 * Constructs a   {@link Schema} from the ""key"":""schema"" pair.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param key The json property name that need to match.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} object representing the schema of the json input.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readInnerSchema(JsonReader reader,String key,Map<String,Schema> knownRecords) throws IOException {
  if (!key.equals(reader.nextName())) {
    throw new IOException(""String_Node_Str"" + key + ""String_Node_Str"");
  }
  return read(reader,knownRecords);
}"
6275,"/** 
 * Reads json value and convert it into   {@link Schema} object.
 * @param reader Source of json
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} reflecting the json.
 * @throws IOException Any error during reading.
 */
private Schema read(JsonReader reader,Set<String> knownRecords) throws IOException {
  JsonToken token=reader.peek();
switch (token) {
case NULL:
    return null;
case STRING:
{
    String name=reader.nextString();
    if (knownRecords.contains(name)) {
      return Schema.recordOf(name);
    }
    return Schema.of(Schema.Type.valueOf(name.toUpperCase()));
  }
case BEGIN_ARRAY:
return readUnion(reader,knownRecords);
case BEGIN_OBJECT:
{
reader.beginObject();
String name=reader.nextName();
if (!""String_Node_Str"".equals(name)) {
  throw new IOException(""String_Node_Str"");
}
Schema.Type schemaType=Schema.Type.valueOf(reader.nextString().toUpperCase());
Schema schema;
switch (schemaType) {
case ENUM:
  schema=readEnum(reader);
break;
case ARRAY:
schema=readArray(reader,knownRecords);
break;
case MAP:
schema=readMap(reader,knownRecords);
break;
case RECORD:
schema=readRecord(reader,knownRecords);
break;
default :
schema=Schema.of(schemaType);
}
reader.endObject();
return schema;
}
}
throw new IOException(""String_Node_Str"");
}","/** 
 * Reads json value and convert it into   {@link Schema} object.
 * @param reader Source of json
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} reflecting the json.
 * @throws IOException Any error during reading.
 */
private Schema read(JsonReader reader,Map<String,Schema> knownRecords) throws IOException {
  JsonToken token=reader.peek();
switch (token) {
case NULL:
    return null;
case STRING:
{
    String name=reader.nextString();
    if (knownRecords.containsKey(name)) {
      Schema schema=knownRecords.get(name);
      return schema == null ? Schema.recordOf(name) : schema;
    }
    return Schema.of(Schema.Type.valueOf(name.toUpperCase()));
  }
case BEGIN_ARRAY:
return readUnion(reader,knownRecords);
case BEGIN_OBJECT:
{
reader.beginObject();
String name=reader.nextName();
if (!""String_Node_Str"".equals(name)) {
  throw new IOException(""String_Node_Str"");
}
Schema.Type schemaType=Schema.Type.valueOf(reader.nextString().toUpperCase());
Schema schema;
switch (schemaType) {
case ENUM:
  schema=readEnum(reader);
break;
case ARRAY:
schema=readArray(reader,knownRecords);
break;
case MAP:
schema=readMap(reader,knownRecords);
break;
case RECORD:
schema=readRecord(reader,knownRecords);
break;
default :
schema=Schema.of(schemaType);
}
reader.endObject();
return schema;
}
}
throw new IOException(""String_Node_Str"");
}"
6276,"/** 
 * Constructs   {@link Schema.Type#MAP MAP} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#MAP MAP}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readMap(JsonReader reader,Set<String> knownRecords) throws IOException {
  return Schema.mapOf(readInnerSchema(reader,""String_Node_Str"",knownRecords),readInnerSchema(reader,""String_Node_Str"",knownRecords));
}","/** 
 * Constructs   {@link Schema.Type#MAP MAP} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#MAP MAP}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readMap(JsonReader reader,Map<String,Schema> knownRecords) throws IOException {
  return Schema.mapOf(readInnerSchema(reader,""String_Node_Str"",knownRecords),readInnerSchema(reader,""String_Node_Str"",knownRecords));
}"
6277,"/** 
 * Constructs   {@link Schema.Type#RECORD RECORD} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#RECORD RECORD}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readRecord(JsonReader reader,Set<String> knownRecords) throws IOException {
  if (!""String_Node_Str"".equals(reader.nextName())) {
    throw new IOException(""String_Node_Str"");
  }
  String recordName=reader.nextString();
  if (!""String_Node_Str"".equals(reader.nextName())) {
    throw new IOException(""String_Node_Str"");
  }
  knownRecords.add(recordName);
  List<Schema.Field> fieldBuilder=new ArrayList<>();
  reader.beginArray();
  while (reader.peek() != JsonToken.END_ARRAY) {
    reader.beginObject();
    if (!""String_Node_Str"".equals(reader.nextName())) {
      throw new IOException(""String_Node_Str"");
    }
    String fieldName=reader.nextString();
    fieldBuilder.add(Schema.Field.of(fieldName,readInnerSchema(reader,""String_Node_Str"",knownRecords)));
    reader.endObject();
  }
  reader.endArray();
  return Schema.recordOf(recordName,fieldBuilder);
}","/** 
 * Constructs   {@link Schema.Type#RECORD RECORD} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#RECORD RECORD}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readRecord(JsonReader reader,Map<String,Schema> knownRecords) throws IOException {
  if (!""String_Node_Str"".equals(reader.nextName())) {
    throw new IOException(""String_Node_Str"");
  }
  String recordName=reader.nextString();
  if (!""String_Node_Str"".equals(reader.nextName())) {
    throw new IOException(""String_Node_Str"");
  }
  knownRecords.put(recordName,null);
  List<Schema.Field> fieldBuilder=new ArrayList<>();
  reader.beginArray();
  while (reader.peek() != JsonToken.END_ARRAY) {
    reader.beginObject();
    if (!""String_Node_Str"".equals(reader.nextName())) {
      throw new IOException(""String_Node_Str"");
    }
    String fieldName=reader.nextString();
    fieldBuilder.add(Schema.Field.of(fieldName,readInnerSchema(reader,""String_Node_Str"",knownRecords)));
    reader.endObject();
  }
  reader.endArray();
  Schema schema=Schema.recordOf(recordName,fieldBuilder);
  knownRecords.put(recordName,schema);
  return schema;
}"
6278,"/** 
 * Constructs   {@link Schema.Type#ARRAY ARRAY} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#ARRAY ARRAY}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readArray(JsonReader reader,Set<String> knownRecords) throws IOException {
  return Schema.arrayOf(readInnerSchema(reader,""String_Node_Str"",knownRecords));
}","/** 
 * Constructs   {@link Schema.Type#ARRAY ARRAY} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#ARRAY ARRAY}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readArray(JsonReader reader,Map<String,Schema> knownRecords) throws IOException {
  return Schema.arrayOf(readInnerSchema(reader,""String_Node_Str"",knownRecords));
}"
6279,"/** 
 * Constructs   {@link Schema.Type#UNION UNION} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#UNION UNION}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readUnion(JsonReader reader,Set<String> knownRecords) throws IOException {
  List<Schema> unionSchemas=new ArrayList<>();
  reader.beginArray();
  while (reader.peek() != JsonToken.END_ARRAY) {
    unionSchemas.add(read(reader,knownRecords));
  }
  reader.endArray();
  return Schema.unionOf(unionSchemas);
}","/** 
 * Constructs   {@link Schema.Type#UNION UNION} type schema from the json input.
 * @param reader The {@link JsonReader} for streaming json input tokens.
 * @param knownRecords Set of record name already encountered during the reading.
 * @return A {@link Schema} of type {@link Schema.Type#UNION UNION}.
 * @throws IOException When fails to construct a valid schema from the input.
 */
private Schema readUnion(JsonReader reader,Map<String,Schema> knownRecords) throws IOException {
  List<Schema> unionSchemas=new ArrayList<>();
  reader.beginArray();
  while (reader.peek() != JsonToken.END_ARRAY) {
    unionSchemas.add(read(reader,knownRecords));
  }
  reader.endArray();
  return Schema.unionOf(unionSchemas);
}"
6280,"private void doMain(String[] args) throws Exception {
  System.out.println(String.format(""String_Node_Str"",getClass().getSimpleName(),ProjectInfo.getVersion()));
  System.out.println();
  if (args.length < 1) {
    printHelp();
    return;
  }
  Action action=parseAction(args[0]);
  if (action == null) {
    System.out.println(String.format(""String_Node_Str"",args[0]));
    printHelp(true);
    return;
  }
  boolean interactive=true;
  if ((args.length >= 2) && (args[1]).equals(""String_Node_Str"")) {
    interactive=false;
    System.out.println(""String_Node_Str"");
  }
  try {
switch (action) {
case UPGRADE:
{
        System.out.println(String.format(""String_Node_Str"",action.name().toLowerCase(),action.getDescription()));
        String response=getResponse(interactive);
        if (response.equalsIgnoreCase(""String_Node_Str"") || response.equalsIgnoreCase(""String_Node_Str"")) {
          System.out.println(""String_Node_Str"");
          try {
            startUp(false);
            performUpgrade();
            System.out.println(""String_Node_Str"");
          }
  finally {
            stop();
          }
        }
 else {
          System.out.println(""String_Node_Str"");
        }
        break;
      }
case UPGRADE_HBASE:
{
      System.out.println(String.format(""String_Node_Str"",action.name().toLowerCase(),action.getDescription()));
      String response=getResponse(interactive);
      if (response.equalsIgnoreCase(""String_Node_Str"") || response.equalsIgnoreCase(""String_Node_Str"")) {
        System.out.println(""String_Node_Str"");
        try {
          startUp(true);
          performHBaseUpgrade();
          System.out.println(""String_Node_Str"");
        }
  finally {
          stop();
        }
      }
 else {
        System.out.println(""String_Node_Str"");
      }
      break;
    }
case HELP:
  printHelp();
break;
}
}
 catch (Exception e) {
System.out.println(String.format(""String_Node_Str"",action,e.getMessage()));
e.printStackTrace(System.out);
}
}","private void doMain(String[] args) throws Exception {
  System.out.println(String.format(""String_Node_Str"",getClass().getSimpleName(),ProjectInfo.getVersion()));
  System.out.println();
  if (args.length < 1) {
    printHelp();
    return;
  }
  Action action=parseAction(args[0]);
  if (action == null) {
    System.out.println(String.format(""String_Node_Str"",args[0]));
    printHelp(true);
    return;
  }
  boolean interactive=true;
  if ((args.length >= 2) && (args[1]).equals(""String_Node_Str"")) {
    interactive=false;
    System.out.println(""String_Node_Str"");
  }
  try {
switch (action) {
case UPGRADE:
{
        System.out.println(String.format(""String_Node_Str"",action.name().toLowerCase(),action.getDescription()));
        String response=getResponse(interactive);
        if (response.equalsIgnoreCase(""String_Node_Str"") || response.equalsIgnoreCase(""String_Node_Str"")) {
          System.out.println(""String_Node_Str"");
          try {
            startUp(false);
            performUpgrade();
            System.out.println(""String_Node_Str"");
          }
  finally {
            stop();
          }
        }
 else {
          System.out.println(""String_Node_Str"");
        }
        break;
      }
case UPGRADE_HBASE:
{
      System.out.println(String.format(""String_Node_Str"",action.name().toLowerCase(),action.getDescription()));
      String response=getResponse(interactive);
      if (response.equalsIgnoreCase(""String_Node_Str"") || response.equalsIgnoreCase(""String_Node_Str"")) {
        System.out.println(""String_Node_Str"");
        try {
          startUp(true);
          performHBaseUpgrade();
          System.out.println(""String_Node_Str"");
        }
  finally {
          stop();
        }
      }
 else {
        System.out.println(""String_Node_Str"");
      }
      break;
    }
case HELP:
  printHelp();
break;
}
}
 catch (Exception e) {
System.out.println(String.format(""String_Node_Str"",action,e.getMessage()));
throw e;
}
}"
6281,"public static void main(String[] args){
  try {
    UpgradeTool upgradeTool=new UpgradeTool();
    upgradeTool.doMain(args);
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",t);
  }
}","public static void main(String[] args){
  try {
    UpgradeTool upgradeTool=new UpgradeTool();
    upgradeTool.doMain(args);
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",t);
    System.exit(1);
  }
}"
6282,"private void performCoprocessorUpgrade() throws Exception {
  LOG.info(""String_Node_Str"");
  dsUpgrade.upgrade();
  LOG.info(""String_Node_Str"");
  queueAdmin.upgrade();
  LOG.info(""String_Node_Str"");
  dsSpecUpgrader.upgrade();
}","private void performCoprocessorUpgrade() throws Exception {
  LOG.info(""String_Node_Str"");
  dsUpgrade.upgrade();
  LOG.info(""String_Node_Str"");
  queueAdmin.upgrade();
}"
6283,"private void performUpgrade() throws Exception {
  performCoprocessorUpgrade();
  LOG.info(""String_Node_Str"");
  datasetBasedTimeScheduleStore.upgrade();
  LOG.info(""String_Node_Str"");
  upgradeBusinessMetadataDatasetSpec();
  LOG.info(""String_Node_Str"");
  metadataStore.upgrade();
  LOG.info(""String_Node_Str"");
  streamStateStoreUpgrader.upgrade();
}","private void performUpgrade() throws Exception {
  performCoprocessorUpgrade();
  LOG.info(""String_Node_Str"");
  dsSpecUpgrader.upgrade();
  LOG.info(""String_Node_Str"");
  datasetBasedTimeScheduleStore.upgrade();
  LOG.info(""String_Node_Str"");
  upgradeBusinessMetadataDatasetSpec();
  LOG.info(""String_Node_Str"");
  metadataStore.upgrade();
  LOG.info(""String_Node_Str"");
  streamStateStoreUpgrader.upgrade();
}"
6284,"/** 
 * Subscribe to the streams heartbeat notification feed. One heartbeat contains data for all existing streams, we filter that to only take into account the streams that this   {@link DistributedStreamService} is a leaderof.
 * @return a {@link Cancellable} to cancel the subscription
 * @throws NotificationFeedNotFoundException if the heartbeat feed does not exist
 */
private Cancellable subscribeToHeartbeatsFeed() throws NotificationFeedNotFoundException {
  LOG.debug(""String_Node_Str"");
  final Id.NotificationFeed heartbeatsFeed=new Id.NotificationFeed.Builder().setNamespaceId(Id.Namespace.SYSTEM.getId()).setCategory(Constants.Notification.Stream.STREAM_INTERNAL_FEED_CATEGORY).setName(Constants.Notification.Stream.STREAM_HEARTBEAT_FEED_NAME).build();
  while (true) {
    try {
      return notificationService.subscribe(heartbeatsFeed,new NotificationHandler<StreamWriterHeartbeat>(){
        @Override public Type getNotificationType(){
          return StreamWriterHeartbeat.class;
        }
        @Override public void received(        StreamWriterHeartbeat heartbeat,        NotificationContext notificationContext){
          LOG.trace(""String_Node_Str"",heartbeat);
          for (          Map.Entry<Id.Stream,Long> entry : heartbeat.getStreamsSizes().entrySet()) {
            StreamSizeAggregator streamSizeAggregator=aggregators.get(entry.getKey());
            if (streamSizeAggregator == null) {
              LOG.trace(""String_Node_Str"",entry.getKey());
              continue;
            }
            streamSizeAggregator.bytesReceived(heartbeat.getInstanceId(),entry.getValue());
          }
        }
      }
,heartbeatsSubscriptionExecutor);
    }
 catch (    NotificationFeedException e) {
      waitBeforeRetryHeartbeatsFeedOperation();
    }
  }
}","/** 
 * Subscribe to the streams heartbeat notification feed. One heartbeat contains data for all existing streams, we filter that to only take into account the streams that this   {@link DistributedStreamService} is a leaderof.
 * @return a {@link Cancellable} to cancel the subscription
 * @throws NotificationFeedNotFoundException if the heartbeat feed does not exist
 */
private Cancellable subscribeToHeartbeatsFeed() throws NotificationFeedNotFoundException {
  LOG.debug(""String_Node_Str"");
  final Id.NotificationFeed heartbeatsFeed=new Id.NotificationFeed.Builder().setNamespaceId(Id.Namespace.SYSTEM.getId()).setCategory(Constants.Notification.Stream.STREAM_INTERNAL_FEED_CATEGORY).setName(Constants.Notification.Stream.STREAM_HEARTBEAT_FEED_NAME).build();
  boolean isRetry=false;
  while (true) {
    try {
      return notificationService.subscribe(heartbeatsFeed,new NotificationHandler<StreamWriterHeartbeat>(){
        @Override public Type getNotificationType(){
          return StreamWriterHeartbeat.class;
        }
        @Override public void received(        StreamWriterHeartbeat heartbeat,        NotificationContext notificationContext){
          LOG.trace(""String_Node_Str"",heartbeat);
          for (          Map.Entry<Id.Stream,Long> entry : heartbeat.getStreamsSizes().entrySet()) {
            StreamSizeAggregator streamSizeAggregator=aggregators.get(entry.getKey());
            if (streamSizeAggregator == null) {
              LOG.trace(""String_Node_Str"",entry.getKey());
              continue;
            }
            streamSizeAggregator.bytesReceived(heartbeat.getInstanceId(),entry.getValue());
          }
        }
      }
,heartbeatsSubscriptionExecutor);
    }
 catch (    NotificationFeedException e) {
      if (!isRetry) {
        LOG.warn(""String_Node_Str"" + ""String_Node_Str"",e);
      }
 else {
        LOG.debug(""String_Node_Str"",e);
      }
      isRetry=true;
      waitBeforeRetryHeartbeatsFeedOperation();
    }
  }
}"
6285,"/** 
 * Create Notification feed for stream's heartbeats, if it does not already exist.
 */
private void createHeartbeatsFeed() throws NotificationFeedException {
  Id.NotificationFeed streamHeartbeatsFeed=new Id.NotificationFeed.Builder().setNamespaceId(Id.Namespace.SYSTEM.getId()).setCategory(Constants.Notification.Stream.STREAM_INTERNAL_FEED_CATEGORY).setName(Constants.Notification.Stream.STREAM_HEARTBEAT_FEED_NAME).setDescription(""String_Node_Str"").build();
  while (true) {
    try {
      feedManager.getFeed(streamHeartbeatsFeed);
      return;
    }
 catch (    NotificationFeedNotFoundException e) {
      feedManager.createFeed(streamHeartbeatsFeed);
      return;
    }
catch (    NotificationFeedException e) {
      waitBeforeRetryHeartbeatsFeedOperation();
    }
  }
}","/** 
 * Create Notification feed for stream's heartbeats, if it does not already exist.
 */
private void createHeartbeatsFeed() throws NotificationFeedException {
  Id.NotificationFeed streamHeartbeatsFeed=new Id.NotificationFeed.Builder().setNamespaceId(Id.Namespace.SYSTEM.getId()).setCategory(Constants.Notification.Stream.STREAM_INTERNAL_FEED_CATEGORY).setName(Constants.Notification.Stream.STREAM_HEARTBEAT_FEED_NAME).setDescription(""String_Node_Str"").build();
  LOG.debug(""String_Node_Str"");
  boolean isRetry=false;
  while (true) {
    try {
      feedManager.getFeed(streamHeartbeatsFeed);
      LOG.debug(""String_Node_Str"");
      return;
    }
 catch (    NotificationFeedNotFoundException notFoundException) {
      if (!isRetry) {
        LOG.debug(""String_Node_Str"");
      }
      feedManager.createFeed(streamHeartbeatsFeed);
      LOG.info(""String_Node_Str"");
      return;
    }
catch (    NotificationFeedException e) {
      if (!isRetry) {
        LOG.warn(""String_Node_Str"" + ""String_Node_Str"",e);
      }
 else {
        LOG.debug(""String_Node_Str"",e);
      }
      isRetry=true;
      waitBeforeRetryHeartbeatsFeedOperation();
    }
  }
}"
6286,"private void waitBeforeRetryHeartbeatsFeedOperation(){
  LOG.info(""String_Node_Str"");
  try {
    TimeUnit.SECONDS.sleep(1);
  }
 catch (  InterruptedException ie) {
    Thread.currentThread().interrupt();
    throw Throwables.propagate(ie);
  }
}","private void waitBeforeRetryHeartbeatsFeedOperation(){
  try {
    TimeUnit.SECONDS.sleep(1);
  }
 catch (  InterruptedException ie) {
    Thread.currentThread().interrupt();
    throw Throwables.propagate(ie);
  }
}"
6287,"@Override protected void initialize() throws Exception {
  createHeartbeatsFeed();
  heartbeatPublisher.startAndWait();
  resourceCoordinatorClient.startAndWait();
  coordinationSubscription=resourceCoordinatorClient.subscribe(discoverableSupplier.get().getName(),new StreamsLeaderHandler());
  heartbeatsSubscriptionExecutor=Executors.newSingleThreadExecutor(Threads.createDaemonThreadFactory(""String_Node_Str""));
  heartbeatsSubscription=subscribeToHeartbeatsFeed();
  leaderListenerCancellable=addLeaderListener(new StreamLeaderListener(){
    @Override public void leaderOf(    Set<Id.Stream> streamIds){
      aggregate(streamIds);
    }
  }
);
  performLeaderElection();
}","@Override protected void initialize() throws Exception {
  LOG.info(""String_Node_Str"");
  createHeartbeatsFeed();
  heartbeatPublisher.startAndWait();
  resourceCoordinatorClient.startAndWait();
  coordinationSubscription=resourceCoordinatorClient.subscribe(discoverableSupplier.get().getName(),new StreamsLeaderHandler());
  heartbeatsSubscriptionExecutor=Executors.newSingleThreadExecutor(Threads.createDaemonThreadFactory(""String_Node_Str""));
  heartbeatsSubscription=subscribeToHeartbeatsFeed();
  leaderListenerCancellable=addLeaderListener(new StreamLeaderListener(){
    @Override public void leaderOf(    Set<Id.Stream> streamIds){
      aggregate(streamIds);
    }
  }
);
  performLeaderElection();
  LOG.info(""String_Node_Str"");
}"
6288,"/** 
 * Delete a namespace from the underlying system Can perform operations such as deleting directories, deleting namespaces, etc. The default implementation deletes the namespace directory on the filesystem. Subclasses can override to add more logic such as delete namespaces in HBase, etc.
 * @param namespaceId {@link Id.Namespace} for the namespace to delete
 * @throws IOException if there are errors while deleting the namespace
 */
protected void delete(Id.Namespace namespaceId) throws IOException, ExploreException, SQLException {
  Location namespaceHome=namespacedLocationFactory.get(namespaceId);
  if (namespaceHome.exists() && !namespaceHome.delete(true)) {
    throw new IOException(String.format(""String_Node_Str"",namespaceHome,namespaceId.getId()));
  }
 else {
    LOG.warn(String.format(""String_Node_Str"",namespaceHome,namespaceId));
  }
  if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED)) {
    exploreFacade.removeNamespace(namespaceId);
  }
}","/** 
 * Delete a namespace from the underlying system Can perform operations such as deleting directories, deleting namespaces, etc. The default implementation deletes the namespace directory on the filesystem. Subclasses can override to add more logic such as delete namespaces in HBase, etc.
 * @param namespaceId {@link Id.Namespace} for the namespace to delete
 * @throws IOException if there are errors while deleting the namespace
 */
protected void delete(Id.Namespace namespaceId) throws IOException, ExploreException, SQLException {
  Location namespaceHome=namespacedLocationFactory.get(namespaceId);
  if (namespaceHome.exists()) {
    if (!namespaceHome.delete(true)) {
      throw new IOException(String.format(""String_Node_Str"",namespaceHome,namespaceId.getId()));
    }
  }
 else {
    LOG.warn(String.format(""String_Node_Str"",namespaceHome,namespaceId));
  }
  if (cConf.getBoolean(Constants.Explore.EXPLORE_ENABLED)) {
    exploreFacade.removeNamespace(namespaceId);
  }
}"
6289,"private BodyConsumer deployApplication(final HttpResponder responder,final Id.Namespace namespace,final String appId,final String archiveName,final String configString) throws IOException {
  Location namespaceHomeLocation=namespacedLocationFactory.get(namespace);
  if (!namespaceHomeLocation.exists()) {
    String msg=String.format(""String_Node_Str"",namespaceHomeLocation,namespace.getId());
    LOG.error(msg);
    responder.sendString(HttpResponseStatus.NOT_FOUND,msg);
    return null;
  }
  if (archiveName == null || archiveName.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,ARCHIVE_NAME_HEADER + ""String_Node_Str"",ImmutableMultimap.of(HttpHeaders.Names.CONNECTION,HttpHeaders.Values.CLOSE));
    return null;
  }
  final Id.Artifact artifactId;
  try {
    artifactId=Id.Artifact.parse(namespace,archiveName);
  }
 catch (  IllegalArgumentException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    return null;
  }
  String namespacesDir=configuration.get(Constants.Namespace.NAMESPACES_DIR);
  File localDataDir=new File(configuration.get(Constants.CFG_LOCAL_DATA_DIR));
  File namespaceBase=new File(localDataDir,namespacesDir);
  File tempDir=new File(new File(namespaceBase,namespace.getId()),configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  if (!DirUtils.mkdirs(tempDir)) {
    throw new IOException(""String_Node_Str"" + tempDir);
  }
  return new AbstractBodyConsumer(File.createTempFile(""String_Node_Str"",""String_Node_Str"",tempDir)){
    @Override protected void onFinish(    HttpResponder responder,    File uploadedFile){
      try {
        applicationLifecycleService.deployAppAndArtifact(namespace,appId,artifactId,uploadedFile,configString,createProgramTerminator());
        responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
      }
 catch (      InvalidArtifactException e) {
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
catch (      ArtifactAlreadyExistsException e) {
        responder.sendString(HttpResponseStatus.CONFLICT,String.format(""String_Node_Str"" + ""String_Node_Str"",artifactId));
      }
catch (      WriteConflictException e) {
        LOG.warn(""String_Node_Str"",artifactId,e);
        responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + ""String_Node_Str"");
      }
catch (      Exception e) {
        LOG.error(""String_Node_Str"",e);
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
    }
  }
;
}","private BodyConsumer deployApplication(final HttpResponder responder,final Id.Namespace namespace,final String appId,final String archiveName,final String configString) throws IOException {
  Location namespaceHomeLocation=namespacedLocationFactory.get(namespace);
  if (!namespaceHomeLocation.exists()) {
    String msg=String.format(""String_Node_Str"",namespaceHomeLocation,namespace.getId());
    LOG.error(msg);
    responder.sendString(HttpResponseStatus.NOT_FOUND,msg);
    return null;
  }
  if (archiveName == null || archiveName.isEmpty()) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,String.format(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",ARCHIVE_NAME_HEADER,HttpHeaders.Names.CONTENT_TYPE,MediaType.APPLICATION_JSON),ImmutableMultimap.of(HttpHeaders.Names.CONNECTION,HttpHeaders.Values.CLOSE));
    return null;
  }
  final Id.Artifact artifactId;
  try {
    artifactId=Id.Artifact.parse(namespace,archiveName);
  }
 catch (  IllegalArgumentException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
    return null;
  }
  String namespacesDir=configuration.get(Constants.Namespace.NAMESPACES_DIR);
  File localDataDir=new File(configuration.get(Constants.CFG_LOCAL_DATA_DIR));
  File namespaceBase=new File(localDataDir,namespacesDir);
  File tempDir=new File(new File(namespaceBase,namespace.getId()),configuration.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile();
  if (!DirUtils.mkdirs(tempDir)) {
    throw new IOException(""String_Node_Str"" + tempDir);
  }
  return new AbstractBodyConsumer(File.createTempFile(""String_Node_Str"",""String_Node_Str"",tempDir)){
    @Override protected void onFinish(    HttpResponder responder,    File uploadedFile){
      try {
        applicationLifecycleService.deployAppAndArtifact(namespace,appId,artifactId,uploadedFile,configString,createProgramTerminator());
        responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
      }
 catch (      InvalidArtifactException e) {
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
catch (      ArtifactAlreadyExistsException e) {
        responder.sendString(HttpResponseStatus.CONFLICT,String.format(""String_Node_Str"" + ""String_Node_Str"",artifactId));
      }
catch (      WriteConflictException e) {
        LOG.warn(""String_Node_Str"",artifactId,e);
        responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + ""String_Node_Str"");
      }
catch (      Exception e) {
        LOG.error(""String_Node_Str"",e);
        responder.sendString(HttpResponseStatus.BAD_REQUEST,e.getMessage());
      }
    }
  }
;
}"
6290,"/** 
 * Ensures that the specified   {@link Id.NamespacedId} exists.
 */
public void ensureEntityExists(Id.NamespacedId entityId) throws NotFoundException {
  try {
    namespaceClient.get(entityId.getNamespace());
  }
 catch (  NamespaceNotFoundException e) {
    throw e;
  }
catch (  Exception e) {
    throw Throwables.propagate(e);
  }
  if (entityId instanceof Id.Program) {
    Id.Program program=(Id.Program)entityId;
    Id.Application application=program.getApplication();
    ApplicationSpecification appSpec=store.getApplication(application);
    if (appSpec == null) {
      throw new ApplicationNotFoundException(application);
    }
    ensureProgramExists(appSpec,program);
  }
 else   if (entityId instanceof Id.Application) {
    Id.Application application=(Id.Application)entityId;
    if (store.getApplication(application) == null) {
      throw new ApplicationNotFoundException(application);
    }
  }
 else   if (entityId instanceof Id.DatasetInstance) {
    Id.DatasetInstance datasetInstance=(Id.DatasetInstance)entityId;
    try {
      if (!datasetFramework.hasInstance(datasetInstance)) {
        throw new DatasetNotFoundException(datasetInstance);
      }
    }
 catch (    DatasetManagementException ex) {
      throw new IllegalStateException(ex);
    }
  }
 else   if (entityId instanceof Id.Stream) {
    Id.Stream stream=(Id.Stream)entityId;
    try {
      if (!streamAdmin.exists(stream)) {
        throw new StreamNotFoundException(stream);
      }
    }
 catch (    StreamNotFoundException streamEx) {
      throw streamEx;
    }
catch (    Exception ex) {
      throw new IllegalStateException(ex);
    }
  }
 else   if (entityId instanceof Id.Artifact) {
    Id.Artifact artifactId=(Id.Artifact)entityId;
    try {
      artifactStore.getArtifact(artifactId);
    }
 catch (    IOException e) {
      throw new RuntimeException(e);
    }
  }
 else   if (entityId instanceof Id.Stream.View) {
    Id.Stream.View viewId=(Id.Stream.View)entityId;
    try {
      if (!streamAdmin.viewExists(viewId)) {
        throw new ViewNotFoundException(viewId);
      }
    }
 catch (    ViewNotFoundException|StreamNotFoundException viewEx) {
      throw viewEx;
    }
catch (    Exception ex) {
      throw new IllegalStateException(ex);
    }
  }
 else {
    throw new IllegalArgumentException(""String_Node_Str"" + entityId);
  }
}","/** 
 * Ensures that the specified   {@link Id.NamespacedId} exists.
 */
public void ensureEntityExists(Id.NamespacedId entityId) throws NotFoundException {
  if (!Id.Namespace.SYSTEM.equals(entityId.getNamespace())) {
    try {
      namespaceClient.get(entityId.getNamespace());
    }
 catch (    NamespaceNotFoundException e) {
      throw e;
    }
catch (    Exception e) {
      throw Throwables.propagate(e);
    }
  }
  if (entityId instanceof Id.Program) {
    Id.Program program=(Id.Program)entityId;
    Id.Application application=program.getApplication();
    ApplicationSpecification appSpec=store.getApplication(application);
    if (appSpec == null) {
      throw new ApplicationNotFoundException(application);
    }
    ensureProgramExists(appSpec,program);
  }
 else   if (entityId instanceof Id.Application) {
    Id.Application application=(Id.Application)entityId;
    if (store.getApplication(application) == null) {
      throw new ApplicationNotFoundException(application);
    }
  }
 else   if (entityId instanceof Id.DatasetInstance) {
    Id.DatasetInstance datasetInstance=(Id.DatasetInstance)entityId;
    try {
      if (!datasetFramework.hasInstance(datasetInstance)) {
        throw new DatasetNotFoundException(datasetInstance);
      }
    }
 catch (    DatasetManagementException ex) {
      throw new IllegalStateException(ex);
    }
  }
 else   if (entityId instanceof Id.Stream) {
    Id.Stream stream=(Id.Stream)entityId;
    try {
      if (!streamAdmin.exists(stream)) {
        throw new StreamNotFoundException(stream);
      }
    }
 catch (    StreamNotFoundException streamEx) {
      throw streamEx;
    }
catch (    Exception ex) {
      throw new IllegalStateException(ex);
    }
  }
 else   if (entityId instanceof Id.Artifact) {
    Id.Artifact artifactId=(Id.Artifact)entityId;
    try {
      artifactStore.getArtifact(artifactId);
    }
 catch (    IOException e) {
      throw new RuntimeException(e);
    }
  }
 else   if (entityId instanceof Id.Stream.View) {
    Id.Stream.View viewId=(Id.Stream.View)entityId;
    try {
      if (!streamAdmin.viewExists(viewId)) {
        throw new ViewNotFoundException(viewId);
      }
    }
 catch (    ViewNotFoundException|StreamNotFoundException viewEx) {
      throw viewEx;
    }
catch (    Exception ex) {
      throw new IllegalStateException(ex);
    }
  }
 else {
    throw new IllegalArgumentException(""String_Node_Str"" + entityId);
  }
}"
6291,"private Set<String> getTags(Id.NamespacedId entityId,@Nullable MetadataScope scope) throws NotFoundException {
  return (scope == null) ? metadataAdmin.getTags(entityId) : metadataAdmin.getTags(scope,entityId);
}","private Set<String> getTags(Id.NamespacedId entityId,@Nullable String scope) throws NotFoundException, BadRequestException {
  return (scope == null) ? metadataAdmin.getTags(entityId) : metadataAdmin.getTags(validateScope(scope),entityId);
}"
6292,"@GET @Path(""String_Node_Str"") public void getAppProperties(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.Application app=Id.Application.from(namespaceId,appId);
  responder.sendJson(HttpResponseStatus.OK,getProperties(app,scope));
}","@GET @Path(""String_Node_Str"") public void getAppProperties(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.Application app=Id.Application.from(namespaceId,appId);
  responder.sendJson(HttpResponseStatus.OK,getProperties(app,scope));
}"
6293,"@GET @Path(""String_Node_Str"") public void getProgramTags(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.Program program=Id.Program.from(Id.Application.from(namespaceId,appId),ProgramType.valueOfCategoryName(programType),programId);
  responder.sendJson(HttpResponseStatus.OK,getTags(program,scope));
}","@GET @Path(""String_Node_Str"") public void getProgramTags(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.Program program=Id.Program.from(Id.Application.from(namespaceId,appId),ProgramType.valueOfCategoryName(programType),programId);
  responder.sendJson(HttpResponseStatus.OK,getTags(program,scope));
}"
6294,"@GET @Path(""String_Node_Str"") public void getViewMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String streamId,@PathParam(""String_Node_Str"") String viewId,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.Stream.View view=Id.Stream.View.from(namespaceId,streamId,viewId);
  responder.sendJson(HttpResponseStatus.OK,getMetadata(view,scope),SET_METADATA_RECORD_TYPE,GSON);
}","@GET @Path(""String_Node_Str"") public void getViewMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String streamId,@PathParam(""String_Node_Str"") String viewId,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.Stream.View view=Id.Stream.View.from(namespaceId,streamId,viewId);
  responder.sendJson(HttpResponseStatus.OK,getMetadata(view,scope),SET_METADATA_RECORD_TYPE,GSON);
}"
6295,"@GET @Path(""String_Node_Str"") public void getDatasetMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String datasetId,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(namespaceId,datasetId);
  responder.sendJson(HttpResponseStatus.OK,getMetadata(datasetInstance,scope),SET_METADATA_RECORD_TYPE,GSON);
}","@GET @Path(""String_Node_Str"") public void getDatasetMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String datasetId,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(namespaceId,datasetId);
  responder.sendJson(HttpResponseStatus.OK,getMetadata(datasetInstance,scope),SET_METADATA_RECORD_TYPE,GSON);
}"
6296,"@GET @Path(""String_Node_Str"") public void getProgramProperties(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.Program program=Id.Program.from(Id.Application.from(namespaceId,appId),ProgramType.valueOfCategoryName(programType),programId);
  responder.sendJson(HttpResponseStatus.OK,getProperties(program,scope));
}","@GET @Path(""String_Node_Str"") public void getProgramProperties(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.Program program=Id.Program.from(Id.Application.from(namespaceId,appId),ProgramType.valueOfCategoryName(programType),programId);
  responder.sendJson(HttpResponseStatus.OK,getProperties(program,scope));
}"
6297,"private Set<MetadataRecord> getMetadata(Id.NamespacedId entityId,@Nullable MetadataScope scope) throws NotFoundException {
  return (scope == null) ? metadataAdmin.getMetadata(entityId) : metadataAdmin.getMetadata(scope,entityId);
}","private Set<MetadataRecord> getMetadata(Id.NamespacedId entityId,@Nullable String scope) throws NotFoundException, BadRequestException {
  return (scope == null) ? metadataAdmin.getMetadata(entityId) : metadataAdmin.getMetadata(validateScope(scope),entityId);
}"
6298,"@GET @Path(""String_Node_Str"") public void getStreamTags(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String streamId,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.Stream stream=Id.Stream.from(namespaceId,streamId);
  responder.sendJson(HttpResponseStatus.OK,getTags(stream,scope));
}","@GET @Path(""String_Node_Str"") public void getStreamTags(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String streamId,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.Stream stream=Id.Stream.from(namespaceId,streamId);
  responder.sendJson(HttpResponseStatus.OK,getTags(stream,scope));
}"
6299,"private Map<String,String> getProperties(Id.NamespacedId entityId,@Nullable MetadataScope scope) throws NotFoundException {
  return (scope == null) ? metadataAdmin.getProperties(entityId) : metadataAdmin.getProperties(scope,entityId);
}","private Map<String,String> getProperties(Id.NamespacedId entityId,@Nullable String scope) throws NotFoundException, BadRequestException {
  return (scope == null) ? metadataAdmin.getProperties(entityId) : metadataAdmin.getProperties(validateScope(scope),entityId);
}"
6300,"@GET @Path(""String_Node_Str"") public void getStreamMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String streamId,@QueryParam(""String_Node_Str"") MetadataScope scope) throws Exception {
  Id.Stream stream=Id.Stream.from(namespaceId,streamId);
  responder.sendJson(HttpResponseStatus.OK,getMetadata(stream,scope),SET_METADATA_RECORD_TYPE,GSON);
}","@GET @Path(""String_Node_Str"") public void getStreamMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String streamId,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.Stream stream=Id.Stream.from(namespaceId,streamId);
  responder.sendJson(HttpResponseStatus.OK,getMetadata(stream,scope),SET_METADATA_RECORD_TYPE,GSON);
}"
6301,"@GET @Path(""String_Node_Str"") public void getAppMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.Application app=Id.Application.from(namespaceId,appId);
  responder.sendJson(HttpResponseStatus.OK,getMetadata(app,scope),SET_METADATA_RECORD_TYPE,GSON);
}","@GET @Path(""String_Node_Str"") public void getAppMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.Application app=Id.Application.from(namespaceId,appId);
  responder.sendJson(HttpResponseStatus.OK,getMetadata(app,scope),SET_METADATA_RECORD_TYPE,GSON);
}"
6302,"@GET @Path(""String_Node_Str"") public void getViewProperties(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String streamId,@PathParam(""String_Node_Str"") String viewId,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.Stream.View view=Id.Stream.View.from(namespaceId,streamId,viewId);
  responder.sendJson(HttpResponseStatus.OK,getProperties(view,scope));
}","@GET @Path(""String_Node_Str"") public void getViewProperties(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String streamId,@PathParam(""String_Node_Str"") String viewId,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.Stream.View view=Id.Stream.View.from(namespaceId,streamId,viewId);
  responder.sendJson(HttpResponseStatus.OK,getProperties(view,scope));
}"
6303,"@GET @Path(""String_Node_Str"") public void getStreamProperties(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String streamId,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.Stream stream=Id.Stream.from(namespaceId,streamId);
  responder.sendJson(HttpResponseStatus.OK,getProperties(stream,scope));
}","@GET @Path(""String_Node_Str"") public void getStreamProperties(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String streamId,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.Stream stream=Id.Stream.from(namespaceId,streamId);
  responder.sendJson(HttpResponseStatus.OK,getProperties(stream,scope));
}"
6304,"@GET @Path(""String_Node_Str"") public void getArtifactProperties(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersionStr,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.from(namespaceId),artifactName,artifactVersionStr);
  responder.sendJson(HttpResponseStatus.OK,getProperties(artifactId,scope));
}","@GET @Path(""String_Node_Str"") public void getArtifactProperties(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersionStr,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.from(namespaceId),artifactName,artifactVersionStr);
  responder.sendJson(HttpResponseStatus.OK,getProperties(artifactId,scope));
}"
6305,"@GET @Path(""String_Node_Str"") public void getProgramMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.Program program=Id.Program.from(Id.Application.from(namespaceId,appId),ProgramType.valueOfCategoryName(programType),programId);
  responder.sendJson(HttpResponseStatus.OK,getMetadata(program,scope),SET_METADATA_RECORD_TYPE,GSON);
}","@GET @Path(""String_Node_Str"") public void getProgramMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.Program program=Id.Program.from(Id.Application.from(namespaceId,appId),ProgramType.valueOfCategoryName(programType),programId);
  responder.sendJson(HttpResponseStatus.OK,getMetadata(program,scope),SET_METADATA_RECORD_TYPE,GSON);
}"
6306,"@GET @Path(""String_Node_Str"") public void getAppTags(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.Application app=Id.Application.from(namespaceId,appId);
  responder.sendJson(HttpResponseStatus.OK,getTags(app,scope));
}","@GET @Path(""String_Node_Str"") public void getAppTags(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.Application app=Id.Application.from(namespaceId,appId);
  responder.sendJson(HttpResponseStatus.OK,getTags(app,scope));
}"
6307,"@GET @Path(""String_Node_Str"") public void getDatasetTags(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String datasetId,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.DatasetInstance dataset=Id.DatasetInstance.from(namespaceId,datasetId);
  responder.sendJson(HttpResponseStatus.OK,getTags(dataset,scope));
}","@GET @Path(""String_Node_Str"") public void getDatasetTags(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String datasetId,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.DatasetInstance dataset=Id.DatasetInstance.from(namespaceId,datasetId);
  responder.sendJson(HttpResponseStatus.OK,getTags(dataset,scope));
}"
6308,"@GET @Path(""String_Node_Str"") public void getArtifactTags(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersionStr,@QueryParam(""String_Node_Str"") MetadataScope scope) throws BadRequestException, NotFoundException {
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.from(namespaceId),artifactName,artifactVersionStr);
  responder.sendJson(HttpResponseStatus.OK,getTags(artifactId,scope));
}","@GET @Path(""String_Node_Str"") public void getArtifactTags(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersionStr,@QueryParam(""String_Node_Str"") String scope) throws BadRequestException, NotFoundException {
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.from(namespaceId),artifactName,artifactVersionStr);
  responder.sendJson(HttpResponseStatus.OK,getTags(artifactId,scope));
}"
6309,"@GET @Path(""String_Node_Str"") public void getArtifactMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersionStr,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.from(namespaceId),artifactName,artifactVersionStr);
  responder.sendJson(HttpResponseStatus.OK,getMetadata(artifactId,scope),SET_METADATA_RECORD_TYPE,GSON);
}","@GET @Path(""String_Node_Str"") public void getArtifactMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersionStr,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.from(namespaceId),artifactName,artifactVersionStr);
  responder.sendJson(HttpResponseStatus.OK,getMetadata(artifactId,scope),SET_METADATA_RECORD_TYPE,GSON);
}"
6310,"@GET @Path(""String_Node_Str"") public void getDatasetProperties(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String datasetId,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(namespaceId,datasetId);
  responder.sendJson(HttpResponseStatus.OK,getProperties(datasetInstance,scope));
}","@GET @Path(""String_Node_Str"") public void getDatasetProperties(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String datasetId,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(namespaceId,datasetId);
  responder.sendJson(HttpResponseStatus.OK,getProperties(datasetInstance,scope));
}"
6311,"@GET @Path(""String_Node_Str"") public void getViewTags(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String streamId,@PathParam(""String_Node_Str"") String viewId,@QueryParam(""String_Node_Str"") MetadataScope scope) throws NotFoundException {
  Id.Stream.View view=Id.Stream.View.from(namespaceId,streamId,viewId);
  responder.sendJson(HttpResponseStatus.OK,getTags(view,scope));
}","@GET @Path(""String_Node_Str"") public void getViewTags(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String streamId,@PathParam(""String_Node_Str"") String viewId,@QueryParam(""String_Node_Str"") String scope) throws NotFoundException, BadRequestException {
  Id.Stream.View view=Id.Stream.View.from(namespaceId,streamId,viewId);
  responder.sendJson(HttpResponseStatus.OK,getTags(view,scope));
}"
6312,"@Test public void testSystemArtifacts() throws Exception {
  Id.Artifact defaultId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addAppArtifact(defaultId,WordCountApp.class).getStatusLine().getStatusCode());
  Id.Artifact systemId=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  File systemArtifact=buildAppArtifact(WordCountApp.class,""String_Node_Str"");
  artifactRepository.addArtifact(systemId,systemArtifact,Sets.<ArtifactRange>newHashSet());
  Set<ArtifactSummary> expectedArtifacts=Sets.newHashSet(new ArtifactSummary(""String_Node_Str"",""String_Node_Str"",ArtifactScope.USER),new ArtifactSummary(""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM));
  Set<ArtifactSummary> actualArtifacts=getArtifacts(Id.Namespace.DEFAULT);
  Assert.assertEquals(expectedArtifacts,actualArtifacts);
  expectedArtifacts=Sets.newHashSet(new ArtifactSummary(""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM));
  actualArtifacts=getArtifacts(Id.Namespace.DEFAULT,ArtifactScope.SYSTEM);
  Assert.assertEquals(expectedArtifacts,actualArtifacts);
  expectedArtifacts=Sets.newHashSet(new ArtifactSummary(""String_Node_Str"",""String_Node_Str"",ArtifactScope.USER));
  actualArtifacts=getArtifacts(Id.Namespace.DEFAULT,ArtifactScope.USER);
  Assert.assertEquals(expectedArtifacts,actualArtifacts);
  expectedArtifacts=Sets.newHashSet(new ArtifactSummary(""String_Node_Str"",""String_Node_Str"",ArtifactScope.USER));
  actualArtifacts=getArtifacts(Id.Namespace.DEFAULT,""String_Node_Str"",ArtifactScope.USER);
  Assert.assertEquals(expectedArtifacts,actualArtifacts);
  expectedArtifacts=Sets.newHashSet(new ArtifactSummary(""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM));
  actualArtifacts=getArtifacts(Id.Namespace.DEFAULT,""String_Node_Str"",ArtifactScope.SYSTEM);
  Assert.assertEquals(expectedArtifacts,actualArtifacts);
  ArtifactClasses classes=ArtifactClasses.builder().addApp(new ApplicationClass(WordCountApp.class.getName(),""String_Node_Str"",null)).build();
  ArtifactInfo expectedInfo=new ArtifactInfo(""String_Node_Str"",""String_Node_Str"",ArtifactScope.USER,classes,ImmutableMap.<String,String>of());
  ArtifactInfo actualInfo=getArtifact(defaultId,ArtifactScope.USER);
  Assert.assertEquals(expectedInfo,actualInfo);
  expectedInfo=new ArtifactInfo(""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM,classes,ImmutableMap.<String,String>of());
  actualInfo=getArtifact(defaultId,ArtifactScope.SYSTEM);
  Assert.assertEquals(expectedInfo,actualInfo);
}","@Test public void testSystemArtifacts() throws Exception {
  Id.Artifact defaultId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),addAppArtifact(defaultId,WordCountApp.class).getStatusLine().getStatusCode());
  Id.Artifact systemId=Id.Artifact.from(Id.Namespace.SYSTEM,""String_Node_Str"",""String_Node_Str"");
  File systemArtifact=buildAppArtifact(WordCountApp.class,""String_Node_Str"");
  artifactRepository.addArtifact(systemId,systemArtifact,new HashSet<ArtifactRange>());
  Set<ArtifactSummary> expectedArtifacts=Sets.newHashSet(new ArtifactSummary(""String_Node_Str"",""String_Node_Str"",ArtifactScope.USER),new ArtifactSummary(""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM));
  Set<ArtifactSummary> actualArtifacts=getArtifacts(Id.Namespace.DEFAULT);
  Assert.assertEquals(expectedArtifacts,actualArtifacts);
  expectedArtifacts=Sets.newHashSet(new ArtifactSummary(""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM));
  actualArtifacts=getArtifacts(Id.Namespace.DEFAULT,ArtifactScope.SYSTEM);
  Assert.assertEquals(expectedArtifacts,actualArtifacts);
  expectedArtifacts=Sets.newHashSet(new ArtifactSummary(""String_Node_Str"",""String_Node_Str"",ArtifactScope.USER));
  actualArtifacts=getArtifacts(Id.Namespace.DEFAULT,ArtifactScope.USER);
  Assert.assertEquals(expectedArtifacts,actualArtifacts);
  expectedArtifacts=Sets.newHashSet(new ArtifactSummary(""String_Node_Str"",""String_Node_Str"",ArtifactScope.USER));
  actualArtifacts=getArtifacts(Id.Namespace.DEFAULT,""String_Node_Str"",ArtifactScope.USER);
  Assert.assertEquals(expectedArtifacts,actualArtifacts);
  expectedArtifacts=Sets.newHashSet(new ArtifactSummary(""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM));
  actualArtifacts=getArtifacts(Id.Namespace.DEFAULT,""String_Node_Str"",ArtifactScope.SYSTEM);
  Assert.assertEquals(expectedArtifacts,actualArtifacts);
  ArtifactClasses classes=ArtifactClasses.builder().addApp(new ApplicationClass(WordCountApp.class.getName(),""String_Node_Str"",null)).build();
  ArtifactInfo expectedInfo=new ArtifactInfo(""String_Node_Str"",""String_Node_Str"",ArtifactScope.USER,classes,ImmutableMap.<String,String>of());
  ArtifactInfo actualInfo=getArtifact(defaultId,ArtifactScope.USER);
  Assert.assertEquals(expectedInfo,actualInfo);
  expectedInfo=new ArtifactInfo(""String_Node_Str"",""String_Node_Str"",ArtifactScope.SYSTEM,classes,ImmutableMap.<String,String>of());
  actualInfo=getArtifact(defaultId,ArtifactScope.SYSTEM);
  Assert.assertEquals(expectedInfo,actualInfo);
}"
6313,"/** 
 * If empty, start our own CDAP standalone instance for testing. If not empty, use the provided remote CDAP instance for testing.
 */
protected String getInstanceURI(){
  return System.getProperty(""String_Node_Str"",""String_Node_Str"");
}","/** 
 * Reads the CDAP instance URI from the system property ""instanceUri"". ""instanceUri"" should be specified in the format [host]:[port]. Defaults to ""localhost:10000"".
 */
protected String getInstanceURI(){
  return System.getProperty(""String_Node_Str"",""String_Node_Str"");
}"
6314,"@Test public void testAll() throws Exception {
  Id.Namespace namespace=Id.Namespace.DEFAULT;
  Id.Stream stream=Id.Stream.from(namespace,""String_Node_Str"");
  Id.Stream.View view1=Id.Stream.View.from(stream,""String_Node_Str"");
  LOG.info(""String_Node_Str"",stream);
  streamClient.create(stream);
  try {
    LOG.info(""String_Node_Str"",stream);
    streamClient.sendEvent(stream,""String_Node_Str"");
    streamClient.sendEvent(stream,""String_Node_Str"");
    streamClient.sendEvent(stream,""String_Node_Str"");
    LOG.info(""String_Node_Str"");
    Assert.assertEquals(ImmutableList.of(),streamViewClient.list(stream));
    try {
      streamViewClient.get(view1);
      Assert.fail();
    }
 catch (    NotFoundException e) {
      Assert.assertEquals(view1,e.getObject());
    }
    FormatSpecification format=new FormatSpecification(""String_Node_Str"",Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING))));
    ViewSpecification viewSpecification=new ViewSpecification(format,""String_Node_Str"");
    LOG.info(""String_Node_Str"",view1,GSON.toJson(viewSpecification));
    Assert.assertEquals(true,streamViewClient.createOrUpdate(view1,viewSpecification));
    LOG.info(""String_Node_Str"",view1);
    Assert.assertEquals(new ViewDetail(view1.getId(),viewSpecification),streamViewClient.get(view1));
    Assert.assertEquals(ImmutableList.of(view1.getId()),streamViewClient.list(stream));
    FormatSpecification newFormat=new FormatSpecification(""String_Node_Str"",Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING))));
    ViewSpecification newViewSpecification=new ViewSpecification(newFormat,""String_Node_Str"");
    LOG.info(""String_Node_Str"",view1,GSON.toJson(newViewSpecification));
    Assert.assertEquals(false,streamViewClient.createOrUpdate(view1,newViewSpecification));
    LOG.info(""String_Node_Str"",view1);
    Assert.assertEquals(new ViewDetail(view1.getId(),newViewSpecification),streamViewClient.get(view1));
    Assert.assertEquals(ImmutableList.of(view1.getId()),streamViewClient.list(stream));
    ExploreExecutionResult executionResult=queryClient.execute(view1.getNamespace(),""String_Node_Str"").get();
    Assert.assertNotNull(executionResult.getResultSchema());
    Assert.assertEquals(3,executionResult.getResultSchema().size());
    Assert.assertEquals(""String_Node_Str"",executionResult.getResultSchema().get(0).getName());
    Assert.assertEquals(""String_Node_Str"",executionResult.getResultSchema().get(1).getName());
    Assert.assertEquals(""String_Node_Str"",executionResult.getResultSchema().get(2).getName());
    List<QueryResult> results=Lists.newArrayList(executionResult);
    Assert.assertNotNull(results);
    Assert.assertEquals(3,results.size());
    Assert.assertEquals(""String_Node_Str"",results.get(0).getColumns().get(0));
    Assert.assertEquals(""String_Node_Str"",results.get(0).getColumns().get(1));
    Assert.assertEquals(""String_Node_Str"",results.get(0).getColumns().get(2));
    Assert.assertEquals(""String_Node_Str"",results.get(1).getColumns().get(0));
    Assert.assertEquals(""String_Node_Str"",results.get(1).getColumns().get(1));
    Assert.assertEquals(""String_Node_Str"",results.get(1).getColumns().get(2));
    Assert.assertEquals(""String_Node_Str"",results.get(2).getColumns().get(0));
    Assert.assertEquals(""String_Node_Str"",results.get(2).getColumns().get(1));
    Assert.assertEquals(""String_Node_Str"",results.get(2).getColumns().get(2));
    LOG.info(""String_Node_Str"",view1);
    streamViewClient.delete(view1);
    LOG.info(""String_Node_Str"",view1);
    try {
      streamViewClient.get(view1);
      Assert.fail();
    }
 catch (    NotFoundException e) {
      Assert.assertEquals(view1,e.getObject());
    }
    Assert.assertEquals(ImmutableList.of(),streamViewClient.list(stream));
  }
  finally {
    streamClient.delete(stream);
  }
  LOG.info(""String_Node_Str"",stream);
  streamClient.create(stream);
  try {
    FormatSpecification format=new FormatSpecification(""String_Node_Str"",Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING))));
    ViewSpecification viewSpecification=new ViewSpecification(format,""String_Node_Str"");
    LOG.info(""String_Node_Str"",view1,GSON.toJson(viewSpecification));
    Assert.assertEquals(true,streamViewClient.createOrUpdate(view1,viewSpecification));
    streamClient.delete(stream);
  }
  finally {
    streamViewClient.delete(view1);
    streamClient.delete(stream);
  }
}","@Test public void testAll() throws Exception {
  Id.Namespace namespace=Id.Namespace.DEFAULT;
  Id.Stream stream=Id.Stream.from(namespace,""String_Node_Str"");
  Id.Stream.View view1=Id.Stream.View.from(stream,""String_Node_Str"");
  LOG.info(""String_Node_Str"",stream);
  streamClient.create(stream);
  try {
    LOG.info(""String_Node_Str"",stream);
    streamClient.sendEvent(stream,""String_Node_Str"");
    streamClient.sendEvent(stream,""String_Node_Str"");
    streamClient.sendEvent(stream,""String_Node_Str"");
    LOG.info(""String_Node_Str"");
    Assert.assertEquals(ImmutableList.of(),streamViewClient.list(stream));
    try {
      streamViewClient.get(view1);
      Assert.fail();
    }
 catch (    NotFoundException e) {
      Assert.assertEquals(view1,e.getObject());
    }
    FormatSpecification format=new FormatSpecification(""String_Node_Str"",Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING))));
    ViewSpecification viewSpecification=new ViewSpecification(format,""String_Node_Str"");
    LOG.info(""String_Node_Str"",view1,GSON.toJson(viewSpecification));
    Assert.assertEquals(true,streamViewClient.createOrUpdate(view1,viewSpecification));
    LOG.info(""String_Node_Str"",view1);
    Assert.assertEquals(new ViewDetail(view1.getId(),viewSpecification),streamViewClient.get(view1));
    Assert.assertEquals(ImmutableList.of(view1.getId()),streamViewClient.list(stream));
    FormatSpecification newFormat=new FormatSpecification(""String_Node_Str"",Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING))));
    ViewSpecification newViewSpecification=new ViewSpecification(newFormat,""String_Node_Str"");
    LOG.info(""String_Node_Str"",view1,GSON.toJson(newViewSpecification));
    Assert.assertEquals(false,streamViewClient.createOrUpdate(view1,newViewSpecification));
    LOG.info(""String_Node_Str"",view1);
    Assert.assertEquals(new ViewDetail(view1.getId(),newViewSpecification),streamViewClient.get(view1));
    Assert.assertEquals(ImmutableList.of(view1.getId()),streamViewClient.list(stream));
    ExploreExecutionResult executionResult=queryClient.execute(view1.getNamespace(),""String_Node_Str"").get();
    Assert.assertNotNull(executionResult.getResultSchema());
    Assert.assertEquals(3,executionResult.getResultSchema().size());
    Assert.assertEquals(""String_Node_Str"",executionResult.getResultSchema().get(0).getName());
    Assert.assertEquals(""String_Node_Str"",executionResult.getResultSchema().get(1).getName());
    Assert.assertEquals(""String_Node_Str"",executionResult.getResultSchema().get(2).getName());
    List<QueryResult> results=Lists.newArrayList(executionResult);
    Assert.assertNotNull(results);
    Assert.assertEquals(3,results.size());
    Assert.assertEquals(""String_Node_Str"",results.get(0).getColumns().get(0));
    Assert.assertEquals(""String_Node_Str"",results.get(0).getColumns().get(1));
    Assert.assertEquals(""String_Node_Str"",results.get(0).getColumns().get(2));
    Assert.assertEquals(""String_Node_Str"",results.get(1).getColumns().get(0));
    Assert.assertEquals(""String_Node_Str"",results.get(1).getColumns().get(1));
    Assert.assertEquals(""String_Node_Str"",results.get(1).getColumns().get(2));
    Assert.assertEquals(""String_Node_Str"",results.get(2).getColumns().get(0));
    Assert.assertEquals(""String_Node_Str"",results.get(2).getColumns().get(1));
    Assert.assertEquals(""String_Node_Str"",results.get(2).getColumns().get(2));
    LOG.info(""String_Node_Str"",view1);
    streamViewClient.delete(view1);
    LOG.info(""String_Node_Str"",view1);
    try {
      streamViewClient.get(view1);
      Assert.fail();
    }
 catch (    NotFoundException e) {
      Assert.assertEquals(view1,e.getObject());
    }
    Assert.assertEquals(ImmutableList.of(),streamViewClient.list(stream));
  }
  finally {
    streamClient.delete(stream);
  }
  LOG.info(""String_Node_Str"",stream);
  streamClient.create(stream);
  try {
    FormatSpecification format=new FormatSpecification(""String_Node_Str"",Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING))));
    ViewSpecification viewSpecification=new ViewSpecification(format,""String_Node_Str"");
    LOG.info(""String_Node_Str"",view1,GSON.toJson(viewSpecification));
    Assert.assertEquals(true,streamViewClient.createOrUpdate(view1,viewSpecification));
  }
  finally {
    streamClient.delete(stream);
  }
}"
6315,"@Override public void run(){
  try {
    Location configLocation=getConfigLocation(streamId);
    if (!configLocation.exists()) {
      return;
    }
    alterExploreStream(StreamUtils.getStreamIdFromLocation(streamLocation),false,null);
    if (!configLocation.delete()) {
      LOG.debug(""String_Node_Str"",streamLocation);
    }
    metadataStore.removeMetadata(streamId);
    List<Id.Stream.View> views=viewAdmin.list(streamId);
    for (    Id.Stream.View view : views) {
      viewAdmin.delete(view);
    }
    Location deleted=StreamUtils.getDeletedLocation(getStreamBaseLocation(streamId.getNamespace()));
    Locations.mkdirsIfNotExists(deleted);
    streamLocation.renameTo(deleted.append(streamId.getId() + System.currentTimeMillis()));
    streamMetaStore.removeStream(streamId);
    metadataStore.removeMetadata(streamId);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","@Override public void run(){
  try {
    Location configLocation=getConfigLocation(streamId);
    if (!configLocation.exists()) {
      return;
    }
    alterExploreStream(StreamUtils.getStreamIdFromLocation(streamLocation),false,null);
    List<Id.Stream.View> views=viewAdmin.list(streamId);
    for (    Id.Stream.View view : views) {
      viewAdmin.delete(view);
    }
    if (!configLocation.delete()) {
      LOG.debug(""String_Node_Str"",streamLocation);
    }
    metadataStore.removeMetadata(streamId);
    Location deleted=StreamUtils.getDeletedLocation(getStreamBaseLocation(streamId.getNamespace()));
    Locations.mkdirsIfNotExists(deleted);
    streamLocation.renameTo(deleted.append(streamId.getId() + System.currentTimeMillis()));
    streamMetaStore.removeStream(streamId);
    metadataStore.removeMetadata(streamId);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
6316,"private void doDrop(final Id.Stream streamId,final Location streamLocation) throws Exception {
  streamCoordinatorClient.deleteStream(streamId,new Runnable(){
    @Override public void run(){
      try {
        Location configLocation=getConfigLocation(streamId);
        if (!configLocation.exists()) {
          return;
        }
        alterExploreStream(StreamUtils.getStreamIdFromLocation(streamLocation),false,null);
        if (!configLocation.delete()) {
          LOG.debug(""String_Node_Str"",streamLocation);
        }
        metadataStore.removeMetadata(streamId);
        List<Id.Stream.View> views=viewAdmin.list(streamId);
        for (        Id.Stream.View view : views) {
          viewAdmin.delete(view);
        }
        Location deleted=StreamUtils.getDeletedLocation(getStreamBaseLocation(streamId.getNamespace()));
        Locations.mkdirsIfNotExists(deleted);
        streamLocation.renameTo(deleted.append(streamId.getId() + System.currentTimeMillis()));
        streamMetaStore.removeStream(streamId);
        metadataStore.removeMetadata(streamId);
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
}","private void doDrop(final Id.Stream streamId,final Location streamLocation) throws Exception {
  streamCoordinatorClient.deleteStream(streamId,new Runnable(){
    @Override public void run(){
      try {
        Location configLocation=getConfigLocation(streamId);
        if (!configLocation.exists()) {
          return;
        }
        alterExploreStream(StreamUtils.getStreamIdFromLocation(streamLocation),false,null);
        List<Id.Stream.View> views=viewAdmin.list(streamId);
        for (        Id.Stream.View view : views) {
          viewAdmin.delete(view);
        }
        if (!configLocation.delete()) {
          LOG.debug(""String_Node_Str"",streamLocation);
        }
        metadataStore.removeMetadata(streamId);
        Location deleted=StreamUtils.getDeletedLocation(getStreamBaseLocation(streamId.getNamespace()));
        Locations.mkdirsIfNotExists(deleted);
        streamLocation.renameTo(deleted.append(streamId.getId() + System.currentTimeMillis()));
        streamMetaStore.removeStream(streamId);
        metadataStore.removeMetadata(streamId);
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
}"
6317,"@GET @Path(""String_Node_Str"") public void searchMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@QueryParam(""String_Node_Str"") String searchQuery,@QueryParam(""String_Node_Str"") String target) throws Exception {
  MetadataSearchTargetType metadataSearchTargetType;
  if (target != null) {
    metadataSearchTargetType=MetadataSearchTargetType.valueOf(target);
  }
 else {
    metadataSearchTargetType=null;
  }
  Set<MetadataSearchResultRecord> results=metadataAdmin.searchMetadata(namespaceId,searchQuery,metadataSearchTargetType);
  responder.sendJson(HttpResponseStatus.OK,results,SET_METADATA_SEARCH_RESULT_TYPE,GSON);
}","@GET @Path(""String_Node_Str"") public void searchMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@QueryParam(""String_Node_Str"") String searchQuery,@QueryParam(""String_Node_Str"") String target) throws Exception {
  MetadataSearchTargetType metadataSearchTargetType;
  if (target != null) {
    metadataSearchTargetType=MetadataSearchTargetType.valueOf(target.toUpperCase());
  }
 else {
    metadataSearchTargetType=null;
  }
  Set<MetadataSearchResultRecord> results=metadataAdmin.searchMetadata(namespaceId,searchQuery,metadataSearchTargetType);
  responder.sendJson(HttpResponseStatus.OK,results,SET_METADATA_SEARCH_RESULT_TYPE,GSON);
}"
6318,"/** 
 * Store indexes for a   {@link MetadataEntry}
 * @param targetId the {@link Id.NamespacedId} from which the metadata indexes has to be stored
 * @param entry the {@link MetadataEntry} which has to be indexed
 */
private void storeIndexes(Id.NamespacedId targetId,MetadataEntry entry){
  Set<String> indexes=Sets.newHashSet(Arrays.asList(VALUE_SPLIT_PATTERN.split(entry.getValue())));
  indexes.add(entry.getValue());
  for (  String index : indexes) {
    indexedTable.put(getIndexPut(targetId,entry.getKey(),entry.getKey() + KEYVALUE_SEPARATOR + index));
    indexedTable.put(getIndexPut(targetId,entry.getKey(),index));
  }
}","/** 
 * Store indexes for a   {@link MetadataEntry}
 * @param targetId the {@link Id.NamespacedId} from which the metadata indexes has to be stored
 * @param entry the {@link MetadataEntry} which has to be indexed
 */
private void storeIndexes(Id.NamespacedId targetId,MetadataEntry entry){
  Set<String> valueIndexes=new HashSet<>();
  if (entry.getValue().contains(TAGS_SEPARATOR)) {
    valueIndexes.addAll(Arrays.asList(TAGS_SEPARATOR_PATTERN.split(entry.getValue())));
  }
 else {
    valueIndexes.add(entry.getValue());
  }
  Set<String> indexes=Sets.newHashSet();
  for (  String index : valueIndexes) {
    indexes.addAll(Arrays.asList(VALUE_SPLIT_PATTERN.split(index)));
  }
  indexes.addAll(valueIndexes);
  for (  String index : indexes) {
    indexedTable.put(getIndexPut(targetId,entry.getKey(),entry.getKey() + KEYVALUE_SEPARATOR + index));
    indexedTable.put(getIndexPut(targetId,entry.getKey(),index));
  }
}"
6319,"@Test public void testSearchOnValue() throws Exception {
  MetadataEntry entry=new MetadataEntry(flow1,""String_Node_Str"",""String_Node_Str"");
  String multiWordValue=""String_Node_Str"";
  MetadataEntry multiWordEntry=new MetadataEntry(flow1,""String_Node_Str"",multiWordValue);
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",multiWordValue);
  List<MetadataEntry> results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  MetadataEntry result=results.get(0);
  Assert.assertEquals(entry,result);
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  result=results.get(0);
  Assert.assertEquals(multiWordEntry,result);
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(0,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(0,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(0,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(0,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(0,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  result=results.get(0);
  Assert.assertEquals(multiWordEntry,result);
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  result=results.get(0);
  Assert.assertEquals(entry,result);
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<MetadataEntry> results2=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(2,results2.size());
  for (  MetadataEntry result2 : results2) {
    Assert.assertEquals(""String_Node_Str"",result2.getValue());
  }
  dataset.setProperty(stream1,""String_Node_Str"",""String_Node_Str"");
  List<MetadataEntry> results3=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(2,results3.size());
  for (  MetadataEntry result3 : results3) {
    Assert.assertTrue(result3.getValue().startsWith(""String_Node_Str""));
  }
  List<MetadataEntry> results4=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(0,results4.size());
}","@Test public void testSearchOnValue() throws Exception {
  MetadataEntry entry=new MetadataEntry(flow1,""String_Node_Str"",""String_Node_Str"");
  String multiWordValue=""String_Node_Str"";
  MetadataEntry multiWordEntry=new MetadataEntry(flow1,""String_Node_Str"",multiWordValue);
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",multiWordValue);
  List<MetadataEntry> results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  MetadataEntry result=results.get(0);
  Assert.assertEquals(entry,result);
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  result=results.get(0);
  Assert.assertEquals(multiWordEntry,result);
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(1,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(0,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(0,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(0,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(0,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  result=results.get(0);
  Assert.assertEquals(multiWordEntry,result);
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  result=results.get(0);
  Assert.assertEquals(entry,result);
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<MetadataEntry> results2=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(2,results2.size());
  for (  MetadataEntry result2 : results2) {
    Assert.assertEquals(""String_Node_Str"",result2.getValue());
  }
  dataset.setProperty(stream1,""String_Node_Str"",""String_Node_Str"");
  List<MetadataEntry> results3=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(2,results3.size());
  for (  MetadataEntry result3 : results3) {
    Assert.assertTrue(result3.getValue().startsWith(""String_Node_Str""));
  }
  List<MetadataEntry> results4=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(0,results4.size());
}"
6320,"@Test public void testSearchOnTags() throws Exception {
  Assert.assertEquals(0,dataset.getTags(app1).size());
  Assert.assertEquals(0,dataset.getTags(appNs2).size());
  Assert.assertEquals(0,dataset.getTags(flow1).size());
  Assert.assertEquals(0,dataset.getTags(dataset1).size());
  Assert.assertEquals(0,dataset.getTags(stream1).size());
  dataset.addTags(app1,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  dataset.addTags(appNs2,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  dataset.addTags(flow1,""String_Node_Str"");
  dataset.addTags(dataset1,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  dataset.addTags(stream1,""String_Node_Str"");
  List<MetadataEntry> results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(4,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(3,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(2,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(1,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(1,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(1,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(2,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(1,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(1,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.APP);
  Assert.assertEquals(1,results.size());
  dataset.removeTags(app1);
  dataset.removeTags(flow1);
  dataset.removeTags(dataset1);
  dataset.removeTags(stream1);
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(0,results.size());
  Assert.assertEquals(0,dataset.getTags(app1).size());
  Assert.assertEquals(0,dataset.getTags(flow1).size());
  Assert.assertEquals(0,dataset.getTags(dataset1).size());
  Assert.assertEquals(0,dataset.getTags(stream1).size());
}","@Test public void testSearchOnTags() throws Exception {
  Assert.assertEquals(0,dataset.getTags(app1).size());
  Assert.assertEquals(0,dataset.getTags(appNs2).size());
  Assert.assertEquals(0,dataset.getTags(flow1).size());
  Assert.assertEquals(0,dataset.getTags(dataset1).size());
  Assert.assertEquals(0,dataset.getTags(stream1).size());
  dataset.addTags(app1,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  dataset.addTags(appNs2,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  dataset.addTags(flow1,""String_Node_Str"");
  dataset.addTags(dataset1,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  dataset.addTags(stream1,""String_Node_Str"");
  List<MetadataEntry> results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(4,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(3,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(2,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(1,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(1,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(1,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(1,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(2,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(1,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(1,results.size());
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.APP);
  Assert.assertEquals(1,results.size());
  dataset.removeTags(app1);
  dataset.removeTags(flow1);
  dataset.removeTags(dataset1);
  dataset.removeTags(stream1);
  results=dataset.search(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(0,results.size());
  Assert.assertEquals(0,dataset.getTags(app1).size());
  Assert.assertEquals(0,dataset.getTags(flow1).size());
  Assert.assertEquals(0,dataset.getTags(dataset1).size());
  Assert.assertEquals(0,dataset.getTags(stream1).size());
}"
6321,"private void generateLogs(LoggingContext loggingContext,Id.Program id,ProgramRunStatus runStatus) throws InterruptedException {
  String entityId=LoggingContextHelper.getEntityId(loggingContext).getValue();
  RunId runId=null;
  Long stopTs=null;
  for (int i=0; i < MAX; ++i) {
    if (i == 20) {
      runId=RunIds.generate(TimeUnit.SECONDS.toMillis(getMockTimeSecs(i)));
    }
 else     if (i == 60 && runStatus != ProgramRunStatus.RUNNING && runStatus != ProgramRunStatus.SUSPENDED) {
      stopTs=getMockTimeSecs(i);
    }
    LoggingEvent event=new LoggingEvent(""String_Node_Str"",(ch.qos.logback.classic.Logger)LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),i % 2 == 0 ? Level.ERROR : Level.WARN,entityId + ""String_Node_Str"" + i,null,null);
    event.setTimeStamp(TimeUnit.SECONDS.toMillis(getMockTimeSecs(i)));
    Map<String,String> tagMap=Maps.newHashMap(Maps.transformValues(loggingContext.getSystemTagsMap(),TAG_TO_STRING_FUNCTION));
    if (runId != null && i % 2 == 0) {
      tagMap.put(ApplicationLoggingContext.TAG_RUNID_ID,runId.getId());
    }
    event.setMDCPropertyMap(tagMap);
    logEvents.add(new LogEvent(event,new LogOffset(i,i)));
  }
  long startTs=RunIds.getTime(runId,TimeUnit.SECONDS);
  if (id != null) {
    runRecordMap.put(id,new RunRecord(runId.getId(),startTs,stopTs,runStatus,null));
    store.setStart(id,runId.getId(),startTs);
    if (stopTs != null) {
      store.setStop(id,runId.getId(),stopTs,runStatus);
    }
  }
}","private void generateLogs(LoggingContext loggingContext,Id.Program id,ProgramRunStatus runStatus) throws InterruptedException {
  String entityId=LoggingContextHelper.getEntityId(loggingContext).getValue();
  RunId runId=null;
  Long stopTs=null;
  for (int i=0; i < MAX; ++i) {
    if (i == 20) {
      runId=RunIds.generate(TimeUnit.SECONDS.toMillis(getMockTimeSecs(i)));
    }
 else     if (i == 60 && runStatus != ProgramRunStatus.RUNNING && runStatus != ProgramRunStatus.SUSPENDED) {
      stopTs=getMockTimeSecs(i);
    }
    LoggingEvent event=new LoggingEvent(""String_Node_Str"",(ch.qos.logback.classic.Logger)LoggerFactory.getLogger(Logger.ROOT_LOGGER_NAME),i % 2 == 0 ? Level.ERROR : Level.WARN,entityId + ""String_Node_Str"" + i,null,null);
    event.setTimeStamp(TimeUnit.SECONDS.toMillis(getMockTimeSecs(i)));
    Map<String,String> tagMap=Maps.newHashMap(Maps.transformValues(loggingContext.getSystemTagsMap(),TAG_TO_STRING_FUNCTION));
    if (runId != null && stopTs == null && i % 2 == 0) {
      tagMap.put(ApplicationLoggingContext.TAG_RUNID_ID,runId.getId());
    }
    event.setMDCPropertyMap(tagMap);
    logEvents.add(new LogEvent(event,new LogOffset(i,i)));
  }
  long startTs=RunIds.getTime(runId,TimeUnit.SECONDS);
  if (id != null) {
    runRecordMap.put(id,new RunRecord(runId.getId(),startTs,stopTs,runStatus,null));
    store.setStart(id,runId.getId(),startTs);
    if (stopTs != null) {
      store.setStop(id,runId.getId(),stopTs,runStatus);
    }
  }
}"
6322,"/** 
 * If readRange is outside runRecord's range, then the readRange is adjusted to fall within runRecords range.
 */
private ReadRange adjustReadRange(ReadRange readRange,@Nullable RunRecordMeta runRecord){
  if (runRecord == null) {
    return readRange;
  }
  long fromTimeMillis=readRange.getFromMillis();
  long toTimeMillis=readRange.getToMillis();
  long runStartMillis=TimeUnit.SECONDS.toMillis(runRecord.getStartTs());
  if (fromTimeMillis < runStartMillis) {
    fromTimeMillis=runStartMillis;
  }
  if (runRecord.getStopTs() != null) {
    long runStopMillis=TimeUnit.SECONDS.toMillis(runRecord.getStopTs());
    if (toTimeMillis > runStopMillis) {
      toTimeMillis=runStopMillis;
    }
  }
  ReadRange adjusted=new ReadRange(fromTimeMillis,toTimeMillis,readRange.getKafkaOffset());
  LOG.trace(""String_Node_Str"",readRange,adjusted);
  return adjusted;
}","/** 
 * If readRange is outside runRecord's range, then the readRange is adjusted to fall within runRecords range.
 */
private ReadRange adjustReadRange(ReadRange readRange,@Nullable RunRecordMeta runRecord){
  if (runRecord == null) {
    return readRange;
  }
  long fromTimeMillis=readRange.getFromMillis();
  long toTimeMillis=readRange.getToMillis();
  long runStartMillis=TimeUnit.SECONDS.toMillis(runRecord.getStartTs());
  if (fromTimeMillis < runStartMillis) {
    fromTimeMillis=runStartMillis;
  }
  if (runRecord.getStopTs() != null) {
    long runStopMillis=TimeUnit.SECONDS.toMillis(runRecord.getStopTs() + 1);
    if (toTimeMillis > runStopMillis) {
      toTimeMillis=runStopMillis;
    }
  }
  ReadRange adjusted=new ReadRange(fromTimeMillis,toTimeMillis,readRange.getKafkaOffset());
  LOG.trace(""String_Node_Str"",readRange,adjusted);
  return adjusted;
}"
6323,"@Test public void testSystemServices() throws Exception {
  Type token=new TypeToken<List<SystemServiceMeta>>(){
  }
.getType();
  HttpURLConnection urlConn=openURL(""String_Node_Str"",HttpMethod.GET);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),urlConn.getResponseCode());
  List<SystemServiceMeta> actual=GSON.fromJson(new String(ByteStreams.toByteArray(urlConn.getInputStream()),Charsets.UTF_8),token);
  Assert.assertEquals(8,actual.size());
  urlConn.disconnect();
}","@Test public void testSystemServices() throws Exception {
  Type token=new TypeToken<List<SystemServiceMeta>>(){
  }
.getType();
  HttpURLConnection urlConn=openURL(""String_Node_Str"",HttpMethod.GET);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),urlConn.getResponseCode());
  List<SystemServiceMeta> actual=GSON.fromJson(new String(ByteStreams.toByteArray(urlConn.getInputStream()),Charsets.UTF_8),token);
  Assert.assertEquals(9,actual.size());
  urlConn.disconnect();
}"
6324,"@Test public void testSystemServicesStatus() throws Exception {
  Type token=new TypeToken<Map<String,String>>(){
  }
.getType();
  HttpURLConnection urlConn=openURL(""String_Node_Str"",HttpMethod.GET);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),urlConn.getResponseCode());
  Map<String,String> result=GSON.fromJson(new String(ByteStreams.toByteArray(urlConn.getInputStream()),Charsets.UTF_8),token);
  Assert.assertEquals(8,result.size());
  urlConn.disconnect();
  Assert.assertEquals(""String_Node_Str"",result.get(Constants.Service.APP_FABRIC_HTTP));
}","@Test public void testSystemServicesStatus() throws Exception {
  Type token=new TypeToken<Map<String,String>>(){
  }
.getType();
  HttpURLConnection urlConn=openURL(""String_Node_Str"",HttpMethod.GET);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),urlConn.getResponseCode());
  Map<String,String> result=GSON.fromJson(new String(ByteStreams.toByteArray(urlConn.getInputStream()),Charsets.UTF_8),token);
  Assert.assertEquals(9,result.size());
  urlConn.disconnect();
  Assert.assertEquals(""String_Node_Str"",result.get(Constants.Service.APP_FABRIC_HTTP));
}"
6325,"private void stopProgramIfRunning(Id.Program programId) throws InterruptedException, ExecutionException {
  ProgramRuntimeService.RuntimeInfo programRunInfo=findRuntimeInfo(programId.getNamespaceId(),programId.getApplicationId(),programId.getId(),programId.getType(),runtimeService);
  if (programRunInfo != null) {
    doStop(programRunInfo);
  }
}","private void stopProgramIfRunning(Id.Program programId) throws InterruptedException, ExecutionException {
  ProgramRuntimeService.RuntimeInfo programRunInfo=findRuntimeInfo(programId,runtimeService);
  if (programRunInfo != null) {
    ProgramController controller=programRunInfo.getController();
    controller.stop().get();
  }
}"
6326,"/** 
 * Set instances of a service.
 */
@PUT @Path(""String_Node_Str"") public void setServiceInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId) throws ExecutionException, InterruptedException {
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.SERVICE,serviceId);
    if (!store.programExists(programId)) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
      return;
    }
    int instances;
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    int oldInstances=store.getServiceInstances(programId);
    if (oldInstances != instances) {
      store.setServiceInstances(programId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(namespaceId,appId,serviceId,ProgramType.SERVICE,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(serviceId,String.valueOf(instances))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable throwable) {
    if (respondIfElementNotFound(throwable,responder)) {
      return;
    }
    throw throwable;
  }
}","/** 
 * Set instances of a service.
 */
@PUT @Path(""String_Node_Str"") public void setServiceInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId) throws ExecutionException, InterruptedException {
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.SERVICE,serviceId);
    if (!store.programExists(programId)) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
      return;
    }
    int instances;
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    int oldInstances=store.getServiceInstances(programId);
    if (oldInstances != instances) {
      store.setServiceInstances(programId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(programId,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(serviceId,String.valueOf(instances))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable throwable) {
    if (respondIfElementNotFound(throwable,responder)) {
      return;
    }
    throw throwable;
  }
}"
6327,"/** 
 * Sets the number of instances of a worker.
 */
@PUT @Path(""String_Node_Str"") public void setWorkerInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workerId) throws ExecutionException, InterruptedException {
  int instances;
  try {
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
  }
 catch (  Throwable th) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.WORKER,workerId);
    int oldInstances=store.getWorkerInstances(programId);
    if (oldInstances != instances) {
      store.setWorkerInstances(programId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(namespaceId,appId,workerId,ProgramType.WORKER,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(programId.getId(),String.valueOf(instances))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    throw e;
  }
}","/** 
 * Sets the number of instances of a worker.
 */
@PUT @Path(""String_Node_Str"") public void setWorkerInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workerId) throws ExecutionException, InterruptedException {
  int instances;
  try {
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
  }
 catch (  Throwable th) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.WORKER,workerId);
    int oldInstances=store.getWorkerInstances(programId);
    if (oldInstances != instances) {
      store.setWorkerInstances(programId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(programId,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(programId.getId(),String.valueOf(instances))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    throw e;
  }
}"
6328,"/** 
 * Returns the program status.
 * @param id the id of the program for which the status call is made
 * @return the status of the program
 * @throws BadRequestException if the program type is invalid
 * @throws NotFoundException if the application to which this program belongs was not found
 */
private ProgramStatus getProgramStatus(Id.Program id) throws BadRequestException, NotFoundException {
  if (id.getType() == null) {
    throw new BadRequestException(String.format(""String_Node_Str"",id.getId()));
  }
  ApplicationSpecification appSpec=store.getApplication(id.getApplication());
  if (appSpec == null) {
    throw new NotFoundException(Id.Application.from(id.getNamespaceId(),id.getApplicationId()));
  }
  ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(id,null);
  if (runtimeInfo == null) {
    if (id.getType() != ProgramType.WEBAPP) {
      ProgramSpecification spec=getProgramSpecification(id);
      if (spec == null) {
        throw new NotFoundException(id);
      }
      if (id.getType() == ProgramType.MAPREDUCE && !store.getRuns(id,ProgramRunStatus.RUNNING,0,Long.MAX_VALUE,1).isEmpty()) {
        return ProgramStatus.RUNNING;
      }
      return ProgramStatus.STOPPED;
    }
    try {
      Location webappLoc=Programs.programLocation(namespacedLocationFactory,appFabricDir,id);
      if (webappLoc != null && webappLoc.exists()) {
        return ProgramStatus.STOPPED;
      }
      throw new NotFoundException(id);
    }
 catch (    IOException ioe) {
      throw new NotFoundException(id,ioe);
    }
  }
  return runtimeInfo.getController().getState().getProgramStatus();
}","/** 
 * Returns the program status.
 * @param id the id of the program for which the status call is made
 * @return the status of the program
 * @throws BadRequestException if the program type is invalid
 * @throws NotFoundException if the application to which this program belongs was not found
 */
private ProgramStatus getProgramStatus(Id.Program id) throws BadRequestException, NotFoundException {
  if (id.getType() == null) {
    throw new BadRequestException(String.format(""String_Node_Str"",id.getId()));
  }
  ApplicationSpecification appSpec=store.getApplication(id.getApplication());
  if (appSpec == null) {
    throw new NotFoundException(Id.Application.from(id.getNamespaceId(),id.getApplicationId()));
  }
  ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(id,(String)null);
  if (runtimeInfo == null) {
    if (id.getType() != ProgramType.WEBAPP) {
      ProgramSpecification spec=getProgramSpecification(id);
      if (spec == null) {
        throw new NotFoundException(id);
      }
      if (id.getType() == ProgramType.MAPREDUCE && !store.getRuns(id,ProgramRunStatus.RUNNING,0,Long.MAX_VALUE,1).isEmpty()) {
        return ProgramStatus.RUNNING;
      }
      return ProgramStatus.STOPPED;
    }
    try {
      Location webappLoc=Programs.programLocation(namespacedLocationFactory,appFabricDir,id);
      if (webappLoc != null && webappLoc.exists()) {
        return ProgramStatus.STOPPED;
      }
      throw new NotFoundException(id);
    }
 catch (    IOException ioe) {
      throw new NotFoundException(id,ioe);
    }
  }
  return runtimeInfo.getController().getState().getProgramStatus();
}"
6329,"/** 
 * Increases number of instance for a flowlet within a flow.
 */
@PUT @Path(""String_Node_Str"") public synchronized void setFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId,@PathParam(""String_Node_Str"") String flowletId) throws ExecutionException, InterruptedException {
  int instances;
  try {
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
  }
 catch (  Throwable th) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.FLOW,flowId);
    int oldInstances=store.getFlowletInstances(programId,flowletId);
    if (oldInstances != instances) {
      FlowSpecification flowSpec=store.setFlowletInstances(programId,flowletId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(namespaceId,appId,flowId,ProgramType.FLOW,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(""String_Node_Str"",flowletId,""String_Node_Str"",String.valueOf(instances),""String_Node_Str"",GSON.toJson(flowSpec,FlowSpecification.class))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    throw e;
  }
}","/** 
 * Increases number of instance for a flowlet within a flow.
 */
@PUT @Path(""String_Node_Str"") public synchronized void setFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId,@PathParam(""String_Node_Str"") String flowletId) throws ExecutionException, InterruptedException {
  int instances;
  try {
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
  }
 catch (  Throwable th) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.FLOW,flowId);
    int oldInstances=store.getFlowletInstances(programId,flowletId);
    if (oldInstances != instances) {
      FlowSpecification flowSpec=store.setFlowletInstances(programId,flowletId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(programId,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(""String_Node_Str"",flowletId,""String_Node_Str"",String.valueOf(instances),""String_Node_Str"",GSON.toJson(flowSpec,FlowSpecification.class))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    throw e;
  }
}"
6330,"protected ProgramRuntimeService.RuntimeInfo findRuntimeInfo(String namespaceId,String appId,String flowId,ProgramType type,ProgramRuntimeService runtimeService){
  Collection<ProgramRuntimeService.RuntimeInfo> runtimeInfos=runtimeService.list(type).values();
  Preconditions.checkNotNull(runtimeInfos,UserMessages.getMessage(UserErrors.RUNTIME_INFO_NOT_FOUND),namespaceId,flowId);
  Id.Program programId=Id.Program.from(namespaceId,appId,type,flowId);
  for (  ProgramRuntimeService.RuntimeInfo info : runtimeInfos) {
    if (programId.equals(info.getProgramId())) {
      return info;
    }
  }
  return null;
}","protected ProgramRuntimeService.RuntimeInfo findRuntimeInfo(Id.Program programId,ProgramRuntimeService runtimeService){
  Collection<ProgramRuntimeService.RuntimeInfo> runtimeInfos=runtimeService.list(programId.getType()).values();
  Preconditions.checkNotNull(runtimeInfos,UserMessages.getMessage(UserErrors.RUNTIME_INFO_NOT_FOUND),programId);
  for (  ProgramRuntimeService.RuntimeInfo info : runtimeInfos) {
    if (programId.equals(info.getProgramId())) {
      return info;
    }
  }
  return null;
}"
6331,"/** 
 * Checks if all schedule requirements are satisfied, then executes the given program without blocking until its completion.
 * @param programId Program Id
 * @param systemOverrides Arguments that would be supplied as system runtime arguments for the program.
 * @return a {@link ListenableFuture} object that completes when the program completes
 * @throws TaskExecutionException if program is already running or program is not found.
 * @throws IOException if program failed to start.
 */
public ListenableFuture<?> run(Id.Program programId,Map<String,String> systemOverrides) throws TaskExecutionException, IOException {
  Map<String,String> userArgs=Maps.newHashMap();
  Map<String,String> systemArgs=Maps.newHashMap();
  String scheduleName=systemOverrides.get(ProgramOptionConstants.SCHEDULE_NAME);
  ApplicationSpecification appSpec=store.getApplication(programId.getApplication());
  if (appSpec == null || appSpec.getSchedules().get(scheduleName) == null) {
    throw new TaskExecutionException(UserMessages.getMessage(UserErrors.PROGRAM_NOT_FOUND),false);
  }
  ScheduleSpecification spec=appSpec.getSchedules().get(scheduleName);
  if (!requirementsChecker.checkSatisfied(programId,spec.getSchedule())) {
    return Futures.<Void>immediateFuture(null);
  }
  userArgs.putAll(spec.getProperties());
  userArgs.putAll(propertiesResolver.getUserProperties(programId));
  systemArgs.putAll(propertiesResolver.getSystemProperties(programId));
  systemArgs.putAll(systemOverrides);
  return execute(programId,systemArgs,userArgs);
}","/** 
 * Checks if all schedule requirements are satisfied, then executes the given program without blocking until its completion.
 * @param programId Program Id
 * @param systemOverrides Arguments that would be supplied as system runtime arguments for the program.
 * @return a {@link ListenableFuture} object that completes when the program completes
 * @throws TaskExecutionException if program is already running or program is not found.
 * @throws IOException if program failed to start.
 */
public ListenableFuture<?> run(Id.Program programId,Map<String,String> systemOverrides) throws TaskExecutionException, IOException {
  Map<String,String> userArgs=Maps.newHashMap();
  Map<String,String> systemArgs=Maps.newHashMap();
  String scheduleName=systemOverrides.get(ProgramOptionConstants.SCHEDULE_NAME);
  ApplicationSpecification appSpec=store.getApplication(programId.getApplication());
  if (appSpec == null || appSpec.getSchedules().get(scheduleName) == null) {
    throw new TaskExecutionException(String.format(UserMessages.getMessage(UserErrors.PROGRAM_NOT_FOUND),programId),false);
  }
  ScheduleSpecification spec=appSpec.getSchedules().get(scheduleName);
  if (!requirementsChecker.checkSatisfied(programId,spec.getSchedule())) {
    return Futures.<Void>immediateFuture(null);
  }
  userArgs.putAll(spec.getProperties());
  userArgs.putAll(propertiesResolver.getUserProperties(programId));
  systemArgs.putAll(propertiesResolver.getSystemProperties(programId));
  systemArgs.putAll(systemOverrides);
  return execute(programId,systemArgs,userArgs);
}"
6332,"/** 
 * Executes a program without blocking until its completion.
 * @return a {@link ListenableFuture} object that completes when the program completes
 */
private ListenableFuture<?> execute(final Id.Program id,Map<String,String> sysArgs,Map<String,String> userArgs) throws IOException, TaskExecutionException {
  ProgramRuntimeService.RuntimeInfo runtimeInfo;
  try {
    runtimeInfo=lifecycleService.start(id,sysArgs,userArgs,false);
  }
 catch (  ProgramNotFoundException|ApplicationNotFoundException e) {
    throw new TaskExecutionException(UserMessages.getMessage(UserErrors.PROGRAM_NOT_FOUND),e,false);
  }
  final ProgramController controller=runtimeInfo.getController();
  final CountDownLatch latch=new CountDownLatch(1);
  controller.addListener(new AbstractListener(){
    @Override public void init(    ProgramController.State state,    @Nullable Throwable cause){
      if (state == ProgramController.State.COMPLETED) {
        completed();
      }
      if (state == ProgramController.State.ERROR) {
        error(controller.getFailureCause());
      }
    }
    @Override public void killed(){
      latch.countDown();
    }
    @Override public void completed(){
      latch.countDown();
    }
    @Override public void error(    Throwable cause){
      latch.countDown();
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  return executorService.submit(new Callable<Void>(){
    @Override public Void call() throws Exception {
      latch.await();
      return null;
    }
  }
);
}","/** 
 * Executes a program without blocking until its completion.
 * @return a {@link ListenableFuture} object that completes when the program completes
 */
private ListenableFuture<?> execute(final Id.Program id,Map<String,String> sysArgs,Map<String,String> userArgs) throws IOException, TaskExecutionException {
  ProgramRuntimeService.RuntimeInfo runtimeInfo;
  try {
    runtimeInfo=lifecycleService.start(id,sysArgs,userArgs,false);
  }
 catch (  ProgramNotFoundException|ApplicationNotFoundException e) {
    throw new TaskExecutionException(String.format(UserMessages.getMessage(UserErrors.PROGRAM_NOT_FOUND),id),e,false);
  }
  final ProgramController controller=runtimeInfo.getController();
  final CountDownLatch latch=new CountDownLatch(1);
  controller.addListener(new AbstractListener(){
    @Override public void init(    ProgramController.State state,    @Nullable Throwable cause){
      if (state == ProgramController.State.COMPLETED) {
        completed();
      }
      if (state == ProgramController.State.ERROR) {
        error(controller.getFailureCause());
      }
    }
    @Override public void killed(){
      latch.countDown();
    }
    @Override public void completed(){
      latch.countDown();
    }
    @Override public void error(    Throwable cause){
      latch.countDown();
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  return executorService.submit(new Callable<Void>(){
    @Override public Void call() throws Exception {
      latch.await();
      return null;
    }
  }
);
}"
6333,"private void doStop(ProgramRuntimeService.RuntimeInfo runtimeInfo) throws ExecutionException, InterruptedException {
  Preconditions.checkNotNull(runtimeInfo,UserMessages.getMessage(UserErrors.RUNTIME_INFO_NOT_FOUND));
  ProgramController controller=runtimeInfo.getController();
  controller.stop().get();
}","private void doStop(ProgramRuntimeService.RuntimeInfo runtimeInfo,Id.Program programId) throws ExecutionException, InterruptedException {
  Preconditions.checkNotNull(runtimeInfo,UserMessages.getMessage(UserErrors.RUNTIME_INFO_NOT_FOUND),programId);
  ProgramController controller=runtimeInfo.getController();
  controller.stop().get();
}"
6334,"private void stopProgramIfRunning(Id.Program programId) throws InterruptedException, ExecutionException {
  ProgramRuntimeService.RuntimeInfo programRunInfo=findRuntimeInfo(programId.getNamespaceId(),programId.getApplicationId(),programId.getId(),programId.getType(),runtimeService);
  if (programRunInfo != null) {
    doStop(programRunInfo);
  }
}","private void stopProgramIfRunning(Id.Program programId) throws InterruptedException, ExecutionException {
  ProgramRuntimeService.RuntimeInfo programRunInfo=findRuntimeInfo(programId,runtimeService);
  if (programRunInfo != null) {
    doStop(programRunInfo,programId);
  }
}"
6335,"/** 
 * Set instances of a service.
 */
@PUT @Path(""String_Node_Str"") public void setServiceInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId) throws ExecutionException, InterruptedException {
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.SERVICE,serviceId);
    if (!store.programExists(programId)) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
      return;
    }
    int instances;
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    int oldInstances=store.getServiceInstances(programId);
    if (oldInstances != instances) {
      store.setServiceInstances(programId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(namespaceId,appId,serviceId,ProgramType.SERVICE,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(serviceId,String.valueOf(instances))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable throwable) {
    if (respondIfElementNotFound(throwable,responder)) {
      return;
    }
    throw throwable;
  }
}","/** 
 * Set instances of a service.
 */
@PUT @Path(""String_Node_Str"") public void setServiceInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId) throws ExecutionException, InterruptedException {
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.SERVICE,serviceId);
    if (!store.programExists(programId)) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
      return;
    }
    int instances;
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    int oldInstances=store.getServiceInstances(programId);
    if (oldInstances != instances) {
      store.setServiceInstances(programId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(programId,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(serviceId,String.valueOf(instances))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable throwable) {
    if (respondIfElementNotFound(throwable,responder)) {
      return;
    }
    throw throwable;
  }
}"
6336,"/** 
 * Sets the number of instances of a worker.
 */
@PUT @Path(""String_Node_Str"") public void setWorkerInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workerId) throws ExecutionException, InterruptedException {
  int instances;
  try {
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
  }
 catch (  Throwable th) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.WORKER,workerId);
    int oldInstances=store.getWorkerInstances(programId);
    if (oldInstances != instances) {
      store.setWorkerInstances(programId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(namespaceId,appId,workerId,ProgramType.WORKER,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(programId.getId(),String.valueOf(instances))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    throw e;
  }
}","/** 
 * Sets the number of instances of a worker.
 */
@PUT @Path(""String_Node_Str"") public void setWorkerInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workerId) throws ExecutionException, InterruptedException {
  int instances;
  try {
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
  }
 catch (  Throwable th) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.WORKER,workerId);
    int oldInstances=store.getWorkerInstances(programId);
    if (oldInstances != instances) {
      store.setWorkerInstances(programId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(programId,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(programId.getId(),String.valueOf(instances))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    throw e;
  }
}"
6337,"/** 
 * Returns the program status.
 * @param id the id of the program for which the status call is made
 * @return the status of the program
 * @throws BadRequestException if the program type is invalid
 * @throws NotFoundException if the application to which this program belongs was not found
 */
private ProgramStatus getProgramStatus(Id.Program id) throws BadRequestException, NotFoundException {
  if (id.getType() == null) {
    throw new BadRequestException(String.format(""String_Node_Str"",id.getId()));
  }
  ApplicationSpecification appSpec=store.getApplication(id.getApplication());
  if (appSpec == null) {
    throw new NotFoundException(Id.Application.from(id.getNamespaceId(),id.getApplicationId()));
  }
  ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(id,null);
  if (runtimeInfo == null) {
    if (id.getType() != ProgramType.WEBAPP) {
      ProgramSpecification spec=getProgramSpecification(id);
      if (spec == null) {
        throw new NotFoundException(id);
      }
      if (id.getType() == ProgramType.MAPREDUCE && !store.getRuns(id,ProgramRunStatus.RUNNING,0,Long.MAX_VALUE,1).isEmpty()) {
        return ProgramStatus.RUNNING;
      }
      return ProgramStatus.STOPPED;
    }
    try {
      Location webappLoc=Programs.programLocation(namespacedLocationFactory,appFabricDir,id);
      if (webappLoc != null && webappLoc.exists()) {
        return ProgramStatus.STOPPED;
      }
      throw new NotFoundException(id);
    }
 catch (    IOException ioe) {
      throw new NotFoundException(id,ioe);
    }
  }
  return runtimeInfo.getController().getState().getProgramStatus();
}","/** 
 * Returns the program status.
 * @param id the id of the program for which the status call is made
 * @return the status of the program
 * @throws BadRequestException if the program type is invalid
 * @throws NotFoundException if the application to which this program belongs was not found
 */
private ProgramStatus getProgramStatus(Id.Program id) throws BadRequestException, NotFoundException {
  if (id.getType() == null) {
    throw new BadRequestException(String.format(""String_Node_Str"",id.getId()));
  }
  ApplicationSpecification appSpec=store.getApplication(id.getApplication());
  if (appSpec == null) {
    throw new NotFoundException(Id.Application.from(id.getNamespaceId(),id.getApplicationId()));
  }
  ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(id,(String)null);
  if (runtimeInfo == null) {
    if (id.getType() != ProgramType.WEBAPP) {
      ProgramSpecification spec=getProgramSpecification(id);
      if (spec == null) {
        throw new NotFoundException(id);
      }
      if (id.getType() == ProgramType.MAPREDUCE && !store.getRuns(id,ProgramRunStatus.RUNNING,0,Long.MAX_VALUE,1).isEmpty()) {
        return ProgramStatus.RUNNING;
      }
      return ProgramStatus.STOPPED;
    }
    try {
      Location webappLoc=Programs.programLocation(namespacedLocationFactory,appFabricDir,id);
      if (webappLoc != null && webappLoc.exists()) {
        return ProgramStatus.STOPPED;
      }
      throw new NotFoundException(id);
    }
 catch (    IOException ioe) {
      throw new NotFoundException(id,ioe);
    }
  }
  return runtimeInfo.getController().getState().getProgramStatus();
}"
6338,"/** 
 * Increases number of instance for a flowlet within a flow.
 */
@PUT @Path(""String_Node_Str"") public synchronized void setFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId,@PathParam(""String_Node_Str"") String flowletId) throws ExecutionException, InterruptedException {
  int instances;
  try {
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
  }
 catch (  Throwable th) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.FLOW,flowId);
    int oldInstances=store.getFlowletInstances(programId,flowletId);
    if (oldInstances != instances) {
      FlowSpecification flowSpec=store.setFlowletInstances(programId,flowletId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(namespaceId,appId,flowId,ProgramType.FLOW,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(""String_Node_Str"",flowletId,""String_Node_Str"",String.valueOf(instances),""String_Node_Str"",GSON.toJson(flowSpec,FlowSpecification.class))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    throw e;
  }
}","/** 
 * Increases number of instance for a flowlet within a flow.
 */
@PUT @Path(""String_Node_Str"") public synchronized void setFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId,@PathParam(""String_Node_Str"") String flowletId) throws ExecutionException, InterruptedException {
  int instances;
  try {
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
  }
 catch (  Throwable th) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.FLOW,flowId);
    int oldInstances=store.getFlowletInstances(programId,flowletId);
    if (oldInstances != instances) {
      FlowSpecification flowSpec=store.setFlowletInstances(programId,flowletId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(programId,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(""String_Node_Str"",flowletId,""String_Node_Str"",String.valueOf(instances),""String_Node_Str"",GSON.toJson(flowSpec,FlowSpecification.class))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    throw e;
  }
}"
6339,"protected ProgramRuntimeService.RuntimeInfo findRuntimeInfo(String namespaceId,String appId,String flowId,ProgramType type,ProgramRuntimeService runtimeService){
  Collection<ProgramRuntimeService.RuntimeInfo> runtimeInfos=runtimeService.list(type).values();
  Preconditions.checkNotNull(runtimeInfos,UserMessages.getMessage(UserErrors.RUNTIME_INFO_NOT_FOUND),namespaceId,flowId);
  Id.Program programId=Id.Program.from(namespaceId,appId,type,flowId);
  for (  ProgramRuntimeService.RuntimeInfo info : runtimeInfos) {
    if (programId.equals(info.getProgramId())) {
      return info;
    }
  }
  return null;
}","protected ProgramRuntimeService.RuntimeInfo findRuntimeInfo(Id.Program programId,ProgramRuntimeService runtimeService){
  Collection<ProgramRuntimeService.RuntimeInfo> runtimeInfos=runtimeService.list(programId.getType()).values();
  Preconditions.checkNotNull(runtimeInfos,UserMessages.getMessage(UserErrors.RUNTIME_INFO_NOT_FOUND),programId);
  for (  ProgramRuntimeService.RuntimeInfo info : runtimeInfos) {
    if (programId.equals(info.getProgramId())) {
      return info;
    }
  }
  return null;
}"
6340,"/** 
 * Checks if all schedule requirements are satisfied, then executes the given program without blocking until its completion.
 * @param programId Program Id
 * @param systemOverrides Arguments that would be supplied as system runtime arguments for the program.
 * @return a {@link ListenableFuture} object that completes when the program completes
 * @throws TaskExecutionException if program is already running or program is not found.
 * @throws IOException if program failed to start.
 */
public ListenableFuture<?> run(Id.Program programId,Map<String,String> systemOverrides) throws TaskExecutionException, IOException {
  Map<String,String> userArgs=Maps.newHashMap();
  Map<String,String> systemArgs=Maps.newHashMap();
  String scheduleName=systemOverrides.get(ProgramOptionConstants.SCHEDULE_NAME);
  ApplicationSpecification appSpec=store.getApplication(programId.getApplication());
  if (appSpec == null || appSpec.getSchedules().get(scheduleName) == null) {
    throw new TaskExecutionException(UserMessages.getMessage(UserErrors.PROGRAM_NOT_FOUND),false);
  }
  ScheduleSpecification spec=appSpec.getSchedules().get(scheduleName);
  if (!requirementsChecker.checkSatisfied(programId,spec.getSchedule())) {
    return Futures.<Void>immediateFuture(null);
  }
  userArgs.putAll(spec.getProperties());
  userArgs.putAll(propertiesResolver.getUserProperties(programId));
  systemArgs.putAll(propertiesResolver.getSystemProperties(programId));
  systemArgs.putAll(systemOverrides);
  return execute(programId,systemArgs,userArgs);
}","/** 
 * Checks if all schedule requirements are satisfied, then executes the given program without blocking until its completion.
 * @param programId Program Id
 * @param systemOverrides Arguments that would be supplied as system runtime arguments for the program.
 * @return a {@link ListenableFuture} object that completes when the program completes
 * @throws TaskExecutionException if program is already running or program is not found.
 * @throws IOException if program failed to start.
 */
public ListenableFuture<?> run(Id.Program programId,Map<String,String> systemOverrides) throws TaskExecutionException, IOException {
  Map<String,String> userArgs=Maps.newHashMap();
  Map<String,String> systemArgs=Maps.newHashMap();
  String scheduleName=systemOverrides.get(ProgramOptionConstants.SCHEDULE_NAME);
  ApplicationSpecification appSpec=store.getApplication(programId.getApplication());
  if (appSpec == null || appSpec.getSchedules().get(scheduleName) == null) {
    throw new TaskExecutionException(String.format(UserMessages.getMessage(UserErrors.PROGRAM_NOT_FOUND),programId),false);
  }
  ScheduleSpecification spec=appSpec.getSchedules().get(scheduleName);
  if (!requirementsChecker.checkSatisfied(programId,spec.getSchedule())) {
    return Futures.<Void>immediateFuture(null);
  }
  userArgs.putAll(spec.getProperties());
  userArgs.putAll(propertiesResolver.getUserProperties(programId));
  systemArgs.putAll(propertiesResolver.getSystemProperties(programId));
  systemArgs.putAll(systemOverrides);
  return execute(programId,systemArgs,userArgs);
}"
6341,"/** 
 * Executes a program without blocking until its completion.
 * @return a {@link ListenableFuture} object that completes when the program completes
 */
private ListenableFuture<?> execute(final Id.Program id,Map<String,String> sysArgs,Map<String,String> userArgs) throws IOException, TaskExecutionException {
  ProgramRuntimeService.RuntimeInfo runtimeInfo;
  try {
    runtimeInfo=lifecycleService.start(id,sysArgs,userArgs,false);
  }
 catch (  ProgramNotFoundException|ApplicationNotFoundException e) {
    throw new TaskExecutionException(UserMessages.getMessage(UserErrors.PROGRAM_NOT_FOUND),e,false);
  }
  final ProgramController controller=runtimeInfo.getController();
  final CountDownLatch latch=new CountDownLatch(1);
  controller.addListener(new AbstractListener(){
    @Override public void init(    ProgramController.State state,    @Nullable Throwable cause){
      if (state == ProgramController.State.COMPLETED) {
        completed();
      }
      if (state == ProgramController.State.ERROR) {
        error(controller.getFailureCause());
      }
    }
    @Override public void killed(){
      latch.countDown();
    }
    @Override public void completed(){
      latch.countDown();
    }
    @Override public void error(    Throwable cause){
      latch.countDown();
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  return executorService.submit(new Callable<Void>(){
    @Override public Void call() throws Exception {
      latch.await();
      return null;
    }
  }
);
}","/** 
 * Executes a program without blocking until its completion.
 * @return a {@link ListenableFuture} object that completes when the program completes
 */
private ListenableFuture<?> execute(final Id.Program id,Map<String,String> sysArgs,Map<String,String> userArgs) throws IOException, TaskExecutionException {
  ProgramRuntimeService.RuntimeInfo runtimeInfo;
  try {
    runtimeInfo=lifecycleService.start(id,sysArgs,userArgs,false);
  }
 catch (  ProgramNotFoundException|ApplicationNotFoundException e) {
    throw new TaskExecutionException(String.format(UserMessages.getMessage(UserErrors.PROGRAM_NOT_FOUND),id),e,false);
  }
  final ProgramController controller=runtimeInfo.getController();
  final CountDownLatch latch=new CountDownLatch(1);
  controller.addListener(new AbstractListener(){
    @Override public void init(    ProgramController.State state,    @Nullable Throwable cause){
      if (state == ProgramController.State.COMPLETED) {
        completed();
      }
      if (state == ProgramController.State.ERROR) {
        error(controller.getFailureCause());
      }
    }
    @Override public void killed(){
      latch.countDown();
    }
    @Override public void completed(){
      latch.countDown();
    }
    @Override public void error(    Throwable cause){
      latch.countDown();
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  return executorService.submit(new Callable<Void>(){
    @Override public Void call() throws Exception {
      latch.await();
      return null;
    }
  }
);
}"
6342,"/** 
 * Deletes a dataset instance, which also deletes the data owned by it.
 * @param namespaceId namespace of the dataset instance
 * @param name name of the dataset instance
 * @throws Exception
 */
@DELETE @Path(""String_Node_Str"") public void drop(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String name) throws Exception {
  LOG.info(""String_Node_Str"",namespaceId,name);
  Id.DatasetInstance instance=Id.DatasetInstance.from(namespaceId,name);
  instanceService.drop(instance);
  responder.sendStatus(HttpResponseStatus.OK);
}","/** 
 * Deletes a dataset instance, which also deletes the data owned by it.
 * @param namespaceId namespace of the dataset instance
 * @param name name of the dataset instance
 * @throws Exception
 */
@DELETE @Path(""String_Node_Str"") public void drop(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String name) throws Exception {
  Id.DatasetInstance instance=ConversionHelpers.toDatasetInstanceId(namespaceId,name);
  instanceService.drop(instance);
  responder.sendStatus(HttpResponseStatus.OK);
}"
6343,"/** 
 * Updates an existing dataset specification properties.
 * @param namespaceId namespace of the dataset instance
 * @param name name of the dataset instance
 * @throws Exception
 */
@PUT @Path(""String_Node_Str"") public void update(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String name) throws Exception {
  Id.DatasetInstance instance=Id.DatasetInstance.from(namespaceId,name);
  Map<String,String> properties=getProperties(request);
  LOG.info(""String_Node_Str"",name,GSON.toJson(properties));
  instanceService.update(instance,properties);
  responder.sendStatus(HttpResponseStatus.OK);
}","/** 
 * Updates an existing dataset specification properties.
 * @param namespaceId namespace of the dataset instance
 * @param name name of the dataset instance
 * @throws Exception
 */
@PUT @Path(""String_Node_Str"") public void update(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String name) throws Exception {
  Id.DatasetInstance instance=ConversionHelpers.toDatasetInstanceId(namespaceId,name);
  Map<String,String> properties=ConversionHelpers.getProperties(request);
  instanceService.update(instance,properties);
  responder.sendStatus(HttpResponseStatus.OK);
}"
6344,"/** 
 * Executes an admin operation on a dataset instance.
 * @param namespaceId namespace of the dataset instance
 * @param name name of the dataset instance
 * @param method the admin operation to execute (e.g. ""exists"", ""truncate"", ""upgrade"")
 * @throws Exception
 */
@POST @Path(""String_Node_Str"") public void executeAdmin(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String name,@PathParam(""String_Node_Str"") String method) throws Exception {
  Id.DatasetInstance instance=Id.DatasetInstance.from(namespaceId,name);
  try {
    DatasetAdminOpResponse response=instanceService.executeAdmin(instance,method);
    responder.sendJson(HttpResponseStatus.OK,response);
  }
 catch (  HandlerException e) {
    responder.sendStatus(e.getFailureStatus());
  }
}","/** 
 * Executes an admin operation on a dataset instance.
 * @param namespaceId namespace of the dataset instance
 * @param name name of the dataset instance
 * @param method the admin operation to execute (e.g. ""exists"", ""truncate"", ""upgrade"")
 * @throws Exception
 */
@POST @Path(""String_Node_Str"") public void executeAdmin(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String name,@PathParam(""String_Node_Str"") String method) throws Exception {
  Id.DatasetInstance instance=ConversionHelpers.toDatasetInstanceId(namespaceId,name);
  try {
    DatasetAdminOpResponse response=instanceService.executeAdmin(instance,method);
    responder.sendJson(HttpResponseStatus.OK,response);
  }
 catch (  HandlerException e) {
    responder.sendStatus(e.getFailureStatus());
  }
}"
6345,"@GET @Path(""String_Node_Str"") public void list(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,spec2Summary(instanceService.list(Id.Namespace.from(namespaceId))));
}","@GET @Path(""String_Node_Str"") public void list(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,ConversionHelpers.spec2Summary(instanceService.list(ConversionHelpers.toNamespaceId(namespaceId))));
}"
6346,"/** 
 * Gets the   {@link DatasetMeta} for a dataset instance.
 * @param namespaceId namespace of the dataset instance
 * @param name name of the dataset instance
 * @param owners a list of owners of the dataset instance, in the form @{code <type>::<id>}(e.g. ""program::namespace:default/application:PurchaseHistory/program:flow:PurchaseFlow"")
 * @throws Exception if the dataset instance was not found
 */
@GET @Path(""String_Node_Str"") public void get(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String name,@QueryParam(""String_Node_Str"") List<String> owners) throws Exception {
  Id.DatasetInstance instance=Id.DatasetInstance.from(namespaceId,name);
  responder.sendJson(HttpResponseStatus.OK,instanceService.get(instance,strings2Ids(owners)),DatasetMeta.class,GSON);
}","/** 
 * Gets the   {@link DatasetMeta} for a dataset instance.
 * @param namespaceId namespace of the dataset instance
 * @param name name of the dataset instance
 * @param owners a list of owners of the dataset instance, in the form @{code <type>::<id>}(e.g. ""program::namespace:default/application:PurchaseHistory/program:flow:PurchaseFlow"")
 * @throws Exception if the dataset instance was not found
 */
@GET @Path(""String_Node_Str"") public void get(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String name,@QueryParam(""String_Node_Str"") List<String> owners) throws Exception {
  responder.sendJson(HttpResponseStatus.OK,instanceService.get(ConversionHelpers.toDatasetInstanceId(namespaceId,name),ConversionHelpers.strings2ProgramIds(owners)),DatasetMeta.class);
}"
6347,"/** 
 * Creates a new dataset instance.
 * @param namespaceId namespace of the new dataset instance
 * @param name name of the new dataset instance
 */
@PUT @Path(""String_Node_Str"") public void create(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String name) throws Exception {
  DatasetInstanceConfiguration creationProperties=getInstanceConfiguration(request);
  Id.Namespace namespace=Id.Namespace.from(namespaceId);
  LOG.info(""String_Node_Str"",namespaceId,name,creationProperties.getTypeName(),creationProperties.getProperties());
  try {
    instanceService.create(namespace,name,creationProperties);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  DatasetAlreadyExistsException e) {
    responder.sendString(HttpResponseStatus.CONFLICT,e.getMessage());
  }
catch (  DatasetTypeNotFoundException e) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
  }
catch (  HandlerException e) {
    responder.sendString(e.getFailureStatus(),e.getMessage());
  }
}","/** 
 * Creates a new dataset instance.
 * @param namespaceId namespace of the new dataset instance
 * @param name name of the new dataset instance
 */
@PUT @Path(""String_Node_Str"") public void create(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String name) throws Exception {
  DatasetInstanceConfiguration creationProperties=ConversionHelpers.getInstanceConfiguration(request);
  try {
    instanceService.create(namespaceId,name,creationProperties);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  DatasetAlreadyExistsException e) {
    responder.sendString(HttpResponseStatus.CONFLICT,e.getMessage());
  }
catch (  DatasetTypeNotFoundException e) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
  }
catch (  HandlerException e) {
    responder.sendString(e.getFailureStatus(),e.getMessage());
  }
}"
6348,"@Test public void testThatDatasetsStayInTransaction() throws TransactionFailureException {
  final AtomicInteger hash=new AtomicInteger();
  Transactions.execute(cache.newTransactionContext(),""String_Node_Str"",new Runnable(){
    @Override public void run(){
      try {
        TestDataset ds=cache.getDataset(""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
        ds.write();
        hash.set(System.identityHashCode(ds));
        cache.discardDataset(ds);
        ds=cache.getDataset(""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
        Assert.assertEquals(hash.get(),System.identityHashCode(ds));
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
      try {
        TestDataset ds=cache.getDataset(""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
        Assert.assertEquals(hash.get(),System.identityHashCode(ds));
        cache.discardDataset(ds);
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
  Transactions.execute(cache.newTransactionContext(),""String_Node_Str"",new Runnable(){
    @Override public void run(){
      try {
        TestDataset ds=cache.getDataset(""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
        Assert.assertEquals(""String_Node_Str"",ds.read());
        Assert.assertNotEquals(hash.get(),System.identityHashCode(ds));
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
      Assert.assertEquals(1,Iterables.size(cache.getTransactionAwares()) - Iterables.size(cache.getStaticTransactionAwares()));
    }
  }
);
}","@Test public void testThatDatasetsStayInTransaction() throws TransactionFailureException {
  final AtomicReference<Object> ref=new AtomicReference<>();
  Transactions.execute(cache.newTransactionContext(),""String_Node_Str"",new Runnable(){
    @Override public void run(){
      try {
        TestDataset ds=cache.getDataset(""String_Node_Str"",X_ARGUMENTS);
        ds.write();
        cache.discardDataset(ds);
        TestDataset ds2=cache.getDataset(""String_Node_Str"",X_ARGUMENTS);
        Assert.assertSame(ds,ds2);
        ref.set(ds);
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
      try {
        TestDataset ds=cache.getDataset(""String_Node_Str"",X_ARGUMENTS);
        Assert.assertSame(ref.get(),ds);
        cache.discardDataset(ds);
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
  Transactions.execute(cache.newTransactionContext(),""String_Node_Str"",new Runnable(){
    @Override public void run(){
      try {
        TestDataset ds=cache.getDataset(""String_Node_Str"",X_ARGUMENTS);
        Assert.assertEquals(""String_Node_Str"",ds.read());
        Assert.assertNotSame(ref.get(),ds);
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
      Assert.assertEquals(1,Iterables.size(cache.getTransactionAwares()) - Iterables.size(cache.getStaticTransactionAwares()));
    }
  }
);
}"
6349,"@Override public void run(){
  try {
    TestDataset ds=cache.getDataset(""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
    Assert.assertEquals(""String_Node_Str"",ds.read());
    Assert.assertNotEquals(hash.get(),System.identityHashCode(ds));
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
  Assert.assertEquals(1,Iterables.size(cache.getTransactionAwares()) - Iterables.size(cache.getStaticTransactionAwares()));
}","@Override public void run(){
  try {
    TestDataset ds=cache.getDataset(""String_Node_Str"",X_ARGUMENTS);
    Assert.assertEquals(""String_Node_Str"",ds.read());
    Assert.assertNotSame(ref.get(),ds);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
  Assert.assertEquals(1,Iterables.size(cache.getTransactionAwares()) - Iterables.size(cache.getStaticTransactionAwares()));
}"
6350,"/** 
 * @param datasetMap if not null, this test method will save the instances of a and b into the map.
 */
protected void testDatasetCache(@Nullable Map<String,TestDataset> datasetMap) throws IOException, DatasetManagementException, TransactionFailureException {
  TestDataset a=cache.getDataset(""String_Node_Str"");
  TestDataset a1=cache.getDataset(""String_Node_Str"");
  TestDataset a2=cache.getDataset(""String_Node_Str"",A_ARGUMENTS);
  Assert.assertSame(a,a1);
  Assert.assertSame(a,a2);
  TestDataset b=cache.getDataset(""String_Node_Str"",B_ARGUMENTS);
  TestDataset b1=cache.getDataset(""String_Node_Str"",B_ARGUMENTS);
  Assert.assertSame(b,b1);
  Assert.assertEquals(2,a.getArguments().size());
  Assert.assertEquals(""String_Node_Str"",a.getKey());
  Assert.assertEquals(""String_Node_Str"",a.getValue());
  Assert.assertEquals(""String_Node_Str"",b.getKey());
  Assert.assertEquals(""String_Node_Str"",b.getValue());
  List<TestDataset> txAwares=getTxAwares();
  Assert.assertTrue(txAwares.isEmpty());
  TransactionContext txContext=cache.newTransactionContext();
  txAwares=getTxAwares();
  Assert.assertEquals(2,txAwares.size());
  Assert.assertSame(a,txAwares.get(0));
  Assert.assertSame(b,txAwares.get(1));
  txContext.start();
  String sysPropA=System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  Assert.assertNotNull(sysPropA);
  String sysPropB=System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  Assert.assertNotNull(sysPropB);
  Assert.assertNotEquals(0L,Long.parseLong(sysPropA));
  Assert.assertEquals(sysPropA,sysPropB);
  TestDataset c=cache.getDataset(""String_Node_Str"",C_ARGUMENTS);
  String sysPropC=System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  Assert.assertNotNull(sysPropC);
  Assert.assertEquals(sysPropA,sysPropC);
  Assert.assertEquals(""String_Node_Str"",c.getKey());
  Assert.assertEquals(""String_Node_Str"",c.getValue());
  txAwares=getTxAwares();
  Assert.assertEquals(3,txAwares.size());
  Assert.assertSame(a,txAwares.get(0));
  Assert.assertSame(b,txAwares.get(1));
  Assert.assertSame(c,txAwares.get(2));
  cache.discardDataset(b);
  cache.discardDataset(c);
  Assert.assertNull(System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  Assert.assertNull(System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  txAwares=getTxAwares();
  Assert.assertEquals(2,txAwares.size());
  Assert.assertSame(a,txAwares.get(0));
  Assert.assertSame(b,txAwares.get(1));
  TestDataset b3=cache.getDataset(""String_Node_Str"",B_ARGUMENTS);
  TestDataset c1=cache.getDataset(""String_Node_Str"",C_ARGUMENTS);
  Assert.assertSame(b3,b);
  Assert.assertSame(c1,c);
  txAwares=getTxAwares();
  Assert.assertEquals(3,txAwares.size());
  Assert.assertSame(a,txAwares.get(0));
  Assert.assertSame(b,txAwares.get(1));
  Assert.assertSame(c,txAwares.get(2));
  cache.discardDataset(b);
  cache.discardDataset(c);
  Assert.assertNull(System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  Assert.assertNull(System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  txAwares=getTxAwares();
  Assert.assertEquals(2,txAwares.size());
  Assert.assertSame(a,txAwares.get(0));
  Assert.assertSame(b,txAwares.get(1));
  txContext.abort();
  Assert.assertNull(System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  Assert.assertNull(System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  Assert.assertNull(System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  txAwares=getTxAwares();
  Assert.assertEquals(2,txAwares.size());
  Assert.assertSame(a,txAwares.get(0));
  Assert.assertSame(b,txAwares.get(1));
  Assert.assertNotNull(System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  Assert.assertNull(System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  txContext.start();
  Assert.assertNotNull(System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  Assert.assertNotNull(System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  Assert.assertNull(System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  cache.discardDataset(c);
  c1=cache.getDataset(""String_Node_Str"",C_ARGUMENTS);
  Assert.assertNotSame(c,c1);
  cache.discardDataset(c1);
  txContext.finish();
  cache.dismissTransactionContext();
  Assert.assertTrue(getTxAwares().isEmpty());
  txContext=cache.newTransactionContext();
  txContext.start();
  Assert.assertNotNull(txContext.getCurrentTransaction());
  String currentTx=Long.toString(txContext.getCurrentTransaction().getWritePointer());
  Assert.assertEquals(currentTx,System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  Assert.assertEquals(currentTx,System.getProperty(TestDataset.generateSystemProperty(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"")));
  txContext.abort();
  if (datasetMap != null) {
    datasetMap.put(""String_Node_Str"",a);
    datasetMap.put(""String_Node_Str"",b);
  }
}","/** 
 * @param datasetMap if not null, this test method will save the instances of a and b into the map.
 */
protected void testDatasetCache(@Nullable Map<String,TestDataset> datasetMap) throws IOException, DatasetManagementException, TransactionFailureException {
  TestDataset a=cache.getDataset(""String_Node_Str"");
  TestDataset a1=cache.getDataset(""String_Node_Str"");
  TestDataset a2=cache.getDataset(""String_Node_Str"",A_ARGUMENTS);
  Assert.assertSame(a,a1);
  Assert.assertSame(a,a2);
  TestDataset b=cache.getDataset(""String_Node_Str"",B_ARGUMENTS);
  TestDataset b1=cache.getDataset(""String_Node_Str"",B_ARGUMENTS);
  Assert.assertSame(b,b1);
  Assert.assertEquals(2,a.getArguments().size());
  Assert.assertEquals(""String_Node_Str"",a.getKey());
  Assert.assertEquals(""String_Node_Str"",a.getValue());
  Assert.assertEquals(""String_Node_Str"",b.getKey());
  Assert.assertEquals(""String_Node_Str"",b.getValue());
  List<TestDataset> txAwares=getTxAwares();
  Assert.assertTrue(txAwares.isEmpty());
  TransactionContext txContext=cache.newTransactionContext();
  txAwares=getTxAwares();
  Assert.assertEquals(2,txAwares.size());
  Assert.assertSame(a,txAwares.get(0));
  Assert.assertSame(b,txAwares.get(1));
  txContext.start();
  Assert.assertNotNull(a.getCurrentTransaction());
  Assert.assertNotEquals(0L,a.getCurrentTransaction().getWritePointer());
  Assert.assertEquals(a.getCurrentTransaction(),b.getCurrentTransaction());
  TestDataset c=cache.getDataset(""String_Node_Str"",C_ARGUMENTS);
  Assert.assertEquals(a.getCurrentTransaction(),c.getCurrentTransaction());
  Assert.assertEquals(""String_Node_Str"",c.getKey());
  Assert.assertEquals(""String_Node_Str"",c.getValue());
  txAwares=getTxAwares();
  Assert.assertEquals(3,txAwares.size());
  Assert.assertSame(a,txAwares.get(0));
  Assert.assertSame(b,txAwares.get(1));
  Assert.assertSame(c,txAwares.get(2));
  cache.discardDataset(b);
  cache.discardDataset(c);
  Assert.assertFalse(b.isClosed());
  Assert.assertFalse(c.isClosed());
  txAwares=getTxAwares();
  Assert.assertEquals(2,txAwares.size());
  Assert.assertSame(a,txAwares.get(0));
  Assert.assertSame(b,txAwares.get(1));
  TestDataset b3=cache.getDataset(""String_Node_Str"",B_ARGUMENTS);
  TestDataset c1=cache.getDataset(""String_Node_Str"",C_ARGUMENTS);
  Assert.assertSame(b3,b);
  Assert.assertSame(c1,c);
  txAwares=getTxAwares();
  Assert.assertEquals(3,txAwares.size());
  Assert.assertSame(a,txAwares.get(0));
  Assert.assertSame(b,txAwares.get(1));
  Assert.assertSame(c,txAwares.get(2));
  cache.discardDataset(b);
  cache.discardDataset(c);
  Assert.assertFalse(b.isClosed());
  Assert.assertFalse(c.isClosed());
  txAwares=getTxAwares();
  Assert.assertEquals(2,txAwares.size());
  Assert.assertSame(a,txAwares.get(0));
  Assert.assertSame(b,txAwares.get(1));
  txContext.abort();
  Assert.assertNull(a.getCurrentTransaction());
  Assert.assertNull(b.getCurrentTransaction());
  Assert.assertNull(c.getCurrentTransaction());
  txAwares=getTxAwares();
  Assert.assertEquals(2,txAwares.size());
  Assert.assertSame(a,txAwares.get(0));
  Assert.assertSame(b,txAwares.get(1));
  Assert.assertTrue(c.isClosed());
  Assert.assertFalse(b.isClosed());
  txContext.start();
  Assert.assertNotNull(a.getCurrentTransaction());
  Assert.assertEquals(a.getCurrentTransaction(),b.getCurrentTransaction());
  Assert.assertNull(c.getCurrentTransaction());
  cache.discardDataset(c);
  c1=cache.getDataset(""String_Node_Str"",C_ARGUMENTS);
  Assert.assertNotSame(c,c1);
  cache.discardDataset(c1);
  txContext.finish();
  cache.dismissTransactionContext();
  Assert.assertTrue(getTxAwares().isEmpty());
  txContext=cache.newTransactionContext();
  txContext.start();
  Assert.assertNotNull(txContext.getCurrentTransaction());
  Assert.assertEquals(txContext.getCurrentTransaction(),a.getCurrentTransaction());
  Assert.assertEquals(txContext.getCurrentTransaction(),b.getCurrentTransaction());
  txContext.abort();
  if (datasetMap != null) {
    datasetMap.put(""String_Node_Str"",a);
    datasetMap.put(""String_Node_Str"",b);
  }
}"
6351,"private Thread createThread(final Map<String,TestDataset> datasetMap){
  return new Thread(){
    @Override public void run(){
      try {
        testDatasetCache(datasetMap);
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
    }
  }
;
}","private Thread createThread(final Map<String,TestDataset> datasetMap,final AtomicReference<Throwable> ref){
  return new Thread(){
    @Override public void run(){
      try {
        testDatasetCache(datasetMap);
      }
 catch (      Throwable e) {
        ref.set(e);
      }
    }
  }
;
}"
6352,"@Override public void run(){
  try {
    testDatasetCache(datasetMap);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","@Override public void run(){
  try {
    testDatasetCache(datasetMap);
  }
 catch (  Throwable e) {
    ref.set(e);
  }
}"
6353,"@Test() public void testDatasetCache() throws Exception {
  Map<String,TestDataset> thread1map=new HashMap<>();
  Map<String,TestDataset> thread2map=new HashMap<>();
  Thread thread1=createThread(thread1map);
  Thread thread2=createThread(thread2map);
  thread1.start();
  thread2.start();
  thread1.join();
  thread2.join();
  Assert.assertNotSame(thread1map.get(""String_Node_Str""),thread2map.get(""String_Node_Str""));
  Assert.assertNotSame(thread1map.get(""String_Node_Str""),thread2map.get(""String_Node_Str""));
  thread1=thread2=null;
  thread1map=thread2map=null;
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      System.gc();
      return ((MultiThreadDatasetCache)cache).getCacheKeys().isEmpty();
    }
  }
,5,TimeUnit.SECONDS,100,TimeUnit.MILLISECONDS);
}","@Test public void testDatasetCache() throws Throwable {
  for (int i=0; i < 25; i++) {
    testDatasetCacheOnce();
  }
}"
6354,"@Override public boolean commitTx() throws Exception {
  clearSystemProperty(""String_Node_Str"");
  return super.commitTx();
}","@Override public boolean commitTx() throws Exception {
  currentTx=null;
  return super.commitTx();
}"
6355,"@Override public boolean rollbackTx() throws Exception {
  clearSystemProperty(""String_Node_Str"");
  return super.rollbackTx();
}","@Override public boolean rollbackTx() throws Exception {
  currentTx=null;
  return super.rollbackTx();
}"
6356,"@Override public void startTx(Transaction tx){
  setSystemProperty(""String_Node_Str"",Long.toString(tx.getWritePointer()));
  super.startTx(tx);
}","@Override public void startTx(Transaction tx){
  currentTx=tx;
  super.startTx(tx);
}"
6357,"public TestDataset(DatasetSpecification spec,KeyValueTable kv,Map<String,String> args){
  super(spec.getName(),kv);
  this.kvTable=kv;
  this.arguments=ImmutableSortedMap.copyOf(args);
  this.key=arguments.get(""String_Node_Str"") == null ? ""String_Node_Str"" : arguments.get(""String_Node_Str"");
  this.value=arguments.get(""String_Node_Str"") == null ? ""String_Node_Str"" : arguments.get(""String_Node_Str"");
  clearSystemProperty(""String_Node_Str"");
  clearSystemProperty(""String_Node_Str"");
}","public TestDataset(DatasetSpecification spec,KeyValueTable kv,Map<String,String> args){
  super(spec.getName(),kv);
  this.kvTable=kv;
  this.arguments=ImmutableSortedMap.copyOf(args);
  this.key=arguments.get(""String_Node_Str"") == null ? ""String_Node_Str"" : arguments.get(""String_Node_Str"");
  this.value=arguments.get(""String_Node_Str"") == null ? ""String_Node_Str"" : arguments.get(""String_Node_Str"");
}"
6358,"@Override public void close(){
  setSystemProperty(""String_Node_Str"",Integer.toString(System.identityHashCode(this)));
}","@Override public void close(){
  isClosed=true;
}"
6359,"private void startHBase() throws Exception {
  getConfiguration().setInt(""String_Node_Str"",5);
  getConfiguration().setInt(""String_Node_Str"",10);
  getConfiguration().setInt(""String_Node_Str"",10);
  getConfiguration().setInt(""String_Node_Str"",Networks.getRandomPort());
  getConfiguration().setInt(""String_Node_Str"",Networks.getRandomPort());
  getConfiguration().setInt(""String_Node_Str"",Networks.getRandomPort());
  getConfiguration().setInt(""String_Node_Str"",Networks.getRandomPort());
  doStartHBase();
}","private void startHBase() throws Exception {
  getConfiguration().setInt(""String_Node_Str"",5);
  getConfiguration().setInt(""String_Node_Str"",10);
  getConfiguration().setInt(""String_Node_Str"",10);
  getConfiguration().setInt(""String_Node_Str"",0);
  getConfiguration().setInt(""String_Node_Str"",0);
  getConfiguration().setInt(""String_Node_Str"",0);
  getConfiguration().setInt(""String_Node_Str"",0);
  doStartHBase();
}"
6360,"public static MDSKey getMDSKey(Id.NamespacedId targetId,MetadataDataset.MetadataType type,@Nullable String key){
  String targetType=KeyHelper.getTargetType(targetId);
  MDSKey.Builder builder=new MDSKey.Builder();
  builder.add(ROW_PREFIX);
  builder.add(targetType);
  KeyHelper.addNamespaceIdToKey(builder,targetId);
  builder.add(type.toString());
  if (key != null) {
    builder.add(key);
  }
  return builder.build();
}","public static MDSKey getMDSKey(Id.NamespacedId targetId,@Nullable MetadataDataset.MetadataType type,@Nullable String key){
  String targetType=KeyHelper.getTargetType(targetId);
  MDSKey.Builder builder=new MDSKey.Builder();
  builder.add(ROW_PREFIX);
  builder.add(targetType);
  KeyHelper.addNamespaceIdToKey(builder,targetId);
  if (type != null) {
    builder.add(type.toString());
  }
  if (key != null) {
    builder.add(key);
  }
  return builder.build();
}"
6361,"public static Id.NamespacedId getNamespaceIdFromKey(String type,MDSKey key){
  MDSKey.Splitter keySplitter=key.split();
  keySplitter.skipBytes();
  keySplitter.skipString();
  return KeyHelper.getNamespaceIdFromKey(keySplitter,type);
}","public static Id.NamespacedId getNamespaceIdFromKey(String type,byte[] rowKey){
  MDSKey.Splitter keySplitter=new MDSKey(rowKey).split();
  keySplitter.skipBytes();
  keySplitter.skipString();
  return KeyHelper.getNamespaceIdFromKey(keySplitter,type);
}"
6362,"private List<MetadataEntry> executeSearch(final String namespaceId,final String column,final String searchValue,final MetadataSearchTargetType type){
  List<MetadataEntry> results=new LinkedList<>();
  Scanner scanner;
  String namespacedSearchValue=namespaceId + KEYVALUE_SEPARATOR + searchValue.toLowerCase();
  boolean modifiedTagsKVSearch=false;
  final String unmodifiedTagKeyValue=searchValue.toLowerCase();
  if (KEYVALUE_COLUMN.equals(column) && unmodifiedTagKeyValue.startsWith(TAGS_KEY + KEYVALUE_SEPARATOR)) {
    namespacedSearchValue=namespacedSearchValue.substring(0,namespacedSearchValue.lastIndexOf(KEYVALUE_SEPARATOR) + 1) + ""String_Node_Str"";
    modifiedTagsKVSearch=true;
  }
  if (namespacedSearchValue.endsWith(""String_Node_Str"")) {
    byte[] startKey=Bytes.toBytes(namespacedSearchValue.substring(0,namespacedSearchValue.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    scanner=indexedTable.scanByIndex(Bytes.toBytes(column),startKey,stopKey);
  }
 else {
    byte[] value=Bytes.toBytes(namespacedSearchValue);
    scanner=indexedTable.readByIndex(Bytes.toBytes(column),value);
  }
  try {
    Row next;
    while ((next=scanner.next()) != null) {
      String rowValue=next.getString(VALUE_COLUMN);
      if (rowValue == null) {
        continue;
      }
      final byte[] rowKey=next.getRow();
      String targetType=MdsValueKey.getTargetType(rowKey);
      if ((type != MetadataSearchTargetType.ALL) && (!targetType.equals(type.getInternalName()))) {
        continue;
      }
      if (modifiedTagsKVSearch) {
        boolean isMatch=false;
        if ((TAGS_KEY + KEYVALUE_SEPARATOR + ""String_Node_Str"").equals(unmodifiedTagKeyValue)) {
          isMatch=true;
        }
 else {
          Iterable<String> tagValues=Splitter.on(TAGS_SEPARATOR).omitEmptyStrings().trimResults().split(rowValue);
          for (          String tagValue : tagValues) {
            String prefixedTagValue=TAGS_KEY + KEYVALUE_SEPARATOR + tagValue;
            if (!unmodifiedTagKeyValue.endsWith(""String_Node_Str"")) {
              if (prefixedTagValue.equals(unmodifiedTagKeyValue)) {
                isMatch=true;
                break;
              }
            }
 else {
              int endAsteriskIndex=unmodifiedTagKeyValue.lastIndexOf(""String_Node_Str"");
              if (prefixedTagValue.startsWith(unmodifiedTagKeyValue.substring(0,endAsteriskIndex))) {
                isMatch=true;
                break;
              }
            }
          }
        }
        if (!isMatch) {
          continue;
        }
      }
      Id.NamespacedId targetId=MdsValueKey.getNamespaceIdFromKey(targetType,new MDSKey(rowKey));
      String key=MdsValueKey.getMetadataKey(targetType,rowKey);
      String value=Bytes.toString(next.get(Bytes.toBytes(MetadataDataset.VALUE_COLUMN)));
      MetadataEntry entry=new MetadataEntry(targetId,key,value);
      results.add(entry);
    }
  }
  finally {
    scanner.close();
  }
  return results;
}","private List<MetadataEntry> executeSearch(final String namespaceId,final String column,final String searchValue,final MetadataSearchTargetType type){
  List<MetadataEntry> results=new LinkedList<>();
  Scanner scanner;
  String namespacedSearchValue=namespaceId + KEYVALUE_SEPARATOR + searchValue.toLowerCase();
  boolean modifiedTagsKVSearch=false;
  final String unmodifiedTagKeyValue=searchValue.toLowerCase();
  if (KEYVALUE_COLUMN.equals(column) && unmodifiedTagKeyValue.startsWith(TAGS_KEY + KEYVALUE_SEPARATOR)) {
    namespacedSearchValue=namespacedSearchValue.substring(0,namespacedSearchValue.lastIndexOf(KEYVALUE_SEPARATOR) + 1) + ""String_Node_Str"";
    modifiedTagsKVSearch=true;
  }
  if (namespacedSearchValue.endsWith(""String_Node_Str"")) {
    byte[] startKey=Bytes.toBytes(namespacedSearchValue.substring(0,namespacedSearchValue.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    scanner=indexedTable.scanByIndex(Bytes.toBytes(column),startKey,stopKey);
  }
 else {
    byte[] value=Bytes.toBytes(namespacedSearchValue);
    scanner=indexedTable.readByIndex(Bytes.toBytes(column),value);
  }
  try {
    Row next;
    while ((next=scanner.next()) != null) {
      String rowValue=next.getString(VALUE_COLUMN);
      if (rowValue == null) {
        continue;
      }
      final byte[] rowKey=next.getRow();
      String targetType=MdsValueKey.getTargetType(rowKey);
      if ((type != MetadataSearchTargetType.ALL) && (!targetType.equals(type.getInternalName()))) {
        continue;
      }
      if (modifiedTagsKVSearch) {
        boolean isMatch=false;
        if ((TAGS_KEY + KEYVALUE_SEPARATOR + ""String_Node_Str"").equals(unmodifiedTagKeyValue)) {
          isMatch=true;
        }
 else {
          Iterable<String> tagValues=Splitter.on(TAGS_SEPARATOR).omitEmptyStrings().trimResults().split(rowValue);
          for (          String tagValue : tagValues) {
            String prefixedTagValue=TAGS_KEY + KEYVALUE_SEPARATOR + tagValue;
            if (!unmodifiedTagKeyValue.endsWith(""String_Node_Str"")) {
              if (prefixedTagValue.equals(unmodifiedTagKeyValue)) {
                isMatch=true;
                break;
              }
            }
 else {
              int endAsteriskIndex=unmodifiedTagKeyValue.lastIndexOf(""String_Node_Str"");
              if (prefixedTagValue.startsWith(unmodifiedTagKeyValue.substring(0,endAsteriskIndex))) {
                isMatch=true;
                break;
              }
            }
          }
        }
        if (!isMatch) {
          continue;
        }
      }
      Id.NamespacedId targetId=MdsValueKey.getNamespaceIdFromKey(targetType,rowKey);
      String key=MdsValueKey.getMetadataKey(targetType,rowKey);
      String value=Bytes.toString(next.get(Bytes.toBytes(MetadataDataset.VALUE_COLUMN)));
      MetadataEntry entry=new MetadataEntry(targetId,key,value);
      results.add(entry);
    }
  }
  finally {
    scanner.close();
  }
  return results;
}"
6363,"private MetadataHistoryEntry getSnapshotBeforeTime(Id.NamespacedId targetId,long timeMillis){
  byte[] scanStartKey=MdsHistoryKey.getMdsScanStartKey(targetId,timeMillis).getKey();
  byte[] scanEndKey=MdsHistoryKey.getMdsScanEndKey(targetId).getKey();
  Scanner scanner=indexedTable.scan(scanStartKey,scanEndKey);
  try {
    Row next=scanner.next();
    if (next != null) {
      return GSON.fromJson(next.getString(HISTORY_COLUMN),MetadataHistoryEntry.class);
    }
 else {
      return new MetadataHistoryEntry(targetId);
    }
  }
  finally {
    scanner.close();
  }
}","private Metadata getSnapshotBeforeTime(Id.NamespacedId targetId,long timeMillis){
  byte[] scanStartKey=MdsHistoryKey.getMdsScanStartKey(targetId,timeMillis).getKey();
  byte[] scanEndKey=MdsHistoryKey.getMdsScanEndKey(targetId).getKey();
  Scanner scanner=indexedTable.scan(scanStartKey,scanEndKey);
  try {
    Row next=scanner.next();
    if (next != null) {
      return GSON.fromJson(next.getString(HISTORY_COLUMN),Metadata.class);
    }
 else {
      return new Metadata(targetId);
    }
  }
  finally {
    scanner.close();
  }
}"
6364,"/** 
 * Snapshots the metadata for the given targetId at the given time.
 * @param targetId target id for which metadata needs snapshotting
 */
private void writeHistory(Id.NamespacedId targetId){
  Map<String,String> properties=getProperties(targetId);
  Set<String> tags=getTags(targetId);
  MetadataHistoryEntry metadataHistoryEntry=new MetadataHistoryEntry(targetId,properties,tags);
  byte[] row=MdsHistoryKey.getMdsKey(targetId,System.currentTimeMillis()).getKey();
  indexedTable.put(row,Bytes.toBytes(HISTORY_COLUMN),Bytes.toBytes(GSON.toJson(metadataHistoryEntry)));
}","/** 
 * Snapshots the metadata for the given targetId at the given time.
 * @param targetId target id for which metadata needs snapshotting
 */
private void writeHistory(Id.NamespacedId targetId){
  Map<String,String> properties=getProperties(targetId);
  Set<String> tags=getTags(targetId);
  Metadata metadata=new Metadata(targetId,properties,tags);
  byte[] row=MdsHistoryKey.getMdsKey(targetId,System.currentTimeMillis()).getKey();
  indexedTable.put(row,Bytes.toBytes(HISTORY_COLUMN),Bytes.toBytes(GSON.toJson(metadata)));
}"
6365,"@Override public Set<MetadataHistoryEntry> apply(MetadataDataset input) throws Exception {
  return input.getSnapshotBeforeTime(entityIds,timeMillis);
}","@Override public Set<Metadata> apply(MetadataDataset input) throws Exception {
  return input.getSnapshotBeforeTime(entityIds,timeMillis);
}"
6366,"@Override public Set<MetadataRecord> getSnapshotBeforeTime(MetadataScope scope,final Set<Id.NamespacedId> entityIds,final long timeMillis){
  Set<MetadataHistoryEntry> metadataHistoryEntries=execute(new TransactionExecutor.Function<MetadataDataset,Set<MetadataHistoryEntry>>(){
    @Override public Set<MetadataHistoryEntry> apply(    MetadataDataset input) throws Exception {
      return input.getSnapshotBeforeTime(entityIds,timeMillis);
    }
  }
,scope);
  ImmutableSet.Builder<MetadataRecord> builder=ImmutableSet.builder();
  for (  MetadataHistoryEntry metadataHistoryEntry : metadataHistoryEntries) {
    builder.add(new MetadataRecord(metadataHistoryEntry.getEntityId(),scope,metadataHistoryEntry.getProperties(),metadataHistoryEntry.getTags()));
  }
  return builder.build();
}","@Override public Set<MetadataRecord> getSnapshotBeforeTime(MetadataScope scope,final Set<Id.NamespacedId> entityIds,final long timeMillis){
  Set<Metadata> metadataHistoryEntries=execute(new TransactionExecutor.Function<MetadataDataset,Set<Metadata>>(){
    @Override public Set<Metadata> apply(    MetadataDataset input) throws Exception {
      return input.getSnapshotBeforeTime(entityIds,timeMillis);
    }
  }
,scope);
  ImmutableSet.Builder<MetadataRecord> builder=ImmutableSet.builder();
  for (  Metadata metadata : metadataHistoryEntries) {
    builder.add(new MetadataRecord(metadata.getEntityId(),scope,metadata.getProperties(),metadata.getTags()));
  }
  return builder.build();
}"
6367,"private void addMetadataHistory(MetadataDataset dataset,MetadataHistoryEntry record){
  for (  Map.Entry<String,String> entry : record.getProperties().entrySet()) {
    dataset.setProperty(record.getEntityId(),entry.getKey(),entry.getValue());
  }
  dataset.addTags(record.getEntityId(),record.getTags().toArray(new String[0]));
}","private void addMetadataHistory(MetadataDataset dataset,Metadata record){
  for (  Map.Entry<String,String> entry : record.getProperties().entrySet()) {
    dataset.setProperty(record.getEntityId(),entry.getKey(),entry.getValue());
  }
  dataset.addTags(record.getEntityId(),record.getTags().toArray(new String[0]));
}"
6368,"private void doTestHistory(MetadataDataset dataset,Id.NamespacedId targetId,String prefix) throws Exception {
  Map<Long,MetadataHistoryEntry> expected=new HashMap<>();
  MetadataHistoryEntry completeRecord=new MetadataHistoryEntry(targetId);
  expected.put(System.currentTimeMillis(),completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis()));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new MetadataHistoryEntry(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  completeRecord=new MetadataHistoryEntry(targetId,toProps(prefix,""String_Node_Str"",""String_Node_Str""),toTags(prefix,""String_Node_Str"",""String_Node_Str""));
  addMetadataHistory(dataset,completeRecord);
  long time=System.currentTimeMillis();
  expected.put(time,completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),time));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new MetadataHistoryEntry(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  dataset.setProperty(targetId,prefix + ""String_Node_Str"",""String_Node_Str"");
  dataset.addTags(targetId,prefix + ""String_Node_Str"");
  completeRecord=new MetadataHistoryEntry(targetId,toProps(prefix,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),toTags(prefix,""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  time=System.currentTimeMillis();
  expected.put(time,completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),time));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new MetadataHistoryEntry(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  dataset.setProperty(targetId,prefix + ""String_Node_Str"",""String_Node_Str"");
  dataset.addTags(targetId,prefix + ""String_Node_Str"");
  completeRecord=new MetadataHistoryEntry(targetId,toProps(prefix,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),toTags(prefix,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  time=System.currentTimeMillis();
  expected.put(time,completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),time));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new MetadataHistoryEntry(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  dataset.setProperty(targetId,prefix + ""String_Node_Str"",""String_Node_Str"");
  dataset.addTags(targetId,prefix + ""String_Node_Str"");
  completeRecord=new MetadataHistoryEntry(targetId,toProps(prefix,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),toTags(prefix,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  time=System.currentTimeMillis();
  expected.put(time,completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),time));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new MetadataHistoryEntry(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  dataset.removeProperties(targetId,prefix + ""String_Node_Str"");
  dataset.removeTags(targetId,prefix + ""String_Node_Str"");
  dataset.removeTags(targetId,prefix + ""String_Node_Str"");
  completeRecord=new MetadataHistoryEntry(targetId,toProps(prefix,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),toTags(prefix,""String_Node_Str"",""String_Node_Str""));
  time=System.currentTimeMillis();
  expected.put(time,completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),time));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new MetadataHistoryEntry(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  dataset.removeProperties(targetId);
  dataset.removeTags(targetId);
  completeRecord=new MetadataHistoryEntry(targetId);
  time=System.currentTimeMillis();
  expected.put(time,completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),time));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new MetadataHistoryEntry(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  dataset.setProperty(targetId,prefix + ""String_Node_Str"",""String_Node_Str"");
  dataset.addTags(targetId,prefix + ""String_Node_Str"");
  completeRecord=new MetadataHistoryEntry(targetId,toProps(prefix,""String_Node_Str"",""String_Node_Str""),toTags(prefix,""String_Node_Str""));
  time=System.currentTimeMillis();
  expected.put(time,completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),time));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new MetadataHistoryEntry(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  for (  Map.Entry<Long,MetadataHistoryEntry> entry : expected.entrySet()) {
    Assert.assertEquals(entry.getValue(),getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),entry.getKey())));
  }
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis()));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new MetadataHistoryEntry(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
}","private void doTestHistory(MetadataDataset dataset,Id.NamespacedId targetId,String prefix) throws Exception {
  Map<Long,Metadata> expected=new HashMap<>();
  Metadata completeRecord=new Metadata(targetId);
  expected.put(System.currentTimeMillis(),completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis()));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new Metadata(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  completeRecord=new Metadata(targetId,toProps(prefix,""String_Node_Str"",""String_Node_Str""),toTags(prefix,""String_Node_Str"",""String_Node_Str""));
  addMetadataHistory(dataset,completeRecord);
  long time=System.currentTimeMillis();
  expected.put(time,completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),time));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new Metadata(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  dataset.setProperty(targetId,prefix + ""String_Node_Str"",""String_Node_Str"");
  dataset.addTags(targetId,prefix + ""String_Node_Str"");
  completeRecord=new Metadata(targetId,toProps(prefix,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),toTags(prefix,""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  time=System.currentTimeMillis();
  expected.put(time,completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),time));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new Metadata(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  dataset.setProperty(targetId,prefix + ""String_Node_Str"",""String_Node_Str"");
  dataset.addTags(targetId,prefix + ""String_Node_Str"");
  completeRecord=new Metadata(targetId,toProps(prefix,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),toTags(prefix,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  time=System.currentTimeMillis();
  expected.put(time,completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),time));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new Metadata(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  dataset.setProperty(targetId,prefix + ""String_Node_Str"",""String_Node_Str"");
  dataset.addTags(targetId,prefix + ""String_Node_Str"");
  completeRecord=new Metadata(targetId,toProps(prefix,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),toTags(prefix,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""));
  time=System.currentTimeMillis();
  expected.put(time,completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),time));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new Metadata(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  dataset.removeProperties(targetId,prefix + ""String_Node_Str"");
  dataset.removeTags(targetId,prefix + ""String_Node_Str"");
  dataset.removeTags(targetId,prefix + ""String_Node_Str"");
  completeRecord=new Metadata(targetId,toProps(prefix,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),toTags(prefix,""String_Node_Str"",""String_Node_Str""));
  time=System.currentTimeMillis();
  expected.put(time,completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),time));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new Metadata(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  dataset.removeProperties(targetId);
  dataset.removeTags(targetId);
  completeRecord=new Metadata(targetId);
  time=System.currentTimeMillis();
  expected.put(time,completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),time));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new Metadata(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  dataset.setProperty(targetId,prefix + ""String_Node_Str"",""String_Node_Str"");
  dataset.addTags(targetId,prefix + ""String_Node_Str"");
  completeRecord=new Metadata(targetId,toProps(prefix,""String_Node_Str"",""String_Node_Str""),toTags(prefix,""String_Node_Str""));
  time=System.currentTimeMillis();
  expected.put(time,completeRecord);
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),time));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new Metadata(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
  TimeUnit.MILLISECONDS.sleep(1);
  for (  Map.Entry<Long,Metadata> entry : expected.entrySet()) {
    Assert.assertEquals(entry.getValue(),getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),entry.getKey())));
  }
  Assert.assertEquals(ImmutableSet.of(completeRecord),dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis()));
  Assert.assertEquals(getFirst(dataset.getSnapshotBeforeTime(ImmutableSet.of(targetId),System.currentTimeMillis())),new Metadata(targetId,dataset.getProperties(targetId),dataset.getTags(targetId)));
}"
6369,"/** 
 * Return the partition for a specific partition key.
 */
@Nullable PartitionDetail getPartition(PartitionKey key);","/** 
 * Return the partition for a specific partition key, or null if key is not found.
 */
@Nullable PartitionDetail getPartition(PartitionKey key);"
6370,"/** 
 * Adds a set of new metadata entries for a particular partition. Note that existing entries cannot be updated.
 * @throws DataSetException when an attempt is made to either update existing entries or add entries for apartition that does not exist
 */
void addMetadata(PartitionKey key,Map<String,String> metadata);","/** 
 * Adds a set of new metadata entries for a particular partition. Note that existing entries cannot be updated.
 * @throws DataSetException when an attempt is made to either update existing entries
 * @throws PartitionNotFoundException when a partition for the given key is not found
 */
void addMetadata(PartitionKey key,Map<String,String> metadata);"
6371,"/** 
 * Remove a partition for a given partition key.
 */
void dropPartition(PartitionKey key);","/** 
 * Remove a partition for a given partition key, silently ignoring if the key is not found.
 */
void dropPartition(PartitionKey key);"
6372,"@Override public Location get(Id.Namespace namespaceId,@Nullable String subPath) throws IOException {
  String namespacesDir=cConf.get(Constants.Namespace.NAMESPACES_DIR);
  Location namespaceLocation=locationFactory.create(namespacesDir).append(namespaceId.getId());
  if (subPath != null) {
    namespaceLocation=namespaceLocation.append(subPath);
  }
  return namespaceLocation;
}","@Override public Location get(Id.Namespace namespaceId,@Nullable String subPath) throws IOException {
  Location namespaceLocation=locationFactory.create(namespaceDir).append(namespaceId.getId());
  if (subPath != null) {
    namespaceLocation=namespaceLocation.append(subPath);
  }
  return namespaceLocation;
}"
6373,"@Inject public DefaultNamespacedLocationFactory(CConfiguration cConf,LocationFactory locationFactory){
  this.cConf=cConf;
  this.locationFactory=locationFactory;
}","@Inject public DefaultNamespacedLocationFactory(CConfiguration cConf,LocationFactory locationFactory){
  this.namespaceDir=cConf.get(Constants.Namespace.NAMESPACES_DIR);
  this.locationFactory=locationFactory;
}"
6374,"@Override public boolean exists(Id.Namespace namespaceId) throws Exception {
  return get(namespaceId) != null;
}","@Override public boolean exists(Id.Namespace namespaceId) throws Exception {
  try {
    get(namespaceId);
  }
 catch (  NotFoundException e) {
    return false;
  }
  return true;
}"
6375,"/** 
 * Returns the base   {@link Location} for all CDAP data. This location contains allthe namespace locations.
 */
Location getBaseLocation() throws IOException ;","/** 
 * Returns the base   {@link Location} for all CDAP data.
 */
Location getBaseLocation() throws IOException ;"
6376,"@Test public void testGet() throws IOException {
  LocationFactory locationFactory=new LocalLocationFactory(TEMP_FOLDER.newFolder());
  NamespacedLocationFactory namespacedLocationFactory=new DefaultNamespacedLocationFactory(CConfiguration.create(),locationFactory);
  Assert.assertTrue(namespacedLocationFactory.list().isEmpty());
  Location defaultLoc=namespacedLocationFactory.get(Id.Namespace.DEFAULT);
  Id.Namespace ns1=Id.Namespace.from(""String_Node_Str"");
  Location ns1Loc=namespacedLocationFactory.get(ns1);
  Assert.assertNotEquals(defaultLoc,ns1Loc);
  defaultLoc.mkdirs();
  ns1Loc.mkdirs();
  Map<Id.Namespace,Location> expected=ImmutableMap.of(Id.Namespace.DEFAULT,defaultLoc,ns1,ns1Loc);
  Assert.assertEquals(expected,namespacedLocationFactory.list());
  Location sub1=namespacedLocationFactory.get(ns1,""String_Node_Str"");
  Location sub2=namespacedLocationFactory.get(ns1,""String_Node_Str"");
  Assert.assertNotEquals(sub1,sub2);
}","@Test public void testGet() throws IOException {
  LocationFactory locationFactory=new LocalLocationFactory(TEMP_FOLDER.newFolder());
  NamespacedLocationFactory namespacedLocationFactory=new DefaultNamespacedLocationFactory(CConfiguration.create(),locationFactory);
  Location defaultLoc=namespacedLocationFactory.get(Id.Namespace.DEFAULT);
  Id.Namespace ns1=Id.Namespace.from(""String_Node_Str"");
  Location ns1Loc=namespacedLocationFactory.get(ns1);
  Assert.assertNotEquals(defaultLoc,ns1Loc);
  Location sub1=namespacedLocationFactory.get(ns1,""String_Node_Str"");
  Location sub2=namespacedLocationFactory.get(ns1,""String_Node_Str"");
  Assert.assertNotEquals(sub1,sub2);
}"
6377,"@BeforeClass public static void init() throws IOException {
  Configuration hConf=new Configuration();
  hConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR,tmpFolder.newFolder().getAbsolutePath());
  dfsCluster=new MiniDFSCluster.Builder(hConf).numDataNodes(1).build();
  final LocationFactory lf=new FileContextLocationFactory(dfsCluster.getFileSystem().getConf());
  final NamespacedLocationFactory nlf=new DefaultNamespacedLocationFactory(cConf,lf);
  Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(LocationFactory.class).toInstance(lf);
      bind(NamespacedLocationFactory.class).toInstance(nlf);
    }
  }
,new TransactionMetricsModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new DataFabricModules().getDistributedModules(),Modules.override(new DataSetsModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(MetadataStore.class).to(NoOpMetadataStore.class);
    }
  }
),new ExploreClientModule(),new ViewAdminModules().getInMemoryModules(),Modules.override(new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamCoordinatorClient.class).to(InMemoryStreamCoordinatorClient.class).in(Scopes.SINGLETON);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
    }
  }
),new AbstractModule(){
    @Override protected void configure(){
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
    }
  }
);
  locationFactory=injector.getInstance(LocationFactory.class);
  namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  streamAdmin=injector.getInstance(StreamAdmin.class);
  fileWriterFactory=injector.getInstance(StreamFileWriterFactory.class);
  streamCoordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  streamCoordinatorClient.startAndWait();
}","@BeforeClass public static void init() throws IOException {
  Configuration hConf=new Configuration();
  hConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR,tmpFolder.newFolder().getAbsolutePath());
  dfsCluster=new MiniDFSCluster.Builder(hConf).numDataNodes(1).build();
  final LocationFactory lf=new FileContextLocationFactory(dfsCluster.getFileSystem().getConf());
  final NamespacedLocationFactory nlf=new DefaultNamespacedLocationFactory(cConf,lf);
  Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(LocationFactory.class).toInstance(lf);
      bind(NamespacedLocationFactory.class).toInstance(nlf);
    }
  }
,new TransactionMetricsModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new DataFabricModules().getDistributedModules(),Modules.override(new DataSetsModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(MetadataStore.class).to(NoOpMetadataStore.class);
    }
  }
),new ExploreClientModule(),new ViewAdminModules().getInMemoryModules(),Modules.override(new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamCoordinatorClient.class).to(InMemoryStreamCoordinatorClient.class).in(Scopes.SINGLETON);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
    }
  }
),new AbstractModule(){
    @Override protected void configure(){
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
      bind(NamespaceStore.class).to(DefaultNamespaceStore.class);
    }
  }
);
  locationFactory=injector.getInstance(LocationFactory.class);
  namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  namespaceStore=injector.getInstance(NamespaceStore.class);
  streamAdmin=injector.getInstance(StreamAdmin.class);
  fileWriterFactory=injector.getInstance(StreamFileWriterFactory.class);
  streamCoordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  streamCoordinatorClient.startAndWait();
}"
6378,"@Override protected void configure(){
  bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
}","@Override protected void configure(){
  bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
  bind(NamespaceStore.class).to(DefaultNamespaceStore.class);
}"
6379,"@BeforeClass public static void init() throws IOException {
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new LocationRuntimeModule().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),new DataSetsModules().getInMemoryModules(),new TransactionMetricsModule(),new DataFabricLevelDBModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new ViewAdminModules().getInMemoryModules(),Modules.override(new StreamAdminModules().getStandaloneModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
    }
  }
),new AbstractModule(){
    @Override protected void configure(){
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
    }
  }
);
  locationFactory=injector.getInstance(LocationFactory.class);
  namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  streamAdmin=injector.getInstance(StreamAdmin.class);
  fileWriterFactory=injector.getInstance(StreamFileWriterFactory.class);
  streamCoordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  streamCoordinatorClient.startAndWait();
}","@BeforeClass public static void init() throws IOException {
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new LocationRuntimeModule().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),new DataSetsModules().getInMemoryModules(),new TransactionMetricsModule(),new DataFabricLevelDBModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new ViewAdminModules().getInMemoryModules(),Modules.override(new StreamAdminModules().getStandaloneModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
    }
  }
),new AbstractModule(){
    @Override protected void configure(){
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
      bind(NamespaceStore.class).to(DefaultNamespaceStore.class);
    }
  }
);
  locationFactory=injector.getInstance(LocationFactory.class);
  namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  namespaceStore=injector.getInstance(NamespaceStore.class);
  streamAdmin=injector.getInstance(StreamAdmin.class);
  fileWriterFactory=injector.getInstance(StreamFileWriterFactory.class);
  streamCoordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  streamCoordinatorClient.startAndWait();
}"
6380,"@Override protected void configure(){
  bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
}","@Override protected void configure(){
  bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
  bind(NamespaceStore.class).to(DefaultNamespaceStore.class);
}"
6381,"@Test public void testCleanupGeneration() throws Exception {
  String streamName=""String_Node_Str"";
  Id.Stream streamId=Id.Stream.from(Id.Namespace.DEFAULT,streamName);
  StreamAdmin streamAdmin=getStreamAdmin();
  streamAdmin.create(streamId);
  StreamConfig streamConfig=streamAdmin.getConfig(streamId);
  StreamFileJanitor janitor=new StreamFileJanitor(getCConfiguration(),getStreamAdmin(),getNamespacedLocationFactory());
  for (int i=0; i < 5; i++) {
    FileWriter<StreamEvent> writer=createWriter(streamId);
    writer.append(StreamFileTestUtils.createEvent(System.currentTimeMillis(),""String_Node_Str""));
    writer.close();
    janitor.clean(streamConfig.getLocation(),streamConfig.getTTL(),System.currentTimeMillis());
    verifyGeneration(streamConfig,i);
    streamAdmin.truncate(streamId);
  }
  int generation=StreamUtils.getGeneration(streamConfig);
  Assert.assertEquals(5,generation);
  janitor.clean(streamConfig.getLocation(),streamConfig.getTTL(),System.currentTimeMillis());
  for (  Location location : streamConfig.getLocation().list()) {
    if (location.isDirectory()) {
      Assert.assertEquals(generation,Integer.parseInt(location.getName()));
    }
  }
}","@Test public void testCleanupGeneration() throws Exception {
  String streamName=""String_Node_Str"";
  Id.Stream streamId=Id.Stream.from(Id.Namespace.DEFAULT,streamName);
  StreamAdmin streamAdmin=getStreamAdmin();
  streamAdmin.create(streamId);
  StreamConfig streamConfig=streamAdmin.getConfig(streamId);
  StreamFileJanitor janitor=new StreamFileJanitor(getCConfiguration(),getStreamAdmin(),getNamespacedLocationFactory(),getNamespaceStore());
  for (int i=0; i < 5; i++) {
    FileWriter<StreamEvent> writer=createWriter(streamId);
    writer.append(StreamFileTestUtils.createEvent(System.currentTimeMillis(),""String_Node_Str""));
    writer.close();
    janitor.clean(streamConfig.getLocation(),streamConfig.getTTL(),System.currentTimeMillis());
    verifyGeneration(streamConfig,i);
    streamAdmin.truncate(streamId);
  }
  int generation=StreamUtils.getGeneration(streamConfig);
  Assert.assertEquals(5,generation);
  janitor.clean(streamConfig.getLocation(),streamConfig.getTTL(),System.currentTimeMillis());
  for (  Location location : streamConfig.getLocation().list()) {
    if (location.isDirectory()) {
      Assert.assertEquals(generation,Integer.parseInt(location.getName()));
    }
  }
}"
6382,"@Before public void setup() throws IOException {
  getNamespacedLocationFactory().get(Id.Namespace.DEFAULT).mkdirs();
}","@Before public void setup() throws Exception {
  getNamespaceStore().create(NamespaceMeta.DEFAULT);
  getNamespacedLocationFactory().get(Id.Namespace.DEFAULT).mkdirs();
}"
6383,"@Test public void testCleanupTTL() throws Exception {
  String streamName=""String_Node_Str"";
  Id.Stream streamId=Id.Stream.from(Id.Namespace.DEFAULT,streamName);
  StreamAdmin streamAdmin=getStreamAdmin();
  StreamFileJanitor janitor=new StreamFileJanitor(getCConfiguration(),getStreamAdmin(),getNamespacedLocationFactory());
  Properties properties=new Properties();
  properties.setProperty(Constants.Stream.PARTITION_DURATION,""String_Node_Str"");
  properties.setProperty(Constants.Stream.TTL,""String_Node_Str"");
  streamAdmin.create(streamId,properties);
  streamAdmin.truncate(streamId);
  StreamConfig config=streamAdmin.getConfig(streamId);
  FileWriter<StreamEvent> writer=createWriter(streamId);
  for (int i=0; i < 10; i++) {
    writer.append(StreamFileTestUtils.createEvent(i * 1000,""String_Node_Str"" + i));
  }
  writer.close();
  Location generationLocation=StreamUtils.createGenerationLocation(config.getLocation(),1);
  Assert.assertEquals(5,generationLocation.list().size());
  janitor.clean(config.getLocation(),config.getTTL(),10000);
  Assert.assertEquals(3,generationLocation.list().size());
  janitor.clean(config.getLocation(),config.getTTL(),16000);
  Assert.assertTrue(generationLocation.list().isEmpty());
}","@Test public void testCleanupTTL() throws Exception {
  String streamName=""String_Node_Str"";
  Id.Stream streamId=Id.Stream.from(Id.Namespace.DEFAULT,streamName);
  StreamAdmin streamAdmin=getStreamAdmin();
  StreamFileJanitor janitor=new StreamFileJanitor(getCConfiguration(),getStreamAdmin(),getNamespacedLocationFactory(),getNamespaceStore());
  Properties properties=new Properties();
  properties.setProperty(Constants.Stream.PARTITION_DURATION,""String_Node_Str"");
  properties.setProperty(Constants.Stream.TTL,""String_Node_Str"");
  streamAdmin.create(streamId,properties);
  streamAdmin.truncate(streamId);
  StreamConfig config=streamAdmin.getConfig(streamId);
  FileWriter<StreamEvent> writer=createWriter(streamId);
  for (int i=0; i < 10; i++) {
    writer.append(StreamFileTestUtils.createEvent(i * 1000,""String_Node_Str"" + i));
  }
  writer.close();
  Location generationLocation=StreamUtils.createGenerationLocation(config.getLocation(),1);
  Assert.assertEquals(5,generationLocation.list().size());
  janitor.clean(config.getLocation(),config.getTTL(),10000);
  Assert.assertEquals(3,generationLocation.list().size());
  janitor.clean(config.getLocation(),config.getTTL(),16000);
  Assert.assertTrue(generationLocation.list().isEmpty());
}"
6384,"@Test public void testCleanupDeletedStream() throws Exception {
  Id.Stream streamId=Id.Stream.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  StreamAdmin streamAdmin=getStreamAdmin();
  StreamFileJanitor janitor=new StreamFileJanitor(getCConfiguration(),streamAdmin,getNamespacedLocationFactory());
  streamAdmin.create(streamId);
  try (FileWriter<StreamEvent> writer=createWriter(streamId)){
    for (int i=0; i < 10; i++) {
      writer.append(StreamFileTestUtils.createEvent(i * 1000,""String_Node_Str"" + i));
    }
  }
   streamAdmin.drop(streamId);
  janitor.cleanAll();
}","@Test public void testCleanupDeletedStream() throws Exception {
  Id.Stream streamId=Id.Stream.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  StreamAdmin streamAdmin=getStreamAdmin();
  StreamFileJanitor janitor=new StreamFileJanitor(getCConfiguration(),streamAdmin,getNamespacedLocationFactory(),getNamespaceStore());
  streamAdmin.create(streamId);
  try (FileWriter<StreamEvent> writer=createWriter(streamId)){
    for (int i=0; i < 10; i++) {
      writer.append(StreamFileTestUtils.createEvent(i * 1000,""String_Node_Str"" + i));
    }
  }
   streamAdmin.drop(streamId);
  janitor.cleanAll();
}"
6385,"@BeforeClass public static void beforeClass() throws IOException {
  zkServer=InMemoryZKServer.builder().setDataDir(TEMP_FOLDER.newFolder()).build();
  zkServer.startAndWait();
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  cConf.setInt(Constants.Stream.CONTAINER_INSTANCE_ID,0);
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TEMP_FOLDER.newFolder().getAbsolutePath());
  Injector injector=Guice.createInjector(Modules.override(new ZKClientModule(),new DataFabricModules().getInMemoryModules(),new ConfigModule(cConf,new Configuration()),new DiscoveryRuntimeModule().getInMemoryModules(),new LocationRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new DataSetServiceModules().getInMemoryModules(),new DataSetsModules().getStandaloneModules(),new NotificationFeedServiceRuntimeModule().getInMemoryModules(),new NotificationServiceRuntimeModule().getInMemoryModules(),new MetricsClientRuntimeModule().getInMemoryModules(),new ViewAdminModules().getInMemoryModules(),new StreamServiceRuntimeModule().getDistributedModules(),new StreamAdminModules().getInMemoryModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
      bind(StreamConsumerStateStoreFactory.class).to(LevelDBStreamConsumerStateStoreFactory.class).in(Singleton.class);
      bind(StreamAdmin.class).to(FileStreamAdmin.class).in(Singleton.class);
      bind(StreamConsumerFactory.class).to(LevelDBStreamFileConsumerFactory.class).in(Singleton.class);
      bind(StreamFileWriterFactory.class).to(LocationStreamFileWriterFactory.class).in(Singleton.class);
      bind(StreamFileJanitorService.class).to(LocalStreamFileJanitorService.class).in(Scopes.SINGLETON);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class).in(Scopes.SINGLETON);
      bind(HeartbeatPublisher.class).to(MockHeartbeatPublisher.class).in(Scopes.SINGLETON);
    }
  }
),new NamespaceClientRuntimeModule().getDistributedModules());
  zkClient=injector.getInstance(ZKClientService.class);
  txManager=injector.getInstance(TransactionManager.class);
  datasetService=injector.getInstance(DatasetService.class);
  notificationService=injector.getInstance(NotificationService.class);
  streamHttpService=injector.getInstance(StreamHttpService.class);
  streamService=injector.getInstance(StreamService.class);
  heartbeatPublisher=(MockHeartbeatPublisher)injector.getInstance(HeartbeatPublisher.class);
  namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  streamAdmin=injector.getInstance(StreamAdmin.class);
  zkClient.startAndWait();
  txManager.startAndWait();
  datasetService.startAndWait();
  notificationService.startAndWait();
  streamHttpService.startAndWait();
  streamService.startAndWait();
  hostname=streamHttpService.getBindAddress().getHostName();
  port=streamHttpService.getBindAddress().getPort();
  Locations.mkdirsIfNotExists(namespacedLocationFactory.get(Id.Namespace.DEFAULT));
}","@BeforeClass public static void beforeClass() throws IOException {
  zkServer=InMemoryZKServer.builder().setDataDir(TEMP_FOLDER.newFolder()).build();
  zkServer.startAndWait();
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  cConf.setInt(Constants.Stream.CONTAINER_INSTANCE_ID,0);
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,TEMP_FOLDER.newFolder().getAbsolutePath());
  Injector injector=Guice.createInjector(Modules.override(new ZKClientModule(),new DataFabricModules().getInMemoryModules(),new ConfigModule(cConf,new Configuration()),new DiscoveryRuntimeModule().getInMemoryModules(),new LocationRuntimeModule().getInMemoryModules(),new ExploreClientModule(),new DataSetServiceModules().getInMemoryModules(),new DataSetsModules().getStandaloneModules(),new NotificationFeedServiceRuntimeModule().getInMemoryModules(),new NotificationServiceRuntimeModule().getInMemoryModules(),new MetricsClientRuntimeModule().getInMemoryModules(),new ViewAdminModules().getInMemoryModules(),new StreamServiceRuntimeModule().getDistributedModules(),new StreamAdminModules().getInMemoryModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
      bind(AbstractNamespaceClient.class).to(InMemoryNamespaceClient.class);
      bind(StreamConsumerStateStoreFactory.class).to(LevelDBStreamConsumerStateStoreFactory.class).in(Singleton.class);
      bind(StreamAdmin.class).to(FileStreamAdmin.class).in(Singleton.class);
      bind(StreamConsumerFactory.class).to(LevelDBStreamFileConsumerFactory.class).in(Singleton.class);
      bind(StreamFileWriterFactory.class).to(LocationStreamFileWriterFactory.class).in(Singleton.class);
      bind(StreamFileJanitorService.class).to(LocalStreamFileJanitorService.class).in(Scopes.SINGLETON);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class).in(Scopes.SINGLETON);
      bind(HeartbeatPublisher.class).to(MockHeartbeatPublisher.class).in(Scopes.SINGLETON);
    }
  }
));
  zkClient=injector.getInstance(ZKClientService.class);
  txManager=injector.getInstance(TransactionManager.class);
  datasetService=injector.getInstance(DatasetService.class);
  notificationService=injector.getInstance(NotificationService.class);
  streamHttpService=injector.getInstance(StreamHttpService.class);
  streamService=injector.getInstance(StreamService.class);
  heartbeatPublisher=(MockHeartbeatPublisher)injector.getInstance(HeartbeatPublisher.class);
  namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  streamAdmin=injector.getInstance(StreamAdmin.class);
  zkClient.startAndWait();
  txManager.startAndWait();
  datasetService.startAndWait();
  notificationService.startAndWait();
  streamHttpService.startAndWait();
  streamService.startAndWait();
  hostname=streamHttpService.getBindAddress().getHostName();
  port=streamHttpService.getBindAddress().getPort();
  Locations.mkdirsIfNotExists(namespacedLocationFactory.get(Id.Namespace.DEFAULT));
}"
6386,"@Override protected void configure(){
  bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
  bind(StreamConsumerStateStoreFactory.class).to(LevelDBStreamConsumerStateStoreFactory.class).in(Singleton.class);
  bind(StreamAdmin.class).to(FileStreamAdmin.class).in(Singleton.class);
  bind(StreamConsumerFactory.class).to(LevelDBStreamFileConsumerFactory.class).in(Singleton.class);
  bind(StreamFileWriterFactory.class).to(LocationStreamFileWriterFactory.class).in(Singleton.class);
  bind(StreamFileJanitorService.class).to(LocalStreamFileJanitorService.class).in(Scopes.SINGLETON);
  bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class).in(Scopes.SINGLETON);
  bind(HeartbeatPublisher.class).to(MockHeartbeatPublisher.class).in(Scopes.SINGLETON);
}","@Override protected void configure(){
  bind(MetricsCollectionService.class).to(NoOpMetricsCollectionService.class);
  bind(AbstractNamespaceClient.class).to(InMemoryNamespaceClient.class);
  bind(StreamConsumerStateStoreFactory.class).to(LevelDBStreamConsumerStateStoreFactory.class).in(Singleton.class);
  bind(StreamAdmin.class).to(FileStreamAdmin.class).in(Singleton.class);
  bind(StreamConsumerFactory.class).to(LevelDBStreamFileConsumerFactory.class).in(Singleton.class);
  bind(StreamFileWriterFactory.class).to(LocationStreamFileWriterFactory.class).in(Singleton.class);
  bind(StreamFileJanitorService.class).to(LocalStreamFileJanitorService.class).in(Scopes.SINGLETON);
  bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class).in(Scopes.SINGLETON);
  bind(HeartbeatPublisher.class).to(MockHeartbeatPublisher.class).in(Scopes.SINGLETON);
}"
6387,"/** 
 * Performs file cleanup for all streams.
 */
public void cleanAll() throws IOException {
  Map<Id.Namespace,Location> namespaceLocations=namespacedLocationFactory.list();
  if (namespaceLocations.size() == 0) {
    return;
  }
  for (  Location namespaceDir : namespaceLocations.values()) {
    Location streamBaseLocation=namespaceDir.append(streamBaseDirPath);
    if (!streamBaseLocation.exists()) {
      continue;
    }
    Location deletedLocation=StreamUtils.getDeletedLocation(streamBaseLocation);
    if (deletedLocation.exists()) {
      Locations.deleteContent(deletedLocation);
    }
    for (    Location streamLocation : StreamUtils.listAllStreams(streamBaseLocation)) {
      Id.Stream streamId=StreamUtils.getStreamIdFromLocation(streamLocation);
      long ttl=0L;
      if (isStreamExists(streamId)) {
        ttl=streamAdmin.getConfig(streamId).getTTL();
      }
      clean(streamLocation,ttl,System.currentTimeMillis());
    }
  }
}","/** 
 * Performs file cleanup for all streams.
 */
public void cleanAll() throws Exception {
  List<NamespaceMeta> namespaces=namespaceStore.list();
  for (  NamespaceMeta namespace : namespaces) {
    Location streamBaseLocation=namespacedLocationFactory.get(Id.Namespace.from(namespace.getName())).append(streamBaseDirPath);
    if (!streamBaseLocation.exists()) {
      continue;
    }
    Location deletedLocation=StreamUtils.getDeletedLocation(streamBaseLocation);
    if (deletedLocation.exists()) {
      Locations.deleteContent(deletedLocation);
    }
    for (    Location streamLocation : StreamUtils.listAllStreams(streamBaseLocation)) {
      Id.Stream streamId=StreamUtils.getStreamIdFromLocation(streamLocation);
      long ttl=0L;
      if (isStreamExists(streamId)) {
        ttl=streamAdmin.getConfig(streamId).getTTL();
      }
      clean(streamLocation,ttl,System.currentTimeMillis());
    }
  }
}"
6388,"@Inject public StreamFileJanitor(CConfiguration cConf,StreamAdmin streamAdmin,NamespacedLocationFactory namespacedLocationFactory){
  this.streamAdmin=streamAdmin;
  this.streamBaseDirPath=cConf.get(Constants.Stream.BASE_DIR);
  this.namespacedLocationFactory=namespacedLocationFactory;
}","@Inject public StreamFileJanitor(CConfiguration cConf,StreamAdmin streamAdmin,NamespacedLocationFactory namespacedLocationFactory,NamespaceStore namespaceStore){
  this.streamAdmin=streamAdmin;
  this.streamBaseDirPath=cConf.get(Constants.Stream.BASE_DIR);
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.namespaceStore=namespaceStore;
}"
6389,"@Override protected void doStart(){
  executor=Executors.newSingleThreadScheduledExecutor(Threads.createDaemonThreadFactory(""String_Node_Str""));
  executor.submit(new Runnable(){
    @Override public void run(){
      if (state() != State.RUNNING) {
        LOG.info(""String_Node_Str"");
        return;
      }
      LOG.debug(""String_Node_Str"");
      try {
        janitor.cleanAll();
        LOG.debug(""String_Node_Str"");
      }
 catch (      Throwable e) {
        LOG.warn(""String_Node_Str"",e.getMessage());
        LOG.debug(""String_Node_Str"",e);
      }
 finally {
        long now=System.currentTimeMillis();
        long delay=(now / cleanupPeriod + 1) * cleanupPeriod - now;
        if (delay <= 0) {
          executor.submit(this);
        }
 else {
          LOG.debug(""String_Node_Str"",delay);
          executor.schedule(this,delay,TimeUnit.MILLISECONDS);
        }
      }
    }
  }
);
  notifyStarted();
}","@Override protected void doStart(){
  executor=Executors.newSingleThreadScheduledExecutor(Threads.createDaemonThreadFactory(""String_Node_Str""));
  executor.submit(new Runnable(){
    @Override public void run(){
      if (state() != State.RUNNING) {
        LOG.info(""String_Node_Str"");
        return;
      }
      LOG.debug(""String_Node_Str"");
      try {
        janitor.cleanAll();
        LOG.debug(""String_Node_Str"");
      }
 catch (      Throwable e) {
        LOG.warn(""String_Node_Str"",e.getMessage());
      }
 finally {
        long now=System.currentTimeMillis();
        long delay=(now / cleanupPeriod + 1) * cleanupPeriod - now;
        if (delay <= 0) {
          executor.submit(this);
        }
 else {
          LOG.debug(""String_Node_Str"",delay);
          executor.schedule(this,delay,TimeUnit.MILLISECONDS);
        }
      }
    }
  }
);
  notifyStarted();
}"
6390,"@Override public Module getDistributedModules(){
  return new AbstractModule(){
    @Override protected void configure(){
      bind(StreamFileJanitorService.class).to(DistributedStreamFileJanitorService.class).in(Scopes.SINGLETON);
      bind(StreamWriterSizeCollector.class).to(BasicStreamWriterSizeCollector.class).in(Scopes.SINGLETON);
      bind(StreamService.class).to(DistributedStreamService.class).in(Scopes.SINGLETON);
      Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.Stream.STREAM_HANDLER));
      handlerBinder.addBinding().to(StreamHandler.class);
      handlerBinder.addBinding().to(StreamFetchHandler.class);
      handlerBinder.addBinding().to(StreamViewHttpHandler.class);
      CommonHandlers.add(handlerBinder);
      bind(HeartbeatPublisher.class).to(NotificationHeartbeatPublisher.class).in(Scopes.SINGLETON);
      bind(StreamHttpService.class).in(Scopes.SINGLETON);
      bind(Key.get(new TypeLiteral<Supplier<Discoverable>>(){
      }
)).to(StreamHttpService.class);
    }
  }
;
}","@Override public Module getDistributedModules(){
  return new AbstractModule(){
    @Override protected void configure(){
      bind(StreamFileJanitorService.class).to(DistributedStreamFileJanitorService.class).in(Scopes.SINGLETON);
      bind(StreamWriterSizeCollector.class).to(BasicStreamWriterSizeCollector.class).in(Scopes.SINGLETON);
      bind(StreamService.class).to(DistributedStreamService.class).in(Scopes.SINGLETON);
      Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.Stream.STREAM_HANDLER));
      handlerBinder.addBinding().to(StreamHandler.class);
      handlerBinder.addBinding().to(StreamFetchHandler.class);
      handlerBinder.addBinding().to(StreamViewHttpHandler.class);
      CommonHandlers.add(handlerBinder);
      bind(HeartbeatPublisher.class).to(NotificationHeartbeatPublisher.class).in(Scopes.SINGLETON);
      bind(NamespaceStore.class).to(DefaultNamespaceStore.class).in(Scopes.SINGLETON);
      bind(StreamHttpService.class).in(Scopes.SINGLETON);
      bind(Key.get(new TypeLiteral<Supplier<Discoverable>>(){
      }
)).to(StreamHttpService.class);
    }
  }
;
}"
6391,"@Override protected void configure(){
  bind(StreamFileJanitorService.class).to(DistributedStreamFileJanitorService.class).in(Scopes.SINGLETON);
  bind(StreamWriterSizeCollector.class).to(BasicStreamWriterSizeCollector.class).in(Scopes.SINGLETON);
  bind(StreamService.class).to(DistributedStreamService.class).in(Scopes.SINGLETON);
  Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.Stream.STREAM_HANDLER));
  handlerBinder.addBinding().to(StreamHandler.class);
  handlerBinder.addBinding().to(StreamFetchHandler.class);
  handlerBinder.addBinding().to(StreamViewHttpHandler.class);
  CommonHandlers.add(handlerBinder);
  bind(HeartbeatPublisher.class).to(NotificationHeartbeatPublisher.class).in(Scopes.SINGLETON);
  bind(StreamHttpService.class).in(Scopes.SINGLETON);
  bind(Key.get(new TypeLiteral<Supplier<Discoverable>>(){
  }
)).to(StreamHttpService.class);
}","@Override protected void configure(){
  bind(StreamFileJanitorService.class).to(DistributedStreamFileJanitorService.class).in(Scopes.SINGLETON);
  bind(StreamWriterSizeCollector.class).to(BasicStreamWriterSizeCollector.class).in(Scopes.SINGLETON);
  bind(StreamService.class).to(DistributedStreamService.class).in(Scopes.SINGLETON);
  Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.Stream.STREAM_HANDLER));
  handlerBinder.addBinding().to(StreamHandler.class);
  handlerBinder.addBinding().to(StreamFetchHandler.class);
  handlerBinder.addBinding().to(StreamViewHttpHandler.class);
  CommonHandlers.add(handlerBinder);
  bind(HeartbeatPublisher.class).to(NotificationHeartbeatPublisher.class).in(Scopes.SINGLETON);
  bind(NamespaceStore.class).to(DefaultNamespaceStore.class).in(Scopes.SINGLETON);
  bind(StreamHttpService.class).in(Scopes.SINGLETON);
  bind(Key.get(new TypeLiteral<Supplier<Discoverable>>(){
  }
)).to(StreamHttpService.class);
}"
6392,"void State(ProgramRunStatus runStatus){
  this.runStatus=runStatus;
}","void State(ProgramRunStatus runStatus){
  this.runStatus=runStatus;
  this.programStatus=ProgramRunStatus.RUNNING == runStatus ? ProgramStatus.RUNNING : ProgramStatus.STOPPED;
}"
6393,"/** 
 * Returns a map where the pairs map from status to program status (e.g. {""status"" : ""RUNNING""}) or in case of an error in the input (e.g. invalid id, program not found), a map from statusCode to integer and error to error message (e.g. {""statusCode"": 404, ""error"": ""Program not found""})
 * @param id The Program Id to get the status of
 * @throws BadRequestException if program type is invalid
 * @throws NotFoundException if the application to which this program belongs was not found
 */
private StatusMap getStatus(final Id.Program id) throws BadRequestException, NotFoundException {
  if (id.getType() == null) {
    throw new BadRequestException(String.format(""String_Node_Str"",id.getId()));
  }
  ApplicationSpecification appSpec=store.getApplication(id.getApplication());
  if (appSpec == null) {
    throw new NotFoundException(Id.Application.from(id.getNamespaceId(),id.getApplicationId()));
  }
  String programStatus=getProgramStatus(id).getStatus();
  StatusMap statusMap=new StatusMap();
  statusMap.setStatus(programStatus);
  statusMap.setStatusCode(HttpResponseStatus.OK.getCode());
  return statusMap;
}","/** 
 * Returns status of a type specified by the type{flows,workflows,mapreduce,spark,services,schedules}.
 */
@GET @Path(""String_Node_Str"") public void getStatus(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String type,@PathParam(""String_Node_Str"") String id) throws NotFoundException, SchedulerException, BadRequestException {
  if (type.equals(""String_Node_Str"")) {
    getScheduleStatus(responder,appId,namespaceId,id);
    return;
  }
  ProgramType programType=ProgramType.valueOfCategoryName(type);
  Id.Program program=Id.Program.from(namespaceId,appId,programType,id);
  ProgramStatus programStatus=getProgramStatus(program);
  Map<String,String> status=ImmutableMap.of(""String_Node_Str"",programStatus.name());
  responder.sendJson(HttpResponseStatus.OK,status);
}"
6394,"/** 
 * Deletes queues.
 */
@DELETE @Path(""String_Node_Str"") public void deleteFlowQueues(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId) throws Exception {
  Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.FLOW,flowId);
  try {
    ProgramStatus status=getProgramStatus(programId);
    if (status.getStatus().equals(HttpResponseStatus.NOT_FOUND.toString())) {
      responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    }
 else     if (status.getStatus().equals(""String_Node_Str"")) {
      responder.sendString(HttpResponseStatus.FORBIDDEN,""String_Node_Str"");
    }
 else {
      queueAdmin.dropAllForFlow(Id.Flow.from(programId.getApplication(),programId.getId()));
      FlowUtils.deleteFlowPendingMetrics(metricStore,namespaceId,appId,flowId);
      responder.sendStatus(HttpResponseStatus.OK);
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
}","/** 
 * Deletes queues.
 */
@DELETE @Path(""String_Node_Str"") public void deleteFlowQueues(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId) throws Exception {
  Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.FLOW,flowId);
  try {
    ProgramStatus status=getProgramStatus(programId);
    if (ProgramStatus.RUNNING == status) {
      responder.sendString(HttpResponseStatus.FORBIDDEN,""String_Node_Str"");
    }
 else {
      queueAdmin.dropAllForFlow(Id.Flow.from(programId.getApplication(),programId.getId()));
      FlowUtils.deleteFlowPendingMetrics(metricStore,namespaceId,appId,flowId);
      responder.sendStatus(HttpResponseStatus.OK);
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
}"
6395,"private boolean isRunning(Id.Program id) throws BadRequestException, NotFoundException {
  String programStatus=getStatus(id).getStatus();
  return programStatus != null && !""String_Node_Str"".equals(programStatus);
}","private boolean isRunning(Id.Program id) throws BadRequestException, NotFoundException {
  return ProgramStatus.STOPPED != getProgramStatus(id);
}"
6396,"@DELETE @Path(""String_Node_Str"") public synchronized void deleteQueues(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  Id.Namespace namespace=Id.Namespace.from(namespaceId);
  try {
    List<ProgramRecord> flows=listPrograms(namespace,ProgramType.FLOW,store);
    for (    ProgramRecord flow : flows) {
      String appId=flow.getApp();
      String flowId=flow.getName();
      Id.Program programId=Id.Program.from(namespace,appId,ProgramType.FLOW,flowId);
      ProgramStatus status=getProgramStatus(programId);
      if (!""String_Node_Str"".equals(status.getStatus())) {
        responder.sendString(HttpResponseStatus.FORBIDDEN,String.format(""String_Node_Str"" + ""String_Node_Str"",flowId,appId,namespaceId));
        return;
      }
    }
    queueAdmin.dropAllInNamespace(namespace);
    FlowUtils.deleteFlowPendingMetrics(metricStore,namespaceId,null,null);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"" + namespace,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}","@DELETE @Path(""String_Node_Str"") public synchronized void deleteQueues(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  Id.Namespace namespace=Id.Namespace.from(namespaceId);
  try {
    List<ProgramRecord> flows=listPrograms(namespace,ProgramType.FLOW,store);
    for (    ProgramRecord flow : flows) {
      String appId=flow.getApp();
      String flowId=flow.getName();
      Id.Program programId=Id.Program.from(namespace,appId,ProgramType.FLOW,flowId);
      ProgramStatus status=getProgramStatus(programId);
      if (ProgramStatus.STOPPED != status) {
        responder.sendString(HttpResponseStatus.FORBIDDEN,String.format(""String_Node_Str"" + ""String_Node_Str"",flowId,appId,namespaceId));
        return;
      }
    }
    queueAdmin.dropAllInNamespace(namespace);
    FlowUtils.deleteFlowPendingMetrics(metricStore,namespaceId,null,null);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"" + namespace,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}"
6397,"/** 
 * Returns the status for all programs that are passed into the data. The data is an array of JSON objects where each object must contain the following three elements: appId, programType, and programId (flow name, service name, etc.). <p> Example input: <pre><code> [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1""}, {""appId"": ""App1"", ""programType"": ""Mapreduce"", ""programId"": ""MapReduce2""}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1""}] </code></pre> </p><p> The response will be an array of JsonObjects each of which will contain the three input parameters as well as 2 fields, ""status"" which maps to the status of the program and ""statusCode"" which maps to the status code for the data in that JsonObjects. </p><p> If an error occurs in the input (for the example above, App2 does not exist), then all JsonObjects for which the parameters have a valid status will have the status field but all JsonObjects for which the parameters do not have a valid status will have an error message and statusCode. </p><p> For example, if there is no App2 in the data above, then the response would be 200 OK with following possible data: </p> <pre><code> [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""statusCode"": 200, ""status"": ""RUNNING""}, {""appId"": ""App1"", ""programType"": ""Mapreduce"", ""programId"": ""Mapreduce2"", ""statusCode"": 200, ""status"": ""STOPPED""}, {""appId"":""App2"", ""programType"":""Flow"", ""programId"":""Flow1"", ""statusCode"":404, ""error"": ""App: App2 not found""}] </code></pre>
 */
@POST @Path(""String_Node_Str"") public void getStatuses(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws IOException, BadRequestException {
  List<BatchProgram> programs=validateAndGetBatchInput(request,BATCH_PROGRAMS_TYPE);
  List<BatchProgramStatus> statuses=new ArrayList<>(programs.size());
  for (  BatchProgram program : programs) {
    Id.Program progId=Id.Program.from(namespaceId,program.getAppId(),program.getProgramType(),program.getProgramId());
    try {
      StatusMap statusMap=getStatus(progId);
      statuses.add(new BatchProgramStatus(program,HttpResponseStatus.OK.getCode(),null,statusMap.getStatus()));
    }
 catch (    BadRequestException e) {
      statuses.add(new BatchProgramStatus(program,HttpResponseStatus.BAD_REQUEST.getCode(),e.getMessage(),null));
    }
catch (    NotFoundException e) {
      statuses.add(new BatchProgramStatus(program,HttpResponseStatus.NOT_FOUND.getCode(),e.getMessage(),null));
    }
  }
  responder.sendJson(HttpResponseStatus.OK,statuses);
}","/** 
 * Returns the status for all programs that are passed into the data. The data is an array of JSON objects where each object must contain the following three elements: appId, programType, and programId (flow name, service name, etc.). <p> Example input: <pre><code> [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1""}, {""appId"": ""App1"", ""programType"": ""Mapreduce"", ""programId"": ""MapReduce2""}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1""}] </code></pre> </p><p> The response will be an array of JsonObjects each of which will contain the three input parameters as well as 2 fields, ""status"" which maps to the status of the program and ""statusCode"" which maps to the status code for the data in that JsonObjects. </p><p> If an error occurs in the input (for the example above, App2 does not exist), then all JsonObjects for which the parameters have a valid status will have the status field but all JsonObjects for which the parameters do not have a valid status will have an error message and statusCode. </p><p> For example, if there is no App2 in the data above, then the response would be 200 OK with following possible data: </p> <pre><code> [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""statusCode"": 200, ""status"": ""RUNNING""}, {""appId"": ""App1"", ""programType"": ""Mapreduce"", ""programId"": ""Mapreduce2"", ""statusCode"": 200, ""status"": ""STOPPED""}, {""appId"":""App2"", ""programType"":""Flow"", ""programId"":""Flow1"", ""statusCode"":404, ""error"": ""App: App2 not found""}] </code></pre>
 */
@POST @Path(""String_Node_Str"") public void getStatuses(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws IOException, BadRequestException {
  List<BatchProgram> programs=validateAndGetBatchInput(request,BATCH_PROGRAMS_TYPE);
  List<BatchProgramStatus> statuses=new ArrayList<>(programs.size());
  for (  BatchProgram program : programs) {
    Id.Program progId=Id.Program.from(namespaceId,program.getAppId(),program.getProgramType(),program.getProgramId());
    try {
      ProgramStatus programStatus=getProgramStatus(progId);
      statuses.add(new BatchProgramStatus(program,HttpResponseStatus.OK.getCode(),null,programStatus.name()));
    }
 catch (    BadRequestException e) {
      statuses.add(new BatchProgramStatus(program,HttpResponseStatus.BAD_REQUEST.getCode(),e.getMessage(),null));
    }
catch (    NotFoundException e) {
      statuses.add(new BatchProgramStatus(program,HttpResponseStatus.NOT_FOUND.getCode(),e.getMessage(),null));
    }
  }
  responder.sendJson(HttpResponseStatus.OK,statuses);
}"
6398,"/** 
 * 'protected' for the workflow handler to use
 */
protected ProgramStatus getProgramStatus(Id.Program id,@Nullable String runId) throws NotFoundException {
  ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(id,runId);
  if (runtimeInfo == null) {
    if (id.getType() != ProgramType.WEBAPP) {
      ProgramSpecification spec=getProgramSpecification(id);
      if (spec == null) {
        throw new NotFoundException(id);
      }
      if (id.getType() == ProgramType.MAPREDUCE && !store.getRuns(id,ProgramRunStatus.RUNNING,0,Long.MAX_VALUE,1).isEmpty()) {
        return new ProgramStatus(id.getApplicationId(),id.getId(),""String_Node_Str"");
      }
      return new ProgramStatus(id.getApplicationId(),id.getId(),""String_Node_Str"");
    }
    try {
      Location webappLoc=Programs.programLocation(namespacedLocationFactory,appFabricDir,id);
      if (webappLoc != null && webappLoc.exists()) {
        return new ProgramStatus(id.getApplicationId(),id.getId(),""String_Node_Str"");
      }
      throw new NotFoundException(id);
    }
 catch (    IOException ioe) {
      throw new NotFoundException(id,ioe);
    }
  }
  String status=controllerStateToString(runtimeInfo.getController().getState());
  return new ProgramStatus(id.getApplicationId(),id.getId(),status);
}","/** 
 * Returns the program status.
 * @param id the id of the program for which the status call is made
 * @return the status of the program
 * @throws BadRequestException if the program type is invalid
 * @throws NotFoundException if the application to which this program belongs was not found
 */
private ProgramStatus getProgramStatus(Id.Program id) throws BadRequestException, NotFoundException {
  if (id.getType() == null) {
    throw new BadRequestException(String.format(""String_Node_Str"",id.getId()));
  }
  ApplicationSpecification appSpec=store.getApplication(id.getApplication());
  if (appSpec == null) {
    throw new NotFoundException(Id.Application.from(id.getNamespaceId(),id.getApplicationId()));
  }
  ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(id,null);
  if (runtimeInfo == null) {
    if (id.getType() != ProgramType.WEBAPP) {
      ProgramSpecification spec=getProgramSpecification(id);
      if (spec == null) {
        throw new NotFoundException(id);
      }
      if (id.getType() == ProgramType.MAPREDUCE && !store.getRuns(id,ProgramRunStatus.RUNNING,0,Long.MAX_VALUE,1).isEmpty()) {
        return ProgramStatus.RUNNING;
      }
      return ProgramStatus.STOPPED;
    }
    try {
      Location webappLoc=Programs.programLocation(namespacedLocationFactory,appFabricDir,id);
      if (webappLoc != null && webappLoc.exists()) {
        return ProgramStatus.STOPPED;
      }
      throw new NotFoundException(id);
    }
 catch (    IOException ioe) {
      throw new NotFoundException(id,ioe);
    }
  }
  return runtimeInfo.getController().getState().getProgramStatus();
}"
6399,"@Test public void test() throws TimeoutException, InterruptedException, IOException {
  ApplicationManager appManager=deployApplication(HelloWorld.class);
  FlowManager flowManager=appManager.getFlowManager(""String_Node_Str"").start();
  StreamManager streamManager=getStreamManager(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  try {
    RuntimeMetrics metrics=RuntimeStats.getFlowletMetrics(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
    metrics.waitForProcessed(5,5,TimeUnit.SECONDS);
  }
  finally {
    flowManager.stop();
  }
  ServiceManager serviceManager=appManager.getServiceManager(HelloWorld.Greeting.SERVICE_NAME);
  serviceManager.start();
  serviceManager.waitForStatus(true);
  URL url=new URL(serviceManager.getServiceURL(),""String_Node_Str"");
  HttpURLConnection connection=(HttpURLConnection)url.openConnection();
  Assert.assertEquals(HttpURLConnection.HTTP_OK,connection.getResponseCode());
  String response;
  try {
    response=new String(ByteStreams.toByteArray(connection.getInputStream()),Charsets.UTF_8);
  }
  finally {
    connection.disconnect();
  }
  Assert.assertEquals(""String_Node_Str"",response);
  appManager.stopAll();
}","@Test public void test() throws Exception {
  ApplicationManager appManager=deployApplication(HelloWorld.class);
  FlowManager flowManager=appManager.getFlowManager(""String_Node_Str"").start();
  Assert.assertTrue(flowManager.isRunning());
  StreamManager streamManager=getStreamManager(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  try {
    RuntimeMetrics metrics=flowManager.getFlowletMetrics(""String_Node_Str"");
    metrics.waitForProcessed(5,5,TimeUnit.SECONDS);
  }
  finally {
    flowManager.stop();
    Assert.assertFalse(flowManager.isRunning());
  }
  ServiceManager serviceManager=appManager.getServiceManager(HelloWorld.Greeting.SERVICE_NAME).start();
  serviceManager.waitForStatus(true);
  URL url=new URL(serviceManager.getServiceURL(),""String_Node_Str"");
  HttpURLConnection connection=(HttpURLConnection)url.openConnection();
  Assert.assertEquals(HttpURLConnection.HTTP_OK,connection.getResponseCode());
  String response;
  try {
    response=new String(ByteStreams.toByteArray(connection.getInputStream()),Charsets.UTF_8);
  }
  finally {
    connection.disconnect();
  }
  Assert.assertEquals(""String_Node_Str"",response);
}"
6400,"@Test public void test() throws Exception {
  ApplicationManager appManager=deployApplication(HelloWorld.class);
  FlowManager flowManager=appManager.getFlowManager(""String_Node_Str"").start();
  Assert.assertTrue(flowManager.isRunning());
  StreamManager streamManager=getStreamManager(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  try {
    RuntimeMetrics metrics=flowManager.getFlowletMetrics(""String_Node_Str"");
    metrics.waitForProcessed(5,5,TimeUnit.SECONDS);
  }
  finally {
    flowManager.stop();
    Assert.assertFalse(flowManager.isRunning());
  }
  ServiceManager serviceManager=appManager.getServiceManager(HelloWorld.Greeting.SERVICE_NAME).start();
  serviceManager.waitForStatus(true);
  URL url=new URL(serviceManager.getServiceURL(15,TimeUnit.SECONDS),""String_Node_Str"");
  HttpURLConnection connection=(HttpURLConnection)url.openConnection();
  Assert.assertEquals(HttpURLConnection.HTTP_OK,connection.getResponseCode());
  String response;
  try {
    response=new String(ByteStreams.toByteArray(connection.getInputStream()),Charsets.UTF_8);
  }
  finally {
    connection.disconnect();
  }
  Assert.assertEquals(""String_Node_Str"",response);
}","@Test public void test() throws Exception {
  ApplicationManager appManager=deployApplication(HelloWorld.class);
  FlowManager flowManager=appManager.getFlowManager(""String_Node_Str"").start();
  Assert.assertTrue(flowManager.isRunning());
  StreamManager streamManager=getStreamManager(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  streamManager.send(""String_Node_Str"");
  try {
    RuntimeMetrics metrics=flowManager.getFlowletMetrics(""String_Node_Str"");
    metrics.waitForProcessed(5,5,TimeUnit.SECONDS);
  }
  finally {
    flowManager.stop();
    Assert.assertFalse(flowManager.isRunning());
  }
  ServiceManager serviceManager=appManager.getServiceManager(HelloWorld.Greeting.SERVICE_NAME).start();
  serviceManager.waitForStatus(true);
  URL url=new URL(serviceManager.getServiceURL(),""String_Node_Str"");
  HttpURLConnection connection=(HttpURLConnection)url.openConnection();
  Assert.assertEquals(HttpURLConnection.HTTP_OK,connection.getResponseCode());
  String response;
  try {
    response=new String(ByteStreams.toByteArray(connection.getInputStream()),Charsets.UTF_8);
  }
  finally {
    connection.disconnect();
  }
  Assert.assertEquals(""String_Node_Str"",response);
}"
6401,"List<BusinessMetadataRecord> executeSearchOnColumns(String namespaceId,String column,String searchValue,MetadataSearchTargetType type){
  List<BusinessMetadataRecord> results=new LinkedList<>();
  Scanner scanner;
  String namespacedSearchValue=namespaceId + KEYVALUE_SEPARATOR + searchValue.toLowerCase();
  if (namespacedSearchValue.endsWith(""String_Node_Str"")) {
    byte[] startKey=Bytes.toBytes(namespacedSearchValue.substring(0,namespacedSearchValue.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    scanner=indexedTable.scanByIndex(Bytes.toBytes(column),startKey,stopKey);
  }
 else {
    byte[] value=Bytes.toBytes(namespacedSearchValue);
    scanner=indexedTable.readByIndex(Bytes.toBytes(column),value);
  }
  try {
    Row next;
    while ((next=scanner.next()) != null) {
      String rowValue=next.getString(VALUE_COLUMN);
      if (rowValue == null) {
        continue;
      }
      final byte[] rowKey=next.getRow();
      String targetType=MdsValueKey.getTargetType(rowKey);
      if ((type != MetadataSearchTargetType.ALL) && (!targetType.equals(type.getInternalName()))) {
        continue;
      }
      Id.NamespacedId targetId=MdsValueKey.getNamespaceIdFromKey(targetType,new MDSKey(rowKey));
      String key=MdsValueKey.getMetadataKey(targetType,rowKey);
      String value=Bytes.toString(next.get(Bytes.toBytes(BusinessMetadataDataset.VALUE_COLUMN)));
      BusinessMetadataRecord record=new BusinessMetadataRecord(targetId,key,value);
      results.add(record);
    }
  }
  finally {
    scanner.close();
  }
  return results;
}","List<BusinessMetadataRecord> executeSearchOnColumns(final String namespaceId,final String column,final String searchValue,final MetadataSearchTargetType type){
  List<BusinessMetadataRecord> results=new LinkedList<>();
  Scanner scanner;
  String namespacedSearchValue=namespaceId + KEYVALUE_SEPARATOR + searchValue.toLowerCase();
  boolean modifiedTagsKVSearch=false;
  final String unmodifiedTagKeyValue=searchValue.toLowerCase();
  if (KEYVALUE_COLUMN.equals(column) && unmodifiedTagKeyValue.startsWith(TAGS_KEY + KEYVALUE_SEPARATOR)) {
    namespacedSearchValue=namespacedSearchValue.substring(0,namespacedSearchValue.lastIndexOf(KEYVALUE_SEPARATOR) + 1) + ""String_Node_Str"";
    modifiedTagsKVSearch=true;
  }
  if (namespacedSearchValue.endsWith(""String_Node_Str"")) {
    byte[] startKey=Bytes.toBytes(namespacedSearchValue.substring(0,namespacedSearchValue.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    scanner=indexedTable.scanByIndex(Bytes.toBytes(column),startKey,stopKey);
  }
 else {
    byte[] value=Bytes.toBytes(namespacedSearchValue);
    scanner=indexedTable.readByIndex(Bytes.toBytes(column),value);
  }
  try {
    Row next;
    while ((next=scanner.next()) != null) {
      String rowValue=next.getString(VALUE_COLUMN);
      if (rowValue == null) {
        continue;
      }
      final byte[] rowKey=next.getRow();
      String targetType=MdsValueKey.getTargetType(rowKey);
      if ((type != MetadataSearchTargetType.ALL) && (!targetType.equals(type.getInternalName()))) {
        continue;
      }
      if (modifiedTagsKVSearch) {
        boolean isMatch=false;
        if ((TAGS_KEY + KEYVALUE_SEPARATOR + ""String_Node_Str"").equals(unmodifiedTagKeyValue)) {
          isMatch=true;
        }
 else {
          Iterable<String> tagValues=Splitter.on(TAGS_SEPARATOR).omitEmptyStrings().trimResults().split(rowValue);
          for (          String tagValue : tagValues) {
            String prefixedTagValue=TAGS_KEY + KEYVALUE_SEPARATOR + tagValue;
            if (!unmodifiedTagKeyValue.endsWith(""String_Node_Str"")) {
              if (prefixedTagValue.equals(unmodifiedTagKeyValue)) {
                isMatch=true;
                break;
              }
            }
 else {
              int endAsteriskIndex=unmodifiedTagKeyValue.lastIndexOf(""String_Node_Str"");
              if (prefixedTagValue.startsWith(unmodifiedTagKeyValue.substring(0,endAsteriskIndex))) {
                isMatch=true;
                break;
              }
            }
          }
        }
        if (!isMatch) {
          continue;
        }
      }
      Id.NamespacedId targetId=MdsValueKey.getNamespaceIdFromKey(targetType,new MDSKey(rowKey));
      String key=MdsValueKey.getMetadataKey(targetType,rowKey);
      String value=Bytes.toString(next.get(Bytes.toBytes(BusinessMetadataDataset.VALUE_COLUMN)));
      BusinessMetadataRecord record=new BusinessMetadataRecord(targetId,key,value);
      results.add(record);
    }
  }
  finally {
    scanner.close();
  }
  return results;
}"
6402,"@Override public Id.NamespacedId deserialize(JsonElement json,Type typeOfT,JsonDeserializationContext context) throws JsonParseException {
  JsonObject jsonObj=json.getAsJsonObject();
  JsonObject id=jsonObj.getAsJsonObject(""String_Node_Str"");
  String type=jsonObj.get(""String_Node_Str"").getAsString();
switch (type) {
case ""String_Node_Str"":
    return deserializeApplicationId(id);
case ""String_Node_Str"":
  return deserializeProgramId(id);
case ""String_Node_Str"":
return deserializeFlowId(id);
case ""String_Node_Str"":
return deserializeFlowletId(id);
case ""String_Node_Str"":
return deserializeServiceId(id);
case ""String_Node_Str"":
return deserializeSchedule(id);
case ""String_Node_Str"":
return deserializeWorkerId(id);
case ""String_Node_Str"":
return deserializeWorkflowId(id);
case ""String_Node_Str"":
return deserializeDatasetInstanceId(id);
case ""String_Node_Str"":
return deserializeStreamId(id);
case ""String_Node_Str"":
return deserializeArtifactId(id);
default :
throw new UnsupportedOperationException(String.format(""String_Node_Str"" + ""String_Node_Str"",type,Id.Application.class.getSimpleName(),Id.Program.class.getSimpleName(),Id.Flow.class.getSimpleName(),Id.Flow.Flowlet.class.getSimpleName(),Id.Service.class.getSimpleName(),Id.Schedule.class.getSimpleName(),Id.Worker.class.getSimpleName(),Id.Workflow.class.getSimpleName(),Id.DatasetInstance.class.getSimpleName(),Id.Stream.class.getSimpleName()));
}
}","@Override public Id.NamespacedId deserialize(JsonElement json,Type typeOfT,JsonDeserializationContext context) throws JsonParseException {
  JsonObject jsonObj=json.getAsJsonObject();
  JsonObject id=jsonObj.getAsJsonObject(""String_Node_Str"");
  String type=jsonObj.get(""String_Node_Str"").getAsString();
switch (type) {
case ""String_Node_Str"":
    return deserializeApplicationId(id);
case ""String_Node_Str"":
  return deserializeProgramId(id);
case ""String_Node_Str"":
return deserializeFlowId(id);
case ""String_Node_Str"":
return deserializeFlowletId(id);
case ""String_Node_Str"":
return deserializeServiceId(id);
case ""String_Node_Str"":
return deserializeSchedule(id);
case ""String_Node_Str"":
return deserializeWorkerId(id);
case ""String_Node_Str"":
return deserializeWorkflowId(id);
case ""String_Node_Str"":
return deserializeDatasetInstanceId(id);
case ""String_Node_Str"":
return deserializeStreamId(id);
case ""String_Node_Str"":
return deserializeArtifactId(id);
default :
throw new UnsupportedOperationException(String.format(""String_Node_Str"" + ""String_Node_Str"",type,Id.Application.class.getSimpleName(),Id.Program.class.getSimpleName(),Id.Flow.class.getSimpleName(),Id.Flow.Flowlet.class.getSimpleName(),Id.Service.class.getSimpleName(),Id.Schedule.class.getSimpleName(),Id.Worker.class.getSimpleName(),Id.Workflow.class.getSimpleName(),Id.DatasetInstance.class.getSimpleName(),Id.Stream.class.getSimpleName(),Id.Artifact.class.getSimpleName()));
}
}"
6403,"/** 
 * Sets up a   {@link DatasetFramework} instance for standalone usage.  NOTE: should NOT be used by applications!!!
 */
private void initializeDSFramework(CConfiguration cConf,DatasetFramework datasetFramework,boolean includeNewDatasets) throws IOException, DatasetManagementException {
  DatasetMetaTableUtil.setupDatasets(datasetFramework);
  if (includeNewDatasets) {
    ArtifactStore.setupDatasets(datasetFramework);
  }
  DefaultStore.setupDatasets(datasetFramework);
  DefaultConfigStore.setupDatasets(datasetFramework);
  LogSaverTableUtil.setupDatasets(datasetFramework);
  ScheduleStoreTableUtil.setupDatasets(datasetFramework);
  DefaultMetricDatasetFactory factory=new DefaultMetricDatasetFactory(cConf,datasetFramework);
  DefaultMetricDatasetFactory.setupDatasets(factory);
  UsageRegistry.setupDatasets(datasetFramework);
}","/** 
 * Sets up a   {@link DatasetFramework} instance for standalone usage.  NOTE: should NOT be used by applications!!!
 */
private void initializeDSFramework(CConfiguration cConf,DatasetFramework datasetFramework,boolean includeNewDatasets) throws IOException, DatasetManagementException {
  DatasetMetaTableUtil.setupDatasets(datasetFramework);
  if (includeNewDatasets) {
    ArtifactStore.setupDatasets(datasetFramework);
    DefaultBusinessMetadataStore.setupDatasets(datasetFramework);
    LineageStore.setupDatasets(datasetFramework);
  }
  DefaultStore.setupDatasets(datasetFramework);
  DefaultConfigStore.setupDatasets(datasetFramework);
  LogSaverTableUtil.setupDatasets(datasetFramework);
  ScheduleStoreTableUtil.setupDatasets(datasetFramework);
  DefaultMetricDatasetFactory factory=new DefaultMetricDatasetFactory(cConf,datasetFramework);
  DefaultMetricDatasetFactory.setupDatasets(factory);
  UsageRegistry.setupDatasets(datasetFramework);
}"
6404,"/** 
 * Constructs an instance by parsing the given string. If the string does not match the version pattern,  {@link #getVersion()} will return null.
 * @param str the version string.
 * @param matchSuffix if {@code true}, try to match the version pattern by the suffix of the string. Otherwise match the whole string.
 */
public ArtifactVersion(String str,boolean matchSuffix){
  String tmpVersion=null;
  Integer major=null;
  Integer minor=null;
  Integer fix=null;
  String suffix=null;
  if (str != null) {
    Matcher matcher=PATTERN.matcher(str);
    boolean matches=matchSuffix ? (matcher.find()) : matcher.matches();
    if (matches) {
      tmpVersion=matcher.group(0);
      major=valueOf(matcher.group(1));
      minor=valueOf(matcher.group(2));
      fix=valueOf(matcher.group(3));
      suffix=matcher.group(4);
    }
  }
  this.version=tmpVersion;
  this.major=major;
  this.minor=minor;
  this.fix=fix;
  this.suffix=suffix;
}","/** 
 * Constructs an instance by parsing the given string. If the string does not match the version pattern,  {@link #getVersion()} will return null.
 * @param str the version string.
 * @param matchSuffix if {@code true}, try to match the version pattern by the suffix of the string. Otherwise match the whole string.
 */
public ArtifactVersion(String str,boolean matchSuffix){
  String tmpVersion=null;
  Integer major=null;
  Integer minor=null;
  Integer fix=null;
  String suffix=null;
  if (str != null) {
    Matcher matcher=matchSuffix ? SUFFIX_PATTERN.matcher(str) : PATTERN.matcher(str);
    boolean matches=matchSuffix ? (matcher.find()) : matcher.matches();
    if (matches) {
      tmpVersion=matchSuffix ? matcher.group(0).substring(1) : matcher.group(0);
      major=valueOf(matcher.group(1));
      minor=valueOf(matcher.group(2));
      fix=valueOf(matcher.group(3));
      suffix=matcher.group(4);
    }
  }
  this.version=tmpVersion;
  this.major=major;
  this.minor=minor;
  this.fix=fix;
  this.suffix=suffix;
}"
6405,"private ProgramController startProgram(ApplicationWithPrograms app,Class<?> programClass) throws Throwable {
  final AtomicReference<Throwable> errorCause=new AtomicReference<>();
  final ProgramController controller=submit(app,programClass,RuntimeArguments.NO_ARGUMENTS);
  runningPrograms.add(controller);
  controller.addListener(new AbstractListener(){
    @Override public void error(    Throwable cause){
      errorCause.set(cause);
    }
    @Override public void killed(){
      errorCause.set(new RuntimeException(""String_Node_Str""));
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  Tasks.waitFor(ProgramController.State.ALIVE,new Callable<ProgramController.State>(){
    @Override public ProgramController.State call() throws Exception {
      Throwable t=errorCause.get();
      if (t != null) {
        Throwables.propagateIfInstanceOf(t,Exception.class);
        throw Throwables.propagate(t);
      }
      return controller.getState();
    }
  }
,30,TimeUnit.SECONDS,50,TimeUnit.MILLISECONDS);
  return controller;
}","private ProgramController startProgram(ApplicationWithPrograms app,Class<?> programClass) throws Throwable {
  final AtomicReference<Throwable> errorCause=new AtomicReference<>();
  final ProgramController controller=submit(app,programClass,RuntimeArguments.NO_ARGUMENTS);
  runningPrograms.add(controller);
  controller.addListener(new AbstractListener(){
    @Override public void error(    Throwable cause){
      errorCause.set(cause);
    }
    @Override public void killed(){
      errorCause.set(new RuntimeException(""String_Node_Str""));
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  Tasks.waitFor(ProgramController.State.ALIVE,new Callable<ProgramController.State>(){
    @Override public ProgramController.State call() throws Exception {
      Throwable t=errorCause.get();
      if (t != null) {
        Throwables.propagateIfInstanceOf(t,Exception.class);
        throw Throwables.propagate(t);
      }
      return controller.getState();
    }
  }
,30,TimeUnit.SECONDS);
  return controller;
}"
6406,"@Test public void testWorkerDatasetWithMetrics() throws Throwable {
  final ApplicationWithPrograms app=AppFabricTestHelper.deployApplicationWithManager(AppWithWorker.class,TEMP_FOLDER_SUPPLIER);
  ProgramController controller=startProgram(app,AppWithWorker.TableWriter.class);
  final TransactionExecutor executor=txExecutorFactory.createExecutor(datasetCache);
  Tasks.waitFor(AppWithWorker.RUN,new Callable<String>(){
    @Override public String call() throws Exception {
      return executor.execute(new Callable<String>(){
        @Override public String call() throws Exception {
          KeyValueTable kvTable=datasetCache.getDataset(AppWithWorker.DATASET);
          return Bytes.toString(kvTable.read(AppWithWorker.RUN));
        }
      }
);
    }
  }
,5,TimeUnit.SECONDS,50,TimeUnit.MILLISECONDS);
  stopProgram(controller);
  txExecutorFactory.createExecutor(datasetCache.getTransactionAwares()).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      KeyValueTable kvTable=datasetCache.getDataset(AppWithWorker.DATASET);
      Assert.assertEquals(AppWithWorker.RUN,Bytes.toString(kvTable.read(AppWithWorker.RUN)));
      Assert.assertEquals(AppWithWorker.INITIALIZE,Bytes.toString(kvTable.read(AppWithWorker.INITIALIZE)));
      Assert.assertEquals(AppWithWorker.STOP,Bytes.toString(kvTable.read(AppWithWorker.STOP)));
    }
  }
);
  Tasks.waitFor(3L,new Callable<Long>(){
    @Override public Long call() throws Exception {
      Collection<MetricTimeSeries> metrics=metricStore.query(new MetricDataQuery(0,System.currentTimeMillis() / 1000L,60,""String_Node_Str"" + Constants.Metrics.Name.Dataset.OP_COUNT,AggregationFunction.SUM,ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE,DefaultId.NAMESPACE.getId(),Constants.Metrics.Tag.APP,AppWithWorker.NAME,Constants.Metrics.Tag.WORKER,AppWithWorker.WORKER,Constants.Metrics.Tag.DATASET,AppWithWorker.DATASET),Collections.<String>emptyList()));
      if (metrics.isEmpty()) {
        return 0L;
      }
      Assert.assertEquals(1,metrics.size());
      MetricTimeSeries ts=metrics.iterator().next();
      Assert.assertEquals(1,ts.getTimeValues().size());
      return ts.getTimeValues().get(0).getValue();
    }
  }
,5L,TimeUnit.SECONDS,50L,TimeUnit.MILLISECONDS);
}","@Test public void testWorkerDatasetWithMetrics() throws Throwable {
  final ApplicationWithPrograms app=AppFabricTestHelper.deployApplicationWithManager(AppWithWorker.class,TEMP_FOLDER_SUPPLIER);
  ProgramController controller=startProgram(app,AppWithWorker.TableWriter.class);
  final TransactionExecutor executor=txExecutorFactory.createExecutor(datasetCache);
  Tasks.waitFor(AppWithWorker.RUN,new Callable<String>(){
    @Override public String call() throws Exception {
      return executor.execute(new Callable<String>(){
        @Override public String call() throws Exception {
          KeyValueTable kvTable=datasetCache.getDataset(AppWithWorker.DATASET);
          return Bytes.toString(kvTable.read(AppWithWorker.RUN));
        }
      }
);
    }
  }
,5,TimeUnit.SECONDS);
  stopProgram(controller);
  txExecutorFactory.createExecutor(datasetCache.getTransactionAwares()).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      KeyValueTable kvTable=datasetCache.getDataset(AppWithWorker.DATASET);
      Assert.assertEquals(AppWithWorker.RUN,Bytes.toString(kvTable.read(AppWithWorker.RUN)));
      Assert.assertEquals(AppWithWorker.INITIALIZE,Bytes.toString(kvTable.read(AppWithWorker.INITIALIZE)));
      Assert.assertEquals(AppWithWorker.STOP,Bytes.toString(kvTable.read(AppWithWorker.STOP)));
    }
  }
);
  Tasks.waitFor(3L,new Callable<Long>(){
    @Override public Long call() throws Exception {
      Collection<MetricTimeSeries> metrics=metricStore.query(new MetricDataQuery(0,System.currentTimeMillis() / 1000L,60,""String_Node_Str"" + Constants.Metrics.Name.Dataset.OP_COUNT,AggregationFunction.SUM,ImmutableMap.of(Constants.Metrics.Tag.NAMESPACE,DefaultId.NAMESPACE.getId(),Constants.Metrics.Tag.APP,AppWithWorker.NAME,Constants.Metrics.Tag.WORKER,AppWithWorker.WORKER,Constants.Metrics.Tag.DATASET,AppWithWorker.DATASET),Collections.<String>emptyList()));
      if (metrics.isEmpty()) {
        return 0L;
      }
      Assert.assertEquals(1,metrics.size());
      MetricTimeSeries ts=metrics.iterator().next();
      Assert.assertEquals(1,ts.getTimeValues().size());
      return ts.getTimeValues().get(0).getValue();
    }
  }
,5L,TimeUnit.SECONDS,50L,TimeUnit.MILLISECONDS);
}"
6407,"/** 
 * Waits for the given program to transit to the given state.
 */
protected void waitState(final Id.Program programId,String state) throws Exception {
  Tasks.waitFor(state,new Callable<String>(){
    @Override public String call() throws Exception {
      String path=String.format(""String_Node_Str"",programId.getApplicationId(),programId.getType().getCategoryName(),programId.getId());
      HttpResponse response=doGet(getVersionedAPIPath(path,programId.getNamespaceId()));
      JsonObject status=GSON.fromJson(EntityUtils.toString(response.getEntity()),JsonObject.class);
      if (status == null || !status.has(""String_Node_Str"")) {
        return null;
      }
      return status.get(""String_Node_Str"").getAsString();
    }
  }
,60,TimeUnit.SECONDS,50,TimeUnit.MILLISECONDS);
}","/** 
 * Waits for the given program to transit to the given state.
 */
protected void waitState(final Id.Program programId,String state) throws Exception {
  Tasks.waitFor(state,new Callable<String>(){
    @Override public String call() throws Exception {
      String path=String.format(""String_Node_Str"",programId.getApplicationId(),programId.getType().getCategoryName(),programId.getId());
      HttpResponse response=doGet(getVersionedAPIPath(path,programId.getNamespaceId()));
      JsonObject status=GSON.fromJson(EntityUtils.toString(response.getEntity()),JsonObject.class);
      if (status == null || !status.has(""String_Node_Str"")) {
        return null;
      }
      return status.get(""String_Node_Str"").getAsString();
    }
  }
,60,TimeUnit.SECONDS);
}"
6408,"protected void verifyProgramRuns(final Id.Program program,final String status,final int expected) throws Exception {
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return getProgramRuns(program,status).size() > expected;
    }
  }
,60,TimeUnit.SECONDS,50,TimeUnit.MILLISECONDS);
}","protected void verifyProgramRuns(final Id.Program program,final String status,final int expected) throws Exception {
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return getProgramRuns(program,status).size() > expected;
    }
  }
,60,TimeUnit.SECONDS);
}"
6409,"private void verifyRunningProgramCount(final Id.Program program,final String runId,final int expected) throws Exception {
  Tasks.waitFor(expected,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return runningProgramCount(program,runId);
    }
  }
,10,TimeUnit.SECONDS,50,TimeUnit.MILLISECONDS);
}","private void verifyRunningProgramCount(final Id.Program program,final String runId,final int expected) throws Exception {
  Tasks.waitFor(expected,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return runningProgramCount(program,runId);
    }
  }
,10,TimeUnit.SECONDS);
}"
6410,"private void verifyFileExists(final List<File> fileList) throws Exception {
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      for (      File file : fileList) {
        if (!file.exists()) {
          return false;
        }
      }
      return true;
    }
  }
,180,TimeUnit.SECONDS,50,TimeUnit.MILLISECONDS);
}","private void verifyFileExists(final List<File> fileList) throws Exception {
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      for (      File file : fileList) {
        if (!file.exists()) {
          return false;
        }
      }
      return true;
    }
  }
,180,TimeUnit.SECONDS);
}"
6411,"private void waitForTableToBePopulated(final DataSetManager<Table> tableManager) throws Exception {
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      tableManager.flush();
      Table table=tableManager.get();
      Row row=table.get(""String_Node_Str"".getBytes(Charsets.UTF_8));
      return row.getColumns().size() != 0;
    }
  }
,10,TimeUnit.SECONDS,50,TimeUnit.MILLISECONDS);
}","private void waitForTableToBePopulated(final DataSetManager<Table> tableManager) throws Exception {
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      tableManager.flush();
      Table table=tableManager.get();
      Row row=table.get(""String_Node_Str"".getBytes(Charsets.UTF_8));
      return row.getColumns().size() != 0;
    }
  }
,10,TimeUnit.SECONDS);
}"
6412,"@Test public void test() throws Exception {
  ETLStage source=new ETLStage(""String_Node_Str"",ImmutableMap.of(DataGeneratorSource.PROPERTY_TYPE,DataGeneratorSource.TABLE_TYPE));
  Map<String,String> datasetProps=ImmutableMap.of(CubeDatasetDefinition.PROPERTY_AGGREGATION_PREFIX + ""String_Node_Str"",""String_Node_Str"");
  Map<String,String> measurementsProps=ImmutableMap.of(Properties.Cube.MEASUREMENT_PREFIX + ""String_Node_Str"",""String_Node_Str"");
  ETLStage sink=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.Cube.DATASET_NAME,""String_Node_Str"",Properties.Cube.DATASET_OTHER,new Gson().toJson(datasetProps),Properties.Cube.MEASUREMENTS,new Gson().toJson(measurementsProps)));
  ETLRealtimeConfig etlConfig=new ETLRealtimeConfig(source,sink,Lists.<ETLStage>newArrayList());
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  AppRequest<ETLRealtimeConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkerManager workerManager=appManager.getWorkerManager(ETLWorker.NAME);
  final long startTs=System.currentTimeMillis() / 1000;
  workerManager.start();
  final DataSetManager<Cube> tableManager=getDataset(""String_Node_Str"");
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      tableManager.flush();
      Cube cube=tableManager.get();
      Collection<TimeSeries> result=cube.query(buildCubeQuery(startTs));
      return !result.isEmpty();
    }
  }
,10,TimeUnit.SECONDS,50,TimeUnit.MILLISECONDS);
  workerManager.stop();
  Cube cube=tableManager.get();
  Collection<TimeSeries> result=cube.query(buildCubeQuery(startTs));
  Iterator<TimeSeries> iterator=result.iterator();
  Assert.assertTrue(iterator.hasNext());
  TimeSeries timeSeries=iterator.next();
  Assert.assertEquals(""String_Node_Str"",timeSeries.getMeasureName());
  Assert.assertFalse(timeSeries.getTimeValues().isEmpty());
  Assert.assertEquals(3,timeSeries.getTimeValues().get(0).getValue());
  Assert.assertFalse(iterator.hasNext());
}","@Test public void test() throws Exception {
  ETLStage source=new ETLStage(""String_Node_Str"",ImmutableMap.of(DataGeneratorSource.PROPERTY_TYPE,DataGeneratorSource.TABLE_TYPE));
  Map<String,String> datasetProps=ImmutableMap.of(CubeDatasetDefinition.PROPERTY_AGGREGATION_PREFIX + ""String_Node_Str"",""String_Node_Str"");
  Map<String,String> measurementsProps=ImmutableMap.of(Properties.Cube.MEASUREMENT_PREFIX + ""String_Node_Str"",""String_Node_Str"");
  ETLStage sink=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.Cube.DATASET_NAME,""String_Node_Str"",Properties.Cube.DATASET_OTHER,new Gson().toJson(datasetProps),Properties.Cube.MEASUREMENTS,new Gson().toJson(measurementsProps)));
  ETLRealtimeConfig etlConfig=new ETLRealtimeConfig(source,sink,Lists.<ETLStage>newArrayList());
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  AppRequest<ETLRealtimeConfig> appRequest=new AppRequest<>(APP_ARTIFACT,etlConfig);
  ApplicationManager appManager=deployApplication(appId,appRequest);
  WorkerManager workerManager=appManager.getWorkerManager(ETLWorker.NAME);
  final long startTs=System.currentTimeMillis() / 1000;
  workerManager.start();
  final DataSetManager<Cube> tableManager=getDataset(""String_Node_Str"");
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      tableManager.flush();
      Cube cube=tableManager.get();
      Collection<TimeSeries> result=cube.query(buildCubeQuery(startTs));
      return !result.isEmpty();
    }
  }
,10,TimeUnit.SECONDS);
  workerManager.stop();
  Cube cube=tableManager.get();
  Collection<TimeSeries> result=cube.query(buildCubeQuery(startTs));
  Iterator<TimeSeries> iterator=result.iterator();
  Assert.assertTrue(iterator.hasNext());
  TimeSeries timeSeries=iterator.next();
  Assert.assertEquals(""String_Node_Str"",timeSeries.getMeasureName());
  Assert.assertFalse(timeSeries.getTimeValues().isEmpty());
  Assert.assertEquals(3,timeSeries.getTimeValues().get(0).getValue());
  Assert.assertFalse(iterator.hasNext());
}"
6413,"/** 
 * Calls callable, waiting sleepDelay between each call, until it returns the desiredValue or the timeout has passed.
 * @param desiredValue the desired value to get from callable
 * @param callable the callable to check
 * @param timeout time until we timeout
 * @param timeoutUnit unit of time for timeout
 * @param sleepDelay time to wait between calls to callable
 * @param sleepDelayUnit unit of time for sleepDelay
 * @param < T > type of desiredValue
 * @throws TimeoutException if timeout has passed, but didn't get the desiredValue
 * @throws InterruptedException if something interrupted this waiting operation
 * @throws ExecutionException if there was an exception in calling the callable
 */
public static <T>void waitFor(T desiredValue,Callable<T> callable,long timeout,TimeUnit timeoutUnit,long sleepDelay,TimeUnit sleepDelayUnit) throws TimeoutException, InterruptedException, ExecutionException {
  long sleepDelayMs=sleepDelayUnit.toMillis(sleepDelay);
  long startTime=System.currentTimeMillis();
  long timeoutMs=timeoutUnit.toMillis(timeout);
  while (System.currentTimeMillis() - startTime < timeoutMs) {
    try {
      if (desiredValue.equals(callable.call())) {
        return;
      }
    }
 catch (    Exception e) {
      throw new ExecutionException(e);
    }
    Thread.sleep(sleepDelayMs);
  }
  throw new TimeoutException();
}","/** 
 * Calls callable, waiting 50 milliseconds between each call, until it returns the desiredValue or the timeout has passed.
 * @param desiredValue the desired value to get from callable
 * @param callable the callable to check
 * @param timeout time until we timeout
 * @param timeoutUnit unit of time for timeout
 * @param < T > type of desiredValue
 * @throws TimeoutException if timeout has passed, but didn't get the desiredValue
 * @throws InterruptedException if something interrupted this waiting operation
 * @throws ExecutionException if there was an exception in calling the callable
 */
public static <T>void waitFor(T desiredValue,Callable<T> callable,long timeout,TimeUnit timeoutUnit) throws TimeoutException, InterruptedException, ExecutionException {
  waitFor(desiredValue,callable,timeout,timeoutUnit,50,TimeUnit.MILLISECONDS);
}"
6414,"private void waitForStreamToBePopulated(final StreamManager streamManager,int numEvents) throws Exception {
  Tasks.waitFor(numEvents,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      List<StreamEvent> streamEvents=streamManager.getEvents(0,Long.MAX_VALUE,Integer.MAX_VALUE);
      return streamEvents.size();
    }
  }
,10,TimeUnit.SECONDS,50,TimeUnit.MILLISECONDS);
}","private void waitForStreamToBePopulated(final StreamManager streamManager,int numEvents) throws Exception {
  Tasks.waitFor(numEvents,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      List<StreamEvent> streamEvents=streamManager.getEvents(0,Long.MAX_VALUE,Integer.MAX_VALUE);
      return streamEvents.size();
    }
  }
,10,TimeUnit.SECONDS);
}"
6415,"@Test public void useTransactionTest() throws Exception {
  namespaceClient.create(new NamespaceMeta.Builder().setName(namespace).build());
  Id.DatasetInstance myTableInstance=Id.DatasetInstance.from(namespace,""String_Node_Str"");
  dsFramework.addInstance(""String_Node_Str"",myTableInstance,DatasetProperties.EMPTY);
  final CountDownLatch receivedLatch=new CountDownLatch(1);
  Assert.assertTrue(feedManager.createFeed(FEED1));
  try {
    Cancellable cancellable=notificationService.subscribe(FEED1,new NotificationHandler<String>(){
      private int received=0;
      @Override public Type getNotificationType(){
        return String.class;
      }
      @Override public void received(      final String notification,      NotificationContext notificationContext){
        notificationContext.execute(new TxRunnable(){
          @Override public void run(          DatasetContext context) throws Exception {
            KeyValueTable table=context.getDataset(""String_Node_Str"");
            table.write(""String_Node_Str"",String.format(""String_Node_Str"",notification,received++));
            receivedLatch.countDown();
          }
        }
,TxRetryPolicy.maxRetries(5));
      }
    }
);
    TimeUnit.MILLISECONDS.sleep(500);
    try {
      notificationService.publish(FEED1,""String_Node_Str"");
      Assert.assertTrue(receivedLatch.await(5,TimeUnit.SECONDS));
      final KeyValueTable table=dsFramework.getDataset(myTableInstance,DatasetDefinition.NO_ARGUMENTS,null);
      Assert.assertNotNull(table);
      final TransactionContext txContext=new TransactionContext(txClient,table);
      Tasks.waitFor(true,new Callable<Boolean>(){
        @Override public Boolean call() throws Exception {
          txContext.start();
          try {
            return ""String_Node_Str"".equals(Bytes.toString(table.read(""String_Node_Str"")));
          }
  finally {
            txContext.finish();
          }
        }
      }
,5,TimeUnit.SECONDS,50,TimeUnit.MILLISECONDS);
    }
  finally {
      cancellable.cancel();
    }
  }
  finally {
    dsFramework.deleteInstance(myTableInstance);
    feedManager.deleteFeed(FEED1);
    namespaceClient.delete(namespace);
  }
}","@Test public void useTransactionTest() throws Exception {
  namespaceClient.create(new NamespaceMeta.Builder().setName(namespace).build());
  Id.DatasetInstance myTableInstance=Id.DatasetInstance.from(namespace,""String_Node_Str"");
  dsFramework.addInstance(""String_Node_Str"",myTableInstance,DatasetProperties.EMPTY);
  final CountDownLatch receivedLatch=new CountDownLatch(1);
  Assert.assertTrue(feedManager.createFeed(FEED1));
  try {
    Cancellable cancellable=notificationService.subscribe(FEED1,new NotificationHandler<String>(){
      private int received=0;
      @Override public Type getNotificationType(){
        return String.class;
      }
      @Override public void received(      final String notification,      NotificationContext notificationContext){
        notificationContext.execute(new TxRunnable(){
          @Override public void run(          DatasetContext context) throws Exception {
            KeyValueTable table=context.getDataset(""String_Node_Str"");
            table.write(""String_Node_Str"",String.format(""String_Node_Str"",notification,received++));
            receivedLatch.countDown();
          }
        }
,TxRetryPolicy.maxRetries(5));
      }
    }
);
    TimeUnit.MILLISECONDS.sleep(500);
    try {
      notificationService.publish(FEED1,""String_Node_Str"");
      Assert.assertTrue(receivedLatch.await(5,TimeUnit.SECONDS));
      final KeyValueTable table=dsFramework.getDataset(myTableInstance,DatasetDefinition.NO_ARGUMENTS,null);
      Assert.assertNotNull(table);
      final TransactionContext txContext=new TransactionContext(txClient,table);
      Tasks.waitFor(true,new Callable<Boolean>(){
        @Override public Boolean call() throws Exception {
          txContext.start();
          try {
            return ""String_Node_Str"".equals(Bytes.toString(table.read(""String_Node_Str"")));
          }
  finally {
            txContext.finish();
          }
        }
      }
,5,TimeUnit.SECONDS);
    }
  finally {
      cancellable.cancel();
    }
  }
  finally {
    dsFramework.deleteInstance(myTableInstance);
    feedManager.deleteFeed(FEED1);
    namespaceClient.delete(namespace);
  }
}"
6416,"private void waitForSuccessfulPing(final URL serviceUrl) throws InterruptedException, ExecutionException, TimeoutException {
  Tasks.waitFor(HttpURLConnection.HTTP_OK,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      HttpURLConnection conn=(HttpURLConnection)serviceUrl.openConnection();
      try {
        return conn.getResponseCode();
      }
  finally {
        conn.disconnect();
      }
    }
  }
,10,TimeUnit.SECONDS,50,TimeUnit.MILLISECONDS);
}","private void waitForSuccessfulPing(final URL serviceUrl) throws InterruptedException, ExecutionException, TimeoutException {
  Tasks.waitFor(HttpURLConnection.HTTP_OK,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      HttpURLConnection conn=(HttpURLConnection)serviceUrl.openConnection();
      try {
        return conn.getResponseCode();
      }
  finally {
        conn.disconnect();
      }
    }
  }
,10,TimeUnit.SECONDS);
}"
6417,"private void waitForGetServiceUrl() throws InterruptedException, ExecutionException, TimeoutException {
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return getContext().getServiceURL(CENTRAL_SERVICE) != null;
    }
  }
,10,TimeUnit.SECONDS,50,TimeUnit.MILLISECONDS);
}","private void waitForGetServiceUrl() throws InterruptedException, ExecutionException, TimeoutException {
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return getContext().getServiceURL(CENTRAL_SERVICE) != null;
    }
  }
,10,TimeUnit.SECONDS);
}"
6418,"@Test public void testAppWithPlugin() throws Exception {
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  addAppArtifact(artifactId,AppWithPlugin.class);
  Id.Artifact pluginArtifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  addPluginArtifact(pluginArtifactId,artifactId,ToStringPlugin.class);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  AppRequest createRequest=new AppRequest(new ArtifactSummary(artifactId.getName(),artifactId.getVersion().getVersion()));
  ApplicationManager appManager=deployApplication(appId,createRequest);
  WorkerManager workerManager=appManager.getWorkerManager(AppWithPlugin.WORKER);
  workerManager.start();
  workerManager.waitForStatus(false,5,1);
  List<RunRecord> workerRun=workerManager.getHistory(ProgramRunStatus.COMPLETED);
  Assert.assertFalse(workerRun.isEmpty());
  ServiceManager serviceManager=appManager.getServiceManager(AppWithPlugin.SERVICE);
  serviceManager.start();
  serviceManager.waitForStatus(true,1,10);
  URL serviceURL=serviceManager.getServiceURL(5,TimeUnit.SECONDS);
  callServiceGet(serviceURL,""String_Node_Str"");
  serviceManager.stop();
  serviceManager.waitForStatus(false,1,10);
  List<RunRecord> serviceRun=serviceManager.getHistory(ProgramRunStatus.KILLED);
  Assert.assertFalse(serviceRun.isEmpty());
  MapReduceManager mrManager=appManager.getMapReduceManager(AppWithPlugin.MAPREDUCE);
  mrManager.start();
  mrManager.waitForFinish(10,TimeUnit.MINUTES);
  List<RunRecord> runRecords=mrManager.getHistory();
  Assert.assertNotEquals(ProgramRunStatus.FAILED,runRecords.get(0).getStatus());
  StreamManager streamManager=getStreamManager(AppWithPlugin.SPARK_STREAM);
  for (int i=0; i < 5; i++) {
    streamManager.send(""String_Node_Str"" + i);
  }
  SparkManager sparkManager=appManager.getSparkManager(AppWithPlugin.SPARK).start();
  sparkManager.waitForFinish(2,TimeUnit.MINUTES);
  DataSetManager<Table> dataSetManager=getDataset(AppWithPlugin.SPARK_TABLE);
  Table table=dataSetManager.get();
  Scanner scanner=table.scan(null,null);
  try {
    for (int i=0; i < 5; i++) {
      Row row=scanner.next();
      Assert.assertNotNull(row);
      String expected=""String_Node_Str"" + i + ""String_Node_Str""+ AppWithPlugin.TEST;
      Assert.assertEquals(expected,Bytes.toString(row.getRow()));
      Assert.assertEquals(expected,Bytes.toString(row.get(expected)));
    }
    Assert.assertNull(scanner.next());
  }
  finally {
    scanner.close();
  }
}","@Test public void testAppWithPlugin() throws Exception {
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  addAppArtifact(artifactId,AppWithPlugin.class);
  Id.Artifact pluginArtifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  addPluginArtifact(pluginArtifactId,artifactId,ToStringPlugin.class);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  AppRequest createRequest=new AppRequest(new ArtifactSummary(artifactId.getName(),artifactId.getVersion().getVersion()));
  ApplicationManager appManager=deployApplication(appId,createRequest);
  final WorkerManager workerManager=appManager.getWorkerManager(AppWithPlugin.WORKER);
  workerManager.start();
  workerManager.waitForStatus(false,5,1);
  Tasks.waitFor(false,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return workerManager.getHistory(ProgramRunStatus.COMPLETED).isEmpty();
    }
  }
,5,TimeUnit.SECONDS,10,TimeUnit.MILLISECONDS);
  final ServiceManager serviceManager=appManager.getServiceManager(AppWithPlugin.SERVICE);
  serviceManager.start();
  serviceManager.waitForStatus(true,1,10);
  URL serviceURL=serviceManager.getServiceURL(5,TimeUnit.SECONDS);
  callServiceGet(serviceURL,""String_Node_Str"");
  serviceManager.stop();
  serviceManager.waitForStatus(false,1,10);
  Tasks.waitFor(false,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      return serviceManager.getHistory(ProgramRunStatus.KILLED).isEmpty();
    }
  }
,5,TimeUnit.SECONDS,10,TimeUnit.MILLISECONDS);
  MapReduceManager mrManager=appManager.getMapReduceManager(AppWithPlugin.MAPREDUCE);
  mrManager.start();
  mrManager.waitForFinish(10,TimeUnit.MINUTES);
  List<RunRecord> runRecords=mrManager.getHistory();
  Assert.assertNotEquals(ProgramRunStatus.FAILED,runRecords.get(0).getStatus());
  StreamManager streamManager=getStreamManager(AppWithPlugin.SPARK_STREAM);
  for (int i=0; i < 5; i++) {
    streamManager.send(""String_Node_Str"" + i);
  }
  SparkManager sparkManager=appManager.getSparkManager(AppWithPlugin.SPARK).start();
  sparkManager.waitForFinish(2,TimeUnit.MINUTES);
  DataSetManager<Table> dataSetManager=getDataset(AppWithPlugin.SPARK_TABLE);
  Table table=dataSetManager.get();
  Scanner scanner=table.scan(null,null);
  try {
    for (int i=0; i < 5; i++) {
      Row row=scanner.next();
      Assert.assertNotNull(row);
      String expected=""String_Node_Str"" + i + ""String_Node_Str""+ AppWithPlugin.TEST;
      Assert.assertEquals(expected,Bytes.toString(row.getRow()));
      Assert.assertEquals(expected,Bytes.toString(row.get(expected)));
    }
    Assert.assertNull(scanner.next());
  }
  finally {
    scanner.close();
  }
}"
6419,"/** 
 * Checks to ensure that a particular    {@param workerManager} has {@param expected} number ofinstances while retrying every 50 ms for 15 seconds.
 * @throws Exception if the worker does not have the specified number of instances after 15 seconds.
 */
private void workerInstancesCheck(final WorkerManager workerManager,int expected) throws Exception {
  Tasks.waitFor(expected,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return workerManager.getInstances();
    }
  }
,15,TimeUnit.SECONDS,50,TimeUnit.MILLISECONDS);
}","/** 
 * Checks to ensure that a particular    {@param workerManager} has {@param expected} number ofinstances while retrying every 50 ms for 15 seconds.
 * @throws Exception if the worker does not have the specified number of instances after 15 seconds.
 */
private void workerInstancesCheck(final WorkerManager workerManager,int expected) throws Exception {
  Tasks.waitFor(expected,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return workerManager.getInstances();
    }
  }
,15,TimeUnit.SECONDS);
}"
6420,"/** 
 * Localizes resources requested by users in the Spark Program's client (or beforeSubmit) phases. In Local mode, also copies resources to a temporary directory.
 * @param contextConfig the {@link SparkContextConfig} for this Spark program
 * @param clientContext the {@link ClientSparkContext} for this Spark program
 * @param allLocalizedResources the list of all (user-requested + CDAP system) {@link LocalizeResource} to belocalized for this Spark program
 * @param targetDir in local mode, a temporary directory to copy the resources to
 * @return a {@link Map} of resource name to the {@link File} handle for the local file.
 */
private Map<String,File> localizeUserResources(SparkContextConfig contextConfig,ClientSparkContext clientContext,List<LocalizeResource> allLocalizedResources,File targetDir) throws IOException {
  Map<String,File> localizedResources=new HashMap<>();
  Map<String,LocalizeResource> resourcesToLocalize=clientContext.getResourcesToLocalize();
  for (  final Map.Entry<String,LocalizeResource> entry : resourcesToLocalize.entrySet()) {
    final File localizedFile;
    if (contextConfig.isLocal()) {
      localizedFile=LocalizationUtils.localizeResource(entry.getKey(),entry.getValue(),targetDir);
    }
 else {
      allLocalizedResources.add(entry.getValue());
      localizedFile=new File(entry.getKey());
    }
    localizedResources.put(entry.getKey(),localizedFile);
  }
  return localizedResources;
}","/** 
 * Localizes resources requested by users in the Spark Program's client (or beforeSubmit) phases.
 * @param contextConfig the {@link SparkContextConfig} for this Spark program
 * @param clientContext the {@link ClientSparkContext} for this Spark program
 * @param allLocalizedResources the list of all (user-requested + CDAP system) {@link LocalizeResource} to belocalized for this Spark program
 * @param targetDir a temporary directory to copy the resources to
 * @return a {@link Map} of resource name to the {@link File} handle for the local file.
 */
private Map<String,File> localizeUserResources(SparkContextConfig contextConfig,ClientSparkContext clientContext,List<LocalizeResource> allLocalizedResources,File targetDir) throws IOException {
  Map<String,File> localizedResources=new HashMap<>();
  Map<String,LocalizeResource> resourcesToLocalize=clientContext.getResourcesToLocalize();
  for (  final Map.Entry<String,LocalizeResource> entry : resourcesToLocalize.entrySet()) {
    File localizedFile=LocalizationUtils.localizeResource(entry.getKey(),entry.getValue(),targetDir);
    if (!contextConfig.isLocal()) {
      try {
        URI uri=localizedFile.toURI();
        URI actualURI=new URI(uri.getScheme(),uri.getAuthority(),uri.getPath(),uri.getQuery(),entry.getKey());
        allLocalizedResources.add(new LocalizeResource(actualURI,entry.getValue().isArchive()));
      }
 catch (      URISyntaxException e) {
        throw Throwables.propagate(e);
      }
    }
    localizedResources.put(entry.getKey(),localizedFile);
  }
  return localizedResources;
}"
6421,"@Override public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  RunRequirements that=(RunRequirements)o;
  return Objects.equals(concurrentProgramRunsThreshold,that.concurrentProgramRunsThreshold);
}","@Override public boolean equals(Object other){
  if (this == other) {
    return true;
  }
  if (other == null || getClass() != other.getClass()) {
    return false;
  }
  RunRequirements that=(RunRequirements)other;
  return Objects.equals(concurrentProgramRunsThreshold,that.concurrentProgramRunsThreshold);
}"
6422,"public RunRequirements getRunRequirements(){
  return runRequirements;
}","public RunRequirements getRunRequirements(){
  return runRequirements == null ? RunRequirements.NONE : runRequirements;
}"
6423,"/** 
 * Checks if the run requirements for the specified schedule and program are all satisfied.
 * @param programId the id of the program to check
 * @param schedule the schedule to check
 * @return whether all run requirements are satisfied
 */
public boolean checkSatisfied(Id.Program programId,Schedule schedule){
  Integer programThreshold=schedule.getRunRequirements().getConcurrentProgramRunsThreshold();
  if (programThreshold != null) {
    try {
      List<RunRecordMeta> running=store.getRuns(programId,ProgramRunStatus.RUNNING,0,Long.MAX_VALUE,programThreshold + 1);
      if (running.size() > programThreshold) {
        LOG.info(""String_Node_Str"",programId,programThreshold);
        return false;
      }
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",programId,e);
      return false;
    }
  }
  return true;
}","/** 
 * Checks if the run requirements for the specified schedule and program are all satisfied.
 * @param programId the id of the program to check
 * @param schedule the schedule to check
 * @return whether all run requirements are satisfied
 */
public boolean checkSatisfied(Id.Program programId,Schedule schedule){
  Integer programThreshold=schedule.getRunRequirements().getConcurrentProgramRunsThreshold();
  if (programThreshold != null) {
    try {
      int max=programThreshold == Integer.MAX_VALUE ? Integer.MAX_VALUE : programThreshold + 1;
      List<RunRecordMeta> running=store.getRuns(programId,ProgramRunStatus.RUNNING,0,Long.MAX_VALUE,max);
      if (running.size() > programThreshold) {
        LOG.info(""String_Node_Str"",programId,programThreshold);
        return false;
      }
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"",programId,e);
      return false;
    }
  }
  return true;
}"
6424,"/** 
 * Constructs a ClassLoader based on the given   {@link Parameters} and also uses the given{@link TaskContextProviderFactory} to create {@link MapReduceTaskContextProvider} on demand.
 */
private MapReduceClassLoader(final Parameters parameters,final TaskContextProviderFactory contextProviderFactory){
  super(null,createDelegates(parameters));
  this.parameters=parameters;
  this.taskContextProviderSupplier=Suppliers.memoize(new Supplier<MapReduceTaskContextProvider>(){
    @Override public MapReduceTaskContextProvider get(){
      return contextProviderFactory.create(parameters.getCConf(),parameters.getHConf());
    }
  }
);
}","/** 
 * Constructs a ClassLoader based on the given   {@link Parameters} and also uses the given{@link TaskContextProviderFactory} to create {@link MapReduceTaskContextProvider} on demand.
 */
private MapReduceClassLoader(final Parameters parameters,final TaskContextProviderFactory contextProviderFactory){
  super(null,createDelegates(parameters));
  this.parameters=parameters;
  this.taskContextProviderSupplier=new Supplier<MapReduceTaskContextProvider>(){
    @Override public MapReduceTaskContextProvider get(){
      return contextProviderFactory.create(parameters.getCConf(),parameters.getHConf());
    }
  }
;
}"
6425,"/** 
 * Returns the   {@link MapReduceTaskContextProvider} associated with this ClassLoader.
 */
public MapReduceTaskContextProvider getTaskContextProvider(){
  MapReduceTaskContextProvider provider=taskContextProviderSupplier.get();
  provider.startAndWait();
  return provider;
}","/** 
 * Returns the   {@link MapReduceTaskContextProvider} associated with this ClassLoader.
 */
public MapReduceTaskContextProvider getTaskContextProvider(){
synchronized (this) {
    taskContextProvider=Optional.fromNullable(taskContextProvider).or(taskContextProviderSupplier);
  }
  taskContextProvider.startAndWait();
  return taskContextProvider;
}"
6426,"@Override public void close() throws Exception {
  try {
    MapReduceTaskContextProvider provider=taskContextProviderSupplier.get();
    Service.State state=provider.state();
    if (state == Service.State.STARTING || state == Service.State.RUNNING) {
      provider.stopAndWait();
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
  }
}","@Override public void close() throws Exception {
  try {
    MapReduceTaskContextProvider provider;
synchronized (this) {
      provider=taskContextProvider;
    }
    if (provider != null) {
      Service.State state=provider.state();
      if (state == Service.State.STARTING || state == Service.State.RUNNING) {
        provider.stopAndWait();
      }
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
  }
}"
6427,"@Override public void run(){
  for (  Object resource : resources) {
    if (resource == null) {
      continue;
    }
    try {
      if (resource instanceof File) {
        if (((File)resource).isDirectory()) {
          DirUtils.deleteDirectoryContents((File)resource);
        }
 else {
          ((File)resource).delete();
        }
      }
 else       if (resource instanceof Location) {
        Locations.deleteQuietly((Location)resource);
      }
 else       if (resource instanceof AutoCloseable) {
        ((AutoCloseable)resource).close();
      }
 else       if (resource instanceof Runnable) {
        ((Runnable)resource).run();
      }
    }
 catch (    Throwable t) {
      LOG.warn(""String_Node_Str"",resource,t);
    }
  }
}","@Override public void run(){
  for (  Object resource : resources) {
    if (resource == null) {
      continue;
    }
    try {
      if (resource instanceof File) {
        if (((File)resource).isDirectory()) {
          DirUtils.deleteDirectoryContents((File)resource);
        }
 else {
          ((File)resource).delete();
        }
      }
 else       if (resource instanceof Location) {
        Locations.deleteQuietly((Location)resource,true);
      }
 else       if (resource instanceof AutoCloseable) {
        ((AutoCloseable)resource).close();
      }
 else       if (resource instanceof Runnable) {
        ((Runnable)resource).run();
      }
    }
 catch (    Throwable t) {
      LOG.warn(""String_Node_Str"",resource,t);
    }
  }
}"
6428,"private Runnable createCleanupTask(final Object... resources){
  return new Runnable(){
    @Override public void run(){
      for (      Object resource : resources) {
        if (resource == null) {
          continue;
        }
        try {
          if (resource instanceof File) {
            if (((File)resource).isDirectory()) {
              DirUtils.deleteDirectoryContents((File)resource);
            }
 else {
              ((File)resource).delete();
            }
          }
 else           if (resource instanceof Location) {
            Locations.deleteQuietly((Location)resource);
          }
 else           if (resource instanceof AutoCloseable) {
            ((AutoCloseable)resource).close();
          }
 else           if (resource instanceof Runnable) {
            ((Runnable)resource).run();
          }
        }
 catch (        Throwable t) {
          LOG.warn(""String_Node_Str"",resource,t);
        }
      }
    }
  }
;
}","private Runnable createCleanupTask(final Object... resources){
  return new Runnable(){
    @Override public void run(){
      for (      Object resource : resources) {
        if (resource == null) {
          continue;
        }
        try {
          if (resource instanceof File) {
            if (((File)resource).isDirectory()) {
              DirUtils.deleteDirectoryContents((File)resource);
            }
 else {
              ((File)resource).delete();
            }
          }
 else           if (resource instanceof Location) {
            Locations.deleteQuietly((Location)resource,true);
          }
 else           if (resource instanceof AutoCloseable) {
            ((AutoCloseable)resource).close();
          }
 else           if (resource instanceof Runnable) {
            ((Runnable)resource).run();
          }
        }
 catch (        Throwable t) {
          LOG.warn(""String_Node_Str"",resource,t);
        }
      }
    }
  }
;
}"
6429,"@Override protected void startUp() throws Exception {
  File tempDir=createTempDirectory();
  cleanupTask=createCleanupTask(tempDir);
  try {
    Job job=createJob(new File(tempDir,""String_Node_Str""));
    Configuration mapredConf=job.getConfiguration();
    classLoader=new MapReduceClassLoader(injector,cConf,mapredConf,context.getProgram().getClassLoader(),context.getPlugins(),context.getPluginInstantiator());
    cleanupTask=createCleanupTask(cleanupTask,classLoader);
    mapredConf.setClassLoader(new WeakReferenceDelegatorClassLoader(classLoader));
    ClassLoaders.setContextClassLoader(mapredConf.getClassLoader());
    context.setJob(job);
    beforeSubmit(job);
    Map<String,String> localizedUserResources=localizeUserResources(job,tempDir);
    String jobName=job.getJobName();
    if (!jobName.isEmpty()) {
      LOG.warn(""String_Node_Str"",jobName);
    }
    job.setJobName(getJobName(context));
    Location tempLocation=createTempLocationDirectory();
    cleanupTask=createCleanupTask(cleanupTask,tempLocation);
    if (!MapReduceTaskContextProvider.isLocal(mapredConf)) {
      File pluginArchive=context.getPluginArchive();
      if (pluginArchive != null) {
        job.addCacheArchive(pluginArchive.toURI());
        mapredConf.set(Constants.Plugin.ARCHIVE,pluginArchive.getName());
      }
    }
    setOutputClassesIfNeeded(job);
    setMapOutputClassesIfNeeded(job);
    TaskType.MAP.setResources(mapredConf,context.getMapperResources());
    TaskType.REDUCE.setResources(mapredConf,context.getReducerResources());
    MapperWrapper.wrap(job);
    ReducerWrapper.wrap(job);
    File jobJar=buildJobJar(job,tempDir);
    job.setJar(jobJar.toURI().toString());
    Location programJar=programJarLocation;
    if (!MapReduceTaskContextProvider.isLocal(mapredConf)) {
      programJar=copyProgramJar(tempLocation);
      job.addCacheFile(programJar.toURI());
      List<String> paths=new ArrayList<>();
      paths.add(""String_Node_Str"");
      paths.add(""String_Node_Str"");
      Location launcherJar=createLauncherJar(Joiner.on(""String_Node_Str"").join(MapReduceContainerHelper.getMapReduceClassPath(mapredConf,paths)),tempLocation);
      job.addCacheFile(launcherJar.toURI());
      URI frameworkURI=MapReduceContainerHelper.getFrameworkURI(mapredConf);
      if (frameworkURI != null) {
        job.addCacheArchive(frameworkURI);
      }
      mapredConf.unset(MRJobConfig.MAPREDUCE_APPLICATION_FRAMEWORK_PATH);
      mapredConf.set(MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH,launcherJar.getName());
      mapredConf.set(YarnConfiguration.YARN_APPLICATION_CLASSPATH,launcherJar.getName());
    }
    MapReduceContextConfig contextConfig=new MapReduceContextConfig(mapredConf);
    Transaction tx=txClient.startLong();
    try {
      CConfiguration cConfCopy=cConf;
      contextConfig.set(context,cConfCopy,tx,programJar.toURI(),localizedUserResources);
      LOG.info(""String_Node_Str"",context);
      job.submit();
      this.job=job;
      this.transaction=tx;
    }
 catch (    Throwable t) {
      Transactions.invalidateQuietly(txClient,tx);
      throw t;
    }
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",context,t);
    cleanupTask.run();
    throw t;
  }
}","@Override protected void startUp() throws Exception {
  File tempDir=createTempDirectory();
  cleanupTask=createCleanupTask(tempDir);
  try {
    Job job=createJob(new File(tempDir,""String_Node_Str""));
    Configuration mapredConf=job.getConfiguration();
    classLoader=new MapReduceClassLoader(injector,cConf,mapredConf,context.getProgram().getClassLoader(),context.getPlugins(),context.getPluginInstantiator());
    cleanupTask=createCleanupTask(cleanupTask,classLoader);
    mapredConf.setClassLoader(new WeakReferenceDelegatorClassLoader(classLoader));
    ClassLoaders.setContextClassLoader(mapredConf.getClassLoader());
    context.setJob(job);
    beforeSubmit(job);
    Map<String,String> localizedUserResources=localizeUserResources(job,tempDir);
    String jobName=job.getJobName();
    if (!jobName.isEmpty()) {
      LOG.warn(""String_Node_Str"",jobName);
    }
    job.setJobName(getJobName(context));
    Location tempLocation=createTempLocationDirectory();
    cleanupTask=createCleanupTask(cleanupTask,tempLocation);
    if (!MapReduceTaskContextProvider.isLocal(mapredConf)) {
      Location pluginArchive=createPluginArchive(tempLocation);
      if (pluginArchive != null) {
        job.addCacheArchive(pluginArchive.toURI());
        mapredConf.set(Constants.Plugin.ARCHIVE,pluginArchive.getName());
      }
    }
    setOutputClassesIfNeeded(job);
    setMapOutputClassesIfNeeded(job);
    TaskType.MAP.setResources(mapredConf,context.getMapperResources());
    TaskType.REDUCE.setResources(mapredConf,context.getReducerResources());
    MapperWrapper.wrap(job);
    ReducerWrapper.wrap(job);
    File jobJar=buildJobJar(job,tempDir);
    job.setJar(jobJar.toURI().toString());
    Location programJar=programJarLocation;
    if (!MapReduceTaskContextProvider.isLocal(mapredConf)) {
      programJar=copyProgramJar(tempLocation);
      job.addCacheFile(programJar.toURI());
      List<String> paths=new ArrayList<>();
      paths.add(""String_Node_Str"");
      paths.add(""String_Node_Str"");
      Location launcherJar=createLauncherJar(Joiner.on(""String_Node_Str"").join(MapReduceContainerHelper.getMapReduceClassPath(mapredConf,paths)),tempLocation);
      job.addCacheFile(launcherJar.toURI());
      URI frameworkURI=MapReduceContainerHelper.getFrameworkURI(mapredConf);
      if (frameworkURI != null) {
        job.addCacheArchive(frameworkURI);
      }
      mapredConf.unset(MRJobConfig.MAPREDUCE_APPLICATION_FRAMEWORK_PATH);
      mapredConf.set(MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH,launcherJar.getName());
      mapredConf.set(YarnConfiguration.YARN_APPLICATION_CLASSPATH,launcherJar.getName());
    }
    MapReduceContextConfig contextConfig=new MapReduceContextConfig(mapredConf);
    Transaction tx=txClient.startLong();
    try {
      CConfiguration cConfCopy=cConf;
      contextConfig.set(context,cConfCopy,tx,programJar.toURI(),localizedUserResources);
      LOG.info(""String_Node_Str"",context);
      job.submit();
      this.job=job;
      this.transaction=tx;
    }
 catch (    Throwable t) {
      Transactions.invalidateQuietly(txClient,tx);
      throw t;
    }
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",context,t);
    cleanupTask.run();
    throw t;
  }
}"
6430,"protected ClientConfig getClientConfig(){
  ClientConfig.Builder builder=new ClientConfig.Builder();
  builder.setConnectionConfig(InstanceURIParser.DEFAULT.parse(URI.create(getInstanceURI()).toString()));
  if (accessToken != null) {
    builder.setAccessToken(accessToken);
  }
  builder.setDefaultConnectTimeout(120000);
  builder.setDefaultReadTimeout(120000);
  builder.setUploadConnectTimeout(0);
  builder.setUploadConnectTimeout(0);
  return builder.build();
}","protected ClientConfig getClientConfig(){
  ClientConfig.Builder builder=new ClientConfig.Builder();
  builder.setConnectionConfig(InstanceURIParser.DEFAULT.parse(URI.create(getInstanceURI()).toString()));
  if (accessToken != null) {
    builder.setAccessToken(accessToken);
  }
  builder.setDefaultConnectTimeout(120000);
  builder.setDefaultReadTimeout(120000);
  builder.setUploadConnectTimeout(0);
  builder.setUploadReadTimeout(0);
  return builder.build();
}"
6431,"/** 
 * Get queue at namespace level if it is empty returns the default queue.
 * @param namespaceId NamespaceId
 * @return schedule queue at namespace level or default queue.
 */
@Nullable public String getQueue(Id.Namespace namespaceId){
  NamespaceMeta meta=store.getNamespace(namespaceId);
  if (meta != null) {
    NamespaceConfig config=meta.getConfig();
    return config.getSchedulerQueueName() != null ? config.getSchedulerQueueName() : getDefaultQueue();
  }
 else {
    return getDefaultQueue();
  }
}","/** 
 * Get queue at namespace level if it is empty returns the default queue.
 * @param namespaceId NamespaceId
 * @return schedule queue at namespace level or default queue.
 */
@Nullable public String getQueue(Id.Namespace namespaceId){
  NamespaceMeta meta=store.getNamespace(namespaceId);
  if (meta != null) {
    NamespaceConfig config=meta.getConfig();
    String namespaceQueue=config.getSchedulerQueueName();
    return Strings.isNullOrEmpty(namespaceQueue) ? getDefaultQueue() : namespaceQueue;
  }
 else {
    return getDefaultQueue();
  }
}"
6432,"/** 
 * Add secure tokens to the   {@link TwillPreparer}.
 */
private TwillPreparer addSecureStore(TwillPreparer preparer,LocationFactory locationFactory){
  Credentials credentials=new Credentials();
  if (User.isHBaseSecurityEnabled(hConf)) {
    HBaseTokenUtils.obtainToken(hConf,credentials);
  }
  try {
    if (locationFactory instanceof HDFSLocationFactory) {
      YarnUtils.addDelegationTokens(hConf,locationFactory,credentials);
    }
 else     if (locationFactory instanceof FileContextLocationFactory) {
      List<Token<?>> tokens=((FileContextLocationFactory)locationFactory).getFileContext().getDelegationTokens(new Path(locationFactory.getHomeLocation().toURI()),YarnUtils.getYarnTokenRenewer(hConf));
      for (      Token<?> token : tokens) {
        credentials.addToken(token.getService(),token);
      }
    }
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
  if (!credentials.getAllTokens().isEmpty()) {
    preparer.addSecureStore(YarnSecureStore.create(credentials));
  }
  return preparer;
}","/** 
 * Add secure tokens to the   {@link TwillPreparer}.
 */
private TwillPreparer addSecureStore(TwillPreparer preparer,LocationFactory locationFactory){
  Credentials credentials=new Credentials();
  if (User.isHBaseSecurityEnabled(hConf)) {
    HBaseTokenUtils.obtainToken(hConf,credentials);
  }
  try {
    if (UserGroupInformation.isSecurityEnabled()) {
      if (locationFactory instanceof HDFSLocationFactory) {
        YarnUtils.addDelegationTokens(hConf,locationFactory,credentials);
      }
 else       if (locationFactory instanceof FileContextLocationFactory) {
        List<Token<?>> tokens=((FileContextLocationFactory)locationFactory).getFileContext().getDelegationTokens(new Path(locationFactory.getHomeLocation().toURI()),YarnUtils.getYarnTokenRenewer(hConf));
        for (        Token<?> token : tokens) {
          credentials.addToken(token.getService(),token);
        }
      }
    }
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
  if (!credentials.getAllTokens().isEmpty()) {
    preparer.addSecureStore(YarnSecureStore.create(credentials));
  }
  return preparer;
}"
6433,"private DynamicDatasetCache entryForCurrentThread(){
  try {
    return perThreadMap.get(Thread.currentThread().getId());
  }
 catch (  ExecutionException e) {
    throw Throwables.propagate(e);
  }
}","private DynamicDatasetCache entryForCurrentThread(){
  try {
    return perThreadMap.get(Thread.currentThread());
  }
 catch (  ExecutionException e) {
    throw Throwables.propagate(e);
  }
}"
6434,"/** 
 * See   {@link DynamicDatasetCache ).}
 * @param staticDatasets  if non-null, a map from dataset name to runtime arguments. These datasets will beinstantiated immediately, and they will participate in every transaction started through  {@link #newTransactionContext()}.
 */
public MultiThreadDatasetCache(final SystemDatasetInstantiator instantiator,final TransactionSystemClient txClient,final Id.Namespace namespace,final Map<String,String> runtimeArguments,final MetricsContext metricsContext,@Nullable final Map<String,Map<String,String>> staticDatasets){
  super(instantiator,txClient,namespace,runtimeArguments,metricsContext);
  this.perThreadMap=CacheBuilder.newBuilder().weakValues().removalListener(new RemovalListener<Long,DynamicDatasetCache>(){
    @Override @ParametersAreNonnullByDefault public void onRemoval(    RemovalNotification<Long,DynamicDatasetCache> notification){
      DynamicDatasetCache cache=notification.getValue();
      if (cache != null) {
        cache.close();
      }
    }
  }
).build(new CacheLoader<Long,SingleThreadDatasetCache>(){
    @Override @ParametersAreNonnullByDefault public SingleThreadDatasetCache load(    Long threadId) throws Exception {
      return new SingleThreadDatasetCache(instantiator,txClient,namespace,runtimeArguments,metricsContext,staticDatasets);
    }
  }
);
}","/** 
 * See   {@link DynamicDatasetCache ).}
 * @param staticDatasets  if non-null, a map from dataset name to runtime arguments. These datasets will beinstantiated immediately, and they will participate in every transaction started through  {@link #newTransactionContext()}.
 */
public MultiThreadDatasetCache(final SystemDatasetInstantiator instantiator,final TransactionSystemClient txClient,final Id.Namespace namespace,final Map<String,String> runtimeArguments,final MetricsContext metricsContext,@Nullable final Map<String,Map<String,String>> staticDatasets){
  super(instantiator,txClient,namespace,runtimeArguments,metricsContext);
  this.perThreadMap=CacheBuilder.newBuilder().weakKeys().removalListener(new RemovalListener<Thread,DynamicDatasetCache>(){
    @Override @ParametersAreNonnullByDefault public void onRemoval(    RemovalNotification<Thread,DynamicDatasetCache> notification){
      DynamicDatasetCache cache=notification.getValue();
      if (cache != null) {
        cache.close();
      }
    }
  }
).build(new CacheLoader<Thread,SingleThreadDatasetCache>(){
    @Override @ParametersAreNonnullByDefault public SingleThreadDatasetCache load(    Thread thread) throws Exception {
      return new SingleThreadDatasetCache(instantiator,txClient,namespace,runtimeArguments,metricsContext,staticDatasets);
    }
  }
);
}"
6435,"@Override @ParametersAreNonnullByDefault public SingleThreadDatasetCache load(Long threadId) throws Exception {
  return new SingleThreadDatasetCache(instantiator,txClient,namespace,runtimeArguments,metricsContext,staticDatasets);
}","@Override @ParametersAreNonnullByDefault public SingleThreadDatasetCache load(Thread thread) throws Exception {
  return new SingleThreadDatasetCache(instantiator,txClient,namespace,runtimeArguments,metricsContext,staticDatasets);
}"
6436,"@Override @ParametersAreNonnullByDefault public void onRemoval(RemovalNotification<Long,DynamicDatasetCache> notification){
  DynamicDatasetCache cache=notification.getValue();
  if (cache != null) {
    cache.close();
  }
}","@Override @ParametersAreNonnullByDefault public void onRemoval(RemovalNotification<Thread,DynamicDatasetCache> notification){
  DynamicDatasetCache cache=notification.getValue();
  if (cache != null) {
    cache.close();
  }
}"
6437,"@Test public void testDatasetCache() throws IOException, DatasetManagementException, TransactionFailureException, InterruptedException {
  final Map<String,TestDataset> thread1map=new HashMap<>();
  final Map<String,TestDataset> thread2map=new HashMap<>();
  Thread thread1=createThread(thread1map);
  Thread thread2=createThread(thread2map);
  thread1.start();
  thread2.start();
  thread1.join();
  thread2.join();
  Assert.assertNotSame(thread1map.get(""String_Node_Str""),thread2map.get(""String_Node_Str""));
  Assert.assertNotSame(thread1map.get(""String_Node_Str""),thread2map.get(""String_Node_Str""));
}","@Test() public void testDatasetCache() throws Exception {
  Map<String,TestDataset> thread1map=new HashMap<>();
  Map<String,TestDataset> thread2map=new HashMap<>();
  Thread thread1=createThread(thread1map);
  Thread thread2=createThread(thread2map);
  thread1.start();
  thread2.start();
  thread1.join();
  thread2.join();
  Assert.assertNotSame(thread1map.get(""String_Node_Str""),thread2map.get(""String_Node_Str""));
  Assert.assertNotSame(thread1map.get(""String_Node_Str""),thread2map.get(""String_Node_Str""));
  thread1=thread2=null;
  thread1map=thread2map=null;
  Tasks.waitFor(true,new Callable<Boolean>(){
    @Override public Boolean call() throws Exception {
      System.gc();
      return ((MultiThreadDatasetCache)cache).getCacheKeys().isEmpty();
    }
  }
,5,TimeUnit.SECONDS,100,TimeUnit.MILLISECONDS);
}"
6438,"@Test public void testKVToKV() throws Exception {
  ETLStage source=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.BatchReadableWritable.NAME,""String_Node_Str""));
  ETLStage sink=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.BatchReadableWritable.NAME,""String_Node_Str""));
  ETLStage transform=new ETLStage(""String_Node_Str"",ImmutableMap.<String,String>of());
  List<ETLStage> transformList=Lists.newArrayList(transform);
  ETLBatchConfig etlConfig=new ETLBatchConfig(""String_Node_Str"",source,sink,transformList);
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(ETLBATCH_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  DataSetManager<KeyValueTable> table1=getDataset(""String_Node_Str"");
  KeyValueTable inputTable=table1.get();
  for (int i=0; i < 10000; i++) {
    inputTable.write(""String_Node_Str"" + i,""String_Node_Str"" + i);
  }
  table1.flush();
  MapReduceManager mrManager=appManager.getMapReduceManager(ETLMapReduce.NAME);
  mrManager.start();
  mrManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<KeyValueTable> table2=getDataset(""String_Node_Str"");
  KeyValueTable outputTable=table2.get();
  for (int i=0; i < 10000; i++) {
    Assert.assertEquals(""String_Node_Str"" + i,Bytes.toString(outputTable.read(""String_Node_Str"" + i)));
  }
}","@Test public void testKVToKV() throws Exception {
  ETLStage source=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.BatchReadableWritable.NAME,""String_Node_Str""));
  ETLStage sink=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.BatchReadableWritable.NAME,""String_Node_Str""));
  ETLStage transform=new ETLStage(""String_Node_Str"",ImmutableMap.<String,String>of());
  List<ETLStage> transformList=Lists.newArrayList(transform);
  ETLBatchConfig etlConfig=new ETLBatchConfig(""String_Node_Str"",source,sink,transformList);
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(ETLBATCH_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  DataSetManager<KeyValueTable> table1=getDataset(""String_Node_Str"");
  KeyValueTable inputTable=table1.get();
  for (int i=0; i < 10000; i++) {
    inputTable.write(""String_Node_Str"" + i,""String_Node_Str"" + i);
  }
  table1.flush();
  MapReduceManager mrManager=appManager.getMapReduceManager(ETLMapReduce.NAME);
  mrManager.start();
  mrManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<KeyValueTable> table2=getDataset(""String_Node_Str"");
  try (KeyValueTable outputTable=table2.get()){
    for (int i=0; i < 10000; i++) {
      Assert.assertEquals(""String_Node_Str"" + i,Bytes.toString(outputTable.read(""String_Node_Str"" + i)));
    }
  }
 }"
6439,"@SuppressWarnings(""String_Node_Str"") @Test public void testTableToTableWithValidations() throws Exception {
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.DOUBLE)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  ETLStage source=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.BatchReadableWritable.NAME,""String_Node_Str"",Properties.Table.PROPERTY_SCHEMA_ROW_FIELD,""String_Node_Str"",Properties.Table.PROPERTY_SCHEMA,schema.toString()));
  String validationScript=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
  ETLStage transform=new ETLStage(""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",validationScript),""String_Node_Str"");
  List<ETLStage> transformList=new ArrayList<>();
  transformList.add(transform);
  ETLStage sink=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.BatchReadableWritable.NAME,""String_Node_Str"",Properties.Table.PROPERTY_SCHEMA_ROW_FIELD,""String_Node_Str""));
  ETLBatchConfig etlConfig=new ETLBatchConfig(""String_Node_Str"",source,sink,transformList);
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(ETLBATCH_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  DataSetManager<Table> inputManager=getDataset(""String_Node_Str"");
  Table inputTable=inputManager.get();
  Put put=new Put(Bytes.toBytes(""String_Node_Str""));
  put.add(""String_Node_Str"",""String_Node_Str"");
  put.add(""String_Node_Str"",5);
  put.add(""String_Node_Str"",123.45);
  put.add(""String_Node_Str"",""String_Node_Str"");
  inputTable.put(put);
  inputManager.flush();
  put=new Put(Bytes.toBytes(""String_Node_Str""));
  put.add(""String_Node_Str"",""String_Node_Str"");
  put.add(""String_Node_Str"",10);
  put.add(""String_Node_Str"",123456789d);
  put.add(""String_Node_Str"",""String_Node_Str"");
  inputTable.put(put);
  inputManager.flush();
  MapReduceManager mrManager=appManager.getMapReduceManager(ETLMapReduce.NAME);
  mrManager.start();
  mrManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<Table> outputManager=getDataset(""String_Node_Str"");
  Table outputTable=outputManager.get();
  Row row=outputTable.get(Bytes.toBytes(""String_Node_Str""));
  Assert.assertEquals(""String_Node_Str"",row.getString(""String_Node_Str""));
  Assert.assertEquals(5,(int)row.getInt(""String_Node_Str""));
  Assert.assertTrue(Math.abs(123.45 - row.getDouble(""String_Node_Str"")) < 0.000001);
  Assert.assertEquals(""String_Node_Str"",row.getString(""String_Node_Str""));
  row=outputTable.get(Bytes.toBytes(""String_Node_Str""));
  Assert.assertEquals(0,row.getColumns().size());
  DataSetManager<TimePartitionedFileSet> fileSetManager=getDataset(""String_Node_Str"");
  TimePartitionedFileSet fileSet=fileSetManager.get();
  List<GenericRecord> records=readOutput(fileSet,ETLMapReduce.ERROR_SCHEMA);
  Assert.assertEquals(1,records.size());
  fileSet.close();
}","@SuppressWarnings(""String_Node_Str"") @Test public void testTableToTableWithValidations() throws Exception {
  Schema schema=Schema.recordOf(""String_Node_Str"",Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.INT)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.DOUBLE)),Schema.Field.of(""String_Node_Str"",Schema.of(Schema.Type.STRING)));
  ETLStage source=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.BatchReadableWritable.NAME,""String_Node_Str"",Properties.Table.PROPERTY_SCHEMA_ROW_FIELD,""String_Node_Str"",Properties.Table.PROPERTY_SCHEMA,schema.toString()));
  String validationScript=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
  ETLStage transform=new ETLStage(""String_Node_Str"",ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",validationScript),""String_Node_Str"");
  List<ETLStage> transformList=new ArrayList<>();
  transformList.add(transform);
  ETLStage sink=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.BatchReadableWritable.NAME,""String_Node_Str"",Properties.Table.PROPERTY_SCHEMA_ROW_FIELD,""String_Node_Str""));
  ETLBatchConfig etlConfig=new ETLBatchConfig(""String_Node_Str"",source,sink,transformList);
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(ETLBATCH_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  DataSetManager<Table> inputManager=getDataset(""String_Node_Str"");
  Table inputTable=inputManager.get();
  Put put=new Put(Bytes.toBytes(""String_Node_Str""));
  put.add(""String_Node_Str"",""String_Node_Str"");
  put.add(""String_Node_Str"",5);
  put.add(""String_Node_Str"",123.45);
  put.add(""String_Node_Str"",""String_Node_Str"");
  inputTable.put(put);
  inputManager.flush();
  put=new Put(Bytes.toBytes(""String_Node_Str""));
  put.add(""String_Node_Str"",""String_Node_Str"");
  put.add(""String_Node_Str"",10);
  put.add(""String_Node_Str"",123456789d);
  put.add(""String_Node_Str"",""String_Node_Str"");
  inputTable.put(put);
  inputManager.flush();
  MapReduceManager mrManager=appManager.getMapReduceManager(ETLMapReduce.NAME);
  mrManager.start();
  mrManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<Table> outputManager=getDataset(""String_Node_Str"");
  Table outputTable=outputManager.get();
  Row row=outputTable.get(Bytes.toBytes(""String_Node_Str""));
  Assert.assertEquals(""String_Node_Str"",row.getString(""String_Node_Str""));
  Assert.assertEquals(5,(int)row.getInt(""String_Node_Str""));
  Assert.assertTrue(Math.abs(123.45 - row.getDouble(""String_Node_Str"")) < 0.000001);
  Assert.assertEquals(""String_Node_Str"",row.getString(""String_Node_Str""));
  row=outputTable.get(Bytes.toBytes(""String_Node_Str""));
  Assert.assertEquals(0,row.getColumns().size());
  DataSetManager<TimePartitionedFileSet> fileSetManager=getDataset(""String_Node_Str"");
  try (TimePartitionedFileSet fileSet=fileSetManager.get()){
    List<GenericRecord> records=readOutput(fileSet,ETLMapReduce.ERROR_SCHEMA);
    Assert.assertEquals(1,records.size());
  }
 }"
6440,"@Test public void testS3toTPFS() throws Exception {
  String testPath=""String_Node_Str"";
  String testData=""String_Node_Str"";
  S3NInMemoryFileSystem fs=new S3NInMemoryFileSystem();
  Configuration conf=new Configuration();
  conf.set(""String_Node_Str"",S3NInMemoryFileSystem.class.getName());
  fs.initialize(URI.create(""String_Node_Str""),conf);
  fs.createNewFile(new Path(testPath));
  FSDataOutputStream writeData=fs.create(new Path(testPath));
  writeData.write(testData.getBytes());
  writeData.flush();
  writeData.close();
  Method method=FileSystem.class.getDeclaredMethod(""String_Node_Str"",new Class[]{URI.class,Configuration.class,FileSystem.class});
  method.setAccessible(true);
  method.invoke(FileSystem.class,URI.create(""String_Node_Str""),conf,fs);
  ETLStage source=new ETLStage(""String_Node_Str"",ImmutableMap.<String,String>builder().put(Properties.S3.ACCESS_KEY,""String_Node_Str"").put(Properties.S3.ACCESS_ID,""String_Node_Str"").put(Properties.S3.PATH,testPath).build());
  ETLStage sink=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.TimePartitionedFileSetDataset.SCHEMA,FileBatchSource.DEFAULT_SCHEMA.toString(),Properties.TimePartitionedFileSetDataset.TPFS_NAME,""String_Node_Str""));
  ETLBatchConfig etlConfig=new ETLBatchConfig(""String_Node_Str"",source,sink,Lists.<ETLStage>newArrayList());
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(ETLBATCH_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  MapReduceManager mrManager=appManager.getMapReduceManager(ETLMapReduce.NAME);
  mrManager.start();
  mrManager.waitForFinish(2,TimeUnit.MINUTES);
  DataSetManager<TimePartitionedFileSet> fileSetManager=getDataset(""String_Node_Str"");
  TimePartitionedFileSet fileSet=fileSetManager.get();
  List<GenericRecord> records=readOutput(fileSet,FileBatchSource.DEFAULT_SCHEMA);
  Assert.assertEquals(1,records.size());
  Assert.assertEquals(testData,records.get(0).get(""String_Node_Str"").toString());
  fileSet.close();
}","@Test public void testS3toTPFS() throws Exception {
  String testPath=""String_Node_Str"";
  String testFile1=""String_Node_Str"";
  String testData1=""String_Node_Str"";
  String testFile2=""String_Node_Str"";
  String testData2=""String_Node_Str"";
  S3NInMemoryFileSystem fs=new S3NInMemoryFileSystem();
  Configuration conf=new Configuration();
  conf.set(""String_Node_Str"",S3NInMemoryFileSystem.class.getName());
  fs.initialize(URI.create(""String_Node_Str""),conf);
  fs.createNewFile(new Path(testPath));
  try (FSDataOutputStream fos1=fs.create(new Path(testPath + testFile1))){
    fos1.write(testData1.getBytes());
    fos1.flush();
  }
   try (FSDataOutputStream fos2=fs.create(new Path(testPath + testFile2))){
    fos2.write(testData2.getBytes());
    fos2.flush();
  }
   Method method=FileSystem.class.getDeclaredMethod(""String_Node_Str"",URI.class,Configuration.class,FileSystem.class);
  method.setAccessible(true);
  method.invoke(FileSystem.class,URI.create(""String_Node_Str""),conf,fs);
  ETLStage source=new ETLStage(""String_Node_Str"",ImmutableMap.<String,String>builder().put(Properties.S3.ACCESS_KEY,""String_Node_Str"").put(Properties.S3.ACCESS_ID,""String_Node_Str"").put(Properties.S3.PATH,testPath).put(Properties.S3.FILE_REGEX,""String_Node_Str"").build());
  ETLStage sink=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.TimePartitionedFileSetDataset.SCHEMA,FileBatchSource.DEFAULT_SCHEMA.toString(),Properties.TimePartitionedFileSetDataset.TPFS_NAME,""String_Node_Str""));
  ETLBatchConfig etlConfig=new ETLBatchConfig(""String_Node_Str"",source,sink,Lists.<ETLStage>newArrayList());
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(ETLBATCH_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  MapReduceManager mrManager=appManager.getMapReduceManager(ETLMapReduce.NAME);
  mrManager.start();
  mrManager.waitForFinish(2,TimeUnit.MINUTES);
  DataSetManager<TimePartitionedFileSet> fileSetManager=getDataset(""String_Node_Str"");
  try (TimePartitionedFileSet fileSet=fileSetManager.get()){
    List<GenericRecord> records=readOutput(fileSet,FileBatchSource.DEFAULT_SCHEMA);
    Assert.assertEquals(1,records.size());
    Assert.assertEquals(testData1,records.get(0).get(""String_Node_Str"").toString());
  }
 }"
6441,"@Test public void testKVToKVMeta() throws Exception {
  ETLStage source=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.BatchReadableWritable.NAME,""String_Node_Str""));
  ETLStage sink=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.BatchReadableWritable.NAME,""String_Node_Str""));
  ETLBatchConfig etlConfig=new ETLBatchConfig(""String_Node_Str"",source,sink);
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(ETLBATCH_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  MapReduceManager mrManager=appManager.getMapReduceManager(ETLMapReduce.NAME);
  mrManager.start();
  mrManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<KeyValueTable> sourceMetaTable=getDataset(MetaKVTableSource.META_TABLE);
  KeyValueTable sourceTable=sourceMetaTable.get();
  Assert.assertEquals(MetaKVTableSource.PREPARE_RUN_KEY,Bytes.toString(sourceTable.read(MetaKVTableSource.PREPARE_RUN_KEY)));
  Assert.assertEquals(MetaKVTableSource.FINISH_RUN_KEY,Bytes.toString(sourceTable.read(MetaKVTableSource.FINISH_RUN_KEY)));
  DataSetManager<KeyValueTable> sinkMetaTable=getDataset(MetaKVTableSink.META_TABLE);
  KeyValueTable sinkTable=sinkMetaTable.get();
  Assert.assertEquals(MetaKVTableSink.PREPARE_RUN_KEY,Bytes.toString(sinkTable.read(MetaKVTableSink.PREPARE_RUN_KEY)));
  Assert.assertEquals(MetaKVTableSink.FINISH_RUN_KEY,Bytes.toString(sinkTable.read(MetaKVTableSink.FINISH_RUN_KEY)));
}","@Test public void testKVToKVMeta() throws Exception {
  ETLStage source=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.BatchReadableWritable.NAME,""String_Node_Str""));
  ETLStage sink=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.BatchReadableWritable.NAME,""String_Node_Str""));
  ETLBatchConfig etlConfig=new ETLBatchConfig(""String_Node_Str"",source,sink);
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(ETLBATCH_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  MapReduceManager mrManager=appManager.getMapReduceManager(ETLMapReduce.NAME);
  mrManager.start();
  mrManager.waitForFinish(5,TimeUnit.MINUTES);
  DataSetManager<KeyValueTable> sourceMetaTable=getDataset(MetaKVTableSource.META_TABLE);
  KeyValueTable sourceTable=sourceMetaTable.get();
  Assert.assertEquals(MetaKVTableSource.PREPARE_RUN_KEY,Bytes.toString(sourceTable.read(MetaKVTableSource.PREPARE_RUN_KEY)));
  Assert.assertEquals(MetaKVTableSource.FINISH_RUN_KEY,Bytes.toString(sourceTable.read(MetaKVTableSource.FINISH_RUN_KEY)));
  DataSetManager<KeyValueTable> sinkMetaTable=getDataset(MetaKVTableSink.META_TABLE);
  try (KeyValueTable sinkTable=sinkMetaTable.get()){
    Assert.assertEquals(MetaKVTableSink.PREPARE_RUN_KEY,Bytes.toString(sinkTable.read(MetaKVTableSink.PREPARE_RUN_KEY)));
    Assert.assertEquals(MetaKVTableSink.FINISH_RUN_KEY,Bytes.toString(sinkTable.read(MetaKVTableSink.FINISH_RUN_KEY)));
  }
 }"
6442,"@Test public void testFiletoMultipleTPFS() throws Exception {
  String filePath=""String_Node_Str"";
  String testData=""String_Node_Str"";
  Path textFile=new Path(filePath);
  Configuration conf=new Configuration();
  FileSystem fs=FileSystem.get(conf);
  FSDataOutputStream writeData=fs.create(textFile);
  writeData.write(testData.getBytes());
  writeData.flush();
  writeData.close();
  ETLStage source=new ETLStage(""String_Node_Str"",ImmutableMap.<String,String>builder().put(Properties.File.FILESYSTEM,""String_Node_Str"").put(Properties.File.PATH,filePath).build());
  ETLStage sink1=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.TimePartitionedFileSetDataset.SCHEMA,FileBatchSource.DEFAULT_SCHEMA.toString(),Properties.TimePartitionedFileSetDataset.TPFS_NAME,""String_Node_Str""));
  ETLStage sink2=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.TimePartitionedFileSetDataset.SCHEMA,FileBatchSource.DEFAULT_SCHEMA.toString(),Properties.TimePartitionedFileSetDataset.TPFS_NAME,""String_Node_Str""));
  ETLBatchConfig etlConfig=new ETLBatchConfig(""String_Node_Str"",source,Lists.newArrayList(sink1,sink2),Lists.<ETLStage>newArrayList(),new Resources(),Lists.<ETLStage>newArrayList());
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(ETLBATCH_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  MapReduceManager mrManager=appManager.getMapReduceManager(ETLMapReduce.NAME);
  mrManager.start();
  mrManager.waitForFinish(2,TimeUnit.MINUTES);
  for (  String sinkName : new String[]{""String_Node_Str"",""String_Node_Str""}) {
    DataSetManager<TimePartitionedFileSet> fileSetManager=getDataset(sinkName);
    TimePartitionedFileSet fileSet=fileSetManager.get();
    List<GenericRecord> records=readOutput(fileSet,FileBatchSource.DEFAULT_SCHEMA);
    Assert.assertEquals(1,records.size());
    Assert.assertEquals(testData,records.get(0).get(""String_Node_Str"").toString());
    fileSet.close();
  }
}","@Test public void testFiletoMultipleTPFS() throws Exception {
  String filePath=""String_Node_Str"";
  String testData=""String_Node_Str"";
  Path textFile=new Path(filePath);
  Configuration conf=new Configuration();
  FileSystem fs=FileSystem.get(conf);
  FSDataOutputStream writeData=fs.create(textFile);
  writeData.write(testData.getBytes());
  writeData.flush();
  writeData.close();
  ETLStage source=new ETLStage(""String_Node_Str"",ImmutableMap.<String,String>builder().put(Properties.File.FILESYSTEM,""String_Node_Str"").put(Properties.File.PATH,filePath).build());
  ETLStage sink1=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.TimePartitionedFileSetDataset.SCHEMA,FileBatchSource.DEFAULT_SCHEMA.toString(),Properties.TimePartitionedFileSetDataset.TPFS_NAME,""String_Node_Str""));
  ETLStage sink2=new ETLStage(""String_Node_Str"",ImmutableMap.of(Properties.TimePartitionedFileSetDataset.SCHEMA,FileBatchSource.DEFAULT_SCHEMA.toString(),Properties.TimePartitionedFileSetDataset.TPFS_NAME,""String_Node_Str""));
  ETLBatchConfig etlConfig=new ETLBatchConfig(""String_Node_Str"",source,Lists.newArrayList(sink1,sink2),Lists.<ETLStage>newArrayList(),new Resources(),Lists.<ETLStage>newArrayList());
  AppRequest<ETLBatchConfig> appRequest=new AppRequest<>(ETLBATCH_ARTIFACT,etlConfig);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  ApplicationManager appManager=deployApplication(appId,appRequest);
  MapReduceManager mrManager=appManager.getMapReduceManager(ETLMapReduce.NAME);
  mrManager.start();
  mrManager.waitForFinish(2,TimeUnit.MINUTES);
  for (  String sinkName : new String[]{""String_Node_Str"",""String_Node_Str""}) {
    DataSetManager<TimePartitionedFileSet> fileSetManager=getDataset(sinkName);
    try (TimePartitionedFileSet fileSet=fileSetManager.get()){
      List<GenericRecord> records=readOutput(fileSet,FileBatchSource.DEFAULT_SCHEMA);
      Assert.assertEquals(1,records.size());
      Assert.assertEquals(testData,records.get(0).get(""String_Node_Str"").toString());
    }
   }
}"
6443,"public S3AvroOutputFormatProvider(S3AvroSinkConfig config,BatchSinkContext context){
  SimpleDateFormat format=new SimpleDateFormat(config.pathFormat);
  conf=Maps.newHashMap();
  conf.put(JobContext.OUTPUT_KEY_CLASS,AvroKey.class.getName());
  conf.put(""String_Node_Str"",config.schema);
  conf.put(FileOutputFormat.OUTDIR,String.format(""String_Node_Str"",config.basePath,format.format(context.getLogicalStartTime())));
}","public S3AvroOutputFormatProvider(S3AvroSinkConfig config,BatchSinkContext context){
  @SuppressWarnings(""String_Node_Str"") SimpleDateFormat format=new SimpleDateFormat(config.pathFormat);
  conf=Maps.newHashMap();
  conf.put(JobContext.OUTPUT_KEY_CLASS,AvroKey.class.getName());
  conf.put(""String_Node_Str"",config.schema);
  conf.put(FileOutputFormat.OUTDIR,String.format(""String_Node_Str"",config.basePath,format.format(context.getLogicalStartTime())));
}"
6444,"public S3AvroSinkConfig(String basePath,String pathFormat,String schema,String accessID,String accessKey){
  super(basePath,pathFormat,accessID,accessKey);
  this.schema=schema;
}","@SuppressWarnings(""String_Node_Str"") public S3AvroSinkConfig(String basePath,String schema,String accessID,String accessKey,String pathFormat,String filesystemProperties){
  super(basePath,accessID,accessKey,pathFormat,filesystemProperties);
  this.schema=schema;
}"
6445,"@Override public void prepareRun(BatchSinkContext context){
  Job job=context.getHadoopJob();
  Configuration conf=job.getConfiguration();
  conf.set(""String_Node_Str"",this.config.accessID);
  conf.set(""String_Node_Str"",this.config.accessKey);
}","@Override public void prepareRun(BatchSinkContext context){
  Job job=context.getHadoopJob();
  Configuration conf=job.getConfiguration();
  if (config.fileSystemProperties != null) {
    Map<String,String> properties=GSON.fromJson(config.fileSystemProperties,MAP_STRING_STRING_TYPE);
    for (    Map.Entry<String,String> entry : properties.entrySet()) {
      conf.set(entry.getKey(),entry.getValue());
    }
  }
}"
6446,"public S3BatchSinkConfig(String basePath,String pathFormat,String accessID,String accessKey){
  this.basePath=basePath;
  this.pathFormat=pathFormat == null || pathFormat.isEmpty() ? DEFAULT_PATH_FORMAT : pathFormat;
  this.accessID=accessID;
  this.accessKey=accessKey;
}","public S3BatchSinkConfig(String basePath,String accessID,String accessKey,@Nullable String pathFormat,@Nullable String fileSystemProperties){
  this.basePath=basePath;
  this.pathFormat=pathFormat == null || pathFormat.isEmpty() ? DEFAULT_PATH_FORMAT : pathFormat;
  this.accessID=accessID;
  this.accessKey=accessKey;
  this.fileSystemProperties=updateFileSystemProperties(fileSystemProperties,accessID,accessKey);
}"
6447,"protected S3BatchSink(S3BatchSinkConfig config){
  this.config=config;
}","protected S3BatchSink(S3BatchSinkConfig config){
  this.config=config;
  this.config.fileSystemProperties=updateFileSystemProperties(this.config.fileSystemProperties,this.config.accessID,this.config.accessKey);
}"
6448,"public S3ParquetOutputFormatProvider(S3ParquetSinkConfig config,BatchSinkContext context){
  SimpleDateFormat format=new SimpleDateFormat(config.pathFormat);
  conf=Maps.newHashMap();
  conf.put(""String_Node_Str"",config.schema);
  conf.put(FileOutputFormat.OUTDIR,String.format(""String_Node_Str"",config.basePath,format.format(context.getLogicalStartTime())));
}","public S3ParquetOutputFormatProvider(S3ParquetSinkConfig config,BatchSinkContext context){
  @SuppressWarnings(""String_Node_Str"") SimpleDateFormat format=new SimpleDateFormat(config.pathFormat);
  conf=Maps.newHashMap();
  conf.put(""String_Node_Str"",config.schema);
  conf.put(FileOutputFormat.OUTDIR,String.format(""String_Node_Str"",config.basePath,format.format(context.getLogicalStartTime())));
}"
6449,"public S3ParquetSinkConfig(String basePath,String pathFormat,String schema,String accessID,String accessKey){
  super(basePath,pathFormat,accessID,accessKey);
  this.schema=schema;
}","@SuppressWarnings(""String_Node_Str"") public S3ParquetSinkConfig(String basePath,String schema,String accessID,String accessKey,String pathFormat,String fileSystemProperties){
  super(basePath,accessID,accessKey,pathFormat,fileSystemProperties);
  this.schema=schema;
}"
6450,"@Test public void testAssignment() throws InterruptedException, ExecutionException {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  String serviceName=""String_Node_Str"";
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules());
  ZKClientService zkClient=injector.getInstance(ZKClientService.class);
  zkClient.startAndWait();
  DiscoveryService discoveryService=injector.getInstance(DiscoveryService.class);
  try {
    ResourceCoordinator coordinator=new ResourceCoordinator(zkClient,injector.getInstance(DiscoveryServiceClient.class),new BalancedAssignmentStrategy());
    coordinator.startAndWait();
    try {
      ResourceCoordinatorClient client=new ResourceCoordinatorClient(zkClient);
      client.startAndWait();
      try {
        ResourceRequirement requirement=ResourceRequirement.builder(serviceName).addPartitions(""String_Node_Str"",5,1).build();
        client.submitRequirement(requirement).get();
        Assert.assertEquals(requirement,client.fetchRequirement(requirement.getName()).get());
        final Discoverable discoverable1=createDiscoverable(serviceName,10000);
        Cancellable cancelDiscoverable1=discoveryService.register(ResolvingDiscoverable.of(discoverable1));
        final BlockingQueue<Collection<PartitionReplica>> assignmentQueue=new SynchronousQueue<>();
        final Semaphore finishSemaphore=new Semaphore(0);
        Cancellable cancelSubscribe1=subscribe(client,discoverable1,assignmentQueue,finishSemaphore);
        Collection<PartitionReplica> assigned=assignmentQueue.poll(5,TimeUnit.SECONDS);
        Assert.assertNotNull(assigned);
        Assert.assertEquals(5,assigned.size());
        cancelDiscoverable1.cancel();
        Assert.assertTrue(assignmentQueue.poll(5,TimeUnit.SECONDS).isEmpty());
        cancelDiscoverable1=discoveryService.register(ResolvingDiscoverable.of(discoverable1));
        assigned=assignmentQueue.poll(5,TimeUnit.SECONDS);
        Assert.assertNotNull(assigned);
        Assert.assertEquals(5,assigned.size());
        final Discoverable discoverable2=createDiscoverable(serviceName,10001);
        Cancellable cancelDiscoverable2=discoveryService.register(ResolvingDiscoverable.of(discoverable2));
        assigned=assignmentQueue.poll(5,TimeUnit.SECONDS);
        Assert.assertNotNull(assigned);
        Assert.assertEquals(3,assigned.size());
        cancelDiscoverable1.cancel();
        Assert.assertTrue(assignmentQueue.poll(5,TimeUnit.SECONDS).isEmpty());
        cancelSubscribe1.cancel();
        Assert.assertTrue(finishSemaphore.tryAcquire(2,TimeUnit.SECONDS));
        Cancellable cancelSubscribe2=subscribe(client,discoverable2,assignmentQueue,finishSemaphore);
        assigned=assignmentQueue.poll(5,TimeUnit.SECONDS);
        Assert.assertNotNull(assigned);
        Assert.assertEquals(5,assigned.size());
        client.submitRequirement(ResourceRequirement.builder(serviceName).build());
        Assert.assertTrue(assignmentQueue.poll(5,TimeUnit.SECONDS).isEmpty());
        client.submitRequirement(ResourceRequirement.builder(serviceName).addPartitions(""String_Node_Str"",1,1).build());
        assigned=assignmentQueue.poll(5,TimeUnit.SECONDS);
        Assert.assertNotNull(assigned);
        Assert.assertEquals(1,assigned.size());
        client.deleteRequirement(requirement.getName());
        Assert.assertTrue(assignmentQueue.poll(5,TimeUnit.SECONDS).isEmpty());
        cancelSubscribe2.cancel();
        Assert.assertTrue(finishSemaphore.tryAcquire(2,TimeUnit.SECONDS));
        cancelDiscoverable2.cancel();
      }
  finally {
        client.stopAndWait();
      }
    }
  finally {
      coordinator.stopAndWait();
    }
  }
  finally {
    zkClient.stopAndWait();
  }
}","@Test public void testAssignment() throws InterruptedException, ExecutionException {
  CConfiguration cConf=CConfiguration.create();
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  String serviceName=""String_Node_Str"";
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules());
  ZKClientService zkClient=injector.getInstance(ZKClientService.class);
  zkClient.startAndWait();
  DiscoveryService discoveryService=injector.getInstance(DiscoveryService.class);
  try {
    ResourceCoordinator coordinator=new ResourceCoordinator(zkClient,injector.getInstance(DiscoveryServiceClient.class),new BalancedAssignmentStrategy());
    coordinator.startAndWait();
    try {
      ResourceCoordinatorClient client=new ResourceCoordinatorClient(zkClient);
      client.startAndWait();
      try {
        ResourceRequirement requirement=ResourceRequirement.builder(serviceName).addPartitions(""String_Node_Str"",5,1).build();
        client.submitRequirement(requirement).get();
        Assert.assertEquals(requirement,client.fetchRequirement(requirement.getName()).get());
        final Discoverable discoverable1=createDiscoverable(serviceName,10000);
        Cancellable cancelDiscoverable1=discoveryService.register(ResolvingDiscoverable.of(discoverable1));
        final BlockingQueue<Collection<PartitionReplica>> assignmentQueue=new SynchronousQueue<>();
        final Semaphore finishSemaphore=new Semaphore(0);
        Cancellable cancelSubscribe1=subscribe(client,discoverable1,assignmentQueue,finishSemaphore);
        Collection<PartitionReplica> assigned=assignmentQueue.poll(30,TimeUnit.SECONDS);
        Assert.assertNotNull(assigned);
        Assert.assertEquals(5,assigned.size());
        cancelDiscoverable1.cancel();
        Assert.assertTrue(assignmentQueue.poll(30,TimeUnit.SECONDS).isEmpty());
        cancelDiscoverable1=discoveryService.register(ResolvingDiscoverable.of(discoverable1));
        assigned=assignmentQueue.poll(30,TimeUnit.SECONDS);
        Assert.assertNotNull(assigned);
        Assert.assertEquals(5,assigned.size());
        final Discoverable discoverable2=createDiscoverable(serviceName,10001);
        Cancellable cancelDiscoverable2=discoveryService.register(ResolvingDiscoverable.of(discoverable2));
        assigned=assignmentQueue.poll(30,TimeUnit.SECONDS);
        Assert.assertNotNull(assigned);
        Assert.assertEquals(3,assigned.size());
        cancelDiscoverable1.cancel();
        Assert.assertTrue(assignmentQueue.poll(30,TimeUnit.SECONDS).isEmpty());
        cancelSubscribe1.cancel();
        Assert.assertTrue(finishSemaphore.tryAcquire(2,TimeUnit.SECONDS));
        Cancellable cancelSubscribe2=subscribe(client,discoverable2,assignmentQueue,finishSemaphore);
        assigned=assignmentQueue.poll(30,TimeUnit.SECONDS);
        Assert.assertNotNull(assigned);
        Assert.assertEquals(5,assigned.size());
        client.submitRequirement(ResourceRequirement.builder(serviceName).build());
        Assert.assertTrue(assignmentQueue.poll(30,TimeUnit.SECONDS).isEmpty());
        client.submitRequirement(ResourceRequirement.builder(serviceName).addPartitions(""String_Node_Str"",1,1).build());
        assigned=assignmentQueue.poll(30,TimeUnit.SECONDS);
        Assert.assertNotNull(assigned);
        Assert.assertEquals(1,assigned.size());
        client.deleteRequirement(requirement.getName());
        Assert.assertTrue(assignmentQueue.poll(30,TimeUnit.SECONDS).isEmpty());
        cancelSubscribe2.cancel();
        Assert.assertTrue(finishSemaphore.tryAcquire(2,TimeUnit.SECONDS));
        cancelDiscoverable2.cancel();
      }
  finally {
        client.stopAndWait();
      }
    }
  finally {
      coordinator.stopAndWait();
    }
  }
  finally {
    zkClient.stopAndWait();
  }
}"
6451,"/** 
 * Returns the upper bound beyond which we can compact any increment deltas into a new sum.
 * @param columnFamily the column family name
 * @return the newest timestamp beyond which can compact delta increments
 */
public long getCompactionBound(byte[] columnFamily){
  if (txnlFamilies.contains(columnFamily)) {
    TransactionSnapshot snapshot=cache.getLatestState();
    return snapshot != null ? snapshot.getVisibilityUpperBound() : 0;
  }
 else {
    return Long.MAX_VALUE;
  }
}","/** 
 * Returns the upper bound beyond which we can compact any increment deltas into a new sum.
 * @param columnFamily the column family name
 * @return the newest timestamp beyond which can compact delta increments
 */
public long getCompactionBound(byte[] columnFamily){
  if (txnlFamilies.contains(columnFamily)) {
    TransactionVisibilityState snapshot=cache.getLatestState();
    return snapshot != null ? snapshot.getVisibilityUpperBound() : 0;
  }
 else {
    return Long.MAX_VALUE;
  }
}"
6452,"/** 
 * This forces an immediate update of the config cache. It should only be called from the refresh thread or from tests, to avoid having to add a sleep for the duration of the refresh interval. This method is synchronized to protect from race conditions if called directly from a test. Otherwise this is only called from the refresh thread, and there will not be concurrent invocations.
 * @throws IOException if failed to update config cache
 */
@VisibleForTesting public synchronized void updateCache() throws IOException {
  Map<byte[],QueueConsumerConfig> newCache=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  long now=System.currentTimeMillis();
  TransactionSnapshot txSnapshot=transactionSnapshotSupplier.get();
  if (txSnapshot == null) {
    LOG.debug(""String_Node_Str"");
    return;
  }
  HTableInterface table=hTableSupplier.getInput();
  try {
    Scan scan=new Scan();
    scan.addFamily(QueueEntryRow.COLUMN_FAMILY);
    Transaction tx=TxUtils.createDummyTransaction(txSnapshot);
    setScanAttribute(scan,TxConstants.TX_OPERATION_ATTRIBUTE_KEY,txCodec.encode(tx));
    ResultScanner scanner=table.getScanner(scan);
    int configCnt=0;
    for (    Result result : scanner) {
      if (!result.isEmpty()) {
        NavigableMap<byte[],byte[]> familyMap=result.getFamilyMap(QueueEntryRow.COLUMN_FAMILY);
        if (familyMap != null) {
          configCnt++;
          Map<ConsumerInstance,byte[]> consumerInstances=new HashMap<>();
          int numGroups=0;
          Long groupId=null;
          for (          Map.Entry<byte[],byte[]> entry : familyMap.entrySet()) {
            if (entry.getKey().length != STATE_COLUMN_SIZE) {
              continue;
            }
            long gid=Bytes.toLong(entry.getKey());
            int instanceId=Bytes.toInt(entry.getKey(),Bytes.SIZEOF_LONG);
            consumerInstances.put(new ConsumerInstance(gid,instanceId),entry.getValue());
            if (groupId == null || groupId != gid) {
              numGroups++;
              groupId=gid;
            }
          }
          byte[] queueName=result.getRow();
          newCache.put(queueName,new QueueConsumerConfig(consumerInstances,numGroups));
        }
      }
    }
    long elapsed=System.currentTimeMillis() - now;
    this.configCache=newCache;
    this.lastUpdated=now;
    if (LOG.isDebugEnabled()) {
      LOG.debug(""String_Node_Str"",configCnt,elapsed);
    }
  }
  finally {
    try {
      table.close();
    }
 catch (    IOException ioe) {
      LOG.error(""String_Node_Str"",queueConfigTableName,ioe);
    }
  }
}","/** 
 * This forces an immediate update of the config cache. It should only be called from the refresh thread or from tests, to avoid having to add a sleep for the duration of the refresh interval. This method is synchronized to protect from race conditions if called directly from a test. Otherwise this is only called from the refresh thread, and there will not be concurrent invocations.
 * @throws IOException if failed to update config cache
 */
@VisibleForTesting public synchronized void updateCache() throws IOException {
  Map<byte[],QueueConsumerConfig> newCache=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
  long now=System.currentTimeMillis();
  TransactionVisibilityState txSnapshot=transactionSnapshotSupplier.get();
  if (txSnapshot == null) {
    LOG.debug(""String_Node_Str"");
    return;
  }
  HTableInterface table=hTableSupplier.getInput();
  try {
    Scan scan=new Scan();
    scan.addFamily(QueueEntryRow.COLUMN_FAMILY);
    Transaction tx=TxUtils.createDummyTransaction(txSnapshot);
    setScanAttribute(scan,TxConstants.TX_OPERATION_ATTRIBUTE_KEY,txCodec.encode(tx));
    ResultScanner scanner=table.getScanner(scan);
    int configCnt=0;
    for (    Result result : scanner) {
      if (!result.isEmpty()) {
        NavigableMap<byte[],byte[]> familyMap=result.getFamilyMap(QueueEntryRow.COLUMN_FAMILY);
        if (familyMap != null) {
          configCnt++;
          Map<ConsumerInstance,byte[]> consumerInstances=new HashMap<>();
          int numGroups=0;
          Long groupId=null;
          for (          Map.Entry<byte[],byte[]> entry : familyMap.entrySet()) {
            if (entry.getKey().length != STATE_COLUMN_SIZE) {
              continue;
            }
            long gid=Bytes.toLong(entry.getKey());
            int instanceId=Bytes.toInt(entry.getKey(),Bytes.SIZEOF_LONG);
            consumerInstances.put(new ConsumerInstance(gid,instanceId),entry.getValue());
            if (groupId == null || groupId != gid) {
              numGroups++;
              groupId=gid;
            }
          }
          byte[] queueName=result.getRow();
          newCache.put(queueName,new QueueConsumerConfig(consumerInstances,numGroups));
        }
      }
    }
    long elapsed=System.currentTimeMillis() - now;
    this.configCache=newCache;
    this.lastUpdated=now;
    if (LOG.isDebugEnabled()) {
      LOG.debug(""String_Node_Str"",configCnt,elapsed);
    }
  }
  finally {
    try {
      table.close();
    }
 catch (    IOException ioe) {
      LOG.error(""String_Node_Str"",queueConfigTableName,ioe);
    }
  }
}"
6453,"/** 
 * Constructs a new instance.
 * @param queueConfigTableName table name that stores queue configuration
 * @param cConfReader reader to read the latest {@link CConfiguration}
 * @param transactionSnapshotSupplier A supplier for the latest {@link TransactionSnapshot}
 * @param hTableSupplier A supplier for creating {@link HTableInterface}.
 */
ConsumerConfigCache(TableName queueConfigTableName,CConfigurationReader cConfReader,Supplier<TransactionSnapshot> transactionSnapshotSupplier,InputSupplier<HTableInterface> hTableSupplier){
  this.queueConfigTableName=queueConfigTableName;
  this.cConfReader=cConfReader;
  this.transactionSnapshotSupplier=transactionSnapshotSupplier;
  this.hTableSupplier=hTableSupplier;
  this.txCodec=new TransactionCodec();
}","/** 
 * Constructs a new instance.
 * @param queueConfigTableName table name that stores queue configuration
 * @param cConfReader reader to read the latest {@link CConfiguration}
 * @param transactionSnapshotSupplier A supplier for the latest {@link TransactionSnapshot}
 * @param hTableSupplier A supplier for creating {@link HTableInterface}.
 */
ConsumerConfigCache(TableName queueConfigTableName,CConfigurationReader cConfReader,Supplier<TransactionVisibilityState> transactionSnapshotSupplier,InputSupplier<HTableInterface> hTableSupplier){
  this.queueConfigTableName=queueConfigTableName;
  this.cConfReader=cConfReader;
  this.transactionSnapshotSupplier=transactionSnapshotSupplier;
  this.hTableSupplier=hTableSupplier;
  this.txCodec=new TransactionCodec();
}"
6454,"public static ConsumerConfigCache getInstance(TableName tableName,CConfigurationReader cConfReader,Supplier<TransactionSnapshot> txSnapshotSupplier,InputSupplier<HTableInterface> hTableSupplier){
  ConsumerConfigCache cache=INSTANCES.get(tableName);
  if (cache == null) {
    cache=new ConsumerConfigCache(tableName,cConfReader,txSnapshotSupplier,hTableSupplier);
    if (INSTANCES.putIfAbsent(tableName,cache) == null) {
      cache.init();
    }
 else {
      cache=INSTANCES.get(tableName);
    }
  }
  return cache;
}","public static ConsumerConfigCache getInstance(TableName tableName,CConfigurationReader cConfReader,Supplier<TransactionVisibilityState> txSnapshotSupplier,InputSupplier<HTableInterface> hTableSupplier){
  ConsumerConfigCache cache=INSTANCES.get(tableName);
  if (cache == null) {
    cache=new ConsumerConfigCache(tableName,cConfReader,txSnapshotSupplier,hTableSupplier);
    if (INSTANCES.putIfAbsent(tableName,cache) == null) {
      cache.init();
    }
 else {
      cache=INSTANCES.get(tableName);
    }
  }
  return cache;
}"
6455,"private ConsumerConfigCache getConsumerConfigCache(QueueName queueName) throws Exception {
  TableId tableId=HBaseQueueAdmin.getConfigTableId(queueName);
  try (HTable hTable=tableUtil.createHTable(hConf,tableId)){
    HTableDescriptor htd=hTable.getTableDescriptor();
    final TableName configTableName=htd.getTableName();
    HTableNameConverter nameConverter=new HTableNameConverterFactory().get();
    CConfigurationReader cConfReader=new CConfigurationReader(hConf,nameConverter.getSysConfigTablePrefix(htd));
    return ConsumerConfigCache.getInstance(configTableName,cConfReader,new Supplier<TransactionSnapshot>(){
      @Override public TransactionSnapshot get(){
        try {
          return transactionManager.getSnapshot();
        }
 catch (        IOException e) {
          throw Throwables.propagate(e);
        }
      }
    }
,new InputSupplier<HTableInterface>(){
      @Override public HTableInterface getInput() throws IOException {
        return new HTable(hConf,configTableName);
      }
    }
);
  }
 }","private ConsumerConfigCache getConsumerConfigCache(QueueName queueName) throws Exception {
  TableId tableId=HBaseQueueAdmin.getConfigTableId(queueName);
  try (HTable hTable=tableUtil.createHTable(hConf,tableId)){
    HTableDescriptor htd=hTable.getTableDescriptor();
    final TableName configTableName=htd.getTableName();
    HTableNameConverter nameConverter=new HTableNameConverterFactory().get();
    CConfigurationReader cConfReader=new CConfigurationReader(hConf,nameConverter.getSysConfigTablePrefix(htd));
    return ConsumerConfigCache.getInstance(configTableName,cConfReader,new Supplier<TransactionVisibilityState>(){
      @Override public TransactionVisibilityState get(){
        try {
          return transactionManager.getSnapshot();
        }
 catch (        IOException e) {
          throw Throwables.propagate(e);
        }
      }
    }
,new InputSupplier<HTableInterface>(){
      @Override public HTableInterface getInput() throws IOException {
        return new HTable(hConf,configTableName);
      }
    }
);
  }
 }"
6456,"@Override public TransactionSnapshot get(){
  try {
    return transactionManager.getSnapshot();
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}","@Override public TransactionVisibilityState get(){
  try {
    return transactionManager.getSnapshot();
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
}"
6457,"@Override public void start(CoprocessorEnvironment env){
  if (env instanceof RegionCoprocessorEnvironment) {
    HTableDescriptor tableDesc=((RegionCoprocessorEnvironment)env).getRegion().getTableDesc();
    String hTableName=tableDesc.getNameAsString();
    String prefixBytes=tableDesc.getValue(HBaseQueueAdmin.PROPERTY_PREFIX_BYTES);
    try {
      this.prefixBytes=prefixBytes == null ? SaltedHBaseQueueStrategy.SALT_BYTES : Integer.parseInt(prefixBytes);
    }
 catch (    NumberFormatException e) {
      LOG.error(""String_Node_Str"" + HBaseQueueAdmin.PROPERTY_PREFIX_BYTES + ""String_Node_Str""+ ""String_Node_Str""+ SaltedHBaseQueueStrategy.SALT_BYTES,e);
      this.prefixBytes=SaltedHBaseQueueStrategy.SALT_BYTES;
    }
    HTable96NameConverter nameConverter=new HTable96NameConverter();
    namespaceId=nameConverter.from(tableDesc).getNamespace().getId();
    appName=HBaseQueueAdmin.getApplicationName(hTableName);
    flowName=HBaseQueueAdmin.getFlowName(hTableName);
    Configuration conf=env.getConfiguration();
    String hbaseNamespacePrefix=nameConverter.getNamespacePrefix(tableDesc);
    TableId queueConfigTableId=HBaseQueueAdmin.getConfigTableId(namespaceId);
    final String sysConfigTablePrefix=nameConverter.getSysConfigTablePrefix(tableDesc);
    txStateCache=new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,conf).get();
    txSnapshotSupplier=new Supplier<TransactionSnapshot>(){
      @Override public TransactionSnapshot get(){
        return txStateCache.getLatestState();
      }
    }
;
    configTableName=nameConverter.toTableName(hbaseNamespacePrefix,queueConfigTableId);
    cConfReader=new CConfigurationReader(conf,sysConfigTablePrefix);
    configCache=createConfigCache(env);
  }
}","@Override public void start(CoprocessorEnvironment env){
  if (env instanceof RegionCoprocessorEnvironment) {
    HTableDescriptor tableDesc=((RegionCoprocessorEnvironment)env).getRegion().getTableDesc();
    String hTableName=tableDesc.getNameAsString();
    String prefixBytes=tableDesc.getValue(HBaseQueueAdmin.PROPERTY_PREFIX_BYTES);
    try {
      this.prefixBytes=prefixBytes == null ? SaltedHBaseQueueStrategy.SALT_BYTES : Integer.parseInt(prefixBytes);
    }
 catch (    NumberFormatException e) {
      LOG.error(""String_Node_Str"" + HBaseQueueAdmin.PROPERTY_PREFIX_BYTES + ""String_Node_Str""+ ""String_Node_Str""+ SaltedHBaseQueueStrategy.SALT_BYTES,e);
      this.prefixBytes=SaltedHBaseQueueStrategy.SALT_BYTES;
    }
    HTable96NameConverter nameConverter=new HTable96NameConverter();
    namespaceId=nameConverter.from(tableDesc).getNamespace().getId();
    appName=HBaseQueueAdmin.getApplicationName(hTableName);
    flowName=HBaseQueueAdmin.getFlowName(hTableName);
    Configuration conf=env.getConfiguration();
    String hbaseNamespacePrefix=nameConverter.getNamespacePrefix(tableDesc);
    TableId queueConfigTableId=HBaseQueueAdmin.getConfigTableId(namespaceId);
    final String sysConfigTablePrefix=nameConverter.getSysConfigTablePrefix(tableDesc);
    txStateCache=new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,conf).get();
    txSnapshotSupplier=new Supplier<TransactionVisibilityState>(){
      @Override public TransactionVisibilityState get(){
        return txStateCache.getLatestState();
      }
    }
;
    configTableName=nameConverter.toTableName(hbaseNamespacePrefix,queueConfigTableId);
    cConfReader=new CConfigurationReader(conf,sysConfigTablePrefix);
    configCache=createConfigCache(env);
  }
}"
6458,"@Override public TransactionSnapshot get(){
  return txStateCache.getLatestState();
}","@Override public TransactionVisibilityState get(){
  return txStateCache.getLatestState();
}"
6459,"@Override public void start(CoprocessorEnvironment env){
  if (env instanceof RegionCoprocessorEnvironment) {
    HTableDescriptor tableDesc=((RegionCoprocessorEnvironment)env).getRegion().getTableDesc();
    String hTableName=tableDesc.getNameAsString();
    String prefixBytes=tableDesc.getValue(HBaseQueueAdmin.PROPERTY_PREFIX_BYTES);
    try {
      this.prefixBytes=prefixBytes == null ? SaltedHBaseQueueStrategy.SALT_BYTES : Integer.parseInt(prefixBytes);
    }
 catch (    NumberFormatException e) {
      LOG.error(""String_Node_Str"" + HBaseQueueAdmin.PROPERTY_PREFIX_BYTES + ""String_Node_Str""+ ""String_Node_Str""+ SaltedHBaseQueueStrategy.SALT_BYTES,e);
      this.prefixBytes=SaltedHBaseQueueStrategy.SALT_BYTES;
    }
    HTable98NameConverter nameConverter=new HTable98NameConverter();
    namespaceId=nameConverter.from(tableDesc).getNamespace().getId();
    appName=HBaseQueueAdmin.getApplicationName(hTableName);
    flowName=HBaseQueueAdmin.getFlowName(hTableName);
    Configuration conf=env.getConfiguration();
    String hbaseNamespacePrefix=nameConverter.getNamespacePrefix(tableDesc);
    TableId queueConfigTableId=HBaseQueueAdmin.getConfigTableId(namespaceId);
    final String sysConfigTablePrefix=nameConverter.getSysConfigTablePrefix(tableDesc);
    txStateCache=new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,conf).get();
    txSnapshotSupplier=new Supplier<TransactionSnapshot>(){
      @Override public TransactionSnapshot get(){
        return txStateCache.getLatestState();
      }
    }
;
    configTableName=nameConverter.toTableName(hbaseNamespacePrefix,queueConfigTableId);
    cConfReader=new CConfigurationReader(conf,sysConfigTablePrefix);
    configCache=createConfigCache(env);
  }
}","@Override public void start(CoprocessorEnvironment env){
  if (env instanceof RegionCoprocessorEnvironment) {
    HTableDescriptor tableDesc=((RegionCoprocessorEnvironment)env).getRegion().getTableDesc();
    String hTableName=tableDesc.getNameAsString();
    String prefixBytes=tableDesc.getValue(HBaseQueueAdmin.PROPERTY_PREFIX_BYTES);
    try {
      this.prefixBytes=prefixBytes == null ? SaltedHBaseQueueStrategy.SALT_BYTES : Integer.parseInt(prefixBytes);
    }
 catch (    NumberFormatException e) {
      LOG.error(""String_Node_Str"" + HBaseQueueAdmin.PROPERTY_PREFIX_BYTES + ""String_Node_Str""+ ""String_Node_Str""+ SaltedHBaseQueueStrategy.SALT_BYTES,e);
      this.prefixBytes=SaltedHBaseQueueStrategy.SALT_BYTES;
    }
    HTable98NameConverter nameConverter=new HTable98NameConverter();
    namespaceId=nameConverter.from(tableDesc).getNamespace().getId();
    appName=HBaseQueueAdmin.getApplicationName(hTableName);
    flowName=HBaseQueueAdmin.getFlowName(hTableName);
    Configuration conf=env.getConfiguration();
    String hbaseNamespacePrefix=nameConverter.getNamespacePrefix(tableDesc);
    TableId queueConfigTableId=HBaseQueueAdmin.getConfigTableId(namespaceId);
    final String sysConfigTablePrefix=nameConverter.getSysConfigTablePrefix(tableDesc);
    txStateCache=new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,conf).get();
    txSnapshotSupplier=new Supplier<TransactionVisibilityState>(){
      @Override public TransactionVisibilityState get(){
        return txStateCache.getLatestState();
      }
    }
;
    configTableName=nameConverter.toTableName(hbaseNamespacePrefix,queueConfigTableId);
    cConfReader=new CConfigurationReader(conf,sysConfigTablePrefix);
    configCache=createConfigCache(env);
  }
}"
6460,"@Override public TransactionSnapshot get(){
  return txStateCache.getLatestState();
}","@Override public TransactionVisibilityState get(){
  return txStateCache.getLatestState();
}"
6461,"@Override public void start(CoprocessorEnvironment env){
  if (env instanceof RegionCoprocessorEnvironment) {
    HTableDescriptor tableDesc=((RegionCoprocessorEnvironment)env).getRegion().getTableDesc();
    String hTableName=tableDesc.getNameAsString();
    String prefixBytes=tableDesc.getValue(HBaseQueueAdmin.PROPERTY_PREFIX_BYTES);
    try {
      this.prefixBytes=prefixBytes == null ? SaltedHBaseQueueStrategy.SALT_BYTES : Integer.parseInt(prefixBytes);
    }
 catch (    NumberFormatException e) {
      LOG.error(""String_Node_Str"" + HBaseQueueAdmin.PROPERTY_PREFIX_BYTES + ""String_Node_Str""+ ""String_Node_Str""+ SaltedHBaseQueueStrategy.SALT_BYTES,e);
      this.prefixBytes=SaltedHBaseQueueStrategy.SALT_BYTES;
    }
    HTable10CDHNameConverter nameConverter=new HTable10CDHNameConverter();
    namespaceId=nameConverter.from(tableDesc).getNamespace().getId();
    appName=HBaseQueueAdmin.getApplicationName(hTableName);
    flowName=HBaseQueueAdmin.getFlowName(hTableName);
    Configuration conf=env.getConfiguration();
    String hbaseNamespacePrefix=nameConverter.getNamespacePrefix(tableDesc);
    TableId queueConfigTableId=HBaseQueueAdmin.getConfigTableId(namespaceId);
    final String sysConfigTablePrefix=nameConverter.getSysConfigTablePrefix(tableDesc);
    txStateCache=new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,conf).get();
    txSnapshotSupplier=new Supplier<TransactionSnapshot>(){
      @Override public TransactionSnapshot get(){
        return txStateCache.getLatestState();
      }
    }
;
    configTableName=nameConverter.toTableName(hbaseNamespacePrefix,queueConfigTableId);
    cConfReader=new CConfigurationReader(conf,sysConfigTablePrefix);
    configCache=createConfigCache(env);
  }
}","@Override public void start(CoprocessorEnvironment env){
  if (env instanceof RegionCoprocessorEnvironment) {
    HTableDescriptor tableDesc=((RegionCoprocessorEnvironment)env).getRegion().getTableDesc();
    String hTableName=tableDesc.getNameAsString();
    String prefixBytes=tableDesc.getValue(HBaseQueueAdmin.PROPERTY_PREFIX_BYTES);
    try {
      this.prefixBytes=prefixBytes == null ? SaltedHBaseQueueStrategy.SALT_BYTES : Integer.parseInt(prefixBytes);
    }
 catch (    NumberFormatException e) {
      LOG.error(""String_Node_Str"" + HBaseQueueAdmin.PROPERTY_PREFIX_BYTES + ""String_Node_Str""+ ""String_Node_Str""+ SaltedHBaseQueueStrategy.SALT_BYTES,e);
      this.prefixBytes=SaltedHBaseQueueStrategy.SALT_BYTES;
    }
    HTable10CDHNameConverter nameConverter=new HTable10CDHNameConverter();
    namespaceId=nameConverter.from(tableDesc).getNamespace().getId();
    appName=HBaseQueueAdmin.getApplicationName(hTableName);
    flowName=HBaseQueueAdmin.getFlowName(hTableName);
    Configuration conf=env.getConfiguration();
    String hbaseNamespacePrefix=nameConverter.getNamespacePrefix(tableDesc);
    TableId queueConfigTableId=HBaseQueueAdmin.getConfigTableId(namespaceId);
    final String sysConfigTablePrefix=nameConverter.getSysConfigTablePrefix(tableDesc);
    txStateCache=new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,conf).get();
    txSnapshotSupplier=new Supplier<TransactionVisibilityState>(){
      @Override public TransactionVisibilityState get(){
        return txStateCache.getLatestState();
      }
    }
;
    configTableName=nameConverter.toTableName(hbaseNamespacePrefix,queueConfigTableId);
    cConfReader=new CConfigurationReader(conf,sysConfigTablePrefix);
    configCache=createConfigCache(env);
  }
}"
6462,"@Override public TransactionSnapshot get(){
  return txStateCache.getLatestState();
}","@Override public TransactionVisibilityState get(){
  return txStateCache.getLatestState();
}"
6463,"@Override public void start(CoprocessorEnvironment env){
  if (env instanceof RegionCoprocessorEnvironment) {
    HTableDescriptor tableDesc=((RegionCoprocessorEnvironment)env).getRegion().getTableDesc();
    String hTableName=tableDesc.getNameAsString();
    String prefixBytes=tableDesc.getValue(HBaseQueueAdmin.PROPERTY_PREFIX_BYTES);
    try {
      this.prefixBytes=prefixBytes == null ? SaltedHBaseQueueStrategy.SALT_BYTES : Integer.parseInt(prefixBytes);
    }
 catch (    NumberFormatException e) {
      LOG.error(""String_Node_Str"" + HBaseQueueAdmin.PROPERTY_PREFIX_BYTES + ""String_Node_Str""+ ""String_Node_Str""+ SaltedHBaseQueueStrategy.SALT_BYTES,e);
      this.prefixBytes=SaltedHBaseQueueStrategy.SALT_BYTES;
    }
    HTable10NameConverter nameConverter=new HTable10NameConverter();
    namespaceId=nameConverter.from(tableDesc).getNamespace().getId();
    appName=HBaseQueueAdmin.getApplicationName(hTableName);
    flowName=HBaseQueueAdmin.getFlowName(hTableName);
    Configuration conf=env.getConfiguration();
    String hbaseNamespacePrefix=nameConverter.getNamespacePrefix(tableDesc);
    TableId queueConfigTableId=HBaseQueueAdmin.getConfigTableId(namespaceId);
    final String sysConfigTablePrefix=nameConverter.getSysConfigTablePrefix(tableDesc);
    txStateCache=new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,conf).get();
    txSnapshotSupplier=new Supplier<TransactionSnapshot>(){
      @Override public TransactionSnapshot get(){
        return txStateCache.getLatestState();
      }
    }
;
    configTableName=nameConverter.toTableName(hbaseNamespacePrefix,queueConfigTableId);
    cConfReader=new CConfigurationReader(conf,sysConfigTablePrefix);
    configCache=createConfigCache(env);
  }
}","@Override public void start(CoprocessorEnvironment env){
  if (env instanceof RegionCoprocessorEnvironment) {
    HTableDescriptor tableDesc=((RegionCoprocessorEnvironment)env).getRegion().getTableDesc();
    String hTableName=tableDesc.getNameAsString();
    String prefixBytes=tableDesc.getValue(HBaseQueueAdmin.PROPERTY_PREFIX_BYTES);
    try {
      this.prefixBytes=prefixBytes == null ? SaltedHBaseQueueStrategy.SALT_BYTES : Integer.parseInt(prefixBytes);
    }
 catch (    NumberFormatException e) {
      LOG.error(""String_Node_Str"" + HBaseQueueAdmin.PROPERTY_PREFIX_BYTES + ""String_Node_Str""+ ""String_Node_Str""+ SaltedHBaseQueueStrategy.SALT_BYTES,e);
      this.prefixBytes=SaltedHBaseQueueStrategy.SALT_BYTES;
    }
    HTable10NameConverter nameConverter=new HTable10NameConverter();
    namespaceId=nameConverter.from(tableDesc).getNamespace().getId();
    appName=HBaseQueueAdmin.getApplicationName(hTableName);
    flowName=HBaseQueueAdmin.getFlowName(hTableName);
    Configuration conf=env.getConfiguration();
    String hbaseNamespacePrefix=nameConverter.getNamespacePrefix(tableDesc);
    TableId queueConfigTableId=HBaseQueueAdmin.getConfigTableId(namespaceId);
    final String sysConfigTablePrefix=nameConverter.getSysConfigTablePrefix(tableDesc);
    txStateCache=new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,conf).get();
    txSnapshotSupplier=new Supplier<TransactionVisibilityState>(){
      @Override public TransactionVisibilityState get(){
        return txStateCache.getLatestState();
      }
    }
;
    configTableName=nameConverter.toTableName(hbaseNamespacePrefix,queueConfigTableId);
    cConfReader=new CConfigurationReader(conf,sysConfigTablePrefix);
    configCache=createConfigCache(env);
  }
}"
6464,"@Override public TransactionSnapshot get(){
  return txStateCache.getLatestState();
}","@Override public TransactionVisibilityState get(){
  return txStateCache.getLatestState();
}"
6465,"@Override public void start(CoprocessorEnvironment env){
  if (env instanceof RegionCoprocessorEnvironment) {
    HTableDescriptor tableDesc=((RegionCoprocessorEnvironment)env).getRegion().getTableDesc();
    String hTableName=tableDesc.getNameAsString();
    String prefixBytes=tableDesc.getValue(HBaseQueueAdmin.PROPERTY_PREFIX_BYTES);
    try {
      this.prefixBytes=prefixBytes == null ? SaltedHBaseQueueStrategy.SALT_BYTES : Integer.parseInt(prefixBytes);
    }
 catch (    NumberFormatException e) {
      LOG.error(""String_Node_Str"" + HBaseQueueAdmin.PROPERTY_PREFIX_BYTES + ""String_Node_Str""+ ""String_Node_Str""+ SaltedHBaseQueueStrategy.SALT_BYTES,e);
      this.prefixBytes=SaltedHBaseQueueStrategy.SALT_BYTES;
    }
    HTable11NameConverter nameConverter=new HTable11NameConverter();
    namespaceId=nameConverter.from(tableDesc).getNamespace().getId();
    appName=HBaseQueueAdmin.getApplicationName(hTableName);
    flowName=HBaseQueueAdmin.getFlowName(hTableName);
    Configuration conf=env.getConfiguration();
    String hbaseNamespacePrefix=nameConverter.getNamespacePrefix(tableDesc);
    TableId queueConfigTableId=HBaseQueueAdmin.getConfigTableId(namespaceId);
    final String sysConfigTablePrefix=nameConverter.getSysConfigTablePrefix(tableDesc);
    txStateCache=new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,conf).get();
    txSnapshotSupplier=new Supplier<TransactionSnapshot>(){
      @Override public TransactionSnapshot get(){
        return txStateCache.getLatestState();
      }
    }
;
    configTableName=nameConverter.toTableName(hbaseNamespacePrefix,queueConfigTableId);
    cConfReader=new CConfigurationReader(conf,sysConfigTablePrefix);
    configCache=createConfigCache(env);
  }
}","@Override public void start(CoprocessorEnvironment env){
  if (env instanceof RegionCoprocessorEnvironment) {
    HTableDescriptor tableDesc=((RegionCoprocessorEnvironment)env).getRegion().getTableDesc();
    String hTableName=tableDesc.getNameAsString();
    String prefixBytes=tableDesc.getValue(HBaseQueueAdmin.PROPERTY_PREFIX_BYTES);
    try {
      this.prefixBytes=prefixBytes == null ? SaltedHBaseQueueStrategy.SALT_BYTES : Integer.parseInt(prefixBytes);
    }
 catch (    NumberFormatException e) {
      LOG.error(""String_Node_Str"" + HBaseQueueAdmin.PROPERTY_PREFIX_BYTES + ""String_Node_Str""+ ""String_Node_Str""+ SaltedHBaseQueueStrategy.SALT_BYTES,e);
      this.prefixBytes=SaltedHBaseQueueStrategy.SALT_BYTES;
    }
    HTable11NameConverter nameConverter=new HTable11NameConverter();
    namespaceId=nameConverter.from(tableDesc).getNamespace().getId();
    appName=HBaseQueueAdmin.getApplicationName(hTableName);
    flowName=HBaseQueueAdmin.getFlowName(hTableName);
    Configuration conf=env.getConfiguration();
    String hbaseNamespacePrefix=nameConverter.getNamespacePrefix(tableDesc);
    TableId queueConfigTableId=HBaseQueueAdmin.getConfigTableId(namespaceId);
    final String sysConfigTablePrefix=nameConverter.getSysConfigTablePrefix(tableDesc);
    txStateCache=new DefaultTransactionStateCacheSupplier(sysConfigTablePrefix,conf).get();
    txSnapshotSupplier=new Supplier<TransactionVisibilityState>(){
      @Override public TransactionVisibilityState get(){
        return txStateCache.getLatestState();
      }
    }
;
    configTableName=nameConverter.toTableName(hbaseNamespacePrefix,queueConfigTableId);
    cConfReader=new CConfigurationReader(conf,sysConfigTablePrefix);
    configCache=createConfigCache(env);
  }
}"
6466,"@Override public TransactionSnapshot get(){
  return txStateCache.getLatestState();
}","@Override public TransactionVisibilityState get(){
  return txStateCache.getLatestState();
}"
6467,"public void tryConnect(CLIConnectionConfig connectionConfig,PrintStream output,boolean debug) throws Exception {
  try {
    UserAccessToken userToken=acquireAccessToken(clientConfig,connectionConfig,output,debug);
    AccessToken accessToken=null;
    if (userToken != null) {
      accessToken=userToken.getAccessToken();
      connectionConfig=new CLIConnectionConfig(connectionConfig,connectionConfig.getNamespace(),userToken.getUsername());
    }
    checkConnection(clientConfig,connectionConfig,accessToken);
    setConnectionConfig(connectionConfig);
    clientConfig.setAccessToken(accessToken);
    output.printf(""String_Node_Str"",connectionConfig.getURI().toString());
    output.println();
  }
 catch (  IOException e) {
    throw new IOException(String.format(""String_Node_Str"",connectionConfig.getURI().toString(),e.getMessage()),e);
  }
}","public void tryConnect(CLIConnectionConfig connectionConfig,boolean verifySSLCert,PrintStream output,boolean debug) throws Exception {
  try {
    clientConfig.setVerifySSLCert(verifySSLCert);
    UserAccessToken userToken=acquireAccessToken(clientConfig,connectionConfig,output,debug);
    AccessToken accessToken=null;
    if (userToken != null) {
      accessToken=userToken.getAccessToken();
      connectionConfig=new CLIConnectionConfig(connectionConfig,connectionConfig.getNamespace(),userToken.getUsername());
    }
    checkConnection(clientConfig,connectionConfig,accessToken);
    setConnectionConfig(connectionConfig);
    clientConfig.setAccessToken(accessToken);
    output.printf(""String_Node_Str"",connectionConfig.getURI().toString());
    output.println();
  }
 catch (  IOException e) {
    throw new IOException(String.format(""String_Node_Str"",connectionConfig.getURI().toString(),e.getMessage()),e);
  }
}"
6468,"public CLIMain(final LaunchOptions options,final CLIConfig cliConfig) throws URISyntaxException, IOException {
  this.options=options;
  this.cliConfig=cliConfig;
  cliConfig.getClientConfig().setVerifySSLCert(options.isVerifySSL());
  injector=Guice.createInjector(new AbstractModule(){
    @Override protected void configure(){
      bind(LaunchOptions.class).toInstance(options);
      bind(CConfiguration.class).toInstance(CConfiguration.create());
      bind(PrintStream.class).toInstance(cliConfig.getOutput());
      bind(CLIConfig.class).toInstance(cliConfig);
      bind(ClientConfig.class).toInstance(cliConfig.getClientConfig());
    }
  }
);
  this.commands=ImmutableList.of(injector.getInstance(DefaultCommands.class),new CommandSet<>(ImmutableList.<Command>of(new HelpCommand(getCommandsSupplier(),cliConfig),new SearchCommandsCommand(getCommandsSupplier(),cliConfig))));
  filePathResolver=injector.getInstance(FilePathResolver.class);
  Map<String,Completer> completers=injector.getInstance(DefaultCompleters.class).get();
  cli=new CLI<>(Iterables.concat(commands),completers);
  cli.setExceptionHandler(new CLIExceptionHandler<Exception>(){
    @Override public boolean handleException(    PrintStream output,    Exception e,    int timesRetried){
      if (e instanceof SSLHandshakeException) {
        output.printf(""String_Node_Str"",VERIFY_SSL_OPTION.getLongOpt());
      }
 else       if (e instanceof InvalidCommandException) {
        InvalidCommandException ex=(InvalidCommandException)e;
        output.printf(""String_Node_Str"",ex.getInput());
      }
 else       if (e instanceof DisconnectedException || e instanceof ConnectException) {
        cli.getReader().setPrompt(""String_Node_Str"");
      }
 else {
        output.println(""String_Node_Str"" + e.getMessage());
      }
      if (options.isDebug()) {
        e.printStackTrace(output);
      }
      return false;
    }
  }
);
  cli.addCompleterSupplier(injector.getInstance(EndpointSupplier.class));
  cli.getReader().setExpandEvents(false);
  cliConfig.addHostnameChangeListener(new CLIConfig.ConnectionChangeListener(){
    @Override public void onConnectionChanged(    CLIConnectionConfig config){
      updateCLIPrompt(config);
    }
  }
);
}","public CLIMain(final LaunchOptions options,final CLIConfig cliConfig) throws URISyntaxException, IOException {
  this.options=options;
  this.cliConfig=cliConfig;
  injector=Guice.createInjector(new AbstractModule(){
    @Override protected void configure(){
      bind(LaunchOptions.class).toInstance(options);
      bind(CConfiguration.class).toInstance(CConfiguration.create());
      bind(PrintStream.class).toInstance(cliConfig.getOutput());
      bind(CLIConfig.class).toInstance(cliConfig);
      bind(ClientConfig.class).toInstance(cliConfig.getClientConfig());
    }
  }
);
  this.commands=ImmutableList.of(injector.getInstance(DefaultCommands.class),new CommandSet<>(ImmutableList.<Command>of(new HelpCommand(getCommandsSupplier(),cliConfig),new SearchCommandsCommand(getCommandsSupplier(),cliConfig))));
  filePathResolver=injector.getInstance(FilePathResolver.class);
  Map<String,Completer> completers=injector.getInstance(DefaultCompleters.class).get();
  cli=new CLI<>(Iterables.concat(commands),completers);
  cli.setExceptionHandler(new CLIExceptionHandler<Exception>(){
    @Override public boolean handleException(    PrintStream output,    Exception e,    int timesRetried){
      if (e instanceof SSLHandshakeException) {
        output.printf(""String_Node_Str"",VERIFY_SSL_OPTION.getLongOpt());
      }
 else       if (e instanceof InvalidCommandException) {
        InvalidCommandException ex=(InvalidCommandException)e;
        output.printf(""String_Node_Str"",ex.getInput());
      }
 else       if (e instanceof DisconnectedException || e instanceof ConnectException) {
        cli.getReader().setPrompt(""String_Node_Str"");
      }
 else {
        output.println(""String_Node_Str"" + e.getMessage());
      }
      if (options.isDebug()) {
        e.printStackTrace(output);
      }
      return false;
    }
  }
);
  cli.addCompleterSupplier(injector.getInstance(EndpointSupplier.class));
  cli.getReader().setExpandEvents(false);
  cliConfig.addHostnameChangeListener(new CLIConfig.ConnectionChangeListener(){
    @Override public void onConnectionChanged(    CLIConnectionConfig config){
      updateCLIPrompt(config);
    }
  }
);
}"
6469,"/** 
 * Tries to autoconnect to the provided URI in options.
 */
public boolean tryAutoconnect(CommandLine command){
  if (!options.isAutoconnect()) {
    return true;
  }
  InstanceURIParser instanceURIParser=injector.getInstance(InstanceURIParser.class);
  try {
    CLIConnectionConfig connection=instanceURIParser.parse(options.getUri());
    cliConfig.tryConnect(connection,cliConfig.getOutput(),options.isDebug());
    return true;
  }
 catch (  Exception e) {
    if (options.isDebug()) {
      e.printStackTrace(cliConfig.getOutput());
    }
 else {
      cliConfig.getOutput().println(e.getMessage());
    }
    if (!command.hasOption(URI_OPTION.getOpt())) {
      cliConfig.getOutput().printf(""String_Node_Str"");
    }
    return false;
  }
}","/** 
 * Tries to autoconnect to the provided URI in options.
 */
public boolean tryAutoconnect(CommandLine command){
  if (!options.isAutoconnect()) {
    return true;
  }
  InstanceURIParser instanceURIParser=injector.getInstance(InstanceURIParser.class);
  try {
    CLIConnectionConfig connection=instanceURIParser.parse(options.getUri());
    cliConfig.tryConnect(connection,options.isVerifySSL(),cliConfig.getOutput(),options.isDebug());
    return true;
  }
 catch (  Exception e) {
    if (options.isDebug()) {
      e.printStackTrace(cliConfig.getOutput());
    }
 else {
      cliConfig.getOutput().println(e.getMessage());
    }
    if (!command.hasOption(URI_OPTION.getOpt())) {
      cliConfig.getOutput().printf(""String_Node_Str"");
    }
    return false;
  }
}"
6470,"@Override public String getPattern(){
  return ""String_Node_Str"";
}","@Override public String getPattern(){
  return String.format(""String_Node_Str"",ArgumentName.INSTANCE_URI,ArgumentName.VERIFY_SSL_CERT);
}"
6471,"@Override public void execute(Arguments arguments,PrintStream output) throws Exception {
  String instanceURI=arguments.get(""String_Node_Str"");
  CLIConnectionConfig connection=instanceURIParser.parse(instanceURI);
  try {
    cliConfig.tryConnect(connection,output,debug);
  }
 catch (  Exception e) {
    output.println(""String_Node_Str"" + instanceURI + ""String_Node_Str""+ e.getMessage());
    if (debug) {
      e.printStackTrace(output);
    }
  }
}","@Override public void execute(Arguments arguments,PrintStream output) throws Exception {
  String instanceURI=arguments.get(ArgumentName.INSTANCE_URI.toString());
  String verifySSLCertString=arguments.getOptional(ArgumentName.VERIFY_SSL_CERT.toString());
  boolean verifySSLCert=verifySSLCertString != null ? Boolean.valueOf(verifySSLCertString) : true;
  CLIConnectionConfig connection=instanceURIParser.parse(instanceURI);
  try {
    cliConfig.tryConnect(connection,verifySSLCert,output,debug);
  }
 catch (  Exception e) {
    output.println(""String_Node_Str"" + instanceURI + ""String_Node_Str""+ e.getMessage());
    if (debug) {
      e.printStackTrace(output);
    }
  }
}"
6472,"@Before public void setUp() throws Throwable {
  super.setUp();
  appClient=new ApplicationClient(clientConfig);
  queryClient=new QueryClient(clientConfig);
  programClient=new ProgramClient(clientConfig);
  streamClient=new StreamClient(clientConfig);
  String accessToken=(clientConfig.getAccessToken() == null) ? null : clientConfig.getAccessToken().getValue();
  ConnectionConfig connectionConfig=clientConfig.getConnectionConfig();
  exploreClient=new FixedAddressExploreClient(connectionConfig.getHostname(),connectionConfig.getPort(),accessToken);
  namespaceClient=new NamespaceClient(clientConfig);
}","@Before public void setUp() throws Throwable {
  super.setUp();
  appClient=new ApplicationClient(clientConfig);
  queryClient=new QueryClient(clientConfig);
  programClient=new ProgramClient(clientConfig);
  streamClient=new StreamClient(clientConfig);
  String accessToken=(clientConfig.getAccessToken() == null) ? null : clientConfig.getAccessToken().getValue();
  ConnectionConfig connectionConfig=clientConfig.getConnectionConfig();
  exploreClient=new FixedAddressExploreClient(connectionConfig.getHostname(),connectionConfig.getPort(),accessToken,connectionConfig.isSSLEnabled(),clientConfig.isVerifySSLCert());
  namespaceClient=new NamespaceClient(clientConfig);
}"
6473,"@Override public String get(){
  if (config.getAccessToken() != null) {
    return config.getAccessToken().getValue();
  }
  return null;
}","@Override public Boolean get(){
  return config.isVerifySSLCert();
}"
6474,"@Inject public QueryClient(final ClientConfig config){
  Supplier<String> hostname=new Supplier<String>(){
    @Override public String get(){
      return config.getConnectionConfig().getHostname();
    }
  }
;
  Supplier<Integer> port=new Supplier<Integer>(){
    @Override public Integer get(){
      return config.getConnectionConfig().getPort();
    }
  }
;
  Supplier<String> accessToken=new Supplier<String>(){
    @Override public String get(){
      if (config.getAccessToken() != null) {
        return config.getAccessToken().getValue();
      }
      return null;
    }
  }
;
  exploreClient=new SuppliedAddressExploreClient(hostname,port,accessToken);
}","@Inject public QueryClient(final ClientConfig config){
  Supplier<String> hostname=new Supplier<String>(){
    @Override public String get(){
      return config.getConnectionConfig().getHostname();
    }
  }
;
  Supplier<Integer> port=new Supplier<Integer>(){
    @Override public Integer get(){
      return config.getConnectionConfig().getPort();
    }
  }
;
  Supplier<String> accessToken=new Supplier<String>(){
    @Override public String get(){
      if (config.getAccessToken() != null) {
        return config.getAccessToken().getValue();
      }
      return null;
    }
  }
;
  Supplier<Boolean> sslEnabled=new Supplier<Boolean>(){
    @Override public Boolean get(){
      return config.getConnectionConfig().isSSLEnabled();
    }
  }
;
  Supplier<Boolean> verifySSLCert=new Supplier<Boolean>(){
    @Override public Boolean get(){
      return config.isVerifySSLCert();
    }
  }
;
  exploreClient=new SuppliedAddressExploreClient(hostname,port,accessToken,sslEnabled,verifySSLCert);
}"
6475,"@Override public FetchedMessage next(){
  recordLastOffset();
  FetchedMessage message=delegate.next();
  lastTopicPartition=message.getTopicPartition();
  lastOffset=message.getNextOffset();
  messageCount.incrementAndGet();
  return message;
}","@Override public FetchedMessage next(){
  FetchedMessage message=delegate.next();
  lastTopicPartition=message.getTopicPartition();
  lastOffset=message.getNextOffset();
  messageCount.incrementAndGet();
  recordOffset();
  return message;
}"
6476,"@Override public void onReceived(Iterator<FetchedMessage> messages){
  delegate.onReceived(new OffsetTrackingIterator(messages));
}","@Override public void onReceived(Iterator<FetchedMessage> messages){
  delegate.onReceived(new OffsetTrackingIterator(messages));
  if (messageCount.get() >= persistThreshold) {
    messageCount.set(0);
    persistOffsets();
  }
}"
6477,"@Override public void finished(){
  try {
    delegate.finished();
  }
  finally {
    persist();
  }
}","@Override public void finished(){
  try {
    delegate.finished();
  }
  finally {
    persistOffsets();
  }
}"
6478,"@GET @Path(""String_Node_Str"") public void getArtifactPlugins(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginType,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws NamespaceNotFoundException, BadRequestException {
  Id.Namespace namespace=validateAndGetNamespace(namespaceId,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(namespace,artifactName,artifactVersion);
  try {
    SortedMap<ArtifactDescriptor,List<PluginClass>> plugins=artifactRepository.getPlugins(artifactId,pluginType);
    List<PluginSummary> pluginSummaries=Lists.newArrayList();
    for (    Map.Entry<ArtifactDescriptor,List<PluginClass>> pluginsEntry : plugins.entrySet()) {
      ArtifactDescriptor pluginArtifact=pluginsEntry.getKey();
      ArtifactSummary pluginArtifactSummary=ArtifactSummary.from(pluginArtifact.getArtifactId());
      for (      PluginClass pluginClass : pluginsEntry.getValue()) {
        pluginSummaries.add(new PluginSummary(pluginClass.getName(),pluginClass.getType(),pluginClass.getDescription(),pluginClass.getClassName(),pluginArtifactSummary));
      }
    }
    responder.sendJson(HttpResponseStatus.OK,pluginSummaries);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
  }
}","@GET @Path(""String_Node_Str"") public void getArtifactPlugins(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginType,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws NamespaceNotFoundException, BadRequestException, ArtifactNotFoundException {
  Id.Namespace namespace=validateAndGetNamespace(namespaceId,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(namespace,artifactName,artifactVersion);
  try {
    SortedMap<ArtifactDescriptor,List<PluginClass>> plugins=artifactRepository.getPlugins(artifactId,pluginType);
    List<PluginSummary> pluginSummaries=Lists.newArrayList();
    for (    Map.Entry<ArtifactDescriptor,List<PluginClass>> pluginsEntry : plugins.entrySet()) {
      ArtifactDescriptor pluginArtifact=pluginsEntry.getKey();
      ArtifactSummary pluginArtifactSummary=ArtifactSummary.from(pluginArtifact.getArtifactId());
      for (      PluginClass pluginClass : pluginsEntry.getValue()) {
        pluginSummaries.add(new PluginSummary(pluginClass.getName(),pluginClass.getType(),pluginClass.getDescription(),pluginClass.getClassName(),pluginArtifactSummary));
      }
    }
    responder.sendJson(HttpResponseStatus.OK,pluginSummaries);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
  }
}"
6479,"@GET @Path(""String_Node_Str"") public void getArtifactPluginTypes(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws NamespaceNotFoundException, BadRequestException {
  Id.Namespace namespace=validateAndGetNamespace(namespaceId,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(namespace,artifactName,artifactVersion);
  try {
    SortedMap<ArtifactDescriptor,List<PluginClass>> plugins=artifactRepository.getPlugins(artifactId);
    Set<String> pluginTypes=Sets.newHashSet();
    for (    List<PluginClass> pluginClasses : plugins.values()) {
      for (      PluginClass pluginClass : pluginClasses) {
        pluginTypes.add(pluginClass.getType());
      }
    }
    responder.sendJson(HttpResponseStatus.OK,pluginTypes);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
  }
}","@GET @Path(""String_Node_Str"") public void getArtifactPluginTypes(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws NamespaceNotFoundException, BadRequestException, ArtifactNotFoundException {
  Id.Namespace namespace=validateAndGetNamespace(namespaceId,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(namespace,artifactName,artifactVersion);
  try {
    SortedMap<ArtifactDescriptor,List<PluginClass>> plugins=artifactRepository.getPlugins(artifactId);
    Set<String> pluginTypes=Sets.newHashSet();
    for (    List<PluginClass> pluginClasses : plugins.values()) {
      for (      PluginClass pluginClass : pluginClasses) {
        pluginTypes.add(pluginClass.getType());
      }
    }
    responder.sendJson(HttpResponseStatus.OK,pluginTypes);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
  }
}"
6480,"@GET @Path(""String_Node_Str"" + ""String_Node_Str"") public void getArtifactPlugin(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginType,@PathParam(""String_Node_Str"") String pluginName,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws NamespaceNotFoundException, BadRequestException {
  Id.Namespace namespace=validateAndGetNamespace(namespaceId,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(namespace,artifactName,artifactVersion);
  try {
    SortedMap<ArtifactDescriptor,PluginClass> plugins=artifactRepository.getPlugins(artifactId,pluginType,pluginName);
    List<PluginInfo> pluginInfos=Lists.newArrayList();
    for (    Map.Entry<ArtifactDescriptor,PluginClass> pluginsEntry : plugins.entrySet()) {
      ArtifactDescriptor pluginArtifact=pluginsEntry.getKey();
      ArtifactSummary pluginArtifactSummary=ArtifactSummary.from(pluginArtifact.getArtifactId());
      PluginClass pluginClass=pluginsEntry.getValue();
      pluginInfos.add(new PluginInfo(pluginClass.getName(),pluginClass.getType(),pluginClass.getDescription(),pluginClass.getClassName(),pluginArtifactSummary,pluginClass.getProperties()));
    }
    responder.sendJson(HttpResponseStatus.OK,pluginInfos);
  }
 catch (  PluginNotExistsException e) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
  }
catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
  }
}","@GET @Path(""String_Node_Str"" + ""String_Node_Str"") public void getArtifactPlugin(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion,@PathParam(""String_Node_Str"") String pluginType,@PathParam(""String_Node_Str"") String pluginName,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") String scope) throws NamespaceNotFoundException, BadRequestException, ArtifactNotFoundException {
  Id.Namespace namespace=validateAndGetNamespace(namespaceId,scope);
  Id.Artifact artifactId=validateAndGetArtifactId(namespace,artifactName,artifactVersion);
  try {
    SortedMap<ArtifactDescriptor,PluginClass> plugins=artifactRepository.getPlugins(artifactId,pluginType,pluginName);
    List<PluginInfo> pluginInfos=Lists.newArrayList();
    for (    Map.Entry<ArtifactDescriptor,PluginClass> pluginsEntry : plugins.entrySet()) {
      ArtifactDescriptor pluginArtifact=pluginsEntry.getKey();
      ArtifactSummary pluginArtifactSummary=ArtifactSummary.from(pluginArtifact.getArtifactId());
      PluginClass pluginClass=pluginsEntry.getValue();
      pluginInfos.add(new PluginInfo(pluginClass.getName(),pluginClass.getType(),pluginClass.getDescription(),pluginClass.getClassName(),pluginArtifactSummary,pluginClass.getProperties()));
    }
    responder.sendJson(HttpResponseStatus.OK,pluginInfos);
  }
 catch (  PluginNotExistsException e) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
  }
catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"");
  }
}"
6481,"@Nullable @Override public <T>T usePlugin(String pluginType,String pluginName,String pluginId,PluginProperties properties,PluginSelector selector){
  Plugin plugin=null;
  try {
    plugin=findPlugin(pluginType,pluginName,pluginId,properties,selector);
  }
 catch (  PluginNotExistsException e) {
    return null;
  }
  try {
    T instance=pluginInstantiator.newInstance(plugin);
    plugins.put(pluginId,plugin);
    return instance;
  }
 catch (  IOException e) {
    return null;
  }
catch (  ClassNotFoundException e) {
    throw Throwables.propagate(e);
  }
}","@Nullable @Override public <T>T usePlugin(String pluginType,String pluginName,String pluginId,PluginProperties properties,PluginSelector selector){
  Plugin plugin;
  try {
    plugin=findPlugin(pluginType,pluginName,pluginId,properties,selector);
  }
 catch (  PluginNotExistsException e) {
    return null;
  }
catch (  ArtifactNotFoundException e) {
    throw new IllegalStateException(String.format(""String_Node_Str"",artifactId));
  }
  try {
    T instance=pluginInstantiator.newInstance(plugin);
    plugins.put(pluginId,plugin);
    return instance;
  }
 catch (  IOException e) {
    return null;
  }
catch (  ClassNotFoundException e) {
    throw Throwables.propagate(e);
  }
}"
6482,"@Nullable @Override public <T>Class<T> usePluginClass(String pluginType,String pluginName,String pluginId,PluginProperties properties,PluginSelector selector){
  Plugin plugin;
  try {
    plugin=findPlugin(pluginType,pluginName,pluginId,properties,selector);
  }
 catch (  PluginNotExistsException e) {
    return null;
  }
  try {
    Class<T> cls=pluginInstantiator.loadClass(plugin);
    plugins.put(pluginId,plugin);
    return cls;
  }
 catch (  IOException e) {
    return null;
  }
catch (  ClassNotFoundException e) {
    throw Throwables.propagate(e);
  }
}","@Nullable @Override public <T>Class<T> usePluginClass(String pluginType,String pluginName,String pluginId,PluginProperties properties,PluginSelector selector){
  Plugin plugin;
  try {
    plugin=findPlugin(pluginType,pluginName,pluginId,properties,selector);
  }
 catch (  PluginNotExistsException e) {
    return null;
  }
catch (  ArtifactNotFoundException e) {
    throw new IllegalStateException(String.format(""String_Node_Str"",artifactId));
  }
  try {
    Class<T> cls=pluginInstantiator.loadClass(plugin);
    plugins.put(pluginId,plugin);
    return cls;
  }
 catch (  IOException e) {
    return null;
  }
catch (  ClassNotFoundException e) {
    throw Throwables.propagate(e);
  }
}"
6483,"private Plugin findPlugin(String pluginType,String pluginName,String pluginId,PluginProperties properties,PluginSelector selector) throws PluginNotExistsException {
  Preconditions.checkArgument(!plugins.containsKey(pluginId),""String_Node_Str"",pluginType,pluginName);
  Preconditions.checkArgument(properties != null,""String_Node_Str"");
  Map.Entry<ArtifactDescriptor,PluginClass> pluginEntry;
  try {
    pluginEntry=artifactRepository.findPlugin(artifactId,pluginType,pluginName,selector);
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
  for (  PluginPropertyField field : pluginEntry.getValue().getProperties().values()) {
    Preconditions.checkArgument(!field.isRequired() || (properties != null && properties.getProperties().containsKey(field.getName())),""String_Node_Str"",field.getName(),pluginType,pluginName);
  }
  ArtifactId artifactId=pluginEntry.getKey().getArtifactId();
  try {
    pluginInstantiator.addArtifact(pluginEntry.getKey().getLocation(),artifactId);
  }
 catch (  IOException e) {
    Throwables.propagate(e);
  }
  return new Plugin(artifactId,pluginEntry.getValue(),properties);
}","private Plugin findPlugin(String pluginType,String pluginName,String pluginId,PluginProperties properties,PluginSelector selector) throws PluginNotExistsException, ArtifactNotFoundException {
  Preconditions.checkArgument(!plugins.containsKey(pluginId),""String_Node_Str"",pluginType,pluginName);
  Preconditions.checkArgument(properties != null,""String_Node_Str"");
  Map.Entry<ArtifactDescriptor,PluginClass> pluginEntry;
  try {
    pluginEntry=artifactRepository.findPlugin(artifactId,pluginType,pluginName,selector);
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
  for (  PluginPropertyField field : pluginEntry.getValue().getProperties().values()) {
    Preconditions.checkArgument(!field.isRequired() || (properties != null && properties.getProperties().containsKey(field.getName())),""String_Node_Str"",field.getName(),pluginType,pluginName);
  }
  ArtifactId artifactId=pluginEntry.getKey().getArtifactId();
  try {
    pluginInstantiator.addArtifact(pluginEntry.getKey().getLocation(),artifactId);
  }
 catch (  IOException e) {
    Throwables.propagate(e);
  }
  return new Plugin(artifactId,pluginEntry.getValue(),properties);
}"
6484,"/** 
 * Returns a   {@link SortedMap} of plugin artifact to plugin available for the given artifact. The keysare sorted by the  {@link ArtifactDescriptor} for the artifact that contains plugins available to the givenartifact.
 * @param artifactId the id of the artifact to get plugins for
 * @param pluginType the type of plugins to get
 * @param pluginName the name of plugins to get
 * @return an unmodifiable sorted map from plugin artifact to plugins in that artifact
 * @throws IOException if there was an exception reading plugin metadata from the artifact store
 */
public SortedMap<ArtifactDescriptor,PluginClass> getPlugins(Id.Artifact artifactId,String pluginType,String pluginName) throws IOException, PluginNotExistsException {
  return artifactStore.getPluginClasses(artifactId,pluginType,pluginName);
}","/** 
 * Returns a   {@link SortedMap} of plugin artifact to plugin available for the given artifact. The keysare sorted by the  {@link ArtifactDescriptor} for the artifact that contains plugins available to the givenartifact.
 * @param artifactId the id of the artifact to get plugins for
 * @param pluginType the type of plugins to get
 * @param pluginName the name of plugins to get
 * @return an unmodifiable sorted map from plugin artifact to plugins in that artifact
 * @throws ArtifactNotFoundException if the given artifact does not exist
 * @throws IOException if there was an exception reading plugin metadata from the artifact store
 */
public SortedMap<ArtifactDescriptor,PluginClass> getPlugins(Id.Artifact artifactId,String pluginType,String pluginName) throws IOException, PluginNotExistsException, ArtifactNotFoundException {
  return artifactStore.getPluginClasses(artifactId,pluginType,pluginName);
}"
6485,"/** 
 * Returns a   {@link Map.Entry} representing the plugin information for the plugin being requested.
 * @param artifactId the id of the artifact to get plugins for
 * @param pluginType plugin type name
 * @param pluginName plugin name
 * @param selector for selecting which plugin to use
 * @return the entry found
 * @throws IOException if there was an exception reading plugin metadata from the artifact store
 * @throws PluginNotExistsException if no plugins of the given type and name are available to the given artifact
 */
public Map.Entry<ArtifactDescriptor,PluginClass> findPlugin(Id.Artifact artifactId,String pluginType,String pluginName,PluginSelector selector) throws IOException, PluginNotExistsException {
  SortedMap<ArtifactDescriptor,PluginClass> pluginClasses=artifactStore.getPluginClasses(artifactId,pluginType,pluginName);
  SortedMap<ArtifactId,PluginClass> artifactIds=Maps.newTreeMap();
  for (  Map.Entry<ArtifactDescriptor,PluginClass> pluginClassEntry : pluginClasses.entrySet()) {
    artifactIds.put(pluginClassEntry.getKey().getArtifactId(),pluginClassEntry.getValue());
  }
  Map.Entry<ArtifactId,PluginClass> chosenArtifact=selector.select(artifactIds);
  if (chosenArtifact == null) {
    throw new PluginNotExistsException(artifactId,pluginType,pluginName);
  }
  for (  Map.Entry<ArtifactDescriptor,PluginClass> pluginClassEntry : pluginClasses.entrySet()) {
    if (pluginClassEntry.getKey().getArtifactId().compareTo(chosenArtifact.getKey()) == 0) {
      return pluginClassEntry;
    }
  }
  throw new PluginNotExistsException(artifactId,pluginType,pluginName);
}","/** 
 * Returns a   {@link Map.Entry} representing the plugin information for the plugin being requested.
 * @param artifactId the id of the artifact to get plugins for
 * @param pluginType plugin type name
 * @param pluginName plugin name
 * @param selector for selecting which plugin to use
 * @return the entry found
 * @throws IOException if there was an exception reading plugin metadata from the artifact store
 * @throws ArtifactNotFoundException if the given artifact does not exist
 * @throws PluginNotExistsException if no plugins of the given type and name are available to the given artifact
 */
public Map.Entry<ArtifactDescriptor,PluginClass> findPlugin(Id.Artifact artifactId,String pluginType,String pluginName,PluginSelector selector) throws IOException, PluginNotExistsException, ArtifactNotFoundException {
  SortedMap<ArtifactDescriptor,PluginClass> pluginClasses=artifactStore.getPluginClasses(artifactId,pluginType,pluginName);
  SortedMap<ArtifactId,PluginClass> artifactIds=Maps.newTreeMap();
  for (  Map.Entry<ArtifactDescriptor,PluginClass> pluginClassEntry : pluginClasses.entrySet()) {
    artifactIds.put(pluginClassEntry.getKey().getArtifactId(),pluginClassEntry.getValue());
  }
  Map.Entry<ArtifactId,PluginClass> chosenArtifact=selector.select(artifactIds);
  if (chosenArtifact == null) {
    throw new PluginNotExistsException(artifactId,pluginType,pluginName);
  }
  for (  Map.Entry<ArtifactDescriptor,PluginClass> pluginClassEntry : pluginClasses.entrySet()) {
    if (pluginClassEntry.getKey().getArtifactId().compareTo(chosenArtifact.getKey()) == 0) {
      return pluginClassEntry;
    }
  }
  throw new PluginNotExistsException(artifactId,pluginType,pluginName);
}"
6486,"/** 
 * Get all plugin classes of the given type and name that extend the given parent artifact. Results are returned as a map from plugin artifact to plugins in that artifact.
 * @param parentArtifactId the id of the artifact to find plugins for
 * @param type the type of plugin to look for
 * @param name the name of the plugin to look for
 * @return an unmodifiable map of plugin artifact to plugin classes of the given type and name, accessible by thegiven artifact. The map will never be null, and will never be empty.
 * @throws PluginNotExistsException if no plugin with the given type and name exists in the namespace
 * @throws IOException if there was an exception reading metadata from the metastore
 */
public SortedMap<ArtifactDescriptor,PluginClass> getPluginClasses(final Id.Artifact parentArtifactId,final String type,final String name) throws IOException, PluginNotExistsException {
  SortedMap<ArtifactDescriptor,PluginClass> plugins=metaTable.executeUnchecked(new TransactionExecutor.Function<DatasetContext<Table>,SortedMap<ArtifactDescriptor,PluginClass>>(){
    @Override public SortedMap<ArtifactDescriptor,PluginClass> apply(    DatasetContext<Table> context) throws Exception {
      SortedMap<ArtifactDescriptor,PluginClass> result=Maps.newTreeMap();
      PluginKey pluginKey=new PluginKey(parentArtifactId.getNamespace(),parentArtifactId.getName(),type,name);
      Row row=context.get().get(pluginKey.getRowKey());
      if (!row.isEmpty()) {
        for (        Map.Entry<byte[],byte[]> column : row.getColumns().entrySet()) {
          ArtifactColumn artifactColumn=ArtifactColumn.parse(column.getKey());
          PluginData pluginData=gson.fromJson(Bytes.toString(column.getValue()),PluginData.class);
          if (pluginData.usableBy.versionIsInRange(parentArtifactId.getVersion())) {
            ArtifactDescriptor artifactInfo=new ArtifactDescriptor(artifactColumn.artifactId.toArtifactId(),locationFactory.create(pluginData.artifactLocationURI));
            result.put(artifactInfo,pluginData.pluginClass);
          }
        }
      }
      return result;
    }
  }
);
  if (plugins.isEmpty()) {
    throw new PluginNotExistsException(parentArtifactId.getNamespace(),type,name);
  }
  return Collections.unmodifiableSortedMap(plugins);
}","/** 
 * Get all plugin classes of the given type and name that extend the given parent artifact. Results are returned as a map from plugin artifact to plugins in that artifact.
 * @param parentArtifactId the id of the artifact to find plugins for
 * @param type the type of plugin to look for
 * @param name the name of the plugin to look for
 * @return an unmodifiable map of plugin artifact to plugin classes of the given type and name, accessible by thegiven artifact. The map will never be null, and will never be empty.
 * @throws PluginNotExistsException if no plugin with the given type and name exists in the namespace
 * @throws IOException if there was an exception reading metadata from the metastore
 */
public SortedMap<ArtifactDescriptor,PluginClass> getPluginClasses(final Id.Artifact parentArtifactId,final String type,final String name) throws IOException, ArtifactNotFoundException, PluginNotExistsException {
  SortedMap<ArtifactDescriptor,PluginClass> plugins=metaTable.executeUnchecked(new TransactionExecutor.Function<DatasetContext<Table>,SortedMap<ArtifactDescriptor,PluginClass>>(){
    @Override public SortedMap<ArtifactDescriptor,PluginClass> apply(    DatasetContext<Table> context) throws Exception {
      Table table=context.get();
      SortedMap<ArtifactDescriptor,PluginClass> result=new TreeMap<>();
      ArtifactCell parentCell=new ArtifactCell(parentArtifactId);
      byte[] parentDataBytes=table.get(parentCell.rowkey,parentCell.column);
      if (parentDataBytes == null) {
        return null;
      }
      ArtifactData parentData=gson.fromJson(Bytes.toString(parentDataBytes),ArtifactData.class);
      Set<PluginClass> parentPlugins=parentData.meta.getClasses().getPlugins();
      for (      PluginClass pluginClass : parentPlugins) {
        if (pluginClass.getName().equals(name) && pluginClass.getType().equals(type)) {
          ArtifactDescriptor parentDescriptor=new ArtifactDescriptor(parentArtifactId.toArtifactId(),locationFactory.create(parentData.locationURI));
          result.put(parentDescriptor,pluginClass);
          break;
        }
      }
      PluginKey pluginKey=new PluginKey(parentArtifactId.getNamespace(),parentArtifactId.getName(),type,name);
      Row row=context.get().get(pluginKey.getRowKey());
      if (!row.isEmpty()) {
        for (        Map.Entry<byte[],byte[]> column : row.getColumns().entrySet()) {
          ArtifactColumn artifactColumn=ArtifactColumn.parse(column.getKey());
          PluginData pluginData=gson.fromJson(Bytes.toString(column.getValue()),PluginData.class);
          if (pluginData.usableBy.versionIsInRange(parentArtifactId.getVersion())) {
            ArtifactDescriptor artifactInfo=new ArtifactDescriptor(artifactColumn.artifactId.toArtifactId(),locationFactory.create(pluginData.artifactLocationURI));
            result.put(artifactInfo,pluginData.pluginClass);
          }
        }
      }
      return result;
    }
  }
);
  if (plugins == null) {
    throw new ArtifactNotFoundException(parentArtifactId);
  }
  if (plugins.isEmpty()) {
    throw new PluginNotExistsException(parentArtifactId.getNamespace(),type,name);
  }
  return Collections.unmodifiableSortedMap(plugins);
}"
6487,"@Override public void run(){
  runThread=Thread.currentThread();
  LOG.info(""String_Node_Str"",name);
  SettableFuture<String> completionFuture=SettableFuture.create();
  for (  Service service : services) {
    service.addListener(createServiceListener(service.getClass().getName(),completionFuture),Threads.SAME_THREAD_EXECUTOR);
  }
  Services.chainStart(services.get(0),services.subList(1,services.size()).toArray(new Service[0]));
  LOG.info(""String_Node_Str"",name);
  try {
    completionFuture.get();
  }
 catch (  InterruptedException e) {
    LOG.debug(""String_Node_Str"",name);
    Thread.currentThread().interrupt();
  }
catch (  ExecutionException e) {
    throw Throwables.propagate(e.getCause());
  }
  List<Service> reverse=Lists.reverse(services);
  Services.chainStop(reverse.get(0),reverse.subList(1,reverse.size()).toArray(new Service[0]));
  LOG.info(""String_Node_Str"",name);
}","@Override public void run(){
  runThread=Thread.currentThread();
  LOG.info(""String_Node_Str"",name);
  SettableFuture<String> completionFuture=SettableFuture.create();
  for (  Service service : services) {
    service.addListener(createServiceListener(service.getClass().getName(),completionFuture),Threads.SAME_THREAD_EXECUTOR);
  }
  Futures.getUnchecked(Services.chainStart(services.get(0),services.subList(1,services.size()).toArray(new Service[0])));
  LOG.info(""String_Node_Str"",name);
  try {
    completionFuture.get();
  }
 catch (  InterruptedException e) {
    LOG.debug(""String_Node_Str"",name);
    Thread.currentThread().interrupt();
  }
catch (  ExecutionException e) {
    throw Throwables.propagate(e.getCause());
  }
  List<Service> reverse=Lists.reverse(services);
  Futures.getUnchecked(Services.chainStop(reverse.get(0),reverse.subList(1,reverse.size()).toArray(new Service[0])));
  LOG.info(""String_Node_Str"",name);
}"
6488,"List<BusinessMetadataRecord> executeSearchOnColumns(String namespaceId,String column,String searchValue,MetadataSearchTargetType type){
  List<BusinessMetadataRecord> results=new LinkedList<>();
  Scanner scanner;
  String namespacedSearchValue=namespaceId + KEYVALUE_SEPARATOR + searchValue.toLowerCase();
  if (namespacedSearchValue.endsWith(""String_Node_Str"")) {
    byte[] startKey=Bytes.toBytes(namespacedSearchValue.substring(0,namespacedSearchValue.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    scanner=indexedTable.scanByIndex(Bytes.toBytes(column),startKey,stopKey);
  }
 else {
    byte[] value=Bytes.toBytes(namespacedSearchValue);
    scanner=indexedTable.readByIndex(Bytes.toBytes(column),value);
  }
  try {
    Row next;
    while ((next=scanner.next()) != null) {
      String rowValue=next.getString(VALUE_COLUMN);
      if (rowValue == null) {
        continue;
      }
      final byte[] rowKey=next.getRow();
      String targetType=MdsValueKey.getTargetType(rowKey);
      if ((type != MetadataSearchTargetType.ALL) && (!targetType.equals(type.getInternalName()))) {
        continue;
      }
      Id.NamespacedId targetId=MdsValueKey.getNamespaceIdFromKey(targetType,new MDSKey(rowKey));
      String key=MdsValueKey.getMetadataKey(targetType,rowKey);
      String value=Bytes.toString(next.get(Bytes.toBytes(BusinessMetadataDataset.VALUE_COLUMN)));
      BusinessMetadataRecord record=new BusinessMetadataRecord(targetId,key,value);
      results.add(record);
    }
  }
  finally {
    scanner.close();
  }
  return results;
}","List<BusinessMetadataRecord> executeSearchOnColumns(final String namespaceId,final String column,final String searchValue,final MetadataSearchTargetType type){
  List<BusinessMetadataRecord> results=new LinkedList<>();
  Scanner scanner;
  String namespacedSearchValue=namespaceId + KEYVALUE_SEPARATOR + searchValue.toLowerCase();
  boolean modifiedTagsKVSearch=false;
  final String unmodifiedTagKeyValue=searchValue.toLowerCase();
  if (KEYVALUE_COLUMN.equals(column) && unmodifiedTagKeyValue.startsWith(TAGS_KEY + KEYVALUE_SEPARATOR)) {
    namespacedSearchValue=namespacedSearchValue.substring(0,namespacedSearchValue.lastIndexOf(KEYVALUE_SEPARATOR) + 1) + ""String_Node_Str"";
    modifiedTagsKVSearch=true;
  }
  if (namespacedSearchValue.endsWith(""String_Node_Str"")) {
    byte[] startKey=Bytes.toBytes(namespacedSearchValue.substring(0,namespacedSearchValue.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    scanner=indexedTable.scanByIndex(Bytes.toBytes(column),startKey,stopKey);
  }
 else {
    byte[] value=Bytes.toBytes(namespacedSearchValue);
    scanner=indexedTable.readByIndex(Bytes.toBytes(column),value);
  }
  try {
    Row next;
    while ((next=scanner.next()) != null) {
      String rowValue=next.getString(VALUE_COLUMN);
      if (rowValue == null) {
        continue;
      }
      final byte[] rowKey=next.getRow();
      String targetType=MdsValueKey.getTargetType(rowKey);
      if ((type != MetadataSearchTargetType.ALL) && (!targetType.equals(type.getInternalName()))) {
        continue;
      }
      if (modifiedTagsKVSearch) {
        boolean isMatch=false;
        if ((TAGS_KEY + KEYVALUE_SEPARATOR + ""String_Node_Str"").equals(unmodifiedTagKeyValue)) {
          isMatch=true;
        }
 else {
          Iterable<String> tagValues=Splitter.on(TAGS_SEPARATOR).omitEmptyStrings().trimResults().split(rowValue);
          for (          String tagValue : tagValues) {
            String prefixedTagValue=TAGS_KEY + KEYVALUE_SEPARATOR + tagValue;
            if (!unmodifiedTagKeyValue.endsWith(""String_Node_Str"")) {
              if (prefixedTagValue.equals(unmodifiedTagKeyValue)) {
                isMatch=true;
                break;
              }
            }
 else {
              int endAsteriskIndex=unmodifiedTagKeyValue.lastIndexOf(""String_Node_Str"");
              if (prefixedTagValue.startsWith(unmodifiedTagKeyValue.substring(0,endAsteriskIndex))) {
                isMatch=true;
                break;
              }
            }
          }
        }
        if (!isMatch) {
          continue;
        }
      }
      Id.NamespacedId targetId=MdsValueKey.getNamespaceIdFromKey(targetType,new MDSKey(rowKey));
      String key=MdsValueKey.getMetadataKey(targetType,rowKey);
      String value=Bytes.toString(next.get(Bytes.toBytes(BusinessMetadataDataset.VALUE_COLUMN)));
      BusinessMetadataRecord record=new BusinessMetadataRecord(targetId,key,value);
      results.add(record);
    }
  }
  finally {
    scanner.close();
  }
  return results;
}"
6489,"@Override public void run(){
  runThread=Thread.currentThread();
  LOG.info(""String_Node_Str"",name);
  SettableFuture<String> completionFuture=SettableFuture.create();
  for (  Service service : services) {
    service.addListener(createServiceListener(service.getClass().getName(),completionFuture),Threads.SAME_THREAD_EXECUTOR);
  }
  Services.chainStart(services.get(0),services.subList(1,services.size()).toArray(new Service[0]));
  LOG.info(""String_Node_Str"",name);
  try {
    completionFuture.get();
  }
 catch (  InterruptedException e) {
    LOG.debug(""String_Node_Str"",name);
    Thread.currentThread().interrupt();
  }
catch (  ExecutionException e) {
    throw Throwables.propagate(e.getCause());
  }
  List<Service> reverse=Lists.reverse(services);
  Services.chainStop(reverse.get(0),reverse.subList(1,reverse.size()).toArray(new Service[0]));
  LOG.info(""String_Node_Str"",name);
}","@Override public void run(){
  runThread=Thread.currentThread();
  LOG.info(""String_Node_Str"",name);
  SettableFuture<String> completionFuture=SettableFuture.create();
  for (  Service service : services) {
    service.addListener(createServiceListener(service.getClass().getName(),completionFuture),Threads.SAME_THREAD_EXECUTOR);
  }
  Futures.getUnchecked(Services.chainStart(services.get(0),services.subList(1,services.size()).toArray(new Service[0])));
  LOG.info(""String_Node_Str"",name);
  try {
    completionFuture.get();
  }
 catch (  InterruptedException e) {
    LOG.debug(""String_Node_Str"",name);
    Thread.currentThread().interrupt();
  }
catch (  ExecutionException e) {
    throw Throwables.propagate(e.getCause());
  }
  List<Service> reverse=Lists.reverse(services);
  Futures.getUnchecked(Services.chainStop(reverse.get(0),reverse.subList(1,reverse.size()).toArray(new Service[0])));
  LOG.info(""String_Node_Str"",name);
}"
6490,"@Inject public LocalApplicationManager(CConfiguration configuration,PipelineFactory pipelineFactory,NamespacedLocationFactory namespacedLocationFactory,Store store,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,DatasetFramework datasetFramework,@Named(""String_Node_Str"") DatasetFramework inMemoryDatasetFramework,StreamAdmin streamAdmin,Scheduler scheduler,AdapterService adapterService,@Assisted ProgramTerminator programTerminator,MetricStore metricStore,UsageRegistry usageRegistry,ArtifactRepository artifactRepository,BusinessMetadataStore businessMetadataStore){
  this.configuration=configuration;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.pipelineFactory=pipelineFactory;
  this.store=store;
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.programTerminator=programTerminator;
  this.datasetFramework=datasetFramework;
  this.inMemoryDatasetFramework=inMemoryDatasetFramework;
  this.streamAdmin=streamAdmin;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
  this.adapterService=adapterService;
  this.usageRegistry=usageRegistry;
  this.artifactRepository=artifactRepository;
  this.businessMetadataStore=businessMetadataStore;
}","@Inject public LocalApplicationManager(CConfiguration configuration,PipelineFactory pipelineFactory,NamespacedLocationFactory namespacedLocationFactory,Store store,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,DatasetFramework datasetFramework,@Named(""String_Node_Str"") DatasetFramework inMemoryDatasetFramework,StreamAdmin streamAdmin,Scheduler scheduler,@Assisted ProgramTerminator programTerminator,MetricStore metricStore,UsageRegistry usageRegistry,ArtifactRepository artifactRepository,BusinessMetadataStore businessMetadataStore){
  this.configuration=configuration;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.pipelineFactory=pipelineFactory;
  this.store=store;
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.programTerminator=programTerminator;
  this.datasetFramework=datasetFramework;
  this.inMemoryDatasetFramework=inMemoryDatasetFramework;
  this.streamAdmin=streamAdmin;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
  this.usageRegistry=usageRegistry;
  this.artifactRepository=artifactRepository;
  this.businessMetadataStore=businessMetadataStore;
}"
6491,"@Override public ListenableFuture<O> deploy(Id.Namespace namespace,@Nullable String appId,I input) throws Exception {
  Pipeline<O> pipeline=pipelineFactory.getPipeline();
  pipeline.addLast(new LocalArtifactLoaderStage(configuration,store,namespace,appId,artifactRepository));
  pipeline.addLast(new ApplicationVerificationStage(store,datasetFramework,adapterService));
  pipeline.addLast(new DeployDatasetModulesStage(configuration,namespace,datasetFramework,inMemoryDatasetFramework));
  pipeline.addLast(new CreateDatasetInstancesStage(configuration,datasetFramework,namespace));
  pipeline.addLast(new CreateStreamsStage(namespace,streamAdmin));
  pipeline.addLast(new DeletedProgramHandlerStage(store,programTerminator,streamConsumerFactory,queueAdmin,metricStore,businessMetadataStore));
  pipeline.addLast(new ProgramGenerationStage(configuration,namespacedLocationFactory));
  pipeline.addLast(new ApplicationRegistrationStage(store,usageRegistry));
  pipeline.addLast(new CreateSchedulesStage(scheduler));
  return pipeline.execute(input);
}","@Override public ListenableFuture<O> deploy(Id.Namespace namespace,@Nullable String appId,I input) throws Exception {
  Pipeline<O> pipeline=pipelineFactory.getPipeline();
  pipeline.addLast(new LocalArtifactLoaderStage(configuration,store,namespace,appId,artifactRepository));
  pipeline.addLast(new ApplicationVerificationStage(store,datasetFramework));
  pipeline.addLast(new DeployDatasetModulesStage(configuration,namespace,datasetFramework,inMemoryDatasetFramework));
  pipeline.addLast(new CreateDatasetInstancesStage(configuration,datasetFramework,namespace));
  pipeline.addLast(new CreateStreamsStage(namespace,streamAdmin));
  pipeline.addLast(new DeletedProgramHandlerStage(store,programTerminator,streamConsumerFactory,queueAdmin,metricStore,businessMetadataStore));
  pipeline.addLast(new ProgramGenerationStage(configuration,namespacedLocationFactory));
  pipeline.addLast(new ApplicationRegistrationStage(store,usageRegistry));
  pipeline.addLast(new CreateSchedulesStage(scheduler));
  return pipeline.execute(input);
}"
6492,"public ApplicationVerificationStage(Store store,DatasetFramework dsFramework,AdapterService adapterService){
  super(TypeToken.of(ApplicationDeployable.class));
  this.store=store;
  this.dsFramework=dsFramework;
  this.adapterService=adapterService;
}","public ApplicationVerificationStage(Store store,DatasetFramework dsFramework){
  super(TypeToken.of(ApplicationDeployable.class));
  this.store=store;
  this.dsFramework=dsFramework;
}"
6493,"@Override public void configure(){
  setMainClass(mainSparkClass);
}","@Override public void configure(){
  setMainClass(ScalaFileCountProgram.class);
}"
6494,"/** 
 * Since calling one of the send methods multiple times logs a warning, upon transaction failures this method is called to allow setting the failure response without an additional warning.
 */
public void setTransactionFailureResponse(Throwable t){
  Throwable rootCause=Throwables.getRootCause(t);
  ByteBuffer buffer=Charsets.UTF_8.encode(""String_Node_Str"" + Throwables.getStackTraceAsString(rootCause));
  bufferedResponse=new BufferedResponse(HttpResponseStatus.INTERNAL_SERVER_ERROR.getCode(),ChannelBuffers.wrappedBuffer(buffer),""String_Node_Str"" + Charsets.UTF_8.name(),null);
}","/** 
 * Since calling one of the send methods multiple times logs a warning, upon transaction failures this method is called to allow setting the failure response without an additional warning.
 */
public void setTransactionFailureResponse(Throwable t){
  LOG.error(""String_Node_Str"",t);
  @SuppressWarnings(""String_Node_Str"") ByteBuffer buffer=Charsets.UTF_8.encode(""String_Node_Str"" + Throwables.getRootCause(t).getMessage());
  bufferedResponse=new BufferedResponse(HttpResponseStatus.INTERNAL_SERVER_ERROR.getCode(),ChannelBuffers.wrappedBuffer(buffer),""String_Node_Str"" + Charsets.UTF_8.name(),null);
}"
6495,"@GET @Path(""String_Node_Str"") public void streamLineage(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream,@QueryParam(""String_Node_Str"") String startStr,@QueryParam(""String_Node_Str"") String endStr,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int levels) throws Exception {
  checkLevels(levels);
  TimeRange range=parseRange(startStr,endStr);
  Id.Stream streamId=Id.Stream.from(namespaceId,stream);
  Lineage lineage=lineageAdmin.computeLineage(streamId,range.getStart(),range.getEnd(),levels);
  responder.sendJson(HttpResponseStatus.OK,new LineageRecord(TimeUnit.MILLISECONDS.toSeconds(range.getStart()),TimeUnit.MILLISECONDS.toSeconds(range.getEnd()),lineage.getRelations()),LineageRecord.class,GSON);
}","@GET @Path(""String_Node_Str"") public void streamLineage(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream,@QueryParam(""String_Node_Str"") String startStr,@QueryParam(""String_Node_Str"") String endStr,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int levels) throws Exception {
  checkLevels(levels);
  TimeRange range=parseRange(startStr,endStr);
  Id.Stream streamId=Id.Stream.from(namespaceId,stream);
  Lineage lineage=lineageAdmin.computeLineage(streamId,range.getStart(),range.getEnd(),levels);
  responder.sendJson(HttpResponseStatus.OK,LineageSerializer.toLineageRecord(TimeUnit.MILLISECONDS.toSeconds(range.getStart()),TimeUnit.MILLISECONDS.toSeconds(range.getEnd()),lineage),LineageRecord.class,GSON);
}"
6496,"@GET @Path(""String_Node_Str"") public void datasetLineage(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String datasetId,@QueryParam(""String_Node_Str"") String startStr,@QueryParam(""String_Node_Str"") String endStr,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int levels) throws Exception {
  checkLevels(levels);
  TimeRange range=parseRange(startStr,endStr);
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(namespaceId,datasetId);
  Lineage lineage=lineageAdmin.computeLineage(datasetInstance,range.getStart(),range.getEnd(),levels);
  responder.sendJson(HttpResponseStatus.OK,new LineageRecord(TimeUnit.MILLISECONDS.toSeconds(range.getStart()),TimeUnit.MILLISECONDS.toSeconds(range.getEnd()),lineage.getRelations()),LineageRecord.class,GSON);
}","@GET @Path(""String_Node_Str"") public void datasetLineage(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String datasetId,@QueryParam(""String_Node_Str"") String startStr,@QueryParam(""String_Node_Str"") String endStr,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int levels) throws Exception {
  checkLevels(levels);
  TimeRange range=parseRange(startStr,endStr);
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(namespaceId,datasetId);
  Lineage lineage=lineageAdmin.computeLineage(datasetInstance,range.getStart(),range.getEnd(),levels);
  responder.sendJson(HttpResponseStatus.OK,LineageSerializer.toLineageRecord(TimeUnit.MILLISECONDS.toSeconds(range.getStart()),TimeUnit.MILLISECONDS.toSeconds(range.getEnd()),lineage),LineageRecord.class,GSON);
}"
6497,"@Test public void testFlowLineage() throws Exception {
  String namespace=""String_Node_Str"";
  Id.Application app=Id.Application.from(namespace,AllProgramsApp.NAME);
  Id.Flow flow=Id.Flow.from(app,AllProgramsApp.NoOpFlow.NAME);
  Id.DatasetInstance dataset=Id.DatasetInstance.from(namespace,AllProgramsApp.DATASET_NAME);
  Id.Stream stream=Id.Stream.from(namespace,AllProgramsApp.STREAM_NAME);
  Assert.assertEquals(200,status(createNamespace(namespace)));
  try {
    Assert.assertEquals(200,status(deploy(AllProgramsApp.class,Constants.Gateway.API_VERSION_3_TOKEN,namespace)));
    ImmutableMap<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(app,appProperties).getResponseCode());
    Assert.assertEquals(appProperties,getProperties(app));
    ImmutableSet<String> appTags=ImmutableSet.of(""String_Node_Str"");
    Assert.assertEquals(200,addTags(app,appTags).getResponseCode());
    Assert.assertEquals(appTags,getTags(app));
    ImmutableMap<String,String> flowProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(flow,flowProperties).getResponseCode());
    Assert.assertEquals(flowProperties,getProperties(flow));
    ImmutableSet<String> flowTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addTags(flow,flowTags).getResponseCode());
    Assert.assertEquals(flowTags,getTags(flow));
    ImmutableMap<String,String> dataProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(dataset,dataProperties).getResponseCode());
    Assert.assertEquals(dataProperties,getProperties(dataset));
    ImmutableSet<String> dataTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addTags(dataset,dataTags).getResponseCode());
    Assert.assertEquals(dataTags,getTags(dataset));
    ImmutableMap<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(stream,streamProperties).getResponseCode());
    Assert.assertEquals(streamProperties,getProperties(stream));
    ImmutableSet<String> streamTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addTags(stream,streamTags).getResponseCode());
    Assert.assertEquals(streamTags,getTags(stream));
    long startTime=TimeMathParser.nowInSeconds();
    RunId flowRunId=runAndWait(flow);
    TimeUnit.SECONDS.sleep(2);
    waitForStop(flow,true);
    long stopTime=TimeMathParser.nowInSeconds();
    HttpResponse httpResponse=fetchLineage(dataset,startTime,stopTime,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    LineageRecord lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    LineageRecord expected=new LineageRecord(startTime,stopTime,ImmutableSet.of(new Relation(dataset,flow,AccessType.UNKNOWN,flowRunId,ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME))),new Relation(stream,flow,AccessType.READ,flowRunId,ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME)))));
    Assert.assertEquals(expected,lineage);
    httpResponse=fetchLineage(dataset,""String_Node_Str"",""String_Node_Str"",10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(expected.getRelations(),lineage.getRelations());
    httpResponse=fetchLineage(stream,startTime,stopTime,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(expected,lineage);
    httpResponse=fetchLineage(stream,""String_Node_Str"",""String_Node_Str"",10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(expected.getRelations(),lineage.getRelations());
    Id.Program programForFlow=Id.Program.from(flow.getApplication(),flow.getType(),flow.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,appProperties,appTags),new MetadataRecord(programForFlow,flowProperties,flowTags),new MetadataRecord(dataset,dataProperties,dataTags),new MetadataRecord(stream,streamProperties,streamTags)),fetchRunMetadata(new Id.Run(flow,flowRunId.getId())));
    long laterStartTime=stopTime + 1000;
    long laterEndTime=stopTime + 5000;
    httpResponse=fetchLineage(stream,laterStartTime,laterEndTime,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(new LineageRecord(laterStartTime,laterEndTime,ImmutableSet.<Relation>of()),lineage);
    long earlierStartTime=startTime - 5000;
    long earlierEndTime=startTime - 1000;
    httpResponse=fetchLineage(stream,earlierStartTime,earlierEndTime,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(new LineageRecord(earlierStartTime,earlierEndTime,ImmutableSet.<Relation>of()),lineage);
    httpResponse=fetchLineage(dataset,""String_Node_Str"",""String_Node_Str"",10);
    Assert.assertEquals(400,httpResponse.getResponseCode());
    httpResponse=fetchLineage(dataset,""String_Node_Str"",""String_Node_Str"",10);
    Assert.assertEquals(400,httpResponse.getResponseCode());
    httpResponse=fetchRunMetadataResponse(new Id.Run(flow,RunIds.generate(1000).getId()));
    Assert.assertEquals(404,httpResponse.getResponseCode());
  }
  finally {
    try {
      deleteNamespace(namespace);
    }
 catch (    Throwable e) {
      LOG.error(""String_Node_Str"",namespace,e);
    }
  }
}","@Test public void testFlowLineage() throws Exception {
  String namespace=""String_Node_Str"";
  Id.Application app=Id.Application.from(namespace,AllProgramsApp.NAME);
  Id.Flow flow=Id.Flow.from(app,AllProgramsApp.NoOpFlow.NAME);
  Id.DatasetInstance dataset=Id.DatasetInstance.from(namespace,AllProgramsApp.DATASET_NAME);
  Id.Stream stream=Id.Stream.from(namespace,AllProgramsApp.STREAM_NAME);
  Assert.assertEquals(200,status(createNamespace(namespace)));
  try {
    Assert.assertEquals(200,status(deploy(AllProgramsApp.class,Constants.Gateway.API_VERSION_3_TOKEN,namespace)));
    ImmutableMap<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(app,appProperties).getResponseCode());
    Assert.assertEquals(appProperties,getProperties(app));
    ImmutableSet<String> appTags=ImmutableSet.of(""String_Node_Str"");
    Assert.assertEquals(200,addTags(app,appTags).getResponseCode());
    Assert.assertEquals(appTags,getTags(app));
    ImmutableMap<String,String> flowProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(flow,flowProperties).getResponseCode());
    Assert.assertEquals(flowProperties,getProperties(flow));
    ImmutableSet<String> flowTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addTags(flow,flowTags).getResponseCode());
    Assert.assertEquals(flowTags,getTags(flow));
    ImmutableMap<String,String> dataProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(dataset,dataProperties).getResponseCode());
    Assert.assertEquals(dataProperties,getProperties(dataset));
    ImmutableSet<String> dataTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addTags(dataset,dataTags).getResponseCode());
    Assert.assertEquals(dataTags,getTags(dataset));
    ImmutableMap<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(stream,streamProperties).getResponseCode());
    Assert.assertEquals(streamProperties,getProperties(stream));
    ImmutableSet<String> streamTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addTags(stream,streamTags).getResponseCode());
    Assert.assertEquals(streamTags,getTags(stream));
    long startTime=TimeMathParser.nowInSeconds();
    RunId flowRunId=runAndWait(flow);
    TimeUnit.SECONDS.sleep(2);
    waitForStop(flow,true);
    long stopTime=TimeMathParser.nowInSeconds();
    HttpResponse httpResponse=fetchLineage(dataset,startTime,stopTime,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    LineageRecord lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    LineageRecord expected=LineageSerializer.toLineageRecord(startTime,stopTime,new Lineage(ImmutableSet.of(new Relation(dataset,flow,AccessType.UNKNOWN,flowRunId,ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME))),new Relation(stream,flow,AccessType.READ,flowRunId,ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME))))));
    Assert.assertEquals(expected,lineage);
    httpResponse=fetchLineage(dataset,""String_Node_Str"",""String_Node_Str"",10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(expected.getRelations(),lineage.getRelations());
    httpResponse=fetchLineage(stream,startTime,stopTime,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(expected,lineage);
    httpResponse=fetchLineage(stream,""String_Node_Str"",""String_Node_Str"",10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(expected.getRelations(),lineage.getRelations());
    Id.Program programForFlow=Id.Program.from(flow.getApplication(),flow.getType(),flow.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,appProperties,appTags),new MetadataRecord(programForFlow,flowProperties,flowTags),new MetadataRecord(dataset,dataProperties,dataTags),new MetadataRecord(stream,streamProperties,streamTags)),fetchRunMetadata(new Id.Run(flow,flowRunId.getId())));
    long laterStartTime=stopTime + 1000;
    long laterEndTime=stopTime + 5000;
    httpResponse=fetchLineage(stream,laterStartTime,laterEndTime,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(LineageSerializer.toLineageRecord(laterStartTime,laterEndTime,new Lineage(ImmutableSet.<Relation>of())),lineage);
    long earlierStartTime=startTime - 5000;
    long earlierEndTime=startTime - 1000;
    httpResponse=fetchLineage(stream,earlierStartTime,earlierEndTime,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(LineageSerializer.toLineageRecord(earlierStartTime,earlierEndTime,new Lineage(ImmutableSet.<Relation>of())),lineage);
    httpResponse=fetchLineage(dataset,""String_Node_Str"",""String_Node_Str"",10);
    Assert.assertEquals(400,httpResponse.getResponseCode());
    httpResponse=fetchLineage(dataset,""String_Node_Str"",""String_Node_Str"",10);
    Assert.assertEquals(400,httpResponse.getResponseCode());
    httpResponse=fetchRunMetadataResponse(new Id.Run(flow,RunIds.generate(1000).getId()));
    Assert.assertEquals(404,httpResponse.getResponseCode());
  }
  finally {
    try {
      deleteNamespace(namespace);
    }
 catch (    Throwable e) {
      LOG.error(""String_Node_Str"",namespace,e);
    }
  }
}"
6498,"@Test public void testAllProgramsLineage() throws Exception {
  String namespace=""String_Node_Str"";
  Id.Application app=Id.Application.from(namespace,AllProgramsApp.NAME);
  Id.Flow flow=Id.Flow.from(app,AllProgramsApp.NoOpFlow.NAME);
  Id.Program mapreduce=Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR.NAME);
  Id.Program spark=Id.Program.from(app,ProgramType.SPARK,AllProgramsApp.NoOpSpark.NAME);
  Id.Program service=Id.Program.from(app,ProgramType.SERVICE,AllProgramsApp.NoOpService.NAME);
  Id.Program worker=Id.Program.from(app,ProgramType.WORKER,AllProgramsApp.NoOpWorker.NAME);
  Id.Program workflow=Id.Program.from(app,ProgramType.WORKFLOW,AllProgramsApp.NoOpWorkflow.NAME);
  Id.DatasetInstance dataset=Id.DatasetInstance.from(namespace,AllProgramsApp.DATASET_NAME);
  Id.Stream stream=Id.Stream.from(namespace,AllProgramsApp.STREAM_NAME);
  Assert.assertEquals(200,status(createNamespace(namespace)));
  try {
    Assert.assertEquals(200,status(deploy(AllProgramsApp.class,Constants.Gateway.API_VERSION_3_TOKEN,namespace)));
    ImmutableSet<String> sparkTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    addTags(spark,sparkTags);
    Assert.assertEquals(sparkTags,getTags(spark));
    ImmutableSet<String> workerTags=ImmutableSet.of(""String_Node_Str"");
    addTags(worker,workerTags);
    Assert.assertEquals(workerTags,getTags(worker));
    ImmutableMap<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    addProperties(dataset,datasetProperties);
    Assert.assertEquals(datasetProperties,getProperties(dataset));
    RunId flowRunId=runAndWait(flow);
    RunId mrRunId=runAndWait(mapreduce);
    RunId sparkRunId=runAndWait(spark);
    runAndWait(workflow);
    RunId workflowMrRunId=getRunId(mapreduce,mrRunId);
    RunId serviceRunId=runAndWait(service);
    RunId workerRunId=runAndWait(worker);
    waitForStop(flow,true);
    waitForStop(mapreduce,false);
    waitForStop(spark,false);
    waitForStop(workflow,false);
    waitForStop(worker,false);
    waitForStop(service,true);
    long now=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
    long oneHour=TimeUnit.HOURS.toSeconds(1);
    HttpResponse httpResponse=fetchLineage(dataset,now - oneHour,now + oneHour,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    LineageRecord lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    LineageRecord expected=new LineageRecord(now - oneHour,now + oneHour,ImmutableSet.of(new Relation(dataset,flow,AccessType.UNKNOWN,flowRunId,ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME))),new Relation(dataset,mapreduce,AccessType.UNKNOWN,mrRunId),new Relation(dataset,spark,AccessType.UNKNOWN,sparkRunId),new Relation(dataset,mapreduce,AccessType.UNKNOWN,workflowMrRunId),new Relation(dataset,service,AccessType.UNKNOWN,serviceRunId),new Relation(dataset,worker,AccessType.UNKNOWN,workerRunId),new Relation(stream,flow,AccessType.READ,flowRunId,ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME))),new Relation(stream,mapreduce,AccessType.READ,mrRunId),new Relation(stream,spark,AccessType.READ,sparkRunId),new Relation(stream,mapreduce,AccessType.READ,workflowMrRunId),new Relation(stream,worker,AccessType.WRITE,workerRunId)));
    Assert.assertEquals(expected,lineage);
    httpResponse=fetchLineage(stream,now - oneHour,now + oneHour,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(expected,lineage);
    Id.Program programForFlow=Id.Program.from(flow.getApplication(),flow.getType(),flow.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,emptyMap(),emptySet()),new MetadataRecord(programForFlow,emptyMap(),emptySet()),new MetadataRecord(dataset,datasetProperties,emptySet()),new MetadataRecord(stream,emptyMap(),emptySet())),fetchRunMetadata(new Id.Run(flow,flowRunId.getId())));
    Id.Program programForWorker=Id.Program.from(worker.getApplication(),worker.getType(),worker.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,emptyMap(),emptySet()),new MetadataRecord(programForWorker,emptyMap(),workerTags),new MetadataRecord(dataset,datasetProperties,emptySet()),new MetadataRecord(stream,emptyMap(),emptySet())),fetchRunMetadata(new Id.Run(worker,workerRunId.getId())));
    Id.Program programForSpark=Id.Program.from(spark.getApplication(),spark.getType(),spark.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,emptyMap(),emptySet()),new MetadataRecord(programForSpark,emptyMap(),sparkTags),new MetadataRecord(dataset,datasetProperties,emptySet()),new MetadataRecord(stream,emptyMap(),emptySet())),fetchRunMetadata(new Id.Run(spark,sparkRunId.getId())));
  }
  finally {
    try {
      deleteNamespace(namespace);
    }
 catch (    Throwable e) {
      LOG.error(""String_Node_Str"",namespace,e);
    }
  }
}","@Test public void testAllProgramsLineage() throws Exception {
  String namespace=""String_Node_Str"";
  Id.Application app=Id.Application.from(namespace,AllProgramsApp.NAME);
  Id.Flow flow=Id.Flow.from(app,AllProgramsApp.NoOpFlow.NAME);
  Id.Program mapreduce=Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR.NAME);
  Id.Program spark=Id.Program.from(app,ProgramType.SPARK,AllProgramsApp.NoOpSpark.NAME);
  Id.Program service=Id.Program.from(app,ProgramType.SERVICE,AllProgramsApp.NoOpService.NAME);
  Id.Program worker=Id.Program.from(app,ProgramType.WORKER,AllProgramsApp.NoOpWorker.NAME);
  Id.Program workflow=Id.Program.from(app,ProgramType.WORKFLOW,AllProgramsApp.NoOpWorkflow.NAME);
  Id.DatasetInstance dataset=Id.DatasetInstance.from(namespace,AllProgramsApp.DATASET_NAME);
  Id.Stream stream=Id.Stream.from(namespace,AllProgramsApp.STREAM_NAME);
  Assert.assertEquals(200,status(createNamespace(namespace)));
  try {
    Assert.assertEquals(200,status(deploy(AllProgramsApp.class,Constants.Gateway.API_VERSION_3_TOKEN,namespace)));
    ImmutableSet<String> sparkTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    addTags(spark,sparkTags);
    Assert.assertEquals(sparkTags,getTags(spark));
    ImmutableSet<String> workerTags=ImmutableSet.of(""String_Node_Str"");
    addTags(worker,workerTags);
    Assert.assertEquals(workerTags,getTags(worker));
    ImmutableMap<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    addProperties(dataset,datasetProperties);
    Assert.assertEquals(datasetProperties,getProperties(dataset));
    RunId flowRunId=runAndWait(flow);
    RunId mrRunId=runAndWait(mapreduce);
    RunId sparkRunId=runAndWait(spark);
    runAndWait(workflow);
    RunId workflowMrRunId=getRunId(mapreduce,mrRunId);
    RunId serviceRunId=runAndWait(service);
    RunId workerRunId=runAndWait(worker);
    waitForStop(flow,true);
    waitForStop(mapreduce,false);
    waitForStop(spark,false);
    waitForStop(workflow,false);
    waitForStop(worker,false);
    waitForStop(service,true);
    long now=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
    long oneHour=TimeUnit.HOURS.toSeconds(1);
    HttpResponse httpResponse=fetchLineage(dataset,now - oneHour,now + oneHour,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    LineageRecord lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    LineageRecord expected=LineageSerializer.toLineageRecord(now - oneHour,now + oneHour,new Lineage(ImmutableSet.of(new Relation(dataset,flow,AccessType.UNKNOWN,flowRunId,ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME))),new Relation(dataset,mapreduce,AccessType.UNKNOWN,mrRunId),new Relation(dataset,spark,AccessType.UNKNOWN,sparkRunId),new Relation(dataset,mapreduce,AccessType.UNKNOWN,workflowMrRunId),new Relation(dataset,service,AccessType.UNKNOWN,serviceRunId),new Relation(dataset,worker,AccessType.UNKNOWN,workerRunId),new Relation(stream,flow,AccessType.READ,flowRunId,ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME))),new Relation(stream,mapreduce,AccessType.READ,mrRunId),new Relation(stream,spark,AccessType.READ,sparkRunId),new Relation(stream,mapreduce,AccessType.READ,workflowMrRunId),new Relation(stream,worker,AccessType.WRITE,workerRunId))));
    Assert.assertEquals(expected,lineage);
    httpResponse=fetchLineage(stream,now - oneHour,now + oneHour,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(expected,lineage);
    Id.Program programForFlow=Id.Program.from(flow.getApplication(),flow.getType(),flow.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,emptyMap(),emptySet()),new MetadataRecord(programForFlow,emptyMap(),emptySet()),new MetadataRecord(dataset,datasetProperties,emptySet()),new MetadataRecord(stream,emptyMap(),emptySet())),fetchRunMetadata(new Id.Run(flow,flowRunId.getId())));
    Id.Program programForWorker=Id.Program.from(worker.getApplication(),worker.getType(),worker.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,emptyMap(),emptySet()),new MetadataRecord(programForWorker,emptyMap(),workerTags),new MetadataRecord(dataset,datasetProperties,emptySet()),new MetadataRecord(stream,emptyMap(),emptySet())),fetchRunMetadata(new Id.Run(worker,workerRunId.getId())));
    Id.Program programForSpark=Id.Program.from(spark.getApplication(),spark.getType(),spark.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,emptyMap(),emptySet()),new MetadataRecord(programForSpark,emptyMap(),sparkTags),new MetadataRecord(dataset,datasetProperties,emptySet()),new MetadataRecord(stream,emptyMap(),emptySet())),fetchRunMetadata(new Id.Run(spark,sparkRunId.getId())));
  }
  finally {
    try {
      deleteNamespace(namespace);
    }
 catch (    Throwable e) {
      LOG.error(""String_Node_Str"",namespace,e);
    }
  }
}"
6499,"private void recordCompletedWorkflow(final Id.Workflow id,String pid){
  final RunRecordMeta run=getRun(id,pid);
  if (run == null) {
    return;
  }
  Id.Application app=id.getApplication();
  ApplicationSpecification appSpec=getApplication(app);
  if (appSpec == null || appSpec.getWorkflows() == null || appSpec.getWorkflows().get(id.getId()) == null) {
    return;
  }
  WorkflowSpecification workflowSpec=appSpec.getWorkflows().get(id.getId());
  Map<String,WorkflowNode> nodeIdMap=workflowSpec.getNodeIdMap();
  final List<WorkflowDataset.ProgramRun> programRunsList=new ArrayList<>();
  for (  Map.Entry<String,String> entry : run.getProperties().entrySet()) {
    if (!(""String_Node_Str"".equals(entry.getKey()) || ""String_Node_Str"".equals(entry.getKey()))) {
      WorkflowActionNode workflowNode=(WorkflowActionNode)nodeIdMap.get(entry.getKey());
      ProgramType programType=ProgramType.valueOfSchedulableType(workflowNode.getProgram().getProgramType());
      Id.Program innerProgram=Id.Program.from(app.getNamespaceId(),app.getId(),programType,entry.getKey());
      RunRecordMeta innerProgramRun=getRun(innerProgram,entry.getValue());
      programRunsList.add(new WorkflowDataset.ProgramRun(entry.getKey(),entry.getValue(),programType,innerProgramRun.getStopTs() - innerProgramRun.getStartTs()));
    }
  }
  txnlWorkflow.executeUnchecked(new TransactionExecutor.Function<WorkflowStatsDataset,Void>(){
    @Override public Void apply(    WorkflowStatsDataset dataset){
      dataset.workflowDataset.write(id,run,programRunsList);
      return null;
    }
  }
);
}","private void recordCompletedWorkflow(final Id.Workflow id,String pid){
  final RunRecordMeta run=getRun(id,pid);
  if (run == null) {
    return;
  }
  Id.Application app=id.getApplication();
  ApplicationSpecification appSpec=getApplication(app);
  if (appSpec == null || appSpec.getWorkflows() == null || appSpec.getWorkflows().get(id.getId()) == null) {
    return;
  }
  boolean workFlowNodeFailed=false;
  WorkflowSpecification workflowSpec=appSpec.getWorkflows().get(id.getId());
  Map<String,WorkflowNode> nodeIdMap=workflowSpec.getNodeIdMap();
  final List<WorkflowDataset.ProgramRun> programRunsList=new ArrayList<>();
  for (  Map.Entry<String,String> entry : run.getProperties().entrySet()) {
    if (!(""String_Node_Str"".equals(entry.getKey()) || ""String_Node_Str"".equals(entry.getKey()))) {
      WorkflowActionNode workflowNode=(WorkflowActionNode)nodeIdMap.get(entry.getKey());
      ProgramType programType=ProgramType.valueOfSchedulableType(workflowNode.getProgram().getProgramType());
      Id.Program innerProgram=Id.Program.from(app.getNamespaceId(),app.getId(),programType,entry.getKey());
      RunRecordMeta innerProgramRun=getRun(innerProgram,entry.getValue());
      if (innerProgramRun.getStatus().equals(ProgramRunStatus.COMPLETED)) {
        programRunsList.add(new WorkflowDataset.ProgramRun(entry.getKey(),entry.getValue(),programType,innerProgramRun.getStopTs() - innerProgramRun.getStartTs()));
      }
 else {
        workFlowNodeFailed=true;
        break;
      }
    }
  }
  if (workFlowNodeFailed) {
    return;
  }
  txnlWorkflow.executeUnchecked(new TransactionExecutor.Function<WorkflowStatsDataset,Void>(){
    @Override public Void apply(    WorkflowStatsDataset dataset){
      dataset.workflowDataset.write(id,run,programRunsList);
      return null;
    }
  }
);
}"
6500,"@Override public Set<MetadataSearchResultRecord> searchMetadata(String namespaceId,String searchQuery,@Nullable final MetadataSearchTargetType type) throws NotFoundException {
  Iterable<BusinessMetadataRecord> results;
  if (type == null) {
    results=businessMds.searchMetadata(namespaceId,searchQuery);
  }
 else {
    results=businessMds.searchMetadataOnType(namespaceId,searchQuery,type);
  }
  Set<MetadataSearchResultRecord> searchResultRecords=new LinkedHashSet<>();
  for (  BusinessMetadataRecord bmr : results) {
    MetadataSearchTargetType finalType=type;
    if (finalType == null || finalType == MetadataSearchTargetType.ALL) {
      Id.NamespacedId namespacedId=bmr.getTargetId();
      String targetType=getTargetType(namespacedId);
      finalType=getMetadataSearchTarget(targetType);
    }
    MetadataSearchResultRecord msr=new MetadataSearchResultRecord(bmr.getTargetId(),finalType);
    searchResultRecords.add(msr);
  }
  return searchResultRecords;
}","@Override public Set<MetadataSearchResultRecord> searchMetadata(String namespaceId,String searchQuery,@Nullable final MetadataSearchTargetType type) throws NotFoundException {
  Iterable<BusinessMetadataRecord> results;
  if (type == null) {
    results=businessMds.searchMetadata(namespaceId,searchQuery);
  }
 else {
    results=businessMds.searchMetadataOnType(namespaceId,searchQuery,type);
  }
  Set<MetadataSearchResultRecord> searchResultRecords=new LinkedHashSet<>();
  for (  BusinessMetadataRecord bmr : results) {
    MetadataSearchResultRecord msr=new MetadataSearchResultRecord(bmr.getTargetId());
    searchResultRecords.add(msr);
  }
  return searchResultRecords;
}"
6501,"private void addRelations(Set<Relation> lineageRelations){
  for (  Relation relation : lineageRelations) {
    String dataKey=makeDataKey(relation.getData());
    String programKey=makeProgramKey(relation.getProgram());
    RelationRecord relationRecord=new RelationRecord(dataKey,programKey,relation.getAccess().toString().toLowerCase(),convertRuns(relation.getRun()),convertComponents(relation.getComponents()));
    relations.add(relationRecord);
    programs.put(programKey,ImmutableMap.of(""String_Node_Str"",toProgramRecord(relation.getProgram())));
    data.put(dataKey,ImmutableMap.of(""String_Node_Str"",toDataRecord(relation.getData())));
  }
}","private void addRelations(Set<Relation> lineageRelations){
  for (  Relation relation : lineageRelations) {
    String dataKey=makeDataKey(relation.getData());
    String programKey=makeProgramKey(relation.getProgram());
    RelationRecord relationRecord=new RelationRecord(dataKey,programKey,relation.getAccess().toString().toLowerCase(),convertRuns(relation.getRun()),convertComponents(relation.getComponents()));
    relations.add(relationRecord);
    programs.put(programKey,ImmutableMap.of(""String_Node_Str"",relation.getProgram()));
    data.put(dataKey,ImmutableMap.of(""String_Node_Str"",relation.getData()));
  }
}"
6502,"@Test public void testSimpleLineage() throws Exception {
  LineageStore lineageStore=new LineageStore(getTxExecFactory(),getDatasetFramework(),Id.DatasetInstance.from(""String_Node_Str"",""String_Node_Str""));
  Store store=getInjector().getInstance(Store.class);
  BusinessMetadataStore businessMetadataStore=getInjector().getInstance(BusinessMetadataStore.class);
  LineageAdmin lineageAdmin=new LineageAdmin(lineageStore,store,businessMetadataStore,new NoOpEntityValidator());
  MetadataRecord run1AppMeta=new MetadataRecord(program1.getApplication(),toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str""));
  MetadataRecord run1ProgramMeta=new MetadataRecord(program1,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str""));
  MetadataRecord run1Data1Meta=new MetadataRecord(dataset1,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str""));
  MetadataRecord run1Data2Meta=new MetadataRecord(dataset2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str""));
  businessMetadataStore.setProperties(program1.getApplication(),run1AppMeta.getProperties());
  businessMetadataStore.addTags(program1.getApplication(),run1AppMeta.getTags().toArray(new String[0]));
  businessMetadataStore.setProperties(program1,run1ProgramMeta.getProperties());
  businessMetadataStore.addTags(program1,run1ProgramMeta.getTags().toArray(new String[0]));
  businessMetadataStore.setProperties(dataset1,run1Data1Meta.getProperties());
  businessMetadataStore.addTags(dataset1,run1Data1Meta.getTags().toArray(new String[0]));
  businessMetadataStore.setProperties(dataset2,run1Data2Meta.getProperties());
  businessMetadataStore.addTags(dataset2,run1Data2Meta.getTags().toArray(new String[0]));
  TimeUnit.MILLISECONDS.sleep(1);
  Id.Run run1=new Id.Run(program1,RunIds.generate(System.currentTimeMillis()).getId());
  Id.Run run2=new Id.Run(program2,RunIds.generate(System.currentTimeMillis()).getId());
  addRuns(store,run1,run2);
  lineageStore.addAccess(run1,dataset1,AccessType.WRITE,System.currentTimeMillis(),flowlet1);
  lineageStore.addAccess(run1,dataset2,AccessType.READ,System.currentTimeMillis(),flowlet1);
  lineageStore.addAccess(run2,dataset2,AccessType.WRITE,System.currentTimeMillis(),flowlet2);
  lineageStore.addAccess(run2,dataset3,AccessType.READ,System.currentTimeMillis(),flowlet2);
  Lineage expectedLineage=new Lineage(ImmutableSet.of(new Relation(dataset1,program1,AccessType.WRITE,twillRunId(run1),toSet(flowlet1)),new Relation(dataset2,program1,AccessType.READ,twillRunId(run1),toSet(flowlet1)),new Relation(dataset2,program2,AccessType.WRITE,twillRunId(run2),toSet(flowlet2)),new Relation(dataset3,program2,AccessType.READ,twillRunId(run2),toSet(flowlet2))));
  Assert.assertEquals(expectedLineage,lineageAdmin.computeLineage(dataset1,500,System.currentTimeMillis() + 10000,100));
  Assert.assertEquals(expectedLineage,lineageAdmin.computeLineage(dataset2,500,System.currentTimeMillis() + 10000,100));
  Lineage oneLevelLineage=lineageAdmin.computeLineage(dataset1,500,System.currentTimeMillis() + 10000,1);
  Assert.assertEquals(ImmutableSet.of(new Relation(dataset1,program1,AccessType.WRITE,twillRunId(run1),toSet(flowlet1)),new Relation(dataset2,program1,AccessType.READ,twillRunId(run1),toSet(flowlet1))),oneLevelLineage.getRelations());
  Assert.assertEquals(toSet(run1AppMeta,run1ProgramMeta,run1Data1Meta,run1Data2Meta),lineageAdmin.getMetadataForRun(run1));
}","@Test public void testSimpleLineage() throws Exception {
  LineageStore lineageStore=new LineageStore(getTxExecFactory(),getDatasetFramework(),Id.DatasetInstance.from(""String_Node_Str"",""String_Node_Str""));
  Store store=getInjector().getInstance(Store.class);
  BusinessMetadataStore businessMetadataStore=getInjector().getInstance(BusinessMetadataStore.class);
  LineageAdmin lineageAdmin=new LineageAdmin(lineageStore,store,businessMetadataStore,new NoOpEntityValidator());
  MetadataRecord run1AppMeta=new MetadataRecord(program1.getApplication(),toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str""));
  MetadataRecord run1ProgramMeta=new MetadataRecord(program1,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str""));
  MetadataRecord run1Data1Meta=new MetadataRecord(dataset1,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str""));
  MetadataRecord run1Data2Meta=new MetadataRecord(dataset2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str""));
  businessMetadataStore.setProperties(program1.getApplication(),run1AppMeta.getProperties());
  businessMetadataStore.addTags(program1.getApplication(),run1AppMeta.getTags().toArray(new String[0]));
  businessMetadataStore.setProperties(program1,run1ProgramMeta.getProperties());
  businessMetadataStore.addTags(program1,run1ProgramMeta.getTags().toArray(new String[0]));
  businessMetadataStore.setProperties(dataset1,run1Data1Meta.getProperties());
  businessMetadataStore.addTags(dataset1,run1Data1Meta.getTags().toArray(new String[0]));
  businessMetadataStore.setProperties(dataset2,run1Data2Meta.getProperties());
  businessMetadataStore.addTags(dataset2,run1Data2Meta.getTags().toArray(new String[0]));
  TimeUnit.MILLISECONDS.sleep(1);
  Id.Run run1=new Id.Run(program1,RunIds.generate(System.currentTimeMillis()).getId());
  Id.Run run2=new Id.Run(program2,RunIds.generate(System.currentTimeMillis()).getId());
  addRuns(store,run1,run2);
  lineageStore.addAccess(run1,dataset1,AccessType.WRITE,System.currentTimeMillis(),flowlet1);
  lineageStore.addAccess(run1,dataset2,AccessType.READ,System.currentTimeMillis(),flowlet1);
  lineageStore.addAccess(run2,dataset2,AccessType.WRITE,System.currentTimeMillis(),flowlet2);
  lineageStore.addAccess(run2,dataset3,AccessType.READ,System.currentTimeMillis(),flowlet2);
  Lineage expectedLineage=new Lineage(ImmutableSet.of(new Relation(dataset1,program1,AccessType.WRITE,twillRunId(run1),toSet(flowlet1)),new Relation(dataset2,program1,AccessType.READ,twillRunId(run1),toSet(flowlet1)),new Relation(dataset2,program2,AccessType.WRITE,twillRunId(run2),toSet(flowlet2)),new Relation(dataset3,program2,AccessType.READ,twillRunId(run2),toSet(flowlet2))));
  Assert.assertEquals(expectedLineage,lineageAdmin.computeLineage(dataset1,500,System.currentTimeMillis() + 10000,100));
  Assert.assertEquals(expectedLineage,lineageAdmin.computeLineage(dataset2,500,System.currentTimeMillis() + 10000,100));
  Lineage oneLevelLineage=lineageAdmin.computeLineage(dataset1,500,System.currentTimeMillis() + 10000,1);
  Assert.assertEquals(ImmutableSet.of(new Relation(dataset1,program1,AccessType.WRITE,twillRunId(run1),toSet(flowlet1)),new Relation(dataset2,program1,AccessType.READ,twillRunId(run1),toSet(flowlet1))),oneLevelLineage.getRelations());
  Assert.assertEquals(toSet(run1AppMeta,run1ProgramMeta,run1Data1Meta,run1Data2Meta),lineageAdmin.getMetadataForRun(run1));
  Id.Namespace customNamespace=Id.Namespace.from(""String_Node_Str"");
  Id.DatasetInstance customDataset1=Id.DatasetInstance.from(customNamespace,dataset1.getId());
  Id.Run customRun1=new Id.Run(Id.Program.from(customNamespace,program1.getApplicationId(),program1.getType(),program1.getId()),run1.getId());
  Assert.assertEquals(new Lineage(ImmutableSet.<Relation>of()),lineageAdmin.computeLineage(customDataset1,500,System.currentTimeMillis() + 10000,100));
  Assert.assertEquals(ImmutableSet.<MetadataRecord>of(),lineageAdmin.getMetadataForRun(customRun1));
}"
6503,"@Test public void testMetadata() throws IOException {
  assertCleanState();
  removeAllMetadata();
  assertCleanState();
  Map<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  Map<String,String> serviceProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  Map<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  Map<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  Set<String> appTags=ImmutableSet.of(""String_Node_Str"");
  Set<String> serviceTags=ImmutableSet.of(""String_Node_Str"");
  Set<String> datasetTags=ImmutableSet.of(""String_Node_Str"");
  Set<String> streamTags=ImmutableSet.of(""String_Node_Str"");
  Assert.assertEquals(200,addProperties(application,appProperties).getResponseCode());
  Assert.assertEquals(200,addProperties(pingService,serviceProperties).getResponseCode());
  Assert.assertEquals(200,addProperties(myds,datasetProperties).getResponseCode());
  Assert.assertEquals(200,addProperties(mystream,streamProperties).getResponseCode());
  Assert.assertEquals(200,addTags(application,appTags).getResponseCode());
  Assert.assertEquals(200,addTags(pingService,serviceTags).getResponseCode());
  Assert.assertEquals(200,addTags(myds,datasetTags).getResponseCode());
  Assert.assertEquals(200,addTags(mystream,streamTags).getResponseCode());
  Set<MetadataRecord> metadataRecords=getMetadata(application);
  Assert.assertEquals(1,metadataRecords.size());
  MetadataRecord metadata=metadataRecords.iterator().next();
  Assert.assertEquals(MetadataScope.USER,metadata.getScope());
  Assert.assertEquals(application,metadata.getTargetId());
  Assert.assertEquals(appProperties,metadata.getProperties());
  Assert.assertEquals(appTags,metadata.getTags());
  metadataRecords=getMetadata(pingService);
  Assert.assertEquals(1,metadataRecords.size());
  metadata=metadataRecords.iterator().next();
  Assert.assertEquals(MetadataScope.USER,metadata.getScope());
  Assert.assertEquals(pingService,metadata.getTargetId());
  Assert.assertEquals(serviceProperties,metadata.getProperties());
  Assert.assertEquals(serviceTags,metadata.getTags());
  metadataRecords=getMetadata(myds);
  Assert.assertEquals(1,metadataRecords.size());
  metadata=metadataRecords.iterator().next();
  Assert.assertEquals(MetadataScope.USER,metadata.getScope());
  Assert.assertEquals(myds,metadata.getTargetId());
  Assert.assertEquals(datasetProperties,metadata.getProperties());
  Assert.assertEquals(datasetTags,metadata.getTags());
  metadataRecords=getMetadata(mystream);
  Assert.assertEquals(1,metadataRecords.size());
  metadata=metadataRecords.iterator().next();
  Assert.assertEquals(MetadataScope.USER,metadata.getScope());
  Assert.assertEquals(mystream,metadata.getTargetId());
  Assert.assertEquals(streamProperties,metadata.getProperties());
  Assert.assertEquals(streamTags,metadata.getTags());
  removeAllMetadata();
  assertCleanState();
}","@Test public void testMetadata() throws IOException {
  assertCleanState();
  removeAllMetadata();
  assertCleanState();
  Map<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  Map<String,String> serviceProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  Map<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  Map<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  Set<String> appTags=ImmutableSet.of(""String_Node_Str"");
  Set<String> serviceTags=ImmutableSet.of(""String_Node_Str"");
  Set<String> datasetTags=ImmutableSet.of(""String_Node_Str"");
  Set<String> streamTags=ImmutableSet.of(""String_Node_Str"");
  Assert.assertEquals(200,addProperties(application,appProperties).getResponseCode());
  Assert.assertEquals(200,addProperties(pingService,serviceProperties).getResponseCode());
  Assert.assertEquals(200,addProperties(myds,datasetProperties).getResponseCode());
  Assert.assertEquals(200,addProperties(mystream,streamProperties).getResponseCode());
  Assert.assertEquals(200,addTags(application,appTags).getResponseCode());
  Assert.assertEquals(200,addTags(pingService,serviceTags).getResponseCode());
  Assert.assertEquals(200,addTags(myds,datasetTags).getResponseCode());
  Assert.assertEquals(200,addTags(mystream,streamTags).getResponseCode());
  Set<MetadataRecord> metadataRecords=getMetadata(application);
  Assert.assertEquals(1,metadataRecords.size());
  MetadataRecord metadata=metadataRecords.iterator().next();
  Assert.assertEquals(MetadataScope.USER,metadata.getScope());
  Assert.assertEquals(application,metadata.getEntityId());
  Assert.assertEquals(appProperties,metadata.getProperties());
  Assert.assertEquals(appTags,metadata.getTags());
  metadataRecords=getMetadata(pingService);
  Assert.assertEquals(1,metadataRecords.size());
  metadata=metadataRecords.iterator().next();
  Assert.assertEquals(MetadataScope.USER,metadata.getScope());
  Assert.assertEquals(pingService,metadata.getEntityId());
  Assert.assertEquals(serviceProperties,metadata.getProperties());
  Assert.assertEquals(serviceTags,metadata.getTags());
  metadataRecords=getMetadata(myds);
  Assert.assertEquals(1,metadataRecords.size());
  metadata=metadataRecords.iterator().next();
  Assert.assertEquals(MetadataScope.USER,metadata.getScope());
  Assert.assertEquals(myds,metadata.getEntityId());
  Assert.assertEquals(datasetProperties,metadata.getProperties());
  Assert.assertEquals(datasetTags,metadata.getTags());
  metadataRecords=getMetadata(mystream);
  Assert.assertEquals(1,metadataRecords.size());
  metadata=metadataRecords.iterator().next();
  Assert.assertEquals(MetadataScope.USER,metadata.getScope());
  Assert.assertEquals(mystream,metadata.getEntityId());
  Assert.assertEquals(streamProperties,metadata.getProperties());
  Assert.assertEquals(streamTags,metadata.getTags());
  removeAllMetadata();
  assertCleanState();
}"
6504,"@Test public void testTags() throws IOException {
  Assert.assertEquals(400,addTags(application).getResponseCode());
  Set<String> appTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(application,appTags).getResponseCode());
  Assert.assertEquals(400,addTags(pingService).getResponseCode());
  Set<String> serviceTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(pingService,serviceTags).getResponseCode());
  Assert.assertEquals(400,addTags(myds).getResponseCode());
  Set<String> datasetTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(myds,datasetTags).getResponseCode());
  Assert.assertEquals(400,addTags(mystream).getResponseCode());
  Set<String> streamTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(mystream,streamTags).getResponseCode());
  Set<String> tags=getTags(application);
  Assert.assertTrue(tags.containsAll(appTags));
  Assert.assertTrue(appTags.containsAll(tags));
  tags=getTags(pingService);
  Assert.assertTrue(tags.containsAll(serviceTags));
  Assert.assertTrue(serviceTags.containsAll(tags));
  tags=getTags(myds);
  Assert.assertTrue(tags.containsAll(datasetTags));
  Assert.assertTrue(datasetTags.containsAll(tags));
  tags=getTags(mystream);
  Assert.assertTrue(tags.containsAll(streamTags));
  Assert.assertTrue(streamTags.containsAll(tags));
  Set<MetadataSearchResultRecord> searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM));
  Assert.assertEquals(expected,searchTags);
  searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM),new MetadataSearchResultRecord(pingService,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(expected,searchTags);
  searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(expected,searchTags);
  searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.<MetadataSearchResultRecord>of(),searchTags);
  removeTags(application,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.of(""String_Node_Str""),getTags(application));
  removeTags(pingService);
  Assert.assertTrue(getTags(pingService).isEmpty());
  removeTags(pingService);
  Assert.assertTrue(getTags(pingService).isEmpty());
  removeTags(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.of(""String_Node_Str""),getTags(myds));
  removeTags(mystream,""String_Node_Str"");
  removeTags(mystream,""String_Node_Str"");
  Assert.assertTrue(getTags(mystream).isEmpty());
  removeTags(application);
  removeTags(pingService);
  removeTags(myds);
  removeTags(mystream);
  Assert.assertTrue(getTags(application).isEmpty());
  Assert.assertTrue(getTags(pingService).isEmpty());
  Assert.assertTrue(getTags(myds).isEmpty());
  Assert.assertTrue(getTags(mystream).isEmpty());
  Assert.assertEquals(404,addTags(nonExistingApp,appTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingService,serviceTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingDataset,datasetTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingStream,streamTags).getResponseCode());
}","@Test public void testTags() throws IOException {
  Assert.assertEquals(400,addTags(application).getResponseCode());
  Set<String> appTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(application,appTags).getResponseCode());
  Assert.assertEquals(400,addTags(pingService).getResponseCode());
  Set<String> serviceTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(pingService,serviceTags).getResponseCode());
  Assert.assertEquals(400,addTags(myds).getResponseCode());
  Set<String> datasetTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(myds,datasetTags).getResponseCode());
  Assert.assertEquals(400,addTags(mystream).getResponseCode());
  Set<String> streamTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(mystream,streamTags).getResponseCode());
  Set<String> tags=getTags(application);
  Assert.assertTrue(tags.containsAll(appTags));
  Assert.assertTrue(appTags.containsAll(tags));
  tags=getTags(pingService);
  Assert.assertTrue(tags.containsAll(serviceTags));
  Assert.assertTrue(serviceTags.containsAll(tags));
  tags=getTags(myds);
  Assert.assertTrue(tags.containsAll(datasetTags));
  Assert.assertTrue(datasetTags.containsAll(tags));
  tags=getTags(mystream);
  Assert.assertTrue(tags.containsAll(streamTags));
  Assert.assertTrue(streamTags.containsAll(tags));
  Set<MetadataSearchResultRecord> searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream));
  Assert.assertEquals(expected,searchTags);
  searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream),new MetadataSearchResultRecord(pingService));
  Assert.assertEquals(expected,searchTags);
  searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(expected,searchTags);
  searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.<MetadataSearchResultRecord>of(),searchTags);
  removeTags(application,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.of(""String_Node_Str""),getTags(application));
  removeTags(pingService);
  Assert.assertTrue(getTags(pingService).isEmpty());
  removeTags(pingService);
  Assert.assertTrue(getTags(pingService).isEmpty());
  removeTags(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.of(""String_Node_Str""),getTags(myds));
  removeTags(mystream,""String_Node_Str"");
  removeTags(mystream,""String_Node_Str"");
  Assert.assertTrue(getTags(mystream).isEmpty());
  removeTags(application);
  removeTags(pingService);
  removeTags(myds);
  removeTags(mystream);
  Assert.assertTrue(getTags(application).isEmpty());
  Assert.assertTrue(getTags(pingService).isEmpty());
  Assert.assertTrue(getTags(myds).isEmpty());
  Assert.assertTrue(getTags(mystream).isEmpty());
  Assert.assertEquals(404,addTags(nonExistingApp,appTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingService,serviceTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingDataset,datasetTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingStream,streamTags).getResponseCode());
}"
6505,"@Test public void testProperties() throws IOException {
  Assert.assertEquals(400,addProperties(application).getResponseCode());
  Map<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(application,appProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(pingService).getResponseCode());
  Map<String,String> serviceProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(pingService,serviceProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(myds).getResponseCode());
  Map<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(myds,datasetProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(mystream).getResponseCode());
  Map<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(mystream,streamProperties).getResponseCode());
  Map<String,String> properties=getProperties(application);
  Assert.assertEquals(appProperties,properties);
  properties=getProperties(pingService);
  Assert.assertEquals(serviceProperties,properties);
  properties=getProperties(myds);
  Assert.assertEquals(datasetProperties,properties);
  properties=getProperties(mystream);
  Assert.assertEquals(streamProperties,properties);
  Set<MetadataSearchResultRecord> searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  expected=ImmutableSet.of(new MetadataSearchResultRecord(pingService,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertTrue(searchProperties.size() == 0);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertTrue(searchProperties.size() == 0);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.<MetadataSearchResultRecord>of(),searchProperties);
  searchProperties=searchMetadata(""String_Node_Str"",""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.of(),searchProperties);
  removeProperties(application);
  Assert.assertTrue(getProperties(application).isEmpty());
  removeProperties(pingService,""String_Node_Str"");
  removeProperties(pingService,""String_Node_Str"");
  Assert.assertTrue(getProperties(pingService).isEmpty());
  removeProperties(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(myds));
  removeProperties(mystream,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(mystream));
  removeProperties(application);
  removeProperties(pingService);
  removeProperties(myds);
  removeProperties(mystream);
  Assert.assertTrue(getProperties(application).isEmpty());
  Assert.assertTrue(getProperties(pingService).isEmpty());
  Assert.assertTrue(getProperties(myds).isEmpty());
  Assert.assertTrue(getProperties(mystream).isEmpty());
  Assert.assertEquals(404,addProperties(nonExistingApp,appProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingService,serviceProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingDataset,datasetProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingStream,streamProperties).getResponseCode());
}","@Test public void testProperties() throws IOException {
  Assert.assertEquals(400,addProperties(application).getResponseCode());
  Map<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(application,appProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(pingService).getResponseCode());
  Map<String,String> serviceProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(pingService,serviceProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(myds).getResponseCode());
  Map<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(myds,datasetProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(mystream).getResponseCode());
  Map<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(mystream,streamProperties).getResponseCode());
  Map<String,String> properties=getProperties(application);
  Assert.assertEquals(appProperties,properties);
  properties=getProperties(pingService);
  Assert.assertEquals(serviceProperties,properties);
  properties=getProperties(myds);
  Assert.assertEquals(datasetProperties,properties);
  properties=getProperties(mystream);
  Assert.assertEquals(streamProperties,properties);
  Set<MetadataSearchResultRecord> searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  expected=ImmutableSet.of(new MetadataSearchResultRecord(pingService));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertTrue(searchProperties.size() == 0);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertTrue(searchProperties.size() == 0);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.<MetadataSearchResultRecord>of(),searchProperties);
  searchProperties=searchMetadata(""String_Node_Str"",""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.of(),searchProperties);
  removeProperties(application);
  Assert.assertTrue(getProperties(application).isEmpty());
  removeProperties(pingService,""String_Node_Str"");
  removeProperties(pingService,""String_Node_Str"");
  Assert.assertTrue(getProperties(pingService).isEmpty());
  removeProperties(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(myds));
  removeProperties(mystream,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(mystream));
  removeProperties(application);
  removeProperties(pingService);
  removeProperties(myds);
  removeProperties(mystream);
  Assert.assertTrue(getProperties(application).isEmpty());
  Assert.assertTrue(getProperties(pingService).isEmpty());
  Assert.assertTrue(getProperties(myds).isEmpty());
  Assert.assertTrue(getProperties(mystream).isEmpty());
  Assert.assertEquals(404,addProperties(nonExistingApp,appProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingService,serviceProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingDataset,datasetProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingStream,streamProperties).getResponseCode());
}"
6506,"@Override public void publish(MetadataChangeRecord changeRecord){
  byte[] changesToPublish=Bytes.toBytes(GSON.toJson(changeRecord));
  Object partitionKey=changeRecord.getPrevious().getTargetId();
  @SuppressWarnings(""String_Node_Str"") ByteBuffer message=ByteBuffer.wrap(changesToPublish);
  try {
    producer.get().send(new KeyedMessage<>(topic,Math.abs(partitionKey.hashCode()),message));
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",topic,brokerList,e);
  }
}","@Override public void publish(MetadataChangeRecord changeRecord){
  byte[] changesToPublish=Bytes.toBytes(GSON.toJson(changeRecord));
  Object partitionKey=changeRecord.getPrevious().getEntityId();
  @SuppressWarnings(""String_Node_Str"") ByteBuffer message=ByteBuffer.wrap(changesToPublish);
  try {
    producer.get().send(new KeyedMessage<>(topic,Math.abs(partitionKey.hashCode()),message));
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",topic,brokerList,e);
  }
}"
6507,"private void addMetadataRecord(BusinessMetadataDataset dataset,MetadataRecord record){
  for (  Map.Entry<String,String> entry : record.getProperties().entrySet()) {
    dataset.setProperty(record.getTargetId(),entry.getKey(),entry.getValue());
  }
  dataset.addTags(record.getTargetId(),record.getTags().toArray(new String[0]));
}","private void addMetadataRecord(BusinessMetadataDataset dataset,MetadataRecord record){
  for (  Map.Entry<String,String> entry : record.getProperties().entrySet()) {
    dataset.setProperty(record.getEntityId(),entry.getKey(),entry.getValue());
  }
  dataset.addTags(record.getEntityId(),record.getTags().toArray(new String[0]));
}"
6508,"@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  String pluginId=getPluginId();
  Class<Object> driver=pipelineConfigurer.usePluginClass(config.jmsPluginType,config.jmsPluginName,pluginId,PluginProperties.builder().build());
  Preconditions.checkArgument(driver != null,""String_Node_Str"");
}","@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  String pluginId=getPluginId();
  Class<Object> driver=pipelineConfigurer.usePluginClass(config.jmsPluginType,config.jmsPluginName,pluginId,PluginProperties.builder().build());
  Preconditions.checkArgument(driver != null,""String_Node_Str"" + ""String_Node_Str"",config.jmsPluginName,config.jmsPluginType);
}"
6509,"@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  String pluginId=getPluginId();
  Class<Object> driver=pipelineConfigurer.usePluginClass(config.jmsPluginType,config.jmsPluginName,pluginId,PluginProperties.builder().build());
  Preconditions.checkArgument(driver != null,""String_Node_Str"");
}","@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  String pluginId=getPluginId();
  Class<Object> driver=pipelineConfigurer.usePluginClass(config.jmsPluginType,config.jmsPluginName,pluginId,PluginProperties.builder().build());
  Preconditions.checkArgument(driver != null,""String_Node_Str"" + ""String_Node_Str"",config.jmsPluginName,config.jmsPluginType);
}"
6510,"@Override public Class<?> loadClass(String s) throws ClassNotFoundException {
  if (s.startsWith(""String_Node_Str"")) {
    return twillClassLoader.loadClass(s);
  }
  return super.loadClass(s);
}","@Override public Class<?> loadClass(String s) throws ClassNotFoundException {
  if (s.startsWith(""String_Node_Str"") || s.equals(""String_Node_Str"")) {
    return twillClassLoader.loadClass(s);
  }
  return super.loadClass(s);
}"
6511,"@Override public Set<MetadataRecord> getMetadata(Id.NamespacedId entityId) throws NotFoundException {
  ensureEntityExists(entityId);
  return ImmutableSet.of(businessMds.getMetadata(entityId));
}","@Override public Set<MetadataRecord> getMetadata(Id.NamespacedId entityId) throws NotFoundException {
  entityValidator.ensureEntityExists(entityId);
  return ImmutableSet.of(businessMds.getMetadata(entityId));
}"
6512,"@Override public Map<String,String> getProperties(Id.NamespacedId entityId) throws NotFoundException {
  ensureEntityExists(entityId);
  return businessMds.getProperties(entityId);
}","@Override public Map<String,String> getProperties(Id.NamespacedId entityId) throws NotFoundException {
  entityValidator.ensureEntityExists(entityId);
  return businessMds.getProperties(entityId);
}"
6513,"@Override public void removeTags(Id.NamespacedId entityId,String... tags) throws NotFoundException {
  ensureEntityExists(entityId);
  businessMds.removeTags(entityId,tags);
}","@Override public void removeTags(Id.NamespacedId entityId,String... tags) throws NotFoundException {
  entityValidator.ensureEntityExists(entityId);
  businessMds.removeTags(entityId,tags);
}"
6514,"@Override public Set<String> getTags(Id.NamespacedId entityId) throws NotFoundException {
  ensureEntityExists(entityId);
  return businessMds.getTags(entityId);
}","@Override public Set<String> getTags(Id.NamespacedId entityId) throws NotFoundException {
  entityValidator.ensureEntityExists(entityId);
  return businessMds.getTags(entityId);
}"
6515,"@Override public void addProperties(Id.NamespacedId entityId,Map<String,String> properties) throws NotFoundException, InvalidMetadataException {
  ensureEntityExists(entityId);
  validateProperties(entityId,properties);
  businessMds.setProperties(entityId,properties);
}","@Override public void addProperties(Id.NamespacedId entityId,Map<String,String> properties) throws NotFoundException, InvalidMetadataException {
  entityValidator.ensureEntityExists(entityId);
  validateProperties(entityId,properties);
  businessMds.setProperties(entityId,properties);
}"
6516,"@Override public void addTags(Id.NamespacedId entityId,String... tags) throws NotFoundException, InvalidMetadataException {
  ensureEntityExists(entityId);
  validateTags(entityId,tags);
  businessMds.addTags(entityId,tags);
}","@Override public void addTags(Id.NamespacedId entityId,String... tags) throws NotFoundException, InvalidMetadataException {
  entityValidator.ensureEntityExists(entityId);
  validateTags(entityId,tags);
  businessMds.addTags(entityId,tags);
}"
6517,"@Inject DefaultMetadataAdmin(AbstractNamespaceClient namespaceClient,BusinessMetadataStore businessMds,CConfiguration cConf,Store store,DatasetFramework datasetFramework,StreamAdmin streamAdmin){
  this.namespaceClient=namespaceClient;
  this.businessMds=businessMds;
  this.cConf=cConf;
  this.store=store;
  this.datasetFramework=datasetFramework;
  this.streamAdmin=streamAdmin;
}","@Inject DefaultMetadataAdmin(AbstractNamespaceClient namespaceClient,BusinessMetadataStore businessMds,CConfiguration cConf,Store store,DatasetFramework datasetFramework,StreamAdmin streamAdmin){
  this.businessMds=businessMds;
  this.cConf=cConf;
  this.entityValidator=new EntityValidator(namespaceClient,store,datasetFramework,streamAdmin);
}"
6518,"@Override public void removeMetadata(Id.NamespacedId entityId) throws NotFoundException {
  ensureEntityExists(entityId);
  businessMds.removeMetadata(entityId);
}","@Override public void removeMetadata(Id.NamespacedId entityId) throws NotFoundException {
  entityValidator.ensureEntityExists(entityId);
  businessMds.removeMetadata(entityId);
}"
6519,"@Override public void removeProperties(Id.NamespacedId entityId,String... keys) throws NotFoundException {
  ensureEntityExists(entityId);
  businessMds.removeProperties(entityId,keys);
}","@Override public void removeProperties(Id.NamespacedId entityId,String... keys) throws NotFoundException {
  entityValidator.ensureEntityExists(entityId);
  businessMds.removeProperties(entityId,keys);
}"
6520,"private static List<Module> createPersistentModules(CConfiguration configuration,Configuration hConf){
  configuration.setIfUnset(Constants.CFG_DATA_LEVELDB_DIR,Constants.DEFAULT_DATA_LEVELDB_DIR);
  configuration.set(Constants.CFG_DATA_INMEMORY_PERSISTENCE,Constants.InMemoryPersistenceType.LEVELDB.name());
  configuration.set(Constants.AppFabric.SERVER_ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Transaction.Container.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Dataset.Manager.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Dataset.Executor.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Stream.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Metrics.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Metrics.SERVER_ADDRESS,""String_Node_Str"");
  configuration.set(Constants.MetricsProcessor.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.LogSaver.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Security.AUTH_SERVER_ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Explore.SERVER_ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Metadata.SERVICE_BIND_ADDRESS,""String_Node_Str"");
  return ImmutableList.of(new ConfigModule(configuration,hConf),new IOModule(),new MetricsHandlerModule(),new DiscoveryRuntimeModule().getStandaloneModules(),new LocationRuntimeModule().getStandaloneModules(),new AppFabricServiceRuntimeModule().getStandaloneModules(),new ProgramRunnerRuntimeModule().getStandaloneModules(),new DataFabricModules().getStandaloneModules(),new DataSetsModules().getStandaloneModules(),new DataSetServiceModules().getStandaloneModules(),new MetricsClientRuntimeModule().getStandaloneModules(),new LoggingModules().getStandaloneModules(),new RouterModules().getStandaloneModules(),new SecurityModules().getStandaloneModules(),new StreamServiceRuntimeModule().getStandaloneModules(),new ExploreRuntimeModule().getStandaloneModules(),new ServiceStoreModules().getStandaloneModules(),new ExploreClientModule(),new NotificationFeedServiceRuntimeModule().getStandaloneModules(),new NotificationServiceRuntimeModule().getStandaloneModules(),new ViewAdminModules().getStandaloneModules(),new StreamAdminModules().getStandaloneModules(),new NamespaceClientRuntimeModule().getStandaloneModules(),new MetadataServiceModule());
}","private static List<Module> createPersistentModules(CConfiguration configuration,Configuration hConf){
  configuration.setIfUnset(Constants.CFG_DATA_LEVELDB_DIR,Constants.DEFAULT_DATA_LEVELDB_DIR);
  configuration.set(Constants.CFG_DATA_INMEMORY_PERSISTENCE,Constants.InMemoryPersistenceType.LEVELDB.name());
  configuration.set(Constants.AppFabric.SERVER_ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Transaction.Container.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Dataset.Manager.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Dataset.Executor.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Stream.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Metrics.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Metrics.SERVER_ADDRESS,""String_Node_Str"");
  configuration.set(Constants.MetricsProcessor.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.LogSaver.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Security.AUTH_SERVER_BIND_ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Explore.SERVER_ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Metadata.SERVICE_BIND_ADDRESS,""String_Node_Str"");
  return ImmutableList.of(new ConfigModule(configuration,hConf),new IOModule(),new MetricsHandlerModule(),new DiscoveryRuntimeModule().getStandaloneModules(),new LocationRuntimeModule().getStandaloneModules(),new AppFabricServiceRuntimeModule().getStandaloneModules(),new ProgramRunnerRuntimeModule().getStandaloneModules(),new DataFabricModules().getStandaloneModules(),new DataSetsModules().getStandaloneModules(),new DataSetServiceModules().getStandaloneModules(),new MetricsClientRuntimeModule().getStandaloneModules(),new LoggingModules().getStandaloneModules(),new RouterModules().getStandaloneModules(),new SecurityModules().getStandaloneModules(),new StreamServiceRuntimeModule().getStandaloneModules(),new ExploreRuntimeModule().getStandaloneModules(),new ServiceStoreModules().getStandaloneModules(),new ExploreClientModule(),new NotificationFeedServiceRuntimeModule().getStandaloneModules(),new NotificationServiceRuntimeModule().getStandaloneModules(),new ViewAdminModules().getStandaloneModules(),new StreamAdminModules().getStandaloneModules(),new NamespaceClientRuntimeModule().getStandaloneModules(),new MetadataServiceModule());
}"
6521,"public MasterServiceMain(){
  this.cConf=CConfiguration.create();
  this.cConf.set(Constants.Dataset.Manager.ADDRESS,getLocalHost().getCanonicalHostName());
  this.hConf=HBaseConfiguration.create();
  Injector injector=createBaseInjector(cConf,hConf);
  this.baseInjector=injector;
  this.zkClient=injector.getInstance(ZKClientService.class);
  this.twillRunner=injector.getInstance(TwillRunnerService.class);
  this.kafkaClient=injector.getInstance(KafkaClientService.class);
  this.metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  this.serviceStore=injector.getInstance(ServiceStore.class);
  this.secureStoreUpdater=baseInjector.getInstance(TokenSecureStoreUpdater.class);
  this.leaderElection=createLeaderElection();
}","public MasterServiceMain(){
  this.cConf=CConfiguration.create();
  this.cConf.set(Constants.Dataset.Manager.ADDRESS,getLocalHost().getCanonicalHostName());
  login();
  this.hConf=HBaseConfiguration.create();
  Injector injector=createBaseInjector(cConf,hConf);
  this.baseInjector=injector;
  this.zkClient=injector.getInstance(ZKClientService.class);
  this.twillRunner=injector.getInstance(TwillRunnerService.class);
  this.kafkaClient=injector.getInstance(KafkaClientService.class);
  this.metricsCollectionService=injector.getInstance(MetricsCollectionService.class);
  this.serviceStore=injector.getInstance(ServiceStore.class);
  this.secureStoreUpdater=baseInjector.getInstance(TokenSecureStoreUpdater.class);
  this.leaderElection=createLeaderElection();
}"
6522,"@Override public void init(String[] args){
  cleanupTempDir();
  checkExploreRequirements();
  login();
}","@Override public void init(String[] args){
  cleanupTempDir();
  checkExploreRequirements();
}"
6523,"@Override public void destroy(){
  Destroyables.destroyQuietly(transformExecutor);
}","@Override public void destroy(){
  Destroyables.destroyQuietly(transformExecutor);
  LOG.debug(""String_Node_Str"",sinks.size());
  for (  WrappedSink<Object,Object,Object> sink : sinks) {
    LOG.trace(""String_Node_Str"",sink.sink);
    Destroyables.destroyQuietly(sink.sink);
  }
}"
6524,"private static List<Module> createPersistentModules(CConfiguration configuration,Configuration hConf){
  configuration.setIfUnset(Constants.CFG_DATA_LEVELDB_DIR,Constants.DEFAULT_DATA_LEVELDB_DIR);
  configuration.set(Constants.CFG_DATA_INMEMORY_PERSISTENCE,Constants.InMemoryPersistenceType.LEVELDB.name());
  return ImmutableList.of(new ConfigModule(configuration,hConf),new IOModule(),new MetricsHandlerModule(),new DiscoveryRuntimeModule().getStandaloneModules(),new LocationRuntimeModule().getStandaloneModules(),new AppFabricServiceRuntimeModule().getStandaloneModules(),new ProgramRunnerRuntimeModule().getStandaloneModules(),new DataFabricModules().getStandaloneModules(),new DataSetsModules().getStandaloneModules(),new DataSetServiceModules().getStandaloneModules(),new MetricsClientRuntimeModule().getStandaloneModules(),new LoggingModules().getStandaloneModules(),new RouterModules().getStandaloneModules(),new SecurityModules().getStandaloneModules(),new StreamServiceRuntimeModule().getStandaloneModules(),new ExploreRuntimeModule().getStandaloneModules(),new ServiceStoreModules().getStandaloneModules(),new ExploreClientModule(),new NotificationFeedServiceRuntimeModule().getStandaloneModules(),new NotificationServiceRuntimeModule().getStandaloneModules(),new ViewAdminModules().getStandaloneModules(),new StreamAdminModules().getStandaloneModules(),new NamespaceClientRuntimeModule().getStandaloneModules(),new MetadataServiceModule());
}","private static List<Module> createPersistentModules(CConfiguration configuration,Configuration hConf){
  configuration.setIfUnset(Constants.CFG_DATA_LEVELDB_DIR,Constants.DEFAULT_DATA_LEVELDB_DIR);
  configuration.set(Constants.CFG_DATA_INMEMORY_PERSISTENCE,Constants.InMemoryPersistenceType.LEVELDB.name());
  configuration.set(Constants.AppFabric.SERVER_ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Transaction.Container.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Dataset.Manager.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Dataset.Executor.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Stream.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Metrics.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Metrics.SERVER_ADDRESS,""String_Node_Str"");
  configuration.set(Constants.MetricsProcessor.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.LogSaver.ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Security.AUTH_SERVER_ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Explore.SERVER_ADDRESS,""String_Node_Str"");
  configuration.set(Constants.Metadata.SERVICE_BIND_ADDRESS,""String_Node_Str"");
  return ImmutableList.of(new ConfigModule(configuration,hConf),new IOModule(),new MetricsHandlerModule(),new DiscoveryRuntimeModule().getStandaloneModules(),new LocationRuntimeModule().getStandaloneModules(),new AppFabricServiceRuntimeModule().getStandaloneModules(),new ProgramRunnerRuntimeModule().getStandaloneModules(),new DataFabricModules().getStandaloneModules(),new DataSetsModules().getStandaloneModules(),new DataSetServiceModules().getStandaloneModules(),new MetricsClientRuntimeModule().getStandaloneModules(),new LoggingModules().getStandaloneModules(),new RouterModules().getStandaloneModules(),new SecurityModules().getStandaloneModules(),new StreamServiceRuntimeModule().getStandaloneModules(),new ExploreRuntimeModule().getStandaloneModules(),new ServiceStoreModules().getStandaloneModules(),new ExploreClientModule(),new NotificationFeedServiceRuntimeModule().getStandaloneModules(),new NotificationServiceRuntimeModule().getStandaloneModules(),new ViewAdminModules().getStandaloneModules(),new StreamAdminModules().getStandaloneModules(),new NamespaceClientRuntimeModule().getStandaloneModules(),new MetadataServiceModule());
}"
6525,"@Inject public LocalApplicationManager(CConfiguration configuration,PipelineFactory pipelineFactory,NamespacedLocationFactory namespacedLocationFactory,Store store,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,DatasetFramework datasetFramework,@Named(""String_Node_Str"") DatasetFramework inMemoryDatasetFramework,StreamAdmin streamAdmin,Scheduler scheduler,AdapterService adapterService,@Assisted ProgramTerminator programTerminator,MetricStore metricStore,UsageRegistry usageRegistry,ArtifactRepository artifactRepository){
  this.configuration=configuration;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.pipelineFactory=pipelineFactory;
  this.store=store;
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.programTerminator=programTerminator;
  this.datasetFramework=datasetFramework;
  this.inMemoryDatasetFramework=inMemoryDatasetFramework;
  this.streamAdmin=streamAdmin;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
  this.adapterService=adapterService;
  this.usageRegistry=usageRegistry;
  this.artifactRepository=artifactRepository;
}","@Inject public LocalApplicationManager(CConfiguration configuration,PipelineFactory pipelineFactory,NamespacedLocationFactory namespacedLocationFactory,Store store,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,DatasetFramework datasetFramework,@Named(""String_Node_Str"") DatasetFramework inMemoryDatasetFramework,StreamAdmin streamAdmin,Scheduler scheduler,AdapterService adapterService,@Assisted ProgramTerminator programTerminator,MetricStore metricStore,UsageRegistry usageRegistry,ArtifactRepository artifactRepository,BusinessMetadataStore businessMetadataStore){
  this.configuration=configuration;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.pipelineFactory=pipelineFactory;
  this.store=store;
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.programTerminator=programTerminator;
  this.datasetFramework=datasetFramework;
  this.inMemoryDatasetFramework=inMemoryDatasetFramework;
  this.streamAdmin=streamAdmin;
  this.scheduler=scheduler;
  this.metricStore=metricStore;
  this.adapterService=adapterService;
  this.usageRegistry=usageRegistry;
  this.artifactRepository=artifactRepository;
  this.businessMetadataStore=businessMetadataStore;
}"
6526,"@Override public ListenableFuture<O> deploy(Id.Namespace namespace,@Nullable String appId,I input) throws Exception {
  Pipeline<O> pipeline=pipelineFactory.getPipeline();
  pipeline.addLast(new LocalArtifactLoaderStage(configuration,store,namespace,appId,artifactRepository));
  pipeline.addLast(new ApplicationVerificationStage(store,datasetFramework,adapterService));
  pipeline.addLast(new DeployDatasetModulesStage(configuration,namespace,datasetFramework,inMemoryDatasetFramework));
  pipeline.addLast(new CreateDatasetInstancesStage(configuration,datasetFramework,namespace));
  pipeline.addLast(new CreateStreamsStage(namespace,streamAdmin));
  pipeline.addLast(new DeletedProgramHandlerStage(store,programTerminator,streamConsumerFactory,queueAdmin,metricStore));
  pipeline.addLast(new ProgramGenerationStage(configuration,namespacedLocationFactory));
  pipeline.addLast(new ApplicationRegistrationStage(store,usageRegistry));
  pipeline.addLast(new CreateSchedulesStage(scheduler));
  return pipeline.execute(input);
}","@Override public ListenableFuture<O> deploy(Id.Namespace namespace,@Nullable String appId,I input) throws Exception {
  Pipeline<O> pipeline=pipelineFactory.getPipeline();
  pipeline.addLast(new LocalArtifactLoaderStage(configuration,store,namespace,appId,artifactRepository));
  pipeline.addLast(new ApplicationVerificationStage(store,datasetFramework,adapterService));
  pipeline.addLast(new DeployDatasetModulesStage(configuration,namespace,datasetFramework,inMemoryDatasetFramework));
  pipeline.addLast(new CreateDatasetInstancesStage(configuration,datasetFramework,namespace));
  pipeline.addLast(new CreateStreamsStage(namespace,streamAdmin));
  pipeline.addLast(new DeletedProgramHandlerStage(store,programTerminator,streamConsumerFactory,queueAdmin,metricStore,businessMetadataStore));
  pipeline.addLast(new ProgramGenerationStage(configuration,namespacedLocationFactory));
  pipeline.addLast(new ApplicationRegistrationStage(store,usageRegistry));
  pipeline.addLast(new CreateSchedulesStage(scheduler));
  return pipeline.execute(input);
}"
6527,"public DeletedProgramHandlerStage(Store store,ProgramTerminator programTerminator,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,MetricStore metricStore){
  super(TypeToken.of(ApplicationDeployable.class));
  this.store=store;
  this.programTerminator=programTerminator;
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.metricStore=metricStore;
}","public DeletedProgramHandlerStage(Store store,ProgramTerminator programTerminator,StreamConsumerFactory streamConsumerFactory,QueueAdmin queueAdmin,MetricStore metricStore,BusinessMetadataStore businessMetadataStore){
  super(TypeToken.of(ApplicationDeployable.class));
  this.store=store;
  this.programTerminator=programTerminator;
  this.streamConsumerFactory=streamConsumerFactory;
  this.queueAdmin=queueAdmin;
  this.metricStore=metricStore;
  this.businessMetadataStore=businessMetadataStore;
}"
6528,"@Override public void process(ApplicationDeployable appSpec) throws Exception {
  List<ProgramSpecification> deletedSpecs=store.getDeletedProgramSpecifications(appSpec.getId(),appSpec.getSpecification());
  List<String> deletedFlows=Lists.newArrayList();
  for (  ProgramSpecification spec : deletedSpecs) {
    ProgramType type=ProgramTypes.fromSpecification(spec);
    Id.Program programId=Id.Program.from(appSpec.getId(),type,spec.getName());
    programTerminator.stop(programId);
    if (ProgramType.FLOW.equals(type)) {
      FlowSpecification flowSpecification=(FlowSpecification)spec;
      Multimap<String,Long> streamGroups=HashMultimap.create();
      for (      FlowletConnection connection : flowSpecification.getConnections()) {
        if (connection.getSourceType() == FlowletConnection.Type.STREAM) {
          long groupId=FlowUtils.generateConsumerGroupId(programId,connection.getTargetName());
          streamGroups.put(connection.getSourceName(),groupId);
        }
      }
      String namespace=String.format(""String_Node_Str"",programId.getApplicationId(),programId.getId());
      for (      Map.Entry<String,Collection<Long>> entry : streamGroups.asMap().entrySet()) {
        streamConsumerFactory.dropAll(Id.Stream.from(appSpec.getId().getNamespaceId(),entry.getKey()),namespace,entry.getValue());
      }
      queueAdmin.dropAllForFlow(Id.Flow.from(programId.getApplication(),programId.getId()));
      deletedFlows.add(programId.getId());
    }
  }
  if (!deletedFlows.isEmpty()) {
    deleteMetrics(appSpec.getId().getNamespaceId(),appSpec.getId().getId(),deletedFlows);
  }
  emit(appSpec);
}","@Override public void process(ApplicationDeployable appSpec) throws Exception {
  List<ProgramSpecification> deletedSpecs=store.getDeletedProgramSpecifications(appSpec.getId(),appSpec.getSpecification());
  List<String> deletedFlows=Lists.newArrayList();
  for (  ProgramSpecification spec : deletedSpecs) {
    ProgramType type=ProgramTypes.fromSpecification(spec);
    Id.Program programId=Id.Program.from(appSpec.getId(),type,spec.getName());
    programTerminator.stop(programId);
    if (ProgramType.FLOW.equals(type)) {
      FlowSpecification flowSpecification=(FlowSpecification)spec;
      Multimap<String,Long> streamGroups=HashMultimap.create();
      for (      FlowletConnection connection : flowSpecification.getConnections()) {
        if (connection.getSourceType() == FlowletConnection.Type.STREAM) {
          long groupId=FlowUtils.generateConsumerGroupId(programId,connection.getTargetName());
          streamGroups.put(connection.getSourceName(),groupId);
        }
      }
      String namespace=String.format(""String_Node_Str"",programId.getApplicationId(),programId.getId());
      for (      Map.Entry<String,Collection<Long>> entry : streamGroups.asMap().entrySet()) {
        streamConsumerFactory.dropAll(Id.Stream.from(appSpec.getId().getNamespaceId(),entry.getKey()),namespace,entry.getValue());
      }
      queueAdmin.dropAllForFlow(Id.Flow.from(programId.getApplication(),programId.getId()));
      deletedFlows.add(programId.getId());
    }
    businessMetadataStore.removeMetadata(programId);
  }
  if (!deletedFlows.isEmpty()) {
    deleteMetrics(appSpec.getId().getNamespaceId(),appSpec.getId().getId(),deletedFlows);
  }
  emit(appSpec);
}"
6529,"private void recordCompletedWorkflow(final Id.Workflow id,String pid){
  final RunRecordMeta run=getRun(id,pid);
  if (run == null) {
    return;
  }
  Id.Application app=id.getApplication();
  ApplicationSpecification appSpec=getApplication(app);
  if (appSpec == null || appSpec.getWorkflows() == null || appSpec.getWorkflows().get(id.getId()) == null) {
    return;
  }
  WorkflowSpecification workflowSpec=appSpec.getWorkflows().get(id.getId());
  Map<String,WorkflowNode> nodeIdMap=workflowSpec.getNodeIdMap();
  final List<WorkflowDataset.ProgramRun> programRunsList=new ArrayList<>();
  for (  Map.Entry<String,String> entry : run.getProperties().entrySet()) {
    if (!(""String_Node_Str"".equals(entry.getKey()) || ""String_Node_Str"".equals(entry.getKey()))) {
      WorkflowActionNode workflowNode=(WorkflowActionNode)nodeIdMap.get(entry.getKey());
      ProgramType programType=ProgramType.valueOfSchedulableType(workflowNode.getProgram().getProgramType());
      Id.Program innerProgram=Id.Program.from(app.getNamespaceId(),app.getId(),programType,entry.getKey());
      RunRecordMeta innerProgramRun=getRun(innerProgram,entry.getValue());
      programRunsList.add(new WorkflowDataset.ProgramRun(entry.getKey(),entry.getValue(),programType,innerProgramRun.getStopTs() - innerProgramRun.getStartTs()));
    }
  }
  txnlWorkflow.executeUnchecked(new TransactionExecutor.Function<WorkflowStatsDataset,Void>(){
    @Override public Void apply(    WorkflowStatsDataset dataset){
      dataset.workflowDataset.write(id,run,programRunsList);
      return null;
    }
  }
);
}","private void recordCompletedWorkflow(final Id.Workflow id,String pid){
  final RunRecordMeta run=getRun(id,pid);
  if (run == null) {
    return;
  }
  Id.Application app=id.getApplication();
  ApplicationSpecification appSpec=getApplication(app);
  if (appSpec == null || appSpec.getWorkflows() == null || appSpec.getWorkflows().get(id.getId()) == null) {
    return;
  }
  boolean workFlowNodeFailed=false;
  WorkflowSpecification workflowSpec=appSpec.getWorkflows().get(id.getId());
  Map<String,WorkflowNode> nodeIdMap=workflowSpec.getNodeIdMap();
  final List<WorkflowDataset.ProgramRun> programRunsList=new ArrayList<>();
  for (  Map.Entry<String,String> entry : run.getProperties().entrySet()) {
    if (!(""String_Node_Str"".equals(entry.getKey()) || ""String_Node_Str"".equals(entry.getKey()))) {
      WorkflowActionNode workflowNode=(WorkflowActionNode)nodeIdMap.get(entry.getKey());
      ProgramType programType=ProgramType.valueOfSchedulableType(workflowNode.getProgram().getProgramType());
      Id.Program innerProgram=Id.Program.from(app.getNamespaceId(),app.getId(),programType,entry.getKey());
      RunRecordMeta innerProgramRun=getRun(innerProgram,entry.getValue());
      if (innerProgramRun.getStatus().equals(ProgramRunStatus.COMPLETED)) {
        programRunsList.add(new WorkflowDataset.ProgramRun(entry.getKey(),entry.getValue(),programType,innerProgramRun.getStopTs() - innerProgramRun.getStartTs()));
      }
 else {
        workFlowNodeFailed=true;
        break;
      }
    }
  }
  if (workFlowNodeFailed) {
    return;
  }
  txnlWorkflow.executeUnchecked(new TransactionExecutor.Function<WorkflowStatsDataset,Void>(){
    @Override public Void apply(    WorkflowStatsDataset dataset){
      dataset.workflowDataset.write(id,run,programRunsList);
      return null;
    }
  }
);
}"
6530,"public void init(Set<Integer> partitions,CheckpointManager checkpointManager){
  partitonCheckpoints.clear();
  try {
    for (    Integer partition : partitions) {
      partitonCheckpoints.put(partition,checkpointManager.getCheckpoint(partition));
    }
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","public void init(Set<Integer> partitions,CheckpointManager checkpointManager){
  partitonCheckpoints.clear();
  try {
    Map<Integer,Checkpoint> partitionMap=checkpointManager.getCheckpoint(partitions);
    for (    Map.Entry<Integer,Checkpoint> partition : partitionMap.entrySet()) {
      partitonCheckpoints.put(partition.getKey(),partition.getValue());
    }
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
6531,"private void checkpoint(){
  try {
    for (    Map.Entry<Integer,Checkpoint> entry : partitionCheckpoints.entrySet()) {
      checkpointManager.saveCheckpoint(entry.getKey(),entry.getValue());
    }
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}","private void checkpoint(){
  try {
    checkpointManager.saveCheckpoint(partitionCheckpoints);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
}"
6532,"@Override public Checkpoint apply(DatasetContext<Table> ctx) throws Exception {
  Row result=ctx.get().get(Bytes.add(rowKeyPrefix,Bytes.toBytes(partition)));
  return new Checkpoint(result.getLong(OFFSET_COLNAME,-1),result.getLong(MAX_TIME_COLNAME,-1));
}","@Override public Checkpoint apply(Table table) throws Exception {
  Row result=table.get(Bytes.add(rowKeyPrefix,Bytes.toBytes(partition)));
  return new Checkpoint(result.getLong(OFFSET_COLNAME,-1),result.getLong(MAX_TIME_COLNAME,-1));
}"
6533,"public CheckpointManager(final LogSaverTableUtil tableUtil,TransactionExecutorFactory txExecutorFactory,String topic,int prefix){
  this.rowKeyPrefix=Bytes.add(Bytes.toBytes(prefix),Bytes.toBytes(topic));
  this.mds=Transactional.of(txExecutorFactory,new Supplier<DatasetContext<Table>>(){
    @Override public DatasetContext<Table> get(){
      try {
        return DatasetContext.of(tableUtil.getMetaTable());
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
}","public CheckpointManager(final LogSaverTableUtil tableUtil,TransactionExecutorFactory txExecutorFactory,String topic,int prefix){
  this.rowKeyPrefix=Bytes.add(Bytes.toBytes(prefix),Bytes.toBytes(topic));
  this.tableUtil=tableUtil;
  this.transactionExecutorFactory=txExecutorFactory;
  this.lastCheckpoint=new HashMap<>();
}"
6534,"public Checkpoint getCheckpoint(final int partition) throws Exception {
  Checkpoint checkpoint=mds.execute(new TransactionExecutor.Function<DatasetContext<Table>,Checkpoint>(){
    @Override public Checkpoint apply(    DatasetContext<Table> ctx) throws Exception {
      Row result=ctx.get().get(Bytes.add(rowKeyPrefix,Bytes.toBytes(partition)));
      return new Checkpoint(result.getLong(OFFSET_COLNAME,-1),result.getLong(MAX_TIME_COLNAME,-1));
    }
  }
);
  LOG.trace(""String_Node_Str"",checkpoint,partition);
  return checkpoint;
}","public Checkpoint getCheckpoint(final int partition) throws Exception {
  Checkpoint checkpoint=execute(new TransactionExecutor.Function<Table,Checkpoint>(){
    @Override public Checkpoint apply(    Table table) throws Exception {
      Row result=table.get(Bytes.add(rowKeyPrefix,Bytes.toBytes(partition)));
      return new Checkpoint(result.getLong(OFFSET_COLNAME,-1),result.getLong(MAX_TIME_COLNAME,-1));
    }
  }
);
  LOG.trace(""String_Node_Str"",checkpoint,partition);
  return checkpoint;
}"
6535,"public void saveCheckpoint(final int partition,final Checkpoint checkpoint) throws Exception {
  LOG.trace(""String_Node_Str"",checkpoint,partition);
  mds.execute(new TransactionExecutor.Function<DatasetContext<Table>,Void>(){
    @Override public Void apply(    DatasetContext<Table> ctx) throws Exception {
      Table table=ctx.get();
      byte[] key=Bytes.add(rowKeyPrefix,Bytes.toBytes(partition));
      table.put(key,OFFSET_COLNAME,Bytes.toBytes(checkpoint.getNextOffset()));
      table.put(key,MAX_TIME_COLNAME,Bytes.toBytes(checkpoint.getMaxEventTime()));
      return null;
    }
  }
);
}","public void saveCheckpoint(final Map<Integer,Checkpoint> checkpoints) throws Exception {
  if (lastCheckpoint.equals(checkpoints)) {
    return;
  }
  execute(new TransactionExecutor.Procedure<Table>(){
    @Override public void apply(    Table table) throws Exception {
      for (      Map.Entry<Integer,Checkpoint> entry : checkpoints.entrySet()) {
        byte[] key=Bytes.add(rowKeyPrefix,Bytes.toBytes(entry.getKey()));
        Checkpoint checkpoint=entry.getValue();
        table.put(key,OFFSET_COLNAME,Bytes.toBytes(checkpoint.getNextOffset()));
        table.put(key,MAX_TIME_COLNAME,Bytes.toBytes(checkpoint.getMaxEventTime()));
      }
      lastCheckpoint=ImmutableMap.copyOf(checkpoints);
    }
  }
);
  LOG.trace(""String_Node_Str"",checkpoints);
}"
6536,"private void flush(boolean force) throws Exception {
  long currentTs=System.currentTimeMillis();
  if (!force && currentTs - lastCheckpointTime < flushIntervalMs) {
    return;
  }
  avroFileWriter.flush();
  for (  Map.Entry<Integer,Checkpoint> entry : partitionCheckpointMap.entrySet()) {
    LOG.trace(""String_Node_Str"",entry.getValue(),entry.getKey());
    checkpointManager.saveCheckpoint(entry.getKey(),entry.getValue());
  }
  lastCheckpointTime=currentTs;
}","private void flush(boolean force) throws Exception {
  long currentTs=System.currentTimeMillis();
  if (!force && currentTs - lastCheckpointTime < flushIntervalMs) {
    return;
  }
  avroFileWriter.flush();
  checkpointManager.saveCheckpoint(partitionCheckpointMap);
  lastCheckpointTime=currentTs;
}"
6537,"@Override public void init(Set<Integer> partitions){
  super.init(partitions,checkpointManager);
  scheduledExecutor=MoreExecutors.listeningDecorator(Executors.newSingleThreadScheduledExecutor(Threads.createDaemonThreadFactory(""String_Node_Str"")));
  partitionCheckpoints.clear();
  try {
    for (    Integer partition : partitions) {
      partitionCheckpoints.put(partition,checkpointManager.getCheckpoint(partition));
    }
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
  checkPointWriter=new CheckPointWriter(checkpointManager,partitionCheckpoints);
  scheduledExecutor.scheduleWithFixedDelay(checkPointWriter,100,10000,TimeUnit.MILLISECONDS);
}","@Override public void init(Set<Integer> partitions){
  super.init(partitions,checkpointManager);
  scheduledExecutor=MoreExecutors.listeningDecorator(Executors.newSingleThreadScheduledExecutor(Threads.createDaemonThreadFactory(""String_Node_Str"")));
  partitionCheckpoints.clear();
  try {
    Map<Integer,Checkpoint> partitionMap=checkpointManager.getCheckpoint(partitions);
    for (    Map.Entry<Integer,Checkpoint> partition : partitionMap.entrySet()) {
      partitionCheckpoints.put(partition.getKey(),partition.getValue());
    }
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
  checkPointWriter=new CheckPointWriter(checkpointManager,partitionCheckpoints);
  scheduledExecutor.scheduleWithFixedDelay(checkPointWriter,100,500,TimeUnit.MILLISECONDS);
}"
6538,"/** 
 * Deletes meta data until a given time, while keeping the latest meta data even if less than tillTime.
 * @param tillTime time till the meta data will be deleted.
 * @param callback callback called before deleting a meta data column.
 * @return total number of columns deleted.
 */
public int cleanMetaData(final long tillTime,final DeleteCallback callback) throws Exception {
  return mds.execute(new TransactionExecutor.Function<DatasetContext<Table>,Integer>(){
    @Override public Integer apply(    DatasetContext<Table> ctx) throws Exception {
      byte[] tillTimeBytes=Bytes.toBytes(tillTime);
      int deletedColumns=0;
      Scanner scanner=ctx.get().scan(ROW_KEY_PREFIX,ROW_KEY_PREFIX_END);
      try {
        Row row;
        while ((row=scanner.next()) != null) {
          byte[] rowKey=row.getRow();
          String namespacedLogDir=LoggingContextHelper.getNamespacedBaseDir(logBaseDir,getLogPartition(rowKey));
          byte[] maxCol=getMaxKey(row.getColumns());
          for (          Map.Entry<byte[],byte[]> entry : row.getColumns().entrySet()) {
            byte[] colName=entry.getKey();
            if (LOG.isDebugEnabled()) {
              LOG.debug(""String_Node_Str"",Bytes.toString(entry.getValue()),Bytes.toLong(colName));
            }
            if (Bytes.compareTo(colName,tillTimeBytes) < 0 && Bytes.compareTo(colName,maxCol) != 0) {
              callback.handle(locationFactory.create(new URI(Bytes.toString(entry.getValue()))),namespacedLogDir);
              ctx.get().delete(rowKey,colName);
              deletedColumns++;
            }
          }
        }
      }
  finally {
        scanner.close();
      }
      return deletedColumns;
    }
  }
);
}","/** 
 * Deletes meta data until a given time, while keeping the latest meta data even if less than tillTime.
 * @param tillTime time till the meta data will be deleted.
 * @param callback callback called before deleting a meta data column.
 * @return total number of columns deleted.
 */
public int cleanMetaData(final long tillTime,final DeleteCallback callback) throws Exception {
  return execute(new TransactionExecutor.Function<Table,Integer>(){
    @Override public Integer apply(    Table table) throws Exception {
      byte[] tillTimeBytes=Bytes.toBytes(tillTime);
      int deletedColumns=0;
      Scanner scanner=table.scan(ROW_KEY_PREFIX,ROW_KEY_PREFIX_END);
      try {
        Row row;
        while ((row=scanner.next()) != null) {
          byte[] rowKey=row.getRow();
          String namespacedLogDir=LoggingContextHelper.getNamespacedBaseDir(logBaseDir,getLogPartition(rowKey));
          byte[] maxCol=getMaxKey(row.getColumns());
          for (          Map.Entry<byte[],byte[]> entry : row.getColumns().entrySet()) {
            byte[] colName=entry.getKey();
            if (LOG.isDebugEnabled()) {
              LOG.debug(""String_Node_Str"",Bytes.toString(entry.getValue()),Bytes.toLong(colName));
            }
            if (Bytes.compareTo(colName,tillTimeBytes) < 0 && Bytes.compareTo(colName,maxCol) != 0) {
              callback.handle(locationFactory.create(new URI(Bytes.toString(entry.getValue()))),namespacedLogDir);
              table.delete(rowKey,colName);
              deletedColumns++;
            }
          }
        }
      }
  finally {
        scanner.close();
      }
      return deletedColumns;
    }
  }
);
}"
6539,"@Override public Integer apply(DatasetContext<Table> ctx) throws Exception {
  byte[] tillTimeBytes=Bytes.toBytes(tillTime);
  int deletedColumns=0;
  Scanner scanner=ctx.get().scan(ROW_KEY_PREFIX,ROW_KEY_PREFIX_END);
  try {
    Row row;
    while ((row=scanner.next()) != null) {
      byte[] rowKey=row.getRow();
      String namespacedLogDir=LoggingContextHelper.getNamespacedBaseDir(logBaseDir,getLogPartition(rowKey));
      byte[] maxCol=getMaxKey(row.getColumns());
      for (      Map.Entry<byte[],byte[]> entry : row.getColumns().entrySet()) {
        byte[] colName=entry.getKey();
        if (LOG.isDebugEnabled()) {
          LOG.debug(""String_Node_Str"",Bytes.toString(entry.getValue()),Bytes.toLong(colName));
        }
        if (Bytes.compareTo(colName,tillTimeBytes) < 0 && Bytes.compareTo(colName,maxCol) != 0) {
          callback.handle(locationFactory.create(new URI(Bytes.toString(entry.getValue()))),namespacedLogDir);
          ctx.get().delete(rowKey,colName);
          deletedColumns++;
        }
      }
    }
  }
  finally {
    scanner.close();
  }
  return deletedColumns;
}","@Override public Integer apply(Table table) throws Exception {
  byte[] tillTimeBytes=Bytes.toBytes(tillTime);
  int deletedColumns=0;
  Scanner scanner=table.scan(ROW_KEY_PREFIX,ROW_KEY_PREFIX_END);
  try {
    Row row;
    while ((row=scanner.next()) != null) {
      byte[] rowKey=row.getRow();
      String namespacedLogDir=LoggingContextHelper.getNamespacedBaseDir(logBaseDir,getLogPartition(rowKey));
      byte[] maxCol=getMaxKey(row.getColumns());
      for (      Map.Entry<byte[],byte[]> entry : row.getColumns().entrySet()) {
        byte[] colName=entry.getKey();
        if (LOG.isDebugEnabled()) {
          LOG.debug(""String_Node_Str"",Bytes.toString(entry.getValue()),Bytes.toLong(colName));
        }
        if (Bytes.compareTo(colName,tillTimeBytes) < 0 && Bytes.compareTo(colName,maxCol) != 0) {
          callback.handle(locationFactory.create(new URI(Bytes.toString(entry.getValue()))),namespacedLogDir);
          table.delete(rowKey,colName);
          deletedColumns++;
        }
      }
    }
  }
  finally {
    scanner.close();
  }
  return deletedColumns;
}"
6540,"@Inject public FileMetaDataManager(final LogSaverTableUtil tableUtil,TransactionExecutorFactory txExecutorFactory,LocationFactory locationFactory,CConfiguration cConf){
  this.mds=Transactional.of(txExecutorFactory,new Supplier<DatasetContext<Table>>(){
    @Override public DatasetContext<Table> get(){
      try {
        return DatasetContext.of(tableUtil.getMetaTable());
      }
 catch (      Exception e) {
        throw Throwables.propagate(e);
      }
    }
  }
);
  this.locationFactory=locationFactory;
  this.logBaseDir=cConf.get(LoggingConfiguration.LOG_BASE_DIR);
}","@Inject public FileMetaDataManager(final LogSaverTableUtil tableUtil,TransactionExecutorFactory txExecutorFactory,LocationFactory locationFactory,CConfiguration cConf){
  this.tableUtil=tableUtil;
  this.transactionExecutorFactory=txExecutorFactory;
  this.locationFactory=locationFactory;
  this.logBaseDir=cConf.get(LoggingConfiguration.LOG_BASE_DIR);
}"
6541,"/** 
 * Returns a list of log files for a logging context.
 * @param loggingContext logging context.
 * @return Sorted map containing key as start time, and value as log file.
 */
public NavigableMap<Long,Location> listFiles(final LoggingContext loggingContext) throws Exception {
  return mds.execute(new TransactionExecutor.Function<DatasetContext<Table>,NavigableMap<Long,Location>>(){
    @Override public NavigableMap<Long,Location> apply(    DatasetContext<Table> ctx) throws Exception {
      Row cols=ctx.get().get(getRowKey(loggingContext));
      if (cols.isEmpty()) {
        return (NavigableMap<Long,Location>)EMPTY_MAP;
      }
      NavigableMap<Long,Location> files=new TreeMap<>();
      for (      Map.Entry<byte[],byte[]> entry : cols.getColumns().entrySet()) {
        files.put(Bytes.toLong(entry.getKey()),locationFactory.create(new URI(Bytes.toString(entry.getValue()))));
      }
      return files;
    }
  }
);
}","/** 
 * Returns a list of log files for a logging context.
 * @param loggingContext logging context.
 * @return Sorted map containing key as start time, and value as log file.
 */
public NavigableMap<Long,Location> listFiles(final LoggingContext loggingContext) throws Exception {
  return execute(new TransactionExecutor.Function<Table,NavigableMap<Long,Location>>(){
    @Override public NavigableMap<Long,Location> apply(    Table table) throws Exception {
      Row cols=table.get(getRowKey(loggingContext));
      if (cols.isEmpty()) {
        return (NavigableMap<Long,Location>)EMPTY_MAP;
      }
      NavigableMap<Long,Location> files=new TreeMap<>();
      for (      Map.Entry<byte[],byte[]> entry : cols.getColumns().entrySet()) {
        files.put(Bytes.toLong(entry.getKey()),locationFactory.create(new URI(Bytes.toString(entry.getValue()))));
      }
      return files;
    }
  }
);
}"
6542,"/** 
 * Persists meta data associated with a log file.
 * @param logPartition partition name that is used to group log messages
 * @param startTimeMs start log time associated with the file.
 * @param location log file.
 */
private void writeMetaData(final String logPartition,final long startTimeMs,final Location location) throws Exception {
  LOG.debug(""String_Node_Str"",logPartition,startTimeMs,location.toURI());
  mds.execute(new TransactionExecutor.Function<DatasetContext<Table>,Void>(){
    @Override public Void apply(    DatasetContext<Table> ctx) throws Exception {
      ctx.get().put(getRowKey(logPartition),Bytes.toBytes(startTimeMs),Bytes.toBytes(location.toURI().toString()));
      return null;
    }
  }
);
}","/** 
 * Persists meta data associated with a log file.
 * @param logPartition partition name that is used to group log messages
 * @param startTimeMs start log time associated with the file.
 * @param location log file.
 */
private void writeMetaData(final String logPartition,final long startTimeMs,final Location location) throws Exception {
  LOG.debug(""String_Node_Str"",logPartition,startTimeMs,location.toURI());
  execute(new TransactionExecutor.Procedure<Table>(){
    @Override public void apply(    Table table) throws Exception {
      table.put(getRowKey(logPartition),Bytes.toBytes(startTimeMs),Bytes.toBytes(location.toURI().toString()));
    }
  }
);
}"
6543,"private static void generateCheckpointTime(LoggingContext loggingContext,int numExpectedEvents) throws Exception {
  FileLogReader logReader=injector.getInstance(FileLogReader.class);
  LoggingTester.LogCallback logCallback=new LoggingTester.LogCallback();
  logReader.getLog(loggingContext,0,Long.MAX_VALUE,Filter.EMPTY_FILTER,logCallback);
  Assert.assertEquals(numExpectedEvents,logCallback.getEvents().size());
  CheckpointManagerFactory checkpointManagerFactory=injector.getInstance(CheckpointManagerFactory.class);
  CheckpointManager checkpointManager=checkpointManagerFactory.create(KafkaTopic.getTopic(),KafkaLogWriterPlugin.CHECKPOINT_ROW_KEY_PREFIX);
  long checkpointTime=logCallback.getEvents().get(numExpectedEvents - 1).getLoggingEvent().getTimeStamp();
  checkpointManager.saveCheckpoint(stringPartitioner.partition(loggingContext.getLogPartition(),-1),new Checkpoint(numExpectedEvents,checkpointTime));
}","private static void generateCheckpointTime(LoggingContext loggingContext,int numExpectedEvents) throws Exception {
  FileLogReader logReader=injector.getInstance(FileLogReader.class);
  LoggingTester.LogCallback logCallback=new LoggingTester.LogCallback();
  logReader.getLog(loggingContext,0,Long.MAX_VALUE,Filter.EMPTY_FILTER,logCallback);
  Assert.assertEquals(numExpectedEvents,logCallback.getEvents().size());
  CheckpointManagerFactory checkpointManagerFactory=injector.getInstance(CheckpointManagerFactory.class);
  CheckpointManager checkpointManager=checkpointManagerFactory.create(KafkaTopic.getTopic(),KafkaLogWriterPlugin.CHECKPOINT_ROW_KEY_PREFIX);
  long checkpointTime=logCallback.getEvents().get(numExpectedEvents - 1).getLoggingEvent().getTimeStamp();
  checkpointManager.saveCheckpoint(ImmutableMap.of(stringPartitioner.partition(loggingContext.getLogPartition(),-1),new Checkpoint(numExpectedEvents,checkpointTime)));
}"
6544,"private void resetLogSaverPluginCheckpoint() throws Exception {
  TypeLiteral<Set<KafkaLogProcessor>> type=new TypeLiteral<Set<KafkaLogProcessor>>(){
  }
;
  Set<KafkaLogProcessor> processors=injector.getInstance(Key.get(type,Names.named(Constants.LogSaver.MESSAGE_PROCESSORS)));
  for (  KafkaLogProcessor processor : processors) {
    if (processor instanceof KafkaLogWriterPlugin) {
      KafkaLogWriterPlugin plugin=(KafkaLogWriterPlugin)processor;
      CheckpointManager manager=plugin.getCheckPointManager();
      manager.saveCheckpoint(0,new Checkpoint(10,-1));
      Set<Integer> partitions=Sets.newHashSet(0,1);
      plugin.init(partitions);
    }
  }
}","private void resetLogSaverPluginCheckpoint() throws Exception {
  TypeLiteral<Set<KafkaLogProcessor>> type=new TypeLiteral<Set<KafkaLogProcessor>>(){
  }
;
  Set<KafkaLogProcessor> processors=injector.getInstance(Key.get(type,Names.named(Constants.LogSaver.MESSAGE_PROCESSORS)));
  for (  KafkaLogProcessor processor : processors) {
    if (processor instanceof KafkaLogWriterPlugin) {
      KafkaLogWriterPlugin plugin=(KafkaLogWriterPlugin)processor;
      CheckpointManager manager=plugin.getCheckPointManager();
      manager.saveCheckpoint(ImmutableMap.of(0,new Checkpoint(10,-1)));
      Set<Integer> partitions=Sets.newHashSet(0,1);
      plugin.init(partitions);
    }
  }
}"
6545,"private Schema.Type getNonNullableType(Schema.Field field){
  Schema.Type type;
  if (field.getSchema().isNullable()) {
    type=field.getSchema().getNonNullable().getType();
  }
 else {
    type=field.getSchema().getType();
  }
  Preconditions.checkArgument(type.isSimpleType(),""String_Node_Str"");
  return type;
}","private Schema.Type getNonNullableType(Schema.Field field){
  Schema.Type type;
  if (field.getSchema().isNullable()) {
    type=field.getSchema().getNonNullable().getType();
  }
 else {
    type=field.getSchema().getType();
  }
  Preconditions.checkArgument(type.isSimpleType(),""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",type);
  return type;
}"
6546,"@Test public void testStreamSizeSchedule() throws Exception {
  AppFabricTestHelper.deployApplication(AppWithStreamSizeSchedule.class);
  Assert.assertEquals(Scheduler.ScheduleState.SUSPENDED,streamSizeScheduler.scheduleState(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1));
  Assert.assertEquals(Scheduler.ScheduleState.SUSPENDED,streamSizeScheduler.scheduleState(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2));
  streamSizeScheduler.resumeSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1);
  streamSizeScheduler.resumeSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  Assert.assertEquals(Scheduler.ScheduleState.SCHEDULED,streamSizeScheduler.scheduleState(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1));
  Assert.assertEquals(Scheduler.ScheduleState.SCHEDULED,streamSizeScheduler.scheduleState(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2));
  int runs=store.getRuns(PROGRAM_ID,ProgramRunStatus.ALL,0,Long.MAX_VALUE,100).size();
  Assert.assertEquals(0,runs);
  StreamMetricsPublisher metricsPublisher=createMetricsPublisher(STREAM_ID);
  metricsPublisher.increment(1024 * 1024);
  waitForRuns(store,PROGRAM_ID,1,5);
  waitUntilFinished(runtimeService,PROGRAM_ID,5);
  metricsPublisher.increment(1024 * 1024);
  waitForRuns(store,PROGRAM_ID,3,5);
  streamSizeScheduler.suspendSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  streamSizeScheduler.suspendSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  Assert.assertEquals(Scheduler.ScheduleState.SUSPENDED,streamSizeScheduler.scheduleState(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2));
  metricsPublisher.increment(1024 * 1024);
  waitForRuns(store,PROGRAM_ID,4,5);
  streamSizeScheduler.resumeSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  streamSizeScheduler.resumeSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  Assert.assertEquals(Scheduler.ScheduleState.SCHEDULED,streamSizeScheduler.scheduleState(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2));
  metricsPublisher.increment(1024 * 1024);
  waitForRuns(store,PROGRAM_ID,6,5);
  streamSizeScheduler.updateSchedule(PROGRAM_ID,PROGRAM_TYPE,UPDATE_SCHEDULE_2);
  metricsPublisher.increment(1024 * 1024);
  waitForRuns(store,PROGRAM_ID,8,5);
  streamSizeScheduler.suspendSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1);
  streamSizeScheduler.suspendSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  streamSizeScheduler.deleteSchedules(PROGRAM_ID,PROGRAM_TYPE);
  waitUntilFinished(runtimeService,PROGRAM_ID,10);
}","@Test public void testStreamSizeSchedule() throws Exception {
  AppFabricTestHelper.deployApplication(AppWithStreamSizeSchedule.class);
  Assert.assertEquals(Scheduler.ScheduleState.SUSPENDED,streamSizeScheduler.scheduleState(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1));
  Assert.assertEquals(Scheduler.ScheduleState.SUSPENDED,streamSizeScheduler.scheduleState(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2));
  streamSizeScheduler.resumeSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1);
  streamSizeScheduler.resumeSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  Assert.assertEquals(Scheduler.ScheduleState.SCHEDULED,streamSizeScheduler.scheduleState(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1));
  Assert.assertEquals(Scheduler.ScheduleState.SCHEDULED,streamSizeScheduler.scheduleState(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2));
  int runs=store.getRuns(PROGRAM_ID,ProgramRunStatus.ALL,0,Long.MAX_VALUE,100).size();
  Assert.assertEquals(0,runs);
  StreamMetricsPublisher metricsPublisher=createMetricsPublisher(STREAM_ID);
  metricsPublisher.increment(1024 * 1024);
  waitForRuns(store,PROGRAM_ID,1,15);
  waitUntilFinished(runtimeService,PROGRAM_ID,15);
  metricsPublisher.increment(1024 * 1024);
  waitForRuns(store,PROGRAM_ID,3,15);
  streamSizeScheduler.suspendSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  streamSizeScheduler.suspendSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  Assert.assertEquals(Scheduler.ScheduleState.SUSPENDED,streamSizeScheduler.scheduleState(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2));
  metricsPublisher.increment(1024 * 1024);
  waitForRuns(store,PROGRAM_ID,4,15);
  streamSizeScheduler.resumeSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  streamSizeScheduler.resumeSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  Assert.assertEquals(Scheduler.ScheduleState.SCHEDULED,streamSizeScheduler.scheduleState(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2));
  metricsPublisher.increment(1024 * 1024);
  waitForRuns(store,PROGRAM_ID,6,15);
  streamSizeScheduler.updateSchedule(PROGRAM_ID,PROGRAM_TYPE,UPDATE_SCHEDULE_2);
  metricsPublisher.increment(1024 * 1024);
  waitForRuns(store,PROGRAM_ID,8,15);
  streamSizeScheduler.suspendSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_1);
  streamSizeScheduler.suspendSchedule(PROGRAM_ID,PROGRAM_TYPE,SCHEDULE_NAME_2);
  streamSizeScheduler.deleteSchedules(PROGRAM_ID,PROGRAM_TYPE);
  waitUntilFinished(runtimeService,PROGRAM_ID,10);
}"
6547,"private void recordCompletedWorkflow(final Id.Workflow id,String pid){
  final RunRecordMeta run=getRun(id,pid);
  if (run == null) {
    return;
  }
  Id.Application app=id.getApplication();
  ApplicationSpecification appSpec=getApplication(app);
  if (appSpec == null || appSpec.getWorkflows() == null || appSpec.getWorkflows().get(id.getId()) == null) {
    return;
  }
  WorkflowSpecification workflowSpec=appSpec.getWorkflows().get(id.getId());
  Map<String,WorkflowNode> nodeIdMap=workflowSpec.getNodeIdMap();
  final List<WorkflowDataset.ProgramRun> programRunsList=new ArrayList<>();
  for (  Map.Entry<String,String> entry : run.getProperties().entrySet()) {
    if (!(""String_Node_Str"".equals(entry.getKey()) || ""String_Node_Str"".equals(entry.getKey()))) {
      WorkflowActionNode workflowNode=(WorkflowActionNode)nodeIdMap.get(entry.getKey());
      ProgramType programType=ProgramType.valueOfSchedulableType(workflowNode.getProgram().getProgramType());
      Id.Program innerProgram=Id.Program.from(app.getNamespaceId(),app.getId(),programType,entry.getKey());
      RunRecordMeta innerProgramRun=getRun(innerProgram,entry.getValue());
      programRunsList.add(new WorkflowDataset.ProgramRun(entry.getKey(),entry.getValue(),programType,innerProgramRun.getStopTs() - innerProgramRun.getStartTs()));
    }
  }
  txnlWorkflow.executeUnchecked(new TransactionExecutor.Function<WorkflowStatsDataset,Void>(){
    @Override public Void apply(    WorkflowStatsDataset dataset){
      dataset.workflowDataset.write(id,run,programRunsList);
      return null;
    }
  }
);
}","private void recordCompletedWorkflow(final Id.Workflow id,String pid){
  final RunRecordMeta run=getRun(id,pid);
  if (run == null) {
    return;
  }
  Id.Application app=id.getApplication();
  ApplicationSpecification appSpec=getApplication(app);
  if (appSpec == null || appSpec.getWorkflows() == null || appSpec.getWorkflows().get(id.getId()) == null) {
    return;
  }
  boolean workFlowNodeFailed=false;
  WorkflowSpecification workflowSpec=appSpec.getWorkflows().get(id.getId());
  Map<String,WorkflowNode> nodeIdMap=workflowSpec.getNodeIdMap();
  final List<WorkflowDataset.ProgramRun> programRunsList=new ArrayList<>();
  for (  Map.Entry<String,String> entry : run.getProperties().entrySet()) {
    if (!(""String_Node_Str"".equals(entry.getKey()) || ""String_Node_Str"".equals(entry.getKey()))) {
      WorkflowActionNode workflowNode=(WorkflowActionNode)nodeIdMap.get(entry.getKey());
      ProgramType programType=ProgramType.valueOfSchedulableType(workflowNode.getProgram().getProgramType());
      Id.Program innerProgram=Id.Program.from(app.getNamespaceId(),app.getId(),programType,entry.getKey());
      RunRecordMeta innerProgramRun=getRun(innerProgram,entry.getValue());
      if (innerProgramRun.getStatus().equals(ProgramRunStatus.COMPLETED)) {
        programRunsList.add(new WorkflowDataset.ProgramRun(entry.getKey(),entry.getValue(),programType,innerProgramRun.getStopTs() - innerProgramRun.getStartTs()));
      }
 else {
        workFlowNodeFailed=true;
        break;
      }
    }
  }
  if (workFlowNodeFailed) {
    return;
  }
  txnlWorkflow.executeUnchecked(new TransactionExecutor.Function<WorkflowStatsDataset,Void>(){
    @Override public Void apply(    WorkflowStatsDataset dataset){
      dataset.workflowDataset.write(id,run,programRunsList);
      return null;
    }
  }
);
}"
6548,"@Inject LineageHandler(LineageGenerator lineageGenerator,LineageStore lineageStore){
  this.lineageGenerator=lineageGenerator;
  this.lineageStore=lineageStore;
}","@Inject LineageHandler(LineageAdmin lineageAdmin){
  this.lineageAdmin=lineageAdmin;
}"
6549,"@GET @Path(""String_Node_Str"") public void streamLineage(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream,@QueryParam(""String_Node_Str"") String startStr,@QueryParam(""String_Node_Str"") String endStr,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int levels) throws Exception {
  checkLevels(levels);
  TimeRange range=parseRange(startStr,endStr);
  Id.Stream streamId=Id.Stream.from(namespaceId,stream);
  Lineage lineage=lineageGenerator.computeLineage(streamId,range.getStart(),range.getEnd(),levels);
  responder.sendJson(HttpResponseStatus.OK,new LineageRecord(range.getStart(),range.getEnd(),lineage.getRelations()),LineageRecord.class,GSON);
}","@GET @Path(""String_Node_Str"") public void streamLineage(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream,@QueryParam(""String_Node_Str"") String startStr,@QueryParam(""String_Node_Str"") String endStr,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int levels) throws Exception {
  checkLevels(levels);
  TimeRange range=parseRange(startStr,endStr);
  Id.Stream streamId=Id.Stream.from(namespaceId,stream);
  Lineage lineage=lineageAdmin.computeLineage(streamId,range.getStart(),range.getEnd(),levels);
  responder.sendJson(HttpResponseStatus.OK,new LineageRecord(TimeUnit.MILLISECONDS.toSeconds(range.getStart()),TimeUnit.MILLISECONDS.toSeconds(range.getEnd()),lineage.getRelations()),LineageRecord.class,GSON);
}"
6550,"@GET @Path(""String_Node_Str"") public void getAccessesForRun(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId,@PathParam(""String_Node_Str"") String runId) throws Exception {
  Id.Run run=new Id.Run(Id.Program.from(namespaceId,appId,ProgramType.valueOfCategoryName(programType),programId),runId);
  responder.sendJson(HttpResponseStatus.OK,lineageStore.getRunMetadata(run),SET_METADATA_RECORD_TYPE,GSON);
}","@GET @Path(""String_Node_Str"") public void getAccessesForRun(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId,@PathParam(""String_Node_Str"") String runId) throws Exception {
  Id.Run run=new Id.Run(Id.Program.from(namespaceId,appId,ProgramType.valueOfCategoryName(programType),programId),runId);
  responder.sendJson(HttpResponseStatus.OK,lineageAdmin.getMetadataForRun(run),SET_METADATA_RECORD_TYPE,GSON);
}"
6551,"@GET @Path(""String_Node_Str"") public void datasetLineage(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String datasetId,@QueryParam(""String_Node_Str"") String startStr,@QueryParam(""String_Node_Str"") String endStr,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int levels) throws Exception {
  checkLevels(levels);
  TimeRange range=parseRange(startStr,endStr);
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(namespaceId,datasetId);
  Lineage lineage=lineageGenerator.computeLineage(datasetInstance,range.getStart(),range.getEnd(),levels);
  responder.sendJson(HttpResponseStatus.OK,new LineageRecord(range.getStart(),range.getEnd(),lineage.getRelations()),LineageRecord.class,GSON);
}","@GET @Path(""String_Node_Str"") public void datasetLineage(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String datasetId,@QueryParam(""String_Node_Str"") String startStr,@QueryParam(""String_Node_Str"") String endStr,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int levels) throws Exception {
  checkLevels(levels);
  TimeRange range=parseRange(startStr,endStr);
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(namespaceId,datasetId);
  Lineage lineage=lineageAdmin.computeLineage(datasetInstance,range.getStart(),range.getEnd(),levels);
  responder.sendJson(HttpResponseStatus.OK,new LineageRecord(TimeUnit.MILLISECONDS.toSeconds(range.getStart()),TimeUnit.MILLISECONDS.toSeconds(range.getEnd()),lineage.getRelations()),LineageRecord.class,GSON);
}"
6552,"/** 
 * Scan all files in the local system artifact directory, looking for jar files and adding them as system artifacts. If the artifact already exists it will not be added again unless it is a snapshot version.
 * @throws IOException if there was some IO error adding the system artifacts
 * @throws WriteConflictException if there was a write conflicting adding the system artifact. This shouldn't happen,but if it does, it should be ok to retry the operation.
 */
public void addSystemArtifacts() throws IOException, WriteConflictException {
  List<ArtifactConfig> systemArtifacts=new ArrayList<>();
  for (  File jarFile : DirUtils.listFiles(systemArtifactDir,""String_Node_Str"")) {
    Id.Artifact artifactId;
    try {
      artifactId=Id.Artifact.parse(Id.Namespace.SYSTEM,jarFile.getName());
    }
 catch (    IllegalArgumentException e) {
      LOG.warn(String.format(""String_Node_Str"",e.getMessage()));
      continue;
    }
    String artifactFileName=jarFile.getName();
    String configFileName=artifactFileName.substring(0,artifactFileName.length() - ""String_Node_Str"".length()) + ""String_Node_Str"";
    File configFile=new File(systemArtifactDir,configFileName);
    try {
      ArtifactConfig artifactConfig=configFile.isFile() ? ArtifactConfig.read(artifactId,configFile,jarFile) : ArtifactConfig.builder(artifactId,jarFile).build();
      validateParentSet(artifactId,artifactConfig.getParents());
      validatePluginSet(artifactConfig.getPlugins());
      systemArtifacts.add(artifactConfig);
    }
 catch (    InvalidArtifactException e) {
      LOG.warn(String.format(""String_Node_Str"",artifactFileName),e);
    }
  }
  Collections.sort(systemArtifacts);
  for (  ArtifactConfig artifactConfig : systemArtifacts) {
    String fileName=artifactConfig.getFile().getName();
    try {
      Id.Artifact artifactId=artifactConfig.getArtifactId();
      if (!artifactId.getVersion().isSnapshot()) {
        try {
          artifactStore.getArtifact(artifactId);
          continue;
        }
 catch (        ArtifactNotFoundException e) {
        }
      }
      addArtifact(artifactId,artifactConfig.getFile(),artifactConfig.getParents(),artifactConfig.getPlugins());
    }
 catch (    ArtifactAlreadyExistsException e) {
    }
catch (    ArtifactRangeNotFoundException e) {
      LOG.warn(String.format(""String_Node_Str"",fileName),e);
    }
catch (    InvalidArtifactException e) {
      LOG.warn(String.format(""String_Node_Str"",fileName),e);
    }
  }
}","/** 
 * Scan all files in the local system artifact directory, looking for jar files and adding them as system artifacts. If the artifact already exists it will not be added again unless it is a snapshot version.
 * @throws IOException if there was some IO error adding the system artifacts
 * @throws WriteConflictException if there was a write conflicting adding the system artifact. This shouldn't happen,but if it does, it should be ok to retry the operation.
 */
public void addSystemArtifacts() throws IOException, WriteConflictException {
  List<ArtifactConfig> systemArtifacts=new ArrayList<>();
  for (  File jarFile : DirUtils.listFiles(systemArtifactDir,""String_Node_Str"")) {
    Id.Artifact artifactId;
    try {
      artifactId=Id.Artifact.parse(Id.Namespace.SYSTEM,jarFile.getName());
    }
 catch (    IllegalArgumentException e) {
      LOG.warn(String.format(""String_Node_Str"",e.getMessage()));
      continue;
    }
    String artifactFileName=jarFile.getName();
    String configFileName=artifactFileName.substring(0,artifactFileName.length() - ""String_Node_Str"".length()) + ""String_Node_Str"";
    File configFile=new File(systemArtifactDir,configFileName);
    try {
      ArtifactConfig artifactConfig=configFile.isFile() ? ArtifactConfig.read(artifactId,configFile,jarFile) : ArtifactConfig.builder(artifactId,jarFile).build();
      validateParentSet(artifactId,artifactConfig.getParents());
      validatePluginSet(artifactConfig.getPlugins());
      systemArtifacts.add(artifactConfig);
    }
 catch (    InvalidArtifactException e) {
      LOG.warn(String.format(""String_Node_Str"",artifactFileName),e);
    }
  }
  Collections.sort(systemArtifacts);
  Set<Id.Artifact> parents=new HashSet<>();
  for (  ArtifactConfig child : systemArtifacts) {
    Id.Artifact childId=child.getArtifactId();
    for (    ArtifactConfig potentialParent : systemArtifacts) {
      Id.Artifact potentialParentId=potentialParent.getArtifactId();
      if (childId.equals(potentialParentId)) {
        continue;
      }
      if (child.hasParent(potentialParentId)) {
        parents.add(potentialParentId);
      }
    }
  }
  for (  ArtifactConfig config : systemArtifacts) {
    if (parents.contains(config.getArtifactId())) {
      addSystemArtifact(config);
    }
  }
  for (  ArtifactConfig config : systemArtifacts) {
    if (!parents.contains(config.getArtifactId())) {
      addSystemArtifact(config);
    }
  }
}"
6553,"List<BusinessMetadataRecord> executeSearchOnColumns(String column,String searchValue,MetadataSearchTargetType type){
  List<BusinessMetadataRecord> results=new LinkedList<>();
  Scanner scanner;
  String lowerCaseSearchValue=searchValue.toLowerCase();
  if (lowerCaseSearchValue.endsWith(""String_Node_Str"")) {
    byte[] startKey=Bytes.toBytes(lowerCaseSearchValue.substring(0,lowerCaseSearchValue.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    scanner=indexedTable.scanByIndex(Bytes.toBytes(column),startKey,stopKey);
  }
 else {
    byte[] value=Bytes.toBytes(lowerCaseSearchValue);
    scanner=indexedTable.readByIndex(Bytes.toBytes(column),value);
  }
  try {
    Row next;
    while ((next=scanner.next()) != null) {
      String rowValue=next.getString(VALUE_COLUMN);
      if (rowValue == null) {
        continue;
      }
      final byte[] rowKey=next.getRow();
      String targetType=getTargetType(rowKey);
      if ((type != MetadataSearchTargetType.ALL) && (!targetType.equals(type.getInternalName()))) {
        continue;
      }
      Id.NamespacedId targetId=getNamespaceIdFromKey(targetType,new MDSKey(rowKey));
      String key=getMetadataKey(targetType,rowKey);
      String value=Bytes.toString(next.get(Bytes.toBytes(BusinessMetadataDataset.VALUE_COLUMN)));
      BusinessMetadataRecord record=new BusinessMetadataRecord(targetId,key,value);
      results.add(record);
    }
  }
  finally {
    scanner.close();
  }
  return results;
}","List<BusinessMetadataRecord> executeSearchOnColumns(String column,String searchValue,MetadataSearchTargetType type){
  List<BusinessMetadataRecord> results=new LinkedList<>();
  Scanner scanner;
  String lowerCaseSearchValue=searchValue.toLowerCase();
  if (lowerCaseSearchValue.endsWith(""String_Node_Str"")) {
    byte[] startKey=Bytes.toBytes(lowerCaseSearchValue.substring(0,lowerCaseSearchValue.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    scanner=indexedTable.scanByIndex(Bytes.toBytes(column),startKey,stopKey);
  }
 else {
    byte[] value=Bytes.toBytes(lowerCaseSearchValue);
    scanner=indexedTable.readByIndex(Bytes.toBytes(column),value);
  }
  try {
    Row next;
    while ((next=scanner.next()) != null) {
      String rowValue=next.getString(VALUE_COLUMN);
      if (rowValue == null) {
        continue;
      }
      final byte[] rowKey=next.getRow();
      String targetType=MdsValueKey.getTargetType(rowKey);
      if ((type != MetadataSearchTargetType.ALL) && (!targetType.equals(type.getInternalName()))) {
        continue;
      }
      Id.NamespacedId targetId=MdsValueKey.getNamespaceIdFromKey(targetType,new MDSKey(rowKey));
      String key=MdsValueKey.getMetadataKey(targetType,rowKey);
      String value=Bytes.toString(next.get(Bytes.toBytes(BusinessMetadataDataset.VALUE_COLUMN)));
      BusinessMetadataRecord record=new BusinessMetadataRecord(targetId,key,value);
      results.add(record);
    }
  }
  finally {
    scanner.close();
  }
  return results;
}"
6554,"private void MetadataType(String serializedForm){
  this.serializedForm=serializedForm;
}","void MetadataType(String serializedForm){
  this.serializedForm=serializedForm;
}"
6555,"/** 
 * Retrieves the business metadata for the specified   {@link Id.NamespacedId}.
 * @param targetId the specified {@link Id.NamespacedId}
 * @param metadataType {@link MetadataType} indicating the type of metadata to retrieve - property or tag
 * @return a Map representing the metadata for the specified {@link Id.NamespacedId}
 */
private Map<String,String> getBusinessMetadata(Id.NamespacedId targetId,MetadataType metadataType){
  String targetType=getTargetType(targetId);
  MDSKey mdsKey=getMDSKey(targetId,metadataType,null);
  byte[] startKey=mdsKey.getKey();
  byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
  Map<String,String> metadata=new HashMap<>();
  Scanner scan=indexedTable.scan(startKey,stopKey);
  try {
    Row next;
    while ((next=scan.next()) != null) {
      String key=getMetadataKey(targetType,next.getRow());
      byte[] value=next.get(VALUE_COLUMN);
      if (key == null || value == null) {
        continue;
      }
      metadata.put(key,Bytes.toString(value));
    }
    return metadata;
  }
  finally {
    scan.close();
  }
}","/** 
 * Retrieves the business metadata for the specified   {@link Id.NamespacedId}.
 * @param targetId the specified {@link Id.NamespacedId}
 * @param metadataType {@link MetadataType} indicating the type of metadata to retrieve - property or tag
 * @return a Map representing the metadata for the specified {@link Id.NamespacedId}
 */
private Map<String,String> getBusinessMetadata(Id.NamespacedId targetId,MetadataType metadataType){
  String targetType=KeyHelper.getTargetType(targetId);
  MDSKey mdsKey=MdsValueKey.getMDSKey(targetId,metadataType,null);
  byte[] startKey=mdsKey.getKey();
  byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
  Map<String,String> metadata=new HashMap<>();
  Scanner scan=indexedTable.scan(startKey,stopKey);
  try {
    Row next;
    while ((next=scan.next()) != null) {
      String key=MdsValueKey.getMetadataKey(targetType,next.getRow());
      byte[] value=next.get(VALUE_COLUMN);
      if (key == null || value == null) {
        continue;
      }
      metadata.put(key,Bytes.toString(value));
    }
    return metadata;
  }
  finally {
    scan.close();
  }
}"
6556,"/** 
 * Add new business metadata.
 * @param metadataRecord The value of the metadata to be saved.
 * @param metadataType {@link MetadataType} indicating the type of metadata - property or tag
 */
private void setBusinessMetadata(BusinessMetadataRecord metadataRecord,MetadataType metadataType){
  Id.NamespacedId targetId=metadataRecord.getTargetId();
  String key=metadataRecord.getKey();
  MDSKey mdsKey=getMDSKey(targetId,metadataType,key);
  write(mdsKey,metadataRecord);
}","/** 
 * Add new business metadata.
 * @param metadataRecord The value of the metadata to be saved.
 * @param metadataType {@link MetadataType} indicating the type of metadata - property or tag
 */
private void setBusinessMetadata(BusinessMetadataRecord metadataRecord,MetadataType metadataType){
  Id.NamespacedId targetId=metadataRecord.getTargetId();
  write(targetId,metadataType,metadataRecord);
}"
6557,"/** 
 * Removes all keys that satisfy a given predicate from the metadata of the specified   {@link Id.NamespacedId}.
 * @param targetId the {@link Id.NamespacedId} for which keys are to be removed
 * @param metadataType {@link MetadataType} indicating the type of metadata to remove - property or tag
 * @param filter the {@link Predicate} that should be satisfied to remove a key
 */
private void removeMetadata(Id.NamespacedId targetId,MetadataType metadataType,Predicate<String> filter){
  String targetType=getTargetType(targetId);
  MDSKey mdsKey=getMDSKey(targetId,metadataType,null);
  byte[] prefix=mdsKey.getKey();
  byte[] stopKey=Bytes.stopKeyForPrefix(prefix);
  Scanner scan=indexedTable.scan(prefix,stopKey);
  try {
    Row next;
    while ((next=scan.next()) != null) {
      String keyValue=next.getString(KEYVALUE_COLUMN);
      String value=next.getString(VALUE_COLUMN);
      if (keyValue == null && value == null) {
        continue;
      }
      if (filter.apply(getMetadataKey(targetType,next.getRow()))) {
        indexedTable.delete(new Delete(next.getRow()));
      }
    }
  }
  finally {
    scan.close();
  }
}","/** 
 * Removes all keys that satisfy a given predicate from the metadata of the specified   {@link Id.NamespacedId}.
 * @param targetId the {@link Id.NamespacedId} for which keys are to be removed
 * @param metadataType {@link MetadataType} indicating the type of metadata to remove - property or tag
 * @param filter the {@link Predicate} that should be satisfied to remove a key
 */
private void removeMetadata(Id.NamespacedId targetId,MetadataType metadataType,Predicate<String> filter){
  String targetType=KeyHelper.getTargetType(targetId);
  MDSKey mdsKey=MdsValueKey.getMDSKey(targetId,metadataType,null);
  byte[] prefix=mdsKey.getKey();
  byte[] stopKey=Bytes.stopKeyForPrefix(prefix);
  Scanner scan=indexedTable.scan(prefix,stopKey);
  try {
    Row next;
    while ((next=scan.next()) != null) {
      String keyValue=next.getString(KEYVALUE_COLUMN);
      String value=next.getString(VALUE_COLUMN);
      if (keyValue == null && value == null) {
        continue;
      }
      if (filter.apply(MdsValueKey.getMetadataKey(targetType,next.getRow()))) {
        indexedTable.delete(new Delete(next.getRow()));
      }
    }
  }
  finally {
    scan.close();
  }
  writeHistory(targetId);
}"
6558,"private void write(MDSKey id,BusinessMetadataRecord record){
  Put put=new Put(id.getKey());
  put.add(Bytes.toBytes(KEYVALUE_COLUMN),Bytes.toBytes(record.getKey().toLowerCase() + KEYVALUE_SEPARATOR + record.getValue().toLowerCase()));
  put.add(Bytes.toBytes(CASE_INSENSITIVE_VALUE_COLUMN),Bytes.toBytes(record.getValue().toLowerCase()));
  put.add(Bytes.toBytes(VALUE_COLUMN),Bytes.toBytes(record.getValue()));
  indexedTable.put(put);
}","private void write(Id.NamespacedId targetId,MetadataType metadataType,BusinessMetadataRecord record){
  String key=record.getKey();
  MDSKey mdsKey=MdsValueKey.getMDSKey(targetId,metadataType,key);
  Put put=new Put(mdsKey.getKey());
  put.add(Bytes.toBytes(KEYVALUE_COLUMN),Bytes.toBytes(record.getKey().toLowerCase() + KEYVALUE_SEPARATOR + record.getValue().toLowerCase()));
  put.add(Bytes.toBytes(CASE_INSENSITIVE_VALUE_COLUMN),Bytes.toBytes(record.getValue().toLowerCase()));
  put.add(Bytes.toBytes(VALUE_COLUMN),Bytes.toBytes(record.getValue()));
  indexedTable.put(put);
  writeHistory(targetId);
}"
6559,"@Test public void testTags() throws IOException {
  Assert.assertEquals(400,addTags(application).getResponseCode());
  Set<String> appTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(application,appTags).getResponseCode());
  Assert.assertEquals(400,addTags(pingService).getResponseCode());
  Set<String> serviceTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(pingService,serviceTags).getResponseCode());
  Assert.assertEquals(400,addTags(myds).getResponseCode());
  Set<String> datasetTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(myds,datasetTags).getResponseCode());
  Assert.assertEquals(400,addTags(mystream).getResponseCode());
  Set<String> streamTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(mystream,streamTags).getResponseCode());
  Set<String> tags=getTags(application);
  Assert.assertTrue(tags.containsAll(appTags));
  Assert.assertTrue(appTags.containsAll(tags));
  tags=getTags(pingService);
  Assert.assertTrue(tags.containsAll(serviceTags));
  Assert.assertTrue(serviceTags.containsAll(tags));
  tags=getTags(myds);
  Assert.assertTrue(tags.containsAll(datasetTags));
  Assert.assertTrue(datasetTags.containsAll(tags));
  tags=getTags(mystream);
  Assert.assertTrue(tags.containsAll(streamTags));
  Assert.assertTrue(streamTags.containsAll(tags));
  Set<MetadataSearchResultRecord> searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM));
  Assert.assertEquals(expected,searchTags);
  searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM),new MetadataSearchResultRecord(pingService,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(expected,searchTags);
  searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(expected,searchTags);
  searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.of(),searchTags);
  removeTags(application,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.of(""String_Node_Str""),getTags(application));
  removeTags(pingService);
  Assert.assertTrue(getTags(pingService).isEmpty());
  removeTags(pingService);
  Assert.assertTrue(getTags(pingService).isEmpty());
  removeTags(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.of(""String_Node_Str""),getTags(myds));
  removeTags(mystream,""String_Node_Str"");
  removeTags(mystream,""String_Node_Str"");
  Assert.assertTrue(getTags(mystream).isEmpty());
  removeTags(application);
  removeTags(pingService);
  removeTags(myds);
  removeTags(mystream);
  Assert.assertTrue(getTags(application).isEmpty());
  Assert.assertTrue(getTags(pingService).isEmpty());
  Assert.assertTrue(getTags(myds).isEmpty());
  Assert.assertTrue(getTags(mystream).isEmpty());
  Assert.assertEquals(404,addTags(nonExistingApp,appTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingService,serviceTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingDataset,datasetTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingStream,streamTags).getResponseCode());
}","@Test public void testTags() throws IOException {
  Assert.assertEquals(400,addTags(application).getResponseCode());
  Set<String> appTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(application,appTags).getResponseCode());
  Assert.assertEquals(400,addTags(pingService).getResponseCode());
  Set<String> serviceTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(pingService,serviceTags).getResponseCode());
  Assert.assertEquals(400,addTags(myds).getResponseCode());
  Set<String> datasetTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(myds,datasetTags).getResponseCode());
  Assert.assertEquals(400,addTags(mystream).getResponseCode());
  Set<String> streamTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(mystream,streamTags).getResponseCode());
  Set<String> tags=getTags(application);
  Assert.assertTrue(tags.containsAll(appTags));
  Assert.assertTrue(appTags.containsAll(tags));
  tags=getTags(pingService);
  Assert.assertTrue(tags.containsAll(serviceTags));
  Assert.assertTrue(serviceTags.containsAll(tags));
  tags=getTags(myds);
  Assert.assertTrue(tags.containsAll(datasetTags));
  Assert.assertTrue(datasetTags.containsAll(tags));
  tags=getTags(mystream);
  Assert.assertTrue(tags.containsAll(streamTags));
  Assert.assertTrue(streamTags.containsAll(tags));
  Set<MetadataSearchResultRecord> searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM));
  Assert.assertEquals(expected,searchTags);
  searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM),new MetadataSearchResultRecord(pingService,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(expected,searchTags);
  searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(expected,searchTags);
  searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.<MetadataSearchResultRecord>of(),searchTags);
  removeTags(application,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.of(""String_Node_Str""),getTags(application));
  removeTags(pingService);
  Assert.assertTrue(getTags(pingService).isEmpty());
  removeTags(pingService);
  Assert.assertTrue(getTags(pingService).isEmpty());
  removeTags(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.of(""String_Node_Str""),getTags(myds));
  removeTags(mystream,""String_Node_Str"");
  removeTags(mystream,""String_Node_Str"");
  Assert.assertTrue(getTags(mystream).isEmpty());
  removeTags(application);
  removeTags(pingService);
  removeTags(myds);
  removeTags(mystream);
  Assert.assertTrue(getTags(application).isEmpty());
  Assert.assertTrue(getTags(pingService).isEmpty());
  Assert.assertTrue(getTags(myds).isEmpty());
  Assert.assertTrue(getTags(mystream).isEmpty());
  Assert.assertEquals(404,addTags(nonExistingApp,appTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingService,serviceTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingDataset,datasetTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingStream,streamTags).getResponseCode());
}"
6560,"@Test public void testProperties() throws IOException {
  Assert.assertEquals(400,addProperties(application).getResponseCode());
  Map<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(application,appProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(pingService).getResponseCode());
  Map<String,String> serviceProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(pingService,serviceProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(myds).getResponseCode());
  Map<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(myds,datasetProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(mystream).getResponseCode());
  Map<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(mystream,streamProperties).getResponseCode());
  Map<String,String> properties=getProperties(application);
  Assert.assertEquals(appProperties,properties);
  properties=getProperties(pingService);
  Assert.assertEquals(serviceProperties,properties);
  properties=getProperties(myds);
  Assert.assertEquals(datasetProperties,properties);
  properties=getProperties(mystream);
  Assert.assertEquals(streamProperties,properties);
  Set<MetadataSearchResultRecord> searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  expected=ImmutableSet.of(new MetadataSearchResultRecord(pingService,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertTrue(searchProperties.size() == 0);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertTrue(searchProperties.size() == 0);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.of(),searchProperties);
  removeProperties(application);
  Assert.assertTrue(getProperties(application).isEmpty());
  removeProperties(pingService,""String_Node_Str"");
  removeProperties(pingService,""String_Node_Str"");
  Assert.assertTrue(getProperties(pingService).isEmpty());
  removeProperties(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(myds));
  removeProperties(mystream,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(mystream));
  removeProperties(application);
  removeProperties(pingService);
  removeProperties(myds);
  removeProperties(mystream);
  Assert.assertTrue(getProperties(application).isEmpty());
  Assert.assertTrue(getProperties(pingService).isEmpty());
  Assert.assertTrue(getProperties(myds).isEmpty());
  Assert.assertTrue(getProperties(mystream).isEmpty());
  Assert.assertEquals(404,addProperties(nonExistingApp,appProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingService,serviceProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingDataset,datasetProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingStream,streamProperties).getResponseCode());
}","@Test public void testProperties() throws IOException {
  Assert.assertEquals(400,addProperties(application).getResponseCode());
  Map<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(application,appProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(pingService).getResponseCode());
  Map<String,String> serviceProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(pingService,serviceProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(myds).getResponseCode());
  Map<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(myds,datasetProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(mystream).getResponseCode());
  Map<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(mystream,streamProperties).getResponseCode());
  Map<String,String> properties=getProperties(application);
  Assert.assertEquals(appProperties,properties);
  properties=getProperties(pingService);
  Assert.assertEquals(serviceProperties,properties);
  properties=getProperties(myds);
  Assert.assertEquals(datasetProperties,properties);
  properties=getProperties(mystream);
  Assert.assertEquals(streamProperties,properties);
  Set<MetadataSearchResultRecord> searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  expected=ImmutableSet.of(new MetadataSearchResultRecord(pingService,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertTrue(searchProperties.size() == 0);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertTrue(searchProperties.size() == 0);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.<MetadataSearchResultRecord>of(),searchProperties);
  removeProperties(application);
  Assert.assertTrue(getProperties(application).isEmpty());
  removeProperties(pingService,""String_Node_Str"");
  removeProperties(pingService,""String_Node_Str"");
  Assert.assertTrue(getProperties(pingService).isEmpty());
  removeProperties(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(myds));
  removeProperties(mystream,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(mystream));
  removeProperties(application);
  removeProperties(pingService);
  removeProperties(myds);
  removeProperties(mystream);
  Assert.assertTrue(getProperties(application).isEmpty());
  Assert.assertTrue(getProperties(pingService).isEmpty());
  Assert.assertTrue(getProperties(myds).isEmpty());
  Assert.assertTrue(getProperties(mystream).isEmpty());
  Assert.assertEquals(404,addProperties(nonExistingApp,appProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingService,serviceProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingDataset,datasetProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingStream,streamProperties).getResponseCode());
}"
6561,"@Override public Set<MetadataSearchResultRecord> searchMetadata(String searchQuery,@Nullable final MetadataSearchTargetType type) throws NotFoundException {
  Iterable<BusinessMetadataRecord> results;
  if (type == null) {
    results=businessMds.searchMetadata(searchQuery);
  }
 else {
    results=businessMds.searchMetadataOnType(searchQuery,type);
  }
  Set<MetadataSearchResultRecord> searchResultRecords=new LinkedHashSet<>();
  for (  BusinessMetadataRecord bmr : results) {
    MetadataSearchTargetType finalType=type;
    if (finalType == null || finalType == MetadataSearchTargetType.ALL) {
      Id.NamespacedId namespacedId=bmr.getTargetId();
      String targetType=getTargetType(namespacedId);
      finalType=getMetadataSearchTarget(targetType);
    }
    MetadataSearchResultRecord msr=new MetadataSearchResultRecord(bmr.getTargetId(),finalType);
    searchResultRecords.add(msr);
  }
  return searchResultRecords;
}","@Override public Set<MetadataSearchResultRecord> searchMetadata(String namespaceId,String searchQuery,@Nullable final MetadataSearchTargetType type) throws NotFoundException {
  Iterable<BusinessMetadataRecord> results;
  if (type == null) {
    results=businessMds.searchMetadata(namespaceId,searchQuery);
  }
 else {
    results=businessMds.searchMetadataOnType(namespaceId,searchQuery,type);
  }
  Set<MetadataSearchResultRecord> searchResultRecords=new LinkedHashSet<>();
  for (  BusinessMetadataRecord bmr : results) {
    MetadataSearchTargetType finalType=type;
    if (finalType == null || finalType == MetadataSearchTargetType.ALL) {
      Id.NamespacedId namespacedId=bmr.getTargetId();
      String targetType=getTargetType(namespacedId);
      finalType=getMetadataSearchTarget(targetType);
    }
    MetadataSearchResultRecord msr=new MetadataSearchResultRecord(bmr.getTargetId(),finalType);
    searchResultRecords.add(msr);
  }
  return searchResultRecords;
}"
6562,"/** 
 * Execute search for metadata for particular type of CDAP object.
 * @param searchQuery The query need to be executed for the search.
 * @param type The particular type of CDAP object that the metadata need to be searched. If null all possible typeswill be searched.
 * @return a {@link Set} records for metadata search.
 * @throws NotFoundException if there is not record found for particular query text.
 */
Set<MetadataSearchResultRecord> searchMetadata(String searchQuery,@Nullable MetadataSearchTargetType type) throws NotFoundException ;","/** 
 * Execute search for metadata for particular type of CDAP object.
 * @param namespaceId The namespace id to be filter the search by.
 * @param searchQuery The query need to be executed for the search.
 * @param type The particular type of CDAP object that the metadata need to be searched. If null all possible typeswill be searched.
 * @return a {@link Set} records for metadata search.
 * @throws NotFoundException if there is not record found for particular query text.
 */
Set<MetadataSearchResultRecord> searchMetadata(String namespaceId,String searchQuery,@Nullable MetadataSearchTargetType type) throws NotFoundException ;"
6563,"@GET @Path(""String_Node_Str"") public void searchMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@QueryParam(""String_Node_Str"") String searchQuery,@QueryParam(""String_Node_Str"") String target) throws Exception {
  MetadataSearchTargetType metadataSearchTargetType;
  if (target != null) {
    metadataSearchTargetType=MetadataSearchTargetType.valueOf(target.toUpperCase());
  }
 else {
    metadataSearchTargetType=null;
  }
  Set<MetadataSearchResultRecord> results=metadataAdmin.searchMetadata(searchQuery,metadataSearchTargetType);
  responder.sendJson(HttpResponseStatus.OK,results,SET_METADATA_SEARCH_RESULT_TYPE,GSON);
}","@GET @Path(""String_Node_Str"") public void searchMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@QueryParam(""String_Node_Str"") String searchQuery,@QueryParam(""String_Node_Str"") String target) throws Exception {
  MetadataSearchTargetType metadataSearchTargetType;
  if (target != null) {
    metadataSearchTargetType=MetadataSearchTargetType.valueOf(target.toUpperCase());
  }
 else {
    metadataSearchTargetType=null;
  }
  Set<MetadataSearchResultRecord> results=metadataAdmin.searchMetadata(namespaceId,searchQuery,metadataSearchTargetType);
  responder.sendJson(HttpResponseStatus.OK,results,SET_METADATA_SEARCH_RESULT_TYPE,GSON);
}"
6564,"@Test public void testProperties() throws IOException {
  Assert.assertEquals(400,addProperties(application).getResponseCode());
  Map<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(application,appProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(pingService).getResponseCode());
  Map<String,String> serviceProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(pingService,serviceProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(myds).getResponseCode());
  Map<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(myds,datasetProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(mystream).getResponseCode());
  Map<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(mystream,streamProperties).getResponseCode());
  Map<String,String> properties=getProperties(application);
  Assert.assertEquals(appProperties,properties);
  properties=getProperties(pingService);
  Assert.assertEquals(serviceProperties,properties);
  properties=getProperties(myds);
  Assert.assertEquals(datasetProperties,properties);
  properties=getProperties(mystream);
  Assert.assertEquals(streamProperties,properties);
  Set<MetadataSearchResultRecord> searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  expected=ImmutableSet.of(new MetadataSearchResultRecord(pingService,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertTrue(searchProperties.size() == 0);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertTrue(searchProperties.size() == 0);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.<MetadataSearchResultRecord>of(),searchProperties);
  removeProperties(application);
  Assert.assertTrue(getProperties(application).isEmpty());
  removeProperties(pingService,""String_Node_Str"");
  removeProperties(pingService,""String_Node_Str"");
  Assert.assertTrue(getProperties(pingService).isEmpty());
  removeProperties(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(myds));
  removeProperties(mystream,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(mystream));
  removeProperties(application);
  removeProperties(pingService);
  removeProperties(myds);
  removeProperties(mystream);
  Assert.assertTrue(getProperties(application).isEmpty());
  Assert.assertTrue(getProperties(pingService).isEmpty());
  Assert.assertTrue(getProperties(myds).isEmpty());
  Assert.assertTrue(getProperties(mystream).isEmpty());
  Assert.assertEquals(404,addProperties(nonExistingApp,appProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingService,serviceProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingDataset,datasetProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingStream,streamProperties).getResponseCode());
}","@Test public void testProperties() throws IOException {
  Assert.assertEquals(400,addProperties(application).getResponseCode());
  Map<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(application,appProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(pingService).getResponseCode());
  Map<String,String> serviceProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(pingService,serviceProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(myds).getResponseCode());
  Map<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(myds,datasetProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(mystream).getResponseCode());
  Map<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(mystream,streamProperties).getResponseCode());
  Map<String,String> properties=getProperties(application);
  Assert.assertEquals(appProperties,properties);
  properties=getProperties(pingService);
  Assert.assertEquals(serviceProperties,properties);
  properties=getProperties(myds);
  Assert.assertEquals(datasetProperties,properties);
  properties=getProperties(mystream);
  Assert.assertEquals(streamProperties,properties);
  Set<MetadataSearchResultRecord> searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  expected=ImmutableSet.of(new MetadataSearchResultRecord(pingService,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertTrue(searchProperties.size() == 0);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertTrue(searchProperties.size() == 0);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.<MetadataSearchResultRecord>of(),searchProperties);
  searchProperties=searchMetadata(""String_Node_Str"",""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.of(),searchProperties);
  removeProperties(application);
  Assert.assertTrue(getProperties(application).isEmpty());
  removeProperties(pingService,""String_Node_Str"");
  removeProperties(pingService,""String_Node_Str"");
  Assert.assertTrue(getProperties(pingService).isEmpty());
  removeProperties(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(myds));
  removeProperties(mystream,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(mystream));
  removeProperties(application);
  removeProperties(pingService);
  removeProperties(myds);
  removeProperties(mystream);
  Assert.assertTrue(getProperties(application).isEmpty());
  Assert.assertTrue(getProperties(pingService).isEmpty());
  Assert.assertTrue(getProperties(myds).isEmpty());
  Assert.assertTrue(getProperties(mystream).isEmpty());
  Assert.assertEquals(404,addProperties(nonExistingApp,appProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingService,serviceProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingDataset,datasetProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingStream,streamProperties).getResponseCode());
}"
6565,"List<BusinessMetadataRecord> executeSearchOnColumns(String column,String searchValue,MetadataSearchTargetType type){
  List<BusinessMetadataRecord> results=new LinkedList<>();
  Scanner scanner;
  String lowerCaseSearchValue=searchValue.toLowerCase();
  if (lowerCaseSearchValue.endsWith(""String_Node_Str"")) {
    byte[] startKey=Bytes.toBytes(lowerCaseSearchValue.substring(0,lowerCaseSearchValue.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    scanner=indexedTable.scanByIndex(Bytes.toBytes(column),startKey,stopKey);
  }
 else {
    byte[] value=Bytes.toBytes(lowerCaseSearchValue);
    scanner=indexedTable.readByIndex(Bytes.toBytes(column),value);
  }
  try {
    Row next;
    while ((next=scanner.next()) != null) {
      String rowValue=next.getString(VALUE_COLUMN);
      if (rowValue == null) {
        continue;
      }
      final byte[] rowKey=next.getRow();
      String targetType=MdsValueKey.getTargetType(rowKey);
      if ((type != MetadataSearchTargetType.ALL) && (!targetType.equals(type.getInternalName()))) {
        continue;
      }
      Id.NamespacedId targetId=MdsValueKey.getNamespaceIdFromKey(targetType,new MDSKey(rowKey));
      String key=MdsValueKey.getMetadataKey(targetType,rowKey);
      String value=Bytes.toString(next.get(Bytes.toBytes(BusinessMetadataDataset.VALUE_COLUMN)));
      BusinessMetadataRecord record=new BusinessMetadataRecord(targetId,key,value);
      results.add(record);
    }
  }
  finally {
    scanner.close();
  }
  return results;
}","List<BusinessMetadataRecord> executeSearchOnColumns(String namespaceId,String column,String searchValue,MetadataSearchTargetType type){
  List<BusinessMetadataRecord> results=new LinkedList<>();
  Scanner scanner;
  String namespacedSearchValue=namespaceId + KEYVALUE_SEPARATOR + searchValue.toLowerCase();
  if (namespacedSearchValue.endsWith(""String_Node_Str"")) {
    byte[] startKey=Bytes.toBytes(namespacedSearchValue.substring(0,namespacedSearchValue.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    scanner=indexedTable.scanByIndex(Bytes.toBytes(column),startKey,stopKey);
  }
 else {
    byte[] value=Bytes.toBytes(namespacedSearchValue);
    scanner=indexedTable.readByIndex(Bytes.toBytes(column),value);
  }
  try {
    Row next;
    while ((next=scanner.next()) != null) {
      String rowValue=next.getString(VALUE_COLUMN);
      if (rowValue == null) {
        continue;
      }
      final byte[] rowKey=next.getRow();
      String targetType=MdsValueKey.getTargetType(rowKey);
      if ((type != MetadataSearchTargetType.ALL) && (!targetType.equals(type.getInternalName()))) {
        continue;
      }
      Id.NamespacedId targetId=MdsValueKey.getNamespaceIdFromKey(targetType,new MDSKey(rowKey));
      String key=MdsValueKey.getMetadataKey(targetType,rowKey);
      String value=Bytes.toString(next.get(Bytes.toBytes(BusinessMetadataDataset.VALUE_COLUMN)));
      BusinessMetadataRecord record=new BusinessMetadataRecord(targetId,key,value);
      results.add(record);
    }
  }
  finally {
    scanner.close();
  }
  return results;
}"
6566,"/** 
 * Find the instance of   {@link BusinessMetadataRecord} for key:value pair
 * @param keyValue The metadata value to be found.
 * @param type The target type of objects to search from.
 * @return The {@link Iterable} of {@link BusinessMetadataRecord} that fit the key value pair.
 */
public List<BusinessMetadataRecord> findBusinessMetadataOnKeyValue(String keyValue,MetadataSearchTargetType type){
  return executeSearchOnColumns(BusinessMetadataDataset.KEYVALUE_COLUMN,keyValue,type);
}","/** 
 * Find the instance of   {@link BusinessMetadataRecord} for key:value pair
 * @param namespaceId The namespace id to filter
 * @param keyValue The metadata value to be found.
 * @param type The target type of objects to search from.
 * @return The {@link Iterable} of {@link BusinessMetadataRecord} that fit the key value pair.
 */
public List<BusinessMetadataRecord> findBusinessMetadataOnKeyValue(String namespaceId,String keyValue,MetadataSearchTargetType type){
  return executeSearchOnColumns(namespaceId,BusinessMetadataDataset.KEYVALUE_COLUMN,keyValue,type);
}"
6567,"/** 
 * Find the instance of   {@link BusinessMetadataRecord} based on key.
 * @param value The metadata value to be found
 * @param type The target type of objects to search from
 * @return The {@link Iterable} of {@link BusinessMetadataRecord} that fit the value
 */
public List<BusinessMetadataRecord> findBusinessMetadataOnValue(String value,MetadataSearchTargetType type){
  return executeSearchOnColumns(BusinessMetadataDataset.CASE_INSENSITIVE_VALUE_COLUMN,value,type);
}","/** 
 * Find the instance of   {@link BusinessMetadataRecord} based on key.
 * @param namespaceId The namespace id to filter
 * @param value The metadata value to be found
 * @param type The target type of objects to search from
 * @return The {@link Iterable} of {@link BusinessMetadataRecord} that fit the value
 */
public List<BusinessMetadataRecord> findBusinessMetadataOnValue(String namespaceId,String value,MetadataSearchTargetType type){
  return executeSearchOnColumns(namespaceId,BusinessMetadataDataset.CASE_INSENSITIVE_VALUE_COLUMN,value,type);
}"
6568,"private void write(Id.NamespacedId targetId,MetadataType metadataType,BusinessMetadataRecord record){
  String key=record.getKey();
  MDSKey mdsKey=MdsValueKey.getMDSKey(targetId,metadataType,key);
  Put put=new Put(mdsKey.getKey());
  put.add(Bytes.toBytes(KEYVALUE_COLUMN),Bytes.toBytes(record.getKey().toLowerCase() + KEYVALUE_SEPARATOR + record.getValue().toLowerCase()));
  put.add(Bytes.toBytes(CASE_INSENSITIVE_VALUE_COLUMN),Bytes.toBytes(record.getValue().toLowerCase()));
  put.add(Bytes.toBytes(VALUE_COLUMN),Bytes.toBytes(record.getValue()));
  indexedTable.put(put);
  writeHistory(targetId);
}","private void write(Id.NamespacedId targetId,MetadataType metadataType,BusinessMetadataRecord record){
  String key=record.getKey();
  MDSKey mdsKey=MdsValueKey.getMDSKey(targetId,metadataType,key);
  Put put=new Put(mdsKey.getKey());
  String lowerCaseKey=record.getKey().toLowerCase();
  String lowerCaseValue=record.getValue().toLowerCase();
  String nameSpacedKVIndexValue=MdsValueKey.getNamespaceId(mdsKey) + KEYVALUE_SEPARATOR + lowerCaseKey+ KEYVALUE_SEPARATOR+ lowerCaseValue;
  put.add(Bytes.toBytes(KEYVALUE_COLUMN),Bytes.toBytes(nameSpacedKVIndexValue));
  String nameSpacedVIndexValue=MdsValueKey.getNamespaceId(mdsKey) + KEYVALUE_SEPARATOR + lowerCaseValue;
  put.add(Bytes.toBytes(CASE_INSENSITIVE_VALUE_COLUMN),Bytes.toBytes(nameSpacedVIndexValue));
  put.add(Bytes.toBytes(VALUE_COLUMN),Bytes.toBytes(record.getValue()));
  indexedTable.put(put);
  writeHistory(targetId);
}"
6569,"/** 
 * Search to the underlying Business Metadata Dataset.
 */
Iterable<BusinessMetadataRecord> searchMetadata(String searchQuery);","/** 
 * Search to the underlying Business Metadata Dataset.
 */
Iterable<BusinessMetadataRecord> searchMetadata(String namespaceId,String searchQuery);"
6570,"/** 
 * Search to the underlying Business Metadata Dataset for a target type.
 */
Iterable<BusinessMetadataRecord> searchMetadataOnType(String searchQuery,MetadataSearchTargetType type);","/** 
 * Search to the underlying Business Metadata Dataset for a target type.
 */
Iterable<BusinessMetadataRecord> searchMetadataOnType(String namespaceId,String searchQuery,MetadataSearchTargetType type);"
6571,"/** 
 * Search to the underlying Business Metadata Dataset.
 */
@Override public Iterable<BusinessMetadataRecord> searchMetadata(final String searchQuery){
  return searchMetadataOnType(searchQuery,MetadataSearchTargetType.ALL);
}","/** 
 * Search to the underlying Business Metadata Dataset.
 */
@Override public Iterable<BusinessMetadataRecord> searchMetadata(final String namespaceId,final String searchQuery){
  return searchMetadataOnType(namespaceId,searchQuery,MetadataSearchTargetType.ALL);
}"
6572,"@Override public Iterable<BusinessMetadataRecord> apply(BusinessMetadataDataset input) throws Exception {
  if (searchQuery.contains(BusinessMetadataDataset.KEYVALUE_SEPARATOR)) {
    return input.findBusinessMetadataOnKeyValue(searchQuery,type);
  }
  return input.findBusinessMetadataOnValue(searchQuery,type);
}","@Override public Iterable<BusinessMetadataRecord> apply(BusinessMetadataDataset input) throws Exception {
  if (searchQuery.contains(BusinessMetadataDataset.KEYVALUE_SEPARATOR)) {
    return input.findBusinessMetadataOnKeyValue(namespaceId,searchQuery,type);
  }
  return input.findBusinessMetadataOnValue(namespaceId,searchQuery,type);
}"
6573,"/** 
 * Search to the underlying Business Metadata Dataset for a target type.
 */
@Override public Iterable<BusinessMetadataRecord> searchMetadataOnType(final String searchQuery,final MetadataSearchTargetType type){
  return execute(new TransactionExecutor.Function<BusinessMetadataDataset,Iterable<BusinessMetadataRecord>>(){
    @Override public Iterable<BusinessMetadataRecord> apply(    BusinessMetadataDataset input) throws Exception {
      if (searchQuery.contains(BusinessMetadataDataset.KEYVALUE_SEPARATOR)) {
        return input.findBusinessMetadataOnKeyValue(searchQuery,type);
      }
      return input.findBusinessMetadataOnValue(searchQuery,type);
    }
  }
);
}","/** 
 * Search to the underlying Business Metadata Dataset for a target type.
 */
@Override public Iterable<BusinessMetadataRecord> searchMetadataOnType(final String namespaceId,final String searchQuery,final MetadataSearchTargetType type){
  return execute(new TransactionExecutor.Function<BusinessMetadataDataset,Iterable<BusinessMetadataRecord>>(){
    @Override public Iterable<BusinessMetadataRecord> apply(    BusinessMetadataDataset input) throws Exception {
      if (searchQuery.contains(BusinessMetadataDataset.KEYVALUE_SEPARATOR)) {
        return input.findBusinessMetadataOnKeyValue(namespaceId,searchQuery,type);
      }
      return input.findBusinessMetadataOnValue(namespaceId,searchQuery,type);
    }
  }
);
}"
6574,"@Override public Iterable<BusinessMetadataRecord> searchMetadata(String searchQuery){
  return null;
}","@Override public Iterable<BusinessMetadataRecord> searchMetadata(String namespaceId,String searchQuery){
  return null;
}"
6575,"@Override public Iterable<BusinessMetadataRecord> searchMetadataOnType(String searchQuery,MetadataSearchTargetType type){
  return null;
}","@Override public Iterable<BusinessMetadataRecord> searchMetadataOnType(String namespaceId,String searchQuery,MetadataSearchTargetType type){
  return null;
}"
6576,"@Test public void testSearchOnKeyValue() throws Exception {
  BusinessMetadataRecord record=new BusinessMetadataRecord(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results=dataset.findBusinessMetadataOnKeyValue(""String_Node_Str"" + BusinessMetadataDataset.KEYVALUE_SEPARATOR + ""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  BusinessMetadataRecord result=results.get(0);
  Assert.assertEquals(record,result);
}","@Test public void testSearchOnKeyValue() throws Exception {
  BusinessMetadataRecord record=new BusinessMetadataRecord(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results=dataset.findBusinessMetadataOnKeyValue(""String_Node_Str"",""String_Node_Str"" + BusinessMetadataDataset.KEYVALUE_SEPARATOR + ""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  BusinessMetadataRecord result=results.get(0);
  Assert.assertEquals(record,result);
  List<BusinessMetadataRecord> results2=dataset.findBusinessMetadataOnKeyValue(""String_Node_Str"",""String_Node_Str"" + BusinessMetadataDataset.KEYVALUE_SEPARATOR + ""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(0,results2.size());
}"
6577,"@Test public void testSearchOnValue() throws Exception {
  BusinessMetadataRecord record=new BusinessMetadataRecord(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  results=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  BusinessMetadataRecord result=results.get(0);
  Assert.assertEquals(record,result);
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results2=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(2,results2.size());
  for (  BusinessMetadataRecord result2 : results2) {
    Assert.assertEquals(""String_Node_Str"",result2.getValue());
  }
  dataset.setProperty(stream1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results3=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(2,results3.size());
  for (  BusinessMetadataRecord result3 : results3) {
    Assert.assertTrue(result3.getValue().startsWith(""String_Node_Str""));
  }
}","@Test public void testSearchOnValue() throws Exception {
  BusinessMetadataRecord record=new BusinessMetadataRecord(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results=dataset.findBusinessMetadataOnValue(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  BusinessMetadataRecord result=results.get(0);
  Assert.assertEquals(record,result);
  results=dataset.findBusinessMetadataOnValue(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  result=results.get(0);
  Assert.assertEquals(record,result);
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results2=dataset.findBusinessMetadataOnValue(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(2,results2.size());
  for (  BusinessMetadataRecord result2 : results2) {
    Assert.assertEquals(""String_Node_Str"",result2.getValue());
  }
  dataset.setProperty(stream1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results3=dataset.findBusinessMetadataOnValue(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(2,results3.size());
  for (  BusinessMetadataRecord result3 : results3) {
    Assert.assertTrue(result3.getValue().startsWith(""String_Node_Str""));
  }
  List<BusinessMetadataRecord> results4=dataset.findBusinessMetadataOnValue(""String_Node_Str"",""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(0,results4.size());
}"
6578,"@PUT @Path(""String_Node_Str"") public void createFeed(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String category,@PathParam(""String_Node_Str"") String name){
  try {
    Id.NotificationFeed combinedFeed;
    try {
      Id.NotificationFeed feed=parseBody(request,Id.NotificationFeed.class);
      combinedFeed=new Id.NotificationFeed.Builder().setNamespaceId(namespaceId).setCategory(category).setName(name).setDescription(feed == null ? null : feed.getDescription()).build();
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,String.format(""String_Node_Str"",e.getMessage()));
      return;
    }
    if (feedManager.createFeed(combinedFeed)) {
      responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
    }
 else {
      LOG.trace(""String_Node_Str"");
      responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
    }
  }
 catch (  NotificationFeedException e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
catch (  JsonSyntaxException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
  }
catch (  IOException e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
catch (  Throwable t) {
    LOG.debug(""String_Node_Str"",t);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,t.getMessage());
  }
}","@PUT @Path(""String_Node_Str"") public void createFeed(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String category,@PathParam(""String_Node_Str"") String name){
  try {
    Id.NotificationFeed combinedFeed;
    try {
      Id.NotificationFeed feed=parseBody(request,Id.NotificationFeed.class);
      combinedFeed=new Id.NotificationFeed.Builder().setNamespaceId(namespaceId).setCategory(category).setName(name).setDescription(feed == null ? null : feed.getDescription()).build();
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,String.format(""String_Node_Str"",e.getMessage()));
      return;
    }
    if (feedManager.createFeed(combinedFeed)) {
      responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
    }
 else {
      LOG.trace(""String_Node_Str"");
      responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
    }
  }
 catch (  NotificationFeedException e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
catch (  JsonSyntaxException e) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
  }
catch (  IOException e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
catch (  Throwable t) {
    LOG.debug(""String_Node_Str"",t);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,t.getMessage());
  }
}"
6579,"/** 
 * Returns a list of services associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllServices(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  programList(responder,namespaceId,ProgramType.SERVICE,store);
}","/** 
 * Returns a list of services associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllServices(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  programList(responder,namespaceId,ProgramType.SERVICE,store);
}"
6580,"@GET @Path(""String_Node_Str"") public void programSpecification(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId){
  ProgramType type=getProgramType(programType);
  if (type == null) {
    responder.sendString(HttpResponseStatus.METHOD_NOT_ALLOWED,String.format(""String_Node_Str"",programType));
    return;
  }
  try {
    Id.Program id=Id.Program.from(namespaceId,appId,type,programId);
    ProgramSpecification specification=getProgramSpecification(id);
    if (specification == null) {
      responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    }
 else {
      responder.sendJson(HttpResponseStatus.OK,specification);
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","@GET @Path(""String_Node_Str"") public void programSpecification(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId){
  ProgramType type=getProgramType(programType);
  if (type == null) {
    responder.sendString(HttpResponseStatus.METHOD_NOT_ALLOWED,String.format(""String_Node_Str"",programType));
    return;
  }
  try {
    Id.Program id=Id.Program.from(namespaceId,appId,type,programId);
    ProgramSpecification specification=getProgramSpecification(id);
    if (specification == null) {
      responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    }
 else {
      responder.sendJson(HttpResponseStatus.OK,specification);
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
}"
6581,"/** 
 * Save program runtime args.
 */
@PUT @Path(""String_Node_Str"") public void saveProgramRuntimeArgs(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId){
  ProgramType type=ProgramType.valueOfCategoryName(programType);
  if (type == null || type == ProgramType.WEBAPP) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    return;
  }
  Id.Program id=Id.Program.from(namespaceId,appId,type,programId);
  try {
    if (!store.programExists(id)) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
      return;
    }
    Map<String,String> args=decodeArguments(request);
    preferencesStore.setProperties(namespaceId,appId,programType,programId,args);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e.getMessage(),e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Save program runtime args.
 */
@PUT @Path(""String_Node_Str"") public void saveProgramRuntimeArgs(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId){
  ProgramType type=ProgramType.valueOfCategoryName(programType);
  if (type == null || type == ProgramType.WEBAPP) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    return;
  }
  Id.Program id=Id.Program.from(namespaceId,appId,type,programId);
  if (!store.programExists(id)) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
    return;
  }
  Map<String,String> args=decodeArguments(request);
  preferencesStore.setProperties(namespaceId,appId,programType,programId,args);
  responder.sendStatus(HttpResponseStatus.OK);
}"
6582,"/** 
 * Deletes queues.
 */
@DELETE @Path(""String_Node_Str"") public void deleteFlowQueues(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId){
  Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.FLOW,flowId);
  try {
    ProgramStatus status=getProgramStatus(programId);
    if (status.getStatus().equals(HttpResponseStatus.NOT_FOUND.toString())) {
      responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    }
 else     if (status.getStatus().equals(""String_Node_Str"")) {
      responder.sendString(HttpResponseStatus.FORBIDDEN,""String_Node_Str"");
    }
 else {
      queueAdmin.dropAllForFlow(Id.Flow.from(programId.getApplication(),programId.getId()));
      FlowUtils.deleteFlowPendingMetrics(metricStore,namespaceId,appId,flowId);
      responder.sendStatus(HttpResponseStatus.OK);
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Deletes queues.
 */
@DELETE @Path(""String_Node_Str"") public void deleteFlowQueues(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId) throws Exception {
  Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.FLOW,flowId);
  try {
    ProgramStatus status=getProgramStatus(programId);
    if (status.getStatus().equals(HttpResponseStatus.NOT_FOUND.toString())) {
      responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    }
 else     if (status.getStatus().equals(""String_Node_Str"")) {
      responder.sendString(HttpResponseStatus.FORBIDDEN,""String_Node_Str"");
    }
 else {
      queueAdmin.dropAllForFlow(Id.Flow.from(programId.getApplication(),programId.getId()));
      FlowUtils.deleteFlowPendingMetrics(metricStore,namespaceId,appId,flowId);
      responder.sendStatus(HttpResponseStatus.OK);
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
}"
6583,"/** 
 * Returns number of instances of a worker.
 */
@GET @Path(""String_Node_Str"") public void getWorkerInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workerId){
  try {
    int count=store.getWorkerInstances(Id.Program.from(namespaceId,appId,ProgramType.WORKER,workerId));
    responder.sendJson(HttpResponseStatus.OK,new Instances(count));
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Returns number of instances of a worker.
 */
@GET @Path(""String_Node_Str"") public void getWorkerInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workerId){
  try {
    int count=store.getWorkerInstances(Id.Program.from(namespaceId,appId,ProgramType.WORKER,workerId));
    responder.sendJson(HttpResponseStatus.OK,new Instances(count));
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    throw e;
  }
}"
6584,"/** 
 * Returns a list of map/reduces associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllMapReduce(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  programList(responder,namespaceId,ProgramType.MAPREDUCE,store);
}","/** 
 * Returns a list of map/reduces associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllMapReduce(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  programList(responder,namespaceId,ProgramType.MAPREDUCE,store);
}"
6585,"/** 
 * Return the number of instances of a service.
 */
@GET @Path(""String_Node_Str"") public void getServiceInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId){
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.SERVICE,serviceId);
    if (!store.programExists(programId)) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
      return;
    }
    ServiceSpecification specification=(ServiceSpecification)getProgramSpecification(programId);
    if (specification == null) {
      responder.sendStatus(HttpResponseStatus.NOT_FOUND);
      return;
    }
    int instances=specification.getInstances();
    responder.sendJson(HttpResponseStatus.OK,new ServiceInstances(instances,getInstanceCount(programId,serviceId)));
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Return the number of instances of a service.
 */
@GET @Path(""String_Node_Str"") public void getServiceInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId){
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.SERVICE,serviceId);
    if (!store.programExists(programId)) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
      return;
    }
    ServiceSpecification specification=(ServiceSpecification)getProgramSpecification(programId);
    if (specification == null) {
      responder.sendStatus(HttpResponseStatus.NOT_FOUND);
      return;
    }
    int instances=specification.getInstances();
    responder.sendJson(HttpResponseStatus.OK,new ServiceInstances(instances,getInstanceCount(programId,serviceId)));
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
}"
6586,"private void getRuns(HttpResponder responder,Id.Program programId,String status,long start,long end,int limit){
  try {
    try {
      ProgramRunStatus runStatus=(status == null) ? ProgramRunStatus.ALL : ProgramRunStatus.valueOf(status.toUpperCase());
      List<RunRecord> records=Lists.transform(store.getRuns(programId,runStatus,start,end,limit),CONVERT_TO_RUN_RECORD);
      responder.sendJson(HttpResponseStatus.OK,records);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","private void getRuns(HttpResponder responder,Id.Program programId,String status,long start,long end,int limit){
  try {
    try {
      ProgramRunStatus runStatus=(status == null) ? ProgramRunStatus.ALL : ProgramRunStatus.valueOf(status.toUpperCase());
      List<RunRecord> records=Lists.transform(store.getRuns(programId,runStatus,start,end,limit),CONVERT_TO_RUN_RECORD);
      responder.sendJson(HttpResponseStatus.OK,records);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
}"
6587,"/** 
 * Set instances of a service.
 */
@PUT @Path(""String_Node_Str"") public void setServiceInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId){
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.SERVICE,serviceId);
    if (!store.programExists(programId)) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
      return;
    }
    int instances;
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    int oldInstances=store.getServiceInstances(programId);
    if (oldInstances != instances) {
      store.setServiceInstances(programId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(namespaceId,appId,serviceId,ProgramType.SERVICE,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(serviceId,String.valueOf(instances))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable throwable) {
    if (respondIfElementNotFound(throwable,responder)) {
      return;
    }
    LOG.error(""String_Node_Str"",throwable);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Set instances of a service.
 */
@PUT @Path(""String_Node_Str"") public void setServiceInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String serviceId) throws ExecutionException, InterruptedException {
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.SERVICE,serviceId);
    if (!store.programExists(programId)) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
      return;
    }
    int instances;
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    int oldInstances=store.getServiceInstances(programId);
    if (oldInstances != instances) {
      store.setServiceInstances(programId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(namespaceId,appId,serviceId,ProgramType.SERVICE,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(serviceId,String.valueOf(instances))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable throwable) {
    if (respondIfElementNotFound(throwable,responder)) {
      return;
    }
    throw throwable;
  }
}"
6588,"private void suspendResumeSchedule(HttpResponder responder,String namespaceId,String appId,String scheduleName,String action){
  try {
    if (!action.equals(""String_Node_Str"") && !action.equals(""String_Node_Str"")) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    ApplicationSpecification appSpec=store.getApplication(Id.Application.from(namespaceId,appId));
    if (appSpec == null) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + appId + ""String_Node_Str"");
      return;
    }
    ScheduleSpecification scheduleSpec=appSpec.getSchedules().get(scheduleName);
    if (scheduleSpec == null) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + scheduleName + ""String_Node_Str"");
      return;
    }
    String programName=scheduleSpec.getProgram().getProgramName();
    ProgramType programType=ProgramType.valueOfSchedulableType(scheduleSpec.getProgram().getProgramType());
    Id.Program programId=Id.Program.from(namespaceId,appId,programType,programName);
    Scheduler.ScheduleState state=scheduler.scheduleState(programId,scheduleSpec.getProgram().getProgramType(),scheduleName);
switch (state) {
case NOT_FOUND:
      responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    break;
case SCHEDULED:
  if (action.equals(""String_Node_Str"")) {
    scheduler.suspendSchedule(programId,scheduleSpec.getProgram().getProgramType(),scheduleName);
    responder.sendJson(HttpResponseStatus.OK,""String_Node_Str"");
  }
 else {
    responder.sendJson(HttpResponseStatus.CONFLICT,""String_Node_Str"");
  }
break;
case SUSPENDED:
if (action.equals(""String_Node_Str"")) {
responder.sendJson(HttpResponseStatus.CONFLICT,""String_Node_Str"");
}
 else {
scheduler.resumeSchedule(programId,scheduleSpec.getProgram().getProgramType(),scheduleName);
responder.sendJson(HttpResponseStatus.OK,""String_Node_Str"");
}
break;
}
}
 catch (SecurityException e) {
responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
}
catch (NotFoundException e) {
responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
}
catch (Throwable e) {
LOG.error(""String_Node_Str"",action,scheduleName,appId,e);
responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
}
}","private void suspendResumeSchedule(HttpResponder responder,String namespaceId,String appId,String scheduleName,String action) throws SchedulerException {
  try {
    if (!action.equals(""String_Node_Str"") && !action.equals(""String_Node_Str"")) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    ApplicationSpecification appSpec=store.getApplication(Id.Application.from(namespaceId,appId));
    if (appSpec == null) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + appId + ""String_Node_Str"");
      return;
    }
    ScheduleSpecification scheduleSpec=appSpec.getSchedules().get(scheduleName);
    if (scheduleSpec == null) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"" + scheduleName + ""String_Node_Str"");
      return;
    }
    String programName=scheduleSpec.getProgram().getProgramName();
    ProgramType programType=ProgramType.valueOfSchedulableType(scheduleSpec.getProgram().getProgramType());
    Id.Program programId=Id.Program.from(namespaceId,appId,programType,programName);
    Scheduler.ScheduleState state=scheduler.scheduleState(programId,scheduleSpec.getProgram().getProgramType(),scheduleName);
switch (state) {
case NOT_FOUND:
      responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    break;
case SCHEDULED:
  if (action.equals(""String_Node_Str"")) {
    scheduler.suspendSchedule(programId,scheduleSpec.getProgram().getProgramType(),scheduleName);
    responder.sendJson(HttpResponseStatus.OK,""String_Node_Str"");
  }
 else {
    responder.sendJson(HttpResponseStatus.CONFLICT,""String_Node_Str"");
  }
break;
case SUSPENDED:
if (action.equals(""String_Node_Str"")) {
responder.sendJson(HttpResponseStatus.CONFLICT,""String_Node_Str"");
}
 else {
scheduler.resumeSchedule(programId,scheduleSpec.getProgram().getProgramType(),scheduleName);
responder.sendJson(HttpResponseStatus.OK,""String_Node_Str"");
}
break;
}
}
 catch (SecurityException e) {
responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
}
catch (NotFoundException e) {
responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
}
}"
6589,"/** 
 * Returns run record for a particular run of a program.
 */
@GET @Path(""String_Node_Str"") public void programRunRecord(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId,@PathParam(""String_Node_Str"") String runid){
  ProgramType type=ProgramType.valueOfCategoryName(programType);
  if (type == null || type == ProgramType.WEBAPP) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    return;
  }
  try {
    RunRecordMeta runRecordMeta=store.getRun(Id.Program.from(namespaceId,appId,type,programId),runid);
    if (runRecordMeta != null) {
      RunRecord runRecord=CONVERT_TO_RUN_RECORD.apply(runRecordMeta);
      responder.sendJson(HttpResponseStatus.OK,runRecord);
      return;
    }
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Returns run record for a particular run of a program.
 */
@GET @Path(""String_Node_Str"") public void programRunRecord(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId,@PathParam(""String_Node_Str"") String runid){
  ProgramType type=ProgramType.valueOfCategoryName(programType);
  if (type == null || type == ProgramType.WEBAPP) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    return;
  }
  try {
    RunRecordMeta runRecordMeta=store.getRun(Id.Program.from(namespaceId,appId,type,programId),runid);
    if (runRecordMeta != null) {
      RunRecord runRecord=CONVERT_TO_RUN_RECORD.apply(runRecordMeta);
      responder.sendJson(HttpResponseStatus.OK,runRecord);
      return;
    }
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
}"
6590,"/** 
 * Sets the number of instances of a worker.
 */
@PUT @Path(""String_Node_Str"") public void setWorkerInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workerId){
  int instances;
  try {
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
  }
 catch (  Throwable th) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.WORKER,workerId);
    int oldInstances=store.getWorkerInstances(programId);
    if (oldInstances != instances) {
      store.setWorkerInstances(programId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(namespaceId,appId,workerId,ProgramType.WORKER,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(programId.getId(),String.valueOf(instances))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Sets the number of instances of a worker.
 */
@PUT @Path(""String_Node_Str"") public void setWorkerInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workerId) throws ExecutionException, InterruptedException {
  int instances;
  try {
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
  }
 catch (  Throwable th) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.WORKER,workerId);
    int oldInstances=store.getWorkerInstances(programId);
    if (oldInstances != instances) {
      store.setWorkerInstances(programId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(namespaceId,appId,workerId,ProgramType.WORKER,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(programId.getId(),String.valueOf(instances))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    throw e;
  }
}"
6591,"@GET @Path(""String_Node_Str"") public void getAllWorkers(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  programList(responder,namespaceId,ProgramType.WORKER,store);
}","@GET @Path(""String_Node_Str"") public void getAllWorkers(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  programList(responder,namespaceId,ProgramType.WORKER,store);
}"
6592,"@Inject public ProgramLifecycleHttpHandler(Store store,CConfiguration cConf,ProgramRuntimeService runtimeService,ProgramLifecycleService lifecycleService,QueueAdmin queueAdmin,Scheduler scheduler,PreferencesStore preferencesStore,NamespacedLocationFactory namespacedLocationFactory,MRJobInfoFetcher mrJobInfoFetcher,PropertiesResolver propertiesResolver,AdapterService adapterService,MetricStore metricStore){
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.store=store;
  this.runtimeService=runtimeService;
  this.lifecycleService=lifecycleService;
  this.metricStore=metricStore;
  this.appFabricDir=cConf.get(Constants.AppFabric.OUTPUT_DIR);
  this.queueAdmin=queueAdmin;
  this.scheduler=scheduler;
  this.preferencesStore=preferencesStore;
  this.mrJobInfoFetcher=mrJobInfoFetcher;
  this.propertiesResolver=propertiesResolver;
  this.adapterService=adapterService;
}","@Inject public ProgramLifecycleHttpHandler(Store store,CConfiguration cConf,ProgramRuntimeService runtimeService,ProgramLifecycleService lifecycleService,QueueAdmin queueAdmin,Scheduler scheduler,PreferencesStore preferencesStore,NamespacedLocationFactory namespacedLocationFactory,MRJobInfoFetcher mrJobInfoFetcher,PropertiesResolver propertiesResolver,MetricStore metricStore){
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.store=store;
  this.runtimeService=runtimeService;
  this.lifecycleService=lifecycleService;
  this.metricStore=metricStore;
  this.appFabricDir=cConf.get(Constants.AppFabric.OUTPUT_DIR);
  this.queueAdmin=queueAdmin;
  this.scheduler=scheduler;
  this.preferencesStore=preferencesStore;
  this.mrJobInfoFetcher=mrJobInfoFetcher;
  this.propertiesResolver=propertiesResolver;
}"
6593,"/** 
 * Get program runtime args.
 */
@GET @Path(""String_Node_Str"") public void getProgramRuntimeArgs(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId){
  ProgramType type=ProgramType.valueOfCategoryName(programType);
  if (type == null || type == ProgramType.WEBAPP) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    return;
  }
  Id.Program id=Id.Program.from(namespaceId,appId,type,programId);
  try {
    if (!store.programExists(id)) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
      return;
    }
    Map<String,String> runtimeArgs=preferencesStore.getProperties(id.getNamespaceId(),appId,programType,programId);
    responder.sendJson(HttpResponseStatus.OK,runtimeArgs);
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e.getMessage(),e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Get program runtime args.
 */
@GET @Path(""String_Node_Str"") public void getProgramRuntimeArgs(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId){
  ProgramType type=ProgramType.valueOfCategoryName(programType);
  if (type == null || type == ProgramType.WEBAPP) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    return;
  }
  Id.Program id=Id.Program.from(namespaceId,appId,type,programId);
  if (!store.programExists(id)) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,""String_Node_Str"");
    return;
  }
  Map<String,String> runtimeArgs=preferencesStore.getProperties(id.getNamespaceId(),appId,programType,programId);
  responder.sendJson(HttpResponseStatus.OK,runtimeArgs);
}"
6594,"/** 
 * Returns the number of instances for all program runnables that are passed into the data. The data is an array of Json objects where each object must contain the following three elements: appId, programType, and programId (flow name, service name). Retrieving instances only applies to flows, and user services. For flows, another parameter, ""runnableId"", must be provided. This corresponds to the flowlet/runnable for which to retrieve the instances. <p> Example input: <pre><code> [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""runnableId"": ""Runnable1""}, {""appId"": ""App1"", ""programType"": ""Mapreduce"", ""programId"": ""Mapreduce2""}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1"", ""runnableId"": ""Flowlet1""}] </code></pre> </p><p> The response will be an array of JsonObjects each of which will contain the three input parameters as well as 3 fields: <ul> <li>""provisioned"" which maps to the number of instances actually provided for the input runnable;</li> <li>""requested"" which maps to the number of instances the user has requested for the input runnable; and</li> <li>""statusCode"" which maps to the http status code for the data in that JsonObjects (200, 400, 404).</li> </ul> </p><p> If an error occurs in the input (for the example above, Flowlet1 does not exist), then all JsonObjects for which the parameters have a valid instances will have the provisioned and requested fields status code fields but all JsonObjects for which the parameters are not valid will have an error message and statusCode. </p><p> For example, if there is no Flowlet1 in the above data, then the response could be 200 OK with the following data: </p> <pre><code> [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""runnableId"": ""Runnable1"", ""statusCode"": 200, ""provisioned"": 2, ""requested"": 2}, {""appId"": ""App1"", ""programType"": ""Mapreduce"", ""programId"": ""Mapreduce2"", ""statusCode"": 200, ""provisioned"": 1, ""requested"": 3}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1"", ""runnableId"": ""Flowlet1"", ""statusCode"": 404, ""error"": ""Program"": Flowlet1 not found""}] </code></pre>
 */
@POST @Path(""String_Node_Str"") public void getInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  try {
    List<BatchEndpointInstances> args=instancesFromBatchArgs(decodeArrayArguments(request,responder));
    if (args == null) {
      return;
    }
    for (    BatchEndpointInstances requestedObj : args) {
      Id.Application appId=Id.Application.from(namespaceId,requestedObj.getAppId());
      ApplicationSpecification spec=store.getApplication(appId);
      if (spec == null) {
        addCodeError(requestedObj,HttpResponseStatus.NOT_FOUND.getCode(),""String_Node_Str"" + appId + ""String_Node_Str"");
        continue;
      }
      ProgramType programType=ProgramType.valueOfPrettyName(requestedObj.getProgramType());
      if (!canHaveInstances(programType)) {
        addCodeError(requestedObj,HttpResponseStatus.BAD_REQUEST.getCode(),""String_Node_Str"" + programType + ""String_Node_Str"");
        continue;
      }
      Id.Program programId=Id.Program.from(appId,programType,requestedObj.getProgramId());
      populateProgramInstances(requestedObj,spec,programId);
    }
    responder.sendJson(HttpResponseStatus.OK,args);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  JsonSyntaxException e) {
    responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Returns the number of instances for all program runnables that are passed into the data. The data is an array of Json objects where each object must contain the following three elements: appId, programType, and programId (flow name, service name). Retrieving instances only applies to flows, and user services. For flows, another parameter, ""runnableId"", must be provided. This corresponds to the flowlet/runnable for which to retrieve the instances. <p> Example input: <pre><code> [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""runnableId"": ""Runnable1""}, {""appId"": ""App1"", ""programType"": ""Mapreduce"", ""programId"": ""Mapreduce2""}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1"", ""runnableId"": ""Flowlet1""}] </code></pre> </p><p> The response will be an array of JsonObjects each of which will contain the three input parameters as well as 3 fields: <ul> <li>""provisioned"" which maps to the number of instances actually provided for the input runnable;</li> <li>""requested"" which maps to the number of instances the user has requested for the input runnable; and</li> <li>""statusCode"" which maps to the http status code for the data in that JsonObjects (200, 400, 404).</li> </ul> </p><p> If an error occurs in the input (for the example above, Flowlet1 does not exist), then all JsonObjects for which the parameters have a valid instances will have the provisioned and requested fields status code fields but all JsonObjects for which the parameters are not valid will have an error message and statusCode. </p><p> For example, if there is no Flowlet1 in the above data, then the response could be 200 OK with the following data: </p> <pre><code> [{""appId"": ""App1"", ""programType"": ""Service"", ""programId"": ""Service1"", ""runnableId"": ""Runnable1"", ""statusCode"": 200, ""provisioned"": 2, ""requested"": 2}, {""appId"": ""App1"", ""programType"": ""Mapreduce"", ""programId"": ""Mapreduce2"", ""statusCode"": 200, ""provisioned"": 1, ""requested"": 3}, {""appId"": ""App2"", ""programType"": ""Flow"", ""programId"": ""Flow1"", ""runnableId"": ""Flowlet1"", ""statusCode"": 404, ""error"": ""Program"": Flowlet1 not found""}] </code></pre>
 */
@POST @Path(""String_Node_Str"") public void getInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws IOException {
  try {
    List<BatchEndpointInstances> args=instancesFromBatchArgs(decodeArrayArguments(request,responder));
    if (args == null) {
      return;
    }
    for (    BatchEndpointInstances requestedObj : args) {
      Id.Application appId=Id.Application.from(namespaceId,requestedObj.getAppId());
      ApplicationSpecification spec=store.getApplication(appId);
      if (spec == null) {
        addCodeError(requestedObj,HttpResponseStatus.NOT_FOUND.getCode(),""String_Node_Str"" + appId + ""String_Node_Str"");
        continue;
      }
      ProgramType programType=ProgramType.valueOfPrettyName(requestedObj.getProgramType());
      if (!canHaveInstances(programType)) {
        addCodeError(requestedObj,HttpResponseStatus.BAD_REQUEST.getCode(),""String_Node_Str"" + programType + ""String_Node_Str"");
        continue;
      }
      Id.Program programId=Id.Program.from(appId,programType,requestedObj.getProgramId());
      populateProgramInstances(requestedObj,spec,programId);
    }
    responder.sendJson(HttpResponseStatus.OK,args);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  JsonSyntaxException e) {
    responder.sendStatus(HttpResponseStatus.BAD_REQUEST);
  }
}"
6595,"/** 
 * Returns a list of workflows associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllWorkflows(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  programList(responder,namespaceId,ProgramType.WORKFLOW,store);
}","/** 
 * Returns a list of workflows associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllWorkflows(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  programList(responder,namespaceId,ProgramType.WORKFLOW,store);
}"
6596,"@POST @Path(""String_Node_Str"") public void performAction(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String type,@PathParam(""String_Node_Str"") String id,@PathParam(""String_Node_Str"") String action) throws NotFoundException, BadRequestException, IOException, NotImplementedException {
  if (type.equals(""String_Node_Str"")) {
    suspendResumeSchedule(responder,namespaceId,appId,id,action);
    return;
  }
  if (!isValidAction(action)) {
    throw new NotFoundException(String.format(""String_Node_Str"",action));
  }
  ProgramType programType;
  try {
    programType=ProgramType.valueOfCategoryName(type);
  }
 catch (  IllegalArgumentException e) {
    throw new BadRequestException(String.format(""String_Node_Str"",type),e);
  }
  if (""String_Node_Str"".equals(action) && !isDebugAllowed(programType)) {
    throw new NotImplementedException(String.format(""String_Node_Str"",programType));
  }
  Id.Program programId=Id.Program.from(namespaceId,appId,programType,id);
  startStopProgram(request,responder,programId,action);
}","@POST @Path(""String_Node_Str"") public void performAction(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String type,@PathParam(""String_Node_Str"") String id,@PathParam(""String_Node_Str"") String action) throws NotFoundException, BadRequestException, IOException, NotImplementedException, SchedulerException {
  if (type.equals(""String_Node_Str"")) {
    suspendResumeSchedule(responder,namespaceId,appId,id,action);
    return;
  }
  if (!isValidAction(action)) {
    throw new NotFoundException(String.format(""String_Node_Str"",action));
  }
  ProgramType programType;
  try {
    programType=ProgramType.valueOfCategoryName(type);
  }
 catch (  IllegalArgumentException e) {
    throw new BadRequestException(String.format(""String_Node_Str"",type),e);
  }
  if (""String_Node_Str"".equals(action) && !isDebugAllowed(programType)) {
    throw new NotImplementedException(String.format(""String_Node_Str"",programType));
  }
  Id.Program programId=Id.Program.from(namespaceId,appId,programType,id);
  startStopProgram(request,responder,programId,action);
}"
6597,"/** 
 * Increases number of instance for a flowlet within a flow.
 */
@PUT @Path(""String_Node_Str"") public synchronized void setFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId,@PathParam(""String_Node_Str"") String flowletId){
  int instances;
  try {
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
  }
 catch (  Throwable th) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.FLOW,flowId);
    int oldInstances=store.getFlowletInstances(programId,flowletId);
    if (oldInstances != instances) {
      FlowSpecification flowSpec=store.setFlowletInstances(programId,flowletId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(namespaceId,appId,flowId,ProgramType.FLOW,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(""String_Node_Str"",flowletId,""String_Node_Str"",String.valueOf(instances),""String_Node_Str"",GSON.toJson(flowSpec,FlowSpecification.class))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Increases number of instance for a flowlet within a flow.
 */
@PUT @Path(""String_Node_Str"") public synchronized void setFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId,@PathParam(""String_Node_Str"") String flowletId) throws ExecutionException, InterruptedException {
  int instances;
  try {
    try {
      instances=getInstances(request);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
catch (    JsonSyntaxException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
    if (instances < 1) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
      return;
    }
  }
 catch (  Throwable th) {
    responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    return;
  }
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.FLOW,flowId);
    int oldInstances=store.getFlowletInstances(programId,flowletId);
    if (oldInstances != instances) {
      FlowSpecification flowSpec=store.setFlowletInstances(programId,flowletId,instances);
      ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(namespaceId,appId,flowId,ProgramType.FLOW,runtimeService);
      if (runtimeInfo != null) {
        runtimeInfo.getController().command(ProgramOptionConstants.INSTANCES,ImmutableMap.of(""String_Node_Str"",flowletId,""String_Node_Str"",String.valueOf(instances),""String_Node_Str"",GSON.toJson(flowSpec,FlowSpecification.class))).get();
      }
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    throw e;
  }
}"
6598,"/** 
 * Returns a list of spark jobs associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllSpark(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  programList(responder,namespaceId,ProgramType.SPARK,store);
}","/** 
 * Returns a list of spark jobs associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllSpark(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  programList(responder,namespaceId,ProgramType.SPARK,store);
}"
6599,"/** 
 * Returns number of instances for a flowlet within a flow.
 */
@GET @Path(""String_Node_Str"") public void getFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId,@PathParam(""String_Node_Str"") String flowletId){
  try {
    int count=store.getFlowletInstances(Id.Program.from(namespaceId,appId,ProgramType.FLOW,flowId),flowletId);
    responder.sendJson(HttpResponseStatus.OK,new Instances(count));
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Returns number of instances for a flowlet within a flow.
 */
@GET @Path(""String_Node_Str"") public void getFlowletInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String flowId,@PathParam(""String_Node_Str"") String flowletId){
  try {
    int count=store.getFlowletInstances(Id.Program.from(namespaceId,appId,ProgramType.FLOW,flowId),flowletId);
    responder.sendJson(HttpResponseStatus.OK,new Instances(count));
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    if (respondIfElementNotFound(e,responder)) {
      return;
    }
    throw e;
  }
}"
6600,"/** 
 * Returns a list of flows associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllFlows(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  programList(responder,namespaceId,ProgramType.FLOW,store);
}","/** 
 * Returns a list of flows associated with a namespace.
 */
@GET @Path(""String_Node_Str"") public void getAllFlows(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId) throws Exception {
  programList(responder,namespaceId,ProgramType.FLOW,store);
}"
6601,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  File artifactFile=resolver.resolvePathToFile(arguments.get(ArgumentName.LOCAL_FILE_PATH.toString()));
  String name=arguments.getOptional(ArgumentName.ARTIFACT_NAME.toString());
  String version=arguments.getOptional(ArgumentName.ARTIFACT_VERSION.toString());
  Id.Artifact artifactId;
  if (name == null && version != null) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
 else   if (name != null && version == null) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
 else   if (name == null) {
    artifactId=Id.Artifact.parse(cliConfig.getCurrentNamespace(),artifactFile.getName());
  }
 else {
    artifactId=Id.Artifact.from(cliConfig.getCurrentNamespace(),name,version);
  }
  String configPath=arguments.getOptional(ArgumentName.ARTIFACT_CONFIG_FILE.toString());
  if (configPath == null) {
    artifactClient.add(artifactId.getNamespace(),artifactId.getName(),Files.newInputStreamSupplier(artifactFile),artifactId.getVersion().getVersion());
  }
 else {
    File configFile=resolver.resolvePathToFile(configPath);
    ArtifactConfig artifactConfig=ArtifactConfig.read(artifactId,artifactFile,configFile);
    artifactClient.add(artifactId.getNamespace(),artifactId.getName(),Files.newInputStreamSupplier(artifactFile),artifactId.getVersion().getVersion(),artifactConfig.getParents(),artifactConfig.getPlugins());
  }
  output.printf(""String_Node_Str"",artifactId.getName());
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  File artifactFile=resolver.resolvePathToFile(arguments.get(ArgumentName.LOCAL_FILE_PATH.toString()));
  String name=arguments.getOptional(ArgumentName.ARTIFACT_NAME.toString());
  String version=arguments.getOptional(ArgumentName.ARTIFACT_VERSION.toString());
  Id.Artifact artifactId;
  if (name == null && version != null) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
 else   if (name != null && version == null) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
 else   if (name == null) {
    artifactId=Id.Artifact.parse(cliConfig.getCurrentNamespace(),artifactFile.getName());
  }
 else {
    artifactId=Id.Artifact.from(cliConfig.getCurrentNamespace(),name,version);
  }
  String configPath=arguments.getOptional(ArgumentName.ARTIFACT_CONFIG_FILE.toString());
  if (configPath == null) {
    artifactClient.add(artifactId.getNamespace(),artifactId.getName(),Files.newInputStreamSupplier(artifactFile),artifactId.getVersion().getVersion());
  }
 else {
    File configFile=resolver.resolvePathToFile(configPath);
    ArtifactConfig artifactConfig=ArtifactConfig.read(artifactId,configFile,artifactFile);
    artifactClient.add(artifactId.getNamespace(),artifactId.getName(),Files.newInputStreamSupplier(artifactFile),artifactId.getVersion().getVersion(),artifactConfig.getParents(),artifactConfig.getPlugins());
  }
  output.printf(""String_Node_Str"",artifactId.getName());
}"
6602,"/** 
 * Adds the names of   {@link Dataset}s used by the flowlet.
 * @param datasets dataset names
 */
void useDatasets(Iterable<String> datasets);","/** 
 * Adds the names of   {@link Dataset}s used by this workflow action.
 * @param datasets dataset names
 */
void useDatasets(Iterable<String> datasets);"
6603,"public DefaultWorkflowActionSpecification(String className,String name,String description,Map<String,String> properties,Set<String> datasets){
  this.className=className;
  this.name=name;
  this.description=description;
  this.properties=Collections.unmodifiableMap(new HashMap<>(properties));
  this.datasets=Collections.unmodifiableSet(new HashSet<>(datasets));
}","/** 
 * Constructor be used by WorkflowActionConfigurer during workflow action configuration.
 */
public DefaultWorkflowActionSpecification(String className,String name,String description,Map<String,String> properties,Set<String> datasets){
  this.className=className;
  this.name=name;
  this.description=description;
  this.properties=Collections.unmodifiableMap(new HashMap<>(properties));
  this.datasets=Collections.unmodifiableSet(new HashSet<>(datasets));
}"
6604,"@Override public WorkflowActionSpecification configure(){
  Map<String,String> options=new HashMap<>();
  options.put(PROGRAM_TYPE,programType.name());
  options.put(PROGRAM_NAME,programName);
  return WorkflowActionSpecification.Builder.with().setName(name).setDescription(""String_Node_Str"" + programName).withOptions(options).build();
}","@Override public void configure(WorkflowActionConfigurer configurer){
  super.configure(configurer);
  setName(programName);
  setDescription(""String_Node_Str"" + programType.name() + ""String_Node_Str""+ programName);
  setProperties(ImmutableMap.of(PROGRAM_TYPE,programType.name(),PROGRAM_NAME,programName));
}"
6605,"public ProgramWorkflowAction(String name,String programName,SchedulableProgramType programType){
  this.name=name;
  this.programName=programName;
  this.programType=programType;
}","public ProgramWorkflowAction(String programName,SchedulableProgramType programType){
  this.programName=programName;
  this.programType=programType;
}"
6606,"private void executeNode(ApplicationSpecification appSpec,WorkflowNode node,InstantiatorFactory instantiator,ClassLoader classLoader,WorkflowToken token) throws Exception {
  WorkflowNodeType nodeType=node.getType();
  ((BasicWorkflowToken)token).setCurrentNode(node.getNodeId());
switch (nodeType) {
case ACTION:
    executeAction(appSpec,(WorkflowActionNode)node,instantiator,classLoader,token);
  break;
case FORK:
executeFork(appSpec,(WorkflowForkNode)node,instantiator,classLoader,token);
break;
case CONDITION:
executeCondition(appSpec,(WorkflowConditionNode)node,instantiator,classLoader,token);
break;
default :
break;
}
}","private void executeNode(ApplicationSpecification appSpec,WorkflowNode node,InstantiatorFactory instantiator,ClassLoader classLoader,WorkflowToken token) throws Exception {
  WorkflowNodeType nodeType=node.getType();
  ((BasicWorkflowToken)token).setCurrentNode(node.getNodeId());
switch (nodeType) {
case ACTION:
    executeAction((WorkflowActionNode)node,instantiator,classLoader,token);
  break;
case FORK:
executeFork(appSpec,(WorkflowForkNode)node,instantiator,classLoader,token);
break;
case CONDITION:
executeCondition(appSpec,(WorkflowConditionNode)node,instantiator,classLoader,token);
break;
default :
break;
}
}"
6607,"private WorkflowActionSpecification getActionSpecification(ApplicationSpecification appSpec,WorkflowActionNode node,SchedulableProgramType programType){
  WorkflowActionSpecification actionSpec;
  ScheduleProgramInfo actionInfo=node.getProgram();
switch (programType) {
case MAPREDUCE:
    MapReduceSpecification mapReduceSpec=appSpec.getMapReduce().get(actionInfo.getProgramName());
  String mapReduce=mapReduceSpec.getName();
actionSpec=new DefaultWorkflowActionSpecification(new ProgramWorkflowAction(mapReduce,mapReduce,SchedulableProgramType.MAPREDUCE));
break;
case SPARK:
SparkSpecification sparkSpec=appSpec.getSpark().get(actionInfo.getProgramName());
String spark=sparkSpec.getName();
actionSpec=new DefaultWorkflowActionSpecification(new ProgramWorkflowAction(spark,spark,SchedulableProgramType.SPARK));
break;
case CUSTOM_ACTION:
actionSpec=node.getActionSpecification();
break;
default :
LOG.error(""String_Node_Str"",actionInfo.getProgramType(),actionInfo.getProgramName());
throw new IllegalStateException(""String_Node_Str"");
}
return actionSpec;
}","private WorkflowActionSpecification getActionSpecification(WorkflowActionNode node,SchedulableProgramType programType){
  WorkflowActionSpecification actionSpec;
  ScheduleProgramInfo actionInfo=node.getProgram();
switch (programType) {
case MAPREDUCE:
case SPARK:
    actionSpec=DefaultWorkflowActionConfigurer.configureAction(new ProgramWorkflowAction(actionInfo.getProgramName(),programType));
  break;
case CUSTOM_ACTION:
actionSpec=node.getActionSpecification();
break;
default :
LOG.error(""String_Node_Str"",actionInfo.getProgramType(),actionInfo.getProgramName());
throw new IllegalStateException(""String_Node_Str"");
}
return actionSpec;
}"
6608,"private void executeAction(ApplicationSpecification appSpec,WorkflowActionNode node,InstantiatorFactory instantiator,final ClassLoader classLoader,WorkflowToken token) throws Exception {
  final SchedulableProgramType programType=node.getProgram().getProgramType();
  final WorkflowActionSpecification actionSpec=getActionSpecification(appSpec,node,programType);
  status.put(node.getNodeId(),node);
  final BasicWorkflowContext workflowContext=createWorkflowContext(actionSpec,token,node.getNodeId());
  final WorkflowAction action=initialize(actionSpec,classLoader,instantiator,workflowContext);
  ExecutorService executor=Executors.newSingleThreadExecutor(new ThreadFactoryBuilder().setNameFormat(""String_Node_Str"").build());
  try {
    Future<?> future=executor.submit(new Runnable(){
      @Override public void run(){
        setContextCombinedClassLoader(action);
        try {
          if (programType == SchedulableProgramType.CUSTOM_ACTION) {
            try {
              runInTransaction(action,workflowContext);
            }
 catch (            TransactionFailureException e) {
              throw Throwables.propagate(e);
            }
          }
 else {
            action.run();
          }
        }
  finally {
          try {
            destroyInTransaction(action,actionSpec,workflowContext);
          }
 catch (          TransactionFailureException e) {
            throw Throwables.propagate(e);
          }
        }
      }
    }
);
    future.get();
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",actionSpec,t);
    Throwables.propagateIfPossible(t,Exception.class);
    throw Throwables.propagate(t);
  }
 finally {
    executor.shutdownNow();
    executor.awaitTermination(Integer.MAX_VALUE,TimeUnit.NANOSECONDS);
    status.remove(node.getNodeId());
  }
  store.updateWorkflowToken(workflowId,runId.getId(),token);
}","private void executeAction(WorkflowActionNode node,InstantiatorFactory instantiator,final ClassLoader classLoader,WorkflowToken token) throws Exception {
  final SchedulableProgramType programType=node.getProgram().getProgramType();
  final WorkflowActionSpecification actionSpec=getActionSpecification(node,programType);
  status.put(node.getNodeId(),node);
  final BasicWorkflowContext workflowContext=createWorkflowContext(actionSpec,token,node.getNodeId());
  final WorkflowAction action=initialize(actionSpec,classLoader,instantiator,workflowContext);
  ExecutorService executor=Executors.newSingleThreadExecutor(new ThreadFactoryBuilder().setNameFormat(""String_Node_Str"").build());
  try {
    Future<?> future=executor.submit(new Runnable(){
      @Override public void run(){
        setContextCombinedClassLoader(action);
        try {
          if (programType == SchedulableProgramType.CUSTOM_ACTION) {
            try {
              runInTransaction(action,workflowContext);
            }
 catch (            TransactionFailureException e) {
              throw Throwables.propagate(e);
            }
          }
 else {
            action.run();
          }
        }
  finally {
          try {
            destroyInTransaction(action,actionSpec,workflowContext);
          }
 catch (          TransactionFailureException e) {
            throw Throwables.propagate(e);
          }
        }
      }
    }
);
    future.get();
  }
 catch (  Throwable t) {
    LOG.error(""String_Node_Str"",actionSpec,t);
    Throwables.propagateIfPossible(t,Exception.class);
    throw Throwables.propagate(t);
  }
 finally {
    executor.shutdownNow();
    executor.awaitTermination(Integer.MAX_VALUE,TimeUnit.NANOSECONDS);
    status.remove(node.getNodeId());
  }
  store.updateWorkflowToken(workflowId,runId.getId(),token);
}"
6609,"public DefaultWorkflowActionConfigurer(WorkflowAction workflowAction){
  this.name=workflowAction.getClass().getSimpleName();
  this.description=""String_Node_Str"";
  this.className=workflowAction.getClass().getName();
  this.propertyFields=new HashMap<>();
  this.datasetFields=new HashSet<>();
  this.properties=new HashMap<>();
  this.datasets=new HashSet<>();
  Reflections.visit(workflowAction,workflowAction.getClass(),new PropertyFieldExtractor(propertyFields),new DataSetFieldExtractor(datasetFields));
}","private DefaultWorkflowActionConfigurer(WorkflowAction workflowAction){
  this.name=workflowAction.getClass().getSimpleName();
  this.description=""String_Node_Str"";
  this.className=workflowAction.getClass().getName();
  this.propertyFields=new HashMap<>();
  this.datasetFields=new HashSet<>();
  this.properties=new HashMap<>();
  this.datasets=new HashSet<>();
  Reflections.visit(workflowAction,workflowAction.getClass(),new PropertyFieldExtractor(propertyFields),new DataSetFieldExtractor(datasetFields));
}"
6610,"public DefaultWorkflowActionSpecification createSpecification(){
  Map<String,String> properties=new HashMap<>(this.properties);
  properties.putAll(propertyFields);
  Set<String> datasets=new HashSet<>(this.datasets);
  datasets.addAll(datasetFields);
  return new DefaultWorkflowActionSpecification(className,name,description,properties,datasets);
}","private DefaultWorkflowActionSpecification createSpecification(){
  Map<String,String> properties=new HashMap<>(this.properties);
  properties.putAll(propertyFields);
  Set<String> datasets=new HashSet<>(this.datasets);
  datasets.addAll(datasetFields);
  return new DefaultWorkflowActionSpecification(className,name,description,properties,datasets);
}"
6611,"static WorkflowNode createWorkflowCustomActionNode(WorkflowAction action){
  Preconditions.checkArgument(action != null,""String_Node_Str"");
  WorkflowActionSpecification spec;
  if (action instanceof AbstractWorkflowAction) {
    DefaultWorkflowActionConfigurer configurer=new DefaultWorkflowActionConfigurer(action);
    ((AbstractWorkflowAction)action).configure(configurer);
    spec=configurer.createSpecification();
  }
 else {
    spec=action.configure();
  }
  return new WorkflowActionNode(spec.getName(),spec);
}","static WorkflowNode createWorkflowCustomActionNode(WorkflowAction action){
  Preconditions.checkArgument(action != null,""String_Node_Str"");
  WorkflowActionSpecification spec;
  if (action instanceof AbstractWorkflowAction) {
    spec=DefaultWorkflowActionConfigurer.configureAction((AbstractWorkflowAction)action);
  }
 else {
    spec=new DefaultWorkflowActionSpecification(action.configure(),action);
  }
  return new WorkflowActionNode(spec.getName(),spec);
}"
6612,"public boolean isSnapshot(){
  return suffix != null && ""String_Node_Str"".equals(suffix.toLowerCase());
}","public boolean isSnapshot(){
  return suffix != null && !suffix.isEmpty() && suffix.toLowerCase().startsWith(""String_Node_Str"");
}"
6613,"@Override public Module getStandaloneModules(){
  return Modules.combine(new AppFabricServiceModule(StreamHandler.class,StreamFetchHandler.class,StreamViewHttpHandler.class),new ConfigStoreModule().getStandaloneModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(AppFabricServer.class).to(StandaloneAppFabricServer.class).in(Scopes.SINGLETON);
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      bind(MRJobInfoFetcher.class).to(LocalMRJobInfoFetcher.class);
      MapBinder<String,MasterServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,MasterServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(InMemoryLogSaverServiceManager.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(InMemoryTransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(InMemoryMetricsProcessorServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(InMemoryMetricsServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(AppFabricServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(InMemoryStreamServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(InMemoryDatasetExecutorServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(InMemoryExploreServiceManager.class);
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      servicesNamesBinder.addBinding().toInstance(Constants.Service.STREAMS);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      handlerHookNamesBinder.addBinding().toInstance(Constants.Stream.STREAM_HANDLER);
    }
  }
);
}","@Override public Module getStandaloneModules(){
  return Modules.combine(new AppFabricServiceModule(StreamHandler.class,StreamFetchHandler.class,StreamViewHttpHandler.class),new ConfigStoreModule().getStandaloneModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(AppFabricServer.class).to(StandaloneAppFabricServer.class).in(Scopes.SINGLETON);
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      bind(MRJobInfoFetcher.class).to(LocalMRJobInfoFetcher.class);
      MapBinder<String,MasterServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,MasterServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(InMemoryLogSaverServiceManager.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(InMemoryTransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(InMemoryMetricsProcessorServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(InMemoryMetricsServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(AppFabricServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(InMemoryStreamServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(InMemoryDatasetExecutorServiceManager.class);
      mapBinder.addBinding(Constants.Service.METADATA_SERVICE).to(InMemoryMetadataServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(InMemoryExploreServiceManager.class);
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      servicesNamesBinder.addBinding().toInstance(Constants.Service.STREAMS);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      handlerHookNamesBinder.addBinding().toInstance(Constants.Stream.STREAM_HANDLER);
    }
  }
);
}"
6614,"@Override public Module getDistributedModules(){
  return Modules.combine(new AppFabricServiceModule(),new ConfigStoreModule().getDistributedModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(DistributedSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      bind(MRJobInfoFetcher.class).to(DistributedMRJobInfoFetcher.class);
      MapBinder<String,MasterServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,MasterServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(LogSaverStatusServiceManager.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(TransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(MetricsProcessorStatusServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(MetricsServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(AppFabricServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(StreamServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(DatasetExecutorServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(ExploreServiceManager.class);
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
    }
  }
);
}","@Override public Module getDistributedModules(){
  return Modules.combine(new AppFabricServiceModule(),new ConfigStoreModule().getDistributedModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(DistributedSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      bind(MRJobInfoFetcher.class).to(DistributedMRJobInfoFetcher.class);
      MapBinder<String,MasterServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,MasterServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(LogSaverStatusServiceManager.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(TransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(MetricsProcessorStatusServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(MetricsServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(AppFabricServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(StreamServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(DatasetExecutorServiceManager.class);
      mapBinder.addBinding(Constants.Service.METADATA_SERVICE).to(MetadataServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(ExploreServiceManager.class);
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
    }
  }
);
}"
6615,"@Override public Module getInMemoryModules(){
  return Modules.combine(new AppFabricServiceModule(StreamHandler.class,StreamFetchHandler.class,StreamViewHttpHandler.class),new ConfigStoreModule().getInMemoryModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      bind(MRJobInfoFetcher.class).to(LocalMRJobInfoFetcher.class);
      MapBinder<String,MasterServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,MasterServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(InMemoryLogSaverServiceManager.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(InMemoryTransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(InMemoryMetricsProcessorServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(InMemoryMetricsServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(AppFabricServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(InMemoryStreamServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(InMemoryDatasetExecutorServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(InMemoryExploreServiceManager.class);
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      servicesNamesBinder.addBinding().toInstance(Constants.Service.STREAMS);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      handlerHookNamesBinder.addBinding().toInstance(Constants.Stream.STREAM_HANDLER);
    }
  }
);
}","@Override public Module getInMemoryModules(){
  return Modules.combine(new AppFabricServiceModule(StreamHandler.class,StreamFetchHandler.class,StreamViewHttpHandler.class),new ConfigStoreModule().getInMemoryModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(SchedulerService.class).to(LocalSchedulerService.class).in(Scopes.SINGLETON);
      bind(Scheduler.class).to(SchedulerService.class);
      bind(MRJobInfoFetcher.class).to(LocalMRJobInfoFetcher.class);
      MapBinder<String,MasterServiceManager> mapBinder=MapBinder.newMapBinder(binder(),String.class,MasterServiceManager.class);
      mapBinder.addBinding(Constants.Service.LOGSAVER).to(InMemoryLogSaverServiceManager.class);
      mapBinder.addBinding(Constants.Service.TRANSACTION).to(InMemoryTransactionServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS_PROCESSOR).to(InMemoryMetricsProcessorServiceManager.class);
      mapBinder.addBinding(Constants.Service.METRICS).to(InMemoryMetricsServiceManager.class);
      mapBinder.addBinding(Constants.Service.APP_FABRIC_HTTP).to(AppFabricServiceManager.class);
      mapBinder.addBinding(Constants.Service.STREAMS).to(InMemoryStreamServiceManager.class);
      mapBinder.addBinding(Constants.Service.DATASET_EXECUTOR).to(InMemoryDatasetExecutorServiceManager.class);
      mapBinder.addBinding(Constants.Service.METADATA_SERVICE).to(InMemoryMetadataServiceManager.class);
      mapBinder.addBinding(Constants.Service.EXPLORE_HTTP_USER_SERVICE).to(InMemoryExploreServiceManager.class);
      Multibinder<String> servicesNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      servicesNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      servicesNamesBinder.addBinding().toInstance(Constants.Service.STREAMS);
      Multibinder<String> handlerHookNamesBinder=Multibinder.newSetBinder(binder(),String.class,Names.named(""String_Node_Str""));
      handlerHookNamesBinder.addBinding().toInstance(Constants.Service.APP_FABRIC_HTTP);
      handlerHookNamesBinder.addBinding().toInstance(Constants.Stream.STREAM_HANDLER);
    }
  }
);
}"
6616,"/** 
 * @return programs that were running between given start and end time.
 */
Set<RunId> getRunningInRange(final long startTimeInSecs,final long endTimeInSecs);","/** 
 * @return programs that were running between given start and end time.
 */
Set<RunId> getRunningInRange(long startTimeInSecs,long endTimeInSecs);"
6617,"public static void setupDatasets(DatasetFramework dsFramework) throws DatasetManagementException, IOException, ServiceUnavailableException {
  dsFramework.addInstance(Table.class.getName(),Id.DatasetInstance.from(Id.Namespace.SYSTEM,Constants.ConfigStore.CONFIG_TABLE),DatasetProperties.EMPTY);
}","public static void setupDatasets(DatasetFramework dsFramework) throws DatasetManagementException, IOException {
  dsFramework.addInstance(Table.class.getName(),Id.DatasetInstance.from(Id.Namespace.SYSTEM,Constants.ConfigStore.CONFIG_TABLE),DatasetProperties.EMPTY);
}"
6618,"/** 
 * Stops a Program.
 */
private AppFabricServiceStatus stop(Id.Program identifier,@Nullable String runId) throws NotFoundException, BadRequestException {
  ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(identifier,runId);
  if (runtimeInfo == null) {
    if (!store.applicationExists(identifier.getApplication())) {
      throw new ApplicationNotFoundException(identifier.getApplication());
    }
 else     if (!store.programExists(identifier)) {
      throw new ProgramNotFoundException(identifier);
    }
 else     if (runId == null) {
      throw new BadRequestException(""String_Node_Str"");
    }
 else {
      throw new NotFoundException(new Id.Run(identifier,runId));
    }
  }
  try {
    ProgramController controller=runtimeInfo.getController();
    controller.stop().get();
    return AppFabricServiceStatus.OK;
  }
 catch (  Throwable throwable) {
    LOG.warn(throwable.getMessage(),throwable);
    return AppFabricServiceStatus.INTERNAL_ERROR;
  }
}","/** 
 * Stops a Program.
 */
private AppFabricServiceStatus stop(Id.Program identifier,@Nullable String runId) throws NotFoundException, BadRequestException {
  ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(identifier,runId);
  if (runtimeInfo == null) {
    if (!store.applicationExists(identifier.getApplication())) {
      throw new ApplicationNotFoundException(identifier.getApplication());
    }
 else     if (!store.programExists(identifier)) {
      throw new ProgramNotFoundException(identifier);
    }
 else     if (runId != null) {
      Id.Run programRunId=new Id.Run(identifier,runId);
      RunRecordMeta runRecord=store.getRun(identifier,runId);
      if (runRecord != null && runRecord.getProperties().containsKey(""String_Node_Str"") && runRecord.getStatus().equals(ProgramRunStatus.RUNNING)) {
        String workflowRunId=runRecord.getProperties().get(""String_Node_Str"");
        throw new BadRequestException(String.format(""String_Node_Str"" + ""String_Node_Str"",programRunId,workflowRunId));
      }
      throw new NotFoundException(programRunId);
    }
    throw new BadRequestException(String.format(""String_Node_Str"",identifier));
  }
  try {
    ProgramController controller=runtimeInfo.getController();
    controller.stop().get();
    return AppFabricServiceStatus.OK;
  }
 catch (  Throwable throwable) {
    LOG.warn(throwable.getMessage(),throwable);
    return AppFabricServiceStatus.INTERNAL_ERROR;
  }
}"
6619,"private String getSpecJson(Application app,final String bundleVersion,final String configString) throws IllegalAccessException, InstantiationException, IOException {
  File tempDir=DirUtils.createTempDir(baseUnpackDir);
  DefaultAppConfigurer configurer;
  try (PluginInstantiator pluginInstantiator=new PluginInstantiator(cConf,app.getClass().getClassLoader(),tempDir)){
    configurer=artifactId == null ? new DefaultAppConfigurer(app,configString) : new DefaultAppConfigurer(artifactId,app,configString,artifactRepository,pluginInstantiator);
    Config appConfig;
    TypeToken typeToken=TypeToken.of(app.getClass());
    TypeToken<?> configToken=typeToken.resolveType(Application.class.getTypeParameters()[0]);
    if (Strings.isNullOrEmpty(configString)) {
      appConfig=(Config)configToken.getRawType().newInstance();
    }
 else {
      try {
        appConfig=GSON.fromJson(configString,configToken.getType());
      }
 catch (      JsonSyntaxException e) {
        throw new IllegalArgumentException(""String_Node_Str"",e);
      }
    }
    app.configure(configurer,new DefaultApplicationContext(appConfig));
  }
  finally {
    DirUtils.deleteDirectoryContents(tempDir);
  }
  ApplicationSpecification specification=configurer.createSpecification();
  return ApplicationSpecificationAdapter.create(new ReflectionSchemaGenerator()).toJson(specification);
}","private String getSpecJson(Application app,final String configString) throws IllegalAccessException, InstantiationException, IOException {
  File tempDir=DirUtils.createTempDir(baseUnpackDir);
  DefaultAppConfigurer configurer;
  try (PluginInstantiator pluginInstantiator=new PluginInstantiator(cConf,app.getClass().getClassLoader(),tempDir)){
    configurer=artifactId == null ? new DefaultAppConfigurer(app,configString) : new DefaultAppConfigurer(artifactId,app,configString,artifactRepository,pluginInstantiator);
    Config appConfig;
    Type configType=Artifacts.getConfigType(app.getClass());
    if (Strings.isNullOrEmpty(configString)) {
      appConfig=((Class<? extends Config>)configType).newInstance();
    }
 else {
      try {
        appConfig=GSON.fromJson(configString,configType);
      }
 catch (      JsonSyntaxException e) {
        throw new IllegalArgumentException(""String_Node_Str"",e);
      }
    }
    app.configure(configurer,new DefaultApplicationContext(appConfig));
  }
  finally {
    DirUtils.deleteDirectoryContents(tempDir);
  }
  ApplicationSpecification specification=configurer.createSpecification();
  return ApplicationSpecificationAdapter.create(new ReflectionSchemaGenerator()).toJson(specification);
}"
6620,"private ConfigResponse createResponse(Application app,String bundleVersion) throws InstantiationException, IllegalAccessException, IOException {
  String specJson=getSpecJson(app,bundleVersion,configString);
  return new DefaultConfigResponse(0,CharStreams.newReaderSupplier(specJson));
}","private ConfigResponse createResponse(Application app) throws InstantiationException, IllegalAccessException, IOException {
  String specJson=getSpecJson(app,configString);
  return new DefaultConfigResponse(0,CharStreams.newReaderSupplier(specJson));
}"
6621,"private void readAppClassName() throws IOException {
  Manifest manifest=BundleJarUtil.getManifest(artifact);
  Preconditions.checkArgument(manifest != null,""String_Node_Str"",artifact.toURI());
  Preconditions.checkArgument(manifest.getMainAttributes() != null,""String_Node_Str"",artifact.toURI());
  appClassName=manifest.getMainAttributes().getValue(ManifestFields.MAIN_CLASS);
  Preconditions.checkArgument(appClassName != null && !appClassName.isEmpty(),""String_Node_Str"");
  version=manifest.getMainAttributes().getValue(ManifestFields.BUNDLE_VERSION);
}","private void readAppClassName() throws IOException {
  Manifest manifest=BundleJarUtil.getManifest(artifact);
  Preconditions.checkArgument(manifest != null,""String_Node_Str"",artifact.toURI());
  Preconditions.checkArgument(manifest.getMainAttributes() != null,""String_Node_Str"",artifact.toURI());
  appClassName=manifest.getMainAttributes().getValue(ManifestFields.MAIN_CLASS);
  Preconditions.checkArgument(appClassName != null && !appClassName.isEmpty(),""String_Node_Str"");
}"
6622,"/** 
 * Executes the <code>Application.configure</code> within the same JVM. <p> This method could be dangerous and should be used only in standalone mode. </p>
 * @return A instance of {@link ListenableFuture}.
 */
@Override public ListenableFuture<ConfigResponse> config(){
  SettableFuture<ConfigResponse> result=SettableFuture.create();
  try {
    if (appClassName == null) {
      readAppClassName();
    }
    try (CloseableClassLoader artifactClassLoader=artifactClassLoaderFactory.createClassLoader(artifact)){
      Object appMain=artifactClassLoader.loadClass(appClassName).newInstance();
      if (!(appMain instanceof Application)) {
        throw new IllegalStateException(String.format(""String_Node_Str"",appMain.getClass().getName()));
      }
      Application app=(Application)appMain;
      ConfigResponse response=createResponse(app,version);
      result.set(response);
    }
     return result;
  }
 catch (  Throwable t) {
    LOG.error(t.getMessage(),t);
    return Futures.immediateFailedFuture(t);
  }
}","/** 
 * Executes the <code>Application.configure</code> within the same JVM. <p> This method could be dangerous and should be used only in standalone mode. </p>
 * @return A instance of {@link ListenableFuture}.
 */
@Override public ListenableFuture<ConfigResponse> config(){
  SettableFuture<ConfigResponse> result=SettableFuture.create();
  try {
    if (appClassName == null) {
      readAppClassName();
    }
    try (CloseableClassLoader artifactClassLoader=artifactClassLoaderFactory.createClassLoader(artifact)){
      Object appMain=artifactClassLoader.loadClass(appClassName).newInstance();
      if (!(appMain instanceof Application)) {
        throw new IllegalStateException(String.format(""String_Node_Str"",appMain.getClass().getName()));
      }
      Application app=(Application)appMain;
      ConfigResponse response=createResponse(app);
      result.set(response);
    }
     return result;
  }
 catch (  Throwable t) {
    LOG.error(t.getMessage(),t);
    return Futures.immediateFailedFuture(t);
  }
}"
6623,"protected void verifyData(Id.Application appId,ApplicationSpecification specification) throws DatasetManagementException, ServiceUnavailableException {
  VerifyResult result;
  for (  DatasetCreationSpec dataSetCreateSpec : specification.getDatasets().values()) {
    result=getVerifier(DatasetCreationSpec.class).verify(appId,dataSetCreateSpec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
    String dsName=dataSetCreateSpec.getInstanceName();
    Id.DatasetInstance datasetInstanceId=Id.DatasetInstance.from(appId.getNamespace(),dsName);
    DatasetSpecification existingSpec=dsFramework.getDatasetSpec(datasetInstanceId);
    if (existingSpec != null && !existingSpec.getType().equals(dataSetCreateSpec.getTypeName())) {
      throw new DataSetException(String.format(""String_Node_Str"",dsName,dataSetCreateSpec.getTypeName()));
    }
  }
  for (  StreamSpecification spec : specification.getStreams().values()) {
    result=getVerifier(StreamSpecification.class).verify(appId,spec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
  }
}","protected void verifyData(Id.Application appId,ApplicationSpecification specification) throws DatasetManagementException {
  VerifyResult result;
  for (  DatasetCreationSpec dataSetCreateSpec : specification.getDatasets().values()) {
    result=getVerifier(DatasetCreationSpec.class).verify(appId,dataSetCreateSpec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
    String dsName=dataSetCreateSpec.getInstanceName();
    Id.DatasetInstance datasetInstanceId=Id.DatasetInstance.from(appId.getNamespace(),dsName);
    DatasetSpecification existingSpec=dsFramework.getDatasetSpec(datasetInstanceId);
    if (existingSpec != null && !existingSpec.getType().equals(dataSetCreateSpec.getTypeName())) {
      throw new DataSetException(String.format(""String_Node_Str"",dsName,dataSetCreateSpec.getTypeName()));
    }
  }
  for (  StreamSpecification spec : specification.getStreams().values()) {
    result=getVerifier(StreamSpecification.class).verify(appId,spec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
  }
}"
6624,"@Override public synchronized void deleteDatasets(Id.Namespace namespaceId) throws NamespaceNotFoundException, NamespaceCannotBeDeletedException {
  if (!exists(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (checkProgramsRunning(namespaceId)) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  try {
    dsFramework.deleteAllInstances(namespaceId);
  }
 catch (  DatasetManagementException|IOException|ServiceUnavailableException e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
  LOG.debug(""String_Node_Str"",namespaceId);
}","@Override public synchronized void deleteDatasets(Id.Namespace namespaceId) throws NamespaceNotFoundException, NamespaceCannotBeDeletedException {
  if (!exists(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (checkProgramsRunning(namespaceId)) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  try {
    dsFramework.deleteAllInstances(namespaceId);
  }
 catch (  DatasetManagementException|IOException e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
  LOG.debug(""String_Node_Str"",namespaceId);
}"
6625,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
public synchronized void create(NamespaceMeta metadata) throws NamespaceCannotBeCreatedException, NamespaceAlreadyExistsException {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  Id.Namespace namespace=Id.Namespace.from(metadata.getName());
  if (exists(Id.Namespace.from(metadata.getName()))) {
    throw new NamespaceAlreadyExistsException(namespace);
  }
  try {
    dsFramework.createNamespace(Id.Namespace.from(metadata.getName()));
  }
 catch (  DatasetManagementException|ServiceUnavailableException e) {
    throw new NamespaceCannotBeCreatedException(namespace,e);
  }
  store.createNamespace(metadata);
}","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
public synchronized void create(NamespaceMeta metadata) throws NamespaceCannotBeCreatedException, NamespaceAlreadyExistsException {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  Id.Namespace namespace=Id.Namespace.from(metadata.getName());
  if (exists(Id.Namespace.from(metadata.getName()))) {
    throw new NamespaceAlreadyExistsException(namespace);
  }
  try {
    dsFramework.createNamespace(Id.Namespace.from(metadata.getName()));
  }
 catch (  DatasetManagementException e) {
    throw new NamespaceCannotBeCreatedException(namespace,e);
  }
  store.createNamespace(metadata);
}"
6626,"List<BusinessMetadataRecord> executeSearchOnColumns(String column,String searchValue,MetadataSearchTargetType type){
  List<BusinessMetadataRecord> results=new LinkedList<>();
  byte[] startKey=Bytes.toBytes(searchValue);
  byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
  Scanner scanner=indexedTable.scanByIndex(Bytes.toBytes(column),startKey,stopKey);
  try {
    Row next;
    while ((next=scanner.next()) != null) {
      String rowValue=next.getString(VALUE_COLUMN);
      if (rowValue == null) {
        continue;
      }
      final byte[] rowKey=next.getRow();
      String targetType=getTargetType(rowKey);
      if ((type != MetadataSearchTargetType.ALL) && (!targetType.equals(type.getInternalName()))) {
        continue;
      }
      Id.NamespacedId targetId=getNamespaceIdFromKey(targetType,new MDSKey(rowKey));
      String key=getMetadataKey(targetType,rowKey);
      String value=Bytes.toString(next.get(Bytes.toBytes(BusinessMetadataDataset.VALUE_COLUMN)));
      BusinessMetadataRecord record=new BusinessMetadataRecord(targetId,key,value);
      results.add(record);
    }
  }
  finally {
    scanner.close();
  }
  return results;
}","List<BusinessMetadataRecord> executeSearchOnColumns(String column,String searchValue,MetadataSearchTargetType type){
  List<BusinessMetadataRecord> results=new LinkedList<>();
  byte[] startKey=Bytes.toBytes(searchValue.toLowerCase());
  byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
  Scanner scanner=indexedTable.scanByIndex(Bytes.toBytes(column),startKey,stopKey);
  try {
    Row next;
    while ((next=scanner.next()) != null) {
      String rowValue=next.getString(VALUE_COLUMN);
      if (rowValue == null) {
        continue;
      }
      final byte[] rowKey=next.getRow();
      String targetType=getTargetType(rowKey);
      if ((type != MetadataSearchTargetType.ALL) && (!targetType.equals(type.getInternalName()))) {
        continue;
      }
      Id.NamespacedId targetId=getNamespaceIdFromKey(targetType,new MDSKey(rowKey));
      String key=getMetadataKey(targetType,rowKey);
      String value=Bytes.toString(next.get(Bytes.toBytes(BusinessMetadataDataset.VALUE_COLUMN)));
      BusinessMetadataRecord record=new BusinessMetadataRecord(targetId,key,value);
      results.add(record);
    }
  }
  finally {
    scanner.close();
  }
  return results;
}"
6627,"/** 
 * Find the instance of   {@link BusinessMetadataRecord} based on key.
 * @param value The metadata value to be found
 * @param type The target type of objects to search from
 * @return The {@link Iterable} of {@link BusinessMetadataRecord} that fit the value
 */
public List<BusinessMetadataRecord> findBusinessMetadataOnValue(String value,MetadataSearchTargetType type){
  return executeSearchOnColumns(BusinessMetadataDataset.VALUE_COLUMN,value,type);
}","/** 
 * Find the instance of   {@link BusinessMetadataRecord} based on key.
 * @param value The metadata value to be found
 * @param type The target type of objects to search from
 * @return The {@link Iterable} of {@link BusinessMetadataRecord} that fit the value
 */
public List<BusinessMetadataRecord> findBusinessMetadataOnValue(String value,MetadataSearchTargetType type){
  return executeSearchOnColumns(BusinessMetadataDataset.CASE_INSENSITIVE_VALUE_COLUMN,value,type);
}"
6628,"private void write(MDSKey id,BusinessMetadataRecord record){
  Put put=new Put(id.getKey());
  put.add(Bytes.toBytes(KEYVALUE_COLUMN),Bytes.toBytes(record.getKey() + KEYVALUE_SEPARATOR + record.getValue()));
  put.add(Bytes.toBytes(VALUE_COLUMN),Bytes.toBytes(record.getValue()));
  indexedTable.put(put);
}","private void write(MDSKey id,BusinessMetadataRecord record){
  Put put=new Put(id.getKey());
  put.add(Bytes.toBytes(KEYVALUE_COLUMN),Bytes.toBytes(record.getKey().toLowerCase() + KEYVALUE_SEPARATOR + record.getValue().toLowerCase()));
  put.add(Bytes.toBytes(CASE_INSENSITIVE_VALUE_COLUMN),Bytes.toBytes(record.getValue().toLowerCase()));
  put.add(Bytes.toBytes(VALUE_COLUMN),Bytes.toBytes(record.getValue()));
  indexedTable.put(put);
}"
6629,"@Test public void testSearchOnValue() throws Exception {
  BusinessMetadataRecord record=new BusinessMetadataRecord(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  BusinessMetadataRecord result=results.get(0);
  Assert.assertEquals(record,result);
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results2=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(2,results2.size());
  for (  BusinessMetadataRecord result2 : results2) {
    Assert.assertEquals(""String_Node_Str"",result2.getValue());
  }
}","@Test public void testSearchOnValue() throws Exception {
  BusinessMetadataRecord record=new BusinessMetadataRecord(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  results=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  BusinessMetadataRecord result=results.get(0);
  Assert.assertEquals(record,result);
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results2=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(2,results2.size());
  for (  BusinessMetadataRecord result2 : results2) {
    Assert.assertEquals(""String_Node_Str"",result2.getValue());
  }
}"
6630,"@Override public Set<MetadataSearchResultRecord> searchMetadata(String searchQuery,@Nullable final MetadataSearchTargetType type) throws NotFoundException {
  Iterable<BusinessMetadataRecord> results;
  if (type == null) {
    results=businessMds.searchMetadata(searchQuery);
  }
 else {
    results=businessMds.searchMetadataOnType(searchQuery,type);
  }
  Set<MetadataSearchResultRecord> searchResultRecords=new LinkedHashSet<>();
  for (  BusinessMetadataRecord bmr : results) {
    MetadataSearchTargetType finalType=type;
    if (finalType == null) {
      Id.NamespacedId namespacedId=bmr.getTargetId();
      String targetType=getTargetType(namespacedId);
      finalType=getMetadataSearchTarget(targetType);
    }
    MetadataSearchResultRecord msr=new MetadataSearchResultRecord(bmr.getTargetId(),finalType);
    searchResultRecords.add(msr);
  }
  return searchResultRecords;
}","@Override public Set<MetadataSearchResultRecord> searchMetadata(String searchQuery,@Nullable final MetadataSearchTargetType type) throws NotFoundException {
  Iterable<BusinessMetadataRecord> results;
  if (type == null) {
    results=businessMds.searchMetadata(searchQuery);
  }
 else {
    results=businessMds.searchMetadataOnType(searchQuery,type);
  }
  Set<MetadataSearchResultRecord> searchResultRecords=new LinkedHashSet<>();
  for (  BusinessMetadataRecord bmr : results) {
    MetadataSearchTargetType finalType=type;
    if (finalType == null || finalType == MetadataSearchTargetType.ALL) {
      Id.NamespacedId namespacedId=bmr.getTargetId();
      String targetType=getTargetType(namespacedId);
      finalType=getMetadataSearchTarget(targetType);
    }
    MetadataSearchResultRecord msr=new MetadataSearchResultRecord(bmr.getTargetId(),finalType);
    searchResultRecords.add(msr);
  }
  return searchResultRecords;
}"
6631,"@GET @Path(""String_Node_Str"") public void searchMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@QueryParam(""String_Node_Str"") String searchQuery,@QueryParam(""String_Node_Str"") String target) throws Exception {
  MetadataSearchTargetType metadataSearchTargetType;
  if (target != null) {
    metadataSearchTargetType=MetadataSearchTargetType.valueOf(target.toUpperCase());
  }
 else {
    metadataSearchTargetType=null;
  }
  Set<MetadataSearchResultRecord> results=metadataAdmin.searchMetadata(searchQuery,metadataSearchTargetType);
  responder.sendJson(HttpResponseStatus.OK,results);
}","@GET @Path(""String_Node_Str"") public void searchMetadata(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@QueryParam(""String_Node_Str"") String searchQuery,@QueryParam(""String_Node_Str"") String target) throws Exception {
  MetadataSearchTargetType metadataSearchTargetType;
  if (target != null) {
    metadataSearchTargetType=MetadataSearchTargetType.valueOf(target.toUpperCase());
  }
 else {
    metadataSearchTargetType=null;
  }
  Set<MetadataSearchResultRecord> results=metadataAdmin.searchMetadata(searchQuery,metadataSearchTargetType);
  responder.sendJson(HttpResponseStatus.OK,results,SET_METADATA_SEARCH_RESULT_TYPE,GSON);
}"
6632,"@Test public void testTags() throws IOException {
  Assert.assertEquals(400,addTags(application).getResponseCode());
  Set<String> appTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(application,appTags).getResponseCode());
  Assert.assertEquals(400,addTags(pingService).getResponseCode());
  Set<String> serviceTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(pingService,serviceTags).getResponseCode());
  Assert.assertEquals(400,addTags(myds).getResponseCode());
  Set<String> datasetTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(myds,datasetTags).getResponseCode());
  Assert.assertEquals(400,addTags(mystream).getResponseCode());
  Set<String> streamTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(mystream,streamTags).getResponseCode());
  Set<String> tags=getTags(application);
  Assert.assertTrue(tags.containsAll(appTags));
  Assert.assertTrue(appTags.containsAll(tags));
  tags=getTags(pingService);
  Assert.assertTrue(tags.containsAll(serviceTags));
  Assert.assertTrue(serviceTags.containsAll(tags));
  tags=getTags(myds);
  Assert.assertTrue(tags.containsAll(datasetTags));
  Assert.assertTrue(datasetTags.containsAll(tags));
  tags=getTags(mystream);
  Assert.assertTrue(tags.containsAll(streamTags));
  Assert.assertTrue(streamTags.containsAll(tags));
  removeTags(application,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.of(""String_Node_Str""),getTags(application));
  removeTags(pingService);
  Assert.assertTrue(getTags(pingService).isEmpty());
  removeTags(pingService);
  Assert.assertTrue(getTags(pingService).isEmpty());
  removeTags(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.of(""String_Node_Str""),getTags(myds));
  removeTags(mystream,""String_Node_Str"");
  removeTags(mystream,""String_Node_Str"");
  Assert.assertTrue(getTags(mystream).isEmpty());
  removeTags(application);
  removeTags(pingService);
  removeTags(myds);
  removeTags(mystream);
  Assert.assertTrue(getTags(application).isEmpty());
  Assert.assertTrue(getTags(pingService).isEmpty());
  Assert.assertTrue(getTags(myds).isEmpty());
  Assert.assertTrue(getTags(mystream).isEmpty());
  Assert.assertEquals(404,addTags(nonExistingApp,appTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingService,serviceTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingDataset,datasetTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingStream,streamTags).getResponseCode());
}","@Test public void testTags() throws IOException {
  Assert.assertEquals(400,addTags(application).getResponseCode());
  Set<String> appTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(application,appTags).getResponseCode());
  Assert.assertEquals(400,addTags(pingService).getResponseCode());
  Set<String> serviceTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(pingService,serviceTags).getResponseCode());
  Assert.assertEquals(400,addTags(myds).getResponseCode());
  Set<String> datasetTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(myds,datasetTags).getResponseCode());
  Assert.assertEquals(400,addTags(mystream).getResponseCode());
  Set<String> streamTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addTags(mystream,streamTags).getResponseCode());
  Set<String> tags=getTags(application);
  Assert.assertTrue(tags.containsAll(appTags));
  Assert.assertTrue(appTags.containsAll(tags));
  tags=getTags(pingService);
  Assert.assertTrue(tags.containsAll(serviceTags));
  Assert.assertTrue(serviceTags.containsAll(tags));
  tags=getTags(myds);
  Assert.assertTrue(tags.containsAll(datasetTags));
  Assert.assertTrue(datasetTags.containsAll(tags));
  tags=getTags(mystream);
  Assert.assertTrue(tags.containsAll(streamTags));
  Assert.assertTrue(streamTags.containsAll(tags));
  Set<MetadataSearchResultRecord> searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM));
  Assert.assertEquals(expected,searchTags);
  searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM),new MetadataSearchResultRecord(pingService,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(expected,searchTags);
  searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(expected,searchTags);
  searchTags=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.of(),searchTags);
  removeTags(application,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.of(""String_Node_Str""),getTags(application));
  removeTags(pingService);
  Assert.assertTrue(getTags(pingService).isEmpty());
  removeTags(pingService);
  Assert.assertTrue(getTags(pingService).isEmpty());
  removeTags(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableSet.of(""String_Node_Str""),getTags(myds));
  removeTags(mystream,""String_Node_Str"");
  removeTags(mystream,""String_Node_Str"");
  Assert.assertTrue(getTags(mystream).isEmpty());
  removeTags(application);
  removeTags(pingService);
  removeTags(myds);
  removeTags(mystream);
  Assert.assertTrue(getTags(application).isEmpty());
  Assert.assertTrue(getTags(pingService).isEmpty());
  Assert.assertTrue(getTags(myds).isEmpty());
  Assert.assertTrue(getTags(mystream).isEmpty());
  Assert.assertEquals(404,addTags(nonExistingApp,appTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingService,serviceTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingDataset,datasetTags).getResponseCode());
  Assert.assertEquals(404,addTags(nonExistingStream,streamTags).getResponseCode());
}"
6633,"@Test public void testProperties() throws IOException {
  Assert.assertEquals(400,addProperties(application).getResponseCode());
  Map<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(application,appProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(pingService).getResponseCode());
  Map<String,String> serviceProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(pingService,serviceProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(myds).getResponseCode());
  Map<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(myds,datasetProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(mystream).getResponseCode());
  Map<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(mystream,streamProperties).getResponseCode());
  Map<String,String> properties=getProperties(application);
  Assert.assertEquals(appProperties,properties);
  properties=getProperties(pingService);
  Assert.assertEquals(serviceProperties,properties);
  properties=getProperties(myds);
  Assert.assertEquals(datasetProperties,properties);
  properties=getProperties(mystream);
  Assert.assertEquals(streamProperties,properties);
  removeProperties(application);
  Assert.assertTrue(getProperties(application).isEmpty());
  removeProperties(pingService,""String_Node_Str"");
  removeProperties(pingService,""String_Node_Str"");
  Assert.assertTrue(getProperties(pingService).isEmpty());
  removeProperties(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(myds));
  removeProperties(mystream,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(mystream));
  removeProperties(application);
  removeProperties(pingService);
  removeProperties(myds);
  removeProperties(mystream);
  Assert.assertTrue(getProperties(application).isEmpty());
  Assert.assertTrue(getProperties(pingService).isEmpty());
  Assert.assertTrue(getProperties(myds).isEmpty());
  Assert.assertTrue(getProperties(mystream).isEmpty());
  Assert.assertEquals(404,addProperties(nonExistingApp,appProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingService,serviceProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingDataset,datasetProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingStream,streamProperties).getResponseCode());
}","@Test public void testProperties() throws IOException {
  Assert.assertEquals(400,addProperties(application).getResponseCode());
  Map<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(application,appProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(pingService).getResponseCode());
  Map<String,String> serviceProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(pingService,serviceProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(myds).getResponseCode());
  Map<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(myds,datasetProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(mystream).getResponseCode());
  Map<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(mystream,streamProperties).getResponseCode());
  Map<String,String> properties=getProperties(application);
  Assert.assertEquals(appProperties,properties);
  properties=getProperties(pingService);
  Assert.assertEquals(serviceProperties,properties);
  properties=getProperties(myds);
  Assert.assertEquals(datasetProperties,properties);
  properties=getProperties(mystream);
  Assert.assertEquals(streamProperties,properties);
  Set<MetadataSearchResultRecord> searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  expected=ImmutableSet.of(new MetadataSearchResultRecord(pingService,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.of(),searchProperties);
  removeProperties(application);
  Assert.assertTrue(getProperties(application).isEmpty());
  removeProperties(pingService,""String_Node_Str"");
  removeProperties(pingService,""String_Node_Str"");
  Assert.assertTrue(getProperties(pingService).isEmpty());
  removeProperties(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(myds));
  removeProperties(mystream,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(mystream));
  removeProperties(application);
  removeProperties(pingService);
  removeProperties(myds);
  removeProperties(mystream);
  Assert.assertTrue(getProperties(application).isEmpty());
  Assert.assertTrue(getProperties(pingService).isEmpty());
  Assert.assertTrue(getProperties(myds).isEmpty());
  Assert.assertTrue(getProperties(mystream).isEmpty());
  Assert.assertEquals(404,addProperties(nonExistingApp,appProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingService,serviceProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingDataset,datasetProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingStream,streamProperties).getResponseCode());
}"
6634,"@Override public void init(Set<Integer> partitions){
  super.init(partitions,checkpointManager);
  scheduledExecutor=MoreExecutors.listeningDecorator(Executors.newSingleThreadScheduledExecutor(Threads.createDaemonThreadFactory(""String_Node_Str"")));
  partitionCheckpoints.clear();
  try {
    for (    Integer partition : partitions) {
      partitionCheckpoints.put(partition,checkpointManager.getCheckpoint(partition));
    }
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
  checkPointWriter=new CheckPointWriter(checkpointManager,partitionCheckpoints);
  scheduledExecutor.scheduleWithFixedDelay(checkPointWriter,100,200,TimeUnit.MILLISECONDS);
}","@Override public void init(Set<Integer> partitions){
  super.init(partitions,checkpointManager);
  scheduledExecutor=MoreExecutors.listeningDecorator(Executors.newSingleThreadScheduledExecutor(Threads.createDaemonThreadFactory(""String_Node_Str"")));
  partitionCheckpoints.clear();
  try {
    for (    Integer partition : partitions) {
      partitionCheckpoints.put(partition,checkpointManager.getCheckpoint(partition));
    }
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
  checkPointWriter=new CheckPointWriter(checkpointManager,partitionCheckpoints);
  scheduledExecutor.scheduleWithFixedDelay(checkPointWriter,100,10000,TimeUnit.MILLISECONDS);
}"
6635,"@Test public void testProperties() throws IOException {
  Assert.assertEquals(400,addProperties(application).getResponseCode());
  Map<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(application,appProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(pingService).getResponseCode());
  Map<String,String> serviceProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(pingService,serviceProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(myds).getResponseCode());
  Map<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(myds,datasetProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(mystream).getResponseCode());
  Map<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(mystream,streamProperties).getResponseCode());
  Map<String,String> properties=getProperties(application);
  Assert.assertEquals(appProperties,properties);
  properties=getProperties(pingService);
  Assert.assertEquals(serviceProperties,properties);
  properties=getProperties(myds);
  Assert.assertEquals(datasetProperties,properties);
  properties=getProperties(mystream);
  Assert.assertEquals(streamProperties,properties);
  Set<MetadataSearchResultRecord> searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  expected=ImmutableSet.of(new MetadataSearchResultRecord(pingService,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.of(),searchProperties);
  removeProperties(application);
  Assert.assertTrue(getProperties(application).isEmpty());
  removeProperties(pingService,""String_Node_Str"");
  removeProperties(pingService,""String_Node_Str"");
  Assert.assertTrue(getProperties(pingService).isEmpty());
  removeProperties(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(myds));
  removeProperties(mystream,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(mystream));
  removeProperties(application);
  removeProperties(pingService);
  removeProperties(myds);
  removeProperties(mystream);
  Assert.assertTrue(getProperties(application).isEmpty());
  Assert.assertTrue(getProperties(pingService).isEmpty());
  Assert.assertTrue(getProperties(myds).isEmpty());
  Assert.assertTrue(getProperties(mystream).isEmpty());
  Assert.assertEquals(404,addProperties(nonExistingApp,appProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingService,serviceProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingDataset,datasetProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingStream,streamProperties).getResponseCode());
}","@Test public void testProperties() throws IOException {
  Assert.assertEquals(400,addProperties(application).getResponseCode());
  Map<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(application,appProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(pingService).getResponseCode());
  Map<String,String> serviceProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(pingService,serviceProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(myds).getResponseCode());
  Map<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(myds,datasetProperties).getResponseCode());
  Assert.assertEquals(400,addProperties(mystream).getResponseCode());
  Map<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(200,addProperties(mystream,streamProperties).getResponseCode());
  Map<String,String> properties=getProperties(application);
  Assert.assertEquals(appProperties,properties);
  properties=getProperties(pingService);
  Assert.assertEquals(serviceProperties,properties);
  properties=getProperties(myds);
  Assert.assertEquals(datasetProperties,properties);
  properties=getProperties(mystream);
  Assert.assertEquals(streamProperties,properties);
  Set<MetadataSearchResultRecord> searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  Set<MetadataSearchResultRecord> expected=ImmutableSet.of(new MetadataSearchResultRecord(mystream,MetadataSearchTargetType.STREAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",""String_Node_Str"");
  expected=ImmutableSet.of(new MetadataSearchResultRecord(pingService,MetadataSearchTargetType.PROGRAM));
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(expected,searchProperties);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertTrue(searchProperties.size() == 0);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertTrue(searchProperties.size() == 0);
  searchProperties=searchMetadata(Id.Namespace.DEFAULT.getId(),""String_Node_Str"",null);
  Assert.assertEquals(ImmutableSet.of(),searchProperties);
  removeProperties(application);
  Assert.assertTrue(getProperties(application).isEmpty());
  removeProperties(pingService,""String_Node_Str"");
  removeProperties(pingService,""String_Node_Str"");
  Assert.assertTrue(getProperties(pingService).isEmpty());
  removeProperties(myds,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(myds));
  removeProperties(mystream,""String_Node_Str"");
  Assert.assertEquals(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),getProperties(mystream));
  removeProperties(application);
  removeProperties(pingService);
  removeProperties(myds);
  removeProperties(mystream);
  Assert.assertTrue(getProperties(application).isEmpty());
  Assert.assertTrue(getProperties(pingService).isEmpty());
  Assert.assertTrue(getProperties(myds).isEmpty());
  Assert.assertTrue(getProperties(mystream).isEmpty());
  Assert.assertEquals(404,addProperties(nonExistingApp,appProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingService,serviceProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingDataset,datasetProperties).getResponseCode());
  Assert.assertEquals(404,addProperties(nonExistingStream,streamProperties).getResponseCode());
}"
6636,"List<BusinessMetadataRecord> executeSearchOnColumns(String column,String searchValue,MetadataSearchTargetType type){
  List<BusinessMetadataRecord> results=new LinkedList<>();
  byte[] startKey=Bytes.toBytes(searchValue.toLowerCase());
  byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
  Scanner scanner=indexedTable.scanByIndex(Bytes.toBytes(column),startKey,stopKey);
  try {
    Row next;
    while ((next=scanner.next()) != null) {
      String rowValue=next.getString(VALUE_COLUMN);
      if (rowValue == null) {
        continue;
      }
      final byte[] rowKey=next.getRow();
      String targetType=getTargetType(rowKey);
      if ((type != MetadataSearchTargetType.ALL) && (!targetType.equals(type.getInternalName()))) {
        continue;
      }
      Id.NamespacedId targetId=getNamespaceIdFromKey(targetType,new MDSKey(rowKey));
      String key=getMetadataKey(targetType,rowKey);
      String value=Bytes.toString(next.get(Bytes.toBytes(BusinessMetadataDataset.VALUE_COLUMN)));
      BusinessMetadataRecord record=new BusinessMetadataRecord(targetId,key,value);
      results.add(record);
    }
  }
  finally {
    scanner.close();
  }
  return results;
}","List<BusinessMetadataRecord> executeSearchOnColumns(String column,String searchValue,MetadataSearchTargetType type){
  List<BusinessMetadataRecord> results=new LinkedList<>();
  Scanner scanner;
  String lowerCaseSearchValue=searchValue.toLowerCase();
  if (lowerCaseSearchValue.endsWith(""String_Node_Str"")) {
    byte[] startKey=Bytes.toBytes(lowerCaseSearchValue.substring(0,lowerCaseSearchValue.lastIndexOf(""String_Node_Str"")));
    byte[] stopKey=Bytes.stopKeyForPrefix(startKey);
    scanner=indexedTable.scanByIndex(Bytes.toBytes(column),startKey,stopKey);
  }
 else {
    byte[] value=Bytes.toBytes(lowerCaseSearchValue);
    scanner=indexedTable.readByIndex(Bytes.toBytes(column),value);
  }
  try {
    Row next;
    while ((next=scanner.next()) != null) {
      String rowValue=next.getString(VALUE_COLUMN);
      if (rowValue == null) {
        continue;
      }
      final byte[] rowKey=next.getRow();
      String targetType=getTargetType(rowKey);
      if ((type != MetadataSearchTargetType.ALL) && (!targetType.equals(type.getInternalName()))) {
        continue;
      }
      Id.NamespacedId targetId=getNamespaceIdFromKey(targetType,new MDSKey(rowKey));
      String key=getMetadataKey(targetType,rowKey);
      String value=Bytes.toString(next.get(Bytes.toBytes(BusinessMetadataDataset.VALUE_COLUMN)));
      BusinessMetadataRecord record=new BusinessMetadataRecord(targetId,key,value);
      results.add(record);
    }
  }
  finally {
    scanner.close();
  }
  return results;
}"
6637,"@Test public void testSearchOnValue() throws Exception {
  BusinessMetadataRecord record=new BusinessMetadataRecord(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  results=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  BusinessMetadataRecord result=results.get(0);
  Assert.assertEquals(record,result);
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results2=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(2,results2.size());
  for (  BusinessMetadataRecord result2 : results2) {
    Assert.assertEquals(""String_Node_Str"",result2.getValue());
  }
}","@Test public void testSearchOnValue() throws Exception {
  BusinessMetadataRecord record=new BusinessMetadataRecord(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(1,results.size());
  results=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  BusinessMetadataRecord result=results.get(0);
  Assert.assertEquals(record,result);
  dataset.setProperty(flow1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results2=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.PROGRAM);
  Assert.assertEquals(2,results2.size());
  for (  BusinessMetadataRecord result2 : results2) {
    Assert.assertEquals(""String_Node_Str"",result2.getValue());
  }
  dataset.setProperty(stream1,""String_Node_Str"",""String_Node_Str"");
  List<BusinessMetadataRecord> results3=dataset.findBusinessMetadataOnValue(""String_Node_Str"",MetadataSearchTargetType.ALL);
  Assert.assertEquals(2,results3.size());
  for (  BusinessMetadataRecord result3 : results3) {
    Assert.assertTrue(result3.getValue().startsWith(""String_Node_Str""));
  }
}"
6638,"private void publishNotification(long absoluteSize){
  try {
    notificationService.publish(streamFeed,new StreamSizeNotification(System.currentTimeMillis(),absoluteSize)).get();
  }
 catch (  NotificationFeedException e) {
    LOG.warn(""String_Node_Str"",streamFeed,e);
  }
catch (  Throwable t) {
    LOG.warn(""String_Node_Str"",streamFeed.getFeedId(),t);
  }
}","private void publishNotification(long absoluteSize){
  try {
    notificationService.publish(streamFeed,new StreamSizeNotification(System.currentTimeMillis(),absoluteSize)).get();
  }
 catch (  NotificationFeedException e) {
    LOG.warn(""String_Node_Str"",streamFeed,e);
  }
catch (  Throwable t) {
    LOG.debug(""String_Node_Str"",streamFeed.getFeedId(),t);
  }
}"
6639,"@Override public <N>ListenableFuture<N> publish(final Id.NotificationFeed feed,final N notification,final Type notificationType) throws NotificationException {
  return executorService.submit(new Callable<N>(){
    @Override public N call() throws Exception {
      notificationReceived(feed,GSON.toJsonTree(notification,notificationType));
      return notification;
    }
  }
);
}","@Override public <N>ListenableFuture<N> publish(final Id.NotificationFeed feed,final N notification,final Type notificationType) throws NotificationException {
  if (executorService == null) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  return executorService.submit(new Callable<N>(){
    @Override public N call() throws Exception {
      notificationReceived(feed,GSON.toJsonTree(notification,notificationType));
      return notification;
    }
  }
);
}"
6640,"@Override protected void doInit(TwillContext context){
  this.user=DEFAULT_USER;
  CConfiguration cConf=getCConfiguration();
  Configuration hConf=getConfiguration();
  cConf.set(Constants.Dataset.Executor.ADDRESS,context.getHost().getHostName());
  injector=Guice.createInjector(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MetricsClientRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new LocationRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(true),new DataSetServiceModules().getDistributedModules(),new LoggingModules().getDistributedModules(),new ExploreClientModule(),new NamespaceClientRuntimeModule().getDistributedModules(),new MetadataServiceModule());
  injector.getInstance(LogAppenderInitializer.class).initialize();
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(Id.Namespace.SYSTEM.getId(),Constants.Logging.COMPONENT_NAME,Constants.Service.DATASET_EXECUTOR));
}","@Override protected void doInit(TwillContext context){
  this.user=DEFAULT_USER;
  CConfiguration cConf=getCConfiguration();
  Configuration hConf=getConfiguration();
  cConf.set(Constants.Dataset.Executor.ADDRESS,context.getHost().getHostName());
  injector=createInjector(cConf,hConf);
  injector.getInstance(LogAppenderInitializer.class).initialize();
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(Id.Namespace.SYSTEM.getId(),Constants.Logging.COMPONENT_NAME,Constants.Service.DATASET_EXECUTOR));
}"
6641,"/** 
 * Stops a Program.
 */
private AppFabricServiceStatus stop(Id.Program identifier,@Nullable String runId) throws NotFoundException, BadRequestException {
  ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(identifier,runId);
  if (runtimeInfo == null) {
    if (!store.applicationExists(identifier.getApplication())) {
      throw new ApplicationNotFoundException(identifier.getApplication());
    }
 else     if (!store.programExists(identifier)) {
      throw new ProgramNotFoundException(identifier);
    }
 else     if (runId == null) {
      throw new BadRequestException(""String_Node_Str"");
    }
 else {
      throw new NotFoundException(new Id.Run(identifier,runId));
    }
  }
  try {
    ProgramController controller=runtimeInfo.getController();
    controller.stop().get();
    return AppFabricServiceStatus.OK;
  }
 catch (  Throwable throwable) {
    LOG.warn(throwable.getMessage(),throwable);
    return AppFabricServiceStatus.INTERNAL_ERROR;
  }
}","/** 
 * Stops a Program.
 */
private AppFabricServiceStatus stop(Id.Program identifier,@Nullable String runId) throws NotFoundException, BadRequestException {
  ProgramRuntimeService.RuntimeInfo runtimeInfo=findRuntimeInfo(identifier,runId);
  if (runtimeInfo == null) {
    if (!store.applicationExists(identifier.getApplication())) {
      throw new ApplicationNotFoundException(identifier.getApplication());
    }
 else     if (!store.programExists(identifier)) {
      throw new ProgramNotFoundException(identifier);
    }
 else     if (runId != null) {
      Id.Run programRunId=new Id.Run(identifier,runId);
      RunRecordMeta runRecord=store.getRun(identifier,runId);
      if (runRecord != null && runRecord.getProperties().containsKey(""String_Node_Str"") && runRecord.getStatus().equals(ProgramRunStatus.RUNNING)) {
        String workflowRunId=runRecord.getProperties().get(""String_Node_Str"");
        throw new BadRequestException(String.format(""String_Node_Str"" + ""String_Node_Str"",programRunId,workflowRunId));
      }
      throw new NotFoundException(programRunId);
    }
    throw new BadRequestException(String.format(""String_Node_Str"",identifier));
  }
  try {
    ProgramController controller=runtimeInfo.getController();
    controller.stop().get();
    return AppFabricServiceStatus.OK;
  }
 catch (  Throwable throwable) {
    LOG.warn(throwable.getMessage(),throwable);
    return AppFabricServiceStatus.INTERNAL_ERROR;
  }
}"
6642,"protected void stopProgram(Id.Program program,int expectedStatusCode,String runId) throws Exception {
  String path;
  if (runId == null) {
    path=String.format(""String_Node_Str"",program.getApplicationId(),program.getType().getCategoryName(),program.getId());
  }
 else {
    path=String.format(""String_Node_Str"",program.getApplicationId(),program.getType().getCategoryName(),program.getId(),runId);
  }
  HttpResponse response=doPost(getVersionedAPIPath(path,program.getNamespaceId()));
  Assert.assertEquals(expectedStatusCode,response.getStatusLine().getStatusCode());
}","protected void stopProgram(Id.Program program,String runId,int expectedStatusCode,String expectedMessage) throws Exception {
  String path;
  if (runId == null) {
    path=String.format(""String_Node_Str"",program.getApplicationId(),program.getType().getCategoryName(),program.getId());
  }
 else {
    path=String.format(""String_Node_Str"",program.getApplicationId(),program.getType().getCategoryName(),program.getId(),runId);
  }
  HttpResponse response=doPost(getVersionedAPIPath(path,program.getNamespaceId()));
  Assert.assertEquals(expectedStatusCode,response.getStatusLine().getStatusCode());
  if (expectedMessage != null) {
    Assert.assertEquals(expectedMessage,EntityUtils.toString(response.getEntity()));
  }
}"
6643,"@Category(XSlowTests.class) @Test public void testProgramStartStopStatus() throws Exception {
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Flow wordcountFlow1=Id.Flow.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,WORDCOUNT_FLOW_NAME);
  Id.Flow wordcountFlow2=Id.Flow.from(TEST_NAMESPACE2,WORDCOUNT_APP_NAME,WORDCOUNT_FLOW_NAME);
  Assert.assertEquals(STOPPED,getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow2,404);
  Assert.assertEquals(STOPPED,getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow1);
  waitState(wordcountFlow1,ProgramRunStatus.RUNNING.toString());
  stopProgram(wordcountFlow1);
  waitState(wordcountFlow1,STOPPED);
  response=deploy(DummyAppWithTrackingTable.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program dummyMR1=Id.Program.from(TEST_NAMESPACE1,DUMMY_APP_ID,ProgramType.MAPREDUCE,DUMMY_MR_NAME);
  Id.Program dummyMR2=Id.Program.from(TEST_NAMESPACE2,DUMMY_APP_ID,ProgramType.MAPREDUCE,DUMMY_MR_NAME);
  Assert.assertEquals(STOPPED,getProgramStatus(dummyMR2));
  startProgram(dummyMR1,404);
  Assert.assertEquals(STOPPED,getProgramStatus(dummyMR2));
  startProgram(dummyMR2);
  waitState(dummyMR2,ProgramRunStatus.RUNNING.toString());
  stopProgram(dummyMR2);
  waitState(dummyMR2,STOPPED);
  startProgram(dummyMR2);
  startProgram(dummyMR2);
  verifyProgramRuns(dummyMR2,""String_Node_Str"",1);
  List<RunRecord> historyRuns=getProgramRuns(dummyMR2,""String_Node_Str"");
  Assert.assertTrue(2 == historyRuns.size());
  String runId=historyRuns.get(0).getPid();
  stopProgram(dummyMR2,200,runId);
  runId=historyRuns.get(1).getPid();
  stopProgram(dummyMR2,200,runId);
  waitState(dummyMR2,STOPPED);
  response=deploy(SleepingWorkflowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program sleepWorkflow1=Id.Program.from(TEST_NAMESPACE1,SLEEP_WORKFLOW_APP_ID,ProgramType.WORKFLOW,SLEEP_WORKFLOW_NAME);
  Id.Program sleepWorkflow2=Id.Program.from(TEST_NAMESPACE2,SLEEP_WORKFLOW_APP_ID,ProgramType.WORKFLOW,SLEEP_WORKFLOW_NAME);
  Assert.assertEquals(STOPPED,getProgramStatus(sleepWorkflow2));
  startProgram(sleepWorkflow1,404);
  Assert.assertEquals(STOPPED,getProgramStatus(sleepWorkflow2));
  startProgram(sleepWorkflow2);
  waitState(sleepWorkflow2,ProgramRunStatus.RUNNING.toString());
  waitState(sleepWorkflow2,STOPPED);
  response=doDelete(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=doDelete(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
}","@Category(XSlowTests.class) @Test public void testProgramStartStopStatus() throws Exception {
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Flow wordcountFlow1=Id.Flow.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,WORDCOUNT_FLOW_NAME);
  Id.Flow wordcountFlow2=Id.Flow.from(TEST_NAMESPACE2,WORDCOUNT_APP_NAME,WORDCOUNT_FLOW_NAME);
  Assert.assertEquals(STOPPED,getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow2,404);
  Assert.assertEquals(STOPPED,getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow1);
  waitState(wordcountFlow1,ProgramRunStatus.RUNNING.toString());
  stopProgram(wordcountFlow1);
  waitState(wordcountFlow1,STOPPED);
  response=deploy(DummyAppWithTrackingTable.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program dummyMR1=Id.Program.from(TEST_NAMESPACE1,DUMMY_APP_ID,ProgramType.MAPREDUCE,DUMMY_MR_NAME);
  Id.Program dummyMR2=Id.Program.from(TEST_NAMESPACE2,DUMMY_APP_ID,ProgramType.MAPREDUCE,DUMMY_MR_NAME);
  Assert.assertEquals(STOPPED,getProgramStatus(dummyMR2));
  startProgram(dummyMR1,404);
  Assert.assertEquals(STOPPED,getProgramStatus(dummyMR2));
  startProgram(dummyMR2);
  waitState(dummyMR2,ProgramRunStatus.RUNNING.toString());
  stopProgram(dummyMR2);
  waitState(dummyMR2,STOPPED);
  startProgram(dummyMR2);
  startProgram(dummyMR2);
  verifyProgramRuns(dummyMR2,""String_Node_Str"",1);
  List<RunRecord> historyRuns=getProgramRuns(dummyMR2,""String_Node_Str"");
  Assert.assertTrue(2 == historyRuns.size());
  String runId=historyRuns.get(0).getPid();
  stopProgram(dummyMR2,runId,200);
  runId=historyRuns.get(1).getPid();
  stopProgram(dummyMR2,runId,200);
  waitState(dummyMR2,STOPPED);
  response=deploy(SleepingWorkflowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program sleepWorkflow1=Id.Program.from(TEST_NAMESPACE1,SLEEP_WORKFLOW_APP_ID,ProgramType.WORKFLOW,SLEEP_WORKFLOW_NAME);
  Id.Program sleepWorkflow2=Id.Program.from(TEST_NAMESPACE2,SLEEP_WORKFLOW_APP_ID,ProgramType.WORKFLOW,SLEEP_WORKFLOW_NAME);
  Assert.assertEquals(STOPPED,getProgramStatus(sleepWorkflow2));
  startProgram(sleepWorkflow1,404);
  Assert.assertEquals(STOPPED,getProgramStatus(sleepWorkflow2));
  startProgram(sleepWorkflow2);
  waitState(sleepWorkflow2,ProgramRunStatus.RUNNING.toString());
  waitState(sleepWorkflow2,STOPPED);
  response=doDelete(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=doDelete(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
}"
6644,"@Category(XSlowTests.class) @Test public void testProgramStartStopStatusErrors() throws Exception {
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  startProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,""String_Node_Str""),404);
  startProgram(Id.Program.from(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW,WORDCOUNT_FLOW_NAME),404);
  startProgram(Id.Program.from(""String_Node_Str"",WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),404);
  debugProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,""String_Node_Str""),404);
  debugProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.MAPREDUCE,WORDCOUNT_MAPREDUCE_NAME),501);
  programStatus(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,""String_Node_Str""),404);
  programStatus(Id.Program.from(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW,WORDCOUNT_FLOW_NAME),404);
  programStatus(Id.Program.from(""String_Node_Str"",WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),404);
  stopProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,""String_Node_Str""),404);
  stopProgram(Id.Program.from(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW,WORDCOUNT_FLOW_NAME),404);
  stopProgram(Id.Program.from(""String_Node_Str"",WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),404);
  stopProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),400);
  stopProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),400,""String_Node_Str"");
  startProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME));
  waitState(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),""String_Node_Str"");
  startProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),409);
  List<RunRecord> runs=getProgramRuns(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),""String_Node_Str"");
  Assert.assertEquals(1,runs.size());
  String runId=runs.get(0).getPid();
  stopProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),200);
  waitState(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),""String_Node_Str"");
  runs=getProgramRuns(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),""String_Node_Str"");
  Assert.assertTrue(runs.isEmpty());
  stopProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),404,runId);
  response=doDelete(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
}","@Category(XSlowTests.class) @Test public void testProgramStartStopStatusErrors() throws Exception {
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  startProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,""String_Node_Str""),404);
  startProgram(Id.Program.from(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW,WORDCOUNT_FLOW_NAME),404);
  startProgram(Id.Program.from(""String_Node_Str"",WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),404);
  debugProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,""String_Node_Str""),404);
  debugProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.MAPREDUCE,WORDCOUNT_MAPREDUCE_NAME),501);
  programStatus(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,""String_Node_Str""),404);
  programStatus(Id.Program.from(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW,WORDCOUNT_FLOW_NAME),404);
  programStatus(Id.Program.from(""String_Node_Str"",WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),404);
  stopProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,""String_Node_Str""),404);
  stopProgram(Id.Program.from(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW,WORDCOUNT_FLOW_NAME),404);
  stopProgram(Id.Program.from(""String_Node_Str"",WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),404);
  stopProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),400);
  stopProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),""String_Node_Str"",400);
  startProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME));
  waitState(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),""String_Node_Str"");
  startProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),409);
  List<RunRecord> runs=getProgramRuns(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),""String_Node_Str"");
  Assert.assertEquals(1,runs.size());
  String runId=runs.get(0).getPid();
  stopProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),200);
  waitState(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),""String_Node_Str"");
  runs=getProgramRuns(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),""String_Node_Str"");
  Assert.assertTrue(runs.isEmpty());
  stopProgram(Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME),runId,404);
  response=doDelete(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
}"
6645,"@Test public void testWorkflowRuns() throws Exception {
  String appName=""String_Node_Str"";
  String workflowName=""String_Node_Str"";
  HttpResponse response=deploy(WorkflowAppWithErrorRuns.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,appName,ProgramType.WORKFLOW,workflowName);
  File instance1File=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File instance2File=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File doneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  Map<String,String> propertyMap=ImmutableMap.of(""String_Node_Str"",instance1File.getAbsolutePath(),""String_Node_Str"",doneFile.getAbsolutePath());
  startProgram(programId,propertyMap);
  propertyMap=ImmutableMap.of(""String_Node_Str"",instance2File.getAbsolutePath(),""String_Node_Str"",doneFile.getAbsolutePath());
  startProgram(programId,propertyMap);
  while (!(instance1File.exists() && instance2File.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  List<RunRecord> historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(historyRuns.size() == 2);
  String runId=historyRuns.get(0).getPid();
  stopProgram(programId,200,runId);
  runId=historyRuns.get(1).getPid();
  stopProgram(programId,200,runId);
  verifyProgramRuns(programId,""String_Node_Str"",1);
  File instanceFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  propertyMap=ImmutableMap.of(""String_Node_Str"",instanceFile.getAbsolutePath(),""String_Node_Str"",doneFile.getAbsolutePath());
  startProgram(programId,propertyMap);
  while (!instanceFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(historyRuns.size() == 1);
  Assert.assertTrue(doneFile.createNewFile());
  verifyProgramRuns(programId,""String_Node_Str"");
  propertyMap=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  startProgram(programId,propertyMap);
  verifyProgramRuns(programId,""String_Node_Str"");
}","@Test public void testWorkflowRuns() throws Exception {
  String appName=""String_Node_Str"";
  String workflowName=""String_Node_Str"";
  HttpResponse response=deploy(WorkflowAppWithErrorRuns.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,appName,ProgramType.WORKFLOW,workflowName);
  File instance1File=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File instance2File=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File doneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  Map<String,String> propertyMap=ImmutableMap.of(""String_Node_Str"",instance1File.getAbsolutePath(),""String_Node_Str"",doneFile.getAbsolutePath());
  startProgram(programId,propertyMap);
  propertyMap=ImmutableMap.of(""String_Node_Str"",instance2File.getAbsolutePath(),""String_Node_Str"",doneFile.getAbsolutePath());
  startProgram(programId,propertyMap);
  while (!(instance1File.exists() && instance2File.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  List<RunRecord> historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(historyRuns.size() == 2);
  String runId=historyRuns.get(0).getPid();
  stopProgram(programId,runId,200);
  runId=historyRuns.get(1).getPid();
  stopProgram(programId,runId,200);
  verifyProgramRuns(programId,""String_Node_Str"",1);
  File instanceFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  propertyMap=ImmutableMap.of(""String_Node_Str"",instanceFile.getAbsolutePath(),""String_Node_Str"",doneFile.getAbsolutePath());
  startProgram(programId,propertyMap);
  while (!instanceFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(historyRuns.size() == 1);
  Assert.assertTrue(doneFile.createNewFile());
  verifyProgramRuns(programId,""String_Node_Str"");
  propertyMap=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
  startProgram(programId,propertyMap);
  verifyProgramRuns(programId,""String_Node_Str"");
}"
6646,"@Category(XSlowTests.class) @Test public void testWorkflowScopedArguments() throws Exception {
  String workflowAppWithScopedParameters=""String_Node_Str"";
  String workflowAppWithScopedParameterWorkflow=""String_Node_Str"";
  HttpResponse response=deploy(WorkflowAppWithScopedParameters.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,workflowAppWithScopedParameters,ProgramType.WORKFLOW,workflowAppWithScopedParameterWorkflow);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  setAndTestRuntimeArgs(programId,runtimeArguments);
  startProgram(programId);
  verifyProgramRuns(programId,""String_Node_Str"");
  List<RunRecord> workflowHistoryRuns=getProgramRuns(programId,""String_Node_Str"");
  Id.Program mr1ProgramId=Id.Program.from(TEST_NAMESPACE2,workflowAppWithScopedParameters,ProgramType.MAPREDUCE,""String_Node_Str"");
  List<RunRecord> oneMRHistoryRuns=getProgramRuns(mr1ProgramId,""String_Node_Str"");
  Id.Program mr2ProgramId=Id.Program.from(TEST_NAMESPACE2,workflowAppWithScopedParameters,ProgramType.MAPREDUCE,""String_Node_Str"");
  List<RunRecord> anotherMRHistoryRuns=getProgramRuns(mr2ProgramId,""String_Node_Str"");
  Id.Program spark1ProgramId=Id.Program.from(TEST_NAMESPACE2,workflowAppWithScopedParameters,ProgramType.SPARK,""String_Node_Str"");
  List<RunRecord> oneSparkHistoryRuns=getProgramRuns(spark1ProgramId,""String_Node_Str"");
  Id.Program spark2ProgramId=Id.Program.from(TEST_NAMESPACE2,workflowAppWithScopedParameters,ProgramType.SPARK,""String_Node_Str"");
  List<RunRecord> anotherSparkHistoryRuns=getProgramRuns(spark2ProgramId,""String_Node_Str"");
  Assert.assertEquals(1,workflowHistoryRuns.size());
  Assert.assertEquals(1,oneMRHistoryRuns.size());
  Assert.assertEquals(1,anotherMRHistoryRuns.size());
  Assert.assertEquals(1,oneSparkHistoryRuns.size());
  Assert.assertEquals(1,anotherSparkHistoryRuns.size());
  Map<String,String> workflowRunRecordProperties=workflowHistoryRuns.get(0).getProperties();
  Map<String,String> oneMRRunRecordProperties=oneMRHistoryRuns.get(0).getProperties();
  Map<String,String> anotherMRRunRecordProperties=anotherMRHistoryRuns.get(0).getProperties();
  Map<String,String> oneSparkRunRecordProperties=oneSparkHistoryRuns.get(0).getProperties();
  Map<String,String> anotherSparkRunRecordProperties=anotherSparkHistoryRuns.get(0).getProperties();
  Assert.assertNotNull(oneMRRunRecordProperties.get(""String_Node_Str""));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),oneMRRunRecordProperties.get(""String_Node_Str""));
  Assert.assertNotNull(anotherMRRunRecordProperties.get(""String_Node_Str""));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),anotherMRRunRecordProperties.get(""String_Node_Str""));
  Assert.assertNotNull(oneSparkRunRecordProperties.get(""String_Node_Str""));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),oneSparkRunRecordProperties.get(""String_Node_Str""));
  Assert.assertNotNull(anotherSparkRunRecordProperties.get(""String_Node_Str""));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),anotherSparkRunRecordProperties.get(""String_Node_Str""));
  Assert.assertEquals(workflowRunRecordProperties.get(""String_Node_Str""),oneMRHistoryRuns.get(0).getPid());
  Assert.assertEquals(workflowRunRecordProperties.get(""String_Node_Str""),oneSparkHistoryRuns.get(0).getPid());
  Assert.assertEquals(workflowRunRecordProperties.get(""String_Node_Str""),anotherMRHistoryRuns.get(0).getPid());
  Assert.assertEquals(workflowRunRecordProperties.get(""String_Node_Str""),anotherSparkHistoryRuns.get(0).getPid());
}","@Category(XSlowTests.class) @Test public void testWorkflowScopedArguments() throws Exception {
  String workflowAppWithScopedParameters=""String_Node_Str"";
  String workflowAppWithScopedParameterWorkflow=""String_Node_Str"";
  HttpResponse response=deploy(WorkflowAppWithScopedParameters.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,workflowAppWithScopedParameters,ProgramType.WORKFLOW,workflowAppWithScopedParameterWorkflow);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",createInput(""String_Node_Str""));
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",new File(tmpFolder.newFolder(),""String_Node_Str"").getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  runtimeArguments.put(""String_Node_Str"",""String_Node_Str"");
  setAndTestRuntimeArgs(programId,runtimeArguments);
  startProgram(programId);
  waitState(programId,""String_Node_Str"");
  List<RunRecord> workflowHistoryRuns=getProgramRuns(programId,""String_Node_Str"");
  String workflowRunId=workflowHistoryRuns.get(0).getPid();
  Id.Program mr1ProgramId=Id.Program.from(TEST_NAMESPACE2,workflowAppWithScopedParameters,ProgramType.MAPREDUCE,""String_Node_Str"");
  waitState(mr1ProgramId,""String_Node_Str"");
  List<RunRecord> oneMRHistoryRuns=getProgramRuns(mr1ProgramId,""String_Node_Str"");
  String expectedMessage=String.format(""String_Node_Str"" + ""String_Node_Str"",new Id.Run(mr1ProgramId,oneMRHistoryRuns.get(0).getPid()),workflowRunId);
  stopProgram(mr1ProgramId,oneMRHistoryRuns.get(0).getPid(),400,expectedMessage);
  verifyProgramRuns(programId,""String_Node_Str"");
  workflowHistoryRuns=getProgramRuns(programId,""String_Node_Str"");
  oneMRHistoryRuns=getProgramRuns(mr1ProgramId,""String_Node_Str"");
  Id.Program mr2ProgramId=Id.Program.from(TEST_NAMESPACE2,workflowAppWithScopedParameters,ProgramType.MAPREDUCE,""String_Node_Str"");
  List<RunRecord> anotherMRHistoryRuns=getProgramRuns(mr2ProgramId,""String_Node_Str"");
  Id.Program spark1ProgramId=Id.Program.from(TEST_NAMESPACE2,workflowAppWithScopedParameters,ProgramType.SPARK,""String_Node_Str"");
  List<RunRecord> oneSparkHistoryRuns=getProgramRuns(spark1ProgramId,""String_Node_Str"");
  Id.Program spark2ProgramId=Id.Program.from(TEST_NAMESPACE2,workflowAppWithScopedParameters,ProgramType.SPARK,""String_Node_Str"");
  List<RunRecord> anotherSparkHistoryRuns=getProgramRuns(spark2ProgramId,""String_Node_Str"");
  Assert.assertEquals(1,workflowHistoryRuns.size());
  Assert.assertEquals(1,oneMRHistoryRuns.size());
  Assert.assertEquals(1,anotherMRHistoryRuns.size());
  Assert.assertEquals(1,oneSparkHistoryRuns.size());
  Assert.assertEquals(1,anotherSparkHistoryRuns.size());
  Map<String,String> workflowRunRecordProperties=workflowHistoryRuns.get(0).getProperties();
  Map<String,String> oneMRRunRecordProperties=oneMRHistoryRuns.get(0).getProperties();
  Map<String,String> anotherMRRunRecordProperties=anotherMRHistoryRuns.get(0).getProperties();
  Map<String,String> oneSparkRunRecordProperties=oneSparkHistoryRuns.get(0).getProperties();
  Map<String,String> anotherSparkRunRecordProperties=anotherSparkHistoryRuns.get(0).getProperties();
  Assert.assertNotNull(oneMRRunRecordProperties.get(""String_Node_Str""));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),oneMRRunRecordProperties.get(""String_Node_Str""));
  Assert.assertNotNull(anotherMRRunRecordProperties.get(""String_Node_Str""));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),anotherMRRunRecordProperties.get(""String_Node_Str""));
  Assert.assertNotNull(oneSparkRunRecordProperties.get(""String_Node_Str""));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),oneSparkRunRecordProperties.get(""String_Node_Str""));
  Assert.assertNotNull(anotherSparkRunRecordProperties.get(""String_Node_Str""));
  Assert.assertEquals(workflowHistoryRuns.get(0).getPid(),anotherSparkRunRecordProperties.get(""String_Node_Str""));
  Assert.assertEquals(workflowRunRecordProperties.get(""String_Node_Str""),oneMRHistoryRuns.get(0).getPid());
  Assert.assertEquals(workflowRunRecordProperties.get(""String_Node_Str""),oneSparkHistoryRuns.get(0).getPid());
  Assert.assertEquals(workflowRunRecordProperties.get(""String_Node_Str""),anotherMRHistoryRuns.get(0).getPid());
  Assert.assertEquals(workflowRunRecordProperties.get(""String_Node_Str""),anotherSparkHistoryRuns.get(0).getPid());
}"
6647,"@Test public void testWorkflowToken() throws Exception {
  Id.Application fakeAppId=Id.Application.from(Id.Namespace.DEFAULT,FakeApp.NAME);
  Id.Workflow fakeWorkflowId=Id.Workflow.from(fakeAppId,FakeWorkflow.NAME);
  String workflow=String.format(""String_Node_Str"",FakeApp.NAME,FakeWorkflow.NAME);
  File doneFile=TMP_FOLDER.newFile(""String_Node_Str"");
  Map<String,String> runtimeArgs=ImmutableMap.of(""String_Node_Str"",doneFile.getAbsolutePath());
  String runtimeArgsKV=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(runtimeArgs);
  testCommandOutputContains(cli,""String_Node_Str"" + workflow + ""String_Node_Str""+ runtimeArgsKV+ ""String_Node_Str"",""String_Node_Str"");
  assertProgramStatus(programClient,fakeWorkflowId,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  String commandOutput=getCommandOutput(cli,""String_Node_Str"" + workflow);
  String[] lines=commandOutput.split(""String_Node_Str"");
  Assert.assertEquals(2,lines.length);
  String[] split=lines[1].split(""String_Node_Str"");
  String runId=split[0];
  List<WorkflowTokenDetail.NodeValueDetail> tokenValues=new ArrayList<>();
  tokenValues.add(new WorkflowTokenDetail.NodeValueDetail(FakeWorkflow.FakeAction.class.getSimpleName(),FakeWorkflow.FakeAction.TOKEN_VALUE));
  tokenValues.add(new WorkflowTokenDetail.NodeValueDetail(FakeWorkflow.FakeAction.ANOTHER_FAKE_NAME,FakeWorkflow.FakeAction.TOKEN_VALUE));
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId),Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,GSON.toJson(tokenValues)));
  testCommandOutputNotContains(cli,String.format(""String_Node_Str"",workflow,runId),Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,GSON.toJson(tokenValues)));
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.TOKEN_KEY),Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,GSON.toJson(tokenValues)));
  String fakeNodeValue=Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,FakeWorkflow.FakeAction.TOKEN_VALUE);
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.class.getSimpleName()),fakeNodeValue);
  testCommandOutputNotContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.ANOTHER_FAKE_NAME),fakeNodeValue);
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.ANOTHER_FAKE_NAME,FakeWorkflow.FakeAction.TOKEN_KEY),fakeNodeValue);
  testCommandOutputContains(cli,""String_Node_Str"" + workflow,""String_Node_Str"");
}","@Test public void testWorkflowToken() throws Exception {
  Id.Application fakeAppId=Id.Application.from(Id.Namespace.DEFAULT,FakeApp.NAME);
  Id.Workflow fakeWorkflowId=Id.Workflow.from(fakeAppId,FakeWorkflow.NAME);
  String workflow=String.format(""String_Node_Str"",FakeApp.NAME,FakeWorkflow.NAME);
  File doneFile=TMP_FOLDER.newFile(""String_Node_Str"");
  Map<String,String> runtimeArgs=ImmutableMap.of(""String_Node_Str"",doneFile.getAbsolutePath());
  String runtimeArgsKV=Joiner.on(""String_Node_Str"").withKeyValueSeparator(""String_Node_Str"").join(runtimeArgs);
  testCommandOutputContains(cli,""String_Node_Str"" + workflow + ""String_Node_Str""+ runtimeArgsKV+ ""String_Node_Str"",""String_Node_Str"");
  assertProgramStatus(programClient,fakeWorkflowId,""String_Node_Str"");
  testCommandOutputContains(cli,""String_Node_Str"",""String_Node_Str"");
  String commandOutput=getCommandOutput(cli,""String_Node_Str"" + workflow);
  String[] lines=commandOutput.split(""String_Node_Str"");
  Assert.assertEquals(2,lines.length);
  String[] split=lines[1].split(""String_Node_Str"");
  String runId=split[0];
  List<WorkflowTokenDetail.NodeValueDetail> tokenValues=new ArrayList<>();
  tokenValues.add(new WorkflowTokenDetail.NodeValueDetail(FakeWorkflow.FakeAction.class.getSimpleName(),FakeWorkflow.FakeAction.TOKEN_VALUE));
  tokenValues.add(new WorkflowTokenDetail.NodeValueDetail(FakeWorkflow.FakeAction.ANOTHER_FAKE_NAME,FakeWorkflow.FakeAction.TOKEN_VALUE));
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId),Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,GSON.toJson(tokenValues)));
  testCommandOutputNotContains(cli,String.format(""String_Node_Str"",workflow,runId),Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,GSON.toJson(tokenValues)));
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.TOKEN_KEY),Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,GSON.toJson(tokenValues)));
  String fakeNodeValue=Joiner.on(""String_Node_Str"").join(FakeWorkflow.FakeAction.TOKEN_KEY,FakeWorkflow.FakeAction.TOKEN_VALUE);
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.class.getSimpleName()),fakeNodeValue);
  testCommandOutputNotContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.ANOTHER_FAKE_NAME),fakeNodeValue);
  testCommandOutputContains(cli,String.format(""String_Node_Str"",workflow,runId,FakeWorkflow.FakeAction.ANOTHER_FAKE_NAME,FakeWorkflow.FakeAction.TOKEN_KEY),fakeNodeValue);
  testCommandOutputContains(cli,""String_Node_Str"" + workflow,String.format(""String_Node_Str"",fakeWorkflowId));
}"
6648,"public static void setupDatasets(DatasetFramework dsFramework) throws DatasetManagementException, IOException, ServiceUnavailableException {
  dsFramework.addInstance(Table.class.getName(),Id.DatasetInstance.from(Id.Namespace.SYSTEM,Constants.ConfigStore.CONFIG_TABLE),DatasetProperties.EMPTY);
}","public static void setupDatasets(DatasetFramework dsFramework) throws DatasetManagementException, IOException {
  dsFramework.addInstance(Table.class.getName(),Id.DatasetInstance.from(Id.Namespace.SYSTEM,Constants.ConfigStore.CONFIG_TABLE),DatasetProperties.EMPTY);
}"
6649,"protected void verifyData(Id.Application appId,ApplicationSpecification specification) throws DatasetManagementException, ServiceUnavailableException {
  VerifyResult result;
  for (  DatasetCreationSpec dataSetCreateSpec : specification.getDatasets().values()) {
    result=getVerifier(DatasetCreationSpec.class).verify(appId,dataSetCreateSpec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
    String dsName=dataSetCreateSpec.getInstanceName();
    Id.DatasetInstance datasetInstanceId=Id.DatasetInstance.from(appId.getNamespace(),dsName);
    DatasetSpecification existingSpec=dsFramework.getDatasetSpec(datasetInstanceId);
    if (existingSpec != null && !existingSpec.getType().equals(dataSetCreateSpec.getTypeName())) {
      throw new DataSetException(String.format(""String_Node_Str"",dsName,dataSetCreateSpec.getTypeName()));
    }
  }
  for (  StreamSpecification spec : specification.getStreams().values()) {
    result=getVerifier(StreamSpecification.class).verify(appId,spec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
  }
}","protected void verifyData(Id.Application appId,ApplicationSpecification specification) throws DatasetManagementException {
  VerifyResult result;
  for (  DatasetCreationSpec dataSetCreateSpec : specification.getDatasets().values()) {
    result=getVerifier(DatasetCreationSpec.class).verify(appId,dataSetCreateSpec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
    String dsName=dataSetCreateSpec.getInstanceName();
    Id.DatasetInstance datasetInstanceId=Id.DatasetInstance.from(appId.getNamespace(),dsName);
    DatasetSpecification existingSpec=dsFramework.getDatasetSpec(datasetInstanceId);
    if (existingSpec != null && !existingSpec.getType().equals(dataSetCreateSpec.getTypeName())) {
      throw new DataSetException(String.format(""String_Node_Str"",dsName,dataSetCreateSpec.getTypeName()));
    }
  }
  for (  StreamSpecification spec : specification.getStreams().values()) {
    result=getVerifier(StreamSpecification.class).verify(appId,spec);
    if (!result.isSuccess()) {
      throw new RuntimeException(result.getMessage());
    }
  }
}"
6650,"@Override public synchronized void deleteDatasets(Id.Namespace namespaceId) throws NamespaceNotFoundException, NamespaceCannotBeDeletedException {
  if (!exists(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (checkProgramsRunning(namespaceId)) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  try {
    dsFramework.deleteAllInstances(namespaceId);
  }
 catch (  DatasetManagementException|IOException|ServiceUnavailableException e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
  LOG.debug(""String_Node_Str"",namespaceId);
}","@Override public synchronized void deleteDatasets(Id.Namespace namespaceId) throws NamespaceNotFoundException, NamespaceCannotBeDeletedException {
  if (!exists(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (checkProgramsRunning(namespaceId)) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  try {
    dsFramework.deleteAllInstances(namespaceId);
  }
 catch (  DatasetManagementException|IOException e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
  LOG.debug(""String_Node_Str"",namespaceId);
}"
6651,"/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
public synchronized void create(NamespaceMeta metadata) throws NamespaceCannotBeCreatedException, NamespaceAlreadyExistsException {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  Id.Namespace namespace=Id.Namespace.from(metadata.getName());
  if (exists(Id.Namespace.from(metadata.getName()))) {
    throw new NamespaceAlreadyExistsException(namespace);
  }
  try {
    dsFramework.createNamespace(Id.Namespace.from(metadata.getName()));
  }
 catch (  DatasetManagementException|ServiceUnavailableException e) {
    throw new NamespaceCannotBeCreatedException(namespace,e);
  }
  store.createNamespace(metadata);
}","/** 
 * Creates a new namespace
 * @param metadata the {@link NamespaceMeta} for the new namespace to be created
 * @throws NamespaceAlreadyExistsException if the specified namespace already exists
 */
public synchronized void create(NamespaceMeta metadata) throws NamespaceCannotBeCreatedException, NamespaceAlreadyExistsException {
  Preconditions.checkArgument(metadata != null,""String_Node_Str"");
  Id.Namespace namespace=Id.Namespace.from(metadata.getName());
  if (exists(Id.Namespace.from(metadata.getName()))) {
    throw new NamespaceAlreadyExistsException(namespace);
  }
  try {
    dsFramework.createNamespace(Id.Namespace.from(metadata.getName()));
  }
 catch (  DatasetManagementException e) {
    throw new NamespaceCannotBeCreatedException(namespace,e);
  }
  store.createNamespace(metadata);
}"
6652,"/** 
 * Adds datasets and types to the given   {@link DatasetFramework} used by artifact store.
 * @param framework framework to add types and datasets to
 */
public static void setupDatasets(DatasetFramework framework) throws IOException, DatasetManagementException, ServiceUnavailableException {
  framework.addInstance(Table.class.getName(),META_ID,META_PROPERTIES);
}","/** 
 * Adds datasets and types to the given   {@link DatasetFramework} used by artifact store.
 * @param framework framework to add types and datasets to
 */
public static void setupDatasets(DatasetFramework framework) throws IOException, DatasetManagementException {
  framework.addInstance(Table.class.getName(),META_ID,META_PROPERTIES);
}"
6653,"/** 
 * Initialize this persistent store.
 */
public void initialize() throws IOException, DatasetManagementException, ServiceUnavailableException {
  table=tableUtil.getMetaTable();
  Preconditions.checkNotNull(table,""String_Node_Str"",ScheduleStoreTableUtil.SCHEDULE_STORE_DATASET_NAME);
}","/** 
 * Initialize this persistent store.
 */
public void initialize() throws IOException, DatasetManagementException {
  table=tableUtil.getMetaTable();
  Preconditions.checkNotNull(table,""String_Node_Str"",ScheduleStoreTableUtil.SCHEDULE_STORE_DATASET_NAME);
}"
6654,"/** 
 * Adds datasets and types to the given   {@link DatasetFramework} used by schedule mds.
 * @param datasetFramework framework to add types and datasets to
 */
public static void setupDatasets(DatasetFramework datasetFramework) throws IOException, DatasetManagementException, ServiceUnavailableException {
  Id.DatasetInstance scheduleStoreDatasetInstance=Id.DatasetInstance.from(Id.Namespace.SYSTEM,SCHEDULE_STORE_DATASET_NAME);
  datasetFramework.addInstance(Table.class.getName(),scheduleStoreDatasetInstance,DatasetProperties.EMPTY);
}","/** 
 * Adds datasets and types to the given   {@link DatasetFramework} used by schedule mds.
 * @param datasetFramework framework to add types and datasets to
 */
public static void setupDatasets(DatasetFramework datasetFramework) throws IOException, DatasetManagementException {
  Id.DatasetInstance scheduleStoreDatasetInstance=Id.DatasetInstance.from(Id.Namespace.SYSTEM,SCHEDULE_STORE_DATASET_NAME);
  datasetFramework.addInstance(Table.class.getName(),scheduleStoreDatasetInstance,DatasetProperties.EMPTY);
}"
6655,"/** 
 * Adds datasets and types to the given   {@link DatasetFramework} used by app mds.
 * @param framework framework to add types and datasets to
 */
public static void setupDatasets(DatasetFramework framework) throws IOException, DatasetManagementException, ServiceUnavailableException {
  framework.addInstance(Table.class.getName(),APP_META_INSTANCE_ID,DatasetProperties.EMPTY);
  framework.addInstance(Table.class.getName(),WORKFLOW_STATS_INSTANCE_ID,DatasetProperties.EMPTY);
}","/** 
 * Adds datasets and types to the given   {@link DatasetFramework} used by app mds.
 * @param framework framework to add types and datasets to
 */
public static void setupDatasets(DatasetFramework framework) throws IOException, DatasetManagementException {
  framework.addInstance(Table.class.getName(),APP_META_INSTANCE_ID,DatasetProperties.EMPTY);
  framework.addInstance(Table.class.getName(),WORKFLOW_STATS_INSTANCE_ID,DatasetProperties.EMPTY);
}"
6656,"@Nullable public <T extends DatasetAdmin>T getDatasetAdmin(Id.DatasetInstance datasetId) throws DatasetManagementException, IOException, ServiceUnavailableException {
  return datasetFramework.getAdmin(datasetId,parentClassLoader,classLoaderProvider);
}","@Nullable public <T extends DatasetAdmin>T getDatasetAdmin(Id.DatasetInstance datasetId) throws DatasetManagementException, IOException {
  return datasetFramework.getAdmin(datasetId,parentClassLoader,classLoaderProvider);
}"
6657,"public DatasetTypeMDS getTypeMetaTable() throws DatasetManagementException, IOException, ServiceUnavailableException {
  return (DatasetTypeMDS)DatasetsUtil.getOrCreateDataset(framework,META_TABLE_INSTANCE_ID,DatasetTypeMDS.class.getName(),DatasetProperties.EMPTY,null,null);
}","public DatasetTypeMDS getTypeMetaTable() throws DatasetManagementException, IOException {
  return (DatasetTypeMDS)DatasetsUtil.getOrCreateDataset(framework,META_TABLE_INSTANCE_ID,DatasetTypeMDS.class.getName(),DatasetProperties.EMPTY,null,null);
}"
6658,"/** 
 * Adds datasets and types to the given   {@link DatasetFramework} used by dataset service mds.
 * @param datasetFramework framework to add types and datasets to
 */
public static void setupDatasets(DatasetFramework datasetFramework) throws IOException, DatasetManagementException, ServiceUnavailableException {
  for (  Map.Entry<String,? extends DatasetModule> entry : getModules().entrySet()) {
    Id.DatasetModule moduleId=Id.DatasetModule.from(Id.Namespace.SYSTEM,entry.getKey());
    datasetFramework.addModule(moduleId,entry.getValue());
  }
  datasetFramework.addInstance(DatasetTypeMDS.class.getName(),Id.DatasetInstance.from(Id.Namespace.SYSTEM,META_TABLE_NAME),DatasetProperties.EMPTY);
  datasetFramework.addInstance(DatasetInstanceMDS.class.getName(),Id.DatasetInstance.from(Id.Namespace.SYSTEM,INSTANCE_TABLE_NAME),DatasetProperties.EMPTY);
}","/** 
 * Adds datasets and types to the given   {@link DatasetFramework} used by dataset service mds.
 * @param datasetFramework framework to add types and datasets to
 */
public static void setupDatasets(DatasetFramework datasetFramework) throws IOException, DatasetManagementException {
  for (  Map.Entry<String,? extends DatasetModule> entry : getModules().entrySet()) {
    Id.DatasetModule moduleId=Id.DatasetModule.from(Id.Namespace.SYSTEM,entry.getKey());
    datasetFramework.addModule(moduleId,entry.getValue());
  }
  datasetFramework.addInstance(DatasetTypeMDS.class.getName(),Id.DatasetInstance.from(Id.Namespace.SYSTEM,META_TABLE_NAME),DatasetProperties.EMPTY);
  datasetFramework.addInstance(DatasetInstanceMDS.class.getName(),Id.DatasetInstance.from(Id.Namespace.SYSTEM,INSTANCE_TABLE_NAME),DatasetProperties.EMPTY);
}"
6659,"public DatasetInstanceMDS getInstanceMetaTable() throws DatasetManagementException, IOException, ServiceUnavailableException {
  return (DatasetInstanceMDS)DatasetsUtil.getOrCreateDataset(framework,INSTANCE_TABLE_INSTANCE_ID,DatasetInstanceMDS.class.getName(),DatasetProperties.EMPTY,null,null);
}","public DatasetInstanceMDS getInstanceMetaTable() throws DatasetManagementException, IOException {
  return (DatasetInstanceMDS)DatasetsUtil.getOrCreateDataset(framework,INSTANCE_TABLE_INSTANCE_ID,DatasetInstanceMDS.class.getName(),DatasetProperties.EMPTY,null,null);
}"
6660,"private HttpResponse doRequest(HttpMethod method,String url) throws DatasetManagementException, ServiceUnavailableException {
  return doRequest(method,url,null,(InputSupplier<? extends InputStream>)null);
}","private HttpResponse doRequest(HttpMethod method,String url) throws DatasetManagementException {
  return doRequest(method,url,null,(InputSupplier<? extends InputStream>)null);
}"
6661,"private HttpResponse doDelete(String resource) throws DatasetManagementException, ServiceUnavailableException {
  return doRequest(HttpMethod.DELETE,resource);
}","private HttpResponse doDelete(String resource) throws DatasetManagementException {
  return doRequest(HttpMethod.DELETE,resource);
}"
6662,"private String resolve(String resource) throws DatasetManagementException, ServiceUnavailableException {
  Discoverable discoverable=endpointStrategySupplier.get().pick(1,TimeUnit.SECONDS);
  if (discoverable == null) {
    throw new ServiceUnavailableException(""String_Node_Str"");
  }
  InetSocketAddress addr=discoverable.getSocketAddress();
  return String.format(""String_Node_Str"",addr.getHostName(),addr.getPort(),Constants.Gateway.API_VERSION_3,namespaceId.getId(),resource);
}","private String resolve(String resource) throws DatasetManagementException {
  Discoverable discoverable=endpointStrategySupplier.get().pick(1,TimeUnit.SECONDS);
  if (discoverable == null) {
    throw new ServiceUnavailableException(""String_Node_Str"");
  }
  InetSocketAddress addr=discoverable.getSocketAddress();
  return String.format(""String_Node_Str"",addr.getHostName(),addr.getPort(),Constants.Gateway.API_VERSION_3,namespaceId.getId(),resource);
}"
6663,"public Collection<DatasetModuleMeta> getAllModules() throws DatasetManagementException, ServiceUnavailableException {
  HttpResponse response=doGet(""String_Node_Str"");
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",response));
  }
  return GSON.fromJson(response.getResponseBodyAsString(),MODULE_META_LIST_TYPE);
}","public Collection<DatasetModuleMeta> getAllModules() throws DatasetManagementException {
  HttpResponse response=doGet(""String_Node_Str"");
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",response));
  }
  return GSON.fromJson(response.getResponseBodyAsString(),MODULE_META_LIST_TYPE);
}"
6664,"public void deleteModules() throws DatasetManagementException, ServiceUnavailableException {
  HttpResponse response=doDelete(""String_Node_Str"");
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",response));
  }
}","public void deleteModules() throws DatasetManagementException {
  HttpResponse response=doDelete(""String_Node_Str"");
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",response));
  }
}"
6665,"public void updateInstance(String datasetInstanceName,DatasetProperties props) throws DatasetManagementException, ServiceUnavailableException {
  HttpResponse response=doPut(""String_Node_Str"" + datasetInstanceName + ""String_Node_Str"",GSON.toJson(props.getProperties()));
  if (HttpResponseStatus.CONFLICT.getCode() == response.getResponseCode()) {
    throw new InstanceConflictException(String.format(""String_Node_Str"",datasetInstanceName,response));
  }
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",datasetInstanceName,response));
  }
}","public void updateInstance(String datasetInstanceName,DatasetProperties props) throws DatasetManagementException {
  HttpResponse response=doPut(""String_Node_Str"" + datasetInstanceName + ""String_Node_Str"",GSON.toJson(props.getProperties()));
  if (HttpResponseStatus.CONFLICT.getCode() == response.getResponseCode()) {
    throw new InstanceConflictException(String.format(""String_Node_Str"",datasetInstanceName,response));
  }
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",datasetInstanceName,response));
  }
}"
6666,"public void addModule(String moduleName,String className,Location jarLocation) throws DatasetManagementException, ServiceUnavailableException {
  HttpResponse response=doRequest(HttpMethod.PUT,""String_Node_Str"" + moduleName,ImmutableMultimap.of(""String_Node_Str"",className),Locations.newInputSupplier(jarLocation));
  if (HttpResponseStatus.CONFLICT.getCode() == response.getResponseCode()) {
    throw new ModuleConflictException(String.format(""String_Node_Str"",moduleName,response));
  }
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",moduleName,response));
  }
}","public void addModule(String moduleName,String className,Location jarLocation) throws DatasetManagementException {
  HttpResponse response=doRequest(HttpMethod.PUT,""String_Node_Str"" + moduleName,ImmutableMultimap.of(""String_Node_Str"",className),Locations.newInputSupplier(jarLocation));
  if (HttpResponseStatus.CONFLICT.getCode() == response.getResponseCode()) {
    throw new ModuleConflictException(String.format(""String_Node_Str"",moduleName,response));
  }
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",moduleName,response));
  }
}"
6667,"public void createNamespace() throws DatasetManagementException, ServiceUnavailableException {
  HttpResponse response=doPut(""String_Node_Str"",GSON.toJson(namespaceId));
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",response));
  }
}","public void createNamespace() throws DatasetManagementException {
  HttpResponse response=doPut(""String_Node_Str"",GSON.toJson(namespaceId));
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",response));
  }
}"
6668,"private HttpResponse doPut(String resource,String body) throws DatasetManagementException, ServiceUnavailableException {
  return doRequest(HttpMethod.PUT,resource,null,body);
}","private HttpResponse doPut(String resource,String body) throws DatasetManagementException {
  return doRequest(HttpMethod.PUT,resource,null,body);
}"
6669,"public void addInstance(String datasetInstanceName,String datasetType,DatasetProperties props) throws DatasetManagementException, ServiceUnavailableException {
  DatasetInstanceConfiguration creationProperties=new DatasetInstanceConfiguration(datasetType,props.getProperties());
  HttpResponse response=doPut(""String_Node_Str"" + datasetInstanceName,GSON.toJson(creationProperties));
  if (HttpResponseStatus.CONFLICT.getCode() == response.getResponseCode()) {
    throw new InstanceConflictException(String.format(""String_Node_Str"",datasetInstanceName,response));
  }
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",datasetInstanceName,response));
  }
}","public void addInstance(String datasetInstanceName,String datasetType,DatasetProperties props) throws DatasetManagementException {
  DatasetInstanceConfiguration creationProperties=new DatasetInstanceConfiguration(datasetType,props.getProperties());
  HttpResponse response=doPut(""String_Node_Str"" + datasetInstanceName,GSON.toJson(creationProperties));
  if (HttpResponseStatus.CONFLICT.getCode() == response.getResponseCode()) {
    throw new InstanceConflictException(String.format(""String_Node_Str"",datasetInstanceName,response));
  }
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",datasetInstanceName,response));
  }
}"
6670,"public void deleteNamespace() throws DatasetManagementException, ServiceUnavailableException {
  HttpResponse response=doDelete(""String_Node_Str"");
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",response));
  }
}","public void deleteNamespace() throws DatasetManagementException {
  HttpResponse response=doDelete(""String_Node_Str"");
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",response));
  }
}"
6671,"public Collection<DatasetSpecificationSummary> getAllInstances() throws DatasetManagementException, ServiceUnavailableException {
  HttpResponse response=doGet(""String_Node_Str"");
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",response));
  }
  return GSON.fromJson(response.getResponseBodyAsString(),SUMMARY_LIST_TYPE);
}","public Collection<DatasetSpecificationSummary> getAllInstances() throws DatasetManagementException {
  HttpResponse response=doGet(""String_Node_Str"");
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",response));
  }
  return GSON.fromJson(response.getResponseBodyAsString(),SUMMARY_LIST_TYPE);
}"
6672,"public DatasetTypeMeta getType(String typeName) throws DatasetManagementException, ServiceUnavailableException {
  HttpResponse response=doGet(""String_Node_Str"" + typeName);
  if (HttpResponseStatus.NOT_FOUND.getCode() == response.getResponseCode()) {
    return null;
  }
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",typeName,response));
  }
  return GSON.fromJson(response.getResponseBodyAsString(),DatasetTypeMeta.class);
}","public DatasetTypeMeta getType(String typeName) throws DatasetManagementException {
  HttpResponse response=doGet(""String_Node_Str"" + typeName);
  if (HttpResponseStatus.NOT_FOUND.getCode() == response.getResponseCode()) {
    return null;
  }
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",typeName,response));
  }
  return GSON.fromJson(response.getResponseBodyAsString(),DatasetTypeMeta.class);
}"
6673,"public void deleteInstance(String datasetInstanceName) throws DatasetManagementException, ServiceUnavailableException {
  HttpResponse response=doDelete(""String_Node_Str"" + datasetInstanceName);
  if (HttpResponseStatus.CONFLICT.getCode() == response.getResponseCode()) {
    throw new InstanceConflictException(String.format(""String_Node_Str"",datasetInstanceName,response));
  }
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",datasetInstanceName,response));
  }
}","public void deleteInstance(String datasetInstanceName) throws DatasetManagementException {
  HttpResponse response=doDelete(""String_Node_Str"" + datasetInstanceName);
  if (HttpResponseStatus.CONFLICT.getCode() == response.getResponseCode()) {
    throw new InstanceConflictException(String.format(""String_Node_Str"",datasetInstanceName,response));
  }
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",datasetInstanceName,response));
  }
}"
6674,"@Nullable public DatasetMeta getInstance(String instanceName) throws DatasetManagementException, ServiceUnavailableException {
  return getInstance(instanceName,null);
}","@Nullable public DatasetMeta getInstance(String instanceName) throws DatasetManagementException {
  return getInstance(instanceName,null);
}"
6675,"public void deleteModule(String moduleName) throws DatasetManagementException, ServiceUnavailableException {
  HttpResponse response=doDelete(""String_Node_Str"" + moduleName);
  if (HttpResponseStatus.CONFLICT.getCode() == response.getResponseCode()) {
    throw new ModuleConflictException(String.format(""String_Node_Str"",moduleName,response));
  }
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",moduleName,response));
  }
}","public void deleteModule(String moduleName) throws DatasetManagementException {
  HttpResponse response=doDelete(""String_Node_Str"" + moduleName);
  if (HttpResponseStatus.CONFLICT.getCode() == response.getResponseCode()) {
    throw new ModuleConflictException(String.format(""String_Node_Str"",moduleName,response));
  }
  if (HttpResponseStatus.OK.getCode() != response.getResponseCode()) {
    throw new DatasetManagementException(String.format(""String_Node_Str"",moduleName,response));
  }
}"
6676,"private HttpResponse doGet(String resource,Multimap<String,String> headers) throws DatasetManagementException, ServiceUnavailableException {
  return doRequest(HttpMethod.GET,resource,headers,(InputSupplier<? extends InputStream>)null);
}","private HttpResponse doGet(String resource,Multimap<String,String> headers) throws DatasetManagementException {
  return doRequest(HttpMethod.GET,resource,headers,(InputSupplier<? extends InputStream>)null);
}"
6677,"@Override public boolean apply(RunRecordMeta record){
  boolean normalCheck=record.getStatus().equals(state.getRunStatus());
  return normalCheck;
}","@Override public RunId apply(RunRecordMeta runRecordMeta){
  return RunIds.fromString(runRecordMeta.getPid());
}"
6678,"@Override public WorkflowToken apply(AppMds mds) throws Exception {
  return mds.apps.getWorkflowToken(workflowId,workflowRunId);
}","@Override public Set<RunId> apply(AppMds input) throws Exception {
  return input.apps.getRunningInRange(startTimeInSecs,endTimeInSecs);
}"
6679,"@Inject LineageHandler(LineageService lineageService,LineageStore lineageStore){
  this.lineageService=lineageService;
  this.lineageStore=lineageStore;
}","@Inject LineageHandler(LineageGenerator lineageGenerator,LineageStore lineageStore){
  this.lineageGenerator=lineageGenerator;
  this.lineageStore=lineageStore;
}"
6680,"@GET @Path(""String_Node_Str"") public void streamLineage(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long start,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long end,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int levels) throws Exception {
  checkArguments(start,end,levels);
  Id.Stream streamId=Id.Stream.from(namespaceId,stream);
  Lineage lineage=lineageService.computeLineage(streamId,start,end,levels);
  responder.sendJson(HttpResponseStatus.OK,new LineageRecord(start,end,lineage.getRelations()),LineageRecord.class,GSON);
}","@GET @Path(""String_Node_Str"") public void streamLineage(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String stream,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long start,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long end,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int levels) throws Exception {
  checkArguments(start,end,levels);
  Id.Stream streamId=Id.Stream.from(namespaceId,stream);
  Lineage lineage=lineageGenerator.computeLineage(streamId,start,end,levels);
  responder.sendJson(HttpResponseStatus.OK,new LineageRecord(start,end,lineage.getRelations()),LineageRecord.class,GSON);
}"
6681,"@GET @Path(""String_Node_Str"") public void datasetLineage(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String datasetId,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long start,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long end,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int levels) throws Exception {
  checkArguments(start,end,levels);
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(namespaceId,datasetId);
  Lineage lineage=lineageService.computeLineage(datasetInstance,start,end,levels);
  responder.sendJson(HttpResponseStatus.OK,new LineageRecord(start,end,lineage.getRelations()),LineageRecord.class,GSON);
}","@GET @Path(""String_Node_Str"") public void datasetLineage(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String datasetId,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long start,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") long end,@QueryParam(""String_Node_Str"") @DefaultValue(""String_Node_Str"") int levels) throws Exception {
  checkArguments(start,end,levels);
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(namespaceId,datasetId);
  Lineage lineage=lineageGenerator.computeLineage(datasetInstance,start,end,levels);
  responder.sendJson(HttpResponseStatus.OK,new LineageRecord(start,end,lineage.getRelations()),LineageRecord.class,GSON);
}"
6682,"private void addRelations(Set<Relation> lineageRelations){
  for (  Relation relation : lineageRelations) {
    String dataKey=makeDataKey(relation.getData());
    String programKey=makeProgramKey(relation.getProgram());
    RelationRecord relationRecord=new RelationRecord(dataKey,programKey,relation.getAccess().toString().toLowerCase(),convertRuns(relation.getRuns()),convertComponents(relation.getComponents()));
    relations.add(relationRecord);
    programs.put(programKey,ImmutableMap.of(""String_Node_Str"",toProgramRecord(relation.getProgram())));
    data.put(dataKey,ImmutableMap.of(""String_Node_Str"",toDataRecord(relation.getData())));
  }
}","private void addRelations(Set<Relation> lineageRelations){
  for (  Relation relation : lineageRelations) {
    String dataKey=makeDataKey(relation.getData());
    String programKey=makeProgramKey(relation.getProgram());
    RelationRecord relationRecord=new RelationRecord(dataKey,programKey,relation.getAccess().toString().toLowerCase(),convertRuns(relation.getRun()),convertComponents(relation.getComponents()));
    relations.add(relationRecord);
    programs.put(programKey,ImmutableMap.of(""String_Node_Str"",toProgramRecord(relation.getProgram())));
    data.put(dataKey,ImmutableMap.of(""String_Node_Str"",toDataRecord(relation.getData())));
  }
}"
6683,"private Set<String> convertRuns(Set<RunId> runIds){
  return Sets.newHashSet(Iterables.transform(runIds,RUN_ID_STRING_FUNCTION));
}","private Set<String> convertRuns(RunId runId){
  return ImmutableSet.of(runId.getId());
}"
6684,"private static Injector getInjector(){
  return dsFrameworkUtil.getInjector().createChildInjector(new AbstractModule(){
    @Override protected void configure(){
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(DatasetFramework.class).annotatedWith(Names.named(DataSetsModules.BASIC_DATASET_FRAMEWORK)).to(InMemoryDatasetFramework.class);
      bind(MetadataChangePublisher.class).to(NoOpMetadataChangePublisher.class);
      bind(BusinessMetadataStore.class).to(InMemoryBusinessMetadataStore.class);
    }
  }
);
}","private static Injector getInjector(){
  return dsFrameworkUtil.getInjector().createChildInjector(new AbstractModule(){
    @Override protected void configure(){
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(DatasetFramework.class).annotatedWith(Names.named(DataSetsModules.BASIC_DATASET_FRAMEWORK)).to(InMemoryDatasetFramework.class);
      bind(MetadataChangePublisher.class).to(NoOpMetadataChangePublisher.class);
      bind(BusinessMetadataStore.class).to(DefaultBusinessMetadataStore.class);
    }
  }
);
}"
6685,"@Override protected void configure(){
  install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
  bind(DatasetFramework.class).annotatedWith(Names.named(DataSetsModules.BASIC_DATASET_FRAMEWORK)).to(InMemoryDatasetFramework.class);
  bind(MetadataChangePublisher.class).to(NoOpMetadataChangePublisher.class);
  bind(BusinessMetadataStore.class).to(InMemoryBusinessMetadataStore.class);
}","@Override protected void configure(){
  install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
  bind(DatasetFramework.class).annotatedWith(Names.named(DataSetsModules.BASIC_DATASET_FRAMEWORK)).to(InMemoryDatasetFramework.class);
  bind(MetadataChangePublisher.class).to(NoOpMetadataChangePublisher.class);
  bind(BusinessMetadataStore.class).to(DefaultBusinessMetadataStore.class);
}"
6686,"private String getV3RoutingService(String[] uriParts,AllowedMethod requestMethod){
  if ((uriParts.length >= 2) && uriParts[1].equals(""String_Node_Str"")) {
    return null;
  }
 else   if ((uriParts.length >= 9) && ""String_Node_Str"".equals(uriParts[5]) && ""String_Node_Str"".equals(uriParts[7])) {
    return String.format(""String_Node_Str"",uriParts[2],uriParts[4],uriParts[6]);
  }
 else   if (matches(uriParts,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if (matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"",null,""String_Node_Str"")) {
    return Constants.Service.METADATA_SERVICE;
  }
 else   if ((matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) && requestMethod.equals(AllowedMethod.GET)) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if ((uriParts.length >= 4) && uriParts[1].equals(""String_Node_Str"") && uriParts[3].equals(""String_Node_Str"")) {
    if (uriParts.length == 4) {
      return Constants.Service.APP_FABRIC_HTTP;
    }
 else {
      return Constants.Service.STREAMS;
    }
  }
 else   if ((uriParts.length >= 8 && uriParts[7].equals(""String_Node_Str"")) || (uriParts.length >= 10 && uriParts[9].equals(""String_Node_Str"")) || (uriParts.length >= 6 && uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 2 && uriParts[1].equals(""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 5 && uriParts[1].equals(""String_Node_Str"") && uriParts[2].equals(""String_Node_Str"") && (uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length >= 6 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if ((uriParts.length == 3) && uriParts[1].equals(""String_Node_Str"") && uriParts[2].equals(""String_Node_Str"")) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length == 7 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str""))) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if ((uriParts.length >= 4) && uriParts[3].equals(""String_Node_Str"")) {
    return Constants.Service.DATASET_MANAGER;
  }
  return Constants.Service.APP_FABRIC_HTTP;
}","private String getV3RoutingService(String[] uriParts,AllowedMethod requestMethod){
  if ((uriParts.length >= 2) && uriParts[1].equals(""String_Node_Str"")) {
    return null;
  }
 else   if ((uriParts.length >= 9) && ""String_Node_Str"".equals(uriParts[5]) && ""String_Node_Str"".equals(uriParts[7])) {
    return String.format(""String_Node_Str"",uriParts[2],uriParts[4],uriParts[6]);
  }
 else   if (matches(uriParts,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if (matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"",null,""String_Node_Str"")) {
    return Constants.Service.METADATA_SERVICE;
  }
 else   if ((matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) && requestMethod.equals(AllowedMethod.GET)) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if ((uriParts.length >= 4) && uriParts[1].equals(""String_Node_Str"") && uriParts[3].equals(""String_Node_Str"")) {
    if (uriParts.length == 4) {
      return Constants.Service.APP_FABRIC_HTTP;
    }
 else {
      return Constants.Service.STREAMS;
    }
  }
 else   if ((uriParts.length >= 8 && uriParts[7].equals(""String_Node_Str"")) || (uriParts.length >= 10 && uriParts[9].equals(""String_Node_Str"")) || (uriParts.length >= 6 && uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 2 && uriParts[1].equals(""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 5 && uriParts[1].equals(""String_Node_Str"") && uriParts[2].equals(""String_Node_Str"") && (uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length >= 6 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if ((uriParts.length == 3) && uriParts[1].equals(""String_Node_Str"") && uriParts[2].equals(""String_Node_Str"")) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length == 7 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str""))) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if ((uriParts.length >= 4) && uriParts[3].equals(""String_Node_Str"")) {
    return Constants.Service.DATASET_MANAGER;
  }
  return Constants.Service.APP_FABRIC_HTTP;
}"
6687,"@Test public void testMetadataPath(){
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
}","@Test public void testMetadataPath(){
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
  assertMetadataRouting(""String_Node_Str"");
}"
6688,"@GET @Path(""String_Node_Str"") public void getAccessesForRun(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId,@PathParam(""String_Node_Str"") String runId) throws Exception {
  Id.Run run=new Id.Run(Id.Program.from(namespaceId,appId,ProgramType.valueOfCategoryName(programType),programId),runId);
  responder.sendJson(HttpResponseStatus.OK,lineageStore.getAccesses(run),SET_METADATA_RECORD_TYPE,GSON);
}","@GET @Path(""String_Node_Str"") public void getAccessesForRun(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId,@PathParam(""String_Node_Str"") String runId) throws Exception {
  Id.Run run=new Id.Run(Id.Program.from(namespaceId,appId,ProgramType.valueOfCategoryName(programType),programId),runId);
  responder.sendJson(HttpResponseStatus.OK,lineageStore.getRunMetadata(run),SET_METADATA_RECORD_TYPE,GSON);
}"
6689,"@Test public void testFlowLineage() throws Exception {
  String namespace=""String_Node_Str"";
  Id.Application app=Id.Application.from(namespace,AllProgramsApp.NAME);
  Id.Flow flow=Id.Flow.from(app,AllProgramsApp.NoOpFlow.NAME);
  Id.DatasetInstance dataset=Id.DatasetInstance.from(namespace,AllProgramsApp.DATASET_NAME);
  Id.Stream stream=Id.Stream.from(namespace,AllProgramsApp.STREAM_NAME);
  Assert.assertEquals(200,status(createNamespace(namespace)));
  try {
    Assert.assertEquals(200,status(deploy(AllProgramsApp.class,Constants.Gateway.API_VERSION_3_TOKEN,namespace)));
    ImmutableMap<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(app,appProperties).getResponseCode());
    Assert.assertEquals(appProperties,getProperties(app));
    ImmutableSet<String> appTags=ImmutableSet.of(""String_Node_Str"");
    Assert.assertEquals(200,addTags(app,appTags).getResponseCode());
    Assert.assertEquals(appTags,getTags(app));
    ImmutableMap<String,String> flowProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(flow,flowProperties).getResponseCode());
    Assert.assertEquals(flowProperties,getProperties(flow));
    ImmutableSet<String> flowTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addTags(flow,flowTags).getResponseCode());
    Assert.assertEquals(flowTags,getTags(flow));
    ImmutableMap<String,String> dataProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(dataset,dataProperties).getResponseCode());
    Assert.assertEquals(dataProperties,getProperties(dataset));
    ImmutableSet<String> dataTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addTags(dataset,dataTags).getResponseCode());
    Assert.assertEquals(dataTags,getTags(dataset));
    ImmutableMap<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(stream,streamProperties).getResponseCode());
    Assert.assertEquals(streamProperties,getProperties(stream));
    ImmutableSet<String> streamTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addTags(stream,streamTags).getResponseCode());
    Assert.assertEquals(streamTags,getTags(stream));
    RunId flowRunId=runAndWait(flow);
    waitForStop(flow,true);
    long now=System.currentTimeMillis();
    long oneHourMillis=TimeUnit.HOURS.toMillis(1);
    HttpResponse httpResponse=fetchLineage(dataset,now - oneHourMillis,now + oneHourMillis,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    LineageRecord lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    LineageRecord expected=new LineageRecord(now - oneHourMillis,now + oneHourMillis,ImmutableSet.of(new Relation(dataset,flow,AccessType.UNKNOWN,ImmutableSet.of(flowRunId),ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME))),new Relation(stream,flow,AccessType.READ,ImmutableSet.of(flowRunId),ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME)))));
    Assert.assertEquals(expected,lineage);
    httpResponse=fetchLineage(stream,now - oneHourMillis,now + oneHourMillis,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(expected,lineage);
    Id.Program programForFlow=Id.Program.from(flow.getApplication(),flow.getType(),flow.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,appProperties,appTags),new MetadataRecord(programForFlow,flowProperties,flowTags),new MetadataRecord(dataset,dataProperties,dataTags),new MetadataRecord(stream,streamProperties,streamTags)),fetchAccesses(new Id.Run(flow,flowRunId.getId())));
  }
  finally {
    try {
      deleteNamespace(namespace);
    }
 catch (    Throwable e) {
      LOG.error(""String_Node_Str"",namespace,e);
    }
  }
}","@Test public void testFlowLineage() throws Exception {
  String namespace=""String_Node_Str"";
  Id.Application app=Id.Application.from(namespace,AllProgramsApp.NAME);
  Id.Flow flow=Id.Flow.from(app,AllProgramsApp.NoOpFlow.NAME);
  Id.DatasetInstance dataset=Id.DatasetInstance.from(namespace,AllProgramsApp.DATASET_NAME);
  Id.Stream stream=Id.Stream.from(namespace,AllProgramsApp.STREAM_NAME);
  Assert.assertEquals(200,status(createNamespace(namespace)));
  try {
    Assert.assertEquals(200,status(deploy(AllProgramsApp.class,Constants.Gateway.API_VERSION_3_TOKEN,namespace)));
    ImmutableMap<String,String> appProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(app,appProperties).getResponseCode());
    Assert.assertEquals(appProperties,getProperties(app));
    ImmutableSet<String> appTags=ImmutableSet.of(""String_Node_Str"");
    Assert.assertEquals(200,addTags(app,appTags).getResponseCode());
    Assert.assertEquals(appTags,getTags(app));
    ImmutableMap<String,String> flowProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(flow,flowProperties).getResponseCode());
    Assert.assertEquals(flowProperties,getProperties(flow));
    ImmutableSet<String> flowTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addTags(flow,flowTags).getResponseCode());
    Assert.assertEquals(flowTags,getTags(flow));
    ImmutableMap<String,String> dataProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(dataset,dataProperties).getResponseCode());
    Assert.assertEquals(dataProperties,getProperties(dataset));
    ImmutableSet<String> dataTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addTags(dataset,dataTags).getResponseCode());
    Assert.assertEquals(dataTags,getTags(dataset));
    ImmutableMap<String,String> streamProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addProperties(stream,streamProperties).getResponseCode());
    Assert.assertEquals(streamProperties,getProperties(stream));
    ImmutableSet<String> streamTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    Assert.assertEquals(200,addTags(stream,streamTags).getResponseCode());
    Assert.assertEquals(streamTags,getTags(stream));
    RunId flowRunId=runAndWait(flow);
    waitForStop(flow,true);
    long now=System.currentTimeMillis();
    long oneHourMillis=TimeUnit.HOURS.toMillis(1);
    HttpResponse httpResponse=fetchLineage(dataset,now - oneHourMillis,now + oneHourMillis,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    LineageRecord lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    LineageRecord expected=new LineageRecord(now - oneHourMillis,now + oneHourMillis,ImmutableSet.of(new Relation(dataset,flow,AccessType.UNKNOWN,ImmutableSet.of(flowRunId),ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME))),new Relation(stream,flow,AccessType.READ,ImmutableSet.of(flowRunId),ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME)))));
    Assert.assertEquals(expected,lineage);
    httpResponse=fetchLineage(stream,now - oneHourMillis,now + oneHourMillis,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(expected,lineage);
    Id.Program programForFlow=Id.Program.from(flow.getApplication(),flow.getType(),flow.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,appProperties,appTags),new MetadataRecord(programForFlow,flowProperties,flowTags),new MetadataRecord(dataset,dataProperties,dataTags),new MetadataRecord(stream,streamProperties,streamTags)),fetchRunMetadata(new Id.Run(flow,flowRunId.getId())));
  }
  finally {
    try {
      deleteNamespace(namespace);
    }
 catch (    Throwable e) {
      LOG.error(""String_Node_Str"",namespace,e);
    }
  }
}"
6690,"@Test public void testAllProgramsLineage() throws Exception {
  String namespace=""String_Node_Str"";
  Id.Application app=Id.Application.from(namespace,AllProgramsApp.NAME);
  Id.Flow flow=Id.Flow.from(app,AllProgramsApp.NoOpFlow.NAME);
  Id.Program mapreduce=Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR.NAME);
  Id.Program spark=Id.Program.from(app,ProgramType.SPARK,AllProgramsApp.NoOpSpark.NAME);
  Id.Program service=Id.Program.from(app,ProgramType.SERVICE,AllProgramsApp.NoOpService.NAME);
  Id.Program worker=Id.Program.from(app,ProgramType.WORKER,AllProgramsApp.NoOpWorker.NAME);
  Id.Program workflow=Id.Program.from(app,ProgramType.WORKFLOW,AllProgramsApp.NoOpWorkflow.NAME);
  Id.DatasetInstance dataset=Id.DatasetInstance.from(namespace,AllProgramsApp.DATASET_NAME);
  Id.Stream stream=Id.Stream.from(namespace,AllProgramsApp.STREAM_NAME);
  Assert.assertEquals(200,status(createNamespace(namespace)));
  try {
    Assert.assertEquals(200,status(deploy(AllProgramsApp.class,Constants.Gateway.API_VERSION_3_TOKEN,namespace)));
    ImmutableSet<String> sparkTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    addTags(spark,sparkTags);
    Assert.assertEquals(sparkTags,getTags(spark));
    ImmutableSet<String> workerTags=ImmutableSet.of(""String_Node_Str"");
    addTags(worker,workerTags);
    Assert.assertEquals(workerTags,getTags(worker));
    ImmutableMap<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    addProperties(dataset,datasetProperties);
    Assert.assertEquals(datasetProperties,getProperties(dataset));
    RunId flowRunId=runAndWait(flow);
    RunId mrRunId=runAndWait(mapreduce);
    RunId sparkRunId=runAndWait(spark);
    runAndWait(workflow);
    RunId workflowMrRunId=getRunId(mapreduce,mrRunId);
    RunId serviceRunId=runAndWait(service);
    RunId workerRunId=runAndWait(worker);
    waitForStop(flow,true);
    waitForStop(mapreduce,false);
    waitForStop(spark,false);
    waitForStop(workflow,false);
    waitForStop(worker,false);
    waitForStop(service,true);
    long now=System.currentTimeMillis();
    long oneHourMillis=TimeUnit.HOURS.toMillis(1);
    HttpResponse httpResponse=fetchLineage(dataset,now - oneHourMillis,now + oneHourMillis,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    LineageRecord lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    LineageRecord expected=new LineageRecord(now - oneHourMillis,now + oneHourMillis,ImmutableSet.of(new Relation(dataset,flow,AccessType.UNKNOWN,ImmutableSet.of(flowRunId),ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME))),new Relation(dataset,mapreduce,AccessType.UNKNOWN,ImmutableSet.of(mrRunId)),new Relation(dataset,spark,AccessType.UNKNOWN,ImmutableSet.of(sparkRunId)),new Relation(dataset,mapreduce,AccessType.UNKNOWN,ImmutableSet.of(workflowMrRunId)),new Relation(dataset,service,AccessType.UNKNOWN,ImmutableSet.of(serviceRunId)),new Relation(dataset,worker,AccessType.UNKNOWN,ImmutableSet.of(workerRunId)),new Relation(stream,flow,AccessType.READ,ImmutableSet.of(flowRunId),ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME))),new Relation(stream,mapreduce,AccessType.READ,ImmutableSet.of(mrRunId)),new Relation(stream,spark,AccessType.READ,ImmutableSet.of(sparkRunId)),new Relation(stream,mapreduce,AccessType.READ,ImmutableSet.of(workflowMrRunId)),new Relation(stream,worker,AccessType.WRITE,ImmutableSet.of(workerRunId))));
    Assert.assertEquals(expected,lineage);
    httpResponse=fetchLineage(stream,now - oneHourMillis,now + oneHourMillis,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(expected,lineage);
    Id.Program programForFlow=Id.Program.from(flow.getApplication(),flow.getType(),flow.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,emptyMap(),emptySet()),new MetadataRecord(programForFlow,emptyMap(),emptySet()),new MetadataRecord(dataset,datasetProperties,emptySet()),new MetadataRecord(stream,emptyMap(),emptySet())),fetchAccesses(new Id.Run(flow,flowRunId.getId())));
    Id.Program programForWorker=Id.Program.from(worker.getApplication(),worker.getType(),worker.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,emptyMap(),emptySet()),new MetadataRecord(programForWorker,emptyMap(),workerTags),new MetadataRecord(dataset,datasetProperties,emptySet()),new MetadataRecord(stream,emptyMap(),emptySet())),fetchAccesses(new Id.Run(worker,workerRunId.getId())));
    Id.Program programForSpark=Id.Program.from(spark.getApplication(),spark.getType(),spark.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,emptyMap(),emptySet()),new MetadataRecord(programForSpark,emptyMap(),sparkTags),new MetadataRecord(dataset,datasetProperties,emptySet()),new MetadataRecord(stream,emptyMap(),emptySet())),fetchAccesses(new Id.Run(spark,sparkRunId.getId())));
  }
  finally {
    try {
      deleteNamespace(namespace);
    }
 catch (    Throwable e) {
      LOG.error(""String_Node_Str"",namespace,e);
    }
  }
}","@Test public void testAllProgramsLineage() throws Exception {
  String namespace=""String_Node_Str"";
  Id.Application app=Id.Application.from(namespace,AllProgramsApp.NAME);
  Id.Flow flow=Id.Flow.from(app,AllProgramsApp.NoOpFlow.NAME);
  Id.Program mapreduce=Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR.NAME);
  Id.Program spark=Id.Program.from(app,ProgramType.SPARK,AllProgramsApp.NoOpSpark.NAME);
  Id.Program service=Id.Program.from(app,ProgramType.SERVICE,AllProgramsApp.NoOpService.NAME);
  Id.Program worker=Id.Program.from(app,ProgramType.WORKER,AllProgramsApp.NoOpWorker.NAME);
  Id.Program workflow=Id.Program.from(app,ProgramType.WORKFLOW,AllProgramsApp.NoOpWorkflow.NAME);
  Id.DatasetInstance dataset=Id.DatasetInstance.from(namespace,AllProgramsApp.DATASET_NAME);
  Id.Stream stream=Id.Stream.from(namespace,AllProgramsApp.STREAM_NAME);
  Assert.assertEquals(200,status(createNamespace(namespace)));
  try {
    Assert.assertEquals(200,status(deploy(AllProgramsApp.class,Constants.Gateway.API_VERSION_3_TOKEN,namespace)));
    ImmutableSet<String> sparkTags=ImmutableSet.of(""String_Node_Str"",""String_Node_Str"");
    addTags(spark,sparkTags);
    Assert.assertEquals(sparkTags,getTags(spark));
    ImmutableSet<String> workerTags=ImmutableSet.of(""String_Node_Str"");
    addTags(worker,workerTags);
    Assert.assertEquals(workerTags,getTags(worker));
    ImmutableMap<String,String> datasetProperties=ImmutableMap.of(""String_Node_Str"",""String_Node_Str"");
    addProperties(dataset,datasetProperties);
    Assert.assertEquals(datasetProperties,getProperties(dataset));
    RunId flowRunId=runAndWait(flow);
    RunId mrRunId=runAndWait(mapreduce);
    RunId sparkRunId=runAndWait(spark);
    runAndWait(workflow);
    RunId workflowMrRunId=getRunId(mapreduce,mrRunId);
    RunId serviceRunId=runAndWait(service);
    RunId workerRunId=runAndWait(worker);
    waitForStop(flow,true);
    waitForStop(mapreduce,false);
    waitForStop(spark,false);
    waitForStop(workflow,false);
    waitForStop(worker,false);
    waitForStop(service,true);
    long now=System.currentTimeMillis();
    long oneHourMillis=TimeUnit.HOURS.toMillis(1);
    HttpResponse httpResponse=fetchLineage(dataset,now - oneHourMillis,now + oneHourMillis,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    LineageRecord lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    LineageRecord expected=new LineageRecord(now - oneHourMillis,now + oneHourMillis,ImmutableSet.of(new Relation(dataset,flow,AccessType.UNKNOWN,ImmutableSet.of(flowRunId),ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME))),new Relation(dataset,mapreduce,AccessType.UNKNOWN,ImmutableSet.of(mrRunId)),new Relation(dataset,spark,AccessType.UNKNOWN,ImmutableSet.of(sparkRunId)),new Relation(dataset,mapreduce,AccessType.UNKNOWN,ImmutableSet.of(workflowMrRunId)),new Relation(dataset,service,AccessType.UNKNOWN,ImmutableSet.of(serviceRunId)),new Relation(dataset,worker,AccessType.UNKNOWN,ImmutableSet.of(workerRunId)),new Relation(stream,flow,AccessType.READ,ImmutableSet.of(flowRunId),ImmutableSet.of(Id.Flow.Flowlet.from(flow,AllProgramsApp.A.NAME))),new Relation(stream,mapreduce,AccessType.READ,ImmutableSet.of(mrRunId)),new Relation(stream,spark,AccessType.READ,ImmutableSet.of(sparkRunId)),new Relation(stream,mapreduce,AccessType.READ,ImmutableSet.of(workflowMrRunId)),new Relation(stream,worker,AccessType.WRITE,ImmutableSet.of(workerRunId))));
    Assert.assertEquals(expected,lineage);
    httpResponse=fetchLineage(stream,now - oneHourMillis,now + oneHourMillis,10);
    Assert.assertEquals(200,httpResponse.getResponseCode());
    lineage=GSON.fromJson(httpResponse.getResponseBodyAsString(),LineageRecord.class);
    Assert.assertEquals(expected,lineage);
    Id.Program programForFlow=Id.Program.from(flow.getApplication(),flow.getType(),flow.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,emptyMap(),emptySet()),new MetadataRecord(programForFlow,emptyMap(),emptySet()),new MetadataRecord(dataset,datasetProperties,emptySet()),new MetadataRecord(stream,emptyMap(),emptySet())),fetchRunMetadata(new Id.Run(flow,flowRunId.getId())));
    Id.Program programForWorker=Id.Program.from(worker.getApplication(),worker.getType(),worker.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,emptyMap(),emptySet()),new MetadataRecord(programForWorker,emptyMap(),workerTags),new MetadataRecord(dataset,datasetProperties,emptySet()),new MetadataRecord(stream,emptyMap(),emptySet())),fetchRunMetadata(new Id.Run(worker,workerRunId.getId())));
    Id.Program programForSpark=Id.Program.from(spark.getApplication(),spark.getType(),spark.getId());
    Assert.assertEquals(toSet(new MetadataRecord(app,emptyMap(),emptySet()),new MetadataRecord(programForSpark,emptyMap(),sparkTags),new MetadataRecord(dataset,datasetProperties,emptySet()),new MetadataRecord(stream,emptyMap(),emptySet())),fetchRunMetadata(new Id.Run(spark,sparkRunId.getId())));
  }
  finally {
    try {
      deleteNamespace(namespace);
    }
 catch (    Throwable e) {
      LOG.error(""String_Node_Str"",namespace,e);
    }
  }
}"
6691,"@Test public void testOneRelation() throws Exception {
  LineageDataset lineageDataset=getLineageDataset(""String_Node_Str"");
  Assert.assertNotNull(lineageDataset);
  RunId runId=RunIds.generate(10000);
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(""String_Node_Str"",""String_Node_Str"");
  Id.Program program=Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.FLOW,""String_Node_Str"");
  Id.Flow.Flowlet flowlet=Id.Flow.Flowlet.from(program.getApplication(),program.getId(),""String_Node_Str"");
  Id.Run run=new Id.Run(program,runId.getId());
  MetadataRecord programMeta=new MetadataRecord(program,toMap(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"",""String_Node_Str""));
  MetadataRecord dataMeta=new MetadataRecord(datasetInstance,toMap(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"",""String_Node_Str""));
  Set<MetadataRecord> metadataRecords=toSet(programMeta,dataMeta);
  lineageDataset.addAccess(run,datasetInstance,AccessType.READ,metadataRecords,flowlet);
  Relation expected=new Relation(datasetInstance,program,AccessType.READ,ImmutableSet.of(runId),ImmutableSet.of(flowlet));
  Set<Relation> relations=lineageDataset.getRelations(datasetInstance,0,100000);
  Assert.assertEquals(1,relations.size());
  Assert.assertEquals(expected,relations.iterator().next());
  System.out.println(lineageDataset.getAccesses(run));
  Assert.assertEquals(metadataRecords,lineageDataset.getAccesses(run));
}","@Test public void testOneRelation() throws Exception {
  LineageDataset lineageDataset=getLineageDataset(""String_Node_Str"");
  Assert.assertNotNull(lineageDataset);
  RunId runId=RunIds.generate(10000);
  Id.DatasetInstance datasetInstance=Id.DatasetInstance.from(""String_Node_Str"",""String_Node_Str"");
  Id.Program program=Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.FLOW,""String_Node_Str"");
  Id.Flow.Flowlet flowlet=Id.Flow.Flowlet.from(program.getApplication(),program.getId(),""String_Node_Str"");
  Id.Run run=new Id.Run(program,runId.getId());
  MetadataRecord programMeta=new MetadataRecord(program,toMap(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"",""String_Node_Str""));
  MetadataRecord dataMeta=new MetadataRecord(datasetInstance,toMap(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"",""String_Node_Str""));
  Set<MetadataRecord> metadataRecords=toSet(programMeta,dataMeta);
  lineageDataset.addAccess(run,datasetInstance,AccessType.READ,metadataRecords,flowlet);
  Relation expected=new Relation(datasetInstance,program,AccessType.READ,ImmutableSet.of(runId),ImmutableSet.of(flowlet));
  Set<Relation> relations=lineageDataset.getRelations(datasetInstance,0,100000);
  Assert.assertEquals(1,relations.size());
  Assert.assertEquals(expected,relations.iterator().next());
  Assert.assertEquals(metadataRecords,lineageDataset.getRunMetadata(run));
}"
6692,"@Test public void testMultipleRelations() throws Exception {
  LineageDataset lineageDataset=getLineageDataset(""String_Node_Str"");
  Assert.assertNotNull(lineageDataset);
  RunId runId1=RunIds.generate(10000);
  RunId runId2=RunIds.generate(20000);
  RunId runId3=RunIds.generate(30000);
  RunId runId4=RunIds.generate(40000);
  Id.DatasetInstance datasetInstance1=Id.DatasetInstance.from(""String_Node_Str"",""String_Node_Str"");
  Id.DatasetInstance datasetInstance2=Id.DatasetInstance.from(""String_Node_Str"",""String_Node_Str"");
  Id.Stream stream1=Id.Stream.from(""String_Node_Str"",""String_Node_Str"");
  Id.Stream stream2=Id.Stream.from(""String_Node_Str"",""String_Node_Str"");
  Id.Program program1=Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.FLOW,""String_Node_Str"");
  Id.Flow.Flowlet flowlet1=Id.Flow.Flowlet.from(program1.getApplication(),program1.getId(),""String_Node_Str"");
  Id.Program program2=Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.WORKER,""String_Node_Str"");
  Id.Program program3=Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.SERVICE,""String_Node_Str"");
  Id.Run run11=new Id.Run(program1,runId1.getId());
  Id.Run run22=new Id.Run(program2,runId2.getId());
  Id.Run run23=new Id.Run(program2,runId3.getId());
  Id.Run run34=new Id.Run(program3,runId4.getId());
  Set<MetadataRecord> metaProgram1Data1Run1=toSet(new MetadataRecord(program1,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")),new MetadataRecord(datasetInstance1,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  MetadataRecord metadataRecordProgram2Run2=new MetadataRecord(program2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str""));
  Set<MetadataRecord> metaProgram2Data2Run2=toSet(metadataRecordProgram2Run2,new MetadataRecord(datasetInstance2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  Set<MetadataRecord> metaProgram2Stream1Run2=toSet(metadataRecordProgram2Run2,new MetadataRecord(stream1,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  MetadataRecord metadataRecordProgram2Run3=new MetadataRecord(program2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str""));
  Set<MetadataRecord> metaProgram2Stream2Run3=toSet(metadataRecordProgram2Run3,new MetadataRecord(stream2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  Set<MetadataRecord> metaProgram2Data2Run3=toSet(metadataRecordProgram2Run3,new MetadataRecord(datasetInstance2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  Set<MetadataRecord> metaProgram3Data2Run4=toSet(new MetadataRecord(program3,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")),new MetadataRecord(datasetInstance2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  lineageDataset.addAccess(run11,datasetInstance1,AccessType.READ,metaProgram1Data1Run1,flowlet1);
  lineageDataset.addAccess(run22,datasetInstance2,AccessType.WRITE,metaProgram2Data2Run2);
  lineageDataset.addAccess(run22,stream1,AccessType.READ,metaProgram2Stream1Run2);
  lineageDataset.addAccess(run23,stream2,AccessType.READ,metaProgram2Stream2Run3);
  lineageDataset.addAccess(run23,datasetInstance2,AccessType.WRITE,metaProgram2Data2Run3);
  lineageDataset.addAccess(run34,datasetInstance2,AccessType.READ_WRITE,metaProgram3Data2Run4);
  lineageDataset.addAccess(run34,stream2,AccessType.UNKNOWN,EMPTY_METADATA);
  Assert.assertEquals(ImmutableSet.of(new Relation(datasetInstance1,program1,AccessType.READ,ImmutableSet.of(runId1),ImmutableSet.of(flowlet1))),lineageDataset.getRelations(datasetInstance1,0,100000));
  Assert.assertEquals(ImmutableSet.of(new Relation(datasetInstance2,program2,AccessType.WRITE,ImmutableSet.of(runId2)),new Relation(datasetInstance2,program2,AccessType.WRITE,ImmutableSet.of(runId3)),new Relation(datasetInstance2,program3,AccessType.READ_WRITE,ImmutableSet.of(runId4))),lineageDataset.getRelations(datasetInstance2,0,100000));
  Assert.assertEquals(ImmutableSet.of(new Relation(stream1,program2,AccessType.READ,ImmutableSet.of(runId2))),lineageDataset.getRelations(stream1,0,100000));
  Assert.assertEquals(ImmutableSet.of(new Relation(stream2,program2,AccessType.READ,ImmutableSet.of(runId3)),new Relation(stream2,program3,AccessType.UNKNOWN,ImmutableSet.of(runId4))),lineageDataset.getRelations(stream2,0,100000));
  Assert.assertEquals(ImmutableSet.of(new Relation(datasetInstance2,program2,AccessType.WRITE,ImmutableSet.of(runId2)),new Relation(stream1,program2,AccessType.READ,ImmutableSet.of(runId2)),new Relation(datasetInstance2,program2,AccessType.WRITE,ImmutableSet.of(runId3)),new Relation(stream2,program2,AccessType.READ,ImmutableSet.of(runId3))),lineageDataset.getRelations(program2,0,100000));
  Assert.assertEquals(ImmutableSet.of(new Relation(datasetInstance2,program2,AccessType.WRITE,ImmutableSet.of(runId2)),new Relation(datasetInstance2,program2,AccessType.WRITE,ImmutableSet.of(runId3))),lineageDataset.getRelations(datasetInstance2,0,35000));
  Assert.assertEquals(metaProgram1Data1Run1,lineageDataset.getAccesses(run11));
  Assert.assertEquals(2,lineageDataset.getAccesses(run11).size());
  Assert.assertEquals(toSet(metaProgram2Data2Run2,metaProgram2Stream1Run2),lineageDataset.getAccesses(run22));
  Assert.assertEquals(3,lineageDataset.getAccesses(run22).size());
  Assert.assertEquals(toSet(metaProgram2Stream2Run3,metaProgram2Data2Run3),lineageDataset.getAccesses(run23));
  Assert.assertEquals(3,lineageDataset.getAccesses(run23).size());
  Assert.assertEquals(metaProgram3Data2Run4,lineageDataset.getAccesses(run34));
  Assert.assertEquals(2,lineageDataset.getAccesses(run34).size());
}","@Test public void testMultipleRelations() throws Exception {
  LineageDataset lineageDataset=getLineageDataset(""String_Node_Str"");
  Assert.assertNotNull(lineageDataset);
  RunId runId1=RunIds.generate(10000);
  RunId runId2=RunIds.generate(20000);
  RunId runId3=RunIds.generate(30000);
  RunId runId4=RunIds.generate(40000);
  Id.DatasetInstance datasetInstance1=Id.DatasetInstance.from(""String_Node_Str"",""String_Node_Str"");
  Id.DatasetInstance datasetInstance2=Id.DatasetInstance.from(""String_Node_Str"",""String_Node_Str"");
  Id.Stream stream1=Id.Stream.from(""String_Node_Str"",""String_Node_Str"");
  Id.Stream stream2=Id.Stream.from(""String_Node_Str"",""String_Node_Str"");
  Id.Program program1=Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.FLOW,""String_Node_Str"");
  Id.Flow.Flowlet flowlet1=Id.Flow.Flowlet.from(program1.getApplication(),program1.getId(),""String_Node_Str"");
  Id.Program program2=Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.WORKER,""String_Node_Str"");
  Id.Program program3=Id.Program.from(""String_Node_Str"",""String_Node_Str"",ProgramType.SERVICE,""String_Node_Str"");
  Id.Run run11=new Id.Run(program1,runId1.getId());
  Id.Run run22=new Id.Run(program2,runId2.getId());
  Id.Run run23=new Id.Run(program2,runId3.getId());
  Id.Run run34=new Id.Run(program3,runId4.getId());
  Set<MetadataRecord> metaProgram1Data1Run1=toSet(new MetadataRecord(program1,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")),new MetadataRecord(datasetInstance1,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  MetadataRecord metadataRecordProgram2Run2=new MetadataRecord(program2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str""));
  Set<MetadataRecord> metaProgram2Data2Run2=toSet(metadataRecordProgram2Run2,new MetadataRecord(datasetInstance2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  Set<MetadataRecord> metaProgram2Stream1Run2=toSet(metadataRecordProgram2Run2,new MetadataRecord(stream1,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  MetadataRecord metadataRecordProgram2Run3=new MetadataRecord(program2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str""));
  Set<MetadataRecord> metaProgram2Stream2Run3=toSet(metadataRecordProgram2Run3,new MetadataRecord(stream2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  Set<MetadataRecord> metaProgram2Data2Run3=toSet(metadataRecordProgram2Run3,new MetadataRecord(datasetInstance2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  Set<MetadataRecord> metaProgram3Data2Run4=toSet(new MetadataRecord(program3,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")),new MetadataRecord(datasetInstance2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  lineageDataset.addAccess(run11,datasetInstance1,AccessType.READ,metaProgram1Data1Run1,flowlet1);
  lineageDataset.addAccess(run22,datasetInstance2,AccessType.WRITE,metaProgram2Data2Run2);
  lineageDataset.addAccess(run22,stream1,AccessType.READ,metaProgram2Stream1Run2);
  lineageDataset.addAccess(run23,stream2,AccessType.READ,metaProgram2Stream2Run3);
  lineageDataset.addAccess(run23,datasetInstance2,AccessType.WRITE,metaProgram2Data2Run3);
  lineageDataset.addAccess(run34,datasetInstance2,AccessType.READ_WRITE,metaProgram3Data2Run4);
  lineageDataset.addAccess(run34,stream2,AccessType.UNKNOWN,EMPTY_METADATA);
  Assert.assertEquals(ImmutableSet.of(new Relation(datasetInstance1,program1,AccessType.READ,ImmutableSet.of(runId1),ImmutableSet.of(flowlet1))),lineageDataset.getRelations(datasetInstance1,0,100000));
  Assert.assertEquals(ImmutableSet.of(new Relation(datasetInstance2,program2,AccessType.WRITE,ImmutableSet.of(runId2)),new Relation(datasetInstance2,program2,AccessType.WRITE,ImmutableSet.of(runId3)),new Relation(datasetInstance2,program3,AccessType.READ_WRITE,ImmutableSet.of(runId4))),lineageDataset.getRelations(datasetInstance2,0,100000));
  Assert.assertEquals(ImmutableSet.of(new Relation(stream1,program2,AccessType.READ,ImmutableSet.of(runId2))),lineageDataset.getRelations(stream1,0,100000));
  Assert.assertEquals(ImmutableSet.of(new Relation(stream2,program2,AccessType.READ,ImmutableSet.of(runId3)),new Relation(stream2,program3,AccessType.UNKNOWN,ImmutableSet.of(runId4))),lineageDataset.getRelations(stream2,0,100000));
  Assert.assertEquals(ImmutableSet.of(new Relation(datasetInstance2,program2,AccessType.WRITE,ImmutableSet.of(runId2)),new Relation(stream1,program2,AccessType.READ,ImmutableSet.of(runId2)),new Relation(datasetInstance2,program2,AccessType.WRITE,ImmutableSet.of(runId3)),new Relation(stream2,program2,AccessType.READ,ImmutableSet.of(runId3))),lineageDataset.getRelations(program2,0,100000));
  Assert.assertEquals(ImmutableSet.of(new Relation(datasetInstance2,program2,AccessType.WRITE,ImmutableSet.of(runId2)),new Relation(datasetInstance2,program2,AccessType.WRITE,ImmutableSet.of(runId3))),lineageDataset.getRelations(datasetInstance2,0,35000));
  Assert.assertEquals(metaProgram1Data1Run1,lineageDataset.getRunMetadata(run11));
  Assert.assertEquals(2,lineageDataset.getRunMetadata(run11).size());
  Assert.assertEquals(toSet(metaProgram2Data2Run2,metaProgram2Stream1Run2),lineageDataset.getRunMetadata(run22));
  Assert.assertEquals(3,lineageDataset.getRunMetadata(run22).size());
  Assert.assertEquals(toSet(metaProgram2Stream2Run3,metaProgram2Data2Run3),lineageDataset.getRunMetadata(run23));
  Assert.assertEquals(3,lineageDataset.getRunMetadata(run23).size());
  Assert.assertEquals(metaProgram3Data2Run4,lineageDataset.getRunMetadata(run34));
  Assert.assertEquals(2,lineageDataset.getRunMetadata(run34).size());
}"
6693,"@Test public void testSimpleLineage() throws Exception {
  LineageStore lineageStore=new LineageStore(getTxExecFactory(),dsFrameworkUtil.getFramework(),Id.DatasetInstance.from(""String_Node_Str"",""String_Node_Str""));
  LineageService lineageService=new LineageService(lineageStore);
  MetadataRecord run1Meta=new MetadataRecord(program1,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str""));
  Set<MetadataRecord> run1data1=toSet(run1Meta,new MetadataRecord(dataset1,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  Set<MetadataRecord> run1data2=toSet(run1Meta,new MetadataRecord(dataset2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  lineageStore.addAccess(run1,dataset1,AccessType.WRITE,run1data1,flowlet1);
  lineageStore.addAccess(run1,dataset2,AccessType.READ,run1data2,flowlet1);
  lineageStore.addAccess(run2,dataset2,AccessType.WRITE,EMPTY_METADATA,flowlet2);
  lineageStore.addAccess(run2,dataset3,AccessType.READ,EMPTY_METADATA,flowlet2);
  Lineage expectedLineage=new Lineage(ImmutableSet.of(new Relation(dataset1,program1,AccessType.WRITE,toSet(twillRunId(run1)),toSet(flowlet1)),new Relation(dataset2,program1,AccessType.READ,toSet(twillRunId(run1)),toSet(flowlet1)),new Relation(dataset2,program2,AccessType.WRITE,toSet(twillRunId(run2)),toSet(flowlet2)),new Relation(dataset3,program2,AccessType.READ,toSet(twillRunId(run2)),toSet(flowlet2))));
  Assert.assertEquals(expectedLineage,lineageService.computeLineage(dataset1,500,20000,100));
  Assert.assertEquals(expectedLineage,lineageService.computeLineage(dataset2,500,20000,100));
  Lineage oneLevelLineage=lineageService.computeLineage(dataset1,500,20000,1);
  Assert.assertEquals(ImmutableSet.of(new Relation(dataset1,program1,AccessType.WRITE,toSet(twillRunId(run1)),toSet(flowlet1)),new Relation(dataset2,program1,AccessType.READ,toSet(twillRunId(run1)),toSet(flowlet1))),oneLevelLineage.getRelations());
  Assert.assertEquals(toSet(run1data1,run1data2),lineageStore.getAccesses(run1));
}","@Test public void testSimpleLineage() throws Exception {
  LineageStore lineageStore=new LineageStore(getTxExecFactory(),dsFrameworkUtil.getFramework(),Id.DatasetInstance.from(""String_Node_Str"",""String_Node_Str""));
  LineageService lineageService=new LineageService(lineageStore);
  MetadataRecord run1Meta=new MetadataRecord(program1,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str""));
  Set<MetadataRecord> run1data1=toSet(run1Meta,new MetadataRecord(dataset1,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  Set<MetadataRecord> run1data2=toSet(run1Meta,new MetadataRecord(dataset2,toMap(""String_Node_Str"",""String_Node_Str""),toSet(""String_Node_Str"")));
  lineageStore.addAccess(run1,dataset1,AccessType.WRITE,run1data1,flowlet1);
  lineageStore.addAccess(run1,dataset2,AccessType.READ,run1data2,flowlet1);
  lineageStore.addAccess(run2,dataset2,AccessType.WRITE,EMPTY_METADATA,flowlet2);
  lineageStore.addAccess(run2,dataset3,AccessType.READ,EMPTY_METADATA,flowlet2);
  Lineage expectedLineage=new Lineage(ImmutableSet.of(new Relation(dataset1,program1,AccessType.WRITE,toSet(twillRunId(run1)),toSet(flowlet1)),new Relation(dataset2,program1,AccessType.READ,toSet(twillRunId(run1)),toSet(flowlet1)),new Relation(dataset2,program2,AccessType.WRITE,toSet(twillRunId(run2)),toSet(flowlet2)),new Relation(dataset3,program2,AccessType.READ,toSet(twillRunId(run2)),toSet(flowlet2))));
  Assert.assertEquals(expectedLineage,lineageService.computeLineage(dataset1,500,20000,100));
  Assert.assertEquals(expectedLineage,lineageService.computeLineage(dataset2,500,20000,100));
  Lineage oneLevelLineage=lineageService.computeLineage(dataset1,500,20000,1);
  Assert.assertEquals(ImmutableSet.of(new Relation(dataset1,program1,AccessType.WRITE,toSet(twillRunId(run1)),toSet(flowlet1)),new Relation(dataset2,program1,AccessType.READ,toSet(twillRunId(run1)),toSet(flowlet1))),oneLevelLineage.getRelations());
  Assert.assertEquals(toSet(run1data1,run1data2),lineageStore.getRunMetadata(run1));
}"
6694,"private String getV3RoutingService(String[] uriParts,AllowedMethod requestMethod){
  if ((uriParts.length >= 2) && uriParts[1].equals(""String_Node_Str"")) {
    return null;
  }
 else   if ((uriParts.length >= 9) && ""String_Node_Str"".equals(uriParts[5]) && ""String_Node_Str"".equals(uriParts[7])) {
    return String.format(""String_Node_Str"",uriParts[2],uriParts[4],uriParts[6]);
  }
 else   if (matches(uriParts,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if (matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")) {
    return Constants.Service.METADATA_SERVICE;
  }
 else   if ((matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) && requestMethod.equals(AllowedMethod.GET)) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if ((uriParts.length >= 4) && uriParts[1].equals(""String_Node_Str"") && uriParts[3].equals(""String_Node_Str"")) {
    if (uriParts.length == 4) {
      return Constants.Service.APP_FABRIC_HTTP;
    }
 else {
      return Constants.Service.STREAMS;
    }
  }
 else   if ((uriParts.length >= 8 && uriParts[7].equals(""String_Node_Str"")) || (uriParts.length >= 10 && uriParts[9].equals(""String_Node_Str"")) || (uriParts.length >= 6 && uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 2 && uriParts[1].equals(""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 5 && uriParts[1].equals(""String_Node_Str"") && uriParts[2].equals(""String_Node_Str"") && (uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length >= 6 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if ((uriParts.length == 3) && uriParts[1].equals(""String_Node_Str"") && uriParts[2].equals(""String_Node_Str"")) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length == 7 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str""))) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if ((uriParts.length >= 4) && uriParts[3].equals(""String_Node_Str"")) {
    return Constants.Service.DATASET_MANAGER;
  }
  return Constants.Service.APP_FABRIC_HTTP;
}","private String getV3RoutingService(String[] uriParts,AllowedMethod requestMethod){
  if ((uriParts.length >= 2) && uriParts[1].equals(""String_Node_Str"")) {
    return null;
  }
 else   if ((uriParts.length >= 9) && ""String_Node_Str"".equals(uriParts[5]) && ""String_Node_Str"".equals(uriParts[7])) {
    return String.format(""String_Node_Str"",uriParts[2],uriParts[4],uriParts[6]);
  }
 else   if (matches(uriParts,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if (matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,null,null,""String_Node_Str"",null,""String_Node_Str"")) {
    return Constants.Service.METADATA_SERVICE;
  }
 else   if ((matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",null,""String_Node_Str"") || matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")|| matches(uriParts,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"",""String_Node_Str"",null,""String_Node_Str"")) && requestMethod.equals(AllowedMethod.GET)) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if ((uriParts.length >= 4) && uriParts[1].equals(""String_Node_Str"") && uriParts[3].equals(""String_Node_Str"")) {
    if (uriParts.length == 4) {
      return Constants.Service.APP_FABRIC_HTTP;
    }
 else {
      return Constants.Service.STREAMS;
    }
  }
 else   if ((uriParts.length >= 8 && uriParts[7].equals(""String_Node_Str"")) || (uriParts.length >= 10 && uriParts[9].equals(""String_Node_Str"")) || (uriParts.length >= 6 && uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 2 && uriParts[1].equals(""String_Node_Str"")) {
    return Constants.Service.METRICS;
  }
 else   if (uriParts.length >= 5 && uriParts[1].equals(""String_Node_Str"") && uriParts[2].equals(""String_Node_Str"") && (uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str"") || uriParts[3].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length >= 6 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"") || uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str"")|| uriParts[5].equals(""String_Node_Str""))) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if ((uriParts.length == 3) && uriParts[1].equals(""String_Node_Str"") && uriParts[2].equals(""String_Node_Str"")) {
    return Constants.Service.EXPLORE_HTTP_USER_SERVICE;
  }
 else   if (uriParts.length == 7 && uriParts[3].equals(""String_Node_Str"") && uriParts[4].equals(""String_Node_Str"") && (uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str"") || uriParts[6].equals(""String_Node_Str""))) {
    return Constants.Service.APP_FABRIC_HTTP;
  }
 else   if ((uriParts.length >= 4) && uriParts[3].equals(""String_Node_Str"")) {
    return Constants.Service.DATASET_MANAGER;
  }
  return Constants.Service.APP_FABRIC_HTTP;
}"
6695,"@BeforeClass public static void init() throws IOException {
  Configuration hConf=new Configuration();
  hConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR,tmpFolder.newFolder().getAbsolutePath());
  dfsCluster=new MiniDFSCluster.Builder(hConf).numDataNodes(1).build();
  FileSystem fileSystem=dfsCluster.getFileSystem();
  final HDFSLocationFactory lf=new HDFSLocationFactory(fileSystem);
  final NamespacedLocationFactory nlf=new DefaultNamespacedLocationFactory(cConf,lf);
  Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(LocationFactory.class).toInstance(lf);
      bind(NamespacedLocationFactory.class).toInstance(nlf);
    }
  }
,new TransactionMetricsModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(false),Modules.override(new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamCoordinatorClient.class).to(InMemoryStreamCoordinatorClient.class).in(Scopes.SINGLETON);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
    }
  }
),new AbstractModule(){
    @Override protected void configure(){
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
    }
  }
);
  locationFactory=injector.getInstance(LocationFactory.class);
  namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  streamAdmin=injector.getInstance(StreamAdmin.class);
  fileWriterFactory=injector.getInstance(StreamFileWriterFactory.class);
  streamCoordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  streamCoordinatorClient.startAndWait();
}","@BeforeClass public static void init() throws IOException {
  Configuration hConf=new Configuration();
  hConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR,tmpFolder.newFolder().getAbsolutePath());
  dfsCluster=new MiniDFSCluster.Builder(hConf).numDataNodes(1).build();
  FileSystem fileSystem=dfsCluster.getFileSystem();
  final HDFSLocationFactory lf=new HDFSLocationFactory(fileSystem);
  final NamespacedLocationFactory nlf=new DefaultNamespacedLocationFactory(cConf,lf);
  Injector injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(LocationFactory.class).toInstance(lf);
      bind(NamespacedLocationFactory.class).toInstance(nlf);
    }
  }
,new TransactionMetricsModule(),new DiscoveryRuntimeModule().getInMemoryModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),Modules.override(new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamCoordinatorClient.class).to(InMemoryStreamCoordinatorClient.class).in(Scopes.SINGLETON);
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
    }
  }
),new AbstractModule(){
    @Override protected void configure(){
      bind(NotificationFeedManager.class).to(NoOpNotificationFeedManager.class);
    }
  }
);
  locationFactory=injector.getInstance(LocationFactory.class);
  namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  streamAdmin=injector.getInstance(StreamAdmin.class);
  fileWriterFactory=injector.getInstance(StreamFileWriterFactory.class);
  streamCoordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  streamCoordinatorClient.startAndWait();
}"
6696,"@BeforeClass public static void init() throws IOException {
  zkServer=InMemoryZKServer.builder().setDataDir(tmpFolder.newFolder()).build();
  zkServer.startAndWait();
  Configuration hConf=new Configuration();
  hConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR,tmpFolder.newFolder().getAbsolutePath());
  dfsCluster=new MiniDFSCluster.Builder(hConf).numDataNodes(1).build();
  FileSystem fileSystem=dfsCluster.getFileSystem();
  final HDFSLocationFactory lf=new HDFSLocationFactory(fileSystem);
  final NamespacedLocationFactory nlf=new DefaultNamespacedLocationFactory(cConf,lf);
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(false),new TransactionMetricsModule(),new NotificationFeedServiceRuntimeModule().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(LocationFactory.class).toInstance(lf);
      bind(NamespacedLocationFactory.class).toInstance(nlf);
    }
  }
,Modules.override(new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
    }
  }
));
  zkClient=injector.getInstance(ZKClientService.class);
  zkClient.startAndWait();
  setupNamespaces(injector.getInstance(NamespacedLocationFactory.class));
  streamAdmin=injector.getInstance(StreamAdmin.class);
  coordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  coordinatorClient.startAndWait();
}","@BeforeClass public static void init() throws IOException {
  zkServer=InMemoryZKServer.builder().setDataDir(tmpFolder.newFolder()).build();
  zkServer.startAndWait();
  Configuration hConf=new Configuration();
  hConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR,tmpFolder.newFolder().getAbsolutePath());
  dfsCluster=new MiniDFSCluster.Builder(hConf).numDataNodes(1).build();
  FileSystem fileSystem=dfsCluster.getFileSystem();
  final HDFSLocationFactory lf=new HDFSLocationFactory(fileSystem);
  final NamespacedLocationFactory nlf=new DefaultNamespacedLocationFactory(cConf,lf);
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  Injector injector=Guice.createInjector(new ConfigModule(cConf),new ZKClientModule(),new DiscoveryRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new TransactionMetricsModule(),new NotificationFeedServiceRuntimeModule().getInMemoryModules(),new AbstractModule(){
    @Override protected void configure(){
      bind(LocationFactory.class).toInstance(lf);
      bind(NamespacedLocationFactory.class).toInstance(nlf);
    }
  }
,Modules.override(new StreamAdminModules().getDistributedModules()).with(new AbstractModule(){
    @Override protected void configure(){
      bind(StreamMetaStore.class).to(InMemoryStreamMetaStore.class);
    }
  }
));
  zkClient=injector.getInstance(ZKClientService.class);
  zkClient.startAndWait();
  setupNamespaces(injector.getInstance(NamespacedLocationFactory.class));
  streamAdmin=injector.getInstance(StreamAdmin.class);
  coordinatorClient=injector.getInstance(StreamCoordinatorClient.class);
  coordinatorClient.startAndWait();
}"
6697,"@BeforeClass public static void beforeClass() throws Exception {
  HBaseTestingUtility hBaseTestingUtility=new HBaseTestingUtility();
  hBaseTestingUtility.startMiniDFSCluster(1);
  Configuration hConf=hBaseTestingUtility.getConfiguration();
  hConf.setBoolean(""String_Node_Str"",true);
  zkServer=InMemoryZKServer.builder().build();
  zkServer.startAndWait();
  cConf=CConfiguration.create();
  cConf.set(Constants.CFG_HDFS_USER,System.getProperty(""String_Node_Str""));
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  cConf.setBoolean(TxConstants.Manager.CFG_DO_PERSIST,true);
  server=TransactionServiceTest.createTxService(zkServer.getConnectionStr(),Networks.getRandomPort(),hConf,tmpFolder.newFolder());
  server.startAndWait();
  injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getDistributedModules(),new TransactionMetricsModule(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(false));
  zkClient=injector.getInstance(ZKClientService.class);
  zkClient.startAndWait();
  txStateStorage=injector.getInstance(TransactionStateStorage.class);
  txStateStorage.startAndWait();
}","@BeforeClass public static void beforeClass() throws Exception {
  HBaseTestingUtility hBaseTestingUtility=new HBaseTestingUtility();
  hBaseTestingUtility.startMiniDFSCluster(1);
  Configuration hConf=hBaseTestingUtility.getConfiguration();
  hConf.setBoolean(""String_Node_Str"",true);
  zkServer=InMemoryZKServer.builder().build();
  zkServer.startAndWait();
  cConf=CConfiguration.create();
  cConf.set(Constants.CFG_HDFS_USER,System.getProperty(""String_Node_Str""));
  cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
  cConf.setBoolean(TxConstants.Manager.CFG_DO_PERSIST,true);
  server=TransactionServiceTest.createTxService(zkServer.getConnectionStr(),Networks.getRandomPort(),hConf,tmpFolder.newFolder());
  server.startAndWait();
  injector=Guice.createInjector(new ConfigModule(cConf,hConf),new ZKClientModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getDistributedModules(),new TransactionMetricsModule(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules());
  zkClient=injector.getInstance(ZKClientService.class);
  zkClient.startAndWait();
  txStateStorage=injector.getInstance(TransactionStateStorage.class);
  txStateStorage.startAndWait();
}"
6698,"@Test(timeout=60000) public void testHA() throws Exception {
  HBaseTestingUtility hBaseTestingUtility=new HBaseTestingUtility();
  hBaseTestingUtility.startMiniDFSCluster(1);
  Configuration hConf=hBaseTestingUtility.getConfiguration();
  hConf.setBoolean(""String_Node_Str"",true);
  InMemoryZKServer zkServer=InMemoryZKServer.builder().build();
  zkServer.startAndWait();
  try {
    CConfiguration cConf=CConfiguration.create();
    cConf.set(Constants.CFG_HDFS_USER,System.getProperty(""String_Node_Str""));
    cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
    cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
    Injector injector=Guice.createInjector(new ConfigModule(cConf),new ZKClientModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getDistributedModules(),new TransactionMetricsModule(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(false));
    ZKClientService zkClient=injector.getInstance(ZKClientService.class);
    zkClient.startAndWait();
    final Table table=createTable(""String_Node_Str"");
    try {
      TransactionSystemClient txClient=injector.getInstance(TransactionSystemClient.class);
      TransactionExecutor txExecutor=new DefaultTransactionExecutor(txClient,ImmutableList.of((TransactionAware)table));
      TransactionService first=createTxService(zkServer.getConnectionStr(),Networks.getRandomPort(),hConf,tmpFolder.newFolder());
      first.startAndWait();
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,null,""String_Node_Str"");
      TransactionService second=createTxService(zkServer.getConnectionStr(),Networks.getRandomPort(),hConf,tmpFolder.newFolder());
      second.startAndWait();
      TimeUnit.SECONDS.sleep(1);
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,""String_Node_Str"",""String_Node_Str"");
      first.stopAndWait();
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,""String_Node_Str"",""String_Node_Str"");
      TransactionService third=createTxService(zkServer.getConnectionStr(),Networks.getRandomPort(),hConf,tmpFolder.newFolder());
      third.start();
      second.stopAndWait();
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,""String_Node_Str"",""String_Node_Str"");
      third.stop();
    }
  finally {
      dropTable(""String_Node_Str"",cConf);
      zkClient.stopAndWait();
    }
  }
  finally {
    zkServer.stop();
  }
}","@Test(timeout=60000) public void testHA() throws Exception {
  HBaseTestingUtility hBaseTestingUtility=new HBaseTestingUtility();
  hBaseTestingUtility.startMiniDFSCluster(1);
  Configuration hConf=hBaseTestingUtility.getConfiguration();
  hConf.setBoolean(""String_Node_Str"",true);
  InMemoryZKServer zkServer=InMemoryZKServer.builder().build();
  zkServer.startAndWait();
  try {
    CConfiguration cConf=CConfiguration.create();
    cConf.set(Constants.CFG_HDFS_USER,System.getProperty(""String_Node_Str""));
    cConf.set(Constants.Zookeeper.QUORUM,zkServer.getConnectionStr());
    cConf.set(Constants.CFG_LOCAL_DATA_DIR,tmpFolder.newFolder().getAbsolutePath());
    Injector injector=Guice.createInjector(new ConfigModule(cConf),new ZKClientModule(),new LocationRuntimeModule().getInMemoryModules(),new DiscoveryRuntimeModule().getDistributedModules(),new TransactionMetricsModule(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules());
    ZKClientService zkClient=injector.getInstance(ZKClientService.class);
    zkClient.startAndWait();
    final Table table=createTable(""String_Node_Str"");
    try {
      TransactionSystemClient txClient=injector.getInstance(TransactionSystemClient.class);
      TransactionExecutor txExecutor=new DefaultTransactionExecutor(txClient,ImmutableList.of((TransactionAware)table));
      TransactionService first=createTxService(zkServer.getConnectionStr(),Networks.getRandomPort(),hConf,tmpFolder.newFolder());
      first.startAndWait();
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,null,""String_Node_Str"");
      TransactionService second=createTxService(zkServer.getConnectionStr(),Networks.getRandomPort(),hConf,tmpFolder.newFolder());
      second.startAndWait();
      TimeUnit.SECONDS.sleep(1);
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,""String_Node_Str"",""String_Node_Str"");
      first.stopAndWait();
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,""String_Node_Str"",""String_Node_Str"");
      TransactionService third=createTxService(zkServer.getConnectionStr(),Networks.getRandomPort(),hConf,tmpFolder.newFolder());
      third.start();
      second.stopAndWait();
      Assert.assertNotNull(txClient.startShort());
      verifyGetAndPut(table,txExecutor,""String_Node_Str"",""String_Node_Str"");
      third.stop();
    }
  finally {
      dropTable(""String_Node_Str"",cConf);
      zkClient.stopAndWait();
    }
  }
  finally {
    zkServer.stop();
  }
}"
6699,"@Override protected void doInit(TwillContext context){
  this.user=DEFAULT_USER;
  CConfiguration cConf=getCConfiguration();
  Configuration hConf=getConfiguration();
  cConf.set(Constants.Dataset.Executor.ADDRESS,context.getHost().getHostName());
  injector=Guice.createInjector(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MetricsClientRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new LocationRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(),new DataSetServiceModules().getDistributedModules(),new LoggingModules().getDistributedModules(),new ExploreClientModule(),new NamespaceClientRuntimeModule().getDistributedModules(),new MetadataServiceModule());
  injector.getInstance(LogAppenderInitializer.class).initialize();
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(Id.Namespace.SYSTEM.getId(),Constants.Logging.COMPONENT_NAME,Constants.Service.DATASET_EXECUTOR));
}","@Override protected void doInit(TwillContext context){
  this.user=DEFAULT_USER;
  CConfiguration cConf=getCConfiguration();
  Configuration hConf=getConfiguration();
  cConf.set(Constants.Dataset.Executor.ADDRESS,context.getHost().getHostName());
  injector=Guice.createInjector(new ConfigModule(cConf,hConf),new IOModule(),new ZKClientModule(),new KafkaClientModule(),new MetricsClientRuntimeModule().getDistributedModules(),new DiscoveryRuntimeModule().getDistributedModules(),new LocationRuntimeModule().getDistributedModules(),new DataFabricModules().getDistributedModules(),new DataSetsModules().getDistributedModules(true),new DataSetServiceModules().getDistributedModules(),new LoggingModules().getDistributedModules(),new ExploreClientModule(),new NamespaceClientRuntimeModule().getDistributedModules(),new MetadataServiceModule());
  injector.getInstance(LogAppenderInitializer.class).initialize();
  LoggingContextAccessor.setLoggingContext(new ServiceLoggingContext(Id.Namespace.SYSTEM.getId(),Constants.Logging.COMPONENT_NAME,Constants.Service.DATASET_EXECUTOR));
}"
6700,"@Override public void reportResources(){
  for (  TwillRunner.LiveInfo info : twillRunner.lookupLive()) {
    Map<String,String> metricContext=getMetricContext(info);
    if (metricContext == null) {
      continue;
    }
    for (    TwillController controller : info.getControllers()) {
      ResourceReport report=controller.getResourceReport();
      if (report == null) {
        continue;
      }
      int memory=report.getAppMasterResources().getMemoryMB();
      int vcores=report.getAppMasterResources().getVirtualCores();
      Map<String,String> runContext=ImmutableMap.<String,String>builder().putAll(metricContext).put(Constants.Metrics.Tag.RUN_ID,controller.getRunId().getId()).build();
      sendMetrics(runContext,1,memory,vcores);
    }
  }
  reportClusterStorage();
  boolean reported=false;
  for (  URL url : rmUrls) {
    if (reportClusterMemory(url)) {
      reported=true;
      break;
    }
  }
  if (!reported) {
    LOG.warn(""String_Node_Str"");
  }
}","@Override public void reportResources(){
  for (  TwillRunner.LiveInfo info : twillRunner.lookupLive()) {
    Map<String,String> metricContext=getMetricContext(info);
    if (metricContext == null) {
      continue;
    }
    for (    TwillController controller : info.getControllers()) {
      ResourceReport report=controller.getResourceReport();
      if (report == null) {
        continue;
      }
      int memory=report.getAppMasterResources().getMemoryMB();
      int vcores=report.getAppMasterResources().getVirtualCores();
      Map<String,String> runContext=ImmutableMap.<String,String>builder().putAll(metricContext).put(Constants.Metrics.Tag.RUN_ID,controller.getRunId().getId()).build();
      sendMetrics(runContext,1,memory,vcores);
    }
  }
  boolean reported=false;
  for (  URL url : rmUrls) {
    if (reportClusterMemory(url)) {
      reported=true;
      break;
    }
  }
  if (!reported) {
    LOG.warn(""String_Node_Str"");
  }
}"
6701,"private boolean reportClusterMemory(URL url){
  Reader reader=null;
  HttpURLConnection conn=null;
  LOG.trace(""String_Node_Str"",url);
  try {
    conn=(HttpURLConnection)url.openConnection();
    conn.setRequestMethod(""String_Node_Str"");
    reader=new InputStreamReader(conn.getInputStream(),Charsets.UTF_8);
    JsonObject response;
    try {
      response=new Gson().fromJson(reader,JsonObject.class);
    }
 catch (    JsonParseException e) {
      return false;
    }
    if (response != null) {
      JsonObject clusterMetrics=response.getAsJsonObject(""String_Node_Str"");
      long totalMemory=clusterMetrics.get(""String_Node_Str"").getAsLong();
      long availableMemory=clusterMetrics.get(""String_Node_Str"").getAsLong();
      MetricsContext collector=getCollector();
      LOG.trace(""String_Node_Str"" + totalMemory + ""String_Node_Str""+ availableMemory);
      collector.gauge(""String_Node_Str"",totalMemory);
      collector.gauge(""String_Node_Str"",availableMemory);
      return true;
    }
    return false;
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e);
    return false;
  }
 finally {
    if (reader != null) {
      try {
        reader.close();
      }
 catch (      IOException e) {
        LOG.error(""String_Node_Str"",e);
      }
    }
    if (conn != null) {
      conn.disconnect();
    }
  }
}","private boolean reportClusterMemory(URL url){
  Reader reader=null;
  HttpURLConnection conn=null;
  LOG.trace(""String_Node_Str"",url);
  try {
    conn=(HttpURLConnection)url.openConnection();
    conn.setRequestMethod(""String_Node_Str"");
    reader=new InputStreamReader(conn.getInputStream(),Charsets.UTF_8);
    JsonObject response;
    try {
      response=new Gson().fromJson(reader,JsonObject.class);
    }
 catch (    JsonParseException e) {
      return false;
    }
    if (response != null) {
      JsonObject clusterMetrics=response.getAsJsonObject(""String_Node_Str"");
      long totalMemory=clusterMetrics.get(""String_Node_Str"").getAsLong();
      long availableMemory=clusterMetrics.get(""String_Node_Str"").getAsLong();
      MetricsContext collector=getCollector();
      LOG.trace(""String_Node_Str"" + totalMemory + ""String_Node_Str""+ availableMemory);
      collector.gauge(""String_Node_Str"",totalMemory);
      collector.gauge(""String_Node_Str"",availableMemory);
      return true;
    }
    return false;
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
    return false;
  }
 finally {
    if (reader != null) {
      try {
        reader.close();
      }
 catch (      IOException e) {
        LOG.warn(""String_Node_Str"",e);
      }
    }
    if (conn != null) {
      conn.disconnect();
    }
  }
}"
6702,"@DELETE @Path(""String_Node_Str"") public void deleteArtifact(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion) throws NamespaceNotFoundException, BadRequestException {
  Id.Namespace namespace=validateAndGetNamespace(namespaceId);
  Id.Artifact artifactId=validateAndGetArtifactId(namespace,artifactName,artifactVersion);
  try {
    artifactRepository.deleteArtifact(artifactId);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactName,namespaceId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + e.getMessage());
  }
}","@DELETE @Path(""String_Node_Str"") public void deleteArtifact(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String artifactName,@PathParam(""String_Node_Str"") String artifactVersion) throws NamespaceNotFoundException, BadRequestException {
  Id.Namespace namespace=Id.Namespace.SYSTEM.getId().equalsIgnoreCase(namespaceId) ? Id.Namespace.SYSTEM : validateAndGetNamespace(namespaceId);
  Id.Artifact artifactId=validateAndGetArtifactId(namespace,artifactName,artifactVersion);
  try {
    artifactRepository.deleteArtifact(artifactId);
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",artifactName,namespaceId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + e.getMessage());
  }
}"
6703,"/** 
 * Write key and value to the hadoop context.
 * @param key         the key
 * @param value       the value
 */
void write(KEYOUT key,VALUEOUT value) throws IOException, InterruptedException ;","/** 
 * Write key and value to the hadoop context. This method must only be used in the MapReduce writes to a single output. If there is more than one outputs,   {@link #write(String,Object,Object)} must be used.
 * @param key         the key
 * @param value       the value
 */
void write(KEYOUT key,VALUEOUT value) throws IOException, InterruptedException ;"
6704,"private WrappedSink(String sinkPluginId,BatchSink<IN,KEY_OUT,VAL_OUT> sink,Set<String> outputNames,MapReduceTaskContext context,Metrics metrics){
  this.sink=sink;
  this.emitter=new DefaultEmitter<>(new StageMetrics(metrics,PluginID.from(sinkPluginId)));
  this.outputNames=outputNames;
  this.context=context;
}","protected WrappedSink(String sinkPluginId,BatchSink<IN,KEY_OUT,VAL_OUT> sink,MapReduceTaskContext<KEY_OUT,VAL_OUT> context,Metrics metrics){
  this.sink=sink;
  this.emitter=new DefaultEmitter<>(new StageMetrics(metrics,PluginID.from(sinkPluginId)));
  this.context=context;
}"
6705,"@Override public void initialize(MapReduceTaskContext context) throws Exception {
  context.getSpecification().getProperties();
  Map<String,String> properties=context.getSpecification().getProperties();
  String sourcePluginId=properties.get(Constants.Source.PLUGINID);
  String transformInfosStr=properties.get(Constants.Transform.PLUGINIDS);
  Preconditions.checkNotNull(transformInfosStr,""String_Node_Str"");
  List<TransformInfo> transformInfos=GSON.fromJson(transformInfosStr,TRANSFORMDETAILS_LIST_TYPE);
  List<TransformDetail> pipeline=Lists.newArrayListWithCapacity(transformInfos.size() + 2);
  BatchSource source=context.newInstance(sourcePluginId);
  BatchRuntimeContext runtimeContext=new MapReduceRuntimeContext(context,mapperMetrics,sourcePluginId);
  source.initialize(runtimeContext);
  pipeline.add(new TransformDetail(sourcePluginId,source,new StageMetrics(mapperMetrics,PluginID.from(sourcePluginId))));
  addTransforms(pipeline,transformInfos,context);
  Context hadoopContext=(Context)context.getHadoopContext();
  String sinkOutputsStr=hadoopContext.getConfiguration().get(SINK_OUTPUTS_KEY);
  Preconditions.checkNotNull(sinkOutputsStr,""String_Node_Str"");
  Map<String,Set<String>> sinkOutputs=GSON.fromJson(sinkOutputsStr,SINK_OUTPUTS_TYPE);
  sinks=new ArrayList<>(sinkOutputs.size());
  for (  Map.Entry<String,Set<String>> sinkOutput : sinkOutputs.entrySet()) {
    String sinkPluginId=sinkOutput.getKey();
    Set<String> sinkOutputNames=sinkOutput.getValue();
    BatchSink<Object,Object,Object> sink=context.newInstance(sinkPluginId);
    runtimeContext=new MapReduceRuntimeContext(context,mapperMetrics,sinkPluginId);
    sink.initialize(runtimeContext);
    sinks.add(new WrappedSink<>(sinkPluginId,sink,sinkOutputNames,context,mapperMetrics));
  }
  transformExecutor=new TransformExecutor<>(pipeline);
}","@Override public void initialize(MapReduceTaskContext<Object,Object> context) throws Exception {
  context.getSpecification().getProperties();
  Map<String,String> properties=context.getSpecification().getProperties();
  String sourcePluginId=properties.get(Constants.Source.PLUGINID);
  String transformInfosStr=properties.get(Constants.Transform.PLUGINIDS);
  Preconditions.checkNotNull(transformInfosStr,""String_Node_Str"");
  List<TransformInfo> transformInfos=GSON.fromJson(transformInfosStr,TRANSFORMDETAILS_LIST_TYPE);
  List<TransformDetail> pipeline=Lists.newArrayListWithCapacity(transformInfos.size() + 2);
  BatchSource source=context.newInstance(sourcePluginId);
  BatchRuntimeContext runtimeContext=new MapReduceRuntimeContext(context,mapperMetrics,sourcePluginId);
  source.initialize(runtimeContext);
  pipeline.add(new TransformDetail(sourcePluginId,source,new StageMetrics(mapperMetrics,PluginID.from(sourcePluginId))));
  addTransforms(pipeline,transformInfos,context);
  Context hadoopContext=context.getHadoopContext();
  String sinkOutputsStr=hadoopContext.getConfiguration().get(SINK_OUTPUTS_KEY);
  Preconditions.checkNotNull(sinkOutputsStr,""String_Node_Str"");
  Map<String,Set<String>> sinkOutputs=GSON.fromJson(sinkOutputsStr,SINK_OUTPUTS_TYPE);
  boolean hasOneOutput=hasOneOutput(transformInfos,sinkOutputs);
  sinks=new ArrayList<>(sinkOutputs.size());
  for (  Map.Entry<String,Set<String>> sinkOutput : sinkOutputs.entrySet()) {
    String sinkPluginId=sinkOutput.getKey();
    Set<String> sinkOutputNames=sinkOutput.getValue();
    BatchSink<Object,Object,Object> sink=context.newInstance(sinkPluginId);
    runtimeContext=new MapReduceRuntimeContext(context,mapperMetrics,sinkPluginId);
    sink.initialize(runtimeContext);
    if (hasOneOutput) {
      sinks.add(new SingleOutputSink<>(sinkPluginId,sink,context,mapperMetrics));
    }
 else {
      sinks.add(new MultiOutputSink<>(sinkPluginId,sink,context,mapperMetrics,sinkOutputNames));
    }
  }
  transformExecutor=new TransformExecutor<>(pipeline);
}"
6706,"private void write(IN input) throws Exception {
  sink.transform(input,emitter);
  for (  KeyValue outputRecord : emitter) {
    for (    String outputName : outputNames) {
      context.write(outputName,outputRecord.getKey(),outputRecord.getValue());
    }
  }
  emitter.reset();
}","public void write(IN input) throws Exception {
  sink.transform(input,emitter);
  for (  KeyValue outputRecord : emitter) {
    for (    String outputName : outputNames) {
      context.write(outputName,outputRecord.getKey(),outputRecord.getValue());
    }
  }
  emitter.reset();
}"
6707,"@Override public boolean apply(@Nullable RunRecordMeta input){
  if (input == null) {
    return false;
  }
  String runId=input.getPid();
  Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
  return (targetProgramId != null) && !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
}","@Override public boolean apply(RunRecordMeta input){
  String runId=input.getPid();
  Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
  if (targetProgramId != null) {
    runIdToProgramId.put(runId,targetProgramId);
    return true;
  }
 else {
    return false;
  }
}"
6708,"@Override public void run(){
  try {
    RunRecordsCorrectorRunnable.LOG.debug(""String_Node_Str"");
    programLifecycleService.validateAndCorrectRunningRunRecords();
    RunRecordsCorrectorRunnable.LOG.debug(""String_Node_Str"");
  }
 catch (  Throwable t) {
    LOG.debug(""String_Node_Str"",t);
  }
}","@Override public void run(){
  try {
    RunRecordsCorrectorRunnable.LOG.debug(""String_Node_Str"");
    programLifecycleService.validateAndCorrectRunningRunRecords();
    RunRecordsCorrectorRunnable.LOG.debug(""String_Node_Str"");
  }
 catch (  Throwable t) {
    LOG.warn(""String_Node_Str"",Throwables.getRootCause(t).getMessage());
    LOG.debug(""String_Node_Str"",t);
  }
}"
6709,"/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @param programType The type of program the run records need to validate and update.
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 */
void validateAndCorrectRunningRunRecords(final ProgramType programType,Set<String> processedInvalidRunRecordIds){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecordMeta> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecordMeta>(){
    @Override public boolean apply(    @Nullable RunRecordMeta input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
      return (targetProgramId != null) && !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"",invalidRunRecords.size(),programType.getPrettyName());
  }
  for (  RunRecordMeta invalidRunRecordMeta : invalidRunRecords) {
    String runId=invalidRunRecordMeta.getPid();
    Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
    boolean shouldCorrect=shouldCorrectForWorkflowChildren(invalidRunRecordMeta,processedInvalidRunRecordIds);
    if (!shouldCorrect) {
      continue;
    }
    LOG.warn(""String_Node_Str"",runId,targetProgramId,programType.getPrettyName());
    store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
    processedInvalidRunRecordIds.add(runId);
  }
}","/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @param programType The type of program the run records need to validate and update.
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 */
void validateAndCorrectRunningRunRecords(final ProgramType programType,Set<String> processedInvalidRunRecordIds){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  LOG.trace(""String_Node_Str"");
  List<RunRecordMeta> notActuallyRunning=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta input){
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  LOG.trace(""String_Node_Str"",notActuallyRunning.size());
  final Map<String,Id.Program> runIdToProgramId=new HashMap<>();
  LOG.trace(""String_Node_Str"");
  Collection<RunRecordMeta> invalidRunRecords=Collections2.filter(notActuallyRunning,new Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta input){
      String runId=input.getPid();
      Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
      if (targetProgramId != null) {
        runIdToProgramId.put(runId,targetProgramId);
        return true;
      }
 else {
        return false;
      }
    }
  }
);
  LOG.trace(""String_Node_Str"");
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"",invalidRunRecords.size(),programType.getPrettyName());
  }
 else {
    LOG.trace(""String_Node_Str"",programType.getPrettyName());
  }
  for (  RunRecordMeta invalidRunRecordMeta : invalidRunRecords) {
    boolean shouldCorrect=shouldCorrectForWorkflowChildren(invalidRunRecordMeta,processedInvalidRunRecordIds);
    if (!shouldCorrect) {
      LOG.trace(""String_Node_Str"",invalidRunRecordMeta);
      continue;
    }
    String runId=invalidRunRecordMeta.getPid();
    Id.Program targetProgramId=runIdToProgramId.get(runId);
    LOG.warn(""String_Node_Str"",runId,targetProgramId,programType.getPrettyName());
    store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
    processedInvalidRunRecordIds.add(runId);
  }
}"
6710,"@Override public void run(){
  LOG.info(""String_Node_Str"");
  Futures.getUnchecked(Services.chainStart(zkClientService,kafkaClientService,metricsCollectionService,streamCoordinatorClient,resourceReporter));
  LOG.info(""String_Node_Str"",name);
  controller=injector.getInstance(getProgramClass()).run(program,programOpts);
  final SettableFuture<ProgramController.State> state=SettableFuture.create();
  controller.addListener(new AbstractListener(){
    @Override public void alive(){
      runlatch.countDown();
    }
    @Override public void init(    ProgramController.State currentState,    @Nullable Throwable cause){
      if (currentState == ProgramController.State.ALIVE) {
        alive();
      }
 else {
        super.init(currentState,cause);
      }
    }
    @Override public void completed(){
      state.set(ProgramController.State.COMPLETED);
    }
    @Override public void killed(){
      state.set(ProgramController.State.KILLED);
    }
    @Override public void error(    Throwable cause){
      LOG.error(""String_Node_Str"",cause);
      state.setException(cause);
    }
  }
,MoreExecutors.sameThreadExecutor());
  try {
    state.get();
    LOG.info(""String_Node_Str"");
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",e);
  }
catch (  ExecutionException e) {
    LOG.error(""String_Node_Str"",e);
    if (propagateServiceError()) {
      throw Throwables.propagate(Throwables.getRootCause(e));
    }
  }
 finally {
    runlatch.countDown();
  }
}","@Override public void run(){
  LOG.info(""String_Node_Str"");
  Futures.getUnchecked(Services.chainStart(zkClientService,kafkaClientService,metricsCollectionService,streamCoordinatorClient,resourceReporter));
  LOG.info(""String_Node_Str"",name);
  controller=injector.getInstance(getProgramClass()).run(program,programOpts);
  final SettableFuture<ProgramController.State> state=SettableFuture.create();
  controller.addListener(new AbstractListener(){
    @Override public void alive(){
      runlatch.countDown();
    }
    @Override public void init(    ProgramController.State currentState,    @Nullable Throwable cause){
      if (currentState == ProgramController.State.ALIVE) {
        alive();
      }
 else {
        super.init(currentState,cause);
      }
    }
    @Override public void completed(){
      state.set(ProgramController.State.COMPLETED);
    }
    @Override public void killed(){
      state.set(ProgramController.State.KILLED);
    }
    @Override public void error(    Throwable cause){
      LOG.error(""String_Node_Str"",cause);
      state.setException(cause);
    }
  }
,MoreExecutors.sameThreadExecutor());
  try {
    state.get();
    LOG.info(""String_Node_Str"",name);
  }
 catch (  InterruptedException e) {
    LOG.warn(""String_Node_Str"",name,e);
    Thread.currentThread().interrupt();
  }
catch (  ExecutionException e) {
    LOG.error(""String_Node_Str"",name,e);
    if (propagateServiceError()) {
      throw Throwables.propagate(Throwables.getRootCause(e));
    }
  }
 finally {
    runlatch.countDown();
  }
}"
6711,"@Override public void stop(){
  try {
    LOG.info(""String_Node_Str"",name);
    controller.stop().get();
    logAppenderInitializer.close();
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",e,e);
    throw Throwables.propagate(e);
  }
}","@Override public void stop(){
  try {
    LOG.info(""String_Node_Str"",name);
    controller.stop().get();
    logAppenderInitializer.close();
  }
 catch (  InterruptedException e) {
    LOG.debug(""String_Node_Str"",name,e);
    Thread.currentThread().interrupt();
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",name,e);
    throw Throwables.propagate(e);
  }
}"
6712,"@Override public void stop(){
  if (!stopped.compareAndSet(false,true)) {
    return;
  }
  scheduledExecutor.shutdownNow();
  close();
  super.stop();
}","@Override public void stop(){
  if (!stopped.compareAndSet(false,true)) {
    return;
  }
  scheduledExecutor.shutdownNow();
  try {
    scheduledExecutor.awaitTermination(5,TimeUnit.MINUTES);
  }
 catch (  InterruptedException e) {
    LOG.debug(""String_Node_Str"");
    Thread.currentThread().interrupt();
  }
  close();
  super.stop();
}"
6713,"@Inject public FileLogAppender(CConfiguration cConfig,DatasetFramework dsFramework,TransactionExecutorFactory txExecutorFactory,LocationFactory locationFactory,NamespacedLocationFactory namespacedLocationFactory){
  setName(APPENDER_NAME);
  this.cConf=cConfig;
  this.tableUtil=new LogSaverTableUtil(dsFramework,cConfig);
  this.txExecutorFactory=txExecutorFactory;
  this.locationFactory=locationFactory;
  this.namespacedLocationFactory=namespacedLocationFactory;
  this.dsFramework=dsFramework;
  this.logBaseDir=cConfig.get(LoggingConfiguration.LOG_BASE_DIR);
  Preconditions.checkNotNull(logBaseDir,""String_Node_Str"");
  this.syncIntervalBytes=cConfig.getInt(LoggingConfiguration.LOG_FILE_SYNC_INTERVAL_BYTES,50 * 1024);
  Preconditions.checkArgument(this.syncIntervalBytes > 0,""String_Node_Str"",this.syncIntervalBytes);
  long retentionDurationDays=cConfig.getLong(LoggingConfiguration.LOG_RETENTION_DURATION_DAYS,-1);
  Preconditions.checkArgument(retentionDurationDays > 0,""String_Node_Str"",retentionDurationDays);
  this.retentionDurationMs=TimeUnit.MILLISECONDS.convert(retentionDurationDays,TimeUnit.DAYS);
  maxLogFileSizeBytes=cConfig.getLong(LoggingConfiguration.LOG_MAX_FILE_SIZE_BYTES,20 * 1024 * 1024);
  Preconditions.checkArgument(maxLogFileSizeBytes > 0,""String_Node_Str"",maxLogFileSizeBytes);
  inactiveIntervalMs=cConfig.getLong(LoggingConfiguration.LOG_SAVER_INACTIVE_FILE_INTERVAL_MS,LoggingConfiguration.DEFAULT_LOG_SAVER_INACTIVE_FILE_INTERVAL_MS);
  Preconditions.checkArgument(inactiveIntervalMs > 0,""String_Node_Str"",inactiveIntervalMs);
  checkpointIntervalMs=cConfig.getLong(LoggingConfiguration.LOG_SAVER_CHECKPOINT_INTERVAL_MS,LoggingConfiguration.DEFAULT_LOG_SAVER_CHECKPOINT_INTERVAL_MS);
  Preconditions.checkArgument(checkpointIntervalMs > 0,""String_Node_Str"",checkpointIntervalMs);
  logCleanupIntervalMins=cConfig.getInt(LoggingConfiguration.LOG_CLEANUP_RUN_INTERVAL_MINS,LoggingConfiguration.DEFAULT_LOG_CLEANUP_RUN_INTERVAL_MINS);
  Preconditions.checkArgument(logCleanupIntervalMins > 0,""String_Node_Str"",logCleanupIntervalMins);
  this.scheduledExecutor=MoreExecutors.listeningDecorator(Executors.newSingleThreadScheduledExecutor(Threads.createDaemonThreadFactory(""String_Node_Str"")));
}","@Inject public FileLogAppender(CConfiguration cConfig,DatasetFramework dsFramework,TransactionExecutorFactory txExecutorFactory,LocationFactory locationFactory){
  setName(APPENDER_NAME);
  this.cConf=cConfig;
  this.tableUtil=new LogSaverTableUtil(dsFramework,cConfig);
  this.txExecutorFactory=txExecutorFactory;
  this.locationFactory=locationFactory;
  this.logBaseDir=cConfig.get(LoggingConfiguration.LOG_BASE_DIR);
  Preconditions.checkNotNull(logBaseDir,""String_Node_Str"");
  this.syncIntervalBytes=cConfig.getInt(LoggingConfiguration.LOG_FILE_SYNC_INTERVAL_BYTES,50 * 1024);
  Preconditions.checkArgument(this.syncIntervalBytes > 0,""String_Node_Str"",this.syncIntervalBytes);
  long retentionDurationDays=cConfig.getLong(LoggingConfiguration.LOG_RETENTION_DURATION_DAYS,-1);
  Preconditions.checkArgument(retentionDurationDays > 0,""String_Node_Str"",retentionDurationDays);
  this.retentionDurationMs=TimeUnit.MILLISECONDS.convert(retentionDurationDays,TimeUnit.DAYS);
  maxLogFileSizeBytes=cConfig.getLong(LoggingConfiguration.LOG_MAX_FILE_SIZE_BYTES,20 * 1024 * 1024);
  Preconditions.checkArgument(maxLogFileSizeBytes > 0,""String_Node_Str"",maxLogFileSizeBytes);
  inactiveIntervalMs=cConfig.getLong(LoggingConfiguration.LOG_SAVER_INACTIVE_FILE_INTERVAL_MS,LoggingConfiguration.DEFAULT_LOG_SAVER_INACTIVE_FILE_INTERVAL_MS);
  Preconditions.checkArgument(inactiveIntervalMs > 0,""String_Node_Str"",inactiveIntervalMs);
  checkpointIntervalMs=cConfig.getLong(LoggingConfiguration.LOG_SAVER_CHECKPOINT_INTERVAL_MS,LoggingConfiguration.DEFAULT_LOG_SAVER_CHECKPOINT_INTERVAL_MS);
  Preconditions.checkArgument(checkpointIntervalMs > 0,""String_Node_Str"",checkpointIntervalMs);
  logCleanupIntervalMins=cConfig.getInt(LoggingConfiguration.LOG_CLEANUP_RUN_INTERVAL_MINS,LoggingConfiguration.DEFAULT_LOG_CLEANUP_RUN_INTERVAL_MINS);
  Preconditions.checkArgument(logCleanupIntervalMins > 0,""String_Node_Str"",logCleanupIntervalMins);
  this.scheduledExecutor=MoreExecutors.listeningDecorator(Executors.newSingleThreadScheduledExecutor(Threads.createDaemonThreadFactory(""String_Node_Str"")));
}"
6714,"@Nullable private PluginInstantiator getArtifactPluginInstantiator(CConfiguration cConf){
  ClassLoader classLoader=Delegators.getDelegate(cConf.getClassLoader(),MapReduceClassLoader.class);
  if (!(classLoader instanceof MapReduceClassLoader)) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  return ((MapReduceClassLoader)classLoader).getArtifactPluginInstantiator();
}","@Nullable private PluginInstantiator getArtifactPluginInstantiator(Configuration hConf){
  ClassLoader classLoader=Delegators.getDelegate(hConf.getClassLoader(),MapReduceClassLoader.class);
  if (!(classLoader instanceof MapReduceClassLoader)) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  return ((MapReduceClassLoader)classLoader).getArtifactPluginInstantiator();
}"
6715,"@Nullable private PluginInstantiator getPluginInstantiator(CConfiguration cConf){
  if (contextConfig.getAdapterSpec() == null) {
    return null;
  }
  ClassLoader classLoader=Delegators.getDelegate(cConf.getClassLoader(),MapReduceClassLoader.class);
  if (!(classLoader instanceof MapReduceClassLoader)) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  return ((MapReduceClassLoader)classLoader).getPluginInstantiator();
}","@Nullable private PluginInstantiator getPluginInstantiator(Configuration hConf){
  if (contextConfig.getAdapterSpec() == null) {
    return null;
  }
  ClassLoader classLoader=Delegators.getDelegate(hConf.getClassLoader(),MapReduceClassLoader.class);
  if (!(classLoader instanceof MapReduceClassLoader)) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  return ((MapReduceClassLoader)classLoader).getPluginInstantiator();
}"
6716,"/** 
 * Creates an instance of   {@link BasicMapReduceContext} that the {@link co.cask.cdap.app.program.Program} containedinside cannot load program classes. It is used for the cases where only the application specification is needed, but no need to load any class from it.
 */
public synchronized BasicMapReduceContext get(){
  if (context == null) {
    CConfiguration cConf=contextConfig.getConf();
    context=getBuilder(cConf).build(type,contextConfig.getRunId(),taskContext.getTaskAttemptID().getTaskID().toString(),contextConfig.getLogicalStartTime(),contextConfig.getProgramNameInWorkflow(),contextConfig.getWorkflowToken(),contextConfig.getArguments(),contextConfig.getTx(),createProgram(contextConfig),artifactLocationFactory,contextConfig.getInputDataSet(),contextConfig.getInputSelection(),contextConfig.getOutputDataSet(),contextConfig.getAdapterSpec(),getPluginInstantiator(cConf),getArtifactPluginInstantiator(cConf));
  }
  return context;
}","/** 
 * Creates an instance of   {@link BasicMapReduceContext} that the {@link co.cask.cdap.app.program.Program} containedinside cannot load program classes. It is used for the cases where only the application specification is needed, but no need to load any class from it.
 */
public synchronized BasicMapReduceContext get(){
  if (context == null) {
    CConfiguration cConf=contextConfig.getConf();
    context=getBuilder(cConf).build(type,contextConfig.getRunId(),taskContext.getTaskAttemptID().getTaskID().toString(),contextConfig.getLogicalStartTime(),contextConfig.getProgramNameInWorkflow(),contextConfig.getWorkflowToken(),contextConfig.getArguments(),contextConfig.getTx(),createProgram(contextConfig),artifactLocationFactory,contextConfig.getInputDataSet(),contextConfig.getInputSelection(),contextConfig.getOutputDataSet(),contextConfig.getAdapterSpec(),getPluginInstantiator(contextConfig.getConfiguration()),getArtifactPluginInstantiator(contextConfig.getConfiguration()));
  }
  return context;
}"
6717,"/** 
 * Creates a service listener to reactor on state changes on   {@link MapReduceRuntimeService}.
 */
private Service.Listener createRuntimeServiceListener(final Program program,final RunId runId,final AdapterDefinition adapterSpec,final PluginInstantiator pluginInstantiator,Arguments arguments){
  final String twillRunId=arguments.getOption(ProgramOptionConstants.TWILL_RUN_ID);
  final String workflowName=arguments.getOption(ProgramOptionConstants.WORKFLOW_NAME);
  final String workflowNodeId=arguments.getOption(ProgramOptionConstants.WORKFLOW_NODE_ID);
  final String workflowRunId=arguments.getOption(ProgramOptionConstants.WORKFLOW_RUN_ID);
  return new ServiceListenerAdapter(){
    @Override public void starting(){
      long startTimeInSeconds=RunIds.getTime(runId,TimeUnit.SECONDS);
      if (startTimeInSeconds == -1) {
        startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
      }
      String adapterName=adapterSpec == null ? null : adapterSpec.getName();
      if (workflowName == null) {
        store.setStart(program.getId(),runId.getId(),startTimeInSeconds,adapterName,twillRunId);
      }
 else {
        store.setWorkflowProgramStart(program.getId(),runId.getId(),workflowName,workflowRunId,workflowNodeId,startTimeInSeconds,adapterName,twillRunId);
      }
    }
    @Override public void terminated(    Service.State from){
      if (pluginInstantiator != null) {
        Closeables.closeQuietly(pluginInstantiator);
      }
      if (from == Service.State.STOPPING) {
        store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.KILLED.getRunStatus());
      }
 else {
        store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.COMPLETED.getRunStatus());
      }
    }
    @Override public void failed(    Service.State from,    Throwable failure){
      if (pluginInstantiator != null) {
        Closeables.closeQuietly(pluginInstantiator);
      }
      store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus());
    }
  }
;
}","/** 
 * Creates a service listener to reactor on state changes on   {@link MapReduceRuntimeService}.
 */
private Service.Listener createRuntimeServiceListener(final Program program,final RunId runId,final AdapterDefinition adapterSpec,@Nullable final PluginInstantiator pluginInstantiator,final PluginInstantiator artifactPluginInstantiator,Arguments arguments){
  final String twillRunId=arguments.getOption(ProgramOptionConstants.TWILL_RUN_ID);
  final String workflowName=arguments.getOption(ProgramOptionConstants.WORKFLOW_NAME);
  final String workflowNodeId=arguments.getOption(ProgramOptionConstants.WORKFLOW_NODE_ID);
  final String workflowRunId=arguments.getOption(ProgramOptionConstants.WORKFLOW_RUN_ID);
  return new ServiceListenerAdapter(){
    @Override public void starting(){
      long startTimeInSeconds=RunIds.getTime(runId,TimeUnit.SECONDS);
      if (startTimeInSeconds == -1) {
        startTimeInSeconds=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
      }
      String adapterName=adapterSpec == null ? null : adapterSpec.getName();
      if (workflowName == null) {
        store.setStart(program.getId(),runId.getId(),startTimeInSeconds,adapterName,twillRunId);
      }
 else {
        store.setWorkflowProgramStart(program.getId(),runId.getId(),workflowName,workflowRunId,workflowNodeId,startTimeInSeconds,adapterName,twillRunId);
      }
    }
    @Override public void terminated(    Service.State from){
      Closeables.closeQuietly(artifactPluginInstantiator);
      if (pluginInstantiator != null) {
        Closeables.closeQuietly(pluginInstantiator);
      }
      if (from == Service.State.STOPPING) {
        store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.KILLED.getRunStatus());
      }
 else {
        store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.COMPLETED.getRunStatus());
      }
    }
    @Override public void failed(    Service.State from,    Throwable failure){
      if (pluginInstantiator != null) {
        Closeables.closeQuietly(pluginInstantiator);
      }
      store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.ERROR.getRunStatus());
    }
  }
;
}"
6718,"@Nullable private PluginInstantiator createArtifactPluginInstantiator(ClassLoader programClassLoader){
  return new PluginInstantiator(cConf,programClassLoader);
}","private PluginInstantiator createArtifactPluginInstantiator(ClassLoader programClassLoader){
  return new PluginInstantiator(cConf,programClassLoader);
}"
6719,"@Override public void terminated(Service.State from){
  if (pluginInstantiator != null) {
    Closeables.closeQuietly(pluginInstantiator);
  }
  if (from == Service.State.STOPPING) {
    store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.KILLED.getRunStatus());
  }
 else {
    store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.COMPLETED.getRunStatus());
  }
}","@Override public void terminated(Service.State from){
  Closeables.closeQuietly(artifactPluginInstantiator);
  if (pluginInstantiator != null) {
    Closeables.closeQuietly(pluginInstantiator);
  }
  if (from == Service.State.STOPPING) {
    store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.KILLED.getRunStatus());
  }
 else {
    store.setStop(program.getId(),runId.getId(),TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis()),ProgramController.State.COMPLETED.getRunStatus());
  }
}"
6720,"public void deployApplication(Id.Application appId,CreateAppRequest createAppRequest) throws Exception {
  DefaultHttpRequest request=new DefaultHttpRequest(HttpVersion.HTTP_1_1,HttpMethod.PUT,String.format(""String_Node_Str"",appId.getNamespaceId(),appId.getId()));
  request.setHeader(Constants.Gateway.API_KEY,""String_Node_Str"");
  MockResponder mockResponder=new MockResponder();
  BodyConsumer bodyConsumer=appLifecycleHttpHandler.deploy(request,mockResponder,appId.getNamespaceId(),appId.getId(),createAppRequest.getArtifact().getName(),GSON.toJson(createAppRequest.getConfig()),MediaType.APPLICATION_JSON);
  Preconditions.checkNotNull(bodyConsumer,""String_Node_Str"");
  bodyConsumer.chunk(ChannelBuffers.wrappedBuffer(Bytes.toBytes(GSON.toJson(createAppRequest))),mockResponder);
  bodyConsumer.finished(mockResponder);
}","public void deployApplication(Id.Application appId,AppRequest appRequest) throws Exception {
  DefaultHttpRequest request=new DefaultHttpRequest(HttpVersion.HTTP_1_1,HttpMethod.PUT,String.format(""String_Node_Str"",appId.getNamespaceId(),appId.getId()));
  request.setHeader(Constants.Gateway.API_KEY,""String_Node_Str"");
  MockResponder mockResponder=new MockResponder();
  BodyConsumer bodyConsumer=appLifecycleHttpHandler.deploy(request,mockResponder,appId.getNamespaceId(),appId.getId(),appRequest.getArtifact().getName(),GSON.toJson(appRequest.getConfig()),MediaType.APPLICATION_JSON);
  Preconditions.checkNotNull(bodyConsumer,""String_Node_Str"");
  bodyConsumer.chunk(ChannelBuffers.wrappedBuffer(Bytes.toBytes(GSON.toJson(appRequest))),mockResponder);
  bodyConsumer.finished(mockResponder);
}"
6721,"@Override public ApplicationManager deployApplication(Id.Application appId,CreateAppRequest createAppRequest) throws Exception {
  applicationClient.deploy(appId,createAppRequest);
  return new RemoteApplicationManager(appId,clientConfig,restClient);
}","@Override public ApplicationManager deployApplication(Id.Application appId,AppRequest appRequest) throws Exception {
  applicationClient.deploy(appId,appRequest);
  return new RemoteApplicationManager(appId,clientConfig,restClient);
}"
6722,"/** 
 * Deploys an   {@link Application}.
 * @param appId the id of the application to create
 * @param createAppRequest the app creation request that includes the artifact to create the app from and any configto pass to the application.
 * @return An {@link ApplicationManager} to manage the deployed application.
 */
ApplicationManager deployApplication(Id.Application appId,CreateAppRequest createAppRequest) throws Exception ;","/** 
 * Deploys an   {@link Application}.
 * @param appId the id of the application to create
 * @param appRequest the app create or update request that includes the artifact to create the app from and any configto pass to the application.
 * @return An {@link ApplicationManager} to manage the deployed application.
 */
ApplicationManager deployApplication(Id.Application appId,AppRequest appRequest) throws Exception ;"
6723,"/** 
 * Deploys an   {@link Application}. The application artifact must already exist.
 * @param appId the id of the application to create
 * @param createAppRequest the application create request
 * @return An {@link ApplicationManager} to manage the deployed application
 */
protected static ApplicationManager deployApplication(Id.Application appId,CreateAppRequest createAppRequest) throws Exception {
  return getTestManager().deployApplication(appId,createAppRequest);
}","/** 
 * Deploys an   {@link Application}. The application artifact must already exist.
 * @param appId the id of the application to create
 * @param appRequest the application create or update request
 * @return An {@link ApplicationManager} to manage the deployed application
 */
protected static ApplicationManager deployApplication(Id.Application appId,AppRequest appRequest) throws Exception {
  return getTestManager().deployApplication(appId,appRequest);
}"
6724,"@Override public ApplicationManager deployApplication(Id.Application appId,CreateAppRequest createAppRequest) throws Exception {
  appFabricClient.deployApplication(appId,createAppRequest);
  ArtifactSummary requestedArtifact=createAppRequest.getArtifact();
  Id.Artifact artifactId=Id.Artifact.from(requestedArtifact.isSystem() ? Id.Namespace.SYSTEM : appId.getNamespace(),requestedArtifact.getName(),requestedArtifact.getVersion());
  ArtifactDetail artifactDetail=artifactRepository.getArtifact(artifactId);
  return appManagerFactory.create(appId,artifactDetail.getDescriptor().getLocation());
}","@Override public ApplicationManager deployApplication(Id.Application appId,AppRequest appRequest) throws Exception {
  appFabricClient.deployApplication(appId,appRequest);
  ArtifactSummary requestedArtifact=appRequest.getArtifact();
  Id.Artifact artifactId=Id.Artifact.from(requestedArtifact.isSystem() ? Id.Namespace.SYSTEM : appId.getNamespace(),requestedArtifact.getName(),requestedArtifact.getVersion());
  ArtifactDetail artifactDetail=artifactRepository.getArtifact(artifactId);
  return appManagerFactory.create(appId,artifactDetail.getDescriptor().getLocation());
}"
6725,"@Test public void testAppWithPlugin() throws Exception {
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  addAppArtifact(artifactId,AppWithPlugin.class);
  ArtifactRange artifactRange=new ArtifactRange(artifactId.getNamespace(),artifactId.getName(),artifactId.getVersion(),true,new ArtifactVersion(""String_Node_Str""),true);
  Id.Artifact pluginArtifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  addPluginArtifact(pluginArtifactId,Sets.<ArtifactRange>newHashSet(artifactRange),ToStringPlugin.class);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  CreateAppRequest createRequest=new CreateAppRequest(new ArtifactSummary(artifactId.getName(),artifactId.getVersion().getVersion(),false));
  ApplicationManager appManager=deployApplication(appId,createRequest);
  WorkerManager workerManager=appManager.getWorkerManager(AppWithPlugin.WORKER);
  workerManager.start();
  workerManager.waitForStatus(false,5,1);
  List<RunRecord> workerRun=workerManager.getHistory(ProgramRunStatus.COMPLETED);
  Assert.assertFalse(workerRun.isEmpty());
  ServiceManager serviceManager=appManager.getServiceManager(AppWithPlugin.SERVICE);
  serviceManager.start();
  serviceManager.waitForStatus(true,1,10);
  URL serviceURL=serviceManager.getServiceURL(5,TimeUnit.SECONDS);
  callServiceGet(serviceURL,""String_Node_Str"");
  serviceManager.stop();
  serviceManager.waitForStatus(false,1,10);
  List<RunRecord> serviceRun=serviceManager.getHistory(ProgramRunStatus.KILLED);
  Assert.assertFalse(serviceRun.isEmpty());
  MapReduceManager mrManager=appManager.getMapReduceManager(AppWithPlugin.MAPREDUCE);
  mrManager.start();
  mrManager.waitForFinish(10,TimeUnit.MINUTES);
  List<RunRecord> runRecords=mrManager.getHistory();
  Assert.assertNotEquals(ProgramRunStatus.FAILED,runRecords.get(0).getStatus());
}","@Test public void testAppWithPlugin() throws Exception {
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  addAppArtifact(artifactId,AppWithPlugin.class);
  ArtifactRange artifactRange=new ArtifactRange(artifactId.getNamespace(),artifactId.getName(),artifactId.getVersion(),true,new ArtifactVersion(""String_Node_Str""),true);
  Id.Artifact pluginArtifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  addPluginArtifact(pluginArtifactId,Sets.<ArtifactRange>newHashSet(artifactRange),ToStringPlugin.class);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  AppRequest createRequest=new AppRequest(new ArtifactSummary(artifactId.getName(),artifactId.getVersion().getVersion(),false));
  ApplicationManager appManager=deployApplication(appId,createRequest);
  WorkerManager workerManager=appManager.getWorkerManager(AppWithPlugin.WORKER);
  workerManager.start();
  workerManager.waitForStatus(false,5,1);
  List<RunRecord> workerRun=workerManager.getHistory(ProgramRunStatus.COMPLETED);
  Assert.assertFalse(workerRun.isEmpty());
  ServiceManager serviceManager=appManager.getServiceManager(AppWithPlugin.SERVICE);
  serviceManager.start();
  serviceManager.waitForStatus(true,1,10);
  URL serviceURL=serviceManager.getServiceURL(5,TimeUnit.SECONDS);
  callServiceGet(serviceURL,""String_Node_Str"");
  serviceManager.stop();
  serviceManager.waitForStatus(false,1,10);
  List<RunRecord> serviceRun=serviceManager.getHistory(ProgramRunStatus.KILLED);
  Assert.assertFalse(serviceRun.isEmpty());
  MapReduceManager mrManager=appManager.getMapReduceManager(AppWithPlugin.MAPREDUCE);
  mrManager.start();
  mrManager.waitForFinish(10,TimeUnit.MINUTES);
  List<RunRecord> runRecords=mrManager.getHistory();
  Assert.assertNotEquals(ProgramRunStatus.FAILED,runRecords.get(0).getStatus());
}"
6726,"@Test public void testAppFromArtifact() throws Exception {
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  addAppArtifact(artifactId,ConfigTestApp.class);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  CreateAppRequest<ConfigTestApp.ConfigClass> createRequest=new CreateAppRequest<>(new ArtifactSummary(artifactId.getName(),artifactId.getVersion().getVersion(),false),new ConfigTestApp.ConfigClass(""String_Node_Str"",""String_Node_Str""));
  ApplicationManager appManager=deployApplication(appId,createRequest);
  testAppConfig(appManager,createRequest.getConfig());
}","@Test public void testAppFromArtifact() throws Exception {
  Id.Artifact artifactId=Id.Artifact.from(Id.Namespace.DEFAULT,""String_Node_Str"",""String_Node_Str"");
  addAppArtifact(artifactId,ConfigTestApp.class);
  Id.Application appId=Id.Application.from(Id.Namespace.DEFAULT,""String_Node_Str"");
  AppRequest<ConfigTestApp.ConfigClass> createRequest=new AppRequest<>(new ArtifactSummary(artifactId.getName(),artifactId.getVersion().getVersion(),false),new ConfigTestApp.ConfigClass(""String_Node_Str"",""String_Node_Str""));
  ApplicationManager appManager=deployApplication(appId,createRequest);
  testAppConfig(appManager,createRequest.getConfig());
}"
6727,"@Override public boolean apply(@Nullable RunRecordMeta input){
  if (input == null) {
    return false;
  }
  String runId=input.getPid();
  return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
}","@Override public boolean apply(@Nullable RunRecordMeta input){
  if (input == null) {
    return false;
  }
  String runId=input.getPid();
  Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
  return (targetProgramId != null) && !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
}"
6728,"/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @param programType The type of program the run records need to validate and update.
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 */
void validateAndCorrectRunningRunRecords(ProgramType programType,Set<String> processedInvalidRunRecordIds){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecordMeta> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecordMeta>(){
    @Override public boolean apply(    @Nullable RunRecordMeta input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"",invalidRunRecords.size());
  }
  for (  RunRecordMeta invalidRunRecordMeta : invalidRunRecords) {
    String runId=invalidRunRecordMeta.getPid();
    Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
    if (targetProgramId == null) {
      continue;
    }
    boolean shouldCorrect=shouldCorrectForWorkflowChildren(invalidRunRecordMeta,processedInvalidRunRecordIds);
    if (!shouldCorrect) {
      continue;
    }
    LOG.warn(""String_Node_Str"",runId,targetProgramId,programType.getPrettyName());
    store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
    processedInvalidRunRecordIds.add(runId);
  }
}","/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @param programType The type of program the run records need to validate and update.
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 */
void validateAndCorrectRunningRunRecords(final ProgramType programType,Set<String> processedInvalidRunRecordIds){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecordMeta> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecordMeta>(){
    @Override public boolean apply(    @Nullable RunRecordMeta input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
      return (targetProgramId != null) && !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"",invalidRunRecords.size());
  }
  for (  RunRecordMeta invalidRunRecordMeta : invalidRunRecords) {
    String runId=invalidRunRecordMeta.getPid();
    Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
    boolean shouldCorrect=shouldCorrectForWorkflowChildren(invalidRunRecordMeta,processedInvalidRunRecordIds);
    if (!shouldCorrect) {
      continue;
    }
    LOG.warn(""String_Node_Str"",runId,targetProgramId,programType.getPrettyName());
    store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
    processedInvalidRunRecordIds.add(runId);
  }
}"
6729,"/** 
 * Resolves a bash-style file path into a   {@link File}. Handles ""."", "".."", and ""~"".
 * @param path bash-style path
 * @return {@link File} of the resolved path
 */
public File resolvePathToFile(String path){
  path=resolveVariables(path);
  if (path.contains(""String_Node_Str"") || path.contains(""String_Node_Str"")) {
    path=path.replace(""String_Node_Str"",File.separator);
    path=path.replace(""String_Node_Str"",File.separator);
  }
  if (path.startsWith(""String_Node_Str"" + File.separator)) {
    path=new File(homeDir,path.substring(2)).getAbsolutePath();
  }
  if (!new File(path).isAbsolute()) {
    path=new File(workingDir,path).getAbsolutePath();
  }
  String[] tokens=path.split(File.separator);
  LinkedList<String> finalTokens=new LinkedList<>();
  for (  String token : tokens) {
    if (token.equals(""String_Node_Str"")) {
      if (!finalTokens.isEmpty()) {
        finalTokens.removeLast();
      }
    }
 else     if (!token.equals(""String_Node_Str"")) {
      finalTokens.addLast(token);
    }
  }
  return new File(File.separator + Joiner.on(File.separator).join(finalTokens));
}","/** 
 * Resolves a bash-style file path into a   {@link File}. Handles ""."", "".."", and ""~"".
 * @param path bash-style path
 * @return {@link File} of the resolved path
 */
public File resolvePathToFile(String path){
  path=resolveVariables(path);
  if (path.contains(""String_Node_Str"") || path.contains(""String_Node_Str"")) {
    path=path.replace(""String_Node_Str"",File.separator);
    path=path.replace(""String_Node_Str"",File.separator);
  }
  if (path.startsWith(""String_Node_Str"" + File.separator)) {
    path=new File(homeDir,path.substring(2)).getAbsolutePath();
  }
  if (!new File(path).isAbsolute()) {
    path=new File(workingDir,path).getAbsolutePath();
  }
  String[] tokens=path.split(Pattern.quote(File.separator));
  LinkedList<String> finalTokens=new LinkedList<>();
  for (  String token : tokens) {
    if (token.equals(""String_Node_Str"")) {
      if (!finalTokens.isEmpty()) {
        finalTokens.removeLast();
      }
    }
 else     if (!token.equals(""String_Node_Str"")) {
      finalTokens.addLast(token);
    }
  }
  return new File(File.separator + Joiner.on(File.separator).join(finalTokens));
}"
6730,"@Override public void initialize(RealtimeContext context) throws Exception {
  super.initialize(context);
  String kafkaZKConnect=getKafkaConfig().getZookeeper();
  if (kafkaZKConnect != null) {
    zkClient=ZKClientServices.delegate(ZKClients.reWatchOnExpire(ZKClients.retryOnFailure(ZKClientService.Builder.of(kafkaZKConnect).build(),RetryStrategies.fixDelay(2,TimeUnit.SECONDS))));
    zkClient.startAndWait();
    brokerService=createBrokerService(zkClient);
    brokerService.startAndWait();
  }
  kafkaConsumers=CacheBuilder.newBuilder().concurrencyLevel(1).expireAfterAccess(60,TimeUnit.SECONDS).removalListener(consumerCacheRemovalListener()).build();
}","@Override public void initialize(RealtimeContext context) throws Exception {
  super.initialize(context);
  String kafkaZKConnect=getKafkaConfig().getZookeeper();
  if (kafkaZKConnect != null) {
    zkClient=ZKClientServices.delegate(ZKClients.reWatchOnExpire(ZKClients.retryOnFailure(ZKClientService.Builder.of(kafkaZKConnect).build(),RetryStrategies.fixDelay(2,TimeUnit.SECONDS))));
    zkClient.startAndWait();
    brokerService=new ZKBrokerService(zkClient);
    brokerService.startAndWait();
  }
  kafkaConsumers=CacheBuilder.newBuilder().concurrencyLevel(1).expireAfterAccess(60,TimeUnit.SECONDS).removalListener(consumerCacheRemovalListener()).build();
}"
6731,"/** 
 * Sends an event to a stream. The writes is asynchronous, meaning when this method returns, it only guarantees the event has been received by the server, but may not get persisted.
 * @param stream ID of the stream
 * @param event event to send to the stream
 * @throws IOException if a network error occurred
 * @throws StreamNotFoundException if the stream with the specified ID was not found
 */
public void asyncSendEvent(Id.Stream stream,String event) throws IOException, StreamNotFoundException, UnauthorizedException {
  URL url=config.resolveNamespacedURLV3(stream.getNamespace(),String.format(""String_Node_Str"",stream.getId()));
  writeEvent(url,stream,event);
}","/** 
 * Sends an event to a stream. The write is asynchronous, meaning when this method returns, it only guarantees the event has been received by the server, but may not get persisted.
 * @param stream ID of the stream
 * @param event event to send to the stream
 * @throws IOException if a network error occurred
 * @throws StreamNotFoundException if the stream with the specified ID was not found
 */
public void asyncSendEvent(Id.Stream stream,String event) throws IOException, StreamNotFoundException, UnauthorizedException {
  URL url=config.resolveNamespacedURLV3(stream.getNamespace(),String.format(""String_Node_Str"",stream.getId()));
  writeEvent(url,stream,event);
}"
6732,"@ProcessInput public void receive(StreamEvent data){
  table.increment(Bytes.toBytes(KEY),1L);
}","@ProcessInput public void receive(StreamEvent data){
  table.increment(Bytes.toBytes(KEY),1L);
  for (  Map.Entry<String,String> header : data.getHeaders().entrySet()) {
    headers.write(header.getKey(),header.getValue());
  }
}"
6733,"@Override public void run(){
  try {
    getContext().write(STREAM,ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str"")));
    getContext().write(STREAM,new StreamEventData(ImmutableMap.<String,String>of(),ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str""))));
    File tempDir=Files.createTempDir();
    File file=File.createTempFile(""String_Node_Str"",""String_Node_Str"",tempDir);
    BufferedWriter fileWriter=Files.newWriter(file,Charsets.UTF_8);
    fileWriter.write(""String_Node_Str"");
    fileWriter.write(""String_Node_Str"");
    fileWriter.close();
    getContext().writeFile(STREAM,file,""String_Node_Str"");
    StreamBatchWriter streamBatchWriter=getContext().createBatchWriter(STREAM,""String_Node_Str"");
    streamBatchWriter.write(ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str"")));
    streamBatchWriter.write(ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str"")));
    streamBatchWriter.write(ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str"")));
    streamBatchWriter.close();
    streamBatchWriter=getContext().createBatchWriter(STREAM,""String_Node_Str"");
    streamBatchWriter.write(ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str"")));
    streamBatchWriter.write(ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str"")));
    streamBatchWriter.close();
  }
 catch (  IOException e) {
    LOG.error(e.getMessage(),e);
  }
  for (int i=9; i < VALUE; i++) {
    try {
      getContext().write(STREAM,String.format(""String_Node_Str"",i));
    }
 catch (    IOException e) {
      LOG.error(e.getMessage(),e);
    }
  }
  try {
    getContext().write(""String_Node_Str"",""String_Node_Str"");
  }
 catch (  IOException e) {
  }
}","@Override public void run(){
  try {
    getContext().write(STREAM,ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str"")));
    getContext().write(STREAM,new StreamEventData(ImmutableMap.of(""String_Node_Str"",""String_Node_Str""),ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str""))));
    File tempDir=Files.createTempDir();
    File file=File.createTempFile(""String_Node_Str"",""String_Node_Str"",tempDir);
    BufferedWriter fileWriter=Files.newWriter(file,Charsets.UTF_8);
    fileWriter.write(""String_Node_Str"");
    fileWriter.write(""String_Node_Str"");
    fileWriter.close();
    getContext().writeFile(STREAM,file,""String_Node_Str"");
    StreamBatchWriter streamBatchWriter=getContext().createBatchWriter(STREAM,""String_Node_Str"");
    streamBatchWriter.write(ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str"")));
    streamBatchWriter.write(ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str"")));
    streamBatchWriter.write(ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str"")));
    streamBatchWriter.close();
    streamBatchWriter=getContext().createBatchWriter(STREAM,""String_Node_Str"");
    streamBatchWriter.write(ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str"")));
    streamBatchWriter.write(ByteBuffer.wrap(Bytes.toBytes(""String_Node_Str"")));
    streamBatchWriter.close();
  }
 catch (  IOException e) {
    LOG.error(e.getMessage(),e);
  }
  for (int i=9; i < VALUE; i++) {
    try {
      getContext().write(STREAM,String.format(""String_Node_Str"",i));
    }
 catch (    IOException e) {
      LOG.error(e.getMessage(),e);
    }
  }
  try {
    getContext().write(""String_Node_Str"",""String_Node_Str"");
  }
 catch (  IOException e) {
  }
}"
6734,"@Test public void testStreamWrites() throws Exception {
  HttpResponse response=GatewayFastTestsSuite.deploy(AppWritingtoStream.class,AppWritingtoStream.APPNAME);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  response=GatewayFastTestsSuite.doPost(String.format(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.FLOW),null);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  response=GatewayFastTestsSuite.doPost(String.format(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.WORKER),null);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  response=GatewayFastTestsSuite.doPost(String.format(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.SERVICE),null);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  waitState(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.FLOW,""String_Node_Str"");
  waitState(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.SERVICE,""String_Node_Str"");
  checkCount(AppWritingtoStream.VALUE);
  response=GatewayFastTestsSuite.doPost(String.format(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.FLOW),null);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  String workerState=getState(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.WORKER);
  if (workerState != null && workerState.equals(""String_Node_Str"")) {
    response=GatewayFastTestsSuite.doPost(String.format(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.WORKER),null);
    Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  }
  response=GatewayFastTestsSuite.doPost(String.format(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.SERVICE),null);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  waitState(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.FLOW,""String_Node_Str"");
  waitState(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.WORKER,""String_Node_Str"");
  waitState(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.SERVICE,""String_Node_Str"");
  response=GatewayFastTestsSuite.doDelete(String.format(""String_Node_Str"",AppWritingtoStream.APPNAME));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
}","@Test public void testStreamWrites() throws Exception {
  HttpResponse response=GatewayFastTestsSuite.deploy(AppWritingtoStream.class,AppWritingtoStream.APPNAME);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  response=GatewayFastTestsSuite.doPost(String.format(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.FLOW),null);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  response=GatewayFastTestsSuite.doPost(String.format(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.WORKER),null);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  response=GatewayFastTestsSuite.doPost(String.format(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.SERVICE),null);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  waitState(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.FLOW,""String_Node_Str"");
  waitState(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.SERVICE,""String_Node_Str"");
  checkCount(AppWritingtoStream.VALUE);
  checkHeader(""String_Node_Str"",""String_Node_Str"");
  response=GatewayFastTestsSuite.doPost(String.format(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.FLOW),null);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  String workerState=getState(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.WORKER);
  if (workerState != null && workerState.equals(""String_Node_Str"")) {
    response=GatewayFastTestsSuite.doPost(String.format(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.WORKER),null);
    Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  }
  response=GatewayFastTestsSuite.doPost(String.format(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.SERVICE),null);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  waitState(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.FLOW,""String_Node_Str"");
  waitState(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.WORKER,""String_Node_Str"");
  waitState(""String_Node_Str"",AppWritingtoStream.APPNAME,AppWritingtoStream.SERVICE,""String_Node_Str"");
  response=GatewayFastTestsSuite.doDelete(String.format(""String_Node_Str"",AppWritingtoStream.APPNAME));
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
}"
6735,"private void addTransforms(List<ETLStage> stageConfigs,List<Transformation> pipeline,List<StageMetrics> stageMetrics,List<String> transformIds,MapReduceContext context) throws Exception {
  Preconditions.checkArgument(stageConfigs.size() == transformIds.size());
  for (int i=0; i < stageConfigs.size(); i++) {
    ETLStage stageConfig=stageConfigs.get(i);
    String transformId=transformIds.get(i);
    Transform transform=context.newPluginInstance(transformId);
    BatchTransformContext transformContext=new BatchTransformContext(context,mapperMetrics,transformId);
    LOG.debug(""String_Node_Str"",stageConfig.getName());
    LOG.debug(""String_Node_Str"",transform.getClass().getName());
    transform.initialize(transformContext);
    pipeline.add(transform);
    transforms.add(transform);
    stageMetrics.add(new StageMetrics(mapperMetrics,StageMetrics.Type.TRANSFORM,stageConfig.getName()));
  }
}","private void addTransforms(List<ETLStage> stageConfigs,List<Transformation> pipeline,List<StageMetrics> stageMetrics,List<String> transformIds,MapReduceContext context) throws Exception {
  Preconditions.checkArgument(stageConfigs.size() == transformIds.size());
  for (int i=0; i < stageConfigs.size(); i++) {
    ETLStage stageConfig=stageConfigs.get(i);
    String transformId=transformIds.get(i);
    Transform transform=context.newPluginInstance(transformId);
    BatchTransformContext transformContext=new BatchTransformContext(context,mapperMetrics,transformId);
    LOG.debug(""String_Node_Str"",stageConfig.getName());
    LOG.debug(""String_Node_Str"",transform.getClass().getName());
    transform.initialize(transformContext);
    pipeline.add(transform);
    transforms.add(transform);
    stageMetrics.add(new StageMetrics(mapperMetrics,PluginID.from(transformId)));
  }
}"
6736,"@Override public void initialize(MapReduceContext context) throws Exception {
  Map<String,String> runtimeArgs=context.getRuntimeArguments();
  ETLBatchConfig etlConfig=GSON.fromJson(runtimeArgs.get(Constants.CONFIG_KEY),ETLBatchConfig.class);
  String sourcePluginId=runtimeArgs.get(Constants.Source.PLUGINID);
  String sinkPluginId=runtimeArgs.get(Constants.Sink.PLUGINID);
  List<String> transformIds=GSON.fromJson(runtimeArgs.get(Constants.Transform.PLUGINIDS),STRING_LIST_TYPE);
  List<ETLStage> stageList=etlConfig.getTransforms();
  List<Transformation> pipeline=Lists.newArrayListWithCapacity(stageList.size() + 2);
  List<StageMetrics> stageMetrics=Lists.newArrayListWithCapacity(stageList.size() + 2);
  transforms=Lists.newArrayListWithCapacity(stageList.size());
  BatchSource source=context.newPluginInstance(sourcePluginId);
  BatchSourceContext batchSourceContext=new MapReduceSourceContext(context,mapperMetrics,sourcePluginId);
  source.initialize(batchSourceContext);
  pipeline.add(source);
  stageMetrics.add(new StageMetrics(mapperMetrics,StageMetrics.Type.SOURCE,etlConfig.getSource().getName()));
  addTransforms(stageList,pipeline,stageMetrics,transformIds,context);
  BatchSink sink=context.newPluginInstance(sinkPluginId);
  BatchSinkContext batchSinkContext=new MapReduceSinkContext(context,mapperMetrics,sinkPluginId);
  sink.initialize(batchSinkContext);
  pipeline.add(sink);
  stageMetrics.add(new StageMetrics(mapperMetrics,StageMetrics.Type.SINK,etlConfig.getSink().getName()));
  transformExecutor=new TransformExecutor<>(pipeline,stageMetrics);
}","@Override public void initialize(MapReduceContext context) throws Exception {
  Map<String,String> runtimeArgs=context.getRuntimeArguments();
  ETLBatchConfig etlConfig=GSON.fromJson(runtimeArgs.get(Constants.CONFIG_KEY),ETLBatchConfig.class);
  String sourcePluginId=runtimeArgs.get(Constants.Source.PLUGINID);
  String sinkPluginId=runtimeArgs.get(Constants.Sink.PLUGINID);
  List<String> transformIds=GSON.fromJson(runtimeArgs.get(Constants.Transform.PLUGINIDS),STRING_LIST_TYPE);
  List<ETLStage> stageList=etlConfig.getTransforms();
  List<Transformation> pipeline=Lists.newArrayListWithCapacity(stageList.size() + 2);
  List<StageMetrics> stageMetrics=Lists.newArrayListWithCapacity(stageList.size() + 2);
  transforms=Lists.newArrayListWithCapacity(stageList.size());
  BatchSource source=context.newPluginInstance(sourcePluginId);
  BatchSourceContext batchSourceContext=new MapReduceSourceContext(context,mapperMetrics,sourcePluginId);
  source.initialize(batchSourceContext);
  pipeline.add(source);
  stageMetrics.add(new StageMetrics(mapperMetrics,PluginID.from(sourcePluginId)));
  addTransforms(stageList,pipeline,stageMetrics,transformIds,context);
  BatchSink sink=context.newPluginInstance(sinkPluginId);
  BatchSinkContext batchSinkContext=new MapReduceSinkContext(context,mapperMetrics,sinkPluginId);
  sink.initialize(batchSinkContext);
  pipeline.add(sink);
  stageMetrics.add(new StageMetrics(mapperMetrics,PluginID.from(sinkPluginId)));
  transformExecutor=new TransformExecutor<>(pipeline,stageMetrics);
}"
6737,"@Override public void configureAdapter(String adapterName,T etlConfig,AdapterConfigurer configurer) throws Exception {
  ETLStage sourceConfig=etlConfig.getSource();
  ETLStage sinkConfig=etlConfig.getSink();
  List<ETLStage> transformConfigs=etlConfig.getTransforms();
  String sourcePluginId=String.format(""String_Node_Str"",Constants.Source.PLUGINTYPE,Constants.ID_SEPARATOR,sourceConfig.getName());
  String sinkPluginId=String.format(""String_Node_Str"",Constants.Sink.PLUGINTYPE,Constants.ID_SEPARATOR,sinkConfig.getName());
  PluginProperties sourceProperties=getPluginProperties(sourceConfig);
  PipelineConfigurable source=configurer.usePlugin(Constants.Source.PLUGINTYPE,sourceConfig.getName(),sourcePluginId,sourceProperties);
  if (source == null) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",Constants.Source.PLUGINTYPE,sourceConfig.getName()));
  }
  PluginProperties sinkProperties=getPluginProperties(sinkConfig);
  PipelineConfigurable sink=configurer.usePlugin(Constants.Sink.PLUGINTYPE,sinkConfig.getName(),sinkPluginId,sinkProperties);
  if (sink == null) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",Constants.Sink.PLUGINTYPE,sinkConfig.getName()));
  }
  List<String> transformIds=Lists.newArrayListWithCapacity(transformConfigs.size());
  List<Transformation> transforms=Lists.newArrayListWithCapacity(transformConfigs.size());
  for (int i=0; i < transformConfigs.size(); i++) {
    ETLStage transformConfig=transformConfigs.get(i);
    String transformId=String.format(""String_Node_Str"",transformConfig.getName(),Constants.ID_SEPARATOR,i);
    PluginProperties transformProperties=getPluginProperties(transformConfig);
    Transform transformObj=configurer.usePlugin(Constants.Transform.PLUGINTYPE,transformConfig.getName(),transformId,transformProperties);
    if (transformObj == null) {
      throw new IllegalArgumentException(String.format(""String_Node_Str"",Constants.Transform.PLUGINTYPE,transformConfig.getName()));
    }
    transformIds.add(transformId);
    transforms.add(transformObj);
  }
  validateStages(source,sink,transforms);
  configure(source,configurer,sourcePluginId);
  configure(sink,configurer,sinkPluginId);
  configurer.addRuntimeArgument(Constants.ADAPTER_NAME,adapterName);
  configurer.addRuntimeArgument(Constants.Source.PLUGINID,sourcePluginId);
  configurer.addRuntimeArgument(Constants.Sink.PLUGINID,sinkPluginId);
  configurer.addRuntimeArgument(Constants.Transform.PLUGINIDS,GSON.toJson(transformIds));
  Resources resources=etlConfig.getResources();
  if (resources != null) {
    configurer.setResources(resources);
  }
}","@Override public void configureAdapter(String adapterName,T etlConfig,AdapterConfigurer configurer) throws Exception {
  ETLStage sourceConfig=etlConfig.getSource();
  ETLStage sinkConfig=etlConfig.getSink();
  List<ETLStage> transformConfigs=etlConfig.getTransforms();
  String sourcePluginId=PluginID.from(Constants.Source.PLUGINTYPE,sourceConfig.getName(),1).getID();
  String sinkPluginId=PluginID.from(Constants.Sink.PLUGINTYPE,sinkConfig.getName(),2 + transformConfigs.size()).getID();
  PluginProperties sourceProperties=getPluginProperties(sourceConfig);
  PipelineConfigurable source=configurer.usePlugin(Constants.Source.PLUGINTYPE,sourceConfig.getName(),sourcePluginId,sourceProperties);
  if (source == null) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",Constants.Source.PLUGINTYPE,sourceConfig.getName()));
  }
  PluginProperties sinkProperties=getPluginProperties(sinkConfig);
  PipelineConfigurable sink=configurer.usePlugin(Constants.Sink.PLUGINTYPE,sinkConfig.getName(),sinkPluginId,sinkProperties);
  if (sink == null) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",Constants.Sink.PLUGINTYPE,sinkConfig.getName()));
  }
  List<String> transformIds=Lists.newArrayListWithCapacity(transformConfigs.size());
  List<Transformation> transforms=Lists.newArrayListWithCapacity(transformConfigs.size());
  for (int i=0; i < transformConfigs.size(); i++) {
    ETLStage transformConfig=transformConfigs.get(i);
    String transformId=PluginID.from(Constants.Transform.PLUGINTYPE,transformConfig.getName(),2 + i).getID();
    PluginProperties transformProperties=getPluginProperties(transformConfig);
    Transform transformObj=configurer.usePlugin(Constants.Transform.PLUGINTYPE,transformConfig.getName(),transformId,transformProperties);
    if (transformObj == null) {
      throw new IllegalArgumentException(String.format(""String_Node_Str"",Constants.Transform.PLUGINTYPE,transformConfig.getName()));
    }
    transformIds.add(transformId);
    transforms.add(transformObj);
  }
  validateStages(source,sink,transforms);
  configure(source,configurer,sourcePluginId);
  configure(sink,configurer,sinkPluginId);
  configurer.addRuntimeArgument(Constants.ADAPTER_NAME,adapterName);
  configurer.addRuntimeArgument(Constants.Source.PLUGINID,sourcePluginId);
  configurer.addRuntimeArgument(Constants.Sink.PLUGINID,sinkPluginId);
  configurer.addRuntimeArgument(Constants.Transform.PLUGINIDS,GSON.toJson(transformIds));
  Resources resources=etlConfig.getResources();
  if (resources != null) {
    configurer.setResources(resources);
  }
}"
6738,"public StageMetrics(Metrics metrics,Type stageType,String name){
  this.metrics=metrics;
  this.prefix=stageType.toString() + ""String_Node_Str"" + name+ ""String_Node_Str"";
}","public StageMetrics(Metrics metrics,PluginID id){
  this.metrics=metrics;
  this.prefix=id.getMetricsContext() + ""String_Node_Str"";
}"
6739,"@Test public void testTransforms() throws Exception {
  MockMetrics mockMetrics=new MockMetrics();
  List<Transformation> transforms=Lists.<Transformation>newArrayList(new IntToDouble(),new Filter(100d),new DoubleToString());
  List<StageMetrics> stageMetrics=Lists.newArrayList(new StageMetrics(mockMetrics,StageMetrics.Type.SOURCE,""String_Node_Str""),new StageMetrics(mockMetrics,StageMetrics.Type.TRANSFORM,""String_Node_Str""),new StageMetrics(mockMetrics,StageMetrics.Type.SINK,""String_Node_Str""));
  TransformExecutor<Integer,String> executor=new TransformExecutor<>(transforms,stageMetrics);
  List<String> results=Lists.newArrayList(executor.runOneIteration(1));
  Assert.assertTrue(results.isEmpty());
  Assert.assertEquals(3,mockMetrics.getCount(""String_Node_Str""));
  Assert.assertEquals(0,mockMetrics.getCount(""String_Node_Str""));
  Assert.assertEquals(0,mockMetrics.getCount(""String_Node_Str""));
  results=Lists.newArrayList(executor.runOneIteration(10));
  Assert.assertEquals(1,results.size());
  Assert.assertEquals(""String_Node_Str"",results.get(0));
  Assert.assertEquals(6,mockMetrics.getCount(""String_Node_Str""));
  Assert.assertEquals(1,mockMetrics.getCount(""String_Node_Str""));
  Assert.assertEquals(1,mockMetrics.getCount(""String_Node_Str""));
  results=Lists.newArrayList(executor.runOneIteration(100));
  Assert.assertEquals(2,results.size());
  Assert.assertEquals(""String_Node_Str"",results.get(0));
  Assert.assertEquals(""String_Node_Str"",results.get(1));
  Assert.assertEquals(9,mockMetrics.getCount(""String_Node_Str""));
  Assert.assertEquals(3,mockMetrics.getCount(""String_Node_Str""));
  Assert.assertEquals(3,mockMetrics.getCount(""String_Node_Str""));
}","@Test public void testTransforms() throws Exception {
  MockMetrics mockMetrics=new MockMetrics();
  List<Transformation> transforms=Lists.<Transformation>newArrayList(new IntToDouble(),new Filter(100d),new DoubleToString());
  List<StageMetrics> stageMetrics=Lists.newArrayList(new StageMetrics(mockMetrics,PluginID.from(Constants.Source.PLUGINTYPE,""String_Node_Str"",1)),new StageMetrics(mockMetrics,PluginID.from(Constants.Transform.PLUGINTYPE,""String_Node_Str"",2)),new StageMetrics(mockMetrics,PluginID.from(Constants.Sink.PLUGINTYPE,""String_Node_Str"",3)));
  TransformExecutor<Integer,String> executor=new TransformExecutor<>(transforms,stageMetrics);
  List<String> results=Lists.newArrayList(executor.runOneIteration(1));
  Assert.assertTrue(results.isEmpty());
  Assert.assertEquals(3,mockMetrics.getCount(""String_Node_Str""));
  Assert.assertEquals(0,mockMetrics.getCount(""String_Node_Str""));
  Assert.assertEquals(0,mockMetrics.getCount(""String_Node_Str""));
  results=Lists.newArrayList(executor.runOneIteration(10));
  Assert.assertEquals(1,results.size());
  Assert.assertEquals(""String_Node_Str"",results.get(0));
  Assert.assertEquals(6,mockMetrics.getCount(""String_Node_Str""));
  Assert.assertEquals(1,mockMetrics.getCount(""String_Node_Str""));
  Assert.assertEquals(1,mockMetrics.getCount(""String_Node_Str""));
  results=Lists.newArrayList(executor.runOneIteration(100));
  Assert.assertEquals(2,results.size());
  Assert.assertEquals(""String_Node_Str"",results.get(0));
  Assert.assertEquals(""String_Node_Str"",results.get(1));
  Assert.assertEquals(9,mockMetrics.getCount(""String_Node_Str""));
  Assert.assertEquals(3,mockMetrics.getCount(""String_Node_Str""));
  Assert.assertEquals(3,mockMetrics.getCount(""String_Node_Str""));
}"
6740,"@SuppressWarnings(""String_Node_Str"") private void initializeSink(WorkerContext context,ETLStage stage) throws Exception {
  String sinkPluginId=context.getRuntimeArguments().get(Constants.Sink.PLUGINID);
  sink=context.newPluginInstance(sinkPluginId);
  RealtimeContext sinkContext=new WorkerRealtimeContext(context,metrics,sinkPluginId);
  LOG.debug(""String_Node_Str"",stage.getName());
  LOG.debug(""String_Node_Str"",sink.getClass().getName());
  sink.initialize(sinkContext);
  sink=new TrackedRealtimeSink(sink,metrics,stage.getName());
}","@SuppressWarnings(""String_Node_Str"") private void initializeSink(WorkerContext context,ETLStage stage) throws Exception {
  String sinkPluginId=context.getRuntimeArguments().get(Constants.Sink.PLUGINID);
  sink=context.newPluginInstance(sinkPluginId);
  RealtimeContext sinkContext=new WorkerRealtimeContext(context,metrics,sinkPluginId);
  LOG.debug(""String_Node_Str"",stage.getName());
  LOG.debug(""String_Node_Str"",sink.getClass().getName());
  sink.initialize(sinkContext);
  sink=new TrackedRealtimeSink(sink,metrics,PluginID.from(sinkPluginId));
}"
6741,"private void initializeSource(WorkerContext context,ETLStage stage) throws Exception {
  String sourcePluginId=context.getRuntimeArguments().get(Constants.Source.PLUGINID);
  source=context.newPluginInstance(sourcePluginId);
  RealtimeContext sourceContext=new WorkerRealtimeContext(context,metrics,sourcePluginId);
  LOG.debug(""String_Node_Str"",stage.getName());
  LOG.debug(""String_Node_Str"",source.getClass().getName());
  source.initialize(sourceContext);
  sourceEmitter=new DefaultEmitter(new StageMetrics(metrics,StageMetrics.Type.SOURCE,stage.getName()));
}","private void initializeSource(WorkerContext context,ETLStage stage) throws Exception {
  String sourcePluginId=context.getRuntimeArguments().get(Constants.Source.PLUGINID);
  source=context.newPluginInstance(sourcePluginId);
  RealtimeContext sourceContext=new WorkerRealtimeContext(context,metrics,sourcePluginId);
  LOG.debug(""String_Node_Str"",stage.getName());
  LOG.debug(""String_Node_Str"",source.getClass().getName());
  source.initialize(sourceContext);
  sourceEmitter=new DefaultEmitter(new StageMetrics(metrics,PluginID.from(sourcePluginId)));
}"
6742,"private List<Transformation> initializeTransforms(WorkerContext context,List<ETLStage> stages) throws Exception {
  List<String> transformIds=GSON.fromJson(context.getRuntimeArguments().get(Constants.Transform.PLUGINIDS),STRING_LIST_TYPE);
  List<Transformation> transforms=Lists.newArrayList();
  Preconditions.checkArgument(transformIds != null);
  Preconditions.checkArgument(stages.size() == transformIds.size());
  transformMetrics=Lists.newArrayListWithCapacity(stages.size());
  for (int i=0; i < stages.size(); i++) {
    ETLStage stage=stages.get(i);
    String transformId=transformIds.get(i);
    try {
      Transform transform=context.newPluginInstance(transformId);
      RealtimeTransformContext transformContext=new RealtimeTransformContext(context,metrics,transformId);
      LOG.debug(""String_Node_Str"",stage.getName());
      LOG.debug(""String_Node_Str"",transform.getClass().getName());
      transform.initialize(transformContext);
      transforms.add(transform);
      transformMetrics.add(new StageMetrics(metrics,StageMetrics.Type.TRANSFORM,stage.getName()));
    }
 catch (    InstantiationException e) {
      LOG.error(""String_Node_Str"",stage.getName(),e);
      Throwables.propagate(e);
    }
  }
  return transforms;
}","private List<Transformation> initializeTransforms(WorkerContext context,List<ETLStage> stages) throws Exception {
  List<String> transformIds=GSON.fromJson(context.getRuntimeArguments().get(Constants.Transform.PLUGINIDS),STRING_LIST_TYPE);
  List<Transformation> transforms=Lists.newArrayList();
  Preconditions.checkArgument(transformIds != null);
  Preconditions.checkArgument(stages.size() == transformIds.size());
  transformMetrics=Lists.newArrayListWithCapacity(stages.size());
  for (int i=0; i < stages.size(); i++) {
    ETLStage stage=stages.get(i);
    String transformId=transformIds.get(i);
    try {
      Transform transform=context.newPluginInstance(transformId);
      RealtimeTransformContext transformContext=new RealtimeTransformContext(context,metrics,transformId);
      LOG.debug(""String_Node_Str"",stage.getName());
      LOG.debug(""String_Node_Str"",transform.getClass().getName());
      transform.initialize(transformContext);
      transforms.add(transform);
      transformMetrics.add(new StageMetrics(metrics,PluginID.from(transformId)));
    }
 catch (    InstantiationException e) {
      LOG.error(""String_Node_Str"",stage.getName(),e);
      Throwables.propagate(e);
    }
  }
  return transforms;
}"
6743,"public TrackedRealtimeSink(RealtimeSink<T> sink,Metrics metrics,String name){
  this.sink=sink;
  this.metrics=new StageMetrics(metrics,StageMetrics.Type.SINK,name);
}","public TrackedRealtimeSink(RealtimeSink<T> sink,Metrics metrics,PluginID id){
  this.sink=sink;
  this.metrics=new StageMetrics(metrics,id);
}"
6744,"/** 
 * Prepare the specs of the twill application for the Explore twill runnable. Add jars needed by the Explore module in the classpath of the containers, and add conf files (hive_site.xml, etc) as resources available for the Explore twill runnable.
 */
private TwillPreparer prepareExploreContainer(TwillPreparer preparer){
  File tempDir=DirUtils.createTempDir(new File(cConf.get(Constants.CFG_LOCAL_DATA_DIR),cConf.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile());
  Preconditions.checkState(tempDir.mkdirs(),""String_Node_Str"" + tempDir.getAbsolutePath());
  try {
    Set<File> jars=ExploreServiceUtils.traceExploreDependencies();
    for (    File jarFile : jars) {
      LOG.trace(""String_Node_Str"",jarFile.getName());
      preparer=preparer.withClassPaths(jarFile.getName());
    }
  }
 catch (  IOException e) {
    throw new RuntimeException(""String_Node_Str"",e);
  }
  String hiveConfFiles=System.getProperty(Constants.Explore.EXPLORE_CONF_FILES);
  LOG.debug(""String_Node_Str"",hiveConfFiles);
  if (hiveConfFiles == null) {
    throw new RuntimeException(""String_Node_Str"" + Constants.Explore.EXPLORE_CONF_FILES + ""String_Node_Str"");
  }
  Iterable<File> hiveConfFilesFiles=ExploreServiceUtils.getClassPathJarsFiles(hiveConfFiles);
  Set<String> addedFiles=Sets.newHashSet();
  for (  File file : hiveConfFilesFiles) {
    if (file.getName().matches(""String_Node_Str"") && !file.getName().equals(""String_Node_Str"")) {
      if (addedFiles.add(file.getName())) {
        LOG.debug(""String_Node_Str"",file.getAbsolutePath());
        preparer=preparer.withResources(ExploreServiceUtils.updateConfFileForExplore(file,tempDir).toURI());
      }
 else {
        LOG.warn(""String_Node_Str"",file.getAbsolutePath());
      }
    }
  }
  return preparer;
}","/** 
 * Prepare the specs of the twill application for the Explore twill runnable. Add jars needed by the Explore module in the classpath of the containers, and add conf files (hive_site.xml, etc) as resources available for the Explore twill runnable.
 */
private TwillPreparer prepareExploreContainer(TwillPreparer preparer){
  File tempDir=DirUtils.createTempDir(new File(cConf.get(Constants.CFG_LOCAL_DATA_DIR),cConf.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile());
  try {
    Set<File> jars=ExploreServiceUtils.traceExploreDependencies();
    for (    File jarFile : jars) {
      LOG.trace(""String_Node_Str"",jarFile.getName());
      preparer=preparer.withClassPaths(jarFile.getName());
    }
  }
 catch (  IOException e) {
    throw new RuntimeException(""String_Node_Str"",e);
  }
  String hiveConfFiles=System.getProperty(Constants.Explore.EXPLORE_CONF_FILES);
  LOG.debug(""String_Node_Str"",hiveConfFiles);
  if (hiveConfFiles == null) {
    throw new RuntimeException(""String_Node_Str"" + Constants.Explore.EXPLORE_CONF_FILES + ""String_Node_Str"");
  }
  Iterable<File> hiveConfFilesFiles=ExploreServiceUtils.getClassPathJarsFiles(hiveConfFiles);
  Set<String> addedFiles=Sets.newHashSet();
  for (  File file : hiveConfFilesFiles) {
    if (file.getName().matches(""String_Node_Str"") && !file.getName().equals(""String_Node_Str"")) {
      if (addedFiles.add(file.getName())) {
        LOG.debug(""String_Node_Str"",file.getAbsolutePath());
        preparer=preparer.withResources(ExploreServiceUtils.updateConfFileForExplore(file,tempDir).toURI());
      }
 else {
        LOG.warn(""String_Node_Str"",file.getAbsolutePath());
      }
    }
  }
  return preparer;
}"
6745,"@Test public void hijackConfFileTest() throws Exception {
  Configuration conf=new Configuration(false);
  conf.set(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(1,conf.size());
  File confFile=tmpFolder.newFile(""String_Node_Str"");
  try (FileOutputStream os=new FileOutputStream(confFile)){
    conf.writeXml(os);
  }
   File newConfFile=ExploreServiceUtils.hijackConfFile(confFile);
  conf=new Configuration(false);
  conf.addResource(newConfFile.toURI().toURL());
  Assert.assertEquals(3,conf.size());
  Assert.assertEquals(""String_Node_Str"",conf.get(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST));
  Assert.assertEquals(""String_Node_Str"",conf.get(Job.MAPREDUCE_JOB_CLASSLOADER));
  Assert.assertEquals(""String_Node_Str"",conf.get(""String_Node_Str""));
  confFile=tmpFolder.newFile(""String_Node_Str"");
  conf=new YarnConfiguration();
  try (FileOutputStream os=new FileOutputStream(confFile)){
    conf.writeXml(os);
  }
   String yarnApplicationClassPath=""String_Node_Str"" + conf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
  newConfFile=ExploreServiceUtils.hijackConfFile(confFile);
  conf=new Configuration(false);
  conf.addResource(newConfFile.toURI().toURL());
  Assert.assertEquals(yarnApplicationClassPath,conf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH));
  confFile=tmpFolder.newFile(""String_Node_Str"");
  conf=new YarnConfiguration();
  try (FileOutputStream os=new FileOutputStream(confFile)){
    conf.writeXml(os);
  }
   String mapredApplicationClassPath=""String_Node_Str"" + conf.get(MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH,MRJobConfig.DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH);
  newConfFile=ExploreServiceUtils.hijackConfFile(confFile);
  conf=new Configuration(false);
  conf.addResource(newConfFile.toURI().toURL());
  Assert.assertEquals(mapredApplicationClassPath,conf.get(MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH));
  confFile=tmpFolder.newFile(""String_Node_Str"");
  Assert.assertEquals(confFile,ExploreServiceUtils.hijackConfFile(confFile));
}","@Test public void hijackConfFileTest() throws Exception {
  Configuration conf=new Configuration(false);
  conf.set(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(1,conf.size());
  File tempDir=tmpFolder.newFolder();
  File confFile=tmpFolder.newFile(""String_Node_Str"");
  try (FileOutputStream os=new FileOutputStream(confFile)){
    conf.writeXml(os);
  }
   File newConfFile=ExploreServiceUtils.updateConfFileForExplore(confFile,tempDir);
  conf=new Configuration(false);
  conf.addResource(newConfFile.toURI().toURL());
  Assert.assertEquals(3,conf.size());
  Assert.assertEquals(""String_Node_Str"",conf.get(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST));
  Assert.assertEquals(""String_Node_Str"",conf.get(Job.MAPREDUCE_JOB_CLASSLOADER));
  Assert.assertEquals(""String_Node_Str"",conf.get(""String_Node_Str""));
  confFile=tmpFolder.newFile(""String_Node_Str"");
  conf=new YarnConfiguration();
  try (FileOutputStream os=new FileOutputStream(confFile)){
    conf.writeXml(os);
  }
   String yarnApplicationClassPath=""String_Node_Str"" + conf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
  newConfFile=ExploreServiceUtils.updateConfFileForExplore(confFile,tempDir);
  conf=new Configuration(false);
  conf.addResource(newConfFile.toURI().toURL());
  Assert.assertEquals(yarnApplicationClassPath,conf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH));
  confFile=tmpFolder.newFile(""String_Node_Str"");
  conf=new YarnConfiguration();
  try (FileOutputStream os=new FileOutputStream(confFile)){
    conf.writeXml(os);
  }
   String mapredApplicationClassPath=""String_Node_Str"" + conf.get(MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH,MRJobConfig.DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH);
  newConfFile=ExploreServiceUtils.updateConfFileForExplore(confFile,tempDir);
  conf=new Configuration(false);
  conf.addResource(newConfFile.toURI().toURL());
  Assert.assertEquals(mapredApplicationClassPath,conf.get(MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH));
  confFile=tmpFolder.newFile(""String_Node_Str"");
  Assert.assertEquals(confFile,ExploreServiceUtils.updateConfFileForExplore(confFile,tempDir));
}"
6746,"/** 
 * Prepare the specs of the twill application for the Explore twill runnable. Add jars needed by the Explore module in the classpath of the containers, and add conf files (hive_site.xml, etc) as resources available for the Explore twill runnable.
 */
private TwillPreparer prepareExploreContainer(TwillPreparer preparer){
  try {
    Set<File> jars=ExploreServiceUtils.traceExploreDependencies();
    for (    File jarFile : jars) {
      LOG.trace(""String_Node_Str"",jarFile.getName());
      preparer=preparer.withClassPaths(jarFile.getName());
    }
  }
 catch (  IOException e) {
    throw new RuntimeException(""String_Node_Str"",e);
  }
  String hiveConfFiles=System.getProperty(Constants.Explore.EXPLORE_CONF_FILES);
  LOG.debug(""String_Node_Str"",hiveConfFiles);
  if (hiveConfFiles == null) {
    throw new RuntimeException(""String_Node_Str"" + Constants.Explore.EXPLORE_CONF_FILES + ""String_Node_Str"");
  }
  Iterable<File> hiveConfFilesFiles=ExploreServiceUtils.getClassPathJarsFiles(hiveConfFiles);
  Set<String> addedFiles=Sets.newHashSet();
  for (  File file : hiveConfFilesFiles) {
    if (file.getName().matches(""String_Node_Str"") && !file.getName().equals(""String_Node_Str"")) {
      if (addedFiles.add(file.getName())) {
        LOG.debug(""String_Node_Str"",file.getAbsolutePath());
        preparer=preparer.withResources(ExploreServiceUtils.hijackConfFile(file).toURI());
      }
 else {
        LOG.warn(""String_Node_Str"",file.getAbsolutePath());
      }
    }
  }
  return preparer;
}","/** 
 * Prepare the specs of the twill application for the Explore twill runnable. Add jars needed by the Explore module in the classpath of the containers, and add conf files (hive_site.xml, etc) as resources available for the Explore twill runnable.
 */
private TwillPreparer prepareExploreContainer(TwillPreparer preparer){
  File tempDir=DirUtils.createTempDir(new File(cConf.get(Constants.CFG_LOCAL_DATA_DIR),cConf.get(Constants.AppFabric.TEMP_DIR)).getAbsoluteFile());
  Preconditions.checkState(tempDir.mkdirs(),""String_Node_Str"" + tempDir.getAbsolutePath());
  try {
    Set<File> jars=ExploreServiceUtils.traceExploreDependencies();
    for (    File jarFile : jars) {
      LOG.trace(""String_Node_Str"",jarFile.getName());
      preparer=preparer.withClassPaths(jarFile.getName());
    }
  }
 catch (  IOException e) {
    throw new RuntimeException(""String_Node_Str"",e);
  }
  String hiveConfFiles=System.getProperty(Constants.Explore.EXPLORE_CONF_FILES);
  LOG.debug(""String_Node_Str"",hiveConfFiles);
  if (hiveConfFiles == null) {
    throw new RuntimeException(""String_Node_Str"" + Constants.Explore.EXPLORE_CONF_FILES + ""String_Node_Str"");
  }
  Iterable<File> hiveConfFilesFiles=ExploreServiceUtils.getClassPathJarsFiles(hiveConfFiles);
  Set<String> addedFiles=Sets.newHashSet();
  for (  File file : hiveConfFilesFiles) {
    if (file.getName().matches(""String_Node_Str"") && !file.getName().equals(""String_Node_Str"")) {
      if (addedFiles.add(file.getName())) {
        LOG.debug(""String_Node_Str"",file.getAbsolutePath());
        preparer=preparer.withResources(ExploreServiceUtils.updateConfFileForExplore(file,tempDir).toURI());
      }
 else {
        LOG.warn(""String_Node_Str"",file.getAbsolutePath());
      }
    }
  }
  return preparer;
}"
6747,"@Override protected void startUp() throws Exception {
  LOG.info(""String_Node_Str"");
  scheduledExecutorService.scheduleWithFixedDelay(new RunRecordsCorrectorRunnable(this),2L,600L,TimeUnit.SECONDS);
}","@Override protected void startUp() throws Exception {
  LOG.info(""String_Node_Str"");
  long interval=configuration.getLong(Constants.AppFabric.PROGRAM_RUNID_CORRECTOR_INTERVAL_SECONDS);
  if (interval <= 0) {
    LOG.debug(""String_Node_Str"",interval);
    interval=180L;
  }
  scheduledExecutorService.scheduleWithFixedDelay(new RunRecordsCorrectorRunnable(this),2L,interval,TimeUnit.SECONDS);
}"
6748,"@Inject public ProgramLifecycleService(Store store,ProgramRuntimeService runtimeService){
  this.store=store;
  this.runtimeService=runtimeService;
  this.scheduledExecutorService=Executors.newScheduledThreadPool(1);
}","@Inject public ProgramLifecycleService(Store store,ProgramRuntimeService runtimeService,CConfiguration configuration){
  this.store=store;
  this.runtimeService=runtimeService;
  this.scheduledExecutorService=Executors.newScheduledThreadPool(1);
  this.configuration=configuration;
}"
6749,"/** 
 * @return the long value
 */
public long getAsLong(){
  return Long.parseLong(value);
}","/** 
 * @return the result of calling <code>Long.parseLong(toString())</code>
 */
public long getAsLong(){
  return Long.parseLong(value);
}"
6750,"/** 
 * @return the boolean value
 */
public boolean getAsBoolean(){
  return Boolean.parseBoolean(value);
}","/** 
 * @return the result of calling <code>Boolean.parseBoolean(toString())</code>
 */
public boolean getAsBoolean(){
  return Boolean.parseBoolean(value);
}"
6751,"/** 
 * @return the int value
 */
public int getAsInt(){
  return Integer.parseInt(value);
}","/** 
 * @return the result of calling <code>Integer.parseInt(toString())</code>
 */
public int getAsInt(){
  return Integer.parseInt(value);
}"
6752,"private BasicWorkflowToken(BasicWorkflowToken other){
  for (  Map.Entry<Scope,Map<String,List<NodeValue>>> entry : other.tokenValueMap.entrySet()) {
    Map<String,List<NodeValue>> tokenValueMapForScope=new HashMap<>();
    for (    Map.Entry<String,List<NodeValue>> valueEntry : entry.getValue().entrySet()) {
      tokenValueMapForScope.put(valueEntry.getKey(),Lists.newArrayList(valueEntry.getValue()));
    }
    this.tokenValueMap.put(entry.getKey(),tokenValueMapForScope);
  }
  this.nodeName=other.nodeName;
  if (other.mapReduceCounters != null) {
    this.mapReduceCounters=copyHadoopCounters(other.mapReduceCounters);
  }
}","private BasicWorkflowToken(BasicWorkflowToken other){
  for (  Map.Entry<Scope,Map<String,List<NodeValue>>> entry : other.tokenValueMap.entrySet()) {
    Map<String,List<NodeValue>> tokenValueMapForScope=new HashMap<>();
    for (    Map.Entry<String,List<NodeValue>> valueEntry : entry.getValue().entrySet()) {
      tokenValueMapForScope.put(valueEntry.getKey(),Lists.newArrayList(valueEntry.getValue()));
    }
    this.tokenValueMap.put(entry.getKey(),tokenValueMapForScope);
  }
  this.nodeName=other.nodeName;
  if (other.mapReduceCounters != null) {
    this.mapReduceCounters=copyHadoopCounters(other.mapReduceCounters);
  }
  this.maxSizeBytes=other.maxSizeBytes;
  this.bytesLeft=other.bytesLeft;
}"
6753,"/** 
 * Merge the other WorkflowToken passed to the method as a parameter with the WorkflowToken on which the method is invoked.
 * @param other the other WorkflowToken to be merged
 */
void mergeToken(BasicWorkflowToken other){
  for (  Map.Entry<Scope,Map<String,List<NodeValue>>> entry : other.tokenValueMap.entrySet()) {
    Map<String,List<NodeValue>> thisTokenValueMapForScope=this.tokenValueMap.get(entry.getKey());
    for (    Map.Entry<String,List<NodeValue>> otherTokenValueMapForScopeEntry : entry.getValue().entrySet()) {
      if (!thisTokenValueMapForScope.containsKey(otherTokenValueMapForScopeEntry.getKey())) {
        thisTokenValueMapForScope.put(otherTokenValueMapForScopeEntry.getKey(),Lists.<NodeValue>newArrayList());
      }
      for (      NodeValue otherNodeValue : otherTokenValueMapForScopeEntry.getValue()) {
        boolean otherNodeValueExist=false;
        for (        NodeValue thisNodeValue : thisTokenValueMapForScope.get(otherTokenValueMapForScopeEntry.getKey())) {
          if (thisNodeValue.equals(otherNodeValue)) {
            otherNodeValueExist=true;
            break;
          }
        }
        if (!otherNodeValueExist) {
          thisTokenValueMapForScope.get(otherTokenValueMapForScopeEntry.getKey()).add(otherNodeValue);
        }
      }
    }
  }
  if (other.getMapReduceCounters() != null) {
    setMapReduceCounters(other.getMapReduceCounters());
  }
}","/** 
 * Merge the other WorkflowToken passed to the method as a parameter with the WorkflowToken on which the method is invoked.
 * @param other the other WorkflowToken to be merged
 */
void mergeToken(BasicWorkflowToken other){
  for (  Map.Entry<Scope,Map<String,List<NodeValue>>> entry : other.tokenValueMap.entrySet()) {
    Map<String,List<NodeValue>> thisTokenValueMapForScope=this.tokenValueMap.get(entry.getKey());
    for (    Map.Entry<String,List<NodeValue>> otherTokenValueMapForScopeEntry : entry.getValue().entrySet()) {
      String otherKey=otherTokenValueMapForScopeEntry.getKey();
      if (!thisTokenValueMapForScope.containsKey(otherKey)) {
        thisTokenValueMapForScope.put(otherKey,Lists.<NodeValue>newArrayList());
      }
      for (      NodeValue otherNodeValue : otherTokenValueMapForScopeEntry.getValue()) {
        boolean otherNodeValueExist=false;
        for (        NodeValue thisNodeValue : thisTokenValueMapForScope.get(otherKey)) {
          if (thisNodeValue.equals(otherNodeValue)) {
            otherNodeValueExist=true;
            break;
          }
        }
        if (!otherNodeValueExist) {
          addOrUpdate(otherKey,otherNodeValue,thisTokenValueMapForScope.get(otherKey),-1);
        }
      }
    }
  }
  if (other.getMapReduceCounters() != null) {
    setMapReduceCounters(other.getMapReduceCounters());
  }
}"
6754,"void put(String key,Value value,Scope scope){
  Preconditions.checkNotNull(key,""String_Node_Str"");
  Preconditions.checkNotNull(value,String.format(""String_Node_Str"",key));
  Preconditions.checkNotNull(value.toString(),String.format(""String_Node_Str"",key));
  Preconditions.checkState(nodeName != null,""String_Node_Str"");
  List<NodeValue> nodeValueList=tokenValueMap.get(scope).get(key);
  if (nodeValueList == null) {
    nodeValueList=Lists.newArrayList();
    tokenValueMap.get(scope).put(key,nodeValueList);
  }
  for (int i=0; i < nodeValueList.size(); i++) {
    if (nodeValueList.get(i).getNodeName().equals(nodeName)) {
      nodeValueList.set(i,new NodeValue(nodeName,value));
      return;
    }
  }
  nodeValueList.add(new NodeValue(nodeName,value));
}","void put(String key,Value value,Scope scope){
  Preconditions.checkNotNull(key,""String_Node_Str"");
  Preconditions.checkNotNull(value,String.format(""String_Node_Str"",key));
  Preconditions.checkNotNull(value.toString(),String.format(""String_Node_Str"",key));
  Preconditions.checkState(nodeName != null,""String_Node_Str"");
  List<NodeValue> nodeValueList=tokenValueMap.get(scope).get(key);
  if (nodeValueList == null) {
    nodeValueList=Lists.newArrayList();
    tokenValueMap.get(scope).put(key,nodeValueList);
  }
  NodeValue nodeValueToAddUpdate=new NodeValue(nodeName,value);
  for (int i=0; i < nodeValueList.size(); i++) {
    NodeValue existingNodeValue=nodeValueList.get(i);
    if (existingNodeValue.getNodeName().equals(nodeName)) {
      addOrUpdate(key,nodeValueToAddUpdate,nodeValueList,i);
      return;
    }
  }
  addOrUpdate(key,nodeValueToAddUpdate,nodeValueList,-1);
}"
6755,"WorkflowDriver(Program program,ProgramOptions options,InetAddress hostname,WorkflowSpecification workflowSpec,ProgramRunnerFactory programRunnerFactory,MetricsCollectionService metricsCollectionService,DatasetFramework datasetFramework,DiscoveryServiceClient discoveryServiceClient,TransactionSystemClient txClient,Store store){
  this.program=program;
  this.hostname=hostname;
  this.runtimeArgs=createRuntimeArgs(options.getUserArguments());
  this.workflowSpec=workflowSpec;
  Arguments arguments=options.getArguments();
  this.logicalStartTime=arguments.hasOption(ProgramOptionConstants.LOGICAL_START_TIME) ? Long.parseLong(arguments.getOption(ProgramOptionConstants.LOGICAL_START_TIME)) : System.currentTimeMillis();
  this.workflowProgramRunnerFactory=new ProgramWorkflowRunnerFactory(workflowSpec,programRunnerFactory,program,options);
  this.lock=new ReentrantLock();
  this.condition=lock.newCondition();
  String adapterSpec=arguments.getOption(ProgramOptionConstants.ADAPTER_SPEC);
  String adapterName=null;
  if (adapterSpec != null) {
    adapterName=GSON.fromJson(adapterSpec,AdapterDefinition.class).getName();
  }
  this.loggingContext=new WorkflowLoggingContext(program.getNamespaceId(),program.getApplicationId(),program.getName(),arguments.getOption(ProgramOptionConstants.RUN_ID),adapterName);
  this.runId=RunIds.fromString(options.getArguments().getOption(ProgramOptionConstants.RUN_ID));
  this.metricsCollectionService=metricsCollectionService;
  this.datasetFramework=datasetFramework;
  this.discoveryServiceClient=discoveryServiceClient;
  this.txClient=txClient;
  this.store=store;
  this.workflowId=Id.Workflow.from(program.getId().getApplication(),workflowSpec.getName());
}","WorkflowDriver(Program program,ProgramOptions options,InetAddress hostname,WorkflowSpecification workflowSpec,ProgramRunnerFactory programRunnerFactory,MetricsCollectionService metricsCollectionService,DatasetFramework datasetFramework,DiscoveryServiceClient discoveryServiceClient,TransactionSystemClient txClient,Store store,CConfiguration cConf){
  this.program=program;
  this.hostname=hostname;
  this.runtimeArgs=createRuntimeArgs(options.getUserArguments());
  this.workflowSpec=workflowSpec;
  Arguments arguments=options.getArguments();
  this.logicalStartTime=arguments.hasOption(ProgramOptionConstants.LOGICAL_START_TIME) ? Long.parseLong(arguments.getOption(ProgramOptionConstants.LOGICAL_START_TIME)) : System.currentTimeMillis();
  this.workflowProgramRunnerFactory=new ProgramWorkflowRunnerFactory(workflowSpec,programRunnerFactory,program,options);
  this.lock=new ReentrantLock();
  this.condition=lock.newCondition();
  String adapterSpec=arguments.getOption(ProgramOptionConstants.ADAPTER_SPEC);
  String adapterName=null;
  if (adapterSpec != null) {
    adapterName=GSON.fromJson(adapterSpec,AdapterDefinition.class).getName();
  }
  this.loggingContext=new WorkflowLoggingContext(program.getNamespaceId(),program.getApplicationId(),program.getName(),arguments.getOption(ProgramOptionConstants.RUN_ID),adapterName);
  this.runId=RunIds.fromString(options.getArguments().getOption(ProgramOptionConstants.RUN_ID));
  this.metricsCollectionService=metricsCollectionService;
  this.datasetFramework=datasetFramework;
  this.discoveryServiceClient=discoveryServiceClient;
  this.txClient=txClient;
  this.store=store;
  this.workflowId=Id.Workflow.from(program.getId().getApplication(),workflowSpec.getName());
  this.cConf=cConf;
}"
6756,"@Override protected void run() throws Exception {
  LOG.info(""String_Node_Str"",workflowSpec);
  WorkflowToken token=new BasicWorkflowToken();
  executeAll(workflowSpec.getNodes().iterator(),program.getApplicationSpecification(),new InstantiatorFactory(false),program.getClassLoader(),token);
  LOG.info(""String_Node_Str"",workflowSpec);
}","@Override protected void run() throws Exception {
  LOG.info(""String_Node_Str"",workflowSpec);
  WorkflowToken token=new BasicWorkflowToken(cConf.getInt(Constants.AppFabric.WORKFLOW_TOKEN_MAX_SIZE_MB));
  executeAll(workflowSpec.getNodes().iterator(),program.getApplicationSpecification(),new InstantiatorFactory(false),program.getClassLoader(),token);
  LOG.info(""String_Node_Str"",workflowSpec);
}"
6757,"@Inject public WorkflowProgramRunner(ProgramRunnerFactory programRunnerFactory,ServiceAnnouncer serviceAnnouncer,@Named(Constants.AppFabric.SERVER_ADDRESS) InetAddress hostname,MetricsCollectionService metricsCollectionService,DatasetFramework datasetFramework,DiscoveryServiceClient discoveryServiceClient,TransactionSystemClient txClient,Store store){
  this.programRunnerFactory=programRunnerFactory;
  this.serviceAnnouncer=serviceAnnouncer;
  this.hostname=hostname;
  this.metricsCollectionService=metricsCollectionService;
  this.datasetFramework=datasetFramework;
  this.discoveryServiceClient=discoveryServiceClient;
  this.txClient=txClient;
  this.store=store;
}","@Inject public WorkflowProgramRunner(ProgramRunnerFactory programRunnerFactory,ServiceAnnouncer serviceAnnouncer,@Named(Constants.AppFabric.SERVER_ADDRESS) InetAddress hostname,MetricsCollectionService metricsCollectionService,DatasetFramework datasetFramework,DiscoveryServiceClient discoveryServiceClient,TransactionSystemClient txClient,Store store,CConfiguration cConf){
  this.programRunnerFactory=programRunnerFactory;
  this.serviceAnnouncer=serviceAnnouncer;
  this.hostname=hostname;
  this.metricsCollectionService=metricsCollectionService;
  this.datasetFramework=datasetFramework;
  this.discoveryServiceClient=discoveryServiceClient;
  this.txClient=txClient;
  this.store=store;
  this.cConf=cConf;
}"
6758,"@Override public ProgramController run(Program program,ProgramOptions options){
  ApplicationSpecification appSpec=program.getApplicationSpecification();
  Preconditions.checkNotNull(appSpec,""String_Node_Str"");
  ProgramType processorType=program.getType();
  Preconditions.checkNotNull(processorType,""String_Node_Str"");
  Preconditions.checkArgument(processorType == ProgramType.WORKFLOW,""String_Node_Str"");
  WorkflowSpecification workflowSpec=appSpec.getWorkflows().get(program.getName());
  Preconditions.checkNotNull(workflowSpec,""String_Node_Str"",program.getName());
  WorkflowDriver driver=new WorkflowDriver(program,options,hostname,workflowSpec,programRunnerFactory,metricsCollectionService,datasetFramework,discoveryServiceClient,txClient,store);
  RunId runId=RunIds.fromString(options.getArguments().getOption(ProgramOptionConstants.RUN_ID));
  ProgramController controller=new WorkflowProgramController(program,driver,serviceAnnouncer,runId);
  driver.start();
  return controller;
}","@Override public ProgramController run(Program program,ProgramOptions options){
  ApplicationSpecification appSpec=program.getApplicationSpecification();
  Preconditions.checkNotNull(appSpec,""String_Node_Str"");
  ProgramType processorType=program.getType();
  Preconditions.checkNotNull(processorType,""String_Node_Str"");
  Preconditions.checkArgument(processorType == ProgramType.WORKFLOW,""String_Node_Str"");
  WorkflowSpecification workflowSpec=appSpec.getWorkflows().get(program.getName());
  Preconditions.checkNotNull(workflowSpec,""String_Node_Str"",program.getName());
  WorkflowDriver driver=new WorkflowDriver(program,options,hostname,workflowSpec,programRunnerFactory,metricsCollectionService,datasetFramework,discoveryServiceClient,txClient,store,cConf);
  RunId runId=RunIds.fromString(options.getArguments().getOption(ProgramOptionConstants.RUN_ID));
  ProgramController controller=new WorkflowProgramController(program,driver,serviceAnnouncer,runId);
  driver.start();
  return controller;
}"
6759,"public WorkflowToken getWorkflowToken(Id.Workflow workflowId,String workflowRunId) throws NotFoundException {
  RunRecordMeta runRecordMeta=getRun(workflowId,workflowRunId);
  if (runRecordMeta == null) {
    throw new NotFoundException(new Id.Run(workflowId,workflowRunId));
  }
  String workflowToken=runRecordMeta.getProperties().get(WORKFLOW_TOKEN_PROPERTY_KEY);
  if (workflowToken == null) {
    LOG.debug(""String_Node_Str"",workflowId,workflowRunId);
    return new BasicWorkflowToken();
  }
  return GSON.fromJson(workflowToken,BasicWorkflowToken.class);
}","public WorkflowToken getWorkflowToken(Id.Workflow workflowId,String workflowRunId) throws NotFoundException {
  RunRecordMeta runRecordMeta=getRun(workflowId,workflowRunId);
  if (runRecordMeta == null) {
    throw new NotFoundException(new Id.Run(workflowId,workflowRunId));
  }
  String workflowToken=runRecordMeta.getProperties().get(WORKFLOW_TOKEN_PROPERTY_KEY);
  if (workflowToken == null) {
    LOG.debug(""String_Node_Str"",workflowId,workflowRunId);
    return new BasicWorkflowToken(0);
  }
  return GSON.fromJson(workflowToken,BasicWorkflowToken.class);
}"
6760,"/** 
 * Shuts down a cleanup thread com.mysql.jdbc.AbandonedConnectionCleanupThread that mysql driver fails to destroy If this is not done, the thread keeps a reference to the classloader, thereby causing OOMs or too many open files
 * @param classLoader the unfiltered classloader of the jdbc driver class
 */
private static void shutDownMySQLAbandonedConnectionCleanupThread(ClassLoader classLoader){
  if (classLoader == null) {
    return;
  }
  try {
    Class<?> mysqlCleanupThreadClass=classLoader.loadClass(""String_Node_Str"");
    Method shutdownMethod=mysqlCleanupThreadClass.getMethod(""String_Node_Str"");
    shutdownMethod.invoke(null);
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable e) {
    LOG.warn(""String_Node_Str"");
  }
}","/** 
 * Shuts down a cleanup thread com.mysql.jdbc.AbandonedConnectionCleanupThread that mysql driver fails to destroy If this is not done, the thread keeps a reference to the classloader, thereby causing OOMs or too many open files
 * @param classLoader the unfiltered classloader of the jdbc driver class
 */
private static void shutDownMySQLAbandonedConnectionCleanupThread(ClassLoader classLoader){
  if (classLoader == null) {
    return;
  }
  try {
    Class<?> mysqlCleanupThreadClass;
    try {
      mysqlCleanupThreadClass=classLoader.loadClass(""String_Node_Str"");
    }
 catch (    ClassNotFoundException e) {
      LOG.trace(""String_Node_Str"" + ""String_Node_Str"",e);
      return;
    }
    Method shutdownMethod=mysqlCleanupThreadClass.getMethod(""String_Node_Str"");
    shutdownMethod.invoke(null);
    LOG.debug(""String_Node_Str"");
  }
 catch (  Throwable e) {
    LOG.warn(""String_Node_Str"",e);
  }
}"
6761,"public static File hijackConfFile(File confFile){
  if (HIVE_SITE_FILE_PATTERN.matcher(confFile.getAbsolutePath()).matches()) {
    return hijackHiveConfFile(confFile);
  }
 else   if (YARN_SITE_FILE_PATTERN.matcher(confFile.getAbsolutePath()).matches()) {
    return hijackYarnConfFile(confFile);
  }
 else {
    return confFile;
  }
}","public static File hijackConfFile(File confFile){
  if (HIVE_SITE_FILE_PATTERN.matcher(confFile.getAbsolutePath()).matches()) {
    return hijackHiveConfFile(confFile);
  }
 else   if (YARN_SITE_FILE_PATTERN.matcher(confFile.getAbsolutePath()).matches()) {
    return hijackYarnConfFile(confFile);
  }
 else   if (MAPRED_SITE_FILE_PATTERN.matcher(confFile.getAbsolutePath()).matches()) {
    return hijackMapredConfFile(confFile);
  }
 else {
    return confFile;
  }
}"
6762,"/** 
 * Check that the file is a hive-site.xml file, and return a temp copy of it to which are added necessary options. If it is not a hive-site.xml file, return it as is.
 */
private static File hijackHiveConfFile(File confFile){
  Configuration conf=new Configuration(false);
  try {
    conf.addResource(confFile.toURI().toURL());
  }
 catch (  MalformedURLException e) {
    LOG.error(""String_Node_Str"",confFile,e);
    throw Throwables.propagate(e);
  }
  conf.setBoolean(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,false);
  conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER,false);
  File newHiveConfFile=new File(Files.createTempDir(),""String_Node_Str"");
  try (FileOutputStream os=new FileOutputStream(newHiveConfFile)){
    conf.writeXml(os);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",newHiveConfFile,e);
    throw Throwables.propagate(e);
  }
  return newHiveConfFile;
}","/** 
 * Change hive-site.xml file, and return a temp copy of it to which are added necessary options.
 */
private static File hijackHiveConfFile(File confFile){
  Configuration conf=new Configuration(false);
  try {
    conf.addResource(confFile.toURI().toURL());
  }
 catch (  MalformedURLException e) {
    LOG.error(""String_Node_Str"",confFile,e);
    throw Throwables.propagate(e);
  }
  conf.setBoolean(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST,false);
  conf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER,false);
  File newHiveConfFile=new File(Files.createTempDir(),""String_Node_Str"");
  try (FileOutputStream os=new FileOutputStream(newHiveConfFile)){
    conf.writeXml(os);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",newHiveConfFile,e);
    throw Throwables.propagate(e);
  }
  return newHiveConfFile;
}"
6763,"/** 
 * Check that the file is a yarn-site.xml file, and return a temp copy of it to which are added necessary options. If it is not a yarn-site.xml file, return it as is.
 */
private static File hijackYarnConfFile(File confFile){
  Configuration conf=new Configuration(false);
  try {
    conf.addResource(confFile.toURI().toURL());
  }
 catch (  MalformedURLException e) {
    LOG.error(""String_Node_Str"",confFile,e);
    throw Throwables.propagate(e);
  }
  String yarnAppClassPath=conf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
  yarnAppClassPath=""String_Node_Str"" + yarnAppClassPath;
  conf.set(YarnConfiguration.YARN_APPLICATION_CLASSPATH,yarnAppClassPath);
  File newYarnConfFile=new File(Files.createTempDir(),""String_Node_Str"");
  try (FileOutputStream os=new FileOutputStream(newYarnConfFile)){
    conf.writeXml(os);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",newYarnConfFile,e);
    throw Throwables.propagate(e);
  }
  return newYarnConfFile;
}","/** 
 * Change yarn-site.xml file, and return a temp copy of it to which are added necessary options.
 */
private static File hijackYarnConfFile(File confFile){
  Configuration conf=new Configuration(false);
  try {
    conf.addResource(confFile.toURI().toURL());
  }
 catch (  MalformedURLException e) {
    LOG.error(""String_Node_Str"",confFile,e);
    throw Throwables.propagate(e);
  }
  String yarnAppClassPath=conf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
  yarnAppClassPath=""String_Node_Str"" + yarnAppClassPath;
  conf.set(YarnConfiguration.YARN_APPLICATION_CLASSPATH,yarnAppClassPath);
  File newYarnConfFile=new File(Files.createTempDir(),""String_Node_Str"");
  try (FileOutputStream os=new FileOutputStream(newYarnConfFile)){
    conf.writeXml(os);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",newYarnConfFile,e);
    throw Throwables.propagate(e);
  }
  return newYarnConfFile;
}"
6764,"@Test public void hijackConfFileTest() throws Exception {
  Configuration conf=new Configuration(false);
  conf.set(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(1,conf.size());
  File confFile=tmpFolder.newFile(""String_Node_Str"");
  try (FileOutputStream os=new FileOutputStream(confFile)){
    conf.writeXml(os);
  }
   File newConfFile=ExploreServiceUtils.hijackConfFile(confFile);
  conf=new Configuration(false);
  conf.addResource(newConfFile.toURI().toURL());
  Assert.assertEquals(3,conf.size());
  Assert.assertEquals(""String_Node_Str"",conf.get(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST));
  Assert.assertEquals(""String_Node_Str"",conf.get(Job.MAPREDUCE_JOB_CLASSLOADER));
  Assert.assertEquals(""String_Node_Str"",conf.get(""String_Node_Str""));
  confFile=tmpFolder.newFile(""String_Node_Str"");
  conf=new YarnConfiguration();
  try (FileOutputStream os=new FileOutputStream(confFile)){
    conf.writeXml(os);
  }
   String yarnApplicationClassPath=""String_Node_Str"" + conf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
  newConfFile=ExploreServiceUtils.hijackConfFile(confFile);
  conf=new Configuration(false);
  conf.addResource(newConfFile.toURI().toURL());
  Assert.assertEquals(yarnApplicationClassPath,conf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH));
  confFile=tmpFolder.newFile(""String_Node_Str"");
  Assert.assertEquals(confFile,ExploreServiceUtils.hijackConfFile(confFile));
}","@Test public void hijackConfFileTest() throws Exception {
  Configuration conf=new Configuration(false);
  conf.set(""String_Node_Str"",""String_Node_Str"");
  Assert.assertEquals(1,conf.size());
  File confFile=tmpFolder.newFile(""String_Node_Str"");
  try (FileOutputStream os=new FileOutputStream(confFile)){
    conf.writeXml(os);
  }
   File newConfFile=ExploreServiceUtils.hijackConfFile(confFile);
  conf=new Configuration(false);
  conf.addResource(newConfFile.toURI().toURL());
  Assert.assertEquals(3,conf.size());
  Assert.assertEquals(""String_Node_Str"",conf.get(Job.MAPREDUCE_JOB_USER_CLASSPATH_FIRST));
  Assert.assertEquals(""String_Node_Str"",conf.get(Job.MAPREDUCE_JOB_CLASSLOADER));
  Assert.assertEquals(""String_Node_Str"",conf.get(""String_Node_Str""));
  confFile=tmpFolder.newFile(""String_Node_Str"");
  conf=new YarnConfiguration();
  try (FileOutputStream os=new FileOutputStream(confFile)){
    conf.writeXml(os);
  }
   String yarnApplicationClassPath=""String_Node_Str"" + conf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH,Joiner.on(""String_Node_Str"").join(YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH));
  newConfFile=ExploreServiceUtils.hijackConfFile(confFile);
  conf=new Configuration(false);
  conf.addResource(newConfFile.toURI().toURL());
  Assert.assertEquals(yarnApplicationClassPath,conf.get(YarnConfiguration.YARN_APPLICATION_CLASSPATH));
  confFile=tmpFolder.newFile(""String_Node_Str"");
  conf=new YarnConfiguration();
  try (FileOutputStream os=new FileOutputStream(confFile)){
    conf.writeXml(os);
  }
   String mapredApplicationClassPath=""String_Node_Str"" + conf.get(MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH,MRJobConfig.DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH);
  newConfFile=ExploreServiceUtils.hijackConfFile(confFile);
  conf=new Configuration(false);
  conf.addResource(newConfFile.toURI().toURL());
  Assert.assertEquals(mapredApplicationClassPath,conf.get(MRJobConfig.MAPREDUCE_APPLICATION_CLASSPATH));
  confFile=tmpFolder.newFile(""String_Node_Str"");
  Assert.assertEquals(confFile,ExploreServiceUtils.hijackConfFile(confFile));
}"
6765,"@Override public void onChange(ServiceDiscovered serviceDiscovered){
  ResourceRequirement requirement=requirements.get(serviceDiscovered.getName());
  if (requirement != null) {
    performAssignment(requirement,ImmutableSortedSet.copyOf(DiscoverableComparator.COMPARATOR,serviceDiscovered));
  }
}","@Override public void onChange(ServiceDiscovered serviceDiscovered){
  ResourceRequirement requirement=requirements.get(serviceDiscovered.getName());
  if (requirement != null) {
    performAssignment(requirement,serviceDiscovered);
  }
}"
6766,"/** 
 * Fetch the   {@link ResourceAssignment} from ZK and then perform resource assignment logic. This is done with besteffort to let the  {@link AssignmentStrategy} has access to existing assignment. If failed to get existing{@link ResourceAssignment} or if it's simply not exists, assignment will still be triggered as if there is noexisting assignment.
 * @param requirement The resource requirement that needs to be fulfilled.
 * @param handlers The set of handlers available.
 */
private void fetchAndPerformAssignment(final ResourceRequirement requirement,final Set<Discoverable> handlers){
  final String name=requirement.getName();
  String zkPath=CoordinationConstants.ASSIGNMENTS_PATH + ""String_Node_Str"" + name;
  Futures.addCallback(zkClient.getData(zkPath),new FutureCallback<NodeData>(){
    @Override public void onSuccess(    NodeData result){
      if (assignments.get(name) != null) {
        return;
      }
      byte[] data=result.getData();
      ResourceAssignment resourceAssignment=new ResourceAssignment(name);
      try {
        if (data != null) {
          resourceAssignment=CoordinationConstants.RESOURCE_ASSIGNMENT_CODEC.decode(data);
        }
      }
 catch (      Throwable t) {
        LOG.warn(""String_Node_Str"",t);
      }
      assignments.put(name,resourceAssignment);
      performAssignment(requirement,handlers);
    }
    @Override public void onFailure(    Throwable t){
      if (!(t instanceof KeeperException.NoNodeException)) {
        LOG.warn(""String_Node_Str"",t);
      }
      assignments.put(name,new ResourceAssignment(name));
      performAssignment(requirement,handlers);
    }
  }
,executor);
}","/** 
 * Fetch the   {@link ResourceAssignment} from ZK and then perform resource assignment logic. This is done with besteffort to let the  {@link AssignmentStrategy} has access to existing assignment. If failed to get existing{@link ResourceAssignment} or if it's simply not exists, assignment will still be triggered as if there is noexisting assignment.
 * @param requirement The resource requirement that needs to be fulfilled.
 * @param serviceDiscovered The set of handlers available.
 */
private void fetchAndPerformAssignment(final ResourceRequirement requirement,final ServiceDiscovered serviceDiscovered){
  final String name=requirement.getName();
  String zkPath=CoordinationConstants.ASSIGNMENTS_PATH + ""String_Node_Str"" + name;
  Futures.addCallback(zkClient.getData(zkPath),new FutureCallback<NodeData>(){
    @Override public void onSuccess(    NodeData result){
      if (assignments.get(name) != null) {
        return;
      }
      byte[] data=result.getData();
      ResourceAssignment resourceAssignment=new ResourceAssignment(name);
      try {
        if (data != null) {
          resourceAssignment=CoordinationConstants.RESOURCE_ASSIGNMENT_CODEC.decode(data);
        }
      }
 catch (      Throwable t) {
        LOG.warn(""String_Node_Str"",t);
      }
      assignments.put(name,resourceAssignment);
      performAssignment(requirement,serviceDiscovered);
    }
    @Override public void onFailure(    Throwable t){
      if (!(t instanceof KeeperException.NoNodeException)) {
        LOG.warn(""String_Node_Str"",t);
      }
      assignments.put(name,new ResourceAssignment(name));
      performAssignment(requirement,serviceDiscovered);
    }
  }
,executor);
}"
6767,"/** 
 * Save a   {@link ResourceAssignment} to local cache as well as ZK ZK.
 * @param assignment The assignment to be persisted.
 */
private void saveAssignment(ResourceAssignment assignment){
  assignments.put(assignment.getName(),assignment);
  try {
    final byte[] data=CoordinationConstants.RESOURCE_ASSIGNMENT_CODEC.encode(assignment);
    String zkPath=CoordinationConstants.ASSIGNMENTS_PATH + ""String_Node_Str"" + assignment.getName();
    Futures.addCallback(ZKExtOperations.setOrCreate(zkClient,zkPath,data,assignment,CoordinationConstants.MAX_ZK_FAILURE_RETRY),new FutureCallback<ResourceAssignment>(){
      @Override public void onSuccess(      ResourceAssignment result){
        LOG.debug(""String_Node_Str"",result.getName(),Bytes.toString(data));
      }
      @Override public void onFailure(      Throwable t){
        LOG.error(""String_Node_Str"",Bytes.toStringBinary(data),t);
        doNotifyFailed(t);
      }
    }
,executor);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",assignment.getName(),e);
  }
}","/** 
 * Save a   {@link ResourceAssignment} to local cache as well as ZK ZK.
 * @param assignment The assignment to be persisted.
 */
private void saveAssignment(ResourceAssignment assignment){
  assignments.put(assignment.getName(),assignment);
  try {
    final byte[] data=CoordinationConstants.RESOURCE_ASSIGNMENT_CODEC.encode(assignment);
    String zkPath=CoordinationConstants.ASSIGNMENTS_PATH + ""String_Node_Str"" + assignment.getName();
    Futures.addCallback(ZKExtOperations.setOrCreate(zkClient,zkPath,data,assignment,CoordinationConstants.MAX_ZK_FAILURE_RETRY),new FutureCallback<ResourceAssignment>(){
      @Override public void onSuccess(      ResourceAssignment result){
        LOG.info(""String_Node_Str"",result.getName(),Bytes.toString(data));
      }
      @Override public void onFailure(      Throwable t){
        LOG.error(""String_Node_Str"",Bytes.toStringBinary(data),t);
        doNotifyFailed(t);
      }
    }
,executor);
  }
 catch (  Exception e) {
    LOG.error(""String_Node_Str"",assignment.getName(),e);
  }
}"
6768,"/** 
 * Gets the data from a resource node, decode it to   {@link ResourceRequirement} and performs resource assignmentif the requirement changed.
 */
private void fetchAndProcessRequirement(final String path,Watcher watcher){
  Futures.addCallback(zkClient.getData(path,watcher),wrapCallback(new FutureCallback<NodeData>(){
    @Override public void onSuccess(    NodeData result){
      byte[] nodeData=result.getData();
      if (nodeData == null) {
        LOG.warn(""String_Node_Str"",zkClient.getConnectString(),path);
        return;
      }
      try {
        ResourceRequirement requirement=CoordinationConstants.RESOURCE_REQUIREMENT_CODEC.decode(nodeData);
        LOG.info(""String_Node_Str"",requirement);
        ResourceRequirement oldRequirement=requirements.get(requirement.getName());
        if (requirement.equals(oldRequirement)) {
          LOG.info(""String_Node_Str"",requirement.getName(),oldRequirement,requirement);
          return;
        }
        requirements.put(requirement.getName(),requirement);
        CancellableServiceDiscovered discovered=serviceDiscovered.get(requirement.getName());
        if (discovered == null) {
          discovered=new CancellableServiceDiscovered(discoveryService.discover(requirement.getName()),discoverableListener,executor);
          serviceDiscovered.put(requirement.getName(),discovered);
        }
 else {
          performAssignment(requirement,ImmutableSortedSet.copyOf(DiscoverableComparator.COMPARATOR,discovered));
        }
      }
 catch (      Exception e) {
        LOG.warn(""String_Node_Str"",zkClient.getConnectString(),path,Bytes.toStringBinary(nodeData),e);
      }
    }
    @Override public void onFailure(    Throwable t){
      LOG.error(""String_Node_Str"",zkClient.getConnectString(),path,t);
    }
  }
),executor);
}","/** 
 * Gets the data from a resource node, decode it to   {@link ResourceRequirement} and performs resource assignmentif the requirement changed.
 */
private void fetchAndProcessRequirement(final String path,Watcher watcher){
  Futures.addCallback(zkClient.getData(path,watcher),wrapCallback(new FutureCallback<NodeData>(){
    @Override public void onSuccess(    NodeData result){
      byte[] nodeData=result.getData();
      if (nodeData == null) {
        LOG.warn(""String_Node_Str"",zkClient.getConnectString(),path);
        return;
      }
      try {
        ResourceRequirement requirement=CoordinationConstants.RESOURCE_REQUIREMENT_CODEC.decode(nodeData);
        LOG.info(""String_Node_Str"",requirement);
        ResourceRequirement oldRequirement=requirements.get(requirement.getName());
        if (requirement.equals(oldRequirement)) {
          LOG.info(""String_Node_Str"",requirement.getName(),oldRequirement,requirement);
          return;
        }
        requirements.put(requirement.getName(),requirement);
        CancellableServiceDiscovered discovered=serviceDiscovered.get(requirement.getName());
        if (discovered == null) {
          discovered=new CancellableServiceDiscovered(discoveryService.discover(requirement.getName()),discoverableListener,executor);
          serviceDiscovered.put(requirement.getName(),discovered);
        }
 else {
          performAssignment(requirement,discovered.serviceDiscovered);
        }
      }
 catch (      Exception e) {
        LOG.warn(""String_Node_Str"",zkClient.getConnectString(),path,Bytes.toStringBinary(nodeData),e);
      }
    }
    @Override public void onFailure(    Throwable t){
      LOG.error(""String_Node_Str"",zkClient.getConnectString(),path,t);
    }
  }
),executor);
}"
6769,"/** 
 * Performs resource assignment based on the resource requirement. This method should only be called from the single thread executor owned by this class.
 * @param requirement The resource requirement that needs to be fulfilled.
 * @param handlers The set of handlers available.
 */
private void performAssignment(ResourceRequirement requirement,Set<Discoverable> handlers){
  ResourceAssignment oldAssignment=assignments.get(requirement.getName());
  if (oldAssignment == null) {
    fetchAndPerformAssignment(requirement,handlers);
    return;
  }
  Map<String,Integer> partitions=Maps.newHashMap();
  for (  ResourceRequirement.Partition partition : requirement.getPartitions()) {
    partitions.put(partition.getName(),partition.getReplicas());
  }
  Multimap<Discoverable,PartitionReplica> assignmentMap=TreeMultimap.create(DiscoverableComparator.COMPARATOR,PartitionReplica.COMPARATOR);
  for (  Map.Entry<Discoverable,PartitionReplica> entry : oldAssignment.getAssignments().entries()) {
    Integer replicas=partitions.get(entry.getValue().getName());
    if (replicas != null && entry.getValue().getReplicaId() < replicas && handlers.contains(entry.getKey())) {
      assignmentMap.put(entry.getKey(),entry.getValue());
    }
  }
  ResourceAssigner<Discoverable> assigner=DefaultResourceAssigner.create(assignmentMap);
  if (!handlers.isEmpty() && !partitions.isEmpty()) {
    assignmentStrategy.assign(requirement,handlers,assigner);
  }
  saveAssignment(new ResourceAssignment(requirement.getName(),assigner.get()));
}","/** 
 * Performs resource assignment based on the resource requirement. This method should only be called from the single thread executor owned by this class.
 * @param requirement The resource requirement that needs to be fulfilled.
 * @param serviceDiscovered The set of handlers available.
 */
private void performAssignment(ResourceRequirement requirement,ServiceDiscovered serviceDiscovered){
  ResourceAssignment oldAssignment=assignments.get(requirement.getName());
  if (oldAssignment == null) {
    fetchAndPerformAssignment(requirement,serviceDiscovered);
    return;
  }
  Set<Discoverable> handlers=ImmutableSortedSet.copyOf(DiscoverableComparator.COMPARATOR,serviceDiscovered);
  LOG.info(""String_Node_Str"",requirement,handlers.size());
  Map<String,Integer> partitions=Maps.newHashMap();
  for (  ResourceRequirement.Partition partition : requirement.getPartitions()) {
    partitions.put(partition.getName(),partition.getReplicas());
  }
  Multimap<Discoverable,PartitionReplica> assignmentMap=TreeMultimap.create(DiscoverableComparator.COMPARATOR,PartitionReplica.COMPARATOR);
  for (  Map.Entry<Discoverable,PartitionReplica> entry : oldAssignment.getAssignments().entries()) {
    Integer replicas=partitions.get(entry.getValue().getName());
    if (replicas != null && entry.getValue().getReplicaId() < replicas && handlers.contains(entry.getKey())) {
      assignmentMap.put(entry.getKey(),entry.getValue());
    }
  }
  ResourceAssigner<Discoverable> assigner=DefaultResourceAssigner.create(assignmentMap);
  if (!handlers.isEmpty() && !partitions.isEmpty()) {
    assignmentStrategy.assign(requirement,handlers,assigner);
  }
  saveAssignment(new ResourceAssignment(requirement.getName(),assigner.get()));
}"
6770,"/** 
 * Send request to restart all instances for a CDAP system service.
 */
@Path(""String_Node_Str"") @PUT public void restartAllServiceInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String serviceName){
  super.restartAllServiceInstances(request,responder,serviceName);
}","/** 
 * Send request to restart all instances for a CDAP system service.
 */
@Path(""String_Node_Str"") @POST public void restartAllServiceInstances(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String serviceName){
  super.restartAllServiceInstances(request,responder,serviceName);
}"
6771,"/** 
 * Send request to restart single instance identified by <instance-id>
 */
@Path(""String_Node_Str"") @PUT public void restartServiceInstance(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String serviceName,@PathParam(""String_Node_Str"") int instanceId){
  super.restartServiceInstance(request,responder,serviceName,instanceId);
}","/** 
 * Send request to restart single instance identified by <instance-id>
 */
@Path(""String_Node_Str"") @POST public void restartServiceInstance(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String serviceName,@PathParam(""String_Node_Str"") int instanceId){
  super.restartServiceInstance(request,responder,serviceName,instanceId);
}"
6772,"@Test public void testInvalidIdRestartInstances() throws Exception {
  String path=String.format(""String_Node_Str"",Constants.Gateway.API_VERSION_3,Constants.Service.APP_FABRIC_HTTP);
  HttpResponse response=doPut(path);
  Assert.assertEquals(HttpResponseStatus.BAD_REQUEST.getCode(),response.getStatusLine().getStatusCode());
  path=String.format(""String_Node_Str"",Constants.Gateway.API_VERSION_3,Constants.Service.APP_FABRIC_HTTP);
  response=doGet(path);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  RestartServiceInstancesStatus result=GSON.fromJson(new String(ByteStreams.toByteArray(response.getEntity().getContent()),Charsets.UTF_8),RestartServiceInstancesStatus.class);
  Assert.assertNotNull(result);
  Assert.assertEquals(Constants.Service.APP_FABRIC_HTTP,result.getServiceName());
  Assert.assertEquals(RestartServiceInstancesStatus.RestartStatus.FAILURE,result.getStatus());
}","@Test public void testInvalidIdRestartInstances() throws Exception {
  String path=String.format(""String_Node_Str"",Constants.Gateway.API_VERSION_3,Constants.Service.APP_FABRIC_HTTP);
  HttpResponse response=doPost(path);
  Assert.assertEquals(HttpResponseStatus.BAD_REQUEST.getCode(),response.getStatusLine().getStatusCode());
  path=String.format(""String_Node_Str"",Constants.Gateway.API_VERSION_3,Constants.Service.APP_FABRIC_HTTP);
  response=doGet(path);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  RestartServiceInstancesStatus result=GSON.fromJson(new String(ByteStreams.toByteArray(response.getEntity().getContent()),Charsets.UTF_8),RestartServiceInstancesStatus.class);
  Assert.assertNotNull(result);
  Assert.assertEquals(Constants.Service.APP_FABRIC_HTTP,result.getServiceName());
  Assert.assertEquals(RestartServiceInstancesStatus.RestartStatus.FAILURE,result.getStatus());
}"
6773,"@Test public void testRestartInstances() throws Exception {
  String path=String.format(""String_Node_Str"",Constants.Service.APP_FABRIC_HTTP);
  HttpURLConnection urlConn=openURL(path,HttpMethod.PUT);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),urlConn.getResponseCode());
  urlConn.disconnect();
  urlConn=openURL(path,HttpMethod.GET);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),urlConn.getResponseCode());
  RestartServiceInstancesStatus result=GSON.fromJson(new String(ByteStreams.toByteArray(urlConn.getInputStream()),Charsets.UTF_8),RestartServiceInstancesStatus.class);
  urlConn.disconnect();
  Assert.assertNotNull(result);
  Assert.assertEquals(Constants.Service.APP_FABRIC_HTTP,result.getServiceName());
  Assert.assertEquals(RestartServiceInstancesStatus.RestartStatus.SUCCESS,result.getStatus());
}","@Test public void testRestartInstances() throws Exception {
  String path=String.format(""String_Node_Str"",Constants.Gateway.API_VERSION_3,Constants.Service.APP_FABRIC_HTTP);
  HttpResponse response=doPost(path);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  path=String.format(""String_Node_Str"",Constants.Gateway.API_VERSION_3,Constants.Service.APP_FABRIC_HTTP);
  response=doGet(path);
  Assert.assertEquals(HttpResponseStatus.OK.getCode(),response.getStatusLine().getStatusCode());
  RestartServiceInstancesStatus result=GSON.fromJson(new String(ByteStreams.toByteArray(response.getEntity().getContent()),Charsets.UTF_8),RestartServiceInstancesStatus.class);
  Assert.assertNotNull(result);
  Assert.assertEquals(Constants.Service.APP_FABRIC_HTTP,result.getServiceName());
  Assert.assertEquals(RestartServiceInstancesStatus.RestartStatus.SUCCESS,result.getStatus());
}"
6774,"private Collection<DatasetSpecificationSummary> spec2Summary(Collection<DatasetSpecification> specs){
  List<DatasetSpecificationSummary> datasetSummaries=Lists.newArrayList();
  for (  DatasetSpecification spec : specs) {
    datasetSummaries.add(new DatasetSpecificationSummary(spec.getName(),spec.getType(),spec.getProperties()));
  }
  return datasetSummaries;
}","private Collection<DatasetSpecificationSummary> spec2Summary(Collection<DatasetSpecification> specs){
  List<DatasetSpecificationSummary> datasetSummaries=Lists.newArrayList();
  for (  DatasetSpecification spec : specs) {
    if (QueueConstants.STATE_STORE_NAME.equals(spec.getName())) {
      continue;
    }
    datasetSummaries.add(new DatasetSpecificationSummary(spec.getName(),spec.getType(),spec.getProperties()));
  }
  return datasetSummaries;
}"
6775,"public static TableId getConfigTableId(String namespace){
  return TableId.from(namespace,HBaseQueueDatasetModule.STATE_STORE_NAME + ""String_Node_Str"" + HBaseQueueDatasetModule.STATE_STORE_EMBEDDED_TABLE_NAME);
}","public static TableId getConfigTableId(String namespace){
  return TableId.from(namespace,QueueConstants.STATE_STORE_NAME + ""String_Node_Str"" + HBaseQueueDatasetModule.STATE_STORE_EMBEDDED_TABLE_NAME);
}"
6776,"private Id.DatasetInstance getStateStoreId(String namespaceId){
  return Id.DatasetInstance.from(namespaceId,HBaseQueueDatasetModule.STATE_STORE_NAME);
}","private Id.DatasetInstance getStateStoreId(String namespaceId){
  return Id.DatasetInstance.from(namespaceId,QueueConstants.STATE_STORE_NAME);
}"
6777,"@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  Preconditions.checkArgument(!(dbSinkConfig.user == null && dbSinkConfig.password != null),""String_Node_Str"" + ""String_Node_Str"");
  Preconditions.checkArgument(!(dbSinkConfig.user != null && dbSinkConfig.password == null),""String_Node_Str"" + ""String_Node_Str"");
  Class<? extends Driver> jdbcDriverClass=pipelineConfigurer.usePluginClass(dbSinkConfig.jdbcPluginType,dbSinkConfig.jdbcPluginName,getJDBCPluginId(),PluginProperties.builder().build());
  Preconditions.checkArgument(jdbcDriverClass != null,""String_Node_Str"");
}","@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  Preconditions.checkArgument(!(dbSinkConfig.user == null && dbSinkConfig.password != null),""String_Node_Str"" + ""String_Node_Str"");
  Preconditions.checkArgument(!(dbSinkConfig.user != null && dbSinkConfig.password == null),""String_Node_Str"" + ""String_Node_Str"");
  Class<? extends Driver> jdbcDriverClass=pipelineConfigurer.usePluginClass(dbSinkConfig.jdbcPluginType,dbSinkConfig.jdbcPluginName,getJDBCPluginId(),PluginProperties.builder().build());
  Preconditions.checkArgument(jdbcDriverClass != null,""String_Node_Str"" + ""String_Node_Str"",dbSinkConfig.jdbcPluginName,dbSinkConfig.jdbcPluginName,dbSinkConfig.jdbcPluginType);
}"
6778,"@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  Preconditions.checkArgument(!(dbSourceConfig.user == null && dbSourceConfig.password != null),""String_Node_Str"" + ""String_Node_Str"");
  Preconditions.checkArgument(!(dbSourceConfig.user != null && dbSourceConfig.password == null),""String_Node_Str"" + ""String_Node_Str"");
  String jdbcPluginId=String.format(""String_Node_Str"",""String_Node_Str"",dbSourceConfig.jdbcPluginType,dbSourceConfig.jdbcPluginName);
  Class<? extends Driver> jdbcDriverClass=pipelineConfigurer.usePluginClass(dbSourceConfig.jdbcPluginType,dbSourceConfig.jdbcPluginName,jdbcPluginId,PluginProperties.builder().build());
  Preconditions.checkArgument(jdbcDriverClass != null,""String_Node_Str"");
}","@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  Preconditions.checkArgument(!(dbSourceConfig.user == null && dbSourceConfig.password != null),""String_Node_Str"" + ""String_Node_Str"");
  Preconditions.checkArgument(!(dbSourceConfig.user != null && dbSourceConfig.password == null),""String_Node_Str"" + ""String_Node_Str"");
  String jdbcPluginId=String.format(""String_Node_Str"",""String_Node_Str"",dbSourceConfig.jdbcPluginType,dbSourceConfig.jdbcPluginName);
  Class<? extends Driver> jdbcDriverClass=pipelineConfigurer.usePluginClass(dbSourceConfig.jdbcPluginType,dbSourceConfig.jdbcPluginName,jdbcPluginId,PluginProperties.builder().build());
  Preconditions.checkArgument(jdbcDriverClass != null,""String_Node_Str"" + ""String_Node_Str"",dbSourceConfig.jdbcPluginName,dbSourceConfig.jdbcPluginName,dbSourceConfig.jdbcPluginType);
}"
6779,"@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  Preconditions.checkArgument(!(dbSinkConfig.user == null && dbSinkConfig.password != null),""String_Node_Str"" + ""String_Node_Str"");
  Preconditions.checkArgument(!(dbSinkConfig.user != null && dbSinkConfig.password == null),""String_Node_Str"" + ""String_Node_Str"");
  Class<? extends Driver> jdbcDriverClass=pipelineConfigurer.usePluginClass(dbSinkConfig.jdbcPluginType,dbSinkConfig.jdbcPluginName,getJDBCPluginId(),PluginProperties.builder().build());
  Preconditions.checkArgument(jdbcDriverClass != null,""String_Node_Str"");
}","@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  Preconditions.checkArgument(!(dbSinkConfig.user == null && dbSinkConfig.password != null),""String_Node_Str"" + ""String_Node_Str"");
  Preconditions.checkArgument(!(dbSinkConfig.user != null && dbSinkConfig.password == null),""String_Node_Str"" + ""String_Node_Str"");
  Class<? extends Driver> jdbcDriverClass=pipelineConfigurer.usePluginClass(dbSinkConfig.jdbcPluginType,dbSinkConfig.jdbcPluginName,getJDBCPluginId(),PluginProperties.builder().build());
  Preconditions.checkArgument(jdbcDriverClass != null,""String_Node_Str"" + ""String_Node_Str"",dbSinkConfig.jdbcPluginName,dbSinkConfig.jdbcPluginName,dbSinkConfig.jdbcPluginType);
}"
6780,"@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  Preconditions.checkArgument(!(dbSourceConfig.user == null && dbSourceConfig.password != null),""String_Node_Str"" + ""String_Node_Str"");
  Preconditions.checkArgument(!(dbSourceConfig.user != null && dbSourceConfig.password == null),""String_Node_Str"" + ""String_Node_Str"");
  String jdbcPluginId=String.format(""String_Node_Str"",""String_Node_Str"",dbSourceConfig.jdbcPluginType,dbSourceConfig.jdbcPluginName);
  Class<? extends Driver> jdbcDriverClass=pipelineConfigurer.usePluginClass(dbSourceConfig.jdbcPluginType,dbSourceConfig.jdbcPluginName,jdbcPluginId,PluginProperties.builder().build());
  Preconditions.checkArgument(jdbcDriverClass != null,""String_Node_Str"");
}","@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  Preconditions.checkArgument(!(dbSourceConfig.user == null && dbSourceConfig.password != null),""String_Node_Str"" + ""String_Node_Str"");
  Preconditions.checkArgument(!(dbSourceConfig.user != null && dbSourceConfig.password == null),""String_Node_Str"" + ""String_Node_Str"");
  String jdbcPluginId=String.format(""String_Node_Str"",""String_Node_Str"",dbSourceConfig.jdbcPluginType,dbSourceConfig.jdbcPluginName);
  Class<? extends Driver> jdbcDriverClass=pipelineConfigurer.usePluginClass(dbSourceConfig.jdbcPluginType,dbSourceConfig.jdbcPluginName,jdbcPluginId,PluginProperties.builder().build());
  Preconditions.checkArgument(jdbcDriverClass != null,""String_Node_Str"" + ""String_Node_Str"",dbSourceConfig.jdbcPluginName,dbSourceConfig.jdbcPluginName,dbSourceConfig.jdbcPluginType);
}"
6781,"@BeforeClass public static void setupBeforeClass() throws Exception {
  testUtil=new HBaseTestingUtility();
  testUtil.startMiniCluster();
  conf=testUtil.getConfiguration();
  cConf=CConfiguration.create();
}","@BeforeClass public static void setupBeforeClass() throws Exception {
  testUtil=new HBase96Test();
  testUtil.startHBase();
  conf=testUtil.getConfiguration();
  cConf=CConfiguration.create();
}"
6782,"@AfterClass public static void shutdownAfterClass() throws Exception {
  testUtil.shutdownMiniCluster();
}","@AfterClass public static void shutdownAfterClass() throws Exception {
  testUtil.stopHBase();
}"
6783,"@BeforeClass public static void setupBeforeClass() throws Exception {
  testUtil=new HBaseTestingUtility();
  testUtil.startMiniCluster();
  conf=testUtil.getConfiguration();
  cConf=CConfiguration.create();
}","@BeforeClass public static void setupBeforeClass() throws Exception {
  testUtil=new HBase98Test();
  testUtil.startHBase();
  conf=testUtil.getConfiguration();
  cConf=CConfiguration.create();
}"
6784,"@AfterClass public static void shutdownAfterClass() throws Exception {
  testUtil.shutdownMiniCluster();
}","@AfterClass public static void shutdownAfterClass() throws Exception {
  testUtil.stopHBase();
}"
6785,"@BeforeClass public static void setupBeforeClass() throws Exception {
  testUtil=new HBaseTestingUtility();
  testUtil.startMiniCluster();
  conf=testUtil.getConfiguration();
  cConf=CConfiguration.create();
}","@BeforeClass public static void setupBeforeClass() throws Exception {
  testUtil=new HBase10CDHTest();
  testUtil.startHBase();
  conf=testUtil.getConfiguration();
  cConf=CConfiguration.create();
}"
6786,"@AfterClass public static void shutdownAfterClass() throws Exception {
  testUtil.shutdownMiniCluster();
}","@AfterClass public static void shutdownAfterClass() throws Exception {
  testUtil.stopHBase();
}"
6787,"@BeforeClass public static void setupBeforeClass() throws Exception {
  testUtil=new HBaseTestingUtility();
  testUtil.startMiniCluster();
  conf=testUtil.getConfiguration();
  cConf=CConfiguration.create();
}","@BeforeClass public static void setupBeforeClass() throws Exception {
  testUtil=new HBase10Test();
  testUtil.startHBase();
  conf=testUtil.getConfiguration();
  cConf=CConfiguration.create();
}"
6788,"@AfterClass public static void shutdownAfterClass() throws Exception {
  testUtil.shutdownMiniCluster();
}","@AfterClass public static void shutdownAfterClass() throws Exception {
  testUtil.stopHBase();
}"
6789,"private QueryHandle createFromSchemaProperty(DatasetSpecification spec,Id.DatasetInstance datasetID,Map<String,String> serdeProperties) throws ExploreException, SQLException, UnsupportedTypeException {
  String schemaStr=spec.getProperty(DatasetProperties.SCHEMA);
  if (schemaStr == null) {
    throw new IllegalArgumentException(String.format(""String_Node_Str"",datasetID.getId(),DatasetProperties.SCHEMA));
  }
  try {
    Schema schema=Schema.parseJson(schemaStr);
    String createStatement=new CreateStatementBuilder(datasetID.getId(),getDatasetTableName(datasetID)).setSchema(schema).setTableComment(""String_Node_Str"").buildWithStorageHandler(Constants.Explore.DATASET_STORAGE_HANDLER_CLASS,serdeProperties);
    return exploreService.execute(datasetID.getNamespace(),createStatement);
  }
 catch (  IOException e) {
    throw new IllegalArgumentException(""String_Node_Str"" + datasetID);
  }
}","private QueryHandle createFromSchemaProperty(DatasetSpecification spec,Id.DatasetInstance datasetID,Map<String,String> serdeProperties,boolean shouldErrorOnMissingSchema) throws ExploreException, SQLException, UnsupportedTypeException {
  String schemaStr=spec.getProperty(DatasetProperties.SCHEMA);
  if (schemaStr == null) {
    if (shouldErrorOnMissingSchema) {
      throw new IllegalArgumentException(String.format(""String_Node_Str"",datasetID.getId(),DatasetProperties.SCHEMA));
    }
 else {
      return QueryHandle.NO_OP;
    }
  }
  try {
    Schema schema=Schema.parseJson(schemaStr);
    String createStatement=new CreateStatementBuilder(datasetID.getId(),getDatasetTableName(datasetID)).setSchema(schema).setTableComment(""String_Node_Str"").buildWithStorageHandler(Constants.Explore.DATASET_STORAGE_HANDLER_CLASS,serdeProperties);
    return exploreService.execute(datasetID.getNamespace(),createStatement);
  }
 catch (  IOException e) {
    throw new IllegalArgumentException(""String_Node_Str"" + datasetID);
  }
}"
6790,"/** 
 * Enable ad-hoc exploration on the given dataset by creating a corresponding Hive table. If exploration has already been enabled on the dataset, this will be a no-op. Assumes the dataset actually exists.
 * @param datasetID the ID of the dataset to enable
 * @param spec the specification for the dataset to enable
 * @return query handle for creating the Hive table for the dataset
 * @throws IllegalArgumentException if some required dataset property like schema is not set
 * @throws UnsupportedTypeException if the schema of the dataset is not compatible with Hive
 * @throws ExploreException if there was an exception submitting the create table statement
 * @throws SQLException if there was a problem with the create table statement
 * @throws DatasetNotFoundException if the dataset had to be instantiated, but could not be found
 * @throws ClassNotFoundException if the was a missing class when instantiating the dataset
 */
public QueryHandle enableDataset(Id.DatasetInstance datasetID,DatasetSpecification spec) throws IllegalArgumentException, ExploreException, SQLException, UnsupportedTypeException, DatasetNotFoundException, ClassNotFoundException {
  String datasetName=datasetID.getId();
  Map<String,String> serdeProperties=ImmutableMap.of(Constants.Explore.DATASET_NAME,datasetName,Constants.Explore.DATASET_NAMESPACE,datasetID.getNamespaceId());
  String createStatement=null;
  String datasetType=spec.getType();
  if (ObjectMappedTableModule.FULL_NAME.equals(datasetType) || ObjectMappedTableModule.SHORT_NAME.equals(datasetType)) {
    return createFromSchemaProperty(spec,datasetID,serdeProperties);
  }
  try (SystemDatasetInstantiator datasetInstantiator=datasetInstantiatorFactory.create()){
    Dataset dataset=datasetInstantiator.getDataset(datasetID);
    if (dataset == null) {
      throw new DatasetNotFoundException(datasetID);
    }
    if (dataset instanceof Table) {
      return createFromSchemaProperty(spec,datasetID,serdeProperties);
    }
    boolean isRecordScannable=dataset instanceof RecordScannable;
    boolean isRecordWritable=dataset instanceof RecordWritable;
    if (isRecordScannable || isRecordWritable) {
      Type recordType=isRecordScannable ? ((RecordScannable)dataset).getRecordType() : ((RecordWritable)dataset).getRecordType();
      if (StructuredRecord.class.equals(recordType)) {
        if (isRecordWritable && !isRecordScannable) {
          throw new UnsupportedTypeException(""String_Node_Str"");
        }
        return createFromSchemaProperty(spec,datasetID,serdeProperties);
      }
      LOG.debug(""String_Node_Str"",datasetName);
      createStatement=new CreateStatementBuilder(datasetName,getDatasetTableName(datasetID)).setSchema(hiveSchemaFor(recordType)).setTableComment(""String_Node_Str"").buildWithStorageHandler(Constants.Explore.DATASET_STORAGE_HANDLER_CLASS,serdeProperties);
    }
 else     if (dataset instanceof FileSet || dataset instanceof PartitionedFileSet) {
      Map<String,String> properties=spec.getProperties();
      if (FileSetProperties.isExploreEnabled(properties)) {
        LOG.debug(""String_Node_Str"",datasetName);
        createStatement=generateFileSetCreateStatement(datasetID,dataset,properties);
      }
    }
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",datasetID,e);
    throw new ExploreException(""String_Node_Str"" + datasetID);
  }
  if (createStatement != null) {
    return exploreService.execute(datasetID.getNamespace(),createStatement);
  }
 else {
    return QueryHandle.NO_OP;
  }
}","/** 
 * Enable ad-hoc exploration on the given dataset by creating a corresponding Hive table. If exploration has already been enabled on the dataset, this will be a no-op. Assumes the dataset actually exists.
 * @param datasetID the ID of the dataset to enable
 * @param spec the specification for the dataset to enable
 * @return query handle for creating the Hive table for the dataset
 * @throws IllegalArgumentException if some required dataset property like schema is not set
 * @throws UnsupportedTypeException if the schema of the dataset is not compatible with Hive
 * @throws ExploreException if there was an exception submitting the create table statement
 * @throws SQLException if there was a problem with the create table statement
 * @throws DatasetNotFoundException if the dataset had to be instantiated, but could not be found
 * @throws ClassNotFoundException if the was a missing class when instantiating the dataset
 */
public QueryHandle enableDataset(Id.DatasetInstance datasetID,DatasetSpecification spec) throws IllegalArgumentException, ExploreException, SQLException, UnsupportedTypeException, DatasetNotFoundException, ClassNotFoundException {
  String datasetName=datasetID.getId();
  Map<String,String> serdeProperties=ImmutableMap.of(Constants.Explore.DATASET_NAME,datasetName,Constants.Explore.DATASET_NAMESPACE,datasetID.getNamespaceId());
  String createStatement=null;
  String datasetType=spec.getType();
  if (ObjectMappedTableModule.FULL_NAME.equals(datasetType) || ObjectMappedTableModule.SHORT_NAME.equals(datasetType)) {
    return createFromSchemaProperty(spec,datasetID,serdeProperties,true);
  }
  try (SystemDatasetInstantiator datasetInstantiator=datasetInstantiatorFactory.create()){
    Dataset dataset=datasetInstantiator.getDataset(datasetID);
    if (dataset == null) {
      throw new DatasetNotFoundException(datasetID);
    }
    if (dataset instanceof Table) {
      return createFromSchemaProperty(spec,datasetID,serdeProperties,false);
    }
    boolean isRecordScannable=dataset instanceof RecordScannable;
    boolean isRecordWritable=dataset instanceof RecordWritable;
    if (isRecordScannable || isRecordWritable) {
      Type recordType=isRecordScannable ? ((RecordScannable)dataset).getRecordType() : ((RecordWritable)dataset).getRecordType();
      if (StructuredRecord.class.equals(recordType)) {
        if (isRecordWritable && !isRecordScannable) {
          throw new UnsupportedTypeException(""String_Node_Str"");
        }
        return createFromSchemaProperty(spec,datasetID,serdeProperties,true);
      }
      LOG.debug(""String_Node_Str"",datasetName);
      createStatement=new CreateStatementBuilder(datasetName,getDatasetTableName(datasetID)).setSchema(hiveSchemaFor(recordType)).setTableComment(""String_Node_Str"").buildWithStorageHandler(Constants.Explore.DATASET_STORAGE_HANDLER_CLASS,serdeProperties);
    }
 else     if (dataset instanceof FileSet || dataset instanceof PartitionedFileSet) {
      Map<String,String> properties=spec.getProperties();
      if (FileSetProperties.isExploreEnabled(properties)) {
        LOG.debug(""String_Node_Str"",datasetName);
        createStatement=generateFileSetCreateStatement(datasetID,dataset,properties);
      }
    }
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",datasetID,e);
    throw new ExploreException(""String_Node_Str"" + datasetID);
  }
  if (createStatement != null) {
    return exploreService.execute(datasetID.getNamespace(),createStatement);
  }
 else {
    return QueryHandle.NO_OP;
  }
}"
6791,"@Test public void testProgramAPI() throws Exception {
  Map<String,String> propMap=Maps.newHashMap();
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  File jarFile=createAppJarFile(AppReturnsArgs.class);
  appClient.deploy(Id.Namespace.DEFAULT,jarFile);
  Id.Application app=Id.Application.from(Id.Namespace.DEFAULT,AppReturnsArgs.NAME);
  Id.Service service=Id.Service.from(app,AppReturnsArgs.SERVICE);
  try {
    client.setInstancePreferences(propMap);
    Map<String,String> setMap=Maps.newHashMap();
    setMap.put(""String_Node_Str"",""String_Node_Str"");
    programClient.setRuntimeArgs(service,setMap);
    assertEquals(setMap,programClient.getRuntimeArgs(service));
    programClient.start(service,false,ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
    assertProgramRunning(programClient,service);
    propMap.put(""String_Node_Str"",""String_Node_Str"");
    propMap.putAll(setMap);
    URL serviceURL=new URL(serviceClient.getServiceURL(service),AppReturnsArgs.ENDPOINT);
    HttpRequest request=HttpRequest.builder(HttpMethod.GET,serviceURL).build();
    HttpResponse response=HttpRequests.execute(request);
    assertEquals(HttpURLConnection.HTTP_OK,response.getResponseCode());
    assertEquals(GSON.toJson(propMap),response.getResponseBodyAsString());
    programClient.stop(service);
    assertProgramStopped(programClient,service);
    client.deleteInstancePreferences();
    programClient.start(service);
    assertProgramRunning(programClient,service);
    propMap.remove(""String_Node_Str"");
    propMap.remove(""String_Node_Str"");
    serviceURL=new URL(serviceClient.getServiceURL(service),AppReturnsArgs.ENDPOINT);
    request=HttpRequest.builder(HttpMethod.GET,serviceURL).build();
    response=HttpRequests.execute(request);
    assertEquals(HttpURLConnection.HTTP_OK,response.getResponseCode());
    assertEquals(GSON.toJson(propMap),response.getResponseBodyAsString());
    programClient.stop(service);
    assertProgramStopped(programClient,service);
    propMap.clear();
    programClient.setRuntimeArgs(service,propMap);
    programClient.start(service);
    assertProgramRunning(programClient,service);
    serviceURL=new URL(serviceClient.getServiceURL(service),AppReturnsArgs.ENDPOINT);
    request=HttpRequest.builder(HttpMethod.GET,serviceURL).build();
    response=HttpRequests.execute(request);
    assertEquals(HttpURLConnection.HTTP_OK,response.getResponseCode());
    assertEquals(GSON.toJson(propMap),response.getResponseBodyAsString());
  }
  finally {
    programClient.stop(service);
    assertProgramStopped(programClient,service);
    appClient.delete(app);
  }
}","@Test public void testProgramAPI() throws Exception {
  Map<String,String> propMap=Maps.newHashMap();
  propMap.put(""String_Node_Str"",""String_Node_Str"");
  File jarFile=createAppJarFile(AppReturnsArgs.class);
  appClient.deploy(Id.Namespace.DEFAULT,jarFile);
  Id.Application app=Id.Application.from(Id.Namespace.DEFAULT,AppReturnsArgs.NAME);
  Id.Service service=Id.Service.from(app,AppReturnsArgs.SERVICE);
  try {
    client.setInstancePreferences(propMap);
    Map<String,String> setMap=Maps.newHashMap();
    setMap.put(""String_Node_Str"",""String_Node_Str"");
    programClient.setRuntimeArgs(service,setMap);
    assertEquals(setMap,programClient.getRuntimeArgs(service));
    programClient.start(service,false,ImmutableMap.of(""String_Node_Str"",""String_Node_Str""));
    assertProgramRunning(programClient,service);
    propMap.put(""String_Node_Str"",""String_Node_Str"");
    propMap.putAll(setMap);
    URL serviceURL=new URL(serviceClient.getServiceURL(service),AppReturnsArgs.ENDPOINT);
    HttpRequest request=HttpRequest.builder(HttpMethod.GET,serviceURL).build();
    HttpResponse response=HttpRequests.execute(request);
    assertEquals(HttpURLConnection.HTTP_OK,response.getResponseCode());
    Map<String,String> responseMap=GSON.fromJson(response.getResponseBodyAsString(),STRING_MAP_TYPE);
    assertEquals(propMap,responseMap);
    programClient.stop(service);
    assertProgramStopped(programClient,service);
    client.deleteInstancePreferences();
    programClient.start(service);
    assertProgramRunning(programClient,service);
    propMap.remove(""String_Node_Str"");
    propMap.remove(""String_Node_Str"");
    serviceURL=new URL(serviceClient.getServiceURL(service),AppReturnsArgs.ENDPOINT);
    request=HttpRequest.builder(HttpMethod.GET,serviceURL).build();
    response=HttpRequests.execute(request);
    assertEquals(HttpURLConnection.HTTP_OK,response.getResponseCode());
    responseMap=GSON.fromJson(response.getResponseBodyAsString(),STRING_MAP_TYPE);
    assertEquals(propMap,responseMap);
    programClient.stop(service);
    assertProgramStopped(programClient,service);
    propMap.clear();
    programClient.setRuntimeArgs(service,propMap);
    programClient.start(service);
    assertProgramRunning(programClient,service);
    serviceURL=new URL(serviceClient.getServiceURL(service),AppReturnsArgs.ENDPOINT);
    request=HttpRequest.builder(HttpMethod.GET,serviceURL).build();
    response=HttpRequests.execute(request);
    assertEquals(HttpURLConnection.HTTP_OK,response.getResponseCode());
    responseMap=GSON.fromJson(response.getResponseBodyAsString(),STRING_MAP_TYPE);
    assertEquals(propMap,responseMap);
  }
  finally {
    programClient.stop(service);
    assertProgramStopped(programClient,service);
    appClient.delete(app);
  }
}"
6792,"/** 
 * Recursively search the datasets of a Dataset for   {@link PartitionedFileSet}s. Return the migrated dataset spec, and add the DatasetInstances of the partitions table that require data migration to the given list.
 * @return recursively migrated dsSpec
 */
DatasetSpecification recursivelyMigrateSpec(Id.Namespace namespaceId,String dsName,DatasetSpecification dsSpec,Multimap<Id.Namespace,DatasetSpecification> addTo) throws Exception {
  if (isPartitionedFileSet(dsSpec)) {
    if (alreadyUpgraded(dsSpec)) {
      LOG.info(""String_Node_Str"",dsSpec.getName());
      return dsSpec;
    }
    DatasetSpecification convertedSpec=convertSpec(dsName,dsSpec);
    addTo.put(namespaceId,convertedSpec);
    return convertedSpec;
  }
  List<DatasetSpecification> newSpecs=Lists.newArrayList();
  for (  Map.Entry<String,DatasetSpecification> entry : dsSpec.getSpecifications().entrySet()) {
    newSpecs.add(recursivelyMigrateSpec(namespaceId,entry.getKey(),entry.getValue(),addTo));
  }
  DatasetSpecification.Builder builder=DatasetSpecification.builder(dsName,dsSpec.getType());
  builder.properties(dsSpec.getProperties());
  builder.datasets(newSpecs);
  return builder.build();
}","/** 
 * Recursively search the datasets of a Dataset for   {@link PartitionedFileSet}s. Return the migrated dataset spec, and add the DatasetInstances of the partitions table that require data migration to the given list.
 * @return recursively migrated dsSpec
 */
DatasetSpecification recursivelyMigrateSpec(Id.Namespace namespaceId,String dsName,DatasetSpecification dsSpec,Multimap<Id.Namespace,DatasetSpecification> addTo) throws Exception {
  if (isPartitionedFileSet(dsSpec)) {
    if (alreadyUpgraded(dsSpec)) {
      LOG.info(""String_Node_Str"",dsSpec.getName());
      return dsSpec;
    }
    DatasetSpecification convertedSpec=convertSpec(dsName,dsSpec);
    addTo.put(namespaceId,changeName(convertedSpec,dsSpec.getName()));
    return convertedSpec;
  }
  List<DatasetSpecification> newSpecs=Lists.newArrayList();
  for (  Map.Entry<String,DatasetSpecification> entry : dsSpec.getSpecifications().entrySet()) {
    newSpecs.add(recursivelyMigrateSpec(namespaceId,entry.getKey(),entry.getValue(),addTo));
  }
  DatasetSpecification.Builder builder=DatasetSpecification.builder(dsName,dsSpec.getType());
  builder.properties(dsSpec.getProperties());
  builder.datasets(newSpecs);
  return builder.build();
}"
6793,"public void upgrade() throws Exception {
  LOG.info(""String_Node_Str"");
  upgradePartitionedFileSets();
  LOG.info(""String_Node_Str"");
}","public void upgrade() throws Exception {
  LOG.info(""String_Node_Str"");
  upgradePartitionedFileSets();
  upgradeModulesDependingOnPfs();
  LOG.info(""String_Node_Str"");
}"
6794,"@Override public void apply() throws Exception {
  MDSKey key=new MDSKey.Builder().add(DatasetInstanceMDS.INSTANCE_PREFIX).build();
  Map<MDSKey,DatasetSpecification> dsSpecs=dsInstancesMDS.listKV(key,DatasetSpecification.class);
  Multimap<Id.Namespace,DatasetSpecification> partitionDatasetsToMigrate=HashMultimap.create();
  for (  Map.Entry<MDSKey,DatasetSpecification> entry : dsSpecs.entrySet()) {
    DatasetSpecification dsSpec=entry.getValue();
    if (!needsConverting(dsSpec)) {
      continue;
    }
    DatasetSpecification migratedSpec=recursivelyMigrateSpec(extractNamespace(entry.getKey()),dsSpec.getName(),dsSpec,partitionDatasetsToMigrate);
    dsInstancesMDS.write(entry.getKey(),migratedSpec);
  }
  LOG.info(""String_Node_Str"",partitionDatasetsToMigrate);
  for (  Map.Entry<Id.Namespace,DatasetSpecification> entry : partitionDatasetsToMigrate.entries()) {
    pfsTableMigrator.upgrade(entry.getKey(),entry.getValue());
  }
}","@Override public void apply() throws Exception {
  MDSKey key=new MDSKey.Builder().add(DatasetTypeMDS.MODULES_PREFIX).build();
  Map<MDSKey,DatasetModuleMeta> dsSpecs=dsTypeMDS.listKV(key,DatasetModuleMeta.class);
  for (  Map.Entry<MDSKey,DatasetModuleMeta> entry : dsSpecs.entrySet()) {
    DatasetModuleMeta moduleMeta=entry.getValue();
    if (!needsConverting(moduleMeta)) {
      continue;
    }
    LOG.info(""String_Node_Str"",moduleMeta);
    DatasetModuleMeta migratedModuleMeta=migrateDatasetModuleMeta(moduleMeta);
    dsTypeMDS.write(entry.getKey(),migratedModuleMeta);
  }
}"
6795,"/** 
 * Recursively search the datasets of a Dataset for   {@link PartitionedFileSet}s. Return the migrated dataset spec, and add the DatasetInstances of the partitions table that require data migration to the given list.
 * @return recursively migrated dsSpec
 */
DatasetSpecification recursivelyMigrateSpec(Id.Namespace namespaceId,String dsName,DatasetSpecification dsSpec,Multimap<Id.Namespace,DatasetSpecification> addTo) throws Exception {
  if (isPartitionedFileSet(dsSpec)) {
    if (alreadyUpgraded(dsSpec)) {
      LOG.info(""String_Node_Str"",dsSpec.getName());
      return dsSpec;
    }
    DatasetSpecification convertedSpec=convertSpec(dsName,dsSpec);
    addTo.put(namespaceId,convertedSpec);
    return convertedSpec;
  }
  List<DatasetSpecification> newSpecs=Lists.newArrayList();
  for (  Map.Entry<String,DatasetSpecification> entry : dsSpec.getSpecifications().entrySet()) {
    newSpecs.add(recursivelyMigrateSpec(namespaceId,entry.getKey(),entry.getValue(),addTo));
  }
  DatasetSpecification.Builder builder=DatasetSpecification.builder(dsName,dsSpec.getType());
  builder.properties(dsSpec.getProperties());
  builder.datasets(newSpecs);
  return builder.build();
}","/** 
 * Recursively search the datasets of a Dataset for   {@link PartitionedFileSet}s. Return the migrated dataset spec, and add the DatasetInstances of the partitions table that require data migration to the given list.
 * @return recursively migrated dsSpec
 */
DatasetSpecification recursivelyMigrateSpec(Id.Namespace namespaceId,String dsName,DatasetSpecification dsSpec,Multimap<Id.Namespace,DatasetSpecification> addTo) throws Exception {
  if (isPartitionedFileSet(dsSpec)) {
    if (alreadyUpgraded(dsSpec)) {
      LOG.info(""String_Node_Str"",dsSpec.getName());
      return dsSpec;
    }
    DatasetSpecification convertedSpec=convertSpec(dsName,dsSpec);
    addTo.put(namespaceId,changeName(convertedSpec,dsSpec.getName()));
    return convertedSpec;
  }
  List<DatasetSpecification> newSpecs=Lists.newArrayList();
  for (  Map.Entry<String,DatasetSpecification> entry : dsSpec.getSpecifications().entrySet()) {
    newSpecs.add(recursivelyMigrateSpec(namespaceId,entry.getKey(),entry.getValue(),addTo));
  }
  DatasetSpecification.Builder builder=DatasetSpecification.builder(dsName,dsSpec.getType());
  builder.properties(dsSpec.getProperties());
  builder.datasets(newSpecs);
  return builder.build();
}"
6796,"/** 
 * Fetches the run record for particular run of a program.
 * @param id        program id
 * @param runid     run id of the program
 * @return          run record for the specified program and runid, null if not found
 */
@Nullable RunRecord getRun(Id.Program id,String runid);","/** 
 * Fetches the run record for particular run of a program.
 * @param id        program id
 * @param runid     run id of the program
 * @return          run record for the specified program and runid, null if not found
 */
@Nullable RunRecordMeta getRun(Id.Program id,String runid);"
6797,"/** 
 * Fetches the run records for the particular status.
 * @param status  status of the program running/completed/failed or all
 * @param filter  predicate to be passed to filter the records
 * @return        list of logged runs
 */
List<RunRecord> getRuns(ProgramRunStatus status,Predicate<RunRecord> filter);","/** 
 * Fetches the run records for the particular status.
 * @param status  status of the program running/completed/failed or all
 * @param filter  predicate to be passed to filter the records
 * @return        list of logged runs
 */
List<RunRecordMeta> getRuns(ProgramRunStatus status,Predicate<RunRecordMeta> filter);"
6798,"/** 
 * Relays job-level and task-level information about a particular MapReduce program run.
 */
@GET @Path(""String_Node_Str"") public void getMapReduceInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String mapreduceId,@PathParam(""String_Node_Str"") String runId){
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.MAPREDUCE,mapreduceId);
    Id.Run run=new Id.Run(programId,runId);
    ApplicationSpecification appSpec=store.getApplication(programId.getApplication());
    if (appSpec == null) {
      throw new NotFoundException(programId.getApplication());
    }
    if (!appSpec.getMapReduce().containsKey(mapreduceId)) {
      throw new NotFoundException(programId);
    }
    RunRecord runRecord=store.getRun(programId,runId);
    if (runRecord == null) {
      throw new NotFoundException(run);
    }
    MRJobInfo mrJobInfo=mrJobInfoFetcher.getMRJobInfo(run);
    mrJobInfo.setState(runRecord.getStatus().name());
    mrJobInfo.setStartTime(TimeUnit.SECONDS.toMillis(runRecord.getStartTs()));
    Long stopTs=runRecord.getStopTs();
    if (stopTs != null) {
      mrJobInfo.setStopTime(TimeUnit.SECONDS.toMillis(stopTs));
    }
    Gson gson=new GsonBuilder().serializeSpecialFloatingPointValues().create();
    responder.sendJson(HttpResponseStatus.OK,mrJobInfo,mrJobInfo.getClass(),gson);
  }
 catch (  NotFoundException e) {
    LOG.warn(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",runId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}","/** 
 * Relays job-level and task-level information about a particular MapReduce program run.
 */
@GET @Path(""String_Node_Str"") public void getMapReduceInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String mapreduceId,@PathParam(""String_Node_Str"") String runId){
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.MAPREDUCE,mapreduceId);
    Id.Run run=new Id.Run(programId,runId);
    ApplicationSpecification appSpec=store.getApplication(programId.getApplication());
    if (appSpec == null) {
      throw new NotFoundException(programId.getApplication());
    }
    if (!appSpec.getMapReduce().containsKey(mapreduceId)) {
      throw new NotFoundException(programId);
    }
    RunRecordMeta runRecordMeta=store.getRun(programId,runId);
    if (runRecordMeta == null) {
      throw new NotFoundException(run);
    }
    MRJobInfo mrJobInfo=mrJobInfoFetcher.getMRJobInfo(run);
    mrJobInfo.setState(runRecordMeta.getStatus().name());
    mrJobInfo.setStartTime(TimeUnit.SECONDS.toMillis(runRecordMeta.getStartTs()));
    Long stopTs=runRecordMeta.getStopTs();
    if (stopTs != null) {
      mrJobInfo.setStopTime(TimeUnit.SECONDS.toMillis(stopTs));
    }
    Gson gson=new GsonBuilder().serializeSpecialFloatingPointValues().create();
    responder.sendJson(HttpResponseStatus.OK,mrJobInfo,mrJobInfo.getClass(),gson);
  }
 catch (  NotFoundException e) {
    LOG.warn(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",runId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}"
6799,"private void getRuns(HttpResponder responder,Id.Program programId,String status,long start,long end,int limit){
  try {
    try {
      ProgramRunStatus runStatus=(status == null) ? ProgramRunStatus.ALL : ProgramRunStatus.valueOf(status.toUpperCase());
      responder.sendJson(HttpResponseStatus.OK,store.getRuns(programId,runStatus,start,end,limit));
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","private void getRuns(HttpResponder responder,Id.Program programId,String status,long start,long end,int limit){
  try {
    try {
      ProgramRunStatus runStatus=(status == null) ? ProgramRunStatus.ALL : ProgramRunStatus.valueOf(status.toUpperCase());
      List<RunRecord> records=Lists.transform(store.getRuns(programId,runStatus,start,end,limit),CONVERT_TO_RUN_RECORD);
      responder.sendJson(HttpResponseStatus.OK,records);
    }
 catch (    IllegalArgumentException e) {
      responder.sendString(HttpResponseStatus.BAD_REQUEST,""String_Node_Str"");
    }
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}"
6800,"/** 
 * Returns run record for a particular run of a program.
 */
@GET @Path(""String_Node_Str"") public void programRunRecord(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId,@PathParam(""String_Node_Str"") String runid){
  ProgramType type=ProgramType.valueOfCategoryName(programType);
  if (type == null || type == ProgramType.WEBAPP) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    return;
  }
  try {
    RunRecord runRecord=store.getRun(Id.Program.from(namespaceId,appId,type,programId),runid);
    if (runRecord != null) {
      responder.sendJson(HttpResponseStatus.OK,runRecord);
      return;
    }
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","/** 
 * Returns run record for a particular run of a program.
 */
@GET @Path(""String_Node_Str"") public void programRunRecord(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String programType,@PathParam(""String_Node_Str"") String programId,@PathParam(""String_Node_Str"") String runid){
  ProgramType type=ProgramType.valueOfCategoryName(programType);
  if (type == null || type == ProgramType.WEBAPP) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
    return;
  }
  try {
    RunRecordMeta runRecordMeta=store.getRun(Id.Program.from(namespaceId,appId,type,programId),runid);
    if (runRecordMeta != null) {
      RunRecord runRecord=CONVERT_TO_RUN_RECORD.apply(runRecordMeta);
      responder.sendJson(HttpResponseStatus.OK,runRecord);
      return;
    }
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}"
6801,"private void stopWorkerAdapter(Id.Namespace namespace,AdapterDefinition adapterSpec) throws NotFoundException, ExecutionException, InterruptedException {
  final Id.Program workerId=getProgramId(namespace,adapterSpec);
  List<RunRecord> runRecords=store.getRuns(workerId,ProgramRunStatus.RUNNING,0,Long.MAX_VALUE,Integer.MAX_VALUE,adapterSpec.getName());
  RunRecord adapterRun=Iterables.getFirst(runRecords,null);
  if (adapterRun != null) {
    RunId runId=RunIds.fromString(adapterRun.getPid());
    lifecycleService.stopProgram(workerId,runId);
  }
 else {
    LOG.warn(""String_Node_Str"",adapterSpec.getName());
  }
}","private void stopWorkerAdapter(Id.Namespace namespace,AdapterDefinition adapterSpec) throws NotFoundException, ExecutionException, InterruptedException {
  final Id.Program workerId=getProgramId(namespace,adapterSpec);
  List<RunRecordMeta> runRecords=store.getRuns(workerId,ProgramRunStatus.RUNNING,0,Long.MAX_VALUE,Integer.MAX_VALUE,adapterSpec.getName());
  RunRecordMeta adapterRun=Iterables.getFirst(runRecords,null);
  if (adapterRun != null) {
    RunId runId=RunIds.fromString(adapterRun.getPid());
    lifecycleService.stopProgram(workerId,runId);
  }
 else {
    LOG.warn(""String_Node_Str"",adapterSpec.getName());
  }
}"
6802,"/** 
 * Fetch RunRecord for a given adapter.
 * @param namespace namespace in which adapter is deployed
 * @param adapterName name of the adapter
 * @param runId run id
 * @return {@link RunRecord}
 * @throws NotFoundException if adapter is not found
 */
public RunRecord getRun(Id.Namespace namespace,String adapterName,String runId) throws NotFoundException {
  Id.Program program=getProgramId(namespace,adapterName);
  RunRecord runRecord=store.getRun(program,runId);
  if (runRecord != null && adapterName.equals(runRecord.getAdapterName())) {
    return runRecord;
  }
  return null;
}","/** 
 * Fetch RunRecord for a given adapter.
 * @param namespace namespace in which adapter is deployed
 * @param adapterName name of the adapter
 * @param runId run id
 * @return {@link RunRecord}
 * @throws NotFoundException if adapter is not found
 */
public RunRecord getRun(Id.Namespace namespace,String adapterName,String runId) throws NotFoundException {
  Id.Program program=getProgramId(namespace,adapterName);
  RunRecordMeta runRecordMeta=store.getRun(program,runId);
  if (runRecordMeta != null && adapterName.equals(runRecordMeta.getAdapterName())) {
    return CONVERT_TO_RUN_RECORD.apply(runRecordMeta);
  }
  return null;
}"
6803,"/** 
 * Fetch RunRecords for a given adapter.
 * @param namespace namespace in which adapter is deployed
 * @param adapterName name of the adapter
 * @param status {@link ProgramRunStatus} status of the program running/completed/failed or all
 * @param start fetch run history that has started after the startTime in seconds
 * @param end fetch run history that has started before the endTime in seconds
 * @param limit max number of entries to fetch for this history call
 * @return list of {@link RunRecord}
 * @throws NotFoundException if adapter is not found
 */
public List<RunRecord> getRuns(Id.Namespace namespace,String adapterName,ProgramRunStatus status,long start,long end,int limit) throws NotFoundException {
  Id.Program program=getProgramId(namespace,adapterName);
  return store.getRuns(program,status,start,end,limit,adapterName);
}","/** 
 * Fetch RunRecords for a given adapter.
 * @param namespace namespace in which adapter is deployed
 * @param adapterName name of the adapter
 * @param status {@link ProgramRunStatus} status of the program running/completed/failed or all
 * @param start fetch run history that has started after the startTime in seconds
 * @param end fetch run history that has started before the endTime in seconds
 * @param limit max number of entries to fetch for this history call
 * @return list of {@link RunRecord}
 * @throws NotFoundException if adapter is not found
 */
public List<RunRecord> getRuns(Id.Namespace namespace,String adapterName,ProgramRunStatus status,long start,long end,int limit) throws NotFoundException {
  Id.Program program=getProgramId(namespace,adapterName);
  return Lists.transform(store.getRuns(program,status,start,end,limit,adapterName),CONVERT_TO_RUN_RECORD);
}"
6804,"@Override public synchronized RuntimeInfo lookup(Id.Program programId,final RunId runId){
  RuntimeInfo runtimeInfo=super.lookup(programId,runId);
  if (runtimeInfo != null) {
    return runtimeInfo;
  }
  for (  TwillRunner.LiveInfo liveInfo : twillRunner.lookupLive()) {
    String appName=liveInfo.getApplicationName();
    Matcher matcher=APP_NAME_PATTERN.matcher(appName);
    if (!matcher.matches()) {
      continue;
    }
    ProgramType type=getType(matcher.group(1));
    Id.Program id=Id.Program.from(matcher.group(2),matcher.group(3),type,matcher.group(4));
    if (!id.equals(programId)) {
      continue;
    }
    RunRecord record=store.getRun(programId,runId.getId());
    if (record == null) {
      return null;
    }
    if (record.getTwillRunId() == null) {
      LOG.warn(""String_Node_Str"",programId,runId.getId());
      return null;
    }
    RunId twillRunIdFromRecord=org.apache.twill.internal.RunIds.fromString(record.getTwillRunId());
    for (    TwillController controller : liveInfo.getControllers()) {
      RunId twillRunId=controller.getRunId();
      if (!twillRunId.equals(twillRunIdFromRecord)) {
        continue;
      }
      runtimeInfo=createRuntimeInfo(programId,controller,runId);
      if (runtimeInfo != null) {
        updateRuntimeInfo(programId.getType(),runId,runtimeInfo);
      }
 else {
        LOG.warn(""String_Node_Str"",runId);
      }
      return runtimeInfo;
    }
  }
  return null;
}","@Override public synchronized RuntimeInfo lookup(Id.Program programId,final RunId runId){
  RuntimeInfo runtimeInfo=super.lookup(programId,runId);
  if (runtimeInfo != null) {
    return runtimeInfo;
  }
  for (  TwillRunner.LiveInfo liveInfo : twillRunner.lookupLive()) {
    String appName=liveInfo.getApplicationName();
    Matcher matcher=APP_NAME_PATTERN.matcher(appName);
    if (!matcher.matches()) {
      continue;
    }
    ProgramType type=getType(matcher.group(1));
    Id.Program id=Id.Program.from(matcher.group(2),matcher.group(3),type,matcher.group(4));
    if (!id.equals(programId)) {
      continue;
    }
    RunRecordMeta record=store.getRun(programId,runId.getId());
    if (record == null) {
      return null;
    }
    if (record.getTwillRunId() == null) {
      LOG.warn(""String_Node_Str"",programId,runId.getId());
      return null;
    }
    RunId twillRunIdFromRecord=org.apache.twill.internal.RunIds.fromString(record.getTwillRunId());
    for (    TwillController controller : liveInfo.getControllers()) {
      RunId twillRunId=controller.getRunId();
      if (!twillRunId.equals(twillRunIdFromRecord)) {
        continue;
      }
      runtimeInfo=createRuntimeInfo(programId,controller,runId);
      if (runtimeInfo != null) {
        updateRuntimeInfo(programId.getType(),runId,runtimeInfo);
      }
 else {
        LOG.warn(""String_Node_Str"",runId);
      }
      return runtimeInfo;
    }
  }
  return null;
}"
6805,"@Override public boolean apply(RunRecord record){
  return record.getTwillRunId() != null && twillRunIds.contains(org.apache.twill.internal.RunIds.fromString(record.getTwillRunId()));
}","@Override public boolean apply(RunRecordMeta record){
  return record.getTwillRunId() != null && twillRunIds.contains(org.apache.twill.internal.RunIds.fromString(record.getTwillRunId()));
}"
6806,"@Override public synchronized Map<RunId,RuntimeInfo> list(ProgramType type){
  Map<RunId,RuntimeInfo> result=Maps.newHashMap();
  result.putAll(super.list(type));
  Table<Id.Program,RunId,TwillController> twillProgramInfo=HashBasedTable.create();
  for (  TwillRunner.LiveInfo liveInfo : twillRunner.lookupLive()) {
    String appName=liveInfo.getApplicationName();
    Matcher matcher=APP_NAME_PATTERN.matcher(appName);
    if (!matcher.matches()) {
      continue;
    }
    if (!type.equals(getType(matcher.group(1)))) {
      continue;
    }
    for (    TwillController controller : liveInfo.getControllers()) {
      RunId twillRunId=controller.getRunId();
      if (isTwillRunIdCached(twillRunId)) {
        continue;
      }
      Id.Program programId=Id.Program.from(matcher.group(2),matcher.group(3),type,matcher.group(4));
      twillProgramInfo.put(programId,twillRunId,controller);
    }
  }
  if (twillProgramInfo.isEmpty()) {
    return ImmutableMap.copyOf(result);
  }
  final Set<RunId> twillRunIds=twillProgramInfo.columnKeySet();
  List<RunRecord> activeRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    RunRecord record){
      return record.getTwillRunId() != null && twillRunIds.contains(org.apache.twill.internal.RunIds.fromString(record.getTwillRunId()));
    }
  }
);
  for (  RunRecord record : activeRunRecords) {
    RunId twillRunIdFromRecord=org.apache.twill.internal.RunIds.fromString(record.getTwillRunId());
    RunId runId=RunIds.fromString(record.getPid());
    Map<Id.Program,TwillController> mapForTwillId=twillProgramInfo.columnMap().get(twillRunIdFromRecord);
    Map.Entry<Id.Program,TwillController> entry=mapForTwillId.entrySet().iterator().next();
    RuntimeInfo runtimeInfo=createRuntimeInfo(entry.getKey(),entry.getValue(),runId);
    if (runtimeInfo != null) {
      result.put(runId,runtimeInfo);
      updateRuntimeInfo(type,runId,runtimeInfo);
    }
 else {
      LOG.warn(""String_Node_Str"",type,entry.getKey());
    }
  }
  return ImmutableMap.copyOf(result);
}","@Override public synchronized Map<RunId,RuntimeInfo> list(ProgramType type){
  Map<RunId,RuntimeInfo> result=Maps.newHashMap();
  result.putAll(super.list(type));
  Table<Id.Program,RunId,TwillController> twillProgramInfo=HashBasedTable.create();
  for (  TwillRunner.LiveInfo liveInfo : twillRunner.lookupLive()) {
    String appName=liveInfo.getApplicationName();
    Matcher matcher=APP_NAME_PATTERN.matcher(appName);
    if (!matcher.matches()) {
      continue;
    }
    if (!type.equals(getType(matcher.group(1)))) {
      continue;
    }
    for (    TwillController controller : liveInfo.getControllers()) {
      RunId twillRunId=controller.getRunId();
      if (isTwillRunIdCached(twillRunId)) {
        continue;
      }
      Id.Program programId=Id.Program.from(matcher.group(2),matcher.group(3),type,matcher.group(4));
      twillProgramInfo.put(programId,twillRunId,controller);
    }
  }
  if (twillProgramInfo.isEmpty()) {
    return ImmutableMap.copyOf(result);
  }
  final Set<RunId> twillRunIds=twillProgramInfo.columnKeySet();
  List<RunRecordMeta> activeRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecordMeta>(){
    @Override public boolean apply(    RunRecordMeta record){
      return record.getTwillRunId() != null && twillRunIds.contains(org.apache.twill.internal.RunIds.fromString(record.getTwillRunId()));
    }
  }
);
  for (  RunRecordMeta record : activeRunRecords) {
    RunId twillRunIdFromRecord=org.apache.twill.internal.RunIds.fromString(record.getTwillRunId());
    RunId runId=RunIds.fromString(record.getPid());
    Map<Id.Program,TwillController> mapForTwillId=twillProgramInfo.columnMap().get(twillRunIdFromRecord);
    Map.Entry<Id.Program,TwillController> entry=mapForTwillId.entrySet().iterator().next();
    RuntimeInfo runtimeInfo=createRuntimeInfo(entry.getKey(),entry.getValue(),runId);
    if (runtimeInfo != null) {
      result.put(runId,runtimeInfo);
      updateRuntimeInfo(type,runId,runtimeInfo);
    }
 else {
      LOG.warn(""String_Node_Str"",type,entry.getKey());
    }
  }
  return ImmutableMap.copyOf(result);
}"
6807,"/** 
 * Helper method to get program id for a run record if it exists in the store.
 * @return instance of {@link Id.Program} if exist for the runId or null if does not.
 */
@Nullable private Id.Program validateProgramForRunRecord(String namespaceName,String appName,ProgramType programType,String programName,String runId){
  Id.Program programId=Id.Program.from(namespaceName,appName,programType,programName);
  RunRecord runRecord=store.getRun(programId,runId);
  if (runRecord != null) {
    return programId;
  }
 else {
    return null;
  }
}","/** 
 * Helper method to get program id for a run record if it exists in the store.
 * @return instance of {@link Id.Program} if exist for the runId or null if does not.
 */
@Nullable private Id.Program validateProgramForRunRecord(String namespaceName,String appName,ProgramType programType,String programName,String runId){
  Id.Program programId=Id.Program.from(namespaceName,appName,programType,programName);
  RunRecordMeta runRecord=store.getRun(programId,runId);
  if (runRecord != null) {
    return programId;
  }
 else {
    return null;
  }
}"
6808,"@Override public boolean apply(@Nullable RunRecord input){
  if (input == null) {
    return false;
  }
  String runId=input.getPid();
  return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
}","@Override public boolean apply(@Nullable RunRecordMeta input){
  if (input == null) {
    return false;
  }
  String runId=input.getPid();
  return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
}"
6809,"/** 
 * Helper method to check if the run record is a child program of a Workflow
 * @param runRecord The target {@link RunRecord} to check
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 * @return {@code true} of we should check and {@code false} otherwise
 */
private boolean shouldCorrectForWorkflowChildren(RunRecord runRecord,Set<String> processedInvalidRunRecordIds){
  if (runRecord.getProperties() != null && runRecord.getProperties().get(""String_Node_Str"") != null) {
    String workflowRunId=runRecord.getProperties().get(""String_Node_Str"");
    if (!processedInvalidRunRecordIds.contains(workflowRunId)) {
      Id.Program workflowProgramId=retrieveProgramIdForRunRecord(ProgramType.WORKFLOW,workflowRunId);
      if (workflowProgramId != null) {
        RunRecord wfRunRecord=store.getRun(workflowProgramId,workflowRunId);
        RuntimeInfo wfRuntimeInfo=runtimeService.lookup(workflowProgramId,RunIds.fromString(workflowRunId));
        if (wfRunRecord != null && wfRunRecord.getStatus() == ProgramRunStatus.RUNNING && wfRuntimeInfo != null) {
          return false;
        }
      }
    }
  }
  return true;
}","/** 
 * Helper method to check if the run record is a child program of a Workflow
 * @param runRecordMeta The target {@link RunRecordMeta} to check
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 * @return {@code true} of we should check and {@code false} otherwise
 */
private boolean shouldCorrectForWorkflowChildren(RunRecordMeta runRecordMeta,Set<String> processedInvalidRunRecordIds){
  if (runRecordMeta.getProperties() != null && runRecordMeta.getProperties().get(""String_Node_Str"") != null) {
    String workflowRunId=runRecordMeta.getProperties().get(""String_Node_Str"");
    if (!processedInvalidRunRecordIds.contains(workflowRunId)) {
      Id.Program workflowProgramId=retrieveProgramIdForRunRecord(ProgramType.WORKFLOW,workflowRunId);
      if (workflowProgramId != null) {
        RunRecordMeta wfRunRecord=store.getRun(workflowProgramId,workflowRunId);
        RuntimeInfo wfRuntimeInfo=runtimeService.lookup(workflowProgramId,RunIds.fromString(workflowRunId));
        if (wfRunRecord != null && wfRunRecord.getStatus() == ProgramRunStatus.RUNNING && wfRuntimeInfo != null) {
          return false;
        }
      }
    }
  }
  return true;
}"
6810,"/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @param programType The type of program the run records need to validate and update.
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 */
void validateAndCorrectRunningRunRecords(ProgramType programType,Set<String> processedInvalidRunRecordIds){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecord> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    @Nullable RunRecord input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"",invalidRunRecords.size());
  }
  for (  RunRecord rr : invalidRunRecords) {
    String runId=rr.getPid();
    Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
    if (targetProgramId == null) {
      continue;
    }
    boolean shouldCorrect=shouldCorrectForWorkflowChildren(rr,processedInvalidRunRecordIds);
    if (!shouldCorrect) {
      continue;
    }
    LOG.warn(""String_Node_Str"",runId,targetProgramId,programType.getPrettyName());
    store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
    processedInvalidRunRecordIds.add(runId);
  }
}","/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @param programType The type of program the run records need to validate and update.
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 */
void validateAndCorrectRunningRunRecords(ProgramType programType,Set<String> processedInvalidRunRecordIds){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecordMeta> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecordMeta>(){
    @Override public boolean apply(    @Nullable RunRecordMeta input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"",invalidRunRecords.size());
  }
  for (  RunRecordMeta invalidRunRecordMeta : invalidRunRecords) {
    String runId=invalidRunRecordMeta.getPid();
    Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
    if (targetProgramId == null) {
      continue;
    }
    boolean shouldCorrect=shouldCorrectForWorkflowChildren(invalidRunRecordMeta,processedInvalidRunRecordIds);
    if (!shouldCorrect) {
      continue;
    }
    LOG.warn(""String_Node_Str"",runId,targetProgramId,programType.getPrettyName());
    store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
    processedInvalidRunRecordIds.add(runId);
  }
}"
6811,"@Override public void process(ApplicationWithPrograms input) throws Exception {
  store.addApplication(input.getId(),input.getSpecification(),input.getLocation());
  ApplicationSpecification app=input.getSpecification();
  Id.Application appId=input.getId();
  Id.Namespace namespace=appId.getNamespace();
  for (  FlowSpecification flow : app.getFlows().values()) {
    Id.Flow programId=Id.Flow.from(appId,flow.getName());
    for (    FlowletConnection connection : flow.getConnections()) {
      if (connection.getSourceType().equals(FlowletConnection.Type.STREAM)) {
        usageRegistry.register(programId,Id.Stream.from(namespace,connection.getSourceName()));
      }
    }
    for (    FlowletDefinition flowlet : flow.getFlowlets().values()) {
      for (      String dataset : flowlet.getDatasets()) {
        usageRegistry.register(programId,Id.DatasetInstance.from(namespace,dataset));
      }
    }
  }
  for (  MapReduceSpecification program : app.getMapReduce().values()) {
    Id.Program programId=Id.Program.from(appId,ProgramType.MAPREDUCE,program.getName());
    for (    String dataset : program.getDataSets()) {
      if (!dataset.startsWith(Constants.Stream.URL_PREFIX)) {
        usageRegistry.register(programId,Id.DatasetInstance.from(namespace,dataset));
      }
    }
    String inputDatasetName=program.getInputDataSet();
    if (inputDatasetName != null && inputDatasetName.startsWith(Constants.Stream.URL_PREFIX)) {
      StreamBatchReadable stream=new StreamBatchReadable(URI.create(inputDatasetName));
      usageRegistry.register(programId,Id.Stream.from(namespace,stream.getStreamName()));
    }
  }
  emit(input);
}","@Override public void process(ApplicationWithPrograms input) throws Exception {
  store.addApplication(input.getId(),input.getSpecification(),input.getLocation());
  registerDatasets(input);
  emit(input);
}"
6812,"@Override public List<ProgramSpecification> getDeletedProgramSpecifications(final Id.Application id,ApplicationSpecification appSpec){
  ApplicationMeta existing=txnl.executeUnchecked(new TransactionExecutor.Function<AppMds,ApplicationMeta>(){
    @Override public ApplicationMeta apply(    AppMds mds) throws Exception {
      return mds.apps.getApplication(id.getNamespaceId(),id.getId());
    }
  }
);
  List<ProgramSpecification> deletedProgramSpecs=Lists.newArrayList();
  if (existing != null) {
    ApplicationSpecification existingAppSpec=existing.getSpec();
    ImmutableMap<String,ProgramSpecification> existingSpec=new ImmutableMap.Builder<String,ProgramSpecification>().putAll(existingAppSpec.getMapReduce()).putAll(existingAppSpec.getSpark()).putAll(existingAppSpec.getWorkflows()).putAll(existingAppSpec.getFlows()).putAll(existingAppSpec.getServices()).build();
    ImmutableMap<String,ProgramSpecification> newSpec=new ImmutableMap.Builder<String,ProgramSpecification>().putAll(appSpec.getMapReduce()).putAll(existingAppSpec.getSpark()).putAll(appSpec.getWorkflows()).putAll(appSpec.getFlows()).putAll(appSpec.getServices()).build();
    MapDifference<String,ProgramSpecification> mapDiff=Maps.difference(existingSpec,newSpec);
    deletedProgramSpecs.addAll(mapDiff.entriesOnlyOnLeft().values());
  }
  return deletedProgramSpecs;
}","@Override public List<ProgramSpecification> getDeletedProgramSpecifications(final Id.Application id,ApplicationSpecification appSpec){
  ApplicationMeta existing=txnl.executeUnchecked(new TransactionExecutor.Function<AppMds,ApplicationMeta>(){
    @Override public ApplicationMeta apply(    AppMds mds) throws Exception {
      return mds.apps.getApplication(id.getNamespaceId(),id.getId());
    }
  }
);
  List<ProgramSpecification> deletedProgramSpecs=Lists.newArrayList();
  if (existing != null) {
    ApplicationSpecification existingAppSpec=existing.getSpec();
    ImmutableMap<String,ProgramSpecification> existingSpec=new ImmutableMap.Builder<String,ProgramSpecification>().putAll(existingAppSpec.getMapReduce()).putAll(existingAppSpec.getSpark()).putAll(existingAppSpec.getWorkflows()).putAll(existingAppSpec.getFlows()).putAll(existingAppSpec.getServices()).putAll(existingAppSpec.getWorkers()).build();
    ImmutableMap<String,ProgramSpecification> newSpec=new ImmutableMap.Builder<String,ProgramSpecification>().putAll(appSpec.getMapReduce()).putAll(appSpec.getSpark()).putAll(appSpec.getWorkflows()).putAll(appSpec.getFlows()).putAll(appSpec.getServices()).putAll(appSpec.getWorkers()).build();
    MapDifference<String,ProgramSpecification> mapDiff=Maps.difference(existingSpec,newSpec);
    deletedProgramSpecs.addAll(mapDiff.entriesOnlyOnLeft().values());
  }
  return deletedProgramSpecs;
}"
6813,"@Override public void configure(){
  setName(NAME);
}","@Override protected void configure(){
  useDatasets(DATASET_NAME);
}"
6814,"@Override public void configure(){
  setName(""String_Node_Str"");
  setDescription(""String_Node_Str"");
  addStream(new Stream(""String_Node_Str""));
  createDataset(""String_Node_Str"",KeyValueTable.class);
  addFlow(new AllProgramsApp.NoOpFlow());
  addMapReduce(new AllProgramsApp.NoOpMR());
}","@Override public void configure(){
  setName(""String_Node_Str"");
  setDescription(""String_Node_Str"");
  addStream(new Stream(""String_Node_Str""));
  createDataset(""String_Node_Str"",KeyValueTable.class);
  addFlow(new AllProgramsApp.NoOpFlow());
  addMapReduce(new AllProgramsApp.NoOpMR());
  addService(new AllProgramsApp.NoOpService());
  addWorker(new AllProgramsApp.NoOpWorker());
  addSpark(new AllProgramsApp.NoOpSpark());
}"
6815,"@Test public void testWorkerUsage() throws Exception {
  final Id.Application app=Id.Application.from(""String_Node_Str"",AllProgramsApp.NAME);
  final Id.Program program=Id.Program.from(app,ProgramType.WORKER,AllProgramsApp.NoOpWorker.NAME);
  final Id.Stream stream=Id.Stream.from(""String_Node_Str"",AllProgramsApp.STREAM_NAME);
  final Id.DatasetInstance dataset=Id.DatasetInstance.from(""String_Node_Str"",AllProgramsApp.DATASET_NAME);
  Assert.assertEquals(0,getAppStreamUsage(app).size());
  Assert.assertEquals(0,getProgramStreamUsage(program).size());
  Assert.assertEquals(0,getStreamProgramUsage(stream).size());
  Assert.assertEquals(0,getAppDatasetUsage(app).size());
  Assert.assertEquals(0,getDatasetProgramUsage(dataset).size());
  deploy(AllProgramsApp.class);
  try {
    startProgram(program);
    waitState(program,""String_Node_Str"");
    stopProgram(program);
    waitState(program,""String_Node_Str"");
    Assert.assertTrue(getAppStreamUsage(app).contains(stream));
    Assert.assertTrue(getProgramStreamUsage(program).contains(stream));
    Assert.assertTrue(getStreamProgramUsage(stream).contains(program));
    Assert.assertTrue(getProgramDatasetUsage(program).contains(dataset));
    Assert.assertTrue(getProgramDatasetUsage(program).contains(dataset));
    Assert.assertTrue(getAppDatasetUsage(app).contains(dataset));
    Assert.assertTrue(getDatasetProgramUsage(dataset).contains(program));
  }
  finally {
    deleteApp(app,200);
    Assert.assertEquals(0,getAppStreamUsage(app).size());
    Assert.assertEquals(0,getProgramStreamUsage(program).size());
    Assert.assertEquals(0,getStreamProgramUsage(stream).size());
    Assert.assertEquals(0,getAppDatasetUsage(app).size());
    Assert.assertEquals(0,getDatasetProgramUsage(dataset).size());
  }
}","@Test public void testWorkerUsage() throws Exception {
  final Id.Application app=Id.Application.from(""String_Node_Str"",AllProgramsApp.NAME);
  final Id.Program program=Id.Program.from(app,ProgramType.WORKER,AllProgramsApp.NoOpWorker.NAME);
  final Id.Stream stream=Id.Stream.from(""String_Node_Str"",AllProgramsApp.STREAM_NAME);
  final Id.DatasetInstance dataset=Id.DatasetInstance.from(""String_Node_Str"",AllProgramsApp.DATASET_NAME);
  Assert.assertEquals(0,getAppStreamUsage(app).size());
  Assert.assertEquals(0,getProgramStreamUsage(program).size());
  Assert.assertEquals(0,getStreamProgramUsage(stream).size());
  Assert.assertEquals(0,getAppDatasetUsage(app).size());
  Assert.assertEquals(0,getDatasetProgramUsage(dataset).size());
  deploy(AllProgramsApp.class);
  try {
    startProgram(program);
    waitState(program,""String_Node_Str"");
    stopProgram(program);
    waitState(program,""String_Node_Str"");
    Assert.assertTrue(getAppStreamUsage(app).contains(stream));
    Assert.assertTrue(getProgramStreamUsage(program).contains(stream));
    Assert.assertTrue(getStreamProgramUsage(stream).contains(program));
    Assert.assertTrue(getProgramDatasetUsage(program).contains(dataset));
    Assert.assertTrue(getAppDatasetUsage(app).contains(dataset));
    Assert.assertTrue(getDatasetProgramUsage(dataset).contains(program));
  }
  finally {
    deleteApp(app,200);
    Assert.assertEquals(0,getAppStreamUsage(app).size());
    Assert.assertEquals(0,getProgramStreamUsage(program).size());
    Assert.assertEquals(0,getStreamProgramUsage(stream).size());
    Assert.assertEquals(0,getAppDatasetUsage(app).size());
    Assert.assertEquals(0,getDatasetProgramUsage(dataset).size());
  }
}"
6816,"@Test public void testSparkUsage() throws Exception {
  final Id.Application app=Id.Application.from(""String_Node_Str"",AllProgramsApp.NAME);
  final Id.Program program=Id.Program.from(app,ProgramType.SPARK,AllProgramsApp.NoOpSpark.NAME);
  final Id.Stream stream=Id.Stream.from(""String_Node_Str"",AllProgramsApp.STREAM_NAME);
  final Id.DatasetInstance dataset=Id.DatasetInstance.from(""String_Node_Str"",AllProgramsApp.DATASET_NAME);
  Assert.assertEquals(0,getAppStreamUsage(app).size());
  Assert.assertEquals(0,getProgramStreamUsage(program).size());
  Assert.assertEquals(0,getStreamProgramUsage(stream).size());
  Assert.assertEquals(0,getAppDatasetUsage(app).size());
  Assert.assertEquals(0,getDatasetProgramUsage(dataset).size());
  deploy(AllProgramsApp.class);
  try {
    startProgram(program);
    waitState(program,""String_Node_Str"");
    Assert.assertTrue(getAppStreamUsage(app).contains(stream));
    Assert.assertTrue(getProgramStreamUsage(program).contains(stream));
    Assert.assertTrue(getStreamProgramUsage(stream).contains(program));
    Assert.assertTrue(getProgramDatasetUsage(program).contains(dataset));
    Assert.assertTrue(getProgramDatasetUsage(program).contains(dataset));
    Assert.assertTrue(getAppDatasetUsage(app).contains(dataset));
    Assert.assertTrue(getDatasetProgramUsage(dataset).contains(program));
  }
  finally {
    deleteApp(app,200);
    Assert.assertEquals(0,getAppStreamUsage(app).size());
    Assert.assertEquals(0,getProgramStreamUsage(program).size());
    Assert.assertEquals(0,getStreamProgramUsage(stream).size());
    Assert.assertEquals(0,getAppDatasetUsage(app).size());
    Assert.assertEquals(0,getDatasetProgramUsage(dataset).size());
  }
}","@Test public void testSparkUsage() throws Exception {
  final Id.Application app=Id.Application.from(""String_Node_Str"",AllProgramsApp.NAME);
  final Id.Program program=Id.Program.from(app,ProgramType.SPARK,AllProgramsApp.NoOpSpark.NAME);
  final Id.Stream stream=Id.Stream.from(""String_Node_Str"",AllProgramsApp.STREAM_NAME);
  final Id.DatasetInstance dataset=Id.DatasetInstance.from(""String_Node_Str"",AllProgramsApp.DATASET_NAME);
  Assert.assertEquals(0,getAppStreamUsage(app).size());
  Assert.assertEquals(0,getProgramStreamUsage(program).size());
  Assert.assertEquals(0,getStreamProgramUsage(stream).size());
  Assert.assertEquals(0,getAppDatasetUsage(app).size());
  Assert.assertEquals(0,getDatasetProgramUsage(dataset).size());
  deploy(AllProgramsApp.class);
  try {
    startProgram(program);
    waitState(program,""String_Node_Str"");
    Assert.assertTrue(getAppStreamUsage(app).contains(stream));
    Assert.assertTrue(getProgramStreamUsage(program).contains(stream));
    Assert.assertTrue(getStreamProgramUsage(stream).contains(program));
    Assert.assertTrue(getProgramDatasetUsage(program).contains(dataset));
    Assert.assertTrue(getAppDatasetUsage(app).contains(dataset));
    Assert.assertTrue(getDatasetProgramUsage(dataset).contains(program));
  }
  finally {
    deleteApp(app,200);
    Assert.assertEquals(0,getAppStreamUsage(app).size());
    Assert.assertEquals(0,getProgramStreamUsage(program).size());
    Assert.assertEquals(0,getStreamProgramUsage(stream).size());
    Assert.assertEquals(0,getAppDatasetUsage(app).size());
    Assert.assertEquals(0,getDatasetProgramUsage(dataset).size());
  }
}"
6817,"@Test public void testFlowUsage() throws Exception {
  final Id.Application app=Id.Application.from(""String_Node_Str"",AllProgramsApp.NAME);
  final Id.Program program=Id.Program.from(app,ProgramType.FLOW,AllProgramsApp.NoOpFlow.NAME);
  final Id.Stream stream=Id.Stream.from(""String_Node_Str"",AllProgramsApp.STREAM_NAME);
  final Id.DatasetInstance dataset=Id.DatasetInstance.from(""String_Node_Str"",AllProgramsApp.DATASET_NAME);
  Assert.assertEquals(0,getAppStreamUsage(app).size());
  Assert.assertEquals(0,getProgramStreamUsage(program).size());
  Assert.assertEquals(0,getStreamProgramUsage(stream).size());
  Assert.assertEquals(0,getProgramDatasetUsage(program).size());
  Assert.assertEquals(0,getAppDatasetUsage(app).size());
  Assert.assertEquals(0,getDatasetProgramUsage(dataset).size());
  deploy(AllProgramsApp.class);
  try {
    Assert.assertTrue(getAppStreamUsage(app).contains(stream));
    Assert.assertTrue(getProgramStreamUsage(program).contains(stream));
    Assert.assertTrue(getStreamProgramUsage(stream).contains(program));
    Assert.assertTrue(getProgramDatasetUsage(program).contains(dataset));
    Assert.assertTrue(getProgramDatasetUsage(program).contains(dataset));
    Assert.assertTrue(getAppDatasetUsage(app).contains(dataset));
    Assert.assertTrue(getDatasetProgramUsage(dataset).contains(program));
  }
  finally {
    deleteApp(app,200);
    Assert.assertEquals(0,getAppStreamUsage(app).size());
    Assert.assertEquals(0,getProgramStreamUsage(program).size());
    Assert.assertEquals(0,getStreamProgramUsage(stream).size());
  }
}","@Test public void testFlowUsage() throws Exception {
  final Id.Application app=Id.Application.from(""String_Node_Str"",AllProgramsApp.NAME);
  final Id.Program program=Id.Program.from(app,ProgramType.FLOW,AllProgramsApp.NoOpFlow.NAME);
  final Id.Stream stream=Id.Stream.from(""String_Node_Str"",AllProgramsApp.STREAM_NAME);
  final Id.DatasetInstance dataset=Id.DatasetInstance.from(""String_Node_Str"",AllProgramsApp.DATASET_NAME);
  Assert.assertEquals(0,getAppStreamUsage(app).size());
  Assert.assertEquals(0,getProgramStreamUsage(program).size());
  Assert.assertEquals(0,getStreamProgramUsage(stream).size());
  Assert.assertEquals(0,getProgramDatasetUsage(program).size());
  Assert.assertEquals(0,getAppDatasetUsage(app).size());
  Assert.assertEquals(0,getDatasetProgramUsage(dataset).size());
  deploy(AllProgramsApp.class);
  try {
    Assert.assertTrue(getAppStreamUsage(app).contains(stream));
    Assert.assertTrue(getProgramStreamUsage(program).contains(stream));
    Assert.assertTrue(getStreamProgramUsage(stream).contains(program));
    Assert.assertTrue(getProgramDatasetUsage(program).contains(dataset));
    Assert.assertTrue(getAppDatasetUsage(app).contains(dataset));
    Assert.assertTrue(getDatasetProgramUsage(dataset).contains(program));
  }
  finally {
    deleteApp(app,200);
    Assert.assertEquals(0,getAppStreamUsage(app).size());
    Assert.assertEquals(0,getProgramStreamUsage(program).size());
    Assert.assertEquals(0,getStreamProgramUsage(stream).size());
  }
}"
6818,"@Test public void testMapReduceUsage() throws Exception {
  final Id.Application app=Id.Application.from(""String_Node_Str"",AllProgramsApp.NAME);
  final Id.Program program=Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR.NAME);
  final Id.Stream stream=Id.Stream.from(""String_Node_Str"",AllProgramsApp.STREAM_NAME);
  final Id.DatasetInstance dataset=Id.DatasetInstance.from(""String_Node_Str"",AllProgramsApp.DATASET_NAME);
  Assert.assertEquals(0,getAppStreamUsage(app).size());
  Assert.assertEquals(0,getProgramStreamUsage(program).size());
  Assert.assertEquals(0,getStreamProgramUsage(stream).size());
  Assert.assertEquals(0,getAppDatasetUsage(app).size());
  Assert.assertEquals(0,getDatasetProgramUsage(dataset).size());
  deploy(AllProgramsApp.class);
  try {
    Assert.assertTrue(getAppStreamUsage(app).contains(stream));
    Assert.assertTrue(getProgramStreamUsage(program).contains(stream));
    Assert.assertTrue(getStreamProgramUsage(stream).contains(program));
    Assert.assertTrue(getProgramDatasetUsage(program).contains(dataset));
    Assert.assertTrue(getProgramDatasetUsage(program).contains(dataset));
    Assert.assertTrue(getAppDatasetUsage(app).contains(dataset));
    Assert.assertTrue(getDatasetProgramUsage(dataset).contains(program));
  }
  finally {
    deleteApp(app,200);
    Assert.assertEquals(0,getAppStreamUsage(app).size());
    Assert.assertEquals(0,getProgramStreamUsage(program).size());
    Assert.assertEquals(0,getStreamProgramUsage(stream).size());
    Assert.assertEquals(0,getAppDatasetUsage(app).size());
    Assert.assertEquals(0,getDatasetProgramUsage(dataset).size());
  }
}","@Test public void testMapReduceUsage() throws Exception {
  final Id.Application app=Id.Application.from(""String_Node_Str"",AllProgramsApp.NAME);
  final Id.Program program=Id.Program.from(app,ProgramType.MAPREDUCE,AllProgramsApp.NoOpMR.NAME);
  final Id.Stream stream=Id.Stream.from(""String_Node_Str"",AllProgramsApp.STREAM_NAME);
  final Id.DatasetInstance dataset=Id.DatasetInstance.from(""String_Node_Str"",AllProgramsApp.DATASET_NAME);
  Assert.assertEquals(0,getAppStreamUsage(app).size());
  Assert.assertEquals(0,getProgramStreamUsage(program).size());
  Assert.assertEquals(0,getStreamProgramUsage(stream).size());
  Assert.assertEquals(0,getAppDatasetUsage(app).size());
  Assert.assertEquals(0,getDatasetProgramUsage(dataset).size());
  deploy(AllProgramsApp.class);
  try {
    Assert.assertTrue(getAppStreamUsage(app).contains(stream));
    Assert.assertTrue(getProgramStreamUsage(program).contains(stream));
    Assert.assertTrue(getStreamProgramUsage(stream).contains(program));
    Assert.assertTrue(getProgramDatasetUsage(program).contains(dataset));
    Assert.assertTrue(getAppDatasetUsage(app).contains(dataset));
    Assert.assertTrue(getDatasetProgramUsage(dataset).contains(program));
  }
  finally {
    deleteApp(app,200);
    Assert.assertEquals(0,getAppStreamUsage(app).size());
    Assert.assertEquals(0,getProgramStreamUsage(program).size());
    Assert.assertEquals(0,getStreamProgramUsage(stream).size());
    Assert.assertEquals(0,getAppDatasetUsage(app).size());
    Assert.assertEquals(0,getDatasetProgramUsage(dataset).size());
  }
}"
6819,"@Test public void testCheckDeletedProgramSpecs() throws Exception {
  AppFabricTestHelper.deployApplication(AllProgramsApp.class);
  ApplicationSpecification spec=Specifications.from(new AllProgramsApp());
  Set<String> specsToBeVerified=Sets.newHashSet();
  specsToBeVerified.addAll(spec.getMapReduce().keySet());
  specsToBeVerified.addAll(spec.getWorkflows().keySet());
  specsToBeVerified.addAll(spec.getFlows().keySet());
  Assert.assertEquals(3,specsToBeVerified.size());
  Id.Application appId=Id.Application.from(DefaultId.NAMESPACE,""String_Node_Str"");
  List<ProgramSpecification> deletedSpecs=store.getDeletedProgramSpecifications(appId,spec);
  Assert.assertEquals(0,deletedSpecs.size());
  spec=Specifications.from(new NoProgramsApp());
  deletedSpecs=store.getDeletedProgramSpecifications(appId,spec);
  Assert.assertEquals(3,deletedSpecs.size());
  for (  ProgramSpecification specification : deletedSpecs) {
    specsToBeVerified.remove(specification.getName());
  }
  Assert.assertEquals(0,specsToBeVerified.size());
}","@Test public void testCheckDeletedProgramSpecs() throws Exception {
  AppFabricTestHelper.deployApplication(AllProgramsApp.class);
  ApplicationSpecification spec=Specifications.from(new AllProgramsApp());
  Set<String> specsToBeVerified=Sets.newHashSet();
  specsToBeVerified.addAll(spec.getMapReduce().keySet());
  specsToBeVerified.addAll(spec.getWorkflows().keySet());
  specsToBeVerified.addAll(spec.getFlows().keySet());
  specsToBeVerified.addAll(spec.getServices().keySet());
  specsToBeVerified.addAll(spec.getWorkers().keySet());
  specsToBeVerified.addAll(spec.getSpark().keySet());
  Assert.assertEquals(6,specsToBeVerified.size());
  Id.Application appId=Id.Application.from(DefaultId.NAMESPACE,""String_Node_Str"");
  List<ProgramSpecification> deletedSpecs=store.getDeletedProgramSpecifications(appId,spec);
  Assert.assertEquals(0,deletedSpecs.size());
  spec=Specifications.from(new NoProgramsApp());
  deletedSpecs=store.getDeletedProgramSpecifications(appId,spec);
  Assert.assertEquals(6,deletedSpecs.size());
  for (  ProgramSpecification specification : deletedSpecs) {
    specsToBeVerified.remove(specification.getName());
  }
  Assert.assertEquals(0,specsToBeVerified.size());
}"
6820,"private void restartInstances(HttpResponder responder,String serviceName,int instanceId,boolean restartAll){
  long startTimeMs=System.currentTimeMillis();
  boolean isSuccess=true;
  if (!serviceManagementMap.containsKey(serviceName)) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",serviceName));
    return;
  }
  MasterServiceManager masterServiceManager=serviceManagementMap.get(serviceName);
  try {
    if (!masterServiceManager.isServiceEnabled()) {
      throw new IllegalStateException();
    }
    if (restartAll) {
      masterServiceManager.restartAllInstances();
    }
 else {
      if (instanceId < 0 || instanceId >= masterServiceManager.getInstances()) {
        throw new IllegalArgumentException();
      }
      masterServiceManager.restartInstances(instanceId);
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  IllegalStateException ise) {
    LOG.debug(String.format(""String_Node_Str"",serviceName));
    isSuccess=false;
    responder.sendString(HttpResponseStatus.FORBIDDEN,String.format(""String_Node_Str"" + ""String_Node_Str"",serviceName));
  }
catch (  IllegalArgumentException iex) {
    LOG.debug(String.format(""String_Node_Str"",serviceName),iex);
    isSuccess=false;
    responder.sendString(HttpResponseStatus.BAD_REQUEST,String.format(""String_Node_Str"",instanceId,serviceName));
  }
catch (  Exception ex) {
    LOG.warn(String.format(""String_Node_Str"",serviceName),ex);
    isSuccess=false;
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,String.format(""String_Node_Str"",instanceId,serviceName));
  }
 finally {
    long endTimeMs=System.currentTimeMillis();
    if (restartAll) {
      serviceStore.setRestartAllInstancesRequest(serviceName,startTimeMs,endTimeMs,isSuccess);
    }
 else {
      serviceStore.setRestartInstanceRequest(serviceName,startTimeMs,endTimeMs,isSuccess,instanceId);
    }
  }
}","private void restartInstances(HttpResponder responder,String serviceName,int instanceId,boolean restartAll){
  long startTimeMs=System.currentTimeMillis();
  boolean isSuccess=true;
  if (!serviceManagementMap.containsKey(serviceName)) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",serviceName));
    return;
  }
  MasterServiceManager masterServiceManager=serviceManagementMap.get(serviceName);
  try {
    if (!masterServiceManager.isServiceEnabled()) {
      String message=String.format(""String_Node_Str"",serviceName);
      LOG.debug(message);
      isSuccess=false;
      responder.sendString(HttpResponseStatus.FORBIDDEN,message);
      return;
    }
    if (restartAll) {
      masterServiceManager.restartAllInstances();
    }
 else {
      if (instanceId < 0 || instanceId >= masterServiceManager.getInstances()) {
        throw new IllegalArgumentException();
      }
      masterServiceManager.restartInstances(instanceId);
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  IllegalStateException ise) {
    String message=String.format(""String_Node_Str"",serviceName);
    LOG.debug(message,ise);
    isSuccess=false;
    responder.sendString(HttpResponseStatus.SERVICE_UNAVAILABLE,message);
  }
catch (  IllegalArgumentException iex) {
    String message=String.format(""String_Node_Str"",instanceId,serviceName);
    LOG.debug(message,iex);
    isSuccess=false;
    responder.sendString(HttpResponseStatus.BAD_REQUEST,message);
  }
catch (  Exception ex) {
    LOG.warn(String.format(""String_Node_Str"",serviceName),ex);
    isSuccess=false;
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,String.format(""String_Node_Str"",instanceId,serviceName));
  }
 finally {
    long endTimeMs=System.currentTimeMillis();
    if (restartAll) {
      serviceStore.setRestartAllInstancesRequest(serviceName,startTimeMs,endTimeMs,isSuccess);
    }
 else {
      serviceStore.setRestartInstanceRequest(serviceName,startTimeMs,endTimeMs,isSuccess,instanceId);
    }
  }
}"
6821,"/** 
 * Fetches column->value pairs for range of columns from persistent store. NOTE: persisted store can also be in-memory, it is called ""persisted"" to distinguish from in-memory buffer. NOTE: Using this method is generally always not efficient since it always hits the persisted store even if all needed data is in-memory buffer. Since columns set is not strictly defined the implementation always looks up for more columns in persistent store.
 * @param row row key defines the row to fetch columns from
 * @param startColumn first column in a range, inclusive
 * @param stopColumn last column in a range, exclusive
 * @param limit max number of columns to fetch
 * @return map of column->value pairs, never null.
 * @throws Exception
 */
protected abstract NavigableMap<byte[],byte[]> getPersisted(byte[] row,byte[] startColumn,byte[] stopColumn,int limit) throws Exception ;","/** 
 * Fetches a list of rows from persistent store. Subclasses should override this if they can batch multiple gets into a single request, as the default implementation simply loops through the gets and calls  {@link #getPersisted(byte[],byte[][])} on each get.NOTE: persisted store can also be in-memory, it is called ""persisted"" to distinguish from in-memory buffer.
 * @param gets list of gets to perform
 * @return list of rows, one for each get
 * @throws Exception
 */
protected List<Map<byte[],byte[]>> getPersisted(List<Get> gets) throws Exception {
  List<Map<byte[],byte[]>> results=Lists.newArrayListWithCapacity(gets.size());
  for (  Get get : gets) {
    List<byte[]> getColumns=get.getColumns();
    byte[][] columns=getColumns.isEmpty() ? null : getColumns.toArray(new byte[getColumns.size()][]);
    results.add(getPersisted(get.getRow(),columns));
  }
  return results;
}"
6822,"@Override public Row get(byte[] row,byte[] startColumn,byte[] stopColumn,int limit){
  reportRead(1);
  NavigableMap<byte[],Update> buffCols=buff.get(row);
  boolean rowDeleted=buffCols == null && buff.containsKey(row);
  if (rowDeleted) {
    return new Result(row,Collections.<byte[],byte[]>emptyMap());
  }
  try {
    Map<byte[],byte[]> persistedCols=getPersisted(row,startColumn,stopColumn,limit);
    NavigableMap<byte[],byte[]> result=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
    if (persistedCols != null) {
      result.putAll(persistedCols);
    }
    if (buffCols != null) {
      buffCols=getRange(buffCols,startColumn,stopColumn,limit);
      mergeToPersisted(result,buffCols,null);
    }
    return new Result(row,head(result,limit));
  }
 catch (  Exception e) {
    LOG.debug(""String_Node_Str"" + getTransactionAwareName() + ""String_Node_Str""+ Bytes.toStringBinary(row),e);
    throw new DataSetException(""String_Node_Str"",e);
  }
}","@Override public List<Row> get(List<Get> gets){
  try {
    List<Map<byte[],byte[]>> persistedRows=getPersisted(gets);
    Preconditions.checkArgument(gets.size() == persistedRows.size(),""String_Node_Str"");
    List<Row> result=Lists.newArrayListWithCapacity(persistedRows.size());
    Iterator<Map<byte[],byte[]>> persistedRowsIter=persistedRows.iterator();
    Iterator<Get> getIter=gets.iterator();
    while (persistedRowsIter.hasNext() && getIter.hasNext()) {
      Get get=getIter.next();
      Map<byte[],byte[]> persistedRow=persistedRowsIter.next();
      NavigableMap<byte[],byte[]> rowColumns=Maps.newTreeMap(Bytes.BYTES_COMPARATOR);
      rowColumns.putAll(persistedRow);
      byte[] row=get.getRow();
      NavigableMap<byte[],Update> buffCols=buff.get(row);
      if (buffCols == null && buff.containsKey(row)) {
        result.add(new Result(row,Collections.<byte[],byte[]>emptyMap()));
        continue;
      }
      if (buffCols != null) {
        List<byte[]> getColumns=get.getColumns();
        byte[][] columns=getColumns.isEmpty() ? null : getColumns.toArray(new byte[getColumns.size()][]);
        mergeToPersisted(rowColumns,buffCols,columns);
      }
      result.add(new Result(row,unwrapDeletes(rowColumns)));
    }
    return result;
  }
 catch (  Exception e) {
    LOG.debug(""String_Node_Str"" + getTransactionAwareName(),e);
    throw new DataSetException(""String_Node_Str"",e);
  }
}"
6823,"@Nullable @Override public Row apply(Result result){
  Map<byte[],byte[]> familyMap=result.getFamilyMap(columnFamily);
  return new co.cask.cdap.api.dataset.table.Result(result.getRow(),familyMap != null ? familyMap : ImmutableMap.<byte[],byte[]>of());
}","@Nullable @Override public Map<byte[],byte[]> apply(Result result){
  Map<byte[],byte[]> familyMap=result.getFamilyMap(columnFamily);
  return familyMap != null ? familyMap : ImmutableMap.<byte[],byte[]>of();
}"
6824,"@GET @Path(""String_Node_Str"") public void getProfile(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") String userId){
  Row row=profiles.get(new Get(userId));
  if (row.isEmpty()) {
    responder.sendError(404,""String_Node_Str"");
    return;
  }
  String name=row.getString(""String_Node_Str"");
  String email=row.getString(""String_Node_Str"");
  Long lastLogin=row.getLong(""String_Node_Str"");
  Long lastActive=row.getLong(""String_Node_Str"");
  Profile profile=new Profile(userId,name,email,lastLogin,lastActive);
  responder.sendJson(200,profile);
}","@GET @Path(""String_Node_Str"") public void getProfile(HttpServiceRequest request,HttpServiceResponder responder,@PathParam(""String_Node_Str"") String userId){
  Row row=profiles.get(new Get(userId));
  if (row.isEmpty()) {
    responder.sendError(404,""String_Node_Str"");
    return;
  }
  responder.sendJson(200,toProfile(row));
}"
6825,"@Override public void visit(Object instance,TypeToken<?> inspectType,TypeToken<?> declareType,Method method) throws Exception {
  Path classPathAnnotation=inspectType.getRawType().getAnnotation(Path.class);
  Path methodPathAnnotation=method.getAnnotation(Path.class);
  if (methodPathAnnotation == null && classPathAnnotation == null) {
    return;
  }
  Set<Class<? extends Annotation>> acceptedMethodTypes=ImmutableSet.of(GET.class,POST.class,DELETE.class,PUT.class,OPTIONS.class,HEAD.class);
  Set<Class<? extends Annotation>> methodAnnotations=Sets.newHashSet();
  for (  Annotation annotation : method.getAnnotations()) {
    Class<? extends Annotation> annotationClz=annotation.annotationType();
    if (acceptedMethodTypes.contains(annotationClz)) {
      methodAnnotations.add(annotationClz);
    }
  }
  for (  Class<? extends Annotation> methodTypeClz : methodAnnotations) {
    String methodType=methodTypeClz.getAnnotation(HttpMethod.class).value();
    String endpoint=""String_Node_Str"";
    endpoint=classPathAnnotation == null ? endpoint : endpoint + classPathAnnotation.value();
    endpoint=methodPathAnnotation == null ? endpoint : endpoint + ""String_Node_Str"" + methodPathAnnotation.value();
    endpoint=endpoint.replaceAll(""String_Node_Str"",""String_Node_Str"");
    endpoints.add(new ServiceHttpEndpoint(methodType,endpoint));
  }
}","@Override public void visit(Object instance,TypeToken<?> inspectType,TypeToken<?> declareType,Method method) throws Exception {
  if (!Modifier.isPublic(method.getModifiers())) {
    return;
  }
  Path classPathAnnotation=inspectType.getRawType().getAnnotation(Path.class);
  Path methodPathAnnotation=method.getAnnotation(Path.class);
  if (methodPathAnnotation == null && classPathAnnotation == null) {
    return;
  }
  Set<Class<? extends Annotation>> acceptedMethodTypes=ImmutableSet.of(GET.class,POST.class,DELETE.class,PUT.class,OPTIONS.class,HEAD.class);
  Set<Class<? extends Annotation>> methodAnnotations=Sets.newHashSet();
  for (  Annotation annotation : method.getAnnotations()) {
    Class<? extends Annotation> annotationClz=annotation.annotationType();
    if (acceptedMethodTypes.contains(annotationClz)) {
      methodAnnotations.add(annotationClz);
    }
  }
  for (  Class<? extends Annotation> methodTypeClz : methodAnnotations) {
    String methodType=methodTypeClz.getAnnotation(HttpMethod.class).value();
    String endpoint=""String_Node_Str"";
    endpoint=classPathAnnotation == null ? endpoint : endpoint + classPathAnnotation.value();
    endpoint=methodPathAnnotation == null ? endpoint : endpoint + ""String_Node_Str"" + methodPathAnnotation.value();
    endpoint=endpoint.replaceAll(""String_Node_Str"",""String_Node_Str"");
    endpoints.add(new ServiceHttpEndpoint(methodType,endpoint));
  }
}"
6826,"private int setServiceInstances(String namespace,String app,String service,int instances) throws Exception {
  String instanceUrl=String.format(""String_Node_Str"",app,service);
  String versionedInstanceUrl=getVersionedAPIPath(instanceUrl,Constants.Gateway.API_VERSION_3_TOKEN,namespace);
  String instancesBody=GSON.toJson(new Instances(instances));
  return doPut(versionedInstanceUrl,instancesBody).getStatusLine().getStatusCode();
}","private int setServiceInstances(Id.Service serviceId,int instances) throws Exception {
  String instanceUrl=String.format(""String_Node_Str"",serviceId.getApplicationId(),serviceId.getId());
  String versionedInstanceUrl=getVersionedAPIPath(instanceUrl,Constants.Gateway.API_VERSION_3_TOKEN,serviceId.getNamespaceId());
  String instancesBody=GSON.toJson(new Instances(instances));
  return doPut(versionedInstanceUrl,instancesBody).getStatusLine().getStatusCode();
}"
6827,"@Category(XSlowTests.class) @Test public void testProgramStartStopStatus() throws Exception {
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program wordcountFlow1=Id.Program.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME);
  Id.Program wordcountFlow2=Id.Program.from(TEST_NAMESPACE2,WORDCOUNT_APP_NAME,ProgramType.FLOW,WORDCOUNT_FLOW_NAME);
  Assert.assertEquals(STOPPED,getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow2,404);
  Assert.assertEquals(STOPPED,getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow1);
  waitState(wordcountFlow1,ProgramRunStatus.RUNNING.toString());
  stopProgram(wordcountFlow1);
  waitState(wordcountFlow1,STOPPED);
  response=deploy(DummyAppWithTrackingTable.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program dummyMR1=Id.Program.from(TEST_NAMESPACE1,DUMMY_APP_ID,ProgramType.MAPREDUCE,DUMMY_MR_NAME);
  Id.Program dummyMR2=Id.Program.from(TEST_NAMESPACE2,DUMMY_APP_ID,ProgramType.MAPREDUCE,DUMMY_MR_NAME);
  Assert.assertEquals(STOPPED,getProgramStatus(dummyMR2));
  startProgram(dummyMR1,404);
  Assert.assertEquals(STOPPED,getProgramStatus(dummyMR2));
  startProgram(dummyMR2);
  waitState(dummyMR2,ProgramRunStatus.RUNNING.toString());
  stopProgram(dummyMR2);
  waitState(dummyMR2,STOPPED);
  startProgram(dummyMR2);
  startProgram(dummyMR2);
  verifyProgramRuns(dummyMR2,""String_Node_Str"",1);
  List<RunRecord> historyRuns=getProgramRuns(dummyMR2,""String_Node_Str"");
  Assert.assertTrue(2 == historyRuns.size());
  String runId=historyRuns.get(0).getPid();
  stopProgram(dummyMR2,200,runId);
  runId=historyRuns.get(1).getPid();
  stopProgram(dummyMR2,200,runId);
  waitState(dummyMR2,STOPPED);
  response=deploy(SleepingWorkflowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program sleepWorkflow1=Id.Program.from(TEST_NAMESPACE1,SLEEP_WORKFLOW_APP_ID,ProgramType.WORKFLOW,SLEEP_WORKFLOW_NAME);
  Id.Program sleepWorkflow2=Id.Program.from(TEST_NAMESPACE2,SLEEP_WORKFLOW_APP_ID,ProgramType.WORKFLOW,SLEEP_WORKFLOW_NAME);
  Assert.assertEquals(STOPPED,getProgramStatus(sleepWorkflow2));
  startProgram(sleepWorkflow1,404);
  Assert.assertEquals(STOPPED,getProgramStatus(sleepWorkflow2));
  startProgram(sleepWorkflow2);
  waitState(sleepWorkflow2,ProgramRunStatus.RUNNING.toString());
  waitState(sleepWorkflow2,STOPPED);
  response=doDelete(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=doDelete(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
}","@Category(XSlowTests.class) @Test public void testProgramStartStopStatus() throws Exception {
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Flow wordcountFlow1=Id.Flow.from(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,WORDCOUNT_FLOW_NAME);
  Id.Flow wordcountFlow2=Id.Flow.from(TEST_NAMESPACE2,WORDCOUNT_APP_NAME,WORDCOUNT_FLOW_NAME);
  Assert.assertEquals(STOPPED,getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow2,404);
  Assert.assertEquals(STOPPED,getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow1);
  waitState(wordcountFlow1,ProgramRunStatus.RUNNING.toString());
  stopProgram(wordcountFlow1);
  waitState(wordcountFlow1,STOPPED);
  response=deploy(DummyAppWithTrackingTable.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program dummyMR1=Id.Program.from(TEST_NAMESPACE1,DUMMY_APP_ID,ProgramType.MAPREDUCE,DUMMY_MR_NAME);
  Id.Program dummyMR2=Id.Program.from(TEST_NAMESPACE2,DUMMY_APP_ID,ProgramType.MAPREDUCE,DUMMY_MR_NAME);
  Assert.assertEquals(STOPPED,getProgramStatus(dummyMR2));
  startProgram(dummyMR1,404);
  Assert.assertEquals(STOPPED,getProgramStatus(dummyMR2));
  startProgram(dummyMR2);
  waitState(dummyMR2,ProgramRunStatus.RUNNING.toString());
  stopProgram(dummyMR2);
  waitState(dummyMR2,STOPPED);
  startProgram(dummyMR2);
  startProgram(dummyMR2);
  verifyProgramRuns(dummyMR2,""String_Node_Str"",1);
  List<RunRecord> historyRuns=getProgramRuns(dummyMR2,""String_Node_Str"");
  Assert.assertTrue(2 == historyRuns.size());
  String runId=historyRuns.get(0).getPid();
  stopProgram(dummyMR2,200,runId);
  runId=historyRuns.get(1).getPid();
  stopProgram(dummyMR2,200,runId);
  waitState(dummyMR2,STOPPED);
  response=deploy(SleepingWorkflowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program sleepWorkflow1=Id.Program.from(TEST_NAMESPACE1,SLEEP_WORKFLOW_APP_ID,ProgramType.WORKFLOW,SLEEP_WORKFLOW_NAME);
  Id.Program sleepWorkflow2=Id.Program.from(TEST_NAMESPACE2,SLEEP_WORKFLOW_APP_ID,ProgramType.WORKFLOW,SLEEP_WORKFLOW_NAME);
  Assert.assertEquals(STOPPED,getProgramStatus(sleepWorkflow2));
  startProgram(sleepWorkflow1,404);
  Assert.assertEquals(STOPPED,getProgramStatus(sleepWorkflow2));
  startProgram(sleepWorkflow2);
  waitState(sleepWorkflow2,ProgramRunStatus.RUNNING.toString());
  waitState(sleepWorkflow2,STOPPED);
  response=doDelete(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=doDelete(getVersionedAPIPath(""String_Node_Str"",Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2));
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
}"
6828,"@Test public void testServices() throws Exception {
  HttpResponse response=deploy(AppWithServices.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program service1=Id.Program.from(TEST_NAMESPACE1,APP_WITH_SERVICES_APP_ID,ProgramType.SERVICE,APP_WITH_SERVICES_SERVICE_NAME);
  Id.Program service2=Id.Program.from(TEST_NAMESPACE2,APP_WITH_SERVICES_APP_ID,ProgramType.SERVICE,APP_WITH_SERVICES_SERVICE_NAME);
  startProgram(service1,404);
  startProgram(service2);
  try {
    getServiceInstances(TEST_NAMESPACE1,APP_WITH_SERVICES_APP_ID,APP_WITH_SERVICES_SERVICE_NAME);
    Assert.fail(""String_Node_Str"" + TEST_NAMESPACE1);
  }
 catch (  AssertionError e) {
  }
  ServiceInstances instances=getServiceInstances(TEST_NAMESPACE2,APP_WITH_SERVICES_APP_ID,APP_WITH_SERVICES_SERVICE_NAME);
  Assert.assertEquals(1,instances.getRequested());
  Assert.assertEquals(1,instances.getProvisioned());
  int code=setServiceInstances(TEST_NAMESPACE1,APP_WITH_SERVICES_APP_ID,APP_WITH_SERVICES_SERVICE_NAME,3);
  Assert.assertEquals(404,code);
  code=setServiceInstances(TEST_NAMESPACE2,APP_WITH_SERVICES_APP_ID,APP_WITH_SERVICES_SERVICE_NAME,3);
  Assert.assertEquals(200,code);
  instances=getServiceInstances(TEST_NAMESPACE2,APP_WITH_SERVICES_APP_ID,APP_WITH_SERVICES_SERVICE_NAME);
  Assert.assertEquals(3,instances.getRequested());
  Assert.assertEquals(3,instances.getProvisioned());
  response=callService(TEST_NAMESPACE1,APP_WITH_SERVICES_APP_ID,APP_WITH_SERVICES_SERVICE_NAME,HttpMethod.POST,""String_Node_Str"");
  code=response.getStatusLine().getStatusCode();
  Assert.assertEquals(404,code);
  response=callService(TEST_NAMESPACE1,APP_WITH_SERVICES_APP_ID,APP_WITH_SERVICES_SERVICE_NAME,HttpMethod.GET,""String_Node_Str"");
  code=response.getStatusLine().getStatusCode();
  Assert.assertEquals(404,code);
  stopProgram(service1,404);
  stopProgram(service2);
}","@Test public void testServices() throws Exception {
  HttpResponse response=deploy(AppWithServices.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Service service1=Id.Service.from(Id.Namespace.from(TEST_NAMESPACE1),APP_WITH_SERVICES_APP_ID,APP_WITH_SERVICES_SERVICE_NAME);
  Id.Service service2=Id.Service.from(Id.Namespace.from(TEST_NAMESPACE2),APP_WITH_SERVICES_APP_ID,APP_WITH_SERVICES_SERVICE_NAME);
  startProgram(service1,404);
  startProgram(service2);
  try {
    getServiceInstances(service1);
    Assert.fail(""String_Node_Str"" + TEST_NAMESPACE1);
  }
 catch (  AssertionError expected) {
  }
  ServiceInstances instances=getServiceInstances(service2);
  Assert.assertEquals(1,instances.getRequested());
  Assert.assertEquals(1,instances.getProvisioned());
  int code=setServiceInstances(service1,3);
  Assert.assertEquals(404,code);
  code=setServiceInstances(service2,3);
  Assert.assertEquals(200,code);
  instances=getServiceInstances(service2);
  Assert.assertEquals(3,instances.getRequested());
  Assert.assertEquals(3,instances.getProvisioned());
  response=callService(service1,HttpMethod.POST,""String_Node_Str"");
  code=response.getStatusLine().getStatusCode();
  Assert.assertEquals(404,code);
  response=callService(service1,HttpMethod.GET,""String_Node_Str"");
  code=response.getStatusLine().getStatusCode();
  Assert.assertEquals(404,code);
  stopProgram(service1,404);
  stopProgram(service2);
}"
6829,"private void verifyProgramList(String namespace,String appName,final String programType,int expected) throws Exception {
  HttpResponse response=requestAppDetail(namespace,appName);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  String json=EntityUtils.toString(response.getEntity());
  ApplicationDetail appDetail=GSON.fromJson(json,ApplicationDetail.class);
  Collection<ProgramRecord> programs=Collections2.filter(appDetail.getPrograms(),new Predicate<ProgramRecord>(){
    @Override public boolean apply(    @Nullable ProgramRecord record){
      return programType.equals(record.getType().getCategoryName());
    }
  }
);
  Assert.assertEquals(expected,programs.size());
}","private void verifyProgramList(String namespace,String appName,final ProgramType programType,int expected) throws Exception {
  HttpResponse response=requestAppDetail(namespace,appName);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  String json=EntityUtils.toString(response.getEntity());
  ApplicationDetail appDetail=GSON.fromJson(json,ApplicationDetail.class);
  Collection<ProgramRecord> programs=Collections2.filter(appDetail.getPrograms(),new Predicate<ProgramRecord>(){
    @Override public boolean apply(    @Nullable ProgramRecord record){
      return programType.getCategoryName().equals(record.getType().getCategoryName());
    }
  }
);
  Assert.assertEquals(expected,programs.size());
}"
6830,"@Override public boolean apply(@Nullable ProgramRecord record){
  return programType.equals(record.getType().getCategoryName());
}","@Override public boolean apply(@Nullable ProgramRecord record){
  return programType.getCategoryName().equals(record.getType().getCategoryName());
}"
6831,"private HttpResponse callService(String namespace,String app,String service,HttpMethod method,String endpoint) throws Exception {
  String serviceUrl=String.format(""String_Node_Str"",app,service,endpoint);
  String versionedServiceUrl=getVersionedAPIPath(serviceUrl,Constants.Gateway.API_VERSION_3_TOKEN,namespace);
  HttpResponse response;
  if (HttpMethod.GET.equals(method)) {
    response=doGet(versionedServiceUrl);
  }
 else   if (HttpMethod.POST.equals(method)) {
    response=doPost(versionedServiceUrl);
  }
 else {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  return response;
}","private HttpResponse callService(Id.Service serviceId,HttpMethod method,String endpoint) throws Exception {
  String serviceUrl=String.format(""String_Node_Str"",serviceId.getApplicationId(),serviceId.getId(),endpoint);
  String versionedServiceUrl=getVersionedAPIPath(serviceUrl,Constants.Gateway.API_VERSION_3_TOKEN,serviceId.getNamespaceId());
  if (HttpMethod.GET.equals(method)) {
    return doGet(versionedServiceUrl);
  }
 else   if (HttpMethod.POST.equals(method)) {
    return doPost(versionedServiceUrl);
  }
  throw new IllegalArgumentException(""String_Node_Str"");
}"
6832,"private ServiceInstances getServiceInstances(String namespace,String app,String service) throws Exception {
  String instanceUrl=String.format(""String_Node_Str"",app,service);
  String versionedInstanceUrl=getVersionedAPIPath(instanceUrl,Constants.Gateway.API_VERSION_3_TOKEN,namespace);
  HttpResponse response=doGet(versionedInstanceUrl);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  ServiceInstances instances=readResponse(response,ServiceInstances.class);
  return instances;
}","private ServiceInstances getServiceInstances(Id.Service serviceId) throws Exception {
  String instanceUrl=String.format(""String_Node_Str"",serviceId.getApplicationId(),serviceId.getId());
  String versionedInstanceUrl=getVersionedAPIPath(instanceUrl,Constants.Gateway.API_VERSION_3_TOKEN,serviceId.getNamespaceId());
  HttpResponse response=doGet(versionedInstanceUrl);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  return readResponse(response,ServiceInstances.class);
}"
6833,"/** 
 * Tests for program list calls
 */
@Test public void testProgramList() throws Exception {
  testListInitialState(TEST_NAMESPACE1,ProgramType.FLOW);
  testListInitialState(TEST_NAMESPACE2,ProgramType.MAPREDUCE);
  testListInitialState(TEST_NAMESPACE1,ProgramType.WORKFLOW);
  testListInitialState(TEST_NAMESPACE2,ProgramType.SPARK);
  testListInitialState(TEST_NAMESPACE1,ProgramType.SERVICE);
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=deploy(AppWithServices.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  verifyProgramList(TEST_NAMESPACE1,ProgramType.FLOW.getCategoryName(),1);
  verifyProgramList(TEST_NAMESPACE1,ProgramType.MAPREDUCE.getCategoryName(),1);
  verifyProgramList(TEST_NAMESPACE2,ProgramType.SERVICE.getCategoryName(),1);
  verifyProgramList(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW.getCategoryName(),1);
  verifyProgramList(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.MAPREDUCE.getCategoryName(),1);
  verifyProgramList(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.WORKFLOW.getCategoryName(),0);
  verifyProgramList(TEST_NAMESPACE2,APP_WITH_SERVICES_APP_ID,ProgramType.SERVICE.getCategoryName(),1);
  Assert.assertEquals(404,getAppFDetailResponseCode(TEST_NAMESPACE1,APP_WITH_SERVICES_APP_ID,ProgramType.SERVICE.getCategoryName()));
  Assert.assertEquals(404,getAppFDetailResponseCode(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW.getCategoryName()));
}","/** 
 * Tests for program list calls
 */
@Test public void testProgramList() throws Exception {
  testListInitialState(TEST_NAMESPACE1,ProgramType.FLOW);
  testListInitialState(TEST_NAMESPACE2,ProgramType.MAPREDUCE);
  testListInitialState(TEST_NAMESPACE1,ProgramType.WORKFLOW);
  testListInitialState(TEST_NAMESPACE2,ProgramType.SPARK);
  testListInitialState(TEST_NAMESPACE1,ProgramType.SERVICE);
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  response=deploy(AppWithServices.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  verifyProgramList(TEST_NAMESPACE1,ProgramType.FLOW,1);
  verifyProgramList(TEST_NAMESPACE1,ProgramType.MAPREDUCE,1);
  verifyProgramList(TEST_NAMESPACE2,ProgramType.SERVICE,1);
  verifyProgramList(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.FLOW,1);
  verifyProgramList(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.MAPREDUCE,1);
  verifyProgramList(TEST_NAMESPACE1,WORDCOUNT_APP_NAME,ProgramType.WORKFLOW,0);
  verifyProgramList(TEST_NAMESPACE2,APP_WITH_SERVICES_APP_ID,ProgramType.SERVICE,1);
  Assert.assertEquals(404,getAppFDetailResponseCode(TEST_NAMESPACE1,APP_WITH_SERVICES_APP_ID,ProgramType.SERVICE.getCategoryName()));
  Assert.assertEquals(404,getAppFDetailResponseCode(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW.getCategoryName()));
}"
6834,"@Override public Location get(Id.Namespace namespaceId,@Nullable String subPath) throws IOException {
  String namespacesDir=cConf.get(Constants.Namespace.NAMESPACES_DIR);
  Location namespaceLocation=locationFactory.create(namespacesDir).append(namespaceId.getId());
  if (subPath != null) {
    namespaceLocation.append(subPath);
  }
  return namespaceLocation;
}","@Override public Location get(Id.Namespace namespaceId,@Nullable String subPath) throws IOException {
  String namespacesDir=cConf.get(Constants.Namespace.NAMESPACES_DIR);
  Location namespaceLocation=locationFactory.create(namespacesDir).append(namespaceId.getId());
  if (subPath != null) {
    namespaceLocation=namespaceLocation.append(subPath);
  }
  return namespaceLocation;
}"
6835,"public void restartAllServiceInstances(HttpRequest request,HttpResponder responder,String serviceName){
  restartInstances(responder,serviceName,-1);
}","public void restartAllServiceInstances(HttpRequest request,HttpResponder responder,String serviceName){
  restartInstances(responder,serviceName,-1,true);
}"
6836,"private void restartInstances(HttpResponder responder,String serviceName,int instanceId){
  long startTimeMs=System.currentTimeMillis();
  boolean isSuccess=true;
  try {
    if (!serviceManagementMap.containsKey(serviceName)) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",serviceName));
      return;
    }
    MasterServiceManager masterServiceManager=serviceManagementMap.get(serviceName);
    if (instanceId < 0) {
      masterServiceManager.restartAllInstances();
    }
 else {
      masterServiceManager.restartInstances(instanceId);
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  Exception ex) {
    LOG.warn(String.format(""String_Node_Str"",serviceName),ex);
    isSuccess=false;
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,String.format(""String_Node_Str"",instanceId,serviceName));
  }
 finally {
    long endTimeMs=System.currentTimeMillis();
    if (instanceId < 0) {
      serviceStore.setRestartAllInstancesRequest(serviceName,startTimeMs,endTimeMs,isSuccess);
    }
 else {
      serviceStore.setRestartInstanceRequest(serviceName,startTimeMs,endTimeMs,isSuccess,instanceId);
    }
  }
}","private void restartInstances(HttpResponder responder,String serviceName,int instanceId,boolean restartAll){
  long startTimeMs=System.currentTimeMillis();
  boolean isSuccess=true;
  if (!serviceManagementMap.containsKey(serviceName)) {
    responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",serviceName));
    return;
  }
  MasterServiceManager masterServiceManager=serviceManagementMap.get(serviceName);
  try {
    if (!masterServiceManager.isServiceEnabled()) {
      throw new IllegalStateException();
    }
    if (restartAll) {
      masterServiceManager.restartAllInstances();
    }
 else {
      if (instanceId < 0 || instanceId >= masterServiceManager.getInstances()) {
        throw new IllegalArgumentException();
      }
      masterServiceManager.restartInstances(instanceId);
    }
    responder.sendStatus(HttpResponseStatus.OK);
  }
 catch (  IllegalStateException ise) {
    LOG.debug(String.format(""String_Node_Str"",serviceName));
    isSuccess=false;
    responder.sendString(HttpResponseStatus.FORBIDDEN,String.format(""String_Node_Str"" + ""String_Node_Str"",serviceName));
  }
catch (  IllegalArgumentException iex) {
    LOG.debug(String.format(""String_Node_Str"",serviceName),iex);
    isSuccess=false;
    responder.sendString(HttpResponseStatus.BAD_REQUEST,String.format(""String_Node_Str"",instanceId,serviceName));
  }
catch (  Exception ex) {
    LOG.warn(String.format(""String_Node_Str"",serviceName),ex);
    isSuccess=false;
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,String.format(""String_Node_Str"",instanceId,serviceName));
  }
 finally {
    long endTimeMs=System.currentTimeMillis();
    if (restartAll) {
      serviceStore.setRestartAllInstancesRequest(serviceName,startTimeMs,endTimeMs,isSuccess);
    }
 else {
      serviceStore.setRestartInstanceRequest(serviceName,startTimeMs,endTimeMs,isSuccess,instanceId);
    }
  }
}"
6837,"public void restartServiceInstance(HttpRequest request,HttpResponder responder,String serviceName,int instanceId){
  restartInstances(responder,serviceName,instanceId);
}","public void restartServiceInstance(HttpRequest request,HttpResponder responder,String serviceName,int instanceId){
  restartInstances(responder,serviceName,instanceId,false);
}"
6838,"@Override public void restartAllInstances(){
  try {
    Iterable<TwillController> twillControllers=twillRunnerService.lookup(Constants.Service.MASTER_SERVICES);
    for (    TwillController twillController : twillControllers) {
      twillController.restartAllInstances(serviceName).get();
    }
  }
 catch (  Throwable t) {
    throw new RuntimeException(String.format(""String_Node_Str"",serviceName),t);
  }
}","@Override public void restartAllInstances(){
  Iterable<TwillController> twillControllers=twillRunnerService.lookup(Constants.Service.MASTER_SERVICES);
  for (  TwillController twillController : twillControllers) {
    Futures.getUnchecked(twillController.restartAllInstances(serviceName));
  }
}"
6839,"@Override public void restartInstances(int instanceId,int... moreInstanceIds){
  try {
    Iterable<TwillController> twillControllers=twillRunnerService.lookup(Constants.Service.MASTER_SERVICES);
    for (    TwillController twillController : twillControllers) {
      twillController.restartInstances(serviceName,instanceId,moreInstanceIds).get();
    }
  }
 catch (  Throwable t) {
    throw new RuntimeException(String.format(""String_Node_Str"",serviceName),t);
  }
}","@Override public void restartInstances(int instanceId,int... moreInstanceIds){
  Iterable<TwillController> twillControllers=twillRunnerService.lookup(Constants.Service.MASTER_SERVICES);
  for (  TwillController twillController : twillControllers) {
    Futures.getUnchecked(twillController.restartInstances(serviceName,instanceId,moreInstanceIds));
  }
}"
6840,"@Override public void dropPartition(PartitionKey key){
  byte[] rowKey=generateRowKey(key,partitioning);
  Partition partition=getPartition(key);
  if (partition == null) {
    return;
  }
  if (!isExternal) {
    try {
      partition.getLocation().delete();
    }
 catch (    IOException e) {
      throw new DataSetException(String.format(""String_Node_Str"",key,partition.getLocation().toURI().getPath(),e.getMessage()),e);
    }
  }
  partitionsTable.delete(rowKey);
  dropPartitionFromExplore(key);
}","@Override public void dropPartition(PartitionKey key){
  byte[] rowKey=generateRowKey(key,partitioning);
  Partition partition=getPartition(key);
  if (partition == null) {
    return;
  }
  if (!isExternal) {
    try {
      if (partition.getLocation().exists()) {
        boolean deleteSuccess=partition.getLocation().delete(true);
        if (!deleteSuccess) {
          throw new DataSetException(String.format(""String_Node_Str"",key,partition.getLocation().toURI().getPath()));
        }
      }
    }
 catch (    IOException e) {
      throw new DataSetException(String.format(""String_Node_Str"",key,partition.getLocation().toURI().getPath(),e.getMessage()),e);
    }
  }
  partitionsTable.delete(rowKey);
  dropPartitionFromExplore(key);
}"
6841,"@Test public void testAddRemoveGetPartition() throws Exception {
  final PartitionedFileSet pfs=dsFrameworkUtil.getInstance(pfsInstance);
  dsFrameworkUtil.newTransactionExecutor((TransactionAware)pfs).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      PartitionOutput output=pfs.getPartitionOutput(PARTITION_KEY);
      Location outputLocation=output.getLocation();
      OutputStream out=outputLocation.getOutputStream();
      out.close();
      output.addPartition();
      Assert.assertTrue(outputLocation.exists());
      Assert.assertNotNull(pfs.getPartition(PARTITION_KEY));
      Assert.assertTrue(pfs.getPartition(PARTITION_KEY).getLocation().exists());
      pfs.dropPartition(PARTITION_KEY);
      Assert.assertFalse(outputLocation.exists());
      Assert.assertNull(pfs.getPartition(PARTITION_KEY));
      pfs.dropPartition(PARTITION_KEY);
    }
  }
);
}","@Test public void testAddRemoveGetPartition() throws Exception {
  final PartitionedFileSet pfs=dsFrameworkUtil.getInstance(pfsInstance);
  dsFrameworkUtil.newTransactionExecutor((TransactionAware)pfs).execute(new TransactionExecutor.Subroutine(){
    @Override public void apply() throws Exception {
      PartitionOutput output=pfs.getPartitionOutput(PARTITION_KEY);
      Location outputLocation=output.getLocation().append(""String_Node_Str"");
      OutputStream out=outputLocation.getOutputStream();
      out.close();
      output.addPartition();
      Assert.assertTrue(outputLocation.exists());
      Assert.assertNotNull(pfs.getPartition(PARTITION_KEY));
      Assert.assertTrue(pfs.getPartition(PARTITION_KEY).getLocation().exists());
      pfs.dropPartition(PARTITION_KEY);
      Assert.assertFalse(outputLocation.exists());
      Assert.assertNull(pfs.getPartition(PARTITION_KEY));
      pfs.dropPartition(PARTITION_KEY);
    }
  }
);
}"
6842,"@GET @Path(""String_Node_Str"") public void getWorkflowStatus(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowName,@PathParam(""String_Node_Str"") String runId){
  try {
    workflowClient.getWorkflowStatus(namespaceId,appId,workflowName,runId,new WorkflowClient.Callback(){
      @Override public void handle(      WorkflowClient.Status status){
        if (status.getCode() == WorkflowClient.Status.Code.NOT_FOUND) {
          responder.sendStatus(HttpResponseStatus.NOT_FOUND);
        }
 else         if (status.getCode() == WorkflowClient.Status.Code.OK) {
          responder.sendByteArray(HttpResponseStatus.OK,status.getResult().getBytes(),ImmutableMultimap.of(HttpHeaders.Names.CONTENT_TYPE,""String_Node_Str""));
        }
 else {
          responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,status.getResult());
        }
      }
    }
);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","@GET @Path(""String_Node_Str"") public void getWorkflowStatus(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowName,@PathParam(""String_Node_Str"") String runId){
  try {
    workflowClient.getWorkflowStatus(namespaceId,appId,workflowName,runId,new WorkflowClient.Callback(){
      @Override public void handle(      WorkflowClient.Status status){
        if (status.getCode() == WorkflowClient.Status.Code.NOT_FOUND) {
          responder.sendStatus(HttpResponseStatus.NOT_FOUND);
        }
 else         if (status.getCode() == WorkflowClient.Status.Code.OK) {
          responder.sendByteArray(HttpResponseStatus.OK,Bytes.toBytes(status.getResult()),ImmutableMultimap.of(HttpHeaders.Names.CONTENT_TYPE,""String_Node_Str""));
        }
 else {
          responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,status.getResult());
        }
      }
    }
);
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}"
6843,"@Override public void handle(WorkflowClient.Status status){
  if (status.getCode() == WorkflowClient.Status.Code.NOT_FOUND) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
 else   if (status.getCode() == WorkflowClient.Status.Code.OK) {
    responder.sendByteArray(HttpResponseStatus.OK,status.getResult().getBytes(),ImmutableMultimap.of(HttpHeaders.Names.CONTENT_TYPE,""String_Node_Str""));
  }
 else {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,status.getResult());
  }
}","@Override public void handle(WorkflowClient.Status status){
  if (status.getCode() == WorkflowClient.Status.Code.NOT_FOUND) {
    responder.sendStatus(HttpResponseStatus.NOT_FOUND);
  }
 else   if (status.getCode() == WorkflowClient.Status.Code.OK) {
    responder.sendByteArray(HttpResponseStatus.OK,Bytes.toBytes(status.getResult()),ImmutableMultimap.of(HttpHeaders.Names.CONTENT_TYPE,""String_Node_Str""));
  }
 else {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,status.getResult());
  }
}"
6844,"@Category(XSlowTests.class) @Test public void testMultipleWorkflowInstances() throws Exception {
  String appWithConcurrentWorkflow=""String_Node_Str"";
  String appWithConcurrentWorkflowSchedule1=""String_Node_Str"";
  String appWithConcurrentWorkflowSchedule2=""String_Node_Str"";
  String concurrentWorkflowName=""String_Node_Str"";
  File schedule1File=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File schedule2File=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File simpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  String defaultNamespace=Id.Namespace.DEFAULT.getId();
  HttpResponse response=deploy(ConcurrentWorkflowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,defaultNamespace);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(Id.Namespace.DEFAULT,appWithConcurrentWorkflow,ProgramType.WORKFLOW,concurrentWorkflowName);
  Map<String,String> propMap=ImmutableMap.of(""String_Node_Str"",schedule1File.getAbsolutePath(),""String_Node_Str"",schedule2File.getAbsolutePath(),""String_Node_Str"",simpleActionDoneFile.getAbsolutePath());
  PreferencesStore store=getInjector().getInstance(PreferencesStore.class);
  store.setProperties(defaultNamespace,appWithConcurrentWorkflow,ProgramType.WORKFLOW.getCategoryName(),concurrentWorkflowName,propMap);
  Assert.assertEquals(200,resumeSchedule(defaultNamespace,appWithConcurrentWorkflow,appWithConcurrentWorkflowSchedule1));
  Assert.assertEquals(200,resumeSchedule(defaultNamespace,appWithConcurrentWorkflow,appWithConcurrentWorkflowSchedule2));
  while (!(schedule1File.exists() && schedule2File.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  List<RunRecord> historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(historyRuns.size() >= 2);
  List<ScheduleSpecification> schedules=getSchedules(defaultNamespace,appWithConcurrentWorkflow,concurrentWorkflowName);
  for (  ScheduleSpecification spec : schedules) {
    Assert.assertEquals(200,suspendSchedule(defaultNamespace,appWithConcurrentWorkflow,spec.getSchedule().getName()));
  }
  response=getWorkflowCurrentStatus(programId,historyRuns.get(0).getPid());
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  String json=EntityUtils.toString(response.getEntity());
  List<WorkflowActionNode> nodes=GSON.fromJson(json,LIST_WORKFLOWACTIONNODE_TYPE);
  Assert.assertEquals(1,nodes.size());
  Assert.assertEquals(""String_Node_Str"",nodes.get(0).getProgram().getProgramName());
  response=getWorkflowCurrentStatus(programId,historyRuns.get(1).getPid());
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  json=EntityUtils.toString(response.getEntity());
  nodes=GSON.fromJson(json,LIST_WORKFLOWACTIONNODE_TYPE);
  Assert.assertEquals(1,nodes.size());
  Assert.assertEquals(""String_Node_Str"",nodes.get(0).getProgram().getProgramName());
  Assert.assertTrue(simpleActionDoneFile.createNewFile());
  deleteApp(programId.getApplication(),200,60,TimeUnit.SECONDS);
}","@Category(XSlowTests.class) @Test public void testMultipleWorkflowInstances() throws Exception {
  String appWithConcurrentWorkflow=""String_Node_Str"";
  String appWithConcurrentWorkflowSchedule1=""String_Node_Str"";
  String appWithConcurrentWorkflowSchedule2=""String_Node_Str"";
  String concurrentWorkflowName=""String_Node_Str"";
  File schedule1File=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File schedule2File=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File simpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  String defaultNamespace=Id.Namespace.DEFAULT.getId();
  HttpResponse response=deploy(ConcurrentWorkflowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,defaultNamespace);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(Id.Namespace.DEFAULT,appWithConcurrentWorkflow,ProgramType.WORKFLOW,concurrentWorkflowName);
  Map<String,String> propMap=ImmutableMap.of(""String_Node_Str"",schedule1File.getAbsolutePath(),""String_Node_Str"",schedule2File.getAbsolutePath(),""String_Node_Str"",simpleActionDoneFile.getAbsolutePath());
  PreferencesStore store=getInjector().getInstance(PreferencesStore.class);
  store.setProperties(defaultNamespace,appWithConcurrentWorkflow,ProgramType.WORKFLOW.getCategoryName(),concurrentWorkflowName,propMap);
  Assert.assertEquals(200,resumeSchedule(defaultNamespace,appWithConcurrentWorkflow,appWithConcurrentWorkflowSchedule1));
  Assert.assertEquals(200,resumeSchedule(defaultNamespace,appWithConcurrentWorkflow,appWithConcurrentWorkflowSchedule2));
  while (!(schedule1File.exists() && schedule2File.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  List<RunRecord> historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(historyRuns.size() >= 2);
  List<ScheduleSpecification> schedules=getSchedules(defaultNamespace,appWithConcurrentWorkflow,concurrentWorkflowName);
  for (  ScheduleSpecification spec : schedules) {
    Assert.assertEquals(200,suspendSchedule(defaultNamespace,appWithConcurrentWorkflow,spec.getSchedule().getName()));
  }
  response=getWorkflowCurrentStatus(programId,historyRuns.get(0).getPid());
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  String json=EntityUtils.toString(response.getEntity());
  List<WorkflowActionNode> nodes=GSON.fromJson(json,LIST_WORKFLOWACTIONNODE_TYPE);
  Assert.assertEquals(1,nodes.size());
  Assert.assertEquals(""String_Node_Str"",nodes.get(0).getProgram().getProgramName());
  response=getWorkflowCurrentStatusOld(programId,historyRuns.get(0).getPid());
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  json=EntityUtils.toString(response.getEntity());
  nodes=GSON.fromJson(json,LIST_WORKFLOWACTIONNODE_TYPE);
  Assert.assertEquals(1,nodes.size());
  Assert.assertEquals(""String_Node_Str"",nodes.get(0).getProgram().getProgramName());
  response=getWorkflowCurrentStatus(programId,historyRuns.get(1).getPid());
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  json=EntityUtils.toString(response.getEntity());
  nodes=GSON.fromJson(json,LIST_WORKFLOWACTIONNODE_TYPE);
  Assert.assertEquals(1,nodes.size());
  Assert.assertEquals(""String_Node_Str"",nodes.get(0).getProgram().getProgramName());
  response=getWorkflowCurrentStatusOld(programId,historyRuns.get(1).getPid());
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  json=EntityUtils.toString(response.getEntity());
  nodes=GSON.fromJson(json,LIST_WORKFLOWACTIONNODE_TYPE);
  Assert.assertEquals(1,nodes.size());
  Assert.assertEquals(""String_Node_Str"",nodes.get(0).getProgram().getProgramName());
  Assert.assertTrue(simpleActionDoneFile.createNewFile());
  deleteApp(programId.getApplication(),200,60,TimeUnit.SECONDS);
}"
6845,"private void testWorkflowCommand(final Id.Program workflow) throws Exception {
  File doneFile=TMP_FOLDER.newFile();
  doneFile.delete();
  LOG.info(""String_Node_Str"");
  programClient.start(workflow,false,ImmutableMap.of(""String_Node_Str"",doneFile.getAbsolutePath()));
  assertProgramRunning(programClient,workflow);
  List<RunRecord> runRecords=programClient.getProgramRuns(workflow,""String_Node_Str"",Long.MIN_VALUE,Long.MAX_VALUE,100);
  Assert.assertEquals(1,runRecords.size());
  final String pid=runRecords.get(0).getPid();
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return programClient.getWorkflowCurrent(workflow.getApplication(),workflow.getId(),pid).size();
    }
  }
,5,TimeUnit.SECONDS,100,TimeUnit.MILLISECONDS);
  doneFile.createNewFile();
  assertProgramStopped(programClient,workflow);
  LOG.info(""String_Node_Str"");
}","private void testWorkflowCommand(final Id.Program workflow) throws Exception {
  File doneFile=TMP_FOLDER.newFile();
  Assert.assertTrue(doneFile.delete());
  LOG.info(""String_Node_Str"");
  programClient.start(workflow,false,ImmutableMap.of(""String_Node_Str"",doneFile.getAbsolutePath()));
  assertProgramRunning(programClient,workflow);
  List<RunRecord> runRecords=programClient.getProgramRuns(workflow,""String_Node_Str"",Long.MIN_VALUE,Long.MAX_VALUE,100);
  Assert.assertEquals(1,runRecords.size());
  final String pid=runRecords.get(0).getPid();
  Tasks.waitFor(1,new Callable<Integer>(){
    @Override public Integer call() throws Exception {
      return programClient.getWorkflowCurrent(workflow.getApplication(),workflow.getId(),pid).size();
    }
  }
,5,TimeUnit.SECONDS,100,TimeUnit.MILLISECONDS);
  Assert.assertTrue(doneFile.createNewFile());
  assertProgramStopped(programClient,workflow);
  LOG.info(""String_Node_Str"");
}"
6846,"@Override protected void doStop(){
  executor.shutdownNow();
}","@Override protected void doStop(){
  executor.submit(new Runnable(){
    @Override public void run(){
      LOG.debug(""String_Node_Str"");
      notifyStopped();
    }
  }
);
  executor.shutdown();
}"
6847,"@Override protected void doStart(){
  executor=new ScheduledThreadPoolExecutor(1,Threads.createDaemonThreadFactory(""String_Node_Str"")){
    @Override protected void terminated(){
      notifyStopped();
    }
  }
;
  executor.submit(new Runnable(){
    @Override public void run(){
      LOG.debug(""String_Node_Str"");
      try {
        janitor.cleanAll();
        LOG.debug(""String_Node_Str"");
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
      }
 finally {
        long now=System.currentTimeMillis();
        long delay=(now / cleanupPeriod + 1) * cleanupPeriod - now;
        if (delay <= 0) {
          executor.submit(this);
        }
 else {
          LOG.debug(""String_Node_Str"",delay);
          executor.schedule(this,delay,TimeUnit.MILLISECONDS);
        }
      }
    }
  }
);
  notifyStarted();
}","@Override protected void doStart(){
  executor=Executors.newSingleThreadScheduledExecutor(Threads.createDaemonThreadFactory(""String_Node_Str""));
  executor.submit(new Runnable(){
    @Override public void run(){
      if (state() != State.RUNNING) {
        LOG.info(""String_Node_Str"");
        return;
      }
      LOG.debug(""String_Node_Str"");
      try {
        janitor.cleanAll();
        LOG.debug(""String_Node_Str"");
      }
 catch (      Throwable e) {
        LOG.error(""String_Node_Str"",e);
      }
 finally {
        long now=System.currentTimeMillis();
        long delay=(now / cleanupPeriod + 1) * cleanupPeriod - now;
        if (delay <= 0) {
          executor.submit(this);
        }
 else {
          LOG.debug(""String_Node_Str"",delay);
          executor.schedule(this,delay,TimeUnit.MILLISECONDS);
        }
      }
    }
  }
);
  notifyStarted();
}"
6848,"@Override public void run(){
  LOG.debug(""String_Node_Str"");
  try {
    janitor.cleanAll();
    LOG.debug(""String_Node_Str"");
  }
 catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
  }
 finally {
    long now=System.currentTimeMillis();
    long delay=(now / cleanupPeriod + 1) * cleanupPeriod - now;
    if (delay <= 0) {
      executor.submit(this);
    }
 else {
      LOG.debug(""String_Node_Str"",delay);
      executor.schedule(this,delay,TimeUnit.MILLISECONDS);
    }
  }
}","@Override public void run(){
  LOG.debug(""String_Node_Str"");
  notifyStopped();
}"
6849,"/** 
 * Get information about all versions of the given artifact.
 * @param namespace the namespace to get artifacts from
 * @param artifactName the name of the artifact to get
 * @return unmodifiable list of information about all versions of the given artifact
 * @throws ArtifactNotExistsException if no version of the given artifact exists
 * @throws IOException if there was an exception reading the artifact information from the metastore
 */
public List<ArtifactDetail> getArtifacts(final Id.Namespace namespace,final String artifactName) throws ArtifactNotExistsException, IOException {
  List<ArtifactDetail> artifacts=metaTable.executeUnchecked(new TransactionExecutor.Function<DatasetContext<Table>,List<ArtifactDetail>>(){
    @Override public List<ArtifactDetail> apply(    DatasetContext<Table> context) throws Exception {
      List<ArtifactDetail> archives=Lists.newArrayList();
      ArtifactKey artifactKey=new ArtifactKey(namespace,artifactName);
      Row row=context.get().get(artifactKey.getRowKey());
      if (!row.isEmpty()) {
        addArchivesToList(archives,row);
      }
      return archives;
    }
  }
);
  if (artifacts.isEmpty()) {
    throw new ArtifactNotExistsException(namespace,artifactName);
  }
  return Collections.unmodifiableList(artifacts);
}","/** 
 * Get information about all versions of the given artifact.
 * @param namespace the namespace to get artifacts from
 * @param artifactName the name of the artifact to get
 * @return unmodifiable list of information about all versions of the given artifact
 * @throws ArtifactNotExistsException if no version of the given artifact exists
 * @throws IOException if there was an exception reading the artifact information from the metastore
 */
public List<ArtifactDetail> getArtifacts(final Id.Namespace namespace,final String artifactName) throws ArtifactNotExistsException, IOException {
  List<ArtifactDetail> artifacts=metaTable.executeUnchecked(new TransactionExecutor.Function<DatasetContext<Table>,List<ArtifactDetail>>(){
    @Override public List<ArtifactDetail> apply(    DatasetContext<Table> context) throws Exception {
      List<ArtifactDetail> archives=Lists.newArrayList();
      ArtifactKey artifactKey=new ArtifactKey(namespace,artifactName);
      Row row=context.get().get(artifactKey.getRowKey());
      if (!row.isEmpty()) {
        addArtifactsToList(archives,row);
      }
      return archives;
    }
  }
);
  if (artifacts.isEmpty()) {
    throw new ArtifactNotExistsException(namespace,artifactName);
  }
  return Collections.unmodifiableList(artifacts);
}"
6850,"@Test public void testPluginSelector() throws Exception {
  try {
    artifactRepository.findPlugin(APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector());
    Assert.fail();
  }
 catch (  PluginNotExistsException e) {
  }
  Id.Artifact artifact1Id=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File jarFile=createJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(APP_ARTIFACT_ID.getNamespace(),APP_ARTIFACT_ID.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  artifactRepository.inspectArtifact(artifact1Id,jarFile,parents);
  Map.Entry<ArtifactDescriptor,PluginClass> plugin=artifactRepository.findPlugin(APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector());
  Assert.assertNotNull(plugin);
  Assert.assertEquals(new ArtifactVersion(""String_Node_Str""),plugin.getKey().getVersion());
  Assert.assertEquals(""String_Node_Str"",plugin.getValue().getName());
  Id.Artifact artifact2Id=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  jarFile=createJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
  artifactRepository.inspectArtifact(artifact2Id,jarFile,parents);
  plugin=artifactRepository.findPlugin(APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector());
  Assert.assertNotNull(plugin);
  Assert.assertEquals(new ArtifactVersion(""String_Node_Str""),plugin.getKey().getVersion());
  Assert.assertEquals(""String_Node_Str"",plugin.getValue().getName());
  PluginInstantiator instantiator=new PluginInstantiator(cConf,appClassLoader);
  ClassLoader pluginClassLoader=instantiator.getArtifactClassLoader(plugin.getKey());
  Class<?> pluginClass=pluginClassLoader.loadClass(TestPlugin2.class.getName());
  plugin=artifactRepository.findPlugin(APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector(){
    @Override public Map.Entry<ArtifactDescriptor,PluginClass> select(    SortedMap<ArtifactDescriptor,PluginClass> plugins){
      return plugins.entrySet().iterator().next();
    }
  }
);
  Assert.assertNotNull(plugin);
  Assert.assertEquals(new ArtifactVersion(""String_Node_Str""),plugin.getKey().getVersion());
  Assert.assertEquals(""String_Node_Str"",plugin.getValue().getName());
  pluginClassLoader=instantiator.getArtifactClassLoader(plugin.getKey());
  Assert.assertNotSame(pluginClass,pluginClassLoader.loadClass(TestPlugin2.class.getName()));
  Class<?> cls=pluginClassLoader.loadClass(PluginTestRunnable.class.getName());
  Assert.assertSame(appClassLoader.loadClass(PluginTestRunnable.class.getName()),cls);
  cls=pluginClassLoader.loadClass(Application.class.getName());
  Assert.assertSame(Application.class,cls);
}","@Test public void testPluginSelector() throws Exception {
  try {
    artifactRepository.findPlugin(APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector());
    Assert.fail();
  }
 catch (  PluginNotExistsException e) {
  }
  Id.Artifact artifact1Id=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File jarFile=createJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(APP_ARTIFACT_ID.getNamespace(),APP_ARTIFACT_ID.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  artifactRepository.addArtifact(artifact1Id,jarFile,parents);
  Map.Entry<ArtifactDescriptor,PluginClass> plugin=artifactRepository.findPlugin(APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector());
  Assert.assertNotNull(plugin);
  Assert.assertEquals(new ArtifactVersion(""String_Node_Str""),plugin.getKey().getVersion());
  Assert.assertEquals(""String_Node_Str"",plugin.getValue().getName());
  Id.Artifact artifact2Id=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  jarFile=createJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
  artifactRepository.addArtifact(artifact2Id,jarFile,parents);
  plugin=artifactRepository.findPlugin(APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector());
  Assert.assertNotNull(plugin);
  Assert.assertEquals(new ArtifactVersion(""String_Node_Str""),plugin.getKey().getVersion());
  Assert.assertEquals(""String_Node_Str"",plugin.getValue().getName());
  PluginInstantiator instantiator=new PluginInstantiator(cConf,appClassLoader);
  ClassLoader pluginClassLoader=instantiator.getArtifactClassLoader(plugin.getKey());
  Class<?> pluginClass=pluginClassLoader.loadClass(TestPlugin2.class.getName());
  plugin=artifactRepository.findPlugin(APP_ARTIFACT_ID,""String_Node_Str"",""String_Node_Str"",new PluginSelector(){
    @Override public Map.Entry<ArtifactDescriptor,PluginClass> select(    SortedMap<ArtifactDescriptor,PluginClass> plugins){
      return plugins.entrySet().iterator().next();
    }
  }
);
  Assert.assertNotNull(plugin);
  Assert.assertEquals(new ArtifactVersion(""String_Node_Str""),plugin.getKey().getVersion());
  Assert.assertEquals(""String_Node_Str"",plugin.getValue().getName());
  pluginClassLoader=instantiator.getArtifactClassLoader(plugin.getKey());
  Assert.assertNotSame(pluginClass,pluginClassLoader.loadClass(TestPlugin2.class.getName()));
  Class<?> cls=pluginClassLoader.loadClass(PluginTestRunnable.class.getName());
  Assert.assertSame(appClassLoader.loadClass(PluginTestRunnable.class.getName()),cls);
  cls=pluginClassLoader.loadClass(Application.class.getName());
  Assert.assertSame(Application.class,cls);
}"
6851,"@Test public void testPlugin() throws Exception {
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File jarFile=createJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(APP_ARTIFACT_ID.getNamespace(),APP_ARTIFACT_ID.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Id.Artifact artifactId=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  artifactRepository.inspectArtifact(artifactId,jarFile,parents);
  SortedMap<ArtifactDescriptor,List<PluginClass>> plugins=artifactRepository.getPlugins(APP_ARTIFACT_ID);
  Assert.assertEquals(1,plugins.size());
  Assert.assertEquals(2,plugins.get(plugins.firstKey()).size());
  try (PluginInstantiator instantiator=new PluginInstantiator(cConf,appClassLoader)){
    for (    Map.Entry<ArtifactDescriptor,List<PluginClass>> entry : plugins.entrySet()) {
      for (      PluginClass pluginClass : entry.getValue()) {
        Callable<String> plugin=instantiator.newInstance(entry.getKey(),pluginClass,PluginProperties.builder().add(""String_Node_Str"",TEST_EMPTY_CLASS).add(""String_Node_Str"",""String_Node_Str"").build());
        Assert.assertEquals(TEST_EMPTY_CLASS,plugin.call());
      }
    }
  }
 }","@Test public void testPlugin() throws Exception {
  Manifest manifest=createManifest(ManifestFields.EXPORT_PACKAGE,TestPlugin.class.getPackage().getName());
  File jarFile=createJar(TestPlugin.class,new File(tmpDir,""String_Node_Str""),manifest);
  Set<ArtifactRange> parents=ImmutableSet.of(new ArtifactRange(APP_ARTIFACT_ID.getNamespace(),APP_ARTIFACT_ID.getName(),new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str"")));
  Id.Artifact artifactId=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  artifactRepository.addArtifact(artifactId,jarFile,parents);
  SortedMap<ArtifactDescriptor,List<PluginClass>> plugins=artifactRepository.getPlugins(APP_ARTIFACT_ID);
  Assert.assertEquals(1,plugins.size());
  Assert.assertEquals(2,plugins.get(plugins.firstKey()).size());
  try (PluginInstantiator instantiator=new PluginInstantiator(cConf,appClassLoader)){
    for (    Map.Entry<ArtifactDescriptor,List<PluginClass>> entry : plugins.entrySet()) {
      for (      PluginClass pluginClass : entry.getValue()) {
        Callable<String> plugin=instantiator.newInstance(entry.getKey(),pluginClass,PluginProperties.builder().add(""String_Node_Str"",TEST_EMPTY_CLASS).add(""String_Node_Str"",""String_Node_Str"").build());
        Assert.assertEquals(TEST_EMPTY_CLASS,plugin.call());
      }
    }
  }
 }"
6852,"@Before public void setupData() throws Exception {
  artifactRepository.clear(Constants.DEFAULT_NAMESPACE_ID);
  File appArtifactFile=createJar(PluginTestAppTemplate.class,new File(tmpDir,""String_Node_Str""),createManifest(ManifestFields.EXPORT_PACKAGE,PluginTestRunnable.class.getPackage().getName()));
  artifactRepository.inspectArtifact(APP_ARTIFACT_ID,appArtifactFile,null);
  appClassLoader=createAppClassLoader(appArtifactFile);
}","@Before public void setupData() throws Exception {
  artifactRepository.clear(Constants.DEFAULT_NAMESPACE_ID);
  File appArtifactFile=createJar(PluginTestAppTemplate.class,new File(tmpDir,""String_Node_Str""),createManifest(ManifestFields.EXPORT_PACKAGE,PluginTestRunnable.class.getPackage().getName()));
  artifactRepository.addArtifact(APP_ARTIFACT_ID,appArtifactFile,null);
  appClassLoader=createAppClassLoader(appArtifactFile);
}"
6853,"@Test public void testGetNonexistantArtifact() throws IOException {
  Id.Namespace namespace=Id.Namespace.from(""String_Node_Str"");
  Assert.assertTrue(artifactStore.getArtifacts(namespace).isEmpty());
  try {
    artifactStore.getArtifacts(namespace,""String_Node_Str"");
    Assert.fail();
  }
 catch (  ArtifactNotExistsException e) {
  }
  try {
    artifactStore.getArtifact(Id.Artifact.from(namespace,""String_Node_Str"",""String_Node_Str""));
    Assert.fail();
  }
 catch (  ArtifactNotExistsException e) {
  }
}","@Test public void testGetNonexistantArtifact() throws IOException {
  Id.Namespace namespace=Id.Namespace.from(""String_Node_Str"");
  Assert.assertTrue(artifactStore.getArtifacts(namespace).isEmpty());
  ArtifactRange range=new ArtifactRange(namespace,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""));
  Assert.assertTrue(artifactStore.getArtifacts(range).isEmpty());
  try {
    artifactStore.getArtifacts(namespace,""String_Node_Str"");
    Assert.fail();
  }
 catch (  ArtifactNotExistsException e) {
  }
  try {
    artifactStore.getArtifact(Id.Artifact.from(namespace,""String_Node_Str"",""String_Node_Str""));
    Assert.fail();
  }
 catch (  ArtifactNotExistsException e) {
  }
}"
6854,"@Test public void testGetArtifacts() throws Exception {
  Id.Artifact artifact1V1=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  String contents1V1=""String_Node_Str"";
  List<PluginClass> plugins1V1=ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of()));
  ArtifactMeta meta1V1=new ArtifactMeta(plugins1V1);
  artifactStore.write(artifact1V1,meta1V1,new ByteArrayInputStream(Bytes.toBytes(contents1V1)));
  Id.Artifact artifact2V1=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  Id.Artifact artifact2V2=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  String contents2V1=""String_Node_Str"";
  String contents2V2=""String_Node_Str"";
  List<PluginClass> plugins2V1=ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of()));
  List<PluginClass> plugins2V2=ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of()));
  ArtifactMeta meta2V1=new ArtifactMeta(plugins2V1);
  ArtifactMeta meta2V2=new ArtifactMeta(plugins2V2);
  artifactStore.write(artifact2V1,meta2V1,new ByteArrayInputStream(Bytes.toBytes(contents2V1)));
  artifactStore.write(artifact2V2,meta2V2,new ByteArrayInputStream(Bytes.toBytes(contents2V2)));
  List<ArtifactDetail> artifact1Versions=artifactStore.getArtifacts(artifact1V1.getNamespace(),artifact1V1.getName());
  Assert.assertEquals(1,artifact1Versions.size());
  assertEqual(artifact1V1,meta1V1,contents1V1,artifact1Versions.get(0));
  List<ArtifactDetail> artifact2Versions=artifactStore.getArtifacts(artifact2V1.getNamespace(),artifact2V1.getName());
  Assert.assertEquals(2,artifact2Versions.size());
  assertEqual(artifact2V1,meta2V1,contents2V1,artifact2Versions.get(0));
  assertEqual(artifact2V2,meta2V2,contents2V2,artifact2Versions.get(1));
  List<ArtifactDetail> artifactVersions=artifactStore.getArtifacts(Constants.DEFAULT_NAMESPACE_ID);
  Assert.assertEquals(3,artifactVersions.size());
  assertEqual(artifact1V1,meta1V1,contents1V1,artifactVersions.get(0));
  assertEqual(artifact2V1,meta2V1,contents2V1,artifactVersions.get(1));
  assertEqual(artifact2V2,meta2V2,contents2V2,artifactVersions.get(2));
}","@Test public void testGetArtifacts() throws Exception {
  Id.Artifact artifact1V1=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  String contents1V1=""String_Node_Str"";
  List<PluginClass> plugins1V1=ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of()));
  ArtifactMeta meta1V1=new ArtifactMeta(plugins1V1);
  artifactStore.write(artifact1V1,meta1V1,new ByteArrayInputStream(Bytes.toBytes(contents1V1)));
  Id.Artifact artifact2V1=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  Id.Artifact artifact2V2=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  String contents2V1=""String_Node_Str"";
  String contents2V2=""String_Node_Str"";
  List<PluginClass> plugins2V1=ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of()));
  List<PluginClass> plugins2V2=ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of()));
  ArtifactMeta meta2V1=new ArtifactMeta(plugins2V1);
  ArtifactMeta meta2V2=new ArtifactMeta(plugins2V2);
  artifactStore.write(artifact2V1,meta2V1,new ByteArrayInputStream(Bytes.toBytes(contents2V1)));
  artifactStore.write(artifact2V2,meta2V2,new ByteArrayInputStream(Bytes.toBytes(contents2V2)));
  List<ArtifactDetail> artifact1Versions=artifactStore.getArtifacts(artifact1V1.getNamespace(),artifact1V1.getName());
  Assert.assertEquals(1,artifact1Versions.size());
  assertEqual(artifact1V1,meta1V1,contents1V1,artifact1Versions.get(0));
  List<ArtifactDetail> artifact2Versions=artifactStore.getArtifacts(artifact2V1.getNamespace(),artifact2V1.getName());
  Assert.assertEquals(2,artifact2Versions.size());
  assertEqual(artifact2V1,meta2V1,contents2V1,artifact2Versions.get(0));
  assertEqual(artifact2V2,meta2V2,contents2V2,artifact2Versions.get(1));
  List<ArtifactDetail> artifactVersions=artifactStore.getArtifacts(Constants.DEFAULT_NAMESPACE_ID);
  Assert.assertEquals(3,artifactVersions.size());
  assertEqual(artifact1V1,meta1V1,contents1V1,artifactVersions.get(0));
  assertEqual(artifact2V1,meta2V1,contents2V1,artifactVersions.get(1));
  assertEqual(artifact2V2,meta2V2,contents2V2,artifactVersions.get(2));
  ArtifactRange range=new ArtifactRange(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""));
  artifactVersions=artifactStore.getArtifacts(range);
  Assert.assertEquals(2,artifactVersions.size());
  assertEqual(artifact2V1,meta2V1,contents2V1,artifactVersions.get(0));
  assertEqual(artifact2V2,meta2V2,contents2V2,artifactVersions.get(1));
  range=new ArtifactRange(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""));
  artifactVersions=artifactStore.getArtifacts(range);
  Assert.assertEquals(1,artifactVersions.size());
  assertEqual(artifact2V2,meta2V2,contents2V2,artifactVersions.get(0));
  range=new ArtifactRange(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",new ArtifactVersion(""String_Node_Str""),new ArtifactVersion(""String_Node_Str""));
  artifactVersions=artifactStore.getArtifacts(range);
  Assert.assertEquals(1,artifactVersions.size());
  assertEqual(artifact2V1,meta2V1,contents2V1,artifactVersions.get(0));
}"
6855,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  File file=resolver.resolvePathToFile(arguments.get(ArgumentName.APP_JAR_FILE.toString()));
  Preconditions.checkArgument(file.exists(),""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"");
  Preconditions.checkArgument(file.canRead(),""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"");
  String appConfig=arguments.get(ArgumentName.RUNTIME_ARGS.toString(),""String_Node_Str"");
  applicationClient.deploy(cliConfig.getCurrentNamespace(),file);
  output.println(""String_Node_Str"");
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  File file=resolver.resolvePathToFile(arguments.get(ArgumentName.APP_JAR_FILE.toString()));
  Preconditions.checkArgument(file.exists(),""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"");
  Preconditions.checkArgument(file.canRead(),""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"");
  String appConfig=arguments.get(ArgumentName.RUNTIME_ARGS.toString(),""String_Node_Str"");
  applicationClient.deploy(cliConfig.getCurrentNamespace(),file,appConfig);
  output.println(""String_Node_Str"");
}"
6856,"ApplicationManager deployApplication(Id.Namespace namespace,Class<? extends Application> applicationClz,Config configObject,File... bundleEmbeddedJars);","/** 
 * Deploys an   {@link Application}.
 * @param namespace The namespace to deploy to
 * @param applicationClz The application class
 * @param configObject Configuration object to be used during deployment and can be accessedin  {@link Application#configure} via {@link ApplicationContext#getConfig}
 * @return An {@link ApplicationManager} to manage the deployed application.
 */
ApplicationManager deployApplication(Id.Namespace namespace,Class<? extends Application> applicationClz,Config configObject,File... bundleEmbeddedJars);"
6857,"@Test public void testAdapters() throws Exception {
  List<AdapterDetail> initialList=adapterClient.list();
  Assert.assertEquals(0,initialList.size());
  DummyWorkerTemplate.Config config=new DummyWorkerTemplate.Config(2);
  String adapterName=""String_Node_Str"";
  AdapterConfig adapterConfig=new AdapterConfig(""String_Node_Str"",DummyWorkerTemplate.NAME,GSON.toJsonTree(config));
  adapterClient.create(adapterName,adapterConfig);
  adapterClient.waitForExists(adapterName,30,TimeUnit.SECONDS);
  Assert.assertTrue(adapterClient.exists(adapterName));
  AdapterDetail someAdapter=adapterClient.get(adapterName);
  Assert.assertNotNull(someAdapter);
  List<AdapterDetail> list=adapterClient.list();
  Assert.assertArrayEquals(new AdapterDetail[]{someAdapter},list.toArray());
  adapterClient.waitForStatus(adapterName,AdapterStatus.STOPPED,30,TimeUnit.SECONDS);
  adapterClient.start(adapterName);
  adapterClient.waitForStatus(adapterName,AdapterStatus.STARTED,30,TimeUnit.SECONDS);
  adapterClient.stop(adapterName);
  adapterClient.waitForStatus(adapterName,AdapterStatus.STOPPED,30,TimeUnit.SECONDS);
  List<RunRecord> runs=adapterClient.getRuns(adapterName,ProgramRunStatus.ALL,0,Long.MAX_VALUE,10);
  Assert.assertEquals(1,runs.size());
  String logs=adapterClient.getLogs(adapterName);
  Assert.assertNotNull(logs);
  adapterClient.delete(adapterName);
  Assert.assertFalse(adapterClient.exists(adapterName));
  try {
    adapterClient.get(adapterName);
    Assert.fail();
  }
 catch (  AdapterNotFoundException e) {
  }
  List<AdapterDetail> finalList=adapterClient.list();
  Assert.assertEquals(0,finalList.size());
}","@Test public void testAdapters() throws Exception {
  List<AdapterDetail> initialList=adapterClient.list();
  Assert.assertEquals(0,initialList.size());
  DummyWorkerTemplate.Config config=new DummyWorkerTemplate.Config(2);
  String adapterName=""String_Node_Str"";
  AdapterConfig adapterConfig=new AdapterConfig(""String_Node_Str"",DummyWorkerTemplate.NAME,GSON.toJsonTree(config));
  adapterClient.create(adapterName,adapterConfig);
  adapterClient.waitForExists(adapterName,30,TimeUnit.SECONDS);
  Assert.assertTrue(adapterClient.exists(adapterName));
  AdapterDetail someAdapter=adapterClient.get(adapterName);
  Assert.assertNotNull(someAdapter);
  List<AdapterDetail> list=adapterClient.list();
  Assert.assertArrayEquals(new AdapterDetail[]{someAdapter},list.toArray());
  adapterClient.waitForStatus(adapterName,AdapterStatus.STOPPED,30,TimeUnit.SECONDS);
  adapterClient.start(adapterName);
  adapterClient.waitForStatus(adapterName,AdapterStatus.STARTED,30,TimeUnit.SECONDS);
  List<RunRecord> adapterRuns=adapterClient.getRuns(adapterName,ProgramRunStatus.COMPLETED,0,Long.MAX_VALUE,null);
  Assert.assertEquals(0,adapterRuns.size());
  adapterClient.stop(adapterName);
  adapterClient.waitForStatus(adapterName,AdapterStatus.STOPPED,30,TimeUnit.SECONDS);
  List<RunRecord> runs=adapterClient.getRuns(adapterName,ProgramRunStatus.ALL,0,Long.MAX_VALUE,10);
  Assert.assertEquals(1,runs.size());
  String logs=adapterClient.getLogs(adapterName);
  Assert.assertNotNull(logs);
  adapterClient.delete(adapterName);
  Assert.assertFalse(adapterClient.exists(adapterName));
  try {
    adapterClient.get(adapterName);
    Assert.fail();
  }
 catch (  AdapterNotFoundException e) {
  }
  List<AdapterDetail> finalList=adapterClient.list();
  Assert.assertEquals(0,finalList.size());
}"
6858,"@Override public void run(){
  try {
    barrier.await();
    ArtifactMeta meta=new ArtifactMeta(ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"" + writer,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of())));
    artifactStore.write(artifactId,meta,new ByteArrayInputStream(Bytes.toBytes(writer)));
    successfulWriters.add(writer);
  }
 catch (  InterruptedException|BrokenBarrierException|ArtifactAlreadyExistsException|IOException e) {
    throw new RuntimeException(e);
  }
catch (  WriteConflictException e) {
  }
 finally {
    latch.countDown();
  }
}","@Override public void run(){
  try {
    barrier.await();
    ArtifactMeta meta=new ArtifactMeta(ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"" + writer,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of())));
    artifactStore.write(artifactId,meta,new ByteArrayInputStream(Bytes.toBytes(writer)));
  }
 catch (  InterruptedException|BrokenBarrierException|ArtifactAlreadyExistsException|IOException e) {
    throw new RuntimeException(e);
  }
catch (  WriteConflictException e) {
  }
 finally {
    latch.countDown();
  }
}"
6859,"@Category(SlowTests.class) @Test public void testConcurrentSnapshotWrite() throws Exception {
  int numThreads=20;
  final Id.Artifact artifactId=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  final List<String> successfulWriters=Collections.synchronizedList(Lists.<String>newArrayList());
  final CyclicBarrier barrier=new CyclicBarrier(numThreads);
  final CountDownLatch latch=new CountDownLatch(numThreads);
  ExecutorService executorService=Executors.newFixedThreadPool(numThreads);
  for (int i=0; i < numThreads; i++) {
    final String writer=String.valueOf(i);
    executorService.execute(new Runnable(){
      @Override public void run(){
        try {
          barrier.await();
          ArtifactMeta meta=new ArtifactMeta(ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"" + writer,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of())));
          artifactStore.write(artifactId,meta,new ByteArrayInputStream(Bytes.toBytes(writer)));
          successfulWriters.add(writer);
        }
 catch (        InterruptedException|BrokenBarrierException|ArtifactAlreadyExistsException|IOException e) {
          throw new RuntimeException(e);
        }
catch (        WriteConflictException e) {
        }
 finally {
          latch.countDown();
        }
      }
    }
);
  }
  latch.await();
  String winnerWriter=successfulWriters.get(successfulWriters.size() - 1);
  ArtifactDetail detail=artifactStore.getArtifact(artifactId);
  ArtifactMeta expectedMeta=new ArtifactMeta(ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"" + winnerWriter,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of())));
  assertEqual(artifactId,expectedMeta,winnerWriter,detail);
  Map<ArtifactInfo,List<PluginClass>> pluginMap=artifactStore.getPluginClasses(artifactId.getNamespace(),""String_Node_Str"");
  Map<ArtifactInfo,List<PluginClass>> expected=Maps.newHashMap();
  expected.put(detail.getInfo(),Lists.newArrayList(new PluginClass(""String_Node_Str"",""String_Node_Str"" + winnerWriter,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of())));
  Assert.assertEquals(expected,pluginMap);
}","@Category(SlowTests.class) @Test public void testConcurrentSnapshotWrite() throws Exception {
  int numThreads=20;
  final Id.Artifact artifactId=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  final CyclicBarrier barrier=new CyclicBarrier(numThreads);
  final CountDownLatch latch=new CountDownLatch(numThreads);
  ExecutorService executorService=Executors.newFixedThreadPool(numThreads);
  for (int i=0; i < numThreads; i++) {
    final String writer=String.valueOf(i);
    executorService.execute(new Runnable(){
      @Override public void run(){
        try {
          barrier.await();
          ArtifactMeta meta=new ArtifactMeta(ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"" + writer,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of())));
          artifactStore.write(artifactId,meta,new ByteArrayInputStream(Bytes.toBytes(writer)));
        }
 catch (        InterruptedException|BrokenBarrierException|ArtifactAlreadyExistsException|IOException e) {
          throw new RuntimeException(e);
        }
catch (        WriteConflictException e) {
        }
 finally {
          latch.countDown();
        }
      }
    }
);
  }
  latch.await();
  ArtifactDetail detail=artifactStore.getArtifact(artifactId);
  String pluginName=detail.getMeta().getPlugins().get(0).getName();
  String winnerWriter=pluginName.substring(""String_Node_Str"".length());
  ArtifactMeta expectedMeta=new ArtifactMeta(ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"" + winnerWriter,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of())));
  assertEqual(artifactId,expectedMeta,winnerWriter,detail);
  Map<ArtifactInfo,List<PluginClass>> pluginMap=artifactStore.getPluginClasses(artifactId.getNamespace(),""String_Node_Str"");
  Map<ArtifactInfo,List<PluginClass>> expected=Maps.newHashMap();
  expected.put(detail.getInfo(),Lists.newArrayList(new PluginClass(""String_Node_Str"",""String_Node_Str"" + winnerWriter,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of())));
  Assert.assertEquals(expected,pluginMap);
}"
6860,"@Override public void start(){
  try {
    URLConnections.setDefaultUseCaches(false);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",e);
  }
  createSystemHBaseNamespace();
  updateConfigurationTable();
  LogAppenderInitializer logAppenderInitializer=baseInjector.getInstance(LogAppenderInitializer.class);
  logAppenderInitializer.initialize();
  zkClient.startAndWait();
  twillRunner.start();
  Futures.getUnchecked(ZKOperations.ignoreError(zkClient.create(""String_Node_Str"",null,CreateMode.PERSISTENT),KeeperException.NodeExistsException.class,null));
  kafkaClient.startAndWait();
  metricsCollectionService.startAndWait();
  serviceStore.startAndWait();
  leaderElection.startAndWait();
}","@Override public void start(){
  try {
    URLConnections.setDefaultUseCaches(false);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",e);
  }
  createSystemHBaseNamespace();
  updateConfigurationTable();
  LogAppenderInitializer logAppenderInitializer=baseInjector.getInstance(LogAppenderInitializer.class);
  logAppenderInitializer.initialize();
  zkClient.startAndWait();
  Futures.getUnchecked(ZKOperations.ignoreError(zkClient.create(""String_Node_Str"",null,CreateMode.PERSISTENT),KeeperException.NodeExistsException.class,null));
  twillRunner.start();
  kafkaClient.startAndWait();
  metricsCollectionService.startAndWait();
  serviceStore.startAndWait();
  leaderElection.startAndWait();
}"
6861,"@Override public void process(final ApplicationDeployable input) throws Exception {
  ImmutableList.Builder<Program> programs=ImmutableList.builder();
  final ApplicationSpecification appSpec=input.getSpecification();
  final String applicationName=appSpec.getName();
  final ArchiveBundler bundler=new ArchiveBundler(input.getLocation());
  Id.Namespace namespaceId=input.getId().getNamespace();
  Location namespacedLocation=namespacedLocationFactory.get(namespaceId);
  final Location appFabricDir=namespacedLocation.append(configuration.get(Constants.AppFabric.OUTPUT_DIR));
  if (!appFabricDir.exists() && !appFabricDir.mkdirs() && !appFabricDir.exists()) {
    throw new IOException(String.format(""String_Node_Str"",appFabricDir.toURI().getPath()));
  }
  Iterable<ProgramSpecification> specifications=Iterables.concat(appSpec.getMapReduce().values(),appSpec.getFlows().values(),appSpec.getWorkflows().values(),appSpec.getServices().values(),appSpec.getSpark().values(),appSpec.getWorkers().values());
  Set<String> servingHostNames=WebappProgramRunner.getServingHostNames(Locations.newInputSupplier(input.getLocation()));
  if (!servingHostNames.isEmpty()) {
    specifications=Iterables.concat(specifications,ImmutableList.of(createWebappSpec(ProgramType.WEBAPP.toString().toLowerCase())));
  }
  ListeningExecutorService executorService=MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(10,Threads.createDaemonThreadFactory(""String_Node_Str"")));
  try {
    List<ListenableFuture<Location>> futures=Lists.newArrayList();
    for (    final ProgramSpecification spec : specifications) {
      ListenableFuture<Location> future=executorService.submit(new Callable<Location>(){
        @Override public Location call() throws Exception {
          ProgramType type=ProgramTypes.fromSpecification(spec);
          String name=String.format(Locale.ENGLISH,""String_Node_Str"",applicationName,type);
          Location programDir=appFabricDir.append(name);
          if (!programDir.exists()) {
            programDir.mkdirs();
          }
          Location output=programDir.append(String.format(""String_Node_Str"",spec.getName()));
          Id.Program programId=Id.Program.from(input.getId(),type,spec.getName());
          return ProgramBundle.create(programId,bundler,output,spec.getClassName(),appSpec);
        }
      }
);
      futures.add(future);
    }
    for (    Location jarLocation : Futures.allAsList(futures).get()) {
      programs.add(Programs.create(jarLocation,null));
    }
  }
  finally {
    executorService.shutdown();
  }
  Location newArchiveLocation=appFabricDir.append(applicationName).append(Constants.ARCHIVE_DIR);
  moveAppArchiveUnderAppDirectory(input.getLocation(),newArchiveLocation);
  ApplicationDeployable updatedAppDeployable=new ApplicationDeployable(input.getId(),input.getSpecification(),input.getExistingAppSpec(),input.getApplicationDeployScope(),newArchiveLocation);
  emit(new ApplicationWithPrograms(updatedAppDeployable,programs.build()));
}","@Override public void process(final ApplicationDeployable input) throws Exception {
  ImmutableList.Builder<Program> programs=ImmutableList.builder();
  final ApplicationSpecification appSpec=input.getSpecification();
  final String applicationName=appSpec.getName();
  final ArchiveBundler bundler=new ArchiveBundler(input.getLocation());
  Id.Namespace namespaceId=input.getId().getNamespace();
  Location namespacedLocation=namespacedLocationFactory.get(namespaceId);
  final Location appFabricDir=namespacedLocation.append(configuration.get(Constants.AppFabric.OUTPUT_DIR));
  if (!appFabricDir.exists() && !appFabricDir.mkdirs() && !appFabricDir.exists()) {
    throw new IOException(String.format(""String_Node_Str"",appFabricDir.toURI().getPath()));
  }
  Iterable<ProgramSpecification> specifications=Iterables.concat(appSpec.getMapReduce().values(),appSpec.getFlows().values(),appSpec.getWorkflows().values(),appSpec.getServices().values(),appSpec.getSpark().values(),appSpec.getWorkers().values());
  Set<String> servingHostNames=WebappProgramRunner.getServingHostNames(Locations.newInputSupplier(input.getLocation()));
  if (!servingHostNames.isEmpty()) {
    specifications=Iterables.concat(specifications,ImmutableList.of(createWebappSpec(ProgramType.WEBAPP.toString().toLowerCase())));
  }
  ListeningExecutorService executorService=MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(10,Threads.createDaemonThreadFactory(""String_Node_Str"")));
  try {
    List<ListenableFuture<Location>> futures=Lists.newArrayList();
    for (    final ProgramSpecification spec : specifications) {
      ListenableFuture<Location> future=executorService.submit(new Callable<Location>(){
        @Override public Location call() throws Exception {
          ProgramType type=ProgramTypes.fromSpecification(spec);
          String name=String.format(Locale.ENGLISH,""String_Node_Str"",applicationName,type);
          Location programDir=appFabricDir.append(name);
          if (!programDir.exists()) {
            programDir.mkdirs();
          }
          Location output=programDir.append(String.format(""String_Node_Str"",spec.getName()));
          Id.Program programId=Id.Program.from(input.getId(),type,spec.getName());
          return ProgramBundle.create(programId,bundler,output,spec.getClassName(),appSpec);
        }
      }
);
      futures.add(future);
    }
    for (    Location jarLocation : Futures.allAsList(futures).get()) {
      programs.add(Programs.create(jarLocation,null));
    }
  }
  finally {
    executorService.shutdown();
  }
  Location newArchiveLocation=appFabricDir.append(applicationName).append(Constants.ARCHIVE_DIR);
  moveAppArchiveUnderAppDirectory(input.getLocation(),newArchiveLocation);
  Location programLocation=newArchiveLocation.append(input.getLocation().getName());
  ApplicationDeployable updatedAppDeployable=new ApplicationDeployable(input.getId(),input.getSpecification(),input.getExistingAppSpec(),input.getApplicationDeployScope(),programLocation);
  emit(new ApplicationWithPrograms(updatedAppDeployable,programs.build()));
}"
6862,"@Override public void process(final ApplicationDeployable input) throws Exception {
  ImmutableList.Builder<Program> programs=ImmutableList.builder();
  final ApplicationSpecification appSpec=input.getSpecification();
  final String applicationName=appSpec.getName();
  final ArchiveBundler bundler=new ArchiveBundler(input.getLocation());
  Id.Namespace namespaceId=input.getId().getNamespace();
  Location namespacedLocation=namespacedLocationFactory.get(namespaceId);
  final Location appFabricDir=namespacedLocation.append(configuration.get(Constants.AppFabric.OUTPUT_DIR));
  if (!appFabricDir.exists() && !appFabricDir.mkdirs() && !appFabricDir.exists()) {
    throw new IOException(String.format(""String_Node_Str"",appFabricDir.toURI().getPath()));
  }
  Iterable<ProgramSpecification> specifications=Iterables.concat(appSpec.getMapReduce().values(),appSpec.getFlows().values(),appSpec.getWorkflows().values(),appSpec.getServices().values(),appSpec.getSpark().values(),appSpec.getWorkers().values());
  Set<String> servingHostNames=WebappProgramRunner.getServingHostNames(Locations.newInputSupplier(input.getLocation()));
  if (!servingHostNames.isEmpty()) {
    specifications=Iterables.concat(specifications,ImmutableList.of(createWebappSpec(ProgramType.WEBAPP.toString().toLowerCase())));
  }
  ListeningExecutorService executorService=MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(10,Threads.createDaemonThreadFactory(""String_Node_Str"")));
  try {
    List<ListenableFuture<Location>> futures=Lists.newArrayList();
    for (    final ProgramSpecification spec : specifications) {
      ListenableFuture<Location> future=executorService.submit(new Callable<Location>(){
        @Override public Location call() throws Exception {
          ProgramType type=ProgramTypes.fromSpecification(spec);
          String name=String.format(Locale.ENGLISH,""String_Node_Str"",applicationName,type);
          Location programDir=appFabricDir.append(name);
          if (!programDir.exists()) {
            programDir.mkdirs();
          }
          Location output=programDir.append(String.format(""String_Node_Str"",spec.getName()));
          Id.Program programId=Id.Program.from(input.getId(),type,spec.getName());
          return ProgramBundle.create(programId,bundler,output,spec.getClassName(),appSpec);
        }
      }
);
      futures.add(future);
    }
    for (    Location jarLocation : Futures.allAsList(futures).get()) {
      programs.add(Programs.create(jarLocation,null));
    }
  }
  finally {
    executorService.shutdown();
  }
  Location newArchiveLocation=appFabricDir.append(applicationName).append(Constants.ARCHIVE_DIR);
  moveAppArchiveUnderAppDirectory(input.getLocation(),newArchiveLocation);
  ApplicationDeployable updatedAppDeployable=new ApplicationDeployable(input.getId(),input.getSpecification(),input.getExistingAppSpec(),input.getApplicationDeployScope(),newArchiveLocation);
  emit(new ApplicationWithPrograms(updatedAppDeployable,programs.build()));
}","@Override public void process(final ApplicationDeployable input) throws Exception {
  ImmutableList.Builder<Program> programs=ImmutableList.builder();
  final ApplicationSpecification appSpec=input.getSpecification();
  final String applicationName=appSpec.getName();
  final ArchiveBundler bundler=new ArchiveBundler(input.getLocation());
  Id.Namespace namespaceId=input.getId().getNamespace();
  Location namespacedLocation=namespacedLocationFactory.get(namespaceId);
  final Location appFabricDir=namespacedLocation.append(configuration.get(Constants.AppFabric.OUTPUT_DIR));
  if (!appFabricDir.exists() && !appFabricDir.mkdirs() && !appFabricDir.exists()) {
    throw new IOException(String.format(""String_Node_Str"",appFabricDir.toURI().getPath()));
  }
  Iterable<ProgramSpecification> specifications=Iterables.concat(appSpec.getMapReduce().values(),appSpec.getFlows().values(),appSpec.getWorkflows().values(),appSpec.getServices().values(),appSpec.getSpark().values(),appSpec.getWorkers().values());
  Set<String> servingHostNames=WebappProgramRunner.getServingHostNames(Locations.newInputSupplier(input.getLocation()));
  if (!servingHostNames.isEmpty()) {
    specifications=Iterables.concat(specifications,ImmutableList.of(createWebappSpec(ProgramType.WEBAPP.toString().toLowerCase())));
  }
  ListeningExecutorService executorService=MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(10,Threads.createDaemonThreadFactory(""String_Node_Str"")));
  try {
    List<ListenableFuture<Location>> futures=Lists.newArrayList();
    for (    final ProgramSpecification spec : specifications) {
      ListenableFuture<Location> future=executorService.submit(new Callable<Location>(){
        @Override public Location call() throws Exception {
          ProgramType type=ProgramTypes.fromSpecification(spec);
          String name=String.format(Locale.ENGLISH,""String_Node_Str"",applicationName,type);
          Location programDir=appFabricDir.append(name);
          if (!programDir.exists()) {
            programDir.mkdirs();
          }
          Location output=programDir.append(String.format(""String_Node_Str"",spec.getName()));
          Id.Program programId=Id.Program.from(input.getId(),type,spec.getName());
          return ProgramBundle.create(programId,bundler,output,spec.getClassName(),appSpec);
        }
      }
);
      futures.add(future);
    }
    for (    Location jarLocation : Futures.allAsList(futures).get()) {
      programs.add(Programs.create(jarLocation,null));
    }
  }
  finally {
    executorService.shutdown();
  }
  Location newArchiveLocation=appFabricDir.append(applicationName).append(Constants.ARCHIVE_DIR);
  moveAppArchiveUnderAppDirectory(input.getLocation(),newArchiveLocation);
  Location programLocation=newArchiveLocation.append(input.getLocation().getName());
  ApplicationDeployable updatedAppDeployable=new ApplicationDeployable(input.getId(),input.getSpecification(),input.getExistingAppSpec(),input.getApplicationDeployScope(),programLocation);
  emit(new ApplicationWithPrograms(updatedAppDeployable,programs.build()));
}"
6863,"private static ClassLoader createClassFilteredClassLoader(Iterable<String> allowedClasses,ClassLoader parentClassLoader){
  Set<String> allowedResources=ImmutableSet.copyOf(Iterables.transform(allowedClasses,CLASS_TO_RESOURCE_NAME));
  return new FilterClassLoader(Predicates.in(allowedResources),Predicates.<String>alwaysTrue(),parentClassLoader);
}","private static ClassLoader createClassFilteredClassLoader(Iterable<String> allowedClasses,ClassLoader parentClassLoader){
  Set<String> allowedResources=ImmutableSet.copyOf(Iterables.transform(allowedClasses,CLASS_TO_RESOURCE_NAME));
  return FilterClassLoader.create(Predicates.in(allowedResources),Predicates.<String>alwaysTrue(),parentClassLoader);
}"
6864,"@BeforeClass public static void setupTest() throws IOException {
  addTemplatePlugins(TEMPLATE_ID,""String_Node_Str"",DBSource.class,KVTableSource.class,StreamBatchSource.class,TableSource.class,DBRecord.class,TimePartitionedFileSetDatasetAvroSource.class,BatchCubeSink.class,DBSink.class,KVTableSink.class,TableSink.class,TimePartitionedFileSetDatasetAvroSink.class,AvroKeyOutputFormat.class,AvroKey.class,TimePartitionedFileSetDatasetParquetSink.class,AvroParquetOutputFormat.class);
  addTemplatePlugins(TEMPLATE_ID,""String_Node_Str"",MetaKVTableSource.class);
  addTemplatePlugins(TEMPLATE_ID,""String_Node_Str"",MetaKVTableSink.class);
  addTemplatePlugins(TEMPLATE_ID,""String_Node_Str"",ProjectionTransform.class,ScriptFilterTransform.class,StructuredRecordToGenericRecordTransform.class);
  addTemplatePlugins(TEMPLATE_ID,""String_Node_Str"",JDBCDriver.class);
  addTemplatePluginJson(TEMPLATE_ID,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",JDBCDriver.class.getName());
  deployTemplate(NAMESPACE,TEMPLATE_ID,ETLBatchTemplate.class,PipelineConfigurable.class.getPackage().getName(),BatchSource.class.getPackage().getName());
}","@BeforeClass public static void setupTest() throws IOException {
  addTemplatePlugins(TEMPLATE_ID,""String_Node_Str"",DBSource.class,KVTableSource.class,StreamBatchSource.class,TableSource.class,DBRecord.class,TimePartitionedFileSetDatasetAvroSource.class,BatchCubeSink.class,DBSink.class,KVTableSink.class,TableSink.class,TimePartitionedFileSetDatasetAvroSink.class,AvroKeyOutputFormat.class,AvroKey.class,TimePartitionedFileSetDatasetParquetSink.class,AvroParquetOutputFormat.class);
  addTemplatePlugins(TEMPLATE_ID,""String_Node_Str"",MetaKVTableSource.class);
  addTemplatePlugins(TEMPLATE_ID,""String_Node_Str"",MetaKVTableSink.class);
  addTemplatePlugins(TEMPLATE_ID,""String_Node_Str"",ProjectionTransform.class,ScriptFilterTransform.class,StructuredRecordToGenericRecordTransform.class);
  addTemplatePlugins(TEMPLATE_ID,""String_Node_Str"",JDBCDriver.class);
  addTemplatePluginJson(TEMPLATE_ID,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",JDBCDriver.class.getName());
  deployTemplate(NAMESPACE,TEMPLATE_ID,ETLBatchTemplate.class,PipelineConfigurable.class.getPackage().getName(),BatchSource.class.getPackage().getName(),ETLConfig.class.getPackage().getName());
}"
6865,"@Override public URL getResource(String name){
  return super.getResource(name);
}","@Override public URL getResource(String name){
  return resourceAcceptor.apply(name) ? super.getResource(name) : null;
}"
6866,"/** 
 * @param resourceAcceptor Filter for accepting resources
 * @param parentClassLoader Parent classloader
 */
public FilterClassLoader(Predicate<String> resourceAcceptor,Predicate<String> packageAcceptor,ClassLoader parentClassLoader){
  super(parentClassLoader);
  this.resourceAcceptor=resourceAcceptor;
  this.packageAcceptor=packageAcceptor;
  this.bootstrapClassLoader=new URLClassLoader(new URL[0],null);
}","/** 
 * @param resourceAcceptor Filter for accepting resources
 * @param parentClassLoader Parent classloader
 */
private FilterClassLoader(Predicate<String> resourceAcceptor,Predicate<String> packageAcceptor,ClassLoader parentClassLoader){
  super(parentClassLoader);
  this.resourceAcceptor=resourceAcceptor;
  this.packageAcceptor=packageAcceptor;
  this.bootstrapClassLoader=new URLClassLoader(new URL[0],null);
}"
6867,"/** 
 * Constructs an instance that load classes from the given directory for the given program type. <p/> The URLs for class loading are: <p/> <pre> [dir] [dir]/*.jar [dir]/lib/*.jar </pre>
 */
public static ProgramClassLoader create(File unpackedJarDir,ClassLoader parentClassLoader,@Nullable ProgramType programType) throws IOException {
  Set<String> visibleResources=ProgramResources.getVisibleResources(programType);
  ImmutableSet.Builder<String> visiblePackages=ImmutableSet.builder();
  for (  String resource : visibleResources) {
    if (resource.endsWith(""String_Node_Str"")) {
      int idx=resource.lastIndexOf('/');
      if (idx > 0) {
        visiblePackages.add(resource.substring(0,idx));
      }
    }
  }
  ClassLoader filteredParent=new FilterClassLoader(Predicates.in(visibleResources),Predicates.in(visiblePackages.build()),parentClassLoader);
  return new ProgramClassLoader(unpackedJarDir,filteredParent);
}","/** 
 * Constructs an instance that load classes from the given directory for the given program type. <p/> The URLs for class loading are: <p/> <pre> [dir] [dir]/*.jar [dir]/lib/*.jar </pre>
 */
public static ProgramClassLoader create(File unpackedJarDir,ClassLoader parentClassLoader,@Nullable ProgramType programType) throws IOException {
  ClassLoader filteredParent=FilterClassLoader.create(programType,parentClassLoader);
  return new ProgramClassLoader(unpackedJarDir,filteredParent);
}"
6868,"public SystemDatasetInstantiator create(@Nullable ClassLoader parentClassLoader){
  return new SystemDatasetInstantiator(datasetFramework,parentClassLoader,new DirectoryClassLoaderProvider(cConf,locationFactory),null);
}","/** 
 * Create a   {@link SystemDatasetInstantiator} that uses the given classloader as the parent when instantiatingdatasets. 
 * @param parentClassLoader the parent classloader to use when instantiating datasets. If null, the systemclassloader will be used
 * @return a {@link SystemDatasetInstantiator} using the given classloader as the parent classloader
 */
public SystemDatasetInstantiator create(@Nullable ClassLoader parentClassLoader){
  return new SystemDatasetInstantiator(datasetFramework,parentClassLoader,new DirectoryClassLoaderProvider(cConf,locationFactory),null);
}"
6869,"@Override public ClassLoader load(CacheKey key) throws Exception {
  if (key.uri == null) {
    return key.parentClassLoader;
  }
  Location jarLocation=locationFactory.create(key.uri);
  File unpackedDir=DirUtils.createTempDir(tmpDir);
  BundleJarUtil.unpackProgramJar(jarLocation,unpackedDir);
  LOG.trace(""String_Node_Str"",key.uri.toString(),unpackedDir.getAbsolutePath());
  return new DirectoryClassLoader(unpackedDir,key.parentClassLoader);
}","@Override public ClassLoader load(CacheKey key) throws Exception {
  if (key.uri == null) {
    return key.parentClassLoader;
  }
  Location jarLocation=locationFactory.create(key.uri);
  File unpackedDir=DirUtils.createTempDir(tmpDir);
  BundleJarUtil.unpackProgramJar(jarLocation,unpackedDir);
  LOG.trace(""String_Node_Str"",key.uri.toString(),unpackedDir.getAbsolutePath());
  return new DirectoryClassLoader(unpackedDir,key.parentClassLoader,""String_Node_Str"");
}"
6870,"public SystemDatasetInstantiator createDatasetInstantiator(ClassLoader parentClassLoader){
  return datasetInstantiatorFactory.create(parentClassLoader);
}","/** 
 * Get a   {@link SystemDatasetInstantiator} that can instantiate datasets using the given classloader as theparent classloader for datasets. Must be closed after it is no longer needed, as dataset jars may be unpacked in order to create classloaders for custom datasets. The given parent classloader will be wrapped in a  {@link FilterClassLoader}to prevent CDAP dependencies from leaking through. For example, if a custom dataset has an avro dependency, the classloader should use the avro from the custom dataset and not from cdap.
 * @param parentClassLoader the parent classloader to use when instantiating datasets. If null, the systemclassloader will be used
 * @return a dataset instantiator that can be used to instantiate datasets
 */
public SystemDatasetInstantiator createDatasetInstantiator(@Nullable ClassLoader parentClassLoader){
  parentClassLoader=parentClassLoader == null ? Objects.firstNonNull(Thread.currentThread().getContextClassLoader(),getClass().getClassLoader()) : parentClassLoader;
  return datasetInstantiatorFactory.create(FilterClassLoader.create(parentClassLoader));
}"
6871,"public BasicWorkflowToken(Map<String,List<NodeValueEntry>> tokenValueMap,String nodeName,@Nullable Map<String,Map<String,Long>> mapReduceCounters){
  for (  Map.Entry<String,List<NodeValueEntry>> entry : tokenValueMap.entrySet()) {
    List<NodeValueEntry> nodeValueList=Lists.newArrayList();
    nodeValueList.addAll(entry.getValue());
    this.tokenValueMap.put(entry.getKey(),nodeValueList);
  }
  this.nodeName=nodeName;
  if (mapReduceCounters != null) {
    this.mapReduceCounters=copyHadoopCounters(mapReduceCounters);
  }
}","public BasicWorkflowToken(BasicWorkflowToken other){
  for (  Map.Entry<String,List<NodeValueEntry>> entry : other.tokenValueMap.entrySet()) {
    List<NodeValueEntry> nodeValueList=Lists.newArrayList();
    nodeValueList.addAll(entry.getValue());
    this.tokenValueMap.put(entry.getKey(),nodeValueList);
  }
  this.nodeName=other.nodeName;
  if (other.mapReduceCounters != null) {
    this.mapReduceCounters=copyHadoopCounters(other.mapReduceCounters);
  }
}"
6872,"/** 
 * Make a deep copy of the   {@link WorkflowToken}.
 * @return copied WorkflowToken
 */
public WorkflowToken deepCopy(){
  return new BasicWorkflowToken(tokenValueMap,nodeName,mapReduceCounters);
}","/** 
 * Make a deep copy of the   {@link WorkflowToken}.
 * @return copied WorkflowToken
 */
public WorkflowToken deepCopy(){
  return new BasicWorkflowToken(this);
}"
6873,"@Override protected void configure(){
  bind(PipelineFactory.class).to(SynchronousPipelineFactory.class);
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<DeploymentInfo,ApplicationWithPrograms>>(){
  }
,new TypeLiteral<LocalApplicationManager<DeploymentInfo,ApplicationWithPrograms>>(){
  }
).build(new TypeLiteral<ManagerFactory<DeploymentInfo,ApplicationWithPrograms>>(){
  }
));
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<DeploymentInfo,ApplicationWithPrograms>>(){
  }
,LocalApplicationTemplateManager.class).build(Key.get(new TypeLiteral<ManagerFactory<DeploymentInfo,ApplicationWithPrograms>>(){
  }
,Names.named(""String_Node_Str""))));
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<AdapterDeploymentInfo,AdapterDefinition>>(){
  }
,LocalAdapterManager.class).build(Key.get(new TypeLiteral<ManagerFactory<AdapterDeploymentInfo,AdapterDefinition>>(){
  }
,Names.named(""String_Node_Str""))));
  bind(Store.class).to(DefaultStore.class);
  bind(AdapterService.class).in(Scopes.SINGLETON);
  bind(ProgramLifecycleService.class).in(Scopes.SINGLETON);
  bind(PluginRepository.class).in(Scopes.SINGLETON);
  bind(NamespaceAdmin.class).to(DefaultNamespaceAdmin.class).in(Scopes.SINGLETON);
  Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.AppFabric.HANDLERS_BINDING));
  CommonHandlers.add(handlerBinder);
  handlerBinder.addBinding().to(ConfigHandler.class);
  handlerBinder.addBinding().to(AppFabricDataHttpHandler.class);
  handlerBinder.addBinding().to(VersionHandler.class);
  handlerBinder.addBinding().to(MonitorHandler.class);
  handlerBinder.addBinding().to(UsageHandler.class);
  handlerBinder.addBinding().to(NamespaceHttpHandler.class);
  handlerBinder.addBinding().to(NotificationFeedHttpHandler.class);
  handlerBinder.addBinding().to(AppLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(DashboardHttpHandler.class);
  handlerBinder.addBinding().to(ProgramLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(PreferencesHttpHandler.class);
  handlerBinder.addBinding().to(ConsoleSettingsHttpHandler.class);
  handlerBinder.addBinding().to(TransactionHttpHandler.class);
  handlerBinder.addBinding().to(AdapterHttpHandler.class);
  handlerBinder.addBinding().to(ApplicationTemplateHandler.class);
  handlerBinder.addBinding().to(WorkflowHttpHandler.class);
  for (  Class<? extends HttpHandler> handlerClass : handlerClasses) {
    handlerBinder.addBinding().to(handlerClass);
  }
}","@Override protected void configure(){
  bind(PipelineFactory.class).to(SynchronousPipelineFactory.class);
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<DeploymentInfo,ApplicationWithPrograms>>(){
  }
,new TypeLiteral<LocalApplicationManager<DeploymentInfo,ApplicationWithPrograms>>(){
  }
).build(new TypeLiteral<ManagerFactory<DeploymentInfo,ApplicationWithPrograms>>(){
  }
));
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<DeploymentInfo,ApplicationWithPrograms>>(){
  }
,LocalApplicationTemplateManager.class).build(Key.get(new TypeLiteral<ManagerFactory<DeploymentInfo,ApplicationWithPrograms>>(){
  }
,Names.named(""String_Node_Str""))));
  install(new FactoryModuleBuilder().implement(new TypeLiteral<Manager<AdapterDeploymentInfo,AdapterDefinition>>(){
  }
,LocalAdapterManager.class).build(Key.get(new TypeLiteral<ManagerFactory<AdapterDeploymentInfo,AdapterDefinition>>(){
  }
,Names.named(""String_Node_Str""))));
  bind(Store.class).to(DefaultStore.class);
  bind(ArtifactStore.class).in(Scopes.SINGLETON);
  bind(AdapterService.class).in(Scopes.SINGLETON);
  bind(ProgramLifecycleService.class).in(Scopes.SINGLETON);
  bind(PluginRepository.class).in(Scopes.SINGLETON);
  bind(NamespaceAdmin.class).to(DefaultNamespaceAdmin.class).in(Scopes.SINGLETON);
  Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,Names.named(Constants.AppFabric.HANDLERS_BINDING));
  CommonHandlers.add(handlerBinder);
  handlerBinder.addBinding().to(ConfigHandler.class);
  handlerBinder.addBinding().to(AppFabricDataHttpHandler.class);
  handlerBinder.addBinding().to(VersionHandler.class);
  handlerBinder.addBinding().to(MonitorHandler.class);
  handlerBinder.addBinding().to(UsageHandler.class);
  handlerBinder.addBinding().to(NamespaceHttpHandler.class);
  handlerBinder.addBinding().to(NotificationFeedHttpHandler.class);
  handlerBinder.addBinding().to(AppLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(DashboardHttpHandler.class);
  handlerBinder.addBinding().to(ProgramLifecycleHttpHandler.class);
  handlerBinder.addBinding().to(PreferencesHttpHandler.class);
  handlerBinder.addBinding().to(ConsoleSettingsHttpHandler.class);
  handlerBinder.addBinding().to(TransactionHttpHandler.class);
  handlerBinder.addBinding().to(AdapterHttpHandler.class);
  handlerBinder.addBinding().to(ApplicationTemplateHandler.class);
  handlerBinder.addBinding().to(WorkflowHttpHandler.class);
  for (  Class<? extends HttpHandler> handlerClass : handlerClasses) {
    handlerBinder.addBinding().to(handlerClass);
  }
}"
6874,"public ArtifactMeta(List<PluginClass> plugins){
  this.plugins=plugins;
}","public ArtifactMeta(List<PluginClass> plugins){
  this.plugins=ImmutableList.copyOf(plugins);
}"
6875,"/** 
 * Write the artifact and its metadata to the store. Once added, artifacts cannot be changed. TODO: add support for snapshot versions, which can be changed
 * @param artifactId the id of the artifact to add
 * @param artifactMeta the metadata for the artifact
 * @param archiveContents the contents of the artifact
 * @throws WriteConflictException if the artifact is already currently being written
 * @throws ArtifactAlreadyExistsException if a non-snapshot version of the artifact already exists
 * @throws IOException if there was an exception persisting the artifact contents to the filesystem,of persisting the artifact metadata to the metastore
 */
public void write(Id.Artifact artifactId,ArtifactMeta artifactMeta,InputStream archiveContents) throws WriteConflictException, ArtifactAlreadyExistsException, IOException {
  ArtifactMeta meta=readMeta(artifactId);
  if (meta != null) {
    throw new ArtifactAlreadyExistsException(artifactId);
  }
  Location fileDirectory=locationFactory.get(artifactId.getNamespace(),ARTIFACTS_PATH).append(artifactId.getName());
  Locations.mkdirsIfNotExists(fileDirectory);
  Location lock=fileDirectory.append(artifactId.getVersion() + ""String_Node_Str"");
  if (!lock.createNew()) {
    throw new WriteConflictException(artifactId);
  }
  Location file=fileDirectory.append(artifactId.getVersion());
  if (file.exists()) {
    file.delete();
  }
  try {
    ByteStreams.copy(archiveContents,file.getOutputStream());
    try {
      writeMeta(artifactId,artifactMeta);
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"" + artifactId,e);
      file.delete();
      throw new IOException(e);
    }
  }
  finally {
    lock.delete();
  }
}","/** 
 * Write the artifact and its metadata to the store. Once added, artifacts cannot be changed. TODO: add support for snapshot versions, which can be changed
 * @param artifactId the id of the artifact to add
 * @param artifactMeta the metadata for the artifact
 * @param archiveContents the contents of the artifact
 * @throws WriteConflictException if the artifact is already currently being written
 * @throws ArtifactAlreadyExistsException if a non-snapshot version of the artifact already exists
 * @throws IOException if there was an exception persisting the artifact contents to the filesystem,of persisting the artifact metadata to the metastore
 */
public void write(Id.Artifact artifactId,ArtifactMeta artifactMeta,InputStream archiveContents) throws WriteConflictException, ArtifactAlreadyExistsException, IOException {
  Location fileDirectory=locationFactory.get(artifactId.getNamespace(),ARTIFACTS_PATH).append(artifactId.getName());
  Locations.mkdirsIfNotExists(fileDirectory);
  Location lock=fileDirectory.append(artifactId.getVersion() + ""String_Node_Str"");
  if (!lock.createNew()) {
    throw new WriteConflictException(artifactId);
  }
  ArtifactMeta meta=readMeta(artifactId);
  if (meta != null) {
    lock.delete();
    throw new ArtifactAlreadyExistsException(artifactId);
  }
  Location file=fileDirectory.append(artifactId.getVersion());
  if (file.exists()) {
    file.delete();
  }
  try {
    ByteStreams.copy(archiveContents,file.getOutputStream());
    try {
      writeMeta(artifactId,artifactMeta);
    }
 catch (    Exception e) {
      LOG.error(""String_Node_Str"" + artifactId,e);
      file.delete();
      throw new IOException(e);
    }
  }
  finally {
    lock.delete();
  }
}"
6876,"@Category(SlowTests.class) @Test public void testConcurrentAdd() throws Exception {
  int numThreads=10;
  final Id.Artifact artifactId=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  final List<String> successfulWriters=Collections.synchronizedList(Lists.<String>newArrayList());
  final CyclicBarrier barrier=new CyclicBarrier(numThreads);
  final CountDownLatch latch=new CountDownLatch(numThreads);
  ExecutorService executorService=Executors.newFixedThreadPool(numThreads);
  for (int i=0; i < numThreads; i++) {
    final String writer=String.valueOf(i);
    executorService.execute(new Runnable(){
      @Override public void run(){
        try {
          barrier.await();
          ArtifactMeta meta=new ArtifactMeta(ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"" + writer,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of())));
          artifactStore.write(artifactId,meta,new ByteArrayInputStream(Bytes.toBytes(writer)));
          successfulWriters.add(writer);
        }
 catch (        InterruptedException|BrokenBarrierException|IOException e) {
          throw new RuntimeException(e);
        }
catch (        ArtifactAlreadyExistsException|WriteConflictException e) {
        }
 finally {
          latch.countDown();
        }
      }
    }
);
  }
  latch.await();
  Assert.assertEquals(1,successfulWriters.size());
  String successfulWriter=successfulWriters.get(0);
  ArtifactInfo info=artifactStore.getArtifact(artifactId);
  ArtifactMeta expectedMeta=new ArtifactMeta(ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"" + successfulWriter,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of())));
  assertEqual(artifactId,expectedMeta,String.valueOf(successfulWriter),info);
}","@Category(SlowTests.class) @Test public void testConcurrentAdd() throws Exception {
  int numThreads=20;
  final Id.Artifact artifactId=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  final List<String> successfulWriters=Collections.synchronizedList(Lists.<String>newArrayList());
  final CyclicBarrier barrier=new CyclicBarrier(numThreads);
  final CountDownLatch latch=new CountDownLatch(numThreads);
  ExecutorService executorService=Executors.newFixedThreadPool(numThreads);
  for (int i=0; i < numThreads; i++) {
    final String writer=String.valueOf(i);
    executorService.execute(new Runnable(){
      @Override public void run(){
        try {
          barrier.await();
          ArtifactMeta meta=new ArtifactMeta(ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"" + writer,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of())));
          artifactStore.write(artifactId,meta,new ByteArrayInputStream(Bytes.toBytes(writer)));
          successfulWriters.add(writer);
        }
 catch (        InterruptedException|BrokenBarrierException|IOException e) {
          throw new RuntimeException(e);
        }
catch (        ArtifactAlreadyExistsException|WriteConflictException e) {
        }
 finally {
          latch.countDown();
        }
      }
    }
);
  }
  latch.await();
  Assert.assertEquals(1,successfulWriters.size());
  String successfulWriter=successfulWriters.get(0);
  ArtifactInfo info=artifactStore.getArtifact(artifactId);
  ArtifactMeta expectedMeta=new ArtifactMeta(ImmutableList.of(new PluginClass(""String_Node_Str"",""String_Node_Str"" + successfulWriter,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",ImmutableMap.<String,PluginPropertyField>of())));
  assertEqual(artifactId,expectedMeta,String.valueOf(successfulWriter),info);
}"
6877,"@Test(expected=ArtifactAlreadyExistsException.class) public void testImmutability() throws Exception {
  Id.Artifact artifactId=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta artifactMeta=new ArtifactMeta(ImmutableList.<PluginClass>of());
  String artifactContents=""String_Node_Str"";
  artifactStore.write(artifactId,artifactMeta,new ByteArrayInputStream(Bytes.toBytes(artifactContents)));
  artifactStore.write(artifactId,artifactMeta,new ByteArrayInputStream(Bytes.toBytes(artifactContents)));
}","@Test(expected=ArtifactAlreadyExistsException.class) public void testImmutability() throws Exception {
  Id.Artifact artifactId=Id.Artifact.from(Constants.DEFAULT_NAMESPACE_ID,""String_Node_Str"",""String_Node_Str"");
  ArtifactMeta artifactMeta=new ArtifactMeta(ImmutableList.<PluginClass>of());
  String artifactContents=""String_Node_Str"";
  try {
    artifactStore.write(artifactId,artifactMeta,new ByteArrayInputStream(Bytes.toBytes(artifactContents)));
  }
 catch (  ArtifactAlreadyExistsException e) {
    Assert.fail();
  }
  artifactStore.write(artifactId,artifactMeta,new ByteArrayInputStream(Bytes.toBytes(artifactContents)));
}"
6878,"@Override public void destroy(){
  DBUtils.cleanup(driverClass);
}","@Override public void destroy(){
  try {
    DriverManager.deregisterDriver(driverShim);
  }
 catch (  SQLException e) {
    LOG.warn(""String_Node_Str"",e);
    throw Throwables.propagate(e);
  }
  DBUtils.cleanup(driverClass);
}"
6879,"@Override public void initialize(BatchSinkContext context) throws Exception {
  super.initialize(context);
  setResultSetMetadata(context);
  driverClass=context.loadPluginClass(getJDBCPluginId());
}","@Override public void initialize(BatchSinkContext context) throws Exception {
  super.initialize(context);
  driverClass=context.loadPluginClass(getJDBCPluginId());
  setResultSetMetadata();
}"
6880,"private void setResultSetMetadata(BatchSinkContext context) throws Exception {
  ensureJDBCDriverIsAvailable(context);
  Connection connection;
  if (dbSinkConfig.user == null) {
    connection=DriverManager.getConnection(dbSinkConfig.connectionString);
  }
 else {
    connection=DriverManager.getConnection(dbSinkConfig.connectionString,dbSinkConfig.user,dbSinkConfig.password);
  }
  try {
    try (Statement statement=connection.createStatement();ResultSet rs=statement.executeQuery(String.format(""String_Node_Str"",dbSinkConfig.columns,dbSinkConfig.tableName))){
      resultSetMetadata=rs.getMetaData();
    }
   }
  finally {
    connection.close();
  }
}","private void setResultSetMetadata() throws Exception {
  ensureJDBCDriverIsAvailable();
  Connection connection;
  if (dbSinkConfig.user == null) {
    connection=DriverManager.getConnection(dbSinkConfig.connectionString);
  }
 else {
    connection=DriverManager.getConnection(dbSinkConfig.connectionString,dbSinkConfig.user,dbSinkConfig.password);
  }
  try {
    try (Statement statement=connection.createStatement();ResultSet rs=statement.executeQuery(String.format(""String_Node_Str"",dbSinkConfig.columns,dbSinkConfig.tableName))){
      resultSetMetadata=rs.getMetaData();
    }
   }
  finally {
    connection.close();
  }
}"
6881,"/** 
 * Ensures that the JDBC driver is available for   {@link DriverManager}
 * @throws Exception if the driver is not available
 */
private void ensureJDBCDriverIsAvailable(BatchSinkContext context) throws Exception {
  try {
    DriverManager.getDriver(dbSinkConfig.connectionString);
  }
 catch (  SQLException e) {
    Class<?> driverClass=context.loadPluginClass(getJDBCPluginId());
    LOG.debug(""String_Node_Str"",dbSinkConfig.jdbcPluginType,dbSinkConfig.jdbcPluginName,driverClass.getName(),JDBCDriverShim.class.getName());
    DriverManager.registerDriver(new JDBCDriverShim((Driver)driverClass.newInstance()));
  }
}","/** 
 * Ensures that the JDBC driver is available for   {@link DriverManager}
 * @throws Exception if the driver is not available
 */
private void ensureJDBCDriverIsAvailable() throws Exception {
  try {
    DriverManager.getDriver(dbSinkConfig.connectionString);
  }
 catch (  SQLException e) {
    LOG.debug(""String_Node_Str"",dbSinkConfig.jdbcPluginType,dbSinkConfig.jdbcPluginName,driverClass.getName(),JDBCDriverShim.class.getName());
    driverShim=new JDBCDriverShim(driverClass.newInstance());
    DBUtils.deregisterAllDrivers(driverClass);
    DriverManager.registerDriver(driverShim);
  }
}"
6882,"@Override public void destroy(){
  ETLDBInputFormat.deregisterDrivers();
  DBUtils.cleanup(driverClass);
}","@Override public void destroy(){
  DBUtils.cleanup(driverClass);
}"
6883,"@Override public Connection getConnection(){
  if (this.connection == null) {
    Configuration conf=getConf();
    try {
      String url=conf.get(DBConfiguration.URL_PROPERTY);
      try {
        DriverManager.getDriver(url);
      }
 catch (      SQLException e) {
        if (driver == null) {
          ClassLoader classLoader=conf.getClassLoader();
          Class<? extends Driver> driverClass=(Class<? extends Driver>)classLoader.loadClass(conf.get(DBConfiguration.DRIVER_CLASS_PROPERTY));
          driver=driverClass.newInstance();
          DBUtils.deRegisterDriver(driverClass);
          driverShim=new JDBCDriverShim(driver);
          DriverManager.registerDriver(driverShim);
          LOG.info(""String_Node_Str"",driverShim,driverShim.hashCode(),driverShim.getClass().getName());
          LOG.info(""String_Node_Str"",driver,driver.hashCode(),driver.getClass().getName());
        }
      }
      if (conf.get(DBConfiguration.USERNAME_PROPERTY) == null) {
        this.connection=DriverManager.getConnection(url);
      }
 else {
        this.connection=DriverManager.getConnection(url,conf.get(DBConfiguration.USERNAME_PROPERTY),conf.get(DBConfiguration.PASSWORD_PROPERTY));
      }
      this.connection.setAutoCommit(false);
      this.connection.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);
    }
 catch (    Exception e) {
      throw Throwables.propagate(e);
    }
  }
  return this.connection;
}","@Override public Connection getConnection(){
  if (this.connection == null) {
    Configuration conf=getConf();
    try {
      String url=conf.get(DBConfiguration.URL_PROPERTY);
      try {
        DriverManager.getDriver(url);
      }
 catch (      SQLException e) {
        if (driverShim == null) {
          if (driver == null) {
            ClassLoader classLoader=conf.getClassLoader();
            @SuppressWarnings(""String_Node_Str"") Class<? extends Driver> driverClass=(Class<? extends Driver>)classLoader.loadClass(conf.get(DBConfiguration.DRIVER_CLASS_PROPERTY));
            driver=driverClass.newInstance();
            DBUtils.deregisterAllDrivers(driverClass);
          }
          driverShim=new JDBCDriverShim(driver);
          DriverManager.registerDriver(driverShim);
          LOG.debug(""String_Node_Str"",driverShim,driver);
        }
      }
      if (conf.get(DBConfiguration.USERNAME_PROPERTY) == null) {
        this.connection=DriverManager.getConnection(url);
      }
 else {
        this.connection=DriverManager.getConnection(url,conf.get(DBConfiguration.USERNAME_PROPERTY),conf.get(DBConfiguration.PASSWORD_PROPERTY));
      }
      this.connection.setAutoCommit(false);
      this.connection.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);
    }
 catch (    Exception e) {
      throw Throwables.propagate(e);
    }
  }
  return this.connection;
}"
6884,"private Connection getConnection(Configuration conf){
  ClassLoader classLoader=conf.getClassLoader();
  Connection connection;
  try {
    Class<?> driverClass=classLoader.loadClass(conf.get(DBConfiguration.DRIVER_CLASS_PROPERTY));
    String url=conf.get(DBConfiguration.URL_PROPERTY);
    LOG.debug(""String_Node_Str"" + JDBCDriverShim.class.getName());
    DriverManager.registerDriver(new JDBCDriverShim((Driver)driverClass.newInstance()));
    if (conf.get(DBConfiguration.USERNAME_PROPERTY) == null) {
      connection=DriverManager.getConnection(url);
    }
 else {
      connection=DriverManager.getConnection(url,conf.get(DBConfiguration.USERNAME_PROPERTY),conf.get(DBConfiguration.PASSWORD_PROPERTY));
    }
    connection.setAutoCommit(false);
    connection.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
  return connection;
}","private Connection getConnection(Configuration conf){
  Connection connection;
  try {
    String url=conf.get(DBConfiguration.URL_PROPERTY);
    try {
      DriverManager.getDriver(url);
    }
 catch (    SQLException e) {
      if (driverShim == null) {
        if (driver == null) {
          ClassLoader classLoader=conf.getClassLoader();
          @SuppressWarnings(""String_Node_Str"") Class<? extends Driver> driverClass=(Class<? extends Driver>)classLoader.loadClass(conf.get(DBConfiguration.DRIVER_CLASS_PROPERTY));
          driver=driverClass.newInstance();
          DBUtils.deregisterAllDrivers(driverClass);
        }
        driverShim=new JDBCDriverShim(driver);
        DriverManager.registerDriver(driverShim);
        LOG.debug(""String_Node_Str"",driverShim,driver);
      }
    }
    if (conf.get(DBConfiguration.USERNAME_PROPERTY) == null) {
      connection=DriverManager.getConnection(url);
    }
 else {
      connection=DriverManager.getConnection(url,conf.get(DBConfiguration.USERNAME_PROPERTY),conf.get(DBConfiguration.PASSWORD_PROPERTY));
    }
    connection.setAutoCommit(false);
    connection.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);
  }
 catch (  Exception e) {
    throw Throwables.propagate(e);
  }
  return connection;
}"
6885,"@Override public RecordWriter<K,V> getRecordWriter(TaskAttemptContext context) throws IOException {
  Configuration conf=context.getConfiguration();
  DBConfiguration dbConf=new DBConfiguration(conf);
  String tableName=dbConf.getOutputTableName();
  String[] fieldNames=dbConf.getOutputFieldNames();
  if (fieldNames == null) {
    fieldNames=new String[dbConf.getOutputFieldCount()];
  }
  try {
    Connection connection=getConnection(conf);
    PreparedStatement statement=connection.prepareStatement(constructQuery(tableName,fieldNames));
    return new DBRecordWriter(connection,statement);
  }
 catch (  Exception ex) {
    throw new IOException(ex.getMessage());
  }
}","@Override public RecordWriter<K,V> getRecordWriter(TaskAttemptContext context) throws IOException {
  Configuration conf=context.getConfiguration();
  DBConfiguration dbConf=new DBConfiguration(conf);
  String tableName=dbConf.getOutputTableName();
  String[] fieldNames=dbConf.getOutputFieldNames();
  if (fieldNames == null) {
    fieldNames=new String[dbConf.getOutputFieldCount()];
  }
  try {
    Connection connection=getConnection(conf);
    PreparedStatement statement=connection.prepareStatement(constructQuery(tableName,fieldNames));
    return new DBRecordWriter(connection,statement){
      @Override public void close(      TaskAttemptContext context) throws IOException {
        super.close(context);
        try {
          DriverManager.deregisterDriver(driverShim);
        }
 catch (        SQLException e) {
          throw new IOException(e);
        }
      }
    }
;
  }
 catch (  Exception ex) {
    throw new IOException(ex.getMessage());
  }
}"
6886,"@Override public void start(){
  createSystemHBaseNamespace();
  updateConfigurationTable();
  LogAppenderInitializer logAppenderInitializer=baseInjector.getInstance(LogAppenderInitializer.class);
  logAppenderInitializer.initialize();
  zkClient.startAndWait();
  Futures.getUnchecked(ZKOperations.ignoreError(zkClient.create(""String_Node_Str"",null,CreateMode.PERSISTENT),KeeperException.NodeExistsException.class,null));
  twillRunner.startAndWait();
  kafkaClient.startAndWait();
  metricsCollectionService.startAndWait();
  serviceStore.startAndWait();
  leaderElection.startAndWait();
}","@Override public void start(){
  try {
    URLConnections.setDefaultUseCaches(false);
  }
 catch (  IOException e) {
    LOG.error(""String_Node_Str"",e);
  }
  createSystemHBaseNamespace();
  updateConfigurationTable();
  LogAppenderInitializer logAppenderInitializer=baseInjector.getInstance(LogAppenderInitializer.class);
  logAppenderInitializer.initialize();
  zkClient.startAndWait();
  Futures.getUnchecked(ZKOperations.ignoreError(zkClient.create(""String_Node_Str"",null,CreateMode.PERSISTENT),KeeperException.NodeExistsException.class,null));
  twillRunner.startAndWait();
  kafkaClient.startAndWait();
  metricsCollectionService.startAndWait();
  serviceStore.startAndWait();
  leaderElection.startAndWait();
}"
6887,"/** 
 * Start the service.
 */
public void startUp() throws Exception {
  cleanupTempDir();
  txService.startAndWait();
  metricsCollectionService.startAndWait();
  datasetService.startAndWait();
  serviceStore.startAndWait();
  streamService.startAndWait();
  logAppenderInitializer.initialize();
  Service.State state=appFabricServer.startAndWait();
  if (state != Service.State.RUNNING) {
    throw new Exception(""String_Node_Str"");
  }
  metricsQueryService.startAndWait();
  router.startAndWait();
  if (userInterfaceService != null) {
    userInterfaceService.startAndWait();
  }
  if (securityEnabled) {
    externalAuthenticationServer.startAndWait();
  }
  if (exploreExecutorService != null) {
    exploreExecutorService.startAndWait();
  }
  String protocol=sslEnabled ? ""String_Node_Str"" : ""String_Node_Str"";
  int dashboardPort=sslEnabled ? configuration.getInt(Constants.Dashboard.SSL_BIND_PORT) : configuration.getInt(Constants.Dashboard.BIND_PORT);
  System.out.println(""String_Node_Str"");
  System.out.printf(""String_Node_Str"",protocol,""String_Node_Str"",dashboardPort);
}","/** 
 * Start the service.
 */
public void startUp() throws Exception {
  URLConnections.setDefaultUseCaches(false);
  cleanupTempDir();
  txService.startAndWait();
  metricsCollectionService.startAndWait();
  datasetService.startAndWait();
  serviceStore.startAndWait();
  streamService.startAndWait();
  logAppenderInitializer.initialize();
  Service.State state=appFabricServer.startAndWait();
  if (state != Service.State.RUNNING) {
    throw new Exception(""String_Node_Str"");
  }
  metricsQueryService.startAndWait();
  router.startAndWait();
  if (userInterfaceService != null) {
    userInterfaceService.startAndWait();
  }
  if (securityEnabled) {
    externalAuthenticationServer.startAndWait();
  }
  if (exploreExecutorService != null) {
    exploreExecutorService.startAndWait();
  }
  String protocol=sslEnabled ? ""String_Node_Str"" : ""String_Node_Str"";
  int dashboardPort=sslEnabled ? configuration.getInt(Constants.Dashboard.SSL_BIND_PORT) : configuration.getInt(Constants.Dashboard.BIND_PORT);
  System.out.println(""String_Node_Str"");
  System.out.printf(""String_Node_Str"",protocol,""String_Node_Str"",dashboardPort);
}"
6888,"private void executeFork(final ApplicationSpecification appSpec,WorkflowForkNode fork,final InstantiatorFactory instantiator,final ClassLoader classLoader,final WorkflowToken token) throws Exception {
  ExecutorService executorService=Executors.newFixedThreadPool(fork.getBranches().size());
  CompletionService<Map.Entry<String,WorkflowToken>> completionService=new ExecutorCompletionService<Map.Entry<String,WorkflowToken>>(executorService);
  try {
    for (    final List<WorkflowNode> branch : fork.getBranches()) {
      completionService.submit(new Callable<Map.Entry<String,WorkflowToken>>(){
        @Override public Map.Entry<String,WorkflowToken> call() throws Exception {
          WorkflowToken copiedToken=((BasicWorkflowToken)token).deepCopy();
          executeAll(branch.iterator(),appSpec,instantiator,classLoader,copiedToken);
          return Maps.immutableEntry(branch.toString(),copiedToken);
        }
      }
);
    }
    boolean assignedCounters=false;
    for (int i=0; i < fork.getBranches().size(); i++) {
      try {
        Future<Map.Entry<String,WorkflowToken>> f=completionService.take();
        Map.Entry<String,WorkflowToken> retValue=f.get();
        String branchInfo=retValue.getKey();
        WorkflowToken branchToken=retValue.getValue();
        if (!assignedCounters && branchToken.getMapReduceCounters() != null) {
          ((BasicWorkflowToken)token).setMapReduceCounters(branchToken.getMapReduceCounters());
          assignedCounters=true;
        }
        LOG.info(""String_Node_Str"",branchInfo,fork);
      }
 catch (      Throwable t) {
        Throwable rootCause=Throwables.getRootCause(t);
        if (rootCause instanceof ExecutionException) {
          LOG.error(""String_Node_Str"",fork);
          throw (ExecutionException)t;
        }
        if (rootCause instanceof InterruptedException) {
          LOG.error(""String_Node_Str"");
          break;
        }
        Throwables.propagateIfPossible(t,Exception.class);
        throw Throwables.propagate(t);
      }
    }
  }
  finally {
    executorService.shutdownNow();
    executorService.awaitTermination(Integer.MAX_VALUE,TimeUnit.NANOSECONDS);
  }
}","private void executeFork(final ApplicationSpecification appSpec,WorkflowForkNode fork,final InstantiatorFactory instantiator,final ClassLoader classLoader,final WorkflowToken token) throws Exception {
  ExecutorService executorService=Executors.newFixedThreadPool(fork.getBranches().size(),new ThreadFactoryBuilder().setNameFormat(""String_Node_Str"").build());
  CompletionService<Map.Entry<String,WorkflowToken>> completionService=new ExecutorCompletionService<Map.Entry<String,WorkflowToken>>(executorService);
  try {
    for (    final List<WorkflowNode> branch : fork.getBranches()) {
      completionService.submit(new Callable<Map.Entry<String,WorkflowToken>>(){
        @Override public Map.Entry<String,WorkflowToken> call() throws Exception {
          WorkflowToken copiedToken=((BasicWorkflowToken)token).deepCopy();
          executeAll(branch.iterator(),appSpec,instantiator,classLoader,copiedToken);
          return Maps.immutableEntry(branch.toString(),copiedToken);
        }
      }
);
    }
    boolean assignedCounters=false;
    for (int i=0; i < fork.getBranches().size(); i++) {
      try {
        Future<Map.Entry<String,WorkflowToken>> f=completionService.take();
        Map.Entry<String,WorkflowToken> retValue=f.get();
        String branchInfo=retValue.getKey();
        WorkflowToken branchToken=retValue.getValue();
        if (!assignedCounters && branchToken.getMapReduceCounters() != null) {
          ((BasicWorkflowToken)token).setMapReduceCounters(branchToken.getMapReduceCounters());
          assignedCounters=true;
        }
        LOG.info(""String_Node_Str"",branchInfo,fork);
      }
 catch (      Throwable t) {
        Throwable rootCause=Throwables.getRootCause(t);
        if (rootCause instanceof ExecutionException) {
          LOG.error(""String_Node_Str"",fork);
          throw (ExecutionException)t;
        }
        if (rootCause instanceof InterruptedException) {
          LOG.error(""String_Node_Str"");
          break;
        }
        Throwables.propagateIfPossible(t,Exception.class);
        throw Throwables.propagate(t);
      }
    }
  }
  finally {
    executorService.shutdownNow();
    executorService.awaitTermination(Integer.MAX_VALUE,TimeUnit.NANOSECONDS);
  }
}"
6889,"private void executeAction(ApplicationSpecification appSpec,WorkflowActionNode node,InstantiatorFactory instantiator,ClassLoader classLoader,WorkflowToken token) throws Exception {
  final WorkflowActionSpecification actionSpec;
  ScheduleProgramInfo actionInfo=node.getProgram();
switch (actionInfo.getProgramType()) {
case MAPREDUCE:
    MapReduceSpecification mapReduceSpec=appSpec.getMapReduce().get(actionInfo.getProgramName());
  String mapReduce=mapReduceSpec.getName();
actionSpec=new DefaultWorkflowActionSpecification(new ProgramWorkflowAction(mapReduce,mapReduce,SchedulableProgramType.MAPREDUCE));
break;
case SPARK:
SparkSpecification sparkSpec=appSpec.getSpark().get(actionInfo.getProgramName());
String spark=sparkSpec.getName();
actionSpec=new DefaultWorkflowActionSpecification(new ProgramWorkflowAction(spark,spark,SchedulableProgramType.SPARK));
break;
case CUSTOM_ACTION:
actionSpec=node.getActionSpecification();
break;
default :
LOG.error(""String_Node_Str"",actionInfo.getProgramType(),actionInfo.getProgramName());
throw new IllegalStateException(""String_Node_Str"");
}
status.put(node.getNodeId(),node);
final WorkflowAction action=initialize(actionSpec,classLoader,instantiator,token,node.getNodeId());
ExecutorService executor=Executors.newSingleThreadExecutor();
try {
Future<?> future=executor.submit(new Runnable(){
@Override public void run(){
ClassLoaders.setContextClassLoader(action.getClass().getClassLoader());
try {
action.run();
}
  finally {
destroy(actionSpec,action);
}
}
}
);
future.get();
}
 catch (Throwable t) {
LOG.error(""String_Node_Str"",actionSpec);
Throwables.propagateIfPossible(t,Exception.class);
throw Throwables.propagate(t);
}
 finally {
executor.shutdownNow();
status.remove(node.getNodeId());
}
}","private void executeAction(ApplicationSpecification appSpec,WorkflowActionNode node,InstantiatorFactory instantiator,ClassLoader classLoader,WorkflowToken token) throws Exception {
  final WorkflowActionSpecification actionSpec;
  ScheduleProgramInfo actionInfo=node.getProgram();
switch (actionInfo.getProgramType()) {
case MAPREDUCE:
    MapReduceSpecification mapReduceSpec=appSpec.getMapReduce().get(actionInfo.getProgramName());
  String mapReduce=mapReduceSpec.getName();
actionSpec=new DefaultWorkflowActionSpecification(new ProgramWorkflowAction(mapReduce,mapReduce,SchedulableProgramType.MAPREDUCE));
break;
case SPARK:
SparkSpecification sparkSpec=appSpec.getSpark().get(actionInfo.getProgramName());
String spark=sparkSpec.getName();
actionSpec=new DefaultWorkflowActionSpecification(new ProgramWorkflowAction(spark,spark,SchedulableProgramType.SPARK));
break;
case CUSTOM_ACTION:
actionSpec=node.getActionSpecification();
break;
default :
LOG.error(""String_Node_Str"",actionInfo.getProgramType(),actionInfo.getProgramName());
throw new IllegalStateException(""String_Node_Str"");
}
status.put(node.getNodeId(),node);
final WorkflowAction action=initialize(actionSpec,classLoader,instantiator,token,node.getNodeId());
ExecutorService executor=Executors.newSingleThreadExecutor(new ThreadFactoryBuilder().setNameFormat(""String_Node_Str"").build());
try {
Future<?> future=executor.submit(new Runnable(){
@Override public void run(){
ClassLoaders.setContextClassLoader(action.getClass().getClassLoader());
try {
action.run();
}
  finally {
destroy(actionSpec,action);
}
}
}
);
future.get();
}
 catch (Throwable t) {
LOG.error(""String_Node_Str"",actionSpec);
Throwables.propagateIfPossible(t,Exception.class);
throw Throwables.propagate(t);
}
 finally {
executor.shutdownNow();
status.remove(node.getNodeId());
}
}"
6890,"@Override public void prepareRun(BatchSinkContext context){
  LOG.debug(""String_Node_Str"",dbSinkConfig.tableName,dbSinkConfig.jdbcPluginType,dbSinkConfig.jdbcPluginName,dbSinkConfig.connectionString,dbSinkConfig.columns);
  Job job=context.getHadoopJob();
  conf=job.getConfiguration();
  Class<?> driverClass=context.loadPluginClass(getJDBCPluginId());
  if (dbSinkConfig.user == null && dbSinkConfig.password == null) {
    DBConfiguration.configureDB(conf,driverClass.getName(),dbSinkConfig.connectionString);
  }
 else {
    DBConfiguration.configureDB(conf,driverClass.getName(),dbSinkConfig.connectionString,dbSinkConfig.user,dbSinkConfig.password);
  }
  List<String> fields=Lists.newArrayList(Splitter.on(""String_Node_Str"").omitEmptyStrings().split(dbSinkConfig.columns));
  try {
    ETLDBOutputFormat.setOutput(job,dbSinkConfig.tableName,fields.toArray(new String[fields.size()]));
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
  job.setOutputFormatClass(ETLDBOutputFormat.class);
}","@Override public void prepareRun(BatchSinkContext context){
  LOG.debug(""String_Node_Str"",dbSinkConfig.tableName,dbSinkConfig.jdbcPluginType,dbSinkConfig.jdbcPluginName,dbSinkConfig.connectionString,dbSinkConfig.columns);
  Job job=context.getHadoopJob();
  Configuration hConf=job.getConfiguration();
  Class<? extends Driver> driverClass=context.loadPluginClass(getJDBCPluginId());
  if (dbSinkConfig.user == null && dbSinkConfig.password == null) {
    DBConfiguration.configureDB(hConf,driverClass.getName(),dbSinkConfig.connectionString);
  }
 else {
    DBConfiguration.configureDB(hConf,driverClass.getName(),dbSinkConfig.connectionString,dbSinkConfig.user,dbSinkConfig.password);
  }
  List<String> fields=Lists.newArrayList(Splitter.on(""String_Node_Str"").omitEmptyStrings().split(dbSinkConfig.columns));
  try {
    ETLDBOutputFormat.setOutput(job,dbSinkConfig.tableName,fields.toArray(new String[fields.size()]));
  }
 catch (  IOException e) {
    throw Throwables.propagate(e);
  }
  job.setOutputFormatClass(ETLDBOutputFormat.class);
}"
6891,"@Override public void initialize(BatchSinkContext context) throws Exception {
  super.initialize(context);
  setResultSetMetadata(context);
}","@Override public void initialize(BatchSinkContext context) throws Exception {
  super.initialize(context);
  setResultSetMetadata(context);
  driverClass=context.loadPluginClass(getJDBCPluginId());
}"
6892,"private void setResultSetMetadata(BatchSinkContext context) throws Exception {
  ensureJDBCDriverIsAvailable(context);
  Connection connection;
  if (dbSinkConfig.user == null) {
    connection=DriverManager.getConnection(dbSinkConfig.connectionString);
  }
 else {
    connection=DriverManager.getConnection(dbSinkConfig.connectionString,dbSinkConfig.user,dbSinkConfig.password);
  }
  try {
    Statement statement=connection.createStatement();
    try {
      ResultSet rs=statement.executeQuery(String.format(""String_Node_Str"",dbSinkConfig.columns,dbSinkConfig.tableName));
      try {
        resultSetMetadata=rs.getMetaData();
      }
  finally {
        rs.close();
      }
    }
  finally {
      statement.close();
    }
  }
  finally {
    connection.close();
  }
}","private void setResultSetMetadata(BatchSinkContext context) throws Exception {
  ensureJDBCDriverIsAvailable(context);
  Connection connection;
  if (dbSinkConfig.user == null) {
    connection=DriverManager.getConnection(dbSinkConfig.connectionString);
  }
 else {
    connection=DriverManager.getConnection(dbSinkConfig.connectionString,dbSinkConfig.user,dbSinkConfig.password);
  }
  try {
    try (Statement statement=connection.createStatement();ResultSet rs=statement.executeQuery(String.format(""String_Node_Str"",dbSinkConfig.columns,dbSinkConfig.tableName))){
      resultSetMetadata=rs.getMetaData();
    }
   }
  finally {
    connection.close();
  }
}"
6893,"@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  Preconditions.checkArgument(!(dbSinkConfig.user == null && dbSinkConfig.password != null),""String_Node_Str"" + ""String_Node_Str"");
  Preconditions.checkArgument(!(dbSinkConfig.user != null && dbSinkConfig.password == null),""String_Node_Str"" + ""String_Node_Str"");
  Class<Object> jdbcDriverClass=pipelineConfigurer.usePluginClass(dbSinkConfig.jdbcPluginType,dbSinkConfig.jdbcPluginName,getJDBCPluginId(),PluginProperties.builder().build());
  Preconditions.checkArgument(jdbcDriverClass != null,""String_Node_Str"");
}","@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  Preconditions.checkArgument(!(dbSinkConfig.user == null && dbSinkConfig.password != null),""String_Node_Str"" + ""String_Node_Str"");
  Preconditions.checkArgument(!(dbSinkConfig.user != null && dbSinkConfig.password == null),""String_Node_Str"" + ""String_Node_Str"");
  Class<? extends Driver> jdbcDriverClass=pipelineConfigurer.usePluginClass(dbSinkConfig.jdbcPluginType,dbSinkConfig.jdbcPluginName,getJDBCPluginId(),PluginProperties.builder().build());
  Preconditions.checkArgument(jdbcDriverClass != null,""String_Node_Str"");
}"
6894,"@Override public void prepareRun(BatchSourceContext context){
  LOG.debug(""String_Node_Str"" + ""String_Node_Str"",dbSourceConfig.tableName,dbSourceConfig.jdbcPluginType,dbSourceConfig.jdbcPluginName,dbSourceConfig.connectionString,dbSourceConfig.importQuery,dbSourceConfig.countQuery);
  Job job=context.getHadoopJob();
  Configuration conf=job.getConfiguration();
  String jdbcPluginId=String.format(""String_Node_Str"",""String_Node_Str"",dbSourceConfig.jdbcPluginType,dbSourceConfig.jdbcPluginName);
  Class<?> driverClass=context.loadPluginClass(jdbcPluginId);
  if (dbSourceConfig.user == null && dbSourceConfig.password == null) {
    DBConfiguration.configureDB(conf,driverClass.getName(),dbSourceConfig.connectionString);
  }
 else {
    DBConfiguration.configureDB(conf,driverClass.getName(),dbSourceConfig.connectionString,dbSourceConfig.user,dbSourceConfig.password);
  }
  ETLDBInputFormat.setInput(job,DBRecord.class,dbSourceConfig.importQuery,dbSourceConfig.countQuery);
  job.setInputFormatClass(ETLDBInputFormat.class);
}","@Override public void prepareRun(BatchSourceContext context){
  LOG.debug(""String_Node_Str"" + ""String_Node_Str"",dbSourceConfig.tableName,dbSourceConfig.jdbcPluginType,dbSourceConfig.jdbcPluginName,dbSourceConfig.connectionString,dbSourceConfig.importQuery,dbSourceConfig.countQuery);
  Job job=context.getHadoopJob();
  Configuration hConf=job.getConfiguration();
  Class<? extends Driver> driverClass=context.loadPluginClass(getJDBCPluginId());
  if (dbSourceConfig.user == null && dbSourceConfig.password == null) {
    DBConfiguration.configureDB(hConf,driverClass.getName(),dbSourceConfig.connectionString);
  }
 else {
    DBConfiguration.configureDB(hConf,driverClass.getName(),dbSourceConfig.connectionString,dbSourceConfig.user,dbSourceConfig.password);
  }
  ETLDBInputFormat.setInput(job,DBRecord.class,dbSourceConfig.importQuery,dbSourceConfig.countQuery);
  job.setInputFormatClass(ETLDBInputFormat.class);
}"
6895,"@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  Preconditions.checkArgument(!(dbSourceConfig.user == null && dbSourceConfig.password != null),""String_Node_Str"" + ""String_Node_Str"");
  Preconditions.checkArgument(!(dbSourceConfig.user != null && dbSourceConfig.password == null),""String_Node_Str"" + ""String_Node_Str"");
  String jdbcPluginId=String.format(""String_Node_Str"",""String_Node_Str"",dbSourceConfig.jdbcPluginType,dbSourceConfig.jdbcPluginName);
  Class<Object> jdbcDriverClass=pipelineConfigurer.usePluginClass(dbSourceConfig.jdbcPluginType,dbSourceConfig.jdbcPluginName,jdbcPluginId,PluginProperties.builder().build());
  Preconditions.checkArgument(jdbcDriverClass != null,""String_Node_Str"");
}","@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  Preconditions.checkArgument(!(dbSourceConfig.user == null && dbSourceConfig.password != null),""String_Node_Str"" + ""String_Node_Str"");
  Preconditions.checkArgument(!(dbSourceConfig.user != null && dbSourceConfig.password == null),""String_Node_Str"" + ""String_Node_Str"");
  String jdbcPluginId=String.format(""String_Node_Str"",""String_Node_Str"",dbSourceConfig.jdbcPluginType,dbSourceConfig.jdbcPluginName);
  Class<? extends Driver> jdbcDriverClass=pipelineConfigurer.usePluginClass(dbSourceConfig.jdbcPluginType,dbSourceConfig.jdbcPluginName,jdbcPluginId,PluginProperties.builder().build());
  Preconditions.checkArgument(jdbcDriverClass != null,""String_Node_Str"");
}"
6896,"@Nullable private Object transformValue(int sqlColumnType,Object original) throws SQLException {
  if (original != null) {
switch (sqlColumnType) {
case Types.NUMERIC:
case Types.DECIMAL:
      return ((BigDecimal)original).doubleValue();
case Types.DATE:
    return ((Date)original).getTime();
case Types.TIME:
  return ((Time)original).getTime();
case Types.TIMESTAMP:
return ((Timestamp)original).getTime();
case Types.BLOB:
Object toReturn;
Blob blob=(Blob)original;
try {
toReturn=blob.getBytes(1,(int)blob.length());
}
  finally {
blob.free();
}
return toReturn;
case Types.CLOB:
String s;
StringBuffer sbf=new StringBuffer();
Clob clob=(Clob)original;
try {
BufferedReader br=new BufferedReader(clob.getCharacterStream(1,(int)clob.length()));
try {
while ((s=br.readLine()) != null) {
sbf.append(s);
sbf.append(System.getProperty(""String_Node_Str""));
}
}
  finally {
br.close();
}
}
 catch (IOException e) {
throw new SQLException(e);
}
 finally {
clob.free();
}
return sbf.toString();
}
}
return original;
}","@Nullable private Object transformValue(int sqlColumnType,Object original) throws SQLException {
  if (original != null) {
switch (sqlColumnType) {
case Types.NUMERIC:
case Types.DECIMAL:
      return ((BigDecimal)original).doubleValue();
case Types.DATE:
    return ((Date)original).getTime();
case Types.TIME:
  return ((Time)original).getTime();
case Types.TIMESTAMP:
return ((Timestamp)original).getTime();
case Types.BLOB:
Object toReturn;
Blob blob=(Blob)original;
try {
toReturn=blob.getBytes(1,(int)blob.length());
}
  finally {
blob.free();
}
return toReturn;
case Types.CLOB:
String s;
StringBuilder sbf=new StringBuilder();
Clob clob=(Clob)original;
try {
try (BufferedReader br=new BufferedReader(clob.getCharacterStream(1,(int)clob.length()))){
while ((s=br.readLine()) != null) {
sbf.append(s);
sbf.append(System.getProperty(""String_Node_Str""));
}
}
 }
 catch (IOException e) {
throw new SQLException(e);
}
 finally {
clob.free();
}
return sbf.toString();
}
}
return original;
}"
6897,"private DatasetInstanceConfiguration getInstanceConfiguration(HttpRequest request){
  Reader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()),Charsets.UTF_8);
  DatasetInstanceConfiguration creationProperties=GSON.fromJson(reader,DatasetInstanceConfiguration.class);
  fixProperties(creationProperties.getProperties());
  return creationProperties;
}","private DatasetInstanceConfiguration getInstanceConfiguration(HttpRequest request){
  Reader reader=new InputStreamReader(new ChannelBufferInputStream(request.getContent()),Charsets.UTF_8);
  DatasetInstanceConfiguration creationProperties=GSON.fromJson(reader,DatasetInstanceConfiguration.class);
  if (creationProperties.getProperties() == null) {
    creationProperties=new DatasetInstanceConfiguration(creationProperties.getTypeName());
  }
  fixProperties(creationProperties.getProperties());
  return creationProperties;
}"
6898,"private int createInstance(String instanceName,String typeName,DatasetProperties props) throws IOException {
  DatasetInstanceConfiguration creationProperties=new DatasetInstanceConfiguration(typeName,props.getProperties());
  HttpRequest request=HttpRequest.put(getUrl(""String_Node_Str"" + instanceName)).withBody(new Gson().toJson(creationProperties)).build();
  return HttpRequests.execute(request).getResponseCode();
}","private int createInstance(String instanceName,String typeName) throws IOException {
  DatasetInstanceConfiguration creationProperties=new DatasetInstanceConfiguration(typeName,null);
  HttpRequest request=HttpRequest.put(getUrl(""String_Node_Str"" + instanceName)).withBody(new Gson().toJson(creationProperties)).build();
  return HttpRequests.execute(request).getResponseCode();
}"
6899,"@Test public void testBasics() throws Exception {
  List<DatasetSpecificationSummary> instances=getInstances().getResponseObject();
  Assert.assertEquals(0,instances.size());
  try {
    DatasetProperties props=DatasetProperties.builder().add(""String_Node_Str"",""String_Node_Str"").build();
    Assert.assertEquals(HttpStatus.SC_NOT_FOUND,createInstance(""String_Node_Str"",""String_Node_Str"",props));
    deployModule(""String_Node_Str"",TestModule1.class);
    deployModule(""String_Node_Str"",TestModule2.class);
    Assert.assertEquals(HttpStatus.SC_OK,createInstance(""String_Node_Str"",""String_Node_Str"",props));
    int modulesBeforeDelete=getModules().getResponseObject().size();
    Assert.assertEquals(HttpStatus.SC_CONFLICT,deleteModule(""String_Node_Str""));
    Assert.assertEquals(HttpStatus.SC_CONFLICT,deleteModules());
    Assert.assertEquals(modulesBeforeDelete,getModules().getResponseObject().size());
    instances=getInstances().getResponseObject();
    Assert.assertEquals(1,instances.size());
    DatasetSpecification dataset1Spec=createSpec(""String_Node_Str"",""String_Node_Str"",props);
    Assert.assertEquals(spec2Summary(dataset1Spec),instances.get(0));
    DatasetMeta datasetInfo=getInstanceObject(""String_Node_Str"").getResponseObject();
    Assert.assertEquals(dataset1Spec,datasetInfo.getSpec());
    Assert.assertEquals(dataset1Spec.getType(),datasetInfo.getType().getName());
    List<DatasetModuleMeta> modules=datasetInfo.getType().getModules();
    Assert.assertEquals(2,modules.size());
    DatasetTypeHandlerTest.verify(modules.get(0),""String_Node_Str"",TestModule1.class,ImmutableList.of(""String_Node_Str""),Collections.<String>emptyList(),ImmutableList.of(""String_Node_Str""));
    DatasetTypeHandlerTest.verify(modules.get(1),""String_Node_Str"",TestModule2.class,ImmutableList.of(""String_Node_Str""),ImmutableList.of(""String_Node_Str""),Collections.<String>emptyList());
    Assert.assertEquals(HttpStatus.SC_NOT_FOUND,getInstance(""String_Node_Str"").getResponseCode());
    Assert.assertEquals(HttpStatus.SC_CONFLICT,createInstance(""String_Node_Str"",""String_Node_Str"",props));
    Assert.assertEquals(1,getInstances().getResponseObject().size());
    Assert.assertEquals(HttpStatus.SC_NOT_FOUND,deleteInstance(""String_Node_Str""));
    Assert.assertEquals(1,getInstances().getResponseObject().size());
  }
  finally {
    Assert.assertEquals(HttpStatus.SC_OK,deleteInstance(""String_Node_Str""));
    Assert.assertEquals(0,getInstances().getResponseObject().size());
    Assert.assertEquals(HttpStatus.SC_OK,deleteModule(""String_Node_Str""));
    Assert.assertEquals(HttpStatus.SC_OK,deleteModule(""String_Node_Str""));
  }
}","@Test public void testBasics() throws Exception {
  List<DatasetSpecificationSummary> instances=getInstances().getResponseObject();
  Assert.assertEquals(0,instances.size());
  try {
    DatasetProperties props=DatasetProperties.builder().add(""String_Node_Str"",""String_Node_Str"").build();
    Assert.assertEquals(HttpStatus.SC_NOT_FOUND,createInstance(""String_Node_Str"",""String_Node_Str"",props));
    deployModule(""String_Node_Str"",TestModule1.class);
    deployModule(""String_Node_Str"",TestModule2.class);
    Assert.assertEquals(HttpStatus.SC_OK,createInstance(""String_Node_Str"",""String_Node_Str"",props));
    int modulesBeforeDelete=getModules().getResponseObject().size();
    Assert.assertEquals(HttpStatus.SC_CONFLICT,deleteModule(""String_Node_Str""));
    Assert.assertEquals(HttpStatus.SC_CONFLICT,deleteModules());
    Assert.assertEquals(modulesBeforeDelete,getModules().getResponseObject().size());
    instances=getInstances().getResponseObject();
    Assert.assertEquals(1,instances.size());
    DatasetSpecification dataset1Spec=createSpec(""String_Node_Str"",""String_Node_Str"",props);
    Assert.assertEquals(spec2Summary(dataset1Spec),instances.get(0));
    DatasetMeta datasetInfo=getInstanceObject(""String_Node_Str"").getResponseObject();
    Assert.assertEquals(dataset1Spec,datasetInfo.getSpec());
    Assert.assertEquals(dataset1Spec.getType(),datasetInfo.getType().getName());
    List<DatasetModuleMeta> modules=datasetInfo.getType().getModules();
    Assert.assertEquals(2,modules.size());
    DatasetTypeHandlerTest.verify(modules.get(0),""String_Node_Str"",TestModule1.class,ImmutableList.of(""String_Node_Str""),Collections.<String>emptyList(),ImmutableList.of(""String_Node_Str""));
    DatasetTypeHandlerTest.verify(modules.get(1),""String_Node_Str"",TestModule2.class,ImmutableList.of(""String_Node_Str""),ImmutableList.of(""String_Node_Str""),Collections.<String>emptyList());
    Assert.assertEquals(HttpStatus.SC_NOT_FOUND,getInstance(""String_Node_Str"").getResponseCode());
    Assert.assertEquals(HttpStatus.SC_CONFLICT,createInstance(""String_Node_Str"",""String_Node_Str"",props));
    Assert.assertEquals(1,getInstances().getResponseObject().size());
    Assert.assertEquals(HttpStatus.SC_NOT_FOUND,deleteInstance(""String_Node_Str""));
    Assert.assertEquals(1,getInstances().getResponseObject().size());
    Assert.assertEquals(HttpStatus.SC_OK,createInstance(""String_Node_Str"",""String_Node_Str""));
  }
  finally {
    Assert.assertEquals(HttpStatus.SC_OK,deleteInstance(""String_Node_Str""));
    Assert.assertEquals(HttpStatus.SC_OK,deleteInstance(""String_Node_Str""));
    Assert.assertEquals(0,getInstances().getResponseObject().size());
    Assert.assertEquals(HttpStatus.SC_OK,deleteModule(""String_Node_Str""));
    Assert.assertEquals(HttpStatus.SC_OK,deleteModule(""String_Node_Str""));
  }
}"
6900,"private String generateFileSetCreateStatement(Id.DatasetInstance datasetID,Dataset dataset,Map<String,String> properties) throws IllegalArgumentException {
  String tableName=getDatasetTableName(datasetID);
  Map<String,String> tableProperties=FileSetProperties.getTableProperties(properties);
  Location baseLocation;
  Partitioning partitioning=null;
  if (dataset instanceof PartitionedFileSet) {
    partitioning=((PartitionedFileSet)dataset).getPartitioning();
    baseLocation=((PartitionedFileSet)dataset).getEmbeddedFileSet().getBaseLocation();
  }
 else {
    baseLocation=((FileSet)dataset).getBaseLocation();
  }
  CreateStatementBuilder createStatementBuilder=new CreateStatementBuilder(datasetID.getId(),tableName).setLocation(baseLocation).setPartitioning(partitioning).setTableProperties(tableProperties);
  String format=FileSetProperties.getExploreFormat(properties);
  if (format != null) {
    if (""String_Node_Str"".equals(format)) {
      return createStatementBuilder.setSchema(FileSetProperties.getExploreSchema(properties)).buildWithFileFormat(""String_Node_Str"");
    }
 else {
      Preconditions.checkArgument(""String_Node_Str"".equals(format) || ""String_Node_Str"".equals(format) || ""String_Node_Str"".equals(format),""String_Node_Str"");
      String schema=FileSetProperties.getExploreSchema(properties);
      Preconditions.checkNotNull(schema,""String_Node_Str"");
      String delimiter=null;
      if (""String_Node_Str"".equals(format)) {
        delimiter=FileSetProperties.getExploreFormatProperties(properties).get(""String_Node_Str"");
      }
 else       if (""String_Node_Str"".equals(format)) {
        delimiter=""String_Node_Str"";
      }
      return createStatementBuilder.setSchema(schema).setRowFormatDelimited(delimiter,null).buildWithFileFormat(""String_Node_Str"");
    }
  }
 else {
    String serde=FileSetProperties.getSerDe(properties);
    String inputFormat=FileSetProperties.getExploreInputFormat(properties);
    String outputFormat=FileSetProperties.getExploreOutputFormat(properties);
    Preconditions.checkArgument(serde != null && inputFormat != null && outputFormat != null,""String_Node_Str"");
    return createStatementBuilder.setRowFormatSerde(serde).buildWithFormats(inputFormat,outputFormat);
  }
}","private String generateFileSetCreateStatement(Id.DatasetInstance datasetID,Dataset dataset,Map<String,String> properties) throws IllegalArgumentException {
  String tableName=getDatasetTableName(datasetID);
  Map<String,String> tableProperties=FileSetProperties.getTableProperties(properties);
  Location baseLocation;
  Partitioning partitioning=null;
  if (dataset instanceof PartitionedFileSet) {
    partitioning=((PartitionedFileSet)dataset).getPartitioning();
    baseLocation=((PartitionedFileSet)dataset).getEmbeddedFileSet().getBaseLocation();
  }
 else {
    baseLocation=((FileSet)dataset).getBaseLocation();
  }
  CreateStatementBuilder createStatementBuilder=new CreateStatementBuilder(datasetID.getId(),tableName).setLocation(baseLocation).setPartitioning(partitioning).setTableProperties(tableProperties);
  String format=FileSetProperties.getExploreFormat(properties);
  if (format != null) {
    if (""String_Node_Str"".equals(format)) {
      return createStatementBuilder.setSchema(FileSetProperties.getExploreSchema(properties)).buildWithFileFormat(""String_Node_Str"");
    }
    Preconditions.checkArgument(""String_Node_Str"".equals(format) || ""String_Node_Str"".equals(format),""String_Node_Str"");
    String schema=FileSetProperties.getExploreSchema(properties);
    Preconditions.checkNotNull(schema,""String_Node_Str"");
    String delimiter=null;
    if (""String_Node_Str"".equals(format)) {
      delimiter=FileSetProperties.getExploreFormatProperties(properties).get(""String_Node_Str"");
    }
 else     if (""String_Node_Str"".equals(format)) {
      delimiter=""String_Node_Str"";
    }
    return createStatementBuilder.setSchema(schema).setRowFormatDelimited(delimiter,null).buildWithFileFormat(""String_Node_Str"");
  }
 else {
    String serde=FileSetProperties.getSerDe(properties);
    String inputFormat=FileSetProperties.getExploreInputFormat(properties);
    String outputFormat=FileSetProperties.getExploreOutputFormat(properties);
    Preconditions.checkArgument(serde != null && inputFormat != null && outputFormat != null,""String_Node_Str"");
    return createStatementBuilder.setRowFormatSerde(serde).buildWithFormats(inputFormat,outputFormat);
  }
}"
6901,"@Override public void setValue(String key,String value){
  Preconditions.checkNotNull(nodeName,""String_Node_Str"");
  WorkflowTokenValue tokenValue=tokenValueMap.get(key);
  if (tokenValue == null) {
    tokenValue=new WorkflowTokenValue();
  }
  tokenValue.putValue(nodeName,value);
  tokenValueMap.put(key,tokenValue);
}","@Override public void setValue(String key,String value){
  Preconditions.checkNotNull(nodeName,""String_Node_Str"");
  WorkflowTokenValue tokenValue=tokenValueMap.get(key);
  if (tokenValue == null) {
    tokenValue=new WorkflowTokenValue();
    tokenValueMap.put(key,tokenValue);
  }
  tokenValue.putValue(nodeName,value);
}"
6902,"public StartDebugProgramCommand(ElementType elementType,ProgramClient programClient,CLIConfig cliConfig){
  super(cliConfig);
  this.elementType=elementType;
  this.programClient=programClient;
}","public StartDebugProgramCommand(ElementType elementType,ProgramClient programClient,CLIConfig cliConfig){
  super(elementType,programClient,cliConfig);
  this.isDebug=true;
}"
6903,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] programIdParts=arguments.get(elementType.getArgumentName().toString()).split(""String_Node_Str"");
  if (programIdParts.length < 2) {
    throw new CommandInputError(this);
  }
  String appId=programIdParts[0];
  String programId=programIdParts[1];
  String runtimeArgsString=arguments.get(ArgumentName.RUNTIME_ARGS.toString(),""String_Node_Str"");
  if (runtimeArgsString == null || runtimeArgsString.isEmpty()) {
    programClient.start(appId,elementType.getProgramType(),programId,false);
    runtimeArgsString=GSON.toJson(programClient.getRuntimeArgs(appId,elementType.getProgramType(),programId));
    output.printf(""String_Node_Str"",elementType.getTitleName(),programId,appId,runtimeArgsString);
  }
 else {
    Map<String,String> runtimeArgs=ArgumentParser.parseMap(runtimeArgsString);
    programClient.start(appId,elementType.getProgramType(),programId,false,runtimeArgs);
    output.printf(""String_Node_Str"",elementType.getTitleName(),programId,appId,runtimeArgsString);
  }
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] programIdParts=arguments.get(elementType.getArgumentName().toString()).split(""String_Node_Str"");
  if (programIdParts.length < 2) {
    throw new CommandInputError(this);
  }
  String appId=programIdParts[0];
  String programId=programIdParts[1];
  String runtimeArgsString=arguments.get(ArgumentName.RUNTIME_ARGS.toString(),""String_Node_Str"");
  if (runtimeArgsString == null || runtimeArgsString.isEmpty()) {
    programClient.start(appId,elementType.getProgramType(),programId,isDebug);
    runtimeArgsString=GSON.toJson(programClient.getRuntimeArgs(appId,elementType.getProgramType(),programId));
    output.printf(""String_Node_Str"",elementType.getTitleName(),programId,appId,runtimeArgsString);
  }
 else {
    Map<String,String> runtimeArgs=ArgumentParser.parseMap(runtimeArgsString);
    programClient.start(appId,elementType.getProgramType(),programId,isDebug,runtimeArgs);
    output.printf(""String_Node_Str"",elementType.getTitleName(),programId,appId,runtimeArgsString);
  }
}"
6904,"private static List<Command> generateCommands(ProgramClient programClient,CLIConfig cliConfig){
  List<Command> commands=Lists.newArrayList();
  for (  ElementType elementType : ElementType.values()) {
    if (elementType.canStart()) {
      commands.add(new StartProgramCommand(elementType,programClient,cliConfig));
    }
  }
  return commands;
}","private static List<Command> generateCommands(ProgramClient programClient,CLIConfig cliConfig){
  List<Command> commands=Lists.newArrayList();
  for (  ElementType elementType : ElementType.values()) {
    if (elementType.canStart()) {
      commands.add(new StartProgramCommand(elementType,programClient,cliConfig));
      commands.add(new StartDebugProgramCommand(elementType,programClient,cliConfig));
    }
  }
  return commands;
}"
6905,"/** 
 * Sets the runtime args of a program.
 * @param appId ID of the application tat the program belongs to
 * @param programType type of the program
 * @param programId ID of the program
 * @param runtimeArgs args of the program
 * @throws IOException if a network error occurred
 * @throws ProgramNotFoundException if the application or program could not be found
 * @throws UnauthorizedException if the request is not authorized successfully in the gateway server
 */
public void setRuntimeArgs(String appId,ProgramType programType,String programId,Map<String,String> runtimeArgs) throws IOException, UnauthorizedException, ProgramNotFoundException {
  Id.Application app=Id.Application.from(config.getNamespace(),appId);
  Id.Program program=Id.Program.from(app,programType,programId);
  String path=String.format(""String_Node_Str"",appId,programType.getCategoryName(),programId);
  URL url=config.resolveNamespacedURLV3(path);
  HttpRequest request=HttpRequest.put(url).withBody(GSON.toJson(runtimeArgs)).build();
  HttpResponse response=restClient.execute(request,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new ProgramNotFoundException(program);
  }
}","/** 
 * Sets the runtime args of a program.
 * @param appId ID of the application that the program belongs to
 * @param programType type of the program
 * @param programId ID of the program
 * @param runtimeArgs args of the program
 * @throws IOException if a network error occurred
 * @throws ProgramNotFoundException if the application or program could not be found
 * @throws UnauthorizedException if the request is not authorized successfully in the gateway server
 */
public void setRuntimeArgs(String appId,ProgramType programType,String programId,Map<String,String> runtimeArgs) throws IOException, UnauthorizedException, ProgramNotFoundException {
  Id.Application app=Id.Application.from(config.getNamespace(),appId);
  Id.Program program=Id.Program.from(app,programType,programId);
  String path=String.format(""String_Node_Str"",appId,programType.getCategoryName(),programId);
  URL url=config.resolveNamespacedURLV3(path);
  HttpRequest request=HttpRequest.put(url).withBody(GSON.toJson(runtimeArgs)).build();
  HttpResponse response=restClient.execute(request,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new ProgramNotFoundException(program);
  }
}"
6906,"/** 
 * Gets the runtime args of a program.
 * @param appId ID of the application tat the program belongs to
 * @param programType type of the program
 * @param programId ID of the program
 * @return runtime args of the program
 * @throws IOException if a network error occurred
 * @throws ProgramNotFoundException if the application or program could not be found
 * @throws UnauthorizedException if the request is not authorized successfully in the gateway server
 */
public Map<String,String> getRuntimeArgs(String appId,ProgramType programType,String programId) throws IOException, UnauthorizedException, ProgramNotFoundException {
  Id.Application app=Id.Application.from(config.getNamespace(),appId);
  Id.Program program=Id.Program.from(app,programType,programId);
  String path=String.format(""String_Node_Str"",appId,programType.getCategoryName(),programId);
  URL url=config.resolveNamespacedURLV3(path);
  HttpResponse response=restClient.execute(HttpMethod.GET,url,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new ProgramNotFoundException(program);
  }
  return ObjectResponse.fromJsonBody(response,new TypeToken<Map<String,String>>(){
  }
).getResponseObject();
}","/** 
 * Gets the runtime args of a program.
 * @param appId ID of the application that the program belongs to
 * @param programType type of the program
 * @param programId ID of the program
 * @return runtime args of the program
 * @throws IOException if a network error occurred
 * @throws ProgramNotFoundException if the application or program could not be found
 * @throws UnauthorizedException if the request is not authorized successfully in the gateway server
 */
public Map<String,String> getRuntimeArgs(String appId,ProgramType programType,String programId) throws IOException, UnauthorizedException, ProgramNotFoundException {
  Id.Application app=Id.Application.from(config.getNamespace(),appId);
  Id.Program program=Id.Program.from(app,programType,programId);
  String path=String.format(""String_Node_Str"",appId,programType.getCategoryName(),programId);
  URL url=config.resolveNamespacedURLV3(path);
  HttpResponse response=restClient.execute(HttpMethod.GET,url,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new ProgramNotFoundException(program);
  }
  return ObjectResponse.fromJsonBody(response,new TypeToken<Map<String,String>>(){
  }
).getResponseObject();
}"
6907,"public StartDebugProgramCommand(ElementType elementType,ProgramClient programClient,CLIConfig cliConfig){
  super(cliConfig);
  this.elementType=elementType;
  this.programClient=programClient;
}","public StartDebugProgramCommand(ElementType elementType,ProgramClient programClient,CLIConfig cliConfig){
  super(elementType,programClient,cliConfig);
  this.isDebug=true;
}"
6908,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] programIdParts=arguments.get(elementType.getArgumentName().toString()).split(""String_Node_Str"");
  if (programIdParts.length < 2) {
    throw new CommandInputError(this);
  }
  String appId=programIdParts[0];
  String programId=programIdParts[1];
  String runtimeArgsString=arguments.get(ArgumentName.RUNTIME_ARGS.toString(),""String_Node_Str"");
  if (runtimeArgsString == null || runtimeArgsString.isEmpty()) {
    programClient.start(appId,elementType.getProgramType(),programId,false);
    runtimeArgsString=GSON.toJson(programClient.getRuntimeArgs(appId,elementType.getProgramType(),programId));
    output.printf(""String_Node_Str"",elementType.getTitleName(),programId,appId,runtimeArgsString);
  }
 else {
    Map<String,String> runtimeArgs=ArgumentParser.parseMap(runtimeArgsString);
    programClient.start(appId,elementType.getProgramType(),programId,false,runtimeArgs);
    output.printf(""String_Node_Str"",elementType.getTitleName(),programId,appId,runtimeArgsString);
  }
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] programIdParts=arguments.get(elementType.getArgumentName().toString()).split(""String_Node_Str"");
  if (programIdParts.length < 2) {
    throw new CommandInputError(this);
  }
  String appId=programIdParts[0];
  String programId=programIdParts[1];
  String runtimeArgsString=arguments.get(ArgumentName.RUNTIME_ARGS.toString(),""String_Node_Str"");
  if (runtimeArgsString == null || runtimeArgsString.isEmpty()) {
    programClient.start(appId,elementType.getProgramType(),programId,isDebug);
    runtimeArgsString=GSON.toJson(programClient.getRuntimeArgs(appId,elementType.getProgramType(),programId));
    output.printf(""String_Node_Str"",elementType.getTitleName(),programId,appId,runtimeArgsString);
  }
 else {
    Map<String,String> runtimeArgs=ArgumentParser.parseMap(runtimeArgsString);
    programClient.start(appId,elementType.getProgramType(),programId,isDebug,runtimeArgs);
    output.printf(""String_Node_Str"",elementType.getTitleName(),programId,appId,runtimeArgsString);
  }
}"
6909,"private static List<Command> generateCommands(ProgramClient programClient,CLIConfig cliConfig){
  List<Command> commands=Lists.newArrayList();
  for (  ElementType elementType : ElementType.values()) {
    if (elementType.canStart()) {
      commands.add(new StartProgramCommand(elementType,programClient,cliConfig));
    }
  }
  return commands;
}","private static List<Command> generateCommands(ProgramClient programClient,CLIConfig cliConfig){
  List<Command> commands=Lists.newArrayList();
  for (  ElementType elementType : ElementType.values()) {
    if (elementType.canStart()) {
      commands.add(new StartProgramCommand(elementType,programClient,cliConfig));
      commands.add(new StartDebugProgramCommand(elementType,programClient,cliConfig));
    }
  }
  return commands;
}"
6910,"/** 
 * Sets the runtime args of a program.
 * @param appId ID of the application tat the program belongs to
 * @param programType type of the program
 * @param programId ID of the program
 * @param runtimeArgs args of the program
 * @throws IOException if a network error occurred
 * @throws ProgramNotFoundException if the application or program could not be found
 * @throws UnauthorizedException if the request is not authorized successfully in the gateway server
 */
public void setRuntimeArgs(String appId,ProgramType programType,String programId,Map<String,String> runtimeArgs) throws IOException, UnauthorizedException, ProgramNotFoundException {
  Id.Application app=Id.Application.from(config.getNamespace(),appId);
  Id.Program program=Id.Program.from(app,programType,programId);
  String path=String.format(""String_Node_Str"",appId,programType.getCategoryName(),programId);
  URL url=config.resolveNamespacedURLV3(path);
  HttpRequest request=HttpRequest.put(url).withBody(GSON.toJson(runtimeArgs)).build();
  HttpResponse response=restClient.execute(request,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new ProgramNotFoundException(program);
  }
}","/** 
 * Sets the runtime args of a program.
 * @param appId ID of the application that the program belongs to
 * @param programType type of the program
 * @param programId ID of the program
 * @param runtimeArgs args of the program
 * @throws IOException if a network error occurred
 * @throws ProgramNotFoundException if the application or program could not be found
 * @throws UnauthorizedException if the request is not authorized successfully in the gateway server
 */
public void setRuntimeArgs(String appId,ProgramType programType,String programId,Map<String,String> runtimeArgs) throws IOException, UnauthorizedException, ProgramNotFoundException {
  Id.Application app=Id.Application.from(config.getNamespace(),appId);
  Id.Program program=Id.Program.from(app,programType,programId);
  String path=String.format(""String_Node_Str"",appId,programType.getCategoryName(),programId);
  URL url=config.resolveNamespacedURLV3(path);
  HttpRequest request=HttpRequest.put(url).withBody(GSON.toJson(runtimeArgs)).build();
  HttpResponse response=restClient.execute(request,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new ProgramNotFoundException(program);
  }
}"
6911,"/** 
 * Gets the runtime args of a program.
 * @param appId ID of the application tat the program belongs to
 * @param programType type of the program
 * @param programId ID of the program
 * @return runtime args of the program
 * @throws IOException if a network error occurred
 * @throws ProgramNotFoundException if the application or program could not be found
 * @throws UnauthorizedException if the request is not authorized successfully in the gateway server
 */
public Map<String,String> getRuntimeArgs(String appId,ProgramType programType,String programId) throws IOException, UnauthorizedException, ProgramNotFoundException {
  Id.Application app=Id.Application.from(config.getNamespace(),appId);
  Id.Program program=Id.Program.from(app,programType,programId);
  String path=String.format(""String_Node_Str"",appId,programType.getCategoryName(),programId);
  URL url=config.resolveNamespacedURLV3(path);
  HttpResponse response=restClient.execute(HttpMethod.GET,url,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new ProgramNotFoundException(program);
  }
  return ObjectResponse.fromJsonBody(response,new TypeToken<Map<String,String>>(){
  }
).getResponseObject();
}","/** 
 * Gets the runtime args of a program.
 * @param appId ID of the application that the program belongs to
 * @param programType type of the program
 * @param programId ID of the program
 * @return runtime args of the program
 * @throws IOException if a network error occurred
 * @throws ProgramNotFoundException if the application or program could not be found
 * @throws UnauthorizedException if the request is not authorized successfully in the gateway server
 */
public Map<String,String> getRuntimeArgs(String appId,ProgramType programType,String programId) throws IOException, UnauthorizedException, ProgramNotFoundException {
  Id.Application app=Id.Application.from(config.getNamespace(),appId);
  Id.Program program=Id.Program.from(app,programType,programId);
  String path=String.format(""String_Node_Str"",appId,programType.getCategoryName(),programId);
  URL url=config.resolveNamespacedURLV3(path);
  HttpResponse response=restClient.execute(HttpMethod.GET,url,config.getAccessToken(),HttpURLConnection.HTTP_NOT_FOUND);
  if (response.getResponseCode() == HttpURLConnection.HTTP_NOT_FOUND) {
    throw new ProgramNotFoundException(program);
  }
  return ObjectResponse.fromJsonBody(response,new TypeToken<Map<String,String>>(){
  }
).getResponseObject();
}"
6912,"@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] programIdParts=arguments.get(elementType.getArgumentName().toString()).split(""String_Node_Str"");
  String appId=programIdParts[0];
  String startString=arguments.get(ArgumentName.START_TIME.toString(),""String_Node_Str"");
  long start=TimeMathParser.parseTime(startString);
  String stopString=arguments.get(ArgumentName.END_TIME.toString(),Long.toString(Integer.MAX_VALUE));
  long stop=TimeMathParser.parseTime(stopString);
  String logs;
  if (elementType.getProgramType() != null) {
    if (programIdParts.length < 2) {
      throw new CommandInputError(this);
    }
    String programId=programIdParts[1];
    logs=programClient.getProgramLogs(appId,elementType.getProgramType(),programId,start,stop);
  }
 else {
    throw new IllegalArgumentException(""String_Node_Str"" + elementType.getNamePlural());
  }
  output.println(logs);
}","@Override public void perform(Arguments arguments,PrintStream output) throws Exception {
  String[] programIdParts=arguments.get(elementType.getArgumentName().toString()).split(""String_Node_Str"");
  String appId=programIdParts[0];
  String startString=arguments.get(ArgumentName.START_TIME.toString(),""String_Node_Str"");
  long start=TimeMathParser.parseTimeInSeconds(startString);
  String stopString=arguments.get(ArgumentName.END_TIME.toString(),Long.toString(Integer.MAX_VALUE));
  long stop=TimeMathParser.parseTimeInSeconds(stopString);
  String logs;
  if (elementType.getProgramType() != null) {
    if (programIdParts.length < 2) {
      throw new CommandInputError(this);
    }
    String programId=programIdParts[1];
    logs=programClient.getProgramLogs(appId,elementType.getProgramType(),programId,start,stop);
  }
 else {
    throw new IllegalArgumentException(""String_Node_Str"" + elementType.getNamePlural());
  }
  output.println(logs);
}"
6913,"private MetricQueryResult executeQuery(MetricQueryRequest queryRequest) throws Exception {
  if (queryRequest.getMetrics().size() == 0) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  Map<String,String> tagsSliceBy=humanToTagNames(transformTagMap(queryRequest.getTags()));
  MetricQueryRequest.TimeRange timeRange=queryRequest.getTimeRange();
  MetricDataQuery query=new MetricDataQuery(timeRange.getStart(),timeRange.getEnd(),timeRange.getResolutionInSeconds(),timeRange.getCount(),toMetrics(queryRequest.getMetrics()),tagsSliceBy,transformGroupByTags(queryRequest.getGroupBy()),timeRange.getInterpolate());
  Collection<MetricTimeSeries> queryResult=metricStore.query(query);
  long endTime=timeRange.getEnd();
  if (timeRange.getResolutionInSeconds() == Integer.MAX_VALUE && endTime == 0) {
    endTime=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
  }
  return decorate(queryResult,timeRange.getStart(),endTime,timeRange.getResolutionInSeconds());
}","private MetricQueryResult executeQuery(MetricQueryRequest queryRequest) throws Exception {
  if (queryRequest.getMetrics().size() == 0) {
    throw new IllegalArgumentException(""String_Node_Str"");
  }
  Map<String,String> tagsSliceBy=humanToTagNames(transformTagMap(queryRequest.getTags()));
  MetricQueryRequest.TimeRange timeRange=queryRequest.getTimeRange();
  MetricDataQuery query=new MetricDataQuery(timeRange.getStart(),timeRange.getEnd(),timeRange.getResolutionInSeconds(),timeRange.getCount(),toMetrics(queryRequest.getMetrics()),tagsSliceBy,transformGroupByTags(queryRequest.getGroupBy()),timeRange.getInterpolate());
  Collection<MetricTimeSeries> queryResult=metricStore.query(query);
  long endTime=timeRange.getEnd();
  if (timeRange.getResolutionInSeconds() == Integer.MAX_VALUE && endTime == 0) {
    endTime=TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis());
  }
  return decorate(queryResult,timeRange.getStart(),endTime,timeRange.getResolutionInSeconds().intValue());
}"
6914,"/** 
 * Relays job-level and task-level information about a particular MapReduce program run.
 */
@GET @Path(""String_Node_Str"") public void getMapReduceInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String mapreduceId,@PathParam(""String_Node_Str"") String runId){
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.MAPREDUCE,mapreduceId);
    Id.Run run=new Id.Run(programId,runId);
    ApplicationSpecification appSpec=store.getApplication(programId.getApplication());
    if (appSpec == null) {
      throw new NotFoundException(programId.getApplication());
    }
    if (!appSpec.getMapReduce().containsKey(mapreduceId)) {
      throw new NotFoundException(programId);
    }
    RunRecord runRecord=store.getRun(programId,runId);
    if (runRecord == null) {
      throw new NotFoundException(run);
    }
    MRJobInfo mrJobInfo=mrJobInfoFetcher.getMRJobInfo(run);
    mrJobInfo.setState(runRecord.getStatus().name());
    mrJobInfo.setStartTime(TimeUnit.SECONDS.toMillis(runRecord.getStartTs()));
    Long stopTs=runRecord.getStopTs();
    if (stopTs != null) {
      mrJobInfo.setStopTime(TimeUnit.SECONDS.toMillis(stopTs));
    }
    responder.sendJson(HttpResponseStatus.OK,mrJobInfo);
  }
 catch (  NotFoundException e) {
    LOG.warn(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",runId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}","/** 
 * Relays job-level and task-level information about a particular MapReduce program run.
 */
@GET @Path(""String_Node_Str"") public void getMapReduceInfo(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String mapreduceId,@PathParam(""String_Node_Str"") String runId){
  try {
    Id.Program programId=Id.Program.from(namespaceId,appId,ProgramType.MAPREDUCE,mapreduceId);
    Id.Run run=new Id.Run(programId,runId);
    ApplicationSpecification appSpec=store.getApplication(programId.getApplication());
    if (appSpec == null) {
      throw new NotFoundException(programId.getApplication());
    }
    if (!appSpec.getMapReduce().containsKey(mapreduceId)) {
      throw new NotFoundException(programId);
    }
    RunRecord runRecord=store.getRun(programId,runId);
    if (runRecord == null) {
      throw new NotFoundException(run);
    }
    MRJobInfo mrJobInfo=mrJobInfoFetcher.getMRJobInfo(run);
    mrJobInfo.setState(runRecord.getStatus().name());
    mrJobInfo.setStartTime(TimeUnit.SECONDS.toMillis(runRecord.getStartTs()));
    Long stopTs=runRecord.getStopTs();
    if (stopTs != null) {
      mrJobInfo.setStopTime(TimeUnit.SECONDS.toMillis(stopTs));
    }
    Gson gson=new GsonBuilder().serializeSpecialFloatingPointValues().create();
    responder.sendJson(HttpResponseStatus.OK,mrJobInfo,mrJobInfo.getClass(),gson);
  }
 catch (  NotFoundException e) {
    LOG.warn(""String_Node_Str"",e);
    responder.sendString(HttpResponseStatus.NOT_FOUND,e.getMessage());
  }
catch (  Exception e) {
    LOG.error(""String_Node_Str"",runId,e);
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,e.getMessage());
  }
}"
6915,"public JmsPluginConfig(String destinationName,String initialContextFactory,String providerUrl,@Nullable Integer messagesToReceive,@Nullable String connectionFactoryName,@Nullable String jmsPluginName,@Nullable String jmsPluginType){
  this.destinationName=destinationName;
  if (messagesToReceive != null) {
    this.messagesToReceive=messagesToReceive;
  }
 else {
    this.messagesToReceive=50;
  }
  this.initialContextFactory=initialContextFactory;
  this.providerUrl=providerUrl;
  if (connectionFactoryName != null) {
    this.connectionFactoryName=connectionFactoryName;
  }
 else {
    this.connectionFactoryName=DEFAULT_CONNECTION_FACTORY;
  }
  this.jmsPluginName=jmsPluginName;
  if (this.jmsPluginName == null) {
    this.jmsPluginName=Context.INITIAL_CONTEXT_FACTORY;
  }
  this.jmsPluginType=jmsPluginType;
  if (this.jmsPluginType == null) {
    this.jmsPluginType=JMS_PROVIDER;
  }
}","public JmsPluginConfig(String destinationName,String initialContextFactory,String providerUrl,@Nullable Integer messagesToReceive,@Nullable String connectionFactoryName,@Nullable String jmsPluginName,@Nullable String jmsPluginType,@Nullable String customProperties){
  this.destinationName=destinationName;
  if (messagesToReceive != null) {
    this.messagesToReceive=messagesToReceive;
  }
 else {
    this.messagesToReceive=50;
  }
  this.initialContextFactory=initialContextFactory;
  this.providerUrl=providerUrl;
  if (connectionFactoryName != null) {
    this.connectionFactoryName=connectionFactoryName;
  }
 else {
    this.connectionFactoryName=DEFAULT_CONNECTION_FACTORY;
  }
  this.jmsPluginName=jmsPluginName;
  if (this.jmsPluginName == null) {
    this.jmsPluginName=Context.INITIAL_CONTEXT_FACTORY;
  }
  this.jmsPluginType=jmsPluginType;
  if (this.jmsPluginType == null) {
    this.jmsPluginType=JMS_PROVIDER;
  }
  this.customProperties=customProperties;
}"
6916,"/** 
 * Initialize the Source.
 * @param context {@link RealtimeContext}
 */
public void initialize(RealtimeContext context) throws Exception {
  super.initialize(context);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  if (config.getProperties() != null) {
    runtimeArguments.putAll(config.getProperties().getProperties());
  }
  Integer configMessagesToReceive=config.messagesToReceive;
  messagesToReceive=configMessagesToReceive.intValue();
  final Hashtable<String,String> envVars=new Hashtable<String,String>();
  for (  Map.Entry<String,String> entry : runtimeArguments.entrySet()) {
    envVars.put(entry.getKey(),entry.getValue());
  }
  envVars.put(Context.INITIAL_CONTEXT_FACTORY,config.initialContextFactory);
  envVars.put(Context.PROVIDER_URL,config.providerUrl);
  Class<Object> driver=context.loadPluginClass(getPluginId());
  ClassLoader driverCL=null;
  if (driver != null) {
    driverCL=driver.getClassLoader();
  }
  initializeJMSConnection(envVars,config.destinationName,config.connectionFactoryName,driverCL);
}","/** 
 * Initialize the Source.
 * @param context {@link RealtimeContext}
 */
public void initialize(RealtimeContext context) throws Exception {
  super.initialize(context);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  if (config.getProperties() != null) {
    runtimeArguments.putAll(config.getProperties().getProperties());
  }
  if (config.customProperties != null) {
    Map<String,String> customProperties=GSON.fromJson(config.customProperties,STRING_MAP_TYPE);
    runtimeArguments.putAll(customProperties);
  }
  Integer configMessagesToReceive=config.messagesToReceive;
  messagesToReceive=configMessagesToReceive.intValue();
  final Hashtable<String,String> envVars=new Hashtable<String,String>();
  for (  Map.Entry<String,String> entry : runtimeArguments.entrySet()) {
    envVars.put(entry.getKey(),entry.getValue());
  }
  envVars.put(Context.INITIAL_CONTEXT_FACTORY,config.initialContextFactory);
  envVars.put(Context.PROVIDER_URL,config.providerUrl);
  Class<Object> driver=context.loadPluginClass(getPluginId());
  ClassLoader driverCL=null;
  if (driver != null) {
    driverCL=driver.getClassLoader();
  }
  initializeJMSConnection(envVars,config.destinationName,config.connectionFactoryName,driverCL);
}"
6917,"private void initializeJmsSource(String destination,int messageReceive,String initialContextFactory,String providerUrl){
  jmsSource=new JmsSource(new JmsPluginConfig(destination,initialContextFactory,providerUrl,messageReceive,JmsSource.DEFAULT_CONNECTION_FACTORY,null,null));
}","private void initializeJmsSource(String destination,int messageReceive,String initialContextFactory,String providerUrl){
  jmsSource=new JmsSource(new JmsPluginConfig(destination,initialContextFactory,providerUrl,messageReceive,JmsSource.DEFAULT_CONNECTION_FACTORY,null,null,null));
}"
6918,"public JmsPluginConfig(String destinationName,String initialContextFactory,String providerUrl,@Nullable Integer messagesToReceive,@Nullable String connectionFactoryName,@Nullable String jmsPluginName,@Nullable String jmsPluginType){
  this.destinationName=destinationName;
  if (messagesToReceive != null) {
    this.messagesToReceive=messagesToReceive;
  }
 else {
    this.messagesToReceive=50;
  }
  this.initialContextFactory=initialContextFactory;
  this.providerUrl=providerUrl;
  if (connectionFactoryName != null) {
    this.connectionFactoryName=connectionFactoryName;
  }
 else {
    this.connectionFactoryName=JMS_CONNECTION_FACTORY_NAME;
  }
  this.jmsPluginName=jmsPluginName;
  if (this.jmsPluginName == null) {
    this.jmsPluginName=Context.INITIAL_CONTEXT_FACTORY;
  }
  this.jmsPluginType=jmsPluginType;
  if (this.jmsPluginType == null) {
    this.jmsPluginType=JMS_PROVIDER;
  }
}","public JmsPluginConfig(String destinationName,String initialContextFactory,String providerUrl,@Nullable Integer messagesToReceive,@Nullable String connectionFactoryName,@Nullable String jmsPluginName,@Nullable String jmsPluginType,@Nullable String customProperties){
  this.destinationName=destinationName;
  if (messagesToReceive != null) {
    this.messagesToReceive=messagesToReceive;
  }
 else {
    this.messagesToReceive=50;
  }
  this.initialContextFactory=initialContextFactory;
  this.providerUrl=providerUrl;
  if (connectionFactoryName != null) {
    this.connectionFactoryName=connectionFactoryName;
  }
 else {
    this.connectionFactoryName=JMS_CONNECTION_FACTORY_NAME;
  }
  this.jmsPluginName=jmsPluginName;
  if (this.jmsPluginName == null) {
    this.jmsPluginName=Context.INITIAL_CONTEXT_FACTORY;
  }
  this.jmsPluginType=jmsPluginType;
  if (this.jmsPluginType == null) {
    this.jmsPluginType=JMS_PROVIDER;
  }
  this.customProperties=customProperties;
}"
6919,"/** 
 * Initialize the Source.
 * @param context {@link RealtimeContext}
 */
public void initialize(RealtimeContext context) throws Exception {
  super.initialize(context);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  if (config.getProperties() != null) {
    runtimeArguments.putAll(config.getProperties().getProperties());
  }
  Integer configMessagesToReceive=config.messagesToReceive;
  messagesToReceive=configMessagesToReceive.intValue();
  final Hashtable<String,String> envVars=new Hashtable<String,String>();
  for (  Map.Entry<String,String> entry : runtimeArguments.entrySet()) {
    envVars.put(entry.getKey(),entry.getValue());
  }
  envVars.put(Context.INITIAL_CONTEXT_FACTORY,config.initialContextFactory);
  envVars.put(Context.PROVIDER_URL,config.providerUrl);
  Class<Object> driver=context.loadPluginClass(getPluginId());
  ClassLoader driverCL=null;
  if (driver != null) {
    driverCL=driver.getClassLoader();
  }
  initializeJMSConnection(envVars,config.destinationName,config.connectionFactoryName,driverCL);
}","/** 
 * Initialize the Source.
 * @param context {@link RealtimeContext}
 */
public void initialize(RealtimeContext context) throws Exception {
  super.initialize(context);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  if (config.getProperties() != null) {
    runtimeArguments.putAll(config.getProperties().getProperties());
  }
  if (config.customProperties != null) {
    Map<String,String> customProperties=GSON.fromJson(config.customProperties,STRING_MAP_TYPE);
    runtimeArguments.putAll(customProperties);
  }
  Integer configMessagesToReceive=config.messagesToReceive;
  messagesToReceive=configMessagesToReceive.intValue();
  final Hashtable<String,String> envVars=new Hashtable<String,String>();
  for (  Map.Entry<String,String> entry : runtimeArguments.entrySet()) {
    envVars.put(entry.getKey(),entry.getValue());
  }
  envVars.put(Context.INITIAL_CONTEXT_FACTORY,config.initialContextFactory);
  envVars.put(Context.PROVIDER_URL,config.providerUrl);
  Class<Object> driver=context.loadPluginClass(getPluginId());
  ClassLoader driverCL=null;
  if (driver != null) {
    driverCL=driver.getClassLoader();
  }
  initializeJMSConnection(envVars,config.destinationName,config.connectionFactoryName,driverCL);
}"
6920,"private void initializeJmsSource(String destination,int messageReceive,String initialContextFactory,String providerUrl){
  jmsSource=new JmsSource(new JmsPluginConfig(destination,initialContextFactory,providerUrl,messageReceive,JmsSource.DEFAULT_CONNECTION_FACTORY,null,null));
}","private void initializeJmsSource(String destination,int messageReceive,String initialContextFactory,String providerUrl){
  jmsSource=new JmsSource(new JmsPluginConfig(destination,initialContextFactory,providerUrl,messageReceive,JmsSource.DEFAULT_CONNECTION_FACTORY,null,null,null));
}"
6921,"/** 
 * Initialize the Source.
 * @param context {@link RealtimeContext}
 */
public void initialize(RealtimeContext context) throws Exception {
  super.initialize(context);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  if (config.getProperties() != null) {
    runtimeArguments.putAll(config.getProperties().getProperties());
  }
  Integer configMessagesToReceive=config.messagesToReceive;
  messagesToReceive=configMessagesToReceive.intValue();
  final Hashtable<String,String> envVars=new Hashtable<String,String>();
  for (  Map.Entry<String,String> entry : runtimeArguments.entrySet()) {
    envVars.put(entry.getKey(),entry.getValue());
  }
  envVars.put(Context.INITIAL_CONTEXT_FACTORY,config.initialContextFactory);
  envVars.put(Context.PROVIDER_URL,config.providerUrl);
  Class<Object> driver=context.loadPluginClass(""String_Node_Str"");
  ClassLoader driverCL=null;
  if (driver != null) {
    driverCL=driver.getClassLoader();
  }
  initializeJMSConnection(envVars,config.destinationName,config.connectionFactoryName,driverCL);
}","/** 
 * Initialize the Source.
 * @param context {@link RealtimeContext}
 */
public void initialize(RealtimeContext context) throws Exception {
  super.initialize(context);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  if (config.getProperties() != null) {
    runtimeArguments.putAll(config.getProperties().getProperties());
  }
  Integer configMessagesToReceive=config.messagesToReceive;
  messagesToReceive=configMessagesToReceive.intValue();
  final Hashtable<String,String> envVars=new Hashtable<String,String>();
  for (  Map.Entry<String,String> entry : runtimeArguments.entrySet()) {
    envVars.put(entry.getKey(),entry.getValue());
  }
  envVars.put(Context.INITIAL_CONTEXT_FACTORY,config.initialContextFactory);
  envVars.put(Context.PROVIDER_URL,config.providerUrl);
  Class<Object> driver=context.loadPluginClass(getPluginId());
  ClassLoader driverCL=null;
  if (driver != null) {
    driverCL=driver.getClassLoader();
  }
  initializeJMSConnection(envVars,config.destinationName,config.connectionFactoryName,driverCL);
}"
6922,"@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  String pluginId=String.format(""String_Node_Str"",""String_Node_Str"",config.jmsPluginType,config.jmsPluginName);
  Class<Object> driver=pipelineConfigurer.usePluginClass(config.jmsPluginType,config.jmsPluginName,pluginId,PluginProperties.builder().build());
  Preconditions.checkArgument(driver != null,""String_Node_Str"");
}","@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  String pluginId=getPluginId();
  Class<Object> driver=pipelineConfigurer.usePluginClass(config.jmsPluginType,config.jmsPluginName,pluginId,PluginProperties.builder().build());
  Preconditions.checkArgument(driver != null,""String_Node_Str"");
}"
6923,"/** 
 * Helper method to create new   {@link KafkaConsumerInfo} for map of topic partitions.
 * @param config
 * @return
 */
private Map<TopicPartition,KafkaConsumerInfo<OFFSET>> createConsumerInfos(Map<TopicPartition,Integer> config){
  ImmutableMap.Builder<TopicPartition,KafkaConsumerInfo<OFFSET>> consumers=ImmutableMap.builder();
  for (  Map.Entry<TopicPartition,Integer> entry : config.entrySet()) {
    consumers.put(entry.getKey(),new KafkaConsumerInfo<OFFSET>(entry.getKey(),entry.getValue(),getBeginOffset(entry.getKey())));
  }
  return consumers.build();
}","/** 
 * Helper method to create new   {@link KafkaConsumerInfo} for map of topic partitions.
 * @param config
 * @return KafkaConsumerInfo mapped to TopicPartitions
 */
private Map<TopicPartition,KafkaConsumerInfo<OFFSET>> createConsumerInfos(Map<TopicPartition,Integer> config){
  ImmutableMap.Builder<TopicPartition,KafkaConsumerInfo<OFFSET>> consumers=ImmutableMap.builder();
  for (  Map.Entry<TopicPartition,Integer> entry : config.entrySet()) {
    consumers.put(entry.getKey(),new KafkaConsumerInfo<OFFSET>(entry.getKey(),entry.getValue(),getBeginOffset(entry.getKey())));
  }
  return consumers.build();
}"
6924,"/** 
 * Deletes the specified namespace
 * @param namespaceId the {@link Id.Namespace} of the specified namespace
 * @throws NamespaceCannotBeDeletedException if the specified namespace cannot be deleted
 * @throws NamespaceNotFoundException if the specified namespace does not exist
 */
public synchronized void deleteNamespace(final Id.Namespace namespaceId) throws NamespaceCannotBeDeletedException, NamespaceNotFoundException {
  if (!hasNamespace(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (checkProgramsRunning(namespaceId)) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  if (checkAdaptersStarted(namespaceId)) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  LOG.info(""String_Node_Str"",namespaceId);
  try {
    preferencesStore.deleteProperties(namespaceId.getId());
    dashboardStore.delete(namespaceId.getId());
    dsFramework.deleteAllInstances(namespaceId);
    dsFramework.deleteAllModules(namespaceId);
    queueAdmin.dropAllInNamespace(namespaceId.getId());
    streamAdmin.dropAllInNamespace(namespaceId);
    scheduler.deleteAllSchedules(namespaceId);
    applicationLifecycleService.removeAll(namespaceId);
    adapterService.removeAdapters(namespaceId);
    store.removeAll(namespaceId);
    deleteMetrics(namespaceId);
    if (!Constants.DEFAULT_NAMESPACE_ID.equals(namespaceId)) {
      dsFramework.deleteNamespace(namespaceId);
      store.deleteNamespace(namespaceId);
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
  LOG.info(""String_Node_Str"",namespaceId);
}","/** 
 * Deletes the specified namespace
 * @param namespaceId the {@link Id.Namespace} of the specified namespace
 * @throws NamespaceCannotBeDeletedException if the specified namespace cannot be deleted
 * @throws NamespaceNotFoundException if the specified namespace does not exist
 */
public synchronized void deleteNamespace(final Id.Namespace namespaceId) throws NamespaceCannotBeDeletedException, NamespaceNotFoundException {
  if (!hasNamespace(namespaceId)) {
    throw new NamespaceNotFoundException(namespaceId);
  }
  if (checkProgramsRunning(namespaceId)) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  if (checkAdaptersStarted(namespaceId)) {
    throw new NamespaceCannotBeDeletedException(namespaceId,String.format(""String_Node_Str"" + ""String_Node_Str"",namespaceId));
  }
  LOG.info(""String_Node_Str"",namespaceId);
  try {
    preferencesStore.deleteProperties(namespaceId.getId());
    dashboardStore.delete(namespaceId.getId());
    dsFramework.deleteAllInstances(namespaceId);
    dsFramework.deleteAllModules(namespaceId);
    queueAdmin.dropAllInNamespace(namespaceId.getId());
    streamAdmin.dropAllInNamespace(namespaceId);
    scheduler.deleteAllSchedules(namespaceId);
    applicationLifecycleService.removeAll(namespaceId);
    adapterService.removeAdapters(namespaceId);
    store.removeAll(namespaceId);
    deleteMetrics(namespaceId);
    if (!Constants.DEFAULT_NAMESPACE_ID.equals(namespaceId)) {
      store.deleteNamespace(namespaceId);
      dsFramework.deleteNamespace(namespaceId);
    }
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",namespaceId,e);
    throw new NamespaceCannotBeDeletedException(namespaceId,e);
  }
  LOG.info(""String_Node_Str"",namespaceId);
}"
6925,"@Override public Module getStandaloneModules(){
  return new PrivateModule(){
    @Override protected void configure(){
      install(new SystemDatasetRuntimeModule().getStandaloneModules());
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str"")).toProvider(DatasetMdsProvider.class);
      expose(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str""));
      bind(MDSDatasetsRegistry.class).in(Singleton.class);
      Multibinder.newSetBinder(binder(),DatasetMetricsReporter.class).addBinding().to(LevelDBDatasetMetricsReporter.class);
      bind(DatasetService.class);
      expose(DatasetService.class);
      Named datasetUserName=Names.named(Constants.Service.DATASET_EXECUTOR);
      Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,datasetUserName);
      CommonHandlers.add(handlerBinder);
      handlerBinder.addBinding().to(DatasetAdminOpHTTPHandler.class);
      bind(DatasetOpExecutorService.class).in(Scopes.SINGLETON);
      expose(DatasetOpExecutorService.class);
      bind(DatasetOpExecutor.class).to(LocalDatasetOpExecutor.class);
      expose(DatasetOpExecutor.class);
      bind(UnderlyingSystemNamespaceAdmin.class).to(LocalUnderlyingSystemNamespaceAdmin.class);
      expose(UnderlyingSystemNamespaceAdmin.class);
    }
  }
;
}","@Override public Module getStandaloneModules(){
  return new PrivateModule(){
    @Override protected void configure(){
      install(new SystemDatasetRuntimeModule().getStandaloneModules());
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str"")).toProvider(DatasetMdsProvider.class);
      expose(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str""));
      bind(MDSDatasetsRegistry.class).in(Singleton.class);
      Multibinder.newSetBinder(binder(),DatasetMetricsReporter.class).addBinding().to(LevelDBDatasetMetricsReporter.class);
      bind(DatasetService.class);
      expose(DatasetService.class);
      Named datasetUserName=Names.named(Constants.Service.DATASET_EXECUTOR);
      Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,datasetUserName);
      CommonHandlers.add(handlerBinder);
      handlerBinder.addBinding().to(DatasetAdminOpHTTPHandler.class);
      bind(DatasetOpExecutorService.class).in(Scopes.SINGLETON);
      expose(DatasetOpExecutorService.class);
      bind(DatasetOpExecutor.class).to(LocalDatasetOpExecutor.class);
      expose(DatasetOpExecutor.class);
      bind(StorageProviderNamespaceAdmin.class).to(LocalStorageProviderNamespaceAdmin.class);
      expose(StorageProviderNamespaceAdmin.class);
    }
  }
;
}"
6926,"@Override public Module getDistributedModules(){
  return new PrivateModule(){
    @Override protected void configure(){
      install(new SystemDatasetRuntimeModule().getDistributedModules());
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str"")).toProvider(DatasetMdsProvider.class);
      expose(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str""));
      bind(MDSDatasetsRegistry.class).in(Singleton.class);
      Multibinder.newSetBinder(binder(),DatasetMetricsReporter.class).addBinding().to(HBaseDatasetMetricsReporter.class);
      bind(DatasetService.class);
      expose(DatasetService.class);
      Named datasetUserName=Names.named(Constants.Service.DATASET_EXECUTOR);
      Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,datasetUserName);
      CommonHandlers.add(handlerBinder);
      handlerBinder.addBinding().to(DatasetAdminOpHTTPHandler.class);
      bind(DatasetOpExecutorService.class).in(Scopes.SINGLETON);
      expose(DatasetOpExecutorService.class);
      bind(DatasetOpExecutor.class).to(YarnDatasetOpExecutor.class);
      expose(DatasetOpExecutor.class);
      bind(UnderlyingSystemNamespaceAdmin.class).to(DistributedUnderlyingSystemNamespaceAdmin.class);
      expose(UnderlyingSystemNamespaceAdmin.class);
    }
  }
;
}","@Override public Module getDistributedModules(){
  return new PrivateModule(){
    @Override protected void configure(){
      install(new SystemDatasetRuntimeModule().getDistributedModules());
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str"")).toProvider(DatasetMdsProvider.class);
      expose(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str""));
      bind(MDSDatasetsRegistry.class).in(Singleton.class);
      Multibinder.newSetBinder(binder(),DatasetMetricsReporter.class).addBinding().to(HBaseDatasetMetricsReporter.class);
      bind(DatasetService.class);
      expose(DatasetService.class);
      Named datasetUserName=Names.named(Constants.Service.DATASET_EXECUTOR);
      Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,datasetUserName);
      CommonHandlers.add(handlerBinder);
      handlerBinder.addBinding().to(DatasetAdminOpHTTPHandler.class);
      bind(DatasetOpExecutorService.class).in(Scopes.SINGLETON);
      expose(DatasetOpExecutorService.class);
      bind(DatasetOpExecutor.class).to(YarnDatasetOpExecutor.class);
      expose(DatasetOpExecutor.class);
      bind(StorageProviderNamespaceAdmin.class).to(DistributedStorageProviderNamespaceAdmin.class);
      expose(StorageProviderNamespaceAdmin.class);
    }
  }
;
}"
6927,"@Override public Module getInMemoryModules(){
  return new PrivateModule(){
    @Override protected void configure(){
      install(new SystemDatasetRuntimeModule().getInMemoryModules());
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str"")).toProvider(DatasetMdsProvider.class);
      expose(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str""));
      bind(MDSDatasetsRegistry.class).in(Singleton.class);
      bind(DatasetService.class);
      expose(DatasetService.class);
      Named datasetUserName=Names.named(Constants.Service.DATASET_EXECUTOR);
      Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,datasetUserName);
      CommonHandlers.add(handlerBinder);
      handlerBinder.addBinding().to(DatasetAdminOpHTTPHandler.class);
      Multibinder.newSetBinder(binder(),DatasetMetricsReporter.class);
      bind(DatasetOpExecutorService.class).in(Scopes.SINGLETON);
      expose(DatasetOpExecutorService.class);
      bind(DatasetOpExecutor.class).to(LocalDatasetOpExecutor.class);
      expose(DatasetOpExecutor.class);
      bind(UnderlyingSystemNamespaceAdmin.class).to(LocalUnderlyingSystemNamespaceAdmin.class);
      expose(UnderlyingSystemNamespaceAdmin.class);
    }
  }
;
}","@Override public Module getInMemoryModules(){
  return new PrivateModule(){
    @Override protected void configure(){
      install(new SystemDatasetRuntimeModule().getInMemoryModules());
      install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
      bind(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str"")).toProvider(DatasetMdsProvider.class);
      expose(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str""));
      bind(MDSDatasetsRegistry.class).in(Singleton.class);
      bind(DatasetService.class);
      expose(DatasetService.class);
      Named datasetUserName=Names.named(Constants.Service.DATASET_EXECUTOR);
      Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,datasetUserName);
      CommonHandlers.add(handlerBinder);
      handlerBinder.addBinding().to(DatasetAdminOpHTTPHandler.class);
      Multibinder.newSetBinder(binder(),DatasetMetricsReporter.class);
      bind(DatasetOpExecutorService.class).in(Scopes.SINGLETON);
      expose(DatasetOpExecutorService.class);
      bind(DatasetOpExecutor.class).to(LocalDatasetOpExecutor.class);
      expose(DatasetOpExecutor.class);
      bind(StorageProviderNamespaceAdmin.class).to(LocalStorageProviderNamespaceAdmin.class);
      expose(StorageProviderNamespaceAdmin.class);
    }
  }
;
}"
6928,"@Override protected void configure(){
  install(new SystemDatasetRuntimeModule().getDistributedModules());
  install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
  bind(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str"")).toProvider(DatasetMdsProvider.class);
  expose(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str""));
  bind(MDSDatasetsRegistry.class).in(Singleton.class);
  Multibinder.newSetBinder(binder(),DatasetMetricsReporter.class).addBinding().to(HBaseDatasetMetricsReporter.class);
  bind(DatasetService.class);
  expose(DatasetService.class);
  Named datasetUserName=Names.named(Constants.Service.DATASET_EXECUTOR);
  Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,datasetUserName);
  CommonHandlers.add(handlerBinder);
  handlerBinder.addBinding().to(DatasetAdminOpHTTPHandler.class);
  bind(DatasetOpExecutorService.class).in(Scopes.SINGLETON);
  expose(DatasetOpExecutorService.class);
  bind(DatasetOpExecutor.class).to(YarnDatasetOpExecutor.class);
  expose(DatasetOpExecutor.class);
  bind(UnderlyingSystemNamespaceAdmin.class).to(DistributedUnderlyingSystemNamespaceAdmin.class);
  expose(UnderlyingSystemNamespaceAdmin.class);
}","@Override protected void configure(){
  install(new SystemDatasetRuntimeModule().getDistributedModules());
  install(new FactoryModuleBuilder().implement(DatasetDefinitionRegistry.class,DefaultDatasetDefinitionRegistry.class).build(DatasetDefinitionRegistryFactory.class));
  bind(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str"")).toProvider(DatasetMdsProvider.class);
  expose(DatasetFramework.class).annotatedWith(Names.named(""String_Node_Str""));
  bind(MDSDatasetsRegistry.class).in(Singleton.class);
  Multibinder.newSetBinder(binder(),DatasetMetricsReporter.class).addBinding().to(HBaseDatasetMetricsReporter.class);
  bind(DatasetService.class);
  expose(DatasetService.class);
  Named datasetUserName=Names.named(Constants.Service.DATASET_EXECUTOR);
  Multibinder<HttpHandler> handlerBinder=Multibinder.newSetBinder(binder(),HttpHandler.class,datasetUserName);
  CommonHandlers.add(handlerBinder);
  handlerBinder.addBinding().to(DatasetAdminOpHTTPHandler.class);
  bind(DatasetOpExecutorService.class).in(Scopes.SINGLETON);
  expose(DatasetOpExecutorService.class);
  bind(DatasetOpExecutor.class).to(YarnDatasetOpExecutor.class);
  expose(DatasetOpExecutor.class);
  bind(StorageProviderNamespaceAdmin.class).to(DistributedStorageProviderNamespaceAdmin.class);
  expose(StorageProviderNamespaceAdmin.class);
}"
6929,"@Inject public DatasetService(CConfiguration cConf,NamespacedLocationFactory namespacedLocationFactory,DiscoveryService discoveryService,DiscoveryServiceClient discoveryServiceClient,DatasetTypeManager typeManager,DatasetInstanceManager instanceManager,MetricsCollectionService metricsCollectionService,DatasetOpExecutor opExecutorClient,MDSDatasetsRegistry mdsDatasets,ExploreFacade exploreFacade,Set<DatasetMetricsReporter> metricReporters,UnderlyingSystemNamespaceAdmin underlyingSystemNamespaceAdmin,UsageRegistry usageRegistry) throws Exception {
  this.typeManager=typeManager;
  DatasetTypeHandler datasetTypeHandler=new DatasetTypeHandler(typeManager,cConf,namespacedLocationFactory);
  DatasetInstanceHandler datasetInstanceHandler=new DatasetInstanceHandler(typeManager,instanceManager,opExecutorClient,exploreFacade,cConf,usageRegistry);
  UnderlyingSystemNamespaceHandler underlyingSystemNamespaceHandler=new UnderlyingSystemNamespaceHandler(underlyingSystemNamespaceAdmin);
  NettyHttpService.Builder builder=new CommonNettyHttpServiceBuilder(cConf);
  builder.addHttpHandlers(ImmutableList.of(datasetTypeHandler,datasetInstanceHandler,underlyingSystemNamespaceHandler));
  builder.setHandlerHooks(ImmutableList.of(new MetricsReporterHook(metricsCollectionService,Constants.Service.DATASET_MANAGER)));
  builder.setHost(cConf.get(Constants.Dataset.Manager.ADDRESS));
  builder.setConnectionBacklog(cConf.getInt(Constants.Dataset.Manager.BACKLOG_CONNECTIONS,Constants.Dataset.Manager.DEFAULT_BACKLOG));
  builder.setExecThreadPoolSize(cConf.getInt(Constants.Dataset.Manager.EXEC_THREADS,Constants.Dataset.Manager.DEFAULT_EXEC_THREADS));
  builder.setBossThreadPoolSize(cConf.getInt(Constants.Dataset.Manager.BOSS_THREADS,Constants.Dataset.Manager.DEFAULT_BOSS_THREADS));
  builder.setWorkerThreadPoolSize(cConf.getInt(Constants.Dataset.Manager.WORKER_THREADS,Constants.Dataset.Manager.DEFAULT_WORKER_THREADS));
  this.httpService=builder.build();
  this.discoveryService=discoveryService;
  this.discoveryServiceClient=discoveryServiceClient;
  this.opExecutorClient=opExecutorClient;
  this.mdsDatasets=mdsDatasets;
  this.metricReporters=metricReporters;
}","@Inject public DatasetService(CConfiguration cConf,NamespacedLocationFactory namespacedLocationFactory,DiscoveryService discoveryService,DiscoveryServiceClient discoveryServiceClient,DatasetTypeManager typeManager,DatasetInstanceManager instanceManager,MetricsCollectionService metricsCollectionService,DatasetOpExecutor opExecutorClient,MDSDatasetsRegistry mdsDatasets,ExploreFacade exploreFacade,Set<DatasetMetricsReporter> metricReporters,StorageProviderNamespaceAdmin storageProviderNamespaceAdmin,UsageRegistry usageRegistry) throws Exception {
  this.typeManager=typeManager;
  DatasetTypeHandler datasetTypeHandler=new DatasetTypeHandler(typeManager,cConf,namespacedLocationFactory);
  DatasetInstanceHandler datasetInstanceHandler=new DatasetInstanceHandler(typeManager,instanceManager,opExecutorClient,exploreFacade,cConf,usageRegistry);
  UnderlyingSystemNamespaceHandler underlyingSystemNamespaceHandler=new UnderlyingSystemNamespaceHandler(storageProviderNamespaceAdmin);
  NettyHttpService.Builder builder=new CommonNettyHttpServiceBuilder(cConf);
  builder.addHttpHandlers(ImmutableList.of(datasetTypeHandler,datasetInstanceHandler,underlyingSystemNamespaceHandler));
  builder.setHandlerHooks(ImmutableList.of(new MetricsReporterHook(metricsCollectionService,Constants.Service.DATASET_MANAGER)));
  builder.setHost(cConf.get(Constants.Dataset.Manager.ADDRESS));
  builder.setConnectionBacklog(cConf.getInt(Constants.Dataset.Manager.BACKLOG_CONNECTIONS,Constants.Dataset.Manager.DEFAULT_BACKLOG));
  builder.setExecThreadPoolSize(cConf.getInt(Constants.Dataset.Manager.EXEC_THREADS,Constants.Dataset.Manager.DEFAULT_EXEC_THREADS));
  builder.setBossThreadPoolSize(cConf.getInt(Constants.Dataset.Manager.BOSS_THREADS,Constants.Dataset.Manager.DEFAULT_BOSS_THREADS));
  builder.setWorkerThreadPoolSize(cConf.getInt(Constants.Dataset.Manager.WORKER_THREADS,Constants.Dataset.Manager.DEFAULT_WORKER_THREADS));
  this.httpService=builder.build();
  this.discoveryService=discoveryService;
  this.discoveryServiceClient=discoveryServiceClient;
  this.opExecutorClient=opExecutorClient;
  this.mdsDatasets=mdsDatasets;
  this.metricReporters=metricReporters;
}"
6930,"@PUT @Path(""String_Node_Str"") public void createNamespace(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  try {
    underlyingSystemNamespaceAdmin.create(Id.Namespace.from(namespaceId));
  }
 catch (  IOException e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + e.getMessage());
    return;
  }
catch (  ExploreException e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + e.getMessage());
    return;
  }
catch (  SQLException e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + e.getMessage());
    return;
  }
  responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",namespaceId));
}","@PUT @Path(""String_Node_Str"") public void createNamespace(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  try {
    storageProviderNamespaceAdmin.create(Id.Namespace.from(namespaceId));
  }
 catch (  IOException e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + e.getMessage());
    return;
  }
catch (  ExploreException e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + e.getMessage());
    return;
  }
catch (  SQLException e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + e.getMessage());
    return;
  }
  responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",namespaceId));
}"
6931,"@Inject public UnderlyingSystemNamespaceHandler(UnderlyingSystemNamespaceAdmin underlyingSystemNamespaceAdmin){
  this.underlyingSystemNamespaceAdmin=underlyingSystemNamespaceAdmin;
}","@Inject public UnderlyingSystemNamespaceHandler(StorageProviderNamespaceAdmin storageProviderNamespaceAdmin){
  this.storageProviderNamespaceAdmin=storageProviderNamespaceAdmin;
}"
6932,"@DELETE @Path(""String_Node_Str"") public void deleteNamespace(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  try {
    underlyingSystemNamespaceAdmin.delete(Id.Namespace.from(namespaceId));
  }
 catch (  IOException e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + e.getMessage());
    return;
  }
catch (  ExploreException e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + e.getMessage());
    return;
  }
catch (  SQLException e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + e.getMessage());
    return;
  }
  responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",namespaceId));
}","@DELETE @Path(""String_Node_Str"") public void deleteNamespace(HttpRequest request,HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId){
  try {
    storageProviderNamespaceAdmin.delete(Id.Namespace.from(namespaceId));
  }
 catch (  IOException e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + e.getMessage());
    return;
  }
catch (  ExploreException e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + e.getMessage());
    return;
  }
catch (  SQLException e) {
    responder.sendString(HttpResponseStatus.INTERNAL_SERVER_ERROR,""String_Node_Str"" + e.getMessage());
    return;
  }
  responder.sendString(HttpResponseStatus.OK,String.format(""String_Node_Str"",namespaceId));
}"
6933,"@Before public void before() throws Exception {
  File dataDir=new File(tmpFolder.newFolder(),""String_Node_Str"");
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,dataDir.getAbsolutePath());
  cConf.set(Constants.Dataset.Manager.ADDRESS,""String_Node_Str"");
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  InMemoryDiscoveryService discoveryService=new InMemoryDiscoveryService();
  MetricsCollectionService metricsCollectionService=new NoOpMetricsCollectionService();
  Configuration txConf=HBaseConfiguration.create();
  CConfigurationUtil.copyTxProperties(cConf,txConf);
  txManager=new TransactionManager(txConf);
  txManager.startAndWait();
  InMemoryTxSystemClient txSystemClient=new InMemoryTxSystemClient(txManager);
  locationFactory=new LocalLocationFactory(new File(cConf.get(Constants.CFG_LOCAL_DATA_DIR)));
  NamespacedLocationFactory namespacedLocationFactory=new DefaultNamespacedLocationFactory(cConf,locationFactory);
  framework=new RemoteDatasetFramework(discoveryService,registryFactory,new LocalDatasetTypeClassLoaderFactory());
  ImmutableSet<HttpHandler> handlers=ImmutableSet.<HttpHandler>of(new DatasetAdminOpHTTPHandler(new NoAuthenticator(),framework));
  opExecutorService=new DatasetOpExecutorService(cConf,discoveryService,metricsCollectionService,handlers);
  opExecutorService.startAndWait();
  ImmutableMap<String,DatasetModule> modules=ImmutableMap.<String,DatasetModule>builder().put(""String_Node_Str"",new InMemoryTableModule()).put(""String_Node_Str"",new CoreDatasetsModule()).putAll(DatasetMetaTableUtil.getModules()).build();
  InMemoryDatasetFramework mdsFramework=new InMemoryDatasetFramework(registryFactory,modules,cConf);
  MDSDatasetsRegistry mdsDatasetsRegistry=new MDSDatasetsRegistry(txSystemClient,mdsFramework);
  ExploreFacade exploreFacade=new ExploreFacade(new DiscoveryExploreClient(discoveryService),cConf);
  service=new DatasetService(cConf,namespacedLocationFactory,discoveryService,discoveryService,new DatasetTypeManager(cConf,mdsDatasetsRegistry,locationFactory,DEFAULT_MODULES),new DatasetInstanceManager(mdsDatasetsRegistry),metricsCollectionService,new InMemoryDatasetOpExecutor(framework),mdsDatasetsRegistry,exploreFacade,new HashSet<DatasetMetricsReporter>(),new LocalUnderlyingSystemNamespaceAdmin(cConf,namespacedLocationFactory,exploreFacade),new UsageRegistry(txExecutorFactory,framework));
  service.start();
  final CountDownLatch startLatch=new CountDownLatch(1);
  discoveryService.discover(Constants.Service.DATASET_MANAGER).watchChanges(new ServiceDiscovered.ChangeListener(){
    @Override public void onChange(    ServiceDiscovered serviceDiscovered){
      if (!Iterables.isEmpty(serviceDiscovered)) {
        startLatch.countDown();
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  startLatch.await(5,TimeUnit.SECONDS);
  framework.createNamespace(Constants.SYSTEM_NAMESPACE_ID);
  framework.createNamespace(NAMESPACE_ID);
}","@Before public void before() throws Exception {
  File dataDir=new File(tmpFolder.newFolder(),""String_Node_Str"");
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,dataDir.getAbsolutePath());
  cConf.set(Constants.Dataset.Manager.ADDRESS,""String_Node_Str"");
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  InMemoryDiscoveryService discoveryService=new InMemoryDiscoveryService();
  MetricsCollectionService metricsCollectionService=new NoOpMetricsCollectionService();
  Configuration txConf=HBaseConfiguration.create();
  CConfigurationUtil.copyTxProperties(cConf,txConf);
  txManager=new TransactionManager(txConf);
  txManager.startAndWait();
  InMemoryTxSystemClient txSystemClient=new InMemoryTxSystemClient(txManager);
  LocalLocationFactory locationFactory=new LocalLocationFactory(new File(cConf.get(Constants.CFG_LOCAL_DATA_DIR)));
  NamespacedLocationFactory namespacedLocationFactory=new DefaultNamespacedLocationFactory(cConf,locationFactory);
  framework=new RemoteDatasetFramework(discoveryService,registryFactory,new LocalDatasetTypeClassLoaderFactory());
  ImmutableSet<HttpHandler> handlers=ImmutableSet.<HttpHandler>of(new DatasetAdminOpHTTPHandler(new NoAuthenticator(),framework));
  opExecutorService=new DatasetOpExecutorService(cConf,discoveryService,metricsCollectionService,handlers);
  opExecutorService.startAndWait();
  ImmutableMap<String,DatasetModule> modules=ImmutableMap.<String,DatasetModule>builder().put(""String_Node_Str"",new InMemoryTableModule()).put(""String_Node_Str"",new CoreDatasetsModule()).putAll(DatasetMetaTableUtil.getModules()).build();
  InMemoryDatasetFramework mdsFramework=new InMemoryDatasetFramework(registryFactory,modules,cConf);
  MDSDatasetsRegistry mdsDatasetsRegistry=new MDSDatasetsRegistry(txSystemClient,mdsFramework);
  ExploreFacade exploreFacade=new ExploreFacade(new DiscoveryExploreClient(discoveryService),cConf);
  service=new DatasetService(cConf,namespacedLocationFactory,discoveryService,discoveryService,new DatasetTypeManager(cConf,mdsDatasetsRegistry,locationFactory,DEFAULT_MODULES),new DatasetInstanceManager(mdsDatasetsRegistry),metricsCollectionService,new InMemoryDatasetOpExecutor(framework),mdsDatasetsRegistry,exploreFacade,new HashSet<DatasetMetricsReporter>(),new LocalStorageProviderNamespaceAdmin(cConf,namespacedLocationFactory,exploreFacade),new UsageRegistry(txExecutorFactory,framework));
  service.start();
  final CountDownLatch startLatch=new CountDownLatch(1);
  discoveryService.discover(Constants.Service.DATASET_MANAGER).watchChanges(new ServiceDiscovered.ChangeListener(){
    @Override public void onChange(    ServiceDiscovered serviceDiscovered){
      if (!Iterables.isEmpty(serviceDiscovered)) {
        startLatch.countDown();
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  startLatch.await(5,TimeUnit.SECONDS);
  framework.createNamespace(Constants.SYSTEM_NAMESPACE_ID);
  framework.createNamespace(NAMESPACE_ID);
}"
6934,"@Before public void before() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  File dataDir=new File(tmpFolder.newFolder(),""String_Node_Str"");
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,dataDir.getAbsolutePath());
  if (!DirUtils.mkdirs(dataDir)) {
    throw new RuntimeException(String.format(""String_Node_Str"",dataDir));
  }
  cConf.set(Constants.Dataset.Manager.OUTPUT_DIR,dataDir.getAbsolutePath());
  cConf.set(Constants.Dataset.Manager.ADDRESS,""String_Node_Str"");
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  discoveryService=new InMemoryDiscoveryService();
  MetricsCollectionService metricsCollectionService=new NoOpMetricsCollectionService();
  Configuration txConf=HBaseConfiguration.create();
  CConfigurationUtil.copyTxProperties(cConf,txConf);
  txManager=new TransactionManager(txConf);
  txManager.startAndWait();
  InMemoryTxSystemClient txSystemClient=new InMemoryTxSystemClient(txManager);
  final Injector injector=Guice.createInjector(new ConfigModule(cConf),new LocationRuntimeModule().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),new TransactionInMemoryModule());
  DatasetDefinitionRegistryFactory registryFactory=new DatasetDefinitionRegistryFactory(){
    @Override public DatasetDefinitionRegistry create(){
      DefaultDatasetDefinitionRegistry registry=new DefaultDatasetDefinitionRegistry();
      injector.injectMembers(registry);
      return registry;
    }
  }
;
  locationFactory=injector.getInstance(LocationFactory.class);
  NamespacedLocationFactory namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  dsFramework=new RemoteDatasetFramework(discoveryService,registryFactory,new LocalDatasetTypeClassLoaderFactory());
  ImmutableSet<HttpHandler> handlers=ImmutableSet.<HttpHandler>of(new DatasetAdminOpHTTPHandler(new NoAuthenticator(),dsFramework));
  opExecutorService=new DatasetOpExecutorService(cConf,discoveryService,metricsCollectionService,handlers);
  opExecutorService.startAndWait();
  ImmutableMap<String,DatasetModule> modules=ImmutableMap.<String,DatasetModule>builder().putAll(injector.getInstance(Key.get(new TypeLiteral<Map<String,DatasetModule>>(){
  }
,Names.named(""String_Node_Str"")))).putAll(DatasetMetaTableUtil.getModules()).build();
  TransactionExecutorFactory txExecutorFactory=injector.getInstance(TransactionExecutorFactory.class);
  MDSDatasetsRegistry mdsDatasetsRegistry=new MDSDatasetsRegistry(txSystemClient,new InMemoryDatasetFramework(registryFactory,modules,cConf));
  ExploreFacade exploreFacade=new ExploreFacade(new DiscoveryExploreClient(discoveryService),cConf);
  service=new DatasetService(cConf,namespacedLocationFactory,discoveryService,discoveryService,new DatasetTypeManager(cConf,mdsDatasetsRegistry,locationFactory,Collections.<String,DatasetModule>emptyMap()),new DatasetInstanceManager(mdsDatasetsRegistry),metricsCollectionService,new InMemoryDatasetOpExecutor(dsFramework),mdsDatasetsRegistry,exploreFacade,new HashSet<DatasetMetricsReporter>(),new LocalUnderlyingSystemNamespaceAdmin(cConf,namespacedLocationFactory,exploreFacade),new UsageRegistry(txExecutorFactory,dsFramework));
  service.start();
  final CountDownLatch startLatch=new CountDownLatch(1);
  discoveryService.discover(Constants.Service.DATASET_MANAGER).watchChanges(new ServiceDiscovered.ChangeListener(){
    @Override public void onChange(    ServiceDiscovered serviceDiscovered){
      if (!Iterables.isEmpty(serviceDiscovered)) {
        startLatch.countDown();
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  startLatch.await(5,TimeUnit.SECONDS);
  Locations.mkdirsIfNotExists(namespacedLocationFactory.get(Constants.DEFAULT_NAMESPACE_ID));
}","@Before public void before() throws Exception {
  CConfiguration cConf=CConfiguration.create();
  File dataDir=new File(tmpFolder.newFolder(),""String_Node_Str"");
  cConf.set(Constants.CFG_LOCAL_DATA_DIR,dataDir.getAbsolutePath());
  if (!DirUtils.mkdirs(dataDir)) {
    throw new RuntimeException(String.format(""String_Node_Str"",dataDir));
  }
  cConf.set(Constants.Dataset.Manager.OUTPUT_DIR,dataDir.getAbsolutePath());
  cConf.set(Constants.Dataset.Manager.ADDRESS,""String_Node_Str"");
  cConf.setBoolean(Constants.Dangerous.UNRECOVERABLE_RESET,true);
  discoveryService=new InMemoryDiscoveryService();
  MetricsCollectionService metricsCollectionService=new NoOpMetricsCollectionService();
  Configuration txConf=HBaseConfiguration.create();
  CConfigurationUtil.copyTxProperties(cConf,txConf);
  txManager=new TransactionManager(txConf);
  txManager.startAndWait();
  InMemoryTxSystemClient txSystemClient=new InMemoryTxSystemClient(txManager);
  final Injector injector=Guice.createInjector(new ConfigModule(cConf),new LocationRuntimeModule().getInMemoryModules(),new SystemDatasetRuntimeModule().getInMemoryModules(),new TransactionInMemoryModule());
  DatasetDefinitionRegistryFactory registryFactory=new DatasetDefinitionRegistryFactory(){
    @Override public DatasetDefinitionRegistry create(){
      DefaultDatasetDefinitionRegistry registry=new DefaultDatasetDefinitionRegistry();
      injector.injectMembers(registry);
      return registry;
    }
  }
;
  locationFactory=injector.getInstance(LocationFactory.class);
  NamespacedLocationFactory namespacedLocationFactory=injector.getInstance(NamespacedLocationFactory.class);
  dsFramework=new RemoteDatasetFramework(discoveryService,registryFactory,new LocalDatasetTypeClassLoaderFactory());
  ImmutableSet<HttpHandler> handlers=ImmutableSet.<HttpHandler>of(new DatasetAdminOpHTTPHandler(new NoAuthenticator(),dsFramework));
  opExecutorService=new DatasetOpExecutorService(cConf,discoveryService,metricsCollectionService,handlers);
  opExecutorService.startAndWait();
  ImmutableMap<String,DatasetModule> modules=ImmutableMap.<String,DatasetModule>builder().putAll(injector.getInstance(Key.get(new TypeLiteral<Map<String,DatasetModule>>(){
  }
,Names.named(""String_Node_Str"")))).putAll(DatasetMetaTableUtil.getModules()).build();
  TransactionExecutorFactory txExecutorFactory=injector.getInstance(TransactionExecutorFactory.class);
  MDSDatasetsRegistry mdsDatasetsRegistry=new MDSDatasetsRegistry(txSystemClient,new InMemoryDatasetFramework(registryFactory,modules,cConf));
  ExploreFacade exploreFacade=new ExploreFacade(new DiscoveryExploreClient(discoveryService),cConf);
  service=new DatasetService(cConf,namespacedLocationFactory,discoveryService,discoveryService,new DatasetTypeManager(cConf,mdsDatasetsRegistry,locationFactory,Collections.<String,DatasetModule>emptyMap()),new DatasetInstanceManager(mdsDatasetsRegistry),metricsCollectionService,new InMemoryDatasetOpExecutor(dsFramework),mdsDatasetsRegistry,exploreFacade,new HashSet<DatasetMetricsReporter>(),new LocalStorageProviderNamespaceAdmin(cConf,namespacedLocationFactory,exploreFacade),new UsageRegistry(txExecutorFactory,dsFramework));
  service.start();
  final CountDownLatch startLatch=new CountDownLatch(1);
  discoveryService.discover(Constants.Service.DATASET_MANAGER).watchChanges(new ServiceDiscovered.ChangeListener(){
    @Override public void onChange(    ServiceDiscovered serviceDiscovered){
      if (!Iterables.isEmpty(serviceDiscovered)) {
        startLatch.countDown();
      }
    }
  }
,Threads.SAME_THREAD_EXECUTOR);
  startLatch.await(5,TimeUnit.SECONDS);
  Locations.mkdirsIfNotExists(namespacedLocationFactory.get(Constants.DEFAULT_NAMESPACE_ID));
}"
6935,"@Test public void test() throws IOException {
  Assert.assertEquals(200,createNamespace(""String_Node_Str"").getResponseCode());
  Assert.assertEquals(500,createNamespace(""String_Node_Str"").getResponseCode());
  Assert.assertEquals(200,deleteNamespace(""String_Node_Str"").getResponseCode());
}","@Test public void test() throws IOException {
  Assert.assertEquals(200,createNamespace(""String_Node_Str"").getResponseCode());
  Assert.assertEquals(200,createNamespace(""String_Node_Str"").getResponseCode());
  Assert.assertEquals(200,deleteNamespace(""String_Node_Str"").getResponseCode());
}"
6936,"@Test public void testNamespaceCreationDeletion() throws DatasetManagementException {
  DatasetFramework framework=getFramework();
  Id.Namespace namespace=Id.Namespace.from(""String_Node_Str"");
  framework.createNamespace(namespace);
  try {
    framework.createNamespace(namespace);
    Assert.fail(""String_Node_Str"");
  }
 catch (  DatasetManagementException e) {
  }
  framework.deleteNamespace(namespace);
}","@Test public void testNamespaceCreationDeletion() throws DatasetManagementException {
  DatasetFramework framework=getFramework();
  Id.Namespace namespace=Id.Namespace.from(""String_Node_Str"");
  framework.createNamespace(namespace);
  framework.deleteNamespace(namespace);
}"
6937,"protected String determinePattern(String action){
  if (action.equals(""String_Node_Str"")) {
switch (type) {
case INSTANCE:
      return String.format(""String_Node_Str"",type.getName(),ArgumentName.RUNTIME_ARGS);
case NAMESPACE:
    return String.format(""String_Node_Str"",type.getName(),ArgumentName.RUNTIME_ARGS);
case APP:
  return String.format(""String_Node_Str"",type.getName(),ArgumentName.RUNTIME_ARGS,type.getArgumentName());
case FLOW:
case MAPREDUCE:
case WORKFLOW:
case SERVICE:
case SPARK:
return String.format(""String_Node_Str"",type.getName(),ArgumentName.RUNTIME_ARGS,type.getArgumentName());
}
}
 else if (action.equals(""String_Node_Str"")) {
switch (type) {
case INSTANCE:
return String.format(""String_Node_Str"",type.getName(),ArgumentName.LOCAL_FILE_PATH,ArgumentName.CONTENT_TYPE);
case NAMESPACE:
return String.format(""String_Node_Str"",type.getName(),ArgumentName.LOCAL_FILE_PATH,ArgumentName.CONTENT_TYPE);
case APP:
return String.format(""String_Node_Str"",type.getName(),ArgumentName.LOCAL_FILE_PATH,ArgumentName.CONTENT_TYPE,type.getArgumentName());
case FLOW:
case MAPREDUCE:
case WORKFLOW:
case SERVICE:
case SPARK:
return String.format(""String_Node_Str"",type.getName(),ArgumentName.LOCAL_FILE_PATH,ArgumentName.CONTENT_TYPE,type.getArgumentName());
}
}
return ""String_Node_Str"";
}","protected String determinePattern(String action){
  if (""String_Node_Str"".equals(action)) {
switch (type) {
case INSTANCE:
case NAMESPACE:
      return String.format(""String_Node_Str"",type.getName(),ArgumentName.RUNTIME_ARGS);
case APP:
    return String.format(""String_Node_Str"",type.getName(),ArgumentName.RUNTIME_ARGS,type.getArgumentName());
case FLOW:
case MAPREDUCE:
case WORKFLOW:
case SERVICE:
case SPARK:
  return String.format(""String_Node_Str"",type.getName(),ArgumentName.RUNTIME_ARGS,type.getArgumentName());
}
}
 else if (""String_Node_Str"".equals(action)) {
switch (type) {
case INSTANCE:
case NAMESPACE:
return String.format(""String_Node_Str"",type.getName(),ArgumentName.LOCAL_FILE_PATH,ArgumentName.CONTENT_TYPE);
case APP:
return String.format(""String_Node_Str"",type.getName(),ArgumentName.LOCAL_FILE_PATH,ArgumentName.CONTENT_TYPE,type.getArgumentName());
case FLOW:
case MAPREDUCE:
case WORKFLOW:
case SERVICE:
case SPARK:
return String.format(""String_Node_Str"",type.getName(),ArgumentName.LOCAL_FILE_PATH,ArgumentName.CONTENT_TYPE,type.getArgumentName());
}
}
return ""String_Node_Str"";
}"
6938,"@Override public String getPattern(){
  return this.determinePattern(""String_Node_Str"");
}","@Override public String getPattern(){
  return determinePattern(""String_Node_Str"");
}"
6939,"@Override public String getPattern(){
  return this.determinePattern(""String_Node_Str"");
}","@Override public String getPattern(){
  return determinePattern(""String_Node_Str"");
}"
6940,"@VisibleForTesting public void registerTemplates(){
  try {
    Map<String,ApplicationTemplateInfo> newInfoMap=Maps.newHashMap();
    Map<File,ApplicationTemplateInfo> newFileTemplateMap=Maps.newHashMap();
    File baseDir=new File(configuration.get(Constants.AppFabric.APP_TEMPLATE_DIR));
    List<File> files=DirUtils.listFiles(baseDir,""String_Node_Str"");
    for (    File file : files) {
      try {
        ApplicationTemplateInfo info=getTemplateInfo(file);
        newInfoMap.put(info.getName(),info);
        newFileTemplateMap.put(info.getFile().getAbsoluteFile(),info);
      }
 catch (      IllegalArgumentException e) {
        LOG.error(""String_Node_Str"",file.getName(),e);
      }
    }
    appTemplateInfos.set(newInfoMap);
    fileToTemplateMap.set(newFileTemplateMap);
    pluginRepository.inspectPlugins(newInfoMap.values());
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"");
  }
}","@VisibleForTesting public void registerTemplates(){
  try {
    Map<String,ApplicationTemplateInfo> newInfoMap=Maps.newHashMap();
    Map<File,ApplicationTemplateInfo> newFileTemplateMap=Maps.newHashMap();
    File baseDir=new File(configuration.get(Constants.AppFabric.APP_TEMPLATE_DIR));
    List<File> files=DirUtils.listFiles(baseDir,""String_Node_Str"");
    for (    File file : files) {
      try {
        ApplicationTemplateInfo info=getTemplateInfo(file);
        newInfoMap.put(info.getName(),info);
        newFileTemplateMap.put(info.getFile().getAbsoluteFile(),info);
      }
 catch (      IllegalArgumentException e) {
        LOG.error(""String_Node_Str"",file.getName(),e);
      }
    }
    appTemplateInfos.set(newInfoMap);
    fileToTemplateMap.set(newFileTemplateMap);
    pluginRepository.inspectPlugins(newInfoMap.values());
  }
 catch (  Exception e) {
    LOG.warn(""String_Node_Str"",e);
  }
}"
6941,"/** 
 * Creates a ClassLoader for the given template application.
 * @param templateJar the template jar file.
 * @return a {@link CloseableClassLoader} for the template application.
 * @throws IOException if failed to expand the jar
 */
private CloseableClassLoader createTemplateClassLoader(File templateJar) throws IOException {
  final File unpackDir=DirUtils.createTempDir(tmpDir);
  BundleJarUtil.unpackProgramJar(Files.newInputStreamSupplier(templateJar),unpackDir);
  ProgramClassLoader programClassLoader=ProgramClassLoader.create(unpackDir,getClass().getClassLoader());
  return new CloseableClassLoader(programClassLoader,new Closeable(){
    @Override public void close() throws IOException {
      DirUtils.deleteDirectoryContents(unpackDir);
    }
  }
);
}","/** 
 * Creates a ClassLoader for the given template application.
 * @param templateJar the template jar file.
 * @return a {@link CloseableClassLoader} for the template application.
 * @throws IOException if failed to expand the jar
 */
private CloseableClassLoader createTemplateClassLoader(File templateJar) throws IOException {
  final File unpackDir=DirUtils.createTempDir(tmpDir);
  BundleJarUtil.unpackProgramJar(Files.newInputStreamSupplier(templateJar),unpackDir);
  ProgramClassLoader programClassLoader=ProgramClassLoader.create(unpackDir,getClass().getClassLoader());
  return new CloseableClassLoader(programClassLoader,new Closeable(){
    @Override public void close(){
      try {
        DirUtils.deleteDirectoryContents(unpackDir);
      }
 catch (      IOException e) {
        LOG.warn(""String_Node_Str"",unpackDir,e);
      }
    }
  }
);
}"
6942,"/** 
 * Helper method to get program id for a run record if it exists in the store.
 * @param store
 * @param namespaceName
 * @param appName
 * @param programType
 * @param programName
 * @param runId
 * @return instance of {@link Id.Program} if exist for the runId or null if does not.
 */
@Nullable private static Id.Program validateProgramForRunRecord(Store store,String namespaceName,String appName,ProgramType programType,String programName,String runId){
  Id.Program programId=Id.Program.from(namespaceName,appName,programType,programName);
  RunRecord runRecord=store.getRun(programId,runId);
  if (runRecord != null) {
    return programId;
  }
 else {
    return null;
  }
}","/** 
 * Helper method to get program id for a run record if it exists in the store.
 * @return instance of {@link Id.Program} if exist for the runId or null if does not.
 */
@Nullable private Id.Program validateProgramForRunRecord(String namespaceName,String appName,ProgramType programType,String programName,String runId){
  Id.Program programId=Id.Program.from(namespaceName,appName,programType,programName);
  RunRecord runRecord=store.getRun(programId,runId);
  if (runRecord != null) {
    return programId;
  }
 else {
    return null;
  }
}"
6943,"@Override public void run(){
  for (  ProgramType programType : ProgramType.values()) {
    programLifecycleService.validateAndCorrectRunningRunRecords(programType);
  }
}","@Override public void run(){
  try {
    RunRecordsCorrectorRunnable.LOG.debug(""String_Node_Str"");
    programLifecycleService.validateAndCorrectRunningRunRecords();
    RunRecordsCorrectorRunnable.LOG.debug(""String_Node_Str"");
  }
 catch (  Throwable t) {
  }
}"
6944,"/** 
 * Helper method to check if the run record is a child program of a Workflow
 * @param runRecord The target {@link RunRecord} to check
 * @return {@code true} of we should check and {@code false} otherwise
 */
private boolean shouldCorrectForWorkflowChildren(RunRecord runRecord){
  if (runRecord.getProperties() != null && runRecord.getProperties().get(""String_Node_Str"") != null) {
    String workflowRunId=runRecord.getProperties().get(""String_Node_Str"");
    Id.Program workflowProgramId=retrieveProgramIdForRunRecord(ProgramType.WORKFLOW,workflowRunId);
    if (workflowProgramId != null) {
      RunRecord wfRunRecord=store.getRun(workflowProgramId,workflowRunId);
      RuntimeInfo wfRuntimeInfo=runtimeService.lookup(workflowProgramId,RunIds.fromString(workflowRunId));
      if (wfRunRecord != null && wfRunRecord.getStatus() == ProgramRunStatus.RUNNING && wfRuntimeInfo != null) {
        return false;
      }
    }
  }
  return true;
}","/** 
 * Helper method to check if the run record is a child program of a Workflow
 * @param runRecord The target {@link RunRecord} to check
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 * @return {@code true} of we should check and {@code false} otherwise
 */
private boolean shouldCorrectForWorkflowChildren(RunRecord runRecord,Set<String> processedInvalidRunRecordIds){
  if (runRecord.getProperties() != null && runRecord.getProperties().get(""String_Node_Str"") != null) {
    String workflowRunId=runRecord.getProperties().get(""String_Node_Str"");
    if (!processedInvalidRunRecordIds.contains(workflowRunId)) {
      Id.Program workflowProgramId=retrieveProgramIdForRunRecord(ProgramType.WORKFLOW,workflowRunId);
      if (workflowProgramId != null) {
        RunRecord wfRunRecord=store.getRun(workflowProgramId,workflowRunId);
        RuntimeInfo wfRuntimeInfo=runtimeService.lookup(workflowProgramId,RunIds.fromString(workflowRunId));
        if (wfRunRecord != null && wfRunRecord.getStatus() == ProgramRunStatus.RUNNING && wfRuntimeInfo != null) {
          return false;
        }
      }
    }
  }
  return true;
}"
6945,"/** 
 * Helper method to get   {@link co.cask.cdap.proto.Id.Program} for a RunRecord for type of program
 * @param programType Type of program to search
 * @param runId The target id of the {@link RunRecord} to find
 * @return the program id of the run record or {@code null} if does not exist.
 */
@Nullable private Id.Program retrieveProgramIdForRunRecord(ProgramType programType,String runId){
  List<NamespaceMeta> namespaceMetas=store.listNamespaces();
  Id.Program targetProgramId=null;
  for (  NamespaceMeta nm : namespaceMetas) {
    Id.Namespace accId=Id.Namespace.from(nm.getName());
    Collection<ApplicationSpecification> appSpecs=store.getAllApplications(accId);
    for (    ApplicationSpecification appSpec : appSpecs) {
switch (programType) {
case FLOW:
        for (        String programName : appSpec.getFlows().keySet()) {
          Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
          if (programId != null) {
            targetProgramId=programId;
            break;
          }
        }
      break;
case MAPREDUCE:
    for (    String programName : appSpec.getMapReduce().keySet()) {
      Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
      if (programId != null) {
        targetProgramId=programId;
        break;
      }
    }
  break;
case SPARK:
for (String programName : appSpec.getSpark().keySet()) {
  Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
  if (programId != null) {
    targetProgramId=programId;
    break;
  }
}
break;
case SERVICE:
for (String programName : appSpec.getServices().keySet()) {
Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case WORKER:
for (String programName : appSpec.getWorkers().keySet()) {
Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case WORKFLOW:
for (String programName : appSpec.getWorkflows().keySet()) {
Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
default :
LOG.debug(""String_Node_Str"" + programType.name());
break;
}
if (targetProgramId != null) {
break;
}
}
if (targetProgramId != null) {
break;
}
}
return targetProgramId;
}","/** 
 * Helper method to get   {@link co.cask.cdap.proto.Id.Program} for a RunRecord for type of program
 * @param programType Type of program to search
 * @param runId The target id of the {@link RunRecord} to find
 * @return the program id of the run record or {@code null} if does not exist.
 */
@Nullable private Id.Program retrieveProgramIdForRunRecord(ProgramType programType,String runId){
  List<NamespaceMeta> namespaceMetas=store.listNamespaces();
  Id.Program targetProgramId=null;
  for (  NamespaceMeta nm : namespaceMetas) {
    Id.Namespace accId=Id.Namespace.from(nm.getName());
    Collection<ApplicationSpecification> appSpecs=store.getAllApplications(accId);
    for (    ApplicationSpecification appSpec : appSpecs) {
switch (programType) {
case FLOW:
        for (        String programName : appSpec.getFlows().keySet()) {
          Id.Program programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
          if (programId != null) {
            targetProgramId=programId;
            break;
          }
        }
      break;
case MAPREDUCE:
    for (    String programName : appSpec.getMapReduce().keySet()) {
      Id.Program programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
      if (programId != null) {
        targetProgramId=programId;
        break;
      }
    }
  break;
case SPARK:
for (String programName : appSpec.getSpark().keySet()) {
  Id.Program programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
  if (programId != null) {
    targetProgramId=programId;
    break;
  }
}
break;
case SERVICE:
for (String programName : appSpec.getServices().keySet()) {
Id.Program programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case WORKER:
for (String programName : appSpec.getWorkers().keySet()) {
Id.Program programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case WORKFLOW:
for (String programName : appSpec.getWorkflows().keySet()) {
Id.Program programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
default :
LOG.debug(""String_Node_Str"" + programType.name());
break;
}
if (targetProgramId != null) {
break;
}
}
if (targetProgramId != null) {
break;
}
}
return targetProgramId;
}"
6946,"/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService}.
 * @param programType The type of programs the run records nee to validate and update.
 */
private void validateAndCorrectRunningRunRecords(ProgramType programType){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecord> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    @Nullable RunRecord input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"",invalidRunRecords.size(),programType.getPrettyName());
  }
  for (  RunRecord rr : invalidRunRecords) {
    boolean shouldCorrect=shouldCorrectForWorkflowChildren(rr);
    if (!shouldCorrect) {
      continue;
    }
    String runId=rr.getPid();
    Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
    if (targetProgramId != null) {
      store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
    }
  }
}","/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @param programType The type of program the run records need to validate and update.
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 */
void validateAndCorrectRunningRunRecords(ProgramType programType,Set<String> processedInvalidRunRecordIds){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecord> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    @Nullable RunRecord input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"",invalidRunRecords.size(),programType.getPrettyName());
  }
  for (  RunRecord rr : invalidRunRecords) {
    boolean shouldCorrect=shouldCorrectForWorkflowChildren(rr,processedInvalidRunRecordIds);
    if (!shouldCorrect) {
      continue;
    }
    String runId=rr.getPid();
    Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
    if (targetProgramId != null) {
      store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
      processedInvalidRunRecordIds.add(runId);
    }
  }
}"
6947,"@POST @Path(""String_Node_Str"") public void suspendWorkflowRun(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowName,@PathParam(""String_Node_Str"") String runId){
  try {
    Id.Program id=Id.Program.from(namespaceId,appId,ProgramType.WORKFLOW,workflowName);
    ProgramRuntimeService.RuntimeInfo runtimeInfo=runtimeService.list(id).get(RunIds.fromString(runId));
    if (runtimeInfo == null) {
      sendInvalidResponse(responder,id);
      return;
    }
    ProgramController controller=runtimeInfo.getController();
    if (controller.getState() == ProgramController.State.SUSPENDED) {
      responder.sendString(AppFabricServiceStatus.PROGRAM_ALREADY_SUSPENDED.getCode(),AppFabricServiceStatus.PROGRAM_ALREADY_SUSPENDED.getMessage());
      return;
    }
    controller.suspend().get();
    responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","@POST @Path(""String_Node_Str"") public void suspendWorkflowRun(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowName,@PathParam(""String_Node_Str"") String runId) throws NotFoundException, ExecutionException, InterruptedException {
  Id.Program id=Id.Program.from(namespaceId,appId,ProgramType.WORKFLOW,workflowName);
  ProgramRuntimeService.RuntimeInfo runtimeInfo=runtimeService.list(id).get(RunIds.fromString(runId));
  if (runtimeInfo == null) {
    throw new NotFoundException(new Id.Run(id,runId));
  }
  ProgramController controller=runtimeInfo.getController();
  if (controller.getState() == ProgramController.State.SUSPENDED) {
    responder.sendString(AppFabricServiceStatus.PROGRAM_ALREADY_SUSPENDED.getCode(),AppFabricServiceStatus.PROGRAM_ALREADY_SUSPENDED.getMessage());
    return;
  }
  controller.suspend().get();
  responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
}"
6948,"@POST @Path(""String_Node_Str"") public void resumeWorkflowRun(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowName,@PathParam(""String_Node_Str"") String runId){
  try {
    Id.Program id=Id.Program.from(namespaceId,appId,ProgramType.WORKFLOW,workflowName);
    ProgramRuntimeService.RuntimeInfo runtimeInfo=runtimeService.list(id).get(RunIds.fromString(runId));
    if (runtimeInfo == null) {
      sendInvalidResponse(responder,id);
      return;
    }
    ProgramController controller=runtimeInfo.getController();
    if (controller.getState() == ProgramController.State.ALIVE) {
      responder.sendString(AppFabricServiceStatus.PROGRAM_ALREADY_RUNNING.getCode(),AppFabricServiceStatus.PROGRAM_ALREADY_RUNNING.getMessage());
      return;
    }
    controller.resume().get();
    responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","@POST @Path(""String_Node_Str"") public void resumeWorkflowRun(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowName,@PathParam(""String_Node_Str"") String runId) throws NotFoundException, ExecutionException, InterruptedException {
  Id.Program id=Id.Program.from(namespaceId,appId,ProgramType.WORKFLOW,workflowName);
  ProgramRuntimeService.RuntimeInfo runtimeInfo=runtimeService.list(id).get(RunIds.fromString(runId));
  if (runtimeInfo == null) {
    throw new NotFoundException(new Id.Run(id,runId));
  }
  ProgramController controller=runtimeInfo.getController();
  if (controller.getState() == ProgramController.State.ALIVE) {
    responder.sendString(AppFabricServiceStatus.PROGRAM_ALREADY_RUNNING.getCode(),AppFabricServiceStatus.PROGRAM_ALREADY_RUNNING.getMessage());
    return;
  }
  controller.resume().get();
  responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
}"
6949,"@Test public void testWorkflowPauseResume() throws Exception {
  String pauseResumeWorkflowApp=""String_Node_Str"";
  String pauseResumeWorkflow=""String_Node_Str"";
  File firstSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File firstSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File forkedSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File forkedSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File anotherForkedSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File anotherForkedSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File lastSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File lastSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  HttpResponse response=deploy(PauseResumeWorklowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,pauseResumeWorkflowApp,ProgramType.WORKFLOW,pauseResumeWorkflow);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  runtimeArguments.put(""String_Node_Str"",firstSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",firstSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",forkedSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",forkedSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",anotherForkedSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",anotherForkedSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",lastSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",lastSimpleActionDoneFile.getAbsolutePath());
  setAndTestRuntimeArgs(programId,runtimeArguments);
  startProgram(programId,200);
  waitState(programId,""String_Node_Str"");
  List<RunRecord> historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(historyRuns.size() == 1);
  String runId=historyRuns.get(0).getPid();
  while (!firstSimpleActionFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,1);
  suspendWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  verifyProgramRuns(programId,""String_Node_Str"");
  suspendWorkflow(programId,runId,409);
  firstSimpleActionDoneFile.createNewFile();
  verifyRunningProgramCount(programId,runId,0);
  verifyProgramRuns(programId,""String_Node_Str"");
  resumeWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  verifyProgramRuns(programId,""String_Node_Str"");
  resumeWorkflow(programId,runId,409);
  while (!(forkedSimpleActionFile.exists() && anotherForkedSimpleActionFile.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,2);
  suspendWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  verifyProgramRuns(programId,""String_Node_Str"");
  forkedSimpleActionDoneFile.createNewFile();
  anotherForkedSimpleActionDoneFile.createNewFile();
  verifyRunningProgramCount(programId,runId,0);
  verifyProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(!lastSimpleActionFile.exists());
  resumeWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  while (!lastSimpleActionFile.exists()) {
    TimeUnit.SECONDS.sleep(1);
  }
  verifyRunningProgramCount(programId,runId,1);
  lastSimpleActionDoneFile.createNewFile();
  verifyProgramRuns(programId,""String_Node_Str"");
  waitState(programId,""String_Node_Str"");
}","@Test public void testWorkflowPauseResume() throws Exception {
  String pauseResumeWorkflowApp=""String_Node_Str"";
  String pauseResumeWorkflow=""String_Node_Str"";
  File firstSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File firstSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File forkedSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File forkedSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File anotherForkedSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File anotherForkedSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File lastSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File lastSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  HttpResponse response=deploy(PauseResumeWorklowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,pauseResumeWorkflowApp,ProgramType.WORKFLOW,pauseResumeWorkflow);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  runtimeArguments.put(""String_Node_Str"",firstSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",firstSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",forkedSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",forkedSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",anotherForkedSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",anotherForkedSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",lastSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",lastSimpleActionDoneFile.getAbsolutePath());
  setAndTestRuntimeArgs(programId,runtimeArguments);
  startProgram(programId,200);
  waitState(programId,""String_Node_Str"");
  List<RunRecord> historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(historyRuns.size() == 1);
  String runId=historyRuns.get(0).getPid();
  while (!firstSimpleActionFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,1);
  suspendWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  verifyProgramRuns(programId,""String_Node_Str"");
  suspendWorkflow(programId,runId,409);
  firstSimpleActionDoneFile.createNewFile();
  verifyRunningProgramCount(programId,runId,0);
  verifyProgramRuns(programId,""String_Node_Str"");
  resumeWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  verifyProgramRuns(programId,""String_Node_Str"");
  resumeWorkflow(programId,runId,409);
  while (!(forkedSimpleActionFile.exists() && anotherForkedSimpleActionFile.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,2);
  suspendWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  verifyProgramRuns(programId,""String_Node_Str"");
  forkedSimpleActionDoneFile.createNewFile();
  anotherForkedSimpleActionDoneFile.createNewFile();
  verifyRunningProgramCount(programId,runId,0);
  verifyProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(!lastSimpleActionFile.exists());
  resumeWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  while (!lastSimpleActionFile.exists()) {
    TimeUnit.SECONDS.sleep(1);
  }
  verifyRunningProgramCount(programId,runId,1);
  lastSimpleActionDoneFile.createNewFile();
  verifyProgramRuns(programId,""String_Node_Str"");
  waitState(programId,""String_Node_Str"");
  suspendWorkflow(programId,runId,404);
  resumeWorkflow(programId,runId,404);
}"
6950,"/** 
 * Helper method to get program id for a run record if it exists in the store.
 * @param store
 * @param namespaceName
 * @param appName
 * @param programType
 * @param programName
 * @param runId
 * @return instance of {@link Id.Program} if exist for the runId or null if does not.
 */
@Nullable private static Id.Program validateProgramForRunRecord(Store store,String namespaceName,String appName,ProgramType programType,String programName,String runId){
  Id.Program programId=Id.Program.from(namespaceName,appName,programType,programName);
  RunRecord runRecord=store.getRun(programId,runId);
  if (runRecord != null) {
    return programId;
  }
 else {
    return null;
  }
}","/** 
 * Helper method to get program id for a run record if it exists in the store.
 * @return instance of {@link Id.Program} if exist for the runId or null if does not.
 */
@Nullable private Id.Program validateProgramForRunRecord(String namespaceName,String appName,ProgramType programType,String programName,String runId){
  Id.Program programId=Id.Program.from(namespaceName,appName,programType,programName);
  RunRecord runRecord=store.getRun(programId,runId);
  if (runRecord != null) {
    return programId;
  }
 else {
    return null;
  }
}"
6951,"@Override public void run(){
  for (  ProgramType programType : ProgramType.values()) {
    programLifecycleService.validateAndCorrectRunningRunRecords(programType);
  }
}","@Override public void run(){
  try {
    RunRecordsCorrectorRunnable.LOG.debug(""String_Node_Str"");
    programLifecycleService.validateAndCorrectRunningRunRecords();
    RunRecordsCorrectorRunnable.LOG.debug(""String_Node_Str"");
  }
 catch (  Throwable t) {
  }
}"
6952,"/** 
 * Helper method to check if the run record is a child program of a Workflow
 * @param runRecord The target {@link RunRecord} to check
 * @return {@code true} of we should check and {@code false} otherwise
 */
private boolean shouldCorrectForWorkflowChildren(RunRecord runRecord){
  if (runRecord.getProperties() != null && runRecord.getProperties().get(""String_Node_Str"") != null) {
    String workflowRunId=runRecord.getProperties().get(""String_Node_Str"");
    Id.Program workflowProgramId=retrieveProgramIdForRunRecord(ProgramType.WORKFLOW,workflowRunId);
    if (workflowProgramId != null) {
      RunRecord wfRunRecord=store.getRun(workflowProgramId,workflowRunId);
      RuntimeInfo wfRuntimeInfo=runtimeService.lookup(workflowProgramId,RunIds.fromString(workflowRunId));
      if (wfRunRecord != null && wfRunRecord.getStatus() == ProgramRunStatus.RUNNING && wfRuntimeInfo != null) {
        return false;
      }
    }
  }
  return true;
}","/** 
 * Helper method to check if the run record is a child program of a Workflow
 * @param runRecord The target {@link RunRecord} to check
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 * @return {@code true} of we should check and {@code false} otherwise
 */
private boolean shouldCorrectForWorkflowChildren(RunRecord runRecord,Set<String> processedInvalidRunRecordIds){
  if (runRecord.getProperties() != null && runRecord.getProperties().get(""String_Node_Str"") != null) {
    String workflowRunId=runRecord.getProperties().get(""String_Node_Str"");
    if (!processedInvalidRunRecordIds.contains(workflowRunId)) {
      Id.Program workflowProgramId=retrieveProgramIdForRunRecord(ProgramType.WORKFLOW,workflowRunId);
      if (workflowProgramId != null) {
        RunRecord wfRunRecord=store.getRun(workflowProgramId,workflowRunId);
        RuntimeInfo wfRuntimeInfo=runtimeService.lookup(workflowProgramId,RunIds.fromString(workflowRunId));
        if (wfRunRecord != null && wfRunRecord.getStatus() == ProgramRunStatus.RUNNING && wfRuntimeInfo != null) {
          return false;
        }
      }
    }
  }
  return true;
}"
6953,"/** 
 * Helper method to get   {@link co.cask.cdap.proto.Id.Program} for a RunRecord for type of program
 * @param programType Type of program to search
 * @param runId The target id of the {@link RunRecord} to find
 * @return the program id of the run record or {@code null} if does not exist.
 */
@Nullable private Id.Program retrieveProgramIdForRunRecord(ProgramType programType,String runId){
  List<NamespaceMeta> namespaceMetas=store.listNamespaces();
  Id.Program targetProgramId=null;
  for (  NamespaceMeta nm : namespaceMetas) {
    Id.Namespace accId=Id.Namespace.from(nm.getName());
    Collection<ApplicationSpecification> appSpecs=store.getAllApplications(accId);
    for (    ApplicationSpecification appSpec : appSpecs) {
switch (programType) {
case FLOW:
        for (        String programName : appSpec.getFlows().keySet()) {
          Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
          if (programId != null) {
            targetProgramId=programId;
            break;
          }
        }
      break;
case MAPREDUCE:
    for (    String programName : appSpec.getMapReduce().keySet()) {
      Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
      if (programId != null) {
        targetProgramId=programId;
        break;
      }
    }
  break;
case SPARK:
for (String programName : appSpec.getSpark().keySet()) {
  Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
  if (programId != null) {
    targetProgramId=programId;
    break;
  }
}
break;
case SERVICE:
for (String programName : appSpec.getServices().keySet()) {
Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case WORKER:
for (String programName : appSpec.getWorkers().keySet()) {
Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case WORKFLOW:
for (String programName : appSpec.getWorkflows().keySet()) {
Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
default :
LOG.debug(""String_Node_Str"" + programType.name());
break;
}
if (targetProgramId != null) {
break;
}
}
if (targetProgramId != null) {
break;
}
}
return targetProgramId;
}","/** 
 * Helper method to get   {@link co.cask.cdap.proto.Id.Program} for a RunRecord for type of program
 * @param programType Type of program to search
 * @param runId The target id of the {@link RunRecord} to find
 * @return the program id of the run record or {@code null} if does not exist.
 */
@Nullable private Id.Program retrieveProgramIdForRunRecord(ProgramType programType,String runId){
  List<NamespaceMeta> namespaceMetas=store.listNamespaces();
  Id.Program targetProgramId=null;
  for (  NamespaceMeta nm : namespaceMetas) {
    Id.Namespace accId=Id.Namespace.from(nm.getName());
    Collection<ApplicationSpecification> appSpecs=store.getAllApplications(accId);
    for (    ApplicationSpecification appSpec : appSpecs) {
switch (programType) {
case FLOW:
        for (        String programName : appSpec.getFlows().keySet()) {
          Id.Program programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
          if (programId != null) {
            targetProgramId=programId;
            break;
          }
        }
      break;
case MAPREDUCE:
    for (    String programName : appSpec.getMapReduce().keySet()) {
      Id.Program programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
      if (programId != null) {
        targetProgramId=programId;
        break;
      }
    }
  break;
case SPARK:
for (String programName : appSpec.getSpark().keySet()) {
  Id.Program programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
  if (programId != null) {
    targetProgramId=programId;
    break;
  }
}
break;
case SERVICE:
for (String programName : appSpec.getServices().keySet()) {
Id.Program programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case WORKER:
for (String programName : appSpec.getWorkers().keySet()) {
Id.Program programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case WORKFLOW:
for (String programName : appSpec.getWorkflows().keySet()) {
Id.Program programId=validateProgramForRunRecord(nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
default :
LOG.debug(""String_Node_Str"" + programType.name());
break;
}
if (targetProgramId != null) {
break;
}
}
if (targetProgramId != null) {
break;
}
}
return targetProgramId;
}"
6954,"/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService}.
 * @param programType The type of programs the run records nee to validate and update.
 */
void validateAndCorrectRunningRunRecords(ProgramType programType){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecord> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    @Nullable RunRecord input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"",invalidRunRecords.size());
  }
  for (  RunRecord rr : invalidRunRecords) {
    boolean shouldCorrect=shouldCorrectForWorkflowChildren(rr);
    if (!shouldCorrect) {
      continue;
    }
    String runId=rr.getPid();
    Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
    if (targetProgramId != null) {
      LOG.warn(""String_Node_Str"",runId,targetProgramId,programType.getPrettyName());
      store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
    }
  }
}","/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService} for a type of CDAP program.
 * @param programType The type of program the run records need to validate and update.
 * @param processedInvalidRunRecordIds the {@link Set} of processed invalid run record ids.
 */
void validateAndCorrectRunningRunRecords(ProgramType programType,Set<String> processedInvalidRunRecordIds){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecord> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    @Nullable RunRecord input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"",invalidRunRecords.size());
  }
  for (  RunRecord rr : invalidRunRecords) {
    boolean shouldCorrect=shouldCorrectForWorkflowChildren(rr,processedInvalidRunRecordIds);
    if (!shouldCorrect) {
      continue;
    }
    String runId=rr.getPid();
    Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
    if (targetProgramId != null) {
      LOG.warn(""String_Node_Str"",runId,targetProgramId,programType.getPrettyName());
      store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
      processedInvalidRunRecordIds.add(runId);
    }
  }
}"
6955,"@Test public void testInvalidFlowRunRecord() throws Exception {
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program wordcountFlow1=Id.Program.from(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW,""String_Node_Str"");
  Assert.assertEquals(""String_Node_Str"",getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow1);
  waitState(wordcountFlow1,ProgramRunStatus.RUNNING.toString());
  List<RunRecord> runRecords=getProgramRuns(wordcountFlow1,ProgramRunStatus.RUNNING.toString());
  Assert.assertEquals(1,runRecords.size());
  RunRecord rr=runRecords.get(0);
  Assert.assertEquals(ProgramRunStatus.RUNNING,rr.getStatus());
  RuntimeInfo runtimeInfo=runtimeService.lookup(wordcountFlow1,RunIds.fromString(rr.getPid()));
  ProgramController programController=runtimeInfo.getController();
  programController.stop();
  Thread.sleep(2000);
  rr=store.getRun(wordcountFlow1,rr.getPid());
  Assert.assertEquals(ProgramRunStatus.KILLED,rr.getStatus());
  long now=System.currentTimeMillis();
  long nowSecs=TimeUnit.MILLISECONDS.toSeconds(now);
  store.setStart(wordcountFlow1,rr.getPid(),nowSecs);
  rr=store.getRun(wordcountFlow1,rr.getPid());
  Assert.assertEquals(ProgramRunStatus.RUNNING,rr.getStatus());
  runRecords=getProgramRuns(wordcountFlow1,ProgramRunStatus.FAILED.toString());
  Assert.assertEquals(0,runRecords.size());
  programLifecycleService.validateAndCorrectRunningRunRecords(ProgramType.FLOW);
  runRecords=getProgramRuns(wordcountFlow1,ProgramRunStatus.FAILED.toString());
  Assert.assertEquals(1,runRecords.size());
  rr=runRecords.get(0);
  Assert.assertEquals(ProgramRunStatus.FAILED,rr.getStatus());
}","@Test public void testInvalidFlowRunRecord() throws Exception {
  HttpResponse response=deploy(WordCountApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE1);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program wordcountFlow1=Id.Program.from(TEST_NAMESPACE1,""String_Node_Str"",ProgramType.FLOW,""String_Node_Str"");
  Assert.assertEquals(""String_Node_Str"",getProgramStatus(wordcountFlow1));
  startProgram(wordcountFlow1);
  waitState(wordcountFlow1,ProgramRunStatus.RUNNING.toString());
  List<RunRecord> runRecords=getProgramRuns(wordcountFlow1,ProgramRunStatus.RUNNING.toString());
  Assert.assertEquals(1,runRecords.size());
  RunRecord rr=runRecords.get(0);
  Assert.assertEquals(ProgramRunStatus.RUNNING,rr.getStatus());
  RuntimeInfo runtimeInfo=runtimeService.lookup(wordcountFlow1,RunIds.fromString(rr.getPid()));
  ProgramController programController=runtimeInfo.getController();
  programController.stop();
  Thread.sleep(2000);
  rr=store.getRun(wordcountFlow1,rr.getPid());
  Assert.assertEquals(ProgramRunStatus.KILLED,rr.getStatus());
  long now=System.currentTimeMillis();
  long nowSecs=TimeUnit.MILLISECONDS.toSeconds(now);
  store.setStart(wordcountFlow1,rr.getPid(),nowSecs);
  rr=store.getRun(wordcountFlow1,rr.getPid());
  Assert.assertEquals(ProgramRunStatus.RUNNING,rr.getStatus());
  runRecords=getProgramRuns(wordcountFlow1,ProgramRunStatus.FAILED.toString());
  Assert.assertEquals(0,runRecords.size());
  Set<String> processedInvalidRunRecordIds=Sets.newHashSet();
  programLifecycleService.validateAndCorrectRunningRunRecords(ProgramType.FLOW,processedInvalidRunRecordIds);
  runRecords=getProgramRuns(wordcountFlow1,ProgramRunStatus.FAILED.toString());
  Assert.assertEquals(1,runRecords.size());
  rr=runRecords.get(0);
  Assert.assertEquals(ProgramRunStatus.FAILED,rr.getStatus());
}"
6956,"@POST @Path(""String_Node_Str"") public void suspendWorkflowRun(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowName,@PathParam(""String_Node_Str"") String runId){
  try {
    Id.Program id=Id.Program.from(namespaceId,appId,ProgramType.WORKFLOW,workflowName);
    ProgramRuntimeService.RuntimeInfo runtimeInfo=runtimeService.list(id).get(RunIds.fromString(runId));
    if (runtimeInfo == null) {
      sendInvalidResponse(responder,id);
      return;
    }
    ProgramController controller=runtimeInfo.getController();
    if (controller.getState() == ProgramController.State.SUSPENDED) {
      responder.sendString(AppFabricServiceStatus.PROGRAM_ALREADY_SUSPENDED.getCode(),AppFabricServiceStatus.PROGRAM_ALREADY_SUSPENDED.getMessage());
      return;
    }
    controller.suspend().get();
    responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","@POST @Path(""String_Node_Str"") public void suspendWorkflowRun(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowName,@PathParam(""String_Node_Str"") String runId){
  try {
    Id.Program id=Id.Program.from(namespaceId,appId,ProgramType.WORKFLOW,workflowName);
    ProgramRuntimeService.RuntimeInfo runtimeInfo=runtimeService.list(id).get(RunIds.fromString(runId));
    if (runtimeInfo == null) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",runId));
      return;
    }
    ProgramController controller=runtimeInfo.getController();
    if (controller.getState() == ProgramController.State.SUSPENDED) {
      responder.sendString(AppFabricServiceStatus.PROGRAM_ALREADY_SUSPENDED.getCode(),AppFabricServiceStatus.PROGRAM_ALREADY_SUSPENDED.getMessage());
      return;
    }
    controller.suspend().get();
    responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}"
6957,"@POST @Path(""String_Node_Str"") public void resumeWorkflowRun(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowName,@PathParam(""String_Node_Str"") String runId){
  try {
    Id.Program id=Id.Program.from(namespaceId,appId,ProgramType.WORKFLOW,workflowName);
    ProgramRuntimeService.RuntimeInfo runtimeInfo=runtimeService.list(id).get(RunIds.fromString(runId));
    if (runtimeInfo == null) {
      sendInvalidResponse(responder,id);
      return;
    }
    ProgramController controller=runtimeInfo.getController();
    if (controller.getState() == ProgramController.State.ALIVE) {
      responder.sendString(AppFabricServiceStatus.PROGRAM_ALREADY_RUNNING.getCode(),AppFabricServiceStatus.PROGRAM_ALREADY_RUNNING.getMessage());
      return;
    }
    controller.resume().get();
    responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}","@POST @Path(""String_Node_Str"") public void resumeWorkflowRun(HttpRequest request,final HttpResponder responder,@PathParam(""String_Node_Str"") String namespaceId,@PathParam(""String_Node_Str"") String appId,@PathParam(""String_Node_Str"") String workflowName,@PathParam(""String_Node_Str"") String runId){
  try {
    Id.Program id=Id.Program.from(namespaceId,appId,ProgramType.WORKFLOW,workflowName);
    ProgramRuntimeService.RuntimeInfo runtimeInfo=runtimeService.list(id).get(RunIds.fromString(runId));
    if (runtimeInfo == null) {
      responder.sendString(HttpResponseStatus.NOT_FOUND,String.format(""String_Node_Str"",runId));
      return;
    }
    ProgramController controller=runtimeInfo.getController();
    if (controller.getState() == ProgramController.State.ALIVE) {
      responder.sendString(AppFabricServiceStatus.PROGRAM_ALREADY_RUNNING.getCode(),AppFabricServiceStatus.PROGRAM_ALREADY_RUNNING.getMessage());
      return;
    }
    controller.resume().get();
    responder.sendString(HttpResponseStatus.OK,""String_Node_Str"");
  }
 catch (  SecurityException e) {
    responder.sendStatus(HttpResponseStatus.UNAUTHORIZED);
  }
catch (  Throwable e) {
    LOG.error(""String_Node_Str"",e);
    responder.sendStatus(HttpResponseStatus.INTERNAL_SERVER_ERROR);
  }
}"
6958,"@Test public void testWorkflowPauseResume() throws Exception {
  String pauseResumeWorkflowApp=""String_Node_Str"";
  String pauseResumeWorkflow=""String_Node_Str"";
  File firstSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File firstSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File forkedSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File forkedSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File anotherForkedSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File anotherForkedSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File lastSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File lastSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  HttpResponse response=deploy(PauseResumeWorklowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,pauseResumeWorkflowApp,ProgramType.WORKFLOW,pauseResumeWorkflow);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  runtimeArguments.put(""String_Node_Str"",firstSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",firstSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",forkedSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",forkedSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",anotherForkedSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",anotherForkedSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",lastSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",lastSimpleActionDoneFile.getAbsolutePath());
  setAndTestRuntimeArgs(programId,runtimeArguments);
  startProgram(programId,200);
  waitState(programId,""String_Node_Str"");
  List<RunRecord> historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(historyRuns.size() == 1);
  String runId=historyRuns.get(0).getPid();
  while (!firstSimpleActionFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,1);
  suspendWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  verifyProgramRuns(programId,""String_Node_Str"");
  suspendWorkflow(programId,runId,409);
  firstSimpleActionDoneFile.createNewFile();
  verifyRunningProgramCount(programId,runId,0);
  verifyProgramRuns(programId,""String_Node_Str"");
  resumeWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  verifyProgramRuns(programId,""String_Node_Str"");
  resumeWorkflow(programId,runId,409);
  while (!(forkedSimpleActionFile.exists() && anotherForkedSimpleActionFile.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,2);
  suspendWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  verifyProgramRuns(programId,""String_Node_Str"");
  forkedSimpleActionDoneFile.createNewFile();
  anotherForkedSimpleActionDoneFile.createNewFile();
  verifyRunningProgramCount(programId,runId,0);
  verifyProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(!lastSimpleActionFile.exists());
  resumeWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  while (!lastSimpleActionFile.exists()) {
    TimeUnit.SECONDS.sleep(1);
  }
  verifyRunningProgramCount(programId,runId,1);
  lastSimpleActionDoneFile.createNewFile();
  verifyProgramRuns(programId,""String_Node_Str"");
  waitState(programId,""String_Node_Str"");
}","@Test public void testWorkflowPauseResume() throws Exception {
  String pauseResumeWorkflowApp=""String_Node_Str"";
  String pauseResumeWorkflow=""String_Node_Str"";
  File firstSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File firstSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File forkedSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File forkedSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File anotherForkedSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File anotherForkedSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File lastSimpleActionFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  File lastSimpleActionDoneFile=new File(tmpFolder.newFolder() + ""String_Node_Str"");
  HttpResponse response=deploy(PauseResumeWorklowApp.class,Constants.Gateway.API_VERSION_3_TOKEN,TEST_NAMESPACE2);
  Assert.assertEquals(200,response.getStatusLine().getStatusCode());
  Id.Program programId=Id.Program.from(TEST_NAMESPACE2,pauseResumeWorkflowApp,ProgramType.WORKFLOW,pauseResumeWorkflow);
  Map<String,String> runtimeArguments=Maps.newHashMap();
  runtimeArguments.put(""String_Node_Str"",firstSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",firstSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",forkedSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",forkedSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",anotherForkedSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",anotherForkedSimpleActionDoneFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",lastSimpleActionFile.getAbsolutePath());
  runtimeArguments.put(""String_Node_Str"",lastSimpleActionDoneFile.getAbsolutePath());
  setAndTestRuntimeArgs(programId,runtimeArguments);
  startProgram(programId,200);
  waitState(programId,""String_Node_Str"");
  List<RunRecord> historyRuns=getProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(historyRuns.size() == 1);
  String runId=historyRuns.get(0).getPid();
  while (!firstSimpleActionFile.exists()) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,1);
  suspendWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  verifyProgramRuns(programId,""String_Node_Str"");
  suspendWorkflow(programId,runId,409);
  firstSimpleActionDoneFile.createNewFile();
  verifyRunningProgramCount(programId,runId,0);
  verifyProgramRuns(programId,""String_Node_Str"");
  resumeWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  verifyProgramRuns(programId,""String_Node_Str"");
  resumeWorkflow(programId,runId,409);
  while (!(forkedSimpleActionFile.exists() && anotherForkedSimpleActionFile.exists())) {
    TimeUnit.MILLISECONDS.sleep(50);
  }
  verifyRunningProgramCount(programId,runId,2);
  suspendWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  verifyProgramRuns(programId,""String_Node_Str"");
  forkedSimpleActionDoneFile.createNewFile();
  anotherForkedSimpleActionDoneFile.createNewFile();
  verifyRunningProgramCount(programId,runId,0);
  verifyProgramRuns(programId,""String_Node_Str"");
  Assert.assertTrue(!lastSimpleActionFile.exists());
  resumeWorkflow(programId,runId,200);
  waitState(programId,""String_Node_Str"");
  while (!lastSimpleActionFile.exists()) {
    TimeUnit.SECONDS.sleep(1);
  }
  verifyRunningProgramCount(programId,runId,1);
  lastSimpleActionDoneFile.createNewFile();
  verifyProgramRuns(programId,""String_Node_Str"");
  waitState(programId,""String_Node_Str"");
  suspendWorkflow(programId,runId,404);
  resumeWorkflow(programId,runId,404);
}"
6959,"/** 
 * Will be called by external source to start poll the Kafka messages one at the time.
 * @param emitter instance of {@link Emitter} to emit the messages.
 */
public void pollMessages(Emitter<StructuredRecord> emitter){
  boolean infosUpdated=false;
  for (  KafkaConsumerInfo<OFFSET> info : consumerInfos.values()) {
    Iterator<KafkaMessage<OFFSET>> iterator=readMessages(info);
    while (iterator.hasNext()) {
      KafkaMessage<OFFSET> message=iterator.next();
      processMessage(message,emitter);
      info.setReadOffset(message.getNextOffset());
    }
    if (info.hasPendingChanges()) {
      infosUpdated=true;
    }
  }
  if (infosUpdated) {
    saveReadOffsets(Maps.transformValues(consumerInfos,consumerToOffset));
  }
}","/** 
 * Will be called by external source to start poll the Kafka messages one at the time.
 * @param emitter instance of {@link Emitter} to emit the messages.
 */
public void pollMessages(Emitter<StructuredRecord> emitter){
  if (consumerInfos == null) {
    consumerInfos=createConsumerInfos(kafkaConfigurer.getTopicPartitions());
  }
  boolean infosUpdated=false;
  for (  KafkaConsumerInfo<OFFSET> info : consumerInfos.values()) {
    Iterator<KafkaMessage<OFFSET>> iterator=readMessages(info);
    while (iterator.hasNext()) {
      KafkaMessage<OFFSET> message=iterator.next();
      processMessage(message,emitter);
      info.setReadOffset(message.getNextOffset());
    }
    if (info.hasPendingChanges()) {
      infosUpdated=true;
    }
  }
  if (infosUpdated) {
    saveReadOffsets(Maps.transformValues(consumerInfos,consumerToOffset));
  }
}"
6960,"/** 
 * <p> The method should be called to initialized the consumer when being used by the   {@code RealtimeSource}</p>
 * @param context
 * @throws Exception
 */
public void initialize(RealtimeContext context) throws Exception {
  sourceContext=context;
  Type superType=TypeToken.of(getClass()).getSupertype(KafkaSimpleApiConsumer.class).getType();
  if (superType instanceof ParameterizedType) {
    Type[] typeArgs=((ParameterizedType)superType).getActualTypeArguments();
    keyDecoder=createKeyDecoder(typeArgs[0]);
    payloadDecoder=createPayloadDecoder(typeArgs[1]);
  }
  DefaultKafkaConfigurer kafkaConfigurer=new DefaultKafkaConfigurer();
  configureKafka(kafkaConfigurer);
  if (kafkaConfigurer.getZookeeper() == null && kafkaConfigurer.getBrokers() == null) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  kafkaConfig=new KafkaConfig(kafkaConfigurer.getZookeeper(),kafkaConfigurer.getBrokers());
  consumerInfos=createConsumerInfos(kafkaConfigurer.getTopicPartitions());
}","/** 
 * <p> The method should be called to initialized the consumer when being used by the   {@code RealtimeSource}</p>
 * @param context
 * @throws Exception
 */
public void initialize(RealtimeContext context) throws Exception {
  sourceContext=context;
  Type superType=TypeToken.of(getClass()).getSupertype(KafkaSimpleApiConsumer.class).getType();
  if (superType instanceof ParameterizedType) {
    Type[] typeArgs=((ParameterizedType)superType).getActualTypeArguments();
    keyDecoder=createKeyDecoder(typeArgs[0]);
    payloadDecoder=createPayloadDecoder(typeArgs[1]);
  }
  kafkaConfigurer=new DefaultKafkaConfigurer();
  configureKafka(kafkaConfigurer);
  if (kafkaConfigurer.getZookeeper() == null && kafkaConfigurer.getBrokers() == null) {
    throw new IllegalStateException(""String_Node_Str"");
  }
  kafkaConfig=new KafkaConfig(kafkaConfigurer.getZookeeper(),kafkaConfigurer.getBrokers());
}"
6961,"@Test public void testKafkaConsumerSimple() throws Exception {
  final String topic=""String_Node_Str"";
  initializeKafkaSource(topic,PARTITIONS,false);
  int msgCount=5;
  Map<String,String> messages=Maps.newHashMap();
  for (int i=0; i < msgCount; i++) {
    messages.put(Integer.toString(i),""String_Node_Str"" + i);
  }
  sendMessage(topic,messages);
  TimeUnit.SECONDS.sleep(2);
  verifyEmittedMessages(kafkaSource,msgCount);
}","@Test public void testKafkaConsumerSimple() throws Exception {
  final String topic=""String_Node_Str"";
  initializeKafkaSource(topic,PARTITIONS,false);
  int msgCount=5;
  Map<String,String> messages=Maps.newHashMap();
  for (int i=0; i < msgCount; i++) {
    messages.put(Integer.toString(i),""String_Node_Str"" + i);
  }
  sendMessage(topic,messages);
  TimeUnit.SECONDS.sleep(2);
  verifyEmittedMessages(kafkaSource,msgCount,new SourceState());
}"
6962,"private void verifyEmittedMessages(KafkaSource source,int msgCount){
  MockEmitter emitter=new MockEmitter();
  SourceState sourceState=source.poll(emitter,new SourceState());
  System.out.println(""String_Node_Str"" + msgCount);
  System.out.println(""String_Node_Str"" + emitter.getInternalSize());
  Assert.assertTrue(sourceState.getState() != null && !sourceState.getState().isEmpty());
  Assert.assertTrue(emitter.getInternalSize() == msgCount);
}","private void verifyEmittedMessages(KafkaSource source,int msgCount,SourceState sourceState){
  MockEmitter emitter=new MockEmitter();
  SourceState updatedSourceState=source.poll(emitter,sourceState);
  System.out.println(""String_Node_Str"" + msgCount);
  System.out.println(""String_Node_Str"" + emitter.getInternalSize());
  Assert.assertTrue(updatedSourceState.getState() != null && !updatedSourceState.getState().isEmpty());
  Assert.assertTrue(emitter.getInternalSize() == msgCount);
}"
6963,"/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService}.
 * @param programType The type of programs the run records nee to validate and update.
 */
private void validateAndCorrectRunningRunRecords(ProgramType programType){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecord> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    @Nullable RunRecord input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"",invalidRunRecords.size(),programType.getPrettyName());
  }
  for (  RunRecord rr : invalidRunRecords) {
    boolean shouldCorrect=shouldCorrectForWorkflowChildren(rr);
    if (!shouldCorrect) {
      continue;
    }
    String runId=rr.getPid();
    Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
    if (targetProgramId != null) {
      store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
    }
  }
}","/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService}.
 * @param programType The type of programs the run records nee to validate and update.
 */
void validateAndCorrectRunningRunRecords(ProgramType programType){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecord> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    @Nullable RunRecord input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"",invalidRunRecords.size());
  }
  for (  RunRecord rr : invalidRunRecords) {
    boolean shouldCorrect=shouldCorrectForWorkflowChildren(rr);
    if (!shouldCorrect) {
      continue;
    }
    String runId=rr.getPid();
    Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
    if (targetProgramId != null) {
      LOG.warn(""String_Node_Str"",runId,targetProgramId,programType.getPrettyName());
      store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
    }
  }
}"
6964,"private StructuredRecord convertTweet(Status tweet){
  recordBuilder.set(ID,tweet.getId());
  recordBuilder.set(MSG,tweet.getText());
  recordBuilder.set(LANG,tweet.getLang());
  recordBuilder.set(TIME,convertDataToTimeStamp(tweet.getCreatedAt()));
  recordBuilder.set(FAVC,tweet.getFavoriteCount());
  recordBuilder.set(RTC,tweet.getRetweetCount());
  recordBuilder.set(SRC,tweet.getSource());
  if (tweet.getGeoLocation() != null) {
    recordBuilder.set(GLAT,tweet.getGeoLocation().getLatitude());
    recordBuilder.set(GLNG,tweet.getGeoLocation().getLongitude());
  }
 else {
    recordBuilder.set(GLAT,-1d);
    recordBuilder.set(GLNG,-1d);
  }
  recordBuilder.set(ISRT,tweet.isRetweet());
  return recordBuilder.build();
}","private StructuredRecord convertTweet(Status tweet){
  StructuredRecord.Builder recordBuilder=StructuredRecord.builder(this.schema);
  recordBuilder.set(ID,tweet.getId());
  recordBuilder.set(MSG,tweet.getText());
  recordBuilder.set(LANG,tweet.getLang());
  recordBuilder.set(TIME,convertDataToTimeStamp(tweet.getCreatedAt()));
  recordBuilder.set(FAVC,tweet.getFavoriteCount());
  recordBuilder.set(RTC,tweet.getRetweetCount());
  recordBuilder.set(SRC,tweet.getSource());
  if (tweet.getGeoLocation() != null) {
    recordBuilder.set(GLAT,tweet.getGeoLocation().getLatitude());
    recordBuilder.set(GLNG,tweet.getGeoLocation().getLongitude());
  }
 else {
    recordBuilder.set(GLAT,-1d);
    recordBuilder.set(GLNG,-1d);
  }
  recordBuilder.set(ISRT,tweet.isRetweet());
  return recordBuilder.build();
}"
6965,"@Override public void initialize(RealtimeContext context) throws Exception {
  super.initialize(context);
  System.setProperty(""String_Node_Str"",""String_Node_Str"");
  Schema.Field idField=Schema.Field.of(ID,Schema.of(Schema.Type.LONG));
  Schema.Field msgField=Schema.Field.of(MSG,Schema.of(Schema.Type.STRING));
  Schema.Field langField=Schema.Field.of(LANG,Schema.of(Schema.Type.STRING));
  Schema.Field timeField=Schema.Field.of(TIME,Schema.of(Schema.Type.LONG));
  Schema.Field favCount=Schema.Field.of(FAVC,Schema.of(Schema.Type.INT));
  Schema.Field rtCount=Schema.Field.of(RTC,Schema.of(Schema.Type.INT));
  Schema.Field sourceField=Schema.Field.of(SRC,Schema.of(Schema.Type.STRING));
  Schema.Field geoLatField=Schema.Field.of(GLAT,Schema.of(Schema.Type.DOUBLE));
  Schema.Field geoLongField=Schema.Field.of(GLNG,Schema.of(Schema.Type.DOUBLE));
  Schema.Field reTweetField=Schema.Field.of(ISRT,Schema.of(Schema.Type.BOOLEAN));
  recordBuilder=StructuredRecord.builder(Schema.recordOf(""String_Node_Str"",idField,msgField,langField,timeField,favCount,rtCount,sourceField,geoLatField,geoLongField,reTweetField));
  statusListener=new StatusListener(){
    @Override public void onStatus(    Status status){
      tweetQ.add(status);
    }
    @Override public void onDeletionNotice(    StatusDeletionNotice statusDeletionNotice){
    }
    @Override public void onTrackLimitationNotice(    int i){
    }
    @Override public void onScrubGeo(    long l,    long l1){
    }
    @Override public void onStallWarning(    StallWarning stallWarning){
    }
    @Override public void onException(    Exception e){
    }
  }
;
  ConfigurationBuilder configurationBuilder=new ConfigurationBuilder();
  configurationBuilder.setDebugEnabled(false).setOAuthConsumerKey(twitterConfig.consumerKey).setOAuthConsumerSecret(twitterConfig.consumeSecret).setOAuthAccessToken(twitterConfig.accessToken).setOAuthAccessTokenSecret(twitterConfig.accessTokenSecret);
  twitterStream=new TwitterStreamFactory(configurationBuilder.build()).getInstance();
  twitterStream.addListener(statusListener);
  twitterStream.sample();
}","@Override public void initialize(RealtimeContext context) throws Exception {
  super.initialize(context);
  System.setProperty(""String_Node_Str"",""String_Node_Str"");
  Schema.Field idField=Schema.Field.of(ID,Schema.of(Schema.Type.LONG));
  Schema.Field msgField=Schema.Field.of(MSG,Schema.of(Schema.Type.STRING));
  Schema.Field langField=Schema.Field.of(LANG,Schema.of(Schema.Type.STRING));
  Schema.Field timeField=Schema.Field.of(TIME,Schema.of(Schema.Type.LONG));
  Schema.Field favCount=Schema.Field.of(FAVC,Schema.of(Schema.Type.INT));
  Schema.Field rtCount=Schema.Field.of(RTC,Schema.of(Schema.Type.INT));
  Schema.Field sourceField=Schema.Field.of(SRC,Schema.of(Schema.Type.STRING));
  Schema.Field geoLatField=Schema.Field.of(GLAT,Schema.of(Schema.Type.DOUBLE));
  Schema.Field geoLongField=Schema.Field.of(GLNG,Schema.of(Schema.Type.DOUBLE));
  Schema.Field reTweetField=Schema.Field.of(ISRT,Schema.of(Schema.Type.BOOLEAN));
  schema=Schema.recordOf(""String_Node_Str"",idField,msgField,langField,timeField,favCount,rtCount,sourceField,geoLatField,geoLongField,reTweetField);
  statusListener=new StatusListener(){
    @Override public void onStatus(    Status status){
      tweetQ.add(status);
    }
    @Override public void onDeletionNotice(    StatusDeletionNotice statusDeletionNotice){
    }
    @Override public void onTrackLimitationNotice(    int i){
    }
    @Override public void onScrubGeo(    long l,    long l1){
    }
    @Override public void onStallWarning(    StallWarning stallWarning){
    }
    @Override public void onException(    Exception e){
    }
  }
;
  ConfigurationBuilder configurationBuilder=new ConfigurationBuilder();
  configurationBuilder.setDebugEnabled(false).setOAuthConsumerKey(twitterConfig.consumerKey).setOAuthConsumerSecret(twitterConfig.consumeSecret).setOAuthAccessToken(twitterConfig.accessToken).setOAuthAccessTokenSecret(twitterConfig.accessTokenSecret);
  twitterStream=new TwitterStreamFactory(configurationBuilder.build()).getInstance();
  twitterStream.addListener(statusListener);
  twitterStream.sample();
}"
6966,"public JmsPluginConfig(String destinationName,@Nullable Integer messagesToReceive,String initialContextFactory,String providerUrl,@Nullable String connectionFactoryName){
  this.destinationName=destinationName;
  if (messagesToReceive != null) {
    this.messagesToReceive=messagesToReceive;
  }
 else {
    this.messagesToReceive=50;
  }
  this.initialContextFactory=initialContextFactory;
  this.providerUrl=providerUrl;
  if (connectionFactoryName != null) {
    this.connectionFactoryName=connectionFactoryName;
  }
 else {
    this.connectionFactoryName=DEFAULT_CONNECTION_FACTORY;
  }
}","public JmsPluginConfig(String destinationName,String initialContextFactory,String providerUrl,@Nullable Integer messagesToReceive,@Nullable String connectionFactoryName,@Nullable String jmsPluginName,@Nullable String jmsPluginType){
  this.destinationName=destinationName;
  if (messagesToReceive != null) {
    this.messagesToReceive=messagesToReceive;
  }
 else {
    this.messagesToReceive=50;
  }
  this.initialContextFactory=initialContextFactory;
  this.providerUrl=providerUrl;
  if (connectionFactoryName != null) {
    this.connectionFactoryName=connectionFactoryName;
  }
 else {
    this.connectionFactoryName=DEFAULT_CONNECTION_FACTORY;
  }
  this.jmsPluginName=jmsPluginName;
  if (this.jmsPluginName == null) {
    this.jmsPluginName=Context.INITIAL_CONTEXT_FACTORY;
  }
  this.jmsPluginType=jmsPluginType;
  if (this.jmsPluginType == null) {
    this.jmsPluginType=JMS_PROVIDER;
  }
}"
6967,"@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  Class<Object> driver=pipelineConfigurer.usePluginClass(""String_Node_Str"",Context.INITIAL_CONTEXT_FACTORY,""String_Node_Str"",PluginProperties.builder().build());
  Preconditions.checkArgument(driver != null,""String_Node_Str"");
}","@Override public void configurePipeline(PipelineConfigurer pipelineConfigurer){
  String pluginId=String.format(""String_Node_Str"",""String_Node_Str"",config.jmsPluginType,config.jmsPluginName);
  Class<Object> driver=pipelineConfigurer.usePluginClass(config.jmsPluginType,config.jmsPluginName,pluginId,PluginProperties.builder().build());
  Preconditions.checkArgument(driver != null,""String_Node_Str"");
}"
6968,"private void initializeJmsSource(String destination,int messageReceive,String initialContextFactory,String providerUrl){
  jmsSource=new JmsSource(new JmsPluginConfig(destination,messageReceive,initialContextFactory,providerUrl,JmsSource.DEFAULT_CONNECTION_FACTORY));
}","private void initializeJmsSource(String destination,int messageReceive,String initialContextFactory,String providerUrl){
  jmsSource=new JmsSource(new JmsPluginConfig(destination,initialContextFactory,providerUrl,messageReceive,JmsSource.DEFAULT_CONNECTION_FACTORY,null,null));
}"
6969,"@Override protected Enumeration<URL> findResources(String name) throws IOException {
  Set<URL> urls=Sets.newHashSet();
  for (  ClassLoader classLoader : delegates) {
    Iterators.addAll(urls,Iterators.forEnumeration(classLoader.getResources(name)));
  }
  return Iterators.asEnumeration(urls.iterator());
}","@Override protected Enumeration<URL> findResources(String name) throws IOException {
  Set<URL> urls=Sets.newLinkedHashSet();
  for (  ClassLoader classLoader : delegates) {
    Iterators.addAll(urls,Iterators.forEnumeration(classLoader.getResources(name)));
  }
  return Iterators.asEnumeration(urls.iterator());
}"
6970,"@Override public void apply(HBaseConsumerStateStore input) throws Exception {
  ImmutablePair<byte[],Map<byte[],byte[]>> result;
  while ((result=scanner.next()) != null) {
    byte[] rowKey=result.getFirst();
    Map<byte[],byte[]> columns=result.getSecond();
    visitRow(outStats,input.getTransaction(),rowKey,columns.get(stateColumnName),queueRowPrefix.length);
  }
}","@Override public void apply(HBaseConsumerStateStore input) throws Exception {
  ImmutablePair<byte[],Map<byte[],byte[]>> result;
  while ((result=scanner.next()) != null) {
    byte[] rowKey=result.getFirst();
    Map<byte[],byte[]> columns=result.getSecond();
    visitRow(outStats,input.getTransaction(),rowKey,columns.get(stateColumnName),queueRowPrefix.length);
    if (Boolean.parseBoolean(System.getProperty(""String_Node_Str"")) && outStats.getTotal() % rowsCache == 0) {
      System.out.printf(""String_Node_Str"",instanceId,outStats.getReport());
    }
  }
}"
6971,"public static void main(String[] args) throws Exception {
  if (args.length == 0) {
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.exit(1);
  }
  final QueueName queueName=QueueName.from(URI.create(args[0]));
  Long consumerGroupId=null;
  if (args.length >= 2) {
    String consumerFlowlet=args[1];
    Id.Program flowId=Id.Program.from(queueName.getFirstComponent(),queueName.getSecondComponent(),ProgramType.FLOW,queueName.getThirdComponent());
    consumerGroupId=FlowUtils.generateConsumerGroupId(flowId,consumerFlowlet);
  }
  Injector injector=Guice.createInjector(new ConfigModule(),new ZKClientModule(),new TransactionClientModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(QueueClientFactory.class).to(HBaseQueueClientFactory.class).in(Singleton.class);
      bind(QueueAdmin.class).to(HBaseQueueAdmin.class).in(Singleton.class);
      bind(HBaseTableUtil.class).toProvider(HBaseTableUtilFactory.class);
    }
  }
,new DiscoveryRuntimeModule().getDistributedModules(),new LocationRuntimeModule().getDistributedModules(),new DataSetsModules().getDistributedModules(),new TransactionModules().getDistributedModules());
  HBaseQueueDebugger debugger=injector.getInstance(HBaseQueueDebugger.class);
  debugger.startAndWait();
  debugger.scanQueue(queueName,consumerGroupId);
  debugger.stopAndWait();
}","public static void main(String[] args) throws Exception {
  if (args.length == 0) {
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println();
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.out.println(""String_Node_Str"");
    System.exit(1);
  }
  final QueueName queueName=QueueName.from(URI.create(args[0]));
  Long consumerGroupId=null;
  if (args.length >= 2) {
    String consumerFlowlet=args[1];
    Id.Program flowId=Id.Program.from(queueName.getFirstComponent(),queueName.getSecondComponent(),ProgramType.FLOW,queueName.getThirdComponent());
    consumerGroupId=FlowUtils.generateConsumerGroupId(flowId,consumerFlowlet);
  }
  Injector injector=Guice.createInjector(new ConfigModule(),new ZKClientModule(),new TransactionClientModule(),new AbstractModule(){
    @Override protected void configure(){
      bind(QueueClientFactory.class).to(HBaseQueueClientFactory.class).in(Singleton.class);
      bind(QueueAdmin.class).to(HBaseQueueAdmin.class).in(Singleton.class);
      bind(HBaseTableUtil.class).toProvider(HBaseTableUtilFactory.class);
    }
  }
,new DiscoveryRuntimeModule().getDistributedModules(),new LocationRuntimeModule().getDistributedModules(),new DataSetsModules().getDistributedModules(),new TransactionModules().getDistributedModules());
  HBaseQueueDebugger debugger=injector.getInstance(HBaseQueueDebugger.class);
  debugger.startAndWait();
  debugger.scanQueue(queueName,consumerGroupId);
  debugger.stopAndWait();
}"
6972,"private void scanQueue(TransactionExecutor txExecutor,HBaseConsumerStateStore stateStore,QueueName queueName,QueueBarrier start,@Nullable QueueBarrier end,final QueueStatistics outStats) throws Exception {
  final byte[] queueRowPrefix=QueueEntryRow.getQueueRowPrefix(queueName);
  ConsumerGroupConfig groupConfig=start.getGroupConfig();
  System.out.printf(""String_Node_Str"",groupConfig);
  HBaseQueueAdmin admin=queueClientFactory.getQueueAdmin();
  TableId tableId=admin.getDataTableId(queueName,QueueConstants.QueueType.SHARDED_QUEUE);
  HTable hTable=queueClientFactory.createHTable(tableId);
  System.out.printf(""String_Node_Str"",Bytes.toString(hTable.getTableName()));
  final byte[] stateColumnName=Bytes.add(QueueEntryRow.STATE_COLUMN_PREFIX,Bytes.toBytes(groupConfig.getGroupId()));
  int distributorBuckets=queueClientFactory.getDistributorBuckets(hTable.getTableDescriptor());
  ShardedHBaseQueueStrategy queueStrategy=new ShardedHBaseQueueStrategy(distributorBuckets);
  Scan scan=new Scan();
  scan.setStartRow(start.getStartRow());
  if (end != null) {
    scan.setStopRow(end.getStartRow());
  }
  scan.addColumn(QueueEntryRow.COLUMN_FAMILY,QueueEntryRow.DATA_COLUMN);
  scan.addColumn(QueueEntryRow.COLUMN_FAMILY,QueueEntryRow.META_COLUMN);
  scan.addColumn(QueueEntryRow.COLUMN_FAMILY,stateColumnName);
  scan.setMaxVersions(1);
  System.out.printf(""String_Node_Str"",scan.toString());
  List<Integer> instanceIds=Lists.newArrayList();
  if (groupConfig.getDequeueStrategy() == DequeueStrategy.FIFO) {
    instanceIds.add(0);
  }
 else {
    for (int instanceId=0; instanceId < groupConfig.getGroupSize(); instanceId++) {
      instanceIds.add(instanceId);
    }
  }
  for (  int instanceId : instanceIds) {
    System.out.printf(""String_Node_Str"",instanceId);
    ConsumerConfig consConfig=new ConsumerConfig(groupConfig,instanceId);
    final QueueScanner scanner=queueStrategy.createScanner(consConfig,hTable,scan,100);
    txExecutor.execute(new TransactionExecutor.Procedure<HBaseConsumerStateStore>(){
      @Override public void apply(      HBaseConsumerStateStore input) throws Exception {
        ImmutablePair<byte[],Map<byte[],byte[]>> result;
        while ((result=scanner.next()) != null) {
          byte[] rowKey=result.getFirst();
          Map<byte[],byte[]> columns=result.getSecond();
          visitRow(outStats,input.getTransaction(),rowKey,columns.get(stateColumnName),queueRowPrefix.length);
        }
      }
    }
,stateStore);
  }
}","private void scanQueue(TransactionExecutor txExecutor,HBaseConsumerStateStore stateStore,QueueName queueName,QueueBarrier start,@Nullable QueueBarrier end,final QueueStatistics outStats) throws Exception {
  final byte[] queueRowPrefix=QueueEntryRow.getQueueRowPrefix(queueName);
  ConsumerGroupConfig groupConfig=start.getGroupConfig();
  System.out.printf(""String_Node_Str"",groupConfig);
  HBaseQueueAdmin admin=queueClientFactory.getQueueAdmin();
  TableId tableId=admin.getDataTableId(queueName,QueueConstants.QueueType.SHARDED_QUEUE);
  HTable hTable=queueClientFactory.createHTable(tableId);
  System.out.printf(""String_Node_Str"",Bytes.toString(hTable.getTableName()));
  final byte[] stateColumnName=Bytes.add(QueueEntryRow.STATE_COLUMN_PREFIX,Bytes.toBytes(groupConfig.getGroupId()));
  int distributorBuckets=queueClientFactory.getDistributorBuckets(hTable.getTableDescriptor());
  ShardedHBaseQueueStrategy queueStrategy=new ShardedHBaseQueueStrategy(distributorBuckets);
  Scan scan=new Scan();
  scan.setStartRow(start.getStartRow());
  if (end != null) {
    scan.setStopRow(end.getStartRow());
  }
 else {
    scan.setStopRow(QueueEntryRow.getQueueEntryRowKey(queueName,Long.MAX_VALUE,Integer.MAX_VALUE));
  }
  scan.addColumn(QueueEntryRow.COLUMN_FAMILY,QueueEntryRow.META_COLUMN);
  scan.addColumn(QueueEntryRow.COLUMN_FAMILY,stateColumnName);
  scan.setCacheBlocks(false);
  scan.setMaxVersions(1);
  System.out.printf(""String_Node_Str"",scan.toString());
  List<Integer> instanceIds=Lists.newArrayList();
  if (groupConfig.getDequeueStrategy() == DequeueStrategy.FIFO) {
    instanceIds.add(0);
  }
 else {
    for (int instanceId=0; instanceId < groupConfig.getGroupSize(); instanceId++) {
      instanceIds.add(instanceId);
    }
  }
  final int rowsCache=Integer.parseInt(System.getProperty(""String_Node_Str"",""String_Node_Str""));
  for (  final int instanceId : instanceIds) {
    System.out.printf(""String_Node_Str"",instanceId);
    ConsumerConfig consConfig=new ConsumerConfig(groupConfig,instanceId);
    final QueueScanner scanner=queueStrategy.createScanner(consConfig,hTable,scan,rowsCache);
    try {
      txExecutor.execute(new TransactionExecutor.Procedure<HBaseConsumerStateStore>(){
        @Override public void apply(        HBaseConsumerStateStore input) throws Exception {
          ImmutablePair<byte[],Map<byte[],byte[]>> result;
          while ((result=scanner.next()) != null) {
            byte[] rowKey=result.getFirst();
            Map<byte[],byte[]> columns=result.getSecond();
            visitRow(outStats,input.getTransaction(),rowKey,columns.get(stateColumnName),queueRowPrefix.length);
            if (Boolean.parseBoolean(System.getProperty(""String_Node_Str"")) && outStats.getTotal() % rowsCache == 0) {
              System.out.printf(""String_Node_Str"",instanceId,outStats.getReport());
            }
          }
        }
      }
,stateStore);
    }
 catch (    TransactionFailureException e) {
      if (!(Throwables.getRootCause(e) instanceof TransactionNotInProgressException)) {
        throw Throwables.propagate(e);
      }
    }
    System.out.printf(""String_Node_Str"",instanceId,outStats.getReport());
  }
}"
6973,"@Override public void run(){
  for (  ProgramType programType : ProgramType.values()) {
    programLifecycleService.validateAndCorrectRunningRunRecords(programType,store,runtimeService);
  }
}","@Override public void run(){
  for (  ProgramType programType : ProgramType.values()) {
    programLifecycleService.validateAndCorrectRunningRunRecords(programType);
  }
}"
6974,"public RunRecordsCorrectorRunnable(ProgramLifecycleService programLifecycleService,Store store,ProgramRuntimeService runtimeService){
  this.programLifecycleService=programLifecycleService;
  this.store=store;
  this.runtimeService=runtimeService;
}","public RunRecordsCorrectorRunnable(ProgramLifecycleService programLifecycleService){
  this.programLifecycleService=programLifecycleService;
}"
6975,"@Override protected void startUp() throws Exception {
  LOG.info(""String_Node_Str"");
  scheduledExecutorService.scheduleWithFixedDelay(new RunRecordsCorrectorRunnable(this,store,runtimeService),2L,600L,TimeUnit.SECONDS);
}","@Override protected void startUp() throws Exception {
  LOG.info(""String_Node_Str"");
  scheduledExecutorService.scheduleWithFixedDelay(new RunRecordsCorrectorRunnable(this),2L,600L,TimeUnit.SECONDS);
}"
6976,"/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService}.
 * @param programType The type of programs the run records nee to validate and update.
 * @param store The data store that manages run records instances for all programs.
 * @param runtimeService The {@link ProgramRuntimeService} instance to check the actual state of the program.
 */
private void validateAndCorrectRunningRunRecords(ProgramType programType,Store store,ProgramRuntimeService runtimeService){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecord> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    @Nullable RunRecord input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.debug(""String_Node_Str"",invalidRunRecords.size());
  }
  for (  RunRecord rr : invalidRunRecords) {
    String runId=rr.getPid();
    List<NamespaceMeta> namespaceMetas=store.listNamespaces();
    Id.Program targetProgramId=null;
    for (    NamespaceMeta nm : namespaceMetas) {
      Id.Namespace accId=Id.Namespace.from(nm.getName());
      Collection<ApplicationSpecification> appSpecs=store.getAllApplications(accId);
      for (      ApplicationSpecification appSpec : appSpecs) {
switch (programType) {
case FLOW:
          for (          String programName : appSpec.getFlows().keySet()) {
            Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
            if (programId != null) {
              targetProgramId=programId;
              break;
            }
          }
        break;
case MAPREDUCE:
      for (      String programName : appSpec.getMapReduce().keySet()) {
        Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
        if (programId != null) {
          targetProgramId=programId;
          break;
        }
      }
    break;
case SPARK:
  for (  String programName : appSpec.getSpark().keySet()) {
    Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
    if (programId != null) {
      targetProgramId=programId;
      break;
    }
  }
break;
case SERVICE:
for (String programName : appSpec.getServices().keySet()) {
Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
  targetProgramId=programId;
  break;
}
}
break;
case WORKER:
for (String programName : appSpec.getWorkers().keySet()) {
Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case WORKFLOW:
for (String programName : appSpec.getWorkflows().keySet()) {
Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
default :
LOG.debug(""String_Node_Str"" + programType.name());
break;
}
if (targetProgramId != null) {
break;
}
}
if (targetProgramId != null) {
break;
}
}
if (targetProgramId != null) {
store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
}
}
}","/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService}.
 * @param programType The type of programs the run records nee to validate and update.
 */
private void validateAndCorrectRunningRunRecords(ProgramType programType){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecord> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    @Nullable RunRecord input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"",invalidRunRecords.size(),programType.getPrettyName());
  }
  for (  RunRecord rr : invalidRunRecords) {
    boolean shouldCorrect=shouldCorrectForWorkflowChildren(rr);
    if (!shouldCorrect) {
      continue;
    }
    String runId=rr.getPid();
    Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
    if (targetProgramId != null) {
      store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
    }
  }
}"
6977,"@Override public boolean apply(RunRecord record){
  if (record.getTwillRunId() == null) {
    return false;
  }
  return twillRunIds.contains(record.getTwillRunId());
}","@Override public boolean apply(RunRecord record){
  return record.getTwillRunId() != null && twillRunIds.contains(org.apache.twill.internal.RunIds.fromString(record.getTwillRunId()));
}"
6978,"@Override public synchronized Map<RunId,RuntimeInfo> list(ProgramType type){
  Map<RunId,RuntimeInfo> result=Maps.newHashMap();
  result.putAll(super.list(type));
  Table<Id.Program,RunId,TwillController> twillProgramInfo=HashBasedTable.create();
  for (  TwillRunner.LiveInfo liveInfo : twillRunner.lookupLive()) {
    String appName=liveInfo.getApplicationName();
    Matcher matcher=APP_NAME_PATTERN.matcher(appName);
    if (!matcher.matches()) {
      continue;
    }
    if (!type.equals(getType(matcher.group(1)))) {
      continue;
    }
    for (    TwillController controller : liveInfo.getControllers()) {
      RunId twillRunId=controller.getRunId();
      if (isTwillRunIdCached(twillRunId)) {
        continue;
      }
      Id.Program programId=Id.Program.from(matcher.group(2),matcher.group(3),type,matcher.group(4));
      twillProgramInfo.put(programId,twillRunId,controller);
    }
  }
  if (twillProgramInfo.isEmpty()) {
    return ImmutableMap.copyOf(result);
  }
  final Set<RunId> twillRunIds=twillProgramInfo.columnKeySet();
  List<RunRecord> activeRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    RunRecord record){
      if (record.getTwillRunId() == null) {
        return false;
      }
      return twillRunIds.contains(record.getTwillRunId());
    }
  }
);
  for (  RunRecord record : activeRunRecords) {
    RunId twillRunIdFromRecord=org.apache.twill.internal.RunIds.fromString(record.getTwillRunId());
    RunId runId=RunIds.fromString(record.getPid());
    Map<Id.Program,TwillController> mapForTwillId=twillProgramInfo.columnMap().get(twillRunIdFromRecord);
    Map.Entry<Id.Program,TwillController> entry=mapForTwillId.entrySet().iterator().next();
    RuntimeInfo runtimeInfo=createRuntimeInfo(type,entry.getKey(),entry.getValue(),runId);
    if (runtimeInfo != null) {
      result.put(runId,runtimeInfo);
      updateRuntimeInfo(type,runId,runtimeInfo);
    }
 else {
      LOG.warn(""String_Node_Str"",type,entry.getKey());
    }
  }
  return ImmutableMap.copyOf(result);
}","@Override public synchronized Map<RunId,RuntimeInfo> list(ProgramType type){
  Map<RunId,RuntimeInfo> result=Maps.newHashMap();
  result.putAll(super.list(type));
  Table<Id.Program,RunId,TwillController> twillProgramInfo=HashBasedTable.create();
  for (  TwillRunner.LiveInfo liveInfo : twillRunner.lookupLive()) {
    String appName=liveInfo.getApplicationName();
    Matcher matcher=APP_NAME_PATTERN.matcher(appName);
    if (!matcher.matches()) {
      continue;
    }
    if (!type.equals(getType(matcher.group(1)))) {
      continue;
    }
    for (    TwillController controller : liveInfo.getControllers()) {
      RunId twillRunId=controller.getRunId();
      if (isTwillRunIdCached(twillRunId)) {
        continue;
      }
      Id.Program programId=Id.Program.from(matcher.group(2),matcher.group(3),type,matcher.group(4));
      twillProgramInfo.put(programId,twillRunId,controller);
    }
  }
  if (twillProgramInfo.isEmpty()) {
    return ImmutableMap.copyOf(result);
  }
  final Set<RunId> twillRunIds=twillProgramInfo.columnKeySet();
  List<RunRecord> activeRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    RunRecord record){
      return record.getTwillRunId() != null && twillRunIds.contains(org.apache.twill.internal.RunIds.fromString(record.getTwillRunId()));
    }
  }
);
  for (  RunRecord record : activeRunRecords) {
    RunId twillRunIdFromRecord=org.apache.twill.internal.RunIds.fromString(record.getTwillRunId());
    RunId runId=RunIds.fromString(record.getPid());
    Map<Id.Program,TwillController> mapForTwillId=twillProgramInfo.columnMap().get(twillRunIdFromRecord);
    Map.Entry<Id.Program,TwillController> entry=mapForTwillId.entrySet().iterator().next();
    RuntimeInfo runtimeInfo=createRuntimeInfo(type,entry.getKey(),entry.getValue(),runId);
    if (runtimeInfo != null) {
      result.put(runId,runtimeInfo);
      updateRuntimeInfo(type,runId,runtimeInfo);
    }
 else {
      LOG.warn(""String_Node_Str"",type,entry.getKey());
    }
  }
  return ImmutableMap.copyOf(result);
}"
6979,"/** 
 * Using the given configuration, configure an Adapter with the given configuration. This method is called when an adapter is created in order to define what Datasets, Streams, and Plugins, and runtime arguments should be available to the adapter.
 * @param name name of the adapter
 * @param configuration adapter configuration. It will be {@code null} if there is no configuration provided.
 * @param configurer {@link AdapterConfigurer} used to configure the adapter.
 * @throws IllegalArgumentException if the configuration is not valid
 * @throws Exception if there was some other error configuring the adapter
 */
public void configureAdapter(String name,@Nullable T configuration,AdapterConfigurer configurer) throws Exception {
}","/** 
 * Called when an adapter is created in order to define what Datasets, Streams, Plugins, and runtime arguments should be available to the adapter, as determined by the given configuration.
 * @param name name of the adapter
 * @param configuration adapter configuration. It will be {@code null} if there is no configuration provided.
 * @param configurer {@link AdapterConfigurer} used to configure the adapter.
 * @throws IllegalArgumentException if the configuration is not valid
 * @throws Exception if there was some other error configuring the adapter
 */
public void configureAdapter(String name,@Nullable T configuration,AdapterConfigurer configurer) throws Exception {
}"
6980,"@Override public void run(){
  for (  ProgramType programType : ProgramType.values()) {
    programLifecycleService.validateAndCorrectRunningRunRecords(programType,store,runtimeService);
  }
}","@Override public void run(){
  for (  ProgramType programType : ProgramType.values()) {
    programLifecycleService.validateAndCorrectRunningRunRecords(programType);
  }
}"
6981,"public RunRecordsCorrectorRunnable(ProgramLifecycleService programLifecycleService,Store store,ProgramRuntimeService runtimeService){
  this.programLifecycleService=programLifecycleService;
  this.store=store;
  this.runtimeService=runtimeService;
}","public RunRecordsCorrectorRunnable(ProgramLifecycleService programLifecycleService){
  this.programLifecycleService=programLifecycleService;
}"
6982,"@Override protected void startUp() throws Exception {
  LOG.info(""String_Node_Str"");
  scheduledExecutorService.scheduleWithFixedDelay(new RunRecordsCorrectorRunnable(this,store,runtimeService),2L,600L,TimeUnit.SECONDS);
}","@Override protected void startUp() throws Exception {
  LOG.info(""String_Node_Str"");
  scheduledExecutorService.scheduleWithFixedDelay(new RunRecordsCorrectorRunnable(this),2L,600L,TimeUnit.SECONDS);
}"
6983,"/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService}.
 * @param programType The type of programs the run records nee to validate and update.
 * @param store The data store that manages run records instances for all programs.
 * @param runtimeService The {@link ProgramRuntimeService} instance to check the actual state of the program.
 */
private void validateAndCorrectRunningRunRecords(ProgramType programType,Store store,ProgramRuntimeService runtimeService){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecord> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    @Nullable RunRecord input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.debug(""String_Node_Str"",invalidRunRecords.size());
  }
  for (  RunRecord rr : invalidRunRecords) {
    String runId=rr.getPid();
    List<NamespaceMeta> namespaceMetas=store.listNamespaces();
    Id.Program targetProgramId=null;
    for (    NamespaceMeta nm : namespaceMetas) {
      Id.Namespace accId=Id.Namespace.from(nm.getName());
      Collection<ApplicationSpecification> appSpecs=store.getAllApplications(accId);
      for (      ApplicationSpecification appSpec : appSpecs) {
switch (programType) {
case FLOW:
          for (          String programName : appSpec.getFlows().keySet()) {
            Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
            if (programId != null) {
              targetProgramId=programId;
              break;
            }
          }
        break;
case MAPREDUCE:
      for (      String programName : appSpec.getMapReduce().keySet()) {
        Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
        if (programId != null) {
          targetProgramId=programId;
          break;
        }
      }
    break;
case SPARK:
  for (  String programName : appSpec.getSpark().keySet()) {
    Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
    if (programId != null) {
      targetProgramId=programId;
      break;
    }
  }
break;
case SERVICE:
for (String programName : appSpec.getServices().keySet()) {
Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
  targetProgramId=programId;
  break;
}
}
break;
case WORKER:
for (String programName : appSpec.getWorkers().keySet()) {
Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
case WORKFLOW:
for (String programName : appSpec.getWorkflows().keySet()) {
Id.Program programId=validateProgramForRunRecord(store,nm.getName(),appSpec.getName(),programType,programName,runId);
if (programId != null) {
targetProgramId=programId;
break;
}
}
break;
default :
LOG.debug(""String_Node_Str"" + programType.name());
break;
}
if (targetProgramId != null) {
break;
}
}
if (targetProgramId != null) {
break;
}
}
if (targetProgramId != null) {
store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
}
}
}","/** 
 * Fix all the possible inconsistent states for RunRecords that shows it is in RUNNING state but actually not via check to   {@link ProgramRuntimeService}.
 * @param programType The type of programs the run records nee to validate and update.
 */
private void validateAndCorrectRunningRunRecords(ProgramType programType){
  final Map<RunId,RuntimeInfo> runIdToRuntimeInfo=runtimeService.list(programType);
  List<RunRecord> invalidRunRecords=store.getRuns(ProgramRunStatus.RUNNING,new Predicate<RunRecord>(){
    @Override public boolean apply(    @Nullable RunRecord input){
      if (input == null) {
        return false;
      }
      String runId=input.getPid();
      return !runIdToRuntimeInfo.containsKey(RunIds.fromString(runId));
    }
  }
);
  if (!invalidRunRecords.isEmpty()) {
    LOG.warn(""String_Node_Str"",invalidRunRecords.size(),programType.getPrettyName());
  }
  for (  RunRecord rr : invalidRunRecords) {
    boolean shouldCorrect=shouldCorrectForWorkflowChildren(rr);
    if (!shouldCorrect) {
      continue;
    }
    String runId=rr.getPid();
    Id.Program targetProgramId=retrieveProgramIdForRunRecord(programType,runId);
    if (targetProgramId != null) {
      store.compareAndSetStatus(targetProgramId,runId,ProgramController.State.ALIVE.getRunStatus(),ProgramController.State.ERROR.getRunStatus());
    }
  }
}"
6984,"public JmsPluginConfig(String destinationName,@Nullable Integer messagesToReceive,String initialContextFactory,String providerUrl,@Nullable String connectionFactoryName){
  this.destinationName=destinationName;
  if (messagesToReceive != null) {
    this.messagesToReceive=messagesToReceive;
  }
 else {
    this.messagesToReceive=50;
  }
  this.initialContextFactory=initialContextFactory;
  this.providerUrl=providerUrl;
  if (connectionFactoryName != null) {
    this.connectionFactoryName=connectionFactoryName;
  }
 else {
    this.connectionFactoryName=JMS_CONNECTION_FACTORY_NAME;
  }
}","public JmsPluginConfig(String destinationName,@Nullable Integer messagesToReceive,String initialContextFactory,String providerUrl,@Nullable String connectionFactoryName){
  this.destinationName=destinationName;
  if (messagesToReceive != null) {
    this.messagesToReceive=messagesToReceive;
  }
 else {
    this.messagesToReceive=50;
  }
  this.initialContextFactory=initialContextFactory;
  this.providerUrl=providerUrl;
  if (connectionFactoryName != null) {
    this.connectionFactoryName=connectionFactoryName;
  }
 else {
    this.connectionFactoryName=DEFAULT_CONNECTION_FACTORY;
  }
}"
6985,"private void initializeJmsSource(String destination,int messageReceive,String initialContextFactory,String providerUrl){
  jmsSource=new JmsSource(new JmsPluginConfig(destination,messageReceive,initialContextFactory,providerUrl,JmsSource.JMS_CONNECTION_FACTORY_NAME));
}","private void initializeJmsSource(String destination,int messageReceive,String initialContextFactory,String providerUrl){
  jmsSource=new JmsSource(new JmsPluginConfig(destination,messageReceive,initialContextFactory,providerUrl,JmsSource.DEFAULT_CONNECTION_FACTORY));
}"
6986,"/** 
 * Get an instance of the specified Dataset.
 * @param name The name of the Dataset
 * @param arguments the arguments for this dataset instance
 * @param < T > The type of the Dataset
 * @return A new instance of the specified Dataset, never null.
 * @throws DatasetInstantiationException If the Dataset cannot be instantiated: its classcannot be loaded; the default constructor throws an exception; or the Dataset cannot be opened (for example, one of the underlying tables in the DataFabric cannot be accessed).
 */
@Beta public <T extends Dataset>T getDataset(String name,Map<String,String> arguments) throws DatasetInstantiationException ;","/** 
 * Get an instance of the specified Dataset.
 * @param name The name of the Dataset
 * @param arguments the arguments for this dataset instance
 * @param < T > The type of the Dataset
 * @return A new instance of the specified Dataset, never null.
 * @throws DatasetInstantiationException If the Dataset cannot be instantiated: its classcannot be loaded; the default constructor throws an exception; or the Dataset cannot be opened (for example, one of the underlying tables in the DataFabric cannot be accessed).
 */
@Beta <T extends Dataset>T getDataset(String name,Map<String,String> arguments) throws DatasetInstantiationException ;"
6987,"/** 
 * Writes the record into a dataset.
 * @param record record to write into the dataset.
 * @throws IOException when the {@code RECORD} could not be written to the dataset.
 */
public void write(RECORD record) throws IOException ;","/** 
 * Writes the record into a dataset.
 * @param record record to write into the dataset.
 * @throws IOException when the {@code RECORD} could not be written to the dataset.
 */
void write(RECORD record) throws IOException ;"
6988,"/** 
 * Writes in batch using   {@link StreamBatchWriter} to a stream
 * @param stream stream id
 * @param contentType content type
 * @return {@link StreamBatchWriter} provides a batch writer
 * @throws IOException if an error occurred during write
 */
public StreamBatchWriter createBatchWriter(String stream,String contentType) throws IOException ;","/** 
 * Writes in batch using   {@link StreamBatchWriter} to a stream
 * @param stream stream id
 * @param contentType content type
 * @return {@link StreamBatchWriter} provides a batch writer
 * @throws IOException if an error occurred during write
 */
StreamBatchWriter createBatchWriter(String stream,String contentType) throws IOException ;"
6989,"/** 
 * Writes a   {@link StreamEventData} to a stream
 * @param stream stream id
 * @param data {@link StreamEventData} data to be written
 * @throws IOException if an error occurred during write
 */
public void write(String stream,StreamEventData data) throws IOException ;","/** 
 * Writes a   {@link StreamEventData} to a stream
 * @param stream stream id
 * @param data {@link StreamEventData} data to be written
 * @throws IOException if an error occurred during write
 */
void write(String stream,StreamEventData data) throws IOException ;"
6990,"/** 
 * Writes a File to a stream in batch
 * @param stream stream id
 * @param file File
 * @param contentType content type
 * @throws IOException if an error occurred during write
 */
public void writeFile(String stream,File file,String contentType) throws IOException ;","/** 
 * Writes a File to a stream in batch
 * @param stream stream id
 * @param file File
 * @param contentType content type
 * @throws IOException if an error occurred during write
 */
void writeFile(String stream,File file,String contentType) throws IOException ;"
6991,"/** 
 * Return the partition associated with the given time, rounded to the minute; or null if no such partition exists.
 */
@Nullable public TimePartition getPartitionByTime(long time);","/** 
 * Return the partition associated with the given time, rounded to the minute; or null if no such partition exists.
 */
@Nullable TimePartition getPartitionByTime(long time);"
6992,"/** 
 * @return the relative path of the partition for a specific time, rounded to the minute.
 */
@Deprecated @Nullable public String getPartition(long time);","/** 
 * @return the relative path of the partition for a specific time, rounded to the minute.
 */
@Deprecated @Nullable String getPartition(long time);"
6993,"/** 
 * Return all partitions within the time range given by startTime (inclusive) and endTime (exclusive), both rounded to the full minute.
 */
public Set<TimePartition> getPartitionsByTime(long startTime,long endTime);","/** 
 * Return all partitions within the time range given by startTime (inclusive) and endTime (exclusive), both rounded to the full minute.
 */
Set<TimePartition> getPartitionsByTime(long startTime,long endTime);"
6994,"/** 
 * @return a mapping from the partition time to the relative path, of all partitions with a timethat is between startTime (inclusive) and endTime (exclusive), both rounded to the full minute.
 */
@Deprecated public Map<Long,String> getPartitions(long startTime,long endTime);","/** 
 * @return a mapping from the partition time to the relative path, of all partitions with a timethat is between startTime (inclusive) and endTime (exclusive), both rounded to the full minute.
 */
@Deprecated Map<Long,String> getPartitions(long startTime,long endTime);"
6995,"/** 
 * @return the relative paths of all partitions with a time that is between startTime (inclusive)and endTime (exclusive), both rounded to the full minute.
 */
@Deprecated public Collection<String> getPartitionPaths(long startTime,long endTime);","/** 
 * @return the relative paths of all partitions with a time that is between startTime (inclusive)and endTime (exclusive), both rounded to the full minute.
 */
@Deprecated Collection<String> getPartitionPaths(long startTime,long endTime);"
6996,"/** 
 * Return a partition output for a specific time, rounded to the minute, in preparation for creating a new partition. Obtain the location to write from the PartitionOutput, then call the   {@link PartitionOutput#addPartition}to add the partition to this dataset.
 */
public TimePartitionOutput getPartitionOutput(long time);","/** 
 * Return a partition output for a specific time, rounded to the minute, in preparation for creating a new partition. Obtain the location to write from the PartitionOutput, then call the   {@link PartitionOutput#addPartition}to add the partition to this dataset.
 */
TimePartitionOutput getPartitionOutput(long time);"
6997,"/** 
 * Add a partition for a given time, stored at a given path (relative to the file set's base path).
 */
public void addPartition(long time,String path);","/** 
 * Add a partition for a given time, stored at a given path (relative to the file set's base path).
 */
void addPartition(long time,String path);"
6998,"/** 
 * Remove a partition for a given time.
 */
public void dropPartition(long time);","/** 
 * Remove a partition for a given time.
 */
void dropPartition(long time);"
6999,"/** 
 * @return the underlying (embedded) file set.
 * @deprecated use {@link #getEmbeddedFileSet} instead.
 */
@Deprecated public FileSet getUnderlyingFileSet();","/** 
 * @return the underlying (embedded) file set.
 * @deprecated use {@link #getEmbeddedFileSet} instead.
 */
@Deprecated FileSet getUnderlyingFileSet();"
7000,"/** 
 * Returns the next row or   {@code null} if the scanner is exhausted.
 */
@Nullable public Row next();","/** 
 * Returns the next row or   {@code null} if the scanner is exhausted.
 */
@Nullable Row next();"
