record_number,buggy_code,fixed_code,code_similarity
89001,"public static Result searchHome(){
  return ok(search.render());
}","public static Result searchHome(){
  Navigation.set(Level.SEARCH);
  return ok(search.render());
}",0.8048780487804879
89002,"/** 
 * @param account - Account (usually: current user or a contact)
 * @param groupList - a list containing all groups we want to search in (usually all groups from account)
 * @param accountList - a list containing all accounts we want to search in (usually contact from account)
 * @return List of Posts
 */
public static Query streamForAccount(String selectClause,Account account,List<Group> groupList,List<Account> accountList,String filter,String orderByClause){
  HashMap<String,String> streamClausesMap=new HashMap<>();
  List<String> streamClausesList=new ArrayList<>();
  String accountPosts=""String_Node_Str"";
  streamClausesMap.put(""String_Node_Str"",accountPosts);
  String accountGroupPosts=""String_Node_Str"";
  streamClausesMap.put(""String_Node_Str"",accountGroupPosts);
  String allGroupPosts=""String_Node_Str"";
  streamClausesMap.put(""String_Node_Str"",allGroupPosts);
  String accountContactPosts=""String_Node_Str"";
  streamClausesMap.put(""String_Node_Str"",accountContactPosts);
  String contactToAccountPosts=""String_Node_Str"";
  streamClausesMap.put(""String_Node_Str"",contactToAccountPosts);
  String contactPosts=""String_Node_Str"";
  streamClausesMap.put(""String_Node_Str"",contactPosts);
switch (filter) {
case ""String_Node_Str"":
    streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
  break;
case ""String_Node_Str"":
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
break;
case ""String_Node_Str"":
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
break;
case ""String_Node_Str"":
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
break;
default :
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
break;
}
streamClausesList.removeAll(Collections.singleton(null));
String completeQuery=selectClause + ""String_Node_Str"" + assembleClauses(streamClausesList)+ orderByClause;
Query query=JPA.em().createQuery(completeQuery);
if (completeQuery.contains(""String_Node_Str"")) query.setParameter(""String_Node_Str"",account);
if (completeQuery.contains(""String_Node_Str"")) query.setParameter(""String_Node_Str"",groupList);
if (completeQuery.contains(""String_Node_Str"")) query.setParameter(""String_Node_Str"",accountList);
return query;
}","/** 
 * @param account - Account (usually: current user or a contact)
 * @param groupList - a list containing all groups we want to search in (usually all groups from account)
 * @param accountList - a list containing all accounts we want to search in (usually contact from account)
 * @return List of Posts
 */
public static Query streamForAccount(String selectClause,Account account,List<Group> groupList,List<Account> accountList,String filter,String orderByClause){
  HashMap<String,String> streamClausesMap=new HashMap<>();
  List<String> streamClausesList=new ArrayList<>();
  String accountPosts=""String_Node_Str"";
  streamClausesMap.put(""String_Node_Str"",accountPosts);
  String accountGroupPosts=""String_Node_Str"";
  streamClausesMap.put(""String_Node_Str"",accountGroupPosts);
  String allGroupPosts=""String_Node_Str"";
  streamClausesMap.put(""String_Node_Str"",allGroupPosts);
  String accountContactPosts=""String_Node_Str"";
  streamClausesMap.put(""String_Node_Str"",accountContactPosts);
  String contactToAccountPosts=""String_Node_Str"";
  streamClausesMap.put(""String_Node_Str"",contactToAccountPosts);
  String contactPosts=""String_Node_Str"";
  streamClausesMap.put(""String_Node_Str"",contactPosts);
switch (filter) {
case ""String_Node_Str"":
    streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
  break;
case ""String_Node_Str"":
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
break;
case ""String_Node_Str"":
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
break;
case ""String_Node_Str"":
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
break;
case ""String_Node_Str"":
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
break;
default :
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
streamClausesList.add(streamClausesMap.get(""String_Node_Str""));
break;
}
streamClausesList.removeAll(Collections.singleton(null));
String completeQuery=selectClause + ""String_Node_Str"" + assembleClauses(streamClausesList)+ orderByClause;
Query query=JPA.em().createQuery(completeQuery);
if (completeQuery.contains(""String_Node_Str"")) query.setParameter(""String_Node_Str"",account);
if (completeQuery.contains(""String_Node_Str"")) query.setParameter(""String_Node_Str"",groupList);
if (completeQuery.contains(""String_Node_Str"")) query.setParameter(""String_Node_Str"",accountList);
return query;
}",0.983242194390545
89003,"static public void resize(File file,int width,int height) throws FileOperationException {
  BufferedImage image;
  try {
    image=ImageIO.read(file);
    image=Scalr.resize(image,Scalr.Method.ULTRA_QUALITY,Scalr.Mode.FIT_EXACT,width,height);
    ImageIO.write(image,""String_Node_Str"",file);
    image.flush();
  }
 catch (  IOException|IllegalArgumentException e) {
    logger.error(e.getMessage(),e);
    throw new FileOperationException(""String_Node_Str"");
  }
}","static public void resize(File file,int width,int height) throws FileOperationException {
  BufferedImage image;
  try {
    image=ImageIO.read(file);
    image=Scalr.resize(image,Scalr.Method.ULTRA_QUALITY,Scalr.Mode.FIT_EXACT,width,height);
    saveToJPG(image,file);
    image.flush();
  }
 catch (  IOException|IllegalArgumentException e) {
    logger.error(e.getMessage(),e);
    throw new FileOperationException(""String_Node_Str"");
  }
}",0.9427312775330396
89004,"static public void crop(File file,int x,int y,int width,int height) throws FileOperationException {
  BufferedImage image;
  try {
    image=ImageIO.read(file);
    image=Scalr.crop(image,x,y,width,height);
    ImageIO.write(image,""String_Node_Str"",file);
    image.flush();
  }
 catch (  IOException|IllegalArgumentException e) {
    logger.error(e.getMessage(),e);
    throw new FileOperationException(""String_Node_Str"");
  }
}","static public void crop(File file,int x,int y,int width,int height) throws FileOperationException {
  BufferedImage image;
  try {
    image=ImageIO.read(file);
    image=Scalr.crop(image,x,y,width,height);
    saveToJPG(image,file);
    image.flush();
  }
 catch (  IOException|IllegalArgumentException e) {
    logger.error(e.getMessage(),e);
    throw new FileOperationException(""String_Node_Str"");
  }
}",0.937799043062201
89005,"/** 
 * Returns all notifications for current user.
 * @return Html rendered instance
 */
@Transactional(readOnly=true) private static Html getNotifications(){
  Account account=Component.currentAccount();
  if (account == null) {
    return new Html(""String_Node_Str"");
  }
  List<Notification> list=null;
  try {
    list=Notification.findByAccountIdUnread(account.id);
  }
 catch (  Throwable throwable) {
    throwable.printStackTrace();
  }
  List<Integer> countedNotifications=NotificationController.countNotifications(list);
  return views.html.Notification.menuitem.render(list,countedNotifications.get(0),Notification.countUnreadNotificationsForAccountId(account.id));
}","/** 
 * Returns all notifications for current user.
 * @return Html rendered instance
 */
@Transactional(readOnly=true) public static Html getNotifications(){
  Account account=Component.currentAccount();
  if (account == null) {
    return new Html(""String_Node_Str"");
  }
  List<Notification> list=null;
  try {
    list=Notification.findByAccountIdUnread(account.id);
  }
 catch (  Throwable throwable) {
    throwable.printStackTrace();
  }
  List<Integer> countedNotifications=NotificationController.countNotifications(list);
  return views.html.Notification.menuitem.render(list,countedNotifications.get(0),Notification.countUnreadNotificationsForAccountId(account.id));
}",0.9918938835666912
89006,"@Override public List<Account> getRecipients(){
  List<Account> recipients=new ArrayList<Account>();
  if (this.type.equals(Group.GROUP_INVITATION)) {
    for (    String accountId : inviteList) {
      try {
        final Account account=Account.findById(Long.parseLong(accountId));
        GroupAccount groupAccount=GroupAccount.find(account,this);
        if (!Group.isMember(this,account) && Friendship.alreadyFriendly(this.getSender(),account) && groupAccount == null) {
          final Group thisGroup=this;
          (new GroupAccount(account,thisGroup,LinkType.invite)).create();
          recipients.add(account);
        }
      }
 catch (      Exception e) {
        e.printStackTrace();
      }
catch (      Throwable t) {
        t.printStackTrace();
      }
    }
    return recipients;
  }
 else   if (this.type.equals(Group.GROUP_NEW_REQUEST)) {
    return this.getAsAccountList(this.owner);
  }
 else   if (this.type.equals(Group.GROUP_NEW_MEDIA)) {
    final Group currentGroup=this;
    return GroupAccount.findAccountsByGroup(currentGroup,LinkType.establish);
  }
  return this.temporaryRecipients;
}","@Override public List<Account> getRecipients(){
  List<Account> recipients=new ArrayList<>();
switch (this.type) {
case Group.GROUP_INVITATION:
    for (    String accountId : inviteList) {
      try {
        final Account account=Account.findByIdTransactional(Long.parseLong(accountId));
        GroupAccount groupAccount=GroupAccount.findTransactional(account,this);
        if (!Group.isMemberTransactional(this,account) && Friendship.alreadyFriendlyTransactional(this.getSender(),account) && groupAccount == null) {
          final Group thisGroup=this;
          JPA.withTransaction(new F.Callback0(){
            @Override public void invoke() throws Throwable {
              (new GroupAccount(account,thisGroup,LinkType.invite)).create();
            }
          }
);
          recipients.add(account);
        }
      }
 catch (      Throwable t) {
        t.printStackTrace();
      }
    }
  return recipients;
case Group.GROUP_NEW_REQUEST:
return this.getAsAccountList(this.owner);
case Group.GROUP_NEW_MEDIA:
final Group currentGroup=this;
return GroupAccount.findAccountsByGroupTransactional(currentGroup,LinkType.establish);
}
return this.temporaryRecipients;
}",0.7505441880713974
89007,"/** 
 * Constructor, sets the web socket attributes.
 * @param accountId Current user ID of this context
 * @param in WebSocket input stream
 * @param out WebSocket output stream
 */
public WebSocketActor(Long accountId,WebSocket.In<JsonNode> in,WebSocket.Out<JsonNode> out){
  this.account=Account.findById(accountId);
  this.in=in;
  this.out=out;
}","/** 
 * Constructor, sets the web socket attributes.
 * @param accountId Current user ID of this context
 * @param in WebSocket input stream
 * @param out WebSocket output stream
 */
public WebSocketActor(Long accountId,WebSocket.In<JsonNode> in,WebSocket.Out<JsonNode> out){
  this.account=Account.findByIdTransactional(accountId);
  this.in=in;
  this.out=out;
}",0.9818181818181818
89008,"/** 
 * Returns all accounts that are assigned to a group as list.
 * @param group Group to retrieve a list of accounts from
 * @return List of accounts of group
 */
public List<Account> getGroupAsAccountList(final Group group){
  return GroupAccount.findAccountsByGroup(group,LinkType.establish);
}","/** 
 * Returns all accounts that are assigned to a group as list.
 * @param group Group to retrieve a list of accounts from
 * @return List of accounts of group
 */
public List<Account> getGroupAsAccountList(final Group group){
  return GroupAccount.findAccountsByGroupTransactional(group,LinkType.establish);
}",0.9787234042553192
89009,"/** 
 * Helper method to return one account as a List<Account> as required by getRecipients() in the INotifiable interface.
 * @param account One account to be returned as list
 * @return List of account instances
 */
public List<Account> getAsAccountList(Account account){
  List<Account> accounts=new ArrayList<Account>();
  accounts.add(account);
  return accounts;
}","/** 
 * Helper method to return one account as a List<Account> as required by getRecipients() in the INotifiable interface.
 * @param account One account to be returned as list
 * @return List of account instances
 */
public List<Account> getAsAccountList(Account account){
  List<Account> accounts=new ArrayList<>();
  accounts.add(account);
  return accounts;
}",0.990450204638472
89010,"/** 
 * WebSocket method when sending chat.
 * @param wsMessage WebSocket message as JsonNode object
 * @param senderActor Sending actor
 * @param sender Sending account
 * @return WebSocket response
 */
@SuppressWarnings(""String_Node_Str"") private JsonNode wsSendChat(JsonNode wsMessage,ActorRef senderActor,Account sender){
  if (!wsMessage.has(""String_Node_Str"") || !wsMessage.has(""String_Node_Str"")) {
    return this.errorResponse(""String_Node_Str"");
  }
  Long recipientAccountId=wsMessage.get(""String_Node_Str"").asLong();
  String text=wsMessage.get(""String_Node_Str"").asText();
  ActorRef recipientActor=this.getActorForAccountId(recipientAccountId);
  Account recipient=Account.findById(recipientAccountId);
  if (recipientActor == null) {
    return this.errorResponse(""String_Node_Str"");
  }
  if (recipientActor.equals(senderActor)) {
    return this.errorResponse(""String_Node_Str"");
  }
  if (sender == null || recipient == null || !(Friendship.alreadyFriendly(sender,recipient))) {
    return this.errorResponse(""String_Node_Str"");
  }
  ObjectNode node=this.successResponseTemplate(WebSocketService.WS_METHOD_RECEIVE_CHAT);
  node.put(""String_Node_Str"",Json.toJson(sender.getAsJson()));
  node.put(""String_Node_Str"",text);
  recipientActor.tell(Json.toJson(node),senderActor);
  return Json.toJson(""String_Node_Str"");
}","/** 
 * WebSocket method when sending chat.
 * @param wsMessage WebSocket message as JsonNode object
 * @param senderActor Sending actor
 * @param sender Sending account
 * @return WebSocket response
 */
@SuppressWarnings(""String_Node_Str"") private JsonNode wsSendChat(JsonNode wsMessage,ActorRef senderActor,Account sender){
  if (!wsMessage.has(""String_Node_Str"") || !wsMessage.has(""String_Node_Str"")) {
    return this.errorResponse(""String_Node_Str"");
  }
  Long recipientAccountId=wsMessage.get(""String_Node_Str"").asLong();
  String text=wsMessage.get(""String_Node_Str"").asText();
  ActorRef recipientActor=this.getActorForAccountId(recipientAccountId);
  Account recipient=Account.findByIdTransactional(recipientAccountId);
  if (recipientActor == null) {
    return this.errorResponse(""String_Node_Str"");
  }
  if (recipientActor.equals(senderActor)) {
    return this.errorResponse(""String_Node_Str"");
  }
  if (sender == null || recipient == null || !(Friendship.alreadyFriendlyTransactional(sender,recipient))) {
    return this.errorResponse(""String_Node_Str"");
  }
  ObjectNode node=this.successResponseTemplate(WebSocketService.WS_METHOD_RECEIVE_CHAT);
  node.put(""String_Node_Str"",Json.toJson(sender.getAsJson()));
  node.put(""String_Node_Str"",text);
  recipientActor.tell(Json.toJson(node),senderActor);
  return Json.toJson(""String_Node_Str"");
}",0.9903560830860534
89011,"/** 
 * LDAP authentication.
 * @return Result
 */
private static Result LdapAuthenticate(){
  Form<Login> form=form(Login.class).bindFromRequest();
  String matriculationNumber=form.field(""String_Node_Str"").value();
  String password=form.field(""String_Node_Str"").value();
  String rememberMe=form.field(""String_Node_Str"").value();
  matriculationNumber=matriculationNumber.trim().toLowerCase();
  LdapService ldap=LdapService.getInstance();
  try {
    ldap.connect(matriculationNumber,password);
  }
 catch (  LdapService.LdapConnectorException e) {
    flash(""String_Node_Str"",e.getMessage());
    Component.addToContext(Component.ContextIdent.loginForm,form);
    return badRequest(index.render());
  }
  Account account=Account.findByLoginName(matriculationNumber);
  AccountRole role=AccountRole.STUDENT;
  if (ldap.getRole() != null) {
    role=ldap.getRole();
  }
  boolean updateAccount=false;
  if (account == null) {
    account=Account.findByEmail(ldap.getEmail());
    if (account == null) {
      account=new Account();
      Logger.info(""String_Node_Str"" + matriculationNumber + ""String_Node_Str"");
      account.firstname=ldap.getFirstName();
      account.lastname=ldap.getLastName();
      account.loginname=matriculationNumber;
      account.email=ldap.getEmail();
      account.password=""String_Node_Str"";
      Random generator=new Random();
      account.avatar=""String_Node_Str"" + generator.nextInt(10);
      account.role=role;
      account.create();
    }
 else {
      updateAccount=true;
    }
  }
 else {
    updateAccount=true;
  }
  if (updateAccount) {
    account.firstname=ldap.getFirstName();
    account.lastname=ldap.getLastName();
    account.email=ldap.getEmail();
    account.role=role;
    account.update();
  }
  session().clear();
  session(""String_Node_Str"",account.id.toString());
  if (rememberMe != null) {
    session(""String_Node_Str"",""String_Node_Str"");
  }
  return redirect(controllers.routes.Application.index());
}","/** 
 * LDAP authentication.
 * @return Result
 */
private static Result LdapAuthenticate(){
  Form<Login> form=form(Login.class).bindFromRequest();
  String matriculationNumber=form.field(""String_Node_Str"").value();
  String password=form.field(""String_Node_Str"").value();
  String rememberMe=form.field(""String_Node_Str"").value();
  matriculationNumber=matriculationNumber.trim().toLowerCase();
  LdapService ldap=LdapService.getInstance();
  try {
    ldap.connect(matriculationNumber,password);
  }
 catch (  LdapService.LdapConnectorException e) {
    flash(""String_Node_Str"",e.getMessage());
    Component.addToContext(Component.ContextIdent.loginForm,form);
    return badRequest(index.render());
  }
  Account account=Account.findByLoginName(matriculationNumber);
  AccountRole role=AccountRole.STUDENT;
  if (ldap.getRole() != null) {
    role=ldap.getRole();
  }
  if (account == null) {
    account=new Account();
    Logger.info(""String_Node_Str"" + matriculationNumber + ""String_Node_Str"");
    account.firstname=ldap.getFirstName();
    account.lastname=ldap.getLastName();
    account.loginname=matriculationNumber;
    account.password=""String_Node_Str"";
    Random generator=new Random();
    account.avatar=""String_Node_Str"" + generator.nextInt(10);
    account.role=role;
    account.create();
  }
 else {
    account.firstname=ldap.getFirstName();
    account.lastname=ldap.getLastName();
    account.role=role;
    account.update();
  }
  session().clear();
  session(""String_Node_Str"",account.id.toString());
  if (rememberMe != null) {
    session(""String_Node_Str"",""String_Node_Str"");
  }
  return redirect(controllers.routes.Application.index());
}",0.5328206536665752
89012,"/** 
 * Private constructor for singleton instance
 */
private LdapService(){
  String server=Play.application().configuration().getString(""String_Node_Str"");
  int port=Integer.parseInt(Play.application().configuration().getString(""String_Node_Str""));
  boolean startTls=Boolean.parseBoolean(Play.application().configuration().getString(""String_Node_Str""));
  LdapConnectionConfig connectionConfig=new LdapConnectionConfig();
  connectionConfig.setLdapHost(server);
  connectionConfig.setLdapPort(port);
  connectionConfig.setUseTls(startTls);
  this.connection=new LdapNetworkConnection(connectionConfig);
}","/** 
 * Private constructor for singleton instance
 */
private LdapService(){
  this.ldapServer=Play.application().configuration().getString(""String_Node_Str"");
  this.ldapPort=Integer.parseInt(Play.application().configuration().getString(""String_Node_Str""));
  this.ldapStartTls=Boolean.parseBoolean(Play.application().configuration().getString(""String_Node_Str""));
}",0.691914022517912
89013,"/** 
 * Connects to the LDAP server. If successful connected, the user is searched. If found, the user data is read and set into firstName and lastName.
 * @param userName User
 * @param password Password
 * @throws LdapConnectorException
 */
public void connect(String userName,String password) throws LdapConnectorException {
  String userRoot=Play.application().configuration().getString(""String_Node_Str"");
  String groupRoot=Play.application().configuration().getString(""String_Node_Str"");
  String connectionBind=Play.application().configuration().getString(""String_Node_Str"").replace(""String_Node_Str"",userName).replace(""String_Node_Str"",userRoot);
  String userSearch=Play.application().configuration().getString(""String_Node_Str"").replace(""String_Node_Str"",userName);
  String groupSearch=Play.application().configuration().getString(""String_Node_Str"").replace(""String_Node_Str"",connectionBind);
  String userFirstName=Play.application().configuration().getString(""String_Node_Str"");
  String userLastName=Play.application().configuration().getString(""String_Node_Str"");
  String userEmail=Play.application().configuration().getString(""String_Node_Str"");
  String groupName=Play.application().configuration().getString(""String_Node_Str"");
  String studentRole=Play.application().configuration().getString(""String_Node_Str"");
  String tutorRole=Play.application().configuration().getString(""String_Node_Str"");
  try {
    this.connection.bind(connectionBind,password);
  }
 catch (  InvalidConnectionException e) {
    e.printStackTrace();
    throw new LdapConnectorException(Messages.get(""String_Node_Str""));
  }
catch (  LdapException e) {
    e.printStackTrace();
    throw new LdapConnectorException(Messages.get(""String_Node_Str""));
  }
  EntryCursor entCursor;
  try {
    entCursor=this.connection.search(userRoot,userSearch,SearchScope.ONELEVEL,""String_Node_Str"");
    entCursor.next();
    Entry entry=entCursor.get();
    this.firstName=entry.get(userFirstName).getString();
    this.lastName=entry.get(userLastName).getString();
    this.email=entry.get(userEmail).getString();
    Logger.info(""String_Node_Str"" + this.firstName + ""String_Node_Str""+ this.lastName+ ""String_Node_Str""+ this.email+ ""String_Node_Str"");
  }
 catch (  LdapException|CursorException e) {
    e.printStackTrace();
    throw new LdapConnectorException(Messages.get(""String_Node_Str""));
  }
  try {
    entCursor=connection.search(groupRoot,groupSearch,SearchScope.ONELEVEL,""String_Node_Str"");
    String role;
    while (entCursor.next()) {
      Entry entry=entCursor.get();
      role=entry.get(groupName).getString();
      if (role.equals(studentRole)) {
        this.role=AccountRole.STUDENT;
      }
      if (role.equals(tutorRole)) {
        this.role=AccountRole.TUTOR;
      }
    }
  }
 catch (  LdapException|CursorException e) {
    e.printStackTrace();
    throw new LdapConnectorException(Messages.get(""String_Node_Str""));
  }
}","/** 
 * Connects to the LDAP ldapServer. If successful connected, the user is searched. If found, the user data is read and set into firstName and lastName.
 * @param userName User
 * @param password Password
 * @throws LdapConnectorException
 */
public void connect(String userName,String password) throws LdapConnectorException {
  String userRoot=Play.application().configuration().getString(""String_Node_Str"");
  String groupRoot=Play.application().configuration().getString(""String_Node_Str"");
  String connectionBind=Play.application().configuration().getString(""String_Node_Str"").replace(""String_Node_Str"",userName).replace(""String_Node_Str"",userRoot);
  String userSearch=Play.application().configuration().getString(""String_Node_Str"").replace(""String_Node_Str"",userName);
  String groupSearch=Play.application().configuration().getString(""String_Node_Str"").replace(""String_Node_Str"",connectionBind);
  String userFirstName=Play.application().configuration().getString(""String_Node_Str"");
  String userLastName=Play.application().configuration().getString(""String_Node_Str"");
  String groupName=Play.application().configuration().getString(""String_Node_Str"");
  String studentRole=Play.application().configuration().getString(""String_Node_Str"");
  String tutorRole=Play.application().configuration().getString(""String_Node_Str"");
  LdapNetworkConnection ldapConnection;
  try {
    LdapConnectionConfig connectionConfig=new LdapConnectionConfig();
    connectionConfig.setLdapHost(this.ldapServer);
    connectionConfig.setLdapPort(this.ldapPort);
    connectionConfig.setUseTls(this.ldapStartTls);
    connectionConfig.setCredentials(password);
    connectionConfig.setName(connectionBind);
    ldapConnection=new LdapNetworkConnection(connectionConfig);
    ldapConnection.bind();
  }
 catch (  InvalidConnectionException e) {
    e.printStackTrace();
    throw new LdapConnectorException(Messages.get(""String_Node_Str""));
  }
catch (  LdapException e) {
    e.printStackTrace();
    throw new LdapConnectorException(Messages.get(""String_Node_Str""));
  }
  EntryCursor entCursor;
  try {
    entCursor=ldapConnection.search(userRoot,userSearch,SearchScope.ONELEVEL,""String_Node_Str"");
    entCursor.next();
    Entry entry=entCursor.get();
    this.firstName=entry.get(userFirstName).getString();
    this.lastName=entry.get(userLastName).getString();
    Logger.info(""String_Node_Str"" + this.firstName + ""String_Node_Str""+ this.lastName);
  }
 catch (  LdapException|CursorException e) {
    e.printStackTrace();
    throw new LdapConnectorException(Messages.get(""String_Node_Str""));
  }
  try {
    entCursor=ldapConnection.search(groupRoot,groupSearch,SearchScope.ONELEVEL,""String_Node_Str"");
    String role;
    while (entCursor.next()) {
      Entry entry=entCursor.get();
      role=entry.get(groupName).getString();
      if (role.equals(studentRole)) {
        this.role=AccountRole.STUDENT;
      }
      if (role.equals(tutorRole)) {
        this.role=AccountRole.TUTOR;
      }
    }
  }
 catch (  LdapException|CursorException e) {
    e.printStackTrace();
    throw new LdapConnectorException(Messages.get(""String_Node_Str""));
  }
}",0.8608467344929439
89014,"/** 
 * LDAP authentication.
 * @return Result
 */
private static Result LDAPAuthenticate(){
  Form<Login> form=form(Login.class).bindFromRequest();
  String matriculationNumber=form.field(""String_Node_Str"").value();
  String password=form.field(""String_Node_Str"").value();
  String rememberMe=form.field(""String_Node_Str"").value();
  matriculationNumber=matriculationNumber.trim().toLowerCase();
  LDAPConnector ldap=new LDAPConnector();
  try {
    ldap.connect(matriculationNumber,password);
  }
 catch (  LDAPConnector.LdapConnectorException e) {
    flash(""String_Node_Str"",e.getMessage());
    Component.addToContext(Component.ContextIdent.loginForm,form);
    return badRequest(index.render());
  }
  Account account=Account.findByLoginName(matriculationNumber);
  AccountRole role=AccountRole.STUDENT;
  if (ldap.getRole() != null) {
    role=ldap.getRole();
  }
  boolean updateAccount=false;
  if (account == null) {
    account=Account.findByEmail(ldap.getEmail());
    if (account == null) {
      account=new Account();
      Logger.info(""String_Node_Str"" + matriculationNumber + ""String_Node_Str"");
      account.firstname=ldap.getFirstName();
      account.lastname=ldap.getLastName();
      account.loginname=matriculationNumber;
      account.email=ldap.getEmail();
      account.password=""String_Node_Str"";
      Random generator=new Random();
      account.avatar=""String_Node_Str"" + generator.nextInt(10);
      account.role=role;
      account.create();
    }
 else {
      updateAccount=true;
    }
  }
 else {
    updateAccount=true;
  }
  if (updateAccount && account instanceof Account) {
    account.firstname=ldap.getFirstName();
    account.lastname=ldap.getLastName();
    account.email=ldap.getEmail();
    account.role=role;
    account.update();
  }
  session().clear();
  session(""String_Node_Str"",account.id.toString());
  if (rememberMe != null) {
    session(""String_Node_Str"",""String_Node_Str"");
  }
  return redirect(controllers.routes.Application.index());
}","/** 
 * LDAP authentication.
 * @return Result
 */
private static Result LDAPAuthenticate(){
  Form<Login> form=form(Login.class).bindFromRequest();
  String matriculationNumber=form.field(""String_Node_Str"").value();
  String password=form.field(""String_Node_Str"").value();
  String rememberMe=form.field(""String_Node_Str"").value();
  matriculationNumber=matriculationNumber.trim().toLowerCase();
  LDAPConnector ldap=new LDAPConnector();
  try {
    ldap.connect(matriculationNumber,password);
  }
 catch (  LDAPConnector.LdapConnectorException e) {
    flash(""String_Node_Str"",e.getMessage());
    Component.addToContext(Component.ContextIdent.loginForm,form);
    return badRequest(index.render());
  }
  Account account=Account.findByLoginName(matriculationNumber);
  AccountRole role=AccountRole.STUDENT;
  if (ldap.getRole() != null) {
    role=ldap.getRole();
  }
  boolean updateAccount=false;
  if (account == null) {
    account=Account.findByEmail(ldap.getEmail());
    if (account == null) {
      account=new Account();
      Logger.info(""String_Node_Str"" + matriculationNumber + ""String_Node_Str"");
      account.firstname=ldap.getFirstName();
      account.lastname=ldap.getLastName();
      account.loginname=matriculationNumber;
      account.email=ldap.getEmail();
      account.password=""String_Node_Str"";
      Random generator=new Random();
      account.avatar=""String_Node_Str"" + generator.nextInt(10);
      account.role=role;
      account.create();
    }
 else {
      updateAccount=true;
    }
  }
 else {
    updateAccount=true;
  }
  if (updateAccount) {
    account.firstname=ldap.getFirstName();
    account.lastname=ldap.getLastName();
    account.email=ldap.getEmail();
    account.role=role;
    account.update();
  }
  session().clear();
  session(""String_Node_Str"",account.id.toString());
  if (rememberMe != null) {
    session(""String_Node_Str"",""String_Node_Str"");
  }
  return redirect(controllers.routes.Application.index());
}",0.798183652875883
89015,"public static Result delete(Long id){
  Group group=Group.findById(id);
  if (Secured.deleteGroup(group)) {
    group.delete();
    flash(""String_Node_Str"",""String_Node_Str"" + group.title + ""String_Node_Str"");
  }
 else {
    flash(""String_Node_Str"",""String_Node_Str"");
  }
  return redirect(controllers.routes.GroupController.index());
}","@Transactional public static Result delete(Long id){
  Group group=Group.findById(id);
  if (Secured.deleteGroup(group)) {
    group.delete();
    flash(""String_Node_Str"",""String_Node_Str"" + group.title + ""String_Node_Str"");
  }
 else {
    flash(""String_Node_Str"",""String_Node_Str"");
  }
  return redirect(controllers.routes.GroupController.index());
}",0.9782923299565848
89016,"/** 
 * Adds a post.
 * @param anyId can be a accountId or groupId
 * @param target define target stream: profile-stream, group-stream
 * @return Result
 */
public static Result addPost(Long anyId,String target){
  Account account=Component.currentAccount();
  Form<Post> filledForm=postForm.bindFromRequest();
  if (target.equals(Post.GROUP)) {
    Group group=Group.findById(anyId);
    if (Secured.isMemberOfGroup(group,account)) {
      if (filledForm.hasErrors()) {
        flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
      }
 else {
        Post post=filledForm.get();
        post.owner=Component.currentAccount();
        post.group=group;
        post.create();
        NotificationService.getInstance().createNotification(post,Post.GROUP);
      }
    }
 else {
      flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
    }
    return redirect(controllers.routes.GroupController.view(group.id,PAGE));
  }
  if (target.equals(Post.PROFILE)) {
    Account profile=Account.findById(anyId);
    if (Secured.isFriend(profile) || profile.equals(account) || Secured.isAdmin()) {
      if (filledForm.hasErrors()) {
        flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
      }
 else {
        Post post=filledForm.get();
        post.account=profile;
        post.owner=account;
        post.create();
        if (!account.equals(profile)) {
          NotificationService.getInstance().createNotification(post,Post.PROFILE);
        }
      }
      return redirect(controllers.routes.ProfileController.stream(anyId,PAGE));
    }
    flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
    return redirect(controllers.routes.ProfileController.stream(anyId,PAGE));
  }
  if (target.equals(Post.STREAM)) {
    Account profile=Account.findById(anyId);
    if (profile.equals(account)) {
      if (filledForm.hasErrors()) {
        flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
      }
 else {
        Post post=filledForm.get();
        post.account=profile;
        post.owner=account;
        post.create();
      }
      return redirect(controllers.routes.Application.stream(PAGE));
    }
    flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
    return redirect(controllers.routes.Application.stream(PAGE));
  }
  return redirect(controllers.routes.Application.index());
}","/** 
 * Adds a post.
 * @param anyId can be a accountId or groupId
 * @param target define target stream: profile-stream, group-stream
 * @return Result
 */
@Transactional public static Result addPost(Long anyId,String target){
  Account account=Component.currentAccount();
  Form<Post> filledForm=postForm.bindFromRequest();
  if (target.equals(Post.GROUP)) {
    Group group=Group.findById(anyId);
    if (Secured.isMemberOfGroup(group,account)) {
      if (filledForm.hasErrors()) {
        flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
      }
 else {
        final Post post=filledForm.get();
        post.owner=Component.currentAccount();
        post.group=group;
        JPA.withTransaction(new F.Callback0(){
          @Override public void invoke() throws Throwable {
            post.create();
          }
        }
);
        NotificationService.getInstance().createNotification(post,Post.GROUP);
      }
    }
 else {
      flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
    }
    return redirect(controllers.routes.GroupController.view(group.id,PAGE));
  }
  if (target.equals(Post.PROFILE)) {
    Account profile=Account.findById(anyId);
    if (Secured.isFriend(profile) || profile.equals(account) || Secured.isAdmin()) {
      if (filledForm.hasErrors()) {
        flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
      }
 else {
        final Post post=filledForm.get();
        post.account=profile;
        post.owner=account;
        JPA.withTransaction(new F.Callback0(){
          @Override public void invoke() throws Throwable {
            post.create();
          }
        }
);
        if (!account.equals(profile)) {
          NotificationService.getInstance().createNotification(post,Post.PROFILE);
        }
      }
      return redirect(controllers.routes.ProfileController.stream(anyId,PAGE));
    }
    flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
    return redirect(controllers.routes.ProfileController.stream(anyId,PAGE));
  }
  if (target.equals(Post.STREAM)) {
    Account profile=Account.findById(anyId);
    if (profile.equals(account)) {
      if (filledForm.hasErrors()) {
        flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
      }
 else {
        final Post post=filledForm.get();
        post.account=profile;
        post.owner=account;
        JPA.withTransaction(new F.Callback0(){
          @Override public void invoke() throws Throwable {
            post.create();
          }
        }
);
      }
      return redirect(controllers.routes.Application.stream(PAGE));
    }
    flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
    return redirect(controllers.routes.Application.stream(PAGE));
  }
  return redirect(controllers.routes.Application.index());
}",0.8996282527881041
89017,"@Override public void delete(){
  Notification.deleteReferences(this);
  JPA.em().remove(this);
}","@Override public void delete(){
  final Friendship self=this;
  JPA.withTransaction(new F.Callback0(){
    @Override public void invoke() throws Throwable {
      Notification.deleteReferences(self);
    }
  }
);
  JPA.em().remove(this);
}",0.369047619047619
89018,"@Override public void delete(){
  List<Post> comments=getCommentsForPost(this.id,0,0);
  for (  Post comment : comments) {
    comment.delete();
  }
  Notification.deleteReferences(this);
  JPA.em().remove(this);
}","@Override public void delete(){
  final Post self=this;
  JPA.withTransaction(new F.Callback0(){
    @Override public void invoke() throws Throwable {
      List<Post> comments=getCommentsForPost(self.id,0,0);
      for (      Post comment : comments) {
        comment.delete();
      }
      Notification.deleteReferences(self);
      JPA.em().remove(self);
    }
  }
);
}",0.5884353741496599
89019,"public static int countPostsForGroup(Group group){
  return ((Number)JPA.em().createQuery(""String_Node_Str"").setParameter(1,group.id).getSingleResult()).intValue();
}","public static int countPostsForGroup(final Group group){
  try {
    return JPA.withTransaction(new F.Function0<Integer>(){
      @Override public Integer apply() throws Throwable {
        return ((Number)JPA.em().createQuery(""String_Node_Str"").setParameter(1,group.id).getSingleResult()).intValue();
      }
    }
);
  }
 catch (  Throwable throwable) {
    throwable.printStackTrace();
    return 0;
  }
}",0.5226480836236934
89020,"@SuppressWarnings(""String_Node_Str"") public static List<Post> getCommentsForPost(Long id,int start,int max){
  return (List<Post>)JPA.em().createQuery(""String_Node_Str"").setParameter(1,id).setFirstResult(start).setMaxResults(max).getResultList();
}","@SuppressWarnings(""String_Node_Str"") public static List<Post> getCommentsForPost(final Long id,final int start,final int max){
  try {
    return JPA.withTransaction(new F.Function0<List<Post>>(){
      @Override public List<Post> apply() throws Throwable {
        return (List<Post>)JPA.em().createQuery(""String_Node_Str"").setParameter(1,id).setFirstResult(start).setMaxResults(max).getResultList();
      }
    }
);
  }
 catch (  Throwable throwable) {
    throwable.printStackTrace();
    return null;
  }
}",0.6534914361001317
89021,"@SuppressWarnings(""String_Node_Str"") public static List<Post> getPostsForGroup(Group group,int limit,int page){
  Query query=JPA.em().createQuery(""String_Node_Str"").setParameter(1,group.id);
  int offset=(page * limit) - limit;
  query=limit(query,limit,offset);
  return query.getResultList();
}","@SuppressWarnings(""String_Node_Str"") public static List<Post> getPostsForGroup(final Group group,final int limit,final int page){
  try {
    return JPA.withTransaction(new F.Function0<List<Post>>(){
      @Override public List<Post> apply() throws Throwable {
        Query query=JPA.em().createQuery(""String_Node_Str"").setParameter(1,group.id);
        int offset=(page * limit) - limit;
        query=limit(query,limit,offset);
        return query.getResultList();
      }
    }
);
  }
 catch (  Throwable throwable) {
    throwable.printStackTrace();
    return null;
  }
}",0.6788571428571428
89022,"/** 
 * Accepts a group entry request.
 * @param groupId Group ID
 * @param accountId Account ID
 * @return Result
 */
public static Result acceptRequest(long groupId,long accountId){
  Account account=Account.findById(accountId);
  Group group=Group.findById(groupId);
  if (group == null) {
    flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
    return redirect(controllers.routes.GroupController.index());
  }
  if (account != null && Secured.isOwnerOfGroup(group,Component.currentAccount())) {
    GroupAccount groupAccount=GroupAccount.find(account,group);
    if (groupAccount != null) {
      groupAccount.linkType=LinkType.establish;
      groupAccount.update();
    }
  }
 else {
    flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
    return redirect(controllers.routes.GroupController.index());
  }
  group.temporarySender=group.owner;
  group.addTemporaryRecipient(account);
  NotificationService.getInstance().createNotification(group,Group.GROUP_REQUEST_SUCCESS);
  return redirect(controllers.routes.GroupController.index());
}","/** 
 * Accepts a group entry request.
 * @param groupId Group ID
 * @param accountId Account ID
 * @return Result
 */
@Transactional public static Result acceptRequest(long groupId,long accountId){
  Account account=Account.findById(accountId);
  Group group=Group.findById(groupId);
  if (group == null) {
    flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
    return redirect(controllers.routes.GroupController.index());
  }
  if (account != null && Secured.isOwnerOfGroup(group,Component.currentAccount())) {
    GroupAccount groupAccount=GroupAccount.find(account,group);
    if (groupAccount != null) {
      groupAccount.linkType=LinkType.establish;
      groupAccount.update();
    }
  }
 else {
    flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
    return redirect(controllers.routes.GroupController.index());
  }
  group.temporarySender=group.owner;
  group.addTemporaryRecipient(account);
  NotificationService.getInstance().createNotification(group,Group.GROUP_REQUEST_SUCCESS);
  return redirect(controllers.routes.GroupController.index());
}",0.9929939280709948
89023,"/** 
 * Declines a group entry request.
 * @param groupId Group ID
 * @param accountId Account ID
 * @return Result
 */
public static Result declineRequest(long groupId,long accountId){
  Account account=Account.findById(accountId);
  Group group=Group.findById(groupId);
  if (group == null) {
    flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
    return redirect(controllers.routes.GroupController.index());
  }
  if (account != null && Secured.isOwnerOfGroup(group,Component.currentAccount())) {
    GroupAccount groupAccount=GroupAccount.find(account,group);
    if (groupAccount != null) {
      groupAccount.linkType=LinkType.reject;
    }
  }
  group.temporarySender=group.owner;
  group.addTemporaryRecipient(account);
  NotificationService.getInstance().createNotification(group,Group.GROUP_REQUEST_DECLINE);
  return redirect(controllers.routes.GroupController.index());
}","/** 
 * Declines a group entry request.
 * @param groupId Group ID
 * @param accountId Account ID
 * @return Result
 */
@Transactional public static Result declineRequest(long groupId,long accountId){
  Account account=Account.findById(accountId);
  Group group=Group.findById(groupId);
  if (group == null) {
    flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
    return redirect(controllers.routes.GroupController.index());
  }
  if (account != null && Secured.isOwnerOfGroup(group,Component.currentAccount())) {
    GroupAccount groupAccount=GroupAccount.find(account,group);
    if (groupAccount != null) {
      groupAccount.linkType=LinkType.reject;
    }
  }
  group.temporarySender=group.owner;
  group.addTemporaryRecipient(account);
  NotificationService.getInstance().createNotification(group,Group.GROUP_REQUEST_DECLINE);
  return redirect(controllers.routes.GroupController.index());
}",0.9916805324459236
89024,"public static Result inviteMember(long groupId){
  Group group=Group.findById(groupId);
  Account currentUser=Component.currentAccount();
  if (Secured.inviteMember(group)) {
    DynamicForm form=Form.form().bindFromRequest();
    group.inviteList=form.data().values();
    if (group.inviteList.size() < 1) {
      flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
      return redirect(controllers.routes.GroupController.invite(groupId));
    }
    group.temporarySender=currentUser;
    NotificationService.getInstance().createNotification(group,Group.GROUP_INVITATION);
  }
  flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
  return redirect(controllers.routes.GroupController.view(groupId,PAGE));
}","@Transactional public static Result inviteMember(long groupId){
  Group group=Group.findById(groupId);
  Account currentUser=Component.currentAccount();
  if (Secured.inviteMember(group)) {
    DynamicForm form=Form.form().bindFromRequest();
    group.inviteList=form.data().values();
    if (group.inviteList.size() < 1) {
      flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
      return redirect(controllers.routes.GroupController.invite(groupId));
    }
    group.temporarySender=currentUser;
    NotificationService.getInstance().createNotification(group,Group.GROUP_INVITATION);
  }
  flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
  return redirect(controllers.routes.GroupController.view(groupId,PAGE));
}",0.9896907216494846
89025,"public static Result join(long id){
  Account account=Component.currentAccount();
  Group group=Group.findById(id);
  GroupAccount groupAccount;
  if (Secured.isMemberOfGroup(group,account)) {
    Logger.debug(""String_Node_Str"");
    flash(""String_Node_Str"",""String_Node_Str"");
    return redirect(controllers.routes.GroupController.view(id,PAGE));
  }
  groupAccount=GroupAccount.find(account,group);
  if (groupAccount != null && groupAccount.linkType.equals(LinkType.request)) {
    flash(""String_Node_Str"",""String_Node_Str"");
    return redirect(controllers.routes.GroupController.index());
  }
  if (groupAccount != null && groupAccount.linkType.equals(LinkType.reject)) {
    flash(""String_Node_Str"",""String_Node_Str"");
    return redirect(controllers.routes.GroupController.index());
  }
  if (groupAccount != null && groupAccount.linkType.equals(LinkType.invite)) {
    groupAccount.linkType=LinkType.establish;
    groupAccount.update();
    flash(""String_Node_Str"",""String_Node_Str"" + group.title + ""String_Node_Str"");
    return redirect(controllers.routes.GroupController.index());
  }
 else   if (group.groupType.equals(GroupType.open)) {
    groupAccount=new GroupAccount(account,group,LinkType.establish);
    groupAccount.create();
    flash(""String_Node_Str"",""String_Node_Str"" + group.title + ""String_Node_Str"");
    return redirect(controllers.routes.GroupController.view(id,PAGE));
  }
 else   if (group.groupType.equals(GroupType.close)) {
    groupAccount=new GroupAccount(account,group,LinkType.request);
    groupAccount.create();
    group.temporarySender=account;
    NotificationService.getInstance().createNotification(group,Group.GROUP_NEW_REQUEST);
    flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
    return redirect(controllers.routes.GroupController.index());
  }
 else   if (group.groupType.equals(GroupType.course)) {
    return redirect(controllers.routes.GroupController.token(id));
  }
  return redirect(controllers.routes.GroupController.index());
}","@Transactional public static Result join(long id){
  Account account=Component.currentAccount();
  Group group=Group.findById(id);
  GroupAccount groupAccount;
  if (Secured.isMemberOfGroup(group,account)) {
    Logger.debug(""String_Node_Str"");
    flash(""String_Node_Str"",""String_Node_Str"");
    return redirect(controllers.routes.GroupController.view(id,PAGE));
  }
  groupAccount=GroupAccount.find(account,group);
  if (groupAccount != null && groupAccount.linkType.equals(LinkType.request)) {
    flash(""String_Node_Str"",""String_Node_Str"");
    return redirect(controllers.routes.GroupController.index());
  }
  if (groupAccount != null && groupAccount.linkType.equals(LinkType.reject)) {
    flash(""String_Node_Str"",""String_Node_Str"");
    return redirect(controllers.routes.GroupController.index());
  }
  if (groupAccount != null && groupAccount.linkType.equals(LinkType.invite)) {
    groupAccount.linkType=LinkType.establish;
    groupAccount.update();
    flash(""String_Node_Str"",""String_Node_Str"" + group.title + ""String_Node_Str"");
    return redirect(controllers.routes.GroupController.index());
  }
 else   if (group.groupType.equals(GroupType.open)) {
    groupAccount=new GroupAccount(account,group,LinkType.establish);
    groupAccount.create();
    flash(""String_Node_Str"",""String_Node_Str"" + group.title + ""String_Node_Str"");
    return redirect(controllers.routes.GroupController.view(id,PAGE));
  }
 else   if (group.groupType.equals(GroupType.close)) {
    groupAccount=new GroupAccount(account,group,LinkType.request);
    groupAccount.create();
    group.temporarySender=account;
    NotificationService.getInstance().createNotification(group,Group.GROUP_NEW_REQUEST);
    flash(""String_Node_Str"",Messages.get(""String_Node_Str""));
    return redirect(controllers.routes.GroupController.index());
  }
 else   if (group.groupType.equals(GroupType.course)) {
    return redirect(controllers.routes.GroupController.token(id));
  }
  return redirect(controllers.routes.GroupController.index());
}",0.99626400996264
89026,"public static Result invite(long groupId){
  Group group=Group.findById(groupId);
  Navigation.set(Level.GROUPS,""String_Node_Str"",group.title,controllers.routes.GroupController.view(group.id,PAGE));
  return ok(invite.render(group,Friendship.friendsToInvite(Component.currentAccount(),group),GroupAccount.findAccountsByGroup(group,LinkType.invite)));
}","@Transactional public static Result invite(long groupId){
  Group group=Group.findById(groupId);
  Navigation.set(Level.GROUPS,""String_Node_Str"",group.title,controllers.routes.GroupController.view(group.id,PAGE));
  return ok(invite.render(group,Friendship.friendsToInvite(Component.currentAccount(),group),GroupAccount.findAccountsByGroup(group,LinkType.invite)));
}",0.9791376912378305
89027,"public List<ValidationError> validate(){
  List<ValidationError> errors=new ArrayList<ValidationError>();
  if (Group.findByTitle(this.title) != null) {
    errors.add(new ValidationError(""String_Node_Str"",""String_Node_Str""));
    return errors;
  }
  return null;
}","public List<ValidationError> validate(){
  List<ValidationError> errors=new ArrayList<>();
  if (Group.findByTitle(this.title) != null) {
    errors.add(new ValidationError(""String_Node_Str"",""String_Node_Str""));
    return errors;
  }
  return null;
}",0.9709864603481624
89028,"@Override public String getTargetUrl(){
  if (this.type.equals(Group.GROUP_REQUEST_SUCCESS) || this.type.equals(Group.GROUP_NEW_MEDIA) || this.type.equals(Group.GROUP_NEW_REQUEST)) {
    return controllers.routes.GroupController.view(this.id,1).toString();
  }
  return controllers.routes.GroupController.index().toString();
}","@Override public String getTargetUrl(){
  if (this.type.equals(Group.GROUP_REQUEST_SUCCESS) || this.type.equals(Group.GROUP_NEW_MEDIA)) {
    return controllers.routes.GroupController.view(this.id,1).toString();
  }
  return controllers.routes.GroupController.index().toString();
}",0.9258649093904447
89029,"@Override public List<Account> getRecipients(){
  List<Account> recipients=new ArrayList<>();
switch (this.type) {
case Group.GROUP_INVITATION:
    for (    String accountId : inviteList) {
      try {
        final Account account=Account.findById(Long.parseLong(accountId));
        GroupAccount groupAccount=GroupAccount.find(account,this);
        if (!Group.isMember(this,account) && Friendship.alreadyFriendly(this.getSender(),account) && groupAccount == null) {
          final Group thisGroup=this;
          JPA.withTransaction(new F.Callback0(){
            @Override public void invoke() throws Throwable {
              (new GroupAccount(account,thisGroup,LinkType.invite)).create();
            }
          }
);
          recipients.add(account);
        }
      }
 catch (      Throwable t) {
        t.printStackTrace();
      }
    }
  return recipients;
case Group.GROUP_NEW_REQUEST:
return this.getAsAccountList(this.owner);
case Group.GROUP_NEW_MEDIA:
final Group currentGroup=this;
return GroupAccount.findAccountsByGroup(currentGroup,LinkType.establish);
}
return this.temporaryRecipients;
}","@Override public List<Account> getRecipients(){
  List<Account> recipients=new ArrayList<>();
switch (this.type) {
case Group.GROUP_INVITATION:
    for (    String accountId : inviteList) {
      try {
        final Account account=Account.findById(Long.parseLong(accountId));
        GroupAccount groupAccount=GroupAccount.find(account,this);
        if (!Group.isMember(this,account) && Friendship.alreadyFriendly(this.getSender(),account) && groupAccount == null) {
          (new GroupAccount(account,this,LinkType.invite)).create();
          recipients.add(account);
        }
      }
 catch (      Throwable t) {
        t.printStackTrace();
      }
    }
  return recipients;
case Group.GROUP_NEW_REQUEST:
return this.getAsAccountList(this.owner);
case Group.GROUP_NEW_MEDIA:
final Group currentGroup=this;
return GroupAccount.findAccountsByGroup(currentGroup,LinkType.establish);
}
return this.temporaryRecipients;
}",0.905252822778596
89030,"@Override protected String getRuleName(AbstractElement element){
  if (nameMappings == null) {
    nameMappings=new HashMap<AbstractElement,String>(){
      private static final long serialVersionUID=1L;
{
        put(grammarAccess.getKTypeSelectionAccess().getAlternatives_1_1(),""String_Node_Str"");
        put(grammarAccess.getKTypeAccess().getAlternatives_3(),""String_Node_Str"");
        put(grammarAccess.getTypeSignatureAccess().getAlternatives_1_1(),""String_Node_Str"");
        put(grammarAccess.getPropertyValueAccess().getAlternatives(),""String_Node_Str"");
        put(grammarAccess.getFloatAccess().getAlternatives(),""String_Node_Str"");
        put(grammarAccess.getKTypeSelectionAccess().getGroup(),""String_Node_Str"");
        put(grammarAccess.getKTypeSelectionAccess().getGroup_1(),""String_Node_Str"");
        put(grammarAccess.getKTypeSelectionAccess().getGroup_1_1_0(),""String_Node_Str"");
        put(grammarAccess.getKTypeSelectionAccess().getGroup_1_1_1(),""String_Node_Str"");
        put(grammarAccess.getKTypeAccess().getGroup(),""String_Node_Str"");
        put(grammarAccess.getKMethodAccess().getGroup(),""String_Node_Str"");
        put(grammarAccess.getKMethodAccess().getGroup_2(),""String_Node_Str"");
        put(grammarAccess.getKMethodAccess().getGroup_2_1(),""String_Node_Str"");
        put(grammarAccess.getKParameterTypeSignatureAccess().getGroup(),""String_Node_Str"");
        put(grammarAccess.getQualifiedIDAccess().getGroup(),""String_Node_Str"");
        put(grammarAccess.getQualifiedIDAccess().getGroup_1(),""String_Node_Str"");
        put(grammarAccess.getTypeSignatureAccess().getGroup(),""String_Node_Str"");
        put(grammarAccess.getTypeSignatureAccess().getGroup_1(),""String_Node_Str"");
        put(grammarAccess.getKTypeSelectionAccess().getJavaProjectsAssignment_1_1_0_1(),""String_Node_Str"");
        put(grammarAccess.getKTypeSelectionAccess().getBundlesAssignment_1_1_1_1(),""String_Node_Str"");
        put(grammarAccess.getKTypeSelectionAccess().getTypesAssignment_2(),""String_Node_Str"");
        put(grammarAccess.getKTypeAccess().getQualifiedNameAssignment_1(),""String_Node_Str"");
        put(grammarAccess.getKTypeAccess().getFieldsAssignment_3_0(),""String_Node_Str"");
        put(grammarAccess.getKTypeAccess().getMethodsAssignment_3_1(),""String_Node_Str"");
        put(grammarAccess.getKFieldAccess().getNameAssignment(),""String_Node_Str"");
        put(grammarAccess.getKMethodAccess().getNameAssignment_0(),""String_Node_Str"");
        put(grammarAccess.getKMethodAccess().getParameterTypeSignaturesAssignment_2_0(),""String_Node_Str"");
        put(grammarAccess.getKMethodAccess().getParameterTypeSignaturesAssignment_2_1_1(),""String_Node_Str"");
        put(grammarAccess.getKParameterTypeSignatureAccess().getNameAssignment_1(),""String_Node_Str"");
      }
    }
;
  }
  return nameMappings.get(element);
}","@Override protected String getRuleName(AbstractElement element){
  if (nameMappings == null) {
    nameMappings=new HashMap<AbstractElement,String>(){
      private static final long serialVersionUID=1L;
{
        put(grammarAccess.getKClassModelAccess().getAlternatives_1_1(),""String_Node_Str"");
        put(grammarAccess.getKTypeAccess().getAlternatives(),""String_Node_Str"");
        put(grammarAccess.getKClassAccess().getAlternatives_3(),""String_Node_Str"");
        put(grammarAccess.getKInterfaceAccess().getAlternatives_3(),""String_Node_Str"");
        put(grammarAccess.getKEnumAccess().getAlternatives_3(),""String_Node_Str"");
        put(grammarAccess.getTypeSignatureAccess().getAlternatives_1_1(),""String_Node_Str"");
        put(grammarAccess.getPropertyValueAccess().getAlternatives(),""String_Node_Str"");
        put(grammarAccess.getFloatAccess().getAlternatives(),""String_Node_Str"");
        put(grammarAccess.getKClassModelAccess().getGroup(),""String_Node_Str"");
        put(grammarAccess.getKClassModelAccess().getGroup_1(),""String_Node_Str"");
        put(grammarAccess.getKClassModelAccess().getGroup_1_1_0(),""String_Node_Str"");
        put(grammarAccess.getKClassModelAccess().getGroup_1_1_1(),""String_Node_Str"");
        put(grammarAccess.getKPackageAccess().getGroup(),""String_Node_Str"");
        put(grammarAccess.getKClassAccess().getGroup(),""String_Node_Str"");
        put(grammarAccess.getKInterfaceAccess().getGroup(),""String_Node_Str"");
        put(grammarAccess.getKEnumAccess().getGroup(),""String_Node_Str"");
        put(grammarAccess.getKMethodAccess().getGroup(),""String_Node_Str"");
        put(grammarAccess.getKMethodAccess().getGroup_2(),""String_Node_Str"");
        put(grammarAccess.getKMethodAccess().getGroup_2_1(),""String_Node_Str"");
        put(grammarAccess.getQualifiedIDAccess().getGroup(),""String_Node_Str"");
        put(grammarAccess.getQualifiedIDAccess().getGroup_1(),""String_Node_Str"");
        put(grammarAccess.getTypeSignatureAccess().getGroup(),""String_Node_Str"");
        put(grammarAccess.getTypeSignatureAccess().getGroup_1(),""String_Node_Str"");
        put(grammarAccess.getKClassModelAccess().getJavaProjectsAssignment_1_1_0_1(),""String_Node_Str"");
        put(grammarAccess.getKClassModelAccess().getBundlesAssignment_1_1_1_1(),""String_Node_Str"");
        put(grammarAccess.getKClassModelAccess().getPackagesAssignment_2(),""String_Node_Str"");
        put(grammarAccess.getKPackageAccess().getNameAssignment_1(),""String_Node_Str"");
        put(grammarAccess.getKPackageAccess().getTypesAssignment_3(),""String_Node_Str"");
        put(grammarAccess.getKClassAccess().getNameAssignment_1(),""String_Node_Str"");
        put(grammarAccess.getKClassAccess().getFieldsAssignment_3_0(),""String_Node_Str"");
        put(grammarAccess.getKClassAccess().getMethodsAssignment_3_1(),""String_Node_Str"");
        put(grammarAccess.getKInterfaceAccess().getNameAssignment_1(),""String_Node_Str"");
        put(grammarAccess.getKInterfaceAccess().getFieldsAssignment_3_0(),""String_Node_Str"");
        put(grammarAccess.getKInterfaceAccess().getMethodsAssignment_3_1(),""String_Node_Str"");
        put(grammarAccess.getKEnumAccess().getNameAssignment_1(),""String_Node_Str"");
        put(grammarAccess.getKEnumAccess().getFieldsAssignment_3_0(),""String_Node_Str"");
        put(grammarAccess.getKEnumAccess().getMethodsAssignment_3_1(),""String_Node_Str"");
        put(grammarAccess.getKFieldAccess().getNameAssignment(),""String_Node_Str"");
        put(grammarAccess.getKMethodAccess().getNameAssignment_0(),""String_Node_Str"");
        put(grammarAccess.getKMethodAccess().getParametersAssignment_2_0(),""String_Node_Str"");
        put(grammarAccess.getKMethodAccess().getParametersAssignment_2_1_1(),""String_Node_Str"");
        put(grammarAccess.getKTypeReferenceAccess().getSignatureAssignment(),""String_Node_Str"");
      }
    }
;
  }
  return nameMappings.get(element);
}",0.1982515928285672
89031,"@Override protected Collection<FollowElement> getFollowElements(AbstractInternalContentAssistParser parser){
  try {
    de.cau.cs.kieler.klassviz.text.ui.contentassist.antlr.internal.InternalClassDataParser typedParser=(de.cau.cs.kieler.klassviz.text.ui.contentassist.antlr.internal.InternalClassDataParser)parser;
    typedParser.entryRuleKTypeSelection();
    return typedParser.getFollowElements();
  }
 catch (  RecognitionException ex) {
    throw new RuntimeException(ex);
  }
}","@Override protected Collection<FollowElement> getFollowElements(AbstractInternalContentAssistParser parser){
  try {
    de.cau.cs.kieler.klassviz.text.ui.contentassist.antlr.internal.InternalClassDataParser typedParser=(de.cau.cs.kieler.klassviz.text.ui.contentassist.antlr.internal.InternalClassDataParser)parser;
    typedParser.entryRuleKClassModel();
    return typedParser.getFollowElements();
  }
 catch (  RecognitionException ex) {
    throw new RuntimeException(ex);
  }
}",0.9762150982419856
89032,"public static void main(String[] args){
  String picture=""String_Node_Str"";
  Tortoise.getBackgroundWindow().setBackgroundImage(picture);
  Tortoise.show();
  Tortoise.setSpeed(10);
  int side=2;
  for (int i=0; i < 75; i++) {
    Color penColor=PenColors.Yellows.Yellow;
    Tortoise.setPenColor(penColor);
    side=side + 1;
    Tortoise.move(side);
    Tortoise.setX(555);
    Tortoise.setY(65);
    Tortoise.turn(360 / 4);
    Tortoise.turn(1);
  }
}","public static void main(String[] args){
  String picture=""String_Node_Str"";
  Tortoise.getBackgroundWindow().setBackgroundImage(picture);
  Tortoise.show();
  Tortoise.setSpeed(10);
  int side=2;
  for (int i=0; i < 75; i++) {
    Color penColor=PenColors.Yellows.Yellow;
    Tortoise.setPenColor(penColor);
    side=side + 1;
    Tortoise.move(side);
    Tortoise.setX(555);
    Tortoise.setY(65);
    Tortoise.turn(360 / 3);
    Tortoise.turn(1);
  }
}",0.9977973568281938
89033,"/** 
 * Removes everything from the window. <br> <b>Example:</b>   {@code  Tortoise.clearWindow()}
 */
public static void clear(){
  turtle().clear();
}","/** 
 * Removes everything from the window. <br> <b>Example:</b>   {@code  Tortoise.clearWindow()}
 */
public static void clear(){
  turtle().clear();
  turtle().getBackgroundWindow().setTurtle(turtle());
}",0.8491620111731844
89034,"public CanvasPanel add(Paintable painter){
  if (!this.painters.contains(painter)) {
    this.painters.add(painter);
  }
  return this;
}","public CanvasPanel add(Paintable painter){
  if (!this.painters.contains(painter)) {
    this.painters.add(painter);
  }
  this.repaint();
  return this;
}",0.9383561643835616
89035,"public void addTo(TurtleWindow panel){
  panel.getCanvas().add(this);
}","/** 
 * Adds a circle to the window <div><b>Example:</b>   {@code  circle.addTo(panel)}</div>
 * @param panel the ProgramWindow or panel
 */
public void addTo(ProgramWindow panel){
  panel.getCanvas().add(this);
}",0.4577464788732394
89036,"private void createBubble(int x,int y){
  programWindow.removePaintable();
  int radius=NumberUtils.getRandomInt(10,50);
  Circle circle=new Circle(radius,ColorWheel.getNextColor());
  circle.setCenter(x,y);
  circle.addTo(programWindow);
}","private void createBubble(int x,int y){
  programWindow.clearWindow();
  int radius=NumberUtils.getRandomInt(10,50);
  Circle circle=new Circle(radius,ColorWheel.getNextColor());
  circle.setCenter(x,y);
  circle.addTo(programWindow);
}",0.9453781512605042
89037,"public TortoiseMaze(){
  Tortoise.setSpeed(10);
  Tortoise.setPenColor(PenColors.Greens.Green);
  Tortoise.setPenWidth(4);
  ImageIcon leftArrow=new ImageIcon(""String_Node_Str"");
  JButton leftButton=new JButton(leftArrow);
  Tortoise.getBackgroundWindow().addButton(leftButton);
  ImageIcon upArrow=new ImageIcon(""String_Node_Str"");
  JButton upButton=new JButton(upArrow);
  Tortoise.getBackgroundWindow().addButton(upButton);
  ImageIcon rightArrow=new ImageIcon(""String_Node_Str"");
  JButton rightButton=new JButton(rightArrow);
  Tortoise.getBackgroundWindow().addButton(rightButton);
  rightButton.addActionListener(new ActionListener(){
    @Override public void actionPerformed(    ActionEvent e){
      Tortoise.turn(90);
    }
  }
);
  leftButton.addActionListener(new ActionListener(){
    @Override public void actionPerformed(    ActionEvent e){
      Tortoise.turn(-90);
    }
  }
);
  upButton.addActionListener(new ActionListener(){
    @Override public void actionPerformed(    ActionEvent e){
      Tortoise.move(25);
    }
  }
);
  Tortoise.setVisible(true);
}","public TortoiseMaze(){
  Tortoise.show();
  Tortoise.setSpeed(10);
  Tortoise.setPenColor(PenColors.Greens.Green);
  Tortoise.setPenWidth(4);
  ImageIcon leftArrow=new ImageIcon(""String_Node_Str"");
  JButton leftButton=new JButton(leftArrow);
  Tortoise.getBackgroundWindow().addButton(leftButton);
  ImageIcon upArrow=new ImageIcon(""String_Node_Str"");
  JButton upButton=new JButton(upArrow);
  Tortoise.getBackgroundWindow().addButton(upButton);
  ImageIcon rightArrow=new ImageIcon(""String_Node_Str"");
  JButton rightButton=new JButton(rightArrow);
  Tortoise.getBackgroundWindow().addButton(rightButton);
  rightButton.addActionListener(new ActionListener(){
    @Override public void actionPerformed(    ActionEvent e){
      Tortoise.turn(90);
    }
  }
);
  leftButton.addActionListener(new ActionListener(){
    @Override public void actionPerformed(    ActionEvent e){
      Tortoise.turn(-90);
    }
  }
);
  upButton.addActionListener(new ActionListener(){
    @Override public void actionPerformed(    ActionEvent e){
      Tortoise.move(25);
    }
  }
);
  Tortoise.setVisible(true);
}",0.9912723932016536
89038,"@Test public void dividing() throws Exception {
  Number number=1.0 / 5;
  Assert.assertEquals(number.getClass(),______.class);
  Assert.assertEquals(number,.2);
}","@Test public void dividing() throws Exception {
  Number number=1.0 / 5;
  Assert.assertEquals(number.getClass(),______.class);
  Assert.assertEquals(number,___);
}",0.9847094801223242
89039,"public static void testFrame(JFrame frame,WindowAdapter... array){
  frame.pack();
  for (  WindowAdapter closer : array) {
    frame.addWindowListener(closer);
  }
  WindowUtils.centerWindow(frame);
  frame.setVisible(true);
}","public static void testFrame(JFrame frame,WindowAdapter... array){
  frame.pack();
  for (  WindowAdapter closer : array) {
    frame.addWindowListener(closer);
  }
  WindowUtils.centerWindow(frame);
}",0.9392523364485982
89040,"/** 
 * This method returns the a random color from the options on the ColorWheel. <br> <b>Example:</b>   {@code  Color color = ColorWheel.getNextColor();}
 * @return A random color from the ColorWheel
 */
public static Color getRandomColorFromWheel(){
  return wheel.getRandomFrom();
}","/** 
 * This method returns the a random color from the options on the ColorWheel. <br> <b>Example:</b>   {@code  Color color = ColorWheel.getNextColor();}
 * @return A random color from the ColorWheel
 */
public static Color getRandomColorFromWheel(){
  assertNonEmpty();
  return wheel.getRandomFrom();
}",0.8783783783783784
89041,"/** 
 * This method returns the next color of the ColorWheel. <br> <b>Example:</b>   {@code  Color color = ColorWheel.getNextColor();}
 * @return the next color of the ColorWheel
 */
public static Color getNextColor(){
  return wheel.next();
}","/** 
 * This method returns the next color of the ColorWheel. <br> <b>Example:</b>   {@code  Color color = ColorWheel.getNextColor();}
 * @return the next color of the ColorWheel
 */
public static Color getNextColor(){
  assertNonEmpty();
  return wheel.next();
}",0.9604743083003952
89042,"/** 
 * Adds a turtle instance to a window  NOTE: this method must be called BEFORE calling any other methods on turtle instances <p><b>Example:</b>   {@code multiTurtleWindow.addTurtle(myTurtle)}</p>
 * @param turtle A turtle instance
 */
public void addTurtle(Turtle turtle){
  if (turtle == null) {
    return;
  }
  this.turtles.add(turtle);
  turtle.setPanel(this);
}","/** 
 * Adds a turtle instance to a window  NOTE: this method must be called BEFORE calling any other methods on turtle instances <p><b>Example:</b>   {@code multiTurtleWindow.addTurtle(myTurtle)}</p>
 * @param turtle A turtle instance
 */
public void addTurtle(Turtle turtle){
  if (turtle == null) {
    return;
  }
  this.turtles.add(turtle);
  turtle.setFrame(this.getFrame());
  turtle.setPanel(this);
}",0.953846153846154
89043,"public void show(){
  hidden=false;
  refreshPanel();
}","public void show(){
  hidden=false;
  Component p=getPanel();
  this.setFrameVisible(true);
  this.setPanelVisible(true);
  refreshPanel(p);
}",0.5583756345177665
89044,"private Component getPanel(){
  if (panel == null) {
    String title=""String_Node_Str"";
    panel=new TurtlePanel();
    if (speed != TEST_SPEED) {
      JFrame frame=new JFrame(title);
      frame.getContentPane().add(panel);
      ProgramWindow.createStandardFrame(frame);
    }
    panel.setTurtle(this);
  }
  return panel;
}","private Component getPanel(){
  if (panel == null) {
    String title=""String_Node_Str"";
    panel=new TurtlePanel();
    if (speed != TEST_SPEED) {
      frame=new JFrame(title);
      frame.getContentPane().add(panel);
      ProgramWindow.createStandardFrame(frame);
    }
    panel.setTurtle(this);
  }
  return panel;
}",0.9892802450229708
89045,"private void assertNonEmpty(){
  if (list.isEmpty()) {
    String message=""String_Node_Str"";
    throw new RuntimeException(message);
  }
}","protected void assertNonEmpty(){
  if (list.isEmpty()) {
    String message=""String_Node_Str"";
    throw new RuntimeException(message);
  }
}",0.9714285714285714
89046,"public static void addButton(JPanel panel,JButton button){
  panel.add(button);
}","public void addButton(JButton button){
  this.add(button);
}",0.7943262411347518
89047,"public static void main(String[] args) throws Exception {
  Tortoise.setX(150);
  Tortoise.drawShape(4,PenColors.Reds.Red,75,4);
  Tortoise.setX(425);
  Tortoise.drawShape(6,PenColors.Blues.Blue,65,40);
  Tortoise.setX(250);
  Tortoise.setY(375);
  Random r=new Random();
  int sides=r.nextInt(10) + 1;
  Tortoise.drawShape(sides,PenColors.Purples.Purple,50,10);
}","public static void main(String[] args) throws Exception {
  Tortoise.show();
  Tortoise.setX(150);
  Tortoise.drawShape(4,PenColors.Reds.Red,75,4);
  Tortoise.setX(425);
  Tortoise.drawShape(6,PenColors.Blues.Blue,65,40);
  Tortoise.setX(250);
  Tortoise.setY(375);
  Random r=new Random();
  int sides=r.nextInt(10) + 1;
  Tortoise.drawShape(sides,PenColors.Purples.Purple,50,10);
}",0.9745649263721552
89048,"public static void main(String[] args) throws Exception {
  Tortoise.drawTortoise();
}","public static void main(String[] args) throws Exception {
  Tortoise.show();
  Tortoise.drawTortoise();
}",0.900523560209424
89049,"@Override public void paint(Graphics g){
  Graphics2D g2d=ProgramWindow.configureGraphics2D(g);
  paintLines(g2d);
  paintTurtle(g2d);
  g2d.dispose();
}","@Override public void paint(Graphics g){
  super.paint(g);
  Graphics2D g2d=ProgramWindow.configureGraphics2D(g);
  paintLines(g2d);
  paintTurtle(g2d);
  g2d.dispose();
}",0.9444444444444444
89050,"private boolean grade3Lime(){
  ColorWheel.removeAllColors();
  quiz.question3();
  return getSafeColor() == Colors.Greens.Lime;
}","private boolean grade3Lime(){
  ColorWheel.removeAllColors();
  quiz.question3();
  return Colors.Greens.Lime.equals(getSafeColor());
}",0.8452830188679246
89051,"private boolean grade4Red(){
  ColorWheel.removeAllColors();
  quiz.question4();
  return getSafeColor() == Colors.Reds.Red;
}","private boolean grade4Red(){
  ColorWheel.removeAllColors();
  quiz.question4();
  return Colors.Reds.Red.equals(getSafeColor());
}",0.8404669260700389
89052,"public Color getSafeColor(){
  try {
    return ColorWheel.getNextColor();
  }
 catch (  Exception e) {
    return null;
  }
}","public Color getSafeColor(){
  try {
    return ColorWheel.getNextColor();
  }
 catch (  Exception e) {
    return Colors.Yellows.Yellow;
  }
}",0.9219330855018588
89053,"public void question3(String templateText,Object model){
}","public void question3(String templateText,Object model){
  word3=Parser.parse(templateText,model);
}",0.7341772151898734
89054,"public void question2(String letter1){
}","public void question2(String letter1){
  word2=word2 + letter1;
}",0.7619047619047619
89055,"public void question4(Pieces pieces){
}","public void question4(Pieces pieces){
  template4=""String_Node_Str"";
}",0.7155963302752294
89056,"public void question1(String letter1,String letter3){
}","public void question1(String letter1,String letter3){
  word1=letter1 + 'o' + letter3;
}",0.7692307692307693
89057,"@Test public void feedTheNinja() throws Exception {
  Tortoise michealangelo=new Tortoise();
  michealangelo.likesTopping(Topping.Pepperoni);
  Pizza pizza=new Pizza();
  pizza.addTopping(_________);
  boolean likedIt=michealangelo.eatPizza(pizza);
  Assert.assertTrue(""String_Node_Str"",likedIt);
}","@Test public void feedTheNinja() throws Exception {
  Tortoise michelangelo=new Tortoise();
  michelangelo.likesTopping(Topping.Pepperoni);
  Pizza pizza=new Pizza();
  pizza.addTopping(_________);
  boolean likedIt=michelangelo.eatPizza(pizza);
  Assert.assertTrue(""String_Node_Str"",likedIt);
}",0.9949409780775716
89058,"@Test public void twoTortoisesYetAgain() throws Exception {
  Tortoise rafael=new Tortoise();
  Tortoise michealangelo=new Tortoise();
  Tortoise anonymousNinja=________;
  boolean result=michealangelo.equals(anonymousNinja);
  Assert.assertEquals(true,result);
}","@Test public void twoTortoisesYetAgain() throws Exception {
  Tortoise rafael=new Tortoise();
  Tortoise michelangelo=new Tortoise();
  Tortoise anonymousNinja=________;
  boolean result=michelangelo.equals(anonymousNinja);
  Assert.assertEquals(true,result);
}",0.9961832061068704
89059,"private boolean grade1You(){
  quiz.word1=""String_Node_Str"";
  quiz.question1(""String_Node_Str"",""String_Node_Str"");
  return ""String_Node_Str"".equals(quiz.word1);
}","private boolean grade1You(){
  quiz.currentWord1=""String_Node_Str"";
  quiz.question1(""String_Node_Str"",""String_Node_Str"");
  return ""String_Node_Str"".equals(quiz.currentWord1);
}",0.9473684210526316
89060,"public static void main(String[] args){
}","public static void main(String[] args){
  Tortoise.explode();
  Tortoise.show();
  Tortoise.move(50);
  Tortoise.getBackgroundWindow().setCursor(Cursor.CROSSHAIR_CURSOR);
}",0.3849765258215962
89061,"public static void main(String[] args){
  if (args.length != 1) {
    System.err.println(""String_Node_Str"");
    System.err.println(""String_Node_Str"" + StaticAnalyser.class.getName() + ""String_Node_Str"");
    System.exit(1);
  }
  G.v().out=System.err;
  Options.v().set_src_prec(Options.src_prec_apk);
  Options.v().set_process_dir(Collections.singletonList(args[0]));
  Options.v().set_output_format(Options.output_format_none);
  Options.v().set_force_android_jar(""String_Node_Str"");
  Options.v().set_allow_phantom_refs(true);
  Options.v().set_whole_program(true);
  PhaseOptions.v().processPhaseOptions(""String_Node_Str"",""String_Node_Str"");
  List<Vulnerability> vulnerabilities=new ArrayList<>();
  Pack wjpp=PackManager.v().getPack(""String_Node_Str"");
  wjpp.add(new Transform(""String_Node_Str"",new EntryMethodTransformer()));
  AnalysisTransformer transformer=new AnalysisTransformer(new KnownHostnameVerifierAnalyser(vulnerabilities),new HostnameVerifierAnalyser(vulnerabilities),new AbstractVerifierAnalyser(vulnerabilities),new DefaultHostnameVerifierAnalyser(vulnerabilities),new HttpsUrlConnectionAnalyser(vulnerabilities),new TrustManagerAnalyser(vulnerabilities),new SslContextAnalyser(vulnerabilities));
  Pack wjtp=PackManager.v().getPack(""String_Node_Str"");
  wjtp.add(new Transform(""String_Node_Str"",transformer));
  Scene.v().loadNecessaryClasses();
  PackManager.v().runPacks();
  System.err.println(vulnerabilities.size() + ""String_Node_Str"");
  System.err.flush();
  for (  Vulnerability vulnerability : vulnerabilities) {
    System.out.println(vulnerability.toString());
  }
}","public static void main(String[] args){
  if (args.length != 1) {
    System.err.println(""String_Node_Str"");
    System.err.println(""String_Node_Str"" + StaticAnalyser.class.getName() + ""String_Node_Str"");
    System.exit(1);
  }
  G.v().out=System.err;
  Options.v().set_src_prec(Options.src_prec_apk);
  Options.v().set_process_dir(Collections.singletonList(args[0]));
  Options.v().set_output_format(Options.output_format_none);
  Options.v().set_force_android_jar(""String_Node_Str"");
  Options.v().set_allow_phantom_refs(true);
  Options.v().set_whole_program(true);
  PhaseOptions.v().processPhaseOptions(""String_Node_Str"",""String_Node_Str"");
  Set<Vulnerability> vulnerabilities=new HashSet<>();
  Pack wjpp=PackManager.v().getPack(""String_Node_Str"");
  wjpp.add(new Transform(""String_Node_Str"",new EntryMethodTransformer()));
  AnalysisTransformer transformer=new AnalysisTransformer(vulnerabilities,new KnownHostnameVerifierAnalyser(vulnerabilities),new HostnameVerifierAnalyser(vulnerabilities),new AbstractVerifierAnalyser(vulnerabilities),new DefaultHostnameVerifierAnalyser(vulnerabilities),new HttpsUrlConnectionAnalyser(vulnerabilities),new TrustManagerAnalyser(vulnerabilities),new SslContextAnalyser(vulnerabilities));
  Pack wjtp=PackManager.v().getPack(""String_Node_Str"");
  wjtp.add(new Transform(""String_Node_Str"",transformer));
  Scene.v().loadNecessaryClasses();
  PackManager.v().runPacks();
  System.err.println(vulnerabilities.size() + ""String_Node_Str"");
  System.err.flush();
  for (  Vulnerability vulnerability : vulnerabilities) {
    System.out.println(vulnerability.toString());
  }
}",0.989120298414672
89062,"public AbstractVerifierAnalyser(List<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}","public AbstractVerifierAnalyser(Set<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}",0.9740932642487048
89063,"public Analyser(List<Vulnerability> vulnerabilities){
  this.vulnerabilities=vulnerabilities;
}","public Analyser(Set<Vulnerability> vulnerabilities){
  this.vulnerabilities=vulnerabilities;
}",0.9735449735449736
89064,"@Override protected void internalTransform(String phase,Map<String,String> options){
  for (  Analyser analyser : analysers) {
    analyser.analyse();
  }
}","@Override protected void internalTransform(String phase,Map<String,String> options){
  int size;
  do {
    size=vulnerabilities.size();
    for (    Analyser analyser : analysers) {
      analyser.analyse();
    }
  }
 while (size != vulnerabilities.size());
}",0.7482014388489209
89065,"public AnalysisTransformer(Analyser... analysers){
  this.analysers=analysers;
}","public AnalysisTransformer(Set<Vulnerability> vulnerabilities,Analyser... analysers){
  this.vulnerabilities=vulnerabilities;
  this.analysers=analysers;
}",0.6808510638297872
89066,"public DefaultHostnameVerifierAnalyser(List<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}","public DefaultHostnameVerifierAnalyser(Set<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}",0.9758454106280192
89067,"public HostnameVerifierAnalyser(List<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}","public HostnameVerifierAnalyser(Set<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}",0.9740932642487048
89068,"public HttpsUrlConnectionAnalyser(List<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}","public HttpsUrlConnectionAnalyser(Set<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}",0.9746192893401016
89069,"public IntraProceduralAnalyser(List<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}","public IntraProceduralAnalyser(Set<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}",0.9738219895287958
89070,"public KnownHostnameVerifierAnalyser(List<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}","public KnownHostnameVerifierAnalyser(Set<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}",0.9753694581280788
89071,"public SslContextAnalyser(List<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}","public SslContextAnalyser(Set<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}",0.9723756906077348
89072,"public TrustManagerAnalyser(List<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}","public TrustManagerAnalyser(Set<Vulnerability> vulnerabilities){
  super(vulnerabilities);
}",0.972972972972973
89073,"@Override public void run(){
  try {
    InetSocketAddress sourceAddr=(InetSocketAddress)socket.getRemoteSocketAddress();
    DestinationFinder destinationFinder=server.getDestinationFinder();
    InetSocketAddress addr=destinationFinder.getDestination(socket);
    InetAddress ip=addr.getAddress();
    int port=addr.getPort();
    logger.info(""String_Node_Str"" + sourceAddr + ""String_Node_Str""+ addr);
    boolean loopback=ip.isLoopbackAddress() || ip.isAnyLocalAddress();
    boolean portMatches=port == server.getServerSocket().getLocalPort();
    for (Enumeration<NetworkInterface> it=NetworkInterface.getNetworkInterfaces(); it.hasMoreElements(); ) {
      NetworkInterface dev=it.nextElement();
      for (Enumeration<InetAddress> it0=dev.getInetAddresses(); it0.hasMoreElements(); ) {
        InetAddress devAddr=it0.nextElement();
        if (ip.equals(devAddr)) {
          loopback=true;
        }
      }
    }
    if (loopback && portMatches) {
      logger.warning(""String_Node_Str"");
      socket.close();
      return;
    }
    logger.info(""String_Node_Str"" + ip.getHostAddress() + ""String_Node_Str""+ port+ ""String_Node_Str"");
    SocketFactory factory=server.getSocketFactory();
    SSLSocket secureOther=factory.openSslSocket(sourceAddr,addr);
    boolean ssl;
    Socket other;
    try {
      secureOther.startHandshake();
      ssl=true;
      other=secureOther;
    }
 catch (    IOException ex) {
      logger.log(Level.WARNING,""String_Node_Str"",ex);
      ssl=false;
      other=factory.openSocket(sourceAddr,addr);
    }
    IoCopyRunnable clientToServerCopier, serverToClientCopier;
    if (ssl) {
      CertificateCache certificateCache=server.getCertificateCache();
      X509Certificate[] fakeChain=certificateCache.getChain(server,secureOther);
      MitmKeyManager keyManager=new MitmKeyManager(server.getPrivateKey(),fakeChain);
      SSLSocket secureSocket=createSecureSocket(socket,keyManager);
      secureSocket.setUseClientMode(false);
      HostnameSniMatcher sniMatcher=new HostnameSniMatcher(server,keyManager,sourceAddr,addr);
      SSLParameters params=secureSocket.getSSLParameters();
      params.setSNIMatchers(Arrays.<SNIMatcher>asList(sniMatcher));
      secureSocket.setSSLParameters(params);
      secureSocket.startHandshake();
      if (sniMatcher.isSniSupported()) {
        logger.info(""String_Node_Str"");
        other=sniMatcher.getSniSocket();
      }
      clientToServerCopier=new IoCopyRunnable(secureSocket.getInputStream(),other.getOutputStream());
      serverToClientCopier=new IoCopyRunnable(other.getInputStream(),secureSocket.getOutputStream());
    }
 else {
      clientToServerCopier=new IoCopyRunnable(socket.getInputStream(),other.getOutputStream());
      serverToClientCopier=new IoCopyRunnable(other.getInputStream(),socket.getOutputStream());
    }
    Executor executor=server.getExecutor();
    executor.execute(clientToServerCopier);
    executor.execute(serverToClientCopier);
  }
 catch (  IOException ex) {
    logger.log(Level.WARNING,""String_Node_Str"",ex);
  }
}","@Override public void run(){
  try {
    InetSocketAddress sourceAddr=(InetSocketAddress)socket.getRemoteSocketAddress();
    DestinationFinder destinationFinder=server.getDestinationFinder();
    InetSocketAddress addr=destinationFinder.getDestination(socket);
    InetAddress ip=addr.getAddress();
    int port=addr.getPort();
    logger.info(""String_Node_Str"" + sourceAddr + ""String_Node_Str""+ addr);
    boolean loopback=ip.isLoopbackAddress() || ip.isAnyLocalAddress();
    boolean portMatches=port == server.getServerSocket().getLocalPort();
    for (Enumeration<NetworkInterface> it=NetworkInterface.getNetworkInterfaces(); it.hasMoreElements(); ) {
      NetworkInterface dev=it.nextElement();
      for (Enumeration<InetAddress> it0=dev.getInetAddresses(); it0.hasMoreElements(); ) {
        InetAddress devAddr=it0.nextElement();
        if (ip.equals(devAddr)) {
          loopback=true;
        }
      }
    }
    if (loopback && portMatches) {
      logger.warning(""String_Node_Str"");
      socket.close();
      return;
    }
    logger.info(""String_Node_Str"" + ip.getHostAddress() + ""String_Node_Str""+ port+ ""String_Node_Str"");
    SocketFactory factory=server.getSocketFactory();
    SSLSocket secureOther=factory.openSslSocket(sourceAddr,addr);
    boolean ssl;
    Socket other;
    try {
      secureOther.startHandshake();
      ssl=true;
      other=secureOther;
    }
 catch (    IOException ex) {
      logger.log(Level.WARNING,""String_Node_Str"",ex);
      ssl=false;
      other=factory.openSocket(sourceAddr,addr);
    }
    IoCopyRunnable clientToServerCopier, serverToClientCopier;
    if (ssl) {
      CertificateCache certificateCache=server.getCertificateCache();
      X509Certificate[] fakeChain=certificateCache.getChain(server,secureOther);
      MitmKeyManager keyManager=new MitmKeyManager(server.getPrivateKey(),fakeChain);
      SSLSocket secureSocket=createSecureSocket(socket,keyManager);
      secureSocket.setUseClientMode(false);
      HostnameSniMatcher sniMatcher=new HostnameSniMatcher(server,keyManager,sourceAddr,addr);
      SSLParameters params=secureSocket.getSSLParameters();
      params.setSNIMatchers(Arrays.<SNIMatcher>asList(sniMatcher));
      secureSocket.setSSLParameters(params);
      secureSocket.startHandshake();
      if (sniMatcher.isSniSupported()) {
        logger.info(""String_Node_Str"");
        other=sniMatcher.getSniSocket();
      }
      clientToServerCopier=new IoCopyRunnable(secureSocket.getInputStream(),SocketUtils.getOutputStream(other));
      serverToClientCopier=new IoCopyRunnable(other.getInputStream(),SocketUtils.getOutputStream(secureSocket));
    }
 else {
      clientToServerCopier=new IoCopyRunnable(socket.getInputStream(),SocketUtils.getOutputStream(other));
      serverToClientCopier=new IoCopyRunnable(other.getInputStream(),SocketUtils.getOutputStream(socket));
    }
    Executor executor=server.getExecutor();
    executor.execute(clientToServerCopier);
    executor.execute(serverToClientCopier);
  }
 catch (  IOException ex) {
    logger.log(Level.WARNING,""String_Node_Str"",ex);
  }
}",0.983702737940026
89074,"private SSLSocket createSecureSocket(Socket socket,MitmKeyManager keyManager) throws IOException {
  String host=socket.getInetAddress().getHostAddress();
  int port=socket.getPort();
  try {
    SSLContext ctx=SSLContext.getInstance(""String_Node_Str"");
    ctx.init(new KeyManager[]{keyManager},null,null);
    return (SSLSocket)ctx.getSocketFactory().createSocket(socket,host,port,true);
  }
 catch (  NoSuchAlgorithmException|KeyManagementException ex) {
    throw new IOException(ex);
  }
}","private SSLSocket createSecureSocket(Socket socket,MitmKeyManager keyManager) throws IOException {
  String host=socket.getInetAddress().getHostAddress();
  int port=socket.getPort();
  try {
    SSLContext ctx=SSLContext.getInstance(""String_Node_Str"");
    ctx.init(new KeyManager[]{keyManager},null,null);
    SSLSocket sslSocket=(SSLSocket)ctx.getSocketFactory().createSocket(socket,host,port,true);
    SocketUtils.fixSslOutputStream(sslSocket);
    return sslSocket;
  }
 catch (  NoSuchAlgorithmException|KeyManagementException ex) {
    throw new IOException(ex);
  }
}",0.908411214953271
89075,"public final SSLSocket openSslSocket(InetSocketAddress source,InetSocketAddress destination,String host) throws IOException {
  Socket raw=openSocket(source,destination);
  return (SSLSocket)sslSocketFactory.createSocket(raw,host,destination.getPort(),true);
}","public final SSLSocket openSslSocket(InetSocketAddress source,InetSocketAddress destination,String host) throws IOException {
  Socket raw=openSocket(source,destination);
  SSLSocket socket=(SSLSocket)sslSocketFactory.createSocket(raw,host,destination.getPort(),true);
  SocketUtils.fixSslOutputStream(socket);
  return socket;
}",0.8590831918505942
89076,"public static void main(String[] args){
  if (args.length != 1) {
    System.err.println(""String_Node_Str"");
    System.err.println(""String_Node_Str"" + StaticAnalyser.class.getName() + ""String_Node_Str"");
    System.exit(1);
  }
  G.v().out=System.err;
  Options.v().set_src_prec(Options.src_prec_apk);
  Options.v().set_process_dir(Arrays.asList(args[0]));
  Options.v().set_output_format(Options.output_format_none);
  Options.v().set_android_jars(""String_Node_Str"");
  Options.v().set_allow_phantom_refs(true);
  Options.v().set_whole_program(true);
  PhaseOptions.v().processPhaseOptions(""String_Node_Str"",""String_Node_Str"");
  List<Vulnerability> vulnerabilities=new ArrayList<>();
  Pack wjpp=PackManager.v().getPack(""String_Node_Str"");
  wjpp.add(new Transform(""String_Node_Str"",new EntryMethodTransformer()));
  AnalysisTransformer transformer=new AnalysisTransformer(new KnownHostnameVerifierAnalyser(vulnerabilities),new HostnameVerifierAnalyser(vulnerabilities),new AbstractVerifierAnalyser(vulnerabilities),new DefaultHostnameVerifierAnalyser(vulnerabilities),new HttpsUrlConnectionAnalyser(vulnerabilities),new TrustManagerAnalyser(vulnerabilities),new SslContextAnalyser(vulnerabilities));
  Pack wjtp=PackManager.v().getPack(""String_Node_Str"");
  wjtp.add(new Transform(""String_Node_Str"",transformer));
  Scene.v().loadNecessaryClasses();
  PackManager.v().runPacks();
  for (  Vulnerability vulnerability : vulnerabilities) {
    System.out.println(vulnerability.toString());
  }
}","public static void main(String[] args){
  if (args.length != 1) {
    System.err.println(""String_Node_Str"");
    System.err.println(""String_Node_Str"" + StaticAnalyser.class.getName() + ""String_Node_Str"");
    System.exit(1);
  }
  G.v().out=System.err;
  Options.v().set_src_prec(Options.src_prec_apk);
  Options.v().set_process_dir(Arrays.asList(args[0]));
  Options.v().set_output_format(Options.output_format_none);
  Options.v().set_force_android_jar(""String_Node_Str"");
  Options.v().set_allow_phantom_refs(true);
  Options.v().set_whole_program(true);
  PhaseOptions.v().processPhaseOptions(""String_Node_Str"",""String_Node_Str"");
  List<Vulnerability> vulnerabilities=new ArrayList<>();
  Pack wjpp=PackManager.v().getPack(""String_Node_Str"");
  wjpp.add(new Transform(""String_Node_Str"",new EntryMethodTransformer()));
  AnalysisTransformer transformer=new AnalysisTransformer(new KnownHostnameVerifierAnalyser(vulnerabilities),new HostnameVerifierAnalyser(vulnerabilities),new AbstractVerifierAnalyser(vulnerabilities),new DefaultHostnameVerifierAnalyser(vulnerabilities),new HttpsUrlConnectionAnalyser(vulnerabilities),new TrustManagerAnalyser(vulnerabilities),new SslContextAnalyser(vulnerabilities));
  Pack wjtp=PackManager.v().getPack(""String_Node_Str"");
  wjtp.add(new Transform(""String_Node_Str"",transformer));
  Scene.v().loadNecessaryClasses();
  PackManager.v().runPacks();
  for (  Vulnerability vulnerability : vulnerabilities) {
    System.out.println(vulnerability.toString());
  }
}",0.9976658886295432
89077,"@Override protected void internalTransform(String phase,Map<String,String> options){
  for (  SootClass clazz : Scene.v().getApplicationClasses()) {
    Scene.v().getEntryPoints().addAll(clazz.getMethods());
  }
}","@Override protected void internalTransform(String phase,Map<String,String> options){
  for (  SootClass clazz : Scene.v().getApplicationClasses()) {
    for (    SootMethod method : clazz.getMethods()) {
      if (method.isConcrete()) {
        Scene.v().getEntryPoints().add(method);
      }
    }
  }
}",0.7388781431334622
89078,"/** 
 * On update of event execution information behaviour\
 * @param childAssocRef child association reference
 * @param isNewNode     true if a new node, false otherwise
 */
@SuppressWarnings(""String_Node_Str"") public void eventExecutionUpdate(ChildAssociationRef childAssocRef,boolean isNewNode){
  NodeRef dispositionAction=childAssocRef.getParentRef();
  NodeRef eventExecution=childAssocRef.getChildRef();
  if (nodeService.exists(dispositionAction) && nodeService.exists(eventExecution)) {
    ChildAssociationRef assoc=nodeService.getPrimaryParent(dispositionAction);
    if (assoc.getTypeQName().equals(ASSOC_NEXT_DISPOSITION_ACTION)) {
      NodeRef record=assoc.getParentRef();
      applySearchAspect(record);
      Collection<String> events=(List<String>)nodeService.getProperty(record,PROP_RS_DISPOSITION_EVENTS);
      if (events == null) {
        events=new ArrayList<String>(1);
      }
      events.add((String)nodeService.getProperty(eventExecution,PROP_EVENT_EXECUTION_NAME));
      nodeService.setProperty(record,PROP_RS_DISPOSITION_EVENTS,(Serializable)events);
    }
  }
}","/** 
 * On update of event execution information behaviour\
 * @param childAssocRef child association reference
 * @param isNewNode     true if a new node, false otherwise
 */
@SuppressWarnings(""String_Node_Str"") public void eventExecutionUpdate(ChildAssociationRef childAssocRef,boolean isNewNode){
  NodeRef dispositionAction=childAssocRef.getParentRef();
  NodeRef eventExecution=childAssocRef.getChildRef();
  if (nodeService.exists(dispositionAction) && nodeService.exists(eventExecution)) {
    ChildAssociationRef assoc=nodeService.getPrimaryParent(dispositionAction);
    if (assoc.getTypeQName().equals(ASSOC_NEXT_DISPOSITION_ACTION)) {
      NodeRef record=assoc.getParentRef();
      applySearchAspect(record);
      Collection<String> events=(Collection<String>)nodeService.getProperty(record,PROP_RS_DISPOSITION_EVENTS);
      if (events == null) {
        events=new ArrayList<>(1);
      }
      events.add((String)nodeService.getProperty(eventExecution,PROP_EVENT_EXECUTION_NAME));
      nodeService.setProperty(record,PROP_RS_DISPOSITION_EVENTS,(Serializable)events);
    }
  }
}",0.9908759124087592
89079,"/** 
 * @param reportsJSON
 */
public void setReportsJSON(String reportsJSON){
  try {
    JSONArray jsonArray=new JSONArray(reportsJSON);
    if (jsonArray != null) {
      for (int i=0; i < jsonArray.length(); i++) {
        JSONObject report=jsonArray.getJSONObject(i);
        if (!report.has(SavedSearchDetails.NAME)) {
          throw new AlfrescoRuntimeException(""String_Node_Str"" + reportsJSON);
        }
        String name=report.getString(SavedSearchDetails.NAME);
        String translatedName=I18NUtil.getMessage(name);
        if (translatedName != null) {
          name=translatedName;
        }
        if (!report.has(SavedSearchDetails.SEARCH)) {
          throw new AlfrescoRuntimeException(""String_Node_Str"" + name + ""String_Node_Str""+ reportsJSON);
        }
        String query=report.getString(SavedSearchDetails.SEARCH);
        String description=""String_Node_Str"";
        if (report.has(SavedSearchDetails.DESCRIPTION)) {
          description=report.getString(SavedSearchDetails.DESCRIPTION);
          String translatedDescription=I18NUtil.getMessage(description);
          if (translatedDescription != null) {
            description=translatedDescription;
          }
        }
        RecordsManagementSearchParameters searchParameters=new RecordsManagementSearchParameters();
        if (report.has(""String_Node_Str"")) {
          searchParameters=RecordsManagementSearchParameters.createFromJSON(report.getJSONObject(""String_Node_Str""),namespaceService);
        }
        ReportDetails reportDetails=new ReportDetails(name,description,query,searchParameters);
        reports.add(reportDetails);
      }
    }
  }
 catch (  JSONException exception) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + reportsJSON,exception);
  }
}","/** 
 * @param reportsJSON
 */
public void setReportsJSON(String reportsJSON){
  try {
    JSONArray jsonArray=new JSONArray(reportsJSON);
    for (int i=0; i < jsonArray.length(); i++) {
      JSONObject report=jsonArray.getJSONObject(i);
      if (!report.has(SavedSearchDetails.NAME)) {
        throw new AlfrescoRuntimeException(""String_Node_Str"" + reportsJSON);
      }
      String name=report.getString(SavedSearchDetails.NAME);
      String translatedName=I18NUtil.getMessage(name);
      if (translatedName != null) {
        name=translatedName;
      }
      if (!report.has(SavedSearchDetails.SEARCH)) {
        throw new AlfrescoRuntimeException(""String_Node_Str"" + name + ""String_Node_Str""+ reportsJSON);
      }
      String query=report.getString(SavedSearchDetails.SEARCH);
      String description=""String_Node_Str"";
      if (report.has(SavedSearchDetails.DESCRIPTION)) {
        description=report.getString(SavedSearchDetails.DESCRIPTION);
        String translatedDescription=I18NUtil.getMessage(description);
        if (translatedDescription != null) {
          description=translatedDescription;
        }
      }
      RecordsManagementSearchParameters searchParameters=new RecordsManagementSearchParameters();
      if (report.has(""String_Node_Str"")) {
        searchParameters=RecordsManagementSearchParameters.createFromJSON(report.getJSONObject(""String_Node_Str""),namespaceService);
      }
      ReportDetails reportDetails=new ReportDetails(name,description,query,searchParameters);
      reports.add(reportDetails);
    }
  }
 catch (  JSONException exception) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + reportsJSON,exception);
  }
}",0.9731446722494946
89080,"/** 
 * Helper method to add information about node
 * @param nodeInfo          node information
 * @param rootJSONObject    root JSON object
 */
@SuppressWarnings(""String_Node_Str"") private void addInfo(final FileInfo nodeInfo,JSONObject rootJSONObject){
  String itemType=(String)rootJSONObject.get(""String_Node_Str"");
  final QName itemTypeQName=QName.createQName(itemType,namespaceService);
  NodeRef originatingLocation=AuthenticationUtil.runAsSystem(new RunAsWork<NodeRef>(){
    public NodeRef doWork(){
      NodeRef originatingLocation=null;
      if (dictionaryService.isSubClass(itemTypeQName,ContentModel.TYPE_CONTENT)) {
        NodeRef nodeRef=nodeInfo.getNodeRef();
        List<ChildAssociationRef> parentAssocs=nodeService.getParentAssocs(nodeRef);
        for (        ChildAssociationRef parent : parentAssocs) {
          if (!parent.isPrimary()) {
            originatingLocation=parent.getParentRef();
            if (!nodeService.hasAspect(originatingLocation,RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT)) {
              break;
            }
          }
        }
      }
      return originatingLocation;
    }
  }
);
  if (originatingLocation != null) {
    String pathSeparator=""String_Node_Str"";
    String displayPath=getDisplayPath(originatingLocation);
    String[] displayPathElements=displayPath.split(pathSeparator);
    Object[] subPath=ArrayUtils.subarray(displayPathElements,5,displayPathElements.length);
    StringBuffer originatingLocationPath=new StringBuffer();
    for (int i=0; i < subPath.length; i++) {
      originatingLocationPath.append(pathSeparator).append(subPath[i]);
    }
    rootJSONObject.put(""String_Node_Str"",originatingLocationPath.toString());
  }
}","/** 
 * Helper method to add information about node
 * @param nodeInfo          node information
 * @param rootJSONObject    root JSON object
 */
@SuppressWarnings(""String_Node_Str"") private void addInfo(final FileInfo nodeInfo,JSONObject rootJSONObject){
  String itemType=(String)rootJSONObject.get(""String_Node_Str"");
  final QName itemTypeQName=QName.createQName(itemType,namespaceService);
  NodeRef originatingLocation=AuthenticationUtil.runAsSystem(new RunAsWork<NodeRef>(){
    public NodeRef doWork(){
      NodeRef originatingLocation=null;
      if (dictionaryService.isSubClass(itemTypeQName,ContentModel.TYPE_CONTENT)) {
        NodeRef nodeRef=nodeInfo.getNodeRef();
        List<ChildAssociationRef> parentAssocs=nodeService.getParentAssocs(nodeRef);
        for (        ChildAssociationRef parent : parentAssocs) {
          if (!parent.isPrimary()) {
            originatingLocation=parent.getParentRef();
            if (!nodeService.hasAspect(originatingLocation,RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT)) {
              break;
            }
          }
        }
      }
      return originatingLocation;
    }
  }
);
  if (originatingLocation != null) {
    String pathSeparator=""String_Node_Str"";
    String displayPath=getDisplayPath(originatingLocation);
    String[] displayPathElements=displayPath.split(pathSeparator);
    Object[] subPath=ArrayUtils.subarray(displayPathElements,5,displayPathElements.length);
    StringBuilder originatingLocationPath=new StringBuilder();
    for (int i=0; i < subPath.length; i++) {
      originatingLocationPath.append(pathSeparator).append(subPath[i]);
    }
    rootJSONObject.put(""String_Node_Str"",originatingLocationPath.toString());
  }
}",0.9970879440885264
89081,"/** 
 * Gets the template name based on the type and mimetype.
 * @param mimetype
 * @return
 */
private String getReportTemplateName(String mimetype){
  String typePrefixName=reportType.getPrefixedQName(namespaceService).getPrefixString().replace(""String_Node_Str"",""String_Node_Str"");
  String extension=mimetypeService.getExtension(mimetype);
  StringBuffer sb=new StringBuffer(128).append(""String_Node_Str"").append(typePrefixName).append(""String_Node_Str"").append(extension).append(""String_Node_Str"");
  return sb.toString();
}","/** 
 * Gets the template name based on the type and mimetype.
 * @param mimetype
 * @return
 */
private String getReportTemplateName(String mimetype){
  String typePrefixName=reportType.getPrefixedQName(namespaceService).getPrefixString().replace(""String_Node_Str"",""String_Node_Str"");
  String extension=mimetypeService.getExtension(mimetype);
  StringBuilder sb=new StringBuilder(128).append(""String_Node_Str"").append(typePrefixName).append(""String_Node_Str"").append(extension).append(""String_Node_Str"");
  return sb.toString();
}",0.9905838041431262
89082,"@SuppressWarnings(""String_Node_Str"") @Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  String reqContentAsString;
  try {
    reqContentAsString=req.getContent().getContent();
  }
 catch (  IOException iox) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",iox);
  }
  String actionName=null;
  List<NodeRef> targetNodeRefs=new ArrayList<NodeRef>(1);
  Map<String,Serializable> actionParams=new HashMap<String,Serializable>(3);
  try {
    JSONObject jsonObj=new JSONObject(new JSONTokener(reqContentAsString));
    if (jsonObj.has(PARAM_NAME)) {
      actionName=jsonObj.getString(PARAM_NAME);
    }
    if (jsonObj.has(PARAM_NODE_REF)) {
      NodeRef nodeRef=new NodeRef(jsonObj.getString(PARAM_NODE_REF));
      targetNodeRefs.add(nodeRef);
    }
    if (jsonObj.has(PARAM_NODE_REFS)) {
      JSONArray jsonArray=jsonObj.getJSONArray(PARAM_NODE_REFS);
      if (jsonArray.length() != 0) {
        targetNodeRefs=new ArrayList<NodeRef>(jsonArray.length());
        for (int i=0; i < jsonArray.length(); i++) {
          NodeRef nodeRef=new NodeRef(jsonArray.getString(i));
          targetNodeRefs.add(nodeRef);
        }
      }
    }
    if (jsonObj.has(PARAM_PARAMS)) {
      JSONObject paramsObj=jsonObj.getJSONObject(PARAM_PARAMS);
      for (Iterator<String> iter=paramsObj.keys(); iter.hasNext(); ) {
        String nextKeyString=iter.next();
        Object nextValue=paramsObj.get(nextKeyString);
        if ((nextValue instanceof JSONObject) && ((JSONObject)nextValue).has(""String_Node_Str"")) {
          String dateStringValue=((JSONObject)nextValue).getString(""String_Node_Str"");
          nextValue=ISO8601DateFormat.parse(dateStringValue);
        }
        actionParams.put(nextKeyString,(Serializable)nextValue);
      }
    }
  }
 catch (  JSONException exception) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",exception);
  }
  if (actionName == null) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"");
  }
  StringBuffer targetNodeRefsString=new StringBuffer(30);
  boolean firstTime=true;
  for (  NodeRef targetNodeRef : targetNodeRefs) {
    if (!nodeService.exists(targetNodeRef)) {
      throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + targetNodeRef.toString() + ""String_Node_Str"");
    }
    if (firstTime) {
      firstTime=false;
    }
 else {
      targetNodeRefsString.append(""String_Node_Str"");
    }
    targetNodeRefsString.append(targetNodeRef.toString());
  }
  if (logger.isDebugEnabled()) {
    StringBuilder msg=new StringBuilder();
    msg.append(""String_Node_Str"").append(actionName).append(""String_Node_Str"").append(targetNodeRefsString.toString()).append(""String_Node_Str"").append(actionParams);
    logger.debug(msg.toString());
  }
  Map<String,Object> model=new HashMap<String,Object>();
  if (targetNodeRefs.isEmpty()) {
    RecordsManagementActionResult result=this.rmActionService.executeRecordsManagementAction(actionName,actionParams);
    if (result.getValue() != null) {
      model.put(""String_Node_Str"",result.getValue().toString());
    }
  }
 else {
    Map<NodeRef,RecordsManagementActionResult> resultMap=this.rmActionService.executeRecordsManagementAction(targetNodeRefs,actionName,actionParams);
    Map<String,String> results=new HashMap<String,String>(resultMap.size());
    for (    Map.Entry<NodeRef,RecordsManagementActionResult> entry : resultMap.entrySet()) {
      Object value=entry.getValue().getValue();
      if (value != null) {
        results.put(entry.getKey().toString(),value.toString());
      }
    }
    model.put(""String_Node_Str"",results);
  }
  model.put(""String_Node_Str"",""String_Node_Str"" + actionName + ""String_Node_Str""+ targetNodeRefsString.toString());
  return model;
}","@SuppressWarnings(""String_Node_Str"") @Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  String reqContentAsString;
  try {
    reqContentAsString=req.getContent().getContent();
  }
 catch (  IOException iox) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",iox);
  }
  String actionName=null;
  List<NodeRef> targetNodeRefs=new ArrayList<NodeRef>(1);
  Map<String,Serializable> actionParams=new HashMap<String,Serializable>(3);
  try {
    JSONObject jsonObj=new JSONObject(new JSONTokener(reqContentAsString));
    if (jsonObj.has(PARAM_NAME)) {
      actionName=jsonObj.getString(PARAM_NAME);
    }
    if (jsonObj.has(PARAM_NODE_REF)) {
      NodeRef nodeRef=new NodeRef(jsonObj.getString(PARAM_NODE_REF));
      targetNodeRefs.add(nodeRef);
    }
    if (jsonObj.has(PARAM_NODE_REFS)) {
      JSONArray jsonArray=jsonObj.getJSONArray(PARAM_NODE_REFS);
      if (jsonArray.length() != 0) {
        targetNodeRefs=new ArrayList<NodeRef>(jsonArray.length());
        for (int i=0; i < jsonArray.length(); i++) {
          NodeRef nodeRef=new NodeRef(jsonArray.getString(i));
          targetNodeRefs.add(nodeRef);
        }
      }
    }
    if (jsonObj.has(PARAM_PARAMS)) {
      JSONObject paramsObj=jsonObj.getJSONObject(PARAM_PARAMS);
      for (Iterator<String> iter=paramsObj.keys(); iter.hasNext(); ) {
        String nextKeyString=iter.next();
        Object nextValue=paramsObj.get(nextKeyString);
        if ((nextValue instanceof JSONObject) && ((JSONObject)nextValue).has(""String_Node_Str"")) {
          String dateStringValue=((JSONObject)nextValue).getString(""String_Node_Str"");
          nextValue=ISO8601DateFormat.parse(dateStringValue);
        }
        actionParams.put(nextKeyString,(Serializable)nextValue);
      }
    }
  }
 catch (  JSONException exception) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",exception);
  }
  if (actionName == null) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"");
  }
  StringBuilder targetNodeRefsString=new StringBuilder(30);
  boolean firstTime=true;
  for (  NodeRef targetNodeRef : targetNodeRefs) {
    if (!nodeService.exists(targetNodeRef)) {
      throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + targetNodeRef.toString() + ""String_Node_Str"");
    }
    if (firstTime) {
      firstTime=false;
    }
 else {
      targetNodeRefsString.append(""String_Node_Str"");
    }
    targetNodeRefsString.append(targetNodeRef.toString());
  }
  if (logger.isDebugEnabled()) {
    StringBuilder msg=new StringBuilder();
    msg.append(""String_Node_Str"").append(actionName).append(""String_Node_Str"").append(targetNodeRefsString.toString()).append(""String_Node_Str"").append(actionParams);
    logger.debug(msg.toString());
  }
  Map<String,Object> model=new HashMap<String,Object>();
  if (targetNodeRefs.isEmpty()) {
    RecordsManagementActionResult result=this.rmActionService.executeRecordsManagementAction(actionName,actionParams);
    if (result.getValue() != null) {
      model.put(""String_Node_Str"",result.getValue().toString());
    }
  }
 else {
    Map<NodeRef,RecordsManagementActionResult> resultMap=this.rmActionService.executeRecordsManagementAction(targetNodeRefs,actionName,actionParams);
    Map<String,String> results=new HashMap<String,String>(resultMap.size());
    for (    Map.Entry<NodeRef,RecordsManagementActionResult> entry : resultMap.entrySet()) {
      Object value=entry.getValue().getValue();
      if (value != null) {
        results.put(entry.getKey().toString(),value.toString());
      }
    }
    model.put(""String_Node_Str"",results);
  }
  model.put(""String_Node_Str"",""String_Node_Str"" + actionName + ""String_Node_Str""+ targetNodeRefsString.toString());
  return model;
}",0.9986938349007316
89083,"private String getDisplayName(String userName){
  String result=personDataCache.get(userName);
  if (result == null) {
    NodeRef person=personService.getPerson(userName);
    if (person != null) {
      StringBuffer displayName=new StringBuffer(128);
      displayName.append(nodeService.getProperty(person,ContentModel.PROP_FIRSTNAME)).append(""String_Node_Str"").append(nodeService.getProperty(person,ContentModel.PROP_LASTNAME));
      result=displayName.toString();
    }
 else {
      result=userName;
    }
    personDataCache.put(userName,result);
  }
  return result;
}","private String getDisplayName(String userName){
  String result=personDataCache.get(userName);
  if (result == null) {
    NodeRef person=personService.getPerson(userName);
    if (person != null) {
      StringBuilder displayName=new StringBuilder(128);
      displayName.append(nodeService.getProperty(person,ContentModel.PROP_FIRSTNAME)).append(""String_Node_Str"").append(nodeService.getProperty(person,ContentModel.PROP_LASTNAME));
      result=displayName.toString();
    }
 else {
      result=userName;
    }
    personDataCache.put(userName,result);
  }
  return result;
}",0.991349480968858
89084,"/** 
 * Gets the failure report for the currently recorded capabilities.
 * @return  {@link String}  capability error report
 */
public String getFailureReport(){
  String result=null;
  if (LOGGER.isDebugEnabled()) {
    Collection<CapabilityReport> capabilities=RMMethodSecurityInterceptor.CAPABILITIES.get().values();
    if (!capabilities.isEmpty()) {
      StringBuffer buffer=new StringBuffer(""String_Node_Str"");
      for (      CapabilityReport capability : capabilities) {
        buffer.append(""String_Node_Str"").append(capability.name).append(""String_Node_Str"").append(capability.status).append(""String_Node_Str"");
        if (!capability.conditions.isEmpty()) {
          for (          Map.Entry<String,Boolean> entry : capability.conditions.entrySet()) {
            buffer.append(""String_Node_Str"").append(entry.getKey()).append(""String_Node_Str"");
            if (entry.getValue()) {
              buffer.append(""String_Node_Str"");
            }
 else {
              buffer.append(""String_Node_Str"");
            }
            buffer.append(""String_Node_Str"");
          }
        }
      }
      result=buffer.toString();
    }
  }
  return result;
}","/** 
 * Gets the failure report for the currently recorded capabilities.
 * @return  {@link String}  capability error report
 */
public String getFailureReport(){
  String result=null;
  if (LOGGER.isDebugEnabled()) {
    Collection<CapabilityReport> capabilities=RMMethodSecurityInterceptor.CAPABILITIES.get().values();
    if (!capabilities.isEmpty()) {
      StringBuilder buffer=new StringBuilder(""String_Node_Str"");
      for (      CapabilityReport capability : capabilities) {
        buffer.append(""String_Node_Str"").append(capability.name).append(""String_Node_Str"").append(capability.status).append(""String_Node_Str"");
        if (!capability.conditions.isEmpty()) {
          for (          Map.Entry<String,Boolean> entry : capability.conditions.entrySet()) {
            buffer.append(""String_Node_Str"").append(entry.getKey()).append(""String_Node_Str"");
            if (entry.getValue()) {
              buffer.append(""String_Node_Str"");
            }
 else {
              buffer.append(""String_Node_Str"");
            }
            buffer.append(""String_Node_Str"");
          }
        }
      }
      result=buffer.toString();
    }
  }
  return result;
}",0.9957228400342172
89085,"/** 
 * @param map
 * @return
 */
private String convertToString(Map<String,String> map){
  StringBuffer buffer=new StringBuffer(256);
  for (  Map.Entry<String,String> entry : map.entrySet()) {
    buffer.append(entry.getKey()).append(""String_Node_Str"").append(entry.getValue()).append(""String_Node_Str"");
  }
  return buffer.toString();
}","/** 
 * @param map
 * @return
 */
private String convertToString(Map<String,String> map){
  StringBuilder buffer=new StringBuilder(256);
  for (  Map.Entry<String,String> entry : map.entrySet()) {
    buffer.append(entry.getKey()).append(""String_Node_Str"").append(entry.getValue()).append(""String_Node_Str"");
  }
  return buffer.toString();
}",0.9853372434017597
89086,"public void refreshCurrentPage(){
  Actions actions=new Actions(Utils.getWebDriver());
  actions.keyDown(Keys.CONTROL).sendKeys(Keys.F5).perform();
}","/** 
 * Refresh the current page. 
 */
public void refreshCurrentPage(){
  webDriver.navigate().refresh();
}",0.4124513618677042
89087,"/** 
 * Downgrade instructions mandatory when downgrade on (or when) set <p> <a href=""https://issues.alfresco.com/jira/browse/RM-2409"">RM-2409</a><pre> Given I am a cleared user And I am classifying the content for the first time And I enter a downgrade date and/or event When I attempt to save the classification information Then I will be informed that downgrade instructions are mandatory when the downgrade date and/or event are set And the save will not be successful Note that downgrade instructions can be set without a date or event specified. </pre>
 */
@Test(groups={""String_Node_Str"",""String_Node_Str""},description=""String_Node_Str"",dependsOnGroups={""String_Node_Str""}) @AlfrescoTest(jira=""String_Node_Str"") public void checkInstructionsFieldStates(){
  String recordName=""String_Node_Str"";
  openPage(filePlan,RM_SITE_ID,createPathFrom(""String_Node_Str"",RECORD_CATEGORY_THREE,RECORD_FOLDER_THREE));
  filePlan.getToolbar().clickOnFile().clickOnElectronic().uploadFile(recordName);
  classifyContentDialog=filePlan.getRecord(recordName).clickOnClassifyAction();
  classifyContentDialog.setLevel(SECRET_CLASSIFICATION_LEVEL_TEXT).setAgency(CLASSIFICATION_AGENCY).addReason(CLASSIFICATION_REASON);
  classifyContentDialog.setDowngradeDate(DOWNGRADE_DATE_INPUT);
  assertTrue(""String_Node_Str"",classifyContentDialog.isInstructionsFieldRequired());
  assertFalse(""String_Node_Str"",classifyContentDialog.isClassifyButtonEnabled());
  classifyContentDialog.clearDowngradeDate();
  classifyContentDialog.setDowngradeEvent(DOWNGRADE_EVENT);
  assertTrue(""String_Node_Str"",classifyContentDialog.isInstructionsFieldRequired());
  assertFalse(""String_Node_Str"",classifyContentDialog.isClassifyButtonEnabled());
  classifyContentDialog.setDowngradeDate(DOWNGRADE_DATE_INPUT);
  assertTrue(""String_Node_Str"",classifyContentDialog.isInstructionsFieldRequired());
  assertFalse(""String_Node_Str"",classifyContentDialog.isClassifyButtonEnabled());
  classifyContentDialog.setDowngradeInstructions(DOWNGRADE_INSTRUCTIONS);
  assertTrue(""String_Node_Str"",classifyContentDialog.isClassifyButtonEnabled());
  classifyContentDialog.clearDowngradeDate().clearDowngradeEvent();
  assertTrue(""String_Node_Str"",classifyContentDialog.isClassifyButtonEnabled());
}","/** 
 * Downgrade instructions mandatory when downgrade on (or when) set <p> <a href=""https://issues.alfresco.com/jira/browse/RM-2409"">RM-2409</a><pre> Given I am a cleared user And I am classifying the content for the first time And I enter a downgrade date and/or event When I attempt to save the classification information Then I will be informed that downgrade instructions are mandatory when the downgrade date and/or event are set And the save will not be successful Note that downgrade instructions can be set without a date or event specified. </pre>
 */
@Test(groups={""String_Node_Str"",""String_Node_Str""},description=""String_Node_Str"",dependsOnGroups={""String_Node_Str""}) @AlfrescoTest(jira=""String_Node_Str"") public void checkInstructionsFieldStates(){
  String recordName=""String_Node_Str"";
  openPage(filePlan,RM_SITE_ID,createPathFrom(""String_Node_Str"",RECORD_CATEGORY_THREE,RECORD_FOLDER_THREE));
  filePlan.getToolbar().clickOnFile().clickOnElectronic().uploadFile(recordName);
  classifyContentDialog=filePlan.getRecord(recordName).clickOnClassifyAction();
  classifyContentDialog.setLevel(SECRET_CLASSIFICATION_LEVEL_TEXT).setAgency(CLASSIFICATION_AGENCY).addReason(CLASSIFICATION_REASON);
  classifyContentDialog.setDowngradeDate(DOWNGRADE_DATE_INPUT);
  assertTrue(""String_Node_Str"",classifyContentDialog.isInstructionsFieldRequired());
  assertFalse(""String_Node_Str"",classifyContentDialog.isClassifyButtonEnabled());
  classifyContentDialog.clearDowngradeDate();
  classifyContentDialog.setDowngradeEvent(DOWNGRADE_EVENT);
  assertTrue(""String_Node_Str"",classifyContentDialog.isInstructionsFieldRequired());
  assertFalse(""String_Node_Str"",classifyContentDialog.isClassifyButtonEnabled());
  classifyContentDialog.setDowngradeDate(DOWNGRADE_DATE_INPUT);
  assertTrue(""String_Node_Str"",classifyContentDialog.isInstructionsFieldRequired());
  assertFalse(""String_Node_Str"",classifyContentDialog.isClassifyButtonEnabled());
  classifyContentDialog.setDowngradeInstructions(DOWNGRADE_INSTRUCTIONS);
  assertTrue(""String_Node_Str"",classifyContentDialog.isClassifyButtonEnabled());
  classifyContentDialog.clearDowngradeDate().clearDowngradeEvent();
  assertTrue(""String_Node_Str"",classifyContentDialog.isClassifyButtonEnabled());
  classifyContentDialog.clickOnCancel();
}",0.9909592061742006
89088,"/** 
 * Set up four users with different clearances and a site containing four documents at different classification levels.
 */
@Test(groups={""String_Node_Str""}) public void setupTestData() throws Exception {
  for (  String user : Arrays.asList(UNCLASSIFIED_USER,CONFIDENTIAL_USER,SECRET_USER,TOP_SECRET_USER)) {
    dataPrepHelper.createUser(user);
  }
  openPage(securityClearancePage).setClearance(CONFIDENTIAL_USER,CONFIDENTIAL_CLASSIFICATION_LEVEL_TEXT).clickOnConfirm(securityClearancePage).setClearance(SECRET_USER,SECRET_CLASSIFICATION_LEVEL_TEXT).clickOnConfirm(securityClearancePage).setClearance(TOP_SECRET_USER,TOP_SECRET_CLASSIFICATION_LEVEL_TEXT).clickOnConfirm(securityClearancePage);
  dataPrepHelper.createSite(SEARCH_TEST_SITE_NAME,SEARCH_TEST_SITE_ID);
  for (  String user : Arrays.asList(UNCLASSIFIED_USER,CONFIDENTIAL_USER,SECRET_USER,TOP_SECRET_USER)) {
    userService.inviteUserToSiteAndAccept(user,getAdminName(),getAdminPassword(),SEARCH_TEST_SITE_ID,""String_Node_Str"");
  }
  openPage(documentLibrary,SEARCH_TEST_SITE_ID);
  for (  String documentName : Arrays.asList(UNCLASSIFIED_DOCUMENT,CONFIDENTIAL_DOCUMENT,SECRET_DOCUMENT,TOP_SECRET_DOCUMENT)) {
    contentService.createDocument(getAdminName(),getAdminPassword(),SEARCH_TEST_SITE_ID,DocumentType.TEXT_PLAIN,documentName,TEST_CONTENT);
  }
  openPage(documentLibrary,SEARCH_TEST_SITE_ID);
  classifyDocument(CONFIDENTIAL_DOCUMENT,CONFIDENTIAL_CLASSIFICATION_LEVEL_TEXT);
  classifyDocument(SECRET_DOCUMENT,SECRET_CLASSIFICATION_LEVEL_TEXT);
  classifyDocument(TOP_SECRET_DOCUMENT,TOP_SECRET_CLASSIFICATION_LEVEL_TEXT);
  new FluentWait<WebDriver>(Utils.getWebDriver()).withTimeout(10,TimeUnit.SECONDS).pollingEvery(1,TimeUnit.SECONDS).until(documentsAvailableForSearch);
}","/** 
 * Set up four users with different clearances and a site containing four documents at different classification levels.
 */
@Test(groups={""String_Node_Str""}) public void setupTestData() throws Exception {
  for (  String user : Arrays.asList(UNCLASSIFIED_USER,CONFIDENTIAL_USER,SECRET_USER,TOP_SECRET_USER)) {
    dataPrepHelper.createUser(user);
  }
  openPage(securityClearancePage).setClearance(CONFIDENTIAL_USER,CONFIDENTIAL_CLASSIFICATION_LEVEL_TEXT).clickOnConfirm(securityClearancePage).setClearance(SECRET_USER,SECRET_CLASSIFICATION_LEVEL_TEXT).clickOnConfirm(securityClearancePage).setClearance(TOP_SECRET_USER,TOP_SECRET_CLASSIFICATION_LEVEL_TEXT).clickOnConfirm(securityClearancePage);
  dataPrepHelper.createSite(SEARCH_TEST_SITE_NAME,SEARCH_TEST_SITE_ID);
  for (  String user : Arrays.asList(UNCLASSIFIED_USER,CONFIDENTIAL_USER,SECRET_USER,TOP_SECRET_USER)) {
    userService.inviteUserToSiteAndAccept(getAdminName(),getAdminPassword(),user,SEARCH_TEST_SITE_ID,""String_Node_Str"");
  }
  openPage(documentLibrary,SEARCH_TEST_SITE_ID);
  for (  String documentName : Arrays.asList(UNCLASSIFIED_DOCUMENT,CONFIDENTIAL_DOCUMENT,SECRET_DOCUMENT,TOP_SECRET_DOCUMENT)) {
    contentService.createDocument(getAdminName(),getAdminPassword(),SEARCH_TEST_SITE_ID,DocumentType.TEXT_PLAIN,documentName,TEST_CONTENT);
  }
  openPage(documentLibrary,SEARCH_TEST_SITE_ID);
  classifyDocument(CONFIDENTIAL_DOCUMENT,CONFIDENTIAL_CLASSIFICATION_LEVEL_TEXT);
  classifyDocument(SECRET_DOCUMENT,SECRET_CLASSIFICATION_LEVEL_TEXT);
  classifyDocument(TOP_SECRET_DOCUMENT,TOP_SECRET_CLASSIFICATION_LEVEL_TEXT);
  new FluentWait<WebDriver>(Utils.getWebDriver()).withTimeout(10,TimeUnit.SECONDS).pollingEvery(1,TimeUnit.SECONDS).until(documentsAvailableForSearch);
}",0.997155858930603
89089,"@Test(groups={""String_Node_Str""},description=""String_Node_Str"",dependsOnMethods=""String_Node_Str"") @AlfrescoTest(jira=""String_Node_Str"") public void confidentialUserSearch(){
  List<SearchResult> results=openPage(CONFIDENTIAL_USER,DEFAULT_PASSWORD,advancedSearchPage).setKeywords(SEARCH_TERM).clickOnSearch().getSearchResults();
  assertEquals(""String_Node_Str"",2,results.size());
  assertTrue(""String_Node_Str"",searchResultsContainDocument(UNCLASSIFIED_DOCUMENT,results));
  assertTrue(""String_Node_Str"",searchResultsContainDocument(CONFIDENTIAL_DOCUMENT,results));
  assertFalse(""String_Node_Str"",searchResultsContainDocument(SECRET_DOCUMENT,results));
  assertFalse(""String_Node_Str"",searchResultsContainDocument(TOP_SECRET_DOCUMENT,results));
  SearchResult unclassifiedDocument=getResult(UNCLASSIFIED_DOCUMENT,results);
  SearchResult confidentialDocument=getResult(CONFIDENTIAL_DOCUMENT,results);
  assertTrue(unclassifiedDocument.isSearchResultActionClickable(DocumentActions.CLASSIFY,unclassifiedDocument.getSearchResultRow()));
  assertTrue(confidentialDocument.isSearchResultActionClickable(DocumentActions.EDIT_CLASSIFICATION,confidentialDocument.getSearchResultRow()));
  assertFalse(confidentialDocument.isSearchResultActionDisplayed(DocumentActions.CLASSIFY,confidentialDocument.getSearchResultRow(),true));
}","@Test(groups={""String_Node_Str""},description=""String_Node_Str"",dependsOnMethods=""String_Node_Str"") @AlfrescoTest(jira=""String_Node_Str"") public void confidentialUserSearch(){
  List<SearchResult> results=openPage(CONFIDENTIAL_USER,DEFAULT_PASSWORD,advancedSearchPage).setKeywords(SEARCH_TERM).clickOnSearch().getSearchResults();
  assertEquals(""String_Node_Str"",2,results.size());
  assertTrue(""String_Node_Str"",searchResultsContainDocument(UNCLASSIFIED_DOCUMENT,results));
  assertTrue(""String_Node_Str"",searchResultsContainDocument(CONFIDENTIAL_DOCUMENT,results));
  assertFalse(""String_Node_Str"",searchResultsContainDocument(SECRET_DOCUMENT,results));
  assertFalse(""String_Node_Str"",searchResultsContainDocument(TOP_SECRET_DOCUMENT,results));
  SearchResult unclassifiedDocument=getResult(UNCLASSIFIED_DOCUMENT,results);
  SearchResult confidentialDocument=getResult(CONFIDENTIAL_DOCUMENT,results);
  assertTrue(unclassifiedDocument.isSearchResultActionClickable(DocumentActions.CLASSIFY,unclassifiedDocument.getSearchResultRow()));
  assertTrue(confidentialDocument.isSearchResultActionClickable(DocumentActions.EDIT_CLASSIFICATION,confidentialDocument.getSearchResultRow()));
}",0.9441340782122905
89090,"/** 
 * Upgrade the classification of a document and check the reclassification fields are set. 
 */
@Test(groups={""String_Node_Str""},description=""String_Node_Str"",dependsOnMethods=""String_Node_Str"") @AlfrescoTest(jira=""String_Node_Str"") public void upgradeDocument(){
  openPage(documentLibrary,SITE_ID);
  documentLibrary.getDocument(DOCUMENT).clickOnEditClassification().setLevel(TOP_SECRET_CLASSIFICATION_LEVEL_TEXT).setReclassifiedBy(""String_Node_Str"").setReclassifyReason(""String_Node_Str"").clickOnClassify();
  documentLibrary.getDocument(DOCUMENT).clickOnLink(classifiedDocumentDetails);
  Map<ClassifiedPropertiesPanelField,String> expectedFields=new HashMap<>();
  expectedFields.put(ClassifiedPropertiesPanelField.CURRENT_CLASSIFICATION,TOP_SECRET_CLASSIFICATION_LEVEL_TEXT);
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_BY,""String_Node_Str"");
  String expectedDate=DateTimeFormatter.ofPattern(""String_Node_Str"").withZone(ZoneOffset.UTC).format(Instant.now());
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_DATE,expectedDate);
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_REASON,""String_Node_Str"");
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_ACTION,""String_Node_Str"");
  expectedFields.forEach((field,value) -> assertEquals(value,classifiedPropertiesPanel.getClassifiedProperty(field)));
}","/** 
 * Upgrade the classification of a document and check the reclassification fields are set. 
 */
@Test(groups={""String_Node_Str""},description=""String_Node_Str"",dependsOnMethods=""String_Node_Str"") @AlfrescoTest(jira=""String_Node_Str"") public void upgradeDocument(){
  openPage(documentLibrary,SITE_ID);
  documentLibrary.getDocument(DOCUMENT).clickOnEditClassification().setLevel(TOP_SECRET_CLASSIFICATION_LEVEL_TEXT).setReclassifiedBy(""String_Node_Str"").setReclassifyReason(""String_Node_Str"").clickOnClassify();
  documentLibrary.getDocument(DOCUMENT).clickOnLink(classifiedDocumentDetails);
  Map<ClassifiedPropertiesPanelField,String> expectedFields=new HashMap<>();
  expectedFields.put(ClassifiedPropertiesPanelField.CURRENT_CLASSIFICATION,TOP_SECRET_CLASSIFICATION_LEVEL_TEXT);
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_BY,""String_Node_Str"");
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_REASON,""String_Node_Str"");
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_ACTION,""String_Node_Str"");
  expectedFields.forEach((field,value) -> assertEquals(value,classifiedPropertiesPanel.getClassifiedProperty(field)));
  String reclassifyDate=classifiedPropertiesPanel.getClassifiedProperty(ClassifiedPropertiesPanelField.RECLASSIFY_DATE);
  checkValidPropertiesPanelDate(reclassifyDate,""String_Node_Str"");
}",0.6876371616678859
89091,"/** 
 * Downgrade the classification of a document and check the reclassification fields are set. 
 */
@Test(groups={""String_Node_Str""},description=""String_Node_Str"",dependsOnMethods=""String_Node_Str"") @AlfrescoTest(jira=""String_Node_Str"") public void downgradeDocument(){
  openPage(documentLibrary,SITE_ID);
  documentLibrary.getDocument(DOCUMENT).clickOnEditClassification().setLevel(CONFIDENTIAL_CLASSIFICATION_LEVEL_TEXT).setReclassifiedBy(""String_Node_Str"").setReclassifyReason(""String_Node_Str"").clickOnClassify();
  documentLibrary.getDocument(DOCUMENT).clickOnLink(classifiedDocumentDetails);
  Map<ClassifiedPropertiesPanelField,String> expectedFields=new HashMap<>();
  expectedFields.put(ClassifiedPropertiesPanelField.CURRENT_CLASSIFICATION,CONFIDENTIAL_CLASSIFICATION_LEVEL_TEXT);
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_BY,""String_Node_Str"");
  String expectedDate=DateTimeFormatter.ofPattern(""String_Node_Str"").withZone(ZoneOffset.UTC).format(Instant.now());
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_DATE,expectedDate);
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_REASON,""String_Node_Str"");
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_ACTION,""String_Node_Str"");
  expectedFields.forEach((field,value) -> assertEquals(value,classifiedPropertiesPanel.getClassifiedProperty(field)));
}","/** 
 * Downgrade the classification of a document and check the reclassification fields are set. 
 */
@Test(groups={""String_Node_Str""},description=""String_Node_Str"",dependsOnMethods=""String_Node_Str"") @AlfrescoTest(jira=""String_Node_Str"") public void downgradeDocument(){
  openPage(documentLibrary,SITE_ID);
  documentLibrary.getDocument(DOCUMENT).clickOnEditClassification().setLevel(CONFIDENTIAL_CLASSIFICATION_LEVEL_TEXT).setReclassifiedBy(""String_Node_Str"").setReclassifyReason(""String_Node_Str"").clickOnClassify();
  documentLibrary.getDocument(DOCUMENT).clickOnLink(classifiedDocumentDetails);
  Map<ClassifiedPropertiesPanelField,String> expectedFields=new HashMap<>();
  expectedFields.put(ClassifiedPropertiesPanelField.CURRENT_CLASSIFICATION,CONFIDENTIAL_CLASSIFICATION_LEVEL_TEXT);
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_BY,""String_Node_Str"");
  String expectedDate=DateTimeFormatter.ofPattern(""String_Node_Str"").withZone(ZoneOffset.UTC).format(Instant.now());
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_DATE,expectedDate);
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_REASON,""String_Node_Str"");
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_ACTION,""String_Node_Str"");
  expectedFields.forEach((field,value) -> assertEquals(value,classifiedPropertiesPanel.getClassifiedProperty(field)));
  String reclassifyDate=classifiedPropertiesPanel.getClassifiedProperty(ClassifiedPropertiesPanelField.RECLASSIFY_DATE);
  checkValidPropertiesPanelDate(reclassifyDate,""String_Node_Str"");
}",0.936271186440678
89092,"/** 
 * Declassify a document and check the reclassification fields are set. This must not be done before the downgrade test and so it 'depends' on it.
 */
@Test(groups={""String_Node_Str""},description=""String_Node_Str"",dependsOnMethods=""String_Node_Str"") @AlfrescoTest(jira=""String_Node_Str"") public void declassifyDocument(){
  openPage(documentLibrary,SITE_ID);
  documentLibrary.getDocument(DOCUMENT).clickOnEditClassification().setLevel(UNCLASSIFIED_CLASSIFICATION_LEVEL_TEXT).setReclassifiedBy(""String_Node_Str"").setReclassifyReason(""String_Node_Str"").clickOnClassify();
  documentLibrary.getDocument(DOCUMENT).clickOnLink(classifiedDocumentDetails);
  Map<ClassifiedPropertiesPanelField,String> expectedFields=new HashMap<>();
  expectedFields.put(ClassifiedPropertiesPanelField.CURRENT_CLASSIFICATION,UNCLASSIFIED_CLASSIFICATION_LEVEL_TEXT);
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_BY,""String_Node_Str"");
  String expectedDate=DateTimeFormatter.ofPattern(""String_Node_Str"").withZone(ZoneOffset.UTC).format(Instant.now());
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_DATE,expectedDate);
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_REASON,""String_Node_Str"");
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_ACTION,""String_Node_Str"");
  expectedFields.forEach((field,value) -> assertEquals(value,classifiedPropertiesPanel.getClassifiedProperty(field)));
}","/** 
 * Declassify a document and check the reclassification fields are set. This must not be done before the downgrade test and so it 'depends' on it.
 */
@Test(groups={""String_Node_Str""},description=""String_Node_Str"",dependsOnMethods=""String_Node_Str"") @AlfrescoTest(jira=""String_Node_Str"") public void declassifyDocument(){
  openPage(documentLibrary,SITE_ID);
  documentLibrary.getDocument(DOCUMENT).clickOnEditClassification().setLevel(UNCLASSIFIED_CLASSIFICATION_LEVEL_TEXT).setReclassifiedBy(""String_Node_Str"").setReclassifyReason(""String_Node_Str"").clickOnClassify();
  documentLibrary.getDocument(DOCUMENT).clickOnLink(classifiedDocumentDetails);
  Map<ClassifiedPropertiesPanelField,String> expectedFields=new HashMap<>();
  expectedFields.put(ClassifiedPropertiesPanelField.CURRENT_CLASSIFICATION,UNCLASSIFIED_CLASSIFICATION_LEVEL_TEXT);
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_BY,""String_Node_Str"");
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_REASON,""String_Node_Str"");
  expectedFields.put(ClassifiedPropertiesPanelField.RECLASSIFY_ACTION,""String_Node_Str"");
  expectedFields.forEach((field,value) -> assertEquals(value,classifiedPropertiesPanel.getClassifiedProperty(field)));
  String reclassifyDate=classifiedPropertiesPanel.getClassifiedProperty(ClassifiedPropertiesPanelField.RECLASSIFY_DATE);
  checkValidPropertiesPanelDate(reclassifyDate,""String_Node_Str"");
}",0.7655703289013296
89093,"/** 
 * Click on the more actions link
 */
private void clickOnMoreActions(){
  Utils.mouseOver(row);
  WebElement moreAction=getMoreActionLink(row);
  if (moreAction != null) {
    Utils.waitFor(ExpectedConditions.elementToBeClickable(moreAction));
    Utils.mouseOver(moreAction);
    moreAction.click();
    waitForVisibilityOf(row.findElement(moreActionsPanelSelector));
  }
}","/** 
 * Click on the more actions link
 */
private void clickOnMoreActions(){
  Utils.mouseOver(row);
  WebElement moreAction=getMoreActionLink(row);
  if (moreAction != null) {
    Utils.mouseOver(moreAction);
    Utils.waitFor(ExpectedConditions.elementToBeClickable(moreAction));
    if (!moreAction.isDisplayed() || !moreAction.isEnabled()) {
      Utils.mouseOver(moreAction);
      Utils.waitFor(ExpectedConditions.elementToBeClickable(moreAction));
    }
    moreAction.click();
    waitForVisibilityOf(row.findElement(moreActionsPanelSelector));
  }
}",0.8093716719914803
89094,"/** 
 * selects incomplete records from saved searches 
 */
public void selectIncompleteRecordsSearch(){
  savedSearches.click();
  Utils.waitForVisibilityOf(incompleteRecords);
  incompleteRecords.click();
}","/** 
 * selects incomplete records from saved searches 
 */
public void selectIncompleteRecordsSearch(){
  Utils.waitForVisibilityOf(savedSearches);
  savedSearches.click();
  Utils.waitForVisibilityOf(incompleteRecords);
  incompleteRecords.click();
}",0.7956521739130434
89095,"/** 
 * click on the search button in order to start the search
 */
public void clickOnSearch(){
  searchButton.click();
  Utils.waitForVisibilityOf(resultsPageEnabled);
  Utils.webDriverWait(3);
  searchRecordsResults.render();
}","/** 
 * click on the search button in order to start the search
 */
public void clickOnSearch(){
  Utils.waitForVisibilityOf(searchButton);
  searchButton.click();
  Utils.waitForVisibilityOf(resultsPageEnabled);
  searchRecordsResults.render();
}",0.7547169811320755
89096,"/** 
 * get the search results names from the results tab
 * @return the list of results from the Name column
 */
public List<String> getResults(){
  List<String> recordsNames=new ArrayList<>();
  Utils.waitForVisibilityOf(resultsContainer);
  Utils.waitForVisibilityOf(By.cssSelector(SearchConstants.RESULTS_NAMES_SELECTOR));
  List<WebElement> results=webDriver.findElements(By.cssSelector(SearchConstants.RESULTS_NAMES_SELECTOR));
  System.out.println(""String_Node_Str"");
  for (  WebElement record : results) {
    String recordName=record.getText();
    recordsNames.add(recordName);
    System.out.println(recordName);
  }
  return recordsNames;
}","/** 
 * get the search results names from the results tab
 * @return the list of results from the Name column
 */
public List<String> getResults(){
  List<String> recordsNames=new ArrayList<>();
  Utils.waitFor(ExpectedConditions.visibilityOfAllElementsLocatedBy(By.cssSelector(SearchConstants.RESULTS_NAMES_SELECTOR)));
  List<WebElement> results=webDriver.findElements(By.cssSelector(SearchConstants.RESULTS_NAMES_SELECTOR));
  for (  WebElement record : results) {
    String recordName=record.getText();
    recordsNames.add(recordName);
  }
  return recordsNames;
}",0.6672117743254292
89097,"/** 
 * navigates to the Criteria tab from the Results tab
 */
public void navigateToCriteriaTab(){
  criteriaPage.click();
  Utils.waitForVisibilityOf(criteriaPageEnabled);
  Utils.webDriverWait(5);
  recordsSearchPage.render();
}","/** 
 * navigates to the Criteria tab from the Results tab
 */
public void navigateToCriteriaTab(){
  criteriaPage.click();
  Utils.waitForVisibilityOf(criteriaPageEnabled);
  recordsSearchPage.render();
}",0.9403669724770642
89098,"/** 
 * Click on search records link
 */
public RecordsSearch clickOnRecordsSearch(){
  search.click();
  Utils.webDriverWait(3);
  return recordsSearch.render();
}","/** 
 * Click on search records link
 */
public RecordsSearch clickOnRecordsSearch(){
  Utils.waitFor(ExpectedConditions.elementToBeClickable(search.getWrappedElement()));
  search.click();
  return recordsSearch.render();
}",0.6701030927835051
89099,"/** 
 * Click on the more actions link
 */
private void clickOnMoreActions(){
  Utils.mouseOver(row);
  WebElement moreAction=getMoreActionLink(row);
  if (moreAction != null) {
    Utils.waitFor(ExpectedConditions.elementToBeClickable(moreAction));
    Utils.mouseOver(moreAction);
    moreAction.click();
    waitForVisibilityOf(row.findElement(moreActionsPanelSelector));
  }
}","/** 
 * Click on the more actions link
 */
private void clickOnMoreActions(){
  Utils.mouseOver(row);
  WebElement moreAction=getMoreActionLink(row);
  if (moreAction != null) {
    Utils.waitFor(ExpectedConditions.elementToBeClickable(moreAction));
    Utils.mouseOver(moreAction);
    Utils.waitFor(ExpectedConditions.visibilityOf(moreAction));
    moreAction.click();
    waitForVisibilityOf(row.findElement(moreActionsPanelSelector));
  }
}",0.8519417475728155
89100,"/** 
 * Create top secret, confidential and unclassified records for search.
 */
@Test(groups={""String_Node_Str"",""String_Node_Str""},description=""String_Node_Str"",dependsOnGroups={""String_Node_Str""}) public void createClassifiedRecordsForSearch(){
  openPage(filePlan,RM_SITE_ID,createPathFrom(""String_Node_Str"",RECORD_CATEGORY_ONE,RECORD_FOLDER_SEARCH));
  filePlan.getToolbar().clickOnFile().clickOnElectronic().uploadFile(TOP_SECRET_RECORD_SEARCH);
  Utils.webDriverWait(2);
  filePlan.getToolbar().clickOnFile().clickOnElectronic().uploadFile(UNCLASSIFIED_RECORD_SEARCH);
  Utils.webDriverWait(2);
  filePlan.getToolbar().clickOnFile().clickOnElectronic().uploadFile(CONFIDENTIAL_RECORD_SEARCH);
  Utils.webDriverWait(2);
  filePlan.getRecord(TOP_SECRET_RECORD_SEARCH).clickOnAction(RecordActionsPanel.CLASSIFY,classifyContentDialog);
  classifyContentDialog.setLevel(TOP_SECRET_CLASSIFICATION_LEVEL_TEXT).setClassifiedBy(CLASSIFIED_BY).setAgency(CLASSIFICATION_AGENCY).addReason(CLASSIFICATION_REASON).clickOnClassify();
  filePlan.getRecord(TOP_SECRET_RECORD_SEARCH).hasIndicator(RecordIndicators.CLASSIFIED);
  Utils.webDriverWait(2);
  filePlan.getRecord(CONFIDENTIAL_RECORD_SEARCH).clickOnAction(RecordActionsPanel.CLASSIFY,classifyContentDialog);
  classifyContentDialog.setLevel(CONFIDENTIAL_CLASSIFICATION_LEVEL_TEXT).setClassifiedBy(CLASSIFIED_BY).setAgency(CLASSIFICATION_AGENCY).addReason(CLASSIFICATION_REASON).clickOnClassify();
  filePlan.getRecord(CONFIDENTIAL_RECORD_SEARCH).hasIndicator(RecordIndicators.CLASSIFIED);
  Utils.webDriverWait(2);
}","/** 
 * Create top secret, confidential and unclassified records for search.
 */
@Test(groups={""String_Node_Str"",""String_Node_Str""},description=""String_Node_Str"",dependsOnGroups={""String_Node_Str""}) public void createClassifiedRecordsForSearch(){
  openPage(filePlan,RM_SITE_ID,createPathFrom(""String_Node_Str"",RECORD_CATEGORY_ONE,RECORD_FOLDER_SEARCH));
  filePlan.getToolbar().clickOnFile().clickOnElectronic().uploadFile(TOP_SECRET_RECORD_SEARCH);
  filePlan.getToolbar().clickOnFile().clickOnElectronic().uploadFile(UNCLASSIFIED_RECORD_SEARCH);
  filePlan.getToolbar().clickOnFile().clickOnElectronic().uploadFile(CONFIDENTIAL_RECORD_SEARCH);
  filePlan.getRecord(TOP_SECRET_RECORD_SEARCH).clickOnAction(RecordActionsPanel.CLASSIFY,classifyContentDialog);
  classifyContentDialog.setLevel(TOP_SECRET_CLASSIFICATION_LEVEL_TEXT).setClassifiedBy(CLASSIFIED_BY).setAgency(CLASSIFICATION_AGENCY).addReason(CLASSIFICATION_REASON).clickOnClassify();
  filePlan.getRecord(TOP_SECRET_RECORD_SEARCH).hasIndicator(RecordIndicators.CLASSIFIED);
  filePlan.getRecord(CONFIDENTIAL_RECORD_SEARCH).clickOnAction(RecordActionsPanel.CLASSIFY,classifyContentDialog);
  classifyContentDialog.setLevel(CONFIDENTIAL_CLASSIFICATION_LEVEL_TEXT).setClassifiedBy(CLASSIFIED_BY).setAgency(CLASSIFICATION_AGENCY).addReason(CLASSIFICATION_REASON).clickOnClassify();
  filePlan.getRecord(CONFIDENTIAL_RECORD_SEARCH).hasIndicator(RecordIndicators.CLASSIFIED);
}",0.619079386257505
89101,"/** 
 * get the item map
 */
private Map<String,ListItem> getItemMap(){
  return Utils.retry(new Retry<Map<String,ListItem>>(){
    @Override public Map<String,ListItem> execute(){
      List<WebElement> rows=webDriver.findElements(rowsSelector);
      if (rows.size() == itemCount) {
        Map<String,ListItem> itemMap=new HashMap<String,ListItem>(rows.size());
        for (        WebElement row : rows) {
          ListItem item=listItemFactory.getItem(row);
          itemMap.put(item.getName(),item);
        }
        return itemMap;
      }
 else {
        throw new IllegalStateException(""String_Node_Str"" + itemCount + ""String_Node_Str""+ rows.size());
      }
    }
  }
,5);
}","/** 
 * get the item map
 */
private Map<String,ListItem> getItemMap(){
  waitForRows();
  List<WebElement> rows=webDriver.findElements(rowsSelector);
  Map<String,ListItem> itemMap=new HashMap<String,ListItem>(rows.size());
  for (  WebElement row : rows) {
    ListItem item=listItemFactory.getItem(row);
    itemMap.put(item.getName(),item);
  }
  return itemMap;
}",0.2556818181818182
89102,"/** 
 * @see org.alfresco.po.common.renderable.Renderable#render()
 */
@Override public <T extends Renderable>T render(){
  T result=super.render();
  String text=current.getText();
  String[] values=text.split(""String_Node_Str"");
  itemCount=Integer.parseInt(values[2]);
  if (itemCount != 0) {
    Utils.waitFor(ExpectedConditions.presenceOfAllElementsLocatedBy(rowsSelector));
  }
  return result;
}","/** 
 * @see org.alfresco.po.common.renderable.Renderable#render()
 */
@Override public <T extends Renderable>T render(){
  T result=super.render();
  waitForRows();
  return result;
}",0.621160409556314
89103,"/** 
 * Does the current row have the requested indicator?
 * @param indicator {String}
 * @return boolean
 */
public boolean hasIndicator(String indicator){
  WebElement indicatorElement=null;
  boolean result=false;
  try {
    indicatorElement=Utils.waitForFind(row,getIndicatorSelector(indicator));
  }
 catch (  NoSuchElementException e) {
  }
  if (indicatorElement != null) {
    result=true;
  }
  return result;
}","/** 
 * Does the current row have the requested indicator?
 * @param indicator {String}
 * @return boolean
 */
public boolean hasIndicator(String indicator){
  WebElement indicatorElement=null;
  boolean result=false;
  try {
    indicatorElement=Utils.waitForFind(row,getIndicatorSelector(indicator));
  }
 catch (  TimeoutException e) {
  }
  if (indicatorElement != null) {
    result=true;
  }
  return result;
}",0.9785202863961814
89104,"/** 
 * Helper method to get the action link
 */
private Link getActionLink(String actionName){
  Link result=null;
  try {
    WebElement link=Utils.waitForFind(row,getActionSelector(actionName));
    result=new Link(link);
  }
 catch (  NoSuchElementException e) {
  }
  return result;
}","/** 
 * Helper method to get the action link
 */
private Link getActionLink(String actionName){
  Link result=null;
  try {
    WebElement link=Utils.waitForFind(row,getActionSelector(actionName));
    result=new Link(link);
  }
 catch (  TimeoutException e) {
  }
  return result;
}",0.9685314685314684
89105,"/** 
 * Check how a document explicitly marked 'Unclassified' is presented. <p> <a href=""https://issues.alfresco.com/jira/browse/RM-2279"">RM-2279</a><pre> Given that a document has been marked as unclassified When I browse the document library Then no classification banner is shown for the document </pre>
 */
@Test(groups={""String_Node_Str""},description=""String_Node_Str"",dependsOnGroups={""String_Node_Str""}) public void unclassifiedDocumentPresentation(){
  openPage(documentLibrary,COLLAB_SITE_ID);
  Document document=documentLibrary.getDocument(UNCLASSIFIED_DOCUMENT);
  try {
    document.getBannerText(ContentBanner.CLASSIFICATION);
    fail(""String_Node_Str"");
  }
 catch (  NoSuchElementException e) {
  }
}","/** 
 * Check how a document explicitly marked 'Unclassified' is presented. <p> <a href=""https://issues.alfresco.com/jira/browse/RM-2279"">RM-2279</a><pre> Given that a document has been marked as unclassified When I browse the document library Then no classification banner is shown for the document </pre>
 */
@Test(groups={""String_Node_Str""},description=""String_Node_Str"",dependsOnGroups={""String_Node_Str""}) public void unclassifiedDocumentPresentation(){
  openPage(documentLibrary,COLLAB_SITE_ID);
  Document document=documentLibrary.getDocument(UNCLASSIFIED_DOCUMENT);
  try {
    document.getBannerText(ContentBanner.CLASSIFICATION);
    fail(""String_Node_Str"");
  }
 catch (  TimeoutException e) {
  }
}",0.9873949579831932
89106,"/** 
 * Reveal the user dropdown menu.
 * @return The user dropdown menu.
 * @throws IllegalStateException If the dropdown menu cannot be found.
 */
public UserDropdown revealDropdown(){
  try {
    WebElement dropdownButton=Utils.getWebDriver().findElement(HEADER_MENU_SELECTOR);
    dropdownButton.click();
  }
 catch (  NoSuchElementException e) {
    throw new IllegalStateException(""String_Node_Str"",e);
  }
  waitForVisibilityOf(LOGOUT_SELECTOR);
  return this;
}","/** 
 * Reveal the user dropdown menu.
 * @return The user dropdown menu.
 * @throws IllegalStateException If the dropdown menu cannot be found.
 */
public UserDropdown revealDropdown(){
  try {
    WebElement dropdownButton=Utils.getWebDriver().findElement(HEADER_MENU_SELECTOR);
    dropdownButton.click();
  }
 catch (  NoSuchElementException e) {
    throw new IllegalStateException(""String_Node_Str"",e);
  }
  return this;
}",0.930957683741648
89107,"public void logout(){
  WebElement logoutButton=Utils.getWebDriver().findElement(LOGOUT_SELECTOR);
  logoutButton.click();
}","public void logout(){
  WebElement logoutButton;
  int attempts=0;
  while (attempts < 3) {
    try {
      logoutButton=Utils.getWebDriver().findElement(LOGOUT_SELECTOR);
      if (logoutButton != null) {
        logoutButton.click();
        return;
      }
    }
 catch (    NoSuchElementException e) {
      Utils.getWebDriver().manage().timeouts().implicitlyWait(3,TimeUnit.SECONDS);
      revealDropdown();
      attempts=attempts + 1;
      if (attempts == 3) {
        fail(""String_Node_Str"");
      }
    }
  }
}",0.3844961240310077
89108,"/** 
 * WireMock config:
 */
@BeforeSuite public void setupWiremock(){
  if (wireMockEnabled) {
    try {
      WireMockServer wireMockServer=new WireMockServer(wireMockConfig().usingFilesUnderClasspath(WIREMOCK_PATH));
      wireMockServer.start();
      wireMockStarted=true;
    }
 catch (    FatalStartupException fatalStartupException) {
      Throwable exception=fatalStartupException;
      while (exception.getCause() != null) {
        exception=exception.getCause();
        if (exception instanceof BindException) {
          wireMockStarted=false;
          break;
        }
      }
    }
  }
}","/** 
 * WireMock config:
 */
@BeforeSuite public void setupWiremock(){
  if (wireMockEnabled) {
    try {
      WireMockServer wireMockServer=new WireMockServer(wireMockConfig().usingFilesUnderClasspath(WIREMOCK_PATH));
      wireMockServer.start();
      wireMockStarted=true;
    }
 catch (    FatalStartupException fatalStartupException) {
      Throwable exception=fatalStartupException;
      boolean rethrow=true;
      while (exception.getCause() != null) {
        exception=exception.getCause();
        if (exception instanceof BindException) {
          rethrow=false;
          break;
        }
      }
      if (rethrow) {
        throw fatalStartupException;
      }
    }
  }
}",0.9106317411402156
89109,"/** 
 * Click on (breadcrumb.size() - index) item on breadcrumb
 * @return - folder/category/file plan browse page
 * @param index - navigate index elements up
 */
public Renderable navigateUp(int index){
  List<WebElement> breadcrumb=webDriver.findElements(breadcrumbSelector);
  if (breadcrumb.size() - index < 0) {
    throw new RuntimeException(""String_Node_Str"");
  }
  WebElement parentLink=breadcrumb.get(breadcrumb.size() - index);
  parentLink.click();
  Utils.waitForStalenessOf(parentLink);
  return browsePage.render();
}","/** 
 * Click on (breadcrumb.size() - index) item on breadcrumb
 * @return - folder/category/file plan browse page
 * @param index - navigate index elements up
 */
public Renderable navigateUp(int index){
  List<WebElement> breadcrumb=webDriver.findElements(breadcrumbSelector);
  if (breadcrumb.size() - index < 0) {
    throw new RuntimeException(""String_Node_Str"");
  }
  WebElement parentLink=breadcrumb.get(breadcrumb.size() - index);
  parentLink.click();
  Utils.waitForStalenessOf(parentLink);
  Utils.waitForVisibilityOf(By.cssSelector(""String_Node_Str""));
  return browsePage.render();
}",0.9433628318584072
89110,"/** 
 * Check that if the reasons supplied on the classpath differ from those already persisted then a warning is logged and no change is made to the persisted reasons. <p> This test uses the underlying log4j implementation to insert a mock Appender, and tests this for the warning message. If the underlying logging framework is changed then this unit test will fail, and it may not be possible to/worth fixing.
 */
@Test public void previouslyStartedSystemShouldWarnIfConfiguredReasonsHaveChanged(){
  when(mockAttributeService.getAttribute(anyString(),anyString(),anyString())).thenReturn((Serializable)PLACEHOLDER_CLASSIFICATION_REASONS);
  when(mockClassificationServiceDAO.getConfiguredReasons()).thenReturn(ALTERNATIVE_CLASSIFICATION_REASONS);
  org.apache.log4j.Logger log4jLogger=org.apache.log4j.Logger.getLogger(ClassificationServiceBootstrap.class);
  log4jLogger.addAppender(mockAppender);
  Level normalLevel=log4jLogger.getLevel();
  log4jLogger.setLevel(Level.WARN);
  classificationServiceBootstrap.initConfiguredClassificationReasons();
  log4jLogger.setLevel(normalLevel);
  verify(mockAttributeService,never()).setAttribute(any(Serializable.class),anyString(),anyString(),anyString());
  verify(mockAppender).doAppend(loggingEventCaptor.capture());
  List<LoggingEvent> loggingEvents=loggingEventCaptor.getAllValues();
  Stream<String> messages=loggingEvents.stream().map(event -> event.getRenderedMessage());
  String expectedMessage=""String_Node_Str"";
  assertTrue(""String_Node_Str"",messages.anyMatch(message -> message == expectedMessage));
}","/** 
 * Check that if the reasons supplied on the classpath differ from those already persisted then a warning is logged and no change is made to the persisted reasons. <p> This test uses the underlying log4j implementation to insert a mock Appender, and tests this for the warning message. If the underlying logging framework is changed then this unit test will fail, and it may not be possible to/worth fixing.
 */
@Test public void previouslyStartedSystemShouldWarnIfConfiguredReasonsHaveChanged(){
  when(mockAttributeService.getAttribute(anyString(),anyString(),anyString())).thenReturn((Serializable)PLACEHOLDER_CLASSIFICATION_REASONS);
  when(mockClassificationServiceDAO.getConfiguredReasons()).thenReturn(ALTERNATIVE_CLASSIFICATION_REASONS);
  org.apache.log4j.Logger log4jLogger=org.apache.log4j.Logger.getLogger(ClassificationServiceBootstrap.class);
  log4jLogger.addAppender(mockAppender);
  Level normalLevel=log4jLogger.getLevel();
  log4jLogger.setLevel(Level.WARN);
  classificationServiceBootstrap.initConfiguredClassificationReasons();
  log4jLogger.setLevel(normalLevel);
  verify(mockAttributeService,never()).setAttribute(any(Serializable.class),anyString(),anyString(),anyString());
  verify(mockAppender).doAppend(loggingEventCaptor.capture());
  List<LoggingEvent> loggingEvents=loggingEventCaptor.getAllValues();
  Stream<String> messages=loggingEvents.stream().map(LoggingEvent::getRenderedMessage);
  String expectedMessage=""String_Node_Str"";
  assertTrue(""String_Node_Str"",messages.anyMatch(message -> expectedMessage.equals(message)));
}",0.901021711366539
89111,"/** 
 * Open this page, login to Share if required.
 * @param server        base server URL
 * @param userName      user name
 * @param password      password
 * @param context       context
 * @return {@link SharePage} rendered page
 */
public SharePage open(String server,String userName,String password,String... context){
  String url=server + getPageURL(context);
  String previousUser=currentLoggedInUser.get();
  if (previousUser != null && !userName.equals(previousUser)) {
    sharePageNavigation.openUserDropdownMenu().logout();
  }
  webDriver.get(url);
  if (webDriver.getTitle().contains(""String_Node_Str"")) {
    loginPage.render();
    loginPage.setUsername(userName).setPassword(password).clickOnLoginButton();
    currentLoggedInUser.set(userName);
  }
  return this.render();
}","/** 
 * Open this page, login to Share if required.
 * @param server        base server URL
 * @param userName      user name
 * @param password      password
 * @param context       context
 * @return {@link SharePage} rendered page
 */
public SharePage open(String server,String userName,String password,String... context){
  String url=server + getPageURL(context);
synchronized (CURRENT_LOGGED_IN_USER_LOCK) {
    if (currentLoggedInUser != null && !userName.equals(currentLoggedInUser)) {
      sharePageNavigation.openUserDropdownMenu().logout();
    }
    webDriver.get(url);
    if (webDriver.getTitle().contains(""String_Node_Str"")) {
      loginPage.render();
      loginPage.setUsername(userName).setPassword(password).clickOnLoginButton();
      currentLoggedInUser=userName;
    }
  }
  return this.render();
}",0.8571428571428571
89112,"/** 
 * Click on action
 */
private <T extends Renderable>T clickOnAction(String actionName,T renderable,boolean waitForActionStaleness){
  Link action=getActionLink(actionName);
  Utils.mouseOver(action);
  action.click();
  if (waitForActionStaleness == true) {
    waitForStalenessOf(action);
  }
  return renderable.render();
}","/** 
 * Click on action
 */
private <T extends Renderable>T clickOnAction(String actionName,T renderable,boolean waitForActionStaleness){
  Link action=getActionLink(actionName);
  Utils.mouseOver(action);
  action.click();
  if (waitForActionStaleness == true) {
    waitForStalenessOf(action);
  }
  if (renderable == null) {
    return null;
  }
  return renderable.render();
}",0.9310829817158932
89113,"/** 
 * Is action clickable 
 */
public boolean isActionClickable(String actionName){
  return getActionLink(actionName).isEnabled();
}","/** 
 * Is action clickable
 */
public boolean isActionClickable(String actionName){
  return getActionLink(actionName).isEnabled();
}",0.9962825278810408
89114,"/** 
 * Create a document that is locked for editing. 
 */
@Test(groups={""String_Node_Str"",""String_Node_Str""},description=""String_Node_Str"",dependsOnGroups={""String_Node_Str""}) public void createLockedDocument(){
  openPage(documentLibrary,COLLAB_SITE_ID);
  documentLibrary.getToolbar().clickOnFile().uploadFile(LOCKED_DOCUMENT);
  documentLibrary.getDocument(LOCKED_DOCUMENT).clickOnLink().getDocumentActionsPanel().clickOnAction(DocumentActions.EDIT_OFFLINE);
}","/** 
 * Create a document that is locked for editing. 
 */
@Test(groups={""String_Node_Str"",""String_Node_Str""},description=""String_Node_Str"",dependsOnGroups={""String_Node_Str""}) public void createLockedDocument(){
  openPage(documentLibrary,COLLAB_SITE_ID);
  documentLibrary.getToolbar().clickOnFile().uploadFile(LOCKED_DOCUMENT);
  documentLibrary.getDocument(LOCKED_DOCUMENT).clickOnLink().getDocumentActionsPanel().clickOnActionAndDontRender(DocumentActions.EDIT_OFFLINE);
  openPage(documentLibrary,COLLAB_SITE_ID);
}",0.9421319796954316
89115,"/** 
 * retry execution
 */
public abstract T execute();","/** 
 * retry execution
 */
T execute();",0.8333333333333334
89116,"/** 
 * Create temp file TODO .. support multiple mimetypes .. build files with real size content
 * @param name file name
 * @return {@link File} file
 */
public static File createTempFile(final String name){
  try {
    File file=File.createTempFile(name,""String_Node_Str"");
    try (OutputStreamWriter writer=new OutputStreamWriter(new FileOutputStream(file),Charset.forName(""String_Node_Str"").newEncoder())){
      writer.write(""String_Node_Str"");
    }
     return file;
  }
 catch (  Exception exception) {
    throw new RuntimeException(""String_Node_Str"",exception);
  }
}","/** 
 * Create temp file TODO .. support multiple mimetypes .. build files with real size content
 * @param name file name
 * @return {@link File} file
 */
public static File createTempFile(final String name){
  try {
    final File file=File.createTempFile(name,""String_Node_Str"");
    try (FileOutputStream fos=new FileOutputStream(file);OutputStreamWriter writer=new OutputStreamWriter(fos,Charset.forName(""String_Node_Str"").newEncoder())){
      writer.write(""String_Node_Str"");
    }
     return file;
  }
 catch (  Exception exception) {
    throw new RuntimeException(""String_Node_Str"",exception);
  }
}",0.9301934398654332
89117,"/** 
 * Helper method to retry, ignoring failures until success or all retries are used up
 * @param retry     retry execution
 * @param count     number of retries
 * @return T        result of retry execution
 */
public static final <T>T retry(Retry<T> retry,int count){
  T result;
  int attempt=0;
  while (true) {
    try {
      result=retry.execute();
      break;
    }
 catch (    Exception exception) {
      if (attempt >= count) {
        throw exception;
      }
    }
  }
  return result;
}","/** 
 * Helper method to retry the provided   {@link Retry code block}, ignoring failures until either the code block completes successfully or the maximum number of retries has been reached. are used up
 * @param < T >       the return type from the code block.
 * @param retry     a code block to execute.
 * @param count     maximum number of retries.
 * @return          result of the code block.
 */
public static final <T>T retry(Retry<T> retry,int count){
  int attempt=0;
  while (true) {
    try {
      return retry.execute();
    }
 catch (    Exception exception) {
      attempt++;
      if (attempt >= count) {
        throw exception;
      }
    }
  }
}",0.6240409207161125
89118,"/** 
 * Set the classification level.
 * @param levelId The label on the classification level dropdown button.
 * @return The dialog to allow chaining of actions.
 */
public ClassifyContentDialog setLevel(String levelId){
  levelSelectButton.click();
  String selector=""String_Node_Str"" + levelId + ""String_Node_Str"";
  WebElement level=Utils.retry(new Retry<WebElement>(){
    @Override public WebElement execute(){
      return levelsMenu.findElement(By.cssSelector(selector));
    }
  }
,5);
  level.click();
  return this;
}","/** 
 * Set the classification level.
 * @param levelId The label on the classification level dropdown button.
 * @return The dialog to allow chaining of actions.
 */
public ClassifyContentDialog setLevel(String levelId){
  levelSelectButton.click();
  final String selector=""String_Node_Str"" + levelId + ""String_Node_Str"";
  WebElement level=Utils.retry(new Retry<WebElement>(){
    @Override public WebElement execute(){
      return levelsMenu.findElement(By.cssSelector(selector));
    }
  }
,5);
  level.click();
  return this;
}",0.9943502824858758
89119,"/** 
 * Main test execution
 */
@Test(groups={""String_Node_Str""},description=""String_Node_Str"",dependsOnGroups={""String_Node_Str""}) public void classifyRecord(){
  openPage(filePlan,RM_SITE_ID,""String_Node_Str"").navigateTo(RECORD_CATEGORY_ONE,SUB_RECORD_CATEGORY_NAME,RECORD_FOLDER_ONE);
  assertTrue(filePlan.getRecord(RECORD).isActionClickable(RecordActions.CLASSIFY));
  filePlan.getRecord(RECORD).clickOnLink();
  assertTrue(recordDetails.getRecordActionsPanel().isActionClickable(RecordActionsPanel.CLASSIFY));
  recordDetails.navigateUp();
  Record nonElectronicRecord=filePlan.getRecord(NON_ELECTRONIC_RECORD);
  assertNotNull(nonElectronicRecord);
  assertTrue(recordDetails.getRecordActionsPanel().isActionClickable(RecordActions.CLASSIFY));
  nonElectronicRecord.clickOnLink();
  assertTrue(recordDetails.getRecordActionsPanel().isActionsClickable(RecordActionsPanel.CLASSIFY));
}","/** 
 * Main test execution
 */
@Test(groups={""String_Node_Str""},description=""String_Node_Str"",dependsOnGroups={""String_Node_Str"",""String_Node_Str"",""String_Node_Str""}) public void classifyRecord(){
  openPage(filePlan,RM_SITE_ID,""String_Node_Str"").navigateTo(RECORD_CATEGORY_ONE,SUB_RECORD_CATEGORY_NAME,RECORD_FOLDER_ONE);
  assertTrue(filePlan.getRecord(RECORD).isActionClickable(RecordActions.CLASSIFY));
  filePlan.getRecord(RECORD).clickOnLink();
  assertTrue(recordDetails.getRecordActionsPanel().isActionClickable(RecordActionsPanel.CLASSIFY));
  recordDetails.navigateUp();
  Record nonElectronicRecord=filePlan.getRecord(NON_ELECTRONIC_RECORD);
  assertNotNull(nonElectronicRecord);
  assertTrue(recordDetails.getRecordActionsPanel().isActionClickable(RecordActions.CLASSIFY));
  nonElectronicRecord.clickOnLink();
  assertTrue(recordDetails.getRecordActionsPanel().isActionsClickable(RecordActionsPanel.CLASSIFY));
}",0.9801762114537445
89120,"/** 
 * Regression test execution
 */
@Test(groups={""String_Node_Str"",""String_Node_Str""},description=""String_Node_Str"") public void createFilePlan(){
  openPage(userDashboardPage);
  if (!mySitesDashlet.siteExists(COLLAB_SITE_ID)) {
    mySitesDashlet.clickOnCreateSite().setSiteName(COLLAB_SITE_NAME).setSiteURL(COLLAB_SITE_ID).setSiteDescription(DESCRIPTION).clickOnOk();
  }
  siteDashboard.getNavigation().clickOnDocumentLibrary().getToolbar().clickOnFile().uploadFile(DOCUMENT);
}","/** 
 * Regression test execution
 */
@Test(groups={""String_Node_Str""},description=""String_Node_Str"") public void createFilePlan(){
  openPage(userDashboardPage);
  if (!mySitesDashlet.siteExists(COLLAB_SITE_ID)) {
    mySitesDashlet.clickOnCreateSite().setSiteName(COLLAB_SITE_NAME).setSiteURL(COLLAB_SITE_ID).setSiteDescription(DESCRIPTION).clickOnOk();
  }
  siteDashboard.getNavigation().clickOnDocumentLibrary().getToolbar().clickOnFile().uploadFile(DOCUMENT);
}",0.98109243697479
89121,"/** 
 * Regression test execution
 */
@Test(groups={""String_Node_Str"",""String_Node_Str""},description=""String_Node_Str"") public void createFilePlan(){
  openPage(userDashboardPage).getMySitesDashlet().clickOnRMSite(RM_SITE_ID).getNavigation().clickOnFilePlan();
  createCategoryAndClickOnLink(RECORD_CATEGORY_ONE,true);
  createCategoryAndClickOnLink(SUB_RECORD_CATEGORY_NAME,true);
  createRecordFolderAndClickOnLink(RECORD_FOLDER_TWO,false);
  createRecordFolderAndClickOnLink(RECORD_FOLDER_ONE,true);
  filePlan.getToolbar().clickOnFile().clickOnNonElectronic().setName(NON_ELECTRONIC_RECORD).setTitle(TITLE).clickOnSave();
  Record nonElectronicRecord=filePlan.getRecord(NON_ELECTRONIC_RECORD);
  assertNotNull(nonElectronicRecord);
  filePlan.getToolbar().clickOnFile().clickOnElectronic().uploadFile(RECORD);
  Record record=filePlan.getRecord(RECORD);
  assertNotNull(record);
  declareInplaceRecord();
  unfiledRecords.getToolbar().clickOnNewUnfiledRecordFolder().setName(UNFILED_RECORD_FOLDER).setTitle(TITLE).clickOnSave();
  UnfiledRecordFolder unfiledRecordFolder=unfiledRecords.getList().get(UNFILED_RECORD_FOLDER,UnfiledRecordFolder.class);
  assertNotNull(unfiledRecordFolder);
  unfiledRecords.getFilterPanel().clickOnHolds();
  createHold(HOLD1);
  createHold(HOLD2);
}","/** 
 * Integration test execution
 */
@Test(groups={""String_Node_Str""},description=""String_Node_Str"") public void createFilePlan(){
  openPage(userDashboardPage).getMySitesDashlet().clickOnRMSite(RM_SITE_ID).getNavigation().clickOnFilePlan();
  createCategoryAndClickOnLink(RECORD_CATEGORY_ONE,true);
  createCategoryAndClickOnLink(SUB_RECORD_CATEGORY_NAME,true);
  createRecordFolderAndClickOnLink(RECORD_FOLDER_TWO,false);
  createRecordFolderAndClickOnLink(RECORD_FOLDER_ONE,true);
  filePlan.getToolbar().clickOnFile().clickOnNonElectronic().setName(NON_ELECTRONIC_RECORD).setTitle(TITLE).clickOnSave();
  Record nonElectronicRecord=filePlan.getRecord(NON_ELECTRONIC_RECORD);
  assertNotNull(nonElectronicRecord);
  filePlan.getToolbar().clickOnFile().clickOnElectronic().uploadFile(RECORD);
  Record record=filePlan.getRecord(RECORD);
  assertNotNull(record);
}",0.7983271375464684
89122,"/** 
 * Create RM site for Integration tests
 */
@Test(groups={""String_Node_Str"",""String_Node_Str""},description=""String_Node_Str"") public void createRMSite(){
  openPage(userDashboardPage);
  if (!mySitesDashlet.siteExists(RM_SITE_ID)) {
    mySitesDashlet.clickOnCreateSite().setSiteType(SiteType.RM_SITE).clickOnOk();
    openPage(userDashboardPage);
  }
  Assert.assertTrue(mySitesDashlet.siteExists(RM_SITE_ID));
  mySitesDashlet.clickOnRMSite(RM_SITE_ID);
}","/** 
 * Create RM site for Integration tests
 */
@Test(groups={""String_Node_Str""},description=""String_Node_Str"") public void createRMSite(){
  openPage(userDashboardPage);
  if (!mySitesDashlet.siteExists(RM_SITE_ID)) {
    mySitesDashlet.clickOnCreateSite().setSiteType(SiteType.RM_SITE).clickOnOk();
    openPage(userDashboardPage);
  }
  Assert.assertTrue(mySitesDashlet.siteExists(RM_SITE_ID));
  mySitesDashlet.clickOnRMSite(RM_SITE_ID);
}",0.9801324503311258
89123,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.hold.HoldService#deleteHold(org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public void deleteHold(final NodeRef hold){
  ParameterCheck.mandatory(""String_Node_Str"",hold);
  if (!isHold(hold)) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + hold.toString() + ""String_Node_Str"");
  }
  List<NodeRef> held=AuthenticationUtil.runAsSystem(new RunAsWork<List<NodeRef>>(){
    @Override public List<NodeRef> doWork(){
      return getHeld(hold);
    }
  }
);
  List<String> heldNames=new ArrayList<String>();
  for (  NodeRef nodeRef : held) {
    try {
      if (permissionService.hasPermission(nodeRef,RMPermissionModel.FILING) == AccessStatus.DENIED) {
        heldNames.add((String)nodeService.getProperty(nodeRef,ContentModel.PROP_NAME));
      }
    }
 catch (    AccessDeniedException ade) {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
  }
  if (heldNames.size() > 0) {
    StringBuilder sb=new StringBuilder();
    for (    String name : heldNames) {
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(name);
      sb.append(""String_Node_Str"");
    }
    throw new AlfrescoRuntimeException(""String_Node_Str"" + sb.toString());
  }
  nodeService.deleteNode(hold);
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.hold.HoldService#deleteHold(org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public void deleteHold(final NodeRef hold){
  ParameterCheck.mandatory(""String_Node_Str"",hold);
  if (!isHold(hold)) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + hold.toString() + ""String_Node_Str"");
  }
  List<NodeRef> held=AuthenticationUtil.runAsSystem(new RunAsWork<List<NodeRef>>(){
    @Override public List<NodeRef> doWork(){
      return getHeld(hold);
    }
  }
);
  List<String> heldNames=new ArrayList<String>();
  for (  NodeRef nodeRef : held) {
    try {
      if (permissionService.hasPermission(nodeRef,RMPermissionModel.FILING) == AccessStatus.DENIED) {
        heldNames.add((String)nodeService.getProperty(nodeRef,ContentModel.PROP_NAME));
      }
    }
 catch (    AccessDeniedException ade) {
      throw new AlfrescoRuntimeException(""String_Node_Str"",ade);
    }
  }
  if (heldNames.size() > 0) {
    StringBuilder sb=new StringBuilder();
    for (    String name : heldNames) {
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(name);
      sb.append(""String_Node_Str"");
    }
    throw new AlfrescoRuntimeException(""String_Node_Str"" + sb.toString());
  }
  nodeService.deleteNode(hold);
}",0.9984697781178272
89124,"/** 
 * Puts the given key and value to the json object
 * @param jsonObject The json object
 * @param key The key
 * @param value The value
 */
public static void putValuetoJSONObject(JSONObject jsonObject,String key,Object value){
  mandatory(""String_Node_Str"",jsonObject);
  mandatoryString(""String_Node_Str"",key);
  mandatory(""String_Node_Str"",value);
  try {
    jsonObject.put(key,value);
  }
 catch (  JSONException error) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"" + key + ""String_Node_Str""+ value+ ""String_Node_Str"");
  }
}","/** 
 * Puts the given key and value to the json object
 * @param jsonObject The json object
 * @param key The key
 * @param value The value
 */
public static void putValuetoJSONObject(JSONObject jsonObject,String key,Object value){
  mandatory(""String_Node_Str"",jsonObject);
  mandatoryString(""String_Node_Str"",key);
  mandatory(""String_Node_Str"",value);
  try {
    jsonObject.put(key,value);
  }
 catch (  JSONException error) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"" + key + ""String_Node_Str""+ value+ ""String_Node_Str"",error);
  }
}",0.9947460595446584
89125,"/** 
 * Helper method that indicates whether a property is considered record metadata or not.
 * @param property  property
 * @return boolea   true if record metadata, false otherwise
 */
private boolean isRecordMetadata(NodeRef filePlan,QName property){
  boolean result=false;
  ClassDefinition parent=null;
  PropertyDefinition def=dictionaryService.getProperty(property);
  if (def != null) {
    parent=def.getContainerClass();
  }
  if (parent != null && TYPE_NON_ELECTRONIC_DOCUMENT.equals(parent.getName())) {
    result=false;
  }
 else {
    result=ArrayUtils.contains(RECORD_MODEL_URIS,property.getNamespaceURI());
    if (!result && !ArrayUtils.contains(NON_RECORD_MODEL_URIS,property.getNamespaceURI())) {
      if (parent != null && parent.isAspect()) {
        result=getRecordMetadataAspects(filePlan).contains(parent.getName());
      }
    }
  }
  return result;
}","/** 
 * Helper method that indicates whether a property is considered record metadata or not.
 * @param property  property
 * @return boolea   true if record metadata, false otherwise
 */
private boolean isRecordMetadata(NodeRef filePlan,QName property){
  boolean result=false;
  ClassDefinition parent=null;
  PropertyDefinition def=dictionaryService.getProperty(property);
  if (def != null) {
    parent=def.getContainerClass();
  }
  if (parent != null && TYPE_NON_ELECTRONIC_DOCUMENT.equals(parent.getName())) {
    result=false;
  }
 else {
    result=RECORD_MODEL_URIS.contains(property.getNamespaceURI());
    if (!result && !ArrayUtils.contains(NON_RECORD_MODEL_URIS,property.getNamespaceURI())) {
      if (parent != null && parent.isAspect()) {
        result=getRecordMetadataAspects(filePlan).contains(parent.getName());
      }
    }
  }
  return result;
}",0.982316029663434
89126,"/** 
 * Process properties map before returning as frozen state.
 * @param properties                                        properties map
 * @return {@link Map}<  {@link QName},   {@link Serializable}> processed property map
 */
protected Map<QName,Serializable> processProperties(NodeRef version,Map<QName,Serializable> properties){
  Map<QName,Serializable> cloneProperties=new HashMap<QName,Serializable>(properties);
  properties.put(ContentModel.PROP_NAME,properties.get(RecordsManagementModel.PROP_ORIGIONAL_NAME));
  for (  QName property : cloneProperties.keySet()) {
    if (!PROP_RECORDABLE_VERSION_POLICY.equals(property) && !PROP_FILE_PLAN.equals(property) && (recordService.isRecordMetadataProperty(property) || ArrayUtils.contains(RecordServiceImpl.RECORD_MODEL_URIS,property.getNamespaceURI()))) {
      properties.remove(property);
    }
  }
  processVersionProperties(version,properties);
  return properties;
}","/** 
 * Process properties map before returning as frozen state.
 * @param properties                                        properties map
 * @return {@link Map}<  {@link QName},   {@link Serializable}> processed property map
 */
protected Map<QName,Serializable> processProperties(NodeRef version,Map<QName,Serializable> properties){
  Map<QName,Serializable> cloneProperties=new HashMap<QName,Serializable>(properties);
  properties.put(ContentModel.PROP_NAME,properties.get(RecordsManagementModel.PROP_ORIGIONAL_NAME));
  for (  QName property : cloneProperties.keySet()) {
    if (!PROP_RECORDABLE_VERSION_POLICY.equals(property) && !PROP_FILE_PLAN.equals(property) && (recordService.isRecordMetadataProperty(property) || RECORD_MODEL_URIS.contains(property.getNamespaceURI()))) {
      properties.remove(property);
    }
  }
  processVersionProperties(version,properties);
  return properties;
}",0.8279628618241398
89127,"/** 
 * Process frozen aspects.
 * @param aspects                       aspect set
 * @return {@link Set}<  {@link QName}>   processed aspect set
 */
protected Set<QName> processAspects(Set<QName> aspects){
  Set<QName> result=new HashSet<QName>(aspects);
  result.remove(ASPECT_VERSION);
  result.remove(ASPECT_RECORDED_VERSION);
  for (  QName aspect : aspects) {
    if (!ASPECT_VERSIONABLE.equals(aspect) && (recordService.isRecordMetadataAspect(aspect) || ArrayUtils.contains(RecordServiceImpl.RECORD_MODEL_URIS,aspect.getNamespaceURI()))) {
      result.remove(aspect);
    }
  }
  return result;
}","/** 
 * Process frozen aspects.
 * @param aspects                       aspect set
 * @return {@link Set}<  {@link QName}>   processed aspect set
 */
protected Set<QName> processAspects(Set<QName> aspects){
  Set<QName> result=new HashSet<QName>(aspects);
  result.remove(ASPECT_VERSION);
  result.remove(ASPECT_RECORDED_VERSION);
  for (  QName aspect : aspects) {
    if (!ASPECT_VERSIONABLE.equals(aspect) && (recordService.isRecordMetadataAspect(aspect) || RECORD_MODEL_URIS.contains(aspect.getNamespaceURI()))) {
      result.remove(aspect);
    }
  }
  return result;
}",0.9584393553859204
89128,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.compatibility.ModulePatchComponent#executePatch()
 */
@Override protected void executePatch(){
  if (!nodeService.exists(RM_CONFIG)) {
    return;
  }
  if (!nodeService.exists(newBehaviorScriptsFolder)) {
    if (LOGGER.isDebugEnabled()) {
      LOGGER.debug(""String_Node_Str"");
    }
    String newBehaviorScriptsFolderName=""String_Node_Str"";
    String newBehaviorScriptsNodeUUID=""String_Node_Str"";
    String newBehaviorScriptsAssocQName=""String_Node_Str"";
    Map<QName,Serializable> newBehaviorScriptsFolderProps=new HashMap<QName,Serializable>();
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_NODE_UUID,newBehaviorScriptsNodeUUID);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_NAME,newBehaviorScriptsFolderName);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_TITLE,newBehaviorScriptsFolderName);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
    newBehaviorScriptsFolder=nodeService.createNode(RM_CONFIG,ContentModel.ASSOC_CONTAINS,QName.createQName(NamespaceService.CONTENT_MODEL_PREFIX,newBehaviorScriptsAssocQName),ContentModel.TYPE_FOLDER,newBehaviorScriptsFolderProps).getChildRef();
  }
  if (nodeService.exists(OLD_BEHAVIOR_SCRIPTS_FOLDER)) {
    AuthenticationUtil.runAs(new RunAsWork<Object>(){
      public Object doWork(){
        RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
          public Void execute() throws Throwable {
            nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
            List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
            if (oldBehaviorScripts != null && !oldBehaviorScripts.isEmpty()) {
              if (LOGGER.isDebugEnabled()) {
                LOGGER.debug(""String_Node_Str"");
              }
              for (              FileInfo script : oldBehaviorScripts) {
                fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
                if (LOGGER.isDebugEnabled()) {
                  LOGGER.debug(""String_Node_Str"" + script.getName());
                }
              }
            }
            return null;
          }
        }
;
        retryingTransactionHelper.doInTransaction(callback);
        return null;
      }
    }
,AuthenticationUtil.getSystemUserName());
  }
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.compatibility.ModulePatchComponent#executePatch()
 */
@Override protected void executePatch(){
  if (!nodeService.exists(RM_CONFIG)) {
    return;
  }
  if (!nodeService.exists(newBehaviorScriptsFolder)) {
    if (LOGGER.isDebugEnabled()) {
      LOGGER.debug(""String_Node_Str"");
    }
    String newBehaviorScriptsFolderName=""String_Node_Str"";
    String newBehaviorScriptsNodeUUID=""String_Node_Str"";
    String newBehaviorScriptsAssocQName=""String_Node_Str"";
    Map<QName,Serializable> newBehaviorScriptsFolderProps=new HashMap<QName,Serializable>();
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_NODE_UUID,newBehaviorScriptsNodeUUID);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_NAME,newBehaviorScriptsFolderName);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_TITLE,newBehaviorScriptsFolderName);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
    newBehaviorScriptsFolder=nodeService.createNode(RM_CONFIG,ContentModel.ASSOC_CONTAINS,QName.createQName(NamespaceService.CONTENT_MODEL_PREFIX,newBehaviorScriptsAssocQName),ContentModel.TYPE_FOLDER,newBehaviorScriptsFolderProps).getChildRef();
  }
  if (nodeService.exists(OLD_BEHAVIOR_SCRIPTS_FOLDER)) {
    AuthenticationUtil.runAs(new RunAsWork<Object>(){
      @SuppressWarnings(""String_Node_Str"") public Object doWork(){
        RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
          public Void execute() throws Throwable {
            nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
            List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
            if (oldBehaviorScripts != null && !oldBehaviorScripts.isEmpty()) {
              if (LOGGER.isDebugEnabled()) {
                LOGGER.debug(""String_Node_Str"");
              }
              for (              FileInfo script : oldBehaviorScripts) {
                fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
                if (LOGGER.isDebugEnabled()) {
                  LOGGER.debug(""String_Node_Str"" + script.getName());
                }
              }
            }
            return null;
          }
        }
;
        retryingTransactionHelper.doInTransaction(callback);
        return null;
      }
    }
,AuthenticationUtil.getSystemUserName());
  }
}",0.9927036087556697
89129,"public Object doWork(){
  RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
    public Void execute() throws Throwable {
      nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
      List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
      if (oldBehaviorScripts != null && !oldBehaviorScripts.isEmpty()) {
        if (LOGGER.isDebugEnabled()) {
          LOGGER.debug(""String_Node_Str"");
        }
        for (        FileInfo script : oldBehaviorScripts) {
          fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
          if (LOGGER.isDebugEnabled()) {
            LOGGER.debug(""String_Node_Str"" + script.getName());
          }
        }
      }
      return null;
    }
  }
;
  retryingTransactionHelper.doInTransaction(callback);
  return null;
}","@SuppressWarnings(""String_Node_Str"") public Object doWork(){
  RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
    public Void execute() throws Throwable {
      nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
      List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
      if (oldBehaviorScripts != null && !oldBehaviorScripts.isEmpty()) {
        if (LOGGER.isDebugEnabled()) {
          LOGGER.debug(""String_Node_Str"");
        }
        for (        FileInfo script : oldBehaviorScripts) {
          fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
          if (LOGGER.isDebugEnabled()) {
            LOGGER.debug(""String_Node_Str"" + script.getName());
          }
        }
      }
      return null;
    }
  }
;
  retryingTransactionHelper.doInTransaction(callback);
  return null;
}",0.981453634085213
89130,"/** 
 * Set the namespace service
 */
public void setTransactionService(TransactionService transactionService){
  this.transactionService=transactionService;
}","/** 
 * Set the transaction service
 */
public void setTransactionService(TransactionService transactionService){
  this.transactionService=transactionService;
}",0.9625
89131,"/** 
 * @return  records management admin service
 */
public RecordsManagementAdminService getRecordsManagementAdminService(){
  return recordsManagementAdminService;
}","/** 
 * @return  records management admin service
 */
protected RecordsManagementAdminService getRecordsManagementAdminService(){
  return recordsManagementAdminService;
}",0.967551622418879
89132,"/** 
 * Gets the node reference of the custom model
 * @param uri The URI of the model namespace
 * @return The node reference of the custom model
 */
protected NodeRef getCustomModelRef(String uri){
  if ((uri.equals(""String_Node_Str"")) || (uri.equals(RecordsManagementModel.RM_CUSTOM_URI))) {
    return RM_CUSTOM_MODEL_NODE_REF;
  }
 else {
    List<NodeRef> modelRefs=getDictionaryRepositoryBootstrap().getModelRefs();
    for (    NodeRef modelRef : modelRefs) {
      try {
        M2Model model=readCustomContentModel(modelRef);
        for (        M2Namespace namespace : model.getNamespaces()) {
          if (namespace.getUri().equals(uri)) {
            return modelRef;
          }
        }
      }
 catch (      DictionaryException de) {
        logger.warn(""String_Node_Str"" + modelRef + ""String_Node_Str""+ uri+ ""String_Node_Str""+ de);
      }
    }
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_CUSTOM_MODEL_NOT_FOUND,uri));
  }
}","/** 
 * Gets the node reference of the custom model
 * @param uri The URI of the model namespace
 * @return The node reference of the custom model
 */
protected NodeRef getCustomModelRef(String uri){
  if ((uri.equals(""String_Node_Str"")) || (uri.equals(RecordsManagementModel.RM_CUSTOM_URI))) {
    return RM_CUSTOM_MODEL_NODE_REF;
  }
 else {
    List<NodeRef> modelRefs=getDictionaryRepositoryBootstrap().getModelRefs();
    for (    NodeRef modelRef : modelRefs) {
      try {
        M2Model model=readCustomContentModel(modelRef);
        for (        M2Namespace namespace : model.getNamespaces()) {
          if (namespace.getUri().equals(uri)) {
            return modelRef;
          }
        }
      }
 catch (      DictionaryException de) {
        logger.warn(""String_Node_Str"" + modelRef + ""String_Node_Str""+ uri+ ""String_Node_Str"",de);
      }
    }
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_CUSTOM_MODEL_NOT_FOUND,uri));
  }
}",0.9984431759211208
89133,"/** 
 * @see org.springframework.extensions.webscripts.DeclarativeWebScript#executeImpl(org.springframework.extensions.webscripts.WebScriptRequest,org.springframework.extensions.webscripts.Status,org.springframework.extensions.webscripts.Cache)
 */
@Override protected Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  Map<String,Object> model=new HashMap<String,Object>(1,1.0f);
  try {
    String dataSetId=req.getServiceMatch().getTemplateVars().get(ARG_DATA_SET_ID);
    if (StringUtils.isBlank(dataSetId)) {
      throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"");
    }
    if (!dataSetService.existsDataSet(dataSetId)) {
      throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + dataSetId + ""String_Node_Str""+ ""String_Node_Str"");
    }
    String siteName=req.getParameter(ARG_SITE_NAME);
    if (StringUtils.isBlank(siteName)) {
      siteName=RmSiteType.DEFAULT_SITE_NAME;
    }
    if (siteService.getSite(siteName) == null) {
      throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"" + siteName + ""String_Node_Str"");
    }
    NodeRef filePlan=siteService.getContainer(siteName,RmSiteType.COMPONENT_DOCUMENT_LIBRARY);
    if (filePlan == null) {
      filePlan=siteService.createContainer(siteName,RmSiteType.COMPONENT_DOCUMENT_LIBRARY,TYPE_FILE_PLAN,null);
    }
    dataSetService.loadDataSet(filePlan,dataSetId);
    model.put(""String_Node_Str"",true);
    model.put(""String_Node_Str"",""String_Node_Str"");
  }
 catch (  Exception ex) {
    model.put(""String_Node_Str"",false);
    model.put(""String_Node_Str"",ex.getMessage());
    logger.error(ExceptionUtils.getFullStackTrace(ex));
  }
  return model;
}","/** 
 * @see org.springframework.extensions.webscripts.DeclarativeWebScript#executeImpl(org.springframework.extensions.webscripts.WebScriptRequest,org.springframework.extensions.webscripts.Status,org.springframework.extensions.webscripts.Cache)
 */
@Override protected Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  Map<String,Object> model=new HashMap<String,Object>(1,1.0f);
  try {
    String dataSetId=req.getServiceMatch().getTemplateVars().get(ARG_DATA_SET_ID);
    if (StringUtils.isBlank(dataSetId)) {
      throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"");
    }
    if (!dataSetService.existsDataSet(dataSetId)) {
      throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + dataSetId + ""String_Node_Str""+ ""String_Node_Str"");
    }
    String siteName=req.getParameter(ARG_SITE_NAME);
    if (StringUtils.isBlank(siteName)) {
      siteName=RmSiteType.DEFAULT_SITE_NAME;
    }
    if (siteService.getSite(siteName) == null) {
      throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"" + siteName + ""String_Node_Str"");
    }
    NodeRef filePlan=siteService.getContainer(siteName,RmSiteType.COMPONENT_DOCUMENT_LIBRARY);
    if (filePlan == null) {
      filePlan=siteService.createContainer(siteName,RmSiteType.COMPONENT_DOCUMENT_LIBRARY,TYPE_FILE_PLAN,null);
    }
    dataSetService.loadDataSet(filePlan,dataSetId);
    model.put(""String_Node_Str"",true);
    model.put(""String_Node_Str"",""String_Node_Str"");
  }
 catch (  Exception ex) {
    model.put(""String_Node_Str"",false);
    model.put(""String_Node_Str"",ex.getMessage());
    logger.error(""String_Node_Str"" + ex);
  }
  return model;
}",0.9835873388042204
89134,"/** 
 * Helper to check that the current version is recorded
 */
protected void checkRecordedVersion(NodeRef document,String description,String versionLabel){
  assertFalse(recordService.isRecord(document));
  Map<QName,Serializable> beforeProperties=nodeService.getProperties(document);
  Set<QName> beforeAspects=nodeService.getAspects(dmDocument);
  Version version=versionService.getCurrentVersion(document);
  assertNotNull(version);
  assertEquals(description,version.getDescription());
  assertEquals(versionLabel,version.getVersionLabel());
  NodeRef frozen=version.getFrozenStateNodeRef();
  checkProperties(frozen,beforeProperties);
  checkAspects(frozen,beforeAspects);
  NodeRef record=(NodeRef)version.getVersionProperties().get(RecordableVersionServiceImpl.PROP_VERSION_RECORD);
  assertNotNull(record);
  assertTrue(nodeService.hasAspect(record,ASPECT_VERSION_RECORD));
  assertEquals(versionLabel,nodeService.getProperty(record,RecordableVersionModel.PROP_VERSION_LABEL));
  assertEquals(description,nodeService.getProperty(record,RecordableVersionModel.PROP_VERSION_DESCRIPTION));
  assertTrue(recordService.isRecord(record));
  assertFalse(recordService.isFiled(record));
  assertFalse(nodeService.hasAspect(record,ContentModel.ASPECT_VERSIONABLE));
  assertFalse(nodeService.hasAspect(record,RecordableVersionModel.ASPECT_VERSIONABLE));
  VersionHistory versionHistory=versionService.getVersionHistory(document);
  assertNotNull(versionHistory);
  Version headVersion=versionHistory.getHeadVersion();
  assertNotNull(headVersion);
}","/** 
 * Helper to check that the current version is recorded
 */
protected void checkRecordedVersion(NodeRef document,String description,String versionLabel){
  assertFalse(recordService.isRecord(document));
  Map<QName,Serializable> beforeProperties=nodeService.getProperties(document);
  Set<QName> beforeAspects=nodeService.getAspects(dmDocument);
  Version version=versionService.getCurrentVersion(document);
  assertNotNull(version);
  assertEquals(description,version.getDescription());
  assertEquals(versionLabel,version.getVersionLabel());
  NodeRef frozen=version.getFrozenStateNodeRef();
  checkProperties(frozen,beforeProperties);
  checkAspects(frozen,beforeAspects);
  NodeRef record=(NodeRef)version.getVersionProperties().get(RecordableVersionServiceImpl.PROP_VERSION_RECORD);
  assertNotNull(record);
  assertTrue(nodeService.hasAspect(record,ASPECT_VERSION_RECORD));
  assertEquals(versionLabel,nodeService.getProperty(record,RecordableVersionModel.PROP_VERSION_LABEL));
  assertEquals(description,nodeService.getProperty(record,RecordableVersionModel.PROP_VERSION_DESCRIPTION));
  assertTrue(recordService.isRecord(record));
  assertFalse(recordService.isFiled(record));
  assertFalse(nodeService.hasAspect(record,RecordableVersionModel.ASPECT_VERSIONABLE));
  VersionHistory versionHistory=versionService.getVersionHistory(document);
  assertNotNull(versionHistory);
  Version headVersion=versionHistory.getHeadVersion();
  assertNotNull(headVersion);
}",0.9629629629629628
89135,"/** 
 * Helper method to get the service registry in order to call services
 * @return Returns the service registry
 */
public static ServiceRegistry getServiceRegistry(){
  ProcessEngineConfigurationImpl config=Context.getProcessEngineConfiguration();
  if (config != null) {
    ServiceRegistry registry=(ServiceRegistry)config.getBeans().get(ActivitiConstants.SERVICE_REGISTRY_BEAN_KEY);
    if (registry == null) {
      throw new RuntimeException(""String_Node_Str"" + ActivitiConstants.SERVICE_REGISTRY_BEAN_KEY);
    }
    return registry;
  }
  throw new IllegalStateException(""String_Node_Str"");
}","/** 
 * Helper method to get the service registry in order to call services
 * @return Returns the service registry
 */
public static ServiceRegistry getServiceRegistry(){
  ProcessEngineConfigurationImpl config=Context.getProcessEngineConfiguration();
  if (config != null) {
    ServiceRegistry registry=(ServiceRegistry)config.getBeans().get(ActivitiConstants.SERVICE_REGISTRY_BEAN_KEY);
    if (registry == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + ActivitiConstants.SERVICE_REGISTRY_BEAN_KEY);
    }
    return registry;
  }
  throw new IllegalStateException(""String_Node_Str"");
}",0.993421052631579
89136,"/** 
 * @see org.alfresco.repo.security.permissions.DynamicAuthority#hasAuthority(org.alfresco.service.cmr.repository.NodeRef,java.lang.String)
 */
@Override public boolean hasAuthority(NodeRef nodeRef,String userName){
  boolean result=false;
  Map<Pair<NodeRef,String>,Boolean> transactionCache=TransactionalResourceHelper.getMap(getTransactionCacheName());
  Pair<NodeRef,String> key=new Pair<NodeRef,String>(nodeRef,userName);
  if (transactionCache.containsKey(key)) {
    result=transactionCache.get(key);
  }
 else {
    if (getNodeService().hasAspect(nodeRef,ASPECT_EXTENDED_SECURITY) == true) {
      Set<String> authorities=getAuthorites(nodeRef);
      if (authorities != null) {
        if (authorities.contains(""String_Node_Str"") || authorities.contains(userName)) {
          result=true;
        }
 else {
          Set<String> contained=getAuthorityService().getAuthoritiesForUser(userName);
          authorities.retainAll(contained);
          result=(authorities.size() != 0);
        }
      }
    }
    transactionCache.put(key,result);
  }
  return result;
}","/** 
 * @see org.alfresco.repo.security.permissions.DynamicAuthority#hasAuthority(org.alfresco.service.cmr.repository.NodeRef,java.lang.String)
 */
@Override public boolean hasAuthority(NodeRef nodeRef,String userName){
  boolean result=false;
  Map<Pair<NodeRef,String>,Boolean> transactionCache=TransactionalResourceHelper.getMap(getTransactionCacheName());
  Pair<NodeRef,String> key=new Pair<NodeRef,String>(nodeRef,userName);
  if (transactionCache.containsKey(key)) {
    result=transactionCache.get(key);
  }
 else {
    if (getNodeService().hasAspect(nodeRef,ASPECT_EXTENDED_SECURITY)) {
      Set<String> authorities=getAuthorites(nodeRef);
      if (authorities != null) {
        if (authorities.contains(""String_Node_Str"") || authorities.contains(userName)) {
          result=true;
        }
 else {
          Set<String> contained=getAuthorityService().getAuthoritiesForUser(userName);
          authorities.retainAll(contained);
          result=(authorities.size() != 0);
        }
      }
    }
    transactionCache.put(key,result);
  }
  return result;
}",0.9962825278810408
89137,"/** 
 * Creates a new recorded version
 * @param sourceTypeRef                 source type name
 * @param versionHistoryRef             version history reference
 * @param standardVersionProperties     standard version properties
 * @param versionProperties             version properties
 * @param versionNumber                 version number
 * @param nodeDetails                   policy scope
 * @return {@link NodeRef}              record version
 */
protected NodeRef createNewRecordedVersion(QName sourceTypeRef,NodeRef versionHistoryRef,Map<QName,Serializable> standardVersionProperties,Map<String,Serializable> versionProperties,int versionNumber,PolicyScope nodeDetails){
  NodeRef versionNodeRef=null;
  policyBehaviourFilter.disableBehaviour(ContentModel.ASPECT_VERSIONABLE);
  policyBehaviourFilter.disableBehaviour(ContentModel.ASPECT_MULTILINGUAL_DOCUMENT);
  policyBehaviourFilter.disableBehaviour(ContentModel.TYPE_MULTILINGUAL_CONTAINER);
  try {
    final NodeRef filePlan=(NodeRef)versionProperties.get(KEY_FILE_PLAN);
    if (filePlan == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
    final NodeRef nodeRef=(NodeRef)standardVersionProperties.get(Version2Model.PROP_QNAME_FROZEN_NODE_REF);
    final NodeRef record=createRecord(nodeRef,filePlan);
    PropertyMap versionRecordProps=new PropertyMap(3);
    versionRecordProps.put(PROP_VERSIONED_NODEREF,nodeRef);
    versionRecordProps.put(RecordableVersionModel.PROP_VERSION_LABEL,standardVersionProperties.get(QName.createQName(Version2Model.NAMESPACE_URI,Version2Model.PROP_VERSION_LABEL)));
    versionRecordProps.put(RecordableVersionModel.PROP_VERSION_DESCRIPTION,standardVersionProperties.get(QName.createQName(Version2Model.NAMESPACE_URI,Version2Model.PROP_VERSION_DESCRIPTION)));
    nodeService.addAspect(record,ASPECT_VERSION_RECORD,versionRecordProps);
    VersionHistory versionHistory=getVersionHistory(nodeRef);
    if (versionHistory != null) {
      Collection<Version> previousVersions=versionHistory.getAllVersions();
      for (      Version previousVersion : previousVersions) {
        final NodeRef previousRecord=(NodeRef)previousVersion.getVersionProperties().get(PROP_VERSION_RECORD);
        if (previousRecord != null) {
          authenticationUtil.runAsSystem(new RunAsWork<Void>(){
            @Override public Void doWork() throws Exception {
              relationshipService.addRelationship(""String_Node_Str"",record,previousRecord);
              return null;
            }
          }
);
          break;
        }
      }
    }
    ChildAssociationRef childAssocRef=dbNodeService.createNode(versionHistoryRef,Version2Model.CHILD_QNAME_VERSIONS,QName.createQName(Version2Model.NAMESPACE_URI,Version2Model.CHILD_VERSIONS + ""String_Node_Str"" + versionNumber),sourceTypeRef,null);
    versionNodeRef=childAssocRef.getChildRef();
    nodeService.addAspect(versionNodeRef,Version2Model.ASPECT_VERSION,standardVersionProperties);
    nodeService.addAspect(versionNodeRef,ASPECT_RECORDED_VERSION,Collections.singletonMap(PROP_RECORD_NODE_REF,(Serializable)record));
    freezeAuditableAspect(nodeRef,versionNodeRef);
  }
  finally {
    this.policyBehaviourFilter.enableBehaviour(ContentModel.ASPECT_VERSIONABLE);
    this.policyBehaviourFilter.enableBehaviour(ContentModel.ASPECT_MULTILINGUAL_DOCUMENT);
    this.policyBehaviourFilter.enableBehaviour(ContentModel.TYPE_MULTILINGUAL_CONTAINER);
  }
  if (dbNodeService.hasAspect(versionNodeRef,ContentModel.ASPECT_AUDITABLE) == false) {
    dbNodeService.addAspect(versionNodeRef,ContentModel.ASPECT_AUDITABLE,null);
  }
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + versionNumber + ""String_Node_Str""+ versionNodeRef);
  }
  return versionNodeRef;
}","/** 
 * Creates a new recorded version
 * @param sourceTypeRef                 source type name
 * @param versionHistoryRef             version history reference
 * @param standardVersionProperties     standard version properties
 * @param versionProperties             version properties
 * @param versionNumber                 version number
 * @param nodeDetails                   policy scope
 * @return {@link NodeRef}              record version
 */
protected NodeRef createNewRecordedVersion(QName sourceTypeRef,NodeRef versionHistoryRef,Map<QName,Serializable> standardVersionProperties,Map<String,Serializable> versionProperties,int versionNumber,PolicyScope nodeDetails){
  NodeRef versionNodeRef=null;
  policyBehaviourFilter.disableBehaviour(ContentModel.ASPECT_VERSIONABLE);
  policyBehaviourFilter.disableBehaviour(ContentModel.ASPECT_MULTILINGUAL_DOCUMENT);
  policyBehaviourFilter.disableBehaviour(ContentModel.TYPE_MULTILINGUAL_CONTAINER);
  try {
    final NodeRef filePlan=(NodeRef)versionProperties.get(KEY_FILE_PLAN);
    if (filePlan == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
    final NodeRef nodeRef=(NodeRef)standardVersionProperties.get(Version2Model.PROP_QNAME_FROZEN_NODE_REF);
    final NodeRef record=createRecord(nodeRef,filePlan);
    PropertyMap versionRecordProps=new PropertyMap(3);
    versionRecordProps.put(PROP_VERSIONED_NODEREF,nodeRef);
    versionRecordProps.put(RecordableVersionModel.PROP_VERSION_LABEL,standardVersionProperties.get(QName.createQName(Version2Model.NAMESPACE_URI,Version2Model.PROP_VERSION_LABEL)));
    versionRecordProps.put(RecordableVersionModel.PROP_VERSION_DESCRIPTION,standardVersionProperties.get(QName.createQName(Version2Model.NAMESPACE_URI,Version2Model.PROP_VERSION_DESCRIPTION)));
    nodeService.addAspect(record,ASPECT_VERSION_RECORD,versionRecordProps);
    VersionHistory versionHistory=getVersionHistory(nodeRef);
    if (versionHistory != null) {
      Collection<Version> previousVersions=versionHistory.getAllVersions();
      for (      Version previousVersion : previousVersions) {
        final NodeRef previousRecord=(NodeRef)previousVersion.getVersionProperties().get(PROP_VERSION_RECORD);
        if (previousRecord != null) {
          authenticationUtil.runAsSystem(new RunAsWork<Void>(){
            @Override public Void doWork() throws Exception {
              relationshipService.addRelationship(""String_Node_Str"",record,previousRecord);
              return null;
            }
          }
);
          break;
        }
      }
    }
    ChildAssociationRef childAssocRef=dbNodeService.createNode(versionHistoryRef,Version2Model.CHILD_QNAME_VERSIONS,QName.createQName(Version2Model.NAMESPACE_URI,Version2Model.CHILD_VERSIONS + ""String_Node_Str"" + versionNumber),sourceTypeRef,null);
    versionNodeRef=childAssocRef.getChildRef();
    nodeService.addAspect(versionNodeRef,Version2Model.ASPECT_VERSION,standardVersionProperties);
    nodeService.addAspect(versionNodeRef,ASPECT_RECORDED_VERSION,Collections.singletonMap(PROP_RECORD_NODE_REF,(Serializable)record));
    freezeAuditableAspect(nodeRef,versionNodeRef);
  }
  finally {
    this.policyBehaviourFilter.enableBehaviour(ContentModel.ASPECT_VERSIONABLE);
    this.policyBehaviourFilter.enableBehaviour(ContentModel.ASPECT_MULTILINGUAL_DOCUMENT);
    this.policyBehaviourFilter.enableBehaviour(ContentModel.TYPE_MULTILINGUAL_CONTAINER);
  }
  if (!dbNodeService.hasAspect(versionNodeRef,ContentModel.ASPECT_AUDITABLE)) {
    dbNodeService.addAspect(versionNodeRef,ContentModel.ASPECT_AUDITABLE,null);
  }
  if (logger.isTraceEnabled()) {
    logger.trace(""String_Node_Str"" + versionNumber + ""String_Node_Str""+ versionNodeRef);
  }
  return versionNodeRef;
}",0.998665955176094
89138,"/** 
 * Override to deal with the possibility of hard coded permission checks in core code. Note:  Eventually we need to merge the RM permission model into the core to make this more rebust.
 * @see org.alfresco.repo.security.permissions.impl.ExtendedPermissionService#hasPermission(org.alfresco.service.cmr.repository.NodeRef,java.lang.String)
 */
@Override public AccessStatus hasPermission(NodeRef nodeRef,String perm){
  AccessStatus acs=super.hasPermission(nodeRef,perm);
  if (AccessStatus.DENIED.equals(acs) == true && PermissionService.READ.equals(perm) == true && nodeService.hasAspect(nodeRef,RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT) == true) {
    return super.hasPermission(nodeRef,RMPermissionModel.READ_RECORDS);
  }
 else   if (AccessStatus.DENIED.equals(acs) == true && PermissionService.WRITE.equals(perm) == true && nodeService.hasAspect(nodeRef,RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT) == true) {
    return super.hasPermission(nodeRef,RMPermissionModel.FILE_RECORDS);
  }
  return acs;
}","/** 
 * Override to deal with the possibility of hard coded permission checks in core code. Note:  Eventually we need to merge the RM permission model into the core to make this more rebust.
 * @see org.alfresco.repo.security.permissions.impl.ExtendedPermissionService#hasPermission(org.alfresco.service.cmr.repository.NodeRef,java.lang.String)
 */
@Override public AccessStatus hasPermission(NodeRef nodeRef,String perm){
  AccessStatus acs=super.hasPermission(nodeRef,perm);
  if (AccessStatus.DENIED.equals(acs) && PermissionService.READ.equals(perm) && nodeService.hasAspect(nodeRef,RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT)) {
    return super.hasPermission(nodeRef,RMPermissionModel.READ_RECORDS);
  }
 else   if (AccessStatus.DENIED.equals(acs) && PermissionService.WRITE.equals(perm) && nodeService.hasAspect(nodeRef,RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT)) {
    return super.hasPermission(nodeRef,RMPermissionModel.FILE_RECORDS);
  }
  return acs;
}",0.9760717846460618
89139,"/** 
 * Helper method to build JSON content to send to hold webscripts.
 */
protected String buildContent(List<NodeRef> nodeRefs,List<NodeRef> holds){
  StringBuilder builder=new StringBuilder(255);
  builder.append(""String_Node_Str"");
  if (nodeRefs != null) {
    builder.append(""String_Node_Str"");
    boolean bFirst=true;
    for (    NodeRef nodeRef : nodeRefs) {
      if (bFirst == false) {
        builder.append(""String_Node_Str"");
      }
 else {
        bFirst=false;
      }
      builder.append(""String_Node_Str"" + nodeRef.toString() + ""String_Node_Str"");
    }
    builder.append(""String_Node_Str"");
  }
  if (nodeRefs != null && holds != null) {
    builder.append(""String_Node_Str"");
  }
  if (holds != null) {
    builder.append(""String_Node_Str"");
    boolean bFirst=true;
    for (    NodeRef hold : holds) {
      if (bFirst == false) {
        builder.append(""String_Node_Str"");
      }
 else {
        bFirst=false;
      }
      builder.append(""String_Node_Str"" + hold.toString() + ""String_Node_Str"");
    }
    builder.append(""String_Node_Str"");
  }
  builder.append(""String_Node_Str"");
  return builder.toString();
}","/** 
 * Helper method to build JSON content to send to hold webscripts.
 */
protected String buildContent(List<NodeRef> nodeRefs,List<NodeRef> holds){
  StringBuilder builder=new StringBuilder(255);
  builder.append(""String_Node_Str"");
  if (nodeRefs != null) {
    builder.append(""String_Node_Str"");
    boolean bFirst=true;
    for (    NodeRef nodeRef : nodeRefs) {
      if (!bFirst) {
        builder.append(""String_Node_Str"");
      }
 else {
        bFirst=false;
      }
      builder.append(""String_Node_Str"" + nodeRef.toString() + ""String_Node_Str"");
    }
    builder.append(""String_Node_Str"");
  }
  if (nodeRefs != null && holds != null) {
    builder.append(""String_Node_Str"");
  }
  if (holds != null) {
    builder.append(""String_Node_Str"");
    boolean bFirst=true;
    for (    NodeRef hold : holds) {
      if (!bFirst) {
        builder.append(""String_Node_Str"");
      }
 else {
        bFirst=false;
      }
      builder.append(""String_Node_Str"" + hold.toString() + ""String_Node_Str"");
    }
    builder.append(""String_Node_Str"");
  }
  builder.append(""String_Node_Str"");
  return builder.toString();
}",0.7625772285966461
89140,"/** 
 * @see org.alfresco.repo.security.permissions.impl.PermissionServiceImpl#setInheritParentPermissions(org.alfresco.service.cmr.repository.NodeRef,boolean)
 */
@Override public void setInheritParentPermissions(final NodeRef nodeRef,boolean inheritParentPermissions){
  final String adminRole=getAdminRole(nodeRef);
  if (nodeService.hasAspect(nodeRef,RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT) && isNotBlank(adminRole)) {
    if (inheritParentPermissions) {
      Set<AccessPermission> accessPermissions=getAllSetPermissions(nodeRef);
      for (      AccessPermission accessPermission : accessPermissions) {
        String authority=accessPermission.getAuthority();
        String permission=accessPermission.getPermission();
        if (accessPermission.isSetDirectly() && (RMPermissionModel.FILING.equals(permission) || RMPermissionModel.READ_RECORDS.equals(permission)) && (ExtendedReaderDynamicAuthority.EXTENDED_READER.equals(authority) || ExtendedWriterDynamicAuthority.EXTENDED_WRITER.equals(authority)) || adminRole.equals(authority)) {
        }
      }
    }
 else {
      setPermission(nodeRef,ExtendedReaderDynamicAuthority.EXTENDED_READER,RMPermissionModel.READ_RECORDS,true);
      setPermission(nodeRef,ExtendedWriterDynamicAuthority.EXTENDED_WRITER,RMPermissionModel.FILING,true);
      setPermission(nodeRef,adminRole,RMPermissionModel.FILING,true);
    }
  }
  super.setInheritParentPermissions(nodeRef,inheritParentPermissions);
}","/** 
 * @see org.alfresco.repo.security.permissions.impl.PermissionServiceImpl#setInheritParentPermissions(org.alfresco.service.cmr.repository.NodeRef,boolean)
 */
@Override public void setInheritParentPermissions(final NodeRef nodeRef,boolean inheritParentPermissions){
  final String adminRole=getAdminRole(nodeRef);
  if (nodeService.hasAspect(nodeRef,RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT) && isNotBlank(adminRole) && !inheritParentPermissions) {
    setPermission(nodeRef,ExtendedReaderDynamicAuthority.EXTENDED_READER,RMPermissionModel.READ_RECORDS,true);
    setPermission(nodeRef,ExtendedWriterDynamicAuthority.EXTENDED_WRITER,RMPermissionModel.FILING,true);
    setPermission(nodeRef,adminRole,RMPermissionModel.FILING,true);
  }
  super.setInheritParentPermissions(nodeRef,inheritParentPermissions);
}",0.5321663019693654
89141,"/** 
 * Gets the relationship type from the association definition
 * @param associationDefinition The association definition
 * @return The type of the relationship definition
 */
private RelationshipType getRelationshipType(AssociationDefinition associationDefinition){
  RelationshipType type;
  if (associationDefinition instanceof ChildAssociationDefinition) {
    type=RelationshipType.PARENTCHILD;
  }
 else   if (associationDefinition instanceof AssociationDefinition) {
    type=RelationshipType.BIDIRECTIONAL;
  }
 else {
    StringBuilder sb=new StringBuilder();
    sb.append(""String_Node_Str"").append(associationDefinition.getName().getLocalName()).append(""String_Node_Str"");
    throw new AlfrescoRuntimeException(sb.toString());
  }
  return type;
}","/** 
 * Gets the relationship type from the association definition
 * @param associationDefinition The association definition
 * @return The type of the relationship definition
 */
private RelationshipType getRelationshipType(AssociationDefinition associationDefinition){
  RelationshipType type;
  if (associationDefinition instanceof ChildAssociationDefinition) {
    type=RelationshipType.PARENTCHILD;
  }
 else {
    type=RelationshipType.BIDIRECTIONAL;
  }
  return type;
}",0.7520128824476651
89142,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.record.InplaceRecordService#hideRecord(org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public void hideRecord(final NodeRef nodeRef){
  ParameterCheck.mandatory(""String_Node_Str"",nodeRef);
  runAsSystem(new RunAsWork<Void>(){
    @Override public Void doWork(){
      NodeRef originatingLocation=(NodeRef)nodeService.getProperty(nodeRef,PROP_RECORD_ORIGINATING_LOCATION);
      List<ChildAssociationRef> parentAssocs=nodeService.getParentAssocs(nodeRef);
      for (      ChildAssociationRef childAssociationRef : parentAssocs) {
        if (!childAssociationRef.isPrimary() && childAssociationRef.getParentRef().equals(originatingLocation)) {
          nodeService.removeChildAssociation(childAssociationRef);
          break;
        }
      }
      extendedSecurityService.removeAllExtendedSecurity(nodeRef);
      return null;
    }
  }
);
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.record.InplaceRecordService#hideRecord(org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public void hideRecord(final NodeRef nodeRef){
  ParameterCheck.mandatory(""String_Node_Str"",nodeRef);
  authenticationUtil.runAsSystem(new RunAsWork<Void>(){
    @Override public Void doWork(){
      NodeRef originatingLocation=(NodeRef)nodeService.getProperty(nodeRef,PROP_RECORD_ORIGINATING_LOCATION);
      List<ChildAssociationRef> parentAssocs=nodeService.getParentAssocs(nodeRef);
      for (      ChildAssociationRef childAssociationRef : parentAssocs) {
        if (!childAssociationRef.isPrimary() && childAssociationRef.getParentRef().equals(originatingLocation)) {
          nodeService.removeChildAssociation(childAssociationRef);
          break;
        }
      }
      extendedSecurityService.removeAllExtendedSecurity(nodeRef);
      return null;
    }
  }
);
}",0.9896907216494846
89143,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.record.InplaceRecordService#moveRecord(org.alfresco.service.cmr.repository.NodeRef,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public void moveRecord(final NodeRef nodeRef,final NodeRef targetNodeRef){
  ParameterCheck.mandatory(""String_Node_Str"",nodeRef);
  ParameterCheck.mandatory(""String_Node_Str"",targetNodeRef);
  NodeRef sourceParentNodeRef=null;
  NodeRef originatingLocation=(NodeRef)nodeService.getProperty(nodeRef,PROP_RECORD_ORIGINATING_LOCATION);
  for (  ChildAssociationRef parentAssoc : nodeService.getParentAssocs(nodeRef)) {
    if (!parentAssoc.isPrimary() && parentAssoc.getParentRef().equals(originatingLocation)) {
      sourceParentNodeRef=parentAssoc.getParentRef();
      break;
    }
  }
  if (sourceParentNodeRef == null) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
  SiteInfo sourceSite=siteService.getSite(sourceParentNodeRef);
  SiteInfo targetSite=siteService.getSite(targetNodeRef);
  if (!sourceSite.equals(targetSite)) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
  if (!sourceSite.getSitePreset().equals(""String_Node_Str"")) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
  final NodeRef source=sourceParentNodeRef;
  runAsSystem(new RunAsWork<Void>(){
    @Override public Void doWork(){
      try {
        fileFolderService.moveFrom(nodeRef,source,targetNodeRef,null);
        nodeService.setProperty(nodeRef,PROP_RECORD_ORIGINATING_LOCATION,targetNodeRef);
      }
 catch (      FileExistsException|FileNotFoundException ex) {
        throw new AlfrescoRuntimeException(""String_Node_Str"" + ex);
      }
      return null;
    }
  }
);
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.record.InplaceRecordService#moveRecord(org.alfresco.service.cmr.repository.NodeRef,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public void moveRecord(final NodeRef nodeRef,final NodeRef targetNodeRef){
  ParameterCheck.mandatory(""String_Node_Str"",nodeRef);
  ParameterCheck.mandatory(""String_Node_Str"",targetNodeRef);
  NodeRef sourceParentNodeRef=null;
  NodeRef originatingLocation=(NodeRef)nodeService.getProperty(nodeRef,PROP_RECORD_ORIGINATING_LOCATION);
  for (  ChildAssociationRef parentAssoc : nodeService.getParentAssocs(nodeRef)) {
    if (!parentAssoc.isPrimary() && parentAssoc.getParentRef().equals(originatingLocation)) {
      sourceParentNodeRef=parentAssoc.getParentRef();
      break;
    }
  }
  if (sourceParentNodeRef == null) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
  SiteInfo sourceSite=siteService.getSite(sourceParentNodeRef);
  SiteInfo targetSite=siteService.getSite(targetNodeRef);
  if (!sourceSite.equals(targetSite)) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
  if (!sourceSite.getSitePreset().equals(""String_Node_Str"")) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
  final NodeRef source=sourceParentNodeRef;
  authenticationUtil.runAsSystem(new RunAsWork<Void>(){
    @Override public Void doWork(){
      try {
        fileFolderService.moveFrom(nodeRef,source,targetNodeRef,null);
        nodeService.setProperty(nodeRef,PROP_RECORD_ORIGINATING_LOCATION,targetNodeRef);
      }
 catch (      FileExistsException|FileNotFoundException ex) {
        throw new AlfrescoRuntimeException(""String_Node_Str"" + ex);
      }
      return null;
    }
  }
);
}",0.9944557922381092
89144,"/** 
 * Override with check for RM read
 * @param aclId
 * @return
 */
private Set<String> getReadersDenied(Long aclId){
  AccessControlList acl=aclDaoComponent.getAccessControlList(aclId);
  if (acl == null) {
    return Collections.emptySet();
  }
  Set<String> denied=readersDeniedCache.get(aclId);
  if (denied != null) {
    return denied;
  }
  denied=new HashSet<String>();
  Set<String> assigned=new HashSet<String>();
  for (  AccessControlEntry ace : acl.getEntries()) {
    assigned.add(ace.getAuthority());
  }
  for (  String authority : assigned) {
    UnconditionalDeniedAclTest test=new UnconditionalDeniedAclTest(getPermissionReference(PermissionService.READ));
    UnconditionalDeniedAclTest rmTest=new UnconditionalDeniedAclTest(getPermissionReference(RMPermissionModel.READ_RECORDS));
    if (test.evaluate(authority,aclId) || rmTest.evaluate(authority,aclId)) {
      denied.add(authority);
    }
  }
  readersDeniedCache.put((Serializable)acl.getProperties(),denied);
  return denied;
}","/** 
 * Override with check for RM read
 * @param aclId
 * @return
 */
public Set<String> getReadersDenied(Long aclId){
  AccessControlList acl=aclDaoComponent.getAccessControlList(aclId);
  if (acl == null) {
    return Collections.emptySet();
  }
  Set<String> denied=readersDeniedCache.get(aclId);
  if (denied != null) {
    return denied;
  }
  denied=new HashSet<String>();
  Set<String> assigned=new HashSet<String>();
  for (  AccessControlEntry ace : acl.getEntries()) {
    assigned.add(ace.getAuthority());
  }
  for (  String authority : assigned) {
    UnconditionalDeniedAclTest test=new UnconditionalDeniedAclTest(getPermissionReference(PermissionService.READ));
    UnconditionalDeniedAclTest rmTest=new UnconditionalDeniedAclTest(getPermissionReference(RMPermissionModel.READ_RECORDS));
    if (test.evaluate(authority,aclId) || rmTest.evaluate(authority,aclId)) {
      denied.add(authority);
    }
  }
  readersDeniedCache.put((Serializable)acl.getProperties(),denied);
  return denied;
}",0.9945409429280396
89145,"@Override public FileInfo create(NodeRef parentNodeRef,String name,QName typeQName,QName assocQName) throws FileExistsException {
  FileInfo result=null;
  recordService.disablePropertyEditableCheck();
  try {
    result=super.create(parentNodeRef,name,typeQName,assocQName);
  }
  finally {
    recordService.enablePropertyEditableCheck();
    recordService.disablePropertyEditableCheck(result.getNodeRef());
  }
  return result;
}","@Override public FileInfo create(NodeRef parentNodeRef,String name,QName typeQName,QName assocQName){
  FileInfo result=null;
  recordService.disablePropertyEditableCheck();
  try {
    result=super.create(parentNodeRef,name,typeQName,assocQName);
  }
  finally {
    recordService.enablePropertyEditableCheck();
    recordService.disablePropertyEditableCheck(result.getNodeRef());
  }
  return result;
}",0.9665071770334928
89146,"/** 
 * Persists any changes made to the events on the given disposition action definition on the given next action.
 * @param dispositionActionDef The disposition action definition node
 * @param nextAction The next disposition action
 */
@Override protected void addParameterDefinitions(List<ParameterDefinition> paramList){
}","@Override protected void addParameterDefinitions(List<ParameterDefinition> paramList){
}",0.4230769230769231
89147,"/** 
 * @see org.alfresco.repo.module.AbstractModuleComponent#executeInternal()
 */
@Override protected void executeInternal(){
  try {
    if (logger.isInfoEnabled()) {
      logger.info(""String_Node_Str"" + getName() + ""String_Node_Str"");
    }
    retryingTransactionHelper.doInTransaction(new RetryingTransactionHelper.RetryingTransactionCallback<Void>(){
      @Override public Void execute(){
        behaviourFilter.disableBehaviour();
        try {
          executePatch();
        }
  finally {
          behaviourFilter.enableBehaviour();
        }
        return null;
      }
    }
,false,true);
    if (logger.isInfoEnabled()) {
      logger.info(""String_Node_Str"" + getName() + ""String_Node_Str"");
    }
  }
 catch (  Throwable exception) {
    if (logger.isInfoEnabled()) {
      logger.info(""String_Node_Str"" + exception.getMessage(),exception);
    }
    throw exception;
  }
}","/** 
 * @see org.alfresco.repo.module.AbstractModuleComponent#executeInternal()
 */
@Override protected void executeInternal(){
  try {
    if (LOGGER.isInfoEnabled()) {
      LOGGER.info(""String_Node_Str"" + getName() + ""String_Node_Str"");
    }
    retryingTransactionHelper.doInTransaction(new RetryingTransactionHelper.RetryingTransactionCallback<Void>(){
      @Override public Void execute(){
        behaviourFilter.disableBehaviour();
        try {
          executePatch();
        }
  finally {
          behaviourFilter.enableBehaviour();
        }
        return null;
      }
    }
,false,true);
    if (LOGGER.isInfoEnabled()) {
      LOGGER.info(""String_Node_Str"" + getName() + ""String_Node_Str"");
    }
  }
 catch (  Throwable exception) {
    if (LOGGER.isInfoEnabled()) {
      LOGGER.info(""String_Node_Str"" + exception.getMessage(),exception);
    }
    throw exception;
  }
}",0.959731543624161
89148,"/** 
 * Attempt to update the template with the updated version
 * @param template
 * @param updatedTemplate
 */
private void updateTemplate(NodeRef template,String templateUpdate){
  if (template == null || !nodeService.exists(template)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
  }
 else {
    String lastPatchUpdate=(String)nodeService.getProperty(template,PROP_LAST_PATCH_UPDATE);
    if (lastPatchUpdate == null || !name.equals(lastPatchUpdate)) {
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + template.toString() + ""String_Node_Str""+ templateUpdate+ ""String_Node_Str"");
      }
      if (!nodeService.hasAspect(template,ContentModel.ASPECT_VERSIONABLE)) {
        nodeService.addAspect(template,ContentModel.ASPECT_VERSIONABLE,null);
        Map<String,Serializable> versionProperties=new HashMap<String,Serializable>(2);
        versionProperties.put(Version.PROP_DESCRIPTION,""String_Node_Str"");
        versionProperties.put(VersionModel.PROP_VERSION_TYPE,VersionType.MINOR);
        versionService.createVersion(template,versionProperties);
      }
      InputStream is=getClass().getClassLoader().getResourceAsStream(templateUpdate);
      ContentWriter writer=contentService.getWriter(template,ContentModel.PROP_CONTENT,true);
      writer.putContent(is);
      boolean enabled=auditService.isAuditEnabled();
      auditService.setAuditEnabled(false);
      try {
        nodeService.setProperty(template,PROP_LAST_PATCH_UPDATE,name);
      }
  finally {
        auditService.setAuditEnabled(enabled);
      }
    }
 else {
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + template.toString() + ""String_Node_Str"");
      }
    }
  }
}","/** 
 * Attempt to update the template with the updated version
 * @param template
 * @param updatedTemplate
 */
private void updateTemplate(NodeRef template,String templateUpdate){
  if (template == null || !nodeService.exists(template)) {
    if (LOGGER.isDebugEnabled()) {
      LOGGER.debug(""String_Node_Str"");
    }
  }
 else {
    String lastPatchUpdate=(String)nodeService.getProperty(template,PROP_LAST_PATCH_UPDATE);
    if (lastPatchUpdate == null || !name.equals(lastPatchUpdate)) {
      if (LOGGER.isDebugEnabled()) {
        LOGGER.debug(""String_Node_Str"" + template.toString() + ""String_Node_Str""+ templateUpdate+ ""String_Node_Str"");
      }
      if (!nodeService.hasAspect(template,ContentModel.ASPECT_VERSIONABLE)) {
        nodeService.addAspect(template,ContentModel.ASPECT_VERSIONABLE,null);
        Map<String,Serializable> versionProperties=new HashMap<String,Serializable>(2);
        versionProperties.put(Version.PROP_DESCRIPTION,""String_Node_Str"");
        versionProperties.put(VersionModel.PROP_VERSION_TYPE,VersionType.MINOR);
        versionService.createVersion(template,versionProperties);
      }
      InputStream is=getClass().getClassLoader().getResourceAsStream(templateUpdate);
      ContentWriter writer=contentService.getWriter(template,ContentModel.PROP_CONTENT,true);
      writer.putContent(is);
      boolean enabled=auditService.isAuditEnabled();
      auditService.setAuditEnabled(false);
      try {
        nodeService.setProperty(template,PROP_LAST_PATCH_UPDATE,name);
      }
  finally {
        auditService.setAuditEnabled(enabled);
      }
    }
 else {
      if (LOGGER.isDebugEnabled()) {
        LOGGER.debug(""String_Node_Str"" + template.toString() + ""String_Node_Str"");
      }
    }
  }
}",0.9793932455638236
89149,"/** 
 * @see org.alfresco.repo.module.AbstractModuleComponent#executeInternal()
 */
@Override protected void executePatch(){
  Pair<Long,QName> aspectPair=qnameDAO.getQName(RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT);
  if (aspectPair != null) {
    List<Long> filePlanComponents=patchDAO.getNodesByAspectQNameId(aspectPair.getFirst(),0L,patchDAO.getMaxAdmNodeID());
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + filePlanComponents.size() + ""String_Node_Str"");
    }
    for (    Long filePlanComponent : filePlanComponents) {
      Pair<Long,NodeRef> recordPair=nodeDAO.getNodePair(filePlanComponent);
      NodeRef filePlanComponentNodeRef=recordPair.getSecond();
      NodeRef filePlan=filePlanService.getFilePlan(filePlanComponentNodeRef);
      if (nodeService.getProperty(filePlanComponentNodeRef,PROP_ROOT_NODEREF) == null) {
        nodeService.setProperty(filePlanComponentNodeRef,PROP_ROOT_NODEREF,filePlan);
      }
      FilePlanComponentKind kind=filePlanService.getFilePlanComponentKind(filePlanComponentNodeRef);
      if (FilePlanComponentKind.RECORD_CATEGORY.equals(kind) || FilePlanComponentKind.RECORD_FOLDER.equals(kind) || FilePlanComponentKind.RECORD.equals(kind)) {
        Role adminRole=filePlanRoleService.getRole(filePlan,""String_Node_Str"");
        if (adminRole != null) {
          permissionService.setPermission(filePlanComponentNodeRef,adminRole.getRoleGroupName(),RMPermissionModel.FILING,true);
        }
        Serializable vitalRecordIndicator=nodeService.getProperty(filePlanComponentNodeRef,PROP_VITAL_RECORD_INDICATOR);
        if (vitalRecordIndicator == null) {
          nodeService.setProperty(filePlanComponentNodeRef,PROP_VITAL_RECORD_INDICATOR,false);
        }
        Serializable reviewPeriod=nodeService.getProperty(filePlanComponentNodeRef,PROP_REVIEW_PERIOD);
        if (reviewPeriod == null) {
          nodeService.setProperty(filePlanComponentNodeRef,PROP_REVIEW_PERIOD,new Period(""String_Node_Str""));
        }
      }
    }
  }
}","/** 
 * @see org.alfresco.repo.module.AbstractModuleComponent#executeInternal()
 */
@Override protected void executePatch(){
  Pair<Long,QName> aspectPair=qnameDAO.getQName(RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT);
  if (aspectPair != null) {
    List<Long> filePlanComponents=patchDAO.getNodesByAspectQNameId(aspectPair.getFirst(),0L,patchDAO.getMaxAdmNodeID());
    if (LOGGER.isDebugEnabled()) {
      LOGGER.debug(""String_Node_Str"" + filePlanComponents.size() + ""String_Node_Str"");
    }
    for (    Long filePlanComponent : filePlanComponents) {
      Pair<Long,NodeRef> recordPair=nodeDAO.getNodePair(filePlanComponent);
      NodeRef filePlanComponentNodeRef=recordPair.getSecond();
      NodeRef filePlan=filePlanService.getFilePlan(filePlanComponentNodeRef);
      if (nodeService.getProperty(filePlanComponentNodeRef,PROP_ROOT_NODEREF) == null) {
        nodeService.setProperty(filePlanComponentNodeRef,PROP_ROOT_NODEREF,filePlan);
      }
      FilePlanComponentKind kind=filePlanService.getFilePlanComponentKind(filePlanComponentNodeRef);
      if (FilePlanComponentKind.RECORD_CATEGORY.equals(kind) || FilePlanComponentKind.RECORD_FOLDER.equals(kind) || FilePlanComponentKind.RECORD.equals(kind)) {
        Role adminRole=filePlanRoleService.getRole(filePlan,""String_Node_Str"");
        if (adminRole != null) {
          permissionService.setPermission(filePlanComponentNodeRef,adminRole.getRoleGroupName(),RMPermissionModel.FILING,true);
        }
        Serializable vitalRecordIndicator=nodeService.getProperty(filePlanComponentNodeRef,PROP_VITAL_RECORD_INDICATOR);
        if (vitalRecordIndicator == null) {
          nodeService.setProperty(filePlanComponentNodeRef,PROP_VITAL_RECORD_INDICATOR,false);
        }
        Serializable reviewPeriod=nodeService.getProperty(filePlanComponentNodeRef,PROP_REVIEW_PERIOD);
        if (reviewPeriod == null) {
          nodeService.setProperty(filePlanComponentNodeRef,PROP_REVIEW_PERIOD,new Period(""String_Node_Str""));
        }
      }
    }
  }
}",0.9940711462450592
89150,"/** 
 * @see org.alfresco.repo.transaction.RetryingTransactionHelper.RetryingTransactionCallback#execute()
 */
@Override public Integer execute() throws Throwable {
  Long maxNodeId=patchDAO.getMaxAdmNodeID();
  Pair<Long,QName> before=qnameDAO.getQName(qnameBefore);
  if (before != null) {
    for (Long i=0L; i < maxNodeId; i+=BATCH_SIZE) {
      if (""String_Node_Str"".equals(reindexClass)) {
        List<Long> nodeIds=patchDAO.getNodesByTypeQNameId(before.getFirst(),i,i + BATCH_SIZE);
        nodeDAO.touchNodes(nodeDAO.getCurrentTransactionId(true),nodeIds);
      }
 else       if (""String_Node_Str"".equals(reindexClass)) {
        List<Long> nodeIds=patchDAO.getNodesByAspectQNameId(before.getFirst(),i,i + BATCH_SIZE);
        nodeDAO.touchNodes(nodeDAO.getCurrentTransactionId(true),nodeIds);
      }
    }
    qnameDAO.updateQName(qnameBefore,qnameAfter);
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + qnameBefore.toString());
    }
  }
 else {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + qnameBefore.toString());
    }
  }
  return 0;
}","/** 
 * @see org.alfresco.repo.transaction.RetryingTransactionHelper.RetryingTransactionCallback#execute()
 */
@Override public Integer execute() throws Throwable {
  Long maxNodeId=patchDAO.getMaxAdmNodeID();
  Pair<Long,QName> before=qnameDAO.getQName(qnameBefore);
  if (before != null) {
    for (Long i=0L; i < maxNodeId; i+=BATCH_SIZE) {
      if (""String_Node_Str"".equals(reindexClass)) {
        List<Long> nodeIds=patchDAO.getNodesByTypeQNameId(before.getFirst(),i,i + BATCH_SIZE);
        nodeDAO.touchNodes(nodeDAO.getCurrentTransactionId(true),nodeIds);
      }
 else       if (""String_Node_Str"".equals(reindexClass)) {
        List<Long> nodeIds=patchDAO.getNodesByAspectQNameId(before.getFirst(),i,i + BATCH_SIZE);
        nodeDAO.touchNodes(nodeDAO.getCurrentTransactionId(true),nodeIds);
      }
    }
    qnameDAO.updateQName(qnameBefore,qnameAfter);
    if (LOGGER.isDebugEnabled()) {
      LOGGER.debug(""String_Node_Str"" + qnameBefore.toString());
    }
  }
 else {
    if (LOGGER.isDebugEnabled()) {
      LOGGER.debug(""String_Node_Str"" + qnameBefore.toString());
    }
  }
  return 0;
}",0.978319783197832
89151,"/** 
 * @see org.alfresco.repo.module.AbstractModuleComponent#executeInternal()
 */
@Override protected void executePatch(){
  if (siteService.getSite(RM_SITE_ID) != null) {
    List<SavedSearchDetails> savedSearches=recordsManagementSearchService.getSavedSearches(RM_SITE_ID);
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + savedSearches.size() + ""String_Node_Str"");
    }
    for (    SavedSearchDetails savedSearchDetails : savedSearches) {
      String refreshedJSON=savedSearchDetails.toJSONString();
      NodeRef nodeRef=savedSearchDetails.getNodeRef();
      if (nodeRef != null) {
        ContentWriter writer=contentService.getWriter(nodeRef,ContentModel.PROP_CONTENT,true);
        writer.putContent(refreshedJSON);
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"" + savedSearchDetails.getName() + ""String_Node_Str""+ nodeRef.toString()+ ""String_Node_Str"");
        }
      }
    }
  }
}","/** 
 * @see org.alfresco.repo.module.AbstractModuleComponent#executeInternal()
 */
@Override protected void executePatch(){
  if (siteService.getSite(RM_SITE_ID) != null) {
    List<SavedSearchDetails> savedSearches=recordsManagementSearchService.getSavedSearches(RM_SITE_ID);
    if (LOGGER.isDebugEnabled()) {
      LOGGER.debug(""String_Node_Str"" + savedSearches.size() + ""String_Node_Str"");
    }
    for (    SavedSearchDetails savedSearchDetails : savedSearches) {
      String refreshedJSON=savedSearchDetails.toJSONString();
      NodeRef nodeRef=savedSearchDetails.getNodeRef();
      if (nodeRef != null) {
        ContentWriter writer=contentService.getWriter(nodeRef,ContentModel.PROP_CONTENT,true);
        writer.putContent(refreshedJSON);
        if (LOGGER.isDebugEnabled()) {
          LOGGER.debug(""String_Node_Str"" + savedSearchDetails.getName() + ""String_Node_Str""+ nodeRef.toString()+ ""String_Node_Str"");
        }
      }
    }
  }
}",0.974869109947644
89152,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.compatibility.ModulePatchComponent#executePatch()
 */
@Override protected void executePatch(){
  if (!nodeService.exists(RM_CONFIG)) {
    return;
  }
  if (!nodeService.exists(newBehaviorScriptsFolder)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    String newBehaviorScriptsFolderName=""String_Node_Str"";
    String newBehaviorScriptsNodeUUID=""String_Node_Str"";
    String newBehaviorScriptsAssocQName=""String_Node_Str"";
    Map<QName,Serializable> newBehaviorScriptsFolderProps=new HashMap<QName,Serializable>();
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_NODE_UUID,newBehaviorScriptsNodeUUID);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_NAME,newBehaviorScriptsFolderName);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_TITLE,newBehaviorScriptsFolderName);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
    newBehaviorScriptsFolder=nodeService.createNode(RM_CONFIG,ContentModel.ASSOC_CONTAINS,QName.createQName(NamespaceService.CONTENT_MODEL_PREFIX,newBehaviorScriptsAssocQName),ContentModel.TYPE_FOLDER,newBehaviorScriptsFolderProps).getChildRef();
  }
  if (nodeService.exists(OLD_BEHAVIOR_SCRIPTS_FOLDER)) {
    AuthenticationUtil.runAs(new RunAsWork<Object>(){
      public Object doWork(){
        RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
          public Void execute() throws Throwable {
            nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
            List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
            if (oldBehaviorScripts != null && !oldBehaviorScripts.isEmpty()) {
              if (logger.isDebugEnabled()) {
                logger.debug(""String_Node_Str"");
              }
              for (              FileInfo script : oldBehaviorScripts) {
                fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
                if (logger.isDebugEnabled()) {
                  logger.debug(""String_Node_Str"" + script.getName());
                }
              }
            }
            return null;
          }
        }
;
        retryingTransactionHelper.doInTransaction(callback);
        return null;
      }
    }
,AuthenticationUtil.getSystemUserName());
  }
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.compatibility.ModulePatchComponent#executePatch()
 */
@Override protected void executePatch(){
  if (!nodeService.exists(RM_CONFIG)) {
    return;
  }
  if (!nodeService.exists(newBehaviorScriptsFolder)) {
    if (LOGGER.isDebugEnabled()) {
      LOGGER.debug(""String_Node_Str"");
    }
    String newBehaviorScriptsFolderName=""String_Node_Str"";
    String newBehaviorScriptsNodeUUID=""String_Node_Str"";
    String newBehaviorScriptsAssocQName=""String_Node_Str"";
    Map<QName,Serializable> newBehaviorScriptsFolderProps=new HashMap<QName,Serializable>();
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_NODE_UUID,newBehaviorScriptsNodeUUID);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_NAME,newBehaviorScriptsFolderName);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_TITLE,newBehaviorScriptsFolderName);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
    newBehaviorScriptsFolder=nodeService.createNode(RM_CONFIG,ContentModel.ASSOC_CONTAINS,QName.createQName(NamespaceService.CONTENT_MODEL_PREFIX,newBehaviorScriptsAssocQName),ContentModel.TYPE_FOLDER,newBehaviorScriptsFolderProps).getChildRef();
  }
  if (nodeService.exists(OLD_BEHAVIOR_SCRIPTS_FOLDER)) {
    AuthenticationUtil.runAs(new RunAsWork<Object>(){
      public Object doWork(){
        RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
          public Void execute() throws Throwable {
            nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
            List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
            if (oldBehaviorScripts != null && !oldBehaviorScripts.isEmpty()) {
              if (LOGGER.isDebugEnabled()) {
                LOGGER.debug(""String_Node_Str"");
              }
              for (              FileInfo script : oldBehaviorScripts) {
                fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
                if (LOGGER.isDebugEnabled()) {
                  LOGGER.debug(""String_Node_Str"" + script.getName());
                }
              }
            }
            return null;
          }
        }
;
        retryingTransactionHelper.doInTransaction(callback);
        return null;
      }
    }
,AuthenticationUtil.getSystemUserName());
  }
}",0.9856972586412396
89153,"public Object doWork(){
  RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
    public Void execute() throws Throwable {
      nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
      List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
      if (oldBehaviorScripts != null && !oldBehaviorScripts.isEmpty()) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"");
        }
        for (        FileInfo script : oldBehaviorScripts) {
          fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
          if (logger.isDebugEnabled()) {
            logger.debug(""String_Node_Str"" + script.getName());
          }
        }
      }
      return null;
    }
  }
;
  retryingTransactionHelper.doInTransaction(callback);
  return null;
}","public Object doWork(){
  RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
    public Void execute() throws Throwable {
      nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
      List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
      if (oldBehaviorScripts != null && !oldBehaviorScripts.isEmpty()) {
        if (LOGGER.isDebugEnabled()) {
          LOGGER.debug(""String_Node_Str"");
        }
        for (        FileInfo script : oldBehaviorScripts) {
          fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
          if (LOGGER.isDebugEnabled()) {
            LOGGER.debug(""String_Node_Str"" + script.getName());
          }
        }
      }
      return null;
    }
  }
;
  retryingTransactionHelper.doInTransaction(callback);
  return null;
}",0.975485188968335
89154,"public Void execute() throws Throwable {
  nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
  List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
  if (oldBehaviorScripts != null && !oldBehaviorScripts.isEmpty()) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    for (    FileInfo script : oldBehaviorScripts) {
      fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + script.getName());
      }
    }
  }
  return null;
}","public Void execute() throws Throwable {
  nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
  List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
  if (oldBehaviorScripts != null && !oldBehaviorScripts.isEmpty()) {
    if (LOGGER.isDebugEnabled()) {
      LOGGER.debug(""String_Node_Str"");
    }
    for (    FileInfo script : oldBehaviorScripts) {
      fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
      if (LOGGER.isDebugEnabled()) {
        LOGGER.debug(""String_Node_Str"" + script.getName());
      }
    }
  }
  return null;
}",0.966804979253112
89155,"/** 
 * Adds a new capability to the specified roles.
 * @param filePlan          file plan
 * @param capabilityName    capability name
 * @param roles             roles
 */
protected void addCapability(NodeRef filePlan,String capabilityName,String... roles){
  Capability capability=capabilityService.getCapability(capabilityName);
  if (capability == null) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + capabilityName + ""String_Node_Str"");
  }
  for (  String roleName : roles) {
    Role role=filePlanRoleService.getRole(filePlan,roleName);
    if (role != null) {
      Set<Capability> capabilities=role.getCapabilities();
      if (!capabilities.contains(capability)) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"" + capabilityName + ""String_Node_Str""+ role.getName());
        }
        capabilities.add(capability);
        filePlanRoleService.updateRole(filePlan,role.getName(),role.getDisplayLabel(),capabilities);
      }
    }
  }
}","/** 
 * Adds a new capability to the specified roles.
 * @param filePlan          file plan
 * @param capabilityName    capability name
 * @param roles             roles
 */
protected void addCapability(NodeRef filePlan,String capabilityName,String... roles){
  Capability capability=capabilityService.getCapability(capabilityName);
  if (capability == null) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + capabilityName + ""String_Node_Str"");
  }
  for (  String roleName : roles) {
    Role role=filePlanRoleService.getRole(filePlan,roleName);
    if (role != null) {
      Set<Capability> capabilities=role.getCapabilities();
      if (!capabilities.contains(capability)) {
        if (LOGGER.isDebugEnabled()) {
          LOGGER.debug(""String_Node_Str"" + capabilityName + ""String_Node_Str""+ role.getName());
        }
        capabilities.add(capability);
        filePlanRoleService.updateRole(filePlan,role.getName(),role.getDisplayLabel(),capabilities);
      }
    }
  }
}",0.9879154078549848
89156,"/** 
 * @see org.alfresco.repo.module.AbstractModuleComponent#executeInternal()
 */
@Override protected void executePatch(){
  Set<NodeRef> filePlans=getFilePlans();
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + filePlans.size() + ""String_Node_Str"");
  }
  for (  NodeRef filePlan : filePlans) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + filePlan.toString());
    }
    addCapability(filePlan,""String_Node_Str"",FilePlanRoleService.ROLE_ADMIN,FilePlanRoleService.ROLE_POWER_USER,FilePlanRoleService.ROLE_RECORDS_MANAGER,FilePlanRoleService.ROLE_SECURITY_OFFICER);
    addCapability(filePlan,""String_Node_Str"",FilePlanRoleService.ROLE_ADMIN);
    addCapability(filePlan,""String_Node_Str"",FilePlanRoleService.ROLE_ADMIN,FilePlanRoleService.ROLE_POWER_USER,FilePlanRoleService.ROLE_RECORDS_MANAGER,FilePlanRoleService.ROLE_SECURITY_OFFICER);
    addCapability(filePlan,""String_Node_Str"",FilePlanRoleService.ROLE_ADMIN,FilePlanRoleService.ROLE_RECORDS_MANAGER);
    addCapability(filePlan,""String_Node_Str"",FilePlanRoleService.ROLE_ADMIN,FilePlanRoleService.ROLE_POWER_USER,FilePlanRoleService.ROLE_RECORDS_MANAGER,FilePlanRoleService.ROLE_SECURITY_OFFICER);
    addCapability(filePlan,""String_Node_Str"",FilePlanRoleService.ROLE_ADMIN,FilePlanRoleService.ROLE_POWER_USER,FilePlanRoleService.ROLE_RECORDS_MANAGER,FilePlanRoleService.ROLE_SECURITY_OFFICER);
    addCapability(filePlan,""String_Node_Str"",FilePlanRoleService.ROLE_ADMIN,FilePlanRoleService.ROLE_POWER_USER,FilePlanRoleService.ROLE_RECORDS_MANAGER,FilePlanRoleService.ROLE_SECURITY_OFFICER);
  }
}","/** 
 * @see org.alfresco.repo.module.AbstractModuleComponent#executeInternal()
 */
@Override protected void executePatch(){
  Set<NodeRef> filePlans=getFilePlans();
  if (LOGGER.isDebugEnabled()) {
    LOGGER.debug(""String_Node_Str"" + filePlans.size() + ""String_Node_Str"");
  }
  for (  NodeRef filePlan : filePlans) {
    if (LOGGER.isDebugEnabled()) {
      LOGGER.debug(""String_Node_Str"" + filePlan.toString());
    }
    addCapability(filePlan,""String_Node_Str"",FilePlanRoleService.ROLE_ADMIN,FilePlanRoleService.ROLE_POWER_USER,FilePlanRoleService.ROLE_RECORDS_MANAGER,FilePlanRoleService.ROLE_SECURITY_OFFICER);
    addCapability(filePlan,""String_Node_Str"",FilePlanRoleService.ROLE_ADMIN);
    addCapability(filePlan,""String_Node_Str"",FilePlanRoleService.ROLE_ADMIN,FilePlanRoleService.ROLE_POWER_USER,FilePlanRoleService.ROLE_RECORDS_MANAGER,FilePlanRoleService.ROLE_SECURITY_OFFICER);
    addCapability(filePlan,""String_Node_Str"",FilePlanRoleService.ROLE_ADMIN,FilePlanRoleService.ROLE_RECORDS_MANAGER);
    addCapability(filePlan,""String_Node_Str"",FilePlanRoleService.ROLE_ADMIN,FilePlanRoleService.ROLE_POWER_USER,FilePlanRoleService.ROLE_RECORDS_MANAGER,FilePlanRoleService.ROLE_SECURITY_OFFICER);
    addCapability(filePlan,""String_Node_Str"",FilePlanRoleService.ROLE_ADMIN,FilePlanRoleService.ROLE_POWER_USER,FilePlanRoleService.ROLE_RECORDS_MANAGER,FilePlanRoleService.ROLE_SECURITY_OFFICER);
    addCapability(filePlan,""String_Node_Str"",FilePlanRoleService.ROLE_ADMIN,FilePlanRoleService.ROLE_POWER_USER,FilePlanRoleService.ROLE_RECORDS_MANAGER,FilePlanRoleService.ROLE_SECURITY_OFFICER);
  }
}",0.9850839030453696
89157,"/** 
 * @see org.alfresco.repo.module.AbstractModuleComponent#executeInternal()
 */
@Override protected void executePatch(){
  Set<NodeRef> filePlans=filePlanService.getFilePlans();
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + filePlans.size() + ""String_Node_Str"");
  }
  for (  NodeRef filePlan : filePlans) {
    if (filePlanService.getUnfiledContainer(filePlan) == null) {
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + filePlan.toString());
      }
      ruleService.disableRules();
      try {
        filePlanPermissionService.setPermission(filePlan,ExtendedReaderDynamicAuthority.EXTENDED_READER,RMPermissionModel.READ_RECORDS);
        filePlanPermissionService.setPermission(filePlan,ExtendedWriterDynamicAuthority.EXTENDED_WRITER,RMPermissionModel.FILING);
        filePlanService.createHoldContainer(filePlan);
        filePlanService.createTransferContainer(filePlan);
        filePlanService.createUnfiledContainer(filePlan);
        moveExistingHolds(filePlan);
        moveExistingTransfers(filePlan);
        filePlanRoleService.createRole(filePlan,FilePlanRoleService.ROLE_EXTENDED_READERS,ROLE_READERS_LABEL,getCapabilities(ROLE_READERS_CAPABILITIES));
        filePlanRoleService.createRole(filePlan,FilePlanRoleService.ROLE_EXTENDED_WRITERS,ROLE_WRITERS_LABEL,getCapabilities(ROLE_WRITERS_CAPABILITIES));
      }
  finally {
        ruleService.enableRules();
      }
    }
  }
}","/** 
 * @see org.alfresco.repo.module.AbstractModuleComponent#executeInternal()
 */
@Override protected void executePatch(){
  Set<NodeRef> filePlans=filePlanService.getFilePlans();
  if (LOGGER.isDebugEnabled()) {
    LOGGER.debug(""String_Node_Str"" + filePlans.size() + ""String_Node_Str"");
  }
  for (  NodeRef filePlan : filePlans) {
    if (filePlanService.getUnfiledContainer(filePlan) == null) {
      if (LOGGER.isDebugEnabled()) {
        LOGGER.debug(""String_Node_Str"" + filePlan.toString());
      }
      ruleService.disableRules();
      try {
        filePlanPermissionService.setPermission(filePlan,ExtendedReaderDynamicAuthority.EXTENDED_READER,RMPermissionModel.READ_RECORDS);
        filePlanPermissionService.setPermission(filePlan,ExtendedWriterDynamicAuthority.EXTENDED_WRITER,RMPermissionModel.FILING);
        filePlanService.createHoldContainer(filePlan);
        filePlanService.createTransferContainer(filePlan);
        filePlanService.createUnfiledContainer(filePlan);
        moveExistingHolds(filePlan);
        moveExistingTransfers(filePlan);
        filePlanRoleService.createRole(filePlan,FilePlanRoleService.ROLE_EXTENDED_READERS,ROLE_READERS_LABEL,getCapabilities(ROLE_READERS_CAPABILITIES));
        filePlanRoleService.createRole(filePlan,FilePlanRoleService.ROLE_EXTENDED_WRITERS,ROLE_WRITERS_LABEL,getCapabilities(ROLE_WRITERS_CAPABILITIES));
      }
  finally {
        ruleService.enableRules();
      }
    }
  }
}",0.9835051546391752
89158,"private void moveExistingTransfers(NodeRef filePlan){
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + filePlan.toString());
  }
  NodeRef container=filePlanService.getTransferContainer(filePlan);
  List<ChildAssociationRef> assocs=nodeService.getChildAssocs(filePlan,ASSOC_TRANSFERS,RegexQNamePattern.MATCH_ALL);
  for (  ChildAssociationRef assoc : assocs) {
    NodeRef transfer=assoc.getChildRef();
    String name=(String)nodeService.getProperty(transfer,ContentModel.PROP_NAME);
    nodeService.moveNode(transfer,container,ContentModel.ASSOC_CONTAINS,QName.createQName(RM_URI,name));
  }
}","private void moveExistingTransfers(NodeRef filePlan){
  if (LOGGER.isDebugEnabled()) {
    LOGGER.debug(""String_Node_Str"" + filePlan.toString());
  }
  NodeRef container=filePlanService.getTransferContainer(filePlan);
  List<ChildAssociationRef> assocs=nodeService.getChildAssocs(filePlan,ASSOC_TRANSFERS,RegexQNamePattern.MATCH_ALL);
  for (  ChildAssociationRef assoc : assocs) {
    NodeRef transfer=assoc.getChildRef();
    String name=(String)nodeService.getProperty(transfer,ContentModel.PROP_NAME);
    nodeService.moveNode(transfer,container,ContentModel.ASSOC_CONTAINS,QName.createQName(RM_URI,name));
  }
}",0.9805194805194806
89159,"private void moveExistingHolds(NodeRef filePlan){
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + filePlan.toString());
  }
  NodeRef container=filePlanService.getHoldContainer(filePlan);
  List<ChildAssociationRef> assocs=nodeService.getChildAssocs(filePlan,ASSOC_HOLDS,RegexQNamePattern.MATCH_ALL);
  for (  ChildAssociationRef assoc : assocs) {
    NodeRef hold=assoc.getChildRef();
    String name=(String)nodeService.getProperty(hold,ContentModel.PROP_NAME);
    nodeService.moveNode(hold,container,ContentModel.ASSOC_CONTAINS,QName.createQName(RM_URI,name));
  }
}","private void moveExistingHolds(NodeRef filePlan){
  if (LOGGER.isDebugEnabled()) {
    LOGGER.debug(""String_Node_Str"" + filePlan.toString());
  }
  NodeRef container=filePlanService.getHoldContainer(filePlan);
  List<ChildAssociationRef> assocs=nodeService.getChildAssocs(filePlan,ASSOC_HOLDS,RegexQNamePattern.MATCH_ALL);
  for (  ChildAssociationRef assoc : assocs) {
    NodeRef hold=assoc.getChildRef();
    String name=(String)nodeService.getProperty(hold,ContentModel.PROP_NAME);
    nodeService.moveNode(hold,container,ContentModel.ASSOC_CONTAINS,QName.createQName(RM_URI,name));
  }
}",0.9797297297297296
89160,"/** 
 * @see org.alfresco.repo.module.AbstractModuleComponent#executeInternal()
 */
@Override protected void executePatch(){
  Pair<Long,QName> aspectPair=qnameDAO.getQName(ASPECT_RECORD);
  if (aspectPair != null) {
    List<Long> records=patchDAO.getNodesByAspectQNameId(aspectPair.getFirst(),0L,patchDAO.getMaxAdmNodeID());
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + records.size() + ""String_Node_Str"");
    }
    for (    Long record : records) {
      Pair<Long,NodeRef> recordPair=nodeDAO.getNodePair(record);
      NodeRef recordNodeRef=recordPair.getSecond();
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + recordNodeRef.toString());
        ChildAssociationRef assoc=nodeService.getPrimaryParent(recordNodeRef);
        NodeRef parent=assoc.getParentRef();
        if (parent != null) {
          filePlanPermissionServiceImpl.setupPermissions(parent,recordNodeRef);
        }
      }
    }
  }
}","/** 
 * @see org.alfresco.repo.module.AbstractModuleComponent#executeInternal()
 */
@Override protected void executePatch(){
  Pair<Long,QName> aspectPair=qnameDAO.getQName(ASPECT_RECORD);
  if (aspectPair != null) {
    List<Long> records=patchDAO.getNodesByAspectQNameId(aspectPair.getFirst(),0L,patchDAO.getMaxAdmNodeID());
    if (LOGGER.isDebugEnabled()) {
      LOGGER.debug(""String_Node_Str"" + records.size() + ""String_Node_Str"");
    }
    for (    Long record : records) {
      Pair<Long,NodeRef> recordPair=nodeDAO.getNodePair(record);
      NodeRef recordNodeRef=recordPair.getSecond();
      if (LOGGER.isDebugEnabled()) {
        LOGGER.debug(""String_Node_Str"" + recordNodeRef.toString());
        ChildAssociationRef assoc=nodeService.getPrimaryParent(recordNodeRef);
        NodeRef parent=assoc.getParentRef();
        if (parent != null) {
          filePlanPermissionServiceImpl.setupPermissions(parent,recordNodeRef);
        }
      }
    }
  }
}",0.9751809720785936
89161,"@Override protected void executePatch(){
  if (nodeService.exists(RM_CONFIG_FOLDER) && !nodeService.exists(TEMPLATE_ROOT)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    NodeRef destructionTemplate=createNode(ContentModel.TYPE_CONTENT,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
    nodeService.addAspect(destructionTemplate,ContentModel.ASPECT_TITLED,null);
    nodeService.addAspect(destructionTemplate,ContentModel.ASPECT_AUTHOR,null);
    ContentWriter writer=contentService.getWriter(destructionTemplate,ContentModel.PROP_CONTENT,true);
    InputStream is=getClass().getClassLoader().getResourceAsStream(PATH_DESTRUCTION_TEMPLATE);
    writer.setEncoding(""String_Node_Str"");
    writer.setMimetype(MimetypeMap.MIMETYPE_TEXT_PLAIN);
    writer.putContent(is);
  }
}","@Override protected void executePatch(){
  if (nodeService.exists(RM_CONFIG_FOLDER) && !nodeService.exists(TEMPLATE_ROOT)) {
    if (LOGGER.isDebugEnabled()) {
      LOGGER.debug(""String_Node_Str"");
    }
    if (LOGGER.isDebugEnabled()) {
      LOGGER.debug(""String_Node_Str"");
    }
    NodeRef destructionTemplate=createNode(ContentModel.TYPE_CONTENT,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
    nodeService.addAspect(destructionTemplate,ContentModel.ASPECT_TITLED,null);
    nodeService.addAspect(destructionTemplate,ContentModel.ASPECT_AUTHOR,null);
    ContentWriter writer=contentService.getWriter(destructionTemplate,ContentModel.PROP_CONTENT,true);
    InputStream is=getClass().getClassLoader().getResourceAsStream(PATH_DESTRUCTION_TEMPLATE);
    writer.setEncoding(""String_Node_Str"");
    writer.setMimetype(MimetypeMap.MIMETYPE_TEXT_PLAIN);
    writer.putContent(is);
  }
}",0.9743589743589745
89162,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.compatibility.ModulePatchComponent#executePatch()
 */
@Override protected void executePatch(){
  Set<NodeRef> filePlans=filePlanService.getFilePlans();
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + filePlans.size() + ""String_Node_Str"");
  }
  for (  NodeRef filePlan : filePlans) {
    boolean parentAddedToZone=false;
    Set<Role> roles=filePlanRoleService.getRoles(filePlan);
    for (    Role role : roles) {
      String roleGroupName=role.getRoleGroupName();
      if (!authorityService.getAuthorityZones(roleGroupName).contains(RMAuthority.ZONE_APP_RM)) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"" + roleGroupName + ""String_Node_Str""+ filePlan.toString());
        }
        addAuthorityToZone(roleGroupName);
        if (!parentAddedToZone) {
          String allRolesGroupName=filePlanRoleService.getAllRolesContainerGroup(filePlan);
          addAuthorityToZone(allRolesGroupName);
          parentAddedToZone=true;
        }
      }
    }
  }
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.compatibility.ModulePatchComponent#executePatch()
 */
@Override protected void executePatch(){
  Set<NodeRef> filePlans=filePlanService.getFilePlans();
  if (LOGGER.isDebugEnabled()) {
    LOGGER.debug(""String_Node_Str"" + filePlans.size() + ""String_Node_Str"");
  }
  for (  NodeRef filePlan : filePlans) {
    boolean parentAddedToZone=false;
    Set<Role> roles=filePlanRoleService.getRoles(filePlan);
    for (    Role role : roles) {
      String roleGroupName=role.getRoleGroupName();
      if (!authorityService.getAuthorityZones(roleGroupName).contains(RMAuthority.ZONE_APP_RM)) {
        if (LOGGER.isDebugEnabled()) {
          LOGGER.debug(""String_Node_Str"" + roleGroupName + ""String_Node_Str""+ filePlan.toString());
        }
        addAuthorityToZone(roleGroupName);
        if (!parentAddedToZone) {
          String allRolesGroupName=filePlanRoleService.getAllRolesContainerGroup(filePlan);
          addAuthorityToZone(allRolesGroupName);
          parentAddedToZone=true;
        }
      }
    }
  }
}",0.9777571825764596
89163,"/** 
 * @see org.alfresco.repo.action.evaluator.ActionConditionEvaluatorAbstractBase#init()
 */
@Override public void init(){
  AuthenticationUtil.runAs(new RunAsWork<Object>(){
    public Object doWork(){
      RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
        public Void execute(){
          recordsManagementActionService.register(RecordsManagementActionConditionEvaluatorAbstractBase.this);
          return null;
        }
      }
;
      retryingTransactionHelper.doInTransaction(callback);
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}","/** 
 * @see org.alfresco.repo.action.evaluator.ActionConditionEvaluatorAbstractBase#init()
 */
@Override public void init(){
  AuthenticationUtil.runAs(new RunAsWork<Object>(){
    public Object doWork(){
      RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
        public Void execute(){
          getRecordsManagementActionService().register(RecordsManagementActionConditionEvaluatorAbstractBase.this);
          return null;
        }
      }
;
      retryingTransactionHelper.doInTransaction(callback);
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}",0.9943043124491456
89164,"public Void execute(){
  recordsManagementActionService.register(RecordsManagementActionConditionEvaluatorAbstractBase.this);
  return null;
}","public Void execute(){
  getRecordsManagementActionService().register(RecordsManagementActionConditionEvaluatorAbstractBase.this);
  return null;
}",0.9757785467128028
89165,"public Object doWork(){
  RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
    public Void execute(){
      recordsManagementActionService.register(RecordsManagementActionConditionEvaluatorAbstractBase.this);
      return null;
    }
  }
;
  retryingTransactionHelper.doInTransaction(callback);
  return null;
}","public Object doWork(){
  RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
    public Void execute(){
      getRecordsManagementActionService().register(RecordsManagementActionConditionEvaluatorAbstractBase.this);
      return null;
    }
  }
;
  retryingTransactionHelper.doInTransaction(callback);
  return null;
}",0.9899569583931134
89166,"/** 
 * Initialise RM action service
 */
public void init(){
  beforeRMActionExecutionDelegate=policyComponent.registerClassPolicy(BeforeRMActionExecution.class);
  onRMActionExecutionDelegate=policyComponent.registerClassPolicy(OnRMActionExecution.class);
}","/** 
 * Initialise RM action service
 */
public void init(){
  beforeRMActionExecutionDelegate=getPolicyComponent().registerClassPolicy(BeforeRMActionExecution.class);
  onRMActionExecutionDelegate=getPolicyComponent().registerClassPolicy(OnRMActionExecution.class);
}",0.973384030418251
89167,"/** 
 * Invoke onRMActionExecution policy
 * @param nodeRef       node reference
 * @param name          action name
 * @param parameters    action parameters
 */
protected void invokeOnRMActionExecution(NodeRef nodeRef,String name,Map<String,Serializable> parameters){
  Set<QName> qnames=PoliciesUtil.getTypeAndAspectQNames(nodeService,nodeRef);
  OnRMActionExecution policy=onRMActionExecutionDelegate.get(qnames);
  policy.onRMActionExecution(nodeRef,name,parameters);
}","/** 
 * Invoke onRMActionExecution policy
 * @param nodeRef       node reference
 * @param name          action name
 * @param parameters    action parameters
 */
protected void invokeOnRMActionExecution(NodeRef nodeRef,String name,Map<String,Serializable> parameters){
  Set<QName> qnames=PoliciesUtil.getTypeAndAspectQNames(getNodeService(),nodeRef);
  OnRMActionExecution policy=onRMActionExecutionDelegate.get(qnames);
  policy.onRMActionExecution(nodeRef,name,parameters);
}",0.9926547743966422
89168,"/** 
 * Invoke beforeRMActionExecution policy
 * @param nodeRef       node reference
 * @param name          action name
 * @param parameters    action parameters
 */
protected void invokeBeforeRMActionExecution(NodeRef nodeRef,String name,Map<String,Serializable> parameters){
  Set<QName> qnames=PoliciesUtil.getTypeAndAspectQNames(nodeService,nodeRef);
  BeforeRMActionExecution policy=beforeRMActionExecutionDelegate.get(qnames);
  policy.beforeRMActionExecution(nodeRef,name,parameters);
}","/** 
 * Invoke beforeRMActionExecution policy
 * @param nodeRef       node reference
 * @param name          action name
 * @param parameters    action parameters
 */
protected void invokeBeforeRMActionExecution(NodeRef nodeRef,String name,Map<String,Serializable> parameters){
  Set<QName> qnames=PoliciesUtil.getTypeAndAspectQNames(getNodeService(),nodeRef);
  BeforeRMActionExecution policy=beforeRMActionExecutionDelegate.get(qnames);
  policy.beforeRMActionExecution(nodeRef,name,parameters);
}",0.9929506545820744
89169,"@Override protected boolean evaluateImpl(ActionCondition actionCondition,NodeRef actionedUponNodeRef){
  boolean result=false;
  String kind=((QName)actionCondition.getParameterValue(PARAM_KIND)).getLocalName();
  FilePlanComponentKind filePlanComponentKind=filePlanService.getFilePlanComponentKind(actionedUponNodeRef);
  if (filePlanComponentKind != null && filePlanComponentKind.toString().equals(kind)) {
    result=true;
  }
  return result;
}","@Override protected boolean evaluateImpl(ActionCondition actionCondition,NodeRef actionedUponNodeRef){
  boolean result=false;
  String kind=((QName)actionCondition.getParameterValue(PARAM_KIND)).getLocalName();
  FilePlanComponentKind filePlanComponentKind=getFilePlanService().getFilePlanComponentKind(actionedUponNodeRef);
  if (filePlanComponentKind != null && filePlanComponentKind.toString().equals(kind)) {
    result=true;
  }
  return result;
}",0.9922308546059934
89170,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  String actionName=action.getActionDefinitionName();
  if (isOkToProceedWithAction(actionedUponNodeRef,actionName)) {
    QName actionedUponType=nodeService.getType(actionedUponNodeRef);
    boolean targetIsUnfiledRecords;
    if (ACTION_FILETO.equals(action.getActionDefinitionName())) {
      targetIsUnfiledRecords=false;
    }
 else {
      targetIsUnfiledRecords=(dictionaryService.isSubClass(actionedUponType,ContentModel.TYPE_CONTENT) && !recordService.isFiled(actionedUponNodeRef)) || TYPE_UNFILED_RECORD_FOLDER.equals(actionedUponType);
    }
    NodeRef recordFolder=(NodeRef)action.getParameterValue(PARAM_DESTINATION_RECORD_FOLDER);
    if (recordFolder == null) {
      recordFolder=createOrResolvePath(action,actionedUponNodeRef,targetIsUnfiledRecords);
    }
    validateActionPostPathResolution(actionedUponNodeRef,recordFolder,actionName,targetIsUnfiledRecords);
    final NodeRef finalRecordFolder=recordFolder;
    AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
      @Override public Void doWork(){
        try {
          if (mode == CopyMoveLinkFileToActionMode.MOVE) {
            fileFolderService.move(actionedUponNodeRef,finalRecordFolder,null);
          }
 else           if (mode == CopyMoveLinkFileToActionMode.COPY) {
            fileFolderService.copy(actionedUponNodeRef,finalRecordFolder,null);
          }
 else           if (mode == CopyMoveLinkFileToActionMode.LINK) {
            recordService.link(actionedUponNodeRef,finalRecordFolder);
          }
        }
 catch (        FileNotFoundException fileNotFound) {
          throw new AlfrescoRuntimeException(""String_Node_Str"" + (mode == CopyMoveLinkFileToActionMode.MOVE ? ""String_Node_Str"" : ""String_Node_Str"") + ""String_Node_Str"",fileNotFound);
        }
        return null;
      }
    }
);
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  String actionName=action.getActionDefinitionName();
  if (isOkToProceedWithAction(actionedUponNodeRef,actionName)) {
    QName actionedUponType=nodeService.getType(actionedUponNodeRef);
    boolean targetIsUnfiledRecords;
    if (ACTION_FILETO.equals(action.getActionDefinitionName())) {
      targetIsUnfiledRecords=false;
    }
 else {
      targetIsUnfiledRecords=(dictionaryService.isSubClass(actionedUponType,ContentModel.TYPE_CONTENT) && !recordService.isFiled(actionedUponNodeRef)) || TYPE_UNFILED_RECORD_FOLDER.equals(actionedUponType);
    }
    NodeRef recordFolder=(NodeRef)action.getParameterValue(PARAM_DESTINATION_RECORD_FOLDER);
    if (recordFolder == null) {
      recordFolder=createOrResolvePath(action,actionedUponNodeRef,targetIsUnfiledRecords);
    }
    validateActionPostPathResolution(actionedUponNodeRef,recordFolder,actionName,targetIsUnfiledRecords);
    final NodeRef finalRecordFolder=recordFolder;
    AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
      @Override public Void doWork(){
        try {
          if (getMode() == CopyMoveLinkFileToActionMode.MOVE) {
            fileFolderService.move(actionedUponNodeRef,finalRecordFolder,null);
          }
 else           if (getMode() == CopyMoveLinkFileToActionMode.COPY) {
            fileFolderService.copy(actionedUponNodeRef,finalRecordFolder,null);
          }
 else           if (getMode() == CopyMoveLinkFileToActionMode.LINK) {
            recordService.link(actionedUponNodeRef,finalRecordFolder);
          }
        }
 catch (        FileNotFoundException fileNotFound) {
          throw new AlfrescoRuntimeException(""String_Node_Str"" + (mode == CopyMoveLinkFileToActionMode.MOVE ? ""String_Node_Str"" : ""String_Node_Str"") + ""String_Node_Str"",fileNotFound);
        }
        return null;
      }
    }
);
  }
}",0.9905910735826297
89171,"@Override public void init(){
  super.init();
  this.mode=CopyMoveLinkFileToActionMode.COPY;
}","@Override public void init(){
  super.init();
  setMode(CopyMoveLinkFileToActionMode.COPY);
}",0.9411764705882352
89172,"@Override public void init(){
  super.init();
  this.mode=CopyMoveLinkFileToActionMode.MOVE;
}","@Override public void init(){
  super.init();
  setMode(CopyMoveLinkFileToActionMode.MOVE);
}",0.9411764705882352
89173,"@Override public void init(){
  super.init();
  this.mode=CopyMoveLinkFileToActionMode.LINK;
}","@Override public void init(){
  super.init();
  setMode(CopyMoveLinkFileToActionMode.LINK);
}",0.9411764705882352
89174,"@Override public void init(){
  super.init();
  this.mode=CopyMoveLinkFileToActionMode.MOVE;
}","@Override public void init(){
  super.init();
  setMode(CopyMoveLinkFileToActionMode.MOVE);
}",0.9411764705882352
89175,"/** 
 * @return   audit event label
 */
public String getLabel(){
  String lookup=I18NUtil.getMessage(label);
  if (lookup == null) {
    lookup=label;
  }
  return lookup;
}","/** 
 * @return   audit event label
 */
public String getLabel(){
  String lookup=I18NUtil.getMessage(label);
  if (StringUtils.isBlank(lookup)) {
    lookup=label;
  }
  return lookup;
}",0.9196675900277008
89176,"/** 
 * Default constructor.
 * @param name  audit event name
 * @param label audit event label (can be actual label or I18N lookup key)
 */
public AuditEvent(String name,String label){
  ParameterCheck.mandatory(""String_Node_Str"",name);
  ParameterCheck.mandatory(""String_Node_Str"",label);
  this.name=name;
  this.label=label;
}","/** 
 * Default constructor.
 * @param name  audit event name
 * @param label audit event label (can be actual label or I18N lookup key)
 */
public AuditEvent(String name,String label){
  ParameterCheck.mandatory(""String_Node_Str"",name);
  ParameterCheck.mandatory(""String_Node_Str"",label);
  setName(name);
  setLabel(label);
}",0.917933130699088
89177,"/** 
 * Compare by label.
 * @param compare   compare to audit event
 * @return int      
 */
@Override public int compareTo(AuditEvent compare){
  return getLabel().compareTo(compare.getLabel());
}","/** 
 * Compare by label.
 * @param compare   compare to audit event
 * @return int
 */
@Override public int compareTo(AuditEvent compare){
  return getLabel().compareTo(compare.getLabel());
}",0.9846153846153848
89178,"/** 
 * @see org.alfresco.repo.node.NodeServicePolicies.OnCreateNodePolicy#onCreateNode(org.alfresco.service.cmr.repository.ChildAssociationRef)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,type=""String_Node_Str"") public void onCreateNode(ChildAssociationRef childAssocRef){
  recordsManagementAuditService.auditEvent(childAssocRef.getChildRef(),name);
}","/** 
 * @see org.alfresco.repo.node.NodeServicePolicies.OnCreateNodePolicy#onCreateNode(org.alfresco.service.cmr.repository.ChildAssociationRef)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,type=""String_Node_Str"") public void onCreateNode(ChildAssociationRef childAssocRef){
  recordsManagementAuditService.auditEvent(childAssocRef.getChildRef(),getName());
}",0.9903181189488244
89179,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.capability.declarative.DeclarativeCapability#evaluateImpl(org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public int evaluateImpl(NodeRef nodeRef){
  int result=AccessDecisionVoter.ACCESS_DENIED;
  for (  Capability capability : capabilities) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + capability.getName() + ""String_Node_Str""+ nodeRef.toString()+ ""String_Node_Str""+ name);
    }
    int capabilityResult=capability.evaluate(nodeRef);
    if (capabilityResult != AccessDecisionVoter.ACCESS_DENIED) {
      result=AccessDecisionVoter.ACCESS_ABSTAIN;
      if (!isUndetermined() && capabilityResult == AccessDecisionVoter.ACCESS_GRANTED) {
        result=AccessDecisionVoter.ACCESS_GRANTED;
      }
      break;
    }
 else {
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + capability.getName() + ""String_Node_Str""+ nodeRef.toString()+ ""String_Node_Str""+ name);
      }
    }
  }
  return result;
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.capability.declarative.DeclarativeCapability#evaluateImpl(org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public int evaluateImpl(NodeRef nodeRef){
  int result=AccessDecisionVoter.ACCESS_DENIED;
  for (  Capability capability : capabilities) {
    if (LOGGER.isDebugEnabled()) {
      LOGGER.debug(""String_Node_Str"" + capability.getName() + ""String_Node_Str""+ nodeRef.toString()+ ""String_Node_Str""+ name);
    }
    int capabilityResult=capability.evaluate(nodeRef);
    if (capabilityResult != AccessDecisionVoter.ACCESS_DENIED) {
      result=AccessDecisionVoter.ACCESS_ABSTAIN;
      if (!isUndetermined() && capabilityResult == AccessDecisionVoter.ACCESS_GRANTED) {
        result=AccessDecisionVoter.ACCESS_GRANTED;
      }
      break;
    }
 else {
      if (LOGGER.isDebugEnabled()) {
        LOGGER.debug(""String_Node_Str"" + capability.getName() + ""String_Node_Str""+ nodeRef.toString()+ ""String_Node_Str""+ name);
      }
    }
  }
  return result;
}",0.9766081871345028
89180,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.capability.declarative.DeclarativeCapability#evaluate(org.alfresco.service.cmr.repository.NodeRef)
 */
public int evaluate(NodeRef nodeRef){
  if (nodeRef != null) {
    if (getFilePlanService().isFilePlanComponent(nodeRef)) {
      return checkRmRead(nodeRef);
    }
 else {
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + nodeRef.toString() + ""String_Node_Str"");
      }
    }
  }
  return AccessDecisionVoter.ACCESS_ABSTAIN;
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.capability.declarative.DeclarativeCapability#evaluate(org.alfresco.service.cmr.repository.NodeRef)
 */
public int evaluate(NodeRef nodeRef){
  if (nodeRef != null) {
    if (getFilePlanService().isFilePlanComponent(nodeRef)) {
      return checkRmRead(nodeRef);
    }
 else {
      if (LOGGER.isDebugEnabled()) {
        LOGGER.debug(""String_Node_Str"" + nodeRef.toString() + ""String_Node_Str"");
      }
    }
  }
  return AccessDecisionVoter.ACCESS_ABSTAIN;
}",0.9766990291262136
89181,"/** 
 * @see org.alfresco.repo.node.NodeServicePolicies.OnCreateNodePolicy#onCreateNode(org.alfresco.service.cmr.repository.ChildAssociationRef)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,notificationFrequency=NotificationFrequency.TRANSACTION_COMMIT) public void onCreateNode(final ChildAssociationRef childAssocRef){
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
  AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
    @Override public Void doWork(){
      filePlanPermissionService.setupRecordCategoryPermissions(childAssocRef.getChildRef());
      return null;
    }
  }
);
}","/** 
 * @see org.alfresco.repo.node.NodeServicePolicies.OnCreateNodePolicy#onCreateNode(org.alfresco.service.cmr.repository.ChildAssociationRef)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,notificationFrequency=NotificationFrequency.TRANSACTION_COMMIT) public void onCreateNode(final ChildAssociationRef childAssocRef){
  if (LOGGER.isDebugEnabled()) {
    LOGGER.debug(""String_Node_Str"");
  }
  AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
    @Override public Void doWork(){
      filePlanPermissionService.setupRecordCategoryPermissions(childAssocRef.getChildRef());
      return null;
    }
  }
);
}",0.9805825242718448
89182,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.ModulePatch#apply()
 */
@Override public void apply(){
  if (logger.isInfoEnabled()) {
    logger.info(""String_Node_Str"" + description + ""String_Node_Str"");
  }
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + id + ""String_Node_Str""+ moduleId+ ""String_Node_Str""+ fixesFromSchema+ ""String_Node_Str""+ fixesToSchema+ ""String_Node_Str""+ targetSchema);
  }
  transactionService.getRetryingTransactionHelper().doInTransaction(new ApplyCallback(),true,false);
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.ModulePatch#apply()
 */
@Override public void apply(){
  if (LOGGER.isInfoEnabled()) {
    LOGGER.info(""String_Node_Str"" + description + ""String_Node_Str"");
  }
  if (LOGGER.isDebugEnabled()) {
    LOGGER.debug(""String_Node_Str"" + id + ""String_Node_Str""+ moduleId+ ""String_Node_Str""+ fixesFromSchema+ ""String_Node_Str""+ fixesToSchema+ ""String_Node_Str""+ targetSchema);
  }
  transactionService.getRetryingTransactionHelper().doInTransaction(new ApplyCallback(),true,false);
  if (LOGGER.isDebugEnabled()) {
    LOGGER.debug(""String_Node_Str"");
  }
}",0.9410801963993454
89183,"/** 
 * @see org.alfresco.repo.module.AbstractModuleComponent#executeInternal()
 */
@Override protected void executeInternal(){
  int currentSchema=getCurrentSchema();
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + currentSchema + ""String_Node_Str""+ moduleSchema+ ""String_Node_Str"");
  }
  if (moduleSchema > currentSchema) {
    List<ModulePatch> patchesToApply=new ArrayList<ModulePatch>(13);
    for (    ModulePatch modulePatch : modulePatches.values()) {
      if (modulePatch.getFixesFromSchema() <= currentSchema && modulePatch.getFixesToSchema() >= currentSchema) {
        patchesToApply.add(modulePatch);
      }
    }
    Collections.sort(patchesToApply);
    for (    ModulePatch patchToApply : patchesToApply) {
      patchToApply.apply();
    }
    updateSchema(moduleSchema);
  }
}","/** 
 * @see org.alfresco.repo.module.AbstractModuleComponent#executeInternal()
 */
@Override protected void executeInternal(){
  int currentSchema=getCurrentSchema();
  if (LOGGER.isDebugEnabled()) {
    LOGGER.debug(""String_Node_Str"" + currentSchema + ""String_Node_Str""+ moduleSchema+ ""String_Node_Str"");
  }
  if (moduleSchema > currentSchema) {
    List<ModulePatch> patchesToApply=new ArrayList<ModulePatch>(13);
    for (    ModulePatch modulePatch : modulePatches.values()) {
      if (modulePatch.getFixesFromSchema() <= currentSchema && modulePatch.getFixesToSchema() >= currentSchema) {
        patchesToApply.add(modulePatch);
      }
    }
    Collections.sort(patchesToApply);
    for (    ModulePatch patchToApply : patchesToApply) {
      patchToApply.apply();
    }
    updateSchema(moduleSchema);
  }
}",0.9853479853479854
89184,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.ModulePatchExecuter#register(org.alfresco.module.org_alfresco_module_rm.patch.ModulePatch)
 */
@Override public void register(ModulePatch modulePatch){
  if (!getModuleId().equals(modulePatch.getModuleId())) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + modulePatch.getId() + ""String_Node_Str""+ getModuleId());
  }
  modulePatches.put(modulePatch.getId(),modulePatch);
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.ModulePatchExecuter#register(org.alfresco.module.org_alfresco_module_rm.patch.ModulePatch)
 */
@Override public void register(ModulePatch modulePatch){
  if (!getModuleId().equals(modulePatch.getModuleId())) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
  if (LOGGER.isDebugEnabled()) {
    LOGGER.debug(""String_Node_Str"" + modulePatch.getId() + ""String_Node_Str""+ getModuleId());
  }
  modulePatches.put(modulePatch.getId(),modulePatch);
}",0.9769673704414588
89185,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.AbstractModulePatch#applyInternal()
 */
@Override public void applyInternal(){
  Set<NodeRef> filePlans=getFilePlans();
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + filePlans.size() + ""String_Node_Str"");
  }
  for (  NodeRef filePlan : filePlans) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + filePlan.toString());
    }
    applyCapabilityPatch(filePlan);
  }
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.AbstractModulePatch#applyInternal()
 */
@Override public void applyInternal(){
  Set<NodeRef> filePlans=getFilePlans();
  if (LOGGER.isDebugEnabled()) {
    LOGGER.debug(""String_Node_Str"" + filePlans.size() + ""String_Node_Str"");
  }
  for (  NodeRef filePlan : filePlans) {
    if (LOGGER.isDebugEnabled()) {
      LOGGER.debug(""String_Node_Str"" + filePlan.toString());
    }
    applyCapabilityPatch(filePlan);
  }
}",0.9498956158663884
89186,"/** 
 * Adds a new capability to the specified roles.
 * @param filePlan          file plan
 * @param capabilityName    capability name
 * @param roles             roles
 */
protected void addCapability(NodeRef filePlan,String capabilityName,String... roles){
  Capability capability=capabilityService.getCapability(capabilityName);
  if (capability == null) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + capabilityName + ""String_Node_Str"");
  }
  for (  String roleName : roles) {
    Role role=filePlanRoleService.getRole(filePlan,roleName);
    if (role != null) {
      Set<Capability> capabilities=role.getCapabilities();
      if (!capabilities.contains(capability)) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"" + capabilityName + ""String_Node_Str""+ role.getName());
        }
        capabilities.add(capability);
        filePlanRoleService.updateRole(filePlan,role.getName(),role.getDisplayLabel(),capabilities);
      }
    }
  }
}","/** 
 * Adds a new capability to the specified roles.
 * @param filePlan          file plan
 * @param capabilityName    capability name
 * @param roles             roles
 */
protected void addCapability(NodeRef filePlan,String capabilityName,String... roles){
  Capability capability=capabilityService.getCapability(capabilityName);
  if (capability == null) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + capabilityName + ""String_Node_Str"");
  }
  for (  String roleName : roles) {
    Role role=filePlanRoleService.getRole(filePlan,roleName);
    if (role != null) {
      Set<Capability> capabilities=role.getCapabilities();
      if (!capabilities.contains(capability)) {
        if (LOGGER.isDebugEnabled()) {
          LOGGER.debug(""String_Node_Str"" + capabilityName + ""String_Node_Str""+ role.getName());
        }
        capabilities.add(capability);
        filePlanRoleService.updateRole(filePlan,role.getName(),role.getDisplayLabel(),capabilities);
      }
    }
  }
}",0.9879154078549848
89187,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.AbstractModulePatch#applyInternal()
 */
@Override public void applyInternal(){
  Long maxNodeId=patchDAO.getMaxAdmNodeID();
  for (  QName qname : qnames) {
    QName origional=QName.createQName(RecordsManagementModel.RM_URI,qname.getLocalName());
    if (qnameDAO.getQName(origional) != null) {
      qnameDAO.updateQName(origional,qname);
    }
  }
  long recordCount=patchDAO.getCountNodesWithAspects(Collections.singleton(ASPECT_RECORD));
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + recordCount + ""String_Node_Str"");
  }
  int completed=0;
  Pair<Long,QName> recordAspect=qnameDAO.getQName(ASPECT_RECORD);
  if (recordAspect != null) {
    for (Long i=0L; i < maxNodeId; i+=BATCH_SIZE) {
      List<Long> nodeIds=patchDAO.getNodesByAspectQNameId(recordAspect.getFirst(),i,i + BATCH_SIZE);
      for (      Long nodeId : nodeIds) {
        nodeDAO.addNodeAspects(nodeId,Collections.singleton(DOD5015Model.ASPECT_DOD_5015_RECORD));
      }
      completed+=completed + nodeIds.size();
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + completed + ""String_Node_Str""+ recordCount);
      }
    }
  }
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.AbstractModulePatch#applyInternal()
 */
@Override public void applyInternal(){
  Long maxNodeId=patchDAO.getMaxAdmNodeID();
  for (  QName qname : qnames) {
    QName origional=QName.createQName(RecordsManagementModel.RM_URI,qname.getLocalName());
    if (qnameDAO.getQName(origional) != null) {
      qnameDAO.updateQName(origional,qname);
    }
  }
  long recordCount=patchDAO.getCountNodesWithAspects(Collections.singleton(ASPECT_RECORD));
  if (LOGGER.isDebugEnabled()) {
    LOGGER.debug(""String_Node_Str"" + recordCount + ""String_Node_Str"");
  }
  int completed=0;
  Pair<Long,QName> recordAspect=qnameDAO.getQName(ASPECT_RECORD);
  if (recordAspect != null) {
    for (Long i=0L; i < maxNodeId; i+=BATCH_SIZE) {
      List<Long> nodeIds=patchDAO.getNodesByAspectQNameId(recordAspect.getFirst(),i,i + BATCH_SIZE);
      for (      Long nodeId : nodeIds) {
        nodeDAO.addNodeAspects(nodeId,Collections.singleton(DOD5015Model.ASPECT_DOD_5015_RECORD));
      }
      completed+=completed + nodeIds.size();
      if (LOGGER.isDebugEnabled()) {
        LOGGER.debug(""String_Node_Str"" + completed + ""String_Node_Str""+ recordCount);
      }
    }
  }
}",0.980279375513558
89188,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.security.RecordsManagementSecurityService#setPermission(org.alfresco.service.cmr.repository.NodeRef,java.lang.String,java.lang.String,boolean)
 */
public void setPermission(final NodeRef nodeRef,final String authority,final String permission){
  ParameterCheck.mandatory(""String_Node_Str"",nodeRef);
  ParameterCheck.mandatory(""String_Node_Str"",authority);
  ParameterCheck.mandatory(""String_Node_Str"",permission);
  runAsSystem(new AuthenticationUtil.RunAsWork<Object>(){
    public Void doWork(){
      if (isFilePlan(nodeRef)) {
        setPermissionDown(nodeRef,authority,permission);
      }
 else       if (isFilePlanContainer(nodeRef) || isRecordFolder(nodeRef) || isRecord(nodeRef)|| isHold(nodeRef)) {
        setReadPermissionUp(nodeRef,authority);
        setPermissionDown(nodeRef,authority,permission);
      }
 else {
        if (logger.isWarnEnabled()) {
          logger.warn(""String_Node_Str"" + nodeRef + ""String_Node_Str""+ authority+ ""String_Node_Str""+ permission+ ""String_Node_Str"");
        }
      }
      return null;
    }
  }
);
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.security.RecordsManagementSecurityService#setPermission(org.alfresco.service.cmr.repository.NodeRef,java.lang.String,java.lang.String,boolean)
 */
public void setPermission(final NodeRef nodeRef,final String authority,final String permission){
  ParameterCheck.mandatory(""String_Node_Str"",nodeRef);
  ParameterCheck.mandatory(""String_Node_Str"",authority);
  ParameterCheck.mandatory(""String_Node_Str"",permission);
  runAsSystem(new AuthenticationUtil.RunAsWork<Object>(){
    public Void doWork(){
      if (isFilePlan(nodeRef)) {
        setPermissionDown(nodeRef,authority,permission);
      }
 else       if (isFilePlanContainer(nodeRef) || isRecordFolder(nodeRef) || isRecord(nodeRef)|| isHold(nodeRef)) {
        setReadPermissionUp(nodeRef,authority);
        setPermissionDown(nodeRef,authority,permission);
      }
 else {
        if (LOGGER.isWarnEnabled()) {
          LOGGER.warn(""String_Node_Str"" + nodeRef + ""String_Node_Str""+ authority+ ""String_Node_Str""+ permission+ ""String_Node_Str"");
        }
      }
      return null;
    }
  }
);
}",0.9891794409377818
89189,"/** 
 * Indicates whether this is a RM security check or not
 * @param newValue  true if RM security check, false otherwise
 */
public static void isRMSecurityChecked(boolean newValue){
  if (logger.isDebugEnabled()) {
    RMMethodSecurityInterceptor.IS_RM_SECURITY_CHECK.set(newValue);
  }
}","/** 
 * Indicates whether this is a RM security check or not
 * @param newValue  true if RM security check, false otherwise
 */
public static void isRMSecurityChecked(boolean newValue){
  if (LOGGER.isDebugEnabled()) {
    RMMethodSecurityInterceptor.IS_RM_SECURITY_CHECK.set(newValue);
  }
}",0.9794520547945206
89190,"/** 
 * Report capability status.
 * @param name      capability name
 * @param status    capability status
 */
public static void reportCapabilityStatus(String name,int status){
  if (logger.isDebugEnabled()) {
    CapabilityReport capability=getCapabilityReport(name);
    capability.status=translate(status);
  }
}","/** 
 * Report capability status.
 * @param name      capability name
 * @param status    capability status
 */
public static void reportCapabilityStatus(String name,int status){
  if (LOGGER.isDebugEnabled()) {
    CapabilityReport capability=getCapabilityReport(name);
    capability.status=translate(status);
  }
}",0.9810725552050472
89191,"/** 
 * Gets the failure report for the currently recorded capabilities.
 * @return  {@link String}  capability error report
 */
public String getFailureReport(){
  String result=null;
  if (logger.isDebugEnabled()) {
    Collection<CapabilityReport> capabilities=RMMethodSecurityInterceptor.CAPABILITIES.get().values();
    if (!capabilities.isEmpty()) {
      StringBuffer buffer=new StringBuffer(""String_Node_Str"");
      for (      CapabilityReport capability : capabilities) {
        buffer.append(""String_Node_Str"").append(capability.name).append(""String_Node_Str"").append(capability.status).append(""String_Node_Str"");
        if (!capability.conditions.isEmpty()) {
          for (          Map.Entry<String,Boolean> entry : capability.conditions.entrySet()) {
            buffer.append(""String_Node_Str"").append(entry.getKey()).append(""String_Node_Str"");
            if (entry.getValue()) {
              buffer.append(""String_Node_Str"");
            }
 else {
              buffer.append(""String_Node_Str"");
            }
            buffer.append(""String_Node_Str"");
          }
        }
      }
      result=buffer.toString();
    }
  }
  return result;
}","/** 
 * Gets the failure report for the currently recorded capabilities.
 * @return  {@link String}  capability error report
 */
public String getFailureReport(){
  String result=null;
  if (LOGGER.isDebugEnabled()) {
    Collection<CapabilityReport> capabilities=RMMethodSecurityInterceptor.CAPABILITIES.get().values();
    if (!capabilities.isEmpty()) {
      StringBuffer buffer=new StringBuffer(""String_Node_Str"");
      for (      CapabilityReport capability : capabilities) {
        buffer.append(""String_Node_Str"").append(capability.name).append(""String_Node_Str"").append(capability.status).append(""String_Node_Str"");
        if (!capability.conditions.isEmpty()) {
          for (          Map.Entry<String,Boolean> entry : capability.conditions.entrySet()) {
            buffer.append(""String_Node_Str"").append(entry.getKey()).append(""String_Node_Str"");
            if (entry.getValue()) {
              buffer.append(""String_Node_Str"");
            }
 else {
              buffer.append(""String_Node_Str"");
            }
            buffer.append(""String_Node_Str"");
          }
        }
      }
      result=buffer.toString();
    }
  }
  return result;
}",0.9948630136986302
89192,"/** 
 * Report capability condition.
 * @param name              capability name
 * @param conditionName     capability condition name
 * @param expected          expected value
 * @param actual            actual value
 */
public static void reportCapabilityCondition(String name,String conditionName,boolean expected,boolean actual){
  if (logger.isDebugEnabled()) {
    CapabilityReport capability=getCapabilityReport(name);
    if (!expected) {
      conditionName=""String_Node_Str"" + conditionName;
    }
    capability.conditions.put(conditionName,(expected == actual));
  }
}","/** 
 * Report capability condition.
 * @param name              capability name
 * @param conditionName     capability condition name
 * @param expected          expected value
 * @param actual            actual value
 */
public static void reportCapabilityCondition(String name,String conditionName,boolean expected,boolean actual){
  if (LOGGER.isDebugEnabled()) {
    CapabilityReport capability=getCapabilityReport(name);
    if (!expected) {
      conditionName=""String_Node_Str"" + conditionName;
    }
    capability.conditions.put(conditionName,(expected == actual));
  }
}",0.9896729776247848
89193,"public static void addMessage(String message,Object... params){
  if (logger.isDebugEnabled()) {
    addMessage(MessageFormat.format(message,params));
  }
}","public static void addMessage(String message,Object... params){
  if (LOGGER.isDebugEnabled()) {
    addMessage(MessageFormat.format(message,params));
  }
}",0.9615384615384616
89194,"/** 
 * @see net.sf.acegisecurity.intercept.AbstractSecurityInterceptor#beforeInvocation(java.lang.Object)
 */
@Override protected InterceptorStatusToken beforeInvocation(Object object){
  InterceptorStatusToken result=null;
  try {
    RMMethodSecurityInterceptor.CAPABILITIES.remove();
    RMMethodSecurityInterceptor.IS_RM_SECURITY_CHECK.remove();
    RMMethodSecurityInterceptor.MESSAGES.remove();
    result=super.beforeInvocation(object);
  }
 catch (  AccessDeniedException exception) {
    if (logger.isDebugEnabled()) {
      MethodInvocation mi=(MethodInvocation)object;
      StringBuilder methodDetails=new StringBuilder(""String_Node_Str"");
      if (RMMethodSecurityInterceptor.IS_RM_SECURITY_CHECK.get()) {
        methodDetails.append(""String_Node_Str"");
      }
 else {
        methodDetails.append(""String_Node_Str"");
      }
      boolean first=true;
      methodDetails.append(""String_Node_Str"").append(mi.getMethod().getName()).append(""String_Node_Str"");
      for (      Object arg : mi.getArguments()) {
        if (first) {
          first=false;
        }
 else {
          methodDetails.append(""String_Node_Str"");
        }
        if (arg != null) {
          methodDetails.append(arg.toString());
        }
 else {
          methodDetails.append(""String_Node_Str"");
        }
      }
      methodDetails.append(""String_Node_Str"");
      List<String> messages=RMMethodSecurityInterceptor.MESSAGES.get();
      for (      String message : messages) {
        methodDetails.append(message).append(""String_Node_Str"");
      }
      String failureReport=getFailureReport();
      if (failureReport == null) {
        throw new AccessDeniedException(exception.getMessage() + methodDetails,exception);
      }
 else {
        throw new AccessDeniedException(exception.getMessage() + methodDetails + getFailureReport(),exception);
      }
    }
 else {
      throw exception;
    }
  }
  return result;
}","/** 
 * @see net.sf.acegisecurity.intercept.AbstractSecurityInterceptor#beforeInvocation(java.lang.Object)
 */
@Override protected InterceptorStatusToken beforeInvocation(Object object){
  InterceptorStatusToken result=null;
  try {
    RMMethodSecurityInterceptor.CAPABILITIES.remove();
    RMMethodSecurityInterceptor.IS_RM_SECURITY_CHECK.remove();
    RMMethodSecurityInterceptor.MESSAGES.remove();
    result=super.beforeInvocation(object);
  }
 catch (  AccessDeniedException exception) {
    if (LOGGER.isDebugEnabled()) {
      MethodInvocation mi=(MethodInvocation)object;
      StringBuilder methodDetails=new StringBuilder(""String_Node_Str"");
      if (RMMethodSecurityInterceptor.IS_RM_SECURITY_CHECK.get()) {
        methodDetails.append(""String_Node_Str"");
      }
 else {
        methodDetails.append(""String_Node_Str"");
      }
      boolean first=true;
      methodDetails.append(""String_Node_Str"").append(mi.getMethod().getName()).append(""String_Node_Str"");
      for (      Object arg : mi.getArguments()) {
        if (first) {
          first=false;
        }
 else {
          methodDetails.append(""String_Node_Str"");
        }
        if (arg != null) {
          methodDetails.append(arg.toString());
        }
 else {
          methodDetails.append(""String_Node_Str"");
        }
      }
      methodDetails.append(""String_Node_Str"");
      List<String> messages=RMMethodSecurityInterceptor.MESSAGES.get();
      for (      String message : messages) {
        methodDetails.append(message).append(""String_Node_Str"");
      }
      String failureReport=getFailureReport();
      if (failureReport == null) {
        throw new AccessDeniedException(exception.getMessage() + methodDetails,exception);
      }
 else {
        throw new AccessDeniedException(exception.getMessage() + methodDetails + getFailureReport(),exception);
      }
    }
 else {
      throw exception;
    }
  }
  return result;
}",0.9968798751950078
89195,"public void setAllowedValues(String[] values){
  this.allowedValues=values;
}","public void setAllowedValues(String[] values){
  this.allowedValues=values.clone();
}",0.9506172839506172
89196,"/** 
 * @param name
 * @param displayLabel
 * @param actions
 */
protected ScriptCapability(String name,String displayLabel,String[] actions){
  this.name=name;
  this.displayLabel=displayLabel;
  this.actions=actions;
}","/** 
 * @param name
 * @param displayLabel
 * @param actions
 */
protected ScriptCapability(String name,String displayLabel,String[] actions){
  this.name=name;
  this.displayLabel=displayLabel;
  this.actions=actions.clone();
}",0.9821428571428572
89197,"public Group(String id,String label,Property[] properties){
  this.id=id;
  this.label=label;
  this.properties=properties;
}","public Group(String id,String label,Property[] properties){
  this.id=id;
  this.label=label;
  this.properties=properties.clone();
}",0.9689922480620154
89198,"/** 
 * @see org.springframework.context.ApplicationContextAware#setApplicationContext(org.springframework.context.ApplicationContext)
 */
@Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
  this.applicationContext=applicationContext;
}","/** 
 * @see org.springframework.context.ApplicationContextAware#setApplicationContext(org.springframework.context.ApplicationContext)
 */
@Override public void setApplicationContext(ApplicationContext applicationContext){
  this.applicationContext=applicationContext;
}",0.9484902309058616
89199,"@SuppressWarnings({""String_Node_Str"",""String_Node_Str""}) private Map decide(Authentication authentication,Object object,ConfigAttributeDefinition config,Map returnedObject) throws AccessDeniedException {
  try {
    if (returnedObject.containsKey(RecordsManagementModel.PROP_HOLD_REASON)) {
      HashMap filtered=new HashMap();
      filtered.putAll(returnedObject);
      String protocol=DefaultTypeConverter.INSTANCE.convert(String.class,filtered.get(ContentModel.PROP_STORE_PROTOCOL));
      String identifier=DefaultTypeConverter.INSTANCE.convert(String.class,filtered.get(ContentModel.PROP_STORE_IDENTIFIER));
      String uuid=DefaultTypeConverter.INSTANCE.convert(String.class,filtered.get(ContentModel.PROP_NODE_UUID));
      StoreRef storeRef=new StoreRef(protocol,identifier);
      NodeRef nodeRef=new NodeRef(storeRef,uuid);
      if ((nodeRef == null) || (permissionService.hasPermission(getFilePlanService().getFilePlan(nodeRef),RMPermissionModel.VIEW_UPDATE_REASONS_FOR_FREEZE) != AccessStatus.ALLOWED)) {
        filtered.remove(RecordsManagementModel.PROP_HOLD_REASON);
      }
      return filtered;
    }
 else {
      return returnedObject;
    }
  }
 catch (  ClassCastException ex) {
    return returnedObject;
  }
}","@SuppressWarnings({""String_Node_Str"",""String_Node_Str""}) private Map decide(Authentication authentication,Object object,ConfigAttributeDefinition config,Map returnedObject){
  try {
    if (returnedObject.containsKey(RecordsManagementModel.PROP_HOLD_REASON)) {
      HashMap filtered=new HashMap();
      filtered.putAll(returnedObject);
      String protocol=DefaultTypeConverter.INSTANCE.convert(String.class,filtered.get(ContentModel.PROP_STORE_PROTOCOL));
      String identifier=DefaultTypeConverter.INSTANCE.convert(String.class,filtered.get(ContentModel.PROP_STORE_IDENTIFIER));
      String uuid=DefaultTypeConverter.INSTANCE.convert(String.class,filtered.get(ContentModel.PROP_NODE_UUID));
      StoreRef storeRef=new StoreRef(protocol,identifier);
      NodeRef nodeRef=new NodeRef(storeRef,uuid);
      if ((nodeRef == null) || (permissionService.hasPermission(getFilePlanService().getFilePlan(nodeRef),RMPermissionModel.VIEW_UPDATE_REASONS_FOR_FREEZE) != AccessStatus.ALLOWED)) {
        filtered.remove(RecordsManagementModel.PROP_HOLD_REASON);
      }
      return filtered;
    }
 else {
      return returnedObject;
    }
  }
 catch (  ClassCastException ex) {
    return returnedObject;
  }
}",0.9877450980392156
89200,"/** 
 * @see org.springframework.context.ApplicationContextAware#setApplicationContext(org.springframework.context.ApplicationContext)
 */
@Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
  this.applicationContext=applicationContext;
}","/** 
 * @see org.springframework.context.ApplicationContextAware#setApplicationContext(org.springframework.context.ApplicationContext)
 */
@Override public void setApplicationContext(ApplicationContext applicationContext){
  this.applicationContext=applicationContext;
}",0.9484902309058616
89201,"/** 
 * @see org.springframework.beans.factory.config.BeanFactoryPostProcessor#postProcessBeanFactory(org.springframework.beans.factory.config.ConfigurableListableBeanFactory)
 */
@Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {
  if (beanFactory.containsBean(BEAN_SITESERVICE_BOOTSTRAP) && beanFactory.containsBean(BEAN_RM_DICTIONARY_BOOTSTRAP)) {
    BeanDefinition beanDef=beanFactory.getBeanDefinition(BEAN_RM_DICTIONARY_BOOTSTRAP);
    beanDef.setDependsOn(new String[]{BEAN_SITESERVICE_BOOTSTRAP});
  }
}","/** 
 * @see org.springframework.beans.factory.config.BeanFactoryPostProcessor#postProcessBeanFactory(org.springframework.beans.factory.config.ConfigurableListableBeanFactory)
 */
@Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory){
  if (beanFactory.containsBean(BEAN_SITESERVICE_BOOTSTRAP) && beanFactory.containsBean(BEAN_RM_DICTIONARY_BOOTSTRAP)) {
    BeanDefinition beanDef=beanFactory.getBeanDefinition(BEAN_RM_DICTIONARY_BOOTSTRAP);
    beanDef.setDependsOn(new String[]{BEAN_SITESERVICE_BOOTSTRAP});
  }
}",0.979591836734694
89202,"/** 
 * @see org.springframework.context.ApplicationContextAware#setApplicationContext(org.springframework.context.ApplicationContext)
 */
@Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
  this.applicationContext=applicationContext;
}","/** 
 * @see org.springframework.context.ApplicationContextAware#setApplicationContext(org.springframework.context.ApplicationContext)
 */
@Override public void setApplicationContext(ApplicationContext applicationContext){
  this.applicationContext=applicationContext;
}",0.9484902309058616
89203,"/** 
 * Checks whether the report generator is applicable given the reported upon node reference. <p> Throws AlfrescoRuntimeException if applicability fails, with reason.
 * @param reportedUponNodeRef          reported upon node reference
 * @throws AlfrescoRuntimeException     if applicability check fails
 */
protected abstract void checkReportApplicability(NodeRef reportedUponNodeRef) throws AlfrescoRuntimeException ;","/** 
 * Checks whether the report generator is applicable given the reported upon node reference. <p> Throws AlfrescoRuntimeException if applicability fails, with reason.
 * @param reportedUponNodeRef          reported upon node reference
 */
protected abstract void checkReportApplicability(NodeRef reportedUponNodeRef);",0.8629032258064516
89204,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.report.generator.BaseReportGenerator#checkReportApplicability(org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void checkReportApplicability(NodeRef reportedUponNodeRef) throws AlfrescoRuntimeException {
  if (applicableTypes != null && applicableTypes.size() != 0) {
    boolean isTypeApplicable=false;
    QName type=nodeService.getType(reportedUponNodeRef);
    for (    QName applicableType : applicableTypes) {
      if (dictionaryService.isSubClass(type,applicableType)) {
        isTypeApplicable=true;
        break;
      }
    }
    if (!isTypeApplicable) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + type.toString() + ""String_Node_Str""+ reportType.toString()+ ""String_Node_Str"");
    }
  }
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.report.generator.BaseReportGenerator#checkReportApplicability(org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void checkReportApplicability(NodeRef reportedUponNodeRef){
  if (applicableTypes != null && applicableTypes.size() != 0) {
    boolean isTypeApplicable=false;
    QName type=nodeService.getType(reportedUponNodeRef);
    for (    QName applicableType : applicableTypes) {
      if (dictionaryService.isSubClass(type,applicableType)) {
        isTypeApplicable=true;
        break;
      }
    }
    if (!isTypeApplicable) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + type.toString() + ""String_Node_Str""+ reportType.toString()+ ""String_Node_Str"");
    }
  }
}",0.4977549711353431
89205,"/** 
 * @see org.springframework.context.ApplicationContextAware#setApplicationContext(org.springframework.context.ApplicationContext)
 */
@Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
  this.applicationContext=applicationContext;
}","/** 
 * @see org.springframework.context.ApplicationContextAware#setApplicationContext(org.springframework.context.ApplicationContext)
 */
@Override public void setApplicationContext(ApplicationContext applicationContext){
  this.applicationContext=applicationContext;
}",0.9484902309058616
89206,"/** 
 * @see org.springframework.beans.factory.config.BeanFactoryPostProcessor#postProcessBeanFactory(org.springframework.beans.factory.config.ConfigurableListableBeanFactory)
 */
@Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException {
  for (  String bean : getSecurityBeanNames(beanFactory)) {
    if (beanFactory.containsBeanDefinition(bean)) {
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + bean);
      }
      BeanDefinition beanDef=beanFactory.getBeanDefinition(bean);
      PropertyValue beanValue=beanDef.getPropertyValues().getPropertyValue(PROP_OBJECT_DEFINITION_SOURCE);
      if (beanValue != null) {
        String beanStringValue=(String)((TypedStringValue)beanValue.getValue()).getValue();
        String mergedStringValue=merge(beanStringValue);
        beanDef.getPropertyValues().addPropertyValue(PROP_OBJECT_DEFINITION_SOURCE,new TypedStringValue(mergedStringValue));
      }
    }
  }
}","/** 
 * @see org.springframework.beans.factory.config.BeanFactoryPostProcessor#postProcessBeanFactory(org.springframework.beans.factory.config.ConfigurableListableBeanFactory)
 */
@Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory){
  for (  String bean : getSecurityBeanNames(beanFactory)) {
    if (beanFactory.containsBeanDefinition(bean)) {
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + bean);
      }
      BeanDefinition beanDef=beanFactory.getBeanDefinition(bean);
      PropertyValue beanValue=beanDef.getPropertyValues().getPropertyValue(PROP_OBJECT_DEFINITION_SOURCE);
      if (beanValue != null) {
        String beanStringValue=(String)((TypedStringValue)beanValue.getValue()).getValue();
        String mergedStringValue=merge(beanStringValue);
        beanDef.getPropertyValues().addPropertyValue(PROP_OBJECT_DEFINITION_SOURCE,new TypedStringValue(mergedStringValue));
      }
    }
  }
}",0.9883189436262062
89207,"/** 
 * Gets the failure report for the currently recorded capabilities.
 * @return  {@link String}  capability error report
 */
public String getFailureReport(){
  String result=null;
  if (logger.isDebugEnabled()) {
    Collection<CapabilityReport> capabilities=RMMethodSecurityInterceptor.capabilities.get().values();
    if (!capabilities.isEmpty()) {
      StringBuffer buffer=new StringBuffer(""String_Node_Str"");
      for (      CapabilityReport capability : capabilities) {
        buffer.append(""String_Node_Str"").append(capability.name).append(""String_Node_Str"").append(capability.status).append(""String_Node_Str"");
        if (!capability.conditions.isEmpty()) {
          for (          Map.Entry<String,Boolean> entry : capability.conditions.entrySet()) {
            buffer.append(""String_Node_Str"").append(entry.getKey()).append(""String_Node_Str"");
            if (entry.getValue() == true) {
              buffer.append(""String_Node_Str"");
            }
 else {
              buffer.append(""String_Node_Str"");
            }
            buffer.append(""String_Node_Str"");
          }
        }
      }
      result=buffer.toString();
    }
  }
  return result;
}","/** 
 * Gets the failure report for the currently recorded capabilities.
 * @return  {@link String}  capability error report
 */
public String getFailureReport(){
  String result=null;
  if (logger.isDebugEnabled()) {
    Collection<CapabilityReport> capabilities=RMMethodSecurityInterceptor.CAPABILITIES.get().values();
    if (!capabilities.isEmpty()) {
      StringBuffer buffer=new StringBuffer(""String_Node_Str"");
      for (      CapabilityReport capability : capabilities) {
        buffer.append(""String_Node_Str"").append(capability.name).append(""String_Node_Str"").append(capability.status).append(""String_Node_Str"");
        if (!capability.conditions.isEmpty()) {
          for (          Map.Entry<String,Boolean> entry : capability.conditions.entrySet()) {
            buffer.append(""String_Node_Str"").append(entry.getKey()).append(""String_Node_Str"");
            if (entry.getValue() == true) {
              buffer.append(""String_Node_Str"");
            }
 else {
              buffer.append(""String_Node_Str"");
            }
            buffer.append(""String_Node_Str"");
          }
        }
      }
      result=buffer.toString();
    }
  }
  return result;
}",0.9897959183673468
89208,"/** 
 * Get capability report object from the thread local, creating one for the given capability name if one does not already exist.
 * @param name  capability name
 * @return {@link CapabilityReport} object containing information about the capability
 */
private static final CapabilityReport getCapabilityReport(String name){
  Map<String,CapabilityReport> map=RMMethodSecurityInterceptor.capabilities.get();
  CapabilityReport capability=map.get(name);
  if (capability == null) {
    capability=new CapabilityReport();
    capability.name=name;
    map.put(name,capability);
  }
  return capability;
}","/** 
 * Get capability report object from the thread local, creating one for the given capability name if one does not already exist.
 * @param name  capability name
 * @return {@link CapabilityReport} object containing information about the capability
 */
private static final CapabilityReport getCapabilityReport(String name){
  Map<String,CapabilityReport> map=RMMethodSecurityInterceptor.CAPABILITIES.get();
  CapabilityReport capability=map.get(name);
  if (capability == null) {
    capability=new CapabilityReport();
    capability.name=name;
    map.put(name,capability);
  }
  return capability;
}",0.9801980198019802
89209,"/** 
 * @see net.sf.acegisecurity.intercept.AbstractSecurityInterceptor#beforeInvocation(java.lang.Object)
 */
@Override protected InterceptorStatusToken beforeInvocation(Object object){
  InterceptorStatusToken result=null;
  try {
    RMMethodSecurityInterceptor.capabilities.remove();
    result=super.beforeInvocation(object);
  }
 catch (  AccessDeniedException exception) {
    String failureReport=getFailureReport();
    if (failureReport == null) {
      throw exception;
    }
 else {
      throw new AccessDeniedException(exception.getMessage() + getFailureReport(),exception);
    }
  }
  return result;
}","/** 
 * @see net.sf.acegisecurity.intercept.AbstractSecurityInterceptor#beforeInvocation(java.lang.Object)
 */
@Override protected InterceptorStatusToken beforeInvocation(Object object){
  InterceptorStatusToken result=null;
  try {
    RMMethodSecurityInterceptor.CAPABILITIES.remove();
    result=super.beforeInvocation(object);
  }
 catch (  AccessDeniedException exception) {
    String failureReport=getFailureReport();
    if (failureReport == null) {
      throw exception;
    }
 else {
      throw new AccessDeniedException(exception.getMessage() + getFailureReport(),exception);
    }
  }
  return result;
}",0.9805510534846028
89210,"@SuppressWarnings(""String_Node_Str"") public Map<String,String> doWork() throws Exception {
  Map<String,String> result=Collections.EMPTY_MAP;
  NodeRef filePlan=filePlanService.getFilePlanBySiteId(FilePlanService.DEFAULT_RM_SITE_ID);
  if (filePlan != null) {
    Set<QName> recordTypes=recordService.getRecordMetadataAspects(filePlan);
    result=new HashMap<String,String>(recordTypes.size());
    for (    QName recordType : recordTypes) {
      AspectDefinition aspectDefinition=dictionaryService.getAspect(recordType);
      if (aspectDefinition != null) {
        result.put(aspectDefinition.getName().getLocalName(),aspectDefinition.getTitle(new StaticMessageLookup()));
      }
    }
  }
  return result;
}","@SuppressWarnings(""String_Node_Str"") public Map<String,String> doWork(){
  Map<String,String> result=Collections.EMPTY_MAP;
  NodeRef filePlan=filePlanService.getFilePlanBySiteId(FilePlanService.DEFAULT_RM_SITE_ID);
  if (filePlan != null) {
    Set<QName> recordTypes=recordService.getRecordMetadataAspects(filePlan);
    result=new HashMap<String,String>(recordTypes.size());
    for (    QName recordType : recordTypes) {
      AspectDefinition aspectDefinition=dictionaryService.getAspect(recordType);
      if (aspectDefinition != null) {
        result.put(aspectDefinition.getName().getLocalName(),aspectDefinition.getTitle(new StaticMessageLookup()));
      }
    }
  }
  return result;
}",0.9872340425531916
89211,"/** 
 * @see org.alfresco.service.cmr.action.ParameterConstraint#getAllowableValues()
 */
protected Map<String,String> getAllowableValuesImpl(){
  return AuthenticationUtil.runAsSystem(new RunAsWork<Map<String,String>>(){
    @SuppressWarnings(""String_Node_Str"") public Map<String,String> doWork() throws Exception {
      Map<String,String> result=Collections.EMPTY_MAP;
      NodeRef filePlan=filePlanService.getFilePlanBySiteId(FilePlanService.DEFAULT_RM_SITE_ID);
      if (filePlan != null) {
        Set<QName> recordTypes=recordService.getRecordMetadataAspects(filePlan);
        result=new HashMap<String,String>(recordTypes.size());
        for (        QName recordType : recordTypes) {
          AspectDefinition aspectDefinition=dictionaryService.getAspect(recordType);
          if (aspectDefinition != null) {
            result.put(aspectDefinition.getName().getLocalName(),aspectDefinition.getTitle(new StaticMessageLookup()));
          }
        }
      }
      return result;
    }
  }
);
}","/** 
 * @see org.alfresco.service.cmr.action.ParameterConstraint#getAllowableValues()
 */
protected Map<String,String> getAllowableValuesImpl(){
  return AuthenticationUtil.runAsSystem(new RunAsWork<Map<String,String>>(){
    @SuppressWarnings(""String_Node_Str"") public Map<String,String> doWork(){
      Map<String,String> result=Collections.EMPTY_MAP;
      NodeRef filePlan=filePlanService.getFilePlanBySiteId(FilePlanService.DEFAULT_RM_SITE_ID);
      if (filePlan != null) {
        Set<QName> recordTypes=recordService.getRecordMetadataAspects(filePlan);
        result=new HashMap<String,String>(recordTypes.size());
        for (        QName recordType : recordTypes) {
          AspectDefinition aspectDefinition=dictionaryService.getAspect(recordType);
          if (aspectDefinition != null) {
            result.put(aspectDefinition.getName().getLocalName(),aspectDefinition.getTitle(new StaticMessageLookup()));
          }
        }
      }
      return result;
    }
  }
);
}",0.991
89212,"@Override public NodeRef doWork() throws Exception {
  return filePlanService.getFilePlanBySiteId(FilePlanService.DEFAULT_RM_SITE_ID);
}","@Override public NodeRef doWork(){
  return filePlanService.getFilePlanBySiteId(FilePlanService.DEFAULT_RM_SITE_ID);
}",0.9291338582677166
89213,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  if (!nodeService.exists(actionedUponNodeRef)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (!dictionaryService.isSubClass(nodeService.getType(actionedUponNodeRef),ContentModel.TYPE_CONTENT)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_RECORD)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ContentModel.ASPECT_WORKING_COPY)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_RECORD_REJECTION_DETAILS)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_SYNCED)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else {
    NodeRef filePlan=(NodeRef)action.getParameterValue(PARAM_FILE_PLAN);
    if (filePlan == null) {
      filePlan=filePlanAuthenticationService.runAsRmAdmin(new RunAsWork<NodeRef>(){
        @Override public NodeRef doWork() throws Exception {
          return filePlanService.getFilePlanBySiteId(FilePlanService.DEFAULT_RM_SITE_ID);
        }
      }
);
      if (filePlan == null) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"");
        }
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
    }
 else {
      if (!filePlanService.isFilePlan(filePlan)) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"");
        }
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
    }
    boolean hideRecord=false;
    Boolean hideRecordValue=((Boolean)action.getParameterValue(PARAM_HIDE_RECORD));
    if (hideRecordValue != null) {
      hideRecord=hideRecordValue.booleanValue();
    }
    recordService.createRecord(filePlan,actionedUponNodeRef,!hideRecord);
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  if (!nodeService.exists(actionedUponNodeRef)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (!dictionaryService.isSubClass(nodeService.getType(actionedUponNodeRef),ContentModel.TYPE_CONTENT)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_RECORD)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ContentModel.ASPECT_WORKING_COPY)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_RECORD_REJECTION_DETAILS)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_SYNCED)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else {
    NodeRef filePlan=(NodeRef)action.getParameterValue(PARAM_FILE_PLAN);
    if (filePlan == null) {
      filePlan=filePlanAuthenticationService.runAsRmAdmin(new RunAsWork<NodeRef>(){
        @Override public NodeRef doWork(){
          return filePlanService.getFilePlanBySiteId(FilePlanService.DEFAULT_RM_SITE_ID);
        }
      }
);
      if (filePlan == null) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"");
        }
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
    }
 else {
      if (!filePlanService.isFilePlan(filePlan)) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"");
        }
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
    }
    boolean hideRecord=false;
    Boolean hideRecordValue=((Boolean)action.getParameterValue(PARAM_HIDE_RECORD));
    if (hideRecordValue != null) {
      hideRecord=hideRecordValue.booleanValue();
    }
    recordService.createRecord(filePlan,actionedUponNodeRef,!hideRecord);
  }
}",0.996594778660613
89214,"@Override public Void doWork() throws Exception {
  recordFolderService.closeRecordFolder(actionedUponNodeRef);
  return null;
}","@Override public Void doWork(){
  recordFolderService.closeRecordFolder(actionedUponNodeRef);
  return null;
}",0.9243697478991596
89215,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,final NodeRef actionedUponNodeRef){
  if (eligibleForAction(actionedUponNodeRef)) {
    AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
      @Override public Void doWork() throws Exception {
        recordFolderService.closeRecordFolder(actionedUponNodeRef);
        return null;
      }
    }
);
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,final NodeRef actionedUponNodeRef){
  if (eligibleForAction(actionedUponNodeRef)) {
    AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
      @Override public Void doWork(){
        recordFolderService.closeRecordFolder(actionedUponNodeRef);
        return null;
      }
    }
);
  }
}",0.9828571428571428
89216,"/** 
 * Create the specified child of the specified parent
 * @param action  Action to use for reporting if anything goes wrong
 * @param parent  Parent of the child to be created
 * @param childName  The name of the child to be created
 * @param targetisUnfiledRecords  true if the child is being created in the unfiled directory (determines type as unfiled container child)
 * @param lastAsFolder  true if this is the last element of the pathe being created and it should be created as a folder. ignored if targetIsUnfiledRecords is true
 * @return
 */
private NodeRef createChild(final Action action,final NodeRef parent,final String childName,final boolean targetisUnfiledRecords,final boolean lastAsFolder){
  return AuthenticationUtil.runAsSystem(new RunAsWork<NodeRef>(){
    public NodeRef doWork() throws Exception {
      NodeRef child=null;
      if (targetisUnfiledRecords) {
        child=fileFolderService.create(parent,childName,RecordsManagementModel.TYPE_UNFILED_RECORD_FOLDER).getNodeRef();
      }
 else       if (lastAsFolder) {
        child=recordFolderService.createRecordFolder(parent,childName);
      }
 else {
        if (RecordsManagementModel.TYPE_RECORD_FOLDER.equals(nodeService.getType(parent))) {
          throw new AlfrescoRuntimeException(""String_Node_Str"" + action.getActionDefinitionName() + ""String_Node_Str"");
        }
        child=filePlanService.createRecordCategory(parent,childName);
      }
      return child;
    }
  }
);
}","/** 
 * Create the specified child of the specified parent
 * @param action  Action to use for reporting if anything goes wrong
 * @param parent  Parent of the child to be created
 * @param childName  The name of the child to be created
 * @param targetisUnfiledRecords  true if the child is being created in the unfiled directory (determines type as unfiled container child)
 * @param lastAsFolder  true if this is the last element of the pathe being created and it should be created as a folder. ignored if targetIsUnfiledRecords is true
 * @return
 */
private NodeRef createChild(final Action action,final NodeRef parent,final String childName,final boolean targetisUnfiledRecords,final boolean lastAsFolder){
  return AuthenticationUtil.runAsSystem(new RunAsWork<NodeRef>(){
    @Override public NodeRef doWork(){
      NodeRef child=null;
      if (targetisUnfiledRecords) {
        child=fileFolderService.create(parent,childName,RecordsManagementModel.TYPE_UNFILED_RECORD_FOLDER).getNodeRef();
      }
 else       if (lastAsFolder) {
        child=recordFolderService.createRecordFolder(parent,childName);
      }
 else {
        if (RecordsManagementModel.TYPE_RECORD_FOLDER.equals(nodeService.getType(parent))) {
          throw new AlfrescoRuntimeException(""String_Node_Str"" + action.getActionDefinitionName() + ""String_Node_Str"");
        }
        child=filePlanService.createRecordCategory(parent,childName);
      }
      return child;
    }
  }
);
}",0.9904632152588556
89217,"public NodeRef doWork() throws Exception {
  NodeRef child=null;
  if (targetisUnfiledRecords) {
    child=fileFolderService.create(parent,childName,RecordsManagementModel.TYPE_UNFILED_RECORD_FOLDER).getNodeRef();
  }
 else   if (lastAsFolder) {
    child=recordFolderService.createRecordFolder(parent,childName);
  }
 else {
    if (RecordsManagementModel.TYPE_RECORD_FOLDER.equals(nodeService.getType(parent))) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + action.getActionDefinitionName() + ""String_Node_Str"");
    }
    child=filePlanService.createRecordCategory(parent,childName);
  }
  return child;
}","@Override public NodeRef doWork(){
  NodeRef child=null;
  if (targetisUnfiledRecords) {
    child=fileFolderService.create(parent,childName,RecordsManagementModel.TYPE_UNFILED_RECORD_FOLDER).getNodeRef();
  }
 else   if (lastAsFolder) {
    child=recordFolderService.createRecordFolder(parent,childName);
  }
 else {
    if (RecordsManagementModel.TYPE_RECORD_FOLDER.equals(nodeService.getType(parent))) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + action.getActionDefinitionName() + ""String_Node_Str"");
    }
    child=filePlanService.createRecordCategory(parent,childName);
  }
  return child;
}",0.9774193548387096
89218,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  String actionName=action.getActionDefinitionName();
  if (isOkToProceedWithAction(actionedUponNodeRef,actionName)) {
    QName actionedUponType=nodeService.getType(actionedUponNodeRef);
    boolean targetIsUnfiledRecords;
    if (ACTION_FILETO.equals(action.getActionDefinitionName())) {
      targetIsUnfiledRecords=false;
    }
 else {
      targetIsUnfiledRecords=(dictionaryService.isSubClass(actionedUponType,ContentModel.TYPE_CONTENT) && !recordService.isFiled(actionedUponNodeRef)) || TYPE_UNFILED_RECORD_FOLDER.equals(actionedUponType);
    }
    NodeRef recordFolder=(NodeRef)action.getParameterValue(PARAM_DESTINATION_RECORD_FOLDER);
    if (recordFolder == null) {
      recordFolder=createOrResolvePath(action,actionedUponNodeRef,targetIsUnfiledRecords);
    }
    validateActionPostPathResolution(actionedUponNodeRef,recordFolder,actionName,targetIsUnfiledRecords);
    final NodeRef finalRecordFolder=recordFolder;
    AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
      @Override public Void doWork() throws Exception {
        try {
          if (mode == CopyMoveLinkFileToActionMode.MOVE) {
            fileFolderService.move(actionedUponNodeRef,finalRecordFolder,null);
          }
 else           if (mode == CopyMoveLinkFileToActionMode.COPY) {
            fileFolderService.copy(actionedUponNodeRef,finalRecordFolder,null);
          }
 else           if (mode == CopyMoveLinkFileToActionMode.LINK) {
            recordService.link(actionedUponNodeRef,finalRecordFolder);
          }
        }
 catch (        FileNotFoundException fileNotFound) {
          throw new AlfrescoRuntimeException(""String_Node_Str"" + (mode == CopyMoveLinkFileToActionMode.MOVE ? ""String_Node_Str"" : ""String_Node_Str"") + ""String_Node_Str"",fileNotFound);
        }
        return null;
      }
    }
);
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  String actionName=action.getActionDefinitionName();
  if (isOkToProceedWithAction(actionedUponNodeRef,actionName)) {
    QName actionedUponType=nodeService.getType(actionedUponNodeRef);
    boolean targetIsUnfiledRecords;
    if (ACTION_FILETO.equals(action.getActionDefinitionName())) {
      targetIsUnfiledRecords=false;
    }
 else {
      targetIsUnfiledRecords=(dictionaryService.isSubClass(actionedUponType,ContentModel.TYPE_CONTENT) && !recordService.isFiled(actionedUponNodeRef)) || TYPE_UNFILED_RECORD_FOLDER.equals(actionedUponType);
    }
    NodeRef recordFolder=(NodeRef)action.getParameterValue(PARAM_DESTINATION_RECORD_FOLDER);
    if (recordFolder == null) {
      recordFolder=createOrResolvePath(action,actionedUponNodeRef,targetIsUnfiledRecords);
    }
    validateActionPostPathResolution(actionedUponNodeRef,recordFolder,actionName,targetIsUnfiledRecords);
    final NodeRef finalRecordFolder=recordFolder;
    AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
      @Override public Void doWork(){
        try {
          if (mode == CopyMoveLinkFileToActionMode.MOVE) {
            fileFolderService.move(actionedUponNodeRef,finalRecordFolder,null);
          }
 else           if (mode == CopyMoveLinkFileToActionMode.COPY) {
            fileFolderService.copy(actionedUponNodeRef,finalRecordFolder,null);
          }
 else           if (mode == CopyMoveLinkFileToActionMode.LINK) {
            recordService.link(actionedUponNodeRef,finalRecordFolder);
          }
        }
 catch (        FileNotFoundException fileNotFound) {
          throw new AlfrescoRuntimeException(""String_Node_Str"" + (mode == CopyMoveLinkFileToActionMode.MOVE ? ""String_Node_Str"" : ""String_Node_Str"") + ""String_Node_Str"",fileNotFound);
        }
        return null;
      }
    }
);
  }
}",0.9956605593056896
89219,"@Override public Void doWork() throws Exception {
  ownableService.setOwner(actionedUponNodeRef,OwnableService.NO_OWNER);
  return null;
}","@Override public Void doWork(){
  ownableService.setOwner(actionedUponNodeRef,OwnableService.NO_OWNER);
  return null;
}",0.9302325581395348
89220,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && recordService.isRecord(actionedUponNodeRef) && !freezeService.isFrozen(actionedUponNodeRef)) {
    if (!recordService.isDeclared(actionedUponNodeRef)) {
      List<String> missingProperties=new ArrayList<String>(5);
      if (mandatoryPropertiesSet(actionedUponNodeRef,missingProperties)) {
        recordService.disablePropertyEditableCheck();
        try {
          Map<QName,Serializable> declaredProps=new HashMap<QName,Serializable>(2);
          declaredProps.put(PROP_DECLARED_AT,new Date());
          declaredProps.put(PROP_DECLARED_BY,AuthenticationUtil.getRunAsUser());
          this.nodeService.addAspect(actionedUponNodeRef,ASPECT_DECLARED_RECORD,declaredProps);
          AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
            @Override public Void doWork() throws Exception {
              ownableService.setOwner(actionedUponNodeRef,OwnableService.NO_OWNER);
              return null;
            }
          }
);
        }
  finally {
          recordService.enablePropertyEditableCheck();
        }
      }
 else {
        logger.debug(buildMissingPropertiesErrorString(missingProperties));
        action.setParameterValue(ActionExecuterAbstractBase.PARAM_RESULT,""String_Node_Str"");
      }
    }
  }
 else {
    if (logger.isWarnEnabled()) {
      logger.warn(I18NUtil.getMessage(MSG_UNDECLARED_ONLY_RECORDS,actionedUponNodeRef.toString()));
    }
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && recordService.isRecord(actionedUponNodeRef) && !freezeService.isFrozen(actionedUponNodeRef)) {
    if (!recordService.isDeclared(actionedUponNodeRef)) {
      List<String> missingProperties=new ArrayList<String>(5);
      if (mandatoryPropertiesSet(actionedUponNodeRef,missingProperties)) {
        recordService.disablePropertyEditableCheck();
        try {
          Map<QName,Serializable> declaredProps=new HashMap<QName,Serializable>(2);
          declaredProps.put(PROP_DECLARED_AT,new Date());
          declaredProps.put(PROP_DECLARED_BY,AuthenticationUtil.getRunAsUser());
          this.nodeService.addAspect(actionedUponNodeRef,ASPECT_DECLARED_RECORD,declaredProps);
          AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
            @Override public Void doWork(){
              ownableService.setOwner(actionedUponNodeRef,OwnableService.NO_OWNER);
              return null;
            }
          }
);
        }
  finally {
          recordService.enablePropertyEditableCheck();
        }
      }
 else {
        logger.debug(buildMissingPropertiesErrorString(missingProperties));
        action.setParameterValue(ActionExecuterAbstractBase.PARAM_RESULT,""String_Node_Str"");
      }
    }
  }
 else {
    if (logger.isWarnEnabled()) {
      logger.warn(I18NUtil.getMessage(MSG_UNDECLARED_ONLY_RECORDS,actionedUponNodeRef.toString()));
    }
  }
}",0.9946871310507674
89221,"@Override public NodeRef doWork() throws Exception {
  return reportService.fileReport(destination,report);
}","@Override public NodeRef doWork(){
  return reportService.fileReport(destination,report);
}",0.91
89222,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  String mimetype=(String)action.getParameterValue(MIMETYPE);
  if (mimetype == null || mimetype.isEmpty()) {
    mimetype=MimetypeMap.MIMETYPE_HTML;
  }
  QName reportType=getReportType(action);
  final NodeRef destination=getDestination(action);
  final Report report=reportService.generateReport(reportType,actionedUponNodeRef,mimetype);
  NodeRef filedReport=AuthenticationUtil.runAsSystem(new RunAsWork<NodeRef>(){
    @Override public NodeRef doWork() throws Exception {
      return reportService.fileReport(destination,report);
    }
  }
);
  String filedReportName=(String)nodeService.getProperty(filedReport,ContentModel.PROP_NAME);
  action.setParameterValue(ActionExecuterAbstractBase.PARAM_RESULT,filedReportName);
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  String mimetype=(String)action.getParameterValue(MIMETYPE);
  if (mimetype == null || mimetype.isEmpty()) {
    mimetype=MimetypeMap.MIMETYPE_HTML;
  }
  QName reportType=getReportType(action);
  final NodeRef destination=getDestination(action);
  final Report report=reportService.generateReport(reportType,actionedUponNodeRef,mimetype);
  NodeRef filedReport=AuthenticationUtil.runAsSystem(new RunAsWork<NodeRef>(){
    @Override public NodeRef doWork(){
      return reportService.fileReport(destination,report);
    }
  }
);
  String filedReportName=(String)nodeService.getProperty(filedReport,ContentModel.PROP_NAME);
  action.setParameterValue(ActionExecuterAbstractBase.PARAM_RESULT,filedReportName);
}",0.9907692307692308
89223,"@Override public Void doWork() throws Exception {
  recordsManagementAdminService.addCustomReference(parentRef,childRef,relationshipQName);
  nodeService.createAssociation(parentRef,childRef,ImapModel.ASSOC_IMAP_ATTACHMENT);
  return null;
}","@Override public Void doWork(){
  recordsManagementAdminService.addCustomReference(parentRef,childRef,relationshipQName);
  nodeService.createAssociation(parentRef,childRef,ImapModel.ASSOC_IMAP_ATTACHMENT);
  return null;
}",0.961206896551724
89224,"/** 
 * Create a link from the message to the attachment
 */
private void createRMReference(final NodeRef parentRef,final NodeRef childRef){
  AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
    @Override public Void doWork() throws Exception {
      recordsManagementAdminService.addCustomReference(parentRef,childRef,relationshipQName);
      nodeService.createAssociation(parentRef,childRef,ImapModel.ASSOC_IMAP_ATTACHMENT);
      return null;
    }
  }
);
}","/** 
 * Create a link from the message to the attachment
 */
private void createRMReference(final NodeRef parentRef,final NodeRef childRef){
  AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
    @Override public Void doWork(){
      recordsManagementAdminService.addCustomReference(parentRef,childRef,relationshipQName);
      nodeService.createAssociation(parentRef,childRef,ImapModel.ASSOC_IMAP_ATTACHMENT);
      return null;
    }
  }
);
}",0.980349344978166
89225,"/** 
 * @see org.alfresco.repo.node.NodeServicePolicies.OnAddAspectPolicy#onAddAspect(org.alfresco.service.cmr.repository.NodeRef,org.alfresco.service.namespace.QName)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,isService=true,notificationFrequency=NotificationFrequency.FIRST_EVENT) public void onAddAspect(final NodeRef nodeRef,final QName aspectTypeQName){
  AuthenticationUtil.runAs(new RunAsWork<Void>(){
    @Override public Void doWork() throws Exception {
      if (nodeService.exists(nodeRef) && dictionaryService.getAllModels().contains(RM_CUSTOM_MODEL) && isCustomisable(aspectTypeQName)) {
        QName customPropertyAspect=getCustomAspect(aspectTypeQName);
        nodeService.addAspect(nodeRef,customPropertyAspect,null);
      }
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}","/** 
 * @see org.alfresco.repo.node.NodeServicePolicies.OnAddAspectPolicy#onAddAspect(org.alfresco.service.cmr.repository.NodeRef,org.alfresco.service.namespace.QName)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,isService=true,notificationFrequency=NotificationFrequency.FIRST_EVENT) public void onAddAspect(final NodeRef nodeRef,final QName aspectTypeQName){
  AuthenticationUtil.runAs(new RunAsWork<Void>(){
    @Override public Void doWork(){
      if (nodeService.exists(nodeRef) && dictionaryService.getAllModels().contains(RM_CUSTOM_MODEL) && isCustomisable(aspectTypeQName)) {
        QName customPropertyAspect=getCustomAspect(aspectTypeQName);
        nodeService.addAspect(nodeRef,customPropertyAspect,null);
      }
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}",0.9889434889434888
89226,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.admin.RecordsManagementAdminService#removeCustomReference(org.alfresco.service.cmr.repository.NodeRef,org.alfresco.service.cmr.repository.NodeRef,org.alfresco.service.namespace.QName)
 */
public void removeCustomReference(final NodeRef fromNode,final NodeRef toNode,final QName assocId){
  Map<QName,AssociationDefinition> availableAssocs=this.getCustomReferenceDefinitions();
  AssociationDefinition assocDef=availableAssocs.get(assocId);
  if (assocDef == null) {
    throw new IllegalArgumentException(I18NUtil.getMessage(MSG_REF_EXIST,assocId));
  }
  invokeBeforeRemoveReference(fromNode,toNode,assocId);
  if (assocDef.isChild()) {
    AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
      @Override public Void doWork() throws Exception {
        List<ChildAssociationRef> children=nodeService.getChildAssocs(fromNode);
        for (        ChildAssociationRef chRef : children) {
          if (assocId.equals(chRef.getTypeQName()) && chRef.getChildRef().equals(toNode)) {
            nodeService.removeChildAssociation(chRef);
          }
        }
        return null;
      }
    }
);
  }
 else {
    nodeService.removeAssociation(fromNode,toNode,assocId);
  }
  invokeOnRemoveReference(fromNode,toNode,assocId);
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.admin.RecordsManagementAdminService#removeCustomReference(org.alfresco.service.cmr.repository.NodeRef,org.alfresco.service.cmr.repository.NodeRef,org.alfresco.service.namespace.QName)
 */
public void removeCustomReference(final NodeRef fromNode,final NodeRef toNode,final QName assocId){
  Map<QName,AssociationDefinition> availableAssocs=this.getCustomReferenceDefinitions();
  AssociationDefinition assocDef=availableAssocs.get(assocId);
  if (assocDef == null) {
    throw new IllegalArgumentException(I18NUtil.getMessage(MSG_REF_EXIST,assocId));
  }
  invokeBeforeRemoveReference(fromNode,toNode,assocId);
  if (assocDef.isChild()) {
    AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
      @Override public Void doWork(){
        List<ChildAssociationRef> children=nodeService.getChildAssocs(fromNode);
        for (        ChildAssociationRef chRef : children) {
          if (assocId.equals(chRef.getTypeQName()) && chRef.getChildRef().equals(toNode)) {
            nodeService.removeChildAssociation(chRef);
          }
        }
        return null;
      }
    }
);
  }
 else {
    nodeService.removeAssociation(fromNode,toNode,assocId);
  }
  invokeOnRemoveReference(fromNode,toNode,assocId);
}",0.9929522317932654
89227,"@Override public Void doWork() throws Exception {
  List<ChildAssociationRef> children=nodeService.getChildAssocs(fromNode);
  for (  ChildAssociationRef chRef : children) {
    if (assocId.equals(chRef.getTypeQName()) && chRef.getChildRef().equals(toNode)) {
      nodeService.removeChildAssociation(chRef);
    }
  }
  return null;
}","@Override public Void doWork(){
  List<ChildAssociationRef> children=nodeService.getChildAssocs(fromNode);
  for (  ChildAssociationRef chRef : children) {
    if (assocId.equals(chRef.getTypeQName()) && chRef.getChildRef().equals(toNode)) {
      nodeService.removeChildAssociation(chRef);
    }
  }
  return null;
}",0.97239263803681
89228,"/** 
 * @see org.alfresco.repo.node.NodeServicePolicies.OnRemoveAspectPolicy#onRemoveAspect(org.alfresco.service.cmr.repository.NodeRef,org.alfresco.service.namespace.QName)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,isService=true,notificationFrequency=NotificationFrequency.FIRST_EVENT) public void onRemoveAspect(final NodeRef nodeRef,final QName aspectTypeQName){
  AuthenticationUtil.runAs(new RunAsWork<Void>(){
    @Override public Void doWork() throws Exception {
      if (nodeService.exists(nodeRef) && isCustomisable(aspectTypeQName)) {
        QName customPropertyAspect=getCustomAspect(aspectTypeQName);
        nodeService.removeAspect(nodeRef,customPropertyAspect);
      }
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}","/** 
 * @see org.alfresco.repo.node.NodeServicePolicies.OnRemoveAspectPolicy#onRemoveAspect(org.alfresco.service.cmr.repository.NodeRef,org.alfresco.service.namespace.QName)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,isService=true,notificationFrequency=NotificationFrequency.FIRST_EVENT) public void onRemoveAspect(final NodeRef nodeRef,final QName aspectTypeQName){
  AuthenticationUtil.runAs(new RunAsWork<Void>(){
    @Override public Void doWork(){
      if (nodeService.exists(nodeRef) && isCustomisable(aspectTypeQName)) {
        QName customPropertyAspect=getCustomAspect(aspectTypeQName);
        nodeService.removeAspect(nodeRef,customPropertyAspect);
      }
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}",0.9881422924901184
89229,"/** 
 * Make sure any custom property aspects are applied to newly created nodes.
 * @see org.alfresco.repo.node.NodeServicePolicies.OnCreateNodePolicy#onCreateNode(org.alfresco.service.cmr.repository.ChildAssociationRef)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,isService=true,notificationFrequency=NotificationFrequency.FIRST_EVENT) public void onCreateNode(final ChildAssociationRef childAssocRef){
  AuthenticationUtil.runAs(new RunAsWork<Void>(){
    @Override public Void doWork() throws Exception {
      if (dictionaryService.getAllModels().contains(RecordsManagementCustomModel.RM_CUSTOM_MODEL)) {
        NodeRef nodeRef=childAssocRef.getChildRef();
        QName type=nodeService.getType(nodeRef);
        while (type != null && !ContentModel.TYPE_CMOBJECT.equals(type)) {
          if (isCustomisable(type)) {
            QName customPropertyAspect=getCustomAspect(type);
            nodeService.addAspect(nodeRef,customPropertyAspect,null);
          }
          TypeDefinition def=dictionaryService.getType(type);
          if (def != null) {
            type=def.getParentName();
          }
 else {
            type=null;
          }
        }
      }
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}","/** 
 * Make sure any custom property aspects are applied to newly created nodes.
 * @see org.alfresco.repo.node.NodeServicePolicies.OnCreateNodePolicy#onCreateNode(org.alfresco.service.cmr.repository.ChildAssociationRef)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,isService=true,notificationFrequency=NotificationFrequency.FIRST_EVENT) public void onCreateNode(final ChildAssociationRef childAssocRef){
  AuthenticationUtil.runAs(new RunAsWork<Void>(){
    @Override public Void doWork(){
      if (dictionaryService.getAllModels().contains(RecordsManagementCustomModel.RM_CUSTOM_MODEL)) {
        NodeRef nodeRef=childAssocRef.getChildRef();
        QName type=nodeService.getType(nodeRef);
        while (type != null && !ContentModel.TYPE_CMOBJECT.equals(type)) {
          if (isCustomisable(type)) {
            QName customPropertyAspect=getCustomAspect(type);
            nodeService.addAspect(nodeRef,customPropertyAspect,null);
          }
          TypeDefinition def=dictionaryService.getType(type);
          if (def != null) {
            type=def.getParentName();
          }
 else {
            type=null;
          }
        }
      }
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}",0.992741935483871
89230,"/** 
 * Register the action
 */
public void registerAction(){
  AuthenticationUtil.runAs(new AuthenticationUtil.RunAsWork<Void>(){
    public Void doWork() throws Exception {
      RecordsManagementAction action=(RecordsManagementAction)getObject();
      recordsManagementActionService.register(action);
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}","/** 
 * Register the action
 */
public void registerAction(){
  AuthenticationUtil.runAs(new AuthenticationUtil.RunAsWork<Void>(){
    public Void doWork(){
      RecordsManagementAction action=(RecordsManagementAction)getObject();
      recordsManagementActionService.register(action);
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}",0.9755434782608696
89231,"public Void doWork() throws Exception {
  RecordsManagementAction action=(RecordsManagementAction)getObject();
  recordsManagementActionService.register(action);
  return null;
}","public Void doWork(){
  RecordsManagementAction action=(RecordsManagementAction)getObject();
  recordsManagementActionService.register(action);
  return null;
}",0.9467455621301776
89232,"public void afterPropertiesSet() throws Exception {
}","public void afterPropertiesSet(){
}",0.7954545454545454
89233,"/** 
 * @see org.springframework.beans.factory.InitializingBean#afterPropertiesSet()
 */
public void afterPropertiesSet() throws Exception {
}","/** 
 * @see org.springframework.beans.factory.InitializingBean#afterPropertiesSet()
 */
public void afterPropertiesSet(){
}",0.9323308270676692
89234,"/** 
 * Temp method to patch AMP'ed data
 */
private void patchLoadedData(){
  AuthenticationUtil.RunAsWork<Object> runAsWork=new AuthenticationUtil.RunAsWork<Object>(){
    public Object doWork() throws Exception {
      Set<NodeRef> rmRoots=filePlanService.getFilePlans();
      logger.info(""String_Node_Str"" + rmRoots.size() + ""String_Node_Str"");
      for (      NodeRef rmRoot : rmRoots) {
        if (permissionService.getInheritParentPermissions(rmRoot)) {
          logger.info(""String_Node_Str"" + rmRoot);
          permissionService.setInheritParentPermissions(rmRoot,false);
        }
        String allRoleShortName=RMAuthority.ALL_ROLES_PREFIX + rmRoot.getId();
        String allRoleGroupName=authorityService.getName(AuthorityType.GROUP,allRoleShortName);
        if (!authorityService.authorityExists(allRoleGroupName)) {
          logger.info(""String_Node_Str"" + rmRoot.toString());
          String allRoles=authorityService.createAuthority(AuthorityType.GROUP,allRoleShortName,RMAuthority.ALL_ROLES_DISPLAY_NAME,new HashSet<String>(Arrays.asList(RMAuthority.ZONE_APP_RM)));
          Set<Role> roles=filePlanRoleService.getRoles(rmRoot);
          for (          Role role : roles) {
            logger.info(""String_Node_Str"" + role.getRoleGroupName() + ""String_Node_Str"");
            authorityService.addAuthority(allRoles,role.getRoleGroupName());
          }
          permissionService.setPermission(rmRoot,allRoles,RMPermissionModel.READ_RECORDS,true);
        }
      }
      ResultSet rs=searchService.query(SPACES_STORE,SearchService.LANGUAGE_LUCENE,""String_Node_Str"");
      try {
        logger.info(""String_Node_Str"" + rs.length() + ""String_Node_Str"");
        for (        NodeRef container : rs.getNodeRefs()) {
          String containerName=(String)nodeService.getProperty(container,ContentModel.PROP_NAME);
          if (permissionService.getInheritParentPermissions(container)) {
            logger.info(""String_Node_Str"" + containerName);
            permissionService.setInheritParentPermissions(container,false);
          }
        }
      }
  finally {
        rs.close();
      }
      rs=searchService.query(SPACES_STORE,SearchService.LANGUAGE_LUCENE,""String_Node_Str"");
      try {
        logger.info(""String_Node_Str"" + rs.length() + ""String_Node_Str"");
        for (        NodeRef recordFolder : rs.getNodeRefs()) {
          String folderName=(String)nodeService.getProperty(recordFolder,ContentModel.PROP_NAME);
          if (permissionService.getInheritParentPermissions(recordFolder)) {
            logger.info(""String_Node_Str"" + folderName);
            permissionService.setInheritParentPermissions(recordFolder,false);
          }
          if (!nodeService.hasAspect(recordFolder,ASPECT_DISPOSITION_LIFECYCLE)) {
            DispositionSchedule ds=dispositionService.getDispositionSchedule(recordFolder);
            if (ds != null) {
              logger.info(""String_Node_Str"" + folderName);
              recordFolderService.setupRecordFolder(recordFolder);
            }
          }
          logger.info(""String_Node_Str"" + folderName);
          recordsManagementSearchBehaviour.fixupSearchAspect(recordFolder);
        }
      }
  finally {
        rs.close();
      }
      return null;
    }
  }
;
  AuthenticationUtil.runAs(runAsWork,AuthenticationUtil.getAdminUserName());
}","/** 
 * Temp method to patch AMP'ed data
 */
private void patchLoadedData(){
  AuthenticationUtil.RunAsWork<Object> runAsWork=new AuthenticationUtil.RunAsWork<Object>(){
    public Object doWork(){
      Set<NodeRef> rmRoots=filePlanService.getFilePlans();
      logger.info(""String_Node_Str"" + rmRoots.size() + ""String_Node_Str"");
      for (      NodeRef rmRoot : rmRoots) {
        if (permissionService.getInheritParentPermissions(rmRoot)) {
          logger.info(""String_Node_Str"" + rmRoot);
          permissionService.setInheritParentPermissions(rmRoot,false);
        }
        String allRoleShortName=RMAuthority.ALL_ROLES_PREFIX + rmRoot.getId();
        String allRoleGroupName=authorityService.getName(AuthorityType.GROUP,allRoleShortName);
        if (!authorityService.authorityExists(allRoleGroupName)) {
          logger.info(""String_Node_Str"" + rmRoot.toString());
          String allRoles=authorityService.createAuthority(AuthorityType.GROUP,allRoleShortName,RMAuthority.ALL_ROLES_DISPLAY_NAME,new HashSet<String>(Arrays.asList(RMAuthority.ZONE_APP_RM)));
          Set<Role> roles=filePlanRoleService.getRoles(rmRoot);
          for (          Role role : roles) {
            logger.info(""String_Node_Str"" + role.getRoleGroupName() + ""String_Node_Str"");
            authorityService.addAuthority(allRoles,role.getRoleGroupName());
          }
          permissionService.setPermission(rmRoot,allRoles,RMPermissionModel.READ_RECORDS,true);
        }
      }
      ResultSet rs=searchService.query(SPACES_STORE,SearchService.LANGUAGE_LUCENE,""String_Node_Str"");
      try {
        logger.info(""String_Node_Str"" + rs.length() + ""String_Node_Str"");
        for (        NodeRef container : rs.getNodeRefs()) {
          String containerName=(String)nodeService.getProperty(container,ContentModel.PROP_NAME);
          if (permissionService.getInheritParentPermissions(container)) {
            logger.info(""String_Node_Str"" + containerName);
            permissionService.setInheritParentPermissions(container,false);
          }
        }
      }
  finally {
        rs.close();
      }
      rs=searchService.query(SPACES_STORE,SearchService.LANGUAGE_LUCENE,""String_Node_Str"");
      try {
        logger.info(""String_Node_Str"" + rs.length() + ""String_Node_Str"");
        for (        NodeRef recordFolder : rs.getNodeRefs()) {
          String folderName=(String)nodeService.getProperty(recordFolder,ContentModel.PROP_NAME);
          if (permissionService.getInheritParentPermissions(recordFolder)) {
            logger.info(""String_Node_Str"" + folderName);
            permissionService.setInheritParentPermissions(recordFolder,false);
          }
          if (!nodeService.hasAspect(recordFolder,ASPECT_DISPOSITION_LIFECYCLE)) {
            DispositionSchedule ds=dispositionService.getDispositionSchedule(recordFolder);
            if (ds != null) {
              logger.info(""String_Node_Str"" + folderName);
              recordFolderService.setupRecordFolder(recordFolder);
            }
          }
          logger.info(""String_Node_Str"" + folderName);
          recordsManagementSearchBehaviour.fixupSearchAspect(recordFolder);
        }
      }
  finally {
        rs.close();
      }
      return null;
    }
  }
;
  AuthenticationUtil.runAs(runAsWork,AuthenticationUtil.getAdminUserName());
}",0.9973005398920216
89235,"public Object doWork() throws Exception {
  Set<NodeRef> rmRoots=filePlanService.getFilePlans();
  logger.info(""String_Node_Str"" + rmRoots.size() + ""String_Node_Str"");
  for (  NodeRef rmRoot : rmRoots) {
    if (permissionService.getInheritParentPermissions(rmRoot)) {
      logger.info(""String_Node_Str"" + rmRoot);
      permissionService.setInheritParentPermissions(rmRoot,false);
    }
    String allRoleShortName=RMAuthority.ALL_ROLES_PREFIX + rmRoot.getId();
    String allRoleGroupName=authorityService.getName(AuthorityType.GROUP,allRoleShortName);
    if (!authorityService.authorityExists(allRoleGroupName)) {
      logger.info(""String_Node_Str"" + rmRoot.toString());
      String allRoles=authorityService.createAuthority(AuthorityType.GROUP,allRoleShortName,RMAuthority.ALL_ROLES_DISPLAY_NAME,new HashSet<String>(Arrays.asList(RMAuthority.ZONE_APP_RM)));
      Set<Role> roles=filePlanRoleService.getRoles(rmRoot);
      for (      Role role : roles) {
        logger.info(""String_Node_Str"" + role.getRoleGroupName() + ""String_Node_Str"");
        authorityService.addAuthority(allRoles,role.getRoleGroupName());
      }
      permissionService.setPermission(rmRoot,allRoles,RMPermissionModel.READ_RECORDS,true);
    }
  }
  ResultSet rs=searchService.query(SPACES_STORE,SearchService.LANGUAGE_LUCENE,""String_Node_Str"");
  try {
    logger.info(""String_Node_Str"" + rs.length() + ""String_Node_Str"");
    for (    NodeRef container : rs.getNodeRefs()) {
      String containerName=(String)nodeService.getProperty(container,ContentModel.PROP_NAME);
      if (permissionService.getInheritParentPermissions(container)) {
        logger.info(""String_Node_Str"" + containerName);
        permissionService.setInheritParentPermissions(container,false);
      }
    }
  }
  finally {
    rs.close();
  }
  rs=searchService.query(SPACES_STORE,SearchService.LANGUAGE_LUCENE,""String_Node_Str"");
  try {
    logger.info(""String_Node_Str"" + rs.length() + ""String_Node_Str"");
    for (    NodeRef recordFolder : rs.getNodeRefs()) {
      String folderName=(String)nodeService.getProperty(recordFolder,ContentModel.PROP_NAME);
      if (permissionService.getInheritParentPermissions(recordFolder)) {
        logger.info(""String_Node_Str"" + folderName);
        permissionService.setInheritParentPermissions(recordFolder,false);
      }
      if (!nodeService.hasAspect(recordFolder,ASPECT_DISPOSITION_LIFECYCLE)) {
        DispositionSchedule ds=dispositionService.getDispositionSchedule(recordFolder);
        if (ds != null) {
          logger.info(""String_Node_Str"" + folderName);
          recordFolderService.setupRecordFolder(recordFolder);
        }
      }
      logger.info(""String_Node_Str"" + folderName);
      recordsManagementSearchBehaviour.fixupSearchAspect(recordFolder);
    }
  }
  finally {
    rs.close();
  }
  return null;
}","public Object doWork(){
  Set<NodeRef> rmRoots=filePlanService.getFilePlans();
  logger.info(""String_Node_Str"" + rmRoots.size() + ""String_Node_Str"");
  for (  NodeRef rmRoot : rmRoots) {
    if (permissionService.getInheritParentPermissions(rmRoot)) {
      logger.info(""String_Node_Str"" + rmRoot);
      permissionService.setInheritParentPermissions(rmRoot,false);
    }
    String allRoleShortName=RMAuthority.ALL_ROLES_PREFIX + rmRoot.getId();
    String allRoleGroupName=authorityService.getName(AuthorityType.GROUP,allRoleShortName);
    if (!authorityService.authorityExists(allRoleGroupName)) {
      logger.info(""String_Node_Str"" + rmRoot.toString());
      String allRoles=authorityService.createAuthority(AuthorityType.GROUP,allRoleShortName,RMAuthority.ALL_ROLES_DISPLAY_NAME,new HashSet<String>(Arrays.asList(RMAuthority.ZONE_APP_RM)));
      Set<Role> roles=filePlanRoleService.getRoles(rmRoot);
      for (      Role role : roles) {
        logger.info(""String_Node_Str"" + role.getRoleGroupName() + ""String_Node_Str"");
        authorityService.addAuthority(allRoles,role.getRoleGroupName());
      }
      permissionService.setPermission(rmRoot,allRoles,RMPermissionModel.READ_RECORDS,true);
    }
  }
  ResultSet rs=searchService.query(SPACES_STORE,SearchService.LANGUAGE_LUCENE,""String_Node_Str"");
  try {
    logger.info(""String_Node_Str"" + rs.length() + ""String_Node_Str"");
    for (    NodeRef container : rs.getNodeRefs()) {
      String containerName=(String)nodeService.getProperty(container,ContentModel.PROP_NAME);
      if (permissionService.getInheritParentPermissions(container)) {
        logger.info(""String_Node_Str"" + containerName);
        permissionService.setInheritParentPermissions(container,false);
      }
    }
  }
  finally {
    rs.close();
  }
  rs=searchService.query(SPACES_STORE,SearchService.LANGUAGE_LUCENE,""String_Node_Str"");
  try {
    logger.info(""String_Node_Str"" + rs.length() + ""String_Node_Str"");
    for (    NodeRef recordFolder : rs.getNodeRefs()) {
      String folderName=(String)nodeService.getProperty(recordFolder,ContentModel.PROP_NAME);
      if (permissionService.getInheritParentPermissions(recordFolder)) {
        logger.info(""String_Node_Str"" + folderName);
        permissionService.setInheritParentPermissions(recordFolder,false);
      }
      if (!nodeService.hasAspect(recordFolder,ASPECT_DISPOSITION_LIFECYCLE)) {
        DispositionSchedule ds=dispositionService.getDispositionSchedule(recordFolder);
        if (ds != null) {
          logger.info(""String_Node_Str"" + folderName);
          recordFolderService.setupRecordFolder(recordFolder);
        }
      }
      logger.info(""String_Node_Str"" + folderName);
      recordsManagementSearchBehaviour.fixupSearchAspect(recordFolder);
    }
  }
  finally {
    rs.close();
  }
  return null;
}",0.9968209113387496
89236,"public Object doWork(){
  StringBuilder queryBuffer=new StringBuilder();
  queryBuffer.append(""String_Node_Str"");
  queryBuffer.append(""String_Node_Str"");
  queryBuffer.append(""String_Node_Str"");
  queryBuffer.append(""String_Node_Str"");
  queryBuffer.append(""String_Node_Str"");
  queryBuffer.append(""String_Node_Str"");
  String query=queryBuffer.toString();
  ResultSet results=searchService.query(StoreRef.STORE_REF_WORKSPACE_SPACESSTORE,SearchService.LANGUAGE_LUCENE,query);
  final List<NodeRef> resultNodes=results.getNodeRefs();
  results.close();
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + resultNodes.size() + ""String_Node_Str"");
  }
  if (resultNodes.size() != 0) {
    RetryingTransactionCallback<Boolean> txCallbackSendEmail=new RetryingTransactionCallback<Boolean>(){
      public Boolean execute(){
        recordsManagementNotificationHelper.recordsDueForReviewEmailNotification(resultNodes);
        return null;
      }
    }
;
    RetryingTransactionCallback<Boolean> txUpdateNodesCallback=new RetryingTransactionCallback<Boolean>(){
      public Boolean execute(){
        for (        NodeRef node : resultNodes) {
          nodeService.setProperty(node,RecordsManagementModel.PROP_NOTIFICATION_ISSUED,""String_Node_Str"");
        }
        return Boolean.TRUE;
      }
    }
;
    retryingTransactionHelper.setMaxRetries(0);
    retryingTransactionHelper.doInTransaction(txCallbackSendEmail);
    retryingTransactionHelper.setMaxRetries(10);
    retryingTransactionHelper.doInTransaction(txUpdateNodesCallback);
  }
  return null;
}","public Object doWork(){
  StringBuilder queryBuffer=new StringBuilder();
  queryBuffer.append(""String_Node_Str"");
  queryBuffer.append(""String_Node_Str"");
  queryBuffer.append(""String_Node_Str"");
  queryBuffer.append(""String_Node_Str"");
  queryBuffer.append(""String_Node_Str"");
  queryBuffer.append(""String_Node_Str"");
  String query=queryBuffer.toString();
  ResultSet results=searchService.query(StoreRef.STORE_REF_WORKSPACE_SPACESSTORE,SearchService.LANGUAGE_LUCENE,query);
  final List<NodeRef> resultNodes=results.getNodeRefs();
  results.close();
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + resultNodes.size() + ""String_Node_Str"");
  }
  if (resultNodes.size() != 0) {
    RetryingTransactionCallback<Void> txCallbackSendEmail=new RetryingTransactionCallback<Void>(){
      public Void execute(){
        recordsManagementNotificationHelper.recordsDueForReviewEmailNotification(resultNodes);
        return null;
      }
    }
;
    RetryingTransactionCallback<Boolean> txUpdateNodesCallback=new RetryingTransactionCallback<Boolean>(){
      public Boolean execute(){
        for (        NodeRef node : resultNodes) {
          nodeService.setProperty(node,RecordsManagementModel.PROP_NOTIFICATION_ISSUED,""String_Node_Str"");
        }
        return Boolean.TRUE;
      }
    }
;
    retryingTransactionHelper.setMaxRetries(0);
    retryingTransactionHelper.doInTransaction(txCallbackSendEmail);
    retryingTransactionHelper.setMaxRetries(10);
    retryingTransactionHelper.doInTransaction(txUpdateNodesCallback);
  }
  return null;
}",0.9895071542130366
89237,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.job.RecordsManagementJobExecuter#execute()
 */
public void executeImpl(){
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + this.getClass().getSimpleName() + ""String_Node_Str"");
  }
  AuthenticationUtil.runAs(new RunAsWork<Object>(){
    public Object doWork(){
      StringBuilder queryBuffer=new StringBuilder();
      queryBuffer.append(""String_Node_Str"");
      queryBuffer.append(""String_Node_Str"");
      queryBuffer.append(""String_Node_Str"");
      queryBuffer.append(""String_Node_Str"");
      queryBuffer.append(""String_Node_Str"");
      queryBuffer.append(""String_Node_Str"");
      String query=queryBuffer.toString();
      ResultSet results=searchService.query(StoreRef.STORE_REF_WORKSPACE_SPACESSTORE,SearchService.LANGUAGE_LUCENE,query);
      final List<NodeRef> resultNodes=results.getNodeRefs();
      results.close();
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + resultNodes.size() + ""String_Node_Str"");
      }
      if (resultNodes.size() != 0) {
        RetryingTransactionCallback<Boolean> txCallbackSendEmail=new RetryingTransactionCallback<Boolean>(){
          public Boolean execute(){
            recordsManagementNotificationHelper.recordsDueForReviewEmailNotification(resultNodes);
            return null;
          }
        }
;
        RetryingTransactionCallback<Boolean> txUpdateNodesCallback=new RetryingTransactionCallback<Boolean>(){
          public Boolean execute(){
            for (            NodeRef node : resultNodes) {
              nodeService.setProperty(node,RecordsManagementModel.PROP_NOTIFICATION_ISSUED,""String_Node_Str"");
            }
            return Boolean.TRUE;
          }
        }
;
        retryingTransactionHelper.setMaxRetries(0);
        retryingTransactionHelper.doInTransaction(txCallbackSendEmail);
        retryingTransactionHelper.setMaxRetries(10);
        retryingTransactionHelper.doInTransaction(txUpdateNodesCallback);
      }
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + this.getClass().getSimpleName() + ""String_Node_Str"");
  }
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.job.RecordsManagementJobExecuter#execute()
 */
public void executeImpl(){
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + this.getClass().getSimpleName() + ""String_Node_Str"");
  }
  AuthenticationUtil.runAs(new RunAsWork<Object>(){
    public Object doWork(){
      StringBuilder queryBuffer=new StringBuilder();
      queryBuffer.append(""String_Node_Str"");
      queryBuffer.append(""String_Node_Str"");
      queryBuffer.append(""String_Node_Str"");
      queryBuffer.append(""String_Node_Str"");
      queryBuffer.append(""String_Node_Str"");
      queryBuffer.append(""String_Node_Str"");
      String query=queryBuffer.toString();
      ResultSet results=searchService.query(StoreRef.STORE_REF_WORKSPACE_SPACESSTORE,SearchService.LANGUAGE_LUCENE,query);
      final List<NodeRef> resultNodes=results.getNodeRefs();
      results.close();
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + resultNodes.size() + ""String_Node_Str"");
      }
      if (resultNodes.size() != 0) {
        RetryingTransactionCallback<Void> txCallbackSendEmail=new RetryingTransactionCallback<Void>(){
          public Void execute(){
            recordsManagementNotificationHelper.recordsDueForReviewEmailNotification(resultNodes);
            return null;
          }
        }
;
        RetryingTransactionCallback<Boolean> txUpdateNodesCallback=new RetryingTransactionCallback<Boolean>(){
          public Boolean execute(){
            for (            NodeRef node : resultNodes) {
              nodeService.setProperty(node,RecordsManagementModel.PROP_NOTIFICATION_ISSUED,""String_Node_Str"");
            }
            return Boolean.TRUE;
          }
        }
;
        retryingTransactionHelper.setMaxRetries(0);
        retryingTransactionHelper.doInTransaction(txCallbackSendEmail);
        retryingTransactionHelper.setMaxRetries(10);
        retryingTransactionHelper.doInTransaction(txUpdateNodesCallback);
      }
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + this.getClass().getSimpleName() + ""String_Node_Str"");
  }
}",0.9925356254241122
89238,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.security.RecordsManagementSecurityService#deleteRole(java.lang.String)
 */
public void deleteRole(final NodeRef rmRootNode,final String role){
  if (ROLE_ADMIN.equals(role)) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
  if (FilePlanRoleService.SYSTEM_ROLES.contains(role)) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + role + ""String_Node_Str"");
  }
  AuthenticationUtil.runAs(new AuthenticationUtil.RunAsWork<Object>(){
    public Boolean doWork(){
      String roleAuthority=authorityService.getName(AuthorityType.GROUP,getFullRoleName(role,rmRootNode));
      authorityService.deleteAuthority(roleAuthority);
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.security.RecordsManagementSecurityService#deleteRole(java.lang.String)
 */
public void deleteRole(final NodeRef rmRootNode,final String role){
  if (ROLE_ADMIN.equals(role)) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
  if (FilePlanRoleService.SYSTEM_ROLES.contains(role)) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + role + ""String_Node_Str"");
  }
  AuthenticationUtil.runAs(new AuthenticationUtil.RunAsWork<Object>(){
    public Void doWork(){
      String roleAuthority=authorityService.getName(AuthorityType.GROUP,getFullRoleName(role,rmRootNode));
      authorityService.deleteAuthority(roleAuthority);
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}",0.9928986442866364
89239,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.security.RecordsManagementSecurityService#deletePermission(org.alfresco.service.cmr.repository.NodeRef,java.lang.String,java.lang.String)
 */
public void deletePermission(final NodeRef nodeRef,final String authority,final String permission){
  runAsSystem(new AuthenticationUtil.RunAsWork<Object>(){
    public Boolean doWork(){
      if (!permissionService.getInheritParentPermissions(nodeRef)) {
        permissionService.deletePermission(nodeRef,authority,permission);
        if (isFilePlanContainer(nodeRef) || isRecordFolder(nodeRef)) {
          List<ChildAssociationRef> assocs=nodeService.getChildAssocs(nodeRef,ContentModel.ASSOC_CONTAINS,RegexQNamePattern.MATCH_ALL);
          for (          ChildAssociationRef assoc : assocs) {
            NodeRef child=assoc.getChildRef();
            if (isFilePlanContainer(child) || isRecordFolder(child) || isRecord(child)|| isHold(child)|| instanceOf(child,TYPE_TRANSFER)) {
              deletePermission(child,authority,permission);
            }
          }
        }
      }
      return null;
    }
  }
);
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.security.RecordsManagementSecurityService#deletePermission(org.alfresco.service.cmr.repository.NodeRef,java.lang.String,java.lang.String)
 */
public void deletePermission(final NodeRef nodeRef,final String authority,final String permission){
  runAsSystem(new AuthenticationUtil.RunAsWork<Object>(){
    public Void doWork(){
      if (!permissionService.getInheritParentPermissions(nodeRef)) {
        permissionService.deletePermission(nodeRef,authority,permission);
        if (isFilePlanContainer(nodeRef) || isRecordFolder(nodeRef)) {
          List<ChildAssociationRef> assocs=nodeService.getChildAssocs(nodeRef,ContentModel.ASSOC_CONTAINS,RegexQNamePattern.MATCH_ALL);
          for (          ChildAssociationRef assoc : assocs) {
            NodeRef child=assoc.getChildRef();
            if (isFilePlanContainer(child) || isRecordFolder(child) || isRecord(child)|| isHold(child)|| instanceOf(child,TYPE_TRANSFER)) {
              deletePermission(child,authority,permission);
            }
          }
        }
      }
      return null;
    }
  }
);
}",0.995091477019188
89240,"public Boolean doWork(){
  if (!permissionService.getInheritParentPermissions(nodeRef)) {
    permissionService.deletePermission(nodeRef,authority,permission);
    if (isFilePlanContainer(nodeRef) || isRecordFolder(nodeRef)) {
      List<ChildAssociationRef> assocs=nodeService.getChildAssocs(nodeRef,ContentModel.ASSOC_CONTAINS,RegexQNamePattern.MATCH_ALL);
      for (      ChildAssociationRef assoc : assocs) {
        NodeRef child=assoc.getChildRef();
        if (isFilePlanContainer(child) || isRecordFolder(child) || isRecord(child)|| isHold(child)|| instanceOf(child,TYPE_TRANSFER)) {
          deletePermission(child,authority,permission);
        }
      }
    }
  }
  return null;
}","public Void doWork(){
  if (!permissionService.getInheritParentPermissions(nodeRef)) {
    permissionService.deletePermission(nodeRef,authority,permission);
    if (isFilePlanContainer(nodeRef) || isRecordFolder(nodeRef)) {
      List<ChildAssociationRef> assocs=nodeService.getChildAssocs(nodeRef,ContentModel.ASSOC_CONTAINS,RegexQNamePattern.MATCH_ALL);
      for (      ChildAssociationRef assoc : assocs) {
        NodeRef child=assoc.getChildRef();
        if (isFilePlanContainer(child) || isRecordFolder(child) || isRecord(child)|| isHold(child)|| instanceOf(child,TYPE_TRANSFER)) {
          deletePermission(child,authority,permission);
        }
      }
    }
  }
  return null;
}",0.9920462762111352
89241,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.security.RecordsManagementSecurityService#setPermission(org.alfresco.service.cmr.repository.NodeRef,java.lang.String,java.lang.String,boolean)
 */
public void setPermission(final NodeRef nodeRef,final String authority,final String permission){
  ParameterCheck.mandatory(""String_Node_Str"",nodeRef);
  ParameterCheck.mandatory(""String_Node_Str"",authority);
  ParameterCheck.mandatory(""String_Node_Str"",permission);
  runAsSystem(new AuthenticationUtil.RunAsWork<Object>(){
    public Boolean doWork(){
      if (isFilePlan(nodeRef)) {
        setPermissionDown(nodeRef,authority,permission);
      }
 else       if (isFilePlanContainer(nodeRef) || isRecordFolder(nodeRef) || isRecord(nodeRef)|| isHold(nodeRef)) {
        setReadPermissionUp(nodeRef,authority);
        setPermissionDown(nodeRef,authority,permission);
      }
 else {
        if (logger.isWarnEnabled()) {
          logger.warn(""String_Node_Str"" + nodeRef + ""String_Node_Str""+ authority+ ""String_Node_Str""+ permission+ ""String_Node_Str"");
        }
      }
      return null;
    }
  }
);
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.security.RecordsManagementSecurityService#setPermission(org.alfresco.service.cmr.repository.NodeRef,java.lang.String,java.lang.String,boolean)
 */
public void setPermission(final NodeRef nodeRef,final String authority,final String permission){
  ParameterCheck.mandatory(""String_Node_Str"",nodeRef);
  ParameterCheck.mandatory(""String_Node_Str"",authority);
  ParameterCheck.mandatory(""String_Node_Str"",permission);
  runAsSystem(new AuthenticationUtil.RunAsWork<Object>(){
    public Void doWork(){
      if (isFilePlan(nodeRef)) {
        setPermissionDown(nodeRef,authority,permission);
      }
 else       if (isFilePlanContainer(nodeRef) || isRecordFolder(nodeRef) || isRecord(nodeRef)|| isHold(nodeRef)) {
        setReadPermissionUp(nodeRef,authority);
        setPermissionDown(nodeRef,authority,permission);
      }
 else {
        if (logger.isWarnEnabled()) {
          logger.warn(""String_Node_Str"" + nodeRef + ""String_Node_Str""+ authority+ ""String_Node_Str""+ permission+ ""String_Node_Str"");
        }
      }
      return null;
    }
  }
);
}",0.995047276001801
89242,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  String actionName=action.getActionDefinitionName();
  if (isOkToProceedWithAction(actionedUponNodeRef,actionName)) {
    QName actionedUponType=nodeService.getType(actionedUponNodeRef);
    ;
    boolean targetIsUnfiledRecords;
    if (ACTION_FILETO.equals(action.getActionDefinitionName())) {
      targetIsUnfiledRecords=false;
    }
 else {
      targetIsUnfiledRecords=(dictionaryService.isSubClass(actionedUponType,ContentModel.TYPE_CONTENT) && !recordService.isFiled(actionedUponNodeRef)) || TYPE_UNFILED_RECORD_FOLDER.equals(actionedUponType);
    }
    NodeRef recordFolder=(NodeRef)action.getParameterValue(PARAM_DESTINATION_RECORD_FOLDER);
    if (recordFolder == null) {
      recordFolder=createOrResolvePath(action,actionedUponNodeRef,targetIsUnfiledRecords);
    }
    validateActionPostPathResolution(actionedUponNodeRef,recordFolder,actionName,targetIsUnfiledRecords);
    final NodeRef finalRecordFolder=recordFolder;
    AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
      @Override public Void doWork() throws Exception {
        try {
          if (mode == CopyMoveLinkFileToActionMode.MOVE) {
            fileFolderService.move(actionedUponNodeRef,finalRecordFolder,null);
          }
 else           if (mode == CopyMoveLinkFileToActionMode.COPY) {
            fileFolderService.copy(actionedUponNodeRef,finalRecordFolder,null);
          }
 else           if (mode == CopyMoveLinkFileToActionMode.LINK) {
            recordService.link(actionedUponNodeRef,finalRecordFolder);
          }
        }
 catch (        FileNotFoundException fileNotFound) {
          throw new AlfrescoRuntimeException(""String_Node_Str"" + (mode == CopyMoveLinkFileToActionMode.MOVE ? ""String_Node_Str"" : ""String_Node_Str"") + ""String_Node_Str"",fileNotFound);
        }
        return null;
      }
    }
);
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  String actionName=action.getActionDefinitionName();
  if (isOkToProceedWithAction(actionedUponNodeRef,actionName)) {
    QName actionedUponType=nodeService.getType(actionedUponNodeRef);
    boolean targetIsUnfiledRecords;
    if (ACTION_FILETO.equals(action.getActionDefinitionName())) {
      targetIsUnfiledRecords=false;
    }
 else {
      targetIsUnfiledRecords=(dictionaryService.isSubClass(actionedUponType,ContentModel.TYPE_CONTENT) && !recordService.isFiled(actionedUponNodeRef)) || TYPE_UNFILED_RECORD_FOLDER.equals(actionedUponType);
    }
    NodeRef recordFolder=(NodeRef)action.getParameterValue(PARAM_DESTINATION_RECORD_FOLDER);
    if (recordFolder == null) {
      recordFolder=createOrResolvePath(action,actionedUponNodeRef,targetIsUnfiledRecords);
    }
    validateActionPostPathResolution(actionedUponNodeRef,recordFolder,actionName,targetIsUnfiledRecords);
    final NodeRef finalRecordFolder=recordFolder;
    AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
      @Override public Void doWork() throws Exception {
        try {
          if (mode == CopyMoveLinkFileToActionMode.MOVE) {
            fileFolderService.move(actionedUponNodeRef,finalRecordFolder,null);
          }
 else           if (mode == CopyMoveLinkFileToActionMode.COPY) {
            fileFolderService.copy(actionedUponNodeRef,finalRecordFolder,null);
          }
 else           if (mode == CopyMoveLinkFileToActionMode.LINK) {
            recordService.link(actionedUponNodeRef,finalRecordFolder);
          }
        }
 catch (        FileNotFoundException fileNotFound) {
          throw new AlfrescoRuntimeException(""String_Node_Str"" + (mode == CopyMoveLinkFileToActionMode.MOVE ? ""String_Node_Str"" : ""String_Node_Str"") + ""String_Node_Str"",fileNotFound);
        }
        return null;
      }
    }
);
  }
}",0.99856184084372
89243,"/** 
 * Helper method to get the transfer node reference. <p> Takes into account records in tranferred record folders.
 * @param nodeRef               node reference
 * @return {@link NodeRef}      transfer node
 */
private NodeRef getTransferNodeRef(NodeRef nodeRef){
  NodeRef result=null;
  List<ChildAssociationRef> parents=nodeService.getParentAssocs(nodeRef,RecordsManagementModel.ASSOC_TRANSFERRED,RegexQNamePattern.MATCH_ALL);
  if (parents.size() == 1) {
    result=parents.get(0).getParentRef();
  }
 else {
    if (recordService.isRecord(nodeRef)) {
      for (      NodeRef recordFolder : recordFolderService.getRecordFolders(nodeRef)) {
        result=getTransferNodeRef(recordFolder);
        if (result != null) {
          break;
        }
      }
      ;
    }
  }
  return result;
}","/** 
 * Helper method to get the transfer node reference. <p> Takes into account records in tranferred record folders.
 * @param nodeRef               node reference
 * @return {@link NodeRef}      transfer node
 */
private NodeRef getTransferNodeRef(NodeRef nodeRef){
  NodeRef result=null;
  List<ChildAssociationRef> parents=nodeService.getParentAssocs(nodeRef,RecordsManagementModel.ASSOC_TRANSFERRED,RegexQNamePattern.MATCH_ALL);
  if (parents.size() == 1) {
    result=parents.get(0).getParentRef();
  }
 else {
    if (recordService.isRecord(nodeRef)) {
      for (      NodeRef recordFolder : recordFolderService.getRecordFolders(nodeRef)) {
        result=getTransferNodeRef(recordFolder);
        if (result != null) {
          break;
        }
      }
    }
  }
  return result;
}",0.9685929648241206
89244,"/** 
 * Report capability status.
 * @param name      capability name
 * @param status    capability status
 */
public static void reportCapabilityStatus(String name,int status){
  if (logger.isDebugEnabled()) {
    CapabilityReport capability=getCapabilityReport(name);
    capability.status=translate(status);
    ;
  }
}","/** 
 * Report capability status.
 * @param name      capability name
 * @param status    capability status
 */
public static void reportCapabilityStatus(String name,int status){
  if (logger.isDebugEnabled()) {
    CapabilityReport capability=getCapabilityReport(name);
    capability.status=translate(status);
  }
}",0.990625
89245,"/** 
 * Get capability report object from the thread local, creating one for the given capability name if one does not already exist.
 * @param name  capability name 
 * @return {@link CapabilityReport} object containing information about the capability
 */
private static final CapabilityReport getCapabilityReport(String name){
  Map<String,CapabilityReport> map=RMMethodSecurityInterceptor.capabilities.get();
  CapabilityReport capability=map.get(name);
  if (capability == null) {
    capability=new CapabilityReport();
    capability.name=name;
    map.put(name,capability);
  }
  return capability;
}","/** 
 * Get capability report object from the thread local, creating one for the given capability name if one does not already exist.
 * @param name  capability name
 * @return {@link CapabilityReport} object containing information about the capability
 */
private static final CapabilityReport getCapabilityReport(String name){
  Map<String,CapabilityReport> map=RMMethodSecurityInterceptor.capabilities.get();
  CapabilityReport capability=map.get(name);
  if (capability == null) {
    capability=new CapabilityReport();
    capability.name=name;
    map.put(name,capability);
  }
  return capability;
}",0.9991755976916736
89246,"/** 
 * @param isRecordLevelDisposition
 * @param rmContainer
 * @param root
 * @return
 */
private List<NodeRef> getDisposableItemsImpl(boolean isRecordLevelDisposition,NodeRef rmContainer){
  List<NodeRef> items=filePlanService.getAllContained(rmContainer);
  List<NodeRef> result=new ArrayList<NodeRef>(items.size());
  for (  NodeRef item : items) {
    if (recordFolderService.isRecordFolder(item)) {
      if (isRecordLevelDisposition) {
        result.addAll(recordService.getRecords(item));
      }
 else {
        result.add(item);
      }
    }
 else     if (filePlanService.isRecordCategory(item)) {
      if (getAssociatedDispositionScheduleImpl(item) == null) {
        result.addAll(getDisposableItemsImpl(isRecordLevelDisposition,item));
      }
    }
  }
  return result;
}","/** 
 * @param isRecordLevelDisposition
 * @param rmContainer
 * @param root
 * @return
 */
private List<NodeRef> getDisposableItemsImpl(boolean isRecordLevelDisposition,NodeRef rmContainer){
  List<NodeRef> items=filePlanService.getAllContained(rmContainer);
  List<NodeRef> result=new ArrayList<NodeRef>(items.size());
  for (  NodeRef item : items) {
    if (recordFolderService.isRecordFolder(item)) {
      if (isRecordLevelDisposition) {
        result.addAll(recordService.getRecords(item));
      }
 else {
        result.add(item);
      }
    }
 else     if (filePlanService.isRecordCategory(item) && getAssociatedDispositionScheduleImpl(item) == null) {
      result.addAll(getDisposableItemsImpl(isRecordLevelDisposition,item));
    }
  }
  return result;
}",0.962772785622593
89247,"@Override public Void doWork() throws Exception {
  Date updatedDateValue=(Date)after.get(propertyName);
  if (updatedDateValue != null) {
    DispositionAction dispositionAction=dispositionService.getNextDispositionAction(nodeRef);
    if (dispositionAction != null) {
      DispositionActionDefinition daDefinition=dispositionAction.getDispositionActionDefinition();
      if (daDefinition != null) {
        if (propertyName.equals(daDefinition.getPeriodProperty())) {
          Period period=daDefinition.getPeriod();
          Date updatedAsOf=period.getNextDate(updatedDateValue);
          NodeRef daNodeRef=dispositionAction.getNodeRef();
          nodeService.setProperty(daNodeRef,PROP_DISPOSITION_AS_OF,updatedAsOf);
        }
      }
    }
  }
 else {
    if (before.get(propertyName) != null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + propertyName.toPrefixString(namespaceService) + ""String_Node_Str"");
    }
  }
  return null;
}","@Override public Void doWork() throws Exception {
  Date updatedDateValue=(Date)after.get(propertyName);
  if (updatedDateValue != null) {
    DispositionAction dispositionAction=dispositionService.getNextDispositionAction(nodeRef);
    if (dispositionAction != null) {
      DispositionActionDefinition daDefinition=dispositionAction.getDispositionActionDefinition();
      if (daDefinition != null && propertyName.equals(daDefinition.getPeriodProperty())) {
        Period period=daDefinition.getPeriod();
        Date updatedAsOf=period.getNextDate(updatedDateValue);
        NodeRef daNodeRef=dispositionAction.getNodeRef();
        nodeService.setProperty(daNodeRef,PROP_DISPOSITION_AS_OF,updatedAsOf);
      }
    }
  }
 else {
    if (before.get(propertyName) != null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + propertyName.toPrefixString(namespaceService) + ""String_Node_Str"");
    }
  }
  return null;
}",0.7795358649789029
89248,"/** 
 * @see org.alfresco.repo.node.NodeServicePolicies.OnUpdatePropertiesPolicy#onUpdateProperties(org.alfresco.service.cmr.repository.NodeRef,java.util.Map,java.util.Map)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,type=""String_Node_Str"",notificationFrequency=NotificationFrequency.TRANSACTION_COMMIT) public void onUpdateProperties(final NodeRef nodeRef,final Map<QName,Serializable> before,final Map<QName,Serializable> after){
  if (nodeService.exists(nodeRef)) {
    if (isPropertyUpdated(before,after)) {
      AuthenticationUtil.runAs(new RunAsWork<Void>(){
        @Override public Void doWork() throws Exception {
          Date updatedDateValue=(Date)after.get(propertyName);
          if (updatedDateValue != null) {
            DispositionAction dispositionAction=dispositionService.getNextDispositionAction(nodeRef);
            if (dispositionAction != null) {
              DispositionActionDefinition daDefinition=dispositionAction.getDispositionActionDefinition();
              if (daDefinition != null) {
                if (propertyName.equals(daDefinition.getPeriodProperty())) {
                  Period period=daDefinition.getPeriod();
                  Date updatedAsOf=period.getNextDate(updatedDateValue);
                  NodeRef daNodeRef=dispositionAction.getNodeRef();
                  nodeService.setProperty(daNodeRef,PROP_DISPOSITION_AS_OF,updatedAsOf);
                }
              }
            }
          }
 else {
            if (before.get(propertyName) != null) {
              throw new AlfrescoRuntimeException(""String_Node_Str"" + propertyName.toPrefixString(namespaceService) + ""String_Node_Str"");
            }
          }
          return null;
        }
      }
,AuthenticationUtil.getSystemUserName());
    }
  }
}","/** 
 * @see org.alfresco.repo.node.NodeServicePolicies.OnUpdatePropertiesPolicy#onUpdateProperties(org.alfresco.service.cmr.repository.NodeRef,java.util.Map,java.util.Map)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,type=""String_Node_Str"",notificationFrequency=NotificationFrequency.TRANSACTION_COMMIT) public void onUpdateProperties(final NodeRef nodeRef,final Map<QName,Serializable> before,final Map<QName,Serializable> after){
  if (nodeService.exists(nodeRef)) {
    if (isPropertyUpdated(before,after)) {
      AuthenticationUtil.runAs(new RunAsWork<Void>(){
        @Override public Void doWork() throws Exception {
          Date updatedDateValue=(Date)after.get(propertyName);
          if (updatedDateValue != null) {
            DispositionAction dispositionAction=dispositionService.getNextDispositionAction(nodeRef);
            if (dispositionAction != null) {
              DispositionActionDefinition daDefinition=dispositionAction.getDispositionActionDefinition();
              if (daDefinition != null && propertyName.equals(daDefinition.getPeriodProperty())) {
                Period period=daDefinition.getPeriod();
                Date updatedAsOf=period.getNextDate(updatedDateValue);
                NodeRef daNodeRef=dispositionAction.getNodeRef();
                nodeService.setProperty(daNodeRef,PROP_DISPOSITION_AS_OF,updatedAsOf);
              }
            }
          }
 else {
            if (before.get(propertyName) != null) {
              throw new AlfrescoRuntimeException(""String_Node_Str"" + propertyName.toPrefixString(namespaceService) + ""String_Node_Str"");
            }
          }
          return null;
        }
      }
,AuthenticationUtil.getSystemUserName());
    }
  }
}",0.8737142857142857
89249,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.fileplan.FilePlanService#getFilePlanBySiteId(java.lang.String)
 */
@Override public NodeRef getFilePlanBySiteId(String siteId){
  NodeRef filePlan=null;
  SiteService siteService=getSiteService();
  SiteInfo siteInfo=siteService.getSite(siteId);
  if (siteInfo != null) {
    if (siteService.hasContainer(siteId,FILE_PLAN_CONTAINER)) {
      NodeRef nodeRef=siteService.getContainer(siteId,FILE_PLAN_CONTAINER);
      if (instanceOf(nodeRef,TYPE_FILE_PLAN)) {
        filePlan=nodeRef;
      }
    }
  }
  return filePlan;
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.fileplan.FilePlanService#getFilePlanBySiteId(java.lang.String)
 */
@Override public NodeRef getFilePlanBySiteId(String siteId){
  NodeRef filePlan=null;
  SiteService siteService=getSiteService();
  SiteInfo siteInfo=siteService.getSite(siteId);
  if (siteInfo != null && siteService.hasContainer(siteId,FILE_PLAN_CONTAINER)) {
    NodeRef nodeRef=siteService.getContainer(siteId,FILE_PLAN_CONTAINER);
    if (instanceOf(nodeRef,TYPE_FILE_PLAN)) {
      filePlan=nodeRef;
    }
  }
  return filePlan;
}",0.9437609841827768
89250,"/** 
 * Marks all the fields that contain data extracted from an email as protected fields.
 * @param form The Form instance to add the property to
 * @param nodeRef The node the form is being generated for
 */
protected void protectEmailExtractedFields(Form form,NodeRef nodeRef){
  List<FieldDefinition> fieldDefs=form.getFieldDefinitions();
  for (  FieldDefinition fieldDef : fieldDefs) {
    String prefixName=fieldDef.getName();
    QName qname=QName.createQName(prefixName,namespaceService);
    Serializable value=nodeService.getProperty(nodeRef,qname);
    if (value != null) {
      if (prefixName.equals(""String_Node_Str"") || prefixName.equals(""String_Node_Str"") || prefixName.equals(""String_Node_Str"")|| prefixName.equals(""String_Node_Str"")|| prefixName.equals(""String_Node_Str"")|| prefixName.equals(""String_Node_Str"")|| prefixName.equals(""String_Node_Str"")) {
        fieldDef.setProtectedField(true);
      }
    }
  }
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
}","/** 
 * Marks all the fields that contain data extracted from an email as protected fields.
 * @param form The Form instance to add the property to
 * @param nodeRef The node the form is being generated for
 */
protected void protectEmailExtractedFields(Form form,NodeRef nodeRef){
  List<FieldDefinition> fieldDefs=form.getFieldDefinitions();
  for (  FieldDefinition fieldDef : fieldDefs) {
    String prefixName=fieldDef.getName();
    QName qname=QName.createQName(prefixName,namespaceService);
    Serializable value=nodeService.getProperty(nodeRef,qname);
    if (value != null && (prefixName.equals(""String_Node_Str"") || prefixName.equals(""String_Node_Str"") || prefixName.equals(""String_Node_Str"")|| prefixName.equals(""String_Node_Str"")|| prefixName.equals(""String_Node_Str"")|| prefixName.equals(""String_Node_Str"")|| prefixName.equals(""String_Node_Str""))) {
      fieldDef.setProtectedField(true);
    }
  }
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
}",0.6106106106106106
89251,"/** 
 * onMoveRecord behaviour
 * @param sourceAssocRef        source association reference
 * @param destinationAssocRef   destination association reference
 */
public void onMoveRecord(final ChildAssociationRef sourceAssocRef,final ChildAssociationRef destinationAssocRef){
  AuthenticationUtil.runAs(new AuthenticationUtil.RunAsWork<Void>(){
    public Void doWork(){
      NodeRef record=sourceAssocRef.getChildRef();
      if (nodeService.exists(record) && nodeService.hasAspect(record,ASPECT_RECORD)) {
        Set<AccessPermission> keepPerms=new HashSet<AccessPermission>(5);
        Set<AccessPermission> origionalParentPerms=permissionService.getAllSetPermissions(sourceAssocRef.getParentRef());
        Set<AccessPermission> origionalRecordPerms=permissionService.getAllSetPermissions(record);
        for (        AccessPermission perm : origionalRecordPerms) {
          if (!ExtendedReaderDynamicAuthority.EXTENDED_READER.equals(perm.getAuthority()) && !ExtendedWriterDynamicAuthority.EXTENDED_WRITER.equals(perm.getAuthority())) {
            if ((perm.getPermission().equals(RMPermissionModel.FILING) || perm.getPermission().equals(RMPermissionModel.FILE_RECORDS)) && !origionalParentPerms.contains(perm)) {
              keepPerms.add(perm);
            }
          }
        }
        permissionService.deletePermissions(record);
        setupPermissions(destinationAssocRef.getParentRef(),record);
        for (        AccessPermission keeper : keepPerms) {
          setPermission(record,keeper.getAuthority(),keeper.getPermission());
        }
      }
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}","/** 
 * onMoveRecord behaviour
 * @param sourceAssocRef        source association reference
 * @param destinationAssocRef   destination association reference
 */
public void onMoveRecord(final ChildAssociationRef sourceAssocRef,final ChildAssociationRef destinationAssocRef){
  AuthenticationUtil.runAs(new AuthenticationUtil.RunAsWork<Void>(){
    public Void doWork(){
      NodeRef record=sourceAssocRef.getChildRef();
      if (nodeService.exists(record) && nodeService.hasAspect(record,ASPECT_RECORD)) {
        Set<AccessPermission> keepPerms=new HashSet<AccessPermission>(5);
        Set<AccessPermission> origionalParentPerms=permissionService.getAllSetPermissions(sourceAssocRef.getParentRef());
        Set<AccessPermission> origionalRecordPerms=permissionService.getAllSetPermissions(record);
        for (        AccessPermission perm : origionalRecordPerms) {
          if (!ExtendedReaderDynamicAuthority.EXTENDED_READER.equals(perm.getAuthority()) && !ExtendedWriterDynamicAuthority.EXTENDED_WRITER.equals(perm.getAuthority()) && (perm.getPermission().equals(RMPermissionModel.FILING) || perm.getPermission().equals(RMPermissionModel.FILE_RECORDS))&& !origionalParentPerms.contains(perm)) {
            keepPerms.add(perm);
          }
        }
        permissionService.deletePermissions(record);
        setupPermissions(destinationAssocRef.getParentRef(),record);
        for (        AccessPermission keeper : keepPerms) {
          setPermission(record,keeper.getAuthority(),keeper.getPermission());
        }
      }
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}",0.9874039938556068
89252,"public static boolean isRmSite(WebScriptRequest req,SiteService siteService){
  boolean isRmSite=false;
  String siteId=req.getParameter(SITE_ID);
  if (StringUtils.isNotBlank(siteId)) {
    SiteInfo site=siteService.getSite(siteId);
    if (site != null) {
      if (site.getSitePreset().equals(SITE_PRESET)) {
        isRmSite=true;
      }
    }
  }
  return isRmSite;
}","public static boolean isRmSite(WebScriptRequest req,SiteService siteService){
  boolean isRmSite=false;
  String siteId=req.getParameter(SITE_ID);
  if (StringUtils.isNotBlank(siteId)) {
    SiteInfo site=siteService.getSite(siteId);
    if (site != null && site.getSitePreset().equals(SITE_PRESET)) {
      isRmSite=true;
    }
  }
  return isRmSite;
}",0.8732782369146006
89253,"/** 
 * Add the disposition schedule associated with the node ref to the passed set of disposition schedule then call this method recursively for this node's children
 * @param nodeRef
 * @param dispositionSchedules
 */
private void getDispositionSchedules(NodeRef nodeRef,Set<DispositionSchedule> dispositionSchedules){
  if (filePlanService.isRecordCategory(nodeRef) == true) {
    DispositionSchedule dispositionSchedule=this.dispositionService.getDispositionSchedule(nodeRef);
    if (dispositionSchedule != null) {
      dispositionSchedules.add(dispositionSchedule);
    }
    List<ChildAssociationRef> children=nodeService.getChildAssocs(nodeRef,ContentModel.ASSOC_CONTAINS,RegexQNamePattern.MATCH_ALL);
    for (    ChildAssociationRef childAssoc : children) {
      getDispositionSchedules(childAssoc.getChildRef(),dispositionSchedules);
    }
  }
}","/** 
 * Add the disposition schedule associated with the node ref to the passed set of disposition schedule then call this method recursively for this node's children
 * @param nodeRef
 * @param dispositionSchedules
 */
private void getDispositionSchedules(NodeRef nodeRef,Set<DispositionSchedule> dispositionSchedules){
  if (filePlanService.isRecordCategory(nodeRef)) {
    DispositionSchedule dispositionSchedule=this.dispositionService.getDispositionSchedule(nodeRef);
    if (dispositionSchedule != null) {
      dispositionSchedules.add(dispositionSchedule);
    }
    List<ChildAssociationRef> children=nodeService.getChildAssocs(nodeRef,ContentModel.ASSOC_CONTAINS,RegexQNamePattern.MATCH_ALL);
    for (    ChildAssociationRef childAssoc : children) {
      getDispositionSchedules(childAssoc.getChildRef(),dispositionSchedules);
    }
  }
}",0.9953161592505856
89254,"/** 
 * Gets the failure report for the currently recorded capabilities.
 * @return  {@link String}  capability error report
 */
public String getFailureReport(){
  String result=null;
  if (logger.isDebugEnabled()) {
    Collection<CapabilityReport> capabilities=RMMethodSecurityInterceptor.CAPABILITIES.get().values();
    if (!capabilities.isEmpty()) {
      StringBuffer buffer=new StringBuffer(""String_Node_Str"");
      for (      CapabilityReport capability : capabilities) {
        buffer.append(""String_Node_Str"").append(capability.name).append(""String_Node_Str"").append(capability.status).append(""String_Node_Str"");
        if (!capability.conditions.isEmpty()) {
          for (          Map.Entry<String,Boolean> entry : capability.conditions.entrySet()) {
            buffer.append(""String_Node_Str"").append(entry.getKey()).append(""String_Node_Str"");
            if (entry.getValue() == true) {
              buffer.append(""String_Node_Str"");
            }
 else {
              buffer.append(""String_Node_Str"");
            }
            buffer.append(""String_Node_Str"");
          }
        }
      }
      result=buffer.toString();
    }
  }
  return result;
}","/** 
 * Gets the failure report for the currently recorded capabilities.
 * @return  {@link String}  capability error report
 */
public String getFailureReport(){
  String result=null;
  if (logger.isDebugEnabled()) {
    Collection<CapabilityReport> capabilities=RMMethodSecurityInterceptor.CAPABILITIES.get().values();
    if (!capabilities.isEmpty()) {
      StringBuffer buffer=new StringBuffer(""String_Node_Str"");
      for (      CapabilityReport capability : capabilities) {
        buffer.append(""String_Node_Str"").append(capability.name).append(""String_Node_Str"").append(capability.status).append(""String_Node_Str"");
        if (!capability.conditions.isEmpty()) {
          for (          Map.Entry<String,Boolean> entry : capability.conditions.entrySet()) {
            buffer.append(""String_Node_Str"").append(entry.getKey()).append(""String_Node_Str"");
            if (entry.getValue()) {
              buffer.append(""String_Node_Str"");
            }
 else {
              buffer.append(""String_Node_Str"");
            }
            buffer.append(""String_Node_Str"");
          }
        }
      }
      result=buffer.toString();
    }
  }
  return result;
}",0.7764505119453925
89255,"/** 
 * Report capability condition.
 * @param name              capability name
 * @param conditionName     capability condition name
 * @param expected          expected value
 * @param actual            actual value
 */
public static void reportCapabilityCondition(String name,String conditionName,boolean expected,boolean actual){
  if (logger.isDebugEnabled()) {
    CapabilityReport capability=getCapabilityReport(name);
    if (expected == false) {
      conditionName=""String_Node_Str"" + conditionName;
    }
    capability.conditions.put(conditionName,(expected == actual));
  }
}","/** 
 * Report capability condition.
 * @param name              capability name
 * @param conditionName     capability condition name
 * @param expected          expected value
 * @param actual            actual value
 */
public static void reportCapabilityCondition(String name,String conditionName,boolean expected,boolean actual){
  if (logger.isDebugEnabled()) {
    CapabilityReport capability=getCapabilityReport(name);
    if (!expected) {
      conditionName=""String_Node_Str"" + conditionName;
    }
    capability.conditions.put(conditionName,(expected == actual));
  }
}",0.9914529914529916
89256,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.transfer.TransferService#transfer(NodeRef,boolean)
 */
@Override public NodeRef transfer(NodeRef nodeRef,boolean isAccession){
  ParameterCheck.mandatory(""String_Node_Str"",nodeRef);
  NodeRef root=filePlanService.getFilePlan(nodeRef);
  NodeRef transferNodeRef=(NodeRef)AlfrescoTransactionSupport.getResource(KEY_TRANSFER_NODEREF);
  if (transferNodeRef == null) {
    QName nodeDbid=QName.createQName(NamespaceService.SYSTEM_MODEL_1_0_URI,""String_Node_Str"");
    Long dbId=(Long)nodeService.getProperty(nodeRef,nodeDbid);
    String transferName=StringUtils.leftPad(dbId.toString(),10,""String_Node_Str"");
    Map<QName,Serializable> transferProps=new HashMap<QName,Serializable>(2);
    transferProps.put(ContentModel.PROP_NAME,transferName);
    transferProps.put(PROP_TRANSFER_ACCESSION_INDICATOR,isAccession);
    DispositionAction da=dispositionService.getNextDispositionAction(nodeRef);
    if (da != null) {
      DispositionActionDefinition actionDef=da.getDispositionActionDefinition();
      if (actionDef != null) {
        transferProps.put(PROP_TRANSFER_LOCATION,actionDef.getLocation());
      }
    }
    NodeRef transferContainer=filePlanService.getTransferContainer(root);
    transferNodeRef=nodeService.createNode(transferContainer,ContentModel.ASSOC_CONTAINS,QName.createQName(RM_URI,transferName),TYPE_TRANSFER,transferProps).getChildRef();
    AlfrescoTransactionSupport.bindResource(KEY_TRANSFER_NODEREF,transferNodeRef);
  }
 else {
    List<ChildAssociationRef> transferredAlready=nodeService.getChildAssocs(transferNodeRef,ASSOC_TRANSFERRED,ASSOC_TRANSFERRED);
    for (    ChildAssociationRef car : transferredAlready) {
      if (car.getChildRef().equals(nodeRef)) {
        throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NODE_ALREADY_TRANSFER,nodeRef.toString()));
      }
    }
  }
  nodeService.addChild(transferNodeRef,nodeRef,ASSOC_TRANSFERRED,ASSOC_TRANSFERRED);
  setPDFIndicationFlag(transferNodeRef,nodeRef);
  nodeService.addAspect(nodeRef,ASPECT_TRANSFERRING,null);
  if (isRecordFolder(nodeRef) == true) {
    for (    NodeRef record : recordService.getRecords(nodeRef)) {
      nodeService.addAspect(record,ASPECT_TRANSFERRING,null);
    }
  }
  return transferNodeRef;
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.transfer.TransferService#transfer(NodeRef,boolean)
 */
@Override public NodeRef transfer(NodeRef nodeRef,boolean isAccession){
  ParameterCheck.mandatory(""String_Node_Str"",nodeRef);
  NodeRef root=filePlanService.getFilePlan(nodeRef);
  NodeRef transferNodeRef=(NodeRef)AlfrescoTransactionSupport.getResource(KEY_TRANSFER_NODEREF);
  if (transferNodeRef == null) {
    QName nodeDbid=QName.createQName(NamespaceService.SYSTEM_MODEL_1_0_URI,""String_Node_Str"");
    Long dbId=(Long)nodeService.getProperty(nodeRef,nodeDbid);
    String transferName=StringUtils.leftPad(dbId.toString(),10,""String_Node_Str"");
    Map<QName,Serializable> transferProps=new HashMap<QName,Serializable>(2);
    transferProps.put(ContentModel.PROP_NAME,transferName);
    transferProps.put(PROP_TRANSFER_ACCESSION_INDICATOR,isAccession);
    DispositionAction da=dispositionService.getNextDispositionAction(nodeRef);
    if (da != null) {
      DispositionActionDefinition actionDef=da.getDispositionActionDefinition();
      if (actionDef != null) {
        transferProps.put(PROP_TRANSFER_LOCATION,actionDef.getLocation());
      }
    }
    NodeRef transferContainer=filePlanService.getTransferContainer(root);
    transferNodeRef=nodeService.createNode(transferContainer,ContentModel.ASSOC_CONTAINS,QName.createQName(RM_URI,transferName),TYPE_TRANSFER,transferProps).getChildRef();
    AlfrescoTransactionSupport.bindResource(KEY_TRANSFER_NODEREF,transferNodeRef);
  }
 else {
    List<ChildAssociationRef> transferredAlready=nodeService.getChildAssocs(transferNodeRef,ASSOC_TRANSFERRED,ASSOC_TRANSFERRED);
    for (    ChildAssociationRef car : transferredAlready) {
      if (car.getChildRef().equals(nodeRef)) {
        throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NODE_ALREADY_TRANSFER,nodeRef.toString()));
      }
    }
  }
  nodeService.addChild(transferNodeRef,nodeRef,ASSOC_TRANSFERRED,ASSOC_TRANSFERRED);
  setPDFIndicationFlag(transferNodeRef,nodeRef);
  nodeService.addAspect(nodeRef,ASPECT_TRANSFERRING,null);
  if (isRecordFolder(nodeRef)) {
    for (    NodeRef record : recordService.getRecords(nodeRef)) {
      nodeService.addAspect(record,ASPECT_TRANSFERRING,null);
    }
  }
  return transferNodeRef;
}",0.9257469244288224
89257,"public HashMap<Integer,Integer> getParameters(){
  return parameters;
}","public Map<Integer,Integer> getParameters(){
  return parameters;
}",0.9710144927536232
89258,"/** 
 * Helper method to build a <b>NodeRef</b> path from the node to the RM root
 */
private void getNodeRefPathRecursive(NodeRef nodeRef,LinkedList<NodeRef> nodeRefPath){
  if (!isFilePlanComponent(nodeRef)) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_INVALID_RM_NODE,ASPECT_FILE_PLAN_COMPONENT.toString()));
  }
  nodeRefPath.addFirst(nodeRef);
  if (!isFilePlan(nodeRef)) {
    ChildAssociationRef assocRef=nodeService.getPrimaryParent(nodeRef);
    if (assocRef == null) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NO_ROOT));
    }
    nodeRef=assocRef.getParentRef();
    getNodeRefPathRecursive(nodeRef,nodeRefPath);
  }
}","/** 
 * Helper method to build a <b>NodeRef</b> path from the node to the RM root
 */
private void getNodeRefPathRecursive(NodeRef nodeRef,Deque<NodeRef> nodeRefPath){
  if (!isFilePlanComponent(nodeRef)) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_INVALID_RM_NODE,ASPECT_FILE_PLAN_COMPONENT.toString()));
  }
  nodeRefPath.addFirst(nodeRef);
  if (!isFilePlan(nodeRef)) {
    ChildAssociationRef assocRef=nodeService.getPrimaryParent(nodeRef);
    if (assocRef == null) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NO_ROOT));
    }
    nodeRef=assocRef.getParentRef();
    getNodeRefPathRecursive(nodeRef,nodeRefPath);
  }
}",0.988814317673378
89259,"/** 
 * Get capability report object from the thread local, creating one for the given capability name if one does not already exist.
 * @param name  capability name
 * @return {@link CapabilityReport} object containing information about the capability
 */
private static final CapabilityReport getCapabilityReport(String name){
  Map<String,CapabilityReport> map=RMMethodSecurityInterceptor.CAPABILITIES.get();
  CapabilityReport capability=map.get(name);
  if (capability == null) {
    capability=new CapabilityReport();
    capability.name=name;
    map.put(name,capability);
  }
  return capability;
}","/** 
 * Get capability report object from the thread local, creating one for the given capability name if one does not already exist.
 * @param name  capability name
 * @return {@link CapabilityReport} object containing information about the capability
 */
private static CapabilityReport getCapabilityReport(String name){
  Map<String,CapabilityReport> map=RMMethodSecurityInterceptor.CAPABILITIES.get();
  CapabilityReport capability=map.get(name);
  if (capability == null) {
    capability=new CapabilityReport();
    capability.name=name;
    map.put(name,capability);
  }
  return capability;
}",0.9950248756218906
89260,"/** 
 * @see org.alfresco.repo.action.evaluator.ActionConditionEvaluatorAbstractBase#init()
 */
@Override public void init(){
  AuthenticationUtil.runAs(new RunAsWork<Object>(){
    public Object doWork(){
      RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
        public Void execute() throws Throwable {
          recordsManagementActionService.register(RecordsManagementActionConditionEvaluatorAbstractBase.this);
          return null;
        }
      }
;
      retryingTransactionHelper.doInTransaction(callback);
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}","/** 
 * @see org.alfresco.repo.action.evaluator.ActionConditionEvaluatorAbstractBase#init()
 */
@Override public void init(){
  AuthenticationUtil.runAs(new RunAsWork<Object>(){
    public Object doWork(){
      RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
        public Void execute(){
          recordsManagementActionService.register(RecordsManagementActionConditionEvaluatorAbstractBase.this);
          return null;
        }
      }
;
      retryingTransactionHelper.doInTransaction(callback);
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}",0.9855072463768116
89261,"public Void execute() throws Throwable {
  recordsManagementActionService.register(RecordsManagementActionConditionEvaluatorAbstractBase.this);
  return null;
}","public Void execute(){
  recordsManagementActionService.register(RecordsManagementActionConditionEvaluatorAbstractBase.this);
  return null;
}",0.9403973509933776
89262,"/** 
 * @see org.alfresco.repo.action.evaluator.ActionConditionEvaluatorAbstractBase#getActionConditionDefintion() TODO base class should provide ""createActionDefinition"" method that can be over-ridden like the ActionExecuter  base class to prevent duplication of code and a cleaner extension.
 */
@Override public ActionConditionDefinition getActionConditionDefintion(){
  if (this.actionConditionDefinition == null) {
    this.actionConditionDefinition=new RecordsManagementActionConditionDefinitionImpl(name);
    ((RecordsManagementActionConditionDefinitionImpl)actionConditionDefinition).setTitleKey(getTitleKey());
    ((RecordsManagementActionConditionDefinitionImpl)actionConditionDefinition).setDescriptionKey(getDescriptionKey());
    ((RecordsManagementActionConditionDefinitionImpl)actionConditionDefinition).setAdhocPropertiesAllowed(getAdhocPropertiesAllowed());
    ((RecordsManagementActionConditionDefinitionImpl)actionConditionDefinition).setConditionEvaluator(name);
    ((RecordsManagementActionConditionDefinitionImpl)actionConditionDefinition).setParameterDefinitions(getParameterDefintions());
  }
  return this.actionConditionDefinition;
}","/** 
 * @see org.alfresco.repo.action.evaluator.ActionConditionEvaluatorAbstractBase#getActionConditionDefintion() TODO base class should provide ""createActionDefinition"" method that can be over-ridden like the ActionExecuter base class to prevent duplication of code and a cleaner extension.
 */
@Override public ActionConditionDefinition getActionConditionDefintion(){
  if (this.actionConditionDefinition == null) {
    this.actionConditionDefinition=new RecordsManagementActionConditionDefinitionImpl(name);
    ((RecordsManagementActionConditionDefinitionImpl)actionConditionDefinition).setTitleKey(getTitleKey());
    ((RecordsManagementActionConditionDefinitionImpl)actionConditionDefinition).setDescriptionKey(getDescriptionKey());
    ((RecordsManagementActionConditionDefinitionImpl)actionConditionDefinition).setAdhocPropertiesAllowed(getAdhocPropertiesAllowed());
    ((RecordsManagementActionConditionDefinitionImpl)actionConditionDefinition).setConditionEvaluator(name);
    ((RecordsManagementActionConditionDefinitionImpl)actionConditionDefinition).setParameterDefinitions(getParameterDefintions());
  }
  return this.actionConditionDefinition;
}",0.9995698924731182
89263,"public Object doWork(){
  RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
    public Void execute() throws Throwable {
      recordsManagementActionService.register(RecordsManagementActionConditionEvaluatorAbstractBase.this);
      return null;
    }
  }
;
  retryingTransactionHelper.doInTransaction(callback);
  return null;
}","public Object doWork(){
  RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
    public Void execute(){
      recordsManagementActionService.register(RecordsManagementActionConditionEvaluatorAbstractBase.this);
      return null;
    }
  }
;
  retryingTransactionHelper.doInTransaction(callback);
  return null;
}",0.9746478873239436
89264,"/** 
 * @see org.alfresco.repo.audit.extractor.DataExtractor#extractData(java.io.Serializable)
 */
public Serializable extractData(Serializable value) throws Throwable {
  NodeRef nodeRef=(NodeRef)value;
  String user=AuthenticationUtil.getFullyAuthenticatedUser();
  if (user == null) {
    return null;
  }
  NodeRef rmRootNodeRef=filePlanService.getFilePlan(nodeRef);
  Set<Role> roles=filePlanRoleService.getRolesByUser(rmRootNodeRef,user);
  StringBuilder sb=new StringBuilder(100);
  for (  Role role : roles) {
    if (sb.length() > 0) {
      sb.append(""String_Node_Str"");
    }
    sb.append(role.getDisplayLabel());
  }
  return sb.toString();
}","/** 
 * @see org.alfresco.repo.audit.extractor.DataExtractor#extractData(java.io.Serializable)
 */
public Serializable extractData(Serializable value){
  NodeRef nodeRef=(NodeRef)value;
  String user=AuthenticationUtil.getFullyAuthenticatedUser();
  if (user == null) {
    return null;
  }
  NodeRef rmRootNodeRef=filePlanService.getFilePlan(nodeRef);
  Set<Role> roles=filePlanRoleService.getRolesByUser(rmRootNodeRef,user);
  StringBuilder sb=new StringBuilder(100);
  for (  Role role : roles) {
    if (sb.length() > 0) {
      sb.append(""String_Node_Str"");
    }
    sb.append(role.getDisplayLabel());
  }
  return sb.toString();
}",0.9860681114551084
89265,"public Serializable extractData(Serializable value) throws Throwable {
  NodeRef nodeRef=(NodeRef)value;
  String identifier=(String)nodeService.getProperty(nodeRef,RecordsManagementModel.PROP_IDENTIFIER);
  return identifier;
}","public Serializable extractData(Serializable value){
  NodeRef nodeRef=(NodeRef)value;
  String identifier=(String)nodeService.getProperty(nodeRef,RecordsManagementModel.PROP_IDENTIFIER);
  return identifier;
}",0.958904109589041
89266,"/** 
 * @see org.alfresco.repo.audit.extractor.DataExtractor#extractData(java.io.Serializable)
 */
public Serializable extractData(Serializable value) throws Throwable {
  NodeRef nodeRef=(NodeRef)value;
  List<NodeRef> nodeRefPath=filePlanService.getNodeRefPath(nodeRef);
  StringBuilder sb=new StringBuilder(128);
  for (  NodeRef pathNodeRef : nodeRefPath) {
    String name=(String)nodeService.getProperty(pathNodeRef,ContentModel.PROP_NAME);
    sb.append(""String_Node_Str"").append(name);
  }
  return sb.toString();
}","/** 
 * @see org.alfresco.repo.audit.extractor.DataExtractor#extractData(java.io.Serializable)
 */
public Serializable extractData(Serializable value){
  NodeRef nodeRef=(NodeRef)value;
  List<NodeRef> nodeRefPath=filePlanService.getNodeRefPath(nodeRef);
  StringBuilder sb=new StringBuilder(128);
  for (  NodeRef pathNodeRef : nodeRefPath) {
    String name=(String)nodeService.getProperty(pathNodeRef,ContentModel.PROP_NAME);
    sb.append(""String_Node_Str"").append(name);
  }
  return sb.toString();
}",0.9824902723735408
89267,"public Serializable extractData(Serializable value) throws Throwable {
  NodeRef nodeRef=(NodeRef)value;
  List<NodeRef> nodeRefPath=filePlanService.getNodeRefPath(nodeRef);
  return (Serializable)nodeRefPath;
}","public Serializable extractData(Serializable value){
  NodeRef nodeRef=(NodeRef)value;
  List<NodeRef> nodeRefPath=filePlanService.getNodeRefPath(nodeRef);
  return (Serializable)nodeRefPath;
}",0.9554455445544554
89268,"public Object doWork(){
  RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
    public Void execute() throws Throwable {
      caveatConfigService.init();
      adminService.initialiseCustomModel();
      SplitEmailAction action=(SplitEmailAction)getApplicationContext().getBean(""String_Node_Str"");
      action.bootstrap();
      return null;
    }
  }
;
  transactionService.getRetryingTransactionHelper().doInTransaction(callback);
  return null;
}","public Object doWork(){
  RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
    public Void execute(){
      caveatConfigService.init();
      adminService.initialiseCustomModel();
      SplitEmailAction action=(SplitEmailAction)getApplicationContext().getBean(""String_Node_Str"");
      action.bootstrap();
      return null;
    }
  }
;
  transactionService.getRetryingTransactionHelper().doInTransaction(callback);
  return null;
}",0.98109243697479
89269,"public Void execute() throws Throwable {
  caveatConfigService.init();
  adminService.initialiseCustomModel();
  SplitEmailAction action=(SplitEmailAction)getApplicationContext().getBean(""String_Node_Str"");
  action.bootstrap();
  return null;
}","public Void execute(){
  caveatConfigService.init();
  adminService.initialiseCustomModel();
  SplitEmailAction action=(SplitEmailAction)getApplicationContext().getBean(""String_Node_Str"");
  action.bootstrap();
  return null;
}",0.961864406779661
89270,"@Override protected void onBootstrap(ApplicationEvent event){
  AuthenticationUtil.runAs(new RunAsWork<Object>(){
    public Object doWork(){
      RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
        public Void execute() throws Throwable {
          caveatConfigService.init();
          adminService.initialiseCustomModel();
          SplitEmailAction action=(SplitEmailAction)getApplicationContext().getBean(""String_Node_Str"");
          action.bootstrap();
          return null;
        }
      }
;
      transactionService.getRetryingTransactionHelper().doInTransaction(callback);
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}","@Override protected void onBootstrap(ApplicationEvent event){
  AuthenticationUtil.runAs(new RunAsWork<Object>(){
    public Object doWork(){
      RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
        public Void execute(){
          caveatConfigService.init();
          adminService.initialiseCustomModel();
          SplitEmailAction action=(SplitEmailAction)getApplicationContext().getBean(""String_Node_Str"");
          action.bootstrap();
          return null;
        }
      }
;
      transactionService.getRetryingTransactionHelper().doInTransaction(callback);
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}",0.9869565217391304
89271,"public Void execute() throws Throwable {
  try {
    updateExtractor();
  }
 catch (  RuntimeException e) {
    if (logger.isWarnEnabled()) {
      logger.warn(e.getMessage());
    }
    customMappings=null;
    throw e;
  }
  return null;
}","public Void execute(){
  try {
    updateExtractor();
  }
 catch (  RuntimeException e) {
    if (logger.isWarnEnabled()) {
      logger.warn(e.getMessage());
    }
    customMappings=null;
    throw e;
  }
  return null;
}",0.961206896551724
89272,"public Object doWork(){
  RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
    public Void execute() throws Throwable {
      try {
        updateExtractor();
      }
 catch (      RuntimeException e) {
        if (logger.isWarnEnabled()) {
          logger.warn(e.getMessage());
        }
        customMappings=null;
        throw e;
      }
      return null;
    }
  }
;
  transactionService.getRetryingTransactionHelper().doInTransaction(callback);
  return null;
}","public Object doWork(){
  RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
    public Void execute(){
      try {
        updateExtractor();
      }
 catch (      RuntimeException e) {
        if (logger.isWarnEnabled()) {
          logger.warn(e.getMessage());
        }
        customMappings=null;
        throw e;
      }
      return null;
    }
  }
;
  transactionService.getRetryingTransactionHelper().doInTransaction(callback);
  return null;
}",0.9818548387096774
89273,"/** 
 * @see org.springframework.extensions.surf.util.AbstractLifecycleBean#onBootstrap(org.springframework.context.ApplicationEvent)
 */
@Override protected void onBootstrap(ApplicationEvent event){
  AuthenticationUtil.runAs(new RunAsWork<Object>(){
    public Object doWork(){
      RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
        public Void execute() throws Throwable {
          try {
            updateExtractor();
          }
 catch (          RuntimeException e) {
            if (logger.isWarnEnabled()) {
              logger.warn(e.getMessage());
            }
            customMappings=null;
            throw e;
          }
          return null;
        }
      }
;
      transactionService.getRetryingTransactionHelper().doInTransaction(callback);
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}","/** 
 * @see org.springframework.extensions.surf.util.AbstractLifecycleBean#onBootstrap(org.springframework.context.ApplicationEvent)
 */
@Override protected void onBootstrap(ApplicationEvent event){
  AuthenticationUtil.runAs(new RunAsWork<Object>(){
    public Object doWork(){
      RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
        public Void execute(){
          try {
            updateExtractor();
          }
 catch (          RuntimeException e) {
            if (logger.isWarnEnabled()) {
              logger.warn(e.getMessage());
            }
            customMappings=null;
            throw e;
          }
          return null;
        }
      }
;
      transactionService.getRetryingTransactionHelper().doInTransaction(callback);
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}",0.9896788990825688
89274,"/** 
 * Action click on new rule button
 * @return {@link RmCreateRulePage}
 */
public RmCreateRulePage clickNewRuleButton(){
  click(NEW_RULE_BUTTON);
  HtmlPage page=drone.getCurrentPage();
  Assert.assertEquals(RmCreateRulePage.class.getSimpleName(),page.getClass().getSimpleName());
  return page.render();
}","/** 
 * Action click on new rule button
 * @return {@link RmCreateRulePage}
 */
public RmCreateRulePage clickNewRuleButton(){
  click(NEW_RULE_BUTTON);
  HtmlPage page=drone.getCurrentPage();
  return page.render();
}",0.7523629489603024
89275,"/** 
 * Action click on edit rule button
 * @return {@link RmCreateRulePage}
 */
public RmCreateRulePage clickEditButton(){
  click(EDIT_BUTTON);
  HtmlPage page=drone.getCurrentPage();
  Assert.assertEquals(RmCreateRulePage.class.getSimpleName(),page.getClass().getSimpleName());
  return page.render();
}","/** 
 * Action click on edit rule button
 * @return {@link RmCreateRulePage}
 */
public RmCreateRulePage clickEditButton(){
  click(EDIT_BUTTON);
  HtmlPage page=drone.getCurrentPage();
  return page.render();
}",0.746615087040619
89276,"/** 
 * Renders the page and waits until the element with the expected name has has been displayed. timer   {@link RenderTime} time to wait
 * @param expectedName {@link String} The name of the expected element
 * @return {@link UnfiledRecordsContainer} The unfiled records container displaying the expected element
 */
private UnfiledRecordsContainer render(RenderTime timer,String expectedName){
  WebDroneUtil.checkMandotaryParam(""String_Node_Str"",timer);
  timer.start();
  try {
    RenderElement filePlan=RenderElement.getVisibleRenderElement(FILEPLAN);
    RenderElement filePlanNav=RenderElement.getVisibleRenderElement(FILEPLAN_NAV);
    RenderElement unfiledRecordsFolderButton=new RenderElement(NEW_UNFILED_RECORDS_FOLDER_BTN,ElementState.CLICKABLE);
    RenderElement newDeclareRecordButton=new RenderElement(NEW_DECLARE_RECORD_BTN,ElementState.CLICKABLE);
    elementRender(timer,filePlan,filePlanNav,unfiledRecordsFolderButton,newDeclareRecordButton);
    setViewType(getNavigation().getViewType());
    if (StringUtils.isNotBlank(expectedName)) {
      while (true) {
        if (timer.timeLeft() <= 0) {
          throw new PageRenderTimeException(""String_Node_Str"" + this.getClass().getName() + ""String_Node_Str"");
        }
        boolean found=false;
        for (        FileDirectoryInfo fileDirectoryInfo : getFiles()) {
          if (fileDirectoryInfo.getName().contains(expectedName)) {
            found=true;
            break;
          }
        }
        if (found) {
          break;
        }
 else {
          continue;
        }
      }
    }
  }
 catch (  NoSuchElementException e) {
  }
 finally {
    timer.end();
  }
  return this;
}","/** 
 * Renders the page and waits until the element with the expected name has has been displayed. timer   {@link RenderTime} time to wait
 * @param expectedName {@link String} The name of the expected element
 * @return {@link UnfiledRecordsContainer} The unfiled records container displaying the expected element
 */
private UnfiledRecordsContainer render(RenderTime timer,String expectedName){
  WebDroneUtil.checkMandotaryParam(""String_Node_Str"",timer);
  timer.start();
  try {
    RenderElement filePlan=RenderElement.getVisibleRenderElement(FILEPLAN);
    RenderElement filePlanNav=RenderElement.getVisibleRenderElement(FILEPLAN_NAV);
    RenderElement unfiledRecordsFolderButton=new RenderElement(NEW_UNFILED_RECORDS_FOLDER_BTN,ElementState.CLICKABLE);
    RenderElement newDeclareRecordButton=new RenderElement(NEW_DECLARE_RECORD_BTN,ElementState.CLICKABLE);
    elementRender(timer,filePlan,filePlanNav,unfiledRecordsFolderButton,newDeclareRecordButton);
    setViewType(getNavigation().getViewType());
    if (StringUtils.isNotBlank(expectedName)) {
      while (true) {
        checkTimeLeft(timer);
        boolean found=false;
        for (        FileDirectoryInfo fileDirectoryInfo : getFiles()) {
          if (fileDirectoryInfo.getName().contains(expectedName)) {
            found=true;
            break;
          }
        }
        if (found) {
          break;
        }
 else {
          continue;
        }
      }
    }
  }
 catch (  NoSuchElementException e) {
  }
 finally {
    timer.end();
  }
  return this;
}",0.8776844070961718
89277,"/** 
 * Renders the page and waits until the element with the expected name has has been displayed. timer   {@link RenderTime} time to wait
 * @param expectedName {@link String} The name of the expected element
 * @return {@link UnfiledRecordsContainer} The unfiled records container displaying the expected element
 */
private UnfiledRecordsContainer render(RenderTime timer,String expectedName){
  WebDroneUtil.checkMandotaryParam(""String_Node_Str"",timer);
  timer.start();
  try {
    RenderElement filePlan=RenderElement.getVisibleRenderElement(FILEPLAN);
    RenderElement filePlanNav=RenderElement.getVisibleRenderElement(FILEPLAN_NAV);
    RenderElement unfiledRecordsFolderButton=new RenderElement(NEW_UNFILED_RECORDS_FOLDER_BTN,ElementState.CLICKABLE);
    RenderElement newDeclareRecordButton=new RenderElement(NEW_DECLARE_RECORD_BTN,ElementState.CLICKABLE);
    elementRender(timer,filePlan,filePlanNav,unfiledRecordsFolderButton,newDeclareRecordButton);
    setViewType(getNavigation().getViewType());
    if (StringUtils.isNotBlank(expectedName)) {
      while (true) {
        boolean found=false;
        for (        FileDirectoryInfo fileDirectoryInfo : getFiles()) {
          if (fileDirectoryInfo.getName().contains(expectedName)) {
            found=true;
            break;
          }
        }
        if (found) {
          break;
        }
 else {
          continue;
        }
      }
    }
  }
 catch (  NoSuchElementException e) {
  }
 finally {
    timer.end();
  }
  return this;
}","/** 
 * Renders the page and waits until the element with the expected name has has been displayed. timer   {@link RenderTime} time to wait
 * @param expectedName {@link String} The name of the expected element
 * @return {@link UnfiledRecordsContainer} The unfiled records container displaying the expected element
 */
private UnfiledRecordsContainer render(RenderTime timer,String expectedName){
  WebDroneUtil.checkMandotaryParam(""String_Node_Str"",timer);
  timer.start();
  try {
    RenderElement filePlan=RenderElement.getVisibleRenderElement(FILEPLAN);
    RenderElement filePlanNav=RenderElement.getVisibleRenderElement(FILEPLAN_NAV);
    RenderElement unfiledRecordsFolderButton=new RenderElement(NEW_UNFILED_RECORDS_FOLDER_BTN,ElementState.CLICKABLE);
    RenderElement newDeclareRecordButton=new RenderElement(NEW_DECLARE_RECORD_BTN,ElementState.CLICKABLE);
    elementRender(timer,filePlan,filePlanNav,unfiledRecordsFolderButton,newDeclareRecordButton);
    setViewType(getNavigation().getViewType());
    if (StringUtils.isNotBlank(expectedName)) {
      while (true) {
        if (timer.timeLeft() <= 0) {
          throw new PageRenderTimeException(""String_Node_Str"" + this.getClass().getName() + ""String_Node_Str"");
        }
        boolean found=false;
        for (        FileDirectoryInfo fileDirectoryInfo : getFiles()) {
          if (fileDirectoryInfo.getName().contains(expectedName)) {
            found=true;
            break;
          }
        }
        if (found) {
          break;
        }
 else {
          continue;
        }
      }
    }
  }
 catch (  NoSuchElementException e) {
  }
 finally {
    timer.end();
  }
  return this;
}",0.9500471253534402
89278,"/** 
 * Verify if item is a record. Part of Records management module, when a document is a record a small banner info is displayed indicating that it is a record.
 * @return <code>true</code> if record banner is visible <code>false</code> otherwise
 */
public boolean isRecord(){
  try {
    return find(BANNER).isDisplayed();
  }
 catch (  NoSuchElementException e) {
  }
  return false;
}","/** 
 * Verify if item is a record. Part of Records management module, when a document is a record a small banner info is displayed indicating that it is a record.
 * @return <code>true</code> if record banner is visible <code>false</code> otherwise
 */
public boolean isRecord(){
  try {
    return drone.find(BANNER).isDisplayed();
  }
 catch (  NoSuchElementException e) {
  }
  return false;
}",0.9923857868020304
89279,"/** 
 * Action of selecting the declare records button that appear in the action drop down.
 * @return {@link HtmlPage}
 */
public HtmlPage declareRecord(){
  selectMoreAction().click();
  try {
    WebElement createRecord=drone.find(CREATE_RECORD);
    createRecord.click();
  }
 catch (  NoSuchElementException nse) {
    throw new PageOperationException(""String_Node_Str"",nse);
  }
  canResume();
  return FactorySharePage.resolvePage(drone);
}","/** 
 * Action of selecting the declare records button that appear in the action drop down.
 * @return {@link HtmlPage}
 */
public HtmlPage declareRecord(){
  selectMoreAction().click();
  try {
    WebElement createRecord=drone.find(CREATE_RECORD);
    createRecord.click();
  }
 catch (  NoSuchElementException nse) {
    throw new PageOperationException(""String_Node_Str"",nse);
  }
  return FactorySharePage.resolvePage(drone);
}",0.9829351535836176
89280,"/** 
 * Checks if File plan link is displayed.
 * @return <code>true</code> if displayed <code>false</code> otherwise
 */
public boolean isFilePlanDisplayed(){
  try {
    WebElement filePlanButton=find(FILE_PLAN);
    String label=filePlanButton.getText();
    String filePlan=""String_Node_Str"";
    if (StringUtils.isNotBlank(label) && filePlan.equalsIgnoreCase(label.trim())) {
      return true;
    }
  }
 catch (  NoSuchElementException e) {
  }
  return false;
}","/** 
 * Checks if File plan link is displayed.
 * @return <code>true</code> if displayed <code>false</code> otherwise
 */
public boolean isFilePlanDisplayed(){
  try {
    WebElement filePlanButton=drone.find(FILE_PLAN);
    String label=filePlanButton.getText();
    String filePlan=""String_Node_Str"";
    if (StringUtils.isNotBlank(label) && filePlan.equalsIgnoreCase(label.trim())) {
      return true;
    }
  }
 catch (  NoSuchElementException e) {
  }
  return false;
}",0.9936440677966102
89281,"/** 
 * Constructor
 * @param drone                 web drone
 * @param fileDirectoryInfo     file directory information 
 */
public RecordInfo(WebDrone drone,FileDirectoryInfo fileDirectoryInfo){
  this.drone=drone;
  this.fileDirectoryInfo=fileDirectoryInfo;
}","/** 
 * Constructor
 * @param drone                 web drone
 * @param fileDirectoryInfo     file directory information
 */
public RecordInfo(WebDrone drone,FileDirectoryInfo fileDirectoryInfo){
  this.drone=drone;
  this.fileDirectoryInfo=fileDirectoryInfo;
}",0.9980879541108988
89282,"/** 
 * Helper method to indicate whether an action is visible or not.
 * @param action    action selector
 * @return boolean  true if visible, false otherwise
 */
private boolean isVisibleAction(By action){
  fileDirectoryInfo.selectMoreAction().click();
  try {
    WebElement link=((FileDirectoryInfoImpl)fileDirectoryInfo).find(action);
    return link.isDisplayed();
  }
 catch (  NoSuchElementException e) {
    return false;
  }
catch (  TimeoutException e) {
    return false;
  }
}","/** 
 * Helper method to indicate whether an action is visible or not.
 * @param action    action selector
 * @return boolean  true if visible, false otherwise
 */
private boolean isVisibleAction(By action){
  fileDirectoryInfo.selectMoreAction().click();
  try {
    WebElement link=drone.find(action);
    return link.isDisplayed();
  }
 catch (  NoSuchElementException e) {
    return false;
  }
catch (  TimeoutException e) {
    return false;
  }
}",0.9501590668080594
89283,"/** 
 * Action renders to RM site
 */
public FilePlanPage openRmSite(){
  drone.navigateTo(shareUrl + ""String_Node_Str"");
  FilePlanPage filePlanPage=(FilePlanPage)rmSiteDashBoard.selectFilePlan();
  return new FilePlanPage(drone).render();
}","/** 
 * Action renders to RM site
 */
public FilePlanPage openRmSite(){
  drone.navigateTo(shareUrl + ""String_Node_Str"");
  return (FilePlanPage)rmSiteDashBoard.selectFilePlan();
}",0.8341232227488151
89284,"/** 
 * Helper method to convert a stream to a string.
 * @param is    input stream
 * @return {@link String}   string
 * @throws IOException
 */
public String convertStreamToString(InputStream is) throws IOException {
  BufferedReader reader=new BufferedReader(new InputStreamReader(is));
  StringBuilder sb=new StringBuilder();
  String line=null;
  try {
    while ((line=reader.readLine()) != null) {
      sb.append(line + ""String_Node_Str"");
    }
  }
  finally {
    try {
      is.close();
    }
 catch (    IOException e) {
    }
  }
  return sb.toString();
}","/** 
 * Helper method to convert a stream to a string.
 * @param is    input stream
 * @return {@link String}   string
 * @throws IOException
 */
public String convertStreamToString(InputStream is) throws IOException {
  BufferedReader reader=new BufferedReader(new InputStreamReader(is,Charset.forName(""String_Node_Str"")));
  StringBuilder sb=new StringBuilder();
  String line=null;
  try {
    while ((line=reader.readLine()) != null) {
      sb.append(line + ""String_Node_Str"");
    }
  }
  finally {
    try {
      is.close();
    }
 catch (    IOException e) {
    }
  }
  return sb.toString();
}",0.9701110162254484
89285,"/** 
 * Generates a File containing the JSON representation of a transfer report.
 * @param transferNode The transfer node
 * @return File containing JSON representation of a transfer report
 * @throws IOException
 */
File generateJSONTransferReport(NodeRef transferNode) throws IOException {
  File report=TempFileProvider.createTempFile(REPORT_FILE_PREFIX,REPORT_FILE_SUFFIX);
  Writer writer=null;
  try {
    NodeRef[] itemsToTransfer=getTransferNodes(transferNode);
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + itemsToTransfer.length + ""String_Node_Str""+ report.getAbsolutePath());
    }
    writer=new FileWriter(report);
    String dispositionAuthority=null;
    if (itemsToTransfer.length > 0) {
      DispositionSchedule ds=dispositionService.getDispositionSchedule(itemsToTransfer[0]);
      if (ds != null) {
        dispositionAuthority=ds.getDispositionAuthority();
      }
    }
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(ISO8601DateFormat.format((Date)this.nodeService.getProperty(transferNode,ContentModel.PROP_CREATED)));
    writer.write(""String_Node_Str"");
    writer.write(AuthenticationUtil.getRunAsUser());
    writer.write(""String_Node_Str"");
    writer.write(dispositionAuthority != null ? dispositionAuthority : ""String_Node_Str"");
    writer.write(""String_Node_Str"");
    generateTransferItemsJSON(writer,itemsToTransfer);
    writer.write(""String_Node_Str"");
  }
  finally {
    if (writer != null) {
      try {
        writer.close();
      }
 catch (      IOException ioe) {
      }
    }
  }
  return report;
}","/** 
 * Generates a File containing the JSON representation of a transfer report.
 * @param transferNode The transfer node
 * @return File containing JSON representation of a transfer report
 * @throws IOException
 */
File generateJSONTransferReport(NodeRef transferNode) throws IOException {
  File report=TempFileProvider.createTempFile(REPORT_FILE_PREFIX,REPORT_FILE_SUFFIX);
  Writer writer=null;
  try {
    NodeRef[] itemsToTransfer=getTransferNodes(transferNode);
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + itemsToTransfer.length + ""String_Node_Str""+ report.getAbsolutePath());
    }
    writer=new OutputStreamWriter(new FileOutputStream(report),Charset.forName(""String_Node_Str""));
    String dispositionAuthority=null;
    if (itemsToTransfer.length > 0) {
      DispositionSchedule ds=dispositionService.getDispositionSchedule(itemsToTransfer[0]);
      if (ds != null) {
        dispositionAuthority=ds.getDispositionAuthority();
      }
    }
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(ISO8601DateFormat.format((Date)this.nodeService.getProperty(transferNode,ContentModel.PROP_CREATED)));
    writer.write(""String_Node_Str"");
    writer.write(AuthenticationUtil.getRunAsUser());
    writer.write(""String_Node_Str"");
    writer.write(dispositionAuthority != null ? dispositionAuthority : ""String_Node_Str"");
    writer.write(""String_Node_Str"");
    generateTransferItemsJSON(writer,itemsToTransfer);
    writer.write(""String_Node_Str"");
  }
  finally {
    if (writer != null) {
      try {
        writer.close();
      }
 catch (      IOException ioe) {
      }
    }
  }
  return report;
}",0.9724325961829748
89286,"/** 
 * Generates a File containing the JSON representation of a transfer report.
 * @param transferNode The transfer node
 * @return File containing JSON representation of a transfer report
 * @throws IOException
 */
File generateHTMLTransferReport(NodeRef transferNode) throws IOException {
  File report=TempFileProvider.createTempFile(REPORT_FILE_PREFIX,REPORT_FILE_SUFFIX);
  Writer writer=null;
  try {
    NodeRef[] itemsToTransfer=getTransferNodes(transferNode);
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + itemsToTransfer.length + ""String_Node_Str""+ report.getAbsolutePath());
    }
    writer=new FileWriter(report);
    String dispositionAuthority=null;
    if (itemsToTransfer.length > 0) {
      DispositionSchedule ds=dispositionService.getDispositionSchedule(itemsToTransfer[0]);
      if (ds != null) {
        dispositionAuthority=ds.getDispositionAuthority();
      }
    }
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    Boolean isAccession=(Boolean)this.nodeService.getProperty(transferNode,PROP_TRANSFER_ACCESSION_INDICATOR);
    if (isAccession) {
      writer.write(""String_Node_Str"");
    }
 else {
      writer.write(""String_Node_Str"");
    }
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    if (isAccession) {
      writer.write(""String_Node_Str"");
    }
 else {
      writer.write(""String_Node_Str"");
    }
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    Date transferDate=(Date)this.nodeService.getProperty(transferNode,ContentModel.PROP_CREATED);
    writer.write(StringEscapeUtils.escapeHtml(transferDate.toString()));
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    if (isAccession) {
      writer.write(""String_Node_Str"");
    }
 else {
      writer.write(StringEscapeUtils.escapeHtml((String)this.nodeService.getProperty(transferNode,RecordsManagementModel.PROP_TRANSFER_LOCATION)));
    }
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(StringEscapeUtils.escapeHtml((String)this.nodeService.getProperty(transferNode,ContentModel.PROP_CREATOR)));
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(dispositionAuthority != null ? StringEscapeUtils.escapeHtml(dispositionAuthority) : ""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    generateTransferItemsHTML(writer,itemsToTransfer);
    writer.write(""String_Node_Str"");
  }
  finally {
    if (writer != null) {
      try {
        writer.close();
      }
 catch (      IOException ioe) {
      }
    }
  }
  return report;
}","/** 
 * Generates a File containing the JSON representation of a transfer report.
 * @param transferNode The transfer node
 * @return File containing JSON representation of a transfer report
 * @throws IOException
 */
File generateHTMLTransferReport(NodeRef transferNode) throws IOException {
  File report=TempFileProvider.createTempFile(REPORT_FILE_PREFIX,REPORT_FILE_SUFFIX);
  Writer writer=null;
  try {
    NodeRef[] itemsToTransfer=getTransferNodes(transferNode);
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + itemsToTransfer.length + ""String_Node_Str""+ report.getAbsolutePath());
    }
    writer=new OutputStreamWriter(new FileOutputStream(report),Charset.forName(""String_Node_Str""));
    String dispositionAuthority=null;
    if (itemsToTransfer.length > 0) {
      DispositionSchedule ds=dispositionService.getDispositionSchedule(itemsToTransfer[0]);
      if (ds != null) {
        dispositionAuthority=ds.getDispositionAuthority();
      }
    }
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    Boolean isAccession=(Boolean)this.nodeService.getProperty(transferNode,PROP_TRANSFER_ACCESSION_INDICATOR);
    if (isAccession) {
      writer.write(""String_Node_Str"");
    }
 else {
      writer.write(""String_Node_Str"");
    }
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    if (isAccession) {
      writer.write(""String_Node_Str"");
    }
 else {
      writer.write(""String_Node_Str"");
    }
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    Date transferDate=(Date)this.nodeService.getProperty(transferNode,ContentModel.PROP_CREATED);
    writer.write(StringEscapeUtils.escapeHtml(transferDate.toString()));
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    if (isAccession) {
      writer.write(""String_Node_Str"");
    }
 else {
      writer.write(StringEscapeUtils.escapeHtml((String)this.nodeService.getProperty(transferNode,RecordsManagementModel.PROP_TRANSFER_LOCATION)));
    }
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(StringEscapeUtils.escapeHtml((String)this.nodeService.getProperty(transferNode,ContentModel.PROP_CREATOR)));
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(dispositionAuthority != null ? StringEscapeUtils.escapeHtml(dispositionAuthority) : ""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    generateTransferItemsHTML(writer,itemsToTransfer);
    writer.write(""String_Node_Str"");
  }
  finally {
    if (writer != null) {
      try {
        writer.close();
      }
 catch (      IOException ioe) {
      }
    }
  }
  return report;
}",0.984536958368734
89287,"/** 
 * Indicates whether the given node reference is a hold or not. <p> Exposed publicly in the   {@link HoldService}
 * @param nodeRef   node reference
 * @return boolean  true if rma:hold or sub-type, false otherwise
 */
public boolean isHold(NodeRef nodeRef){
  ParameterCheck.mandatory(""String_Node_Str"",nodeRef);
  if (nodeService.exists(nodeRef) && instanceOf(nodeRef,TYPE_HOLD)) {
    return true;
  }
 else {
    return false;
  }
}","/** 
 * Indicates whether the given node reference is a hold or not. <p> Exposed publicly in the   {@link HoldService}
 * @param nodeRef   node reference
 * @return boolean  true if rma:hold or sub-type, false otherwise
 */
public boolean isHold(NodeRef nodeRef){
  ParameterCheck.mandatory(""String_Node_Str"",nodeRef);
  boolean isHold=false;
  if (nodeService.exists(nodeRef) && instanceOf(nodeRef,TYPE_HOLD)) {
    isHold=true;
  }
  return isHold;
}",0.9115341545352744
89288,"/** 
 * This method converts a Map of String, Serializable to a Map of QName, Serializable. To do this, it assumes that each parameter name is a String representing a qname of the form prefix:localName.
 */
private Map<QName,Serializable> getPropertyValues(Action action){
  Map<String,Serializable> paramValues=action.getParameterValues();
  Map<QName,Serializable> result=new HashMap<QName,Serializable>(paramValues.size());
  for (  String paramName : paramValues.keySet()) {
    QName propQName=QName.createQName(paramName,this.namespaceService);
    result.put(propQName,paramValues.get(paramName));
  }
  return result;
}","/** 
 * This method converts a Map of String, Serializable to a Map of QName, Serializable. To do this, it assumes that each parameter name is a String representing a qname of the form prefix:localName.
 */
private Map<QName,Serializable> getPropertyValues(Action action){
  Map<String,Serializable> paramValues=action.getParameterValues();
  Map<QName,Serializable> result=new HashMap<QName,Serializable>(paramValues.size());
  for (  Map.Entry<String,Serializable> entry : paramValues.entrySet()) {
    QName propQName=QName.createQName(entry.getKey(),this.namespaceService);
    result.put(propQName,entry.getValue());
  }
  return result;
}",0.912667191188041
89289,"@Override protected synchronized List<ParameterDefinition> getParameterDefintions(){
  if (this.parameterDefinitions == null) {
    AspectDefinition aspectDefinition=dictionaryService.getAspect(customTypeAspect);
    if (aspectDefinition == null) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_CUSTOM_ASPECT_NOT_RECOGNISED,customTypeAspect));
    }
    Map<QName,PropertyDefinition> props=aspectDefinition.getProperties();
    this.parameterDefinitions=new ArrayList<ParameterDefinition>(props.size());
    for (    QName qn : props.keySet()) {
      String paramName=qn.toPrefixString(namespaceService);
      QName paramType=props.get(qn).getDataType().getName();
      boolean paramIsMandatory=props.get(qn).isMandatory();
      parameterDefinitions.add(new ParameterDefinitionImpl(paramName,paramType,paramIsMandatory,null));
    }
  }
  return parameterDefinitions;
}","@Override protected synchronized List<ParameterDefinition> getParameterDefintions(){
  if (this.parameterDefinitions == null) {
    AspectDefinition aspectDefinition=dictionaryService.getAspect(customTypeAspect);
    if (aspectDefinition == null) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_CUSTOM_ASPECT_NOT_RECOGNISED,customTypeAspect));
    }
    Map<QName,PropertyDefinition> props=aspectDefinition.getProperties();
    this.parameterDefinitions=new ArrayList<ParameterDefinition>(props.size());
    for (    Map.Entry<QName,PropertyDefinition> entry : props.entrySet()) {
      String paramName=entry.getKey().toPrefixString(namespaceService);
      PropertyDefinition value=entry.getValue();
      QName paramType=value.getDataType().getName();
      boolean paramIsMandatory=value.isMandatory();
      parameterDefinitions.add(new ParameterDefinitionImpl(paramName,paramType,paramIsMandatory,null));
    }
  }
  return parameterDefinitions;
}",0.9216738197424892
89290,"static Map<String,List<String>> getPivot(Map<String,List<String>> source){
  Map<String,List<String>> pivot=new HashMap<String,List<String>>();
  for (  String authority : source.keySet()) {
    List<String> values=source.get(authority);
    for (    String value : values) {
      if (pivot.containsKey(value)) {
        List<String> list=pivot.get(value);
        list.add(authority);
      }
 else {
        List<String> list=new ArrayList<String>();
        list.add(authority);
        pivot.put(value,list);
      }
    }
  }
  return pivot;
}","static Map<String,List<String>> getPivot(Map<String,List<String>> source){
  Map<String,List<String>> pivot=new HashMap<String,List<String>>();
  for (  Map.Entry<String,List<String>> entry : source.entrySet()) {
    List<String> values=entry.getValue();
    for (    String value : values) {
      String authority=entry.getKey();
      if (pivot.containsKey(value)) {
        List<String> list=pivot.get(value);
        list.add(authority);
      }
 else {
        List<String> list=new ArrayList<String>();
        list.add(authority);
        pivot.put(value,list);
      }
    }
  }
  return pivot;
}",0.7694974003466204
89291,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.email.CustomEmailMappingService#getCustomMappings()
 */
public Set<CustomMapping> getCustomMappings(){
  if (customMappings == null) {
    if (nodeService.exists(CONFIG_NODE_REF)) {
      customMappings=loadConfig();
    }
 else {
      customMappings=new HashSet<CustomMapping>();
      Map<String,Set<QName>> currentMapping=extracter.getCurrentMapping();
      for (      String key : currentMapping.keySet()) {
        Set<QName> set=currentMapping.get(key);
        for (        QName qname : set) {
          CustomMapping value=new CustomMapping();
          value.setFrom(key);
          QName resolvedQname=qname.getPrefixedQName(nspr);
          value.setTo(resolvedQname.toPrefixString());
          customMappings.add(value);
        }
      }
      NodeRef oldConfigNode=getOldConfigNode();
      if (oldConfigNode != null) {
        Set<CustomMapping> oldMappings=readOldConfig(oldConfigNode);
        customMappings.addAll(oldMappings);
      }
      for (      CustomMapping mapping : DEFAULT_MAPPINGS) {
        customMappings.add(mapping);
      }
      saveConfig(customMappings);
    }
  }
  return customMappings;
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.email.CustomEmailMappingService#getCustomMappings()
 */
public Set<CustomMapping> getCustomMappings(){
  if (customMappings == null) {
    if (nodeService.exists(CONFIG_NODE_REF)) {
      customMappings=loadConfig();
    }
 else {
      customMappings=new HashSet<CustomMapping>();
      Map<String,Set<QName>> currentMapping=extracter.getCurrentMapping();
      for (      Map.Entry<String,Set<QName>> entry : currentMapping.entrySet()) {
        Set<QName> set=entry.getValue();
        for (        QName qname : set) {
          CustomMapping value=new CustomMapping();
          value.setFrom(entry.getKey());
          QName resolvedQname=qname.getPrefixedQName(nspr);
          value.setTo(resolvedQname.toPrefixString());
          customMappings.add(value);
        }
      }
      NodeRef oldConfigNode=getOldConfigNode();
      if (oldConfigNode != null) {
        Set<CustomMapping> oldMappings=readOldConfig(oldConfigNode);
        customMappings.addAll(oldMappings);
      }
      for (      CustomMapping mapping : DEFAULT_MAPPINGS) {
        customMappings.add(mapping);
      }
      saveConfig(customMappings);
    }
  }
  return customMappings;
}",0.9643449419568824
89292,"/** 
 * This method compares the oldProps map against the newProps map and returns a set of QNames of the properties that have changed. Changed here means one of <ul> <li>the property has been removed</li> <li>the property has had its value changed</li> <li>the property has been added</li> </ul>
 */
private Set<QName> determineChangedProps(Map<QName,Serializable> oldProps,Map<QName,Serializable> newProps){
  Set<QName> result=new HashSet<QName>();
  for (  QName qn : oldProps.keySet()) {
    if (newProps.get(qn) == null || !newProps.get(qn).equals(oldProps.get(qn))) {
      result.add(qn);
    }
  }
  for (  QName qn : newProps.keySet()) {
    if (oldProps.get(qn) == null) {
      result.add(qn);
    }
  }
  return result;
}","/** 
 * This method compares the oldProps map against the newProps map and returns a set of QNames of the properties that have changed. Changed here means one of <ul> <li>the property has been removed</li> <li>the property has had its value changed</li> <li>the property has been added</li> </ul>
 */
private Set<QName> determineChangedProps(Map<QName,Serializable> oldProps,Map<QName,Serializable> newProps){
  Set<QName> result=new HashSet<QName>();
  for (  Map.Entry<QName,Serializable> entry : oldProps.entrySet()) {
    QName qn=entry.getKey();
    if (newProps.get(qn) == null || !newProps.get(qn).equals(entry.getValue())) {
      result.add(qn);
    }
  }
  for (  QName qn : newProps.keySet()) {
    if (oldProps.get(qn) == null) {
      result.add(qn);
    }
  }
  return result;
}",0.91218872870249
89293,"/** 
 * @see org.alfresco.repo.node.NodeServicePolicies.OnUpdatePropertiesPolicy#onUpdateProperties(org.alfresco.service.cmr.repository.NodeRef,java.util.Map,java.util.Map)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,isService=true,name=""String_Node_Str"") public void onUpdateProperties(NodeRef nodeRef,Map<QName,Serializable> before,Map<QName,Serializable> after){
  if (enabled && AuthenticationUtil.getFullyAuthenticatedUser() != null && !AuthenticationUtil.isRunAsUserTheSystemUser() && nodeService.exists(nodeRef)) {
    for (    QName property : after.keySet()) {
      if (isProtectedProperty(property)) {
        if (before == null || before.isEmpty() || before.get(property) == null) {
          return;
        }
        if (!EqualsHelper.nullSafeEquals(before.get(property),after.get(property)) && !canEditProtectedProperty(nodeRef,property)) {
          throw new ModelAccessDeniedException(""String_Node_Str"" + AuthenticationUtil.getFullyAuthenticatedUser() + ""String_Node_Str""+ property.toPrefixString(namespaceService)+ ""String_Node_Str""+ nodeRef.toString());
        }
      }
    }
  }
}","/** 
 * @see org.alfresco.repo.node.NodeServicePolicies.OnUpdatePropertiesPolicy#onUpdateProperties(org.alfresco.service.cmr.repository.NodeRef,java.util.Map,java.util.Map)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,isService=true,name=""String_Node_Str"") public void onUpdateProperties(NodeRef nodeRef,Map<QName,Serializable> before,Map<QName,Serializable> after){
  if (enabled && AuthenticationUtil.getFullyAuthenticatedUser() != null && !AuthenticationUtil.isRunAsUserTheSystemUser() && nodeService.exists(nodeRef)) {
    for (    Map.Entry<QName,Serializable> entry : after.entrySet()) {
      QName property=entry.getKey();
      if (isProtectedProperty(property)) {
        if (before == null || before.isEmpty() || before.get(property) == null) {
          return;
        }
        if (!EqualsHelper.nullSafeEquals(before.get(property),entry.getValue()) && !canEditProtectedProperty(nodeRef,property)) {
          throw new ModelAccessDeniedException(""String_Node_Str"" + AuthenticationUtil.getFullyAuthenticatedUser() + ""String_Node_Str""+ property.toPrefixString(namespaceService)+ ""String_Node_Str""+ nodeRef.toString());
        }
      }
    }
  }
}",0.9503296703296704
89294,"/** 
 * Ensure that the user only updates record properties that they have permission to.
 * @see org.alfresco.repo.node.NodeServicePolicies.OnUpdatePropertiesPolicy#onUpdateProperties(org.alfresco.service.cmr.repository.NodeRef,java.util.Map,java.util.Map)
 */
@Override @Behaviour(name=""String_Node_Str"",kind=BehaviourKind.CLASS,type=""String_Node_Str"") public void onUpdateProperties(final NodeRef nodeRef,final Map<QName,Serializable> before,final Map<QName,Serializable> after){
  if (AuthenticationUtil.getFullyAuthenticatedUser() != null && !AuthenticationUtil.isRunAsUserTheSystemUser() && nodeService.exists(nodeRef) && isRecord(nodeRef)) {
    for (    QName property : after.keySet()) {
      Serializable beforeValue=null;
      if (before != null) {
        beforeValue=before.get(property);
      }
      Serializable afterValue=null;
      if (after != null) {
        afterValue=after.get(property);
      }
      boolean propertyUnchanged=false;
      if (beforeValue instanceof Date && afterValue instanceof Date) {
        propertyUnchanged=(((Date)beforeValue).compareTo((Date)afterValue) == 0);
      }
 else {
        propertyUnchanged=EqualsHelper.nullSafeEquals(beforeValue,afterValue);
      }
      if (!propertyUnchanged && !isPropertyEditable(nodeRef,property)) {
        throw new ModelAccessDeniedException(""String_Node_Str"" + AuthenticationUtil.getFullyAuthenticatedUser() + ""String_Node_Str""+ property.toString()+ ""String_Node_Str""+ nodeRef.toString());
      }
    }
  }
}","/** 
 * Ensure that the user only updates record properties that they have permission to.
 * @see org.alfresco.repo.node.NodeServicePolicies.OnUpdatePropertiesPolicy#onUpdateProperties(org.alfresco.service.cmr.repository.NodeRef,java.util.Map,java.util.Map)
 */
@Override @Behaviour(name=""String_Node_Str"",kind=BehaviourKind.CLASS,type=""String_Node_Str"") public void onUpdateProperties(final NodeRef nodeRef,final Map<QName,Serializable> before,final Map<QName,Serializable> after){
  if (AuthenticationUtil.getFullyAuthenticatedUser() != null && !AuthenticationUtil.isRunAsUserTheSystemUser() && nodeService.exists(nodeRef) && isRecord(nodeRef)) {
    for (    Map.Entry<QName,Serializable> entry : after.entrySet()) {
      Serializable beforeValue=null;
      QName property=entry.getKey();
      if (before != null) {
        beforeValue=before.get(property);
      }
      Serializable afterValue=entry.getValue();
      boolean propertyUnchanged=false;
      if (beforeValue instanceof Date && afterValue instanceof Date) {
        propertyUnchanged=(((Date)beforeValue).compareTo((Date)afterValue) == 0);
      }
 else {
        propertyUnchanged=EqualsHelper.nullSafeEquals(beforeValue,afterValue);
      }
      if (!propertyUnchanged && !isPropertyEditable(nodeRef,property)) {
        throw new ModelAccessDeniedException(""String_Node_Str"" + AuthenticationUtil.getFullyAuthenticatedUser() + ""String_Node_Str""+ property.toString()+ ""String_Node_Str""+ nodeRef.toString());
      }
    }
  }
}",0.9417637271214644
89295,"@SuppressWarnings(""String_Node_Str"") @Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  String reqContentAsString;
  try {
    reqContentAsString=req.getContent().getContent();
  }
 catch (  IOException iox) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",iox);
  }
  String actionName=null;
  List<NodeRef> targetNodeRefs=new ArrayList<NodeRef>(1);
  Map<String,Serializable> actionParams=new HashMap<String,Serializable>(3);
  try {
    JSONObject jsonObj=new JSONObject(new JSONTokener(reqContentAsString));
    if (jsonObj.has(PARAM_NAME)) {
      actionName=jsonObj.getString(PARAM_NAME);
    }
    if (jsonObj.has(PARAM_NODE_REF)) {
      NodeRef nodeRef=new NodeRef(jsonObj.getString(PARAM_NODE_REF));
      targetNodeRefs.add(nodeRef);
    }
    if (jsonObj.has(PARAM_NODE_REFS)) {
      JSONArray jsonArray=jsonObj.getJSONArray(PARAM_NODE_REFS);
      if (jsonArray.length() != 0) {
        targetNodeRefs=new ArrayList<NodeRef>(jsonArray.length());
        for (int i=0; i < jsonArray.length(); i++) {
          NodeRef nodeRef=new NodeRef(jsonArray.getString(i));
          targetNodeRefs.add(nodeRef);
        }
      }
    }
    if (jsonObj.has(PARAM_PARAMS)) {
      JSONObject paramsObj=jsonObj.getJSONObject(PARAM_PARAMS);
      for (Iterator<String> iter=paramsObj.keys(); iter.hasNext(); ) {
        String nextKeyString=iter.next();
        Object nextValue=paramsObj.get(nextKeyString);
        if ((nextValue instanceof JSONObject) && ((JSONObject)nextValue).has(""String_Node_Str"")) {
          String dateStringValue=((JSONObject)nextValue).getString(""String_Node_Str"");
          nextValue=ISO8601DateFormat.parse(dateStringValue);
        }
        actionParams.put(nextKeyString,(Serializable)nextValue);
      }
    }
  }
 catch (  JSONException exception) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",exception);
  }
  if (actionName == null) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"");
  }
  StringBuffer targetNodeRefsString=new StringBuffer(30);
  boolean firstTime=true;
  for (  NodeRef targetNodeRef : targetNodeRefs) {
    if (!nodeService.exists(targetNodeRef)) {
      throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + targetNodeRef.toString() + ""String_Node_Str"");
    }
    if (firstTime) {
      firstTime=false;
    }
 else {
      targetNodeRefsString.append(""String_Node_Str"");
    }
    targetNodeRefsString.append(targetNodeRef.toString());
  }
  if (logger.isDebugEnabled()) {
    StringBuilder msg=new StringBuilder();
    msg.append(""String_Node_Str"").append(actionName).append(""String_Node_Str"").append(targetNodeRefsString.toString()).append(""String_Node_Str"").append(actionParams);
    logger.debug(msg.toString());
  }
  Map<String,Object> model=new HashMap<String,Object>();
  if (targetNodeRefs.isEmpty()) {
    RecordsManagementActionResult result=this.rmActionService.executeRecordsManagementAction(actionName,actionParams);
    if (result.getValue() != null) {
      model.put(""String_Node_Str"",result.getValue().toString());
    }
  }
 else {
    Map<NodeRef,RecordsManagementActionResult> resultMap=this.rmActionService.executeRecordsManagementAction(targetNodeRefs,actionName,actionParams);
    Map<String,String> results=new HashMap<String,String>(resultMap.size());
    for (    NodeRef nodeRef : resultMap.keySet()) {
      Object value=resultMap.get(nodeRef).getValue();
      if (value != null) {
        results.put(nodeRef.toString(),resultMap.get(nodeRef).getValue().toString());
      }
    }
    model.put(""String_Node_Str"",results);
  }
  model.put(""String_Node_Str"",""String_Node_Str"" + actionName + ""String_Node_Str""+ targetNodeRefsString.toString());
  return model;
}","@SuppressWarnings(""String_Node_Str"") @Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  String reqContentAsString;
  try {
    reqContentAsString=req.getContent().getContent();
  }
 catch (  IOException iox) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",iox);
  }
  String actionName=null;
  List<NodeRef> targetNodeRefs=new ArrayList<NodeRef>(1);
  Map<String,Serializable> actionParams=new HashMap<String,Serializable>(3);
  try {
    JSONObject jsonObj=new JSONObject(new JSONTokener(reqContentAsString));
    if (jsonObj.has(PARAM_NAME)) {
      actionName=jsonObj.getString(PARAM_NAME);
    }
    if (jsonObj.has(PARAM_NODE_REF)) {
      NodeRef nodeRef=new NodeRef(jsonObj.getString(PARAM_NODE_REF));
      targetNodeRefs.add(nodeRef);
    }
    if (jsonObj.has(PARAM_NODE_REFS)) {
      JSONArray jsonArray=jsonObj.getJSONArray(PARAM_NODE_REFS);
      if (jsonArray.length() != 0) {
        targetNodeRefs=new ArrayList<NodeRef>(jsonArray.length());
        for (int i=0; i < jsonArray.length(); i++) {
          NodeRef nodeRef=new NodeRef(jsonArray.getString(i));
          targetNodeRefs.add(nodeRef);
        }
      }
    }
    if (jsonObj.has(PARAM_PARAMS)) {
      JSONObject paramsObj=jsonObj.getJSONObject(PARAM_PARAMS);
      for (Iterator<String> iter=paramsObj.keys(); iter.hasNext(); ) {
        String nextKeyString=iter.next();
        Object nextValue=paramsObj.get(nextKeyString);
        if ((nextValue instanceof JSONObject) && ((JSONObject)nextValue).has(""String_Node_Str"")) {
          String dateStringValue=((JSONObject)nextValue).getString(""String_Node_Str"");
          nextValue=ISO8601DateFormat.parse(dateStringValue);
        }
        actionParams.put(nextKeyString,(Serializable)nextValue);
      }
    }
  }
 catch (  JSONException exception) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",exception);
  }
  if (actionName == null) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"");
  }
  StringBuffer targetNodeRefsString=new StringBuffer(30);
  boolean firstTime=true;
  for (  NodeRef targetNodeRef : targetNodeRefs) {
    if (!nodeService.exists(targetNodeRef)) {
      throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + targetNodeRef.toString() + ""String_Node_Str"");
    }
    if (firstTime) {
      firstTime=false;
    }
 else {
      targetNodeRefsString.append(""String_Node_Str"");
    }
    targetNodeRefsString.append(targetNodeRef.toString());
  }
  if (logger.isDebugEnabled()) {
    StringBuilder msg=new StringBuilder();
    msg.append(""String_Node_Str"").append(actionName).append(""String_Node_Str"").append(targetNodeRefsString.toString()).append(""String_Node_Str"").append(actionParams);
    logger.debug(msg.toString());
  }
  Map<String,Object> model=new HashMap<String,Object>();
  if (targetNodeRefs.isEmpty()) {
    RecordsManagementActionResult result=this.rmActionService.executeRecordsManagementAction(actionName,actionParams);
    if (result.getValue() != null) {
      model.put(""String_Node_Str"",result.getValue().toString());
    }
  }
 else {
    Map<NodeRef,RecordsManagementActionResult> resultMap=this.rmActionService.executeRecordsManagementAction(targetNodeRefs,actionName,actionParams);
    Map<String,String> results=new HashMap<String,String>(resultMap.size());
    for (    Map.Entry<NodeRef,RecordsManagementActionResult> entry : resultMap.entrySet()) {
      Object value=entry.getValue().getValue();
      if (value != null) {
        results.put(entry.getKey().toString(),value.toString());
      }
    }
    model.put(""String_Node_Str"",results);
  }
  model.put(""String_Node_Str"",""String_Node_Str"" + actionName + ""String_Node_Str""+ targetNodeRefsString.toString());
  return model;
}",0.9777486910994764
89296,"/** 
 * Process the supplied map of property definitions and add the ones that match the supplied fragment to the list of suggestions.
 * @param definition  Definition (aspect or type) to get properties of and the call this method for associated aspects
 * @param substitutionFragment  Substitution fragment to search for
 * @param suggestions  The current list of suggestions to which we will add newly found suggestions
 */
private boolean processPropertyDefinitions(Map<QName,PropertyDefinition> properties,String substitutionFragment,Set<String> suggestions){
  boolean gotMaximumSuggestions=false;
  if (properties != null) {
    for (    QName key : properties.keySet()) {
      PropertyDefinition propertyDefinition=properties.get(key);
      QName type=propertyDefinition.getDataType().getName();
      if (ArrayUtils.contains(supportedDataTypes,type)) {
        String suggestion=getName() + ""String_Node_Str"" + key.getPrefixString();
        if (suggestion.toLowerCase().contains(substitutionFragment)) {
          if (suggestions.size() < this.maximumNumberSuggestions) {
            suggestions.add(suggestion);
          }
 else {
            gotMaximumSuggestions=true;
            break;
          }
        }
      }
    }
  }
  return gotMaximumSuggestions;
}","/** 
 * Process the supplied map of property definitions and add the ones that match the supplied fragment to the list of suggestions.
 * @param definition  Definition (aspect or type) to get properties of and the call this method for associated aspects
 * @param substitutionFragment  Substitution fragment to search for
 * @param suggestions  The current list of suggestions to which we will add newly found suggestions
 */
private boolean processPropertyDefinitions(Map<QName,PropertyDefinition> properties,String substitutionFragment,Set<String> suggestions){
  boolean gotMaximumSuggestions=false;
  if (properties != null) {
    for (    Map.Entry<QName,PropertyDefinition> entry : properties.entrySet()) {
      PropertyDefinition propertyDefinition=entry.getValue();
      QName type=propertyDefinition.getDataType().getName();
      if (ArrayUtils.contains(supportedDataTypes,type)) {
        String suggestion=getName() + ""String_Node_Str"" + entry.getKey().getPrefixString();
        if (suggestion.toLowerCase().contains(substitutionFragment)) {
          if (suggestions.size() < this.maximumNumberSuggestions) {
            suggestions.add(suggestion);
          }
 else {
            gotMaximumSuggestions=true;
            break;
          }
        }
      }
    }
  }
  return gotMaximumSuggestions;
}",0.9668465690053972
89297,"/** 
 * Helper method to remove system properties from maps
 * @param properties
 */
private void removeAuditProperties(List<String> ignoredAuditProperties,Map<QName,Serializable> before,Map<QName,Serializable> after){
  if (before != null) {
    before.keySet().removeAll(this.propertiesToBeRemoved);
  }
  if (after != null) {
    after.keySet().removeAll(this.propertiesToBeRemoved);
  }
}","/** 
 * Helper method to remove system properties from maps
 * @param properties
 */
private void removeAuditProperties(Map<QName,Serializable> before,Map<QName,Serializable> after){
  if (before != null) {
    before.keySet().removeAll(this.propertiesToBeRemoved);
  }
  if (after != null) {
    after.keySet().removeAll(this.propertiesToBeRemoved);
  }
}",0.9518716577540108
89298,"/** 
 * Helper method to build audit map
 * @param nodeRef
 * @param eventName
 * @return
 * @since 2.0.3
 */
private Map<String,Serializable> buildAuditMap(NodeRef nodeRef,String eventName,Map<QName,Serializable> propertiesBefore,Map<QName,Serializable> propertiesAfter,boolean removeOnNoPropertyChange){
  Map<String,Serializable> auditMap=new HashMap<String,Serializable>(13);
  auditMap.put(AuditApplication.buildPath(RM_AUDIT_SNIPPET_EVENT,RM_AUDIT_SNIPPET_NAME),eventName);
  if (nodeRef != null) {
    auditMap.put(AuditApplication.buildPath(RM_AUDIT_SNIPPET_EVENT,RM_AUDIT_SNIPPET_NODE),nodeRef);
  }
  if (!ignoredAuditProperties.isEmpty()) {
    removeAuditProperties(ignoredAuditProperties,propertiesBefore,propertiesAfter);
  }
  Pair<Map<QName,Serializable>,Map<QName,Serializable>> deltaPair=PropertyMap.getBeforeAndAfterMapsForChanges(propertiesBefore,propertiesAfter);
  if (deltaPair.getFirst().isEmpty() && deltaPair.getSecond().isEmpty() && removeOnNoPropertyChange) {
    auditMap.clear();
  }
 else {
    auditMap.put(AuditApplication.buildPath(RM_AUDIT_SNIPPET_EVENT,RM_AUDIT_SNIPPET_NODE,RM_AUDIT_SNIPPET_CHANGES,RM_AUDIT_SNIPPET_BEFORE),(Serializable)deltaPair.getFirst());
    auditMap.put(AuditApplication.buildPath(RM_AUDIT_SNIPPET_EVENT,RM_AUDIT_SNIPPET_NODE,RM_AUDIT_SNIPPET_CHANGES,RM_AUDIT_SNIPPET_AFTER),(Serializable)deltaPair.getSecond());
  }
  return auditMap;
}","/** 
 * Helper method to build audit map
 * @param nodeRef
 * @param eventName
 * @return
 * @since 2.0.3
 */
private Map<String,Serializable> buildAuditMap(NodeRef nodeRef,String eventName,Map<QName,Serializable> propertiesBefore,Map<QName,Serializable> propertiesAfter,boolean removeOnNoPropertyChange){
  Map<String,Serializable> auditMap=new HashMap<String,Serializable>(13);
  auditMap.put(AuditApplication.buildPath(RM_AUDIT_SNIPPET_EVENT,RM_AUDIT_SNIPPET_NAME),eventName);
  if (nodeRef != null) {
    auditMap.put(AuditApplication.buildPath(RM_AUDIT_SNIPPET_EVENT,RM_AUDIT_SNIPPET_NODE),nodeRef);
  }
  if (!ignoredAuditProperties.isEmpty()) {
    removeAuditProperties(propertiesBefore,propertiesAfter);
  }
  Pair<Map<QName,Serializable>,Map<QName,Serializable>> deltaPair=PropertyMap.getBeforeAndAfterMapsForChanges(propertiesBefore,propertiesAfter);
  if (deltaPair.getFirst().isEmpty() && deltaPair.getSecond().isEmpty() && removeOnNoPropertyChange) {
    auditMap.clear();
  }
 else {
    auditMap.put(AuditApplication.buildPath(RM_AUDIT_SNIPPET_EVENT,RM_AUDIT_SNIPPET_NODE,RM_AUDIT_SNIPPET_CHANGES,RM_AUDIT_SNIPPET_BEFORE),(Serializable)deltaPair.getFirst());
    auditMap.put(AuditApplication.buildPath(RM_AUDIT_SNIPPET_EVENT,RM_AUDIT_SNIPPET_NODE,RM_AUDIT_SNIPPET_CHANGES,RM_AUDIT_SNIPPET_AFTER),(Serializable)deltaPair.getSecond());
  }
  return auditMap;
}",0.991705733862243
89299,"/** 
 * @see org.alfresco.repo.jscript.app.JSONConversionComponent#setRootValues(org.alfresco.service.cmr.model.FileInfo,org.json.simple.JSONObject,boolean)
 */
@SuppressWarnings(""String_Node_Str"") @Override protected void setRootValues(FileInfo nodeInfo,JSONObject rootJSONObject,boolean useShortQNames){
  super.setRootValues(nodeInfo,rootJSONObject,useShortQNames);
  NodeRef nodeRef=nodeInfo.getNodeRef();
  if (AccessStatus.ALLOWED.equals(capabilityService.getCapabilityAccessState(nodeRef,RMPermissionModel.VIEW_RECORDS))) {
    boolean isFilePlanComponent=filePlanService.isFilePlanComponent(nodeInfo.getNodeRef());
    rootJSONObject.put(""String_Node_Str"",isFilePlanComponent);
    if (isFilePlanComponent) {
      rootJSONObject.put(""String_Node_Str"",setRmNodeValues(nodeRef,rootJSONObject,useShortQNames));
    }
  }
}","/** 
 * @see org.alfresco.repo.jscript.app.JSONConversionComponent#setRootValues(org.alfresco.service.cmr.model.FileInfo,org.json.simple.JSONObject,boolean)
 */
@SuppressWarnings(""String_Node_Str"") @Override protected void setRootValues(FileInfo nodeInfo,JSONObject rootJSONObject,boolean useShortQNames){
  super.setRootValues(nodeInfo,rootJSONObject,useShortQNames);
  NodeRef nodeRef=nodeInfo.getNodeRef();
  if (AccessStatus.ALLOWED.equals(capabilityService.getCapabilityAccessState(nodeRef,RMPermissionModel.VIEW_RECORDS))) {
    boolean isFilePlanComponent=filePlanService.isFilePlanComponent(nodeInfo.getNodeRef());
    rootJSONObject.put(""String_Node_Str"",isFilePlanComponent);
    if (isFilePlanComponent) {
      rootJSONObject.put(""String_Node_Str"",setRmNodeValues(nodeRef,useShortQNames));
    }
  }
}",0.9908592321755028
89300,"/** 
 * @param nodeRef
 * @param rootJSONObject
 * @param useShortQName
 * @return
 */
@SuppressWarnings(""String_Node_Str"") private JSONObject setRmNodeValues(NodeRef nodeRef,JSONObject rootJSONObject,boolean useShortQName){
  JSONObject rmNodeValues=new JSONObject();
  rmNodeValues.put(""String_Node_Str"",getUIType(nodeRef));
  FilePlanComponentKind kind=filePlanService.getFilePlanComponentKind(nodeRef);
  rmNodeValues.put(""String_Node_Str"",kind.toString());
  NodeRef filePlan=filePlanService.getFilePlan(nodeRef);
  rmNodeValues.put(""String_Node_Str"",filePlan.toString());
  NodeRef unfiledRecordContainer=filePlanService.getUnfiledContainer(filePlan);
  if (unfiledRecordContainer != null) {
    rmNodeValues.put(""String_Node_Str"",unfiledRecordContainer.toString());
    rmNodeValues.put(""String_Node_Str"",propertiesToJSON(unfiledRecordContainer,nodeService.getProperties(unfiledRecordContainer),useShortQName));
    QName type=fileFolderService.getFileInfo(unfiledRecordContainer).getType();
    rmNodeValues.put(""String_Node_Str"",useShortQName ? type.toPrefixString(namespaceService) : type.toString());
  }
  setIndicators(rmNodeValues,nodeRef);
  setActions(rmNodeValues,nodeRef);
  return rmNodeValues;
}","/** 
 * @param nodeRef
 * @param useShortQName
 * @return
 */
@SuppressWarnings(""String_Node_Str"") private JSONObject setRmNodeValues(NodeRef nodeRef,boolean useShortQName){
  JSONObject rmNodeValues=new JSONObject();
  rmNodeValues.put(""String_Node_Str"",getUIType(nodeRef));
  FilePlanComponentKind kind=filePlanService.getFilePlanComponentKind(nodeRef);
  rmNodeValues.put(""String_Node_Str"",kind.toString());
  NodeRef filePlan=filePlanService.getFilePlan(nodeRef);
  rmNodeValues.put(""String_Node_Str"",filePlan.toString());
  NodeRef unfiledRecordContainer=filePlanService.getUnfiledContainer(filePlan);
  if (unfiledRecordContainer != null) {
    rmNodeValues.put(""String_Node_Str"",unfiledRecordContainer.toString());
    rmNodeValues.put(""String_Node_Str"",propertiesToJSON(unfiledRecordContainer,nodeService.getProperties(unfiledRecordContainer),useShortQName));
    QName type=fileFolderService.getFileInfo(unfiledRecordContainer).getType();
    rmNodeValues.put(""String_Node_Str"",useShortQName ? type.toPrefixString(namespaceService) : type.toString());
  }
  setIndicators(rmNodeValues,nodeRef);
  setActions(rmNodeValues,nodeRef);
  return rmNodeValues;
}",0.9163514081546869
89301,"@Override protected void executePatch() throws Throwable {
  if (nodeService.exists(RM_CONFIG_FOLDER) && !nodeService.exists(TEMPLATE_ROOT)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    NodeRef templateRoot=createNode(ContentModel.TYPE_FOLDER,RM_CONFIG_FOLDER,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    NodeRef destructionTemplate=createNode(ContentModel.TYPE_CONTENT,templateRoot,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
    nodeService.addAspect(destructionTemplate,ContentModel.ASPECT_TITLED,null);
    nodeService.addAspect(destructionTemplate,ContentModel.ASPECT_AUTHOR,null);
    ContentWriter writer=contentService.getWriter(destructionTemplate,ContentModel.PROP_CONTENT,true);
    InputStream is=getClass().getClassLoader().getResourceAsStream(PATH_DESTRUCTION_TEMPLATE);
    writer.setEncoding(""String_Node_Str"");
    writer.setMimetype(MimetypeMap.MIMETYPE_TEXT_PLAIN);
    writer.putContent(is);
  }
}","@Override protected void executePatch() throws Throwable {
  if (nodeService.exists(RM_CONFIG_FOLDER) && !nodeService.exists(TEMPLATE_ROOT)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    NodeRef destructionTemplate=createNode(ContentModel.TYPE_CONTENT,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
    nodeService.addAspect(destructionTemplate,ContentModel.ASPECT_TITLED,null);
    nodeService.addAspect(destructionTemplate,ContentModel.ASPECT_AUTHOR,null);
    ContentWriter writer=contentService.getWriter(destructionTemplate,ContentModel.PROP_CONTENT,true);
    InputStream is=getClass().getClassLoader().getResourceAsStream(PATH_DESTRUCTION_TEMPLATE);
    writer.setEncoding(""String_Node_Str"");
    writer.setMimetype(MimetypeMap.MIMETYPE_TEXT_PLAIN);
    writer.putContent(is);
  }
}",0.8187470109995217
89302,"private NodeRef createNode(QName type,NodeRef parent,String id,String name,String assocName,String title,String description){
  Map<QName,Serializable> props=new HashMap<QName,Serializable>(4);
  props.put(ContentModel.PROP_DESCRIPTION,description);
  props.put(ContentModel.PROP_TITLE,title);
  props.put(ContentModel.PROP_NAME,name);
  props.put(ContentModel.PROP_NODE_UUID,id);
  QName assocQName=QName.createQName(NamespaceService.CONTENT_MODEL_1_0_URI,QName.createValidLocalName(assocName));
  return nodeService.createNode(RM_CONFIG_FOLDER,ContentModel.ASSOC_CONTAINS,assocQName,type,props).getChildRef();
}","private NodeRef createNode(QName type,String id,String name,String assocName,String title,String description){
  Map<QName,Serializable> props=new HashMap<QName,Serializable>(4);
  props.put(ContentModel.PROP_DESCRIPTION,description);
  props.put(ContentModel.PROP_TITLE,title);
  props.put(ContentModel.PROP_NAME,name);
  props.put(ContentModel.PROP_NODE_UUID,id);
  QName assocQName=QName.createQName(NamespaceService.CONTENT_MODEL_1_0_URI,QName.createValidLocalName(assocName));
  return nodeService.createNode(RM_CONFIG_FOLDER,ContentModel.ASSOC_CONTAINS,assocQName,type,props).getChildRef();
}",0.9876135425268372
89303,"private boolean isAuthorityNameMatching(Set<String> authorities,String authorityName,AuthorityType type){
  boolean isMatching=false;
  if (type == null || AuthorityType.getAuthorityType(authorityName).equals(type) && !getAuthorityZones(authorityName).contains(""String_Node_Str"")) {
    isMatching=true;
  }
  return isMatching;
}","private boolean isAuthorityNameMatching(String authorityName,AuthorityType type){
  boolean isMatching=false;
  if (type == null || AuthorityType.getAuthorityType(authorityName).equals(type) && !getAuthorityZones(authorityName).contains(""String_Node_Str"")) {
    isMatching=true;
  }
  return isMatching;
}",0.9622641509433962
89304,"protected void addAuthorityNameIfMatches(Set<String> authorities,String authorityName,AuthorityType type,Pattern pattern){
  if (isAuthorityNameMatching(authorities,authorityName,type)) {
    if (pattern == null) {
      authorities.add(authorityName);
    }
 else {
      if (pattern.matcher(getShortName(authorityName)).matches()) {
        authorities.add(authorityName);
      }
 else {
        String displayName=getAuthorityDisplayName(authorityName);
        if (displayName != null && pattern.matcher(displayName).matches()) {
          authorities.add(authorityName);
        }
      }
    }
  }
}","protected void addAuthorityNameIfMatches(Set<String> authorities,String authorityName,AuthorityType type,Pattern pattern){
  if (isAuthorityNameMatching(authorityName,type)) {
    if (pattern == null) {
      authorities.add(authorityName);
    }
 else {
      if (pattern.matcher(getShortName(authorityName)).matches()) {
        authorities.add(authorityName);
      }
 else {
        String displayName=getAuthorityDisplayName(authorityName);
        if (displayName != null && pattern.matcher(displayName).matches()) {
          authorities.add(authorityName);
        }
      }
    }
  }
}",0.99
89305,"/** 
 * Execute custom Java logic
 * @param req  Web Script request
 * @param status  Web Script status
 * @param cache  Web Script cache
 * @param isRM  indicates whether the request comes from an RM site or not
 * @return custom service model
 */
private Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache,boolean isRM){
  String classFilter=getValidInput(req.getParameter(REQ_URL_TEMPL_VAR_CLASS_FILTER));
  String namespacePrefix=getValidInput(req.getParameter(REQ_URL_TEMPL_VAR_NAMESPACE_PREFIX));
  String name=getValidInput(req.getParameter(REQ_URL_TEMPL_VAR_NAME));
  String className=null;
  Map<QName,ClassDefinition> classdef=new HashMap<QName,ClassDefinition>();
  Map<QName,Collection<PropertyDefinition>> propdef=new HashMap<QName,Collection<PropertyDefinition>>();
  Map<QName,Collection<AssociationDefinition>> assocdef=new HashMap<QName,Collection<AssociationDefinition>>();
  Map<String,Object> model=new HashMap<String,Object>();
  List<QName> qnames=new ArrayList<QName>();
  QName classQname=null;
  QName myModel=null;
  if (classFilter == null) {
    classFilter=""String_Node_Str"";
  }
  if (!isValidClassFilter(classFilter)) {
    throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + classFilter + ""String_Node_Str"");
  }
  if (namespacePrefix == null && name != null) {
    throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"");
  }
  if (namespacePrefix != null) {
    if (name != null) {
      className=namespacePrefix + ""String_Node_Str"" + name;
      if (!isValidClassname(className)) {
        throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + name + ""String_Node_Str"");
      }
      classQname=QName.createQName(getFullNamespaceURI(className));
      classdef.put(classQname,this.dictionaryservice.getClass(classQname));
      propdef.put(classQname,this.dictionaryservice.getClass(classQname).getProperties().values());
      assocdef.put(classQname,this.dictionaryservice.getClass(classQname).getAssociations().values());
    }
 else {
      String namespaceUri=namespaceService.getNamespaceURI(namespacePrefix);
      for (      QName qnameObj : this.dictionaryservice.getAllModels()) {
        if (qnameObj.getNamespaceURI().equals(namespaceUri)) {
          name=qnameObj.getLocalName();
          myModel=QName.createQName(getFullNamespaceURI(namespacePrefix + ""String_Node_Str"" + name));
          if (classFilter.equalsIgnoreCase(CLASS_FILTER_OPTION_TYPE1)) {
            qnames.addAll(this.dictionaryservice.getAspects(myModel));
            qnames.addAll(this.dictionaryservice.getTypes(myModel));
          }
 else           if (classFilter.equalsIgnoreCase(CLASS_FILTER_OPTION_TYPE3)) {
            qnames.addAll(this.dictionaryservice.getTypes(myModel));
          }
 else           if (classFilter.equalsIgnoreCase(CLASS_FILTER_OPTION_TYPE2)) {
            qnames.addAll(this.dictionaryservice.getAspects(myModel));
          }
        }
      }
    }
  }
  if (myModel == null) {
    if (classFilter.equalsIgnoreCase(CLASS_FILTER_OPTION_TYPE1)) {
      qnames.addAll(getAspects(isRM));
      qnames.addAll(getTypes(isRM));
    }
 else     if (classFilter.equalsIgnoreCase(CLASS_FILTER_OPTION_TYPE3)) {
      qnames.addAll(getTypes(isRM));
    }
 else     if (classFilter.equalsIgnoreCase(CLASS_FILTER_OPTION_TYPE2)) {
      qnames.addAll(getAspects(isRM));
    }
  }
  if (classdef.isEmpty()) {
    for (    QName qnameObj : qnames) {
      classdef.put(qnameObj,this.dictionaryservice.getClass(qnameObj));
      propdef.put(qnameObj,this.dictionaryservice.getClass(qnameObj).getProperties().values());
      assocdef.put(qnameObj,this.dictionaryservice.getClass(qnameObj).getAssociations().values());
    }
  }
  List<ClassDefinition> classDefinitions=new ArrayList<ClassDefinition>(classdef.values());
  Collections.sort(classDefinitions,new DictionaryComparators.ClassDefinitionComparator(dictionaryservice));
  model.put(MODEL_PROP_KEY_CLASS_DEFS,classDefinitions);
  model.put(MODEL_PROP_KEY_PROPERTY_DETAILS,propdef.values());
  model.put(MODEL_PROP_KEY_ASSOCIATION_DETAILS,assocdef.values());
  model.put(MODEL_PROP_KEY_MESSAGE_LOOKUP,dictionaryservice);
  return model;
}","/** 
 * Execute custom Java logic
 * @param req  Web Script request
 * @param isRM  indicates whether the request comes from an RM site or not
 * @return custom service model
 */
private Map<String,Object> executeImpl(WebScriptRequest req,boolean isRM){
  String classFilter=getValidInput(req.getParameter(REQ_URL_TEMPL_VAR_CLASS_FILTER));
  String namespacePrefix=getValidInput(req.getParameter(REQ_URL_TEMPL_VAR_NAMESPACE_PREFIX));
  String name=getValidInput(req.getParameter(REQ_URL_TEMPL_VAR_NAME));
  String className=null;
  Map<QName,ClassDefinition> classdef=new HashMap<QName,ClassDefinition>();
  Map<QName,Collection<PropertyDefinition>> propdef=new HashMap<QName,Collection<PropertyDefinition>>();
  Map<QName,Collection<AssociationDefinition>> assocdef=new HashMap<QName,Collection<AssociationDefinition>>();
  Map<String,Object> model=new HashMap<String,Object>();
  List<QName> qnames=new ArrayList<QName>();
  QName classQname=null;
  QName myModel=null;
  if (classFilter == null) {
    classFilter=""String_Node_Str"";
  }
  if (!isValidClassFilter(classFilter)) {
    throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + classFilter + ""String_Node_Str"");
  }
  if (namespacePrefix == null && name != null) {
    throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"");
  }
  if (namespacePrefix != null) {
    if (name != null) {
      className=namespacePrefix + ""String_Node_Str"" + name;
      if (!isValidClassname(className)) {
        throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + name + ""String_Node_Str"");
      }
      classQname=QName.createQName(getFullNamespaceURI(className));
      classdef.put(classQname,this.dictionaryservice.getClass(classQname));
      propdef.put(classQname,this.dictionaryservice.getClass(classQname).getProperties().values());
      assocdef.put(classQname,this.dictionaryservice.getClass(classQname).getAssociations().values());
    }
 else {
      String namespaceUri=namespaceService.getNamespaceURI(namespacePrefix);
      for (      QName qnameObj : this.dictionaryservice.getAllModels()) {
        if (qnameObj.getNamespaceURI().equals(namespaceUri)) {
          name=qnameObj.getLocalName();
          myModel=QName.createQName(getFullNamespaceURI(namespacePrefix + ""String_Node_Str"" + name));
          if (classFilter.equalsIgnoreCase(CLASS_FILTER_OPTION_TYPE1)) {
            qnames.addAll(this.dictionaryservice.getAspects(myModel));
            qnames.addAll(this.dictionaryservice.getTypes(myModel));
          }
 else           if (classFilter.equalsIgnoreCase(CLASS_FILTER_OPTION_TYPE3)) {
            qnames.addAll(this.dictionaryservice.getTypes(myModel));
          }
 else           if (classFilter.equalsIgnoreCase(CLASS_FILTER_OPTION_TYPE2)) {
            qnames.addAll(this.dictionaryservice.getAspects(myModel));
          }
        }
      }
    }
  }
  if (myModel == null) {
    if (classFilter.equalsIgnoreCase(CLASS_FILTER_OPTION_TYPE1)) {
      qnames.addAll(getAspects(isRM));
      qnames.addAll(getTypes(isRM));
    }
 else     if (classFilter.equalsIgnoreCase(CLASS_FILTER_OPTION_TYPE3)) {
      qnames.addAll(getTypes(isRM));
    }
 else     if (classFilter.equalsIgnoreCase(CLASS_FILTER_OPTION_TYPE2)) {
      qnames.addAll(getAspects(isRM));
    }
  }
  if (classdef.isEmpty()) {
    for (    QName qnameObj : qnames) {
      classdef.put(qnameObj,this.dictionaryservice.getClass(qnameObj));
      propdef.put(qnameObj,this.dictionaryservice.getClass(qnameObj).getProperties().values());
      assocdef.put(qnameObj,this.dictionaryservice.getClass(qnameObj).getAssociations().values());
    }
  }
  List<ClassDefinition> classDefinitions=new ArrayList<ClassDefinition>(classdef.values());
  Collections.sort(classDefinitions,new DictionaryComparators.ClassDefinitionComparator(dictionaryservice));
  model.put(MODEL_PROP_KEY_CLASS_DEFS,classDefinitions);
  model.put(MODEL_PROP_KEY_PROPERTY_DETAILS,propdef.values());
  model.put(MODEL_PROP_KEY_ASSOCIATION_DETAILS,assocdef.values());
  model.put(MODEL_PROP_KEY_MESSAGE_LOOKUP,dictionaryservice);
  return model;
}",0.988480921526278
89306,"/** 
 * Execute custom Java logic
 * @param req  Web Script request
 * @param status  Web Script status
 * @param cache  Web Script cache
 * @param isRM  indicates whether the request comes from an RM site or not
 * @return custom service model
 */
private Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache,boolean isRM){
  QName classQName=null;
  String className=req.getServiceMatch().getTemplateVars().get(DICTIONARY_CLASS_NAME);
  if (className != null && className.length() != 0) {
    classQName=createClassQName(className);
    if (classQName == null) {
      throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + className + ""String_Node_Str"");
    }
  }
  String[] names=req.getParameterValues(PARAM_NAME);
  String namespacePrefix=req.getParameter(REQ_URL_TEMPL_VAR_NAMESPACE_PREFIX);
  String namespaceURI=null;
  if (namespacePrefix != null) {
    namespaceURI=this.namespaceService.getNamespaceURI(namespacePrefix);
  }
  Map<QName,PropertyDefinition> propMap=null;
  if (classQName == null) {
    if (names != null) {
      propMap=new HashMap<QName,PropertyDefinition>(names.length);
      for (      String name : names) {
        QName propQName=QName.createQName(name,namespaceService);
        PropertyDefinition propDef=dictionaryservice.getProperty(propQName);
        if (propDef != null) {
          propMap.put(propQName,propDef);
        }
      }
    }
 else {
      Collection<QName> propQNames=getProperties(isRM);
      propMap=new HashMap<QName,PropertyDefinition>(propQNames.size());
      for (      QName propQName : propQNames) {
        propMap.put(propQName,dictionaryservice.getProperty(propQName));
      }
    }
  }
 else {
    propMap=dictionaryservice.getClass(classQName).getProperties();
  }
  List<PropertyDefinition> props=new ArrayList<PropertyDefinition>(propMap.size());
  for (  Map.Entry<QName,PropertyDefinition> entry : propMap.entrySet()) {
    if ((namespaceURI != null && namespaceURI.equals(entry.getKey().getNamespaceURI())) || namespaceURI == null) {
      props.add(entry.getValue());
    }
  }
  Collections.sort(props,new DictionaryComparators.PropertyDefinitionComparator(dictionaryservice));
  Map<String,Object> model=new HashMap<String,Object>();
  model.put(MODEL_PROP_KEY_PROPERTY_DETAILS,props);
  model.put(MODEL_PROP_KEY_MESSAGE_LOOKUP,dictionaryservice);
  return model;
}","/** 
 * Execute custom Java logic
 * @param req  Web Script request
 * @param isRM  indicates whether the request comes from an RM site or not
 * @return custom service model
 */
private Map<String,Object> executeImpl(WebScriptRequest req,boolean isRM){
  QName classQName=null;
  String className=req.getServiceMatch().getTemplateVars().get(DICTIONARY_CLASS_NAME);
  if (className != null && className.length() != 0) {
    classQName=createClassQName(className);
    if (classQName == null) {
      throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + className + ""String_Node_Str"");
    }
  }
  String[] names=req.getParameterValues(PARAM_NAME);
  String namespacePrefix=req.getParameter(REQ_URL_TEMPL_VAR_NAMESPACE_PREFIX);
  String namespaceURI=null;
  if (namespacePrefix != null) {
    namespaceURI=this.namespaceService.getNamespaceURI(namespacePrefix);
  }
  Map<QName,PropertyDefinition> propMap=null;
  if (classQName == null) {
    if (names != null) {
      propMap=new HashMap<QName,PropertyDefinition>(names.length);
      for (      String name : names) {
        QName propQName=QName.createQName(name,namespaceService);
        PropertyDefinition propDef=dictionaryservice.getProperty(propQName);
        if (propDef != null) {
          propMap.put(propQName,propDef);
        }
      }
    }
 else {
      Collection<QName> propQNames=getProperties(isRM);
      propMap=new HashMap<QName,PropertyDefinition>(propQNames.size());
      for (      QName propQName : propQNames) {
        propMap.put(propQName,dictionaryservice.getProperty(propQName));
      }
    }
  }
 else {
    propMap=dictionaryservice.getClass(classQName).getProperties();
  }
  List<PropertyDefinition> props=new ArrayList<PropertyDefinition>(propMap.size());
  for (  Map.Entry<QName,PropertyDefinition> entry : propMap.entrySet()) {
    if ((namespaceURI != null && namespaceURI.equals(entry.getKey().getNamespaceURI())) || namespaceURI == null) {
      props.add(entry.getValue());
    }
  }
  Collections.sort(props,new DictionaryComparators.PropertyDefinitionComparator(dictionaryservice));
  Map<String,Object> model=new HashMap<String,Object>();
  model.put(MODEL_PROP_KEY_PROPERTY_DETAILS,props);
  model.put(MODEL_PROP_KEY_MESSAGE_LOOKUP,dictionaryservice);
  return model;
}",0.9795134443021768
89307,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.admin.RecordsManagementAdminService#getCustomReferencesTo(org.alfresco.service.cmr.repository.NodeRef)
 */
public List<AssociationRef> getCustomReferencesTo(NodeRef node){
  List<AssociationRef> retrievedAssocs=nodeService.getSourceAssocs(node,RegexQNamePattern.MATCH_ALL);
  return retrievedAssocs;
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.admin.RecordsManagementAdminService#getCustomReferencesTo(org.alfresco.service.cmr.repository.NodeRef)
 */
public List<AssociationRef> getCustomReferencesTo(NodeRef node){
  return nodeService.getSourceAssocs(node,RegexQNamePattern.MATCH_ALL);
}",0.8936170212765957
89308,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.admin.RecordsManagementAdminService#getCustomParentReferences(org.alfresco.service.cmr.repository.NodeRef)
 */
public List<ChildAssociationRef> getCustomParentReferences(NodeRef node){
  List<ChildAssociationRef> result=nodeService.getParentAssocs(node);
  return result;
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.admin.RecordsManagementAdminService#getCustomParentReferences(org.alfresco.service.cmr.repository.NodeRef)
 */
public List<ChildAssociationRef> getCustomParentReferences(NodeRef node){
  return nodeService.getParentAssocs(node);
}",0.9073170731707316
89309,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.admin.RecordsManagementAdminService#getCustomChildReferences(org.alfresco.service.cmr.repository.NodeRef)
 */
public List<ChildAssociationRef> getCustomChildReferences(NodeRef node){
  List<ChildAssociationRef> childAssocs=nodeService.getChildAssocs(node);
  return childAssocs;
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.admin.RecordsManagementAdminService#getCustomChildReferences(org.alfresco.service.cmr.repository.NodeRef)
 */
public List<ChildAssociationRef> getCustomChildReferences(NodeRef node){
  return nodeService.getChildAssocs(node);
}",0.8917609046849758
89310,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.admin.RecordsManagementAdminService#getCustomReferencesFrom(org.alfresco.service.cmr.repository.NodeRef)
 */
public List<AssociationRef> getCustomReferencesFrom(NodeRef node){
  List<AssociationRef> retrievedAssocs=nodeService.getTargetAssocs(node,RegexQNamePattern.MATCH_ALL);
  return retrievedAssocs;
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.admin.RecordsManagementAdminService#getCustomReferencesFrom(org.alfresco.service.cmr.repository.NodeRef)
 */
public List<AssociationRef> getCustomReferencesFrom(NodeRef node){
  return nodeService.getTargetAssocs(node,RegexQNamePattern.MATCH_ALL);
}",0.8948948948948949
89311,"/** 
 * @param invocation
 * @param params
 * @param position
 * @return
 */
@SuppressWarnings(""String_Node_Str"") protected Serializable getProperty(MethodInvocation invocation,Class[] params,int position){
  if (invocation.getArguments()[position] == null) {
    return null;
  }
  if (Serializable.class.isAssignableFrom(params[position]) && invocation.getArguments()[position] != null) {
    Serializable property=(Serializable)invocation.getArguments()[position];
    return property;
  }
  throw new ACLEntryVoterException(""String_Node_Str"");
}","/** 
 * @param invocation
 * @param params
 * @param position
 * @return
 */
@SuppressWarnings(""String_Node_Str"") protected Serializable getProperty(MethodInvocation invocation,Class[] params,int position){
  if (invocation.getArguments()[position] == null) {
    return null;
  }
  if (Serializable.class.isAssignableFrom(params[position]) && invocation.getArguments()[position] != null) {
    return (Serializable)invocation.getArguments()[position];
  }
  throw new ACLEntryVoterException(""String_Node_Str"");
}",0.9510357815442562
89312,"/** 
 * @param invocation
 * @param params
 * @param position
 * @param parent
 * @return
 */
@SuppressWarnings(""String_Node_Str"") protected QName getType(MethodInvocation invocation,Class[] params,int position,boolean parent){
  if (QName.class.isAssignableFrom(params[position])) {
    if (invocation.getArguments()[position] != null) {
      QName qname=(QName)invocation.getArguments()[position];
      return qname;
    }
  }
 else   if (NodeRef.class.isAssignableFrom(params[position]) && invocation.getArguments()[position] != null) {
    NodeRef nodeRef=(NodeRef)invocation.getArguments()[position];
    return nodeService.getType(nodeRef);
  }
  return null;
}","/** 
 * @param invocation
 * @param params
 * @param position
 * @param parent
 * @return
 */
@SuppressWarnings(""String_Node_Str"") protected QName getType(MethodInvocation invocation,Class[] params,int position,boolean parent){
  if (QName.class.isAssignableFrom(params[position])) {
    if (invocation.getArguments()[position] != null) {
      return (QName)invocation.getArguments()[position];
    }
  }
 else   if (NodeRef.class.isAssignableFrom(params[position]) && invocation.getArguments()[position] != null) {
    NodeRef nodeRef=(NodeRef)invocation.getArguments()[position];
    return nodeService.getType(nodeRef);
  }
  return null;
}",0.9687738004569688
89313,"/** 
 * @param invocation
 * @param params
 * @param position
 * @return
 */
@SuppressWarnings({""String_Node_Str"",""String_Node_Str""}) protected Map<QName,Serializable> getProperties(MethodInvocation invocation,Class[] params,int position){
  if (invocation.getArguments()[position] == null) {
    return null;
  }
  if (Map.class.isAssignableFrom(params[position]) && invocation.getArguments()[position] != null) {
    Map<QName,Serializable> properties=(Map<QName,Serializable>)invocation.getArguments()[position];
    return properties;
  }
  throw new ACLEntryVoterException(""String_Node_Str"");
}","/** 
 * @param invocation
 * @param params
 * @param position
 * @return
 */
@SuppressWarnings({""String_Node_Str"",""String_Node_Str""}) protected Map<QName,Serializable> getProperties(MethodInvocation invocation,Class[] params,int position){
  if (invocation.getArguments()[position] == null) {
    return null;
  }
  if (Map.class.isAssignableFrom(params[position]) && invocation.getArguments()[position] != null) {
    return (Map<QName,Serializable>)invocation.getArguments()[position];
  }
  throw new ACLEntryVoterException(""String_Node_Str"");
}",0.941586748038361
89314,"/** 
 * @param invocation
 * @param params
 * @param position
 * @return
 */
@SuppressWarnings(""String_Node_Str"") protected QName getQName(MethodInvocation invocation,Class[] params,int position){
  if (QName.class.isAssignableFrom(params[position]) && invocation.getArguments()[position] != null) {
    QName qname=(QName)invocation.getArguments()[position];
    return qname;
  }
  throw new ACLEntryVoterException(""String_Node_Str"");
}","/** 
 * @param invocation
 * @param params
 * @param position
 * @return
 */
@SuppressWarnings(""String_Node_Str"") protected QName getQName(MethodInvocation invocation,Class[] params,int position){
  if (QName.class.isAssignableFrom(params[position]) && invocation.getArguments()[position] != null) {
    return (QName)invocation.getArguments()[position];
  }
  throw new ACLEntryVoterException(""String_Node_Str"");
}",0.9542790152403282
89315,"public String getName(){
  String xxx=info.getName().replace(""String_Node_Str"",""String_Node_Str"");
  return xxx;
}","public String getName(){
  return info.getName().replace(""String_Node_Str"",""String_Node_Str"");
}",0.8857142857142857
89316,"public ScriptConstraintValue[] getValues(){
  Map<String,List<String>> details=rmCaveatconfigService.getListDetails(info.getName());
  if (details == null) {
    details=new HashMap<String,List<String>>();
  }
  Map<String,List<String>> pivot=PivotUtil.getPivot(details);
  Set<String> values=pivot.keySet();
  ArrayList<ScriptConstraintValue> constraints=new ArrayList<ScriptConstraintValue>(pivot.size());
  for (  String value : values) {
    ScriptConstraintValue constraint=new ScriptConstraintValue();
    constraint.setValueName(value);
    constraint.setValueTitle(value);
    List<String> authorities=pivot.get(value);
    List<ScriptAuthority> sauth=new ArrayList<ScriptAuthority>();
    for (    String authority : authorities) {
      ScriptAuthority a=new ScriptAuthority();
      a.setAuthorityName(authority);
      String displayName=authorityService.getAuthorityDisplayName(authority);
      if (StringUtils.isNotBlank(displayName)) {
        a.setAuthorityTitle(displayName);
      }
 else {
        a.setAuthorityTitle(authority);
      }
      sauth.add(a);
    }
    constraint.setAuthorities(sauth);
    constraints.add(constraint);
  }
  for (  String value : info.getAllowedValues()) {
    if (!values.contains(value)) {
      ScriptConstraintValue constraint=new ScriptConstraintValue();
      constraint.setValueName(value);
      constraint.setValueTitle(value);
      List<ScriptAuthority> sauth=new ArrayList<ScriptAuthority>();
      constraint.setAuthorities(sauth);
      constraints.add(constraint);
    }
  }
  ScriptConstraintValue[] retVal=constraints.toArray(new ScriptConstraintValue[constraints.size()]);
  return retVal;
}","public ScriptConstraintValue[] getValues(){
  Map<String,List<String>> details=rmCaveatconfigService.getListDetails(info.getName());
  if (details == null) {
    details=new HashMap<String,List<String>>();
  }
  Map<String,List<String>> pivot=PivotUtil.getPivot(details);
  Set<String> values=pivot.keySet();
  ArrayList<ScriptConstraintValue> constraints=new ArrayList<ScriptConstraintValue>(pivot.size());
  for (  String value : values) {
    ScriptConstraintValue constraint=new ScriptConstraintValue();
    constraint.setValueName(value);
    constraint.setValueTitle(value);
    List<String> authorities=pivot.get(value);
    List<ScriptAuthority> sauth=new ArrayList<ScriptAuthority>();
    for (    String authority : authorities) {
      ScriptAuthority a=new ScriptAuthority();
      a.setAuthorityName(authority);
      String displayName=authorityService.getAuthorityDisplayName(authority);
      if (StringUtils.isNotBlank(displayName)) {
        a.setAuthorityTitle(displayName);
      }
 else {
        a.setAuthorityTitle(authority);
      }
      sauth.add(a);
    }
    constraint.setAuthorities(sauth);
    constraints.add(constraint);
  }
  for (  String value : info.getAllowedValues()) {
    if (!values.contains(value)) {
      ScriptConstraintValue constraint=new ScriptConstraintValue();
      constraint.setValueName(value);
      constraint.setValueTitle(value);
      List<ScriptAuthority> sauth=new ArrayList<ScriptAuthority>();
      constraint.setAuthorities(sauth);
      constraints.add(constraint);
    }
  }
  return constraints.toArray(new ScriptConstraintValue[constraints.size()]);
}",0.9570514773073407
89317,"public ScriptConstraint createConstraint(String listName,String title,String[] allowedValues){
  if (listName != null) {
    listName=listName.replace(""String_Node_Str"",""String_Node_Str"");
  }
  RMConstraintInfo info=caveatConfigService.addRMConstraint(listName,title,allowedValues);
  ScriptConstraint c=new ScriptConstraint(info,caveatConfigService,getAuthorityService());
  return c;
}","public ScriptConstraint createConstraint(String listName,String title,String[] allowedValues){
  if (listName != null) {
    listName=listName.replace(""String_Node_Str"",""String_Node_Str"");
  }
  RMConstraintInfo info=caveatConfigService.addRMConstraint(listName,title,allowedValues);
  return new ScriptConstraint(info,caveatConfigService,getAuthorityService());
}",0.9388297872340424
89318,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.identifier.IdentifierGenerator#generateId(java.util.Map)
 */
@Override public String generateId(Map<String,Serializable> context){
  NodeRef nodeRef=(NodeRef)context.get(IdentifierService.CONTEXT_NODEREF);
  Long dbId=0l;
  if (nodeRef != null) {
    dbId=(Long)nodeService.getProperty(nodeRef,ContentModel.PROP_NODE_DBID);
  }
 else {
    dbId=System.currentTimeMillis();
  }
  Calendar fileCalendar=Calendar.getInstance();
  String year=Integer.toString(fileCalendar.get(Calendar.YEAR));
  String identifier=year + ""String_Node_Str"" + padString(dbId.toString(),10);
  return identifier;
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.identifier.IdentifierGenerator#generateId(java.util.Map)
 */
@Override public String generateId(Map<String,Serializable> context){
  NodeRef nodeRef=(NodeRef)context.get(IdentifierService.CONTEXT_NODEREF);
  Long dbId=0l;
  if (nodeRef != null) {
    dbId=(Long)nodeService.getProperty(nodeRef,ContentModel.PROP_NODE_DBID);
  }
 else {
    dbId=System.currentTimeMillis();
  }
  Calendar fileCalendar=Calendar.getInstance();
  String year=Integer.toString(fileCalendar.get(Calendar.YEAR));
  return year + ""String_Node_Str"" + padString(dbId.toString(),10);
}",0.9634920634920636
89319,"@Override public boolean evaluate(JSONObject jsonObject){
  boolean result=super.evaluate(jsonObject);
  return !result;
}","@Override public boolean evaluate(JSONObject jsonObject){
  return !super.evaluate(jsonObject);
}",0.8401826484018264
89320,"@SuppressWarnings(""String_Node_Str"") @Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  String reqContentAsString;
  try {
    reqContentAsString=req.getContent().getContent();
  }
 catch (  IOException iox) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",iox);
  }
  String actionName=null;
  List<NodeRef> targetNodeRefs=new ArrayList<NodeRef>(1);
  Map<String,Serializable> actionParams=new HashMap<String,Serializable>(3);
  try {
    JSONObject jsonObj=new JSONObject(new JSONTokener(reqContentAsString));
    if (jsonObj.has(PARAM_NAME)) {
      actionName=jsonObj.getString(PARAM_NAME);
    }
    if (jsonObj.has(PARAM_NODE_REF)) {
      NodeRef nodeRef=new NodeRef(jsonObj.getString(PARAM_NODE_REF));
      targetNodeRefs.add(nodeRef);
    }
    if (jsonObj.has(PARAM_NODE_REFS)) {
      JSONArray jsonArray=jsonObj.getJSONArray(PARAM_NODE_REFS);
      if (jsonArray.length() != 0) {
        targetNodeRefs=new ArrayList<NodeRef>(jsonArray.length());
        for (int i=0; i < jsonArray.length(); i++) {
          NodeRef nodeRef=new NodeRef(jsonArray.getString(i));
          targetNodeRefs.add(nodeRef);
        }
      }
    }
    if (jsonObj.has(PARAM_PARAMS)) {
      JSONObject paramsObj=jsonObj.getJSONObject(PARAM_PARAMS);
      for (Iterator<String> iter=paramsObj.keys(); iter.hasNext(); ) {
        String nextKeyString=iter.next();
        Object nextValue=paramsObj.get(nextKeyString);
        if (nextValue instanceof JSONObject) {
          if (((JSONObject)nextValue).has(""String_Node_Str"")) {
            String dateStringValue=((JSONObject)nextValue).getString(""String_Node_Str"");
            nextValue=ISO8601DateFormat.parse(dateStringValue);
          }
        }
        actionParams.put(nextKeyString,(Serializable)nextValue);
      }
    }
  }
 catch (  JSONException exception) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",exception);
  }
  if (actionName == null) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"");
  }
  StringBuffer targetNodeRefsString=new StringBuffer(30);
  boolean firstTime=true;
  for (  NodeRef targetNodeRef : targetNodeRefs) {
    if (!nodeService.exists(targetNodeRef)) {
      throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + targetNodeRef.toString() + ""String_Node_Str"");
    }
    if (firstTime) {
      firstTime=false;
    }
 else {
      targetNodeRefsString.append(""String_Node_Str"");
    }
    targetNodeRefsString.append(targetNodeRef.toString());
  }
  if (logger.isDebugEnabled()) {
    StringBuilder msg=new StringBuilder();
    msg.append(""String_Node_Str"").append(actionName).append(""String_Node_Str"").append(targetNodeRefsString.toString()).append(""String_Node_Str"").append(actionParams);
    logger.debug(msg.toString());
  }
  Map<String,Object> model=new HashMap<String,Object>();
  if (targetNodeRefs.isEmpty()) {
    RecordsManagementActionResult result=this.rmActionService.executeRecordsManagementAction(actionName,actionParams);
    if (result.getValue() != null) {
      model.put(""String_Node_Str"",result.getValue().toString());
    }
  }
 else {
    Map<NodeRef,RecordsManagementActionResult> resultMap=this.rmActionService.executeRecordsManagementAction(targetNodeRefs,actionName,actionParams);
    Map<String,String> results=new HashMap<String,String>(resultMap.size());
    for (    NodeRef nodeRef : resultMap.keySet()) {
      Object value=resultMap.get(nodeRef).getValue();
      if (value != null) {
        results.put(nodeRef.toString(),resultMap.get(nodeRef).getValue().toString());
      }
    }
    model.put(""String_Node_Str"",results);
  }
  model.put(""String_Node_Str"",""String_Node_Str"" + actionName + ""String_Node_Str""+ targetNodeRefsString.toString());
  return model;
}","@SuppressWarnings(""String_Node_Str"") @Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  String reqContentAsString;
  try {
    reqContentAsString=req.getContent().getContent();
  }
 catch (  IOException iox) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",iox);
  }
  String actionName=null;
  List<NodeRef> targetNodeRefs=new ArrayList<NodeRef>(1);
  Map<String,Serializable> actionParams=new HashMap<String,Serializable>(3);
  try {
    JSONObject jsonObj=new JSONObject(new JSONTokener(reqContentAsString));
    if (jsonObj.has(PARAM_NAME)) {
      actionName=jsonObj.getString(PARAM_NAME);
    }
    if (jsonObj.has(PARAM_NODE_REF)) {
      NodeRef nodeRef=new NodeRef(jsonObj.getString(PARAM_NODE_REF));
      targetNodeRefs.add(nodeRef);
    }
    if (jsonObj.has(PARAM_NODE_REFS)) {
      JSONArray jsonArray=jsonObj.getJSONArray(PARAM_NODE_REFS);
      if (jsonArray.length() != 0) {
        targetNodeRefs=new ArrayList<NodeRef>(jsonArray.length());
        for (int i=0; i < jsonArray.length(); i++) {
          NodeRef nodeRef=new NodeRef(jsonArray.getString(i));
          targetNodeRefs.add(nodeRef);
        }
      }
    }
    if (jsonObj.has(PARAM_PARAMS)) {
      JSONObject paramsObj=jsonObj.getJSONObject(PARAM_PARAMS);
      for (Iterator<String> iter=paramsObj.keys(); iter.hasNext(); ) {
        String nextKeyString=iter.next();
        Object nextValue=paramsObj.get(nextKeyString);
        if ((nextValue instanceof JSONObject) && ((JSONObject)nextValue).has(""String_Node_Str"")) {
          String dateStringValue=((JSONObject)nextValue).getString(""String_Node_Str"");
          nextValue=ISO8601DateFormat.parse(dateStringValue);
        }
        actionParams.put(nextKeyString,(Serializable)nextValue);
      }
    }
  }
 catch (  JSONException exception) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",exception);
  }
  if (actionName == null) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"");
  }
  StringBuffer targetNodeRefsString=new StringBuffer(30);
  boolean firstTime=true;
  for (  NodeRef targetNodeRef : targetNodeRefs) {
    if (!nodeService.exists(targetNodeRef)) {
      throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + targetNodeRef.toString() + ""String_Node_Str"");
    }
    if (firstTime) {
      firstTime=false;
    }
 else {
      targetNodeRefsString.append(""String_Node_Str"");
    }
    targetNodeRefsString.append(targetNodeRef.toString());
  }
  if (logger.isDebugEnabled()) {
    StringBuilder msg=new StringBuilder();
    msg.append(""String_Node_Str"").append(actionName).append(""String_Node_Str"").append(targetNodeRefsString.toString()).append(""String_Node_Str"").append(actionParams);
    logger.debug(msg.toString());
  }
  Map<String,Object> model=new HashMap<String,Object>();
  if (targetNodeRefs.isEmpty()) {
    RecordsManagementActionResult result=this.rmActionService.executeRecordsManagementAction(actionName,actionParams);
    if (result.getValue() != null) {
      model.put(""String_Node_Str"",result.getValue().toString());
    }
  }
 else {
    Map<NodeRef,RecordsManagementActionResult> resultMap=this.rmActionService.executeRecordsManagementAction(targetNodeRefs,actionName,actionParams);
    Map<String,String> results=new HashMap<String,String>(resultMap.size());
    for (    NodeRef nodeRef : resultMap.keySet()) {
      Object value=resultMap.get(nodeRef).getValue();
      if (value != null) {
        results.put(nodeRef.toString(),resultMap.get(nodeRef).getValue().toString());
      }
    }
    model.put(""String_Node_Str"",results);
  }
  model.put(""String_Node_Str"",""String_Node_Str"" + actionName + ""String_Node_Str""+ targetNodeRefsString.toString());
  return model;
}",0.995296576953227
89321,"/** 
 * Helper method to check whether all the mandatory properties of the node have been set
 * @param nodeRef node reference
 * @return boolean true if all mandatory properties are set, false otherwise
 */
private boolean mandatoryPropertiesSet(NodeRef nodeRef,List<String> missingProperties){
  boolean result=true;
  Map<QName,Serializable> nodeRefProps=this.nodeService.getProperties(nodeRef);
  QName nodeRefType=this.nodeService.getType(nodeRef);
  TypeDefinition typeDef=this.dictionaryService.getType(nodeRefType);
  for (  PropertyDefinition propDef : typeDef.getProperties().values()) {
    if (propDef.isMandatory() && nodeRefProps.get(propDef.getName()) == null) {
      logMissingProperty(propDef,missingProperties);
      result=false;
      break;
    }
  }
  if (result != false) {
    Set<QName> aspects=this.nodeService.getAspects(nodeRef);
    for (    QName aspect : aspects) {
      AspectDefinition aspectDef=this.dictionaryService.getAspect(aspect);
      for (      PropertyDefinition propDef : aspectDef.getProperties().values()) {
        if (propDef.isMandatory() && nodeRefProps.get(propDef.getName()) == null) {
          logMissingProperty(propDef,missingProperties);
          result=false;
          break;
        }
      }
    }
  }
  return result;
}","/** 
 * Helper method to check whether all the mandatory properties of the node have been set
 * @param nodeRef node reference
 * @return boolean true if all mandatory properties are set, false otherwise
 */
private boolean mandatoryPropertiesSet(NodeRef nodeRef,List<String> missingProperties){
  boolean result=true;
  Map<QName,Serializable> nodeRefProps=this.nodeService.getProperties(nodeRef);
  QName nodeRefType=this.nodeService.getType(nodeRef);
  TypeDefinition typeDef=this.dictionaryService.getType(nodeRefType);
  for (  PropertyDefinition propDef : typeDef.getProperties().values()) {
    if (propDef.isMandatory() && nodeRefProps.get(propDef.getName()) == null) {
      logMissingProperty(propDef,missingProperties);
      result=false;
      break;
    }
  }
  if (result) {
    Set<QName> aspects=this.nodeService.getAspects(nodeRef);
    for (    QName aspect : aspects) {
      AspectDefinition aspectDef=this.dictionaryService.getAspect(aspect);
      for (      PropertyDefinition propDef : aspectDef.getProperties().values()) {
        if (propDef.isMandatory() && nodeRefProps.get(propDef.getName()) == null) {
          logMissingProperty(propDef,missingProperties);
          result=false;
          break;
        }
      }
    }
  }
  return result;
}",0.9964884900507218
89322,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.compatibility.ModulePatchComponent#executePatch()
 */
@Override protected void executePatch() throws Throwable {
  if (!nodeService.exists(RM_CONFIG)) {
    return;
  }
  if (!nodeService.exists(newBehaviorScriptsFolder)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    String newBehaviorScriptsFolderName=""String_Node_Str"";
    String newBehaviorScriptsNodeUUID=""String_Node_Str"";
    String newBehaviorScriptsAssocQName=""String_Node_Str"";
    Map<QName,Serializable> newBehaviorScriptsFolderProps=new HashMap<QName,Serializable>();
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_NODE_UUID,newBehaviorScriptsNodeUUID);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_NAME,newBehaviorScriptsFolderName);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_TITLE,newBehaviorScriptsFolderName);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
    newBehaviorScriptsFolder=nodeService.createNode(RM_CONFIG,ContentModel.ASSOC_CONTAINS,QName.createQName(NamespaceService.CONTENT_MODEL_PREFIX,newBehaviorScriptsAssocQName),ContentModel.TYPE_FOLDER,newBehaviorScriptsFolderProps).getChildRef();
  }
  if (nodeService.exists(OLD_BEHAVIOR_SCRIPTS_FOLDER)) {
    AuthenticationUtil.runAs(new RunAsWork<Object>(){
      public Object doWork(){
        RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
          public Void execute() throws Throwable {
            nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
            List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
            if (oldBehaviorScripts != null && oldBehaviorScripts.isEmpty() != true) {
              if (logger.isDebugEnabled()) {
                logger.debug(""String_Node_Str"");
              }
              for (              FileInfo script : oldBehaviorScripts) {
                fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
                if (logger.isDebugEnabled()) {
                  logger.debug(""String_Node_Str"" + script.getName());
                }
              }
            }
            return null;
          }
        }
;
        retryingTransactionHelper.doInTransaction(callback);
        return null;
      }
    }
,AuthenticationUtil.getSystemUserName());
  }
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.patch.compatibility.ModulePatchComponent#executePatch()
 */
@Override protected void executePatch() throws Throwable {
  if (!nodeService.exists(RM_CONFIG)) {
    return;
  }
  if (!nodeService.exists(newBehaviorScriptsFolder)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    String newBehaviorScriptsFolderName=""String_Node_Str"";
    String newBehaviorScriptsNodeUUID=""String_Node_Str"";
    String newBehaviorScriptsAssocQName=""String_Node_Str"";
    Map<QName,Serializable> newBehaviorScriptsFolderProps=new HashMap<QName,Serializable>();
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_NODE_UUID,newBehaviorScriptsNodeUUID);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_NAME,newBehaviorScriptsFolderName);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_TITLE,newBehaviorScriptsFolderName);
    newBehaviorScriptsFolderProps.put(ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
    newBehaviorScriptsFolder=nodeService.createNode(RM_CONFIG,ContentModel.ASSOC_CONTAINS,QName.createQName(NamespaceService.CONTENT_MODEL_PREFIX,newBehaviorScriptsAssocQName),ContentModel.TYPE_FOLDER,newBehaviorScriptsFolderProps).getChildRef();
  }
  if (nodeService.exists(OLD_BEHAVIOR_SCRIPTS_FOLDER)) {
    AuthenticationUtil.runAs(new RunAsWork<Object>(){
      public Object doWork(){
        RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
          public Void execute() throws Throwable {
            nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
            List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
            if (oldBehaviorScripts != null && !oldBehaviorScripts.isEmpty()) {
              if (logger.isDebugEnabled()) {
                logger.debug(""String_Node_Str"");
              }
              for (              FileInfo script : oldBehaviorScripts) {
                fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
                if (logger.isDebugEnabled()) {
                  logger.debug(""String_Node_Str"" + script.getName());
                }
              }
            }
            return null;
          }
        }
;
        retryingTransactionHelper.doInTransaction(callback);
        return null;
      }
    }
,AuthenticationUtil.getSystemUserName());
  }
}",0.99822729958637
89323,"public Object doWork(){
  RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
    public Void execute() throws Throwable {
      nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
      List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
      if (oldBehaviorScripts != null && oldBehaviorScripts.isEmpty() != true) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"");
        }
        for (        FileInfo script : oldBehaviorScripts) {
          fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
          if (logger.isDebugEnabled()) {
            logger.debug(""String_Node_Str"" + script.getName());
          }
        }
      }
      return null;
    }
  }
;
  retryingTransactionHelper.doInTransaction(callback);
  return null;
}","public Object doWork(){
  RetryingTransactionCallback<Void> callback=new RetryingTransactionCallback<Void>(){
    public Void execute() throws Throwable {
      nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
      List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
      if (oldBehaviorScripts != null && !oldBehaviorScripts.isEmpty()) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"");
        }
        for (        FileInfo script : oldBehaviorScripts) {
          fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
          if (logger.isDebugEnabled()) {
            logger.debug(""String_Node_Str"" + script.getName());
          }
        }
      }
      return null;
    }
  }
;
  retryingTransactionHelper.doInTransaction(callback);
  return null;
}",0.9954198473282444
89324,"public Void execute() throws Throwable {
  nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
  List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
  if (oldBehaviorScripts != null && oldBehaviorScripts.isEmpty() != true) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    for (    FileInfo script : oldBehaviorScripts) {
      fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + script.getName());
      }
    }
  }
  return null;
}","public Void execute() throws Throwable {
  nodeService.setProperty(OLD_BEHAVIOR_SCRIPTS_FOLDER,ContentModel.PROP_DESCRIPTION,""String_Node_Str"");
  List<FileInfo> oldBehaviorScripts=fileFolderService.listFiles(OLD_BEHAVIOR_SCRIPTS_FOLDER);
  if (oldBehaviorScripts != null && !oldBehaviorScripts.isEmpty()) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"");
    }
    for (    FileInfo script : oldBehaviorScripts) {
      fileFolderService.moveFrom(script.getNodeRef(),OLD_BEHAVIOR_SCRIPTS_FOLDER,RMv21BehaviorScriptsPatch.newBehaviorScriptsFolder,script.getName());
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + script.getName());
      }
    }
  }
  return null;
}",0.993805918788713
89325,"@Override public boolean evaluate(JSONObject jsonObject){
  if (type == null) {
    return false;
  }
  if (isDocLibRecord(jsonObject) == true) {
    return false;
  }
  try {
    JSONObject rmNode=getRMNode(jsonObject);
    if (rmNode == null) {
      return false;
    }
 else {
      String uiType=(String)rmNode.get(""String_Node_Str"");
      if (uiType.equalsIgnoreCase(type)) {
        return true;
      }
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err);
  }
  return false;
}","@Override public boolean evaluate(JSONObject jsonObject){
  if (type == null) {
    return false;
  }
  if (isDocLibRecord(jsonObject)) {
    return false;
  }
  try {
    JSONObject rmNode=getRMNode(jsonObject);
    if (rmNode == null) {
      return false;
    }
 else {
      String uiType=(String)rmNode.get(""String_Node_Str"");
      if (uiType.equalsIgnoreCase(type)) {
        return true;
      }
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err);
  }
  return false;
}",0.8173258003766478
89326,"/** 
 * @see org.alfresco.web.scripts.forms.FormUIGet#getVisibleFieldsInSet(org.alfresco.web.scripts.forms.FormUIGet.ModelContext,org.alfresco.web.config.forms.FormSet)
 */
@Override protected List<String> getVisibleFieldsInSet(ModelContext context,FormSet setConfig){
  List<String> result=null;
  String id=setConfig.getSetId();
  if (SET_RM_CUSTOM.equals(id) == true || id.startsWith(SET_RM_METADATA) == true) {
    Map<String,List<String>> setMembership=discoverSetMembership(context);
    result=setMembership.get(id);
  }
 else {
    result=super.getVisibleFieldsInSet(context,setConfig);
  }
  return result;
}","/** 
 * @see org.alfresco.web.scripts.forms.FormUIGet#getVisibleFieldsInSet(org.alfresco.web.scripts.forms.FormUIGet.ModelContext,org.alfresco.web.config.forms.FormSet)
 */
@Override protected List<String> getVisibleFieldsInSet(ModelContext context,FormSet setConfig){
  List<String> result=null;
  String id=setConfig.getSetId();
  if (SET_RM_CUSTOM.equals(id) || id.startsWith(SET_RM_METADATA)) {
    Map<String,List<String>> setMembership=discoverSetMembership(context);
    result=setMembership.get(id);
  }
 else {
    result=super.getVisibleFieldsInSet(context,setConfig);
  }
  return result;
}",0.986863711001642
89327,"/** 
 * Gets all the record meta-data sets present in the form data.
 * @param context
 * @return
 */
protected Collection<FormSet> getRecordMetaDataSetConfig(ModelContext context){
  Map<String,FormSet> result=new HashMap<String,FormSet>(13);
  try {
    JSONObject data=context.getFormDefinition().getJSONObject(MODEL_DATA);
    JSONObject definition=data.getJSONObject(MODEL_DEFINITION);
    JSONArray fieldsFromServer=definition.getJSONArray(MODEL_FIELDS);
    for (int x=0; x < fieldsFromServer.length(); x++) {
      JSONObject fieldDefinition=fieldsFromServer.getJSONObject(x);
      if (fieldDefinition.has(MODEL_GROUP)) {
        String set=fieldDefinition.getString(MODEL_GROUP);
        if (!result.containsKey(set) && set.startsWith(SET_RM_METADATA) == true) {
          FormSet formSet=new FormSet(set,null,""String_Node_Str"",null,LABEL_ID_PREFIX + set);
          result.put(set,formSet);
        }
      }
    }
  }
 catch (  JSONException je) {
  }
  return result.values();
}","/** 
 * Gets all the record meta-data sets present in the form data.
 * @param context
 * @return
 */
protected Collection<FormSet> getRecordMetaDataSetConfig(ModelContext context){
  Map<String,FormSet> result=new HashMap<String,FormSet>(13);
  try {
    JSONObject data=context.getFormDefinition().getJSONObject(MODEL_DATA);
    JSONObject definition=data.getJSONObject(MODEL_DEFINITION);
    JSONArray fieldsFromServer=definition.getJSONArray(MODEL_FIELDS);
    for (int x=0; x < fieldsFromServer.length(); x++) {
      JSONObject fieldDefinition=fieldsFromServer.getJSONObject(x);
      if (fieldDefinition.has(MODEL_GROUP)) {
        String set=fieldDefinition.getString(MODEL_GROUP);
        if (!result.containsKey(set) && set.startsWith(SET_RM_METADATA)) {
          FormSet formSet=new FormSet(set,null,""String_Node_Str"",null,LABEL_ID_PREFIX + set);
          result.put(set,formSet);
        }
      }
    }
  }
 catch (  JSONException je) {
  }
  return result.values();
}",0.9959473150962512
89328,"/** 
 * Determines whether the given node type matches the path of the given object
 * @see org.springframework.extensions.config.evaluator.Evaluator#applies(java.lang.Object,java.lang.String)
 */
public boolean applies(Object obj,String condition){
  boolean result=false;
  if (obj instanceof String) {
    String objAsString=(String)obj;
    if (objAsString.indexOf(':') != -1 || objAsString.startsWith(""String_Node_Str"") == true) {
      Matcher m=NODE_REF_PATTERN.matcher(objAsString);
      if (m.matches()) {
        try {
          String jsonResponseString=callService(""String_Node_Str"" + objAsString);
          if (jsonResponseString != null) {
            result=checkJsonAgainstCondition(condition,jsonResponseString);
          }
 else           if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"");
          }
        }
 catch (        ConnectorServiceException e) {
          if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"",e);
          }
        }
      }
 else {
        try {
          String jsonResponseString=callService(""String_Node_Str"" + URLEncoder.encodeUriComponent(objAsString));
          if (jsonResponseString != null) {
            result=checkJsonAgainstCondition(condition,jsonResponseString);
          }
 else           if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"");
          }
        }
 catch (        ConnectorServiceException e) {
          if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"",e);
          }
        }
      }
    }
  }
  return result;
}","/** 
 * Determines whether the given node type matches the path of the given object
 * @see org.springframework.extensions.config.evaluator.Evaluator#applies(java.lang.Object,java.lang.String)
 */
public boolean applies(Object obj,String condition){
  boolean result=false;
  if (obj instanceof String) {
    String objAsString=(String)obj;
    if (objAsString.indexOf(':') != -1 || objAsString.startsWith(""String_Node_Str"")) {
      Matcher m=NODE_REF_PATTERN.matcher(objAsString);
      if (m.matches()) {
        try {
          String jsonResponseString=callService(""String_Node_Str"" + objAsString);
          if (jsonResponseString != null) {
            result=checkJsonAgainstCondition(condition,jsonResponseString);
          }
 else           if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"");
          }
        }
 catch (        ConnectorServiceException e) {
          if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"",e);
          }
        }
      }
 else {
        try {
          String jsonResponseString=callService(""String_Node_Str"" + URLEncoder.encodeUriComponent(objAsString));
          if (jsonResponseString != null) {
            result=checkJsonAgainstCondition(condition,jsonResponseString);
          }
 else           if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"");
          }
        }
 catch (        ConnectorServiceException e) {
          if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"",e);
          }
        }
      }
    }
  }
  return result;
}",0.9975384615384616
89329,"/** 
 * Extend the default resolution code to account for records.
 * @see org.alfresco.web.resolver.doclib.DefaultDoclistActionGroupResolver#resolve(org.json.simple.JSONObject,java.lang.String)
 */
@Override public String resolve(JSONObject jsonObject,String view){
  String result=null;
  JSONObject node=(org.json.simple.JSONObject)jsonObject.get(""String_Node_Str"");
  Boolean isRMNode=(Boolean)node.get(""String_Node_Str"");
  if (isRMNode != null && isRMNode.booleanValue() == true) {
    result=filePlanDoclistActionGroupResolver.resolve(jsonObject,view,true);
  }
 else {
    result=super.resolve(jsonObject,view);
  }
  return result;
}","/** 
 * Extend the default resolution code to account for records.
 * @see org.alfresco.web.resolver.doclib.DefaultDoclistActionGroupResolver#resolve(org.json.simple.JSONObject,java.lang.String)
 */
@Override public String resolve(JSONObject jsonObject,String view){
  String result=null;
  JSONObject node=(org.json.simple.JSONObject)jsonObject.get(""String_Node_Str"");
  Boolean isRMNode=(Boolean)node.get(""String_Node_Str"");
  if (isRMNode != null && isRMNode.booleanValue()) {
    result=filePlanDoclistActionGroupResolver.resolve(jsonObject,view,true);
  }
 else {
    result=super.resolve(jsonObject,view);
  }
  return result;
}",0.9937304075235108
89330,"/** 
 * @param jsonObject
 * @param view
 * @param isDocLib
 * @return
 */
public String resolve(JSONObject jsonObject,String view,boolean isDocLib){
  String actionGroupId=""String_Node_Str"";
  if (isDocLib == true) {
    actionGroupId+=""String_Node_Str"";
  }
  JSONObject node=(org.json.simple.JSONObject)jsonObject.get(""String_Node_Str"");
  boolean isLink=(Boolean)node.get(""String_Node_Str"");
  if (isLink) {
    actionGroupId+=""String_Node_Str"";
  }
 else {
    JSONObject rmNode=(JSONObject)node.get(""String_Node_Str"");
    actionGroupId+=(String)rmNode.get(""String_Node_Str"") + ""String_Node_Str"";
  }
  if (view.equals(""String_Node_Str"")) {
    actionGroupId+=""String_Node_Str"";
  }
 else {
    actionGroupId+=""String_Node_Str"";
  }
  return actionGroupId;
}","/** 
 * @param jsonObject
 * @param view
 * @param isDocLib
 * @return
 */
public String resolve(JSONObject jsonObject,String view,boolean isDocLib){
  String actionGroupId=""String_Node_Str"";
  if (isDocLib) {
    actionGroupId+=""String_Node_Str"";
  }
  JSONObject node=(org.json.simple.JSONObject)jsonObject.get(""String_Node_Str"");
  boolean isLink=(Boolean)node.get(""String_Node_Str"");
  if (isLink) {
    actionGroupId+=""String_Node_Str"";
  }
 else {
    JSONObject rmNode=(JSONObject)node.get(""String_Node_Str"");
    actionGroupId+=(String)rmNode.get(""String_Node_Str"") + ""String_Node_Str"";
  }
  if (view.equals(""String_Node_Str"")) {
    actionGroupId+=""String_Node_Str"";
  }
 else {
    actionGroupId+=""String_Node_Str"";
  }
  return actionGroupId;
}",0.9947368421052633
89331,"/** 
 * Gets the hold name
 * @return The name of the hold
 */
public String getName(){
  return this.Name;
}","/** 
 * Gets the hold name
 * @return The name of the hold
 */
public String getName(){
  return this.name;
}",0.9908256880733946
89332,"/** 
 * Constructor
 * @param name The name of the hold
 * @param nodeRef The {@link NodeRef} of the hold
 */
public Hold(String name,NodeRef nodeRef){
  this.Name=name;
  this.nodeRef=nodeRef;
}","/** 
 * Constructor
 * @param name The name of the hold
 * @param nodeRef The {@link NodeRef} of the hold
 */
public Hold(String name,NodeRef nodeRef){
  this.name=name;
  this.nodeRef=nodeRef;
}",0.9948717948717948
89333,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#init()
 */
@Override public void init(){
  if (this instanceof RecordsManagementAction == false) {
    super.init();
  }
  if (auditable) {
    getAuditService().registerAuditEvent(this.getActionDefinition().getName(),this.getActionDefinition().getTitle());
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#init()
 */
@Override public void init(){
  if (!(this instanceof RecordsManagementAction)) {
    super.init();
  }
  if (auditable) {
    getAuditService().registerAuditEvent(this.getActionDefinition().getName(),this.getActionDefinition().getTitle());
  }
}",0.9820359281437124
89334,"/** 
 * Calculates and updates the <code>rma:dispositionEventsEligible</code> property for the given next disposition action.
 * @param nextAction The next disposition action
 * @return The result of calculation
 */
protected boolean updateEventEligible(DispositionAction nextAction){
  List<EventCompletionDetails> events=nextAction.getEventCompletionDetails();
  boolean eligible=false;
  if (nextAction.getDispositionActionDefinition().eligibleOnFirstCompleteEvent() == false) {
    eligible=true;
    for (    EventCompletionDetails event : events) {
      if (event.isEventComplete() == false) {
        eligible=false;
        break;
      }
    }
  }
 else {
    for (    EventCompletionDetails event : events) {
      if (event.isEventComplete()) {
        eligible=true;
        break;
      }
    }
  }
  this.nodeService.setProperty(nextAction.getNodeRef(),PROP_DISPOSITION_EVENTS_ELIGIBLE,eligible);
  return eligible;
}","/** 
 * Calculates and updates the <code>rma:dispositionEventsEligible</code> property for the given next disposition action.
 * @param nextAction The next disposition action
 * @return The result of calculation
 */
protected boolean updateEventEligible(DispositionAction nextAction){
  List<EventCompletionDetails> events=nextAction.getEventCompletionDetails();
  boolean eligible=false;
  if (!nextAction.getDispositionActionDefinition().eligibleOnFirstCompleteEvent()) {
    eligible=true;
    for (    EventCompletionDetails event : events) {
      if (!event.isEventComplete()) {
        eligible=false;
        break;
      }
    }
  }
 else {
    for (    EventCompletionDetails event : events) {
      if (event.isEventComplete()) {
        eligible=true;
        break;
      }
    }
  }
  this.nodeService.setProperty(nextAction.getNodeRef(),PROP_DISPOSITION_EVENTS_ELIGIBLE,eligible);
  return eligible;
}",0.8928571428571429
89335,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  NodeRef nextDispositionActionNodeRef=getNextDispostionAction(actionedUponNodeRef);
  boolean checkError=true;
  Boolean checkErrorValue=(Boolean)action.getParameterValue(PARAM_NO_ERROR_CHECK);
  if (checkErrorValue != null) {
    checkError=checkErrorValue.booleanValue();
  }
  DispositionSchedule di=checkDispositionActionExecutionValidity(actionedUponNodeRef,nextDispositionActionNodeRef,checkError);
  if (di != null) {
    if (checkEligibility(actionedUponNodeRef) == false || dispositionService.isNextDispositionActionEligible(actionedUponNodeRef)) {
      if (di.isRecordLevelDisposition()) {
        if (recordService.isRecord(actionedUponNodeRef)) {
          if (recordService.isDeclared(actionedUponNodeRef)) {
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_AT,new Date());
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_BY,AuthenticationUtil.getRunAsUser());
            executeRecordLevelDisposition(action,actionedUponNodeRef);
            if (nodeService.exists(nextDispositionActionNodeRef) && getSetDispositionActionComplete()) {
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_AT,new Date());
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_BY,AuthenticationUtil.getRunAsUser());
            }
          }
 else {
            throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_RECORD_NOT_DECLARED,getName(),actionedUponNodeRef.toString()));
          }
        }
 else {
          throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_EXPECTED_RECORD_LEVEL,getName(),actionedUponNodeRef.toString()));
        }
      }
 else {
        if (recordFolderService.isRecordFolder(actionedUponNodeRef)) {
          if (recordFolderService.isRecordFolderDeclared(actionedUponNodeRef)) {
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_AT,new Date());
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_BY,AuthenticationUtil.getRunAsUser());
            executeRecordFolderLevelDisposition(action,actionedUponNodeRef);
            if (nodeService.exists(nextDispositionActionNodeRef) && getSetDispositionActionComplete()) {
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_AT,new Date());
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_BY,AuthenticationUtil.getRunAsUser());
            }
          }
 else {
            throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_ALL_RECORDS_DECLARED,getName(),actionedUponNodeRef.toString()));
          }
        }
 else {
          throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_RECORD_FOLDER,getName(),actionedUponNodeRef.toString()));
        }
      }
      if (nodeService.exists(actionedUponNodeRef) && getSetDispositionActionComplete()) {
        dispositionService.updateNextDispositionAction(actionedUponNodeRef);
      }
    }
 else {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_ELIGIBLE,getName(),actionedUponNodeRef.toString()));
    }
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  NodeRef nextDispositionActionNodeRef=getNextDispostionAction(actionedUponNodeRef);
  boolean checkError=true;
  Boolean checkErrorValue=(Boolean)action.getParameterValue(PARAM_NO_ERROR_CHECK);
  if (checkErrorValue != null) {
    checkError=checkErrorValue.booleanValue();
  }
  DispositionSchedule di=checkDispositionActionExecutionValidity(actionedUponNodeRef,nextDispositionActionNodeRef,checkError);
  if (di != null) {
    if (!checkEligibility(actionedUponNodeRef) || dispositionService.isNextDispositionActionEligible(actionedUponNodeRef)) {
      if (di.isRecordLevelDisposition()) {
        if (recordService.isRecord(actionedUponNodeRef)) {
          if (recordService.isDeclared(actionedUponNodeRef)) {
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_AT,new Date());
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_BY,AuthenticationUtil.getRunAsUser());
            executeRecordLevelDisposition(action,actionedUponNodeRef);
            if (nodeService.exists(nextDispositionActionNodeRef) && getSetDispositionActionComplete()) {
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_AT,new Date());
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_BY,AuthenticationUtil.getRunAsUser());
            }
          }
 else {
            throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_RECORD_NOT_DECLARED,getName(),actionedUponNodeRef.toString()));
          }
        }
 else {
          throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_EXPECTED_RECORD_LEVEL,getName(),actionedUponNodeRef.toString()));
        }
      }
 else {
        if (recordFolderService.isRecordFolder(actionedUponNodeRef)) {
          if (recordFolderService.isRecordFolderDeclared(actionedUponNodeRef)) {
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_AT,new Date());
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_BY,AuthenticationUtil.getRunAsUser());
            executeRecordFolderLevelDisposition(action,actionedUponNodeRef);
            if (nodeService.exists(nextDispositionActionNodeRef) && getSetDispositionActionComplete()) {
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_AT,new Date());
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_BY,AuthenticationUtil.getRunAsUser());
            }
          }
 else {
            throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_ALL_RECORDS_DECLARED,getName(),actionedUponNodeRef.toString()));
          }
        }
 else {
          throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_RECORD_FOLDER,getName(),actionedUponNodeRef.toString()));
        }
      }
      if (nodeService.exists(actionedUponNodeRef) && getSetDispositionActionComplete()) {
        dispositionService.updateNextDispositionAction(actionedUponNodeRef);
      }
    }
 else {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_ELIGIBLE,getName(),actionedUponNodeRef.toString()));
    }
  }
}",0.9985843714609286
89336,"/** 
 * @param nodeRef
 * @return
 */
protected DispositionSchedule checkDispositionActionExecutionValidity(NodeRef nodeRef,NodeRef nextDispositionActionNodeRef,boolean throwError){
  DispositionSchedule di=dispositionService.getDispositionSchedule(nodeRef);
  if (di == null) {
    if (throwError) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NO_DISPOITION_INSTRUCTIONS,getName(),nodeRef.toString()));
    }
 else {
      return null;
    }
  }
  if (nodeService.hasAspect(nodeRef,ASPECT_DISPOSITION_LIFECYCLE) == false) {
    if (throwError) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NO_DIS_LIFECYCLE_SET,getName(),nodeRef.toString()));
    }
 else {
      return null;
    }
  }
  if (checkNextDispositionAction(nodeRef)) {
    NodeRef nextDispositionAction=nextDispositionActionNodeRef;
    if (nextDispositionAction == null) {
      if (throwError) {
        throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NEXT_DISP_NOT_SET,getName(),nodeRef.toString()));
      }
 else {
        return null;
      }
    }
    String actionName=(String)nodeService.getProperty(nextDispositionAction,PROP_DISPOSITION_ACTION);
    if (actionName == null || actionName.equals(getName()) == false) {
      if (throwError) {
        throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_NEXT_DISP,getName(),nodeRef.toString()));
      }
 else {
        return null;
      }
    }
  }
  return di;
}","/** 
 * @param nodeRef
 * @return
 */
protected DispositionSchedule checkDispositionActionExecutionValidity(NodeRef nodeRef,NodeRef nextDispositionActionNodeRef,boolean throwError){
  DispositionSchedule di=dispositionService.getDispositionSchedule(nodeRef);
  if (di == null) {
    if (throwError) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NO_DISPOITION_INSTRUCTIONS,getName(),nodeRef.toString()));
    }
 else {
      return null;
    }
  }
  if (!nodeService.hasAspect(nodeRef,ASPECT_DISPOSITION_LIFECYCLE)) {
    if (throwError) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NO_DIS_LIFECYCLE_SET,getName(),nodeRef.toString()));
    }
 else {
      return null;
    }
  }
  if (checkNextDispositionAction(nodeRef)) {
    NodeRef nextDispositionAction=nextDispositionActionNodeRef;
    if (nextDispositionAction == null) {
      if (throwError) {
        throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NEXT_DISP_NOT_SET,getName(),nodeRef.toString()));
      }
 else {
        return null;
      }
    }
    String actionName=(String)nodeService.getProperty(nextDispositionAction,PROP_DISPOSITION_ACTION);
    if (actionName == null || !actionName.equals(getName())) {
      if (throwError) {
        throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_NEXT_DISP,getName(),nodeRef.toString()));
      }
 else {
        return null;
      }
    }
  }
  return di;
}",0.9930651872399444
89337,"public void register(RecordsManagementActionCondition rmCondition){
  if (rmConditions.containsKey(rmCondition.getName()) == false) {
    rmConditions.put(rmCondition.getName(),rmCondition);
  }
}","public void register(RecordsManagementActionCondition rmCondition){
  if (!rmConditions.containsKey(rmCondition.getName())) {
    rmConditions.put(rmCondition.getName(),rmCondition);
  }
}",0.9739583333333334
89338,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) == false) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (dictionaryService.isSubClass(nodeService.getType(actionedUponNodeRef),ContentModel.TYPE_CONTENT) == false) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_RECORD)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ContentModel.ASPECT_WORKING_COPY)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_RECORD_REJECTION_DETAILS)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_SYNCED)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else {
    NodeRef filePlan=(NodeRef)action.getParameterValue(PARAM_FILE_PLAN);
    if (filePlan == null) {
      filePlan=filePlanAuthenticationService.runAsRmAdmin(new RunAsWork<NodeRef>(){
        @Override public NodeRef doWork() throws Exception {
          return filePlanService.getFilePlanBySiteId(FilePlanService.DEFAULT_RM_SITE_ID);
        }
      }
);
      if (filePlan == null) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"");
        }
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
    }
 else {
      if (filePlanService.isFilePlan(filePlan) == false) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"");
        }
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
    }
    boolean hideRecord=false;
    Boolean hideRecordValue=((Boolean)action.getParameterValue(PARAM_HIDE_RECORD));
    if (hideRecordValue != null) {
      hideRecord=hideRecordValue.booleanValue();
    }
    recordService.createRecord(filePlan,actionedUponNodeRef,!hideRecord);
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  if (!nodeService.exists(actionedUponNodeRef)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (!dictionaryService.isSubClass(nodeService.getType(actionedUponNodeRef),ContentModel.TYPE_CONTENT)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_RECORD)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ContentModel.ASPECT_WORKING_COPY)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_RECORD_REJECTION_DETAILS)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_SYNCED)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else {
    NodeRef filePlan=(NodeRef)action.getParameterValue(PARAM_FILE_PLAN);
    if (filePlan == null) {
      filePlan=filePlanAuthenticationService.runAsRmAdmin(new RunAsWork<NodeRef>(){
        @Override public NodeRef doWork() throws Exception {
          return filePlanService.getFilePlanBySiteId(FilePlanService.DEFAULT_RM_SITE_ID);
        }
      }
);
      if (filePlan == null) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"");
        }
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
    }
 else {
      if (!filePlanService.isFilePlan(filePlan)) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"");
        }
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
    }
    boolean hideRecord=false;
    Boolean hideRecordValue=((Boolean)action.getParameterValue(PARAM_HIDE_RECORD));
    if (hideRecordValue != null) {
      hideRecord=hideRecordValue.booleanValue();
    }
    recordService.createRecord(filePlan,actionedUponNodeRef,!hideRecord);
  }
}",0.9943693693693694
89339,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_RECORD) == false) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else {
    recordService.hideRecord(actionedUponNodeRef);
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (!nodeService.hasAspect(actionedUponNodeRef,ASPECT_RECORD)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else {
    recordService.hideRecord(actionedUponNodeRef);
  }
}",0.9905123339658444
89340,"/** 
 * Helper method to check the actioned upon node reference to decide to execute the action The preconditions are: - The node must exist - The node must not be frozen - The node must be record - The node must not be declared
 * @param actionedUponNodeRef node reference
 * @return Return true if the node reference passes all the preconditions for executing the action, false otherwise
 */
private boolean eligibleForAction(NodeRef actionedUponNodeRef){
  boolean result=false;
  if (nodeService.exists(actionedUponNodeRef) && freezeService.isFrozen(actionedUponNodeRef) == false && recordService.isRecord(actionedUponNodeRef) && recordService.isDeclared(actionedUponNodeRef) == false) {
    result=true;
  }
  return result;
}","/** 
 * Helper method to check the actioned upon node reference to decide to execute the action The preconditions are: - The node must exist - The node must not be frozen - The node must be record - The node must not be declared
 * @param actionedUponNodeRef node reference
 * @return Return true if the node reference passes all the preconditions for executing the action, false otherwise
 */
private boolean eligibleForAction(NodeRef actionedUponNodeRef){
  boolean result=false;
  if (nodeService.exists(actionedUponNodeRef) && !freezeService.isFrozen(actionedUponNodeRef) && recordService.isRecord(actionedUponNodeRef)&& !recordService.isDeclared(actionedUponNodeRef)) {
    result=true;
  }
  return result;
}",0.966089965397924
89341,"/** 
 * Persists any changes made to the events on the given disposition action definition on the given next action.
 * @param dispositionActionDef The disposition action definition node
 * @param nextAction The next disposition action
 */
@SuppressWarnings(""String_Node_Str"") private void persistEventChanges(NodeRef dispositionActionDef,DispositionAction nextAction){
  List<String> stepEvents=(List<String>)nodeService.getProperty(dispositionActionDef,PROP_DISPOSITION_EVENT);
  List<EventCompletionDetails> eventsList=nextAction.getEventCompletionDetails();
  List<String> nextActionEvents=new ArrayList<String>(eventsList.size());
  for (  EventCompletionDetails event : eventsList) {
    String eventName=event.getEventName();
    nextActionEvents.add(eventName);
    if (stepEvents != null && stepEvents.contains(event.getEventName()) == false) {
      nodeService.removeChild(nextAction.getNodeRef(),event.getNodeRef());
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + eventName + ""String_Node_Str""+ nextAction.getName()+ ""String_Node_Str""+ nextAction.getNodeRef()+ ""String_Node_Str"");
      }
    }
  }
  if (stepEvents != null) {
    for (    String eventName : stepEvents) {
      if (!nextActionEvents.contains(eventName)) {
        createEvent(recordsManagementEventService.getEvent(eventName),nextAction.getNodeRef());
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"" + eventName + ""String_Node_Str""+ nextAction.getName()+ ""String_Node_Str""+ nextAction.getNodeRef()+ ""String_Node_Str"");
        }
      }
    }
  }
  boolean eligible=updateEventEligible(nextAction);
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + eligible + ""String_Node_Str""+ nextAction.getName()+ ""String_Node_Str""+ nextAction.getNodeRef()+ ""String_Node_Str"");
  }
}","/** 
 * Persists any changes made to the events on the given disposition action definition on the given next action.
 * @param dispositionActionDef The disposition action definition node
 * @param nextAction The next disposition action
 */
@SuppressWarnings(""String_Node_Str"") private void persistEventChanges(NodeRef dispositionActionDef,DispositionAction nextAction){
  List<String> stepEvents=(List<String>)nodeService.getProperty(dispositionActionDef,PROP_DISPOSITION_EVENT);
  List<EventCompletionDetails> eventsList=nextAction.getEventCompletionDetails();
  List<String> nextActionEvents=new ArrayList<String>(eventsList.size());
  for (  EventCompletionDetails event : eventsList) {
    String eventName=event.getEventName();
    nextActionEvents.add(eventName);
    if (stepEvents != null && !stepEvents.contains(event.getEventName())) {
      nodeService.removeChild(nextAction.getNodeRef(),event.getNodeRef());
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + eventName + ""String_Node_Str""+ nextAction.getName()+ ""String_Node_Str""+ nextAction.getNodeRef()+ ""String_Node_Str"");
      }
    }
  }
  if (stepEvents != null) {
    for (    String eventName : stepEvents) {
      if (!nextActionEvents.contains(eventName)) {
        createEvent(recordsManagementEventService.getEvent(eventName),nextAction.getNodeRef());
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"" + eventName + ""String_Node_Str""+ nextAction.getName()+ ""String_Node_Str""+ nextAction.getNodeRef()+ ""String_Node_Str"");
        }
      }
    }
  }
  boolean eligible=updateEventEligible(nextAction);
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + eligible + ""String_Node_Str""+ nextAction.getName()+ ""String_Node_Str""+ nextAction.getNodeRef()+ ""String_Node_Str"");
  }
}",0.9748496446145436
89342,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@SuppressWarnings(""String_Node_Str"") @Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (RecordsManagementModel.TYPE_DISPOSITION_ACTION_DEFINITION.equals(nodeService.getType(actionedUponNodeRef)) == false) {
    return;
  }
  List<QName> changedProps=(List<QName>)action.getParameterValue(CHANGED_PROPERTIES);
  NodeRef dispositionScheduleNode=nodeService.getPrimaryParent(actionedUponNodeRef).getParentRef();
  NodeRef rmContainer=nodeService.getPrimaryParent(dispositionScheduleNode).getParentRef();
  DispositionSchedule dispositionSchedule=dispositionService.getAssociatedDispositionSchedule(rmContainer);
  behaviourFilter.disableBehaviour();
  try {
    List<NodeRef> disposableItems=dispositionService.getDisposableItems(dispositionSchedule);
    for (    NodeRef disposableItem : disposableItems) {
      updateDisposableItem(dispositionSchedule,disposableItem,actionedUponNodeRef,changedProps);
    }
  }
  finally {
    behaviourFilter.enableBehaviour();
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@SuppressWarnings(""String_Node_Str"") @Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (!RecordsManagementModel.TYPE_DISPOSITION_ACTION_DEFINITION.equals(nodeService.getType(actionedUponNodeRef))) {
    return;
  }
  List<QName> changedProps=(List<QName>)action.getParameterValue(CHANGED_PROPERTIES);
  NodeRef dispositionScheduleNode=nodeService.getPrimaryParent(actionedUponNodeRef).getParentRef();
  NodeRef rmContainer=nodeService.getPrimaryParent(dispositionScheduleNode).getParentRef();
  DispositionSchedule dispositionSchedule=dispositionService.getAssociatedDispositionSchedule(rmContainer);
  behaviourFilter.disableBehaviour();
  try {
    List<NodeRef> disposableItems=dispositionService.getDisposableItems(dispositionSchedule);
    for (    NodeRef disposableItem : disposableItems) {
      updateDisposableItem(dispositionSchedule,disposableItem,actionedUponNodeRef,changedProps);
    }
  }
  finally {
    behaviourFilter.enableBehaviour();
  }
}",0.9957446808510638
89343,"/** 
 * Helper method to check the actioned upon node reference to decide to execute the action The preconditions are: - The node must exist - The node must not be frozen
 * @param actionedUponNodeRef node reference
 * @return Return true if the node reference passes all the preconditions for executing the action, false otherwise
 */
private boolean eligibleForAction(NodeRef actionedUponNodeRef){
  boolean result=false;
  if (nodeService.exists(actionedUponNodeRef) && freezeService.isFrozen(actionedUponNodeRef) == false) {
    result=true;
  }
  return result;
}","/** 
 * Helper method to check the actioned upon node reference to decide to execute the action The preconditions are: - The node must exist - The node must not be frozen
 * @param actionedUponNodeRef node reference
 * @return Return true if the node reference passes all the preconditions for executing the action, false otherwise
 */
private boolean eligibleForAction(NodeRef actionedUponNodeRef){
  boolean result=false;
  if (nodeService.exists(actionedUponNodeRef) && !freezeService.isFrozen(actionedUponNodeRef)) {
    result=true;
  }
  return result;
}",0.9663120567375888
89344,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && freezeService.isFrozen(actionedUponNodeRef) == false) {
    String eventName=(String)action.getParameterValue(PARAM_EVENT_NAME);
    String eventCompletedBy=(String)action.getParameterValue(PARAM_EVENT_COMPLETED_BY);
    Date eventCompletedAt=(Date)action.getParameterValue(PARAM_EVENT_COMPLETED_AT);
    if (this.nodeService.hasAspect(actionedUponNodeRef,ASPECT_DISPOSITION_LIFECYCLE)) {
      DispositionAction da=this.dispositionService.getNextDispositionAction(actionedUponNodeRef);
      if (da != null) {
        EventCompletionDetails event=getEvent(da,eventName);
        if (event != null) {
          if (eventCompletedAt == null) {
            eventCompletedAt=new Date();
          }
          if (eventCompletedBy == null) {
            eventCompletedBy=AuthenticationUtil.getRunAsUser();
          }
          NodeRef eventNodeRef=event.getNodeRef();
          Map<QName,Serializable> props=this.nodeService.getProperties(eventNodeRef);
          props.put(PROP_EVENT_EXECUTION_COMPLETE,true);
          props.put(PROP_EVENT_EXECUTION_COMPLETED_AT,eventCompletedAt);
          props.put(PROP_EVENT_EXECUTION_COMPLETED_BY,eventCompletedBy);
          this.nodeService.setProperties(eventNodeRef,props);
          updateEventEligible(da);
        }
      }
    }
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && !freezeService.isFrozen(actionedUponNodeRef)) {
    String eventName=(String)action.getParameterValue(PARAM_EVENT_NAME);
    String eventCompletedBy=(String)action.getParameterValue(PARAM_EVENT_COMPLETED_BY);
    Date eventCompletedAt=(Date)action.getParameterValue(PARAM_EVENT_COMPLETED_AT);
    if (this.nodeService.hasAspect(actionedUponNodeRef,ASPECT_DISPOSITION_LIFECYCLE)) {
      DispositionAction da=this.dispositionService.getNextDispositionAction(actionedUponNodeRef);
      if (da != null) {
        EventCompletionDetails event=getEvent(da,eventName);
        if (event != null) {
          if (eventCompletedAt == null) {
            eventCompletedAt=new Date();
          }
          if (eventCompletedBy == null) {
            eventCompletedBy=AuthenticationUtil.getRunAsUser();
          }
          NodeRef eventNodeRef=event.getNodeRef();
          Map<QName,Serializable> props=this.nodeService.getProperties(eventNodeRef);
          props.put(PROP_EVENT_EXECUTION_COMPLETE,true);
          props.put(PROP_EVENT_EXECUTION_COMPLETED_AT,eventCompletedAt);
          props.put(PROP_EVENT_EXECUTION_COMPLETED_BY,eventCompletedBy);
          this.nodeService.setProperties(eventNodeRef,props);
          updateEventEligible(da);
        }
      }
    }
  }
}",0.9968354430379748
89345,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && recordService.isRecord(actionedUponNodeRef) && freezeService.isFrozen(actionedUponNodeRef) == false) {
    if (recordService.isDeclared(actionedUponNodeRef) == false) {
      List<String> missingProperties=new ArrayList<String>(5);
      if (mandatoryPropertiesSet(actionedUponNodeRef,missingProperties)) {
        recordService.disablePropertyEditableCheck();
        try {
          Map<QName,Serializable> declaredProps=new HashMap<QName,Serializable>(2);
          declaredProps.put(PROP_DECLARED_AT,new Date());
          declaredProps.put(PROP_DECLARED_BY,AuthenticationUtil.getRunAsUser());
          this.nodeService.addAspect(actionedUponNodeRef,ASPECT_DECLARED_RECORD,declaredProps);
          AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
            @Override public Void doWork() throws Exception {
              ownableService.setOwner(actionedUponNodeRef,OwnableService.NO_OWNER);
              return null;
            }
          }
);
        }
  finally {
          recordService.enablePropertyEditableCheck();
        }
      }
 else {
        logger.debug(buildMissingPropertiesErrorString(missingProperties));
        action.setParameterValue(ActionExecuterAbstractBase.PARAM_RESULT,""String_Node_Str"");
      }
    }
  }
 else {
    if (logger.isWarnEnabled()) {
      logger.warn(I18NUtil.getMessage(MSG_UNDECLARED_ONLY_RECORDS,actionedUponNodeRef.toString()));
    }
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && recordService.isRecord(actionedUponNodeRef) && !freezeService.isFrozen(actionedUponNodeRef)) {
    if (!recordService.isDeclared(actionedUponNodeRef)) {
      List<String> missingProperties=new ArrayList<String>(5);
      if (mandatoryPropertiesSet(actionedUponNodeRef,missingProperties)) {
        recordService.disablePropertyEditableCheck();
        try {
          Map<QName,Serializable> declaredProps=new HashMap<QName,Serializable>(2);
          declaredProps.put(PROP_DECLARED_AT,new Date());
          declaredProps.put(PROP_DECLARED_BY,AuthenticationUtil.getRunAsUser());
          this.nodeService.addAspect(actionedUponNodeRef,ASPECT_DECLARED_RECORD,declaredProps);
          AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
            @Override public Void doWork() throws Exception {
              ownableService.setOwner(actionedUponNodeRef,OwnableService.NO_OWNER);
              return null;
            }
          }
);
        }
  finally {
          recordService.enablePropertyEditableCheck();
        }
      }
 else {
        logger.debug(buildMissingPropertiesErrorString(missingProperties));
        action.setParameterValue(ActionExecuterAbstractBase.PARAM_RESULT,""String_Node_Str"");
      }
    }
  }
 else {
    if (logger.isWarnEnabled()) {
      logger.warn(I18NUtil.getMessage(MSG_UNDECLARED_ONLY_RECORDS,actionedUponNodeRef.toString()));
    }
  }
}",0.994155464640561
89346,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && (checkFrozen == false || freezeService.isFrozen(actionedUponNodeRef) == false)) {
    if (allowParameterSubstitutions) {
      parameterProcessorComponent.process(action,delegateActionExecuter.getActionDefinition(),actionedUponNodeRef);
    }
    delegateActionExecuter.execute(action,actionedUponNodeRef);
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && (!checkFrozen || !freezeService.isFrozen(actionedUponNodeRef))) {
    if (allowParameterSubstitutions) {
      parameterProcessorComponent.process(action,delegateActionExecuter.getActionDefinition(),actionedUponNodeRef);
    }
    delegateActionExecuter.execute(action,actionedUponNodeRef);
  }
}",0.9835526315789472
89347,"/** 
 * @param action
 * @param actionedUponNodeRef
 * @return
 */
private NodeRef createOrResolveRecordFolder(Action action,NodeRef actionedUponNodeRef){
  NodeRef context=filePlanService.getFilePlan(actionedUponNodeRef);
  if (context == null) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
 else   if (nodeService.exists(context) == false) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
  String path=(String)action.getParameterValue(PARAM_PATH);
  String[] pathValues=ArrayUtils.EMPTY_STRING_ARRAY;
  if (path != null && path.isEmpty() == false) {
    pathValues=StringUtils.tokenizeToStringArray(path,""String_Node_Str"",false,true);
  }
  boolean create=false;
  Boolean createValue=(Boolean)action.getParameterValue(PARAM_CREATE_RECORD_PATH);
  if (createValue != null) {
    create=createValue.booleanValue();
  }
  NodeRef recordFolder=resolvePath(context,pathValues);
  if (recordFolder == null) {
    if (create) {
      NodeRef parent=resolveParent(context,pathValues,create);
      if (parent == null) {
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
      if (filePlanService.isRecordCategory(parent) == false) {
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
      String recordFolderName=pathValues[pathValues.length - 1];
      recordFolder=recordFolderService.createRecordFolder(parent,recordFolderName);
    }
 else {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
  }
  return recordFolder;
}","/** 
 * @param action
 * @param actionedUponNodeRef
 * @return
 */
private NodeRef createOrResolveRecordFolder(Action action,NodeRef actionedUponNodeRef){
  NodeRef context=filePlanService.getFilePlan(actionedUponNodeRef);
  if (context == null) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
 else   if (!nodeService.exists(context)) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
  String path=(String)action.getParameterValue(PARAM_PATH);
  String[] pathValues=ArrayUtils.EMPTY_STRING_ARRAY;
  if (path != null && !path.isEmpty()) {
    pathValues=StringUtils.tokenizeToStringArray(path,""String_Node_Str"",false,true);
  }
  boolean create=false;
  Boolean createValue=(Boolean)action.getParameterValue(PARAM_CREATE_RECORD_PATH);
  if (createValue != null) {
    create=createValue.booleanValue();
  }
  NodeRef recordFolder=resolvePath(context,pathValues);
  if (recordFolder == null) {
    if (create) {
      NodeRef parent=resolveParent(context,pathValues,create);
      if (parent == null) {
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
      if (!filePlanService.isRecordCategory(parent)) {
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
      String recordFolderName=pathValues[pathValues.length - 1];
      recordFolder=recordFolderService.createRecordFolder(parent,recordFolderName);
    }
 else {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
  }
  return recordFolder;
}",0.9900133155792276
89348,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && freezeService.isFrozen(actionedUponNodeRef) == false && recordService.isFiled(actionedUponNodeRef) == false) {
    NodeRef recordFolder=(NodeRef)action.getParameterValue(PARAM_DESTINATION_RECORD_FOLDER);
    if (recordFolder == null) {
      recordFolder=createOrResolveRecordFolder(action,actionedUponNodeRef);
    }
    if (recordFolder == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
    if (recordFolderService.isRecordFolder(recordFolder)) {
      final NodeRef finalRecordFolder=recordFolder;
      AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
        @Override public Void doWork() throws Exception {
          try {
            fileFolderService.move(actionedUponNodeRef,finalRecordFolder,null);
          }
 catch (          FileNotFoundException fileNotFound) {
            throw new AlfrescoRuntimeException(""String_Node_Str"",fileNotFound);
          }
          return null;
        }
      }
);
    }
 else {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && !freezeService.isFrozen(actionedUponNodeRef) && !recordService.isFiled(actionedUponNodeRef)) {
    NodeRef recordFolder=(NodeRef)action.getParameterValue(PARAM_DESTINATION_RECORD_FOLDER);
    if (recordFolder == null) {
      recordFolder=createOrResolveRecordFolder(action,actionedUponNodeRef);
    }
    if (recordFolder == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
    if (recordFolderService.isRecordFolder(recordFolder)) {
      final NodeRef finalRecordFolder=recordFolder;
      AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
        @Override public Void doWork() throws Exception {
          try {
            fileFolderService.move(actionedUponNodeRef,finalRecordFolder,null);
          }
 catch (          FileNotFoundException fileNotFound) {
            throw new AlfrescoRuntimeException(""String_Node_Str"",fileNotFound);
          }
          return null;
        }
      }
);
    }
 else {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
  }
}",0.9925428784489188
89349,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && nodeService.hasAspect(actionedUponNodeRef,ContentModel.ASPECT_PENDING_DELETE) == false && (recordService.isRecord(actionedUponNodeRef) || recordFolderService.isRecordFolder(actionedUponNodeRef)) && freezeService.isFrozen(actionedUponNodeRef) == false) {
    freezeService.freeze((String)action.getParameterValue(PARAM_REASON),actionedUponNodeRef);
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && !nodeService.hasAspect(actionedUponNodeRef,ContentModel.ASPECT_PENDING_DELETE) && (recordService.isRecord(actionedUponNodeRef) || recordFolderService.isRecordFolder(actionedUponNodeRef))&& !freezeService.isFrozen(actionedUponNodeRef)) {
    freezeService.freeze((String)action.getParameterValue(PARAM_REASON),actionedUponNodeRef);
  }
}",0.9838087895142636
89350,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && freezeService.isFrozen(actionedUponNodeRef) == false) {
    if (recordService.isRecord(actionedUponNodeRef)) {
      ChildAssociationRef assocRef=nodeService.getPrimaryParent(actionedUponNodeRef);
      if (assocRef != null) {
        actionedUponNodeRef=assocRef.getParentRef();
      }
    }
    if (recordFolderService.isRecordFolder(actionedUponNodeRef)) {
      Boolean isClosed=(Boolean)nodeService.getProperty(actionedUponNodeRef,PROP_IS_CLOSED);
      if (Boolean.TRUE.equals(isClosed)) {
        nodeService.setProperty(actionedUponNodeRef,PROP_IS_CLOSED,false);
      }
    }
 else {
      if (logger.isWarnEnabled()) {
        logger.warn(I18NUtil.getMessage(MSG_NO_OPEN_RECORD_FOLDER,actionedUponNodeRef.toString()));
      }
    }
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && !freezeService.isFrozen(actionedUponNodeRef)) {
    if (recordService.isRecord(actionedUponNodeRef)) {
      ChildAssociationRef assocRef=nodeService.getPrimaryParent(actionedUponNodeRef);
      if (assocRef != null) {
        actionedUponNodeRef=assocRef.getParentRef();
      }
    }
    if (recordFolderService.isRecordFolder(actionedUponNodeRef)) {
      Boolean isClosed=(Boolean)nodeService.getProperty(actionedUponNodeRef,PROP_IS_CLOSED);
      if (Boolean.TRUE.equals(isClosed)) {
        nodeService.setProperty(actionedUponNodeRef,PROP_IS_CLOSED,false);
      }
    }
 else {
      if (logger.isWarnEnabled()) {
        logger.warn(I18NUtil.getMessage(MSG_NO_OPEN_RECORD_FOLDER,actionedUponNodeRef.toString()));
      }
    }
  }
}",0.9952335557673976
89351,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && freezeService.isFrozen(actionedUponNodeRef) == false && nodeService.getProperty(actionedUponNodeRef,PROP_RECORD_ORIGINATING_LOCATION) != null) {
    recordService.rejectRecord(actionedUponNodeRef,(String)action.getParameterValue(PARAM_REASON));
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && !freezeService.isFrozen(actionedUponNodeRef) && nodeService.getProperty(actionedUponNodeRef,PROP_RECORD_ORIGINATING_LOCATION) != null) {
    recordService.rejectRecord(actionedUponNodeRef,(String)action.getParameterValue(PARAM_REASON));
  }
}",0.990909090909091
89352,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && nodeService.hasAspect(actionedUponNodeRef,ContentModel.ASPECT_PENDING_DELETE) == false && recordService.isRecord(actionedUponNodeRef) && recordService.isDeclared(actionedUponNodeRef) == false) {
    String workflowDefinitionId=workflowService.getDefinitionByName(REQUEST_INFO_WORKFLOW_DEFINITION_NAME).getId();
    Map<QName,Serializable> parameters=new HashMap<QName,Serializable>();
    parameters.put(WorkflowModel.ASSOC_PACKAGE,getWorkflowPackage(action,actionedUponNodeRef));
    parameters.put(RMWorkflowModel.RM_MIXED_ASSIGNEES,getAssignees(action));
    parameters.put(RMWorkflowModel.RM_REQUESTED_INFORMATION,getRequestedInformation(action));
    parameters.put(RMWorkflowModel.RM_RULE_CREATOR,getRuleCreator(action));
    workflowService.startWorkflow(workflowDefinitionId,parameters);
  }
 else {
    logger.info(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && !nodeService.hasAspect(actionedUponNodeRef,ContentModel.ASPECT_PENDING_DELETE) && recordService.isRecord(actionedUponNodeRef)&& !recordService.isDeclared(actionedUponNodeRef)) {
    String workflowDefinitionId=workflowService.getDefinitionByName(REQUEST_INFO_WORKFLOW_DEFINITION_NAME).getId();
    Map<QName,Serializable> parameters=new HashMap<QName,Serializable>();
    parameters.put(WorkflowModel.ASSOC_PACKAGE,getWorkflowPackage(action,actionedUponNodeRef));
    parameters.put(RMWorkflowModel.RM_MIXED_ASSIGNEES,getAssignees(action));
    parameters.put(RMWorkflowModel.RM_REQUESTED_INFORMATION,getRequestedInformation(action));
    parameters.put(RMWorkflowModel.RM_RULE_CREATOR,getRuleCreator(action));
    workflowService.startWorkflow(workflowDefinitionId,parameters);
  }
 else {
    logger.info(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
  }
}",0.991231732776618
89353,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  nodeService.getType(actionedUponNodeRef);
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + actionedUponNodeRef);
  }
  if (recordService.isRecord(actionedUponNodeRef)) {
    if (recordService.isDeclared(actionedUponNodeRef) == false) {
      ChildAssociationRef parent=nodeService.getPrimaryParent(actionedUponNodeRef);
      List<AssociationRef> refs=nodeService.getTargetAssocs(actionedUponNodeRef,ImapModel.ASSOC_IMAP_ATTACHMENT);
      if (refs.size() > 0) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"");
        }
        return;
      }
      try {
        ContentReader reader=contentService.getReader(actionedUponNodeRef,ContentModel.PROP_CONTENT);
        InputStream is=reader.getContentInputStream();
        MimeMessage mimeMessage=new MimeMessage(null,is);
        Object content=mimeMessage.getContent();
        if (content instanceof Multipart) {
          Multipart multipart=(Multipart)content;
          for (int i=0, n=multipart.getCount(); i < n; i++) {
            Part part=multipart.getBodyPart(i);
            if (""String_Node_Str"".equalsIgnoreCase(part.getDisposition())) {
              createAttachment(actionedUponNodeRef,parent.getParentRef(),part);
            }
          }
        }
      }
 catch (      Exception e) {
        throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NO_READ_MIME_MESSAGE,e.toString()),e);
      }
    }
 else {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_EMAIL_DECLARED,actionedUponNodeRef.toString()));
    }
  }
 else {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_EMAIL_NOT_RECORD,actionedUponNodeRef.toString()));
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  nodeService.getType(actionedUponNodeRef);
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + actionedUponNodeRef);
  }
  if (recordService.isRecord(actionedUponNodeRef)) {
    if (!recordService.isDeclared(actionedUponNodeRef)) {
      ChildAssociationRef parent=nodeService.getPrimaryParent(actionedUponNodeRef);
      List<AssociationRef> refs=nodeService.getTargetAssocs(actionedUponNodeRef,ImapModel.ASSOC_IMAP_ATTACHMENT);
      if (refs.size() > 0) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"");
        }
        return;
      }
      try {
        ContentReader reader=contentService.getReader(actionedUponNodeRef,ContentModel.PROP_CONTENT);
        InputStream is=reader.getContentInputStream();
        MimeMessage mimeMessage=new MimeMessage(null,is);
        Object content=mimeMessage.getContent();
        if (content instanceof Multipart) {
          Multipart multipart=(Multipart)content;
          for (int i=0, n=multipart.getCount(); i < n; i++) {
            Part part=multipart.getBodyPart(i);
            if (""String_Node_Str"".equalsIgnoreCase(part.getDisposition())) {
              createAttachment(actionedUponNodeRef,parent.getParentRef(),part);
            }
          }
        }
      }
 catch (      Exception e) {
        throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NO_READ_MIME_MESSAGE,e.toString()),e);
      }
    }
 else {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_EMAIL_DECLARED,actionedUponNodeRef.toString()));
    }
  }
 else {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_EMAIL_NOT_RECORD,actionedUponNodeRef.toString()));
  }
}",0.9974253347064882
89354,"/** 
 * Checks if the actioned upon node reference is a sub class of transfer
 * @param actionedUponNodeRef actioned upon node reference
 */
private void checkTransferSubClass(NodeRef actionedUponNodeRef){
  QName type=nodeService.getType(actionedUponNodeRef);
  if (dictionaryService.isSubClass(type,TYPE_TRANSFER) == false) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NODE_NOT_TRANSFER));
  }
}","/** 
 * Checks if the actioned upon node reference is a sub class of transfer
 * @param actionedUponNodeRef actioned upon node reference
 */
private void checkTransferSubClass(NodeRef actionedUponNodeRef){
  QName type=nodeService.getType(actionedUponNodeRef);
  if (!dictionaryService.isSubClass(type,TYPE_TRANSFER)) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NODE_NOT_TRANSFER));
  }
}",0.9878934624697336
89355,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_DISPOSITION_LIFECYCLE) && nodeService.hasAspect(actionedUponNodeRef,ASPECT_CUT_OFF)) {
    DispositionAction da=dispositionService.getLastCompletedDispostionAction(actionedUponNodeRef);
    if (da == null || da.getName().equals(""String_Node_Str"") == false) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_UNDO_NOT_LAST));
    }
    nodeService.removeAspect(actionedUponNodeRef,ASPECT_CUT_OFF);
    if (recordFolderService.isRecordFolder(actionedUponNodeRef)) {
      List<NodeRef> records=recordService.getRecords(actionedUponNodeRef);
      for (      NodeRef record : records) {
        nodeService.removeAspect(record,ASPECT_CUT_OFF);
      }
    }
    DispositionAction currentDa=dispositionService.getNextDispositionAction(actionedUponNodeRef);
    if (currentDa != null) {
      nodeService.deleteNode(currentDa.getNodeRef());
    }
    nodeService.moveNode(da.getNodeRef(),actionedUponNodeRef,ASSOC_NEXT_DISPOSITION_ACTION,ASSOC_NEXT_DISPOSITION_ACTION);
    nodeService.setProperty(da.getNodeRef(),PROP_DISPOSITION_ACTION_STARTED_AT,null);
    nodeService.setProperty(da.getNodeRef(),PROP_DISPOSITION_ACTION_STARTED_BY,null);
    nodeService.setProperty(da.getNodeRef(),PROP_DISPOSITION_ACTION_COMPLETED_AT,null);
    nodeService.setProperty(da.getNodeRef(),PROP_DISPOSITION_ACTION_COMPLETED_BY,null);
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_DISPOSITION_LIFECYCLE) && nodeService.hasAspect(actionedUponNodeRef,ASPECT_CUT_OFF)) {
    DispositionAction da=dispositionService.getLastCompletedDispostionAction(actionedUponNodeRef);
    if (da == null || !da.getName().equals(""String_Node_Str"")) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_UNDO_NOT_LAST));
    }
    nodeService.removeAspect(actionedUponNodeRef,ASPECT_CUT_OFF);
    if (recordFolderService.isRecordFolder(actionedUponNodeRef)) {
      List<NodeRef> records=recordService.getRecords(actionedUponNodeRef);
      for (      NodeRef record : records) {
        nodeService.removeAspect(record,ASPECT_CUT_OFF);
      }
    }
    DispositionAction currentDa=dispositionService.getNextDispositionAction(actionedUponNodeRef);
    if (currentDa != null) {
      nodeService.deleteNode(currentDa.getNodeRef());
    }
    nodeService.moveNode(da.getNodeRef(),actionedUponNodeRef,ASSOC_NEXT_DISPOSITION_ACTION,ASSOC_NEXT_DISPOSITION_ACTION);
    nodeService.setProperty(da.getNodeRef(),PROP_DISPOSITION_ACTION_STARTED_AT,null);
    nodeService.setProperty(da.getNodeRef(),PROP_DISPOSITION_ACTION_STARTED_BY,null);
    nodeService.setProperty(da.getNodeRef(),PROP_DISPOSITION_ACTION_COMPLETED_AT,null);
    nodeService.setProperty(da.getNodeRef(),PROP_DISPOSITION_ACTION_COMPLETED_BY,null);
  }
}",0.9969512195121952
89356,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef)) {
    if (recordService.isRecord(actionedUponNodeRef)) {
      if (recordService.isDeclared(actionedUponNodeRef) && freezeService.isFrozen(actionedUponNodeRef) == false) {
        this.nodeService.removeAspect(actionedUponNodeRef,ASPECT_DECLARED_RECORD);
      }
    }
 else {
      if (logger.isWarnEnabled()) {
        logger.warn(I18NUtil.getMessage(MSG_RECORDS_ONLY_UNDECLARED));
      }
    }
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef)) {
    if (recordService.isRecord(actionedUponNodeRef)) {
      if (recordService.isDeclared(actionedUponNodeRef) && !freezeService.isFrozen(actionedUponNodeRef)) {
        this.nodeService.removeAspect(actionedUponNodeRef,ASPECT_DECLARED_RECORD);
      }
    }
 else {
      if (logger.isWarnEnabled()) {
        logger.warn(I18NUtil.getMessage(MSG_RECORDS_ONLY_UNDECLARED));
      }
    }
  }
}",0.992867332382311
89357,"/** 
 * @param da
 * @param nodeRef
 */
private void updateEventEigible(DispositionAction da){
  List<EventCompletionDetails> events=da.getEventCompletionDetails();
  boolean eligible=false;
  if (da.getDispositionActionDefinition().eligibleOnFirstCompleteEvent() == false) {
    eligible=true;
    for (    EventCompletionDetails event : events) {
      if (event.isEventComplete() == false) {
        eligible=false;
        break;
      }
    }
  }
 else {
    for (    EventCompletionDetails event : events) {
      if (event.isEventComplete()) {
        eligible=true;
        break;
      }
    }
  }
  this.nodeService.setProperty(da.getNodeRef(),PROP_DISPOSITION_EVENTS_ELIGIBLE,eligible);
}","/** 
 * @param da
 * @param nodeRef
 */
private void updateEventEigible(DispositionAction da){
  List<EventCompletionDetails> events=da.getEventCompletionDetails();
  boolean eligible=false;
  if (!da.getDispositionActionDefinition().eligibleOnFirstCompleteEvent()) {
    eligible=true;
    for (    EventCompletionDetails event : events) {
      if (!event.isEventComplete()) {
        eligible=false;
        break;
      }
    }
  }
 else {
    for (    EventCompletionDetails event : events) {
      if (event.isEventComplete()) {
        eligible=true;
        break;
      }
    }
  }
  this.nodeService.setProperty(da.getNodeRef(),PROP_DISPOSITION_EVENTS_ELIGIBLE,eligible);
}",0.8567293777134588
89358,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.RecordsManagementAdminService#updateCustomPropertyDefinitionName(org.alfresco.service.namespace.QName,java.lang.String)
 */
public QName updateCustomPropertyDefinitionName(QName propQName,String newName) throws CustomMetadataException {
  ParameterCheck.mandatory(""String_Node_Str"",propQName);
  PropertyDefinition propDefn=dictionaryService.getProperty(propQName);
  if (propDefn == null) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_PROP_EXIST,propQName));
  }
  if (newName == null) {
    return propQName;
  }
  QName newPropQName=getQNameForClientId(newName);
  if (newPropQName != null) {
    PropertyDefinition newPropDefn=dictionaryService.getProperty(newPropQName);
    if (newPropDefn != null && propDefn.equals(newPropDefn) == false) {
      String propIdAsString=newPropQName.toPrefixString(namespaceService);
      throw new PropertyAlreadyExistsMetadataException(propIdAsString);
    }
  }
  NodeRef modelRef=getCustomModelRef(propQName.getNamespaceURI());
  M2Model deserializedModel=readCustomContentModel(modelRef);
  M2Property targetProperty=findProperty(propQName,deserializedModel);
  targetProperty.setName(new StringBuilder().append(RecordsManagementCustomModel.RM_CUSTOM_PREFIX).append(QName.NAMESPACE_PREFIX).append(newName).toString());
  targetProperty.setTitle(URLDecoder.decode(newName));
  writeCustomContentModel(modelRef,deserializedModel);
  if (logger.isInfoEnabled()) {
    logger.info(""String_Node_Str"" + propQName + ""String_Node_Str""+ newName);
  }
  return propQName;
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.RecordsManagementAdminService#updateCustomPropertyDefinitionName(org.alfresco.service.namespace.QName,java.lang.String)
 */
public QName updateCustomPropertyDefinitionName(QName propQName,String newName) throws CustomMetadataException {
  ParameterCheck.mandatory(""String_Node_Str"",propQName);
  PropertyDefinition propDefn=dictionaryService.getProperty(propQName);
  if (propDefn == null) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_PROP_EXIST,propQName));
  }
  if (newName == null) {
    return propQName;
  }
  QName newPropQName=getQNameForClientId(newName);
  if (newPropQName != null) {
    PropertyDefinition newPropDefn=dictionaryService.getProperty(newPropQName);
    if (newPropDefn != null && !propDefn.equals(newPropDefn)) {
      String propIdAsString=newPropQName.toPrefixString(namespaceService);
      throw new PropertyAlreadyExistsMetadataException(propIdAsString);
    }
  }
  NodeRef modelRef=getCustomModelRef(propQName.getNamespaceURI());
  M2Model deserializedModel=readCustomContentModel(modelRef);
  M2Property targetProperty=findProperty(propQName,deserializedModel);
  targetProperty.setName(new StringBuilder().append(RecordsManagementCustomModel.RM_CUSTOM_PREFIX).append(QName.NAMESPACE_PREFIX).append(newName).toString());
  targetProperty.setTitle(URLDecoder.decode(newName));
  writeCustomContentModel(modelRef,deserializedModel);
  if (logger.isInfoEnabled()) {
    logger.info(""String_Node_Str"" + propQName + ""String_Node_Str""+ newName);
  }
  return propQName;
}",0.9968334388853703
89359,"private M2Model readCustomContentModel(NodeRef modelNodeRef){
  ContentReader reader=this.contentService.getReader(modelNodeRef,ContentModel.TYPE_CONTENT);
  if (reader.exists() == false) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_CUSTOM_MODEL_NO_CONTENT,modelNodeRef.toString()));
  }
  InputStream contentIn=null;
  M2Model deserializedModel=null;
  try {
    contentIn=reader.getContentInputStream();
    deserializedModel=M2Model.createModel(contentIn);
  }
  finally {
    try {
      if (contentIn != null) {
        contentIn.close();
      }
    }
 catch (    IOException ignored) {
    }
  }
  return deserializedModel;
}","private M2Model readCustomContentModel(NodeRef modelNodeRef){
  ContentReader reader=this.contentService.getReader(modelNodeRef,ContentModel.TYPE_CONTENT);
  if (!reader.exists()) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_CUSTOM_MODEL_NO_CONTENT,modelNodeRef.toString()));
  }
  InputStream contentIn=null;
  M2Model deserializedModel=null;
  try {
    contentIn=reader.getContentInputStream();
    deserializedModel=M2Model.createModel(contentIn);
  }
  finally {
    try {
      if (contentIn != null) {
        contentIn.close();
      }
    }
 catch (    IOException ignored) {
    }
  }
  return deserializedModel;
}",0.992283950617284
89360,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.RecordsManagementAdminService#getCustomisable(org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public Set<QName> getCustomisable(NodeRef nodeRef){
  Set<QName> result=new HashSet<QName>(5);
  QName type=nodeService.getType(nodeRef);
  while (type != null && ContentModel.TYPE_CMOBJECT.equals(type) == false) {
    if (isCustomisable(type)) {
      result.add(type);
    }
    TypeDefinition def=dictionaryService.getType(type);
    if (def != null) {
      type=def.getParentName();
    }
 else {
      type=null;
    }
  }
  Set<QName> aspects=nodeService.getAspects(nodeRef);
  for (  QName aspect : aspects) {
    QName tempAspect=QName.createQName(aspect.toString());
    while (tempAspect != null) {
      if (isCustomisable(tempAspect)) {
        result.add(tempAspect);
      }
      AspectDefinition aspectDef=dictionaryService.getAspect(tempAspect);
      if (aspectDef != null) {
        tempAspect=aspectDef.getParentName();
      }
 else {
        tempAspect=null;
      }
    }
  }
  return result;
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.RecordsManagementAdminService#getCustomisable(org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public Set<QName> getCustomisable(NodeRef nodeRef){
  Set<QName> result=new HashSet<QName>(5);
  QName type=nodeService.getType(nodeRef);
  while (type != null && !ContentModel.TYPE_CMOBJECT.equals(type)) {
    if (isCustomisable(type)) {
      result.add(type);
    }
    TypeDefinition def=dictionaryService.getType(type);
    if (def != null) {
      type=def.getParentName();
    }
 else {
      type=null;
    }
  }
  Set<QName> aspects=nodeService.getAspects(nodeRef);
  for (  QName aspect : aspects) {
    QName tempAspect=QName.createQName(aspect.toString());
    while (tempAspect != null) {
      if (isCustomisable(tempAspect)) {
        result.add(tempAspect);
      }
      AspectDefinition aspectDef=dictionaryService.getAspect(tempAspect);
      if (aspectDef != null) {
        tempAspect=aspectDef.getParentName();
      }
 else {
        tempAspect=null;
      }
    }
  }
  return result;
}",0.9953488372093025
89361,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.RecordsManagementAdminService#removeCustomPropertyDefinition(org.alfresco.service.namespace.QName)
 */
public void removeCustomPropertyDefinition(QName propQName){
  ParameterCheck.mandatory(""String_Node_Str"",propQName);
  NodeRef modelRef=getCustomModelRef(propQName.getNamespaceURI());
  M2Model deserializedModel=readCustomContentModel(modelRef);
  String propQNameAsString=propQName.toPrefixString(namespaceService);
  String aspectName=null;
  boolean found=false;
  for (  QName customisableType : getCustomisable()) {
    aspectName=getCustomAspect(customisableType).toPrefixString(namespaceService);
    M2Aspect customPropsAspect=deserializedModel.getAspect(aspectName);
    if (customPropsAspect == null) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_UNKNOWN_ASPECT,aspectName));
    }
    M2Property prop=customPropsAspect.getProperty(propQNameAsString);
    if (prop != null) {
      if (logger.isDebugEnabled()) {
        StringBuilder msg=new StringBuilder();
        msg.append(""String_Node_Str"");
        msg.append(propQNameAsString);
        logger.debug(msg.toString());
      }
      found=true;
      customPropsAspect.removeProperty(propQNameAsString);
      break;
    }
  }
  if (found == false) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_PROP_EXIST,propQNameAsString));
  }
  writeCustomContentModel(modelRef,deserializedModel);
  if (logger.isInfoEnabled()) {
    logger.info(""String_Node_Str"" + propQNameAsString + ""String_Node_Str""+ aspectName);
  }
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.RecordsManagementAdminService#removeCustomPropertyDefinition(org.alfresco.service.namespace.QName)
 */
public void removeCustomPropertyDefinition(QName propQName){
  ParameterCheck.mandatory(""String_Node_Str"",propQName);
  NodeRef modelRef=getCustomModelRef(propQName.getNamespaceURI());
  M2Model deserializedModel=readCustomContentModel(modelRef);
  String propQNameAsString=propQName.toPrefixString(namespaceService);
  String aspectName=null;
  boolean found=false;
  for (  QName customisableType : getCustomisable()) {
    aspectName=getCustomAspect(customisableType).toPrefixString(namespaceService);
    M2Aspect customPropsAspect=deserializedModel.getAspect(aspectName);
    if (customPropsAspect == null) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_UNKNOWN_ASPECT,aspectName));
    }
    M2Property prop=customPropsAspect.getProperty(propQNameAsString);
    if (prop != null) {
      if (logger.isDebugEnabled()) {
        StringBuilder msg=new StringBuilder();
        msg.append(""String_Node_Str"");
        msg.append(propQNameAsString);
        logger.debug(msg.toString());
      }
      found=true;
      customPropsAspect.removeProperty(propQNameAsString);
      break;
    }
  }
  if (!found) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_PROP_EXIST,propQNameAsString));
  }
  writeCustomContentModel(modelRef,deserializedModel);
  if (logger.isInfoEnabled()) {
    logger.info(""String_Node_Str"" + propQNameAsString + ""String_Node_Str""+ aspectName);
  }
}",0.9936467598475224
89362,"public void changeCustomConstraintValues(QName constraintName,List<String> newAllowedValues){
  ParameterCheck.mandatory(""String_Node_Str"",constraintName);
  ParameterCheck.mandatory(""String_Node_Str"",newAllowedValues);
  NodeRef modelRef=getCustomModelRef(constraintName.getNamespaceURI());
  M2Model deserializedModel=readCustomContentModel(modelRef);
  String constraintNameAsPrefixString=constraintName.toPrefixString(namespaceService);
  M2Constraint customConstraint=deserializedModel.getConstraint(constraintNameAsPrefixString);
  if (customConstraint == null) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_CANNOT_FIND_CONSTRAINT,constraintNameAsPrefixString));
  }
  String type=customConstraint.getType();
  if (type == null || (type.equals(CUSTOM_CONSTRAINT_TYPE) == false && type.equals(CAPATIBILITY_CUSTOM_CONTRAINT_TYPE) == false)) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_UNEXPECTED_TYPE_CONSTRAINT,type,constraintNameAsPrefixString,CUSTOM_CONSTRAINT_TYPE));
  }
  customConstraint.removeParameter(PARAM_ALLOWED_VALUES);
  customConstraint.createParameter(PARAM_ALLOWED_VALUES,newAllowedValues);
  writeCustomContentModel(modelRef,deserializedModel);
  if (logger.isInfoEnabled()) {
    logger.info(""String_Node_Str"" + constraintNameAsPrefixString + ""String_Node_Str""+ newAllowedValues.size()+ ""String_Node_Str"");
  }
}","public void changeCustomConstraintValues(QName constraintName,List<String> newAllowedValues){
  ParameterCheck.mandatory(""String_Node_Str"",constraintName);
  ParameterCheck.mandatory(""String_Node_Str"",newAllowedValues);
  NodeRef modelRef=getCustomModelRef(constraintName.getNamespaceURI());
  M2Model deserializedModel=readCustomContentModel(modelRef);
  String constraintNameAsPrefixString=constraintName.toPrefixString(namespaceService);
  M2Constraint customConstraint=deserializedModel.getConstraint(constraintNameAsPrefixString);
  if (customConstraint == null) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_CANNOT_FIND_CONSTRAINT,constraintNameAsPrefixString));
  }
  String type=customConstraint.getType();
  if (type == null || (!type.equals(CUSTOM_CONSTRAINT_TYPE) && !type.equals(CAPATIBILITY_CUSTOM_CONTRAINT_TYPE))) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_UNEXPECTED_TYPE_CONSTRAINT,type,constraintNameAsPrefixString,CUSTOM_CONSTRAINT_TYPE));
  }
  customConstraint.removeParameter(PARAM_ALLOWED_VALUES);
  customConstraint.createParameter(PARAM_ALLOWED_VALUES,newAllowedValues);
  writeCustomContentModel(modelRef,deserializedModel);
  if (logger.isInfoEnabled()) {
    logger.info(""String_Node_Str"" + constraintNameAsPrefixString + ""String_Node_Str""+ newAllowedValues.size()+ ""String_Node_Str"");
  }
}",0.9926900584795322
89363,"/** 
 * Make sure any custom property aspects are applied to newly created nodes.
 * @see org.alfresco.repo.node.NodeServicePolicies.OnCreateNodePolicy#onCreateNode(org.alfresco.service.cmr.repository.ChildAssociationRef)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,isService=true,notificationFrequency=NotificationFrequency.FIRST_EVENT) public void onCreateNode(final ChildAssociationRef childAssocRef){
  AuthenticationUtil.runAs(new RunAsWork<Void>(){
    @Override public Void doWork() throws Exception {
      if (dictionaryService.getAllModels().contains(RecordsManagementCustomModel.RM_CUSTOM_MODEL)) {
        NodeRef nodeRef=childAssocRef.getChildRef();
        QName type=nodeService.getType(nodeRef);
        while (type != null && ContentModel.TYPE_CMOBJECT.equals(type) == false) {
          if (isCustomisable(type)) {
            QName customPropertyAspect=getCustomAspect(type);
            nodeService.addAspect(nodeRef,customPropertyAspect,null);
          }
          TypeDefinition def=dictionaryService.getType(type);
          if (def != null) {
            type=def.getParentName();
          }
 else {
            type=null;
          }
        }
      }
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}","/** 
 * Make sure any custom property aspects are applied to newly created nodes.
 * @see org.alfresco.repo.node.NodeServicePolicies.OnCreateNodePolicy#onCreateNode(org.alfresco.service.cmr.repository.ChildAssociationRef)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,isService=true,notificationFrequency=NotificationFrequency.FIRST_EVENT) public void onCreateNode(final ChildAssociationRef childAssocRef){
  AuthenticationUtil.runAs(new RunAsWork<Void>(){
    @Override public Void doWork() throws Exception {
      if (dictionaryService.getAllModels().contains(RecordsManagementCustomModel.RM_CUSTOM_MODEL)) {
        NodeRef nodeRef=childAssocRef.getChildRef();
        QName type=nodeService.getType(nodeRef);
        while (type != null && !ContentModel.TYPE_CMOBJECT.equals(type)) {
          if (isCustomisable(type)) {
            QName customPropertyAspect=getCustomAspect(type);
            nodeService.addAspect(nodeRef,customPropertyAspect,null);
          }
          TypeDefinition def=dictionaryService.getType(type);
          if (def != null) {
            type=def.getParentName();
          }
 else {
            type=null;
          }
        }
      }
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}",0.9960095770151636
89364,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.RecordsManagementAdminService#addCustomPropertyDefinition(org.alfresco.service.namespace.QName,org.alfresco.service.namespace.QName,java.lang.String,org.alfresco.service.namespace.QName,java.lang.String,java.lang.String,java.lang.String,boolean,boolean,boolean,org.alfresco.service.namespace.QName)
 */
public QName addCustomPropertyDefinition(QName propId,QName aspectName,String label,QName dataType,String title,String description,String defaultValue,boolean multiValued,boolean mandatory,boolean isProtected,QName lovConstraint) throws CustomMetadataException {
  if (isCustomisable(aspectName) == false) {
    throw new NotCustomisableMetadataException(aspectName.toPrefixString(namespaceService));
  }
  if (propId == null) {
    propId=this.generateQNameFor(label);
  }
  ParameterCheck.mandatory(""String_Node_Str"",aspectName);
  ParameterCheck.mandatory(""String_Node_Str"",label);
  ParameterCheck.mandatory(""String_Node_Str"",dataType);
  NodeRef modelRef=getCustomModelRef(propId.getNamespaceURI());
  M2Model deserializedModel=readCustomContentModel(modelRef);
  QName customAspect=getCustomAspect(aspectName);
  M2Aspect customPropsAspect=deserializedModel.getAspect(customAspect.toPrefixString(namespaceService));
  if (customPropsAspect == null) {
    throw new InvalidCustomAspectMetadataException(customAspect,aspectName.toPrefixString(namespaceService));
  }
  String propIdAsString=propId.toPrefixString(namespaceService);
  M2Property customProp=customPropsAspect.getProperty(propIdAsString);
  if (customProp != null) {
    throw new PropertyAlreadyExistsMetadataException(propIdAsString);
  }
  M2Property newProp=customPropsAspect.createProperty(propIdAsString);
  newProp.setName(propIdAsString);
  newProp.setType(dataType.toPrefixString(namespaceService));
  newProp.setTitle(label);
  newProp.setDescription(description);
  newProp.setDefaultValue(defaultValue);
  newProp.setMandatory(mandatory);
  newProp.setProtected(isProtected);
  newProp.setMultiValued(multiValued);
  newProp.setIndexed(true);
  newProp.setIndexedAtomically(true);
  newProp.setStoredInIndex(false);
  newProp.setIndexTokenisationMode(IndexTokenisationMode.FALSE);
  if (lovConstraint != null) {
    if (!dataType.equals(DataTypeDefinition.TEXT)) {
      throw new CannotApplyConstraintMetadataException(lovConstraint,propIdAsString,dataType);
    }
    String lovConstraintQNameAsString=lovConstraint.toPrefixString(namespaceService);
    newProp.addConstraintRef(lovConstraintQNameAsString);
  }
  writeCustomContentModel(modelRef,deserializedModel);
  if (logger.isInfoEnabled()) {
    logger.info(""String_Node_Str"" + label + ""String_Node_Str""+ propIdAsString+ ""String_Node_Str""+ aspectName);
  }
  return propId;
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.RecordsManagementAdminService#addCustomPropertyDefinition(org.alfresco.service.namespace.QName,org.alfresco.service.namespace.QName,java.lang.String,org.alfresco.service.namespace.QName,java.lang.String,java.lang.String,java.lang.String,boolean,boolean,boolean,org.alfresco.service.namespace.QName)
 */
public QName addCustomPropertyDefinition(QName propId,QName aspectName,String label,QName dataType,String title,String description,String defaultValue,boolean multiValued,boolean mandatory,boolean isProtected,QName lovConstraint) throws CustomMetadataException {
  if (!isCustomisable(aspectName)) {
    throw new NotCustomisableMetadataException(aspectName.toPrefixString(namespaceService));
  }
  if (propId == null) {
    propId=this.generateQNameFor(label);
  }
  ParameterCheck.mandatory(""String_Node_Str"",aspectName);
  ParameterCheck.mandatory(""String_Node_Str"",label);
  ParameterCheck.mandatory(""String_Node_Str"",dataType);
  NodeRef modelRef=getCustomModelRef(propId.getNamespaceURI());
  M2Model deserializedModel=readCustomContentModel(modelRef);
  QName customAspect=getCustomAspect(aspectName);
  M2Aspect customPropsAspect=deserializedModel.getAspect(customAspect.toPrefixString(namespaceService));
  if (customPropsAspect == null) {
    throw new InvalidCustomAspectMetadataException(customAspect,aspectName.toPrefixString(namespaceService));
  }
  String propIdAsString=propId.toPrefixString(namespaceService);
  M2Property customProp=customPropsAspect.getProperty(propIdAsString);
  if (customProp != null) {
    throw new PropertyAlreadyExistsMetadataException(propIdAsString);
  }
  M2Property newProp=customPropsAspect.createProperty(propIdAsString);
  newProp.setName(propIdAsString);
  newProp.setType(dataType.toPrefixString(namespaceService));
  newProp.setTitle(label);
  newProp.setDescription(description);
  newProp.setDefaultValue(defaultValue);
  newProp.setMandatory(mandatory);
  newProp.setProtected(isProtected);
  newProp.setMultiValued(multiValued);
  newProp.setIndexed(true);
  newProp.setIndexedAtomically(true);
  newProp.setStoredInIndex(false);
  newProp.setIndexTokenisationMode(IndexTokenisationMode.FALSE);
  if (lovConstraint != null) {
    if (!dataType.equals(DataTypeDefinition.TEXT)) {
      throw new CannotApplyConstraintMetadataException(lovConstraint,propIdAsString,dataType);
    }
    String lovConstraintQNameAsString=lovConstraint.toPrefixString(namespaceService);
    newProp.addConstraintRef(lovConstraintQNameAsString);
  }
  writeCustomContentModel(modelRef,deserializedModel);
  if (logger.isInfoEnabled()) {
    logger.info(""String_Node_Str"" + label + ""String_Node_Str""+ propIdAsString+ ""String_Node_Str""+ aspectName);
  }
  return propId;
}",0.998194293968942
89365,"@Override public int hashCode(){
  int var_code=(null == name ? 0 : name.hashCode());
  return 31 + var_code;
}","@Override public int hashCode(){
  int varCode=(null == name ? 0 : name.hashCode());
  return 31 + varCode;
}",0.9727272727272728
89366,"/** 
 * This method examines the old and new property sets and for those properties which have changed, looks for script resources corresponding to those properties. Those scripts are then called via the ScriptService.
 * @param nodeWithChangedProperties the node whose properties have changed.
 * @param oldProps the old properties and their values.
 * @param newProps the new properties and their values.
 * @see #lookupScripts(Map<QName, Serializable>, Map<QName, Serializable>)
 */
private void lookupAndExecuteScripts(NodeRef nodeWithChangedProperties,Map<QName,Serializable> oldProps,Map<QName,Serializable> newProps){
  List<NodeRef> scriptRefs=lookupScripts(oldProps,newProps);
  Map<String,Object> objectModel=new HashMap<String,Object>(1);
  objectModel.put(""String_Node_Str"",nodeWithChangedProperties);
  objectModel.put(""String_Node_Str"",oldProps);
  objectModel.put(""String_Node_Str"",newProps);
  ;
  for (  NodeRef scriptRef : scriptRefs) {
    scriptService.executeScript(scriptRef,null,objectModel);
  }
}","/** 
 * This method examines the old and new property sets and for those properties which have changed, looks for script resources corresponding to those properties. Those scripts are then called via the ScriptService.
 * @param nodeWithChangedProperties the node whose properties have changed.
 * @param oldProps the old properties and their values.
 * @param newProps the new properties and their values.
 * @see #lookupScripts(Map<QName, Serializable>, Map<QName, Serializable>)
 */
private void lookupAndExecuteScripts(NodeRef nodeWithChangedProperties,Map<QName,Serializable> oldProps,Map<QName,Serializable> newProps){
  List<NodeRef> scriptRefs=lookupScripts(oldProps,newProps);
  Map<String,Object> objectModel=new HashMap<String,Object>(1);
  objectModel.put(""String_Node_Str"",nodeWithChangedProperties);
  objectModel.put(""String_Node_Str"",oldProps);
  objectModel.put(""String_Node_Str"",newProps);
  for (  NodeRef scriptRef : scriptRefs) {
    scriptService.executeScript(scriptRef,null,objectModel);
  }
}",0.998037291462218
89367,"@Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  Map<String,Object> model=new HashMap<String,Object>();
  Map<String,String> templateVars=req.getServiceMatch().getTemplateVars();
  String refId=templateVars.get(REF_ID);
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + String.valueOf(refId));
  }
  Map<QName,AssociationDefinition> currentCustomRefs=rmAdminService.getCustomReferenceDefinitions();
  if (refId != null) {
    QName qn=rmAdminService.getQNameForClientId(refId);
    AssociationDefinition assDef=currentCustomRefs.get(qn);
    if (assDef == null) {
      StringBuilder msg=new StringBuilder();
      msg.append(""String_Node_Str"").append(refId);
      if (logger.isDebugEnabled()) {
        logger.debug(msg.toString());
      }
      throw new WebScriptException(HttpServletResponse.SC_NOT_FOUND,msg.toString());
    }
    currentCustomRefs=new HashMap<QName,AssociationDefinition>(1);
    currentCustomRefs.put(qn,assDef);
  }
  List<Map<String,String>> listOfReferenceData=new ArrayList<Map<String,String>>();
  for (  Entry<QName,AssociationDefinition> entry : currentCustomRefs.entrySet()) {
    Map<String,String> data=new HashMap<String,String>();
    AssociationDefinition nextValue=entry.getValue();
    CustomReferenceType referenceType=nextValue instanceof ChildAssociationDefinition ? CustomReferenceType.PARENT_CHILD : CustomReferenceType.BIDIRECTIONAL;
    data.put(REFERENCE_TYPE,referenceType.toString());
    String nextTitle=nextValue.getTitle(dictionaryService);
    if (CustomReferenceType.PARENT_CHILD.equals(referenceType)) {
      if (nextTitle != null) {
        String[] sourceAndTarget=rmAdminService.splitSourceTargetId(nextTitle);
        data.put(SOURCE,sourceAndTarget[0]);
        data.put(TARGET,sourceAndTarget[1]);
        data.put(REF_ID,entry.getKey().getLocalName());
      }
    }
 else     if (CustomReferenceType.BIDIRECTIONAL.equals(referenceType)) {
      if (nextTitle != null) {
        data.put(LABEL,nextTitle);
        data.put(REF_ID,entry.getKey().getLocalName());
      }
    }
 else {
      throw new WebScriptException(""String_Node_Str"" + referenceType);
    }
    listOfReferenceData.add(data);
  }
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + listOfReferenceData.size());
  }
  model.put(CUSTOM_REFS,listOfReferenceData);
  return model;
}","@Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  Map<String,Object> model=new HashMap<String,Object>();
  Map<String,String> templateVars=req.getServiceMatch().getTemplateVars();
  String refId=templateVars.get(REF_ID);
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + refId);
  }
  Map<QName,AssociationDefinition> currentCustomRefs=rmAdminService.getCustomReferenceDefinitions();
  if (refId != null) {
    QName qn=rmAdminService.getQNameForClientId(refId);
    AssociationDefinition assDef=currentCustomRefs.get(qn);
    if (assDef == null) {
      StringBuilder msg=new StringBuilder();
      msg.append(""String_Node_Str"").append(refId);
      if (logger.isDebugEnabled()) {
        logger.debug(msg.toString());
      }
      throw new WebScriptException(HttpServletResponse.SC_NOT_FOUND,msg.toString());
    }
    currentCustomRefs=new HashMap<QName,AssociationDefinition>(1);
    currentCustomRefs.put(qn,assDef);
  }
  List<Map<String,String>> listOfReferenceData=new ArrayList<Map<String,String>>();
  for (  Entry<QName,AssociationDefinition> entry : currentCustomRefs.entrySet()) {
    Map<String,String> data=new HashMap<String,String>();
    AssociationDefinition nextValue=entry.getValue();
    CustomReferenceType referenceType=nextValue instanceof ChildAssociationDefinition ? CustomReferenceType.PARENT_CHILD : CustomReferenceType.BIDIRECTIONAL;
    data.put(REFERENCE_TYPE,referenceType.toString());
    String nextTitle=nextValue.getTitle(dictionaryService);
    if (CustomReferenceType.PARENT_CHILD.equals(referenceType)) {
      if (nextTitle != null) {
        String[] sourceAndTarget=rmAdminService.splitSourceTargetId(nextTitle);
        data.put(SOURCE,sourceAndTarget[0]);
        data.put(TARGET,sourceAndTarget[1]);
        data.put(REF_ID,entry.getKey().getLocalName());
      }
    }
 else     if (CustomReferenceType.BIDIRECTIONAL.equals(referenceType)) {
      if (nextTitle != null) {
        data.put(LABEL,nextTitle);
        data.put(REF_ID,entry.getKey().getLocalName());
      }
    }
 else {
      throw new WebScriptException(""String_Node_Str"" + referenceType);
    }
    listOfReferenceData.add(data);
  }
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + listOfReferenceData.size());
  }
  model.put(CUSTOM_REFS,listOfReferenceData);
  return model;
}",0.996658312447786
89368,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.security.RecordsManagementSecurityService#existsRole(java.lang.String)
 */
public boolean existsRole(final NodeRef rmRootNode,final String role){
  return AuthenticationUtil.runAs(new AuthenticationUtil.RunAsWork<Boolean>(){
    public Boolean doWork() throws Exception {
      String fullRoleName=authorityService.getName(AuthorityType.GROUP,getFullRoleName(role,rmRootNode));
      String zone=getZoneName(rmRootNode);
      Set<String> roles=authorityService.getAllAuthoritiesInZone(zone,AuthorityType.GROUP);
      return new Boolean(roles.contains(fullRoleName));
    }
  }
,AuthenticationUtil.getSystemUserName()).booleanValue();
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.security.RecordsManagementSecurityService#existsRole(java.lang.String)
 */
public boolean existsRole(final NodeRef rmRootNode,final String role){
  return AuthenticationUtil.runAs(new AuthenticationUtil.RunAsWork<Boolean>(){
    public Boolean doWork() throws Exception {
      String fullRoleName=authorityService.getName(AuthorityType.GROUP,getFullRoleName(role,rmRootNode));
      String zone=getZoneName(rmRootNode);
      Set<String> roles=authorityService.getAllAuthoritiesInZone(zone,AuthorityType.GROUP);
      return Boolean.valueOf(roles.contains(fullRoleName));
    }
  }
,AuthenticationUtil.getSystemUserName()).booleanValue();
}",0.9913669064748202
89369,"/** 
 * @see org.springframework.extensions.webscripts.DeclarativeWebScript#executeImpl(org.springframework.extensions.webscripts.WebScriptRequest,org.springframework.extensions.webscripts.Status,org.springframework.extensions.webscripts.Cache)
 */
@Override protected Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  String siteName=req.getParameter(ARG_SITE_NAME);
  if (StringUtils.isBlank(siteName)) {
    siteName=RmSiteType.DEFAULT_SITE_NAME;
  }
  NodeRef filePlan=siteService.getContainer(siteName,RmSiteType.COMPONENT_DOCUMENT_LIBRARY);
  String unloadedOnlyParam=req.getParameter(ARG_UNLOADED_ONLY);
  boolean unloadedOnly=false;
  if (StringUtils.isNotBlank(unloadedOnlyParam)) {
    unloadedOnly=new Boolean(unloadedOnlyParam).booleanValue();
  }
  Map<String,DataSet> dataSets=dataSetService.getDataSets(filePlan,unloadedOnly);
  List<Map<String,String>> dataSetList=new ArrayList<Map<String,String>>(dataSets.size());
  for (  Map.Entry<String,DataSet> entry : dataSets.entrySet()) {
    Map<String,String> dataSet=new HashMap<String,String>(3);
    DataSet value=entry.getValue();
    String dataSetId=value.getId();
    String isLoaded=String.valueOf(dataSetService.isLoadedDataSet(filePlan,dataSetId));
    dataSet.put(""String_Node_Str"",value.getLabel());
    dataSet.put(""String_Node_Str"",dataSetId);
    dataSet.put(""String_Node_Str"",isLoaded);
    dataSetList.add(dataSet);
  }
  Map<String,Object> model=new HashMap<String,Object>(1);
  model.put(""String_Node_Str"",dataSetList);
  return model;
}","/** 
 * @see org.springframework.extensions.webscripts.DeclarativeWebScript#executeImpl(org.springframework.extensions.webscripts.WebScriptRequest,org.springframework.extensions.webscripts.Status,org.springframework.extensions.webscripts.Cache)
 */
@Override protected Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  String siteName=req.getParameter(ARG_SITE_NAME);
  if (StringUtils.isBlank(siteName)) {
    siteName=RmSiteType.DEFAULT_SITE_NAME;
  }
  NodeRef filePlan=siteService.getContainer(siteName,RmSiteType.COMPONENT_DOCUMENT_LIBRARY);
  String unloadedOnlyParam=req.getParameter(ARG_UNLOADED_ONLY);
  boolean unloadedOnly=false;
  if (StringUtils.isNotBlank(unloadedOnlyParam)) {
    unloadedOnly=Boolean.valueOf(unloadedOnlyParam).booleanValue();
  }
  Map<String,DataSet> dataSets=dataSetService.getDataSets(filePlan,unloadedOnly);
  List<Map<String,String>> dataSetList=new ArrayList<Map<String,String>>(dataSets.size());
  for (  Map.Entry<String,DataSet> entry : dataSets.entrySet()) {
    Map<String,String> dataSet=new HashMap<String,String>(3);
    DataSet value=entry.getValue();
    String dataSetId=value.getId();
    String isLoaded=String.valueOf(dataSetService.isLoadedDataSet(filePlan,dataSetId));
    dataSet.put(""String_Node_Str"",value.getLabel());
    dataSet.put(""String_Node_Str"",dataSetId);
    dataSet.put(""String_Node_Str"",isLoaded);
    dataSetList.add(dataSet);
  }
  Map<String,Object> model=new HashMap<String,Object>(1);
  model.put(""String_Node_Str"",dataSetList);
  return model;
}",0.9961340206185568
89370,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#init()
 */
@Override public void init(){
  if (this instanceof RecordsManagementAction == false) {
    super.init();
  }
  if (auditable == true) {
    getAuditService().registerAuditEvent(this.getActionDefinition().getName(),this.getActionDefinition().getTitle());
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#init()
 */
@Override public void init(){
  if (this instanceof RecordsManagementAction == false) {
    super.init();
  }
  if (auditable) {
    getAuditService().registerAuditEvent(this.getActionDefinition().getName(),this.getActionDefinition().getTitle());
  }
}",0.9882697947214076
89371,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#execute(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public void execute(Action action,NodeRef actionedUponNodeRef){
  if (auditable == true) {
    if (auditedImmediately == true) {
      getAuditService().auditEvent(actionedUponNodeRef,this.getActionDefinition().getName(),null,null,true);
    }
 else {
      getAuditService().auditEvent(actionedUponNodeRef,this.getActionDefinition().getName());
    }
  }
  super.execute(action,actionedUponNodeRef);
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#execute(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public void execute(Action action,NodeRef actionedUponNodeRef){
  if (auditable) {
    if (auditedImmediately) {
      getAuditService().auditEvent(actionedUponNodeRef,this.getActionDefinition().getName(),null,null,true);
    }
 else {
      getAuditService().auditEvent(actionedUponNodeRef,this.getActionDefinition().getName());
    }
  }
  super.execute(action,actionedUponNodeRef);
}",0.9860627177700348
89372,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#execute(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public void execute(Action action,NodeRef actionedUponNodeRef){
  if (allowParameterSubstitutions == true) {
    parameterProcessorComponent.process(action,getActionDefinition(),actionedUponNodeRef);
  }
  super.execute(action,actionedUponNodeRef);
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#execute(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public void execute(Action action,NodeRef actionedUponNodeRef){
  if (allowParameterSubstitutions) {
    parameterProcessorComponent.process(action,getActionDefinition(),actionedUponNodeRef);
  }
  super.execute(action,actionedUponNodeRef);
}",0.9906103286384976
89373,"/** 
 * Calculates and updates the <code>rma:dispositionEventsEligible</code> property for the given next disposition action.
 * @param nextAction The next disposition action
 * @return The result of calculation
 */
protected boolean updateEventEligible(DispositionAction nextAction){
  List<EventCompletionDetails> events=nextAction.getEventCompletionDetails();
  boolean eligible=false;
  if (nextAction.getDispositionActionDefinition().eligibleOnFirstCompleteEvent() == false) {
    eligible=true;
    for (    EventCompletionDetails event : events) {
      if (event.isEventComplete() == false) {
        eligible=false;
        break;
      }
    }
  }
 else {
    for (    EventCompletionDetails event : events) {
      if (event.isEventComplete() == true) {
        eligible=true;
        break;
      }
    }
  }
  this.nodeService.setProperty(nextAction.getNodeRef(),PROP_DISPOSITION_EVENTS_ELIGIBLE,eligible);
  return eligible;
}","/** 
 * Calculates and updates the <code>rma:dispositionEventsEligible</code> property for the given next disposition action.
 * @param nextAction The next disposition action
 * @return The result of calculation
 */
protected boolean updateEventEligible(DispositionAction nextAction){
  List<EventCompletionDetails> events=nextAction.getEventCompletionDetails();
  boolean eligible=false;
  if (nextAction.getDispositionActionDefinition().eligibleOnFirstCompleteEvent() == false) {
    eligible=true;
    for (    EventCompletionDetails event : events) {
      if (event.isEventComplete() == false) {
        eligible=false;
        break;
      }
    }
  }
 else {
    for (    EventCompletionDetails event : events) {
      if (event.isEventComplete()) {
        eligible=true;
        break;
      }
    }
  }
  this.nodeService.setProperty(nextAction.getNodeRef(),PROP_DISPOSITION_EVENTS_ELIGIBLE,eligible);
  return eligible;
}",0.9957264957264956
89374,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  NodeRef nextDispositionActionNodeRef=getNextDispostionAction(actionedUponNodeRef);
  boolean checkError=true;
  Boolean checkErrorValue=(Boolean)action.getParameterValue(PARAM_NO_ERROR_CHECK);
  if (checkErrorValue != null) {
    checkError=checkErrorValue.booleanValue();
  }
  DispositionSchedule di=checkDispositionActionExecutionValidity(actionedUponNodeRef,nextDispositionActionNodeRef,checkError);
  if (di != null) {
    if (checkEligibility(actionedUponNodeRef) == false || dispositionService.isNextDispositionActionEligible(actionedUponNodeRef) == true) {
      if (di.isRecordLevelDisposition() == true) {
        if (recordService.isRecord(actionedUponNodeRef) == true) {
          if (recordService.isDeclared(actionedUponNodeRef) == true) {
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_AT,new Date());
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_BY,AuthenticationUtil.getRunAsUser());
            executeRecordLevelDisposition(action,actionedUponNodeRef);
            if (nodeService.exists(nextDispositionActionNodeRef) == true && getSetDispositionActionComplete() == true) {
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_AT,new Date());
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_BY,AuthenticationUtil.getRunAsUser());
            }
          }
 else {
            throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_RECORD_NOT_DECLARED,getName(),actionedUponNodeRef.toString()));
          }
        }
 else {
          throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_EXPECTED_RECORD_LEVEL,getName(),actionedUponNodeRef.toString()));
        }
      }
 else {
        if (recordFolderService.isRecordFolder(actionedUponNodeRef) == true) {
          if (recordFolderService.isRecordFolderDeclared(actionedUponNodeRef) == true) {
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_AT,new Date());
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_BY,AuthenticationUtil.getRunAsUser());
            executeRecordFolderLevelDisposition(action,actionedUponNodeRef);
            if (nodeService.exists(nextDispositionActionNodeRef) == true && getSetDispositionActionComplete() == true) {
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_AT,new Date());
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_BY,AuthenticationUtil.getRunAsUser());
            }
          }
 else {
            throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_ALL_RECORDS_DECLARED,getName(),actionedUponNodeRef.toString()));
          }
        }
 else {
          throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_RECORD_FOLDER,getName(),actionedUponNodeRef.toString()));
        }
      }
      if (nodeService.exists(actionedUponNodeRef) == true && getSetDispositionActionComplete() == true) {
        dispositionService.updateNextDispositionAction(actionedUponNodeRef);
      }
    }
 else {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_ELIGIBLE,getName(),actionedUponNodeRef.toString()));
    }
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  NodeRef nextDispositionActionNodeRef=getNextDispostionAction(actionedUponNodeRef);
  boolean checkError=true;
  Boolean checkErrorValue=(Boolean)action.getParameterValue(PARAM_NO_ERROR_CHECK);
  if (checkErrorValue != null) {
    checkError=checkErrorValue.booleanValue();
  }
  DispositionSchedule di=checkDispositionActionExecutionValidity(actionedUponNodeRef,nextDispositionActionNodeRef,checkError);
  if (di != null) {
    if (checkEligibility(actionedUponNodeRef) == false || dispositionService.isNextDispositionActionEligible(actionedUponNodeRef)) {
      if (di.isRecordLevelDisposition()) {
        if (recordService.isRecord(actionedUponNodeRef)) {
          if (recordService.isDeclared(actionedUponNodeRef)) {
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_AT,new Date());
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_BY,AuthenticationUtil.getRunAsUser());
            executeRecordLevelDisposition(action,actionedUponNodeRef);
            if (nodeService.exists(nextDispositionActionNodeRef) && getSetDispositionActionComplete()) {
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_AT,new Date());
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_BY,AuthenticationUtil.getRunAsUser());
            }
          }
 else {
            throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_RECORD_NOT_DECLARED,getName(),actionedUponNodeRef.toString()));
          }
        }
 else {
          throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_EXPECTED_RECORD_LEVEL,getName(),actionedUponNodeRef.toString()));
        }
      }
 else {
        if (recordFolderService.isRecordFolder(actionedUponNodeRef)) {
          if (recordFolderService.isRecordFolderDeclared(actionedUponNodeRef)) {
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_AT,new Date());
            nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_STARTED_BY,AuthenticationUtil.getRunAsUser());
            executeRecordFolderLevelDisposition(action,actionedUponNodeRef);
            if (nodeService.exists(nextDispositionActionNodeRef) && getSetDispositionActionComplete()) {
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_AT,new Date());
              nodeService.setProperty(nextDispositionActionNodeRef,PROP_DISPOSITION_ACTION_COMPLETED_BY,AuthenticationUtil.getRunAsUser());
            }
          }
 else {
            throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_ALL_RECORDS_DECLARED,getName(),actionedUponNodeRef.toString()));
          }
        }
 else {
          throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_RECORD_FOLDER,getName(),actionedUponNodeRef.toString()));
        }
      }
      if (nodeService.exists(actionedUponNodeRef) && getSetDispositionActionComplete()) {
        dispositionService.updateNextDispositionAction(actionedUponNodeRef);
      }
    }
 else {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_ELIGIBLE,getName(),actionedUponNodeRef.toString()));
    }
  }
}",0.9866071428571428
89375,"/** 
 * @param nodeRef
 * @return
 */
protected DispositionSchedule checkDispositionActionExecutionValidity(NodeRef nodeRef,NodeRef nextDispositionActionNodeRef,boolean throwError){
  DispositionSchedule di=dispositionService.getDispositionSchedule(nodeRef);
  if (di == null) {
    if (throwError) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NO_DISPOITION_INSTRUCTIONS,getName(),nodeRef.toString()));
    }
 else {
      return null;
    }
  }
  if (nodeService.hasAspect(nodeRef,ASPECT_DISPOSITION_LIFECYCLE) == false) {
    if (throwError) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NO_DIS_LIFECYCLE_SET,getName(),nodeRef.toString()));
    }
 else {
      return null;
    }
  }
  if (checkNextDispositionAction(nodeRef) == true) {
    NodeRef nextDispositionAction=nextDispositionActionNodeRef;
    if (nextDispositionAction == null) {
      if (throwError) {
        throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NEXT_DISP_NOT_SET,getName(),nodeRef.toString()));
      }
 else {
        return null;
      }
    }
    String actionName=(String)nodeService.getProperty(nextDispositionAction,PROP_DISPOSITION_ACTION);
    if (actionName == null || actionName.equals(getName()) == false) {
      if (throwError) {
        throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_NEXT_DISP,getName(),nodeRef.toString()));
      }
 else {
        return null;
      }
    }
  }
  return di;
}","/** 
 * @param nodeRef
 * @return
 */
protected DispositionSchedule checkDispositionActionExecutionValidity(NodeRef nodeRef,NodeRef nextDispositionActionNodeRef,boolean throwError){
  DispositionSchedule di=dispositionService.getDispositionSchedule(nodeRef);
  if (di == null) {
    if (throwError) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NO_DISPOITION_INSTRUCTIONS,getName(),nodeRef.toString()));
    }
 else {
      return null;
    }
  }
  if (nodeService.hasAspect(nodeRef,ASPECT_DISPOSITION_LIFECYCLE) == false) {
    if (throwError) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NO_DIS_LIFECYCLE_SET,getName(),nodeRef.toString()));
    }
 else {
      return null;
    }
  }
  if (checkNextDispositionAction(nodeRef)) {
    NodeRef nextDispositionAction=nextDispositionActionNodeRef;
    if (nextDispositionAction == null) {
      if (throwError) {
        throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NEXT_DISP_NOT_SET,getName(),nodeRef.toString()));
      }
 else {
        return null;
      }
    }
    String actionName=(String)nodeService.getProperty(nextDispositionAction,PROP_DISPOSITION_ACTION);
    if (actionName == null || actionName.equals(getName()) == false) {
      if (throwError) {
        throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NOT_NEXT_DISP,getName(),nodeRef.toString()));
      }
 else {
        return null;
      }
    }
  }
  return di;
}",0.997248968363136
89376,"/** 
 * @see org.quartz.Job#execute(org.quartz.JobExecutionContext)
 */
public void execute(JobExecutionContext context) throws JobExecutionException {
  RecordsManagementActionService rmActionService=(RecordsManagementActionService)context.getJobDetail().getJobDataMap().get(""String_Node_Str"");
  NodeService nodeService=(NodeService)context.getJobDetail().getJobDataMap().get(""String_Node_Str"");
  Calendar cal=Calendar.getInstance();
  String year=String.valueOf(cal.get(Calendar.YEAR));
  String month=String.valueOf(cal.get(Calendar.MONTH) + 1);
  String dayOfMonth=String.valueOf(cal.get(Calendar.DAY_OF_MONTH));
  final String currentDate=padString(year,2) + ""String_Node_Str"" + padString(month,2)+ ""String_Node_Str""+ padString(dayOfMonth,2)+ ""String_Node_Str"";
  if (logger.isDebugEnabled() == true) {
    StringBuilder msg=new StringBuilder();
    msg.append(""String_Node_Str"").append(this.getClass().getSimpleName()).append(""String_Node_Str"").append(currentDate);
    logger.debug(msg.toString());
  }
  String dateRange=""String_Node_Str"" + currentDate + ""String_Node_Str"";
  String query=""String_Node_Str"" + dateRange;
  SearchService search=(SearchService)context.getJobDetail().getJobDataMap().get(""String_Node_Str"");
  ResultSet results=search.query(StoreRef.STORE_REF_WORKSPACE_SPACESSTORE,SearchService.LANGUAGE_LUCENE,query);
  List<NodeRef> resultNodes=results.getNodeRefs();
  results.close();
  if (logger.isDebugEnabled() == true) {
    StringBuilder msg=new StringBuilder();
    msg.append(""String_Node_Str"").append(resultNodes.size()).append(""String_Node_Str"");
    logger.debug(msg.toString());
  }
  for (  NodeRef node : resultNodes) {
    String dispActionName=(String)nodeService.getProperty(node,RecordsManagementModel.PROP_DISPOSITION_ACTION_NAME);
    if (dispActionName != null && dispActionName.equalsIgnoreCase(""String_Node_Str"")) {
      rmActionService.executeRecordsManagementAction(node,dispActionName);
      if (logger.isDebugEnabled() == true) {
        logger.debug(""String_Node_Str"" + dispActionName + ""String_Node_Str""+ node.toString());
      }
    }
  }
}","/** 
 * @see org.quartz.Job#execute(org.quartz.JobExecutionContext)
 */
public void execute(JobExecutionContext context) throws JobExecutionException {
  RecordsManagementActionService rmActionService=(RecordsManagementActionService)context.getJobDetail().getJobDataMap().get(""String_Node_Str"");
  NodeService nodeService=(NodeService)context.getJobDetail().getJobDataMap().get(""String_Node_Str"");
  Calendar cal=Calendar.getInstance();
  String year=String.valueOf(cal.get(Calendar.YEAR));
  String month=String.valueOf(cal.get(Calendar.MONTH) + 1);
  String dayOfMonth=String.valueOf(cal.get(Calendar.DAY_OF_MONTH));
  final String currentDate=padString(year,2) + ""String_Node_Str"" + padString(month,2)+ ""String_Node_Str""+ padString(dayOfMonth,2)+ ""String_Node_Str"";
  if (logger.isDebugEnabled()) {
    StringBuilder msg=new StringBuilder();
    msg.append(""String_Node_Str"").append(this.getClass().getSimpleName()).append(""String_Node_Str"").append(currentDate);
    logger.debug(msg.toString());
  }
  String dateRange=""String_Node_Str"" + currentDate + ""String_Node_Str"";
  String query=""String_Node_Str"" + dateRange;
  SearchService search=(SearchService)context.getJobDetail().getJobDataMap().get(""String_Node_Str"");
  ResultSet results=search.query(StoreRef.STORE_REF_WORKSPACE_SPACESSTORE,SearchService.LANGUAGE_LUCENE,query);
  List<NodeRef> resultNodes=results.getNodeRefs();
  results.close();
  if (logger.isDebugEnabled()) {
    StringBuilder msg=new StringBuilder();
    msg.append(""String_Node_Str"").append(resultNodes.size()).append(""String_Node_Str"");
    logger.debug(msg.toString());
  }
  for (  NodeRef node : resultNodes) {
    String dispActionName=(String)nodeService.getProperty(node,RecordsManagementModel.PROP_DISPOSITION_ACTION_NAME);
    if (dispActionName != null && dispActionName.equalsIgnoreCase(""String_Node_Str"")) {
      rmActionService.executeRecordsManagementAction(node,dispActionName);
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + dispActionName + ""String_Node_Str""+ node.toString());
      }
    }
  }
}",0.9942556247008136
89377,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) == false) {
    if (logger.isDebugEnabled() == true) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (dictionaryService.isSubClass(nodeService.getType(actionedUponNodeRef),ContentModel.TYPE_CONTENT) == false) {
    if (logger.isDebugEnabled() == true) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_RECORD) == true) {
    if (logger.isDebugEnabled() == true) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ContentModel.ASPECT_WORKING_COPY) == true) {
    if (logger.isDebugEnabled() == true) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_RECORD_REJECTION_DETAILS) == true) {
    if (logger.isDebugEnabled() == true) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_SYNCED) == true) {
    if (logger.isDebugEnabled() == true) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else {
    NodeRef filePlan=(NodeRef)action.getParameterValue(PARAM_FILE_PLAN);
    if (filePlan == null) {
      filePlan=filePlanAuthenticationService.runAsRmAdmin(new RunAsWork<NodeRef>(){
        @Override public NodeRef doWork() throws Exception {
          return filePlanService.getFilePlanBySiteId(FilePlanService.DEFAULT_RM_SITE_ID);
        }
      }
);
      if (filePlan == null) {
        if (logger.isDebugEnabled() == true) {
          logger.debug(""String_Node_Str"");
        }
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
    }
 else {
      if (filePlanService.isFilePlan(filePlan) == false) {
        if (logger.isDebugEnabled() == true) {
          logger.debug(""String_Node_Str"");
        }
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
    }
    boolean hideRecord=false;
    Boolean hideRecordValue=((Boolean)action.getParameterValue(PARAM_HIDE_RECORD));
    if (hideRecordValue != null) {
      hideRecord=hideRecordValue.booleanValue();
    }
    recordService.createRecord(filePlan,actionedUponNodeRef,!hideRecord);
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) == false) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (dictionaryService.isSubClass(nodeService.getType(actionedUponNodeRef),ContentModel.TYPE_CONTENT) == false) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_RECORD)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ContentModel.ASPECT_WORKING_COPY)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_RECORD_REJECTION_DETAILS)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else   if (nodeService.hasAspect(actionedUponNodeRef,ASPECT_SYNCED)) {
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + actionedUponNodeRef.toString() + ""String_Node_Str"");
    }
  }
 else {
    NodeRef filePlan=(NodeRef)action.getParameterValue(PARAM_FILE_PLAN);
    if (filePlan == null) {
      filePlan=filePlanAuthenticationService.runAsRmAdmin(new RunAsWork<NodeRef>(){
        @Override public NodeRef doWork() throws Exception {
          return filePlanService.getFilePlanBySiteId(FilePlanService.DEFAULT_RM_SITE_ID);
        }
      }
);
      if (filePlan == null) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"");
        }
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
    }
 else {
      if (filePlanService.isFilePlan(filePlan) == false) {
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"");
        }
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
    }
    boolean hideRecord=false;
    Boolean hideRecordValue=((Boolean)action.getParameterValue(PARAM_HIDE_RECORD));
    if (hideRecordValue != null) {
      hideRecord=hideRecordValue.booleanValue();
    }
    recordService.createRecord(filePlan,actionedUponNodeRef,!hideRecord);
  }
}",0.9118942731277532
89378,"@Override public boolean evaluate(JSONObject jsonObject){
  if (indicator == null) {
    return false;
  }
  boolean result=false;
  try {
    JSONArray indicators=getRMIndicators(jsonObject);
    if (indicators != null) {
      if (indicators.contains(indicator)) {
        result=true;
      }
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err);
  }
  return (result == expected);
}","@Override public boolean evaluate(JSONObject jsonObject){
  if (indicator == null) {
    return false;
  }
  boolean result=false;
  try {
    JSONArray indicators=getRMIndicators(jsonObject);
    if (indicators != null && indicators.contains(indicator)) {
      result=true;
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err);
  }
  return (result == expected);
}",0.8679245283018868
89379,"/** 
 * Evaluate capability
 * @param nodeRef       node reference
 * @param aspectQName   aspect qname
 * @param properties    property values
 * @return 
 */
public int evaluate(NodeRef nodeRef,QName aspectQName,Map<QName,Serializable> properties){
  return evaluate(nodeRef);
}","/** 
 * Evaluate capability
 * @param nodeRef       node reference
 * @param aspectQName   aspect qname
 * @param properties    property values
 * @return
 */
public int evaluate(NodeRef nodeRef,QName aspectQName,Map<QName,Serializable> properties){
  return evaluate(nodeRef);
}",0.998211091234347
89380,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.dataset.DataSetService#loadDataSet(java.lang.String,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public void loadDataSet(NodeRef filePlan,String dataSetId){
  ParameterCheck.mandatory(""String_Node_Str"",filePlan);
  ParameterCheck.mandatoryString(""String_Node_Str"",dataSetId);
  DataSet dataSet=getDataSets().get(dataSetId);
  InputStream is=null;
  try {
    is=getClass().getClassLoader().getResourceAsStream(dataSet.getPath());
    if (is == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + dataSet.getLabel() + ""String_Node_Str"");
    }
    Reader viewReader=new InputStreamReader(is,charsetName);
    Location location=new Location(filePlan);
    importerService.importView(viewReader,location,null,null);
    patchLoadedData();
    setDataSetIdIntoFilePlan(dataSetId,filePlan);
  }
 catch (  Exception ex) {
    throw new RuntimeException(""String_Node_Str"",ex);
  }
 finally {
    if (is != null) {
      try {
        is.close();
        is=null;
      }
 catch (      IOException ex) {
        throw new RuntimeException(""String_Node_Str"",ex);
      }
    }
  }
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.dataset.DataSetService#loadDataSet(java.lang.String,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public void loadDataSet(NodeRef filePlan,String dataSetId){
  ParameterCheck.mandatory(""String_Node_Str"",filePlan);
  ParameterCheck.mandatoryString(""String_Node_Str"",dataSetId);
  DataSet dataSet=getDataSets().get(dataSetId);
  InputStream is=null;
  try {
    is=getClass().getClassLoader().getResourceAsStream(dataSet.getPath());
    if (is == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + dataSet.getLabel() + ""String_Node_Str"");
    }
    Reader viewReader=new InputStreamReader(is,CHARSET_NAME);
    Location location=new Location(filePlan);
    importerService.importView(viewReader,location,null,null);
    patchLoadedData();
    setDataSetIdIntoFilePlan(dataSetId,filePlan);
  }
 catch (  Exception ex) {
    throw new RuntimeException(""String_Node_Str"",ex);
  }
 finally {
    if (is != null) {
      try {
        is.close();
        is=null;
      }
 catch (      IOException ex) {
        throw new RuntimeException(""String_Node_Str"",ex);
      }
    }
  }
}",0.990952175786299
89381,"@Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  boolean importData=false;
  if (req.getParameter(ARG_IMPORT) != null) {
    importData=Boolean.parseBoolean(req.getParameter(ARG_IMPORT));
  }
  String siteName=RmSiteType.DEFAULT_SITE_NAME;
  if (req.getParameter(ARG_SITE_NAME) != null) {
    siteName=req.getParameter(ARG_SITE_NAME);
  }
  if (importData) {
    SiteInfo site=siteService.getSite(siteName);
    if (site == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + siteName);
    }
    NodeRef filePlan=siteService.getContainer(siteName,RmSiteType.COMPONENT_DOCUMENT_LIBRARY);
    if (filePlan == null) {
      filePlan=siteService.createContainer(siteName,RmSiteType.COMPONENT_DOCUMENT_LIBRARY,TYPE_FILE_PLAN,null);
    }
    InputStream is=BootstrapTestDataGet.class.getClassLoader().getResourceAsStream(XML_IMPORT);
    if (is == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
    Reader viewReader=null;
    try {
      viewReader=new InputStreamReader(is,charsetName);
    }
 catch (    UnsupportedEncodingException error) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + charsetName + ""String_Node_Str"",error);
    }
    Location location=new Location(filePlan);
    importerService.importView(viewReader,location,null,null);
  }
  BootstrapTestDataGet.patchLoadedData(searchService,nodeService,recordsManagementService,recordsManagementActionService,permissionService,authorityService,recordsManagementSecurityService,recordsManagementSearchBehaviour,dispositionService,recordFolderService);
  Map<String,Object> model=new HashMap<String,Object>(1,1.0f);
  model.put(""String_Node_Str"",true);
  return model;
}","@Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  boolean importData=false;
  if (req.getParameter(ARG_IMPORT) != null) {
    importData=Boolean.parseBoolean(req.getParameter(ARG_IMPORT));
  }
  String siteName=RmSiteType.DEFAULT_SITE_NAME;
  if (req.getParameter(ARG_SITE_NAME) != null) {
    siteName=req.getParameter(ARG_SITE_NAME);
  }
  if (importData) {
    SiteInfo site=siteService.getSite(siteName);
    if (site == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + siteName);
    }
    NodeRef filePlan=siteService.getContainer(siteName,RmSiteType.COMPONENT_DOCUMENT_LIBRARY);
    if (filePlan == null) {
      filePlan=siteService.createContainer(siteName,RmSiteType.COMPONENT_DOCUMENT_LIBRARY,TYPE_FILE_PLAN,null);
    }
    InputStream is=BootstrapTestDataGet.class.getClassLoader().getResourceAsStream(XML_IMPORT);
    if (is == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
    Reader viewReader=null;
    try {
      viewReader=new InputStreamReader(is,CHARSET_NAME);
    }
 catch (    UnsupportedEncodingException error) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + CHARSET_NAME + ""String_Node_Str"",error);
    }
    Location location=new Location(filePlan);
    importerService.importView(viewReader,location,null,null);
  }
  BootstrapTestDataGet.patchLoadedData(searchService,nodeService,recordsManagementService,recordsManagementActionService,permissionService,authorityService,recordsManagementSecurityService,recordsManagementSearchBehaviour,dispositionService,recordFolderService);
  Map<String,Object> model=new HashMap<String,Object>(1,1.0f);
  model.put(""String_Node_Str"",true);
  return model;
}",0.9366724237190558
89382,"@Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  Map<String,Object> model=new HashMap<String,Object>();
  List<AspectDefinition> customTypeAspectDefinitions=new ArrayList<AspectDefinition>(4);
  for (  QName aspectQName : customTypeAspects) {
    AspectDefinition nextAspectDef=dictionaryService.getAspect(aspectQName);
    customTypeAspectDefinitions.add(nextAspectDef);
  }
  model.put(""String_Node_Str"",customTypeAspectDefinitions);
  return model;
}","@Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  Map<String,Object> model=new HashMap<String,Object>();
  List<AspectDefinition> customTypeAspectDefinitions=new ArrayList<AspectDefinition>(4);
  for (  QName aspectQName : CUSTOM_TYPE_ASPECTS) {
    AspectDefinition nextAspectDef=dictionaryService.getAspect(aspectQName);
    customTypeAspectDefinitions.add(nextAspectDef);
  }
  model.put(""String_Node_Str"",customTypeAspectDefinitions);
  return model;
}",0.9662698412698412
89383,"@Override protected Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  Map<String,Object> model=new HashMap<String,Object>(1);
  boolean extended=false;
  String result=""String_Node_Str"";
  String nodeRef=req.getParameter(PARAM_NODEREF);
  if (nodeRef == null || nodeRef.length() == 0) {
    String type=req.getParameter(PARAM_TYPE);
    if (type != null && type.length() != 0 && type.indexOf(':') != -1) {
      Matcher m=qnamePattern.matcher(type);
      if (m.matches() == true) {
        QName qname=QName.createQName(type,namespaceService);
        FilePlanComponentKind kind=filePlanService.getFilePlanComponentKindFromType(qname);
        if (kind != null) {
          result=kind.toString();
        }
      }
    }
  }
 else {
    if (nodeRef.indexOf(':') != -1) {
      Matcher m=nodeRefPattern.matcher(nodeRef);
      if (m.matches()) {
        NodeRef nodeRefObj=new NodeRef(nodeRef);
        FilePlanComponentKind kind=filePlanService.getFilePlanComponentKind(nodeRefObj);
        if (kind != null) {
          result=kind.toString();
        }
        String extendedValue=req.getParameter(PARAM_EXTENDED);
        if (extendedValue != null && extendedValue.length() != 0) {
          extended=Boolean.parseBoolean(extendedValue);
          if (extended == true) {
            model.put(""String_Node_Str"",getAspects(nodeRefObj));
          }
        }
      }
    }
  }
  model.put(""String_Node_Str"",result);
  model.put(""String_Node_Str"",extended);
  return model;
}","@Override protected Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  Map<String,Object> model=new HashMap<String,Object>(1);
  boolean extended=false;
  String result=""String_Node_Str"";
  String nodeRef=req.getParameter(PARAM_NODEREF);
  if (nodeRef == null || nodeRef.length() == 0) {
    String type=req.getParameter(PARAM_TYPE);
    if (type != null && type.length() != 0 && type.indexOf(':') != -1) {
      Matcher m=QNAME_PATTERN.matcher(type);
      if (m.matches() == true) {
        QName qname=QName.createQName(type,namespaceService);
        FilePlanComponentKind kind=filePlanService.getFilePlanComponentKindFromType(qname);
        if (kind != null) {
          result=kind.toString();
        }
      }
    }
  }
 else {
    if (nodeRef.indexOf(':') != -1) {
      Matcher m=NODE_REF_PATTERN.matcher(nodeRef);
      if (m.matches()) {
        NodeRef nodeRefObj=new NodeRef(nodeRef);
        FilePlanComponentKind kind=filePlanService.getFilePlanComponentKind(nodeRefObj);
        if (kind != null) {
          result=kind.toString();
        }
        String extendedValue=req.getParameter(PARAM_EXTENDED);
        if (extendedValue != null && extendedValue.length() != 0) {
          extended=Boolean.parseBoolean(extendedValue);
          if (extended == true) {
            model.put(""String_Node_Str"",getAspects(nodeRefObj));
          }
        }
      }
    }
  }
  model.put(""String_Node_Str"",result);
  model.put(""String_Node_Str"",extended);
  return model;
}",0.983151635282458
89384,"/** 
 * Determines whether the given node type matches the path of the given object
 * @see org.springframework.extensions.config.evaluator.Evaluator#applies(java.lang.Object,java.lang.String)
 */
public boolean applies(Object obj,String condition){
  boolean result=false;
  if (obj instanceof String) {
    String objAsString=(String)obj;
    if (objAsString.indexOf(':') != -1 || objAsString.startsWith(""String_Node_Str"") == true) {
      Matcher m=nodeRefPattern.matcher(objAsString);
      if (m.matches()) {
        try {
          String jsonResponseString=callService(""String_Node_Str"" + objAsString);
          if (jsonResponseString != null) {
            result=checkJsonAgainstCondition(condition,jsonResponseString);
          }
 else           if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"");
          }
        }
 catch (        ConnectorServiceException e) {
          if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"",e);
          }
        }
      }
 else {
        try {
          String jsonResponseString=callService(""String_Node_Str"" + URLEncoder.encodeUriComponent(objAsString));
          if (jsonResponseString != null) {
            result=checkJsonAgainstCondition(condition,jsonResponseString);
          }
 else           if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"");
          }
        }
 catch (        ConnectorServiceException e) {
          if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"",e);
          }
        }
      }
    }
  }
  return result;
}","/** 
 * Determines whether the given node type matches the path of the given object
 * @see org.springframework.extensions.config.evaluator.Evaluator#applies(java.lang.Object,java.lang.String)
 */
public boolean applies(Object obj,String condition){
  boolean result=false;
  if (obj instanceof String) {
    String objAsString=(String)obj;
    if (objAsString.indexOf(':') != -1 || objAsString.startsWith(""String_Node_Str"") == true) {
      Matcher m=NODE_REF_PATTERN.matcher(objAsString);
      if (m.matches()) {
        try {
          String jsonResponseString=callService(""String_Node_Str"" + objAsString);
          if (jsonResponseString != null) {
            result=checkJsonAgainstCondition(condition,jsonResponseString);
          }
 else           if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"");
          }
        }
 catch (        ConnectorServiceException e) {
          if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"",e);
          }
        }
      }
 else {
        try {
          String jsonResponseString=callService(""String_Node_Str"" + URLEncoder.encodeUriComponent(objAsString));
          if (jsonResponseString != null) {
            result=checkJsonAgainstCondition(condition,jsonResponseString);
          }
 else           if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"");
          }
        }
 catch (        ConnectorServiceException e) {
          if (getLogger().isWarnEnabled()) {
            getLogger().warn(""String_Node_Str"",e);
          }
        }
      }
    }
  }
  return result;
}",0.992014742014742
89385,"/** 
 * Writes an audit trail entry to the given writer
 * @param writer The writer to write to
 * @param entry The entry to write
 * @param reportFormat The format to write the header in
 * @throws IOException
 */
private void writeAuditTrailEntry(Writer writer,RecordsManagementAuditEntry entry,ReportFormat reportFormat) throws IOException {
  if (writer == null) {
    return;
  }
  if (reportFormat == ReportFormat.HTML) {
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(StringEscapeUtils.escapeHtml(entry.getTimestamp().toString()));
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(entry.getFullName() != null ? StringEscapeUtils.escapeHtml(entry.getFullName()) : StringEscapeUtils.escapeHtml(entry.getUserName()));
    writer.write(""String_Node_Str"");
    if (entry.getUserRole() != null && entry.getUserRole().length() > 0) {
      writer.write(""String_Node_Str"");
      writer.write(""String_Node_Str"");
      writer.write(StringEscapeUtils.escapeHtml(entry.getUserRole()));
      writer.write(""String_Node_Str"");
    }
    if (entry.getEvent() != null && entry.getEvent().length() > 0) {
      writer.write(""String_Node_Str"");
      writer.write(""String_Node_Str"");
      writer.write(StringEscapeUtils.escapeHtml(getAuditEventLabel(entry.getEvent())));
      writer.write(""String_Node_Str"");
    }
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    if (entry.getIdentifier() != null && entry.getIdentifier().length() > 0) {
      writer.write(""String_Node_Str"");
      writer.write(""String_Node_Str"");
      writer.write(StringEscapeUtils.escapeHtml(entry.getIdentifier()));
      writer.write(""String_Node_Str"");
    }
    if (entry.getNodeType() != null && entry.getNodeType().length() > 0) {
      writer.write(""String_Node_Str"");
      writer.write(""String_Node_Str"");
      writer.write(StringEscapeUtils.escapeHtml(entry.getNodeType()));
      writer.write(""String_Node_Str"");
    }
    if (entry.getPath() != null && entry.getPath().length() > 0) {
      String path=entry.getPath();
      String displayPath=path;
      int idx=path.indexOf(""String_Node_Str"",1);
      if (idx != -1) {
        displayPath=""String_Node_Str"" + path.substring(idx);
      }
      writer.write(""String_Node_Str"");
      writer.write(""String_Node_Str"");
      writer.write(StringEscapeUtils.escapeHtml(displayPath));
      writer.write(""String_Node_Str"");
    }
    writer.write(""String_Node_Str"");
    if (entry.getChangedProperties() != null) {
      writer.write(""String_Node_Str"");
      writer.write(""String_Node_Str"");
      for (      QName valueName : entry.getChangedProperties().keySet()) {
        Pair<Serializable,Serializable> values=entry.getChangedProperties().get(valueName);
        writer.write(""String_Node_Str"");
        writer.write(getPropertyLabel(valueName));
        writer.write(""String_Node_Str"");
        Serializable oldValue=values.getFirst();
        writer.write(oldValue == null ? ""String_Node_Str"" : StringEscapeUtils.escapeHtml(oldValue.toString()));
        writer.write(""String_Node_Str"");
        Serializable newValue=values.getSecond();
        writer.write(newValue == null ? ""String_Node_Str"" : StringEscapeUtils.escapeHtml(newValue.toString()));
        writer.write(""String_Node_Str"");
      }
      writer.write(""String_Node_Str"");
    }
    writer.write(""String_Node_Str"");
  }
 else {
    try {
      JSONObject json=new JSONObject();
      json.put(""String_Node_Str"",entry.getTimestampString());
      json.put(""String_Node_Str"",entry.getUserName());
      json.put(""String_Node_Str"",entry.getUserRole() == null ? ""String_Node_Str"" : entry.getUserRole());
      json.put(""String_Node_Str"",entry.getFullName() == null ? ""String_Node_Str"" : entry.getFullName());
      json.put(""String_Node_Str"",entry.getNodeRef() == null ? ""String_Node_Str"" : entry.getNodeRef());
      if (entry.getEvent().equals(""String_Node_Str"") == true && entry.getNodeRef() != null) {
        NodeRef nodeRef=entry.getNodeRef();
        String userName=(String)nodeService.getProperty(nodeRef,ContentModel.PROP_USERNAME);
        json.put(""String_Node_Str"",userName == null ? ""String_Node_Str"" : userName);
      }
 else {
        json.put(""String_Node_Str"",entry.getNodeName() == null ? ""String_Node_Str"" : entry.getNodeName());
      }
      json.put(""String_Node_Str"",entry.getNodeType() == null ? ""String_Node_Str"" : entry.getNodeType());
      json.put(""String_Node_Str"",entry.getEvent() == null ? ""String_Node_Str"" : getAuditEventLabel(entry.getEvent()));
      json.put(""String_Node_Str"",entry.getIdentifier() == null ? ""String_Node_Str"" : entry.getIdentifier());
      json.put(""String_Node_Str"",entry.getPath() == null ? ""String_Node_Str"" : entry.getPath());
      JSONArray changedValues=new JSONArray();
      if (entry.getChangedProperties() != null) {
        for (        QName valueName : entry.getChangedProperties().keySet()) {
          Pair<Serializable,Serializable> values=entry.getChangedProperties().get(valueName);
          JSONObject changedValue=new JSONObject();
          changedValue.put(""String_Node_Str"",getPropertyLabel(valueName));
          changedValue.put(""String_Node_Str"",values.getFirst() == null ? ""String_Node_Str"" : values.getFirst().toString());
          changedValue.put(""String_Node_Str"",values.getSecond() == null ? ""String_Node_Str"" : values.getSecond().toString());
          changedValues.put(changedValue);
        }
      }
      json.put(""String_Node_Str"",changedValues);
      writer.write(json.toString());
    }
 catch (    JSONException je) {
      writer.write(""String_Node_Str"");
    }
  }
}","/** 
 * Writes an audit trail entry to the given writer
 * @param writer The writer to write to
 * @param entry The entry to write
 * @param reportFormat The format to write the header in
 * @throws IOException
 */
private void writeAuditTrailEntry(Writer writer,RecordsManagementAuditEntry entry,ReportFormat reportFormat) throws IOException {
  if (writer == null) {
    return;
  }
  if (reportFormat == ReportFormat.HTML) {
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(StringEscapeUtils.escapeHtml(entry.getTimestamp().toString()));
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    writer.write(entry.getFullName() != null ? StringEscapeUtils.escapeHtml(entry.getFullName()) : StringEscapeUtils.escapeHtml(entry.getUserName()));
    writer.write(""String_Node_Str"");
    if (entry.getUserRole() != null && entry.getUserRole().length() > 0) {
      writer.write(""String_Node_Str"");
      writer.write(""String_Node_Str"");
      writer.write(StringEscapeUtils.escapeHtml(entry.getUserRole()));
      writer.write(""String_Node_Str"");
    }
    if (entry.getEvent() != null && entry.getEvent().length() > 0) {
      writer.write(""String_Node_Str"");
      writer.write(""String_Node_Str"");
      writer.write(StringEscapeUtils.escapeHtml(getAuditEventLabel(entry.getEvent())));
      writer.write(""String_Node_Str"");
    }
    writer.write(""String_Node_Str"");
    writer.write(""String_Node_Str"");
    if (entry.getIdentifier() != null && entry.getIdentifier().length() > 0) {
      writer.write(""String_Node_Str"");
      writer.write(""String_Node_Str"");
      writer.write(StringEscapeUtils.escapeHtml(entry.getIdentifier()));
      writer.write(""String_Node_Str"");
    }
    if (entry.getNodeType() != null && entry.getNodeType().length() > 0) {
      writer.write(""String_Node_Str"");
      writer.write(""String_Node_Str"");
      writer.write(StringEscapeUtils.escapeHtml(entry.getNodeType()));
      writer.write(""String_Node_Str"");
    }
    if (entry.getPath() != null && entry.getPath().length() > 0) {
      String path=entry.getPath();
      String displayPath=path;
      int idx=path.indexOf('/',1);
      if (idx != -1) {
        displayPath=""String_Node_Str"" + path.substring(idx);
      }
      writer.write(""String_Node_Str"");
      writer.write(""String_Node_Str"");
      writer.write(StringEscapeUtils.escapeHtml(displayPath));
      writer.write(""String_Node_Str"");
    }
    writer.write(""String_Node_Str"");
    if (entry.getChangedProperties() != null) {
      writer.write(""String_Node_Str"");
      writer.write(""String_Node_Str"");
      for (      QName valueName : entry.getChangedProperties().keySet()) {
        Pair<Serializable,Serializable> values=entry.getChangedProperties().get(valueName);
        writer.write(""String_Node_Str"");
        writer.write(getPropertyLabel(valueName));
        writer.write(""String_Node_Str"");
        Serializable oldValue=values.getFirst();
        writer.write(oldValue == null ? ""String_Node_Str"" : StringEscapeUtils.escapeHtml(oldValue.toString()));
        writer.write(""String_Node_Str"");
        Serializable newValue=values.getSecond();
        writer.write(newValue == null ? ""String_Node_Str"" : StringEscapeUtils.escapeHtml(newValue.toString()));
        writer.write(""String_Node_Str"");
      }
      writer.write(""String_Node_Str"");
    }
    writer.write(""String_Node_Str"");
  }
 else {
    try {
      JSONObject json=new JSONObject();
      json.put(""String_Node_Str"",entry.getTimestampString());
      json.put(""String_Node_Str"",entry.getUserName());
      json.put(""String_Node_Str"",entry.getUserRole() == null ? ""String_Node_Str"" : entry.getUserRole());
      json.put(""String_Node_Str"",entry.getFullName() == null ? ""String_Node_Str"" : entry.getFullName());
      json.put(""String_Node_Str"",entry.getNodeRef() == null ? ""String_Node_Str"" : entry.getNodeRef());
      if (entry.getEvent().equals(""String_Node_Str"") == true && entry.getNodeRef() != null) {
        NodeRef nodeRef=entry.getNodeRef();
        String userName=(String)nodeService.getProperty(nodeRef,ContentModel.PROP_USERNAME);
        json.put(""String_Node_Str"",userName == null ? ""String_Node_Str"" : userName);
      }
 else {
        json.put(""String_Node_Str"",entry.getNodeName() == null ? ""String_Node_Str"" : entry.getNodeName());
      }
      json.put(""String_Node_Str"",entry.getNodeType() == null ? ""String_Node_Str"" : entry.getNodeType());
      json.put(""String_Node_Str"",entry.getEvent() == null ? ""String_Node_Str"" : getAuditEventLabel(entry.getEvent()));
      json.put(""String_Node_Str"",entry.getIdentifier() == null ? ""String_Node_Str"" : entry.getIdentifier());
      json.put(""String_Node_Str"",entry.getPath() == null ? ""String_Node_Str"" : entry.getPath());
      JSONArray changedValues=new JSONArray();
      if (entry.getChangedProperties() != null) {
        for (        QName valueName : entry.getChangedProperties().keySet()) {
          Pair<Serializable,Serializable> values=entry.getChangedProperties().get(valueName);
          JSONObject changedValue=new JSONObject();
          changedValue.put(""String_Node_Str"",getPropertyLabel(valueName));
          changedValue.put(""String_Node_Str"",values.getFirst() == null ? ""String_Node_Str"" : values.getFirst().toString());
          changedValue.put(""String_Node_Str"",values.getSecond() == null ? ""String_Node_Str"" : values.getSecond().toString());
          changedValues.put(changedValue);
        }
      }
      json.put(""String_Node_Str"",changedValues);
      writer.write(json.toString());
    }
 catch (    JSONException je) {
      writer.write(""String_Node_Str"");
    }
  }
}",0.9982719889407292
89386,"/** 
 * Creates a record from the given document
 * @param document the document from which a record will be created
 */
@Override public void makeRecord(NodeRef document){
  ParameterCheck.mandatory(""String_Node_Str"",document);
  ruleService.disableRules();
  try {
    String recordId=identifierService.generateIdentifier(ASPECT_RECORD,nodeService.getPrimaryParent(document).getParentRef());
    String name=(String)nodeService.getProperty(document,ContentModel.PROP_NAME);
    int dotIndex=name.lastIndexOf(""String_Node_Str"");
    String prefix=name;
    String postfix=""String_Node_Str"";
    if (dotIndex != -1) {
      prefix=name.substring(0,dotIndex);
      postfix=name.substring(dotIndex);
    }
    String recordName=prefix + ""String_Node_Str"" + recordId+ ""String_Node_Str""+ postfix;
    fileFolderService.rename(document,recordName);
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + name + ""String_Node_Str""+ recordName);
    }
    Map<QName,Serializable> props=new HashMap<QName,Serializable>(2);
    props.put(PROP_IDENTIFIER,recordId);
    props.put(PROP_ORIGIONAL_NAME,name);
    nodeService.addAspect(document,RecordsManagementModel.ASPECT_RECORD,props);
  }
 catch (  FileNotFoundException e) {
    throw new AlfrescoRuntimeException(""String_Node_Str"",e);
  }
 finally {
    ruleService.enableRules();
  }
}","/** 
 * Creates a record from the given document
 * @param document the document from which a record will be created
 */
@Override public void makeRecord(NodeRef document){
  ParameterCheck.mandatory(""String_Node_Str"",document);
  ruleService.disableRules();
  try {
    String recordId=identifierService.generateIdentifier(ASPECT_RECORD,nodeService.getPrimaryParent(document).getParentRef());
    String name=(String)nodeService.getProperty(document,ContentModel.PROP_NAME);
    int dotIndex=name.lastIndexOf('.');
    String prefix=name;
    String postfix=""String_Node_Str"";
    if (dotIndex != -1) {
      prefix=name.substring(0,dotIndex);
      postfix=name.substring(dotIndex);
    }
    String recordName=prefix + ""String_Node_Str"" + recordId+ ""String_Node_Str""+ postfix;
    fileFolderService.rename(document,recordName);
    if (logger.isDebugEnabled()) {
      logger.debug(""String_Node_Str"" + name + ""String_Node_Str""+ recordName);
    }
    Map<QName,Serializable> props=new HashMap<QName,Serializable>(2);
    props.put(PROP_IDENTIFIER,recordId);
    props.put(PROP_ORIGIONAL_NAME,name);
    nodeService.addAspect(document,RecordsManagementModel.ASPECT_RECORD,props);
  }
 catch (  FileNotFoundException e) {
    throw new AlfrescoRuntimeException(""String_Node_Str"",e);
  }
 finally {
    ruleService.enableRules();
  }
}",0.992548435171386
89387,"/** 
 * Helper method to check whether all the mandatory properties of the node have been set
 * @param nodeRef node reference
 * @return boolean true if all mandatory properties are set, false otherwise
 */
private boolean mandatoryPropertiesSet(NodeRef nodeRef,List<String> missingProperties){
  boolean result=true;
  Map<QName,Serializable> nodeRefProps=this.nodeService.getProperties(nodeRef);
  QName nodeRefType=this.nodeService.getType(nodeRef);
  TypeDefinition typeDef=this.dictionaryService.getType(nodeRefType);
  for (  PropertyDefinition propDef : typeDef.getProperties().values()) {
    if (propDef.isMandatory() == true) {
      if (nodeRefProps.get(propDef.getName()) == null) {
        logMissingProperty(propDef,missingProperties);
        result=false;
        break;
      }
    }
  }
  if (result != false) {
    Set<QName> aspects=this.nodeService.getAspects(nodeRef);
    for (    QName aspect : aspects) {
      AspectDefinition aspectDef=this.dictionaryService.getAspect(aspect);
      for (      PropertyDefinition propDef : aspectDef.getProperties().values()) {
        if (propDef.isMandatory() == true) {
          if (nodeRefProps.get(propDef.getName()) == null) {
            logMissingProperty(propDef,missingProperties);
            result=false;
            break;
          }
        }
      }
    }
  }
  return result;
}","/** 
 * Helper method to check whether all the mandatory properties of the node have been set
 * @param nodeRef node reference
 * @return boolean true if all mandatory properties are set, false otherwise
 */
private boolean mandatoryPropertiesSet(NodeRef nodeRef,List<String> missingProperties){
  boolean result=true;
  Map<QName,Serializable> nodeRefProps=this.nodeService.getProperties(nodeRef);
  QName nodeRefType=this.nodeService.getType(nodeRef);
  TypeDefinition typeDef=this.dictionaryService.getType(nodeRefType);
  for (  PropertyDefinition propDef : typeDef.getProperties().values()) {
    if (propDef.isMandatory() && nodeRefProps.get(propDef.getName()) == null) {
      logMissingProperty(propDef,missingProperties);
      result=false;
      break;
    }
  }
  if (result != false) {
    Set<QName> aspects=this.nodeService.getAspects(nodeRef);
    for (    QName aspect : aspects) {
      AspectDefinition aspectDef=this.dictionaryService.getAspect(aspect);
      for (      PropertyDefinition propDef : aspectDef.getProperties().values()) {
        if (propDef.isMandatory() && nodeRefProps.get(propDef.getName()) == null) {
          logMissingProperty(propDef,missingProperties);
          result=false;
          break;
        }
      }
    }
  }
  return result;
}",0.7980332829046899
89388,"/** 
 * @param action
 * @param actionedUponNodeRef
 * @return
 */
private NodeRef createOrResolveRecordFolder(Action action,NodeRef actionedUponNodeRef){
  NodeRef context=filePlanService.getFilePlan(actionedUponNodeRef);
  if (context == null) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
 else   if (nodeService.exists(context) == false) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
  String path=(String)action.getParameterValue(PARAM_PATH);
  String[] pathValues=ArrayUtils.EMPTY_STRING_ARRAY;
  if (path != null && path.isEmpty() == false) {
    pathValues=StringUtils.tokenizeToStringArray(path,""String_Node_Str"",false,true);
  }
  boolean create=false;
  Boolean createValue=(Boolean)action.getParameterValue(PARAM_CREATE_RECORD_PATH);
  if (createValue != null) {
    create=createValue.booleanValue();
  }
  NodeRef recordFolder=resolvePath(context,pathValues);
  if (recordFolder == null) {
    if (create == true) {
      NodeRef parent=resolveParent(context,pathValues,create);
      if (parent == null) {
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
      if (filePlanService.isRecordCategory(parent) == false) {
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
      String recordFolderName=pathValues[pathValues.length - 1];
      recordFolder=recordFolderService.createRecordFolder(parent,recordFolderName);
    }
 else {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
  }
  return recordFolder;
}","/** 
 * @param action
 * @param actionedUponNodeRef
 * @return
 */
private NodeRef createOrResolveRecordFolder(Action action,NodeRef actionedUponNodeRef){
  NodeRef context=filePlanService.getFilePlan(actionedUponNodeRef);
  if (context == null) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
 else   if (nodeService.exists(context) == false) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
  String path=(String)action.getParameterValue(PARAM_PATH);
  String[] pathValues=ArrayUtils.EMPTY_STRING_ARRAY;
  if (path != null && path.isEmpty() == false) {
    pathValues=StringUtils.tokenizeToStringArray(path,""String_Node_Str"",false,true);
  }
  boolean create=false;
  Boolean createValue=(Boolean)action.getParameterValue(PARAM_CREATE_RECORD_PATH);
  if (createValue != null) {
    create=createValue.booleanValue();
  }
  NodeRef recordFolder=resolvePath(context,pathValues);
  if (recordFolder == null) {
    if (create) {
      NodeRef parent=resolveParent(context,pathValues,create);
      if (parent == null) {
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
      if (filePlanService.isRecordCategory(parent) == false) {
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
      String recordFolderName=pathValues[pathValues.length - 1];
      recordFolder=recordFolderService.createRecordFolder(parent,recordFolderName);
    }
 else {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
  }
  return recordFolder;
}",0.997364953886693
89389,"/** 
 * @param context
 * @param pathValues
 * @param create      Create any missing path elements
 * @return
 */
private NodeRef resolveParent(NodeRef context,String[] pathValues,boolean create){
  NodeRef result=null;
  if (ArrayUtils.isEmpty(pathValues) == true) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
 else   if (pathValues.length == 1) {
    result=context;
  }
 else {
    pathValues=(String[])ArrayUtils.remove(pathValues,pathValues.length - 1);
    result=resolvePath(context,pathValues,create);
  }
  return result;
}","/** 
 * @param context
 * @param pathValues
 * @param create      Create any missing path elements
 * @return
 */
private NodeRef resolveParent(NodeRef context,String[] pathValues,boolean create){
  NodeRef result=null;
  if (ArrayUtils.isEmpty(pathValues)) {
    throw new AlfrescoRuntimeException(""String_Node_Str"");
  }
 else   if (pathValues.length == 1) {
    result=context;
  }
 else {
    pathValues=(String[])ArrayUtils.remove(pathValues,pathValues.length - 1);
    result=resolvePath(context,pathValues,create);
  }
  return result;
}",0.9927007299270072
89390,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) == true && freezeService.isFrozen(actionedUponNodeRef) == false) {
    if (recordService.isFiled(actionedUponNodeRef) == false) {
      NodeRef recordFolder=(NodeRef)action.getParameterValue(PARAM_DESTINATION_RECORD_FOLDER);
      if (recordFolder == null) {
        recordFolder=createOrResolveRecordFolder(action,actionedUponNodeRef);
      }
      if (recordFolder == null) {
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
      if (recordFolderService.isRecordFolder(recordFolder) == true) {
        final NodeRef finalRecordFolder=recordFolder;
        AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
          @Override public Void doWork() throws Exception {
            try {
              fileFolderService.move(actionedUponNodeRef,finalRecordFolder,null);
            }
 catch (            FileNotFoundException fileNotFound) {
              throw new AlfrescoRuntimeException(""String_Node_Str"",fileNotFound);
            }
            return null;
          }
        }
);
      }
 else {
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
    }
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(final Action action,final NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) && freezeService.isFrozen(actionedUponNodeRef) == false && recordService.isFiled(actionedUponNodeRef) == false) {
    NodeRef recordFolder=(NodeRef)action.getParameterValue(PARAM_DESTINATION_RECORD_FOLDER);
    if (recordFolder == null) {
      recordFolder=createOrResolveRecordFolder(action,actionedUponNodeRef);
    }
    if (recordFolder == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
    if (recordFolderService.isRecordFolder(recordFolder)) {
      final NodeRef finalRecordFolder=recordFolder;
      AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
        @Override public Void doWork() throws Exception {
          try {
            fileFolderService.move(actionedUponNodeRef,finalRecordFolder,null);
          }
 catch (          FileNotFoundException fileNotFound) {
            throw new AlfrescoRuntimeException(""String_Node_Str"",fileNotFound);
          }
          return null;
        }
      }
);
    }
 else {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
  }
}",0.8990627253064167
89391,"/** 
 * @return              Returns <tt>true</tt> if the data is a NodeRef and it representsa fileplan component
 */
public boolean isSupported(Serializable data){
  if (data == null || !(data instanceof NodeRef)) {
    return false;
  }
  return nodeService.hasAspect((NodeRef)data,RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT);
}","/** 
 * @return              Returns <tt>true</tt> if the data is a NodeRef and it representsa fileplan component
 */
public boolean isSupported(Serializable data){
  if (!(data instanceof NodeRef)) {
    return false;
  }
  return nodeService.hasAspect((NodeRef)data,RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT);
}",0.9756838905775076
89392,"/** 
 * @return              Returns <tt>true</tt> if the data is a NodeRef and it representsa fileplan component
 */
public boolean isSupported(Serializable data){
  if (data == null || !(data instanceof NodeRef)) {
    return false;
  }
  return nodeService.hasAspect((NodeRef)data,RecordsManagementModel.ASPECT_RECORD_COMPONENT_ID);
}","/** 
 * @return              Returns <tt>true</tt> if the data is a NodeRef and it representsa fileplan component
 */
public boolean isSupported(Serializable data){
  if (!(data instanceof NodeRef)) {
    return false;
  }
  return nodeService.hasAspect((NodeRef)data,RecordsManagementModel.ASPECT_RECORD_COMPONENT_ID);
}",0.9756838905775076
89393,"/** 
 * @return              Returns <tt>true</tt> if the data is a NodeRef and it representsa fileplan component
 */
public boolean isSupported(Serializable data){
  if (data == null || !(data instanceof NodeRef)) {
    return false;
  }
  return nodeService.hasAspect((NodeRef)data,RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT);
}","/** 
 * @return              Returns <tt>true</tt> if the data is a NodeRef and it representsa fileplan component
 */
public boolean isSupported(Serializable data){
  if (!(data instanceof NodeRef)) {
    return false;
  }
  return nodeService.hasAspect((NodeRef)data,RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT);
}",0.9756838905775076
89394,"/** 
 * @return              Returns <tt>true</tt> if the data is a NodeRef and it representsa fileplan component
 */
public boolean isSupported(Serializable data){
  if (data == null || !(data instanceof NodeRef)) {
    return false;
  }
  return nodeService.hasAspect((NodeRef)data,RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT);
}","/** 
 * @return              Returns <tt>true</tt> if the data is a NodeRef and it representsa fileplan component
 */
public boolean isSupported(Serializable data){
  if (!(data instanceof NodeRef)) {
    return false;
  }
  return nodeService.hasAspect((NodeRef)data,RecordsManagementModel.ASPECT_FILE_PLAN_COMPONENT);
}",0.9756838905775076
89395,"/** 
 * Constructor 
 * @param services
 * @param dispositionActionNodeRef
 */
public DispositionActionImpl(RecordsManagementServiceRegistry services,NodeRef dispositionActionNodeRef){
  this.services=services;
  this.dispositionNodeRef=dispositionActionNodeRef;
}","/** 
 * Constructor
 * @param services
 * @param dispositionActionNodeRef
 */
public DispositionActionImpl(RecordsManagementServiceRegistry services,NodeRef dispositionActionNodeRef){
  this.services=services;
  this.dispositionNodeRef=dispositionActionNodeRef;
}",0.9981024667931688
89396,"/** 
 * Helper method to deal with boolean values
 * @param value
 * @param defaultValue
 * @return
 */
private boolean getBooleanValue(Object value,boolean defaultValue){
  boolean result=defaultValue;
  if (value != null && value instanceof Boolean) {
    result=((Boolean)value).booleanValue();
  }
  return result;
}","/** 
 * Helper method to deal with boolean values
 * @param value
 * @param defaultValue
 * @return
 */
private boolean getBooleanValue(Object value,boolean defaultValue){
  boolean result=defaultValue;
  if (value instanceof Boolean) {
    result=((Boolean)value).booleanValue();
  }
  return result;
}",0.8635634028892456
89397,"/** 
 * Ensure that the user only updates record properties that they have permission to.
 * @see org.alfresco.repo.node.NodeServicePolicies.OnUpdatePropertiesPolicy#onUpdateProperties(org.alfresco.service.cmr.repository.NodeRef,java.util.Map,java.util.Map)
 */
@Override @Behaviour(name=""String_Node_Str"",kind=BehaviourKind.CLASS,type=""String_Node_Str"") public void onUpdateProperties(final NodeRef nodeRef,final Map<QName,Serializable> before,final Map<QName,Serializable> after){
  if (AuthenticationUtil.getFullyAuthenticatedUser() != null && AuthenticationUtil.isRunAsUserTheSystemUser() == false && nodeService.exists(nodeRef) && isRecord(nodeRef)) {
    for (    QName property : after.keySet()) {
      Serializable beforeValue=null;
      if (before != null) {
        beforeValue=before.get(property);
      }
      Serializable afterValue=null;
      if (after != null) {
        afterValue=after.get(property);
      }
      boolean propertyUnchanged=false;
      if (beforeValue != null && afterValue != null && beforeValue instanceof Date && afterValue instanceof Date) {
        propertyUnchanged=(((Date)beforeValue).compareTo((Date)afterValue) == 0);
      }
 else {
        propertyUnchanged=EqualsHelper.nullSafeEquals(beforeValue,afterValue);
      }
      if (propertyUnchanged == false && isPropertyEditable(nodeRef,property) == false) {
        throw new ModelAccessDeniedException(""String_Node_Str"" + AuthenticationUtil.getFullyAuthenticatedUser() + ""String_Node_Str""+ property.toString()+ ""String_Node_Str""+ nodeRef.toString());
      }
    }
  }
}","/** 
 * Ensure that the user only updates record properties that they have permission to.
 * @see org.alfresco.repo.node.NodeServicePolicies.OnUpdatePropertiesPolicy#onUpdateProperties(org.alfresco.service.cmr.repository.NodeRef,java.util.Map,java.util.Map)
 */
@Override @Behaviour(name=""String_Node_Str"",kind=BehaviourKind.CLASS,type=""String_Node_Str"") public void onUpdateProperties(final NodeRef nodeRef,final Map<QName,Serializable> before,final Map<QName,Serializable> after){
  if (AuthenticationUtil.getFullyAuthenticatedUser() != null && AuthenticationUtil.isRunAsUserTheSystemUser() == false && nodeService.exists(nodeRef) && isRecord(nodeRef)) {
    for (    QName property : after.keySet()) {
      Serializable beforeValue=null;
      if (before != null) {
        beforeValue=before.get(property);
      }
      Serializable afterValue=null;
      if (after != null) {
        afterValue=after.get(property);
      }
      boolean propertyUnchanged=false;
      if (beforeValue instanceof Date && afterValue instanceof Date) {
        propertyUnchanged=(((Date)beforeValue).compareTo((Date)afterValue) == 0);
      }
 else {
        propertyUnchanged=EqualsHelper.nullSafeEquals(beforeValue,afterValue);
      }
      if (propertyUnchanged == false && isPropertyEditable(nodeRef,property) == false) {
        throw new ModelAccessDeniedException(""String_Node_Str"" + AuthenticationUtil.getFullyAuthenticatedUser() + ""String_Node_Str""+ property.toString()+ ""String_Node_Str""+ nodeRef.toString());
      }
    }
  }
}",0.8210254756530152
89398,"/** 
 * @see org.alfresco.repo.action.ActionServiceImpl#getActionConditionDefinition(java.lang.String)
 */
public ActionConditionDefinition getActionConditionDefinition(String name){
  ActionConditionDefinition definition=null;
  Object bean=extendedApplicationContext.getBean(name);
  if (bean != null && bean instanceof ActionConditionEvaluator) {
    ActionConditionEvaluator evaluator=(ActionConditionEvaluator)bean;
    definition=evaluator.getActionConditionDefintion();
  }
  return definition;
}","/** 
 * @see org.alfresco.repo.action.ActionServiceImpl#getActionConditionDefinition(java.lang.String)
 */
public ActionConditionDefinition getActionConditionDefinition(String name){
  ActionConditionDefinition definition=null;
  Object bean=extendedApplicationContext.getBean(name);
  if (bean instanceof ActionConditionEvaluator) {
    ActionConditionEvaluator evaluator=(ActionConditionEvaluator)bean;
    definition=evaluator.getActionConditionDefintion();
  }
  return definition;
}",0.6222222222222222
89399,"/** 
 * Parses the given request and builds an instance of RecordsManagementAuditQueryParameters to retrieve the relevant audit entries
 * @param req The request
 * @return RecordsManagementAuditQueryParameters instance
 */
protected RecordsManagementAuditQueryParameters parseQueryParameters(WebScriptRequest req){
  RecordsManagementAuditQueryParameters params=new RecordsManagementAuditQueryParameters();
  NodeRef nodeRef=null;
  Map<String,String> templateVars=req.getServiceMatch().getTemplateVars();
  String storeType=templateVars.get(""String_Node_Str"");
  if (storeType != null && storeType.length() > 0) {
    String storeId=templateVars.get(""String_Node_Str"");
    String nodeId=templateVars.get(""String_Node_Str"");
    nodeRef=new NodeRef(new StoreRef(storeType,storeId),nodeId);
  }
  String size=null;
  String user=null;
  String event=null;
  String from=null;
  String to=null;
  String property=null;
  if (MimetypeMap.MIMETYPE_JSON.equals(req.getContentType())) {
    try {
      JSONObject json=new JSONObject(new JSONTokener(req.getContent().getContent()));
      if (json.has(PARAM_SIZE)) {
        size=json.getString(PARAM_SIZE);
      }
      if (json.has(PARAM_USER)) {
        user=json.getString(PARAM_USER);
      }
      if (json.has(PARAM_EVENT)) {
        event=json.getString(PARAM_EVENT);
      }
      if (json.has(PARAM_FROM)) {
        from=json.getString(PARAM_FROM);
      }
      if (json.has(PARAM_TO)) {
        to=json.getString(PARAM_TO);
      }
      if (json.has(PARAM_PROPERTY)) {
        property=json.getString(PARAM_PROPERTY);
      }
    }
 catch (    IOException ioe) {
      if (logger.isWarnEnabled()) {
        logger.warn(""String_Node_Str"" + ioe.getMessage());
      }
    }
catch (    JSONException je) {
      if (logger.isWarnEnabled()) {
        logger.warn(""String_Node_Str"" + je.getMessage());
      }
    }
  }
 else {
    size=req.getParameter(PARAM_SIZE);
    user=req.getParameter(PARAM_USER);
    event=req.getParameter(PARAM_EVENT);
    from=req.getParameter(PARAM_FROM);
    to=req.getParameter(PARAM_TO);
    property=req.getParameter(PARAM_PROPERTY);
  }
  params.setNodeRef(nodeRef);
  params.setUser(user);
  params.setEvent(event);
  if (size != null && size.length() > 0) {
    try {
      params.setMaxEntries(Integer.parseInt(size));
    }
 catch (    NumberFormatException nfe) {
      if (logger.isWarnEnabled()) {
        logger.warn(""String_Node_Str"" + size + ""String_Node_Str"");
      }
    }
  }
  if (from != null && from.length() > 0) {
    try {
      SimpleDateFormat dateFormat=new SimpleDateFormat(DATE_PATTERN);
      params.setDateFrom(dateFormat.parse(from));
    }
 catch (    ParseException pe) {
      if (logger.isWarnEnabled()) {
        logger.warn(""String_Node_Str"" + from + ""String_Node_Str""+ DATE_PATTERN);
      }
    }
  }
  if (to != null && to.length() > 0) {
    try {
      SimpleDateFormat dateFormat=new SimpleDateFormat(DATE_PATTERN);
      params.setDateTo(dateFormat.parse(to));
    }
 catch (    ParseException pe) {
      if (logger.isWarnEnabled()) {
        logger.warn(""String_Node_Str"" + to + ""String_Node_Str""+ DATE_PATTERN);
      }
    }
  }
  if (property != null && property.length() > 0) {
    try {
      params.setProperty(QName.createQName(property,namespaceService));
    }
 catch (    InvalidQNameException iqe) {
      if (logger.isWarnEnabled())       logger.warn(""String_Node_Str"" + property + ""String_Node_Str"");
    }
  }
  return params;
}","/** 
 * Parses the given request and builds an instance of RecordsManagementAuditQueryParameters to retrieve the relevant audit entries
 * @param req The request
 * @return RecordsManagementAuditQueryParameters instance
 */
protected RecordsManagementAuditQueryParameters parseQueryParameters(WebScriptRequest req){
  RecordsManagementAuditQueryParameters params=new RecordsManagementAuditQueryParameters();
  NodeRef nodeRef=null;
  Map<String,String> templateVars=req.getServiceMatch().getTemplateVars();
  String storeType=templateVars.get(""String_Node_Str"");
  if (storeType != null && storeType.length() > 0) {
    String storeId=templateVars.get(""String_Node_Str"");
    String nodeId=templateVars.get(""String_Node_Str"");
    nodeRef=new NodeRef(new StoreRef(storeType,storeId),nodeId);
  }
  String size=null;
  String user=null;
  String event=null;
  String from=null;
  String to=null;
  String property=null;
  if (MimetypeMap.MIMETYPE_JSON.equals(req.getContentType())) {
    try {
      JSONObject json=new JSONObject(new JSONTokener(req.getContent().getContent()));
      if (json.has(PARAM_SIZE)) {
        size=json.getString(PARAM_SIZE);
      }
      if (json.has(PARAM_USER)) {
        user=json.getString(PARAM_USER);
      }
      if (json.has(PARAM_EVENT)) {
        event=json.getString(PARAM_EVENT);
      }
      if (json.has(PARAM_FROM)) {
        from=json.getString(PARAM_FROM);
      }
      if (json.has(PARAM_TO)) {
        to=json.getString(PARAM_TO);
      }
      if (json.has(PARAM_PROPERTY)) {
        property=json.getString(PARAM_PROPERTY);
      }
    }
 catch (    IOException ioe) {
      if (logger.isWarnEnabled()) {
        logger.warn(""String_Node_Str"" + ioe.getMessage());
      }
    }
catch (    JSONException je) {
      if (logger.isWarnEnabled()) {
        logger.warn(""String_Node_Str"" + je.getMessage());
      }
    }
  }
 else {
    size=req.getParameter(PARAM_SIZE);
    user=req.getParameter(PARAM_USER);
    event=req.getParameter(PARAM_EVENT);
    from=req.getParameter(PARAM_FROM);
    to=req.getParameter(PARAM_TO);
    property=req.getParameter(PARAM_PROPERTY);
  }
  params.setNodeRef(nodeRef);
  params.setUser(user);
  params.setEvent(event);
  if (size != null && size.length() > 0) {
    try {
      params.setMaxEntries(Integer.parseInt(size));
    }
 catch (    NumberFormatException nfe) {
      if (logger.isWarnEnabled()) {
        logger.warn(""String_Node_Str"" + size + ""String_Node_Str"");
      }
    }
  }
  if (from != null && from.length() > 0) {
    try {
      SimpleDateFormat dateFormat=new SimpleDateFormat(DATE_PATTERN);
      params.setDateFrom(dateFormat.parse(from));
    }
 catch (    ParseException pe) {
      if (logger.isWarnEnabled()) {
        logger.warn(""String_Node_Str"" + from + ""String_Node_Str""+ DATE_PATTERN);
      }
    }
  }
  if (to != null && to.length() > 0) {
    try {
      SimpleDateFormat dateFormat=new SimpleDateFormat(DATE_PATTERN);
      params.setDateTo(dateFormat.parse(to));
    }
 catch (    ParseException pe) {
      if (logger.isWarnEnabled()) {
        logger.warn(""String_Node_Str"" + to + ""String_Node_Str""+ DATE_PATTERN);
      }
    }
  }
  if (property != null && property.length() > 0) {
    try {
      params.setProperty(QName.createQName(property,namespaceService));
    }
 catch (    InvalidQNameException iqe) {
      if (logger.isWarnEnabled()) {
        logger.warn(""String_Node_Str"" + property + ""String_Node_Str"");
      }
    }
  }
  return params;
}",0.9939672507900028
89400,"/** 
 * Ensure that the content of a ghosted node can not be updated.
 * @see org.alfresco.repo.content.ContentServicePolicies.OnContentUpdatePolicy#onContentUpdate(org.alfresco.service.cmr.repository.NodeRef,boolean)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,notificationFrequency=NotificationFrequency.EVERY_EVENT,policy=""String_Node_Str"",type=""String_Node_Str"") public void onContentUpdate(NodeRef Content,boolean bNew){
  throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_GHOSTED_PROP_UPDATE));
}","/** 
 * Ensure that the content of a ghosted node can not be updated.
 * @see org.alfresco.repo.content.ContentServicePolicies.OnContentUpdatePolicy#onContentUpdate(org.alfresco.service.cmr.repository.NodeRef,boolean)
 */
@Override @Behaviour(kind=BehaviourKind.CLASS,notificationFrequency=NotificationFrequency.EVERY_EVENT,policy=""String_Node_Str"",type=""String_Node_Str"") public void onContentUpdate(NodeRef content,boolean bNew){
  throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_GHOSTED_PROP_UPDATE));
}",0.9980657640232108
89401,"/** 
 * Returns the number of milliseconds for the ""from date"".
 * @param date The date for which the number of milliseconds should retrieved.
 * @return Returns null if the given date is null, otherwise the number of milliseconds for the given date.
 */
private Long getFromDateTime(Date date){
  Long fromDateTime=null;
  Date fromDate=getFromDate(date);
  if (fromDate != null) {
    fromDateTime=new Long(fromDate.getTime());
  }
  return fromDateTime;
}","/** 
 * Returns the number of milliseconds for the ""from date"".
 * @param date The date for which the number of milliseconds should retrieved.
 * @return Returns null if the given date is null, otherwise the number of milliseconds for the given date.
 */
private Long getFromDateTime(Date date){
  Long fromDateTime=null;
  Date fromDate=getFromDate(date);
  if (fromDate != null) {
    fromDateTime=Long.valueOf(fromDate.getTime());
  }
  return fromDateTime;
}",0.9869565217391304
89402,"/** 
 * Returns the number of milliseconds for the ""to date"".
 * @param date The date for which the number of milliseconds should retrieved.
 * @return Returns null if the given date is null, otherwise the number of milliseconds for the given date.
 */
private Long getToDateTime(Date date){
  Long toDateTime=null;
  Date toDate=getToDate(date);
  if (toDate != null) {
    toDateTime=new Long(toDate.getTime());
  }
  return toDateTime;
}","/** 
 * Returns the number of milliseconds for the ""to date"".
 * @param date The date for which the number of milliseconds should retrieved.
 * @return Returns null if the given date is null, otherwise the number of milliseconds for the given date.
 */
private Long getToDateTime(Date date){
  Long toDateTime=null;
  Date toDate=getToDate(date);
  if (toDate != null) {
    toDateTime=Long.valueOf(toDate.getTime());
  }
  return toDateTime;
}",0.986425339366516
89403,"private void checkSupportedDefinitions(List<ConfigAttributeDefintion> supportedDefinitions,int parentResult,int childResult){
  for (  ConfigAttributeDefintion cad : supportedDefinitions) {
    if (cad.parent == true && parentResult == AccessDecisionVoter.ACCESS_DENIED) {
      throw new AccessDeniedException(""String_Node_Str"");
    }
 else     if (cad.parent == false && childResult == AccessDecisionVoter.ACCESS_DENIED) {
      throw new AccessDeniedException(""String_Node_Str"");
    }
  }
}","private void checkSupportedDefinitions(List<ConfigAttributeDefintion> supportedDefinitions,int parentResult,int childResult){
  for (  ConfigAttributeDefintion cad : supportedDefinitions) {
    if (cad.parent && parentResult == AccessDecisionVoter.ACCESS_DENIED) {
      throw new AccessDeniedException(""String_Node_Str"");
    }
 else     if (cad.parent == false && childResult == AccessDecisionVoter.ACCESS_DENIED) {
      throw new AccessDeniedException(""String_Node_Str"");
    }
  }
}",0.9918533604887984
89404,"public int compare(final NodeRef f1,final NodeRef f2){
  return filePlanAuthenticationService.runAsRmAdmin(new RunAsWork<Integer>(){
    public Integer doWork() throws Exception {
      return new Integer(compareImpl(f1,f2));
    }
  }
).intValue();
}","public int compare(final NodeRef f1,final NodeRef f2){
  return filePlanAuthenticationService.runAsRmAdmin(new RunAsWork<Integer>(){
    public Integer doWork() throws Exception {
      return Integer.valueOf(compareImpl(f1,f2));
    }
  }
).intValue();
}",0.9762845849802372
89405,"private int compareImpl(NodeRef f1,NodeRef f2){
  DispositionAction da1=dispositionService.getNextDispositionAction(f1);
  DispositionAction da2=dispositionService.getNextDispositionAction(f2);
  if (da1 != null && da2 != null) {
    Date asOfDate1=da1.getAsOfDate();
    Date asOfDate2=da2.getAsOfDate();
    if (asOfDate1 != null && asOfDate2 != null) {
      return asOfDate1.compareTo(asOfDate2);
    }
 else     if (asOfDate1 != null || asOfDate2 != null) {
      return asOfDate1 == null ? +1 : -1;
    }
 else {
      DispositionActionDefinition dad1=da1.getDispositionActionDefinition();
      DispositionActionDefinition dad2=da2.getDispositionActionDefinition();
      int eventsCount1=0;
      int eventsCount2=0;
      if (dad1 != null) {
        eventsCount1=dad1.getEvents().size();
      }
      if (dad2 != null) {
        eventsCount2=dad2.getEvents().size();
      }
      return new Integer(eventsCount1).compareTo(eventsCount2);
    }
  }
  return 0;
}","private int compareImpl(NodeRef f1,NodeRef f2){
  DispositionAction da1=dispositionService.getNextDispositionAction(f1);
  DispositionAction da2=dispositionService.getNextDispositionAction(f2);
  if (da1 != null && da2 != null) {
    Date asOfDate1=da1.getAsOfDate();
    Date asOfDate2=da2.getAsOfDate();
    if (asOfDate1 != null && asOfDate2 != null) {
      return asOfDate1.compareTo(asOfDate2);
    }
 else     if (asOfDate1 != null || asOfDate2 != null) {
      return asOfDate1 == null ? +1 : -1;
    }
 else {
      DispositionActionDefinition dad1=da1.getDispositionActionDefinition();
      DispositionActionDefinition dad2=da2.getDispositionActionDefinition();
      int eventsCount1=0;
      int eventsCount2=0;
      if (dad1 != null) {
        eventsCount1=dad1.getEvents().size();
      }
      if (dad2 != null) {
        eventsCount2=dad2.getEvents().size();
      }
      return Integer.valueOf(eventsCount1).compareTo(eventsCount2);
    }
  }
  return 0;
}",0.9938398357289528
89406,"public Integer doWork() throws Exception {
  return new Integer(compareImpl(f1,f2));
}","public Integer doWork() throws Exception {
  return Integer.valueOf(compareImpl(f1,f2));
}",0.9318181818181818
89407,"/** 
 * Get the currently recorded schema version for the module 
 * @return  int currently recorded schema version
 */
protected int getCurrentSchema(){
  Integer result=START_SCHEMA;
  if (attributeService.exists(KEY_MODULE_SCHEMA,getModuleId()) == true) {
    result=(Integer)attributeService.getAttribute(KEY_MODULE_SCHEMA,getModuleId());
  }
  return result;
}","/** 
 * Get the currently recorded schema version for the module
 * @return  int currently recorded schema version
 */
protected int getCurrentSchema(){
  Integer result=START_SCHEMA;
  if (attributeService.exists(KEY_MODULE_SCHEMA,getModuleId()) == true) {
    result=(Integer)attributeService.getAttribute(KEY_MODULE_SCHEMA,getModuleId());
  }
  return result;
}",0.9986282578875172
89408,"/** 
 * Update the recorded schema version for the module.
 * @param newSchema new schema version
 */
protected void updateSchema(int newSchema){
  attributeService.setAttribute(new Integer(newSchema),KEY_MODULE_SCHEMA,getModuleId());
}","/** 
 * Update the recorded schema version for the module.
 * @param newSchema new schema version
 */
protected void updateSchema(int newSchema){
  attributeService.setAttribute(Integer.valueOf(newSchema),KEY_MODULE_SCHEMA,getModuleId());
}",0.9747899159663864
89409,"public void beforeRMActionExecution(NodeRef nodeRef,String name,Map<String,Serializable> parameters);","void beforeRMActionExecution(NodeRef nodeRef,String name,Map<String,Serializable> parameters);",0.964102564102564
89410,"public void beforeCreateReference(NodeRef fromNodeRef,NodeRef toNodeRef,QName reference);","void beforeCreateReference(NodeRef fromNodeRef,NodeRef toNodeRef,QName reference);",0.95906432748538
89411,"/** 
 * @param fromNodeRef   from node reference
 * @param toNodeRef     to node reference
 * @param reference     name of reference
 */
public void onRemoveReference(NodeRef fromNodeRef,NodeRef toNodeRef,QName reference);","/** 
 * @param fromNodeRef   from node reference
 * @param toNodeRef     to node reference
 * @param reference     name of reference
 */
void onRemoveReference(NodeRef fromNodeRef,NodeRef toNodeRef,QName reference);",0.9839816933638444
89412,"public void onRMActionExecution(NodeRef nodeRef,String name,Map<String,Serializable> parameters);","void onRMActionExecution(NodeRef nodeRef,String name,Map<String,Serializable> parameters);",0.962566844919786
89413,"/** 
 * @param nodeRef   node reference
 */
public void onFileRecord(NodeRef nodeRef);","/** 
 * @param nodeRef   node reference
 */
void onFileRecord(NodeRef nodeRef);",0.9575757575757576
89414,"public void onCreateReference(NodeRef fromNodeRef,NodeRef toNodeRef,QName reference);","void onCreateReference(NodeRef fromNodeRef,NodeRef toNodeRef,QName reference);",0.9570552147239264
89415,"/** 
 * @param nodeRef   node reference
 */
public void beforeFileRecord(NodeRef nodeRef);","/** 
 * @param nodeRef   node reference
 */
void beforeFileRecord(NodeRef nodeRef);",0.9595375722543352
89416,"public void beforeRemoveReference(NodeRef fromNodeRef,NodeRef toNodeRef,QName reference);","void beforeRemoveReference(NodeRef fromNodeRef,NodeRef toNodeRef,QName reference);",0.95906432748538
89417,"/** 
 * Some admin-related rmActions execute against a target nodeRef which is not provided by the calling code, but is instead an implementation detail of the action.
 * @return the target nodeRef
 */
public NodeRef getImplicitTargetNodeRef();","/** 
 * Some admin-related rmActions execute against a target nodeRef which is not provided by the calling code, but is instead an implementation detail of the action.
 * @return the target nodeRef
 */
NodeRef getImplicitTargetNodeRef();",0.9854469854469856
89418,"/** 
 * Get the name of the action
 * @return  String  action name
 */
public String getName();","/** 
 * Get the name of the action
 * @return  String  action name
 */
String getName();",0.9617486338797814
89419,"/** 
 * Get the label of the action
 * @return  String  action label
 */
public String getLabel();","/** 
 * Get the label of the action
 * @return  String  action label
 */
String getLabel();",0.9629629629629628
89420,"/** 
 * Get the records management action definition.
 * @return
 * @since 2.1
 */
public RecordsManagementActionDefinition getRecordsManagementActionDefinition();","/** 
 * Get the records management action definition.
 * @return
 * @since 2.1
 */
RecordsManagementActionDefinition getRecordsManagementActionDefinition();",0.9780564263322884
89421,"/** 
 * Indicates whether the action is public or not
 * @return
 * @since 2.1
 */
public boolean isPublicAction();","/** 
 * Indicates whether the action is public or not
 * @return
 * @since 2.1
 */
boolean isPublicAction();",0.968609865470852
89422,"/** 
 * Get the description of the action
 * @return  String  action description 
 */
public String getDescription();","/** 
 * Get the description of the action
 * @return  String  action description
 */
String getDescription();",0.9646017699115044
89423,"/** 
 * Execution of the action
 * @param filePlanComponent     file plan component the action is executed upon
 * @param parameters            action parameters
 */
public RecordsManagementActionResult execute(NodeRef filePlanComponent,Map<String,Serializable> parameters);","/** 
 * Execution of the action
 * @param filePlanComponent     file plan component the action is executed upon
 * @param parameters            action parameters
 */
RecordsManagementActionResult execute(NodeRef filePlanComponent,Map<String,Serializable> parameters);",0.9870609981515712
89424,"/** 
 * @return
 */
public boolean isPublicCondition();","/** 
 * @return
 */
boolean isPublicCondition();",0.9320388349514565
89425,"/** 
 * Get the name of the action condition
 * @return  String  action condition name
 */
public String getName();","/** 
 * Get the name of the action condition
 * @return  String  action condition name
 */
String getName();",0.968609865470852
89426,"/** 
 * Get the label of the action condition
 * @return  String  action condition label
 */
public String getLabel();","/** 
 * Get the label of the action condition
 * @return  String  action condition label
 */
String getLabel();",0.9694323144104804
89427,"/** 
 * @return
 */
public RecordsManagementActionConditionDefinition getRecordsManagementActionConditionDefinition();","/** 
 * @return
 */
RecordsManagementActionConditionDefinition getRecordsManagementActionConditionDefinition();",0.9694323144104804
89428,"/** 
 * Get the description of the action condition
 * @return  String  action condition description 
 */
public String getDescription();","/** 
 * Get the description of the action condition
 * @return  String  action condition description
 */
String getDescription();",0.9699248120300752
89429,"/** 
 * Persists any changes made to the events on the given disposition action definition on the given next action.
 * @param dispositionActionDef The disposition action definition node
 * @param nextAction The next disposition action
 */
@SuppressWarnings(""String_Node_Str"") private void persistEventChanges(NodeRef dispositionActionDef,DispositionAction nextAction){
  List<String> stepEvents=(List<String>)nodeService.getProperty(dispositionActionDef,PROP_DISPOSITION_EVENT);
  List<EventCompletionDetails> eventsList=nextAction.getEventCompletionDetails();
  List<String> nextActionEvents=new ArrayList<String>(eventsList.size());
  for (  EventCompletionDetails event : eventsList) {
    String eventName=event.getEventName();
    nextActionEvents.add(eventName);
    if (stepEvents != null && stepEvents.contains(event.getEventName()) == false) {
      nodeService.removeChild(nextAction.getNodeRef(),event.getNodeRef());
      if (logger.isDebugEnabled())       logger.debug(""String_Node_Str"" + eventName + ""String_Node_Str""+ nextAction.getName()+ ""String_Node_Str""+ nextAction.getNodeRef()+ ""String_Node_Str"");
    }
  }
  if (stepEvents != null) {
    for (    String eventName : stepEvents) {
      if (!nextActionEvents.contains(eventName)) {
        createEvent(recordsManagementEventService.getEvent(eventName),nextAction.getNodeRef());
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"" + eventName + ""String_Node_Str""+ nextAction.getName()+ ""String_Node_Str""+ nextAction.getNodeRef()+ ""String_Node_Str"");
        }
      }
    }
  }
  boolean eligible=updateEventEligible(nextAction);
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + eligible + ""String_Node_Str""+ nextAction.getName()+ ""String_Node_Str""+ nextAction.getNodeRef()+ ""String_Node_Str"");
  }
}","/** 
 * Persists any changes made to the events on the given disposition action definition on the given next action.
 * @param dispositionActionDef The disposition action definition node
 * @param nextAction The next disposition action
 */
@SuppressWarnings(""String_Node_Str"") private void persistEventChanges(NodeRef dispositionActionDef,DispositionAction nextAction){
  List<String> stepEvents=(List<String>)nodeService.getProperty(dispositionActionDef,PROP_DISPOSITION_EVENT);
  List<EventCompletionDetails> eventsList=nextAction.getEventCompletionDetails();
  List<String> nextActionEvents=new ArrayList<String>(eventsList.size());
  for (  EventCompletionDetails event : eventsList) {
    String eventName=event.getEventName();
    nextActionEvents.add(eventName);
    if (stepEvents != null && stepEvents.contains(event.getEventName()) == false) {
      nodeService.removeChild(nextAction.getNodeRef(),event.getNodeRef());
      if (logger.isDebugEnabled()) {
        logger.debug(""String_Node_Str"" + eventName + ""String_Node_Str""+ nextAction.getName()+ ""String_Node_Str""+ nextAction.getNodeRef()+ ""String_Node_Str"");
      }
    }
  }
  if (stepEvents != null) {
    for (    String eventName : stepEvents) {
      if (!nextActionEvents.contains(eventName)) {
        createEvent(recordsManagementEventService.getEvent(eventName),nextAction.getNodeRef());
        if (logger.isDebugEnabled()) {
          logger.debug(""String_Node_Str"" + eventName + ""String_Node_Str""+ nextAction.getName()+ ""String_Node_Str""+ nextAction.getNodeRef()+ ""String_Node_Str"");
        }
      }
    }
  }
  boolean eligible=updateEventEligible(nextAction);
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"" + eligible + ""String_Node_Str""+ nextAction.getName()+ ""String_Node_Str""+ nextAction.getNodeRef()+ ""String_Node_Str"");
  }
}",0.9967159277504104
89430,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) == true && freezeService.isFrozen(actionedUponNodeRef) == false) {
    if (recordService.isRecord(actionedUponNodeRef)) {
      ChildAssociationRef assocRef=nodeService.getPrimaryParent(actionedUponNodeRef);
      if (assocRef != null) {
        actionedUponNodeRef=assocRef.getParentRef();
      }
    }
    if (recordFolderService.isRecordFolder(actionedUponNodeRef) == true) {
      Boolean isClosed=(Boolean)nodeService.getProperty(actionedUponNodeRef,PROP_IS_CLOSED);
      if (Boolean.TRUE.equals(isClosed) == true) {
        nodeService.setProperty(actionedUponNodeRef,PROP_IS_CLOSED,false);
      }
    }
 else {
      if (logger.isWarnEnabled())       logger.warn(I18NUtil.getMessage(MSG_NO_OPEN_RECORD_FOLDER,actionedUponNodeRef.toString()));
    }
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) == true && freezeService.isFrozen(actionedUponNodeRef) == false) {
    if (recordService.isRecord(actionedUponNodeRef)) {
      ChildAssociationRef assocRef=nodeService.getPrimaryParent(actionedUponNodeRef);
      if (assocRef != null) {
        actionedUponNodeRef=assocRef.getParentRef();
      }
    }
    if (recordFolderService.isRecordFolder(actionedUponNodeRef) == true) {
      Boolean isClosed=(Boolean)nodeService.getProperty(actionedUponNodeRef,PROP_IS_CLOSED);
      if (Boolean.TRUE.equals(isClosed) == true) {
        nodeService.setProperty(actionedUponNodeRef,PROP_IS_CLOSED,false);
      }
    }
 else {
      if (logger.isWarnEnabled()) {
        logger.warn(I18NUtil.getMessage(MSG_NO_OPEN_RECORD_FOLDER,actionedUponNodeRef.toString()));
      }
    }
  }
}",0.9943977591036416
89431,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.RecordsManagementAdminService#setCustomPropertyDefinitionLabel(org.alfresco.service.namespace.QName,java.lang.String)
 */
public QName setCustomPropertyDefinitionLabel(QName propQName,String newLabel){
  ParameterCheck.mandatory(""String_Node_Str"",propQName);
  PropertyDefinition propDefn=dictionaryService.getProperty(propQName);
  if (propDefn == null) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_PROP_EXIST,propQName));
  }
  if (newLabel == null)   return propQName;
  NodeRef modelRef=getCustomModelRef(propQName.getNamespaceURI());
  M2Model deserializedModel=readCustomContentModel(modelRef);
  M2Property targetProperty=findProperty(propQName,deserializedModel);
  targetProperty.setTitle(newLabel);
  writeCustomContentModel(modelRef,deserializedModel);
  if (logger.isInfoEnabled()) {
    logger.info(""String_Node_Str"" + propQName + ""String_Node_Str""+ newLabel);
  }
  return propQName;
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.RecordsManagementAdminService#setCustomPropertyDefinitionLabel(org.alfresco.service.namespace.QName,java.lang.String)
 */
public QName setCustomPropertyDefinitionLabel(QName propQName,String newLabel){
  ParameterCheck.mandatory(""String_Node_Str"",propQName);
  PropertyDefinition propDefn=dictionaryService.getProperty(propQName);
  if (propDefn == null) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_PROP_EXIST,propQName));
  }
  if (newLabel == null) {
    return propQName;
  }
  NodeRef modelRef=getCustomModelRef(propQName.getNamespaceURI());
  M2Model deserializedModel=readCustomContentModel(modelRef);
  M2Property targetProperty=findProperty(propQName,deserializedModel);
  targetProperty.setTitle(newLabel);
  writeCustomContentModel(modelRef,deserializedModel);
  if (logger.isInfoEnabled()) {
    logger.info(""String_Node_Str"" + propQName + ""String_Node_Str""+ newLabel);
  }
  return propQName;
}",0.9765066394279878
89432,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.RecordsManagementAdminService#updateCustomPropertyDefinitionName(org.alfresco.service.namespace.QName,java.lang.String)
 */
public QName updateCustomPropertyDefinitionName(QName propQName,String newName) throws CustomMetadataException {
  ParameterCheck.mandatory(""String_Node_Str"",propQName);
  PropertyDefinition propDefn=dictionaryService.getProperty(propQName);
  if (propDefn == null) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_PROP_EXIST,propQName));
  }
  if (newName == null)   return propQName;
  QName newPropQName=getQNameForClientId(newName);
  if (newPropQName != null) {
    PropertyDefinition newPropDefn=dictionaryService.getProperty(newPropQName);
    if (newPropDefn != null && propDefn.equals(newPropDefn) == false) {
      String propIdAsString=newPropQName.toPrefixString(namespaceService);
      throw new PropertyAlreadyExistsMetadataException(propIdAsString);
    }
  }
  NodeRef modelRef=getCustomModelRef(propQName.getNamespaceURI());
  M2Model deserializedModel=readCustomContentModel(modelRef);
  M2Property targetProperty=findProperty(propQName,deserializedModel);
  targetProperty.setName(new StringBuilder().append(RecordsManagementCustomModel.RM_CUSTOM_PREFIX).append(QName.NAMESPACE_PREFIX).append(newName).toString());
  targetProperty.setTitle(URLDecoder.decode(newName));
  writeCustomContentModel(modelRef,deserializedModel);
  if (logger.isInfoEnabled()) {
    logger.info(""String_Node_Str"" + propQName + ""String_Node_Str""+ newName);
  }
  return propQName;
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.RecordsManagementAdminService#updateCustomPropertyDefinitionName(org.alfresco.service.namespace.QName,java.lang.String)
 */
public QName updateCustomPropertyDefinitionName(QName propQName,String newName) throws CustomMetadataException {
  ParameterCheck.mandatory(""String_Node_Str"",propQName);
  PropertyDefinition propDefn=dictionaryService.getProperty(propQName);
  if (propDefn == null) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_PROP_EXIST,propQName));
  }
  if (newName == null) {
    return propQName;
  }
  QName newPropQName=getQNameForClientId(newName);
  if (newPropQName != null) {
    PropertyDefinition newPropDefn=dictionaryService.getProperty(newPropQName);
    if (newPropDefn != null && propDefn.equals(newPropDefn) == false) {
      String propIdAsString=newPropQName.toPrefixString(namespaceService);
      throw new PropertyAlreadyExistsMetadataException(propIdAsString);
    }
  }
  NodeRef modelRef=getCustomModelRef(propQName.getNamespaceURI());
  M2Model deserializedModel=readCustomContentModel(modelRef);
  M2Property targetProperty=findProperty(propQName,deserializedModel);
  targetProperty.setName(new StringBuilder().append(RecordsManagementCustomModel.RM_CUSTOM_PREFIX).append(QName.NAMESPACE_PREFIX).append(newName).toString());
  targetProperty.setTitle(URLDecoder.decode(newName));
  writeCustomContentModel(modelRef,deserializedModel);
  if (logger.isInfoEnabled()) {
    logger.info(""String_Node_Str"" + propQName + ""String_Node_Str""+ newName);
  }
  return propQName;
}",0.9974667511082964
89433,"private M2Model readCustomContentModel(NodeRef modelNodeRef){
  ContentReader reader=this.contentService.getReader(modelNodeRef,ContentModel.TYPE_CONTENT);
  if (reader.exists() == false) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_CUSTOM_MODEL_NO_CONTENT,modelNodeRef.toString()));
  }
  InputStream contentIn=null;
  M2Model deserializedModel=null;
  try {
    contentIn=reader.getContentInputStream();
    deserializedModel=M2Model.createModel(contentIn);
  }
  finally {
    try {
      if (contentIn != null)       contentIn.close();
    }
 catch (    IOException ignored) {
    }
  }
  return deserializedModel;
}","private M2Model readCustomContentModel(NodeRef modelNodeRef){
  ContentReader reader=this.contentService.getReader(modelNodeRef,ContentModel.TYPE_CONTENT);
  if (reader.exists() == false) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_CUSTOM_MODEL_NO_CONTENT,modelNodeRef.toString()));
  }
  InputStream contentIn=null;
  M2Model deserializedModel=null;
  try {
    contentIn=reader.getContentInputStream();
    deserializedModel=M2Model.createModel(contentIn);
  }
  finally {
    try {
      if (contentIn != null) {
        contentIn.close();
      }
    }
 catch (    IOException ignored) {
    }
  }
  return deserializedModel;
}",0.953560371517028
89434,"/** 
 * @see java.lang.Object#equals(java.lang.Object)
 */
@Override public boolean equals(Object obj){
  if (this == obj)   return true;
  if (obj == null)   return false;
  if (getClass() != obj.getClass())   return false;
  final AbstractCapability other=(AbstractCapability)obj;
  if (getName() == null) {
    if (other.getName() != null)     return false;
  }
 else   if (!getName().equals(other.getName()))   return false;
  return true;
}","/** 
 * @see java.lang.Object#equals(java.lang.Object)
 */
@Override public boolean equals(Object obj){
  if (this == obj) {
    return true;
  }
  if (obj == null) {
    return false;
  }
  if (getClass() != obj.getClass()) {
    return false;
  }
  final AbstractCapability other=(AbstractCapability)obj;
  if (getName() == null) {
    if (other.getName() != null) {
      return false;
    }
  }
 else   if (!getName().equals(other.getName())) {
    return false;
  }
  return true;
}",0.7124463519313304
89435,"@Override public boolean equals(Object obj){
  if (this == obj)   return true;
  if (obj == null)   return false;
  if (getClass() != obj.getClass())   return false;
  final CustomMapping other=(CustomMapping)obj;
  if (!from.equals(other.getFrom())) {
    return false;
  }
  if (!to.equals(other.getTo())) {
    return false;
  }
  return true;
}","@Override public boolean equals(Object obj){
  if (this == obj) {
    return true;
  }
  if (obj == null) {
    return false;
  }
  if (getClass() != obj.getClass()) {
    return false;
  }
  final CustomMapping other=(CustomMapping)obj;
  if (!from.equals(other.getFrom())) {
    return false;
  }
  if (!to.equals(other.getTo())) {
    return false;
  }
  return true;
}",0.7888888888888889
89436,"/** 
 * Load the events from the persistant storage
 */
private void loadEvents(){
  AuthenticationUtil.runAs(new AuthenticationUtil.RunAsWork<Object>(){
    public Object doWork() throws Exception {
      if (nodeService.exists(CONFIG_NODE_REF) == false) {
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
      ContentReader reader=contentService.getReader(CONFIG_NODE_REF,ContentModel.PROP_CONTENT);
      String jsonString=reader.getContentString();
      JSONObject configJSON=new JSONObject(jsonString);
      JSONArray eventsJSON=configJSON.getJSONArray(""String_Node_Str"");
      events=new HashMap<String,RecordsManagementEvent>(eventsJSON.length());
      for (int i=0; i < eventsJSON.length(); i++) {
        JSONObject eventJSON=eventsJSON.getJSONObject(i);
        String eventType=eventJSON.getString(""String_Node_Str"");
        String eventName=eventJSON.getString(""String_Node_Str"");
        String eventDisplayLabel=eventJSON.getString(""String_Node_Str"");
        String translated=I18NUtil.getMessage(eventDisplayLabel);
        if (translated != null)         eventDisplayLabel=translated;
        if (eventTypes.containsKey(eventType) == false) {
          throw new AlfrescoRuntimeException(""String_Node_Str"" + eventName + ""String_Node_Str""+ eventType+ ""String_Node_Str"");
        }
        RecordsManagementEvent event=new RecordsManagementEvent(eventType,eventName,eventDisplayLabel);
        events.put(event.getName(),event);
      }
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}","/** 
 * Load the events from the persistant storage
 */
private void loadEvents(){
  AuthenticationUtil.runAs(new AuthenticationUtil.RunAsWork<Object>(){
    public Object doWork() throws Exception {
      if (nodeService.exists(CONFIG_NODE_REF) == false) {
        throw new AlfrescoRuntimeException(""String_Node_Str"");
      }
      ContentReader reader=contentService.getReader(CONFIG_NODE_REF,ContentModel.PROP_CONTENT);
      String jsonString=reader.getContentString();
      JSONObject configJSON=new JSONObject(jsonString);
      JSONArray eventsJSON=configJSON.getJSONArray(""String_Node_Str"");
      events=new HashMap<String,RecordsManagementEvent>(eventsJSON.length());
      for (int i=0; i < eventsJSON.length(); i++) {
        JSONObject eventJSON=eventsJSON.getJSONObject(i);
        String eventType=eventJSON.getString(""String_Node_Str"");
        String eventName=eventJSON.getString(""String_Node_Str"");
        String eventDisplayLabel=eventJSON.getString(""String_Node_Str"");
        String translated=I18NUtil.getMessage(eventDisplayLabel);
        if (translated != null) {
          eventDisplayLabel=translated;
        }
        if (eventTypes.containsKey(eventType) == false) {
          throw new AlfrescoRuntimeException(""String_Node_Str"" + eventName + ""String_Node_Str""+ eventType+ ""String_Node_Str"");
        }
        RecordsManagementEvent event=new RecordsManagementEvent(eventType,eventName,eventDisplayLabel);
        events.put(event.getName(),event);
      }
      return null;
    }
  }
,AuthenticationUtil.getSystemUserName());
}",0.9955099422706865
89437,"/** 
 * Marks the recordLevelDisposition property as protected to disable editing
 * @param form The Form instance
 */
protected void protectRecordLevelDispositionPropertyField(Form form){
  List<FieldDefinition> fieldDefs=form.getFieldDefinitions();
  for (  FieldDefinition fieldDef : fieldDefs) {
    if (fieldDef.getName().equals(RecordsManagementModel.PROP_RECORD_LEVEL_DISPOSITION.toPrefixString(this.namespaceService))) {
      fieldDef.setProtectedField(true);
      break;
    }
  }
  if (logger.isDebugEnabled())   logger.debug(""String_Node_Str"");
}","/** 
 * Marks the recordLevelDisposition property as protected to disable editing
 * @param form The Form instance
 */
protected void protectRecordLevelDispositionPropertyField(Form form){
  List<FieldDefinition> fieldDefs=form.getFieldDefinitions();
  for (  FieldDefinition fieldDef : fieldDefs) {
    if (fieldDef.getName().equals(RecordsManagementModel.PROP_RECORD_LEVEL_DISPOSITION.toPrefixString(this.namespaceService))) {
      fieldDef.setProtectedField(true);
      break;
    }
  }
  if (logger.isDebugEnabled()) {
    logger.debug(""String_Node_Str"");
  }
}",0.9928952042628776
89438,"/** 
 * Get the allowed values.  Note that these are <tt>String</tt> instances, but may  represent non-<tt>String</tt> values.  It is up to the caller to distinguish.
 * @return Returns the values allowed
 */
@Override public List<String> getRawAllowedValues(){
  String runAsUser=AuthenticationUtil.getRunAsUser();
  if ((runAsUser != null) && (!runAsUser.equals(AuthenticationUtil.getSystemUserName())) && (caveatConfigService != null)) {
    List<String> allowedForUser=caveatConfigService.getRMAllowedValues(getShortName());
    List<String> filteredList=new ArrayList<String>(allowedForUser.size());
    for (    String allowed : allowedForUser) {
      if (this.allowedValues.contains(allowed)) {
        filteredList.add(allowed);
      }
    }
    return filteredList;
  }
 else {
    return this.allowedValues;
  }
}","/** 
 * Get the allowed values.  Note that these are <tt>String</tt> instances, but may represent non-<tt>String</tt> values.  It is up to the caller to distinguish.
 * @return Returns the values allowed
 */
@Override public List<String> getRawAllowedValues(){
  String runAsUser=AuthenticationUtil.getRunAsUser();
  if ((runAsUser != null) && (!runAsUser.equals(AuthenticationUtil.getSystemUserName())) && (caveatConfigService != null)) {
    List<String> allowedForUser=caveatConfigService.getRMAllowedValues(getShortName());
    List<String> filteredList=new ArrayList<String>(allowedForUser.size());
    for (    String allowed : allowedForUser) {
      if (this.allowedValues.contains(allowed)) {
        filteredList.add(allowed);
      }
    }
    return filteredList;
  }
 else {
    return this.allowedValues;
  }
}",0.9993935718617344
89439,"@Override protected void evaluateSingleValue(Object value){
  String valueStr=null;
  try {
    valueStr=DefaultTypeConverter.INSTANCE.convert(String.class,value);
  }
 catch (  TypeConversionException e) {
    throw new ConstraintException(ERR_NON_STRING,value);
  }
  if (isCaseSensitive()) {
    if (!getAllowedValues().contains(valueStr)) {
      throw new ConstraintException(ERR_INVALID_VALUE,value);
    }
  }
 else {
    if (!getAllowedValuesUpper().contains(valueStr.toUpperCase())) {
      throw new ConstraintException(ERR_INVALID_VALUE,value);
    }
  }
}","@Override protected void evaluateSingleValue(Object value){
  String valueStr=null;
  try {
    valueStr=DefaultTypeConverter.INSTANCE.convert(String.class,value);
  }
 catch (  TypeConversionException e) {
    throw new ConstraintException(ERR_NON_STRING,value,e);
  }
  if (isCaseSensitive()) {
    if (!getAllowedValues().contains(valueStr)) {
      throw new ConstraintException(ERR_INVALID_VALUE,value);
    }
  }
 else {
    if (!getAllowedValuesUpper().contains(valueStr.toUpperCase())) {
      throw new ConstraintException(ERR_INVALID_VALUE,value);
    }
  }
}",0.9982394366197184
89440,"@Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  boolean importData=false;
  if (req.getParameter(ARG_IMPORT) != null) {
    importData=Boolean.parseBoolean(req.getParameter(ARG_IMPORT));
  }
  String siteName=RmSiteType.DEFAULT_SITE_NAME;
  if (req.getParameter(ARG_SITE_NAME) != null) {
    siteName=req.getParameter(ARG_SITE_NAME);
  }
  if (importData) {
    SiteInfo site=siteService.getSite(siteName);
    if (site == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + siteName);
    }
    NodeRef filePlan=siteService.getContainer(siteName,RmSiteType.COMPONENT_DOCUMENT_LIBRARY);
    if (filePlan == null) {
      filePlan=siteService.createContainer(siteName,RmSiteType.COMPONENT_DOCUMENT_LIBRARY,TYPE_FILE_PLAN,null);
    }
    InputStream is=BootstrapTestDataGet.class.getClassLoader().getResourceAsStream(XML_IMPORT);
    if (is == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
    Reader viewReader=null;
    try {
      viewReader=new InputStreamReader(is,charsetName);
    }
 catch (    UnsupportedEncodingException error) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + charsetName + ""String_Node_Str"");
    }
    Location location=new Location(filePlan);
    importerService.importView(viewReader,location,null,null);
  }
  BootstrapTestDataGet.patchLoadedData(searchService,nodeService,recordsManagementService,recordsManagementActionService,permissionService,authorityService,recordsManagementSecurityService,recordsManagementSearchBehaviour,dispositionService,recordFolderService);
  Map<String,Object> model=new HashMap<String,Object>(1,1.0f);
  model.put(""String_Node_Str"",true);
  return model;
}","@Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  boolean importData=false;
  if (req.getParameter(ARG_IMPORT) != null) {
    importData=Boolean.parseBoolean(req.getParameter(ARG_IMPORT));
  }
  String siteName=RmSiteType.DEFAULT_SITE_NAME;
  if (req.getParameter(ARG_SITE_NAME) != null) {
    siteName=req.getParameter(ARG_SITE_NAME);
  }
  if (importData) {
    SiteInfo site=siteService.getSite(siteName);
    if (site == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + siteName);
    }
    NodeRef filePlan=siteService.getContainer(siteName,RmSiteType.COMPONENT_DOCUMENT_LIBRARY);
    if (filePlan == null) {
      filePlan=siteService.createContainer(siteName,RmSiteType.COMPONENT_DOCUMENT_LIBRARY,TYPE_FILE_PLAN,null);
    }
    InputStream is=BootstrapTestDataGet.class.getClassLoader().getResourceAsStream(XML_IMPORT);
    if (is == null) {
      throw new AlfrescoRuntimeException(""String_Node_Str"");
    }
    Reader viewReader=null;
    try {
      viewReader=new InputStreamReader(is,charsetName);
    }
 catch (    UnsupportedEncodingException error) {
      throw new AlfrescoRuntimeException(""String_Node_Str"" + charsetName + ""String_Node_Str"",error);
    }
    Location location=new Location(filePlan);
    importerService.importView(viewReader,location,null,null);
  }
  BootstrapTestDataGet.patchLoadedData(searchService,nodeService,recordsManagementService,recordsManagementActionService,permissionService,authorityService,recordsManagementSecurityService,recordsManagementSearchBehaviour,dispositionService,recordFolderService);
  Map<String,Object> model=new HashMap<String,Object>(1,1.0f);
  model.put(""String_Node_Str"",true);
  return model;
}",0.998268897864974
89441,"/** 
 * @see org.alfresco.web.scripts.DeclarativeWebScript#executeImpl(org.alfresco.web.scripts.WebScriptRequest,org.alfresco.web.scripts.Status,org.alfresco.web.scripts.Cache)
 */
@Override protected Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  Map<String,Object> model=new HashMap<String,Object>(1);
  try {
    JSONObject json=new JSONObject(new JSONTokener(req.getContent().getContent()));
    customEmailMappingService.addCustomMapping(json.getString(""String_Node_Str""),json.getString(""String_Node_Str""));
    model.put(""String_Node_Str"",customEmailMappingService.getCustomMappings());
  }
 catch (  IOException iox) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",iox);
  }
catch (  JSONException je) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",je);
  }
catch (  AlfrescoRuntimeException are) {
    throw new WebScriptException(Status.STATUS_INTERNAL_SERVER_ERROR,are.getMessage());
  }
  return model;
}","/** 
 * @see org.alfresco.web.scripts.DeclarativeWebScript#executeImpl(org.alfresco.web.scripts.WebScriptRequest,org.alfresco.web.scripts.Status,org.alfresco.web.scripts.Cache)
 */
@Override protected Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  Map<String,Object> model=new HashMap<String,Object>(1);
  try {
    JSONObject json=new JSONObject(new JSONTokener(req.getContent().getContent()));
    customEmailMappingService.addCustomMapping(json.getString(""String_Node_Str""),json.getString(""String_Node_Str""));
    model.put(""String_Node_Str"",customEmailMappingService.getCustomMappings());
  }
 catch (  IOException iox) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",iox);
  }
catch (  JSONException je) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",je);
  }
catch (  AlfrescoRuntimeException are) {
    throw new WebScriptException(Status.STATUS_INTERNAL_SERVER_ERROR,are.getMessage(),are);
  }
  return model;
}",0.9980295566502464
89442,"@SuppressWarnings(""String_Node_Str"") @Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  String reqContentAsString;
  try {
    reqContentAsString=req.getContent().getContent();
  }
 catch (  IOException iox) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",iox);
  }
  String actionName=null;
  List<NodeRef> targetNodeRefs=new ArrayList<NodeRef>(1);
  Map<String,Serializable> actionParams=new HashMap<String,Serializable>(3);
  try {
    JSONObject jsonObj=new JSONObject(new JSONTokener(reqContentAsString));
    if (jsonObj.has(PARAM_NAME) == true) {
      actionName=jsonObj.getString(PARAM_NAME);
    }
    if (jsonObj.has(PARAM_NODE_REF) == true) {
      NodeRef nodeRef=new NodeRef(jsonObj.getString(PARAM_NODE_REF));
      targetNodeRefs.add(nodeRef);
    }
    if (jsonObj.has(PARAM_NODE_REFS) == true) {
      JSONArray jsonArray=jsonObj.getJSONArray(PARAM_NODE_REFS);
      if (jsonArray.length() != 0) {
        targetNodeRefs=new ArrayList<NodeRef>(jsonArray.length());
        for (int i=0; i < jsonArray.length(); i++) {
          NodeRef nodeRef=new NodeRef(jsonArray.getString(i));
          targetNodeRefs.add(nodeRef);
        }
      }
    }
    if (jsonObj.has(PARAM_PARAMS)) {
      JSONObject paramsObj=jsonObj.getJSONObject(PARAM_PARAMS);
      for (Iterator<String> iter=paramsObj.keys(); iter.hasNext(); ) {
        String nextKeyString=iter.next();
        Object nextValue=paramsObj.get(nextKeyString);
        if (nextValue instanceof JSONObject) {
          if (((JSONObject)nextValue).has(""String_Node_Str"") == true) {
            String dateStringValue=((JSONObject)nextValue).getString(""String_Node_Str"");
            nextValue=ISO8601DateFormat.parse(dateStringValue);
          }
        }
        actionParams.put(nextKeyString,(Serializable)nextValue);
      }
    }
  }
 catch (  JSONException exception) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"");
  }
  if (actionName == null) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"");
  }
  StringBuffer targetNodeRefsString=new StringBuffer(30);
  boolean firstTime=true;
  for (  NodeRef targetNodeRef : targetNodeRefs) {
    if (nodeService.exists(targetNodeRef) == false) {
      throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + targetNodeRef.toString() + ""String_Node_Str"");
    }
    if (firstTime == true) {
      firstTime=false;
    }
 else {
      targetNodeRefsString.append(""String_Node_Str"");
    }
    targetNodeRefsString.append(targetNodeRef.toString());
  }
  if (logger.isDebugEnabled()) {
    StringBuilder msg=new StringBuilder();
    msg.append(""String_Node_Str"").append(actionName).append(""String_Node_Str"").append(targetNodeRefsString.toString()).append(""String_Node_Str"").append(actionParams);
    logger.debug(msg.toString());
  }
  Map<String,Object> model=new HashMap<String,Object>();
  if (targetNodeRefs.isEmpty()) {
    RecordsManagementActionResult result=this.rmActionService.executeRecordsManagementAction(actionName,actionParams);
    if (result.getValue() != null) {
      model.put(""String_Node_Str"",result.getValue().toString());
    }
  }
 else {
    Map<NodeRef,RecordsManagementActionResult> resultMap=this.rmActionService.executeRecordsManagementAction(targetNodeRefs,actionName,actionParams);
    Map<String,String> results=new HashMap<String,String>(resultMap.size());
    for (    NodeRef nodeRef : resultMap.keySet()) {
      Object value=resultMap.get(nodeRef).getValue();
      if (value != null) {
        results.put(nodeRef.toString(),resultMap.get(nodeRef).getValue().toString());
      }
    }
    model.put(""String_Node_Str"",results);
  }
  model.put(""String_Node_Str"",""String_Node_Str"" + actionName + ""String_Node_Str""+ targetNodeRefsString.toString());
  return model;
}","@SuppressWarnings(""String_Node_Str"") @Override public Map<String,Object> executeImpl(WebScriptRequest req,Status status,Cache cache){
  String reqContentAsString;
  try {
    reqContentAsString=req.getContent().getContent();
  }
 catch (  IOException iox) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",iox);
  }
  String actionName=null;
  List<NodeRef> targetNodeRefs=new ArrayList<NodeRef>(1);
  Map<String,Serializable> actionParams=new HashMap<String,Serializable>(3);
  try {
    JSONObject jsonObj=new JSONObject(new JSONTokener(reqContentAsString));
    if (jsonObj.has(PARAM_NAME) == true) {
      actionName=jsonObj.getString(PARAM_NAME);
    }
    if (jsonObj.has(PARAM_NODE_REF) == true) {
      NodeRef nodeRef=new NodeRef(jsonObj.getString(PARAM_NODE_REF));
      targetNodeRefs.add(nodeRef);
    }
    if (jsonObj.has(PARAM_NODE_REFS) == true) {
      JSONArray jsonArray=jsonObj.getJSONArray(PARAM_NODE_REFS);
      if (jsonArray.length() != 0) {
        targetNodeRefs=new ArrayList<NodeRef>(jsonArray.length());
        for (int i=0; i < jsonArray.length(); i++) {
          NodeRef nodeRef=new NodeRef(jsonArray.getString(i));
          targetNodeRefs.add(nodeRef);
        }
      }
    }
    if (jsonObj.has(PARAM_PARAMS)) {
      JSONObject paramsObj=jsonObj.getJSONObject(PARAM_PARAMS);
      for (Iterator<String> iter=paramsObj.keys(); iter.hasNext(); ) {
        String nextKeyString=iter.next();
        Object nextValue=paramsObj.get(nextKeyString);
        if (nextValue instanceof JSONObject) {
          if (((JSONObject)nextValue).has(""String_Node_Str"") == true) {
            String dateStringValue=((JSONObject)nextValue).getString(""String_Node_Str"");
            nextValue=ISO8601DateFormat.parse(dateStringValue);
          }
        }
        actionParams.put(nextKeyString,(Serializable)nextValue);
      }
    }
  }
 catch (  JSONException exception) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"",exception);
  }
  if (actionName == null) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,""String_Node_Str"");
  }
  StringBuffer targetNodeRefsString=new StringBuffer(30);
  boolean firstTime=true;
  for (  NodeRef targetNodeRef : targetNodeRefs) {
    if (nodeService.exists(targetNodeRef) == false) {
      throw new WebScriptException(Status.STATUS_NOT_FOUND,""String_Node_Str"" + targetNodeRef.toString() + ""String_Node_Str"");
    }
    if (firstTime == true) {
      firstTime=false;
    }
 else {
      targetNodeRefsString.append(""String_Node_Str"");
    }
    targetNodeRefsString.append(targetNodeRef.toString());
  }
  if (logger.isDebugEnabled()) {
    StringBuilder msg=new StringBuilder();
    msg.append(""String_Node_Str"").append(actionName).append(""String_Node_Str"").append(targetNodeRefsString.toString()).append(""String_Node_Str"").append(actionParams);
    logger.debug(msg.toString());
  }
  Map<String,Object> model=new HashMap<String,Object>();
  if (targetNodeRefs.isEmpty()) {
    RecordsManagementActionResult result=this.rmActionService.executeRecordsManagementAction(actionName,actionParams);
    if (result.getValue() != null) {
      model.put(""String_Node_Str"",result.getValue().toString());
    }
  }
 else {
    Map<NodeRef,RecordsManagementActionResult> resultMap=this.rmActionService.executeRecordsManagementAction(targetNodeRefs,actionName,actionParams);
    Map<String,String> results=new HashMap<String,String>(resultMap.size());
    for (    NodeRef nodeRef : resultMap.keySet()) {
      Object value=resultMap.get(nodeRef).getValue();
      if (value != null) {
        results.put(nodeRef.toString(),resultMap.get(nodeRef).getValue().toString());
      }
    }
    model.put(""String_Node_Str"",results);
  }
  model.put(""String_Node_Str"",""String_Node_Str"" + actionName + ""String_Node_Str""+ targetNodeRefsString.toString());
  return model;
}",0.998712667353244
89443,"/** 
 * Helper method for checking if an event can be edited or not. Throws an error if an event with the same display label already exists.
 * @param eventDisplayLabel The display label of the event
 * @param eventName The name of the event
 * @param eventType The type of the event
 * @return true if the event can be edited, false otherwise
 */
private boolean canEditEvent(String eventDisplayLabel,String eventName,String eventType){
  boolean canEditEvent;
  try {
    canEditEvent=rmEventService.canEditEvent(eventDisplayLabel,eventName,eventType);
  }
 catch (  AlfrescoRuntimeException are) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,are.getLocalizedMessage());
  }
  return canEditEvent;
}","/** 
 * Helper method for checking if an event can be edited or not. Throws an error if an event with the same display label already exists.
 * @param eventDisplayLabel The display label of the event
 * @param eventName The name of the event
 * @param eventType The type of the event
 * @return true if the event can be edited, false otherwise
 */
private boolean canEditEvent(String eventDisplayLabel,String eventName,String eventType){
  boolean canEditEvent;
  try {
    canEditEvent=rmEventService.canEditEvent(eventDisplayLabel,eventName,eventType);
  }
 catch (  AlfrescoRuntimeException are) {
    throw new WebScriptException(Status.STATUS_BAD_REQUEST,are.getLocalizedMessage(),are);
  }
  return canEditEvent;
}",0.9972144846796658
89444,"@Override public boolean evaluate(JSONObject jsonObject){
  if (action == null) {
    return false;
  }
  try {
    JSONArray actions=getRMActions(jsonObject);
    if (actions == null) {
      return false;
    }
 else {
      if (actions.contains(action)) {
        return true;
      }
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err.getMessage());
  }
  return false;
}","@Override public boolean evaluate(JSONObject jsonObject){
  if (action == null) {
    return false;
  }
  try {
    JSONArray actions=getRMActions(jsonObject);
    if (actions == null) {
      return false;
    }
 else {
      if (actions.contains(action)) {
        return true;
      }
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err);
  }
  return false;
}",0.9269461077844312
89445,"/** 
 * Retrieve a JSONObject representing the RM extended properties
 * @param jsonObject JSONObject containing a ""node"" object as returned from the ApplicationScriptUtils class.
 * @return JSONArray containing aspects on the node
 */
public final JSONObject getRMNode(JSONObject jsonObject){
  JSONObject rmNode=null;
  try {
    JSONObject node=(JSONObject)jsonObject.get(""String_Node_Str"");
    if (node != null) {
      rmNode=(JSONObject)node.get(""String_Node_Str"");
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err.getMessage());
  }
  return rmNode;
}","/** 
 * Retrieve a JSONObject representing the RM extended properties
 * @param jsonObject JSONObject containing a ""node"" object as returned from the ApplicationScriptUtils class.
 * @return JSONArray containing aspects on the node
 */
public final JSONObject getRMNode(JSONObject jsonObject){
  JSONObject rmNode=null;
  try {
    JSONObject node=(JSONObject)jsonObject.get(""String_Node_Str"");
    if (node != null) {
      rmNode=(JSONObject)node.get(""String_Node_Str"");
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err);
  }
  return rmNode;
}",0.9892294946147472
89446,"/** 
 * Retrieve a JSONArray of applicable indicators
 * @param jsonObject JSONObject containing a ""node"" object as returned from the ApplicationScriptUtils class.
 * @return JSONArray containing applicable indicators the UI may choose to display
 */
public final JSONArray getRMIndicators(JSONObject jsonObject){
  JSONArray indicators=null;
  try {
    JSONObject rmNode=getRMNode(jsonObject);
    if (rmNode != null) {
      indicators=(JSONArray)rmNode.get(""String_Node_Str"");
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err.getMessage());
  }
  return indicators;
}","/** 
 * Retrieve a JSONArray of applicable indicators
 * @param jsonObject JSONObject containing a ""node"" object as returned from the ApplicationScriptUtils class.
 * @return JSONArray containing applicable indicators the UI may choose to display
 */
public final JSONArray getRMIndicators(JSONObject jsonObject){
  JSONArray indicators=null;
  try {
    JSONObject rmNode=getRMNode(jsonObject);
    if (rmNode != null) {
      indicators=(JSONArray)rmNode.get(""String_Node_Str"");
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err);
  }
  return indicators;
}",0.9894394800974816
89447,"/** 
 * Retrieve a JSONArray of applicable actions
 * @param jsonObject JSONObject containing a ""node"" object as returned from the ApplicationScriptUtils class.
 * @return JSONArray containing applicable actions the UI may choose to display
 */
public final JSONArray getRMActions(JSONObject jsonObject){
  JSONArray actions=null;
  try {
    JSONObject rmNode=getRMNode(jsonObject);
    if (rmNode != null) {
      actions=(JSONArray)rmNode.get(""String_Node_Str"");
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err.getMessage());
  }
  return actions;
}","/** 
 * Retrieve a JSONArray of applicable actions
 * @param jsonObject JSONObject containing a ""node"" object as returned from the ApplicationScriptUtils class.
 * @return JSONArray containing applicable actions the UI may choose to display
 */
public final JSONArray getRMActions(JSONObject jsonObject){
  JSONArray actions=null;
  try {
    JSONObject rmNode=getRMNode(jsonObject);
    if (rmNode != null) {
      actions=(JSONArray)rmNode.get(""String_Node_Str"");
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err);
  }
  return actions;
}",0.989121338912134
89448,"@Override public boolean evaluate(JSONObject jsonObject){
  if (indicator == null) {
    return false;
  }
  boolean result=false;
  try {
    JSONArray indicators=getRMIndicators(jsonObject);
    if (indicators != null) {
      if (indicators.contains(indicator)) {
        result=true;
      }
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err.getMessage());
  }
  return (result == expected);
}","@Override public boolean evaluate(JSONObject jsonObject){
  if (indicator == null) {
    return false;
  }
  boolean result=false;
  try {
    JSONArray indicators=getRMIndicators(jsonObject);
    if (indicators != null) {
      if (indicators.contains(indicator)) {
        result=true;
      }
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err);
  }
  return (result == expected);
}",0.985244040862656
89449,"@Override public boolean evaluate(JSONObject jsonObject){
  if (type == null) {
    return false;
  }
  if (isDocLibRecord(jsonObject) == true) {
    return false;
  }
  try {
    JSONObject rmNode=getRMNode(jsonObject);
    if (rmNode == null) {
      return false;
    }
 else {
      String uiType=(String)rmNode.get(""String_Node_Str"");
      if (uiType.equalsIgnoreCase(type)) {
        return true;
      }
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err.getMessage());
  }
  return false;
}","@Override public boolean evaluate(JSONObject jsonObject){
  if (type == null) {
    return false;
  }
  if (isDocLibRecord(jsonObject) == true) {
    return false;
  }
  try {
    JSONObject rmNode=getRMNode(jsonObject);
    if (rmNode == null) {
      return false;
    }
 else {
      String uiType=(String)rmNode.get(""String_Node_Str"");
      if (uiType.equalsIgnoreCase(type)) {
        return true;
      }
    }
  }
 catch (  Exception err) {
    throw new AlfrescoRuntimeException(""String_Node_Str"" + err);
  }
  return false;
}",0.943674976915974
89450,"/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) == true && freezeService.isFrozen(actionedUponNodeRef) == false) {
    String eventName=(String)action.getParameterValue(PARAM_EVENT_NAME);
    String eventCompletedBy=(String)action.getParameterValue(PARAM_EVENT_COMPLETED_BY);
    Date eventCompletedAt=(Date)action.getParameterValue(PARAM_EVENT_COMPLETED_AT);
    if (this.nodeService.hasAspect(actionedUponNodeRef,ASPECT_DISPOSITION_LIFECYCLE) == true) {
      DispositionAction da=this.dispositionService.getNextDispositionAction(actionedUponNodeRef);
      if (da != null) {
        EventCompletionDetails event=getEvent(da,eventName);
        if (event != null) {
          if (eventCompletedAt == null) {
            eventCompletedAt=new Date();
          }
          if (eventCompletedBy == null) {
            eventCompletedBy=AuthenticationUtil.getRunAsUser();
          }
          NodeRef eventNodeRef=event.getNodeRef();
          Map<QName,Serializable> props=this.nodeService.getProperties(eventNodeRef);
          props.put(PROP_EVENT_EXECUTION_COMPLETE,true);
          props.put(PROP_EVENT_EXECUTION_COMPLETED_AT,eventCompletedAt);
          props.put(PROP_EVENT_EXECUTION_COMPLETED_BY,eventCompletedBy);
          this.nodeService.setProperties(eventNodeRef,props);
          updateEventEligible(da);
        }
 else {
        }
      }
    }
  }
}","/** 
 * @see org.alfresco.repo.action.executer.ActionExecuterAbstractBase#executeImpl(org.alfresco.service.cmr.action.Action,org.alfresco.service.cmr.repository.NodeRef)
 */
@Override protected void executeImpl(Action action,NodeRef actionedUponNodeRef){
  if (nodeService.exists(actionedUponNodeRef) == true && freezeService.isFrozen(actionedUponNodeRef) == false) {
    String eventName=(String)action.getParameterValue(PARAM_EVENT_NAME);
    String eventCompletedBy=(String)action.getParameterValue(PARAM_EVENT_COMPLETED_BY);
    Date eventCompletedAt=(Date)action.getParameterValue(PARAM_EVENT_COMPLETED_AT);
    if (this.nodeService.hasAspect(actionedUponNodeRef,ASPECT_DISPOSITION_LIFECYCLE) == true) {
      DispositionAction da=this.dispositionService.getNextDispositionAction(actionedUponNodeRef);
      if (da != null) {
        EventCompletionDetails event=getEvent(da,eventName);
        if (event != null) {
          if (eventCompletedAt == null) {
            eventCompletedAt=new Date();
          }
          if (eventCompletedBy == null) {
            eventCompletedBy=AuthenticationUtil.getRunAsUser();
          }
          NodeRef eventNodeRef=event.getNodeRef();
          Map<QName,Serializable> props=this.nodeService.getProperties(eventNodeRef);
          props.put(PROP_EVENT_EXECUTION_COMPLETE,true);
          props.put(PROP_EVENT_EXECUTION_COMPLETED_AT,eventCompletedAt);
          props.put(PROP_EVENT_EXECUTION_COMPLETED_BY,eventCompletedBy);
          this.nodeService.setProperties(eventNodeRef,props);
          updateEventEligible(da);
        }
      }
    }
  }
}",0.9944064636420136
89451,"/** 
 * Validate the caveat config and optionally update the cache.
 * @param nodeRef The nodeRef of the config
 * @param updateCache Set to <code>true</code> to update the cache
 */
@SuppressWarnings(""String_Node_Str"") protected void validateAndReset(NodeRef nodeRef){
  ContentReader cr=contentService.getReader(nodeRef,ContentModel.PROP_CONTENT);
  if (cr != null) {
    String caveatConfigData=cr.getContentString();
    if (caveatConfigData != null) {
      NodeRef existing=getCaveatConfigNode();
      if ((existing != null && (!existing.equals(nodeRef)))) {
        throw new AlfrescoRuntimeException(""String_Node_Str"" + existing + ""String_Node_Str""+ nodeRef+ ""String_Node_Str"");
      }
      try {
        if (logger.isTraceEnabled()) {
          logger.trace(caveatConfigData);
        }
        Set<QName> models=new HashSet<QName>(1);
        Set<QName> props=new HashSet<QName>(10);
        Set<String> expectedPrefixes=new HashSet<String>(10);
        if (caveatModelQNames.size() > 0) {
          models.addAll(caveatModelQNames);
        }
 else {
          models.addAll(dictionaryService.getAllModels());
        }
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + models);
        }
        for (        QName model : models) {
          props.addAll(dictionaryService.getProperties(model,DATATYPE_TEXT));
          expectedPrefixes.addAll(namespaceService.getPrefixes(model.getNamespaceURI()));
        }
        if (props.size() == 0) {
          logger.warn(""String_Node_Str"");
        }
 else {
          if (logger.isTraceEnabled()) {
            logger.trace(""String_Node_Str"" + props);
          }
        }
        Map<String,Object> caveatConfigMap=JSONtoFmModel.convertJSONObjectToMap(caveatConfigData);
        for (        Map.Entry<String,Object> conEntry : caveatConfigMap.entrySet()) {
          String conStr=conEntry.getKey();
          QName conQName=QName.resolveToQName(namespaceService,conStr);
          String conPrefix=QName.splitPrefixedQName(conStr)[0];
          boolean prefixFound=false;
          for (          String expectedPrefix : expectedPrefixes) {
            if (conPrefix.equals(expectedPrefix)) {
              prefixFound=true;
            }
          }
          if (!prefixFound) {
            throw new AlfrescoRuntimeException(""String_Node_Str"" + conPrefix + ""String_Node_Str""+ conStr+ ""String_Node_Str""+ expectedPrefixes+ ""String_Node_Str"");
          }
          Map<String,List<String>> caveatMap=(Map<String,List<String>>)conEntry.getValue();
          List<String> allowedValues=null;
          boolean found=false;
          for (          QName propertyName : props) {
            PropertyDefinition propDef=dictionaryService.getProperty(propertyName);
            List<ConstraintDefinition> conDefs=propDef.getConstraints();
            for (            ConstraintDefinition conDef : conDefs) {
              final Constraint con=conDef.getConstraint();
              if (con instanceof RMListOfValuesConstraint) {
                String conName=((RMListOfValuesConstraint)con).getShortName();
                if (conName.equals(conStr)) {
                  allowedValues=AuthenticationUtil.runAs(new RunAsWork<List<String>>(){
                    public List<String> doWork(){
                      return ((RMListOfValuesConstraint)con).getAllowedValues();
                    }
                  }
,AuthenticationUtil.getSystemUserName());
                  found=true;
                  break;
                }
              }
            }
          }
          if (!found) {
          }
          if (allowedValues != null) {
            if (logger.isInfoEnabled()) {
              logger.info(""String_Node_Str"" + conQName);
            }
            for (            Map.Entry<String,List<String>> caveatEntry : caveatMap.entrySet()) {
              String authorityName=caveatEntry.getKey();
              List<String> caveatList=caveatEntry.getValue();
              if ((!authorityService.authorityExists(authorityName) && !personService.personExists(authorityName))) {
                String msg=""String_Node_Str"" + authorityName + ""String_Node_Str""+ conStr+ ""String_Node_Str"";
                logger.warn(msg);
              }
              for (              String value : caveatList) {
                if (!allowedValues.contains(value)) {
                  String msg=""String_Node_Str"" + value + ""String_Node_Str""+ authorityName+ ""String_Node_Str""+ conStr+ ""String_Node_Str"";
                  logger.warn(msg);
                }
              }
            }
          }
        }
        try {
          writeLock.lock();
          caveatConfig.getKeys().retainAll(caveatConfigMap.keySet());
          for (          Map.Entry<String,Object> conEntry : caveatConfigMap.entrySet()) {
            String conStr=conEntry.getKey();
            Map<String,List<String>> caveatMap=(Map<String,List<String>>)conEntry.getValue();
            Map<String,List<String>> cacheValue=caveatConfig.get(conStr);
            if (cacheValue == null || !cacheValue.equals(caveatMap)) {
              caveatConfig.put(conStr,caveatMap);
            }
          }
        }
  finally {
          writeLock.unlock();
        }
      }
 catch (      JSONException e) {
        throw new AlfrescoRuntimeException(""String_Node_Str"" + e);
      }
    }
  }
}","/** 
 * Validate the caveat config and optionally update the cache.
 * @param nodeRef The nodeRef of the config
 * @param updateCache Set to <code>true</code> to update the cache
 */
@SuppressWarnings(""String_Node_Str"") protected void validateAndReset(NodeRef nodeRef){
  ContentReader cr=contentService.getReader(nodeRef,ContentModel.PROP_CONTENT);
  if (cr != null) {
    String caveatConfigData=cr.getContentString();
    if (caveatConfigData != null) {
      NodeRef existing=getCaveatConfigNode();
      if ((existing != null && (!existing.equals(nodeRef)))) {
        throw new AlfrescoRuntimeException(""String_Node_Str"" + existing + ""String_Node_Str""+ nodeRef+ ""String_Node_Str"");
      }
      try {
        if (logger.isTraceEnabled()) {
          logger.trace(caveatConfigData);
        }
        Set<QName> models=new HashSet<QName>(1);
        Set<QName> props=new HashSet<QName>(10);
        Set<String> expectedPrefixes=new HashSet<String>(10);
        if (caveatModelQNames.size() > 0) {
          models.addAll(caveatModelQNames);
        }
 else {
          models.addAll(dictionaryService.getAllModels());
        }
        if (logger.isTraceEnabled()) {
          logger.trace(""String_Node_Str"" + models);
        }
        for (        QName model : models) {
          props.addAll(dictionaryService.getProperties(model,DATATYPE_TEXT));
          expectedPrefixes.addAll(namespaceService.getPrefixes(model.getNamespaceURI()));
        }
        if (props.size() == 0) {
          logger.warn(""String_Node_Str"");
        }
 else {
          if (logger.isTraceEnabled()) {
            logger.trace(""String_Node_Str"" + props);
          }
        }
        Map<String,Object> caveatConfigMap=JSONtoFmModel.convertJSONObjectToMap(caveatConfigData);
        for (        Map.Entry<String,Object> conEntry : caveatConfigMap.entrySet()) {
          String conStr=conEntry.getKey();
          QName conQName=QName.resolveToQName(namespaceService,conStr);
          String conPrefix=QName.splitPrefixedQName(conStr)[0];
          boolean prefixFound=false;
          for (          String expectedPrefix : expectedPrefixes) {
            if (conPrefix.equals(expectedPrefix)) {
              prefixFound=true;
            }
          }
          if (!prefixFound) {
            throw new AlfrescoRuntimeException(""String_Node_Str"" + conPrefix + ""String_Node_Str""+ conStr+ ""String_Node_Str""+ expectedPrefixes+ ""String_Node_Str"");
          }
          Map<String,List<String>> caveatMap=(Map<String,List<String>>)conEntry.getValue();
          List<String> allowedValues=null;
          @SuppressWarnings(""String_Node_Str"") boolean found=false;
          for (          QName propertyName : props) {
            PropertyDefinition propDef=dictionaryService.getProperty(propertyName);
            List<ConstraintDefinition> conDefs=propDef.getConstraints();
            for (            ConstraintDefinition conDef : conDefs) {
              final Constraint con=conDef.getConstraint();
              if (con instanceof RMListOfValuesConstraint) {
                String conName=((RMListOfValuesConstraint)con).getShortName();
                if (conName.equals(conStr)) {
                  allowedValues=AuthenticationUtil.runAs(new RunAsWork<List<String>>(){
                    public List<String> doWork(){
                      return ((RMListOfValuesConstraint)con).getAllowedValues();
                    }
                  }
,AuthenticationUtil.getSystemUserName());
                  found=true;
                  break;
                }
              }
            }
          }
          if (allowedValues != null) {
            if (logger.isInfoEnabled()) {
              logger.info(""String_Node_Str"" + conQName);
            }
            for (            Map.Entry<String,List<String>> caveatEntry : caveatMap.entrySet()) {
              String authorityName=caveatEntry.getKey();
              List<String> caveatList=caveatEntry.getValue();
              if ((!authorityService.authorityExists(authorityName) && !personService.personExists(authorityName))) {
                String msg=""String_Node_Str"" + authorityName + ""String_Node_Str""+ conStr+ ""String_Node_Str"";
                logger.warn(msg);
              }
              for (              String value : caveatList) {
                if (!allowedValues.contains(value)) {
                  String msg=""String_Node_Str"" + value + ""String_Node_Str""+ authorityName+ ""String_Node_Str""+ conStr+ ""String_Node_Str"";
                  logger.warn(msg);
                }
              }
            }
          }
        }
        try {
          writeLock.lock();
          caveatConfig.getKeys().retainAll(caveatConfigMap.keySet());
          for (          Map.Entry<String,Object> conEntry : caveatConfigMap.entrySet()) {
            String conStr=conEntry.getKey();
            Map<String,List<String>> caveatMap=(Map<String,List<String>>)conEntry.getValue();
            Map<String,List<String>> cacheValue=caveatConfig.get(conStr);
            if (cacheValue == null || !cacheValue.equals(caveatMap)) {
              caveatConfig.put(conStr,caveatMap);
            }
          }
        }
  finally {
          writeLock.unlock();
        }
      }
 catch (      JSONException e) {
        throw new AlfrescoRuntimeException(""String_Node_Str"" + e);
      }
    }
  }
}",0.9931871208586094
89452,"public void removeRMConstraintListValue(String listName,String valueName){
  Map<String,List<String>> members=null;
  try {
    readLock.lock();
    members=caveatConfig.get(listName);
    if (members == null) {
    }
 else {
      try {
        readLock.unlock();
        writeLock.lock();
        members=caveatConfig.get(listName);
        if (members == null) {
        }
 else {
          Map<String,List<String>> pivot=PivotUtil.getPivot(members);
          List<String> existingAuthorities=pivot.get(valueName);
          if (existingAuthorities != null) {
            for (            String authority : existingAuthorities) {
              List<String> vals=members.get(authority);
              vals.remove(valueName);
            }
            caveatConfig.put(listName,members);
          }
        }
        updateOrCreateCaveatConfig(convertToJSONString(caveatConfig));
      }
  finally {
        readLock.lock();
        writeLock.unlock();
      }
    }
  }
  finally {
    readLock.unlock();
  }
}","public void removeRMConstraintListValue(String listName,String valueName){
  Map<String,List<String>> members=null;
  try {
    readLock.lock();
    members=caveatConfig.get(listName);
    if (members != null) {
      try {
        readLock.unlock();
        writeLock.lock();
        members=caveatConfig.get(listName);
        if (members != null) {
          Map<String,List<String>> pivot=PivotUtil.getPivot(members);
          List<String> existingAuthorities=pivot.get(valueName);
          if (existingAuthorities != null) {
            for (            String authority : existingAuthorities) {
              List<String> vals=members.get(authority);
              vals.remove(valueName);
            }
            caveatConfig.put(listName,members);
          }
        }
        updateOrCreateCaveatConfig(convertToJSONString(caveatConfig));
      }
  finally {
        readLock.lock();
        writeLock.unlock();
      }
    }
  }
  finally {
    readLock.unlock();
  }
}",0.8458458458458459
89453,"/** 
 * Helper method to build a <b>NodeRef</b> path from the node to the RM root
 */
private void getNodeRefPathRecursive(NodeRef nodeRef,LinkedList<NodeRef> nodeRefPath){
  if (isFilePlanComponent(nodeRef) == false) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_INVALID_RM_NODE,ASPECT_FILE_PLAN_COMPONENT.toString()));
  }
  nodeRefPath.addFirst(nodeRef);
  if (isFilePlan(nodeRef) == true) {
  }
 else {
    ChildAssociationRef assocRef=nodeService.getPrimaryParent(nodeRef);
    if (assocRef == null) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NO_ROOT));
    }
    nodeRef=assocRef.getParentRef();
    getNodeRefPathRecursive(nodeRef,nodeRefPath);
  }
}","/** 
 * Helper method to build a <b>NodeRef</b> path from the node to the RM root
 */
private void getNodeRefPathRecursive(NodeRef nodeRef,LinkedList<NodeRef> nodeRefPath){
  if (isFilePlanComponent(nodeRef) == false) {
    throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_INVALID_RM_NODE,ASPECT_FILE_PLAN_COMPONENT.toString()));
  }
  nodeRefPath.addFirst(nodeRef);
  if (isFilePlan(nodeRef) == false) {
    ChildAssociationRef assocRef=nodeService.getPrimaryParent(nodeRef);
    if (assocRef == null) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_NO_ROOT));
    }
    nodeRef=assocRef.getParentRef();
    getNodeRefPathRecursive(nodeRef,nodeRefPath);
  }
}",0.9834413246940245
89454,"/** 
 * Updates the uniqueness check using the values provided.  If the after value is <tt>null</tt> then this is considered to be a removal.
 * @param nodeRef   node reference
 * @param beforeId  id before
 * @param afterId   id after
 */
private void updateUniqueness(NodeRef nodeRef,String beforeId,String afterId){
  NodeRef contextNodeRef=filePlanService.getFilePlan(nodeRef);
  if (beforeId == null) {
    if (afterId != null) {
      attributeService.createAttribute(null,CONTEXT_VALUE,contextNodeRef,afterId);
    }
  }
 else   if (afterId == null) {
    if (beforeId != null) {
      attributeService.removeAttribute(CONTEXT_VALUE,contextNodeRef,beforeId);
    }
    attributeService.removeAttributes(CONTEXT_VALUE,nodeRef);
  }
 else {
    attributeService.updateOrCreateAttribute(CONTEXT_VALUE,contextNodeRef,beforeId,CONTEXT_VALUE,contextNodeRef,afterId);
  }
}","/** 
 * Updates the uniqueness check using the values provided.  If the after value is <tt>null</tt> then this is considered to be a removal.
 * @param nodeRef   node reference
 * @param beforeId  id before
 * @param afterId   id after
 */
private void updateUniqueness(NodeRef nodeRef,String beforeId,String afterId){
  NodeRef contextNodeRef=filePlanService.getFilePlan(nodeRef);
  if (beforeId == null) {
    if (afterId != null) {
      attributeService.createAttribute(null,CONTEXT_VALUE,contextNodeRef,afterId);
    }
  }
 else   if (afterId == null) {
    attributeService.removeAttribute(CONTEXT_VALUE,contextNodeRef,beforeId);
    attributeService.removeAttributes(CONTEXT_VALUE,nodeRef);
  }
 else {
    attributeService.updateOrCreateAttribute(CONTEXT_VALUE,contextNodeRef,beforeId,CONTEXT_VALUE,contextNodeRef,afterId);
  }
}",0.9789473684210528
89455,"@Test(dependsOnMethods=""String_Node_Str"") public void editUnfiledRecordsFolderMetadata(){
  navigateToUnfiledRecords();
  FileDirectoryInfo folder=filePlanPage.getFileDirectoryInfo(RM_UNFILED_RECORDS_CONTAINER_NAME);
  WebElement actions=folder.findElement(By.cssSelector(""String_Node_Str""));
  drone.mouseOverOnElement(actions);
  WebElement editProperties=folder.findElement(By.cssSelector(""String_Node_Str""));
  editProperties.click();
  WebElement title=drone.find(INPUT_TITLE_SELECTOR);
  title.clear();
  title.sendKeys(""String_Node_Str"");
  WebElement description=drone.find(INPUT_DESCRIPTION_SELECTOR);
  description.clear();
  description.sendKeys(""String_Node_Str"");
  WebElement saveButton=drone.find(By.cssSelector(""String_Node_Str""));
  saveButton.click();
}","@Test(dependsOnMethods=""String_Node_Str"") public void editUnfiledRecordsFolderMetadata(){
  navigateToUnfiledRecords();
  FileDirectoryInfo folder=filePlanPage.getFileDirectoryInfo(RM_UNFILED_RECORDS_CONTAINER_NAME);
  WebElement actions=folder.findElement(By.cssSelector(""String_Node_Str""));
  drone.mouseOverOnElement(actions);
  WebElement editProperties=folder.findElement(By.cssSelector(""String_Node_Str""));
  editProperties.click();
  drone.waitForElement(INPUT_TITLE_SELECTOR,5);
  WebElement title=drone.find(INPUT_TITLE_SELECTOR);
  title.clear();
  title.sendKeys(""String_Node_Str"");
  WebElement description=drone.find(INPUT_DESCRIPTION_SELECTOR);
  description.clear();
  description.sendKeys(""String_Node_Str"");
  WebElement saveButton=drone.find(By.cssSelector(""String_Node_Str""));
  saveButton.click();
}",0.8540880503144654
89456,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.model.BaseTypeBehaviour#onCreateChildAssociation(org.alfresco.service.cmr.repository.ChildAssociationRef,boolean)
 */
@Behaviour(kind=BehaviourKind.ASSOCIATION,notificationFrequency=NotificationFrequency.TRANSACTION_COMMIT) public void onCreateChildAssociation(final ChildAssociationRef childAssocRef,boolean isNewNode){
  AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
    @Override public Void doWork() throws Exception {
      final NodeRef child=childAssocRef.getChildRef();
      if (nodeService.exists(child) == true) {
        QName childType=nodeService.getType(child);
        if (dictionaryService.isSubClass(childType,ContentModel.TYPE_FOLDER) == true) {
          if (dictionaryService.isSubClass(childType,ContentModel.TYPE_SYSTEM_FOLDER) == true) {
            nodeService.addAspect(child,ASPECT_FILE_PLAN_COMPONENT,null);
          }
 else {
            if (nodeService.hasAspect(child,ASPECT_FILE_PLAN_COMPONENT) == false) {
              nodeService.setType(child,TYPE_RECORD_FOLDER);
            }
            setIdenifierProperty(child);
          }
        }
 else {
          NodeRef parentRef=childAssocRef.getParentRef();
          QName parentType=nodeService.getType(parentRef);
          boolean isContentSubType=dictionaryService.isSubClass(childType,ContentModel.TYPE_CONTENT);
          boolean isUnfiledRecordContainerChild=parentType.equals(RecordsManagementModel.TYPE_UNFILED_RECORD_CONTAINER_CHILD);
          if (isContentSubType == true && isUnfiledRecordContainerChild == true) {
            if (nodeService.hasAspect(child,ASPECT_FILE_PLAN_COMPONENT) == false) {
              nodeService.addAspect(child,ASPECT_FILE_PLAN_COMPONENT,null);
            }
            if (nodeService.hasAspect(child,ASPECT_RECORD) == false) {
              recordService.makeRecord(child);
            }
          }
        }
      }
      return null;
    }
  }
);
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.model.BaseTypeBehaviour#onCreateChildAssociation(org.alfresco.service.cmr.repository.ChildAssociationRef,boolean)
 */
@Behaviour(kind=BehaviourKind.ASSOCIATION,notificationFrequency=NotificationFrequency.TRANSACTION_COMMIT) public void onCreateChildAssociation(final ChildAssociationRef childAssocRef,boolean isNewNode){
  AuthenticationUtil.runAsSystem(new RunAsWork<Void>(){
    @Override public Void doWork() throws Exception {
      final NodeRef child=childAssocRef.getChildRef();
      if (nodeService.exists(child) == true) {
        QName childType=nodeService.getType(child);
        if (dictionaryService.isSubClass(childType,ContentModel.TYPE_FOLDER) == true) {
          if (dictionaryService.isSubClass(childType,ContentModel.TYPE_SYSTEM_FOLDER) == true) {
            nodeService.addAspect(child,ASPECT_FILE_PLAN_COMPONENT,null);
          }
 else {
            if (nodeService.hasAspect(child,ASPECT_FILE_PLAN_COMPONENT) == false) {
              nodeService.setType(child,TYPE_RECORD_FOLDER);
            }
            setIdenifierProperty(child);
          }
        }
 else {
          NodeRef parentRef=childAssocRef.getParentRef();
          QName parentType=nodeService.getType(parentRef);
          boolean isContentSubType=dictionaryService.isSubClass(childType,ContentModel.TYPE_CONTENT);
          boolean isUnfiledRecordContainer=parentType.equals(RecordsManagementModel.TYPE_UNFILED_RECORD_CONTAINER);
          boolean isUnfiledRecordContainerChild=parentType.equals(RecordsManagementModel.TYPE_UNFILED_RECORD_CONTAINER_CHILD);
          if (isContentSubType && (isUnfiledRecordContainer || isUnfiledRecordContainerChild)) {
            if (nodeService.hasAspect(child,ASPECT_FILE_PLAN_COMPONENT) == false) {
              nodeService.addAspect(child,ASPECT_FILE_PLAN_COMPONENT,null);
            }
            if (nodeService.hasAspect(child,ASPECT_RECORD) == false) {
              recordService.makeRecord(child);
            }
          }
        }
      }
      return null;
    }
  }
);
}",0.9572776949826132
89457,"@Override protected boolean evaluateImpl(ActionCondition actionCondition,NodeRef actionedUponNodeRef){
  boolean result=false;
  String kind=((QName)actionCondition.getParameterValue(PARAM_KIND)).getLocalName();
  FilePlanComponentKind filePlanComponentKind=filePlanService.getFilePlanComponentKind(actionedUponNodeRef);
  if (filePlanComponentKind.toString().equals(kind)) {
    result=true;
  }
  return result;
}","@Override protected boolean evaluateImpl(ActionCondition actionCondition,NodeRef actionedUponNodeRef){
  boolean result=false;
  String kind=((QName)actionCondition.getParameterValue(PARAM_KIND)).getLocalName();
  FilePlanComponentKind filePlanComponentKind=filePlanService.getFilePlanComponentKind(actionedUponNodeRef);
  if (filePlanComponentKind != null && filePlanComponentKind.toString().equals(kind)) {
    result=true;
  }
  return result;
}",0.9617612977983776
89458,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.action.RMActionExecuterAbstractBase#isExecutableImpl(org.alfresco.service.cmr.repository.NodeRef,java.util.Map,boolean)
 */
@Override protected boolean isExecutableImpl(NodeRef filePlanComponent,Map<String,Serializable> parameters,boolean throwException){
  if (recordService.isRecord(filePlanComponent) == true) {
    if (recordService.isDeclared(filePlanComponent) == false) {
      List<String> missingProperties=new ArrayList<String>(10);
      if (mandatoryPropertiesSet(filePlanComponent,missingProperties) == true) {
        return true;
      }
 else {
        if (throwException) {
          throw new AlfrescoRuntimeException(buildMissingPropertiesErrorString(missingProperties));
        }
 else {
          return false;
        }
      }
    }
 else {
      return false;
    }
  }
 else {
    if (throwException) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_UNDECLARED_ONLY_RECORDS,filePlanComponent.toString()));
    }
 else {
      return false;
    }
  }
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.action.RMActionExecuterAbstractBase#isExecutableImpl(org.alfresco.service.cmr.repository.NodeRef,java.util.Map,boolean)
 */
@Override protected boolean isExecutableImpl(NodeRef filePlanComponent,Map<String,Serializable> parameters,boolean throwException){
  if (recordService.isRecord(filePlanComponent) == true) {
    if (recordService.isDeclared(filePlanComponent) == false) {
      List<String> missingProperties=new ArrayList<String>(10);
      if (mandatoryPropertiesSet(filePlanComponent,missingProperties) == true) {
        return true;
      }
    }
    return false;
  }
 else {
    if (throwException) {
      throw new AlfrescoRuntimeException(I18NUtil.getMessage(MSG_UNDECLARED_ONLY_RECORDS,filePlanComponent.toString()));
    }
 else {
      return false;
    }
  }
}",0.864406779661017
89459,"/** 
 * Log information about missing properties.
 * @param propDef             property definition
 * @param missingProperties   missing properties
 */
private void logMissingProperty(PropertyDefinition propDef,List<String> missingProperties){
  if (logger.isWarnEnabled()) {
    StringBuilder msg=new StringBuilder();
    msg.append(""String_Node_Str"").append(propDef.getName());
    logger.warn(msg.toString());
  }
  missingProperties.add(propDef.getName().toString());
}","/** 
 * Log information about missing properties.
 * @param propDef               property definition
 * @param missingProperties     missing properties
 */
private void logMissingProperty(PropertyDefinition propDef,List<String> missingProperties){
  if (logger.isWarnEnabled()) {
    StringBuilder msg=new StringBuilder();
    msg.append(""String_Node_Str"").append(propDef.getName());
    logger.warn(msg.toString());
  }
  missingProperties.add(propDef.getName().toString());
}",0.9957983193277312
89460,"/** 
 * Helper method to check whether all the mandatory properties of the node have been set
 * @param nodeRef    node reference
 * @return boolean   true if all mandatory properties are set, false otherwise
 */
private boolean mandatoryPropertiesSet(NodeRef nodeRef,List<String> missingProperties){
  boolean result=true;
  Map<QName,Serializable> nodeRefProps=this.nodeService.getProperties(nodeRef);
  QName nodeRefType=this.nodeService.getType(nodeRef);
  TypeDefinition typeDef=this.dictionaryService.getType(nodeRefType);
  for (  PropertyDefinition propDef : typeDef.getProperties().values()) {
    if (propDef.isMandatory() == true) {
      if (nodeRefProps.get(propDef.getName()) == null) {
        logMissingProperty(propDef,missingProperties);
        result=false;
        break;
      }
    }
  }
  if (result != false) {
    Set<QName> aspects=this.nodeService.getAspects(nodeRef);
    for (    QName aspect : aspects) {
      AspectDefinition aspectDef=this.dictionaryService.getAspect(aspect);
      for (      PropertyDefinition propDef : aspectDef.getProperties().values()) {
        if (propDef.isMandatory() == true) {
          if (nodeRefProps.get(propDef.getName()) == null) {
            logMissingProperty(propDef,missingProperties);
            result=false;
            break;
          }
        }
      }
    }
  }
  return result;
}","/** 
 * Helper method to check whether all the mandatory properties of the node have been set
 * @param nodeRef node reference
 * @return boolean true if all mandatory properties are set, false otherwise
 */
private boolean mandatoryPropertiesSet(NodeRef nodeRef,List<String> missingProperties){
  boolean result=true;
  Map<QName,Serializable> nodeRefProps=this.nodeService.getProperties(nodeRef);
  QName nodeRefType=this.nodeService.getType(nodeRef);
  TypeDefinition typeDef=this.dictionaryService.getType(nodeRefType);
  for (  PropertyDefinition propDef : typeDef.getProperties().values()) {
    if (propDef.isMandatory() == true) {
      if (nodeRefProps.get(propDef.getName()) == null) {
        logMissingProperty(propDef,missingProperties);
        result=false;
        break;
      }
    }
  }
  if (result != false) {
    Set<QName> aspects=this.nodeService.getAspects(nodeRef);
    for (    QName aspect : aspects) {
      AspectDefinition aspectDef=this.dictionaryService.getAspect(aspect);
      for (      PropertyDefinition propDef : aspectDef.getProperties().values()) {
        if (propDef.isMandatory() == true) {
          if (nodeRefProps.get(propDef.getName()) == null) {
            logMissingProperty(propDef,missingProperties);
            result=false;
            break;
          }
        }
      }
    }
  }
  return result;
}",0.9981624402793092
89461,"/** 
 * @see org.alfresco.module.org_alfresco_module_rm.dataset.DataSetService#getLoadedDataSets(org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public Map<String,DataSet> getLoadedDataSets(NodeRef filePlan){
  ParameterCheck.mandatory(""String_Node_Str"",filePlan);
  Map<String,DataSet> availableDataSets=new HashMap<String,DataSet>(getDataSets());
  Serializable dataSetIds=nodeService.getProperty(filePlan,PROP_LOADED_DATA_SET_IDS);
  if (dataSetIds != null) {
    @SuppressWarnings(""String_Node_Str"") ArrayList<String> loadedDataSetIds=(ArrayList<String>)dataSetIds;
    for (    Map.Entry<String,DataSet> entry : availableDataSets.entrySet()) {
      String key=entry.getKey();
      if (!loadedDataSetIds.contains(key)) {
        availableDataSets.remove(key);
      }
    }
    return availableDataSets;
  }
  return new HashMap<String,DataSet>();
}","/** 
 * @see org.alfresco.module.org_alfresco_module_rm.dataset.DataSetService#getLoadedDataSets(org.alfresco.service.cmr.repository.NodeRef)
 */
@Override public Map<String,DataSet> getLoadedDataSets(NodeRef filePlan){
  ParameterCheck.mandatory(""String_Node_Str"",filePlan);
  Map<String,DataSet> availableDataSets=new HashMap<String,DataSet>(getDataSets());
  Serializable dataSetIds=nodeService.getProperty(filePlan,PROP_LOADED_DATA_SET_IDS);
  if (dataSetIds != null) {
    @SuppressWarnings(""String_Node_Str"") ArrayList<String> loadedDataSetIds=(ArrayList<String>)dataSetIds;
    Iterator<Map.Entry<String,DataSet>> iterator=availableDataSets.entrySet().iterator();
    while (iterator.hasNext()) {
      Entry<String,DataSet> entry=iterator.next();
      String key=entry.getKey();
      if (!loadedDataSetIds.contains(key)) {
        iterator.remove();
      }
    }
    return availableDataSets;
  }
  return new HashMap<String,DataSet>();
}",0.7482093663911846
89462,"private static void doCleanSaveSettings(String string) throws Exception, SQLException {
  PreparedStatement pre=SQLUtils.getPreparedSQL(""String_Node_Str"");
  pre.setString(1,string);
  pre.execute();
}","private static void doCleanSaveSettings(String string) throws Exception {
  PreparedStatement pre=SQLUtils.getPreparedSQL(""String_Node_Str"");
  pre.setString(1,string);
  pre.execute();
}",0.963917525773196
89463,"private static void doSaveSettings(String string,List<Config> cfg) throws Exception {
  PreparedStatement pre=SQLUtils.getPreparedSQL(""String_Node_Str"");
  for (  Config c : cfg) {
    for (    ConfigTuple sel : c.getSelected()) {
      pre.setString(1,string);
      String value=sel.getObj().toString();
      if (sel.getObj() instanceof ScriptObjectMirror) {
        ScriptObjectMirror o=(ScriptObjectMirror)sel.getObj();
        value=o.callMember(""String_Node_Str"").toString();
      }
      pre.setString(2,value);
      pre.setString(3,sel.getDescription());
      pre.execute();
    }
  }
}","private static void doSaveSettings(String wpid,List<Config> cfg) throws Exception {
  PreparedStatement pre=SQLUtils.getPreparedSQL(""String_Node_Str"");
  for (  Config c : cfg) {
    for (    ConfigTuple sel : c.getSelected()) {
      pre.setString(1,wpid);
      pre.setString(2,c.getBeschreibung());
      pre.setString(3,sel.getDescription());
      pre.execute();
    }
  }
}",0.7185261003070624
89464,"@Override public void run(ProgressMonitor monitor) throws ApplicationException {
  try {
    float proWert=100.0f / wertpapiere.length;
    float babysteps=proWert / 5;
    BaseFetcher base;
    for (int i=0; i < wertpapiere.length; i++) {
      if (abort) {
        return;
      }
      GenericObjectSQL wertpapier=wertpapiere[i];
      String wpid=wertpapier.getAttribute(""String_Node_Str"").toString();
      String searchterm=null;
      if (!wertpapier.isEmpty(""String_Node_Str"")) {
        searchterm=wertpapier.getAttribute(""String_Node_Str"").toString();
      }
 else       if (!wertpapier.isEmpty(""String_Node_Str"")) {
        searchterm=wertpapier.getAttribute(""String_Node_Str"").toString();
      }
      String anbietername=getAnbieterName(wpid);
      monitor.setPercentComplete((int)(proWert * i));
      monitor.setStatusText(""String_Node_Str"" + searchterm);
      Logger.info(""String_Node_Str"" + searchterm);
      if (anbietername == null || forceNewSettings) {
        KursAktualisierenAnbieterAuswahlDialog dialog1=new KursAktualisierenAnbieterAuswahlDialog(KursAktualisierenDialog.POSITION_CENTER,wertpapier.getAttribute(""String_Node_Str"").toString());
        base=(BaseFetcher)dialog1.open();
        Boolean saveSettings=dialog1.getSpeichernSetting();
        if (saveSettings) {
          doSaveAnbieter(wertpapier.getAttribute(""String_Node_Str"").toString(),base.getName());
        }
        Date d=new Date();
        base.prepare(searchterm,2000,1,1,d.getYear() + 1900,d.getMonth() + 1,d.getDate());
        while (base.hasMoreConfig()) {
          if (abort) {
            return;
          }
          List<Config> cfg=base.getConfigs();
          monitor.setPercentComplete((int)(monitor.getPercentComplete() + babysteps));
          monitor.setStatusText(cfg.toString());
          KursAktualisierenDialog dialog=new KursAktualisierenDialog(KursAktualisierenDialog.POSITION_CENTER,cfg);
          dialog.open();
          base.process(cfg);
          if (saveSettings) {
            Logger.debug(""String_Node_Str"" + cfg);
            doSaveSettings(wertpapier.getAttribute(""String_Node_Str"").toString(),cfg);
          }
        }
      }
 else {
        Logger.debug(""String_Node_Str"" + anbietername);
        base=null;
        for (        BaseFetcher x : Factory.getHistoryFetcher()) {
          if (anbietername.equals(x.getName())) {
            base=x;
            break;
          }
        }
        if (base == null) {
          Logger.debug(""String_Node_Str"");
          doCleanSaveSettings(wpid);
          throw new ApplicationException(""String_Node_Str"");
        }
        Date d=new Date();
        base.prepare(searchterm,2000,1,1,d.getYear() + 1900,d.getMonth() + 1,d.getDate());
        PreparedStatement getCfg=SQLUtils.getPreparedSQL(""String_Node_Str"");
        getCfg.setString(1,wpid);
        while (base.hasMoreConfig()) {
          if (abort) {
            return;
          }
          List<Config> cfgs=base.getConfigs();
          monitor.setPercentComplete((int)(monitor.getPercentComplete() + babysteps));
          monitor.setStatusText(cfgs.toString());
          for (          Config cfg : cfgs) {
            getCfg.setString(2,cfg.getBeschreibung());
            String ret=(String)SQLUtils.getObject(getCfg);
            Logger.debug(""String_Node_Str"" + cfg.getBeschreibung() + ""String_Node_Str""+ ret);
            if (ret == null) {
              doCleanSaveSettings(wpid);
              Logger.debug(""String_Node_Str"");
              throw new ApplicationException(""String_Node_Str"");
            }
            ConfigTuple selected=null;
            for (            ConfigTuple opts : cfg.getOptions()) {
              if (ret.equals(opts.getDescription().toString())) {
                selected=opts;
                break;
              }
            }
            if (selected == null) {
              doCleanSaveSettings(wpid);
              Logger.debug(""String_Node_Str"");
              throw new ApplicationException(""String_Node_Str"");
            }
            cfg.addSelectedOptions(selected);
          }
          base.process(cfgs);
        }
      }
      monitor.setPercentComplete((int)(monitor.getPercentComplete() + babysteps));
      monitor.setStatusText(""String_Node_Str"");
      saveStockData(wertpapier,base);
      monitor.setStatusText(""String_Node_Str"" + searchterm);
      Application.getMessagingFactory().sendMessage(new KursUpdatesMsg(wpid));
    }
  }
 catch (  ApplicationException e) {
    monitor.setStatus(ProgressMonitor.STATUS_ERROR);
    monitor.setStatusText(e.getMessage());
    e.printStackTrace();
    throw e;
  }
catch (  Exception e) {
    monitor.setStatus(ProgressMonitor.STATUS_ERROR);
    monitor.setStatusText(e.getMessage());
    e.printStackTrace();
    throw new ApplicationException(""String_Node_Str"",e);
  }
  monitor.setStatusText(""String_Node_Str"");
  monitor.setStatus(ProgressMonitor.STATUS_DONE);
  monitor.setPercentComplete(101);
}","@Override public void run(ProgressMonitor monitor) throws ApplicationException {
  try {
    float proWert=100.0f / wertpapiere.length;
    float babysteps=proWert / 5;
    for (int i=0; i < wertpapiere.length; i++) {
      if (abort) {
        return;
      }
      GenericObjectSQL wertpapier=wertpapiere[i];
      String wpid=wertpapier.getAttribute(""String_Node_Str"").toString();
      String searchterm=getSearchTerm(wertpapier);
      String anbietername=getAnbieterName(wpid);
      monitor.setPercentComplete((int)(proWert * i));
      monitor.setStatusText(""String_Node_Str"" + searchterm);
      Logger.info(""String_Node_Str"" + searchterm);
      BaseFetcher base;
      boolean manualWay=(anbietername == null || forceNewSettings);
      if (manualWay) {
        base=updateStock(monitor,babysteps,wertpapier,searchterm);
      }
 else {
        base=updateStockAutomatic(anbietername,monitor,babysteps,wertpapier,searchterm,wpid);
      }
      monitor.setPercentComplete((int)(monitor.getPercentComplete() + babysteps));
      monitor.setStatusText(""String_Node_Str"");
      saveStockData(wertpapier,base);
      monitor.setStatusText(""String_Node_Str"" + searchterm);
      Application.getMessagingFactory().sendMessage(new KursUpdatesMsg(wpid));
    }
  }
 catch (  ApplicationException e) {
    monitor.setStatus(ProgressMonitor.STATUS_ERROR);
    monitor.setStatusText(e.getMessage());
    e.printStackTrace();
    throw e;
  }
catch (  Exception e) {
    monitor.setStatus(ProgressMonitor.STATUS_ERROR);
    monitor.setStatusText(e.getMessage());
    e.printStackTrace();
    throw new ApplicationException(""String_Node_Str"",e);
  }
  monitor.setStatusText(""String_Node_Str"");
  monitor.setStatus(ProgressMonitor.STATUS_DONE);
  monitor.setPercentComplete(101);
}",0.4006524317912218
89465,"private static void saveStockData(GenericObjectSQL wertpapier,BaseFetcher base) throws Exception, SQLException, RemoteException, ApplicationException {
  Connection conn=SQLUtils.getConnection();
  PreparedStatement del=conn.prepareStatement(""String_Node_Str"");
  del.setString(1,wertpapier.getID());
  del.executeUpdate();
  PreparedStatement insert=conn.prepareStatement(""String_Node_Str"");
  for (  Datacontainer dc : base.getHistQuotes()) {
    insert.setString(1,wertpapier.getID());
    insert.setBigDecimal(2,(BigDecimal)dc.data.get(""String_Node_Str""));
    insert.setString(3,(String)dc.data.get(""String_Node_Str""));
    insert.setDate(4,new java.sql.Date(((Date)dc.data.get(""String_Node_Str"")).getTime()));
    insert.addBatch();
  }
  insert.executeBatch();
  del=conn.prepareStatement(""String_Node_Str"");
  del.setString(1,wertpapier.getID());
  del.executeUpdate();
  if (base.getHistEvents() != null) {
    insert=conn.prepareStatement(""String_Node_Str"");
    for (    Datacontainer dc : base.getHistEvents()) {
      String action=(String)dc.data.get(""String_Node_Str"");
      if (action.equals(Const.CASHDIVIDEND)) {
        action=""String_Node_Str"";
      }
 else       if (action.equals(Const.STOCKDIVIDEND)) {
        action=""String_Node_Str"";
      }
 else       if (action.equals(Const.STOCKSPLIT)) {
        action=""String_Node_Str"";
      }
 else       if (action.equals(Const.SUBSCRIPTIONRIGHTS)) {
        action=""String_Node_Str"";
      }
 else {
        System.out.println(""String_Node_Str"" + action + ""String_Node_Str"");
        continue;
      }
      insert.setString(1,wertpapier.getID());
      insert.setString(2,(String)dc.data.get(""String_Node_Str""));
      insert.setBigDecimal(3,(BigDecimal)dc.data.get(""String_Node_Str""));
      insert.setString(4,action);
      insert.setDate(5,new java.sql.Date(((Date)dc.data.get(""String_Node_Str"")).getTime()));
      insert.setString(6,(String)dc.data.get(""String_Node_Str""));
      insert.addBatch();
    }
    insert.executeBatch();
  }
  calcPerformanceKurse(wertpapier,conn);
  Utils.markRecalc(null);
}","private static void saveStockData(GenericObjectSQL wertpapier,BaseFetcher base) throws Exception {
  Connection conn=SQLUtils.getConnection();
  PreparedStatement del=conn.prepareStatement(""String_Node_Str"");
  del.setString(1,wertpapier.getID());
  del.executeUpdate();
  PreparedStatement insert=conn.prepareStatement(""String_Node_Str"");
  for (  Datacontainer dc : base.getHistQuotes()) {
    insert.setString(1,wertpapier.getID());
    insert.setBigDecimal(2,(BigDecimal)dc.data.get(""String_Node_Str""));
    insert.setString(3,(String)dc.data.get(""String_Node_Str""));
    insert.setDate(4,new java.sql.Date(((Date)dc.data.get(""String_Node_Str"")).getTime()));
    insert.addBatch();
  }
  insert.executeBatch();
  del=conn.prepareStatement(""String_Node_Str"");
  del.setString(1,wertpapier.getID());
  del.executeUpdate();
  if (base.getHistEvents() != null) {
    insert=conn.prepareStatement(""String_Node_Str"");
    for (    Datacontainer dc : base.getHistEvents()) {
      String action=(String)dc.data.get(""String_Node_Str"");
      if (action.equals(Const.CASHDIVIDEND)) {
        action=""String_Node_Str"";
      }
 else       if (action.equals(Const.STOCKDIVIDEND)) {
        action=""String_Node_Str"";
      }
 else       if (action.equals(Const.STOCKSPLIT)) {
        action=""String_Node_Str"";
      }
 else       if (action.equals(Const.SUBSCRIPTIONRIGHTS)) {
        action=""String_Node_Str"";
      }
 else {
        System.out.println(""String_Node_Str"" + action + ""String_Node_Str"");
        continue;
      }
      insert.setString(1,wertpapier.getID());
      insert.setString(2,(String)dc.data.get(""String_Node_Str""));
      insert.setBigDecimal(3,(BigDecimal)dc.data.get(""String_Node_Str""));
      insert.setString(4,action);
      insert.setDate(5,new java.sql.Date(((Date)dc.data.get(""String_Node_Str"")).getTime()));
      insert.setString(6,(String)dc.data.get(""String_Node_Str""));
      insert.addBatch();
    }
    insert.executeBatch();
  }
  calcPerformanceKurse(wertpapier,conn);
  Utils.markRecalc(null);
}",0.987114028689521
89466,"@SuppressWarnings(""String_Node_Str"") public void runUmsaetze(Konto konto) throws ApplicationException {
  List<String> fehlerhafteOrder=new ArrayList<String>();
  String depotnummer=null;
  try {
    depotnummer=konto.getKontonummer();
  }
 catch (  RemoteException e2) {
    throw new ApplicationException(""String_Node_Str"",e2);
  }
  ArrayList<String> seiten=new ArrayList<String>();
  try {
    String username=konto.getMeta(PROP_KUNDENNUMMER,null);
    if (username == null || username.length() == 0) {
      throw new ApplicationException(""String_Node_Str"");
    }
    String password=konto.getMeta(PROP_PASSWORD,null);
    try {
      if (password == null || password.length() == 0) {
        password=Application.getCallback().askPassword(getName());
      }
    }
 catch (    Exception e1) {
      e1.printStackTrace();
      throw new ApplicationException(""String_Node_Str"" + e1.getMessage());
    }
    final WebClient webClient=new WebClient();
    String url=""String_Node_Str"";
    String request=""String_Node_Str"" + password + ""String_Node_Str""+ username+ ""String_Node_Str"";
    String json=getRequest(webClient,url,request);
    String sessionID=JsonPath.parse(json).read(""String_Node_Str"");
    String allordersRequest=""String_Node_Str"" + depotnummer + ""String_Node_Str""+ sessionID+ ""String_Node_Str"";
    json=getRequest(webClient,""String_Node_Str"",allordersRequest);
    List<Map<String,Object>> orders=JsonPath.parse(json).read(""String_Node_Str"",List.class);
    for (    Map<String,Object> orderinfo : orders) {
      String orderRequest=""String_Node_Str"" + depotnummer + ""String_Node_Str""+ orderinfo.get(""String_Node_Str"").toString()+ ""String_Node_Str""+ sessionID+ ""String_Node_Str"";
      String order=getRequest(webClient,""String_Node_Str"",orderRequest);
      Map<String,Object> detailInfo=JsonPath.parse(order).read(""String_Node_Str"",Map.class);
      CortalConsorsMitHBCIJSONWrapper wrapper=new CortalConsorsMitHBCIJSONWrapper(orderinfo,detailInfo);
      if (wrapper.addUmsatz(konto.getID())) {
        fehlerhafteOrder.add(wrapper.getAnnoymisierterBuchungstext());
      }
    }
    String logoutRequest=""String_Node_Str"" + sessionID + ""String_Node_Str"";
    json=getRequest(webClient,""String_Node_Str"",logoutRequest);
  }
 catch (  IOException e) {
    throw new ApplicationException(e);
  }
 finally {
    try {
      debug(seiten,konto);
    }
 catch (    RemoteException e) {
      throw new ApplicationException(e);
    }
  }
  try {
    if (fehlerhafteOrder.size() > 0) {
      DebugDialogWithTextarea dialog=new DebugDialogWithTextarea(DebugDialog.POSITION_CENTER,fehlerhafteOrder);
      dialog.open();
    }
  }
 catch (  OperationCanceledException oce) {
  }
catch (  Exception e) {
    Logger.error(""String_Node_Str"",e);
  }
}","@SuppressWarnings(""String_Node_Str"") public void runUmsaetze(Konto konto) throws ApplicationException {
  List<String> fehlerhafteOrder=new ArrayList<String>();
  String depotnummer=null;
  try {
    depotnummer=konto.getKontonummer();
  }
 catch (  RemoteException e2) {
    throw new ApplicationException(""String_Node_Str"",e2);
  }
  ArrayList<String> seiten=new ArrayList<String>();
  try {
    String username=konto.getMeta(PROP_KUNDENNUMMER,null);
    if (username == null || username.length() == 0) {
      throw new ApplicationException(""String_Node_Str"");
    }
    String password=konto.getMeta(PROP_PASSWORD,null);
    try {
      if (password == null || password.length() == 0) {
        password=Application.getCallback().askPassword(getName());
      }
    }
 catch (    Exception e1) {
      e1.printStackTrace();
      throw new ApplicationException(""String_Node_Str"" + e1.getMessage());
    }
    final WebClient webClient=new WebClient();
    String url=""String_Node_Str"";
    String request=""String_Node_Str"" + password + ""String_Node_Str""+ username+ ""String_Node_Str"";
    String json=getRequest(webClient,url,request);
    String sessionID=JsonPath.parse(json).read(""String_Node_Str"");
    String allordersRequest=""String_Node_Str"" + depotnummer + ""String_Node_Str""+ sessionID+ ""String_Node_Str"";
    json=getRequest(webClient,""String_Node_Str"",allordersRequest);
    List<Map<String,Object>> orders=JsonPath.parse(json).read(""String_Node_Str"",List.class);
    for (    Map<String,Object> orderinfo : orders) {
      String orderRequest=""String_Node_Str"" + depotnummer + ""String_Node_Str""+ orderinfo.get(""String_Node_Str"").toString()+ ""String_Node_Str""+ sessionID+ ""String_Node_Str"";
      String order=getRequest(webClient,""String_Node_Str"",orderRequest);
      Map<String,Object> detailInfo=JsonPath.parse(order).read(""String_Node_Str"",Map.class);
      CortalConsorsMitHBCIJSONWrapper wrapper=new CortalConsorsMitHBCIJSONWrapper(orderinfo,detailInfo);
      if (!wrapper.addUmsatz(konto.getID())) {
        fehlerhafteOrder.add(wrapper.getAnnoymisierterBuchungstext());
      }
    }
    String logoutRequest=""String_Node_Str"" + sessionID + ""String_Node_Str"";
    json=getRequest(webClient,""String_Node_Str"",logoutRequest);
  }
 catch (  IOException e) {
    throw new ApplicationException(e);
  }
 finally {
    try {
      debug(seiten,konto);
    }
 catch (    RemoteException e) {
      throw new ApplicationException(e);
    }
  }
  try {
    if (fehlerhafteOrder.size() > 0) {
      DebugDialogWithTextarea dialog=new DebugDialogWithTextarea(DebugDialog.POSITION_CENTER,fehlerhafteOrder);
      dialog.open();
    }
  }
 catch (  OperationCanceledException oce) {
  }
catch (  Exception e) {
    Logger.error(""String_Node_Str"",e);
  }
}",0.9998191354675348
89467,"public boolean check(){
  return orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"") && orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"") && orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"")&& orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"")&& orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"")&& orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"")&& orderinfo.get(""String_Node_Str"").equals(orderinfo.get(""String_Node_Str""))&& orderinfo.get(""String_Node_Str"").equals(orderinfo.get(""String_Node_Str""))&& orderinfo.get(""String_Node_Str"").equals(orderinfo.get(""String_Node_Str""))&& !orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"")&& (orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"") || orderinfo.get(""String_Node_Str"").equals(""String_Node_Str""));
}","public boolean check(){
  System.out.println(orderinfo.entrySet());
  return orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"") && orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"") && orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"")&& orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"")&& orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"")&& orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"")&& orderinfo.get(""String_Node_Str"").equals(orderinfo.get(""String_Node_Str""))&& orderinfo.get(""String_Node_Str"").equals(orderinfo.get(""String_Node_Str""))&& orderinfo.get(""String_Node_Str"").equals(orderinfo.get(""String_Node_Str""))&& !orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"")&& (orderinfo.get(""String_Node_Str"").equals(""String_Node_Str"") || orderinfo.get(""String_Node_Str"").equals(""String_Node_Str""));
}",0.973747016706444
89468,"@Override public void handleAction(Object context) throws ApplicationException {
  String kontoid;
  try {
    kontoid=askUserForKonto();
  }
 catch (  Exception e1) {
    e1.printStackTrace();
    Logger.error(""String_Node_Str"",e1);
    return;
  }
  ArrayList<FeldDefinitionen> fd=new ArrayList<FeldDefinitionen>();
  fd.add(new FeldDefinitionen(""String_Node_Str"",java.util.Date.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimal.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  List<GenericObjectHashMap> daten;
  try {
    CSVImportHelper csv=new CSVImportHelper(""String_Node_Str"" + kontoid);
    daten=csv.run(fd);
  }
 catch (  Exception e) {
    Logger.error(""String_Node_Str"",e);
    throw new ApplicationException(e);
  }
  if (daten == null) {
    return;
  }
  String fehlt=""String_Node_Str"";
  try {
    for (    GenericObjectHashMap x : daten) {
      for (      FeldDefinitionen f : fd) {
        Object value=x.getAttribute(f.getAttr());
        if (f.isRequired() && value.toString().isEmpty()) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
        if (value instanceof BigDecimalWithCurrency) {
          BigDecimalWithCurrency b=(BigDecimalWithCurrency)value;
          x.setAttribute(f.getAttr(),b.getZahl());
          x.setAttribute(f.getAttr() + ""String_Node_Str"",b.getWaehrung());
        }
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && !x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).divide((BigDecimal)x.getAttribute(""String_Node_Str""),5,RoundingMode.HALF_UP);
        x.setAttribute(""String_Node_Str"",d);
      }
      if (!x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).multiply((BigDecimal)x.getAttribute(""String_Node_Str""));
        x.setAttribute(""String_Node_Str"",d);
      }
      for (      FeldDefinitionen f : fd) {
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (x.getAttribute(f.getAttr()).toString().isEmpty()) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
      }
      if (!fehlt.isEmpty()) {
        Logger.error(""String_Node_Str"" + fehlt);
        throw new ApplicationException(""String_Node_Str"" + fehlt.substring(1));
      }
    }
    Konto konto=Utils.getKontoByID(kontoid);
    UmsatzeAusBestandsAenderung umsaetzeAusBestaenden=new UmsatzeAusBestandsAenderung(konto);
    Utils.clearBestand(konto);
    for (    GenericObjectHashMap x : daten) {
      Utils.addBestand(Utils.getORcreateWKN((String)x.getAttribute(""String_Node_Str""),(String)x.getAttribute(""String_Node_Str""),(String)x.getAttribute(""String_Node_Str"")),konto,((BigDecimal)x.getAttribute(""String_Node_Str"")).doubleValue(),((BigDecimal)x.getAttribute(""String_Node_Str"")).doubleValue(),((Currency)x.getAttribute(""String_Node_Str"")).getCurrencyCode(),((BigDecimal)x.getAttribute(""String_Node_Str"")).doubleValue(),((Currency)x.getAttribute(""String_Node_Str"")).getCurrencyCode(),(Date)x.getAttribute(""String_Node_Str""),(Date)x.getAttribute(""String_Node_Str""));
    }
    umsaetzeAusBestaenden.erzeugeUmsaetze();
  }
 catch (  RemoteException e) {
    throw new ApplicationException(e);
  }
  Application.getMessagingFactory().sendMessage(new StatusBarMessage(Application.getI18n().tr(""String_Node_Str""),StatusBarMessage.TYPE_INFO));
}","@Override public void handleAction(Object context) throws ApplicationException {
  String kontoid;
  try {
    kontoid=askUserForKonto();
  }
 catch (  Exception e1) {
    e1.printStackTrace();
    Logger.error(""String_Node_Str"",e1);
    return;
  }
  ArrayList<FeldDefinitionen> fd=new ArrayList<FeldDefinitionen>();
  fd.add(new FeldDefinitionen(""String_Node_Str"",java.util.Date.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimal.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  List<GenericObjectHashMap> daten;
  try {
    CSVImportHelper csv=new CSVImportHelper(""String_Node_Str"" + kontoid);
    daten=csv.run(fd);
  }
 catch (  Exception e) {
    Logger.error(""String_Node_Str"",e);
    throw new ApplicationException(e);
  }
  if (daten == null) {
    return;
  }
  String fehlt=""String_Node_Str"";
  try {
    for (    GenericObjectHashMap x : daten) {
      for (      FeldDefinitionen f : fd) {
        Object value=x.getAttribute(f.getAttr());
        if (f.isRequired() && value.toString().isEmpty()) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
        if (value instanceof BigDecimalWithCurrency) {
          BigDecimalWithCurrency b=(BigDecimalWithCurrency)value;
          x.setAttribute(f.getAttr(),b.getZahl());
          x.setAttribute(f.getAttr() + ""String_Node_Str"",b.getWaehrung());
        }
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && !x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).divide((BigDecimal)x.getAttribute(""String_Node_Str""),5,RoundingMode.HALF_UP);
        x.setAttribute(""String_Node_Str"",d);
      }
      if (!x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).multiply((BigDecimal)x.getAttribute(""String_Node_Str""));
        x.setAttribute(""String_Node_Str"",d);
      }
      for (      FeldDefinitionen f : fd) {
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (x.getAttribute(f.getAttr()).toString().isEmpty()) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
      }
      if (!fehlt.isEmpty()) {
        Logger.error(""String_Node_Str"" + fehlt);
        throw new ApplicationException(""String_Node_Str"" + fehlt.substring(1));
      }
    }
    Konto konto=Utils.getKontoByID(kontoid);
    UmsatzeAusBestandsAenderung umsaetzeAusBestaenden=new UmsatzeAusBestandsAenderung(konto);
    Utils.clearBestand(konto);
    for (    GenericObjectHashMap x : daten) {
      Object kursW=x.getAttribute(""String_Node_Str"");
      Object wertW=x.getAttribute(""String_Node_Str"");
      Utils.addBestand(Utils.getORcreateWKN((String)x.getAttribute(""String_Node_Str""),(String)x.getAttribute(""String_Node_Str""),(String)x.getAttribute(""String_Node_Str"")),konto,((BigDecimal)x.getAttribute(""String_Node_Str"")).doubleValue(),((BigDecimal)x.getAttribute(""String_Node_Str"")).doubleValue(),handleCurrency(kursW),((BigDecimal)x.getAttribute(""String_Node_Str"")).doubleValue(),handleCurrency(wertW),(Date)x.getAttribute(""String_Node_Str""),(Date)x.getAttribute(""String_Node_Str""));
    }
    umsaetzeAusBestaenden.erzeugeUmsaetze();
  }
 catch (  RemoteException e) {
    throw new ApplicationException(e);
  }
  Application.getMessagingFactory().sendMessage(new StatusBarMessage(Application.getI18n().tr(""String_Node_Str""),StatusBarMessage.TYPE_INFO));
}",0.9702751556537458
89469,"@Override public void handleAction(Object context) throws ApplicationException {
  String kontoid;
  try {
    kontoid=askUserForKonto();
  }
 catch (  Exception e1) {
    e1.printStackTrace();
    Logger.error(""String_Node_Str"",e1);
    return;
  }
  ArrayList<FeldDefinitionen> fd=new ArrayList<FeldDefinitionen>();
  fd.add(new FeldDefinitionen(""String_Node_Str"",java.util.Date.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",DepotAktion.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimal.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  List<GenericObjectHashMap> daten;
  try {
    CSVImportHelper csv=new CSVImportHelper(""String_Node_Str"" + kontoid);
    daten=csv.run(fd);
  }
 catch (  Exception e) {
    Logger.error(""String_Node_Str"",e);
    throw new ApplicationException(e);
  }
  if (daten == null) {
    return;
  }
  String fehlt=""String_Node_Str"";
  try {
    for (    GenericObjectHashMap x : daten) {
      for (      FeldDefinitionen f : fd) {
        Object value=x.getAttribute(f.getAttr());
        if (f.isRequired() && value.toString().isEmpty()) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
        if (value instanceof BigDecimalWithCurrency) {
          BigDecimalWithCurrency b=(BigDecimalWithCurrency)value;
          x.setAttribute(f.getAttr(),b.getZahl());
          x.setAttribute(f.getAttr() + ""String_Node_Str"",b.getWaehrung());
        }
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",BigDecimal.ZERO);
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",BigDecimal.ZERO);
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && !x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).divide((BigDecimal)x.getAttribute(""String_Node_Str""),5,RoundingMode.HALF_UP);
        x.setAttribute(""String_Node_Str"",d);
      }
      if (!x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).multiply((BigDecimal)x.getAttribute(""String_Node_Str""));
        x.setAttribute(""String_Node_Str"",d);
      }
      DepotAktion aktion=Utils.checkTransaktionsBezeichnung(x.getAttribute(""String_Node_Str"").toString().toUpperCase());
      if (aktion == null) {
        continue;
      }
      if (aktion.equals(DepotAktion.KAUF)) {
        x.setAttribute(""String_Node_Str"",((BigDecimal)x.getAttribute(""String_Node_Str"")).abs().negate());
      }
      if (aktion.equals(DepotAktion.VERKAUF)) {
        x.setAttribute(""String_Node_Str"",((BigDecimal)x.getAttribute(""String_Node_Str"")).abs());
      }
      for (      FeldDefinitionen f : fd) {
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (x.getAttribute(f.getAttr()).toString().isEmpty()) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
      }
      if (!fehlt.isEmpty()) {
        Logger.error(""String_Node_Str"" + fehlt);
        throw new ApplicationException(""String_Node_Str"" + fehlt.substring(1));
      }
    }
    for (    GenericObjectHashMap x : daten) {
      if (UmsatzHelper.existsOrder((String)x.getAttribute(""String_Node_Str""))) {
        Log.warn(""String_Node_Str"");
        continue;
      }
      if (x.getAttribute(""String_Node_Str"") == null) {
        Log.error(""String_Node_Str"");
      }
      Umsatz p=(Umsatz)Settings.getDBService().createObject(Umsatz.class,null);
      p.setKontoid(Integer.parseInt(kontoid));
      p.setAktion((DepotAktion)x.getAttribute(""String_Node_Str""));
      p.setBuchungsinformationen(""String_Node_Str"");
      p.setWPid(Utils.getORcreateWKN(x.getAttribute(""String_Node_Str"").toString(),x.getAttribute(""String_Node_Str"").toString(),x.getAttribute(""String_Node_Str"").toString()));
      p.setAnzahl((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setKurs((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setKurzW(x.getAttribute(""String_Node_Str"").toString());
      p.setKosten((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setKostenW(x.getAttribute(""String_Node_Str"").toString());
      p.setBuchungsdatum((Date)x.getAttribute(""String_Node_Str""));
      p.setKommentar(""String_Node_Str"");
      p.setSteuern((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setSteuernW(x.getAttribute(""String_Node_Str"").toString());
      p.setTransaktionsgebuehren((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setTransaktionsgebuehrenW(x.getAttribute(""String_Node_Str"").toString());
      String orderid=(String)x.getAttribute(""String_Node_Str"");
      if (orderid.isEmpty()) {
        orderid=p.generateOrderId();
      }
      p.setOrderid(orderid);
      p.store();
    }
  }
 catch (  RemoteException e) {
    throw new ApplicationException(e);
  }
  Application.getMessagingFactory().sendMessage(new StatusBarMessage(Application.getI18n().tr(""String_Node_Str""),StatusBarMessage.TYPE_INFO));
}","@Override public void handleAction(Object context) throws ApplicationException {
  String kontoid;
  try {
    kontoid=askUserForKonto();
  }
 catch (  Exception e1) {
    e1.printStackTrace();
    Logger.error(""String_Node_Str"",e1);
    return;
  }
  ArrayList<FeldDefinitionen> fd=new ArrayList<FeldDefinitionen>();
  fd.add(new FeldDefinitionen(""String_Node_Str"",java.util.Date.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",DepotAktion.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimal.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  List<GenericObjectHashMap> daten;
  try {
    CSVImportHelper csv=new CSVImportHelper(""String_Node_Str"" + kontoid);
    daten=csv.run(fd);
  }
 catch (  Exception e) {
    Logger.error(""String_Node_Str"",e);
    throw new ApplicationException(e);
  }
  if (daten == null) {
    return;
  }
  String fehlt=""String_Node_Str"";
  try {
    for (    GenericObjectHashMap x : daten) {
      for (      FeldDefinitionen f : fd) {
        Object value=x.getAttribute(f.getAttr());
        if (f.isRequired() && (value == null || value.toString().isEmpty())) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
        if (value instanceof BigDecimalWithCurrency) {
          BigDecimalWithCurrency b=(BigDecimalWithCurrency)value;
          x.setAttribute(f.getAttr(),b.getZahl());
          x.setAttribute(f.getAttr() + ""String_Node_Str"",b.getWaehrung());
        }
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",BigDecimal.ZERO);
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",BigDecimal.ZERO);
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && !x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).divide((BigDecimal)x.getAttribute(""String_Node_Str""),5,RoundingMode.HALF_UP);
        x.setAttribute(""String_Node_Str"",d);
      }
      if (!x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).multiply((BigDecimal)x.getAttribute(""String_Node_Str""));
        x.setAttribute(""String_Node_Str"",d);
      }
      DepotAktion aktion=Utils.checkTransaktionsBezeichnung(x.getAttribute(""String_Node_Str"").toString().toUpperCase());
      if (aktion == null) {
        continue;
      }
      if (aktion.equals(DepotAktion.KAUF)) {
        x.setAttribute(""String_Node_Str"",((BigDecimal)x.getAttribute(""String_Node_Str"")).abs().negate());
      }
      if (aktion.equals(DepotAktion.VERKAUF)) {
        x.setAttribute(""String_Node_Str"",((BigDecimal)x.getAttribute(""String_Node_Str"")).abs());
      }
      for (      FeldDefinitionen f : fd) {
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (x.getAttribute(f.getAttr()).toString().isEmpty()) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
      }
      if (!fehlt.isEmpty()) {
        Logger.error(""String_Node_Str"" + fehlt);
        throw new ApplicationException(""String_Node_Str"" + fehlt.substring(1));
      }
    }
    for (    GenericObjectHashMap x : daten) {
      if (UmsatzHelper.existsOrder((String)x.getAttribute(""String_Node_Str""))) {
        Log.warn(""String_Node_Str"");
        continue;
      }
      if (x.getAttribute(""String_Node_Str"") == null) {
        Log.error(""String_Node_Str"");
      }
      Umsatz p=(Umsatz)Settings.getDBService().createObject(Umsatz.class,null);
      p.setKontoid(Integer.parseInt(kontoid));
      p.setAktion((DepotAktion)x.getAttribute(""String_Node_Str""));
      p.setBuchungsinformationen(""String_Node_Str"");
      p.setWPid(Utils.getORcreateWKN(x.getAttribute(""String_Node_Str"").toString(),x.getAttribute(""String_Node_Str"").toString(),x.getAttribute(""String_Node_Str"").toString()));
      p.setAnzahl((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setKurs((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setKurzW(x.getAttribute(""String_Node_Str"").toString());
      p.setKosten((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setKostenW(x.getAttribute(""String_Node_Str"").toString());
      p.setBuchungsdatum((Date)x.getAttribute(""String_Node_Str""));
      p.setKommentar(""String_Node_Str"");
      p.setSteuern((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setSteuernW(x.getAttribute(""String_Node_Str"").toString());
      p.setTransaktionsgebuehren((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setTransaktionsgebuehrenW(x.getAttribute(""String_Node_Str"").toString());
      String orderid=(String)x.getAttribute(""String_Node_Str"");
      if (orderid.isEmpty()) {
        orderid=p.generateOrderId();
      }
      p.setOrderid(orderid);
      p.store();
    }
  }
 catch (  RemoteException e) {
    throw new ApplicationException(e);
  }
  Application.getMessagingFactory().sendMessage(new StatusBarMessage(Application.getI18n().tr(""String_Node_Str""),StatusBarMessage.TYPE_INFO));
}",0.998716476389921
89470,"@Override public void handleAction(Object context) throws ApplicationException {
  String kontoid;
  try {
    kontoid=askUserForKonto();
  }
 catch (  Exception e1) {
    e1.printStackTrace();
    Logger.error(""String_Node_Str"",e1);
    return;
  }
  ArrayList<FeldDefinitionen> fd=new ArrayList<FeldDefinitionen>();
  fd.add(new FeldDefinitionen(""String_Node_Str"",java.util.Date.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimal.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  List<GenericObjectHashMap> daten;
  try {
    CSVImportHelper csv=new CSVImportHelper(""String_Node_Str"" + kontoid);
    daten=csv.run(fd);
  }
 catch (  Exception e) {
    Logger.error(""String_Node_Str"",e);
    throw new ApplicationException(e);
  }
  if (daten == null) {
    return;
  }
  String fehlt=""String_Node_Str"";
  try {
    for (    GenericObjectHashMap x : daten) {
      for (      FeldDefinitionen f : fd) {
        Object value=x.getAttribute(f.getAttr());
        if (f.isRequired() && value.toString().isEmpty()) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
        if (value instanceof BigDecimalWithCurrency) {
          BigDecimalWithCurrency b=(BigDecimalWithCurrency)value;
          x.setAttribute(f.getAttr(),b.getZahl());
          x.setAttribute(f.getAttr() + ""String_Node_Str"",b.getWaehrung());
        }
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && !x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).divide((BigDecimal)x.getAttribute(""String_Node_Str""),5,RoundingMode.HALF_UP);
        x.setAttribute(""String_Node_Str"",d);
      }
      if (!x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).multiply((BigDecimal)x.getAttribute(""String_Node_Str""));
        x.setAttribute(""String_Node_Str"",d);
      }
      for (      FeldDefinitionen f : fd) {
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (x.getAttribute(f.getAttr()).toString().isEmpty()) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
      }
      if (!fehlt.isEmpty()) {
        Logger.error(""String_Node_Str"" + fehlt);
        throw new ApplicationException(""String_Node_Str"" + fehlt.substring(1));
      }
    }
    Konto konto=Utils.getKontoByID(kontoid);
    UmsatzeAusBestandsAenderung umsaetzeAusBestaenden=new UmsatzeAusBestandsAenderung(konto);
    Utils.clearBestand(konto);
    for (    GenericObjectHashMap x : daten) {
      Object kursW=x.getAttribute(""String_Node_Str"");
      Object wertW=x.getAttribute(""String_Node_Str"");
      Utils.addBestand(Utils.getORcreateWKN((String)x.getAttribute(""String_Node_Str""),(String)x.getAttribute(""String_Node_Str""),(String)x.getAttribute(""String_Node_Str"")),konto,((BigDecimal)x.getAttribute(""String_Node_Str"")).doubleValue(),((BigDecimal)x.getAttribute(""String_Node_Str"")).doubleValue(),handleCurrency(kursW),((BigDecimal)x.getAttribute(""String_Node_Str"")).doubleValue(),handleCurrency(wertW),(Date)x.getAttribute(""String_Node_Str""),(Date)x.getAttribute(""String_Node_Str""));
    }
    umsaetzeAusBestaenden.erzeugeUmsaetze();
  }
 catch (  RemoteException e) {
    throw new ApplicationException(e);
  }
  Application.getMessagingFactory().sendMessage(new StatusBarMessage(Application.getI18n().tr(""String_Node_Str""),StatusBarMessage.TYPE_INFO));
}","@Override public void handleAction(Object context) throws ApplicationException {
  String kontoid;
  try {
    kontoid=askUserForKonto();
  }
 catch (  Exception e1) {
    e1.printStackTrace();
    Logger.error(""String_Node_Str"",e1);
    return;
  }
  ArrayList<FeldDefinitionen> fd=new ArrayList<FeldDefinitionen>();
  fd.add(new FeldDefinitionen(""String_Node_Str"",java.util.Date.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimal.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  List<GenericObjectHashMap> daten;
  try {
    CSVImportHelper csv=new CSVImportHelper(""String_Node_Str"" + kontoid);
    daten=csv.run(fd);
  }
 catch (  Exception e) {
    Logger.error(""String_Node_Str"",e);
    throw new ApplicationException(e);
  }
  if (daten == null) {
    return;
  }
  String fehlt=""String_Node_Str"";
  try {
    for (    GenericObjectHashMap x : daten) {
      for (      FeldDefinitionen f : fd) {
        Object value=x.getAttribute(f.getAttr());
        if (f.isRequired() && value.toString().isEmpty()) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
        if (value instanceof BigDecimalWithCurrency) {
          BigDecimalWithCurrency b=(BigDecimalWithCurrency)value;
          x.setAttribute(f.getAttr(),b.getZahl());
          x.setAttribute(f.getAttr() + ""String_Node_Str"",b.getWaehrung());
        }
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && !x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).divide((BigDecimal)x.getAttribute(""String_Node_Str""),5,RoundingMode.HALF_UP);
        x.setAttribute(""String_Node_Str"",d);
      }
      if (!x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).multiply((BigDecimal)x.getAttribute(""String_Node_Str""));
        x.setAttribute(""String_Node_Str"",d);
      }
      for (      FeldDefinitionen f : fd) {
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (x.getAttribute(f.getAttr()).toString().isEmpty()) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
      }
      if (!fehlt.isEmpty()) {
        Logger.error(""String_Node_Str"" + fehlt);
        throw new ApplicationException(""String_Node_Str"" + fehlt.substring(1));
      }
    }
    Konto konto=Utils.getKontoByID(kontoid);
    UmsatzeAusBestandsAenderung umsaetzeAusBestaenden=new UmsatzeAusBestandsAenderung(konto);
    Utils.clearBestand(konto);
    for (    GenericObjectHashMap x : daten) {
      Object kursW=x.getAttribute(""String_Node_Str"");
      Object wertW=x.getAttribute(""String_Node_Str"");
      System.out.println(x);
      Utils.addBestand(Utils.getORcreateWKN((String)x.getAttribute(""String_Node_Str""),(String)x.getAttribute(""String_Node_Str""),(String)x.getAttribute(""String_Node_Str"")),konto,((BigDecimal)x.getAttribute(""String_Node_Str"")).doubleValue(),((BigDecimal)x.getAttribute(""String_Node_Str"")).doubleValue(),handleCurrency(kursW),((BigDecimal)x.getAttribute(""String_Node_Str"")).doubleValue(),handleCurrency(wertW),(Date)x.getAttribute(""String_Node_Str""),(Date)x.getAttribute(""String_Node_Str""));
    }
    umsaetzeAusBestaenden.erzeugeUmsaetze();
  }
 catch (  RemoteException e) {
    throw new ApplicationException(e);
  }
  Application.getMessagingFactory().sendMessage(new StatusBarMessage(Application.getI18n().tr(""String_Node_Str""),StatusBarMessage.TYPE_INFO));
}",0.9971031864948556
89471,"@Override public void handleAction(Object context) throws ApplicationException {
  String kontoid;
  try {
    kontoid=askUserForKonto();
  }
 catch (  Exception e1) {
    e1.printStackTrace();
    Logger.error(""String_Node_Str"",e1);
    return;
  }
  ArrayList<FeldDefinitionen> fd=new ArrayList<FeldDefinitionen>();
  fd.add(new FeldDefinitionen(""String_Node_Str"",java.util.Date.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",DepotAktion.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimal.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  List<GenericObjectHashMap> daten;
  try {
    CSVImportHelper csv=new CSVImportHelper(""String_Node_Str"" + kontoid);
    daten=csv.run(fd);
  }
 catch (  Exception e) {
    Logger.error(""String_Node_Str"",e);
    throw new ApplicationException(e);
  }
  if (daten == null) {
    return;
  }
  String fehlt=""String_Node_Str"";
  try {
    for (    GenericObjectHashMap x : daten) {
      for (      FeldDefinitionen f : fd) {
        Object value=x.getAttribute(f.getAttr());
        if (f.isRequired() && (value == null || value.toString().isEmpty())) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
        if (value instanceof BigDecimalWithCurrency) {
          BigDecimalWithCurrency b=(BigDecimalWithCurrency)value;
          x.setAttribute(f.getAttr(),b.getZahl());
          x.setAttribute(f.getAttr() + ""String_Node_Str"",b.getWaehrung());
        }
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",BigDecimal.ZERO);
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",BigDecimal.ZERO);
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && !x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).divide((BigDecimal)x.getAttribute(""String_Node_Str""),5,RoundingMode.HALF_UP);
        x.setAttribute(""String_Node_Str"",d);
      }
      if (!x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).multiply((BigDecimal)x.getAttribute(""String_Node_Str""));
        x.setAttribute(""String_Node_Str"",d);
      }
      DepotAktion aktion=Utils.checkTransaktionsBezeichnung(x.getAttribute(""String_Node_Str"").toString().toUpperCase());
      if (aktion == null) {
        continue;
      }
      if (aktion.equals(DepotAktion.KAUF)) {
        x.setAttribute(""String_Node_Str"",((BigDecimal)x.getAttribute(""String_Node_Str"")).abs().negate());
      }
      if (aktion.equals(DepotAktion.VERKAUF)) {
        x.setAttribute(""String_Node_Str"",((BigDecimal)x.getAttribute(""String_Node_Str"")).abs());
      }
      for (      FeldDefinitionen f : fd) {
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (x.getAttribute(f.getAttr()).toString().isEmpty()) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
      }
      if (!fehlt.isEmpty()) {
        Logger.error(""String_Node_Str"" + fehlt);
        throw new ApplicationException(""String_Node_Str"" + fehlt.substring(1));
      }
    }
    for (    GenericObjectHashMap x : daten) {
      if (UmsatzHelper.existsOrder((String)x.getAttribute(""String_Node_Str""))) {
        Log.warn(""String_Node_Str"");
        continue;
      }
      if (x.getAttribute(""String_Node_Str"") == null) {
        Log.error(""String_Node_Str"");
      }
      Umsatz p=(Umsatz)Settings.getDBService().createObject(Umsatz.class,null);
      p.setKontoid(Integer.parseInt(kontoid));
      p.setAktion((DepotAktion)x.getAttribute(""String_Node_Str""));
      p.setBuchungsinformationen(""String_Node_Str"");
      p.setWPid(Utils.getORcreateWKN(x.getAttribute(""String_Node_Str"").toString(),x.getAttribute(""String_Node_Str"").toString(),x.getAttribute(""String_Node_Str"").toString()));
      p.setAnzahl((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setKurs((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setKurzW(x.getAttribute(""String_Node_Str"").toString());
      p.setKosten((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setKostenW(x.getAttribute(""String_Node_Str"").toString());
      p.setBuchungsdatum((Date)x.getAttribute(""String_Node_Str""));
      p.setKommentar(""String_Node_Str"");
      p.setSteuern((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setSteuernW(x.getAttribute(""String_Node_Str"").toString());
      p.setTransaktionsgebuehren((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setTransaktionsgebuehrenW(x.getAttribute(""String_Node_Str"").toString());
      String orderid=(String)x.getAttribute(""String_Node_Str"");
      if (orderid.isEmpty()) {
        orderid=p.generateOrderId();
      }
      p.setOrderid(orderid);
      p.store();
    }
  }
 catch (  RemoteException e) {
    throw new ApplicationException(e);
  }
  Application.getMessagingFactory().sendMessage(new StatusBarMessage(Application.getI18n().tr(""String_Node_Str""),StatusBarMessage.TYPE_INFO));
}","@Override public void handleAction(Object context) throws ApplicationException {
  String kontoid;
  try {
    kontoid=askUserForKonto();
  }
 catch (  Exception e1) {
    e1.printStackTrace();
    Logger.error(""String_Node_Str"",e1);
    return;
  }
  ArrayList<FeldDefinitionen> fd=new ArrayList<FeldDefinitionen>();
  fd.add(new FeldDefinitionen(""String_Node_Str"",java.util.Date.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",DepotAktion.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimal.class,""String_Node_Str"",true));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",BigDecimalWithCurrency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",Currency.class,""String_Node_Str"",false));
  fd.add(new FeldDefinitionen(""String_Node_Str"",String.class,""String_Node_Str"",false));
  List<GenericObjectHashMap> daten;
  try {
    CSVImportHelper csv=new CSVImportHelper(""String_Node_Str"" + kontoid);
    daten=csv.run(fd);
  }
 catch (  Exception e) {
    Logger.error(""String_Node_Str"",e);
    throw new ApplicationException(e);
  }
  if (daten == null) {
    return;
  }
  String fehlt=""String_Node_Str"";
  try {
    for (    GenericObjectHashMap x : daten) {
      for (      FeldDefinitionen f : fd) {
        Object value=x.getAttribute(f.getAttr());
        if (f.isRequired() && (value == null || value.toString().isEmpty())) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
        if (value instanceof BigDecimalWithCurrency) {
          BigDecimalWithCurrency b=(BigDecimalWithCurrency)value;
          x.setAttribute(f.getAttr(),b.getZahl());
          x.setAttribute(f.getAttr() + ""String_Node_Str"",b.getWaehrung());
        }
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        fehlt+=""String_Node_Str"";
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",BigDecimal.ZERO);
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",BigDecimal.ZERO);
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        x.setAttribute(""String_Node_Str"",x.getAttribute(""String_Node_Str""));
      }
      if (((BigDecimal)x.getAttribute(""String_Node_Str"")).signum() == -1) {
        x.setAttribute(""String_Node_Str"",((BigDecimal)x.getAttribute(""String_Node_Str"")).abs());
      }
      if (x.getAttribute(""String_Node_Str"").toString().isEmpty() && !x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).divide((BigDecimal)x.getAttribute(""String_Node_Str""),5,RoundingMode.HALF_UP);
        x.setAttribute(""String_Node_Str"",d);
      }
      if (!x.getAttribute(""String_Node_Str"").toString().isEmpty() && x.getAttribute(""String_Node_Str"").toString().isEmpty()) {
        BigDecimal d=((BigDecimal)x.getAttribute(""String_Node_Str"")).multiply((BigDecimal)x.getAttribute(""String_Node_Str""));
        x.setAttribute(""String_Node_Str"",d);
      }
      DepotAktion aktion=Utils.checkTransaktionsBezeichnung(x.getAttribute(""String_Node_Str"").toString().toUpperCase());
      if (aktion.equals(DepotAktion.KAUF)) {
        x.setAttribute(""String_Node_Str"",((BigDecimal)x.getAttribute(""String_Node_Str"")).abs().negate());
      }
      if (aktion.equals(DepotAktion.VERKAUF)) {
        x.setAttribute(""String_Node_Str"",((BigDecimal)x.getAttribute(""String_Node_Str"")).abs());
      }
      for (      FeldDefinitionen f : fd) {
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (f.getAttr().equals(""String_Node_Str"")) {
          continue;
        }
        if (x.getAttribute(f.getAttr()).toString().isEmpty()) {
          fehlt+=""String_Node_Str"" + f.getBeschreibung();
        }
      }
      if (!fehlt.isEmpty()) {
        Logger.error(""String_Node_Str"" + fehlt);
        throw new ApplicationException(""String_Node_Str"" + fehlt.substring(1));
      }
    }
    for (    GenericObjectHashMap x : daten) {
      if (UmsatzHelper.existsOrder((String)x.getAttribute(""String_Node_Str""))) {
        Log.warn(""String_Node_Str"");
        continue;
      }
      if (x.getAttribute(""String_Node_Str"") == null) {
        Log.error(""String_Node_Str"");
      }
      Umsatz p=(Umsatz)Settings.getDBService().createObject(Umsatz.class,null);
      p.setKontoid(Integer.parseInt(kontoid));
      p.setAktion((DepotAktion)x.getAttribute(""String_Node_Str""));
      p.setBuchungsinformationen(""String_Node_Str"");
      p.setWPid(Utils.getORcreateWKN(x.getAttribute(""String_Node_Str"").toString(),x.getAttribute(""String_Node_Str"").toString(),x.getAttribute(""String_Node_Str"").toString()));
      p.setAnzahl((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setKurs((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setKurzW(x.getAttribute(""String_Node_Str"").toString());
      p.setKosten((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setKostenW(x.getAttribute(""String_Node_Str"").toString());
      p.setBuchungsdatum((Date)x.getAttribute(""String_Node_Str""));
      p.setKommentar(""String_Node_Str"");
      p.setSteuern((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setSteuernW(x.getAttribute(""String_Node_Str"").toString());
      p.setTransaktionsgebuehren((BigDecimal)x.getAttribute(""String_Node_Str""));
      p.setTransaktionsgebuehrenW(x.getAttribute(""String_Node_Str"").toString());
      String orderid=(String)x.getAttribute(""String_Node_Str"");
      if (orderid.isEmpty()) {
        orderid=p.generateOrderId();
      }
      p.setOrderid(orderid);
      p.store();
    }
  }
 catch (  RemoteException e) {
    throw new ApplicationException(e);
  }
  Application.getMessagingFactory().sendMessage(new StatusBarMessage(Application.getI18n().tr(""String_Node_Str""),StatusBarMessage.TYPE_INFO));
}",0.9842798849421366
89472,"private void reload() throws RemoteException {
  this.getError().setValue(""String_Node_Str"");
  boolean enable=true;
  list.clear();
  header.clear();
  Integer headerline=((Integer)getSkipLines().getValue());
  try {
    SWTUtil.disposeChildren(this.comp);
    comp.setLayoutData(new GridData(GridData.FILL_BOTH));
    this.comp.setLayout(new GridLayout());
    BufferedReader br=new BufferedReader(new InputStreamReader(new FileInputStream(file),Charset.forName((String)getCharset().getValue())));
    String line=null;
    int counter=0;
    while ((line=br.readLine()) != null && counter < 30) {
      GenericObjectHashMap g=new GenericObjectHashMap();
      g.setAttribute(""String_Node_Str"",line);
      list.add(g);
      counter++;
    }
    header.add(""String_Node_Str"");
    br.close();
    FileInputStream is=new FileInputStream(file);
    InputStreamReader isr=new InputStreamReader(is,Charset.forName((String)getCharset().getValue()));
    CSVFormat format=CSVFormat.RFC4180.withDelimiter(((String)getTrennzeichen().getValue()).charAt(0)).withSkipLines(headerline - 1);
    CSVParser parser=new CSVParser(isr,format);
    list.clear();
    boolean iserror=false;
    boolean isheader=true;
    header.clear();
    for (    CSVRecord record : parser) {
      if (isheader) {
        for (int i=0; i < record.size(); i++) {
          String name=record.get(i);
          if (name.isEmpty()) {
            name=""String_Node_Str"";
          }
          String neuername=name;
          counter=1;
          while (header.contains(neuername)) {
            neuername=name + ""String_Node_Str"" + counter+ ""String_Node_Str"";
            counter++;
          }
          header.add(neuername);
        }
        isheader=false;
        continue;
      }
      System.out.println(header);
      GenericObjectHashMap g=new GenericObjectHashMap();
      g.setAttribute(""String_Node_Str"",""String_Node_Str"");
      g.setAttribute(""String_Node_Str"",""String_Node_Str"" + (record.getRecordNumber() + headerline));
      for (int i=0; i < header.size(); i++) {
        if (i >= record.size()) {
          g.setAttribute(""String_Node_Str"",""String_Node_Str"");
          iserror=true;
          continue;
        }
        g.setAttribute(header.get(i),record.get(i));
      }
      list.add(g);
    }
    if (iserror) {
      getError().setValue(""String_Node_Str"");
    }
    parser.close();
  }
 catch (  Exception e) {
    Logger.error(""String_Node_Str"",e);
    this.getError().setValue(""String_Node_Str"" + e.getMessage());
    enable=false;
  }
  TablePart tab=new TablePart(list,null);
  tab.addColumn(""String_Node_Str"" + headerline,""String_Node_Str"");
  tab.addColumn(""String_Node_Str"",""String_Node_Str"");
  for (  String h : header) {
    tab.addColumn(h,h);
  }
  tab.paint(comp);
  comp.layout(true);
  if (weiterbutton != null) {
    weiterbutton.setEnabled(enable);
  }
}","private void reload() throws RemoteException {
  this.getError().setValue(""String_Node_Str"");
  boolean enable=true;
  list.clear();
  header.clear();
  Integer headerline=((Integer)getSkipLines().getValue());
  try {
    SWTUtil.disposeChildren(this.comp);
    comp.setLayoutData(new GridData(GridData.FILL_BOTH));
    this.comp.setLayout(new GridLayout());
    BufferedReader br=new BufferedReader(new InputStreamReader(new FileInputStream(file),Charset.forName((String)getCharset().getValue())));
    String line=null;
    int counter=0;
    while ((line=br.readLine()) != null && counter < 30) {
      GenericObjectHashMap g=new GenericObjectHashMap();
      g.setAttribute(""String_Node_Str"",line);
      list.add(g);
      counter++;
    }
    header.add(""String_Node_Str"");
    br.close();
    FileInputStream is=new FileInputStream(file);
    InputStreamReader isr=new InputStreamReader(is,Charset.forName((String)getCharset().getValue()));
    CSVFormat format=CSVFormat.RFC4180.withDelimiter(((String)getTrennzeichen().getValue()).charAt(0)).withSkipLines(headerline - 1);
    CSVParser parser=new CSVParser(isr,format);
    list.clear();
    boolean iserror=false;
    boolean isheader=true;
    header.clear();
    for (    CSVRecord record : parser) {
      if (isheader) {
        for (int i=0; i < record.size(); i++) {
          String name=record.get(i);
          if (name.isEmpty()) {
            name=""String_Node_Str"";
          }
          String neuername=name;
          counter=1;
          while (header.contains(neuername)) {
            neuername=name + ""String_Node_Str"" + counter+ ""String_Node_Str"";
            counter++;
          }
          header.add(neuername);
        }
        isheader=false;
        continue;
      }
      GenericObjectHashMap g=new GenericObjectHashMap();
      g.setAttribute(""String_Node_Str"",""String_Node_Str"");
      g.setAttribute(""String_Node_Str"",""String_Node_Str"" + (record.getRecordNumber() + headerline));
      for (int i=0; i < header.size(); i++) {
        if (i >= record.size()) {
          g.setAttribute(""String_Node_Str"",""String_Node_Str"");
          iserror=true;
          continue;
        }
        g.setAttribute(header.get(i),record.get(i));
      }
      list.add(g);
    }
    if (iserror) {
      getError().setValue(""String_Node_Str"");
    }
    parser.close();
  }
 catch (  Exception e) {
    Logger.error(""String_Node_Str"",e);
    this.getError().setValue(""String_Node_Str"" + e.getMessage());
    enable=false;
  }
  TablePart tab=new TablePart(list,null);
  tab.addColumn(""String_Node_Str"" + headerline,""String_Node_Str"");
  tab.addColumn(""String_Node_Str"",""String_Node_Str"");
  for (  String h : header) {
    tab.addColumn(h,h);
  }
  tab.paint(comp);
  comp.layout(true);
  if (weiterbutton != null) {
    weiterbutton.setEnabled(enable);
  }
}",0.9940434477925718
89473,"private void transformiereDaten(List<GenericObjectHashMap> tablist) throws RemoteException {
  tablist.clear();
  int fehler=0;
  for (  GenericObjectHashMap source : quellDaten) {
    if (!source.getAttribute(""String_Node_Str"").toString().isEmpty()) {
      continue;
    }
    GenericObjectHashMap g=new GenericObjectHashMap();
    for (    Entry<FeldDefinitionen,AbstractInput> entry : controls.entrySet()) {
      boolean required=entry.getValue().isMandatory();
      String sourceattr=(String)entry.getValue().getValue();
      String destattr=entry.getKey().getAttr();
      if (sourceattr.isEmpty()) {
        if (required) {
          g.setAttribute(destattr,""String_Node_Str"");
          fehler++;
        }
 else {
          g.setAttribute(destattr,""String_Node_Str"");
        }
        continue;
      }
      Object sourcedata=source.getAttribute(sourceattr);
      Class<?> feldtype=entry.getKey().getFeldtype();
      try {
        if (feldtype.equals(BigDecimal.class)) {
          FeldConverter fc=(FeldConverter)zahlenFormat.getValue();
          sourcedata=fc.convert(sourcedata.toString());
        }
 else         if (feldtype.equals(BigDecimalWithCurrency.class)) {
          FeldConverter fc=(FeldConverter)zahlenFormat.getValue();
          String data=sourcedata.toString().trim();
          int spaceCount=StringUtils.countMatches(data,""String_Node_Str"");
          if (spaceCount == 0) {
            sourcedata=fc.convert(data);
          }
 else           if (spaceCount == 1) {
            String[] splitted=data.split(""String_Node_Str"");
            BigDecimalWithCurrency bdwc=new BigDecimalWithCurrency((BigDecimal)fc.convert(splitted[0]),splitted[1]);
            sourcedata=bdwc;
          }
 else {
            sourcedata=fc.convert(data);
          }
        }
 else         if (feldtype.equals(Date.class)) {
          SimpleDateFormat dp=getDateParser(datumsFormat.getValue().toString());
          sourcedata=dp.parse(sourcedata.toString());
        }
 else         if (feldtype.equals(DepotAktion.class)) {
          sourcedata=DepotAktion.getByString(sourcedata.toString());
        }
 else         if (feldtype.equals(Currency.class)) {
          sourcedata=Currency.getInstance(sourcedata.toString());
        }
 else         if (feldtype.equals(String.class)) {
        }
 else {
          sourcedata=""String_Node_Str"";
        }
      }
 catch (      ParseException|IllegalArgumentException e) {
        fehler++;
        sourcedata=""String_Node_Str"" + sourcedata.toString();
      }
      g.setAttribute(destattr,sourcedata);
    }
    g.setAttribute(""String_Node_Str"",waehrung.getValue().toString());
    tablist.add(g);
  }
  if (fehler == 0) {
    error.setValue(""String_Node_Str"");
  }
 else {
    error.setValue(""String_Node_Str"" + fehler);
  }
  weiterbutton.setEnabled(fehler == 0);
}","private void transformiereDaten(List<GenericObjectHashMap> tablist) throws RemoteException {
  tablist.clear();
  int fehler=0;
  for (  GenericObjectHashMap source : quellDaten) {
    if (!source.getAttribute(""String_Node_Str"").toString().isEmpty()) {
      continue;
    }
    GenericObjectHashMap g=new GenericObjectHashMap();
    for (    Entry<FeldDefinitionen,AbstractInput> entry : controls.entrySet()) {
      boolean required=entry.getValue().isMandatory();
      String sourceattr=(String)entry.getValue().getValue();
      String destattr=entry.getKey().getAttr();
      if (sourceattr.isEmpty()) {
        if (required) {
          g.setAttribute(destattr,""String_Node_Str"");
          fehler++;
        }
 else {
          g.setAttribute(destattr,""String_Node_Str"");
        }
        continue;
      }
      Object sourcedata=source.getAttribute(sourceattr);
      Class<?> feldtype=entry.getKey().getFeldtype();
      try {
        if (feldtype.equals(BigDecimal.class)) {
          FeldConverter fc=(FeldConverter)zahlenFormat.getValue();
          sourcedata=fc.convert(sourcedata.toString());
        }
 else         if (feldtype.equals(BigDecimalWithCurrency.class)) {
          FeldConverter fc=(FeldConverter)zahlenFormat.getValue();
          String data=sourcedata.toString().trim();
          int spaceCount=StringUtils.countMatches(data,""String_Node_Str"");
          if (spaceCount == 0) {
            sourcedata=fc.convert(data);
          }
 else           if (spaceCount == 1) {
            String[] splitted=data.split(""String_Node_Str"");
            BigDecimalWithCurrency bdwc=new BigDecimalWithCurrency((BigDecimal)fc.convert(splitted[0]),splitted[1]);
            sourcedata=bdwc;
          }
 else {
            sourcedata=fc.convert(data);
          }
        }
 else         if (feldtype.equals(Date.class)) {
          SimpleDateFormat dp=getDateParser(datumsFormat.getValue().toString());
          sourcedata=dp.parse(sourcedata.toString());
        }
 else         if (feldtype.equals(DepotAktion.class)) {
          DepotAktion da=DepotAktion.getByString(sourcedata.toString());
          if (da == null) {
            throw new IllegalArgumentException();
          }
          sourcedata=da;
        }
 else         if (feldtype.equals(Currency.class)) {
          sourcedata=Currency.getInstance(sourcedata.toString());
        }
 else         if (feldtype.equals(String.class)) {
        }
 else {
          sourcedata=""String_Node_Str"";
        }
      }
 catch (      ParseException|IllegalArgumentException e) {
        fehler++;
        sourcedata=""String_Node_Str"" + sourcedata.toString();
      }
      g.setAttribute(destattr,sourcedata);
    }
    g.setAttribute(""String_Node_Str"",waehrung.getValue().toString());
    tablist.add(g);
  }
  if (fehler == 0) {
    error.setValue(""String_Node_Str"");
  }
 else {
    error.setValue(""String_Node_Str"" + fehler);
  }
  weiterbutton.setEnabled(fehler == 0);
}",0.9756602796478508
89474,"private void tohash(HashMap<String,String> infos,HtmlTable tab,boolean b){
  if (!b) {
    for (    HtmlTableRow row : tab.getRows()) {
      List<HtmlTableCell> cells=row.getCells();
      if (cells.size() == 1 && row.asText().toLowerCase().contains(""String_Node_Str"")) {
        continue;
      }
      if (cells.size() != 2) {
        System.out.println(""String_Node_Str"" + cells.size() + ""String_Node_Str""+ row.asText());
        System.out.println(cells.get(0).toString().toLowerCase());
        continue;
      }
      infos.put(cells.get(0).asText().toLowerCase(),cells.get(1).asText());
    }
    return;
  }
  List<HtmlTableRow> rows=tab.getRows();
  if (rows.size() < 2) {
    System.out.println(""String_Node_Str"" + rows.toString());
    return;
  }
  List<HtmlTableCell> r1=rows.get(0).getCells();
  for (int zeile=1; zeile < rows.size(); zeile++) {
    List<HtmlTableCell> r2=rows.get(zeile).getCells();
    if (r1.size() != r2.size()) {
      System.out.println(""String_Node_Str"" + r1.toString());
      continue;
    }
    int missing=0;
    for (int i=0; i < r1.size(); i++) {
      String header=r1.get(i).asText().toLowerCase();
      if (""String_Node_Str"".equals(header)) {
        header=""String_Node_Str"" + missing;
        missing++;
      }
      infos.put(header,r2.get(i).asText());
    }
  }
}","private void tohash(HashMap<String,String> infos,HtmlTable tab,boolean b){
  if (!b) {
    for (    HtmlTableRow row : tab.getRows()) {
      List<HtmlTableCell> cells=row.getCells();
      if (cells.size() == 1 && row.asText().toLowerCase().contains(""String_Node_Str"")) {
        continue;
      }
      if (cells.size() != 2) {
        Logger.info(""String_Node_Str"" + cells.size() + ""String_Node_Str""+ row.asText());
        continue;
      }
      infos.put(cells.get(0).asText().toLowerCase(),cells.get(1).asText());
    }
    return;
  }
  List<HtmlTableRow> rows=tab.getRows();
  if (rows.size() < 2) {
    System.out.println(""String_Node_Str"" + rows.toString());
    return;
  }
  List<HtmlTableCell> r1=rows.get(0).getCells();
  for (int zeile=1; zeile < rows.size(); zeile++) {
    List<HtmlTableCell> r2=rows.get(zeile).getCells();
    if (r1.size() != r2.size()) {
      Logger.info(""String_Node_Str"" + r1.toString());
      continue;
    }
    int missing=0;
    for (int i=0; i < r1.size(); i++) {
      String header=r1.get(i).asText().toLowerCase();
      if (""String_Node_Str"".equals(header)) {
        header=""String_Node_Str"" + missing;
        missing++;
      }
      infos.put(header,r2.get(i).asText());
    }
  }
}",0.9510763209393346
89475,"public void run(Konto konto) throws ApplicationException {
  ArrayList<String> seiten=new ArrayList<String>();
  try {
    String username=konto.getKundennummer();
    String password=konto.getMeta(PROP_PASSWORD,null);
    if (username == null || username.length() == 0) {
      throw new ApplicationException(i18n.tr(""String_Node_Str""));
    }
    try {
      if (password == null || password.length() == 0) {
        password=Application.getCallback().askPassword(getName());
      }
    }
 catch (    Exception e1) {
      e1.printStackTrace();
      throw new ApplicationException(""String_Node_Str"" + e1.getMessage());
    }
    final WebClient webClient=new WebClient(BrowserVersion.INTERNET_EXPLORER_8);
    webClient.setCssErrorHandler(new SilentCssErrorHandler());
    webClient.setRefreshHandler(new ThreadedRefreshHandler());
    webClient.getOptions().setJavaScriptEnabled(false);
    HtmlPage page=webClient.getPage(""String_Node_Str"");
    seiten.add(page.asXml());
    List<HtmlForm> forms=(List<HtmlForm>)page.getByXPath(""String_Node_Str"");
    if (forms.size() != 1) {
      throw new ApplicationException(""String_Node_Str"");
    }
    HtmlForm form=forms.get(0);
    form.getInputByName(""String_Node_Str"").setValueAttribute(username);
    form.getInputByName(""String_Node_Str"").setValueAttribute(password);
    HtmlInput input=form.getInputByName(""String_Node_Str"");
    page=input.click();
    seiten.add(page.asXml());
    try {
      page=page.getAnchorByText(""String_Node_Str"").click();
    }
 catch (    com.gargoylesoftware.htmlunit.ElementNotFoundException e) {
      return;
    }
    HashMap<String,String> infos=new HashMap<String,String>();
    DateFormat df=new SimpleDateFormat(""String_Node_Str"");
    for (    HtmlAnchor x : page.getAnchors()) {
      if (x.getAttribute(""String_Node_Str"").contains(""String_Node_Str"")) {
        infos.clear();
        page=x.click();
        seiten.add(page.asXml());
        DomNodeList<DomElement> list=page.getElementsByTagName(""String_Node_Str"");
        if (list.size() != 3) {
        }
        tohash(infos,(HtmlTable)list.get(2),false);
        tohash(infos,(HtmlTable)list.get(3),true);
        String[] kurs=((String)infos.get(""String_Node_Str"")).replaceAll(""String_Node_Str"",""String_Node_Str"").split(""String_Node_Str"");
        Date d;
        try {
          d=df.parse(infos.get(""String_Node_Str"").substring(0,10));
        }
 catch (        ParseException e) {
          throw new ApplicationException(""String_Node_Str"" + infos.get(""String_Node_Str""));
        }
        Utils.addUmsatz(infos.get(""String_Node_Str""),infos.get(""String_Node_Str""),infos.get(""String_Node_Str""),infos.toString(),Utils.getDoubleFromZahl(infos.get(""String_Node_Str"")),Utils.getDoubleFromZahl(kurs[0]),kurs[1],Math.rint(Utils.getDoubleFromZahl(kurs[0]) * Utils.getDoubleFromZahl(infos.get(""String_Node_Str"")) * 100 / 100),kurs[1],d,infos.get(""String_Node_Str""));
      }
    }
    infos=null;
    try {
      page=page.getAnchorByText(""String_Node_Str"").click();
      seiten.add(page.asXml());
    }
 catch (    com.gargoylesoftware.htmlunit.ElementNotFoundException e) {
      return;
    }
    List<HtmlTable> tabs=(List<HtmlTable>)page.getByXPath(""String_Node_Str"");
    if (tabs.size() == 0) {
      Utils.report((List<? extends HtmlElement>)page.getByXPath(""String_Node_Str""));
      throw new ApplicationException(""String_Node_Str"");
    }
    ArrayList<HashMap<String,String>> liste=tableRowsToHashs(tabs.get(0));
    Utils.clearBestand(konto);
    double bestandswert=0;
    for (    HashMap<String,String> i : liste) {
      String[] bk=i.get(""String_Node_Str"").split(""String_Node_Str"");
      Utils.addBestand(konto,Utils.getDoubleFromZahl(i.get(""String_Node_Str"")),i.get(""String_Node_Str""),Utils.getDoubleFromZahl(bk[0]),bk[1],Utils.getDoubleFromZahl(i.get(""String_Node_Str"")) * Utils.getDoubleFromZahl(bk[0]),bk[1],new Date());
      bestandswert+=Utils.getDoubleFromZahl(i.get(""String_Node_Str"")) * Utils.getDoubleFromZahl(bk[0]);
    }
    konto.setSaldo(bestandswert);
    konto.store();
  }
 catch (  IOException e) {
    throw new ApplicationException(e);
  }
 finally {
    try {
      debug(seiten,konto);
    }
 catch (    RemoteException e) {
      throw new ApplicationException(e);
    }
  }
}","public void run(Konto konto) throws ApplicationException {
  ArrayList<String> seiten=new ArrayList<String>();
  try {
    String username=konto.getKundennummer();
    String password=konto.getMeta(PROP_PASSWORD,null);
    if (username == null || username.length() == 0) {
      throw new ApplicationException(i18n.tr(""String_Node_Str""));
    }
    try {
      if (password == null || password.length() == 0) {
        password=Application.getCallback().askPassword(getName());
      }
    }
 catch (    Exception e1) {
      e1.printStackTrace();
      throw new ApplicationException(""String_Node_Str"" + e1.getMessage());
    }
    final WebClient webClient=new WebClient(BrowserVersion.INTERNET_EXPLORER_8);
    webClient.setCssErrorHandler(new SilentCssErrorHandler());
    webClient.setRefreshHandler(new ThreadedRefreshHandler());
    webClient.getOptions().setJavaScriptEnabled(false);
    HtmlPage page=webClient.getPage(""String_Node_Str"");
    seiten.add(page.asXml());
    List<HtmlForm> forms=(List<HtmlForm>)page.getByXPath(""String_Node_Str"");
    if (forms.size() != 1) {
      Utils.report((List<? extends HtmlElement>)page.getByXPath(""String_Node_Str""));
      throw new ApplicationException(""String_Node_Str"");
    }
    HtmlForm form=forms.get(0);
    form.getInputByName(""String_Node_Str"").setValueAttribute(username);
    form.getInputByName(""String_Node_Str"").setValueAttribute(password);
    HtmlInput input=form.getInputByName(""String_Node_Str"");
    page=input.click();
    seiten.add(page.asXml());
    try {
      page=page.getAnchorByText(""String_Node_Str"").click();
    }
 catch (    com.gargoylesoftware.htmlunit.ElementNotFoundException e) {
      Utils.report(page.getAnchors());
      throw new ApplicationException(""String_Node_Str"");
    }
    HashMap<String,String> infos=new HashMap<String,String>();
    DateFormat df=new SimpleDateFormat(""String_Node_Str"");
    for (    HtmlAnchor x : page.getAnchors()) {
      if (x.getAttribute(""String_Node_Str"").contains(""String_Node_Str"")) {
        infos.clear();
        page=x.click();
        seiten.add(page.asXml());
        DomNodeList<DomElement> list=page.getElementsByTagName(""String_Node_Str"");
        if (list.size() != 3) {
        }
        tohash(infos,(HtmlTable)list.get(2),false);
        tohash(infos,(HtmlTable)list.get(3),true);
        String[] kurs=((String)infos.get(""String_Node_Str"")).replaceAll(""String_Node_Str"",""String_Node_Str"").split(""String_Node_Str"");
        Date d;
        try {
          d=df.parse(infos.get(""String_Node_Str"").substring(0,10));
        }
 catch (        ParseException e) {
          throw new ApplicationException(""String_Node_Str"" + infos.get(""String_Node_Str""));
        }
        Utils.addUmsatz(infos.get(""String_Node_Str""),infos.get(""String_Node_Str""),infos.get(""String_Node_Str""),infos.toString(),Utils.getDoubleFromZahl(infos.get(""String_Node_Str"")),Utils.getDoubleFromZahl(kurs[0]),kurs[1],Math.rint(Utils.getDoubleFromZahl(kurs[0]) * Utils.getDoubleFromZahl(infos.get(""String_Node_Str"")) * 100 / 100),kurs[1],d,infos.get(""String_Node_Str""));
      }
    }
    infos=null;
    try {
      page=page.getAnchorByText(""String_Node_Str"").click();
      seiten.add(page.asXml());
    }
 catch (    com.gargoylesoftware.htmlunit.ElementNotFoundException e) {
      Utils.report(page.getAnchors());
      throw new ApplicationException(""String_Node_Str"");
    }
    List<HtmlTable> tabs=(List<HtmlTable>)page.getByXPath(""String_Node_Str"");
    if (tabs.size() == 0) {
      Utils.report((List<? extends HtmlElement>)page.getByXPath(""String_Node_Str""));
      throw new ApplicationException(""String_Node_Str"");
    }
    ArrayList<HashMap<String,String>> liste=tableRowsToHashs(tabs.get(0));
    Utils.clearBestand(konto);
    double bestandswert=0;
    for (    HashMap<String,String> i : liste) {
      String[] bk=i.get(""String_Node_Str"").split(""String_Node_Str"");
      Utils.addBestand(konto,Utils.getDoubleFromZahl(i.get(""String_Node_Str"")),i.get(""String_Node_Str""),Utils.getDoubleFromZahl(bk[0]),bk[1],Utils.getDoubleFromZahl(i.get(""String_Node_Str"")) * Utils.getDoubleFromZahl(bk[0]),bk[1],new Date());
      bestandswert+=Utils.getDoubleFromZahl(i.get(""String_Node_Str"")) * Utils.getDoubleFromZahl(bk[0]);
    }
    konto.setSaldo(bestandswert);
    konto.store();
  }
 catch (  IOException e) {
    throw new ApplicationException(e);
  }
 finally {
    try {
      debug(seiten,konto);
    }
 catch (    RemoteException e) {
      throw new ApplicationException(e);
    }
  }
}",0.9689455124559208
89476,"private int ensureFrameMetadata() throws IOException {
  int offsetDiff=networkBufferWriteOffset - networkBufferReadOffset;
  if ((offsetDiff > 10) || (networkBuffer.length - networkBufferWriteOffset > 10)) {
    return 0;
  }
  int bytesRead=0;
switch (offsetDiff) {
case 1:
    System.arraycopy(networkBuffer,networkBufferReadOffset,networkBuffer,0,offsetDiff);
  networkBufferWriteOffset=offsetDiff;
case 0:
bytesRead=in.read(networkBuffer,offsetDiff,networkBuffer.length);
if (bytesRead == -1) {
return -1;
}
networkBufferReadOffset=0;
networkBufferWriteOffset+=bytesRead;
break;
default :
int frameMetadataLength=2;
int b1=networkBuffer[networkBufferReadOffset];
int b2=networkBuffer[networkBufferReadOffset + 1] & 0x7F;
try {
OpCode.fromInt(b1 & 0x0F);
}
 catch (Exception ex) {
return -1;
}
if (b2 > 0) {
switch (b2) {
case 126:
frameMetadataLength+=2;
break;
case 127:
frameMetadataLength+=8;
break;
default :
break;
}
int remainingCapacity=networkBuffer.length - networkBufferWriteOffset;
if (remainingCapacity > frameMetadataLength) {
bytesRead=in.read(networkBuffer,networkBufferWriteOffset,remainingCapacity);
if (bytesRead == -1) {
return -1;
}
networkBufferWriteOffset+=bytesRead;
}
 else {
System.arraycopy(networkBuffer,networkBufferReadOffset,networkBuffer,0,offsetDiff);
networkBufferReadOffset=0;
networkBufferWriteOffset=offsetDiff;
bytesRead=in.read(networkBuffer,networkBufferWriteOffset,networkBuffer.length);
if (bytesRead == -1) {
return -1;
}
networkBufferWriteOffset+=bytesRead;
}
}
}
return bytesRead;
}","private int ensureFrameMetadata() throws IOException {
  int offsetDiff=networkBufferWriteOffset - networkBufferReadOffset;
  if (offsetDiff > 10) {
    return 0;
  }
  int bytesRead=0;
  int maxMetadata=10;
  int length=maxMetadata - offsetDiff;
  int frameMetadataLength=2;
switch (offsetDiff) {
case 1:
    System.arraycopy(networkBuffer,networkBufferReadOffset,networkBuffer,0,offsetDiff);
  networkBufferWriteOffset=offsetDiff;
case 0:
length=frameMetadataLength - offsetDiff;
while (length > 0) {
bytesRead=in.read(networkBuffer,offsetDiff,length);
if (bytesRead == -1) {
  return -1;
}
length-=bytesRead;
networkBufferWriteOffset+=bytesRead;
}
networkBufferReadOffset=0;
default :
int b2=networkBuffer[networkBufferReadOffset + 1] & 0x7F;
if (b2 > 0) {
switch (b2) {
case 126:
frameMetadataLength+=2;
break;
case 127:
frameMetadataLength+=8;
break;
default :
break;
}
if (offsetDiff >= frameMetadataLength) {
return 0;
}
int remainingMetadata=networkBufferReadOffset + frameMetadataLength - networkBufferWriteOffset;
if (networkBuffer.length <= networkBufferWriteOffset + remainingMetadata) {
System.arraycopy(networkBuffer,networkBufferReadOffset,networkBuffer,0,offsetDiff);
networkBufferReadOffset=0;
networkBufferWriteOffset=offsetDiff;
}
while (remainingMetadata > 0) {
bytesRead=in.read(networkBuffer,networkBufferWriteOffset,remainingMetadata);
if (bytesRead == -1) {
return -1;
}
remainingMetadata-=bytesRead;
networkBufferWriteOffset+=bytesRead;
}
}
}
return bytesRead;
}",0.5102717031146454
89477,"private int ensureFrameMetadata() throws IOException {
  int offsetDiff=networkBufferWriteOffset - networkBufferReadOffset;
  if ((offsetDiff > 10) || (networkBuffer.length - networkBufferWriteOffset > 10)) {
    return 0;
  }
  int bytesRead=0;
switch (offsetDiff) {
case 1:
    System.arraycopy(networkBuffer,networkBufferReadOffset,networkBuffer,0,offsetDiff);
  networkBufferWriteOffset=offsetDiff;
case 0:
bytesRead=in.read(networkBuffer,offsetDiff,networkBuffer.length);
if (bytesRead == -1) {
return -1;
}
networkBufferReadOffset=0;
networkBufferWriteOffset+=bytesRead;
break;
default :
int frameMetadataLength=2;
int b1=networkBuffer[networkBufferReadOffset];
int b2=networkBuffer[networkBufferReadOffset + 1] & 0x7F;
try {
OpCode.fromInt(b1 & 0x0F);
}
 catch (Exception ex) {
return -1;
}
if (b2 > 0) {
switch (b2) {
case 126:
frameMetadataLength+=2;
break;
case 127:
frameMetadataLength+=8;
break;
default :
break;
}
int remainingCapacity=networkBuffer.length - networkBufferWriteOffset;
if (remainingCapacity > frameMetadataLength) {
bytesRead=in.read(networkBuffer,networkBufferWriteOffset,remainingCapacity);
if (bytesRead == -1) {
return -1;
}
networkBufferWriteOffset+=bytesRead;
}
 else {
System.arraycopy(networkBuffer,networkBufferReadOffset,networkBuffer,0,offsetDiff);
networkBufferReadOffset=0;
networkBufferWriteOffset=offsetDiff;
bytesRead=in.read(networkBuffer,networkBufferWriteOffset,networkBuffer.length);
if (bytesRead == -1) {
return -1;
}
networkBufferWriteOffset+=bytesRead;
}
}
}
return bytesRead;
}","private int ensureFrameMetadata() throws IOException {
  int offsetDiff=networkBufferWriteOffset - networkBufferReadOffset;
  if (offsetDiff > 10) {
    return 0;
  }
  int bytesRead=0;
  int maxMetadata=10;
  int length=maxMetadata - offsetDiff;
  int frameMetadataLength=2;
switch (offsetDiff) {
case 1:
    System.arraycopy(networkBuffer,networkBufferReadOffset,networkBuffer,0,offsetDiff);
  networkBufferWriteOffset=offsetDiff;
case 0:
length=frameMetadataLength - offsetDiff;
while (length > 0) {
bytesRead=in.read(networkBuffer,offsetDiff,length);
if (bytesRead == -1) {
  return -1;
}
length-=bytesRead;
networkBufferWriteOffset+=bytesRead;
}
networkBufferReadOffset=0;
default :
int b2=networkBuffer[networkBufferReadOffset + 1] & 0x7F;
if (b2 > 0) {
switch (b2) {
case 126:
frameMetadataLength+=2;
break;
case 127:
frameMetadataLength+=8;
break;
default :
break;
}
if (offsetDiff >= frameMetadataLength) {
return 0;
}
int remainingMetadata=networkBufferReadOffset + frameMetadataLength - networkBufferWriteOffset;
if (networkBuffer.length <= networkBufferWriteOffset + remainingMetadata) {
System.arraycopy(networkBuffer,networkBufferReadOffset,networkBuffer,0,offsetDiff);
networkBufferReadOffset=0;
networkBufferWriteOffset=offsetDiff;
}
while (remainingMetadata > 0) {
bytesRead=in.read(networkBuffer,networkBufferWriteOffset,remainingMetadata);
if (bytesRead == -1) {
return -1;
}
remainingMetadata-=bytesRead;
networkBufferWriteOffset+=bytesRead;
}
}
}
return bytesRead;
}",0.5102717031146454
89478,"@Override public int read(char[] cbuf,int offset,int length) throws IOException {
}","@Override public int read(char[] cbuf,int offset,int length) throws IOException {
  if ((offset < 0) || ((offset + length) > cbuf.length) || (length < 0)) {
    int len=offset + length;
    throw new IndexOutOfBoundsException(format(MSG_INDEX_OUT_OF_BOUNDS,offset,len,cbuf.length));
  }
  while (payloadLength == 0) {
    while (payloadOffset == -1) {
      int headerByte=in.read();
      if (headerByte == -1) {
        return -1;
      }
      header[headerOffset++]=(byte)headerByte;
switch (headerOffset) {
case 1:
        int flags=(header[0] & 0xF0) >> 4;
switch (flags) {
case 0:
case 8:
        break;
default :
      connection.doFail(WS_PROTOCOL_ERROR,format(MSG_RESERVED_BITS_SET,flags));
  }
int opcode=header[0] & 0x0F;
switch (opcode) {
case 0x08:
case 0x09:
case 0x0A:
if ((headerByte & 0x80) == 0) {
  connection.doFail(WS_PROTOCOL_ERROR,format(MSG_FRAGMENTED_CONTROL_FRAME,headerByte));
}
break;
case 0x00:
connection.doFail(WS_PROTOCOL_ERROR,format(MSG_FRAGMENTED_FRAME,headerByte));
break;
case 0x01:
break;
default :
connection.doFail(WS_PROTOCOL_ERROR,MSG_NON_TEXT_FRAME);
}
break;
case 2:
boolean masked=(header[1] & 0x80) != 0x00;
if (masked) {
connection.doFail(WS_PROTOCOL_ERROR,MSG_MASKED_FRAME_FROM_SERVER);
}
switch (header[1] & 0x7f) {
case 126:
case 127:
break;
default :
payloadOffset=0;
payloadLength=payloadLength(header);
break;
}
break;
case 4:
switch (header[1] & 0x7f) {
case 126:
payloadOffset=0;
payloadLength=payloadLength(header);
break;
default :
break;
}
break;
case 10:
switch (header[1] & 0x7f) {
case 127:
payloadOffset=0;
payloadLength=payloadLength(header);
break;
default :
break;
}
break;
}
}
filterControlFrames();
if ((header[0] & 0x0F) == 0x08) {
return -1;
}
if (payloadLength == 0) {
payloadOffset=-1;
headerOffset=0;
}
 else {
if (payloadLength > receiveBuffer.length) {
receiveBuffer=new char[(int)payloadLength];
}
charPayloadLength=connection.doTextFrame(receiveBuffer,0,receiveBuffer.length,payloadLength);
}
}
int charsRead=Math.min(length,charPayloadLength - charPayloadOffset);
System.arraycopy(receiveBuffer,charPayloadOffset,cbuf,offset,charsRead);
charPayloadOffset+=charsRead;
if (charPayloadOffset == charPayloadLength) {
headerOffset=0;
payloadOffset=-1;
payloadLength=0;
charPayloadOffset=0;
charPayloadLength=0;
}
return charsRead;
}",0.0695142378559464
89479,"/** 
 * Creates a new instance of   {@link ApplicationBasicChallengeHandler} with thespecified  {@link ClassLoader} using the {@link ServiceLoader} API withthe implementation specified under META-INF/services.
 * @param classLoader          ClassLoader to be used to instantiate
 * @return BasicChallengeHandler
 */
public static ApplicationBasicChallengeHandler create(ClassLoader classLoader){
  return create(ApplicationBasicChallengeHandler.class,classLoader);
}","/** 
 * Creates a new instance of   {@link ApplicationBasicChallengeHandler} with thespecified  {@link ClassLoader} using the {@link ServiceLoader} API withthe implementation specified under META-INF/services.
 * @param classLoader          ClassLoader to be used to instantiate
 * @return ApplicationBasicChallengeHandler
 */
public static ApplicationBasicChallengeHandler create(ClassLoader classLoader){
  return create(ApplicationBasicChallengeHandler.class,classLoader);
}",0.9883351007423118
89480,"/** 
 * Provide a general (non-realm-specific) login handler to be used in association with this challenge handler. The login handler is used to assist in obtaining credentials to respond to any Basic challenge requests when no realm-specific login handler matches the realm parameter of the request (if any).
 * @param loginHandler a login handler for credentials.
 */
public abstract ApplicationBasicChallengeHandler setLoginHandler(LoginHandler loginHandler);","/** 
 * Provide a general (non-realm-specific) login handler to be used in association with this challenge handler. The login handler is used to assist in obtaining credentials to respond to any Basic challenge requests when no realm-specific login handler matches the realm parameter of the request (if any).
 * @param loginHandler a login handler for credentials.
 * @return ApplicationBasicChallengeHandler
 */
public abstract ApplicationBasicChallengeHandler setLoginHandler(LoginHandler loginHandler);",0.9545454545454546
89481,"/** 
 * Creates a new instance of the sub-class of   {@link ChallengeHandler} withspecified  {@link ClassLoader} using the {@link ServiceLoader} API withthe implementation specified under META-INF/services.
 * @param T                    sub-type of ChallengeHandler
 * @param clazz                Class object of the sub-type
 * @param classLoader          ClassLoader to be used to instantiate
 * @return ChallengeHandler
 */
protected static <T extends ChallengeHandler>T create(Class<T> clazz,ClassLoader clazzLoader){
  return load0(clazz,ServiceLoader.load(clazz,clazzLoader));
}","/** 
 * Creates a new instance of the sub-class of   {@link ChallengeHandler} withspecified  {@link ClassLoader} using the {@link ServiceLoader} API withthe implementation specified under META-INF/services.
 * @param < T >                  subclass of ChallengeHandler
 * @param clazz                Class object of the sub-type
 * @param classLoader          ClassLoader to be used to instantiate
 * @return ChallengeHandler
 */
protected static <T extends ChallengeHandler>T create(Class<T> clazz,ClassLoader classLoader){
  return load0(clazz,ServiceLoader.load(clazz,classLoader));
}",0.9812286689419796
89482,"/** 
 * Handle the presented challenge by creating a challenge response or returning   {@code null}. This responsibility is usually achieved by using the associated   {@link LoginHandler} to obtain user credentials, and transforming those credentialsinto a  {@link ChallengeResponse}. <p/> When it is not possible to create a   {@link ChallengeResponse}, this method MUST return   {@code null}.
 * @param challengeRequest a challenge object
 * @return a challenge response object or {@code null} if no response is possible.
 */
public abstract ChallengeResponse handle(ChallengeRequest challengeRequest);","/** 
 * Handle the presented challenge by creating a challenge response or returning   {@code null}. This responsibility is usually achieved by using the associated   {@link LoginHandler} to obtain user credentials, and transforming those credentialsinto a  {@link ChallengeResponse}. <p> When it is not possible to create a   {@link ChallengeResponse}, this method MUST return   {@code null}.
 * @param challengeRequest a challenge object
 * @return a challenge response object or {@code null} if no response is possible.
 */
public abstract ChallengeResponse handle(ChallengeRequest challengeRequest);",0.9991714995857498
89483,"/** 
 * Return the protected URI the access of which triggered this challenge as a   {@code String}. <p/> For some authentication schemes, the production of a response to the challenge may require access to the location of the URI triggering the challenge.
 * @return the protected URI the access of which triggered this challenge as a {@code String}
 */
public String getLocation(){
  return location;
}","/** 
 * Return the protected URI the access of which triggered this challenge as a   {@code String}. <p> For some authentication schemes, the production of a response to the challenge may require access to the location of the URI triggering the challenge.
 * @return the protected URI the access of which triggered this challenge as a {@code String}
 */
public String getLocation(){
  return location;
}",0.9987608426270136
89484,"/** 
 * Clear the credentials of this response. <p/> Calling this method once the credentials have been communicated to the network layer protects credentials in memory.
 */
public void clearCredentials(){
  if (getCredentials() != null) {
    Arrays.fill(getCredentials(),(char)0);
  }
}","/** 
 * Clear the credentials of this response. <p> Calling this method once the credentials have been communicated to the network layer protects credentials in memory.
 */
public void clearCredentials(){
  if (getCredentials() != null) {
    Arrays.fill(getCredentials(),(char)0);
  }
}",0.9982608695652174
89485,"/** 
 * If the provided challengeHandler is registered at the provided location, clear that association such that any future challenge requests matching the location will never be handled by the provided challenge handler. <p/> If no such location or challengeHandler registration exists, this method silently succeeds.
 * @param locationDescription the exact location description at which the challenge handler was originally registered
 * @param challengeHandler the challenge handler to de-register.
 * @return a reference to this object for chained call support
 */
public abstract DispatchChallengeHandler unregister(String locationDescription,ChallengeHandler challengeHandler);","/** 
 * If the provided challengeHandler is registered at the provided location, clear that association such that any future challenge requests matching the location will never be handled by the provided challenge handler. <p> If no such location or challengeHandler registration exists, this method silently succeeds.
 * @param locationDescription the exact location description at which the challenge handler was originally registered
 * @param challengeHandler the challenge handler to de-register.
 * @return a reference to this object for chained call support
 */
public abstract DispatchChallengeHandler unregister(String locationDescription,ChallengeHandler challengeHandler);",0.9992684711046086
89486,"/** 
 * Register a challenge handler to respond to challenges at one or more locations. <p/> When a challenge response is received for a protected URI, the   {@code locationDescription}matches against elements of the protected URI; if a match is found, one consults the challenge handler(s) registered at that   {@code locationDescription} to finda challenge handler suitable to respond to the challenge. <p/> A  {@code locationDescription} comprises a username, password, host, port and paths,any of which can be wild-carded with the ""*"" character to match any number of request URIs. If no port is explicitly mentioned in a  {@code locationDescription}, a default port will be inferred based on the scheme mentioned in the location description, according to the following table: <table border=1> <tr><th>scheme</th><th>default port</th><th>Sample locationDescription</th></tr> <tr><td>http</td><td>80</td><td>foo.example.com or http://foo.example.com</td></tr> <tr><td>ws</td><td>80</td><td>foo.example.com or ws://foo.example.com</td></tr> <tr><td>https</td><td>443</td><td>https://foo.example.com</td></tr> <tr><td>wss</td><td>443</td><td>wss://foo.example.com</td></tr> </table> <p/> The protocol scheme (e.g. http or ws) if present in   {@code locationDescription} will not be used tomatch  {@code locationDescription} with the protected URI, because authentication challenges areimplemented on top of one of the HTTP/s protocols always, whether one is initiating web socket connections or regular HTTP connections.  That is to say for example, the locationDescription  {@code ""foo.example.com""}matches both URIs   {@code http://foo.example.com} and {@code ws://foo.example.com}. <p/> Some examples of   {@code locationDescription} values with wildcards are:<ol> <li> {@code *}&#047; -- matches all requests to any host on port 80 (default port), with no user info or path specified.  </li> <li>  {@code *.hostname.com:8000}  -- matches all requests to port 8000 on any sub-domain of {@code hostname.com}, but not   {@code hostname.com} itself.</li><li> {@code server.hostname.com:*}&#047;  {@code *} -- matches all requests to a particular server onany port on any path but not the empty path. </li> </ol>
 * @param locationDescription the (possibly wild-carded) location(s) at which to register a handler.
 * @param challengeHandler the challenge handler to register at the location(s).
 * @return a reference to this challenge handler for chained calls
 */
public abstract DispatchChallengeHandler register(String locationDescription,ChallengeHandler challengeHandler);","/** 
 * Register a challenge handler to respond to challenges at one or more locations. <p> When a challenge response is received for a protected URI, the   {@code locationDescription}matches against elements of the protected URI; if a match is found, one consults the challenge handler(s) registered at that   {@code locationDescription} to finda challenge handler suitable to respond to the challenge. <p> A  {@code locationDescription} comprises a username, password, host, port and paths,any of which can be wild-carded with the ""*"" character to match any number of request URIs. If no port is explicitly mentioned in a  {@code locationDescription}, a default port will be inferred based on the scheme mentioned in the location description, according to the following table: <table summary=""locationDescriptions"" border=1> <tr><th>scheme</th><th>default port</th><th>Sample locationDescription</th></tr> <tr><td>http</td><td>80</td><td>foo.example.com or http://foo.example.com</td></tr> <tr><td>ws</td><td>80</td><td>foo.example.com or ws://foo.example.com</td></tr> <tr><td>https</td><td>443</td><td>https://foo.example.com</td></tr> <tr><td>wss</td><td>443</td><td>wss://foo.example.com</td></tr> </table> <p> The protocol scheme (e.g. http or ws) if present in   {@code locationDescription} will not be used tomatch  {@code locationDescription} with the protected URI, because authentication challenges areimplemented on top of one of the HTTP/s protocols always, whether one is initiating web socket connections or regular HTTP connections.  That is to say for example, the locationDescription  {@code ""foo.example.com""}matches both URIs   {@code http://foo.example.com} and {@code ws://foo.example.com}. <p> Some examples of   {@code locationDescription} values with wildcards are:<ol> <li> {@code *}&#047; -- matches all requests to any host on port 80 (default port), with no user info or path specified.  </li> <li>  {@code *.hostname.com:8000}  -- matches all requests to port 8000 on any sub-domain of {@code hostname.com}, but not   {@code hostname.com} itself.</li><li> {@code server.hostname.com:*}&#047;  {@code *} -- matches all requests to a particular server onany port on any path but not the empty path. </li> </ol>
 * @param locationDescription the (possibly wild-carded) location(s) at which to register a handler.
 * @param challengeHandler the challenge handler to register at the location(s).
 * @return a reference to this challenge handler for chained calls
 */
public abstract DispatchChallengeHandler register(String locationDescription,ChallengeHandler challengeHandler);",0.9932471541578236
89487,"private static int payloadLength(byte[] header){
  int length=header[1] & 0x7f;
switch (length) {
case 126:
    return (header[2] & 0xff) << 8 | (header[3] & 0xff);
case 127:
  return (header[2] & 0xff) << 56 | (header[3] & 0xff) << 48 | (header[4] & 0xff) << 40 | (header[4] & 0xff) << 32 | (header[6] & 0xff) << 24 | (header[5] & 0xff) << 16 | (header[8] & 0xff) << 8 | (header[9] & 0xff);
default :
return length;
}
}","private static int payloadLength(byte[] header){
  int length=header[1] & 0x7f;
switch (length) {
case 126:
    return (header[2] & 0xff) << 8 | (header[3] & 0xff);
case 127:
  return (header[2] & 0xff) << 56 | (header[3] & 0xff) << 48 | (header[4] & 0xff) << 40 | (header[5] & 0xff) << 32 | (header[6] & 0xff) << 24 | (header[7] & 0xff) << 16 | (header[8] & 0xff) << 8 | (header[9] & 0xff);
default :
return length;
}
}",0.8714285714285714
89488,"@Override public void write(byte[] b,int off,int len) throws IOException {
  out.write(0x82);
switch (highestOneBit(len)) {
case 0x0000:
case 0x0001:
case 0x0002:
case 0x0004:
case 0x0008:
case 0x0010:
case 0x0020:
    out.write(0x80 | len);
  break;
case 0x0040:
switch (len) {
case 126:
  out.write(0x80 | 126);
out.write(0x00);
out.write(126);
break;
case 127:
out.write(0x80 | 126);
out.write(0x00);
out.write(127);
break;
default :
out.write(0x80 | len);
break;
}
break;
case 0x0080:
case 0x0100:
case 0x0200:
case 0x0400:
case 0x0800:
case 0x1000:
case 0x2000:
case 0x4000:
case 0x8000:
out.write(0x80 | 126);
out.write((len >> 8) & 0xff);
out.write((len >> 0) & 0xff);
break;
default :
out.write(0x80 | 127);
out.write((len >> 24) & 0xff);
out.write((len >> 16) & 0xff);
out.write((len >> 8) & 0xff);
out.write((len >> 0) & 0xff);
break;
}
byte[] mask=new byte[4];
random.nextBytes(mask);
out.write(mask);
byte[] masked=new byte[len];
for (int i=0; i < len; i++) {
int ioff=off + i;
masked[i]=(byte)(b[ioff] ^ mask[i % mask.length]);
}
out.write(masked);
}","@Override public void write(byte[] b,int off,int len) throws IOException {
  out.write(0x82);
switch (highestOneBit(len)) {
case 0x0000:
case 0x0001:
case 0x0002:
case 0x0004:
case 0x0008:
case 0x0010:
case 0x0020:
    out.write(0x80 | len);
  break;
case 0x0040:
switch (len) {
case 126:
  out.write(0x80 | 126);
out.write(0x00);
out.write(126);
break;
case 127:
out.write(0x80 | 126);
out.write(0x00);
out.write(127);
break;
default :
out.write(0x80 | len);
break;
}
break;
case 0x0080:
case 0x0100:
case 0x0200:
case 0x0400:
case 0x0800:
case 0x1000:
case 0x2000:
case 0x4000:
case 0x8000:
out.write(0x80 | 126);
out.write((len >> 8) & 0xff);
out.write((len >> 0) & 0xff);
break;
default :
out.write(0x80 | 127);
long length=len;
out.write((int)((length >> 56) & 0xff));
out.write((int)((length >> 48) & 0xff));
out.write((int)((length >> 40) & 0xff));
out.write((int)((length >> 32) & 0xff));
out.write((int)((length >> 24) & 0xff));
out.write((int)((length >> 16) & 0xff));
out.write((int)((length >> 8) & 0xff));
out.write((int)((length >> 0) & 0xff));
break;
}
byte[] mask=new byte[4];
random.nextBytes(mask);
out.write(mask);
byte[] masked=new byte[len];
for (int i=0; i < len; i++) {
int ioff=off + i;
masked[i]=(byte)(b[ioff] ^ mask[i % mask.length]);
}
out.write(masked);
}",0.8487430762675756
89489,"public WsURLConnectionHelper(){
  Map<String,String> supportedProtocols=new HashMap<String,String>();
  supportedProtocols.put(""String_Node_Str"",""String_Node_Str"");
  supportedProtocols.put(""String_Node_Str"",""String_Node_Str"");
  supportedProtocols.put(""String_Node_Str"",""String_Node_Str"");
  this.supportedProtocols=unmodifiableMap(supportedProtocols);
  this.extensionFactory=WebSocketExtensionFactory.newInstance();
  this.random=new SecureRandom();
}","public WsURLConnectionHelper(){
  Map<String,String> supportedProtocols=new HashMap<String,String>();
  supportedProtocols.put(""String_Node_Str"",""String_Node_Str"");
  this.supportedProtocols=unmodifiableMap(supportedProtocols);
  this.extensionFactory=WebSocketExtensionFactory.newInstance();
  this.random=new SecureRandom();
}",0.8388746803069054
89490,"@Test @Specification({""String_Node_Str"",""String_Node_Str""}) public void shouldEchoBinaryFrameWithPayloadLength127() throws Exception {
  k3po.join();
}","@Test @Specification({""String_Node_Str""}) public void shouldEchoBinaryFrameWithPayloadLength127() throws Exception {
  URLConnectionHelper helper=URLConnectionHelper.newInstance();
  URI location=URI.create(""String_Node_Str"");
  WsURLConnection connection=(WsURLConnection)helper.openConnection(location);
  OutputStream out=connection.getOutputStream();
  InputStream in=connection.getInputStream();
  byte[] writeBytes=new byte[127];
  random.nextBytes(writeBytes);
  out.write(writeBytes);
  byte[] readBytes=new byte[127];
  in.read(readBytes);
  k3po.join();
  assertArrayEquals(writeBytes,readBytes);
  connection.close();
}",0.3405889884763124
89491,"@Test @Specification({""String_Node_Str"",""String_Node_Str""}) public void shouldEchoBinaryFrameWithPayloadLength128() throws Exception {
  k3po.join();
}","@Test @Specification({""String_Node_Str""}) public void shouldEchoBinaryFrameWithPayloadLength128() throws Exception {
  URLConnectionHelper helper=URLConnectionHelper.newInstance();
  URI location=URI.create(""String_Node_Str"");
  WsURLConnection connection=(WsURLConnection)helper.openConnection(location);
  OutputStream out=connection.getOutputStream();
  InputStream in=connection.getInputStream();
  byte[] writeBytes=new byte[128];
  random.nextBytes(writeBytes);
  out.write(writeBytes);
  byte[] readBytes=new byte[128];
  in.read(readBytes);
  k3po.join();
  assertArrayEquals(writeBytes,readBytes);
  connection.close();
}",0.3405889884763124
89492,"@Test @Specification({""String_Node_Str"",""String_Node_Str""}) public void shouldEchoBinaryFrameWithPayloadLength126() throws Exception {
  k3po.join();
}","@Test @Specification({""String_Node_Str""}) public void shouldEchoBinaryFrameWithPayloadLength126() throws Exception {
  URLConnectionHelper helper=URLConnectionHelper.newInstance();
  URI location=URI.create(""String_Node_Str"");
  WsURLConnection connection=(WsURLConnection)helper.openConnection(location);
  OutputStream out=connection.getOutputStream();
  InputStream in=connection.getInputStream();
  byte[] writeBytes=new byte[126];
  random.nextBytes(writeBytes);
  out.write(writeBytes);
  byte[] readBytes=new byte[126];
  in.read(readBytes);
  k3po.join();
  assertArrayEquals(writeBytes,readBytes);
  connection.close();
}",0.3405889884763124
89493,"@Test @Specification({""String_Node_Str"",""String_Node_Str""}) public void shouldEchoBinaryFrameWithPayloadLength0() throws Exception {
  k3po.join();
}","@Test @Specification({""String_Node_Str""}) public void shouldEchoBinaryFrameWithPayloadLength0() throws Exception {
  URLConnectionHelper helper=URLConnectionHelper.newInstance();
  URI location=URI.create(""String_Node_Str"");
  WsURLConnection connection=(WsURLConnection)helper.openConnection(location);
  OutputStream out=connection.getOutputStream();
  InputStream in=connection.getInputStream();
  byte[] writeBytes=new byte[0];
  random.nextBytes(writeBytes);
  out.write(writeBytes);
  byte[] readBytes=new byte[0];
  in.read(readBytes);
  k3po.join();
  assertArrayEquals(writeBytes,readBytes);
  connection.close();
}",0.1966364812419146
89494,"@Test @Specification({""String_Node_Str"",""String_Node_Str""}) public void shouldEchoBinaryFrameWithPayloadLength65536() throws Exception {
  k3po.join();
}","@Test @Specification({""String_Node_Str""}) public void shouldEchoBinaryFrameWithPayloadLength65536() throws Exception {
  URLConnectionHelper helper=URLConnectionHelper.newInstance();
  URI location=URI.create(""String_Node_Str"");
  WsURLConnection connection=(WsURLConnection)helper.openConnection(location);
  OutputStream out=connection.getOutputStream();
  InputStream in=connection.getInputStream();
  byte[] writeBytes=new byte[65536];
  random.nextBytes(writeBytes);
  out.write(writeBytes);
  byte[] readBytes=new byte[65536];
  in.read(readBytes);
  k3po.join();
  assertArrayEquals(writeBytes,readBytes);
  connection.close();
}",0.3422053231939163
89495,"@Test @Specification({""String_Node_Str"",""String_Node_Str""}) public void shouldEchoBinaryFrameWithPayloadLength65535() throws Exception {
  k3po.join();
}","@Test @Specification({""String_Node_Str""}) public void shouldEchoBinaryFrameWithPayloadLength65535() throws Exception {
  URLConnectionHelper helper=URLConnectionHelper.newInstance();
  URI location=URI.create(""String_Node_Str"");
  WsURLConnection connection=(WsURLConnection)helper.openConnection(location);
  OutputStream out=connection.getOutputStream();
  InputStream in=connection.getInputStream();
  byte[] writeBytes=new byte[65535];
  random.nextBytes(writeBytes);
  out.write(writeBytes);
  byte[] readBytes=new byte[65535];
  in.read(readBytes);
  k3po.join();
  assertArrayEquals(writeBytes,readBytes);
  connection.close();
}",0.2027883396704689
89496,"/** 
 * Sets a list of regular expressions defining urls that should never be harvested from this domain. The list (after trimming the strings, and any empty strings have been removed) is copied to a list that is stored immutably.
 * @param regExps The list defining urls never to be harvested.
 * @param strictMode If true, we throw ArgumentNotValid exception if invalid regexps are found
 * @throws ArgumentNotValid if regExps is null or regExps contains invalid regular expressions (unless strictMode isfalse).
 */
public void setCrawlerTraps(List<String> regExps,boolean strictMode){
  ArgumentNotValid.checkNotNull(regExps,""String_Node_Str"");
  List<String> cleanedListOfCrawlerTraps=new ArrayList<String>();
  for (  String crawlerTrap : regExps) {
    log.trace(""String_Node_Str"" + crawlerTrap + ""String_Node_Str"");
    String trimmedString=crawlerTrap.trim();
    log.trace(""String_Node_Str"" + trimmedString + ""String_Node_Str"");
    if (!(trimmedString.length() == 0)) {
      cleanedListOfCrawlerTraps.add(crawlerTrap);
    }
 else {
      log.trace(""String_Node_Str"");
    }
  }
  List<String> errMsgs=new ArrayList<String>();
  for (  String regexp : cleanedListOfCrawlerTraps) {
    boolean wellformed=false;
    try {
      Pattern.compile(regexp);
      wellformed=CrawlertrapsUtils.isCrawlertrapsWellformedXML(regexp);
      if (!wellformed) {
        errMsgs.add(""String_Node_Str"" + regexp + ""String_Node_Str""+ ""String_Node_Str"");
      }
    }
 catch (    PatternSyntaxException e) {
      errMsgs.add(""String_Node_Str"" + regexp + ""String_Node_Str""+ e.getDescription()+ ""String_Node_Str"");
    }
  }
  if (strictMode)   if (errMsgs.size() > 0) {
    throw new ArgumentNotValid(errMsgs.size() + ""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",errMsgs));
  }
 else {
    log.warn(errMsgs.size() + ""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",errMsgs));
  }
  crawlerTraps=Collections.unmodifiableList(cleanedListOfCrawlerTraps);
  if (!crawlerTraps.isEmpty()) {
    log.trace(""String_Node_Str"",domainName,crawlerTraps.size());
  }
}","/** 
 * Sets a list of regular expressions defining urls that should never be harvested from this domain. The list (after trimming the strings, and any empty strings have been removed) is copied to a list that is stored immutably.
 * @param regExps The list defining urls never to be harvested.
 * @param strictMode If true, we throw ArgumentNotValid exception if invalid regexps are found
 * @throws ArgumentNotValid if regExps is null or regExps contains invalid regular expressions (unless strictMode isfalse).
 */
public void setCrawlerTraps(List<String> regExps,boolean strictMode){
  ArgumentNotValid.checkNotNull(regExps,""String_Node_Str"");
  List<String> cleanedListOfCrawlerTraps=new ArrayList<String>();
  for (  String crawlerTrap : regExps) {
    log.trace(""String_Node_Str"" + crawlerTrap + ""String_Node_Str"");
    String trimmedString=crawlerTrap.trim();
    log.trace(""String_Node_Str"" + trimmedString + ""String_Node_Str"");
    if (!(trimmedString.length() == 0)) {
      cleanedListOfCrawlerTraps.add(crawlerTrap);
    }
 else {
      log.trace(""String_Node_Str"");
    }
  }
  List<String> errMsgs=new ArrayList<String>();
  for (  String regexp : cleanedListOfCrawlerTraps) {
    boolean wellformed=false;
    try {
      Pattern.compile(regexp);
      wellformed=CrawlertrapsUtils.isCrawlertrapsWellformedXML(regexp);
      if (!wellformed) {
        errMsgs.add(""String_Node_Str"" + regexp + ""String_Node_Str""+ ""String_Node_Str"");
      }
    }
 catch (    PatternSyntaxException e) {
      errMsgs.add(""String_Node_Str"" + regexp + ""String_Node_Str""+ e.getDescription()+ ""String_Node_Str"");
    }
  }
  if (errMsgs.size() > 0) {
    if (strictMode) {
      throw new ArgumentNotValid(errMsgs.size() + ""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",errMsgs));
    }
 else {
      log.warn(errMsgs.size() + ""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",errMsgs));
    }
  }
  crawlerTraps=Collections.unmodifiableList(cleanedListOfCrawlerTraps);
  if (!crawlerTraps.isEmpty()) {
    log.trace(""String_Node_Str"",domainName,crawlerTraps.size());
  }
}",0.9206349206349206
89497,"protected Set<Long> cacheData(Set<Long> jobIDs){
  cacheCalled++;
  cacheParameter=jobIDs;
switch (mode) {
case SILENT:
    return null;
case WAITING:
  try {
    long before=System.currentTimeMillis();
synchronized (this) {
      o.notifyAll();
      o.wait(TIMEOUT);
      o.notifyAll();
    }
    woken|=System.currentTimeMillis() - before < TIMEOUT;
  }
 catch (  InterruptedException e) {
    return null;
  }
return null;
case REPLYING:
try {
File temp=getCacheFile(jobIDs);
temp.deleteOnExit();
FileOutputStream fos=new FileOutputStream(temp);
for (Long job : jobIDs) {
  fos.write(job.intValue());
}
fos.close();
return jobIDs;
}
 catch (IOException e) {
System.out.println(""String_Node_Str"" + e);
e.printStackTrace();
return null;
}
case REPLYING_DIR:
try {
File tempDir=getCacheFile(jobIDs);
tempDir.deleteOnExit();
tempDir.mkdir();
OutputStream fos=new FileOutputStream(new File(tempDir,""String_Node_Str""));
fos.close();
return jobIDs;
}
 catch (IOException e) {
System.out.println(""String_Node_Str"" + e);
e.printStackTrace();
return null;
}
case FAILING:
throw new IOFailure(""String_Node_Str"");
default :
return null;
}
}","protected Set<Long> cacheData(Set<Long> jobIDs){
  cacheCalled++;
  cacheParameter=jobIDs;
switch (mode) {
case SILENT:
    return null;
case WAITING:
  try {
    long before=System.currentTimeMillis();
synchronized (o) {
      o.notifyAll();
      o.wait(TIMEOUT);
      o.notifyAll();
    }
    woken|=System.currentTimeMillis() - before < TIMEOUT;
  }
 catch (  InterruptedException e) {
    return null;
  }
return null;
case REPLYING:
try {
File temp=getCacheFile(jobIDs);
temp.deleteOnExit();
FileOutputStream fos=new FileOutputStream(temp);
for (Long job : jobIDs) {
  fos.write(job.intValue());
}
fos.close();
return jobIDs;
}
 catch (IOException e) {
System.out.println(""String_Node_Str"" + e);
e.printStackTrace();
return null;
}
case REPLYING_DIR:
try {
File tempDir=getCacheFile(jobIDs);
tempDir.deleteOnExit();
tempDir.mkdir();
OutputStream fos=new FileOutputStream(new File(tempDir,""String_Node_Str""));
fos.close();
return jobIDs;
}
 catch (IOException e) {
System.out.println(""String_Node_Str"" + e);
e.printStackTrace();
return null;
}
case FAILING:
throw new IOFailure(""String_Node_Str"");
default :
return null;
}
}",0.9977905435262924
89498,"public void setUp(){
  cacheCalled=0;
  mode=Mode.SILENT;
  cacheParameter=null;
  o=new Object();
  woken=false;
}","public void setUp(){
  cacheCalled=0;
  mode=Mode.SILENT;
  cacheParameter=null;
  woken=false;
}",0.9150943396226416
89499,"/** 
 * Looks for a log statement like ""Created 212 jobs for harvest hgj8hy"".
 * @return
 */
static boolean isFinished(String harvestName,TestEnvironmentController testEnvironmentController){
  try {
    String output=testEnvironmentController.runCommand(TestEnvironment.JOB_ADMIN_SERVER,""String_Node_Str"" + testEnvironmentController.ENV.getTESTX() + ""String_Node_Str"",new int[]{0,1});
    final String harvestNamePattern=""String_Node_Str"" + harvestName + ""String_Node_Str"";
    Pattern finished=Pattern.compile(harvestNamePattern,Pattern.DOTALL);
    log.info(""String_Node_Str"" + harvestNamePattern + ""String_Node_Str""+ output+ ""String_Node_Str"");
    final Matcher matcher=finished.matcher(output);
    if (matcher.matches()) {
      return true;
    }
 else {
      return false;
    }
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
}","/** 
 * Looks for a log statement like ""Created 212 jobs for harvest hgj8hy"".
 * @return
 */
static boolean isFinished(String harvestName,TestEnvironmentController testEnvironmentController){
  try {
    String output=testEnvironmentController.runCommand(TestEnvironment.JOB_ADMIN_SERVER,""String_Node_Str"" + testEnvironmentController.ENV.getTESTX() + ""String_Node_Str"",new int[]{0,1});
    final String harvestNamePattern=""String_Node_Str"" + harvestName + ""String_Node_Str"";
    Pattern finished=Pattern.compile(harvestNamePattern,Pattern.DOTALL);
    log.debug(""String_Node_Str"" + harvestNamePattern + ""String_Node_Str""+ output+ ""String_Node_Str"");
    final Matcher matcher=finished.matcher(output);
    if (matcher.matches()) {
      return true;
    }
 else {
      return false;
    }
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
}",0.9947582993593476
89500,"public static void waitForJobGeneration(String harvestName,TestEnvironmentController testEnvironmentController){
  boolean keepWaiting=true;
  int secondsWaitingForJob=0;
  int maxSecondsToWaitForAllHarvests=360;
  while (keepWaiting) {
    System.err.print(""String_Node_Str"");
    try {
      Thread.sleep(10000);
      secondsWaitingForJob=secondsWaitingForJob + 10;
    }
 catch (    InterruptedException e) {
    }
    if (secondsWaitingForJob > maxSecondsToWaitForAllHarvests) {
      throw new RuntimeException(""String_Node_Str"" + harvestName + ""String_Node_Str""+ maxSecondsToWaitForAllHarvests+ ""String_Node_Str""+ ""String_Node_Str"");
    }
    keepWaiting=isFinished(harvestName,testEnvironmentController);
  }
}","public static void waitForJobGeneration(String harvestName,TestEnvironmentController testEnvironmentController){
  boolean keepWaiting=true;
  int secondsWaitingForJob=0;
  int maxSecondsToWaitForAllHarvests=360;
  while (keepWaiting) {
    System.err.print(""String_Node_Str"");
    try {
      Thread.sleep(10000);
      secondsWaitingForJob=secondsWaitingForJob + 10;
    }
 catch (    InterruptedException e) {
    }
    if (secondsWaitingForJob > maxSecondsToWaitForAllHarvests) {
      throw new RuntimeException(""String_Node_Str"" + harvestName + ""String_Node_Str""+ maxSecondsToWaitForAllHarvests+ ""String_Node_Str""+ ""String_Node_Str"");
    }
    keepWaiting=!isFinished(harvestName,testEnvironmentController);
  }
}",0.9993050729673384
89501,"public void killScheduling(){
  log.debug(""String_Node_Str"" + ""String_Node_Str"",id,harvestDefinition.getName(),harvestDefinitionsBeingScheduled);
  harvestDefinitionsBeingScheduled.remove(id);
  schedulingStartedMap.remove(id);
  threadMap.remove(id);
}","public void killScheduling(){
  harvestDefinitionsBeingScheduled.remove(id);
  schedulingStartedMap.remove(id);
  threadMap.remove(id);
  log.debug(""String_Node_Str"" + ""String_Node_Str"",id,harvestDefinition.getName(),harvestDefinitionsBeingScheduled);
}",0.5810276679841897
89502,"public synchronized void init(){
  try {
    if (bActive && !bInitialized) {
      if (job == null) {
        job=Heritrix3JobMonitorThread.jobDAO.read(jobId);
      }
      if (h3wrapper == null) {
        StartedJobInfo startedInfo=Heritrix3JobMonitorThread.runningJobsInfoDAO.getMostRecentByJobId(jobId);
        if (startedInfo != null) {
          hostUrl=startedInfo.getHostUrl();
          if (hostUrl != null && hostUrl.length() > 0) {
            h3wrapper=Heritrix3WrapperManager.getHeritrix3Wrapper(hostUrl,environment.h3AdminName,environment.h3AdminPassword);
          }
        }
      }
      if (jobname == null && h3wrapper != null) {
        jobname=Heritrix3WrapperManager.getJobname(h3wrapper,jobId);
      }
      if ((jobResult == null || jobResult.job == null) && jobname != null) {
        jobResult=h3wrapper.job(jobname);
      }
      if (jobResult != null && jobResult.job != null) {
        crawlLogFilePath=jobResult.job.crawlLogFilePath;
      }
      if (crawlLogFilePath != null) {
        logRaf=new RandomAccessFile(logFile,""String_Node_Str"");
        idxRaf=new RandomAccessFile(idxFile,""String_Node_Str"");
        idxRaf.writeLong(0);
        bInitialized=true;
      }
    }
  }
 catch (  Throwable t) {
  }
}","public synchronized void init(){
  try {
    if (bActive && !bInitialized) {
      if (job == null) {
        job=Heritrix3JobMonitorThread.jobDAO.read(jobId);
      }
      if (h3wrapper == null) {
        StartedJobInfo startedInfo=Heritrix3JobMonitorThread.runningJobsInfoDAO.getMostRecentByJobId(jobId);
        if (startedInfo != null) {
          hostUrl=startedInfo.getHostUrl();
          if (hostUrl != null && hostUrl.length() > 0) {
            h3wrapper=Heritrix3WrapperManager.getHeritrix3Wrapper(hostUrl,environment.h3AdminName,environment.h3AdminPassword);
          }
        }
      }
      if (jobname == null && h3wrapper != null) {
        jobname=Heritrix3WrapperManager.getJobname(h3wrapper,jobId);
      }
      if ((jobResult == null || jobResult.job == null) && jobname != null) {
        jobResult=h3wrapper.job(jobname);
      }
      if (jobResult != null && jobResult.job != null) {
        crawlLogFilePath=jobResult.job.crawlLogFilePath;
      }
      if (crawlLogFilePath != null) {
        logRaf=new RandomAccessFile(logFile,""String_Node_Str"");
        idxRaf=new RandomAccessFile(idxFile,""String_Node_Str"");
        if (idxRaf.length() == 0) {
          idxRaf.writeLong(0);
        }
 else {
          idxRaf.seek(idxRaf.length() - 8);
          lastIndexed=idxRaf.readLong();
          totalCachedLines=(idxRaf.length() / 8) - 1;
        }
        idxRaf.seek(idxRaf.length());
        logRaf.seek(logRaf.length());
        bInitialized=true;
      }
    }
  }
 catch (  Throwable t) {
  }
}",0.8980180180180181
89503,"public synchronized void updateCrawlLog(byte[] tmpBuf){
  long pos;
  long to;
  int idx;
  boolean bLoop;
  ByteRange byteRange;
  try {
    if (bActive && !bInitialized) {
      init();
    }
    if (bActive && bInitialized) {
      bLoop=true;
      while (bLoop) {
        idxRaf.seek(idxRaf.length());
        pos=logRaf.length();
        to=pos;
        if (jobResult != null && jobResult.job != null && jobResult.job.crawlLogFilePath != null) {
          StreamResult anypathResult=h3wrapper.anypath(jobResult.job.crawlLogFilePath,pos,pos + tmpBuf.length - 1);
          if (anypathResult != null && anypathResult.byteRange != null && anypathResult.in != null) {
            byteRange=anypathResult.byteRange;
            if (byteRange.contentLength > 0) {
              logRaf.seek(pos);
              int read;
              try {
                while ((read=anypathResult.in.read(tmpBuf)) != -1) {
                  logRaf.write(tmpBuf,0,read);
                  to+=read;
                  idx=0;
                  while (read > 0) {
                    ++pos;
                    --read;
                    if (tmpBuf[idx++] == '\n') {
                      idxRaf.writeLong(pos);
                      lastIndexed=pos;
                      totalCachedLines++;
                    }
                  }
                }
              }
 catch (              IOException e) {
                e.printStackTrace();
              }
              IOUtils.closeQuietly(anypathResult);
              if (byteRange.contentLength == to) {
                bLoop=false;
              }
            }
 else {
              bLoop=false;
            }
          }
 else {
            bLoop=false;
          }
        }
 else {
          bLoop=false;
        }
      }
    }
  }
 catch (  Throwable t) {
  }
}","public synchronized void updateCrawlLog(byte[] tmpBuf){
  long pos;
  long to;
  int idx;
  boolean bLoop;
  ByteRange byteRange;
  try {
    if (bActive && !bInitialized) {
      init();
    }
    if (bActive && bInitialized) {
      bLoop=true;
      while (bLoop) {
        idxRaf.seek(idxRaf.length());
        pos=logRaf.length();
        to=pos;
        if (jobResult != null && jobResult.job != null && jobResult.job.crawlLogFilePath != null) {
          long rangeFrom=pos;
          long rangeTo=pos + tmpBuf.length - 1;
          StreamResult anypathResult=h3wrapper.anypath(jobResult.job.crawlLogFilePath,null,null,true);
          LOG.info(""String_Node_Str"",jobId,anypathResult.contentLength);
          if (anypathResult != null && rangeFrom < anypathResult.contentLength) {
            if (rangeTo > anypathResult.contentLength) {
              rangeTo=anypathResult.contentLength;
            }
            anypathResult=h3wrapper.anypath(jobResult.job.crawlLogFilePath,rangeFrom,rangeTo);
            LOG.info(""String_Node_Str"",jobId,rangeFrom,rangeTo);
            if (anypathResult != null && anypathResult.byteRange != null && anypathResult.in != null) {
              byteRange=anypathResult.byteRange;
              if (byteRange.contentLength > 0) {
                logRaf.seek(pos);
                int read;
                try {
                  while ((read=anypathResult.in.read(tmpBuf)) != -1) {
                    logRaf.write(tmpBuf,0,read);
                    to+=read;
                    idx=0;
                    while (read > 0) {
                      ++pos;
                      --read;
                      if (tmpBuf[idx++] == '\n') {
                        idxRaf.writeLong(pos);
                        lastIndexed=pos;
                        totalCachedLines++;
                      }
                    }
                  }
                }
 catch (                IOException e) {
                  e.printStackTrace();
                }
                IOUtils.closeQuietly(anypathResult);
                if (byteRange.contentLength == to) {
                  bLoop=false;
                }
              }
 else {
                bLoop=false;
              }
            }
 else {
              bLoop=false;
            }
          }
 else {
            bLoop=false;
          }
        }
 else {
          bLoop=false;
        }
      }
    }
  }
 catch (  Throwable t) {
  }
}",0.8118672003767365
89504,"public void config(HttpServletRequest req,HttpServletResponse resp,List<Integer> numerics) throws IOException {
  Locale locale=resp.getLocale();
  resp.setContentType(""String_Node_Str"");
  ServletOutputStream out=resp.getOutputStream();
  Caching.caching_disable_headers(resp);
  TemplateBuilderFactory<ConfigTemplateBuilder> configTplBuilderFactory=TemplateBuilderFactory.getInstance(environment.templateMaster,""String_Node_Str"",""String_Node_Str"",ConfigTemplateBuilder.class);
  ConfigTemplateBuilder configTplBuilder=configTplBuilderFactory.getTemplateBuilder();
  StringBuilder sb=new StringBuilder();
  StringBuilder enabledhostsSb=new StringBuilder();
  List<String> invalidPatternsList=new LinkedList<String>();
  String method=req.getMethod().toUpperCase();
  if (""String_Node_Str"".equals(method)) {
    String enabledhostsStr=req.getParameter(""String_Node_Str"");
    String tmpStr;
    if (enabledhostsStr != null) {
      BufferedReader reader=new BufferedReader(new StringReader(enabledhostsStr));
      List<String> enabledhostsList=new LinkedList<String>();
      while ((tmpStr=reader.readLine()) != null) {
        enabledhostsList.add(tmpStr);
      }
      reader.close();
      environment.replaceH3HostnamePortRegexList(enabledhostsList,invalidPatternsList);
      environment.h3JobMonitorThread.updateH3HostnamePortFilter();
    }
  }
synchronized (environment.h3HostPortAllowRegexList) {
    StringMatcher stringMatcher;
    for (int i=0; i < environment.h3HostPortAllowRegexList.size(); ++i) {
      stringMatcher=environment.h3HostPortAllowRegexList.get(i);
      enabledhostsSb.append(stringMatcher.str);
      enabledhostsSb.append(""String_Node_Str"");
    }
  }
  if (invalidPatternsList.size() > 0) {
    sb.append(""String_Node_Str"");
    sb.append(environment.I18N.getString(locale,""String_Node_Str""));
    sb.append(""String_Node_Str"");
    for (int i=0; i < invalidPatternsList.size(); ++i) {
      sb.append(invalidPatternsList.get(i));
      sb.append(""String_Node_Str"");
    }
  }
synchronized (environment.h3JobMonitorThread.h3HostnamePortEnabledList) {
    sb.append(""String_Node_Str"");
    sb.append(environment.I18N.getString(locale,""String_Node_Str""));
    sb.append(""String_Node_Str"");
    if (environment.h3JobMonitorThread.h3HostnamePortEnabledList.size() > 0) {
      for (int i=0; i < environment.h3JobMonitorThread.h3HostnamePortEnabledList.size(); ++i) {
        sb.append(environment.h3JobMonitorThread.h3HostnamePortEnabledList.get(i));
        sb.append(""String_Node_Str"");
      }
    }
 else {
      sb.append(""String_Node_Str"");
      sb.append(environment.I18N.getString(locale,""String_Node_Str""));
      sb.append(""String_Node_Str"");
    }
  }
  sb.append(""String_Node_Str"");
synchronized (environment.h3JobMonitorThread.h3HostnamePortDisabledList) {
    sb.append(""String_Node_Str"");
    sb.append(environment.I18N.getString(locale,""String_Node_Str""));
    sb.append(""String_Node_Str"");
    if (environment.h3JobMonitorThread.h3HostnamePortDisabledList.size() > 0) {
      for (int i=0; i < environment.h3JobMonitorThread.h3HostnamePortDisabledList.size(); ++i) {
        sb.append(environment.h3JobMonitorThread.h3HostnamePortDisabledList.get(i));
        sb.append(""String_Node_Str"");
      }
    }
 else {
      sb.append(""String_Node_Str"");
      sb.append(environment.I18N.getString(locale,""String_Node_Str""));
      sb.append(""String_Node_Str"");
    }
  }
  StringBuilder menuSb=configTplBuilder.buildMenu(new StringBuilder(),null);
  configTplBuilder.insertContent(""String_Node_Str"",menuSb.toString(),environment.generateLanguageLinks(locale),""String_Node_Str"",enabledhostsSb.toString(),sb.toString(),""String_Node_Str"" + Settings.get(HarvesterSettings.HARVEST_MONITOR_REFRESH_INTERVAL) + ""String_Node_Str"").write(out);
  out.flush();
  out.close();
}","public void config(HttpServletRequest req,HttpServletResponse resp,List<Integer> numerics) throws IOException {
  Locale locale=resp.getLocale();
  resp.setContentType(""String_Node_Str"");
  ServletOutputStream out=resp.getOutputStream();
  Caching.caching_disable_headers(resp);
  TemplateBuilderFactory<ConfigTemplateBuilder> configTplBuilderFactory=TemplateBuilderFactory.getInstance(environment.templateMaster,""String_Node_Str"",""String_Node_Str"",ConfigTemplateBuilder.class);
  ConfigTemplateBuilder configTplBuilder=configTplBuilderFactory.getTemplateBuilder();
  StringBuilder sb=new StringBuilder();
  StringBuilder enabledhostsSb=new StringBuilder();
  List<String> invalidPatternsList=new LinkedList<String>();
  String method=req.getMethod().toUpperCase();
  if (""String_Node_Str"".equals(method)) {
    String enabledhostsStr=req.getParameter(""String_Node_Str"");
    String tmpStr;
    if (enabledhostsStr != null) {
      BufferedReader reader=new BufferedReader(new StringReader(enabledhostsStr));
      List<String> enabledhostsList=new LinkedList<String>();
      while ((tmpStr=reader.readLine()) != null) {
        enabledhostsList.add(tmpStr);
      }
      reader.close();
      environment.replaceH3HostnamePortRegexList(enabledhostsList,invalidPatternsList);
      environment.h3JobMonitorThread.updateH3HostnamePortFilter();
    }
  }
synchronized (environment.h3HostPortAllowRegexList) {
    StringMatcher stringMatcher;
    for (int i=0; i < environment.h3HostPortAllowRegexList.size(); ++i) {
      stringMatcher=environment.h3HostPortAllowRegexList.get(i);
      enabledhostsSb.append(stringMatcher.str);
      enabledhostsSb.append(""String_Node_Str"");
    }
  }
  if (invalidPatternsList.size() > 0) {
    sb.append(""String_Node_Str"");
    sb.append(environment.I18N.getString(locale,""String_Node_Str""));
    sb.append(""String_Node_Str"");
    for (int i=0; i < invalidPatternsList.size(); ++i) {
      sb.append(invalidPatternsList.get(i));
      sb.append(""String_Node_Str"");
    }
  }
synchronized (environment.h3JobMonitorThread.h3HostnamePortEnabledList) {
    sb.append(""String_Node_Str"");
    sb.append(environment.I18N.getString(locale,""String_Node_Str""));
    sb.append(""String_Node_Str"");
    if (environment.h3JobMonitorThread.h3HostnamePortEnabledList.size() > 0) {
      for (int i=0; i < environment.h3JobMonitorThread.h3HostnamePortEnabledList.size(); ++i) {
        sb.append(environment.h3JobMonitorThread.h3HostnamePortEnabledList.get(i));
        sb.append(""String_Node_Str"");
      }
    }
 else {
      sb.append(""String_Node_Str"");
      sb.append(environment.I18N.getString(locale,""String_Node_Str""));
      sb.append(""String_Node_Str"");
    }
  }
  sb.append(""String_Node_Str"");
synchronized (environment.h3JobMonitorThread.h3HostnamePortDisabledList) {
    sb.append(""String_Node_Str"");
    sb.append(environment.I18N.getString(locale,""String_Node_Str""));
    sb.append(""String_Node_Str"");
    if (environment.h3JobMonitorThread.h3HostnamePortDisabledList.size() > 0) {
      for (int i=0; i < environment.h3JobMonitorThread.h3HostnamePortDisabledList.size(); ++i) {
        sb.append(environment.h3JobMonitorThread.h3HostnamePortDisabledList.get(i));
        sb.append(""String_Node_Str"");
      }
    }
 else {
      sb.append(""String_Node_Str"");
      sb.append(environment.I18N.getString(locale,""String_Node_Str""));
      sb.append(""String_Node_Str"");
    }
  }
  StringBuilder menuSb=configTplBuilder.buildMenu(new StringBuilder(),null);
  configTplBuilder.insertContent(""String_Node_Str"",menuSb.toString(),environment.generateLanguageLinks(locale),""String_Node_Str"",enabledhostsSb.toString(),sb.toString(),""String_Node_Str"").write(out);
  out.flush();
  out.close();
}",0.9884477493028814
89505,"public void budget_change(HttpServletRequest req,HttpServletResponse resp,List<Integer> numerics) throws IOException {
  Locale locale=resp.getLocale();
  resp.setContentType(""String_Node_Str"");
  ServletOutputStream out=resp.getOutputStream();
  Caching.caching_disable_headers(resp);
  TemplateBuilderFactory<MasterTemplateBuilder> tplBuilder=TemplateBuilderFactory.getInstance(environment.templateMaster,""String_Node_Str"",""String_Node_Str"",MasterTemplateBuilder.class);
  MasterTemplateBuilder masterTplBuilder=tplBuilder.getTemplateBuilder();
  StringBuilder sb=new StringBuilder();
  String budget=req.getParameter(""String_Node_Str"");
  if (budget == null) {
    budget=""String_Node_Str"";
  }
  String key=req.getParameter(""String_Node_Str"");
  if (key == null) {
    key=""String_Node_Str"";
  }
  String submit1=req.getParameter(""String_Node_Str"");
  String submit2=req.getParameter(""String_Node_Str"");
  String initials=""String_Node_Str"";
  if (submit1 != null) {
    initials=req.getParameter(""String_Node_Str"");
  }
 else   if (submit2 != null) {
    initials=req.getParameter(""String_Node_Str"");
  }
  if (initials == null) {
    initials=""String_Node_Str"";
  }
  if (submit1 == null) {
    submit1=""String_Node_Str"";
  }
  if (submit2 == null) {
    submit2=""String_Node_Str"";
  }
  boolean isNumber=true;
  String script=environment.NAS_GROOVY_SCRIPT;
  String originalScript=script;
  script+=""String_Node_Str"";
  if ((!submit1.isEmpty() || !submit2.isEmpty()) && !initials.isEmpty()) {
    if (!submit1.isEmpty() && !budget.trim().isEmpty() && !key.trim().isEmpty()) {
      script+=""String_Node_Str"" + initials + ""String_Node_Str"";
      script+=""String_Node_Str"" + key + ""String_Node_Str""+ budget+ ""String_Node_Str"";
    }
 else {
      if (!submit2.isEmpty()) {
        String[] queues=req.getParameterValues(""String_Node_Str"");
        if (queues != null && queues.length > 0) {
          script+=""String_Node_Str"" + initials + ""String_Node_Str"";
          for (int i=0; i < queues.length; i++) {
            budget=req.getParameter(queues[i] + ""String_Node_Str"");
            if (budget != null && !budget.isEmpty()) {
              try {
                Integer.parseInt(budget);
                script+=""String_Node_Str"" + queues[i] + ""String_Node_Str""+ budget+ ""String_Node_Str"";
              }
 catch (              NumberFormatException e) {
                isNumber=false;
              }
            }
          }
        }
      }
    }
  }
  script+=""String_Node_Str"";
  script+=""String_Node_Str"";
  originalScript+=""String_Node_Str"";
  long jobId=numerics.get(0);
  Heritrix3JobMonitor h3Job=environment.h3JobMonitorThread.getRunningH3Job(jobId);
  if (h3Job != null && h3Job.isReady()) {
    boolean submitWithInitials=true;
    if ((!submit1.isEmpty() || !submit2.isEmpty()) && initials.isEmpty()) {
      sb.append(""String_Node_Str"");
    }
    if (!submit1.isEmpty() && initials.isEmpty()) {
      submitWithInitials=false;
    }
    try {
      if (budget != null && budget.length() > 0) {
        Integer.parseInt(budget);
      }
    }
 catch (    NumberFormatException e) {
      sb.append(""String_Node_Str"");
    }
    if (isNumber == false) {
      sb.append(""String_Node_Str"");
    }
    ScriptResult scriptResult=h3Job.h3wrapper.ExecuteShellScriptInJob(h3Job.jobResult.job.shortName,""String_Node_Str"",originalScript);
    if (scriptResult != null && scriptResult.script != null && scriptResult.script.htmlOutput != null) {
      sb.append(""String_Node_Str"");
      sb.append(scriptResult.script.htmlOutput);
      sb.append(""String_Node_Str"");
    }
    sb.append(""String_Node_Str"");
    scriptResult=h3Job.h3wrapper.ExecuteShellScriptInJob(h3Job.jobResult.job.shortName,""String_Node_Str"",script);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    if (!submitWithInitials) {
      sb.append(key);
    }
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    if (!submitWithInitials) {
      sb.append(budget);
    }
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + initials + ""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    if (scriptResult != null && scriptResult.script != null && scriptResult.script.htmlOutput != null) {
      sb.append(""String_Node_Str"");
      sb.append(scriptResult.script.htmlOutput);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"" + initials + ""String_Node_Str"");
      sb.append(""String_Node_Str"");
    }
    sb.append(""String_Node_Str"");
  }
 else {
    sb.append(""String_Node_Str"");
    sb.append(jobId);
    sb.append(""String_Node_Str"");
  }
  StringBuilder menuSb=masterTplBuilder.buildMenu(new StringBuilder(),h3Job);
  masterTplBuilder.insertContent(""String_Node_Str"" + jobId + ""String_Node_Str"",menuSb.toString(),environment.generateLanguageLinks(locale),""String_Node_Str"" + jobId + ""String_Node_Str"",sb.toString(),""String_Node_Str"" + Settings.get(HarvesterSettings.HARVEST_MONITOR_REFRESH_INTERVAL) + ""String_Node_Str"").write(out);
  out.flush();
  out.close();
}","public void budget_change(HttpServletRequest req,HttpServletResponse resp,List<Integer> numerics) throws IOException {
  Locale locale=resp.getLocale();
  resp.setContentType(""String_Node_Str"");
  ServletOutputStream out=resp.getOutputStream();
  Caching.caching_disable_headers(resp);
  TemplateBuilderFactory<MasterTemplateBuilder> tplBuilder=TemplateBuilderFactory.getInstance(environment.templateMaster,""String_Node_Str"",""String_Node_Str"",MasterTemplateBuilder.class);
  MasterTemplateBuilder masterTplBuilder=tplBuilder.getTemplateBuilder();
  StringBuilder sb=new StringBuilder();
  String budget=req.getParameter(""String_Node_Str"");
  if (budget == null) {
    budget=""String_Node_Str"";
  }
  String key=req.getParameter(""String_Node_Str"");
  if (key == null) {
    key=""String_Node_Str"";
  }
  String submit1=req.getParameter(""String_Node_Str"");
  String submit2=req.getParameter(""String_Node_Str"");
  String initials=""String_Node_Str"";
  if (submit1 != null) {
    initials=req.getParameter(""String_Node_Str"");
  }
 else   if (submit2 != null) {
    initials=req.getParameter(""String_Node_Str"");
  }
  if (initials == null) {
    initials=""String_Node_Str"";
  }
  if (submit1 == null) {
    submit1=""String_Node_Str"";
  }
  if (submit2 == null) {
    submit2=""String_Node_Str"";
  }
  boolean isNumber=true;
  String script=environment.NAS_GROOVY_SCRIPT;
  String originalScript=script;
  script+=""String_Node_Str"";
  if ((!submit1.isEmpty() || !submit2.isEmpty()) && !initials.isEmpty()) {
    if (!submit1.isEmpty() && !budget.trim().isEmpty() && !key.trim().isEmpty()) {
      script+=""String_Node_Str"" + initials + ""String_Node_Str"";
      script+=""String_Node_Str"" + key + ""String_Node_Str""+ budget+ ""String_Node_Str"";
    }
 else {
      if (!submit2.isEmpty()) {
        String[] queues=req.getParameterValues(""String_Node_Str"");
        if (queues != null && queues.length > 0) {
          script+=""String_Node_Str"" + initials + ""String_Node_Str"";
          for (int i=0; i < queues.length; i++) {
            budget=req.getParameter(queues[i] + ""String_Node_Str"");
            if (budget != null && !budget.isEmpty()) {
              try {
                Integer.parseInt(budget);
                script+=""String_Node_Str"" + queues[i] + ""String_Node_Str""+ budget+ ""String_Node_Str"";
              }
 catch (              NumberFormatException e) {
                isNumber=false;
              }
            }
          }
        }
      }
    }
  }
  script+=""String_Node_Str"";
  script+=""String_Node_Str"";
  originalScript+=""String_Node_Str"";
  long jobId=numerics.get(0);
  Heritrix3JobMonitor h3Job=environment.h3JobMonitorThread.getRunningH3Job(jobId);
  if (h3Job != null && h3Job.isReady()) {
    boolean submitWithInitials=true;
    if ((!submit1.isEmpty() || !submit2.isEmpty()) && initials.isEmpty()) {
      sb.append(""String_Node_Str"");
    }
    if (!submit1.isEmpty() && initials.isEmpty()) {
      submitWithInitials=false;
    }
    try {
      if (budget != null && budget.length() > 0) {
        Integer.parseInt(budget);
      }
    }
 catch (    NumberFormatException e) {
      sb.append(""String_Node_Str"");
    }
    if (isNumber == false) {
      sb.append(""String_Node_Str"");
    }
    ScriptResult scriptResult=h3Job.h3wrapper.ExecuteShellScriptInJob(h3Job.jobResult.job.shortName,""String_Node_Str"",originalScript);
    if (scriptResult != null && scriptResult.script != null && scriptResult.script.htmlOutput != null) {
      sb.append(""String_Node_Str"");
      sb.append(scriptResult.script.htmlOutput);
      sb.append(""String_Node_Str"");
    }
    sb.append(""String_Node_Str"");
    scriptResult=h3Job.h3wrapper.ExecuteShellScriptInJob(h3Job.jobResult.job.shortName,""String_Node_Str"",script);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    if (!submitWithInitials) {
      sb.append(key);
    }
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    if (!submitWithInitials) {
      sb.append(budget);
    }
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + initials + ""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    if (scriptResult != null && scriptResult.script != null && scriptResult.script.htmlOutput != null) {
      sb.append(""String_Node_Str"");
      sb.append(scriptResult.script.htmlOutput);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"" + initials + ""String_Node_Str"");
      sb.append(""String_Node_Str"");
    }
    sb.append(""String_Node_Str"");
  }
 else {
    sb.append(""String_Node_Str"");
    sb.append(jobId);
    sb.append(""String_Node_Str"");
  }
  StringBuilder menuSb=masterTplBuilder.buildMenu(new StringBuilder(),h3Job);
  masterTplBuilder.insertContent(""String_Node_Str"" + jobId + ""String_Node_Str"",menuSb.toString(),environment.generateLanguageLinks(locale),""String_Node_Str"" + jobId + ""String_Node_Str"",sb.toString(),""String_Node_Str"").write(out);
  out.flush();
  out.close();
}",0.9914546704645908
89506,"public void crawllog_list(HttpServletRequest req,HttpServletResponse resp,List<Integer> numerics) throws IOException {
  Locale locale=resp.getLocale();
  resp.setContentType(""String_Node_Str"");
  ServletOutputStream out=resp.getOutputStream();
  Caching.caching_disable_headers(resp);
  TemplateBuilderFactory<MasterTemplateBuilder> masterTplBuilderFactory=TemplateBuilderFactory.getInstance(environment.templateMaster,""String_Node_Str"",""String_Node_Str"",MasterTemplateBuilder.class);
  MasterTemplateBuilder masterTplBuilder=masterTplBuilderFactory.getTemplateBuilder();
  long lines;
  long linesPerPage=100;
  long page=1;
  long pages=0;
  String q=null;
  String tmpStr;
  tmpStr=req.getParameter(""String_Node_Str"");
  if (tmpStr != null && tmpStr.length() > 0) {
    try {
      page=Long.parseLong(tmpStr);
    }
 catch (    NumberFormatException e) {
    }
  }
  tmpStr=req.getParameter(""String_Node_Str"");
  if (tmpStr != null && tmpStr.length() > 0) {
    try {
      linesPerPage=Long.parseLong(tmpStr);
    }
 catch (    NumberFormatException e) {
    }
  }
  if (linesPerPage < 25) {
    linesPerPage=25;
  }
  if (linesPerPage > 1000) {
    linesPerPage=1000;
  }
  String additionalParams;
  tmpStr=req.getParameter(""String_Node_Str"");
  if (tmpStr != null && tmpStr.length() > 0 && !tmpStr.equalsIgnoreCase(""String_Node_Str"")) {
    q=tmpStr;
    additionalParams=""String_Node_Str"" + URLEncoder.encode(q,""String_Node_Str"");
  }
 else {
    additionalParams=""String_Node_Str"";
  }
  StringBuilder sb=new StringBuilder();
  long jobId=numerics.get(0);
  Heritrix3JobMonitor h3Job=environment.h3JobMonitorThread.getRunningH3Job(jobId);
  Pageable pageable=h3Job;
  if (h3Job != null && h3Job.isReady()) {
    String actionStr=req.getParameter(""String_Node_Str"");
    if (""String_Node_Str"".equalsIgnoreCase(actionStr)) {
      byte[] tmpBuf=new byte[1024 * 1024];
      h3Job.updateCrawlLog(tmpBuf);
    }
    long totalCachedLines=h3Job.getTotalCachedLines();
    long totalCachedSize=h3Job.getLastIndexed();
    SearchResult searchResult=null;
    if (q != null) {
      searchResult=h3Job.getSearchResult(q);
      searchResult.update();
      pageable=searchResult;
    }
 else {
      q=""String_Node_Str"";
    }
    lines=pageable.getIndexSize();
    if (lines > 0) {
      lines=(lines / 8) - 1;
      pages=Pagination.getPages(lines,linesPerPage);
    }
 else {
      lines=0;
    }
    if (page > pages) {
      page=pages;
    }
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(totalCachedLines);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(totalCachedSize);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + linesPerPage + ""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + q + ""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(lines);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(Pagination.getPagination(page,linesPerPage,pages,false,additionalParams));
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    if (lines > 0) {
      byte[] pageBytes=pageable.readPage(page,linesPerPage,true);
      sb.append(new String(pageBytes,""String_Node_Str""));
    }
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(Pagination.getPagination(page,linesPerPage,pages,false,additionalParams));
    sb.append(""String_Node_Str"");
  }
 else {
    sb.append(""String_Node_Str"");
    sb.append(jobId);
    sb.append(""String_Node_Str"");
  }
  StringBuilder menuSb=masterTplBuilder.buildMenu(new StringBuilder(),h3Job);
  masterTplBuilder.insertContent(""String_Node_Str"" + jobId + ""String_Node_Str"",menuSb.toString(),environment.generateLanguageLinks(locale),""String_Node_Str"" + jobId + ""String_Node_Str"",sb.toString(),""String_Node_Str"" + Settings.get(HarvesterSettings.HARVEST_MONITOR_REFRESH_INTERVAL) + ""String_Node_Str"").write(out);
  out.flush();
  out.close();
}","public void crawllog_list(HttpServletRequest req,HttpServletResponse resp,List<Integer> numerics) throws IOException {
  Locale locale=resp.getLocale();
  resp.setContentType(""String_Node_Str"");
  ServletOutputStream out=resp.getOutputStream();
  Caching.caching_disable_headers(resp);
  TemplateBuilderFactory<MasterTemplateBuilder> masterTplBuilderFactory=TemplateBuilderFactory.getInstance(environment.templateMaster,""String_Node_Str"",""String_Node_Str"",MasterTemplateBuilder.class);
  MasterTemplateBuilder masterTplBuilder=masterTplBuilderFactory.getTemplateBuilder();
  long lines;
  long linesPerPage=100;
  long page=1;
  long pages=0;
  String q=null;
  String tmpStr;
  tmpStr=req.getParameter(""String_Node_Str"");
  if (tmpStr != null && tmpStr.length() > 0) {
    try {
      page=Long.parseLong(tmpStr);
    }
 catch (    NumberFormatException e) {
    }
  }
  tmpStr=req.getParameter(""String_Node_Str"");
  if (tmpStr != null && tmpStr.length() > 0) {
    try {
      linesPerPage=Long.parseLong(tmpStr);
    }
 catch (    NumberFormatException e) {
    }
  }
  if (linesPerPage < 25) {
    linesPerPage=25;
  }
  if (linesPerPage > 1000) {
    linesPerPage=1000;
  }
  String additionalParams;
  tmpStr=req.getParameter(""String_Node_Str"");
  if (tmpStr != null && tmpStr.length() > 0 && !tmpStr.equalsIgnoreCase(""String_Node_Str"")) {
    q=tmpStr;
    additionalParams=""String_Node_Str"" + URLEncoder.encode(q,""String_Node_Str"");
  }
 else {
    additionalParams=""String_Node_Str"";
  }
  StringBuilder sb=new StringBuilder();
  long jobId=numerics.get(0);
  Heritrix3JobMonitor h3Job=environment.h3JobMonitorThread.getRunningH3Job(jobId);
  Pageable pageable=h3Job;
  if (h3Job != null && h3Job.isReady()) {
    String actionStr=req.getParameter(""String_Node_Str"");
    if (""String_Node_Str"".equalsIgnoreCase(actionStr)) {
      byte[] tmpBuf=new byte[1024 * 1024];
      h3Job.updateCrawlLog(tmpBuf);
    }
    long totalCachedLines=h3Job.getTotalCachedLines();
    long totalCachedSize=h3Job.getLastIndexed();
    SearchResult searchResult=null;
    if (q != null) {
      searchResult=h3Job.getSearchResult(q);
      searchResult.update();
      pageable=searchResult;
    }
 else {
      q=""String_Node_Str"";
    }
    lines=pageable.getIndexSize();
    if (lines > 0) {
      lines=(lines / 8) - 1;
      pages=Pagination.getPages(lines,linesPerPage);
    }
 else {
      lines=0;
    }
    if (page > pages) {
      page=pages;
    }
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(totalCachedLines);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(totalCachedSize);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + linesPerPage + ""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + q + ""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(lines);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(Pagination.getPagination(page,linesPerPage,pages,false,additionalParams));
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    if (lines > 0) {
      byte[] pageBytes=pageable.readPage(page,linesPerPage,true);
      sb.append(new String(pageBytes,""String_Node_Str""));
    }
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(Pagination.getPagination(page,linesPerPage,pages,false,additionalParams));
    sb.append(""String_Node_Str"");
  }
 else {
    sb.append(""String_Node_Str"");
    sb.append(jobId);
    sb.append(""String_Node_Str"");
  }
  StringBuilder menuSb=masterTplBuilder.buildMenu(new StringBuilder(),h3Job);
  masterTplBuilder.insertContent(""String_Node_Str"" + jobId + ""String_Node_Str"",menuSb.toString(),environment.generateLanguageLinks(locale),""String_Node_Str"" + jobId + ""String_Node_Str"",sb.toString(),""String_Node_Str"").write(out);
  out.flush();
  out.close();
}",0.9904824417459795
89507,"public void filter_add(HttpServletRequest req,HttpServletResponse resp,List<Integer> numerics) throws IOException {
  Locale locale=resp.getLocale();
  resp.setContentType(""String_Node_Str"");
  ServletOutputStream out=resp.getOutputStream();
  Caching.caching_disable_headers(resp);
  TemplateBuilderFactory<MasterTemplateBuilder> masterTplBuilderFactory=TemplateBuilderFactory.getInstance(environment.templateMaster,""String_Node_Str"",""String_Node_Str"",MasterTemplateBuilder.class);
  MasterTemplateBuilder masterTplBuilder=masterTplBuilderFactory.getTemplateBuilder();
  StringBuilder sb=new StringBuilder();
  String regex=req.getParameter(""String_Node_Str"");
  if (regex == null) {
    regex=""String_Node_Str"";
  }
  String[] removeIndexes=req.getParameterValues(""String_Node_Str"");
  if (removeIndexes == null) {
    removeIndexes=new String[0];
  }
  String initials=""String_Node_Str"";
  if (req.getParameter(""String_Node_Str"") != null) {
    initials=req.getParameter(""String_Node_Str"");
  }
 else   if (req.getParameter(""String_Node_Str"") != null) {
    initials=req.getParameter(""String_Node_Str"");
  }
  if (initials == null) {
    initials=""String_Node_Str"";
  }
  String script=environment.NAS_GROOVY_SCRIPT;
  if (regex.length() > 0 && !initials.isEmpty()) {
    String[] lines=regex.split(System.getProperty(""String_Node_Str""));
    for (    String line : lines) {
      if (line.endsWith(System.getProperty(""String_Node_Str"")) || line.endsWith(""String_Node_Str"") || line.endsWith(""String_Node_Str"")) {
        line=line.substring(0,line.length() - 1);
      }
      script+=""String_Node_Str"" + initials + ""String_Node_Str"";
      script+=""String_Node_Str"" + line.replace(""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"";
    }
  }
  if (removeIndexes.length > 0 && !initials.isEmpty()) {
    script+=""String_Node_Str"" + initials + ""String_Node_Str"";
    script+=""String_Node_Str"" + Arrays.toString(removeIndexes) + ""String_Node_Str"";
  }
  script+=""String_Node_Str"";
  long jobId=numerics.get(0);
  Heritrix3JobMonitor h3Job=environment.h3JobMonitorThread.getRunningH3Job(jobId);
  if (h3Job != null && h3Job.isReady()) {
    boolean keepRegexTextArea=false;
    if (req.getParameter(""String_Node_Str"") != null && removeIndexes.length == 0) {
      sb.append(""String_Node_Str"");
    }
    if (req.getParameter(""String_Node_Str"") != null && regex.isEmpty()) {
      sb.append(""String_Node_Str"");
    }
    if ((req.getParameter(""String_Node_Str"") != null || req.getParameter(""String_Node_Str"") != null) && initials.isEmpty()) {
      sb.append(""String_Node_Str"");
      keepRegexTextArea=true;
    }
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    if (keepRegexTextArea) {
      sb.append(regex);
    }
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + initials + ""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    ScriptResult scriptResult=h3Job.h3wrapper.ExecuteShellScriptInJob(h3Job.jobResult.job.shortName,""String_Node_Str"",script);
    if (scriptResult != null && scriptResult.script != null && scriptResult.script.htmlOutput != null) {
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(scriptResult.script.htmlOutput);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"" + initials + ""String_Node_Str"");
      sb.append(""String_Node_Str"");
    }
    sb.append(""String_Node_Str"");
  }
 else {
    sb.append(""String_Node_Str"");
    sb.append(jobId);
    sb.append(""String_Node_Str"");
  }
  StringBuilder menuSb=masterTplBuilder.buildMenu(new StringBuilder(),h3Job);
  masterTplBuilder.insertContent(""String_Node_Str"" + jobId + ""String_Node_Str"",menuSb.toString(),environment.generateLanguageLinks(locale),""String_Node_Str"" + jobId + ""String_Node_Str"",sb.toString(),""String_Node_Str"" + Settings.get(HarvesterSettings.HARVEST_MONITOR_REFRESH_INTERVAL) + ""String_Node_Str"").write(out);
  out.flush();
  out.close();
}","public void filter_add(HttpServletRequest req,HttpServletResponse resp,List<Integer> numerics) throws IOException {
  Locale locale=resp.getLocale();
  resp.setContentType(""String_Node_Str"");
  ServletOutputStream out=resp.getOutputStream();
  Caching.caching_disable_headers(resp);
  TemplateBuilderFactory<MasterTemplateBuilder> masterTplBuilderFactory=TemplateBuilderFactory.getInstance(environment.templateMaster,""String_Node_Str"",""String_Node_Str"",MasterTemplateBuilder.class);
  MasterTemplateBuilder masterTplBuilder=masterTplBuilderFactory.getTemplateBuilder();
  StringBuilder sb=new StringBuilder();
  String regex=req.getParameter(""String_Node_Str"");
  if (regex == null) {
    regex=""String_Node_Str"";
  }
  String[] removeIndexes=req.getParameterValues(""String_Node_Str"");
  if (removeIndexes == null) {
    removeIndexes=new String[0];
  }
  String initials=""String_Node_Str"";
  if (req.getParameter(""String_Node_Str"") != null) {
    initials=req.getParameter(""String_Node_Str"");
  }
 else   if (req.getParameter(""String_Node_Str"") != null) {
    initials=req.getParameter(""String_Node_Str"");
  }
  if (initials == null) {
    initials=""String_Node_Str"";
  }
  String script=environment.NAS_GROOVY_SCRIPT;
  if (regex.length() > 0 && !initials.isEmpty()) {
    String[] lines=regex.split(System.getProperty(""String_Node_Str""));
    for (    String line : lines) {
      if (line.endsWith(System.getProperty(""String_Node_Str"")) || line.endsWith(""String_Node_Str"") || line.endsWith(""String_Node_Str"")) {
        line=line.substring(0,line.length() - 1);
      }
      script+=""String_Node_Str"" + initials + ""String_Node_Str"";
      script+=""String_Node_Str"" + line.replace(""String_Node_Str"",""String_Node_Str"") + ""String_Node_Str"";
    }
  }
  if (removeIndexes.length > 0 && !initials.isEmpty()) {
    script+=""String_Node_Str"" + initials + ""String_Node_Str"";
    script+=""String_Node_Str"" + Arrays.toString(removeIndexes) + ""String_Node_Str"";
  }
  script+=""String_Node_Str"";
  long jobId=numerics.get(0);
  Heritrix3JobMonitor h3Job=environment.h3JobMonitorThread.getRunningH3Job(jobId);
  if (h3Job != null && h3Job.isReady()) {
    boolean keepRegexTextArea=false;
    if (req.getParameter(""String_Node_Str"") != null && removeIndexes.length == 0) {
      sb.append(""String_Node_Str"");
    }
    if (req.getParameter(""String_Node_Str"") != null && regex.isEmpty()) {
      sb.append(""String_Node_Str"");
    }
    if ((req.getParameter(""String_Node_Str"") != null || req.getParameter(""String_Node_Str"") != null) && initials.isEmpty()) {
      sb.append(""String_Node_Str"");
      keepRegexTextArea=true;
    }
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    if (keepRegexTextArea) {
      sb.append(regex);
    }
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + initials + ""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    ScriptResult scriptResult=h3Job.h3wrapper.ExecuteShellScriptInJob(h3Job.jobResult.job.shortName,""String_Node_Str"",script);
    if (scriptResult != null && scriptResult.script != null && scriptResult.script.htmlOutput != null) {
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(scriptResult.script.htmlOutput);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"" + initials + ""String_Node_Str"");
      sb.append(""String_Node_Str"");
    }
    sb.append(""String_Node_Str"");
  }
 else {
    sb.append(""String_Node_Str"");
    sb.append(jobId);
    sb.append(""String_Node_Str"");
  }
  StringBuilder menuSb=masterTplBuilder.buildMenu(new StringBuilder(),h3Job);
  masterTplBuilder.insertContent(""String_Node_Str"" + jobId + ""String_Node_Str"",menuSb.toString(),environment.generateLanguageLinks(locale),""String_Node_Str"" + jobId + ""String_Node_Str"",sb.toString(),""String_Node_Str"").write(out);
  out.flush();
  out.close();
}",0.9893212225359028
89508,"public void frontier_list(HttpServletRequest req,HttpServletResponse resp,List<Integer> numerics) throws IOException {
  Locale locale=resp.getLocale();
  resp.setContentType(""String_Node_Str"");
  ServletOutputStream out=resp.getOutputStream();
  Caching.caching_disable_headers(resp);
  TemplateBuilderFactory<MasterTemplateBuilder> masterTplBuilderFactory=TemplateBuilderFactory.getInstance(environment.templateMaster,""String_Node_Str"",""String_Node_Str"",MasterTemplateBuilder.class);
  MasterTemplateBuilder masterTplBuilder=masterTplBuilderFactory.getTemplateBuilder();
  StringBuilder sb=new StringBuilder();
  long lines;
  long linesPerPage=100;
  long page=1;
  long pages=0;
  String q=null;
  String tmpStr;
  tmpStr=req.getParameter(""String_Node_Str"");
  if (tmpStr != null && tmpStr.length() > 0) {
    try {
      page=Long.parseLong(tmpStr);
    }
 catch (    NumberFormatException e) {
    }
  }
  tmpStr=req.getParameter(""String_Node_Str"");
  if (tmpStr != null && tmpStr.length() > 0) {
    try {
      linesPerPage=Long.parseLong(tmpStr);
    }
 catch (    NumberFormatException e) {
    }
  }
  if (linesPerPage < 25) {
    linesPerPage=25;
  }
  if (linesPerPage > 1000) {
    linesPerPage=1000;
  }
  String initials=req.getParameter(""String_Node_Str"");
  if (initials == null) {
    initials=""String_Node_Str"";
  }
  tmpStr=req.getParameter(""String_Node_Str"");
  if (tmpStr != null && tmpStr.length() > 0 && !tmpStr.equalsIgnoreCase(""String_Node_Str"")) {
    q=tmpStr;
  }
 else {
    q=""String_Node_Str"";
  }
  String additionalParams=""String_Node_Str"";
  if (q.length() > 0) {
    additionalParams+=""String_Node_Str"" + URLEncoder.encode(q,""String_Node_Str"");
  }
  if (initials.length() > 0) {
    additionalParams+=""String_Node_Str"" + URLEncoder.encode(initials,""String_Node_Str"");
  }
  String script=environment.NAS_GROOVY_SCRIPT;
  String deleteStr=req.getParameter(""String_Node_Str"");
  if (deleteStr != null && ""String_Node_Str"".equals(deleteStr) && initials != null && initials.length() > 0) {
    script+=""String_Node_Str"";
    script+=""String_Node_Str"" + initials + ""String_Node_Str"";
    script+=""String_Node_Str"" + q + ""String_Node_Str"";
  }
 else {
    script+=""String_Node_Str"";
    script+=""String_Node_Str"" + q + ""String_Node_Str""+ linesPerPage+ ""String_Node_Str""+ (page - 1)+ ""String_Node_Str"";
  }
  long jobId=numerics.get(0);
  Heritrix3JobMonitor h3Job=environment.h3JobMonitorThread.getRunningH3Job(jobId);
  if (h3Job != null && h3Job.isReady()) {
    if (deleteStr != null && ""String_Node_Str"".equals(deleteStr) && (initials == null || initials.length() == 0)) {
      sb.append(""String_Node_Str"");
    }
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + linesPerPage + ""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + q + ""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + initials + ""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    ScriptResult scriptResult=h3Job.h3wrapper.ExecuteShellScriptInJob(h3Job.jobResult.job.shortName,""String_Node_Str"",script);
    lines=extractLinesAmount(scriptResult);
    if (lines > 0) {
      pages=(lines + linesPerPage - 1) / linesPerPage;
      if (pages == 0) {
        pages=1;
      }
    }
    if (page > pages) {
      page=pages;
    }
    if (scriptResult != null && scriptResult.script != null) {
      if (scriptResult.script.failure) {
        if (scriptResult.script.stackTrace != null) {
          sb.append(""String_Node_Str"");
          sb.append(""String_Node_Str"");
          sb.append(StringEscapeUtils.escapeHtml(scriptResult.script.stackTrace));
          sb.append(""String_Node_Str"");
        }
 else         if (scriptResult.script.exception != null) {
          sb.append(""String_Node_Str"");
          sb.append(""String_Node_Str"");
          sb.append(StringEscapeUtils.escapeHtml(scriptResult.script.exception));
          sb.append(""String_Node_Str"");
        }
 else {
          sb.append(""String_Node_Str"");
        }
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(StringEscapeUtils.escapeHtml(new String(scriptResult.response,""String_Node_Str"")));
        sb.append(""String_Node_Str"");
      }
 else {
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(lines);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(Pagination.getPagination(page,linesPerPage,pages,false,additionalParams));
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        if (scriptResult != null && scriptResult.script != null) {
          if (scriptResult.script.htmlOutput != null) {
            sb.append(""String_Node_Str"");
            sb.append(scriptResult.script.htmlOutput);
            sb.append(""String_Node_Str"");
          }
          if (scriptResult.script.rawOutput != null) {
            sb.append(""String_Node_Str"");
            sb.append(""String_Node_Str"");
            sb.append(scriptResult.script.rawOutput);
            sb.append(""String_Node_Str"");
            sb.append(""String_Node_Str"");
          }
        }
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(Pagination.getPagination(page,linesPerPage,pages,false,additionalParams));
        sb.append(""String_Node_Str"");
      }
    }
 else {
      sb.append(""String_Node_Str"");
    }
  }
 else {
    sb.append(""String_Node_Str"");
    sb.append(jobId);
    sb.append(""String_Node_Str"");
  }
  StringBuilder menuSb=masterTplBuilder.buildMenu(new StringBuilder(),h3Job);
  masterTplBuilder.insertContent(""String_Node_Str"" + jobId + ""String_Node_Str"",menuSb.toString(),environment.generateLanguageLinks(locale),""String_Node_Str"" + jobId + ""String_Node_Str"",sb.toString(),""String_Node_Str"" + Settings.get(HarvesterSettings.HARVEST_MONITOR_REFRESH_INTERVAL) + ""String_Node_Str"").write(out);
  out.flush();
  out.close();
}","public void frontier_list(HttpServletRequest req,HttpServletResponse resp,List<Integer> numerics) throws IOException {
  Locale locale=resp.getLocale();
  resp.setContentType(""String_Node_Str"");
  ServletOutputStream out=resp.getOutputStream();
  Caching.caching_disable_headers(resp);
  TemplateBuilderFactory<MasterTemplateBuilder> masterTplBuilderFactory=TemplateBuilderFactory.getInstance(environment.templateMaster,""String_Node_Str"",""String_Node_Str"",MasterTemplateBuilder.class);
  MasterTemplateBuilder masterTplBuilder=masterTplBuilderFactory.getTemplateBuilder();
  StringBuilder sb=new StringBuilder();
  long lines;
  long linesPerPage=100;
  long page=1;
  long pages=0;
  String q=null;
  String tmpStr;
  tmpStr=req.getParameter(""String_Node_Str"");
  if (tmpStr != null && tmpStr.length() > 0) {
    try {
      page=Long.parseLong(tmpStr);
    }
 catch (    NumberFormatException e) {
    }
  }
  tmpStr=req.getParameter(""String_Node_Str"");
  if (tmpStr != null && tmpStr.length() > 0) {
    try {
      linesPerPage=Long.parseLong(tmpStr);
    }
 catch (    NumberFormatException e) {
    }
  }
  if (linesPerPage < 25) {
    linesPerPage=25;
  }
  if (linesPerPage > 1000) {
    linesPerPage=1000;
  }
  String initials=req.getParameter(""String_Node_Str"");
  if (initials == null) {
    initials=""String_Node_Str"";
  }
  tmpStr=req.getParameter(""String_Node_Str"");
  if (tmpStr != null && tmpStr.length() > 0 && !tmpStr.equalsIgnoreCase(""String_Node_Str"")) {
    q=tmpStr;
  }
 else {
    q=""String_Node_Str"";
  }
  String additionalParams=""String_Node_Str"";
  if (q.length() > 0) {
    additionalParams+=""String_Node_Str"" + URLEncoder.encode(q,""String_Node_Str"");
  }
  if (initials.length() > 0) {
    additionalParams+=""String_Node_Str"" + URLEncoder.encode(initials,""String_Node_Str"");
  }
  String script=environment.NAS_GROOVY_SCRIPT;
  String deleteStr=req.getParameter(""String_Node_Str"");
  if (deleteStr != null && ""String_Node_Str"".equals(deleteStr) && initials != null && initials.length() > 0) {
    script+=""String_Node_Str"";
    script+=""String_Node_Str"" + initials + ""String_Node_Str"";
    script+=""String_Node_Str"" + q + ""String_Node_Str"";
  }
 else {
    script+=""String_Node_Str"";
    script+=""String_Node_Str"" + q + ""String_Node_Str""+ linesPerPage+ ""String_Node_Str""+ (page - 1)+ ""String_Node_Str"";
  }
  long jobId=numerics.get(0);
  Heritrix3JobMonitor h3Job=environment.h3JobMonitorThread.getRunningH3Job(jobId);
  if (h3Job != null && h3Job.isReady()) {
    if (deleteStr != null && ""String_Node_Str"".equals(deleteStr) && (initials == null || initials.length() == 0)) {
      sb.append(""String_Node_Str"");
    }
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + linesPerPage + ""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + q + ""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + initials + ""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    ScriptResult scriptResult=h3Job.h3wrapper.ExecuteShellScriptInJob(h3Job.jobResult.job.shortName,""String_Node_Str"",script);
    lines=extractLinesAmount(scriptResult);
    if (lines > 0) {
      pages=(lines + linesPerPage - 1) / linesPerPage;
      if (pages == 0) {
        pages=1;
      }
    }
    if (page > pages) {
      page=pages;
    }
    if (scriptResult != null && scriptResult.script != null) {
      if (scriptResult.script.failure) {
        if (scriptResult.script.stackTrace != null) {
          sb.append(""String_Node_Str"");
          sb.append(""String_Node_Str"");
          sb.append(StringEscapeUtils.escapeHtml(scriptResult.script.stackTrace));
          sb.append(""String_Node_Str"");
        }
 else         if (scriptResult.script.exception != null) {
          sb.append(""String_Node_Str"");
          sb.append(""String_Node_Str"");
          sb.append(StringEscapeUtils.escapeHtml(scriptResult.script.exception));
          sb.append(""String_Node_Str"");
        }
 else {
          sb.append(""String_Node_Str"");
        }
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(StringEscapeUtils.escapeHtml(new String(scriptResult.response,""String_Node_Str"")));
        sb.append(""String_Node_Str"");
      }
 else {
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(lines);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(Pagination.getPagination(page,linesPerPage,pages,false,additionalParams));
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        if (scriptResult != null && scriptResult.script != null) {
          if (scriptResult.script.htmlOutput != null) {
            sb.append(""String_Node_Str"");
            sb.append(scriptResult.script.htmlOutput);
            sb.append(""String_Node_Str"");
          }
          if (scriptResult.script.rawOutput != null) {
            sb.append(""String_Node_Str"");
            sb.append(""String_Node_Str"");
            sb.append(scriptResult.script.rawOutput);
            sb.append(""String_Node_Str"");
            sb.append(""String_Node_Str"");
          }
        }
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(Pagination.getPagination(page,linesPerPage,pages,false,additionalParams));
        sb.append(""String_Node_Str"");
      }
    }
 else {
      sb.append(""String_Node_Str"");
    }
  }
 else {
    sb.append(""String_Node_Str"");
    sb.append(jobId);
    sb.append(""String_Node_Str"");
  }
  StringBuilder menuSb=masterTplBuilder.buildMenu(new StringBuilder(),h3Job);
  masterTplBuilder.insertContent(""String_Node_Str"" + jobId + ""String_Node_Str"",menuSb.toString(),environment.generateLanguageLinks(locale),""String_Node_Str"" + jobId + ""String_Node_Str"",sb.toString(),""String_Node_Str"").write(out);
  out.flush();
  out.close();
}",0.9928857633494154
89509,"public void job(HttpServletRequest req,HttpServletResponse resp,List<Integer> numerics) throws IOException {
  Locale locale=resp.getLocale();
  resp.setContentType(""String_Node_Str"");
  ServletOutputStream out=resp.getOutputStream();
  Caching.caching_disable_headers(resp);
  TemplateBuilderFactory<MasterTemplateBuilder> masterTplBuilderFactory=TemplateBuilderFactory.getInstance(environment.templateMaster,""String_Node_Str"",""String_Node_Str"",MasterTemplateBuilder.class);
  MasterTemplateBuilder masterTplBuilder=masterTplBuilderFactory.getTemplateBuilder();
  StringBuilder sb=new StringBuilder();
  long jobId=numerics.get(0);
  Heritrix3JobMonitor h3Job=environment.h3JobMonitorThread.getRunningH3Job(jobId);
  Job job;
  HarvestDefinitionDAO dao=HarvestDefinitionDAO.getInstance();
  if (h3Job != null && !h3Job.bInitialized) {
    h3Job.init();
  }
  if (h3Job != null && h3Job.isReady()) {
    String action=req.getParameter(""String_Node_Str"");
    if (action != null && action.length() > 0) {
      if (""String_Node_Str"".equalsIgnoreCase(action)) {
        h3Job.h3wrapper.buildJobConfiguration(h3Job.jobname);
      }
      if (""String_Node_Str"".equalsIgnoreCase(action)) {
        h3Job.h3wrapper.launchJob(h3Job.jobname);
      }
      if (""String_Node_Str"".equalsIgnoreCase(action)) {
        h3Job.h3wrapper.pauseJob(h3Job.jobname);
      }
      if (""String_Node_Str"".equalsIgnoreCase(action)) {
        h3Job.h3wrapper.unpauseJob(h3Job.jobname);
      }
      if (""String_Node_Str"".equalsIgnoreCase(action)) {
        h3Job.h3wrapper.checkpointJob(h3Job.jobname);
      }
      if (""String_Node_Str"".equalsIgnoreCase(action)) {
        h3Job.h3wrapper.terminateJob(h3Job.jobname);
      }
      if (""String_Node_Str"".equalsIgnoreCase(action)) {
        h3Job.h3wrapper.teardownJob(h3Job.jobname);
      }
    }
    h3Job.update();
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + h3Job.jobId + ""String_Node_Str"");
    sb.append(h3Job.jobId);
    sb.append(""String_Node_Str"");
    if (h3Job.jobResult != null && h3Job.jobResult.job != null) {
      sb.append(""String_Node_Str"");
      sb.append(h3Job.jobResult.job.crawlControllerState);
      sb.append(""String_Node_Str"");
    }
    String harvestName=dao.getHarvestName(h3Job.job.getOrigHarvestDefinitionID());
    sb.append(""String_Node_Str"" + harvestName.replace(' ','+') + ""String_Node_Str"");
    sb.append(harvestName);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.job.getHarvestNum());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.job.isSnapshot());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.job.getChannel());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + h3Job.jobId + ""String_Node_Str"");
    sb.append(h3Job.job.getOrderXMLName());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.job.getCountDomains());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.job.getMaxBytesPerDomain());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.job.getMaxObjectsPerDomain());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.job.getMaxJobRunningTime());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.hostUrl);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.jobId);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    URL url1=new URL(h3Job.hostUrl);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + url1.getHost() + ""String_Node_Str""+ url1.getPort()+ ""String_Node_Str"");
    sb.append(h3Job.crawlLogFilePath);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(NASEnvironment.servicePath);
    sb.append(""String_Node_Str"");
    sb.append(h3Job.jobId);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(NASEnvironment.servicePath);
    sb.append(""String_Node_Str"");
    sb.append(h3Job.jobId);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(NASEnvironment.servicePath);
    sb.append(""String_Node_Str"");
    sb.append(h3Job.jobId);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(NASEnvironment.servicePath);
    sb.append(""String_Node_Str"");
    sb.append(h3Job.jobId);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(NASEnvironment.servicePath);
    sb.append(""String_Node_Str"");
    sb.append(h3Job.jobId);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    if (h3Job.jobResult != null && h3Job.jobResult.job != null) {
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      job=h3Job.jobResult.job;
      for (int i=0; i < job.availableActions.size(); ++i) {
        if (i > 0) {
          sb.append(""String_Node_Str"");
        }
        sb.append(""String_Node_Str"");
        String thisAction=job.availableActions.get(i);
        sb.append(thisAction);
        sb.append(""String_Node_Str"");
        if (""String_Node_Str"".equals(thisAction) || ""String_Node_Str"".equals(thisAction)) {
          sb.append(""String_Node_Str"");
          sb.append(thisAction);
          sb.append(""String_Node_Str"");
          sb.append(""String_Node_Str"");
          sb.append(""String_Node_Str"");
        }
 else {
          sb.append(""String_Node_Str"");
        }
        sb.append(job.availableActions.get(i).substring(0,1).toUpperCase() + job.availableActions.get(i).substring(1));
        sb.append(""String_Node_Str"");
      }
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(NASEnvironment.servicePath);
      sb.append(""String_Node_Str"");
      sb.append(h3Job.jobId);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      File logDir=new File(h3Job.crawlLogFilePath);
      sb.append(""String_Node_Str"");
      URL url=new URL(h3Job.hostUrl);
      sb.append(""String_Node_Str"" + url.getHost() + ""String_Node_Str""+ url.getPort()+ ""String_Node_Str""+ logDir.getParentFile().getAbsolutePath()+ ""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.shortName);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.crawlControllerState);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.crawlExitStatus);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.statusDescription);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(h3Job.hostUrl + ""String_Node_Str"" + h3Job.jobname);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(h3Job.hostUrl + ""String_Node_Str"" + h3Job.jobname);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      if (job.jobLogTail != null) {
        for (int i=0; i < job.jobLogTail.size(); ++i) {
          sb.append(""String_Node_Str"");
          sb.append(i);
          sb.append(""String_Node_Str"");
          sb.append(job.jobLogTail.get(i));
          sb.append(""String_Node_Str"");
        }
      }
      if (job.uriTotalsReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.uriTotalsReport.downloadedUriCount);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.uriTotalsReport.queuedUriCount);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.uriTotalsReport.totalUriCount);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.uriTotalsReport.futureUriCount);
        sb.append(""String_Node_Str"");
      }
      if (job.sizeTotalsReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.dupByHash);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.dupByHashCount);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.novel);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.novelCount);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.notModified);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.notModifiedCount);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.total);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.totalCount);
        sb.append(""String_Node_Str"");
      }
      if (job.rateReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.rateReport.currentDocsPerSecond);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.rateReport.averageDocsPerSecond);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.rateReport.currentKiBPerSec);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.rateReport.averageKiBPerSec);
        sb.append(""String_Node_Str"");
      }
      if (job.loadReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.loadReport.busyThreads);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.loadReport.totalThreads);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.loadReport.congestionRatio);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.loadReport.averageQueueDepth);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.loadReport.deepestQueueDepth);
        sb.append(""String_Node_Str"");
      }
      if (job.elapsedReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.elapsedReport.elapsedPretty);
        sb.append(""String_Node_Str"");
        sb.append(job.elapsedReport.elapsedMilliseconds);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
      }
      if (job.threadReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.threadReport.toeCount);
        sb.append(""String_Node_Str"");
        if (job.threadReport.steps != null) {
          for (int i=0; i < job.threadReport.steps.size(); ++i) {
            sb.append(""String_Node_Str"");
            sb.append(i);
            sb.append(""String_Node_Str"");
            sb.append(job.threadReport.steps.get(i));
            sb.append(""String_Node_Str"");
          }
        }
        if (job.threadReport.processors != null) {
          for (int i=0; i < job.threadReport.processors.size(); ++i) {
            sb.append(""String_Node_Str"");
            sb.append(i);
            sb.append(""String_Node_Str"");
            sb.append(job.threadReport.processors.get(i));
            sb.append(""String_Node_Str"");
          }
        }
      }
      if (job.frontierReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.totalQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.inProcessQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.readyQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.snoozedQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.activeQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.inactiveQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.ineligibleQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.retiredQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.exhaustedQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.lastReachedState);
        sb.append(""String_Node_Str"");
      }
      if (job.crawlLogTail != null) {
        for (int i=0; i < job.crawlLogTail.size(); ++i) {
          sb.append(""String_Node_Str"");
          sb.append(i);
          sb.append(""String_Node_Str"");
          sb.append(job.crawlLogTail.get(i));
          sb.append(""String_Node_Str"");
        }
      }
      sb.append(""String_Node_Str"");
      sb.append(job.isRunning);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.isLaunchable);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.alertCount);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.alertLogFilePath);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.crawlLogFilePath);
      sb.append(""String_Node_Str"");
      if (job.heapReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.heapReport.usedBytes);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.heapReport.totalBytes);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.heapReport.maxBytes);
        sb.append(""String_Node_Str"");
      }
    }
  }
 else {
    sb.append(""String_Node_Str"");
    sb.append(jobId);
    sb.append(""String_Node_Str"");
  }
  StringBuilder menuSb=masterTplBuilder.buildMenu(new StringBuilder(),h3Job);
  masterTplBuilder.insertContent(""String_Node_Str"" + jobId,menuSb.toString(),environment.generateLanguageLinks(locale),""String_Node_Str"" + jobId,sb.toString(),""String_Node_Str"" + Settings.get(HarvesterSettings.HARVEST_MONITOR_REFRESH_INTERVAL) + ""String_Node_Str"").write(out);
  out.flush();
  out.close();
}","public void job(HttpServletRequest req,HttpServletResponse resp,List<Integer> numerics) throws IOException {
  Locale locale=resp.getLocale();
  resp.setContentType(""String_Node_Str"");
  ServletOutputStream out=resp.getOutputStream();
  Caching.caching_disable_headers(resp);
  TemplateBuilderFactory<MasterTemplateBuilder> masterTplBuilderFactory=TemplateBuilderFactory.getInstance(environment.templateMaster,""String_Node_Str"",""String_Node_Str"",MasterTemplateBuilder.class);
  MasterTemplateBuilder masterTplBuilder=masterTplBuilderFactory.getTemplateBuilder();
  StringBuilder sb=new StringBuilder();
  long jobId=numerics.get(0);
  Heritrix3JobMonitor h3Job=environment.h3JobMonitorThread.getRunningH3Job(jobId);
  Job job;
  HarvestDefinitionDAO dao=HarvestDefinitionDAO.getInstance();
  if (h3Job != null && !h3Job.bInitialized) {
    h3Job.init();
  }
  if (h3Job != null && h3Job.isReady()) {
    String action=req.getParameter(""String_Node_Str"");
    if (action != null && action.length() > 0) {
      if (""String_Node_Str"".equalsIgnoreCase(action)) {
        h3Job.h3wrapper.buildJobConfiguration(h3Job.jobname);
      }
      if (""String_Node_Str"".equalsIgnoreCase(action)) {
        h3Job.h3wrapper.launchJob(h3Job.jobname);
      }
      if (""String_Node_Str"".equalsIgnoreCase(action)) {
        h3Job.h3wrapper.pauseJob(h3Job.jobname);
      }
      if (""String_Node_Str"".equalsIgnoreCase(action)) {
        h3Job.h3wrapper.unpauseJob(h3Job.jobname);
      }
      if (""String_Node_Str"".equalsIgnoreCase(action)) {
        h3Job.h3wrapper.checkpointJob(h3Job.jobname);
      }
      if (""String_Node_Str"".equalsIgnoreCase(action)) {
        h3Job.h3wrapper.terminateJob(h3Job.jobname);
      }
      if (""String_Node_Str"".equalsIgnoreCase(action)) {
        h3Job.h3wrapper.teardownJob(h3Job.jobname);
      }
    }
    h3Job.update();
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + h3Job.jobId + ""String_Node_Str"");
    sb.append(h3Job.jobId);
    sb.append(""String_Node_Str"");
    if (h3Job.jobResult != null && h3Job.jobResult.job != null) {
      sb.append(""String_Node_Str"");
      sb.append(h3Job.jobResult.job.crawlControllerState);
      sb.append(""String_Node_Str"");
    }
    String harvestName=dao.getHarvestName(h3Job.job.getOrigHarvestDefinitionID());
    sb.append(""String_Node_Str"" + harvestName.replace(' ','+') + ""String_Node_Str"");
    sb.append(harvestName);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.job.getHarvestNum());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.job.isSnapshot());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.job.getChannel());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.jobId);
    sb.append(""String_Node_Str"");
    sb.append(h3Job.job.getOrderXMLName());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.job.getCountDomains());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.job.getMaxBytesPerDomain());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.job.getMaxObjectsPerDomain());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.job.getMaxJobRunningTime());
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.hostUrl);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(h3Job.jobId);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    URL url1=new URL(h3Job.hostUrl);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"" + url1.getHost() + ""String_Node_Str""+ url1.getPort()+ ""String_Node_Str"");
    sb.append(h3Job.crawlLogFilePath);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(NASEnvironment.servicePath);
    sb.append(""String_Node_Str"");
    sb.append(h3Job.jobId);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(NASEnvironment.servicePath);
    sb.append(""String_Node_Str"");
    sb.append(h3Job.jobId);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(NASEnvironment.servicePath);
    sb.append(""String_Node_Str"");
    sb.append(h3Job.jobId);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(NASEnvironment.servicePath);
    sb.append(""String_Node_Str"");
    sb.append(h3Job.jobId);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(NASEnvironment.servicePath);
    sb.append(""String_Node_Str"");
    sb.append(h3Job.jobId);
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    sb.append(""String_Node_Str"");
    if (h3Job.jobResult != null && h3Job.jobResult.job != null) {
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      job=h3Job.jobResult.job;
      for (int i=0; i < job.availableActions.size(); ++i) {
        if (i > 0) {
          sb.append(""String_Node_Str"");
        }
        sb.append(""String_Node_Str"");
        String thisAction=job.availableActions.get(i);
        sb.append(thisAction);
        sb.append(""String_Node_Str"");
        if (""String_Node_Str"".equals(thisAction) || ""String_Node_Str"".equals(thisAction)) {
          sb.append(""String_Node_Str"");
          sb.append(thisAction);
          sb.append(""String_Node_Str"");
          sb.append(""String_Node_Str"");
          sb.append(""String_Node_Str"");
        }
 else {
          sb.append(""String_Node_Str"");
        }
        sb.append(job.availableActions.get(i).substring(0,1).toUpperCase() + job.availableActions.get(i).substring(1));
        sb.append(""String_Node_Str"");
      }
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(NASEnvironment.servicePath);
      sb.append(""String_Node_Str"");
      sb.append(h3Job.jobId);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      File logDir=new File(h3Job.crawlLogFilePath);
      sb.append(""String_Node_Str"");
      URL url=new URL(h3Job.hostUrl);
      sb.append(""String_Node_Str"" + url.getHost() + ""String_Node_Str""+ url.getPort()+ ""String_Node_Str""+ logDir.getParentFile().getAbsolutePath()+ ""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.shortName);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.crawlControllerState);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.crawlExitStatus);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.statusDescription);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(h3Job.hostUrl + ""String_Node_Str"" + h3Job.jobname);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(h3Job.hostUrl + ""String_Node_Str"" + h3Job.jobname);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      if (job.jobLogTail != null) {
        for (int i=0; i < job.jobLogTail.size(); ++i) {
          sb.append(""String_Node_Str"");
          sb.append(i);
          sb.append(""String_Node_Str"");
          sb.append(job.jobLogTail.get(i));
          sb.append(""String_Node_Str"");
        }
      }
      if (job.uriTotalsReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.uriTotalsReport.downloadedUriCount);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.uriTotalsReport.queuedUriCount);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.uriTotalsReport.totalUriCount);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.uriTotalsReport.futureUriCount);
        sb.append(""String_Node_Str"");
      }
      if (job.sizeTotalsReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.dupByHash);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.dupByHashCount);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.novel);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.novelCount);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.notModified);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.notModifiedCount);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.total);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.sizeTotalsReport.totalCount);
        sb.append(""String_Node_Str"");
      }
      if (job.rateReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.rateReport.currentDocsPerSecond);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.rateReport.averageDocsPerSecond);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.rateReport.currentKiBPerSec);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.rateReport.averageKiBPerSec);
        sb.append(""String_Node_Str"");
      }
      if (job.loadReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.loadReport.busyThreads);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.loadReport.totalThreads);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.loadReport.congestionRatio);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.loadReport.averageQueueDepth);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.loadReport.deepestQueueDepth);
        sb.append(""String_Node_Str"");
      }
      if (job.elapsedReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.elapsedReport.elapsedPretty);
        sb.append(""String_Node_Str"");
        sb.append(job.elapsedReport.elapsedMilliseconds);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
      }
      if (job.threadReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.threadReport.toeCount);
        sb.append(""String_Node_Str"");
        if (job.threadReport.steps != null) {
          for (int i=0; i < job.threadReport.steps.size(); ++i) {
            sb.append(""String_Node_Str"");
            sb.append(i);
            sb.append(""String_Node_Str"");
            sb.append(job.threadReport.steps.get(i));
            sb.append(""String_Node_Str"");
          }
        }
        if (job.threadReport.processors != null) {
          for (int i=0; i < job.threadReport.processors.size(); ++i) {
            sb.append(""String_Node_Str"");
            sb.append(i);
            sb.append(""String_Node_Str"");
            sb.append(job.threadReport.processors.get(i));
            sb.append(""String_Node_Str"");
          }
        }
      }
      if (job.frontierReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.totalQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.inProcessQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.readyQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.snoozedQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.activeQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.inactiveQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.ineligibleQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.retiredQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.exhaustedQueues);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.frontierReport.lastReachedState);
        sb.append(""String_Node_Str"");
      }
      if (job.crawlLogTail != null) {
        for (int i=0; i < job.crawlLogTail.size(); ++i) {
          sb.append(""String_Node_Str"");
          sb.append(i);
          sb.append(""String_Node_Str"");
          sb.append(job.crawlLogTail.get(i));
          sb.append(""String_Node_Str"");
        }
      }
      sb.append(""String_Node_Str"");
      sb.append(job.isRunning);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.isLaunchable);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.alertCount);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.alertLogFilePath);
      sb.append(""String_Node_Str"");
      sb.append(""String_Node_Str"");
      sb.append(job.crawlLogFilePath);
      sb.append(""String_Node_Str"");
      if (job.heapReport != null) {
        sb.append(""String_Node_Str"");
        sb.append(job.heapReport.usedBytes);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.heapReport.totalBytes);
        sb.append(""String_Node_Str"");
        sb.append(""String_Node_Str"");
        sb.append(job.heapReport.maxBytes);
        sb.append(""String_Node_Str"");
      }
    }
  }
 else {
    sb.append(""String_Node_Str"");
    sb.append(jobId);
    sb.append(""String_Node_Str"");
  }
  StringBuilder menuSb=masterTplBuilder.buildMenu(new StringBuilder(),h3Job);
  masterTplBuilder.insertContent(""String_Node_Str"" + jobId,menuSb.toString(),environment.generateLanguageLinks(locale),""String_Node_Str"" + jobId,sb.toString(),""String_Node_Str"" + Settings.get(HarvesterSettings.HARVEST_MONITOR_REFRESH_INTERVAL) + ""String_Node_Str"").write(out);
  out.flush();
  out.close();
}",0.9977123778904414
89510,"public void script(HttpServletRequest req,HttpServletResponse resp,List<Integer> numerics) throws IOException {
  Locale locale=resp.getLocale();
  resp.setContentType(""String_Node_Str"");
  ServletOutputStream out=resp.getOutputStream();
  Caching.caching_disable_headers(resp);
  TemplateBuilderFactory<H3ScriptTemplateBuilder> scriptTplBuilderFactory=TemplateBuilderFactory.getInstance(environment.templateMaster,""String_Node_Str"",""String_Node_Str"",H3ScriptTemplateBuilder.class);
  H3ScriptTemplateBuilder scriptTplBuilder=scriptTplBuilderFactory.getTemplateBuilder();
  String engineStr=req.getParameter(""String_Node_Str"");
  String scriptStr=req.getParameter(""String_Node_Str"");
  if (scriptStr == null) {
    scriptStr=""String_Node_Str"";
  }
  StringBuilder sb=new StringBuilder();
  long jobId=numerics.get(0);
  Heritrix3JobMonitor h3Job=environment.h3JobMonitorThread.getRunningH3Job(jobId);
  if (h3Job != null && h3Job.isReady()) {
    if (engineStr != null && engineStr.length() > 0 && scriptStr != null && scriptStr.length() > 0) {
      ScriptResult scriptResult=h3Job.h3wrapper.ExecuteShellScriptInJob(h3Job.jobResult.job.shortName,engineStr,scriptStr);
      if (scriptResult != null && scriptResult.script != null) {
        if (scriptResult.script.failure) {
          if (scriptResult.script.stackTrace != null) {
            sb.append(""String_Node_Str"");
            sb.append(""String_Node_Str"");
            sb.append(StringEscapeUtils.escapeHtml(scriptResult.script.stackTrace));
            sb.append(""String_Node_Str"");
          }
 else           if (scriptResult.script.exception != null) {
            sb.append(""String_Node_Str"");
            sb.append(""String_Node_Str"");
            sb.append(StringEscapeUtils.escapeHtml(scriptResult.script.exception));
            sb.append(""String_Node_Str"");
          }
 else {
            sb.append(""String_Node_Str"");
          }
          sb.append(""String_Node_Str"");
          sb.append(""String_Node_Str"");
          sb.append(StringEscapeUtils.escapeHtml(new String(scriptResult.response,""String_Node_Str"")));
          sb.append(""String_Node_Str"");
        }
 else {
          if (scriptResult.script.htmlOutput != null) {
            sb.append(scriptResult.script.htmlOutput);
          }
          if (scriptResult.script.rawOutput != null) {
            sb.append(""String_Node_Str"");
            sb.append(scriptResult.script.rawOutput);
            sb.append(""String_Node_Str"");
          }
          sb.append(""String_Node_Str"");
          sb.append(new String(scriptResult.response,""String_Node_Str""));
          sb.append(""String_Node_Str"");
        }
      }
 else {
        sb.append(""String_Node_Str"");
      }
    }
  }
  StringBuilder menuSb=scriptTplBuilder.buildMenu(new StringBuilder(),h3Job);
  scriptTplBuilder.insertContent(""String_Node_Str"",menuSb.toString(),environment.generateLanguageLinks(locale),""String_Node_Str"",scriptStr,sb.toString(),""String_Node_Str"" + Settings.get(HarvesterSettings.HARVEST_MONITOR_REFRESH_INTERVAL) + ""String_Node_Str"").write(out);
  out.flush();
  out.close();
}","public void script(HttpServletRequest req,HttpServletResponse resp,List<Integer> numerics) throws IOException {
  Locale locale=resp.getLocale();
  resp.setContentType(""String_Node_Str"");
  ServletOutputStream out=resp.getOutputStream();
  Caching.caching_disable_headers(resp);
  TemplateBuilderFactory<H3ScriptTemplateBuilder> scriptTplBuilderFactory=TemplateBuilderFactory.getInstance(environment.templateMaster,""String_Node_Str"",""String_Node_Str"",H3ScriptTemplateBuilder.class);
  H3ScriptTemplateBuilder scriptTplBuilder=scriptTplBuilderFactory.getTemplateBuilder();
  String engineStr=req.getParameter(""String_Node_Str"");
  String scriptStr=req.getParameter(""String_Node_Str"");
  if (scriptStr == null) {
    scriptStr=""String_Node_Str"";
  }
  StringBuilder sb=new StringBuilder();
  long jobId=numerics.get(0);
  Heritrix3JobMonitor h3Job=environment.h3JobMonitorThread.getRunningH3Job(jobId);
  if (h3Job != null && h3Job.isReady()) {
    if (engineStr != null && engineStr.length() > 0 && scriptStr != null && scriptStr.length() > 0) {
      ScriptResult scriptResult=h3Job.h3wrapper.ExecuteShellScriptInJob(h3Job.jobResult.job.shortName,engineStr,scriptStr);
      if (scriptResult != null && scriptResult.script != null) {
        if (scriptResult.script.failure) {
          if (scriptResult.script.stackTrace != null) {
            sb.append(""String_Node_Str"");
            sb.append(""String_Node_Str"");
            sb.append(StringEscapeUtils.escapeHtml(scriptResult.script.stackTrace));
            sb.append(""String_Node_Str"");
          }
 else           if (scriptResult.script.exception != null) {
            sb.append(""String_Node_Str"");
            sb.append(""String_Node_Str"");
            sb.append(StringEscapeUtils.escapeHtml(scriptResult.script.exception));
            sb.append(""String_Node_Str"");
          }
 else {
            sb.append(""String_Node_Str"");
          }
          sb.append(""String_Node_Str"");
          sb.append(""String_Node_Str"");
          sb.append(StringEscapeUtils.escapeHtml(new String(scriptResult.response,""String_Node_Str"")));
          sb.append(""String_Node_Str"");
        }
 else {
          if (scriptResult.script.htmlOutput != null) {
            sb.append(scriptResult.script.htmlOutput);
          }
          if (scriptResult.script.rawOutput != null) {
            sb.append(""String_Node_Str"");
            sb.append(scriptResult.script.rawOutput);
            sb.append(""String_Node_Str"");
          }
          sb.append(""String_Node_Str"");
          sb.append(new String(scriptResult.response,""String_Node_Str""));
          sb.append(""String_Node_Str"");
        }
      }
 else {
        sb.append(""String_Node_Str"");
      }
    }
  }
  StringBuilder menuSb=scriptTplBuilder.buildMenu(new StringBuilder(),h3Job);
  scriptTplBuilder.insertContent(""String_Node_Str"",menuSb.toString(),environment.generateLanguageLinks(locale),""String_Node_Str"",scriptStr,sb.toString(),""String_Node_Str"").write(out);
  out.flush();
  out.close();
}",0.9857072449482503
89511,"@Override public void run(){
  Map<Long,Heritrix3JobMonitor> tmpJobMonitorMap;
  Iterator<Heritrix3JobMonitor> jobmonitorIter;
  byte[] tmpBuf=new byte[1024 * 1024];
  try {
    LOG.info(""String_Node_Str"");
    File tmpFolder=environment.tempPath;
    ;
    File[] oldFiles=tmpFolder.listFiles(new FilenameFilter(){
      @Override public boolean accept(      File dir,      String name){
        if (name.startsWith(""String_Node_Str"")) {
          if (name.endsWith(""String_Node_Str"") || name.endsWith(""String_Node_Str"")) {
            return true;
          }
        }
        return false;
      }
    }
);
    Map<String,File> oldFilesMap=new HashMap<String,File>();
    File tmpFile;
    for (int i=0; i < oldFiles.length; ++i) {
      tmpFile=oldFiles[i];
      oldFilesMap.put(tmpFile.getName(),tmpFile);
    }
    ;
    List<File> oldFilesList=new ArrayList<File>();
    while (!bExit) {
      Set<Long> runningJobs=getRunningJobs();
      if (runningJobs != null) {
        Iterator<Long> jobidIter=runningJobs.iterator();
        Heritrix3JobMonitor jobmonitor;
synchronized (runningJobMonitorMap) {
          filterJobMonitorMap.clear();
          while (jobidIter.hasNext()) {
            Long jobId=jobidIter.next();
            if (jobId != null) {
              jobmonitor=runningJobMonitorMap.remove(jobId);
              if (jobmonitor == null) {
                try {
                  jobmonitor=Heritrix3WrapperManager.getJobMonitor(jobId,environment);
                }
 catch (                IOException e) {
                }
              }
              filterJobMonitorMap.put(jobId,jobmonitor);
            }
          }
          tmpJobMonitorMap=filterJobMonitorMap;
          filterJobMonitorMap=runningJobMonitorMap;
          runningJobMonitorMap=tmpJobMonitorMap;
          LOG.debug(""String_Node_Str"" + runningJobMonitorMap.hashCode());
        }
        jobmonitorIter=filterJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          jobmonitor.cleanup(oldFilesList);
        }
        jobmonitorIter=runningJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          if (oldFilesMap != null) {
            oldFilesMap.remove(jobmonitor.logFile.getName());
            oldFilesMap.remove(jobmonitor.idxFile.getName());
          }
          if (!jobmonitor.bInitialized) {
            jobmonitor.init();
          }
          checkH3HostnamePort(jobmonitor);
          isH3HostnamePortEnabled(jobmonitor);
          if (jobmonitor.bPull) {
            jobmonitor.updateCrawlLog(tmpBuf);
          }
        }
        if (oldFilesMap != null) {
          oldFilesList.addAll(oldFilesMap.values());
          oldFilesMap=null;
        }
        int idx=0;
        while (idx < oldFilesList.size()) {
          if (oldFilesList.get(idx).delete()) {
            idx++;
          }
 else {
            oldFilesList.remove(idx);
          }
        }
      }
      try {
        Thread.sleep(60 * 1000);
      }
 catch (      InterruptedException e) {
      }
    }
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    throwable=t;
    LOG.error(""String_Node_Str"",t);
  }
}","@Override public void run(){
  Map<Long,Heritrix3JobMonitor> tmpJobMonitorMap;
  Iterator<Heritrix3JobMonitor> jobmonitorIter;
  byte[] tmpBuf=new byte[1024 * 1024];
  try {
    LOG.info(""String_Node_Str"");
    File tmpFolder=environment.tempPath;
    ;
    File[] oldFiles=tmpFolder.listFiles(new FilenameFilter(){
      @Override public boolean accept(      File dir,      String name){
        if (name.startsWith(""String_Node_Str"")) {
          if (name.endsWith(""String_Node_Str"") || name.endsWith(""String_Node_Str"")) {
            return true;
          }
        }
        return false;
      }
    }
);
    Map<String,File> oldFilesMap=new HashMap<String,File>();
    File tmpFile;
    for (int i=0; i < oldFiles.length; ++i) {
      tmpFile=oldFiles[i];
      oldFilesMap.put(tmpFile.getName(),tmpFile);
    }
    ;
    List<File> oldFilesList=new ArrayList<File>();
    while (!bExit) {
      Set<Long> runningJobs=getRunningJobs();
      if (runningJobs != null) {
        Iterator<Long> jobidIter=runningJobs.iterator();
        Heritrix3JobMonitor jobmonitor;
synchronized (runningJobMonitorMap) {
          filterJobMonitorMap.clear();
          while (jobidIter.hasNext()) {
            Long jobId=jobidIter.next();
            if (jobId != null) {
              jobmonitor=runningJobMonitorMap.remove(jobId);
              if (jobmonitor == null) {
                try {
                  jobmonitor=Heritrix3WrapperManager.getJobMonitor(jobId,environment);
                }
 catch (                IOException e) {
                }
              }
              filterJobMonitorMap.put(jobId,jobmonitor);
            }
          }
          tmpJobMonitorMap=filterJobMonitorMap;
          filterJobMonitorMap=runningJobMonitorMap;
          runningJobMonitorMap=tmpJobMonitorMap;
          LOG.debug(""String_Node_Str"" + runningJobMonitorMap.hashCode(),runningJobMonitorMap.hashCode());
        }
        jobmonitorIter=filterJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          jobmonitor.cleanup(oldFilesList);
        }
        jobmonitorIter=runningJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          if (oldFilesMap != null) {
            oldFilesMap.remove(jobmonitor.logFile.getName());
            oldFilesMap.remove(jobmonitor.idxFile.getName());
          }
          if (!jobmonitor.bInitialized) {
            jobmonitor.init();
          }
          checkH3HostnamePort(jobmonitor);
          isH3HostnamePortEnabled(jobmonitor);
          if (jobmonitor.bPull) {
            jobmonitor.updateCrawlLog(tmpBuf);
          }
        }
        if (oldFilesMap != null) {
          oldFilesList.addAll(oldFilesMap.values());
          oldFilesMap=null;
        }
        int idx=0;
        while (idx < oldFilesList.size()) {
          if (oldFilesList.get(idx).delete()) {
            idx++;
          }
 else {
            oldFilesList.remove(idx);
          }
        }
      }
      try {
        Thread.sleep(60 * 1000);
      }
 catch (      InterruptedException e) {
      }
    }
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    throwable=t;
    LOG.error(""String_Node_Str"",t);
  }
}",0.9951189749847468
89512,"public Heritrix3JobMonitor getRunningH3Job(long jobId){
  Heritrix3JobMonitor h3Job;
synchronized (runningJobMonitorMap) {
    h3Job=runningJobMonitorMap.get(jobId);
    LOG.debug(""String_Node_Str"" + runningJobMonitorMap.hashCode());
  }
  return h3Job;
}","public Heritrix3JobMonitor getRunningH3Job(long jobId){
  Heritrix3JobMonitor h3Job;
synchronized (runningJobMonitorMap) {
    h3Job=runningJobMonitorMap.get(jobId);
    LOG.debug(""String_Node_Str"" + runningJobMonitorMap.hashCode(),runningJobMonitorMap.hashCode());
  }
  return h3Job;
}",0.940959409594096
89513,"@Override public void run(){
  Map<Long,Heritrix3JobMonitor> tmpJobMonitorMap;
  Iterator<Heritrix3JobMonitor> jobmonitorIter;
  byte[] tmpBuf=new byte[1024 * 1024];
  try {
    LOG.info(""String_Node_Str"");
    File tmpFolder=environment.tempPath;
    ;
    File[] oldFiles=tmpFolder.listFiles(new FilenameFilter(){
      @Override public boolean accept(      File dir,      String name){
        if (name.startsWith(""String_Node_Str"")) {
          if (name.endsWith(""String_Node_Str"") || name.endsWith(""String_Node_Str"")) {
            return true;
          }
        }
        return false;
      }
    }
);
    Map<String,File> oldFilesMap=new HashMap<String,File>();
    File tmpFile;
    for (int i=0; i < oldFiles.length; ++i) {
      tmpFile=oldFiles[i];
      oldFilesMap.put(tmpFile.getName(),tmpFile);
    }
    ;
    List<File> oldFilesList=new ArrayList<File>();
    while (!bExit) {
      Set<Long> runningJobs=getRunningJobs();
      if (runningJobs != null) {
        Iterator<Long> jobidIter=runningJobs.iterator();
        Heritrix3JobMonitor jobmonitor;
synchronized (runningJobMonitorMap) {
          filterJobMonitorMap.clear();
          while (jobidIter.hasNext()) {
            Long jobId=jobidIter.next();
            if (jobId != null) {
              jobmonitor=runningJobMonitorMap.remove(jobId);
              if (jobmonitor == null) {
                try {
                  jobmonitor=Heritrix3WrapperManager.getJobMonitor(jobId,environment);
                }
 catch (                IOException e) {
                }
              }
              filterJobMonitorMap.put(jobId,jobmonitor);
            }
          }
          tmpJobMonitorMap=filterJobMonitorMap;
          filterJobMonitorMap=runningJobMonitorMap;
          runningJobMonitorMap=tmpJobMonitorMap;
          LOG.debug(""String_Node_Str"",runningJobMonitorMap.hashCode());
        }
        jobmonitorIter=filterJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          jobmonitor.cleanup(oldFilesList);
        }
        jobmonitorIter=runningJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          if (oldFilesMap != null) {
            oldFilesMap.remove(jobmonitor.logFile.getName());
            oldFilesMap.remove(jobmonitor.idxFile.getName());
          }
          if (!jobmonitor.bInitialized) {
            jobmonitor.init();
          }
          checkH3HostnamePort(jobmonitor);
          isH3HostnamePortEnabled(jobmonitor);
          if (jobmonitor.bPull) {
            jobmonitor.updateCrawlLog(tmpBuf);
          }
        }
        if (oldFilesMap != null) {
          oldFilesList.addAll(oldFilesMap.values());
          oldFilesMap=null;
        }
        int idx=0;
        while (idx < oldFilesList.size()) {
          if (oldFilesList.get(idx).delete()) {
            idx++;
          }
 else {
            oldFilesList.remove(idx);
          }
        }
      }
      try {
        Thread.sleep(60 * 1000);
      }
 catch (      InterruptedException e) {
      }
    }
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    throwable=t;
    LOG.error(""String_Node_Str"",t);
  }
}","@Override public void run(){
  Map<Long,Heritrix3JobMonitor> tmpJobMonitorMap;
  Iterator<Heritrix3JobMonitor> jobmonitorIter;
  byte[] tmpBuf=new byte[1024 * 1024];
  try {
    LOG.info(""String_Node_Str"");
    File tmpFolder=environment.tempPath;
    ;
    File[] oldFiles=tmpFolder.listFiles(new FilenameFilter(){
      @Override public boolean accept(      File dir,      String name){
        if (name.startsWith(""String_Node_Str"")) {
          if (name.endsWith(""String_Node_Str"") || name.endsWith(""String_Node_Str"")) {
            return true;
          }
        }
        return false;
      }
    }
);
    Map<String,File> oldFilesMap=new HashMap<String,File>();
    File tmpFile;
    for (int i=0; i < oldFiles.length; ++i) {
      tmpFile=oldFiles[i];
      oldFilesMap.put(tmpFile.getName(),tmpFile);
    }
    ;
    List<File> oldFilesList=new ArrayList<File>();
    while (!bExit) {
      Set<Long> runningJobs=getRunningJobs();
      if (runningJobs != null) {
        Iterator<Long> jobidIter=runningJobs.iterator();
        Heritrix3JobMonitor jobmonitor;
synchronized (runningJobMonitorMap) {
          filterJobMonitorMap.clear();
          while (jobidIter.hasNext()) {
            Long jobId=jobidIter.next();
            if (jobId != null) {
              jobmonitor=runningJobMonitorMap.remove(jobId);
              if (jobmonitor == null) {
                try {
                  jobmonitor=Heritrix3WrapperManager.getJobMonitor(jobId,environment);
                }
 catch (                IOException e) {
                }
              }
              filterJobMonitorMap.put(jobId,jobmonitor);
            }
          }
          tmpJobMonitorMap=filterJobMonitorMap;
          filterJobMonitorMap=runningJobMonitorMap;
          runningJobMonitorMap=tmpJobMonitorMap;
          LOG.debug(""String_Node_Str"" + runningJobMonitorMap.hashCode());
        }
        jobmonitorIter=filterJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          jobmonitor.cleanup(oldFilesList);
        }
        jobmonitorIter=runningJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          if (oldFilesMap != null) {
            oldFilesMap.remove(jobmonitor.logFile.getName());
            oldFilesMap.remove(jobmonitor.idxFile.getName());
          }
          if (!jobmonitor.bInitialized) {
            jobmonitor.init();
          }
          checkH3HostnamePort(jobmonitor);
          isH3HostnamePortEnabled(jobmonitor);
          if (jobmonitor.bPull) {
            jobmonitor.updateCrawlLog(tmpBuf);
          }
        }
        if (oldFilesMap != null) {
          oldFilesList.addAll(oldFilesMap.values());
          oldFilesMap=null;
        }
        int idx=0;
        while (idx < oldFilesList.size()) {
          if (oldFilesList.get(idx).delete()) {
            idx++;
          }
 else {
            oldFilesList.remove(idx);
          }
        }
      }
      try {
        Thread.sleep(60 * 1000);
      }
 catch (      InterruptedException e) {
      }
    }
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    throwable=t;
    LOG.error(""String_Node_Str"",t);
  }
}",0.9993866911990188
89514,"public Heritrix3JobMonitor getRunningH3Job(long jobId){
  Heritrix3JobMonitor h3Job;
synchronized (runningJobMonitorMap) {
    h3Job=runningJobMonitorMap.get(jobId);
    LOG.debug(""String_Node_Str"",runningJobMonitorMap.hashCode());
  }
  return h3Job;
}","public Heritrix3JobMonitor getRunningH3Job(long jobId){
  Heritrix3JobMonitor h3Job;
synchronized (runningJobMonitorMap) {
    h3Job=runningJobMonitorMap.get(jobId);
    LOG.debug(""String_Node_Str"" + runningJobMonitorMap.hashCode());
  }
  return h3Job;
}",0.9921259842519684
89515,"@Override public void run(){
  Map<Long,Heritrix3JobMonitor> tmpJobMonitorMap;
  Iterator<Heritrix3JobMonitor> jobmonitorIter;
  byte[] tmpBuf=new byte[1024 * 1024];
  try {
    LOG.info(""String_Node_Str"");
    File tmpFolder=environment.tempPath;
    ;
    File[] oldFiles=tmpFolder.listFiles(new FilenameFilter(){
      @Override public boolean accept(      File dir,      String name){
        if (name.startsWith(""String_Node_Str"")) {
          if (name.endsWith(""String_Node_Str"") || name.endsWith(""String_Node_Str"")) {
            return true;
          }
        }
        return false;
      }
    }
);
    Map<String,File> oldFilesMap=new HashMap<String,File>();
    File tmpFile;
    for (int i=0; i < oldFiles.length; ++i) {
      tmpFile=oldFiles[i];
      oldFilesMap.put(tmpFile.getName(),tmpFile);
    }
    ;
    List<File> oldFilesList=new ArrayList<File>();
    while (!bExit) {
      Set<Long> runningJobs=getRunningJobs();
      if (runningJobs != null) {
        Iterator<Long> jobidIter=runningJobs.iterator();
        Heritrix3JobMonitor jobmonitor;
synchronized (runningJobMonitorMap) {
          filterJobMonitorMap.clear();
          while (jobidIter.hasNext()) {
            Long jobId=jobidIter.next();
            if (jobId != null) {
              jobmonitor=runningJobMonitorMap.remove(jobId);
              if (jobmonitor == null) {
                try {
                  jobmonitor=Heritrix3WrapperManager.getJobMonitor(jobId,environment);
                }
 catch (                IOException e) {
                }
              }
              filterJobMonitorMap.put(jobId,jobmonitor);
            }
          }
          tmpJobMonitorMap=filterJobMonitorMap;
          filterJobMonitorMap=runningJobMonitorMap;
          runningJobMonitorMap=tmpJobMonitorMap;
          LOG.debug(""String_Node_Str"" + runningJobMonitorMap.hashCode(),runningJobMonitorMap.hashCode());
        }
        jobmonitorIter=filterJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          jobmonitor.cleanup(oldFilesList);
        }
        jobmonitorIter=runningJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          if (oldFilesMap != null) {
            oldFilesMap.remove(jobmonitor.logFile.getName());
            oldFilesMap.remove(jobmonitor.idxFile.getName());
          }
          if (!jobmonitor.bInitialized) {
            jobmonitor.init();
          }
          checkH3HostnamePort(jobmonitor);
          isH3HostnamePortEnabled(jobmonitor);
          if (jobmonitor.bPull) {
            jobmonitor.updateCrawlLog(tmpBuf);
          }
        }
        if (oldFilesMap != null) {
          oldFilesList.addAll(oldFilesMap.values());
          oldFilesMap=null;
        }
        int idx=0;
        while (idx < oldFilesList.size()) {
          if (oldFilesList.get(idx).delete()) {
            idx++;
          }
 else {
            oldFilesList.remove(idx);
          }
        }
      }
      try {
        Thread.sleep(60 * 1000);
      }
 catch (      InterruptedException e) {
      }
    }
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    throwable=t;
    LOG.error(""String_Node_Str"",t);
  }
}","@Override public void run(){
  Map<Long,Heritrix3JobMonitor> tmpJobMonitorMap;
  Iterator<Heritrix3JobMonitor> jobmonitorIter;
  byte[] tmpBuf=new byte[1024 * 1024];
  try {
    LOG.info(""String_Node_Str"");
    File tmpFolder=environment.tempPath;
    ;
    File[] oldFiles=tmpFolder.listFiles(new FilenameFilter(){
      @Override public boolean accept(      File dir,      String name){
        if (name.startsWith(""String_Node_Str"")) {
          if (name.endsWith(""String_Node_Str"") || name.endsWith(""String_Node_Str"")) {
            return true;
          }
        }
        return false;
      }
    }
);
    Map<String,File> oldFilesMap=new HashMap<String,File>();
    File tmpFile;
    for (int i=0; i < oldFiles.length; ++i) {
      tmpFile=oldFiles[i];
      oldFilesMap.put(tmpFile.getName(),tmpFile);
    }
    ;
    List<File> oldFilesList=new ArrayList<File>();
    while (!bExit) {
      Set<Long> runningJobs=getRunningJobs();
      if (runningJobs != null) {
        Iterator<Long> jobidIter=runningJobs.iterator();
        Heritrix3JobMonitor jobmonitor;
synchronized (runningJobMonitorMap) {
          filterJobMonitorMap.clear();
          while (jobidIter.hasNext()) {
            Long jobId=jobidIter.next();
            if (jobId != null) {
              jobmonitor=runningJobMonitorMap.remove(jobId);
              if (jobmonitor == null) {
                try {
                  jobmonitor=Heritrix3WrapperManager.getJobMonitor(jobId,environment);
                }
 catch (                IOException e) {
                }
              }
              filterJobMonitorMap.put(jobId,jobmonitor);
            }
          }
          tmpJobMonitorMap=filterJobMonitorMap;
          filterJobMonitorMap=runningJobMonitorMap;
          runningJobMonitorMap=tmpJobMonitorMap;
          LOG.debug(""String_Node_Str"",runningJobMonitorMap.hashCode());
        }
        jobmonitorIter=filterJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          jobmonitor.cleanup(oldFilesList);
        }
        jobmonitorIter=runningJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          if (oldFilesMap != null) {
            oldFilesMap.remove(jobmonitor.logFile.getName());
            oldFilesMap.remove(jobmonitor.idxFile.getName());
          }
          if (!jobmonitor.bInitialized) {
            jobmonitor.init();
          }
          checkH3HostnamePort(jobmonitor);
          isH3HostnamePortEnabled(jobmonitor);
          if (jobmonitor.bPull) {
            jobmonitor.updateCrawlLog(tmpBuf);
          }
        }
        if (oldFilesMap != null) {
          oldFilesList.addAll(oldFilesMap.values());
          oldFilesMap=null;
        }
        int idx=0;
        while (idx < oldFilesList.size()) {
          if (oldFilesList.get(idx).delete()) {
            idx++;
          }
 else {
            oldFilesList.remove(idx);
          }
        }
      }
      try {
        Thread.sleep(60 * 1000);
      }
 catch (      InterruptedException e) {
      }
    }
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    throwable=t;
    LOG.error(""String_Node_Str"",t);
  }
}",0.9948123283490998
89516,"public Heritrix3JobMonitor getRunningH3Job(long jobId){
  Heritrix3JobMonitor h3Job;
synchronized (runningJobMonitorMap) {
    h3Job=runningJobMonitorMap.get(jobId);
    LOG.debug(""String_Node_Str"" + runningJobMonitorMap.hashCode(),runningJobMonitorMap.hashCode());
  }
  return h3Job;
}","public Heritrix3JobMonitor getRunningH3Job(long jobId){
  Heritrix3JobMonitor h3Job;
synchronized (runningJobMonitorMap) {
    h3Job=runningJobMonitorMap.get(jobId);
    LOG.debug(""String_Node_Str"",runningJobMonitorMap.hashCode());
  }
  return h3Job;
}",0.9333333333333332
89517,"@Override public void run(){
  Map<Long,Heritrix3JobMonitor> tmpJobMonitorMap;
  Iterator<Heritrix3JobMonitor> jobmonitorIter;
  byte[] tmpBuf=new byte[1024 * 1024];
  try {
    LOG.info(""String_Node_Str"");
    File tmpFolder=environment.tempPath;
    ;
    File[] oldFiles=tmpFolder.listFiles(new FilenameFilter(){
      @Override public boolean accept(      File dir,      String name){
        if (name.startsWith(""String_Node_Str"")) {
          if (name.endsWith(""String_Node_Str"") || name.endsWith(""String_Node_Str"")) {
            return true;
          }
        }
        return false;
      }
    }
);
    Map<String,File> oldFilesMap=new HashMap<String,File>();
    File tmpFile;
    for (int i=0; i < oldFiles.length; ++i) {
      tmpFile=oldFiles[i];
      oldFilesMap.put(tmpFile.getName(),tmpFile);
    }
    ;
    List<File> oldFilesList=new ArrayList<File>();
    while (!bExit) {
      Set<Long> runningJobs=getRunningJobs();
      if (runningJobs != null) {
        Iterator<Long> jobidIter=runningJobs.iterator();
        Heritrix3JobMonitor jobmonitor;
synchronized (runningJobMonitorMap) {
          filterJobMonitorMap.clear();
          while (jobidIter.hasNext()) {
            Long jobId=jobidIter.next();
            if (jobId != null) {
              jobmonitor=runningJobMonitorMap.remove(jobId);
              if (jobmonitor == null) {
                try {
                  jobmonitor=Heritrix3WrapperManager.getJobMonitor(jobId,environment);
                }
 catch (                IOException e) {
                }
              }
              filterJobMonitorMap.put(jobId,jobmonitor);
            }
          }
          tmpJobMonitorMap=filterJobMonitorMap;
          filterJobMonitorMap=runningJobMonitorMap;
          runningJobMonitorMap=tmpJobMonitorMap;
          LOG.debug(""String_Node_Str"",runningJobMonitorMap.hashCode());
        }
        jobmonitorIter=filterJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          jobmonitor.cleanup(oldFilesList);
        }
        jobmonitorIter=runningJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          if (oldFilesMap != null) {
            oldFilesMap.remove(jobmonitor.logFile.getName());
            oldFilesMap.remove(jobmonitor.idxFile.getName());
          }
          if (!jobmonitor.bInitialized) {
            jobmonitor.init();
          }
          checkH3HostnamePort(jobmonitor);
          isH3HostnamePortEnabled(jobmonitor);
          if (jobmonitor.bPull) {
            jobmonitor.updateCrawlLog(tmpBuf);
          }
        }
        if (oldFilesMap != null) {
          oldFilesList.addAll(oldFilesMap.values());
          oldFilesMap=null;
        }
        int idx=0;
        while (idx < oldFilesList.size()) {
          if (oldFilesList.get(idx).delete()) {
            idx++;
          }
 else {
            oldFilesList.remove(idx);
          }
        }
      }
      try {
        Thread.sleep(60 * 1000);
      }
 catch (      InterruptedException e) {
      }
    }
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    throwable=t;
    LOG.error(""String_Node_Str"",t);
  }
}","@Override public void run(){
  Map<Long,Heritrix3JobMonitor> tmpJobMonitorMap;
  Iterator<Heritrix3JobMonitor> jobmonitorIter;
  byte[] tmpBuf=new byte[1024 * 1024];
  try {
    LOG.info(""String_Node_Str"");
    File tmpFolder=environment.tempPath;
    ;
    File[] oldFiles=tmpFolder.listFiles(new FilenameFilter(){
      @Override public boolean accept(      File dir,      String name){
        if (name.startsWith(""String_Node_Str"")) {
          if (name.endsWith(""String_Node_Str"") || name.endsWith(""String_Node_Str"")) {
            return true;
          }
        }
        return false;
      }
    }
);
    Map<String,File> oldFilesMap=new HashMap<String,File>();
    File tmpFile;
    for (int i=0; i < oldFiles.length; ++i) {
      tmpFile=oldFiles[i];
      oldFilesMap.put(tmpFile.getName(),tmpFile);
    }
    ;
    List<File> oldFilesList=new ArrayList<File>();
    while (!bExit) {
      Set<Long> runningJobs=getRunningJobs();
      if (runningJobs != null) {
        Iterator<Long> jobidIter=runningJobs.iterator();
        Heritrix3JobMonitor jobmonitor;
synchronized (runningJobMonitorMapSynchronizer) {
          filterJobMonitorMap.clear();
          while (jobidIter.hasNext()) {
            Long jobId=jobidIter.next();
            if (jobId != null) {
              jobmonitor=runningJobMonitorMap.remove(jobId);
              if (jobmonitor == null) {
                try {
                  jobmonitor=Heritrix3WrapperManager.getJobMonitor(jobId,environment);
                }
 catch (                IOException e) {
                }
              }
              filterJobMonitorMap.put(jobId,jobmonitor);
            }
          }
          tmpJobMonitorMap=filterJobMonitorMap;
          filterJobMonitorMap=runningJobMonitorMap;
          runningJobMonitorMap=tmpJobMonitorMap;
          LOG.debug(""String_Node_Str"",runningJobMonitorMap.super.hashCode());
        }
        jobmonitorIter=filterJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          jobmonitor.cleanup(oldFilesList);
        }
        jobmonitorIter=runningJobMonitorMap.values().iterator();
        while (jobmonitorIter.hasNext()) {
          jobmonitor=jobmonitorIter.next();
          if (oldFilesMap != null) {
            oldFilesMap.remove(jobmonitor.logFile.getName());
            oldFilesMap.remove(jobmonitor.idxFile.getName());
          }
          if (!jobmonitor.bInitialized) {
            jobmonitor.init();
          }
          checkH3HostnamePort(jobmonitor);
          isH3HostnamePortEnabled(jobmonitor);
          if (jobmonitor.bPull) {
            jobmonitor.updateCrawlLog(tmpBuf);
          }
        }
        if (oldFilesMap != null) {
          oldFilesList.addAll(oldFilesMap.values());
          oldFilesMap=null;
        }
        int idx=0;
        while (idx < oldFilesList.size()) {
          if (oldFilesList.get(idx).delete()) {
            idx++;
          }
 else {
            oldFilesList.remove(idx);
          }
        }
      }
      try {
        Thread.sleep(60 * 1000);
      }
 catch (      InterruptedException e) {
      }
    }
    LOG.info(""String_Node_Str"");
  }
 catch (  Throwable t) {
    throwable=t;
    LOG.error(""String_Node_Str"",t);
  }
}",0.9972468644845518
89518,"public List<Heritrix3JobMonitor> getRunningH3Jobs(){
  List<Heritrix3JobMonitor> h3JobsList=new LinkedList<Heritrix3JobMonitor>();
synchronized (runningJobMonitorMap) {
    h3JobsList.addAll(runningJobMonitorMap.values());
  }
  return h3JobsList;
}","public List<Heritrix3JobMonitor> getRunningH3Jobs(){
  List<Heritrix3JobMonitor> h3JobsList=new LinkedList<Heritrix3JobMonitor>();
synchronized (runningJobMonitorMapSynchronizer) {
    h3JobsList.addAll(runningJobMonitorMap.values());
  }
  return h3JobsList;
}",0.976470588235294
89519,"public Heritrix3JobMonitor getRunningH3Job(long jobId){
  Heritrix3JobMonitor h3Job;
synchronized (runningJobMonitorMap) {
    h3Job=runningJobMonitorMap.get(jobId);
    LOG.debug(""String_Node_Str"",runningJobMonitorMap.hashCode());
  }
  return h3Job;
}","public Heritrix3JobMonitor getRunningH3Job(long jobId){
  Heritrix3JobMonitor h3Job;
synchronized (runningJobMonitorMapSynchronizer) {
    h3Job=runningJobMonitorMap.get(jobId);
    LOG.debug(""String_Node_Str"",runningJobMonitorMap.hashCode());
  }
  return h3Job;
}",0.9768339768339768
89520,"@Test public void DSIABLED_testChecksumCalls() throws Exception {
  ArcRepository.getInstance().cleanup();
  Settings.set(CommonSettings.USE_REPLICA_ID,""String_Node_Str"");
  DISABLED_testOnBatchReply();
}","public void DSIABLED_testChecksumCalls() throws Exception {
  ArcRepository.getInstance().cleanup();
  Settings.set(CommonSettings.USE_REPLICA_ID,""String_Node_Str"");
  DISABLED_testOnBatchReply();
}",0.9850746268656716
89521,"/** 
 * Test that the OnBatchReply method updates state and responds correctly. This is a rather complex test, but should not attempt to test processCheckSum(). It has to set up the following: outstandingChecksumFiles should contain an entry replyOfId->arcfilename msg should contain id, errmsg, resultfile, filesprocessed, filesfailed, but the channels are not used. ad should contain some checksum for the arcfilename but no replyinfo -- we can check the effect by seeing warnings and state.
 * @throws Exception if exception is thrown
 */
@Test public void DISABLED_testOnBatchReply() throws Exception {
  LogbackRecorder lr=LogbackRecorder.startRecorder();
  ArcRepository a=ArcRepository.getInstance();
  UpdateableAdminData ad=UpdateableAdminData.getUpdateableInstance();
  Field ocf=a.getClass().getDeclaredField(""String_Node_Str"");
  ocf.setAccessible(true);
  Map<String,String> outstanding=(Map<String,String>)ocf.get(a);
  Field adm=ad.getClass().getSuperclass().getDeclaredField(""String_Node_Str"");
  adm.setAccessible(true);
  Map<String,ArcRepositoryEntry> admindataentries=(Map<String,ArcRepositoryEntry>)adm.get(ad);
  String id1=""String_Node_Str"";
  String arcname1=""String_Node_Str"";
  outstanding.put(id1,arcname1);
  ad.addEntry(arcname1,null,""String_Node_Str"");
  BatchReplyMessage bamsg0=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new StringRemoteFile(arcname1 + ChecksumJob.STRING_FILENAME_SEPARATOR + ""String_Node_Str""));
  JMSConnectionMockupMQ.updateMsgID(bamsg0,id1);
  a.onBatchReply(bamsg0);
  lr.assertLogNotContains(""String_Node_Str"",""String_Node_Str"");
  assertEquals(""String_Node_Str"",ReplicaStoreState.UPLOAD_COMPLETED,ad.getState(arcname1,Channels.getTheBamon().getName()));
  outstanding.put(id1,arcname1);
  ad.addEntry(arcname1,null,""String_Node_Str"");
  BatchReplyMessage bamsg2=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new NullRemoteFile());
  JMSConnectionMockupMQ.updateMsgID(bamsg2,id1);
  bamsg2.setNotOk(""String_Node_Str"");
  a.onBatchReply(bamsg2);
  lr.assertLogContains(""String_Node_Str"",""String_Node_Str"");
  assertEquals(""String_Node_Str"",ReplicaStoreState.UPLOAD_FAILED,ad.getState(arcname1,Channels.getTheBamon().getName()));
  admindataentries.remove(arcname1);
  outstanding.put(id1,arcname1);
  BatchReplyMessage bamsg3=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new NullRemoteFile());
  JMSConnectionMockupMQ.updateMsgID(bamsg3,id1);
  bamsg3.setNotOk(""String_Node_Str"");
  try {
    a.onBatchReply(bamsg3);
    fail(""String_Node_Str"" + arcname1);
  }
 catch (  UnknownID e) {
    StringAsserts.assertStringContains(""String_Node_Str"" + ""String_Node_Str"",arcname1,e.getMessage());
  }
  lr.assertLogContains(""String_Node_Str"",""String_Node_Str"");
  assertFalse(""String_Node_Str"",ad.hasEntry(arcname1));
  BatchReplyMessage bamsg1=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new NullRemoteFile());
  a.onBatchReply(bamsg1);
  lr.assertLogContains(""String_Node_Str"",""String_Node_Str"" + id1);
  assertFalse(""String_Node_Str"",ad.hasEntry(arcname1));
  lr.stopRecorder();
}","/** 
 * DISABLED because it fails on travis constantly! Test that the OnBatchReply method updates state and responds correctly. This is a rather complex test, but should not attempt to test processCheckSum(). It has to set up the following: outstandingChecksumFiles should contain an entry replyOfId->arcfilename msg should contain id, errmsg, resultfile, filesprocessed, filesfailed, but the channels are not used. ad should contain some checksum for the arcfilename but no replyinfo -- we can check the effect by seeing warnings and state.
 * @throws Exception if exception is thrown
 */
public void DISABLED_testOnBatchReply() throws Exception {
  LogbackRecorder lr=LogbackRecorder.startRecorder();
  ArcRepository a=ArcRepository.getInstance();
  UpdateableAdminData ad=UpdateableAdminData.getUpdateableInstance();
  Field ocf=a.getClass().getDeclaredField(""String_Node_Str"");
  ocf.setAccessible(true);
  Map<String,String> outstanding=(Map<String,String>)ocf.get(a);
  Field adm=ad.getClass().getSuperclass().getDeclaredField(""String_Node_Str"");
  adm.setAccessible(true);
  Map<String,ArcRepositoryEntry> admindataentries=(Map<String,ArcRepositoryEntry>)adm.get(ad);
  String id1=""String_Node_Str"";
  String arcname1=""String_Node_Str"";
  outstanding.put(id1,arcname1);
  ad.addEntry(arcname1,null,""String_Node_Str"");
  BatchReplyMessage bamsg0=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new StringRemoteFile(arcname1 + ChecksumJob.STRING_FILENAME_SEPARATOR + ""String_Node_Str""));
  JMSConnectionMockupMQ.updateMsgID(bamsg0,id1);
  a.onBatchReply(bamsg0);
  lr.assertLogNotContains(""String_Node_Str"",""String_Node_Str"");
  assertEquals(""String_Node_Str"",ReplicaStoreState.UPLOAD_COMPLETED,ad.getState(arcname1,Channels.getTheBamon().getName()));
  outstanding.put(id1,arcname1);
  ad.addEntry(arcname1,null,""String_Node_Str"");
  BatchReplyMessage bamsg2=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new NullRemoteFile());
  JMSConnectionMockupMQ.updateMsgID(bamsg2,id1);
  bamsg2.setNotOk(""String_Node_Str"");
  a.onBatchReply(bamsg2);
  lr.assertLogContains(""String_Node_Str"",""String_Node_Str"");
  assertEquals(""String_Node_Str"",ReplicaStoreState.UPLOAD_FAILED,ad.getState(arcname1,Channels.getTheBamon().getName()));
  admindataentries.remove(arcname1);
  outstanding.put(id1,arcname1);
  BatchReplyMessage bamsg3=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new NullRemoteFile());
  JMSConnectionMockupMQ.updateMsgID(bamsg3,id1);
  bamsg3.setNotOk(""String_Node_Str"");
  try {
    a.onBatchReply(bamsg3);
    fail(""String_Node_Str"" + arcname1);
  }
 catch (  UnknownID e) {
    StringAsserts.assertStringContains(""String_Node_Str"" + ""String_Node_Str"",arcname1,e.getMessage());
  }
  lr.assertLogContains(""String_Node_Str"",""String_Node_Str"");
  assertFalse(""String_Node_Str"",ad.hasEntry(arcname1));
  BatchReplyMessage bamsg1=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new NullRemoteFile());
  a.onBatchReply(bamsg1);
  lr.assertLogContains(""String_Node_Str"",""String_Node_Str"" + id1);
  assertFalse(""String_Node_Str"",ad.hasEntry(arcname1));
  lr.stopRecorder();
}",0.991717791411043
89522,"/** 
 * Return the directory name of this site section.
 * @return The dirname.
 */
public String getDirname(){
  return dirname;
}","/** 
 * Return the directory name of this site section.
 * @return The dirname.
 */
public String getDirname(){
}",0.9262295081967212
89523,"/** 
 * Generate this section's part of the navigation tree (sidebar). This outputs balanced HTML to the JspWriter. It uses a locale to generate the right titles.
 * @param out A place to write our HTML
 * @param url The url of the page we're currently viewing. The list of subpages will only be displayed if the pagewe're viewing is one that belongs to this section.
 * @param locale The locale to generate the navigation tree for.
 * @throws IOException If there is a problem writing to the page.
 */
public void generateNavigationTree(JspWriter out,String url,Locale locale) throws IOException {
  String firstPage=pagesAndTitles.keySet().iterator().next();
  out.print(""String_Node_Str"");
  out.print(""String_Node_Str"" + HTMLUtils.encode(dirname) + ""String_Node_Str""+ HTMLUtils.encode(firstPage)+ ""String_Node_Str""+ HTMLUtils.escapeHtmlValues(I18n.getString(bundle,locale,mainname))+ ""String_Node_Str"");
  out.print(""String_Node_Str"");
  String page=getPage(url);
  if (page == null) {
    return;
  }
  if (pagesAndTitles.containsKey(page)) {
    int i=0;
    for (    Map.Entry<String,String> pageAndTitle : pagesAndTitles.entrySet()) {
      if (i == visiblePages) {
        break;
      }
      out.print(""String_Node_Str"");
      out.print(""String_Node_Str"" + HTMLUtils.encode(dirname) + ""String_Node_Str""+ HTMLUtils.encode(pageAndTitle.getKey())+ ""String_Node_Str""+ HTMLUtils.escapeHtmlValues(I18n.getString(bundle,locale,pageAndTitle.getValue()))+ ""String_Node_Str"");
      out.print(""String_Node_Str"");
      i++;
    }
    if (this.getClass().getName().equalsIgnoreCase(""String_Node_Str"")) {
      out.print(""String_Node_Str"");
      out.print(""String_Node_Str"" + HTMLUtils.encode(dirname) + ""String_Node_Str""+ HTMLUtils.encode(""String_Node_Str"")+ ""String_Node_Str""+ HTMLUtils.escapeHtmlValues(I18n.getString(bundle,locale,""String_Node_Str""))+ ""String_Node_Str"");
      out.print(""String_Node_Str"");
    }
  }
}","/** 
 * Generate this section's part of the navigation tree (sidebar). This outputs balanced HTML to the JspWriter. It uses a locale to generate the right titles.
 * @param out A place to write our HTML
 * @param url The url of the page we're currently viewing. The list of subpages will only be displayed if the pagewe're viewing is one that belongs to this section.
 * @param locale The locale to generate the navigation tree for.
 * @throws IOException If there is a problem writing to the page.
 */
public void generateNavigationTree(JspWriter out,String url,Locale locale) throws IOException {
  String firstPage=pagesAndTitles.keySet().iterator().next();
  out.print(""String_Node_Str"");
  out.print(""String_Node_Str"" + HTMLUtils.encode(dirname) + ""String_Node_Str""+ HTMLUtils.encode(firstPage)+ ""String_Node_Str""+ HTMLUtils.escapeHtmlValues(I18n.getString(bundle,locale,mainname))+ ""String_Node_Str"");
  out.print(""String_Node_Str"");
  String page=getPage(url);
  if (page == null) {
    return;
  }
  if (pagesAndTitles.containsKey(page)) {
    int i=0;
    for (    Map.Entry<String,String> pageAndTitle : pagesAndTitles.entrySet()) {
      if (i == visiblePages) {
        break;
      }
      out.print(""String_Node_Str"");
      out.print(""String_Node_Str"" + HTMLUtils.encode(dirname) + ""String_Node_Str""+ pageAndTitle.getKey()+ ""String_Node_Str""+ HTMLUtils.escapeHtmlValues(I18n.getString(bundle,locale,pageAndTitle.getValue()))+ ""String_Node_Str"");
      out.print(""String_Node_Str"");
      i++;
    }
  }
}",0.8579726982282893
89524,"/** 
 * Create a new SiteSection object.
 * @param mainname The name of the entire section used in the sidebar.
 * @param prefix The prefix that all the JSP pages will have.
 * @param visiblePages How many of the pages will be visible in the menu (taken from the start of the list).
 * @param pagesAndTitles The actual pages and title-labels, without the prefix and jsp extension, involved in thesection. They must be given as an array of 2-element arrays.
 * @param dirname The top level directory this site section is deployed under.
 * @param bundle The resource bundle with translations of this sitesection.
 * @throws ArgumentNotValid if any of the elements of pagesAndTitles are not a 2-element array.
 */
public SiteSection(String mainname,String prefix,int visiblePages,String[][] pagesAndTitles,String dirname,String bundle){
  ArgumentNotValid.checkNotNullOrEmpty(mainname,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(prefix,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(visiblePages,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(pagesAndTitles,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(dirname,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(bundle,""String_Node_Str"");
  this.dirname=dirname;
  this.mainname=mainname;
  this.visiblePages=visiblePages;
  this.bundle=bundle;
  for (  String[] pageAndTitle : pagesAndTitles) {
    if (pageAndTitle.length != 2) {
      throw new ArgumentNotValid(""String_Node_Str"" + prefix);
    }
    this.pagesAndTitles.put(prefix + ""String_Node_Str"" + pageAndTitle[0]+ JSP_EXTENSION,pageAndTitle[1]);
  }
}","/** 
 * Create a new SiteSection object.
 * @param mainname The name of the entire section used in the sidebar.
 * @param prefix The prefix that all the JSP pages will have.
 * @param visiblePages How many of the pages will be visible in the menu (taken from the start of the list).
 * @param pagesAndTitles The actual pages and title-labels, without the prefix and jsp extension, involved in thesection. They must be given as an array of 2-element arrays.
 * @param dirname The top level directory this site section is deployed under.
 * @param bundle The resource bundle with translations of this sitesection.
 * @throws ArgumentNotValid if any of the elements of pagesAndTitles are not a 2-element array.
 */
public SiteSection(String mainname,String prefix,int visiblePages,String[][] pagesAndTitles,String dirname,String bundle){
  ArgumentNotValid.checkNotNullOrEmpty(mainname,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(prefix,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(visiblePages,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(pagesAndTitles,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(dirname,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(bundle,""String_Node_Str"");
  this.dirname=dirname;
  this.mainname=mainname;
  this.visiblePages=visiblePages;
  this.bundle=bundle;
  for (  String[] pageAndTitle : pagesAndTitles) {
    if (pageAndTitle.length != 2) {
      throw new ArgumentNotValid(""String_Node_Str"" + prefix);
    }
    String pageurl=prefix + ""String_Node_Str"" + pageAndTitle[0]+ JSP_EXTENSION;
    if (pageAndTitle[0].endsWith(""String_Node_Str"")) {
      pageurl=pageAndTitle[0];
    }
    String pagelabel=pageAndTitle[1];
    this.pagesAndTitles.put(pageurl,pagelabel);
  }
}",0.9398451459201906
89525,"/** 
 * Create a new history SiteSection object.
 */
public HistorySiteSection(){
  super(""String_Node_Str"",""String_Node_Str"",2,new String[][]{{""String_Node_Str"",""String_Node_Str""},{""String_Node_Str"",""String_Node_Str""},{""String_Node_Str"",""String_Node_Str""},{""String_Node_Str"",""String_Node_Str""},{""String_Node_Str"",""String_Node_Str""},{""String_Node_Str"",""String_Node_Str""},{""String_Node_Str"",""String_Node_Str""},{""String_Node_Str"",""String_Node_Str""}},""String_Node_Str"",dk.netarkivet.harvester.Constants.TRANSLATIONS_BUNDLE);
}","/** 
 * Create a new history SiteSection object.
 */
public HistorySiteSection(){
  super(""String_Node_Str"",""String_Node_Str"",3,new String[][]{{""String_Node_Str"",""String_Node_Str""},{""String_Node_Str"",""String_Node_Str""},{""String_Node_Str"",""String_Node_Str""},{""String_Node_Str"",""String_Node_Str""},{""String_Node_Str"",""String_Node_Str""},{""String_Node_Str"",""String_Node_Str""},{""String_Node_Str"",""String_Node_Str""},{""String_Node_Str"",""String_Node_Str""},{""String_Node_Str"",""String_Node_Str""}},""String_Node_Str"",dk.netarkivet.harvester.Constants.TRANSLATIONS_BUNDLE);
}",0.96309963099631
89526,"/** 
 * Initialises a GUI Web Server and adds web applications.
 * @throws IOFailure on trouble starting server.
 */
public GUIWebServer(){
  int port=Integer.parseInt(Settings.get(CommonSettings.HTTP_PORT_NUMBER));
  if (port < HTTP_PORT_NUMBER_LOWER_LIMIT || port > HTTP_PORT_NUMBER_UPPER_LIMIT) {
    throw new IOFailure(""String_Node_Str"" + HTTP_PORT_NUMBER_LOWER_LIMIT + ""String_Node_Str""+ HTTP_PORT_NUMBER_UPPER_LIMIT+ ""String_Node_Str""+ port);
  }
  String[] webApps=Settings.getAll(CommonSettings.SITESECTION_WEBAPPLICATION);
  String[] classes=Settings.getAll(CommonSettings.SITESECTION_CLASS);
  if (webApps.length != classes.length) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",webApps) + ""String_Node_Str""+ ""String_Node_Str""+ StringUtils.conjoin(""String_Node_Str"",classes)+ ""String_Node_Str"");
  }
  log.info(""String_Node_Str"" + port + ""String_Node_Str""+ StringUtils.conjoin(""String_Node_Str"",webApps)+ ""String_Node_Str""+ StringUtils.conjoin(""String_Node_Str"",classes)+ ""String_Node_Str"");
  server=new Tomcat();
  System.setProperty(""String_Node_Str"",""String_Node_Str"");
  final File tempDir=FileUtils.getTempDir();
  log.debug(""String_Node_Str"" + tempDir);
  File basedir=tempDir.getAbsoluteFile().getParentFile();
  log.debug(""String_Node_Str"" + basedir);
  server.setBaseDir(basedir.getAbsolutePath());
  File webapps=new File(basedir,""String_Node_Str"");
  if (webapps.exists()) {
    FileUtils.removeRecursively(webapps);
    log.info(""String_Node_Str"" + webapps.getAbsolutePath() + ""String_Node_Str"");
  }
  webapps.mkdirs();
  server.setPort(port);
  boolean taglibsScanningDisabled=false;
  if (System.getProperty(Constants.SKIP_JARS_PROPERTY) == null) {
    log.info(""String_Node_Str"" + Constants.SKIP_JARS_PROPERTY + ""String_Node_Str"");
    taglibsScanningDisabled=true;
  }
  for (  String webappFilename : webApps) {
    String webbase=""String_Node_Str"" + webappFilename;
    final String warSuffix=""String_Node_Str"";
    if (webappFilename.toLowerCase().endsWith(warSuffix)) {
      webbase=""String_Node_Str"" + webappFilename.substring(0,webappFilename.length() - warSuffix.length());
    }
    for (    SiteSection section : SiteSection.getSections()) {
      if (webbase.equals(""String_Node_Str"" + section.getDirname())) {
        section.initialize();
        break;
      }
    }
    try {
      String warfile=new File(basedir,webappFilename).getAbsolutePath();
      StandardContext ctx=(StandardContext)server.addWebapp(webbase,warfile);
      if (taglibsScanningDisabled) {
        StandardJarScanFilter jarScanFilter=(StandardJarScanFilter)ctx.getJarScanner().getJarScanFilter();
        jarScanFilter.setTldSkip(""String_Node_Str"");
      }
      if (webappFilename.equals(webApps[0])) {
        StandardContext rootCtx=(StandardContext)server.addWebapp(""String_Node_Str"",warfile);
        if (taglibsScanningDisabled) {
          StandardJarScanFilter jarScanFilter=(StandardJarScanFilter)rootCtx.getJarScanner().getJarScanFilter();
          jarScanFilter.setTldSkip(""String_Node_Str"");
        }
      }
    }
 catch (    ServletException e) {
      log.error(""String_Node_Str"" + webappFilename,e);
    }
  }
}","/** 
 * Initialises a GUI Web Server and adds web applications.
 * @throws IOFailure on trouble starting server.
 */
public GUIWebServer(){
  int port=Integer.parseInt(Settings.get(CommonSettings.HTTP_PORT_NUMBER));
  if (port < HTTP_PORT_NUMBER_LOWER_LIMIT || port > HTTP_PORT_NUMBER_UPPER_LIMIT) {
    throw new IOFailure(""String_Node_Str"" + HTTP_PORT_NUMBER_LOWER_LIMIT + ""String_Node_Str""+ HTTP_PORT_NUMBER_UPPER_LIMIT+ ""String_Node_Str""+ port);
  }
  String[] webApps=Settings.getAll(CommonSettings.SITESECTION_WEBAPPLICATION);
  String[] classes=Settings.getAll(CommonSettings.SITESECTION_CLASS);
  if (webApps.length != classes.length) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",webApps) + ""String_Node_Str""+ ""String_Node_Str""+ StringUtils.conjoin(""String_Node_Str"",classes)+ ""String_Node_Str"");
  }
  log.info(""String_Node_Str"" + port + ""String_Node_Str""+ StringUtils.conjoin(""String_Node_Str"",webApps)+ ""String_Node_Str""+ StringUtils.conjoin(""String_Node_Str"",classes)+ ""String_Node_Str"");
  server=new Tomcat();
  System.setProperty(""String_Node_Str"",""String_Node_Str"");
  final File tempDir=FileUtils.getTempDir();
  log.debug(""String_Node_Str"" + tempDir);
  File basedir=tempDir.getAbsoluteFile().getParentFile();
  log.debug(""String_Node_Str"" + basedir);
  server.setBaseDir(basedir.getAbsolutePath());
  File webapps=new File(basedir,""String_Node_Str"");
  if (webapps.exists()) {
    FileUtils.removeRecursively(webapps);
    log.info(""String_Node_Str"" + webapps.getAbsolutePath() + ""String_Node_Str"");
  }
  webapps.mkdirs();
  server.setPort(port);
  boolean taglibsScanningDisabled=false;
  if (System.getProperty(Constants.SKIP_JARS_PROPERTY) == null) {
    log.info(""String_Node_Str"" + Constants.SKIP_JARS_PROPERTY + ""String_Node_Str"");
    taglibsScanningDisabled=true;
  }
  for (  String webapp : webApps) {
    String webappFilename=new File(webapp).getName();
    String webbase=""String_Node_Str"" + webappFilename;
    final String warSuffix=""String_Node_Str"";
    if (webappFilename.toLowerCase().endsWith(warSuffix)) {
      webbase=""String_Node_Str"" + webappFilename.substring(0,webappFilename.length() - warSuffix.length());
    }
    for (    SiteSection section : SiteSection.getSections()) {
      if (webbase.equals(""String_Node_Str"" + section.getDirname())) {
        section.initialize();
        break;
      }
    }
    try {
      String warfile=new File(basedir,webapp).getAbsolutePath();
      log.info(""String_Node_Str"",webbase,warfile);
      StandardContext ctx=(StandardContext)server.addWebapp(webbase,warfile);
      if (taglibsScanningDisabled) {
        StandardJarScanFilter jarScanFilter=(StandardJarScanFilter)ctx.getJarScanner().getJarScanFilter();
        jarScanFilter.setTldSkip(""String_Node_Str"");
      }
      if (webappFilename.equals(webApps[0])) {
        StandardContext rootCtx=(StandardContext)server.addWebapp(""String_Node_Str"",warfile);
        if (taglibsScanningDisabled) {
          StandardJarScanFilter jarScanFilter=(StandardJarScanFilter)rootCtx.getJarScanner().getJarScanFilter();
          jarScanFilter.setTldSkip(""String_Node_Str"");
        }
      }
    }
 catch (    ServletException e) {
      log.error(""String_Node_Str"" + webapp,e);
    }
  }
}",0.9787168886847344
89527,"/** 
 * Return a key for queue names based on domain names (last two parts of host name) or IP address.  They key may include a #<portnr> at the end.
 * @param cauri A potential URI.
 * @return a class key (really an arbitrary string), one of <domainOrIP>,<domainOrIP>#<port>, or ""default..."".
 * @see HostnameQueueAssignmentPolicy#getClassKey(CrawlURI)
 */
public String getClassKey(CrawlURI cauri){
  String candidate;
  boolean ignoreSourceSeed=cauri != null && cauri.getCanonicalString().startsWith(""String_Node_Str"");
  try {
    candidate=super.getClassKey(cauri);
  }
 catch (  NullPointerException e) {
    log.debug(""String_Node_Str"" + cauri);
    candidate=DEFAULT_CLASS_KEY;
  }
  String sourceSeedCandidate=null;
  if (!ignoreSourceSeed) {
    sourceSeedCandidate=getCandidateFromSource(cauri);
  }
  if (sourceSeedCandidate != null) {
    return sourceSeedCandidate;
  }
 else {
    String[] hostnameandportnr=candidate.split(""String_Node_Str"");
    if (hostnameandportnr.length == 0 || hostnameandportnr.length > 2) {
      return candidate;
    }
    String domainName=DomainUtils.domainNameFromHostname(hostnameandportnr[0]);
    if (domainName == null) {
      log.debug(""String_Node_Str"" + candidate + ""String_Node_Str""+ cauri+ ""String_Node_Str"");
      return candidate;
    }
    return domainName;
  }
}","/** 
 * Return a key for queue names based on domain names (last two parts of host name) or IP address.  They key may include a #<portnr> at the end.
 * @param cauri A potential URI.
 * @return a class key (really an arbitrary string), one of <domainOrIP>,<domainOrIP>#<port>, or ""default..."".
 * @see HostnameQueueAssignmentPolicy#getClassKey(CrawlURI)
 */
public String getClassKey(CrawlURI cauri){
  String candidate;
  boolean ignoreSourceSeed=cauri != null;
  try {
    candidate=super.getClassKey(cauri);
  }
 catch (  NullPointerException e) {
    log.debug(""String_Node_Str"" + cauri);
    candidate=DEFAULT_CLASS_KEY;
  }
  String sourceSeedCandidate=null;
  if (!ignoreSourceSeed) {
    sourceSeedCandidate=getCandidateFromSource(cauri);
  }
  if (sourceSeedCandidate != null) {
    return sourceSeedCandidate;
  }
 else {
    String[] hostnameandportnr=candidate.split(""String_Node_Str"");
    if (hostnameandportnr.length == 0 || hostnameandportnr.length > 2) {
      return candidate;
    }
    String domainName=DomainUtils.domainNameFromHostname(hostnameandportnr[0]);
    if (domainName == null) {
      log.debug(""String_Node_Str"" + candidate + ""String_Node_Str""+ cauri+ ""String_Node_Str"");
      return candidate;
    }
    return domainName;
  }
}",0.9768160741885626
89528,"/** 
 * Find a candidate from the source.
 * @param cauri A potential URI
 * @return a candidate from the source or null if none found
 */
private String getCandidateFromSource(CrawlURI cauri){
  String sourceCandidate=null;
  try {
    sourceCandidate=cauri.getSourceTag();
  }
 catch (  NoSuchElementException e) {
    log.warn(""String_Node_Str"");
    return null;
  }
  String hostname=null;
  try {
    hostname=UURIFactory.getInstance(sourceCandidate).getHost();
  }
 catch (  URIException e) {
    log.warn(""String_Node_Str"" + sourceCandidate);
    return null;
  }
  return DomainUtils.domainNameFromHostname(hostname);
}","/** 
 * Find a candidate from the source.
 * @param cauri A potential URI
 * @return a candidate from the source or null if none found
 */
private String getCandidateFromSource(CrawlURI cauri){
  String sourceCandidate=null;
  try {
    sourceCandidate=cauri.getSourceTag();
  }
 catch (  NoSuchElementException e) {
    log.warn(""String_Node_Str"");
    return null;
  }
  String hostname=null;
  try {
    hostname=UURIFactory.getInstance(sourceCandidate).getHost();
  }
 catch (  URIException e) {
    log.warn(""String_Node_Str"" + sourceCandidate);
    return null;
  }
  String candidateKey=DomainUtils.domainNameFromHostname(hostname);
  return candidateKey;
}",0.9613003095975232
89529,"/** 
 * Generates a name for an archive(ARC/WARC) file containing metadata regarding a given job.
 * @param jobID The number of the job that generated the archive file.
 * @return A ""flat"" file name (i.e. no path) containing the jobID parameter and ending on ""-metadata-N.(w)arc"",where N is the serial number of the metadata files for this job, e.g. ""42-metadata-1.(w)arc"". Currently, only one file is ever made.
 * @throws ArgumentNotValid if any parameter was null.
 */
public static String getMetadataArchiveFileName(String jobID) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(jobID,""String_Node_Str"");
  if (metadataFormat == 0) {
    initializeMetadataFormat();
  }
  boolean compressionOn=compressRecords();
  String possibleGzSuffix=""String_Node_Str"";
  if (compressionOn) {
    possibleGzSuffix=""String_Node_Str"";
  }
switch (metadataFormat) {
case MDF_ARC:
    return jobID + ""String_Node_Str"" + 1+ ""String_Node_Str""+ possibleGzSuffix;
case MDF_WARC:
  return jobID + ""String_Node_Str"" + 1+ ""String_Node_Str""+ possibleGzSuffix;
default :
throw new ArgumentNotValid(""String_Node_Str"" + HarvesterSettings.METADATA_FORMAT + ""String_Node_Str"");
}
}","/** 
 * Generates a name for an archive(ARC/WARC) file containing metadata regarding a given job.
 * @param jobID The number of the job that generated the archive file.
 * @return A ""flat"" file name (i.e. no path) containing the jobID parameter and ending on ""-metadata-N.(w)arc"",where N is the serial number of the metadata files for this job, e.g. ""42-metadata-1.(w)arc"". Currently, only one file is ever made.
 * @throws ArgumentNotValid if any parameter was null.
 */
public static String getMetadataArchiveFileName(String jobID,Long harvestID) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(jobID,""String_Node_Str"");
  String collectionName=""String_Node_Str"";
  boolean isPrefix=false;
  if (""String_Node_Str"".equals(Settings.get(HarvesterSettings.METADATA_FILENAME_FORMAT))) {
    try {
      collectionName=Settings.get(HarvesterSettings.HERITRIX_METADATA_PREFIX_COLLECTION_NAME);
      if (collectionName == null || collectionName.length() == 0) {
        collectionName=Settings.get(HarvesterSettings.HERITRIX3_METADATA_PREFIX_COLLECTION_NAME);
      }
      isPrefix=true;
    }
 catch (    UnknownID e) {
    }
  }
  if (metadataFormat == 0) {
    initializeMetadataFormat();
  }
  boolean compressionOn=compressRecords();
  String possibleGzSuffix=""String_Node_Str"";
  if (compressionOn) {
    possibleGzSuffix=""String_Node_Str"";
  }
switch (metadataFormat) {
case MDF_ARC:
    if (isPrefix) {
      return collectionName + ""String_Node_Str"" + jobID+ ""String_Node_Str""+ harvestID+ ""String_Node_Str""+ 1+ possibleGzSuffix;
    }
 else {
      return jobID + ""String_Node_Str"" + 1+ ""String_Node_Str""+ possibleGzSuffix;
    }
case MDF_WARC:
  if (isPrefix) {
    return collectionName + ""String_Node_Str"" + jobID+ ""String_Node_Str""+ harvestID+ ""String_Node_Str""+ 1+ possibleGzSuffix;
  }
 else {
    return jobID + ""String_Node_Str"" + 1+ ""String_Node_Str""+ possibleGzSuffix;
  }
default :
throw new ArgumentNotValid(""String_Node_Str"" + HarvesterSettings.METADATA_FORMAT + ""String_Node_Str"");
}
}",0.4252909720037747
89530,"/** 
 * Constructs the single metadata arc file from the crawlDir and the jobID.
 * @return metadata arc file as a File
 */
protected File getMetadataFile(){
  return new File(getMetadataDir(),MetadataFileWriter.getMetadataArchiveFileName(Long.toString(jobId)));
}","/** 
 * Constructs the single metadata arc file from the crawlDir and the jobID.
 * @return metadata arc file as a File
 */
protected File getMetadataFile(){
  return new File(getMetadataDir(),MetadataFileWriter.getMetadataArchiveFileName(Long.toString(jobId),harvestId));
}",0.9814126394052044
89531,"/** 
 * Constructs the TEMPORARY metadata arc file from the crawlDir and the jobID.
 * @return tmp-metadata arc file as a File
 */
private File getTmpMetadataFile(){
  return new File(getTmpMetadataDir(),MetadataFileWriter.getMetadataArchiveFileName(Long.toString(jobId)));
}","/** 
 * Constructs the TEMPORARY metadata arc file from the crawlDir and the jobID.
 * @return tmp-metadata arc file as a File
 */
private File getTmpMetadataFile(){
  return new File(getTmpMetadataDir(),MetadataFileWriter.getMetadataArchiveFileName(Long.toString(jobId),harvestId));
}",0.9821428571428572
89532,"/** 
 * @param name Name of this writer.
 */
public WARCWriterProcessor(final String name){
  super(name,""String_Node_Str"");
  Type e=addElementToDefinition(new SimpleType(ATTR_WRITE_REQUESTS,""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new SimpleType(ATTR_WRITE_METADATA,""String_Node_Str"",new Boolean(true)));
  e=addElementToDefinition(new SimpleType(ATTR_WRITE_METADATA_OUTLINKS,""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new SimpleType(ATTR_WRITE_REVISIT_FOR_IDENTICAL_DIGESTS,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new SimpleType(ATTR_WRITE_REVISIT_FOR_NOT_MODIFIED,""String_Node_Str"" + ""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new MapType(ATTR_METADATA_ITEMS,""String_Node_Str"",String.class));
  e.setOverrideable(true);
  e.setExpertSetting(true);
}","/** 
 * @param name Name of this writer.
 */
public WARCWriterProcessor(final String name){
  super(name,""String_Node_Str"");
  Type e=addElementToDefinition(new SimpleType(ATTR_WRITE_REQUESTS,""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new SimpleType(ATTR_WRITE_METADATA,""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new SimpleType(ATTR_WRITE_METADATA_OUTLINKS,""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new SimpleType(ATTR_WRITE_REVISIT_FOR_IDENTICAL_DIGESTS,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new SimpleType(ATTR_WRITE_REVISIT_FOR_NOT_MODIFIED,""String_Node_Str"" + ""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new MapType(ATTR_METADATA_ITEMS,""String_Node_Str"",String.class));
  e.setOverrideable(true);
  e.setExpertSetting(true);
}",0.9762213575443148
89533,"/** 
 * Constructs the single metadata arc file from the crawlDir and the jobID.
 * @return metadata arc file as a File
 */
protected File getMetadataFile(){
  return new File(getMetadataDir(),MetadataFileWriter.getMetadataArchiveFileName(Long.toString(jobId)));
}","/** 
 * Constructs the single metadata arc file from the crawlDir and the jobID.
 * @return metadata arc file as a File
 */
protected File getMetadataFile(){
  return new File(getMetadataDir(),MetadataFileWriter.getMetadataArchiveFileName(Long.toString(jobId),harvestId));
}",0.9814126394052044
89534,"/** 
 * Constructs the TEMPORARY metadata arc file from the crawlDir and the jobID.
 * @return tmp-metadata arc file as a File
 */
private File getTmpMetadataFile(){
  return new File(getTmpMetadataDir(),MetadataFileWriter.getMetadataArchiveFileName(Long.toString(jobId)));
}","/** 
 * Constructs the TEMPORARY metadata arc file from the crawlDir and the jobID.
 * @return tmp-metadata arc file as a File
 */
private File getTmpMetadataFile(){
  return new File(getTmpMetadataDir(),MetadataFileWriter.getMetadataArchiveFileName(Long.toString(jobId),harvestId));
}",0.9821428571428572
89535,"/** 
 * Insert WARC-archiver beans and remove placeholder for ARC-Archiver-beans It is an error, if the WARC place-holders doesnt't exist. It is not an error, if the property placeholder does not exist.
 */
private void setWarcArchiveformat(){
  String warcWriterbeanReference=""String_Node_Str"";
  String warcWriterProcessorBean=""String_Node_Str"";
  String propertyName=""String_Node_Str"";
  String valuePrefix=""String_Node_Str"";
  String valueSuffix=""String_Node_Str"";
  String propertyEnd=""String_Node_Str"";
  if (!template.contains(ARCHIVER_BEAN_REFERENCE_PLACEHOLDER)) {
    throw new IllegalState(""String_Node_Str"" + ARCHIVER_BEAN_REFERENCE_PLACEHOLDER + ""String_Node_Str"");
  }
  if (!template.contains(ARCHIVER_PROCESSOR_BEAN_PLACEHOLDER)) {
    throw new IllegalState(""String_Node_Str"" + ARCHIVER_PROCESSOR_BEAN_PLACEHOLDER + ""String_Node_Str"");
  }
  StringBuilder propertyBuilder=new StringBuilder();
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_TEMPLATE)+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_COMPRESSION)+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ ARCHIVE_FILE_PREFIX_PLACEHOLDER+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_MAXSIZE)+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_POOL_MAXACTIVE)+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_WRITE_REQUESTS)+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_WRITE_METADATA)+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_SKIP_IDENTICAL_DIGESTS)+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_START_NEW_FILES_ON_CHECKPOINT)+ valueSuffix+ propertyEnd);
  warcWriterProcessorBean+=propertyBuilder.toString();
  warcWriterProcessorBean+=""String_Node_Str"";
  String templateNew=template.replace(ARCHIVER_BEAN_REFERENCE_PLACEHOLDER,warcWriterbeanReference);
  this.template=templateNew.replace(ARCHIVER_PROCESSOR_BEAN_PLACEHOLDER,warcWriterProcessorBean);
}","/** 
 * Insert WARC-archiver beans and remove placeholder for ARC-Archiver-beans It is an error, if the WARC place-holders doesnt't exist. It is not an error, if the property placeholder does not exist.
 */
private void setWarcArchiveformat(){
  String warcWriterbeanReference=""String_Node_Str"";
  String warcWriterProcessorBean=""String_Node_Str"";
  String propertyName=""String_Node_Str"";
  String valuePrefix=""String_Node_Str"";
  String valueSuffix=""String_Node_Str"";
  String propertyEnd=""String_Node_Str"";
  if (!template.contains(ARCHIVER_BEAN_REFERENCE_PLACEHOLDER)) {
    throw new IllegalState(""String_Node_Str"" + ARCHIVER_BEAN_REFERENCE_PLACEHOLDER + ""String_Node_Str"");
  }
  if (!template.contains(ARCHIVER_PROCESSOR_BEAN_PLACEHOLDER)) {
    throw new IllegalState(""String_Node_Str"" + ARCHIVER_PROCESSOR_BEAN_PLACEHOLDER + ""String_Node_Str"");
  }
  StringBuilder propertyBuilder=new StringBuilder();
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_TEMPLATE)+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_COMPRESSION)+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ ARCHIVE_FILE_PREFIX_PLACEHOLDER+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_MAXSIZE)+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_POOL_MAXACTIVE)+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_WRITE_REQUESTS)+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_WRITE_METADATA)+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_WRITE_METADATA_OUTLINKS)+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_SKIP_IDENTICAL_DIGESTS)+ valueSuffix+ propertyEnd);
  propertyBuilder.append(propertyName + ""String_Node_Str"" + valuePrefix+ Settings.get(HarvesterSettings.HERITRIX3_WARC_START_NEW_FILES_ON_CHECKPOINT)+ valueSuffix+ propertyEnd);
  warcWriterProcessorBean+=propertyBuilder.toString();
  warcWriterProcessorBean+=""String_Node_Str"";
  String templateNew=template.replace(ARCHIVER_BEAN_REFERENCE_PLACEHOLDER,warcWriterbeanReference);
  this.template=templateNew.replace(ARCHIVER_PROCESSOR_BEAN_PLACEHOLDER,warcWriterProcessorBean);
}",0.9686131386861314
89536,"private void writeFtpRecords(WARCWriter w,final CrawlURI curi,final URI baseid,final String timestamp) throws IOException {
  ANVLRecord headers=new ANVLRecord(3);
  headers.addLabelValue(HEADER_KEY_IP,getHostAddress(curi));
  String controlConversation=curi.getString(A_FTP_CONTROL_CONVERSATION);
  URI rid=writeFtpControlConversation(w,timestamp,baseid,curi,headers,controlConversation);
  if (curi.getContentDigest() != null) {
    headers.addLabelValue(HEADER_KEY_PAYLOAD_DIGEST,curi.getContentDigestSchemeString());
  }
  if (curi.getHttpRecorder() != null) {
    if (IdenticalDigestDecideRule.hasIdenticalDigest(curi) && ((Boolean)getUncheckedAttribute(curi,ATTR_WRITE_REVISIT_FOR_IDENTICAL_DIGESTS))) {
      rid=writeRevisitDigest(w,timestamp,null,baseid,curi,headers);
    }
 else {
      headers=new ANVLRecord(3);
      if (curi.isTruncatedFetch()) {
        String value=curi.isTimeTruncatedFetch() ? NAMED_FIELD_TRUNCATED_VALUE_TIME : curi.isLengthTruncatedFetch() ? NAMED_FIELD_TRUNCATED_VALUE_LENGTH : curi.isHeaderTruncatedFetch() ? NAMED_FIELD_TRUNCATED_VALUE_HEAD : TRUNCATED_VALUE_UNSPECIFIED;
        headers.addLabelValue(HEADER_KEY_TRUNCATED,value);
      }
      if (curi.getContentDigest() != null) {
        headers.addLabelValue(HEADER_KEY_PAYLOAD_DIGEST,curi.getContentDigestSchemeString());
      }
      headers.addLabelValue(HEADER_KEY_CONCURRENT_TO,'<' + rid.toString() + '>');
      rid=writeResource(w,timestamp,curi.getContentType(),baseid,curi,headers);
    }
  }
  if (((Boolean)getUncheckedAttribute(curi,ATTR_WRITE_METADATA))) {
    headers=new ANVLRecord(1);
    headers.addLabelValue(HEADER_KEY_CONCURRENT_TO,'<' + rid.toString() + '>');
    writeMetadata(w,timestamp,baseid,curi,headers);
  }
}","private void writeFtpRecords(WARCWriter w,final CrawlURI curi,final URI baseid,final String timestamp) throws IOException {
  ANVLRecord headers=new ANVLRecord(3);
  headers.addLabelValue(HEADER_KEY_IP,getHostAddress(curi));
  String controlConversation=curi.getString(A_FTP_CONTROL_CONVERSATION);
  URI rid=writeFtpControlConversation(w,timestamp,baseid,curi,headers,controlConversation);
  if (curi.getContentDigest() != null) {
    headers.addLabelValue(HEADER_KEY_PAYLOAD_DIGEST,curi.getContentDigestSchemeString());
  }
  if (curi.getHttpRecorder() != null) {
    if (IdenticalDigestDecideRule.hasIdenticalDigest(curi) && ((Boolean)getUncheckedAttribute(curi,ATTR_WRITE_REVISIT_FOR_IDENTICAL_DIGESTS))) {
      rid=writeRevisitDigest(w,timestamp,null,baseid,curi,headers);
    }
 else {
      headers=new ANVLRecord(3);
      if (curi.isTruncatedFetch()) {
        String value=curi.isTimeTruncatedFetch() ? NAMED_FIELD_TRUNCATED_VALUE_TIME : curi.isLengthTruncatedFetch() ? NAMED_FIELD_TRUNCATED_VALUE_LENGTH : curi.isHeaderTruncatedFetch() ? NAMED_FIELD_TRUNCATED_VALUE_HEAD : TRUNCATED_VALUE_UNSPECIFIED;
        headers.addLabelValue(HEADER_KEY_TRUNCATED,value);
      }
      if (curi.getContentDigest() != null) {
        headers.addLabelValue(HEADER_KEY_PAYLOAD_DIGEST,curi.getContentDigestSchemeString());
      }
      headers.addLabelValue(HEADER_KEY_CONCURRENT_TO,'<' + rid.toString() + '>');
      rid=writeResource(w,timestamp,curi.getContentType(),baseid,curi,headers);
    }
  }
  if (((Boolean)getUncheckedAttribute(curi,ATTR_WRITE_METADATA))) {
    headers=new ANVLRecord(1);
    headers.addLabelValue(HEADER_KEY_CONCURRENT_TO,'<' + rid.toString() + '>');
    writeMetadata(w,timestamp,baseid,curi,headers,((Boolean)getUncheckedAttribute(curi,ATTR_WRITE_METADATA_OUTLINKS)));
  }
}",0.9807801017524024
89537,"protected URI writeMetadata(final WARCWriter w,final String timestamp,final URI baseid,final CrawlURI curi,final ANVLRecord namedFields) throws IOException {
  final URI uid=qualifyRecordID(baseid,TYPE,METADATA);
  ANVLRecord r=new ANVLRecord();
  if (curi.isSeed()) {
    r.addLabel(""String_Node_Str"");
  }
 else {
    if (curi.forceFetch()) {
      r.addLabel(""String_Node_Str"");
    }
    r.addLabelValue(""String_Node_Str"",curi.flattenVia());
    r.addLabelValue(""String_Node_Str"",curi.getPathFromSeed());
    if (curi.containsKey(A_SOURCE_TAG)) {
      r.addLabelValue(""String_Node_Str"",curi.getString(A_SOURCE_TAG));
    }
  }
  long duration=curi.getFetchDuration();
  if (duration > -1) {
    r.addLabelValue(""String_Node_Str"",Long.toString(duration));
  }
  if (curi.containsKey(A_FTP_FETCH_STATUS)) {
    r.addLabelValue(""String_Node_Str"",curi.getString(A_FTP_FETCH_STATUS));
  }
  Collection<Link> links=curi.getOutLinks();
  if (links != null && links.size() > 0) {
    for (    Link link : links) {
      r.addLabelValue(""String_Node_Str"",link.toString());
    }
  }
  byte[] b=r.getUTF8Bytes();
  w.writeMetadataRecord(curi.toString(),timestamp,ANVLRecord.MIMETYPE,uid,namedFields,new ByteArrayInputStream(b),b.length);
  return uid;
}","protected URI writeMetadata(final WARCWriter w,final String timestamp,final URI baseid,final CrawlURI curi,final ANVLRecord namedFields,final boolean writeMetadataOutlinks) throws IOException {
  final URI uid=qualifyRecordID(baseid,TYPE,METADATA);
  ANVLRecord r=new ANVLRecord();
  if (curi.isSeed()) {
    r.addLabel(""String_Node_Str"");
  }
 else {
    if (curi.forceFetch()) {
      r.addLabel(""String_Node_Str"");
    }
    r.addLabelValue(""String_Node_Str"",curi.flattenVia());
    r.addLabelValue(""String_Node_Str"",curi.getPathFromSeed());
    if (curi.containsKey(A_SOURCE_TAG)) {
      r.addLabelValue(""String_Node_Str"",curi.getString(A_SOURCE_TAG));
    }
  }
  long duration=curi.getFetchDuration();
  if (duration > -1) {
    r.addLabelValue(""String_Node_Str"",Long.toString(duration));
  }
  if (curi.containsKey(A_FTP_FETCH_STATUS)) {
    r.addLabelValue(""String_Node_Str"",curi.getString(A_FTP_FETCH_STATUS));
  }
  if (writeMetadataOutlinks == true) {
    Collection<Link> links=curi.getOutLinks();
    if (links != null && links.size() > 0) {
      for (      Link link : links) {
        r.addLabelValue(""String_Node_Str"",link.toString());
      }
    }
  }
  byte[] b=r.getUTF8Bytes();
  w.writeMetadataRecord(curi.toString(),timestamp,ANVLRecord.MIMETYPE,uid,namedFields,new ByteArrayInputStream(b),b.length);
  return uid;
}",0.9602162997296252
89538,"private void writeHttpRecords(WARCWriter w,final CrawlURI curi,final URI baseid,final String timestamp) throws IOException {
  ANVLRecord headers=new ANVLRecord(5);
  if (curi.getContentDigest() != null) {
    headers.addLabelValue(HEADER_KEY_PAYLOAD_DIGEST,curi.getContentDigestSchemeString());
  }
  headers.addLabelValue(HEADER_KEY_IP,getHostAddress(curi));
  URI rid;
  if (IdenticalDigestDecideRule.hasIdenticalDigest(curi) && ((Boolean)getUncheckedAttribute(curi,ATTR_WRITE_REVISIT_FOR_IDENTICAL_DIGESTS))) {
    rid=writeRevisitDigest(w,timestamp,HTTP_RESPONSE_MIMETYPE,baseid,curi,headers);
  }
 else   if (curi.getFetchStatus() == HttpStatus.SC_NOT_MODIFIED && ((Boolean)getUncheckedAttribute(curi,ATTR_WRITE_REVISIT_FOR_NOT_MODIFIED))) {
    rid=writeRevisitNotModified(w,timestamp,baseid,curi,headers);
  }
 else {
    if (curi.isTruncatedFetch()) {
      String value=curi.isTimeTruncatedFetch() ? NAMED_FIELD_TRUNCATED_VALUE_TIME : curi.isLengthTruncatedFetch() ? NAMED_FIELD_TRUNCATED_VALUE_LENGTH : curi.isHeaderTruncatedFetch() ? NAMED_FIELD_TRUNCATED_VALUE_HEAD : TRUNCATED_VALUE_UNSPECIFIED;
      headers.addLabelValue(HEADER_KEY_TRUNCATED,value);
    }
    rid=writeResponse(w,timestamp,HTTP_RESPONSE_MIMETYPE,baseid,curi,headers);
  }
  headers=new ANVLRecord(1);
  headers.addLabelValue(HEADER_KEY_CONCURRENT_TO,'<' + rid.toString() + '>');
  if (((Boolean)getUncheckedAttribute(curi,ATTR_WRITE_REQUESTS))) {
    writeRequest(w,timestamp,HTTP_REQUEST_MIMETYPE,baseid,curi,headers);
  }
  if (((Boolean)getUncheckedAttribute(curi,ATTR_WRITE_METADATA))) {
    writeMetadata(w,timestamp,baseid,curi,headers);
  }
}","private void writeHttpRecords(WARCWriter w,final CrawlURI curi,final URI baseid,final String timestamp) throws IOException {
  ANVLRecord headers=new ANVLRecord(5);
  if (curi.getContentDigest() != null) {
    headers.addLabelValue(HEADER_KEY_PAYLOAD_DIGEST,curi.getContentDigestSchemeString());
  }
  headers.addLabelValue(HEADER_KEY_IP,getHostAddress(curi));
  URI rid;
  if (IdenticalDigestDecideRule.hasIdenticalDigest(curi) && ((Boolean)getUncheckedAttribute(curi,ATTR_WRITE_REVISIT_FOR_IDENTICAL_DIGESTS))) {
    rid=writeRevisitDigest(w,timestamp,HTTP_RESPONSE_MIMETYPE,baseid,curi,headers);
  }
 else   if (curi.getFetchStatus() == HttpStatus.SC_NOT_MODIFIED && ((Boolean)getUncheckedAttribute(curi,ATTR_WRITE_REVISIT_FOR_NOT_MODIFIED))) {
    rid=writeRevisitNotModified(w,timestamp,baseid,curi,headers);
  }
 else {
    if (curi.isTruncatedFetch()) {
      String value=curi.isTimeTruncatedFetch() ? NAMED_FIELD_TRUNCATED_VALUE_TIME : curi.isLengthTruncatedFetch() ? NAMED_FIELD_TRUNCATED_VALUE_LENGTH : curi.isHeaderTruncatedFetch() ? NAMED_FIELD_TRUNCATED_VALUE_HEAD : TRUNCATED_VALUE_UNSPECIFIED;
      headers.addLabelValue(HEADER_KEY_TRUNCATED,value);
    }
    rid=writeResponse(w,timestamp,HTTP_RESPONSE_MIMETYPE,baseid,curi,headers);
  }
  headers=new ANVLRecord(1);
  headers.addLabelValue(HEADER_KEY_CONCURRENT_TO,'<' + rid.toString() + '>');
  if (((Boolean)getUncheckedAttribute(curi,ATTR_WRITE_REQUESTS))) {
    writeRequest(w,timestamp,HTTP_REQUEST_MIMETYPE,baseid,curi,headers);
  }
  if (((Boolean)getUncheckedAttribute(curi,ATTR_WRITE_METADATA))) {
    writeMetadata(w,timestamp,baseid,curi,headers,((Boolean)getUncheckedAttribute(curi,ATTR_WRITE_METADATA_OUTLINKS)));
  }
}",0.9796040791841631
89539,"/** 
 * @param name Name of this writer.
 */
public WARCWriterProcessor(final String name){
  super(name,""String_Node_Str"");
  Type e=addElementToDefinition(new SimpleType(ATTR_WRITE_REQUESTS,""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new SimpleType(ATTR_WRITE_METADATA,""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new SimpleType(ATTR_WRITE_REVISIT_FOR_IDENTICAL_DIGESTS,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new SimpleType(ATTR_WRITE_REVISIT_FOR_NOT_MODIFIED,""String_Node_Str"" + ""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new MapType(ATTR_METADATA_ITEMS,""String_Node_Str"",String.class));
  e.setOverrideable(true);
  e.setExpertSetting(true);
}","/** 
 * @param name Name of this writer.
 */
public WARCWriterProcessor(final String name){
  super(name,""String_Node_Str"");
  Type e=addElementToDefinition(new SimpleType(ATTR_WRITE_REQUESTS,""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new SimpleType(ATTR_WRITE_METADATA,""String_Node_Str"",new Boolean(true)));
  e=addElementToDefinition(new SimpleType(ATTR_WRITE_METADATA_OUTLINKS,""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new SimpleType(ATTR_WRITE_REVISIT_FOR_IDENTICAL_DIGESTS,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new SimpleType(ATTR_WRITE_REVISIT_FOR_NOT_MODIFIED,""String_Node_Str"" + ""String_Node_Str"",new Boolean(true)));
  e.setOverrideable(true);
  e.setExpertSetting(true);
  e=addElementToDefinition(new MapType(ATTR_METADATA_ITEMS,""String_Node_Str"",String.class));
  e.setOverrideable(true);
  e.setExpertSetting(true);
}",0.9487895716945997
89540,"private String knownJobsToString(EngineResult engineResult){
  String result=null;
  if (engineResult == null || engineResult.engine == null || engineResult.engine.jobs == null) {
    result=null;
  }
 else {
    List<JobShort> knownjobs=engineResult.engine.jobs;
    for (    JobShort js : knownjobs) {
      result+=js.shortName + ""String_Node_Str"";
    }
  }
  return result;
}","private String knownJobsToString(EngineResult engineResult){
  String result=""String_Node_Str"";
  if (engineResult == null || engineResult.engine == null || engineResult.engine.jobs == null) {
    result=null;
  }
 else {
    List<JobShort> knownjobs=engineResult.engine.jobs;
    for (    JobShort js : knownjobs) {
      result+=js.shortName + ""String_Node_Str"";
    }
  }
  return result;
}",0.9728331177231564
89541,"@Override public void requestCrawlStart(){
  File cxmlFile=getHeritrixFiles().getOrderFile();
  File seedsFile=getHeritrixFiles().getSeedsFile();
  JobResult jobResult;
  File jobDir=files.getHeritrixJobDir();
  if (!jobDir.exists()) {
    jobDir.mkdirs();
  }
  try {
    log.info(""String_Node_Str"",jobDir);
    Heritrix3Wrapper.copyFile(cxmlFile,jobDir);
    Heritrix3Wrapper.copyFileAs(seedsFile,jobDir,""String_Node_Str"");
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
  EngineResult engineResult=null;
  try {
    engineResult=h3wrapper.rescanJobDirectory();
    log.info(""String_Node_Str"",knownJobsToString(engineResult));
    log.debug(""String_Node_Str"" + new String(engineResult.response,""String_Node_Str""));
    jobResult=h3wrapper.buildJobConfiguration(jobName);
    log.debug(""String_Node_Str"" + new String(jobResult.response,""String_Node_Str""));
    if (jobResult.status == ResultStatus.OK) {
      if (jobResult.job.statusDescription.equalsIgnoreCase(""String_Node_Str"")) {
        throw new HeritrixLaunchException(""String_Node_Str"" + jobName + ""String_Node_Str""+ StringUtils.join(jobResult.job.jobLogTail,""String_Node_Str""));
      }
 else       if (jobResult.job.statusDescription.equalsIgnoreCase(""String_Node_Str"")) {
        log.info(""String_Node_Str"",jobName);
      }
 else       if (jobResult.job.statusDescription.startsWith(""String_Node_Str"")) {
        log.warn(""String_Node_Str"",jobName,StringUtils.join(jobResult.job.jobLogTail,""String_Node_Str""));
        jobResult=h3wrapper.teardownJob(jobName);
        log.debug(""String_Node_Str"" + new String(jobResult.response,""String_Node_Str""));
        throw new HeritrixLaunchException(""String_Node_Str"" + jobName + ""String_Node_Str""+ StringUtils.join(jobResult.job.jobLogTail,""String_Node_Str""));
      }
 else {
        throw new IllegalState(""String_Node_Str"" + jobResult.job.statusDescription);
      }
    }
 else {
      throw new IllegalState(""String_Node_Str"" + ResultStatus.toString(jobResult.status));
    }
    jobResult=h3wrapper.waitForJobState(jobName,CrawlControllerState.NASCENT,60,1000);
    if (jobResult.job.crawlControllerState.equals(CrawlControllerState.NASCENT)) {
      log.info(""String_Node_Str"",jobName);
    }
 else {
      log.warn(""String_Node_Str"",jobResult.job.crawlControllerState);
    }
    jobResult=h3wrapper.launchJob(jobName);
    log.debug(""String_Node_Str"" + new String(jobResult.response,""String_Node_Str""));
    jobResult=h3wrapper.waitForJobState(jobName,CrawlControllerState.PAUSED,60,1000);
    if (jobResult.job.crawlControllerState.equals(CrawlControllerState.PAUSED)) {
      log.info(""String_Node_Str"",jobName);
    }
 else {
      log.warn(""String_Node_Str"",jobResult.job.crawlControllerState);
    }
    jobResult=h3wrapper.unpauseJob(jobName);
    log.info(""String_Node_Str"",jobName,jobResult.job.crawlControllerState);
    log.debug(""String_Node_Str"",jobName,new String(jobResult.response,""String_Node_Str""));
  }
 catch (  UnsupportedEncodingException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
}","@Override public void requestCrawlStart(){
  File cxmlFile=getHeritrixFiles().getOrderFile();
  File seedsFile=getHeritrixFiles().getSeedsFile();
  JobResult jobResult;
  File jobDir=files.getHeritrixJobDir();
  if (!jobDir.exists()) {
    jobDir.mkdirs();
  }
  try {
    log.info(""String_Node_Str"",jobDir);
    Heritrix3Wrapper.copyFile(cxmlFile,jobDir);
    Heritrix3Wrapper.copyFileAs(seedsFile,jobDir,""String_Node_Str"");
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
  EngineResult engineResult=null;
  try {
    engineResult=h3wrapper.rescanJobDirectory();
    log.info(""String_Node_Str"",knownJobsToString(engineResult));
    log.debug(""String_Node_Str"" + new String(engineResult.response,""String_Node_Str""));
    jobResult=h3wrapper.buildJobConfiguration(jobName);
    log.debug(""String_Node_Str"" + new String(jobResult.response,""String_Node_Str""));
    if (jobResult.status == ResultStatus.OK) {
      if (jobResult.job.statusDescription.equalsIgnoreCase(""String_Node_Str"")) {
        throw new HeritrixLaunchException(""String_Node_Str"" + jobName + ""String_Node_Str""+ StringUtils.join(jobResult.job.jobLogTail,""String_Node_Str""));
      }
 else       if (jobResult.job.statusDescription.equalsIgnoreCase(""String_Node_Str"")) {
        log.info(""String_Node_Str"",jobName);
      }
 else       if (jobResult.job.statusDescription.startsWith(""String_Node_Str"")) {
        log.warn(""String_Node_Str"",jobName,StringUtils.join(jobResult.job.jobLogTail,""String_Node_Str""));
        jobResult=h3wrapper.teardownJob(jobName);
        log.debug(""String_Node_Str"" + new String(jobResult.response,""String_Node_Str""));
        throw new HeritrixLaunchException(""String_Node_Str"" + jobName + ""String_Node_Str""+ StringUtils.join(jobResult.job.jobLogTail,""String_Node_Str""));
      }
 else {
        throw new IllegalState(""String_Node_Str"" + jobResult.job.statusDescription);
      }
    }
 else {
      throw new IllegalState(""String_Node_Str"" + ResultStatus.toString(jobResult.status));
    }
    jobResult=h3wrapper.waitForJobState(jobName,CrawlControllerState.NASCENT,60,1000);
    if (jobResult.job.crawlControllerState.equalsIgnoreCase(CrawlControllerState.NASCENT.toString())) {
      log.info(""String_Node_Str"",jobName);
    }
 else {
      log.warn(""String_Node_Str"",jobResult.job.crawlControllerState);
    }
    jobResult=h3wrapper.launchJob(jobName);
    log.debug(""String_Node_Str"" + new String(jobResult.response,""String_Node_Str""));
    jobResult=h3wrapper.waitForJobState(jobName,CrawlControllerState.PAUSED,60,1000);
    if (jobResult.job.crawlControllerState.equalsIgnoreCase(CrawlControllerState.PAUSED.toString())) {
      log.info(""String_Node_Str"",jobName);
    }
 else {
      log.warn(""String_Node_Str"",jobResult.job.crawlControllerState);
    }
    jobResult=h3wrapper.unpauseJob(jobName);
    log.info(""String_Node_Str"",jobName,jobResult.job.crawlControllerState);
    log.debug(""String_Node_Str"",jobName,new String(jobResult.response,""String_Node_Str""));
  }
 catch (  UnsupportedEncodingException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
}",0.9931972789115646
89542,"/** 
 * The constructor. Starts by initialising the parent abstract class, then sets the operating system dependent variables.
 * @param subTreeRoot The XML root element.
 * @param parentSettings The Settings to be inherited from the PhysicalLocation, where this machine is placed.
 * @param param The machine parameters to be inherited from the PhysicalLocation.
 * @param netarchiveSuiteSource The name of the NetarchiveSuite package file. Must end with '.zip'.
 * @param securityPolicy The security policy file, to be copied into machine directory.
 * @param dbFile The name of the database file.
 * @param arcdbFile The name of the archive file.
 * @param resetDir Whether the temporary directory should be reset.
 * @param externalJarFolder The folder containing the external jar library files.
 * @param deployConfiguration The general deployment configuration.
 */
public LinuxMachine(Element subTreeRoot,XmlStructure parentSettings,Parameters param,String netarchiveSuiteSource,File slf4JConfig,File securityPolicy,File dbFile,File arcdbFile,boolean resetDir,File externalJarFolder,DeployConfiguration deployConfiguration){
  super(subTreeRoot,parentSettings,param,netarchiveSuiteSource,slf4JConfig,securityPolicy,dbFile,arcdbFile,resetDir,externalJarFolder);
  operatingSystem=Constants.OPERATING_SYSTEM_LINUX_ATTRIBUTE;
  scriptExtension=Constants.SCRIPT_EXTENSION_LINUX;
  String[] bundlesArr;
  String[] certificatesArr;
  String srcStr;
  String dstStr;
  int idx;
  for (  Application app : applications) {
    if (app.isBundledHarvester()) {
      bundlesArr=app.getSettingsValues(Constants.SETTINGS_HARVEST_HERITRIX3_BUNDLE_LEAF);
      if ((bundlesArr == null || bundlesArr.length == 0) && deployConfiguration.getDefaultBundlerZip() != null) {
        bundlesArr=new String[]{deployConfiguration.getDefaultBundlerZip().getAbsolutePath()};
      }
 else       if ((bundlesArr == null || bundlesArr.length == 0) && deployConfiguration.getDefaultBundlerZip() == null) {
        throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"");
      }
      certificatesArr=app.getSettingsValues(Constants.SETTINGS_HARVEST_HERITRIX3_CERTIFICATE_LEAF);
      if (bundlesArr != null && bundlesArr.length > 0 && certificatesArr != null) {
        for (int i=0; i < bundlesArr.length; ++i) {
          srcStr=bundlesArr[i];
          if (!bundles.containsKey(srcStr)) {
            idx=srcStr.lastIndexOf('/');
            if (idx != -1) {
              dstStr=srcStr.substring(idx + 1);
            }
 else {
              dstStr=srcStr;
            }
            dstStr=machineParameters.getInstallDirValue() + ""String_Node_Str"" + getEnvironmentName()+ ""String_Node_Str""+ dstStr;
            bundles.put(srcStr,dstStr);
            System.out.println(srcStr + ""String_Node_Str"" + dstStr);
          }
        }
        for (int i=0; i < certificatesArr.length; ++i) {
          srcStr=certificatesArr[i];
          if (!certificates.containsKey(srcStr)) {
            idx=srcStr.lastIndexOf('/');
            if (idx != -1) {
              dstStr=srcStr.substring(idx + 1);
            }
 else {
              dstStr=srcStr;
            }
            dstStr=machineParameters.getInstallDirValue() + ""String_Node_Str"" + getEnvironmentName()+ ""String_Node_Str""+ dstStr;
            certificates.put(srcStr,dstStr);
            System.out.println(srcStr + ""String_Node_Str"" + dstStr);
          }
        }
        XmlStructure appSettings=app.getSettings();
        Element heritrixBundleElement=appSettings.getSubChild(Constants.SETTINGS_HARVEST_HERITRIX3_BUNDLE_LEAF);
        if (heritrixBundleElement == null) {
          appSettings.getSubChild(Constants.SETTINGS_HERITRIX3_BRANCH).addElement(""String_Node_Str"");
          heritrixBundleElement=appSettings.getSubChild(Constants.SETTINGS_HARVEST_HERITRIX3_BUNDLE_LEAF);
          heritrixBundleElement.setText((String)bundles.values().toArray()[0]);
        }
 else {
          heritrixBundleElement.setText(bundles.get(heritrixBundleElement.getText()));
        }
        Element h3KeystoreElement=appSettings.getSubChild(Constants.SETTINGS_HARVEST_HERITRIX3_CERTIFICATE_LEAF);
        if (h3KeystoreElement != null) {
          String h3KeystoreName=certificates.get(h3KeystoreElement.getText());
          if (h3KeystoreName != null) {
            h3KeystoreElement.setText(h3KeystoreName);
          }
        }
      }
    }
  }
}","/** 
 * The constructor. Starts by initialising the parent abstract class, then sets the operating system dependent variables.
 * @param subTreeRoot The XML root element.
 * @param parentSettings The Settings to be inherited from the PhysicalLocation, where this machine is placed.
 * @param param The machine parameters to be inherited from the PhysicalLocation.
 * @param netarchiveSuiteSource The name of the NetarchiveSuite package file. Must end with '.zip'.
 * @param securityPolicy The security policy file, to be copied into machine directory.
 * @param dbFile The name of the database file.
 * @param arcdbFile The name of the archive file.
 * @param resetDir Whether the temporary directory should be reset.
 * @param externalJarFolder The folder containing the external jar library files.
 * @param deployConfiguration The general deployment configuration.
 */
public LinuxMachine(Element subTreeRoot,XmlStructure parentSettings,Parameters param,String netarchiveSuiteSource,File slf4JConfig,File securityPolicy,File dbFile,File arcdbFile,boolean resetDir,File externalJarFolder,DeployConfiguration deployConfiguration){
  super(subTreeRoot,parentSettings,param,netarchiveSuiteSource,slf4JConfig,securityPolicy,dbFile,arcdbFile,resetDir,externalJarFolder);
  operatingSystem=Constants.OPERATING_SYSTEM_LINUX_ATTRIBUTE;
  scriptExtension=Constants.SCRIPT_EXTENSION_LINUX;
  String[] bundlesArr;
  String[] certificatesArr;
  String srcStr;
  String dstStr;
  int idx;
  for (  Application app : applications) {
    if (app.isBundledHarvester()) {
      bundlesArr=app.getSettingsValues(Constants.SETTINGS_HARVEST_HERITRIX3_BUNDLE_LEAF);
      if ((bundlesArr == null || bundlesArr.length == 0) && deployConfiguration.getDefaultBundlerZip() != null) {
        bundlesArr=new String[]{deployConfiguration.getDefaultBundlerZip().getAbsolutePath()};
      }
 else       if ((bundlesArr == null || bundlesArr.length == 0 || bundlesArr[0].length() == 0) && deployConfiguration.getDefaultBundlerZip() == null) {
        throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"");
      }
      certificatesArr=app.getSettingsValues(Constants.SETTINGS_HARVEST_HERITRIX3_CERTIFICATE_LEAF);
      if (bundlesArr != null && bundlesArr.length > 0 && certificatesArr != null) {
        for (int i=0; i < bundlesArr.length; ++i) {
          srcStr=bundlesArr[i];
          if (!bundles.containsKey(srcStr)) {
            idx=srcStr.lastIndexOf('/');
            if (idx != -1) {
              dstStr=srcStr.substring(idx + 1);
            }
 else {
              dstStr=srcStr;
            }
            dstStr=machineParameters.getInstallDirValue() + ""String_Node_Str"" + getEnvironmentName()+ ""String_Node_Str""+ dstStr;
            bundles.put(srcStr,dstStr);
            System.out.println(srcStr + ""String_Node_Str"" + dstStr);
          }
        }
        for (int i=0; i < certificatesArr.length; ++i) {
          srcStr=certificatesArr[i];
          if (!certificates.containsKey(srcStr)) {
            idx=srcStr.lastIndexOf('/');
            if (idx != -1) {
              dstStr=srcStr.substring(idx + 1);
            }
 else {
              dstStr=srcStr;
            }
            dstStr=machineParameters.getInstallDirValue() + ""String_Node_Str"" + getEnvironmentName()+ ""String_Node_Str""+ dstStr;
            certificates.put(srcStr,dstStr);
            System.out.println(srcStr + ""String_Node_Str"" + dstStr);
          }
        }
        XmlStructure appSettings=app.getSettings();
        Element heritrixBundleElement=appSettings.getSubChild(Constants.SETTINGS_HARVEST_HERITRIX3_BUNDLE_LEAF);
        if (heritrixBundleElement == null) {
          appSettings.getSubChild(Constants.SETTINGS_HERITRIX3_BRANCH).addElement(""String_Node_Str"");
          heritrixBundleElement=appSettings.getSubChild(Constants.SETTINGS_HARVEST_HERITRIX3_BUNDLE_LEAF);
          heritrixBundleElement.setText((String)bundles.values().toArray()[0]);
        }
 else {
          heritrixBundleElement.setText(bundles.get(heritrixBundleElement.getText()));
        }
        Element h3KeystoreElement=appSettings.getSubChild(Constants.SETTINGS_HARVEST_HERITRIX3_CERTIFICATE_LEAF);
        if (h3KeystoreElement != null) {
          String h3KeystoreName=certificates.get(h3KeystoreElement.getText());
          if (h3KeystoreName != null) {
            h3KeystoreElement.setText(h3KeystoreName);
          }
        }
      }
    }
  }
}",0.9964967792970956
89543,"/** 
 * Get the login name for accessing the Heritrix GUI. This name can be set in the settings.xml file.
 * @return Name to use for accessing Heritrix web GUI
 */
protected String getHeritrixAdminName(){
  return Settings.get(Heritrix3Settings.HERITRIX_ADMIN_NAME);
}","/** 
 * Get the login name for accessing the Heritrix3 GUI. This name can be set in the settings.xml file.
 * @return Name to use for accessing Heritrix3 web GUI
 */
protected String getHeritrixAdminName(){
  return Settings.get(Heritrix3Settings.HERITRIX_ADMIN_NAME);
}",0.9962825278810408
89544,"@Override public void exitValue(int exitValue){
  semaphore.release();
  log.info(""String_Node_Str"",exitValue);
}","@Override public void exitValue(int exitValue){
  semaphore.release();
  if (exitValue != 0) {
    log.error(""String_Node_Str"",exitValue);
  }
 else {
    log.info(""String_Node_Str"",exitValue);
  }
}",0.7243589743589743
89545,"/** 
 * @return the HTTP port used by the Heritrix GUI.
 */
protected int getGuiPort(){
  return guiPort;
}","/** 
 * @return the HTTP port used by the Heritrix3 GUI.
 */
protected int getGuiPort(){
  return guiPort;
}",0.9953488372093025
89546,"/** 
 * Get the login password for accessing the Heritrix GUI. This password can be set in the settings.xml file.
 * @return Password to use for accessing the Heritrix GUI
 */
protected String getHeritrixAdminPassword(){
  return Settings.get(Heritrix3Settings.HERITRIX_ADMIN_PASSWORD);
}","/** 
 * Get the login password for accessing the Heritrix3 GUI. This password can be set in the settings.xml file.
 * @return Password to use for accessing the Heritrix3 GUI
 */
protected String getHeritrixAdminPassword(){
  return Settings.get(Heritrix3Settings.HERITRIX_ADMIN_PASSWORD);
}",0.9965397923875432
89547,"/** 
 * @return the Heritrix files wrapper.
 */
protected Heritrix3Files getHeritrixFiles(){
  return files;
}","/** 
 * @return the Heritrix3 files wrapper.
 */
protected Heritrix3Files getHeritrixFiles(){
  return files;
}",0.995475113122172
89548,"/** 
 * Create a AbstractRestHeritrixController  object.
 * @param files Files that are used to set up Heritrix.
 */
public AbstractRestHeritrixController(Heritrix3Files files){
  ArgumentNotValid.checkNotNull(files,""String_Node_Str"");
  this.files=files;
  SystemUtils.checkPortNotUsed(guiPort);
  hostName=SystemUtils.getLocalHostName();
  try {
    log.info(""String_Node_Str"",this,files.getCrawlDir());
    String zipFileStr=files.getHeritrixZip().getAbsolutePath();
    heritrixBaseDir=files.getHeritrixBaseDir();
    if (!heritrixBaseDir.isDirectory()) {
      heritrixBaseDir.mkdirs();
    }
    if (!heritrixBaseDir.isDirectory()) {
      throw new IOFailure(""String_Node_Str"" + heritrixBaseDir.getAbsolutePath());
    }
    log.debug(""String_Node_Str"");
    UnzipUtils.unzip(zipFileStr,1,heritrixBaseDir.getAbsolutePath());
    if (files.getCertificateFile() != null) {
      log.debug(""String_Node_Str"");
      Heritrix3Wrapper.copyFileAs(files.getCertificateFile(),heritrixBaseDir,""String_Node_Str"");
    }
    String[] cmd={""String_Node_Str"",""String_Node_Str"",hostName,""String_Node_Str"",Integer.toString(guiPort),""String_Node_Str"",getHeritrixAdminName() + ""String_Node_Str"" + getHeritrixAdminPassword(),""String_Node_Str"",""String_Node_Str""};
    log.info(""String_Node_Str"",StringUtils.conjoin(""String_Node_Str"",cmd));
    h3launcher=CommandLauncher.getInstance();
    h3launcher.init(heritrixBaseDir,cmd);
    h3launcher.env.put(""String_Node_Str"",""String_Node_Str"");
    log.info(""String_Node_Str"");
    String javaOpts=""String_Node_Str"";
    String jvmOptsStr=Settings.get(Heritrix3Settings.HERITRIX_JVM_OPTS);
    if ((jvmOptsStr != null) && (!jvmOptsStr.isEmpty())) {
      javaOpts=""String_Node_Str"" + jvmOptsStr;
    }
    String javaOptsValue=""String_Node_Str"" + Settings.get(Heritrix3Settings.HERITRIX_HEAP_SIZE) + ""String_Node_Str""+ javaOpts+ ""String_Node_Str""+ getSettingsProperty();
    h3launcher.env.put(""String_Node_Str"",javaOptsValue);
    log.info(""String_Node_Str"",javaOptsValue);
    String heritrixOutValue=files.getHeritrixOutput().getAbsolutePath();
    h3launcher.env.put(""String_Node_Str"",heritrixOutValue);
    log.info(""String_Node_Str"",heritrixOutValue);
    outputPrinter=new PrintWriter(files.getHeritrixStdoutLog(),""String_Node_Str"");
    errorPrinter=new PrintWriter(files.getHeritrixStderrLog(),""String_Node_Str"");
    h3handler=new LaunchResultHandler(outputPrinter,errorPrinter);
    h3launcher.start(h3handler);
    Runtime.getRuntime().addShutdownHook(new HeritrixKiller());
    log.info(""String_Node_Str"");
  }
 catch (  Throwable e) {
    String errMsg=""String_Node_Str"";
    log.debug(errMsg,e);
    throw new IOFailure(errMsg,e);
  }
}","/** 
 * Create a AbstractRestHeritrixController  object.
 * @param files Files that are used to set up Heritrix.
 */
public AbstractRestHeritrixController(Heritrix3Files files){
  ArgumentNotValid.checkNotNull(files,""String_Node_Str"");
  this.files=files;
  SystemUtils.checkPortNotUsed(guiPort);
  hostName=SystemUtils.getLocalHostName();
  try {
    log.info(""String_Node_Str"",this,files.getCrawlDir());
    String zipFileStr=files.getHeritrixZip().getAbsolutePath();
    heritrixBaseDir=files.getHeritrixBaseDir();
    if (!heritrixBaseDir.isDirectory()) {
      heritrixBaseDir.mkdirs();
    }
    if (!heritrixBaseDir.isDirectory()) {
      throw new IOFailure(""String_Node_Str"" + heritrixBaseDir.getAbsolutePath());
    }
    log.debug(""String_Node_Str"");
    UnzipUtils.unzip(zipFileStr,1,heritrixBaseDir.getAbsolutePath());
    if (files.getCertificateFile() != null) {
      log.debug(""String_Node_Str"");
      Heritrix3Wrapper.copyFileAs(files.getCertificateFile(),heritrixBaseDir,""String_Node_Str"");
    }
    String[] cmd={""String_Node_Str"",""String_Node_Str"",hostName,""String_Node_Str"",Integer.toString(guiPort),""String_Node_Str"",getHeritrixAdminName() + ""String_Node_Str"" + getHeritrixAdminPassword(),""String_Node_Str"",""String_Node_Str""};
    log.info(""String_Node_Str"",StringUtils.conjoin(""String_Node_Str"",cmd));
    h3launcher=CommandLauncher.getInstance();
    h3launcher.init(heritrixBaseDir,cmd);
    h3launcher.env.put(""String_Node_Str"",""String_Node_Str"");
    log.info(""String_Node_Str"");
    String javaOpts=""String_Node_Str"";
    String jvmOptsStr=Settings.get(Heritrix3Settings.HERITRIX_JVM_OPTS);
    if ((jvmOptsStr != null) && (!jvmOptsStr.isEmpty())) {
      javaOpts=""String_Node_Str"" + jvmOptsStr;
    }
    String javaOptsValue=""String_Node_Str"" + Settings.get(Heritrix3Settings.HERITRIX_HEAP_SIZE) + ""String_Node_Str""+ javaOpts+ ""String_Node_Str""+ getSettingsProperty();
    h3launcher.env.put(""String_Node_Str"",javaOptsValue);
    log.info(""String_Node_Str"",javaOptsValue);
    String heritrixOutValue=files.getHeritrixOutput().getAbsolutePath();
    h3launcher.env.put(""String_Node_Str"",heritrixOutValue);
    log.info(""String_Node_Str"",heritrixOutValue);
    outputPrinter=new PrintWriter(files.getHeritrixStdoutLog(),""String_Node_Str"");
    errorPrinter=new PrintWriter(files.getHeritrixStderrLog(),""String_Node_Str"");
    log.info(""String_Node_Str"",files.getHeritrixStdoutLog(),files.getHeritrixStderrLog());
    h3handler=new LaunchResultHandler(outputPrinter,errorPrinter);
    h3launcher.start(h3handler);
    Runtime.getRuntime().addShutdownHook(new HeritrixKiller());
    log.info(""String_Node_Str"");
  }
 catch (  Throwable e) {
    String errMsg=""String_Node_Str"";
    log.debug(errMsg,e);
    throw new IOFailure(errMsg,e);
  }
}",0.9833241707898112
89549,"/** 
 * Test a BatchMessage is sent to the_bamon queue.
 */
@Test public void testVisitBatchMessage(){
  DummyServer serverTheBamonQueue=new DummyServer();
  serverTheBamonQueue.reset();
  con.setListener(Channels.getTheBamon(),serverTheBamonQueue);
  ArcRepository arc=ArcRepository.getInstance();
  BatchMessage msg=new BatchMessage(Channels.getTheBamon(),Channels.getError(),new TestBatchJobRuns(),Settings.get(CommonSettings.USE_REPLICA_ID));
  new ArcRepositoryServer(arc).visit(msg);
  ((JMSConnectionMockupMQ)con).waitForConcurrentTasksToFinish();
  assertEquals(""String_Node_Str"",1,serverTheBamonQueue.msgReceived);
  arc.close();
}","/** 
 * Test a BatchMessage is sent to the_bamon queue.
 */
public void testVisitBatchMessage(){
  DummyServer serverTheBamonQueue=new DummyServer();
  serverTheBamonQueue.reset();
  con.setListener(Channels.getTheBamon(),serverTheBamonQueue);
  ArcRepository arc=ArcRepository.getInstance();
  BatchMessage msg=new BatchMessage(Channels.getTheBamon(),Channels.getError(),new TestBatchJobRuns(),Settings.get(CommonSettings.USE_REPLICA_ID));
  new ArcRepositoryServer(arc).visit(msg);
  ((JMSConnectionMockupMQ)con).waitForConcurrentTasksToFinish();
  assertEquals(""String_Node_Str"",1,serverTheBamonQueue.msgReceived);
  arc.close();
}",0.9952904238618524
89550,"/** 
 * Test message is resent.
 */
@Test public void testGet(){
  file=new File(BITARCHIVE_DIR,STORABLE_FILES.get(0).toString());
  GetMessage msg=new GetMessage(Channels.getTheRepos(),Channels.getError(),""String_Node_Str"",0);
  JMSConnectionMockupMQ testCon=(JMSConnectionMockupMQ)JMSConnectionMockupMQ.getInstance();
  TestMessageListener listener=new TestMessageListener();
  testCon.setListener(Channels.getAllBa(),listener);
  ArcRepositoryServer arc=new ArcRepositoryServer(ArcRepository.getInstance());
  arc.visit(msg);
  testCon.waitForConcurrentTasksToFinish();
  assertEquals(""String_Node_Str"",1,listener.getNumReceived());
  arc.close();
}","/** 
 * Test message is resent.
 */
public void testGet(){
  file=new File(BITARCHIVE_DIR,STORABLE_FILES.get(0).toString());
  GetMessage msg=new GetMessage(Channels.getTheRepos(),Channels.getError(),""String_Node_Str"",0);
  JMSConnectionMockupMQ testCon=(JMSConnectionMockupMQ)JMSConnectionMockupMQ.getInstance();
  TestMessageListener listener=new TestMessageListener();
  testCon.setListener(Channels.getAllBa(),listener);
  ArcRepositoryServer arc=new ArcRepositoryServer(ArcRepository.getInstance());
  arc.visit(msg);
  testCon.waitForConcurrentTasksToFinish();
  assertEquals(""String_Node_Str"",1,listener.getNumReceived());
  arc.close();
}",0.99537750385208
89551,"/** 
 * Create a BnfHeritrixController object.
 * @param files Files that are used to set up Heritrix.
 */
public AbstractRestHeritrixController(Heritrix3Files files){
  ArgumentNotValid.checkNotNull(files,""String_Node_Str"");
  this.files=files;
  SystemUtils.checkPortNotUsed(guiPort);
  hostName=SystemUtils.getLocalHostName();
  try {
    log.info(""String_Node_Str"",this,files.getCrawlDir());
    String zipFileStr=files.getHeritrixZip().getAbsolutePath();
    heritrixBaseDir=files.getHeritrixBaseDir();
    if (!heritrixBaseDir.isDirectory()) {
      heritrixBaseDir.mkdirs();
    }
    if (!heritrixBaseDir.isDirectory()) {
      throw new IOFailure(""String_Node_Str"" + heritrixBaseDir.getAbsolutePath());
    }
    log.debug(""String_Node_Str"");
    UnzipUtils.unzip(zipFileStr,1,heritrixBaseDir.getAbsolutePath());
    if (files.getCertificateFile() != null) {
      log.debug(""String_Node_Str"");
      Heritrix3Wrapper.copyFileAs(files.getCertificateFile(),heritrixBaseDir,""String_Node_Str"");
    }
    String[] cmd={""String_Node_Str"",""String_Node_Str"",hostName,""String_Node_Str"",Integer.toString(guiPort),""String_Node_Str"",getHeritrixAdminName() + ""String_Node_Str"" + getHeritrixAdminPassword(),""String_Node_Str"",""String_Node_Str""};
    log.info(""String_Node_Str"",StringUtils.conjoin(""String_Node_Str"",cmd));
    h3launcher=CommandLauncher.getInstance();
    h3launcher.init(heritrixBaseDir,cmd);
    h3launcher.env.put(""String_Node_Str"",""String_Node_Str"");
    log.info(""String_Node_Str"");
    String javaOpts=""String_Node_Str"";
    String jvmOptsStr=Settings.get(Heritrix3Settings.HERITRIX_JVM_OPTS);
    if ((jvmOptsStr != null) && (!jvmOptsStr.isEmpty())) {
      javaOpts=""String_Node_Str"" + jvmOptsStr;
    }
    String javaOptsValue=""String_Node_Str"" + Settings.get(Heritrix3Settings.HERITRIX_HEAP_SIZE) + javaOpts;
    h3launcher.env.put(""String_Node_Str"",javaOptsValue);
    log.info(""String_Node_Str"",javaOptsValue);
    String heritrixOutValue=files.getHeritrixOutput().getAbsolutePath();
    h3launcher.env.put(""String_Node_Str"",heritrixOutValue);
    log.info(""String_Node_Str"",heritrixOutValue);
    outputPrinter=new PrintWriter(files.getHeritrixStdoutLog(),""String_Node_Str"");
    errorPrinter=new PrintWriter(files.getHeritrixStderrLog(),""String_Node_Str"");
    h3handler=new LaunchResultHandler(outputPrinter,errorPrinter);
    log.info(""String_Node_Str"");
    h3launcher.start(h3handler);
    Runtime.getRuntime().addShutdownHook(new HeritrixKiller());
    log.info(""String_Node_Str"");
  }
 catch (  Throwable e) {
    log.debug(""String_Node_Str"",e);
    throw new IOFailure(""String_Node_Str"",e);
  }
}","/** 
 * Create a BnfHeritrixController object.
 * @param files Files that are used to set up Heritrix.
 */
public AbstractRestHeritrixController(Heritrix3Files files){
  ArgumentNotValid.checkNotNull(files,""String_Node_Str"");
  this.files=files;
  SystemUtils.checkPortNotUsed(guiPort);
  hostName=SystemUtils.getLocalHostName();
  try {
    log.info(""String_Node_Str"",this,files.getCrawlDir());
    String zipFileStr=files.getHeritrixZip().getAbsolutePath();
    heritrixBaseDir=files.getHeritrixBaseDir();
    if (!heritrixBaseDir.isDirectory()) {
      heritrixBaseDir.mkdirs();
    }
    if (!heritrixBaseDir.isDirectory()) {
      throw new IOFailure(""String_Node_Str"" + heritrixBaseDir.getAbsolutePath());
    }
    log.debug(""String_Node_Str"");
    UnzipUtils.unzip(zipFileStr,1,heritrixBaseDir.getAbsolutePath());
    if (files.getCertificateFile() != null) {
      log.debug(""String_Node_Str"");
      Heritrix3Wrapper.copyFileAs(files.getCertificateFile(),heritrixBaseDir,""String_Node_Str"");
    }
    String[] cmd={""String_Node_Str"",""String_Node_Str"",hostName,""String_Node_Str"",Integer.toString(guiPort),""String_Node_Str"",getHeritrixAdminName() + ""String_Node_Str"" + getHeritrixAdminPassword(),""String_Node_Str"",""String_Node_Str""};
    log.info(""String_Node_Str"",StringUtils.conjoin(""String_Node_Str"",cmd));
    h3launcher=CommandLauncher.getInstance();
    h3launcher.init(heritrixBaseDir,cmd);
    h3launcher.env.put(""String_Node_Str"",""String_Node_Str"");
    log.info(""String_Node_Str"");
    String javaOpts=""String_Node_Str"";
    String jvmOptsStr=Settings.get(Heritrix3Settings.HERITRIX_JVM_OPTS);
    if ((jvmOptsStr != null) && (!jvmOptsStr.isEmpty())) {
      javaOpts=""String_Node_Str"" + jvmOptsStr;
    }
    String javaOptsValue=""String_Node_Str"" + Settings.get(Heritrix3Settings.HERITRIX_HEAP_SIZE) + ""String_Node_Str""+ javaOpts;
    h3launcher.env.put(""String_Node_Str"",javaOptsValue);
    log.info(""String_Node_Str"",javaOptsValue);
    String heritrixOutValue=files.getHeritrixOutput().getAbsolutePath();
    h3launcher.env.put(""String_Node_Str"",heritrixOutValue);
    log.info(""String_Node_Str"",heritrixOutValue);
    outputPrinter=new PrintWriter(files.getHeritrixStdoutLog(),""String_Node_Str"");
    errorPrinter=new PrintWriter(files.getHeritrixStderrLog(),""String_Node_Str"");
    h3handler=new LaunchResultHandler(outputPrinter,errorPrinter);
    log.info(""String_Node_Str"");
    h3launcher.start(h3handler);
    Runtime.getRuntime().addShutdownHook(new HeritrixKiller());
    log.info(""String_Node_Str"");
  }
 catch (  Throwable e) {
    log.debug(""String_Node_Str"",e);
    throw new IOFailure(""String_Node_Str"",e);
  }
}",0.9964008334911916
89552,"/** 
 * Initialise options by setting legal parameters for batch jobs.
 */
ArgumentParameters(){
  options.addOption(Constants.ARG_CONFIG_FILE,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_NETARCHIVE_SUITE_FILE,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_SECURITY_FILE,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_SLF4J_CONFIG_FILE,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_OUTPUT_DIRECTORY,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_DATABASE_FILE,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_TEST,HAS_ARG,""String_Node_Str"" + ""String_Node_Str"");
  options.addOption(Constants.ARG_RESET,HAS_ARG,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"");
  options.addOption(Constants.ARG_EVALUATE,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_ARC_DB,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_JAR_FOLDER,HAS_ARG,""String_Node_Str"" + ""String_Node_Str"");
  options.addOption(Constants.ARG_SOURCE_ENCODING,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_DEFAULT_BUNDLER_ZIP,HAS_ARG,""String_Node_Str"");
}","/** 
 * Initialise options by setting legal parameters for batch jobs.
 */
ArgumentParameters(){
  options.addOption(Constants.ARG_CONFIG_FILE,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_NETARCHIVE_SUITE_FILE,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_SECURITY_FILE,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_SLF4J_CONFIG_FILE,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_OUTPUT_DIRECTORY,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_DATABASE_FILE,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_TEST,HAS_ARG,""String_Node_Str"" + ""String_Node_Str"");
  options.addOption(Constants.ARG_RESET,HAS_ARG,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"");
  options.addOption(Constants.ARG_EVALUATE,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_ARC_DB,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_JAR_FOLDER,HAS_ARG,""String_Node_Str"" + ""String_Node_Str"");
  options.addOption(Constants.ARG_SOURCE_ENCODING,HAS_ARG,""String_Node_Str"");
  options.addOption(Constants.ARG_DEFAULT_BUNDLER_ZIP,HAS_ARG,""String_Node_Str"" + ""String_Node_Str"");
}",0.991554054054054
89553,"/** 
 * The constructor. Starts by initialising the parent abstract class, then sets the operating system dependent variables.
 * @param subTreeRoot The XML root element.
 * @param parentSettings The Settings to be inherited from the PhysicalLocation, where this machine is placed.
 * @param param The machine parameters to be inherited from the PhysicalLocation.
 * @param netarchiveSuiteSource The name of the NetarchiveSuite package file. Must end with '.zip'.
 * @param securityPolicy The security policy file, to be copied into machine directory.
 * @param dbFile The name of the database file.
 * @param arcdbFile The name of the archive file.
 * @param resetDir Whether the temporary directory should be reset.
 * @param externalJarFolder The folder containing the external jar library files.
 * @param deployConfiguration The general deployment configuration.
 */
public LinuxMachine(Element subTreeRoot,XmlStructure parentSettings,Parameters param,String netarchiveSuiteSource,File slf4JConfig,File securityPolicy,File dbFile,File arcdbFile,boolean resetDir,File externalJarFolder,DeployConfiguration deployConfiguration){
  super(subTreeRoot,parentSettings,param,netarchiveSuiteSource,slf4JConfig,securityPolicy,dbFile,arcdbFile,resetDir,externalJarFolder);
  operatingSystem=Constants.OPERATING_SYSTEM_LINUX_ATTRIBUTE;
  scriptExtension=Constants.SCRIPT_EXTENSION_LINUX;
  String[] bundlesArr;
  String[] certificatesArr;
  String srcStr;
  String dstStr;
  int idx;
  for (  Application app : applications) {
    if (app.isBundledHarvester()) {
      bundlesArr=app.getSettingsValues(Constants.SETTINGS_HARVEST_HERITRIX3_BUNDLE_LEAF);
      if ((bundlesArr != null || bundlesArr.length == 0) && deployConfiguration.getDefaultBundlerZip().isPresent()) {
        bundlesArr=new String[]{deployConfiguration.getDefaultBundlerZip().get().getAbsolutePath()};
      }
 else       if ((bundlesArr == null || bundlesArr.length == 0) && !deployConfiguration.getDefaultBundlerZip().isPresent()) {
        throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"");
      }
      certificatesArr=app.getSettingsValues(Constants.SETTINGS_HARVEST_HERITRIX3_CERTIFICATE_LEAF);
      if (bundlesArr != null && bundlesArr.length > 0 && certificatesArr != null) {
        for (int i=0; i < bundlesArr.length; ++i) {
          srcStr=bundlesArr[i];
          if (!bundles.containsKey(srcStr)) {
            idx=srcStr.lastIndexOf('/');
            if (idx != -1) {
              dstStr=srcStr.substring(idx + 1);
            }
 else {
              dstStr=srcStr;
            }
            dstStr=machineParameters.getInstallDirValue() + ""String_Node_Str"" + getEnvironmentName()+ ""String_Node_Str""+ dstStr;
            bundles.put(srcStr,dstStr);
            System.out.println(srcStr + ""String_Node_Str"" + dstStr);
          }
        }
        for (int i=0; i < certificatesArr.length; ++i) {
          srcStr=certificatesArr[i];
          if (!certificates.containsKey(srcStr)) {
            idx=srcStr.lastIndexOf('/');
            if (idx != -1) {
              dstStr=srcStr.substring(idx + 1);
            }
 else {
              dstStr=srcStr;
            }
            dstStr=machineParameters.getInstallDirValue() + ""String_Node_Str"" + getEnvironmentName()+ ""String_Node_Str""+ dstStr;
            certificates.put(srcStr,dstStr);
            System.out.println(srcStr + ""String_Node_Str"" + dstStr);
          }
        }
        XmlStructure appSettings=app.getSettings();
        Element heritrixBundleElement=appSettings.getSubChild(Constants.SETTINGS_HARVEST_HERITRIX3_BUNDLE_LEAF);
        if (heritrixBundleElement == null) {
          appSettings.getSubChild(Constants.SETTINGS_HERITRIX_BRANCH).addElement(""String_Node_Str"");
          heritrixBundleElement=appSettings.getSubChild(Constants.SETTINGS_HARVEST_HERITRIX3_BUNDLE_LEAF);
          heritrixBundleElement.setText((String)bundles.values().toArray()[0]);
        }
 else {
          heritrixBundleElement.setText(bundles.get(heritrixBundleElement.getText()));
        }
        Element h3KeystoreElement=appSettings.getSubChild(Constants.SETTINGS_HARVEST_HERITRIX3_CERTIFICATE_LEAF);
        if (h3KeystoreElement != null) {
          String h3KeystoreName=certificates.get(h3KeystoreElement.getText());
          if (h3KeystoreName != null) {
            h3KeystoreElement.setText(h3KeystoreName);
          }
        }
      }
    }
  }
}","/** 
 * The constructor. Starts by initialising the parent abstract class, then sets the operating system dependent variables.
 * @param subTreeRoot The XML root element.
 * @param parentSettings The Settings to be inherited from the PhysicalLocation, where this machine is placed.
 * @param param The machine parameters to be inherited from the PhysicalLocation.
 * @param netarchiveSuiteSource The name of the NetarchiveSuite package file. Must end with '.zip'.
 * @param securityPolicy The security policy file, to be copied into machine directory.
 * @param dbFile The name of the database file.
 * @param arcdbFile The name of the archive file.
 * @param resetDir Whether the temporary directory should be reset.
 * @param externalJarFolder The folder containing the external jar library files.
 * @param deployConfiguration The general deployment configuration.
 */
public LinuxMachine(Element subTreeRoot,XmlStructure parentSettings,Parameters param,String netarchiveSuiteSource,File slf4JConfig,File securityPolicy,File dbFile,File arcdbFile,boolean resetDir,File externalJarFolder,DeployConfiguration deployConfiguration){
  super(subTreeRoot,parentSettings,param,netarchiveSuiteSource,slf4JConfig,securityPolicy,dbFile,arcdbFile,resetDir,externalJarFolder);
  operatingSystem=Constants.OPERATING_SYSTEM_LINUX_ATTRIBUTE;
  scriptExtension=Constants.SCRIPT_EXTENSION_LINUX;
  String[] bundlesArr;
  String[] certificatesArr;
  String srcStr;
  String dstStr;
  int idx;
  for (  Application app : applications) {
    if (app.isBundledHarvester()) {
      bundlesArr=app.getSettingsValues(Constants.SETTINGS_HARVEST_HERITRIX3_BUNDLE_LEAF);
      if ((bundlesArr == null || bundlesArr.length == 0) && deployConfiguration.getDefaultBundlerZip().isPresent()) {
        bundlesArr=new String[]{deployConfiguration.getDefaultBundlerZip().get().getAbsolutePath()};
      }
 else       if ((bundlesArr == null || bundlesArr.length == 0) && !deployConfiguration.getDefaultBundlerZip().isPresent()) {
        throw new IllegalArgumentException(""String_Node_Str"" + ""String_Node_Str"");
      }
      certificatesArr=app.getSettingsValues(Constants.SETTINGS_HARVEST_HERITRIX3_CERTIFICATE_LEAF);
      if (bundlesArr != null && bundlesArr.length > 0 && certificatesArr != null) {
        for (int i=0; i < bundlesArr.length; ++i) {
          srcStr=bundlesArr[i];
          if (!bundles.containsKey(srcStr)) {
            idx=srcStr.lastIndexOf('/');
            if (idx != -1) {
              dstStr=srcStr.substring(idx + 1);
            }
 else {
              dstStr=srcStr;
            }
            dstStr=machineParameters.getInstallDirValue() + ""String_Node_Str"" + getEnvironmentName()+ ""String_Node_Str""+ dstStr;
            bundles.put(srcStr,dstStr);
            System.out.println(srcStr + ""String_Node_Str"" + dstStr);
          }
        }
        for (int i=0; i < certificatesArr.length; ++i) {
          srcStr=certificatesArr[i];
          if (!certificates.containsKey(srcStr)) {
            idx=srcStr.lastIndexOf('/');
            if (idx != -1) {
              dstStr=srcStr.substring(idx + 1);
            }
 else {
              dstStr=srcStr;
            }
            dstStr=machineParameters.getInstallDirValue() + ""String_Node_Str"" + getEnvironmentName()+ ""String_Node_Str""+ dstStr;
            certificates.put(srcStr,dstStr);
            System.out.println(srcStr + ""String_Node_Str"" + dstStr);
          }
        }
        XmlStructure appSettings=app.getSettings();
        Element heritrixBundleElement=appSettings.getSubChild(Constants.SETTINGS_HARVEST_HERITRIX3_BUNDLE_LEAF);
        if (heritrixBundleElement == null) {
          appSettings.getSubChild(Constants.SETTINGS_HERITRIX_BRANCH).addElement(""String_Node_Str"");
          heritrixBundleElement=appSettings.getSubChild(Constants.SETTINGS_HARVEST_HERITRIX3_BUNDLE_LEAF);
          heritrixBundleElement.setText((String)bundles.values().toArray()[0]);
        }
 else {
          heritrixBundleElement.setText(bundles.get(heritrixBundleElement.getText()));
        }
        Element h3KeystoreElement=appSettings.getSubChild(Constants.SETTINGS_HARVEST_HERITRIX3_CERTIFICATE_LEAF);
        if (h3KeystoreElement != null) {
          String h3KeystoreName=certificates.get(h3KeystoreElement.getText());
          if (h3KeystoreName != null) {
            h3KeystoreElement.setText(h3KeystoreName);
          }
        }
      }
    }
  }
}",0.9997739091114628
89554,"/** 
 * Will create a NodeTraverser for accessing the indicated node. If the current node does existes it will be created.
 * @param element The tag for the element to make available, eg. <crawl-order>.....
 * @param name A optional name attribute to add to the element if created.
 * @return The NodeTraverser reference, which can be used to create further child nodes.
 */
public NodeTraverser getChildNode(String element,String name){
  Node childNode=null;
  NodeList nodes=currentNode.getChildNodes();
  if (nodes != null) {
    for (int i=0; i < nodes.getLength(); i++) {
      Node node=nodes.item(i);
      if ((name == null || name.equals(node.getNodeName())) && element.equals(node.getAttributes().getNamedItem(""String_Node_Str""))) {
        childNode=node;
        break;
      }
    }
  }
  if (childNode == null) {
    Element newNode=doc.createElement(element);
    if (name != null) {
      newNode.setAttribute(""String_Node_Str"",name);
    }
    currentNode.appendChild(newNode);
    childNode=newNode;
  }
  currentNode=childNode;
  return this;
}","/** 
 * Will create a NodeTraverser for accessing the indicated node. If the current node does exist it will be created.
 * @param element The tag for the element to make available, eg. <crawl-order>.....
 * @param name A optional name attribute to add to the element if created.
 * @return The NodeTraverser reference, which can be used to create further child nodes.
 */
public NodeTraverser getChildNode(String element,String name){
  Node childNode=null;
  NodeList nodes=currentNode.getChildNodes();
  if (nodes != null) {
    for (int i=0; i < nodes.getLength(); i++) {
      Node node=nodes.item(i);
      NamedNodeMap attributes=node.getAttributes();
      String nameAttr=null;
      if (attributes != null) {
        Node attrNode=attributes.getNamedItem(""String_Node_Str"");
        if (attrNode != null && attrNode.getNodeType() == Node.ATTRIBUTE_NODE) {
          nameAttr=attrNode.getNodeValue();
        }
      }
      if (element.equals(node.getNodeName()) && (name == null || name.equals(nameAttr))) {
        childNode=node;
        break;
      }
    }
  }
  if (childNode == null) {
    Element newNode=doc.createElement(element);
    if (name != null) {
      newNode.setAttribute(""String_Node_Str"",name);
    }
    currentNode.appendChild(newNode);
    childNode=newNode;
  }
  currentNode=childNode;
  return this;
}",0.657511444028298
89555,"/** 
 * Simple test of new job submitting.
 */
@Test public void testSubmitNewJobs() throws DocumentException {
  prepareDefaultMockAnswers(SELECTIVE_HARVEST_CHANNEL,jobMock);
  jobDispatcher.submitNextNewJob(SELECTIVE_HARVEST_CHANNEL);
  verify(jobMock).setStatus(JobStatus.SUBMITTED);
  verify(jobMock).setSubmittedDate(any(Date.class));
  verify(jobDAO).update(jobMock);
  verify(jmsConnection).send(crawlMessageCaptor.capture());
  assertTrue(jobMock == crawlMessageCaptor.getValue().getJob());
  assertEquals(HarvesterChannels.getHarvestJobChannelId(SELECTIVE_HARVEST_CHANNEL),crawlMessageCaptor.getValue().getTo());
  assertEquals(harvest.getName(),crawlMessageCaptor.getValue().getOrigHarvestInfo().getOrigHarvestName());
}","/** 
 * Simple test of new job submitting.
 */
@Test public void testSubmitNewJobs() throws DocumentException {
  prepareDefaultMockAnswers(SELECTIVE_HARVEST_CHANNEL,jobMock,false);
  jobDispatcher.submitNextNewJob(SELECTIVE_HARVEST_CHANNEL);
  verify(jobMock).setStatus(JobStatus.SUBMITTED);
  verify(jobMock).setSubmittedDate(any(Date.class));
  verify(jobDAO).update(jobMock);
  verify(jmsConnection).send(crawlMessageCaptor.capture());
  assertTrue(jobMock == crawlMessageCaptor.getValue().getJob());
  assertEquals(HarvesterChannels.getHarvestJobChannelId(SELECTIVE_HARVEST_CHANNEL),crawlMessageCaptor.getValue().getTo());
  assertEquals(harvest.getName(),crawlMessageCaptor.getValue().getOrigHarvestInfo().getOrigHarvestName());
}",0.9959072305593452
89556,"/** 
 * Test sending + check that we send a message Uses MessageTestHandler()
 */
@Test public void testSendingToCorrectQueue(){
  prepareDefaultMockAnswers(SELECTIVE_HARVEST_CHANNEL,jobMock);
  jobDispatcher.submitNextNewJob(SELECTIVE_HARVEST_CHANNEL);
  verify(jmsConnection).send(crawlMessageCaptor.capture());
  assertTrue(jobMock == crawlMessageCaptor.getValue().getJob());
  assertEquals(HarvesterChannels.getHarvestJobChannelId(SELECTIVE_HARVEST_CHANNEL),crawlMessageCaptor.getValue().getTo());
  reset(jmsConnection);
  Job snapshotJob=createJob(2);
  prepareDefaultMockAnswers(SNAPSHOT,snapshotJob);
  jobDispatcher.submitNextNewJob(SNAPSHOT);
  verify(jmsConnection).send(crawlMessageCaptor.capture());
  assertTrue(snapshotJob == crawlMessageCaptor.getValue().getJob());
  assertEquals(HarvesterChannels.getHarvestJobChannelId(SNAPSHOT),crawlMessageCaptor.getValue().getTo());
}","/** 
 * Test sending + check that we send a message Uses MessageTestHandler()
 */
@Test public void testSendingToCorrectQueue(){
  prepareDefaultMockAnswers(SELECTIVE_HARVEST_CHANNEL,jobMock,false);
  jobDispatcher.submitNextNewJob(SELECTIVE_HARVEST_CHANNEL);
  verify(jmsConnection).send(crawlMessageCaptor.capture());
  assertTrue(jobMock == crawlMessageCaptor.getValue().getJob());
  assertEquals(HarvesterChannels.getHarvestJobChannelId(SELECTIVE_HARVEST_CHANNEL),crawlMessageCaptor.getValue().getTo());
  reset(jmsConnection);
  Job snapshotJob=createJob(2);
  prepareDefaultMockAnswers(SNAPSHOT,snapshotJob,false);
  jobDispatcher.submitNextNewJob(SNAPSHOT);
  verify(jmsConnection).send(crawlMessageCaptor.capture());
  assertTrue(snapshotJob == crawlMessageCaptor.getValue().getJob());
  assertEquals(HarvesterChannels.getHarvestJobChannelId(SNAPSHOT),crawlMessageCaptor.getValue().getTo());
}",0.9932960893854748
89557,"/** 
 * Test that runNewJobs makes correct duplication reduction information.
 */
@Test public void testSubmitNewJobsMakesDuplicateReductionInfo() throws DocumentException {
  prepareDefaultMockAnswers(SELECTIVE_HARVEST_CHANNEL,jobMock);
  Document doc=OrderXmlBuilder.create().enableDeduplication().getDoc();
  HeritrixTemplate h1temp=new H1HeritrixTemplate(doc,false);
  when(jobMock.getOrderXMLdoc()).thenReturn(h1temp);
  List<Long> jobIDsForDuplicateReduction=Arrays.asList(new Long[]{1L});
  when(jobDAO.getJobIDsForDuplicateReduction(jobMock.getJobID())).thenReturn(jobIDsForDuplicateReduction);
  jobDispatcher.submitNextNewJob(SELECTIVE_HARVEST_CHANNEL);
  verify(jmsConnection).send(crawlMessageCaptor.capture());
  DoOneCrawlMessage crawlMessage=crawlMessageCaptor.getValue();
  assertEquals(""String_Node_Str"",1,crawlMessage.getMetadata().size());
  MetadataEntry metadataEntry=crawlMessage.getMetadata().get(0);
  assertNotNull(""String_Node_Str"",metadataEntry);
  assertEquals(""String_Node_Str"",""String_Node_Str"",metadataEntry.getMimeType());
  assertEquals(""String_Node_Str"",""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"" + harvest.getOid() + ""String_Node_Str""+ jobMock.getHarvestNum()+ ""String_Node_Str""+ jobMock.getJobID(),metadataEntry.getURL());
  assertEquals(""String_Node_Str"",jobIDsForDuplicateReduction.get(0) + ""String_Node_Str"",new String(metadataEntry.getData()));
}","/** 
 * Test that runNewJobs makes correct duplication reduction information.
 */
@Test public void testSubmitNewJobsMakesDuplicateReductionInfo() throws DocumentException {
  prepareDefaultMockAnswers(SELECTIVE_HARVEST_CHANNEL,jobMock,true);
  List<Long> jobIDsForDuplicateReduction=Arrays.asList(new Long[]{1L});
  when(jobDAO.getJobIDsForDuplicateReduction(jobMock.getJobID())).thenReturn(jobIDsForDuplicateReduction);
  jobDispatcher.submitNextNewJob(SELECTIVE_HARVEST_CHANNEL);
  verify(jmsConnection).send(crawlMessageCaptor.capture());
  DoOneCrawlMessage crawlMessage=crawlMessageCaptor.getValue();
  assertEquals(""String_Node_Str"",1,crawlMessage.getMetadata().size());
  MetadataEntry metadataEntry=crawlMessage.getMetadata().get(0);
  assertNotNull(""String_Node_Str"",metadataEntry);
  assertEquals(""String_Node_Str"",""String_Node_Str"",metadataEntry.getMimeType());
  assertEquals(""String_Node_Str"",""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"" + harvest.getOid() + ""String_Node_Str""+ jobMock.getHarvestNum()+ ""String_Node_Str""+ jobMock.getJobID(),metadataEntry.getURL());
  assertEquals(""String_Node_Str"",jobIDsForDuplicateReduction.get(0) + ""String_Node_Str"",new String(metadataEntry.getData()));
}",0.9289206468597218
89558,"private void prepareDefaultMockAnswers(HarvestChannel channel,Job job){
  Iterator<Long> jobIDIterator=Arrays.asList(new Long[]{job.getJobID()}).iterator();
  when(jobDAO.getAllJobIds(JobStatus.NEW,channel)).thenReturn(jobIDIterator);
  when(jobDAO.read(job.getJobID())).thenReturn(job);
  when(harvestDefinitionDAO.getHarvestName(harvest.getOid())).thenReturn(harvest.getName());
  when(harvestDefinitionDAO.getSparsePartialHarvest(harvest.getName())).thenReturn(harvest);
}","private void prepareDefaultMockAnswers(HarvestChannel channel,Job job,boolean dedup){
  Iterator<Long> jobIDIterator=Arrays.asList(new Long[]{job.getJobID()}).iterator();
  when(jobDAO.getAllJobIds(JobStatus.NEW,channel)).thenReturn(jobIDIterator);
  when(jobDAO.read(job.getJobID())).thenReturn(job);
  when(harvestDefinitionDAO.getHarvestName(harvest.getOid())).thenReturn(harvest.getName());
  when(harvestDefinitionDAO.getSparsePartialHarvest(harvest.getName())).thenReturn(harvest);
  OrderXmlBuilder builder=OrderXmlBuilder.createDefault();
  if (dedup) {
    builder=builder.enableDeduplication();
  }
 else {
    builder=builder.disableDeduplication();
  }
  HeritrixTemplate h1temp=new H1HeritrixTemplate(builder.getDoc(),false);
  when(job.getOrderXMLdoc()).thenReturn(h1temp);
  when(job.getChannel()).thenReturn(SELECTIVE_HARVEST_CHANNEL.getName());
  when(job.getOrderXMLName()).thenReturn(""String_Node_Str"");
  when(job.getHarvestFilenamePrefix()).thenReturn(""String_Node_Str"");
}",0.6466984343090538
89559,"/** 
 * Test that runNewJobs generates correct alias information for the job.s
 */
@Test public void testSubmitNewJobsMakesAliasInfo() throws SQLException {
  prepareDefaultMockAnswers(SELECTIVE_HARVEST_CHANNEL,jobMock);
  String originalDomain=""String_Node_Str"";
  String aliasDomain=""String_Node_Str"";
  when(jobDAO.getJobAliasInfo(any(Job.class))).thenReturn(Arrays.asList(new AliasInfo[]{new AliasInfo(""String_Node_Str"",""String_Node_Str"",new Date())}));
  jobDispatcher.submitNextNewJob(SELECTIVE_HARVEST_CHANNEL);
  verify(jmsConnection).send(crawlMessageCaptor.capture());
  assertEquals(""String_Node_Str"",1,crawlMessageCaptor.getValue().getMetadata().size());
  assertEquals(aliasDomain + ""String_Node_Str"" + originalDomain+ ""String_Node_Str"",new String(crawlMessageCaptor.getValue().getMetadata().get(0).getData()));
}","/** 
 * Test that runNewJobs generates correct alias information for the job.s
 */
@Test public void testSubmitNewJobsMakesAliasInfo() throws SQLException {
  prepareDefaultMockAnswers(SELECTIVE_HARVEST_CHANNEL,jobMock,false);
  String originalDomain=""String_Node_Str"";
  String aliasDomain=""String_Node_Str"";
  when(jobDAO.getJobAliasInfo(any(Job.class))).thenReturn(Arrays.asList(new AliasInfo[]{new AliasInfo(""String_Node_Str"",""String_Node_Str"",new Date())}));
  jobDispatcher.submitNextNewJob(SELECTIVE_HARVEST_CHANNEL);
  verify(jmsConnection).send(crawlMessageCaptor.capture());
  assertEquals(""String_Node_Str"",1,crawlMessageCaptor.getValue().getMetadata().size());
  assertEquals(aliasDomain + ""String_Node_Str"" + originalDomain+ ""String_Node_Str"",new String(crawlMessageCaptor.getValue().getMetadata().get(0).getData()));
}",0.9963811821471652
89560,"/** 
 * test constructor behaviour given bad arguments. The introduction of a factory for the HeritrixLauncher hides the actual cause behind the message ""Error creating singleton of class 'dk.netarkivet.harvester.harvesting.controller.BnfHeritrixLauncher'.
 */
@Test public void testRunHarvest() throws Exception {
  HeritrixFiles files=HeritrixFiles.getH1HeritrixFilesWithDefaultJmxFiles(new File(TestInfo.WORKING_DIR,""String_Node_Str""),new JobInfoTestImpl(42L,23L));
  hc=HarvestController.getInstance();
  String cause=""String_Node_Str"" + ""String_Node_Str"";
  try {
    hc.runHarvest(files);
    fail(""String_Node_Str"");
  }
 catch (  IOFailure e) {
    System.out.println(""String_Node_Str"" + e.getMessage());
    StringAsserts.assertStringContains(""String_Node_Str"",""String_Node_Str"",e.getMessage());
  }
catch (  ArgumentNotValid e) {
    StringAsserts.assertStringMatches(""String_Node_Str"",cause,e.getMessage());
  }
}","/** 
 * test constructor behaviour given bad arguments. The introduction of a factory for the HeritrixLauncher hides the actual cause behind the message ""Error creating singleton of class 'dk.netarkivet.harvester.harvesting.controller.BnfHeritrixLauncher'.
 */
@Test public void testRunHarvest() throws Exception {
  HeritrixFiles files=HeritrixFiles.getH1HeritrixFilesWithDefaultJmxFiles(new File(Heritrix1ControllerTestInfo.WORKING_DIR,""String_Node_Str""),new JobInfoTestImpl(42L,23L));
  hc=HarvestController.getInstance();
  String cause=""String_Node_Str"" + ""String_Node_Str"";
  try {
    hc.runHarvest(files);
    fail(""String_Node_Str"");
  }
 catch (  IOFailure e) {
    System.out.println(""String_Node_Str"" + e.getMessage());
    StringAsserts.assertStringContains(""String_Node_Str"",""String_Node_Str"",e.getMessage());
  }
catch (  ArgumentNotValid e) {
    StringAsserts.assertStringMatches(""String_Node_Str"",cause,e.getMessage());
  }
}",0.9898232458489556
89561,"/** 
 * Test that the launcher handles an empty order file correctly.
 */
@Test public void testStartEmptyFile(){
  HeritrixLauncher hl=getHeritrixLauncher(Heritrix1ControllerTestInfo.EMPTY_ORDER_FILE,null);
  try {
    hl.doCrawl();
    fail(""String_Node_Str"");
  }
 catch (  IOFailure e) {
  }
}","/** 
 * Test that the launcher handles an empty order file correctly.
 */
@Test public void testStartEmptyFile(){
  HeritrixLauncher hl=getHeritrixLauncher(Heritrix1ControllerTestInfo.EMPTY_ORDER_FILE,null);
  try {
    hl.doCrawl();
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
}",0.9584026622296172
89562,"/** 
 * This method is used to test various scenarios with bad order-files.
 * @param orderfile
 */
private void myTesterOfBadOrderfiles(File orderfile){
  HeritrixLauncher hl=getHeritrixLauncher(orderfile,null);
  try {
    hl.doCrawl();
    fail(""String_Node_Str"");
  }
 catch (  IOFailure e) {
  }
catch (  IllegalState e) {
  }
}","/** 
 * This method is used to test various scenarios with bad order-files.
 * @param orderfile
 */
private void myTesterOfBadOrderfiles(File orderfile){
  try {
    HeritrixLauncher hl=getHeritrixLauncher(orderfile,null);
    hl.doCrawl();
    fail(""String_Node_Str"");
  }
 catch (  IOFailure e) {
  }
catch (  IllegalState e) {
  }
catch (  ArgumentNotValid e) {
  }
}",0.92176386913229
89563,"public H1HeritrixTemplate(Clob clob) throws SQLException {
  Document doc;
  try {
    SAXReader reader=new SAXReader();
    doc=reader.read(clob.getCharacterStream());
  }
 catch (  DocumentException e) {
    String errMsg=""String_Node_Str"" + clob.getSubString(1,(int)clob.length());
    log.warn(errMsg,e);
    throw new IOFailure(errMsg,e);
  }
  this.template=doc;
}","public H1HeritrixTemplate(String templateAsString) throws DocumentException {
  ArgumentNotValid.checkNotNull(templateAsString,""String_Node_Str"");
  this.template=XmlUtils.documentFromString(templateAsString);
}",0.2237521514629948
89564,"/** 
 * Alternate constructor, taking a clob as an argument.
 * @param clob The template as SQL CLOB
 */
public H3HeritrixTemplate(Clob clob){
  StringBuilder sb=new StringBuilder();
  try {
    Reader reader=clob.getCharacterStream();
    BufferedReader br=new BufferedReader(reader);
    String line;
    while (null != (line=br.readLine())) {
      sb.append(line);
      sb.append(""String_Node_Str"");
    }
    br.close();
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
  this.template=sb.toString();
}","/** 
 * Constructor for HeritrixTemplate class.
 * @param doc the order.xml
 * @param verify If true, verifies if the given dom4j Document contains the elements required by our software.
 * @throws ArgumentNotValid if doc is null, or verify is true and doc does not obey the constraints required by oursoftware.
 */
public H3HeritrixTemplate(String template){
  ArgumentNotValid.checkNotNull(template,""String_Node_Str"");
  this.template=template;
}",0.1408450704225352
89565,"@Override public void writeTemplate(OutputStream os) throws IOException, ArgumentNotValid {
  XMLWriter writer;
  try {
    writer=new XMLWriter(os);
    writer.write(this.template);
  }
 catch (  UnsupportedEncodingException e) {
    String errMsg=""String_Node_Str"";
    log.error(errMsg,e);
    throw new ArgumentNotValid(errMsg,e);
  }
}","@Override public void writeTemplate(JspWriter out) throws IOFailure {
  try {
    out.write(template.asXML());
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
}",0.5553470919324578
89566,"@Override public void writeTemplate(OutputStream os) throws IOFailure {
  String templateWithTraps=writeCrawlerTrapsToTemplate();
  try {
    os.write(templateWithTraps.getBytes(Charset.forName(""String_Node_Str"")));
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
}","@Override public void writeTemplate(JspWriter out) throws IOFailure {
  try {
    out.write(template);
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
}",0.7122153209109731
89567,"public abstract void writeTemplate(OutputStream os) throws IOException, ArgumentNotValid ;",public abstract void writeTemplate(JspWriter out);,0.5571428571428572
89568,"/** 
 * Builds a new message.
 * @param harvestChannelName the channel of jobs crawled by the sender.
 * @param applicationInstanceId the sender's application instance ID.
 */
public HarvesterReadyMessage(String applicationInstanceId,String harvestChannelName){
  super(HarvesterChannels.getHarvesterStatusChannel(),Channels.getError());
  this.applicationInstanceId=applicationInstanceId;
  this.harvestChannelName=harvestChannelName;
}","/** 
 * Builds a new message.
 * @param harvestChannelName the channel of jobs crawled by the sender.
 * @param applicationInstanceId the sender's application instance ID.
 */
public HarvesterReadyMessage(String applicationInstanceId,String harvestChannelName){
  super(HarvesterChannels.getHarvesterStatusChannel(),Channels.getError());
  this.applicationInstanceId=applicationInstanceId;
  this.harvestChannelName=harvestChannelName;
  this.hostName=SystemUtils.getLocalHostName();
}",0.9479392624728852
89569,"public HarvesterRegistrationRequest(final String harvestChannelName,final String instanceId){
  super(HarvesterChannels.getHarvesterRegistrationRequestChannel(),Channels.getError());
  this.harvestChannelName=harvestChannelName;
  this.instanceId=instanceId;
}","public HarvesterRegistrationRequest(final String harvestChannelName,final String instanceId){
  super(HarvesterChannels.getHarvesterRegistrationRequestChannel(),Channels.getError());
  this.harvestChannelName=harvestChannelName;
  this.instanceId=instanceId;
  this.hostname=SystemUtils.getLocalHostName();
}",0.9154929577464788
89570,"@Override public void visit(HarvesterRegistrationRequest msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  String harvesterInstanceId=msg.getInstanceId();
  String channelName=msg.getHarvestChannelName();
  boolean isSnapshot=true;
  boolean isValid=true;
  try {
    HarvestChannel chan=harvestChannelDao.getByName(channelName);
    isSnapshot=chan.isSnapshot();
  }
 catch (  UnknownID e) {
    isValid=false;
  }
  if (isValid) {
    harvestChannelRegistry.register(channelName,harvesterInstanceId);
  }
  jmsConnection.send(new HarvesterRegistrationResponse(channelName,isValid,isSnapshot));
  log.info(""String_Node_Str"",channelName,(isValid ? ""String_Node_Str"" : ""String_Node_Str""));
}","@Override public void visit(HarvesterRegistrationRequest msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  String harvesterInstanceId=msg.getInstanceId();
  String channelName=msg.getHarvestChannelName();
  boolean isSnapshot=true;
  boolean isValid=true;
  try {
    HarvestChannel chan=harvestChannelDao.getByName(channelName);
    isSnapshot=chan.isSnapshot();
  }
 catch (  UnknownID e) {
    isValid=false;
  }
  if (isValid) {
    harvestChannelRegistry.register(channelName,harvesterInstanceId);
  }
  jmsConnection.send(new HarvesterRegistrationResponse(channelName,isValid,isSnapshot));
  log.info(""String_Node_Str"",msg.getHostname(),channelName,(isValid ? ""String_Node_Str"" : ""String_Node_Str""));
}",0.9719495091164096
89571,"/** 
 * Return relevant values as header-like fields (here ANVLRecord, but spec-defined ""application/warc-fields"" type when written). Field names from from DCMI Terms and the WARC/0.17 specification.
 * @see org.archive.crawler.framework.WriterPoolProcessor#getFirstrecordBody(java.io.File)
 */
@Override protected String getFirstrecordBody(File orderFile){
  ANVLRecord record=new ANVLRecord(7);
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"" + Heritrix.getVersion() + ""String_Node_Str"");
  try {
    InetAddress host=InetAddress.getLocalHost();
    record.addLabelValue(""String_Node_Str"",host.getHostAddress());
    record.addLabelValue(""String_Node_Str"",host.getCanonicalHostName());
  }
 catch (  UnknownHostException e) {
    logger.log(Level.WARNING,""String_Node_Str"",e);
  }
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"");
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"");
  try {
    Document doc=XmlUtils.getDocument(orderFile);
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    if (metadataMap == null) {
      metadataMap=getMetadataItems();
    }
  }
 catch (  IOException e) {
    logger.log(Level.WARNING,""String_Node_Str"",e);
  }
catch (  AttributeNotFoundException e) {
    e.printStackTrace();
  }
catch (  MBeanException e) {
    e.printStackTrace();
  }
catch (  ReflectionException e) {
    e.printStackTrace();
  }
  String netarchiveSuiteComment=""String_Node_Str"" + dk.netarkivet.common.Constants.getVersionString();
  ANVLRecord recordNAS=new ANVLRecord(7);
  recordNAS.addLabelValue(HARVESTINFO_VERSION,(String)metadataMap.get(HARVESTINFO_VERSION));
  recordNAS.addLabelValue(HARVESTINFO_JOBID,(String)metadataMap.get(HARVESTINFO_JOBID));
  recordNAS.addLabelValue(HARVESTINFO_CHANNEL,(String)metadataMap.get(HARVESTINFO_CHANNEL));
  recordNAS.addLabelValue(HARVESTINFO_HARVESTNUM,(String)metadataMap.get(HARVESTINFO_HARVESTNUM));
  recordNAS.addLabelValue(HARVESTINFO_ORIGHARVESTDEFINITIONID,(String)metadataMap.get(HARVESTINFO_ORIGHARVESTDEFINITIONID));
  recordNAS.addLabelValue(HARVESTINFO_MAXBYTESPERDOMAIN,(String)metadataMap.get(HARVESTINFO_MAXBYTESPERDOMAIN));
  recordNAS.addLabelValue(HARVESTINFO_MAXOBJECTSPERDOMAIN,(String)metadataMap.get(HARVESTINFO_MAXOBJECTSPERDOMAIN));
  recordNAS.addLabelValue(HARVESTINFO_ORDERXMLNAME,(String)metadataMap.get(HARVESTINFO_ORDERXMLNAME));
  recordNAS.addLabelValue(HARVESTINFO_ORIGHARVESTDEFINITIONNAME,(String)metadataMap.get(HARVESTINFO_ORIGHARVESTDEFINITIONNAME));
  if (metadataMap.containsKey((HARVESTINFO_SCHEDULENAME))) {
    recordNAS.addLabelValue(HARVESTINFO_SCHEDULENAME,(String)metadataMap.get(HARVESTINFO_SCHEDULENAME));
  }
  recordNAS.addLabelValue(HARVESTINFO_HARVESTFILENAMEPREFIX,(String)metadataMap.get(HARVESTINFO_HARVESTFILENAMEPREFIX));
  recordNAS.addLabelValue(HARVESTINFO_JOBSUBMITDATE,(String)metadataMap.get(HARVESTINFO_JOBSUBMITDATE));
  if (metadataMap.containsKey(HARVESTINFO_PERFORMER)) {
    recordNAS.addLabelValue(HARVESTINFO_PERFORMER,(String)metadataMap.get(HARVESTINFO_PERFORMER));
  }
  if (metadataMap.containsKey(HARVESTINFO_AUDIENCE)) {
    recordNAS.addLabelValue(HARVESTINFO_AUDIENCE,(String)metadataMap.get(HARVESTINFO_AUDIENCE));
  }
  return record.toString() + netarchiveSuiteComment + ""String_Node_Str""+ recordNAS.toString();
}","/** 
 * Return relevant values as header-like fields (here ANVLRecord, but spec-defined ""application/warc-fields"" type when written). Field names from from DCMI Terms and the WARC/0.17 specification.
 * @see org.archive.crawler.framework.WriterPoolProcessor#getFirstrecordBody(java.io.File)
 */
@Override protected String getFirstrecordBody(File orderFile){
  ANVLRecord record=new ANVLRecord(7);
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"" + Heritrix.getVersion() + ""String_Node_Str"");
  try {
    InetAddress host=InetAddress.getLocalHost();
    record.addLabelValue(""String_Node_Str"",host.getHostAddress());
    record.addLabelValue(""String_Node_Str"",host.getCanonicalHostName());
  }
 catch (  UnknownHostException e) {
    logger.log(Level.WARNING,""String_Node_Str"",e);
  }
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"");
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"");
  try {
    Document doc=XmlUtils.getDocument(orderFile);
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    if (metadataMap == null) {
      metadataMap=getMetadataItems();
    }
  }
 catch (  IOException e) {
    logger.log(Level.WARNING,""String_Node_Str"",e);
  }
catch (  AttributeNotFoundException e) {
    logger.log(Level.WARNING,""String_Node_Str"",e);
  }
catch (  MBeanException e) {
    logger.log(Level.WARNING,""String_Node_Str"",e);
  }
catch (  ReflectionException e) {
    logger.log(Level.WARNING,""String_Node_Str"",e);
  }
  String netarchiveSuiteComment=""String_Node_Str"" + dk.netarkivet.common.Constants.getVersionString();
  ANVLRecord recordNAS=new ANVLRecord(7);
  recordNAS.addLabelValue(HARVESTINFO_VERSION,(String)metadataMap.get(HARVESTINFO_VERSION));
  recordNAS.addLabelValue(HARVESTINFO_JOBID,(String)metadataMap.get(HARVESTINFO_JOBID));
  recordNAS.addLabelValue(HARVESTINFO_CHANNEL,(String)metadataMap.get(HARVESTINFO_CHANNEL));
  recordNAS.addLabelValue(HARVESTINFO_HARVESTNUM,(String)metadataMap.get(HARVESTINFO_HARVESTNUM));
  recordNAS.addLabelValue(HARVESTINFO_ORIGHARVESTDEFINITIONID,(String)metadataMap.get(HARVESTINFO_ORIGHARVESTDEFINITIONID));
  recordNAS.addLabelValue(HARVESTINFO_MAXBYTESPERDOMAIN,(String)metadataMap.get(HARVESTINFO_MAXBYTESPERDOMAIN));
  recordNAS.addLabelValue(HARVESTINFO_MAXOBJECTSPERDOMAIN,(String)metadataMap.get(HARVESTINFO_MAXOBJECTSPERDOMAIN));
  recordNAS.addLabelValue(HARVESTINFO_ORDERXMLNAME,(String)metadataMap.get(HARVESTINFO_ORDERXMLNAME));
  recordNAS.addLabelValue(HARVESTINFO_ORIGHARVESTDEFINITIONNAME,(String)metadataMap.get(HARVESTINFO_ORIGHARVESTDEFINITIONNAME));
  if (metadataMap.containsKey((HARVESTINFO_SCHEDULENAME))) {
    recordNAS.addLabelValue(HARVESTINFO_SCHEDULENAME,(String)metadataMap.get(HARVESTINFO_SCHEDULENAME));
  }
  recordNAS.addLabelValue(HARVESTINFO_HARVESTFILENAMEPREFIX,(String)metadataMap.get(HARVESTINFO_HARVESTFILENAMEPREFIX));
  recordNAS.addLabelValue(HARVESTINFO_JOBSUBMITDATE,(String)metadataMap.get(HARVESTINFO_JOBSUBMITDATE));
  if (metadataMap.containsKey(HARVESTINFO_PERFORMER)) {
    recordNAS.addLabelValue(HARVESTINFO_PERFORMER,(String)metadataMap.get(HARVESTINFO_PERFORMER));
  }
  if (metadataMap.containsKey(HARVESTINFO_AUDIENCE)) {
    recordNAS.addLabelValue(HARVESTINFO_AUDIENCE,(String)metadataMap.get(HARVESTINFO_AUDIENCE));
  }
  return record.toString() + netarchiveSuiteComment + ""String_Node_Str""+ recordNAS.toString();
}",0.9646616541353384
89572,"private void doUpdateChecksumAndFileStatus() throws Exception {
  Long stepTimeout=24 * 3600 * 1000L;
  String minStepTimeHoursString=System.getProperty(""String_Node_Str"",""String_Node_Str"");
  System.out.println(""String_Node_Str"" + minStepTimeHoursString + ""String_Node_Str"");
  Long minStepTime=Integer.parseInt(minStepTimeHoursString) * 3600 * 1000L;
  WebDriver driver=new FirefoxDriver();
  ApplicationManager applicationManager=new ApplicationManager(environmentManager);
  driver.manage().timeouts().implicitlyWait(1,TimeUnit.SECONDS);
  String baseUrl=environmentManager.getGuiHost() + ""String_Node_Str"" + environmentManager.getGuiPort();
  PageHelper.initialize(driver,baseUrl);
  applicationManager.waitForGUIToStart(60);
  addFixture(""String_Node_Str"");
  addStep(""String_Node_Str"",""String_Node_Str"");
  driver.manage().timeouts().pageLoadTimeout(10L,TimeUnit.MINUTES);
  driver.findElement(By.linkText(""String_Node_Str"")).click();
  WebElement updateLink=driver.findElement(By.linkText(""String_Node_Str""));
  updateLink.click();
  Long startTime=System.currentTimeMillis();
  String progressS=""String_Node_Str"";
  Pattern progress=Pattern.compile(progressS,Pattern.DOTALL);
  String finishedS=""String_Node_Str"";
  Pattern finished=Pattern.compile(finishedS,Pattern.DOTALL);
  boolean isFinished=false;
  while (!isFinished) {
    Long timeElapsed=System.currentTimeMillis() - startTime;
    if (timeElapsed > stepTimeout) {
      fail(""String_Node_Str"" + stepTimeout / 1000 + ""String_Node_Str"");
    }
    driver.findElement(By.linkText(""String_Node_Str"")).click();
    List<WebElement> logMessages=driver.findElements(By.tagName(""String_Node_Str""));
    for (    WebElement webElement : logMessages) {
      if (finished.matcher(webElement.getText()).matches()) {
        isFinished=true;
        if (timeElapsed < minStepTime) {
          fail(""String_Node_Str"" + timeElapsed / 1000 + ""String_Node_Str"" + minStepTime / 1000 + ""String_Node_Str"");
        }
        TestEventManager.getInstance().addResult(""String_Node_Str"" + timeElapsed / 1000 + ""String_Node_Str"");
      }
    }
    if (!isFinished) {
      for (      WebElement webElement : logMessages) {
        Matcher matcher=progress.matcher(webElement.getText());
        if (matcher.matches()) {
          System.out.println(""String_Node_Str"" + matcher.group(1) + ""String_Node_Str""+ timeElapsed / 1000 + ""String_Node_Str"");
        }
      }
    }
    if (!isFinished) {
      Thread.sleep(10 * 1000L);
    }
  }
  driver.close();
}","private void doUpdateChecksumAndFileStatus() throws Exception {
  Long stepTimeout=24 * 3600 * 1000L;
  String minStepTimeHoursString=System.getProperty(""String_Node_Str"",""String_Node_Str"");
  System.out.println(""String_Node_Str"" + minStepTimeHoursString + ""String_Node_Str"");
  Long minStepTime=Integer.parseInt(minStepTimeHoursString) * 3600 * 1000L;
  UpdateChecksumJob updateChecksumJob=new UpdateChecksumJob(this,new ApplicationManager(environmentManager),new FirefoxDriver(),60 * 1000L,10 * 1000L,stepTimeout,""String_Node_Str"");
  updateChecksumJob.run();
}",0.2503259452411995
89573,"@Override void startJob(){
  driver.manage().timeouts().implicitlyWait(1,TimeUnit.SECONDS);
  String baseUrl=environmentManager.getGuiHost() + ""String_Node_Str"" + environmentManager.getGuiPort();
  PageHelper.initialize(driver,baseUrl);
  addStep(""String_Node_Str"",""String_Node_Str"");
  driver.manage().timeouts().pageLoadTimeout(20L,TimeUnit.MINUTES);
  driver.findElement(By.linkText(""String_Node_Str"")).click();
  WebElement updateLink=driver.findElement(By.linkText(""String_Node_Str""));
  updateLink.click();
}","@Override void startJob(){
  driver.manage().timeouts().pageLoadTimeout(10L,TimeUnit.MINUTES);
  driver.findElement(By.linkText(""String_Node_Str"")).click();
  WebElement updateLink=driver.findElement(By.linkText(""String_Node_Str""));
  updateLink.click();
}",0.6311688311688312
89574,"@Override boolean isStarted(){
  return true;
}","@Override boolean isStarted(){
  try {
    String output=environmentManager.runCommand(TestEnvironment.JOB_ADMIN_SERVER,""String_Node_Str"" + TESTNAME + ""String_Node_Str"",new int[]{0,1});
    final String startedS=""String_Node_Str"";
    Pattern startedP=Pattern.compile(startedS,Pattern.DOTALL);
    final Matcher matcher=startedP.matcher(output);
    if (matcher.matches()) {
      total=matcher.group(1);
      return true;
    }
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
  return false;
}",0.1209964412811387
89575,"@Override boolean isFinished(){
  driver.manage().timeouts().pageLoadTimeout(20L,TimeUnit.MINUTES);
  driver.findElement(By.linkText(""String_Node_Str"")).click();
  WebElement updateLink=driver.findElement(By.linkText(""String_Node_Str""));
  String idNumber=""String_Node_Str"";
  String idMissing=""String_Node_Str"";
  String numberS=driver.findElement(By.id(idNumber)).getText();
  return numberS.equals(""String_Node_Str"");
}","@Override boolean isFinished(){
  try {
    String output=environmentManager.runCommand(TestEnvironment.JOB_ADMIN_SERVER,""String_Node_Str"" + TESTNAME + ""String_Node_Str"",new int[]{0,1});
    final String finishedS=""String_Node_Str"";
    Pattern finishedP=Pattern.compile(finishedS,Pattern.DOTALL);
    final Matcher matcher=finishedP.matcher(output);
    if (matcher.matches()) {
      total=matcher.group(1);
      return true;
    }
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
  return false;
}",0.1528662420382165
89576,"@Override String getProgress(){
  String numberS=driver.findElement(By.id(idNumber)).getText();
  String missingS=driver.findElement(By.id(idMissing)).getText();
  return ""String_Node_Str"" + numberS + ""String_Node_Str""+ missingS;
}","@Override String getProgress(){
  driver.findElement(By.linkText(""String_Node_Str"")).click();
  driver.findElement(By.linkText(""String_Node_Str"")).click();
  final String progressS=""String_Node_Str"";
  Pattern progressP=Pattern.compile(progressS,Pattern.DOTALL);
  WebElement webElement=driver.findElement(By.tagName(""String_Node_Str""));
  if (webElement != null) {
    Matcher matcher=progressP.matcher(webElement.getText());
    if (matcher.matches()) {
      return ""String_Node_Str"" + matcher.group(1) + ""String_Node_Str""+ total;
    }
  }
  return null;
}",0.3716814159292035
89577,"@Override void startJob(){
  driver.manage().timeouts().implicitlyWait(1,TimeUnit.SECONDS);
  String baseUrl=environmentManager.getGuiHost() + ""String_Node_Str"" + environmentManager.getGuiPort();
  PageHelper.initialize(driver,baseUrl);
  addStep(""String_Node_Str"",""String_Node_Str"");
  driver.manage().timeouts().pageLoadTimeout(20L,TimeUnit.MINUTES);
  driver.findElement(By.linkText(""String_Node_Str"")).click();
  WebElement updateLink=driver.findElement(By.linkText(""String_Node_Str""));
}","@Override void startJob(){
  driver.manage().timeouts().implicitlyWait(1,TimeUnit.SECONDS);
  String baseUrl=environmentManager.getGuiHost() + ""String_Node_Str"" + environmentManager.getGuiPort();
  PageHelper.initialize(driver,baseUrl);
  addStep(""String_Node_Str"",""String_Node_Str"");
  driver.manage().timeouts().pageLoadTimeout(20L,TimeUnit.MINUTES);
  driver.findElement(By.linkText(""String_Node_Str"")).click();
  WebElement updateLink=driver.findElement(By.linkText(""String_Node_Str""));
  updateLink.click();
}",0.978131212723658
89578,"/** 
 * When nodata is true, restore schema only PLUS the ordertemplates table in the harvestdb.
 * @param nodata
 * @throws Exception
 */
protected void replaceDatabasesWithProd(boolean nodata) throws Exception {
  String dropAdminDB=""String_Node_Str"";
  String createAdminDB=""String_Node_Str"";
  String dropHarvestDB=""String_Node_Str"";
  String createHarvestDB=""String_Node_Str"";
  addFixture(""String_Node_Str"");
  environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,dropHarvestDB);
  environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,createHarvestDB);
  addFixture(""String_Node_Str"");
  environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,dropAdminDB);
  environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,createAdminDB);
  addFixture(""String_Node_Str"");
  if (nodata) {
    addFixture(""String_Node_Str"");
    String createRelationsAdminDB=""String_Node_Str"";
    String populateSchemaversionsAdminDB=""String_Node_Str"";
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,createRelationsAdminDB);
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,populateSchemaversionsAdminDB);
    addFixture(""String_Node_Str"");
    String createRelationsHarvestDB=""String_Node_Str"";
    String populateSchemaVersionsHarvestDB=""String_Node_Str"";
    String populateOrdertemplatesHarvestDB=""String_Node_Str"";
    String populateSchedulesDB=""String_Node_Str"";
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,createRelationsHarvestDB);
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,populateSchemaVersionsHarvestDB);
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,populateOrdertemplatesHarvestDB);
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,populateSchedulesDB);
  }
 else {
    addFixture(""String_Node_Str"");
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,""String_Node_Str"" + TESTNAME.toLowerCase() + ""String_Node_Str"");
    addFixture(""String_Node_Str"");
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,""String_Node_Str"" + TESTNAME.toLowerCase() + ""String_Node_Str"");
  }
  addFixture(""String_Node_Str"");
  environmentManager.runTestXCommand(TestEnvironment.CHECKSUM_SERVER,""String_Node_Str"");
  environmentManager.runTestXCommand(TestEnvironment.CHECKSUM_SERVER,""String_Node_Str"");
}","/** 
 * When nodata is true, restore schema only PLUS the ordertemplates table in the harvestdb.
 * @param nodata
 * @throws Exception
 */
protected void replaceDatabasesWithProd(boolean nodata) throws Exception {
  String dropAdminDB=""String_Node_Str"";
  String createAdminDB=""String_Node_Str"";
  String dropHarvestDB=""String_Node_Str"";
  String createHarvestDB=""String_Node_Str"";
  addFixture(""String_Node_Str"");
  environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,dropHarvestDB);
  environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,createHarvestDB);
  addFixture(""String_Node_Str"");
  environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,dropAdminDB);
  environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,createAdminDB);
  addFixture(""String_Node_Str"");
  if (nodata) {
    addFixture(""String_Node_Str"");
    String createRelationsAdminDB=""String_Node_Str"";
    String populateSchemaversionsAdminDB=""String_Node_Str"";
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,createRelationsAdminDB);
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,populateSchemaversionsAdminDB);
    addFixture(""String_Node_Str"");
    String createRelationsHarvestDB=""String_Node_Str"";
    String populateSchemaVersionsHarvestDB=""String_Node_Str"";
    String populateOrdertemplatesHarvestDB=""String_Node_Str"";
    String populateSchedulesDB=""String_Node_Str"";
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,createRelationsHarvestDB);
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,populateSchemaVersionsHarvestDB);
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,populateOrdertemplatesHarvestDB);
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,populateSchedulesDB);
    addFixture(""String_Node_Str"");
    environmentManager.runTestXCommand(TestEnvironment.CHECKSUM_SERVER,""String_Node_Str"");
    environmentManager.runTestXCommand(TestEnvironment.CHECKSUM_SERVER,""String_Node_Str"");
    environmentManager.runTestXCommand(TestEnvironment.CHECKSUM_SERVER,""String_Node_Str"");
  }
 else {
    addFixture(""String_Node_Str"");
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,""String_Node_Str"" + TESTNAME.toLowerCase() + ""String_Node_Str"");
    addFixture(""String_Node_Str"");
    environmentManager.runTestXCommand(TestEnvironment.JOB_ADMIN_SERVER,""String_Node_Str"" + TESTNAME.toLowerCase() + ""String_Node_Str"");
    addFixture(""String_Node_Str"");
    environmentManager.runTestXCommand(TestEnvironment.CHECKSUM_SERVER,""String_Node_Str"");
    environmentManager.runTestXCommand(TestEnvironment.CHECKSUM_SERVER,""String_Node_Str"");
  }
}",0.8578052550231839
89579,"@BeforeTest(alwaysRun=true) protected void setupTest(){
  environmentManager=new TestEnvironmentManager(TESTNAME,null,8073);
}","@BeforeTest(alwaysRun=true) protected void setupTest(){
  environmentManager=new TestEnvironmentManager(TESTNAME,""String_Node_Str"",8073);
}",0.9283018867924528
89580,"/** 
 * Iterates over the (W)ARC files in the given dir and moves away files that do not belong to the given job into a ""lost-files"" directory under oldjobs  named with a timestamp.
 * @param archiveProfile archive profile including filters, patterns, etc.
 * @param dir A directory containing one or more (W)ARC files.
 * @param files Information about the files produced by heritrix (jobId and harvestnamePrefix)
 */
private static void moveAwayForeignFiles(ArchiveProfile archiveProfile,File dir,IngestableFiles files){
  File[] archiveFiles=dir.listFiles(archiveProfile.filename_filter);
  File oldJobsDir=new File(Settings.get(HarvesterSettings.HARVEST_CONTROLLER_OLDJOBSDIR));
  File lostfilesDir=new File(oldJobsDir,""String_Node_Str"" + new Date().getTime());
  List<File> movedFiles=new ArrayList<File>();
  log.info(""String_Node_Str"" + files.getHarvestnamePrefix() + ""String_Node_Str"");
  for (  File archiveFile : archiveFiles) {
    if (!(archiveFile.getName().startsWith(files.getHarvestnamePrefix()))) {
      System.out.println(""String_Node_Str"" + archiveFile.getAbsolutePath());
      try {
        if (!lostfilesDir.exists()) {
          FileUtils.createDir(lostfilesDir);
          File moveTo=new File(lostfilesDir,archiveFile.getName());
          archiveFile.renameTo(moveTo);
          movedFiles.add(moveTo);
        }
      }
 catch (      PermissionDenied e) {
        log.warn(""String_Node_Str"" + lostfilesDir.getAbsolutePath() + ""String_Node_Str"",e);
      }
    }
  }
  if (!movedFiles.isEmpty()) {
    log.warn(""String_Node_Str"" + files.getJobId() + ""String_Node_Str""+ movedFiles);
  }
}","/** 
 * Iterates over the (W)ARC files in the given dir and moves away files that do not belong to the given job into a ""lost-files"" directory under oldjobs  named with a timestamp.
 * @param archiveProfile archive profile including filters, patterns, etc.
 * @param dir A directory containing one or more (W)ARC files.
 * @param files Information about the files produced by heritrix (jobId and harvestnamePrefix)
 */
private static void moveAwayForeignFiles(ArchiveProfile archiveProfile,File dir,IngestableFiles files){
  File[] archiveFiles=dir.listFiles(archiveProfile.filename_filter);
  File oldJobsDir=new File(Settings.get(HarvesterSettings.HARVEST_CONTROLLER_OLDJOBSDIR));
  File lostfilesDir=new File(oldJobsDir,""String_Node_Str"" + new Date().getTime());
  List<File> movedFiles=new ArrayList<File>();
  log.info(""String_Node_Str"" + files.getHarvestnamePrefix() + ""String_Node_Str"");
  for (  File archiveFile : archiveFiles) {
    if (!(archiveFile.getName().startsWith(files.getHarvestnamePrefix()))) {
      log.info(""String_Node_Str"" + archiveFile.getAbsolutePath());
      try {
        if (!lostfilesDir.exists()) {
          FileUtils.createDir(lostfilesDir);
        }
        File moveTo=new File(lostfilesDir,archiveFile.getName());
        archiveFile.renameTo(moveTo);
        movedFiles.add(moveTo);
      }
 catch (      PermissionDenied e) {
        log.warn(""String_Node_Str"" + lostfilesDir.getAbsolutePath() + ""String_Node_Str"",e);
      }
    }
  }
  if (!movedFiles.isEmpty()) {
    log.warn(""String_Node_Str"" + files.getJobId() + ""String_Node_Str""+ movedFiles);
  }
}",0.9850560398505604
89581,"public void testMoveAwayForeignFiles() throws Exception {
  Method m=ReflectUtils.getPrivateMethod(HarvestDocumentation.class,""String_Node_Str"",ArchiveProfile.class,File.class,IngestableFiles.class);
  Settings.set(HarvesterSettings.HARVEST_CONTROLLER_OLDJOBSDIR,new File(TestInfo.WORKING_DIR,""String_Node_Str"").getAbsolutePath());
  TestInfo.WORKING_DIR.mkdirs();
  File arcsDir=new File(TestInfo.WORKING_DIR,Constants.ARCDIRECTORY_NAME);
  TestFileUtils.copyDirectoryNonCVS(TestInfo.METADATA_TEST_DIR_INCONSISTENT,arcsDir);
  FileUtils.copyFile(new File(TestInfo.METADATA_TEST_DIR,""String_Node_Str""),new File(arcsDir,""String_Node_Str""));
  JobInfo harvestJob=new JobInfoTestImpl(42L,117L);
  HeritrixFiles files117=new HeritrixFiles(TestInfo.WORKING_DIR,harvestJob);
  OkIngestables=new IngestableFiles(files117);
  m.invoke(null,ArchiveProfile.ARC_PROFILE,arcsDir,OkIngestables);
  LogUtils.flushLogs(HarvestDocumentation.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",TestInfo.LOG_FILE);
  String badFile=""String_Node_Str"";
  String goodFile=""String_Node_Str"";
  System.out.println(""String_Node_Str"");
  File oddjobsDir=new File(TestInfo.WORKING_DIR,""String_Node_Str"");
  File[] oddDirs=oddjobsDir.listFiles();
  assertTrue(oddDirs.length == 1);
  for (  File oddDir : oddDirs) {
    System.out.println(oddDir.getAbsolutePath());
  }
  File oldJobsDir43=oddDirs[0];
  assertTrue(""String_Node_Str"",oldJobsDir43.getName().startsWith(""String_Node_Str""));
  File movedFile=new File(oldJobsDir43,badFile);
  assertTrue(""String_Node_Str"" + movedFile + ""String_Node_Str"",movedFile.exists());
  File goneFile=new File(TestInfo.WORKING_DIR,Constants.ARCDIRECTORY_NAME + ""String_Node_Str"" + badFile);
  assertFalse(""String_Node_Str"" + goneFile + ""String_Node_Str"",goneFile.exists());
  File keptFile=new File(TestInfo.WORKING_DIR,Constants.ARCDIRECTORY_NAME + ""String_Node_Str"" + goodFile);
  assertTrue(""String_Node_Str"" + keptFile + ""String_Node_Str"",keptFile.exists());
}","/** 
 * Test that any files not part of the current harvest are moved away to the oldjobsdir during postprocessing. This test includes a check for issue https://sbforge.org/jira/browse/NAS-2270 which resulted in at most one old file being movied away.
 * @throws Exception
 */
public void testMoveAwayForeignFiles() throws Exception {
  Method m=ReflectUtils.getPrivateMethod(HarvestDocumentation.class,""String_Node_Str"",ArchiveProfile.class,File.class,IngestableFiles.class);
  Settings.set(HarvesterSettings.HARVEST_CONTROLLER_OLDJOBSDIR,new File(TestInfo.WORKING_DIR,""String_Node_Str"").getAbsolutePath());
  TestInfo.WORKING_DIR.mkdirs();
  File arcsDir=new File(TestInfo.WORKING_DIR,Constants.ARCDIRECTORY_NAME);
  TestFileUtils.copyDirectoryNonCVS(TestInfo.METADATA_TEST_DIR_INCONSISTENT,arcsDir);
  FileUtils.copyFile(new File(TestInfo.METADATA_TEST_DIR,""String_Node_Str""),new File(arcsDir,""String_Node_Str""));
  JobInfo harvestJob=new JobInfoTestImpl(42L,117L);
  HeritrixFiles files117=new HeritrixFiles(TestInfo.WORKING_DIR,harvestJob);
  OkIngestables=new IngestableFiles(files117);
  m.invoke(null,ArchiveProfile.ARC_PROFILE,arcsDir,OkIngestables);
  LogUtils.flushLogs(HarvestDocumentation.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",TestInfo.LOG_FILE);
  String badFile=""String_Node_Str"";
  String goodFile=""String_Node_Str"";
  System.out.println(""String_Node_Str"");
  File oddjobsDir=new File(TestInfo.WORKING_DIR,""String_Node_Str"");
  File[] oddDirs=oddjobsDir.listFiles();
  assertTrue(oddDirs.length == 1);
  for (  File oddDir : oddDirs) {
    System.out.println(oddDir.getAbsolutePath());
  }
  File oldJobsDir43=oddDirs[0];
  assertTrue(""String_Node_Str"",oldJobsDir43.getName().startsWith(""String_Node_Str""));
  File movedFile=new File(oldJobsDir43,badFile);
  assertTrue(""String_Node_Str"" + movedFile + ""String_Node_Str"",movedFile.exists());
  File goneFile=new File(TestInfo.WORKING_DIR,Constants.ARCDIRECTORY_NAME + ""String_Node_Str"" + badFile);
  assertFalse(""String_Node_Str"" + goneFile + ""String_Node_Str"",goneFile.exists());
  File keptFile=new File(TestInfo.WORKING_DIR,Constants.ARCDIRECTORY_NAME + ""String_Node_Str"" + goodFile);
  assertTrue(""String_Node_Str"" + keptFile + ""String_Node_Str"",keptFile.exists());
  File badMetadataFile=new File(TestInfo.WORKING_DIR,Constants.ARCDIRECTORY_NAME + ""String_Node_Str"");
  assertFalse(""String_Node_Str"" + badMetadataFile + ""String_Node_Str"",badMetadataFile.exists());
  File movedMetadataFile=new File(oldJobsDir43,badMetadataFile.getName());
  assertTrue(""String_Node_Str"" + movedMetadataFile + ""String_Node_Str"",movedMetadataFile.exists());
}",0.8603256212510711
89582,"/** 
 * Turns a raw CDX file for the given jobID into a metadatafile containing the CDX lines in one ARC entry per ARC file indexed. The output is put into a file called &lt;jobID&gt;-metadata-1.arc.
 * @param resultFile The CDX file returned by a ExtractCDXJob for thegiven jobID.
 * @param jobID The jobID we work on.
 * @throws IOException If an I/O error occurs, or the resultFiledoes not exist
 */
private void arcifyResultFile(File resultFile,long jobID) throws IOException {
  BufferedReader reader=new BufferedReader(new FileReader(resultFile));
  try {
    ARCWriter writer=ARCUtils.createARCWriter(new File(MetadataFileWriter.getMetadataArchiveFileName(Long.toString(jobID))));
    try {
      String line;
      ByteArrayOutputStream baos=new ByteArrayOutputStream();
      String lastFilename=null;
      FileUtils.FilenameParser parser=null;
      while ((line=reader.readLine()) != null) {
        parser=parseLine(line,jobID);
        if (parser == null) {
          continue;
        }
        if (!parser.getFilename().equals(lastFilename)) {
          writeCDXEntry(writer,parser,baos.toByteArray());
          baos.reset();
          lastFilename=parser.getFilename();
        }
        baos.write(line.getBytes());
        baos.write(""String_Node_Str"".getBytes());
      }
      if (parser != null) {
        writeCDXEntry(writer,parser,baos.toByteArray());
      }
    }
  finally {
      writer.close();
    }
  }
  finally {
    reader.close();
  }
}","/** 
 * Turns a raw CDX file for the given jobID into a metadatafile containing the CDX lines in one archive record per each ARC or WARC file indexed. The output is put into a file called &lt;jobID&gt;-metadata-1.arc.
 * @param resultFile The CDX file returned by a ExtractCDXJob for thegiven jobID.
 * @param jobID The jobID we work on.
 * @param warcmode write the data to a WARC metadata file or not.
 * @throws IOException If an I/O error occurs, or the resultFiledoes not exist
 */
private void arcifyResultFile(File resultFile,long jobID) throws IOException {
  BufferedReader reader=new BufferedReader(new FileReader(resultFile));
  File outputFile=new File(MetadataFileWriter.getMetadataArchiveFileName(Long.toString(jobID)));
  try {
    MetadataFileWriter writer=MetadataFileWriter.createWriter(outputFile);
    try {
      String line;
      ByteArrayOutputStream baos=new ByteArrayOutputStream();
      String lastFilename=null;
      FileUtils.FilenameParser parser=null;
      while ((line=reader.readLine()) != null) {
        parser=parseLine(line,jobID);
        if (parser == null) {
          continue;
        }
        if (!parser.getFilename().equals(lastFilename)) {
          writeCDXEntry(writer,parser,baos.toByteArray());
          baos.reset();
          lastFilename=parser.getFilename();
        }
        baos.write(line.getBytes());
        baos.write(""String_Node_Str"".getBytes());
      }
      if (parser != null) {
        writeCDXEntry(writer,parser,baos.toByteArray());
      }
    }
  finally {
      writer.close();
    }
  }
  finally {
    reader.close();
  }
}",0.918075422626788
89583,"/** 
 * Writes a full entry of CDX files to the ARCWriter.
 * @param writer The writer we're currently writing to.
 * @param parser The filename of all the entries stored in baos.  Thisis used to generate the URI for the entry.
 * @param bytes The bytes of the CDX records to be written under thisentry.
 * @throws IOFailure if the write fails for any reason
 */
private void writeCDXEntry(ARCWriter writer,FileUtils.FilenameParser parser,byte[] bytes) throws IOFailure {
  try {
    ByteArrayInputStream bais=new ByteArrayInputStream(bytes);
    writer.write(HarvestDocumentation.getCDXURI(parser.getHarvestID(),parser.getJobID(),parser.getTimeStamp(),parser.getSerialNo()).toString(),Constants.CDX_MIME_TYPE,SystemUtils.getLocalIP(),System.currentTimeMillis(),bytes.length,bais);
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + parser.getFilename(),e);
  }
}","/** 
 * Writes a full entry of CDX files to the ARCWriter.
 * @param writer The writer we're currently writing to.
 * @param parser The filename of all the entries stored in baos.  Thisis used to generate the URI for the entry.
 * @param bytes The bytes of the CDX records to be written under thisentry.
 * @throws IOFailure if the write fails for any reason
 */
private void writeCDXEntry(MetadataFileWriter writer,FileUtils.FilenameParser parser,byte[] bytes) throws IOFailure {
  try {
    writer.write(HarvestDocumentation.getCDXURI(parser.getHarvestID(),parser.getJobID(),parser.getTimeStamp(),parser.getSerialNo()).toString(),Constants.CDX_MIME_TYPE,SystemUtils.getLocalIP(),System.currentTimeMillis(),bytes);
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + parser.getFilename(),e);
  }
}",0.948512585812357
89584,"/** 
 * The workhorse method of this tool: Runs the batch job, copies the result, then turns the result into a proper metadata file.
 * @param args Arguments given on the command line.
 */
public void run(String... args){
  long jobID=Long.parseLong(args[0]);
  FileBatchJob job=new ExtractCDXJob();
  job.processOnlyFilesMatching(jobID + REMAINING_ARC_FILE_PATTERN);
  BatchStatus status=arcrep.batch(job,Settings.get(CommonSettings.USE_REPLICA_ID));
  if (status.hasResultFile()) {
    File resultFile=null;
    try {
      resultFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"",FileUtils.getTempDir());
      resultFile.deleteOnExit();
      status.copyResults(resultFile);
      arcifyResultFile(resultFile,jobID);
    }
 catch (    IOException e) {
      throw new IOFailure(""String_Node_Str"" + jobID,e);
    }
 finally {
      if (resultFile != null) {
        FileUtils.remove(resultFile);
      }
    }
  }
}","/** 
 * The workhorse method of this tool: Runs the batch job, copies the result, then turns the result into a proper metadata file. This method assumes that the args have already been read  by the checkArgs method, and thus jobId has been parsed,  and the isWarcOutputMode established
 * @param args Arguments given on the command line.
 */
public void run(String... args){
  long jobID=this.jobId;
  FileBatchJob job=new ArchiveExtractCDXJob();
  Settings.set(HarvesterSettings.METADATA_FORMAT,(isWarcOutputMode) ? ""String_Node_Str"" : ""String_Node_Str"");
  job.processOnlyFilesMatching(jobID + REMAINING_ARCHIVE_FILE_PATTERN);
  BatchStatus status=arcrep.batch(job,Settings.get(CommonSettings.USE_REPLICA_ID));
  if (status.hasResultFile()) {
    File resultFile=null;
    try {
      resultFile=File.createTempFile(""String_Node_Str"",""String_Node_Str"",FileUtils.getTempDir());
      resultFile.deleteOnExit();
      status.copyResults(resultFile);
      arcifyResultFile(resultFile,jobID);
    }
 catch (    IOException e) {
      throw new IOFailure(""String_Node_Str"" + jobID,e);
    }
 finally {
      if (resultFile != null) {
        FileUtils.remove(resultFile);
      }
    }
  }
}",0.8549834671705243
89585,"/** 
 * Check that a valid jobID were given.  This does not check whether jobs actually exist for that ID.
 * @param args The args given on the command line.
 * @return True if the args are legal.
 */
public boolean checkArgs(String... args){
  if (args.length < 1) {
    System.err.println(""String_Node_Str"");
    return false;
  }
  if (args.length > 1) {
    System.err.println(""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",Arrays.asList(args)) + ""String_Node_Str"");
    return false;
  }
  try {
    if (Long.parseLong(args[0]) < 1) {
      System.err.println(""String_Node_Str"" + args[0] + ""String_Node_Str"");
      return false;
    }
    return true;
  }
 catch (  NumberFormatException e) {
    System.err.println(""String_Node_Str"" + args[0] + ""String_Node_Str"");
    return false;
  }
}","/** 
 * Check that a valid jobID were given.  This does not check whether jobs actually exist for that ID.
 * @param args The args given on the command line.
 * @return True if the args are legal.
 */
public boolean checkArgs(String... args){
  Options options=new Options();
  options.addOption(""String_Node_Str"",false,""String_Node_Str"");
  options.addOption(""String_Node_Str"",false,""String_Node_Str"");
  CommandLineParser parser=new PosixParser();
  try {
    CommandLine cl=parser.parse(options,args);
    if (cl.hasOption('a') && cl.hasOption('w')) {
      System.err.println(""String_Node_Str"");
      return false;
    }
    if (cl.hasOption('a')) {
      this.isWarcOutputMode=false;
    }
 else {
      this.isWarcOutputMode=true;
    }
  }
 catch (  ParseException e) {
    System.err.println(""String_Node_Str"" + e);
    return false;
  }
  if (args.length < 1) {
    System.err.println(""String_Node_Str"");
    return false;
  }
  int jobIdIndex=args.length - 1;
  try {
    if (Long.parseLong(args[jobIdIndex]) < 1) {
      System.err.println(""String_Node_Str"" + args[jobIdIndex] + ""String_Node_Str"");
      return false;
    }
    return true;
  }
 catch (  NumberFormatException e) {
    System.err.println(""String_Node_Str"" + args[jobIdIndex] + ""String_Node_Str"");
    return false;
  }
}",0.5622032288698955
89586,"@Override public ArchiveBatchFilter getFilter(){
  return new ArchiveBatchFilter(""String_Node_Str""){
    public boolean accept(    ArchiveRecordBase record){
      return record.getHeader().getUrl().startsWith(SETUP_URL_FORMAT);
    }
  }
;
}","@Override public ArchiveBatchFilter getFilter(){
  return new ArchiveBatchFilter(""String_Node_Str""){
    public boolean accept(    ArchiveRecordBase record){
      String URL=record.getHeader().getUrl();
      if (URL == null) {
        return false;
      }
 else {
        return URL.startsWith(SETUP_URL_FORMAT);
      }
    }
  }
;
}",0.8117443868739206
89587,"public boolean accept(ArchiveRecordBase record){
  return record.getHeader().getUrl().startsWith(SETUP_URL_FORMAT);
}","public boolean accept(ArchiveRecordBase record){
  String URL=record.getHeader().getUrl();
  if (URL == null) {
    return false;
  }
 else {
    return URL.startsWith(SETUP_URL_FORMAT);
  }
}",0.7313915857605178
89588,"/** 
 * Builds a harvest definition info object.
 * @param origHarvestName the harvest definition's name
 * @param origHarvestDesc the harvest definition's comments
 * @param scheduleName the harvest definition's schedule name (only applicable for selective harvests) 
 */
public HarvestDefinitionInfo(String origHarvestName,String origHarvestDesc,String scheduleName){
  super();
  ArgumentNotValid.checkNotNullOrEmpty(origHarvestName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(origHarvestDesc,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(scheduleName,""String_Node_Str"");
  this.origHarvestName=origHarvestName;
  this.origHarvestDesc=origHarvestDesc;
  this.scheduleName=scheduleName;
}","/** 
 * Builds a harvest definition info object.
 * @param origHarvestName the harvest definition's name
 * @param origHarvestDesc the harvest definition's comments (can be empty string)
 * @param scheduleName the harvest definition's schedule name (only applicable for selective harvests) 
 */
public HarvestDefinitionInfo(String origHarvestName,String origHarvestDesc,String scheduleName){
  super();
  ArgumentNotValid.checkNotNullOrEmpty(origHarvestName,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(origHarvestDesc,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(scheduleName,""String_Node_Str"");
  this.origHarvestName=origHarvestName;
  this.origHarvestDesc=origHarvestDesc;
  this.scheduleName=scheduleName;
}",0.9798190675017396
89589,"/** 
 * Documents the harvest under the given dir in a packaged metadata arc file in a directory 'metadata' under the current dir. Only documents the files belonging to the given jobID, the rest are moved to oldjobs. In the current implementation, the documentation consists of CDX indices over all ARC files (with one CDX record per harvested ARC file), plus packaging of log files. If this method finishes without an exception, it is guaranteed that metadata is ready for upload. TODO Place preharvestmetadata in IngestableFiles-defined area TODO This method may be a good place to copy deduplicate information from the crawl log to the cdx file.
 * @param crawlDir The directory the crawl was performed in
 * @param jobID the ID of the job for this harvest
 * @param harvestID the ID of the harvestdefinition which created this job.
 * @throws ArgumentNotValid if crawlDir is null or does not exist, or ifjobID or harvestID is negative.
 * @throws IOFailure if- reading ARC files or temporary files fails - writing a file to arcFilesDir fails
 */
public static void documentHarvest(File crawlDir,long jobID,long harvestID) throws IOFailure {
  ArgumentNotValid.checkNotNull(crawlDir,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(jobID,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(harvestID,""String_Node_Str"");
  ArgumentNotValid.checkExistsDirectory(crawlDir,""String_Node_Str"");
  IngestableFiles ingestables=new IngestableFiles(crawlDir,jobID);
  if (ingestables.isMetadataReady()) {
    log.warn(""String_Node_Str"" + ingestables.getMetadataFile().getAbsolutePath() + ""String_Node_Str"");
    return;
  }
  List<File> filesAddedAndNowDeletable=null;
  try {
    MetadataFileWriter mdfw=null;
    mdfw=ingestables.getMetadataWriter();
    if (mdfw instanceof MetadataFileWriterWarc) {
      ANVLRecord infoPayload=new ANVLRecord(3);
      infoPayload.addLabelValue(""String_Node_Str"",""String_Node_Str"" + dk.netarkivet.common.Constants.getVersionString() + ""String_Node_Str""+ dk.netarkivet.common.Constants.PROJECT_WEBSITE);
      infoPayload.addLabelValue(""String_Node_Str"",SystemUtils.getLocalIP());
      infoPayload.addLabelValue(""String_Node_Str"",SystemUtils.getLocalHostName());
      infoPayload.addLabelValue(""String_Node_Str"",""String_Node_Str"");
      PersistentJobData psj=new PersistentJobData(crawlDir);
      infoPayload.addLabelValue(""String_Node_Str"",""String_Node_Str"" + psj.getJobID());
      MetadataFileWriterWarc mfww=(MetadataFileWriterWarc)mdfw;
      mfww.insertInfoRecord(infoPayload);
    }
    List<MetadataEntry> storedMetadata=getStoredMetadata(crawlDir);
    try {
      for (      MetadataEntry m : storedMetadata) {
        mdfw.write(m.getURL(),m.getMimeType(),SystemUtils.getLocalIP(),System.currentTimeMillis(),m.getData());
      }
    }
 catch (    IOException e) {
      log.warn(""String_Node_Str"",e);
    }
    filesAddedAndNowDeletable=writeHarvestDetails(jobID,harvestID,crawlDir,mdfw,Constants.getHeritrixVersionString());
    Iterator<File> iterator=filesAddedAndNowDeletable.iterator();
    while (iterator.hasNext()) {
      File f=iterator.next();
      if (f.getName().equals(""String_Node_Str"") || f.getName().equals(""String_Node_Str"") || f.getName().equals(""String_Node_Str"")) {
        iterator.remove();
      }
    }
    boolean cdxGenerationSucceeded=false;
    File arcFilesDir=ingestables.getArcsDir();
    File warcFilesDir=ingestables.getWarcsDir();
    if (arcFilesDir.exists()) {
      addCDXes(ingestables,arcFilesDir,mdfw,ArchiveProfile.ARC_PROFILE);
      cdxGenerationSucceeded=true;
    }
    if (warcFilesDir.exists()) {
      addCDXes(ingestables,arcFilesDir,mdfw,ArchiveProfile.WARC_PROFILE);
      cdxGenerationSucceeded=true;
    }
    if (cdxGenerationSucceeded) {
      ingestables.setMetadataGenerationSucceeded(true);
    }
 else {
      log.warn(""String_Node_Str"" + arcFilesDir.getAbsolutePath() + ""String_Node_Str""+ warcFilesDir.getAbsolutePath()+ ""String_Node_Str"");
    }
  }
  finally {
    if (!ingestables.isMetadataReady()) {
      ingestables.setMetadataGenerationSucceeded(false);
    }
 else {
      for (      File fileAdded : filesAddedAndNowDeletable) {
        FileUtils.remove(fileAdded);
      }
      ingestables.cleanup();
    }
  }
}","/** 
 * Documents the harvest under the given dir in a packaged metadata arc file in a directory 'metadata' under the current dir. Only documents the files belonging to the given jobID, the rest are moved to oldjobs. In the current implementation, the documentation consists of CDX indices over all ARC files (with one CDX record per harvested ARC file), plus packaging of log files. If this method finishes without an exception, it is guaranteed that metadata is ready for upload. TODO Place preharvestmetadata in IngestableFiles-defined area TODO This method may be a good place to copy deduplicate information from the crawl log to the cdx file.
 * @param crawlDir The directory the crawl was performed in
 * @param jobID the ID of the job for this harvest
 * @param harvestID the ID of the harvestdefinition which created this job.
 * @throws ArgumentNotValid if crawlDir is null or does not exist, or ifjobID or harvestID is negative.
 * @throws IOFailure if- reading ARC files or temporary files fails - writing a file to arcFilesDir fails
 */
public static void documentHarvest(File crawlDir,long jobID,long harvestID) throws IOFailure {
  ArgumentNotValid.checkNotNull(crawlDir,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(jobID,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(harvestID,""String_Node_Str"");
  ArgumentNotValid.checkExistsDirectory(crawlDir,""String_Node_Str"");
  IngestableFiles ingestables=new IngestableFiles(crawlDir,jobID);
  if (ingestables.isMetadataReady()) {
    log.warn(""String_Node_Str"" + ingestables.getMetadataFile().getAbsolutePath() + ""String_Node_Str"");
    return;
  }
  List<File> filesAddedAndNowDeletable=null;
  try {
    MetadataFileWriter mdfw=null;
    mdfw=ingestables.getMetadataWriter();
    if (mdfw instanceof MetadataFileWriterWarc) {
      ANVLRecord infoPayload=new ANVLRecord(3);
      infoPayload.addLabelValue(""String_Node_Str"",""String_Node_Str"" + dk.netarkivet.common.Constants.getVersionString() + ""String_Node_Str""+ dk.netarkivet.common.Constants.PROJECT_WEBSITE);
      infoPayload.addLabelValue(""String_Node_Str"",SystemUtils.getLocalIP());
      infoPayload.addLabelValue(""String_Node_Str"",SystemUtils.getLocalHostName());
      infoPayload.addLabelValue(""String_Node_Str"",""String_Node_Str"");
      PersistentJobData psj=new PersistentJobData(crawlDir);
      infoPayload.addLabelValue(""String_Node_Str"",""String_Node_Str"" + psj.getJobID());
      MetadataFileWriterWarc mfww=(MetadataFileWriterWarc)mdfw;
      mfww.insertInfoRecord(infoPayload);
    }
    List<MetadataEntry> storedMetadata=getStoredMetadata(crawlDir);
    try {
      for (      MetadataEntry m : storedMetadata) {
        mdfw.write(m.getURL(),m.getMimeType(),SystemUtils.getLocalIP(),System.currentTimeMillis(),m.getData());
      }
    }
 catch (    IOException e) {
      log.warn(""String_Node_Str"",e);
    }
    filesAddedAndNowDeletable=writeHarvestDetails(jobID,harvestID,crawlDir,mdfw,Constants.getHeritrixVersionString());
    Iterator<File> iterator=filesAddedAndNowDeletable.iterator();
    while (iterator.hasNext()) {
      File f=iterator.next();
      if (f.getName().equals(""String_Node_Str"") || f.getName().equals(""String_Node_Str"") || f.getName().equals(""String_Node_Str"")) {
        iterator.remove();
      }
    }
    boolean cdxGenerationSucceeded=false;
    File arcFilesDir=ingestables.getArcsDir();
    File warcFilesDir=ingestables.getWarcsDir();
    if (arcFilesDir.isDirectory() && FileUtils.hasFiles(arcFilesDir)) {
      addCDXes(ingestables,arcFilesDir,mdfw,ArchiveProfile.ARC_PROFILE);
      cdxGenerationSucceeded=true;
    }
    if (warcFilesDir.isDirectory() && FileUtils.hasFiles(warcFilesDir)) {
      addCDXes(ingestables,warcFilesDir,mdfw,ArchiveProfile.WARC_PROFILE);
      cdxGenerationSucceeded=true;
    }
    if (cdxGenerationSucceeded) {
      ingestables.setMetadataGenerationSucceeded(true);
    }
 else {
      log.warn(""String_Node_Str"" + arcFilesDir.getAbsolutePath() + ""String_Node_Str""+ warcFilesDir.getAbsolutePath()+ ""String_Node_Str"");
    }
  }
  finally {
    if (!ingestables.isMetadataReady()) {
      ingestables.setMetadataGenerationSucceeded(false);
    }
 else {
      for (      File fileAdded : filesAddedAndNowDeletable) {
        FileUtils.remove(fileAdded);
      }
      ingestables.cleanup();
    }
  }
}",0.9871615312791784
89590,"@Override public int generateJobs(HarvestDefinition harvest){
  log.info(""String_Node_Str"" + harvest.getOid());
  int jobsMade=0;
  final Iterator<DomainConfiguration> domainConfigurations=harvest.getDomainConfigurations();
  while (domainConfigurations.hasNext()) {
    List<DomainConfiguration> subset=new ArrayList<DomainConfiguration>();
    while (domainConfigurations.hasNext() && subset.size() < DOMAIN_CONFIG_SUBSET_SIZE) {
      subset.add(domainConfigurations.next());
    }
    Collections.sort(subset,getDomainConfigurationSubsetComparator(harvest));
    jobsMade+=processDomainConfigurationSubset(harvest,subset.iterator());
  }
  harvest.setNumEvents(harvest.getNumEvents() + 1);
  if (!harvest.isSnapShot()) {
    PartialHarvest focused=(PartialHarvest)harvest;
    Schedule schedule=focused.getSchedule();
    int numEvents=harvest.getNumEvents();
    Date now=new Date();
    Date nextEvent=schedule.getNextEvent(focused.getNextDate(),numEvents);
    if (nextEvent != null && nextEvent.before(now)) {
      int eventsSkipped=0;
      while (nextEvent != null && nextEvent.before(now)) {
        nextEvent=schedule.getNextEvent(nextEvent,numEvents);
        eventsSkipped++;
      }
      if (log.isWarnEnabled()) {
        log.warn(""String_Node_Str"" + harvest.getName() + ""String_Node_Str""+ eventsSkipped+ ""String_Node_Str""+ focused.getNextDate()+ ""String_Node_Str""+ nextEvent);
      }
    }
    focused.setNextDate(nextEvent);
    if (log.isTraceEnabled()) {
      log.trace(""String_Node_Str"" + harvest.getName() + ""String_Node_Str""+ (nextEvent == null ? ""String_Node_Str"" : nextEvent.toString()));
    }
  }
  return jobsMade;
}","@Override public int generateJobs(HarvestDefinition harvest){
  log.info(""String_Node_Str"" + harvest.getOid());
  int jobsMade=0;
  final Iterator<DomainConfiguration> domainConfigurations=harvest.getDomainConfigurations();
  while (domainConfigurations.hasNext()) {
    List<DomainConfiguration> subset=new ArrayList<DomainConfiguration>();
    while (domainConfigurations.hasNext() && subset.size() < DOMAIN_CONFIG_SUBSET_SIZE) {
      subset.add(domainConfigurations.next());
    }
    Collections.sort(subset,getDomainConfigurationSubsetComparator(harvest));
    jobsMade+=processDomainConfigurationSubset(harvest,subset.iterator());
  }
  harvest.setNumEvents(harvest.getNumEvents() + 1);
  if (!harvest.isSnapShot()) {
    PartialHarvest focused=(PartialHarvest)harvest;
    Schedule schedule=focused.getSchedule();
    int numEvents=harvest.getNumEvents();
    Date now=new Date();
    Date nextEvent=schedule.getNextEvent(focused.getNextDate(),numEvents);
    if (nextEvent != null && nextEvent.before(now)) {
      int eventsSkipped=0;
      while (nextEvent != null && nextEvent.before(now)) {
        nextEvent=schedule.getNextEvent(nextEvent,numEvents);
        eventsSkipped++;
      }
      if (log.isWarnEnabled()) {
        log.warn(""String_Node_Str"" + harvest.getName() + ""String_Node_Str""+ eventsSkipped+ ""String_Node_Str""+ focused.getNextDate()+ ""String_Node_Str""+ nextEvent);
      }
    }
    focused.setNextDate(nextEvent);
    if (log.isTraceEnabled()) {
      log.trace(""String_Node_Str"" + harvest.getName() + ""String_Node_Str""+ (nextEvent == null ? ""String_Node_Str"" : nextEvent.toString()));
    }
  }
  log.info(""String_Node_Str"" + jobsMade + ""String_Node_Str""+ harvest.getOid());
  return jobsMade;
}",0.9727488151658767
89591,"/** 
 * Create a new LocalArcRepositoryClient based on current settings. 
 */
public LocalArcRepositoryClient(){
  List<String> fileDirs=Arrays.asList(Settings.getAll(FILE_DIRS));
  if (fileDirs.size() == 0) {
    fileDirs.add(DEFAULT_DIR_NAME);
  }
  for (  String fileName : fileDirs) {
    File f=new File(fileName);
    FileUtils.createDir(f);
    log.info(""String_Node_Str"" + f.getAbsolutePath() + ""String_Node_Str"");
    storageDirs.add(f);
  }
}","/** 
 * Create a new LocalArcRepositoryClient based on current settings. 
 */
public LocalArcRepositoryClient(){
  List<String> fileDirs=Arrays.asList(Settings.getAll(FILE_DIRS));
  for (  String fileName : fileDirs) {
    File f=new File(fileName);
    FileUtils.createDir(f);
    log.info(""String_Node_Str"" + f.getAbsolutePath() + ""String_Node_Str"");
    storageDirs.add(f);
  }
}",0.4676258992805755
89592,"/** 
 * Return relevant values as header-like fields (here ANVLRecord, but  spec-defined ""application/warc-fields"" type when written). Field names from from DCMI Terms and the WARC/0.17 specification.
 * @see org.archive.crawler.framework.WriterPoolProcessor#getFirstrecordBody(java.io.File)
 */
@Override protected String getFirstrecordBody(File orderFile){
  ANVLRecord record=new ANVLRecord(7);
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"" + Heritrix.getVersion() + ""String_Node_Str"");
  try {
    InetAddress host=InetAddress.getLocalHost();
    record.addLabelValue(""String_Node_Str"",host.getHostAddress());
    record.addLabelValue(""String_Node_Str"",host.getCanonicalHostName());
  }
 catch (  UnknownHostException e) {
    logger.log(Level.WARNING,""String_Node_Str"",e);
  }
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"");
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"");
  try {
    Document doc=XmlUtils.getDocument(orderFile);
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    if (pjd == null) {
      loadPersistentJobData(doc);
    }
  }
 catch (  IOException e) {
    logger.log(Level.WARNING,""String_Node_Str"",e);
  }
  record.addLabel(""String_Node_Str"" + dk.netarkivet.common.Constants.getVersionString());
  record.addLabelValue(""String_Node_Str"",pjd.getVersion());
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"" + pjd.getJobID());
  record.addLabelValue(""String_Node_Str"",pjd.getJobPriority().name());
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"" + pjd.getJobHarvestNum());
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"" + pjd.getOrigHarvestDefinitionID());
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"" + pjd.getMaxBytesPerDomain());
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"" + pjd.getMaxObjectsPerDomain());
  record.addLabelValue(""String_Node_Str"",pjd.getOrderXMLName());
  record.addLabelValue(""String_Node_Str"",pjd.getharvestName());
  record.addLabelValue(""String_Node_Str"",pjd.getScheduleName());
  record.addLabelValue(""String_Node_Str"",pjd.getHarvestFilenamePrefix());
  record.addLabelValue(""String_Node_Str"",pjd.getJobSubmitDate());
  record.addLabelValue(""String_Node_Str"",pjd.getPerformer());
  return record.toString();
}","/** 
 * Return relevant values as header-like fields (here ANVLRecord, but  spec-defined ""application/warc-fields"" type when written). Field names from from DCMI Terms and the WARC/0.17 specification.
 * @see org.archive.crawler.framework.WriterPoolProcessor#getFirstrecordBody(java.io.File)
 */
@Override protected String getFirstrecordBody(File orderFile){
  ANVLRecord record=new ANVLRecord(7);
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"" + Heritrix.getVersion() + ""String_Node_Str"");
  try {
    InetAddress host=InetAddress.getLocalHost();
    record.addLabelValue(""String_Node_Str"",host.getHostAddress());
    record.addLabelValue(""String_Node_Str"",host.getCanonicalHostName());
  }
 catch (  UnknownHostException e) {
    logger.log(Level.WARNING,""String_Node_Str"",e);
  }
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"");
  record.addLabelValue(""String_Node_Str"",""String_Node_Str"");
  try {
    Document doc=XmlUtils.getDocument(orderFile);
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    addIfNotBlank(record,""String_Node_Str"",XmlUtils.xpathOrNull(doc,""String_Node_Str""));
    if (pjd == null) {
      loadPersistentJobData(doc);
    }
  }
 catch (  IOException e) {
    logger.log(Level.WARNING,""String_Node_Str"",e);
  }
  String netarchiveSuiteComment=""String_Node_Str"" + dk.netarkivet.common.Constants.getVersionString();
  ANVLRecord recordNAS=new ANVLRecord(7);
  recordNAS.addLabelValue(""String_Node_Str"",pjd.getVersion());
  recordNAS.addLabelValue(""String_Node_Str"",""String_Node_Str"" + pjd.getJobID());
  recordNAS.addLabelValue(""String_Node_Str"",pjd.getJobPriority().name());
  recordNAS.addLabelValue(""String_Node_Str"",""String_Node_Str"" + pjd.getJobHarvestNum());
  recordNAS.addLabelValue(""String_Node_Str"",""String_Node_Str"" + pjd.getOrigHarvestDefinitionID());
  recordNAS.addLabelValue(""String_Node_Str"",""String_Node_Str"" + pjd.getMaxBytesPerDomain());
  recordNAS.addLabelValue(""String_Node_Str"",""String_Node_Str"" + pjd.getMaxObjectsPerDomain());
  recordNAS.addLabelValue(""String_Node_Str"",pjd.getOrderXMLName());
  recordNAS.addLabelValue(""String_Node_Str"",pjd.getharvestName());
  recordNAS.addLabelValue(""String_Node_Str"",pjd.getScheduleName());
  recordNAS.addLabelValue(""String_Node_Str"",pjd.getHarvestFilenamePrefix());
  recordNAS.addLabelValue(""String_Node_Str"",pjd.getJobSubmitDate());
  recordNAS.addLabelValue(""String_Node_Str"",pjd.getPerformer());
  return record.toString() + netarchiveSuiteComment + ""String_Node_Str""+ recordNAS.toString();
}",0.9419204526543518
89593,"/** 
 * Add tests to suite. One line for each unit-test class in this testsuite.
 * @param suite the suite.
 */
public static void addToSuite(TestSuite suite){
  suite.addTestSuite(DomainnameQueueAssignmentPolicyTester.class);
  suite.addTestSuite(HeritrixControllerFactoryTester.class);
  suite.addTestSuite(HarvestControllerTester.class);
  suite.addTestSuite(HarvestDocumentationTester.class);
  suite.addTestSuite(LegacyHarvestReportTester.class);
  suite.addTestSuite(HeritrixFilesTester.class);
  suite.addTestSuite(HeritrixLauncherTester.class);
  suite.addTestSuite(IngestableFilesTester.class);
  suite.addTestSuite(OnNSDomainsDecideRuleTester.class);
  suite.addTestSuite(ExtractorOAITest.class);
  suite.addTestSuite(MetadataFileWriterTester.class);
}","/** 
 * Add tests to suite. One line for each unit-test class in this testsuite.
 * @param suite the suite.
 */
public static void addToSuite(TestSuite suite){
  suite.addTestSuite(DomainnameQueueAssignmentPolicyTester.class);
  suite.addTestSuite(HeritrixControllerFactoryTester.class);
  suite.addTestSuite(HarvestControllerTester.class);
  suite.addTestSuite(HarvestDocumentationTester.class);
  suite.addTestSuite(LegacyHarvestReportTester.class);
  suite.addTestSuite(HeritrixFilesTester.class);
  suite.addTestSuite(HeritrixLauncherTester.class);
  suite.addTestSuite(IngestableFilesTester.class);
  suite.addTestSuite(OnNSDomainsDecideRuleTester.class);
  suite.addTestSuite(ExtractorOAITest.class);
  suite.addTestSuite(MetadataFileWriterTester.class);
  suite.addTestSuite(WARCWriterProcessorTester.class);
}",0.9651678277390754
89594,"/** 
 * Creates an instance in persistent storage of the given job. If the job doesn't have an ID, one is generated for it.
 * @param job a given job to add to persistent storage
 * @throws PermissionDenied If a job already exists in persistent storagewith the same id as the given job
 * @throws IOFailure        If some IOException occurs whilewriting the job to persistent storage
 */
public synchronized void create(Job job){
  ArgumentNotValid.checkNotNull(job,""String_Node_Str"");
  Long harvestId=job.getOrigHarvestDefinitionID();
  if (!HarvestDefinitionDAO.getInstance().exists(harvestId)) {
    throw new UnknownID(""String_Node_Str"" + harvestId);
  }
  Connection connection=HarvestDBConnection.get();
  if (job.getJobID() != null) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"");
  }
 else {
    job.setJobID(generateNextID(connection));
  }
  log.debug(""String_Node_Str"" + job.toString());
  PreparedStatement statement=null;
  try {
    connection.setAutoCommit(false);
    statement=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    statement.setLong(1,job.getJobID());
    statement.setLong(2,job.getOrigHarvestDefinitionID());
    statement.setInt(3,job.getStatus().ordinal());
    statement.setInt(4,job.getPriority().ordinal());
    statement.setLong(5,job.getForceMaxObjectsPerDomain());
    statement.setLong(6,job.getMaxBytesPerDomain());
    statement.setLong(7,job.getMaxJobRunningTime());
    DBUtils.setStringMaxLength(statement,8,job.getOrderXMLName(),Constants.MAX_NAME_SIZE,job,""String_Node_Str"");
    final String orderreader=job.getOrderXMLdoc().asXML();
    DBUtils.setClobMaxLength(statement,9,orderreader,Constants.MAX_ORDERXML_SIZE,job,""String_Node_Str"");
    DBUtils.setClobMaxLength(statement,10,job.getSeedListAsString(),Constants.MAX_COMBINED_SEED_LIST_SIZE,job,""String_Node_Str"");
    statement.setInt(11,job.getHarvestNum());
    DBUtils.setDateMaybeNull(statement,12,job.getActualStart());
    DBUtils.setDateMaybeNull(statement,13,job.getActualStop());
    DBUtils.setDateMaybeNull(statement,14,job.getSubmittedDate());
    DBUtils.setDateMaybeNull(statement,15,job.getCreationDate());
    statement.setInt(16,job.getDomainConfigurationMap().size());
    long initialEdition=1;
    statement.setLong(17,initialEdition);
    DBUtils.setLongMaybeNull(statement,18,job.getResubmittedAsJob());
    statement.executeUpdate();
    createJobConfigsEntries(connection,job);
    connection.commit();
    job.setEdition(initialEdition);
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + job + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.rollbackIfNeeded(connection,""String_Node_Str"",job);
    HarvestDBConnection.release(connection);
  }
}","/** 
 * Creates an instance in persistent storage of the given job. If the job doesn't have an ID, one is generated for it.
 * @param job a given job to add to persistent storage
 * @throws PermissionDenied If a job already exists in persistent storagewith the same id as the given job
 * @throws IOFailure        If some IOException occurs whilewriting the job to persistent storage
 */
public synchronized void create(Job job){
  ArgumentNotValid.checkNotNull(job,""String_Node_Str"");
  Long harvestId=job.getOrigHarvestDefinitionID();
  if (!HarvestDefinitionDAO.getInstance().exists(harvestId)) {
    throw new UnknownID(""String_Node_Str"" + harvestId);
  }
  Connection connection=HarvestDBConnection.get();
  if (job.getJobID() != null) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"");
  }
 else {
    job.setJobID(generateNextID(connection));
  }
  if (job.getCreationDate() != null) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"");
  }
 else {
    job.setCreationDate(new Date());
  }
  log.debug(""String_Node_Str"" + job.toString());
  PreparedStatement statement=null;
  try {
    connection.setAutoCommit(false);
    statement=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    statement.setLong(1,job.getJobID());
    statement.setLong(2,job.getOrigHarvestDefinitionID());
    statement.setInt(3,job.getStatus().ordinal());
    statement.setInt(4,job.getPriority().ordinal());
    statement.setLong(5,job.getForceMaxObjectsPerDomain());
    statement.setLong(6,job.getMaxBytesPerDomain());
    statement.setLong(7,job.getMaxJobRunningTime());
    DBUtils.setStringMaxLength(statement,8,job.getOrderXMLName(),Constants.MAX_NAME_SIZE,job,""String_Node_Str"");
    final String orderreader=job.getOrderXMLdoc().asXML();
    DBUtils.setClobMaxLength(statement,9,orderreader,Constants.MAX_ORDERXML_SIZE,job,""String_Node_Str"");
    DBUtils.setClobMaxLength(statement,10,job.getSeedListAsString(),Constants.MAX_COMBINED_SEED_LIST_SIZE,job,""String_Node_Str"");
    statement.setInt(11,job.getHarvestNum());
    DBUtils.setDateMaybeNull(statement,12,job.getActualStart());
    DBUtils.setDateMaybeNull(statement,13,job.getActualStop());
    DBUtils.setDateMaybeNull(statement,14,job.getSubmittedDate());
    DBUtils.setDateMaybeNull(statement,15,job.getCreationDate());
    statement.setInt(16,job.getDomainConfigurationMap().size());
    long initialEdition=1;
    statement.setLong(17,initialEdition);
    DBUtils.setLongMaybeNull(statement,18,job.getResubmittedAsJob());
    statement.executeUpdate();
    createJobConfigsEntries(connection,job);
    connection.commit();
    job.setEdition(initialEdition);
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + job + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.rollbackIfNeeded(connection,""String_Node_Str"",job);
    HarvestDBConnection.release(connection);
  }
}",0.9760607561499092
89595,"/** 
 * Computes the domain-name/byte-count and domain-name/object-count and domain-name/stopreason maps for a crawl.log.
 * @param file the local file to be processed
 * @throws IOFailure if there is problem reading the file
 */
private void parseCrawlLog(File file) throws IOFailure {
  boolean disregardSeedUrls=Settings.getBoolean(HarvesterSettings.DISREGARD_SEEDURL_INFORMATION_IN_CRAWLLOG);
  BufferedReader in=null;
  try {
    in=new BufferedReader(new FileReader(file));
    String line;
    int lineCnt=0;
    while ((line=in.readLine()) != null) {
      ++lineCnt;
      try {
        processHarvestLine(line,disregardSeedUrls);
      }
 catch (      ArgumentNotValid e) {
        final String message=""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str""+ lineCnt+ ""String_Node_Str""+ line+ ""String_Node_Str"";
        LOG.debug(message,e);
      }
    }
  }
 catch (  IOException e) {
    String msg=""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"";
    LOG.warn(msg,e);
    throw new IOFailure(msg,e);
  }
 finally {
    if (in != null) {
      try {
        in.close();
      }
 catch (      IOException e) {
        LOG.debug(""String_Node_Str"" + file,e);
      }
    }
  }
}","/** 
 * Computes the domain-name/byte-count and domain-name/object-count and domain-name/stopreason maps for a crawl.log.
 * @param file the local file to be processed
 * @throws IOFailure if there is problem reading the file
 */
private void parseCrawlLog(File file) throws IOFailure {
  boolean disregardSeedUrls=Settings.getBoolean(HarvesterSettings.DISREGARD_SEEDURL_INFORMATION_IN_CRAWLLOG);
  BufferedReader in=null;
  try {
    in=new BufferedReader(new FileReader(file));
    String line;
    int lineCnt=0;
    while ((line=in.readLine()) != null) {
      ++lineCnt;
      try {
        processHarvestLine(line,disregardSeedUrls);
      }
 catch (      ArgumentNotValid e) {
        final String message=""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str""+ lineCnt+ ""String_Node_Str""+ line+ ""String_Node_Str""+ e.getMessage();
        System.out.println(message);
        LOG.debug(message);
      }
    }
  }
 catch (  IOException e) {
    String msg=""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"";
    LOG.warn(msg,e);
    throw new IOFailure(msg,e);
  }
 finally {
    if (in != null) {
      try {
        in.close();
      }
 catch (      IOException e) {
        LOG.debug(""String_Node_Str"" + file,e);
      }
    }
  }
}",0.9777598059037608
89596,"/** 
 * Get the number of objects found for the given domain.
 * @param domainName A domain name (as given by getDomainNames())
 * @return How many objects were collected for that domain
 * @throws ArgumentNotValid if null or empty domainName
 */
public final Long getObjectCount(String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  final DomainStats domainStats=domainstats.get(domainName);
  if (domainStats != null) {
    return domainStats.getObjectCount();
  }
  return null;
}","/** 
 * Get the number of objects found for the given domain.
 * @param domainName A domain name (as given by getDomainNames())
 * @return How many objects were collected for that domain
 * @throws ArgumentNotValid if null or empty domainName
 */
@Override public final Long getObjectCount(String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  final DomainStats domainStats=domainstats.get(domainName);
  if (domainStats != null) {
    return domainStats.getObjectCount();
  }
  return null;
}",0.9904942965779469
89597,"/** 
 * Returns the set of domain names that are contained in hosts-report.txt (i.e. host names mapped to domains)
 * @return a Set of Strings
 */
public final Set<String> getDomainNames(){
  return Collections.unmodifiableSet(domainstats.keySet());
}","/** 
 * Returns the set of domain names that are contained in hosts-report.txt (i.e. host names mapped to domains)
 * @return a Set of Strings
 */
@Override public final Set<String> getDomainNames(){
  return Collections.unmodifiableSet(domainstats.keySet());
}",0.98046875
89598,"/** 
 * Sort a crawl.log file according to the timestamp.
 * @param file The file containing the unsorted data.
 * @param toFile The file that the sorted data can be put into.
 * @throws IOFailure if there were errors running the sort process, orif the file does not exist.
 */
public static void sortCrawlLog(File file,File toFile){
  ArgumentNotValid.checkNotNull(file,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(toFile,""String_Node_Str"");
  if (!file.exists()) {
    String errMsg=""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"";
    log.warn(errMsg);
    throw new IOFailure(errMsg);
  }
  File sortTempDir=null;
  if (Settings.getBoolean(CommonSettings.UNIX_SORT_USE_COMMON_TEMP_DIR)) {
    sortTempDir=FileUtils.getTempDir();
  }
  boolean sortLikeCrawllog=false;
  int error=ProcessUtils.runUnixSort(file,toFile,sortTempDir,sortLikeCrawllog);
  if (error != 0) {
    final String errMsg=""String_Node_Str"" + error + ""String_Node_Str""+ file+ ""String_Node_Str"";
    log.warn(errMsg);
    throw new IOFailure(errMsg);
  }
}","/** 
 * Sort a crawl.log file according to the url.
 * @param file The file containing the unsorted data.
 * @param toFile The file that the sorted data can be put into.
 * @throws IOFailure if there were errors running the sort process, orif the file does not exist.
 */
public static void sortCrawlLog(File file,File toFile){
  ArgumentNotValid.checkNotNull(file,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(toFile,""String_Node_Str"");
  if (!file.exists()) {
    String errMsg=""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"";
    log.warn(errMsg);
    throw new IOFailure(errMsg);
  }
  File sortTempDir=null;
  if (Settings.getBoolean(CommonSettings.UNIX_SORT_USE_COMMON_TEMP_DIR)) {
    sortTempDir=FileUtils.getTempDir();
  }
  boolean sortLikeCrawllog=true;
  int error=ProcessUtils.runUnixSort(file,toFile,sortTempDir,sortLikeCrawllog);
  if (error != 0) {
    final String errMsg=""String_Node_Str"" + error + ""String_Node_Str""+ file+ ""String_Node_Str"";
    log.warn(errMsg);
    throw new IOFailure(errMsg);
  }
}",0.9909134385461502
89599,"/** 
 * Helper method to get result from a batchjob.
 * @param batchJob a certain FileBatchJob
 * @return a file with the result.
 */
private static File getResultFile(FileBatchJob batchJob){
  File f;
  File fsorted;
  try {
    final String uuid=UUID.randomUUID().toString();
    f=File.createTempFile(""String_Node_Str"",uuid + ""String_Node_Str"",FileUtils.getTempDir());
    f.deleteOnExit();
    fsorted=File.createTempFile(""String_Node_Str"",uuid + ""String_Node_Str"",FileUtils.getTempDir());
    fsorted.deleteOnExit();
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
  BatchStatus status=ArcRepositoryClientFactory.getViewerInstance().batch(batchJob,Settings.get(CommonSettings.USE_REPLICA_ID));
  status.getResultFile().copyTo(f);
  FileUtils.sortCrawlLog(f,fsorted);
  FileUtils.remove(f);
  return fsorted;
}","/** 
 * Helper method to get result from a batchjob.
 * @param batchJob a certain FileBatchJob
 * @return a file with the result.
 */
private static File getResultFile(FileBatchJob batchJob){
  File f;
  File fsorted;
  try {
    final String uuid=UUID.randomUUID().toString();
    f=File.createTempFile(""String_Node_Str"",uuid + ""String_Node_Str"",FileUtils.getTempDir());
    f.deleteOnExit();
    fsorted=File.createTempFile(""String_Node_Str"",uuid + ""String_Node_Str"",FileUtils.getTempDir());
    fsorted.deleteOnExit();
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
  BatchStatus status=ArcRepositoryClientFactory.getViewerInstance().batch(batchJob,Settings.get(CommonSettings.USE_REPLICA_ID));
  status.getResultFile().copyTo(f);
  FileUtils.sortCrawlLogOnTimestamp(f,fsorted);
  FileUtils.remove(f);
  return fsorted;
}",0.9935483870967742
89600,"@Override protected String osInstallScript(){
  StringBuilder res=new StringBuilder();
  res.append(ScriptConstants.ECHO_COPYING + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE + ScriptConstants.TO + Constants.COLON);
  res.append(name);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SCP + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.COLON);
  res.append(machineParameters.getInstallDirValue());
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_UNZIPPING + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE + ScriptConstants.AT + Constants.COLON);
  res.append(name);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.LINUX_UNZIP_COMMAND + Constants.SPACE);
  res.append(machineParameters.getInstallDirValue());
  res.append(Constants.SLASH);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE + ScriptConstants.SCRIPT_DIR + Constants.SPACE);
  res.append(getInstallDirPath());
  res.append(Constants.NEWLINE);
  res.append(osInstallScriptCreateDir());
  res.append(ScriptConstants.ECHO_PREPARING_FOR_COPY);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + Constants.SPACE+ ScriptConstants.LINUX_HOME_DIR+ Constants.SEMICOLON+ Constants.SPACE+ ScriptConstants.LINUX_IF_EXIST+ Constants.SPACE);
  res.append(getConfDirPath());
  res.append(Constants.JMX_PASSWORD_FILE_NAME);
  res.append(Constants.SPACE + ScriptConstants.LINUX_THEN + Constants.SPACE+ ScriptConstants.LINUX_USER_ONLY+ Constants.SPACE);
  res.append(getConfDirPath());
  res.append(Constants.JMX_PASSWORD_FILE_NAME + Constants.SEMICOLON + Constants.SPACE+ ScriptConstants.FI+ Constants.SEMICOLON+ Constants.SPACE+ Constants.QUOTE_MARK);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + Constants.SPACE+ ScriptConstants.LINUX_HOME_DIR+ Constants.SEMICOLON+ Constants.SPACE+ ScriptConstants.LINUX_IF_EXIST+ Constants.SPACE);
  res.append(getConfDirPath());
  res.append(Constants.JMX_ACCESS_FILE_NAME);
  res.append(Constants.SPACE + ScriptConstants.LINUX_THEN + Constants.SPACE+ ScriptConstants.LINUX_USER_ONLY+ Constants.SPACE);
  res.append(getConfDirPath());
  res.append(Constants.JMX_ACCESS_FILE_NAME + Constants.SEMICOLON + Constants.SPACE+ ScriptConstants.FI+ Constants.SEMICOLON+ Constants.SPACE+ Constants.QUOTE_MARK);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_COPY_SETTINGS_AND_SCRIPTS);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SCP + Constants.SPACE + ScriptConstants.SCRIPT_REPOSITORY+ Constants.SPACE);
  res.append(name);
  res.append(Constants.SLASH + Constants.STAR + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.COLON);
  res.append(getConfDirPath());
  res.append(Constants.NEWLINE);
  res.append(osInstallExternalJarFiles());
  res.append(osInstallDatabase());
  res.append(osInstallArchiveDatabase());
  res.append(ScriptConstants.ECHO_MAKE_EXECUTABLE);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + ScriptConstants.LINUX_USER_700+ Constants.SPACE);
  res.append(getConfDirPath());
  res.append(Constants.STAR + Constants.SCRIPT_EXTENSION_LINUX + Constants.SPACE+ Constants.QUOTE_MARK);
  res.append(Constants.NEWLINE);
  res.append(getJMXremoteFilesCommand());
  return res.toString();
}","@Override protected String osInstallScript(){
  StringBuilder res=new StringBuilder();
  res.append(ScriptConstants.ECHO_COPYING + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE + ScriptConstants.TO + Constants.COLON);
  res.append(name);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SCP + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.COLON);
  res.append(machineParameters.getInstallDirValue());
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_DELETING + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.COLON);
  res.append(getInstallDirPath() + getLocalLibDirPath());
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE);
  res.append(ScriptConstants.LINUX_FORCE_RECURSIVE_DELETE);
  res.append(Constants.SPACE);
  res.append(getInstallDirPath() + getLocalLibDirPath());
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_UNZIPPING + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE + ScriptConstants.AT + Constants.COLON);
  res.append(name);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.LINUX_UNZIP_COMMAND + Constants.SPACE);
  res.append(machineParameters.getInstallDirValue());
  res.append(Constants.SLASH);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE + ScriptConstants.SCRIPT_DIR + Constants.SPACE);
  res.append(getInstallDirPath());
  res.append(Constants.NEWLINE);
  res.append(osInstallScriptCreateDir());
  res.append(ScriptConstants.ECHO_PREPARING_FOR_COPY);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + Constants.SPACE+ ScriptConstants.LINUX_HOME_DIR+ Constants.SEMICOLON+ Constants.SPACE+ ScriptConstants.LINUX_IF_EXIST+ Constants.SPACE);
  res.append(getConfDirPath());
  res.append(Constants.JMX_PASSWORD_FILE_NAME);
  res.append(Constants.SPACE + ScriptConstants.LINUX_THEN + Constants.SPACE+ ScriptConstants.LINUX_USER_ONLY+ Constants.SPACE);
  res.append(getConfDirPath());
  res.append(Constants.JMX_PASSWORD_FILE_NAME + Constants.SEMICOLON + Constants.SPACE+ ScriptConstants.FI+ Constants.SEMICOLON+ Constants.SPACE+ Constants.QUOTE_MARK);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + Constants.SPACE+ ScriptConstants.LINUX_HOME_DIR+ Constants.SEMICOLON+ Constants.SPACE+ ScriptConstants.LINUX_IF_EXIST+ Constants.SPACE);
  res.append(getConfDirPath());
  res.append(Constants.JMX_ACCESS_FILE_NAME);
  res.append(Constants.SPACE + ScriptConstants.LINUX_THEN + Constants.SPACE+ ScriptConstants.LINUX_USER_ONLY+ Constants.SPACE);
  res.append(getConfDirPath());
  res.append(Constants.JMX_ACCESS_FILE_NAME + Constants.SEMICOLON + Constants.SPACE+ ScriptConstants.FI+ Constants.SEMICOLON+ Constants.SPACE+ Constants.QUOTE_MARK);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_COPY_SETTINGS_AND_SCRIPTS);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SCP + Constants.SPACE + ScriptConstants.SCRIPT_REPOSITORY+ Constants.SPACE);
  res.append(name);
  res.append(Constants.SLASH + Constants.STAR + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.COLON);
  res.append(getConfDirPath());
  res.append(Constants.NEWLINE);
  res.append(osInstallExternalJarFiles());
  res.append(osInstallDatabase());
  res.append(osInstallArchiveDatabase());
  res.append(ScriptConstants.ECHO_MAKE_EXECUTABLE);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + ScriptConstants.LINUX_USER_700+ Constants.SPACE);
  res.append(getConfDirPath());
  res.append(Constants.STAR + Constants.SCRIPT_EXTENSION_LINUX + Constants.SPACE+ Constants.QUOTE_MARK);
  res.append(Constants.NEWLINE);
  res.append(getJMXremoteFilesCommand());
  return res.toString();
}",0.816608158818545
89601,"/** 
 * Creates the operation system specific installation script for  this machine. Pseudo code: - echo copying 'NetarchiveSuite.zip' to: 'machine' - scp 'NetarchiveSuite.zip' 'login'@'machine': - echo unzipping 'NetarchiveSuite.zip' at: 'machine' - ssh 'login'@'machine' 'unzip' 'environmentName' -o 'NetarchiveSuite.zip - $'create directories'. - echo preparing for copying of settings and scripts - if [ $( ssh 'login'@'machine' cmd /c if exist  'environmentName'\\conf\\jmxremote.password echo 1 ) ]; then echo Y |  ssh 'login'@'machine' cmd /c cacls  'environmentName'\\conf\\jmxremote.password /P BITARKIV\\'login':F; fi - if [ $( ssh 'login'@'machine' cmd /c if exist  'environmentName'\\conf\\jmxremote.access echo 1 ) ]; then echo Y | ssh  'login'@'machine' cmd /c cacls 'environmentName'\\conf\\jmxremote.access /P BITARKIV\\'login':F; fi - echo copying settings and scripts - scp -r 'machine'/* 'login'@'machine':'environmentName'\\conf\\ - $'apply database script' - echo make password files readonly * if 'jmxremote-password-path' != 'jmxremote-password-defaultpath' - ssh 'login'@'machine' move /Y 'jmxremote-password-defaultpath'  'jmxremote-password-path' * if 'jmxremote-access-path' != 'jmxremote-access-defaultpath' - ssh 'login'@'machine' move /Y 'jmxremote-access-defaultpath'  'jmxremote-access-path' - echo Y | ssh 'login'@'machine' cmd /c cacls  'environmentName'\\'jmxremote-password-path' /P BITARKIV\\'login':R - echo Y | ssh 'login'@'machine' cmd /c cacls  'environmentName'\\'jmxremote-access-path' /P BITARKIV\\'login':R variables: 'NetarchiveSuite.zip' = The NetarchiveSuitePackage with '.zip' extension. 'machine' = The machine name. 'login' = The username for the machine. 'unzip' = The command for unzipping. 'environmentName' = the environmentName from the configuration file. $'...' = call other function.
 * @return Operation system specific part of the installscript
 */
@Override protected String osInstallScript(){
  StringBuilder res=new StringBuilder();
  res.append(ScriptConstants.ECHO_COPYING + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE + ScriptConstants.TO + Constants.COLON+ Constants.SPACE);
  res.append(name);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SCP + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.COLON);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_UNZIPPING + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE + ScriptConstants.AT + Constants.COLON+ Constants.SPACE);
  res.append(name);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_UNZIP_COMMAND + Constants.SPACE);
  res.append(getEnvironmentName());
  res.append(Constants.SPACE + ScriptConstants.SCRIPT_OUTPUT + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.NEWLINE);
  res.append(osInstallScriptCreateDir());
  res.append(ScriptConstants.ECHO_PREPARING_FOR_COPY);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.IF + Constants.SPACE + Constants.SQUARE_BRACKET_BEGIN+ Constants.SPACE+ Constants.DOLLAR_SIGN+ Constants.BRACKET_BEGIN+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.IF+ Constants.SPACE+ ScriptConstants.EXIST+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_PASSWORD_FILE_NAME);
  res.append(Constants.SPACE + ScriptConstants.ECHO_ONE);
  res.append(Constants.SPACE + Constants.BRACKET_END + Constants.SPACE+ Constants.SQUARE_BRACKET_END+ Constants.SEMICOLON+ Constants.SPACE+ ScriptConstants.THEN+ Constants.SPACE);
  res.append(ScriptConstants.ECHO_Y + Constants.SPACE + Constants.SEPARATOR+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.CACLS+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_PASSWORD_FILE_NAME + Constants.SPACE + ScriptConstants.SLASH_P+ Constants.SPACE+ ScriptConstants.BITARKIV_BACKSLASH_BACKSLASH);
  res.append(machineParameters.getMachineUserName().getText());
  res.append(ScriptConstants.COLON_F + Constants.SEMICOLON + Constants.SPACE+ ScriptConstants.FI+ Constants.SEMICOLON);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.IF + Constants.SPACE + Constants.SQUARE_BRACKET_BEGIN+ Constants.SPACE+ Constants.DOLLAR_SIGN+ Constants.BRACKET_BEGIN+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.IF+ Constants.SPACE+ ScriptConstants.EXIST+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_ACCESS_FILE_NAME);
  res.append(Constants.SPACE + ScriptConstants.ECHO_ONE);
  res.append(Constants.SPACE + Constants.BRACKET_END + Constants.SPACE+ Constants.SQUARE_BRACKET_END+ Constants.SEMICOLON+ Constants.SPACE+ ScriptConstants.THEN+ Constants.SPACE+ ScriptConstants.ECHO_Y+ Constants.SPACE);
  res.append(Constants.SEPARATOR + Constants.SPACE + ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.CACLS+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_ACCESS_FILE_NAME + Constants.SPACE + ScriptConstants.SLASH_P+ Constants.SPACE+ ScriptConstants.BITARKIV_BACKSLASH_BACKSLASH);
  res.append(machineParameters.getMachineUserName().getText());
  res.append(ScriptConstants.COLON_F + Constants.SEMICOLON + Constants.SPACE+ ScriptConstants.FI+ Constants.SEMICOLON);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_COPY_SETTINGS_AND_SCRIPTS);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SCP + Constants.SPACE + ScriptConstants.DASH_R+ Constants.SPACE);
  res.append(name);
  res.append(Constants.SLASH + Constants.STAR + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.COLON);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.NEWLINE);
  res.append(osInstallExternalJarFiles());
  res.append(osInstallDatabase());
  res.append(osInstallArchiveDatabase());
  res.append(getJMXremoteFilesCommand());
  return res.toString();
}","/** 
 * Creates the operation system specific installation script for  this machine. Pseudo code: - echo copying 'NetarchiveSuite.zip' to: 'machine' - scp 'NetarchiveSuite.zip' 'login'@'machine': - echo unzipping 'NetarchiveSuite.zip' at: 'machine' - ssh 'login'@'machine' 'unzip' 'environmentName' -o 'NetarchiveSuite.zip - $'create directories'. - echo preparing for copying of settings and scripts - if [ $( ssh 'login'@'machine' cmd /c if exist  'environmentName'\\conf\\jmxremote.password echo 1 ) ]; then echo Y |  ssh 'login'@'machine' cmd /c cacls  'environmentName'\\conf\\jmxremote.password /P BITARKIV\\'login':F; fi - if [ $( ssh 'login'@'machine' cmd /c if exist  'environmentName'\\conf\\jmxremote.access echo 1 ) ]; then echo Y | ssh  'login'@'machine' cmd /c cacls 'environmentName'\\conf\\jmxremote.access /P BITARKIV\\'login':F; fi - echo copying settings and scripts - scp -r 'machine'/* 'login'@'machine':'environmentName'\\conf\\ - $'apply database script' - echo make password files readonly * if 'jmxremote-password-path' != 'jmxremote-password-defaultpath' - ssh 'login'@'machine' move /Y 'jmxremote-password-defaultpath'  'jmxremote-password-path' * if 'jmxremote-access-path' != 'jmxremote-access-defaultpath' - ssh 'login'@'machine' move /Y 'jmxremote-access-defaultpath'  'jmxremote-access-path' - echo Y | ssh 'login'@'machine' cmd /c cacls  'environmentName'\\'jmxremote-password-path' /P BITARKIV\\'login':R - echo Y | ssh 'login'@'machine' cmd /c cacls  'environmentName'\\'jmxremote-access-path' /P BITARKIV\\'login':R variables: 'NetarchiveSuite.zip' = The NetarchiveSuitePackage with '.zip' extension. 'machine' = The machine name. 'login' = The username for the machine. 'unzip' = The command for unzipping. 'environmentName' = the environmentName from the configuration file. $'...' = call other function.
 * @return Operation system specific part of the installscript
 */
@Override protected String osInstallScript(){
  StringBuilder res=new StringBuilder();
  res.append(ScriptConstants.ECHO_COPYING + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE + ScriptConstants.TO + Constants.COLON+ Constants.SPACE);
  res.append(name);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SCP + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.COLON);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_DELETING_OLD_LIBRARIES);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.IF+ Constants.SPACE+ ScriptConstants.EXIST+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalLibDirPath()) + Constants.SPACE + ""String_Node_Str""+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalLibDirPath()));
  res.append(Constants.SPACE + Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_UNZIPPING + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE + ScriptConstants.AT + Constants.COLON+ Constants.SPACE);
  res.append(name);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_UNZIP_COMMAND + Constants.SPACE);
  res.append(getEnvironmentName());
  res.append(Constants.SPACE + ScriptConstants.SCRIPT_OUTPUT + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.NEWLINE);
  res.append(osInstallScriptCreateDir());
  res.append(ScriptConstants.ECHO_PREPARING_FOR_COPY);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.IF + Constants.SPACE + Constants.SQUARE_BRACKET_BEGIN+ Constants.SPACE+ Constants.DOLLAR_SIGN+ Constants.BRACKET_BEGIN+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.IF+ Constants.SPACE+ ScriptConstants.EXIST+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_PASSWORD_FILE_NAME);
  res.append(Constants.SPACE + ScriptConstants.ECHO_ONE);
  res.append(Constants.SPACE + Constants.BRACKET_END + Constants.SPACE+ Constants.SQUARE_BRACKET_END+ Constants.SEMICOLON+ Constants.SPACE+ ScriptConstants.THEN+ Constants.SPACE);
  res.append(ScriptConstants.ECHO_Y + Constants.SPACE + Constants.SEPARATOR+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.CACLS+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_PASSWORD_FILE_NAME + Constants.SPACE + ScriptConstants.SLASH_P+ Constants.SPACE+ ScriptConstants.BITARKIV_BACKSLASH_BACKSLASH);
  res.append(machineParameters.getMachineUserName().getText());
  res.append(ScriptConstants.COLON_F + Constants.SEMICOLON + Constants.SPACE+ ScriptConstants.FI+ Constants.SEMICOLON);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.IF + Constants.SPACE + Constants.SQUARE_BRACKET_BEGIN+ Constants.SPACE+ Constants.DOLLAR_SIGN+ Constants.BRACKET_BEGIN+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.IF+ Constants.SPACE+ ScriptConstants.EXIST+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_ACCESS_FILE_NAME);
  res.append(Constants.SPACE + ScriptConstants.ECHO_ONE);
  res.append(Constants.SPACE + Constants.BRACKET_END + Constants.SPACE+ Constants.SQUARE_BRACKET_END+ Constants.SEMICOLON+ Constants.SPACE+ ScriptConstants.THEN+ Constants.SPACE+ ScriptConstants.ECHO_Y+ Constants.SPACE);
  res.append(Constants.SEPARATOR + Constants.SPACE + ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.CACLS+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_ACCESS_FILE_NAME + Constants.SPACE + ScriptConstants.SLASH_P+ Constants.SPACE+ ScriptConstants.BITARKIV_BACKSLASH_BACKSLASH);
  res.append(machineParameters.getMachineUserName().getText());
  res.append(ScriptConstants.COLON_F + Constants.SEMICOLON + Constants.SPACE+ ScriptConstants.FI+ Constants.SEMICOLON);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_COPY_SETTINGS_AND_SCRIPTS);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SCP + Constants.SPACE + ScriptConstants.DASH_R+ Constants.SPACE);
  res.append(name);
  res.append(Constants.SLASH + Constants.STAR + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.COLON);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.NEWLINE);
  res.append(osInstallExternalJarFiles());
  res.append(osInstallDatabase());
  res.append(osInstallArchiveDatabase());
  res.append(getJMXremoteFilesCommand());
  return res.toString();
}",0.958500105329682
89602,"/** 
 * On windows machines console output can cause problesms so any uses of java.util.logging.ConsoleHandler are removed.
 * @param logProperties the original contents of the logging properties file.
 * @return logging properties with the ConsoleHandler removed
 */
@Override protected String modifyLogProperties(String logProperties){
  String output;
  output=logProperties.replaceAll(""String_Node_Str"",""String_Node_Str"");
  output=output.replaceAll(""String_Node_Str"",""String_Node_Str"");
  return output;
}","/** 
 * On windows machines console output can cause problems so any uses of java.util.logging.ConsoleHandler are removed.
 * @param logProperties the original contents of the logging properties file.
 * @return logging properties with the ConsoleHandler removed
 */
@Override protected String modifyLogProperties(String logProperties){
  String output;
  output=logProperties.replaceAll(""String_Node_Str"",""String_Node_Str"");
  output=output.replaceAll(""String_Node_Str"",""String_Node_Str"");
  return output;
}",0.9990186457311088
89603,"/** 
 * Test that the BitarchiveServer outputs logging information. This verifies the fix of bug #99.
 * @throws IOException If unable to read the logfile.
 */
public void testLogging() throws IOException {
  Settings.set(ArchiveSettings.BITARCHIVE_SERVER_FILEDIR,TestInfo.BITARCHIVE_APP_DIR_1);
  Settings.set(CommonSettings.DIR_COMMONTEMPDIR,TestInfo.BITARCHIVE_SERVER_DIR_1);
  bas=BitarchiveServer.getInstance();
  bas.close();
  LogUtils.flushLogs(""String_Node_Str"");
  String log=FileUtils.readFile(TestInfo.LOG_FILE);
  assertFalse(""String_Node_Str"",log.equals(""String_Node_Str""));
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",TestInfo.LOG_FILE);
}","/** 
 * Test that the BitarchiveServer outputs logging information. This verifies the fix of bug #99.
 * @throws IOException If unable to read the logfile.
 */
public void testLogging() throws IOException {
  Settings.set(ArchiveSettings.BITARCHIVE_SERVER_FILEDIR,TestInfo.BITARCHIVE_APP_DIR_1);
  Settings.set(CommonSettings.DIR_COMMONTEMPDIR,TestInfo.BITARCHIVE_SERVER_DIR_1);
  bas=BitarchiveServer.getInstance();
  bas.close();
  LogUtils.flushLogs(""String_Node_Str"");
  String log=FileUtils.readFile(TestInfo.LOG_FILE);
  assertFalse(""String_Node_Str"",log.isEmpty());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",TestInfo.LOG_FILE);
}",0.9761549925484352
89604,"/** 
 * Add the tests here.
 */
public static void addToSuite(TestSuite suite){
  suite.addTestSuite(CreateIndexTester.class);
  suite.addTestSuite(GetFileTester.class);
  suite.addTestSuite(GetRecordTester.class);
  suite.addTestSuite(ReestablishAdminDatabaseTester.class);
  suite.addTestSuite(RunBatchTester.class);
  suite.addTestSuite(UploadTester.class);
}","/** 
 * Add the tests here.
 */
public static void addToSuite(TestSuite suite){
  suite.addTestSuite(GetFileTester.class);
  suite.addTestSuite(GetRecordTester.class);
  suite.addTestSuite(ReestablishAdminDatabaseTester.class);
  suite.addTestSuite(RunBatchTester.class);
  suite.addTestSuite(UploadTester.class);
}",0.930576070901034
89605,"/** 
 * Create FTPClient and log on to ftp-server, if not already connected to ftp-server.  Attempts to set binary mode and passive mode. Will try to login up to FTP_RETRIES times, if login fails.
 */
private void logOn(){
  if (currentFTPClient != null && currentFTPClient.isConnected()) {
    return;
  }
 else {
    currentFTPClient=new FTPClient();
  }
  log.trace(""String_Node_Str"" + ftpUserName + ""String_Node_Str""+ ftpUserPassword.replaceAll(""String_Node_Str"",""String_Node_Str"")+ ""String_Node_Str""+ ftpServerName+ ""String_Node_Str""+ ftpServerPort);
  int tries=0;
  boolean logOnSuccessful=false;
  while (!logOnSuccessful && tries < CommonSettings.FTP_RETRIES) {
    tries++;
    try {
      currentFTPClient.connect(ftpServerName,ftpServerPort);
      currentFTPClient.setDataTimeout(CommonSettings.FTP_DATATIMEOUT);
      if (!currentFTPClient.login(ftpUserName,ftpUserPassword)) {
        final String message=""String_Node_Str"" + SystemUtils.getLocalHostName() + ""String_Node_Str""+ ftpServerName+ ""String_Node_Str""+ ftpServerPort+ ""String_Node_Str""+ ftpUserName+ ""String_Node_Str""+ ftpUserPassword.replaceAll(""String_Node_Str"",""String_Node_Str"")+ ""String_Node_Str""+ getFtpErrorMessage();
        log.warn(message);
        throw new IOFailure(message);
      }
      if (!currentFTPClient.setFileType(FTPClient.BINARY_FILE_TYPE)) {
        final String message=""String_Node_Str"" + ftpServerName + ""String_Node_Str""+ getFtpErrorMessage();
        log.warn(message);
        throw new IOFailure(message);
      }
      currentFTPClient.enterLocalPassiveMode();
      log.debug(""String_Node_Str"" + currentFTPClient.getDefaultTimeout());
      logOnSuccessful=true;
    }
 catch (    IOException e) {
      final String msg=""String_Node_Str"" + ftpServerName + ""String_Node_Str""+ SystemUtils.getLocalHostName()+ ""String_Node_Str"";
      if (tries < CommonSettings.FTP_RETRIES) {
        log.debug(msg + ""String_Node_Str"" + tries+ ""String_Node_Str""+ CommonSettings.FTP_RETRIES+ ""String_Node_Str""+ ""String_Node_Str"",e);
        TimeUtils.exponentialBackoffSleep(tries,Calendar.MINUTE);
      }
 else {
        log.warn(msg + ""String_Node_Str"" + tries+ ""String_Node_Str"");
        throw new IOFailure(msg,e);
      }
    }
  }
  log.debug(""String_Node_Str"" + ftpUserName + ""String_Node_Str""+ ftpUserPassword.replaceAll(""String_Node_Str"",""String_Node_Str"")+ ""String_Node_Str""+ ftpServerName+ ""String_Node_Str""+ ftpServerPort);
}","/** 
 * Create FTPClient and log on to ftp-server, if not already connected to ftp-server.  Attempts to set binary mode and passive mode. Will try to login up to FTP_RETRIES times, if login fails.
 */
private void logOn(){
  if (currentFTPClient != null && currentFTPClient.isConnected()) {
    return;
  }
 else {
    currentFTPClient=new FTPClient();
  }
  log.trace(""String_Node_Str"" + ftpUserName + ""String_Node_Str""+ ftpUserPassword.replaceAll(""String_Node_Str"",""String_Node_Str"")+ ""String_Node_Str""+ ftpServerName+ ""String_Node_Str""+ ftpServerPort);
  int tries=0;
  boolean logOnSuccessful=false;
  while (!logOnSuccessful && tries < FTP_RETRIES) {
    tries++;
    try {
      currentFTPClient.connect(ftpServerName,ftpServerPort);
      currentFTPClient.setDataTimeout(FTP_DATATIMEOUT);
      if (!currentFTPClient.login(ftpUserName,ftpUserPassword)) {
        final String message=""String_Node_Str"" + SystemUtils.getLocalHostName() + ""String_Node_Str""+ ftpServerName+ ""String_Node_Str""+ ftpServerPort+ ""String_Node_Str""+ ftpUserName+ ""String_Node_Str""+ ftpUserPassword.replaceAll(""String_Node_Str"",""String_Node_Str"")+ ""String_Node_Str""+ getFtpErrorMessage();
        log.warn(message);
        throw new IOFailure(message);
      }
      if (!currentFTPClient.setFileType(FTPClient.BINARY_FILE_TYPE)) {
        final String message=""String_Node_Str"" + ftpServerName + ""String_Node_Str""+ getFtpErrorMessage();
        log.warn(message);
        throw new IOFailure(message);
      }
      currentFTPClient.enterLocalPassiveMode();
      log.debug(""String_Node_Str"" + currentFTPClient.getDefaultTimeout());
      logOnSuccessful=true;
    }
 catch (    IOException e) {
      final String msg=""String_Node_Str"" + ftpServerName + ""String_Node_Str""+ SystemUtils.getLocalHostName()+ ""String_Node_Str"";
      if (tries < FTP_RETRIES) {
        log.debug(msg + ""String_Node_Str"" + tries+ ""String_Node_Str""+ FTP_RETRIES+ ""String_Node_Str""+ ""String_Node_Str"",e);
        TimeUtils.exponentialBackoffSleep(tries,Calendar.MINUTE);
      }
 else {
        log.warn(msg + ""String_Node_Str"" + tries+ ""String_Node_Str"");
        throw new IOFailure(msg,e);
      }
    }
  }
  log.debug(""String_Node_Str"" + ftpUserName + ""String_Node_Str""+ ftpUserPassword.replaceAll(""String_Node_Str"",""String_Node_Str"")+ ""String_Node_Str""+ ftpServerName+ ""String_Node_Str""+ ftpServerPort);
}",0.9875
89606,"/** 
 * Private constructor used by getInstance() static-method Tries to generate unique name on ftp-server.
 * @param localFile         File used to create new file on ftp-server.
 * @param useChecksums      If true, checksums will be used to checktransfers.
 * @param fileDeletable     If true, this file will be deleted after uploadto FTP.
 * @param multipleDownloads If true, the file will not be removed from FTPserver automatically after first download.
 * @param connectionParams  If not null, contains connection parameters to the FTP-server desired by the user 
 * @throws IOFailure        if MD5 checksum fails, or ftp fails
 * @throws ArgumentNotValid if the local file cannot be read.
 */
private FTPRemoteFile(File localFile,boolean useChecksums,boolean fileDeletable,boolean multipleDownloads,RemoteFileSettings connectionParams) throws IOFailure {
  super(localFile,useChecksums,fileDeletable,multipleDownloads);
  if (connectionParams != null) {
    this.ftpServerName=connectionParams.getServerName();
    this.ftpServerPort=connectionParams.getServerPort();
    this.ftpUserName=connectionParams.getUserName();
    this.ftpUserPassword=connectionParams.getUserPassword();
  }
 else {
    this.ftpServerName=Settings.get(CommonSettings.FTP_SERVER_NAME);
    this.ftpServerPort=Settings.getInt(CommonSettings.FTP_SERVER_PORT);
    this.ftpUserName=Settings.get(CommonSettings.FTP_USER_NAME);
    this.ftpUserPassword=Settings.get(CommonSettings.FTP_USER_PASSWORD);
  }
  if (filesize == 0) {
    try {
      if (useChecksums) {
        checksum=MD5.generateMD5onFile(file);
      }
 else {
        checksum=null;
      }
      ftpFileName=""String_Node_Str"";
    }
 catch (    IOException e) {
      String msg=""String_Node_Str"";
      if (e instanceof CopyStreamException) {
        CopyStreamException realException=(CopyStreamException)e;
        msg+=""String_Node_Str"" + realException.getIOException() + ""String_Node_Str"";
      }
      throw new IOFailure(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str""+ msg,e);
    }
  }
 else {
    if (ftpServerName.equalsIgnoreCase(""String_Node_Str"")) {
      ftpServerName=SystemUtils.getLocalHostName();
      log.debug(""String_Node_Str"" + SystemUtils.getLocalHostName() + ""String_Node_Str""+ ftpServerName);
    }
    final int aMagicNumber=100000;
    ftpFileName=file.getName() + ""String_Node_Str"" + new Random().nextInt(aMagicNumber)+ ""String_Node_Str""+ new Date().getTime();
    InputStream in;
    try {
      in=new FileInputStream(localFile);
    }
 catch (    FileNotFoundException e) {
      final String message=""String_Node_Str"" + localFile + ""String_Node_Str"";
      log.debug(message,e);
      throw new IOFailure(message,e);
    }
    log.debug(""String_Node_Str"" + file.getName() + ""String_Node_Str""+ ftpFileName+ ""String_Node_Str""+ ftpServerName);
    try {
      logOn();
      if (useChecksums) {
        in=new DigestInputStream(in,MD5.getMessageDigestInstance());
      }
      boolean success=false;
      int tried=0;
      while (!success && tried < CommonSettings.FTP_RETRIES) {
        tried++;
        try {
          success=currentFTPClient.storeFile(ftpFileName,in);
          if (!success) {
            log.debug(""String_Node_Str"" + tried + ""String_Node_Str""+ CommonSettings.FTP_RETRIES+ ""String_Node_Str""+ getFtpErrorMessage());
          }
        }
 catch (        IOException e) {
          String message=""String_Node_Str"" + ftpFileName + ""String_Node_Str""+ tried+ ""String_Node_Str""+ CommonSettings.FTP_RETRIES;
          if (e instanceof CopyStreamException) {
            CopyStreamException realException=(CopyStreamException)e;
            message+=""String_Node_Str"" + realException.getIOException() + ""String_Node_Str"";
          }
          log.debug(message,e);
        }
      }
      if (!success) {
        final String msg=""String_Node_Str"" + localFile + ""String_Node_Str""+ tried+ ""String_Node_Str"";
        log.warn(msg);
        throw new IOFailure(msg);
      }
      log.debug(""String_Node_Str"" + ftpFileName + ""String_Node_Str"");
      if (useChecksums) {
        checksum=MD5.toHex(((DigestInputStream)in).getMessageDigest().digest());
        log.debug(""String_Node_Str"" + ftpFileName + ""String_Node_Str""+ checksum);
      }
 else {
        checksum=null;
      }
    }
  finally {
      try {
        if (in != null) {
          in.close();
        }
      }
 catch (      IOException e) {
        log.warn(""String_Node_Str"" + e);
      }
      logOut();
      log.debug(""String_Node_Str"");
    }
  }
  if (fileDeletable) {
    try {
      FileUtils.removeRecursively(localFile);
    }
 catch (    IOFailure e) {
      log.warn(""String_Node_Str"" + localFile,e);
    }
  }
}","/** 
 * Private constructor used by getInstance() static-method Tries to generate unique name on ftp-server.
 * @param localFile         File used to create new file on ftp-server.
 * @param useChecksums      If true, checksums will be used to checktransfers.
 * @param fileDeletable     If true, this file will be deleted after uploadto FTP.
 * @param multipleDownloads If true, the file will not be removed from FTPserver automatically after first download.
 * @param connectionParams  If not null, contains connection parameters to the FTP-server desired by the user 
 * @throws IOFailure        if MD5 checksum fails, or ftp fails
 * @throws ArgumentNotValid if the local file cannot be read.
 */
private FTPRemoteFile(File localFile,boolean useChecksums,boolean fileDeletable,boolean multipleDownloads,RemoteFileSettings connectionParams) throws IOFailure {
  super(localFile,useChecksums,fileDeletable,multipleDownloads);
  if (connectionParams != null) {
    this.ftpServerName=connectionParams.getServerName();
    this.ftpServerPort=connectionParams.getServerPort();
    this.ftpUserName=connectionParams.getUserName();
    this.ftpUserPassword=connectionParams.getUserPassword();
  }
 else {
    this.ftpServerName=Settings.get(CommonSettings.FTP_SERVER_NAME);
    this.ftpServerPort=Settings.getInt(CommonSettings.FTP_SERVER_PORT);
    this.ftpUserName=Settings.get(CommonSettings.FTP_USER_NAME);
    this.ftpUserPassword=Settings.get(CommonSettings.FTP_USER_PASSWORD);
  }
  if (filesize == 0) {
    try {
      if (useChecksums) {
        checksum=MD5.generateMD5onFile(file);
      }
 else {
        checksum=null;
      }
      ftpFileName=""String_Node_Str"";
    }
 catch (    IOException e) {
      String msg=""String_Node_Str"";
      if (e instanceof CopyStreamException) {
        CopyStreamException realException=(CopyStreamException)e;
        msg+=""String_Node_Str"" + realException.getIOException() + ""String_Node_Str"";
      }
      throw new IOFailure(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str""+ msg,e);
    }
  }
 else {
    if (ftpServerName.equalsIgnoreCase(""String_Node_Str"")) {
      ftpServerName=SystemUtils.getLocalHostName();
      log.debug(""String_Node_Str"" + SystemUtils.getLocalHostName() + ""String_Node_Str""+ ftpServerName);
    }
    final int aMagicNumber=100000;
    ftpFileName=file.getName() + ""String_Node_Str"" + new Random().nextInt(aMagicNumber)+ ""String_Node_Str""+ new Date().getTime();
    InputStream in;
    try {
      in=new FileInputStream(localFile);
    }
 catch (    FileNotFoundException e) {
      final String message=""String_Node_Str"" + localFile + ""String_Node_Str"";
      log.debug(message,e);
      throw new IOFailure(message,e);
    }
    log.debug(""String_Node_Str"" + file.getName() + ""String_Node_Str""+ ftpFileName+ ""String_Node_Str""+ ftpServerName);
    try {
      logOn();
      if (useChecksums) {
        in=new DigestInputStream(in,MD5.getMessageDigestInstance());
      }
      boolean success=false;
      int tried=0;
      while (!success && tried < FTP_RETRIES) {
        tried++;
        try {
          success=currentFTPClient.storeFile(ftpFileName,in);
          if (!success) {
            log.debug(""String_Node_Str"" + tried + ""String_Node_Str""+ FTP_RETRIES+ ""String_Node_Str""+ getFtpErrorMessage());
          }
        }
 catch (        IOException e) {
          String message=""String_Node_Str"" + ftpFileName + ""String_Node_Str""+ tried+ ""String_Node_Str""+ FTP_RETRIES;
          if (e instanceof CopyStreamException) {
            CopyStreamException realException=(CopyStreamException)e;
            message+=""String_Node_Str"" + realException.getIOException() + ""String_Node_Str"";
          }
          log.debug(message,e);
        }
      }
      if (!success) {
        final String msg=""String_Node_Str"" + localFile + ""String_Node_Str""+ tried+ ""String_Node_Str"";
        log.warn(msg);
        throw new IOFailure(msg);
      }
      log.debug(""String_Node_Str"" + ftpFileName + ""String_Node_Str"");
      if (useChecksums) {
        checksum=MD5.toHex(((DigestInputStream)in).getMessageDigest().digest());
        log.debug(""String_Node_Str"" + ftpFileName + ""String_Node_Str""+ checksum);
      }
 else {
        checksum=null;
      }
    }
  finally {
      try {
        if (in != null) {
          in.close();
        }
      }
 catch (      IOException e) {
        log.warn(""String_Node_Str"" + e);
      }
      logOut();
      log.debug(""String_Node_Str"");
    }
  }
  if (fileDeletable) {
    try {
      FileUtils.removeRecursively(localFile);
    }
 catch (    IOFailure e) {
      log.warn(""String_Node_Str"" + localFile,e);
    }
  }
}",0.995191793995085
89607,"/** 
 * Retrieval of the number of retries for retrieving a file from a FTP server. Returns the setting for number of retries.
 * @return The number of retries for the FTP connection, defined in settings.
 */
@Override public int getNumberOfRetries(){
  return CommonSettings.FTP_RETRIES;
}","/** 
 * Retrieval of the number of retries for retrieving a file from a FTP server. Returns the setting for number of retries.
 * @return The number of retries for the FTP connection, defined in settings.
 */
@Override public int getNumberOfRetries(){
  return FTP_RETRIES;
}",0.9734513274336284
89608,"/** 
 * Check if a given domainName is valid domain. A valid domain is an IP  address or a domain name part followed by a TLD as defined in settings.
 * @param domainName A name of a domain (netarkivet.dk)
 * @return true if domain is valid; otherwise it returns false. 
 */
public static boolean isValidDomainName(String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  return VALID_DOMAIN_MATCHER.matcher(domainName).matches();
}","/** 
 * Check if a given domainName is valid domain. A valid domain is an IP  address or a domain name part followed by a TLD as defined in settings.
 * @param domainName A name of a domain (netarkivet.dk)
 * @return true if domain is valid; otherwise it returns false. 
 */
public static boolean isValidDomainName(String domainName){
  ArgumentNotValid.checkNotNull(domainName,""String_Node_Str"");
  return VALID_DOMAIN_MATCHER.matcher(domainName).matches();
}",0.9924487594390508
89609,"/** 
 * Utillity class, do not initialise. 
 */
private DomainUtils(){
}","/** 
 * Utility class, do not initialise. 
 */
private DomainUtils(){
}",0.993006993006993
89610,"/** 
 * Return a domain name. A domain name is defined as either an IP address if the given host is an IP address, or a postfix of the given host name containing one hostnamepart and a TLD as defined in settings. E.g. if '.dk' and 'co.uk' are valid TLDs, www.netarchive.dk will be become netarchive.dk and news.bbc.co.uk will be come bbc.co.uk
 * @param hostname A hostname or IP address.
 * @return A domain name (foo.bar) or IP address, or null if no validhostname could be obtained from the given hostname.  If non-null, the return value is guaranteed to be a valid hostname as determined by isValidDomainName().
 */
public static String domainNameFromHostname(String hostname){
  String result=hostname;
  if (!Constants.IP_KEY_REGEXP.matcher(hostname).matches()) {
    Matcher matcher=HOSTNAME_REGEX.matcher(hostname);
    if (matcher.matches()) {
      result=matcher.group(2);
    }
  }
  if (isValidDomainName(result)) {
    return result;
  }
  return null;
}","/** 
 * Return a domain name. A domain name is defined as either an IP address if the given host is an IP address, or a postfix of the given host name containing one hostnamepart and a TLD as defined in settings. E.g. if '.dk' and 'co.uk' are valid TLDs, www.netarchive.dk will be become netarchive.dk and news.bbc.co.uk will be come bbc.co.uk
 * @param hostname A hostname or IP address. Null hostname is not allowed
 * @return A domain name (foo.bar) or IP address, or null if no validdomain could be obtained from the given hostname.  If non-null, the return value is guaranteed to be a valid domain as determined by isValidDomainName().
 */
public static String domainNameFromHostname(String hostname){
  ArgumentNotValid.checkNotNull(hostname,""String_Node_Str"");
  String result=hostname;
  if (!Constants.IP_KEY_REGEXP.matcher(hostname).matches()) {
    Matcher matcher=HOSTNAME_REGEX.matcher(hostname);
    if (matcher.matches()) {
      result=matcher.group(2);
    }
  }
  if (isValidDomainName(result)) {
    return result;
  }
  return null;
}",0.9416419386745796
89611,"/** 
 * Get the domain, that the given URL belongs to.
 * @param url an URL
 * @return the domain, that the given URL belongs to.
 */
private String getDomain(String url){
  try {
    URL uri=new URL(url);
    return DomainUtils.domainNameFromHostname(uri.getHost());
  }
 catch (  MalformedURLException e) {
    throw new ArgumentNotValid(""String_Node_Str"" + url + ""String_Node_Str"");
  }
}","/** 
 * Get the domain, that the given URL belongs to.
 * @param url an URL
 * @return the domain, that the given URL belongs to, or null if unable to do so.
 */
private String getDomain(String url){
  try {
    URL uri=new URL(url);
    return DomainUtils.domainNameFromHostname(uri.getHost());
  }
 catch (  MalformedURLException e) {
    throw new ArgumentNotValid(""String_Node_Str"" + url + ""String_Node_Str"");
  }
}",0.965432098765432
89612,"/** 
 * Set the seedlist from a seedlist, where the individual seeds are separated by a '\n' character. Duplicate seeds are removed.
 * @param seedList List of seeds as one String
 */
public void setSeedList(String seedList){
  ArgumentNotValid.checkNotNullOrEmpty(seedList,""String_Node_Str"");
  seedListSet=new HashSet<String>();
  BufferedReader reader=new BufferedReader(new StringReader(seedList));
  String seed;
  try {
    while ((seed=reader.readLine()) != null) {
      seedListSet.add(seed);
    }
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
  if (log.isTraceEnabled()) {
    log.trace(""String_Node_Str"" + seedListSet.size() + ""String_Node_Str"");
  }
}","/** 
 * Set the seedlist of the job from the seedList argument. Individual seeds are separated by a '\n' character.  Duplicate seeds are removed.
 * @param seedList List of seeds as one String
 */
public void setSeedList(String seedList){
  ArgumentNotValid.checkNotNullOrEmpty(seedList,""String_Node_Str"");
  seedListSet=new HashSet<String>();
  BufferedReader reader=new BufferedReader(new StringReader(seedList));
  String seed;
  try {
    while ((seed=reader.readLine()) != null) {
      seedListSet.add(seed);
    }
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
 finally {
    IOUtils.closeQuietly(reader);
  }
  if (log.isTraceEnabled()) {
    log.trace(""String_Node_Str"" + seedListSet.size() + ""String_Node_Str"");
  }
}",0.926027397260274
89613,"/** 
 * Returns a list of sorted seeds for this job. The sorting is by domain, and inside each domain, the list is sorted by url
 * @return a list of sorted seeds for this job.
 */
public List<String> getSortedSeedList(){
  Map<String,Set<String>> urlMap=new HashMap<String,Set<String>>();
  for (  String seed : seedListSet) {
    String url;
    if (!seed.matches(Constants.PROTOCOL_REGEXP)) {
      url=""String_Node_Str"" + seed;
    }
 else {
      url=seed;
    }
    String domain=getDomain(url);
    Set<String> set;
    if (urlMap.containsKey(domain)) {
      set=urlMap.get(domain);
    }
 else {
      set=new TreeSet<String>();
      urlMap.put(domain,set);
    }
    set.add(seed);
  }
  List<String> result=new ArrayList<String>();
  for (  Set<String> set : urlMap.values()) {
    result.addAll(set);
  }
  return result;
}","/** 
 * Returns a list of sorted seeds for this job. The sorting is by domain, and inside each domain, the list is sorted by url
 * @return a list of sorted seeds for this job.
 */
public List<String> getSortedSeedList(){
  Map<String,Set<String>> urlMap=new HashMap<String,Set<String>>();
  for (  String seed : seedListSet) {
    String url;
    if (!seed.matches(Constants.PROTOCOL_REGEXP)) {
      url=""String_Node_Str"" + seed;
    }
 else {
      url=seed;
    }
    String domain=getDomain(url);
    if (domain == null) {
      continue;
    }
    Set<String> set;
    if (urlMap.containsKey(domain)) {
      set=urlMap.get(domain);
    }
 else {
      set=new TreeSet<String>();
      urlMap.put(domain,set);
    }
    set.add(seed);
  }
  List<String> result=new ArrayList<String>();
  for (  Set<String> set : urlMap.values()) {
    result.addAll(set);
  }
  return result;
}",0.972093023255814
89614,"/** 
 * Test method getSortedSeedList.
 */
public void testGetSortedSeedList() throws Exception {
  DomainConfiguration dc=TestInfo.getNetarkivetConfiguration();
  dc.setMaxBytes(-1);
  final int harvestNum=4;
  Job j=new Job(42L,dc,JobPriority.HIGHPRIORITY,-1L,-1L,Constants.DEFAULT_MAX_JOB_RUNNING_TIME,harvestNum);
  String seeds=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
  j.setSeedList(seeds);
  List<String> list=j.getSortedSeedList();
  assertTrue(list.size() == 18);
  Set<Integer> order=new TreeSet<Integer>();
  order.add(Integer.valueOf(list.indexOf(""String_Node_Str"")));
  order.add(Integer.valueOf(list.indexOf(""String_Node_Str"")));
  order.add(Integer.valueOf(list.indexOf(""String_Node_Str"")));
  int last=-1;
  for (  Integer i : order) {
    if (last != -1) {
      assertTrue(""String_Node_Str"",i == (last + 1));
    }
    last=i;
  }
}","/** 
 * Test method getSortedSeedList.
 */
public void testGetSortedSeedList() throws Exception {
  DomainConfiguration dc=TestInfo.getNetarkivetConfiguration();
  dc.setMaxBytes(-1);
  final int harvestNum=4;
  Job j=new Job(42L,dc,JobPriority.HIGHPRIORITY,-1L,-1L,Constants.DEFAULT_MAX_JOB_RUNNING_TIME,harvestNum);
  String seeds=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
  j.setSeedList(seeds);
  List<String> list=j.getSortedSeedList();
  assertTrue(list.size() == 18);
  Set<Integer> order=new TreeSet<Integer>();
  order.add(Integer.valueOf(list.indexOf(""String_Node_Str"")));
  order.add(Integer.valueOf(list.indexOf(""String_Node_Str"")));
  order.add(Integer.valueOf(list.indexOf(""String_Node_Str"")));
  int last=-1;
  for (  Integer i : order) {
    if (last != -1) {
      assertTrue(""String_Node_Str"",i == (last + 1));
    }
    last=i;
  }
  String alternateSeeds=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"";
  j.setSeedList(alternateSeeds);
  List<String> sortedSeeds=j.getSortedSeedList();
  assertTrue(""String_Node_Str"",sortedSeeds.size() == 2);
}",0.9064914992272024
89615,"/** 
 * Constructor. throws IllegalState if unable to initialize the database.
 */
private ReplicaCacheDatabase(){
  Connection con=ArchiveDBConnection.get();
  try {
    int retries=0;
    boolean initialized=false;
    while (retries < INIT_DB_RETRIES && !initialized) {
      retries++;
      try {
        initialiseDB(con);
        initialized=true;
        log.info(""String_Node_Str"");
        return;
      }
 catch (      IOFailure e) {
        log.warn(""String_Node_Str"" + ""String_Node_Str"",e);
        if (retries < INIT_DB_RETRIES) {
          log.info(""String_Node_Str"" + WAIT_BEFORE_INIT_RETRY + ""String_Node_Str"");
          waitSome();
        }
 else {
          throw new IllegalState(""String_Node_Str"");
        }
      }
    }
  }
  finally {
    ArchiveDBConnection.release(con);
  }
}","/** 
 * Constructor. throws IllegalState if unable to initialize the database.
 */
private ReplicaCacheDatabase(){
  Connection con=ArchiveDBConnection.get();
  try {
    int retries=0;
    boolean initialized=false;
    while (retries < INIT_DB_RETRIES && !initialized) {
      retries++;
      try {
        initialiseDB(con);
        initialized=true;
        log.info(""String_Node_Str"");
        return;
      }
 catch (      IOFailure e) {
        if (retries < INIT_DB_RETRIES) {
          log.info(""String_Node_Str"" + ""String_Node_Str"" + WAIT_BEFORE_INIT_RETRY + ""String_Node_Str"",e);
          waitSome();
        }
 else {
          throw new IllegalState(""String_Node_Str"");
        }
      }
    }
  }
  finally {
    ArchiveDBConnection.release(con);
  }
}",0.9485060394151305
89616,"/** 
 * Combine a number of crawl.log files into one Lucene index.  This index is placed as gzip files under the directory returned by getCacheFile().
 * @param rawfiles The map from job ID into crawl.log contents. Nonull values are allowed in this map.
 */
protected synchronized void combine(Map<Long,File> rawfiles){
  long datasetSize=rawfiles.values().size();
  log.info(""String_Node_Str"" + datasetSize + ""String_Node_Str"");
  File resultFile=getCacheFile(rawfiles.keySet());
  Set<File> tmpfiles=new HashSet<File>();
  String indexLocation=resultFile.getAbsolutePath() + ""String_Node_Str"";
  try {
    DigestIndexer indexer=createStandardIndexer(indexLocation);
    final boolean verboseIndexing=false;
    DigestOptions indexingOptions=new DigestOptions(this.useBlacklist,verboseIndexing,this.mimeFilter);
    long count=0;
    Set<IndexingState> outstandingJobs=new HashSet<IndexingState>();
    final int maxThreads=Settings.getInt(ArchiveSettings.INDEXSERVER_INDEXING_MAXTHREADS);
    ThreadPoolExecutor executor=new ThreadPoolExecutor(maxThreads,maxThreads,0L,TimeUnit.MILLISECONDS,new LinkedBlockingQueue<Runnable>());
    executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());
    for (    Map.Entry<Long,File> entry : rawfiles.entrySet()) {
      Long jobId=entry.getKey();
      File crawlLog=entry.getValue();
      File tmpFile=new File(FileUtils.getTempDir(),UUID.randomUUID().toString());
      tmpfiles.add(tmpFile);
      String localindexLocation=tmpFile.getAbsolutePath();
      Long cached=cdxcache.cache(jobId);
      if (cached == null) {
        log.warn(""String_Node_Str"" + entry.getKey() + ""String_Node_Str"");
        continue;
      }
      File cachedCDXFile=cdxcache.getCacheFile(cached);
      count++;
      log.debug(""String_Node_Str"" + jobId + ""String_Node_Str""+ count+ ""String_Node_Str""+ datasetSize);
      Callable<Boolean> task=new DigestIndexerWorker(localindexLocation,jobId,crawlLog,cachedCDXFile,indexingOptions);
      Future<Boolean> result=executor.submit(task);
      outstandingJobs.add(new IndexingState(jobId,localindexLocation,result));
    }
    Set<Directory> subindices=new HashSet<Directory>();
    long combineTimeout=Settings.getLong(ArchiveSettings.INDEXSERVER_INDEXING_TIMEOUT);
    long timeOutTime=System.currentTimeMillis() + combineTimeout;
    while (outstandingJobs.size() > 0) {
      Iterator<IndexingState> iterator=outstandingJobs.iterator();
      if (timeOutTime < System.currentTimeMillis()) {
        log.warn(""String_Node_Str"" + combineTimeout + ""String_Node_Str""+ ""String_Node_Str""+ outstandingJobs+ ""String_Node_Str"");
        break;
      }
      while (iterator.hasNext()) {
        Future<Boolean> nextResult;
        IndexingState next=iterator.next();
        if (next.getResultObject().isDone()) {
          nextResult=next.getResultObject();
          try {
            if (nextResult.get()) {
              subindices.add(new SimpleFSDirectory(new File(next.getIndex())));
            }
 else {
              log.warn(""String_Node_Str"" + next.getJobIdentifier() + ""String_Node_Str"");
            }
          }
 catch (          InterruptedException e) {
            log.warn(""String_Node_Str"" + ""String_Node_Str"",e);
          }
catch (          ExecutionException e) {
            log.warn(""String_Node_Str"" + ""String_Node_Str"",e);
          }
          iterator.remove();
        }
      }
      sleepAwhile();
    }
    log.debug(""String_Node_Str"");
    indexer.getIndex().addIndexesNoOptimize(subindices.toArray(new Directory[0]));
    long docsInIndex=indexer.getIndex().numDocs();
    indexer.close(false);
    ZipUtils.gzipFiles(new File(indexLocation),resultFile);
    log.info(""String_Node_Str"" + datasetSize + ""String_Node_Str""+ docsInIndex+ ""String_Node_Str""+ FileUtils.getHumanReadableFileSize(resultFile));
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"" + resultFile.getAbsolutePath(),e);
  }
 finally {
    FileUtils.removeRecursively(new File(indexLocation));
    for (    File temporaryFile : tmpfiles) {
      FileUtils.removeRecursively(temporaryFile);
    }
  }
}","/** 
 * Combine a number of crawl.log files into one Lucene index.  This index is placed as gzip files under the directory returned by getCacheFile().
 * @param rawfiles The map from job ID into crawl.log contents. Nonull values are allowed in this map.
 */
protected synchronized void combine(Map<Long,File> rawfiles){
  long datasetSize=rawfiles.values().size();
  log.info(""String_Node_Str"" + datasetSize + ""String_Node_Str"");
  File resultDir=getCacheFile(rawfiles.keySet());
  Set<File> tmpfiles=new HashSet<File>();
  String indexLocation=resultDir.getAbsolutePath() + ""String_Node_Str"";
  try {
    DigestIndexer indexer=createStandardIndexer(indexLocation);
    final boolean verboseIndexing=false;
    DigestOptions indexingOptions=new DigestOptions(this.useBlacklist,verboseIndexing,this.mimeFilter);
    long count=0;
    Set<IndexingState> outstandingJobs=new HashSet<IndexingState>();
    final int maxThreads=Settings.getInt(ArchiveSettings.INDEXSERVER_INDEXING_MAXTHREADS);
    ThreadPoolExecutor executor=new ThreadPoolExecutor(maxThreads,maxThreads,0L,TimeUnit.MILLISECONDS,new LinkedBlockingQueue<Runnable>());
    executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());
    for (    Map.Entry<Long,File> entry : rawfiles.entrySet()) {
      Long jobId=entry.getKey();
      File crawlLog=entry.getValue();
      File tmpFile=new File(FileUtils.getTempDir(),UUID.randomUUID().toString());
      tmpfiles.add(tmpFile);
      String localindexLocation=tmpFile.getAbsolutePath();
      Long cached=cdxcache.cache(jobId);
      if (cached == null) {
        log.warn(""String_Node_Str"" + entry.getKey() + ""String_Node_Str"");
        continue;
      }
      File cachedCDXFile=cdxcache.getCacheFile(cached);
      count++;
      log.debug(""String_Node_Str"" + jobId + ""String_Node_Str""+ count+ ""String_Node_Str""+ datasetSize);
      Callable<Boolean> task=new DigestIndexerWorker(localindexLocation,jobId,crawlLog,cachedCDXFile,indexingOptions);
      Future<Boolean> result=executor.submit(task);
      outstandingJobs.add(new IndexingState(jobId,localindexLocation,result));
    }
    Set<Directory> subindices=new HashSet<Directory>();
    long combineTimeout=Settings.getLong(ArchiveSettings.INDEXSERVER_INDEXING_TIMEOUT);
    long timeOutTime=System.currentTimeMillis() + combineTimeout;
    while (outstandingJobs.size() > 0) {
      Iterator<IndexingState> iterator=outstandingJobs.iterator();
      if (timeOutTime < System.currentTimeMillis()) {
        log.warn(""String_Node_Str"" + combineTimeout + ""String_Node_Str""+ ""String_Node_Str""+ outstandingJobs+ ""String_Node_Str"");
        break;
      }
      while (iterator.hasNext()) {
        Future<Boolean> nextResult;
        IndexingState next=iterator.next();
        if (next.getResultObject().isDone()) {
          nextResult=next.getResultObject();
          try {
            if (nextResult.get()) {
              subindices.add(new SimpleFSDirectory(new File(next.getIndex())));
            }
 else {
              log.warn(""String_Node_Str"" + next.getJobIdentifier() + ""String_Node_Str"");
            }
          }
 catch (          InterruptedException e) {
            log.warn(""String_Node_Str"" + ""String_Node_Str"",e);
          }
catch (          ExecutionException e) {
            log.warn(""String_Node_Str"" + ""String_Node_Str"",e);
          }
          iterator.remove();
        }
      }
      sleepAwhile();
    }
    log.debug(""String_Node_Str"");
    indexer.getIndex().addIndexesNoOptimize(subindices.toArray(new Directory[0]));
    long docsInIndex=indexer.getIndex().numDocs();
    indexer.close(false);
    ZipUtils.gzipFiles(new File(indexLocation),resultDir);
    log.info(""String_Node_Str"" + datasetSize + ""String_Node_Str""+ docsInIndex+ ""String_Node_Str""+ FileUtils.getHumanReadableFileSize(resultDir));
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"" + resultDir.getAbsolutePath(),e);
  }
 finally {
    FileUtils.removeRecursively(new File(indexLocation));
    for (    File temporaryFile : tmpfiles) {
      FileUtils.removeRecursively(temporaryFile);
    }
  }
}",0.9631968905623708
89617,"/** 
 * Get a humanly readable representation of the file size. The number is given with 2 decimals.
 * @param aFile a File object
 * @return a humanly readable representation of the file size (rounded)
 */
public static String getHumanReadableFileSize(File aFile){
  ArgumentNotValid.checkNotNull(aFile,""String_Node_Str"");
  final long bytesPerOneKilobyte=1000L;
  final long bytesPerOneMegabyte=1000000L;
  final long bytesPerOneGigabyte=1000000000L;
  double filesize=aFile.length();
  NumberFormat decFormat=new DecimalFormat(""String_Node_Str"");
  if (filesize < bytesPerOneKilobyte) {
    return filesize + ""String_Node_Str"";
  }
 else   if (filesize >= bytesPerOneKilobyte && filesize < bytesPerOneMegabyte) {
    return decFormat.format(filesize / bytesPerOneKilobyte) + ""String_Node_Str"";
  }
 else   if (filesize >= bytesPerOneMegabyte && filesize < bytesPerOneGigabyte) {
    return decFormat.format(filesize / bytesPerOneMegabyte) + ""String_Node_Str"";
  }
 else {
    return decFormat.format(filesize / bytesPerOneGigabyte) + ""String_Node_Str"";
  }
}","/** 
 * Get a humanly readable representation of the file size. If the file is a directory, the size is the aggregate of the files in the directory except that subdirectories are ignored. The number is given with 2 decimals.
 * @param aFile a File object
 * @return a humanly readable representation of the file size (rounded)
 */
public static String getHumanReadableFileSize(File aFile){
  ArgumentNotValid.checkNotNull(aFile,""String_Node_Str"");
  final long bytesPerOneKilobyte=1000L;
  final long bytesPerOneMegabyte=1000000L;
  final long bytesPerOneGigabyte=1000000000L;
  double filesize=0L;
  if (aFile.isDirectory()) {
    for (    File f : aFile.listFiles()) {
      if (f.isFile()) {
        filesize=+f.length();
      }
    }
  }
 else {
    filesize=aFile.length();
  }
  NumberFormat decFormat=new DecimalFormat(""String_Node_Str"");
  if (filesize < bytesPerOneKilobyte) {
    return filesize + ""String_Node_Str"";
  }
 else   if (filesize >= bytesPerOneKilobyte && filesize < bytesPerOneMegabyte) {
    return decFormat.format(filesize / bytesPerOneKilobyte) + ""String_Node_Str"";
  }
 else   if (filesize >= bytesPerOneMegabyte && filesize < bytesPerOneGigabyte) {
    return decFormat.format(filesize / bytesPerOneMegabyte) + ""String_Node_Str"";
  }
 else {
    return decFormat.format(filesize / bytesPerOneGigabyte) + ""String_Node_Str"";
  }
}",0.8772219925589086
89618,"/** 
 * Execution of the batchjob in its own thread (use start() instead).
 * @throws IOFailure If an IOException is caught while writing the results. 
 */
public void run() throws IOFailure {
  ViewerArcRepositoryClient arcrep=ArcRepositoryClientFactory.getViewerInstance();
  String timestamp=new Long(new Date().getTime()).toString();
  String jobName=BatchGUI.getJobName(batchJob.getClass().getName());
  if (batchJob instanceof LoadableJarBatchJob) {
    LoadableJarBatchJob ljbj=(LoadableJarBatchJob)batchJob;
    jobName=BatchGUI.getJobName(ljbj.getLoadedJobClass());
    log.debug(""String_Node_Str"" + jobName + ""String_Node_Str"");
  }
  try {
    File outputFile=new File(BatchGUI.getBatchDir(),jobName + Constants.NAME_TIMSTAMP_SEPARATOR + timestamp+ Constants.OUTPUT_FILE_EXTENSION);
    outputFile.createNewFile();
    File eventLogFile=new File(BatchGUI.getBatchDir(),jobName + Constants.NAME_TIMSTAMP_SEPARATOR + timestamp+ Constants.ERROR_FILE_EXTENSION);
    eventLogFile.createNewFile();
    batchJob.processOnlyFilesMatching(regex);
    FileWriter fw=new FileWriter(eventLogFile,APPEND);
    String processInfo=""String_Node_Str"" + jobName + ""String_Node_Str""+ timestamp+ ""String_Node_Str""+ rep.getId()+ ""String_Node_Str""+ regex+ ""String_Node_Str"";
    log.info(processInfo);
    fw.write(processInfo + ""String_Node_Str"");
    BatchStatus status=arcrep.batch(batchJob,rep.getId());
    final Collection<File> failedFiles=status.getFilesFailed();
    Collection<ExceptionOccurrence> exceptions=status.getExceptions();
    processInfo=""String_Node_Str"" + jobName + ""String_Node_Str""+ status.getNoOfFilesProcessed()+ ""String_Node_Str"";
    log.info(processInfo);
    fw.write(processInfo + ""String_Node_Str"");
    processInfo=""String_Node_Str"" + jobName + ""String_Node_Str""+ failedFiles.size()+ ""String_Node_Str""+ exceptions.size()+ ""String_Node_Str"";
    log.info(processInfo);
    fw.write(processInfo + ""String_Node_Str"");
    if (outputFile != null && outputFile.exists()) {
      status.copyResults(outputFile);
    }
 else {
      log.warn(""String_Node_Str"" + ""String_Node_Str"" + StreamUtils.getInputStreamAsString(status.getResultFile().getInputStream()));
    }
    if (!failedFiles.isEmpty()) {
      fw.write(""String_Node_Str"" + failedFiles.size() + ""String_Node_Str"");
      for (      File f : failedFiles) {
        fw.write(f.getPath() + ""String_Node_Str"");
      }
    }
    log.info(""String_Node_Str"" + jobName + ""String_Node_Str""+ exceptions.size()+ ""String_Node_Str"");
    fw.write(""String_Node_Str"" + exceptions.size() + ""String_Node_Str"");
    if (!exceptions.isEmpty()) {
      for (      ExceptionOccurrence e : exceptions) {
        fw.write(e.getFileName() + ""String_Node_Str"");
        fw.write(e.getException() + ""String_Node_Str"");
      }
    }
    fw.flush();
    fw.close();
  }
 catch (  IOException e) {
    String errMsg=""String_Node_Str"" + jobName + ""String_Node_Str""+ timestamp+ ""String_Node_Str"";
    log.error(errMsg,e);
    throw new IOFailure(errMsg,e);
  }
catch (  Throwable e) {
    log.error(""String_Node_Str"",e);
    throw new IOFailure(""String_Node_Str"",e);
  }
}","/** 
 * Execution of the batchjob in its own thread (use start() instead).
 * @throws IOFailure If an IOException is caught while writing the results. 
 */
public void run() throws IOFailure {
  ViewerArcRepositoryClient arcrep=ArcRepositoryClientFactory.getViewerInstance();
  String timestamp=Long.valueOf(new Date().getTime()).toString();
  String jobName=BatchGUI.getJobName(batchJob.getClass().getName());
  if (batchJob instanceof LoadableJarBatchJob) {
    LoadableJarBatchJob ljbj=(LoadableJarBatchJob)batchJob;
    jobName=BatchGUI.getJobName(ljbj.getLoadedJobClass());
    log.debug(""String_Node_Str"" + jobName + ""String_Node_Str"");
  }
  try {
    File outputFile=new File(BatchGUI.getBatchDir(),jobName + Constants.NAME_TIMSTAMP_SEPARATOR + timestamp+ Constants.OUTPUT_FILE_EXTENSION);
    outputFile.createNewFile();
    File eventLogFile=new File(BatchGUI.getBatchDir(),jobName + Constants.NAME_TIMSTAMP_SEPARATOR + timestamp+ Constants.ERROR_FILE_EXTENSION);
    eventLogFile.createNewFile();
    batchJob.processOnlyFilesMatching(regex);
    FileWriter fw=new FileWriter(eventLogFile,APPEND);
    String processInfo=""String_Node_Str"" + jobName + ""String_Node_Str""+ timestamp+ ""String_Node_Str""+ rep.getId()+ ""String_Node_Str""+ regex+ ""String_Node_Str"";
    log.info(processInfo);
    fw.write(processInfo + ""String_Node_Str"");
    BatchStatus status=arcrep.batch(batchJob,rep.getId());
    final Collection<File> failedFiles=status.getFilesFailed();
    Collection<ExceptionOccurrence> exceptions=status.getExceptions();
    processInfo=""String_Node_Str"" + jobName + ""String_Node_Str""+ status.getNoOfFilesProcessed()+ ""String_Node_Str"";
    log.info(processInfo);
    fw.write(processInfo + ""String_Node_Str"");
    processInfo=""String_Node_Str"" + jobName + ""String_Node_Str""+ failedFiles.size()+ ""String_Node_Str""+ exceptions.size()+ ""String_Node_Str"";
    log.info(processInfo);
    fw.write(processInfo + ""String_Node_Str"");
    if (outputFile != null && outputFile.exists()) {
      status.copyResults(outputFile);
    }
 else {
      log.warn(""String_Node_Str"" + ""String_Node_Str"" + StreamUtils.getInputStreamAsString(status.getResultFile().getInputStream()));
    }
    if (!failedFiles.isEmpty()) {
      fw.write(""String_Node_Str"" + failedFiles.size() + ""String_Node_Str"");
      for (      File f : failedFiles) {
        fw.write(f.getPath() + ""String_Node_Str"");
      }
    }
    log.info(""String_Node_Str"" + jobName + ""String_Node_Str""+ exceptions.size()+ ""String_Node_Str"");
    fw.write(""String_Node_Str"" + exceptions.size() + ""String_Node_Str"");
    if (!exceptions.isEmpty()) {
      for (      ExceptionOccurrence e : exceptions) {
        fw.write(e.getFileName() + ""String_Node_Str"");
        fw.write(e.getException() + ""String_Node_Str"");
      }
    }
    fw.flush();
    fw.close();
  }
 catch (  IOException e) {
    String errMsg=""String_Node_Str"" + jobName + ""String_Node_Str""+ timestamp+ ""String_Node_Str"";
    log.error(errMsg,e);
    throw new IOFailure(errMsg,e);
  }
catch (  Throwable e) {
    log.error(""String_Node_Str"",e);
    throw new IOFailure(""String_Node_Str"",e);
  }
}",0.9980787704130644
89619,"/** 
 * Test of preparecombine.
 * @throws Exception
 */
public void testPrepareCombine() throws NoSuchFieldException, IllegalAccessException {
  CrawlLogIndexCache cache=new FullCrawlLogIndexCache();
  ReflectUtils.getPrivateField(CrawlLogIndexCache.class,""String_Node_Str"").set(cache,new CDXDataCache(){
    public Long cache(    long ID){
      if (ID % 3 == 0) {
        return null;
      }
 else {
        return ID;
      }
    }
  }
);
  ReflectUtils.getPrivateField(CombiningMultiFileBasedCache.class,""String_Node_Str"").set(cache,new CrawlLogDataCache(){
    public File getCacheFile(    Long id){
      return new File(TestInfo.WORKING_DIR,""String_Node_Str"" + id);
    }
    protected Long cacheData(    Long id){
      return null;
    }
  }
);
  Set<Long> jobIDs=new HashSet<Long>();
  jobIDs.add(1L);
  cache.prepareCombine(jobIDs);
  LogUtils.flushLogs(CrawlLogIndexCache.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"" + jobIDs.size() + ""String_Node_Str""+ jobIDs,TestInfo.LOG_FILE);
}","/** 
 * Test of preparecombine.
 * @throws Exception
 */
public void testPrepareCombine() throws NoSuchFieldException, IllegalAccessException {
  CrawlLogIndexCache cache=new FullCrawlLogIndexCache();
  ReflectUtils.getPrivateField(CrawlLogIndexCache.class,""String_Node_Str"").set(cache,new CDXDataCache(){
    public Long cache(    Long ID){
      if (ID % 3 == 0) {
        return null;
      }
 else {
        return ID;
      }
    }
  }
);
  ReflectUtils.getPrivateField(CombiningMultiFileBasedCache.class,""String_Node_Str"").set(cache,new CrawlLogDataCache(){
    public File getCacheFile(    Long id){
      return new File(TestInfo.WORKING_DIR,""String_Node_Str"" + id);
    }
    protected Long cacheData(    Long id){
      return null;
    }
  }
);
  Set<Long> jobIDs=new HashSet<Long>();
  jobIDs.add(1L);
  cache.prepareCombine(jobIDs);
  LogUtils.flushLogs(CrawlLogIndexCache.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"" + jobIDs.size() + ""String_Node_Str""+ jobIDs,TestInfo.LOG_FILE);
}",0.9990366088631984
89620,"public Long cache(long ID){
  if (ID % 3 == 0) {
    return null;
  }
 else {
    return ID;
  }
}","public Long cache(Long ID){
  if (ID % 3 == 0) {
    return null;
  }
 else {
    return ID;
  }
}",0.9897959183673468
89621,"public GetListener(String arcFileName,long offset){
  this.arcFileName=arcFileName;
  this.offset=offset;
  try {
    HashMap<String,Object> map=new HashMap<String,Object>();
    for (    Object o : ARCConstants.REQUIRED_VERSION_1_HEADER_FIELDS) {
      map.put((String)o,Integer.toString(CONTENT.length()));
    }
    map.put(ARCConstants.ABSOLUTE_OFFSET_KEY,new Long(0L));
    ARCRecordMetaData meta=new ARCRecordMetaData(""String_Node_Str"",map);
    InputStream is=new ByteArrayInputStream(CONTENT.getBytes());
    myRec=new ARCRecord(is,meta,0,false,false,false);
  }
 catch (  IOException e) {
    myRec=null;
  }
}","public GetListener(String arcFileName,long offset){
  this.arcFileName=arcFileName;
  this.offset=offset;
  try {
    HashMap<String,Object> map=new HashMap<String,Object>();
    for (    Object o : ARCConstants.REQUIRED_VERSION_1_HEADER_FIELDS) {
      map.put((String)o,Integer.toString(CONTENT.length()));
    }
    map.put(ARCConstants.ABSOLUTE_OFFSET_KEY,Long.valueOf(0L));
    ARCRecordMetaData meta=new ARCRecordMetaData(""String_Node_Str"",map);
    InputStream is=new ByteArrayInputStream(CONTENT.getBytes());
    myRec=new ARCRecord(is,meta,0,false,false,false);
  }
 catch (  IOException e) {
    myRec=null;
  }
}",0.9903381642512076
89622,"/** 
 * Get the singleton, and initialise it if it is new.
 * @return A JMSConnection
 */
public static JMSConnection getInstance(){
  if (instance == null) {
    instance=new JMSConnectionMockupMQ();
    instance.initConnection();
  }
  return instance;
}","/** 
 * Get the singleton, and initialise it if it is new.
 * @return A JMSConnection
 */
public static synchronized JMSConnection getInstance(){
  if (instance == null) {
    instance=new JMSConnectionMockupMQ();
    instance.initConnection();
  }
  return instance;
}",0.9752380952380952
89623,"/** 
 * Returns an OK BitarchiveRecord. Content is simply arcfile name and index encoded in a stream. 
 */
public BitarchiveRecord get(String arcFile,long index){
  final Map<String,Object> metadata=new HashMap<String,Object>();
  for (  String header_field : (List<String>)ARCConstants.REQUIRED_VERSION_1_HEADER_FIELDS) {
    metadata.put(header_field,""String_Node_Str"");
  }
  metadata.put(ARCConstants.ABSOLUTE_OFFSET_KEY,new Long(0L));
  byte[] data=(""String_Node_Str"" + arcFile + ""String_Node_Str""+ arcFile+ ""String_Node_Str""+ index).getBytes();
  metadata.put(ARCConstants.LENGTH_FIELD_KEY,Integer.toString(data.length));
  try {
    ARCRecordMetaData meta=new ARCRecordMetaData(arcFile,metadata);
    return new BitarchiveRecord(new ARCRecord(new ByteArrayInputStream(data),meta),arcFile);
  }
 catch (  IOException e) {
    fail(""String_Node_Str"");
    return null;
  }
}","/** 
 * Returns an OK BitarchiveRecord. Content is simply arcfile name and index encoded in a stream. 
 */
public BitarchiveRecord get(String arcFile,long index){
  final Map<String,Object> metadata=new HashMap<String,Object>();
  for (  String header_field : (List<String>)ARCConstants.REQUIRED_VERSION_1_HEADER_FIELDS) {
    metadata.put(header_field,""String_Node_Str"");
  }
  metadata.put(ARCConstants.ABSOLUTE_OFFSET_KEY,Long.valueOf(0L));
  byte[] data=(""String_Node_Str"" + arcFile + ""String_Node_Str""+ arcFile+ ""String_Node_Str""+ index).getBytes();
  metadata.put(ARCConstants.LENGTH_FIELD_KEY,Integer.toString(data.length));
  try {
    ARCRecordMetaData meta=new ARCRecordMetaData(arcFile,metadata);
    return new BitarchiveRecord(new ARCRecord(new ByteArrayInputStream(data),meta),arcFile);
  }
 catch (  IOException e) {
    fail(""String_Node_Str"");
    return null;
  }
}",0.9931895573212258
89624,"/** 
 * Test that files larger than 2GB can be copied! 
 */
public void testCopyLargeFiles() throws IOException {
  byte[] block=new byte[BLOCKSIZE];
  SUBDIR.mkdirs();
  File largeFile=new File(SUBDIR,LARGE_FILE);
  OutputStream os=new BufferedOutputStream(new FileOutputStream(largeFile));
  System.out.println(""String_Node_Str"");
  for (long l=0; l < LARGE / ((long)BLOCKSIZE) + 1L; l++) {
    os.write(block);
  }
  System.out.println(""String_Node_Str"");
  FileUtils.copyDirectory(SUBDIR,SUBDIR2);
  File file1=new File(SUBDIR,LARGE_FILE);
  File file2=new File(SUBDIR2,LARGE_FILE);
  assertEquals(""String_Node_Str"",file1.length(),file2.length());
}","/** 
 * Test that files larger than 2GB can be copied! 
 */
public void testCopyLargeFiles() throws IOException {
  byte[] block=new byte[BLOCKSIZE];
  SUBDIR.mkdirs();
  File largeFile=new File(SUBDIR,LARGE_FILE);
  OutputStream os=null;
  try {
    os=new BufferedOutputStream(new FileOutputStream(largeFile));
    System.out.println(""String_Node_Str"");
    for (long l=0; l < LARGE / ((long)BLOCKSIZE) + 1L; l++) {
      os.write(block);
    }
    System.out.println(""String_Node_Str"");
    FileUtils.copyDirectory(SUBDIR,SUBDIR2);
    File file1=new File(SUBDIR,LARGE_FILE);
    File file2=new File(SUBDIR2,LARGE_FILE);
    assertEquals(""String_Node_Str"",file1.length(),file2.length());
  }
  finally {
    IOUtils.closeQuietly(os);
  }
}",0.936200716845878
89625,"/** 
 * Test that files larger than 2GB can be gzipped and gunzipped! 
 */
public void testGZipGUnZipLargeFiles() throws IOException {
  byte[] block=new byte[BLOCKSIZE];
  SUBDIR.mkdirs();
  File largeFile=new File(SUBDIR,LARGE_FILE);
  OutputStream os=new FileOutputStream(largeFile);
  System.out.println(""String_Node_Str"");
  for (long l=0; l < LARGE / ((long)BLOCKSIZE) + 1L; l++) {
    os.write(block);
  }
  System.out.println(""String_Node_Str"");
  ZipUtils.gzipFiles(SUBDIR,SUBDIR2);
  System.out.println(""String_Node_Str"");
  ZipUtils.gunzipFiles(SUBDIR2,SUBDIR3);
  File file1=new File(SUBDIR,LARGE_FILE);
  File file2=new File(SUBDIR3,LARGE_FILE);
  assertEquals(""String_Node_Str"",file1.length(),file2.length());
}","/** 
 * Test that files larger than 2GB can be gzipped and gunzipped! 
 */
public void testGZipGUnZipLargeFiles() throws IOException {
  byte[] block=new byte[BLOCKSIZE];
  SUBDIR.mkdirs();
  File largeFile=new File(SUBDIR,LARGE_FILE);
  OutputStream os=null;
  try {
    os=new FileOutputStream(largeFile);
    System.out.println(""String_Node_Str"");
    for (long l=0; l < LARGE / ((long)BLOCKSIZE) + 1L; l++) {
      os.write(block);
    }
    System.out.println(""String_Node_Str"");
    ZipUtils.gzipFiles(SUBDIR,SUBDIR2);
    System.out.println(""String_Node_Str"");
    ZipUtils.gunzipFiles(SUBDIR2,SUBDIR3);
    File file1=new File(SUBDIR,LARGE_FILE);
    File file2=new File(SUBDIR3,LARGE_FILE);
    assertEquals(""String_Node_Str"",file1.length(),file2.length());
  }
  finally {
    IOUtils.closeQuietly(os);
  }
}",0.9397278029812054
89626,"/** 
 * Test the dk.netarkivet.common.utils.arc.BatchFilter.getMimetypeBatchFilter Test the validity of the given mimetype
 */
public void testGetMimetype(){
  String invalidMimetype=new String(""String_Node_Str"");
  String validMimetype=""String_Node_Str"";
  ARCBatchFilter cfilter=null;
  try {
    cfilter=ARCBatchFilter.getMimetypeBatchFilter(invalidMimetype);
    fail(""String_Node_Str"" + invalidMimetype);
  }
 catch (  MimeTypeParseException e) {
  }
  try {
    cfilter=ARCBatchFilter.getMimetypeBatchFilter(validMimetype);
  }
 catch (  MimeTypeParseException e) {
    fail(""String_Node_Str"" + validMimetype);
  }
catch (  Exception e) {
    fail(""String_Node_Str"" + e + ""String_Node_Str""+ validMimetype);
  }
  assertTrue(""String_Node_Str"",cfilter != null);
  assertEquals(cfilter.getName(),""String_Node_Str"");
}","/** 
 * Test the dk.netarkivet.common.utils.arc.BatchFilter.getMimetypeBatchFilter Test the validity of the given mimetype
 */
public void testGetMimetype(){
  String invalidMimetype=""String_Node_Str"";
  String validMimetype=""String_Node_Str"";
  ARCBatchFilter cfilter=null;
  try {
    cfilter=ARCBatchFilter.getMimetypeBatchFilter(invalidMimetype);
    fail(""String_Node_Str"" + invalidMimetype);
  }
 catch (  MimeTypeParseException e) {
  }
  try {
    cfilter=ARCBatchFilter.getMimetypeBatchFilter(validMimetype);
  }
 catch (  MimeTypeParseException e) {
    fail(""String_Node_Str"" + validMimetype);
  }
catch (  Exception e) {
    fail(""String_Node_Str"" + e + ""String_Node_Str""+ validMimetype);
  }
  assertTrue(""String_Node_Str"",cfilter != null);
  assertEquals(cfilter.getName(),""String_Node_Str"");
}",0.9926289926289926
89627,"@Override public boolean processFile(File file,OutputStream os){
  try {
    os.write(new String(file.getName() + ""String_Node_Str"").getBytes());
  }
 catch (  Exception e) {
    return false;
  }
  return true;
}","@Override public boolean processFile(File file,OutputStream os){
  try {
    os.write((file.getName() + ""String_Node_Str"").getBytes());
  }
 catch (  Exception e) {
    return false;
  }
  return true;
}",0.9759615384615384
89628,"@Override public boolean processFile(File file,OutputStream os){
  try {
    os.write(new String(file.getName() + ""String_Node_Str"").getBytes());
  }
 catch (  Exception e) {
    return false;
  }
  return true;
}","@Override public boolean processFile(File file,OutputStream os){
  try {
    os.write((file.getName() + ""String_Node_Str"").getBytes());
  }
 catch (  Exception e) {
    return false;
  }
  return true;
}",0.9759615384615384
89629,"@Override public boolean postProcess(InputStream input,OutputStream output){
  Log log=LogFactory.getLog(this.getClass());
  try {
    List<String> filenames=new ArrayList<String>();
    log.info(""String_Node_Str"");
    BufferedReader br=new BufferedReader(new InputStreamReader(input));
    String line;
    while ((line=br.readLine()) != null) {
      filenames.add(line);
    }
    log.info(""String_Node_Str"");
    Collections.sort(filenames);
    for (    String file : filenames) {
      output.write(file.getBytes());
      output.write(new String(""String_Node_Str"").getBytes());
    }
    return true;
  }
 catch (  Exception e) {
    log.warn(e.getMessage());
    return false;
  }
}","@Override public boolean postProcess(InputStream input,OutputStream output){
  Log log=LogFactory.getLog(this.getClass());
  try {
    List<String> filenames=new ArrayList<String>();
    log.info(""String_Node_Str"");
    BufferedReader br=new BufferedReader(new InputStreamReader(input));
    String line;
    while ((line=br.readLine()) != null) {
      filenames.add(line);
    }
    log.info(""String_Node_Str"");
    Collections.sort(filenames);
    for (    String file : filenames) {
      output.write(file.getBytes());
      output.write(""String_Node_Str"".getBytes());
    }
    return true;
  }
 catch (  Exception e) {
    log.warn(e.getMessage());
    return false;
  }
}",0.9912408759124088
89630,"@Override public void finish(OutputStream os){
  try {
    os.write(new String(""String_Node_Str"").getBytes());
    os.write(new String(""String_Node_Str"" + urlCount + ""String_Node_Str"").getBytes());
    os.write(new String(""String_Node_Str"" + mimeCount + ""String_Node_Str"").getBytes());
    os.write(new String(""String_Node_Str"" + bothCount + ""String_Node_Str"").getBytes());
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"",e);
  }
}","@Override public void finish(OutputStream os){
  try {
    os.write(""String_Node_Str"".getBytes());
    os.write((""String_Node_Str"" + urlCount + ""String_Node_Str"").getBytes());
    os.write((""String_Node_Str"" + mimeCount + ""String_Node_Str"").getBytes());
    os.write((""String_Node_Str"" + bothCount + ""String_Node_Str"").getBytes());
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"",e);
  }
}",0.9164835164835164
89631,"@Override public void processRecord(ARCRecord record,OutputStream os){
  totalCount++;
  boolean valid=true;
  if (record.getMetaData().getUrl().matches(regex)) {
    urlCount++;
  }
 else {
    valid=false;
  }
  if (record.getMetaData().getMimetype().matches(mimetype)) {
    mimeCount++;
  }
 else {
    valid=false;
  }
  if (valid) {
    bothCount++;
    try {
      os.write(new String(record.getMetaData().getUrl() + ""String_Node_Str"" + record.getMetaData().getMimetype()+ ""String_Node_Str"").getBytes());
    }
 catch (    IOException e) {
      throw new IOFailure(""String_Node_Str"",e);
    }
  }
}","@Override public void processRecord(ARCRecord record,OutputStream os){
  totalCount++;
  boolean valid=true;
  if (record.getMetaData().getUrl().matches(regex)) {
    urlCount++;
  }
 else {
    valid=false;
  }
  if (record.getMetaData().getMimetype().matches(mimetype)) {
    mimeCount++;
  }
 else {
    valid=false;
  }
  if (valid) {
    bothCount++;
    try {
      os.write((record.getMetaData().getUrl() + ""String_Node_Str"" + record.getMetaData().getMimetype()+ ""String_Node_Str"").getBytes());
    }
 catch (    IOException e) {
      throw new IOFailure(""String_Node_Str"",e);
    }
  }
}",0.9916805324459236
89632,"public void testParseOptionalLong(){
  Map<String,String[]> parameterMap=new HashMap<String,String[]>();
  parameterMap.put(""String_Node_Str"",new String[]{""String_Node_Str""});
  WebinterfaceTestCase.TestServletRequest request=new WebinterfaceTestCase.TestServletRequest();
  request.setParameterMap(parameterMap);
  PageContext pageContext=new WebinterfaceTestCase.TestPageContext(request);
  assertEquals(""String_Node_Str"",new Long(10L),HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-1L));
  parameterMap.put(""String_Node_Str"",new String[]{""String_Node_Str""});
  assertEquals(""String_Node_Str"",new Long(-11L),HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-1L));
  assertEquals(""String_Node_Str"",new Long(-1L),HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-1L));
  parameterMap.put(""String_Node_Str"",new String[]{Long.toString(((long)Integer.MAX_VALUE) * 5)});
  assertEquals(""String_Node_Str"",new Long(((long)Integer.MAX_VALUE) * 5),HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-1L));
  parameterMap.put(""String_Node_Str"",new String[]{""String_Node_Str""});
  assertEquals(""String_Node_Str"",new Long(-2),HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-2L));
  parameterMap.put(""String_Node_Str"",new String[]{""String_Node_Str""});
  assertEquals(""String_Node_Str"",new Long(-2),HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-2L));
  parameterMap.put(""String_Node_Str"",new String[]{""String_Node_Str""});
  assertEquals(""String_Node_Str"",null,HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",null));
  try {
    parameterMap.put(""String_Node_Str"",new String[]{""String_Node_Str""});
    HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-1L);
    fail(""String_Node_Str"");
  }
 catch (  ForwardedToErrorPage e) {
  }
  try {
    parameterMap.put(""String_Node_Str"",new String[]{""String_Node_Str""});
    Long noLong=HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-1L);
  }
 catch (  ForwardedToErrorPage e) {
    fail(""String_Node_Str"");
  }
}","public void testParseOptionalLong(){
  Map<String,String[]> parameterMap=new HashMap<String,String[]>();
  parameterMap.put(""String_Node_Str"",new String[]{""String_Node_Str""});
  WebinterfaceTestCase.TestServletRequest request=new WebinterfaceTestCase.TestServletRequest();
  request.setParameterMap(parameterMap);
  PageContext pageContext=new WebinterfaceTestCase.TestPageContext(request);
  assertEquals(""String_Node_Str"",Long.valueOf(10L),HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-1L));
  parameterMap.put(""String_Node_Str"",new String[]{""String_Node_Str""});
  assertEquals(""String_Node_Str"",Long.valueOf(-11L),HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-1L));
  assertEquals(""String_Node_Str"",Long.valueOf(-1L),HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-1L));
  parameterMap.put(""String_Node_Str"",new String[]{Long.toString(((long)Integer.MAX_VALUE) * 5)});
  assertEquals(""String_Node_Str"",new Long(((long)Integer.MAX_VALUE) * 5),HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-1L));
  parameterMap.put(""String_Node_Str"",new String[]{""String_Node_Str""});
  assertEquals(""String_Node_Str"",Long.valueOf(-2L),HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-2L));
  parameterMap.put(""String_Node_Str"",new String[]{""String_Node_Str""});
  assertEquals(""String_Node_Str"",Long.valueOf(-2L),HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-2L));
  parameterMap.put(""String_Node_Str"",new String[]{""String_Node_Str""});
  assertEquals(""String_Node_Str"",null,HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",null));
  try {
    parameterMap.put(""String_Node_Str"",new String[]{""String_Node_Str""});
    HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-1L);
    fail(""String_Node_Str"");
  }
 catch (  ForwardedToErrorPage e) {
  }
  try {
    parameterMap.put(""String_Node_Str"",new String[]{""String_Node_Str""});
    Long noLong=HTMLUtils.parseOptionalLong(pageContext,""String_Node_Str"",-1L);
  }
 catch (  ForwardedToErrorPage e) {
    fail(""String_Node_Str"");
  }
}",0.975024485798237
89633,"/** 
 * Test header. 
 */
public void testGenerateHeader() throws Exception {
  JspWriterMockup out=new JspWriterMockup();
  ServletRequest confRequest=makeHttpServletRequest(""String_Node_Str"");
  PageContext pageContext=new WebinterfaceTestCase.TestPageContext(confRequest,out,new Locale(""String_Node_Str""));
  HTMLUtils.generateHeader(""String_Node_Str"",pageContext);
  String result=out.sw.toString();
  StringAsserts.assertStringContains(""String_Node_Str"",""String_Node_Str"",result);
  for (  SiteSection ss : SiteSection.getSections()) {
    JspWriterMockup jwm=new JspWriterMockup();
    ss.generateNavigationTree(jwm,""String_Node_Str"",new Locale(""String_Node_Str""));
    String tree=jwm.sw.toString();
    StringAsserts.assertStringContains(""String_Node_Str"",tree,result);
  }
  int i=0;
  StringTree<String> webinterfaceSettings=Settings.getTree(CommonSettings.WEBINTERFACE_SETTINGS);
  for (  StringTree<String> language : webinterfaceSettings.getSubTrees(CommonSettings.WEBINTERFACE_LANGUAGE)) {
    String locale=language.getValue(CommonSettings.WEBINTERFACE_LANGUAGE_LOCALE);
    String name=language.getValue(CommonSettings.WEBINTERFACE_LANGUAGE_NAME);
    StringAsserts.assertStringContains(""String_Node_Str"",""String_Node_Str"" + locale,result);
    StringAsserts.assertStringContains(""String_Node_Str"",""String_Node_Str"" + HTMLUtils.encodeAndEscapeHTML(name),result);
  }
  out=new JspWriterMockup();
  confRequest=makeHttpServletRequest(""String_Node_Str"");
  pageContext=new WebinterfaceTestCase.TestPageContext(confRequest,out,new Locale(""String_Node_Str""));
  HTMLUtils.generateHeader(pageContext);
  result=out.sw.toString();
  StringAsserts.assertStringContains(""String_Node_Str"",""String_Node_Str"",result);
  out=new JspWriterMockup();
  pageContext=new WebinterfaceTestCase.TestPageContext(confRequest,out,new Locale(""String_Node_Str""));
  HTMLUtils.generateHeader(pageContext);
  result=out.sw.toString();
  StringAsserts.assertStringContains(""String_Node_Str"",""String_Node_Str"",result);
}","/** 
 * Test header. 
 */
public void testGenerateHeader() throws Exception {
  JspWriterMockup out=new JspWriterMockup();
  ServletRequest confRequest=makeHttpServletRequest(""String_Node_Str"");
  PageContext pageContext=new WebinterfaceTestCase.TestPageContext(confRequest,out,new Locale(""String_Node_Str""));
  HTMLUtils.generateHeader(""String_Node_Str"",pageContext);
  String result=out.sw.toString();
  StringAsserts.assertStringContains(""String_Node_Str"",""String_Node_Str"",result);
  for (  SiteSection ss : SiteSection.getSections()) {
    JspWriterMockup jwm=new JspWriterMockup();
    ss.generateNavigationTree(jwm,""String_Node_Str"",new Locale(""String_Node_Str""));
    String tree=jwm.sw.toString();
    StringAsserts.assertStringContains(""String_Node_Str"",tree,result);
  }
  StringTree<String> webinterfaceSettings=Settings.getTree(CommonSettings.WEBINTERFACE_SETTINGS);
  for (  StringTree<String> language : webinterfaceSettings.getSubTrees(CommonSettings.WEBINTERFACE_LANGUAGE)) {
    String locale=language.getValue(CommonSettings.WEBINTERFACE_LANGUAGE_LOCALE);
    String name=language.getValue(CommonSettings.WEBINTERFACE_LANGUAGE_NAME);
    StringAsserts.assertStringContains(""String_Node_Str"",""String_Node_Str"" + locale,result);
    StringAsserts.assertStringContains(""String_Node_Str"",""String_Node_Str"" + HTMLUtils.encodeAndEscapeHTML(name),result);
  }
  out=new JspWriterMockup();
  confRequest=makeHttpServletRequest(""String_Node_Str"");
  pageContext=new WebinterfaceTestCase.TestPageContext(confRequest,out,new Locale(""String_Node_Str""));
  HTMLUtils.generateHeader(pageContext);
  result=out.sw.toString();
  StringAsserts.assertStringContains(""String_Node_Str"",""String_Node_Str"",result);
  out=new JspWriterMockup();
  pageContext=new WebinterfaceTestCase.TestPageContext(confRequest,out,new Locale(""String_Node_Str""));
  HTMLUtils.generateHeader(pageContext);
  result=out.sw.toString();
  StringAsserts.assertStringContains(""String_Node_Str"",""String_Node_Str"",result);
}",0.9972520609542844
89634,"public static void assertEquals(Set<Object> expectedSet,Set<Object> resultSet){
  Set<Object> disjunctInExpectedSet=new HashSet<Object>(expectedSet);
  disjunctInExpectedSet.removeAll(resultSet);
  Set<Object> disjunctInResultSet=new HashSet<Object>(resultSet);
  disjunctInResultSet.removeAll(expectedSet);
  if (!disjunctInExpectedSet.isEmpty() || !disjunctInResultSet.isEmpty()) {
    fail(""String_Node_Str"" + disjunctInExpectedSet.size() + ""String_Node_Str""+ ""String_Node_Str""+ disjunctInExpectedSet+ ""String_Node_Str""+ disjunctInResultSet.size+ ""String_Node_Str""+ ""String_Node_Str""+ disjunctInResultSet);
  }
}","public static void assertEquals(Set<Object> expectedSet,Set<Object> resultSet){
  Set<Object> disjunctInExpectedSet=new HashSet<Object>(expectedSet);
  disjunctInExpectedSet.removeAll(resultSet);
  Set<Object> disjunctInResultSet=new HashSet<Object>(resultSet);
  disjunctInResultSet.removeAll(expectedSet);
  if (!disjunctInExpectedSet.isEmpty() || !disjunctInResultSet.isEmpty()) {
    fail(""String_Node_Str"" + disjunctInExpectedSet.size() + ""String_Node_Str""+ ""String_Node_Str""+ disjunctInExpectedSet+ ""String_Node_Str""+ disjunctInResultSet.size()+ ""String_Node_Str""+ ""String_Node_Str""+ disjunctInResultSet);
  }
}",0.9983766233766234
89635,"@Override public List<HarvestRunInfo> getHarvestRunInfo(long harvestID){
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    ResultSet res=null;
    Map<Integer,HarvestRunInfo> runInfos=new HashMap<Integer,HarvestRunInfo>();
    List<HarvestRunInfo> infoList=new ArrayList<HarvestRunInfo>();
synchronized (this) {
      s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
      s.setLong(1,harvestID);
      res=s.executeQuery();
      while (res.next()) {
        int runNr=res.getInt(2);
        HarvestRunInfo info=runInfos.get(runNr);
        if (info == null) {
          String name=res.getString(1);
          info=new HarvestRunInfo(harvestID,name,runNr);
          runInfos.put(runNr,info);
          infoList.add(info);
        }
        JobStatus status=JobStatus.fromOrdinal(res.getInt(3));
        if (status != JobStatus.NEW && status != JobStatus.SUBMITTED && status != JobStatus.RESUBMITTED) {
          Date startDate=DBUtils.getDateMaybeNull(res,4);
          if (info.getStartDate() == null || (startDate != null && startDate.before(info.getStartDate()))) {
            info.setStartDate(startDate);
          }
        }
        if (status == JobStatus.DONE || status == JobStatus.FAILED) {
          Date endDate=DBUtils.getDateMaybeNull(res,5);
          if (info.getEndDate() == null || (endDate != null && endDate.after(info.getEndDate()))) {
            info.setEndDate(endDate);
          }
        }
        int count=res.getInt(6);
        info.setStatusCount(status,count);
      }
      s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
      s.setLong(1,harvestID);
      res=s.executeQuery();
    }
    while (res.next()) {
      final int harvestNum=res.getInt(1);
      HarvestRunInfo info=runInfos.get(harvestNum);
      info.setBytesHarvested(res.getLong(2));
      info.setDocsHarvested(res.getLong(3));
    }
    for (    HarvestRunInfo info : infoList) {
      if (info.getJobCount(JobStatus.STARTED) != 0 || info.getJobCount(JobStatus.NEW) != 0 || info.getJobCount(JobStatus.SUBMITTED) != 0) {
        info.setEndDate(null);
      }
    }
    return infoList;
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + harvestID + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}","@Override public List<HarvestRunInfo> getHarvestRunInfo(long harvestID){
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    ResultSet res=null;
    Map<Integer,HarvestRunInfo> runInfos=new HashMap<Integer,HarvestRunInfo>();
    List<HarvestRunInfo> infoList=new ArrayList<HarvestRunInfo>();
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setLong(1,harvestID);
    res=s.executeQuery();
    while (res.next()) {
      int runNr=res.getInt(2);
      HarvestRunInfo info=runInfos.get(runNr);
      if (info == null) {
        String name=res.getString(1);
        info=new HarvestRunInfo(harvestID,name,runNr);
        runInfos.put(runNr,info);
        infoList.add(info);
      }
      JobStatus status=JobStatus.fromOrdinal(res.getInt(3));
      if (status != JobStatus.NEW && status != JobStatus.SUBMITTED && status != JobStatus.RESUBMITTED) {
        Date startDate=DBUtils.getDateMaybeNull(res,4);
        if (info.getStartDate() == null || (startDate != null && startDate.before(info.getStartDate()))) {
          info.setStartDate(startDate);
        }
      }
      if (status == JobStatus.DONE || status == JobStatus.FAILED) {
        Date endDate=DBUtils.getDateMaybeNull(res,5);
        if (info.getEndDate() == null || (endDate != null && endDate.after(info.getEndDate()))) {
          info.setEndDate(endDate);
        }
      }
      int count=res.getInt(6);
      info.setStatusCount(status,count);
    }
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setLong(1,harvestID);
    res=s.executeQuery();
    while (res.next()) {
      final int harvestNum=res.getInt(1);
      HarvestRunInfo info=runInfos.get(harvestNum);
      if (info != null) {
        info.setBytesHarvested(res.getLong(2));
        info.setDocsHarvested(res.getLong(3));
      }
 else {
        log.debug(""String_Node_Str"" + harvestNum + ""String_Node_Str""+ harvestID+ ""String_Node_Str"");
      }
    }
    for (    HarvestRunInfo info : infoList) {
      if (info.getJobCount(JobStatus.STARTED) != 0 || info.getJobCount(JobStatus.NEW) != 0 || info.getJobCount(JobStatus.SUBMITTED) != 0) {
        info.setEndDate(null);
      }
    }
    return infoList;
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + harvestID + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}",0.9415021221627606
89636,"/** 
 * Check if a harvestdefinition exists with the given id.
 * @param c An open connection to the harvestDatabase
 * @param oid A potentiel identifier for a harvestdefinition
 * @return true If a harvestdefinition exists with the given id.
 * @see HarvestDefinitionDAO#exists(Long)
 */
private boolean exists(Connection c,Long oid){
  return 1 == DBUtils.selectIntValue(c,""String_Node_Str"" + ""String_Node_Str"",oid);
}","/** 
 * Check if a harvestdefinition exists with the given id.
 * @param c An open connection to the harvestDatabase
 * @param oid A potential identifier for a harvestdefinition
 * @return true If a harvestdefinition exists with the given id.
 * @see HarvestDefinitionDAO#exists(Long)
 */
private boolean exists(Connection c,Long oid){
  return 1 == DBUtils.selectIntValue(c,""String_Node_Str"" + ""String_Node_Str"",oid);
}",0.9976190476190476
89637,"/** 
 * saves all extended Field values for a Domain in the Database.
 * @param c Connection to Database
 * @param d Domain where loaded extended Field Values will be set
 * @throws SQLException If database errors occur.
 */
private void saveExtendedFieldValues(Connection c,Domain d) throws SQLException {
  List<ExtendedFieldValue> list=d.getExtendedFieldValues();
  for (int i=0; i < list.size(); i++) {
    ExtendedFieldValue efv=list.get(i);
    efv.setInstanceID(d.getID());
    ExtendedFieldValueDBDAO dao=new ExtendedFieldValueDBDAO();
    if (efv.getExtendedFieldValueID() != null) {
      dao.update(c,efv,false);
    }
 else {
      dao.create(c,efv,false);
    }
  }
}","/** 
 * Saves all extended Field values for a Domain in the Database.
 * @param c Connection to Database
 * @param d Domain where loaded extended Field Values will be set
 * @throws SQLException If database errors occur.
 */
private void saveExtendedFieldValues(Connection c,Domain d) throws SQLException {
  List<ExtendedFieldValue> list=d.getExtendedFieldValues();
  for (int i=0; i < list.size(); i++) {
    ExtendedFieldValue efv=list.get(i);
    efv.setInstanceID(d.getID());
    ExtendedFieldValueDBDAO dao=(ExtendedFieldValueDBDAO)ExtendedFieldValueDAO.getInstance();
    if (efv.getExtendedFieldValueID() != null) {
      dao.update(c,efv,false);
    }
 else {
      dao.create(c,efv,false);
    }
  }
}",0.9705248023005032
89638,"/** 
 * Delete all entries from the config_seedlists table that refer to the given configuration and insert the current ones.
 * @param d A domain to operate on
 * @param dc Configuration to update.
 * @throws SQLException If any database problems occur during the update process.
 */
private void updateConfigSeedlistsEntries(Connection c,Domain d,DomainConfiguration dc) throws SQLException {
  deleteConfigFromTable(c,dc.getID(),""String_Node_Str"");
  createConfigSeedlistsEntries(c,d,dc);
}","/** 
 * Delete all entries from the config_seedlists table that refer to the given configuration and insert the current ones.
 * @param c An open connection to the harvestDatabase. 
 * @param d A domain to operate on
 * @param dc Configuration to update.
 * @throws SQLException If any database problems occur during the update process.
 */
private void updateConfigSeedlistsEntries(Connection c,Domain d,DomainConfiguration dc) throws SQLException {
  deleteConfigFromTable(c,dc.getID(),""String_Node_Str"");
  createConfigSeedlistsEntries(c,d,dc);
}",0.946257197696737
89639,"/** 
 * Change an existing domain in the DB.
 * @see DomainDAO#update(Domain)
 */
public synchronized void update(Domain d){
  ArgumentNotValid.checkNotNull(d,""String_Node_Str"");
  if (!exists(d.getName())) {
    throw new UnknownID(""String_Node_Str"" + d.getName() + ""String_Node_Str"");
  }
  Connection connection=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    connection.setAutoCommit(false);
    long domainID=DBUtils.selectLongValue(connection,""String_Node_Str"",d.getName());
    if (d.hasID() && d.getID() != domainID) {
      String message=""String_Node_Str"" + d + ""String_Node_Str""+ d.getID()+ ""String_Node_Str""+ domainID;
      log.warn(message);
      throw new ArgumentNotValid(message);
    }
    d.setID(domainID);
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"");
    DBUtils.setComments(s,1,d,Constants.MAX_COMMENT_SIZE);
    DBUtils.setStringMaxLength(s,2,StringUtils.conjoin(""String_Node_Str"",d.getCrawlerTraps()),Constants.MAX_CRAWLER_TRAP_SIZE,d,""String_Node_Str"");
    final long newEdition=d.getEdition() + 1;
    s.setLong(3,newEdition);
    AliasInfo aliasInfo=d.getAliasInfo();
    DBUtils.setLongMaybeNull(s,4,aliasInfo == null ? null : DBUtils.selectLongValue(connection,""String_Node_Str"",aliasInfo.getAliasOf()));
    DBUtils.setDateMaybeNull(s,5,aliasInfo == null ? null : aliasInfo.getLastChange());
    s.setLong(6,d.getID());
    s.setLong(7,d.getEdition());
    int rows=s.executeUpdate();
    if (rows == 0) {
      String message=""String_Node_Str"" + d.getEdition() + ""String_Node_Str""+ d;
      log.debug(message);
      throw new PermissionDenied(message);
    }
    s.close();
    updatePasswords(connection,d);
    updateSeedlists(connection,d);
    updateConfigurations(connection,d);
    updateOwnerInfo(connection,d);
    updateHarvestInfo(connection,d);
    saveExtendedFieldValues(connection,d);
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setLong(1,d.getID());
    s.setString(2,d.getDefaultConfiguration().getName());
    s.setLong(3,d.getID());
    s.executeUpdate();
    connection.commit();
    d.setEdition(newEdition);
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + d + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.rollbackIfNeeded(connection,""String_Node_Str"",d);
    HarvestDBConnection.release(connection);
  }
}","@Override public synchronized void update(Domain d){
  ArgumentNotValid.checkNotNull(d,""String_Node_Str"");
  if (!exists(d.getName())) {
    throw new UnknownID(""String_Node_Str"" + d.getName() + ""String_Node_Str"");
  }
  Connection connection=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    connection.setAutoCommit(false);
    long domainID=DBUtils.selectLongValue(connection,""String_Node_Str"",d.getName());
    if (d.hasID() && d.getID() != domainID) {
      String message=""String_Node_Str"" + d + ""String_Node_Str""+ d.getID()+ ""String_Node_Str""+ domainID;
      log.warn(message);
      throw new ArgumentNotValid(message);
    }
    d.setID(domainID);
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"");
    DBUtils.setComments(s,1,d,Constants.MAX_COMMENT_SIZE);
    DBUtils.setStringMaxLength(s,2,StringUtils.conjoin(""String_Node_Str"",d.getCrawlerTraps()),Constants.MAX_CRAWLER_TRAP_SIZE,d,""String_Node_Str"");
    final long newEdition=d.getEdition() + 1;
    s.setLong(3,newEdition);
    AliasInfo aliasInfo=d.getAliasInfo();
    DBUtils.setLongMaybeNull(s,4,aliasInfo == null ? null : DBUtils.selectLongValue(connection,""String_Node_Str"",aliasInfo.getAliasOf()));
    DBUtils.setDateMaybeNull(s,5,aliasInfo == null ? null : aliasInfo.getLastChange());
    s.setLong(6,d.getID());
    s.setLong(7,d.getEdition());
    int rows=s.executeUpdate();
    if (rows == 0) {
      String message=""String_Node_Str"" + d.getEdition() + ""String_Node_Str""+ d;
      log.debug(message);
      throw new PermissionDenied(message);
    }
    s.close();
    updatePasswords(connection,d);
    updateSeedlists(connection,d);
    updateConfigurations(connection,d);
    updateOwnerInfo(connection,d);
    updateHarvestInfo(connection,d);
    saveExtendedFieldValues(connection,d);
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setLong(1,d.getID());
    s.setString(2,d.getDefaultConfiguration().getName());
    s.setLong(3,d.getID());
    s.executeUpdate();
    connection.commit();
    d.setEdition(newEdition);
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + d + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.rollbackIfNeeded(connection,""String_Node_Str"",d);
    HarvestDBConnection.release(connection);
  }
}",0.9828660436137072
89640,"/** 
 * Create a new domain in the DB.
 * @see DomainDAO#create(Connection,Domain)
 */
@Override protected void create(Connection connection,Domain d){
  ArgumentNotValid.checkNotNull(d,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(d.getName(),""String_Node_Str"");
  if (exists(connection,d.getName())) {
    String msg=""String_Node_Str"" + d;
    log.debug(msg);
    throw new PermissionDenied(msg);
  }
  PreparedStatement s=null;
  log.debug(""String_Node_Str"" + d.getName());
  try {
    connection.setAutoCommit(false);
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",Statement.RETURN_GENERATED_KEYS);
    DBUtils.setName(s,1,d,Constants.MAX_NAME_SIZE);
    DBUtils.setComments(s,2,d,Constants.MAX_COMMENT_SIZE);
    DBUtils.setStringMaxLength(s,3,StringUtils.conjoin(""String_Node_Str"",d.getCrawlerTraps()),Constants.MAX_CRAWLER_TRAP_SIZE,d,""String_Node_Str"");
    long initialEdition=1;
    s.setLong(4,initialEdition);
    AliasInfo aliasInfo=d.getAliasInfo();
    DBUtils.setLongMaybeNull(s,5,aliasInfo == null ? null : DBUtils.selectLongValue(connection,""String_Node_Str"",aliasInfo.getAliasOf()));
    DBUtils.setDateMaybeNull(s,6,aliasInfo == null ? null : aliasInfo.getLastChange());
    s.executeUpdate();
    d.setID(DBUtils.getGeneratedID(s));
    s.close();
    Iterator<Password> passwords=d.getAllPasswords();
    while (passwords.hasNext()) {
      Password p=passwords.next();
      insertPassword(connection,d,p);
    }
    Iterator<SeedList> seedlists=d.getAllSeedLists();
    if (!seedlists.hasNext()) {
      String msg=""String_Node_Str"" + d;
      log.debug(msg);
      throw new ArgumentNotValid(msg);
    }
    while (seedlists.hasNext()) {
      SeedList sl=seedlists.next();
      insertSeedlist(connection,d,sl);
    }
    Iterator<DomainConfiguration> dcs=d.getAllConfigurations();
    if (!dcs.hasNext()) {
      String msg=""String_Node_Str"" + d;
      log.debug(msg);
      throw new ArgumentNotValid(msg);
    }
    while (dcs.hasNext()) {
      DomainConfiguration dc=dcs.next();
      insertConfiguration(connection,d,dc);
      createConfigSeedlistsEntries(connection,d,dc);
      createConfigPasswordsEntries(connection,d,dc);
    }
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    DBUtils.setName(s,1,d.getDefaultConfiguration(),Constants.MAX_NAME_SIZE);
    s.setLong(2,d.getID());
    s.setLong(3,d.getID());
    s.executeUpdate();
    s.close();
    for (Iterator<HarvestInfo> hi=d.getHistory().getHarvestInfo(); hi.hasNext(); ) {
      insertHarvestInfo(connection,d,hi.next());
    }
    for (    DomainOwnerInfo doi : d.getAllDomainOwnerInfo()) {
      insertOwnerInfo(connection,d,doi);
    }
    addExtendedFieldValues(d);
    saveExtendedFieldValues(connection,d);
    connection.commit();
    d.setEdition(initialEdition);
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + d + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.rollbackIfNeeded(connection,""String_Node_Str"",d);
  }
}","@Override protected void create(Connection connection,Domain d){
  ArgumentNotValid.checkNotNull(d,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(d.getName(),""String_Node_Str"");
  if (exists(connection,d.getName())) {
    String msg=""String_Node_Str"" + d;
    log.debug(msg);
    throw new PermissionDenied(msg);
  }
  PreparedStatement s=null;
  log.debug(""String_Node_Str"" + d.getName());
  try {
    connection.setAutoCommit(false);
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",Statement.RETURN_GENERATED_KEYS);
    DBUtils.setName(s,1,d,Constants.MAX_NAME_SIZE);
    DBUtils.setComments(s,2,d,Constants.MAX_COMMENT_SIZE);
    DBUtils.setStringMaxLength(s,3,StringUtils.conjoin(""String_Node_Str"",d.getCrawlerTraps()),Constants.MAX_CRAWLER_TRAP_SIZE,d,""String_Node_Str"");
    long initialEdition=1;
    s.setLong(4,initialEdition);
    AliasInfo aliasInfo=d.getAliasInfo();
    DBUtils.setLongMaybeNull(s,5,aliasInfo == null ? null : DBUtils.selectLongValue(connection,""String_Node_Str"",aliasInfo.getAliasOf()));
    DBUtils.setDateMaybeNull(s,6,aliasInfo == null ? null : aliasInfo.getLastChange());
    s.executeUpdate();
    d.setID(DBUtils.getGeneratedID(s));
    s.close();
    Iterator<Password> passwords=d.getAllPasswords();
    while (passwords.hasNext()) {
      Password p=passwords.next();
      insertPassword(connection,d,p);
    }
    Iterator<SeedList> seedlists=d.getAllSeedLists();
    if (!seedlists.hasNext()) {
      String msg=""String_Node_Str"" + d;
      log.debug(msg);
      throw new ArgumentNotValid(msg);
    }
    while (seedlists.hasNext()) {
      SeedList sl=seedlists.next();
      insertSeedlist(connection,d,sl);
    }
    Iterator<DomainConfiguration> dcs=d.getAllConfigurations();
    if (!dcs.hasNext()) {
      String msg=""String_Node_Str"" + d;
      log.debug(msg);
      throw new ArgumentNotValid(msg);
    }
    while (dcs.hasNext()) {
      DomainConfiguration dc=dcs.next();
      insertConfiguration(connection,d,dc);
      createConfigSeedlistsEntries(connection,d,dc);
      createConfigPasswordsEntries(connection,d,dc);
    }
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    DBUtils.setName(s,1,d.getDefaultConfiguration(),Constants.MAX_NAME_SIZE);
    s.setLong(2,d.getID());
    s.setLong(3,d.getID());
    s.executeUpdate();
    s.close();
    for (Iterator<HarvestInfo> hi=d.getHistory().getHarvestInfo(); hi.hasNext(); ) {
      insertHarvestInfo(connection,d,hi.next());
    }
    for (    DomainOwnerInfo doi : d.getAllDomainOwnerInfo()) {
      insertOwnerInfo(connection,d,doi);
    }
    addExtendedFieldValues(d);
    saveExtendedFieldValues(connection,d);
    connection.commit();
    d.setEdition(initialEdition);
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + d + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.rollbackIfNeeded(connection,""String_Node_Str"",d);
  }
}",0.9863357939374902
89641,"@Override protected synchronized Domain read(Connection c,String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  if (!exists(domainName)) {
    throw new UnknownID(""String_Node_Str"" + domainName + ""String_Node_Str"");
  }
  Domain result;
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,domainName);
    ResultSet res=s.executeQuery();
    if (!res.next()) {
      final String message=""String_Node_Str"" + domainName + ""String_Node_Str"";
      log.warn(message);
      throw new IOFailure(message);
    }
    int domainId=res.getInt(1);
    String comments=res.getString(2);
    String crawlertraps=res.getString(3);
    long edition=res.getLong(4);
    String defaultconfig=res.getString(5);
    String alias=res.getString(6);
    Date lastAliasUpdate=DBUtils.getDateMaybeNull(res,7);
    s.close();
    Domain d=new Domain(domainName);
    d.setComments(comments);
    boolean strictMode=false;
    d.setCrawlerTraps(Arrays.asList(crawlertraps.split(""String_Node_Str"")),strictMode);
    d.setID(domainId);
    d.setEdition(edition);
    if (alias != null) {
      d.setAliasInfo(new AliasInfo(domainName,alias,lastAliasUpdate));
    }
    readSeedlists(c,d);
    readPasswords(c,d);
    readConfigurations(c,d);
    d.setDefaultConfiguration(defaultconfig);
    readOwnerInfo(c,d);
    readHistoryInfo(c,d);
    readExtendedFieldValues(c,d);
    result=d;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + domainName + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
  return result;
}","@Override protected synchronized Domain read(Connection c,String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  if (!exists(domainName)) {
    throw new UnknownID(""String_Node_Str"" + domainName + ""String_Node_Str"");
  }
  Domain result;
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,domainName);
    ResultSet res=s.executeQuery();
    if (!res.next()) {
      final String message=""String_Node_Str"" + domainName + ""String_Node_Str"";
      log.warn(message);
      throw new IOFailure(message);
    }
    int domainId=res.getInt(1);
    String comments=res.getString(2);
    String crawlertraps=res.getString(3);
    long edition=res.getLong(4);
    String defaultconfig=res.getString(5);
    String alias=res.getString(6);
    Date lastAliasUpdate=DBUtils.getDateMaybeNull(res,7);
    s.close();
    Domain d=new Domain(domainName);
    d.setComments(comments);
    boolean strictMode=false;
    d.setCrawlerTraps(Arrays.asList(crawlertraps.split(""String_Node_Str"")),strictMode);
    d.setID(domainId);
    d.setEdition(edition);
    if (alias != null) {
      d.setAliasInfo(new AliasInfo(domainName,alias,lastAliasUpdate));
    }
    readSeedlists(c,d);
    readPasswords(c,d);
    readConfigurations(c,d);
    d.setDefaultConfiguration(defaultconfig);
    readOwnerInfo(c,d);
    readHistoryInfo(c,d);
    readExtendedFieldValues(d);
    result=d;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + domainName + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
  return result;
}",0.9994472084024324
89642,"/** 
 * reads all extended Field values from the database for a domain.
 * @param c Connection to Database
 * @param d Domain where loaded extended Field Values will be set
 * @throws SQLException If database errors occur.
 */
private void readExtendedFieldValues(Connection c,Domain d) throws SQLException {
  ExtendedFieldDBDAO dao=new ExtendedFieldDBDAO();
  List<ExtendedField> list=dao.getAll(ExtendedFieldTypes.DOMAIN);
  for (int i=0; i < list.size(); i++) {
    ExtendedField ef=list.get(i);
    ExtendedFieldValueDBDAO dao2=new ExtendedFieldValueDBDAO();
    ExtendedFieldValue efv=dao2.read(ef.getExtendedFieldID(),d.getID());
    if (efv == null) {
      efv=new ExtendedFieldValue();
      efv.setExtendedFieldID(ef.getExtendedFieldID());
      efv.setInstanceID(d.getID());
      efv.setContent(ef.getDefaultValue());
    }
    d.addExtendedFieldValue(efv);
  }
}","/** 
 * Reads all extended Field values from the database for a domain.
 * @param d Domain where loaded extended Field Values will be set
 * @throws SQLException If database errors occur.
 */
private void readExtendedFieldValues(Domain d) throws SQLException {
  ExtendedFieldDAO dao=ExtendedFieldDAO.getInstance();
  List<ExtendedField> list=dao.getAll(ExtendedFieldTypes.DOMAIN);
  for (int i=0; i < list.size(); i++) {
    ExtendedField ef=list.get(i);
    ExtendedFieldValueDAO dao2=ExtendedFieldValueDAO.getInstance();
    ExtendedFieldValue efv=dao2.read(ef.getExtendedFieldID(),d.getID());
    if (efv == null) {
      efv=new ExtendedFieldValue();
      efv.setExtendedFieldID(ef.getExtendedFieldID());
      efv.setInstanceID(d.getID());
      efv.setContent(ef.getDefaultValue());
    }
    d.addExtendedFieldValue(efv);
  }
}",0.9322429906542056
89643,"/** 
 * Adds Defaultvalues for all extended fields of this entity.
 * @param d the domain to which to add the values
 */
private void addExtendedFieldValues(Domain d){
  ExtendedFieldDAO extendedFieldDAO=ExtendedFieldDBDAO.getInstance();
  List<ExtendedField> list=extendedFieldDAO.getAll(ExtendedFieldTypes.DOMAIN);
  Iterator<ExtendedField> it=list.iterator();
  while (it.hasNext()) {
    ExtendedField ef=it.next();
    ExtendedFieldValue efv=new ExtendedFieldValue();
    efv.setContent(ef.getDefaultValue());
    efv.setExtendedFieldID(ef.getExtendedFieldID());
    d.getExtendedFieldValues().add(efv);
  }
}","/** 
 * Adds Defaultvalues for all extended fields of this entity.
 * @param d the domain to which to add the values
 */
private void addExtendedFieldValues(Domain d){
  ExtendedFieldDAO extendedFieldDAO=ExtendedFieldDAO.getInstance();
  List<ExtendedField> list=extendedFieldDAO.getAll(ExtendedFieldTypes.DOMAIN);
  Iterator<ExtendedField> it=list.iterator();
  while (it.hasNext()) {
    ExtendedField ef=it.next();
    ExtendedFieldValue efv=new ExtendedFieldValue();
    efv.setContent(ef.getDefaultValue());
    efv.setExtendedFieldID(ef.getExtendedFieldID());
    d.getExtendedFieldValues().add(efv);
  }
}",0.99836867862969
89644,"/** 
 * Get a sorted list of all seeds of a Domain in a HarvestDefinition.
 * @param harvestName of HarvestDefinition
 * @param domainName of Domain
 * @return List of all seeds of the Domain in the HarvestDefinition.
 */
@Override public List<String> getListOfSeedsOfDomainOfHarvestDefinition(String harvestName,String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(harvestName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,domainName);
    s.setString(2,harvestName);
    ResultSet res=s.executeQuery();
    List<String> seeds=new ArrayList<String>();
    while (res.next()) {
      String seedsOfDomain=res.getString(1);
      StringTokenizer st=new StringTokenizer(seedsOfDomain,""String_Node_Str"");
      while (st.hasMoreTokens()) {
        String seed=st.nextToken();
        boolean isDuplicate=false;
        for (        String entry : seeds) {
          if (entry.equals(seed)) {
            isDuplicate=true;
            break;
          }
        }
        if (!isDuplicate) {
          seeds.add(seed);
        }
      }
    }
    Collections.sort(seeds,Collator.getInstance());
    return seeds;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * Get a sorted list of all seeds of a Domain in a HarvestDefinition.
 * @param harvestName of HarvestDefinition
 * @param domainName of Domain
 * @return List of all seeds of the Domain in the HarvestDefinition.
 */
@Override public List<String> getListOfSeedsOfDomainOfHarvestDefinition(String harvestName,String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(harvestName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,domainName);
    s.setString(2,harvestName);
    ResultSet res=s.executeQuery();
    List<String> seeds=new ArrayList<String>();
    while (res.next()) {
      String seedsOfDomain=res.getString(1);
      StringTokenizer st=new StringTokenizer(seedsOfDomain,""String_Node_Str"");
      while (st.hasMoreTokens()) {
        String seed=st.nextToken();
        boolean isDuplicate=false;
        for (        String entry : seeds) {
          if (entry.equals(seed)) {
            isDuplicate=true;
            break;
          }
        }
        if (!isDuplicate) {
          seeds.add(seed);
        }
      }
    }
    Collections.sort(seeds,Collator.getInstance());
    return seeds;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}",0.9947528307097488
89645,"/** 
 * Get list of harvests previous to this one.
 * @param thisHarvest The id of this harvestdefinition
 * @return a list of IDs belonging to harvests previous to this one.
 */
private List<Long> getPreviousFullHarvests(Long thisHarvest){
  List<Long> results=new ArrayList<Long>();
  Connection c=HarvestDBConnection.get();
  for (Long originatingHarvest=thisHarvest; originatingHarvest != null; originatingHarvest=DBUtils.selectFirstLongValueIfAny(c,""String_Node_Str"" + ""String_Node_Str"",originatingHarvest)) {
    if (!originatingHarvest.equals(thisHarvest)) {
      results.add(originatingHarvest);
    }
  }
  Long firstHarvest=thisHarvest;
  if (!results.isEmpty()) {
    firstHarvest=results.get(results.size() - 1);
  }
  Long olderHarvest=DBUtils.selectFirstLongValueIfAny(c,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ HarvestStatusQuery.SORT_ORDER.DESC.name(),firstHarvest);
  for (Long originatingHarvest=olderHarvest; originatingHarvest != null; originatingHarvest=DBUtils.selectFirstLongValueIfAny(c,""String_Node_Str"" + ""String_Node_Str"",originatingHarvest)) {
    results.add(originatingHarvest);
  }
  return results;
}","/** 
 * Get list of harvests previous to this one.
 * @param thisHarvest The id of this harvestdefinition
 * @return a list of IDs belonging to harvests previous to this one.
 */
private List<Long> getPreviousFullHarvests(Long thisHarvest){
  List<Long> results=new ArrayList<Long>();
  Connection c=HarvestDBConnection.get();
  for (Long originatingHarvest=thisHarvest; originatingHarvest != null; originatingHarvest=DBUtils.selectFirstLongValueIfAny(c,""String_Node_Str"" + ""String_Node_Str"",originatingHarvest)) {
    if (!originatingHarvest.equals(thisHarvest)) {
      results.add(originatingHarvest);
    }
  }
  Long firstHarvest=thisHarvest;
  if (!results.isEmpty()) {
    firstHarvest=results.get(results.size() - 1);
  }
  Long olderHarvest=DBUtils.selectFirstLongValueIfAny(c,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ HarvestStatusQuery.SORT_ORDER.DESC.name(),firstHarvest);
  for (Long originatingHarvest=olderHarvest; originatingHarvest != null; originatingHarvest=DBUtils.selectFirstLongValueIfAny(c,""String_Node_Str"" + ""String_Node_Str"",originatingHarvest)) {
    results.add(originatingHarvest);
  }
  return results;
}",0.9847512038523274
89646,"private synchronized void setJmxUsername(String jmxUsername){
  this.jmxUsername=jmxUsername;
}","/** 
 * Set the JMX-username with a new value. Null or empty username is not  allowed.
 * @param newJmxUsername New value for the JMX-username
 */
private synchronized void setJmxUsername(String newJmxUsername){
  ArgumentNotValid.checkNotNullOrEmpty(newJmxUsername,""String_Node_Str"");
  this.jmxUsername=newJmxUsername;
}",0.4460431654676259
89647,"private synchronized void setJmxPassword(String jmxPassword){
  this.jmxPassword=jmxPassword;
}","/** 
 * Set the JMX-password with a new value. Null or empty password is not  allowed.
 * @param newJmxPassword New value for the JMX-password 
 */
private synchronized void setJmxPassword(String newJmxPassword){
  ArgumentNotValid.checkNotNullOrEmpty(newJmxPassword,""String_Node_Str"");
  this.jmxPassword=newJmxPassword;
}",0.4449760765550239
89648,"private String getJmxUsername(){
  return jmxUsername;
}","/** 
 * @return the JMX-Username
 */
private String getJmxUsername(){
  return jmxUsername;
}",0.7516778523489933
89649,"/** 
 * Returns the domain from a given query. Used for constructing an error-mbean-name on connection trouble.
 * @param mBeanQuery The query to return the domain from.
 * @return the domain from a given query.
 */
private String queryToDomain(String mBeanQuery){
  return mBeanQuery.replaceAll(""String_Node_Str"",""String_Node_Str"");
}","/** 
 * Returns the domain from a given query. Used for constructing an error-mbean-name on connection trouble.
 * @param aMBeanQuery The query to return the domain from.
 * @return the domain from a given query.
 */
private String queryToDomain(String aMBeanQuery){
  return aMBeanQuery.replaceAll(""String_Node_Str"",""String_Node_Str"");
}",0.986627043090639
89650,"private String getJmxPassword(){
  return jmxUsername;
}","/** 
 * @return the JMX-password
 */
private String getJmxPassword(){
  return jmxPassword;
}",0.6711409395973155
89651,"/** 
 * Get the log record on a given index from the top as a string. This will be formatted by some formatter, depending on implementation.
 * @return A String representation of the LogRecord, or null for none.
 */
public String getRecordString();","/** 
 * Get the log record on a given index from the top as a string. This will be formatted by some formatter, depending on implementation.
 * @return A String representation of the LogRecord, or null for none.
 */
String getRecordString();",0.985685071574642
89652,"/** 
 * Update the given PartialHarvest (i.e. Selective Harvest) with a new  time for the next harvestrun.
 * @see HarvestDefinitionDAO#updateNextdate(PartialHarvest,Date)
 */
@Override public void updateNextdate(PartialHarvest ph,Date nextdate){
  ArgumentNotValid.checkNotNull(ph,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(nextdate,""String_Node_Str"");
  if (ph.getOid() == null) {
    return;
  }
  Connection connection=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"");
    DBUtils.setDateMaybeNull(s,1,ph.getNextDate());
    s.setLong(2,ph.getOid());
    s.executeUpdate();
  }
 catch (  SQLException e) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    DBUtils.rollbackIfNeeded(connection,""String_Node_Str"",ph);
    HarvestDBConnection.release(connection);
  }
}","/** 
 * Update the given PartialHarvest (i.e. Selective Harvest) with a new  time for the next harvestrun.
 * @see HarvestDefinitionDAO#updateNextdate(PartialHarvest,Date)
 */
@Override public void updateNextdate(PartialHarvest ph,Date nextdate){
  ArgumentNotValid.checkNotNull(ph,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(nextdate,""String_Node_Str"");
  if (ph.getOid() == null) {
    return;
  }
  Connection connection=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"");
    DBUtils.setDateMaybeNull(s,1,nextdate);
    s.setLong(2,ph.getOid());
    s.executeUpdate();
  }
 catch (  SQLException e) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    DBUtils.rollbackIfNeeded(connection,""String_Node_Str"",ph);
    HarvestDBConnection.release(connection);
  }
}",0.9904862579281184
89653,"/** 
 * Test the   {@link HarvestDefinitionDAO#updateNextdate(PartialHarvest,Date)} method.
 */
public void testUpdateNextDate(){
  HarvestDefinitionDAO hddao=HarvestDefinitionDAO.getInstance();
  long partialharvestId=42L;
  HarvestDefinition hd=hddao.read(partialharvestId);
  if (!(hd instanceof PartialHarvest)) {
    fail(""String_Node_Str"" + partialharvestId + ""String_Node_Str"");
  }
  PartialHarvest ph=(PartialHarvest)hd;
  Date now=new Date();
  ph.setNextDate(now);
  hddao.updateNextdate(ph,now);
  PartialHarvest ph1=(PartialHarvest)hddao.read(partialharvestId);
  assertTrue(""String_Node_Str"",ph1.getNextDate().equals(now));
}","/** 
 * Test the   {@link HarvestDefinitionDAO#updateNextdate(PartialHarvest,Date)} method.
 */
public void testUpdateNextDate(){
  HarvestDefinitionDAO hddao=HarvestDefinitionDAO.getInstance();
  long partialharvestId=42L;
  HarvestDefinition hd=hddao.read(partialharvestId);
  if (!(hd instanceof PartialHarvest)) {
    fail(""String_Node_Str"" + partialharvestId + ""String_Node_Str"");
  }
  PartialHarvest ph=(PartialHarvest)hd;
  Date now=new Date();
  hddao.updateNextdate(ph,now);
  PartialHarvest ph1=(PartialHarvest)hddao.read(partialharvestId);
  assertTrue(""String_Node_Str"",ph1.getNextDate().equals(now));
}",0.9816733067729084
89654,"/** 
 * Adds a directory with jsp files on the given basepath of the web server. Note: This must be done BEFORE starting the server. The webbase is deduced from the name of the webapp.
 * @param webapp  a directory with jsp files or a war file.
 * @throws IOFailure        if directory is not found.
 * @throws ArgumentNotValid if either argument is null or empty or ifwebbase doesn't start with '/'.
 * @throws PermissionDenied if the server is already running.
 */
private void addWebApplication(String webapp) throws IOFailure, ArgumentNotValid, PermissionDenied {
  if (!new File(webapp).exists()) {
    throw new IOFailure(""String_Node_Str"" + webapp + ""String_Node_Str"");
  }
  String webappFilename=new File(webapp).getName();
  String webbase=""String_Node_Str"" + webappFilename;
  if (webappFilename.toLowerCase().endsWith(""String_Node_Str"")) {
    webbase=""String_Node_Str"" + webappFilename.substring(0,webappFilename.length() - 4);
  }
  for (  SiteSection section : SiteSection.getSections()) {
    if (webbase.equals(""String_Node_Str"" + section.getDirname())) {
      section.initialize();
      break;
    }
  }
  WebAppContext webApplication=new WebAppContext(webapp,webbase);
  webApplication.setMaxFormContentSize(-1);
  webApplication.setTempDirectory(new File(FileUtils.getTempDir(),webbase));
  server.addHandler(webApplication);
  log.info(""String_Node_Str"" + webapp + ""String_Node_Str""+ webbase+ ""String_Node_Str"");
}","/** 
 * Adds a directory with jsp files on the given basepath of the web server. Note: This must be done BEFORE starting the server. The webbase is deduced from the name of the webapp.
 * @param webapp  a directory with jsp files or a war file.
 * @throws IOFailure        if directory is not found.
 * @throws ArgumentNotValid if either argument is null or empty or ifwebbase doesn't start with '/'.
 * @throws PermissionDenied if the server is already running.
 */
private void addWebApplication(String webapp) throws IOFailure, ArgumentNotValid, PermissionDenied {
  if (!new File(webapp).exists()) {
    throw new IOFailure(""String_Node_Str"" + webapp + ""String_Node_Str"");
  }
  String webappFilename=new File(webapp).getName();
  String webbase=""String_Node_Str"" + webappFilename;
  if (webappFilename.toLowerCase().endsWith(""String_Node_Str"")) {
    webbase=""String_Node_Str"" + webappFilename.substring(0,webappFilename.length() - 4);
  }
  for (  SiteSection section : SiteSection.getSections()) {
    if (webbase.equals(""String_Node_Str"" + section.getDirname())) {
      section.initialize();
      break;
    }
  }
  WebAppContext webApplication=new WebAppContext(webapp,webbase);
  webApplication.setMaxFormContentSize(-1);
  File tmpdir=new File(FileUtils.getTempDir(),webbase);
  if (tmpdir.exists()) {
    FileUtils.removeRecursively(tmpdir);
    log.info(""String_Node_Str"" + tmpdir.getAbsolutePath() + ""String_Node_Str"");
  }
  webApplication.setTempDirectory(tmpdir);
  server.addHandler(webApplication);
  log.info(""String_Node_Str"" + webapp + ""String_Node_Str""+ webbase+ ""String_Node_Str"");
}",0.9152987524622456
89655,"@Override public void setUp() throws Exception {
  super.setUp();
  try {
    LogManager.getLogManager().readConfiguration(new FileInputStream(TESTLOGPROP));
  }
 catch (  IOException e) {
    fail(""String_Node_Str"");
  }
  originalSettings.setUp();
  System.setProperty(WaybackSettings.WAYBACK_AGGREGATOR_TEMP_DIR,tempDirName);
  System.setProperty(WaybackSettings.WAYBACK_AGGREGATOR_OUTPUT_DIR,outputDirName);
  System.setProperty(WaybackSettings.WAYBACK_AGGREGATOR_AGGREGATION_INTERVAL,""String_Node_Str"");
  System.setProperty(WaybackSettings.WAYBACK_AGGREGATOR_MAX_INTERMEDIATE_INDEX_FILE_SIZE,""String_Node_Str"");
  System.setProperty(WaybackSettings.WAYBACK_AGGREGATOR_MAX_MAIN_INDEX_FILE_SIZE,""String_Node_Str"");
  FileUtils.removeRecursively(new File(testWorkingDirectory));
  new File(inputDirName).mkdirs();
  new File(outputDirName).mkdirs();
}","@Override public void setUp() throws Exception {
  super.setUp();
  try {
    LogManager.getLogManager().readConfiguration(new FileInputStream(TESTLOGPROP));
  }
 catch (  IOException e) {
    fail(""String_Node_Str"");
  }
  originalSettings.setUp();
  System.setProperty(WaybackSettings.WAYBACK_BATCH_OUTPUTDIR,inputDirName);
  System.setProperty(WaybackSettings.WAYBACK_AGGREGATOR_TEMP_DIR,tempDirName);
  System.setProperty(WaybackSettings.WAYBACK_AGGREGATOR_OUTPUT_DIR,outputDirName);
  System.setProperty(WaybackSettings.WAYBACK_AGGREGATOR_AGGREGATION_INTERVAL,""String_Node_Str"");
  System.setProperty(WaybackSettings.WAYBACK_AGGREGATOR_MAX_INTERMEDIATE_INDEX_FILE_SIZE,""String_Node_Str"");
  System.setProperty(WaybackSettings.WAYBACK_AGGREGATOR_MAX_MAIN_INDEX_FILE_SIZE,""String_Node_Str"");
  FileUtils.removeRecursively(new File(testWorkingDirectory));
  new File(inputDirName).mkdirs();
  new File(outputDirName).mkdirs();
}",0.9573991031390134
89656,"/** 
 * Check if jobs should be generated for any ready harvest definitions for the specified time.
 * @param timeToGenerateJobsFor Jobs will be generated which should berun at this time. Note: In a production system the provided time will normally be current time, but during testing we need to simulated other points-in-time
 */
static void generateJobs(Date timeToGenerateJobsFor){
  final Iterable<Long> readyHarvestDefinitions=haDefinitionDAO.getReadyHarvestDefinitions(timeToGenerateJobsFor);
  for (  final Long id : readyHarvestDefinitions) {
    if (harvestDefinitionsBeingScheduled.contains(id)) {
      log.debug(""String_Node_Str"" + id + ""String_Node_Str"");
      continue;
    }
    final HarvestDefinition harvestDefinition=haDefinitionDAO.read(id);
    harvestDefinitionsBeingScheduled.add(id);
    if (!harvestDefinition.runNow(timeToGenerateJobsFor)) {
      log.trace(""String_Node_Str"" + harvestDefinition.getName() + ""String_Node_Str"");
      log.trace(""String_Node_Str"" + harvestDefinition.getNumEvents());
      continue;
    }
    new Thread(""String_Node_Str"" + id){
      public void run(){
        try {
          int jobsMade=harvestDefinition.createJobs();
          log.info(""String_Node_Str"" + jobsMade + ""String_Node_Str""+ harvestDefinition.getName()+ ""String_Node_Str"");
          haDefinitionDAO.update(harvestDefinition);
        }
 catch (        Throwable e) {
          try {
            HarvestDefinition hd=haDefinitionDAO.read(harvestDefinition.getOid());
            hd.setActive(false);
            haDefinitionDAO.update(hd);
            String errMsg=""String_Node_Str"" + ""String_Node_Str"" + harvestDefinition.getName() + ""String_Node_Str""+ ""String_Node_Str"";
            log.warn(errMsg,e);
            NotificationsFactory.getInstance().errorEvent(errMsg,e);
          }
 catch (          Exception e1) {
            String errMsg=""String_Node_Str"" + ""String_Node_Str"" + harvestDefinition.getName() + ""String_Node_Str""+ ""String_Node_Str"";
            log.warn(errMsg,e);
            log.warn(""String_Node_Str"",e1);
            NotificationsFactory.getInstance().errorEvent(errMsg,e);
          }
        }
 finally {
          harvestDefinitionsBeingScheduled.remove(id);
          log.debug(""String_Node_Str"" + harvestDefinition.getName() + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ harvestDefinitionsBeingScheduled);
        }
      }
    }
.start();
  }
}","/** 
 * Check if jobs should be generated for any ready harvest definitions for the specified time.
 * @param timeToGenerateJobsFor Jobs will be generated which should berun at this time. Note: In a production system the provided time will normally be current time, but during testing we need to simulated other points-in-time
 */
static void generateJobs(Date timeToGenerateJobsFor){
  final Iterable<Long> readyHarvestDefinitions=haDefinitionDAO.getReadyHarvestDefinitions(timeToGenerateJobsFor);
  for (  final Long id : readyHarvestDefinitions) {
    if (harvestDefinitionsBeingScheduled.contains(id)) {
      String harvestName=haDefinitionDAO.getHarvestName(id);
      String errMsg=""String_Node_Str"" + id + ""String_Node_Str""+ harvestName+ ""String_Node_Str""+ ""String_Node_Str"";
      log.warn(errMsg);
      NotificationsFactory.getInstance().errorEvent(errMsg);
      continue;
    }
    final HarvestDefinition harvestDefinition=haDefinitionDAO.read(id);
    harvestDefinitionsBeingScheduled.add(id);
    if (!harvestDefinition.runNow(timeToGenerateJobsFor)) {
      log.trace(""String_Node_Str"" + harvestDefinition.getName() + ""String_Node_Str"");
      log.trace(""String_Node_Str"" + harvestDefinition.getNumEvents());
      continue;
    }
    new Thread(""String_Node_Str"" + id){
      public void run(){
        try {
          int jobsMade=harvestDefinition.createJobs();
          log.info(""String_Node_Str"" + jobsMade + ""String_Node_Str""+ harvestDefinition.getName()+ ""String_Node_Str"");
          haDefinitionDAO.update(harvestDefinition);
        }
 catch (        Throwable e) {
          try {
            HarvestDefinition hd=haDefinitionDAO.read(harvestDefinition.getOid());
            hd.setActive(false);
            haDefinitionDAO.update(hd);
            String errMsg=""String_Node_Str"" + ""String_Node_Str"" + harvestDefinition.getName() + ""String_Node_Str""+ ""String_Node_Str"";
            log.warn(errMsg,e);
            NotificationsFactory.getInstance().errorEvent(errMsg,e);
          }
 catch (          Exception e1) {
            String errMsg=""String_Node_Str"" + ""String_Node_Str"" + harvestDefinition.getName() + ""String_Node_Str""+ ""String_Node_Str"";
            log.warn(errMsg,e);
            log.warn(""String_Node_Str"",e1);
            NotificationsFactory.getInstance().errorEvent(errMsg,e);
          }
        }
 finally {
          harvestDefinitionsBeingScheduled.remove(id);
          log.debug(""String_Node_Str"" + harvestDefinition.getName() + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ harvestDefinitionsBeingScheduled);
        }
      }
    }
.start();
  }
}",0.956262425447316
89657,"/** 
 * Update the given PartialHarvest (i.e. Selective Harvest) with a new  time for the next harvestrun.
 * @See {@link HarvestDefinitionDAO#updateNextdate(PartialHarvest,Date)}
 */
@Override public void updateNextdate(PartialHarvest ph,Date nextdate){
  ArgumentNotValid.checkNotNull(ph,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(nextdate,""String_Node_Str"");
  if (ph.getOid() == null) {
    return;
  }
  Connection connection=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"");
    DBUtils.setDateMaybeNull(s,1,ph.getNextDate());
    s.setLong(2,ph.getOid());
    s.executeUpdate();
  }
 catch (  SQLException e) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    DBUtils.rollbackIfNeeded(connection,""String_Node_Str"",ph);
    HarvestDBConnection.release(connection);
  }
}","/** 
 * Update the given PartialHarvest (i.e. Selective Harvest) with a new  time for the next harvestrun.
 * @see HarvestDefinitionDAO#updateNextdate(PartialHarvest,Date)
 */
@Override public void updateNextdate(PartialHarvest ph,Date nextdate){
  ArgumentNotValid.checkNotNull(ph,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(nextdate,""String_Node_Str"");
  if (ph.getOid() == null) {
    return;
  }
  Connection connection=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"");
    DBUtils.setDateMaybeNull(s,1,ph.getNextDate());
    s.setLong(2,ph.getOid());
    s.executeUpdate();
  }
 catch (  SQLException e) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    DBUtils.rollbackIfNeeded(connection,""String_Node_Str"",ph);
    HarvestDBConnection.release(connection);
  }
}",0.9926624737945492
89658,"/** 
 * Takes a seed list and creates any necessary domains, configurations, and seedlists to enable them to be harvested with the given template and other parameters. <A href=""https://gforge.statsbiblioteket.dk/tracker/?group_id=7&atid=105&func=detail&aid=717"">Bug 717</A> addresses this issue. Current naming of the seedlists and domainconfigurations are: one of <br> harvestdefinitionname + ""_"" + templateName + ""_"" + ""UnlimitedBytes"" (if maxbytes is negative)<br> harvestdefinitionname + ""_"" + templateName + ""_"" + maxBytes + ""Bytes"" (if maxbytes is zero or postive).
 * @see EventHarvest#addConfigurations(PageContext,I18n,PartialHarvest) for details
 * @param seeds a newline-separated list of the seeds to be added
 * @param templateName the name of the template to be used
 * @param maxBytes Maximum number of bytes to harvest per domain
 * @param maxObjects Maximum number of objects to harvest per domain
 */
public void addSeeds(String seeds,String templateName,long maxBytes,int maxObjects){
  ArgumentNotValid.checkNotNullOrEmpty(seeds,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(templateName,""String_Node_Str"");
  if (!TemplateDAO.getInstance().exists(templateName)) {
    throw new UnknownID(""String_Node_Str"" + templateName);
  }
  final String maxbytesSuffix=""String_Node_Str"";
  String maxBytesS=""String_Node_Str"" + maxbytesSuffix;
  if (maxBytes >= 0) {
    maxBytesS=Long.toString(maxBytes);
    maxBytesS=maxBytesS + maxbytesSuffix;
  }
  final String maxobjectsSuffix=""String_Node_Str"";
  String maxObjectsS=""String_Node_Str"" + maxobjectsSuffix;
  if (maxObjects >= 0) {
    maxObjectsS=Long.toString(maxObjects);
    maxObjectsS=maxObjectsS + maxobjectsSuffix;
  }
  String name=harvestDefName + ""String_Node_Str"" + templateName+ ""String_Node_Str""+ maxBytesS+ ""String_Node_Str""+ maxObjectsS;
  String[] seedArray=seeds.split(""String_Node_Str"");
  Map<String,Set<String>> acceptedSeeds=new HashMap<String,Set<String>>();
  StringBuilder invalidMessage=new StringBuilder(""String_Node_Str"" + ""String_Node_Str"");
  boolean valid=true;
  for (  String seed : seedArray) {
    seed=seed.trim();
    if (seed.length() != 0) {
      if (!(seed.startsWith(""String_Node_Str"") || seed.startsWith(""String_Node_Str""))) {
        seed=""String_Node_Str"" + seed;
      }
      URL url=null;
      try {
        url=new URL(seed);
      }
 catch (      MalformedURLException e) {
        valid=false;
        invalidMessage.append(seed);
        invalidMessage.append('\n');
        continue;
      }
      String host=url.getHost();
      String domainName=DomainUtils.domainNameFromHostname(host);
      if (domainName == null) {
        valid=false;
        invalidMessage.append(seed);
        invalidMessage.append('\n');
        continue;
      }
      Set<String> seedsForDomain=acceptedSeeds.get(domainName);
      if (seedsForDomain == null) {
        seedsForDomain=new HashSet<String>();
      }
      seedsForDomain.add(seed);
      acceptedSeeds.put(domainName,seedsForDomain);
    }
  }
  if (!valid) {
    throw new ArgumentNotValid(invalidMessage.toString());
  }
  for (  Map.Entry<String,Set<String>> entry : acceptedSeeds.entrySet()) {
    String domainName=entry.getKey();
    Domain domain;
    SeedList seedlist=new SeedList(name,""String_Node_Str"");
    List<SeedList> seedListList=new ArrayList<SeedList>();
    seedListList.add(seedlist);
    if (DomainDAO.getInstance().exists(domainName)) {
      domain=DomainDAO.getInstance().read(domainName);
      if (!domain.hasSeedList(name)) {
        domain.addSeedList(seedlist);
      }
    }
 else {
      domain=Domain.getDefaultDomain(domainName);
      domain.addSeedList(seedlist);
      DomainDAO.getInstance().create(domain);
    }
    DomainConfiguration dc=null;
    if (domain.hasConfiguration(name)) {
      dc=domain.getConfiguration(name);
    }
 else {
      dc=new DomainConfiguration(name,domain,seedListList,new ArrayList<Password>());
      dc.setOrderXmlName(templateName);
      dc.setMaxBytes(maxBytes);
      dc.setMaxObjects(maxObjects);
      domain.addConfiguration(dc);
    }
    seedlist=domain.getSeedList(name);
    List<String> currentSeeds=seedlist.getSeeds();
    entry.getValue().addAll(currentSeeds);
    List<String> allSeeds=new ArrayList<String>();
    allSeeds.addAll(entry.getValue());
    domain.updateSeedList(new SeedList(name,allSeeds));
    addConfiguration(dc);
    DomainDAO.getInstance().update(domain);
  }
  HarvestDefinition thisInDAO=HarvestDefinitionDAO.getInstance().getHarvestDefinition(this.harvestDefName);
  if (thisInDAO == null) {
    HarvestDefinitionDAO.getInstance().create(this);
  }
 else {
    HarvestDefinitionDAO.getInstance().update(this);
  }
}","/** 
 * Takes a seed list and creates any necessary domains, configurations, and seedlists to enable them to be harvested with the given template and other parameters. <A href=""https://sbforge.org/jira/browse/NAS-1317"">JIRA issue NAS-1317</A> addresses this issue. Current naming of the seedlists and domainconfigurations are: one of <br> harvestdefinitionname + ""_"" + templateName + ""_"" + ""UnlimitedBytes"" (if maxbytes is negative)<br> harvestdefinitionname + ""_"" + templateName + ""_"" + maxBytes + ""Bytes"" (if maxbytes is zero or postive).
 * @see EventHarvest#addConfigurations(PageContext,I18n,String) for details
 * @param seeds a newline-separated list of the seeds to be added
 * @param templateName the name of the template to be used
 * @param maxBytes Maximum number of bytes to harvest per domain
 * @param maxObjects Maximum number of objects to harvest per domain
 */
public void addSeeds(String seeds,String templateName,long maxBytes,int maxObjects){
  ArgumentNotValid.checkNotNullOrEmpty(seeds,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(templateName,""String_Node_Str"");
  if (!TemplateDAO.getInstance().exists(templateName)) {
    throw new UnknownID(""String_Node_Str"" + templateName);
  }
  final String maxbytesSuffix=""String_Node_Str"";
  String maxBytesS=""String_Node_Str"" + maxbytesSuffix;
  if (maxBytes >= 0) {
    maxBytesS=Long.toString(maxBytes);
    maxBytesS=maxBytesS + maxbytesSuffix;
  }
  final String maxobjectsSuffix=""String_Node_Str"";
  String maxObjectsS=""String_Node_Str"" + maxobjectsSuffix;
  if (maxObjects >= 0) {
    maxObjectsS=Long.toString(maxObjects);
    maxObjectsS=maxObjectsS + maxobjectsSuffix;
  }
  String name=harvestDefName + ""String_Node_Str"" + templateName+ ""String_Node_Str""+ maxBytesS+ ""String_Node_Str""+ maxObjectsS;
  String[] seedArray=seeds.split(""String_Node_Str"");
  Map<String,Set<String>> acceptedSeeds=new HashMap<String,Set<String>>();
  StringBuilder invalidMessage=new StringBuilder(""String_Node_Str"" + ""String_Node_Str"");
  boolean valid=true;
  for (  String seed : seedArray) {
    seed=seed.trim();
    if (seed.length() != 0) {
      if (!(seed.startsWith(""String_Node_Str"") || seed.startsWith(""String_Node_Str""))) {
        seed=""String_Node_Str"" + seed;
      }
      URL url=null;
      try {
        url=new URL(seed);
      }
 catch (      MalformedURLException e) {
        valid=false;
        invalidMessage.append(seed);
        invalidMessage.append('\n');
        continue;
      }
      String host=url.getHost();
      String domainName=DomainUtils.domainNameFromHostname(host);
      if (domainName == null) {
        valid=false;
        invalidMessage.append(seed);
        invalidMessage.append('\n');
        continue;
      }
      Set<String> seedsForDomain=acceptedSeeds.get(domainName);
      if (seedsForDomain == null) {
        seedsForDomain=new HashSet<String>();
      }
      seedsForDomain.add(seed);
      acceptedSeeds.put(domainName,seedsForDomain);
    }
  }
  if (!valid) {
    throw new ArgumentNotValid(invalidMessage.toString());
  }
  for (  Map.Entry<String,Set<String>> entry : acceptedSeeds.entrySet()) {
    String domainName=entry.getKey();
    Domain domain;
    SeedList seedlist=new SeedList(name,""String_Node_Str"");
    List<SeedList> seedListList=new ArrayList<SeedList>();
    seedListList.add(seedlist);
    if (DomainDAO.getInstance().exists(domainName)) {
      domain=DomainDAO.getInstance().read(domainName);
      if (!domain.hasSeedList(name)) {
        domain.addSeedList(seedlist);
      }
    }
 else {
      domain=Domain.getDefaultDomain(domainName);
      domain.addSeedList(seedlist);
      DomainDAO.getInstance().create(domain);
    }
    DomainConfiguration dc=null;
    if (domain.hasConfiguration(name)) {
      dc=domain.getConfiguration(name);
    }
 else {
      dc=new DomainConfiguration(name,domain,seedListList,new ArrayList<Password>());
      dc.setOrderXmlName(templateName);
      dc.setMaxBytes(maxBytes);
      dc.setMaxObjects(maxObjects);
      domain.addConfiguration(dc);
    }
    seedlist=domain.getSeedList(name);
    List<String> currentSeeds=seedlist.getSeeds();
    entry.getValue().addAll(currentSeeds);
    List<String> allSeeds=new ArrayList<String>();
    allSeeds.addAll(entry.getValue());
    domain.updateSeedList(new SeedList(name,allSeeds));
    addConfiguration(dc);
    DomainDAO.getInstance().update(domain);
  }
  HarvestDefinition thisInDAO=HarvestDefinitionDAO.getInstance().getHarvestDefinition(this.harvestDefName);
  if (thisInDAO == null) {
    HarvestDefinitionDAO.getInstance().create(this);
  }
 else {
    HarvestDefinitionDAO.getInstance().update(this);
  }
}",0.9855907780979828
89659,"/** 
 * updates a extendedFieldValue by extendedField Is
 * @param aExtendedFieldId id of the extendedfield
 * @param aContent id content to set
 */
public void updateExtendedFieldValue(Long aExtendedFieldId,String aContent){
  for (int i=0; i < extendedFieldValues.size(); i++) {
    if (extendedFieldValues.get(i).getExtendedFieldID().equals(aExtendedFieldId)) {
      extendedFieldValues.get(i).setContent(aContent);
      return;
    }
  }
}","/** 
 * updates a extendedFieldValue by extendedField Id.
 * @param aExtendedFieldId id of the extendedfield
 * @param aContent id content to set
 */
public void updateExtendedFieldValue(Long aExtendedFieldId,String aContent){
  for (int i=0; i < extendedFieldValues.size(); i++) {
    if (extendedFieldValues.get(i).getExtendedFieldID().equals(aExtendedFieldId)) {
      extendedFieldValues.get(i).setContent(aContent);
      return;
    }
  }
}",0.9966329966329966
89660,"/** 
 * Gets all configurations belonging to this domain. The returned list is sorted by name according to language given in the parameter
 * @param loc contains the language sorting must adhere to
 * @return all configurations belonging to this domain sorted according tolanguage
 */
public List<DomainConfiguration> getAllConfigurationsAsSortedList(Locale loc){
  ArgumentNotValid.checkNotNull(loc,""String_Node_Str"");
  List<DomainConfiguration> resultSet=new ArrayList<DomainConfiguration>(domainConfigurations.values());
  NamedUtils.sortNamedObjectList(loc,resultSet);
  return resultSet;
}","/** 
 * Gets all configurations belonging to this domain. The returned list is sorted by name according to language given in the parameter.
 * @param loc contains the language sorting must adhere to
 * @return all configurations belonging to this domain sorted according tolanguage
 */
public List<DomainConfiguration> getAllConfigurationsAsSortedList(Locale loc){
  ArgumentNotValid.checkNotNull(loc,""String_Node_Str"");
  List<DomainConfiguration> resultSet=new ArrayList<DomainConfiguration>(domainConfigurations.values());
  NamedUtils.sortNamedObjectList(loc,resultSet);
  return resultSet;
}",0.9991603694374476
89661,"/** 
 * adds a Value to the ExtendedFieldValue List
 * @param aValue Valueobject of the extended Field
 */
public void addExtendedFieldValue(ExtendedFieldValue aValue){
  extendedFieldValues.add(aValue);
}","/** 
 * adds a Value to the ExtendedFieldValue List.
 * @param aValue Valueobject of the extended Field
 */
public void addExtendedFieldValue(ExtendedFieldValue aValue){
  extendedFieldValues.add(aValue);
}",0.9975669099756692
89662,"/** 
 * gets a extendedFieldValue by extendedField Is
 * @param aExtendedFieldId id of the extendedfield
 * @return ExtendedFieldValue Object
 */
public ExtendedFieldValue getExtendedFieldValue(Long aExtendedFieldId){
  for (int i=0; i < extendedFieldValues.size(); i++) {
    if (extendedFieldValues.get(i).getExtendedFieldID().equals(aExtendedFieldId)) {
      return extendedFieldValues.get(i);
    }
  }
  return new ExtendedFieldValue();
}","/** 
 * gets a extendedFieldValue by extendedField ID.
 * @param aExtendedFieldId id of the extendedfield
 * @return ExtendedFieldValue Object
 */
public ExtendedFieldValue getExtendedFieldValue(Long aExtendedFieldId){
  for (int i=0; i < extendedFieldValues.size(); i++) {
    if (extendedFieldValues.get(i).getExtendedFieldID().equals(aExtendedFieldId)) {
      return extendedFieldValues.get(i);
    }
  }
  return new ExtendedFieldValue();
}",0.9966254218222722
89663,"/** 
 * sets a List of extendedFieldValues
 * @param aValue List of extended Field objects
 */
public void setExtendedFieldValues(List<ExtendedFieldValue> aList){
  extendedFieldValues=aList;
}","/** 
 * sets a List of extendedFieldValues.
 * @param aList List of extended Field objects
 */
public void setExtendedFieldValues(List<ExtendedFieldValue> aList){
  extendedFieldValues=aList;
}",0.9740932642487048
89664,"/** 
 * Run a batch job to index this file, storing the result locally. If this method runs successfully, the isIndexed flag will be set to true and the originalIndexFileName field will be set to the (arbitrary) name of the file containing the results. The values are persisted to the datastore.
 * @throws IllegalState If the indexing has already been done.
 */
public void index() throws IllegalState {
  log.info(""String_Node_Str"" + toString());
  if (isIndexed) {
    throw new IllegalState(""String_Node_Str"" + filename + ""String_Node_Str"");
  }
  FileBatchJob theJob=null;
  if (filename.contains(""String_Node_Str"")) {
    theJob=new ExtractDeduplicateCDXBatchJob();
  }
 else {
    theJob=new ExtractWaybackCDXBatchJob();
  }
  theJob.processOnlyFileNamed(filename);
  PreservationArcRepositoryClient client=ArcRepositoryClientFactory.getPreservationInstance();
  BatchStatus batchStatus=client.batch(theJob,Settings.get(WaybackSettings.WAYBACK_REPLICA));
  if (!batchStatus.getFilesFailed().isEmpty() || batchStatus.getNoOfFilesProcessed() != 1 || !batchStatus.getExceptions().isEmpty()) {
    logBatchError(batchStatus);
  }
 else {
    collectResults(batchStatus);
  }
}","/** 
 * Run a batch job to index this file, storing the result locally. If this method runs successfully, the isIndexed flag will be set to true and the originalIndexFileName field will be set to the (arbitrary) name of the file containing the results. The values are persisted to the datastore.
 * @throws IllegalState If the indexing has already been done.
 */
public void index() throws IllegalState {
  log.info(""String_Node_Str"" + this.getFilename());
  if (isIndexed) {
    throw new IllegalState(""String_Node_Str"" + filename + ""String_Node_Str"");
  }
  FileBatchJob theJob=null;
  if (filename.contains(""String_Node_Str"")) {
    theJob=new ExtractDeduplicateCDXBatchJob();
  }
 else {
    theJob=new ExtractWaybackCDXBatchJob();
  }
  theJob.processOnlyFileNamed(filename);
  PreservationArcRepositoryClient client=ArcRepositoryClientFactory.getPreservationInstance();
  String replicaId=Settings.get(WaybackSettings.WAYBACK_REPLICA);
  log.info(""String_Node_Str"" + theJob.getClass().getName() + ""String_Node_Str""+ getFilename()+ ""String_Node_Str""+ replicaId);
  BatchStatus batchStatus=client.batch(theJob,replicaId);
  log.info(""String_Node_Str"" + this.getFilename() + ""String_Node_Str"");
  if (!batchStatus.getFilesFailed().isEmpty() || batchStatus.getNoOfFilesProcessed() != 1 || !batchStatus.getExceptions().isEmpty()) {
    logBatchError(batchStatus);
  }
 else {
    collectResults(batchStatus);
  }
}",0.8704703161141095
89665,"/** 
 * Collects the batch results from the BatchStatus, first to a file in temporary directory, after which they are renamed to the directory WAYBACK_BATCH_OUTPUTDIR. The status of this object is then updated to reflect that the object has been indexed.
 * @param status the status of a batch job.
 */
private void collectResults(BatchStatus status){
  String outputFilename=UUID.randomUUID().toString();
  String tempBatchOutputDir=Settings.get(WaybackSettings.WAYBACK_INDEX_TEMPDIR);
  final File outDir=new File(tempBatchOutputDir);
  FileUtils.createDir(outDir);
  File batchOutputFile=new File(outDir,outputFilename);
  status.copyResults(batchOutputFile);
  String finalBatchOutputDir=Settings.get(WaybackSettings.WAYBACK_BATCH_OUTPUTDIR);
  final File finalDirectory=new File(finalBatchOutputDir);
  FileUtils.createDir(finalDirectory);
  File finalFile=new File(finalDirectory,outputFilename);
  batchOutputFile.renameTo(finalFile);
  originalIndexFileName=outputFilename;
  isIndexed=true;
  log.info(""String_Node_Str"" + this.filename + ""String_Node_Str""+ originalIndexFileName+ ""String_Node_Str"");
  (new ArchiveFileDAO()).update(this);
}","/** 
 * Collects the batch results from the BatchStatus, first to a file in temporary directory, after which they are renamed to the directory WAYBACK_BATCH_OUTPUTDIR. The status of this object is then updated to reflect that the object has been indexed.
 * @param status the status of a batch job.
 */
private void collectResults(BatchStatus status){
  String outputFilename=UUID.randomUUID().toString();
  String tempBatchOutputDir=Settings.get(WaybackSettings.WAYBACK_INDEX_TEMPDIR);
  final File outDir=new File(tempBatchOutputDir);
  FileUtils.createDir(outDir);
  File batchOutputFile=new File(outDir,outputFilename);
  log.info(""String_Node_Str"" + this.getFilename() + ""String_Node_Str""+ batchOutputFile.getAbsolutePath()+ ""String_Node_Str"");
  status.copyResults(batchOutputFile);
  log.info(""String_Node_Str"" + this.getFilename() + ""String_Node_Str""+ batchOutputFile.getAbsolutePath()+ ""String_Node_Str"");
  String finalBatchOutputDir=Settings.get(WaybackSettings.WAYBACK_BATCH_OUTPUTDIR);
  final File finalDirectory=new File(finalBatchOutputDir);
  FileUtils.createDir(finalDirectory);
  File finalFile=new File(finalDirectory,outputFilename);
  batchOutputFile.renameTo(finalFile);
  originalIndexFileName=outputFilename;
  isIndexed=true;
  log.info(""String_Node_Str"" + this.filename + ""String_Node_Str""+ finalFile.getAbsolutePath()+ ""String_Node_Str"");
  (new ArchiveFileDAO()).update(this);
}",0.8826291079812206
89666,"/** 
 * Update an existing harvest definition with new info.
 * @param hd An updated harvest definition
 * @see HarvestDefinitionDAO#update(HarvestDefinition)
 */
public synchronized void update(HarvestDefinition hd){
  ArgumentNotValid.checkNotNull(hd,""String_Node_Str"");
  if (hd.getOid() == null || !exists(hd.getOid())) {
    final String message=""String_Node_Str"" + ""String_Node_Str"" + hd.getName() + ""String_Node_Str"";
    log.debug(message);
    throw new PermissionDenied(message);
  }
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    c.setAutoCommit(false);
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    DBUtils.setName(s,1,hd,Constants.MAX_NAME_SIZE);
    DBUtils.setComments(s,2,hd,Constants.MAX_COMMENT_SIZE);
    s.setInt(3,hd.getNumEvents());
    s.setTimestamp(4,new Timestamp(hd.getSubmissionDate().getTime()));
    s.setBoolean(5,hd.getActive());
    long nextEdition=hd.getEdition() + 1;
    s.setLong(6,nextEdition);
    s.setLong(7,hd.getOid());
    s.setLong(8,hd.getEdition());
    int rows=s.executeUpdate();
    if (rows == 0) {
      String message=""String_Node_Str"" + hd + ""String_Node_Str""+ hd.getEdition()+ ""String_Node_Str"";
      log.debug(message);
      throw new PermissionDenied(message);
    }
    s.close();
    if (hd instanceof FullHarvest) {
      FullHarvest fh=(FullHarvest)hd;
      s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
      if (fh.getPreviousHarvestDefinition() != null) {
        s.setLong(1,fh.getPreviousHarvestDefinition().getOid());
      }
 else {
        s.setNull(1,Types.BIGINT);
      }
      s.setLong(2,fh.getMaxCountObjects());
      s.setLong(3,fh.getMaxBytes());
      s.setLong(4,fh.getMaxJobRunningTime());
      s.setBoolean(5,fh.getIndexReady());
      s.setLong(6,fh.getOid());
      rows=s.executeUpdate();
      log.debug(rows + ""String_Node_Str"");
    }
 else     if (hd instanceof PartialHarvest) {
      PartialHarvest ph=(PartialHarvest)hd;
      s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
      s.setString(1,ph.getSchedule().getName());
      DBUtils.setDateMaybeNull(s,2,ph.getNextDate());
      s.setLong(3,ph.getOid());
      rows=s.executeUpdate();
      log.debug(rows + ""String_Node_Str"");
      s.close();
      createHarvestConfigsEntries(c,ph,ph.getOid());
    }
 else {
      String message=""String_Node_Str"" + hd + ""String_Node_Str""+ hd.getClass();
      log.warn(message);
      throw new ArgumentNotValid(message);
    }
    c.commit();
    hd.setEdition(nextEdition);
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + hd + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    DBUtils.rollbackIfNeeded(c,""String_Node_Str"",hd);
    HarvestDBConnection.release(c);
  }
}","/** 
 * Update an existing harvest definition with new info.
 * @param hd An updated harvest definition
 * @see HarvestDefinitionDAO#update(HarvestDefinition)
 */
public synchronized void update(HarvestDefinition hd){
  ArgumentNotValid.checkNotNull(hd,""String_Node_Str"");
  if (hd.getOid() == null || !exists(hd.getOid())) {
    final String message=""String_Node_Str"" + ""String_Node_Str"" + hd.getName() + ""String_Node_Str"";
    log.debug(message);
    throw new PermissionDenied(message);
  }
  HarvestDefinition preHD=null;
  if (hd instanceof FullHarvest) {
    preHD=((FullHarvest)hd).getPreviousHarvestDefinition();
  }
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    c.setAutoCommit(false);
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    DBUtils.setName(s,1,hd,Constants.MAX_NAME_SIZE);
    DBUtils.setComments(s,2,hd,Constants.MAX_COMMENT_SIZE);
    s.setInt(3,hd.getNumEvents());
    s.setTimestamp(4,new Timestamp(hd.getSubmissionDate().getTime()));
    s.setBoolean(5,hd.getActive());
    long nextEdition=hd.getEdition() + 1;
    s.setLong(6,nextEdition);
    s.setLong(7,hd.getOid());
    s.setLong(8,hd.getEdition());
    int rows=s.executeUpdate();
    if (rows == 0) {
      String message=""String_Node_Str"" + hd + ""String_Node_Str""+ hd.getEdition()+ ""String_Node_Str"";
      log.debug(message);
      throw new PermissionDenied(message);
    }
    s.close();
    if (hd instanceof FullHarvest) {
      FullHarvest fh=(FullHarvest)hd;
      s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
      if (preHD != null) {
        s.setLong(1,preHD.getOid());
      }
 else {
        s.setNull(1,Types.BIGINT);
      }
      s.setLong(2,fh.getMaxCountObjects());
      s.setLong(3,fh.getMaxBytes());
      s.setLong(4,fh.getMaxJobRunningTime());
      s.setBoolean(5,fh.getIndexReady());
      s.setLong(6,fh.getOid());
      rows=s.executeUpdate();
      log.debug(rows + ""String_Node_Str"");
    }
 else     if (hd instanceof PartialHarvest) {
      PartialHarvest ph=(PartialHarvest)hd;
      s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
      s.setString(1,ph.getSchedule().getName());
      DBUtils.setDateMaybeNull(s,2,ph.getNextDate());
      s.setLong(3,ph.getOid());
      rows=s.executeUpdate();
      log.debug(rows + ""String_Node_Str"");
      s.close();
      createHarvestConfigsEntries(c,ph,ph.getOid());
    }
 else {
      String message=""String_Node_Str"" + hd + ""String_Node_Str""+ hd.getClass();
      log.warn(message);
      throw new ArgumentNotValid(message);
    }
    c.commit();
    hd.setEdition(nextEdition);
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + hd + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    DBUtils.rollbackIfNeeded(c,""String_Node_Str"",hd);
    HarvestDBConnection.release(c);
  }
}",0.9682159399456955
89667,"public static void resetDAOs(){
  DomainDAOTester.resetDomainDAO();
  TemplateDAOTester.resetTemplateDAO();
  HarvestDefinitionDAOTester.resetDAO();
  ScheduleDAOTester.resetDAO();
  JobDAOTester.resetDAO();
  GlobalCrawlerTrapListDBDAO.reset();
}","public static void resetDAOs(){
  ExtendedFieldDBDAO.reset();
  DomainDAOTester.resetDomainDAO();
  TemplateDAOTester.resetTemplateDAO();
  HarvestDefinitionDAOTester.resetDAO();
  ScheduleDAOTester.resetDAO();
  JobDAOTester.resetDAO();
  GlobalCrawlerTrapListDBDAO.reset();
}",0.9427480916030534
89668,"/** 
 * Set the seedlist from a seedlist, where the individual seeds are separated by a '\n' character. Duplicate seeds are removed.
 * @param seedList List of seeds as one String
 */
public void setSeedList(String seedList){
  ArgumentNotValid.checkNotNull(seedList,""String_Node_Str"");
  seedListSet=new HashSet<String>();
  BufferedReader reader=new BufferedReader(new StringReader(seedList));
  String seed;
  try {
    while ((seed=reader.readLine()) != null) {
      seedListSet.add(seed);
    }
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
  if (log.isTraceEnabled()) {
    log.trace(""String_Node_Str"" + seedListSet.size() + ""String_Node_Str"");
  }
}","/** 
 * Set the seedlist from a seedlist, where the individual seeds are separated by a '\n' character. Duplicate seeds are removed.
 * @param seedList List of seeds as one String
 */
public void setSeedList(String seedList){
  ArgumentNotValid.checkNotNullOrEmpty(seedList,""String_Node_Str"");
  seedListSet=new HashSet<String>();
  BufferedReader reader=new BufferedReader(new StringReader(seedList));
  String seed;
  try {
    while ((seed=reader.readLine()) != null) {
      seedListSet.add(seed);
    }
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
  if (log.isTraceEnabled()) {
    log.trace(""String_Node_Str"" + seedListSet.size() + ""String_Node_Str"");
  }
}",0.9949676491732566
89669,"/** 
 * Set the name of a Named object into the given field.
 * @param s a prepared statement
 * @param fieldNum the index of the given field to be set
 * @param o the Named object
 * @throws SQLException
 * @throws PermissionDenied If length of o.getName() is larger thanConstants.MAX_NAME_SIZE
 */
public static void setName(PreparedStatement s,int fieldNum,Named o,int maxFieldSize) throws SQLException {
  ArgumentNotValid.checkNotNull(s,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(fieldNum,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(o,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(maxFieldSize,""String_Node_Str"");
  if (o.getName().length() > maxFieldSize) {
    throw new PermissionDenied(""String_Node_Str"" + o.getName().length() + ""String_Node_Str""+ maxFieldSize);
  }
  setStringMaxLength(s,fieldNum,o.getName(),maxFieldSize,o,""String_Node_Str"");
}","/** 
 * Set the name of a Named object into the given field.
 * @param s a prepared statement
 * @param fieldNum the index of the given field to be set
 * @param o the Named object
 * @param maxFieldSize max size of the name field
 * @throws SQLException  If any trouble accessing the database during the operation
 * @throws PermissionDenied If length of o.getName() is larger thanConstants.MAX_NAME_SIZE
 */
public static void setName(PreparedStatement s,int fieldNum,Named o,int maxFieldSize) throws SQLException {
  ArgumentNotValid.checkNotNull(s,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(fieldNum,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(o,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(maxFieldSize,""String_Node_Str"");
  if (o.getName().length() > maxFieldSize) {
    throw new PermissionDenied(""String_Node_Str"" + o.getName().length() + ""String_Node_Str""+ maxFieldSize);
  }
  setStringMaxLength(s,fieldNum,o.getName(),maxFieldSize,o,""String_Node_Str"");
}",0.9416135881104034
89670,"/** 
 * Set the Date into the given field of a statement.
 * @param s a prepared statement
 * @param fieldNum the index of the given field to be set
 * @param date the date (may be null)
 * @throws SQLException
 */
public static void setDateMaybeNull(PreparedStatement s,int fieldNum,Date date) throws SQLException {
  ArgumentNotValid.checkNotNull(s,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(fieldNum,""String_Node_Str"");
  if (date != null) {
    s.setTimestamp(fieldNum,new Timestamp(date.getTime()));
  }
 else {
    s.setNull(fieldNum,Types.DATE);
  }
}","/** 
 * Set the Date into the given field of a statement.
 * @param s a prepared statement
 * @param fieldNum the index of the given field to be set
 * @param date the date (may be null)
 * @throws SQLException  If any trouble accessing the database during the operation
 */
public static void setDateMaybeNull(PreparedStatement s,int fieldNum,Date date) throws SQLException {
  ArgumentNotValid.checkNotNull(s,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(fieldNum,""String_Node_Str"");
  if (date != null) {
    s.setTimestamp(fieldNum,new Timestamp(date.getTime()));
  }
 else {
    s.setNull(fieldNum,Types.DATE);
  }
}",0.9500831946755408
89671,"/** 
 * Set the comments of a Named object into the given field of statement.
 * @param s a prepared statement
 * @param fieldNum the index of the given field to be set
 * @param o the Named object
 * @param maxFieldSize max size of the comments field
 * @throws SQLException
 * @throws PermissionDenied If length of o.getComments() is larger thanConstants.MAX_COMMENT_SIZE
 */
public static void setComments(PreparedStatement s,int fieldNum,Named o,int maxFieldSize) throws SQLException {
  ArgumentNotValid.checkNotNull(s,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(fieldNum,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(o,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(maxFieldSize,""String_Node_Str"");
  if (o.getComments().length() > maxFieldSize) {
    throw new PermissionDenied(""String_Node_Str"" + o.getComments().length() + ""String_Node_Str""+ maxFieldSize);
  }
  setStringMaxLength(s,fieldNum,o.getComments(),maxFieldSize,o,""String_Node_Str"");
}","/** 
 * Set the comments of a Named object into the given field of statement.
 * @param s a prepared statement
 * @param fieldNum the index of the given field to be set
 * @param o the Named object
 * @param maxFieldSize max size of the comments field
 * @throws SQLException If any trouble accessing the database during the operation
 * @throws PermissionDenied If length of o.getComments() is larger thanConstants.MAX_COMMENT_SIZE
 */
public static void setComments(PreparedStatement s,int fieldNum,Named o,int maxFieldSize) throws SQLException {
  ArgumentNotValid.checkNotNull(s,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(fieldNum,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(o,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(maxFieldSize,""String_Node_Str"");
  if (o.getComments().length() > maxFieldSize) {
    throw new PermissionDenied(""String_Node_Str"" + o.getComments().length() + ""String_Node_Str""+ maxFieldSize);
  }
  setStringMaxLength(s,fieldNum,o.getComments(),maxFieldSize,o,""String_Node_Str"");
}",0.9708065314200892
89672,"/** 
 * Returns the version of a table according to schemaversions, or 0 for the initial, unnumbered version. NB: the provided connection is not closed
 * @param connection connection to the database.
 * @param tablename The name of a table in the database.
 * @return Version of the given table.
 * @throws IOFailure if DB table schemaversions does not exist
 */
public static int getTableVersion(Connection connection,String tablename) throws IOFailure {
  ArgumentNotValid.checkNotNull(connection,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(tablename,""String_Node_Str"");
  PreparedStatement s=null;
  int version=0;
  try {
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"");
    s.setString(1,tablename);
    ResultSet res=s.executeQuery();
    if (!res.next()) {
      log.warn(""String_Node_Str"" + tablename + ""String_Node_Str"");
    }
 else {
      version=res.getInt(1);
      if (res.wasNull()) {
        log.warn(""String_Node_Str"" + tablename + ""String_Node_Str"");
      }
    }
    return version;
  }
 catch (  SQLException e) {
    String msg=""String_Node_Str"" + tablename + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(msg,e);
    throw new IOFailure(msg,e);
  }
}","/** 
 * Returns the version of a table according to schemaversions, or 0 for the initial, unnumbered version. NB: the provided connection is not closed
 * @param connection connection to the database.
 * @param tablename The name of a table in the database.
 * @return Version of the given table.
 * @throws IOFailure if DB table schemaversions does not exist
 */
public static int getTableVersion(Connection connection,String tablename) throws IOFailure {
  ArgumentNotValid.checkNotNull(connection,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(tablename,""String_Node_Str"");
  PreparedStatement s=null;
  int version=0;
  try {
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"");
    s.setString(1,tablename);
    ResultSet res=s.executeQuery();
    if (!res.next()) {
      log.warn(""String_Node_Str"" + tablename + ""String_Node_Str""+ ""String_Node_Str"");
    }
 else {
      version=res.getInt(1);
      if (res.wasNull()) {
        log.warn(""String_Node_Str"" + tablename + ""String_Node_Str"");
      }
    }
    return version;
  }
 catch (  SQLException e) {
    String msg=""String_Node_Str"" + tablename + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(msg,e);
    throw new IOFailure(msg,e);
  }
}",0.9924030387844862
89673,"/** 
 * This method makes sure the actual caching of underlying data is done using the index server. It will convert calls into an IndexRequestMessage which is sent to the server. The Set&lt;Long&gt; of found jobs, and  the side effect of caching the index, is done using this communication  with the server. The resulting files will be unzipped into the cache dir. This method should not be called directly! Instead call cache() or getIndex().
 * @param jobSet The set of job IDs.
 * @return The set of found job IDs.
 * @throws ArgumentNotValid on null argument; or on wrong parameters inreplied message.
 * @throws IOFailure on trouble in communication or invalid reply types.
 * @throws IllegalState if message is not OK.
 * @see dk.netarkivet.archive.indexserver.FileBasedCache#cache
 * @see dk.netarkivet.archive.indexserver.FileBasedCache#getIndex
 */
protected Set<Long> cacheData(Set<Long> jobSet) throws IOFailure, IllegalState, ArgumentNotValid {
  ArgumentNotValid.checkNotNull(jobSet,""String_Node_Str"");
  log.info(""String_Node_Str"" + this.requestType + ""String_Node_Str""+ StringUtils.conjoin(""String_Node_Str"",jobSet)+ ""String_Node_Str"");
  IndexRequestMessage irMsg=new IndexRequestMessage(requestType,jobSet);
  log.debug(""String_Node_Str"" + getIndexTimeout() + ""String_Node_Str"");
  NetarkivetMessage msg=getSynchronizer().sendAndWaitForOneReply(irMsg,getIndexTimeout());
  checkMessageValid(jobSet,msg);
  IndexRequestMessage reply=(IndexRequestMessage)msg;
  Set<Long> foundJobs=reply.getFoundJobs();
  if (jobSet.equals(foundJobs)) {
    log.debug(""String_Node_Str"" + this.requestType + ""String_Node_Str""+ StringUtils.conjoin(""String_Node_Str"",jobSet)+ ""String_Node_Str"");
    try {
      if (reply.isIndexIsStoredInDirectory()) {
        gunzipToDir(reply.getResultFiles(),getCacheFile(jobSet));
      }
 else {
        unzipAndDeleteRemoteFile(reply.getResultFile(),getCacheFile(jobSet));
      }
    }
 catch (    IOFailure e) {
      log.warn(""String_Node_Str"",e);
      return new HashSet<Long>();
    }
  }
  return foundJobs;
}","/** 
 * This method makes sure the actual caching of underlying data is done using the index server. It will convert calls into an IndexRequestMessage which is sent to the server. The Set&lt;Long&gt; of found jobs, and  the side effect of caching the index, is done using this communication  with the server. The resulting files will be unzipped into the cache dir. This method should not be called directly! Instead call cache() or getIndex().
 * @param jobSet The set of job IDs.
 * @return The set of found job IDs.
 * @throws ArgumentNotValid on null argument; or on wrong parameters inreplied message.
 * @throws IOFailure on trouble in communication or invalid reply types.
 * @throws IllegalState if message is not OK.
 * @see dk.netarkivet.archive.indexserver.FileBasedCache#cache
 * @see dk.netarkivet.archive.indexserver.FileBasedCache#getIndex
 */
protected Set<Long> cacheData(Set<Long> jobSet) throws IOFailure, IllegalState, ArgumentNotValid {
  ArgumentNotValid.checkNotNull(jobSet,""String_Node_Str"");
  log.info(""String_Node_Str"" + this.requestType + ""String_Node_Str""+ StringUtils.conjoin(""String_Node_Str"",jobSet)+ ""String_Node_Str"");
  IndexRequestMessage irMsg=new IndexRequestMessage(requestType,jobSet);
  log.debug(""String_Node_Str"" + TimeUtils.readableTimeInterval(getIndexTimeout()) + ""String_Node_Str"");
  NetarkivetMessage msg=getSynchronizer().sendAndWaitForOneReply(irMsg,getIndexTimeout());
  checkMessageValid(jobSet,msg);
  IndexRequestMessage reply=(IndexRequestMessage)msg;
  Set<Long> foundJobs=reply.getFoundJobs();
  if (jobSet.equals(foundJobs)) {
    log.debug(""String_Node_Str"" + this.requestType + ""String_Node_Str""+ StringUtils.conjoin(""String_Node_Str"",jobSet)+ ""String_Node_Str"");
    try {
      if (reply.isIndexIsStoredInDirectory()) {
        gunzipToDir(reply.getResultFiles(),getCacheFile(jobSet));
      }
 else {
        unzipAndDeleteRemoteFile(reply.getResultFile(),getCacheFile(jobSet));
      }
    }
 catch (    IOFailure e) {
      log.warn(""String_Node_Str"",e);
      return new HashSet<Long>();
    }
  }
  return foundJobs;
}",0.9922705314009662
89674,"/** 
 * Get a list of all domains in snapshot harvest order. Warning: This will build a string list of the domains, which will be rather long in a full system.
 * @see DomainDAO#getAllDomainsInSnapshotHarvestOrder()
 */
public Iterator<Domain> getAllDomainsInSnapshotHarvestOrder(){
  Connection c=HarvestDBConnection.get();
  try {
    List<String> domainNames=DBUtils.selectStringList(c,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    List<Domain> orderedDomains=new LinkedList<Domain>();
    for (    String domainName : domainNames) {
      orderedDomains.add(read(c,domainName));
    }
    return orderedDomains.iterator();
  }
  finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * Get a list of all domains in snapshot harvest order. Warning: This will build a string list of the domains, which will be rather long in a full system.
 * @see DomainDAO#getAllDomainsInSnapshotHarvestOrder()
 */
public Iterator<Domain> getAllDomainsInSnapshotHarvestOrder(){
  Connection c=HarvestDBConnection.get();
  try {
    List<String> domainNames=DBUtils.selectStringList(c,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    return new FilterIterator<String,Domain>(domainNames.iterator()){
      public Domain filter(      String s){
        return read(s);
      }
    }
;
  }
  finally {
    HarvestDBConnection.release(c);
  }
}",0.8163522012578617
89675,"/** 
 * Read a domain from the persistent storage.
 * @param connection a connection to the harvest definition database.
 * @param domainName the name of the domain to retrieve
 * @return the retrieved Domain
 */
protected abstract Domain read(Connection c,String domainName);","/** 
 * Read a domain from the persistent storage.
 * @param connection a connection to the harvest definition database.
 * @param domainName the name of the domain to retrieve
 * @return the retrieved Domain
 */
protected abstract Domain read(Connection connection,String domainName);",0.983957219251337
89676,"/** 
 * @see DomainDAO#getAliases(String)
 */
public List<AliasInfo> getAliases(String domain){
  ArgumentNotValid.checkNotNullOrEmpty(domain,""String_Node_Str"");
  List<AliasInfo> resultSet=new ArrayList<AliasInfo>();
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  if (!exists(c,domain)) {
    log.debug(""String_Node_Str"" + domain + ""String_Node_Str"");
    return resultSet;
  }
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,domain);
    ResultSet res=s.executeQuery();
    while (res.next()) {
      AliasInfo ai=new AliasInfo(res.getString(1),domain,DBUtils.getDateMaybeNull(res,2));
      resultSet.add(ai);
    }
    return resultSet;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * @see DomainDAO#getAliases(String)
 */
@Override public List<AliasInfo> getAliases(String domain){
  ArgumentNotValid.checkNotNullOrEmpty(domain,""String_Node_Str"");
  List<AliasInfo> resultSet=new ArrayList<AliasInfo>();
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  if (!exists(c,domain)) {
    log.debug(""String_Node_Str"" + domain + ""String_Node_Str"");
    return resultSet;
  }
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,domain);
    ResultSet res=s.executeQuery();
    while (res.next()) {
      AliasInfo ai=new AliasInfo(res.getString(1),domain,DBUtils.getDateMaybeNull(res,2));
      resultSet.add(ai);
    }
    return resultSet;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}",0.9949290060851927
89677,"/** 
 * @see DomainDAO#getDomainHarvestInfo(String)
 */
public List<DomainHarvestInfo> getDomainHarvestInfo(String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  final ArrayList<DomainHarvestInfo> domainHarvestInfos=new ArrayList<DomainHarvestInfo>();
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,domainName);
    ResultSet res=s.executeQuery();
    while (res.next()) {
      final int jobID=res.getInt(1);
      final String harvestName=res.getString(2);
      final int harvestID=res.getInt(3);
      final int harvestNum=res.getInt(4);
      final String configName=res.getString(5);
      final Date startDate=DBUtils.getDateMaybeNull(res,6);
      final Date endDate=DBUtils.getDateMaybeNull(res,7);
      final long objectCount=res.getLong(8);
      final long byteCount=res.getLong(9);
      final StopReason reason=StopReason.getStopReason(res.getInt(10));
      domainHarvestInfos.add(new DomainHarvestInfo(domainName,jobID,harvestName,harvestID,harvestNum,configName,startDate,endDate,byteCount,objectCount,reason));
    }
    return domainHarvestInfos;
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + domainName + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * @see DomainDAO#getDomainHarvestInfo(String)
 */
@Override public List<DomainHarvestInfo> getDomainHarvestInfo(String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  final ArrayList<DomainHarvestInfo> domainHarvestInfos=new ArrayList<DomainHarvestInfo>();
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,domainName);
    ResultSet res=s.executeQuery();
    while (res.next()) {
      final int jobID=res.getInt(1);
      final String harvestName=res.getString(2);
      final int harvestID=res.getInt(3);
      final int harvestNum=res.getInt(4);
      final String configName=res.getString(5);
      final Date startDate=DBUtils.getDateMaybeNull(res,6);
      final Date endDate=DBUtils.getDateMaybeNull(res,7);
      final long objectCount=res.getLong(8);
      final long byteCount=res.getLong(9);
      final StopReason reason=StopReason.getStopReason(res.getInt(10));
      domainHarvestInfos.add(new DomainHarvestInfo(domainName,jobID,harvestName,harvestID,harvestNum,configName,startDate,endDate,byteCount,objectCount,reason));
    }
    return domainHarvestInfos;
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + domainName + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}",0.997263273125342
89678,"/** 
 * Get a list of all domains in snapshot harvest order. Warning: This will build a string list of the domains, which will be rather long in a full system.
 * @see DomainDAO#getAllDomainsInSnapshotHarvestOrder()
 */
public Iterator<Domain> getAllDomainsInSnapshotHarvestOrder(){
  Connection c=HarvestDBConnection.get();
  try {
    List<String> domainNames=DBUtils.selectStringList(c,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    return new FilterIterator<String,Domain>(domainNames.iterator()){
      public Domain filter(      String s){
        return read(s);
      }
    }
;
  }
  finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * Get a list of all domains in snapshot harvest order. Warning: This will build a string list of the domains, which will be rather long in a full system.
 * @see DomainDAO#getAllDomainsInSnapshotHarvestOrder()
 */
@Override public Iterator<Domain> getAllDomainsInSnapshotHarvestOrder(){
  Connection c=HarvestDBConnection.get();
  try {
    List<String> domainNames=DBUtils.selectStringList(c,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    return new FilterIterator<String,Domain>(domainNames.iterator()){
      public Domain filter(      String s){
        return read(s);
      }
    }
;
  }
  finally {
    HarvestDBConnection.release(c);
  }
}",0.993581514762516
89679,"/** 
 * Create a new domain in the DB.
 * @see DomainDAO#create(Connection,Domain)
 */
protected void create(Connection connection,Domain d){
  ArgumentNotValid.checkNotNull(d,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(d.getName(),""String_Node_Str"");
  if (exists(connection,d.getName())) {
    String msg=""String_Node_Str"" + d;
    log.debug(msg);
    throw new PermissionDenied(msg);
  }
  PreparedStatement s=null;
  log.debug(""String_Node_Str"" + d.getName());
  try {
    connection.setAutoCommit(false);
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",Statement.RETURN_GENERATED_KEYS);
    DBUtils.setName(s,1,d,Constants.MAX_NAME_SIZE);
    DBUtils.setComments(s,2,d,Constants.MAX_COMMENT_SIZE);
    DBUtils.setStringMaxLength(s,3,StringUtils.conjoin(""String_Node_Str"",d.getCrawlerTraps()),Constants.MAX_CRAWLER_TRAP_SIZE,d,""String_Node_Str"");
    long initialEdition=1;
    s.setLong(4,initialEdition);
    AliasInfo aliasInfo=d.getAliasInfo();
    DBUtils.setLongMaybeNull(s,5,aliasInfo == null ? null : DBUtils.selectLongValue(connection,""String_Node_Str"",aliasInfo.getAliasOf()));
    DBUtils.setDateMaybeNull(s,6,aliasInfo == null ? null : aliasInfo.getLastChange());
    s.executeUpdate();
    d.setID(DBUtils.getGeneratedID(s));
    s.close();
    Iterator<Password> passwords=d.getAllPasswords();
    while (passwords.hasNext()) {
      Password p=passwords.next();
      insertPassword(connection,d,p);
    }
    Iterator<SeedList> seedlists=d.getAllSeedLists();
    if (!seedlists.hasNext()) {
      String msg=""String_Node_Str"" + d;
      log.debug(msg);
      throw new ArgumentNotValid(msg);
    }
    while (seedlists.hasNext()) {
      SeedList sl=seedlists.next();
      insertSeedlist(connection,d,sl);
    }
    Iterator<DomainConfiguration> dcs=d.getAllConfigurations();
    if (!dcs.hasNext()) {
      String msg=""String_Node_Str"" + d;
      log.debug(msg);
      throw new ArgumentNotValid(msg);
    }
    while (dcs.hasNext()) {
      DomainConfiguration dc=dcs.next();
      insertConfiguration(connection,d,dc);
      createConfigSeedlistsEntries(connection,d,dc);
      createConfigPasswordsEntries(connection,d,dc);
    }
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    DBUtils.setName(s,1,d.getDefaultConfiguration(),Constants.MAX_NAME_SIZE);
    s.setLong(2,d.getID());
    s.setLong(3,d.getID());
    s.executeUpdate();
    s.close();
    for (Iterator<HarvestInfo> hi=d.getHistory().getHarvestInfo(); hi.hasNext(); ) {
      insertHarvestInfo(connection,d,hi.next());
    }
    for (    DomainOwnerInfo doi : d.getAllDomainOwnerInfo()) {
      insertOwnerInfo(connection,d,doi);
    }
    connection.commit();
    d.setEdition(initialEdition);
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + d + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.rollbackIfNeeded(connection,""String_Node_Str"",d);
  }
}","/** 
 * Create a new domain in the DB.
 * @see DomainDAO#create(Connection,Domain)
 */
@Override protected void create(Connection connection,Domain d){
  ArgumentNotValid.checkNotNull(d,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(d.getName(),""String_Node_Str"");
  if (exists(connection,d.getName())) {
    String msg=""String_Node_Str"" + d;
    log.debug(msg);
    throw new PermissionDenied(msg);
  }
  PreparedStatement s=null;
  log.debug(""String_Node_Str"" + d.getName());
  try {
    connection.setAutoCommit(false);
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",Statement.RETURN_GENERATED_KEYS);
    DBUtils.setName(s,1,d,Constants.MAX_NAME_SIZE);
    DBUtils.setComments(s,2,d,Constants.MAX_COMMENT_SIZE);
    DBUtils.setStringMaxLength(s,3,StringUtils.conjoin(""String_Node_Str"",d.getCrawlerTraps()),Constants.MAX_CRAWLER_TRAP_SIZE,d,""String_Node_Str"");
    long initialEdition=1;
    s.setLong(4,initialEdition);
    AliasInfo aliasInfo=d.getAliasInfo();
    DBUtils.setLongMaybeNull(s,5,aliasInfo == null ? null : DBUtils.selectLongValue(connection,""String_Node_Str"",aliasInfo.getAliasOf()));
    DBUtils.setDateMaybeNull(s,6,aliasInfo == null ? null : aliasInfo.getLastChange());
    s.executeUpdate();
    d.setID(DBUtils.getGeneratedID(s));
    s.close();
    Iterator<Password> passwords=d.getAllPasswords();
    while (passwords.hasNext()) {
      Password p=passwords.next();
      insertPassword(connection,d,p);
    }
    Iterator<SeedList> seedlists=d.getAllSeedLists();
    if (!seedlists.hasNext()) {
      String msg=""String_Node_Str"" + d;
      log.debug(msg);
      throw new ArgumentNotValid(msg);
    }
    while (seedlists.hasNext()) {
      SeedList sl=seedlists.next();
      insertSeedlist(connection,d,sl);
    }
    Iterator<DomainConfiguration> dcs=d.getAllConfigurations();
    if (!dcs.hasNext()) {
      String msg=""String_Node_Str"" + d;
      log.debug(msg);
      throw new ArgumentNotValid(msg);
    }
    while (dcs.hasNext()) {
      DomainConfiguration dc=dcs.next();
      insertConfiguration(connection,d,dc);
      createConfigSeedlistsEntries(connection,d,dc);
      createConfigPasswordsEntries(connection,d,dc);
    }
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    DBUtils.setName(s,1,d.getDefaultConfiguration(),Constants.MAX_NAME_SIZE);
    s.setLong(2,d.getID());
    s.setLong(3,d.getID());
    s.executeUpdate();
    s.close();
    for (Iterator<HarvestInfo> hi=d.getHistory().getHarvestInfo(); hi.hasNext(); ) {
      insertHarvestInfo(connection,d,hi.next());
    }
    for (    DomainOwnerInfo doi : d.getAllDomainOwnerInfo()) {
      insertOwnerInfo(connection,d,doi);
    }
    connection.commit();
    d.setEdition(initialEdition);
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + d + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.rollbackIfNeeded(connection,""String_Node_Str"",d);
  }
}",0.9984116899618806
89680,"/** 
 * Get the total number of domains in the database.
 * @see DomainDAO#getCountDomains()
 */
public synchronized int getCountDomains(){
  Connection c=HarvestDBConnection.get();
  try {
    return DBUtils.selectIntValue(c,""String_Node_Str"");
  }
  finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * Get the total number of domains in the database.
 * @see DomainDAO#getCountDomains()
 */
@Override public synchronized int getCountDomains(){
  Connection c=HarvestDBConnection.get();
  try {
    return DBUtils.selectIntValue(c,""String_Node_Str"");
  }
  finally {
    HarvestDBConnection.release(c);
  }
}",0.9837662337662336
89681,"/** 
 * @see DomainDAO#mayDelete(DomainConfiguration)
 */
public boolean mayDelete(DomainConfiguration config){
  ArgumentNotValid.checkNotNull(config,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  try {
    return config != config.getDomain().getDefaultConfiguration() && !DBUtils.selectAny(c,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",config.getID());
  }
  finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * @see DomainDAO#mayDelete(DomainConfiguration)
 */
@Override public boolean mayDelete(DomainConfiguration config){
  ArgumentNotValid.checkNotNull(config,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  try {
    return config != config.getDomain().getDefaultConfiguration() && !DBUtils.selectAny(c,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",config.getID());
  }
  finally {
    HarvestDBConnection.release(c);
  }
}",0.9888392857142856
89682,"/** 
 * Read a domain from the database.
 * @see DomainDAO#read(Connection,String)
 */
protected synchronized Domain read(Connection c,String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  if (!exists(domainName)) {
    throw new UnknownID(""String_Node_Str"" + domainName + ""String_Node_Str"");
  }
  Domain result;
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,domainName);
    ResultSet res=s.executeQuery();
    if (!res.next()) {
      final String message=""String_Node_Str"" + domainName + ""String_Node_Str"";
      log.warn(message);
      throw new IOFailure(message);
    }
    int domainId=res.getInt(1);
    String comments=res.getString(2);
    String crawlertraps=res.getString(3);
    long edition=res.getLong(4);
    String defaultconfig=res.getString(5);
    String alias=res.getString(6);
    Date lastAliasUpdate=DBUtils.getDateMaybeNull(res,7);
    s.close();
    Domain d=new Domain(domainName);
    d.setComments(comments);
    boolean strictMode=false;
    d.setCrawlerTraps(Arrays.asList(crawlertraps.split(""String_Node_Str"")),strictMode);
    d.setID(domainId);
    d.setEdition(edition);
    if (alias != null) {
      d.setAliasInfo(new AliasInfo(domainName,alias,lastAliasUpdate));
    }
    readSeedlists(c,d);
    readPasswords(c,d);
    readConfigurations(c,d);
    d.setDefaultConfiguration(defaultconfig);
    readOwnerInfo(c,d);
    readHistoryInfo(c,d);
    result=d;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + domainName + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
  return result;
}","/** 
 * Read a domain from the database.
 * @see DomainDAO#read(Connection,String)
 */
@Override protected synchronized Domain read(Connection c,String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  if (!exists(domainName)) {
    throw new UnknownID(""String_Node_Str"" + domainName + ""String_Node_Str"");
  }
  Domain result;
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,domainName);
    ResultSet res=s.executeQuery();
    if (!res.next()) {
      final String message=""String_Node_Str"" + domainName + ""String_Node_Str"";
      log.warn(message);
      throw new IOFailure(message);
    }
    int domainId=res.getInt(1);
    String comments=res.getString(2);
    String crawlertraps=res.getString(3);
    long edition=res.getLong(4);
    String defaultconfig=res.getString(5);
    String alias=res.getString(6);
    Date lastAliasUpdate=DBUtils.getDateMaybeNull(res,7);
    s.close();
    Domain d=new Domain(domainName);
    d.setComments(comments);
    boolean strictMode=false;
    d.setCrawlerTraps(Arrays.asList(crawlertraps.split(""String_Node_Str"")),strictMode);
    d.setID(domainId);
    d.setEdition(edition);
    if (alias != null) {
      d.setAliasInfo(new AliasInfo(domainName,alias,lastAliasUpdate));
    }
    readSeedlists(c,d);
    readPasswords(c,d);
    readConfigurations(c,d);
    d.setDefaultConfiguration(defaultconfig);
    readOwnerInfo(c,d);
    readHistoryInfo(c,d);
    result=d;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + domainName + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
  return result;
}",0.9973089343379978
89683,"/** 
 * @see DomainDAO#getDomains(String)
 */
public List<String> getDomains(String glob){
  ArgumentNotValid.checkNotNullOrEmpty(glob,""String_Node_Str"");
  String sqlGlob=DBUtils.makeSQLGlob(glob);
  Connection c=HarvestDBConnection.get();
  try {
    return DBUtils.selectStringList(c,""String_Node_Str"",sqlGlob);
  }
  finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * @see DomainDAO#getDomains(String)
 */
@Override public List<String> getDomains(String glob){
  ArgumentNotValid.checkNotNullOrEmpty(glob,""String_Node_Str"");
  String sqlGlob=DBUtils.makeSQLGlob(glob);
  Connection c=HarvestDBConnection.get();
  try {
    return DBUtils.selectStringList(c,""String_Node_Str"",sqlGlob);
  }
  finally {
    HarvestDBConnection.release(c);
  }
}",0.986737400530504
89684,"/** 
 * @see DomainDAO#getDomainJobInfo(Job,String,String)
 */
public HarvestInfo getDomainJobInfo(Job j,String domainName,String configName){
  ArgumentNotValid.checkNotNull(j,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(configName,""String_Node_Str"");
  HarvestInfo resultInfo=null;
  Connection connection=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    long domainId=DBUtils.selectLongValue(connection,""String_Node_Str"",domainName);
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setLong(1,j.getJobID());
    s.setLong(2,DBUtils.selectLongValue(connection,""String_Node_Str"" + ""String_Node_Str"",configName,domainId));
    s.setLong(3,j.getOrigHarvestDefinitionID());
    ResultSet res=s.executeQuery();
    if (res.next()) {
      StopReason reason=StopReason.getStopReason(res.getInt(1));
      long objectCount=res.getLong(2);
      long byteCount=res.getLong(3);
      Date harvestTime=res.getDate(4);
      resultInfo=new HarvestInfo(j.getOrigHarvestDefinitionID(),j.getJobID(),domainName,configName,harvestTime,byteCount,objectCount,reason);
    }
    return resultInfo;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(connection);
  }
}","/** 
 * @see DomainDAO#getDomainJobInfo(Job,String,String)
 */
@Override public HarvestInfo getDomainJobInfo(Job j,String domainName,String configName){
  ArgumentNotValid.checkNotNull(j,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(configName,""String_Node_Str"");
  HarvestInfo resultInfo=null;
  Connection connection=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    long domainId=DBUtils.selectLongValue(connection,""String_Node_Str"",domainName);
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setLong(1,j.getJobID());
    s.setLong(2,DBUtils.selectLongValue(connection,""String_Node_Str"" + ""String_Node_Str"",configName,domainId));
    s.setLong(3,j.getOrigHarvestDefinitionID());
    ResultSet res=s.executeQuery();
    if (res.next()) {
      StopReason reason=StopReason.getStopReason(res.getInt(1));
      long objectCount=res.getLong(2);
      long byteCount=res.getLong(3);
      Date harvestTime=res.getDate(4);
      resultInfo=new HarvestInfo(j.getOrigHarvestDefinitionID(),j.getJobID(),domainName,configName,harvestTime,byteCount,objectCount,reason);
    }
    return resultInfo;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(connection);
  }
}",0.9966666666666668
89685,"/** 
 * Get a list of all domains. Warning: This will build a string list of the domains, which will be rather long in a full system.
 * @see DomainDAO#getAllDomains()
 */
public synchronized Iterator<Domain> getAllDomains(){
  Connection c=HarvestDBConnection.get();
  try {
    List<String> domainNames=DBUtils.selectStringList(c,""String_Node_Str"");
    List<Domain> orderedDomains=new LinkedList<Domain>();
    for (    String name : domainNames) {
      orderedDomains.add(read(c,name));
    }
    return orderedDomains.iterator();
  }
  finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * Get a list of all domains. Warning: This will build a string list of the domains, which will be rather long in a full system.
 * @see DomainDAO#getAllDomains()
 */
@Override public synchronized Iterator<Domain> getAllDomains(){
  Connection c=HarvestDBConnection.get();
  try {
    List<String> domainNames=DBUtils.selectStringList(c,""String_Node_Str"");
    List<Domain> orderedDomains=new LinkedList<Domain>();
    for (    String name : domainNames) {
      orderedDomains.add(read(c,name));
    }
    return orderedDomains.iterator();
  }
  finally {
    HarvestDBConnection.release(c);
  }
}",0.991638795986622
89686,"/** 
 * Return all TLDs represented by the domains in the domains table. it was asked that a level X TLD belong appear in TLD list where the level is <=X for example bidule.bnf.fr belong to .bnf.fr and to .fr it appear in the level 1 list of TLD and in the level 2 list
 * @param level maximum level of TLD
 * @return a list of TLDs
 * @see DomainDAO#getTLDs()
 */
public List<TLDInfo> getTLDs(int level){
  Map<String,TLDInfo> resultMap=new HashMap<String,TLDInfo>();
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"");
    ResultSet res=s.executeQuery();
    while (res.next()) {
      String domain=res.getString(1);
      int domainTLDLevel=TLDInfo.getTLDLevel(domain);
      if (domainTLDLevel > level) {
        domainTLDLevel=level;
      }
      for (int currentLevel=1; currentLevel <= domainTLDLevel; currentLevel++) {
        String tld=TLDInfo.getMultiLevelTLD(domain,currentLevel);
        TLDInfo i=resultMap.get(tld);
        if (i == null) {
          i=new TLDInfo(tld);
          resultMap.put(tld,i);
        }
        i.addSubdomain(domain);
      }
    }
    List<TLDInfo> resultSet=new ArrayList<TLDInfo>(resultMap.values());
    Collections.sort(resultSet);
    return resultSet;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * Return all TLDs represented by the domains in the domains table. it was asked that a level X TLD belong appear in TLD list where the level is <=X for example bidule.bnf.fr belong to .bnf.fr and to .fr it appear in the level 1 list of TLD and in the level 2 list
 * @param level maximum level of TLD
 * @return a list of TLDs
 * @see DomainDAO#getTLDs(int)
 */
@Override public List<TLDInfo> getTLDs(int level){
  Map<String,TLDInfo> resultMap=new HashMap<String,TLDInfo>();
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"");
    ResultSet res=s.executeQuery();
    while (res.next()) {
      String domain=res.getString(1);
      int domainTLDLevel=TLDInfo.getTLDLevel(domain);
      if (domainTLDLevel > level) {
        domainTLDLevel=level;
      }
      for (int currentLevel=1; currentLevel <= domainTLDLevel; currentLevel++) {
        String tld=TLDInfo.getMultiLevelTLD(domain,currentLevel);
        TLDInfo i=resultMap.get(tld);
        if (i == null) {
          i=new TLDInfo(tld);
          resultMap.put(tld,i);
        }
        i.addSubdomain(domain);
      }
    }
    List<TLDInfo> resultSet=new ArrayList<TLDInfo>(resultMap.values());
    Collections.sort(resultSet);
    return resultSet;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}",0.9955947136563876
89687,"/** 
 * Read a Domain from Database, and return the domain information as a SparseDomain object. We only read information relevant for the GUI listing.
 * @param domainName a given domain
 * @return a SparseDomain.
 * @throws ArgumentNotValid if domainName is null or empty.
 * @throws UnknownID if domain does not exist
 */
public synchronized SparseDomain readSparse(String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  try {
    List<String> domainConfigurationNames=DBUtils.selectStringList(c,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",domainName);
    if (domainConfigurationNames.size() == 0) {
      throw new UnknownID(""String_Node_Str"" + domainName + ""String_Node_Str"");
    }
    return new SparseDomain(domainName,domainConfigurationNames);
  }
  finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * Read a Domain from Database, and return the domain information as a SparseDomain object. We only read information relevant for the GUI listing.
 * @param domainName a given domain
 * @return a SparseDomain.
 * @throws ArgumentNotValid if domainName is null or empty.
 * @throws UnknownID if domain does not exist
 */
@Override public synchronized SparseDomain readSparse(String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  try {
    List<String> domainConfigurationNames=DBUtils.selectStringList(c,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"",domainName);
    if (domainConfigurationNames.size() == 0) {
      throw new UnknownID(""String_Node_Str"" + domainName + ""String_Node_Str"");
    }
    return new SparseDomain(domainName,domainConfigurationNames);
  }
  finally {
    HarvestDBConnection.release(c);
  }
}",0.9946178686759956
89688,"/** 
 * @see DomainDAO#getAllAliases()
 */
public List<AliasInfo> getAllAliases(){
  List<AliasInfo> resultSet=new ArrayList<AliasInfo>();
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    ResultSet res=s.executeQuery();
    while (res.next()) {
      String domainName=res.getString(1);
      String aliasOf=res.getString(2);
      Date lastchanged=DBUtils.getDateMaybeNull(res,3);
      AliasInfo ai=new AliasInfo(domainName,aliasOf,lastchanged);
      resultSet.add(ai);
    }
    return resultSet;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * @see DomainDAO#getAllAliases()
 */
@Override public List<AliasInfo> getAllAliases(){
  List<AliasInfo> resultSet=new ArrayList<AliasInfo>();
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    ResultSet res=s.executeQuery();
    while (res.next()) {
      String domainName=res.getString(1);
      String aliasOf=res.getString(2);
      Date lastchanged=DBUtils.getDateMaybeNull(res,3);
      AliasInfo ai=new AliasInfo(domainName,aliasOf,lastchanged);
      resultSet.add(ai);
    }
    return resultSet;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}",0.994640943193998
89689,"/** 
 * Get the harvest definition that has the given name, if any.
 * @param name The name of a harvest definition.
 * @return The HarvestDefinition object with that name, or null if nonehas that name.
 */
public synchronized HarvestDefinition getHarvestDefinition(String name){
  ArgumentNotValid.checkNotNullOrEmpty(name,""String_Node_Str"");
  log.debug(""String_Node_Str"" + name + ""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"");
    s.setString(1,name);
    ResultSet res=s.executeQuery();
    if (res.next()) {
      long harvestDefinitionID=res.getLong(1);
      s.close();
      return read(c,harvestDefinitionID);
    }
    return null;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * Get the harvest definition that has the given name, if any.
 * @param name The name of a harvest definition.
 * @return The HarvestDefinition object with that name, or null if nonehas that name.
 */
@Override public synchronized HarvestDefinition getHarvestDefinition(String name){
  ArgumentNotValid.checkNotNullOrEmpty(name,""String_Node_Str"");
  log.debug(""String_Node_Str"" + name + ""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"");
    s.setString(1,name);
    ResultSet res=s.executeQuery();
    if (res.next()) {
      long harvestDefinitionID=res.getLong(1);
      s.close();
      return read(c,harvestDefinitionID);
    }
    return null;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}",0.9947807933194156
89690,"/** 
 * Gets default configurations for all domains that are not aliases. This method currently gives an iterator that reads in all domains, although only on demand, that is: when calling ""hasNext"".
 * @return Iterator containing the default DomainConfiguration for alldomains that are not aliases
 */
public synchronized Iterator<DomainConfiguration> getSnapShotConfigurations(){
  return new FilterIterator<Domain,DomainConfiguration>(DomainDAO.getInstance().getAllDomainsInSnapshotHarvestOrder()){
    public DomainConfiguration filter(    Domain domain){
      if (domain.getAliasInfo() == null || domain.getAliasInfo().isExpired()) {
        return domain.getDefaultConfiguration();
      }
 else {
        return null;
      }
    }
  }
;
}","/** 
 * Gets default configurations for all domains that are not aliases. This method currently gives an iterator that reads in all domains, although only on demand, that is: when calling ""hasNext"".
 * @return Iterator containing the default DomainConfiguration for alldomains that are not aliases
 */
@Override public synchronized Iterator<DomainConfiguration> getSnapShotConfigurations(){
  return new FilterIterator<Domain,DomainConfiguration>(DomainDAO.getInstance().getAllDomainsInSnapshotHarvestOrder()){
    public DomainConfiguration filter(    Domain domain){
      if (domain.getAliasInfo() == null || domain.getAliasInfo().isExpired()) {
        return domain.getDefaultConfiguration();
      }
 else {
        return null;
      }
    }
  }
;
}",0.9933422103861518
89691,"/** 
 * Get a list of all existing harvest definitions ordered by name.
 * @return An iterator that give the existing harvest definitions in turn
 */
public synchronized Iterator<HarvestDefinition> getAllHarvestDefinitions(){
  Connection c=HarvestDBConnection.get();
  try {
    List<Long> hds=DBUtils.selectLongList(c,""String_Node_Str"" + ""String_Node_Str"");
    if (log.isDebugEnabled()) {
      log.debug(""String_Node_Str"" + ""String_Node_Str"");
    }
    List<HarvestDefinition> orderedList=new LinkedList<HarvestDefinition>();
    for (    Long id : hds) {
      orderedList.add(read(c,id));
    }
    return orderedList.iterator();
  }
  finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * Get a list of all existing harvest definitions ordered by name.
 * @return An iterator that give the existing harvest definitions in turn
 */
@Override public synchronized Iterator<HarvestDefinition> getAllHarvestDefinitions(){
  Connection c=HarvestDBConnection.get();
  try {
    List<Long> hds=DBUtils.selectLongList(c,""String_Node_Str"" + ""String_Node_Str"");
    if (log.isDebugEnabled()) {
      log.debug(""String_Node_Str"" + ""String_Node_Str"");
    }
    List<HarvestDefinition> orderedList=new LinkedList<HarvestDefinition>();
    for (    Long id : hds) {
      orderedList.add(read(c,id));
    }
    return orderedList.iterator();
  }
  finally {
    HarvestDBConnection.release(c);
  }
}",0.9928469241773964
89692,"/** 
 * Returns a list of IDs of harvest definitions that are ready to be scheduled.
 * @param now The current date
 * @return List of ready harvest definitions.   No check is performed forwhether these are already in the middle of being scheduled.
 */
public Iterable<Long> getReadyHarvestDefinitions(Date now){
  ArgumentNotValid.checkNotNull(now,""String_Node_Str"");
  Connection connection=HarvestDBConnection.get();
  try {
    List<Long> ids=DBUtils.selectLongList(connection,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"",true);
    ids.addAll(DBUtils.selectLongList(connection,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"",true,now));
    return ids;
  }
  finally {
    HarvestDBConnection.release(connection);
  }
}","/** 
 * Returns a list of IDs of harvest definitions that are ready to be scheduled.
 * @param now The current date
 * @return List of ready harvest definitions.   No check is performed forwhether these are already in the middle of being scheduled.
 */
@Override public Iterable<Long> getReadyHarvestDefinitions(Date now){
  ArgumentNotValid.checkNotNull(now,""String_Node_Str"");
  Connection connection=HarvestDBConnection.get();
  try {
    List<Long> ids=DBUtils.selectLongList(connection,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"",true);
    ids.addAll(DBUtils.selectLongList(connection,""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"",true,now));
    return ids;
  }
  finally {
    HarvestDBConnection.release(connection);
  }
}",0.9942263279445728
89693,"/** 
 * Get a sparse version of a full harvest for GUI purposes.
 * @param harvestName Name of harvest definition.
 * @return Sparse version of full harvest or null for none.
 * @throws ArgumentNotValid on null or empty name.
 * @throws UnknownID        if no harvest has the given ID.
 * @throws IOFailure        on any other error talking to the database
 */
public SparseFullHarvest getSparseFullHarvest(String harvestName){
  ArgumentNotValid.checkNotNullOrEmpty(harvestName,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,harvestName);
    ResultSet res=s.executeQuery();
    if (res.next()) {
      return new SparseFullHarvest(res.getLong(1),harvestName,res.getString(2),res.getInt(3),res.getBoolean(4),res.getLong(5),res.getLong(6),res.getLong(7),res.getLong(8),DBUtils.getLongMaybeNull(res,9));
    }
 else {
      return null;
    }
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * Get a sparse version of a full harvest for GUI purposes.
 * @param harvestName Name of harvest definition.
 * @return Sparse version of full harvest or null for none.
 * @throws ArgumentNotValid on null or empty name.
 * @throws UnknownID        if no harvest has the given ID.
 * @throws IOFailure        on any other error talking to the database
 */
@Override public SparseFullHarvest getSparseFullHarvest(String harvestName){
  ArgumentNotValid.checkNotNullOrEmpty(harvestName,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,harvestName);
    ResultSet res=s.executeQuery();
    if (res.next()) {
      return new SparseFullHarvest(res.getLong(1),harvestName,res.getString(2),res.getInt(3),res.getBoolean(4),res.getLong(5),res.getLong(6),res.getLong(7),res.getLong(8),DBUtils.getLongMaybeNull(res,9));
    }
 else {
      return null;
    }
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}",0.9963924963924964
89694,"/** 
 * Get a sorted list of all seeds of a Domain in a HarvestDefinition.
 * @param harvestName of HarvestDefinition
 * @param domainName of Domain
 * @return List of all seeds of the Domain in the HarvestDefinition.
 */
public List<String> getListOfSeedsOfDomainOfHarvestDefinition(String harvestName,String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(harvestName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,domainName);
    s.setString(2,harvestName);
    ResultSet res=s.executeQuery();
    List<String> seeds=new ArrayList<String>();
    while (res.next()) {
      String seedsOfDomain=res.getString(1);
      StringTokenizer st=new StringTokenizer(seedsOfDomain,""String_Node_Str"");
      while (st.hasMoreTokens()) {
        String seed=st.nextToken();
        boolean bFound=false;
        for (        String entry : seeds) {
          if (entry.equals(seed)) {
            bFound=true;
            break;
          }
        }
        if (!bFound) {
          seeds.add(seed);
        }
      }
    }
    Collections.sort(seeds,Collator.getInstance());
    return seeds;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * Get a sorted list of all seeds of a Domain in a HarvestDefinition.
 * @param harvestName of HarvestDefinition
 * @param domainName of Domain
 * @return List of all seeds of the Domain in the HarvestDefinition.
 */
@Override public List<String> getListOfSeedsOfDomainOfHarvestDefinition(String harvestName,String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(harvestName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,domainName);
    s.setString(2,harvestName);
    ResultSet res=s.executeQuery();
    List<String> seeds=new ArrayList<String>();
    while (res.next()) {
      String seedsOfDomain=res.getString(1);
      StringTokenizer st=new StringTokenizer(seedsOfDomain,""String_Node_Str"");
      while (st.hasMoreTokens()) {
        String seed=st.nextToken();
        boolean bFound=false;
        for (        String entry : seeds) {
          if (entry.equals(seed)) {
            bFound=true;
            break;
          }
        }
        if (!bFound) {
          seeds.add(seed);
        }
      }
    }
    Collections.sort(seeds,Collator.getInstance());
    return seeds;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}",0.9971925884334644
89695,"/** 
 * Get a sparse version of a partial harvest for GUI purposes.
 * @param harvestName Name of harvest definition.
 * @return Sparse version of partial harvest or null for none.
 * @throws ArgumentNotValid on null or empty name.
 */
public SparsePartialHarvest getSparsePartialHarvest(String harvestName){
  ArgumentNotValid.checkNotNullOrEmpty(harvestName,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,harvestName);
    ResultSet res=s.executeQuery();
    if (res.next()) {
      return new SparsePartialHarvest(res.getLong(1),harvestName,res.getString(2),res.getInt(3),new Date(res.getTimestamp(4).getTime()),res.getBoolean(5),res.getLong(6),res.getString(7),DBUtils.getDateMaybeNull(res,8));
    }
 else {
      return null;
    }
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * Get a sparse version of a partial harvest for GUI purposes.
 * @param harvestName Name of harvest definition.
 * @return Sparse version of partial harvest or null for none.
 * @throws ArgumentNotValid on null or empty name.
 */
@Override public SparsePartialHarvest getSparsePartialHarvest(String harvestName){
  ArgumentNotValid.checkNotNullOrEmpty(harvestName,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,harvestName);
    ResultSet res=s.executeQuery();
    if (res.next()) {
      return new SparsePartialHarvest(res.getLong(1),harvestName,res.getString(2),res.getInt(3),new Date(res.getTimestamp(4).getTime()),res.getBoolean(5),res.getLong(6),res.getString(7),DBUtils.getDateMaybeNull(res,8));
    }
 else {
      return null;
    }
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}",0.9961568024596464
89696,"/** 
 * @see HarvestDefinitionDAO#getHarvestRunInfo(long)
 */
public List<HarvestRunInfo> getHarvestRunInfo(long harvestID){
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    ResultSet res=null;
    Map<Integer,HarvestRunInfo> runInfos=new HashMap<Integer,HarvestRunInfo>();
    List<HarvestRunInfo> infoList=new ArrayList<HarvestRunInfo>();
synchronized (this) {
      s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
      s.setLong(1,harvestID);
      res=s.executeQuery();
      while (res.next()) {
        int runNr=res.getInt(2);
        HarvestRunInfo info=runInfos.get(runNr);
        if (info == null) {
          String name=res.getString(1);
          info=new HarvestRunInfo(harvestID,name,runNr);
          runInfos.put(runNr,info);
          infoList.add(info);
        }
        JobStatus status=JobStatus.fromOrdinal(res.getInt(3));
        if (status != JobStatus.NEW && status != JobStatus.SUBMITTED && status != JobStatus.RESUBMITTED) {
          Date startDate=DBUtils.getDateMaybeNull(res,4);
          if (info.getStartDate() == null || (startDate != null && startDate.before(info.getStartDate()))) {
            info.setStartDate(startDate);
          }
        }
        if (status == JobStatus.DONE || status == JobStatus.FAILED) {
          Date endDate=DBUtils.getDateMaybeNull(res,5);
          if (info.getEndDate() == null || (endDate != null && endDate.after(info.getEndDate()))) {
            info.setEndDate(endDate);
          }
        }
        int count=res.getInt(6);
        info.setStatusCount(status,count);
      }
      s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
      s.setLong(1,harvestID);
      res=s.executeQuery();
    }
    while (res.next()) {
      final int harvestNum=res.getInt(1);
      HarvestRunInfo info=runInfos.get(harvestNum);
      info.setBytesHarvested(res.getLong(2));
      info.setDocsHarvested(res.getLong(3));
    }
    for (    HarvestRunInfo info : infoList) {
      if (info.getJobCount(JobStatus.STARTED) != 0 || info.getJobCount(JobStatus.NEW) != 0 || info.getJobCount(JobStatus.SUBMITTED) != 0) {
        info.setEndDate(null);
      }
    }
    return infoList;
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + harvestID + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * @see HarvestDefinitionDAO#getHarvestRunInfo(long)
 */
@Override public List<HarvestRunInfo> getHarvestRunInfo(long harvestID){
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    ResultSet res=null;
    Map<Integer,HarvestRunInfo> runInfos=new HashMap<Integer,HarvestRunInfo>();
    List<HarvestRunInfo> infoList=new ArrayList<HarvestRunInfo>();
synchronized (this) {
      s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
      s.setLong(1,harvestID);
      res=s.executeQuery();
      while (res.next()) {
        int runNr=res.getInt(2);
        HarvestRunInfo info=runInfos.get(runNr);
        if (info == null) {
          String name=res.getString(1);
          info=new HarvestRunInfo(harvestID,name,runNr);
          runInfos.put(runNr,info);
          infoList.add(info);
        }
        JobStatus status=JobStatus.fromOrdinal(res.getInt(3));
        if (status != JobStatus.NEW && status != JobStatus.SUBMITTED && status != JobStatus.RESUBMITTED) {
          Date startDate=DBUtils.getDateMaybeNull(res,4);
          if (info.getStartDate() == null || (startDate != null && startDate.before(info.getStartDate()))) {
            info.setStartDate(startDate);
          }
        }
        if (status == JobStatus.DONE || status == JobStatus.FAILED) {
          Date endDate=DBUtils.getDateMaybeNull(res,5);
          if (info.getEndDate() == null || (endDate != null && endDate.after(info.getEndDate()))) {
            info.setEndDate(endDate);
          }
        }
        int count=res.getInt(6);
        info.setStatusCount(status,count);
      }
      s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
      s.setLong(1,harvestID);
      res=s.executeQuery();
    }
    while (res.next()) {
      final int harvestNum=res.getInt(1);
      HarvestRunInfo info=runInfos.get(harvestNum);
      info.setBytesHarvested(res.getLong(2));
      info.setDocsHarvested(res.getLong(3));
    }
    for (    HarvestRunInfo info : infoList) {
      if (info.getJobCount(JobStatus.STARTED) != 0 || info.getJobCount(JobStatus.NEW) != 0 || info.getJobCount(JobStatus.SUBMITTED) != 0) {
        info.setEndDate(null);
      }
    }
    return infoList;
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + harvestID + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}",0.9981711777615216
89697,"/** 
 * Activates or deactivates a partial harvest definition. This method is actually to be used not to have to read from the DB big harvest definitions and optimize the activation / deactivation, it is sort of a lightweight version of update.
 * @param harvestDefinition the harvest definition object.
 */
public synchronized void flipActive(SparsePartialHarvest hd){
  ArgumentNotValid.checkNotNull(hd,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    if (hd.getOid() == null || !exists(c,hd.getOid())) {
      final String message=""String_Node_Str"" + ""String_Node_Str"" + hd.getName() + ""String_Node_Str"";
      log.debug(message);
      throw new PermissionDenied(message);
    }
    c.setAutoCommit(false);
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    DBUtils.setName(s,1,hd,Constants.MAX_NAME_SIZE);
    DBUtils.setComments(s,2,hd,Constants.MAX_COMMENT_SIZE);
    s.setInt(3,hd.getNumEvents());
    s.setTimestamp(4,new Timestamp(hd.getSubmissionDate().getTime()));
    s.setBoolean(5,!hd.isActive());
    long nextEdition=hd.getEdition() + 1;
    s.setLong(6,nextEdition);
    s.setLong(7,hd.getOid());
    s.setLong(8,hd.getEdition());
    int rows=s.executeUpdate();
    if (rows == 0) {
      String message=""String_Node_Str"" + hd + ""String_Node_Str""+ hd.getEdition()+ ""String_Node_Str"";
      log.debug(message);
      throw new PermissionDenied(message);
    }
    s.close();
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,hd.getScheduleName());
    DBUtils.setDateMaybeNull(s,2,hd.getNextDate());
    s.setLong(3,hd.getOid());
    rows=s.executeUpdate();
    log.debug(rows + ""String_Node_Str"");
    s.close();
    c.commit();
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + hd + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    DBUtils.rollbackIfNeeded(c,""String_Node_Str"",hd);
    HarvestDBConnection.release(c);
  }
}","/** 
 * Activates or deactivates a partial harvest definition. This method is actually to be used not to have to read from the DB big harvest definitions and optimize the activation / deactivation, it is sort of a lightweight version of update.
 * @param harvestDefinition the harvest definition object.
 */
@Override public synchronized void flipActive(SparsePartialHarvest harvestDefinition){
  ArgumentNotValid.checkNotNull(harvestDefinition,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    if (harvestDefinition.getOid() == null || !exists(c,harvestDefinition.getOid())) {
      final String message=""String_Node_Str"" + ""String_Node_Str"" + harvestDefinition.getName() + ""String_Node_Str"";
      log.debug(message);
      throw new PermissionDenied(message);
    }
    c.setAutoCommit(false);
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    DBUtils.setName(s,1,harvestDefinition,Constants.MAX_NAME_SIZE);
    DBUtils.setComments(s,2,harvestDefinition,Constants.MAX_COMMENT_SIZE);
    s.setInt(3,harvestDefinition.getNumEvents());
    s.setTimestamp(4,new Timestamp(harvestDefinition.getSubmissionDate().getTime()));
    s.setBoolean(5,!harvestDefinition.isActive());
    long nextEdition=harvestDefinition.getEdition() + 1;
    s.setLong(6,nextEdition);
    s.setLong(7,harvestDefinition.getOid());
    s.setLong(8,harvestDefinition.getEdition());
    int rows=s.executeUpdate();
    if (rows == 0) {
      String message=""String_Node_Str"" + harvestDefinition + ""String_Node_Str""+ harvestDefinition.getEdition()+ ""String_Node_Str"";
      log.debug(message);
      throw new PermissionDenied(message);
    }
    s.close();
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,harvestDefinition.getScheduleName());
    DBUtils.setDateMaybeNull(s,2,harvestDefinition.getNextDate());
    s.setLong(3,harvestDefinition.getOid());
    rows=s.executeUpdate();
    log.debug(rows + ""String_Node_Str"");
    s.close();
    c.commit();
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + harvestDefinition + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    DBUtils.rollbackIfNeeded(c,""String_Node_Str"",harvestDefinition);
    HarvestDBConnection.release(c);
  }
}",0.925595238095238
89698,"/** 
 * Create a harvest definition in Database. The harvest definition object should not have its ID set unless we are in the middle of migrating.
 * @param harvestDefinition A new harvest definition to store inthe database.
 * @return The harvestId for the just created harvest definition.
 * @see HarvestDefinitionDAO#create(HarvestDefinition)
 */
public synchronized Long create(HarvestDefinition harvestDefinition){
  Connection connection=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    Long id=harvestDefinition.getOid();
    if (id == null) {
      id=generateNextID(connection);
    }
    connection.setAutoCommit(false);
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"");
    s.setLong(1,id);
    DBUtils.setName(s,2,harvestDefinition,Constants.MAX_NAME_SIZE);
    DBUtils.setComments(s,3,harvestDefinition,Constants.MAX_COMMENT_SIZE);
    s.setLong(4,harvestDefinition.getNumEvents());
    Date submissiondate=new Date();
    s.setTimestamp(5,new Timestamp(submissiondate.getTime()));
    s.setBoolean(6,harvestDefinition.getActive());
    final int edition=1;
    s.setLong(7,edition);
    s.executeUpdate();
    s.close();
    if (harvestDefinition instanceof FullHarvest) {
      FullHarvest fh=(FullHarvest)harvestDefinition;
      s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"");
      s.setLong(1,id);
      s.setLong(2,fh.getMaxCountObjects());
      s.setLong(3,fh.getMaxBytes());
      s.setLong(4,fh.getMaxJobRunningTime());
      if (fh.getPreviousHarvestDefinition() != null) {
        s.setLong(5,fh.getPreviousHarvestDefinition().getOid());
      }
 else {
        s.setNull(5,Types.BIGINT);
      }
      s.executeUpdate();
    }
 else     if (harvestDefinition instanceof PartialHarvest) {
      PartialHarvest ph=(PartialHarvest)harvestDefinition;
      long scheduleId=DBUtils.selectLongValue(connection,""String_Node_Str"",ph.getSchedule().getName());
      s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"");
      s.setLong(1,id);
      s.setLong(2,scheduleId);
      DBUtils.setDateMaybeNull(s,3,ph.getNextDate());
      s.executeUpdate();
      createHarvestConfigsEntries(connection,ph,id);
    }
 else {
      String message=""String_Node_Str"" + harvestDefinition + ""String_Node_Str""+ harvestDefinition.getClass();
      log.warn(message);
      throw new ArgumentNotValid(message);
    }
    connection.commit();
    harvestDefinition.setSubmissionDate(submissiondate);
    harvestDefinition.setEdition(edition);
    harvestDefinition.setOid(id);
    return id;
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + harvestDefinition + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.rollbackIfNeeded(connection,""String_Node_Str"",harvestDefinition);
    HarvestDBConnection.release(connection);
  }
}","/** 
 * Create a harvest definition in Database. The harvest definition object should not have its ID set unless we are in the middle of migrating.
 * @param harvestDefinition A new harvest definition to store inthe database.
 * @return The harvestId for the just created harvest definition.
 * @see HarvestDefinitionDAO#create(HarvestDefinition)
 */
@Override public synchronized Long create(HarvestDefinition harvestDefinition){
  Connection connection=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    Long id=harvestDefinition.getOid();
    if (id == null) {
      id=generateNextID(connection);
    }
    connection.setAutoCommit(false);
    s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"");
    s.setLong(1,id);
    DBUtils.setName(s,2,harvestDefinition,Constants.MAX_NAME_SIZE);
    DBUtils.setComments(s,3,harvestDefinition,Constants.MAX_COMMENT_SIZE);
    s.setLong(4,harvestDefinition.getNumEvents());
    Date submissiondate=new Date();
    s.setTimestamp(5,new Timestamp(submissiondate.getTime()));
    s.setBoolean(6,harvestDefinition.getActive());
    final int edition=1;
    s.setLong(7,edition);
    s.executeUpdate();
    s.close();
    if (harvestDefinition instanceof FullHarvest) {
      FullHarvest fh=(FullHarvest)harvestDefinition;
      s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str"");
      s.setLong(1,id);
      s.setLong(2,fh.getMaxCountObjects());
      s.setLong(3,fh.getMaxBytes());
      s.setLong(4,fh.getMaxJobRunningTime());
      if (fh.getPreviousHarvestDefinition() != null) {
        s.setLong(5,fh.getPreviousHarvestDefinition().getOid());
      }
 else {
        s.setNull(5,Types.BIGINT);
      }
      s.executeUpdate();
    }
 else     if (harvestDefinition instanceof PartialHarvest) {
      PartialHarvest ph=(PartialHarvest)harvestDefinition;
      long scheduleId=DBUtils.selectLongValue(connection,""String_Node_Str"",ph.getSchedule().getName());
      s=connection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"");
      s.setLong(1,id);
      s.setLong(2,scheduleId);
      DBUtils.setDateMaybeNull(s,3,ph.getNextDate());
      s.executeUpdate();
      createHarvestConfigsEntries(connection,ph,id);
    }
 else {
      String message=""String_Node_Str"" + harvestDefinition + ""String_Node_Str""+ harvestDefinition.getClass();
      log.warn(message);
      throw new ArgumentNotValid(message);
    }
    connection.commit();
    harvestDefinition.setSubmissionDate(submissiondate);
    harvestDefinition.setEdition(edition);
    harvestDefinition.setOid(id);
    return id;
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + harvestDefinition + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.rollbackIfNeeded(connection,""String_Node_Str"",harvestDefinition);
    HarvestDBConnection.release(connection);
  }
}",0.9983622666229938
89699,"/** 
 * Read the stored harvest definition for the given ID.
 * @see HarvestDefinitionDAO#read(Long)
 * @param harvestDefinitionID An ID number for a harvest definition
 * @return A harvest definition that has been read from persistent storage.
 * @throws UnknownID if no entry with that ID exists in the database
 * @throws IOFailure If DB-failure occurs?
 */
private HarvestDefinition read(Connection c,Long harvestDefinitionID) throws UnknownID, IOFailure {
  if (!exists(c,harvestDefinitionID)) {
    String message=""String_Node_Str"" + harvestDefinitionID;
    log.debug(message);
    throw new UnknownID(message);
  }
  log.debug(""String_Node_Str"" + harvestDefinitionID);
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setLong(1,harvestDefinitionID);
    ResultSet res=s.executeQuery();
    if (res.next()) {
      log.debug(""String_Node_Str"" + harvestDefinitionID);
      final String name=res.getString(1);
      final String comments=res.getString(2);
      final int numEvents=res.getInt(3);
      final Date submissionDate=new Date(res.getTimestamp(4).getTime());
      final long maxObjects=res.getLong(6);
      final long maxBytes=res.getLong(7);
      final long maxJobRunningtime=res.getLong(8);
      FullHarvest fh;
      final long prevhd=res.getLong(5);
      if (!res.wasNull()) {
        fh=new FullHarvest(name,comments,prevhd,maxObjects,maxBytes,maxJobRunningtime);
      }
 else {
        fh=new FullHarvest(name,comments,null,maxObjects,maxBytes,maxJobRunningtime);
      }
      fh.setSubmissionDate(submissionDate);
      fh.setNumEvents(numEvents);
      fh.setActive(res.getBoolean(9));
      fh.setOid(harvestDefinitionID);
      fh.setEdition(res.getLong(10));
      log.debug(""String_Node_Str"" + harvestDefinitionID);
      return fh;
    }
    s.close();
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setLong(1,harvestDefinitionID);
    res=s.executeQuery();
    boolean foundPartialHarvest=res.next();
    if (foundPartialHarvest) {
      log.debug(""String_Node_Str"" + harvestDefinitionID);
      final String name=res.getString(1);
      final String comments=res.getString(2);
      final int numEvents=res.getInt(3);
      final Date submissionDate=new Date(res.getTimestamp(4).getTime());
      final boolean active=res.getBoolean(5);
      final long edition=res.getLong(6);
      final String scheduleName=res.getString(7);
      final Date nextDate=DBUtils.getDateMaybeNull(res,8);
      s.close();
      final DomainDAO domainDao=DomainDAO.getInstance();
      /** 
 * Helper class that contain (domainName, configName) pairs.
 */
class DomainConfigPair {
        /** 
 * The domain name. 
 */
        final String domainName;
        /** 
 * The config name. 
 */
        final String configName;
        /** 
 * Constructor for the DomainConfigPair class.
 * @param domainName A given domain name
 * @param configName A name for a specific configuration
 */
        public DomainConfigPair(        String domainName,        String configName){
          this.domainName=domainName;
          this.configName=configName;
        }
      }
      List<DomainConfigPair> configs=new ArrayList<DomainConfigPair>();
      s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
      s.setLong(1,harvestDefinitionID);
      res=s.executeQuery();
      while (res.next()) {
        configs.add(new DomainConfigPair(res.getString(1),res.getString(2)));
      }
      s.close();
      List<DomainConfiguration> configurations=new ArrayList<DomainConfiguration>();
      for (      DomainConfigPair domainConfig : configs) {
        Domain d=domainDao.read(domainConfig.domainName);
        configurations.add(d.getConfiguration(domainConfig.configName));
      }
      Schedule schedule=ScheduleDAO.getInstance().read(scheduleName);
      PartialHarvest ph=new PartialHarvest(configurations,schedule,name,comments);
      ph.setNumEvents(numEvents);
      ph.setSubmissionDate(submissionDate);
      ph.setActive(active);
      ph.setEdition(edition);
      ph.setNextDate(nextDate);
      ph.setOid(harvestDefinitionID);
      return ph;
    }
 else {
      throw new IllegalState(""String_Node_Str"" + ""String_Node_Str"" + harvestDefinitionID);
    }
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + harvestDefinitionID + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
}","/** 
 * Read the stored harvest definition for the given ID.
 * @see HarvestDefinitionDAO#read(Long)
 * @param c The used database connection
 * @param harvestDefinitionID An ID number for a harvest definition
 * @return A harvest definition that has been read from persistent storage.
 * @throws UnknownID if no entry with that ID exists in the database
 * @throws IOFailure If DB-failure occurs?
 */
private HarvestDefinition read(Connection c,Long harvestDefinitionID) throws UnknownID, IOFailure {
  if (!exists(c,harvestDefinitionID)) {
    String message=""String_Node_Str"" + harvestDefinitionID;
    log.debug(message);
    throw new UnknownID(message);
  }
  log.debug(""String_Node_Str"" + harvestDefinitionID);
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setLong(1,harvestDefinitionID);
    ResultSet res=s.executeQuery();
    if (res.next()) {
      log.debug(""String_Node_Str"" + harvestDefinitionID);
      final String name=res.getString(1);
      final String comments=res.getString(2);
      final int numEvents=res.getInt(3);
      final Date submissionDate=new Date(res.getTimestamp(4).getTime());
      final long maxObjects=res.getLong(6);
      final long maxBytes=res.getLong(7);
      final long maxJobRunningtime=res.getLong(8);
      FullHarvest fh;
      final long prevhd=res.getLong(5);
      if (!res.wasNull()) {
        fh=new FullHarvest(name,comments,prevhd,maxObjects,maxBytes,maxJobRunningtime);
      }
 else {
        fh=new FullHarvest(name,comments,null,maxObjects,maxBytes,maxJobRunningtime);
      }
      fh.setSubmissionDate(submissionDate);
      fh.setNumEvents(numEvents);
      fh.setActive(res.getBoolean(9));
      fh.setOid(harvestDefinitionID);
      fh.setEdition(res.getLong(10));
      log.debug(""String_Node_Str"" + harvestDefinitionID);
      return fh;
    }
    s.close();
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setLong(1,harvestDefinitionID);
    res=s.executeQuery();
    boolean foundPartialHarvest=res.next();
    if (foundPartialHarvest) {
      log.debug(""String_Node_Str"" + harvestDefinitionID);
      final String name=res.getString(1);
      final String comments=res.getString(2);
      final int numEvents=res.getInt(3);
      final Date submissionDate=new Date(res.getTimestamp(4).getTime());
      final boolean active=res.getBoolean(5);
      final long edition=res.getLong(6);
      final String scheduleName=res.getString(7);
      final Date nextDate=DBUtils.getDateMaybeNull(res,8);
      s.close();
      final DomainDAO domainDao=DomainDAO.getInstance();
      /** 
 * Helper class that contain (domainName, configName) pairs.
 */
class DomainConfigPair {
        /** 
 * The domain name. 
 */
        final String domainName;
        /** 
 * The config name. 
 */
        final String configName;
        /** 
 * Constructor for the DomainConfigPair class.
 * @param domainName A given domain name
 * @param configName A name for a specific configuration
 */
        public DomainConfigPair(        String domainName,        String configName){
          this.domainName=domainName;
          this.configName=configName;
        }
      }
      List<DomainConfigPair> configs=new ArrayList<DomainConfigPair>();
      s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
      s.setLong(1,harvestDefinitionID);
      res=s.executeQuery();
      while (res.next()) {
        configs.add(new DomainConfigPair(res.getString(1),res.getString(2)));
      }
      s.close();
      List<DomainConfiguration> configurations=new ArrayList<DomainConfiguration>();
      for (      DomainConfigPair domainConfig : configs) {
        Domain d=domainDao.read(domainConfig.domainName);
        configurations.add(d.getConfiguration(domainConfig.configName));
      }
      Schedule schedule=ScheduleDAO.getInstance().read(scheduleName);
      PartialHarvest ph=new PartialHarvest(configurations,schedule,name,comments);
      ph.setNumEvents(numEvents);
      ph.setSubmissionDate(submissionDate);
      ph.setActive(active);
      ph.setEdition(edition);
      ph.setNextDate(nextDate);
      ph.setOid(harvestDefinitionID);
      return ph;
    }
 else {
      throw new IllegalState(""String_Node_Str"" + ""String_Node_Str"" + harvestDefinitionID);
    }
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + harvestDefinitionID + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
}",0.995784061696658
89700,"/** 
 * Get the name of a harvest given its ID.
 * @param harvestDefinitionID The ID of a harvest
 * @return The name of the given harvest.
 * @throws ArgumentNotValid on null argument
 * @throws UnknownID        if no harvest has the given ID.
 * @throws IOFailure        on any other error talking to the database
 */
public String getHarvestName(Long harvestDefinitionID){
  ArgumentNotValid.checkNotNull(harvestDefinitionID,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"");
    s.setLong(1,harvestDefinitionID);
    ResultSet res=s.executeQuery();
    String name=null;
    while (res.next()) {
      if (name != null) {
        throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + harvestDefinitionID + ""String_Node_Str""+ name+ ""String_Node_Str""+ res.getString(1)+ ""String_Node_Str"");
      }
      name=res.getString(1);
    }
    if (name == null) {
      throw new UnknownID(""String_Node_Str"" + harvestDefinitionID);
    }
    return name;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + harvestDefinitionID + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * Get the name of a harvest given its ID.
 * @param harvestDefinitionID The ID of a harvest
 * @return The name of the given harvest.
 * @throws ArgumentNotValid on null argument
 * @throws UnknownID        if no harvest has the given ID.
 * @throws IOFailure        on any other error talking to the database
 */
@Override public String getHarvestName(Long harvestDefinitionID){
  ArgumentNotValid.checkNotNull(harvestDefinitionID,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"");
    s.setLong(1,harvestDefinitionID);
    ResultSet res=s.executeQuery();
    String name=null;
    while (res.next()) {
      if (name != null) {
        throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + harvestDefinitionID + ""String_Node_Str""+ name+ ""String_Node_Str""+ res.getString(1)+ ""String_Node_Str"");
      }
      name=res.getString(1);
    }
    if (name == null) {
      throw new UnknownID(""String_Node_Str"" + harvestDefinitionID);
    }
    return name;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + harvestDefinitionID + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}",0.9961210240496507
89701,"/** 
 * Get a sorted list of all domain names of a HarvestDefinition.
 * @param harvestName of HarvestDefinition
 * @return List of all domains of the HarvestDefinition.
 */
public List<String> getListOfDomainsOfHarvestDefinition(String harvestName){
  ArgumentNotValid.checkNotNullOrEmpty(harvestName,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,harvestName);
    ResultSet res=s.executeQuery();
    List<String> domains=new ArrayList<String>();
    while (res.next()) {
      domains.add(res.getString(1));
    }
    return domains;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}","/** 
 * Get a sorted list of all domain names of a HarvestDefinition.
 * @param harvestName of HarvestDefinition
 * @return List of all domains of the HarvestDefinition.
 */
@Override public List<String> getListOfDomainsOfHarvestDefinition(String harvestName){
  ArgumentNotValid.checkNotNullOrEmpty(harvestName,""String_Node_Str"");
  Connection c=HarvestDBConnection.get();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,harvestName);
    ResultSet res=s.executeQuery();
    List<String> domains=new ArrayList<String>();
    while (res.next()) {
      domains.add(res.getString(1));
    }
    return domains;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    HarvestDBConnection.release(c);
  }
}",0.9953746530989824
89702,"/** 
 * Get whether a given harvest is a snapshot or selective harvest.
 * @param harvestDefinitionID ID of a harvest
 * @return True if the given harvest is a snapshot harvest, falseotherwise.
 * @throws ArgumentNotValid on null argument
 * @throws UnknownID        if no harvest has the given ID.
 */
public boolean isSnapshot(Long harvestDefinitionID){
  ArgumentNotValid.checkNotNull(harvestDefinitionID,""String_Node_Str"");
  Connection connection=HarvestDBConnection.get();
  try {
    boolean isSnapshot=DBUtils.selectAny(connection,""String_Node_Str"",harvestDefinitionID);
    if (isSnapshot) {
      return true;
    }
    boolean isSelective=DBUtils.selectAny(connection,""String_Node_Str"" + ""String_Node_Str"",harvestDefinitionID);
    if (isSelective) {
      return false;
    }
    throw new UnknownID(""String_Node_Str"" + harvestDefinitionID);
  }
  finally {
    HarvestDBConnection.release(connection);
  }
}","/** 
 * Get whether a given harvest is a snapshot or selective harvest.
 * @param harvestDefinitionID ID of a harvest
 * @return True if the given harvest is a snapshot harvest, falseotherwise.
 * @throws ArgumentNotValid on null argument
 * @throws UnknownID        if no harvest has the given ID.
 */
@Override public boolean isSnapshot(Long harvestDefinitionID){
  ArgumentNotValid.checkNotNull(harvestDefinitionID,""String_Node_Str"");
  Connection connection=HarvestDBConnection.get();
  try {
    boolean isSnapshot=DBUtils.selectAny(connection,""String_Node_Str"",harvestDefinitionID);
    if (isSnapshot) {
      return true;
    }
    boolean isSelective=DBUtils.selectAny(connection,""String_Node_Str"" + ""String_Node_Str"",harvestDefinitionID);
    if (isSelective) {
      return false;
    }
    throw new UnknownID(""String_Node_Str"" + harvestDefinitionID);
  }
  finally {
    HarvestDBConnection.release(connection);
  }
}",0.9945945945945946
89703,"/** 
 * Returns an iterator on the given sort key.
 * @param the cursor (sort key) to iterate on.
 */
ReportIterator(EntityCursor<PersistentLine> cursor){
  this.cursor=cursor;
  iter=cursor.iterator();
}","/** 
 * Returns an iterator on the given sort key.
 * @param cursor The cursor (sort key) to iterate on.
 */
ReportIterator(EntityCursor<PersistentLine> cursor){
  this.cursor=cursor;
  iter=cursor.iterator();
}",0.9783132530120482
89704,"/** 
 * Properties of the hibernate session are loaded from settings.xml. There is therefore no need for a separate hibernate configuration file.
 */
private static void initialiseFactory(){
  if (sessionFactory == null || sessionFactory.isClosed()) {
    try {
      log.info(""String_Node_Str"");
      AnnotationConfiguration config=new AnnotationConfiguration();
      config.setProperty(CONNECTION_PROVIDER_CLASS,ORG_HIBERNATE_CONNECTION_C3_P0_CONNECTION_PROVIDER);
      config.setProperty(C3P0_ACQUIRE_INCREMENT,Settings.get(WaybackSettings.C3P0_ACQUIRE_INCREMENT));
      config.setProperty(C3P0_IDLE_TEST_PERIOD,Settings.get(WaybackSettings.C3P0_IDLE_PERIOD));
      config.setProperty(C3P0_MAX_SIZE,Settings.get(WaybackSettings.C3P0_MAX_SIZE));
      config.setProperty(C3P0_MAX_STATEMENTS,Settings.get(WaybackSettings.C3P0_MAX_STATEMENTS));
      config.setProperty(C3P0_MIN_SIZE,Settings.get(WaybackSettings.C3P0_MIN_SIZE));
      config.setProperty(C3P0_TIMEOUT,Settings.get(WaybackSettings.C3P0_TIMEOUT));
      config.setProperty(HIBERNATE_CONNECTION_DRIVER_CLASS,Settings.get(WaybackSettings.HIBERNATE_DB_DRIVER));
      config.setProperty(HIBERNATE_CONNECTION_URL,Settings.get(WaybackSettings.HIBERNATE_DB_URL));
      config.setProperty(HIBERNATE_DIALECT,Settings.get(WaybackSettings.HIBERNATE_DIALECT));
      config.setProperty(HIBERNATE_FORMAT_SQL,Settings.get(WaybackSettings.HIBERNATE_FORMAT_SQL));
      config.setProperty(HIBERNATE_BYTECODE_USE_REFLECTION_OPTIMIZER,Settings.get(WaybackSettings.HIBERNATE_REFLECTION_OPTIMIZER));
      config.setProperty(HIBERNATE_HBM2DDL_AUTO,Settings.get(WaybackSettings.HIBERNATE_HBM2DDL_AUTO));
      config.setProperty(HIBERNATE_TRANSACTION_FACTORY_CLASS,Settings.get(WaybackSettings.HIBERNATE_TRANSACTION_FACTORY));
      config.setProperty(HIBERNATE_SHOW_SQL,Settings.get(WaybackSettings.HIBERNATE_SHOW_SQL));
      log.info(""String_Node_Str"" + CONNECTION_PROVIDER_CLASS + ""String_Node_Str""+ ORG_HIBERNATE_CONNECTION_C3_P0_CONNECTION_PROVIDER+ ""String_Node_Str""+ C3P0_ACQUIRE_INCREMENT+ ""String_Node_Str""+ Settings.get(WaybackSettings.C3P0_ACQUIRE_INCREMENT)+ ""String_Node_Str""+ C3P0_IDLE_TEST_PERIOD+ ""String_Node_Str""+ Settings.get(WaybackSettings.C3P0_IDLE_PERIOD + ""String_Node_Str"" + C3P0_MAX_SIZE+ ""String_Node_Str""+ Settings.get(WaybackSettings.C3P0_MAX_SIZE)+ ""String_Node_Str""+ C3P0_MAX_STATEMENTS+ ""String_Node_Str""+ Settings.get(WaybackSettings.C3P0_MAX_STATEMENTS)+ ""String_Node_Str""+ C3P0_MIN_SIZE+ ""String_Node_Str""+ Settings.get(WaybackSettings.C3P0_MIN_SIZE)+ ""String_Node_Str""+ C3P0_TIMEOUT+ ""String_Node_Str""+ Settings.get(WaybackSettings.C3P0_TIMEOUT)+ ""String_Node_Str""+ HIBERNATE_CONNECTION_DRIVER_CLASS+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_DB_DRIVER)+ ""String_Node_Str""+ HIBERNATE_CONNECTION_URL+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_DB_URL)+ ""String_Node_Str""+ HIBERNATE_DIALECT+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_DIALECT)+ ""String_Node_Str""+ HIBERNATE_FORMAT_SQL+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_FORMAT_SQL)+ ""String_Node_Str""+ HIBERNATE_BYTECODE_USE_REFLECTION_OPTIMIZER+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_REFLECTION_OPTIMIZER)+ ""String_Node_Str""+ HIBERNATE_HBM2DDL_AUTO+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_HBM2DDL_AUTO)+ ""String_Node_Str""+ HIBERNATE_TRANSACTION_FACTORY_CLASS+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_TRANSACTION_FACTORY)+ ""String_Node_Str""+ HIBERNATE_SHOW_SQL+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_SHOW_SQL)));
      if (!Settings.get(WaybackSettings.HIBERNATE_USERNAME).isEmpty()) {
        config.setProperty(HIBERNATE_CONNECTION_USERNAME,Settings.get(WaybackSettings.HIBERNATE_USERNAME));
      }
      if (!Settings.get(WaybackSettings.HIBERNATE_PASSWORD).isEmpty()) {
        config.setProperty(HIBERNATE_CONNECTION_PASSWORD,Settings.get(WaybackSettings.HIBERNATE_PASSWORD));
      }
      config.addAnnotatedClass(ArchiveFile.class);
      sessionFactory=config.buildSessionFactory();
    }
 catch (    Throwable ex) {
      log.fatal(""String_Node_Str"" + ""String_Node_Str"",ex);
      throw new IllegalStateException(""String_Node_Str"" + ""String_Node_Str"",ex);
    }
  }
}","/** 
 * Properties of the hibernate session are loaded from settings.xml. There is therefore no need for a separate hibernate configuration file.
 */
private static void initialiseFactory(){
  if (sessionFactory == null || sessionFactory.isClosed()) {
    try {
      log.info(""String_Node_Str"");
      AnnotationConfiguration config=new AnnotationConfiguration();
      config.setProperty(CONNECTION_PROVIDER_CLASS,ORG_HIBERNATE_CONNECTION_C3_P0_CONNECTION_PROVIDER);
      config.setProperty(C3P0_ACQUIRE_INCREMENT,Settings.get(WaybackSettings.C3P0_ACQUIRE_INCREMENT));
      config.setProperty(C3P0_IDLE_TEST_PERIOD,Settings.get(WaybackSettings.C3P0_IDLE_PERIOD));
      config.setProperty(C3P0_MAX_SIZE,Settings.get(WaybackSettings.C3P0_MAX_SIZE));
      config.setProperty(C3P0_MAX_STATEMENTS,Settings.get(WaybackSettings.C3P0_MAX_STATEMENTS));
      config.setProperty(C3P0_MIN_SIZE,Settings.get(WaybackSettings.C3P0_MIN_SIZE));
      config.setProperty(C3P0_TIMEOUT,Settings.get(WaybackSettings.C3P0_TIMEOUT));
      config.setProperty(HIBERNATE_CONNECTION_DRIVER_CLASS,Settings.get(WaybackSettings.HIBERNATE_DB_DRIVER));
      config.setProperty(HIBERNATE_CONNECTION_URL,Settings.get(WaybackSettings.HIBERNATE_DB_URL));
      config.setProperty(HIBERNATE_DIALECT,Settings.get(WaybackSettings.HIBERNATE_DIALECT));
      config.setProperty(HIBERNATE_FORMAT_SQL,Settings.get(WaybackSettings.HIBERNATE_FORMAT_SQL));
      config.setProperty(HIBERNATE_BYTECODE_USE_REFLECTION_OPTIMIZER,Settings.get(WaybackSettings.HIBERNATE_REFLECTION_OPTIMIZER));
      config.setProperty(HIBERNATE_HBM2DDL_AUTO,Settings.get(WaybackSettings.HIBERNATE_HBM2DDL_AUTO));
      config.setProperty(HIBERNATE_TRANSACTION_FACTORY_CLASS,Settings.get(WaybackSettings.HIBERNATE_TRANSACTION_FACTORY));
      config.setProperty(HIBERNATE_SHOW_SQL,Settings.get(WaybackSettings.HIBERNATE_SHOW_SQL));
      log.info(""String_Node_Str"" + CONNECTION_PROVIDER_CLASS + ""String_Node_Str""+ ORG_HIBERNATE_CONNECTION_C3_P0_CONNECTION_PROVIDER+ ""String_Node_Str""+ C3P0_ACQUIRE_INCREMENT+ ""String_Node_Str""+ Settings.get(WaybackSettings.C3P0_ACQUIRE_INCREMENT)+ ""String_Node_Str""+ C3P0_IDLE_TEST_PERIOD+ ""String_Node_Str""+ Settings.get(WaybackSettings.C3P0_IDLE_PERIOD)+ ""String_Node_Str""+ C3P0_MAX_SIZE+ ""String_Node_Str""+ Settings.get(WaybackSettings.C3P0_MAX_SIZE)+ ""String_Node_Str""+ C3P0_MAX_STATEMENTS+ ""String_Node_Str""+ Settings.get(WaybackSettings.C3P0_MAX_STATEMENTS)+ ""String_Node_Str""+ C3P0_MIN_SIZE+ ""String_Node_Str""+ Settings.get(WaybackSettings.C3P0_MIN_SIZE)+ ""String_Node_Str""+ C3P0_TIMEOUT+ ""String_Node_Str""+ Settings.get(WaybackSettings.C3P0_TIMEOUT)+ ""String_Node_Str""+ HIBERNATE_CONNECTION_DRIVER_CLASS+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_DB_DRIVER)+ ""String_Node_Str""+ HIBERNATE_CONNECTION_URL+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_DB_URL)+ ""String_Node_Str""+ HIBERNATE_DIALECT+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_DIALECT)+ ""String_Node_Str""+ HIBERNATE_FORMAT_SQL+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_FORMAT_SQL)+ ""String_Node_Str""+ HIBERNATE_BYTECODE_USE_REFLECTION_OPTIMIZER+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_REFLECTION_OPTIMIZER)+ ""String_Node_Str""+ HIBERNATE_HBM2DDL_AUTO+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_HBM2DDL_AUTO)+ ""String_Node_Str""+ HIBERNATE_TRANSACTION_FACTORY_CLASS+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_TRANSACTION_FACTORY)+ ""String_Node_Str""+ HIBERNATE_SHOW_SQL+ ""String_Node_Str""+ Settings.get(WaybackSettings.HIBERNATE_SHOW_SQL));
      if (!Settings.get(WaybackSettings.HIBERNATE_USERNAME).isEmpty()) {
        config.setProperty(HIBERNATE_CONNECTION_USERNAME,Settings.get(WaybackSettings.HIBERNATE_USERNAME));
      }
      if (!Settings.get(WaybackSettings.HIBERNATE_PASSWORD).isEmpty()) {
        config.setProperty(HIBERNATE_CONNECTION_PASSWORD,Settings.get(WaybackSettings.HIBERNATE_PASSWORD));
      }
      config.addAnnotatedClass(ArchiveFile.class);
      sessionFactory=config.buildSessionFactory();
    }
 catch (    Throwable ex) {
      log.fatal(""String_Node_Str"" + ""String_Node_Str"",ex);
      throw new IllegalStateException(""String_Node_Str"" + ""String_Node_Str"",ex);
    }
  }
}",0.9950704225352112
89705,"/** 
 * Look up a given URI and return the contents as an InputStream. The uri is first checked using url-decoding (e.g. "","" in the argument is converted to ""%2C""). If this returns no match, the method then searches for a non-url-decoded match. If neither returns a match the method returns null.
 * @param uri The URI to find in the archive.  If the URI does notmatch any entries in the archive, null is returned.
 * @return An InputStream Containing all the data in the entry, ornull if the entry was not found
 * @throws IOFailure If the ARC file was found in the Lucene index but notin the bit archive, or if some other failure happened while finding the file.
 */
public InputStream lookup(URI uri){
  ArgumentNotValid.checkNotNull(uri,""String_Node_Str"");
  ARCKey key=luceneLookup(uri.getScheme() + ""String_Node_Str"" + uri.getSchemeSpecificPart());
  if (key == null) {
    key=luceneLookup(uri.getScheme() + ""String_Node_Str"" + uri.getRawSchemeSpecificPart());
  }
  if (key == null) {
    return null;
  }
 else {
    final BitarchiveRecord bitarchiveRecord=arcRepositoryClient.get(key.getFile().getName(),key.getOffset());
    if (bitarchiveRecord == null) {
      String message=""String_Node_Str"" + key.getFile().getName() + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
      log.debug(message);
      throw new IOFailure(message);
    }
    return bitarchiveRecord.getData();
  }
}","/** 
 * Look up a given URI and return the contents as an InputStream. The uri is first checked using url-decoding (e.g. "","" in the argument is converted to ""%2C""). If this returns no match, the method then searches for a non-url-decoded match. If neither returns a match the method returns null. If the tryToLookupUriAsFtp field is set to true, we will try exchanging the schema with ftp, whenever we can't lookup the uri with the original schema.
 * @param uri The URI to find in the archive.  If the URI does notmatch any entries in the archive, null is returned.
 * @return An InputStream Containing all the data in the entry, ornull if the entry was not found
 * @throws IOFailure If the ARC file was found in the Lucene index but notin the bit archive, or if some other failure happened while finding the file.
 */
public InputStream lookup(URI uri){
  ArgumentNotValid.checkNotNull(uri,""String_Node_Str"");
  ARCKey key=luceneLookup(uri.getScheme() + ""String_Node_Str"" + uri.getSchemeSpecificPart());
  if (key == null) {
    key=luceneLookup(uri.getScheme() + ""String_Node_Str"" + uri.getRawSchemeSpecificPart());
  }
  if (key == null && tryToLookupUriAsFtp) {
    log.debug(""String_Node_Str"" + uri.getScheme() + ""String_Node_Str"");
    final String ftpSchema=""String_Node_Str"";
    key=luceneLookup(ftpSchema + ""String_Node_Str"" + uri.getSchemeSpecificPart());
    if (key == null) {
      key=luceneLookup(ftpSchema + ""String_Node_Str"" + uri.getRawSchemeSpecificPart());
    }
  }
  if (key == null) {
    return null;
  }
 else {
    final BitarchiveRecord bitarchiveRecord=arcRepositoryClient.get(key.getFile().getName(),key.getOffset());
    if (bitarchiveRecord == null) {
      String message=""String_Node_Str"" + key.getFile().getName() + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
      log.debug(message);
      throw new IOFailure(message);
    }
    return bitarchiveRecord.getData();
  }
}",0.8477366255144033
89706,"/** 
 * Updates the members from a   {@link CrawlProgressMessage} instance.
 * @param msg the  {@link CrawlProgressMessage} to process.
 */
public static StartedJobInfo build(CrawlProgressMessage msg){
  String harvestName=HarvestDefinitionDAO.getInstance().read(msg.getHarvestID()).getName();
  StartedJobInfo sji=new StartedJobInfo(harvestName,msg.getJobID());
  CrawlServiceInfo heritrixInfo=msg.getHeritrixStatus();
  CrawlServiceJobInfo jobInfo=msg.getJobStatus();
  CrawlStatus newStatus=msg.getStatus();
switch (newStatus) {
case PRE_CRAWL:
    sji.activeQueuesCount=0;
  sji.activeToeCount=0;
sji.alertsCount=0;
sji.currentProcessedDocsPerSec=0;
sji.currentProcessedKBPerSec=0;
sji.downloadedFilesCount=0;
sji.elapsedSeconds=0;
sji.hostUrl=""String_Node_Str"";
sji.processedDocsPerSec=0;
sji.processedKBPerSec=0;
sji.progress=0;
sji.queuedFilesCount=0;
sji.totalQueuesCount=0;
break;
case CRAWLER_ACTIVE:
case CRAWLER_PAUSING:
case CRAWLER_PAUSED:
double discoveredCount=jobInfo.getDiscoveredFilesCount();
double downloadedCount=jobInfo.getDownloadedFilesCount();
sji.progress=100 * downloadedCount / discoveredCount;
String frontierShortReport=jobInfo.getFrontierShortReport();
if (frontierShortReport != null) {
try {
Object[] params=FRONTIER_SHORT_FMT.parse(frontierShortReport);
sji.totalQueuesCount=Long.parseLong((String)params[0]);
sji.activeQueuesCount=Long.parseLong((String)params[1]);
sji.retiredQueuesCount=Long.parseLong((String)params[6]);
sji.exhaustedQueuesCount=Long.parseLong((String)params[7]);
}
 catch (ParseException e) {
throw new ArgumentNotValid(frontierShortReport,e);
}
}
sji.activeToeCount=jobInfo.getActiveToeCount();
sji.alertsCount=heritrixInfo.getAlertCount();
sji.currentProcessedDocsPerSec=jobInfo.getCurrentProcessedDocsPerSec();
sji.currentProcessedKBPerSec=jobInfo.getCurrentProcessedKBPerSec();
sji.downloadedFilesCount=jobInfo.getDownloadedFilesCount();
sji.elapsedSeconds=jobInfo.getElapsedSeconds();
sji.hostUrl=msg.getHostUrl();
sji.processedDocsPerSec=jobInfo.getProcessedDocsPerSec();
sji.processedKBPerSec=jobInfo.getProcessedKBPerSec();
sji.queuedFilesCount=jobInfo.getQueuedUriCount();
break;
case CRAWLING_FINISHED:
sji.progress=100;
sji.hostUrl=""String_Node_Str"";
sji.activeQueuesCount=0;
sji.activeToeCount=0;
sji.currentProcessedDocsPerSec=0;
sji.currentProcessedKBPerSec=0;
sji.processedDocsPerSec=0;
sji.processedKBPerSec=0;
sji.queuedFilesCount=0;
sji.totalQueuesCount=0;
break;
}
sji.status=newStatus;
return sji;
}","/** 
 * Updates the members from a   {@link CrawlProgressMessage} instance.
 * @param msg the  {@link CrawlProgressMessage} to process.
 */
public static StartedJobInfo build(CrawlProgressMessage msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  String harvestName=HarvestDefinitionDAO.getInstance().getHarvestName(msg.getHarvestID());
  StartedJobInfo sji=new StartedJobInfo(harvestName,msg.getJobID());
  CrawlServiceInfo heritrixInfo=msg.getHeritrixStatus();
  CrawlServiceJobInfo jobInfo=msg.getJobStatus();
  CrawlStatus newStatus=msg.getStatus();
switch (newStatus) {
case PRE_CRAWL:
    sji.activeQueuesCount=0;
  sji.activeToeCount=0;
sji.alertsCount=0;
sji.currentProcessedDocsPerSec=0;
sji.currentProcessedKBPerSec=0;
sji.downloadedFilesCount=0;
sji.elapsedSeconds=0;
sji.hostUrl=""String_Node_Str"";
sji.processedDocsPerSec=0;
sji.processedKBPerSec=0;
sji.progress=0;
sji.queuedFilesCount=0;
sji.totalQueuesCount=0;
break;
case CRAWLER_ACTIVE:
case CRAWLER_PAUSING:
case CRAWLER_PAUSED:
double discoveredCount=jobInfo.getDiscoveredFilesCount();
double downloadedCount=jobInfo.getDownloadedFilesCount();
sji.progress=100 * downloadedCount / discoveredCount;
String frontierShortReport=jobInfo.getFrontierShortReport();
if (frontierShortReport != null) {
try {
Object[] params=FRONTIER_SHORT_FMT.parse(frontierShortReport);
sji.totalQueuesCount=Long.parseLong((String)params[0]);
sji.activeQueuesCount=Long.parseLong((String)params[1]);
sji.retiredQueuesCount=Long.parseLong((String)params[6]);
sji.exhaustedQueuesCount=Long.parseLong((String)params[7]);
}
 catch (ParseException e) {
throw new ArgumentNotValid(frontierShortReport,e);
}
}
sji.activeToeCount=jobInfo.getActiveToeCount();
sji.alertsCount=heritrixInfo.getAlertCount();
sji.currentProcessedDocsPerSec=jobInfo.getCurrentProcessedDocsPerSec();
sji.currentProcessedKBPerSec=jobInfo.getCurrentProcessedKBPerSec();
sji.downloadedFilesCount=jobInfo.getDownloadedFilesCount();
sji.elapsedSeconds=jobInfo.getElapsedSeconds();
sji.hostUrl=msg.getHostUrl();
sji.processedDocsPerSec=jobInfo.getProcessedDocsPerSec();
sji.processedKBPerSec=jobInfo.getProcessedKBPerSec();
sji.queuedFilesCount=jobInfo.getQueuedUriCount();
break;
case CRAWLING_FINISHED:
sji.progress=100;
sji.hostUrl=""String_Node_Str"";
sji.activeQueuesCount=0;
sji.activeToeCount=0;
sji.currentProcessedDocsPerSec=0;
sji.currentProcessedKBPerSec=0;
sji.processedDocsPerSec=0;
sji.processedKBPerSec=0;
sji.queuedFilesCount=0;
sji.totalQueuesCount=0;
break;
}
sji.status=newStatus;
return sji;
}",0.9828274760383386
89707,"/** 
 * Alternate constructor, which does not have the DomainHarvestreport as argument.
 * @param jobID (see description for the other constructor)
 * @param statusCode (see description for the other constructor)
 * @see CrawlStatusMessage#CrawlStatusMessage(long,JobStatus,AbstractHarvestReport)
 */
public CrawlStatusMessage(long jobID,JobStatus statusCode){
  this(jobID,statusCode,null);
}","/** 
 * Alternate constructor, which does not have the DomainHarvestreport as argument.
 * @param jobID (see description for the other constructor)
 * @param statusCode (see description for the other constructor)
 * @see CrawlStatusMessage#CrawlStatusMessage(long,JobStatus,HarvestReport)
 */
public CrawlStatusMessage(long jobID,JobStatus statusCode){
  this(jobID,statusCode,null);
}",0.9897172236503856
89708,"@Override public void run(){
  CrawlProgressMessage cpm;
  try {
    cpm=heritrixController.getCrawlProgress();
  }
 catch (  IOFailure iof) {
    log.warn(""String_Node_Str"",iof);
    return;
  }
  getJMSConnection().send(cpm);
  HeritrixFiles files=getHeritrixFiles();
  if (cpm.crawlIsFinished()) {
    log.info(""String_Node_Str"" + files.getJobID() + ""String_Node_Str"");
    crawlIsOver=true;
    return;
  }
  log.info(""String_Node_Str"" + files.getJobID() + ""String_Node_Str""+ files.getHarvestID()+ ""String_Node_Str""+ cpm.getHostUrl()+ ""String_Node_Str""+ cpm.getProgressStatisticsLegend()+ ""String_Node_Str""+ cpm.getJobStatus().getStatus()+ ""String_Node_Str""+ cpm.getJobStatus().getProgressStatistics());
}","@Override public void run(){
  CrawlProgressMessage cpm;
  if (crawlIsOver) {
    return;
  }
  try {
    cpm=heritrixController.getCrawlProgress();
  }
 catch (  IOFailure iof) {
    log.warn(""String_Node_Str"",iof);
    return;
  }
  getJMSConnection().send(cpm);
  HeritrixFiles files=getHeritrixFiles();
  if (cpm.crawlIsFinished()) {
    log.info(""String_Node_Str"" + files.getJobID() + ""String_Node_Str"");
    crawlIsOver=true;
    return;
  }
  log.info(""String_Node_Str"" + files.getJobID() + ""String_Node_Str""+ files.getHarvestID()+ ""String_Node_Str""+ cpm.getHostUrl()+ ""String_Node_Str""+ cpm.getProgressStatisticsLegend()+ ""String_Node_Str""+ cpm.getJobStatus().getStatus()+ ""String_Node_Str""+ cpm.getJobStatus().getProgressStatistics());
}",0.9745704467353952
89709,"/** 
 * Initialise common fields in remote file. Overriding classes should also initalise checksum field.
 * @param file The file to make remote file for.
 * @param useChecksums If true, communications should be checksummed.
 * @param fileDeletable If true, the file may be downloaded multple times.Otherwise, the remote file is invalidated after first transfer.
 * @param multipleDownloads If useChecksums is true, contains the filechecksum.
 */
public AbstractRemoteFile(File file,boolean useChecksums,boolean fileDeletable,boolean multipleDownloads){
  ArgumentNotValid.checkNotNull(file,""String_Node_Str"");
  if (!file.isFile() || !file.canRead()) {
    throw new ArgumentNotValid(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"");
  }
  this.file=file;
  this.fileDeletable=fileDeletable;
  this.multipleDownloads=multipleDownloads;
  this.useChecksums=useChecksums;
  this.filesize=file.length();
}","/** 
 * Initialise common fields in remote file. Overriding classes should also initialise checksum field.
 * @param file The file to make remote file for.
 * @param useChecksums If true, communications should be checksummed.
 * @param fileDeletable If true, the file may be downloaded multiple times.Otherwise, the remote file is invalidated after first transfer.
 * @param multipleDownloads If useChecksums is true, contains the filechecksum.
 */
public AbstractRemoteFile(File file,boolean useChecksums,boolean fileDeletable,boolean multipleDownloads){
  ArgumentNotValid.checkNotNull(file,""String_Node_Str"");
  if (!file.isFile() || !file.canRead()) {
    throw new ArgumentNotValid(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"");
  }
  this.file=file;
  this.fileDeletable=fileDeletable;
  this.multipleDownloads=multipleDownloads;
  this.useChecksums=useChecksums;
  this.filesize=file.length();
}",0.998911860718172
89710,"/** 
 * Private constructor used by getInstance() static-method Tries to generate unique name on ftp-server.
 * @param localFile         File used to create new file on ftp-server.
 * @param useChecksums      If true, checksums will be used to checktransfers.
 * @param fileDeletable     If true, this file will be deleted after uploadto FTP.
 * @param multipleDownloads If true, the file will not be removed from FTPserver automatically after first download.
 * @throws IOFailure        if MD5 checksum fails, or ftp fails
 * @throws ArgumentNotValid if the local file cannot be read.
 */
private FTPRemoteFile(File localFile,boolean useChecksums,boolean fileDeletable,boolean multipleDownloads) throws IOFailure {
  super(localFile,useChecksums,fileDeletable,multipleDownloads);
  if (filesize == 0) {
    try {
      if (useChecksums) {
        checksum=MD5.generateMD5onFile(file);
      }
 else {
        checksum=null;
      }
      ftpFileName=""String_Node_Str"";
    }
 catch (    IOException e) {
      throw new IOFailure(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"",e);
    }
  }
 else {
    if (ftpServerName.equalsIgnoreCase(""String_Node_Str"")) {
      ftpServerName=SystemUtils.getLocalHostName();
      log.debug(""String_Node_Str"" + SystemUtils.getLocalHostName() + ""String_Node_Str""+ ftpServerName);
    }
    final int aMagicNumber=100000;
    ftpFileName=file.getName() + ""String_Node_Str"" + (int)(Math.random() * aMagicNumber)+ ""String_Node_Str""+ new Date().getTime();
    InputStream in;
    try {
      in=new FileInputStream(localFile);
    }
 catch (    FileNotFoundException e) {
      final String message=""String_Node_Str"" + localFile + ""String_Node_Str"";
      log.debug(message,e);
      throw new IOFailure(message,e);
    }
    log.debug(""String_Node_Str"" + file.getName() + ""String_Node_Str""+ ftpFileName+ ""String_Node_Str""+ ftpServerName);
    try {
      logOn();
      if (useChecksums) {
        in=new DigestInputStream(in,MD5.getMessageDigestInstance());
      }
      boolean success=false;
      int tried=0;
      while (!success && tried < FTP_COPYTO_RETRIES) {
        tried++;
        try {
          success=currentFTPClient.storeFile(ftpFileName,in);
          if (!success) {
            log.debug(""String_Node_Str"" + tried + ""String_Node_Str""+ FTP_COPYTO_RETRIES+ ""String_Node_Str""+ getFtpErrorMessage());
          }
        }
 catch (        IOException e) {
          final String message=""String_Node_Str"" + ftpFileName + ""String_Node_Str""+ tried+ ""String_Node_Str""+ FTP_COPYTO_RETRIES;
          log.debug(message,e);
        }
      }
      if (!success) {
        final String msg=""String_Node_Str"" + localFile + ""String_Node_Str""+ tried+ ""String_Node_Str"";
        log.warn(msg);
        throw new IOFailure(msg);
      }
      log.debug(""String_Node_Str"" + ftpFileName + ""String_Node_Str"");
      if (useChecksums) {
        checksum=MD5.toHex(((DigestInputStream)in).getMessageDigest().digest());
        log.debug(""String_Node_Str"" + ftpFileName + ""String_Node_Str""+ checksum);
      }
 else {
        checksum=null;
      }
    }
  finally {
      try {
        if (in != null) {
          in.close();
        }
      }
 catch (      IOException e) {
        log.warn(""String_Node_Str"");
      }
      logOut();
      log.debug(""String_Node_Str"");
    }
  }
  if (fileDeletable) {
    try {
      FileUtils.removeRecursively(localFile);
    }
 catch (    IOFailure e) {
      log.warn(""String_Node_Str"" + localFile,e);
    }
  }
}","/** 
 * Private constructor used by getInstance() static-method Tries to generate unique name on ftp-server.
 * @param localFile         File used to create new file on ftp-server.
 * @param useChecksums      If true, checksums will be used to checktransfers.
 * @param fileDeletable     If true, this file will be deleted after uploadto FTP.
 * @param multipleDownloads If true, the file will not be removed from FTPserver automatically after first download.
 * @throws IOFailure        if MD5 checksum fails, or ftp fails
 * @throws ArgumentNotValid if the local file cannot be read.
 */
private FTPRemoteFile(File localFile,boolean useChecksums,boolean fileDeletable,boolean multipleDownloads) throws IOFailure {
  super(localFile,useChecksums,fileDeletable,multipleDownloads);
  if (filesize == 0) {
    try {
      if (useChecksums) {
        checksum=MD5.generateMD5onFile(file);
      }
 else {
        checksum=null;
      }
      ftpFileName=""String_Node_Str"";
    }
 catch (    IOException e) {
      String msg=""String_Node_Str"";
      if (e instanceof CopyStreamException) {
        CopyStreamException realException=(CopyStreamException)e;
        msg+=""String_Node_Str"" + realException.getIOException() + ""String_Node_Str"";
      }
      throw new IOFailure(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str""+ msg,e);
    }
  }
 else {
    if (ftpServerName.equalsIgnoreCase(""String_Node_Str"")) {
      ftpServerName=SystemUtils.getLocalHostName();
      log.debug(""String_Node_Str"" + SystemUtils.getLocalHostName() + ""String_Node_Str""+ ftpServerName);
    }
    final int aMagicNumber=100000;
    ftpFileName=file.getName() + ""String_Node_Str"" + (int)(Math.random() * aMagicNumber)+ ""String_Node_Str""+ new Date().getTime();
    InputStream in;
    try {
      in=new FileInputStream(localFile);
    }
 catch (    FileNotFoundException e) {
      final String message=""String_Node_Str"" + localFile + ""String_Node_Str"";
      log.debug(message,e);
      throw new IOFailure(message,e);
    }
    log.debug(""String_Node_Str"" + file.getName() + ""String_Node_Str""+ ftpFileName+ ""String_Node_Str""+ ftpServerName);
    try {
      logOn();
      if (useChecksums) {
        in=new DigestInputStream(in,MD5.getMessageDigestInstance());
      }
      boolean success=false;
      int tried=0;
      while (!success && tried < FTP_COPYTO_RETRIES) {
        tried++;
        try {
          success=currentFTPClient.storeFile(ftpFileName,in);
          if (!success) {
            log.debug(""String_Node_Str"" + tried + ""String_Node_Str""+ FTP_COPYTO_RETRIES+ ""String_Node_Str""+ getFtpErrorMessage());
          }
        }
 catch (        IOException e) {
          String message=""String_Node_Str"" + ftpFileName + ""String_Node_Str""+ tried+ ""String_Node_Str""+ FTP_COPYTO_RETRIES;
          if (e instanceof CopyStreamException) {
            CopyStreamException realException=(CopyStreamException)e;
            message+=""String_Node_Str"" + realException.getIOException() + ""String_Node_Str"";
          }
          log.debug(message,e);
        }
      }
      if (!success) {
        final String msg=""String_Node_Str"" + localFile + ""String_Node_Str""+ tried+ ""String_Node_Str"";
        log.warn(msg);
        throw new IOFailure(msg);
      }
      log.debug(""String_Node_Str"" + ftpFileName + ""String_Node_Str"");
      if (useChecksums) {
        checksum=MD5.toHex(((DigestInputStream)in).getMessageDigest().digest());
        log.debug(""String_Node_Str"" + ftpFileName + ""String_Node_Str""+ checksum);
      }
 else {
        checksum=null;
      }
    }
  finally {
      try {
        if (in != null) {
          in.close();
        }
      }
 catch (      IOException e) {
        log.warn(""String_Node_Str"" + e);
      }
      logOut();
      log.debug(""String_Node_Str"");
    }
  }
  if (fileDeletable) {
    try {
      FileUtils.removeRecursively(localFile);
    }
 catch (    IOFailure e) {
      log.warn(""String_Node_Str"" + localFile,e);
    }
  }
}",0.9357037829167224
89711,"/** 
 * Create FTPClient and log on to ftp-server, if not already connected to ftp-server.  Attempts to set binary mode and passive mode.
 */
private void logOn(){
  if (currentFTPClient != null && currentFTPClient.isConnected()) {
    return;
  }
 else {
    currentFTPClient=new FTPClient();
  }
  log.trace(""String_Node_Str"" + ftpUserName + ""String_Node_Str""+ ftpUserPassword.replaceAll(""String_Node_Str"",""String_Node_Str"")+ ""String_Node_Str""+ ftpServerName+ ""String_Node_Str""+ ftpServerPort);
  try {
    currentFTPClient.connect(ftpServerName,ftpServerPort);
    if (!currentFTPClient.login(ftpUserName,ftpUserPassword)) {
      final String message=""String_Node_Str"" + SystemUtils.getLocalHostName() + ""String_Node_Str""+ ftpServerName+ ""String_Node_Str""+ ftpServerPort+ ""String_Node_Str""+ ftpUserName+ ""String_Node_Str""+ ftpUserPassword.replaceAll(""String_Node_Str"",""String_Node_Str"")+ ""String_Node_Str""+ getFtpErrorMessage();
      log.warn(message);
      throw new IOFailure(message);
    }
    if (!currentFTPClient.setFileType(FTPClient.BINARY_FILE_TYPE)) {
      final String message=""String_Node_Str"" + ftpServerName + ""String_Node_Str""+ getFtpErrorMessage();
      log.warn(message);
      throw new IOFailure(message);
    }
    currentFTPClient.enterLocalPassiveMode();
  }
 catch (  IOException e) {
    final String msg=""String_Node_Str"" + ftpServerName + ""String_Node_Str""+ SystemUtils.getLocalHostName()+ ""String_Node_Str"";
    log.warn(msg,e);
    throw new IOFailure(msg,e);
  }
  log.debug(""String_Node_Str"" + ftpUserName + ""String_Node_Str""+ ftpUserPassword.replaceAll(""String_Node_Str"",""String_Node_Str"")+ ""String_Node_Str""+ ftpServerName+ ""String_Node_Str""+ ftpServerPort);
}","/** 
 * Create FTPClient and log on to ftp-server, if not already connected to ftp-server.  Attempts to set binary mode and passive mode.
 */
private void logOn(){
  if (currentFTPClient != null && currentFTPClient.isConnected()) {
    return;
  }
 else {
    currentFTPClient=new FTPClient();
  }
  log.trace(""String_Node_Str"" + ftpUserName + ""String_Node_Str""+ ftpUserPassword.replaceAll(""String_Node_Str"",""String_Node_Str"")+ ""String_Node_Str""+ ftpServerName+ ""String_Node_Str""+ ftpServerPort);
  try {
    currentFTPClient.connect(ftpServerName,ftpServerPort);
    if (!currentFTPClient.login(ftpUserName,ftpUserPassword)) {
      final String message=""String_Node_Str"" + SystemUtils.getLocalHostName() + ""String_Node_Str""+ ftpServerName+ ""String_Node_Str""+ ftpServerPort+ ""String_Node_Str""+ ftpUserName+ ""String_Node_Str""+ ftpUserPassword.replaceAll(""String_Node_Str"",""String_Node_Str"")+ ""String_Node_Str""+ getFtpErrorMessage();
      log.warn(message);
      throw new IOFailure(message);
    }
    if (!currentFTPClient.setFileType(FTPClient.BINARY_FILE_TYPE)) {
      final String message=""String_Node_Str"" + ftpServerName + ""String_Node_Str""+ getFtpErrorMessage();
      log.warn(message);
      throw new IOFailure(message);
    }
    currentFTPClient.enterLocalPassiveMode();
    log.debug(""String_Node_Str"" + currentFTPClient.getDefaultTimeout());
    log.debug(""String_Node_Str"" + currentFTPClient.getConnectTimeout());
  }
 catch (  IOException e) {
    final String msg=""String_Node_Str"" + ftpServerName + ""String_Node_Str""+ SystemUtils.getLocalHostName()+ ""String_Node_Str"";
    log.warn(msg,e);
    throw new IOFailure(msg,e);
  }
  log.debug(""String_Node_Str"" + ftpUserName + ""String_Node_Str""+ ftpUserPassword.replaceAll(""String_Node_Str"",""String_Node_Str"")+ ""String_Node_Str""+ ftpServerName+ ""String_Node_Str""+ ftpServerPort);
}",0.9588500563697858
89712,"/** 
 * Write the contents of this ftp remote file to an output stream. Notice that while the checksum of the transferred data is checked, no retries are performed, and in case of failure, there is no guarantee that any data have been transferred.
 * @param out OutputStream that the data will be written to.  This streamwill not be closed by this operation.
 * @throws IOFailure If append operation fails
 */
public void appendTo(OutputStream out){
  ArgumentNotValid.checkNotNull(out,""String_Node_Str"");
  if (filesize == 0) {
    return;
  }
  try {
    logOn();
    if (useChecksums) {
      out=new DigestOutputStream(out,MD5.getMessageDigestInstance());
    }
    if (!currentFTPClient.retrieveFile(ftpFileName,out)) {
      final String msg=""String_Node_Str"" + ftpFileName + ""String_Node_Str""+ getFtpErrorMessage();
      log.warn(msg);
      throw new IOFailure(msg);
    }
    out.flush();
    if (useChecksums) {
      String newChecksum=MD5.toHex(((DigestOutputStream)out).getMessageDigest().digest());
      if (checksum != null && !checksum.equals(newChecksum)) {
        final String msg=""String_Node_Str"" + ftpFileName + ""String_Node_Str""+ checksum+ ""String_Node_Str""+ newChecksum;
        log.warn(msg);
        throw new IOFailure(msg);
      }
    }
  }
 catch (  IOException e) {
    final String msg=""String_Node_Str"" + ftpFileName + ""String_Node_Str"";
    log.warn(msg,e);
    throw new IOFailure(msg,e);
  }
 finally {
    logOut();
    if (!multipleDownloads) {
      cleanup();
    }
  }
}","/** 
 * Write the contents of this ftp remote file to an output stream. Notice that while the checksum of the transferred data is checked, no retries are performed, and in case of failure, there is no guarantee that any data have been transferred.
 * @param out OutputStream that the data will be written to.  This streamwill not be closed by this operation.
 * @throws IOFailure If append operation fails
 */
public void appendTo(OutputStream out){
  ArgumentNotValid.checkNotNull(out,""String_Node_Str"");
  if (filesize == 0) {
    return;
  }
  try {
    logOn();
    if (useChecksums) {
      out=new DigestOutputStream(out,MD5.getMessageDigestInstance());
    }
    if (!currentFTPClient.retrieveFile(ftpFileName,out)) {
      final String msg=""String_Node_Str"" + ftpFileName + ""String_Node_Str""+ getFtpErrorMessage();
      log.warn(msg);
      throw new IOFailure(msg);
    }
    out.flush();
    if (useChecksums) {
      String newChecksum=MD5.toHex(((DigestOutputStream)out).getMessageDigest().digest());
      if (checksum != null && !checksum.equals(newChecksum)) {
        final String msg=""String_Node_Str"" + ftpFileName + ""String_Node_Str""+ checksum+ ""String_Node_Str""+ newChecksum;
        log.warn(msg);
        throw new IOFailure(msg);
      }
    }
  }
 catch (  IOException e) {
    String msg=""String_Node_Str"" + ftpFileName + ""String_Node_Str"";
    if (e instanceof CopyStreamException) {
      CopyStreamException realException=(CopyStreamException)e;
      msg+=""String_Node_Str"" + realException.getIOException() + ""String_Node_Str"";
    }
    log.warn(msg,e);
    throw new IOFailure(msg,e);
  }
 finally {
    logOut();
    if (!multipleDownloads) {
      cleanup();
    }
  }
}",0.936897730805098
89713,"/** 
 * An implementation of the getInputStream operation that works with FTP. Notice that most of the special work (logging out and checking MD5) happens in the close() method of the returned InputStream, since that is the only place where we can know we're done.
 * @return An InputStream that will deliver the data transferred by FTP.Holding on to this for long periods without reading any data might cause a timeout.
 */
public InputStream getInputStream(){
  if (filesize == 0) {
    return new ByteArrayInputStream(new byte[]{});
  }
  try {
    logOn();
    InputStream in=currentFTPClient.retrieveFileStream(ftpFileName);
    if (in == null) {
      throw new IOFailure(""String_Node_Str"" + getFtpErrorMessage());
    }
    if (useChecksums) {
      in=new DigestInputStream(in,MD5.getMessageDigestInstance());
    }
    return new FilterInputStream(in){
      public void close() throws IOException {
        try {
          super.close();
          if (useChecksums) {
            String newChecksum=MD5.toHex(((DigestInputStream)in).getMessageDigest().digest());
            if (!newChecksum.equals(checksum)) {
              final String msg=""String_Node_Str"" + ftpFileName + ""String_Node_Str""+ checksum+ ""String_Node_Str""+ newChecksum;
              log.warn(msg);
              throw new IOFailure(msg);
            }
          }
        }
  finally {
          logOut();
          if (!multipleDownloads) {
            cleanup();
          }
        }
      }
    }
;
  }
 catch (  IOException e) {
    final String msg=""String_Node_Str"" + ftpFileName + ""String_Node_Str"";
    log.warn(msg,e);
    throw new IOFailure(msg,e);
  }
}","/** 
 * An implementation of the getInputStream operation that works with FTP. Notice that most of the special work (logging out and checking MD5) happens in the close() method of the returned InputStream, since that is the only place where we can know we're done.
 * @return An InputStream that will deliver the data transferred by FTP.Holding on to this for long periods without reading any data might cause a timeout.
 */
public InputStream getInputStream(){
  if (filesize == 0) {
    return new ByteArrayInputStream(new byte[]{});
  }
  try {
    logOn();
    InputStream in=currentFTPClient.retrieveFileStream(ftpFileName);
    if (in == null) {
      throw new IOFailure(""String_Node_Str"" + getFtpErrorMessage());
    }
    if (useChecksums) {
      in=new DigestInputStream(in,MD5.getMessageDigestInstance());
    }
    return new FilterInputStream(in){
      public void close() throws IOException {
        try {
          super.close();
          if (useChecksums) {
            String newChecksum=MD5.toHex(((DigestInputStream)in).getMessageDigest().digest());
            if (!newChecksum.equals(checksum)) {
              final String msg=""String_Node_Str"" + ftpFileName + ""String_Node_Str""+ checksum+ ""String_Node_Str""+ newChecksum;
              log.warn(msg);
              throw new IOFailure(msg);
            }
          }
        }
  finally {
          logOut();
          if (!multipleDownloads) {
            cleanup();
          }
        }
      }
    }
;
  }
 catch (  IOException e) {
    String msg=""String_Node_Str"" + ftpFileName + ""String_Node_Str"";
    if (e instanceof CopyStreamException) {
      CopyStreamException realException=(CopyStreamException)e;
      msg+=""String_Node_Str"" + realException.getIOException() + ""String_Node_Str"";
    }
    log.warn(msg,e);
    throw new IOFailure(msg,e);
  }
}",0.941683424303361
89714,"/** 
 * Log out from the FTP server.
 * @throws IOFailure if disconnecting fails.
 */
private void logOut(){
  try {
    if (currentFTPClient != null) {
      currentFTPClient.disconnect();
    }
  }
 catch (  IOException e) {
    final String msg=""String_Node_Str"" + ftpServerName + ""String_Node_Str"";
    log.warn(msg,e);
    throw new IOFailure(msg,e);
  }
catch (  NullPointerException e) {
  }
}","/** 
 * Log out from the FTP server.
 * @throws IOFailure if disconnecting fails.
 */
private void logOut(){
  try {
    if (currentFTPClient != null) {
      currentFTPClient.disconnect();
    }
  }
 catch (  IOException e) {
    String msg=""String_Node_Str"" + ftpServerName + ""String_Node_Str"";
    if (e instanceof CopyStreamException) {
      CopyStreamException realException=(CopyStreamException)e;
      msg+=""String_Node_Str"" + realException.getIOException() + ""String_Node_Str"";
    }
    log.warn(msg,e);
    throw new IOFailure(msg,e);
  }
catch (  NullPointerException e) {
  }
}",0.7951564076690212
89715,"/** 
 * Creates the operation system specific starting script for this machine. pseudocode: - ssh 'login'@'machine' cmd /c 'environmentName'\\conf\\startall.bat variables: 'login' = machine user name 'machine' = name of the machine 'environmentName' = the environmentName from configuration.
 * @return Operation system specific part of the startscript.
 */
@Override protected String osStartScript(){
  StringBuilder res=new StringBuilder();
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + ScriptConstants.WINDOWS_COMMAND_RUN+ Constants.SPACE+ Constants.SPACE);
  res.append(getLocalConfDirPath());
  res.append(Constants.SCRIPT_NAME_START_ALL);
  res.append(scriptExtension);
  res.append(Constants.SPACE + Constants.QUOTE_MARK + Constants.SPACE);
  res.append(Constants.NEWLINE);
  return res.toString();
}","/** 
 * Creates the operation system specific starting script for this machine. pseudocode: - ssh 'login'@'machine' cmd /c 'environmentName'\\conf\\startall.bat - sleep 5 - ssh 'login'@'machine' ""more 'environmentName'\\start_APP.log variables: 'login' = machine user name 'machine' = name of the machine 'environmentName' = the environmentName from configuration.
 * @return Operation system specific part of the startscript.
 */
@Override protected String osStartScript(){
  StringBuilder res=new StringBuilder();
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + ScriptConstants.WINDOWS_COMMAND_RUN+ Constants.SPACE+ Constants.SPACE);
  res.append(getLocalConfDirPath());
  res.append(Constants.SCRIPT_NAME_START_ALL);
  res.append(scriptExtension);
  res.append(Constants.SPACE + Constants.QUOTE_MARK + Constants.SPACE);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SLEEP_5);
  res.append(Constants.NEWLINE);
  for (  Application app : applications) {
    res.append(ScriptConstants.SSH + Constants.SPACE);
    res.append(machineUserLogin());
    res.append(Constants.SPACE + Constants.QUOTE_MARK + ScriptConstants.WINDOWS_COMMAND_MORE+ Constants.SPACE);
    res.append(getEnvironmentName() + Constants.BACKSLASH + Constants.SCRIPT_NAME_LOCAL_START+ app.getIdentification()+ Constants.EXTENSION_LOG_FILES);
    res.append(Constants.QUOTE_MARK + Constants.SPACE);
    res.append(Constants.NEWLINE);
  }
  return res.toString();
}",0.7387387387387387
89716,"/** 
 * Receives replies from a message queue and triggers the blocked call in sendAndWaitForOneReply().
 * @param msg an ObjectMessage containing a NetarkivetMessage.
 */
public void onMessage(Message msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  NetarkivetMessage naMsg=JMSConnection.unpack(msg);
  NetarkivetMessage requestMsg;
synchronized (requests) {
    requestMsg=requests.get(naMsg.getReplyOfId());
  }
  if (requestMsg != null) {
synchronized (requestMsg) {
      replies.put(naMsg.getReplyOfId(),naMsg);
      requestMsg.notifyAll();
    }
  }
 else {
    log.warn(""String_Node_Str"" + naMsg.getReplyOfId() + ""String_Node_Str"");
  }
}","/** 
 * Receives replies from a message queue and triggers the blocked call in sendAndWaitForOneReply().
 * @param msg an ObjectMessage containing a NetarkivetMessage.
 */
public void onMessage(Message msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  NetarkivetMessage naMsg=JMSConnection.unpack(msg);
  NetarkivetMessage requestMsg;
synchronized (requests) {
    requestMsg=requests.get(naMsg.getReplyOfId());
  }
  if (requestMsg != null) {
synchronized (requestMsg) {
      replies.put(naMsg.getReplyOfId(),naMsg);
      requestMsg.notifyAll();
    }
  }
 else {
    log.warn(""String_Node_Str"" + naMsg.getReplyOfId() + ""String_Node_Str""+ naMsg.getClass().getName()+ ""String_Node_Str""+ naMsg.toString());
  }
}",0.953203743700504
89717,"/** 
 * Read a single job from the job database.
 * @param jobID ID of the job.
 * @return A Job object
 * @throws UnknownID if the job id does not exist.
 * @throws IOFailure if there was some problem talking to the database.
 */
public synchronized Job read(Long jobID){
  if (!exists(jobID)) {
    throw new UnknownID(""String_Node_Str"" + jobID + ""String_Node_Str"");
  }
  Connection dbconnection=DBConnect.getDBConnection();
  PreparedStatement statement=null;
  try {
    statement=dbconnection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    statement.setLong(1,jobID);
    ResultSet result=statement.executeQuery();
    result.next();
    long harvestID=result.getLong(1);
    JobStatus status=JobStatus.fromOrdinal(result.getInt(2));
    JobPriority pri=JobPriority.fromOrdinal(result.getInt(3));
    long forceMaxCount=result.getLong(4);
    long forceMaxBytes=result.getLong(5);
    String orderxml=result.getString(6);
    Document orderXMLdoc=null;
    boolean useClobs=DBSpecifics.getInstance().supportsClob();
    if (useClobs) {
      Clob clob=result.getClob(7);
      orderXMLdoc=getOrderXMLdocFromClob(clob);
    }
 else {
      orderXMLdoc=XmlUtils.documentFromString(result.getString(7));
    }
    String seedlist=""String_Node_Str"";
    if (useClobs) {
      Clob clob=result.getClob(8);
      seedlist=clob.getSubString(1,(int)clob.length());
    }
 else {
      seedlist=result.getString(8);
    }
    int harvestNum=result.getInt(9);
    String harvestErrors=result.getString(10);
    String harvestErrorDetails=result.getString(11);
    String uploadErrors=result.getString(12);
    String uploadErrorDetails=result.getString(13);
    Date startdate=DBUtils.getDateMaybeNull(result,14);
    Date stopdate=DBUtils.getDateMaybeNull(result,15);
    Date submittedDate=DBUtils.getDateMaybeNull(result,16);
    Long edition=result.getLong(17);
    Long resubmittedAsJob=DBUtils.getLongMaybeNull(result,18);
    statement.close();
    String domainStatement=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
    if (Settings.get(CommonSettings.DB_SPECIFICS_CLASS).contains(CommonSettings.DB_IS_DERBY_IF_CONTAINS)) {
      statement=dbconnection.prepareStatement(domainStatement + ""String_Node_Str"");
    }
 else {
      statement=dbconnection.prepareStatement(domainStatement);
    }
    statement.setLong(1,jobID);
    result=statement.executeQuery();
    Map<String,String> configurationMap=new HashMap<String,String>();
    while (result.next()) {
      String domainName=result.getString(1);
      String configName=result.getString(2);
      configurationMap.put(domainName,configName);
    }
    final Job job=new Job(harvestID,configurationMap,pri,forceMaxCount,forceMaxBytes,status,orderxml,orderXMLdoc,seedlist,harvestNum);
    job.appendHarvestErrors(harvestErrors);
    job.appendHarvestErrorDetails(harvestErrorDetails);
    job.appendUploadErrors(uploadErrors);
    job.appendUploadErrorDetails(uploadErrorDetails);
    if (startdate != null) {
      job.setActualStart(startdate);
    }
    if (stopdate != null) {
      job.setActualStop(stopdate);
    }
    if (submittedDate != null) {
      job.setSubmittedDate(submittedDate);
    }
    job.configsChanged=false;
    job.setJobID(jobID);
    job.setEdition(edition);
    if (resubmittedAsJob != null) {
      job.setResubmittedAsJob(resubmittedAsJob);
    }
    return job;
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + jobID + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
catch (  DocumentException e) {
    String message=""String_Node_Str"" + jobID + ""String_Node_Str"";
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.closeStatementIfOpen(statement);
  }
}","/** 
 * Read a single job from the job database.
 * @param jobID ID of the job.
 * @return A Job object
 * @throws UnknownID if the job id does not exist.
 * @throws IOFailure if there was some problem talking to the database.
 */
@Override public synchronized Job read(Long jobID){
  if (!exists(jobID)) {
    throw new UnknownID(""String_Node_Str"" + jobID + ""String_Node_Str"");
  }
  Connection dbconnection=DBConnect.getDBConnection();
  PreparedStatement statement=null;
  try {
    statement=dbconnection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    statement.setLong(1,jobID);
    ResultSet result=statement.executeQuery();
    result.next();
    long harvestID=result.getLong(1);
    JobStatus status=JobStatus.fromOrdinal(result.getInt(2));
    JobPriority pri=JobPriority.fromOrdinal(result.getInt(3));
    long forceMaxCount=result.getLong(4);
    long forceMaxBytes=result.getLong(5);
    String orderxml=result.getString(6);
    Document orderXMLdoc=null;
    boolean useClobs=DBSpecifics.getInstance().supportsClob();
    if (useClobs) {
      Clob clob=result.getClob(7);
      orderXMLdoc=getOrderXMLdocFromClob(clob);
    }
 else {
      orderXMLdoc=XmlUtils.documentFromString(result.getString(7));
    }
    String seedlist=""String_Node_Str"";
    if (useClobs) {
      Clob clob=result.getClob(8);
      seedlist=clob.getSubString(1,(int)clob.length());
    }
 else {
      seedlist=result.getString(8);
    }
    int harvestNum=result.getInt(9);
    String harvestErrors=result.getString(10);
    String harvestErrorDetails=result.getString(11);
    String uploadErrors=result.getString(12);
    String uploadErrorDetails=result.getString(13);
    Date startdate=DBUtils.getDateMaybeNull(result,14);
    Date stopdate=DBUtils.getDateMaybeNull(result,15);
    Date submittedDate=DBUtils.getDateMaybeNull(result,16);
    Long edition=result.getLong(17);
    Long resubmittedAsJob=DBUtils.getLongMaybeNull(result,18);
    statement.close();
    String domainStatement=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
    if (Settings.get(CommonSettings.DB_SPECIFICS_CLASS).contains(CommonSettings.DB_IS_DERBY_IF_CONTAINS)) {
      statement=dbconnection.prepareStatement(domainStatement + ""String_Node_Str"");
    }
 else {
      statement=dbconnection.prepareStatement(domainStatement);
    }
    statement.setLong(1,jobID);
    result=statement.executeQuery();
    Map<String,String> configurationMap=new HashMap<String,String>();
    while (result.next()) {
      String domainName=result.getString(1);
      String configName=result.getString(2);
      configurationMap.put(domainName,configName);
    }
    final Job job=new Job(harvestID,configurationMap,pri,forceMaxCount,forceMaxBytes,status,orderxml,orderXMLdoc,seedlist,harvestNum);
    job.appendHarvestErrors(harvestErrors);
    job.appendHarvestErrorDetails(harvestErrorDetails);
    job.appendUploadErrors(uploadErrors);
    job.appendUploadErrorDetails(uploadErrorDetails);
    if (startdate != null) {
      job.setActualStart(startdate);
    }
    if (stopdate != null) {
      job.setActualStop(stopdate);
    }
    if (submittedDate != null) {
      job.setSubmittedDate(submittedDate);
    }
    job.configsChanged=false;
    job.setJobID(jobID);
    job.setEdition(edition);
    if (resubmittedAsJob != null) {
      job.setResubmittedAsJob(resubmittedAsJob);
    }
    return job;
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + jobID + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
catch (  DocumentException e) {
    String message=""String_Node_Str"" + jobID + ""String_Node_Str"";
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.closeStatementIfOpen(statement);
  }
}",0.9987518721917124
89718,"/** 
 * Return a list of all jobs.
 * @return A list of all jobs
 */
public synchronized Iterator<Job> getAll(){
  List<Long> idList=DBUtils.selectLongList(DBConnect.getDBConnection(),""String_Node_Str"");
  return new FilterIterator<Long,Job>(idList.iterator()){
    public Job filter(    Long aLong){
      return read(aLong);
    }
  }
;
}","/** 
 * Return a list of all jobs.
 * @return A list of all jobs
 */
@Override public synchronized Iterator<Job> getAll(){
  List<Long> idList=DBUtils.selectLongList(DBConnect.getDBConnection(),""String_Node_Str"");
  return new FilterIterator<Long,Job>(idList.iterator()){
    public Job filter(    Long aLong){
      return read(aLong);
    }
  }
;
}",0.9855072463768116
89719,"/** 
 * Update a Job in persistent storage.
 * @param job The Job to update
 * @throws ArgumentNotValid If the Job is null
 * @throws UnknownID If the Job doesn't exist in the DAO
 * @throws IOFailure If writing the job to persistent storage fails
 * @throws PermissionDenied If the job has been updated behind our backs
 */
public synchronized void update(Job job){
  ArgumentNotValid.checkNotNull(job,""String_Node_Str"");
  final Long jobID=job.getJobID();
  if (!exists(jobID)) {
    throw new UnknownID(""String_Node_Str"" + jobID + ""String_Node_Str"");
  }
  Connection dbconnection=DBConnect.getDBConnection();
  PreparedStatement statement=null;
  try {
    dbconnection.setAutoCommit(false);
    statement=dbconnection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    statement.setLong(1,job.getOrigHarvestDefinitionID());
    statement.setInt(2,job.getStatus().ordinal());
    statement.setInt(3,job.getPriority().ordinal());
    statement.setLong(4,job.getForceMaxObjectsPerDomain());
    statement.setLong(5,job.getMaxBytesPerDomain());
    DBUtils.setStringMaxLength(statement,6,job.getOrderXMLName(),Constants.MAX_NAME_SIZE,job,""String_Node_Str"");
    final String orderreader=job.getOrderXMLdoc().asXML();
    DBUtils.setClobMaxLength(statement,7,orderreader,Constants.MAX_ORDERXML_SIZE,job,""String_Node_Str"");
    DBUtils.setClobMaxLength(statement,8,job.getSeedListAsString(),Constants.MAX_COMBINED_SEED_LIST_SIZE,job,""String_Node_Str"");
    statement.setInt(9,job.getHarvestNum());
    DBUtils.setStringMaxLength(statement,10,job.getHarvestErrors(),Constants.MAX_ERROR_SIZE,job,""String_Node_Str"");
    DBUtils.setStringMaxLength(statement,11,job.getHarvestErrorDetails(),Constants.MAX_ERROR_DETAIL_SIZE,job,""String_Node_Str"");
    DBUtils.setStringMaxLength(statement,12,job.getUploadErrors(),Constants.MAX_ERROR_SIZE,job,""String_Node_Str"");
    DBUtils.setStringMaxLength(statement,13,job.getUploadErrorDetails(),Constants.MAX_ERROR_DETAIL_SIZE,job,""String_Node_Str"");
    long edition=job.getEdition() + 1;
    DBUtils.setDateMaybeNull(statement,14,job.getActualStart());
    DBUtils.setDateMaybeNull(statement,15,job.getActualStop());
    statement.setInt(16,job.getDomainConfigurationMap().size());
    statement.setLong(17,edition);
    DBUtils.setDateMaybeNull(statement,18,job.getSubmittedDate());
    DBUtils.setLongMaybeNull(statement,19,job.getResubmittedAsJob());
    statement.setLong(20,job.getJobID());
    statement.setLong(21,job.getEdition());
    final int rows=statement.executeUpdate();
    if (rows == 0) {
      String message=""String_Node_Str"" + job.getEdition() + ""String_Node_Str"";
      log.debug(message);
      throw new PermissionDenied(message);
    }
    createJobConfigsEntries(dbconnection,job);
    dbconnection.commit();
    job.setEdition(edition);
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + job + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.rollbackIfNeeded(dbconnection,""String_Node_Str"",job);
    DBUtils.closeStatementIfOpen(statement);
  }
}","/** 
 * Update a Job in persistent storage.
 * @param job The Job to update
 * @throws ArgumentNotValid If the Job is null
 * @throws UnknownID If the Job doesn't exist in the DAO
 * @throws IOFailure If writing the job to persistent storage fails
 * @throws PermissionDenied If the job has been updated behind our backs
 */
@Override public synchronized void update(Job job){
  ArgumentNotValid.checkNotNull(job,""String_Node_Str"");
  final Long jobID=job.getJobID();
  if (!exists(jobID)) {
    throw new UnknownID(""String_Node_Str"" + jobID + ""String_Node_Str"");
  }
  Connection dbconnection=DBConnect.getDBConnection();
  PreparedStatement statement=null;
  try {
    dbconnection.setAutoCommit(false);
    statement=dbconnection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    statement.setLong(1,job.getOrigHarvestDefinitionID());
    statement.setInt(2,job.getStatus().ordinal());
    statement.setInt(3,job.getPriority().ordinal());
    statement.setLong(4,job.getForceMaxObjectsPerDomain());
    statement.setLong(5,job.getMaxBytesPerDomain());
    DBUtils.setStringMaxLength(statement,6,job.getOrderXMLName(),Constants.MAX_NAME_SIZE,job,""String_Node_Str"");
    final String orderreader=job.getOrderXMLdoc().asXML();
    DBUtils.setClobMaxLength(statement,7,orderreader,Constants.MAX_ORDERXML_SIZE,job,""String_Node_Str"");
    DBUtils.setClobMaxLength(statement,8,job.getSeedListAsString(),Constants.MAX_COMBINED_SEED_LIST_SIZE,job,""String_Node_Str"");
    statement.setInt(9,job.getHarvestNum());
    DBUtils.setStringMaxLength(statement,10,job.getHarvestErrors(),Constants.MAX_ERROR_SIZE,job,""String_Node_Str"");
    DBUtils.setStringMaxLength(statement,11,job.getHarvestErrorDetails(),Constants.MAX_ERROR_DETAIL_SIZE,job,""String_Node_Str"");
    DBUtils.setStringMaxLength(statement,12,job.getUploadErrors(),Constants.MAX_ERROR_SIZE,job,""String_Node_Str"");
    DBUtils.setStringMaxLength(statement,13,job.getUploadErrorDetails(),Constants.MAX_ERROR_DETAIL_SIZE,job,""String_Node_Str"");
    long edition=job.getEdition() + 1;
    DBUtils.setDateMaybeNull(statement,14,job.getActualStart());
    DBUtils.setDateMaybeNull(statement,15,job.getActualStop());
    statement.setInt(16,job.getDomainConfigurationMap().size());
    statement.setLong(17,edition);
    DBUtils.setDateMaybeNull(statement,18,job.getSubmittedDate());
    DBUtils.setLongMaybeNull(statement,19,job.getResubmittedAsJob());
    statement.setLong(20,job.getJobID());
    statement.setLong(21,job.getEdition());
    final int rows=statement.executeUpdate();
    if (rows == 0) {
      String message=""String_Node_Str"" + job.getEdition() + ""String_Node_Str"";
      log.debug(message);
      throw new PermissionDenied(message);
    }
    createJobConfigsEntries(dbconnection,job);
    dbconnection.commit();
    job.setEdition(edition);
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + job + ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.rollbackIfNeeded(dbconnection,""String_Node_Str"",job);
    DBUtils.closeStatementIfOpen(statement);
  }
}",0.998500299940012
89720,"/** 
 * Get statusInfo.
 * @see JobDAO#getStatusInfo(long,long,boolean,Set)
 */
public List<JobStatusInfo> getStatusInfo(long harvestId,long harvestNum,boolean asc,Set<JobStatus> selectedStatusSet){
  ArgumentNotValid.checkNotNegative(harvestId,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(harvestNum,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(selectedStatusSet,""String_Node_Str"");
  String ascdescString=(asc) ? HarvestStatusQuery.SORT_ORDER.ASC.name() : HarvestStatusQuery.SORT_ORDER.DESC.name();
  StringBuffer statusSortBuffer=new StringBuffer();
  boolean selectAllJobStates=(selectedStatusSet.size() == JobStatus.values().length);
  if (!selectAllJobStates) {
    if (selectedStatusSet.size() == 1) {
      int theWantedStatus=selectedStatusSet.iterator().next().ordinal();
      statusSortBuffer.append(""String_Node_Str"").append(theWantedStatus);
    }
 else {
      Iterator<JobStatus> it=selectedStatusSet.iterator();
      int nextInt=it.next().ordinal();
      StringBuffer res=new StringBuffer(""String_Node_Str"" + nextInt);
      while (it.hasNext()) {
        nextInt=it.next().ordinal();
        res.append(""String_Node_Str"").append(nextInt);
      }
      res.append(""String_Node_Str"");
      statusSortBuffer.append(res);
    }
  }
  Connection dbconnection=DBConnect.getDBConnection();
  PreparedStatement statement=null;
  try {
    statement=dbconnection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"" + harvestId + ""String_Node_Str""+ harvestNum+ statusSortBuffer.toString()+ ""String_Node_Str""+ ascdescString);
    ResultSet res=statement.executeQuery();
    return makeJobStatusInfoListFromResultset(res);
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.closeStatementIfOpen(statement);
  }
}","/** 
 * Get a list of small and immediately usable status information for given job status and in given job id order.
 * @param query the user query
 * @throws IOFailure on trouble getting data from database
 */
@Override public HarvestStatus getStatusInfo(HarvestStatusQuery query){
  log.debug(""String_Node_Str"");
  PreparedStatement s=null;
  long totalRowsCount=0;
  try {
    s=buildSqlQuery(query,true).getPopulatedStatement();
    ResultSet res=s.executeQuery();
    res.next();
    totalRowsCount=res.getLong(1);
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.closeStatementIfOpen(s);
  }
  List<JobStatusInfo> jobs=null;
  try {
    s=buildSqlQuery(query,false).getPopulatedStatement();
    ResultSet res=s.executeQuery();
    jobs=makeJobStatusInfoListFromResultset(res);
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.closeStatementIfOpen(s);
  }
  log.debug(""String_Node_Str"");
  return new HarvestStatus(totalRowsCount,jobs);
}",0.2019405700424499
89721,"/** 
 * Returns the number of existing jobs.
 * @return Number of jobs in 'jobs' table
 */
public synchronized int getCountJobs(){
  return DBUtils.selectIntValue(DBConnect.getDBConnection(),""String_Node_Str"");
}","/** 
 * Returns the number of existing jobs.
 * @return Number of jobs in 'jobs' table
 */
@Override public synchronized int getCountJobs(){
  return DBUtils.selectIntValue(DBConnect.getDBConnection(),""String_Node_Str"");
}",0.976958525345622
89722,"/** 
 * Check whether a particular job exists.
 * @param jobID Id of the job.
 * @return true if the job exists in any state.
 */
public synchronized boolean exists(Long jobID){
  ArgumentNotValid.checkNotNull(jobID,""String_Node_Str"");
  return 1 == DBUtils.selectLongValue(DBConnect.getDBConnection(),""String_Node_Str"",jobID);
}","/** 
 * Check whether a particular job exists.
 * @param jobID Id of the job.
 * @return true if the job exists in any state.
 */
@Override public synchronized boolean exists(Long jobID){
  ArgumentNotValid.checkNotNull(jobID,""String_Node_Str"");
  return 1 == DBUtils.selectLongValue(DBConnect.getDBConnection(),""String_Node_Str"",jobID);
}",0.9850299401197604
89723,"/** 
 * @see JobDAO#rescheduleJob(long)
 */
public synchronized long rescheduleJob(long oldJobID){
  Connection dbconnection=DBConnect.getDBConnection();
  long newJobID=generateNextID();
  PreparedStatement statement=null;
  try {
    statement=dbconnection.prepareStatement(""String_Node_Str"");
    statement.setLong(1,oldJobID);
    ResultSet res=statement.executeQuery();
    if (!res.next()) {
      throw new UnknownID(""String_Node_Str"" + oldJobID + ""String_Node_Str"");
    }
    final JobStatus currentJobStatus=JobStatus.fromOrdinal(res.getInt(1));
    if (currentJobStatus != JobStatus.SUBMITTED && currentJobStatus != JobStatus.FAILED) {
      throw new IllegalState(""String_Node_Str"" + oldJobID + ""String_Node_Str"");
    }
    dbconnection.setAutoCommit(false);
    statement=dbconnection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    statement.setLong(1,newJobID);
    statement.setLong(2,JobStatus.NEW.ordinal());
    long initialEdition=1;
    statement.setLong(3,initialEdition);
    statement.setLong(4,oldJobID);
    statement.executeUpdate();
    statement=dbconnection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    statement.setLong(1,newJobID);
    statement.setLong(2,oldJobID);
    statement.executeUpdate();
    statement=dbconnection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"");
    statement.setInt(1,JobStatus.RESUBMITTED.ordinal());
    statement.setLong(2,newJobID);
    statement.setLong(3,oldJobID);
    statement.executeUpdate();
    dbconnection.commit();
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.closeStatementIfOpen(statement);
    DBUtils.rollbackIfNeeded(dbconnection,""String_Node_Str"",oldJobID);
  }
  return newJobID;
}","/** 
 * @see JobDAO#rescheduleJob(long)
 */
@Override public synchronized long rescheduleJob(long oldJobID){
  Connection dbconnection=DBConnect.getDBConnection();
  long newJobID=generateNextID();
  PreparedStatement statement=null;
  try {
    statement=dbconnection.prepareStatement(""String_Node_Str"");
    statement.setLong(1,oldJobID);
    ResultSet res=statement.executeQuery();
    if (!res.next()) {
      throw new UnknownID(""String_Node_Str"" + oldJobID + ""String_Node_Str"");
    }
    final JobStatus currentJobStatus=JobStatus.fromOrdinal(res.getInt(1));
    if (currentJobStatus != JobStatus.SUBMITTED && currentJobStatus != JobStatus.FAILED) {
      throw new IllegalState(""String_Node_Str"" + oldJobID + ""String_Node_Str"");
    }
    dbconnection.setAutoCommit(false);
    statement=dbconnection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    statement.setLong(1,newJobID);
    statement.setLong(2,JobStatus.NEW.ordinal());
    long initialEdition=1;
    statement.setLong(3,initialEdition);
    statement.setLong(4,oldJobID);
    statement.executeUpdate();
    statement=dbconnection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    statement.setLong(1,newJobID);
    statement.setLong(2,oldJobID);
    statement.executeUpdate();
    statement=dbconnection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"");
    statement.setInt(1,JobStatus.RESUBMITTED.ordinal());
    statement.setLong(2,newJobID);
    statement.setLong(3,oldJobID);
    statement.executeUpdate();
    dbconnection.commit();
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.closeStatementIfOpen(statement);
    DBUtils.rollbackIfNeeded(dbconnection,""String_Node_Str"",oldJobID);
  }
  return newJobID;
}",0.9976099426386232
89724,"/** 
 * Initialise the status on a fresh batch request. Apart from the given values, a file is created to store batch results in. <b>Sideeffect</b>: BatchTimeout is started here
 * @param originalRequestID      The ID of the originating request.
 * @param originalRequestReplyTo The reply channel for the originatingrequest.
 * @param bitarchiveBatchID      The ID of the job sent to bitarchives.
 * @param missingRespondents     List of all live bitarchives, used toknow which bitarchives to await reply from.
 * @param timeout                Timeout for Batch job
 * @throws IOFailure if a file for batch results cannot be made.
 */
private BatchJobStatus(String originalRequestID,ChannelID originalRequestReplyTo,String bitarchiveBatchID,Set<String> missingRespondents,long timeout) throws IOFailure {
  this.originalRequestID=originalRequestID;
  this.originalRequestReplyTo=originalRequestReplyTo;
  this.bitarchiveBatchID=bitarchiveBatchID;
  this.missingRespondents=missingRespondents;
  batchTimeoutTask=new BatchTimeoutTask(bitarchiveBatchID);
  batchTimeout=timeout;
  Timer timer=new Timer(true);
  timer.schedule(batchTimeoutTask,batchTimeout);
  this.noOfFilesProcessed=0;
  try {
    this.batchResultFile=File.createTempFile(bitarchiveBatchID,""String_Node_Str"",FileUtils.getTempDir());
  }
 catch (  IOException e) {
    final String errMsg=""String_Node_Str"";
    log.warn(errMsg);
    throw new IOFailure(errMsg,e);
  }
  this.filesFailed=new ArrayList<File>();
  this.errorMessages=null;
  this.notifyInitiated=false;
  exceptions=new ArrayList<FileBatchJob.ExceptionOccurrence>();
}","/** 
 * Initialise the status on a fresh batch request. Apart from the given values, a file is created to store batch results in. <b>Sideeffect</b>: BatchTimeout is started here
 * @param originalRequestID      The ID of the originating request.
 * @param originalRequestReplyTo The reply channel for the originatingrequest.
 * @param bitarchiveBatchID      The ID of the job sent to bitarchives.
 * @param missingRespondents     List of all live bitarchives, used toknow which bitarchives to await reply from.
 * @param timeout                Timeout for Batch job
 * @throws IOFailure if a file for batch results cannot be made.
 */
private BatchJobStatus(String originalRequestID,ChannelID originalRequestReplyTo,String bitarchiveBatchID,Set<String> missingRespondents,long timeout) throws IOFailure {
  this.originalRequestID=originalRequestID;
  this.originalRequestReplyTo=originalRequestReplyTo;
  this.bitarchiveBatchID=bitarchiveBatchID;
  this.missingRespondents=missingRespondents;
  batchTimeoutTask=new BatchTimeoutTask(bitarchiveBatchID);
  batchTimeout=timeout;
  batchTimer.schedule(batchTimeoutTask,batchTimeout);
  this.noOfFilesProcessed=0;
  try {
    this.batchResultFile=File.createTempFile(bitarchiveBatchID,""String_Node_Str"",FileUtils.getTempDir());
  }
 catch (  IOException e) {
    final String errMsg=""String_Node_Str"";
    log.warn(errMsg);
    throw new IOFailure(errMsg,e);
  }
  this.filesFailed=new ArrayList<File>();
  this.errorMessages=null;
  this.notifyInitiated=false;
  exceptions=new ArrayList<FileBatchJob.ExceptionOccurrence>();
}",0.9880201765447668
89725,"/** 
 * Schedule all jobs ready for execution and perform backup if required. 
 */
synchronized void dispatchJobs(){
  stopTimeoutJobs();
  submitNewJobs();
}","/** 
 * Dispatched new jobs Stop jobs with status STARTED, which have been on running for more  than settings.harvester.scheduler.jobtimeouttime time.
 */
void dispatchJobs(){
  stopTimeoutJobs();
  submitNewJobs();
}",0.4586666666666666
89726,"/** 
 * Submit those jobs that are ready for submission if the relevant message queue is empty.
 */
synchronized void submitNewJobs(){
  final JobDAO dao=JobDAO.getInstance();
  Iterator<Long> jobsToSubmit=dao.getAllJobIds(JobStatus.NEW);
  if (!jobsToSubmit.hasNext()) {
    log.trace(""String_Node_Str"");
  }
 else {
    log.trace(""String_Node_Str"");
  }
  int numberOfSubmittedJobs=0;
  while (jobsToSubmit.hasNext()) {
    final long jobID=jobsToSubmit.next();
    Job jobToSubmit=null;
    try {
      jobToSubmit=dao.read(jobID);
      if (isQueueEmpty(JobChannelUtil.getChannel(jobToSubmit.getPriority()))) {
        jobToSubmit.setStatus(JobStatus.SUBMITTED);
        jobToSubmit.setSubmittedDate(new Date());
        dao.update(jobToSubmit);
        List<MetadataEntry> metadata=new ArrayList<MetadataEntry>();
        MetadataEntry aliasMetadataEntry=MetadataEntry.makeAliasMetadataEntry(jobToSubmit.getJobAliasInfo(),jobToSubmit.getOrigHarvestDefinitionID(),jobToSubmit.getHarvestNum(),jobToSubmit.getJobID());
        if (aliasMetadataEntry != null) {
          metadata.add(aliasMetadataEntry);
        }
        if (HeritrixLauncher.isDeduplicationEnabledInTemplate(jobToSubmit.getOrderXMLdoc())) {
          MetadataEntry duplicateReductionMetadataEntry=MetadataEntry.makeDuplicateReductionMetadataEntry(dao.getJobIDsForDuplicateReduction(jobID),jobToSubmit.getOrigHarvestDefinitionID(),jobToSubmit.getHarvestNum(),jobToSubmit.getJobID());
          if (duplicateReductionMetadataEntry != null) {
            metadata.add(duplicateReductionMetadataEntry);
          }
        }
        doOneCrawl(jobToSubmit,metadata);
        numberOfSubmittedJobs++;
        log.trace(""String_Node_Str"" + jobToSubmit + ""String_Node_Str"");
      }
    }
 catch (    Throwable e) {
      String message=""String_Node_Str"" + jobID;
      log.warn(message,e);
      if (jobToSubmit != null) {
        jobToSubmit.setStatus(JobStatus.FAILED);
        jobToSubmit.appendHarvestErrors(message);
        jobToSubmit.appendHarvestErrorDetails(ExceptionUtils.getStackTrace(e));
        dao.update(jobToSubmit);
      }
    }
  }
  if (numberOfSubmittedJobs > 0) {
    log.info(""String_Node_Str"" + numberOfSubmittedJobs + ""String_Node_Str"");
  }
}","/** 
 * Submit the next new job if the relevant message queue is empty. 
 */
synchronized void submitNewJobs(){
  try {
    for (    JobPriority priority : JobPriority.values()) {
      if (isQueueEmpty(JobChannelUtil.getChannel(priority))) {
        submitNextNewJob(priority);
      }
    }
  }
 catch (  JMSException e) {
    log.error(""String_Node_Str"",e);
  }
}",0.113033448673587
89727,"/** 
 * Test that runNewJobs makes correct duplication reduction information.
 * @throws Exception if HarvestScheduler throws exception
 */
public void testSubmitNewJobsMakesDuplicateReductionInfo() throws Exception {
  clearNewJobs();
  DataModelTestCase.createTestJobs(2L,15L);
  TestMessageListener hacoListener=new TestMessageListener();
  JMSConnectionMockupMQ.getInstance().setListener(JobChannelUtil.getChannel(JobPriority.HIGHPRIORITY),hacoListener);
  JMSConnectionMockupMQ.getInstance().setListener(JobChannelUtil.getChannel(JobPriority.LOWPRIORITY),hacoListener);
  submitNewJobs();
  ((JMSConnectionMockupMQ)JMSConnectionMockupMQ.getInstance()).waitForConcurrentTasksToFinish();
  assertEquals(""String_Node_Str"",14,hacoListener.getNumReceived());
  DoOneCrawlMessage crawlMessage=(DoOneCrawlMessage)hacoListener.getReceived();
  assertEquals(""String_Node_Str"",1,crawlMessage.getMetadata().size());
  MetadataEntry metadataEntry=crawlMessage.getMetadata().get(0);
  assertNotNull(""String_Node_Str"",metadataEntry);
  assertEquals(""String_Node_Str"",""String_Node_Str"",metadataEntry.getMimeType());
  assertEquals(""String_Node_Str"",""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",metadataEntry.getURL());
  assertEquals(""String_Node_Str"",""String_Node_Str"",new String(metadataEntry.getData()));
}","/** 
 * Test that runNewJobs makes correct duplication reduction information.
 * @throws Exception If HarvestScheduler throws exception
 */
public void testSubmitNewJobsMakesDuplicateReductionInfo() throws Exception {
  clearNewJobs();
  DataModelTestCase.createTestJobs(2L,15L);
  TestMessageListener hacoListener=new TestMessageListener();
  JMSConnectionMockupMQ.getInstance().setListener(JobChannelUtil.getChannel(JobPriority.HIGHPRIORITY),hacoListener);
  JMSConnectionMockupMQ.getInstance().setListener(JobChannelUtil.getChannel(JobPriority.LOWPRIORITY),hacoListener);
  submitNewJobs();
  ((JMSConnectionMockupMQ)JMSConnectionMockupMQ.getInstance()).waitForConcurrentTasksToFinish();
  assertEquals(""String_Node_Str"",14,hacoListener.getNumReceived());
  DoOneCrawlMessage crawlMessage=(DoOneCrawlMessage)hacoListener.getReceived();
  assertEquals(""String_Node_Str"",1,crawlMessage.getMetadata().size());
  MetadataEntry metadataEntry=crawlMessage.getMetadata().get(0);
  assertNotNull(""String_Node_Str"",metadataEntry);
  assertEquals(""String_Node_Str"",""String_Node_Str"",metadataEntry.getMimeType());
  assertEquals(""String_Node_Str"",""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"",metadataEntry.getURL());
  assertEquals(""String_Node_Str"",""String_Node_Str"",new String(metadataEntry.getData()));
}",0.9992378048780488
89728,"/** 
 * Calls the <code>submitNewJobs</code> method on the current harvestScheduler test instance
 * @throws Exception
 */
private void submitNewJobs() throws Exception {
  ReflectUtils.getPrivateMethod(HarvestScheduler.class,""String_Node_Str"").invoke(harvestScheduler);
  ((JMSConnectionMockupMQ)JMSConnectionMockupMQ.getInstance()).waitForConcurrentTasksToFinish();
}","/** 
 * Calls the <code>submitNewJobs</code> method on the current harvestScheduler test instance
 * @throws Exception
 */
private void submitNewJobs() throws Exception {
  harvestScheduler.submitNewJobs();
  ((JMSConnectionMockupMQ)JMSConnectionMockupMQ.getInstance()).waitForConcurrentTasksToFinish();
}",0.8249258160237388
89729,"/** 
 * Checks that the connection is valid (i.e. still open on the server side). This implementation can be overriden if a specific RDBM is not handling the   {@link Connection#isValid(int)} JDBC4 method properly.
 * @param connection
 * @param validityTimeout
 * @return
 * @throws SQLException 
 */
public boolean connectionIsValid(Connection connection,int validityTimeout) throws SQLException {
  return connection.isValid(validityTimeout);
}","/** 
 * Checks that the connection is valid (i.e. still open on the server side). This implementation can be overriden if a specific RDBM is not handling the   {@link Connection#isValid(int)} JDBC4 method properly.
 * @param connection A database connection.
 * @param validityTimeout The time in seconds to wait for the database operation used to validate the connection 
 * @return true, if the connection is valid; false otherwise
 * @throws SQLException 
 */
public boolean connectionIsValid(Connection connection,int validityTimeout) throws SQLException {
  return connection.isValid(validityTimeout);
}",0.8473933649289099
89730,"/** 
 * Checks that the message queue for the given harvest job is empty and  therefore ready for the next message.
 * @param job The job to check the queue for
 * @return Is the queue empty
 * @throws JMSException Unable to retrieve queue information
 */
private boolean isQueueEmpty(ChannelID channelId) throws JMSException {
  QueueBrowser qBrowser=jmsConnection.createQueueBrowser(channelId);
  return !qBrowser.getEnumeration().hasMoreElements();
}","/** 
 * Checks that the message queue for the given harvest job is empty and  therefore ready for the next message.
 * @param channelId The id of the channel to check
 * @return Is the queue empty
 * @throws JMSException Unable to retrieve queue information
 */
private boolean isQueueEmpty(ChannelID channelId) throws JMSException {
  QueueBrowser qBrowser=jmsConnection.createQueueBrowser(channelId);
  return !qBrowser.getEnumeration().hasMoreElements();
}",0.949561403508772
89731,"/** 
 * Submit those jobs that are ready for submission if the relevant  message queue is empty.
 */
synchronized void submitNewJobs(){
  final JobDAO dao=JobDAO.getInstance();
  Iterator<Long> jobsToSubmit=dao.getAllJobIds(JobStatus.NEW);
  if (!jobsToSubmit.hasNext()) {
    log.trace(""String_Node_Str"");
  }
 else {
    log.trace(""String_Node_Str"");
  }
  int numberOfSubmittedJobs=0;
  while (jobsToSubmit.hasNext()) {
    final long jobID=jobsToSubmit.next();
    Job jobToSubmit=null;
    try {
      jobToSubmit=dao.read(jobID);
      if (isQueueEmpty(JobChannelUtil.getChannel(jobToSubmit.getPriority()))) {
        jobToSubmit.setStatus(JobStatus.SUBMITTED);
        jobToSubmit.setSubmittedDate(new Date());
        dao.update(jobToSubmit);
        List<MetadataEntry> metadata=new ArrayList<MetadataEntry>();
        MetadataEntry aliasMetadataEntry=MetadataEntry.makeAliasMetadataEntry(jobToSubmit.getJobAliasInfo(),jobToSubmit.getOrigHarvestDefinitionID(),jobToSubmit.getHarvestNum(),jobToSubmit.getJobID());
        if (aliasMetadataEntry != null) {
          metadata.add(aliasMetadataEntry);
        }
        if (HeritrixLauncher.isDeduplicationEnabledInTemplate(jobToSubmit.getOrderXMLdoc())) {
          MetadataEntry duplicateReductionMetadataEntry=MetadataEntry.makeDuplicateReductionMetadataEntry(dao.getJobIDsForDuplicateReduction(jobID),jobToSubmit.getOrigHarvestDefinitionID(),jobToSubmit.getHarvestNum(),jobToSubmit.getJobID());
          if (duplicateReductionMetadataEntry != null) {
            metadata.add(duplicateReductionMetadataEntry);
          }
        }
        doOneCrawl(jobToSubmit,metadata);
        numberOfSubmittedJobs++;
        log.trace(""String_Node_Str"" + jobToSubmit + ""String_Node_Str"");
      }
    }
 catch (    Throwable e) {
      String message=""String_Node_Str"" + jobID;
      log.warn(message,e);
      if (jobToSubmit != null) {
        jobToSubmit.setStatus(JobStatus.FAILED);
        jobToSubmit.appendHarvestErrors(message);
        jobToSubmit.appendHarvestErrorDetails(ExceptionUtils.getStackTrace(e));
        dao.update(jobToSubmit);
      }
    }
  }
  if (numberOfSubmittedJobs > 0) {
    log.info(""String_Node_Str"" + numberOfSubmittedJobs + ""String_Node_Str"");
  }
}","/** 
 * Submit those jobs that are ready for submission if the relevant message queue is empty.
 */
synchronized void submitNewJobs(){
  final JobDAO dao=JobDAO.getInstance();
  Iterator<Long> jobsToSubmit=dao.getAllJobIds(JobStatus.NEW);
  if (!jobsToSubmit.hasNext()) {
    log.trace(""String_Node_Str"");
  }
 else {
    log.trace(""String_Node_Str"");
  }
  int numberOfSubmittedJobs=0;
  while (jobsToSubmit.hasNext()) {
    final long jobID=jobsToSubmit.next();
    Job jobToSubmit=null;
    try {
      jobToSubmit=dao.read(jobID);
      if (isQueueEmpty(JobChannelUtil.getChannel(jobToSubmit.getPriority()))) {
        jobToSubmit.setStatus(JobStatus.SUBMITTED);
        jobToSubmit.setSubmittedDate(new Date());
        dao.update(jobToSubmit);
        List<MetadataEntry> metadata=new ArrayList<MetadataEntry>();
        MetadataEntry aliasMetadataEntry=MetadataEntry.makeAliasMetadataEntry(jobToSubmit.getJobAliasInfo(),jobToSubmit.getOrigHarvestDefinitionID(),jobToSubmit.getHarvestNum(),jobToSubmit.getJobID());
        if (aliasMetadataEntry != null) {
          metadata.add(aliasMetadataEntry);
        }
        if (HeritrixLauncher.isDeduplicationEnabledInTemplate(jobToSubmit.getOrderXMLdoc())) {
          MetadataEntry duplicateReductionMetadataEntry=MetadataEntry.makeDuplicateReductionMetadataEntry(dao.getJobIDsForDuplicateReduction(jobID),jobToSubmit.getOrigHarvestDefinitionID(),jobToSubmit.getHarvestNum(),jobToSubmit.getJobID());
          if (duplicateReductionMetadataEntry != null) {
            metadata.add(duplicateReductionMetadataEntry);
          }
        }
        doOneCrawl(jobToSubmit,metadata);
        numberOfSubmittedJobs++;
        log.trace(""String_Node_Str"" + jobToSubmit + ""String_Node_Str"");
      }
    }
 catch (    Throwable e) {
      String message=""String_Node_Str"" + jobID;
      log.warn(message,e);
      if (jobToSubmit != null) {
        jobToSubmit.setStatus(JobStatus.FAILED);
        jobToSubmit.appendHarvestErrors(message);
        jobToSubmit.appendHarvestErrorDetails(ExceptionUtils.getStackTrace(e));
        dao.update(jobToSubmit);
      }
    }
  }
  if (numberOfSubmittedJobs > 0) {
    log.info(""String_Node_Str"" + numberOfSubmittedJobs + ""String_Node_Str"");
  }
}",0.9997763363900694
89732,"/** 
 * Release allocated resources (JMS connections) and stops dispatching  harvest jobs, all without logging.
 * @override
 */
public void shutdown(){
  log.debug(""String_Node_Str"");
  if (dispatcherThread != null) {
    dispatcherThread.interrupt();
    dispatcherThread=null;
  }
  jmsConnection=null;
}","/** 
 * Release allocated resources (JMS connections) and stops dispatching  harvest jobs, all without logging.
 */
@Override public void shutdown(){
  log.debug(""String_Node_Str"");
  if (dispatcherThread != null) {
    dispatcherThread.interrupt();
    dispatcherThread=null;
  }
  jmsConnection=null;
}",0.9819967266775778
89733,"/** 
 * Get a hibernate session for communicating with the object store for the wayback indexer. This method has the side effect of creating and initialising a SessionFactory object if there is no current open SessionFactory.
 * @return
 */
public static Session getSession(){
  initialiseFactory();
  return sessionFactory.openSession();
}","/** 
 * Get a hibernate session for communicating with the object store for the wayback indexer. This method has the side effect of creating and initialising a SessionFactory object if there is no current open SessionFactory.
 * @return the abovementioned hibernate session.
 */
public static Session getSession(){
  initialiseFactory();
  return sessionFactory.openSession();
}",0.947075208913649
89734,"/** 
 * Processes an existing harvestInfoFile:</br> 1. Retrieve jobID, and crawlDir from the harvestInfoFile using class PersistentJobData</br> 2. finds JobId and arcsdir</br> 3. calls storeArcFiles</br> 4. moves harvestdir to oldjobs and deletes crawl.log and other superfluous files.
 * @param crawlDir The location of harvest-info to be processed
 * @param crawlException any exceptions thrown by the crawl which need tobe reported back to the scheduler (may be null for success)
 * @throws IOFailure if the file cannot be read
 */
private void processHarvestInfoFile(File crawlDir,Throwable crawlException) throws IOFailure {
  log.debug(""String_Node_Str"" + crawlDir.getAbsolutePath() + ""String_Node_Str"");
  if (!PersistentJobData.existsIn(crawlDir)) {
    throw new IOFailure(""String_Node_Str"" + crawlDir.getAbsolutePath());
  }
  PersistentJobData harvestInfo=new PersistentJobData(crawlDir);
  Long jobID=harvestInfo.getJobID();
  StringBuilder errorMessage=new StringBuilder();
  DomainHarvestReport dhr=null;
  List<File> failedFiles=new ArrayList<File>();
  HeritrixFiles files=new HeritrixFiles(crawlDir,jobID,harvestInfo.getOrigHarvestDefinitionID());
  try {
    log.info(""String_Node_Str"" + crawlDir + ""String_Node_Str""+ ""String_Node_Str""+ jobID+ ""String_Node_Str"");
    dhr=controller.storeFiles(files,errorMessage,failedFiles);
  }
 catch (  Exception e) {
    String msg=""String_Node_Str"" + crawlDir.getAbsolutePath() + ""String_Node_Str"";
    log.warn(msg,e);
    errorMessage.append(e.getMessage()).append(""String_Node_Str"");
    NotificationsFactory.getInstance().errorEvent(msg + ""String_Node_Str"" + errorMessage.toString(),e);
  }
 finally {
    CrawlStatusMessage csm;
    if (crawlException == null && errorMessage.length() == 0) {
      log.warn(""String_Node_Str"" + jobID + ""String_Node_Str"");
      csm=new CrawlStatusMessage(jobID,JobStatus.DONE,dhr);
    }
 else {
      log.warn(""String_Node_Str"" + jobID + ""String_Node_Str"");
      csm=new CrawlStatusMessage(jobID,JobStatus.FAILED,dhr);
      setErrorMessages(csm,crawlException,errorMessage.toString(),dhr == null,failedFiles.size());
    }
    try {
      jmsConnection.send(csm);
      if (crawlException == null && errorMessage.length() == 0) {
        files.deleteFinalLogs();
      }
    }
  finally {
      log.info(""String_Node_Str"" + jobID + ""String_Node_Str"");
      files.cleanUpAfterHarvest(new File(Settings.get(HarvesterSettings.HARVEST_CONTROLLER_OLDJOBSDIR)));
    }
  }
  log.info(""String_Node_Str"" + jobID + ""String_Node_Str""+ crawlDir.getAbsolutePath()+ ""String_Node_Str"");
}","/** 
 * Processes an existing harvestInfoFile:</br> 1. Retrieve jobID, and crawlDir from the harvestInfoFile using class PersistentJobData</br> 2. finds JobId and arcsdir</br> 3. calls storeArcFiles</br> 4. moves harvestdir to oldjobs and deletes crawl.log and other superfluous files.
 * @param crawlDir The location of harvest-info to be processed
 * @param crawlException any exceptions thrown by the crawl which need tobe reported back to the scheduler (may be null for success)
 * @throws IOFailure if the file cannot be read
 */
private void processHarvestInfoFile(File crawlDir,Throwable crawlException) throws IOFailure {
  log.debug(""String_Node_Str"" + crawlDir.getAbsolutePath() + ""String_Node_Str"");
  if (!PersistentJobData.existsIn(crawlDir)) {
    throw new IOFailure(""String_Node_Str"" + crawlDir.getAbsolutePath());
  }
  PersistentJobData harvestInfo=new PersistentJobData(crawlDir);
  Long jobID=harvestInfo.getJobID();
  StringBuilder errorMessage=new StringBuilder();
  DomainHarvestReport dhr=null;
  List<File> failedFiles=new ArrayList<File>();
  HeritrixFiles files=new HeritrixFiles(crawlDir,jobID,harvestInfo.getOrigHarvestDefinitionID());
  try {
    log.info(""String_Node_Str"" + crawlDir + ""String_Node_Str""+ ""String_Node_Str""+ jobID+ ""String_Node_Str"");
    dhr=controller.storeFiles(files,errorMessage,failedFiles);
  }
 catch (  Exception e) {
    String msg=""String_Node_Str"" + crawlDir.getAbsolutePath() + ""String_Node_Str"";
    log.warn(msg,e);
    errorMessage.append(e.getMessage()).append(""String_Node_Str"");
    NotificationsFactory.getInstance().errorEvent(msg + ""String_Node_Str"" + errorMessage.toString(),e);
  }
 finally {
    CrawlStatusMessage csm;
    if (crawlException == null && errorMessage.length() == 0) {
      log.info(""String_Node_Str"" + jobID + ""String_Node_Str"");
      csm=new CrawlStatusMessage(jobID,JobStatus.DONE,dhr);
    }
 else {
      log.warn(""String_Node_Str"" + jobID + ""String_Node_Str"");
      csm=new CrawlStatusMessage(jobID,JobStatus.FAILED,dhr);
      setErrorMessages(csm,crawlException,errorMessage.toString(),dhr == null,failedFiles.size());
    }
    try {
      jmsConnection.send(csm);
      if (crawlException == null && errorMessage.length() == 0) {
        files.deleteFinalLogs();
      }
    }
  finally {
      log.info(""String_Node_Str"" + jobID + ""String_Node_Str"");
      files.cleanUpAfterHarvest(new File(Settings.get(HarvesterSettings.HARVEST_CONTROLLER_OLDJOBSDIR)));
    }
  }
  log.info(""String_Node_Str"" + jobID + ""String_Node_Str""+ crawlDir.getAbsolutePath()+ ""String_Node_Str"");
}",0.9984466019417476
89735,"/** 
 * Reads output from a checksum file.  Only the first instance of the desired file will be used.  If other filenames are encountered than the wanted one, they will be logged at level warning, as that is indicative of serious errors.  Having more than one instance of the desired file merely means it was found in several bitarchives, which is not our problem.
 * @param outputFile The file to read checksum from.
 * @param arcfileName The arcfile to find checksum for.
 * @return The checksum, or the empty stringif no checksum found for arcfilename.
 * @throws IOFailure If any error occurs reading the file.
 * @throws IllegalState if the read format is wrong
 */
private String readChecksum(File outputFile,String arcfileName) throws IOFailure, IllegalState {
  List<String> lines=FileUtils.readListFromFile(outputFile);
  List<String> checksumList=new ArrayList<String>();
  for (  String line : lines) {
    String readFileName=""String_Node_Str"";
    String checksum=""String_Node_Str"";
    String[] tokens=line.split(dk.netarkivet.archive.arcrepository.bitpreservation.Constants.STRING_FILENAME_SEPARATOR);
    boolean ignoreLine=false;
    ignoreLine=(tokens.length == 0 || line.isEmpty());
    if (tokens.length != 2 && !ignoreLine) {
      throw new IllegalState(""String_Node_Str"" + ""String_Node_Str"" + line + ""String_Node_Str"");
    }
    if (!ignoreLine) {
      readFileName=tokens[0];
      checksum=tokens[1];
      if (checksum.length() == 0) {
        ignoreLine=true;
        log.warn(""String_Node_Str"" + ""String_Node_Str"" + arcfileName + ""String_Node_Str""+ line+ ""String_Node_Str"");
      }
 else {
        if (!readFileName.equals(arcfileName)) {
          ignoreLine=true;
          log.warn(""String_Node_Str"" + ""String_Node_Str"" + arcfileName + ""String_Node_Str""+ ""String_Node_Str""+ line+ ""String_Node_Str"");
        }
      }
    }
    if (checksumList.size() > 0 && !ignoreLine && !checksum.equals(checksumList.get(checksumList.size() - 1))) {
      String errMsg=""String_Node_Str"" + arcfileName + ""String_Node_Str""+ checksumList.get(0)+ ""String_Node_Str""+ checksum+ ""String_Node_Str""+ line+ ""String_Node_Str"";
      log.warn(errMsg);
      throw new IllegalState(errMsg);
    }
    if (!ignoreLine) {
      checksumList.add(checksum);
    }
  }
  if (checksumList.size() > 1) {
    log.warn(""String_Node_Str"" + arcfileName + ""String_Node_Str""+ checksumList.size()+ ""String_Node_Str""+ checksumList.get(0));
  }
  if (checksumList.size() == 0) {
    log.debug(""String_Node_Str"" + arcfileName + ""String_Node_Str""+ outputFile+ ""String_Node_Str""+ FileUtils.readListFromFile(outputFile));
    return ""String_Node_Str"";
  }
 else {
    return checksumList.get(0);
  }
}","/** 
 * Reads output from a checksum file.  Only the first instance of the desired file will be used.  If other filenames are encountered than the wanted one, they will be logged at level warning, as that is indicative of serious errors.  Having more than one instance of the desired file merely means it was found in several bitarchives, which is not our problem.
 * @param outputFile The file to read checksum from.
 * @param arcfileName The arcfile to find checksum for.
 * @return The checksum, or the empty stringif no checksum found for arcfilename.
 * @throws IOFailure If any error occurs reading the file.
 * @throws IllegalState if the read format is wrong
 */
private String readChecksum(File outputFile,String arcfileName) throws IOFailure, IllegalState {
  List<String> lines=FileUtils.readListFromFile(outputFile);
  List<String> checksumList=new ArrayList<String>();
  for (  String line : lines) {
    String readFileName=""String_Node_Str"";
    String checksum=""String_Node_Str"";
    String[] tokens=line.split(ChecksumJob.STRING_FILENAME_SEPARATOR);
    boolean ignoreLine=false;
    ignoreLine=(tokens.length == 0 || line.isEmpty());
    if (tokens.length != 2 && !ignoreLine) {
      throw new IllegalState(""String_Node_Str"" + ""String_Node_Str"" + line + ""String_Node_Str"");
    }
    if (!ignoreLine) {
      readFileName=tokens[0];
      checksum=tokens[1];
      if (checksum.length() == 0) {
        ignoreLine=true;
        log.warn(""String_Node_Str"" + ""String_Node_Str"" + arcfileName + ""String_Node_Str""+ line+ ""String_Node_Str"");
      }
 else {
        if (!readFileName.equals(arcfileName)) {
          ignoreLine=true;
          log.warn(""String_Node_Str"" + ""String_Node_Str"" + arcfileName + ""String_Node_Str""+ ""String_Node_Str""+ line+ ""String_Node_Str"");
        }
      }
    }
    if (checksumList.size() > 0 && !ignoreLine && !checksum.equals(checksumList.get(checksumList.size() - 1))) {
      String errMsg=""String_Node_Str"" + arcfileName + ""String_Node_Str""+ checksumList.get(0)+ ""String_Node_Str""+ checksum+ ""String_Node_Str""+ line+ ""String_Node_Str"";
      log.warn(errMsg);
      throw new IllegalState(errMsg);
    }
    if (!ignoreLine) {
      checksumList.add(checksum);
    }
  }
  if (checksumList.size() > 1) {
    log.warn(""String_Node_Str"" + arcfileName + ""String_Node_Str""+ checksumList.size()+ ""String_Node_Str""+ checksumList.get(0));
  }
  if (checksumList.size() == 0) {
    log.debug(""String_Node_Str"" + arcfileName + ""String_Node_Str""+ outputFile+ ""String_Node_Str""+ FileUtils.readListFromFile(outputFile));
    return ""String_Node_Str"";
  }
 else {
    return checksumList.get(0);
  }
}",0.9868667917448404
89736,"/** 
 * Test that correct checksums are generated in ChecksumJob, i.e. that the expected checksums are written to the remote file for a given set of files.
 * @throws IOException
 */
public void testGeneratedChecksum() throws IOException {
  ChecksumJob checkJob=new ChecksumJob();
  BatchStatus batchStatus=arClient.batch(checkJob,Settings.get(CommonSettings.USE_REPLICA_ID));
  batchStatus.getResultFile().copyTo(OUTPUT_FILE);
  List<String> jobChecksums=new ArrayList<String>();
  assertTrue(""String_Node_Str"",OUTPUT_FILE.exists());
  BufferedReader reader=new BufferedReader(new FileReader(OUTPUT_FILE));
  String line;
  while ((line=reader.readLine()) != null) {
    jobChecksums.add(line.split(dk.netarkivet.archive.arcrepository.bitpreservation.Constants.STRING_FILENAME_SEPARATOR)[1]);
  }
  reader.close();
  FileUtils.removeRecursively(OUTPUT_FILE);
  String[] refChecksums=new String[testFiles.length];
  for (int i=0; i < refChecksums.length; i++) {
    refChecksums[i]=MD5.generateMD5onFile(testFiles[i]);
  }
  for (int i=0; i < testFiles.length; i++) {
    assertTrue(""String_Node_Str"" + refChecksums[i],jobChecksums.contains(refChecksums[i]));
  }
}","/** 
 * Test that correct checksums are generated in ChecksumJob, i.e. that the expected checksums are written to the remote file for a given set of files.
 * @throws IOException
 */
public void testGeneratedChecksum() throws IOException {
  ChecksumJob checkJob=new ChecksumJob();
  BatchStatus batchStatus=arClient.batch(checkJob,Settings.get(CommonSettings.USE_REPLICA_ID));
  batchStatus.getResultFile().copyTo(OUTPUT_FILE);
  List<String> jobChecksums=new ArrayList<String>();
  assertTrue(""String_Node_Str"",OUTPUT_FILE.exists());
  BufferedReader reader=new BufferedReader(new FileReader(OUTPUT_FILE));
  String line;
  while ((line=reader.readLine()) != null) {
    jobChecksums.add(line.split(ChecksumJob.STRING_FILENAME_SEPARATOR)[1]);
  }
  reader.close();
  FileUtils.removeRecursively(OUTPUT_FILE);
  String[] refChecksums=new String[testFiles.length];
  for (int i=0; i < refChecksums.length; i++) {
    refChecksums[i]=MD5.generateMD5onFile(testFiles[i]);
  }
  for (int i=0; i < testFiles.length; i++) {
    assertTrue(""String_Node_Str"" + refChecksums[i],jobChecksums.contains(refChecksums[i]));
  }
}",0.9684487291849256
89737,"/** 
 * Test that the OnBatchReply method updates state and responds correctly. This is a rather complex test, but should not attempt to test processCheckSum().  It has to set up the following: outstandingChecksumFiles should contain an entry replyOfId->arcfilename msg should contain id, errmsg, resultfile, filesprocessed, filesfailed, but the channels are not used. ad should contain some checksum for the arcfilename but no replyinfo -- we can check the effect by seeing warnings and state.
 * @throws Exception if exception is thrown
 */
@SuppressWarnings(""String_Node_Str"") public void testOnBatchReply() throws Exception {
  ArcRepository a=ArcRepository.getInstance();
  UpdateableAdminData ad=UpdateableAdminData.getUpdateableInstance();
  Field ocf=a.getClass().getDeclaredField(""String_Node_Str"");
  ocf.setAccessible(true);
  Map<String,String> outstanding=(Map<String,String>)ocf.get(a);
  Field adm=ad.getClass().getSuperclass().getDeclaredField(""String_Node_Str"");
  adm.setAccessible(true);
  Map<String,ArcRepositoryEntry> admindataentries=(Map<String,ArcRepositoryEntry>)adm.get(ad);
  String id1=""String_Node_Str"";
  String arcname1=""String_Node_Str"";
  outstanding.put(id1,arcname1);
  ad.addEntry(arcname1,null,""String_Node_Str"");
  BatchReplyMessage bamsg0=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new StringRemoteFile(arcname1 + dk.netarkivet.archive.arcrepository.bitpreservation.Constants.STRING_FILENAME_SEPARATOR + ""String_Node_Str""));
  JMSConnectionMockupMQ.updateMsgID(bamsg0,id1);
  a.onBatchReply(bamsg0);
  LogUtils.flushLogs(ArcRepository.class.getName());
  FileAsserts.assertFileNotContains(""String_Node_Str"",TestInfo.LOG_FILE,""String_Node_Str"");
  assertEquals(""String_Node_Str"",ReplicaStoreState.UPLOAD_COMPLETED,ad.getState(arcname1,Channels.getTheBamon().getName()));
  outstanding.put(id1,arcname1);
  ad.addEntry(arcname1,null,""String_Node_Str"");
  BatchReplyMessage bamsg2=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new NullRemoteFile());
  JMSConnectionMockupMQ.updateMsgID(bamsg2,id1);
  bamsg2.setNotOk(""String_Node_Str"");
  a.onBatchReply(bamsg2);
  LogUtils.flushLogs(ArcRepository.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",TestInfo.LOG_FILE);
  assertEquals(""String_Node_Str"",ReplicaStoreState.UPLOAD_FAILED,ad.getState(arcname1,Channels.getTheBamon().getName()));
  admindataentries.remove(arcname1);
  outstanding.put(id1,arcname1);
  BatchReplyMessage bamsg3=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new NullRemoteFile());
  JMSConnectionMockupMQ.updateMsgID(bamsg3,id1);
  bamsg3.setNotOk(""String_Node_Str"");
  try {
    a.onBatchReply(bamsg3);
    fail(""String_Node_Str"" + ""String_Node_Str"" + arcname1);
  }
 catch (  UnknownID e) {
    StringAsserts.assertStringContains(""String_Node_Str"" + ""String_Node_Str"",arcname1,e.getMessage());
  }
  LogUtils.flushLogs(ArcRepository.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",TestInfo.LOG_FILE);
  assertFalse(""String_Node_Str"",ad.hasEntry(arcname1));
  BatchReplyMessage bamsg1=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new NullRemoteFile());
  a.onBatchReply(bamsg1);
  LogUtils.flushLogs(ArcRepository.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"" + id1,TestInfo.LOG_FILE);
  assertFalse(""String_Node_Str"",ad.hasEntry(arcname1));
}","/** 
 * Test that the OnBatchReply method updates state and responds correctly. This is a rather complex test, but should not attempt to test processCheckSum().  It has to set up the following: outstandingChecksumFiles should contain an entry replyOfId->arcfilename msg should contain id, errmsg, resultfile, filesprocessed, filesfailed, but the channels are not used. ad should contain some checksum for the arcfilename but no replyinfo -- we can check the effect by seeing warnings and state.
 * @throws Exception if exception is thrown
 */
@SuppressWarnings(""String_Node_Str"") public void testOnBatchReply() throws Exception {
  ArcRepository a=ArcRepository.getInstance();
  UpdateableAdminData ad=UpdateableAdminData.getUpdateableInstance();
  Field ocf=a.getClass().getDeclaredField(""String_Node_Str"");
  ocf.setAccessible(true);
  Map<String,String> outstanding=(Map<String,String>)ocf.get(a);
  Field adm=ad.getClass().getSuperclass().getDeclaredField(""String_Node_Str"");
  adm.setAccessible(true);
  Map<String,ArcRepositoryEntry> admindataentries=(Map<String,ArcRepositoryEntry>)adm.get(ad);
  String id1=""String_Node_Str"";
  String arcname1=""String_Node_Str"";
  outstanding.put(id1,arcname1);
  ad.addEntry(arcname1,null,""String_Node_Str"");
  BatchReplyMessage bamsg0=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new StringRemoteFile(arcname1 + ChecksumJob.STRING_FILENAME_SEPARATOR + ""String_Node_Str""));
  JMSConnectionMockupMQ.updateMsgID(bamsg0,id1);
  a.onBatchReply(bamsg0);
  LogUtils.flushLogs(ArcRepository.class.getName());
  FileAsserts.assertFileNotContains(""String_Node_Str"",TestInfo.LOG_FILE,""String_Node_Str"");
  assertEquals(""String_Node_Str"",ReplicaStoreState.UPLOAD_COMPLETED,ad.getState(arcname1,Channels.getTheBamon().getName()));
  outstanding.put(id1,arcname1);
  ad.addEntry(arcname1,null,""String_Node_Str"");
  BatchReplyMessage bamsg2=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new NullRemoteFile());
  JMSConnectionMockupMQ.updateMsgID(bamsg2,id1);
  bamsg2.setNotOk(""String_Node_Str"");
  a.onBatchReply(bamsg2);
  LogUtils.flushLogs(ArcRepository.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",TestInfo.LOG_FILE);
  assertEquals(""String_Node_Str"",ReplicaStoreState.UPLOAD_FAILED,ad.getState(arcname1,Channels.getTheBamon().getName()));
  admindataentries.remove(arcname1);
  outstanding.put(id1,arcname1);
  BatchReplyMessage bamsg3=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new NullRemoteFile());
  JMSConnectionMockupMQ.updateMsgID(bamsg3,id1);
  bamsg3.setNotOk(""String_Node_Str"");
  try {
    a.onBatchReply(bamsg3);
    fail(""String_Node_Str"" + ""String_Node_Str"" + arcname1);
  }
 catch (  UnknownID e) {
    StringAsserts.assertStringContains(""String_Node_Str"" + ""String_Node_Str"",arcname1,e.getMessage());
  }
  LogUtils.flushLogs(ArcRepository.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",TestInfo.LOG_FILE);
  assertFalse(""String_Node_Str"",ad.hasEntry(arcname1));
  BatchReplyMessage bamsg1=new BatchReplyMessage(Channels.getTheRepos(),Channels.getTheBamon(),id1,0,new ArrayList<File>(0),new NullRemoteFile());
  a.onBatchReply(bamsg1);
  LogUtils.flushLogs(ArcRepository.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"" + id1,TestInfo.LOG_FILE);
  assertFalse(""String_Node_Str"",ad.hasEntry(arcname1));
}",0.9904386951631046
89738,"/** 
 * Test that correct checksums are generated in ChecksumJob, i.e. that the expected checksums are written to the remote file for a given set of files.
 * @throws IOException
 */
public void testGeneratedChecksum() throws IOException {
  ChecksumJob checkJob=new ChecksumJob();
  BatchStatus batchStatus=arClient.batch(checkJob,Settings.get(CommonSettings.USE_REPLICA_ID));
  batchStatus.getResultFile().copyTo(OUTPUT_FILE);
  List<String> jobChecksums=new ArrayList<String>();
  assertTrue(""String_Node_Str"",OUTPUT_FILE.exists());
  BufferedReader reader=new BufferedReader(new FileReader(OUTPUT_FILE));
  String line;
  while ((line=reader.readLine()) != null) {
    jobChecksums.add(line.split(dk.netarkivet.archive.arcrepository.bitpreservation.Constants.STRING_FILENAME_SEPARATOR)[1]);
  }
  reader.close();
  FileUtils.removeRecursively(OUTPUT_FILE);
  String[] refChecksums=new String[testFiles.length];
  for (int i=0; i < refChecksums.length; i++) {
    refChecksums[i]=MD5.generateMD5onFile(testFiles[i]);
  }
  for (int i=0; i < testFiles.length; i++) {
    assertTrue(""String_Node_Str"" + refChecksums[i],jobChecksums.contains(refChecksums[i]));
  }
}","/** 
 * Test that correct checksums are generated in ChecksumJob, i.e. that the expected checksums are written to the remote file for a given set of files.
 * @throws IOException
 */
public void testGeneratedChecksum() throws IOException {
  ChecksumJob checkJob=new ChecksumJob();
  BatchStatus batchStatus=arClient.batch(checkJob,Settings.get(CommonSettings.USE_REPLICA_ID));
  batchStatus.getResultFile().copyTo(OUTPUT_FILE);
  List<String> jobChecksums=new ArrayList<String>();
  assertTrue(""String_Node_Str"",OUTPUT_FILE.exists());
  BufferedReader reader=new BufferedReader(new FileReader(OUTPUT_FILE));
  String line;
  while ((line=reader.readLine()) != null) {
    jobChecksums.add(line.split(ChecksumJob.STRING_FILENAME_SEPARATOR)[1]);
  }
  reader.close();
  FileUtils.removeRecursively(OUTPUT_FILE);
  String[] refChecksums=new String[testFiles.length];
  for (int i=0; i < refChecksums.length; i++) {
    refChecksums[i]=MD5.generateMD5onFile(testFiles[i]);
  }
  for (int i=0; i < testFiles.length; i++) {
    assertTrue(""String_Node_Str"" + refChecksums[i],jobChecksums.contains(refChecksums[i]));
  }
}",0.9684487291849256
89739,"public void testProcessMissingRequest() throws Exception {
  Settings.set(ArchiveSettings.DIR_ARCREPOSITORY_BITPRESERVATION,TestInfo.WORKING_DIR.getAbsolutePath());
  Settings.set(ArchiveSettings.DIRS_ARCREPOSITORY_ADMIN,TestInfo.WORKING_DIR.getAbsolutePath());
  AdminData.getUpdateableInstance();
  MockFileBasedActiveBitPreservation mockabp=new MockFileBasedActiveBitPreservation();
  MockHttpServletRequest request=new MockHttpServletRequest();
  String replicaID1=""String_Node_Str"";
  String replicaID2=""String_Node_Str"";
  String filename1=""String_Node_Str"";
  String filename2=""String_Node_Str"";
  Locale defaultLocale=new Locale(""String_Node_Str"");
  Map<String,String[]> args=new HashMap<String,String[]>();
  args.put(ADD_COMMAND,new String[]{Replica.getReplicaFromId(replicaID1).getName() + Constants.STRING_FILENAME_SEPARATOR + filename1});
  request.setupAddParameter(ADD_COMMAND,new String[]{Replica.getReplicaFromId(replicaID1).getName() + Constants.STRING_FILENAME_SEPARATOR + filename1});
  args.put(GET_INFO_COMMAND,new String[]{filename1});
  request.setupAddParameter(GET_INFO_COMMAND,new String[]{filename1});
  args.put(BITARCHIVE_NAME_PARAM,new String[]{Replica.getReplicaFromId(replicaID1).getName()});
  request.setupAddParameter(BITARCHIVE_NAME_PARAM,new String[]{Replica.getReplicaFromId(replicaID1).getName()});
  request.setupGetParameterMap(args);
  request.setupGetParameterNames(new Vector<String>(args.keySet()).elements());
  Map<String,PreservationState> status=BitpreserveFileState.processMissingRequest(WebinterfaceTestCase.getDummyPageContext(defaultLocale,request),new StringBuilder());
  assertEquals(""String_Node_Str"",1,mockabp.getCallCount(ADD_METHOD));
  assertEquals(""String_Node_Str"",1,mockabp.getCallCount(GET_INFO_METHOD));
  assertEquals(""String_Node_Str"",null,status.get(filename1));
  mockabp.calls.clear();
  request=new MockHttpServletRequest();
  args.clear();
  args.put(BITARCHIVE_NAME_PARAM,new String[]{Replica.getReplicaFromId(replicaID1).getName()});
  request.setupAddParameter(BITARCHIVE_NAME_PARAM,new String[]{Replica.getReplicaFromId(replicaID1).getName()});
  request.setupGetParameterMap(args);
  status=BitpreserveFileState.processMissingRequest(WebinterfaceTestCase.getDummyPageContext(defaultLocale,request),new StringBuilder());
  assertEquals(""String_Node_Str"",0,mockabp.getCallCount(ADD_METHOD));
  assertEquals(""String_Node_Str"",0,mockabp.getCallCount(GET_INFO_METHOD));
  assertEquals(""String_Node_Str"",0,status.size());
  mockabp.calls.clear();
  request=new MockHttpServletRequest();
  args.clear();
  args.put(BITARCHIVE_NAME_PARAM,new String[]{Replica.getReplicaFromId(replicaID2).getName()});
  request.setupAddParameter(BITARCHIVE_NAME_PARAM,new String[]{Replica.getReplicaFromId(replicaID2).getName()});
  request.setupAddParameter(ADD_COMMAND,new String[]{Replica.getReplicaFromId(replicaID2).getName() + Constants.STRING_FILENAME_SEPARATOR + filename1,Replica.getReplicaFromId(replicaID2).getName() + Constants.STRING_FILENAME_SEPARATOR + filename1});
  args.put(ADD_COMMAND,new String[]{Replica.getReplicaFromId(replicaID2).getName() + Constants.STRING_FILENAME_SEPARATOR + filename1,Replica.getReplicaFromId(replicaID2).getName() + Constants.STRING_FILENAME_SEPARATOR + filename1});
  request.setupAddParameter(GET_INFO_COMMAND,new String[]{filename1,filename2,filename1});
  args.put(GET_INFO_COMMAND,new String[]{filename1,filename2,filename1});
  request.setupGetParameterMap(args);
  status=BitpreserveFileState.processMissingRequest(WebinterfaceTestCase.getDummyPageContext(defaultLocale,request),new StringBuilder());
  assertEquals(""String_Node_Str"",2,mockabp.getCallCount(ADD_METHOD));
  assertEquals(""String_Node_Str"",3,mockabp.getCallCount(GET_INFO_METHOD));
  assertEquals(""String_Node_Str"",2,status.size());
  assertEquals(""String_Node_Str"",null,status.get(filename1));
  assertEquals(""String_Node_Str"",null,status.get(filename2));
  CollectionAsserts.assertIteratorEquals(""String_Node_Str"",Arrays.asList(new String[]{filename1 + ""String_Node_Str"" + replicaID2,filename1 + ""String_Node_Str"" + replicaID2}).iterator(),mockabp.calls.get(ADD_METHOD).iterator());
  CollectionAsserts.assertIteratorEquals(""String_Node_Str"",Arrays.asList(new String[]{filename1,filename2,filename1}).iterator(),mockabp.calls.get(GET_INFO_METHOD).iterator());
}","public void testProcessMissingRequest() throws Exception {
  Settings.set(ArchiveSettings.DIR_ARCREPOSITORY_BITPRESERVATION,TestInfo.WORKING_DIR.getAbsolutePath());
  Settings.set(ArchiveSettings.DIRS_ARCREPOSITORY_ADMIN,TestInfo.WORKING_DIR.getAbsolutePath());
  AdminData.getUpdateableInstance();
  MockFileBasedActiveBitPreservation mockabp=new MockFileBasedActiveBitPreservation();
  MockHttpServletRequest request=new MockHttpServletRequest();
  String replicaID1=""String_Node_Str"";
  String replicaID2=""String_Node_Str"";
  String filename1=""String_Node_Str"";
  String filename2=""String_Node_Str"";
  Locale defaultLocale=new Locale(""String_Node_Str"");
  Map<String,String[]> args=new HashMap<String,String[]>();
  args.put(ADD_COMMAND,new String[]{Replica.getReplicaFromId(replicaID1).getName() + STRING_FILENAME_SEPARATOR + filename1});
  request.setupAddParameter(ADD_COMMAND,new String[]{Replica.getReplicaFromId(replicaID1).getName() + STRING_FILENAME_SEPARATOR + filename1});
  args.put(GET_INFO_COMMAND,new String[]{filename1});
  request.setupAddParameter(GET_INFO_COMMAND,new String[]{filename1});
  args.put(BITARCHIVE_NAME_PARAM,new String[]{Replica.getReplicaFromId(replicaID1).getName()});
  request.setupAddParameter(BITARCHIVE_NAME_PARAM,new String[]{Replica.getReplicaFromId(replicaID1).getName()});
  request.setupGetParameterMap(args);
  request.setupGetParameterNames(new Vector<String>(args.keySet()).elements());
  Map<String,PreservationState> status=BitpreserveFileState.processMissingRequest(WebinterfaceTestCase.getDummyPageContext(defaultLocale,request),new StringBuilder());
  assertEquals(""String_Node_Str"",1,mockabp.getCallCount(ADD_METHOD));
  assertEquals(""String_Node_Str"",1,mockabp.getCallCount(GET_INFO_METHOD));
  assertEquals(""String_Node_Str"",null,status.get(filename1));
  mockabp.calls.clear();
  request=new MockHttpServletRequest();
  args.clear();
  args.put(BITARCHIVE_NAME_PARAM,new String[]{Replica.getReplicaFromId(replicaID1).getName()});
  request.setupAddParameter(BITARCHIVE_NAME_PARAM,new String[]{Replica.getReplicaFromId(replicaID1).getName()});
  request.setupGetParameterMap(args);
  status=BitpreserveFileState.processMissingRequest(WebinterfaceTestCase.getDummyPageContext(defaultLocale,request),new StringBuilder());
  assertEquals(""String_Node_Str"",0,mockabp.getCallCount(ADD_METHOD));
  assertEquals(""String_Node_Str"",0,mockabp.getCallCount(GET_INFO_METHOD));
  assertEquals(""String_Node_Str"",0,status.size());
  mockabp.calls.clear();
  request=new MockHttpServletRequest();
  args.clear();
  args.put(BITARCHIVE_NAME_PARAM,new String[]{Replica.getReplicaFromId(replicaID2).getName()});
  request.setupAddParameter(BITARCHIVE_NAME_PARAM,new String[]{Replica.getReplicaFromId(replicaID2).getName()});
  request.setupAddParameter(ADD_COMMAND,new String[]{Replica.getReplicaFromId(replicaID2).getName() + STRING_FILENAME_SEPARATOR + filename1,Replica.getReplicaFromId(replicaID2).getName() + STRING_FILENAME_SEPARATOR + filename1});
  args.put(ADD_COMMAND,new String[]{Replica.getReplicaFromId(replicaID2).getName() + STRING_FILENAME_SEPARATOR + filename1,Replica.getReplicaFromId(replicaID2).getName() + STRING_FILENAME_SEPARATOR + filename1});
  request.setupAddParameter(GET_INFO_COMMAND,new String[]{filename1,filename2,filename1});
  args.put(GET_INFO_COMMAND,new String[]{filename1,filename2,filename1});
  request.setupGetParameterMap(args);
  status=BitpreserveFileState.processMissingRequest(WebinterfaceTestCase.getDummyPageContext(defaultLocale,request),new StringBuilder());
  assertEquals(""String_Node_Str"",2,mockabp.getCallCount(ADD_METHOD));
  assertEquals(""String_Node_Str"",3,mockabp.getCallCount(GET_INFO_METHOD));
  assertEquals(""String_Node_Str"",2,status.size());
  assertEquals(""String_Node_Str"",null,status.get(filename1));
  assertEquals(""String_Node_Str"",null,status.get(filename2));
  CollectionAsserts.assertIteratorEquals(""String_Node_Str"",Arrays.asList(new String[]{filename1 + ""String_Node_Str"" + replicaID2,filename1 + ""String_Node_Str"" + replicaID2}).iterator(),mockabp.calls.get(ADD_METHOD).iterator());
  CollectionAsserts.assertIteratorEquals(""String_Node_Str"",Arrays.asList(new String[]{filename1,filename2,filename1}).iterator(),mockabp.calls.get(GET_INFO_METHOD).iterator());
}",0.9930313588850174
89740,"/** 
 * Calls the Unix sort command with the options <code>$filesNames -o $outputfile -T WaybackSettings#WAYBACK_AGGREGATOR_TEMP_DIR. Sets the LC_ALL environment variable before making the call.
 * @param files The files to merge and sort
 * @param outputFile The resulting sorted file
 * @param additionalArgs A list af extra arguments, which (if different fromnull) are added to the sort call.<p> Note: If any of the args contain a whitespace the call will fail.
 */
private void processFiles(File[] files,File outputFile,List<String> additionalArgs){
  if (files.length == 0) {
    return;
  }
  Process p=null;
  try {
    List<String> inputFileList=new LinkedList<String>();
    for (int i=0; i < files.length; i++) {
      if (files[i].exists() && files[i].isFile()) {
        inputFileList.add(files[i].getCanonicalPath());
      }
 else {
        log.warn(""String_Node_Str"" + files[i] + ""String_Node_Str""+ ""String_Node_Str"");
      }
    }
    List<String> cmd=new LinkedList<String>();
    cmd.add(""String_Node_Str"");
    cmd.addAll(inputFileList);
    cmd.add(""String_Node_Str"");
    cmd.add(outputFile.getCanonicalPath());
    cmd.add(""String_Node_Str"");
    cmd.add(Settings.get(WaybackSettings.WAYBACK_AGGREGATOR_TEMP_DIR));
    if (additionalArgs != null || additionalArgs.isEmpty()) {
      for (      String argument : additionalArgs) {
        ArgumentNotValid.checkTrue(argument.indexOf(' ') == -1,""String_Node_Str"" + argument + ""String_Node_Str"");
      }
      cmd.addAll(additionalArgs);
    }
    ProcessBuilder pb=new ProcessBuilder(cmd);
    pb.environment().put(""String_Node_Str"",""String_Node_Str"");
    pb.directory(new File(System.getProperty(""String_Node_Str"")));
    p=pb.start();
    p.waitFor();
    if (p.exitValue() != 0) {
      log.error(""String_Node_Str"" + p.exitValue());
    }
  }
 catch (  Exception e) {
    log.error(""String_Node_Str"",e);
  }
}","/** 
 * Calls the Unix sort command with the options <code>$filesNames -o $outputfile -T WaybackSettings#WAYBACK_AGGREGATOR_TEMP_DIR. Sets the LC_ALL environment variable before making the call.
 * @param files The files to merge and sort
 * @param outputFile The resulting sorted file
 * @param additionalArgs A list af extra arguments, which (if different fromnull) are added to the sort call.<p> Note: If any of the args contain a whitespace the call will fail.
 */
private void processFiles(File[] files,File outputFile,List<String> additionalArgs){
  if (files.length == 0) {
    return;
  }
  Process p=null;
  try {
    List<String> inputFileList=new LinkedList<String>();
    for (int i=0; i < files.length; i++) {
      if (files[i].exists() && files[i].isFile()) {
        inputFileList.add(files[i].getCanonicalPath());
      }
 else {
        log.warn(""String_Node_Str"" + files[i] + ""String_Node_Str""+ ""String_Node_Str"");
      }
    }
    List<String> cmd=new LinkedList<String>();
    cmd.add(""String_Node_Str"");
    cmd.addAll(inputFileList);
    cmd.add(""String_Node_Str"");
    cmd.add(outputFile.getCanonicalPath());
    cmd.add(""String_Node_Str"");
    cmd.add(Settings.get(WaybackSettings.WAYBACK_AGGREGATOR_TEMP_DIR));
    if (additionalArgs != null && !additionalArgs.isEmpty()) {
      for (      String argument : additionalArgs) {
        ArgumentNotValid.checkTrue(argument.indexOf(' ') == -1,""String_Node_Str"" + argument + ""String_Node_Str"");
      }
      cmd.addAll(additionalArgs);
    }
    ProcessBuilder pb=new ProcessBuilder(cmd);
    pb.environment().put(""String_Node_Str"",""String_Node_Str"");
    pb.directory(new File(System.getProperty(""String_Node_Str"")));
    p=pb.start();
    p.waitFor();
    if (p.exitValue() != 0) {
      log.error(""String_Node_Str"" + p.exitValue());
    }
  }
 catch (  Exception e) {
    log.error(""String_Node_Str"",e);
  }
}",0.998143728453991
89741,"/** 
 * Given the output of a checksum job, add the results to the database. The following fields in the table are updated for each corresponding  entry in the replicafileinfo table: <br/> - checksum = the given checksum.  <br/> - filelist_status = ok. <br/> - filelist_checkdatetime = now. <br/> - checksum_checkdatetime = now.
 * @param checksumOutput The output of a checksum job.
 * @param replica The replica this checksum job is for.
 */
@Override public void addChecksumInformation(List<String> checksumOutput,Replica replica){
  ArgumentNotValid.checkNotNull(checksumOutput,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  if (!existsReplicaInDB(replica)) {
    String msg=""String_Node_Str"" + replica.toString() + ""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg);
  }
  log.info(""String_Node_Str"" + checksumOutput.size() + ""String_Node_Str""+ replica.getId());
  Collections.sort(checksumOutput);
  List<Long> missingReplicaRFIs=retrieveReplicaFileInfoGuidsForReplica(replica.getId());
  String lastFilename=""String_Node_Str"";
  String lastChecksum=""String_Node_Str"";
  for (  String line : checksumOutput) {
    KeyValuePair<String,String> entry=ChecksumJob.parseLine(line);
    String filename=entry.getKey();
    String checksum=entry.getValue();
    if (filename.equals(lastFilename)) {
      if (!checksum.equals(lastChecksum)) {
        String errMsg=""String_Node_Str"" + filename + ""String_Node_Str""+ lastChecksum+ ""String_Node_Str""+ checksum+ ""String_Node_Str"";
        log.error(errMsg);
        NotificationsFactory.getInstance().errorEvent(errMsg);
      }
 else {
        log.debug(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str""+ checksum+ ""String_Node_Str"");
      }
      continue;
    }
    lastFilename=filename;
    lastChecksum=checksum;
    long fileid=-1;
    if (!existsFileInDB(filename)) {
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      fileid=insertFileIntoDB(filename);
    }
 else {
      fileid=retrieveIdForFile(filename);
    }
    if (fileid < 0) {
      log.warn(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      fileid=insertFileIntoDB(filename);
    }
    long rfiId=retrieveReplicaFileInfoGuid(fileid,replica.getId());
    if (rfiId < 0) {
      createReplicaFileInfoEntriesInDB(fileid);
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
    }
    updateReplicaFileInfoChecksum(rfiId,checksum);
    log.trace(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
    missingReplicaRFIs.remove(rfiId);
  }
  if (missingReplicaRFIs.size() > 0) {
    log.warn(""String_Node_Str"" + missingReplicaRFIs.size() + ""String_Node_Str""+ ""String_Node_Str""+ replica+ ""String_Node_Str"");
    for (    long rfi : missingReplicaRFIs) {
      updateReplicaFileInfoMissingFromFilelist(rfi);
    }
  }
  updateChecksumDateForReplica(replica);
  updateFilelistDateForReplica(replica);
  log.info(""String_Node_Str"" + checksumOutput.size() + ""String_Node_Str""+ replica.getId());
}","/** 
 * Given the output of a checksum job, add the results to the database. The following fields in the table are updated for each corresponding  entry in the replicafileinfo table: <br/> - checksum = the given checksum.  <br/> - filelist_status = ok. <br/> - filelist_checkdatetime = now. <br/> - checksum_checkdatetime = now.
 * @param checksumOutput The output of a checksum job.
 * @param replica The replica this checksum job is for.
 */
@Override public void addChecksumInformation(List<String> checksumOutput,Replica replica){
  ArgumentNotValid.checkNotNull(checksumOutput,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  if (!existsReplicaInDB(replica)) {
    String msg=""String_Node_Str"" + replica.toString() + ""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg);
  }
  log.info(""String_Node_Str"" + checksumOutput.size() + ""String_Node_Str""+ replica.getId());
  Collections.sort(checksumOutput);
  List<Long> missingReplicaRFIs=retrieveReplicaFileInfoGuidsForReplica(replica.getId());
  String lastFilename=""String_Node_Str"";
  String lastChecksum=""String_Node_Str"";
  int i=0;
  for (  String line : checksumOutput) {
    if (i % 10000 == 0) {
      log.info(""String_Node_Str"" + i + ""String_Node_Str""+ replica);
    }
    i++;
    KeyValuePair<String,String> entry=ChecksumJob.parseLine(line);
    String filename=entry.getKey();
    String checksum=entry.getValue();
    if (filename.equals(lastFilename)) {
      if (!checksum.equals(lastChecksum)) {
        String errMsg=""String_Node_Str"" + filename + ""String_Node_Str""+ lastChecksum+ ""String_Node_Str""+ checksum+ ""String_Node_Str"";
        log.error(errMsg);
        NotificationsFactory.getInstance().errorEvent(errMsg);
      }
 else {
        log.debug(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str""+ checksum+ ""String_Node_Str"");
      }
      continue;
    }
    lastFilename=filename;
    lastChecksum=checksum;
    long fileid=-1;
    if (!existsFileInDB(filename)) {
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      fileid=insertFileIntoDB(filename);
    }
 else {
      fileid=retrieveIdForFile(filename);
    }
    if (fileid < 0) {
      log.warn(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      fileid=insertFileIntoDB(filename);
    }
    long rfiId=retrieveReplicaFileInfoGuid(fileid,replica.getId());
    if (rfiId < 0) {
      createReplicaFileInfoEntriesInDB(fileid);
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
    }
    updateReplicaFileInfoChecksum(rfiId,checksum);
    log.trace(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
    missingReplicaRFIs.remove(rfiId);
  }
  if (missingReplicaRFIs.size() > 0) {
    log.warn(""String_Node_Str"" + missingReplicaRFIs.size() + ""String_Node_Str""+ ""String_Node_Str""+ replica+ ""String_Node_Str"");
    for (    long rfi : missingReplicaRFIs) {
      updateReplicaFileInfoMissingFromFilelist(rfi);
    }
  }
  updateChecksumDateForReplica(replica);
  updateFilelistDateForReplica(replica);
  log.info(""String_Node_Str"" + checksumOutput.size() + ""String_Node_Str""+ replica.getId());
}",0.9811853245531514
89742,"/** 
 * Method for adding the results from a list of filenames on a replica. This list of filenames should return the list of all the files within  the database.  For each file in the FileListJob the following fields are set for the  corresponding entry in the replicafileinfo table:  <br/> - filelist_status = ok. <br/> - filelist_checkdatetime = now. For each entry in the replicafileinfo table for the replica which are  missing in the results from the FileListJob the following fields are  assigned the following values: <br/> - filelist_status = missing. <br/> - filelist_checkdatetime = now.
 * @param filelist The list of filenames either parsed from a FilelistJob or the result from a GetAllFilenamesMessage.
 * @param replica The replica, which the FilelistBatchjob has run upon.
 * @throws ArgumentNotValid If the filelist or the replica is null.
 */
@Override public void addFileListInformation(List<String> filelist,Replica replica) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(filelist,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  List<Long> missingReplicaRFIs=retrieveReplicaFileInfoGuidsForReplica(replica.getId());
  Collections.sort(filelist);
  String lastFileName=""String_Node_Str"";
  for (  String file : filelist) {
    if (file.equals(lastFileName)) {
      log.warn(""String_Node_Str"" + file + ""String_Node_Str"");
      continue;
    }
    lastFileName=file;
    long fileId=retrieveIdForFile(file);
    if (fileId < 0) {
      log.info(""String_Node_Str"" + file + ""String_Node_Str""+ ""String_Node_Str"");
      fileId=insertFileIntoDB(file);
    }
    long rfiId=retrieveReplicaFileInfoGuid(fileId,replica.getId());
    if (rfiId < 0) {
      log.warn(""String_Node_Str"" + file + ""String_Node_Str""+ ""String_Node_Str""+ replica.getId()+ ""String_Node_Str""+ ""String_Node_Str"");
      createReplicaFileInfoEntriesInDB(fileId);
    }
    missingReplicaRFIs.remove(rfiId);
    updateReplicaFileInfoFilelist(rfiId);
  }
  for (  long rfi : missingReplicaRFIs) {
    updateReplicaFileInfoMissingFromFilelist(rfi);
  }
  updateFilelistDateForReplica(replica);
}","/** 
 * Method for adding the results from a list of filenames on a replica. This list of filenames should return the list of all the files within  the database.  For each file in the FileListJob the following fields are set for the  corresponding entry in the replicafileinfo table:  <br/> - filelist_status = ok. <br/> - filelist_checkdatetime = now. For each entry in the replicafileinfo table for the replica which are  missing in the results from the FileListJob the following fields are  assigned the following values: <br/> - filelist_status = missing. <br/> - filelist_checkdatetime = now.
 * @param filelist The list of filenames either parsed from a FilelistJob or the result from a GetAllFilenamesMessage.
 * @param replica The replica, which the FilelistBatchjob has run upon.
 * @throws ArgumentNotValid If the filelist or the replica is null.
 */
@Override public void addFileListInformation(List<String> filelist,Replica replica) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(filelist,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  List<Long> missingReplicaRFIs=retrieveReplicaFileInfoGuidsForReplica(replica.getId());
  Collections.sort(filelist);
  String lastFileName=""String_Node_Str"";
  int i=0;
  for (  String file : filelist) {
    if (i % 10000 == 0) {
      log.info(""String_Node_Str"" + i + ""String_Node_Str""+ replica);
    }
    i++;
    if (file.equals(lastFileName)) {
      log.warn(""String_Node_Str"" + file + ""String_Node_Str"");
      continue;
    }
    lastFileName=file;
    long fileId=retrieveIdForFile(file);
    if (fileId < 0) {
      log.info(""String_Node_Str"" + file + ""String_Node_Str""+ ""String_Node_Str"");
      fileId=insertFileIntoDB(file);
    }
    long rfiId=retrieveReplicaFileInfoGuid(fileId,replica.getId());
    if (rfiId < 0) {
      log.warn(""String_Node_Str"" + file + ""String_Node_Str""+ ""String_Node_Str""+ replica.getId()+ ""String_Node_Str""+ ""String_Node_Str"");
      createReplicaFileInfoEntriesInDB(fileId);
    }
    missingReplicaRFIs.remove(rfiId);
    updateReplicaFileInfoFilelist(rfiId);
  }
  for (  long rfi : missingReplicaRFIs) {
    updateReplicaFileInfoMissingFromFilelist(rfi);
  }
  updateFilelistDateForReplica(replica);
}",0.9724770642201837
89743,"/** 
 * Method for updating the checksum status of a replicafileinfo instance. Updates the following fields for the entry in the replicafileinfo: <br/> checksum_status = OK. <br/> checksum_checkdatetime = current time. 
 * @param replicafileinfoId The id of the replicafileinfo.
 */
private void updateReplicaFileInfoChecksumOk(long replicafileinfoId){
  try {
    String sql=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"";
    PreparedStatement statement=null;
    Connection connection=getDbConnection();
    Date now=new Date(Calendar.getInstance().getTimeInMillis());
    statement=DBUtils.prepareStatement(connection,sql,ChecksumStatus.OK.ordinal(),now,replicafileinfoId);
    statement.executeUpdate();
    connection.commit();
  }
 catch (  Exception e) {
    String msg=""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg,e);
  }
}","/** 
 * Method for updating the checksum status of a replicafileinfo instance. Updates the following fields for the entry in the replicafileinfo: <br/> checksum_status = OK. <br/> upload_status = UPLOAD_COMLPETE. <br/> checksum_checkdatetime = current time.  <br/><br/> The file is required to exist in the replica.
 * @param replicafileinfoId The id of the replicafileinfo.
 */
private void updateReplicaFileInfoChecksumOk(long replicafileinfoId){
  try {
    String sql=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"";
    PreparedStatement statement=null;
    Connection connection=getDbConnection();
    Date now=new Date(Calendar.getInstance().getTimeInMillis());
    statement=DBUtils.prepareStatement(connection,sql,ChecksumStatus.OK.ordinal(),now,ReplicaStoreState.UPLOAD_COMPLETED.ordinal(),replicafileinfoId);
    statement.executeUpdate();
    connection.commit();
  }
 catch (  Exception e) {
    String msg=""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg,e);
  }
}",0.9245585874799358
89744,"/** 
 * Method for retrieving the FilePreservationState for a specific file.
 * @param filename The name of the file for whom the FilePreservationStateshould be retrieved.
 * @return The FilePreservationState for the file.
 * @throws ArgumentNotValid If the filename is null or the empty string.
 */
@Override public PreservationState getPreservationState(String filename) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  List<ReplicaFileInfo> rfis=new ArrayList<ReplicaFileInfo>(Replica.getKnown().size());
  for (  Replica replica : Replica.getKnown()) {
    rfis.add(cache.getReplicaFileInfo(filename,replica));
  }
  return new DatabasePreservationState(filename,rfis);
}","/** 
 * Method for retrieving the FilePreservationState for a specific file.
 * @param filename The name of the file for whom the FilePreservationStateshould be retrieved.
 * @return The FilePreservationState for the file.
 * @throws ArgumentNotValid If the filename is null or the empty string.
 */
@Override public PreservationState getPreservationState(String filename) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  List<ReplicaFileInfo> rfis=new ArrayList<ReplicaFileInfo>(Replica.getKnown().size());
  for (  Replica replica : Replica.getKnown()) {
    updateChecksumStatus(filename,replica);
    rfis.add(cache.getReplicaFileInfo(filename,replica));
  }
  return new DatabasePreservationState(filename,rfis);
}",0.9704301075268816
89745,"/** 
 * The method for voting about the checksum of a file. <br/> Each entry in the replicafileinfo table containing the file is retrieved. All the unique checksums are retrieved, e.g. if a checksum is found more than one, then it is ignored. <br/> If only one unique checksum is found, then if must be the correct one,  and all the replicas with this file will have their checksum_status set  to 'OK'. <br/> If more than one checksum is found, then a vote for the correct checksum is performed. This is done by counting the amount of time each of the  unique checksum is found among the replicafileinfo entries for the  current file. The checksum with most votes is chosen as the correct one, and the checksum_status for all the replicafileinfo entries with this  checksum is set to 'OK', whereas the replicafileinfo entries with a  different checksum is set to 'CORRUPT'. <br/>   If no winner is found then a warning and a notification is issued, and  the checksum_status for all the replicafileinfo entries with for the  current file is set to 'UNKNOWN'. <br/>
 * @param fileId The id for the file to vote about.
 */
private void fileChecksumVote(long fileId){
  List<Long> rfiGuids=retrieveReplicaFileInfoGuidsForFile(fileId);
  List<ReplicaFileInfo> rfis=retrieveReplicaFileInfosWithChecksum(rfiGuids);
  if (rfis.size() == 0) {
    log.warn(""String_Node_Str"" + retrieveFilenameForFileId(fileId) + ""String_Node_Str"");
    return;
  }
  Set<String> hs=new HashSet<String>(rfis.size());
  for (  ReplicaFileInfo rfi : rfis) {
    hs.add(rfi.getChecksum());
  }
  if (hs.size() <= 1) {
    log.trace(""String_Node_Str"" + fileId + ""String_Node_Str"");
    for (    ReplicaFileInfo rfi : rfis) {
      updateReplicaFileInfoChecksumOk(rfi.getGuid());
    }
    return;
  }
  int[] csCount=new int[hs.size()];
  String[] uniqueCs=hs.toArray(new String[hs.size()]);
  for (  ReplicaFileInfo rfi : rfis) {
    for (int i=0; i < hs.size(); i++) {
      if (rfi.getChecksum().equals(uniqueCs[i])) {
        csCount[i]++;
      }
    }
  }
  int largest=0;
  boolean unique=false;
  int index=-1;
  for (int i=0; i < csCount.length; i++) {
    if (csCount[i] > largest) {
      largest=csCount[i];
      unique=true;
      index=i;
    }
 else     if (csCount[i] == largest) {
      unique=false;
    }
  }
  if (unique) {
    for (    ReplicaFileInfo rfi : rfis) {
      if (!rfi.getChecksum().equals(uniqueCs[index])) {
        updateReplicaFileInfoChecksumCorrupt(rfi.getGuid());
      }
 else {
        updateReplicaFileInfoChecksumOk(rfi.getGuid());
      }
    }
  }
 else {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + retrieveFilenameForFileId(fileId) + ""String_Node_Str"";
    log.error(errMsg);
    NotificationsFactory.getInstance().errorEvent(errMsg);
    for (    ReplicaFileInfo rfi : rfis) {
      updateReplicaFileInfoChecksumUnknown(rfi.getGuid());
    }
  }
}","/** 
 * The method for voting about the checksum of a file. <br/> Each entry in the replicafileinfo table containing the file is retrieved. All the unique checksums are retrieved, e.g. if a checksum is found more than one, then it is ignored. <br/> If only one unique checksum is found, then if must be the correct one,  and all the replicas with this file will have their checksum_status set  to 'OK'. <br/> If more than one checksum is found, then a vote for the correct checksum is performed. This is done by counting the amount of time each of the  unique checksum is found among the replicafileinfo entries for the  current file. The checksum with most votes is chosen as the correct one, and the checksum_status for all the replicafileinfo entries with this  checksum is set to 'OK', whereas the replicafileinfo entries with a  different checksum is set to 'CORRUPT'. <br/>   If no winner is found then a warning and a notification is issued, and  the checksum_status for all the replicafileinfo entries with for the  current file is set to 'UNKNOWN'. <br/>
 * @param fileId The id for the file to vote about.
 */
private void fileChecksumVote(long fileId){
  List<Long> rfiGuids=retrieveReplicaFileInfoGuidsForFile(fileId);
  List<ReplicaFileInfo> rfis=retrieveReplicaFileInfosWithChecksum(rfiGuids);
  if (rfis.size() == 0) {
    log.warn(""String_Node_Str"" + retrieveFilenameForFileId(fileId) + ""String_Node_Str"");
    return;
  }
  Set<String> hs=new HashSet<String>(rfis.size());
  for (  ReplicaFileInfo rfi : rfis) {
    if (rfi.getFileListState() == FileListStatus.OK) {
      hs.add(rfi.getChecksum());
    }
  }
  if (hs.size() == 0) {
    String errMsg=""String_Node_Str"" + fileId + ""String_Node_Str"";
    log.warn(errMsg);
    NotificationsFactory.getInstance().errorEvent(errMsg);
    return;
  }
  if (hs.size() == 1) {
    log.trace(""String_Node_Str"" + fileId + ""String_Node_Str"");
    for (    ReplicaFileInfo rfi : rfis) {
      updateReplicaFileInfoChecksumOk(rfi.getGuid());
    }
    return;
  }
  int[] csCount=new int[hs.size()];
  String[] uniqueCs=hs.toArray(new String[hs.size()]);
  for (  ReplicaFileInfo rfi : rfis) {
    if (rfi.getFileListState() == FileListStatus.OK) {
      for (int i=0; i < hs.size(); i++) {
        if (rfi.getChecksum().equals(uniqueCs[i])) {
          csCount[i]++;
        }
      }
    }
  }
  int largest=0;
  boolean unique=false;
  int index=-1;
  for (int i=0; i < csCount.length; i++) {
    if (csCount[i] > largest) {
      largest=csCount[i];
      unique=true;
      index=i;
    }
 else     if (csCount[i] == largest) {
      unique=false;
    }
  }
  if (unique) {
    for (    ReplicaFileInfo rfi : rfis) {
      if (!rfi.getChecksum().equals(uniqueCs[index])) {
        updateReplicaFileInfoChecksumCorrupt(rfi.getGuid());
      }
 else {
        updateReplicaFileInfoChecksumOk(rfi.getGuid());
      }
    }
  }
 else {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + retrieveFilenameForFileId(fileId) + ""String_Node_Str"";
    log.error(errMsg);
    NotificationsFactory.getInstance().errorEvent(errMsg);
    for (    ReplicaFileInfo rfi : rfis) {
      updateReplicaFileInfoChecksumUnknown(rfi.getGuid());
    }
  }
}",0.9429370169380036
89746,"/** 
 * Test that it correctly finds a changed file.
 * @throws Exception if error.
 */
public void testChangedFiles() throws Exception {
  ReplicaCacheDatabase.getInstance();
  dbabp=DatabaseBasedActiveBitPreservation.getInstance();
  Date date=dbabp.getDateForMissingFiles(THREE);
  assertTrue(""String_Node_Str"" + (Calendar.getInstance().getTimeInMillis() - date.getTime()),Calendar.getInstance().getTimeInMillis() - date.getTime() < 1000 * 60 * 30);
  dbabp.findMissingFiles(THREE);
  Date beforeUpdate=new Date(Calendar.getInstance().getTimeInMillis());
  dbabp.findChangedFiles(ONE);
  date=dbabp.getDateForChangedFiles(ONE);
  assertTrue(""String_Node_Str"" + beforeUpdate.getTime() + ""String_Node_Str""+ date.getTime(),date.getTime() > beforeUpdate.getTime());
  assertTrue(""String_Node_Str"" + Calendar.getInstance().getTimeInMillis() + ""String_Node_Str""+ date.getTime(),date.getTime() < Calendar.getInstance().getTimeInMillis());
  assertEquals(""String_Node_Str"" + THREE + ""String_Node_Str"",2,dbabp.getNumberOfChangedFiles(THREE));
  assertEquals(""String_Node_Str"" + THREE + ""String_Node_Str""+ ""String_Node_Str"",Arrays.asList(""String_Node_Str"",""String_Node_Str""),dbabp.getChangedFiles(THREE));
  dbabp.replaceChangedFile(ONE,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  dbabp.replaceChangedFile(THREE,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Map<String,PreservationState> presMap=dbabp.getPreservationStateMap(new String[]{""String_Node_Str""});
  assertEquals(""String_Node_Str"",1,presMap.size());
  PreservationState pres=presMap.get(""String_Node_Str"");
  assertNotNull(""String_Node_Str"",pres);
  assertEquals(""String_Node_Str"",ReplicaStoreState.UPLOAD_FAILED.toString(),pres.getAdminReplicaState(THREE));
  dbabp.findChangedFiles(THREE);
  pres=dbabp.getPreservationState(""String_Node_Str"");
  assertEquals(""String_Node_Str"",ReplicaStoreState.UPLOAD_COMPLETED.toString(),pres.getAdminReplicaState(THREE));
  try {
    dbabp.uploadMissingFiles(THREE,""String_Node_Str"");
    fail(""String_Node_Str"");
  }
 catch (  IOFailure e) {
  }
  List<String> filelist=new ArrayList<String>();
  filelist.add(""String_Node_Str"");
  filelist.add(""String_Node_Str"");
  filelist.add(""String_Node_Str"");
  ReplicaCacheDatabase.getInstance().addFileListInformation(filelist,THREE);
  String misFiles=dbabp.getMissingFiles(THREE).toString();
  assertTrue(""String_Node_Str"",misFiles.contains(""String_Node_Str""));
  assertTrue(""String_Node_Str"",misFiles.contains(""String_Node_Str""));
  dbabp.uploadMissingFiles(THREE,""String_Node_Str"",""String_Node_Str"");
  dbabp.findMissingFiles(THREE);
  misFiles=dbabp.getMissingFiles(THREE).toString();
  assertFalse(""String_Node_Str"",misFiles.contains(""String_Node_Str""));
  assertFalse(""String_Node_Str"",misFiles.contains(""String_Node_Str""));
}","/** 
 * Test that it correctly finds a changed file.
 * @throws Exception if error.
 */
public void testChangedFiles() throws Exception {
  ReplicaCacheDatabase.getInstance();
  dbabp=DatabaseBasedActiveBitPreservation.getInstance();
  Date date=dbabp.getDateForMissingFiles(THREE);
  assertTrue(""String_Node_Str"" + (Calendar.getInstance().getTimeInMillis() - date.getTime()),Calendar.getInstance().getTimeInMillis() - date.getTime() < 1000 * 60 * 30);
  dbabp.findMissingFiles(THREE);
  Date beforeUpdate=new Date(Calendar.getInstance().getTimeInMillis());
  dbabp.findChangedFiles(ONE);
  date=dbabp.getDateForChangedFiles(ONE);
  assertTrue(""String_Node_Str"" + beforeUpdate.getTime() + ""String_Node_Str""+ date.getTime(),date.getTime() > beforeUpdate.getTime());
  assertTrue(""String_Node_Str"" + Calendar.getInstance().getTimeInMillis() + ""String_Node_Str""+ date.getTime(),date.getTime() < Calendar.getInstance().getTimeInMillis());
  assertEquals(""String_Node_Str"" + THREE + ""String_Node_Str"",2,dbabp.getNumberOfChangedFiles(THREE));
  assertEquals(""String_Node_Str"" + THREE + ""String_Node_Str""+ ""String_Node_Str"",Arrays.asList(""String_Node_Str"",""String_Node_Str""),dbabp.getChangedFiles(THREE));
  dbabp.replaceChangedFile(ONE,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  dbabp.replaceChangedFile(THREE,""String_Node_Str"",""String_Node_Str"",""String_Node_Str"");
  Map<String,PreservationState> presMap=dbabp.getPreservationStateMap(new String[]{""String_Node_Str""});
  assertEquals(""String_Node_Str"",1,presMap.size());
  PreservationState pres=presMap.get(""String_Node_Str"");
  assertNotNull(""String_Node_Str"",pres);
  assertEquals(""String_Node_Str"",ReplicaStoreState.UPLOAD_FAILED.toString(),pres.getAdminReplicaState(THREE));
  dbabp.findChangedFiles(THREE);
  pres=dbabp.getPreservationState(""String_Node_Str"");
  assertEquals(""String_Node_Str"",ReplicaStoreState.UPLOAD_COMPLETED.toString(),pres.getAdminReplicaState(THREE));
  try {
    dbabp.uploadMissingFiles(THREE,""String_Node_Str"");
    fail(""String_Node_Str"");
  }
 catch (  IOFailure e) {
  }
  List<String> filelist=new ArrayList<String>();
  filelist.add(""String_Node_Str"");
  filelist.add(""String_Node_Str"");
  ReplicaCacheDatabase.getInstance().addFileListInformation(filelist,THREE);
  String misFiles=dbabp.getMissingFiles(THREE).toString();
  assertTrue(""String_Node_Str"",misFiles.contains(""String_Node_Str""));
  assertTrue(""String_Node_Str"",misFiles.contains(""String_Node_Str""));
  assertTrue(""String_Node_Str"",misFiles.contains(""String_Node_Str""));
  dbabp.uploadMissingFiles(THREE,""String_Node_Str"",""String_Node_Str"");
  try {
    dbabp.uploadMissingFiles(THREE,""String_Node_Str"");
    fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  IOFailure e) {
  }
  dbabp.findMissingFiles(THREE);
  misFiles=dbabp.getMissingFiles(THREE).toString();
  assertFalse(""String_Node_Str"",misFiles.contains(""String_Node_Str""));
  assertFalse(""String_Node_Str"",misFiles.contains(""String_Node_Str""));
}",0.9202772963604852
89747,"/** 
 * The method for voting about the checksum of a file. <br/> Each entry in the replicafileinfo table containing the file is retrieved. All the unique checksums are retrieved, e.g. if a checksum is found more than one, then it is ignored. <br/> If only one unique checksum is found, then if must be the correct one,  and all the replicas with this file will have their checksum_status set  to 'OK'. <br/> If more than one checksum is found, then a vote for the correct checksum is performed. This is done by counting the amount of time each of the  unique checksum is found among the replicafileinfo entries for the  current file. The checksum with most votes is chosen as the correct one, and the checksum_status for all the replicafileinfo entries with this  checksum is set to 'OK', whereas the replicafileinfo entries with a  different checksum is set to 'CORRUPT'. <br/>   If no winner is found then a warning and a notification is issued, and  the checksum_status for all the replicafileinfo entries with for the  current file is set to 'UNKNOWN'. <br/>
 * @param fileId The id for the file to vote about.
 */
private void fileChecksumVote(long fileId){
  List<Long> rfiGuids=retrieveReplicaFileInfoGuidsForFile(fileId);
  List<ReplicaFileInfo> rfis=retrieveReplicaFileInfosWithChecksum(rfiGuids);
  if (rfis.size() == 0) {
    log.warn(""String_Node_Str"" + retrieveFilenameForFileId(fileId) + ""String_Node_Str"");
    return;
  }
  Set<String> hs=new HashSet<String>(rfis.size());
  for (  ReplicaFileInfo rfi : rfis) {
    if (rfi.getFileListState() == FileListStatus.OK) {
      hs.add(rfi.getChecksum());
    }
  }
  if (hs.size() == 0) {
    log.warn(""String_Node_Str"" + retrieveFilenameForFileId(fileId) + ""String_Node_Str"");
    return;
  }
  if (hs.size() == 1) {
    log.trace(""String_Node_Str"" + fileId + ""String_Node_Str"");
    for (    ReplicaFileInfo rfi : rfis) {
      updateReplicaFileInfoChecksumOk(rfi.getGuid());
    }
    return;
  }
  List<String> checksums=new ArrayList<String>();
  for (  ReplicaFileInfo rfi : rfis) {
    if (rfi.getFileListState() == FileListStatus.OK) {
      checksums.add(rfi.getChecksum());
    }
  }
  String uniqueChecksum=vote(checksums);
  if (uniqueChecksum != null) {
    for (    ReplicaFileInfo rfi : rfis) {
      if (!rfi.getChecksum().equals(uniqueChecksum)) {
        updateReplicaFileInfoChecksumCorrupt(rfi.getGuid());
      }
 else {
        updateReplicaFileInfoChecksumOk(rfi.getGuid());
      }
    }
  }
 else {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + retrieveFilenameForFileId(fileId) + ""String_Node_Str"";
    log.error(errMsg);
    NotificationsFactory.getInstance().errorEvent(errMsg);
    for (    ReplicaFileInfo rfi : rfis) {
      updateReplicaFileInfoChecksumUnknown(rfi.getGuid());
    }
  }
}","/** 
 * The method for voting about the checksum of a file. <br/> Each entry in the replicafileinfo table containing the file is retrieved. All the unique checksums are retrieved, e.g. if a checksum is found more than one, then it is ignored. <br/> If only one unique checksum is found, then if must be the correct one,  and all the replicas with this file will have their checksum_status set  to 'OK'. <br/> If more than one checksum is found, then a vote for the correct checksum is performed. This is done by counting the amount of time each of the  unique checksum is found among the replicafileinfo entries for the  current file. The checksum with most votes is chosen as the correct one, and the checksum_status for all the replicafileinfo entries with this  checksum is set to 'OK', whereas the replicafileinfo entries with a  different checksum is set to 'CORRUPT'. <br/>   If no winner is found then a warning and a notification is issued, and  the checksum_status for all the replicafileinfo entries with for the  current file is set to 'UNKNOWN'. <br/>
 * @param fileId The id for the file to vote about.
 */
private void fileChecksumVote(long fileId){
  List<Long> rfiGuids=retrieveReplicaFileInfoGuidsForFile(fileId);
  List<ReplicaFileInfo> rfis=retrieveReplicaFileInfosWithChecksum(rfiGuids);
  if (rfis.size() == 0) {
    log.warn(""String_Node_Str"" + retrieveFilenameForFileId(fileId) + ""String_Node_Str"");
    return;
  }
  Set<String> hs=new HashSet<String>(rfis.size());
  for (  ReplicaFileInfo rfi : rfis) {
    if (rfi.getFileListState() == FileListStatus.OK) {
      hs.add(rfi.getChecksum());
    }
  }
  if (hs.size() == 0) {
    log.warn(""String_Node_Str"" + retrieveFilenameForFileId(fileId) + ""String_Node_Str"");
    return;
  }
  if (hs.size() == 1) {
    log.trace(""String_Node_Str"" + fileId + ""String_Node_Str"");
    for (    ReplicaFileInfo rfi : rfis) {
      if (rfi.getFileListState() == FileListStatus.OK) {
        updateReplicaFileInfoChecksumOk(rfi.getGuid());
      }
    }
    return;
  }
  List<String> checksums=new ArrayList<String>();
  for (  ReplicaFileInfo rfi : rfis) {
    if (rfi.getFileListState() == FileListStatus.OK) {
      checksums.add(rfi.getChecksum());
    }
  }
  String uniqueChecksum=vote(checksums);
  if (uniqueChecksum != null) {
    for (    ReplicaFileInfo rfi : rfis) {
      if (!rfi.getChecksum().equals(uniqueChecksum)) {
        updateReplicaFileInfoChecksumCorrupt(rfi.getGuid());
      }
 else {
        updateReplicaFileInfoChecksumOk(rfi.getGuid());
      }
    }
  }
 else {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + retrieveFilenameForFileId(fileId) + ""String_Node_Str"";
    log.error(errMsg);
    NotificationsFactory.getInstance().errorEvent(errMsg);
    for (    ReplicaFileInfo rfi : rfis) {
      updateReplicaFileInfoChecksumUnknown(rfi.getGuid());
    }
  }
}",0.9881520778072502
89748,"/** 
 * Given the output of a checksum job, add the results to the database. The following fields in the table are updated for each corresponding  entry in the replicafileinfo table: <br/> - checksum = the given checksum.  <br/> - filelist_status = ok. <br/> - filelist_checkdatetime = now. <br/> - checksum_checkdatetime = now.
 * @param checksumOutput The output of a checksum job.
 * @param replica The replica this checksum job is for.
 */
@Override public void addChecksumInformation(List<String> checksumOutput,Replica replica){
  ArgumentNotValid.checkNotNull(checksumOutput,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  if (!existsReplicaInDB(replica)) {
    String msg=""String_Node_Str"" + replica.toString() + ""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg);
  }
  log.info(""String_Node_Str"" + checksumOutput.size() + ""String_Node_Str""+ replica.getId());
  Collections.sort(checksumOutput);
  String lastFilename=""String_Node_Str"";
  String lastChecksum=""String_Node_Str"";
  for (  String line : checksumOutput) {
    KeyValuePair<String,String> entry=ChecksumJob.parseLine(line);
    String filename=entry.getKey();
    String checksum=entry.getValue();
    if (filename.equals(lastFilename)) {
      if (!checksum.equals(lastChecksum)) {
        String errMsg=""String_Node_Str"" + filename + ""String_Node_Str""+ lastChecksum+ ""String_Node_Str""+ checksum+ ""String_Node_Str"";
        log.error(errMsg);
        NotificationsFactory.getInstance().errorEvent(errMsg);
      }
 else {
        log.debug(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str""+ checksum+ ""String_Node_Str"");
      }
      continue;
    }
    lastFilename=filename;
    lastChecksum=checksum;
    long fileid=-1;
    if (!existsFileInDB(filename)) {
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      fileid=insertFileIntoDB(filename);
    }
 else {
      fileid=retrieveIdForFile(filename);
    }
    if (fileid < 0) {
      log.warn(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      fileid=insertFileIntoDB(filename);
    }
    long rfiId=retrieveReplicaFileInfoGuid(fileid,replica.getId());
    if (rfiId < 0) {
      createReplicaFileInfoEntriesInDB(fileid);
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
    }
    updateReplicaFileInfoChecksum(rfiId,checksum);
    log.trace(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
  }
  updateChecksumDateForReplica(replica);
  updateFilelistDateForReplica(replica);
  log.info(""String_Node_Str"" + checksumOutput.size() + ""String_Node_Str""+ replica.getId());
}","/** 
 * Given the output of a checksum job, add the results to the database. The following fields in the table are updated for each corresponding  entry in the replicafileinfo table: <br/> - checksum = the given checksum.  <br/> - filelist_status = ok. <br/> - filelist_checkdatetime = now. <br/> - checksum_checkdatetime = now.
 * @param checksumOutput The output of a checksum job.
 * @param replica The replica this checksum job is for.
 */
@Override public void addChecksumInformation(List<String> checksumOutput,Replica replica){
  ArgumentNotValid.checkNotNull(checksumOutput,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  if (!existsReplicaInDB(replica)) {
    String msg=""String_Node_Str"" + replica.toString() + ""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg);
  }
  log.info(""String_Node_Str"" + checksumOutput.size() + ""String_Node_Str""+ replica.getId());
  Collections.sort(checksumOutput);
  List<Long> missingReplicaRFIs=retrieveReplicaFileInfoGuidsForReplica(replica.getId());
  String lastFilename=""String_Node_Str"";
  String lastChecksum=""String_Node_Str"";
  for (  String line : checksumOutput) {
    KeyValuePair<String,String> entry=ChecksumJob.parseLine(line);
    String filename=entry.getKey();
    String checksum=entry.getValue();
    if (filename.equals(lastFilename)) {
      if (!checksum.equals(lastChecksum)) {
        String errMsg=""String_Node_Str"" + filename + ""String_Node_Str""+ lastChecksum+ ""String_Node_Str""+ checksum+ ""String_Node_Str"";
        log.error(errMsg);
        NotificationsFactory.getInstance().errorEvent(errMsg);
      }
 else {
        log.debug(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str""+ checksum+ ""String_Node_Str"");
      }
      continue;
    }
    lastFilename=filename;
    lastChecksum=checksum;
    long fileid=-1;
    if (!existsFileInDB(filename)) {
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      fileid=insertFileIntoDB(filename);
    }
 else {
      fileid=retrieveIdForFile(filename);
    }
    if (fileid < 0) {
      log.warn(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      fileid=insertFileIntoDB(filename);
    }
    long rfiId=retrieveReplicaFileInfoGuid(fileid,replica.getId());
    if (rfiId < 0) {
      createReplicaFileInfoEntriesInDB(fileid);
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
    }
    updateReplicaFileInfoChecksum(rfiId,checksum);
    log.trace(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
    missingReplicaRFIs.remove(rfiId);
  }
  if (missingReplicaRFIs.size() > 0) {
    log.warn(""String_Node_Str"" + missingReplicaRFIs.size() + ""String_Node_Str""+ ""String_Node_Str""+ replica+ ""String_Node_Str"");
    for (    long rfi : missingReplicaRFIs) {
      updateReplicaFileInfoMissingFromFilelist(rfi);
    }
  }
  updateChecksumDateForReplica(replica);
  updateFilelistDateForReplica(replica);
  log.info(""String_Node_Str"" + checksumOutput.size() + ""String_Node_Str""+ replica.getId());
}",0.9311699402220324
89749,"public void setUp(){
  rs.setUp();
  mjms.setUp();
  listener=new GetFileListener(mtf.working(new File(TestInfo.DATA_DIR,""String_Node_Str"")));
  JMSConnectionFactory.getInstance().setListener(Channels.getTheRepos(),listener);
  Settings.set(CommonSettings.REMOTE_FILE_CLASS,""String_Node_Str"");
  mtf.setUp();
  pss.setUp();
  pse.setUp();
}","public void setUp(){
  ChannelsTester.resetChannels();
  rs.setUp();
  mjms.setUp();
  listener=new GetFileListener(mtf.working(new File(TestInfo.DATA_DIR,""String_Node_Str"")));
  JMSConnectionFactory.getInstance().setListener(Channels.getTheRepos(),listener);
  Settings.set(CommonSettings.REMOTE_FILE_CLASS,""String_Node_Str"");
  mtf.setUp();
  pss.setUp();
  pse.setUp();
}",0.9523809523809524
89750,"/** 
 * Retrieves and update the status of a file for a specific replica.
 * @param filename The name of the file.
 * @param replica The replica to retrieve and update the file status for.
 */
private void updateChecksumStatus(String filename,Replica replica){
  String checksum=ArcRepositoryClientFactory.getPreservationInstance().getChecksum(replica.getId(),filename);
  cache.insertSingleChecksumResult(filename,checksum,replica);
}","/** 
 * Retrieves and update the status of a file for a specific replica.
 * @param filename The name of the file.
 * @param replica The replica to retrieve and update the file status for.
 */
private void updateChecksumStatus(String filename){
  for (  Replica replica : Replica.getKnown()) {
    String checksum=ArcRepositoryClientFactory.getPreservationInstance().getChecksum(replica.getId(),filename);
    cache.insertSingleChecksumResult(filename,checksum,replica);
  }
  cache.updateChecksumStatus(filename);
}",0.91062039957939
89751,"/** 
 * Method for retrieving the FilePreservationState for a specific file.
 * @param filename The name of the file for whom the FilePreservationStateshould be retrieved.
 * @return The FilePreservationState for the file.
 * @throws ArgumentNotValid If the filename is null or the empty string.
 */
@Override public PreservationState getPreservationState(String filename) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  List<ReplicaFileInfo> rfis=new ArrayList<ReplicaFileInfo>(Replica.getKnown().size());
  for (  Replica replica : Replica.getKnown()) {
    updateChecksumStatus(filename,replica);
    rfis.add(cache.getReplicaFileInfo(filename,replica));
  }
  return new DatabasePreservationState(filename,rfis);
}","/** 
 * Method for retrieving the FilePreservationState for a specific file.
 * @param filename The name of the file for whom the FilePreservationStateshould be retrieved.
 * @return The FilePreservationState for the file.
 * @throws ArgumentNotValid If the filename is null or the empty string.
 */
@Override public PreservationState getPreservationState(String filename) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  List<ReplicaFileInfo> rfis=new ArrayList<ReplicaFileInfo>(Replica.getKnown().size());
  updateChecksumStatus(filename);
  for (  Replica replica : Replica.getKnown()) {
    rfis.add(cache.getReplicaFileInfo(filename,replica));
  }
  return new DatabasePreservationState(filename,rfis);
}",0.8475689881734559
89752,"/** 
 * Method for updating the status for the files for all the replicas. If the checksums of the archives differ for some replicas, then based on a checksum vote, a specific checksum is chosen as the 'correct' one, and the entries with another checksum that this 'correct' one will be marked as corrupt.
 */
void updateChecksumStatus();","/** 
 * Method for updating the status for a specific file for all the replicas. If the checksums for the replicas differ for some replica, then based on  a checksum vote, a specific checksum is chosen as the 'correct' one, and the entries with another checksum than the 'correct one' will be marked as corrupt.
 * @param filename The name of the file to update the status for.
 */
void updateChecksumStatus(String filename);",0.3197903014416776
89753,"/** 
 * This method is used to update the status for the checksums for all  replicafileinfo entries. <br/> <br/> For each file in the database, the checksum vote is made in the  following way. <br/> Each entry in the replicafileinfo table containing the file is retrieved. All the unique checksums are retrieved, e.g. if a checksum is found more than one, then it is ignored. <br/> If only one unique checksum is found, then if must be the correct one,  and all the replicas with this file will have their checksum_status set  to 'OK'. <br/> If more than one checksum is found, then a vote for the correct checksum is performed. This is done by counting the amount of time each of the  unique checksum is found among the replicafileinfo entries for the  current file. The checksum with most votes is chosen as the correct one, and the checksum_status for all the replicafileinfo entries with this  checksum is set to 'OK', whereas the replicafileinfo entries with a  different checksum is set to 'CORRUPT'. <br/>   If no winner is found then a warning and a notification is issued, and  the checksum_status for all the replicafileinfo entries with for the  current file is set to 'UNKNOWN'. <br/>
 */
public void updateChecksumStatus(){
  List<Long> fileIds=retrieveAllFileIds();
  for (  long fileId : fileIds) {
    fileChecksumVote(fileId);
  }
}","/** 
 * Method for updating the status for a specific file for all the replicas. If the checksums for the replicas differ for some replica, then based on  a checksum vote, a specific checksum is chosen as the 'correct' one, and the entries with another checksum than the 'correct one' will be marked as corrupt.
 * @param filename The name of the file to update the status for.
 */
@Override public void updateChecksumStatus(String filename){
  Long fileId=retrieveIdForFile(filename);
  fileChecksumVote(fileId);
}",0.1158798283261802
89754,"/** 
 * Method for retrieving the checksum for a specific file. Since a file is not directly attached with a checksum, the checksum of  a file must be found by having the replicafileinfo entries for the file vote about it.
 * @param filename The name of the file, whose checksum are to be found.
 * @return The checksum of the file, or a Null if no validated checksumcan be found.
 * @throws ArgumentNotValid If teh filename is either null or the empty string.
 */
public String getChecksum(String filename) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  long fileId=retrieveIdForFile(filename);
  for (  Replica rep : Replica.getKnown()) {
    if (retrieveChecksumStatusForReplicaFileInfoEntry(fileId,rep.getId()) == ChecksumStatus.OK) {
      return retrieveChecksumForReplicaFileInfoEntry(fileId,rep.getId());
    }
  }
  log.debug(""String_Node_Str"" + filename + ""String_Node_Str"");
  fileChecksumVote(fileId);
  for (  Replica rep : Replica.getKnown()) {
    if (retrieveChecksumStatusForReplicaFileInfoEntry(fileId,rep.getId()) == ChecksumStatus.OK) {
      return retrieveChecksumForReplicaFileInfoEntry(fileId,rep.getId());
    }
  }
  log.warn(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
  return null;
}","/** 
 * Method for retrieving the checksum for a specific file. Since a file is not directly attached with a checksum, the checksum of  a file must be found by having the replicafileinfo entries for the file vote about it.
 * @param filename The name of the file, whose checksum are to be found.
 * @return The checksum of the file, or a Null if no validated checksumcan be found.
 * @throws ArgumentNotValid If teh filename is either null or the empty string.
 */
public String getChecksum(String filename) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  long fileId=retrieveIdForFile(filename);
  for (  Replica rep : Replica.getKnown()) {
    if (retrieveChecksumStatusForReplicaFileInfoEntry(fileId,rep.getId()) == ChecksumStatus.OK) {
      return retrieveChecksumForReplicaFileInfoEntry(fileId,rep.getId());
    }
  }
  log.debug(""String_Node_Str"" + filename + ""String_Node_Str"");
  Set<String> checksums=new HashSet<String>();
  for (  Replica rep : Replica.getKnown()) {
    if (retrieveChecksumStatusForReplicaFileInfoEntry(fileId,rep.getId()) != ChecksumStatus.CORRUPT) {
      String tmpChecksum=retrieveChecksumForReplicaFileInfoEntry(fileId,rep.getId());
      if (tmpChecksum != null) {
        checksums.add(tmpChecksum);
      }
    }
  }
  if (checksums.size() == 1) {
    return checksums.iterator().next();
  }
  if (checksums.size() == 0) {
    log.warn(""String_Node_Str"" + filename + ""String_Node_Str"");
    return null;
  }
  log.info(""String_Node_Str"" + filename + ""String_Node_Str"");
  List<String> checksumList=new ArrayList<String>();
  for (  Replica rep : Replica.getKnown()) {
    String cs=retrieveChecksumForReplicaFileInfoEntry(fileId,rep.getId());
    if (cs != null) {
      checksumList.add(cs);
    }
  }
  return vote(checksumList);
}",0.6934964584674823
89755,"/** 
 * The method for voting about the checksum of a file. <br/> Each entry in the replicafileinfo table containing the file is retrieved. All the unique checksums are retrieved, e.g. if a checksum is found more than one, then it is ignored. <br/> If only one unique checksum is found, then if must be the correct one,  and all the replicas with this file will have their checksum_status set  to 'OK'. <br/> If more than one checksum is found, then a vote for the correct checksum is performed. This is done by counting the amount of time each of the  unique checksum is found among the replicafileinfo entries for the  current file. The checksum with most votes is chosen as the correct one, and the checksum_status for all the replicafileinfo entries with this  checksum is set to 'OK', whereas the replicafileinfo entries with a  different checksum is set to 'CORRUPT'. <br/>   If no winner is found then a warning and a notification is issued, and  the checksum_status for all the replicafileinfo entries with for the  current file is set to 'UNKNOWN'. <br/>
 * @param fileId The id for the file to vote about.
 */
private void fileChecksumVote(long fileId){
  List<Long> rfiGuids=retrieveReplicaFileInfoGuidsForFile(fileId);
  List<ReplicaFileInfo> rfis=retrieveReplicaFileInfosWithChecksum(rfiGuids);
  if (rfis.size() == 0) {
    log.warn(""String_Node_Str"" + retrieveFilenameForFileId(fileId) + ""String_Node_Str"");
    return;
  }
  Set<String> hs=new HashSet<String>(rfis.size());
  for (  ReplicaFileInfo rfi : rfis) {
    if (rfi.getFileListState() == FileListStatus.OK) {
      hs.add(rfi.getChecksum());
    }
  }
  if (hs.size() == 0) {
    String errMsg=""String_Node_Str"" + fileId + ""String_Node_Str"";
    log.warn(errMsg);
    NotificationsFactory.getInstance().errorEvent(errMsg);
    return;
  }
  if (hs.size() == 1) {
    log.trace(""String_Node_Str"" + fileId + ""String_Node_Str"");
    for (    ReplicaFileInfo rfi : rfis) {
      updateReplicaFileInfoChecksumOk(rfi.getGuid());
    }
    return;
  }
  int[] csCount=new int[hs.size()];
  String[] uniqueCs=hs.toArray(new String[hs.size()]);
  for (  ReplicaFileInfo rfi : rfis) {
    if (rfi.getFileListState() == FileListStatus.OK) {
      for (int i=0; i < hs.size(); i++) {
        if (rfi.getChecksum().equals(uniqueCs[i])) {
          csCount[i]++;
        }
      }
    }
  }
  int largest=0;
  boolean unique=false;
  int index=-1;
  for (int i=0; i < csCount.length; i++) {
    if (csCount[i] > largest) {
      largest=csCount[i];
      unique=true;
      index=i;
    }
 else     if (csCount[i] == largest) {
      unique=false;
    }
  }
  if (unique) {
    for (    ReplicaFileInfo rfi : rfis) {
      if (!rfi.getChecksum().equals(uniqueCs[index])) {
        updateReplicaFileInfoChecksumCorrupt(rfi.getGuid());
      }
 else {
        updateReplicaFileInfoChecksumOk(rfi.getGuid());
      }
    }
  }
 else {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + retrieveFilenameForFileId(fileId) + ""String_Node_Str"";
    log.error(errMsg);
    NotificationsFactory.getInstance().errorEvent(errMsg);
    for (    ReplicaFileInfo rfi : rfis) {
      updateReplicaFileInfoChecksumUnknown(rfi.getGuid());
    }
  }
}","/** 
 * The method for voting about the checksum of a file. <br/> Each entry in the replicafileinfo table containing the file is retrieved. All the unique checksums are retrieved, e.g. if a checksum is found more than one, then it is ignored. <br/> If only one unique checksum is found, then if must be the correct one,  and all the replicas with this file will have their checksum_status set  to 'OK'. <br/> If more than one checksum is found, then a vote for the correct checksum is performed. This is done by counting the amount of time each of the  unique checksum is found among the replicafileinfo entries for the  current file. The checksum with most votes is chosen as the correct one, and the checksum_status for all the replicafileinfo entries with this  checksum is set to 'OK', whereas the replicafileinfo entries with a  different checksum is set to 'CORRUPT'. <br/>   If no winner is found then a warning and a notification is issued, and  the checksum_status for all the replicafileinfo entries with for the  current file is set to 'UNKNOWN'. <br/>
 * @param fileId The id for the file to vote about.
 */
private void fileChecksumVote(long fileId){
  List<Long> rfiGuids=retrieveReplicaFileInfoGuidsForFile(fileId);
  List<ReplicaFileInfo> rfis=retrieveReplicaFileInfosWithChecksum(rfiGuids);
  if (rfis.size() == 0) {
    log.warn(""String_Node_Str"" + retrieveFilenameForFileId(fileId) + ""String_Node_Str"");
    return;
  }
  Set<String> hs=new HashSet<String>(rfis.size());
  for (  ReplicaFileInfo rfi : rfis) {
    if (rfi.getFileListState() == FileListStatus.OK) {
      hs.add(rfi.getChecksum());
    }
  }
  if (hs.size() == 0) {
    log.warn(""String_Node_Str"" + retrieveFilenameForFileId(fileId) + ""String_Node_Str"");
    return;
  }
  if (hs.size() == 1) {
    log.trace(""String_Node_Str"" + fileId + ""String_Node_Str"");
    for (    ReplicaFileInfo rfi : rfis) {
      updateReplicaFileInfoChecksumOk(rfi.getGuid());
    }
    return;
  }
  List<String> checksums=new ArrayList<String>();
  for (  ReplicaFileInfo rfi : rfis) {
    if (rfi.getFileListState() == FileListStatus.OK) {
      checksums.add(rfi.getChecksum());
    }
  }
  String uniqueChecksum=vote(checksums);
  if (uniqueChecksum != null) {
    for (    ReplicaFileInfo rfi : rfis) {
      if (!rfi.getChecksum().equals(uniqueChecksum)) {
        updateReplicaFileInfoChecksumCorrupt(rfi.getGuid());
      }
 else {
        updateReplicaFileInfoChecksumOk(rfi.getGuid());
      }
    }
  }
 else {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + retrieveFilenameForFileId(fileId) + ""String_Node_Str"";
    log.error(errMsg);
    NotificationsFactory.getInstance().errorEvent(errMsg);
    for (    ReplicaFileInfo rfi : rfis) {
      updateReplicaFileInfoChecksumUnknown(rfi.getGuid());
    }
  }
}",0.6005004170141784
89756,"public void tearDown(){
  JMSConnectionMockupMQ.clearTestQueues();
}","public void tearDown(){
  JMSConnectionMockupMQ.clearTestQueues();
  ChannelsTester.resetChannels();
}",0.8
89757,"/** 
 * Returns a list of either all active or all inactive trap lists.
 * @param isActive whether to return active or inactive lists.
 * @return  a list if global crawler trap lists.
 */
private List<GlobalCrawlerTrapList> getAllByActivity(boolean isActive){
  List<GlobalCrawlerTrapList> result=new ArrayList<GlobalCrawlerTrapList>();
  Connection conn=DBConnect.getDBConnection();
  PreparedStatement stmt=null;
  try {
    stmt=conn.prepareStatement(SELECT_BY_ACTIVITY);
    stmt.setBoolean(1,isActive);
    ResultSet rs=stmt.executeQuery();
    while (rs.next()) {
      result.add(read(rs.getInt(1)));
    }
    return result;
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new UnknownID(message,e);
  }
}","/** 
 * Returns a list of either all active or all inactive trap lists.
 * @param isActive whether to return active or inactive lists.
 * @return  a list if global crawler trap lists.
 */
private List<GlobalCrawlerTrapList> getAllByActivity(boolean isActive){
  List<GlobalCrawlerTrapList> result=new ArrayList<GlobalCrawlerTrapList>();
  Connection conn=DBConnect.getDBConnection();
  PreparedStatement stmt=null;
  try {
    stmt=conn.prepareStatement(SELECT_BY_ACTIVITY);
    stmt.setBoolean(1,isActive);
    ResultSet rs=stmt.executeQuery();
    while (rs.next()) {
      result.add(read(rs.getInt(1)));
    }
    return result;
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new UnknownID(message,e);
  }
 finally {
    DBUtils.closeStatementIfOpen(stmt);
  }
}",0.9671641791044776
89758,"@Override public GlobalCrawlerTrapList read(int id){
  Connection conn=DBConnect.getDBConnection();
  PreparedStatement stmt=null;
  try {
    stmt=conn.prepareStatement(SELECT_TRAPLIST_STMT);
    stmt.setInt(1,id);
    ResultSet rs=stmt.executeQuery();
    if (!rs.next()) {
      throw new UnknownID(""String_Node_Str"" + id + ""String_Node_Str"");
    }
    String name=rs.getString(""String_Node_Str"");
    String description=rs.getString(""String_Node_Str"");
    boolean isActive=rs.getBoolean(""String_Node_Str"");
    stmt=conn.prepareStatement(SELECT_TRAP_EXPRESSIONS_STMT);
    stmt.setInt(1,id);
    rs=stmt.executeQuery();
    List<String> exprs=new ArrayList<String>();
    while (rs.next()) {
      exprs.add(rs.getString(""String_Node_Str""));
    }
    return new GlobalCrawlerTrapList(id,exprs,name,description,isActive);
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + id + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
}","@Override public GlobalCrawlerTrapList read(int id){
  Connection conn=DBConnect.getDBConnection();
  PreparedStatement stmt=null;
  try {
    stmt=conn.prepareStatement(SELECT_TRAPLIST_STMT);
    stmt.setInt(1,id);
    ResultSet rs=stmt.executeQuery();
    if (!rs.next()) {
      throw new UnknownID(""String_Node_Str"" + id + ""String_Node_Str"");
    }
    String name=rs.getString(""String_Node_Str"");
    String description=rs.getString(""String_Node_Str"");
    boolean isActive=rs.getBoolean(""String_Node_Str"");
    stmt=conn.prepareStatement(SELECT_TRAP_EXPRESSIONS_STMT);
    stmt.setInt(1,id);
    rs=stmt.executeQuery();
    List<String> exprs=new ArrayList<String>();
    while (rs.next()) {
      exprs.add(rs.getString(""String_Node_Str""));
    }
    return new GlobalCrawlerTrapList(id,exprs,name,description,isActive);
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + id + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.closeStatementIfOpen(stmt);
  }
}",0.9739706578324656
89759,"@Override public List<String> getAllActiveTrapExpressions(){
  Connection conn=DBConnect.getDBConnection();
  List<String> result=new ArrayList<String>();
  try {
    PreparedStatement stmt=conn.prepareStatement(SELECT_EXPRS_STMT);
    ResultSet rs=stmt.executeQuery();
    while (rs.next()) {
      result.add(rs.getString(1));
    }
    return result;
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
}","@Override public List<String> getAllActiveTrapExpressions(){
  Connection conn=DBConnect.getDBConnection();
  List<String> result=new ArrayList<String>();
  PreparedStatement stmt=null;
  try {
    stmt=conn.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    stmt.setBoolean(1,true);
    ResultSet rs=stmt.executeQuery();
    while (rs.next()) {
      result.add(rs.getString(1));
    }
    return result;
  }
 catch (  SQLException e) {
    String message=""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
 finally {
    DBUtils.closeStatementIfOpen(stmt);
  }
}",0.7898089171974523
89760,"/** 
 * Get a list of URLs as calculated by the Heritrix tests.
 * @return The list of URLs.
 */
private URL[] classPathAsURLS(){
  URL[] urls=new URL[0];
  try {
    Method method=ReflectUtils.getPrivateMethod(JMXHeritrixController.class,""String_Node_Str"",Map.class);
    Map<String,String> environment=new HashMap<String,String>(System.getenv());
    method.invoke(null,environment);
    String[] urlStrings=environment.get(""String_Node_Str"").split(""String_Node_Str"");
    urls=new URL[urlStrings.length];
    for (int i=0; i < urlStrings.length; i++) {
      urls[i]=new URL(""String_Node_Str"" + urlStrings[i]);
    }
  }
 catch (  NoSuchMethodException e) {
    e.printStackTrace(System.err);
    fail(""String_Node_Str"" + e.getMessage());
  }
catch (  IllegalAccessException e) {
    e.printStackTrace(System.err);
    fail(""String_Node_Str"" + e.getMessage());
  }
catch (  InvocationTargetException e) {
    e.printStackTrace(System.err);
    fail(""String_Node_Str"" + e.getMessage());
  }
catch (  MalformedURLException e) {
    e.printStackTrace(System.err);
    fail(""String_Node_Str"" + e.getMessage());
  }
  return urls;
}","/** 
 * Get a list of URLs as calculated by the Heritrix tests.
 * @return The list of URLs.
 */
private URL[] classPathAsURLS(){
  URL[] urls=new URL[0];
  try {
    Method method=ReflectUtils.getPrivateMethod(AbstractJMXHeritrixController.class,""String_Node_Str"",Map.class);
    Map<String,String> environment=new HashMap<String,String>(System.getenv());
    method.invoke(null,environment);
    String[] urlStrings=environment.get(""String_Node_Str"").split(""String_Node_Str"");
    urls=new URL[urlStrings.length];
    for (int i=0; i < urlStrings.length; i++) {
      urls[i]=new URL(""String_Node_Str"" + urlStrings[i]);
    }
  }
 catch (  NoSuchMethodException e) {
    e.printStackTrace(System.err);
    fail(""String_Node_Str"" + e.getMessage());
  }
catch (  IllegalAccessException e) {
    e.printStackTrace(System.err);
    fail(""String_Node_Str"" + e.getMessage());
  }
catch (  InvocationTargetException e) {
    e.printStackTrace(System.err);
    fail(""String_Node_Str"" + e.getMessage());
  }
catch (  MalformedURLException e) {
    e.printStackTrace(System.err);
    fail(""String_Node_Str"" + e.getMessage());
  }
  return urls;
}",0.9964726631393298
89761,"public static Test suite(){
  TestSuite suite;
  suite=new TestSuite(""String_Node_Str"");
  addToSuite(suite);
  return suite;
}","public static Test suite(){
  TestSuite suite;
  suite=new TestSuite(HeritrixTesterSuite.class.getName());
  addToSuite(suite);
  return suite;
}",0.8529411764705882
89762,"public static void main(String[] args){
  String[] args2={""String_Node_Str"",""String_Node_Str""};
  TestRunner.main(args2);
}","public static void main(String[] args){
  String[] args2={""String_Node_Str"",HeritrixTesterSuite.class.getName()};
  TestRunner.main(args2);
}",0.8484848484848485
89763,"public static void addToSuite(TestSuite suite){
  suite.addTestSuite(HeritrixTests.class);
}","public static void addToSuite(TestSuite suite){
  suite.addTestSuite(dk.netarkivet.externalsoftware.HeritrixTests.class);
}",0.8558139534883721
89764,"/** 
 * Prints the header information for the webpages in the GUI. This includes the navigation menu, and links for changing the language.
 * @param title An internationalised title of the page.
 * @param context The context of the web page request.
 * @throws IOException if an error occurs during writing to output.
 */
public static void generateHeader(String title,PageContext context) throws IOException {
  ArgumentNotValid.checkNotNull(title,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(context,""String_Node_Str"");
  JspWriter out=context.getOut();
  String url=((HttpServletRequest)context.getRequest()).getRequestURL().toString();
  Locale locale=context.getResponse().getLocale();
  title=escapeHtmlValues(title);
  log.debug(""String_Node_Str"" + url + ""String_Node_Str""+ title+ ""String_Node_Str"");
  out.print(WEBPAGE_HEADER_TEMPLATE.replace(TITLE_PLACEHOLDER,title));
  out.print(""String_Node_Str"");
  generateNavigationTree(out,url,locale);
  out.print(""String_Node_Str"");
  generateLanguageLinks(out);
}","/** 
 * Prints the header information for the webpages in the GUI. This includes the navigation menu, and links for changing the language.
 * @param title An internationalised title of the page.
 * @param context The context of the web page request.
 * @param refreshInSeconds auto-refresh time in seconds
 * @throws IOException if an error occurs during writing to output.
 */
public static void generateHeader(String title,long refreshInSeconds,PageContext context) throws IOException {
  ArgumentNotValid.checkNotNull(title,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(context,""String_Node_Str"");
  JspWriter out=context.getOut();
  String url=((HttpServletRequest)context.getRequest()).getRequestURL().toString();
  Locale locale=context.getResponse().getLocale();
  title=escapeHtmlValues(title);
  log.debug(""String_Node_Str"" + url + ""String_Node_Str""+ title+ ""String_Node_Str"");
  out.print(WEBPAGE_HEADER_TEMPLATE_TOP);
  if (refreshInSeconds > 0) {
    out.print(WEBPAGE_HEADER_AUTOREFRESH.replace(TITLE_PLACEHOLDER,Long.toString(refreshInSeconds)));
  }
  out.print(WEBPAGE_HEADER_TEMPLATE_BOTTOM.replace(TITLE_PLACEHOLDER,title));
  out.print(""String_Node_Str"");
  generateNavigationTree(out,url,locale);
  out.print(""String_Node_Str"");
  generateLanguageLinks(out);
}",0.8860978778692075
89765,"/** 
 * Given the output of a checksum job, add the results to the database. The following fields in the table are updated for each corresponding  entry in the replicafileinfo table: <br/> - checksum = the given checksum.  <br/> - filelist_status = ok. <br/> - filelist_checkdatetime = now. <br/> - checksum_checkdatetime = now.
 * @param checksumOutput The output of a checksum job.
 * @param replica The replica this checksum job is for.
 */
@Override public void addChecksumInformation(List<String> checksumOutput,Replica replica){
  ArgumentNotValid.checkNotNull(checksumOutput,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  if (!existsReplicaInDB(replica)) {
    String msg=""String_Node_Str"" + replica.toString() + ""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg);
  }
  log.info(""String_Node_Str"" + checksumOutput.size() + ""String_Node_Str"");
  Collections.sort(checksumOutput);
  String lastFilename=""String_Node_Str"";
  String lastChecksum=""String_Node_Str"";
  for (  String line : checksumOutput) {
    KeyValuePair<String,String> entry=ChecksumJob.parseLine(line);
    String filename=entry.getKey();
    String checksum=entry.getValue();
    if (filename.equals(lastFilename)) {
      if (!checksum.equals(lastChecksum)) {
        String errMsg=""String_Node_Str"" + filename + ""String_Node_Str""+ lastChecksum+ ""String_Node_Str""+ checksum+ ""String_Node_Str"";
        log.error(errMsg);
        NotificationsFactory.getInstance().errorEvent(errMsg);
      }
 else {
        log.debug(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str""+ checksum+ ""String_Node_Str"");
      }
      continue;
    }
    lastFilename=filename;
    lastChecksum=checksum;
    long fileid=-1;
    if (!existsFileInDB(filename)) {
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      fileid=insertFileIntoDB(filename);
    }
 else {
      fileid=retrieveIdForFile(filename);
    }
    if (fileid < 0) {
      log.warn(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      fileid=insertFileIntoDB(filename);
    }
    long rfiId=retrieveReplicaFileInfoGuid(fileid,replica.getId());
    if (rfiId < 0) {
      createReplicaFileInfoEntriesInDB(fileid);
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
    }
    updateReplicaFileInfoChecksum(rfiId,checksum);
    log.trace(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
  }
  updateChecksumDateForReplica(replica);
  updateFilelistDateForReplica(replica);
  log.info(""String_Node_Str"" + checksumOutput.size() + ""String_Node_Str"");
}","/** 
 * Given the output of a checksum job, add the results to the database. The following fields in the table are updated for each corresponding  entry in the replicafileinfo table: <br/> - checksum = the given checksum.  <br/> - filelist_status = ok. <br/> - filelist_checkdatetime = now. <br/> - checksum_checkdatetime = now.
 * @param checksumOutput The output of a checksum job.
 * @param replica The replica this checksum job is for.
 */
@Override public void addChecksumInformation(List<String> checksumOutput,Replica replica){
  ArgumentNotValid.checkNotNull(checksumOutput,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  if (!existsReplicaInDB(replica)) {
    String msg=""String_Node_Str"" + replica.toString() + ""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg);
  }
  log.info(""String_Node_Str"" + checksumOutput.size() + ""String_Node_Str""+ replica.getId());
  Collections.sort(checksumOutput);
  String lastFilename=""String_Node_Str"";
  String lastChecksum=""String_Node_Str"";
  for (  String line : checksumOutput) {
    KeyValuePair<String,String> entry=ChecksumJob.parseLine(line);
    String filename=entry.getKey();
    String checksum=entry.getValue();
    if (filename.equals(lastFilename)) {
      if (!checksum.equals(lastChecksum)) {
        String errMsg=""String_Node_Str"" + filename + ""String_Node_Str""+ lastChecksum+ ""String_Node_Str""+ checksum+ ""String_Node_Str"";
        log.error(errMsg);
        NotificationsFactory.getInstance().errorEvent(errMsg);
      }
 else {
        log.debug(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str""+ checksum+ ""String_Node_Str"");
      }
      continue;
    }
    lastFilename=filename;
    lastChecksum=checksum;
    long fileid=-1;
    if (!existsFileInDB(filename)) {
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      fileid=insertFileIntoDB(filename);
    }
 else {
      fileid=retrieveIdForFile(filename);
    }
    if (fileid < 0) {
      log.warn(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      fileid=insertFileIntoDB(filename);
    }
    long rfiId=retrieveReplicaFileInfoGuid(fileid,replica.getId());
    if (rfiId < 0) {
      createReplicaFileInfoEntriesInDB(fileid);
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
    }
    updateReplicaFileInfoChecksum(rfiId,checksum);
    log.trace(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
  }
  updateChecksumDateForReplica(replica);
  updateFilelistDateForReplica(replica);
  log.info(""String_Node_Str"" + checksumOutput.size() + ""String_Node_Str""+ replica.getId());
}",0.9937246216315984
89766,"/** 
 * The file represented by WAYBACK_INDEXER_INITIAL_FILES is read line by line and each line is ingested as an already-indexed archive file
 */
private static void ingestInitialFiles(){
  File initialFile=null;
  try {
    initialFile=Settings.getFile(WaybackSettings.WAYBACK_INDEXER_INITIAL_FILES);
  }
 catch (  UnknownID e) {
    log.info(""String_Node_Str"");
    return;
  }
  if (!initialFile.isFile()) {
    throw new ArgumentNotValid(""String_Node_Str"" + initialFile.getAbsolutePath() + ""String_Node_Str"");
  }
  BufferedReader br=null;
  try {
    br=new BufferedReader(new FileReader(initialFile));
  }
 catch (  FileNotFoundException e) {
    throw new IOFailure(""String_Node_Str"" + initialFile + ""String_Node_Str"",e);
  }
  String fileName=null;
  ArchiveFileDAO dao=new ArchiveFileDAO();
  Date today=new Date();
  try {
    while ((fileName=br.readLine()) != null) {
      ArchiveFile archiveFile=new ArchiveFile();
      archiveFile.setFilename(fileName);
      archiveFile.setIndexed(true);
      archiveFile.setIndexedDate(today);
      if (!dao.exists(fileName)) {
        log.info(""String_Node_Str"" + fileName + ""String_Node_Str"");
        dao.create(archiveFile);
      }
    }
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
}","/** 
 * The file represented by WAYBACK_INDEXER_INITIAL_FILES is read line by line and each line is ingested as an already-indexed archive file
 */
private static void ingestInitialFiles(){
  String initialFileString=Settings.get(WaybackSettings.WAYBACK_INDEXER_INITIAL_FILES);
  if (""String_Node_Str"".equals(initialFileString)) {
    log.info(""String_Node_Str"");
    return;
  }
  File initialFile=null;
  try {
    initialFile=Settings.getFile(WaybackSettings.WAYBACK_INDEXER_INITIAL_FILES);
  }
 catch (  UnknownID e) {
    log.info(""String_Node_Str"");
    return;
  }
  if (!initialFile.isFile()) {
    throw new ArgumentNotValid(""String_Node_Str"" + initialFile.getAbsolutePath() + ""String_Node_Str"");
  }
  BufferedReader br=null;
  try {
    br=new BufferedReader(new FileReader(initialFile));
  }
 catch (  FileNotFoundException e) {
    throw new IOFailure(""String_Node_Str"" + initialFile + ""String_Node_Str"",e);
  }
  String fileName=null;
  ArchiveFileDAO dao=new ArchiveFileDAO();
  Date today=new Date();
  try {
    while ((fileName=br.readLine()) != null) {
      ArchiveFile archiveFile=new ArchiveFile();
      archiveFile.setFilename(fileName);
      archiveFile.setIndexed(true);
      archiveFile.setIndexedDate(today);
      if (!dao.exists(fileName)) {
        log.info(""String_Node_Str"" + fileName + ""String_Node_Str"");
        dao.create(archiveFile);
      }
    }
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
}",0.8859011627906976
89767,"/** 
 * Builds a query to fetch jobs according to selection criteria.
 * @param query the selection criteria.
 * @param count build a count query instead of selecting columns.
 * @return the proper SQL query.
 */
private String buildSqlQuery(HarvestStatusQuery query,boolean count){
  StringBuffer sql=new StringBuffer(""String_Node_Str"");
  if (count) {
    sql.append(""String_Node_Str"");
  }
 else {
    sql.append(""String_Node_Str"");
    sql.append(""String_Node_Str"");
    sql.append(""String_Node_Str"");
    sql.append(""String_Node_Str"");
    sql.append(""String_Node_Str"");
  }
  sql.append(""String_Node_Str"");
  sql.append(""String_Node_Str"");
  JobStatus[] jobStatuses=query.getSelectedJobStatuses();
  if (jobStatuses.length > 0) {
    if (jobStatuses.length == 1) {
      int statusOrdinal=jobStatuses[0].ordinal();
      sql.append(""String_Node_Str"");
      sql.append(statusOrdinal);
    }
 else {
      sql.append(""String_Node_Str"");
      sql.append(jobStatuses[0].ordinal());
      for (int i=1; i < jobStatuses.length; i++) {
        sql.append(""String_Node_Str"");
        sql.append(jobStatuses[i].ordinal());
      }
      sql.append(""String_Node_Str"");
    }
  }
  String harvestName=query.getHarvestName();
  if (!harvestName.isEmpty()) {
    if (harvestName.indexOf(HarvestStatusQuery.HARVEST_NAME_WILDCARD) == -1) {
      sql.append(""String_Node_Str"");
      sql.append(harvestName);
      sql.append(""String_Node_Str"");
    }
 else {
      String harvestNamePattern=harvestName.replaceAll(""String_Node_Str"",""String_Node_Str"");
      sql.append(""String_Node_Str"");
      sql.append(harvestNamePattern);
      sql.append(""String_Node_Str"");
    }
  }
  Long harvestRun=query.getHarvestRunNumber();
  if (harvestRun != null) {
    sql.append(""String_Node_Str"" + harvestRun);
  }
  Long harvestId=query.getHarvestId();
  if (harvestId != null) {
    sql.append(""String_Node_Str"" + harvestId);
  }
  long startDate=query.getStartDate();
  if (startDate != HarvestStatusQuery.DATE_NONE) {
    sql.append(""String_Node_Str"");
    sql.append(new java.sql.Date(startDate).toString());
    sql.append(""String_Node_Str"");
  }
  long endDate=query.getEndDate();
  if (endDate != HarvestStatusQuery.DATE_NONE) {
    sql.append(""String_Node_Str"");
    sql.append(new java.sql.Date(endDate).toString());
    sql.append(""String_Node_Str"");
  }
  if (!count) {
    sql.append(""String_Node_Str"");
    if (!query.isSortAscending()) {
      sql.append(""String_Node_Str"" + SORT_ORDER.DESC.name());
    }
 else {
      sql.append(""String_Node_Str"" + SORT_ORDER.ASC.name());
    }
    long pagesize=query.getPageSize();
    if (pagesize != HarvestStatusQuery.PAGE_SIZE_NONE) {
      sql.append(""String_Node_Str"" + DBSpecifics.getInstance().getOrderByLimitAndOffsetSubClause(pagesize,(query.getStartPageIndex() - 1) * pagesize));
    }
  }
  return sql.toString();
}","/** 
 * Builds a query to fetch jobs according to selection criteria.
 * @param query the selection criteria.
 * @param count build a count query instead of selecting columns.
 * @return the proper SQL query.
 */
private HarvestStatusQueryBuilder buildSqlQuery(HarvestStatusQuery query,boolean count){
  HarvestStatusQueryBuilder sq=new HarvestStatusQueryBuilder();
  StringBuffer sql=new StringBuffer(""String_Node_Str"");
  if (count) {
    sql.append(""String_Node_Str"");
  }
 else {
    sql.append(""String_Node_Str"");
    sql.append(""String_Node_Str"");
    sql.append(""String_Node_Str"");
    sql.append(""String_Node_Str"");
    sql.append(""String_Node_Str"");
  }
  sql.append(""String_Node_Str"");
  sql.append(""String_Node_Str"");
  JobStatus[] jobStatuses=query.getSelectedJobStatuses();
  if (jobStatuses.length > 0) {
    if (jobStatuses.length == 1) {
      int statusOrdinal=jobStatuses[0].ordinal();
      sql.append(""String_Node_Str"");
      sq.addParameter(Integer.class,statusOrdinal);
    }
 else {
      sql.append(""String_Node_Str"");
      sql.append(jobStatuses[0].ordinal());
      for (int i=1; i < jobStatuses.length; i++) {
        sql.append(""String_Node_Str"");
        sq.addParameter(Integer.class,jobStatuses[i].ordinal());
      }
      sql.append(""String_Node_Str"");
    }
  }
  String harvestName=query.getHarvestName();
  if (!harvestName.isEmpty()) {
    if (harvestName.indexOf(HarvestStatusQuery.HARVEST_NAME_WILDCARD) == -1) {
      sql.append(""String_Node_Str"");
      sq.addParameter(String.class,harvestName);
    }
 else {
      String harvestNamePattern=harvestName.replaceAll(""String_Node_Str"",""String_Node_Str"");
      sql.append(""String_Node_Str"");
      sq.addParameter(String.class,harvestNamePattern);
    }
  }
  Long harvestRun=query.getHarvestRunNumber();
  if (harvestRun != null) {
    sql.append(""String_Node_Str"");
    sq.addParameter(Long.class,harvestRun);
  }
  Long harvestId=query.getHarvestId();
  if (harvestId != null) {
    sql.append(""String_Node_Str"");
    sq.addParameter(Long.class,harvestId);
  }
  long startDate=query.getStartDate();
  if (startDate != HarvestStatusQuery.DATE_NONE) {
    sql.append(""String_Node_Str"");
    sq.addParameter(java.sql.Date.class,new java.sql.Date(startDate));
  }
  long endDate=query.getEndDate();
  if (endDate != HarvestStatusQuery.DATE_NONE) {
    sql.append(""String_Node_Str"");
    sq.addParameter(java.sql.Date.class,new java.sql.Date(endDate));
  }
  if (!count) {
    sql.append(""String_Node_Str"");
    if (!query.isSortAscending()) {
      sql.append(""String_Node_Str"" + SORT_ORDER.DESC.name());
    }
 else {
      sql.append(""String_Node_Str"" + SORT_ORDER.ASC.name());
    }
    long pagesize=query.getPageSize();
    if (pagesize != HarvestStatusQuery.PAGE_SIZE_NONE) {
      sql.append(""String_Node_Str"" + DBSpecifics.getInstance().getOrderByLimitAndOffsetSubClause(pagesize,(query.getStartPageIndex() - 1) * pagesize));
    }
  }
  sq.setSqlString(sql.toString());
  return sq;
}",0.891854893908282
89768,"public String getHarvestName(){
  return harvestName;
}","public String getHarvestName(){
  if (harvestName == null) {
    return ""String_Node_Str"";
  }
  return harvestName;
}",0.6358381502890174
89769,"/** 
 * Get the unique index server instance.
 * @return The instance;
 */
public static IndexServer getInstance(){
  if (instance == null) {
    instance=new IndexServer();
  }
  return instance;
}","/** 
 * Get the unique index server instance.
 * @return The instance;
 */
public static synchronized IndexServer getInstance(){
  if (instance == null) {
    instance=new IndexServer();
  }
  return instance;
}",0.9682151589242054
89770,"/** 
 * Instantiates the two handlers, and starts listening for requests. 
 */
public IndexServer(){
  FileBasedCache<Set<Long>> cdxCache=new CDXIndexCache();
  FileBasedCache<Set<Long>> dedupCrawlLogCache=new DedupCrawlLogIndexCache();
  FileBasedCache<Set<Long>> fullCrawlLogCache=new FullCrawlLogIndexCache();
  Set<Long> emptySet=new HashSet<Long>();
  cdxCache.getIndex(emptySet);
  dedupCrawlLogCache.getIndex(emptySet);
  fullCrawlLogCache.getIndex(emptySet);
  remoteServer=IndexRequestServer.getInstance();
  remoteServer.setHandler(RequestType.CDX,cdxCache);
  remoteServer.setHandler(RequestType.DEDUP_CRAWL_LOG,dedupCrawlLogCache);
  remoteServer.setHandler(RequestType.FULL_CRAWL_LOG,fullCrawlLogCache);
}","/** 
 * Instantiates the two handlers, and starts listening for requests. 
 */
protected IndexServer(){
  FileBasedCache<Set<Long>> cdxCache=new CDXIndexCache();
  FileBasedCache<Set<Long>> dedupCrawlLogCache=new DedupCrawlLogIndexCache();
  FileBasedCache<Set<Long>> fullCrawlLogCache=new FullCrawlLogIndexCache();
  Set<Long> emptySet=new HashSet<Long>();
  cdxCache.getIndex(emptySet);
  dedupCrawlLogCache.getIndex(emptySet);
  fullCrawlLogCache.getIndex(emptySet);
  remoteServer=IndexRequestServer.getInstance();
  remoteServer.setHandler(RequestType.CDX,cdxCache);
  remoteServer.setHandler(RequestType.DEDUP_CRAWL_LOG,dedupCrawlLogCache);
  remoteServer.setHandler(RequestType.FULL_CRAWL_LOG,fullCrawlLogCache);
}",0.9909659485753995
89771,"/** 
 * Process a single file.
 * @param job The job that does the processing
 * @param file The file to process
 * @param os Where to put the output.
 */
private void processFile(FileBatchJob job,final File file,OutputStream os){
  log.debug(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"");
  boolean success=false;
  try {
    success=job.processFile(file,os);
  }
 catch (  Exception e) {
    log.warn(""String_Node_Str"" + file + ""String_Node_Str""+ job,e);
  }
  job.noOfFilesProcessed++;
  if (!success) {
    job.filesFailed.add(file);
  }
}","/** 
 * Process a single file.
 * @param job The job that does the processing
 * @param file The file to process
 * @param os Where to put the output.
 */
private void processFile(FileBatchJob job,final File file,OutputStream os){
  log.trace(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"");
  boolean success=false;
  try {
    success=job.processFile(file,os);
  }
 catch (  Exception e) {
    log.warn(""String_Node_Str"" + file + ""String_Node_Str""+ job,e);
  }
  job.noOfFilesProcessed++;
  if (!success) {
    job.filesFailed.add(file);
  }
}",0.9910873440285204
89772,"/** 
 * Process one file stored in the bit archive.
 * @param file the file to be processed.
 * @param os   the OutputStream to which output should be written
 * @return true if the file was successfully processed, false otherwise
 */
public boolean processFile(File file,OutputStream os){
  log.debug(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"");
  ArgumentNotValid.checkNotNull(file,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(os,""String_Node_Str"");
  return loadedJob.processFile(file,os);
}","/** 
 * Process one file stored in the bit archive.
 * @param file the file to be processed.
 * @param os   the OutputStream to which output should be written
 * @return true if the file was successfully processed, false otherwise
 */
public boolean processFile(File file,OutputStream os){
  log.trace(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"");
  ArgumentNotValid.checkNotNull(file,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(os,""String_Node_Str"");
  return loadedJob.processFile(file,os);
}",0.9904030710172744
89773,"/** 
 * Extract the name of the replica (parameter Constants.BITARCHIVE_NAME_PARAM) and the type of update  requested (parameter Constants.UPDATE_TYPE_PARAM). The latter is set to to Constants.FIND_MISSING_FILES_OPTION if the request is to update missing files, or to Constants.CHECKSUM_OPTION if the request is to update the  checksum information.
 * @param context the current JSP context
 * @throws ForwardedToErrorPage if an unknown bitarchive or update type is posted, or one of the two required parameters are missing. 
 * @throws ArgumentNotValid If the context is null.
 * @return an I18N string telling which type of update has just been initiated. 
 */
public static String processUpdateRequest(PageContext context) throws ArgumentNotValid, ForwardedToErrorPage {
  ArgumentNotValid.checkNotNull(context,""String_Node_Str"");
  ServletRequest request=context.getRequest();
  String bitarchiveName=request.getParameter(Constants.BITARCHIVE_NAME_PARAM);
  if (bitarchiveName == null) {
    HTMLUtils.forwardWithErrorMessage(context,I18N,""String_Node_Str"",bitarchiveName);
    throw new ForwardedToErrorPage(""String_Node_Str"" + Constants.BITARCHIVE_NAME_PARAM + ""String_Node_Str"");
  }
  String updateTypeRequested=request.getParameter(Constants.UPDATE_TYPE_PARAM);
  if (updateTypeRequested == null) {
    HTMLUtils.forwardWithErrorMessage(context,I18N,""String_Node_Str"",updateTypeRequested);
    throw new ForwardedToErrorPage(""String_Node_Str"" + Constants.UPDATE_TYPE_PARAM + ""String_Node_Str"");
  }
  if (!Replica.isKnownReplicaName(bitarchiveName)) {
    HTMLUtils.forwardWithErrorMessage(context,I18N,""String_Node_Str"",bitarchiveName);
    throw new ForwardedToErrorPage(""String_Node_Str"" + bitarchiveName);
  }
  Replica bitarchive=Replica.getReplicaFromName(bitarchiveName);
  Locale l=context.getResponse().getLocale();
  String statusmessage=HTMLUtils.escapeHtmlValues(I18N.getString(l,""String_Node_Str"",updateTypeRequested,bitarchiveName));
  if (updateTypeRequested.equalsIgnoreCase(Constants.FIND_MISSING_FILES_OPTION)) {
    new BitpreservationUpdateThread(bitarchive,BitpreservationUpdateType.FINDMISSING).start();
    return statusmessage;
  }
 else   if (updateTypeRequested.equalsIgnoreCase(Constants.CHECKSUM_OPTION)) {
    new BitpreservationUpdateThread(bitarchive,BitpreservationUpdateType.CHECKSUM).start();
    return statusmessage;
  }
 else {
    HTMLUtils.forwardWithErrorMessage(context,I18N,""String_Node_Str"",updateTypeRequested);
    throw new ForwardedToErrorPage(""String_Node_Str"" + bitarchiveName);
  }
}","/** 
 * Extract the name of the replica (parameter Constants.BITARCHIVE_NAME_PARAM) and the type of update  requested (parameter Constants.UPDATE_TYPE_PARAM). The latter is set to to Constants.FIND_MISSING_FILES_OPTION if the request is to update  missing files, or to Constants.CHECKSUM_OPTION if the request is to  update the checksum information.
 * @param context the current JSP context
 * @throws ForwardedToErrorPage if an unknown bitarchive or update type is posted, or one of the two required parameters are missing. 
 * @throws ArgumentNotValid If the context is null.
 * @return an I18N string telling which type of update has just been initiated. 
 */
public static String processUpdateRequest(PageContext context) throws ArgumentNotValid, ForwardedToErrorPage {
  ArgumentNotValid.checkNotNull(context,""String_Node_Str"");
  ServletRequest request=context.getRequest();
  String bitarchiveName=request.getParameter(Constants.BITARCHIVE_NAME_PARAM);
  if (bitarchiveName == null) {
    HTMLUtils.forwardWithErrorMessage(context,I18N,""String_Node_Str"",Constants.BITARCHIVE_NAME_PARAM);
    throw new ForwardedToErrorPage(""String_Node_Str"" + Constants.BITARCHIVE_NAME_PARAM + ""String_Node_Str"");
  }
  String updateTypeRequested=request.getParameter(Constants.UPDATE_TYPE_PARAM);
  if (updateTypeRequested == null) {
    HTMLUtils.forwardWithErrorMessage(context,I18N,""String_Node_Str"",Constants.UPDATE_TYPE_PARAM);
    throw new ForwardedToErrorPage(""String_Node_Str"" + Constants.UPDATE_TYPE_PARAM + ""String_Node_Str"");
  }
  if (!Replica.isKnownReplicaName(bitarchiveName)) {
    HTMLUtils.forwardWithErrorMessage(context,I18N,""String_Node_Str"",bitarchiveName);
    throw new ForwardedToErrorPage(""String_Node_Str"" + bitarchiveName);
  }
  Replica bitarchive=Replica.getReplicaFromName(bitarchiveName);
  Locale l=context.getResponse().getLocale();
  String statusmessage=HTMLUtils.escapeHtmlValues(I18N.getString(l,""String_Node_Str"",updateTypeRequested,bitarchiveName));
  if (updateTypeRequested.equalsIgnoreCase(Constants.FIND_MISSING_FILES_OPTION)) {
    new BitpreservationUpdateThread(bitarchive,BitpreservationUpdateType.FINDMISSING).start();
    return statusmessage;
  }
 else   if (updateTypeRequested.equalsIgnoreCase(Constants.CHECKSUM_OPTION)) {
    new BitpreservationUpdateThread(bitarchive,BitpreservationUpdateType.CHECKSUM).start();
    return statusmessage;
  }
 else {
    HTMLUtils.forwardWithErrorMessage(context,I18N,""String_Node_Str"",updateTypeRequested);
    throw new ForwardedToErrorPage(""String_Node_Str"" + bitarchiveName);
  }
}",0.5052837573385519
89774,"/** 
 * Get a connection to our database. If a connection is already registered  to the current thread, checks that it is valid, and if not renews it.  Assumes that AutoCommit is true.
 * @param dbUrl The url to the database.
 * @return a connection to our database.
 * @throws IOFailure if we cannot connect to the database (or find thedriver).
 * @throws ArgumentNotValid If the dbUrl is either null or the empty string.
 */
public static Connection getDBConnection(String dbUrl) throws IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(dbUrl,""String_Node_Str"");
  try {
    int validityCheckTimeout=Settings.getInt(CommonSettings.DB_CONN_VALID_CHECK_TIMEOUT);
    Connection connection=connectionPool.get(Thread.currentThread());
    boolean renew=((connection == null) || (!connection.isValid(validityCheckTimeout)));
    if (renew) {
      Class.forName(DBSpecifics.getInstance().getDriverClassName());
      connection=DriverManager.getConnection(dbUrl);
      connectionPool.put(Thread.currentThread(),connection);
      log.info(""String_Node_Str"" + dbUrl + ""String_Node_Str""+ DBSpecifics.getInstance().getDriverClassName()+ ""String_Node_Str"");
    }
    return connection;
  }
 catch (  ClassNotFoundException e) {
    final String message=""String_Node_Str"" + DBSpecifics.getInstance().getDriverClassName() + ""String_Node_Str"";
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
catch (  SQLException e) {
    final String message=""String_Node_Str"" + dbUrl + ""String_Node_Str""+ DBSpecifics.getInstance().getDriverClassName()+ ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
}","/** 
 * Get a connection to our database. If a connection is already registered  to the current thread, checks that it is valid, and if not renews it.  This sets AutoCommit to false as part of getting a fresh connection.
 * @param dbUrl The url to the database.
 * @return a connection to our database.
 * @throws IOFailure if we cannot connect to the database (or find thedriver).
 * @throws ArgumentNotValid If the dbUrl is either null or the empty string.
 */
public static Connection getDBConnection(String dbUrl) throws IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(dbUrl,""String_Node_Str"");
  try {
    int validityCheckTimeout=Settings.getInt(CommonSettings.DB_CONN_VALID_CHECK_TIMEOUT);
    Connection connection=connectionPool.get(Thread.currentThread());
    boolean renew=((connection == null) || (!connection.isValid(validityCheckTimeout)));
    if (renew) {
      Class.forName(DBSpecifics.getInstance().getDriverClassName());
      connection=DriverManager.getConnection(dbUrl);
      connection.setAutoCommit(false);
      connectionPool.put(Thread.currentThread(),connection);
      log.info(""String_Node_Str"" + dbUrl + ""String_Node_Str""+ DBSpecifics.getInstance().getDriverClassName()+ ""String_Node_Str"");
    }
    return connection;
  }
 catch (  ClassNotFoundException e) {
    final String message=""String_Node_Str"" + DBSpecifics.getInstance().getDriverClassName() + ""String_Node_Str"";
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
catch (  SQLException e) {
    final String message=""String_Node_Str"" + dbUrl + ""String_Node_Str""+ DBSpecifics.getInstance().getDriverClassName()+ ""String_Node_Str""+ ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e);
    log.warn(message,e);
    throw new IOFailure(message,e);
  }
}",0.9613954818415784
89775,"/** 
 * The constructor. Initializes the time out for this job.
 */
public FileListJob(){
  batchJobTimeout=Constants.ONE_HOUR_IN_MILLIES;
}","/** 
 * The constructor. 
 */
public FileListJob(){
}",0.5492227979274611
89776,"/** 
 * Retrieves the names of all the files in the replica through a GetAllFilenamesMessage. This is the checksum archive alternative to running a FilelistBatchJob.
 * @param replicaId The id of the replica from which the list of filenamesshould be retrieved.
 * @return A file with all the filenames within the archive of the givenreplica. A null is returned if the message timeout.
 * @throws IOFailure If the reply is not of type GetAllFilenamesMessageor if the file could not properly be retrieved from the reply message
 * @throws ArgumentNotValid If the replicaId is null or empty. 
 * @see dk.netarkivet.archive.checksum.distribute.GetAllFilenamesMessage
 */
public File getAllFilenames(String replicaId) throws ArgumentNotValid, IOFailure {
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  log.debug(""String_Node_Str"" + replicaId + ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetAllFilenamesMessage gafMsg=new GetAllFilenamesMessage(Channels.getTheRepos(),replyQ,replicaId);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(gafMsg,getTimeout);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / MILLISECONDS_PER_SECOND) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    throw new IOFailure(""String_Node_Str"" + (getTimeout / MILLISECONDS_PER_SECOND) + ""String_Node_Str"");
  }
  GetAllFilenamesMessage replyCSMsg;
  try {
    replyCSMsg=(GetAllFilenamesMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    throw new IOFailure(""String_Node_Str"" + replyNetMsg,e);
  }
  try {
    File result=File.createTempFile(""String_Node_Str"",""String_Node_Str"",FileUtils.getTempDir());
    replyCSMsg.getData(result);
    return result;
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + replyCSMsg,e);
  }
}","/** 
 * Retrieves the names of all the files in the replica through a GetAllFilenamesMessage. This is the checksum archive alternative to running a FilelistBatchJob.
 * @param replicaId The id of the replica from which the list of filenamesshould be retrieved.
 * @return A file with all the filenames within the archive of the givenreplica. A null is returned if the message timeout.
 * @throws IOFailure If the reply is not of type GetAllFilenamesMessageor if the file could not properly be retrieved from the reply message
 * @throws ArgumentNotValid If the replicaId is null or empty. 
 * @see dk.netarkivet.archive.checksum.distribute.GetAllFilenamesMessage
 */
public File getAllFilenames(String replicaId) throws ArgumentNotValid, IOFailure {
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  log.debug(""String_Node_Str"" + replicaId + ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetAllFilenamesMessage gafMsg=new GetAllFilenamesMessage(Channels.getTheRepos(),replyQ,replicaId);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(gafMsg,0);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / MILLISECONDS_PER_SECOND) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    throw new IOFailure(""String_Node_Str"" + (getTimeout / MILLISECONDS_PER_SECOND) + ""String_Node_Str"");
  }
  GetAllFilenamesMessage replyCSMsg;
  try {
    replyCSMsg=(GetAllFilenamesMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    throw new IOFailure(""String_Node_Str"" + replyNetMsg,e);
  }
  try {
    File result=File.createTempFile(""String_Node_Str"",""String_Node_Str"",FileUtils.getTempDir());
    replyCSMsg.getData(result);
    return result;
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + replyCSMsg,e);
  }
}",0.9970278303161308
89777,"/** 
 * Method for correcting an entry in a replica. This is done by sending a correct message to the replica. The file which is removed from the replica is put into the tempDir.
 * @param replicaId The id of the replica to send the message.
 * @param checksum The checksum of the corrupt entry in the archive. It is important to validate that the checksum actually is wrong before  correcting the entry.
 * @param file The file to correct the entry in the archive of the replica.
 * @param credentials A string with the password for allowing changes insidean archive. If it does not correspond to the credentials of the archive,  the correction will not be allowed.
 * @throws IOFailure If the message is not handled properly.
 * @throws ArgumentNotValid If the replicaId, the checksum or the credentials are either null or empty, or if file is null.
 */
public File correct(String replicaId,String checksum,File file,String credentials) throws IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(checksum,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(file,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(credentials,""String_Node_Str"");
  RemoteFile rm=RemoteFileFactory.getCopyfileInstance(file);
  CorrectMessage correctMsg=new CorrectMessage(Channels.getTheRepos(),replyQ,checksum,rm,replicaId,credentials);
  CorrectMessage responseMessage=(CorrectMessage)sendAndWaitForOneReply(correctMsg,getTimeout);
  if (responseMessage == null) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"");
  }
 else   if (!responseMessage.isOk()) {
    throw new IOFailure(""String_Node_Str"" + responseMessage.getErrMsg());
  }
  RemoteFile removedFile=responseMessage.getRemovedFile();
  try {
    File destFile=new File(FileUtils.getTempDir(),removedFile.getName());
    removedFile.copyTo(destFile);
    return destFile;
  }
 catch (  Throwable e) {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"";
    log.warn(errMsg,e);
    throw new IOFailure(errMsg,e);
  }
}","/** 
 * Method for correcting an entry in a replica. This is done by sending a correct message to the replica. The file which is removed from the replica is put into the tempDir.
 * @param replicaId The id of the replica to send the message.
 * @param checksum The checksum of the corrupt entry in the archive. It is important to validate that the checksum actually is wrong before  correcting the entry.
 * @param file The file to correct the entry in the archive of the replica.
 * @param credentials A string with the password for allowing changes insidean archive. If it does not correspond to the credentials of the archive,  the correction will not be allowed.
 * @throws IOFailure If the message is not handled properly.
 * @throws ArgumentNotValid If the replicaId, the checksum or the credentials are either null or empty, or if file is null.
 */
public File correct(String replicaId,String checksum,File file,String credentials) throws IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(checksum,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(file,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(credentials,""String_Node_Str"");
  RemoteFile rm=RemoteFileFactory.getCopyfileInstance(file);
  CorrectMessage correctMsg=new CorrectMessage(Channels.getTheRepos(),replyQ,checksum,rm,replicaId,credentials);
  CorrectMessage responseMessage=(CorrectMessage)sendAndWaitForOneReply(correctMsg,0);
  if (responseMessage == null) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"");
  }
 else   if (!responseMessage.isOk()) {
    throw new IOFailure(""String_Node_Str"" + responseMessage.getErrMsg());
  }
  RemoteFile removedFile=responseMessage.getRemovedFile();
  try {
    File destFile=new File(FileUtils.getTempDir(),removedFile.getName());
    removedFile.copyTo(destFile);
    return destFile;
  }
 catch (  Throwable e) {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"";
    log.warn(errMsg,e);
    throw new IOFailure(errMsg,e);
  }
}",0.9973436368027048
89778,"/** 
 * Retrieves the checksum of a specific file. This is the checksum archive alternative to running a ChecksumJob  limited to a specific file.
 * @param replicaId The ID of the replica to send the message.
 * @param filename The name of the file for whom the checksum should beretrieved.
 * @return The checksum of the file in the replica. 
 * @throws IOFailure If the reply is not of type GetChecksumMessage. Or ifthe message timed out.
 * @throws ArgumentNotValid If either the replicaId of the filename is null or empty. 
 */
public String getChecksum(String replicaId,String filename) throws ArgumentNotValid, IOFailure {
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  log.debug(""String_Node_Str"" + replicaId + ""String_Node_Str""+ filename+ ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetChecksumMessage gcsMsg=new GetChecksumMessage(Channels.getTheRepos(),replyQ,filename,replicaId);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(gcsMsg,getTimeout);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / MILLISECONDS_PER_SECOND) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    throw new IOFailure(""String_Node_Str"" + (getTimeout / MILLISECONDS_PER_SECOND) + ""String_Node_Str"");
  }
  GetChecksumMessage replyCSMsg;
  try {
    replyCSMsg=(GetChecksumMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    throw new IOFailure(""String_Node_Str"" + replyNetMsg,e);
  }
  if (!replyCSMsg.isOk()) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"" + replyCSMsg.getErrMsg());
  }
  return replyCSMsg.getChecksum();
}","/** 
 * Retrieves the checksum of a specific file. This is the checksum archive alternative to running a ChecksumJob  limited to a specific file.
 * @param replicaId The ID of the replica to send the message.
 * @param filename The name of the file for whom the checksum should beretrieved.
 * @return The checksum of the file in the replica. 
 * @throws IOFailure If the reply is not of type GetChecksumMessage. Or ifthe message timed out.
 * @throws ArgumentNotValid If either the replicaId of the filename is null or empty. 
 */
public String getChecksum(String replicaId,String filename) throws ArgumentNotValid, IOFailure {
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  log.debug(""String_Node_Str"" + replicaId + ""String_Node_Str""+ filename+ ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetChecksumMessage gcsMsg=new GetChecksumMessage(Channels.getTheRepos(),replyQ,filename,replicaId);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(gcsMsg,0);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / MILLISECONDS_PER_SECOND) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    throw new IOFailure(""String_Node_Str"" + (getTimeout / MILLISECONDS_PER_SECOND) + ""String_Node_Str"");
  }
  GetChecksumMessage replyCSMsg;
  try {
    replyCSMsg=(GetChecksumMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    throw new IOFailure(""String_Node_Str"" + replyNetMsg,e);
  }
  if (!replyCSMsg.isOk()) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"" + replyCSMsg.getErrMsg());
  }
  return replyCSMsg.getChecksum();
}",0.9967542047801712
89779,"/** 
 * Retrieves all the checksum from the replica through a GetAllChecksumMessage. This is the checksum archive alternative to running a ChecksumBatchJob.
 * @param replicaId The id of the replica from which the checksums should beretrieved.
 * @return A file containing filename and checksum of all the files in an archive in the same format as a ChecksumJob. 
 * @throws IOFailure If the reply is not of type GetAllChecksumsMessageor if the file could not properly be retrieved from the reply message or if the message timed out.
 * @throws ArgumentNotValid If the replicaId is null or empty. 
 * @see dk.netarkivet.archive.checksum.distribute.GetAllChecksumsMessage
 */
public File getAllChecksums(String replicaId) throws IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  log.debug(""String_Node_Str"" + replicaId + ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetAllChecksumsMessage gacMsg=new GetAllChecksumsMessage(Channels.getTheRepos(),replyQ,replicaId);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(gacMsg,getTimeout);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / MILLISECONDS_PER_SECOND) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    throw new IOFailure(""String_Node_Str"" + (getTimeout / MILLISECONDS_PER_SECOND) + ""String_Node_Str"");
  }
  GetAllChecksumsMessage replyCSMsg;
  try {
    replyCSMsg=(GetAllChecksumsMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    throw new IOFailure(""String_Node_Str"" + replyNetMsg,e);
  }
  try {
    File result=File.createTempFile(""String_Node_Str"",""String_Node_Str"",FileUtils.getTempDir());
    replyCSMsg.getData(result);
    return result;
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + replyCSMsg,e);
  }
}","/** 
 * Retrieves all the checksum from the replica through a GetAllChecksumMessage. This is the checksum archive alternative to running a ChecksumBatchJob.
 * @param replicaId The id of the replica from which the checksums should beretrieved.
 * @return A file containing filename and checksum of all the files in an archive in the same format as a ChecksumJob. 
 * @throws IOFailure If the reply is not of type GetAllChecksumsMessageor if the file could not properly be retrieved from the reply message or if the message timed out.
 * @throws ArgumentNotValid If the replicaId is null or empty. 
 * @see dk.netarkivet.archive.checksum.distribute.GetAllChecksumsMessage
 */
public File getAllChecksums(String replicaId) throws IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  log.debug(""String_Node_Str"" + replicaId + ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetAllChecksumsMessage gacMsg=new GetAllChecksumsMessage(Channels.getTheRepos(),replyQ,replicaId);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(gacMsg,0);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / MILLISECONDS_PER_SECOND) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    throw new IOFailure(""String_Node_Str"" + (getTimeout / MILLISECONDS_PER_SECOND) + ""String_Node_Str"");
  }
  GetAllChecksumsMessage replyCSMsg;
  try {
    replyCSMsg=(GetAllChecksumsMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    throw new IOFailure(""String_Node_Str"" + replyNetMsg,e);
  }
  try {
    File result=File.createTempFile(""String_Node_Str"",""String_Node_Str"",FileUtils.getTempDir());
    replyCSMsg.getData(result);
    return result;
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + replyCSMsg,e);
  }
}",0.9970406241592682
89780,"/** 
 * The constructor.
 */
public ChecksumJob(){
  batchJobTimeout=CHECKSUM_JOB_TIME;
}","/** 
 * The constructor.
 */
public ChecksumJob(){
  batchJobTimeout=-1;
}",0.8834355828220859
89781,"/** 
 * The method is used to update the checksum for all the files in a replica. The checksum for the replica is retrieved through GetAllChecksumMessages. This will take a very large amount of time for the bitarchive, but a  more limited amount of time for the checksumarchive. The corresponding replicafileinfo entries in the database for the  retrieved checksum results will be updated. Then a checksum update will  be performed to check for corrupted replicafileinfo.
 * @param replica The replica to find the changed files for.
 * @throws ArgumentNotValid If the replica is null.
 */
public void findChangedFiles(Replica replica) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  runChecksum(replica);
  initChecksumStatusUpdate();
  cache.updateChecksumStatus();
}","/** 
 * The method is used to update the checksum for all the files in a replica. The checksum for the replica is retrieved through GetAllChecksumMessages. This will take a very large amount of time for the bitarchive, but a  more limited amount of time for the checksumarchive. The corresponding replicafileinfo entries in the database for the  retrieved checksum results will be updated. Then a checksum update will  be performed to check for corrupted replicafileinfo.
 * @param replica The replica to find the changed files for.
 * @throws ArgumentNotValid If the replica is null.
 */
public synchronized void findChangedFiles(Replica replica) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  runChecksum(replica);
  initChecksumStatusUpdate();
  cache.updateChecksumStatus();
}",0.9920196439533456
89782,"/** 
 * This method retrieves the filelist for the replica, and then it updates the database with this list of filenames.
 * @param replica The replica to find the missing files for.
 * @throws ArgumentNotValid If the replica is null.
 */
public void findMissingFiles(Replica replica) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  List<String> filenames=getFilenamesList(replica);
  cache.addFileListInformation(filenames,replica);
}","/** 
 * This method retrieves the filelist for the replica, and then it updates the database with this list of filenames.
 * @param replica The replica to find the missing files for.
 * @throws ArgumentNotValid If the replica is null.
 */
public synchronized void findMissingFiles(Replica replica) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  List<String> filenames=getFilenamesList(replica);
  cache.addFileListInformation(filenames,replica);
}",0.9865005192107996
89783,"/** 
 * Method for retrieving the current instance of this class.
 * @return The instance.
 */
public static DatabaseBasedActiveBitPreservation getInstance(){
  if (instance == null) {
    instance=new DatabaseBasedActiveBitPreservation();
  }
  return instance;
}","/** 
 * Method for retrieving the current instance of this class.
 * @return The instance.
 */
public static synchronized DatabaseBasedActiveBitPreservation getInstance(){
  if (instance == null) {
    instance=new DatabaseBasedActiveBitPreservation();
  }
  return instance;
}",0.9759704251386322
89784,"/** 
 * Given the output of a checksum job, add the results to the database. The following fields in the table are updated for each corresponding  entry in the replicafileinfo table: <br/> - checksum = the given checksum.  <br/> - filelist_status = ok. <br/> - filelist_checkdatetime = now. <br/> - checksum_checkdatetime = now.
 * @param checksumOutput The output of a checksum job.
 * @param replica The replica this checksum job is for.
 */
@Override public void addChecksumInformation(List<ChecksumEntry> checksumOutput,Replica replica){
  ArgumentNotValid.checkNotNull(checksumOutput,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  if (!existsReplicaInDB(replica)) {
    String msg=""String_Node_Str"" + replica.toString() + ""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg);
  }
  for (  ChecksumEntry entry : checksumOutput) {
    String filename=entry.getFilename();
    String checksum=entry.getChecksum();
    if (!existsFileInDB(filename)) {
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      insertFileIntoDB(filename);
    }
    long fileid=retrieveIdForFile(filename);
    if (fileid < 0) {
      insertFileIntoDB(filename);
      fileid=retrieveIdForFile(filename);
    }
    long rfiId=retrieveReplicaFileInfoGuid(fileid,replica.getId());
    if (rfiId < 0) {
      createReplicaFileInfoEntriesInDB(fileid);
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
    }
    updateReplicaFileInfoChecksum(rfiId,checksum);
    log.trace(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
  }
  updateChecksumDateForReplica(replica);
  updateFilelistDateForReplica(replica);
}","/** 
 * Given the output of a checksum job, add the results to the database. The following fields in the table are updated for each corresponding  entry in the replicafileinfo table: <br/> - checksum = the given checksum.  <br/> - filelist_status = ok. <br/> - filelist_checkdatetime = now. <br/> - checksum_checkdatetime = now.
 * @param checksumOutput The output of a checksum job.
 * @param replica The replica this checksum job is for.
 */
@Override public void addChecksumInformation(List<ChecksumEntry> checksumOutput,Replica replica){
  ArgumentNotValid.checkNotNull(checksumOutput,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  if (!existsReplicaInDB(replica)) {
    String msg=""String_Node_Str"" + replica.toString() + ""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg);
  }
  for (  ChecksumEntry entry : checksumOutput) {
    String filename=entry.getFilename();
    String checksum=entry.getChecksum();
    if (!existsFileInDB(filename)) {
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      insertFileIntoDB(filename);
    }
    long fileid=retrieveIdForFile(filename);
    if (fileid < 0) {
      log.warn(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      insertFileIntoDB(filename);
      fileid=retrieveIdForFile(filename);
    }
    long rfiId=retrieveReplicaFileInfoGuid(fileid,replica.getId());
    if (rfiId < 0) {
      createReplicaFileInfoEntriesInDB(fileid);
      log.info(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
    }
    updateReplicaFileInfoChecksum(rfiId,checksum);
    log.trace(""String_Node_Str"" + filename + ""String_Node_Str""+ replica.toString()+ ""String_Node_Str"");
  }
  updateChecksumDateForReplica(replica);
  updateFilelistDateForReplica(replica);
}",0.9764998617638928
89785,"/** 
 * Method to create a new entry in the file table in the database. The file_id is automatically created by the database, and the argument is used for the filename for this new entry to the table.  This will also create a replicafileinfo entry for each replica.
 * @param filename The filename for the new entry in the file table.
 * @throws IllegalState If the file cannot be inserted into the database.
 */
private void insertFileIntoDB(String filename) throws IllegalState {
  try {
    PreparedStatement statement=null;
    dbConnection.setAutoCommit(false);
    statement=dbConnection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"");
    statement.setString(1,filename);
    statement.executeUpdate();
    dbConnection.commit();
    long fileId=retrieveIdForFile(filename);
    createReplicaFileInfoEntriesInDB(fileId);
  }
 catch (  SQLException e) {
    throw new IllegalState(""String_Node_Str"",e);
  }
}","/** 
 * Method to create a new entry in the file table in the database. The file_id is automatically created by the database, and the argument is used for the filename for this new entry to the table.  This will also create a replicafileinfo entry for each replica.
 * @param filename The filename for the new entry in the file table.
 * @throws IllegalState If the file cannot be inserted into the database.
 */
private void insertFileIntoDB(String filename) throws IllegalState {
  try {
    PreparedStatement statement=null;
    dbConnection.setAutoCommit(false);
    statement=dbConnection.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"");
    statement.setString(1,filename);
    statement.executeUpdate();
    dbConnection.commit();
    long fileId=retrieveIdForFile(filename);
    createReplicaFileInfoEntriesInDB(fileId);
  }
 catch (  SQLException e) {
    throw new IllegalState(""String_Node_Str"" + filename + ""String_Node_Str"",e);
  }
}",0.983828899321857
89786,"/** 
 * Method for retrieving the current instance of this class.
 * @return The current instance.
 */
public static ReplicaCacheDatabase getInstance(){
  if (instance == null) {
    instance=new ReplicaCacheDatabase();
  }
  return instance;
}","/** 
 * Method for retrieving the current instance of this class.
 * @return The current instance.
 */
public static synchronized ReplicaCacheDatabase getInstance(){
  if (instance == null) {
    instance=new ReplicaCacheDatabase();
  }
  return instance;
}",0.9740518962075848
89787,"/** 
 * Get the date for the last file list job.
 * @param replica The replica to get the date for.
 * @return The date of the last missing files update for the replica.A null is returned if no last missing files update has been performed.
 * @throws ArgumentNotValid If the replica is null.
 * @throws IllegalArgumentException If the Date of the Timestamp cannot beinstanciated.
 */
@Override public Date getDateOfLastMissingFilesUpdate(Replica replica) throws ArgumentNotValid, IllegalArgumentException {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  String sql=""String_Node_Str"" + ""String_Node_Str"";
  String result=DBUtils.selectStringValue(dbConnection,sql,replica.getId());
  if (result == null) {
    return null;
  }
 else {
    return new Date(Timestamp.valueOf(result).getTime());
  }
}","/** 
 * Get the date for the last file list job.
 * @param replica The replica to get the date for.
 * @return The date of the last missing files update for the replica.A null is returned if no last missing files update has been performed.
 * @throws ArgumentNotValid If the replica is null.
 * @throws IllegalArgumentException If the Date of the Timestamp cannot beinstanciated.
 */
@Override public Date getDateOfLastMissingFilesUpdate(Replica replica) throws ArgumentNotValid, IllegalArgumentException {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  String sql=""String_Node_Str"" + ""String_Node_Str"";
  String result=DBUtils.selectStringValue(dbConnection,sql,replica.getId());
  if (result == null) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"");
    return null;
  }
 else {
    return new Date(Timestamp.valueOf(result).getTime());
  }
}",0.9683959451401312
89788,"/** 
 * Method for retrieving the date for the last update for corrupted files. This method does not contact the replicas, it only retrieves the data from the last time the checksum was retrieved. 
 * @param replica The replica to find the date for the latest update forcorruption of files. 
 * @return The date for the last checksum update. A null is returned if nowrong files update has been performed for this replica.
 * @throws ArgumentNotValid If the replica is null.
 * @throws IllegalArgumentException If the Date of the Timestamp cannot beinstanciated.
 */
@Override public Date getDateOfLastWrongFilesUpdate(Replica replica) throws ArgumentNotValid, IllegalArgumentException {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  String sql=""String_Node_Str"" + ""String_Node_Str"";
  String result=DBUtils.selectStringValue(dbConnection,sql,replica.getId());
  if (result == null) {
    return null;
  }
 else {
    return new Date(Timestamp.valueOf(result).getTime());
  }
}","/** 
 * Method for retrieving the date for the last update for corrupted files. This method does not contact the replicas, it only retrieves the data from the last time the checksum was retrieved. 
 * @param replica The replica to find the date for the latest update forcorruption of files. 
 * @return The date for the last checksum update. A null is returned if nowrong files update has been performed for this replica.
 * @throws ArgumentNotValid If the replica is null.
 * @throws IllegalArgumentException If the Date of the Timestamp cannot beinstanciated.
 */
@Override public Date getDateOfLastWrongFilesUpdate(Replica replica) throws ArgumentNotValid, IllegalArgumentException {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  String sql=""String_Node_Str"" + ""String_Node_Str"";
  String result=DBUtils.selectStringValue(dbConnection,sql,replica.getId());
  if (result == null) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"");
    return null;
  }
 else {
    return new Date(Timestamp.valueOf(result).getTime());
  }
}",0.9739813451153656
89789,"/** 
 * Print HTML formatted state for checksum errors on a given replica in a given locale.
 * @param out      The writer to write state to.
 * @param replica The replica to write state for.
 * @param locale   The locale to write state in.
 * @throws IOException On IO trouble writing state to the writer.
 */
public static void printChecksumErrorStateForReplica(JspWriter out,Replica replica,Locale locale) throws IOException {
  ArgumentNotValid.checkNotNull(out,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(locale,""String_Node_Str"");
  ActiveBitPreservation bitPreservation=ActiveBitPreservationFactory.getInstance();
  out.println(I18N.getString(locale,""String_Node_Str"") + ""String_Node_Str"" + HTMLUtils.escapeHtmlValues(replica.getName())+ ""String_Node_Str"");
  out.println(""String_Node_Str"");
  out.println(I18N.getString(locale,""String_Node_Str"") + ""String_Node_Str"" + HTMLUtils.localiseLong(bitPreservation.getNumberOfChangedFiles(replica),locale));
  if (bitPreservation.getNumberOfChangedFiles(replica) > 0) {
    out.print(""String_Node_Str"" + Constants.FILESTATUS_CHECKSUM_PAGE + ""String_Node_Str""+ (Constants.BITARCHIVE_NAME_PARAM + ""String_Node_Str"" + HTMLUtils.encodeAndEscapeHTML(replica.getName()))+ ""String_Node_Str"");
    out.print(I18N.getString(locale,""String_Node_Str""));
    out.print(""String_Node_Str"");
  }
  out.println(""String_Node_Str"");
  out.println(I18N.getString(locale,""String_Node_Str"",bitPreservation.getDateForChangedFiles(replica)));
  out.println(""String_Node_Str"");
  out.println(""String_Node_Str"" + Constants.FILESTATUS_PAGE + ""String_Node_Str""+ Constants.CHECKSUM_PARAM+ ""String_Node_Str""+ (Constants.BITARCHIVE_NAME_PARAM + ""String_Node_Str"" + HTMLUtils.encodeAndEscapeHTML(replica.getName()))+ ""String_Node_Str""+ I18N.getString(locale,""String_Node_Str"")+ ""String_Node_Str"");
  out.println(""String_Node_Str"");
}","/** 
 * Print HTML formatted state for checksum errors on a given replica in a given locale.
 * @param out      The writer to write state to.
 * @param replica The replica to write state for.
 * @param locale   The locale to write state in.
 * @throws IOException On IO trouble writing state to the writer.
 */
public static void printChecksumErrorStateForReplica(JspWriter out,Replica replica,Locale locale) throws IOException {
  ArgumentNotValid.checkNotNull(out,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(locale,""String_Node_Str"");
  ActiveBitPreservation bitPreservation=ActiveBitPreservationFactory.getInstance();
  out.println(I18N.getString(locale,""String_Node_Str"") + ""String_Node_Str"" + HTMLUtils.escapeHtmlValues(replica.getName())+ ""String_Node_Str"");
  out.println(""String_Node_Str"");
  out.println(I18N.getString(locale,""String_Node_Str"") + ""String_Node_Str"" + HTMLUtils.localiseLong(bitPreservation.getNumberOfChangedFiles(replica),locale));
  if (bitPreservation.getNumberOfChangedFiles(replica) > 0) {
    out.print(""String_Node_Str"" + Constants.FILESTATUS_CHECKSUM_PAGE + ""String_Node_Str""+ (Constants.BITARCHIVE_NAME_PARAM + ""String_Node_Str"" + HTMLUtils.encodeAndEscapeHTML(replica.getName()))+ ""String_Node_Str"");
    out.print(I18N.getString(locale,""String_Node_Str""));
    out.print(""String_Node_Str"");
  }
  out.println(""String_Node_Str"");
  Date lastChangedFilesupdate=bitPreservation.getDateForChangedFiles(replica);
  if (lastChangedFilesupdate == null) {
    lastChangedFilesupdate=new Date(0);
  }
  out.println(I18N.getString(locale,""String_Node_Str"",lastChangedFilesupdate));
  out.println(""String_Node_Str"");
  out.println(""String_Node_Str"" + Constants.FILESTATUS_PAGE + ""String_Node_Str""+ Constants.CHECKSUM_PARAM+ ""String_Node_Str""+ (Constants.BITARCHIVE_NAME_PARAM + ""String_Node_Str"" + HTMLUtils.encodeAndEscapeHTML(replica.getName()))+ ""String_Node_Str""+ I18N.getString(locale,""String_Node_Str"")+ ""String_Node_Str"");
  out.println(""String_Node_Str"");
}",0.9477124183006536
89790,"/** 
 * Print HTML formatted state for missing files on a given replica in a given locale.
 * @param out      The writer to write state to.
 * @param replica The replica to write state for.
 * @param locale   The locale to write state in.
 * @throws IOException On IO trouble writing state to the writer.
 */
public static void printMissingFileStateForReplica(JspWriter out,Replica replica,Locale locale) throws IOException {
  ArgumentNotValid.checkNotNull(out,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(locale,""String_Node_Str"");
  ActiveBitPreservation activeBitPreservation=ActiveBitPreservationFactory.getInstance();
  out.println(I18N.getString(locale,""String_Node_Str"") + ""String_Node_Str"" + HTMLUtils.escapeHtmlValues(replica.getName())+ ""String_Node_Str"");
  out.println(""String_Node_Str"");
  out.println(I18N.getString(locale,""String_Node_Str"") + ""String_Node_Str"" + HTMLUtils.localiseLong(activeBitPreservation.getNumberOfFiles(replica),locale));
  out.println(""String_Node_Str"");
  out.println(I18N.getString(locale,""String_Node_Str"") + ""String_Node_Str"" + HTMLUtils.localiseLong(activeBitPreservation.getNumberOfMissingFiles(replica),locale));
  if (activeBitPreservation.getNumberOfMissingFiles(replica) > 0) {
    out.print(""String_Node_Str"" + Constants.FILESTATUS_MISSING_PAGE + ""String_Node_Str""+ (Constants.BITARCHIVE_NAME_PARAM + ""String_Node_Str"" + HTMLUtils.encodeAndEscapeHTML(replica.getName()))+ ""String_Node_Str"");
    out.print(I18N.getString(locale,""String_Node_Str""));
    out.print(""String_Node_Str"");
  }
  out.println(""String_Node_Str"");
  out.println(I18N.getString(locale,""String_Node_Str"",activeBitPreservation.getDateForMissingFiles(replica)));
  out.println(""String_Node_Str"");
  out.println(""String_Node_Str"" + Constants.FILESTATUS_PAGE + ""String_Node_Str""+ Constants.FIND_MISSING_FILES_PARAM+ ""String_Node_Str""+ (Constants.BITARCHIVE_NAME_PARAM + ""String_Node_Str"" + HTMLUtils.encodeAndEscapeHTML(replica.getName()))+ ""String_Node_Str""+ I18N.getString(locale,""String_Node_Str"")+ ""String_Node_Str"");
  out.println(""String_Node_Str"");
}","/** 
 * Print HTML formatted state for missing files on a given replica in a given locale.
 * @param out      The writer to write state to.
 * @param replica The replica to write state for.
 * @param locale   The locale to write state in.
 * @throws IOException On IO trouble writing state to the writer.
 */
public static void printMissingFileStateForReplica(JspWriter out,Replica replica,Locale locale) throws IOException {
  ArgumentNotValid.checkNotNull(out,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(locale,""String_Node_Str"");
  ActiveBitPreservation activeBitPreservation=ActiveBitPreservationFactory.getInstance();
  out.println(I18N.getString(locale,""String_Node_Str"") + ""String_Node_Str"" + HTMLUtils.escapeHtmlValues(replica.getName())+ ""String_Node_Str"");
  out.println(""String_Node_Str"");
  out.println(I18N.getString(locale,""String_Node_Str"") + ""String_Node_Str"" + HTMLUtils.localiseLong(activeBitPreservation.getNumberOfFiles(replica),locale));
  out.println(""String_Node_Str"");
  out.println(I18N.getString(locale,""String_Node_Str"") + ""String_Node_Str"" + HTMLUtils.localiseLong(activeBitPreservation.getNumberOfMissingFiles(replica),locale));
  if (activeBitPreservation.getNumberOfMissingFiles(replica) > 0) {
    out.print(""String_Node_Str"" + Constants.FILESTATUS_MISSING_PAGE + ""String_Node_Str""+ (Constants.BITARCHIVE_NAME_PARAM + ""String_Node_Str"" + HTMLUtils.encodeAndEscapeHTML(replica.getName()))+ ""String_Node_Str"");
    out.print(I18N.getString(locale,""String_Node_Str""));
    out.print(""String_Node_Str"");
  }
  out.println(""String_Node_Str"");
  Date lastMissingFilesupdate=activeBitPreservation.getDateForMissingFiles(replica);
  if (lastMissingFilesupdate == null) {
    lastMissingFilesupdate=new Date(0);
  }
  out.println(I18N.getString(locale,""String_Node_Str"",lastMissingFilesupdate));
  out.println(""String_Node_Str"");
  out.println(""String_Node_Str"" + Constants.FILESTATUS_PAGE + ""String_Node_Str""+ Constants.FIND_MISSING_FILES_PARAM+ ""String_Node_Str""+ (Constants.BITARCHIVE_NAME_PARAM + ""String_Node_Str"" + HTMLUtils.encodeAndEscapeHTML(replica.getName()))+ ""String_Node_Str""+ I18N.getString(locale,""String_Node_Str"")+ ""String_Node_Str"");
  out.println(""String_Node_Str"");
}",0.9452488687782804
89791,"/** 
 * Extract the name of the bitarchive (parameter Constants.BITARCHIVE_NAME_PARAM) and whether to update missing files (parameter Constants.FIND_MISSING_FILES_PARAM) or checksums (parameter Constants.CHECKSUM_PARAM). Does nothing if parameter 'bitarchive' is not set.
 * @param context the current JSP context
 * @throws ForwardedToErrorPage if an unknown bitarchive is posted.
 * @throws ArgumentNotValid If the context is null.
 */
public static void processUpdateRequest(PageContext context) throws ArgumentNotValid, ForwardedToErrorPage {
  ArgumentNotValid.checkNotNull(context,""String_Node_Str"");
  ServletRequest request=context.getRequest();
  String bitarchiveName=request.getParameter(Constants.BITARCHIVE_NAME_PARAM);
  if (bitarchiveName == null) {
    log.info(""String_Node_Str"");
    return;
  }
  if (!Replica.isKnownReplicaName(bitarchiveName)) {
    HTMLUtils.forwardWithErrorMessage(context,I18N,""String_Node_Str"",bitarchiveName);
    throw new ForwardedToErrorPage(""String_Node_Str"" + bitarchiveName);
  }
  Replica bitarchive=Replica.getReplicaFromName(bitarchiveName);
  String findmissingfiles=request.getParameter(Constants.FIND_MISSING_FILES_PARAM);
  String checksum=request.getParameter(Constants.CHECKSUM_PARAM);
  ActiveBitPreservation preserve=ActiveBitPreservationFactory.getInstance();
  if (findmissingfiles != null) {
    preserve.findMissingFiles(bitarchive);
  }
  if (checksum != null) {
    preserve.findChangedFiles(bitarchive);
  }
}","/** 
 * Extract the name of the bitarchive (parameter Constants.BITARCHIVE_NAME_PARAM) and whether to update missing files (parameter Constants.FIND_MISSING_FILES_PARAM) or checksums (parameter Constants.CHECKSUM_PARAM). Does nothing if parameter 'bitarchive' is not set.
 * @param context the current JSP context
 * @throws ForwardedToErrorPage if an unknown bitarchive is posted.
 * @throws ArgumentNotValid If the context is null.
 */
public static void processUpdateRequest(PageContext context) throws ArgumentNotValid, ForwardedToErrorPage {
  ArgumentNotValid.checkNotNull(context,""String_Node_Str"");
  ServletRequest request=context.getRequest();
  String bitarchiveName=request.getParameter(Constants.BITARCHIVE_NAME_PARAM);
  if (bitarchiveName == null) {
    log.debug(""String_Node_Str"" + Constants.BITARCHIVE_NAME_PARAM + ""String_Node_Str""+ ""String_Node_Str"");
    return;
  }
  if (!Replica.isKnownReplicaName(bitarchiveName)) {
    HTMLUtils.forwardWithErrorMessage(context,I18N,""String_Node_Str"",bitarchiveName);
    throw new ForwardedToErrorPage(""String_Node_Str"" + bitarchiveName);
  }
  Replica bitarchive=Replica.getReplicaFromName(bitarchiveName);
  String findmissingfiles=request.getParameter(Constants.FIND_MISSING_FILES_PARAM);
  String checksum=request.getParameter(Constants.CHECKSUM_PARAM);
  ActiveBitPreservation preserve=ActiveBitPreservationFactory.getInstance();
  if (findmissingfiles != null) {
    preserve.findMissingFiles(bitarchive);
  }
  if (checksum != null) {
    preserve.findChangedFiles(bitarchive);
  }
}",0.9722405816259088
89792,"/** 
 * Constructor.
 */
private ChecksumFileServer(){
  log.info(""String_Node_Str"");
  jmsCon=JMSConnectionFactory.getInstance();
  theCR=Channels.getTheCR();
  jmsCon.setListener(theCR,this);
  checksumAppId=createAppId();
  cs=FileChecksumArchive.getInstance();
  log.info(""String_Node_Str"" + checksumAppId + ""String_Node_Str"");
}","/** 
 * Constructor.
 */
private ChecksumFileServer(){
  log.info(""String_Node_Str"");
  cs=FileChecksumArchive.getInstance();
  jmsCon=JMSConnectionFactory.getInstance();
  theCR=Channels.getTheCR();
  jmsCon.setListener(theCR,this);
  checksumAppId=createAppId();
  log.info(""String_Node_Str"" + checksumAppId + ""String_Node_Str"");
}",0.7687687687687688
89793,"/** 
 * Tests the writeHarvestFiles method.
 * @throws Exception
 */
public void testWriteHarvestFiles() throws Exception {
  Job j=TestInfo.getJob();
  j.setJobID(1L);
  assertTrue(""String_Node_Str"",j.getSeedListAsString() != ""String_Node_Str"");
  assertTrue(""String_Node_Str"",j.getOrderXMLdoc().hasContent());
  File crawlDir=new File(TestInfo.WORKING_DIR,""String_Node_Str"");
  FileUtils.createDir(crawlDir);
  List<MetadataEntry> metadata=Arrays.asList(new MetadataEntry[]{TestInfo.sampleEntry});
  File harvestInfo=new File(crawlDir,""String_Node_Str"");
  File seedsTxt=new File(crawlDir,""String_Node_Str"");
  File orderXml=new File(crawlDir,""String_Node_Str"");
  File metadataFile=new File(crawlDir,j.getJobID() + ""String_Node_Str"");
  assertFalse(""String_Node_Str"",metadataFile.exists());
  assertFalse(""String_Node_Str"",harvestInfo.exists());
  assertFalse(""String_Node_Str"",seedsTxt.exists());
  assertFalse(""String_Node_Str"",orderXml.exists());
  HarvestController controller=HarvestController.getInstance();
  HeritrixFiles files=controller.writeHarvestFiles(crawlDir,j,metadata);
  assertTrue(""String_Node_Str"",harvestInfo.exists());
  assertTrue(""String_Node_Str"",seedsTxt.exists());
  assertTrue(""String_Node_Str"",orderXml.exists());
  assertTrue(""String_Node_Str"",metadataFile.exists());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"" + j.getJobID() + ""String_Node_Str"",harvestInfo);
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"" + j.getOrigHarvestDefinitionID() + ""String_Node_Str"",harvestInfo);
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",orderXml);
  new HeritrixTemplate(XmlUtils.getXmlDoc(orderXml),true);
  FileAsserts.assertFileContains(""String_Node_Str"",j.getSeedListAsString(),seedsTxt);
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",metadataFile);
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",metadataFile);
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",metadataFile);
  assertEquals(""String_Node_Str"",crawlDir,files.getCrawlDir());
  assertEquals(""String_Node_Str"",orderXml,files.getOrderXmlFile());
  assertEquals(""String_Node_Str"",seedsTxt,files.getSeedsTxtFile());
  assertTrue(""String_Node_Str"",files.getIndexDir().isDirectory());
  assertEquals(""String_Node_Str"",3,files.getIndexDir().listFiles().length);
  assertTrue(""String_Node_Str"",files.getArcsDir().isDirectory());
}","/** 
 * Tests the writeHarvestFiles method. FIXME fails when run as part of the UnitTesterSuite.java. See bug 1912.
 * @throws Exception
 */
public void testWriteHarvestFiles() throws Exception {
  Job j=TestInfo.getJob();
  j.setJobID(1L);
  assertTrue(""String_Node_Str"",j.getSeedListAsString() != ""String_Node_Str"");
  assertTrue(""String_Node_Str"",j.getOrderXMLdoc().hasContent());
  File crawlDir=new File(TestInfo.WORKING_DIR,""String_Node_Str"");
  FileUtils.createDir(crawlDir);
  List<MetadataEntry> metadata=Arrays.asList(new MetadataEntry[]{TestInfo.sampleEntry});
  File harvestInfo=new File(crawlDir,""String_Node_Str"");
  File seedsTxt=new File(crawlDir,""String_Node_Str"");
  File orderXml=new File(crawlDir,""String_Node_Str"");
  File metadataFile=new File(crawlDir,j.getJobID() + ""String_Node_Str"");
  assertFalse(""String_Node_Str"",metadataFile.exists());
  assertFalse(""String_Node_Str"",harvestInfo.exists());
  assertFalse(""String_Node_Str"",seedsTxt.exists());
  assertFalse(""String_Node_Str"",orderXml.exists());
  HarvestController controller=HarvestController.getInstance();
  HeritrixFiles files=controller.writeHarvestFiles(crawlDir,j,metadata);
  assertTrue(""String_Node_Str"",harvestInfo.exists());
  assertTrue(""String_Node_Str"",seedsTxt.exists());
  assertTrue(""String_Node_Str"",orderXml.exists());
  assertTrue(""String_Node_Str"",metadataFile.exists());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"" + j.getJobID() + ""String_Node_Str"",harvestInfo);
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"" + j.getOrigHarvestDefinitionID() + ""String_Node_Str"",harvestInfo);
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",orderXml);
  new HeritrixTemplate(XmlUtils.getXmlDoc(orderXml),true);
  FileAsserts.assertFileContains(""String_Node_Str"",j.getSeedListAsString(),seedsTxt);
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",metadataFile);
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",metadataFile);
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",metadataFile);
  assertEquals(""String_Node_Str"",crawlDir,files.getCrawlDir());
  assertEquals(""String_Node_Str"",orderXml,files.getOrderXmlFile());
  assertEquals(""String_Node_Str"",seedsTxt,files.getSeedsTxtFile());
  assertTrue(""String_Node_Str"",files.getIndexDir().isDirectory());
  assertEquals(""String_Node_Str"",3,files.getIndexDir().listFiles().length);
  assertTrue(""String_Node_Str"",files.getArcsDir().isDirectory());
}",0.9854721549636803
89794,"/** 
 * Creates a script for starting the archive database on a given machine. This is only created if the &lt;globalArchiveDatabaseDir&gt; parameter is defined on the machine level. <br/> &gt; #!/bin/bash <br/> &gt; cd InstallDir <br/> &gt; java -cp 'DB-CLASSPATH'  org.apache.derby.drda.NetworkServerControl start < /dev/null >  start_external_database.log 2>&1 &
 * @param dir The directory where the script will be placed.
 * @throws IOFailure If the script cannot be written.
 */
@Override protected void createArchiveDatabaseStartScript(File dir) throws IOFailure {
  String dbDir=machineParameters.getArchiveDatabaseDirValue();
  if (dbDir.isEmpty()) {
    return;
  }
  try {
    File startArcDBScript=new File(dir,Constants.SCRIPT_NAME_ARC_DB_START + scriptExtension);
    String port=settings.getLeafValue(Constants.SETTINGS_ARCHIVE_DATABASE_PORT);
    PrintWriter startDBPrint=new PrintWriter(startArcDBScript);
    try {
      startDBPrint.println(ScriptConstants.BIN_BASH_COMMENT);
      startDBPrint.print(ScriptConstants.CD + Constants.SPACE);
      startDBPrint.println(getInstallDirPath());
      startDBPrint.print(ScriptConstants.JAVA + Constants.SPACE);
      startDBPrint.print(ScriptConstants.JAVA_CLASSPATH);
      startDBPrint.print(Constants.SPACE + getDbClasspaths());
      startDBPrint.print(ScriptConstants.DERBY_ACCESS_METHOD);
      if (port != null && !port.isEmpty()) {
        startDBPrint.print(Constants.SPACE);
        startDBPrint.print(ScriptConstants.DATABASE_PORT_ARGUMENT);
        startDBPrint.print(Constants.SPACE);
        startDBPrint.print(port);
      }
      startDBPrint.print(Constants.SPACE);
      startDBPrint.print(ScriptConstants.DERBY_COMMAND_START);
      startDBPrint.print(Constants.SPACE);
      startDBPrint.print(ScriptConstants.LINUX_DEV_NULL);
      startDBPrint.print(Constants.SPACE);
      startDBPrint.print(Constants.SCRIPT_NAME_ARC_DB_START);
      startDBPrint.print(Constants.EXTENSION_LOG_FILES);
      startDBPrint.print(Constants.SPACE);
      startDBPrint.println(ScriptConstants.LINUX_ERROR_MESSAGE_TO_1);
    }
  finally {
      startDBPrint.close();
    }
  }
 catch (  IOException e) {
    log.trace(Constants.MSG_ERROR_DB_START_FILE,e);
    throw new IOFailure(Constants.MSG_ERROR_DB_START_FILE,e);
  }
}","/** 
 * Creates a script for starting the archive database on a given machine. This is only created if the &lt;globalArchiveDatabaseDir&gt; parameter is defined on the machine level. <br/> &gt; #!/bin/bash <br/> &gt; cd InstallDir <br/> &gt; java -cp 'DB-CLASSPATH'  org.apache.derby.drda.NetworkServerControl start < /dev/null >  start_external_database.log 2>&1 &
 * @param dir The directory where the script will be placed.
 * @throws IOFailure If the script cannot be written.
 */
@Override protected void createArchiveDatabaseStartScript(File dir) throws IOFailure {
  String dbDir=machineParameters.getArchiveDatabaseDirValue();
  if (dbDir.isEmpty()) {
    return;
  }
  try {
    File startArcDBScript=new File(dir,Constants.SCRIPT_NAME_ARC_DB_START + scriptExtension);
    String port=settings.getLeafValue(Constants.SETTINGS_ARCHIVE_DATABASE_PORT);
    PrintWriter startDBPrint=new PrintWriter(startArcDBScript);
    try {
      startDBPrint.println(ScriptConstants.BIN_BASH_COMMENT);
      startDBPrint.print(ScriptConstants.CD + Constants.SPACE);
      startDBPrint.println(getInstallDirPath());
      startDBPrint.print(ScriptConstants.JAVA + Constants.SPACE);
      startDBPrint.print(machineParameters.writeJavaOptions());
      startDBPrint.print(Constants.SPACE);
      startDBPrint.print(ScriptConstants.JAVA_CLASSPATH);
      startDBPrint.print(Constants.SPACE + getDbClasspaths());
      startDBPrint.print(ScriptConstants.DERBY_ACCESS_METHOD);
      if (port != null && !port.isEmpty()) {
        startDBPrint.print(Constants.SPACE);
        startDBPrint.print(ScriptConstants.DATABASE_PORT_ARGUMENT);
        startDBPrint.print(Constants.SPACE);
        startDBPrint.print(port);
      }
      startDBPrint.print(Constants.SPACE);
      startDBPrint.print(ScriptConstants.DERBY_COMMAND_START);
      startDBPrint.print(Constants.SPACE);
      startDBPrint.print(ScriptConstants.LINUX_DEV_NULL);
      startDBPrint.print(Constants.SPACE);
      startDBPrint.print(Constants.SCRIPT_NAME_ARC_DB_START);
      startDBPrint.print(Constants.EXTENSION_LOG_FILES);
      startDBPrint.print(Constants.SPACE);
      startDBPrint.println(ScriptConstants.LINUX_ERROR_MESSAGE_TO_1);
    }
  finally {
      startDBPrint.close();
    }
  }
 catch (  IOException e) {
    log.trace(Constants.MSG_ERROR_DB_START_FILE,e);
    throw new IOFailure(Constants.MSG_ERROR_DB_START_FILE,e);
  }
}",0.977141636402478
89795,"/** 
 * Get a sorted list of all seeds of a Domain in a HarvestDefinition.
 * @param harvestName of HarvestDefintion
 * @param domainName of Domain
 * @return List of all seeds of the Domain in the HarvestDefintion.
 */
public List<String> getListOfSeedsOfDomainOfHarvestDefinition(String harvestName,String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(harvestName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  Connection c=DBConnect.getDBConnection();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,domainName);
    s.setString(2,harvestName);
    ResultSet res=s.executeQuery();
    List<String> seeds=new ArrayList<String>();
    while (res.next()) {
      String seedsOfDomain=res.getString(1);
      StringTokenizer st=new StringTokenizer(seedsOfDomain,""String_Node_Str"");
      while (st.hasMoreTokens()) {
        String seed=st.nextToken();
        boolean bFound=false;
        for (        String entry : seeds) {
          if (entry.equals(seed)) {
            bFound=true;
            break;
          }
        }
        if (!bFound) {
          seeds.add(seed);
        }
      }
    }
    Collections.sort(seeds,Collator.getInstance());
    return seeds;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    DBUtils.closeStatementIfOpen(s);
  }
}","/** 
 * Get a sorted list of all seeds of a Domain in a HarvestDefinition.
 * @param harvestName of HarvestDefinition
 * @param domainName of Domain
 * @return List of all seeds of the Domain in the HarvestDefinition.
 */
public List<String> getListOfSeedsOfDomainOfHarvestDefinition(String harvestName,String domainName){
  ArgumentNotValid.checkNotNullOrEmpty(harvestName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(domainName,""String_Node_Str"");
  Connection c=DBConnect.getDBConnection();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,domainName);
    s.setString(2,harvestName);
    ResultSet res=s.executeQuery();
    List<String> seeds=new ArrayList<String>();
    while (res.next()) {
      String seedsOfDomain=res.getString(1);
      StringTokenizer st=new StringTokenizer(seedsOfDomain,""String_Node_Str"");
      while (st.hasMoreTokens()) {
        String seed=st.nextToken();
        boolean bFound=false;
        for (        String entry : seeds) {
          if (entry.equals(seed)) {
            bFound=true;
            break;
          }
        }
        if (!bFound) {
          seeds.add(seed);
        }
      }
    }
    Collections.sort(seeds,Collator.getInstance());
    return seeds;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    DBUtils.closeStatementIfOpen(s);
  }
}",0.999418942475305
89796,"/** 
 * Get a sorted list of all domainnames of a HarvestDefinition.
 * @param harvestName of HarvestDefintion
 * @return List of all domains of the HarvestDefintion.
 */
public List<String> getListOfDomainsOfHarvestDefinition(String harvestName){
  ArgumentNotValid.checkNotNullOrEmpty(harvestName,""String_Node_Str"");
  Connection c=DBConnect.getDBConnection();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,harvestName);
    ResultSet res=s.executeQuery();
    List<String> domains=new ArrayList<String>();
    while (res.next()) {
      domains.add(res.getString(1));
    }
    return domains;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    DBUtils.closeStatementIfOpen(s);
  }
}","/** 
 * Get a sorted list of all domainnames of a HarvestDefinition.
 * @param harvestName of HarvestDefinition
 * @return List of all domains of the HarvestDefinition.
 */
public List<String> getListOfDomainsOfHarvestDefinition(String harvestName){
  ArgumentNotValid.checkNotNullOrEmpty(harvestName,""String_Node_Str"");
  Connection c=DBConnect.getDBConnection();
  PreparedStatement s=null;
  try {
    s=c.prepareStatement(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
    s.setString(1,harvestName);
    ResultSet res=s.executeQuery();
    List<String> domains=new ArrayList<String>();
    while (res.next()) {
      domains.add(res.getString(1));
    }
    return domains;
  }
 catch (  SQLException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str""+ ExceptionUtils.getSQLExceptionCause(e),e);
  }
 finally {
    DBUtils.closeStatementIfOpen(s);
  }
}",0.9990375360923964
89797,"/** 
 * Test whether the correct error message when sending to a checksum replica.  
 */
public void testArgumentsChecksumReplica(){
  String expectedErrorMessage=""String_Node_Str"" + ""String_Node_Str"" + TestInfo.BATCH_CS_REPLICA_NAME + ""String_Node_Str""+ ""String_Node_Str""+ Replica.getReplicaFromName(TestInfo.BATCH_CS_REPLICA_NAME)+ ""String_Node_Str"";
  String[] args=new String[]{""String_Node_Str"" + TestInfo.BATCH_TEST_JAR_FILE.getAbsolutePath(),""String_Node_Str"" + TestInfo.BATCH_TEST_JAR_GOOD_CLASS,""String_Node_Str"" + TestInfo.BATCH_CS_REPLICA_NAME};
  try {
    RunBatch.main(args);
    fail(""String_Node_Str"");
  }
 catch (  SecurityException e) {
  }
  String errMsg=pss.getErr();
  int pseVal=pse.getExitValue();
  assertEquals(""String_Node_Str"" + pseVal,1,pseVal);
  assertTrue(""String_Node_Str"" + expectedErrorMessage + ""String_Node_Str""+ errMsg,errMsg.startsWith(expectedErrorMessage));
}","/** 
 * Test whether the correct error message when sending to a checksum replica.  
 */
public void testArgumentsChecksumReplica(){
  String expectedErrorMessage=""String_Node_Str"" + ""String_Node_Str"" + Replica.getReplicaFromName(TestInfo.BATCH_CS_REPLICA_NAME) + ""String_Node_Str""+ Replica.getReplicaFromName(TestInfo.BATCH_CS_REPLICA_NAME).getType()+ ""String_Node_Str"";
  String[] args=new String[]{""String_Node_Str"" + TestInfo.BATCH_TEST_JAR_FILE.getAbsolutePath(),""String_Node_Str"" + TestInfo.BATCH_TEST_JAR_GOOD_CLASS,""String_Node_Str"" + TestInfo.BATCH_CS_REPLICA_NAME};
  try {
    RunBatch.main(args);
    fail(""String_Node_Str"");
  }
 catch (  SecurityException e) {
  }
  String errMsg=pss.getErr();
  int pseVal=pse.getExitValue();
  assertEquals(""String_Node_Str"" + pseVal,1,pseVal);
  assertTrue(""String_Node_Str"" + expectedErrorMessage + ""String_Node_Str""+ errMsg,errMsg.startsWith(expectedErrorMessage));
}",0.9676002196595276
89798,"/** 
 * Closes this BitarchiveMonitorServer cleanly.
 */
public void cleanup(){
  if (instance != null) {
    con.removeListener(Channels.getTheBamon(),this);
    instance=null;
    if (bamon != null) {
      bamon.cleanup();
      bamon=null;
    }
  }
}","/** 
 * Closes this BitarchiveMonitorServer cleanly.
 */
public void cleanup(){
  if (instance != null) {
    con.removeListener(Channels.getTheBamon(),this);
    batchConversions.clear();
    instance=null;
    if (bamon != null) {
      bamon.cleanup();
      bamon=null;
    }
  }
}",0.9444444444444444
89799,"/** 
 * Handles notifications from the bitarchive monitor, that a batch job is complete. Spawns a new thread in which all the results are wrapped and sent back in a reply to the originator of this batch request.
 * @param o   the observable object. Should always be the bitarchivemonitor. If it isn't, this notification will be logged and ignored.
 * @param arg an argument passed from the bitarchive monitor. This shouldalways be a batch status object indicating the end of that batchjob. If it isn't, this notification will be logged and ignored.
 */
public void update(Observable o,final Object arg){
  if (o != bamon) {
    log.warn(""String_Node_Str"" + o + ""String_Node_Str"");
    return;
  }
  if (arg == null) {
    log.warn(""String_Node_Str"");
    return;
  }
  if (!(arg instanceof dk.netarkivet.archive.bitarchive.BitarchiveMonitor.BatchJobStatus)) {
    log.warn(""String_Node_Str"" + arg.getClass() + ""String_Node_Str""+ arg+ ""String_Node_Str"");
    return;
  }
  new Thread(){
    public void run(){
      doBatchReply((BitarchiveMonitor.BatchJobStatus)arg);
    }
  }
.start();
}","/** 
 * Handles notifications from the bitarchive monitor, that a batch job is complete. Spawns a new thread in which all the results are wrapped and sent back in a reply to the originator of this batch request.
 * @param o   the observable object. Should always be the bitarchivemonitor. If it isn't, this notification will be logged and ignored.
 * @param arg an argument passed from the bitarchive monitor. This shouldalways be a batch status object indicating the end of that batchjob. If it isn't, this notification will be logged and ignored.
 */
public void update(Observable o,final Object arg){
  if (o != bamon) {
    log.warn(""String_Node_Str"" + o + ""String_Node_Str"");
    return;
  }
  if (arg == null) {
    log.warn(""String_Node_Str"");
    return;
  }
  if (!(arg instanceof dk.netarkivet.archive.bitarchive.BitarchiveMonitor.BatchJobStatus)) {
    log.warn(""String_Node_Str"" + arg.getClass() + ""String_Node_Str""+ arg+ ""String_Node_Str"");
    return;
  }
  new Thread(){
    public void run(){
      BitarchiveMonitor.BatchJobStatus bjs=(BitarchiveMonitor.BatchJobStatus)arg;
      if (batchConversions.containsKey(bjs.originalRequestID)) {
        replyConvertedBatch(bjs);
      }
 else {
        doBatchReply(bjs);
      }
    }
  }
.start();
}",0.8940876222883879
89800,"public void run(){
  doBatchReply((BitarchiveMonitor.BatchJobStatus)arg);
}","public void run(){
  BitarchiveMonitor.BatchJobStatus bjs=(BitarchiveMonitor.BatchJobStatus)arg;
  if (batchConversions.containsKey(bjs.originalRequestID)) {
    replyConvertedBatch(bjs);
  }
 else {
    doBatchReply(bjs);
  }
}",0.2508250825082508
89801,"/** 
 * This is the message handling method for HeartBeatMessages. Registers a sign of life from a bitarchive.
 * @param hbMsg the message that represents the sign of life
 */
public void visit(HeartBeatMessage hbMsg){
  try {
    bamon.signOfLife(hbMsg.getBitarchiveID());
  }
 catch (  Exception e) {
    log.warn(""String_Node_Str"" + hbMsg + ""String_Node_Str"",e);
  }
}","/** 
 * Method for handling the 
 * @param msg The GetAllChecksumsMessage, which will be made into a batchjoband sent to the 
 */
public void visit(GetChecksumMessage msg){
  log.info(""String_Node_Str"" + msg + ""String_Node_Str"");
  ChecksumJob cj=new ChecksumJob();
  cj.processOnlyFileNamed(msg.getArcfileName());
  executeConvertedBatch(cj,msg);
}",0.2222222222222222
89802,"/** 
 * This method sends a reply based on the information from bitarchives received and stored in the given batch job status. It will concatenate the results from all the bitarchives in one file, and construct a reply to the originating requester with all information.
 * @param bjs Status of received messages from bitarchives.
 */
private void doBatchReply(BitarchiveMonitor.BatchJobStatus bjs){
  RemoteFile resultsFile=null;
  try {
    resultsFile=RemoteFileFactory.getMovefileInstance(bjs.batchResultFile);
  }
 catch (  Exception e) {
    log.warn(""String_Node_Str"" + bjs.batchResultFile,e);
    bjs.appendError(""String_Node_Str"" + e);
  }
  BatchReplyMessage brMsg=new BatchReplyMessage(bjs.originalRequestReplyTo,Channels.getTheBamon(),bjs.originalRequestID,bjs.noOfFilesProcessed,bjs.filesFailed,resultsFile);
  if (bjs.errorMessages != null) {
    brMsg.setNotOk(bjs.errorMessages);
  }
  con.send(brMsg);
  log.info(""String_Node_Str"" + brMsg + ""String_Node_Str""+ brMsg.getTo()+ ""String_Node_Str"");
}","/** 
 * This method sends a reply based on the information from bitarchives received and stored in the given batch job status. It will concatenate the results from all the bitarchives in one file, and construct a reply to the originating requester with all information.
 * @param bjs Status of received messages from bitarchives.
 */
private void doBatchReply(BitarchiveMonitor.BatchJobStatus bjs){
  RemoteFile resultsFile=null;
  try {
    resultsFile=RemoteFileFactory.getMovefileInstance(bjs.batchResultFile);
  }
 catch (  Exception e) {
    log.warn(""String_Node_Str"" + bjs.batchResultFile,e);
    bjs.appendError(""String_Node_Str"" + e);
  }
  BatchReplyMessage brMsg=new BatchReplyMessage(bjs.originalRequestReplyTo,Channels.getTheBamon(),bjs.originalRequestID,bjs.noOfFilesProcessed,bjs.filesFailed,resultsFile);
  if (bjs.errorMessages != null) {
    brMsg.setNotOk(bjs.errorMessages);
  }
  con.send(brMsg);
  log.info(""String_Node_Str"" + brMsg + ""String_Node_Str""+ ""String_Node_Str""+ brMsg.getTo()+ ""String_Node_Str"");
}",0.990699951052374
89803,"/** 
 * Method for retrieving a map containing all the checksums and their  corresponding filenames within the archive.
 * @param msg The GetAllChecksumMessage.
 * @throws ArgumentNotValid If the GetAllChecksumMessage is null.
 */
public void visit(GetAllChecksumsMessage msg) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  log.debug(""String_Node_Str"" + msg.toString());
  try {
    msg.setResultingFile(cs.getArchiveAsFile());
  }
 catch (  Throwable e) {
    log.warn(""String_Node_Str"",e);
    msg.setNotOk(e);
  }
 finally {
    log.info(""String_Node_Str"" + msg.toString());
    jmsCon.reply(msg);
  }
}","/** 
 * Method for retrieving a map containing all the checksums and their  corresponding filenames within the archive.
 * @param msg The GetAllChecksumMessage.
 * @throws ArgumentNotValid If the GetAllChecksumMessage is null.
 */
public void visit(GetAllChecksumsMessage msg) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  log.debug(""String_Node_Str"" + msg.toString());
  try {
    msg.setFile(cs.getArchiveAsFile());
  }
 catch (  Throwable e) {
    log.warn(""String_Node_Str"",e);
    msg.setNotOk(e);
  }
 finally {
    log.info(""String_Node_Str"" + msg.toString());
    jmsCon.reply(msg);
  }
}",0.9929522317932654
89804,"/** 
 * Method for retrieving the checksum of a specific file from the archive of a specific replica. If the replica is a BitArchive, then a Batch  message with the ChecksumJob is sent. If the replica is a  ChecksumArchive, then a GetChecksumMessage is sent.
 * @param filename The file to checksum.
 * @param replicaClient The client to retrieve the checksum of the file from.
 */
private void sendChecksumRequestForFile(String filename,ReplicaClient replicaClient){
  NetarkivetMessage msg;
  if (replicaClient.getType() == ReplicaType.BITARCHIVE) {
    ChecksumJob checksumJob=new ChecksumJob();
    checksumJob.processOnlyFileNamed(filename);
    msg=replicaClient.batch(Channels.getTheRepos(),checksumJob);
  }
 else   if (replicaClient.getType() == ReplicaType.CHECKSUM) {
    msg=replicaClient.getChecksum(Channels.getTheRepos(),filename);
  }
 else {
    String errMsg=""String_Node_Str"" + replicaClient + ""String_Node_Str""+ filename+ ""String_Node_Str"";
    log.error(errMsg);
    throw new IllegalState(errMsg);
  }
  outstandingChecksumFiles.put(msg.getID(),filename);
  log.debug(""String_Node_Str"" + filename + ""String_Node_Str""+ msg.getID()+ ""String_Node_Str"");
}","/** 
 * Method for retrieving the checksum of a specific file from the archive of a specific replica. If the replica is a BitArchive, then a Batch  message with the ChecksumJob is sent. If the replica is a  ChecksumArchive, then a GetChecksumMessage is sent.
 * @param filename The file to checksum.
 * @param replicaClient The client to retrieve the checksum of the file from.
 */
private void sendChecksumRequestForFile(String filename,ReplicaClient replicaClient){
  NetarkivetMessage msg;
  msg=replicaClient.getChecksum(Channels.getTheRepos(),filename);
  outstandingChecksumFiles.put(msg.getID(),filename);
  log.debug(""String_Node_Str"" + filename + ""String_Node_Str""+ msg.getID()+ ""String_Node_Str"");
}",0.7148167817312798
89805,"/** 
 * Method for retrieving the checksums from a specific replica. The checksum are retrieved differently for the different types of  replica: <br/> The checksums of a bitarchive is retrieved through running  the BatchJob ChecksumJob on the replica. <br/> The checksums of a checksumsarchive is retrieved through a  GetAllChecksumMessage.
 * @param replica The replica to retrieve the checksums from.
 * @return The checksums in the format of ChecksumEntry.
 * @throws IOFailure If a bitarchive does not return a output file.
 * @throws IllegalState If the replica has an unknown replica type.
 */
private List<ChecksumEntry> getChecksumList(Replica replica) throws IOFailure, IllegalState {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  if (replica.getType() == ReplicaType.BITARCHIVE) {
    File outputFile=WorkFiles.getFile(replica,WorkFiles.CHECKSUMS_ON_BA);
    log.info(""String_Node_Str"" + replica + ""String_Node_Str"");
    runBatchJob(new ChecksumJob(),replica,null,outputFile);
    if (outputFile == null || !outputFile.exists()) {
      String errMsg=""String_Node_Str"" + replica + ""String_Node_Str"";
      log.warn(errMsg);
      throw new IOFailure(errMsg);
    }
    return ChecksumEntry.parseChecksumJob(outputFile);
  }
 else   if (replica.getType() == ReplicaType.CHECKSUM) {
    log.info(""String_Node_Str"" + replica + ""String_Node_Str"");
    return ChecksumEntry.parseChecksumJob(ArcRepositoryClientFactory.getPreservationInstance().getAllChecksums(replica.getId()));
  }
 else {
    String msg=""String_Node_Str"" + replica + ""String_Node_Str"";
    log.warn(msg);
    throw new IllegalState(msg);
  }
}","/** 
 * Method for retrieving the checksums from a specific replica. The checksum are retrieved differently for the different types of  replica: <br/> The checksums of a bitarchive is retrieved through running  the BatchJob ChecksumJob on the replica. <br/> The checksums of a checksumsarchive is retrieved through a  GetAllChecksumMessage.
 * @param replica The replica to retrieve the checksums from.
 * @return The checksums in the format of ChecksumEntry.
 * @throws IOFailure If a bitarchive does not return a output file.
 * @throws IllegalState If the replica has an unknown replica type.
 */
private List<ChecksumEntry> getChecksumList(Replica replica) throws IOFailure, IllegalState {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  log.info(""String_Node_Str"" + replica + ""String_Node_Str"");
  return ChecksumEntry.parseChecksumJob(ArcRepositoryClientFactory.getPreservationInstance().getAllChecksums(replica.getId()));
}",0.7323506594259116
89806,"/** 
 * Method for retrieving the filelist from a specific replica. The filelist are retrieved differently for the different types of  replica: <br/> The filelist of a bitarchive is retrieved through running  the BatchJob FilelistJob on the replica. <br/> The filelist of a checksumsarchive is retrieved through a  GetAllFilenamesMessage.
 * @param replica The replica to retrieve the filelist from.
 * @return The names of the files in a list.
 * @throws ArgumentNotValid If the replica is 'null'.
 * @throws UnknownID If the replica has a unhandled replica type.
 */
private List<String> getFilenamesList(Replica replica) throws ArgumentNotValid, UnknownID {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  if (replica.getType() == ReplicaType.BITARCHIVE) {
    File outputFile=WorkFiles.getFile(replica,WorkFiles.CHECKSUMS_ON_BA);
    log.info(""String_Node_Str"" + replica + ""String_Node_Str"");
    runBatchJob(new FileListJob(),replica,null,outputFile);
    return FileUtils.readListFromFile(outputFile);
  }
 else   if (replica.getType() == ReplicaType.CHECKSUM) {
    log.info(""String_Node_Str"" + replica + ""String_Node_Str"");
    return FileUtils.readListFromFile(ArcRepositoryClientFactory.getPreservationInstance().getAllFilenames(replica.getId()));
  }
 else {
    String msg=""String_Node_Str"" + replica + ""String_Node_Str"";
    log.warn(msg);
    throw new UnknownID(msg);
  }
}","/** 
 * Method for retrieving the filelist from a specific replica. The filelist are retrieved differently for the different types of  replica: <br/> The filelist of a bitarchive is retrieved through running  the BatchJob FilelistJob on the replica. <br/> The filelist of a checksumsarchive is retrieved through a  GetAllFilenamesMessage.
 * @param replica The replica to retrieve the filelist from.
 * @return The names of the files in a list.
 * @throws ArgumentNotValid If the replica is 'null'.
 * @throws UnknownID If the replica has a unhandled replica type.
 */
private List<String> getFilenamesList(Replica replica) throws ArgumentNotValid, UnknownID {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  log.info(""String_Node_Str"" + replica + ""String_Node_Str"");
  return FileUtils.readListFromFile(ArcRepositoryClientFactory.getPreservationInstance().getAllFilenames(replica.getId()));
}",0.7856214811606756
89807,"/** 
 * This should creates a batch job for retrieving all the filenames.
 * @param msg The message.
 * @throws NotImplementedException Always, since this method has not yet been implemented.
 * @throws ArgumentNotValid If the GetAllFilenamesMessage is null.
 */
public void getAllFilenames(GetAllFilenamesMessage msg) throws NotImplementedException, ArgumentNotValid {
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  throw new NotImplementedException(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"");
}","/** 
 * Method for sending a GetAllFilenamesMessage to a checksum archive.
 * @param msg The GetAllFilenamesMessage, which will be send through the jms connection to the checksum archive.
 * @throws ArgumentNotValid If the GetAllFilenamesMessage is null.
 */
public void getAllFilenames(GetAllFilenamesMessage msg) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  jmsCon.resend(msg,theBamon);
  log.debug(""String_Node_Str"" + msg.toString() + ""String_Node_Str"");
}",0.5034280117531832
89808,"/** 
 * Forward the message to ALL_BA.
 * @param msg the message to forward.
 */
public void removeAndGetFile(RemoveAndGetFileMessage msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  con.resend(msg,this.allBa);
}","/** 
 * Forward the message to ALL_BA.
 * @param msg the message to forward.
 */
public void removeAndGetFile(RemoveAndGetFileMessage msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  jmsCon.resend(msg,this.allBa);
}",0.9890590809628008
89809,"/** 
 * Submit an upload request to the bitarchive.
 * @param rf The file to upload.
 * @throws IOFailure If access to file denied.
 * @throws ArgumentNotValid If arcfile is null.
 */
public void upload(RemoteFile rf) throws IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNull(rf,""String_Node_Str"");
  UploadMessage up=new UploadMessage(anyBa,clientId,rf);
  log.debug(""String_Node_Str"" + up.toString());
  con.send(up);
}","/** 
 * Submit an upload request to the bitarchive.
 * @param rf The file to upload.
 * @throws IOFailure If access to file denied.
 * @throws ArgumentNotValid If arcfile is null.
 */
public void upload(RemoteFile rf) throws IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNull(rf,""String_Node_Str"");
  UploadMessage up=new UploadMessage(anyBa,clientId,rf);
  log.debug(""String_Node_Str"" + up.toString());
  jmsCon.send(up);
}",0.994232987312572
89810,"/** 
 * Submit a batch job to the archive. This is used by the ArcRepository when it needs to run batch jobs for its own reasons, i.e. when checksumming a file as part of the Store operation.
 * @param replyChannel The channel that the reply of this job should be sent to.
 * @param job The job that should be run on the bit archive handled by thisclient.
 * @return The submitted message.
 * @throws ArgumentNotValid If any parameter was null.
 * @throws IOFailure If sending the batch message did not succeed.
 */
public BatchMessage batch(ChannelID replyChannel,FileBatchJob job) throws ArgumentNotValid, IOFailure {
  ArgumentNotValid.checkNotNull(replyChannel,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(job,""String_Node_Str"");
  BatchMessage bMsg=new BatchMessage(this.theBamon,replyChannel,job,""String_Node_Str"" + ""String_Node_Str"");
  con.send(bMsg);
  return bMsg;
}","/** 
 * Submit a batch job to the archive. This is used by the ArcRepository when it needs to run batch jobs for its own reasons, i.e. when checksumming a file as part of the Store operation.
 * @param replyChannel The channel that the reply of this job should be sent to.
 * @param job The job that should be run on the bit archive handled by thisclient.
 * @return The submitted message.
 * @throws ArgumentNotValid If any parameter was null.
 * @throws IOFailure If sending the batch message did not succeed.
 */
public BatchMessage batch(ChannelID replyChannel,FileBatchJob job) throws ArgumentNotValid, IOFailure {
  ArgumentNotValid.checkNotNull(replyChannel,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(job,""String_Node_Str"");
  BatchMessage bMsg=new BatchMessage(this.theBamon,replyChannel,job,""String_Node_Str"" + ""String_Node_Str"");
  jmsCon.send(bMsg);
  return bMsg;
}",0.9971735443753532
89811,"/** 
 * Establish the connection to the server.
 * @param allBaIn topic to all bitarchives
 * @param anyBaIn queue to one of the bitarchives
 * @param theBamonIn queue to the bitarchive monitor
 * @throws IOFailure If there is a problem making the connection.
 */
private BitarchiveClient(ChannelID allBaIn,ChannelID anyBaIn,ChannelID theBamonIn) throws IOFailure {
  this.allBa=allBaIn;
  this.anyBa=anyBaIn;
  this.theBamon=theBamonIn;
  con=JMSConnectionFactory.getInstance();
}","/** 
 * Establish the connection to the server.
 * @param allBaIn topic to all bitarchives
 * @param anyBaIn queue to one of the bitarchives
 * @param theBamonIn queue to the bitarchive monitor
 * @throws IOFailure If there is a problem making the connection.
 */
private BitarchiveClient(ChannelID allBaIn,ChannelID anyBaIn,ChannelID theBamonIn) throws IOFailure {
  this.allBa=allBaIn;
  this.anyBa=anyBaIn;
  this.theBamon=theBamonIn;
  jmsCon=JMSConnectionFactory.getInstance();
}",0.9948186528497408
89812,"/** 
 * This should creates a batch job for retrieving the checksum of the  wanted files.
 * @param replyChannel The channel where the reply should be sent.
 * @param filename The name of the file to retrieve the checksum from.
 * @return The message when it has been sent.
 * @throws NotImplementedException Always, since it has not yet been implemented.
 * @throws ArgumentNotValid If the replyChannel is null or the filename either is null or empty.
 */
public GetChecksumMessage getChecksum(ChannelID replyChannel,String filename) throws NotImplementedException, ArgumentNotValid {
  ArgumentNotValid.checkNotNull(replyChannel,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  throw new NotImplementedException(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"");
}","/** 
 * Method for retrieving the checksum of a specific arcfile within the archive.
 * @param replyChannel The channel where the reply should be sent.
 * @param filename The GetChecksumMessage which has been sent to the checksum archive though the jms connection.
 * @return The GetChecksumMessage which is sent.
 * @throws ArgumentNotValid If the reply channel is null or if the filenameis either null or the empty string.
 */
public GetChecksumMessage getChecksum(ChannelID replyChannel,String filename) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(replyChannel,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  GetChecksumMessage msg=new GetChecksumMessage(theBamon,replyChannel,filename,""String_Node_Str"" + ""String_Node_Str"");
  jmsCon.send(msg);
  log.debug(""String_Node_Str"" + msg.toString() + ""String_Node_Str"");
  return msg;
}",0.375366568914956
89813,"/** 
 * Submit an already constructed getfile message to the archive.
 * @param msg get file message to retrieve.
 */
public void getFile(GetFileMessage msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  log.debug(""String_Node_Str"" + msg + ""String_Node_Str"");
  con.resend(msg,this.allBa);
}","/** 
 * Submit an already constructed getfile message to the archive.
 * @param msg get file message to retrieve.
 */
public void getFile(GetFileMessage msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  log.debug(""String_Node_Str"" + msg + ""String_Node_Str"");
  jmsCon.resend(msg,this.allBa);
}",0.9918166939443536
89814,"/** 
 * Submit an already constructed batch message to the archive. The reply goes directly back to whoever sent the message.
 * @param msg the message to be processed by the get command.
 */
public void get(GetMessage msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  log.debug(""String_Node_Str"" + msg + ""String_Node_Str"");
  try {
    con.resend(msg,Channels.getAllBa());
  }
 catch (  Throwable e) {
    log.warn(""String_Node_Str"" + msg,e);
    try {
      msg.setNotOk(e);
      con.reply(msg);
    }
 catch (    Throwable e1) {
      log.warn(""String_Node_Str"",e1);
    }
  }
}","/** 
 * Submit an already constructed batch message to the archive. The reply goes directly back to whoever sent the message.
 * @param msg the message to be processed by the get command.
 */
public void get(GetMessage msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  log.debug(""String_Node_Str"" + msg + ""String_Node_Str"");
  try {
    jmsCon.resend(msg,Channels.getAllBa());
  }
 catch (  Throwable e) {
    log.warn(""String_Node_Str"" + msg,e);
    try {
      msg.setNotOk(e);
      jmsCon.reply(msg);
    }
 catch (    Throwable e1) {
      log.warn(""String_Node_Str"",e1);
    }
  }
}",0.991652754590985
89815,"/** 
 * This should creates a batch job for retrieving the checksum of all the  files.
 * @param msg The message.
 * @throws NotImplementedException Always, since this method has not yet been implemented.
 * @throws ArgumentNotValid If the GetAllChecksumMessage is null.
 */
public void getAllChecksums(GetAllChecksumsMessage msg) throws NotImplementedException, ArgumentNotValid {
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  throw new NotImplementedException(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"");
}","/** 
 * Method for sending the GetAllChecksumMessage to the ChecksumReplica.
 * @param msg The GetAllChecksumMessage, which will be sent through the jmsconnection to the checksum archive.
 * @throws ArgumentnotValid If the GetAllChecksumsMessage is null.
 */
public void getAllChecksums(GetAllChecksumsMessage msg) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  jmsCon.resend(msg,theBamon);
  log.debug(""String_Node_Str"" + msg.toString() + ""String_Node_Str"");
}",0.4685382381413359
89816,"/** 
 * The constructor.
 * @param configSource The source configuration file.
 */
public CreateTestInstance(File configSource){
  ArgumentNotValid.checkNotNull(configSource,""String_Node_Str"");
  source=configSource;
  deployConfiguration=new XmlStructure(source);
  offsetPaths=new OffsetSystem[0];
  offsetVal=new String();
}","/** 
 * The constructor.
 * @param configSource The source configuration file.
 */
public CreateTestInstance(File configSource){
  ArgumentNotValid.checkNotNull(configSource,""String_Node_Str"");
  source=configSource;
  deployConfiguration=new XmlStructure(source);
  offsetPaths=new OffsetSystem[0];
  offsetVal=""String_Node_Str"";
}",0.9559939301972686
89817,"/** 
 * Applies the test arguments. If the test arguments are given correctly, the configuration file is  loaded and changed appropriately, then written to a test configuration  file. The new test configuration file has the same name as the original  configuration file, except "".xml"" is replaced by ""_text.xml"". Thus ""path/config.xml"" -> ""path/config_test.xml"".  
 * @param testArguments The test arguments.
 */
private static void initTestArguments(String testArguments){
  if (testArguments == null || testArguments.isEmpty()) {
    return;
  }
  String[] changes=testArguments.split(Constants.REGEX_COMMA_CHARACTER);
  if (changes.length != Constants.TEST_ARGUMENTS_REQUIRED) {
    System.err.print(Constants.MSG_ERROR_TEST_ARGUMENTS);
    System.out.println();
    System.out.println(changes.length + ""String_Node_Str"" + Constants.TEST_ARGUMENTS_REQUIRED+ ""String_Node_Str"");
    System.out.println(""String_Node_Str"" + testArguments);
    System.exit(1);
  }
  try {
    CreateTestInstance cti=new CreateTestInstance(deployConfigFile);
    final int offsetIndex=0;
    final int httpIndex=1;
    final int environmentNameIndex=2;
    final int mailIndex=3;
    cti.applyTestArguments(changes[offsetIndex],changes[httpIndex],changes[environmentNameIndex],changes[mailIndex]);
    String tmp=deployConfigFile.getPath();
    String[] configFile=tmp.split(Constants.REGEX_DOT_CHARACTER);
    String nameOfNewConfig=configFile[0] + Constants.TEST_CONFIG_FILE_REPLACE_ENDING;
    cti.createConfigurationFile(nameOfNewConfig);
    deployConfigFile=new File(nameOfNewConfig);
  }
 catch (  IOException e) {
    System.out.println(""String_Node_Str"" + e);
    System.exit(1);
  }
}","/** 
 * Applies the test arguments. If the test arguments are given correctly, the configuration file is  loaded and changed appropriately, then written to a test configuration  file. The new test configuration file has the same name as the original  configuration file, except "".xml"" is replaced by ""_text.xml"". Thus ""path/config.xml"" -> ""path/config_test.xml"".  
 * @param testArguments The test arguments.
 */
private static void initTestArguments(String testArguments){
  if (testArguments == null || testArguments.isEmpty()) {
    return;
  }
  String[] changes=testArguments.split(Constants.REGEX_COMMA_CHARACTER);
  if (changes.length != Constants.TEST_ARGUMENTS_REQUIRED) {
    System.err.print(Constants.MSG_ERROR_TEST_ARGUMENTS);
    System.out.println();
    System.out.println(changes.length + ""String_Node_Str"" + Constants.TEST_ARGUMENTS_REQUIRED+ ""String_Node_Str"");
    System.out.println(""String_Node_Str"" + testArguments);
    System.exit(1);
  }
  try {
    CreateTestInstance cti=new CreateTestInstance(deployConfigFile);
    cti.applyTestArguments(changes[Constants.TEST_ARGUMENT_OFFSET_INDEX],changes[Constants.TEST_ARGUMENT_HTTP_INDEX],changes[Constants.TEST_ARGUMENT_ENVIRONMENT_NAME_INDEX],changes[Constants.TEST_ARGUMENT_MAIL_INDEX]);
    String tmp=deployConfigFile.getPath();
    String[] configFile=tmp.split(Constants.REGEX_DOT_CHARACTER);
    String nameOfNewConfig=configFile[0] + Constants.TEST_CONFIG_FILE_REPLACE_ENDING;
    cti.createConfigurationFile(nameOfNewConfig);
    deployConfigFile=new File(nameOfNewConfig);
  }
 catch (  IOException e) {
    System.out.println(""String_Node_Str"" + e);
    System.exit(1);
  }
}",0.8877551020408163
89818,"/** 
 * Creates a the log property file for every application. This is done by taking the inherited log file and changing  ""APPID"" in the file into the identification of the application.
 * @param directory The local directory for this machine
 */
protected void createLogPropertyFiles(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  for (  Application app : applications) {
    try {
      File logProp=new File(directory,Constants.LOG_PROP_APPLICATION_PREFIX + app.getIdentification() + Constants.LOG_PROP_APPLICATION_SUFFIX);
      FileWriter logfw=new FileWriter(logProp);
      String prop=FileUtils.readFile(inheritedLogPropFile);
      prop=prop.replace(Constants.LOG_PROPERTY_APPLICATION_ID_TAG,app.getIdentification());
      try {
        logfw.write(prop);
      }
  finally {
        if (logfw != null) {
          logfw.close();
        }
      }
    }
 catch (    IOException e) {
      String errMsg=""String_Node_Str"";
      log.warn(errMsg,e);
      throw new IOFailure(errMsg,e);
    }
  }
}","/** 
 * Creates a the log property file for every application. This is done by taking the inherited log file and changing  ""APPID"" in the file into the identification of the application.
 * @param directory The local directory for this machine
 */
protected void createLogPropertyFiles(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  for (  Application app : applications) {
    File logProp=new File(directory,Constants.LOG_PROP_APPLICATION_PREFIX + app.getIdentification() + Constants.LOG_PROP_APPLICATION_SUFFIX);
    try {
      PrintWriter logPrinter=new PrintWriter(logProp);
      try {
        String prop=FileUtils.readFile(inheritedLogPropFile);
        prop=prop.replace(Constants.LOG_PROPERTY_APPLICATION_ID_TAG,app.getIdentification());
        logPrinter.write(prop);
      }
  finally {
        logPrinter.close();
      }
    }
 catch (    IOException e) {
      String errMsg=""String_Node_Str"";
      log.warn(errMsg,e);
      throw new IOFailure(errMsg,e);
    }
  }
}",0.887378640776699
89819,"/** 
 * Copy inherited securityPolicyFile to local directory.
 * @param directory The local directory for this machine
 */
protected void createSecurityPolicyFile(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  try {
    File secPolFile=new File(directory,Constants.SECURITY_POLICY_FILE_NAME);
    FileWriter secfw=new FileWriter(secPolFile);
    String prop=FileUtils.readFile(inheritedSecurityPolicyFile);
    String monitorRole=settings.getLeafValue(Constants.SETTINGS_MONITOR_JMX_NAME_LEAF);
    if (monitorRole != null) {
      prop=prop.replace(Constants.SECURITY_JMX_PRINCIPAL_NAME_TAG,monitorRole);
    }
    String ctd=settings.getLeafValue(Constants.SETTINGS_TEMPDIR_LEAF);
    if (ctd != null) {
      prop=prop.replace(Constants.SECURITY_COMMON_TEMP_DIR_TAG,ctd);
    }
    try {
      secfw.write(prop);
      List<String> dirs=new ArrayList<String>();
      for (      Application app : applications) {
        String[] tmpDirs=app.getSettingsValues(Constants.SETTINGS_BITARCHIVE_BASEFILEDIR_LEAF);
        if (tmpDirs != null && tmpDirs.length > 0) {
          for (          String st : tmpDirs) {
            dirs.add(st);
          }
        }
      }
      if (!dirs.isEmpty()) {
        secfw.write(""String_Node_Str"" + ""String_Node_Str"");
        for (        String dir : dirs) {
          secfw.write(ScriptConstants.writeSecurityPolicyDirPermission(changeFileDirPathForSecurity(dir)));
        }
        secfw.write(""String_Node_Str"");
      }
    }
  finally {
      if (secfw != null) {
        secfw.close();
      }
    }
  }
 catch (  IOException e) {
    String errMsg=""String_Node_Str"";
    log.warn(errMsg,e);
    throw new IOFailure(errMsg,e);
  }
}","/** 
 * Copy inherited securityPolicyFile to local directory.
 * @param directory The local directory for this machine
 */
protected void createSecurityPolicyFile(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  File secPolFile=new File(directory,Constants.SECURITY_POLICY_FILE_NAME);
  try {
    PrintWriter secPrinter=new PrintWriter(secPolFile);
    try {
      String prop=FileUtils.readFile(inheritedSecurityPolicyFile);
      String monitorRole=settings.getLeafValue(Constants.SETTINGS_MONITOR_JMX_NAME_LEAF);
      if (monitorRole != null) {
        prop=prop.replace(Constants.SECURITY_JMX_PRINCIPAL_NAME_TAG,monitorRole);
      }
      String ctd=settings.getLeafValue(Constants.SETTINGS_TEMPDIR_LEAF);
      if (ctd != null) {
        prop=prop.replace(Constants.SECURITY_COMMON_TEMP_DIR_TAG,ctd);
      }
      secPrinter.write(prop);
      List<String> dirs=new ArrayList<String>();
      for (      Application app : applications) {
        String[] tmpDirs=app.getSettingsValues(Constants.SETTINGS_BITARCHIVE_BASEFILEDIR_LEAF);
        if (tmpDirs != null && tmpDirs.length > 0) {
          for (          String st : tmpDirs) {
            dirs.add(st);
          }
        }
      }
      if (!dirs.isEmpty()) {
        secPrinter.write(""String_Node_Str"" + ""String_Node_Str"");
        for (        String dir : dirs) {
          secPrinter.write(ScriptConstants.writeSecurityPolicyDirPermission(changeFileDirPathForSecurity(dir)));
        }
        secPrinter.write(""String_Node_Str"");
      }
    }
  finally {
      secPrinter.close();
    }
  }
 catch (  IOException e) {
    String errMsg=""String_Node_Str"";
    log.warn(errMsg,e);
    throw new IOFailure(errMsg,e);
  }
}",0.8659494038964816
89820,"/** 
 * Creates the following scripts for this physical location. * killall. * install. * startall. The scripts for a physical location will only work from Linux/Unix.  
 * @param directory The directory where the scripts are to be placed.
 */
private void makeScripts(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  String ext=Constants.UNDERSCORE + name + Constants.SCRIPT_EXTENSION_LINUX;
  File killall=new File(directory,Constants.SCRIPT_NAME_KILL_ALL + ext);
  File install=new File(directory,Constants.SCRIPT_NAME_INSTALL_ALL + ext);
  File startall=new File(directory,Constants.SCRIPT_NAME_START_ALL + ext);
  try {
    PrintWriter kWriter=new PrintWriter(killall);
    PrintWriter iWriter=new PrintWriter(install);
    PrintWriter sWriter=new PrintWriter(startall);
    try {
      kWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      iWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      sWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      for (      Machine mac : machines) {
        iWriter.println(ScriptConstants.writeDashLine());
        iWriter.print(mac.writeToGlobalInstallScript());
        sWriter.println(ScriptConstants.writeDashLine());
        sWriter.print(mac.writeToGlobalStartScript());
        kWriter.println(ScriptConstants.writeDashLine());
        kWriter.print(mac.writeToGlobalKillScript());
      }
      kWriter.println(ScriptConstants.writeDashLine());
      iWriter.println(ScriptConstants.writeDashLine());
      sWriter.println(ScriptConstants.writeDashLine());
    }
  finally {
      if (kWriter != null) {
        kWriter.flush();
        kWriter.close();
      }
      if (iWriter != null) {
        iWriter.flush();
        iWriter.close();
      }
      if (sWriter != null) {
        sWriter.flush();
        sWriter.close();
      }
    }
  }
 catch (  IOException e) {
    String msg=""String_Node_Str"" + ""String_Node_Str"" + name + ""String_Node_Str"";
    log.trace(msg,e);
    throw new IOFailure(msg,e);
  }
}","/** 
 * Creates the following scripts for this physical location. * killall. * install. * startall. The scripts for a physical location will only work from Linux/Unix.  
 * @param directory The directory where the scripts are to be placed.
 */
private void makeScripts(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  String ext=Constants.UNDERSCORE + name + Constants.SCRIPT_EXTENSION_LINUX;
  File killall=new File(directory,Constants.SCRIPT_NAME_KILL_ALL + ext);
  File install=new File(directory,Constants.SCRIPT_NAME_INSTALL_ALL + ext);
  File startall=new File(directory,Constants.SCRIPT_NAME_START_ALL + ext);
  try {
    PrintWriter kWriter=new PrintWriter(killall);
    try {
      kWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      for (      Machine mac : machines) {
        kWriter.println(ScriptConstants.writeDashLine());
        kWriter.print(mac.writeToGlobalKillScript());
      }
      kWriter.println(ScriptConstants.writeDashLine());
    }
  finally {
      kWriter.flush();
      kWriter.close();
    }
    PrintWriter iWriter=new PrintWriter(install);
    try {
      iWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      for (      Machine mac : machines) {
        iWriter.println(ScriptConstants.writeDashLine());
        iWriter.print(mac.writeToGlobalInstallScript());
      }
      iWriter.println(ScriptConstants.writeDashLine());
    }
  finally {
      iWriter.flush();
      iWriter.close();
    }
    PrintWriter sWriter=new PrintWriter(startall);
    try {
      sWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      for (      Machine mac : machines) {
        sWriter.println(ScriptConstants.writeDashLine());
        sWriter.print(mac.writeToGlobalStartScript());
      }
      sWriter.println(ScriptConstants.writeDashLine());
    }
  finally {
      sWriter.flush();
      sWriter.close();
    }
  }
 catch (  IOException e) {
    String msg=""String_Node_Str"" + ""String_Node_Str"" + name + ""String_Node_Str"";
    log.trace(msg,e);
    throw new IOFailure(msg,e);
  }
}",0.6342786683107275
89821,"/** 
 * Creates the kill scripts for all the applications. Two script files are created: kill_app.bat and kill_ps_app.bat.  kill_ps_app.bat kills the process of the application. kill_app.bat runs kill_ps_app.bat if the application is running. run_app tells if the application is running. It is deleted during kill. The kill_app.bat should have the following structure: - ECHO Killing application : app - CD ""path"" - IF EXIST run_app GOTO KILL - GOTO NOKILL -  - :KILL - cmdrun kill_ps_app.bat - DEL run_app - GOTO DONE -  - :NOKILL - ECHO Cannot kill application. Already running. - - :DONE  where: app = application name. path = the path to the ./conf directory. cmdrun = the windows command to run other batch programs. The kill_ps_app.bat is empty upon creation. When the application is started, the command to kill the process of  he application is written to this file as the only content. It will look something like this: - taskkill /F /PID id where: id = the process identification number of the running application.  TODO kill the potential heritrix process, created by a harvester. Just like on Linux/Unix.  If we in the future add the possibility of running heritrix on Windows.
 * @param directory The directory for this machine (use global variable?).
 */
@Override protected void createApplicationKillScripts(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  for (  Application app : applications) {
    String id=app.getIdentification();
    String killPsName=Constants.SCRIPT_KILL_PS + id + scriptExtension;
    File appKillScript=new File(directory,Constants.SCRIPT_NAME_LOCAL_KILL + id + scriptExtension);
    File appKillPsScript=new File(directory,killPsName);
    try {
      PrintWriter appPrint=new PrintWriter(appKillScript);
      PrintWriter appPsPrint=new PrintWriter(appKillPsScript);
      try {
        appPsPrint.println(""String_Node_Str"");
        String tmpRunApp=Constants.FILE_TEMPORARY_RUN_WINDOWS_NAME + id;
        appPrint.println(ScriptConstants.ECHO_KILL_WINDOWS_APPLICATION + Constants.COLON + Constants.SPACE+ id);
        appPrint.println(ScriptConstants.CD + Constants.SPACE + Constants.QUOTE_MARK+ app.installPathWindows()+ Constants.CONF_DIR_WINDOWS+ Constants.QUOTE_MARK);
        appPrint.println(ScriptConstants.IF + Constants.SPACE + ScriptConstants.EXIST+ Constants.SPACE+ tmpRunApp+ Constants.SPACE+ ScriptConstants.GOTO+ Constants.SPACE+ ScriptConstants.LABEL_KILL);
        appPrint.println(ScriptConstants.GOTO + Constants.SPACE + ScriptConstants.LABEL_NOKILL);
        appPrint.println();
        appPrint.println(Constants.COLON + ScriptConstants.LABEL_KILL);
        appPrint.println(ScriptConstants.OPERATING_SYSTEM_WINDOWS_RUN_BATCH_FILE + Constants.QUOTE_MARK + killPsName+ Constants.QUOTE_MARK);
        appPrint.println(ScriptConstants.DEL + Constants.SPACE + tmpRunApp);
        appPrint.println(ScriptConstants.GOTO + Constants.SPACE + ScriptConstants.LABEL_DONE);
        appPrint.println();
        appPrint.println(Constants.COLON + ScriptConstants.LABEL_NOKILL);
        appPrint.println(ScriptConstants.ECHO_CANNOT_KILL_APP);
        appPrint.println();
        appPrint.println(Constants.COLON + ScriptConstants.LABEL_DONE);
      }
  finally {
        if (appPrint != null) {
          appPrint.close();
        }
        if (appPsPrint != null) {
          appPsPrint.close();
        }
      }
    }
 catch (    IOException e) {
      String msg=""String_Node_Str"" + ""String_Node_Str"" + app.getIdentification() + ""String_Node_Str""+ name+ ""String_Node_Str"";
      log.trace(msg,e);
      throw new IOFailure(msg,e);
    }
  }
}","/** 
 * Creates the kill scripts for all the applications. Two script files are created: kill_app.bat and kill_ps_app.bat.  kill_ps_app.bat kills the process of the application. kill_app.bat runs kill_ps_app.bat if the application is running. run_app tells if the application is running. It is deleted during kill. The kill_app.bat should have the following structure: - ECHO Killing application : app - CD ""path"" - IF EXIST run_app GOTO KILL - GOTO NOKILL -  - :KILL - cmdrun kill_ps_app.bat - DEL run_app - GOTO DONE -  - :NOKILL - ECHO Cannot kill application. Already running. - - :DONE  where: app = application name. path = the path to the ./conf directory. cmdrun = the windows command to run other batch programs. The kill_ps_app.bat is empty upon creation. When the application is started, the command to kill the process of  he application is written to this file as the only content. It will look something like this: - taskkill /F /PID id where: id = the process identification number of the running application.  TODO kill the potential heritrix process, created by a harvester. Just like on Linux/Unix.  If we in the future add the possibility of running heritrix on Windows.
 * @param directory The directory for this machine (use global variable?).
 */
@Override protected void createApplicationKillScripts(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  for (  Application app : applications) {
    String id=app.getIdentification();
    String killPsName=Constants.SCRIPT_KILL_PS + id + scriptExtension;
    File appKillScript=new File(directory,Constants.SCRIPT_NAME_LOCAL_KILL + id + scriptExtension);
    File appKillPsScript=new File(directory,killPsName);
    try {
      PrintWriter appPrint=new PrintWriter(appKillScript);
      try {
        String tmpRunApp=Constants.FILE_TEMPORARY_RUN_WINDOWS_NAME + id;
        appPrint.println(ScriptConstants.ECHO_KILL_WINDOWS_APPLICATION + Constants.COLON + Constants.SPACE+ id);
        appPrint.println(ScriptConstants.CD + Constants.SPACE + Constants.QUOTE_MARK+ app.installPathWindows()+ Constants.CONF_DIR_WINDOWS+ Constants.QUOTE_MARK);
        appPrint.println(ScriptConstants.IF + Constants.SPACE + ScriptConstants.EXIST+ Constants.SPACE+ tmpRunApp+ Constants.SPACE+ ScriptConstants.GOTO+ Constants.SPACE+ ScriptConstants.LABEL_KILL);
        appPrint.println(ScriptConstants.GOTO + Constants.SPACE + ScriptConstants.LABEL_NOKILL);
        appPrint.println();
        appPrint.println(Constants.COLON + ScriptConstants.LABEL_KILL);
        appPrint.println(ScriptConstants.OPERATING_SYSTEM_WINDOWS_RUN_BATCH_FILE + Constants.QUOTE_MARK + killPsName+ Constants.QUOTE_MARK);
        appPrint.println(ScriptConstants.DEL + Constants.SPACE + tmpRunApp);
        appPrint.println(ScriptConstants.GOTO + Constants.SPACE + ScriptConstants.LABEL_DONE);
        appPrint.println();
        appPrint.println(Constants.COLON + ScriptConstants.LABEL_NOKILL);
        appPrint.println(ScriptConstants.ECHO_CANNOT_KILL_APP);
        appPrint.println();
        appPrint.println(Constants.COLON + ScriptConstants.LABEL_DONE);
      }
  finally {
        appPrint.close();
      }
      PrintWriter appPsPrint=new PrintWriter(appKillPsScript);
      try {
        appPsPrint.println(""String_Node_Str"");
      }
  finally {
        appPsPrint.close();
      }
    }
 catch (    IOException e) {
      String msg=""String_Node_Str"" + ""String_Node_Str"" + app.getIdentification() + ""String_Node_Str""+ name+ ""String_Node_Str"";
      log.trace(msg,e);
      throw new IOFailure(msg,e);
    }
  }
}",0.9564614531336662
89822,"/** 
 * For acquiring all the values of the leafs at the end of the path.
 * @param path The path to the branches.
 * @return The values of the leafs.
 */
public String[] getSettingsValues(String[] path){
  ArgumentNotValid.checkNotNull(path,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(path.length,""String_Node_Str"");
  return settings.getLeafValues(path);
}","/** 
 * For acquiring all the values of the leafs at the end of the path.
 * @param path The path to the branches.
 * @return The values of the leafs. If no values were found, then an emptycollection of strings are returned.
 */
public String[] getSettingsValues(String[] path){
  ArgumentNotValid.checkNotNull(path,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(path.length,""String_Node_Str"");
  return settings.getLeafValues(path);
}",0.9090909090909092
89823,"/** 
 * This method does the following: Retrieves the path to the jmxremote.access and jmxremote.password files. Moves these files, if they are different from standard. Makes the jmxremote.access and jmxremote.password files readonly.
 * @return The commands for handling the jmxremote files.
 */
@Override protected String getJMXremoteFilesCommand(){
  String accessFilePath;
  String passwordFilePath;
  String[] options;
  options=settings.getLeafValues(Constants.SETTINGS_COMMON_JMX_ACCESSFILE);
  if (options == null || options.length < 0) {
    accessFilePath=Constants.JMX_ACCESS_FILE_PATH_DEFAULT;
  }
 else {
    accessFilePath=options[0];
    if (options.length > 1) {
      log.debug(Constants.MSG_WARN_TOO_MANY_JMXREMOTE_FILE_PATHS);
    }
  }
  options=settings.getLeafValues(Constants.SETTINGS_COMMON_JMX_PASSWORDFILE);
  if (options == null || options.length < 0) {
    passwordFilePath=Constants.JMX_PASSWORD_FILE_PATH_DEFAULT;
  }
 else {
    passwordFilePath=options[0];
    if (options.length > 1) {
      log.debug(Constants.MSG_WARN_TOO_MANY_JMXREMOTE_FILE_PATHS);
    }
  }
  StringBuilder res=new StringBuilder();
  res.append(ScriptConstants.ECHO_MAKE_PASSWORD_FILES);
  res.append(Constants.NEWLINE);
  if (!accessFilePath.equals(Constants.JMX_ACCESS_FILE_PATH_DEFAULT)) {
    res.append(ScriptConstants.SSH + Constants.SPACE);
    res.append(machineUserLogin());
    res.append(Constants.SPACE + Constants.QUOTE_MARK);
    res.append(ScriptConstants.LINUX_FORCE_MOVE);
    res.append(Constants.SPACE);
    res.append(getInstallDirPath());
    res.append(Constants.SLASH);
    res.append(Constants.JMX_ACCESS_FILE_PATH_DEFAULT);
    res.append(Constants.SPACE);
    res.append(getInstallDirPath());
    res.append(Constants.SLASH);
    res.append(accessFilePath);
    res.append(Constants.QUOTE_MARK);
    res.append(Constants.NEWLINE);
  }
  if (!passwordFilePath.equals(Constants.JMX_PASSWORD_FILE_PATH_DEFAULT)) {
    res.append(ScriptConstants.SSH + Constants.SPACE);
    res.append(machineUserLogin());
    res.append(Constants.SPACE + Constants.QUOTE_MARK);
    res.append(ScriptConstants.LINUX_FORCE_MOVE);
    res.append(Constants.SPACE);
    res.append(getInstallDirPath());
    res.append(Constants.SLASH);
    res.append(Constants.JMX_PASSWORD_FILE_PATH_DEFAULT);
    res.append(Constants.SPACE);
    res.append(getInstallDirPath());
    res.append(Constants.SLASH);
    res.append(passwordFilePath);
    res.append(Constants.QUOTE_MARK);
    res.append(Constants.NEWLINE);
  }
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + ScriptConstants.LINUX_USER_400+ Constants.SPACE);
  res.append(getInstallDirPath());
  res.append(Constants.SLASH);
  res.append(passwordFilePath);
  res.append(Constants.QUOTE_MARK);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + ScriptConstants.LINUX_USER_400+ Constants.SPACE);
  res.append(getInstallDirPath());
  res.append(Constants.SLASH);
  res.append(accessFilePath);
  res.append(Constants.QUOTE_MARK);
  res.append(Constants.NEWLINE);
  return res.toString();
}","/** 
 * This method does the following: Retrieves the path to the jmxremote.access and jmxremote.password files. Moves these files, if they are different from standard. Makes the jmxremote.access and jmxremote.password files readonly.
 * @return The commands for handling the jmxremote files.
 */
@Override protected String getJMXremoteFilesCommand(){
  String accessFilePath;
  String passwordFilePath;
  String[] options;
  options=settings.getLeafValues(Constants.SETTINGS_COMMON_JMX_ACCESSFILE);
  if (options.length == 0) {
    accessFilePath=Constants.JMX_ACCESS_FILE_PATH_DEFAULT;
  }
 else {
    accessFilePath=options[0];
    if (options.length > 1) {
      log.debug(Constants.MSG_WARN_TOO_MANY_JMXREMOTE_FILE_PATHS);
    }
  }
  options=settings.getLeafValues(Constants.SETTINGS_COMMON_JMX_PASSWORDFILE);
  if (options.length == 0) {
    passwordFilePath=Constants.JMX_PASSWORD_FILE_PATH_DEFAULT;
  }
 else {
    passwordFilePath=options[0];
    if (options.length > 1) {
      log.debug(Constants.MSG_WARN_TOO_MANY_JMXREMOTE_FILE_PATHS);
    }
  }
  StringBuilder res=new StringBuilder();
  res.append(ScriptConstants.ECHO_MAKE_PASSWORD_FILES);
  res.append(Constants.NEWLINE);
  if (!accessFilePath.equals(Constants.JMX_ACCESS_FILE_PATH_DEFAULT)) {
    res.append(ScriptConstants.SSH + Constants.SPACE);
    res.append(machineUserLogin());
    res.append(Constants.SPACE + Constants.QUOTE_MARK);
    res.append(ScriptConstants.LINUX_FORCE_MOVE);
    res.append(Constants.SPACE);
    res.append(getInstallDirPath());
    res.append(Constants.SLASH);
    res.append(Constants.JMX_ACCESS_FILE_PATH_DEFAULT);
    res.append(Constants.SPACE);
    res.append(getInstallDirPath());
    res.append(Constants.SLASH);
    res.append(accessFilePath);
    res.append(Constants.QUOTE_MARK);
    res.append(Constants.NEWLINE);
  }
  if (!passwordFilePath.equals(Constants.JMX_PASSWORD_FILE_PATH_DEFAULT)) {
    res.append(ScriptConstants.SSH + Constants.SPACE);
    res.append(machineUserLogin());
    res.append(Constants.SPACE + Constants.QUOTE_MARK);
    res.append(ScriptConstants.LINUX_FORCE_MOVE);
    res.append(Constants.SPACE);
    res.append(getInstallDirPath());
    res.append(Constants.SLASH);
    res.append(Constants.JMX_PASSWORD_FILE_PATH_DEFAULT);
    res.append(Constants.SPACE);
    res.append(getInstallDirPath());
    res.append(Constants.SLASH);
    res.append(passwordFilePath);
    res.append(Constants.QUOTE_MARK);
    res.append(Constants.NEWLINE);
  }
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + ScriptConstants.LINUX_USER_400+ Constants.SPACE);
  res.append(getInstallDirPath());
  res.append(Constants.SLASH);
  res.append(passwordFilePath);
  res.append(Constants.QUOTE_MARK);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + ScriptConstants.LINUX_USER_400+ Constants.SPACE);
  res.append(getInstallDirPath());
  res.append(Constants.SLASH);
  res.append(accessFilePath);
  res.append(Constants.QUOTE_MARK);
  res.append(Constants.NEWLINE);
  return res.toString();
}",0.9901112484548824
89824,"/** 
 * Creates the kill scripts for all the applications. The script starts by finding all running processes of the application. If it finds any processes, it kills them.  The kill_app.sh should have the following structure: - echo Killing linux application. - #!/bin/bash - PIDS = $(ps -wwfe | grep fullapp | grep -v grep | grep  path\settings_app.xml | awk ""{print \\$2}"") - if [ -n ""$PIDS"" ]; then -     kill -9 $PIDS; - fi Also, if a heritrix process is started, the following is added: - PIDS = $(ps -wwfe | grep heritrix | grep -v grep | grep  path\settings_app.xml | awk ""{print \\$2}"") - if [ -n ""$PIDS"" ]; then -     kill -9 $PIDS; - fi where: path = the path to the ./conf directory. fullapp = the full application name with class path. app = the id of the application (name + instanceId). heritrix = the heritrix class path.
 * @param directory The directory for this machine (use global variable?).
 */
@Override protected void createApplicationKillScripts(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  for (  Application app : applications) {
    File appKillScript=new File(directory,Constants.SCRIPT_NAME_LOCAL_KILL + app.getIdentification() + scriptExtension);
    try {
      PrintWriter appPrint=new PrintWriter(appKillScript);
      try {
        appPrint.println(ScriptConstants.ECHO_KILL_LINUX_APPLICATION + Constants.COLON + Constants.SPACE+ app.getIdentification());
        appPrint.println(ScriptConstants.BIN_BASH_COMMENT);
        appPrint.println(ScriptConstants.getLinuxPIDS(app.getTotalName(),getConfDirPath(),app.getIdentification()));
        appPrint.println(ScriptConstants.LINUX_IF_N_EXIST + Constants.SPACE + Constants.QUOTE_MARK+ ScriptConstants.PIDS+ Constants.QUOTE_MARK+ Constants.SPACE+ ScriptConstants.LINUX_N_THEN);
        appPrint.println(ScriptConstants.KILL_9_PIDS + Constants.SEMICOLON);
        appPrint.println(ScriptConstants.FI);
        String[] heritrixJmxPort=app.getSettingsValues(Constants.SETTINGS_HARVEST_HETRIX_JMX_PORT);
        if (heritrixJmxPort != null) {
          if (heritrixJmxPort.length > 1) {
            log.trace(heritrixJmxPort.length + ""String_Node_Str"" + ""String_Node_Str"");
          }
          appPrint.println(ScriptConstants.getLinuxPIDS(Heritrix.class.getName(),getConfDirPath(),app.getIdentification()));
          appPrint.println(ScriptConstants.LINUX_IF_N_EXIST + Constants.SPACE + Constants.QUOTE_MARK+ ScriptConstants.PIDS+ Constants.QUOTE_MARK+ Constants.SPACE+ ScriptConstants.LINUX_N_THEN);
          appPrint.println(ScriptConstants.KILL_9_PIDS);
          appPrint.println(ScriptConstants.FI);
        }
      }
  finally {
        appPrint.close();
      }
    }
 catch (    IOException e) {
      String msg=""String_Node_Str"" + e;
      log.trace(msg);
      throw new IOFailure(msg);
    }
catch (    Exception e) {
      String msg=""String_Node_Str"" + e;
      log.trace(msg);
      System.out.println(msg);
    }
  }
}","/** 
 * Creates the kill scripts for all the applications. The script starts by finding all running processes of the application. If it finds any processes, it kills them.  The kill_app.sh should have the following structure: - echo Killing linux application. - #!/bin/bash - PIDS = $(ps -wwfe | grep fullapp | grep -v grep | grep  path\settings_app.xml | awk ""{print \\$2}"") - if [ -n ""$PIDS"" ]; then -     kill -9 $PIDS; - fi Also, if a heritrix process is started, the following is added: - PIDS = $(ps -wwfe | grep heritrix | grep -v grep | grep  path\settings_app.xml | awk ""{print \\$2}"") - if [ -n ""$PIDS"" ]; then -     kill -9 $PIDS; - fi where: path = the path to the ./conf directory. fullapp = the full application name with class path. app = the id of the application (name + instanceId). heritrix = the heritrix class path.
 * @param directory The directory for this machine (use global variable?).
 */
@Override protected void createApplicationKillScripts(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  for (  Application app : applications) {
    File appKillScript=new File(directory,Constants.SCRIPT_NAME_LOCAL_KILL + app.getIdentification() + scriptExtension);
    try {
      PrintWriter appPrint=new PrintWriter(appKillScript);
      try {
        appPrint.println(ScriptConstants.ECHO_KILL_LINUX_APPLICATION + Constants.COLON + Constants.SPACE+ app.getIdentification());
        appPrint.println(ScriptConstants.BIN_BASH_COMMENT);
        appPrint.println(ScriptConstants.getLinuxPIDS(app.getTotalName(),getConfDirPath(),app.getIdentification()));
        appPrint.println(ScriptConstants.LINUX_IF_N_EXIST + Constants.SPACE + Constants.QUOTE_MARK+ ScriptConstants.PIDS+ Constants.QUOTE_MARK+ Constants.SPACE+ ScriptConstants.LINUX_N_THEN);
        appPrint.println(ScriptConstants.KILL_9_PIDS + Constants.SEMICOLON);
        appPrint.println(ScriptConstants.FI);
        String[] heritrixJmxPort=app.getSettingsValues(Constants.SETTINGS_HARVEST_HETRIX_JMX_PORT);
        if (heritrixJmxPort != null && heritrixJmxPort.length > 0) {
          if (heritrixJmxPort.length > 1) {
            log.trace(heritrixJmxPort.length + ""String_Node_Str"" + ""String_Node_Str"");
          }
          appPrint.println(ScriptConstants.getLinuxPIDS(Heritrix.class.getName(),getConfDirPath(),app.getIdentification()));
          appPrint.println(ScriptConstants.LINUX_IF_N_EXIST + Constants.SPACE + Constants.QUOTE_MARK+ ScriptConstants.PIDS+ Constants.QUOTE_MARK+ Constants.SPACE+ ScriptConstants.LINUX_N_THEN);
          appPrint.println(ScriptConstants.KILL_9_PIDS);
          appPrint.println(ScriptConstants.FI);
        }
      }
  finally {
        appPrint.close();
      }
    }
 catch (    IOException e) {
      String msg=""String_Node_Str"" + e;
      log.trace(msg);
      throw new IOFailure(msg);
    }
catch (    Exception e) {
      String msg=""String_Node_Str"" + e;
      log.trace(msg);
      System.out.println(msg);
    }
  }
}",0.9949460916442048
89825,"/** 
 * Creates the following scripts for this physical location. * killall. * install. * startall. The scripts for a physical location will only work from Linux/Unix.  
 * @param directory The directory where the scripts are to be placed.
 */
private void makeScripts(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  String ext=Constants.UNDERSCORE + name + Constants.SCRIPT_EXTENSION_LINUX;
  File killall=new File(directory,Constants.SCRIPT_NAME_KILL_ALL + ext);
  File install=new File(directory,Constants.SCRIPT_NAME_INSTALL_ALL + ext);
  File startall=new File(directory,Constants.SCRIPT_NAME_START_ALL + ext);
  try {
    PrintWriter kWriter=new PrintWriter(killall);
    PrintWriter iWriter=new PrintWriter(install);
    PrintWriter sWriter=new PrintWriter(startall);
    try {
      kWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      iWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      sWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      for (      Machine mac : machines) {
        iWriter.println(ScriptConstants.writeDashLine());
        iWriter.print(mac.writeToGlobalInstallScript());
        sWriter.println(ScriptConstants.writeDashLine());
        sWriter.print(mac.writeToGlobalStartScript());
        kWriter.println(ScriptConstants.writeDashLine());
        kWriter.print(mac.writeToGlobalKillScript());
      }
      kWriter.println(ScriptConstants.writeDashLine());
      iWriter.println(ScriptConstants.writeDashLine());
      sWriter.println(ScriptConstants.writeDashLine());
    }
  finally {
      if (kWriter != null) {
        kWriter.close();
      }
      if (iWriter != null) {
        iWriter.close();
      }
      if (sWriter != null) {
        sWriter.close();
      }
    }
  }
 catch (  IOException e) {
    String msg=""String_Node_Str"" + ""String_Node_Str"" + e;
    log.trace(msg);
    throw new IOFailure(msg);
  }
catch (  Exception e) {
    String msg=""String_Node_Str"" + ""String_Node_Str"" + e;
    log.trace(msg);
    System.out.println(msg);
  }
}","/** 
 * Creates the following scripts for this physical location. * killall. * install. * startall. The scripts for a physical location will only work from Linux/Unix.  
 * @param directory The directory where the scripts are to be placed.
 */
private void makeScripts(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  String ext=Constants.UNDERSCORE + name + Constants.SCRIPT_EXTENSION_LINUX;
  File killall=new File(directory,Constants.SCRIPT_NAME_KILL_ALL + ext);
  File install=new File(directory,Constants.SCRIPT_NAME_INSTALL_ALL + ext);
  File startall=new File(directory,Constants.SCRIPT_NAME_START_ALL + ext);
  try {
    PrintWriter kWriter=new PrintWriter(killall);
    PrintWriter iWriter=new PrintWriter(install);
    PrintWriter sWriter=new PrintWriter(startall);
    try {
      kWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      iWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      sWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      for (      Machine mac : machines) {
        iWriter.println(ScriptConstants.writeDashLine());
        iWriter.print(mac.writeToGlobalInstallScript());
        sWriter.println(ScriptConstants.writeDashLine());
        sWriter.print(mac.writeToGlobalStartScript());
        kWriter.println(ScriptConstants.writeDashLine());
        kWriter.print(mac.writeToGlobalKillScript());
      }
      kWriter.println(ScriptConstants.writeDashLine());
      iWriter.println(ScriptConstants.writeDashLine());
      sWriter.println(ScriptConstants.writeDashLine());
    }
  finally {
      if (kWriter != null) {
        kWriter.flush();
        kWriter.close();
      }
      if (iWriter != null) {
        iWriter.flush();
        iWriter.close();
      }
      if (sWriter != null) {
        sWriter.flush();
        sWriter.close();
      }
    }
  }
 catch (  IOException e) {
    String msg=""String_Node_Str"" + ""String_Node_Str"" + e;
    log.trace(msg);
    throw new IOFailure(msg);
  }
catch (  Exception e) {
    String msg=""String_Node_Str"" + ""String_Node_Str"" + e;
    log.trace(msg);
    System.out.println(msg);
  }
}",0.9819233550253073
89826,"/** 
 * Creates the script for creating the application specified directories. Also creates the directories along the path to the directories.
 * @return The script for creating the application specified directories.
 */
@Override protected String getAppDirectories(){
  StringBuilder res=new StringBuilder();
  String[] dirs;
  for (  Application app : applications) {
    dirs=app.getSettingsValues(Constants.SETTINGS_BITARCHIVE_BASEFILEDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      for (      String dir : dirs) {
        res.append(createPathToDir(dir));
        res.append(scriptCreateDir(dir,false));
        for (        String subdir : Constants.BASEFILEDIR_SUBDIRECTORIES) {
          res.append(scriptCreateDir(dir + Constants.BACKSLASH + subdir,false));
        }
      }
    }
    dirs=app.getSettingsValues(Constants.SETTINGS_HARVEST_SERVERDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      for (      String dir : dirs) {
        res.append(createPathToDir(dir));
        res.append(scriptCreateDir(dir,false));
      }
    }
    dirs=app.getSettingsValues(Constants.SETTINGS_VIEWERPROXY_BASEDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      for (      String dir : dirs) {
        res.append(createPathToDir(dir));
        res.append(scriptCreateDir(dir,false));
      }
    }
    dirs=app.getSettingsValues(Constants.SETTINGS_TEMPDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      String machineDir=settings.getLeafValue(Constants.SETTINGS_TEMPDIR_LEAF);
      for (      String dir : dirs) {
        if (dir.equals(machineDir)) {
          res.append(createPathToDir(dir));
          res.append(scriptCreateDir(dir,resetTempDir));
        }
      }
    }
  }
  return res.toString();
}","/** 
 * Creates the script for creating the application specified directories. Also creates the directories along the path to the directories.
 * @return The script for creating the application specified directories.
 */
@Override protected String getAppDirectories(){
  StringBuilder res=new StringBuilder();
  String[] dirs;
  for (  Application app : applications) {
    dirs=app.getSettingsValues(Constants.SETTINGS_BITARCHIVE_BASEFILEDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      for (      String dir : dirs) {
        res.append(createPathToDir(dir));
        res.append(scriptCreateDir(dir,false));
        for (        String subdir : Constants.BASEFILEDIR_SUBDIRECTORIES) {
          res.append(scriptCreateDir(dir + Constants.BACKSLASH + subdir,false));
        }
      }
    }
    dirs=app.getSettingsValues(Constants.SETTINGS_HARVEST_SERVERDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      for (      String dir : dirs) {
        res.append(createPathToDir(dir));
        res.append(scriptCreateDir(dir,false));
      }
    }
    dirs=app.getSettingsValues(Constants.SETTINGS_VIEWERPROXY_BASEDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      for (      String dir : dirs) {
        res.append(createPathToDir(dir));
        res.append(scriptCreateDir(dir,false));
      }
    }
    dirs=app.getSettingsValues(Constants.SETTINGS_TEMPDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      String machineDir=settings.getLeafValue(Constants.SETTINGS_TEMPDIR_LEAF);
      for (      String dir : dirs) {
        if (!dir.equals(machineDir)) {
          res.append(createPathToDir(dir));
          res.append(scriptCreateDir(dir,resetTempDir));
        }
      }
    }
  }
  return res.toString();
}",0.999713384924047
89827,"/** 
 * This method does the following: Retrieves the path to the jmxremote.access and jmxremote.password files. Moves these files, if they are different from standard. This has to be a force move (command 'move /Y'). Makes the jmxremote.access and jmxremote.password files readonly.
 * @return The commands for handling the jmxremote files.
 */
@Override protected String getJMXremoteFilesCommand(){
  String accessFilePath;
  String passwordFilePath;
  String[] options;
  options=settings.getLeafValues(Constants.SETTINGS_COMMON_JMX_ACCESSFILE);
  if (options == null || options.length < 0) {
    accessFilePath=Constants.JMX_ACCESS_FILE_PATH_DEFAULT;
  }
 else {
    accessFilePath=options[0];
    if (options.length > 1) {
      log.debug(Constants.MSG_WARN_TOO_MANY_JMXREMOTE_FILE_PATHS);
    }
  }
  options=settings.getLeafValues(Constants.SETTINGS_COMMON_JMX_PASSWORDFILE);
  if (options == null || options.length < 0) {
    passwordFilePath=Constants.JMX_PASSWORD_FILE_PATH_DEFAULT;
  }
 else {
    passwordFilePath=options[0];
    if (options.length > 1) {
      log.debug(Constants.MSG_WARN_TOO_MANY_JMXREMOTE_FILE_PATHS);
    }
  }
  accessFilePath=ScriptConstants.replaceWindowsDirSeparators(accessFilePath);
  passwordFilePath=ScriptConstants.replaceWindowsDirSeparators(passwordFilePath);
  StringBuilder res=new StringBuilder();
  res.append(ScriptConstants.ECHO_MAKE_PASSWORD_FILES);
  res.append(Constants.NEWLINE);
  if (!accessFilePath.equals(ScriptConstants.replaceWindowsDirSeparators(Constants.JMX_ACCESS_FILE_PATH_DEFAULT))) {
    res.append(ScriptConstants.SSH + Constants.SPACE);
    res.append(machineUserLogin());
    res.append(Constants.SPACE + Constants.QUOTE_MARK);
    res.append(ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE);
    res.append(ScriptConstants.WINDOWS_FORCE_MOVE);
    res.append(Constants.SPACE);
    res.append(ScriptConstants.doubleBackslashes(getLocalInstallDirPath()));
    res.append(ScriptConstants.replaceWindowsDirSeparators(Constants.JMX_ACCESS_FILE_PATH_DEFAULT));
    res.append(Constants.SPACE);
    res.append(ScriptConstants.doubleBackslashes(getLocalInstallDirPath()));
    res.append(accessFilePath);
    res.append(Constants.QUOTE_MARK);
    res.append(Constants.NEWLINE);
  }
  if (!passwordFilePath.equals(ScriptConstants.replaceWindowsDirSeparators(Constants.JMX_PASSWORD_FILE_PATH_DEFAULT))) {
    res.append(ScriptConstants.SSH + Constants.SPACE);
    res.append(machineUserLogin());
    res.append(Constants.SPACE + Constants.QUOTE_MARK);
    res.append(ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE);
    res.append(ScriptConstants.WINDOWS_FORCE_MOVE);
    res.append(Constants.SPACE);
    res.append(ScriptConstants.doubleBackslashes(getLocalInstallDirPath()));
    res.append(ScriptConstants.replaceWindowsDirSeparators(Constants.JMX_PASSWORD_FILE_PATH_DEFAULT));
    res.append(Constants.SPACE);
    res.append(ScriptConstants.doubleBackslashes(getLocalInstallDirPath()));
    res.append(passwordFilePath);
    res.append(Constants.QUOTE_MARK);
    res.append(Constants.NEWLINE);
  }
  res.append(ScriptConstants.ECHO_Y + Constants.SPACE + Constants.SEPARATOR+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + ScriptConstants.WINDOWS_COMMAND_RUN+ Constants.SPACE+ ScriptConstants.CACLS+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalInstallDirPath()));
  res.append(passwordFilePath);
  res.append(Constants.SPACE + ScriptConstants.SLASH_P + Constants.SPACE+ ScriptConstants.BITARKIV_BACKSLASH_BACKSLASH);
  res.append(machineParameters.getMachineUserName().getText());
  res.append(ScriptConstants.COLON_R + Constants.QUOTE_MARK);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_Y + Constants.SPACE + Constants.SEPARATOR+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + ScriptConstants.WINDOWS_COMMAND_RUN+ Constants.SPACE+ ScriptConstants.CACLS+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalInstallDirPath()));
  res.append(accessFilePath);
  res.append(Constants.SPACE + ScriptConstants.SLASH_P + Constants.SPACE+ ScriptConstants.BITARKIV_BACKSLASH_BACKSLASH);
  res.append(machineParameters.getMachineUserName().getText());
  res.append(ScriptConstants.COLON_R + Constants.QUOTE_MARK);
  res.append(Constants.NEWLINE);
  return res.toString();
}","/** 
 * This method does the following: Retrieves the path to the jmxremote.access and jmxremote.password files. Moves these files, if they are different from standard. This has to be a force move (command 'move /Y'). Makes the jmxremote.access and jmxremote.password files readonly.
 * @return The commands for handling the jmxremote files.
 */
@Override protected String getJMXremoteFilesCommand(){
  String accessFilePath;
  String passwordFilePath;
  String[] options;
  options=settings.getLeafValues(Constants.SETTINGS_COMMON_JMX_ACCESSFILE);
  if (options.length == 0) {
    accessFilePath=Constants.JMX_ACCESS_FILE_PATH_DEFAULT;
  }
 else {
    accessFilePath=options[0];
    if (options.length > 1) {
      log.debug(Constants.MSG_WARN_TOO_MANY_JMXREMOTE_FILE_PATHS);
    }
  }
  options=settings.getLeafValues(Constants.SETTINGS_COMMON_JMX_PASSWORDFILE);
  if (options.length == 0) {
    passwordFilePath=Constants.JMX_PASSWORD_FILE_PATH_DEFAULT;
  }
 else {
    passwordFilePath=options[0];
    if (options.length > 1) {
      log.debug(Constants.MSG_WARN_TOO_MANY_JMXREMOTE_FILE_PATHS);
    }
  }
  accessFilePath=ScriptConstants.replaceWindowsDirSeparators(accessFilePath);
  passwordFilePath=ScriptConstants.replaceWindowsDirSeparators(passwordFilePath);
  StringBuilder res=new StringBuilder();
  res.append(ScriptConstants.ECHO_MAKE_PASSWORD_FILES);
  res.append(Constants.NEWLINE);
  if (!accessFilePath.equals(ScriptConstants.replaceWindowsDirSeparators(Constants.JMX_ACCESS_FILE_PATH_DEFAULT))) {
    res.append(ScriptConstants.SSH + Constants.SPACE);
    res.append(machineUserLogin());
    res.append(Constants.SPACE + Constants.QUOTE_MARK);
    res.append(ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE);
    res.append(ScriptConstants.WINDOWS_FORCE_MOVE);
    res.append(Constants.SPACE);
    res.append(ScriptConstants.doubleBackslashes(getLocalInstallDirPath()));
    res.append(ScriptConstants.replaceWindowsDirSeparators(Constants.JMX_ACCESS_FILE_PATH_DEFAULT));
    res.append(Constants.SPACE);
    res.append(ScriptConstants.doubleBackslashes(getLocalInstallDirPath()));
    res.append(accessFilePath);
    res.append(Constants.QUOTE_MARK);
    res.append(Constants.NEWLINE);
  }
  if (!passwordFilePath.equals(ScriptConstants.replaceWindowsDirSeparators(Constants.JMX_PASSWORD_FILE_PATH_DEFAULT))) {
    res.append(ScriptConstants.SSH + Constants.SPACE);
    res.append(machineUserLogin());
    res.append(Constants.SPACE + Constants.QUOTE_MARK);
    res.append(ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE);
    res.append(ScriptConstants.WINDOWS_FORCE_MOVE);
    res.append(Constants.SPACE);
    res.append(ScriptConstants.doubleBackslashes(getLocalInstallDirPath()));
    res.append(ScriptConstants.replaceWindowsDirSeparators(Constants.JMX_PASSWORD_FILE_PATH_DEFAULT));
    res.append(Constants.SPACE);
    res.append(ScriptConstants.doubleBackslashes(getLocalInstallDirPath()));
    res.append(passwordFilePath);
    res.append(Constants.QUOTE_MARK);
    res.append(Constants.NEWLINE);
  }
  res.append(ScriptConstants.ECHO_Y + Constants.SPACE + Constants.SEPARATOR+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + ScriptConstants.WINDOWS_COMMAND_RUN+ Constants.SPACE+ ScriptConstants.CACLS+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalInstallDirPath()));
  res.append(passwordFilePath);
  res.append(Constants.SPACE + ScriptConstants.SLASH_P + Constants.SPACE+ ScriptConstants.BITARKIV_BACKSLASH_BACKSLASH);
  res.append(machineParameters.getMachineUserName().getText());
  res.append(ScriptConstants.COLON_R + Constants.QUOTE_MARK);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_Y + Constants.SPACE + Constants.SEPARATOR+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + Constants.QUOTE_MARK + ScriptConstants.WINDOWS_COMMAND_RUN+ Constants.SPACE+ ScriptConstants.CACLS+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalInstallDirPath()));
  res.append(accessFilePath);
  res.append(Constants.SPACE + ScriptConstants.SLASH_P + Constants.SPACE+ ScriptConstants.BITARKIV_BACKSLASH_BACKSLASH);
  res.append(machineParameters.getMachineUserName().getText());
  res.append(ScriptConstants.COLON_R + Constants.QUOTE_MARK);
  res.append(Constants.NEWLINE);
  return res.toString();
}",0.9928714635776342
89828,"/** 
 * Function to create the script which installs the new directories. This is only used for windows machines!
 * @param directory The directory to put the file
 */
@Override protected void createInstallDirScript(File directory){
  File dirScript=new File(directory,getMakeDirectoryName());
  try {
    PrintWriter dirPrint=new PrintWriter(dirScript);
    try {
      dirPrint.print(ScriptConstants.CD + Constants.SPACE);
      dirPrint.print(getInstallDirPath());
      dirPrint.print(Constants.NEWLINE);
      String dir;
      dir=settings.getLeafValue(Constants.SETTINGS_ARCHIVE_BP_BASEDIR_LEAF);
      if (dir != null && !dir.isEmpty() && !dir.equalsIgnoreCase(Constants.DOT)) {
        dirPrint.print(createPathToDir(dir));
        dirPrint.print(scriptCreateDir(dir,false));
      }
      dir=settings.getLeafValue(Constants.SETTINGS_ARCHIVE_ARC_BASEDIR_LEAF);
      if (dir != null && !dir.isEmpty() && !dir.equalsIgnoreCase(Constants.DOT)) {
        dirPrint.print(createPathToDir(dir));
        dirPrint.print(scriptCreateDir(dir,false));
      }
      dirPrint.print(getAppDirectories());
      dir=settings.getLeafValue(Constants.SETTINGS_TEMPDIR_LEAF);
      if (dir != null && !dir.isEmpty() && !dir.equalsIgnoreCase(Constants.DOT)) {
        dirPrint.print(createPathToDir(dir));
        dirPrint.print(scriptCreateDir(dir,resetTempDir));
      }
    }
  finally {
      dirPrint.close();
    }
  }
 catch (  IOException e) {
    String msg=""String_Node_Str"" + e;
    log.trace(msg);
    throw new IOFailure(msg);
  }
}","/** 
 * Function to create the script which installs the new directories. This is only used for windows machines!
 * @param directory The directory to put the file
 */
@Override protected void createInstallDirScript(File directory){
  File dirScript=new File(directory,getMakeDirectoryName());
  try {
    PrintWriter dirPrint=new PrintWriter(dirScript);
    try {
      dirPrint.print(ScriptConstants.CD + Constants.SPACE);
      dirPrint.print(getInstallDirPath());
      dirPrint.print(Constants.NEWLINE);
      String dir;
      dir=settings.getLeafValue(Constants.SETTINGS_ARCHIVE_BP_BASEDIR_LEAF);
      if (dir != null && !dir.isEmpty() && !dir.equalsIgnoreCase(Constants.DOT)) {
        dirPrint.print(createPathToDir(dir));
        dirPrint.print(scriptCreateDir(dir,false));
      }
      dir=settings.getLeafValue(Constants.SETTINGS_ARCHIVE_ARC_BASEDIR_LEAF);
      if (dir != null && !dir.isEmpty() && !dir.equalsIgnoreCase(Constants.DOT)) {
        dirPrint.print(createPathToDir(dir));
        dirPrint.print(scriptCreateDir(dir,false));
      }
      dirPrint.print(getAppDirectories());
      dir=settings.getLeafValue(Constants.SETTINGS_TEMPDIR_LEAF);
      if (dir != null && !dir.isEmpty() && !dir.equalsIgnoreCase(Constants.DOT)) {
        dirPrint.print(createPathToDir(dir));
        dirPrint.print(scriptCreateDir(dir,resetTempDir));
      }
    }
  finally {
      dirPrint.close();
    }
  }
 catch (  IOException e) {
    String msg=""String_Node_Str"";
    log.trace(msg,e);
    throw new IOFailure(msg,e);
  }
}",0.9973975276512688
89829,"/** 
 * Retrieves the content of a the leafs deep in the tree structure. It only retrieves branches at the first path.
 * @param path Specifies the path in the tree (e.g. in HTML: GetSubChildValue(""HTML"", ""HEAD"", ""TITLE"") to get the title of  a HTML document)
 * @return The content of the leaf. If it is not a leaf, return null.Returns 'null' if the path to the branch cannot be found.   
 */
public String[] getLeafValues(String... path){
  ArgumentNotValid.checkNotNull(path,""String_Node_Str"");
  List<Element> elemList=getAllChildrenAlongPath(root,path);
  if (elemList.isEmpty()) {
    return new String[0];
  }
  String[] res=new String[elemList.size()];
  for (int i=0; i < elemList.size(); i++) {
    res[i]=elemList.get(i).getText();
  }
  return res;
}","/** 
 * Retrieves the content of a the leafs deep in the tree structure. It only retrieves branches at the first path.
 * @param path Specifies the path in the tree (e.g. in HTML: GetSubChildValue(""HTML"", ""HEAD"", ""TITLE"") to get the title of  a HTML document)
 * @return The content of the leaf. If no leafs are found then an emptycollection of strings are returned (new String[0]).   
 */
public String[] getLeafValues(String... path){
  ArgumentNotValid.checkNotNull(path,""String_Node_Str"");
  List<Element> elemList=getAllChildrenAlongPath(root,path);
  if (elemList.isEmpty()) {
    return new String[0];
  }
  String[] res=new String[elemList.size()];
  for (int i=0; i < elemList.size(); i++) {
    res[i]=elemList.get(i).getText();
  }
  return res;
}",0.8986842105263158
89830,"/** 
 * Increment the number of upload retries.
 * @param replicaChannelName The name of the identification channelfor the replica.
 * @param arcfileName The name of a given ARC file.
 */
private void incRetry(String replicaChannelName,String arcfileName){
  Map<String,Integer> replicaRetries=uploadRetries.get(replicaChannelName);
  if (replicaRetries == null) {
    replicaRetries=new HashMap<String,Integer>();
    uploadRetries.put(replicaChannelName,replicaRetries);
  }
  Integer retryCount=replicaRetries.get(arcfileName);
  if (retryCount == null) {
    replicaRetries.put(arcfileName,new Integer(1));
    return;
  }
  replicaRetries.put(arcfileName,new Integer(retryCount + 1));
}","/** 
 * Increment the number of upload retries.
 * @param replicaChannelName The name of the identification channelfor the replica.
 * @param arcfileName The name of a given ARC file.
 */
private void incRetry(String replicaChannelName,String arcfileName){
  Map<String,Integer> replicaRetries=uploadRetries.get(replicaChannelName);
  if (replicaRetries == null) {
    replicaRetries=new HashMap<String,Integer>();
    uploadRetries.put(replicaChannelName,replicaRetries);
  }
  Integer retryCount=replicaRetries.get(arcfileName);
  if (retryCount == null) {
    replicaRetries.put(arcfileName,Integer.valueOf(1));
    return;
  }
  replicaRetries.put(arcfileName,Integer.valueOf(retryCount + 1));
}",0.9827338129496402
89831,"/** 
 * Reads output from a checksum file.  Only the first instance of the desired file will be used.  If other filenames are encountered than the wanted one, they will be logged at level warning, as that is indicative of serious errors.  Having more than one instance of the desired file merely means it was found in several bitarchives, which is not our problem.
 * @param outputFile The file to read checksum from.
 * @param arcfileName The arcfile to find checksum for.
 * @return The checksum, or the empty stringif no checksum found for arcfilename.
 * @throws IOFailure If any error occurs reading the file.
 * @throws IllegalState if the read format is wrong
 */
private String readChecksum(File outputFile,String arcfileName) throws IOFailure, IllegalState {
  List<String> lines=FileUtils.readListFromFile(outputFile);
  List<String> checksumList=new ArrayList<String>();
  for (  String line : lines) {
    String readFileName=""String_Node_Str"";
    String checksum=""String_Node_Str"";
    String[] tokens=line.split(dk.netarkivet.archive.arcrepository.bitpreservation.Constants.STRING_FILENAME_SEPARATOR);
    boolean ignoreLine=false;
    ignoreLine=(tokens.length == 0 || line.isEmpty());
    if (tokens.length != 2 && !ignoreLine) {
      throw new IllegalState(""String_Node_Str"" + ""String_Node_Str"" + line + ""String_Node_Str"");
    }
    if (!ignoreLine) {
      readFileName=tokens[0];
      checksum=tokens[1];
      if (checksum.length() == 0) {
        ignoreLine=true;
        log.warn(""String_Node_Str"" + ""String_Node_Str"" + arcfileName + ""String_Node_Str""+ line+ ""String_Node_Str"");
      }
 else {
        if (!readFileName.equals(arcfileName)) {
          ignoreLine=true;
          log.warn(""String_Node_Str"" + ""String_Node_Str"" + arcfileName + ""String_Node_Str""+ ""String_Node_Str""+ line+ ""String_Node_Str"");
        }
      }
    }
    if (checksumList.size() > 0 && !ignoreLine) {
      if (!checksum.equals(checksumList.get(checksumList.size() - 1))) {
        String errMsg=""String_Node_Str"" + arcfileName + ""String_Node_Str""+ checksumList.get(0)+ ""String_Node_Str""+ checksum+ ""String_Node_Str""+ line+ ""String_Node_Str"";
        log.warn(errMsg);
        throw new IllegalState(errMsg);
      }
    }
    if (!ignoreLine) {
      checksumList.add(checksum);
    }
  }
  if (checksumList.size() > 1) {
    log.warn(""String_Node_Str"" + arcfileName + ""String_Node_Str""+ checksumList.size()+ ""String_Node_Str""+ checksumList.get(0));
  }
  if (checksumList.size() == 0) {
    log.debug(""String_Node_Str"" + arcfileName + ""String_Node_Str""+ outputFile+ ""String_Node_Str""+ FileUtils.readListFromFile(outputFile));
    return ""String_Node_Str"";
  }
 else {
    return checksumList.get(0);
  }
}","/** 
 * Reads output from a checksum file.  Only the first instance of the desired file will be used.  If other filenames are encountered than the wanted one, they will be logged at level warning, as that is indicative of serious errors.  Having more than one instance of the desired file merely means it was found in several bitarchives, which is not our problem.
 * @param outputFile The file to read checksum from.
 * @param arcfileName The arcfile to find checksum for.
 * @return The checksum, or the empty stringif no checksum found for arcfilename.
 * @throws IOFailure If any error occurs reading the file.
 * @throws IllegalState if the read format is wrong
 */
private String readChecksum(File outputFile,String arcfileName) throws IOFailure, IllegalState {
  List<String> lines=FileUtils.readListFromFile(outputFile);
  List<String> checksumList=new ArrayList<String>();
  for (  String line : lines) {
    String readFileName=""String_Node_Str"";
    String checksum=""String_Node_Str"";
    String[] tokens=line.split(dk.netarkivet.archive.arcrepository.bitpreservation.Constants.STRING_FILENAME_SEPARATOR);
    boolean ignoreLine=false;
    ignoreLine=(tokens.length == 0 || line.isEmpty());
    if (tokens.length != 2 && !ignoreLine) {
      throw new IllegalState(""String_Node_Str"" + ""String_Node_Str"" + line + ""String_Node_Str"");
    }
    if (!ignoreLine) {
      readFileName=tokens[0];
      checksum=tokens[1];
      if (checksum.length() == 0) {
        ignoreLine=true;
        log.warn(""String_Node_Str"" + ""String_Node_Str"" + arcfileName + ""String_Node_Str""+ line+ ""String_Node_Str"");
      }
 else {
        if (!readFileName.equals(arcfileName)) {
          ignoreLine=true;
          log.warn(""String_Node_Str"" + ""String_Node_Str"" + arcfileName + ""String_Node_Str""+ ""String_Node_Str""+ line+ ""String_Node_Str"");
        }
      }
    }
    if (checksumList.size() > 0 && !ignoreLine && !checksum.equals(checksumList.get(checksumList.size() - 1))) {
      String errMsg=""String_Node_Str"" + arcfileName + ""String_Node_Str""+ checksumList.get(0)+ ""String_Node_Str""+ checksum+ ""String_Node_Str""+ line+ ""String_Node_Str"";
      log.warn(errMsg);
      throw new IllegalState(errMsg);
    }
    if (!ignoreLine) {
      checksumList.add(checksum);
    }
  }
  if (checksumList.size() > 1) {
    log.warn(""String_Node_Str"" + arcfileName + ""String_Node_Str""+ checksumList.size()+ ""String_Node_Str""+ checksumList.get(0));
  }
  if (checksumList.size() == 0) {
    log.debug(""String_Node_Str"" + arcfileName + ""String_Node_Str""+ outputFile+ ""String_Node_Str""+ FileUtils.readListFromFile(outputFile));
    return ""String_Node_Str"";
  }
 else {
    return checksumList.get(0);
  }
}",0.9052553663952628
89832,"/** 
 * Return files with filelist_status CORRUPT for the replica, but not  present in the last missing files job. This is done by querying the database for files with different checksum from the checksum in the last known update date for bitarchive, but which are present from admin data.
 * @param replica The replica to check for.
 */
Iterable<String> getWrongFilesInLastUpdate(Replica replica);","/** 
 * Return files with filelist_status CORRUPT for the replica, but not  present in the last missing files job. This is done by querying the database for files with different checksum  from the checksum in the last known update date for bitarchive, but  which are present from admin data.
 * @param replica The replica to check for.
 * @return The list of wrong files for the replica in the last update.
 */
Iterable<String> getWrongFilesInLastUpdate(Replica replica);",0.9159953970080552
89833,"/** 
 * Return the count of corrupt files for replica.
 * @param replica The replica to get the count for.
 */
long getNumberOfWrongFilesInLastUpdate(Replica replica);","/** 
 * Return the count of corrupt files for replica.
 * @param replica The replica to get the count for.
 * @return The number of wrong files for a replica.
 */
long getNumberOfWrongFilesInLastUpdate(Replica replica);",0.8082901554404145
89834,"/** 
 * Get the date for the last file list job.
 * @param replica The replica to get the date for.
 */
Date getDateOfLastWrongFilesUpdate(Replica replica);","/** 
 * Get the date for the last file list job.
 * @param replica The replica to get the date for.
 * @return The date of the last wrong file update for the replica.
 */
Date getDateOfLastWrongFilesUpdate(Replica replica);",0.8232189973614775
89835,"/** 
 * Return the count of missing files for replica.
 * @param replica The replica to get the count for.
 */
long getNumberOfMissingFilesInLastUpdate(Replica replica);","/** 
 * Return the count of missing files for replica.
 * @param replica The replica to get the count for.
 * @return The count of missing files for a replica.
 */
long getNumberOfMissingFilesInLastUpdate(Replica replica);",0.8644501278772379
89836,"/** 
 * Return files with upload_status = COMPLETE for the replica, but the filelist_status = MISSING. This is done by querying the database for files with no or different update date from the last known update date for bitarchive, but which are present from admin data.
 * @param replica The replica to check for.
 */
Iterable<String> getMissingFilesInLastUpdate(Replica replica);","/** 
 * Return files with upload_status = COMPLETE for the replica, but the filelist_status = MISSING. This is done by querying the database for files with no or different  update date from the last known update date for bitarchive, but which  are present from admin data.
 * @param replica The replica to check for.
 * @return The list of missing files for a specific replica.
 */
Iterable<String> getMissingFilesInLastUpdate(Replica replica);",0.9236363636363636
89837,"/** 
 * Get the date for the last file list job.
 * @param replica The replica to get the date for.
 */
Date getDateOfLastMissingFilesUpdate(Replica replica);","/** 
 * Get the date for the last file list job.
 * @param replica The replica to get the date for.
 * @return The date of the last missing files update for the replica.
 */
Date getDateOfLastMissingFilesUpdate(Replica replica);",0.8186528497409327
89838,"/** 
 * Method for testing whether a ChecksumEntry is identical to another ChecksumEntry. 
 * @param o The object to evaluate whether it is identical to this ChecksumEntry.
 * @return Whether the argument has the same values as this ChecksumEntry.It returns false if the argument is not of type ChecksumEntry, or if it has either different filename or different checksum.
 */
public boolean equals(Object o){
  ChecksumEntry ce;
  try {
    ce=(ChecksumEntry)o;
  }
 catch (  Throwable e) {
    return false;
  }
  if (!ce.getFilename().equals(filename)) {
    return false;
  }
  if (!ce.getChecksum().equals(checksum)) {
    return false;
  }
  return true;
}","/** 
 * Method for testing whether a ChecksumEntry is identical to another ChecksumEntry. 
 * @param o The object to evaluate whether it is identical to this ChecksumEntry.
 * @return Whether the argument has the same values as this ChecksumEntry.It returns false if the argument is not of type ChecksumEntry, or if it has either different filename or different checksum.
 */
public boolean equals(Object obj){
  if (this == obj)   return true;
  if (obj == null)   return false;
  if (getClass() != obj.getClass())   return false;
  ChecksumEntry other=(ChecksumEntry)obj;
  if (checksum == null) {
    if (other.checksum != null)     return false;
  }
 else   if (!checksum.equals(other.checksum))   return false;
  if (filename == null) {
    if (other.filename != null)     return false;
  }
 else   if (!filename.equals(other.filename))   return false;
  return true;
}",0.6566775244299674
89839,"/** 
 * Invoke default method for serializing object.
 * @param s the OutputStream
 */
private void writeObject(ObjectOutputStream s){
  try {
    s.defaultWriteObject();
  }
 catch (  Exception e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
}","/** 
 * Invoke default method for serializing object.
 * @param s the OutputStream
 * @throws IOFailure If an exception is caught during writing of the object.
 */
private void writeObject(ObjectOutputStream s) throws IOFailure {
  try {
    s.defaultWriteObject();
  }
 catch (  Exception e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
}",0.8408710217755444
89840,"public ChecksumJob(){
  batchJobTimeout=15 * Constants.ONE_MIN_IN_MILLIES;
}","/** 
 * The constructor.
 */
public ChecksumJob(){
  batchJobTimeout=CHECKSUM_JOB_TIME;
}",0.5575757575757576
89841,"/** 
 * Finishing the job requires nothing particular.
 * @see FileBatchJob#finish(OutputStream)
 */
public void finish(OutputStream os){
}","/** 
 * Finishing the job requires nothing particular.
 * @param os The output stream where the output data is written.
 * @see FileBatchJob#finish(OutputStream)
 */
public void finish(OutputStream os){
}",0.8104956268221575
89842,"/** 
 * Initialization of a ChecksumJob: a new structure for storing files failed is created.
 * @see FileBatchJob#initialize(OutputStream)
 */
public void initialize(OutputStream os){
}","/** 
 * Initialization of a ChecksumJob: a new structure for storing files failed is created.
 * @param os The output stream where the output data is written.
 * @see FileBatchJob#initialize(OutputStream)
 */
public void initialize(OutputStream os){
}",0.851258581235698
89843,"/** 
 * Parse a line of output into a key-value pair.
 * @param line The line to parse, of the form<filename>##<checksum>
 * @return The filename->checksum mapping.
 * @throws ArgumentNotValid if the line is not on the correct form.
 */
public static KeyValuePair<String,String> parseLine(String line){
  ArgumentNotValid.checkNotNull(line,""String_Node_Str"");
  String[] parts=line.split(Constants.STRING_FILENAME_SEPARATOR);
  if (parts.length != 2) {
    throw new ArgumentNotValid(""String_Node_Str"" + line + ""String_Node_Str""+ ""String_Node_Str"");
  }
  return new KeyValuePair<String,String>(parts[0],parts[1]);
}","/** 
 * Parse a line of output into a key-value pair.
 * @param line The line to parse, of the form<b>filename</b>##<b>checksum</b>
 * @return The filename->checksum mapping.
 * @throws ArgumentNotValid if the line is not on the correct form.
 */
public static KeyValuePair<String,String> parseLine(String line) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(line,""String_Node_Str"");
  String[] parts=line.split(Constants.STRING_FILENAME_SEPARATOR);
  if (parts.length != 2) {
    throw new ArgumentNotValid(""String_Node_Str"" + line + ""String_Node_Str""+ ""String_Node_Str"");
  }
  return new KeyValuePair<String,String>(parts[0],parts[1]);
}",0.9597474348855564
89844,"/** 
 * This should replace the current 'GetAndRemoveFile' followed by 'Upload'.
 * @param msg The message.
 * @throws NotImplementedException Always, since this method has not yet been implemented.
 */
public void correct(RemoteFile arcfile,String checksum){
  throw new NotImplementedException(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"");
}","/** 
 * This should replace the current 'GetAndRemoveFile' followed by 'Upload'.
 * @param arcfile The arcfile to replace the current bad entry
 * @param checksum The checksum of the bad entry to verify that the entry is still bad
 * @throws NotImplementedException Always, since this method has not yet been implemented.
 */
public void correct(RemoteFile arcfile,String checksum){
  throw new NotImplementedException(""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"");
}",0.8267622461170848
89845,"/** 
 * Method for removing a bad entry from the archive. This finds the record and removes it if it has the incorrect checksum. The incorrect record is not deleted, but instead put into a backup file for all the incorrect records.
 * @param filename The name of the file whose record should be removed.
 * @param correctFile The correct remote file to replace the bad one in the archive.
 * @param incorrectChecksum The checksum of the bad entry.
 * @throws ArgumentNotValid If one of the arguments are not valid.
 * @throws IOFailure If the entry cannot be corrected.
 * @throws IllegalState If no such entry exists to be corrected, or if the entry has a different checksum than the incorrectChecksum.
 */
public abstract void correct(String filename,File correctFile) throws IOFailure, ArgumentNotValid, IllegalState ;","/** 
 * Method for removing a bad entry from the archive. This finds the record and removes it if it has the incorrect checksum. The incorrect record is not deleted, but instead put into a backup file for all the incorrect records.
 * @param filename The name of the file whose record should be removed.
 * @param correctFile The correct remote file to replace the bad one in the archive.
 * @throws ArgumentNotValid If one of the arguments are not valid.
 * @throws IOFailure If the entry cannot be corrected.
 * @throws IllegalState If no such entry exists to be corrected, or if the entry has a different checksum than the incorrectChecksum.
 */
public abstract void correct(String filename,File correctFile) throws IOFailure, ArgumentNotValid, IllegalState ;",0.9627289955780164
89846,"/** 
 * Method for correcting a bad entry from the archive. The current incorrect entry is put into the wrongEntryFile.  Then it calculates the checksum and corrects the entry for the file, and  then the checksum file is recreated from the archive in the memory.
 * @param filename The name of the file whose record should be removed.
 * @param rf The correct remote file to replace the bad one in the archive.
 * @param incorrectChecksum The checksum of the bad entry.
 * @throws ArgumentNotValid If one of the arguments are not valid.
 * @throws IOFailure If the entry cannot be corrected. Either the bad entrycannot be stored, or the new checksum file cannot be created.
 * @throws IllegalState If no such entry exists to be corrected, or if the entry has a different checksum than the incorrectChecksum.
 */
@Override public void correct(String filename,File correctFile) throws IOFailure, ArgumentNotValid, IllegalState {
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(correctFile,""String_Node_Str"");
  if (!checksumArchive.containsKey(filename)) {
    String errMsg=""String_Node_Str"" + filename + ""String_Node_Str"";
    log.error(errMsg);
    throw new IllegalState(errMsg);
  }
  String currentChecksum=checksumArchive.get(filename);
  appendWrongRecordToWrongEntryFile(ChecksumJob.makeLine(filename,currentChecksum));
  String newChecksum=calculateChecksum(correctFile);
  if (newChecksum.equals(currentChecksum)) {
    log.warn(""String_Node_Str"");
    return;
  }
  checksumArchive.put(filename,newChecksum);
  recreateArchiveFile();
}","/** 
 * Method for correcting a bad entry from the archive. The current incorrect entry is put into the wrongEntryFile.  Then it calculates the checksum and corrects the entry for the file, and  then the checksum file is recreated from the archive in the memory.
 * @param filename The name of the file whose record should be removed.
 * @param correctFile The file that should replace the current entry
 * @throws ArgumentNotValid If one of the arguments are not valid.
 * @throws IOFailure If the entry cannot be corrected. Either the bad entrycannot be stored, or the new checksum file cannot be created.
 * @throws IllegalState If no such entry exists to be corrected, or if the entry has a different checksum than the incorrectChecksum.
 */
@Override public void correct(String filename,File correctFile) throws IOFailure, ArgumentNotValid, IllegalState {
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(correctFile,""String_Node_Str"");
  if (!checksumArchive.containsKey(filename)) {
    String errMsg=""String_Node_Str"" + filename + ""String_Node_Str"";
    log.error(errMsg);
    throw new IllegalState(errMsg);
  }
  String currentChecksum=checksumArchive.get(filename);
  appendWrongRecordToWrongEntryFile(ChecksumJob.makeLine(filename,currentChecksum));
  String newChecksum=calculateChecksum(correctFile);
  if (newChecksum.equals(currentChecksum)) {
    log.warn(""String_Node_Str"");
    return;
  }
  checksumArchive.put(filename,newChecksum);
  recreateArchiveFile();
}",0.9571611253196932
89847,"/** 
 * Method for copying the NetarchiveSuite file to output directory. This handles the following three scenarios:  <ol> <li> outputDir == directory of zip file </li>  Do nothing.  <br> <li> outputDir != directory of zip file, but another zip file with same  name exists in the directory </li>  Remove the existing file before copy of the given file. <br> <li> outputDir != directory of zip file and no other zip file </li> Copy file to output directory.
 */
private void copyNetarchiveSuiteFile(){
  try {
    File newNetarchiveSuiteFile=new File(outputDir,netarchiveSuiteFile.getName());
    if (newNetarchiveSuiteFile.getCanonicalPath().equals(netarchiveSuiteFile.getCanonicalPath())) {
      return;
    }
    if (newNetarchiveSuiteFile.exists()) {
      System.out.println(Constants.MSG_WARN_ZIPFILE_ALREADY_EXISTS + newNetarchiveSuiteFile.getCanonicalPath());
      newNetarchiveSuiteFile.delete();
    }
    FileUtils.copyFile(netarchiveSuiteFile,newNetarchiveSuiteFile);
  }
 catch (  IOException e) {
    System.out.println(Constants.MSG_ERROR_ZIP_CANNONICAL_PATH + netarchiveSuiteFile.getAbsolutePath());
    e.printStackTrace();
    System.exit(1);
  }
}","/** 
 * Method for copying the NetarchiveSuite file to output directory. This handles the following three scenarios:  <ol> <li> outputDir == directory of zip file </li>  Do nothing.  <br> <li> outputDir != directory of zip file, but another zip file with same  name exists in the directory </li>  Remove the existing file before copy of the given file. <br> <li> outputDir != directory of zip file and no other zip file </li> Copy file to output directory. </ol>
 */
private void copyNetarchiveSuiteFile(){
  try {
    File newNetarchiveSuiteFile=new File(outputDir,netarchiveSuiteFile.getName());
    if (newNetarchiveSuiteFile.getCanonicalPath().equals(netarchiveSuiteFile.getCanonicalPath())) {
      return;
    }
    if (newNetarchiveSuiteFile.exists()) {
      System.out.println(Constants.MSG_WARN_ZIPFILE_ALREADY_EXISTS + newNetarchiveSuiteFile.getCanonicalPath());
      newNetarchiveSuiteFile.delete();
    }
    FileUtils.copyFile(netarchiveSuiteFile,newNetarchiveSuiteFile);
  }
 catch (  IOException e) {
    System.out.println(Constants.MSG_ERROR_ZIP_CANNONICAL_PATH + netarchiveSuiteFile.getAbsolutePath());
    e.printStackTrace();
    System.exit(1);
  }
}",0.9974358974358974
89848,"/** 
 * The constructor.  Starts by initialising the parent abstract class, then sets the  operating system dependent variables.
 * @param subTreeRoot The XML root element.
 * @param parentSettings The Settings to be inherited from the PhysicalLocation, where this machine is placed.
 * @param param The machine parameters to be inherited from the PhysicalLocation.
 * @param netarchiveSuiteSource The name of the NetarchiveSuite package file. Must end with '.zip'.
 * @param logProp The logging property file, to be copied into machine directory.
 * @param securityPolicy The security policy file, to be copied intomachine directory.
 * @param dbFile The name of the database file.
 * @param resetDir Whether the temporary directory should be reset.
 */
public LinuxMachine(Element subTreeRoot,XmlStructure parentSettings,Parameters param,String netarchiveSuiteSource,File logProp,File securityPolicy,File dbFile,boolean resetDir){
  super(subTreeRoot,parentSettings,param,netarchiveSuiteSource,logProp,securityPolicy,dbFile,resetDir);
  OS=Constants.OPERATING_SYSTEM_LINUX_ATTRIBUTE;
  scriptExtension=Constants.SCRIPT_EXTENSION_LINUX;
}","/** 
 * The constructor.  Starts by initialising the parent abstract class, then sets the  operating system dependent variables.
 * @param subTreeRoot The XML root element.
 * @param parentSettings The Settings to be inherited from the PhysicalLocation, where this machine is placed.
 * @param param The machine parameters to be inherited from the PhysicalLocation.
 * @param netarchiveSuiteSource The name of the NetarchiveSuite package file. Must end with '.zip'.
 * @param logProp The logging property file, to be copied into machine directory.
 * @param securityPolicy The security policy file, to be copied intomachine directory.
 * @param dbFile The name of the database file.
 * @param resetDir Whether the temporary directory should be reset.
 */
public LinuxMachine(Element subTreeRoot,XmlStructure parentSettings,Parameters param,String netarchiveSuiteSource,File logProp,File securityPolicy,File dbFile,boolean resetDir){
  super(subTreeRoot,parentSettings,param,netarchiveSuiteSource,logProp,securityPolicy,dbFile,resetDir);
  operatingSystem=Constants.OPERATING_SYSTEM_LINUX_ATTRIBUTE;
  scriptExtension=Constants.SCRIPT_EXTENSION_LINUX;
}",0.99257965953732
89849,"/** 
 * Creates the operation system specific installation script for  this machine. Pseudo code: - echo copying 'NetarchiveSuite.zip' to: 'machine' - scp 'NetarchiveSuite.zip' 'login'@'machine': - echo unzipping 'NetarchiveSuite.zip' at: 'machine' - ssh 'login'@'machine' 'unzip' 'environmentName' -o 'NetarchiveSuite.zip - $'create directories'. - echo preparing for copying of settings and scripts - if [ $( ssh 'login'@'machine' cmd /c if exist  'environmentName'\\conf\\jmxremote.password echo 1 ) ]; then echo Y | ssh  'login'@'machine' cmd /c cacls  'environmentName'\\conf\\jmxremote.password /P BITARKIV\\'login':F; fi - if [ $( ssh 'login'@'machine' cmd /c if exist  'environmentName'\\conf\\jmxremote.access echo 1 ) ]; then echo Y | ssh  'login'@'machine' cmd /c cacls 'environmentName'\\conf\\jmxremote.access /P BITARKIV\\'login':F; fi - echo copying settings and scripts - scp -r 'machine'/* 'login'@'machine':'environmentName'\\conf\\ - $'apply database script' - echo make password files readonly * if 'jmxremote-password-path' != 'jmxremote-password-defaultpath' - ssh 'login'@'machine' move /Y 'jmxremote-password-defaultpath'  'jmxremote-password-path' * if 'jmxremote-access-path' != 'jmxremote-access-defaultpath' - ssh 'login'@'machine' move /Y 'jmxremote-access-defaultpath'  'jmxremote-access-path' - echo Y | ssh 'login'@'machine' cmd /c cacls  'environmentName'\\'jmxremote-password-path' /P BITARKIV\\'login':R - echo Y | ssh 'login'@'machine' cmd /c cacls  'environmentName'\\'jmxremote-access-path' /P BITARKIV\\'login':R variables: 'NetarchiveSuite.zip' = The NetarchiveSuitePackage with '.zip' extension. 'machine' = The machine name. 'login' = The username for the machine. 'unzip' = The command for unzipping. 'environmentName' = the environmentName from the configuration file. $'...' = call other function.
 * @return Operation system specific part of the installscript
 */
@Override protected String osInstallScript(){
  StringBuilder res=new StringBuilder();
  res.append(ScriptConstants.ECHO_COPYING + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE + ScriptConstants.TO + Constants.COLON+ Constants.SPACE);
  res.append(name);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SCP + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.COLON);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_UNZIPPING + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE + ScriptConstants.AT + Constants.COLON+ Constants.SPACE);
  res.append(name);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_UNZIP_COMMAND + Constants.SPACE);
  res.append(getEnvironmentName());
  res.append(Constants.SPACE + ScriptConstants.SCRIPT_OUTPUT + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.NEWLINE);
  res.append(osInstallScriptCreateDir());
  res.append(ScriptConstants.ECHO_PREPARING_FOR_COPY);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.IF + Constants.SPACE + Constants.SQUARE_BRACKET_BEGIN+ Constants.SPACE+ Constants.DOLLAR_SIGN+ Constants.BRACKET_BEGIN+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.IF+ Constants.SPACE+ ScriptConstants.EXIST+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_PASSWORD_FILE_NAME);
  res.append(Constants.SPACE + ScriptConstants.ECHO_ONE);
  res.append(Constants.SPACE + Constants.BRACKET_END + Constants.SPACE+ Constants.SQUARE_BRACKET_END+ Constants.SEMICOLON+ Constants.SPACE+ ScriptConstants.THEN+ Constants.SPACE);
  res.append(ScriptConstants.ECHO_Y + Constants.SPACE + Constants.SEPARATOR+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.CACLS+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_PASSWORD_FILE_NAME + Constants.SPACE + ScriptConstants.SLASH_P+ Constants.SPACE+ ScriptConstants.BITARKIV_BACKSLASH_BACKSLASH);
  res.append(machineParameters.getMachineUserName().getText());
  res.append(ScriptConstants.COLON_F + Constants.SEMICOLON + Constants.SPACE+ ScriptConstants.FI+ Constants.SEMICOLON);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.IF + Constants.SPACE + Constants.SQUARE_BRACKET_BEGIN+ Constants.SPACE+ Constants.DOLLAR_SIGN+ Constants.BRACKET_BEGIN+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.IF+ Constants.SPACE+ ScriptConstants.EXIST+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_PASSWORD_FILE_NAME);
  res.append(Constants.SPACE + ScriptConstants.ECHO_ONE);
  res.append(Constants.SPACE + Constants.BRACKET_END + Constants.SPACE+ Constants.SQUARE_BRACKET_END+ Constants.SEMICOLON+ Constants.SPACE+ ScriptConstants.THEN+ Constants.SPACE+ ScriptConstants.ECHO_Y+ Constants.SPACE);
  res.append(Constants.SEPARATOR + Constants.SPACE + ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.CACLS+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_PASSWORD_FILE_NAME + Constants.SPACE + ScriptConstants.SLASH_P+ Constants.SPACE+ ScriptConstants.BITARKIV_BACKSLASH_BACKSLASH);
  res.append(machineParameters.getMachineUserName().getText());
  res.append(ScriptConstants.COLON_F + Constants.SEMICOLON + Constants.SPACE+ ScriptConstants.FI+ Constants.SEMICOLON);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_COPY_SETTINGS_AND_SCRIPTS);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SCP + Constants.SPACE + ScriptConstants.DASH_R+ Constants.SPACE);
  res.append(name);
  res.append(Constants.SLASH + Constants.STAR + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.COLON);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.NEWLINE);
  res.append(osInstallDatabase());
  res.append(getJMXremoteFilesCommand());
  return res.toString();
}","/** 
 * Creates the operation system specific installation script for  this machine. Pseudo code: - echo copying 'NetarchiveSuite.zip' to: 'machine' - scp 'NetarchiveSuite.zip' 'login'@'machine': - echo unzipping 'NetarchiveSuite.zip' at: 'machine' - ssh 'login'@'machine' 'unzip' 'environmentName' -o 'NetarchiveSuite.zip - $'create directories'. - echo preparing for copying of settings and scripts - if [ $( ssh 'login'@'machine' cmd /c if exist  'environmentName'\\conf\\jmxremote.password echo 1 ) ]; then echo Y | ssh  'login'@'machine' cmd /c cacls  'environmentName'\\conf\\jmxremote.password /P BITARKIV\\'login':F; fi - if [ $( ssh 'login'@'machine' cmd /c if exist  'environmentName'\\conf\\jmxremote.access echo 1 ) ]; then echo Y | ssh  'login'@'machine' cmd /c cacls 'environmentName'\\conf\\jmxremote.access /P BITARKIV\\'login':F; fi - echo copying settings and scripts - scp -r 'machine'/* 'login'@'machine':'environmentName'\\conf\\ - $'apply database script' - echo make password files readonly * if 'jmxremote-password-path' != 'jmxremote-password-defaultpath' - ssh 'login'@'machine' move /Y 'jmxremote-password-defaultpath'  'jmxremote-password-path' * if 'jmxremote-access-path' != 'jmxremote-access-defaultpath' - ssh 'login'@'machine' move /Y 'jmxremote-access-defaultpath'  'jmxremote-access-path' - echo Y | ssh 'login'@'machine' cmd /c cacls  'environmentName'\\'jmxremote-password-path' /P BITARKIV\\'login':R - echo Y | ssh 'login'@'machine' cmd /c cacls  'environmentName'\\'jmxremote-access-path' /P BITARKIV\\'login':R variables: 'NetarchiveSuite.zip' = The NetarchiveSuitePackage with '.zip' extension. 'machine' = The machine name. 'login' = The username for the machine. 'unzip' = The command for unzipping. 'environmentName' = the environmentName from the configuration file. $'...' = call other function.
 * @return Operation system specific part of the installscript
 */
@Override protected String osInstallScript(){
  StringBuilder res=new StringBuilder();
  res.append(ScriptConstants.ECHO_COPYING + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE + ScriptConstants.TO + Constants.COLON+ Constants.SPACE);
  res.append(name);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SCP + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.COLON);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_UNZIPPING + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.SPACE + ScriptConstants.AT + Constants.COLON+ Constants.SPACE);
  res.append(name);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SSH + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_UNZIP_COMMAND + Constants.SPACE);
  res.append(getEnvironmentName());
  res.append(Constants.SPACE + ScriptConstants.SCRIPT_OUTPUT + Constants.SPACE);
  res.append(netarchiveSuiteFileName);
  res.append(Constants.NEWLINE);
  res.append(osInstallScriptCreateDir());
  res.append(ScriptConstants.ECHO_PREPARING_FOR_COPY);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.IF + Constants.SPACE + Constants.SQUARE_BRACKET_BEGIN+ Constants.SPACE+ Constants.DOLLAR_SIGN+ Constants.BRACKET_BEGIN+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.IF+ Constants.SPACE+ ScriptConstants.EXIST+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_PASSWORD_FILE_NAME);
  res.append(Constants.SPACE + ScriptConstants.ECHO_ONE);
  res.append(Constants.SPACE + Constants.BRACKET_END + Constants.SPACE+ Constants.SQUARE_BRACKET_END+ Constants.SEMICOLON+ Constants.SPACE+ ScriptConstants.THEN+ Constants.SPACE);
  res.append(ScriptConstants.ECHO_Y + Constants.SPACE + Constants.SEPARATOR+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.CACLS+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_PASSWORD_FILE_NAME + Constants.SPACE + ScriptConstants.SLASH_P+ Constants.SPACE+ ScriptConstants.BITARKIV_BACKSLASH_BACKSLASH);
  res.append(machineParameters.getMachineUserName().getText());
  res.append(ScriptConstants.COLON_F + Constants.SEMICOLON + Constants.SPACE+ ScriptConstants.FI+ Constants.SEMICOLON);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.IF + Constants.SPACE + Constants.SQUARE_BRACKET_BEGIN+ Constants.SPACE+ Constants.DOLLAR_SIGN+ Constants.BRACKET_BEGIN+ Constants.SPACE+ ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.IF+ Constants.SPACE+ ScriptConstants.EXIST+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_ACCESS_FILE_NAME);
  res.append(Constants.SPACE + ScriptConstants.ECHO_ONE);
  res.append(Constants.SPACE + Constants.BRACKET_END + Constants.SPACE+ Constants.SQUARE_BRACKET_END+ Constants.SEMICOLON+ Constants.SPACE+ ScriptConstants.THEN+ Constants.SPACE+ ScriptConstants.ECHO_Y+ Constants.SPACE);
  res.append(Constants.SEPARATOR + Constants.SPACE + ScriptConstants.SSH+ Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.SPACE + ScriptConstants.WINDOWS_COMMAND_RUN + Constants.SPACE+ ScriptConstants.CACLS+ Constants.SPACE);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.JMX_ACCESS_FILE_NAME + Constants.SPACE + ScriptConstants.SLASH_P+ Constants.SPACE+ ScriptConstants.BITARKIV_BACKSLASH_BACKSLASH);
  res.append(machineParameters.getMachineUserName().getText());
  res.append(ScriptConstants.COLON_F + Constants.SEMICOLON + Constants.SPACE+ ScriptConstants.FI+ Constants.SEMICOLON);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.ECHO_COPY_SETTINGS_AND_SCRIPTS);
  res.append(Constants.NEWLINE);
  res.append(ScriptConstants.SCP + Constants.SPACE + ScriptConstants.DASH_R+ Constants.SPACE);
  res.append(name);
  res.append(Constants.SLASH + Constants.STAR + Constants.SPACE);
  res.append(machineUserLogin());
  res.append(Constants.COLON);
  res.append(ScriptConstants.doubleBackslashes(getLocalConfDirPath()));
  res.append(Constants.NEWLINE);
  res.append(osInstallDatabase());
  res.append(getJMXremoteFilesCommand());
  return res.toString();
}",0.9979234648472264
89850,"/** 
 * The constructor.  Starts by initialising the parent abstract class, then sets the  operating system dependent variables.
 * @param e The XML root element.
 * @param parentSettings The Settings to be inherited from the PhysicalLocation, where this machine is placed.
 * @param param The machine parameters to be inherited from the PhysicalLocation.
 * @param netarchiveSuiteSource The name of the NetarchiveSuite package file. Must be '.zip'.
 * @param logProp The logging property file, to be copied into machine directory.
 * @param securityPolicy The security policy file, to be copied intomachine directory.
 * @param dbFile The name of the database file.
 * @param resetDir Whether the temporary directory should be reset.
 */
public WindowsMachine(Element e,XmlStructure parentSettings,Parameters param,String netarchiveSuiteSource,File logProp,File securityPolicy,File dbFile,boolean resetDir){
  super(e,parentSettings,param,netarchiveSuiteSource,logProp,securityPolicy,dbFile,resetDir);
  OS=Constants.OPERATING_SYSTEM_WINDOWS_ATTRIBUTE;
  scriptExtension=Constants.SCRIPT_EXTENSION_WINDOWS;
}","/** 
 * The constructor.  Starts by initializing the parent abstract class, then sets the  operating system dependent variables.
 * @param e The XML root element.
 * @param parentSettings The Settings to be inherited from the PhysicalLocation, where this machine is placed.
 * @param param The machine parameters to be inherited from the PhysicalLocation.
 * @param netarchiveSuiteSource The name of the NetarchiveSuite package file. Must be '.zip'.
 * @param logProp The logging property file, to be copied into machine directory.
 * @param securityPolicy The security policy file, to be copied intomachine directory.
 * @param dbFile The name of the database file.
 * @param resetDir Whether the temporary directory should be reset.
 */
public WindowsMachine(Element e,XmlStructure parentSettings,Parameters param,String netarchiveSuiteSource,File logProp,File securityPolicy,File dbFile,boolean resetDir){
  super(e,parentSettings,param,netarchiveSuiteSource,logProp,securityPolicy,dbFile,resetDir);
  operatingSystem=Constants.OPERATING_SYSTEM_WINDOWS_ATTRIBUTE;
  scriptExtension=Constants.SCRIPT_EXTENSION_WINDOWS;
}",0.9914836396234872
89851,"/** 
 * Applies the environment name on the name of the file-directories. Thus: fileDir -> fileDir/environmentName
 * @param app The application where this has to be applied.
 */
private void applyEnvironmentNameOnBaseFileDir(Element app){
  ArgumentNotValid.checkNotNull(app,""String_Node_Str"");
  List<Element> elems=XmlStructure.getAllChildrenAlongPath(app.element(Constants.COMPLETE_SETTINGS_BRANCH),Constants.SETTINGS_BITARCHIVE_BASEFILEDIR_LEAF);
  for (  Element el : elems) {
    String content=el.getText();
    if (content.contains(Constants.BACKSLASH)) {
      content+=Constants.BACKSLASH + environmentNameVal;
    }
 else {
      content+=Constants.SLASH + environmentNameVal;
    }
    el.setText(content);
  }
}","/** 
 * Applies the environment name on the name of the file-directories. Thus: fileDir -> fileDir/environmentName
 * @param app The application where this has to be applied.
 */
private void applyEnvironmentNameOnBaseFileDir(Element app){
  ArgumentNotValid.checkNotNull(app,""String_Node_Str"");
  List<Element> elems=XmlStructure.getAllChildrenAlongPath(app.element(Constants.COMPLETE_SETTINGS_BRANCH),Constants.SETTINGS_BITARCHIVE_BASEFILEDIR_LEAF);
  for (  Element el : elems) {
    StringBuilder content=new StringBuilder(el.getText());
    if (content.indexOf(Constants.BACKSLASH) > -1) {
      content.append(Constants.BACKSLASH + environmentNameVal);
    }
 else {
      content.append(Constants.SLASH + environmentNameVal);
    }
    el.setText(content.toString());
  }
}",0.8106312292358804
89852,"/** 
 * Function to apply the variables.
 * @param offset The input offset value (1-9 below httpPort).
 * @param httpPort The new value for the HTTP port.
 * @param environmentName The new value for the environment name.
 * @param mailReceiver The new value for the mailReceiver.
 */
public void applyTestArguments(String offset,String httpPort,String environmentName,String mailReceiver){
  ArgumentNotValid.checkNotNullOrEmpty(offset,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(httpPort,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(environmentName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(mailReceiver,""String_Node_Str"");
  int offsetInt=(new Integer(httpPort)).intValue() - (new Integer(offset)).intValue();
  if (offsetInt > Constants.TEST_OFFSET_INTEGER_MAXIMUM_VALUE || offsetInt < 0) {
    System.err.print(Constants.MSG_ERROR_TEST_OFFSET);
    System.out.println();
    System.exit(1);
  }
  offsetVal=new String(new Integer(offsetInt).toString());
  if (!Constants.validEnvironmentName(environmentName)) {
    System.err.print(Constants.MSG_ERROR_INVALID_ENVIRONMENT_NAME + environmentName);
    System.out.println();
    System.exit(1);
  }
  httpPortVal=httpPort;
  environmentNameVal=environmentName;
  mailReceiverVal=mailReceiver;
  httpPortPath=Constants.COMPLETE_HTTP_PORT_LEAF;
  environmentNamePath=Constants.COMPLETE_ENVIRONMENT_NAME_LEAF;
  mailReceiverPath=Constants.SETTINGS_NOTIFICATION_RECEIVER_PATH;
  offsetPaths=new OffsetSystem[]{new OffsetSystem(Constants.TEST_OFFSET_MONITOR_JMX_PORT,Constants.COMPLETE_JMX_PORT_PATH),new OffsetSystem(Constants.TEST_OFFSET_MONITOR_RMI_PORT,Constants.COMPLETE_JMX_RMIPORT_PATH),new OffsetSystem(Constants.TEST_OFFSET_HERITRIX_GUI_PORT,Constants.COMPLETE_HARVEST_HETRIX_GUI_PORT_PATH),new OffsetSystem(Constants.TEST_OFFSET_HERITRIX_JMX_PORT,Constants.COMPLETE_HARVEST_HETRIX_JMX_PORT)};
  apply();
}","/** 
 * Function to apply the variables.
 * @param offset The input offset value (1-9 below httpPort).
 * @param httpPort The new value for the HTTP port.
 * @param environmentName The new value for the environment name.
 * @param mailReceiver The new value for the mailReceiver.
 */
public void applyTestArguments(String offset,String httpPort,String environmentName,String mailReceiver){
  ArgumentNotValid.checkNotNullOrEmpty(offset,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(httpPort,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(environmentName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(mailReceiver,""String_Node_Str"");
  int offsetInt=(new Integer(httpPort)).intValue() - (new Integer(offset)).intValue();
  if (offsetInt > Constants.TEST_OFFSET_INTEGER_MAXIMUM_VALUE || offsetInt < 0) {
    System.err.print(Constants.MSG_ERROR_TEST_OFFSET);
    System.out.println();
    System.exit(1);
  }
  offsetVal=Integer.toString(offsetInt);
  if (!Constants.validEnvironmentName(environmentName)) {
    System.err.print(Constants.MSG_ERROR_INVALID_ENVIRONMENT_NAME + environmentName);
    System.out.println();
    System.exit(1);
  }
  httpPortVal=httpPort;
  environmentNameVal=environmentName;
  mailReceiverVal=mailReceiver;
  httpPortPath=Constants.COMPLETE_HTTP_PORT_LEAF;
  environmentNamePath=Constants.COMPLETE_ENVIRONMENT_NAME_LEAF;
  mailReceiverPath=Constants.SETTINGS_NOTIFICATION_RECEIVER_PATH;
  offsetPaths=new OffsetSystem[]{new OffsetSystem(Constants.TEST_OFFSET_MONITOR_JMX_PORT,Constants.COMPLETE_JMX_PORT_PATH),new OffsetSystem(Constants.TEST_OFFSET_MONITOR_RMI_PORT,Constants.COMPLETE_JMX_RMIPORT_PATH),new OffsetSystem(Constants.TEST_OFFSET_HERITRIX_GUI_PORT,Constants.COMPLETE_HARVEST_HETRIX_GUI_PORT_PATH),new OffsetSystem(Constants.TEST_OFFSET_HERITRIX_JMX_PORT,Constants.COMPLETE_HARVEST_HETRIX_JMX_PORT)};
  apply();
}",0.9826315789473684
89853,"/** 
 * Creates a file containing the new configuration instance.
 * @param filename The name of the file to be written.
 * @throws IOException If anything goes wrong.
 */
public void createConfigurationFile(String filename) throws IOException {
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  File f=new File(filename);
  FileWriter fw=new FileWriter(f);
  fw.write(deployConfiguration.getXML());
  fw.close();
}","/** 
 * Creates a file containing the new configuration instance.
 * @param filename The name of the file to be written.
 * @throws IOException If anything goes wrong.
 */
public void createConfigurationFile(String filename) throws IOException {
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  File f=new File(filename);
  FileWriter fw=new FileWriter(f);
  try {
    fw.write(deployConfiguration.getXML());
  }
  finally {
    if (fw != null) {
      fw.close();
    }
  }
}",0.9334763948497854
89854,"/** 
 * The constructor.
 * @param configSource The source configuration file.
 */
public CreateTestInstance(File configSource){
  ArgumentNotValid.checkNotNull(configSource,""String_Node_Str"");
  source=configSource;
  deployConfiguration=new XmlStructure(source);
}","/** 
 * The constructor.
 * @param configSource The source configuration file.
 */
public CreateTestInstance(File configSource){
  ArgumentNotValid.checkNotNull(configSource,""String_Node_Str"");
  source=configSource;
  deployConfiguration=new XmlStructure(source);
  offsetPaths=new OffsetSystem[0];
  offsetVal=new String();
}",0.897133220910624
89855,"/** 
 * Creates the script for creating the application specified directories.
 * @return The script for creating the application specified directories.
 */
@Override protected String getAppDirectories(){
  StringBuilder res=new StringBuilder();
  String[] dirs;
  for (  Application app : applications) {
    dirs=app.getSettingsValues(Constants.SETTINGS_BITARCHIVE_BASEFILEDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      for (      String dir : dirs) {
        res.append(createPathToDir(dir));
        res.append(scriptCreateDir(dir,false));
        for (        String subdir : Constants.BASEFILEDIR_SUBDIRECTORIES) {
          res.append(scriptCreateDir(dir + Constants.SLASH + subdir,false));
        }
      }
    }
    dirs=app.getSettingsValues(Constants.SETTINGS_HARVEST_SERVERDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      for (      String dir : dirs) {
        res.append(createPathToDir(dir));
        res.append(scriptCreateDir(dir,false));
      }
    }
    dirs=app.getSettingsValues(Constants.SETTINGS_VIEWERPROXY_BASEDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      for (      String dir : dirs) {
        res.append(createPathToDir(dir));
        res.append(scriptCreateDir(dir,false));
      }
    }
    dirs=app.getSettingsValues(Constants.SETTINGS_TEMPDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      String machineDir=settings.getLeafValue(Constants.SETTINGS_TEMPDIR_LEAF);
      for (      String dir : dirs) {
        if (dir != machineDir) {
          res.append(createPathToDir(dir));
          res.append(scriptCreateDir(dir,resetTempDir));
        }
      }
    }
  }
  return res.toString();
}","/** 
 * Creates the script for creating the application specified directories.
 * @return The script for creating the application specified directories.
 */
@Override protected String getAppDirectories(){
  StringBuilder res=new StringBuilder();
  String[] dirs;
  for (  Application app : applications) {
    dirs=app.getSettingsValues(Constants.SETTINGS_BITARCHIVE_BASEFILEDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      for (      String dir : dirs) {
        res.append(createPathToDir(dir));
        res.append(scriptCreateDir(dir,false));
        for (        String subdir : Constants.BASEFILEDIR_SUBDIRECTORIES) {
          res.append(scriptCreateDir(dir + Constants.SLASH + subdir,false));
        }
      }
    }
    dirs=app.getSettingsValues(Constants.SETTINGS_HARVEST_SERVERDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      for (      String dir : dirs) {
        res.append(createPathToDir(dir));
        res.append(scriptCreateDir(dir,false));
      }
    }
    dirs=app.getSettingsValues(Constants.SETTINGS_VIEWERPROXY_BASEDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      for (      String dir : dirs) {
        res.append(createPathToDir(dir));
        res.append(scriptCreateDir(dir,false));
      }
    }
    dirs=app.getSettingsValues(Constants.SETTINGS_TEMPDIR_LEAF);
    if (dirs != null && dirs.length > 0) {
      String machineDir=settings.getLeafValue(Constants.SETTINGS_TEMPDIR_LEAF);
      for (      String dir : dirs) {
        if (!dir.equals(machineDir)) {
          res.append(createPathToDir(dir));
          res.append(scriptCreateDir(dir,resetTempDir));
        }
      }
    }
  }
  return res.toString();
}",0.9946236559139784
89856,"/** 
 * Creates the start scripts for all the applications. The application should only be started, if it is not running already. The script starts by finding all running processes of the application. If any processes are found, a new application should not be started. Otherwise start the application. The start_app.sh should have the following structure: - echo Starting linux application: app - cd path - #!/bin/bash - PIDS = $(ps -wwfe | grep fullapp | grep -v grep | grep  path\settings_app.xml | awk ""{print \\$2}"") - if [ -n ""$PIDS"" ]; then -     echo Application already running. - else -     export CLASSPATH = cp:$CLASSPATH; -     JAVA - fi where: path = the path to the install directory. fullapp = the full name application with java path. app = the name of the application. cp = the classpaths for the application. JAVA = the command to run the java application.
 * @param directory The directory for this machine (use global variable?).
 */
@Override protected void createApplicationStartScripts(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  for (  Application app : applications) {
    File appStartScript=new File(directory,Constants.SCRIPT_NAME_LOCAL_START + app.getIdentification() + scriptExtension);
    try {
      PrintWriter appPrint=new PrintWriter(appStartScript);
      try {
        appPrint.println(ScriptConstants.ECHO_START_LINUX_APP + Constants.COLON + Constants.SPACE+ app.getIdentification());
        appPrint.println(ScriptConstants.CD + Constants.SPACE + app.installPathLinux());
        appPrint.println(ScriptConstants.getLinuxPIDS(app.getTotalName(),getConfDirPath(),app.getIdentification()));
        appPrint.println(ScriptConstants.LINUX_IF_N_EXIST + Constants.SPACE + Constants.QUOTE_MARK+ ScriptConstants.PIDS+ Constants.QUOTE_MARK+ Constants.SPACE+ ScriptConstants.LINUX_N_THEN);
        appPrint.println(ScriptConstants.ECHO_APP_ALREADY_RUNNING);
        appPrint.println(ScriptConstants.ELSE);
        appPrint.println(ScriptConstants.EXPORT_CLASSPATH + osGetClassPath(app) + ScriptConstants.VALUE_OF_CLASSPATH+ Constants.SEMICOLON);
        appPrint.println(ScriptConstants.MULTI_SPACE_2 + ScriptConstants.JAVA + Constants.SPACE+ app.getMachineParameters().writeJavaOptions()+ Constants.SPACE+ Constants.DASH+ ScriptConstants.OPTION_SETTINGS+ getConfDirPath()+ Constants.PREFIX_SETTINGS+ app.getIdentification()+ Constants.EXTENSION_XML_FILES+ Constants.SPACE+ Constants.DASH+ ScriptConstants.OPTION_LOG_COMPLETE+ Constants.SPACE+ Constants.DASH+ ScriptConstants.OPTION_LOG_CONFIG+ getConfDirPath()+ Constants.LOG_PREFIX+ app.getIdentification()+ Constants.EXTENSION_LOG_PROPERTY_FILES+ Constants.SPACE+ Constants.DASH+ ScriptConstants.OPTION_SECURITY_MANAGER+ Constants.SPACE+ Constants.DASH+ ScriptConstants.OPTION_SECIRITY_POLICY+ getConfDirPath()+ Constants.SECURITY_POLICY_FILE_NAME+ Constants.SPACE+ app.getTotalName()+ Constants.SPACE+ ScriptConstants.LINUX_DEV_NULL+ Constants.SPACE+ Constants.SCRIPT_NAME_LOCAL_START+ app.getIdentification()+ Constants.EXTENSION_LOG_FILES+ Constants.SPACE+ ScriptConstants.LINUX_ERROR_MESSAGE_TO_1);
        appPrint.println(ScriptConstants.FI);
      }
  finally {
        appPrint.close();
      }
    }
 catch (    IOException e) {
      String msg=""String_Node_Str"" + e;
      log.trace(msg);
      throw new IOFailure(msg);
    }
catch (    Exception e) {
      String msg=""String_Node_Str"" + e;
      log.trace(msg);
      System.out.println(msg);
    }
  }
}","/** 
 * Creates the start scripts for all the applications. The application should only be started, if it is not running already. The script starts by finding all running processes of the application. If any processes are found, a new application should not be started. Otherwise start the application. The start_app.sh should have the following structure: - echo Starting linux application: app - cd path - #!/bin/bash - PIDS = $(ps -wwfe | grep fullapp | grep -v grep | grep  path\settings_app.xml | awk ""{print \\$2}"") - if [ -n ""$PIDS"" ]; then -     echo Application already running. - else -     export CLASSPATH = cp:$CLASSPATH; -     JAVA - fi where: path = the path to the install directory. fullapp = the full name application with java path. app = the name of the application. cp = the classpaths for the application. JAVA = the command to run the java application.
 * @param directory The directory for this machine (use global variable?).
 */
@Override protected void createApplicationStartScripts(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  for (  Application app : applications) {
    File appStartScript=new File(directory,Constants.SCRIPT_NAME_LOCAL_START + app.getIdentification() + scriptExtension);
    try {
      PrintWriter appPrint=new PrintWriter(appStartScript);
      try {
        appPrint.println(ScriptConstants.ECHO_START_LINUX_APP + Constants.COLON + Constants.SPACE+ app.getIdentification());
        appPrint.println(ScriptConstants.CD + Constants.SPACE + app.installPathLinux());
        appPrint.println(ScriptConstants.getLinuxPIDS(app.getTotalName(),getConfDirPath(),app.getIdentification()));
        appPrint.println(ScriptConstants.LINUX_IF_N_EXIST + Constants.SPACE + Constants.QUOTE_MARK+ ScriptConstants.PIDS+ Constants.QUOTE_MARK+ Constants.SPACE+ ScriptConstants.LINUX_N_THEN);
        appPrint.println(ScriptConstants.ECHO_APP_ALREADY_RUNNING);
        appPrint.println(ScriptConstants.ELSE);
        appPrint.println(ScriptConstants.EXPORT_CLASSPATH + osGetClassPath(app) + ScriptConstants.VALUE_OF_CLASSPATH+ Constants.SEMICOLON);
        appPrint.println(ScriptConstants.MULTI_SPACE_2 + ScriptConstants.JAVA + Constants.SPACE+ app.getMachineParameters().writeJavaOptions()+ Constants.SPACE+ Constants.DASH+ ScriptConstants.OPTION_SETTINGS+ getConfDirPath()+ Constants.PREFIX_SETTINGS+ app.getIdentification()+ Constants.EXTENSION_XML_FILES+ Constants.SPACE+ Constants.DASH+ ScriptConstants.OPTION_LOG_COMPLETE+ Constants.SPACE+ Constants.DASH+ ScriptConstants.OPTION_LOG_CONFIG+ getConfDirPath()+ Constants.LOG_PREFIX+ app.getIdentification()+ Constants.EXTENSION_LOG_PROPERTY_FILES+ Constants.SPACE+ Constants.DASH+ ScriptConstants.OPTION_SECURITY_MANAGER+ Constants.SPACE+ Constants.DASH+ ScriptConstants.OPTION_SECIRITY_POLICY+ getConfDirPath()+ Constants.SECURITY_POLICY_FILE_NAME+ Constants.SPACE+ app.getTotalName()+ Constants.SPACE+ ScriptConstants.LINUX_DEV_NULL+ Constants.SPACE+ Constants.SCRIPT_NAME_LOCAL_START+ app.getIdentification()+ Constants.EXTENSION_LOG_FILES+ Constants.SPACE+ ScriptConstants.LINUX_ERROR_MESSAGE_TO_1);
        appPrint.println(ScriptConstants.FI);
      }
  finally {
        appPrint.close();
      }
    }
 catch (    IOException e) {
      String msg=""String_Node_Str"" + e;
      log.trace(msg);
      throw new IOFailure(msg);
    }
catch (    Throwable e) {
      String msg=""String_Node_Str"" + e;
      log.trace(msg);
      System.out.println(msg);
    }
  }
}",0.9974219421369236
89857,"/** 
 * Function for creating the directories along the path until the end directory. Does not create the end directory.
 * @param dir The path to the directory.
 * @return The script for creating the directory.
 */
protected String createPathToDir(String dir){
  StringBuilder res=new StringBuilder();
  String[] pathDirs=dir.split(Constants.REGEX_SLASH_CHARACTER);
  String path=""String_Node_Str"";
  for (int i=0; i < pathDirs.length - 1; i++) {
    if (!pathDirs[i].isEmpty()) {
      path+=pathDirs[i];
      res.append(scriptCreateDir(path,false));
    }
    path+=Constants.SLASH;
  }
  return res.toString();
}","/** 
 * Function for creating the directories along the path until the end directory. Does not create the end directory.
 * @param dir The path to the directory.
 * @return The script for creating the directory.
 */
protected String createPathToDir(String dir){
  StringBuilder res=new StringBuilder();
  String[] pathDirs=dir.split(Constants.REGEX_SLASH_CHARACTER);
  StringBuilder path=new StringBuilder();
  for (int i=0; i < pathDirs.length - 1; i++) {
    if (!pathDirs[i].isEmpty()) {
      path.append(pathDirs[i]);
      res.append(scriptCreateDir(path.toString(),false));
    }
    path.append(Constants.SLASH);
  }
  return res.toString();
}",0.94006309148265
89858,"/** 
 * Creates a the log property file for every application. This is done by taking the inherited log file and changing  ""APPID"" in the file into the identification of the application.
 * @param directory The local directory for this machine
 */
protected void createLogPropertyFiles(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  for (  Application app : applications) {
    try {
      File logProp=new File(directory,Constants.LOG_PROP_APPLICATION_PREFIX + app.getIdentification() + Constants.LOG_PROP_APPLICATION_SUFFIX);
      FileWriter logfw=new FileWriter(logProp);
      String prop=FileUtils.readFile(inheritedLogPropFile);
      prop=prop.replace(Constants.LOG_PROPERTY_APPLICATION_ID_TAG,app.getIdentification());
      logfw.write(prop);
      logfw.close();
    }
 catch (    IOException e) {
      log.warn(""String_Node_Str"" + e);
    }
  }
}","/** 
 * Creates a the log property file for every application. This is done by taking the inherited log file and changing  ""APPID"" in the file into the identification of the application.
 * @param directory The local directory for this machine
 */
protected void createLogPropertyFiles(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  for (  Application app : applications) {
    try {
      File logProp=new File(directory,Constants.LOG_PROP_APPLICATION_PREFIX + app.getIdentification() + Constants.LOG_PROP_APPLICATION_SUFFIX);
      FileWriter logfw=new FileWriter(logProp);
      String prop=FileUtils.readFile(inheritedLogPropFile);
      prop=prop.replace(Constants.LOG_PROPERTY_APPLICATION_ID_TAG,app.getIdentification());
      try {
        logfw.write(prop);
      }
  finally {
        if (logfw != null) {
          logfw.close();
        }
      }
    }
 catch (    IOException e) {
      log.warn(""String_Node_Str"" + e);
    }
  }
}",0.9545697487974344
89859,"/** 
 * Copy inherited securityPolicyFile to local directory.
 * @param directory The local directory for this machine
 */
protected void createSecurityPolicyFile(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  try {
    File secPolFile=new File(directory,Constants.SECURITY_POLICY_FILE_NAME);
    FileWriter secfw=new FileWriter(secPolFile);
    String prop=FileUtils.readFile(inheritedSecurityPolicyFile);
    String monitorRole=settings.getLeafValue(Constants.SETTINGS_MONITOR_JMX_NAME_LEAF);
    if (monitorRole != null) {
      prop=prop.replace(Constants.SECURITY_JMX_PRINCIPAL_NAME_TAG,monitorRole);
    }
    String ctd=settings.getLeafValue(Constants.SETTINGS_TEMPDIR_LEAF);
    if (ctd != null) {
      prop=prop.replace(Constants.SECURITY_COMMON_TEMP_DIR_TAG,ctd);
    }
    secfw.write(prop);
    List<String> dirs=new ArrayList<String>();
    for (    Application app : applications) {
      String[] tmpDirs=app.getSettingsValues(Constants.SETTINGS_BITARCHIVE_BASEFILEDIR_LEAF);
      if (tmpDirs != null && tmpDirs.length > 0) {
        for (        String st : tmpDirs) {
          dirs.add(st);
        }
      }
    }
    if (!dirs.isEmpty()) {
      secfw.write(""String_Node_Str"" + ""String_Node_Str"");
      for (      String dir : dirs) {
        secfw.write(ScriptConstants.writeSecurityPolicyDirPermission(changeFileDirPathForSecurity(dir)));
      }
      secfw.write(""String_Node_Str"");
    }
    secfw.close();
  }
 catch (  IOException e) {
    log.warn(""String_Node_Str"" + e);
  }
}","/** 
 * Copy inherited securityPolicyFile to local directory.
 * @param directory The local directory for this machine
 */
protected void createSecurityPolicyFile(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  try {
    File secPolFile=new File(directory,Constants.SECURITY_POLICY_FILE_NAME);
    FileWriter secfw=new FileWriter(secPolFile);
    String prop=FileUtils.readFile(inheritedSecurityPolicyFile);
    String monitorRole=settings.getLeafValue(Constants.SETTINGS_MONITOR_JMX_NAME_LEAF);
    if (monitorRole != null) {
      prop=prop.replace(Constants.SECURITY_JMX_PRINCIPAL_NAME_TAG,monitorRole);
    }
    String ctd=settings.getLeafValue(Constants.SETTINGS_TEMPDIR_LEAF);
    if (ctd != null) {
      prop=prop.replace(Constants.SECURITY_COMMON_TEMP_DIR_TAG,ctd);
    }
    try {
      secfw.write(prop);
      List<String> dirs=new ArrayList<String>();
      for (      Application app : applications) {
        String[] tmpDirs=app.getSettingsValues(Constants.SETTINGS_BITARCHIVE_BASEFILEDIR_LEAF);
        if (tmpDirs != null && tmpDirs.length > 0) {
          for (          String st : tmpDirs) {
            dirs.add(st);
          }
        }
      }
      if (!dirs.isEmpty()) {
        secfw.write(""String_Node_Str"" + ""String_Node_Str"");
        for (        String dir : dirs) {
          secfw.write(ScriptConstants.writeSecurityPolicyDirPermission(changeFileDirPathForSecurity(dir)));
        }
        secfw.write(""String_Node_Str"");
      }
    }
  finally {
      if (secfw != null) {
        secfw.close();
      }
    }
  }
 catch (  IOException e) {
    log.warn(""String_Node_Str"" + e);
  }
}",0.955270566155771
89860,"/** 
 * Creates the following scripts for this physical location. * killall. * install. * startall. The scripts for a physical location will only work from Linux/Unix.  
 * @param directory The directory where the scripts are to be placed.
 */
private void makeScripts(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  String ext=Constants.UNDERSCORE + name + Constants.SCRIPT_EXTENSION_LINUX;
  File killall=new File(directory,Constants.SCRIPT_NAME_KILL_ALL + ext);
  File install=new File(directory,Constants.SCRIPT_NAME_INSTALL_ALL + ext);
  File startall=new File(directory,Constants.SCRIPT_NAME_START_ALL + ext);
  try {
    PrintWriter kWriter=new PrintWriter(killall);
    PrintWriter iWriter=new PrintWriter(install);
    PrintWriter sWriter=new PrintWriter(startall);
    try {
      kWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      iWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      sWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      for (      Machine mac : machines) {
        iWriter.println(ScriptConstants.writeDashLine());
        iWriter.print(mac.writeToGlobalInstallScript());
        sWriter.println(ScriptConstants.writeDashLine());
        sWriter.print(mac.writeToGlobalStartScript());
        kWriter.println(ScriptConstants.writeDashLine());
        kWriter.print(mac.writeToGlobalKillScript());
      }
    }
  finally {
      kWriter.println(ScriptConstants.writeDashLine());
      kWriter.close();
      iWriter.println(ScriptConstants.writeDashLine());
      iWriter.close();
      sWriter.println(ScriptConstants.writeDashLine());
      sWriter.close();
    }
  }
 catch (  IOException e) {
    String msg=""String_Node_Str"" + ""String_Node_Str"" + e;
    log.trace(msg);
    throw new IOFailure(msg);
  }
catch (  Exception e) {
    String msg=""String_Node_Str"" + ""String_Node_Str"" + e;
    log.trace(msg);
    System.out.println(msg);
  }
}","/** 
 * Creates the following scripts for this physical location. * killall. * install. * startall. The scripts for a physical location will only work from Linux/Unix.  
 * @param directory The directory where the scripts are to be placed.
 */
private void makeScripts(File directory){
  ArgumentNotValid.checkNotNull(directory,""String_Node_Str"");
  String ext=Constants.UNDERSCORE + name + Constants.SCRIPT_EXTENSION_LINUX;
  File killall=new File(directory,Constants.SCRIPT_NAME_KILL_ALL + ext);
  File install=new File(directory,Constants.SCRIPT_NAME_INSTALL_ALL + ext);
  File startall=new File(directory,Constants.SCRIPT_NAME_START_ALL + ext);
  try {
    PrintWriter kWriter=new PrintWriter(killall);
    PrintWriter iWriter=new PrintWriter(install);
    PrintWriter sWriter=new PrintWriter(startall);
    try {
      kWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      iWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      sWriter.println(ScriptConstants.BIN_BASH_COMMENT);
      for (      Machine mac : machines) {
        iWriter.println(ScriptConstants.writeDashLine());
        iWriter.print(mac.writeToGlobalInstallScript());
        sWriter.println(ScriptConstants.writeDashLine());
        sWriter.print(mac.writeToGlobalStartScript());
        kWriter.println(ScriptConstants.writeDashLine());
        kWriter.print(mac.writeToGlobalKillScript());
      }
      kWriter.println(ScriptConstants.writeDashLine());
      iWriter.println(ScriptConstants.writeDashLine());
      sWriter.println(ScriptConstants.writeDashLine());
    }
  finally {
      if (kWriter != null) {
        kWriter.close();
      }
      if (iWriter != null) {
        iWriter.close();
      }
      if (sWriter != null) {
        sWriter.close();
      }
    }
  }
 catch (  IOException e) {
    String msg=""String_Node_Str"" + ""String_Node_Str"" + e;
    log.trace(msg);
    throw new IOFailure(msg);
  }
catch (  Exception e) {
    String msg=""String_Node_Str"" + ""String_Node_Str"" + e;
    log.trace(msg);
    System.out.println(msg);
  }
}",0.9289866060146575
89861,"/** 
 * Method for updating the filelist of a replicafileinfo instance. Updates the following fields for the entry in the replicafileinfo: <br/> filelist_status = OK. <br/> filelist_checkdatetime = current time. 
 * @param replicafileinfoId The id of the replicafileinfo.
 */
private void updateReplicaFileInfoFilelist(long replicafileinfoId){
  try {
    String sql=""String_Node_Str"" + ""String_Node_Str"";
    PreparedStatement statement=null;
    dbConnection.setAutoCommit(false);
    Date now=new Date(Calendar.getInstance().getTimeInMillis());
    statement=DBUtils.prepareStatement(dbConnection,sql,FileListStatus.OK.ordinal(),now,replicafileinfoId);
    statement.executeUpdate();
    dbConnection.commit();
  }
 catch (  Exception e) {
    String msg=""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg,e);
  }
}","/** 
 * Method for updating the filelist of a replicafileinfo instance. Updates the following fields for the entry in the replicafileinfo: <br/> filelist_status = OK. <br/> filelist_checkdatetime = current time. 
 * @param replicafileinfoId The id of the replicafileinfo.
 */
private void updateReplicaFileInfoFilelist(long replicafileinfoId){
  try {
    String sql=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"";
    PreparedStatement statement=null;
    dbConnection.setAutoCommit(false);
    Date now=new Date(Calendar.getInstance().getTimeInMillis());
    statement=DBUtils.prepareStatement(dbConnection,sql,FileListStatus.OK.ordinal(),now,replicafileinfoId);
    statement.executeUpdate();
    dbConnection.commit();
  }
 catch (  Exception e) {
    String msg=""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg,e);
  }
}",0.9881376037959668
89862,"/** 
 * Method for updating the filelist of a replicafileinfo instance. Updates the following fields for the entry in the replicafileinfo: <br/> filelist_status = missing. <br/> filelist_checkdatetime = current time.  The replicafileinfo is in the filelist.
 * @param replicafileinfoId The id of the replicafileinfo.
 */
private void updateReplicaFileInfoMissingFromFilelist(long replicafileinfoId){
  try {
    String sql=""String_Node_Str"" + ""String_Node_Str"";
    PreparedStatement statement=null;
    dbConnection.setAutoCommit(false);
    Date now=new Date(Calendar.getInstance().getTimeInMillis());
    statement=DBUtils.prepareStatement(dbConnection,sql,FileListStatus.MISSING.ordinal(),now,replicafileinfoId);
    statement.executeUpdate();
    dbConnection.commit();
  }
 catch (  Exception e) {
    String msg=""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg,e);
  }
}","/** 
 * Method for updating the filelist of a replicafileinfo instance. Updates the following fields for the entry in the replicafileinfo: <br/> filelist_status = missing. <br/> filelist_checkdatetime = current time.  The replicafileinfo is in the filelist.
 * @param replicafileinfoId The id of the replicafileinfo.
 */
private void updateReplicaFileInfoMissingFromFilelist(long replicafileinfoId){
  try {
    String sql=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"";
    PreparedStatement statement=null;
    dbConnection.setAutoCommit(false);
    Date now=new Date(Calendar.getInstance().getTimeInMillis());
    statement=DBUtils.prepareStatement(dbConnection,sql,FileListStatus.MISSING.ordinal(),now,replicafileinfoId);
    statement.executeUpdate();
    dbConnection.commit();
  }
 catch (  Exception e) {
    String msg=""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg,e);
  }
}",0.9889380530973452
89863,"/** 
 * Method for finding a replica with a valid version of a file. This method is used in order to find a replica from which a file should be retrieved, during the process of restoring a corrupt file on another replica. This replica must of the type bitarchive, since a file cannot be  retrieved from a checksum replica.
 * @param filename The name of the file which needs to have a valid versionin a bitarchive.
 * @return A bitarchive which contains a valid version of the file, or nullif no such bitarchive exists.
 */
@Override public Replica getBitarchiveWithGoodFile(String filename,Replica badReplica){
  long fileId=retrieveIdForFile(filename);
  List<String> replicaIds=retrieveReplicaIdsWithOKChecksumStatus(fileId);
  replicaIds.remove(badReplica.getId());
  for (  String repId : replicaIds) {
    ReplicaType repType=ReplicaType.fromOrdinal(retrieveReplicaType(repId));
    if (repType.equals(ReplicaType.BITARCHIVE)) {
      log.trace(""String_Node_Str"" + repId + ""String_Node_Str""+ ""String_Node_Str""+ filename+ ""String_Node_Str"");
      return Replica.getReplicaFromId(repId);
    }
  }
  log.warn(""String_Node_Str"" + filename + ""String_Node_Str"");
  return null;
}","/** 
 * Method for finding a replica with a valid version of a file. This method is used in order to find a replica from which a file should be retrieved, during the process of restoring a corrupt file on another replica. This replica must of the type bitarchive, since a file cannot be  retrieved from a checksum replica.
 * @param filename The name of the file which needs to have a valid versionin a bitarchive.
 * @param badReplica The Replica which has a bad copy of the given file
 * @return A bitarchive which contains a valid version of the file, or nullif no such bitarchive exists.
 */
@Override public Replica getBitarchiveWithGoodFile(String filename,Replica badReplica){
  long fileId=retrieveIdForFile(filename);
  List<String> replicaIds=retrieveReplicaIdsWithOKChecksumStatus(fileId);
  replicaIds.remove(badReplica.getId());
  for (  String repId : replicaIds) {
    ReplicaType repType=ReplicaType.fromOrdinal(retrieveReplicaType(repId));
    if (repType.equals(ReplicaType.BITARCHIVE)) {
      log.trace(""String_Node_Str"" + repId + ""String_Node_Str""+ ""String_Node_Str""+ filename+ ""String_Node_Str"");
      return Replica.getReplicaFromId(repId);
    }
  }
  log.warn(""String_Node_Str"" + filename + ""String_Node_Str"");
  return null;
}",0.9704190632703368
89864,"/** 
 * Method for updating the checksum status of a replicafileinfo instance. Updates the following fields for the entry in the replicafileinfo: <br/> checksum_status = OK. <br/> checksum_checkdatetime = current time.  The replicafileinfo is in the filelist.
 * @param replicafileinfoId The id of the replicafileinfo.
 */
private void updateReplicaFileInfoChecksumOk(long replicafileinfoId){
  try {
    String sql=""String_Node_Str"" + ""String_Node_Str"";
    PreparedStatement statement=null;
    dbConnection.setAutoCommit(false);
    Date now=new Date(Calendar.getInstance().getTimeInMillis());
    statement=DBUtils.prepareStatement(dbConnection,sql,ChecksumStatus.OK.ordinal(),now,replicafileinfoId);
    statement.executeUpdate();
    dbConnection.commit();
  }
 catch (  Exception e) {
    String msg=""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg,e);
  }
}","/** 
 * Method for updating the checksum status of a replicafileinfo instance. Updates the following fields for the entry in the replicafileinfo: <br/> checksum_status = OK. <br/> checksum_checkdatetime = current time.  The replicafileinfo is in the filelist.
 * @param replicafileinfoId The id of the replicafileinfo.
 */
private void updateReplicaFileInfoChecksumOk(long replicafileinfoId){
  try {
    String sql=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"";
    PreparedStatement statement=null;
    dbConnection.setAutoCommit(false);
    Date now=new Date(Calendar.getInstance().getTimeInMillis());
    statement=DBUtils.prepareStatement(dbConnection,sql,ChecksumStatus.OK.ordinal(),now,replicafileinfoId);
    statement.executeUpdate();
    dbConnection.commit();
  }
 catch (  Exception e) {
    String msg=""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg,e);
  }
}",0.9887892376681614
89865,"/** 
 * Method for updating the checksum status of a replicafileinfo instance. Updates the following fields for the entry in the replicafileinfo: <br/> checksum_status = CORRUPT. <br/> checksum_checkdatetime = current time.  The replicafileinfo is in the filelist.
 * @param replicafileinfoId The id of the replicafileinfo.
 */
private void updateReplicaFileInfoChecksumCorrupt(long replicafileinfoId){
  try {
    String sql=""String_Node_Str"" + ""String_Node_Str"";
    PreparedStatement statement=null;
    dbConnection.setAutoCommit(false);
    Date now=new Date(Calendar.getInstance().getTimeInMillis());
    statement=DBUtils.prepareStatement(dbConnection,sql,ChecksumStatus.CORRUPT.ordinal(),now,replicafileinfoId);
    statement.executeUpdate();
    dbConnection.commit();
  }
 catch (  Exception e) {
    String msg=""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg,e);
  }
}","/** 
 * Method for updating the checksum status of a replicafileinfo instance. Updates the following fields for the entry in the replicafileinfo: <br/> checksum_status = CORRUPT. <br/> checksum_checkdatetime = current time.  The replicafileinfo is in the filelist.
 * @param replicafileinfoId The id of the replicafileinfo.
 */
private void updateReplicaFileInfoChecksumCorrupt(long replicafileinfoId){
  try {
    String sql=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"";
    PreparedStatement statement=null;
    dbConnection.setAutoCommit(false);
    Date now=new Date(Calendar.getInstance().getTimeInMillis());
    statement=DBUtils.prepareStatement(dbConnection,sql,ChecksumStatus.CORRUPT.ordinal(),now,replicafileinfoId);
    statement.executeUpdate();
    dbConnection.commit();
  }
 catch (  Exception e) {
    String msg=""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg,e);
  }
}",0.9889746416758544
89866,"/** 
 * Method for updating the checksum status of a replicafileinfo instance. Updates the following fields for the entry in the replicafileinfo: <br/> checksum_status = UNKNOWN. <br/> checksum_checkdatetime = current time.  The replicafileinfo is in the filelist.
 * @param replicafileinfoId The id of the replicafileinfo.
 */
private void updateReplicaFileInfoChecksumUnknown(long replicafileinfoId){
  try {
    String sql=""String_Node_Str"" + ""String_Node_Str"";
    PreparedStatement statement=null;
    dbConnection.setAutoCommit(false);
    Date now=new Date(Calendar.getInstance().getTimeInMillis());
    statement=DBUtils.prepareStatement(dbConnection,sql,ChecksumStatus.UNKNOWN.ordinal(),now,replicafileinfoId);
    statement.executeUpdate();
    dbConnection.commit();
  }
 catch (  Exception e) {
    String msg=""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg,e);
  }
}","/** 
 * Method for updating the checksum status of a replicafileinfo instance. Updates the following fields for the entry in the replicafileinfo: <br/> checksum_status = UNKNOWN. <br/> checksum_checkdatetime = current time.  The replicafileinfo is in the filelist.
 * @param replicafileinfoId The id of the replicafileinfo.
 */
private void updateReplicaFileInfoChecksumUnknown(long replicafileinfoId){
  try {
    String sql=""String_Node_Str"" + ""String_Node_Str"" + ""String_Node_Str"";
    PreparedStatement statement=null;
    dbConnection.setAutoCommit(false);
    Date now=new Date(Calendar.getInstance().getTimeInMillis());
    statement=DBUtils.prepareStatement(dbConnection,sql,ChecksumStatus.UNKNOWN.ordinal(),now,replicafileinfoId);
    statement.executeUpdate();
    dbConnection.commit();
  }
 catch (  Exception e) {
    String msg=""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg,e);
  }
}",0.9889746416758544
89867,"/** 
 * Check that the checksum of the file is indeed different to the value in admin data and reference replica. If so, remove missing file and upload it from reference replica to this replica.
 * @param replica The replica to restore file to
 * @param filename The name of the file
 * @param credentials The credentials used to perform this replace operation
 * @param checksum  The known bad checksum. Only a file with this badchecksum is attempted repaired.
 * @throws IOFailure if the file cannot be reestablished.
 * @throws PermissionDenied if the file is not in correct state.
 * @throws ArgumentNotValid if any of the arguments are not valid.
 */
public void replaceChangedFile(Replica replica,String filename,String credentials,String checksum){
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(checksum,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(credentials,""String_Node_Str"");
  if (replica.getType().equals(ReplicaType.BITARCHIVE)) {
    System.out.println(""String_Node_Str"");
    ArcRepositoryClientFactory.getPreservationInstance().removeAndGetFile(filename,replica.getId(),checksum,credentials);
    uploadMissingFiles(replica,filename);
  }
 else   if (replica.getType().equals(ReplicaType.CHECKSUM)) {
    System.out.println(""String_Node_Str"");
    Replica repWithFile=cache.getBitarchiveWithGoodFile(filename,replica);
    File missingFile=retrieveRemoteFile(filename,repWithFile);
    ArcRepositoryClientFactory.getPreservationInstance().correct(replica.getId(),checksum,missingFile,credentials);
  }
 else {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + replica;
    log.warn(errMsg);
    throw new NotImplementedException(errMsg);
  }
}","/** 
 * Check that the checksum of the file is indeed different to the value in admin data and reference replica. If so, remove missing file and upload it from reference replica to this replica.
 * @param replica The replica to restore file to
 * @param filename The name of the file
 * @param credentials The credentials used to perform this replace operation
 * @param checksum  The known bad checksum. Only a file with this badchecksum is attempted repaired.
 * @throws IOFailure if the file cannot be reestablished.
 * @throws UnknownID if the file is not in correct state.
 * @throws ArgumentNotValid if any of the arguments are not valid.
 */
public void replaceChangedFile(Replica replica,String filename,String credentials,String checksum) throws UnknownID, IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(checksum,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(credentials,""String_Node_Str"");
  if (replica.getType().equals(ReplicaType.BITARCHIVE)) {
    System.out.println(""String_Node_Str"");
    ArcRepositoryClientFactory.getPreservationInstance().removeAndGetFile(filename,replica.getId(),checksum,credentials);
    uploadMissingFiles(replica,filename);
  }
 else   if (replica.getType().equals(ReplicaType.CHECKSUM)) {
    System.out.println(""String_Node_Str"");
    Replica repWithFile=cache.getBitarchiveWithGoodFile(filename,replica);
    File missingFile=retrieveRemoteFile(filename,repWithFile);
    ArcRepositoryClientFactory.getPreservationInstance().correct(replica.getId(),checksum,missingFile,credentials);
  }
 else {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + replica;
    log.warn(errMsg);
    throw new UnknownID(errMsg);
  }
}",0.972482113373693
89868,"/** 
 * Run any batch job on a replica, possibly restricted to a certain set of files, and place the output in the given file. The results will also be checked to verify that there for each file processed is a line in the output file.
 * @param job The BatchJob to run upon the archive.
 * @param replica The replica (which has to be a bitarchive) that the job should run on.
 * @param specifiedFiles  The files to run the job on, or null if it shouldrun on all files.
 * @param batchOutputFile Where to put the result of the job.
 * @throws IllegalState If the replica is not of the type 'BITARCHIVE', which is required for a batchjob.
 */
private void runBatchJob(FileBatchJob job,Replica replica,List<String> specifiedFiles,File batchOutputFile){
  if (!replica.getType().equals(ReplicaType.BITARCHIVE)) {
    String msg=""String_Node_Str"" + replica + ""String_Node_Str""+ ""String_Node_Str""+ ReplicaType.BITARCHIVE.name();
    log.warn(msg);
    throw new IllegalState(msg);
  }
  job.processOnlyFilesNamed(specifiedFiles);
  BatchStatus status=ArcRepositoryClientFactory.getPreservationInstance().batch(job,replica.getId());
  if (!status.hasResultFile()) {
    String msg=""String_Node_Str"" + replica + ""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg);
  }
  if (!status.getFilesFailed().isEmpty()) {
    String msg=""String_Node_Str"" + status.getFilesFailed().toString();
    log.warn(msg);
    throw new IOFailure(msg);
  }
  status.copyResults(batchOutputFile);
  log.info(""String_Node_Str"" + status.getNoOfFilesProcessed() + ""String_Node_Str""+ replica);
}","/** 
 * Run any batch job on a replica, possibly restricted to a certain set of files, and place the output in the given file. The results will also be checked to verify that there for each file processed is a line in the output file.
 * @param job The BatchJob to run upon the archive.
 * @param replica The replica (which has to be a bitarchive) that the job should run on.
 * @param specifiedFiles  The files to run the job on, or null if it shouldrun on all files.
 * @param batchOutputFile Where to put the result of the job.
 * @throws IllegalState If the replica is not of the type 'BITARCHIVE', which is required for a batchjob.
 * @throws IOFailure If the batchjob status is invalid.
 */
private void runBatchJob(FileBatchJob job,Replica replica,List<String> specifiedFiles,File batchOutputFile) throws IOFailure, IllegalState {
  if (!replica.getType().equals(ReplicaType.BITARCHIVE)) {
    String msg=""String_Node_Str"" + replica + ""String_Node_Str""+ ""String_Node_Str""+ ReplicaType.BITARCHIVE.name();
    log.warn(msg);
    throw new IllegalState(msg);
  }
  job.processOnlyFilesNamed(specifiedFiles);
  BatchStatus status=ArcRepositoryClientFactory.getPreservationInstance().batch(job,replica.getId());
  if (!status.hasResultFile()) {
    String msg=""String_Node_Str"" + replica + ""String_Node_Str"";
    log.warn(msg);
    throw new IOFailure(msg);
  }
  if (!status.getFilesFailed().isEmpty()) {
    String msg=""String_Node_Str"" + status.getFilesFailed().toString();
    log.warn(msg);
    throw new IOFailure(msg);
  }
  status.copyResults(batchOutputFile);
  log.info(""String_Node_Str"" + status.getNoOfFilesProcessed() + ""String_Node_Str""+ replica);
}",0.972856261566934
89869,"/** 
 * The method is used to update the checksum for all the files in a replica. The checksum for the replica is retrieved either through a ChecksumJob (for a bitarchive) or through a GetAllChecksumMessage (for a  checksumarchive). This will take a very large amount of time for the bitarchive, but a  more limited amount of time for the checksumarchive. The corresponding replicafileinfo entries in the database for the  retrieved checksum results will be updated. Then a checksum update will  be performed to check for corrupted replicafileinfo.
 * @param replica The replica to find the changed files for.
 */
public void findChangedFiles(Replica replica){
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  runChecksum(replica);
  initChecksumStatusUpdate();
  cache.updateChecksumStatus();
}","/** 
 * The method is used to update the checksum for all the files in a replica. The checksum for the replica is retrieved either through a ChecksumJob (for a bitarchive) or through a GetAllChecksumMessage (for a  checksumarchive). This will take a very large amount of time for the bitarchive, but a  more limited amount of time for the checksumarchive. The corresponding replicafileinfo entries in the database for the  retrieved checksum results will be updated. Then a checksum update will  be performed to check for corrupted replicafileinfo.
 * @param replica The replica to find the changed files for.
 * @throws ArgumentNotValid If the replica is null.
 */
public void findChangedFiles(Replica replica) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  runChecksum(replica);
  initChecksumStatusUpdate();
  cache.updateChecksumStatus();
}",0.954518606024808
89870,"@Override public FilePreservationState getFilePreservationState(String filename){
  throw new NotImplementedException(""String_Node_Str"");
}","/** 
 * Method for retrieving the FilePreservationState for a specific file.
 * @param filename The name of the file for whom the FilePreservationStateshould be retrieved.
 * @return The FilePreservationState for the file.
 * @throws NotImplementedException This method has not yet been implemented.
 * @throws ArgumentNotValid If the filename does not have a valid name.
 */
@Override public FilePreservationState getFilePreservationState(String filename) throws NotImplementedException, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  throw new NotImplementedException(""String_Node_Str"");
}",0.3419689119170984
89871,"@Override public Iterable<String> getChangedFilesForAdminData(){
  throw new NotImplementedException(""String_Node_Str"" + ""String_Node_Str"");
}","/** 
 * Old method, which refers to the checksum replica part of admin data.
 * @return nothing, since it always throws an exception.
 * @throws NotImplementedException This method will not be implemented.
 */
@Override public Iterable<String> getChangedFilesForAdminData(){
  throw new NotImplementedException(""String_Node_Str"" + ""String_Node_Str"");
}",0.5748987854251012
89872,"@Override public Map<String,FilePreservationState> getFilePreservationStateMap(String... filenames){
  throw new NotImplementedException(""String_Node_Str"");
}","/** 
 * Method for retrieving the FilePreservationState for a list of files.
 * @param filenames The list of filenames whos FilePreservationState shouldbe retrieved.
 * @return A mapping between the filenames and their FilePreservationState.
 * @throws NotImplementedException Since it has not yet been implemented.
 * @throws ArgumentNotValid If the filenames are invalid.
 */
@Override public Map<String,FilePreservationState> getFilePreservationStateMap(String... filenames) throws ArgumentNotValid, NotImplementedException {
  ArgumentNotValid.checkNotNull(filenames,""String_Node_Str"");
  throw new NotImplementedException(""String_Node_Str"");
}",0.1861042183622828
89873,"@Override public Iterable<String> getMissingFilesForAdminData(){
  throw new NotImplementedException(""String_Node_Str"" + ""String_Node_Str"");
}","/** 
 * Old method, which refers to the checksum replica part of admin data.
 * @return nothing, since it always throws an exception.
 * @throws NotImplementedException This method will not be implemented.
 */
@Override public Iterable<String> getMissingFilesForAdminData(){
  throw new NotImplementedException(""String_Node_Str"" + ""String_Node_Str"");
}",0.5748987854251012
89874,"/** 
 * Method for retrieving the filelist from a specific replica. The filelist are retrieved differently for the different types of  replica: <br/> The filelist of a bitarchive is retrieved through running  the BatchJob FilelistJob on the replica. <br/> The filelist of a checksumsarchive is retrieved through a  GetAllFilenamesMessage.
 * @param replica The replica to retrieve the filelist from.
 * @return The names of the files in a list.
 */
private List<String> getFilenamesList(Replica replica){
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  if (replica.getType() == ReplicaType.BITARCHIVE) {
    File outputFile=WorkFiles.getFile(replica,WorkFiles.CHECKSUMS_ON_BA);
    log.info(""String_Node_Str"" + replica + ""String_Node_Str"");
    runBatchJob(new FileListJob(),replica,null,outputFile);
    return FileUtils.readListFromFile(outputFile);
  }
 else   if (replica.getType() == ReplicaType.CHECKSUM) {
    log.info(""String_Node_Str"" + replica + ""String_Node_Str"");
    return FileUtils.readListFromFile(ArcRepositoryClientFactory.getPreservationInstance().getAllFilenames(replica.getId()));
  }
 else {
    String msg=""String_Node_Str"" + replica + ""String_Node_Str"";
    log.warn(msg);
    throw new IllegalState(msg);
  }
}","/** 
 * Method for retrieving the filelist from a specific replica. The filelist are retrieved differently for the different types of  replica: <br/> The filelist of a bitarchive is retrieved through running  the BatchJob FilelistJob on the replica. <br/> The filelist of a checksumsarchive is retrieved through a  GetAllFilenamesMessage.
 * @param replica The replica to retrieve the filelist from.
 * @return The names of the files in a list.
 * @throws ArgumentNotValid If the replica is 'null'.
 * @throws UnknownID If the replica has a unhandled replica type.
 */
private List<String> getFilenamesList(Replica replica) throws ArgumentNotValid, UnknownID {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  if (replica.getType() == ReplicaType.BITARCHIVE) {
    File outputFile=WorkFiles.getFile(replica,WorkFiles.CHECKSUMS_ON_BA);
    log.info(""String_Node_Str"" + replica + ""String_Node_Str"");
    runBatchJob(new FileListJob(),replica,null,outputFile);
    return FileUtils.readListFromFile(outputFile);
  }
 else   if (replica.getType() == ReplicaType.CHECKSUM) {
    log.info(""String_Node_Str"" + replica + ""String_Node_Str"");
    return FileUtils.readListFromFile(ArcRepositoryClientFactory.getPreservationInstance().getAllFilenames(replica.getId()));
  }
 else {
    String msg=""String_Node_Str"" + replica + ""String_Node_Str"";
    log.warn(msg);
    throw new UnknownID(msg);
  }
}",0.9339871746510752
89875,"/** 
 * This method retrieves the filelist for the replica, and then it updates the database with this list of filenames.
 * @param replica The replica to find the missing files for.
 */
public void findMissingFiles(Replica replica){
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  List<String> filenames=getFilenamesList(replica);
  cache.addFileListInformation(filenames,replica);
}","/** 
 * This method retrieves the filelist for the replica, and then it updates the database with this list of filenames.
 * @param replica The replica to find the missing files for.
 * @throws ArgumentNotValid If the replica is null.
 */
public void findMissingFiles(Replica replica) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  List<String> filenames=getFilenamesList(replica);
  cache.addFileListInformation(filenames,replica);
}",0.9117983963344788
89876,"@Override public void changeStateForAdminData(String filename){
  throw new NotImplementedException(""String_Node_Str"");
}","/** 
 * This should reestablish the state for the file.
 * @param filename The name of the file to change the state for.
 * @throws ArgumentNotValid If the filename is invalid.
 * @throws NotImplementedException Since it has not yet been implemented.
 */
@Override public void changeStateForAdminData(String filename) throws ArgumentNotValid, NotImplementedException {
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  throw new NotImplementedException(""String_Node_Str"");
}",0.3707317073170731
89877,"@Override public void cleanup(){
  instance=null;
  cache.cleanup();
}","/** 
 * Method for cleaning up this instance.
 */
@Override public void cleanup(){
  instance=null;
  cache.cleanup();
}",0.7368421052631579
89878,"@Override public void addMissingFilesToAdminData(String... filenames){
  throw new NotImplementedException(""String_Node_Str"" + ""String_Node_Str"");
}","/** 
 * Old method, which refers to teh checksum replica part of admin data.
 * @param filenames The list of filenames which should be added to admin data.
 * @throws NotImplementedException This method will not be implemented.
 * @throws ArgumentNotValid If filenames invalid.
 */
@Override public void addMissingFilesToAdminData(String... filenames) throws ArgumentNotValid, NotImplementedException {
  ArgumentNotValid.checkNotNull(filenames,""String_Node_Str"");
  throw new NotImplementedException(""String_Node_Str"" + ""String_Node_Str"");
}",0.3333333333333333
89879,"/** 
 * Reloads the settings. This will reload the settings from disk, and forget all settings that were set with   {@link #set}The field   {@link #lastModified} is updated to timestamp of the settingsfile that has been changed most recently.
 * @throws IOFailure if settings cannot be loaded
 * @see #conditionalReload()
 */
public static synchronized void reload(){
  lastModified=0;
  List<File> settingsFiles=getSettingsFiles();
  List<SimpleXml> simpleXmlList=new ArrayList<SimpleXml>();
  for (  File settingsFile : settingsFiles) {
    if (settingsFile.isFile()) {
      simpleXmlList.add(new SimpleXml(settingsFile));
    }
 else {
      log.warn(""String_Node_Str"" + settingsFile.getAbsolutePath() + ""String_Node_Str"");
    }
    if (settingsFile.lastModified() > lastModified) {
      lastModified=settingsFile.lastModified();
    }
  }
  fileSettingsXmlList.addAll(simpleXmlList);
}","/** 
 * Reloads the settings. This will reload the settings from disk, and forget all settings that were set with   {@link #set}The field   {@link #lastModified} is updated to timestamp of the settingsfile that has been changed most recently.
 * @throws IOFailure if settings cannot be loaded
 * @see #conditionalReload()
 */
public static synchronized void reload(){
  lastModified=0;
  List<File> settingsFiles=getSettingsFiles();
  List<SimpleXml> simpleXmlList=new ArrayList<SimpleXml>();
  for (  File settingsFile : settingsFiles) {
    if (settingsFile.isFile()) {
      simpleXmlList.add(new SimpleXml(settingsFile));
    }
 else {
      log.warn(""String_Node_Str"" + settingsFile.getAbsolutePath() + ""String_Node_Str"");
    }
    if (settingsFile.lastModified() > lastModified) {
      lastModified=settingsFile.lastModified();
    }
  }
  fileSettingsXmlList.clear();
  fileSettingsXmlList.addAll(simpleXmlList);
}",0.9829201101928374
89880,"/** 
 * This method returns an instance of the UrlCanonicalizer class specified in the settings.xml for the dk.netarkivet.wayback module. In the event that reading this file generates a SecurityException, as may occur in batch operation if security does not allow System properties to be read, the method will fall back on returning an instance of the class dk.netarkivet.wayback.batch.copycode.NetarchiveSuiteAggressiveUrlCanonicalizer
 * @return a canonicalizer for urls
 */
public static UrlCanonicalizer getDefaultUrlCanonicalizer(){
  try {
    return SettingsFactory.getInstance(WaybackSettings.URL_CANONICALIZER_CLASSNAME);
  }
 catch (  SecurityException e) {
    return new NetarchiveSuiteAggressiveUrlCanonicalizer();
  }
}","/** 
 * This method returns an instance of the UrlCanonicalizer class specified in the settings.xml for the dk.netarkivet.wayback module. In the event that reading this file generates a SecurityException, as may occur in batch operation if security does not allow System properties to be read, the method will fall back on returning an instance of the class dk.netarkivet.wayback.batch.copycode.NetarchiveSuiteAggressiveUrlCanonicalizer
 * @return a canonicalizer for urls
 */
public static UrlCanonicalizer getDefaultUrlCanonicalizer(){
  try {
    return SettingsFactory.getInstance(WaybackSettings.URL_CANONICALIZER_CLASSNAME);
  }
 catch (  SecurityException e) {
    logger.debug(""String_Node_Str"" + ""String_Node_Str"" + NetarchiveSuiteAggressiveUrlCanonicalizer.class.toString(),e);
    return new NetarchiveSuiteAggressiveUrlCanonicalizer();
  }
}",0.9243379571248423
89881,"/** 
 * Monitors the crawling performed by Heritrix. Regularly checks whether any progress is made. If no progress has been made for too long, the crawl is ended.
 * @throws IOFailure if the call to HeritrixController.requestCrawlStop()fails. Other failures in calls to the controller are caught and logged.
 */
private void doCrawlLoop() throws IOFailure {
  String errorMessage=""String_Node_Str"";
  long lastNonZeroActiveQueuesTime=System.currentTimeMillis();
  long lastTimeReceivedData=System.currentTimeMillis();
  boolean crawlIsEnded=false;
  try {
    crawlIsEnded=heritrixController.crawlIsEnded();
  }
 catch (  IOFailure e) {
    log.warn(errorMessage,e);
  }
  while (!crawlIsEnded) {
    String harvestInformation=null;
    String progressStats=null;
    try {
      harvestInformation=heritrixController.getHarvestInformation();
      progressStats=heritrixController.getProgressStats();
    }
 catch (    IOFailure e) {
      log.warn(errorMessage,e);
    }
    log.info(""String_Node_Str"" + files.getJobID() + ""String_Node_Str""+ files.getHarvestID()+ ""String_Node_Str""+ harvestInformation+ ""String_Node_Str""+ ((progressStats == null) ? ""String_Node_Str"" : progressStats));
    int processedKBPerSec=0;
    boolean paused=false;
    try {
      processedKBPerSec=heritrixController.getCurrentProcessedKBPerSec();
      paused=heritrixController.isPaused();
    }
 catch (    IOFailure e) {
      log.warn(errorMessage,e);
    }
    if (processedKBPerSec > 0 || paused) {
      lastTimeReceivedData=System.currentTimeMillis();
    }
    int activeToeCount=0;
    paused=false;
    try {
      activeToeCount=heritrixController.getActiveToeCount();
      paused=heritrixController.isPaused();
    }
 catch (    IOFailure e) {
      log.warn(errorMessage,e);
    }
    if (activeToeCount > 0 || paused) {
      lastNonZeroActiveQueuesTime=System.currentTimeMillis();
    }
    if ((lastNonZeroActiveQueuesTime + timeOutInMillis < System.currentTimeMillis()) || (lastTimeReceivedData + timeOutInMillisReceivedData < System.currentTimeMillis())) {
      final double noActiveQueuesTimeoutInSeconds=timeOutInMillis / 1000.0;
      final double noDataReceivedTimeoutInSeconds=timeOutInMillisReceivedData / 1000.0;
      long queuedUriCount=0;
      try {
        queuedUriCount=heritrixController.getQueuedUriCount();
      }
 catch (      IOFailure e) {
        log.warn(errorMessage,e);
      }
      log.warn(""String_Node_Str"" + ""String_Node_Str"" + ((System.currentTimeMillis() - lastNonZeroActiveQueuesTime) / 1000.0) + ""String_Node_Str""+ noActiveQueuesTimeoutInSeconds+ ""String_Node_Str""+ ((System.currentTimeMillis() - lastTimeReceivedData) / 1000.0)+ ""String_Node_Str""+ noDataReceivedTimeoutInSeconds+ ""String_Node_Str""+ queuedUriCount);
      heritrixController.requestCrawlStop(""String_Node_Str"");
    }
    try {
      crawlIsEnded=heritrixController.crawlIsEnded();
    }
 catch (    IOFailure e) {
      log.warn(errorMessage,e);
    }
    if (!crawlIsEnded) {
      try {
synchronized (this) {
          wait(WAIT_PERIOD);
        }
      }
 catch (      InterruptedException e) {
        log.trace(""String_Node_Str"" + e.getMessage());
      }
    }
  }
}","/** 
 * Monitors the crawling performed by Heritrix. Regularly checks whether any progress is made. If no progress has been made for too long, the crawl is ended.
 * @throws IOFailure if the call to HeritrixController.requestCrawlStop()fails. Other failures in calls to the controller are caught and logged.
 */
private void doCrawlLoop() throws IOFailure {
  String errorMessage=""String_Node_Str"" + ""String_Node_Str"";
  long lastNonZeroActiveQueuesTime=System.currentTimeMillis();
  long lastTimeReceivedData=System.currentTimeMillis();
  boolean crawlIsEnded=false;
  try {
    crawlIsEnded=heritrixController.crawlIsEnded();
  }
 catch (  IOFailure e) {
    log.debug(errorMessage,e);
  }
  while (!crawlIsEnded) {
    String harvestInformation=null;
    String progressStats=null;
    try {
      harvestInformation=heritrixController.getHarvestInformation();
      progressStats=heritrixController.getProgressStats();
    }
 catch (    IOFailure e) {
      log.debug(errorMessage,e);
    }
    log.info(""String_Node_Str"" + files.getJobID() + ""String_Node_Str""+ files.getHarvestID()+ ""String_Node_Str""+ harvestInformation+ ""String_Node_Str""+ ((progressStats == null) ? ""String_Node_Str"" : progressStats));
    int processedKBPerSec=0;
    boolean paused=false;
    try {
      processedKBPerSec=heritrixController.getCurrentProcessedKBPerSec();
      paused=heritrixController.isPaused();
    }
 catch (    IOFailure e) {
      log.debug(errorMessage,e);
    }
    if (processedKBPerSec > 0 || paused) {
      lastTimeReceivedData=System.currentTimeMillis();
    }
    int activeToeCount=0;
    paused=false;
    try {
      activeToeCount=heritrixController.getActiveToeCount();
      paused=heritrixController.isPaused();
    }
 catch (    IOFailure e) {
      log.debug(errorMessage,e);
    }
    if (activeToeCount > 0 || paused) {
      lastNonZeroActiveQueuesTime=System.currentTimeMillis();
    }
    if ((lastNonZeroActiveQueuesTime + timeOutInMillis < System.currentTimeMillis()) || (lastTimeReceivedData + timeOutInMillisReceivedData < System.currentTimeMillis())) {
      final double noActiveQueuesTimeoutInSeconds=timeOutInMillis / 1000.0;
      final double noDataReceivedTimeoutInSeconds=timeOutInMillisReceivedData / 1000.0;
      long queuedUriCount=0;
      try {
        queuedUriCount=heritrixController.getQueuedUriCount();
      }
 catch (      IOFailure e) {
        log.debug(errorMessage,e);
      }
      log.warn(""String_Node_Str"" + ""String_Node_Str"" + ((System.currentTimeMillis() - lastNonZeroActiveQueuesTime) / 1000.0) + ""String_Node_Str""+ noActiveQueuesTimeoutInSeconds+ ""String_Node_Str""+ ((System.currentTimeMillis() - lastTimeReceivedData) / 1000.0)+ ""String_Node_Str""+ noDataReceivedTimeoutInSeconds+ ""String_Node_Str""+ queuedUriCount);
      heritrixController.requestCrawlStop(""String_Node_Str"");
    }
    try {
      crawlIsEnded=heritrixController.crawlIsEnded();
    }
 catch (    IOFailure e) {
      log.debug(errorMessage,e);
    }
    if (!crawlIsEnded) {
      try {
synchronized (this) {
          wait(WAIT_PERIOD);
        }
      }
 catch (      InterruptedException e) {
        log.trace(""String_Node_Str"" + e.getMessage());
      }
    }
  }
}",0.9883939774153074
89882,"/** 
 * This method launches heritrix in the following way:</br>  1. copies the orderfile and the seedsfile to current working directory. </br> 2. sets up the newly created copy of the orderfile </br> 3. starts the crawler </br> 4. stops the crawler (Either when heritrix has finished crawling, or when heritrix is forcefully stopped due to inactivity). </p> The exit from the while-loop depends on Heritrix calling the crawlEnded() method, when the crawling is finished. This method is called from the HarvestControllerServer.onDoOneCrawl() method.
 * @throws IOFailure - if the order.xml is invalid if unable to initializeHeritrix CrawlController if Heritrix process interrupted
 */
public void doCrawl() throws IOFailure {
  setupOrderfile();
  heritrixController=HeritrixControllerFactory.getDefaultHeritrixController(args);
  try {
    heritrixController.initialize();
    log.debug(""String_Node_Str"");
    heritrixController.requestCrawlStart();
    if (heritrixController.atFinish()) {
      heritrixController.beginCrawlStop();
    }
 else {
      doCrawlLoop();
    }
  }
 catch (  IOFailure e) {
    log.warn(""String_Node_Str"",e);
    throw (e);
  }
catch (  Exception e) {
    log.warn(""String_Node_Str"",e);
    throw new RuntimeException(""String_Node_Str"",e);
  }
 finally {
    if (heritrixController != null) {
      heritrixController.cleanup();
    }
  }
  log.debug(""String_Node_Str"");
}","/** 
 * This method launches heritrix in the following way:</br> 1. copies the orderfile and the seedsfile to current working directory. </br> 2. sets up the newly created copy of the orderfile </br> 3. starts the crawler </br> 4. stops the crawler (Either when heritrix has finished crawling, or when heritrix is forcefully stopped due to inactivity). </p> The exit from the while-loop depends on Heritrix calling the crawlEnded() method, when the crawling is finished. This method is called from the HarvestControllerServer.onDoOneCrawl() method.
 * @throws IOFailure - if the order.xml is invalid if unable to initializeHeritrix CrawlController if Heritrix process interrupted
 */
public void doCrawl() throws IOFailure {
  setupOrderfile();
  heritrixController=HeritrixControllerFactory.getDefaultHeritrixController(args);
  try {
    heritrixController.initialize();
    log.debug(""String_Node_Str"");
    heritrixController.requestCrawlStart();
    if (heritrixController.atFinish()) {
      heritrixController.beginCrawlStop();
    }
 else {
      doCrawlLoop();
    }
  }
 catch (  IOFailure e) {
    log.warn(""String_Node_Str"",e);
    throw (e);
  }
catch (  Exception e) {
    log.warn(""String_Node_Str"",e);
    throw new RuntimeException(""String_Node_Str"",e);
  }
 finally {
    if (heritrixController != null) {
      heritrixController.cleanup();
    }
  }
  log.debug(""String_Node_Str"");
}",0.9996437477734236
89883,"/** 
 * Get instance of this class.
 * @param files Object encapsulating location of Heritrix crawldir andconfiguration files
 * @return HeritrixLauncher object
 * @throws ArgumentNotValid If either order.xmlor seeds.txt does not exist, or argument files is null.
 */
public static HeritrixLauncher getInstance(HeritrixFiles files) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(files,""String_Node_Str"");
  return new HeritrixLauncher(files);
}","/** 
 * Get instance of this class.
 * @param files Object encapsulating location of Heritrix crawldir andconfiguration files
 * @return HeritrixLauncher object
 * @throws ArgumentNotValid If either order.xml or seeds.txt does not exist,or argument files is null.
 */
public static HeritrixLauncher getInstance(HeritrixFiles files) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNull(files,""String_Node_Str"");
  return new HeritrixLauncher(files);
}",0.9978021978021978
89884,"/** 
 * Gets a list of settings. First it is checked, if the key is registered as a System property. If yes, registered value is returned in a list of length 1. If no, the data loaded from the settings xml files are examined.  If value is there, it is returned in a list. If not, the default settings from classpath are examined. If values for this setting are found here, they are returned. Otherwise, an UnknownId exception is thrown. Note that the values will not be concatenated, the first place with a match will define the entire list. Furthemore the list cannot be empty.
 * @param key name of the setting to retrieve
 * @return the retrieved values (as a non-empty String array)
 * @throws ArgumentNotValid if key is null or the empty string
 * @throws UnknownID        if no setting loaded matches key
 */
public static String[] getAll(String key) throws UnknownID, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(key,""String_Node_Str"");
  log.debug(""String_Node_Str"" + key);
  String val=System.getProperty(key);
  if (val != null) {
    log.debug(""String_Node_Str"" + val);
    return new String[]{val};
  }
  if (fileSettingsXmlList.isEmpty()) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"");
  }
synchronized (fileSettingsXmlList) {
    for (    SimpleXml settingsXml : fileSettingsXmlList) {
      List<String> result=settingsXml.getList(key);
      if (result.size() == 0) {
        continue;
      }
      log.debug(""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",result));
      return result.toArray(new String[result.size()]);
    }
  }
synchronized (defaultClasspathSettingsXmlList) {
    for (    SimpleXml settingsXml : defaultClasspathSettingsXmlList) {
      List<String> result=settingsXml.getList(key);
      if (result.size() == 0) {
        continue;
      }
      log.debug(""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",result));
      return result.toArray(new String[result.size()]);
    }
  }
  throw new UnknownID(""String_Node_Str"" + key + ""String_Node_Str"");
}","/** 
 * Gets a list of settings. First it is checked, if the key is registered as a System property. If yes, registered value is returned in a list of length 1. If no, the data loaded from the settings xml files are examined. If value is there, it is returned in a list. If not, the default settings from classpath are examined. If values for this setting are found here, they are returned. Otherwise, an UnknownId exception is thrown. Note that the values will not be concatenated, the first place with a match will define the entire list. Furthemore the list cannot be empty.
 * @param key name of the setting to retrieve
 * @return the retrieved values (as a non-empty String array)
 * @throws ArgumentNotValid if key is null or the empty string
 * @throws UnknownID        if no setting loaded matches key
 */
public static String[] getAll(String key) throws UnknownID, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(key,""String_Node_Str"");
  log.debug(""String_Node_Str"" + key);
  String val=System.getProperty(key);
  if (val != null) {
    log.debug(""String_Node_Str"" + val);
    return new String[]{val};
  }
  if (fileSettingsXmlList.isEmpty()) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"");
  }
synchronized (fileSettingsXmlList) {
    for (    SimpleXml settingsXml : fileSettingsXmlList) {
      List<String> result=settingsXml.getList(key);
      if (result.size() == 0) {
        continue;
      }
      log.debug(""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",result));
      return result.toArray(new String[result.size()]);
    }
  }
synchronized (defaultClasspathSettingsXmlList) {
    for (    SimpleXml settingsXml : defaultClasspathSettingsXmlList) {
      List<String> result=settingsXml.getList(key);
      if (result.size() == 0) {
        continue;
      }
      log.debug(""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",result));
      return result.toArray(new String[result.size()]);
    }
  }
  throw new UnknownID(""String_Node_Str"" + key + ""String_Node_Str"");
}",0.9997537552327012
89885,"/** 
 * Sets the key to one or more values.  Calls to this method are forgotten whenever the   {@link #reload()} is executed.TODO write these values to its own simpleXml structure, that are not reset during reload.
 * @param key     The settings key to add this under, legal keys arefields in this class.
 * @param values  The (ordered) list of values to put under this key.
 * @throws ArgumentNotValid if key or values are null
 * @throws UnknownID        if the key does not already exist
 */
public static void set(String key,String... values){
  ArgumentNotValid.checkNotNullOrEmpty(key,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(values,""String_Node_Str"");
  if (fileSettingsXmlList.isEmpty()) {
    fileSettingsXmlList.add(new SimpleXml(""String_Node_Str""));
  }
  SimpleXml simpleXml=fileSettingsXmlList.get(0);
  if (simpleXml.hasKey(key)) {
    simpleXml.update(key,values);
  }
 else {
    simpleXml.add(key,values);
  }
}","/** 
 * Sets the key to one or more values. Calls to this method are forgotten whenever the   {@link #reload()} is executed.TODO write these values to its own simpleXml structure, that are not reset during reload.
 * @param key    The settings key to add this under, legal keys are fieldsin this class.
 * @param values The (ordered) list of values to put under this key.
 * @throws ArgumentNotValid if key or values are null
 * @throws UnknownID        if the key does not already exist
 */
public static void set(String key,String... values){
  ArgumentNotValid.checkNotNullOrEmpty(key,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(values,""String_Node_Str"");
  if (fileSettingsXmlList.isEmpty()) {
    fileSettingsXmlList.add(new SimpleXml(""String_Node_Str""));
  }
  SimpleXml simpleXml=fileSettingsXmlList.get(0);
  if (simpleXml.hasKey(key)) {
    simpleXml.update(key,values);
  }
 else {
    simpleXml.add(key,values);
  }
}",0.9973333333333332
89886,"/** 
 * Reloads the settings. This will reload the settings from disk, and forget all settings that were set with   {@link #set}The field   {@link #lastModified} is updated to timestampof the settings file that has been changed most recently.
 * @throws IOFailure if settings cannot be loaded
 * @see #conditionalReload()
 */
public static synchronized void reload(){
  lastModified=0;
  List<File> settingsFiles=getSettingsFiles();
  List<SimpleXml> simpleXmlList=new ArrayList<SimpleXml>();
  for (  File settingsFile : settingsFiles) {
    if (settingsFile.isFile()) {
      simpleXmlList.add(new SimpleXml(settingsFile));
    }
 else {
      log.warn(""String_Node_Str"" + settingsFile.getAbsolutePath() + ""String_Node_Str"");
    }
    if (settingsFile.lastModified() > lastModified) {
      lastModified=settingsFile.lastModified();
    }
  }
  fileSettingsXmlList=Collections.synchronizedList(simpleXmlList);
}","/** 
 * Reloads the settings. This will reload the settings from disk, and forget all settings that were set with   {@link #set}The field   {@link #lastModified} is updated to timestamp of the settingsfile that has been changed most recently.
 * @throws IOFailure if settings cannot be loaded
 * @see #conditionalReload()
 */
public static synchronized void reload(){
  lastModified=0;
  List<File> settingsFiles=getSettingsFiles();
  List<SimpleXml> simpleXmlList=new ArrayList<SimpleXml>();
  for (  File settingsFile : settingsFiles) {
    if (settingsFile.isFile()) {
      simpleXmlList.add(new SimpleXml(settingsFile));
    }
 else {
      log.warn(""String_Node_Str"" + settingsFile.getAbsolutePath() + ""String_Node_Str"");
    }
    if (settingsFile.lastModified() > lastModified) {
      lastModified=settingsFile.lastModified();
    }
  }
  fileSettingsXmlList.addAll(simpleXmlList);
}",0.9523809523809524
89887,"/** 
 * Add the settings file represented by this path to the list of default classpath settings.
 * @param defaultClasspathSettingsPath the given default classpath setting.
 */
public static void addDefaultClasspathSettings(String defaultClasspathSettingsPath){
  ArgumentNotValid.checkNotNullOrEmpty(defaultClasspathSettingsPath,""String_Node_Str"");
  InputStream stream=Thread.currentThread().getContextClassLoader().getResourceAsStream(defaultClasspathSettingsPath);
  if (stream != null) {
    if (defaultClasspathSettingsXmlList == null) {
      defaultClasspathSettingsXmlList=Collections.synchronizedList(new ArrayList<SimpleXml>());
    }
    defaultClasspathSettingsXmlList.add(new SimpleXml(stream));
  }
 else {
    log.warn(""String_Node_Str"" + defaultClasspathSettingsPath + ""String_Node_Str"");
  }
}","/** 
 * Add the settings file represented by this path to the list of default classpath settings.
 * @param defaultClasspathSettingsPath the given default classpath setting.
 */
public static void addDefaultClasspathSettings(String defaultClasspathSettingsPath){
  ArgumentNotValid.checkNotNullOrEmpty(defaultClasspathSettingsPath,""String_Node_Str"");
  InputStream stream=Thread.currentThread().getContextClassLoader().getResourceAsStream(defaultClasspathSettingsPath);
  if (stream != null) {
    defaultClasspathSettingsXmlList.add(new SimpleXml(stream));
  }
 else {
    log.warn(""String_Node_Str"" + defaultClasspathSettingsPath + ""String_Node_Str"");
  }
}",0.8959891230455472
89888,"/** 
 * Gets a setting. The search order for a given setting is as follows: First it is checked, if the argument key is set as a System property. If yes, return this value. If no, we continue the search. Secondly, we check, if the setting is in one of the loaded settings xml files. If the value is there, it is returned. If no, we continue the search. Finally, we check if the setting is in one of default settings  files from classpath. If the value is there, it is returned. Otherwise an UnknownId exception is thrown. Note: The retrieved value can be the empty string
 * @param key name of the setting to retrieve
 * @return the retrieved value
 * @throws ArgumentNotValid if key is null or the empty string
 * @throws UnknownID        if no setting loaded matches key
 * @throws IOFailure        if IO Failure
 */
public static String get(String key) throws UnknownID, IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(key,""String_Node_Str"");
  String val=System.getProperty(key);
  if (val != null) {
    return val;
  }
synchronized (fileSettingsXmlList) {
    for (    SimpleXml settingsXml : fileSettingsXmlList) {
      if (settingsXml.hasKey(key)) {
        return settingsXml.getString(key);
      }
    }
  }
synchronized (defaultClasspathSettingsXmlList) {
    for (    SimpleXml settingsXml : defaultClasspathSettingsXmlList) {
      if (settingsXml.hasKey(key)) {
        return settingsXml.getString(key);
      }
    }
  }
  throw new UnknownID(""String_Node_Str"" + key + ""String_Node_Str"");
}","/** 
 * Gets a setting. The search order for a given setting is as follows: First it is checked, if the argument key is set as a System property. If yes, return this value. If no, we continue the search. Secondly, we check, if the setting is in one of the loaded settings xml files. If the value is there, it is returned. If no, we continue the search. Finally, we check if the setting is in one of default settings files from classpath. If the value is there, it is returned. Otherwise an UnknownId exception is thrown. Note: The retrieved value can be the empty string
 * @param key name of the setting to retrieve
 * @return the retrieved value
 * @throws ArgumentNotValid if key is null or the empty string
 * @throws UnknownID        if no setting loaded matches key
 * @throws IOFailure        if IO Failure
 */
public static String get(String key) throws UnknownID, IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(key,""String_Node_Str"");
  String val=System.getProperty(key);
  if (val != null) {
    return val;
  }
synchronized (fileSettingsXmlList) {
    for (    SimpleXml settingsXml : fileSettingsXmlList) {
      if (settingsXml.hasKey(key)) {
        return settingsXml.getString(key);
      }
    }
  }
synchronized (defaultClasspathSettingsXmlList) {
    for (    SimpleXml settingsXml : defaultClasspathSettingsXmlList) {
      if (settingsXml.hasKey(key)) {
        return settingsXml.getString(key);
      }
    }
  }
  throw new UnknownID(""String_Node_Str"" + key + ""String_Node_Str"");
}",0.9996728819103696
89889,"/** 
 * Monitors the crawling performed by Heritrix. Regularly checks whether any progress is made. If no progress has been made for too long, the crawl is ended.
 * @throws IOFailure if the call to HeritrixController.requestCrawlStop()fails. Other failures in calls to the controller are caught and logged.
 */
private void doCrawlLoop() throws IOFailure {
  String errorMessage=""String_Node_Str"";
  long lastNonZeroActiveQueuesTime=System.currentTimeMillis();
  long lastTimeReceivedData=System.currentTimeMillis();
  boolean crawlIsEnded=false;
  try {
    crawlIsEnded=heritrixController.crawlIsEnded();
  }
 catch (  IOFailure e) {
    log.warn(errorMessage,e);
  }
  while (!crawlIsEnded) {
    String harvestInformation=null;
    String progressStats=null;
    try {
      harvestInformation=heritrixController.getHarvestInformation();
      progressStats=heritrixController.getProgressStats();
    }
 catch (    IOFailure e) {
      log.warn(errorMessage,e);
    }
    log.info(""String_Node_Str"" + files.getJobID() + ""String_Node_Str""+ files.getHarvestID()+ ""String_Node_Str""+ harvestInformation+ ""String_Node_Str""+ progressStats);
    int processedKBPerSec=0;
    boolean paused=false;
    try {
      processedKBPerSec=heritrixController.getCurrentProcessedKBPerSec();
      paused=heritrixController.isPaused();
    }
 catch (    IOFailure e) {
      log.warn(errorMessage,e);
    }
    if (processedKBPerSec > 0 || paused) {
      lastTimeReceivedData=System.currentTimeMillis();
    }
    int activeToeCount=0;
    paused=false;
    try {
      activeToeCount=heritrixController.getActiveToeCount();
      paused=heritrixController.isPaused();
    }
 catch (    IOFailure e) {
      log.warn(errorMessage,e);
    }
    if (activeToeCount > 0 || paused) {
      lastNonZeroActiveQueuesTime=System.currentTimeMillis();
    }
    if ((lastNonZeroActiveQueuesTime + timeOutInMillis < System.currentTimeMillis()) || (lastTimeReceivedData + timeOutInMillisReceivedData < System.currentTimeMillis())) {
      final double noActiveQueuesTimeoutInSeconds=timeOutInMillis / 1000.0;
      final double noDataReceivedTimeoutInSeconds=timeOutInMillisReceivedData / 1000.0;
      long queuedUriCount=0;
      try {
        queuedUriCount=heritrixController.getQueuedUriCount();
      }
 catch (      IOFailure e) {
        log.warn(errorMessage,e);
      }
      log.warn(""String_Node_Str"" + ""String_Node_Str"" + ((System.currentTimeMillis() - lastNonZeroActiveQueuesTime) / 1000.0) + ""String_Node_Str""+ noActiveQueuesTimeoutInSeconds+ ""String_Node_Str""+ ((System.currentTimeMillis() - lastTimeReceivedData) / 1000.0)+ ""String_Node_Str""+ noDataReceivedTimeoutInSeconds+ ""String_Node_Str""+ queuedUriCount);
      heritrixController.requestCrawlStop(""String_Node_Str"");
    }
    try {
      crawlIsEnded=heritrixController.crawlIsEnded();
    }
 catch (    IOFailure e) {
      log.warn(errorMessage,e);
    }
    if (!crawlIsEnded) {
      try {
synchronized (this) {
          wait(WAIT_PERIOD);
        }
      }
 catch (      InterruptedException e) {
        log.trace(""String_Node_Str"" + e.getMessage());
      }
    }
  }
}","/** 
 * Monitors the crawling performed by Heritrix. Regularly checks whether any progress is made. If no progress has been made for too long, the crawl is ended.
 * @throws IOFailure if the call to HeritrixController.requestCrawlStop()fails. Other failures in calls to the controller are caught and logged.
 */
private void doCrawlLoop() throws IOFailure {
  String errorMessage=""String_Node_Str"";
  long lastNonZeroActiveQueuesTime=System.currentTimeMillis();
  long lastTimeReceivedData=System.currentTimeMillis();
  boolean crawlIsEnded=false;
  try {
    crawlIsEnded=heritrixController.crawlIsEnded();
  }
 catch (  IOFailure e) {
    log.warn(errorMessage,e);
  }
  while (!crawlIsEnded) {
    String harvestInformation=null;
    String progressStats=null;
    try {
      harvestInformation=heritrixController.getHarvestInformation();
      progressStats=heritrixController.getProgressStats();
    }
 catch (    IOFailure e) {
      log.warn(errorMessage,e);
    }
    log.info(""String_Node_Str"" + files.getJobID() + ""String_Node_Str""+ files.getHarvestID()+ ""String_Node_Str""+ harvestInformation+ ""String_Node_Str""+ ((progressStats == null) ? ""String_Node_Str"" : progressStats));
    int processedKBPerSec=0;
    boolean paused=false;
    try {
      processedKBPerSec=heritrixController.getCurrentProcessedKBPerSec();
      paused=heritrixController.isPaused();
    }
 catch (    IOFailure e) {
      log.warn(errorMessage,e);
    }
    if (processedKBPerSec > 0 || paused) {
      lastTimeReceivedData=System.currentTimeMillis();
    }
    int activeToeCount=0;
    paused=false;
    try {
      activeToeCount=heritrixController.getActiveToeCount();
      paused=heritrixController.isPaused();
    }
 catch (    IOFailure e) {
      log.warn(errorMessage,e);
    }
    if (activeToeCount > 0 || paused) {
      lastNonZeroActiveQueuesTime=System.currentTimeMillis();
    }
    if ((lastNonZeroActiveQueuesTime + timeOutInMillis < System.currentTimeMillis()) || (lastTimeReceivedData + timeOutInMillisReceivedData < System.currentTimeMillis())) {
      final double noActiveQueuesTimeoutInSeconds=timeOutInMillis / 1000.0;
      final double noDataReceivedTimeoutInSeconds=timeOutInMillisReceivedData / 1000.0;
      long queuedUriCount=0;
      try {
        queuedUriCount=heritrixController.getQueuedUriCount();
      }
 catch (      IOFailure e) {
        log.warn(errorMessage,e);
      }
      log.warn(""String_Node_Str"" + ""String_Node_Str"" + ((System.currentTimeMillis() - lastNonZeroActiveQueuesTime) / 1000.0) + ""String_Node_Str""+ noActiveQueuesTimeoutInSeconds+ ""String_Node_Str""+ ((System.currentTimeMillis() - lastTimeReceivedData) / 1000.0)+ ""String_Node_Str""+ noDataReceivedTimeoutInSeconds+ ""String_Node_Str""+ queuedUriCount);
      heritrixController.requestCrawlStop(""String_Node_Str"");
    }
    try {
      crawlIsEnded=heritrixController.crawlIsEnded();
    }
 catch (    IOFailure e) {
      log.warn(errorMessage,e);
    }
    if (!crawlIsEnded) {
      try {
synchronized (this) {
          wait(WAIT_PERIOD);
        }
      }
 catch (      InterruptedException e) {
        log.trace(""String_Node_Str"" + e.getMessage());
      }
    }
  }
}",0.9923833703586163
89890,"/** 
 * The constructor of Channels class.  Validates that the current value of the setting USE_REPLICA_ID corresponds to one of the replicas listed in the settings. Furthermore we here fill content in the ALL_BA_ARRAY, ANY_BA_ARRAY, THE_BAMON_ARRAY, and initialize ALL_BA, ANY_BA, and THE_BAMON.
 */
private Channels(){
  if (indexOfUseReplicaId < 0) {
    throw new ArgumentNotValid(""String_Node_Str"" + ""String_Node_Str"" + useReplicaId + ""String_Node_Str"");
  }
  for (int i=0; i < allReplicaIds.length; i++) {
    if (allReplicaTypes[i].equals(ReplicaType.BITARCHIVE_REPLICATYPE_AS_STRING)) {
      ALL_BA_ARRAY[i]=new ChannelID(ALLBA_CHANNEL_PREFIX,allReplicaIds[i],ChannelID.NO_IP,ChannelID.NO_APPLINST_ID,ChannelID.TOPIC);
    }
 else {
      ALL_BA_ARRAY[i]=null;
    }
  }
  ALL_BA=ALL_BA_ARRAY[indexOfUseReplicaId];
  for (int i=0; i < allReplicaIds.length; i++) {
    if (allReplicaTypes[i].equals(ReplicaType.BITARCHIVE_REPLICATYPE_AS_STRING)) {
      ANY_BA_ARRAY[i]=new ChannelID(ANYBA_CHANNEL_PREFIX,allReplicaIds[i],ChannelID.NO_IP,ChannelID.NO_APPLINST_ID,ChannelID.QUEUE);
    }
 else {
      ANY_BA_ARRAY[i]=null;
    }
  }
  ANY_BA=ANY_BA_ARRAY[indexOfUseReplicaId];
  for (int i=0; i < allReplicaIds.length; i++) {
    if (allReplicaTypes[i].equals(ReplicaType.BITARCHIVE_REPLICATYPE_AS_STRING)) {
      THE_BAMON_ARRAY[i]=new ChannelID(THEBAMON_CHANNEL_PREFIX,allReplicaIds[i],ChannelID.NO_IP,ChannelID.NO_APPLINST_ID,ChannelID.QUEUE);
    }
 else {
      THE_BAMON_ARRAY[i]=null;
    }
  }
  THE_BAMON=THE_BAMON_ARRAY[indexOfUseReplicaId];
  for (int i=0; i < allReplicaIds.length; i++) {
    if (allReplicaTypes[i].equals(ReplicaType.CHECKSUM_REPLICATYPE_AS_STRING)) {
      THE_CR_ARRAY[i]=new ChannelID(THECR_CHANNEL_PREFIX,allReplicaIds[i],ChannelID.NO_IP,ChannelID.NO_APPLINST_ID,ChannelID.QUEUE);
    }
 else {
      THE_CR_ARRAY[i]=null;
    }
  }
  THE_CR=THE_CR_ARRAY[indexOfUseReplicaId];
}","/** 
 * The constructor of Channels class.  Validates that the current value of the setting USE_REPLICA_ID corresponds to one of the replicas listed in the settings. Furthermore we here fill content in the ALL_BA_ARRAY, ANY_BA_ARRAY, THE_BAMON_ARRAY, and initialize ALL_BA, ANY_BA, and THE_BAMON.
 */
private Channels(){
  int i=0;
  int useReplicaIndex=-1;
  for (  Replica rep : replicas) {
    if (rep.getType() == ReplicaType.BITARCHIVE) {
      ALL_BA_ARRAY[i]=new ChannelID(ALLBA_CHANNEL_PREFIX,rep.getId(),ChannelID.NO_IP,ChannelID.NO_APPLINST_ID,ChannelID.TOPIC);
      ANY_BA_ARRAY[i]=new ChannelID(ANYBA_CHANNEL_PREFIX,rep.getId(),ChannelID.NO_IP,ChannelID.NO_APPLINST_ID,ChannelID.QUEUE);
      THE_BAMON_ARRAY[i]=new ChannelID(THEBAMON_CHANNEL_PREFIX,rep.getId(),ChannelID.NO_IP,ChannelID.NO_APPLINST_ID,ChannelID.QUEUE);
      THE_CR_ARRAY[i]=null;
    }
 else     if (rep.getType() == ReplicaType.CHECKSUM) {
      ALL_BA_ARRAY[i]=null;
      ANY_BA_ARRAY[i]=null;
      THE_BAMON_ARRAY[i]=null;
      THE_CR_ARRAY[i]=new ChannelID(THECR_CHANNEL_PREFIX,rep.getId(),ChannelID.NO_IP,ChannelID.NO_APPLINST_ID,ChannelID.QUEUE);
    }
 else {
      throw new UnknownID(""String_Node_Str"" + rep + ""String_Node_Str""+ ""String_Node_Str"");
    }
    if (rep == useReplica) {
      useReplicaIndex=i;
    }
    i++;
  }
  if (useReplicaIndex < 0 || useReplicaIndex >= replicas.size()) {
    throw new ArgumentNotValid(""String_Node_Str"" + useReplica + ""String_Node_Str""+ ""String_Node_Str""+ replicas+ ""String_Node_Str"");
  }
  ALL_BA=ALL_BA_ARRAY[useReplicaIndex];
  ANY_BA=ANY_BA_ARRAY[useReplicaIndex];
  THE_BAMON=THE_BAMON_ARRAY[useReplicaIndex];
  THE_CR=THE_CR_ARRAY[useReplicaIndex];
}",0.2102933038184836
89891,"/** 
 * Returns the queue for sending messages to bitarchive monitors.
 * @return the <code>ChannelID</code> object for this queue.
 * @throws IllegalState If the current replica is not a checksum replica.
 */
public static ChannelID getTheBamon() throws IllegalState {
  ChannelID res=getInstance().THE_BAMON;
  if (res == null) {
    throw new IllegalState(""String_Node_Str"" + ""String_Node_Str"" + Replica.getReplicaFromId(getInstance().allReplicaIds[getInstance().indexOfUseReplicaId]) + ""String_Node_Str"");
  }
  return res;
}","/** 
 * Returns the queue for sending messages to bitarchive monitors.
 * @return the <code>ChannelID</code> object for this queue.
 * @throws IllegalState If the current replica is not a checksum replica.
 */
public static ChannelID getTheBamon() throws IllegalState {
  ChannelID res=getInstance().THE_BAMON;
  if (res == null) {
    throw new IllegalState(""String_Node_Str"" + ""String_Node_Str"" + getInstance().useReplica + ""String_Node_Str"");
  }
  return res;
}",0.9295774647887324
89892,"/** 
 * Retrieves the names of all the files in the replica through a  GetAllFilenamesMessage. This is the checksum archive alternative to running a FilelistBatchJob. 
 * @param replicaId The id of the replica from which the list of filenamesshould be retrieved.
 * @return A list of all the filenames within the archive of the given replica.
 * @see dk.netarkivet.archive.checksum.distribute.GetAllFilenamesMessage
 */
public File getAllFilenames(String replicaId){
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  log.debug(""String_Node_Str"" + replicaId + ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetAllFilenamesMessage gafMsg=new GetAllFilenamesMessage(Channels.getTheRepos(),replyQ,replicaId);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(gafMsg,getTimeout);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / 1000) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    log.info(""String_Node_Str"" + (getTimeout / 1000) + ""String_Node_Str"");
    return null;
  }
  GetAllFilenamesMessage replyCSMsg;
  try {
    replyCSMsg=(GetAllFilenamesMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    String errorMsg=""String_Node_Str"" + replyNetMsg;
    log.warn(errorMsg,e);
    throw new IOFailure(errorMsg,e);
  }
  try {
    File result=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    replyCSMsg.getData(result);
    return result;
  }
 catch (  IOException e) {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + replyCSMsg;
    log.warn(errMsg);
    throw new IOFailure(errMsg,e);
  }
}","/** 
 * Retrieves the names of all the files in the replica through a GetAllFilenamesMessage. This is the checksum archive alternative to running a FilelistBatchJob.
 * @param replicaId The id of the replica from which the list of filenamesshould be retrieved.
 * @return A list of all the filenames within the archive of the givenreplica.
 * @see dk.netarkivet.archive.checksum.distribute.GetAllFilenamesMessage
 */
public File getAllFilenames(String replicaId){
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  log.debug(""String_Node_Str"" + replicaId + ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetAllFilenamesMessage gafMsg=new GetAllFilenamesMessage(Channels.getTheRepos(),replyQ,replicaId);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(gafMsg,getTimeout);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / 1000) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    log.info(""String_Node_Str"" + (getTimeout / 1000) + ""String_Node_Str"");
    return null;
  }
  GetAllFilenamesMessage replyCSMsg;
  try {
    replyCSMsg=(GetAllFilenamesMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    String errorMsg=""String_Node_Str"" + replyNetMsg;
    log.warn(errorMsg,e);
    throw new IOFailure(errorMsg,e);
  }
  try {
    File result=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    replyCSMsg.getData(result);
    return result;
  }
 catch (  IOException e) {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + replyCSMsg;
    log.warn(errMsg);
    throw new IOFailure(errMsg,e);
  }
}",0.9990737882062364
89893,"/** 
 * Removes a file from the bitarchives, if given credentials and checksum are correct.
 * @param fileName The name of the file to delete
 * @param bitarchiveId The id of the bitarchive to delete the file in
 * @param checksum The checksum of the deleted file
 * @param credentials The credentials used to delete the file
 * @throws ArgumentNotValid if arguments are null orequal to the empty string
 * @throws IOFailure if we could not delete the remote file, orsthere was no response to our RemoveAndGetFileMessage within the allotted time defined by the setting  {@link JMSArcRepositoryClient#ARCREPOSITORY_STORE_TIMEOUT}.
 * @return The file that was removed
 */
public File removeAndGetFile(String fileName,String bitarchiveId,String checksum,String credentials){
  ArgumentNotValid.checkNotNullOrEmpty(fileName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(bitarchiveId,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(checksum,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(credentials,""String_Node_Str"");
  String msg=""String_Node_Str"" + fileName + ""String_Node_Str""+ checksum+ ""String_Node_Str""+ bitarchiveId+ ""String_Node_Str"";
  log.warn(msg);
  NotificationsFactory.getInstance().errorEvent(msg);
  RemoveAndGetFileMessage aMsg=new RemoveAndGetFileMessage(fileName,bitarchiveId,checksum,credentials);
  RemoveAndGetFileMessage replyMsg=(RemoveAndGetFileMessage)sendAndWaitForOneReply(aMsg,storeTimeout);
  if (replyMsg != null) {
    if (replyMsg.isOk()) {
      File removedFile=replyMsg.getData();
      log.debug(""String_Node_Str"" + fileName + ""String_Node_Str""+ removedFile.getAbsolutePath());
      return removedFile;
    }
 else {
      String message=""String_Node_Str"" + replyMsg.getErrMsg();
      log.warn(message);
      throw new IOFailure(message);
    }
  }
 else {
    String message=""String_Node_Str"" + ""String_Node_Str"" + fileName + ""String_Node_Str""+ bitarchiveId+ ""String_Node_Str"";
    log.warn(message);
    throw new IOFailure(message);
  }
}","/** 
 * Removes a file from the bitarchives, if given credentials and checksum are correct.
 * @param fileName     The name of the file to delete
 * @param bitarchiveId The id of the bitarchive to delete the file in
 * @param checksum     The checksum of the deleted file
 * @param credentials  The credentials used to delete the file
 * @return The file that was removed
 * @throws ArgumentNotValid if arguments are null or equal to the emptystring
 * @throws IOFailure        if we could not delete the remote file, orsthere was no response to our RemoveAndGetFileMessage within the allotted time defined by the setting {@link JMSArcRepositoryClient#ARCREPOSITORY_STORE_TIMEOUT}.
 */
public File removeAndGetFile(String fileName,String bitarchiveId,String checksum,String credentials){
  ArgumentNotValid.checkNotNullOrEmpty(fileName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(bitarchiveId,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(checksum,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(credentials,""String_Node_Str"");
  String msg=""String_Node_Str"" + fileName + ""String_Node_Str""+ checksum+ ""String_Node_Str""+ bitarchiveId+ ""String_Node_Str"";
  log.warn(msg);
  NotificationsFactory.getInstance().errorEvent(msg);
  RemoveAndGetFileMessage aMsg=new RemoveAndGetFileMessage(fileName,bitarchiveId,checksum,credentials);
  RemoveAndGetFileMessage replyMsg=(RemoveAndGetFileMessage)sendAndWaitForOneReply(aMsg,storeTimeout);
  if (replyMsg != null) {
    if (replyMsg.isOk()) {
      File removedFile=replyMsg.getData();
      log.debug(""String_Node_Str"" + fileName + ""String_Node_Str""+ removedFile.getAbsolutePath());
      return removedFile;
    }
 else {
      String message=""String_Node_Str"" + replyMsg.getErrMsg();
      log.warn(message);
      throw new IOFailure(message);
    }
  }
 else {
    String message=""String_Node_Str"" + ""String_Node_Str"" + fileName + ""String_Node_Str""+ bitarchiveId+ ""String_Node_Str"";
    log.warn(message);
    throw new IOFailure(message);
  }
}",0.9750309023485784
89894,"@Override public void correct(String replicaId,String checksum,File file,String credentials){
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(checksum,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(file,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(credentials,""String_Node_Str"");
  RemoteFile rm=RemoteFileFactory.getCopyfileInstance(file);
  CorrectMessage correctMsg=new CorrectMessage(Channels.getTheRepos(),replyQ,checksum,rm);
  CorrectMessage responseMessage=(CorrectMessage)sendAndWaitForOneReply(correctMsg,0);
  if (responseMessage == null) {
    String msg=""String_Node_Str"" + ""String_Node_Str"";
    log.debug(msg);
    throw new IOFailure(msg);
  }
 else   if (!responseMessage.isOk()) {
    String msg=""String_Node_Str"" + responseMessage.getErrMsg();
    log.warn(msg);
    throw new IOFailure(msg);
  }
}","public void correct(String replicaId,String checksum,File file,String credentials){
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(checksum,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(file,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(credentials,""String_Node_Str"");
  RemoteFile rm=RemoteFileFactory.getCopyfileInstance(file);
  CorrectMessage correctMsg=new CorrectMessage(Channels.getTheRepos(),replyQ,checksum,rm);
  CorrectMessage responseMessage=(CorrectMessage)sendAndWaitForOneReply(correctMsg,0);
  if (responseMessage == null) {
    String msg=""String_Node_Str"" + ""String_Node_Str"";
    log.debug(msg);
    throw new IOFailure(msg);
  }
 else   if (!responseMessage.isOk()) {
    String msg=""String_Node_Str"" + responseMessage.getErrMsg();
    log.warn(msg);
    throw new IOFailure(msg);
  }
}",0.9943883277216612
89895,"/** 
 * Sends a BatchMessage to the Arcrepos queue and waits for the BatchReplyMessage reply before returning.
 * @param job        An object that implements the FileBatchJob interface.The initialize() method will be called before processing and the finish() method will be called afterwards.  The process() method will be called with each File entry.
 * @param replicaId The id of the archive to execute the job on
 * @return           A local batch status
 * @throws IOFailure if no results can be read at all
 */
public BatchStatus batch(FileBatchJob job,String replicaId){
  ArgumentNotValid.checkNotNull(job,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  log.debug(""String_Node_Str"" + job + ""String_Node_Str""+ replicaId+ ""String_Node_Str"");
  BatchMessage bMsg=new BatchMessage(Channels.getTheRepos(),replyQ,job,replicaId);
  log.debug(""String_Node_Str"" + Channels.getTheRepos() + ""String_Node_Str""+ replyQ+ ""String_Node_Str"");
  BatchReplyMessage brMsg=(BatchReplyMessage)sendAndWaitForOneReply(bMsg,0);
  if (!brMsg.isOk()) {
    String msg=""String_Node_Str"" + bMsg + ""String_Node_Str""+ ""String_Node_Str""+ brMsg.getErrMsg();
    log.warn(msg);
    if (brMsg.getResultFile() == null) {
      throw new IOFailure(msg);
    }
  }
  return new BatchStatus(brMsg.getFilesFailed(),brMsg.getNoOfFilesProcessed(),brMsg.getResultFile(),job.getExceptions());
}","/** 
 * Sends a BatchMessage to the Arcrepos queue and waits for the BatchReplyMessage reply before returning.
 * @param job       An object that implements the FileBatchJob interface.The initialize() method will be called before processing and the finish() method will be called afterwards.  The process() method will be called with each File entry.
 * @param replicaId The id of the archive to execute the job on
 * @return A local batch status
 * @throws IOFailure if no results can be read at all
 */
public BatchStatus batch(FileBatchJob job,String replicaId){
  ArgumentNotValid.checkNotNull(job,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  log.debug(""String_Node_Str"" + job + ""String_Node_Str""+ replicaId+ ""String_Node_Str"");
  BatchMessage bMsg=new BatchMessage(Channels.getTheRepos(),replyQ,job,replicaId);
  log.debug(""String_Node_Str"" + Channels.getTheRepos() + ""String_Node_Str""+ replyQ+ ""String_Node_Str"");
  BatchReplyMessage brMsg=(BatchReplyMessage)sendAndWaitForOneReply(bMsg,0);
  if (!brMsg.isOk()) {
    String msg=""String_Node_Str"" + bMsg + ""String_Node_Str""+ ""String_Node_Str""+ brMsg.getErrMsg();
    log.warn(msg);
    if (brMsg.getResultFile() == null) {
      throw new IOFailure(msg);
    }
  }
  return new BatchStatus(brMsg.getFilesFailed(),brMsg.getNoOfFilesProcessed(),brMsg.getResultFile(),job.getExceptions());
}",0.9960587603009674
89896,"/** 
 * Sends a StoreMessage via the synchronized JMS connection method sendAndWaitForOneReply(). After a successful storage operation, both the local copy of the file and the copy on the ftp server are deleted.
 * @param file A file to be stored. Must exist.
 * @throws IOFailure thrown if store is unsuccessful, or failed to cleanup files locally or on the ftp server after the store operation.
 * @throws ArgumentNotValid if file parameter is null or file is not anexisting file.
 */
public void store(File file) throws IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNull(file,""String_Node_Str"");
  ArgumentNotValid.checkTrue(file.isFile(),""String_Node_Str"" + file.getPath() + ""String_Node_Str""+ ""String_Node_Str"");
  StringBuilder messages=new StringBuilder();
  for (long i=0; i < storeRetries; i++) {
    StoreMessage outMsg=null;
    try {
      log.debug(""String_Node_Str"" + file.getPath() + ""String_Node_Str"");
      outMsg=new StoreMessage(replyQ,file);
      NetarkivetMessage replyMsg=sendAndWaitForOneReply(outMsg,storeTimeout);
      if (replyMsg != null && replyMsg.isOk()) {
        try {
          FileUtils.removeRecursively(file);
        }
 catch (        IOFailure e) {
          log.warn(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"",e);
        }
        return;
      }
 else       if (replyMsg == null) {
        String msg=""String_Node_Str"" + ""String_Node_Str"" + file.getPath() + ""String_Node_Str""+ (i + 1)+ ""String_Node_Str""+ storeRetries;
        log.warn(msg);
        messages.append(msg).append(""String_Node_Str"");
      }
 else {
        String msg=""String_Node_Str"" + replyMsg + ""String_Node_Str""+ ""String_Node_Str""+ file.getPath()+ ""String_Node_Str""+ (i + 1)+ ""String_Node_Str""+ storeRetries+ ""String_Node_Str""+ replyMsg.getErrMsg()+ ""String_Node_Str"";
        log.warn(msg);
        messages.append(msg).append(""String_Node_Str"");
      }
    }
 catch (    NetarkivetException e) {
      String msg=""String_Node_Str"" + file.getPath() + ""String_Node_Str""+ (i + 1)+ ""String_Node_Str""+ storeRetries+ ""String_Node_Str"";
      log.warn(msg,e);
      messages.append(msg).append(""String_Node_Str"");
      messages.append(ExceptionUtils.getStackTrace(e));
    }
 finally {
      if (outMsg != null) {
        cleanUpAfterStore(outMsg);
      }
    }
  }
  String errMsg=""String_Node_Str"" + file.getPath() + ""String_Node_Str""+ storeRetries+ ""String_Node_Str""+ messages;
  log.warn(errMsg);
  NotificationsFactory.getInstance().errorEvent(errMsg);
  throw new IOFailure(errMsg);
}","/** 
 * Sends a StoreMessage via the synchronized JMS connection method sendAndWaitForOneReply(). After a successful storage operation, both the local copy of the file and the copy on the ftp server are deleted.
 * @param file A file to be stored. Must exist.
 * @throws IOFailure        thrown if store is unsuccessful, or failed toclean up files locally or on the ftp server after the store operation.
 * @throws ArgumentNotValid if file parameter is null or file is not anexisting file.
 */
public void store(File file) throws IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNull(file,""String_Node_Str"");
  ArgumentNotValid.checkTrue(file.isFile(),""String_Node_Str"" + file.getPath() + ""String_Node_Str""+ ""String_Node_Str"");
  StringBuilder messages=new StringBuilder();
  for (long i=0; i < storeRetries; i++) {
    StoreMessage outMsg=null;
    try {
      log.debug(""String_Node_Str"" + file.getPath() + ""String_Node_Str"");
      outMsg=new StoreMessage(replyQ,file);
      NetarkivetMessage replyMsg=sendAndWaitForOneReply(outMsg,storeTimeout);
      if (replyMsg != null && replyMsg.isOk()) {
        try {
          FileUtils.removeRecursively(file);
        }
 catch (        IOFailure e) {
          log.warn(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"",e);
        }
        return;
      }
 else       if (replyMsg == null) {
        String msg=""String_Node_Str"" + ""String_Node_Str"" + file.getPath() + ""String_Node_Str""+ (i + 1)+ ""String_Node_Str""+ storeRetries;
        log.warn(msg);
        messages.append(msg).append(""String_Node_Str"");
      }
 else {
        String msg=""String_Node_Str"" + replyMsg + ""String_Node_Str""+ ""String_Node_Str""+ file.getPath()+ ""String_Node_Str""+ (i + 1)+ ""String_Node_Str""+ storeRetries+ ""String_Node_Str""+ replyMsg.getErrMsg()+ ""String_Node_Str"";
        log.warn(msg);
        messages.append(msg).append(""String_Node_Str"");
      }
    }
 catch (    Exception e) {
      String msg=""String_Node_Str"" + file.getPath() + ""String_Node_Str""+ (i + 1)+ ""String_Node_Str""+ storeRetries+ ""String_Node_Str"";
      log.warn(msg,e);
      messages.append(msg).append(""String_Node_Str"");
      messages.append(ExceptionUtils.getStackTrace(e));
    }
 finally {
      if (outMsg != null) {
        cleanUpAfterStore(outMsg);
      }
    }
  }
  String errMsg=""String_Node_Str"" + file.getPath() + ""String_Node_Str""+ storeRetries+ ""String_Node_Str""+ messages;
  log.warn(errMsg);
  NotificationsFactory.getInstance().errorEvent(errMsg);
  throw new IOFailure(errMsg);
}",0.9942721706498124
89897,"/** 
 * Synchronously retrieves a file from a bitarchive and places it in a local file. This is the interface for sending GetFileMessage on the ""TheArcrepos"" queue. This is a blocking call.
 * @param arcfilename    Name of the arcfile to retrieve.
 * @param replica   The bitarchive to retrieve the data from.
 * @param toFile Filename of a place where the file fetched can be put.
 * @throws IOFailure if there are problems getting a reply or the filecould not be found.
 */
public void getFile(String arcfilename,Replica replica,File toFile){
  ArgumentNotValid.checkNotNullOrEmpty(arcfilename,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(toFile,""String_Node_Str"");
  log.debug(""String_Node_Str"" + arcfilename + ""String_Node_Str""+ replica+ ""String_Node_Str"");
  GetFileMessage gfMsg=new GetFileMessage(Channels.getTheRepos(),replyQ,arcfilename,replica.getId());
  GetFileMessage getFileMessage=(GetFileMessage)sendAndWaitForOneReply(gfMsg,0);
  if (getFileMessage == null) {
    String msg=""String_Node_Str"" + ""String_Node_Str"";
    log.debug(msg);
    throw new IOFailure(msg);
  }
 else   if (!getFileMessage.isOk()) {
    String msg=""String_Node_Str"" + getFileMessage.getErrMsg();
    log.warn(msg);
    throw new IOFailure(msg);
  }
 else {
    getFileMessage.getData(toFile);
  }
}","/** 
 * Synchronously retrieves a file from a bitarchive and places it in a local file. This is the interface for sending GetFileMessage on the ""TheArcrepos"" queue. This is a blocking call.
 * @param arcfilename Name of the arcfile to retrieve.
 * @param replica     The bitarchive to retrieve the data from.
 * @param toFile      Filename of a place where the file fetched can beput.
 * @throws IOFailure if there are problems getting a reply or the file couldnot be found.
 */
public void getFile(String arcfilename,Replica replica,File toFile){
  ArgumentNotValid.checkNotNullOrEmpty(arcfilename,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replica,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(toFile,""String_Node_Str"");
  log.debug(""String_Node_Str"" + arcfilename + ""String_Node_Str""+ replica+ ""String_Node_Str"");
  GetFileMessage gfMsg=new GetFileMessage(Channels.getTheRepos(),replyQ,arcfilename,replica.getId());
  GetFileMessage getFileMessage=(GetFileMessage)sendAndWaitForOneReply(gfMsg,0);
  if (getFileMessage == null) {
    String msg=""String_Node_Str"" + ""String_Node_Str"";
    log.debug(msg);
    throw new IOFailure(msg);
  }
 else   if (!getFileMessage.isOk()) {
    String msg=""String_Node_Str"" + getFileMessage.getErrMsg();
    log.warn(msg);
    throw new IOFailure(msg);
  }
 else {
    getFileMessage.getData(toFile);
  }
}",0.9915223000368596
89898,"/** 
 * Request update of admin data to specific state.
 * @param fileName The file for which admin data should be updated.
 * @param bitarchiveName The bitarchive for which admin data should beupdated.
 * @param newval The new value in admin data.
 * @throws IOFailure If the reply to the request update timed out.
 */
public void updateAdminData(String fileName,String bitarchiveName,BitArchiveStoreState newval){
  ArgumentNotValid.checkNotNullOrEmpty(fileName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(bitarchiveName,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(newval,""String_Node_Str"");
  String msg=""String_Node_Str"" + fileName + ""String_Node_Str""+ bitarchiveName+ ""String_Node_Str""+ newval;
  log.warn(msg);
  NotificationsFactory.getInstance().errorEvent(msg);
  AdminDataMessage aMsg=new AdminDataMessage(fileName,bitarchiveName,newval);
  sendAndWaitForOneReply(aMsg,0);
}","/** 
 * Request update of admin data to specific state.
 * @param fileName       The file for which admin data should be updated.
 * @param bitarchiveName The bitarchive for which admin data should beupdated.
 * @param newval         The new value in admin data.
 * @throws IOFailure If the reply to the request update timed out.
 */
public void updateAdminData(String fileName,String bitarchiveName,BitArchiveStoreState newval){
  ArgumentNotValid.checkNotNullOrEmpty(fileName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(bitarchiveName,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(newval,""String_Node_Str"");
  String msg=""String_Node_Str"" + fileName + ""String_Node_Str""+ bitarchiveName+ ""String_Node_Str""+ newval;
  log.warn(msg);
  NotificationsFactory.getInstance().errorEvent(msg);
  AdminDataMessage aMsg=new AdminDataMessage(fileName,bitarchiveName,newval);
  sendAndWaitForOneReply(aMsg,0);
}",0.9923413566739606
89899,"/** 
 * Sends a GetMessage on the ""TheArcrepos"" queue and waits for a reply. This is a blocking call. Returns null if no message is returned within Settings.ARCREPOSITORY_GET_TIMEOUT
 * @param arcfile The name of a file.
 * @param index   The offset of the wanted record in the file
 * @return a BitarchiveRecord-object or null if request times out or objectis not found.
 * @throws ArgumentNotValid If the given arcfile is null or empty, or the given index is negative.
 * @throws IOFailure If a wrong message is returned or theget operation failed.
 */
public BitarchiveRecord get(String arcfile,long index) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(arcfile,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(index,""String_Node_Str"");
  log.debug(""String_Node_Str"" + arcfile + ""String_Node_Str""+ index+ ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetMessage requestGetMsg=new GetMessage(Channels.getTheRepos(),replyQ,arcfile,index);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(requestGetMsg,getTimeout);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / 1000) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    log.info(""String_Node_Str"" + arcfile + ""String_Node_Str""+ index+ ""String_Node_Str""+ (getTimeout / 1000)+ ""String_Node_Str"");
    return null;
  }
  GetMessage replyGetMsg;
  try {
    replyGetMsg=(GetMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    String errorMsg=""String_Node_Str"" + replyNetMsg + ""String_Node_Str"";
    log.warn(errorMsg,e);
    throw new ArgumentNotValid(errorMsg,e);
  }
  if (!replyGetMsg.isOk()) {
    final String errMsg=""String_Node_Str"" + replyGetMsg.getErrMsg() + ""String_Node_Str"";
    log.warn(errMsg);
    throw new ArgumentNotValid(errMsg);
  }
  return replyGetMsg.getRecord();
}","/** 
 * Sends a GetMessage on the ""TheArcrepos"" queue and waits for a reply. This is a blocking call. Returns null if no message is returned within Settings.ARCREPOSITORY_GET_TIMEOUT
 * @param arcfile The name of a file.
 * @param index   The offset of the wanted record in the file
 * @return a BitarchiveRecord-object or null if request times out or objectis not found.
 * @throws ArgumentNotValid If the given arcfile is null or empty, or thegiven index is negative.
 * @throws IOFailure        If a wrong message is returned or the getoperation failed.
 */
public BitarchiveRecord get(String arcfile,long index) throws ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(arcfile,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(index,""String_Node_Str"");
  log.debug(""String_Node_Str"" + arcfile + ""String_Node_Str""+ index+ ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetMessage requestGetMsg=new GetMessage(Channels.getTheRepos(),replyQ,arcfile,index);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(requestGetMsg,getTimeout);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / 1000) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    log.info(""String_Node_Str"" + arcfile + ""String_Node_Str""+ index+ ""String_Node_Str""+ (getTimeout / 1000)+ ""String_Node_Str"");
    return null;
  }
  GetMessage replyGetMsg;
  try {
    replyGetMsg=(GetMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    String errorMsg=""String_Node_Str"" + replyNetMsg + ""String_Node_Str"";
    log.warn(errorMsg,e);
    throw new ArgumentNotValid(errorMsg,e);
  }
  if (!replyGetMsg.isOk()) {
    final String errMsg=""String_Node_Str"" + replyGetMsg.getErrMsg() + ""String_Node_Str"";
    log.warn(errMsg);
    throw new ArgumentNotValid(errMsg);
  }
  return replyGetMsg.getRecord();
}",0.995682676740421
89900,"/** 
 * Adds this Synchronizer as listener on a jms connection.
 */
protected JMSArcRepositoryClient(){
  storeRetries=Settings.getLong(ARCREPOSITORY_STORE_RETRIES);
  storeTimeout=Settings.getLong(ARCREPOSITORY_STORE_TIMEOUT);
  getTimeout=Settings.getLong(ARCREPOSITORY_GET_TIMEOUT);
  log.info(""String_Node_Str"" + storeRetries + ""String_Node_Str""+ storeTimeout+ ""String_Node_Str""+ getTimeout+ ""String_Node_Str"");
  replyQ=Channels.getThisReposClient();
  ArgumentNotValid.checkNotNull(replyQ,""String_Node_Str"");
  JMSConnectionFactory.getInstance().setListener(replyQ,this);
  log.info(""String_Node_Str"" + replyQ + ""String_Node_Str"");
}","/** 
 * Adds this Synchronizer as listener on a jms connection. 
 */
protected JMSArcRepositoryClient(){
  storeRetries=Settings.getLong(ARCREPOSITORY_STORE_RETRIES);
  storeTimeout=Settings.getLong(ARCREPOSITORY_STORE_TIMEOUT);
  getTimeout=Settings.getLong(ARCREPOSITORY_GET_TIMEOUT);
  log.info(""String_Node_Str"" + storeRetries + ""String_Node_Str""+ storeTimeout+ ""String_Node_Str""+ getTimeout+ ""String_Node_Str"");
  replyQ=Channels.getThisReposClient();
  JMSConnectionFactory.getInstance().setListener(replyQ,this);
  log.info(""String_Node_Str"" + replyQ + ""String_Node_Str"");
}",0.8377049180327869
89901,"/** 
 * Get an JMSArcRepositoryClient instance. This is guaranteed to be a singleton. 
 * @return an JMSArcRepositoryClient instance.
 */
public static synchronized JMSArcRepositoryClient getInstance(){
  if (instance == null) {
    instance=new JMSArcRepositoryClient();
  }
  return instance;
}","/** 
 * Get an JMSArcRepositoryClient instance. This is guaranteed to be a singleton.
 * @return an JMSArcRepositoryClient instance.
 */
public static synchronized JMSArcRepositoryClient getInstance(){
  if (instance == null) {
    instance=new JMSArcRepositoryClient();
  }
  return instance;
}",0.9983079526226736
89902,"/** 
 * Removes this object as a JMS listener.
 */
public void close(){
synchronized (this.getClass()) {
    JMSConnectionFactory.getInstance().removeListener(replyQ,this);
    instance=null;
  }
}","/** 
 * Removes this object as a JMS listener. 
 */
public void close(){
synchronized (JMSArcRepositoryClient.class) {
    JMSConnectionFactory.getInstance().removeListener(replyQ,this);
    instance=null;
  }
}",0.9019607843137256
89903,"/** 
 * Retrieves all the checksum from the replica through a  GetAllChecksumMessage. This is the checksum archive alternative to running a ChecksumBatchJob. 
 * @param replicaId The id of the replica from which the checksums should be retrieved.
 * @return A list of ChecksumEntries which is the results of the GetAllChecksumMessage.
 * @see dk.netarkivet.archive.checksum.distribute.GetAllChecksumMessage
 */
public File getAllChecksums(String replicaId){
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  log.debug(""String_Node_Str"" + replicaId + ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetAllChecksumMessage gacMsg=new GetAllChecksumMessage(Channels.getTheRepos(),replyQ,replicaId);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(gacMsg,getTimeout);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / 1000) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    log.info(""String_Node_Str"" + (getTimeout / 1000) + ""String_Node_Str"");
    return null;
  }
  GetAllChecksumMessage replyCSMsg;
  try {
    replyCSMsg=(GetAllChecksumMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    String errorMsg=""String_Node_Str"" + replyNetMsg;
    log.warn(errorMsg,e);
    throw new IOFailure(errorMsg,e);
  }
  try {
    File result=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    replyCSMsg.getData(result);
    return result;
  }
 catch (  IOException e) {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + replyCSMsg;
    log.warn(errMsg);
    throw new IOFailure(errMsg,e);
  }
}","/** 
 * Retrieves all the checksum from the replica through a GetAllChecksumMessage. This is the checksum archive alternative to running a ChecksumBatchJob.
 * @param replicaId The id of the replica from which the checksums should beretrieved.
 * @return A list of ChecksumEntries which is the results of theGetAllChecksumMessage.
 * @see dk.netarkivet.archive.checksum.distribute.GetAllChecksumMessage
 */
public File getAllChecksums(String replicaId){
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  log.debug(""String_Node_Str"" + replicaId + ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetAllChecksumMessage gacMsg=new GetAllChecksumMessage(Channels.getTheRepos(),replyQ,replicaId);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(gacMsg,getTimeout);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / 1000) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    log.info(""String_Node_Str"" + (getTimeout / 1000) + ""String_Node_Str"");
    return null;
  }
  GetAllChecksumMessage replyCSMsg;
  try {
    replyCSMsg=(GetAllChecksumMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    String errorMsg=""String_Node_Str"" + replyNetMsg;
    log.warn(errorMsg,e);
    throw new IOFailure(errorMsg,e);
  }
  try {
    File result=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    replyCSMsg.getData(result);
    return result;
  }
 catch (  IOException e) {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + replyCSMsg;
    log.warn(errMsg);
    throw new IOFailure(errMsg,e);
  }
}",0.9987546699875468
89904,"/** 
 * Message to signal from BitarchiveMonitorServer that the batch job identified by originatingBatchMsgId is completed. Holds status information: list of files processed and a list of files on which the batch job failed
 * @param to The queue to which this message is to be sent. This willnormally be the ARCREPOS queue
 * @param replyTo The queue that should receive replies.
 * @param originatingBatchMsgId The Id of the BathMessage which gaverise to this reply
 * @param filesProcessed  the total number of file processed in thisbatch job
 * @param filesFailed a Collection<String> of the names of files onwhich this batch job failed
 * @param resultFile The RemoteFile containing the output fromthe batch job, or null if an error occurred that prevented the creation of the file.
 * @throws ArgumentNotValid if the input parameters are not meaningful
 */
public BatchReplyMessage(ChannelID to,ChannelID replyTo,String originatingBatchMsgId,int filesProcessed,Collection<File> filesFailed,RemoteFile resultFile) throws ArgumentNotValid {
  super(to,replyTo);
  ArgumentNotValid.checkNotNullOrEmpty(originatingBatchMsgId,""String_Node_Str"");
  ArgumentNotValid.checkTrue(filesProcessed >= 0,""String_Node_Str"");
  this.replyOfId=originatingBatchMsgId;
  this.noOfFilesProcessed=filesProcessed;
  if (filesFailed != null) {
    this.filesFailed=new HashSet<File>(filesFailed);
  }
 else {
    this.filesFailed=new HashSet<File>();
  }
  this.resultFile=resultFile;
}","/** 
 * Message to signal from BitarchiveMonitorServer that the batch job identified by originatingBatchMsgId is completed. Holds status information: list of files processed and a list of files on which the batch job failed
 * @param to                    The queue to which this message is to besent. This will normally be the ARCREPOS queue
 * @param replyTo               The queue that should receive replies.
 * @param originatingBatchMsgId The Id of the BathMessage which gave rise tothis reply
 * @param filesProcessed        the total number of file processed in thisbatch job
 * @param filesFailed           a Collection<String> of the names of fileson which this batch job failed. May be null or empty for no errors.
 * @param resultFile            The RemoteFile containing the output fromthe batch job, or null if an error occurred that prevented the creation of the file.
 * @throws ArgumentNotValid if the input parameters are not meaningful
 */
public BatchReplyMessage(ChannelID to,ChannelID replyTo,String originatingBatchMsgId,int filesProcessed,Collection<File> filesFailed,RemoteFile resultFile) throws ArgumentNotValid {
  super(to,replyTo);
  ArgumentNotValid.checkNotNullOrEmpty(originatingBatchMsgId,""String_Node_Str"");
  ArgumentNotValid.checkTrue(filesProcessed >= 0,""String_Node_Str"");
  this.replyOfId=originatingBatchMsgId;
  this.noOfFilesProcessed=filesProcessed;
  if (filesFailed != null) {
    this.filesFailed=new HashSet<File>(filesFailed);
  }
 else {
    this.filesFailed=new HashSet<File>();
  }
  this.resultFile=resultFile;
}",0.9601317957166392
89905,"/** 
 * Writes the files involved with a harvests. Creates the Heritrix arcs directory to ensure that this directory exists in advance.
 * @param crawldir        The directory that the crawl should take placein.
 * @param job             The Job object containing various harvest setupdata.
 * @param metadataEntries Any metadata entries sent along with the job thatshould be stored for later use.
 * @return An object encapsulating where these files have been written.
 */
public HeritrixFiles writeHarvestFiles(File crawldir,Job job,List<MetadataEntry> metadataEntries){
  final HeritrixFiles files=new HeritrixFiles(crawldir,job.getJobID(),job.getOrigHarvestDefinitionID());
  log.debug(""String_Node_Str"" + job.getJobID());
  new PersistentJobData(files.getCrawlDir()).write(job);
  writePreharvestMetadata(job,metadataEntries,crawldir);
  files.writeSeedsTxt(job.getSeedListAsString());
  files.writeOrderXml(job.getOrderXMLdoc());
  if (HeritrixLauncher.isDeduplicationEnabledInTemplate(job.getOrderXMLdoc())) {
    files.setIndexDir(fetchDeduplicateIndex(metadataEntries));
  }
  boolean created=files.getArcsDir().mkdir();
  if (!created) {
    log.warn(""String_Node_Str"" + files.getArcsDir());
  }
  return files;
}","/** 
 * Writes the files involved with a harvests. Creates the Heritrix arcs directory to ensure that this directory exists in advance.
 * @param crawldir        The directory that the crawl should take placein.
 * @param job             The Job object containing various harvest setupdata.
 * @param metadataEntries Any metadata entries sent along with the job thatshould be stored for later use.
 * @return An object encapsulating where these files have been written.
 */
public HeritrixFiles writeHarvestFiles(File crawldir,Job job,List<MetadataEntry> metadataEntries){
  final HeritrixFiles files=new HeritrixFiles(crawldir,job.getJobID(),job.getOrigHarvestDefinitionID());
  log.debug(""String_Node_Str"" + job.getJobID());
  new PersistentJobData(files.getCrawlDir()).write(job);
  writePreharvestMetadata(job,metadataEntries,crawldir);
  files.writeSeedsTxt(job.getSeedListAsString());
  files.writeOrderXml(job.getOrderXMLdoc());
  if (HeritrixLauncher.isDeduplicationEnabledInTemplate(job.getOrderXMLdoc())) {
    log.debug(""String_Node_Str"");
    files.setIndexDir(fetchDeduplicateIndex(metadataEntries));
  }
 else {
    log.debug(""String_Node_Str"");
  }
  boolean created=files.getArcsDir().mkdir();
  if (!created) {
    log.warn(""String_Node_Str"" + files.getArcsDir());
  }
  return files;
}",0.9683293745051464
89906,"/** 
 * Retrieve the list of jobs for deduplicate reduction. Runs through all metadata entries, finding duplicate reduction entries, and parsing all jobIDs in them, warning only on errors.
 * @param metadataEntries list of metadataEntries
 * @return the list of jobs for deduplicate reduction
 */
private List<Long> parseJobIDsForDuplicateReduction(List<MetadataEntry> metadataEntries){
  List<Long> result=new ArrayList<Long>();
  for (  MetadataEntry me : metadataEntries) {
    if (me.isDuplicateReductionMetadataEntry()) {
      String s=new String(me.getData());
      String[] longs=s.split(""String_Node_Str"");
      for (      String stringLong : longs) {
        try {
          result.add(Long.getLong(stringLong));
        }
 catch (        NumberFormatException e) {
          log.warn(""String_Node_Str"" + stringLong + ""String_Node_Str""+ ""String_Node_Str""+ s+ ""String_Node_Str"",e);
        }
      }
    }
  }
  return result;
}","/** 
 * Retrieve the list of jobs for deduplicate reduction. Runs through all metadata entries, finding duplicate reduction entries, and parsing all jobIDs in them, warning only on errors.
 * @param metadataEntries list of metadataEntries
 * @return the list of jobs for deduplicate reduction
 */
private List<Long> parseJobIDsForDuplicateReduction(List<MetadataEntry> metadataEntries){
  List<Long> result=new ArrayList<Long>();
  for (  MetadataEntry me : metadataEntries) {
    if (me.isDuplicateReductionMetadataEntry()) {
      String s=new String(me.getData());
      String[] longs=s.split(""String_Node_Str"");
      for (      String stringLong : longs) {
        try {
          result.add(Long.parseLong(stringLong));
        }
 catch (        NumberFormatException e) {
          log.warn(""String_Node_Str"" + stringLong + ""String_Node_Str""+ ""String_Node_Str""+ s+ ""String_Node_Str"",e);
        }
      }
    }
  }
  return result;
}",0.9957446808510638
89907,"/** 
 * Ensure that a file containing the appropriate content exists for the ID. If the content cannot be found, this method may return null (if I is a simple type) or an appropriate subset (if I is, say, a Set) indicating the data that is actually available.  In the latter case, calling cache on the returned set should always fill the file for that subset (barring catastrophic failure). Locking:  If the file is not immediately found, we enter a file-creation state.  To avoid corrupted data, we must ensure that only one cache instance, and only one thread within any instance, creates the file. Thus as long as somebody else seems to be creating the file, we wait and see if they finish.  This is checked by having an exclusive lock on a "".working"" file (we cannot use the result file, as it has to be created to be locked, and we may end up with a different cached file than we thought, see above).  The .working file itself is irrelevant, only the lock on it matters.
 * @param id Some sort of id that uniquely identifies the item withinthe cache.
 * @return The id given if it was successfully fetched, otherwise nullif the type parameter I does not allow subsets, or a subset of id if it does.  This subset should be immediately cacheable. FIXME added method synchronization. Try to fix bug 1547
 */
public I cache(I id){
  ArgumentNotValid.checkNotNull(id,""String_Node_Str"");
  File cachedFile=getCacheFile(id);
  if (cachedFile.exists()) {
    return id;
  }
 else {
    try {
      File fileBehindLockFile=new File(cachedFile.getAbsolutePath() + ""String_Node_Str"");
      FileOutputStream lockFile=new FileOutputStream(fileBehindLockFile);
      FileLock lock=null;
synchronized (fileBehindLockFile.getAbsolutePath().intern()) {
        try {
          log.debug(""String_Node_Str"" + fileBehindLockFile.getAbsolutePath() + ""String_Node_Str""+ Thread.currentThread().getName()+ ""String_Node_Str"");
          try {
            lock=lockFile.getChannel().lock();
          }
 catch (          OverlappingFileLockException e) {
            log.warn(e);
            throw new IOException(e);
          }
          if (cachedFile.exists()) {
            return id;
          }
          return cacheData(id);
        }
  finally {
          if (lock != null) {
            log.debug(""String_Node_Str"" + lockFile.getChannel());
            lock.release();
          }
          lockFile.close();
        }
      }
    }
 catch (    IOException e) {
      String errMsg=""String_Node_Str"" + cachedFile.getAbsolutePath() + ""String_Node_Str"";
      log.warn(errMsg,e);
      throw new IOFailure(errMsg,e);
    }
  }
}","/** 
 * Ensure that a file containing the appropriate content exists for the ID. If the content cannot be found, this method may return null (if I is a simple type) or an appropriate subset (if I is, say, a Set) indicating the data that is actually available.  In the latter case, calling cache on the returned set should always fill the file for that subset (barring catastrophic failure). Locking:  If the file is not immediately found, we enter a file-creation state.  To avoid corrupted data, we must ensure that only one cache instance, and only one thread within any instance, creates the file. Thus as long as somebody else seems to be creating the file, we wait and see if they finish.  This is checked by having an exclusive lock on a "".working"" file (we cannot use the result file, as it has to be created to be locked, and we may end up with a different cached file than we thought, see above).  The .working file itself is irrelevant, only the lock on it matters.
 * @param id Some sort of id that uniquely identifies the item within thecache.
 * @return The id given if it was successfully fetched, otherwise null ifthe type parameter I does not allow subsets, or a subset of id if it does.  This subset should be immediately cacheable.
 */
public I cache(I id){
  ArgumentNotValid.checkNotNull(id,""String_Node_Str"");
  File cachedFile=getCacheFile(id);
  try {
    File fileBehindLockFile=new File(cachedFile.getAbsolutePath() + ""String_Node_Str"");
    FileOutputStream lockFile=new FileOutputStream(fileBehindLockFile);
    FileLock lock=null;
synchronized (fileBehindLockFile.getAbsolutePath().intern()) {
      try {
        log.debug(""String_Node_Str"" + fileBehindLockFile.getAbsolutePath() + ""String_Node_Str""+ Thread.currentThread().getName()+ ""String_Node_Str"");
        try {
          lock=lockFile.getChannel().lock();
        }
 catch (        OverlappingFileLockException e) {
          throw new IOException(e.getMessage(),e);
        }
        if (cachedFile.exists()) {
          return id;
        }
        return cacheData(id);
      }
  finally {
        if (lock != null) {
          log.debug(""String_Node_Str"" + lockFile.getChannel());
          lock.release();
        }
        lockFile.close();
      }
    }
  }
 catch (  IOException e) {
    String errMsg=""String_Node_Str"" + cachedFile.getAbsolutePath() + ""String_Node_Str"";
    log.warn(errMsg,e);
    throw new IOFailure(errMsg,e);
  }
}",0.9532488114104596
89908,"/** 
 * Forgiving index generating method, that returns a file with an index, of the greatest possible subset of a given id, and the subset. If the type I for instance is a Set, you may get an index of only a subset. If I is a File, null may be seen as a subset.
 * @see #cache for more information.
 * @param id The requested index.
 * @return An index over the greatest possible subset, and the subset.
 */
public Index<I> getIndex(I id){
  I response=id;
  I lastResponse=null;
  while (response != null && !response.equals(lastResponse)) {
    if (lastResponse != null) {
      log.info(""String_Node_Str"" + this.getCacheDir().getName() + ""String_Node_Str""+ lastResponse+ ""String_Node_Str""+ response+ ""String_Node_Str"");
    }
    lastResponse=response;
    response=cache(lastResponse);
  }
  File cacheFile=getCacheFile(response);
  log.info(""String_Node_Str"" + cacheFile + ""String_Node_Str""+ response+ ""String_Node_Str""+ id+ ""String_Node_Str"");
  return new Index<I>(cacheFile,response);
}","/** 
 * Forgiving index generating method, that returns a file with an index, of the greatest possible subset of a given id, and the subset. If the type I for instance is a Set, you may get an index of only a subset. If I is a File, null may be seen as a subset.
 * @param id The requested index.
 * @return An index over the greatest possible subset, and the subset.
 * @see #cache for more information.
 */
public Index<I> getIndex(I id){
  I response=id;
  I lastResponse=null;
  while (response != null && !response.equals(lastResponse)) {
    if (lastResponse != null) {
      log.info(""String_Node_Str"" + this.getCacheDir().getName() + ""String_Node_Str""+ lastResponse+ ""String_Node_Str""+ response+ ""String_Node_Str"");
    }
    lastResponse=response;
    response=cache(lastResponse);
  }
  File cacheFile=getCacheFile(response);
  log.info(""String_Node_Str"" + cacheFile + ""String_Node_Str""+ response+ ""String_Node_Str""+ id+ ""String_Node_Str"");
  return new Index<I>(cacheFile,response);
}",0.8944723618090452
89909,"/** 
 * Fill in actual data in the file in the cache.  This is the workhorse method that is allowed to modify the cache.  When this method is called, the cache can assume that getCacheFile(id) does not exist.
 * @param id Some identifier for the item to be cached.
 * @return An id of content actually available.  In most cases, this willbe the same as id, but for complex I it could be a subset (or null if the type argument I is a simple type).  If the return value is not the same as id, the file will not contain cached data, and may not even exist.
 */
protected abstract I cacheData(I id);","/** 
 * Fill in actual data in the file in the cache.  This is the workhorse method that is allowed to modify the cache.  When this method is called, the cache can assume that getCacheFile(id) does not exist.
 * @param id Some identifier for the item to be cached.
 * @return An id of content actually available.  In most cases, this will bethe same as id, but for complex I it could be a subset (or null if the type argument I is a simple type).  If the return value is not the same as id, the file will not contain cached data, and may not even exist.
 */
protected abstract I cacheData(I id);",0.9949579831932772
89910,"/** 
 * Utility method to get a number of cache entries at a time. Implementations of FileBasedCache may override this to perform the caching more efficiently, if caching overhead per file is large.
 * @param IDs List of IDs that uniquely identify a set of items withinthe cache.
 * @return A map from ID to the files containing cached data for thoseIDs.  If caching failed, even partially, for an ID, the entry for the ID doesn't exist.
 */
public Map<I,File> get(Set<I> IDs){
  ArgumentNotValid.checkNotNull(IDs,""String_Node_Str"");
  Map<I,File> result=new HashMap<I,File>(IDs.size());
  for (  I ID : IDs) {
    if (ID.equals(cache(ID))) {
      result.put(ID,getCacheFile(ID));
    }
 else {
      result.put(ID,null);
    }
  }
  return result;
}","/** 
 * Utility method to get a number of cache entries at a time. Implementations of FileBasedCache may override this to perform the caching more efficiently, if caching overhead per file is large.
 * @param IDs List of IDs that uniquely identify a set of items within thecache.
 * @return A map from ID to the files containing cached data for those IDs.If caching failed, even partially, for an ID, the entry for the ID doesn't exist.
 */
public Map<I,File> get(Set<I> IDs){
  ArgumentNotValid.checkNotNull(IDs,""String_Node_Str"");
  Map<I,File> result=new HashMap<I,File>(IDs.size());
  for (  I ID : IDs) {
    if (ID.equals(cache(ID))) {
      result.put(ID,getCacheFile(ID));
    }
 else {
      result.put(ID,null);
    }
  }
  return result;
}",0.9873417721518988
89911,"public void testGenerateCDX() throws FileNotFoundException {
  File file1=new File(TestInfo.WORKING_DIR,""String_Node_Str"");
  File file2=new File(TestInfo.WORKING_DIR,""String_Node_Str"");
  String[] files=new String[]{file1.getAbsolutePath(),file2.getAbsolutePath()};
  DeduplicateToCDXApplication app=new DeduplicateToCDXApplication();
  app.generateCDX(files);
  String output=((ByteArrayOutputStream)new_std_out).toString();
  String error=((ByteArrayOutputStream)new_std_err).toString();
  assertTrue(""String_Node_Str"",error.equals(""String_Node_Str""));
  assertTrue(""String_Node_Str"" + output + ""String_Node_Str"",output.split(""String_Node_Str"").length > 20);
}","public void testGenerateCDX() throws IOException {
  File file1=new File(TestInfo.WORKING_DIR,""String_Node_Str"");
  File file2=new File(TestInfo.WORKING_DIR,""String_Node_Str"");
  String[] files=new String[]{file1.getAbsolutePath(),file2.getAbsolutePath()};
  DeduplicateToCDXApplication app=new DeduplicateToCDXApplication();
  app.generateCDX(files);
  String output=((ByteArrayOutputStream)new_std_out).toString();
  String error=((ByteArrayOutputStream)new_std_err).toString();
  assertTrue(""String_Node_Str"",error.equals(""String_Node_Str""));
  assertTrue(""String_Node_Str"" + output + ""String_Node_Str"",output.split(""String_Node_Str"").length > 20);
}",0.9893617021276596
89912,"/** 
 * Returns a string representing the instance.
 * @return a string
 */
public String toString(){
  return ""String_Node_Str"" + replyOfId + ""String_Node_Str""+ noOfFilesProcessed+ ""String_Node_Str""+ filesFailed.size()+ ""String_Node_Str""+ super.toString();
}","/** 
 * Returns a string representing the instance.
 * @return a string
 */
public String toString(){
  return ""String_Node_Str"" + replyOfId + ""String_Node_Str""+ noOfFilesProcessed+ ""String_Node_Str""+ (filesFailed == null ? ""String_Node_Str"" : filesFailed.size())+ ""String_Node_Str""+ super.toString();
}",0.9217081850533808
89913,"/** 
 * Test message is sent and returned, and set ""OK"" if no errors occurs.
 */
public void testStore(){
  file=new File(new File(BITARCHIVE_DIR,""String_Node_Str""),STORABLE_FILES.get(0).toString());
  ArcRepository arc=ArcRepository.getInstance();
  StoreMessage msg=new StoreMessage(Channels.getError(),file);
  new ArcRepositoryServer(arc).visit(msg);
  assertTrue(""String_Node_Str"",msg.isOk());
  arc.close();
}","/** 
 * Test message is sent and returned, and set ""OK"" if no errors occurs.
 */
public void testStore(){
  file=new File(new File(BITARCHIVE_DIR,""String_Node_Str""),STORABLE_FILES.get(0).toString());
  ArcRepository arc=ArcRepository.getInstance();
  StoreMessage msg=new StoreMessage(Channels.getError(),file);
  JMSConnectionMockupMQ.updateMsgID(msg,""String_Node_Str"");
  new ArcRepositoryServer(arc).visit(msg);
  assertTrue(""String_Node_Str"",msg.isOk());
  arc.close();
}",0.9325842696629212
89914,"/** 
 * Test message is sent and returned, and set ""Not OK"" if an error occurs.
 */
public void testStoreNoSuchFile(){
  file=new File(BITARCHIVE_DIR,""String_Node_Str"");
  ArcRepository arc=ArcRepository.getInstance();
  try {
    new StoreMessage(Channels.getError(),file);
    fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  Settings.set(CommonSettings.REMOTE_FILE_CLASS,NullRemoteFile.class.getName());
  file=new File(new File(BITARCHIVE_DIR,""String_Node_Str""),STORABLE_FILES.get(0).toString());
  StoreMessage msg=new StoreMessage(Channels.getError(),file);
  dummyServer.reset();
  new ArcRepositoryServer(arc).visit(msg);
  ((JMSConnectionMockupMQ)JMSConnectionMockupMQ.getInstance()).waitForConcurrentTasksToFinish();
  assertFalse(""String_Node_Str"",msg.isOk());
  assertEquals(""String_Node_Str"",dummyServer.msgNotOK,1);
  arc.close();
}","/** 
 * Test message is sent and returned, and set ""Not OK"" if an error occurs.
 */
public void testStoreNoSuchFile(){
  file=new File(BITARCHIVE_DIR,""String_Node_Str"");
  ArcRepository arc=ArcRepository.getInstance();
  try {
    new StoreMessage(Channels.getError(),file);
    fail(""String_Node_Str"" + ""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  Settings.set(CommonSettings.REMOTE_FILE_CLASS,NullRemoteFile.class.getName());
  file=new File(new File(BITARCHIVE_DIR,""String_Node_Str""),STORABLE_FILES.get(0).toString());
  StoreMessage msg=new StoreMessage(Channels.getError(),file);
  JMSConnectionMockupMQ.updateMsgID(msg,""String_Node_Str"");
  dummyServer.reset();
  new ArcRepositoryServer(arc).visit(msg);
  ((JMSConnectionMockupMQ)JMSConnectionMockupMQ.getInstance()).waitForConcurrentTasksToFinish();
  assertFalse(""String_Node_Str"",msg.isOk());
  assertEquals(""String_Node_Str"",dummyServer.msgNotOK,1);
  arc.close();
}",0.9671772428884028
89915,"public void testStoreTimeouts() throws IOException, InterruptedException {
  Settings.set(JMSArcRepositoryClient.ARCREPOSITORY_STORE_TIMEOUT,""String_Node_Str"");
  arc.close();
  arc=(JMSArcRepositoryClient)ArcRepositoryClientFactory.getHarvesterInstance();
  final boolean[] ok=new boolean[]{false};
  new Thread(){
    public void run(){
      try {
        arc.store(ARCFILE);
      }
 catch (      IOFailure e) {
      }
      ok[0]=true;
synchronized (JMSArcRepositoryClientTester.this) {
        JMSArcRepositoryClientTester.this.notify();
      }
    }
  }
.start();
synchronized (this) {
    wait(3000);
  }
  assertTrue(""String_Node_Str"",ok[0]);
  CollectionAsserts.assertListEquals(""String_Node_Str"",new ArrayList<RemoteFile>(TestRemoteFile.remainingFiles()));
}","public void testStoreTimeouts() throws IOException, InterruptedException {
  Settings.set(JMSArcRepositoryClient.ARCREPOSITORY_STORE_TIMEOUT,""String_Node_Str"");
  arc.close();
  arc=(JMSArcRepositoryClient)ArcRepositoryClientFactory.getHarvesterInstance();
  final boolean[] ok=new boolean[]{false};
  new Thread(){
    public void run(){
      try {
        arc.store(ARCFILE);
      }
 catch (      IOFailure e) {
      }
      ok[0]=true;
synchronized (JMSArcRepositoryClientTester.this) {
        JMSArcRepositoryClientTester.this.notify();
      }
    }
  }
.start();
synchronized (this) {
    wait(3200);
  }
  assertTrue(""String_Node_Str"",ok[0]);
  CollectionAsserts.assertListEquals(""String_Node_Str"",new ArrayList<RemoteFile>(TestRemoteFile.remainingFiles()));
}",0.9987029831387808
89916,"/** 
 * Test that monitor can receive and aggregate data from more than one BitarchiveServer and aggregate the data and upload.
 * @throws ArgumentNotValid
 * @throws UnknownID
 * @throws IOFailure        it via RemoteFile
 */
public void testBatchEndedMessageAggregation() throws InterruptedException {
  BitarchiveMonitorServer bms=BitarchiveMonitorServer.getInstance();
  TestMessageListener listener=new TestMessageListener();
  con.setListener(Channels.getTheRepos(),listener);
  con.setListener(Channels.getAllBa(),listener);
  File output_file=new File(TestInfo.BAMON_WORKING,""String_Node_Str"");
  BatchMessage bm=new BatchMessage(Channels.getTheBamon(),Channels.getTheRepos(),new ChecksumJob(),Settings.get(CommonSettings.USE_REPLICA_ID));
  JMSConnectionMockupMQ.updateMsgID(bm,""String_Node_Str"");
  String baID1=""String_Node_Str"";
  HeartBeatMessage hbm=new HeartBeatMessage(Channels.getTheBamon(),baID1);
  String baID2=""String_Node_Str"";
  HeartBeatMessage hbm2=new HeartBeatMessage(Channels.getTheBamon(),baID2);
  bms.visit(hbm);
  bms.visit(hbm2);
  con.waitForConcurrentTasksToFinish();
  bms.visit(bm);
  con.waitForConcurrentTasksToFinish();
  String forwardedID=((BatchMessage)listener.getAllReceived().get(0)).getID();
  File data1=new File(TestInfo.BAMON_WORKING,""String_Node_Str"");
  File data2=new File(TestInfo.BAMON_WORKING,""String_Node_Str"");
  RemoteFile rf1=RemoteFileFactory.getInstance(data1,true,false,true);
  BatchEndedMessage bem1=new BatchEndedMessage(Channels.getTheBamon(),baID1,forwardedID,rf1);
  JMSConnectionMockupMQ.updateMsgID(bem1,""String_Node_Str"");
  RemoteFile rf2=RemoteFileFactory.getInstance(data2,true,false,true);
  BatchEndedMessage bem2=new BatchEndedMessage(Channels.getTheBamon(),baID2,forwardedID,rf2);
  JMSConnectionMockupMQ.updateMsgID(bem2,""String_Node_Str"");
  bms.visit(bem1);
  bms.visit(bem2);
  con.waitForConcurrentTasksToFinish();
synchronized (this) {
    wait(100);
  }
  bms.close();
  assertEquals(""String_Node_Str"",2,listener.getNumReceived());
  List<NetarkivetMessage> l=listener.getAllReceived();
  BatchReplyMessage brmsg=null;
  for (  NetarkivetMessage naMsg : l) {
    if (naMsg instanceof BatchReplyMessage) {
      brmsg=(BatchReplyMessage)naMsg;
    }
  }
  brmsg.getResultFile().copyTo(output_file);
  FileAsserts.assertFileNumberOfLines(""String_Node_Str"" + ""String_Node_Str"",output_file,2);
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",output_file);
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",output_file);
  int expected_length=FileUtils.readBinaryFile(data1).length + FileUtils.readBinaryFile(data2).length;
  int actual_length=FileUtils.readBinaryFile(output_file).length;
  assertEquals(""String_Node_Str"",expected_length,actual_length);
  assertTrue(""String_Node_Str"",((TestRemoteFile)rf1).isDeleted());
  assertTrue(""String_Node_Str"",((TestRemoteFile)rf2).isDeleted());
}","/** 
 * Test that monitor can receive and aggregate data from more than one BitarchiveServer and aggregate the data and upload.
 * @throws ArgumentNotValid
 * @throws UnknownID
 * @throws IOFailure        it via RemoteFile
 */
public void testBatchEndedMessageAggregation() throws InterruptedException {
  BitarchiveMonitorServer bms=BitarchiveMonitorServer.getInstance();
  TestMessageListener listener=new TestMessageListener();
  con.setListener(Channels.getTheRepos(),listener);
  con.setListener(Channels.getAllBa(),listener);
  File output_file=new File(TestInfo.BAMON_WORKING,""String_Node_Str"");
  BatchMessage bm=new BatchMessage(Channels.getTheBamon(),Channels.getTheRepos(),new ChecksumJob(),Settings.get(CommonSettings.USE_REPLICA_ID));
  JMSConnectionMockupMQ.updateMsgID(bm,""String_Node_Str"");
  String baID1=""String_Node_Str"";
  HeartBeatMessage hbm=new HeartBeatMessage(Channels.getTheBamon(),baID1);
  String baID2=""String_Node_Str"";
  HeartBeatMessage hbm2=new HeartBeatMessage(Channels.getTheBamon(),baID2);
  bms.visit(hbm);
  bms.visit(hbm2);
  con.waitForConcurrentTasksToFinish();
  bms.visit(bm);
  con.waitForConcurrentTasksToFinish();
  String forwardedID=((BatchMessage)listener.getAllReceived().get(0)).getID();
  File data1=new File(TestInfo.BAMON_WORKING,""String_Node_Str"");
  File data2=new File(TestInfo.BAMON_WORKING,""String_Node_Str"");
  RemoteFile rf1=RemoteFileFactory.getInstance(data1,true,false,true);
  BatchEndedMessage bem1=new BatchEndedMessage(Channels.getTheBamon(),baID1,forwardedID,rf1);
  JMSConnectionMockupMQ.updateMsgID(bem1,""String_Node_Str"");
  RemoteFile rf2=RemoteFileFactory.getInstance(data2,true,false,true);
  BatchEndedMessage bem2=new BatchEndedMessage(Channels.getTheBamon(),baID2,forwardedID,rf2);
  JMSConnectionMockupMQ.updateMsgID(bem2,""String_Node_Str"");
  bms.visit(bem1);
  bms.visit(bem2);
  con.waitForConcurrentTasksToFinish();
synchronized (this) {
    wait(200);
  }
  bms.close();
  assertEquals(""String_Node_Str"",2,listener.getNumReceived());
  List<NetarkivetMessage> l=listener.getAllReceived();
  BatchReplyMessage brmsg=null;
  for (  NetarkivetMessage naMsg : l) {
    if (naMsg instanceof BatchReplyMessage) {
      brmsg=(BatchReplyMessage)naMsg;
    }
  }
  brmsg.getResultFile().copyTo(output_file);
  FileAsserts.assertFileNumberOfLines(""String_Node_Str"" + ""String_Node_Str"",output_file,2);
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",output_file);
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",output_file);
  int expected_length=FileUtils.readBinaryFile(data1).length + FileUtils.readBinaryFile(data2).length;
  int actual_length=FileUtils.readBinaryFile(output_file).length;
  assertEquals(""String_Node_Str"",expected_length,actual_length);
  assertTrue(""String_Node_Str"",((TestRemoteFile)rf1).isDeleted());
  assertTrue(""String_Node_Str"",((TestRemoteFile)rf2).isDeleted());
}",0.999656475437994
89917,"/** 
 * Test that a BitarchiveServer is removed as listener of the ANY_BA queue when trying to upload a file that cannot fit in the archive. We currently don't resend the message, but just reply.
 */
public void testVisitUploadMessage(){
  SERVER1.mkdirs();
  Settings.set(ArchiveSettings.BITARCHIVE_MIN_SPACE_LEFT,""String_Node_Str"" + (FileUtils.getBytesFree(SERVER1) - 12000));
  Settings.set(CommonSettings.DIR_COMMONTEMPDIR,SERVER1.getAbsolutePath());
  Settings.set(ArchiveSettings.BITARCHIVE_SERVER_FILEDIR,BITARCHIVE1.getAbsolutePath());
  bas=BitarchiveServer.getInstance();
  ChannelID arcReposQ=Channels.getTheRepos();
  ChannelID anyBa=Channels.getAnyBa();
  JMSConnectionMockupMQ conn=(JMSConnectionMockupMQ)JMSConnectionFactory.getInstance();
  GenericMessageListener listener=new GenericMessageListener();
  conn.setListener(arcReposQ,listener);
  int expectedListeners=1;
  assertEquals(""String_Node_Str"" + anyBa + ""String_Node_Str""+ expectedListeners+ ""String_Node_Str"",expectedListeners,conn.getListeners(anyBa).size());
  File testFile=TestInfo.UPLOADMESSAGE_TESTFILE_1;
  RemoteFile rf=RemoteFileFactory.getInstance(testFile,false,false,true);
  UploadMessage msg=new UploadMessage(anyBa,arcReposQ,rf);
  bas.visit(msg);
  conn.waitForConcurrentTasksToFinish();
  expectedListeners=0;
  assertEquals(""String_Node_Str"" + anyBa + ""String_Node_Str""+ expectedListeners+ ""String_Node_Str"",expectedListeners,conn.getListeners(anyBa).size());
  assertTrue(""String_Node_Str"",listener.messagesReceived.size() >= 1);
  assertEquals(""String_Node_Str"" + ""String_Node_Str"",msg,listener.messagesReceived.get(0));
}","/** 
 * Test that a BitarchiveServer is removed as listener of the ANY_BA queue when trying to upload a file that cannot fit in the archive. We currently don't resend the message, but just reply.
 */
public void testVisitUploadMessage(){
  SERVER1.mkdirs();
  Settings.set(ArchiveSettings.BITARCHIVE_MIN_SPACE_LEFT,""String_Node_Str"" + (FileUtils.getBytesFree(SERVER1) - 12000));
  Settings.set(CommonSettings.DIR_COMMONTEMPDIR,SERVER1.getAbsolutePath());
  Settings.set(ArchiveSettings.BITARCHIVE_SERVER_FILEDIR,BITARCHIVE1.getAbsolutePath());
  bas=BitarchiveServer.getInstance();
  ChannelID arcReposQ=Channels.getTheRepos();
  ChannelID anyBa=Channels.getAnyBa();
  JMSConnectionMockupMQ conn=(JMSConnectionMockupMQ)JMSConnectionFactory.getInstance();
  GenericMessageListener listener=new GenericMessageListener();
  conn.setListener(arcReposQ,listener);
  int expectedListeners=1;
  assertEquals(""String_Node_Str"" + anyBa + ""String_Node_Str""+ expectedListeners+ ""String_Node_Str"",expectedListeners,conn.getListeners(anyBa).size());
  File testFile=TestInfo.UPLOADMESSAGE_TESTFILE_1;
  RemoteFile rf=RemoteFileFactory.getInstance(testFile,false,false,true);
  UploadMessage msg=new UploadMessage(anyBa,arcReposQ,rf);
  JMSConnectionMockupMQ.updateMsgID(msg,""String_Node_Str"");
  bas.visit(msg);
  conn.waitForConcurrentTasksToFinish();
  expectedListeners=0;
  assertEquals(""String_Node_Str"" + anyBa + ""String_Node_Str""+ expectedListeners+ ""String_Node_Str"",expectedListeners,conn.getListeners(anyBa).size());
  assertTrue(""String_Node_Str"",listener.messagesReceived.size() >= 1);
  assertEquals(""String_Node_Str"" + ""String_Node_Str"",msg,listener.messagesReceived.get(0));
}",0.9817961165048544
89918,"/** 
 * Test that a BitarchiveServer is removed as listener of the ANY_BA queue when a directory disappears. We currently don't resend the message, but just reply.
 */
public void testVisitUploadMessageDiskcrash(){
  Settings.set(CommonSettings.DIR_COMMONTEMPDIR,SERVER1.getAbsolutePath());
  Settings.set(ArchiveSettings.BITARCHIVE_SERVER_FILEDIR,BITARCHIVE1.getAbsolutePath());
  bas=BitarchiveServer.getInstance();
  ChannelID arcReposQ=Channels.getTheRepos();
  ChannelID anyBa=Channels.getAnyBa();
  JMSConnectionMockupMQ conn=(JMSConnectionMockupMQ)JMSConnectionFactory.getInstance();
  GenericMessageListener listener=new GenericMessageListener();
  conn.setListener(arcReposQ,listener);
  int expectedListeners=1;
  assertEquals(""String_Node_Str"" + anyBa + ""String_Node_Str""+ expectedListeners+ ""String_Node_Str"",expectedListeners,conn.getListeners(anyBa).size());
  File testFile=TestInfo.UPLOADMESSAGE_TESTFILE_1;
  RemoteFile rf=TestRemoteFile.getInstance(testFile,false,false,true);
  UploadMessage msg=new UploadMessage(anyBa,arcReposQ,rf);
  bas.visit(msg);
  conn.waitForConcurrentTasksToFinish();
  expectedListeners=1;
  assertEquals(""String_Node_Str"" + anyBa + ""String_Node_Str""+ expectedListeners+ ""String_Node_Str"",expectedListeners,conn.getListeners(anyBa).size());
  assertTrue(""String_Node_Str"",listener.messagesReceived.size() >= 1);
  FileUtils.removeRecursively(BITARCHIVE1);
  bas.visit(msg);
  conn.waitForConcurrentTasksToFinish();
  expectedListeners=0;
  assertEquals(""String_Node_Str"" + anyBa + ""String_Node_Str""+ expectedListeners+ ""String_Node_Str"",expectedListeners,conn.getListeners(anyBa).size());
  assertTrue(""String_Node_Str"",listener.messagesReceived.size() >= 2);
}","/** 
 * Test that a BitarchiveServer is removed as listener of the ANY_BA queue when a directory disappears. We currently don't resend the message, but just reply.
 */
public void testVisitUploadMessageDiskcrash(){
  Settings.set(CommonSettings.DIR_COMMONTEMPDIR,SERVER1.getAbsolutePath());
  Settings.set(ArchiveSettings.BITARCHIVE_SERVER_FILEDIR,BITARCHIVE1.getAbsolutePath());
  bas=BitarchiveServer.getInstance();
  ChannelID arcReposQ=Channels.getTheRepos();
  ChannelID anyBa=Channels.getAnyBa();
  JMSConnectionMockupMQ conn=(JMSConnectionMockupMQ)JMSConnectionFactory.getInstance();
  GenericMessageListener listener=new GenericMessageListener();
  conn.setListener(arcReposQ,listener);
  int expectedListeners=1;
  assertEquals(""String_Node_Str"" + anyBa + ""String_Node_Str""+ expectedListeners+ ""String_Node_Str"",expectedListeners,conn.getListeners(anyBa).size());
  File testFile=TestInfo.UPLOADMESSAGE_TESTFILE_1;
  RemoteFile rf=TestRemoteFile.getInstance(testFile,false,false,true);
  UploadMessage msg=new UploadMessage(anyBa,arcReposQ,rf);
  JMSConnectionMockupMQ.updateMsgID(msg,""String_Node_Str"");
  bas.visit(msg);
  conn.waitForConcurrentTasksToFinish();
  expectedListeners=1;
  assertEquals(""String_Node_Str"" + anyBa + ""String_Node_Str""+ expectedListeners+ ""String_Node_Str"",expectedListeners,conn.getListeners(anyBa).size());
  assertTrue(""String_Node_Str"",listener.messagesReceived.size() >= 1);
  FileUtils.removeRecursively(BITARCHIVE1);
  bas.visit(msg);
  conn.waitForConcurrentTasksToFinish();
  expectedListeners=0;
  assertEquals(""String_Node_Str"" + anyBa + ""String_Node_Str""+ expectedListeners+ ""String_Node_Str"",expectedListeners,conn.getListeners(anyBa).size());
  assertTrue(""String_Node_Str"",listener.messagesReceived.size() >= 2);
}",0.9827288428324698
89919,"public void testListenerNotRemovedOnErrors(){
  bas=BitarchiveServer.getInstance();
  ChannelID arcReposQ=Channels.getTheRepos();
  ChannelID anyBa=Channels.getAnyBa();
  JMSConnectionMockupMQ conn=(JMSConnectionMockupMQ)JMSConnectionFactory.getInstance();
  GenericMessageListener listener=new GenericMessageListener();
  conn.setListener(arcReposQ,listener);
  int expectedListeners=1;
  assertEquals(""String_Node_Str"" + anyBa + ""String_Node_Str""+ expectedListeners+ ""String_Node_Str"",expectedListeners,conn.getListeners(anyBa).size());
  File testFile=TestInfo.UPLOADMESSAGE_TESTFILE_1;
  RemoteFile rf=TestRemoteFile.getInstance(testFile,false,false,true);
  ((TestRemoteFile)rf).failsOnCopy=true;
  UploadMessage msg=new UploadMessage(anyBa,arcReposQ,rf);
  bas.visit(msg);
  conn.waitForConcurrentTasksToFinish();
  assertEquals(""String_Node_Str"" + anyBa + ""String_Node_Str""+ expectedListeners+ ""String_Node_Str"",expectedListeners,conn.getListeners(anyBa).size());
  assertTrue(""String_Node_Str"",listener.messagesReceived.size() >= 1);
  assertEquals(""String_Node_Str"",msg,listener.messagesReceived.get(0));
  assertFalse(""String_Node_Str"",((NetarkivetMessage)listener.messagesReceived.get(0)).isOk());
}","public void testListenerNotRemovedOnErrors(){
  bas=BitarchiveServer.getInstance();
  ChannelID arcReposQ=Channels.getTheRepos();
  ChannelID anyBa=Channels.getAnyBa();
  JMSConnectionMockupMQ conn=(JMSConnectionMockupMQ)JMSConnectionFactory.getInstance();
  GenericMessageListener listener=new GenericMessageListener();
  conn.setListener(arcReposQ,listener);
  int expectedListeners=1;
  assertEquals(""String_Node_Str"" + anyBa + ""String_Node_Str""+ expectedListeners+ ""String_Node_Str"",expectedListeners,conn.getListeners(anyBa).size());
  File testFile=TestInfo.UPLOADMESSAGE_TESTFILE_1;
  RemoteFile rf=TestRemoteFile.getInstance(testFile,false,false,true);
  ((TestRemoteFile)rf).failsOnCopy=true;
  UploadMessage msg=new UploadMessage(anyBa,arcReposQ,rf);
  JMSConnectionMockupMQ.updateMsgID(msg,""String_Node_Str"");
  bas.visit(msg);
  conn.waitForConcurrentTasksToFinish();
  assertEquals(""String_Node_Str"" + anyBa + ""String_Node_Str""+ expectedListeners+ ""String_Node_Str"",expectedListeners,conn.getListeners(anyBa).size());
  assertTrue(""String_Node_Str"",listener.messagesReceived.size() >= 1);
  assertEquals(""String_Node_Str"",msg,listener.messagesReceived.get(0));
  assertFalse(""String_Node_Str"",((NetarkivetMessage)listener.messagesReceived.get(0)).isOk());
}",0.9758064516129032
89920,"/** 
 * Test that a visit(RemoveAndGetMessage) call actually removes (moves) the file.
 * @throws Exception
 */
public void testVisitRemoveAndGetFileMessage() throws Exception {
  String arcFile=TestInfo.BA1_FILENAME;
  String dummyReplicaId=""String_Node_Str"";
  String checksum=TestInfo.BA1_CHECKSUM;
  String credentials=Settings.get(ArchiveSettings.ENVIRONMENT_THIS_CREDENTIALS);
  Settings.set(ArchiveSettings.BITARCHIVE_SERVER_FILEDIR,BITARCHIVE1.getAbsolutePath());
  Settings.set(CommonSettings.DIR_COMMONTEMPDIR,SERVER1.getAbsolutePath());
  bas=BitarchiveServer.getInstance();
  File baFile=TestInfo.BA1_ORG_FILE;
  File backupFile=TestInfo.BA1_ATTIC_FILE;
  assertTrue(""String_Node_Str"",baFile.exists());
  assertFalse(""String_Node_Str"",backupFile.exists());
  RemoveAndGetFileMessage m1=new RemoveAndGetFileMessage(arcFile + ""String_Node_Str"",dummyReplicaId,checksum,credentials);
  bas.visit(m1);
  assertTrue(""String_Node_Str"",baFile.exists());
  assertFalse(""String_Node_Str"",backupFile.exists());
  RemoveAndGetFileMessage m2=new RemoveAndGetFileMessage(arcFile,dummyReplicaId,checksum + ""String_Node_Str"",credentials);
  bas.visit(m2);
  assertTrue(""String_Node_Str"",baFile.exists());
  assertFalse(""String_Node_Str"",backupFile.exists());
  LogUtils.flushLogs(BitarchiveServer.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",TestInfo.LOG_FILE);
  assertFalse(""String_Node_Str"",m2.isOk());
  RemoveAndGetFileMessage m3=new RemoveAndGetFileMessage(arcFile,dummyReplicaId,checksum,credentials + ""String_Node_Str"");
  bas.visit(m3);
  assertTrue(""String_Node_Str"",baFile.exists());
  assertFalse(""String_Node_Str"",backupFile.exists());
  LogUtils.flushLogs(BitarchiveServer.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",TestInfo.LOG_FILE);
  assertFalse(""String_Node_Str"",m3.isOk());
  RemoveAndGetFileMessage m4=new RemoveAndGetFileMessage(arcFile,dummyReplicaId,checksum,credentials);
  long len=baFile.length();
  bas.visit(m4);
  MessageAsserts.assertMessageOk(""String_Node_Str"",m4);
  assertFalse(""String_Node_Str"",baFile.exists());
  assertTrue(""String_Node_Str"",backupFile.exists());
  LogUtils.flushLogs(BitarchiveServer.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",TestInfo.LOG_FILE);
  File f=m4.getData();
  assertNotNull(""String_Node_Str"",f);
  assertEquals(""String_Node_Str"",len,f.length());
  assertEquals(""String_Node_Str"",0,TestRemoteFile.remainingFiles().size());
}","/** 
 * Test that a visit(RemoveAndGetMessage) call actually removes (moves) the file.
 * @throws Exception
 */
public void testVisitRemoveAndGetFileMessage() throws Exception {
  String arcFile=TestInfo.BA1_FILENAME;
  String dummyReplicaId=""String_Node_Str"";
  String checksum=TestInfo.BA1_CHECKSUM;
  String credentials=Settings.get(ArchiveSettings.ENVIRONMENT_THIS_CREDENTIALS);
  Settings.set(ArchiveSettings.BITARCHIVE_SERVER_FILEDIR,BITARCHIVE1.getAbsolutePath());
  Settings.set(CommonSettings.DIR_COMMONTEMPDIR,SERVER1.getAbsolutePath());
  bas=BitarchiveServer.getInstance();
  File baFile=TestInfo.BA1_ORG_FILE;
  File backupFile=TestInfo.BA1_ATTIC_FILE;
  assertTrue(""String_Node_Str"",baFile.exists());
  assertFalse(""String_Node_Str"",backupFile.exists());
  RemoveAndGetFileMessage m1=new RemoveAndGetFileMessage(arcFile + ""String_Node_Str"",dummyReplicaId,checksum,credentials);
  bas.visit(m1);
  assertTrue(""String_Node_Str"",baFile.exists());
  assertFalse(""String_Node_Str"",backupFile.exists());
  RemoveAndGetFileMessage m2=new RemoveAndGetFileMessage(arcFile,dummyReplicaId,checksum + ""String_Node_Str"",credentials);
  JMSConnectionMockupMQ.updateMsgID(m2,""String_Node_Str"");
  bas.visit(m2);
  assertTrue(""String_Node_Str"",baFile.exists());
  assertFalse(""String_Node_Str"",backupFile.exists());
  LogUtils.flushLogs(BitarchiveServer.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",TestInfo.LOG_FILE);
  assertFalse(""String_Node_Str"",m2.isOk());
  RemoveAndGetFileMessage m3=new RemoveAndGetFileMessage(arcFile,dummyReplicaId,checksum,credentials + ""String_Node_Str"");
  JMSConnectionMockupMQ.updateMsgID(m3,""String_Node_Str"");
  bas.visit(m3);
  assertTrue(""String_Node_Str"",baFile.exists());
  assertFalse(""String_Node_Str"",backupFile.exists());
  LogUtils.flushLogs(BitarchiveServer.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",TestInfo.LOG_FILE);
  assertFalse(""String_Node_Str"",m3.isOk());
  RemoveAndGetFileMessage m4=new RemoveAndGetFileMessage(arcFile,dummyReplicaId,checksum,credentials);
  long len=baFile.length();
  JMSConnectionMockupMQ.updateMsgID(m4,""String_Node_Str"");
  bas.visit(m4);
  MessageAsserts.assertMessageOk(""String_Node_Str"",m4);
  assertFalse(""String_Node_Str"",baFile.exists());
  assertTrue(""String_Node_Str"",backupFile.exists());
  LogUtils.flushLogs(BitarchiveServer.class.getName());
  FileAsserts.assertFileContains(""String_Node_Str"",""String_Node_Str"",TestInfo.LOG_FILE);
  File f=m4.getData();
  assertNotNull(""String_Node_Str"",f);
  assertEquals(""String_Node_Str"",len,f.length());
  assertEquals(""String_Node_Str"",0,TestRemoteFile.remainingFiles().size());
}",0.9660203493952774
89921,"/** 
 * Verify that visit() - throws exception on null message or message that is not ok - returns a non-ok message if handler fails with exception or no handler registered
 */
public void testVisitFailures() throws InterruptedException {
  server=IndexRequestServer.getInstance();
  mmfbc.setMode(MockupMultiFileBasedCache.Mode.FAILING);
  server.setHandler(RequestType.CDX,mmfbc);
  try {
    server.visit((IndexRequestMessage)null);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  IndexRequestMessage irMsg=new IndexRequestMessage(RequestType.CDX,JOB_SET);
  GenericMessageListener listener=new GenericMessageListener();
  JMSConnectionMockupMQ conn=(JMSConnectionMockupMQ)JMSConnectionFactory.getInstance();
  conn.setListener(irMsg.getReplyTo(),listener);
  server.visit(irMsg);
  conn.waitForConcurrentTasksToFinish();
  Thread.sleep(200);
  conn.waitForConcurrentTasksToFinish();
  assertEquals(""String_Node_Str"",1,listener.messagesReceived.size());
  assertTrue(""String_Node_Str"",listener.messagesReceived.get(0) instanceof IndexRequestMessage);
  IndexRequestMessage msg=(IndexRequestMessage)listener.messagesReceived.get(0);
  assertEquals(""String_Node_Str"",irMsg.getID(),msg.getID());
  assertFalse(""String_Node_Str"",msg.isOk());
  irMsg=new IndexRequestMessage(RequestType.DEDUP_CRAWL_LOG,JOB_SET);
  server.visit(irMsg);
  conn.waitForConcurrentTasksToFinish();
  Thread.sleep(200);
  conn.waitForConcurrentTasksToFinish();
  assertEquals(""String_Node_Str"",2,listener.messagesReceived.size());
  assertTrue(""String_Node_Str"",listener.messagesReceived.get(1) instanceof IndexRequestMessage);
  msg=(IndexRequestMessage)listener.messagesReceived.get(1);
  assertEquals(""String_Node_Str"",irMsg.getID(),msg.getID());
  assertFalse(""String_Node_Str"",msg.isOk());
  irMsg=new IndexRequestMessage(RequestType.DEDUP_CRAWL_LOG,JOB_SET);
}","/** 
 * Verify that visit() - throws exception on null message or message that is not ok - returns a non-ok message if handler fails with exception or no handler registered
 */
public void testVisitFailures() throws InterruptedException {
  server=IndexRequestServer.getInstance();
  mmfbc.setMode(MockupMultiFileBasedCache.Mode.FAILING);
  server.setHandler(RequestType.CDX,mmfbc);
  try {
    server.visit((IndexRequestMessage)null);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  IndexRequestMessage irMsg=new IndexRequestMessage(RequestType.CDX,JOB_SET);
  JMSConnectionMockupMQ.updateMsgID(irMsg,""String_Node_Str"");
  GenericMessageListener listener=new GenericMessageListener();
  JMSConnectionMockupMQ conn=(JMSConnectionMockupMQ)JMSConnectionFactory.getInstance();
  conn.setListener(irMsg.getReplyTo(),listener);
  server.visit(irMsg);
  conn.waitForConcurrentTasksToFinish();
  Thread.sleep(200);
  conn.waitForConcurrentTasksToFinish();
  assertEquals(""String_Node_Str"",1,listener.messagesReceived.size());
  assertTrue(""String_Node_Str"",listener.messagesReceived.get(0) instanceof IndexRequestMessage);
  IndexRequestMessage msg=(IndexRequestMessage)listener.messagesReceived.get(0);
  assertEquals(""String_Node_Str"",irMsg.getID(),msg.getID());
  assertFalse(""String_Node_Str"",msg.isOk());
  irMsg=new IndexRequestMessage(RequestType.DEDUP_CRAWL_LOG,JOB_SET);
  JMSConnectionMockupMQ.updateMsgID(irMsg,""String_Node_Str"");
  server.visit(irMsg);
  conn.waitForConcurrentTasksToFinish();
  Thread.sleep(200);
  conn.waitForConcurrentTasksToFinish();
  assertEquals(""String_Node_Str"",2,listener.messagesReceived.size());
  assertTrue(""String_Node_Str"",listener.messagesReceived.get(1) instanceof IndexRequestMessage);
  msg=(IndexRequestMessage)listener.messagesReceived.get(1);
  assertEquals(""String_Node_Str"",irMsg.getID(),msg.getID());
  assertFalse(""String_Node_Str"",msg.isOk());
  irMsg=new IndexRequestMessage(RequestType.DEDUP_CRAWL_LOG,JOB_SET);
  JMSConnectionMockupMQ.updateMsgID(irMsg,""String_Node_Str"");
}",0.9525025536261492
89922,"/** 
 * Retrieves the names of all the files in the replica through a  GetAllFilenamesMessage. This is the checksum archive alternative to running a FilelistBatchJob. 
 * @param replicaId The id of the replica from which the list of filenamesshould be retrieved.
 * @return A list of all the filenames within the archive of the given replica.
 * @see dk.netarkivet.archive.checksum.GetAllFilenamesMessage
 */
public File getAllFilenames(String replicaId){
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  log.debug(""String_Node_Str"" + replicaId + ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetAllFilenamesMessage gafMsg=new GetAllFilenamesMessage(Channels.getTheRepos(),replyQ,replicaId);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(gafMsg,getTimeout);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / 1000) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    log.info(""String_Node_Str"" + (getTimeout / 1000) + ""String_Node_Str"");
    return null;
  }
  GetAllFilenamesMessage replyCSMsg;
  try {
    replyCSMsg=(GetAllFilenamesMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    String errorMsg=""String_Node_Str"" + replyNetMsg;
    log.warn(errorMsg,e);
    throw new IOFailure(errorMsg,e);
  }
  try {
    File result=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    replyCSMsg.getData(result);
    return result;
  }
 catch (  IOException e) {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + replyCSMsg;
    log.warn(errMsg);
    throw new IOFailure(errMsg,e);
  }
}","/** 
 * Retrieves the names of all the files in the replica through a  GetAllFilenamesMessage. This is the checksum archive alternative to running a FilelistBatchJob. 
 * @param replicaId The id of the replica from which the list of filenamesshould be retrieved.
 * @return A list of all the filenames within the archive of the given replica.
 * @see dk.netarkivet.archive.checksum.distribute.GetAllFilenamesMessage
 */
public File getAllFilenames(String replicaId){
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  log.debug(""String_Node_Str"" + replicaId + ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetAllFilenamesMessage gafMsg=new GetAllFilenamesMessage(Channels.getTheRepos(),replyQ,replicaId);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(gafMsg,getTimeout);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / 1000) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    log.info(""String_Node_Str"" + (getTimeout / 1000) + ""String_Node_Str"");
    return null;
  }
  GetAllFilenamesMessage replyCSMsg;
  try {
    replyCSMsg=(GetAllFilenamesMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    String errorMsg=""String_Node_Str"" + replyNetMsg;
    log.warn(errorMsg,e);
    throw new IOFailure(errorMsg,e);
  }
  try {
    File result=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    replyCSMsg.getData(result);
    return result;
  }
 catch (  IOException e) {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + replyCSMsg;
    log.warn(errMsg);
    throw new IOFailure(errMsg,e);
  }
}",0.996595481275147
89923,"/** 
 * Retrieves all the checksum from the replica through a  GetAllChecksumMessage. This is the checksum archive alternative to running a ChecksumBatchJob. 
 * @param replicaId The id of the replica from which the checksums should be retrieved.
 * @return A list of ChecksumEntries which is the results of the GetAllChecksumMessage.
 * @see dk.netarkivet.archive.checksum.GetAllChecksumMessage
 */
public File getAllChecksums(String replicaId){
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  log.debug(""String_Node_Str"" + replicaId + ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetAllChecksumMessage gacMsg=new GetAllChecksumMessage(Channels.getTheRepos(),replyQ,replicaId);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(gacMsg,getTimeout);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / 1000) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    log.info(""String_Node_Str"" + (getTimeout / 1000) + ""String_Node_Str"");
    return null;
  }
  GetAllChecksumMessage replyCSMsg;
  try {
    replyCSMsg=(GetAllChecksumMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    String errorMsg=""String_Node_Str"" + replyNetMsg;
    log.warn(errorMsg,e);
    throw new IOFailure(errorMsg,e);
  }
  try {
    File result=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    replyCSMsg.getData(result);
    return result;
  }
 catch (  IOException e) {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + replyCSMsg;
    log.warn(errMsg);
    throw new IOFailure(errMsg,e);
  }
}","/** 
 * Retrieves all the checksum from the replica through a  GetAllChecksumMessage. This is the checksum archive alternative to running a ChecksumBatchJob. 
 * @param replicaId The id of the replica from which the checksums should be retrieved.
 * @return A list of ChecksumEntries which is the results of the GetAllChecksumMessage.
 * @see dk.netarkivet.archive.checksum.distribute.GetAllChecksumMessage
 */
public File getAllChecksums(String replicaId){
  ArgumentNotValid.checkNotNullOrEmpty(replicaId,""String_Node_Str"");
  log.debug(""String_Node_Str"" + replicaId + ""String_Node_Str"");
  long start=System.currentTimeMillis();
  GetAllChecksumMessage gacMsg=new GetAllChecksumMessage(Channels.getTheRepos(),replyQ,replicaId);
  NetarkivetMessage replyNetMsg=sendAndWaitForOneReply(gacMsg,getTimeout);
  long timePassed=System.currentTimeMillis() - start;
  log.debug(""String_Node_Str"" + (timePassed / 1000) + ""String_Node_Str"");
  if (replyNetMsg == null) {
    log.info(""String_Node_Str"" + (getTimeout / 1000) + ""String_Node_Str"");
    return null;
  }
  GetAllChecksumMessage replyCSMsg;
  try {
    replyCSMsg=(GetAllChecksumMessage)replyNetMsg;
  }
 catch (  ClassCastException e) {
    String errorMsg=""String_Node_Str"" + replyNetMsg;
    log.warn(errorMsg,e);
    throw new IOFailure(errorMsg,e);
  }
  try {
    File result=File.createTempFile(""String_Node_Str"",""String_Node_Str"");
    replyCSMsg.getData(result);
    return result;
  }
 catch (  IOException e) {
    String errMsg=""String_Node_Str"" + ""String_Node_Str"" + replyCSMsg;
    log.warn(errMsg);
    throw new IOFailure(errMsg,e);
  }
}",0.9965678627145086
89924,"/** 
 * The message for handling the results of the GetChecksumMessage.
 * @param msg The message containing the checksum of a specific file.
 */
public void onChecksumReply(GetChecksumMessage msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  log.debug(""String_Node_Str"" + msg.toString());
  if (!outstandingChecksumFiles.containsKey(msg.getID())) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"" + msg.getReplyOfId() + ""String_Node_Str""+ msg.toString()+ ""String_Node_Str""+ outstandingChecksumFiles.keySet().toString());
    return;
  }
  String arcfileName=outstandingChecksumFiles.remove(msg.getReplyOfId());
  if (!msg.isOk()) {
    log.warn(""String_Node_Str"" + msg.getID() + ""String_Node_Str""+ ""String_Node_Str""+ msg.getErrMsg()+ ""String_Node_Str""+ ""String_Node_Str"");
  }
  String orgChecksum=ad.getCheckSum(arcfileName);
  String repChannelName=resolveReplicaChannel(msg.getReplyTo().getName());
  String reportedChecksum=msg.getChecksum();
  if (orgChecksum.equals(reportedChecksum) && !reportedChecksum.isEmpty()) {
    ad.setState(arcfileName,repChannelName,BitArchiveStoreState.UPLOAD_COMPLETED);
  }
 else {
    log.warn(""String_Node_Str"" + arcfileName + ""String_Node_Str""+ ""String_Node_Str""+ orgChecksum+ ""String_Node_Str""+ reportedChecksum+ ""String_Node_Str"");
    ad.setState(arcfileName,repChannelName,BitArchiveStoreState.UPLOAD_FAILED);
  }
  considerReplyingOnStore(arcfileName);
  log.debug(""String_Node_Str"");
}","/** 
 * The message for handling the results of the GetChecksumMessage.
 * @param msg The message containing the checksum of a specific file.
 */
public void onChecksumReply(GetChecksumMessage msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  log.debug(""String_Node_Str"" + msg.toString());
  if (!outstandingChecksumFiles.containsKey(msg.getID())) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"" + msg.getReplyOfId() + ""String_Node_Str""+ msg.toString()+ ""String_Node_Str""+ outstandingChecksumFiles.keySet().toString());
    return;
  }
  String arcfileName=outstandingChecksumFiles.remove(msg.getID());
  if (!msg.isOk()) {
    log.warn(""String_Node_Str"" + msg.getID() + ""String_Node_Str""+ ""String_Node_Str""+ msg.getErrMsg()+ ""String_Node_Str""+ ""String_Node_Str"");
  }
  String orgChecksum=ad.getCheckSum(arcfileName);
  String repChannelName=resolveReplicaChannel(msg.getReplyTo().getName());
  String reportedChecksum=msg.getChecksum();
  if (orgChecksum.equals(reportedChecksum) && !reportedChecksum.isEmpty()) {
    ad.setState(arcfileName,repChannelName,BitArchiveStoreState.UPLOAD_COMPLETED);
  }
 else {
    log.warn(""String_Node_Str"" + arcfileName + ""String_Node_Str""+ ""String_Node_Str""+ orgChecksum+ ""String_Node_Str""+ reportedChecksum+ ""String_Node_Str"");
    ad.setState(arcfileName,repChannelName,BitArchiveStoreState.UPLOAD_FAILED);
  }
  considerReplyingOnStore(arcfileName);
  log.debug(""String_Node_Str"");
}",0.99688689034936
89925,"/** 
 * Forwards a getfile message to requested bitarchive replica. Note that this circumvents the ArcRepository entirely and that the reply goes directly back to whoever set the message.
 * @param msg the message to be processed by the get command.
 * @throws ArgumentNotValid If one of the arguments are null.
 */
public void visit(GetFileMessage msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  ReplicaClient bc=ar.getReplicaClientFromReplicaId(msg.getReplicaId());
  try {
    bc.getFile(msg);
  }
 catch (  Throwable t) {
    log.warn(""String_Node_Str"",t);
    msg.setNotOk(t);
    JMSConnectionFactory.getInstance().reply(msg);
  }
}","/** 
 * Method for retrieving all the checksums from a replica. Currently only checksum replicas.
 */
public void visit(GetAllChecksumMessage msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  ReplicaClient cc=ar.getReplicaClientFromReplicaId(msg.getReplicaId());
  try {
    cc.getAllChecksums(msg);
  }
 catch (  Throwable t) {
    log.warn(""String_Node_Str"",t);
    msg.setNotOk(t);
    JMSConnectionFactory.getInstance().reply(msg);
  }
}",0.6203787195671776
89926,"@Override public GetAllFilenamesMessage getAllFilenames(){
  throw new NotImplementedException(""String_Node_Str"");
}","@Override public void getAllFilenames(GetAllFilenamesMessage msg){
  throw new NotImplementedException(""String_Node_Str"" + msg.toString() + ""String_Node_Str"");
}",0.7292418772563177
89927,"/** 
 * Forward the message to ALL_BA.
 * @param msg the message to forward
 */
public void removeAndGetFile(RemoveAndGetFileMessage msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  con.resend(msg,this.all_ba);
}","/** 
 * Forward the message to ALL_BA.
 * @param msg the message to forward.
 */
public void removeAndGetFile(RemoveAndGetFileMessage msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  con.resend(msg,this.all_ba);
}",0.9978021978021978
89928,"/** 
 * Submit an upload request to the bitarchive.
 * @param rf The file to upload
 * @throws IOFailure If access to file denied
 * @throws ArgumentNotValid    if arcfile is null
 */
public void upload(RemoteFile rf){
  ArgumentNotValid.checkNotNull(rf,""String_Node_Str"");
  UploadMessage up=new UploadMessage(any_ba,clientId,rf);
  log.debug(""String_Node_Str"" + up.toString());
  con.send(up);
}","/** 
 * Submit an upload request to the bitarchive.
 * @param rf The file to upload.
 * @throws IOFailure If access to file denied.
 * @throws ArgumentNotValid If arcfile is null.
 */
public void upload(RemoteFile rf){
  ArgumentNotValid.checkNotNull(rf,""String_Node_Str"");
  UploadMessage up=new UploadMessage(any_ba,clientId,rf);
  log.debug(""String_Node_Str"" + up.toString());
  con.send(up);
}",0.947103274559194
89929,"/** 
 * Submit a batch job to the archive. This is used by the ArcRepository when it needs to run batch jobs for its own reasons (i.e. when checksumming a file as part of the Store operation.
 * @param replyChannel The channel that the reply of this job shouldbe sent to.
 * @param job The job that should be run on the bit archivehandled by this client.
 * @return The submitted message
 * @throws ArgumentNotValid if any parameter was null.
 * @throws IOFailure if sending the batch message did not succeed.
 */
public BatchMessage batch(ChannelID replyChannel,FileBatchJob job) throws ArgumentNotValid, IOFailure {
  ArgumentNotValid.checkNotNull(replyChannel,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(job,""String_Node_Str"");
  BatchMessage bMsg=new BatchMessage(this.the_bamon,replyChannel,job,""String_Node_Str"" + ""String_Node_Str"");
  con.send(bMsg);
  return bMsg;
}","/** 
 * Submit a batch job to the archive. This is used by the ArcRepository when it needs to run batch jobs for its own reasons (i.e. when checksumming a file as part of the Store operation.
 * @param replyChannel The channel that the reply of this job should be sent to.
 * @param job The job that should be run on the bit archive handled by thisclient.
 * @return The submitted message.
 * @throws ArgumentNotValid If any parameter was null.
 * @throws IOFailure If sending the batch message did not succeed.
 */
public BatchMessage batch(ChannelID replyChannel,FileBatchJob job) throws ArgumentNotValid, IOFailure {
  ArgumentNotValid.checkNotNull(replyChannel,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(job,""String_Node_Str"");
  BatchMessage bMsg=new BatchMessage(this.the_bamon,replyChannel,job,""String_Node_Str"" + ""String_Node_Str"");
  con.send(bMsg);
  return bMsg;
}",0.9954699886749716
89930,"/** 
 * This creates a batch job for retrieving the checksums.
 * @param arcName The name of the arcfile to retrieve the checksum from.
 */
@Override public GetChecksumMessage getChecksum(String arcName){
  throw new NotImplementedException(""String_Node_Str"");
}","/** 
 * This should creates a batch job for retrieving the checksum of the  wanted files.
 * @param msg The message.
 */
@Override public void getChecksum(GetChecksumMessage msg){
  throw new NotImplementedException(""String_Node_Str"");
}",0.7214428857715431
89931,"/** 
 * Submit an already constructed getfile message to the archive.
 * @param msg get file message to retrieve
 */
public void getFile(GetFileMessage msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  log.debug(""String_Node_Str"" + msg + ""String_Node_Str"");
  con.resend(msg,this.all_ba);
}","/** 
 * Submit an already constructed getfile message to the archive.
 * @param msg get file message to retrieve.
 */
public void getFile(GetFileMessage msg){
  ArgumentNotValid.checkNotNull(msg,""String_Node_Str"");
  log.debug(""String_Node_Str"" + msg + ""String_Node_Str"");
  con.resend(msg,this.all_ba);
}",0.9983579638752051
89932,"/** 
 * Create a single mbean object. This is a helper method for the constructor taking a domain, which take the domain from a preconstructed ObjectName and replaces the nameProperties with the properties from the given object name. Use this if you have an object name created already, which you wish to use.
 * @param name        The object name to register under.
 * @param o           The object to register.
 * @param asInterface The interface o should implement.
 * @param mBeanServer The mbean server to register o in.
 * @throws ArgumentNotValid on any null parameter.
 */
public SingleMBeanObject(ObjectName name,I o,Class<I> asInterface,MBeanServer mBeanServer){
  this(name.getDomain(),o,asInterface,mBeanServer);
  nameProperties.clear();
  nameProperties.putAll((Hashtable<String,String>)name.getKeyPropertyList());
}","/** 
 * Create a single mbean object. This is a helper method for the constructor taking a domain, which take the domain from a preconstructed ObjectName and replaces the nameProperties with the properties from the given object name. Use this if you have an object name created already, which you wish to use.
 * @param name        The object name to register under.
 * @param o           The object to register.
 * @param asInterface The interface o should implement.
 * @param mBeanServer The mbean server to register o in.
 * @throws ArgumentNotValid on any null parameter.
 */
public SingleMBeanObject(ObjectName name,I o,Class<I> asInterface,MBeanServer mBeanServer){
  this(name.getDomain(),o,asInterface,mBeanServer);
  nameProperties.clear();
  nameProperties.putAll(name.getKeyPropertyList());
}",0.9840881272949816
89933,"/** 
 * Properties for the ObjectName name. Update these before registering. On construction, initialised with location, hostname, httpport, priority, replica applicationname, applicationinstid.
 */
public Hashtable<String,String> getNameProperties(){
  return nameProperties;
}","/** 
 * Properties for the ObjectName name. Update these before registering. On construction, initialised with location, hostname, httpport, priority, replica, applicationname, applicationinstid.
 * @return Hashtable of the object name properties.
 */
public Hashtable<String,String> getNameProperties(){
  return nameProperties;
}",0.9129720853858784
89934,"/** 
 * @return the name
 */
public ObjectName getName(){
  return name;
}","/** 
 * @return the name 
 */
public ObjectName getName(){
  return name;
}",0.9932885906040269
89935,"/** 
 * Registers this object as a standard MBean, with a name generated from domain given in constructor and the nameProperties hashTable.
 * @throws IllegalState if bean is already registered.
 * @throws IOFailure on trouble registering.
 */
public void register(){
  try {
    name=new ObjectName(domain,nameProperties);
    mBeanServer.registerMBean(new StandardMBean(exposedObject,asInterface),name);
    log.trace(""String_Node_Str"" + name + ""String_Node_Str"");
  }
 catch (  InstanceAlreadyExistsException e) {
    String msg=""String_Node_Str"" + name + ""String_Node_Str""+ ""String_Node_Str"";
    log.warn(msg,e);
    throw new IllegalState(msg,e);
  }
catch (  JMException e) {
    throw new IOFailure(""String_Node_Str"" + name + ""String_Node_Str"",e);
  }
}","/** 
 * Registers this object as a standard MBean, with a name generated from domain given in constructor and the nameProperties hashTable.
 * @throws IllegalState if bean is already registered.
 * @throws IOFailure    on trouble registering.
 */
public void register(){
  try {
    name=new ObjectName(domain,nameProperties);
    mBeanServer.registerMBean(new StandardMBean(exposedObject,asInterface),name);
    log.trace(""String_Node_Str"" + name + ""String_Node_Str"");
  }
 catch (  InstanceAlreadyExistsException e) {
    String msg=""String_Node_Str"" + name + ""String_Node_Str""+ ""String_Node_Str"";
    log.warn(msg,e);
    throw new IllegalState(msg,e);
  }
catch (  JMException e) {
    throw new IOFailure(""String_Node_Str"" + name + ""String_Node_Str"",e);
  }
}",0.998032786885246
89936,"/** 
 * hashCode method, that overrides the Object.hashCode method.
 * @see Object#hashCode()
 * @return the hashcode for this object
 */
public int hashCode(){
  int result;
  result=server.hashCode();
  result=31 * result + port;
  result=31 * result + rmiPort;
  result=31 * result + userName.hashCode();
  result=31 * result + password.hashCode();
  return result;
}","/** 
 * hashCode method, that overrides the Object.hashCode method.
 * @return the hashcode for this object
 * @see Object#hashCode()
 */
public int hashCode(){
  int result;
  result=server.hashCode();
  result=31 * result + port;
  result=31 * result + rmiPort;
  result=31 * result + userName.hashCode();
  result=31 * result + password.hashCode();
  return result;
}",0.8918918918918919
89937,"/** 
 * Equals method, that overrides the Object.equals method.
 * @see Object#equals(java.lang.Object)
 * @param o anObject
 * @return true, if o is equal to this object; else false
 */
public boolean equals(Object o){
  if (this == o)   return true;
  if (o == null || getClass() != o.getClass())   return false;
  CacheKey cacheKey=(CacheKey)o;
  if (port != cacheKey.port)   return false;
  if (rmiPort != cacheKey.rmiPort)   return false;
  if (!password.equals(cacheKey.password))   return false;
  if (!server.equals(cacheKey.server))   return false;
  if (!userName.equals(cacheKey.userName))   return false;
  return true;
}","/** 
 * Equals method, that overrides the Object.equals method.
 * @param o anObject
 * @return true, if o is equal to this object; else false
 * @see Object#equals(java.lang.Object)
 */
public boolean equals(Object o){
  if (this == o) {
    return true;
  }
  if (o == null || getClass() != o.getClass()) {
    return false;
  }
  CacheKey cacheKey=(CacheKey)o;
  if (port != cacheKey.port) {
    return false;
  }
  if (rmiPort != cacheKey.rmiPort) {
    return false;
  }
  if (!password.equals(cacheKey.password)) {
    return false;
  }
  if (!server.equals(cacheKey.server)) {
    return false;
  }
  if (!userName.equals(cacheKey.userName)) {
    return false;
  }
  return true;
}",0.649016641452345
89938,"/** 
 * Constructor for this class. 
 */
public CacheKey(String server,int port,int rmiPort,String userName,String password){
  this.server=server;
  this.port=port;
  this.rmiPort=rmiPort;
  this.userName=userName;
  this.password=password;
}","/** 
 * Constructor for this class.
 * @param server   The server name.
 * @param port     The JMX port number.
 * @param rmiPort  The RMI callback number.
 * @param userName The JMX user name.
 * @param password The JMX password.
 */
public CacheKey(String server,int port,int rmiPort,String userName,String password){
  this.server=server;
  this.port=port;
  this.rmiPort=rmiPort;
  this.userName=userName;
  this.password=password;
}",0.711764705882353
89939,"/** 
 * Check that IOFailure is thrown by the JMXHeritrixController if the JMXPasswordFile does not exist / is hidden / unreadable / impossible to open for other reasons. 
 */
public void testIOFailureThrown() throws IOException {
  File passwordFile=new File(TestInfo.WORKING_DIR,""String_Node_Str"");
  FileUtils.remove(passwordFile);
  File tempDir=mtf.newTmpDir();
  hl=getHeritrixLauncher(TestInfo.DEFAULT_ORDERXML_FILE,TestInfo.SEEDS_FILE,tempDir,passwordFile,new File(Settings.get(CommonSettings.JMX_ACCESS_FILE)));
  try {
    hl.doCrawl();
    fail(""String_Node_Str"" + ""String_Node_Str"" + passwordFile.getAbsolutePath() + ""String_Node_Str"");
  }
 catch (  IOFailure iof) {
    assertTrue(""String_Node_Str"" + iof,iof.getMessage().contains(""String_Node_Str""));
  }
catch (  Exception ex) {
    if (!ex.getCause().getMessage().contains(""String_Node_Str"" + passwordFile.getAbsolutePath() + ""String_Node_Str"")) {
      fail(""String_Node_Str"" + ""String_Node_Str"" + passwordFile.getAbsolutePath() + ""String_Node_Str""+ ex);
    }
  }
}","/** 
 * Check that IOFailure is thrown by the JMXHeritrixController if the JMXPasswordFile does not exist / is hidden / unreadable / impossible to open for other reasons. 
 */
public void testIOFailureThrown() throws IOException {
  File passwordFile=new File(TestInfo.WORKING_DIR,""String_Node_Str"");
  FileUtils.remove(passwordFile);
  File tempDir=mtf.newTmpDir();
  hl=getHeritrixLauncher(TestInfo.DEFAULT_ORDERXML_FILE,TestInfo.SEEDS_FILE,tempDir,passwordFile,new File(Settings.get(CommonSettings.JMX_ACCESS_FILE)));
  try {
    hl.doCrawl();
    fail(""String_Node_Str"" + ""String_Node_Str"" + passwordFile.getAbsolutePath() + ""String_Node_Str"");
  }
 catch (  IOFailure iof) {
    assertTrue(""String_Node_Str"" + iof,iof.getMessage().contains(""String_Node_Str""));
  }
catch (  Exception ex) {
    if (!ex.getCause().getMessage().contains(""String_Node_Str"" + passwordFile.getAbsolutePath() + ""String_Node_Str"")) {
      ex.printStackTrace();
      fail(""String_Node_Str"" + ""String_Node_Str"" + passwordFile.getAbsolutePath() + ""String_Node_Str""+ ExceptionUtils.getStackTrace(ex));
    }
  }
}",0.9717779868297272
89940,"public CorrectMessage(ChannelID to,ChannelID replyTo,String checksum,RemoteFile file){
  super(to,replyTo,CORRECT_MESSAGE_PREFIX);
  ArgumentNotValid.checkNotNull(file,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(checksum,""String_Node_Str"");
  this.theChecksum=checksum;
  this.theRemoteFile=file;
  this.arcFilename=file.getName();
}","public CorrectMessage(ChannelID to,ChannelID replyTo,String checksum,RemoteFile file){
  super(to,replyTo);
  ArgumentNotValid.checkNotNull(file,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(checksum,""String_Node_Str"");
  this.theChecksum=checksum;
  this.theRemoteFile=file;
  this.arcFilename=file.getName();
}",0.9658246656760772
89941,"/** 
 * Constructor.
 * @param to Where this message is headed.
 * @param replyTo Where the reply on this message is sent.
 * @param repId The replica where the job involved in this message isto be performed.
 */
public GetAllChecksumMessage(ChannelID to,ChannelID replyTo,String repId){
  super(to,replyTo,GET_ALL_CHECKSUM_MESSAGE_PREFIX);
  this.replicaId=repId;
}","/** 
 * Constructor.
 * @param to Where this message is headed.
 * @param replyTo Where the reply on this message is sent.
 * @param repId The replica where the job involved in this message isto be performed.
 */
public GetAllChecksumMessage(ChannelID to,ChannelID replyTo,String repId){
  super(to,replyTo);
  this.replicaId=repId;
}",0.9542857142857144
89942,"/** 
 * Constructor.
 * @param to The channel the message is sent to.
 * @param replyTo The channel the reply is sent to.
 */
public GetAllFilenamesMessage(ChannelID to,ChannelID replyTo,String repId){
  super(to,replyTo,IDPREFIX);
  this.replicaId=repId;
}","/** 
 * Constructor.
 * @param to The channel the message is sent to.
 * @param replyTo The channel the reply is sent to.
 */
public GetAllFilenamesMessage(ChannelID to,ChannelID replyTo,String repId){
  super(to,replyTo);
  this.replicaId=repId;
}",0.9821782178217822
89943,"/** 
 * Constructor.
 * @param to Where this message should be sent.
 * @param replyTo Where the reply for this message should be sent.
 * @param filename The name of the file.
 */
protected GetChecksumMessage(ChannelID to,ChannelID replyTo,String filename){
  super(to,replyTo,GET_CHECKSUM_MESSAGE_PREFIX);
  ArgumentNotValid.checkNotNull(to,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replyTo,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  arcFilename=filename;
}","/** 
 * Constructor.
 * @param to Where this message should be sent.
 * @param replyTo Where the reply for this message should be sent.
 * @param filename The name of the file.
 */
protected GetChecksumMessage(ChannelID to,ChannelID replyTo,String filename){
  super(to,replyTo);
  ArgumentNotValid.checkNotNull(to,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replyTo,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(filename,""String_Node_Str"");
  arcFilename=filename;
}",0.9721115537848606
89944,"/** 
 * Test a valid BitarchiveClient is returned.
 */
public void testGetBitarchiveClientFromLocationName(){
  ArcRepository a=ArcRepository.getInstance();
  String[] locations=Settings.getAll(CommonSettings.REPLICA_IDS);
  for (int n=0; n < locations.length; n++) {
    BitarchiveClient bc=a.getBitarchiveClientFromReplicaId(locations[n]);
    assertNotNull(""String_Node_Str"",bc);
  }
}","/** 
 * Test a valid BitarchiveClient is returned.
 */
public void testGetBitarchiveClientFromLocationName(){
  ArcRepository a=ArcRepository.getInstance();
  String[] locations=Settings.getAll(CommonSettings.REPLICA_IDS);
  for (int n=0; n < locations.length; n++) {
    ReplicaClient bc=a.getReplicaClientFromReplicaId(locations[n]);
    assertNotNull(""String_Node_Str"",bc);
  }
}",0.9558441558441558
89945,"/** 
 * Test parameters.
 */
public void testGetBitarchiveClientFromLocationNameParameters(){
  ArcRepository a=ArcRepository.getInstance();
  try {
    a.getBitarchiveClientFromReplicaId(null);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  try {
    a.getBitarchiveClientFromReplicaId(""String_Node_Str"");
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
}","/** 
 * Test parameters.
 */
public void testGetBitarchiveClientFromLocationNameParameters(){
  ArcRepository a=ArcRepository.getInstance();
  try {
    a.getReplicaClientFromReplicaId(null);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  try {
    a.getReplicaClientFromReplicaId(""String_Node_Str"");
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
}",0.5100502512562815
89946,"/** 
 * Writes metadata to an ARC with the following name: <jobid>-preharvest-metadata-1.arc.
 * @param harvestJob a given Job.
 * @param metadata   the list of metadata entries to write to ARC.
 * @param crawlDir   the directory, where the ARC-file will be written.
 * @throws IOFailure If there are errors in writing the ARC file.
 */
private void writePreharvestMetadata(Job harvestJob,List<MetadataEntry> metadata,File crawlDir) throws IOFailure {
  if (metadata.size() == 0) {
    return;
  }
  File arcFile=new File(crawlDir,HarvestDocumentation.getPreharvestMetadataARCFileName(harvestJob.getJobID()));
  try {
    ARCWriter aw=null;
    try {
      aw=ARCUtils.createARCWriter(arcFile);
      for (      MetadataEntry m : metadata) {
        ByteArrayOutputStream baos=new ByteArrayOutputStream();
        baos.write(m.getData());
        aw.write(m.getURL(),m.getMimeType(),SystemUtils.getLocalIP(),System.currentTimeMillis(),baos.size(),baos);
      }
    }
  finally {
      try {
        if (aw != null) {
          aw.close();
        }
      }
 catch (      IOException e) {
        log.warn(""String_Node_Str"" + aw.getFile().getAbsolutePath() + ""String_Node_Str"",e);
      }
    }
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"" + harvestJob.getJobID() + ""String_Node_Str"",e);
  }
}","/** 
 * Writes metadata to an ARC with the following name: <jobid>-preharvest-metadata-1.arc.
 * @param harvestJob a given Job.
 * @param metadata   the list of metadata entries to write to ARC.
 * @param crawlDir   the directory, where the ARC-file will be written.
 * @throws IOFailure If there are errors in writing the ARC file.
 */
private void writePreharvestMetadata(Job harvestJob,List<MetadataEntry> metadata,File crawlDir) throws IOFailure {
  if (metadata.size() == 0) {
    return;
  }
  File arcFile=new File(crawlDir,HarvestDocumentation.getPreharvestMetadataARCFileName(harvestJob.getJobID()));
  try {
    ARCWriter aw=null;
    try {
      aw=ARCUtils.createARCWriter(arcFile);
      for (      MetadataEntry m : metadata) {
        ByteArrayInputStream bais=new ByteArrayInputStream(m.getData());
        aw.write(m.getURL(),m.getMimeType(),SystemUtils.getLocalIP(),System.currentTimeMillis(),m.getData().length,bais);
      }
    }
  finally {
      try {
        if (aw != null) {
          aw.close();
        }
      }
 catch (      IOException e) {
        log.warn(""String_Node_Str"" + aw.getFile().getAbsolutePath() + ""String_Node_Str"",e);
      }
    }
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"" + harvestJob.getJobID() + ""String_Node_Str"",e);
  }
}",0.9752380952380952
89947,"/** 
 * Retrieve the list of jobs for deduplicate reduction. Runs through all metadata entries, finding duplicate reduction entries, and parsing all jobIDs in them, warning only on errors.
 * @param metadataEntries list of metadataEntries
 * @return the list of jobs for deduplicate reduction
 */
private List<Long> parseJobIDsForDuplicateReduction(List<MetadataEntry> metadataEntries){
  List<Long> result=new ArrayList<Long>();
  for (  MetadataEntry me : metadataEntries) {
    if (me.isDuplicateReductionMetadataEntry()) {
      String s=new String(me.getData());
      String[] longs=s.split(""String_Node_Str"");
      for (      String stringLong : longs) {
        try {
          result.add(Long.valueOf(Long.parseLong(stringLong)));
        }
 catch (        NumberFormatException e) {
          log.warn(""String_Node_Str"" + stringLong + ""String_Node_Str""+ ""String_Node_Str""+ s+ ""String_Node_Str"",e);
        }
      }
    }
  }
  return result;
}","/** 
 * Retrieve the list of jobs for deduplicate reduction. Runs through all metadata entries, finding duplicate reduction entries, and parsing all jobIDs in them, warning only on errors.
 * @param metadataEntries list of metadataEntries
 * @return the list of jobs for deduplicate reduction
 */
private List<Long> parseJobIDsForDuplicateReduction(List<MetadataEntry> metadataEntries){
  List<Long> result=new ArrayList<Long>();
  for (  MetadataEntry me : metadataEntries) {
    if (me.isDuplicateReductionMetadataEntry()) {
      String s=new String(me.getData());
      String[] longs=s.split(""String_Node_Str"");
      for (      String stringLong : longs) {
        try {
          result.add(Long.getLong(stringLong));
        }
 catch (        NumberFormatException e) {
          log.warn(""String_Node_Str"" + stringLong + ""String_Node_Str""+ ""String_Node_Str""+ s+ ""String_Node_Str"",e);
        }
      }
    }
  }
  return result;
}",0.972544878563886
89948,"/** 
 * Turns a raw CDX file for the given jobID into a metadatafile containing the CDX lines in one ARC entry per ARC file indexed. The output is put into a file called &lt;jobID&gt;-metadata-1.arc.
 * @param resultFile The CDX file returned by a ExtractCDXJob for thegiven jobID.
 * @param jobID The jobID we work on.
 * @throws IOException If an I/O error occurs, or the resultFiledoes not exist
 */
private void arcifyResultFile(File resultFile,long jobID) throws IOException {
  BufferedReader reader=new BufferedReader(new FileReader(resultFile));
  try {
    ARCWriter writer=ARCUtils.createARCWriter(new File(HarvestDocumentation.getMetadataARCFileName(Long.toString(jobID))));
    try {
      String line;
      ByteArrayOutputStream baos=new ByteArrayOutputStream();
      String lastFilename=null;
      FileUtils.FilenameParser parser=null;
      while ((line=reader.readLine()) != null) {
        parser=parseLine(line,jobID);
        if (parser == null) {
          continue;
        }
        if (!parser.getFilename().equals(lastFilename)) {
          writeCDXEntry(writer,parser,baos);
          lastFilename=parser.getFilename();
        }
        baos.write(line.getBytes());
        baos.write(""String_Node_Str"".getBytes());
      }
      if (parser != null) {
        writeCDXEntry(writer,parser,baos);
      }
    }
  finally {
      writer.close();
    }
  }
  finally {
    reader.close();
  }
}","/** 
 * Turns a raw CDX file for the given jobID into a metadatafile containing the CDX lines in one ARC entry per ARC file indexed. The output is put into a file called &lt;jobID&gt;-metadata-1.arc.
 * @param resultFile The CDX file returned by a ExtractCDXJob for thegiven jobID.
 * @param jobID The jobID we work on.
 * @throws IOException If an I/O error occurs, or the resultFiledoes not exist
 */
private void arcifyResultFile(File resultFile,long jobID) throws IOException {
  BufferedReader reader=new BufferedReader(new FileReader(resultFile));
  try {
    ARCWriter writer=ARCUtils.createARCWriter(new File(HarvestDocumentation.getMetadataARCFileName(Long.toString(jobID))));
    try {
      String line;
      ByteArrayOutputStream baos=new ByteArrayOutputStream();
      String lastFilename=null;
      FileUtils.FilenameParser parser=null;
      while ((line=reader.readLine()) != null) {
        parser=parseLine(line,jobID);
        if (parser == null) {
          continue;
        }
        if (!parser.getFilename().equals(lastFilename)) {
          writeCDXEntry(writer,parser,baos.toByteArray());
          baos.reset();
          lastFilename=parser.getFilename();
        }
        baos.write(line.getBytes());
        baos.write(""String_Node_Str"".getBytes());
      }
      if (parser != null) {
        writeCDXEntry(writer,parser,baos.toByteArray());
      }
    }
  finally {
      writer.close();
    }
  }
  finally {
    reader.close();
  }
}",0.9820069204152247
89949,"/** 
 * Writes a full entry of CDX files to the ARCWriter.
 * @param writer The writer we're currently writing to.
 * @param parser The filename of all the entries stored in baos.  Thisis used to generate the URI for the entry.
 * @param baos An outputstream containing the bytes of the CDX recordsto be written under this entry.  This outputstream is cleared after writing.
 * @throws IOFailure if the write fails for any reason
 */
private void writeCDXEntry(ARCWriter writer,FileUtils.FilenameParser parser,ByteArrayOutputStream baos) throws IOFailure {
  try {
    writer.write(HarvestDocumentation.getCDXURI(parser.getHarvestID(),parser.getJobID(),parser.getTimeStamp(),parser.getSerialNo()).toString(),Constants.CDX_MIME_TYPE,SystemUtils.getLocalIP(),System.currentTimeMillis(),baos.size(),baos);
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + parser.getFilename(),e);
  }
  baos.reset();
}","/** 
 * Writes a full entry of CDX files to the ARCWriter.
 * @param writer The writer we're currently writing to.
 * @param parser The filename of all the entries stored in baos.  Thisis used to generate the URI for the entry.
 * @param bytes The bytes of the CDX records to be written under thisentry.
 * @throws IOFailure if the write fails for any reason
 */
private void writeCDXEntry(ARCWriter writer,FileUtils.FilenameParser parser,byte[] bytes) throws IOFailure {
  try {
    ByteArrayInputStream bais=new ByteArrayInputStream(bytes);
    writer.write(HarvestDocumentation.getCDXURI(parser.getHarvestID(),parser.getJobID(),parser.getTimeStamp(),parser.getSerialNo()).toString(),Constants.CDX_MIME_TYPE,SystemUtils.getLocalIP(),System.currentTimeMillis(),bytes.length,bais);
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"" + ""String_Node_Str"" + parser.getFilename(),e);
  }
}",0.8708806050783361
89950,"/** 
 * Constructor used when you want to change the checksum for the given filename.
 * @param theFileName the given filename
 * @param theChecksum the new checksum for the filename
 */
public AdminDataMessage(String theFileName,String theChecksum){
  super(Channels.getTheRepos(),Channels.getThisReposClient(),IDPREFIX);
  fileName=theFileName;
  checksum=theChecksum;
  changechecksum=true;
}","/** 
 * Constructor used when you want to change the checksum for the given filename.
 * @param theFileName the given filename
 * @param theChecksum the new checksum for the filename
 */
public AdminDataMessage(String theFileName,String theChecksum){
  super(Channels.getTheRepos(),Channels.getThisReposClient());
  fileName=theFileName;
  checksum=theChecksum;
  changechecksum=true;
}",0.9884763124199744
89951,"/** 
 * Construct StoreMessage.
 * @param replyTo Channel to reply back to
 * @param arcfile The file to store
 */
public StoreMessage(ChannelID replyTo,File arcfile){
  super(Channels.getTheRepos(),replyTo,STORE_MESSAGE_PREFIX);
  ArgumentNotValid.checkNotNull(arcfile,""String_Node_Str"");
  theRemoteFile=RemoteFileFactory.getDistributefileInstance(arcfile);
}","/** 
 * Construct StoreMessage.
 * @param replyTo Channel to reply back to
 * @param arcfile The file to store
 */
public StoreMessage(ChannelID replyTo,File arcfile){
  super(Channels.getTheRepos(),replyTo);
  ArgumentNotValid.checkNotNull(arcfile,""String_Node_Str"");
  theRemoteFile=RemoteFileFactory.getDistributefileInstance(arcfile);
}",0.970042796005706
89952,"/** 
 * Message to signal from a BitarchiveServer to the BitarchiveMonitorServer that the Bit Archive Application identified by BA_ApplicationId has completed its part of the batch job. Holds status information: list of files processed and a list of ARC files (file names) on which the batch job failed.
 * @param to the channel to which this message is to be sent (must be aBAMON channel)
 * @param originatingBatchMsgId the Id field from the original batch message
 * @param status The object containing status info.
 */
public BatchEndedMessage(ChannelID to,String originatingBatchMsgId,BatchStatus status){
  super(to,Channels.getError(),IDPREFIX);
  ArgumentNotValid.checkNotNull(to,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(originatingBatchMsgId,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(status,""String_Node_Str"");
  this.originatingBatchMsgId=originatingBatchMsgId;
  this.BA_ApplicationId=status.getBitArchiveAppId();
  this.rf=status.getResultFile();
  this.noOfFilesProcessed=status.getNoOfFilesProcessed();
  this.filesFailed=status.getFilesFailed();
  this.exceptions=status.getExceptions();
}","/** 
 * Message to signal from a BitarchiveServer to the BitarchiveMonitorServer that the Bit Archive Application identified by BA_ApplicationId has completed its part of the batch job. Holds status information: list of files processed and a list of ARC files (file names) on which the batch job failed.
 * @param to the channel to which this message is to be sent (must be aBAMON channel)
 * @param originatingBatchMsgId the Id field from the original batch message
 * @param status The object containing status info.
 */
public BatchEndedMessage(ChannelID to,String originatingBatchMsgId,BatchStatus status){
  super(to,Channels.getError());
  ArgumentNotValid.checkNotNull(to,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(originatingBatchMsgId,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(status,""String_Node_Str"");
  this.originatingBatchMsgId=originatingBatchMsgId;
  this.BA_ApplicationId=status.getBitArchiveAppId();
  this.rf=status.getResultFile();
  this.noOfFilesProcessed=status.getNoOfFilesProcessed();
  this.filesFailed=status.getFilesFailed();
  this.exceptions=status.getExceptions();
}",0.9960088691796009
89953,"/** 
 * Creates a BatchMessage object which can be used to initiate a batch job.
 * @param to The channel to which the batch message is to be sent
 * @param replyTo The channel whereto the reply to this message is sent.
 * @param job  The batch job to be executed
 * @param replicaId id of this replica.
 */
public BatchMessage(ChannelID to,ChannelID replyTo,FileBatchJob job,String replicaId){
  super(to,replyTo,IDPREFIX);
  ArgumentNotValid.checkNotNull(job,""String_Node_Str"");
  this.job=job;
  this.replicaId=replicaId;
}","/** 
 * Creates a BatchMessage object which can be used to initiate a batch job.
 * @param to The channel to which the batch message is to be sent
 * @param replyTo The channel whereto the reply to this message is sent.
 * @param job  The batch job to be executed
 * @param replicaId id of this replica.
 */
public BatchMessage(ChannelID to,ChannelID replyTo,FileBatchJob job,String replicaId){
  super(to,replyTo);
  ArgumentNotValid.checkNotNull(job,""String_Node_Str"");
  this.job=job;
  this.replicaId=replicaId;
}",0.9913710450623202
89954,"/** 
 * Message to signal from BitarchiveMonitorServer that the batch job identified by originatingBatchMsgId is completed. Holds status information: list of files processed and a list of files on which the batch job failed
 * @param to The queue to which this message is to be sent. This willnormally be the ARCREPOS queue
 * @param replyTo
 * @param originatingBatchMsgId The Id of the BathMessage which gaverise to this reply
 * @param filesProcessed  the total number of file processed in thisbatch job
 * @param filesFailed a Collection<String> of the names of files onwhich this batch job failed
 * @param resultFile The RemoteFile containing the output fromthe batch job, or null if an error occurred that prevented the creation of the file.
 * @throws ArgumentNotValid if the input parameters are not meaningful
 */
public BatchReplyMessage(ChannelID to,ChannelID replyTo,String originatingBatchMsgId,int filesProcessed,Collection<File> filesFailed,RemoteFile resultFile) throws ArgumentNotValid {
  super(to,replyTo,IDPREFIX);
  ArgumentNotValid.checkNotNullOrEmpty(originatingBatchMsgId,""String_Node_Str"");
  ArgumentNotValid.checkTrue(filesProcessed >= 0,""String_Node_Str"");
  this.replyOfId=originatingBatchMsgId;
  this.noOfFilesProcessed=filesProcessed;
  if (filesFailed != null) {
    this.filesFailed=new HashSet<File>(filesFailed);
  }
 else {
    this.filesFailed=new HashSet<File>();
  }
  this.resultFile=resultFile;
}","/** 
 * Message to signal from BitarchiveMonitorServer that the batch job identified by originatingBatchMsgId is completed. Holds status information: list of files processed and a list of files on which the batch job failed
 * @param to The queue to which this message is to be sent. This willnormally be the ARCREPOS queue
 * @param replyTo The queue that should receive replies.
 * @param originatingBatchMsgId The Id of the BathMessage which gaverise to this reply
 * @param filesProcessed  the total number of file processed in thisbatch job
 * @param filesFailed a Collection<String> of the names of files onwhich this batch job failed
 * @param resultFile The RemoteFile containing the output fromthe batch job, or null if an error occurred that prevented the creation of the file.
 * @throws ArgumentNotValid if the input parameters are not meaningful
 */
public BatchReplyMessage(ChannelID to,ChannelID replyTo,String originatingBatchMsgId,int filesProcessed,Collection<File> filesFailed,RemoteFile resultFile) throws ArgumentNotValid {
  super(to,replyTo);
  ArgumentNotValid.checkNotNullOrEmpty(originatingBatchMsgId,""String_Node_Str"");
  ArgumentNotValid.checkTrue(filesProcessed >= 0,""String_Node_Str"");
  this.replyOfId=originatingBatchMsgId;
  this.noOfFilesProcessed=filesProcessed;
  if (filesFailed != null) {
    this.filesFailed=new HashSet<File>(filesFailed);
  }
 else {
    this.filesFailed=new HashSet<File>();
  }
  this.resultFile=resultFile;
}",0.9834938101788172
89955,"/** 
 * Constructor for get file message
 * @param to Recepient
 * @param replyTo Original sender
 * @param arcfileName The file to retrieve
 * @param replicaId The bitarchive replica id to retrieve it from.
 */
public GetFileMessage(ChannelID to,ChannelID replyTo,String arcfileName,String replicaId){
  super(to,replyTo,IDPREFIX);
  this.arcfileName=arcfileName;
  this.replicaId=replicaId;
}","/** 
 * Constructor for get file message
 * @param to Recepient
 * @param replyTo Original sender
 * @param arcfileName The file to retrieve
 * @param replicaId The bitarchive replica id to retrieve it from.
 */
public GetFileMessage(ChannelID to,ChannelID replyTo,String arcfileName,String replicaId){
  super(to,replyTo);
  this.arcfileName=arcfileName;
  this.replicaId=replicaId;
}",0.9884467265725289
89956,"public GetMessage(ChannelID to,ChannelID replyTo,String arcfile,long index){
  super(to,replyTo,IDPREFIX);
  this.arcfile=arcfile;
  this.index=index;
}","public GetMessage(ChannelID to,ChannelID replyTo,String arcfile,long index){
  super(to,replyTo);
  this.arcfile=arcfile;
  this.index=index;
}",0.9694915254237289
89957,"/** 
 * Creates a heartbeat message. The time of the heartbeat is set to the creation of this object.
 * @param in_receiver   ChannelID for the recipient of this message.
 * @param applicationId - id of the application that sent the heartbeat
 */
public HeartBeatMessage(ChannelID in_receiver,String applicationId){
  super(in_receiver,Channels.getError(),IDPREFIX);
  ArgumentNotValid.checkNotNullOrEmpty(applicationId,""String_Node_Str"");
  timestamp=System.currentTimeMillis();
  this.applicationId=applicationId;
}","/** 
 * Creates a heartbeat message. The time of the heartbeat is set to the creation of this object.
 * @param in_receiver   ChannelID for the recipient of this message.
 * @param applicationId - id of the application that sent the heartbeat
 */
public HeartBeatMessage(ChannelID in_receiver,String applicationId){
  super(in_receiver,Channels.getError());
  ArgumentNotValid.checkNotNullOrEmpty(applicationId,""String_Node_Str"");
  timestamp=System.currentTimeMillis();
  this.applicationId=applicationId;
}",0.991219512195122
89958,"/** 
 * Constructor.
 * @param arcfileName The file to retrieve & remove
 * @param replicaId The id of the bitarchive to receive the response
 * @param checksum the checksum of the file to retrieve & remove
 * @param credentials the right credentials for this operation
 */
public RemoveAndGetFileMessage(String arcfileName,String replicaId,String checksum,String credentials){
  super(Channels.getTheRepos(),Channels.getThisReposClient(),IDPREFIX);
  this.arcfileName=arcfileName;
  this.replicaId=replicaId;
  this.checksum=checksum;
  this.credentials=credentials;
}","/** 
 * Constructor.
 * @param arcfileName The file to retrieve & remove
 * @param replicaId The id of the bitarchive to receive the response
 * @param checksum the checksum of the file to retrieve & remove
 * @param credentials the right credentials for this operation
 */
public RemoveAndGetFileMessage(String arcfileName,String replicaId,String checksum,String credentials){
  super(Channels.getTheRepos(),Channels.getThisReposClient());
  this.arcfileName=arcfileName;
  this.replicaId=replicaId;
  this.checksum=checksum;
  this.credentials=credentials;
}",0.992028343666962
89959,"/** 
 * Construct UploadMessage.
 * @param to      Channel to message to
 * @param replyTo Channel to reply back to
 * @param rf The RemoteFile to upload
 */
public UploadMessage(ChannelID to,ChannelID replyTo,RemoteFile rf){
  super(to,replyTo,UPLOAD_MESSAGE_PREFIX);
  ArgumentNotValid.checkNotNull(rf,""String_Node_Str"");
  arcfileName=rf.getName();
  theRemoteFile=rf;
}","/** 
 * Construct UploadMessage.
 * @param to      Channel to message to
 * @param replyTo Channel to reply back to
 * @param rf The RemoteFile to upload
 */
public UploadMessage(ChannelID to,ChannelID replyTo,RemoteFile rf){
  super(to,replyTo);
  ArgumentNotValid.checkNotNull(rf,""String_Node_Str"");
  arcfileName=rf.getName();
  theRemoteFile=rf;
}",0.9696132596685084
89960,"/** 
 * Creates a new ArchiveMessage.
 * @param to        the initial receiver of the message
 * @param replyTo   the initial sender of the message
 * @param id_prefix A string to be prepended to the message id foridentification purposes
 * @throws ArgumentNotValid if to==replyTo or there is a null parameter.
 */
protected ArchiveMessage(ChannelID to,ChannelID replyTo,String id_prefix){
  super(to,replyTo,id_prefix);
}","/** 
 * Creates a new ArchiveMessage.
 * @param to        the initial receiver of the message
 * @param replyTo   the initial sender of the message
 * @throws ArgumentNotValid if to==replyTo or there is a null parameter.
 */
protected ArchiveMessage(ChannelID to,ChannelID replyTo){
  super(to,replyTo);
}",0.8390646492434664
89961,"/** 
 * Invoke default method for serializing object.
 * @param s
 */
private void writeObject(ObjectOutputStream s){
  try {
    s.defaultWriteObject();
  }
 catch (  Exception e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
}","/** 
 * Invoke default method for serializing object.
 * @param s The stream the object is written to.
 */
private void writeObject(ObjectOutputStream s){
  try {
    s.defaultWriteObject();
  }
 catch (  Exception e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
}",0.9267326732673268
89962,"/** 
 * Invoke default method for deserializing object, and reinitialise the logger.
 * @param s
 */
private void readObject(ObjectInputStream s){
  try {
    s.defaultReadObject();
  }
 catch (  Exception e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
  log=LogFactory.getLog(getClass().getName());
}","/** 
 * Invoke default method for deserializing object, and reinitialise the logger.
 * @param s The stream the object is read from.
 */
private void readObject(ObjectInputStream s){
  try {
    s.defaultReadObject();
  }
 catch (  Exception e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
  log=LogFactory.getLog(getClass().getName());
}",0.944954128440367
89963,"/** 
 * Generate an index request message. Receiver is always the index server channel, replyto is always this index client.
 * @param requestType Type of index requested.
 * @param jobSet      Jobs for which the index is requested.
 * @throws ArgumentNotValid if wither argument is null.
 */
public IndexRequestMessage(RequestType requestType,Set<Long> jobSet){
  super(Channels.getTheIndexServer(),Channels.getThisIndexClient(),""String_Node_Str"");
  ArgumentNotValid.checkNotNull(requestType,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(jobSet,""String_Node_Str"");
  this.requestedJobs=new HashSet<Long>(jobSet);
  this.requestType=requestType;
}","/** 
 * Generate an index request message. Receiver is always the index server channel, replyto is always this index client.
 * @param requestType Type of index requested.
 * @param jobSet      Jobs for which the index is requested.
 * @throws ArgumentNotValid if wither argument is null.
 */
public IndexRequestMessage(RequestType requestType,Set<Long> jobSet){
  super(Channels.getTheIndexServer(),Channels.getThisIndexClient());
  ArgumentNotValid.checkNotNull(requestType,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(jobSet,""String_Node_Str"");
  this.requestedJobs=new HashSet<Long>(jobSet);
  this.requestType=requestType;
}",0.9054263565891472
89964,"public TestMessage(ChannelID to,ChannelID replyTo,String testID){
  super(to,replyTo,""String_Node_Str"");
  this.testID=testID;
}","public TestMessage(ChannelID to,ChannelID replyTo,String testID){
  super(to,replyTo);
  this.testID=testID;
}",0.9243697478991596
89965,"/** 
 * Creates a new NetarkivetMessage.
 * @param to the initial receiver of the message
 * @param replyTo the initial sender of the message
 * @param id_prefix A string to be prepended to the message id foridentification purposes
 * @throws ArgumentNotValid if to==replyTo or there is a null parameter.
 */
protected NetarkivetMessage(ChannelID to,ChannelID replyTo,String id_prefix){
  ArgumentNotValid.checkNotNull(to,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replyTo,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(id_prefix,""String_Node_Str"");
  if (to.getName().equals(replyTo.getName())) {
    throw new ArgumentNotValid(""String_Node_Str"");
  }
  if (replyTo.isTopic()) {
    throw new ArgumentNotValid(""String_Node_Str"" + replyTo.toString() + ""String_Node_Str"");
  }
  this.to=to;
  this.replyTo=replyTo;
  this.id=null;
  this.replyOfId=null;
}","/** 
 * Creates a new NetarkivetMessage.
 * @param to the initial receiver of the message
 * @param replyTo the initial sender of the message
 * @throws ArgumentNotValid if to==replyTo, the replyTo parameter is atopic instead of a queue, or there is a null parameter.
 */
protected NetarkivetMessage(ChannelID to,ChannelID replyTo){
  ArgumentNotValid.checkNotNull(to,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(replyTo,""String_Node_Str"");
  if (to.getName().equals(replyTo.getName())) {
    throw new ArgumentNotValid(""String_Node_Str"");
  }
  if (replyTo.isTopic()) {
    throw new ArgumentNotValid(""String_Node_Str"" + replyTo.toString() + ""String_Node_Str"");
  }
  this.to=to;
  this.replyTo=replyTo;
  this.id=null;
  this.replyOfId=null;
}",0.8629629629629629
89966,"/** 
 * Invoke default method for serializing object.
 * @param s
 */
private void writeObject(ObjectOutputStream s){
  try {
    s.defaultWriteObject();
  }
 catch (  Exception e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
}","/** 
 * Invoke default method for serializing object.
 * @param s The stream the object is written to.
 */
private void writeObject(ObjectOutputStream s){
  try {
    s.defaultWriteObject();
  }
 catch (  Exception e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
}",0.9267326732673268
89967,"/** 
 * Invoke default method for deserializing object.
 * @param s
 */
private void readObject(ObjectInputStream s){
  try {
    s.defaultReadObject();
  }
 catch (  Exception e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
}","/** 
 * Invoke default method for deserializing object.
 * @param s The stream the object is read from.
 */
private void readObject(ObjectInputStream s){
  try {
    s.defaultReadObject();
  }
 catch (  Exception e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
}",0.9282868525896414
89968,"public TestMessage(ChannelID to,ChannelID replyTo,String testID){
  super(to,replyTo,""String_Node_Str"");
  this.testID=testID;
}","public TestMessage(ChannelID to,ChannelID replyTo,String testID){
  super(to,replyTo);
  this.testID=testID;
}",0.9243697478991596
89969,"public void testErrorcodes(){
  Settings.set(JMSConnectionSunMQ.JMS_BROKER_PORT,""String_Node_Str"");
  JMSConnection con=JMSConnectionFactory.getInstance();
  NetarkivetMessage msg=null;
  int msgNr=0;
  while (msgNr < 50) {
    msg=new TestMessage(Channels.getError(),Channels.getTheRepos(),""String_Node_Str"" + msgNr);
    System.out.println(""String_Node_Str"" + msgNr);
    con.send(msg);
    System.out.println(""String_Node_Str"" + msgNr + ""String_Node_Str"");
    TimeUtils.exponentialBackoffSleep(1,Calendar.MINUTE);
    msgNr++;
  }
  con.cleanup();
}","public void testErrorcodes(){
  Settings.set(JMSConnectionSunMQ.JMS_BROKER_PORT,""String_Node_Str"");
  JMSConnection con=JMSConnectionFactory.getInstance();
  NetarkivetMessage msg;
  int msgNr=0;
  while (msgNr < 50) {
    msg=new TestMessage(Channels.getError(),Channels.getTheRepos(),""String_Node_Str"" + msgNr);
    System.out.println(""String_Node_Str"" + msgNr);
    con.send(msg);
    System.out.println(""String_Node_Str"" + msgNr + ""String_Node_Str"");
    TimeUtils.exponentialBackoffSleep(1,Calendar.MINUTE);
    msgNr++;
  }
  con.cleanup();
}",0.9954586739327884
89970,"/** 
 * Return the URL for monitoring this instance.
 * @return the URL for monitoring this instance.
 */
public String getHarvestInformation(){
  return ""String_Node_Str"" + getHostName() + ""String_Node_Str""+ getGUIPort();
}","/** 
 * Return the URL for monitoring this instance.
 * @return the URL for monitoring this instance.
 */
public String getHarvestInformation(){
  return ""String_Node_Str"" + SystemUtils.getLocalHostName() + ""String_Node_Str""+ getGUIPort();
}",0.9505376344086022
89971,"/** 
 * Gets a list of settings. First it is checked, if the key is registered as a System property. If yes, registered value is returned in a list of length 1. If no, the data loaded from the settings xml files are examined.  If value is there, it is returned in a list. If not, the default settings from classpath are examined. If values for this setting are found here, they are returned. Otherwise, an UnknownId exception is thrown. Note that the values will not be concatenated, the first place with a match will define the entire list. Furthemore the list cannot be empty.
 * @param key name of the setting to retrieve
 * @return the retrieved values (as a non-empty String array)
 * @throws ArgumentNotValid if key is null or the empty string
 * @throws UnknownID        if no setting loaded matches key
 */
public static String[] getAll(String key) throws UnknownID, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(key,""String_Node_Str"");
  log.debug(""String_Node_Str"" + key);
  String val=System.getProperty(key);
  if (val != null) {
    log.debug(""String_Node_Str"" + val);
    return new String[]{val};
  }
  if (fileSettingsXmlList.isEmpty()) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"");
  }
  for (  SimpleXml settingsXml : getFileSettingsXmlList()) {
    List<String> result=settingsXml.getList(key);
    if (result.size() == 0) {
      continue;
    }
    log.debug(""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",result));
    return result.toArray(new String[result.size()]);
  }
  for (  SimpleXml settingsXml : getDefaultClasspathSettingsXmlList()) {
    List<String> result=settingsXml.getList(key);
    if (result.size() == 0) {
      continue;
    }
    log.debug(""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",result));
    return result.toArray(new String[result.size()]);
  }
  throw new UnknownID(""String_Node_Str"" + key + ""String_Node_Str"");
}","/** 
 * Gets a list of settings. First it is checked, if the key is registered as a System property. If yes, registered value is returned in a list of length 1. If no, the data loaded from the settings xml files are examined.  If value is there, it is returned in a list. If not, the default settings from classpath are examined. If values for this setting are found here, they are returned. Otherwise, an UnknownId exception is thrown. Note that the values will not be concatenated, the first place with a match will define the entire list. Furthemore the list cannot be empty.
 * @param key name of the setting to retrieve
 * @return the retrieved values (as a non-empty String array)
 * @throws ArgumentNotValid if key is null or the empty string
 * @throws UnknownID        if no setting loaded matches key
 */
public static String[] getAll(String key) throws UnknownID, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(key,""String_Node_Str"");
  log.debug(""String_Node_Str"" + key);
  String val=System.getProperty(key);
  if (val != null) {
    log.debug(""String_Node_Str"" + val);
    return new String[]{val};
  }
  if (fileSettingsXmlList.isEmpty()) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"");
  }
synchronized (fileSettingsXmlList) {
    for (    SimpleXml settingsXml : fileSettingsXmlList) {
      List<String> result=settingsXml.getList(key);
      if (result.size() == 0) {
        continue;
      }
      log.debug(""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",result));
      return result.toArray(new String[result.size()]);
    }
  }
synchronized (defaultClasspathSettingsXmlList) {
    for (    SimpleXml settingsXml : defaultClasspathSettingsXmlList) {
      List<String> result=settingsXml.getList(key);
      if (result.size() == 0) {
        continue;
      }
      log.debug(""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",result));
      return result.toArray(new String[result.size()]);
    }
  }
  throw new UnknownID(""String_Node_Str"" + key + ""String_Node_Str"");
}",0.9492643328259768
89972,"/** 
 * Get a tree view of a part of the settings. Note: settings read with this mechanism do not support overriding with system properties!
 * @param path Dotted path to a unique element in the tree.
 * @return The part of the setting structure below the element given.
 */
public static StringTree<String> getTree(String path){
  for (  SimpleXml settingsXml : getFileSettingsXmlList()) {
    if (settingsXml.hasKey(path)) {
      return settingsXml.getTree(path);
    }
  }
  for (  SimpleXml settingsXml : getDefaultClasspathSettingsXmlList()) {
    if (settingsXml.hasKey(path)) {
      return settingsXml.getTree(path);
    }
  }
  throw new UnknownID(""String_Node_Str"" + path + ""String_Node_Str"");
}","/** 
 * Get a tree view of a part of the settings. Note: settings read with this mechanism do not support overriding with system properties!
 * @param path Dotted path to a unique element in the tree.
 * @return The part of the setting structure below the element given.
 */
public static StringTree<String> getTree(String path){
synchronized (fileSettingsXmlList) {
    for (    SimpleXml settingsXml : fileSettingsXmlList) {
      if (settingsXml.hasKey(path)) {
        return settingsXml.getTree(path);
      }
    }
  }
synchronized (defaultClasspathSettingsXmlList) {
    for (    SimpleXml settingsXml : defaultClasspathSettingsXmlList) {
      if (settingsXml.hasKey(path)) {
        return settingsXml.getTree(path);
      }
    }
  }
  throw new UnknownID(""String_Node_Str"" + path + ""String_Node_Str"");
}",0.8973684210526316
89973,"/** 
 * Reloads the settings. This will reload the settings from disk, and forget all settings that were set with   {@link #set}The field   {@link #lastModified} is updated to timestampof the settings file that has been changed most recently.
 * @throws IOFailure if settings cannot be loaded
 * @see #conditionalReload()
 */
public static synchronized void reload(){
  lastModified=0;
  List<File> settingsFiles=getSettingsFiles();
  List<SimpleXml> simpleXmlList=new ArrayList<SimpleXml>();
  for (  File settingsFile : settingsFiles) {
    if (settingsFile.isFile()) {
      simpleXmlList.add(new SimpleXml(settingsFile));
    }
 else {
      log.warn(""String_Node_Str"" + settingsFile.getAbsolutePath() + ""String_Node_Str"");
    }
    if (settingsFile.lastModified() > lastModified) {
      lastModified=settingsFile.lastModified();
    }
  }
  fileSettingsXmlList=simpleXmlList;
}","/** 
 * Reloads the settings. This will reload the settings from disk, and forget all settings that were set with   {@link #set}The field   {@link #lastModified} is updated to timestampof the settings file that has been changed most recently.
 * @throws IOFailure if settings cannot be loaded
 * @see #conditionalReload()
 */
public static synchronized void reload(){
  lastModified=0;
  List<File> settingsFiles=getSettingsFiles();
  List<SimpleXml> simpleXmlList=new ArrayList<SimpleXml>();
  for (  File settingsFile : settingsFiles) {
    if (settingsFile.isFile()) {
      simpleXmlList.add(new SimpleXml(settingsFile));
    }
 else {
      log.warn(""String_Node_Str"" + settingsFile.getAbsolutePath() + ""String_Node_Str"");
    }
    if (settingsFile.lastModified() > lastModified) {
      lastModified=settingsFile.lastModified();
    }
  }
  fileSettingsXmlList=Collections.synchronizedList(simpleXmlList);
}",0.9833147942157954
89974,"/** 
 * Add the settings file represented by this path to the list of default classpath settings.
 * @param defaultClasspathSettingsPath the given default classpath setting.
 */
public static void addDefaultClasspathSettings(String defaultClasspathSettingsPath){
  ArgumentNotValid.checkNotNullOrEmpty(defaultClasspathSettingsPath,""String_Node_Str"");
  InputStream stream=Thread.currentThread().getContextClassLoader().getResourceAsStream(defaultClasspathSettingsPath);
  if (stream != null) {
    if (defaultClasspathSettingsXmlList == null) {
      defaultClasspathSettingsXmlList=new ArrayList<SimpleXml>();
    }
    defaultClasspathSettingsXmlList.add(new SimpleXml(stream));
  }
 else {
    log.warn(""String_Node_Str"" + defaultClasspathSettingsPath + ""String_Node_Str"");
  }
}","/** 
 * Add the settings file represented by this path to the list of default classpath settings.
 * @param defaultClasspathSettingsPath the given default classpath setting.
 */
public static void addDefaultClasspathSettings(String defaultClasspathSettingsPath){
  ArgumentNotValid.checkNotNullOrEmpty(defaultClasspathSettingsPath,""String_Node_Str"");
  InputStream stream=Thread.currentThread().getContextClassLoader().getResourceAsStream(defaultClasspathSettingsPath);
  if (stream != null) {
    if (defaultClasspathSettingsXmlList == null) {
      defaultClasspathSettingsXmlList=Collections.synchronizedList(new ArrayList<SimpleXml>());
    }
    defaultClasspathSettingsXmlList.add(new SimpleXml(stream));
  }
 else {
    log.warn(""String_Node_Str"" + defaultClasspathSettingsPath + ""String_Node_Str"");
  }
}",0.9811794228356336
89975,"/** 
 * Gets a setting. The search order for a given setting is as follows: First it is checked, if the argument key is set as a System property. If yes, return this value. If no, we continue the search. Secondly, we check, if the setting is in one of the loaded settings xml files. If the value is there, it is returned. If no, we continue the search. Finally, we check if the setting is in one of default settings  files from classpath. If the value is there, it is returned. Otherwise an UnknownId exception is thrown. Note: The retrieved value can be the empty string
 * @param key name of the setting to retrieve
 * @return the retrieved value
 * @throws ArgumentNotValid if key is null or the empty string
 * @throws UnknownID        if no setting loaded matches key
 * @throws IOFailure        if IO Failure
 */
public static String get(String key) throws UnknownID, IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(key,""String_Node_Str"");
  String val=System.getProperty(key);
  if (val != null) {
    return val;
  }
  for (  SimpleXml settingsXml : getFileSettingsXmlList()) {
    if (settingsXml.hasKey(key)) {
      return settingsXml.getString(key);
    }
  }
  for (  SimpleXml settingsXml : getDefaultClasspathSettingsXmlList()) {
    if (settingsXml.hasKey(key)) {
      return settingsXml.getString(key);
    }
  }
  throw new UnknownID(""String_Node_Str"" + key + ""String_Node_Str"");
}","/** 
 * Gets a setting. The search order for a given setting is as follows: First it is checked, if the argument key is set as a System property. If yes, return this value. If no, we continue the search. Secondly, we check, if the setting is in one of the loaded settings xml files. If the value is there, it is returned. If no, we continue the search. Finally, we check if the setting is in one of default settings  files from classpath. If the value is there, it is returned. Otherwise an UnknownId exception is thrown. Note: The retrieved value can be the empty string
 * @param key name of the setting to retrieve
 * @return the retrieved value
 * @throws ArgumentNotValid if key is null or the empty string
 * @throws UnknownID        if no setting loaded matches key
 * @throws IOFailure        if IO Failure
 */
public static String get(String key) throws UnknownID, IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(key,""String_Node_Str"");
  String val=System.getProperty(key);
  if (val != null) {
    return val;
  }
synchronized (fileSettingsXmlList) {
    for (    SimpleXml settingsXml : fileSettingsXmlList) {
      if (settingsXml.hasKey(key)) {
        return settingsXml.getString(key);
      }
    }
  }
synchronized (defaultClasspathSettingsXmlList) {
    for (    SimpleXml settingsXml : defaultClasspathSettingsXmlList) {
      if (settingsXml.hasKey(key)) {
        return settingsXml.getString(key);
      }
    }
  }
  throw new UnknownID(""String_Node_Str"" + key + ""String_Node_Str"");
}",0.8535593220338983
89976,"/** 
 * Handle an HTTP request. Overrides default behaviour of Jetty. This will forward the URI and response to the wrapped URI resolver. Note that the server will NOT force the return value to be the one returned by the uri resolver, rather it will use the one the uri resolver has set in the response object. Exceptions will generate an internal server error-page with the details.
 * @param target URL or name for request. Not used
 * @param request The original request, including URL
 * @param response The object that receives the result
 * @param dispatch The dispatch mode. Not used.
 * @see Handler#handle(java.lang.String,HttpServletRequest,HttpServletResponse,int)
 */
public void handle(String target,HttpServletRequest request,HttpServletResponse response,int dispatch){
  URI uri=null;
  HttpResponse netarkivetResponse=new HttpResponse(response);
  HttpRequest netarkivetRequest=new HttpRequest(request);
  try {
    uriResolver.lookup(netarkivetRequest,netarkivetResponse);
    ((org.mortbay.jetty.Request)request).setHandled(true);
  }
 catch (  Exception e) {
    createErrorResponse(uri,netarkivetResponse,e);
  }
}","/** 
 * Handle an HTTP request. Overrides default behaviour of Jetty. This will forward the URI and response to the wrapped URI resolver. Note that the server will NOT force the return value to be the one returned by the uri resolver, rather it will use the one the uri resolver has set in the response object. Exceptions will generate an internal server error-page with the details.
 * @param target URL or name for request. Not used
 * @param request The original request, including URL
 * @param response The object that receives the result
 * @param dispatch The dispatch mode. Not used.
 * @see Handler#handle(java.lang.String,HttpServletRequest,HttpServletResponse,int)
 */
public void handle(String target,HttpServletRequest request,HttpServletResponse response,int dispatch){
  HttpResponse netarkivetResponse=new HttpResponse(response);
  HttpRequest netarkivetRequest=new HttpRequest(request);
  try {
    uriResolver.lookup(netarkivetRequest,netarkivetResponse);
    ((org.mortbay.jetty.Request)request).setHandled(true);
  }
 catch (  Exception e) {
    createErrorResponse(netarkivetRequest.getURI(),netarkivetResponse,e);
  }
}",0.9802197802197802
89977,"/** 
 * Ensure that a file containing the appropriate content exists for the ID. If the content cannot be found, this method may return null (if I is a simple type) or an appropriate subset (if I is, say, a Set) indicating the data that is actually available.  In the latter case, calling cache on the returned set should always fill the file for that subset (barring catastrophic failure). Locking:  If the file is not immediately found, we enter a file-creation state.  To avoid corrupted data, we must ensure that only one cache instance, and only one thread within any instance, creates the file. Thus as long as somebody else seems to be creating the file, we wait and see if they finish.  This is checked by having an exclusive lock on a "".working"" file (we cannot use the result file, as it has to be created to be locked, and we may end up with a different cached file than we thought, see above).  The .working file itself is irrelevant, only the lock on it matters.
 * @param id Some sort of id that uniquely identifies the item withinthe cache.
 * @return The id given if it was successfully fetched, otherwise nullif the type parameter I does not allow subsets, or a subset of id if it does.  This subset should be immediately cacheable. FIXME added method synchronization. Try to fix bug 1547
 */
public I cache(I id){
  ArgumentNotValid.checkNotNull(id,""String_Node_Str"");
  File cachedFile=getCacheFile(id);
  if (cachedFile.exists()) {
    return id;
  }
 else {
    try {
      File fileBehindLockFile=new File(cachedFile.getAbsolutePath() + ""String_Node_Str"");
      FileOutputStream lockFile=new FileOutputStream(fileBehindLockFile);
      FileLock lock=null;
      try {
synchronized (fileBehindLockFile.getAbsolutePath().intern()) {
          log.debug(""String_Node_Str"" + fileBehindLockFile.getAbsolutePath() + ""String_Node_Str""+ Thread.currentThread().getName()+ ""String_Node_Str"");
          try {
            lock=lockFile.getChannel().lock();
          }
 catch (          OverlappingFileLockException e) {
            log.warn(e);
            throw new IOException(e);
          }
          if (cachedFile.exists()) {
            return id;
          }
          return cacheData(id);
        }
      }
  finally {
        if (lock != null) {
          log.debug(""String_Node_Str"" + lockFile.getChannel());
          lock.release();
        }
        lockFile.close();
      }
    }
 catch (    IOException e) {
      String errMsg=""String_Node_Str"" + cachedFile.getAbsolutePath() + ""String_Node_Str"";
      log.warn(errMsg,e);
      throw new IOFailure(errMsg,e);
    }
  }
}","/** 
 * Ensure that a file containing the appropriate content exists for the ID. If the content cannot be found, this method may return null (if I is a simple type) or an appropriate subset (if I is, say, a Set) indicating the data that is actually available.  In the latter case, calling cache on the returned set should always fill the file for that subset (barring catastrophic failure). Locking:  If the file is not immediately found, we enter a file-creation state.  To avoid corrupted data, we must ensure that only one cache instance, and only one thread within any instance, creates the file. Thus as long as somebody else seems to be creating the file, we wait and see if they finish.  This is checked by having an exclusive lock on a "".working"" file (we cannot use the result file, as it has to be created to be locked, and we may end up with a different cached file than we thought, see above).  The .working file itself is irrelevant, only the lock on it matters.
 * @param id Some sort of id that uniquely identifies the item withinthe cache.
 * @return The id given if it was successfully fetched, otherwise nullif the type parameter I does not allow subsets, or a subset of id if it does.  This subset should be immediately cacheable. FIXME added method synchronization. Try to fix bug 1547
 */
public I cache(I id){
  ArgumentNotValid.checkNotNull(id,""String_Node_Str"");
  File cachedFile=getCacheFile(id);
  if (cachedFile.exists()) {
    return id;
  }
 else {
    try {
      File fileBehindLockFile=new File(cachedFile.getAbsolutePath() + ""String_Node_Str"");
      FileOutputStream lockFile=new FileOutputStream(fileBehindLockFile);
      FileLock lock=null;
synchronized (fileBehindLockFile.getAbsolutePath().intern()) {
        try {
          log.debug(""String_Node_Str"" + fileBehindLockFile.getAbsolutePath() + ""String_Node_Str""+ Thread.currentThread().getName()+ ""String_Node_Str"");
          try {
            lock=lockFile.getChannel().lock();
          }
 catch (          OverlappingFileLockException e) {
            log.warn(e);
            throw new IOException(e);
          }
          if (cachedFile.exists()) {
            return id;
          }
          return cacheData(id);
        }
  finally {
          if (lock != null) {
            log.debug(""String_Node_Str"" + lockFile.getChannel());
            lock.release();
          }
          lockFile.close();
        }
      }
    }
 catch (    IOException e) {
      String errMsg=""String_Node_Str"" + cachedFile.getAbsolutePath() + ""String_Node_Str"";
      log.warn(errMsg,e);
      throw new IOFailure(errMsg,e);
    }
  }
}",0.9896551724137932
89978,"/** 
 * Return the lowest limit for the two values, or MAX_DOMAIN_SIZE if both are infinite
 * @param objectLimit A long value defining an object limit, or 0 forinfinite
 * @param byteLimit A long value defining a byte limit, or -1 for inifite.
 * @param expectedObjectSize The expected number of bytes per object
 * @return The lowest of the two boundaries, or MAX_DOMAIN_SIZE if both areunlimited.
 */
public long minObjectsBytesLimit(long objectLimit,long byteLimit,long expectedObjectSize){
  long maxObjectsByBytes=byteLimit / expectedObjectSize;
  if (objectLimit != Constants.HERITRIX_MAXOBJECTS_INFINITY) {
    if (byteLimit != Constants.HERITRIX_MAXBYTES_INFINITY) {
      return Math.min(objectLimit,maxObjectsByBytes);
    }
 else {
      return objectLimit;
    }
  }
 else {
    if (byteLimit != Constants.HERITRIX_MAXBYTES_INFINITY) {
      return maxObjectsByBytes;
    }
 else {
      return Settings.getLong(HarvesterSettings.MAX_DOMAIN_SIZE);
    }
  }
}","/** 
 * Return the lowest limit for the two values, or MAX_DOMAIN_SIZE if both are infinite, which is the max size we harvest from this domain.
 * @param objectLimit A long value defining an object limit, or 0 forinfinite
 * @param byteLimit A long value defining a byte limit, or -1 for inifite.
 * @param expectedObjectSize The expected number of bytes per object
 * @return The lowest of the two boundaries, or MAX_DOMAIN_SIZE if both areunlimited.
 */
public long minObjectsBytesLimit(long objectLimit,long byteLimit,long expectedObjectSize){
  long maxObjectsByBytes=byteLimit / expectedObjectSize;
  if (objectLimit != Constants.HERITRIX_MAXOBJECTS_INFINITY) {
    if (byteLimit != Constants.HERITRIX_MAXBYTES_INFINITY) {
      return Math.min(objectLimit,maxObjectsByBytes);
    }
 else {
      return objectLimit;
    }
  }
 else {
    if (byteLimit != Constants.HERITRIX_MAXBYTES_INFINITY) {
      return maxObjectsByBytes;
    }
 else {
      return Settings.getLong(HarvesterSettings.MAX_DOMAIN_SIZE);
    }
  }
}",0.9739478957915833
89979,"/** 
 * Gets the best expectation for how many objects a harvest using this configuration will retrieve, given a job with a maximum limit pr. domain
 * @param objectLimit The maximum limit, or 0 for no limit.This limit overrides the limit set on the configuration, unless override is in effect.
 * @param byteLimit The maximum number of bytes that will be used aslimit in the harvest.  This limit overrides the limit set on the configuration, unless override is in effect.  It is used to modify the expected number of objects based on what we know of object sizes. -1 means not limit.
 * @return The expected number.
 */
public long getExpectedNumberOfObjects(long objectLimit,long byteLimit){
  long prevresultfactor=Settings.getLong(HarvesterSettings.ERRORFACTOR_PERMITTED_PREVRESULT);
  long bestguessfactor=Settings.getLong(HarvesterSettings.ERRORFACTOR_PERMITTED_BESTGUESS);
  HarvestInfo best=getBestHarvestInfoExpectation();
  log.trace(""String_Node_Str"" + best + ""String_Node_Str""+ toString()+ ""String_Node_Str"");
  long expectedObjectSize=getExpectedBytesPerObject(best);
  long maximum;
  if (objectLimit != Constants.HERITRIX_MAXOBJECTS_INFINITY || byteLimit != Constants.HERITRIX_MAXBYTES_INFINITY) {
    maximum=minObjectsBytesLimit(objectLimit,byteLimit,expectedObjectSize);
  }
 else   if (maxObjects != Constants.HERITRIX_MAXOBJECTS_INFINITY || maxBytes != Constants.HERITRIX_MAXBYTES_INFINITY) {
    maximum=minObjectsBytesLimit(maxObjects,maxBytes,expectedObjectSize);
  }
 else {
    maximum=Settings.getLong(HarvesterSettings.MAX_DOMAIN_SIZE);
  }
  long minimum;
  if (best != null) {
    minimum=best.getCountObjectRetrieved();
  }
 else {
    minimum=0;
  }
  long expectation;
  if (best != null && best.getStopReason() == StopReason.DOWNLOAD_COMPLETE) {
    expectation=minimum + ((maximum - minimum) / prevresultfactor);
  }
 else {
    expectation=minimum + ((maximum - minimum) / bestguessfactor);
  }
  if ((maxObjects > Constants.HERITRIX_MAXOBJECTS_INFINITY && maximum > maxObjects) || (maxBytes > Constants.HERITRIX_MAXBYTES_INFINITY && maximum > maxBytes / expectedObjectSize)) {
    maximum=minObjectsBytesLimit(maxObjects,maxBytes,expectedObjectSize);
  }
  expectation=Math.min(expectation,maximum);
  log.trace(""String_Node_Str"" + toString() + ""String_Node_Str""+ expectation);
  return expectation;
}","/** 
 * Gets the best expectation for how many objects a harvest using this configuration will retrieve, given a job with a maximum limit pr. domain
 * @param objectLimit The maximum limit, or 0 for no limit.This limit overrides the limit set on the configuration, unless override is in effect.
 * @param byteLimit The maximum number of bytes that will be used aslimit in the harvest.  This limit overrides the limit set on the configuration, unless override is in effect.  It is used to modify the expected number of objects based on what we know of object sizes. -1 means not limit.
 * @return The expected number of objects.
 */
public long getExpectedNumberOfObjects(long objectLimit,long byteLimit){
  long prevresultfactor=Settings.getLong(HarvesterSettings.ERRORFACTOR_PERMITTED_PREVRESULT);
  HarvestInfo best=getBestHarvestInfoExpectation();
  log.trace(""String_Node_Str"" + best + ""String_Node_Str""+ toString()+ ""String_Node_Str"");
  long expectedObjectSize=getExpectedBytesPerObject(best);
  long maximum=-1;
  if (objectLimit != Constants.HERITRIX_MAXOBJECTS_INFINITY || byteLimit != Constants.HERITRIX_MAXBYTES_INFINITY) {
    maximum=minObjectsBytesLimit(objectLimit,byteLimit,expectedObjectSize);
  }
 else   if (maxObjects != Constants.HERITRIX_MAXOBJECTS_INFINITY || maxBytes != Constants.HERITRIX_MAXBYTES_INFINITY) {
    maximum=minObjectsBytesLimit(maxObjects,maxBytes,expectedObjectSize);
  }
  long minimum;
  if (best != null) {
    minimum=best.getCountObjectRetrieved();
  }
 else {
    minimum=0;
  }
  long expectation;
  if (best != null && best.getStopReason() == StopReason.DOWNLOAD_COMPLETE && maximum != -1) {
    expectation=minimum + ((maximum - minimum) / prevresultfactor);
  }
 else {
    expectation=Settings.getLong(HarvesterSettings.MAX_DOMAIN_SIZE);
  }
  if ((maxObjects > Constants.HERITRIX_MAXOBJECTS_INFINITY && maximum > maxObjects) || (maxBytes > Constants.HERITRIX_MAXBYTES_INFINITY && maximum > maxBytes / expectedObjectSize)) {
    maximum=minObjectsBytesLimit(maxObjects,maxBytes,expectedObjectSize);
  }
  expectation=Math.min(expectation,maximum);
  log.trace(""String_Node_Str"" + toString() + ""String_Node_Str""+ expectation);
  return expectation;
}",0.9343323049801676
89980,"/** 
 * Get the value of an attribute, closing the connector afterwards.  If you wish to hold on to the connector, call JMXUtils#executeCommand(MBeanServerConnection, String, String, String[])
 * @param connector A one-shot connector object.
 * @param beanName The name of the bean to get an attribute from.
 * @param attribute The attribute to get.
 * @return Whatever the command returned.
 */
public static Object getAttribute(JMXConnector connector,String beanName,String attribute){
  ArgumentNotValid.checkNotNull(connector,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(beanName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(attribute,""String_Node_Str"");
  MBeanServerConnection connection=null;
  try {
    connection=connector.getMBeanServerConnection();
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
  try {
    return getAttribute(beanName,attribute,connection);
  }
  finally {
    try {
      connector.close();
    }
 catch (    IOException e) {
      log.warn(""String_Node_Str"" + beanName,e);
    }
  }
}","/** 
 * Get the value of an attribute, closing the connector afterwards.  If you wish to hold on to the connector, call JMXUtils#executeCommand(MBeanServerConnection, String, String, String[])
 * @param connector A one-shot connector object.
 * @param beanName The name of the bean to get an attribute from.
 * @param attribute The attribute to get.
 * @return Whatever the command returned.
 */
public static Object getAttribute(JMXConnector connector,String beanName,String attribute){
  ArgumentNotValid.checkNotNull(connector,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(beanName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(attribute,""String_Node_Str"");
  MBeanServerConnection connection;
  try {
    connection=connector.getMBeanServerConnection();
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
  try {
    return getAttribute(beanName,attribute,connection);
  }
  finally {
    try {
      connector.close();
    }
 catch (    IOException e) {
      log.warn(""String_Node_Str"" + beanName,e);
    }
  }
}",0.997677659080353
89981,"/** 
 * Get a JMXConnector to a given host and port, using login and password.
 * @param hostName The host to attempt to connect to.
 * @param jmxPort The port on the host to connect to (a non-negative number)
 * @param login The login name to authenticate as (typically ""controlRole""or ""monitorRole"".
 * @param password The password for JMX access
 * @return A JMX connector to the given host and port, using default RMI
 */
public static JMXConnector getJMXConnector(String hostName,int jmxPort,final String login,final String password){
  ArgumentNotValid.checkNotNullOrEmpty(hostName,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(jmxPort,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(login,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(password,""String_Node_Str"");
  JMXServiceURL rmiurl=getUrl(hostName,jmxPort,-1);
  Map<String,?> environment=(Map<String,?>)packageCredentials(login,password);
  Throwable lastException;
  int retries=0;
  do {
    try {
      return JMXConnectorFactory.connect(rmiurl,environment);
    }
 catch (    IOException e) {
      lastException=e;
      if (retries < MAX_TRIES && e.getCause() != null && (e.getCause() instanceof ServiceUnavailableException || e.getCause() instanceof SocketTimeoutException)) {
        TimeUtils.exponentialBackoffSleep(retries);
        continue;
      }
      break;
    }
  }
 while (retries++ < MAX_TRIES);
  throw new IOFailure(""String_Node_Str"" + rmiurl + ""String_Node_Str""+ retries+ ""String_Node_Str"",lastException);
}","/** 
 * Get a JMXConnector to a given host and port, using login and password.
 * @param hostName The host to attempt to connect to.
 * @param jmxPort The port on the host to connect to (a non-negative number)
 * @param login The login name to authenticate as (typically ""controlRole""or ""monitorRole"".
 * @param password The password for JMX access
 * @return A JMX connector to the given host and port, using default RMI
 */
public static JMXConnector getJMXConnector(String hostName,int jmxPort,final String login,final String password){
  ArgumentNotValid.checkNotNullOrEmpty(hostName,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(jmxPort,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(login,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(password,""String_Node_Str"");
  JMXServiceURL rmiurl=getUrl(hostName,jmxPort,-1);
  Map<String,?> environment=packageCredentials(login,password);
  Throwable lastException;
  int retries=0;
  do {
    try {
      return JMXConnectorFactory.connect(rmiurl,environment);
    }
 catch (    IOException e) {
      lastException=e;
      if (retries < MAX_TRIES && e.getCause() != null && (e.getCause() instanceof ServiceUnavailableException || e.getCause() instanceof SocketTimeoutException)) {
        TimeUtils.exponentialBackoffSleep(retries,Calendar.SECOND);
        continue;
      }
      break;
    }
  }
 while (retries++ < MAX_TRIES);
  throw new IOFailure(""String_Node_Str"" + rmiurl + ""String_Node_Str""+ retries+ ""String_Node_Str""+ MAX_TRIES+ ""String_Node_Str""+ lastException.getCause().getClass().getName(),lastException);
}",0.9649010848755584
89982,"/** 
 * Execute a single command, closing the connector afterwards.  If you wish to hold on to the connector, call JMXUtils#executeCommand(MBeanServerConnection, String, String, String[])
 * @param connector A one-shot connector object.
 * @param beanName The name of the bean to execute a command on.
 * @param command The command to execute.
 * @param arguments The arguments to the command (all strings)
 * @return Whatever the command returned.
 */
public static Object executeCommand(JMXConnector connector,String beanName,String command,String... arguments){
  ArgumentNotValid.checkNotNull(connector,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(beanName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(command,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(arguments,""String_Node_Str"");
  MBeanServerConnection connection=null;
  try {
    connection=connector.getMBeanServerConnection();
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
  try {
    return executeCommand(connection,beanName,command,arguments);
  }
  finally {
    try {
      connector.close();
    }
 catch (    IOException e) {
      log.warn(""String_Node_Str"" + beanName,e);
    }
  }
}","/** 
 * Execute a single command, closing the connector afterwards.  If you wish to hold on to the connector, call JMXUtils#executeCommand(MBeanServerConnection, String, String, String[])
 * @param connector A one-shot connector object.
 * @param beanName The name of the bean to execute a command on.
 * @param command The command to execute.
 * @param arguments The arguments to the command (all strings)
 * @return Whatever the command returned.
 */
public static Object executeCommand(JMXConnector connector,String beanName,String command,String... arguments){
  ArgumentNotValid.checkNotNull(connector,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(beanName,""String_Node_Str"");
  ArgumentNotValid.checkNotNullOrEmpty(command,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(arguments,""String_Node_Str"");
  MBeanServerConnection connection;
  try {
    connection=connector.getMBeanServerConnection();
  }
 catch (  IOException e) {
    throw new IOFailure(""String_Node_Str"",e);
  }
  try {
    return executeCommand(connection,beanName,command,arguments);
  }
  finally {
    try {
      connector.close();
    }
 catch (    IOException e) {
      log.warn(""String_Node_Str"" + beanName,e);
    }
  }
}",0.9979566816510012
89983,"/** 
 * Cleanup after an Heritrix process. This entails sending the shutdown command to the Heritrix process, and killing it forcefully, if it is still alive after waiting  the period of time specified by the CommonSettings.PROCESS_TIMEOUT setting.
 * @see HeritrixController#cleanup() 
 */
public void cleanup(){
  try {
    executeHeritrixCommand(SHUTDOWN_COMMAND);
  }
 catch (  IOFailure e) {
    log.error(""String_Node_Str"",e);
  }
  final long maxWait=Settings.getLong(CommonSettings.PROCESS_TIMEOUT);
  Integer exitValue=ProcessUtils.waitFor(heritrixProcess,maxWait);
  if (exitValue != null) {
    log.info(""String_Node_Str"" + this + ""String_Node_Str""+ exitValue);
  }
 else {
    log.warn(""String_Node_Str"" + this + ""String_Node_Str""+ maxWait+ ""String_Node_Str"");
    heritrixProcess.destroy();
    exitValue=ProcessUtils.waitFor(heritrixProcess,maxWait);
    if (exitValue != null) {
      log.info(""String_Node_Str"" + this + ""String_Node_Str""+ exitValue);
    }
 else {
      log.warn(""String_Node_Str"" + this + ""String_Node_Str"");
    }
  }
  Runtime.getRuntime().removeShutdownHook(processKillerHook);
  int attempt=0;
  do {
    boolean anyAlive=false;
    for (    Thread t : collectionThreads) {
      if (t.isAlive()) {
        anyAlive=true;
      }
    }
    if (!anyAlive) {
      break;
    }
    TimeUtils.exponentialBackoffSleep(attempt);
  }
 while (attempt++ < JMXUtils.MAX_TRIES);
}","/** 
 * Cleanup after an Heritrix process. This entails sending the shutdown command to the Heritrix process, and killing it forcefully, if it is still alive after waiting  the period of time specified by the CommonSettings.PROCESS_TIMEOUT setting.
 * @see HeritrixController#cleanup() 
 */
public void cleanup(){
  try {
    executeHeritrixCommand(SHUTDOWN_COMMAND);
  }
 catch (  IOFailure e) {
    log.error(""String_Node_Str"",e);
  }
  final long maxWait=Settings.getLong(CommonSettings.PROCESS_TIMEOUT);
  Integer exitValue=ProcessUtils.waitFor(heritrixProcess,maxWait);
  if (exitValue != null) {
    log.info(""String_Node_Str"" + this + ""String_Node_Str""+ exitValue);
  }
 else {
    log.warn(""String_Node_Str"" + this + ""String_Node_Str""+ maxWait+ ""String_Node_Str"");
    heritrixProcess.destroy();
    exitValue=ProcessUtils.waitFor(heritrixProcess,maxWait);
    if (exitValue != null) {
      log.info(""String_Node_Str"" + this + ""String_Node_Str""+ exitValue);
    }
 else {
      log.warn(""String_Node_Str"" + this + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
      NotificationsFactory.getInstance().errorEvent(""String_Node_Str"" + this + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"");
      System.exit(1);
    }
  }
  Runtime.getRuntime().removeShutdownHook(processKillerHook);
  int attempt=0;
  do {
    boolean anyAlive=false;
    for (    Thread t : collectionThreads) {
      if (t.isAlive()) {
        anyAlive=true;
      }
    }
    if (!anyAlive) {
      break;
    }
    TimeUtils.exponentialBackoffSleep(attempt);
  }
 while (attempt++ < JMXUtils.MAX_TRIES);
}",0.9229760734185511
89984,"/** 
 * Creates a new BitarchiveAdmin object for an existing bit archive. Reads the directories to use from settings.
 * @throws PermissionDenied If any of the directories cannot be created orare not writeable.
 */
private BitarchiveAdmin(){
  String[] filedirnames=Settings.getAll(ArchiveSettings.BITARCHIVE_SERVER_FILEDIR);
  minSpaceLeft=Settings.getLong(ArchiveSettings.BITARCHIVE_MIN_SPACE_LEFT);
  if (minSpaceLeft <= 0L) {
    log.warn(""String_Node_Str"" + minSpaceLeft);
    throw new ArgumentNotValid(""String_Node_Str"" + minSpaceLeft);
  }
  log.info(""String_Node_Str"" + minSpaceLeft + ""String_Node_Str"");
  for (int i=0; i < filedirnames.length; i++) {
    File basedir=new File(filedirnames[i]);
    File filedir=new File(basedir,Constants.FILE_DIRECTORY_NAME);
    ApplicationUtils.dirMustExist(filedir);
    File tempdir=new File(basedir,Constants.TEMPORARY_DIRECTORY_NAME);
    ApplicationUtils.dirMustExist(tempdir);
    File atticdir=new File(basedir,Constants.ATTIC_DIRECTORY_NAME);
    ApplicationUtils.dirMustExist(atticdir);
    archivePaths.add(basedir);
    final Long bytesUsedInDir=new Long(calculateBytesUsed(basedir));
    log.info(""String_Node_Str"" + basedir + ""String_Node_Str""+ bytesUsedInDir+ ""String_Node_Str""+ FileUtils.getBytesFree(basedir)+ ""String_Node_Str"");
  }
}","/** 
 * Creates a new BitarchiveAdmin object for an existing bit archive. Reads the directories to use from settings.
 * @throws ArgumentNotValid If the settings for minSpaceLeft is non-positiveor the setting for minSpaceRequired is negative. 
 * @throws PermissionDenied If any of the directories cannot be created orare not writeable.
 */
private BitarchiveAdmin(){
  String[] filedirnames=Settings.getAll(ArchiveSettings.BITARCHIVE_SERVER_FILEDIR);
  minSpaceLeft=Settings.getLong(ArchiveSettings.BITARCHIVE_MIN_SPACE_LEFT);
  if (minSpaceLeft <= 0L) {
    log.warn(""String_Node_Str"" + minSpaceLeft);
    throw new ArgumentNotValid(""String_Node_Str"" + minSpaceLeft);
  }
  minSpaceRequired=Settings.getLong(ArchiveSettings.BITARCHIVE_MIN_SPACE_REQUIRED);
  if (minSpaceLeft < 0L) {
    log.warn(""String_Node_Str"" + minSpaceLeft);
    throw new ArgumentNotValid(""String_Node_Str"" + minSpaceLeft);
  }
  log.info(""String_Node_Str"" + minSpaceRequired + ""String_Node_Str"");
  log.info(""String_Node_Str"" + minSpaceLeft + ""String_Node_Str"");
  for (  String filedirname : filedirnames) {
    File basedir=new File(filedirname);
    File filedir=new File(basedir,Constants.FILE_DIRECTORY_NAME);
    ApplicationUtils.dirMustExist(filedir);
    File tempdir=new File(basedir,Constants.TEMPORARY_DIRECTORY_NAME);
    ApplicationUtils.dirMustExist(tempdir);
    File atticdir=new File(basedir,Constants.ATTIC_DIRECTORY_NAME);
    ApplicationUtils.dirMustExist(atticdir);
    archivePaths.add(basedir);
    final Long bytesUsedInDir=calculateBytesUsed(basedir);
    log.info(""String_Node_Str"" + basedir + ""String_Node_Str""+ bytesUsedInDir+ ""String_Node_Str""+ FileUtils.getBytesFree(basedir)+ ""String_Node_Str"");
  }
}",0.7835051546391752
89985,"/** 
 * Return the path that a given arc file can be found in.
 * @param arcFileName Name of an arc file (with no path)
 * @return A BitarchiveARCFile for the given file, or null if thefile does not exist.
 */
public BitarchiveARCFile lookup(String arcFileName){
  ArgumentNotValid.checkNotNullOrEmpty(arcFileName,""String_Node_Str"");
  for (Iterator<File> i=archivePaths.iterator(); i.hasNext(); ) {
    File archiveDir=new File(i.next(),Constants.FILE_DIRECTORY_NAME);
    if (checkArchiveDir(archiveDir)) {
      File filename=new File(archiveDir,arcFileName);
      if (filename.exists()) {
        if (filename.isFile()) {
          return new BitarchiveARCFile(arcFileName,filename);
        }
        log.fatal(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      }
    }
  }
  log.trace(""String_Node_Str"" + arcFileName + ""String_Node_Str"");
  return null;
}","/** 
 * Return the path that a given arc file can be found in.
 * @param arcFileName Name of an arc file (with no path)
 * @return A BitarchiveARCFile for the given file, or null if thefile does not exist.
 */
public BitarchiveARCFile lookup(String arcFileName){
  ArgumentNotValid.checkNotNullOrEmpty(arcFileName,""String_Node_Str"");
  for (  File archivePath : archivePaths) {
    File archiveDir=new File(archivePath,Constants.FILE_DIRECTORY_NAME);
    if (checkArchiveDir(archiveDir)) {
      File filename=new File(archiveDir,arcFileName);
      if (filename.exists()) {
        if (filename.isFile()) {
          return new BitarchiveARCFile(arcFileName,filename);
        }
        log.fatal(""String_Node_Str"" + filename + ""String_Node_Str""+ ""String_Node_Str"");
      }
    }
  }
  log.trace(""String_Node_Str"" + arcFileName + ""String_Node_Str"");
  return null;
}",0.9150997150997152
89986,"/** 
 * Return array with references to all files in the archive.
 * @return array with references to all files in the archive
 */
public File[] getFiles(){
  List<File> files=new ArrayList<File>();
  for (Iterator<File> i=archivePaths.iterator(); i.hasNext(); ) {
    File archiveDir=new File(i.next(),Constants.FILE_DIRECTORY_NAME);
    if (checkArchiveDir(archiveDir)) {
      File[] filesHere=archiveDir.listFiles();
      for (      File file : filesHere) {
        if (!file.isFile()) {
          log.warn(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"");
        }
 else {
          files.add(file);
        }
      }
    }
  }
  return (files.toArray(new File[0]));
}","/** 
 * Return array with references to all files in the archive.
 * @return array with references to all files in the archive
 */
public File[] getFiles(){
  List<File> files=new ArrayList<File>();
  for (  File archivePath : archivePaths) {
    File archiveDir=new File(archivePath,Constants.FILE_DIRECTORY_NAME);
    if (checkArchiveDir(archiveDir)) {
      File[] filesHere=archiveDir.listFiles();
      for (      File file : filesHere) {
        if (!file.isFile()) {
          log.warn(""String_Node_Str"" + file.getAbsolutePath() + ""String_Node_Str"");
        }
 else {
          files.add(file);
        }
      }
    }
  }
  return files.toArray(new File[files.size()]);
}",0.8802919708029197
89987,"/** 
 * Checks whether a directory is one of the known bitarchive directories.
 * @param theDir The dir to check
 * @return true If it is a valid archive directory; otherwise returns false.
 * @throws IOFailure if theDir or one of the valid archive directoriesdoes not exist
 * @throws ArgumentNotValid if theDir is null
 */
protected boolean isBitarchiveDirectory(File theDir){
  ArgumentNotValid.checkNotNull(theDir,""String_Node_Str"");
  try {
    theDir=theDir.getCanonicalFile();
    for (Iterator<File> i=archivePaths.iterator(); i.hasNext(); ) {
      File knowndir=i.next();
      if (knowndir.getCanonicalFile().equals(theDir)) {
        return true;
      }
    }
    return false;
  }
 catch (  IOException e) {
    final String errMsg=""String_Node_Str"";
    log.warn(errMsg,e);
    throw new IOFailure(errMsg,e);
  }
}","/** 
 * Checks whether a directory is one of the known bitarchive directories.
 * @param theDir The dir to check
 * @return true If it is a valid archive directory; otherwise returns false.
 * @throws IOFailure if theDir or one of the valid archive directoriesdoes not exist
 * @throws ArgumentNotValid if theDir is null
 */
protected boolean isBitarchiveDirectory(File theDir){
  ArgumentNotValid.checkNotNull(theDir,""String_Node_Str"");
  try {
    theDir=theDir.getCanonicalFile();
    for (    File knowndir : archivePaths) {
      if (knowndir.getCanonicalFile().equals(theDir)) {
        return true;
      }
    }
    return false;
  }
 catch (  IOException e) {
    final String errMsg=""String_Node_Str"";
    log.warn(errMsg,e);
    throw new IOFailure(errMsg,e);
  }
}",0.9470404984423676
89988,"/** 
 * Returns a temporary place for the the file to be stored.
 * @param arcFileName The simple name (i.e. no dirs) of the ARC file.
 * @param requestedSize How large the file is in bytes.
 * @return The path where the arcFile should go.
 * @throws ArgumentNotValid If arcFileName is null or empty, or requestedSize is negative.
 * @throws IOFailure if there is no more room left to store this file ofsize=requestedSize
 */
public File getTemporaryPath(String arcFileName,long requestedSize) throws ArgumentNotValid, IOFailure {
  ArgumentNotValid.checkNotNullOrEmpty(arcFileName,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(requestedSize,""String_Node_Str"");
  for (Iterator<File> it=archivePaths.iterator(); it.hasNext(); ) {
    File dir=it.next();
    long bytesFreeInDir=FileUtils.getBytesFree(dir);
    if (checkArchiveDir(dir) && (bytesFreeInDir > minSpaceLeft + requestedSize)) {
      File filedir=new File(dir,Constants.TEMPORARY_DIRECTORY_NAME);
      return new File(filedir,arcFileName);
    }
 else {
      log.warn(""String_Node_Str"" + dir.getAbsolutePath() + ""String_Node_Str""+ arcFileName+ ""String_Node_Str""+ requestedSize+ ""String_Node_Str""+ bytesFreeInDir+ ""String_Node_Str"");
    }
  }
  String errMsg=""String_Node_Str"" + arcFileName + ""String_Node_Str""+ requestedSize;
  log.fatal(errMsg);
  throw new IOFailure(errMsg);
}","/** 
 * Returns a temporary place for the the file to be stored.
 * @param arcFileName The simple name (i.e. no dirs) of the ARC file.
 * @param requestedSize How large the file is in bytes.
 * @return The path where the arcFile should go.
 * @throws ArgumentNotValid If arcFileName is null or empty, or requestedSize is negative.
 * @throws IOFailure if there is no more room left to store this file ofsize=requestedSize
 */
public File getTemporaryPath(String arcFileName,long requestedSize) throws ArgumentNotValid, IOFailure {
  ArgumentNotValid.checkNotNullOrEmpty(arcFileName,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(requestedSize,""String_Node_Str"");
  for (  File dir : archivePaths) {
    long bytesFreeInDir=FileUtils.getBytesFree(dir);
    if (checkArchiveDir(dir) && (bytesFreeInDir > minSpaceLeft) && (bytesFreeInDir - requestedSize > minSpaceRequired)) {
      File filedir=new File(dir,Constants.TEMPORARY_DIRECTORY_NAME);
      return new File(filedir,arcFileName);
    }
 else {
      log.debug(""String_Node_Str"" + dir.getAbsolutePath() + ""String_Node_Str""+ arcFileName+ ""String_Node_Str""+ requestedSize+ ""String_Node_Str""+ bytesFreeInDir+ ""String_Node_Str"");
    }
  }
  String errMsg=""String_Node_Str"" + arcFileName + ""String_Node_Str""+ requestedSize;
  log.error(errMsg);
  throw new IOFailure(errMsg);
}",0.9465081723625556
89989,"/** 
 * Get a tree view of a part of the settings. Note: settings read with this mechanism do not support overriding with system properties!
 * @param path Dotted path to a unique element in the tree.
 * @return The part of the setting structure below the element given.
 */
public static StringTree<String> getTree(String path){
  for (  SimpleXml settingsXml : fileSettingsXmlList) {
    if (settingsXml.hasKey(path)) {
      return settingsXml.getTree(path);
    }
  }
  for (  SimpleXml settingsXml : defaultClasspathSettingsXmlList) {
    if (settingsXml.hasKey(path)) {
      return settingsXml.getTree(path);
    }
  }
  throw new UnknownID(""String_Node_Str"" + path + ""String_Node_Str"");
}","/** 
 * Get a tree view of a part of the settings. Note: settings read with this mechanism do not support overriding with system properties!
 * @param path Dotted path to a unique element in the tree.
 * @return The part of the setting structure below the element given.
 */
public static StringTree<String> getTree(String path){
  for (  SimpleXml settingsXml : getFileSettingsXmlList()) {
    if (settingsXml.hasKey(path)) {
      return settingsXml.getTree(path);
    }
  }
  for (  SimpleXml settingsXml : getDefaultClasspathSettingsXmlList()) {
    if (settingsXml.hasKey(path)) {
      return settingsXml.getTree(path);
    }
  }
  throw new UnknownID(""String_Node_Str"" + path + ""String_Node_Str"");
}",0.7689015691868759
89990,"/** 
 * Gets a list of settings. First it is checked, if the key is registered as a System property. If yes, registered value is returned in a list of length 1. If no, the data loaded from the settings xml files are examined.  If value is there, it is returned in a list. If not, the default settings from classpath are examined. If values for this setting are found here, they are returned. Otherwise, an UnknownId exception is thrown. Note that the values will not be concatenated, the first place with a match will define the entire list. Furthemore the list cannot be empty.
 * @param key name of the setting to retrieve
 * @return the retrieved values (as a non-empty String array)
 * @throws ArgumentNotValid if key is null or the empty string
 * @throws UnknownID        if no setting loaded matches key
 */
public static String[] getAll(String key) throws UnknownID, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(key,""String_Node_Str"");
  log.debug(""String_Node_Str"" + key);
  String val=System.getProperty(key);
  if (val != null) {
    log.debug(""String_Node_Str"" + val);
    return new String[]{val};
  }
  if (fileSettingsXmlList.isEmpty()) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"");
  }
  for (  SimpleXml settingsXml : fileSettingsXmlList) {
    List<String> result=settingsXml.getList(key);
    if (result.size() == 0) {
      continue;
    }
    log.debug(""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",result));
    return result.toArray(new String[result.size()]);
  }
  for (  SimpleXml settingsXml : defaultClasspathSettingsXmlList) {
    List<String> result=settingsXml.getList(key);
    if (result.size() == 0) {
      continue;
    }
    log.debug(""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",result));
    return result.toArray(new String[result.size()]);
  }
  throw new UnknownID(""String_Node_Str"" + key + ""String_Node_Str"");
}","/** 
 * Gets a list of settings. First it is checked, if the key is registered as a System property. If yes, registered value is returned in a list of length 1. If no, the data loaded from the settings xml files are examined.  If value is there, it is returned in a list. If not, the default settings from classpath are examined. If values for this setting are found here, they are returned. Otherwise, an UnknownId exception is thrown. Note that the values will not be concatenated, the first place with a match will define the entire list. Furthemore the list cannot be empty.
 * @param key name of the setting to retrieve
 * @return the retrieved values (as a non-empty String array)
 * @throws ArgumentNotValid if key is null or the empty string
 * @throws UnknownID        if no setting loaded matches key
 */
public static String[] getAll(String key) throws UnknownID, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(key,""String_Node_Str"");
  log.debug(""String_Node_Str"" + key);
  String val=System.getProperty(key);
  if (val != null) {
    log.debug(""String_Node_Str"" + val);
    return new String[]{val};
  }
  if (fileSettingsXmlList.isEmpty()) {
    log.warn(""String_Node_Str"" + ""String_Node_Str"");
  }
  for (  SimpleXml settingsXml : getFileSettingsXmlList()) {
    List<String> result=settingsXml.getList(key);
    if (result.size() == 0) {
      continue;
    }
    log.debug(""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",result));
    return result.toArray(new String[result.size()]);
  }
  for (  SimpleXml settingsXml : getDefaultClasspathSettingsXmlList()) {
    List<String> result=settingsXml.getList(key);
    if (result.size() == 0) {
      continue;
    }
    log.debug(""String_Node_Str"" + StringUtils.conjoin(""String_Node_Str"",result));
    return result.toArray(new String[result.size()]);
  }
  throw new UnknownID(""String_Node_Str"" + key + ""String_Node_Str"");
}",0.9963273871983211
89991,"/** 
 * Gets a setting. The search order for a given setting is as follows: First it is checked, if the argument key is set as a System property. If yes, return this value. If no, we continue the search. Secondly, we check, if the setting is in one of the loaded settings xml files. If the value is there, it is returned. If no, we continue the search. Finally, we check if the setting is in one of default settings  files from classpath. If the value is there, it is returned. Otherwise an UnknownId exception is thrown. Note: The retrieved value can be the empty string
 * @param key name of the setting to retrieve
 * @return the retrieved value
 * @throws ArgumentNotValid if key is null or the empty string
 * @throws UnknownID        if no setting loaded matches key
 */
public static String get(String key) throws UnknownID, IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(key,""String_Node_Str"");
  String val=System.getProperty(key);
  if (val != null) {
    return val;
  }
  for (  SimpleXml settingsXml : fileSettingsXmlList) {
    if (settingsXml.hasKey(key)) {
      return settingsXml.getString(key);
    }
  }
  for (  SimpleXml settingsXml : defaultClasspathSettingsXmlList) {
    if (settingsXml.hasKey(key)) {
      return settingsXml.getString(key);
    }
  }
  throw new UnknownID(""String_Node_Str"" + key + ""String_Node_Str"");
}","/** 
 * Gets a setting. The search order for a given setting is as follows: First it is checked, if the argument key is set as a System property. If yes, return this value. If no, we continue the search. Secondly, we check, if the setting is in one of the loaded settings xml files. If the value is there, it is returned. If no, we continue the search. Finally, we check if the setting is in one of default settings  files from classpath. If the value is there, it is returned. Otherwise an UnknownId exception is thrown. Note: The retrieved value can be the empty string
 * @param key name of the setting to retrieve
 * @return the retrieved value
 * @throws ArgumentNotValid if key is null or the empty string
 * @throws UnknownID        if no setting loaded matches key
 */
public static String get(String key) throws UnknownID, IOFailure, ArgumentNotValid {
  ArgumentNotValid.checkNotNullOrEmpty(key,""String_Node_Str"");
  String val=System.getProperty(key);
  if (val != null) {
    return val;
  }
  for (  SimpleXml settingsXml : getFileSettingsXmlList()) {
    if (settingsXml.hasKey(key)) {
      return settingsXml.getString(key);
    }
  }
  for (  SimpleXml settingsXml : getDefaultClasspathSettingsXmlList()) {
    if (settingsXml.hasKey(key)) {
      return settingsXml.getString(key);
    }
  }
  throw new UnknownID(""String_Node_Str"" + key + ""String_Node_Str"");
}",0.99490538573508
89992,"/** 
 * Generate a MetadataEntry from a list of job ids for duplicate reduction.
 * @param jobIDsForDuplicateReduction the list of jobids (possibly empty)
 * @param origHarvestDefinitionID
 * @param harvestNum
 * @param jobId
 * @return null, if the list is empty,otherwise returns a MetadataEntry from the list of jobids.
 */
public static MetadataEntry makeDuplicateReductionMetadataEntry(List<Long> jobIDsForDuplicateReduction,Long origHarvestDefinitionID,int harvestNum,Long jobId){
  ArgumentNotValid.checkNotNull(jobIDsForDuplicateReduction,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(origHarvestDefinitionID,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(harvestNum,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(jobId,""String_Node_Str"");
  if (jobIDsForDuplicateReduction.isEmpty()) {
    return null;
  }
  String metadataUrl=String.format(DUPLICATEREDUCTION_METADATA_URL,origHarvestDefinitionID,harvestNum,jobId);
  return new MetadataEntry(metadataUrl,MIMETYPE_TEXT_PLAIN,StringUtils.conjoin(""String_Node_Str"",jobIDsForDuplicateReduction));
}","/** 
 * Generate a MetadataEntry from a list of job ids for duplicate reduction.
 * @param jobIDsForDuplicateReduction the list of jobids (possibly empty)
 * @param origHarvestDefinitionID The harvestdefinition that is behind the job with the given jobId
 * @param harvestNum The number of the harvest that the job with the givenjobid belongs to
 * @param jobId The id of the Job, which this metadata belongs to
 * @return null, if the list is empty,otherwise returns a MetadataEntry from the list of jobids.
 */
public static MetadataEntry makeDuplicateReductionMetadataEntry(List<Long> jobIDsForDuplicateReduction,Long origHarvestDefinitionID,int harvestNum,Long jobId){
  ArgumentNotValid.checkNotNull(jobIDsForDuplicateReduction,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(origHarvestDefinitionID,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(harvestNum,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(jobId,""String_Node_Str"");
  if (jobIDsForDuplicateReduction.isEmpty()) {
    return null;
  }
  String metadataUrl=String.format(DUPLICATEREDUCTION_METADATA_URL,origHarvestDefinitionID,harvestNum,jobId);
  return new MetadataEntry(metadataUrl,MIMETYPE_TEXT_PLAIN,StringUtils.conjoin(""String_Node_Str"",jobIDsForDuplicateReduction));
}",0.9203767123287672
89993,"/** 
 * Method needed to serializable an object of this class.
 * @param s the given ObjectOutputStream
 * @throws ClassNotFoundException
 * @throws IOException
 */
private void writeObject(ObjectOutputStream s) throws ClassNotFoundException, IOException {
  s.defaultWriteObject();
}","/** 
 * Method needed to serializable an object of this class.
 * @param s the given ObjectOutputStream
 * @throws IOException If an I/O error occurred while writing to theoutputstream
 */
private void writeObject(ObjectOutputStream s) throws IOException {
  s.defaultWriteObject();
}",0.778169014084507
89994,"/** 
 * Generate a MetadataEntry from a list of AliasInfo objects (VERSION 2) Expired aliases is skipped by this method.
 * @param aliases the list of aliases (possibly empty)
 * @param origHarvestDefinitionID
 * @param harvestNum
 * @param jobId
 * @return null, if the list if empty (or only consists of expired aliases),otherwise returns a MetadataEntry from a list of AliasInfo objects containing unexpired aliases.
 */
public static MetadataEntry makeAliasMetadataEntry(List<AliasInfo> aliases,Long origHarvestDefinitionID,int harvestNum,Long jobId){
  ArgumentNotValid.checkNotNull(aliases,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(origHarvestDefinitionID,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(harvestNum,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(jobId,""String_Node_Str"");
  if (aliases.isEmpty()) {
    return null;
  }
  List<AliasInfo> nonExpiredAliases=new ArrayList<AliasInfo>();
  for (  AliasInfo alias : aliases) {
    if (!alias.isExpired()) {
      nonExpiredAliases.add(alias);
    }
  }
  if (nonExpiredAliases.isEmpty()) {
    return null;
  }
  String metadataUrl=String.format(ALIAS_METADATA_URL,origHarvestDefinitionID,harvestNum,jobId);
  StringBuffer sb=new StringBuffer();
  for (  AliasInfo alias : nonExpiredAliases) {
    sb.append(alias.getDomain()).append(""String_Node_Str"").append(alias.getAliasOf()).append(""String_Node_Str"");
  }
  return new MetadataEntry(metadataUrl,MIMETYPE_TEXT_PLAIN,sb.toString());
}","/** 
 * Generate a MetadataEntry from a list of AliasInfo objects (VERSION 2) Expired aliases is skipped by this method.
 * @param aliases the list of aliases (possibly empty)
 * @param origHarvestDefinitionID The harvestdefinition that is behind the job with the given jobId
 * @param harvestNum The number of the harvest that the job with the givenjobid belongs to
 * @param jobId The id of the Job, which this metadata belongs to
 * @return null, if the list if empty (or only consists of expired aliases),otherwise returns a MetadataEntry from a list of AliasInfo objects containing unexpired aliases.
 */
public static MetadataEntry makeAliasMetadataEntry(List<AliasInfo> aliases,Long origHarvestDefinitionID,int harvestNum,Long jobId){
  ArgumentNotValid.checkNotNull(aliases,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(origHarvestDefinitionID,""String_Node_Str"");
  ArgumentNotValid.checkNotNegative(harvestNum,""String_Node_Str"");
  ArgumentNotValid.checkNotNull(jobId,""String_Node_Str"");
  if (aliases.isEmpty()) {
    return null;
  }
  List<AliasInfo> nonExpiredAliases=new ArrayList<AliasInfo>();
  for (  AliasInfo alias : aliases) {
    if (!alias.isExpired()) {
      nonExpiredAliases.add(alias);
    }
  }
  if (nonExpiredAliases.isEmpty()) {
    return null;
  }
  String metadataUrl=String.format(ALIAS_METADATA_URL,origHarvestDefinitionID,harvestNum,jobId);
  StringBuffer sb=new StringBuffer();
  for (  AliasInfo alias : nonExpiredAliases) {
    sb.append(alias.getDomain()).append(""String_Node_Str"").append(alias.getAliasOf()).append(""String_Node_Str"");
  }
  return new MetadataEntry(metadataUrl,MIMETYPE_TEXT_PLAIN,sb.toString());
}",0.9407643312101912
89995,"private void setURL(String url){
  if (isURLValid(url)) {
    this.url=url;
  }
 else {
    throw new ArgumentNotValid(""String_Node_Str"" + url);
  }
}","/** 
 * Set the url for this object.
 * @param url a given URL
 * @throws ArgumentNotValid if the URL is not valid
 */
private void setURL(String url){
  if (isURLValid(url)) {
    this.url=url;
  }
 else {
    throw new ArgumentNotValid(""String_Node_Str"" + url);
  }
}",0.7159904534606205
89996,"/** 
 * Method needed to de-serializable an object of this class.
 * @param s the given ObjectInputStream
 * @throws ClassNotFoundException
 * @throws IOException
 */
private void readObject(ObjectInputStream s) throws ClassNotFoundException, IOException {
  s.defaultReadObject();
}","/** 
 * Method needed to de-serializable an object of this class.
 * @param s the given ObjectInputStream
 * @throws ClassNotFoundException If the class of the serialized objectcould not be found
 * @throws IOException If an I/O error occurred while reading the serializedobject
 */
private void readObject(ObjectInputStream s) throws ClassNotFoundException, IOException {
  s.defaultReadObject();
}",0.8299120234604106
89997,"private void setMimetype(String mimetype){
  if (isMimetypeValid(mimetype)) {
    this.mimeType=mimetype;
  }
 else {
    throw new ArgumentNotValid(""String_Node_Str"" + mimetype);
  }
}","/** 
 * Set the mimetype for this object.
 * @param mimetype a given mimetype
 * @throws ArgumentNotValid if the mimetype is not valid
 */
private void setMimetype(String mimetype){
  if (isMimetypeValid(mimetype)) {
    this.mimeType=mimetype;
  }
 else {
    throw new ArgumentNotValid(""String_Node_Str"" + mimetype);
  }
}",0.7269155206286837
89998,"public void setUp(){
  aRealURL=""String_Node_Str"" + ""String_Node_Str"";
  aRealMimetype=""String_Node_Str"";
  realData=""String_Node_Str"" + ""String_Node_Str"";
  anEmptyURL=""String_Node_Str"";
  anEmptyMimetype=""String_Node_Str"";
  emptyData=""String_Node_Str"";
}","public void setUp(){
  aRealURL=""String_Node_Str"" + ""String_Node_Str"";
  aRealMimetype=""String_Node_Str"";
  realData=""String_Node_Str"" + ""String_Node_Str"";
  anEmptyURL=""String_Node_Str"";
  anEmptyMimetype=""String_Node_Str"";
  emptyData=""String_Node_Str"";
  anInvalidUrl=""String_Node_Str"";
  anInvalidMimetype=""String_Node_Str"";
}",0.8756388415672913
89999,"public void testMetadataEntry(){
  try {
    new MetadataEntry(aNullURL,aRealMimetype,realData);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  try {
    new MetadataEntry(anEmptyURL,aRealMimetype,realData);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  try {
    new MetadataEntry(aRealURL,aNullMimetype,realData);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  try {
    new MetadataEntry(aRealURL,anEmptyMimetype,realData);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  try {
    new MetadataEntry(aRealURL,aRealMimetype,nullData);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  try {
    new MetadataEntry(aRealURL,aRealMimetype,emptyData);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  try {
    new MetadataEntry(aRealURL,aRealMimetype,realData);
  }
 catch (  ArgumentNotValid e) {
    fail(""String_Node_Str"");
  }
}","public void testMetadataEntry(){
  try {
    new MetadataEntry(aNullURL,aRealMimetype,realData);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  try {
    new MetadataEntry(anEmptyURL,aRealMimetype,realData);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  try {
    new MetadataEntry(aRealURL,aNullMimetype,realData);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  try {
    new MetadataEntry(aRealURL,anEmptyMimetype,realData);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  try {
    new MetadataEntry(aRealURL,aRealMimetype,nullData);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  try {
    new MetadataEntry(aRealURL,aRealMimetype,emptyData);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  try {
    new MetadataEntry(aRealURL,aRealMimetype,realData);
  }
 catch (  ArgumentNotValid e) {
    fail(""String_Node_Str"");
  }
  try {
    new MetadataEntry(anInvalidUrl,aRealMimetype,realData);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
  try {
    new MetadataEntry(aRealURL,anInvalidMimetype,realData);
    fail(""String_Node_Str"");
  }
 catch (  ArgumentNotValid e) {
  }
}",0.8753387533875339
90000,"/** 
 * Look up a given URI and return the contents as an InputStream.
 * @param uri The URI to find in the archive.  If the URI does notmatch any entries in the archive, IOFailure is thrown.
 * @return An InputStream Containing all the data in the entry, ornull if the entry was not found
 * @throws IOFailure If the ARC file was found in the Lucene index but notin the bit archive, or if some other failure happened while finding the file.
 */
public InputStream lookup(URI uri){
  ArgumentNotValid.checkNotNull(uri,""String_Node_Str"");
  ARCKey key=luceneLookup(uri.getScheme() + ""String_Node_Str"" + uri.getSchemeSpecificPart());
  if (key == null) {
    return null;
  }
 else {
    final BitarchiveRecord bitarchiveRecord=arcRepositoryClient.get(key.getFile().getName(),key.getOffset());
    if (bitarchiveRecord == null) {
      String message=""String_Node_Str"" + key.getFile().getName() + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
      log.debug(message);
      throw new IOFailure(message);
    }
    return bitarchiveRecord.getData();
  }
}","/** 
 * Look up a given URI and return the contents as an InputStream.
 * @param uri The URI to find in the archive.  If the URI does notmatch any entries in the archive, IOFailure is thrown.
 * @return An InputStream Containing all the data in the entry, ornull if the entry was not found
 * @throws IOFailure If the ARC file was found in the Lucene index but notin the bit archive, or if some other failure happened while finding the file.
 */
public InputStream lookup(URI uri){
  ArgumentNotValid.checkNotNull(uri,""String_Node_Str"");
  ARCKey key=luceneLookup(uri.getScheme() + ""String_Node_Str"" + uri.getSchemeSpecificPart());
  if (key == null) {
    key=luceneLookup(uri.getScheme() + ""String_Node_Str"" + uri.getRawSchemeSpecificPart());
  }
  if (key == null) {
    return null;
  }
 else {
    final BitarchiveRecord bitarchiveRecord=arcRepositoryClient.get(key.getFile().getName(),key.getOffset());
    if (bitarchiveRecord == null) {
      String message=""String_Node_Str"" + key.getFile().getName() + ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str""+ ""String_Node_Str"";
      log.debug(message);
      throw new IOFailure(message);
    }
    return bitarchiveRecord.getData();
  }
}",0.949590693666523
